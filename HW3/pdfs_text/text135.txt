Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

Yichen Chen 1 Dongdong Ge 2 Mengdi Wang 1 Zizhuo Wang 3 Yinyu Ye 4 Hao Yin 4

Abstract

Consider the regularized sparse minimization
problem, which involves empirical sums of loss
functions for n data points (each of dimension
d) and a nonconvex sparsity penalty. We prove
that ﬁnding an O(nc1dc2)-optimal solution to
the regularized sparse optimization problem is
strongly NP-hard for any c1, c2 ∈ [0, 1) such that
c1 + c2 < 1. The result applies to a broad class
of loss functions and sparse penalty functions.
It suggests that one cannot even approximately
solve the sparse optimization problem in polyno-
mial time, unless P = NP.

Keywords: Nonconvex optimization · Computational
complexity · NP-hardness · Concave penalty · Sparsity

1

Introduction

We study the sparse minimization problem, where the ob-
jective is the sum of empirical losses over input data and a
sparse penalty function. Such problems commonly arise
from empirical risk minimization and variable selection.
The role of the penalty function is to induce sparsity in the
optimal solution, i.e., to minimize the empirical loss using
as few nonzero coefﬁcients as possible.
Problem 1 Given the loss function (cid:96) : R × R (cid:55)→ R+,
penalty function p : R (cid:55)→ R+, and regularization parameter
λ > 0, consider the problem

min
x∈Rd

n
(cid:88)

i=1

(cid:96) (cid:0)aT

i x, bi

(cid:1) + λ

p (|xj|) ,

d
(cid:88)

j=1

where A = (a1, . . . , an)T ∈ Rn×d, b = (b1, . . . , bn)T
∈ Rn are input data.

1Princeton University, NJ, USA 2Shanghai University of Fi-
nance and Economics, Shanghai, China 3University of Minnesota,
MN, USA 4Stanford University, CA, USA. Correspondence to:
Mengdi Wang <mengdiw@princeton.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

We are interested in the computational complexity of Prob-
lem 1 under general conditions of the loss function (cid:96) and
the sparse penalty p. In particular, we focus on the case
where (cid:96) is a convex loss function and p is a concave penalty
with a unique minimizer at 0. Optimization problems with
convex (cid:96) and concave p are common in sparse regression,
compressive sensing, and sparse approximation. A list of
applicable examples of (cid:96) and p is given in Section 3.

For certain special cases of Problem 1, it has been shown
that ﬁnding an exact solution is strongly NP-hard (Huo &
Chen, 2010; Chen et al., 2014). However, these results have
not excluded the possibility of the existence of polynomial-
(Chen
time algorithms with small approximation error.
& Wang, 2016) established the hardness of approximately
solving Problem 1 when p is the L0 norm.

In this paper, we prove that it is strongly NP-hard to ap-
proximately solve Problem 1 within certain optimality er-
ror. More precisely, we show that there exists a lower
bound on the suboptimality error of any polynomial-time
deterministic algorithm. Our results apply to a variety of
optimization problems in estimation and machine learning.
Examples include sparse classiﬁcation, sparse logistic re-
gression, and many more. The strong NP-hardness of ap-
proximation is one of the strongest forms of complexity
result for continuous optimization. To our best knowledge,
this paper gives the ﬁrst and strongest set of hardness re-
sults for Problem 1 under very general assumptions regard-
ing the loss and penalty functions.

Our main contributions are three-fold.

1. We prove the strong NP-hardness for Problem 1 with
general loss functions. This is the ﬁrst results that
apply to the broad class of problems including but
not limited to: least squares regression, linear model
with Laplacian noise, robust regression, Poisson re-
gression, logistic regression, inverse Gaussian models,
etc.

2. We present a general condition on the sparse penalty
function p such that Problem 1 is strongly NP-hard.
The condition is a slight weaker version of strict con-
cavity. It is satisﬁed by typical penalty functions such
as the Lq norm (q ∈ [0, 1)), clipped L1 norm, SCAD,
etc. To the best of our knowledge, this is the most gen-

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

eral condition on the penalty function in the literature.

3. We prove that ﬁnding an O (λnc1dc2)-optimal solu-
tion to Problem 1 is strongly NP-hard, for any c1, c2 ∈
[0, 1) such that c1 + c2 < 1. Here the O(·) hides pa-
rameters that depend on the penalty function p, which
is to be speciﬁed later.
It illustrates a gap between
the optimization error achieved by any tractable algo-
rithm and the desired statistical precision. Our proof
provides a ﬁrst uniﬁed analysis that deals with a broad
class of problems taking the form of Problem 1.

Section 2 summarizes related literatures from optimization,
machine learning and statistics. Section 3 presents the key
assumptions and illustrates examples of loss and penalty
functions that satisfy the assumptions. Section 4 gives the
main results. Section 5 discusses the implications of our
hardness results. Section 6 provides a proof of the main
results in a simpliﬁed setting. The full proofs are deferred
to the appendix.

2 Background and Related Works

Sparse optimization is a powerful machine learning tool for
extracting useful information for massive data. In Problem
1, the sparse penalty serves to select the most relevant vari-
ables from a large number of variables, in order to avoid
overﬁtting. In recent years, nonconvex choices of p have re-
ceived much attention; see (Frank & Friedman, 1993; Fan
& Li, 2001; Chartrand, 2007; Candes et al., 2008; Fan &
Lv, 2010; Xue et al., 2012; Loh & Wainwright, 2013; Wang
et al., 2014; Fan et al., 2015).

Within the optimization and mathematical programming
community, the complexity of Problem 1 has been consid-
ered in a number of special cases. (Huo & Chen, 2010) ﬁrst
proved the hardness result for a relaxed family of penalty
functions with L2 loss. They show that for the penalties in
L0, hard-thresholded (Antoniadis & Fan, 2001) and SCAD
(Fan & Li, 2001), the above optimization problem is NP-
hard. (Chen et al., 2014) showed that the L2-Lp minimiza-
tion is strongly NP-hard when p ∈ (0, 1). At the same time,
(Bian & Chen, 2014) proved the strongly NP-hardness for
another class of penalty functions. The preceding existing
analyses mainly focused on ﬁnding an exact global opti-
mum to Problem 1. For this purpose, they implicitly as-
sumed that all the input and parameters involved in the re-
duction are rational numbers with a ﬁnite numerical repre-
sentation, otherwise ﬁnding a global optimum to a contin-
uous problem would be always intractable. A recent tech-
nical report (Chen & Wang, 2016) proves the hardness of
obtaining an (cid:15)-optimal solution when p is the L0 norm.

Within the theoretical computer science community, there
have been several early works on the complexity of sparse

recovery, beginning with (Arora et al., 1993). (Amaldi &
Kann, 1998) proved that the problem min{(cid:107)x(cid:107)0 | Ax = b}
is not approximable within a factor 2log1−(cid:15) d for any (cid:15) > 0.
(Natarajan, 1995) showed that, given (cid:15) > 0, A and b, the
problem min{(cid:107)x(cid:107)0 | (cid:107)Ax − b(cid:107)2 ≤ (cid:15)} is NP-hard. (Davis
et al., 1997) proved a similar result that for some given
(cid:15) > 0 and M > 0, it is NP-complete to ﬁnd a solution
x such that (cid:107)x(cid:107)0 ≤ M and (cid:107)Ax − b(cid:107) ≤ (cid:15). More recently,
(Foster et al., 2015) studied sparse recovery and sparse lin-
ear regression with subgaussian noises. Assuming that the
true solution is K-sparse, it showed that no polynomial-
time (randomized) algorithm can ﬁnd a K · 2log1−δ d-sparse
solution x with (cid:107)Ax−b(cid:107)2
2 ≤ dC1 n1−C2 with high probabil-
ity, where δ, C1, C2 are arbitrary positive scalars. Another
work (Zhang et al., 2014) showed that under the Gaussian
linear model, there exists a gap between the mean square
loss that can be achieved by polynomial-time algorithms
and the statistically optimal mean squared error. These two
works focus on estimation of linear models and impose dis-
tributional assumptions regarding the input data. These re-
sults on estimation are different in nature with our results
on optimization.

In contrast, we focus on the optimization problem itself.
Our results apply to a variety of loss functions and penalty
functions, not limited to linear regression. Moreover, we do
not make any distributional assumption regarding the input
data.

There remain several open questions. First, existing results
mainly considered least square problems or Lq minimiza-
tion problems. Second, existing results focused mainly on
the L0 penalty function. The complexity of Problem 1 with
general loss function and penalty function is yet to be es-
tablished. Things get complicated when p is a continuous
function instead of the discrete L0 norm function. The
complexity for ﬁnding an (cid:15)-optimal solution with general
(cid:96) and p is not fully understood. We will address these ques-
tions in this paper.

3 Assumptions

In this section, we state the two critical assumptions that
lead to the strong NP-hardness results: one for the penalty
function p, the other one for the loss function (cid:96). We ar-
gue that these assumptions are essential and very general.
They apply to a broad class of loss functions and penalty
functions that are commonly used.

3.1 Assumption About Sparse Penalty

Throughout this paper, we make the following assumption
regarding the sparse penalty function p(·).

Assumption 1. The penalty function p(·) satisﬁes the fol-

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

lowing conditions:

3.2 Assumption About Loss Function

(i) (Monotonicity) p(·) is non-decreasing on [0, +∞).

(ii) (Concavity) There exists τ > 0 such that p(·) is con-

cave but not linear on [0, τ ].

In words, condition (ii) means that the concave penalty p(·)
is nonlinear. Assumption 1 is the most general condition
on penalty functions in the existing literature of sparse op-
timization. Below we present a few such examples.

1. In variable selection problems, the L0 penalization
p(t) = I{t(cid:54)=0} arises naturally as a penalty for the
number of factors selected.

2. A natural generalization of the L0 penalization is the
Lp penalization p(t) = tp where (0 < p < 1).
The corresponding minimization problem is called the
bridge regression problem (Frank & Friedman, 1993).

3. To obtain a hard-thresholding estimator, Antoniadis &
Fan (2001) use the penalty functions pγ(t) = γ2 −
((γ − t)+)2 with γ > 0, where (x)+ := max{x, 0}
denotes the positive part of x.

4. Any penalty function that belongs to the folded con-
cave penalty family (Fan et al., 2014) satisﬁes the con-
ditions in Theorem 1. Examples include the SCAD
(Fan & Li, 2001) and the MCP (Zhang, 2010a),
whose derivatives on (0, +∞) are p(cid:48)
γ(t) = γI{t≤γ} +
(aγ−t)+
b )+, respec-
a−1

γ(t) = (γ − t

and p(cid:48)

I{t>γ}

tively, where γ > 0, a > 2 and b > 1.

5. The conditions in Theorem 1 are also satisﬁed by the
clipped L1 penalty function (Antoniadis & Fan, 2001;
Zhang, 2010b) pγ(t) = γ · min(t, γ) with γ > 0.
This is a special case of the piecewise linear penalty
function:

(cid:26) k1t

p(t) =

k2t + (k1 − k2)a if t > a

if 0 ≤ t ≤ a

where 0 ≤ k2 < k1 and a > 0.

6. Another family of penalty functions which bridges the
L0 and L1 penalties are the fraction penalty functions

pγ(t) =

(γ + 1)t
γ + t

with γ > 0 (Lv & Fan, 2009).

7. The family of log-penalty functions:

pγ(t) =

1
log(1 + γ)

log(1 + γt)

We state our assumption about the loss function (cid:96).
Assumption 2. Let M be an arbitrary constant. For any
interval [τ1, τ2] where 0 < τ1 < τ2 < M , there exists
k ∈ Z+ and b ∈ Qk such that h(y) = (cid:80)k
i=1 (cid:96)(y, bi) has
the following properties:

(i) h(y) is convex and Lipschitz continuous on [τ1, τ2].

(ii) h(y) has a unique minimizer y∗ in (τ1, τ2).
(iii) There exists N ∈ Z+, ¯δ ∈ Q+ and C ∈ Q+ such that

when δ ∈ (0, ¯δ), we have

h(y∗ ± δ) − h(y∗)
δN

≥ C.

(iv) h(y∗), {bi}k

i=1 can be represented in O(log

1
τ2−τ1

)

bits.

Assumption 2 is a critical, but very general, assumption
regarding the loss function (cid:96)(y, b). Condition (i) requires
convexity and Lipschitz continuity within a neighborhood.
Conditions (ii), (iii) essentially require that, given an in-
terval [τ1, τ2], one can artiﬁcially pick b1, . . . , bk to con-
struct a function h(y) = (cid:80)k
i=1 (cid:96)(y, bi) such that h has its
unique minimizer in [τ1, τ2] and has enough curvature near
the minimizer. This property ensures that a bound on the
minimal value of h(y) can be translated to a meaningful
bound on the minimizer y∗. The conditions (i), (ii), (iii)
are typical properties that a loss function usually satisﬁes.
Condition (iv) is a technical condition that is used to avoid
dealing with inﬁnitely-long irrational numbers. It can be
easily veriﬁed for almost all common loss functions.

We will show that Assumptions 2 is satisﬁed by a variety
of loss functions. An (incomplete) list is given below.

1. In the least squares regression, the loss function has

the form

n
(cid:88)

i=1

(cid:0)aT

i x − bi

(cid:1)2

.

Using our notation, the corresponding loss function
is (cid:96)(y, b) = (y − b)2. For all τ1, τ2, we choose an
arbitrary b(cid:48) ∈ [τ1, τ2]. We can verify that h(y) =
(cid:96)(y, b(cid:48)) satisﬁes all the conditions in Assumption 2.

2. In the linear model with Laplacian noise, the negative

log-likelihood function is

n
(cid:88)

i=1

(cid:12)
(cid:12)aT

i x − bi

(cid:12)
(cid:12) .

with γ > 0, also bridges the L0 and L1 penalties (Can-
des et al., 2008).

So the loss function is (cid:96)(y, b) = |y − b|. As in the case
of least squares regression, the loss function satisfy

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

Assumption 2. Similar argument also holds when we
consider the Lq loss | · |q with q ≥ 1.

3. In robust regression, we consider the Huber loss (Hu-
ber, 1964) which is a mixture of L1 and L2 norms.
The loss function takes the form

Lδ(y, b) =

(cid:26) 1

2 |y − b|2
δ(|y − b| − 1

2 δ)

for |y − b| ≤ δ,
otherwise.

for some δ > 0 where y = aT x. We then verify that
Assumption 2 is satisﬁed. For any interval [τ1, τ2], we
pick an arbitrary b ∈ [τ1, τ2] and let h(y) = (cid:96)(y, b).
We can see that h(y) satisﬁes all the conditions in As-
sumption 2.

4. In Poisson regression (Cameron & Trivedi, 2013), the

negative log-likelihood minimization is

min
x∈Rd

− log L(x; A, b) = min
x∈Rd

(exp(aT

i x)−bi·aT

i x).

n
(cid:88)

i=1

We now show that (cid:96)(y, b) = ey − b · y satisﬁes As-
sumption 2. For any interval [τ1, τ2], we choose q and
r such that q/r ∈ [eτ1, eτ2]. Note that eτ2 − eτ1 =
eτ1+τ2−τ1 − eτ1 ≥ τ2 − τ1. Also, eτ2 is bounded
by eM . Thus, q, r can be chosen to be polynomial
in (cid:100)1/(τ2 − τ1)(cid:101) by letting r = (cid:100)1/(τ2 − τ1)(cid:101) and q
be some number less than r · eM . Then, we choose
k = r and b ∈ Zk such that h(y) = (cid:80)k
i=1 (cid:96)(y, bi) =
r · ey − q · y. Let us verify Assumption 2. (i), (iv)
are straightforward by our construction. For (ii), note
that h(y) take its minimum at ln(q/r) which is inside
[τ1, τ2] by our construction. To verify (iii), consider
the second order Taylor expansion of h(y) at ln(q/r),

h(y + δ) − h(y) =

· δ2 + o(δ2) ≥

+ o(δ2),

r · ey
2

δ2
2

We can see that (iii) is satisﬁed. Therefore, Assump-
tion 2 is satisﬁed.

at y = ln q/r
ond order Taylor expansion at y = ln q/r
is

1−q/r . To verify (iii), we consider the sec-
1−q/r , which

h(y + δ) − h(y) =

q
2(1 + ey)

δ2 + o(δ2)

where y ∈ [τ1, τ2]. Note that ey is bounded by eM ,
which can be computed beforehand. As a result, (iii)
holds as well.

6. In the mean estimation of inverse Gaussian models
(McCullagh, 1984), the negative log-likelihood func-
tion minimization is

min
x∈Rd

n
(cid:88)

i=1

(bi · (cid:112)aT
i x − 1)2
bi

.

the loss function (cid:96)(y, b) =

Now we show that
(b·

√

y−1)2
b

satisﬁes Assumption 2. By setting the
derivative to be zero with regard to y, we can see that
y take its minimum at y = 1/b2. Thus for any [τ1, τ2],
we choose b(cid:48) = q/r ∈ [1/
τ1]. We can see
that h(y) = (cid:96)(y, b(cid:48)) satisﬁes all the conditions in As-
sumption 2.

τ2, 1/

√

√

7. In the estimation of generalized linear model under the
exponential distribution (McCullagh, 1984), the nega-
tive log-likelihood function minimization is

min
x∈Rd

− log L(x; A, b) = min
x∈Rd

bi
aT
i x

+ log(aT

i x).

By setting the derivative to 0 with regard to y, we can
see that (cid:96)(y, b) = b
y + log y has a unique minimizer at
y = b. Thus by choosing b(cid:48) ∈ [τ1, τ2] appropriately,
we can readily show that h(y) = (cid:96)(y, b(cid:48)) satisﬁes all
the conditions in Assumption 2.

To sum up, the combination of any loss function given in
Section 3.1 and any penalty function given in Section 3.2
results in a strongly NP-hard optimization problem.

5. In logistic regression,
function minimization is

the negative log-likelihood

4 Main Results

min
x∈Rd

n
(cid:88)

i=1

log(1 + exp(aT

i x)) −

bi · aT

i x.

n
(cid:88)

i=1

We claim that the loss function (cid:96)(y, b) = log(1 +
exp(y)) − b · y satisﬁes Assumption 2. By a similar ar-
gument as the one in Poisson regression, we can verify
that h(y) = (cid:80)r
i=1 (cid:96)(y, bi) = r log(1 + exp(y)) − qy
where q/r ∈ [ eτ1
eτ2
1+eτ2 ] and q, r are polynomial
1+eτ1 ,
in (cid:100)1/(τ2 −τ1)(cid:101) satisﬁes all the conditions in Assump-
tion 2. For (ii), observe that (cid:96)(y, b) take its minimum

In this section, we state our main results on the strong NP-
hardness of Problem 1. We warm up with a preliminary
result for a special case of Problem 1.
Theorem 1 (A Preliminary Result). Let Assumption 1
hold, and let p(·) be twice continuously differentiable in
(0, ∞). Then the minimization problem

min
x∈Rn

(cid:107)Ax − b(cid:107)q

q + λ

p(|xj|),

(1)

d
(cid:88)

j=1

is strongly NP-hard.

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

The result shows that many of the penalized least squares
problems, e.g., (Fan & Lv, 2010), while enjoying small
estimation errors, are hard to compute.
It suggests that
there does not exist a fully polynomial-time approximation
scheme for Problem 1. It has not answered the question:
whether one can approximately solve Problem 1 within cer-
tain constant error.

Now we show that it is not even possible to efﬁciently ap-
proximate the global optimal solution of Problem 1, unless
P = N P . Given an optimization problem minx∈X f (x),
we say that a solution ¯x is (cid:15)-optimal if ¯x ∈ X and f (¯x) ≤
inf x∈X f (x) + (cid:15).

Theorem 2 (Strong NP-Hardness of Problem 1). Let As-
sumptions 1 and 2 hold, and let c1, c2 ∈ [0, 1) be ar-
bitrary such that c1 + c2 < 1. Then it is strongly NP-
hard to ﬁnd a λ · κ · nc1dc2-optimal solution of Prob-
lem 1, where d is the dimension of variable space and
κ = mint∈[τ /2,τ ]{ 2p(t/2)−p(t)

}.

t

The non-approximable error in Theorem 2 involves the
constant κ which is determined by the sparse penalty func-
tion p. In the case where p is the L0 norm function, we can
take κ = 1. In the case of piecewise linear L1 penalty, we
have κ = (k1 − k2)/4. In the case of SCAD penalty, we
have κ = Θ(γ2).

According to Theorem 2, the non-approximable error λ ·
κ · nc1dc2 is determined by three factors: (i) properties of
the regularization penalty λ · κ; (ii) data size n; and (iii) di-
mension or number of variables d. This result illustrates a
fundamental gap that can not be closed by any polynomial-
time deterministic algorithm. This gap scales up when ei-
ther the data size or the number of variables increases. In
Section 5.1, we will see that this gap is substantially larger
than the desired estimation precision in a special case of
sparse linear regression.

Theorems 1 and 2 validate the long-lasting belief that op-
timization involving nonconvex penalty is hard. More im-
portantly, Theorem 2 provide lower bounds for the opti-
mization error that can be achieved by any polynomial-time
algorithm. This is one of the strongest forms of hardness
result for continuous optimization.

5 An Application and Remarks

In this section, we analyze the strong NP-hardness results
in the special case of linear regression with SCAD penalty
(Problem 1). We give a few remarks on the implication of
our hardness results.

5.1 Hardness of Regression with SCAD

Penalty

Let us try to understand how signiﬁcant
is the non-
approximable error of Problem 1. We consider the special
case of linear models with SCAD penalty. Let the input
data (A, b) be generated by the linear model A¯x + ε = b,
where ¯x is the unknown true sparse coefﬁcients and ε is a
zero-mean multivariate subgaussian noise. Given the data
size n and variable dimension d, we follow (Fan & Li,
2001) and obtain a special case of Problem 1, given by

min
x

1
2

(cid:107)Ax − b(cid:107)2

2 + n

pγ(|xj|),

(2)

d
(cid:88)

j=1

where γ = (cid:112)log d/n. (Fan & Li, 2001) showed that the
optimal solution x∗ of problem (2) has a small statistical
(cid:1) , where an =
error, i.e., (cid:107)¯x − x∗(cid:107)2
max{p(cid:48)
λ(|x∗
j (cid:54)= 0}. (Fan et al., 2015) further showed
n log d-optimal solution to (2)
that we only need to ﬁnd a
to achieve such a small estimation error.

2 = O (cid:0)n−1/2 + an

j |) : x∗

√

However, Theorem 2 tells us that it is not possible to com-
pute an (cid:15)d,n-optimal solution for problem (2) in polynomial
time, where (cid:15)d,n = λκn1/2d1/3 (by letting c1 = 1/2, c2 =
1/3). In the special case of problem (2), we can verify that
λ = n and κ = Ω(γ2) = Ω(log d/n). As a result, we see
that

(cid:15)d,n = Ω(n1/2d1/3) (cid:29) (cid:112)n log d,
for high values of the dimension d. According to Theorem
2, it is strongly NP-hard to approximately solve problem
n log d. This
(2) within the required statistical precision
result illustrates a sharp contrast between statistical proper-
ties of sparse estimation and the worst-case computational
complexity.

√

5.2 Remarks on the NP-Hardness Results

illustrated by the preceding analysis,

the non-
As
approximibility of Problem 1 suggests that computing the
sparse estimator is hard. The results suggest a funda-
mental conﬂict between computation efﬁciency and esti-
mation accuracy in sparse data analysis. Although the re-
sults seem negative, they should not discourage researchers
from studying computational perspectives of sparse opti-
mization. We make the following remarks:

1. Theorems 1, 2 are worst-case complexity results.
They suggest that one cannot ﬁnd a tractable solution
to the sparse optimization problems, without making
any additional assumption to rule out the worst-case
instances.

2. Our results do not exclude the possibility that, under
more stringent modeling and distributional assump-

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

tions, the problem would be tractable with high prob-
ability or on average.

(τ0, τ );

2. gθ,µ(t) has a unique global minimizer t∗(θ, µ) ∈

In short, the sparse optimization Problem 1 is fundamen-
tally hard from a purely computational perspective. This
paper together with the prior related works provide a com-
plete answer to the computational complexity of sparse op-
timization.

6 Proof of Theorem 1

In this section, we prove Theorem 1. The proof of The-
orems 2 is deferred to the appendix which is based on
the idea of the proof in this section. We construct a
polynomial-time reduction from the 3-partition problem
(Garey & Johnson, 1978) to the sparse optimization prob-
lem. Given a set S of 3m integers s1, ...s3m, the three par-
tition problem is to determine whether S can be partitioned
into m triplets such that the sum of the numbers in each
subset is equal. This problem is known to be strongly NP-
hard (Garey & Johnson, 1978). The main proof idea bears
a similar spirit as the works by Huo & Chen (2010), Chen
et al. (2014) and Chen & Wang (2016). The proofs of all
the lemmas can be found in the appendix.

We ﬁrst illustrate several properties of the penalty function
if it satisﬁes the conditions in Theorem 1.
Lemma 3. If p(t) satisﬁes the conditions in Theorem 1,
then for any l ≥ 2, and any t1, t2, . . . , tl ∈ R, we have
p(|t1|) + · · · + p(|tl|) ≥ min{p(|t1 + · · · + tl|), p(τ )}.
Lemma 4. If p(t) satisﬁes the conditions in Theorem 1,
then there exists τ0 ∈ (0, τ ) such that p(·) is concave
but not linear on [0, τ0] and is twice continuously differ-
entiable on [τ0, τ ]. Furthermore, for any ˜t ∈ (τ0, τ ), let
¯δ = min{τ0/3, ˜t − τ0, τ − ˜t}. Then for any δ ∈ (0, ¯δ)
l ≥ 2, and any t1, t2, . . . , tl such that t1 + · · · + tl = ˜t, we
have

p(|t1|) + · · · + p(|tl|) < p(˜t) + C1δ
only if |ti − ˜t| < δ for some i while |tj| < δ for all j (cid:54)= i,
where C1 = p(τ0/3)+p(2τ0/3)−p(τ0)

> 0.

τ0/3

In our proof of Theorem 1, we will consider the following
function

gθ,µ(t) := p(|t|) + θ · |t|q + µ · |t − ˆτ |q

with θ, µ > 0, where ˆτ is an arbitrary ﬁxed rational number
in (τ0, τ ). We have the following lemma about gθ,µ(t).
Lemma 5. If p(t) satisﬁes the conditions in Theorem 1,
q > 1, and τ0 satisﬁes the properties in Lemma 4, then
there exist θ > 0 and µ > 0 such that for any θ ≥ θ and
µ ≥ µ · θ, the following properties are satisﬁed:

1. g(cid:48)(cid:48)

θ,µ(t) ≥ 1 for any t ∈ [τ0, τ ];

3. Let ¯δ = min{t∗(θ, µ) − τ0, τ − t∗(θ, µ), 1}, then for
any δ ∈ (0, ¯δ), we have gθ,µ(t) < h(θ, µ) + δ2 only if
|t − t∗(θ, µ)| < δ, where h(θ, µ) is the minimal value
of gθ,µ(t).

Lemma 6. If p(t) satisﬁes the conditions in Theorem 1,
q = 1, and τ0 satisﬁes the properties in Lemma 4, then
there exist ˆµ > 0 such that for any µ ≥ ˆµ, the following
properties are satisﬁed:

1. g(cid:48)

0,µ(t) < −1 for any t ∈ [τ0, ˆτ ) and g(cid:48)
any t ∈ (ˆτ , τ ];

0,µ(t) > 1 for

2. g0,µ(t) has a unique global minimizer t∗(0, µ) = ˆτ ∈

(τ0, τ );

3. Let ¯δ = min{ˆτ − τ0, τ − ˆτ , 1}, then for any δ ∈ (0, ¯δ),
we have g0,µ(t) < h(0, µ) + δ2 only if |t − ˆτ | < δ.

By combining the above results, we have the following
lemma, which is useful in our proof of Theorem 1.

Lemma 7. Suppose p(t) satisﬁes the conditions in The-
orem 1 and τ0 satisﬁes the properties in Lemma 4. Let
h(θ, µ) and t∗(θ, µ) be as deﬁned in Lemma 5 and Lemma
6 respectively for the case q > 1 and q = 1. Then we can
ﬁnd θ and µ such that for any l ≥ 2, t1, . . . , tl ∈ R,

l
(cid:88)

j=1

p(|tj|) + θ ·

tj

+ µ ·

≥ h(θ, µ).

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

l
(cid:88)

j=1

q

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

q

(cid:12)
(cid:12)
l
(cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

j=1

(cid:12)
(cid:12)
(cid:12)
tj − ˆτ
(cid:12)
(cid:12)
(cid:12)

(cid:110) τ0

Moreover, let ¯δ = min
where C1 is deﬁned in Lemma 4, then for any δ ∈ (0, ¯δ),
we have

3 , t∗(θ,µ)−τ0

, τ −t∗(θ,µ)
2

, 1, C1

2

(cid:111)

l
(cid:88)

j=1

p(|tj|) + θ ·

tj

+ µ ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

l
(cid:88)

j=1

(cid:12)
q
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

l
(cid:88)

j=1

q

(cid:12)
(cid:12)
(cid:12)
tj − ˆτ
(cid:12)
(cid:12)
(cid:12)

< h(θ, µ) + δ2

(3)
holds only if |ti − t∗(θ, µ)| < 2δ for some i while |tj| ≤ δ
for all j (cid:54)= i.

Proof of Theorem 1. We present a polynomial
time re-
duction to problem (1) from the 3-partition problem.
For any given instance of the 3-partition problem with
b = (b1, . . . , b3m), we consider the minimization problem
minx f (x) in the form of (1) with x = {xij}, 1 ≤ i ≤

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

3m, 1 ≤ j ≤ m, where

f (x) :=

bixij −

bixi1

+

3m
(cid:88)

i=1

m
(cid:88)

3m
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

j=2

+

3m
(cid:88)

i=1

i=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
q

(λµ)





m
(cid:88)

j=1

q

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

q

3m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
q

(λθ)

m
(cid:88)

j=1

xij

q

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

xij − ˆτ

+ λ

p(|xij|).

3m
(cid:88)

m
(cid:88)

i=1

j=1

Note that the lower bounds θ, µ, and ˆµ only depend on
the penalty function p(·), we can choose θ ≥ θ and µ ≥
µθ if q > 1, or θ = 0 and µ ≥ ˆµ if q = 1, such that
(λθ)1/q and (λµ)1/q are both rational numbers. Since ˆτ is
also rational, all the coefﬁcients of f (x) are of ﬁnite size
and independent of the input size of the given 3-partition
instance. Therefore, the minimization problem minx f (x)
has polynomial size with respect to the given 3-partition
instance.

For any x, by Lemma 7,

f (x) ≥0 + λ ·

p(|xij|) + θ ·

xij

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m
(cid:88)

j=1

(cid:12)
q
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(4)

3m
(cid:88)

(cid:40) m
(cid:88)

i=1

j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m
(cid:88)

j=1

(cid:12)
q (cid:41)
(cid:12)
(cid:12)
xij − ˆτ
(cid:12)
(cid:12)
(cid:12)

+ µ ·

≥ 3mλ · h(θ, µ).

Now we claim that there exists an equitable partition to the
3-partition problem if and only if the optimal value of f (x)
is smaller than 3mλ · h(θ, µ) + (cid:15) where (cid:15) is speciﬁed later.
On one hand, if S can be equally partitioned into m subsets,
then we deﬁne

xij =

(cid:26) t∗(θ, µ)
0

if bi belongs to the jth subset;
otherwise.

It can be easily veriﬁed that these xij’s satisfy f (x) =
3mλ · h(θ, µ). Then due to (4), we know that these xij’s
provide an optimal solution to f (x) with optimal value
3mλ · h(θ, µ).

On the other hand, suppose the optimal value of f (x) is
3mλ · h(θ, µ), and there is a polynomial-time algorithm
that solves (1). Then for

(cid:40)

τ0
8 (cid:80)3m
i=1 bi

(cid:41)

, ¯δ

δ = min

where

and

(cid:15) = min{λδ2, (τ0/2)q}

¯δ = min

,

t∗(θ, µ) − τ0
2

(cid:26) τ0
3
p(τ0/3) + p(2τ0/3) − p(τ0)
τ0/3

τ − t∗(θ, µ)
2

, 1

,

,

(cid:27)

,

we are able to ﬁnd a near-optimal solution x such that
f (x) < 3mλ · h(θ, µ) + (cid:15) within a polynomial time of
log(1/(cid:15)) and the size of f (x), which is polynomial with
respect to the size of the given 3-partition instance. Now
we show that we can ﬁnd an equitable partition based on
this near-optimal solution. By the deﬁnition of (cid:15), f (x) <
3mλ · h(θ, µ) + (cid:15) implies

m
(cid:88)

j=1

p(|xij|) + θ

<h(θ, µ) + δ2,

q

xij

m
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∀i = 1, . . . , 3m.

+ µ ·

j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m
(cid:88)

j=1

q

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

xij − τ

(5)

According to Lemma 7, for each i = 1, . . . , 3m, (5) im-
plies that there exists k such that |xik − t∗(θ, µ)| < 2δ and
|xij| < δ for any j (cid:54)= k. Now let

yij =

(cid:26) t∗(θ, µ)
0

if |xik − t∗(θ, µ)| < 2δ
if |xij| < δ

.

We deﬁne a partition by assigning bi to the jth subset Sj if
yij = t∗(θ, µ). Note that this partition is well-deﬁned since
for each i, by the deﬁnition of δ, there exists one and only
one yik = t∗(θ, µ) while the others equal 0. Now we show
that this is an equitable partition.

Note that for any j = 1, . . . , m, the difference between the
sum of the j-th subset and the ﬁrst subset is
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

yij
t∗(θ, µ)

yi1
t∗(θ, µ)

· bi −

3m
(cid:88)

3m
(cid:88)

bi −

(cid:88)

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

· bi

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

i=1

=

bi

S1

Sj

=

1
t∗(θ, µ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

3m
(cid:88)

i=1

biyij −

biyi1

.

3m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

By triangle inequality, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

Sj

bi −

bi

≤

(cid:88)

S1

1
t∗(θ, µ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:32) 3m
(cid:88)

i=1

3m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

bi · |yij − xij|

3m
(cid:88)

i=1

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

bi · |yi1 − xi1| +

bixij −

bixi1

.

3m
(cid:88)

i=1

By the deﬁnition of yij, we have |yij − xij| < 2δ for any
i, j. for the last term, since f (x) < 3mλ · h(θ, µ) + (cid:15), we
know that
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< (cid:15)1/q ≤ τ0/2.

bixij −

bixi1

n
(cid:88)

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

i=1

Therefore, we have
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

bi −

(cid:88)

bi

S1

Sj

(cid:32)

<

1
t∗(θ, µ)

n
(cid:88)

i=1

(cid:33)

τ0
2

4δ

bi +

≤ 1.

Now since bi’s are all integers, we must have (cid:80)
(cid:80)
bi, which means that the partition is equitable.

Sj

S1

bi =

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

References

Amaldi, E. and Kann, V. On the approximability of min-
imizing nonzero variables or unsatisﬁed relations in lin-
ear systems. Theoretical Computer Science, 209(1):237–
260, 1998.

Antoniadis, A. and Fan, J. Regularization of wavelet ap-
proximations. Journal of the American Statistical Asso-
ciation, 96(455):939–967, 2001.

Arora, S., Babai, L., Stern, J., and Sweedy, Z. The hardness
of approximate optima in lattices, codes, and systems
In Foundations of Computer Sci-
of linear equations.
ence, 1993. Proceedings., 34th Annual Symposium on,
pp. 724–733. IEEE, 1993.

Bian, W. and Chen, X. Optimality conditions and complex-
ity for non-lipschitz constrained optimization problems.
Preprint, 2014.

Cameron, A. C. and Trivedi, P. K. Regression analysis
of count data, volume 53. Cambridge university press,
2013.

Candes, E., Wakin, M., and Boyd, S. Enhancing sparsity by
reweighted L1 minimization. Journal of Fourier Analy-
sis and Applications, 14(5-6):877–905, 2008.

Chartrand, R. Exact reconstruction of sparse signals via
Signal Processing Letters,

nonconvex minimization.
IEEE, 14(10):707–710, 2007.

Fan,

J., Liu, H., Sun, Q., and Zhang, T.

TAC
for sparse learning: Simultaneous control of algorith-
arXiv preprint
mic complexity and statistical error.
arXiv:1507.01037, 2015.

Foster, D., Karloff, H., and Thaler, J. Variable selection is

hard. In COLT, pp. 696–709, 2015.

Frank, L. E. and Friedman, J. H. A statistical view of some
chemometrics regression tools. Technometrics, 35(2):
109–135, 1993.

Garey, M. R. and Johnson, D. S. “Strong”NP-completeness
results: Motivation, examples, and implications. Journal
of the ACM (JACM), 25(3):499–508, 1978.

Huber, P. J. Robust estimation of a location parameter. The
Annals of Mathematical Statistics, 35(1):73–101, 1964.

Huo, X. and Chen, J. Complexity of penalized likelihood
estimation. Journal of Statistical Computation and Sim-
ulation, 80(7):747–759, 2010.

Loh, P.-L. and Wainwright, M. J. Regularized M-estimators
with nonconvexity: Statistical and algorithmic theory for
In Advances in Neural Information Pro-
local optima.
cessing Systems, pp. 476–484, 2013.

Lv, J. and Fan, Y. A uniﬁed approach to model selection
and sparse recovery using regularized least squares. The
Annals of Statistics, 37(6A):3498–3528, 2009.

McCullagh, P. Generalized linear models. European Jour-
nal of Operational Research, 16(3):285–292, 1984.

Chen, X., Ge, D., Wang, Z., and Ye, Y. Complexity of un-
constrained L2 − Lp minimization. Mathematical Pro-
gramming, 143(1-2):371–383, 2014.

Natarajan, B. K. Sparse approximate solutions to linear
systems. SIAM journal on computing, 24(2):227–234,
1995.

Chen, Y. and Wang, M. Hardness of approximation for
sparse optimization with L0 norm. Technical Report,
2016.

Davis, G., Mallat, S., and Avellaneda, M. Adaptive greedy
approximations. Constructive approximation, 13(1):57–
98, 1997.

Fan, J. and Li, R. Variable selection via nonconcave pe-
Journal
nalized likelihood and its oracle properties.
of the American Statistical Association, 96(456):1348–
1360, 2001.

Wang, Z., Liu, H., and Zhang, T. Optimal computational
and statistical rates of convergence for sparse noncon-
vex learning problems. Annals of statistics, 42(6):2164,
2014.

Xue, L., Zou, H., Cai, T., et al. Nonconcave penalized
composite conditional likelihood estimation of sparse
ising models. The Annals of Statistics, 40(3):1403–1429,
2012.

Zhang, C.-H. Nearly unbiased variable selection under
minimax concave penalty. The Annals of Statistics, 38
(2):894–942, 2010a.

Fan, J. and Lv, J. A selective overview of variable selection
in high dimensional feature space. Statistica Sinica, 20
(1):101–148, 2010.

Zhang, T. Analysis of multi-stage convex relaxation for
sparse regularization. Journal of Machine Learning Re-
search, 11:1081–1107, 2010b.

Fan, J., Xue, L., and Zou, H. Strong oracle optimality
of folded concave penalized estimation. The Annals of
Statistics, 42(3):819–849, 2014.

Zhang, Y., Wainwright, M. J., and Jordan, M. I. Lower
bounds on the performance of polynomial-time algo-
rithms for sparse linear regression. In COLT, 2014.

