A. Detailed Proof of Main Theorem

How to Escape Saddle Points Efﬁciently

In this section, we give detailed proof for the main theorem. We will ﬁrst state two key lemmas that show how the algorithm
can make progress when the gradient is large or near a saddle point, and show how the main theorem follows from the two
lemmas. Then we will focus on the novel technique in this paper: how to analyze gradient descent near saddle point.

A.1. General Framework

In order to prove the main theorem, we need to show that the algorithm will not be stuck at any point that either has a large
gradient or is a saddle point. This idea is similar to previous works (e.g.(Ge et al., 2015)). We ﬁrst state a standard Lemma
that shows if the current gradient is large, then we make progress in function value.
Lemma 12. Assume f (·) satisﬁes A1, then for gradient descent with stepsize η < 1

(cid:96) , we have:

Proof. By Assumption A1 and its property, we have:

f (xt+1) ≤ f (xt) −

(cid:107)∇f (xt)(cid:107)2

η
2

(cid:96)
2

f (xt+1) ≤f (xt) + ∇f (xt)(cid:62)(xt+1 − xt) +

(cid:107)xt+1 − xt(cid:107)2

=f (xt) − η(cid:107)∇f (xt)(cid:107)2 +

(cid:107)∇f (xt)(cid:107)2 ≤ f (xt) −

(cid:107)∇f (xt)(cid:107)2

η2(cid:96)
2

η
2

The next lemma says that if we are “close to a saddle points”, i.e., we are at a point where the gradient is small, but the
Hessian has a reasonably large negative eigenvalue. This is the main difﬁculty in the analysis. We show a perturbation
followed by small (polylog) number of standard gradient descent steps can also make the function value decrease with high
probability.
Lemma 13. There exist absolute constant cmax,
η, r, gthres, fthres, tthres calculated same way as in Algorithm 2. Then, if ˜xt satisﬁes:

for f (·) satisﬁes A1, and any c ≤ cmax, and χ ≥ 1.

Let

(cid:107)∇f (˜xt)(cid:107) ≤ gthres

and

λmin(∇2f (˜xt)) ≤ −

√

ρ(cid:15)

Let xt = ˜xt + ξt where ξt comes from the uniform distribution over B0(r), and let xt+i be the iterates of gradient descent
from xt with stepsize η, then with at least probability 1 − d(cid:96)√

ρ(cid:15) e−χ, we have:

f (xt+tthres) − f (˜xt) ≤ −fthres

The proof of this lemma is deferred to Section A.2. Using this Lemma, we can then prove the main Theorem.
Theorem 3. There exist absolute constant cmax such that: if f (·) satisﬁes A1, then for any δ > 0, (cid:15) ≤ (cid:96)2
ρ , ∆f ≥ f (x0)−f (cid:63),
and constant c ≤ cmax, with probability 1 − δ, the output of PGD(x0, (cid:96), ρ, (cid:15), c, δ, ∆f ) will be (cid:15)−second order stationary
point, and terminate in iterations:

O

(cid:18) (cid:96)(f (x0) − f (cid:63))
(cid:15)2

log4

(cid:18) d(cid:96)∆f
(cid:15)2δ

(cid:19)(cid:19)

Proof. Denote ˜cmax to be the absolute constant allowed in Theorem 13. In this theorem, we let cmax = min{˜cmax, 1/2},
and choose any constant c ≤ cmax.

In this proof, we will actually achieve some point satisfying following condition:

(cid:107)∇f (x)(cid:107) ≤ gthres =

√
c
χ2 · (cid:15),

λmin(∇2f (x)) ≥ −

√

ρ(cid:15)

(3)

Since c ≤ 1, χ ≥ 1, we have

√
c
χ2 ≤ 1, which implies any x satisfy Eq.(3) is also a (cid:15)-second-order stationary point.

Starting from x0, we know if x0 does not satisfy Eq.(3), there are only two possibilities:

1. (cid:107)∇f (x0)(cid:107) > gthres: In this case, Algorithm 2 will not add perturbation. By Lemma 12:

How to Escape Saddle Points Efﬁciently

f (x1) − f (x0) ≤ −

· g2

thres = −

η
2

c2
2χ4 ·

(cid:15)2
(cid:96)

2. (cid:107)∇f (x0)(cid:107) ≤ gthres: In this case, Algorithm 2 will add a perturbation of radius r, and will perform gradient descent
(without perturbations) for the next tthres steps. Algorithm 2 will then check termination condition. If the condition is
not met, we must have:

This means on average every step decreases the function value by

f (xtthres ) − f (x0) ≤ −fthres = −

(cid:115)

(cid:15)3
ρ

c
χ3 ·

f (xtthres ) − f (x0)
tthres

≤ −

c3
χ4 ·

(cid:15)2
(cid:96)

In case 1, we can repeat this argument for t = 1 and in case 2, we can repeat this argument for t = tthres. Hence, we can
conclude as long as algorithm 2 has not terminated yet, on average, every step decrease function value by at least c3
χ4 · (cid:15)2
(cid:96) .
However, we clearly can not decrease function value by more than f (x0) − f (cid:63), where f (cid:63) is the function value of global
minima. This means algorithm 2 must terminate within the following number of iterations:

f (x0) − f (cid:63)
χ4 · (cid:15)2
c3

(cid:96)

=

χ4
c3 ·

(cid:96)(f (x0) − f (cid:63))
(cid:15)2

= O

(cid:18) (cid:96)(f (x0) − f (cid:63))
(cid:15)2

log4

(cid:18) d(cid:96)∆f
(cid:15)2δ

(cid:19)(cid:19)

Finally, we would like to ensure when Algorithm 2 terminates, the point it ﬁnds is actually an (cid:15)-second-order stationary
point. The algorithm can only terminate when the gradient is small, and the function value does not decrease after a
perturbation and tthres iterations. We shall show every time when we add perturbation to iterate ˜xt, if λmin(∇2f (˜xt)) <
ρ(cid:15), then we will have f (xt+tthres) − f (˜xt) ≤ −fthres. Thus, whenever the current point is not an (cid:15)-second-order
−
stationary point, the algorithm cannot terminate.

√

According to Algorithm 2, we immediately know (cid:107)∇f (˜xt)(cid:107) ≤ gthres (otherwise we will not add perturbation at time t).
By Lemma 13, we know this event happens with probability at least 1 − d(cid:96)√
ρ(cid:15) e−χ each time. On the other hand, during one
entire run of Algorithm 2, the number of times we add perturbations is at most:

1
tthres

·

χ4
c3 ·

(cid:96)(f (x0) − f (cid:63))
(cid:15)2

=

χ3
c

ρ(cid:15)(f (x0) − f (cid:63))
(cid:15)2

√

By union bound, for all these perturbations, with high probability Lemma 13 is satisﬁed. As a result Algorithm 2 works
correctly. The probability of that is at least

√

1 −

d(cid:96)
√
ρ(cid:15)

e−χ ·

χ3
c

ρ(cid:15)(f (x0) − f (cid:63))
(cid:15)2

= 1 −

χ3e−χ
c

·

d(cid:96)(f (x0) − f (cid:63))
(cid:15)2

Recall our choice of χ = 3 max{log( d(cid:96)∆f

c(cid:15)2δ ), 4}. Since χ ≥ 12, we have χ3e−χ ≤ e−χ/3, this gives:

χ3e−χ
c

·

d(cid:96)(f (x0) − f (cid:63))
(cid:15)2

≤ e−χ/3 d(cid:96)(f (x0) − f (cid:63))

≤ δ

c(cid:15)2

which ﬁnishes the proof.

A.2. Main Lemma: Escaping from Saddle Points Quickly

How to Escape Saddle Points Efﬁciently

Now we prove the main lemma, which shows near a saddle point, a small perturbation followed by a small number of
gradient descent steps will decrease the function value with high probability. This is the main step where we need new
analysis, as the analysis previous works (e.g.(Ge et al., 2015)) do not work when the step size and perturbation do not
depend polynomially in dimension d.

Intuitively, after adding a perturbation, the current point of the algorithm is a uniform distribution over a d-dimensional ball
centered at ˜x, which we call perturbation ball. After a small number of gradient steps, some points in this ball (which we
call the escaping region) will signiﬁcantly decrease the function; other points (which we call the stuck region) does not
see a signiﬁcant decrease in function value. We hope to show that the escaping region constitutes at least 1 − δ fraction of
the volume of the perturbation ball.

However, we do not know the exact form of the function near the saddle point, so the escaping region does not have a
clean analytic description. Explicitly computing its volume can be very difﬁcult. Our proof rely on a crucial observation:
although we do not know the shape of the stuck region, we know the “width” of it must be small, therefore it cannot have
a large volume. We will formalize this intuition later in Lemma 15.

The proof of the main lemma requires carefully balancing between different quantities including function value, gradient,
parameter space and number of iterations. For clarify, we deﬁne following scalar quantities, which serve as the “units” for
function value, gradient, parameter space, and time (iterations). We will use these notations throughout the proof.

Let the condition number be the ratio of the smoothness parameter (largest eigenvalue of Hessian) and the negative eigen-
value γ: κ = (cid:96)/γ ≥ 1, we deﬁne the following units:

F := η(cid:96)

γ3
ρ2 · log−3(

dκ
δ

),

S := (cid:112)η(cid:96)

· log−1(

γ
ρ

dκ
δ

),

G := (cid:112)η(cid:96)

· log−2(

dκ
δ

)

γ2
ρ
log( dκ
δ )
ηγ

T :=

Intuitively, if we plug in our choice of step size η(cid:96) = O(1) (which we will prove later) and hide the logarithmic de-
pendences, we have F = ˜O( γ3
ρ ), which is the only way to correctly discribe the units of
function value, gradient, parameter space by just γ and ρ. Moreover, these units are closely related, in particular, we know
(cid:113) F ·log( dκ
δ )

ρ2 ), G = ˜O( γ2

ρ ), S = ˜O( γ

=

G ·log( dκ
δ )
γ

= S .

γ

For simplicity of later proofs, we ﬁrst restate Lemma 13 into a slightly more general form as follows. Lemma 13 is directly
implied following lemma.
Lemma 14 (Lemma 13 restated). There exists universal constant cmax, for f (·) satisﬁes A1, for any δ ∈ (0, dκ
we start with point ˜x satisfying following conditions:

e ], suppose

(cid:107)∇f (˜x)(cid:107) ≤ G

and

λmin(∇2f (˜x)) ≤ −γ

Let x0 = ˜x + ξ where ξ come from the uniform distribution over ball with radius S /(κ · log( dκ
δ )), and let xt be the iterates
of gradient descent from x0. Then, when stepsize η ≤ cmax/(cid:96), with at least probability 1 − δ, we have following for any
T ≥ 1

T :

cmax

f (xT ) − f (˜x) ≤ −F

Lemma 14 is almost the same as Lemma 13. It is easy to verify that by substituting η = c
Lemma 14, we immediately obtain Lemma 13.

√

(cid:96) , γ =

ρ(cid:15) and δ = d(cid:96)√

ρ(cid:15) e−χ into

Now we will formalize the intuition that the “width” of stuck region is small.
Lemma 15. There exists a universal constant cmax, for any δ ∈ (0, dκ
e ], let f (·), ˜x satisﬁes the conditions in Lemma 14,
and without loss of generality let e1 be the minimum eigenvector of ∇2f (˜x). Consider two gradient descent sequences
{ut}, {wt} with initial points u0, w0 satisfying: (denote radius r = S /(κ · log( dκ

δ )))
√

(cid:107)u0 − ˜x(cid:107) ≤ r, w0 = u0 + µ · r · e1, µ ∈ [δ/(2

d), 1]

Then, for any stepsize η ≤ cmax/(cid:96), and any T ≥ 1
cmax

T , we have:
min{f (uT ) − f (u0), f (wT ) − f (w0)} ≤ −2.5F

How to Escape Saddle Points Efﬁciently

Intuitively, lemma 15 claims for any two points u0, w0 inside the perturbation ball, if u0 − w0 lies in the direction of
minimum eigenvector of ∇2f (˜x), and (cid:107)u0 − w0(cid:107) is greater than threshold δr/(2
d), then at least one of two sequences
{ut}, {wt} will “efﬁciently escape saddle point”. In other words, if u0 is a point in the stuck region, consider any point
d) from u0, it must be in the escaping
w0 that is on a straight line along direction of e1. As long as w0 is slightly far (δr/
region. This is what we mean by the “width” of the stuck region being small.Now we prove the main Lemma using this
observation:

√

√

Proof of Lemma 14. By adding perturbation, in worst case we increase function value by:

f (x0) − f (˜x) ≤ ∇f (˜x)(cid:62)ξ +

(cid:107)ξ(cid:107)2 ≤ G (

(cid:96)
2

S
κ · log( dκ
δ )

) +

(cid:96)(

1
2

S
κ · log( dκ
δ )

)2 ≤

F

3
2

S
κ·log( dκ

δ ) . We know x0 come froms uniform distribution over B˜x(r). Let Xstuck ⊂ B˜x(r)
On the other hand, let radius r =
denote the set of bad starting points so that if x0 ∈ Xstuck, then f (xT ) − f (x0) > −2.5F (thus stuck at a saddle point);
otherwise if x0 ∈ B˜x(r) − Xstuck, we have f (xT ) − f (x0) ≤ −2.5F .
By applying Lemma 15, we know for any x0 ∈ Xstuck, it is guaranteed that (x0 ± µre1) (cid:54)∈ Xstuck where µ ∈ [ δ
, 1].
√
2
Denote IXstuck (·) be the indicator function of being inside set Xstuck; and vector x = (x(1), x(−1)), where x(1) is the
component along e1 direction, and x(−1) is the remaining d − 1 dimensional vector. Recall B(d)(r) be d-dimensional ball
with radius r; By calculus, this gives an upper bound on the volumn of Xstuck:
(cid:90)

d

Vol(Xstuck) =

dx · IXstuck(x)

B(d)

˜x (r)

(cid:90)

(cid:90)

=

≤

B(d−1)

(r)

˜x

B(d−1)

(r)

˜x

√

(cid:90) ˜x(1)+
√

dx(−1)

r2−(cid:107)˜x(−1)−x(−1)(cid:107)2

dx(1) · IXstuck (x)

dx(−1) ·

˜x(1)−
(cid:18)

2 ·

(cid:19)

r2−(cid:107)˜x(−1)−x(−1)(cid:107)2
δ
√
2

= Vol(B(d−1)

d

r

0

(r)) ×

δr
√
d

Then, we immediately have the ratio:

Vol(Xstuck)
Vol(B(d)
˜x (r))

δr√
d

≤

× Vol(B(d−1)
0
Vol(B(d)

0 (r))

(r))

=

δ
√
πd

Γ( d
Γ( d

2 + 1)
2 + 1
2 )

≤

δ
√
πd

·

(cid:114)

d
2

+

≤ δ

1
2

The second last inequality is by the property of Gamma function that Γ(x+1)
with at least probability 1 − δ, x0 (cid:54)∈ Xstuck. In this case, we have:

Γ(x+1/2) <

(cid:113)

x + 1

2 as long as x ≥ 0. Therefore,

f (xT ) − f (˜x) =f (xT ) − f (x0) + f (x0) − f (0)

≤ − 2.5F + 1.5F ≤ −F

which ﬁnishes the proof.

A.3. Bounding the Width of Stuck Region

In order to prove Lemma 15, we do it in two steps:

1. We ﬁrst show if gradient descent from u0 does not decrease function value, then all the iterates must lie within a small

ball around u0 (Lemma 16).

2. If gradient descent starting from a point u0 stuck in a small ball around a saddle point, then gradient descent from w0

(moving u0 along e1 direction for at least a certain distance), will decreases the function value (Lemma 17).

Recall we assumed without loss of generality e1 is the minimum eigenvector of ∇2f (˜x).
H := ∇2f (˜x), and for simplicity of calculation, we consider following quadratic approximation:

In this context, we denote

˜fy(x) := f (y) + ∇f (y)(cid:62)(x − y) +

(x − y)(cid:62)H(x − y)

(4)

1
2

How to Escape Saddle Points Efﬁciently

Now we are ready to state two lemmas formally:
Lemma 16. For any constant ˆc ≥ 3, there exists absolute constant cmax: for any δ ∈ (0, dκ
condition in Lemma 14, for any initial point u0 with (cid:107)u0 − ˜x(cid:107) ≤ 2S /(κ · log( dκ

e ], let f (·), ˜x satisﬁes the

T = min

(cid:110)

inf
t

(cid:110)
t| ˜fu0 (ut) − f (u0) ≤ −3F

δ )), deﬁne:
(cid:111)
(cid:111)

, ˆcT

then, for any η ≤ cmax/(cid:96), we have for all t < T that (cid:107)ut − ˜x(cid:107) ≤ 100(S · ˆc).
Lemma 17. There exists absolute constant cmax, ˆc such that: for any δ ∈ (0, dκ
Lemma 14, and sequences {ut}, {wt} satisfy the conditions in Lemma 15, deﬁne:

e ], let f (·), ˜x satisﬁes the condition in

T = min

(cid:110)

inf
t

(cid:110)
t| ˜fw0(wt) − f (w0) ≤ −3F

(cid:111)

, ˆcT

(cid:111)

then, for any η ≤ cmax/(cid:96), if (cid:107)ut − ˜x(cid:107) ≤ 100(S · ˆc) for all t < T , we will have T < ˆcT .

Note the conclusion T < ˆcT in Lemma 17 equivalently means:

(cid:110)
t| ˜fw0 (wt) − f (w0) ≤ −3F

(cid:111)

< ˆcT

inf
t

That is, for some T < ˆcT , {wt} sequence “escape the saddle point” in the sense of sufﬁcient function value decrease
˜fw0(wt) − f (w0) ≤ −3F . Now, we are ready to prove Lemma 15.

Proof of Lemma 15. W.L.O.G, let ˜x = 0 be the origin. Let (c(2)
also let c(1)
max, c(2)
min{c(1)
T (cid:63) := ˆcT and deﬁne:

max, ˆc) be the absolute constant so that Lemma 17 holds,
max be the absolute constant to make Lemma 16 holds based on our current choice of ˆc. We choose cmax ≤
max} so that our step size η ≤ cmax/(cid:96) is small enough which make both Lemma 16 and Lemma 17 hold. Let

T (cid:48) = inf
t

(cid:110)
t| ˜fu0(ut) − f (u0) ≤ −3F

(cid:111)

Let’s consider following two cases:

Case T (cid:48) ≤ T (cid:63):

In this case, by Lemma 16, we know (cid:107)uT (cid:48)−1(cid:107) ≤ O(S ), and therefore

(cid:107)uT (cid:48)(cid:107) ≤(cid:107)uT (cid:48)−1(cid:107) + η(cid:107)∇f (uT (cid:48)−1)(cid:107) ≤ (cid:107)uT (cid:48)−1(cid:107) + η(cid:107)∇f (˜x)(cid:107) + η(cid:96)(cid:107)uT (cid:48)−1(cid:107) ≤ O(S )

By choosing cmax small enough and η ≤ cmax/(cid:96), this gives:

f (uT (cid:48)) − f (u0) ≤∇f (u0)(cid:62)(uT (cid:48) − u0) +

(uT (cid:48) − u0)(cid:62)∇2f (u0)(uT (cid:48) − u0) +

(cid:107)uT (cid:48) − u0(cid:107)3

ρ
6

≤ ˜fu0 (ut) − f (u0) +
≤ − 3F + O(ρS 3) = −3F + O((cid:112)η(cid:96) · F ) ≤ −2.5F

(cid:107)u0 − ˜x(cid:107)(cid:107)uT (cid:48) − u0(cid:107)2 +

(cid:107)uT (cid:48) − u0(cid:107)3

ρ
6

1
2

ρ
2

By choose cmax ≤ min{1, 1
Therefore, for any T ≥ 1
cmax

ˆc }. We know η < 1
T ≥ ˆcT = T (cid:63) ≥ T (cid:48), we have:

(cid:96) , by Lemma 12, we know gradient descent always decrease function value.

f (uT ) − f (u0) ≤ f (uT (cid:63) ) − f (u0) ≤ f (uT (cid:48)) − f (u0) ≤ −2.5F

Case T (cid:48) > T (cid:63):

In this case, by Lemma 16, we know (cid:107)ut(cid:107) ≤ O(S ) for all t ≤ T (cid:63). Deﬁne

T (cid:48)(cid:48) = inf
t

(cid:110)
t| ˜fw0(wt) − f (w0) ≤ −2F

(cid:111)

By Lemma 17, we immediately have T (cid:48)(cid:48) ≤ T (cid:63). Apply same argument as in ﬁrst case, we have for all T ≥ 1
cmax
f (wT ) − f (w0) ≤ f (wT (cid:63) ) − f (w0) ≤ −2.5F .

T that

Next we ﬁnish the proof by proving Lemma 16 and Lemma 17.

A.3.1. PROOF OF LEMMA 16

How to Escape Saddle Points Efﬁciently

In Lemma 16, we hope to show if the function value did not decrease, then all the iterations must be constrained in a
small ball. We do that by analyzing the dynamics of the iterations, and we decompose the d-dimensional space into
two subspaces: a subspace S which is the span of signiﬁcantly negative eigenvectors of the Hessian and its orthogonal
compliment.
Recall notation H := ∇2f (˜x) and quadratic approximation ˜fy(x) as deﬁned in Eq.(4). Since δ ∈ (0, dκ
log( dκ
δ ) ≥ 1. W.L.O.G, set u0 = 0 to be the origin, by gradient descent update function, we have:

e ], we always have

ut+1 =ut − η∇f (ut)

=ut − η∇f (0) − η

∇2f (θut)dθ

ut

(cid:21)

(cid:20)(cid:90) 1

0

=ut − η∇f (0) − η(H + ∆t)ut
=(I − ηH − η∆t)ut − η∇f (0)

Here, ∆t := (cid:82) 1
gradient, we have (cid:107)∇f (0)(cid:107) ≤ (cid:107)∇f (˜x)(cid:107) + (cid:96)(cid:107)˜x(cid:107) ≤ G + (cid:96) · 2S /(κ · log( dκ

0 ∇2f (θut)dθ − H. By Hessian Lipschitz, we have (cid:107)∆t(cid:107) ≤ ρ((cid:107)ut(cid:107) + (cid:107)˜x(cid:107)), and by smoothness of the

δ )) ≤ 3G .

We will now compute the projections of ut in different eigenspaces of H. Let S be the subspace spanned by all eigenvectors
of H whose eigenvalue is less than − γ
δ ) . S c denotes the subspace of remaining eigenvectors. Let αt and βt denote
the projections of ut onto S and S c respectively i.e., αt = PS ut, and βt = PS cut. We can decompose the update
equations Eq.(5) into:

ˆc log( dκ

αt+1 =(I − ηH)αt − ηPS ∆tut − ηPS ∇f (0)
βt+1 =(I − ηH)βt − ηPS c∆tut − ηPS c∇f (0)

By deﬁnition of T , we know for all t < T :

−3F < ˜f0(ut) − f (0) =∇f (0)(cid:62)ut −

u(cid:62)
t Hut ≤ ∇f (0)(cid:62)ut −

1
2

γ
2

(cid:107)αt(cid:107)2
ˆc log( dκ
δ )

+

β(cid:62)

t Hβt

1
2

Combined with the fact (cid:107)ut(cid:107)2 = (cid:107)αt(cid:107)2 + (cid:107)βt(cid:107)2, we have:

(cid:107)ut(cid:107)2 ≤

(cid:18)

2ˆc log( dκ
δ )
γ

3F + ∇f (0)(cid:62)ut +

(cid:19)

+ (cid:107)βt(cid:107)2

≤14 · max

(cid:40) G ˆc log( dκ
δ )
γ

(cid:107)ut(cid:107),

t Hβtˆc log( dκ
β(cid:62)
δ )
γ

, (cid:107)βt(cid:107)2

(cid:41)

t Hβt

β(cid:62)

1
2
F ˆc log( dκ
δ )
γ

,

where last inequality is due to (cid:107)∇f (0)(cid:107) ≤ 3G . This gives:

(cid:107)ut(cid:107) ≤ 14 · max






(cid:115)

(cid:115)

G ˆc log( dκ
δ )
γ

,

F ˆc log( dκ
δ )
γ

,

t Hβtˆc log( dκ
β(cid:62)
δ )
γ

, (cid:107)βt(cid:107)






Now, we use induction to prove that

(cid:107)ut(cid:107) ≤ 100(S · ˆc)

Clearly Eq.(9) is true for t = 0 since u0 = 0. Suppose Eq.(9) is true for all τ ≤ t. We will now show that Eq.(9) holds for
t + 1 < T . Note that by the deﬁnition of S , F and G , we only need to bound the last two terms of Eq.(8) i.e., (cid:107)βt+1(cid:107)
and β(cid:62)

t+1Hβt+1.

By update function of βt (Eq.(7)), we have:

βt+1 ≤(I − ηH)βt + ηδt

(5)

(6)

(7)

(8)

(9)

(10)

and the norm of δt is bounded as follows:

How to Escape Saddle Points Efﬁciently

(cid:107)δt(cid:107) ≤ (cid:107)∆t(cid:107)(cid:107)ut(cid:107) + (cid:107)∇f (0)(cid:107)

≤ ρ ((cid:107)ut(cid:107) + (cid:107)˜x(cid:107)) (cid:107)ut(cid:107) + (cid:107)∇f (0)(cid:107)

≤ ρ · 100ˆc(100ˆc + 2/(κ · log(
= [100ˆc(100ˆc + 2)(cid:112)η(cid:96) + 1]G ≤ 2G

dκ
δ

)))S 2 + G

The last step follows by choosing small enough constant cmax ≤

1

100ˆc(100ˆc+2) and stepsize η < cmax/(cid:96).

Bounding (cid:107)βt+1(cid:107): Combining Eq.(10), Eq.(11) and using the deﬁniton of S c, we have:

Since (cid:107)β0(cid:107) = 0 and t + 1 ≤ T , by applying above relation recurrsively, we have:

(cid:107)βt+1(cid:107) ≤ (1 +

)(cid:107)βt(cid:107) + 2ηG

ηγ
ˆc log( dκ
δ )

(11)

(cid:107)βt+1(cid:107) ≤

2(1 +

)τ ηG ≤ 2 · 3 · T ηG ≤ 6(S · ˆc)

(12)

t
(cid:88)

τ =0

ηγ
ˆc log( dκ
δ )

The second last inequality is because T ≤ ˆcT by deﬁnition, so that (1 + ηγ

)T ≤ 3.

ˆc log( dκ
δ )

Bounding β(cid:62)

t+1Hβt+1: Using Eq.(10), we can also write the update equation as:

Combining with Eq.(11), this gives

βt =

(I − ηH)τ δτ

t−1
(cid:88)

τ =0

t+1Hβt+1 =η2
β(cid:62)

δ(cid:62)
τ1

(I − ηH)τ1 H(I − ηH)τ2δτ2

t
(cid:88)

t
(cid:88)

τ1=0
t
(cid:88)

τ2=0
t
(cid:88)

τ1=0

τ2=0
t
(cid:88)

t
(cid:88)

≤4η2G 2

τ1=0

τ2=0

≤η2

(cid:107)δτ1 (cid:107)(cid:107)(I − ηH)τ1 H(I − ηH)τ2(cid:107)(cid:107)δτ2 (cid:107)

(cid:107)(I − ηH)τ1 H(I − ηH)τ2 (cid:107)

Let the eigenvalues of H to be {λi}, then for any τ1, τ2 ≥ 0, we know the eigenvalues of (I − ηH)τ1H(I − ηH)τ2 are
{λi(1 − ηλi)τ1+τ2}. Let gt(λ) := λ(1 − ηλ)t, and setting its derivative to zero, we obtain:

∇gt(λ) = (1 − ηλ)t − tηλ(1 − ηλ)t−1 = 0

We see that λ(cid:63)

t = 1

(1+t)η is the unique maximizer, and gt(λ) is monotonically increasing in (−∞, λ(cid:63)

t ]. This gives:

(cid:107)(I − ηH)τ1H(I − ηH)τ2(cid:107) = max

λi(1 − ηλi)τ1+τ2 ≤ ˆλ(1 − ηˆλ)τ1+τ2 ≤

i

1
(1 + τ1 + τ2)η

where ˆλ = min{(cid:96), λ(cid:63)

τ1+τ2

}. Therefore, we have:

t+1Hβt+1 ≤4η2G 2
β(cid:62)

(cid:107)(I − ηH)τ1H(I − ηH)τ2 (cid:107)

t
(cid:88)

t
(cid:88)

τ1=0
t
(cid:88)

τ2=0
t
(cid:88)

τ1=0

τ2=0

1
1 + τ1 + τ2

≤4ηG 2

≤ 8ηT G 2 ≤ 8S 2γˆc · log−1(

(13)

dκ
δ

)

The second last inequality is because by rearrange summation:

How to Escape Saddle Points Efﬁciently

t
(cid:88)

t
(cid:88)

τ1=0

τ2=0

1
1 + τ1 + τ2

2t
(cid:88)

τ =0

=

min{1 + τ, 2t + 1 − τ } ·

≤ 2t + 1 < 2T

1
1 + τ

Finally, substitue Eq.(12) and Eq.(13) into Eq.(8), this gives:




(cid:115)

(cid:115)

G ˆc log( dκ
δ )
γ

,

F ˆc log( dκ
δ )
γ

,

t Hβtˆc log( dκ
β(cid:62)
δ )
γ

, (cid:107)βt(cid:107)






(cid:107)ut+1(cid:107) ≤14 · max


≤100(S · ˆc)

This ﬁnishes the induction as well as the proof of the lemma.

A.3.2. PROOF OF LEMMA 17

In this Lemma we try to show if all the iterates from u0 are constrained in a small ball, iterates from w0 must be able
to decrease the function value. In order to do that, we keep track of vector v which is the difference between u and w.
Similar as before, we also decompose v into different eigenspaces. However, this time we only care about the projection
of v on the direction e1 and its orthognal subspace.
Again, recall notation H := ∇2f (˜x), e1 as minimum eigenvector of H and quadratic approximation ˜fy(x) as deﬁned in
Eq.(4). Since δ ∈ (0, dκ
δ ) ≥ 1. W.L.O.G, set u0 = 0 to be the origin. Deﬁne vt = wt − ut, by
assumptions in Lemma 17, we have v0 = µ[S /(κ · log( dκ
d), 1]. Now, consider the update equation
for wt:

e ], we always have log( dκ

δ ))]e1, µ ∈ [δ/(2

√

ut+1 + vt+1 = wt+1 =wt − η∇f (wt)

=ut + vt − η∇f (ut + vt)

=ut + vt − η∇f (ut) − η

∇2f (ut + θvt)dθ

vt

(cid:21)

(cid:20)(cid:90) 1

0

=ut + vt − η∇f (ut) − η(H + ∆(cid:48)
=ut − η∇f (ut) + (I − ηH − η∆(cid:48)

t)vt
t)vt

t := (cid:82) 1
where ∆(cid:48)
dynamic for vt satisfy:

0 ∇2f (ut + θvt)dθ − H. By Hessian Lipschitz, we have (cid:107)∆(cid:48)

t(cid:107) ≤ ρ((cid:107)ut(cid:107) + (cid:107)vt(cid:107) + (cid:107)˜x(cid:107)). This gives the

vt+1 = (I − ηH − η∆(cid:48)

t)vt

Since (cid:107)w0 − ˜x(cid:107) ≤ (cid:107)u0 − ˜x(cid:107) + (cid:107)v0(cid:107) ≤ S /(κ · log( dκ
t ≤ T . By condition of Lemma 17, we know (cid:107)ut(cid:107) ≤ 100(S · ˆc) for all t < T . This gives:

δ )), directly applying Lemma 16, we know wt ≤ 100(S · ˆc) for all

(14)

(15)

This in sum gives for t < T :

(cid:107)vt(cid:107) ≤ (cid:107)ut(cid:107) + (cid:107)wt(cid:107) ≤ 200(S · ˆc) for all t < T

(cid:107)∆(cid:48)

t(cid:107) ≤ ρ((cid:107)ut(cid:107) + (cid:107)vt(cid:107) + (cid:107)˜x(cid:107)) ≤ ρ(300ˆcS + S /(κ · log(

))) ≤ ρS (300ˆc + 1)

dκ
δ

On the other hand, denote ψt be the norm of vt projected onto e1 direction, and ϕt be the norm of vt projected onto
remaining subspace. Eq.(14) gives us:

ψt+1 ≥(1 + γη)ψt − µ

ψ2

t + ϕ2
t

ϕt+1 ≤(1 + γη)ϕt + µ

ψ2

t + ϕ2
t

(cid:113)

(cid:113)

where µ = ηρS (300ˆc + 1). We will now prove via induction that for all t < T :

How to Escape Saddle Points Efﬁciently

By hypothesis of Lemma 17, we know ϕ0 = 0, thus the base case of induction holds. Assume Eq.(16) is true for τ ≤ t,
For t + 1 ≤ T , we have:

ϕt ≤ 4µt · ψt

(16)

4µ(t + 1)ψt+1 ≥4µ(t + 1)

(1 + γη)ψt − µ

(cid:18)

(cid:113)

ψ2

t + ϕ2
t

(cid:19)

ϕt+1 ≤4µt(1 + γη)ψt + µ

(cid:113)

ψ2

t + ϕ2
t

From above inequalities, we see that we only need to show:

By choosing

√

cmax ≤ 1

300ˆc+1 min{ 1

√

2

2

, 1
4ˆc }, and η ≤ cmax/(cid:96), we have

(1 + 4µ(t + 1))

ψ2

t + ϕ2

t ≤ 4(1 + γη)ψt

(cid:113)

4µ(t + 1) ≤ 4µT ≤ 4ηρS (300ˆc + 1)ˆcT = 4(cid:112)η(cid:96)(300ˆc + 1)ˆc ≤ 1

This gives:

which ﬁnishes the induction.

4(1 + γη)ψt ≥ 4ψt ≤ 2

2ψ2

t ≥ (1 + 4µ(t + 1))

ψ2

t + ϕ2
t

(cid:113)

(cid:113)

Now, we know ϕt ≤ 4µt · ψt ≤ ψt, this gives:

ψt+1 ≥ (1 + γη)ψt −

2µψt ≥ (1 +

)ψt

(17)

γη
2

where the last step follows from µ = ηρS (300ˆc + 1) ≤

cmax(300ˆc + 1)γη · log−1( dκ

δ ) < γη

√

2

2

.

Finally, combining Eq.(15) and (17) we have for all t < T :

200(S · ˆc) ≥(cid:107)vt(cid:107) ≥ ψt ≥ (1 +

)tψ0

=(1 +

γη
2

)tc0

S

κ

) ≥ (1 +

γη
2

)t δ
√
2

S

d

κ

log−1(

dκ
δ

)

√

√

γη
2
log−1(

dκ
δ

This implies:

√

T <

1
2

log(400 κ

δ

d

· ˆc log( dκ
log(1 + γη
2 )

δ ))

≤

√

δ

d

γη

log(400 κ

· ˆc log( dκ

δ ))

≤ (2 + log(400ˆc))T

The last inequality is due to δ ∈ (0, dκ
2 + log(400ˆc) ≤ ˆc, we will have T < ˆcT , which ﬁnishes the proof.

e ] we have log( dκ

δ ) ≥ 1. By choosing constant ˆc to be large enough to satisfy

B. Improve Convergence by Local Structure

In this section, we show if the objective function has nice local structure (e.g. satisﬁes Assumptions A3.a or A3.b), then it
is possible to combine our analysis with the local analysis in order to get very fast convergence to a local minimum.

In particular, we prove Theorem 5.
Theorem 5. There exist absolute constant cmax such that: if f (·) satisﬁes A1, A2, and A3.a (or A3.b), then for any
δ > 0, (cid:15) > 0, ∆f ≥ f (x0) − f (cid:63), constant c ≤ cmax, let ˜(cid:15) = min(θ, γ2/ρ), with probability 1 − δ, the output of
PGDli(x0, (cid:96), ρ, ˜(cid:15), c, δ, ∆f , β) will be (cid:15)-close to X (cid:63) in iterations:

O

(cid:18) (cid:96)(f (x0) − f (cid:63))
˜(cid:15)2

log4

(cid:19)

(cid:18) d(cid:96)∆f
˜(cid:15)2δ

+

log

β
α

(cid:19)

ζ
(cid:15)

How to Escape Saddle Points Efﬁciently

Proof. Theorem 5 runs PGDli(x0, (cid:96), ρ, ˜(cid:15), c, δ, ∆f , β). According to algorithm 3, we know it calls PGD(x0, (cid:96), ρ, (cid:15), c, δ, ∆f )
ﬁrst (denote its output as ˆx), then run standard gradient descent with step size 1

β starting from ˆx.

By Corollary 4, we know ˆx is already in the ζ-neighborhood of X (cid:63), where X (cid:63) is the set of local minima. Therefore, to
prove this theorem, we only need to show prove following two claims:

1. Suppose {xt} is the sequence of gradient descent starting from x0 = ˆx with step size 1

β , then xt is always in the

2. Local structure (assumption A3.a or A3.b) allows iterates to converge to points (cid:15)-close to X (cid:63) within O( β

α log ζ
(cid:15) )

ζ-neighborhood of X (cid:63).

iterations.

We will focus on Assumption A3.b (as we will later see Assumption A3.a is a special case of Assumption A3.b). Assume
xt is in ζ-neighborhood of X (cid:63), by gradient updates and the deﬁnition of projection, we have:

(cid:107)xt+1 − PX (cid:63) (xt+1)(cid:107)2 ≤(cid:107)xt+1 − PX (cid:63) (xt)(cid:107)2 = (cid:107)xt − η∇f (xt) − PX (cid:63) (xt)(cid:107)2

=(cid:107)xt − PX (cid:63) (xt)(cid:107)2 − 2η(cid:104)∇f (xt), xt − PX (cid:63) (xt)(cid:105) + η2(cid:107)∇f (xt)(cid:107)2
)(cid:107)∇f (x)(cid:107)2
≤(cid:107)xt − PX (cid:63) (xt)(cid:107)2 − ηα(cid:107)xt − PX (cid:63) (xt)(cid:107)2 + (η2 −

η
β

≤(1 −

)(cid:107)xt − PX (cid:63) (xt)(cid:107)2

α
β

The second last inequality is due to (α, β)-regularity condition. The last inequality is because of the choice η = 1
β .

There are two consequences of this calculation. First, it shows (cid:107)xt+1 − PX (cid:63) (xt+1)(cid:107)2 ≤ (cid:107)xt − PX (cid:63) (xt)(cid:107)2. As a result if
xt in ζ-neighborhood of X (cid:63), xt+1 is also in this ζ-neighborhood. Since x0 is in the ζ-neighborhood by Corollary 4, by
induction we know all later iterations are in the same neighborhood.

Now, since we know all the points xt are in the neighborhood, the equation also shows linear convergence rate (1 − α
β ).
The initial distance is bounded by (cid:107)x0 − PX (cid:63) (x0)(cid:107) ≤ ζ, therefore to converge to points (cid:15)-close to X (cid:63), we only need the
following number of iterations:

log((cid:15)/ζ)
log(1 − α/β)

= O(

log

).

β
α

ζ
(cid:15)

This ﬁnishes the proof under Assumption A3.b.

Finally, we argue assumption A3.a implies A3.b. First, notice that if a function is locally strongly convex, then its local
minima are isolated: for any two points x, x(cid:48) ∈ X (cid:63), the local region Bx(ζ) and Bx(cid:48)(ζ) must be disjoint (otherwise function
f (x) is strongly convex in connected domain Bx(ζ) ∪ Bx(cid:48)(ζ) but has two distinct local minima, which is impossible).
Therefore, W.L.O.G, it sufﬁces to consider one perticular disjoint region, with unique local minimum we denote as x(cid:63),
clearly, for all x ∈ Bx(cid:63) (ζ) we have PX (cid:63) (x) = x(cid:63).

Now by α-strong convexity:

f (x(cid:63)) ≥ f (x) + (cid:104)∇f (x), x(cid:63) − x(cid:105) +

(cid:107)x − x(cid:63)(cid:107)2

On the other hand, for any x in this ζ-neighborhood, we already proved x − 1
β-smoothness, we also have:

β ∇f (x) also in this ζ-neighborhood. By

(18)

(19)

f (x −

∇f (x)) ≤ f (x) −

(cid:107)∇f (x)(cid:107)2

1
2β

1
β

Combining Eq.(18) and Eq.(19), and using the fact f (x(cid:63)) ≤ f (x − 1

β ∇f (x)), we get:

(cid:104)∇f (x), x − x(cid:63)(cid:105) ≥

(cid:107)x − x(cid:63)(cid:107)2 +

(cid:107)∇f (x)(cid:107)2

α
2

which ﬁnishes the proof.

α
2

1
2β

C. Geometric Structures of Matrix Factorization Problem

How to Escape Saddle Points Efﬁciently

In this Section we investigate the global geometric structures of the matrix factorization problem. These properties are
summarized in Lemmas 6 and 7. Such structures allow us to apply our main Theorem and get fast convergence (as shown
in Theorem 8).

Note that our main results Theorems 3 and 5 are proved for functions f (·) whose input x is a vector. For the current
function in 2, though the input U ∈ Rd×r is a matrix, we can always vectorize it to be a vector in Rdr and apply our
results. However, for simplicity of presentation, we still write everything in matrix form (without explicit vectorization),
while the reader should keep in mind the operations are same if one vectorizes everything ﬁrst.

Recall for vectors we use (cid:107)·(cid:107) to denote the 2-norm, and for matrices we use (cid:107)·(cid:107) and (cid:107)·(cid:107)F to denote spectral norm, and
Frobenius norm respectively. Furthermore, we always use σi(·) to denote the i-th largest singular value of the matrix.

We ﬁrst show how the geometric properties (Lemma 6 and Lemma 7) imply a fast convergence (Theorem 8).
Theorem 8. There exists an absolute constant cmax such that
let Γ1/2
tion (2),
for any δ > 0 and constant c ≤ cmax,
PGDli(U0, 8Γ, 12Γ1/2,

the following holds.
:= 2 max{(cid:107)U0(cid:107), 3(σ(cid:63)

r )2

(σ(cid:63)
108Γ1/2 , c, δ, rΓ2

2 , 10σ(cid:63)

1), then:

For matrix factoriza-
1)1/2}, suppose we run

1. With probability 1, the iterates satisfy (cid:107)Ut(cid:107) ≤ Γ1/2 for every t ≥ 0.

2. With probability 1 − δ, the output will be (cid:15)-close to global minima set X (cid:63) in

(cid:32)
r

O

(cid:19)4

(cid:18) Γ
σ(cid:63)
r

log4

(cid:19)

(cid:18) dΓ
δσ(cid:63)
r

+

log

σ(cid:63)
1
σ(cid:63)
r

(cid:33)

σ(cid:63)
r
(cid:15)

iterations.

Proof of Theorem 8. Denote ˜cmax to be the absolute constant allowed in Theorem 5.
min{˜cmax, 1

2 }, and choose any constant c ≤ cmax.

In this theorem, we let cmax =

Theorem 8 consists of two parts. In part 1 we show that the iterations never bring the matrix to a very large norm, while in
part 2 we apply our main Theorem to get fast convergence. We will ﬁrst prove the bound on number of iterations assuming
the bound on the norm. We will then proceed to prove part 1.

Part 2: Assume part 1 of the theorem is true i.e., with probability 1, the iterates satisfy (cid:107)Ut(cid:107) ≤ Γ1/2 for every t ≥ 0. In
this case, although we are doing unconstrained optimization, we can still use the geometric properties that hold inside this
region. .

r )3/2, 1

By Lemma 6 and 7, we know objective
( 1
24 (σ(cid:63)
local minima (also global minima) X (cid:63). Furthermore, note f (cid:63) = 0 and recall Γ1/2 = 2 max{(cid:107)U0(cid:107), 3(σ(cid:63)
have:

r )1/2)-strict saddle, and holds ( 2

function Eq.(2)
r , 10σ(cid:63)
3 σ(cid:63)

1)-regularity condition in 1

12Γ1/2-Lipschitz Hessian,
r )1/2 neighborhood of
3 (σ(cid:63)
1)1/2}, then, we

is 8Γ-smooth,

3 (σ(cid:63)

3 σ(cid:63)

r , 1

f (U0) − f (cid:63) = (cid:107)U0U(cid:62)

0 − M(cid:63)(cid:107)2

F ≤ 2r(cid:107)U0U(cid:62)

0 − M(cid:63)(cid:107)2 ≤

rΓ2
2

.

Thus, we can choose ∆f = rΓ2
PGDli(U0, 8Γ, 12Γ1/2,
in iterations:

(σ(cid:63)
108Γ1/2 , c, δ, rΓ2

r )2

2 . Substituting the corresponding parameters from Theorem 5, we know by running
1), with probability 1 − δ, the output will be (cid:15)-close to global minima set X (cid:63)

2 , 10σ(cid:63)
(cid:32)

O

r

(cid:19)4

(cid:18) Γ
σ(cid:63)
r

log4

(cid:19)

(cid:18) dΓ
δσ(cid:63)
r

+

log

σ(cid:63)
1
σ(cid:63)
r

(cid:33)

.

σ(cid:63)
r
(cid:15)

Part 1: We will now show part 1 of the theorem. Recall PGDli (Algorithm 3) runs PGD (Algorithm 2) ﬁrst, and then runs
gradient descent within 1
r )1/2 neighborhood of X (cid:63) is a subset
of {U|(cid:107)U(cid:107)2 ≤ Γ}. Therefore, we only need to show that ﬁrst phase PGD will not leave the region. Speciﬁcally, we now
use induction to prove the following for PGD:

r )1/2 neighborhood of X (cid:63). It is easy to verify that 1

3 (σ(cid:63)

3 (σ(cid:63)

How to Escape Saddle Points Efﬁciently

1. Suppose at iteration τ we add perturbation, and denote ˜Uτ to be the iterate before adding perturbation (i.e., Uτ =

˜Uτ + ξτ , and ˜Uτ = Uτ −1 − η∇f (Uτ −1)). Then, (cid:107) ˜Uτ (cid:107) ≤ 1

2 Γ, and

2. (cid:107)Ut(cid:107) ≤ Γ for all t ≥ 0.

By choice of parameters of Algorithm 2, we know η = c

8Γ . First, consider gradient descent step without perturbations:

(cid:107)Ut+1(cid:107) =(cid:107)Ut − η∇f (Ut)(cid:107) = (cid:107)Ut − η(UtU(cid:62)

t − M(cid:63))Ut(cid:107)

≤(cid:107)Ut − ηUtU(cid:62)
≤ max
i

[σi(Ut) − ησ3

t Ut(cid:107) + η(cid:107)M(cid:63)Ut(cid:107)

i (Ut)] + η(cid:107)M(cid:63)Ut(cid:107)

For the ﬁrst term, we know function f (t) = t − ηt3 is monotonically increasing in [0, 1/
induction assumption, we also know (cid:107)Ut(cid:107) ≤ Γ1/2 ≤ (cid:112)8Γ/(3c) = 1/

√

3η. Therefore, the max is taken when i = 1:

3η]. On the other hand, by

(cid:107)Ut+1(cid:107) ≤(cid:107)Ut(cid:107) − η(cid:107)Ut(cid:107)3 + η(cid:107)M(cid:63)Ut(cid:107)
≤(cid:107)Ut(cid:107) − η((cid:107)Ut(cid:107)2 − σ(cid:63)
1)(cid:107)Ut(cid:107).

(20)

We seperate our discussion into following cases.

Case (cid:107)Ut(cid:107) > 1
Γ ≥ 36σ(cid:63)

1, we know:

2 Γ1/2: In this case (cid:107)Ut(cid:107) ≥ max{(cid:107)U0(cid:107), 3(σ(cid:63)

1)1/2}. Recall Γ1/2 = 2 max{(cid:107)U0(cid:107), 3(σ(cid:63)

1)1/2}. Clearly,

(cid:107)Ut+1(cid:107) ≤(cid:107)Ut(cid:107) − η((cid:107)Ut(cid:107)2 − σ(cid:63)

1)(cid:107)Ut(cid:107) ≤ (cid:107)Ut(cid:107) −

Γ − σ(cid:63)
1)

Γ1/2

≤(cid:107)Ut(cid:107) −

c
8Γ

1
4

(

Γ −

1
36

1
2

Γ)

Γ1/2 = (cid:107)Ut(cid:107) −

1
4
Γ1/2.

(

c
8Γ
c
72

72 Γ1/2.

This means that in each iteration, the spectral norm would decrease by at least c

Case (cid:107)Ut(cid:107) ≤ 1
1
2 Γ1/2. For (cid:107)Ut(cid:107)2 ≤ (cid:107)M(cid:63)(cid:107), we have:

2 Γ1/2: From (20), we know that as long as (cid:107)Ut(cid:107)2 ≥ (cid:107)M(cid:63)(cid:107), we will always have (cid:107)Ut+1(cid:107) ≤ (cid:107)Ut(cid:107) ≤

√

1
2

(cid:107)Ut+1(cid:107) ≤(cid:107)Ut(cid:107) − η((cid:107)Ut(cid:107)2 − σ(cid:63)

≤(cid:107)Ut(cid:107) + ((σ(cid:63)

1)1/2 − (cid:107)Ut(cid:107)) ×

1)1/2 + (cid:107)Ut(cid:107))(cid:107)Ut(cid:107)

≤(cid:107)Ut(cid:107) + ((σ(cid:63)

1)1/2 − (cid:107)Ut(cid:107)) ×

1)1/2

1)(cid:107)Ut(cid:107) = (cid:107)Ut(cid:107) +
c
8Γ
cσ(cid:63)
1
4Γ

≤ (σ(cid:63)

((σ(cid:63)

c
8Γ

(σ(cid:63)

1 − (cid:107)Ut(cid:107)2)(cid:107)Ut(cid:107)

Thus, in this case, we always have (cid:107)Ut+1(cid:107) ≤ 1

2 Γ1/2.

In conclusion, if we don’t add perturbation in iteration t, we have:

• If (cid:107)Ut(cid:107) > 1

2 Γ1/2, then (cid:107)Ut+1(cid:107) ≤ (cid:107)Ut(cid:107) − c

72 Γ1/2.

• If (cid:107)Ut(cid:107) ≤ 1

2 Γ1/2, then (cid:107)Ut+1(cid:107) ≤ 1

2 Γ1/2.

Now consider the iterations where we add perturbation. By the choice of radius of perturbation in Algorithm 2 , we increase
spectral norm by at most :

(cid:107)ξt(cid:107) ≤ (cid:107)ξt(cid:107)F ≤

√

c
χ2

r )2
(σ(cid:63)
108Γ1/2 · 8Γ

≤

Γ1/2

1
2

The ﬁrst inequality is because χ ≥ 1 and c ≤ 1. That is, if before perturbation we have (cid:107) ˜Ut(cid:107) ≤ 1
(cid:107) ˜Ut + ξt(cid:107) ≤ Γ1/2.

2 Γ1/2, then (cid:107)Ut(cid:107) =

How to Escape Saddle Points Efﬁciently

On the other hand, according to Algorithm 2, once we add perturbation, we will not add perturbation for next tthres =
χ·24Γ
c2σ(cid:63)
r

c iterations. Let T = min{inf i{Ut+i|(cid:107)Ut+i(cid:107) ≤ 1

2 Γ1/2}, tthres}:

c2 ≥ 48

≥ 24

(cid:107)Ut+T −1(cid:107) ≤ (cid:107)Ut(cid:107) −

Γ1/2(T − 1) ≤ Γ1/2(1 −

c
72

c(T − 1)
72

)

This gives T ≤ 36
know (cid:107)UT +i(cid:107) ≤ 1

Finally, (cid:107)U0(cid:107) ≤ 1
theorem.

c ≤ tthres. Let τ > t be the next time when we add perturbation (τ ≥ t + tthres), we immediately

c < 48
2 Γ1/2 for 0 ≤ i < τ − T and (cid:107) ˜Uτ (cid:107) ≤ 1
2 Γ1/2 by deﬁnition of Γ, so the initial condition holds. This ﬁnishes induction and the proof of the

2 Γ1/2.

In the next subsections we prove the geometric structures.

C.1. Smoothness and Hessian Lipschitz

Before we start proofs of lemmas, we ﬁrst state some properties about gradient and Hessians. The gradient of the objective
function f (U) is

∇f (U) = 2(UU(cid:62) − M(cid:63))U.

Furthermore, we have the gradient and Hessian satisfy for any Z ∈ Rd×r:

(cid:104)∇f (U), Z(cid:105) = 2(cid:104)(UU(cid:62) − M(cid:63))U, Z(cid:105), and

∇2f (U)(Z, Z) = (cid:107)UZ(cid:62) + ZU(cid:62)(cid:107)2

F + 2(cid:104)UU(cid:62) − M(cid:63), ZZ(cid:62)(cid:105).

(21)

(22)

Lemma 6. For any Γ ≥ σ(cid:63)
Lipschitz.

1, inside the region {U|(cid:107)U(cid:107)2 < Γ}, f (U) deﬁned in Eq.(2) is 8Γ-smooth and 12Γ1/2-Hessian

Proof. Denote D = {U|(cid:107)U(cid:107)2 < Γ}, and recall Γ ≥ σ(cid:63)
1.

Part 1: For any U, V ∈ D, we have:

(cid:107)∇f (U) − ∇f (V)(cid:107)F =2(cid:107)(UU(cid:62) − M(cid:63))U − (VV(cid:62) − M(cid:63))V(cid:107)F

≤2 (cid:2)(cid:107)UU(cid:62)U − VV(cid:62)V(cid:107)F + (cid:107)M(cid:63)(U − V)(cid:107)F
≤2 [3 · Γ(cid:107)U − V(cid:107)F + σ(cid:63)

1(cid:107)U − V(cid:107)F] ≤ 8Γ · (cid:107)U − V(cid:107)F.

(cid:3)

The last line is due to following decomposition and triangle inequality:

UU(cid:62)U − VV(cid:62)V = UU(cid:62)(U − V) + U(U − V)(cid:62)V + (U − V)V(cid:62)V.

Part 2: For any U, V ∈ D, and any Z ∈ Rd×r, according to Eq.(22), we have:

|∇2f (U)(Z, Z) − ∇2f (V)(Z, Z)| = (cid:107)UZ(cid:62) + ZU(cid:62)(cid:107)2

(cid:124)

F − (cid:107)VZ(cid:62) + ZV(cid:62)(cid:107)2
F
(cid:123)(cid:122)
(cid:125)
A

+ 2(cid:104)UU(cid:62) − VV(cid:62), ZZ(cid:62)(cid:105)
(cid:123)(cid:122)
(cid:125)
B

(cid:124)

.

For term A, we have:

A =(cid:104)UZ(cid:62) + ZU(cid:62), (U − V)Z(cid:62) + Z(U − V)(cid:62)(cid:105) + (cid:104)(U − V)Z(cid:62) + Z(U − V)(cid:62), VZ(cid:62) + ZV(cid:62)(cid:105)

≤(cid:107)UZ(cid:62) + ZU(cid:62)(cid:107)F(cid:107)(U − V)Z(cid:62) + Z(U − V)(cid:62)(cid:107)F + (cid:107)(U − V)Z(cid:62) + Z(U − V)(cid:62)(cid:107)F(cid:107)VZ(cid:62) + ZV(cid:62)(cid:107)F
≤4(cid:107)U(cid:107)(cid:107)Z(cid:107)2

F(cid:107)U − V(cid:107)F ≤ 8Γ1/2(cid:107)Z(cid:107)2

F(cid:107)U − V(cid:107)F + 4(cid:107)V(cid:107)(cid:107)Z(cid:107)2

F(cid:107)U − V(cid:107)F.

How to Escape Saddle Points Efﬁciently

For term B, we have:

B ≤ 2(cid:107)UU(cid:62) − VV(cid:62)(cid:107)F(cid:107)ZZ(cid:62)(cid:107)F ≤ 4Γ1/2(cid:107)Z(cid:107)2

F(cid:107)U − V(cid:107)F.

The inequality is due to following decomposition and triangle inequality:

UU(cid:62) − VV(cid:62) = U(U − V)(cid:62) + (U − V)V(cid:62).

Therefore, in sum we have:

max
Z:(cid:107)Z(cid:107)F≤1

|∇2f (U)(Z, Z) − ∇2f (V)(Z, Z)| ≤ max

12Γ1/2(cid:107)Z(cid:107)2

F(cid:107)U − V(cid:107)F

Z:(cid:107)Z(cid:107)F≤1

≤12Γ1/2(cid:107)U − V(cid:107)F.

C.2. Strict-Saddle Property and Local Regularity

Recall the gradient and Hessian of objective function is calculated as in Eq.(21) and Eq.(22). We ﬁrst prove an elementary
inequality regarding to the trace of product of two symmetric PSD matrices. This lemma will be frequently used in the
proof.
Lemma 18. For A, B ∈ Rd×d both symmetric PSD matrices, we have:

Proof. Let A = VDV(cid:62) be the eigendecomposition of A, where D is diagonal matrix, and V is orthogonal matrix. Then
we have:

σmin(A)tr(B) ≤ tr(AB) ≤ (cid:107)A(cid:107)tr(B)

tr(AB) = tr(DV(cid:62)BV) =

Dii(V(cid:62)BV)ii.

d
(cid:88)

i=1

Since B is PSD, we know V(cid:62)BV is also PSD, thus the diagonal entries are non-negative. That is, (V(cid:62)BV)ii ≥ 0 for
all i = 1, . . . , d. Finally, the lemma follows from the fact that σmin(A) ≤ Dii ≤ (cid:107)A(cid:107) and tr(V(cid:62)BV) = tr(BVV(cid:62)) =
tr(B).

Now, we are ready to prove Lemma 7.
Lemma 7. For f (U) deﬁned in Eq.(2), all local minima are global minima. The set of global minima is X (cid:63) =
{V(cid:63)R|RR(cid:62) = R(cid:62)R = I}. Furthermore, f (U) satisﬁes:

1. ( 1

24 (σ(cid:63)

r )3/2, 1

3 σ(cid:63)

r , 1

3 (σ(cid:63)

r )1/2)-strict saddle property, and

2. ( 2

3 σ(cid:63)

r , 10σ(cid:63)

1)-regularity condition in 1

3 (σ(cid:63)

r )1/2 neighborhood of X (cid:63).

Proof. Let us denote the set X (cid:63) := {V(cid:63)R|RR(cid:62) = R(cid:62)R = I}, in the end of proof, we will show this set is the set of all
local minima (which is also global minima).

Throughout the proof of this lemma, we always focus on the ﬁrst-order and second-order property for one matrix U. For
simplicity of calculation, when it is clear from the context, we denote U(cid:63) = PX (cid:63) (U) and ∆ = U−PX (cid:63) (U). By deﬁnition
of X (cid:63), we know U(cid:63) = V(cid:63)RU and ∆ = U − V(cid:63)RU, where

We ﬁrst prove following claim, which will used in many places across this proof:

RU =

argmin
R:RR(cid:62)=R(cid:62)R=I

(cid:107)U − V(cid:63)R(cid:107)2
F

U(cid:62)U(cid:63) = U(cid:62)V(cid:63)RU is a symmetric PSD matrix.

(23)

This because by expanding the Frobenius norm, and letting the SVD of V(cid:63)(cid:62)U be ADB(cid:62), we have:

How to Escape Saddle Points Efﬁciently

argmin
R:RR(cid:62)=R(cid:62)R=I
argmin
R:RR(cid:62)=R(cid:62)R=I

=

(cid:107)U − V(cid:63)R(cid:107)2

F =

−tr(U(cid:62)V(cid:63)R) =

argmin
R:RR(cid:62)=R(cid:62)R=I
argmin
R:RR(cid:62)=R(cid:62)R=I

−(cid:104)U, V(cid:63)R(cid:105)

−tr(DA(cid:62)RB)

Since A, B, R are all orthonormal matrix, we know A(cid:62)RB is also orthonormal matrix. Moreover for any orthonormal
matrix T, we have:

tr(DT) =

DiiTii ≤

(cid:88)

i

(cid:88)

Dii

i

The last inequality is because Dii is singular value thus non-negative, and T is orthonormal, thus Tii ≤ 1. This means
the maximum of tr(DT) is achieved when T = I, i.e., the minimum of −tr(DA(cid:62)RB) is achieved when R = AB(cid:62).
Therefore, U(cid:62)V(cid:63)RU = BDA(cid:62)AB(cid:62) = BDB(cid:62) is symmetric PSD matrix.
Part 1: In order to show the strict saddle property, we only need to show that for any U satisfying (cid:107)∇f (U)(cid:107)F ≤ 1
r )1/2, we always have σmin(∇2f (U)) ≤ − 1
and (cid:107)∆(cid:107)F = (cid:107)U − U(cid:63)(cid:107)F ≥ 1
3 σ(cid:63)
r .
Let’s consider Hessian ∇2(U) in the direction of ∆ = U − U(cid:63). Clearly, we have:

24 (σ(cid:63)

r )3/2

3 (σ(cid:63)

UU(cid:62) − M(cid:63) = UU(cid:62) − (U − ∆)(U − ∆)(cid:62) = (U∆(cid:62) + ∆U(cid:62)) − ∆∆(cid:62)

and by (21):

(cid:104)∇f (U), ∆(cid:105) =2(cid:104)(UU(cid:62) − M(cid:63))U, ∆(cid:105) = (cid:104)UU(cid:62) − M(cid:63), ∆U(cid:62) + U∆(cid:62)(cid:105)

=(cid:104)UU(cid:62) − M(cid:63), UU(cid:62) − M(cid:63) + ∆∆(cid:62)(cid:105)

Therefore, by Eq.(22) and above two equalities, we have:

∇2f (U)(∆, ∆) =(cid:107)U∆(cid:62) + ∆U(cid:62)(cid:107)2

F + 2(cid:104)UU(cid:62) − M(cid:63), ∆∆(cid:62)(cid:105)

=(cid:107)UU(cid:62) − M(cid:63) + ∆∆(cid:62)(cid:107)2
=(cid:107)∆∆(cid:62)(cid:107)2
=(cid:107)∆∆(cid:62)(cid:107)2

F − 3(cid:107)UU(cid:62) − M(cid:63)(cid:107)2
F − 3(cid:107)UU(cid:62) − M(cid:63)(cid:107)2

F + 2(cid:104)UU(cid:62) − M(cid:63), ∆∆(cid:62)(cid:105)

F + 4(cid:104)UU(cid:62) − M(cid:63), UU(cid:62) − M(cid:63) + ∆∆(cid:62)(cid:105)
F + 4(cid:104)∇f (U), ∆(cid:105)

Consider the ﬁrst two terms, by expanding, we have:

3(cid:107)UU(cid:62) − M(cid:63)(cid:107)2

F − (cid:107)∆∆(cid:62)(cid:107)2

F = 3(cid:107)(U(cid:63)∆(cid:62) + ∆U(cid:63)(cid:62)) + ∆∆(cid:62)(cid:107)2

F − (cid:107)∆∆(cid:62)(cid:107)2
F

=3 · tr (cid:0)2U(cid:63)(cid:62)U(cid:63)∆(cid:62)∆ + 2(U(cid:63)(cid:62)∆)2 + 4U(cid:63)(cid:62)∆∆(cid:62)∆ + (∆(cid:62)∆)2(cid:1) − tr((∆(cid:62)∆)2)
=tr (cid:0)6U(cid:63)(cid:62)U(cid:63)∆(cid:62)∆ + 6(U(cid:63)(cid:62)∆)2 + 12U(cid:63)(cid:62)∆∆(cid:62)∆ + 2(∆(cid:62)∆)2(cid:1)
=tr((4
√

3 − 6)U(cid:63)(cid:62)U(cid:63)∆(cid:62)∆ + (12 − 4

3)U(cid:63)(cid:62)(U(cid:63) + ∆)∆(cid:62)∆ + 2(

√

√

√

√

≥(4

3 − 6)tr(U(cid:63)(cid:62)U(cid:63)∆(cid:62)∆) ≥ (4

3 − 6)σ(cid:63)

r (cid:107)∆(cid:107)2
F

3U(cid:63)(cid:62)∆ + ∆(cid:62)∆)2)

where the second last inequality is because U(cid:63)(cid:62)(U(cid:63) + ∆)∆(cid:62)∆ = U(cid:63)(cid:62)U∆(cid:62)∆ is the product of two symmetric PSD
matrices (thus its trace is non-negative); the last inequality is by Lemma 18.
Finally, in case we have (cid:107)∇f (U)(cid:107)F ≤ 1

r )3/2 and (cid:107)∆(cid:107)F = (cid:107)U − U(cid:63)(cid:107)F ≥ 1

r )1/2

24 (σ(cid:63)

3 (σ(cid:63)

σmin(∇2f (U)) ≤

∇2f (U)(∆, ∆) ≤ −(4

3 − 6)σ(cid:63)

r + 4

√

(cid:104)∇f (U), ∆(cid:105)
(cid:107)∆(cid:107)2
F

1
(cid:107)∆(cid:107)2
F
√

≤ − (4

3 − 6)σ(cid:63)

r + 4

≤ −(4

3 − 6.5)σ(cid:63)

r ≤ −

σ(cid:63)
r

(cid:107)∇f (U)(cid:107)F
(cid:107)∆(cid:107)F

√

1
3

Part 2: In 1

3 (σ(cid:63)

r )1/2 neigborhood of X (cid:63), by deﬁnition, we know,

How to Escape Saddle Points Efﬁciently

(cid:107)∆(cid:107)2

F = (cid:107)U − U(cid:63)(cid:107)2

F ≤

σ(cid:63)
r .

1
9

Clearly, by Weyl’s inequality, we have (cid:107)U(cid:107) ≤ (cid:107)U(cid:63)(cid:107) + (cid:107)∆(cid:107) ≤ 4
Moreover, since U(cid:63)(cid:62)U is symmetric matrix, we have:

3 (σ(cid:63)

1)1/2, and σr(U) ≥ σr(U(cid:63)) − (cid:107)∆(cid:107) ≥ 2

3 (σ(cid:63)

r )1/2.

σr(U(cid:63)(cid:62)U) =

(cid:0)σr(U(cid:62)U(cid:63) + U(cid:63)(cid:62)U)(cid:1)

1
2
1
2
1
2
1
2

≥

≥

≥

(cid:0)σr(U(cid:62)U + U(cid:63)(cid:62)U(cid:63)) − (cid:107)(U − U(cid:63))(cid:62)(U − U(cid:63))(cid:107)(cid:1)

(cid:0)σr(U(cid:62)U) + σr(U(cid:63)(cid:62)U(cid:63)) − (cid:107)∆(cid:107)2
2
3

r =

σ(cid:63)
r .

)σ(cid:63)

4
9

1
9

(1 +

−

F

(cid:1)

At a highlevel, we will prove (α, β)-regularity property (1) by proving that:

1. (cid:104)∇f (x), x − PX (cid:63) (x)(cid:105) ≥ α(cid:107)x − PX (cid:63) (x)(cid:107)2, and

2. (cid:104)∇f (x), x − PX (cid:63) (x)(cid:105) ≥ 1

β (cid:107)∇f (x)(cid:107)2.

According to (21), we know:

(cid:104)∇f (U), U − PX (cid:63) (U)(cid:105) =2(cid:104)(UU(cid:62) − M(cid:63))U, ∆(cid:105) = 2(cid:104)U∆(cid:62) + ∆U(cid:63)(cid:62), ∆U(cid:62)(cid:105)

=2(tr(U∆(cid:62)U∆(cid:62)) + tr(∆U(cid:63)(cid:62)U∆(cid:62)))
=2((cid:107)∆(cid:62)U(cid:107)2

F + tr(U(cid:63)(cid:62)U∆(cid:62)∆)).

The last equality is because ∆(cid:62)U is symmetric matrix. Since U(cid:63)(cid:62)U is symmetric PSD matrix, and recall σr(U(cid:63)(cid:62)U) ≥
2
3 σ(cid:63)

r , by Lemma 18 we have:

(cid:104)∇f (U), U − PX (cid:63) (U)(cid:105) ≥ σr(U(cid:63)(cid:62)U)tr(∆(cid:62)∆) ≥

r (cid:107)∆(cid:107)2
σ(cid:63)
F.

2
3

(24)

(25)

On the other hand, we also have:

(cid:107)∇f (U)(cid:107)2

F =4(cid:104)(UU(cid:62) − M(cid:63))U, (UU(cid:62) − M(cid:63))U(cid:105)

=4(cid:104)(U∆(cid:62) + ∆U(cid:63)(cid:62))U, (U∆(cid:62) + ∆U(cid:63)(cid:62))U(cid:105)

=4


tr[(∆(cid:62)UU(cid:62)∆)U(cid:62)U] + 2tr[∆(cid:62)UU(cid:62)U(cid:63)∆(cid:62)U]
(cid:123)(cid:122)
(cid:125)
A

(cid:124)

+ tr(U(cid:63)(cid:62)UU(cid:62)U(cid:63)∆(cid:62)∆)
(cid:123)(cid:122)
(cid:125)
B

(cid:124)



 .

For term A, by Lemma 18, and ∆(cid:62)U being a symmetric matrix, we have:

A ≤ (cid:107)U(cid:62)U(cid:107)(cid:107)∆(cid:62)U(cid:107)2

F + 2(cid:107)U(cid:62)U(cid:63)(cid:107)(cid:107)∆(cid:62)U(cid:107)2

F ≤ (

+

)σ(cid:63)

1(cid:107)∆(cid:62)U(cid:107)2

F ≤ 5σ(cid:63)

1(cid:107)∆(cid:62)U(cid:107)2
F

For term B, by Eq.(23) we can denote C = U(cid:63)(cid:62)U = U(cid:62)U(cid:63) which is symmetric PSD matrix, by Lemma 18, we have:

B =tr(C2∆(cid:62)∆) = tr(C(C1/2∆(cid:62)∆C1/2))

≤(cid:107)C(cid:107)tr(C1/2∆(cid:62)∆C1/2) = (cid:107)C(cid:107)tr(C∆(cid:62)∆) ≤

1tr(U(cid:63)(cid:62)U∆(cid:62)∆).
σ(cid:63)

16
9

8
3

4
3

Combining with (24) we have:

(cid:107)∇f (U)(cid:107)2

F ≤ σ(cid:63)

1(20(cid:107)∆(cid:62)U(cid:107)2

F +

tr(U(cid:63)(cid:62)U∆(cid:62)∆)) ≤ 10σ(cid:63)

1(cid:104)∇f (U), U − PX (cid:63) (U)(cid:105).

(26)

16
3

Combining (25) and (26), we have:

How to Escape Saddle Points Efﬁciently

(cid:104)∇f (U), U − PX (cid:63) (U)(cid:105) ≥

r (cid:107)U − PX (cid:63) (U)(cid:107)2
σ(cid:63)

F +

(cid:107)∇f (U)(cid:107)2
F.

1
3

1
20σ(cid:63)
1

