Appendix for Learning Important Features Through
Propagating Activation Differences

1 A: Proof of summation-to-delta with the multiplier chain rule

The chain rule for multipliers is:

Where multipliers are deﬁned as:

m∆xi∆t =

m∆xi∆yj m∆yj ∆t

(cid:88)

j

m∆x∆t =

C∆x∆t
∆x

(1)

(2)

Given C∆xi∆yj and C∆yj ∆t both satisfy summation-to-delta, we show that deﬁning m∆xi∆t according to the chain
rule also satisﬁes summation-to-delta (that is, (cid:80)

i C∆xi∆t = ∆t). We have:

(cid:88)

i

C∆xi∆t =

∆xim∆xi∆t (By deﬁnition of m∆xi∆t)

(cid:88)

∆xi

m∆xi∆yj m∆yj ∆t (By the chain rule)

(cid:88)

i
(cid:88)

i

(cid:88)

i
(cid:88)

j
(cid:88)

j

(cid:88)

j
(cid:88)

j

=

=

=

=

=

=

=

(cid:88)

i
(cid:88)

j
(cid:88)

i

(cid:88)

∆xi

C∆xi∆yj
∆xi

j

j

C∆xi∆yj m∆yj ∆t

m∆yj ∆t (By deﬁnition of m∆xi∆yj )

C∆xi∆yj m∆yj ∆t (Flipping the order of summation)

∆yjm∆yj ∆t (By summation-to-delta of C∆xi∆yj )

∆yj

C∆yj ∆t
∆yj

(By deﬁnition of m∆yj ∆t)

C∆yj ∆t = ∆t (By summation-to-delta of C∆yj ∆t)

2 B: Propagation of Linear rule multipliers using standard neural network operations

We can frame the propagation of the multipliers for the Linear rule in terms of standard operations provided by GPU
backends such as tensorﬂow and theano.

2.1 Dense layers

Let W represent the tensor of weights, and let ∆X and ∆Y represent a 2d matrix with dimensions sample × features
such that ∆Y = matrix_mul(W, ∆X). Here, matrix_mul is matrix multiplication. Let M∆X∆t and M∆Y ∆t represent
tensors of multipliers (again with dimensions sample × features). Let · represent an elementwise product, and let
1{condition} represent a binary matrix that is 1 where "condition" is true and 0 otherwise. We have:

M∆X∆t =(matrix_mul(W T (cid:12) 1{W T > 0}, M∆Y +∆t) (cid:12) 1{∆X > 0}

+ matrix_mul(W T (cid:12) 1{W T < 0}, M∆Y +∆t) (cid:12) 1{∆X < 0})

+(matrix_mul(W T (cid:12) 1{W T > 0}, M∆Y −∆t) (cid:12) 1{∆X < 0}

+ matrix_mul(W T (cid:12) 1{W T < 0}, M∆Y −∆t) (cid:12) 1{∆X > 0})
+ matrix_mul(W T , 0.5(M∆Y +∆t + M∆Y −∆t)) (cid:12) 1{∆X = 0})

2.2 Convolutional layers

Let W represent a tensor of convolutional weights such that ∆Y = conv(W, ∆X), where conv represents the convolu-
tion operation. Let transposed_conv represent a transposed convolution (comparable to the gradient operation for a
convolution) such that d
dt Y ). We have:

dt X = transposed_conv(W, d

M∆X∆t =(transposed_conv(W (cid:12) 1{W > 0}, M∆Y +∆t) (cid:12) 1{∆X > 0}

+ transposed_conv(W (cid:12) 1{W < 0}, M∆Y +∆t) (cid:12) 1{∆X < 0})

+(transposed_conv(W (cid:12) 1{W > 0}, M∆Y −∆t) (cid:12) 1{∆X < 0}

+ transposed_conv(W (cid:12) 1{W < 0}, M∆Y −∆t) (cid:12) 1{∆X > 0})
+ transposed_conv(W, 0.5(M∆Y +∆t + M∆Y −∆t)) (cid:12) 1{∆X = 0})

2

3 C: Detailed calculation of contributions in Fig. 3 using various backpropagation-based

approaches

We reproduce Figure 3 from the main text below:

Figure 1: Network computing o = min(i1, i2).

We walk through the calculation of importance using various backpropagation methods. All methods except RevealCan-
cel put importance either exclusively on i1 or on i2.

3.1 Gradients, Gradient × input and integrated gradients

Because o = min(i1, i2), we have do
= 0 when i2 > i1. Thus, importance would be
di1
assigned either exclusively to i1 or exclusively to i2. Because the integrated gradients methods computes gradients at
points interpolated linearly between 0 and the actual input values, it would also give importance either exclusively to i1
or i2.

= 0 when i1 > i2 and do
di2

3.2 Guided Backprop

We note that do
dh2
placed on i1.

3.3 DeepLIFT with the Rescale rule

< 0. Thus, Guided Backprop would propagate zero importance to h2, and all importance would be

We compute the contributions and multipliers at every step of the calculation, assuming i0
2 = 0. First, we compute
the reference activations for all the neurons by forward propagating the reference input, which gives: h0
2 = o0 = 0.
1 = h0
Then, we ﬁnd the differences-from-reference, which are: ∆i1 = i1, ∆i2 = i2, ∆h1 = h1 = i1 − i2, ∆h2 = h2 =
max(0, h1) and ∆o = o = i1 − h2.

1 = i0

C∆h2∆o = − max(0, i1 − i2)
m∆h2∆o = −1
C∆h1∆h2 = max(0, i1 − i2)

m∆h1∆h2 =

C∆h1∆h2
∆h1

=

max(0, i1 − i2)
i1 − i2

m∆h1∆o = m∆h1∆h2m∆h2∆o = −

max(0, i1 − i2)
i1 − i2

C∆i1∆h1 = i1

m∆i1∆h1 =

C∆i1∆h1
∆i1

= 1

C∆i2∆h1 = −i2

m∆i2∆h1 =

C∆i2∆h1
∆i2

= −1

m∆i1∆o = 1 + m∆i1∆h1m∆h1∆o = 1 −

max(0, i1 − i2)
i1 − i2

m∆i2∆o = m∆i2∆h1 m∆h1∆o =
(cid:18)

C∆i1∆o = ∆i1m∆i1∆o = i1

1 −

max(0, i1 − i2)
i1 − i2

max(0, i1 − i2)
i1 − i2

(cid:19)

3

C∆i2∆o = ∆i2m∆i2∆o = i2

max(0, i1 − i2)
i1 − i2

Based on the formula above, we can see that when i1 − i2 > 0, we get max(0,i1−i2)
C∆i2∆o = i2. When i1 − i2 < 0, we have C∆i1∆o = i1 and C∆i2∆o = 0.

i1−i2

= 1, giving C∆i1∆o = 0 and

3.4 DeepLIFT with the RevealCancel rule

We again start with i0
∆o− = −h2. We compute ∆h+

1 = i0

2 = 0. This gives ∆h+
2 as follows:

2 and ∆h−

1 = i1, ∆h−

1 = −i2, ∆h2 = max(0, i1 − i2), ∆o+ = i1 and

∆h+

2 =

(max(0, ∆h+

1 ) − max(0, 0)) +

(max(0, ∆h−

1 + ∆h+

1 ) − max(0, ∆h−

1 ))

∆h−

2 =

(max(0, ∆h−

1 ) − max(0, 0)) +

(max(0, ∆h+

1 + ∆h−

1 ) − max(0, ∆h+

1 ))

1
2
1
2
1
2
1
2

=

(i1 + max(0, i1 − i2))

=

(max(0, i1 − i2) − i1)

We then compute contributions as follows:

C∆h2∆o = C∆h2∆o+ + C∆h2∆o− = 0 + −h2 = − max(0, i1 − i2)

m∆h2∆o = m∆h+

C∆h2∆o
∆h2

= −1

2 ∆o =

2 ∆o = m∆h−
C∆h+

1 ∆h+

2

m∆h+

1 ∆h+

2

= ∆h+
2
∆h+
2
∆h+
1

=

m∆h+

1 ∆o = m∆h+
= ∆h−
2
∆h−
2
∆h−
1

=

2

2

C∆h−

1 ∆h−

m∆h−

1 ∆h−

=

0.5(i1 + max(0, i1 − i2))
i1

1 ∆h+

2

m∆h+

2 ∆o =

−0.5(i1 + max(0, i1 − i2))
i1

=

0.5(i1 − max(0, i1 − i2))
i2

−0.5(i1 − max(0, i1 − i2))
i2

2 ∆o =
= 1

m∆h−

1 ∆o = m∆h−

1 ∆h−

2

m∆h−

1

C∆i1∆h+
C∆i1∆h−
C∆i2∆h+
C∆i2∆h−

1

1

1

1

= i1 ; m∆i1∆h+
= 0 ; m∆i1∆h−
= 0 ; m∆i2∆h+
= −i2 ; m∆i2∆h−

1

1

1

= 0

= 0

= −1

mi1o = 1 + m∆i1∆h+

m∆h+

1 ∆o + m∆i1∆h−

1

m∆h−

1 ∆o = 1 −

1

mi2o = m∆i2∆h+

1

m∆h+

1 ∆o + m∆i2∆h−

1

m∆h−

1 ∆o = 0 +

0.5(i1 + max(0, i1 − i2))
i1
0.5(i1 − max(0, i1 − i2))
i2

+ 0

Ci1o = ∆i1mi1o = i1 − 0.5(i1 − max(0, i1 − i2)) = 0.5(i1 + max(0, i1 − i2))
Ci2o = ∆i2mi2o = 0.5(i1 − max(0, i1 − i2))

In short, using the RevealCancel rule, we get that the contribution scores on both i1 and i2 are 0.5(i1 − max(0, i1 − i2)).
When i1 > i2, we have a contribution of 0.5i2 on both inputs, and when i1 < i2, we get a contribution of 0.5i1 on both
inputs.

1
2

1
2

4

4 D: Architecture and training details for MNIST

We describe the architecture and training procedure for the MNIST model. This architecture was based on the MNIST
architecture in the Keras examples, with strided convolutions replacing pooling layers and adjustments to the kernel
size to cover all inputs due to the valid convolutions. The layers are as follows:

• Convolutional layer with 32 ﬁlters of kernel size (4,4) and stride (2,2)
• ReLU activation
• Convolutional layer with 64 ﬁlters of kernel size (4,4) and stride (2,2)
• ReLU activation
• Dropout with probability 0.25 of leaving out units
• Fully connected layer with 128 neurons
• ReLU activation
• Dropout with probability 0.5 of leaving out units
• Fully connected layer with 10 units
• Softmax output

The model was trained with a loss of categorical cross-entropy and the Adam optimizer with learning rate 0.001. We
used a batch size of 128 and a training set consisting of 60K images. The testing set had 10K images. The model was
trained for 19 epochs; training was terminated using early stopping. The ﬁnal testing accuracy was 99.24%

5

5 E: Performance of MNIST without RevealCancel on all layers

Figure 2: RevealCancel applied to all layers outperforms RevealCancel applied only to some layers on MNIST.
The Y-axis represents the change in the log-odds score between the original class and the target class after pixel erasure
(see Fig. 4 in the main text for more details). We compared the performance of DeepLIFT with the RevealCancel
used on all layers (RC-only), to the performance when the Rescale rule was used on the ﬁrst convolutional layer
instead of RevealCancel (RC-fc&conv2, RS-conv1), to the performance when the Rescale rule was used on the ﬁrst two
convolutional layers (RC-fc, RS-conv1&2), to the performance when only the Rescale rule was used (RS-only). Using
the Rescale rule appeared to degrade performance for MNIST.

6

6 F: Details of genomics simulation and model architecture

6.1 Simulation details

The human genome has millions of DNA sequence elements (roughly 1000 letters long) containing speciﬁc combinations
of short functional words to which regulatory proteins (RPs) bind to regulate gene activity. Each RP has binding
afﬁnity to speciﬁc collections of short DNA words (motifs). E.g.
the GATA1 RP has high afﬁnity to GATAA
and moderate afﬁnity to GATTA. Motifs of RPs are commonly represented as position weight matrix (PWMs)
which are probability distributions over {A,C,G,T} at each position in the motif i.e. PWMs for a motif of length
L is a matrix of probabilities of size 4 x L where the rows represent the 4 letters and the columns represent each
position in the motif. The probabilities in each column sum to 1. PWMs are visualized as sequence logos (See
https://en.wikipedia.org/wiki/Sequence_logo for additional details).

A key problem in computational genomics is the discovery of functional motifs in regulatory DNA sequence elements
that give rise to distinct molecular signatures which can be measured experimentally (thus providing labels). Here,
in order to benchmark DeepLIFT and other methods, we design a simulation that captures the essence of the motif
discovery problem described above.

8K sequences of length 200 were simulated with a train-valid-test split of 0.8-0.1-0.1. First, a background sequence
was generated by sampling the letters ACGT at each position with probabilities 0.3, 0.2, 0.2 and 0.3 respectively. Then,
motifs were sampled from PWMs [3] for the GATA1 and TAL1 regulatory proteins and inserted into the background
sequence at random non-overlapping positions. For the GATA1 motif, we used the GATA_disc1 PWM and for the
TAL1 motif, we used the TAL1_known1 PWM. 1/4 of the sequences had both TAL1 and GATA1 motifs, 1/4 had only
TAL1 motifs, 1/4 had only GATA1 motifs, and 1/4 had no motifs embedded.

The number of each kind of motif to be inserted into a given sequence was determined by sampling from a Poisson
distribution that was truncated to have a minimum of 1 and a maximum of 3 (when a number was sampled outside the
range, it was resampled until a number within the range was achieved). For sequences containing either only GATA1 or
only TAL1, the mean of the Poisson distribution was 2, and for sequences containing both GATA1 and TAL1, the mean
of the Poisson for each motif was 1.

6.2 Architecture details

The input sequences were represented using one-hot encoding. We trained a model with the the following layers:

• Convolution with 50 ﬁlters of width 11
• ReLU
• Convolution with 50 ﬁlters of width 11
• ReLU
• Global Average Pooling
• Fully connected layer with 50 neurons
• ReLU
• Dropout layer with probability 0.5 of leaving out units
• Fully connected layer with 3 neurons
• Sigmoid output

7

The model was trained for 18 epochs with a binary crossentropy loss and the Adam optimizer and a learning rate of
0.001 (training was terminated with early stopping). The ﬁnal auROC was 0.985 on Task 0, 0.983 on Task 1 and 0.997
on Task 2.

7 G: Equivalent of Fig. 5 in the main text for the GATA1 motif

Figure 3: DeepLIFT with RevealCancel gives qualitatively desirable behavior on TAL-GATA simulation. (a)
Scatter plots showing importance score vs. strength of GATA1 motif match for different tasks and methods. For each
region, the top 5 motif matches are plotted. X-axes: log-odds score of GATA1 motif vs. background. Y-axes: total
importance assigned to the match for the speciﬁed task. Red dots are from regions where both TAL1 and GATA1
motifs were inserted during simulation; blue have GATA1 only, green have TAL1 only, black have no motifs inserted.
“DeepLIFT-fc-RC-conv-RS" refers to using the RevealCancel rule on the fully-connected layer and the Rescale rule on
the convolutional layers, which appears to reduce noise relative to using RevealCancel on all the layers. (b) proportion
of strong matches (log-odds > 7) to GATA1 motif in regions containing both TAL1 and GATA1 that had total score ≤ 0
for task 0; Guided Backprop×inp and DeepLIFT with RevealCancel have no false negatives, but Guided Backprop has
false positives for Task 2 (see Panel (a)).

8

8 H: Non-speciﬁc ﬁring of Guided Backprop example sequence

Figure 4: Guided Backprop×input can lead to non-speciﬁc highlighting of sequence features. Scores for Task 1
("contains GATA1") computed using various methods. Heights of letters indicate sizes of importance scores. Blue
boxes indicate locations of embedded GATA1 motifs and green boxes indicate locations of embedded TAL1 motifs.
Guided Backprop×input (simply annotated as "Guided Backprop" in the ﬁgure) highlights the TAL1 motif as being
relevant for the "contains GATA1" task.

9

9

I: Tiers in importance score are caused by having multiple instances of motifs

Figure 5: Magnitude of scores on individual motif instances for Task 0 ("Both TAL1 and GATA1") depends on
total number of motif instances in the sequence. Top 5 matches (as ranked by the log-odds score) to the TAL1 motif
for each sequence in the test set are plotted. X-axis indicates strength of the log-odds score of the match, and y-axis
indicates total importance score for Task 0 assigned to match. Plots are separated by whether the entire sequence
contains 1, 2 or 3 instances of the TAL1 motif. As illustrated, for sequences that contain multiple instances of the TAL1
motif, integrated gradients and DeepLIFT assign less importance to each individual instance of the motif. In each case,
TAL1 motifs from sequences that contain a GATA1 motif (red dots) tend to receive more importance that TAL1 motifs
from sequences that do not contain a GATA1 motif (green dots), consistent with property (5) discussed in Section 4.2.

10

10

J: Results with shufﬂed sequences as a reference

One undesirable aspect of using the frequencies of ACGT as the reference is that such a representation is not a one-hot
encoding and thus never occurs in the training data. Thus, the network may behave unexpectedly on the reference
sequence. As an alternative, we can average results over several one-hot encoded sequences generated by shufﬂing the
original sequence. The results are shown below. On real genomic data, a dinucleotide-preserving shufﬂe would be more
appropriate than a random shufﬂe as CG dinucleotides are underrepresented in genomic sequence due to spontaneous
deamination from C to T.

Figure 6: Shufﬂed sequences are a viable choice of reference for genomic data. For each sequence, scores were
computed by averaging results over multiple references generated by randomly shufﬂing the original sequence. First
column has results from using ACGT frequency as a reference; remaining columns use shufﬂed one-hot encoded
sequences as reference. Multiref-n means n references were generated per sequence. Scores were computed using the
RevealCancel rule on the fully-connected layer and the rescale rule on the convolutional layers. Shufﬂed sequences
seem to result in higher DeepLIFT scores relative to ACGT frequency as the reference, and tiers caused by multiple
motif instances are more noticeable (see Appendix J). Other qualitative aspects appear to be similar.

11

11 K: Weight normalization for one-hot encoded inputs

While it may seem natural to use a reference input of all zeros for one-hot encoded inputs, this neglects the fact that
one-hot encoded inputs have the constraint that one of the inputs will always be a 1. For genomics, we suggest using
a reference that is the average of one-hot encoded sequences from the negative set. However, if one would prefer to
use a reference of all zeros, we propose applying the following output-preserving transformation that guarantees the
activation of a convolutional ﬁlter given an input of all-zeros is equal to the average activation over all possible one-hot
encoded inputs:

Let W and b represent the weights and bias of a 1d convolutional ﬁlter such that for a given input patch X, the output y
of the convolutional neuron is:

Here, i iterates over the length dimension and j iterates over the channel dimension. We also assume that the inputs are
one-hot encoded, that is:

Let n be the number of channels. We propose the following transformation:

(cid:88)

(cid:88)

y =

WijXij + b

i

j

(cid:88)

j

Xij = 1

W (cid:48)

ij = Wij −

(cid:88)

Wij

1
n

j

b(cid:48) = b +

1
n

(cid:88)

(cid:88)

Wij

i

j

(3)

(4)

(5)

(6)

In other words, we normalize the weights at each position by subtracting the mean weight across all channels, and add
the mean to the bias to preserve the output. Because the weights are now mean-normalized, the average output over all
possible one-hot encoded values for X is equal to the output when X is zero everywhere (which in turn is equal to the
new bias b(cid:48)). Doing this normalization can improve results when using a reference of all-zeros.
We can prove that the output is preserved on one-hot encoded inputs as follows. Let 1
n

j Wij = ci. We have:

(cid:80)

(cid:88)

=

b +

ci

+

(cid:88)

(cid:88)

(Wij − ci) Xij

(cid:88)

=

b +

ci

+

(cid:88)

(cid:88)

WijXij − ci

(cid:88)

Xij





j


(cid:88)

(cid:88)

b(cid:48) +

W (cid:48)

ijXij

i

j

(cid:32)

(cid:32)

(cid:32)

(cid:33)

(cid:33)

(cid:33)

i

i

= b +

WijXij

i
(cid:88)
(cid:88)

i

j

= y

i

i

i

j








j

j

(cid:88)

=

b +

ci

+

(cid:88)

(cid:88)

WijXij − ci

 (Because

Xij = 1)

(cid:88)

j

The proof above is for the one-hot encoded case where the sum over all input channels at each position is 1 - however,
note that a similar analysis could apply to any input constraint such that the sum over all channels at each position is a
nonzero value.

12

12 L: Choice of reference for CIFAR10 model

We performed preliminary analysis of different choices of reference for CIFAR10 [4] data. We trained a model based on
the architecture of the larger network in [2] for 50 epochs and attained an accuracy of 80.67%. We explored two different
choices of reference: the all-zeros reference and a blurred version of the original image. Blurring was performed using
the cv2.GaussianBlur function in the python OpenCV [1] package. The results are illustrated in Figure 7.

Figure 7: Choice of reference has a large impact on importance scores. A CNN was trained on CIFAR10 data and
importance scores with two difference choices of reference were visualized. Pictured above is a correctly-classiﬁed
example from the ‘ship’ class. The left column shows the original image, the middle column shows two different
choices of reference (a blurred version of the original image and an all-zeros image), and the right column shows
DeepLIFT-RevealCancel scores obtained with each choice of reference. A blurred reference highlights the outline of
the ship and the horizon. An all-zero reference highlights several pixels in the background. One possible explanation
for this result is that features such as the colour of the ocean or colour of the sky are relevant for classifying an object in
the ‘ship’ class and are therefore highlighted when using an all-black reference.

13

References

[1] G. Bradski. Dr. Dobb’s Journal of Software Tools.

Brownlee.
keras

[2] Jason
the
object-recognition-convolutional-neural-networks-keras-deep-learning-library/.
cessed: 2017-04-20.

convolutional
in
neural
http://machinelearningmastery.com/
Ac-

recognition
library.

Object
learning

networks

deep

with

[3] Pouya Kheradpour and Manolis Kellis. Systematic discovery and characterization of regulatory motifs in encode tf

binding experiments. Nucleic acids research, 42(5):2976–2987, 2014.

[4] Alex Krizhevsky and Geoff Hinton. Learning multiple layers of features from tiny images. 2009.

14

