Learning Hawkes Processes from Short Doubly-Censored Event Sequences

Hongteng Xu 1 Dixin Luo 2 Hongyuan Zha 1

Abstract
Many real-world applications require robust al-
gorithms to learn point processes based on a type
of incomplete data — the so-called short doubly-
censored (SDC) event sequences. We study this
critical problem of quantitative asynchronous
event sequence analysis under the framework
of Hawkes processes by leveraging the idea of
data synthesis. Given SDC event sequences ob-
served in a variety of time intervals, we pro-
pose a sampling-stitching data synthesis method,
sampling predecessors and successors for each
SDC event sequence from potential candidates
and stitching them together to synthesize long
training sequences. The rationality and the fea-
sibility of our method are discussed in terms
of arguments based on likelihood. Experiments
on both synthetic and real-world data demon-
strate that the proposed data synthesis method
improves learning results indeed for both time-
invariant and time-varying Hawkes processes.

1. Introduction

Real-world interactions among multiple entities are often
recorded as asynchronous event sequences, such as user be-
haviors in social networks, job hunting and hopping among
companies, and diseases and their complications. The en-
tities or event types in the sequences often exhibit self-
triggering and mutually-triggering patterns. For example,
a tweet of a twitter user may trigger further responses from
her friends (Zhao et al., 2015). A disease of a patient may
trigger other complications (Choi et al., 2015). Hawkes
processes, an important kind of temporal point process
model (Hawkes & Oakes, 1974), have capability to de-
scribe the triggering patterns quantitatively and capture the
infectivity network of the entities.

1Georgia Institute of Technology, Atlanta, Georgia, USA
2University of Toronto, Toronto, Ontario, Canada. Correspon-
dence to: Hongteng Xu <hxu42@gatech.edu>, Dixin Luo
<dixin.luo@utoronto.ca>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure 1. For each SDC sequence, i.e., incomplete disease history
of a person in his lifetime, we design a mechanism to select other
SDC sequences as predecessors/successors and synthesize a long
sequence. Then, we can estimate the unobserved triggering pat-
terns among diseases, i.e., the red dashed arrows, and construct a
disease network.

Despite the usefulness of Hawkes processes, robust learn-
ing of Hawkes processes often needs many event sequences
with events occurring over a long observation window. Un-
fortunately, the observation window is likely to be very
short and sequence-speciﬁc in many important practical ap-
plications, i.e., within an imagined universal window, each
sequence is only observed with a corresponding short sub-
interval of it, and the events outside this sub-interval are
not observed — we call them short doubly-censored (SDC)
event sequences. Existing learning algorithms of Hawkes
processes directly applied to SDCs may suffer from over-
ﬁtting, and what is worse, the triggering patterns between
historical events and current ones are lost, so that the trig-
gering patterns learned from SDC event sequences are of-
ten unreliable. This problem is a thorny issue in sev-
eral practical applications, especially in those having time-
varying triggering patterns. For example, the disease net-
works of patients should evolve with the increase of age.
However, it is very hard to track and record people’s dis-
eases on a life-time scale. Instead, we can only obtain their
several admissions (even only one admission) in a hos-
pital during one or two years, which are just SDC event
sequences. Therefore, it is highly desirable to propose a
method to learn Hawkes processes having a longtime sup-
port from a collection of SDC event sequences

In this paper, we propose a novel and simple data synthesis
method to enhance the robustness of learning algorithms
for Hawkes processes. Fig. 1 illustrates the principle of our
method. Given a set of SDC event sequences, we sample
predecessor for each event sequence from potential can-
didates and stitch them together as new training data. In
the sampling step, the distribution of predecessor (and suc-
cessor) is estimated according to the similarities between

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

current sequence and its candidates, and the similarity is
deﬁned based on the information of time stamps and (op-
tional) features of event sequences. We analyze the ratio-
nality and the feasibility of our data synthesis method and
discuss the necessary condition for using the method. Ex-
perimental results show that our data synthesis method in-
deed helps to improve the robustness of various learning
algorithms for Hawkes processes. Especially in the case
of time-varying Hawkes processes, applying our method in
the learning phase achieves much better results than learn-
ing directly from SDC event sequences, which is meaning-
ful for many practical applications, e.g., constructing dy-
namical disease network, and learning long-term infectiv-
ity among different IT companies.

2. Related Work

An event sequence can be represented as s = {(ti, ci)}M
i=1,
where time stamps ti’s are in an observation window
[Tb, Te] and events ci’s are in a set of event types C =
{1, ..., C}. A point process {Nc}c∈C is a random pro-
cess model taking event sequences as instances, where
Nc = {Nc(t)|t ∈ [Tb, Te]} and Nc(t) is the number of
type-c events occurring at or before time t. A point pro-
cess can be characterized via its conditional intensity func-
tion λc(t) = E[dNc(t)|HC
t =
{(ti, ci)|ti < t, ci ∈ C} is the set of history. It represents
the expected instantaneous happening rate of events given
historical record (Daley & Vere-Jones, 2007). The inten-
sity is often modeled with certain parameters Θ to capture
the phenomena of interests, i.e., self-triggering (Hawkes &
Oakes, 1974) or self-correcting (Xu et al., 2015). Based on
{λc(t)}c∈C, the likelihood of an event sequence s is

t ]/dt, where c ∈ C and HC

L(s; Θ) =

λci(ti) exp

−

(cid:89)
i

(cid:16)

(cid:88)
c

(cid:90) Te

Tb

λc(s)ds

(1)

(cid:17)

.

Hawkes Processes. Hawkes processes (Hawkes & Oakes,
1974) have a particular form of intensity:

λc(t) = µc +

φcc(cid:48)(t, s)dNc(cid:48)(s),

(2)

(cid:88)C

(cid:90) t

c(cid:48)=1

0

where µc is the exogenous base intensity independent of the
history while (cid:82) t
0 φcc(cid:48)(t, s)dNc(cid:48)(s) is the endogenous inten-
sity capturing the inﬂuence of historical events on type-c
ones at time t (Xu et al., 2016a). Here, φcc(cid:48)(t, s) ≥ 0
is called impact function.
It quantiﬁes the inﬂuence of
the type-c(cid:48) event at time s to the type-c event at time t.
Hawkes processes provide us with a physically-meaningful
model to capture the infectivity among various events,
which are used in social network analysis (Zhou et al.,
2013b; Zhao et al., 2015), behavior analysis (Yang & Zha,
2013; Luo et al., 2015) and ﬁnancial analysis (Bacry et al.,
2013). However, the methods in these references assume

that the impact function is shift-invariant (i.e., φcc(cid:48)(t, s) =
φcc(cid:48)(t − s), t ≥ s), which limits their applications on long-
time scale. Recently, the time-dependent Hawkes process
(TiDeH) in (Kobayashi & Lambiotte, 2016) and the neural
network-based Hawkes process in (Mei & Eisner, 2016)
learn very ﬂexible Hawkes processes with complicated in-
tensity functions. Because they highly depend on the size
and the quality of data, they may fail in the case of SDC
event sequences.

Learning from Imperfect Observations. In practice, we
need to learn sequential models from imperfect observa-
tions (e.g., interleaved (Xu et al., 2016b), aggregated (Luo
et al., 2016) and extremely-short sequences (Xu et al.,
2016c)). Multiple imputation (MI) (Rubin, 2009) is a gen-
eral framework to build surrogate observations from the
current model. For time series, bootstrap method (Efron,
1982; Politis & Romano, 1994; Gonc¸alves & Kilian, 2004)
and its variants (Paparoditis & Politis, 2001; Guan & Loh,
2007) have been used to improve learning results when ob-
servations are insufﬁcient. In survival analysis, many tech-
niques have been made to deal with truncated and censored
data (Turnbull, 1974; De Gruttola & Lagakos, 1989; Klein
& Moeschberger, 2005; Van den Berg & Drepper, 2016).
For point processes, the global (Streit, 2010) or local (Fan,
2009) likelihood maximization estimators (MLE) are used
to learn Poisson processes. Nonparametric approaches
for non-homogeneous Poisson processes use the pseudo
MLE (Sun & Kalbﬂeisch, 1995) or full MLE (Wellner &
Zhang, 2000). The bootstrap methods above are also used
to learn point processes (Cowling et al., 1996; Guan & Loh,
2007; Kirk & Stumpf, 2009). To learn Hawkes processes
robustly, structural constraints, e.g., low-rank (Luo et al.,
2015) and group-sparse regularizers (Xu et al., 2016a), are
introduced. However, all of these methods do not consider
the case of SDC event sequences for Hawkes processes.

3. Learning from SDC Event Sequences

Suppose that the original complete event sequences are
in a long observation window. However, the observation
window in practice might be segmented into several inter-
vals {T n
b , T n
n=1, and we can only observe Kn SDC se-
quences {sn
k=1 in the n-th interval, n = 1, ..., N . Al-
though we can still apply maximum likelihood estimator to
learn Hawkes processes, i.e.,

e }N
k }Kn

(cid:88)

minΘ −

log L(sn

k ; Θ),

n,k

(3)

the SDC event sequences would lead to over-ﬁtting prob-
lem and the loss of triggering patterns. Can we do better in
such a situation? In this work, we propose a data synthesis
method based on a sampling-stitching mechanism, which
extends SDC event sequences to longer ones and enhances
the robustness of learning algorithms.

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

3.1. Data Synthesis via Sampling-Stitching

Denote the k-th SDC event sequence in the n-th interval as
sn
k . Because its predecessor is unavailable, if we learn the
parameters of our model via (3) directly, we actually im-
pose a strong assumption on our data that there is no event
happening before sn
k (or previous events are too far away
from sn
k ). Obviously, this assump-
tion is questionable — it is likely that there are inﬂuential
events happening before sn
k . A more reasonable strategy is
enumerating potential predecessors and maximizing the ex-
pected log-likelihood over the whole observation window:

k to have inﬂuences on sn

(cid:88)

minΘ −

E

n,k

s∼HC

T n
b

[log L([s, sn

k ]; Θ)].

(4)

T n
b

means all possible history before T n

Here Ex∼D[f (x)] represents the expectation of function
f (x) with random variable x yielding to a distribution
D. s ∼ HC
b , and
k ]; Θ) is the likelihood of stitched sequence [s, sn
L([s, sn
k ].
The stitched sequence [s, sn
k ] can be generated via sam-
pling SDC sequence s from previous 1st, ..., (k − 1)-th in-
tervals and stitching s to sn
k . The sampling process yields
to the probabilistic distribution of the stitched sequences.
Given sn
k , we can compute its similarity between its poten-
tial predecessor sn(cid:48)
e ] as

b , T n(cid:48)

k(cid:48) in [T n(cid:48)
(cid:40)

w(sn(cid:48)

k(cid:48) , sn

k ) =

0, when T n(cid:48)
b , T n(cid:48)
S(T n

e ≥ T n
b

e )S(f n

k , f n(cid:48)

k(cid:48) ), o.w.

(5)

k(cid:48) , sn

Here, S(a, b) = exp(−(cid:107)b − a(cid:107)2
2/σs) is a predeﬁned sim-
ilarity function with parameter σs. f n
k is the feature of
sn
k , which is available in some applications. Note that the
availability of feature is optional — even if the feature of
sequence is unavailable, we can still deﬁne the similarity
measurement purely based on time stamps. The normal-
k )} provides us with the probability that sn(cid:48)
ized {w(sn(cid:48)
k(cid:48)
k ) ∝ w(sn(cid:48)
appears before sn
k ). Then, we
can sample sn(cid:48)
k(cid:48) according to the categorical distribution,
i.e., sn(cid:48)

k )).
We can apply such a sampling-stitching mechanism L
times iteratively to the SDC sequences in both backward
and forward directions and get long stitched event se-
quences. Speciﬁcally, we represent a stitched event se-
quence as sstitch = [s1, ..., s2L+1], sl ∈ {sn
k }, l =
1, ..., 2L + 1, whose probability is

k(cid:48) ∼ Category(w(·, sn

k , i.e., p(sn(cid:48)

k(cid:48) , sn

k(cid:48) |sn

p(sstitch) ∝

w(sl, sl+1).

(6)

(cid:89)2L
l=1

e ≤ T n

relax the constraint T n(cid:48)
b in (5) and allow a SDC se-
quence to have an overlap with its predecessor/successor.
In this case, we preserve the overlap part randomly either
from itself or its predecessor/successor before applying our
sampling-stitching method. These two variants ensure that
our data synthesis method is doable in practice, which are
used in the following experiments on real-world data.

3.2. Justiﬁcation

After applying our data synthesis method, we obtain many
stitched event sequences, which can be used as instances
for estimating E
k ]; Θ)]. Speciﬁcally,
taking advantage of stitched sequences, we can rewrite the
learning problem in (4) approximately as

[log L([s, sn

s∼HC

T n
b

(cid:88)

minΘ −

sstitch∈S

p(sstitch) log L(sstitch; Θ),

(7)

which is actually the minimum cross-entropy estimation.
p(sstitch) represents the “true” probability that the stitched
sequence happens, which is estimated via the predeﬁned
similarity measurement and the sampling mechanism. The
likelihood L(sstitch; Θ) represents the “unnatural” prob-
ability that the stitched sequence happens, which is esti-
mated based on the deﬁnition in (1). Our data synthesis
method takes advantage of the information of time stamps
and (optional) features and makes p(sstitch) suitable for
practical situations. For example, the likelihood of a se-
quence generally reduces with the increase of observation
time window. The proposed probability p(sstitch) yields to
the same pattern — according to (6), the longer a stitched
sequence is, the smaller its probability becomes.

The set of all possible stitched sequences, i.e., the S in (7),
is very large, whose cardinality is |S| = O((cid:81)N
n=1 Kn).
In practice, we cannot and do not need to enumerate all
possible combinations. An empirical setting is making the
number of stitched sequences comparable to that of orig-
inal SDC event sequences, i.e., generating O((cid:80)N
n=1 Kn)
stitched sequences. In the following experiments, we just
apply 5(= U ) trials and generate 5 stitched sequences for
each original SDC event sequence, which achieves a trade-
off between computational complexity and performance.

3.3. Feasibility

It should be noted that our data synthesis method is only
suitable for those complicated point processes whose his-
torical events have inﬂuences on current and future ones.
Speciﬁcally, we analyze the feasibility of our method for
several typical point processes.

Note that our data synthesis method naturally contains two
variants. When the starting (the ending) point of time win-
dow is unavailable, we use the time stamp of the ﬁrst (the
last) event of SDC sequence instead. Additionally, we can

Poisson Processes. Our data synthesis method cannot im-
prove learning results if the event sequences are generated
via Poisson processes. For Poisson processes, the hap-
pening rate of future events is independent of historical

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

events. In other words, the intensity function of each inter-
val can be learned independently based on the SDC event
sequences. The stitched sequences do not provide us with
any additional information.

Hawkes Processes. For Hawkes processes, whose inten-
sity function is deﬁned as (2), our data synthesis method
can enhance the robustness of learning algorithm generally.
In particular, consider a “long” event sequence generated
via a Hawkes process in the time window [Tb, Te]. If we
divide the time window into 2 intervals, i.e., [Tb, T ] and
(T, Te], the intensity function corresponding to the second
interval can be written as

λc(t) = µc +

φcci(t, ti) +

(cid:88)

ti≤T

(cid:88)

T <ti≤Te

φcci(t, ti).

(8)

If the events in the ﬁrst interval are unobserved, we just
have a SDC event sequence, and the second term in (8) is
unavailable. Learning Hawkes processes directly from the
SDC event sequence ignores the information of the second
term, which has a negative inﬂuence on learning results.
Our data synthesis method leverages the information from
other potential predecessors and generates multiple candi-
date long sequences. As a result, we obtain multiple in-
tensity functions sharing the second interval and maximize
the weighted sum of their log-likelihood functions (i.e., an
estimated expectation of the log-likelihood of the real long
sequence), as (7) does.

Compared with learning from SDC event sequences di-
rectly, applying our data synthesis method can im-
prove learning results
the term
(cid:80)
ti≤T φcci(t, ti) is ignorable. Speciﬁcally, we can model
the impact functions {φcc(cid:48)(t, s)}c,c(cid:48)∈C of Hawkes pro-
cesses based on basis representation:

in general, unless

φcc(cid:48)(t, s) = ψcc(cid:48)(t)
(cid:124) (cid:123)(cid:122) (cid:125)
Infectivity
(cid:88)M

=

m=1

× g(t − s)
(cid:124) (cid:123)(cid:122) (cid:125)
Triggering kernel

acc(cid:48)mκm(t)g(t − s).

Here, we decompose impact functions into two parts: 1)
Infectivity ψcc(cid:48)(t) = (cid:80)M
m=1 acc(cid:48)mκm(t) represents the in-
fectivity of event type c(cid:48) to c at time t.1 2) Triggering kernel
g(t) = exp(−βt) measures the time decay of infectivity. It
means that the infectivity of a historical event to current
one reduces exponentially with the increase of temporal
distance between them. When β is very large, φcc(cid:48)(t, s)
decays rapidly with the increase of t − s, and the events
happening long ago can be ignored. In such a situation, our
data synthesis method is unable to improve learning results.

1When M = 1 and κm(t) ≡ 1, we obtain the simplest time-
invariant Hawkes process. Relaxing the shift-invariant assump-
tion, i.e., M > 1 and κm(t) is Gaussian, we obtain a ﬂexible
time-varying Hawkes process model.

4. Implementation for Hawkes Processes

Hawkes process is a kind of physically-interpretable model
for many natural and social phenomena. The proposed
model in (9) reﬂects many common properties of real-
world event sequences. First, the infectivity among vari-
ous event types often changes smoothly in practice: in so-
cial networks, the interaction between two users changes
smoothly, which is not established or blocked suddenly;
in disease networks, the infectivity among diseases should
change smoothly with the increase of patient’s age. Apply-
ing Gaussian basis representation guarantees the smooth-
ness of infectivity function. Second, the triggering ker-
nel measures the decay of infectivity over time. Accord-
ing to existing work,
the decay of infectivity is expo-
nential approximately, which has been veriﬁed in many
real-world data (Zhou et al., 2013a; Kobayashi & Lam-
biotte, 2016; Choi et al., 2015). For learning Hawkes
processes from SDC event sequences, we combine our
data synthesis method with an EM-based learning algo-
rithm of Hawkes processes. Applying our data synthe-
sis method, we obtain a set of stitched event sequences
S = {sn} and their appearance probabilities {pn}, where
sn = {(tn
b , T n
i ∈ C} and pn is cal-
culated based on (5). According to (7, 9), we can learn
target Hawkes process via

i ∈ [T n

e ], cn

i=1|tn

i , cn

i )In

min
µ≥0, A≥0

−

(cid:88)|S|
n=1

pn log L(sn; Θ) + γR(A).

(10)

Here Θ = {µ, A} represents the parameters of our model.
The vector µ = [µc] and the tensor A = [acc(cid:48)m] are non-
negative. Based on (1, 9), the log-likelihood function is

log L(sn; Θ)

=

(cid:88)In
i=1

=

(cid:88)In
i=1

log λci(ti) −

λc(s)ds

(cid:88)C

(cid:90) T n

e

c=1

T n
b

(cid:20)
µcn

i

log

+

(cid:88)

j<i

(cid:88)

g(τ n
ij)

acn

i cn

(9)

(cid:88)C

−

(cid:16)
∆nµc −

c=1

(cid:88)

(cid:88)In
i=1

m

(cid:21)
j mκm(tn
i )
(cid:17)

accn

j mGij

,

m

(cid:88)

j≤i

i+1

tn
i

κm(s)g(s − tn

i − tn
e − T n

j , Gij = (cid:82) tn
where τ n
ij = tn
j )ds,
and ∆n = T n
b . R(A) represents the regularizer
of parameters, whose weight is γ. Following existing
work in (Luo et al., 2015; Zhou et al., 2013a; Xu et al.,
2016a), we assume the infectivity connections among dif-
ferent event types to be sparse and impose a (cid:96)1-norm regu-
larizer on the coefﬁcient tensor A, i.e., R(A) = (cid:107)A(cid:107)1 =
(cid:80)

c,c(cid:48),m |acc(cid:48)m|.

We can solve the problem via an EM algorithm. Specif-
ically, when sparse regularizer is applied, we take ad-
vantage of ADMM method, introducing auxiliary variable
Z = [zcc(cid:48)m] and dual variable U = [ucc(cid:48)m] for A and

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

rewriting the objective function in (10) as

−

pn log L(sn; Θ) + 0.5ρ(cid:107)A − Z(cid:107)2
F

(cid:88)
n
+ ρtr(U (cid:62)(A − Z)) + γ(cid:107)Z(cid:107)1.

Here ρ controls the weights of regularization terms, which
increases with the number of EM iterations. tr(·) computes
the trace of matrix. Then, we can update {µ, A}, Z, and
U alternatively.

Update µ and A: Given the parameters in the k-th iter-
ation, we apply Jensen’s inequality to − (cid:80)
n log L(sn; Θ)
and obtain a surrogate objective function for µ and A:

Q(µ, A; µk, Ak, Zk, U k)
(cid:26) In(cid:88)

(cid:20)(cid:88)

M
(cid:88)

N
(cid:88)

= −

pn

n=1

i=1

j<i

m=1

qijm log

g(τ n

ij)acn

j mκm(tn
i )

i cn
qijm

+ qi log

i

µcn
qi

−

C
(cid:88)

M
(cid:88)

(cid:88)

(cid:21)

accn

j mGij

− ∆n

(cid:27)

µc

C
(cid:88)

c=1

m=1
j≤i
+ 0.5ρ(cid:107)A − Zk + U k(cid:107)2
F ,

c=1

where qi =

µk
cn
i ) and qijm =
i
(tn

λk
cn
i

g(τ n

ij )ak
cn
i
λk
cn
i

mκm(tn
i )
cn
j
(tn
i )

, and

(tn

i ) is calculated based on µk and Ak. Then, we can
∂A = 0. Both of

λk
cn
i
update µ and A via solving ∂Q
these two equations have closed-form solution:

∂µ = 0 and ∂Q

µk+1

c =

ak+1
cc(cid:48)m =

(cid:80)

(cid:80)

,

n pn
(cid:80)

i =c qi
cn
n pn∆n
(cid:112)B2 − 4ρC − B
2ρ

,

(11)

where

B = ρ(uk

C = −

cc(cid:48)m − zk
(cid:88)
(cid:88)
n

pn

cc(cid:48)m) +

(cid:88)
n

pn

(cid:88)

i =c, cn
cn

j =c(cid:48), j≤i

Gij,

i =c, cn
cn

j =c(cid:48), j≤i

qijm.

Update Z: Given Ak+1 and U k, we can update Z via
solving the following optimization problem:

minZ γ(cid:107)Z(cid:107)1 + 0.5ρ(cid:107)Ak+1 − Z + U k(cid:107)2
F .

Applying soft-thresholding method, we have

Zk+1 = F γ

(Ak+1 + U k),

ρ

(12)

where Fη(x) = sign(x) min{|x| − η, 0} is the soft-
thresholding function.

Update U : Given Ak+1 and Zk+1, we can further update
dual variable as

U k+1 = U k + (Ak+1 − Zk+1).

(13)

Algorithm 1 Learning Algorithm of Hawkes Processes
1: Input: Event sequences S. The threshold V . Prede-

ﬁned parameters β, σκ, and γ.
2: Output: Parameters A and µ.
3: Initialize Ak and µk randomly. Zk = Ak, U k = 0.

k = 0, ρ = 1.

4: repeat
Obtain Ak+1 and µk+1 via (11).
5:
Obtain Zk+1 via (12).
6:
Obtain U k+1 via (13).
7:
k = k + 1, ρ = 1.5ρ.
8:
9: until (cid:107)Ak − Ak−1(cid:107)F < V
10: A = Ak, µ = µk.

In summary, Algorithm 1 shows the scheme of our learning
method. Note that the algorithm can be applied to SDC
event sequences directly via ignoring pn’s.

5. Experiments

5.1. Implementation Details

To demonstrate the usefulness of our data synthesis
method, we combine it with various learning algorithms of
Hawkes processes and learn different models accordingly
from SDC event sequences. For time-invariant Hawkes
processes, we consider two learning algorithms — our EM-
based learning algorithm and the least squares (LS) algo-
rithm in (Eichler et al., 2016). For time-varying Hawkes
processes, we apply our EM-based learning algorithm. In
the following experiments, we use Gaussian basis func-
tions: κm(t) = exp((t − tm)2/σκ) with center tm and
bandwidth σκ. The number and the bandwidth of basis can
be set according to the basis selection method proposed
in (Xu et al., 2016a). Additionally, we set V = 10−4,
γ = 1, and σs = 1 in our algorithm. Given SDC event
sequences, we learn Hawkes processes in three ways: 1)
learning directly from SDC event sequences; 2) apply-
ing the stationary bootstrap method in (Politis & Romano,
1994) to generate more synthetic SDC event sequences
and learning from these sequences accordingly; 3) learn-
ing from stitched sequences generated via our data synthe-
sis method. For real-world data, whose SDC sequences do
not have predeﬁned starting and ending time stamps, we
applied the variants of our method mentioned in the end of
Section 3.1.

5.2. Synthetic Data

The synthetic SDC event sequences are generated via the
following method: 2000 complete event sequences are sim-
ulated in the time window [0, 50] based on a 2-dimensional
Hawkes process. The base intensity {µc}2
c=1 are randomly
generated in the range [0.1, 0.2]. The parameter of trigger-

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

(a) Our learning algorithm

Figure 3. Comparisons on log-likelihood and relative error.

(b) Least squares algorithm

Figure 2. Comparisons on log-likelihood and relative error.

ing kernel, β, is set to be 0.2. For time-invariant Hawkes
processes, we set the infectivity {ψcc(cid:48)(t)} to be 4 constants
randomly generated in the range [0, 0.2]. For time-varying
Hawkes processes, we set ψcc(cid:48)(t) = 0.2 cos(2π ωcc(cid:48)
50 t),
where {ωcc(cid:48)} are randomly generated in the range [1, 4].
Given these complete event sequences, we select 1000 se-
quences as testing set while the remaining 1000 sequences
as training set. To generate SDC event sequences, we seg-
ment time window into 10 intervals, and just randomly pre-
serve the data in one interval for each training sequences.
We test all methods in 10 trials and compare them on the
relative error between real parameters Θ and their estima-
tion results (cid:98)Θ, i.e., (cid:107)Θ− (cid:98)Θ(cid:107)2
, and the log-likelihood of test-
(cid:107)Θ(cid:107)2
ing sequences.

Time-invariant Hawkes Processes. Fig. 2 shows the com-
parisons on log-likelihood and relative error for various
methods. In Fig. 2(a) we can ﬁnd that compared with the
learning results based on complete event sequences, the re-
sults based on SDC event sequences degrade a lot (lower
log-likelihood and higher relative error) because of the loss
of information. Our data synthesis method improves the
learning results consistently with the increase of training
sequences, which outperforms its bootstrap-based competi-
tor (Politis & Romano, 1994) as well. To demonstrate the
universality of our method, besides our EM-based algo-
rithm, we apply our method to the Least Squares (LS) algo-

Figure 4. Comparisons on infectivity functions {ψcc(cid:48) (t)}. The
number of original SDC sequences is 200 and stitched via our
method once.

rithm (Eichler et al., 2016). Fig. 2(b) shows that our method
also improves the learning results of the LS algorithm in
the case of SDC event sequences. Both the log-likelihood
and the relative error obtained from the stitched sequences
approach to the results learned from complete sequences.

Time-varying Hawkes Processes. Fig. 3 shows the com-
parisons on log-likelihood and relative error for various
methods. Similarly, the learning results are improved be-
cause of applying our method — higher log-likelihood and
lower relative error are obtained and their standard devia-
tion (the error bars associated with curves) is shrunk. In
this case, applying our method twice achieves better results
than applying once, which veriﬁes the usefulness of the it-
erative framework in our sampling-stitching algorithm. Be-
sides objective measurements, in Fig. 4 we visualize the
infectivity functions {ψcc(cid:48)(t)}. It is easy to ﬁnd that the
infectivity functions learned from stitched sequences (red
curves) are comparable to those learned from complete
event sequences (yellow curves), which have small estima-
tion errors of the ground truth (black curves).

Note that our iterative framework is useful, especially
for time-varying Hawkes processes, when the number of
In our experiments, we ﬁxed
stitches is not very large.
the maximum number of synthetic sequences. As a result,
Figs. 2 and 3 show that the likelihoods ﬁrst increase (i.e.,
stitching once or twice) and then decrease (i.e., stitching
more than three times) while the relative errors have oppo-
nent changes w.r.t. the number of stitches. These phenom-
ena imply that too many stitches introduce too much un-
reliable interdependency among events. Therefore, we ﬁx

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

(a) LinkedIn data

(a) LinkedIn data

(b) MIMIC III data

Figure 5. Comparisons on log-likelihood.

the number of stitches to 2 in the following experiments.

(b) MIMIC III data

5.3. Real-World Data

Besides synthetic data, we also test our method on real-
world data, including the LinkedIn data collected by our-
selves and the MIMIC III data set (Johnson et al., 2016).

LinkedIn Data. The LinkedIn data we collected online
contain job hopping records of 3, 000 LinkedIn users in 82
IT companies. For each user, her/his check-in time stamps
corresponding to different companies are recorded as an
event sequence, and her/his proﬁle (e.g., education back-
ground, skill list, etc.) is treated as the feature associated
with the sequence. For each person, the attractiveness of
a company is always time-varying. For example, a young
man may be willing to join in startup companies and in-
crease his income via jumping between different compa-
nies. With the increase of age, he would more like to stay
in the same company and achieve internal promotions. In
other words, the infectivity network among different com-
panies should be dynamical w.r.t.
the age of employees.
Unfortunately, most of the records in LinkedIn are short
and doubly-censored — only the job hopping events in re-
cent years are recorded. How to construct the dynamical in-
fectivity network among different companies from the SDC
event sequences is still an open problem.

Applying our data synthesis method, we can stitch differ-
ent users’ job hopping sequences based on their ages (time
stamps) and their proﬁle (feature) and learn the dynamical
network of company over time. In particular, we select 100
users with relatively complete job hopping history (i.e., the
range of their working experience is over 25 years) as test-
ing set. The remaining 2, 900 users are randomly selected
as training set. The log-likelihood of testing set in 10 trials
is shown in Fig. 5(a). We can ﬁnd that the log-likelihood
obtained from stitched sequences is higher than that ob-
tained from original SDC sequences or that obtained from
the sequences generated via the bootstrap method (Politis
& Romano, 1994), and its standard deviation is bounded

Figure 6. Comparisons on infectivity functions {ψcc(cid:48) (t)}.

stably. Fig. 6(a) visualizes the adjacent matrix of infec-
tivity network. The properties of the network veriﬁes the
rationality of our results: 1) the diagonal elements of the
adjacent matrix are larger than other elements in general,
which reﬂects the fact that most employees would like to
stay in the same company and achieve a series of internal
promotions; 2) with the increase of age, the infectivity net-
work becomes sparse, which reﬂects the fact that users are
more likely to try different companies in the early stages of
their careers.

MIMIC III Data. The MIMIC III data contain admis-
sion records of over 40, 000 patients in the Beth Israel
Deaconess Medical Center between 2001 and 2012. For
each patient, her/his admission time stamps and diseases
(represented via the ICD-9 codes (Deyo et al., 1992)) are
recorded as an event sequence, and her/his proﬁle (includ-
ing gender, race and chronic history) is represented as bi-
nary feature of the sequence. As aforementioned, some
work (Choi et al., 2015) has been done to extract time-
invariant disease network from admission records. How-
ever, the real disease network should be time-varying w.r.t.
the age of patient. Similar to the LinkedIn data, here we
only obtain SDC event sequences — the ranges of most
admission records are just 1 or 2 years.

Applying our data synthesis method, we can leverage the
information from different patients and stitch their se-
quences based on their ages and their proﬁle. Focusing on
600 common diseases in 12 categories, we select 15, 000
patients’ admission records randomly as training set and
1, 000 patients with relatively complete records as test-
ing set. Fig. 5(b) shows that applying our data synthesis
method indeed helps to improve log-likelihood of testing
data. Compared with our bootstrap-based competitor, our
data synthesis method gets more obvious improvements.

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

Figure 7. The diseases (nodes) are labeled with ICD-9 codes. The colors indicate their sub-categories. The size of the c-th node is
(cid:80)

c(cid:48) ψcc(cid:48) (t)). The width of edge is ψcc(cid:48) (t).

Furthermore, we visualize the adjacent matrix of dynami-
cal network of disease categories in Fig. 6(b). We can ﬁnd
that: 1) with the increase of age the disease network be-
comes dense, which reﬂects the fact that the complications
of diseases are more and more common when people be-
come old; 2) the networks show that neoplasms and the
diseases of circulatory, respiratory, and digestive systems
have strong self-triggering patterns because the treatments
of these diseases often include several phases and require
patients to make admission multiple times; 3) for kids and
teenagers, their disease networks (i.e., “Age 0” and “Age
10” networks) are very sparse, and their common diseases
mainly include neoplasms and the diseases of circulatory,
respiratory, and digestive systems; 4) for middle-aged peo-
ple, the reasons for their admissions are diverse and com-
plicated so that their disease networks are dense and in-
clude many mutually-triggering patterns; 5) for longevity
people, their disease networks (i.e., “Age 80” and “Age 90”
networks) are relatively sparser than those of middle-aged
people, because their admissions are generally caused by
elderly chronic diseases.

Additionally, we visualize the dynamical networks of the
diseases of circulatory systems in Fig. 7, and ﬁnd some in-
teresting triggering patterns. For example, for kids (“Age
0” network), the typical circulatory diseases are “diseases
of mitral and aortic valves” (ICD-9 396) and “cardiac dys-
rhythmias” (ICD-9 427), which are common for premature
babies and the kids having congenital heart disease. For
the old (“Age 80” network), the network becomes dense.
We can ﬁnd that 1) as a main cause of death, “heart fail-
ure” (ICD-9 428) is triggered via multiple other diseases,
especially “secondary hypertension” (ICD-9 405); 2) “sec-

ondary hypertension” is also likely to cause “other and ill-
deﬁned cerebrovascular disease” (ICD-9 437); 3) “Hem-
orrhoids” (ICD-9 455), as a common disease with strong
self-triggering pattern, will cause frequent admissions of
patients. In summary, the analysis above veriﬁes the ratio-
nality of our result — the dynamical disease networks we
learned indeed reﬂect the properties of human’s health tra-
jectory. The list of ICD-9 codes and the complete enlarged
network over age are shown in the supplementary ﬁle.

6. Conclusion

In this paper, we propose a novel data synthesis method to
learn Hawkes processes from SDC event sequences. With
the help of temporal information and optional features,
we measure the similarities among different SDC event
sequences and estimate the distribution of potential long
event sequences. Applying a sampling-stitching mecha-
nism, we successfully synthesize a large amount of long
event sequences and learn point processes robustly. We
test our method for both time-invariant and time-varying
Hawkes processes. Experiments show that our data synthe-
sis method improves the robustness of learning algorithms
for various models. In the future, we plan to provide more
theoretical and quantitative analysis to our data synthesis
method and apply it to more applications.

Acknowledgements

This work is supported in part via NSF IIS-1639792,
DMS-1317424, NIH R01 GM108341, NSFC 61628203,
U1609220 and the Key Program of Shanghai Science and
Technology Commission 15JC1401700.

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

References

Bacry, Emmanuel, Delattre, Sylvain, Hoffmann, Marc, and
Muzy, Jean-Francois. Some limit theorems for hawkes
processes and application to ﬁnancial statistics. Stochas-
tic Processes and their Applications, 123(7):2475–2499,
2013.

Choi, Edward, Du, Nan, Chen, Robert, Song, Le, and Sun,
Jimeng. Constructing disease network and temporal pro-
gression model via context-sensitive hawkes process. In
ICDM, 2015.

Cowling, Ann, Hall, Peter, and Phillips, Michael J. Boot-
strap conﬁdence regions for the intensity of a poisson
point process. Journal of the American Statistical Asso-
ciation, 91(436):1516–1524, 1996.

Daley, Daryl J and Vere-Jones, David. An introduction to
the theory of point processes: volume II: general the-
ory and structure. Springer Science & Business Media,
2007.

De Gruttola, Victor and Lagakos, Stephen W. Analysis of
doubly-censored survival data, with application to aids.
Biometrics, pp. 1–11, 1989.

Deyo, Richard A, Cherkin, Daniel C, and Ciol, Marcia A.
Adapting a clinical comorbidity index for use with icd-
9-cm administrative databases. Journal of clinical epi-
demiology, 45(6):613–619, 1992.

Efron, Bradley. The jackknife, the bootstrap and other re-

sampling plans, volume 38. SIAM, 1982.

Eichler, Michael, Dahlhaus, Rainer, and Dueck, Johannes.
Graphical modeling for multivariate hawkes processes
with nonparametric link functions. Journal of Time Se-
ries Analysis, 2016.

Fan, Chun-Po Steve.

Local Likelihood for Interval-
censored and Aggregated Point Process Data. PhD the-
sis, University of Toronto, 2009.

Gonc¸alves, Sılvia and Kilian, Lutz. Bootstrapping au-
toregressions with conditional heteroskedasticity of un-
known form. Journal of Econometrics, 123(1):89–120,
2004.

Johnson, Alistair EW, Pollard, Tom J, Shen, Lu, Lehman,
Li-wei H, Feng, Mengling, Ghassemi, Mohammad,
Moody, Benjamin, Szolovits, Peter, Celi, Leo Anthony,
and Mark, Roger G. Mimic-iii, a freely accessible criti-
cal care database. Scientiﬁc data, 3, 2016.

Kirk, Paul DW and Stumpf, Michael PH. Gaussian pro-
cess regression bootstrapping: exploring the effects of
uncertainty in time course data. Bioinformatics, 25(10):
1300–1306, 2009.

Klein, John P and Moeschberger, Melvin L.

Survival
analysis: techniques for censored and truncated data.
Springer Science & Business Media, 2005.

Kobayashi, Ryota and Lambiotte, Renaud. Tideh: Time-
dependent hawkes process for predicting retweet dynam-
ics. arXiv preprint arXiv:1603.09449, 2016.

Luo, Dixin, Xu, Hongteng, Zhen, Yi, Ning, Xia, Zha,
Hongyuan, Yang, Xiaokang, and Zhang, Wenjun. Multi-
task multi-dimensional hawkes processes for modeling
event sequences. In IJCAI, 2015.

Luo, Dixin, Xu, Hongteng, Zhen, Yi, Dilkina, Bistra, Zha,
Hongyuan, Yang, Xiaokang, and Zhang, Wenjun. Learn-
ing mixtures of markov chains from aggregate data with
structural constraints. Transactions on Knowledge and
Data Engineering, 28(6):1518–1531, 2016.

Mei, Hongyuan and Eisner, Jason. The neural hawkes pro-
cess: A neurally self-modulating multivariate point pro-
cess. arXiv preprint arXiv:1612.09328, 2016.

Paparoditis, Efstathios and Politis, Dimitris N. Tapered
block bootstrap. Biometrika, 88(4):1105–1119, 2001.

Politis, Dimitris N and Romano, Joseph P. The stationary
bootstrap. Journal of the American Statistical associa-
tion, 89(428):1303–1313, 1994.

Rubin, Donald B. Multiple Imputation for Nonresponse in

Surveys, volume 307. John Wiley & Sons, 2009.

Streit, Roy L. Poisson point processes: imaging, tracking,
and sensing. Springer Science & Business Media, 2010.

Sun, J and Kalbﬂeisch, JD. Estimation of the mean function
of point processes based on panel count data. Statistica
Sinica, pp. 279–289, 1995.

Guan, Yongtao and Loh, Ji Meng. A thinned block boot-
strap variance estimation procedure for inhomogeneous
spatial point patterns. Journal of the American Statistical
Association, 102(480):1377–1386, 2007.

Turnbull, Bruce W. Nonparametric estimation of a sur-
vivorship function with doubly censored data. Journal of
the American Statistical Association, 69(345):169–173,
1974.

Hawkes, Alan G and Oakes, David. A cluster process rep-
resentation of a self-exciting process. Journal of Applied
Probability, pp. 493–503, 1974.

Van den Berg, Gerard J and Drepper, Bettina. Inference for
shared-frailty survival models with left-truncated data.
Econometric Reviews, 35(6):1075–1098, 2016.

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

Wellner, Jon A and Zhang, Ying. Two estimators of the
mean of a counting process with panel count data. An-
nals of Statistics, pp. 779–814, 2000.

Xu, Hongteng, Zhen, Yi, and Zha, Hongyuan. Trailer gen-
eration via a point process-based visual attractiveness
model. In IJCAI, 2015.

Xu, Hongteng, Farajtabar, Mehrdad, and Zha, Hongyuan.
Learning granger causality for hawkes processes.
In
ICML, 2016a.

Xu, Hongteng, Ning, Xia, Zhang, Hui, Rhee, Junghwan,
and Jiang, Guofei. Pinfer: Learning to infer concur-
rent request paths from system kernel events. In ICAC,
2016b.

Xu, Hongteng, Wu, Weichang, Nemati, Shamim, and Zha,
Hongyuan.
Icu patient ﬂow prediction via discrimina-
tive learning of mutually-correcting processes. arXiv
preprint arXiv:1602.05112, 2016c.

Yang, Shuang-Hong and Zha, Hongyuan. Mixture of mu-
In ICML,

tually exciting processes for viral diffusion.
2013.

Zhao, Qingyuan, Erdogdu, Murat A, He, Hera Y, Rajara-
man, Anand, and Leskovec, Jure. Seismic: A self-
exciting point process model for predicting tweet pop-
ularity. In KDD, 2015.

Zhou, Ke, Zha, Hongyuan, and Song, Le. Learning so-
cial infectivity in sparse low-rank networks using multi-
dimensional hawkes processes. In AISTATS, 2013a.

Zhou, Ke, Zha, Hongyuan, and Song, Le. Learning trigger-
ing kernels for multi-dimensional hawkes processes. In
ICML, 2013b.

