Source-Target Similarity Modelings for Multi-Source Transfer Gaussian
Process Regression

Pengfei Wei 1 2 Ramon Sagarna 1 2 Yiping Ke 1 2 Yew-Soon Ong 1 2 Chi-Keong Goh 3 2

Abstract

A key challenge in multi-source transfer learn-
ing is to capture the diverse inter-domain sim-
ilarities.
In this paper, we study different ap-
proaches based on Gaussian process models to
solve the multi-source transfer regression prob-
lem. Precisely, we ﬁrst investigate the feasibility
and performance of a family of transfer covari-
ance functions that represent the pairwise simi-
larity of each source and the target domain. We
theoretically show that using such a transfer co-
variance function for general Gaussian process
modelling can only capture the same similarity
coefﬁcient for all the sources, and thus may re-
sult in unsatisfactory transfer performance. This
leads us to propose TCM SStack, an integrated
strategy incorporating the beneﬁts of the trans-
fer covariance function and stacking. Exten-
sive experiments on one synthetic and two real-
world datasets, with learning settings of up to 11
sources for the latter, demonstrate the effective-
ness of our proposed TCM SStack.

1. Introduction

Transfer learning (TL) methods show specially appealing
for real-world applications where the data from the target
domain is scarce but a good amount of data from another
source domain is available. With research efforts largely
conﬁned to the single-source setting (Pan et al., 2011; Wei
et al., 2016; Zhou et al., 2016), an increasing amount of
studies are contributing to a realistic applicability of TL by
addressing the multi-source scenario, mainly for classiﬁca-

1School of Computer Science and Engineering, Nanyang
Technological University, Singapore 2Rolls-Royce@Nanyang
Technological University Corporate Lab 3Rolls-Royce Ad-
vanced Technology Centre, Singapore.
Correspondence to:
Pengfei Wei <Pwei001@e.ntu.edu.sg>, Ramon Sagarna <sara-
mon@ntu.edu.sg>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

tion (Tommasi et al., 2014; Fang et al., 2015; Bhatt et al.,
2016). The problem of regression, however, has been much
less studied, despite of the variety of real-world domains
in which it arises; for instance wiﬁ or indoor signal loca-
tion (Pan et al., 2008), biological data analysis (Lam et al.,
2016), or mechanical system design (Ghosh et al., 2015).
In this work, we concentrate on multi-source transfer re-
gression (MSTR) based on Gaussian process (GP) models.

All the way through, the TL community has been paying
attention to modeling the similarity between different do-
mains so that only the source knowledge that is helpful
for the target domain is transferred. This is because de-
signing a TL method based on the assumption that domains
are mutually relevant may lead to negative transfer (Pan &
Yang, 2010). Similarity capture is particularly crucial in
multi-source TL as the transfer capacity to the target task
may differ considerably across the diverse source domains.
Thus, TL methods that are capable of tuning the strength of
the knowledge transfer to the similarity of the domains are
attracting increasing interest (Luo et al., 2008; Wang et al.,
2014; Al-Stouhi & Reddy, 2011).

As regards to MSTR, a key issue is to capture the diverse
Source-Target (S-T) similarities. The relatively few efforts
to date have focused on ensemble methods. Particularly, an
amount of works rely on the boosting strategy due to its ca-
pability to capture ﬁne-grained S-T similarities be weight-
ing the contribution of train instances individually (Dai
et al., 2007; Pardoe & Stone, 2010; Yao & Doretto, 2010).
However, as outlined in (Al-Stouhi & Reddy, 2011), such
an instance-based similarity strategy in boosting has shown
issues with slow/premature weights convergence that have
seriously penalized the computational cost or the transfer
performance. Another type of ensemble strategy for multi-
source transfer is stacking (Wolpert, 1992). Pardoe and
Stone propose a meta-model that aggregates the predictions
of several base models previously learned with each source
in isolation (Pardoe & Stone, 2010) . The aggregation is
done by assigning each base model a model importance. In
this case, the S-T similarities can be captured through the
model importance. However, in such stacking-based meth-
ods, the base models suffer from a lack of consideration of
the dependencies between the different source domains.

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

Another popular idea to model the S-T similarity is to
construct a transfer covariance function that relates two
data points from distinct domains through the similarity
coefﬁcients (Bonilla et al., 2008; Williams et al., 2009).
Such idea has been proposed in multi-task learning (Bonilla
et al., 2008), where each task pair is assigned a particular
similarity coefﬁcient. Note, however, that multi-task learn-
ing differs from the TL problem in that the former aims at
improving performance across all the domains while the
objective of the latter focuses on the target domain only.
Nevertheless, the idea of transfer covariance function is ref-
erential for the TL problem. In (Cao et al., 2010), a single
source transfer covariance function (TCSS) was proposed.
In the corresponding transfer covariance matrix, one sim-
ilarity coefﬁcient was assigned to the S-T block to model
the inter-domain similarity. A GP with such TCSS (called
GP-TCSS) was then trained for the transfer task.

When generalizing to MSTR, one may naturally consider
a multi-source transfer covariance function (TCM S) with
different similarity coefﬁcients attached to distinct S-T
blocks in the corresponding transfer covariance matrix. In
this work, we investigate the feasibility of such covariance
function. We theoretically prove that a general GP with
TCM S (GP-TCM S) fails to capture the similarity diversi-
ties of various S-T domain pairs. Although TCM S intends
to utilize different similarity coefﬁcients, the learnt GP-
TCM S would give the same similarity coefﬁcient for all the
S-T domain pairs. The generalization error bounds of the
learnt GP-TCM S show that this coefﬁcient is taking effect
in every source domain. Considering the diverse S-T sim-
ilarities between the sources and the target, this may jeop-
ardize the transfer performance, especially when the num-
ber of sources increases. Moreover, the learning of GP-
TCM S rapidly poses a computational issue with increas-
ing amounts of source domains as usually O(m3) compu-
tations are required to evaluate a model for m data points.

The unsatisfactory performance of GP-TCM S leads us to
exploit the transfer covariance function in another way.
Considering that both the stacking strategy and the trans-
fer covariance function can model the S-T similarity and
using the transfer covariance function at the base models
would therefore add ﬂexibility to the similarity capture ca-
pability of the stacking approach, we propose to integrate
them into one uniﬁed model. Speciﬁcally, we ﬁrst dis-
cuss TCSSStack, a method that simply stacks GP-TCSS
base models. TCSSStack alleviates the computational is-
sue of GP since it allows to stretch the number of sources
due to its O(N n3) cost for N sources with n points each.
However, TCSSStack still suffers from the aforementioned
limitation of conventional stacking. Thus, we propose a
more involved TCM SStack. Two salient features make
TCM SStack signiﬁcantly different from TCSSStack: (i) it
associates the similarity coefﬁcients in the base GP-TCSS

with the model importance during learning, and (ii) it learns
the model importance and the base GP-TCSS jointly. By
doing so, on the one hand, TCM SStack further reduces the
computational cost by lowering the number of optimization
variables. On the other hand, although the similarity coefﬁ-
cient in TCM SStack represents bivariate S-T similarity re-
lations, they are elicited by pondering all the inter-domain
dependencies. In the experiments, we show the superior-
ity of TCM SStack on the transfer performance compared
to TCSSStack, GP-TCM S, and other MSTR methods.

2. Related Work

A main challenge in MSTR is to precisely capture the di-
verse S-T transfer capacities across the different sources.
Ensemble approaches (Dai et al., 2007), which can pro-
vide an explicit, ﬁne-grained similarity capture, are widely
used to handle the MSTR problems. In (Pardoe & Stone,
2010), TrAdaBoost.R2 was proposed, a boosting based al-
gorithm that weights the contribution of train instances in-
dividually, and thus delivers a model accounting for the S-T
similarities for every instance. However, such boosting-
like methods suffer from slow/premature convergence is-
sues that tremendously jeopardize the transfer performance
(Al-Stouhi & Reddy, 2011). Pardoe and Stone also intro-
duced a multi-source transfer stacking in which base mod-
els are pretrained in different source domains separately,
and a meta-model is trained by aggregating the outputs of
the base models (Pardoe & Stone, 2010). By doing so, the
S-T similarities are captured at meta-model level by the
learnt model importance. Although the stacking methods
show success in some MSTR problems, they have the limi-
tation that inter-domain dependencies between sources are
ignored at the base models.

At the other end of the spectrum are transfer covariance
function representing a multivariate similarity relation over
sources and target domains. A popular representative of
this family is the work by Bonilla et al.
(Bonilla et al.,
2008) on multi-task learning, where a free-form kernel re-
lates each pair of tasks. Apart from the difference of the ap-
plication domain (multi-task learning versus TL), this kind
of models often imply ﬁtting an increasingly large num-
in the free-form kernel, this
ber of hyperparameters; e.g.
number grows as (N 2 − N )/2, where N is the number of
sources. Motivated by (Bonilla et al., 2008), (Cao et al.,
2010) develops another transfer covariance function for the
single source transfer.

In this work, we ﬁrst describe a family of transfer covari-
ance functions, and investigate their feasibility for MSTR.
With the theoretical analysis showing the unsatisfactory
performance of such transfer covariance function, we pro-
pose to unify the S-T similarity capture of stacking and the
transfer covariance function. To the best of our knowledge,

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

this is the ﬁrst work that analyzes the feasibility and perfor-
mance of such family of transfer covariance functions for
MSTR, and further combines them with stacking.

3. Problem Statement

We denote a domain set for MSTR as D = S ∪ T where
S = {Si : 1 ≤ i ≤ N } is a set of source domains and
T is the target domain. All source domain data and few
target domain data are labeled. Denote the data matrix
and its corresponding label vector in each source domain
Si as X(Si) ∈ RnSi ×d and y(Si) ∈ RnSi . Likewise, we
represent the target data set with X(T ) = {X(Tl), X(Tu)}
where X(Tl) ∈ RnTl ×d is the labeled target data matrix and
X(Tu) ∈ RnTu ×d is the unlabeled one. We further deﬁne
y(Tl) ∈ RnTl as the label vector for X(Tl). Moreover, we
assume nTl (cid:28) min(nS1 , ..., nSN , nTu). Our objective is
i=1 and {X(Tl), y(Tl)} to predict
to utilize {X(Si), y(Si)}N
labels for X(Tu).

We use the GP model for this regression task. We denote
the underlying latent function between the inputs x and the
outputs y as f , and the noise variance as σ2. Thus, f de-
notes the function vector over X. A GP model deﬁnes a
Gaussian distribution over the functions, f ∼ N (µ, K) in
which µ is the mean vector and K is the covariance ma-
trix which is positive semi-deﬁnite (PSD, or equivalently
denoted as K (cid:23) 0). Usually µ is assumed to be 0, and
thus the GP model is completely speciﬁed by K given a
covariance function which is parameterized by Ω.

4. GP with Transfer Covariance Function

In this section, we analyze the transfer performance of GP
using a speciﬁc family of transfer covariance function.

4.1. Transfer Covariance Function for Multi-Source

Since the GP model is speciﬁed by K, one straightfor-
ward way to achieve the knowledge transfer across multiple
source domains and the target domain is to design a trans-
fer covariance function for multi-source. Different from a
classical GP which uses a ﬁxed covariance function for the
data from different domains, we focus on the covariance
function of the form (TCM S):



λik(x, x(cid:48)), x ∈ X(Si) & x(cid:48) ∈ X(T )
or x ∈ X(T )& x(cid:48) ∈ X(Si),

k∗(x, x(cid:48)) =



k(x, x(cid:48)),

otherwise.

where k(·, ·) is any valid covariance function, and λi is the
metric measuring the similarity between the source Si and
the target T . Through the learning, λi is expected to cap-
ture the different transfer strengths in different S-T domain
pairs. Those highly target-related sources will play a more

important role in transfer, while those completely target-
unrelated sources will not be considered. However, to guar-
antee the GP model is always valid, any covariance matrix
K∗ constructed by k∗(·, ·) should be PSD. Theorem 1 gives
the sufﬁcient and necessary condition for a PSD K∗.
Theorem 1. Let KDiDj (Di, Dj ∈ D) denote a covariance
matrix for points in Di and Dj. A Gram matrix

K∗ =







KS1S1
...
KSN S1
λ1KT S1

...

... KS1SN
...
... KSN SN
... λN KT SN

λ1KS1T
...
λN KSN T
KT T







is PSD for any covariance matrix K in the form

K =







KS1S1
...
KSN S1
KT S1

...

... KS1SN KS1T
...
... KSN SN KSN T
... KT SN KT T

...







if and only if λ1 = ... = λN and |λi| ≤ 1.

Proof. Necessary condition: Let K∗ be a PSD matrix. We
use KSS to represent the sources-to-sources block matrix,
and KST , KST ∗ to represent the sources-to-target block
matrix in K and K∗, respectively. Thus, we have:
(cid:21)

(cid:21)

K =

(cid:20) KSS KST
ST KT T

KT

, K∗ =

(cid:20) KSS KST ∗
KT T

KT

ST ∗

.

Since K is PSD, according to the Schur complement theo-
rem (Zhang, 2006), we have:

(I − KSS (cid:101)KSS )KST = 0,

KT T − KT

ST (cid:101)KSS KST (cid:23) 0,

(2)

(3)

where (cid:101)KSS is the generalized inverse of KSS . By rewrit-
ing (cid:101)KSS as a block matrix using (cid:101)KSiSj as the element, we
further derive eq. (2) and eq. (3) as:

KS1T −

KS1Si (cid:101)KSiSj KSj T

KSN T −

KSN Si (cid:101)KSiSj KSj T









= 0,

(4)

















N
(cid:80)
i=1

N
(cid:80)
j=1

N
(cid:80)
i=1

N
(cid:80)
j=1

...

N
(cid:88)

N
(cid:88)

i=1

j=1

N
(cid:80)
i=1

N
(cid:80)
j=1

N
(cid:80)
i=1

N
(cid:80)
j=1

...

KT T −

KT Si (cid:101)KSiSj KSj T (cid:23) 0.

(5)

Likewise, for the PSD matrix K∗, we have the following
two Schur complement derivations:

(1)

λ1KS1T −

λjKS1Si (cid:101)KSiSj KSj T

λN KSN T −

λjKSN Si (cid:101)KSiSj KSj T









= 0.

(6)

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

To sum up, we conclude that if K∗ is a PSD matrix, λi
should satisfy λ1 = ... = λN and |λi| ≤ 1.

C∗ = Λ

KT T −

λiλjKT Si (cid:101)KSiSj KSj T (cid:23) 0.

(7)

N
(cid:88)

N
(cid:88)

i=1

j=1

Combining eq. (4) and eq. (6), we get:









N
(cid:80)
i=1

N
(cid:80)
j=1

N
(cid:80)
i=1

N
(cid:80)
j=1

(λ1 − λj)KS1Si (cid:101)KSiSj KSj T

...

= 0.

(8)

(λN − λj)KSN Si (cid:101)KSiSj KSj T









Since Eq. (8) must hold for all PSD K, we induce λ1 =
... = λN = λ. Based on such conclusion, we combine eq.
(5) and eq. (7):

(1 − λ2)KT T + λ2M (cid:23) 0,

(9)

N
(cid:80)
i=1

N
(cid:80)
j=1

where M = KT T −

KT Si (cid:101)KSiSj KSj T . Since eq.

(9) must hold for all PSD KT T and PSD M, we resolve
that |λ| ≤ 1.

Sufﬁcient condition: Let λ1 = ... = λN = λ, and |λ| ≤ 1.
According to the Theorem 1 in (Cao et al., 2010), we obtain
that K∗ is a PSD matrix.

From Theorem 1, we can see that |λi| ≤ 1, which means
a highly target-related source results in a full transfer of
KSi,T , but a completely target-unrelated source results in
a zero block matrix. This indicates the adaptiveness of λi.
However, Theorem 1 also shows that k∗(·, ·) can just give
one similarity coefﬁcient for all S-T domain pairs to ensure
the validity of GP-TCM S. Such single similarity coefﬁ-
cient compromises the diverse similarities between differ-
ent S-T domain pairs. This violates the original intention of
λi which is to distinguish the similarity diversity between
different S-T domain pairs.

4.2. Generalization Bounds of GP-TCM S

To investigate the effect of a single compromised similar-
ity coefﬁcient on the performance of GP-TCM S, we de-
rive its generalization error bounds.
In (Chai, 2009), an
earlier analysis can be found for the generalization errors
and learning curves in multi-task learning (speciﬁcally, two
learning tasks with the same noise variance). Our investiga-
tion is different from that work however as we are working
on a TL setting, and more importantly, on multiple sources
with different noise variances.

We denote the single compromised similarity coefﬁcient as
λ, and the noise variance for different domains as σ2
d, d ∈
{S1, ..., SN , T }. Thus, the transfer covariance matrix of

the noisy training data is C∗ = K∗ + Σ where

K∗ =

(cid:20) KSS
λKT

λKST
ST KT T

(cid:21)

, Σ =

(cid:20) ΣS
0

(cid:21)

0
σ2
T IT T

IS1S1, ..., σ2
SN

and ΣS is a diagonal block matrix with the diagonal block
elements {σ2
ISN SN }. According to (Ras-
S1
mussen, 2006), if the GP prior is correctly speciﬁed, the
generalization error at a point is also the posterior variance
at such point. Speciﬁcally, for GP-TCM S, the posterior
variance at the target point xt is:
T (xt, λ, {X(Si), σ2
δ2
Si
= ktt − kT
∗ k∗t,
St, kT

∗tC−1
∗t = (λkT
where kT
T t), kSt (kT t) is the vector of co-
i=1 (X(T )) and xt, and ktt is
variances between {X(Si)}N
the prior variance at xt. Wherever it is not misleading,
we will simplify the posterior variance expression using
T (λ, {σ2
δ2
T ). The generalization error for the tar-
Si
get domain can be obtained by averaging eq. (10) over xt:
i=1, XT , σ2
}N
T )

(cid:15)T (λ, {X(Si), σ2
Si

i=1, XT , σ2
}N
T )

i=1, σ2
}N

(10)

(cid:90)

=

T (xt, λ, {σ2
δ2
Si

i=1, σ2
}N

T )p(xt)dxt.

(11)

To derive the generalization error bounds for GP-TCM S,
we ﬁrst rewrite

(cid:20) λ−2(KSS + ΣS )
KT
ST

(cid:20) λISS
0

(cid:21)

0
IT T

KST
KT T + σ2

T IT T

(cid:21)

Λ,

where Λ =

. Thus, the posterior variance

T ) = ktt − kT

t Φ(λ, {σ2
Si

i=1, σ2
}N

T )−1kt,
(12)

at point xt becomes:
i=1, σ2
}N
T (λ, {σ2
δ2
Si

where kT

t = (kT
Φ(λ, {σ2
Si

St, kT
T t) and
}N
i=1, σ2
T )
(cid:20) λ−2(KSS + ΣS )
KT
ST

=

KST
KT T + σ2

T IT T

(cid:21)

.

Note that the above derivation excludes the situation where
λ = 0. When λ = 0, all the source domains are unrelated
to the target domain, and thus no knowledge is transferred.
This is easy to verify by plugging λ = 0 into eq. (12).

Further, we observe that δ2
T is equal for λ and −λ, so we
only investigate the case λ ∈ (0, 1]. For eq. (12), we further
decompose it as:
Φ(λ, {σ2
Si

i=1, σ2
}N
T )
(cid:20) KSS KST
ST KT T

KT

=

(cid:20) σ2

(cid:21)

+

(λ−2 − 1)

= Φ(1, {σ2

(cid:20) KSS + ΣS 0
0
0
T }N , σ2
T , ..., σ2

(cid:21)

+

T ISS
0
(cid:21)

+

0
σ2
T IT T
(cid:20) ΣS − σ2
0

(cid:21)

T ISS 0
0

T ) + E1 + E2,

(13)

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

where E1 = (λ−2 − 1)
(cid:20) ΣS − σ2
0

T ISS 0
0

(cid:21)

(cid:20) KSS + ΣS 0
0
0

(cid:21)

and E2 =

. Eq. (13) unveils that the posterior

variance of having instances from different source domains
is equivalent to the posterior variance of having those in-
stances from target domain with two additional correlated
noise terms, E1 and E2. This shows us the two main fac-
tors that matter in the transfer performance; namely, the
S-T similarity and the noise variances. To further analyze
how the S-T similarity affects the transfer performance, we
focus on one factor and ﬁx the other. Assuming that all the
sources are totally related to the target, i.e. λ = 1, and con-
sequently, the noise variance for each source becomes ξ2
,
Si
we deﬁne the difference:

∆ = δ2

T (1, {ξ2
Si

i=1, σ2
}N

T ) − δ2

T (λ, {σ2
Si

i=1, σ2
}N

T ).

To obtain the upper (lower) bound of δ2
we are interested in those ξ
0) for all the target points.

Si (ξ2
Si

2

T (λ, {σ2
T ),
Si
) that make ∆ ≥ 0 (∆ <

i=1, σ2
}N

Proposition 1. Let δ and δ be the maximum and mini-
2
− (1 − λ−2)δ
mum eigenvalues of KSS , ξ
Si
and ξ2
− (1 − λ−2)δ for every source Si.
= λ−2σ2
Si
Si
T (1, {ξ2
Then, for all the target data points, δ2
T ) ≤
Si
T (λ, {σ2
δ2
i=1, σ2
}N
T ).
Si

= λ−2σ2
Si

T ) ≤ δ2

i=1, σ2
}N

i=1, σ2
}N

T (1, {ξ

2
Si

Proof. By applying eq. (12), we have:

∆ = δ2
= kT

T (1, {ξ2
Si
t [Φ(λ, {σ2
Si

i=1, σ2
}N
i=1, σ2
}N

T ) − δ2
T (λ, {σ2
Si
T )−1 − Φ(1, {ξ2
Si

i=1, σ2
}N
T )
i=1, σ2
}N

T )−1]kt

To make ∆ ≥ 0 for all the target data points, we need
T )−1 is
to prove Φ(λ, {σ2
Si
PSD, which means:

T )−1 − Φ(1, {ξ2
Si

i=1, σ2
}N

i=1, σ2
}N

Φ(λ, {σ2
Si

i=1, σ2
}N

T )−1 − Φ(1, {ξ2
Si

i=1, σ2
}N

T )−1 (cid:23) 0

⇐⇒

⇐⇒

Φ(λ, {σ2
Si
(cid:20) (1 − λ−2)KSS + (Σ(cid:48)

i=1, σ2
}N

T ) (cid:22) Φ(1, {ξ2
Si

i=1, σ2
}N
T )
(cid:21)

S − λ−2ΣS ) 0
0

0

(cid:23) 0

where Σ(cid:48)
block elements {ξ2
S1

S is a diagonal block matrix with the diagonal
IS1S1, ..., ξ2
SN

ISN SN }

⇐⇒

⇐⇒

⇐⇒

(1 − λ−2)KSS + (Σ(cid:48)

S − λ−2ΣS ) (cid:23) 0

KSS (cid:22)

(λ−2ΣS − Σ(cid:48)
(1 − λ−2)

S )

δ ≤

− ξ2
(λ−2σ2
Si
Si
(1 − λ−2)

)

f or every Si

⇐⇒ ξ2
Si

≥ λ−2σ2
Si

− (1 − λ−2)δ f or every Si

Note that ∆ is a monotonically increasing function of ξ2
,
Si
2
− (1 − λ−2)δ as ξ
thus we take the minimum λ−2σ2
Si to
Si
T (λ, {σ2
be the smallest upper bound of σ2
i=1, σ2
}N
T ). Sim-
Si
ilarly, we have ξ2
− (1 − λ−2)δ to construct
= λ−2σ2
Si
Si
T (λ, {σ2
the largest lower bound of σ2
Si

i=1, σ2
}N

T ).

Proposition 1 gives the lower and upper bounds of the pos-
i=1, σ2
terior variance δ2
T (λ, {σ2
}N
T ). By applying eq. (11),
Si
we readily obtain the generalization error bounds.

Corollary 1. Let

(cid:15)T (λ, {σ2
Si
(cid:15)T (λ, {σ2
Si

i=1, σ2
}N
i=1, σ2
}N

2
T ) = (cid:15)T (1, {ξ
Si
T ) = (cid:15)T (1, {ξ2
Si

i=1, σ2
}N
T )
i=1, σ2
}N
T )

Then, (cid:15)T (λ, {σ2
Si
i=1, σ2
(cid:15)T (λ, {σ2
}N
Si

}N
i=1, σ2
T ).

T ) ≤ (cid:15)T (λ, {σ2
Si

i=1, σ2
}N

T ) ≤

Proposition 1 serves to demonstrate that λ takes effect in
every source on the ﬁnal transfer performance. With the
assumption that different source domains have different S-
T similarities with the target domain, a single λ that works
for every sources has a great difﬁculty capturing such S-
T similarity diversity. This leads us to exploit the transfer
covariance function in another way.

5. Transfer Covariance Function Stacking

Considering the effectiveness showed by the stacking strat-
egy for MSTR (Pardoe & Stone, 2010), we propose a frame-
work that can integrate the capability for S-T similarity cap-
ture of both the transfer covariance function and stacking.
We ﬁrst introduce TCSSStack, a conventional way of stack-
ing the transfer covariance function. Then, we design a
more involved stacking-inspired approach that overcomes
some limitations of the conventional stacking method.

5.1. Conventional Transfer Stacking TCSSStack

Motivated by the fact that both the stacking strategy and
the transfer covariance function can model the S-T sim-
ilarity and using the transfer covariance function at the
base models would therefore add ﬂexibility to the simi-
larity capture capability of the stacking approach, we pro-
pose a TCSSStack method.
In TCSSStack, we ﬁrst train
multiple GP-TCSS models using each Si and T (denoted
as {f (Si,T )(·|Ωi, λi)}N
i=1) and then apply the conventional
stacking strategy to combine their predictions. Given a tar-
get point x, the ﬁnal prediction is given by:

f (x) =

(cid:88)N

i=1

ωif (Si,T )(x|Ωi, λi),

ωi = 1 (14)

(cid:88)N

i=1

where ωi are coefﬁcients learned by minimizing the least
square error on the target labeled data.

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

There are two major issues for the above model. (1) Since
each f (Si,T ) is pretrained separately, the parameters learnt
for each f (Si,T ) do not take the inter-domain dependen-
cies between different source domains into account.
(2)
Both λi and ωi reﬂect the S-T domain similarity. How-
ever, TCSSStack takes them as two different variables and
learns them separately. Intuitively, the model importance
ωi should be positively correlated with the similarity coef-
ﬁcient λi. For example, the prediction of a GP-TCSS using
an unrelated source is less trustful, and should be assigned
a smaller coefﬁcient in the stacking.

5.2. Improved Transfer Stacking TCM SStack

To overcome the above issues, we propose a new transfer
stacking model (TCM SStack) as follows:

f ∗(x) =

(cid:88)N

i=1

(g(λi)/Z)f (Si,T )(x, Ωi, λi).

(15)

where λi refers to the similarity coefﬁcient in the GP-TCSS
for the i-th source, Z = (cid:80)N
i=1 g(λi) is the normalization
term, and g(λi) is any function preserving the monotonicity
of |λi| so that it coordinates the model importance and the
similarity coefﬁcient. This also reduces the search efforts
by lowering the number of free parameters to ﬁt. More-
over, instead of pretraining f (Si,T )(·|Ωi, λi) separately, we
jointly learn f (Si,T )(·, Ωi, λi) for all the source domains.
By doing so, the multiple GP-TCSS models are learned
together with the dependencies between multiple sources
taken into account.

Notice that the model in eq. (15) allows for multiple op-
tions to characterize the relative importance of GP-TCSS
models through g(·). In this paper, we use a simple func-
tion g(λi) = |λi|. However, the absolute value function is
not smooth at the origin. Thus, we use a smooth function
studied in (Yong, 2015) to approximate it as follows:

|λi| ≈ αLn(

λi
α +

e

e− λi

α ).

1
2

1
2

We set α = 0.01 which is the best approximation stated in
(Yong, 2015). Since Theorem 1 also tells us −1 ≤ λi ≤ 1,
we propose to deﬁne λi = 2(1/(1 + µi))bi − 1 (µi ≥ 0
and bi ≥ 0), as in (Cao et al., 2010). Then, we conduct the
learning by minimizing the squared errors:

min
{Ωi,µi,bi}N

i=1

(cid:88)nTl
j=1

(y(Tl)

j − f ∗(x(Tl)

j

2

))

.

(16)

In the optimization, we propose to use the conjugate gra-
dient method. Other optimization methods can also be ap-
plied to solve this objective function.

5.3. Complexity Analysis

As in usual GP model training, the computational time
complexity of each f (Si,T ) is dominated by the calculation

Table 1. Amazon review products dataset 15.

Top category
Beauty Health Grocery
Electronics
Home Garden Tool
Movies Music Game

Source domains
Beauty, Grocery Food, Health
Electronic, Ofﬁce Product, Kindle Store
Kitchen, Pet Supplies
CD Vinyl, Digital Music, Video Games

Target domain
Clothing Shoes Jewelry
Cellphone Accessory
Tools Home Improvements
Movies TV

of the inverse of its covariance matrix, i.e. O((nTl +nSi )3).
Considering nTl (cid:28) nSi and assuming nS1 = ... =
nSN = nS , the evaluation of a TCM SStack model takes
then O(N n3
S ). Notice that by following the stacking strat-
egy of eq. (14) the training involves the steps of learning
each f (Si,T ) and subsequently learning the ωi coefﬁcients.
The latter calls for some cross-validation approach to eval-
uate a meta-model, as the f (Si,T ) from the previous step
have been induced using the target data (Pardoe & Stone,
In the extreme case of leave-one-out, this would
2010).
take O(nTl N n3
S ). Even if we also choose a leave-one-out
validation to solve eq. (16) the cost of a TCM SStack model
evaluation would be lower, since the ﬁrst step of stacking
is not required. On the other side, by following TrAd-
aBoost.R2 or the GP-TCM S approach, a GP model eval-
uation requires O((N nS )3), which even exceeds the cost
for TCM SStack using leave-one-out whenever N >
nTl .

√

6. Experimental Study

In the following experimental study we aim at two main
goals: (i) to assess the ability of TCM SStack in capturing
inter-domain similarity, and (ii) to evaluate its predictive
effectiveness compared to other approaches.

6.1. Experiment Setting

All the GPs herein build upon a standard squared expo-
nential covariance function. The hyperparameters of each
method are optimized using the conjugate gradient imple-
mentation from the gpml package (Rasmussen & Nickisch,
2010). For each search, we allow a maximum of 200 eval-
uations. The reported results correspond to the model pro-
viding the best objective function value over 5 independent
runs with random initial solutions each. We use one syn-
thetic dataset and two real-world datasets.

Synthetic dataset. We consider a linear function f (x) =
0 x + (cid:15), where w0 ∈ R100 and (cid:15) is a zero-mean Gaussian
wT
noise term, as the target. We use this function to generate
100 points as target test data, and 20 points as target train
data. For the source task, we use g(x) = (wT
0 +δ∆w)x+(cid:15),
where ∆w is a random ﬂuctuation vector and δ is the vari-
able controlling the similarity between f and g (higher δ
indicates lower similarity), to generate 380 points for each
source with different δ.

Amazon reviews. We extract the raw data containing 15

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

2 Sources

5 Sources

10 Sources

15 Sources

product reviews from (McAuley et al., 2015), and catego-
rize the products into four top categories according to the
Amazon website. Products in the same category are con-
ceptually similar. Each product is taken as a domain, and
we select one as target from each category (see Table 1).
Reviews in each domain are represented by the count fea-
tures and are labeled using stars in the set {1, 2, 3, 4, 5}.

UJIIndoorLoc. The building location dataset covers three
buildings of Universitat Jaume I with four ﬂoors each
(Torres-Sospedra et al., 2014). We build 12 domains by
taking the location data from each ﬂoor of each building as
a domain. The ﬁrst ﬂoor of each building is taken as the
target. Domains from the same building are taken as sim-
ilar. The received signal strength intensity from 520 wire-
less access points is used as the features, and the location
represented by the latitude and longitude is taken as label.

6.2. Domain Similarity Capture

We ﬁrst elucidate the ability of TCM SStack in capturing
the diverse S-T similarities through the λi coefﬁcients. To
rationalize the assessment, we use the synthetic dataset
and consider a variety of problems covering a broad spec-
trum of TL settings. Precisely, we build four scenarios
of N = 2, 5, 10, 15 sources. In each scenario, we spec-
ify six problems, each given by a different combination of
sources. Three problems represent settings in which all the
sources are equally similar to the target, with high (δ = 0),
medium (δ = 15) and low (δ = 35) similarity strength.
The other three problems reﬂect diverse S-T similarities.
Each source is given by a δ randomly sampled from the set
{0, 4, 7, 10, 15, 20, 25, 30, 35} and with replacement. We
enforce the three problems to be different and avoid all the
sources to be equal. We show the results in Figure 1.

In the ﬁgure, the ﬁrst three problems of each scenario are
the cases with equal S-T similarities. It can be observed in
the bar plots on the left hand side that the λ values learnt
by TCM SStack are strictly reverse-correlated with the pre-
deﬁned δ values, which indicates an accurate capture of the
high, medium and low strengths of S-T similarity. We fur-
ther observe from the black dots that GP-TCM S is also able
to strongly coordinate δ with a single compromised λ. This
is because all the sources share the same δ with the target,
and thus can be regarded as a single larger source.

The remaining three problems of each scenario reﬂect di-
verse similarities across source domains. In this case, Fig-
ure 1 shows that the λ values of TCM SStack reﬂect the rel-
ative differences of δ across different sources fairly well
in general. The learnt λ is generally reverse-correlated
with the predeﬁned δ values, but it is not strict and tight.
For instance, in problem 6 of the 5-sources scenario or
in the problem 5 of the 15-sources scenario, such reverse-
correlated relations do not hold in all the sources. This is

Figure 1. Results on six problem settings for each scenario of 2, 5,
10 and 15 sources. Bar plots on the left show the correspondence
between each problem similarity (δ) and the similarity captured
by the model (λ); bars for TCM SStack and black dots for GP-
TCM S. Bar plots on the right show the RMSE for TCM SStack
(red) and GP-TCM S (green).

because, although the learnt λs only represent the bivariate
S-T similarities, each of them is speciﬁed during learning
by considering its inﬂuence relative to the rest of similar-
ity coefﬁcients, i.e. the inter-domain dependencies between
different sources are taken into account during the learning
of the λs. Thus, in some cases, the learnt λs may not strictly
approximate the real S-T similarities. However, λs are al-
ways learnt to guarantee the outcome of the best transfer
performance. That is the reason why the above two cases

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

the different amounts of source domains. This showcases
the capability of TCM SStack to transfer knowledge from
various sources with different S-T similarities.

For the other baselines, we observe that TrAdaBoost.R2
gives the poorest results due to the premature weights con-
vergence issue. If, in addition, we consider the high com-
putational cost for the models involved, TrAdaBoost.R2
does not seem to be a good choice for MSTR, especially
when the number of source domains is large. As for GP-
TCM S, it presents a steadily inferior performance than
TCM SStack. Overall, the outcomes are in line with those
in the synthetic dataset, offering further support to the su-
periority of TCM SStack to GP-TCM S. Notice that, since
the current benchmark was generated randomly, it is likely
a scenario to comprise diverse problem settings. Therefore,
capturing the diverse similarities through a single λ coefﬁ-
cient may compromise the performance of GP-TCM S. As
opposed to GP-TCM S, TCM SStack offers more robust per-
formance improvements.

Finally,
the comparison with the other stacking-based
methods exposes the beneﬁts of the two salient features of
TCM SStack. Both TCSSStack and λ-Stacking are beaten
by TCSSStack-Joint and TCM SStack. Since these two sets
of methods only differ in the joint learning of the param-
eters, the outcomes point at the beneﬁts of bringing in
the inter-domain dependencies of the other sources dur-
ing the learning. On the other side, the results for λ-
Stacking and TCM SStack are better or comparable to those
by TCSSStack and TCSSStack-Joint, repectively. This pro-
vides support to the correlation of the model importance
with the similarity coefﬁcients, which allows to specify
the model by estimating fewer hyper-parameters while pre-
serving the similarity capture capability.

7. Conclusions

We investigate a family of transfer covariance functions
that represent the pairwise similarity between each source
and the target domain for the MSTR problem. We prove
that, GP-TCM S, a Gaussian process with such a transfer
covariance function can only capture the same similarity
coefﬁcient for all the sources. By further analyzing the gen-
eralization errors of GP-TCM S, we conclude the bounds
depend on the single similarity coefﬁcient, which may pe-
nalize the transfer performance. As an alternative, we pro-
pose TCM SStack, an approach that integrates the transfer
covariance function and the stacking strategy into one uni-
ﬁed model. TCM SStack aligns the S-T similarity coef-
ﬁcients with the model importance and jointly learns the
base models. Extensive experiments on one synthetic and
two real-world datasets, with learning settings of up to 11
sources for the latter, show the superiority of TCM SStack
to other MSTR methods.

Figure 2. Comparison results on the two datasets.

still achieve satisfactory transfer performance in terms of
RMSE. By contrast, we ﬁnd that GP-TCM S only gives a
trade-off value of λ over the diverse δ values. The right
hand side of the ﬁgure shows a consistently lower RMSE
for TCM SStack than for GP-TCM S in all the problems. In
particular, a dramatic improvement is observed by utiliz-
ing TCM SStack for the diverse problems 4-6. These results
indicate the superiority of TCM SStack over GP-TCM S for
MSTR. We further verify this conclusion on the two real-
world datasets in the next section.

6.3. Performance on Real-World Datasets

We compare TCM SStack with several MSTR approaches,
namely: Tradaboost.R2, GP-TCM S, TCSSStack, a variant
of TCSSStack with joint learning of model importance co-
efﬁcient and λ which we call TCSSStack-Joint, and a vari-
ant of TCSSStack using the learnt λ as the model impor-
tance which we call λ-Stacking. The evaluation comprises
both the Amazon and the UJIIndoorLoc datasets. For each
source domain we sample 500 points uniformly at random
for training. Likewise, train and test data from each tar-
get domain are obtained by sampling 25 points and 1000
points, respectively. For the Amazon dataset, we gener-
ated a set of problems by using each target domain in Table
1, and by randomly choosing a number of source domains
subsets. More precisely, for each scenario of 2, 3 and 5
sources, ten different source combinations were randomly
constructed. In addition, a scenario of 11 sources with all
the source domains was selected. Thus, we construct 40
transfer problems for each scenario of 2, 3 and 5 sources,
and 4 problems for the scenario of 11 sources. For the UJI-
IndoorLoc dataset, we generate the transfer problems in a
similar way to the Amazon dataset described above.

In Figure 2, we show the average RMSE results over all
the problems in each scenario for the two datasets. Over-
all, TCM SStack is the winner among all the baselines on
the two datasets, improving the transfer performance across

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

Acknowledgments

This work was conducted within the Rolls-Royce@Nan-
yang Technological University Corporate Lab with support
from the National Research Foundation (NRF) Singapore
It is also par-
under the Corp Lab@University Scheme.
tially supported by the School of Computer Science and
Engineering at Nanyang Technological University.

References

Al-Stouhi, Samir and Reddy, Chandan. Adaptive boosting
for transfer learning using dynamic updates. Machine
Learning and Knowledge Discovery in Databases, pp.
60–75, 2011.

Bhatt, Himanshu Sharad, Rajkumar, Arun, and Roy, Shou-
rya. Multi-source iterative adaptation for cross-domain
classiﬁcation. In Proceedings of the Twenty-Fifth Inter-
national Joint Conference on Artiﬁcial Intelligence, pp.
3691–3697. AAAI Press, 2016.

Bonilla, Edwin V., Chai, Kian M., and Williams, Christo-
pher. Multi-task gaussian process prediction. In Platt,
J. C., Koller, D., Singer, Y., and Roweis, S. T. (eds.),
Advances in Neural Information Processing Systems 20,
pp. 153–160. Curran Associates, Inc., 2008.

Cao, Bin, Pan, Sinno Jialin, Zhang, Yu, Yeung, Dit-Yan,
In Pro-
and Yang, Qiang. Adaptive transfer learning.
ceedings of the Twenty-Fourth AAAI Conference on Arti-
ﬁcial Intelligence, AAAI’10, pp. 407–712. AAAI Press,
2010.

Chai, Kian M. Generalization errors and learning curves
for regression with multi-task gaussian processes.
In
Advances in neural information processing systems, pp.
279–287, 2009.

Dai, Wenyuan, Yang, Qiang, Xue, Gui-Rong, and Yu,
Yong. Boosting for transfer learning. In Proceedings of
the 24th International Conference on Machine Learning,
ICML ’07, pp. 193–200, New York, NY, USA, 2007.
ACM. ISBN 978-1-59593-793-3.

Fang, Min, Guo, Yong, Zhang, Xiaosong, and Li, Xiao.
Multi-source transfer learning based on label shared sub-
space. Pattern Recognition Letters, 51:101–106, 2015.

Ghosh, Sayan, Jacobs, Ryan, and Mavris, Dimitri N. Multi-
source surrogate modeling with bayesian hierarchical re-
gression. In 17th AIAA Non-Deterministic Approaches
Conference, pp. 1817–1829, 2015.

Lam, Kari Y, Westrick, Zachary M, M¨uller, Christian L,
Christiaen, Lionel, and Bonneau, Richard. Fused regres-
sion for multi-source gene regulatory network inference.
PLOS Computational Biology, 12:1–23, 2016.

Luo, Ping, Zhuang, Fuzhen, Xiong, Hui, Xiong, Yuhong,
and He, Qing. Transfer learning from multiple source
domains via consensus regularization. In Proceedings of
the 17th ACM conference on Information and knowledge
management, pp. 103–112. ACM, 2008.

McAuley, Julian, Targett, Christopher, Shi, Qinfeng, and
van den Hengel, Anton. Image-based recommendations
In Proceedings of the 38th
on styles and substitutes.
International ACM SIGIR Conference on Research and
Development in Information Retrieval, pp. 43–52. ACM,
2015.

Pan, Sinno Jialin and Yang, Qiang. A survey on trans-
fer learning. Knowledge and Data Engineering, IEEE
Transactions on, 22(10):1345–1359, 2010.

Pan, Sinno Jialin, Kwok, James T, and Yang, Qiang. Trans-
fer learning via dimensionality reduction. In AAAI, vol-
ume 8, pp. 677–682, 2008.

Pan, Sinno Jialin, Tsang, Ivor W, Kwok, James T, and
Yang, Qiang. Domain adaptation via transfer compo-
nent analysis. IEEE Transactions on Neural Networks,
22(2):199–210, 2011.

Pardoe, David and Stone, Peter. Boosting for regression
transfer. In Frnkranz, Johannes and Joachims, Thorsten
(eds.), Proceedings of the 27th International Conference
on Machine Learning (ICML-10), pp. 863–870. Omni-
press, 2010.

Rasmussen, Carl Edward. Gaussian processes for machine

learning. Citeseer, 2006.

Rasmussen, Carl Edward and Nickisch, Hannes. Gaussian
processes for machine learning (gpml) toolbox. Journal
of Machine Learning Research, 11:3011–3015, Decem-
ber 2010. ISSN 1532-4435.

Tommasi, Tatiana, Orabona, Francesco, and Caputo, Bar-
bara. Learning categories from few examples with multi
IEEE transactions on pat-
model knowledge transfer.
tern analysis and machine intelligence, 36(5):928–941,
2014.

Torres-Sospedra, Joaqu´ın, Montoliu, Ra´ul, Us´o, Ado-
lfo Mart´ınez, Avariento, Joan P., Arnau, Tomas J.,
Benedito-Bordonau, Mauri, and Huerta, Joaqu´ın. Ujiin-
doorloc: A new multi-building and multi-ﬂoor database
for wlan ﬁngerprint-based indoor localization problems.
In International Conference on Indoor Positioning and
Indoor Navigation, pp. 261–270. IEEE, 2014.

Wang, Qifan, Ruan, Lingyun, and Si, Luo. Adaptive
knowledge transfer for multiple instance learning in im-
age classiﬁcation. In AAAI, pp. 1334–1340, 2014.

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

Wei, Pengfei, Ke, Yiping, and Goh, Chi Keong. Deep non-
linear feature coding for unsupervised domain adapta-
tion. In In Proceedings of the Twenty-Fifth International
Joint Conference on Artiﬁcial Intelligence, pp. 2189–
2195. AAAI Press, 2016.

Williams, Christopher, Klanke, Stefan, Vijayakumar, Se-
thu, and Chai, Kian M. Multi-task gaussian process
learning of robot inverse dynamics. In Advances in Neu-
ral Information Processing Systems, pp. 265–272, 2009.

Wolpert, David H. Stacked generalization. Neural Net-
works, 5(2):241–259, February 1992. ISSN 0893-6080.

Yao, Yi and Doretto, Gianfranco. Boosting for transfer
learning with multiple sources. In Computer vision and
pattern recognition (CVPR), 2010 IEEE conference on,
pp. 1855–1862. IEEE, 2010.

Yong, Longquan. Uniform smooth approximation func-
tions for absolute value function. Mathematics in prac-
tice and theory, pp. 250–255, 2015.

Zhang, Fuzhen. The Schur complement and its applica-
tions, volume 4. Springer Science & Business Media,
2006.

Zhou, Joey Tianyi, Pan, Sinno Jialin, Tsang, Ivor W, and
Ho, Shen-Shyang. Transfer learning for cross-language
text categorization through active correspondences con-
struction. In AAAI, pp. 2400–2406, 2016.

