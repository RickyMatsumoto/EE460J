Robust Structured Estimation with Single-Index Models

Sheng Chen 1 Arindam Banerjee 1

Abstract
In this paper, we investigate general single-index
models (SIMs) in high dimensions. Based on
U -statistics, we propose two types of robust es-
timators for the recovery of model parameters,
which can be viewed as generalizations of sev-
eral existing algorithms for one-bit compressed
sensing (1-bit CS). With minimal assumption on
noise, the statistical guarantees are established
for the generalized estimators under suitable con-
ditions, which allow general structures of under-
lying parameter. Moreover, the proposed estima-
tor is novelly instantiated for SIMs with mono-
tone transfer function, and the obtained estima-
tor can better leverage the monotonicity. Exper-
imental results are provided to support our theo-
retical analyses.

1. Introduction

In machine learning and statistics, a linear model of the for-
m y = ⟨(cid:18)∗, x⟩+ϵ is widely used to ﬁnd the relationship be-
tween feature and response, which has gained overwhelm-
ing popularity for a very long time. Here y ∈ R and x ∈ Rp
is the pair of observed response and feature/measurement
vector, ϵ is a zero-mean noise, and (cid:18)∗ ∈ Rp is the un-
known parameter to be estimated. The simplicity of linear
model leads to its great interpretability and computational
efﬁciency, which are often favored in practical application-
s. On theoretical side, even in high-dimensional regime
where sample size is smaller than the problem dimension
p, strong statistical guarantees have been established un-
der mild assumptions for various estimators, such as Las-
so (Tibshirani, 1996) and Dantzig selector (Candes & Tao,
2007). Despite its attractive merits, one main drawback
of linear models is the stringent assumption of linear rela-
tionship between x and y, which may fail to hold in com-

1Department of Computer Science & Engineering, Univer-
sity of Minnesota-Twin Cities, Minnesota, USA. Correspon-
dence to: Sheng Chen <shengc@cs.umn.edu>, Arindam Baner-
jee <banerjee@cs.umn.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

plicated scenarios. To introduce more ﬂexibility, one op-
tion is to consider the general single-index models (SIMs)
(Ichimura, 1993; Horowitz & Hardle, 1996),

E[y|x] = f ∗(⟨(cid:18)∗, x⟩) ,

(1)

where f ∗ : R 7→ R is an unknown univariate transfer func-
tion (a.k.a. link function). This class of models enjoys rich
modeling power in the sense that it encompasses several
useful models as special cases, which are brieﬂy described
below:

• One-bit Compressed Sensing:

In one-bit com-
pressed sensing (1-bit CS) (Boufounos & Baraniuk,
2008; Plan & Vershynin, 2013), the response y is re-
stricted to be binary, i.e., y ∈ {+1, −1}, and the range
of transfer function f ∗ is [−1, 1]. Given the measure-
ment vector x, one can generate y from the Bernoulli
model,

(

)

y + 1
2

∼ Ber

f ∗(⟨(cid:18)∗, x⟩) + 1
2

.

(2)

In the noiseless case, f ∗(z) = sign(z) and y always
reﬂects the true sign of ⟨(cid:18)∗, x⟩, while y can be incor-
rect for other f ∗ whose shape determines the noise
level in some way.

• Generalized Linear Models: In generalized linear
models (GLMs) (McCullagh, 1984), the transfer func-
tion is assumed to be monotonically increasing and
conditional distribution of y|x belongs to exponen-
tial family. Different choices of f ∗ give rise to dif-
ferent members in GLMs. If f ∗ is identity function
f ∗(z) = z, one has the simple linear models, while
the sigmoid function f ∗(z) = 1
1+e(cid:0)z results in the l-
ogistic model for binary classiﬁcation. In this work,
however, we have no access to exact f ∗ other than
knowing it is monotonic.

• Noise in Monotone Transfer: Instead of having the
general expectation form of y as GLMs, one could di-
rectly introduce the noise inside monotone transfer ~f
to model the randomness of y (Plan et al., 2016),

y = ~f (⟨(cid:18)∗, x⟩ + ϵ) .

(3)

In this setting, the transfer function ~f is slightly differ-
ent from the f ∗ in (1), which are related by f ∗(z) =
Eϵ[ ~f (z + ϵ)|z].

Robust Structured Estimation with Single-Index Models

A key advantage of SIM is its robustness. First, allowing
unknown f ∗ prevents the mis-speciﬁcation of transfer func-
tion, which could otherwise lead to a poor estimate of (cid:18)∗.
Secondly, the model in (1) makes minimal assumption on
the distribution of y, thus being able to tolerate potentially
heavy-tailed noise.
In order to estimate (cid:18)∗, we are given n measurements of
(x, y) ∈ Rp × R, denoted by {(xi, yi)}n
i=1. In this work,
we focus on the n < p regime. In such high-dimensional
the recovery of (cid:18)∗ is quite challenging as the
setting,
problem is ill-posed even when f ∗ is given. Over the last
decade, substantial progress has been made to address the
challenge by exploiting the apriori structure of parameter
(cid:18)∗, like sparsity (Tibshirani, 1996). For simple linear
models or GLMs with known transfer, extensive studies
have shown that sparse (cid:18)∗ can be consistently estimated
under mild assumptions, with much lower sample com-
plexity than p (Candes & Tao, 2007; Wainwright, 2009;
Bickel et al., 2009; Kakade et al., 2010; Negahban et al.,
2012; Yang et al., 2016). Recently the notion of structure
has been suitably generalized beyond the unstructured
sparsity (Bach et al., 2012), and Gaussian width (Gordon,
1985) has emerged as a useful measure to characterize
the structural complexity which further determines the
recovery guarantee of (cid:18)∗ (Chandrasekaran et al., 2012;
Rao et al., 2012; Oymak et al., 2013; Amelunxen et al.,
2014; Banerjee et al.,
2014;
Vershynin, 2015; Tropp, 2015; Chen & Banerjee, 2016).
In the absence of exact f ∗,
though 1-bit CS and
related variants were well-studied in recent years
(Boufounos & Baraniuk,
2013;
2008;
Plan & Vershynin, 2013; Gopi et al., 2013; Zhang et al.,
2014; Chen & Banerjee, 2015a; Zhu & Gu, 2015; Yi et al.,
2015; Slawski & Li, 2015; Li, 2016; Slawski & Li,
2016), the exploration of general SIMs or the cases with
monotone transfers is relatively limited, especially in
the high-dimensional regime. Kalai & Sastry (2009) and
investigated the low-dimensional
(2011)
Kakade et al.
SIMs with monotone transfers,
and they proposed
perceptron-type algorithms to estimate both f ∗ and (cid:18)∗,
with provable guarantees on prediction error.
In high
dimension, general SIMs were studied by Alquier & Biau
(2013) and Radchenko (2015), in which only unstructured
sparsity of (cid:18)∗ is considered. The algorithm developed
relies on reversible jump
in (Alquier & Biau, 2013)
MCMC, which could be slow.
In Radchenko (2015), a
path ﬁtting algorithm is designed to recover f ∗ and (cid:18)∗,
but only asymptotic guarantees are provided. Ganti et al.
(2015) considered the high-dimensional setting with
monotone transfer, and their iterative algorithm is based
is hard to
on non-convex optimization,
establish the convergence. Besides, the prediction error
bound they derived is also weak (in the sense that it

2014; Chatterjee et al.,

Jacques et al.,

for which it

is even worse than the initialization of the algorithm).
(2016) proposed a
Recently Oymak & Soltanolkotabi
constrained least-squares method to estimate (cid:18)∗, with
recovery error characterized by Gaussian width and related
quantities. Though their analysis considered the general
structure of (cid:18)∗, it only holds for noiseless setting where
y = f (⟨(cid:18)∗, x⟩). General structure of (cid:18)∗ was also explored
in Vershynin (2015) and Plan et al. (2016). Other types of
statistical guarantees for high-dimensional SIMs is also
available, such as support recovery of (cid:18)∗ in Neykov et al.
(2016). It is worth noting that all the aforementioned sta-
tistical analyses rely on sub-Gaussian noise or the transfer
function being bounded or Lipschitz, which indicates that
none of the results can immediately hold for heavy-tailed
noise (or without Lipschitzness and boundedness).
In this paper, we focus on the parameter estimation of (cid:18)∗
instead of the prediction of y given new x. In particular, we
propose two families of generalized estimators, constrained
and regularized, for model (1) under Gaussian measure-
ment. The parameter (cid:18)∗ is assumed to possess certain low-
complexity structure, which can be either captured by a
constraint (cid:18)∗ ∈ K or a norm regularization term ∥(cid:18)∗∥.
Our general approach is inspired by U -statistics and the ad-
vances in 1-bit CS, and subsumes several existing 1-bit CS
algorithms as special cases. Similar to those algorithms,
our estimator is simple and often admits closed-form solu-
tions. Regarding the recovery analysis, there are two ap-
pealing aspects. First our results work for general struc-
ture, with error bound characterized by Gaussian width and
some other easy-to-compute geometric measures. Instanti-
ating our results with speciﬁc structure of (cid:18)∗ recovers pre-
viously established error bounds for 1-bit CS (Zhang et al.,
2014; Chen & Banerjee, 2015a), which are sharper than
those yielded by the general analysis in Plan & Vershynin
(2013). Second, our analysis works with limited assump-
tions on the condition distribution of y. In particular, our
estimator is robust to heavy-tailed noise and permit un-
bounded transfer functions f ∗ as well as non-Lipschitz
ones. At the heart of our analysis is the generic chaining
method (Talagrand, 2014), an advanced tool in probability
theory, which has been successfully applied to sparse re-
covery (Koltchinskii, 2011) and dimensionality reduction
(Dirksen, 2016), etc. Another key ingredient in our proof is
a Hoeffding-type concentration inequality for U -statistics
(Lee, 1990) with sub-Gaussian tails, which is less known
yet generalizes the popular one for bounded U -statistics
(Hoeffding, 1963). Apart from 1-bit CS, we particularly
investigate the model (3), for which the generalized esti-
mator is specialized in a novel way. The resulting estimator
better leverages the monotonicity of the transfer function,
which is also demonstrated through experiments. For the
ease of exposition, whenever we say “monotone”, it mean-
s “monotonically increasing” by default. Throughout the

Robust Structured Estimation with Single-Index Models

paper, we will use c, C, C ′, C0, C1 and so on to denote ab-
solute constants, which may differ from context to context.
Detailed proofs are deferred to the supplementary material
due to page limit.

The rest of the paper is organized as follows. In Section
2, we introduce our estimators for SIMs along with their
recovery guarantees. We also provide a few examples in
1-bit CS for illustration. Section 3 is focused on model (3),
for which we instantiate the general results in a new way.
Other structures of (cid:18)∗ beyond unstructured sparsity are also
discussed. Section 4 provides the proof of our main result-
s and the related lemmas.
In Section 5, we complement
our theoretical developments with some experiment result-
s. The ﬁnal section is dedicated to conclusions.

2. Generalized Estimation for Structured

Parameter

2.1. Assumptions and Preliminaries

For the sake of identiﬁability, we assume w.l.o.g.
that
∥(cid:18)∗∥2 = 1 throughout the paper. At the ﬁrst glimpse
of model (1), we may realize that it is difﬁcult to recov-
er (cid:18)∗ due to unknown f ∗.
In contrast, when f ∗ is giv-
en, the recovery guarantees of (cid:18)∗ can be established under
mild assumptions of x and y, such as boundedness or sub-
Gaussianity. If we know certain properties of the transfer
function like the monotonicity introduced in GLMs and (3),
the structure of f ∗ is largely restricted, and it is tempting
to expect that similar results will continue to hold. Un-
fortunately, we ﬁrst have the following claim, which in-
dicates that without other constraints on f ∗ beyond strict
monotonicity, (cid:18)∗ cannot be consistently estimated under
general sub-Gaussian (or bounded) measurement, even in
the noiseless setting of (3).

Claim 1 Suppose that each element xi of x is sampled
from Rademacher distribution, i.e., P(xi = 1) =
i.i.d.
P(xi = −1) = 0.5. Under model (3) with noise ϵ = 0,
there exists a (cid:22)(cid:18) ∈ Sp−1 together with a monotone (cid:22)f , such
that supp( (cid:22)(cid:18)) = supp((cid:18)∗) and yi = (cid:22)f (⟨ (cid:22)(cid:18), xi⟩) for da-
ta {(xi, yi)}n
i=1 with arbitrarily large sample size n, while
∥ (cid:22)(cid:18) − (cid:18)∗∥2 > δ for some constant δ.

Now that consistent estimation of (cid:18)∗ is not possible for
general sub-Gaussian measurement, it might be reasonable
to focus on certain special cases. For this work, we assume
that x is standard Gaussian N (0, I). For SIM (1), we ad-
ditionally assume that the distribution of y depends on x
only through the value of ⟨(cid:18)∗, x⟩, i.e., the distribution of
y|x is ﬁxed if ⟨(cid:18)∗, x⟩ is given (no matter what the exact x
is). This assumption is quite minimal, and it turns out that
the examples we provide in Section 1 all satisfy it (if noise
ϵ is independent of x in (3)). The same assumption is used

in Plan et al. (2016) as well.

Under the assumptions above, given m i.i.d. observations
(x1, y1), . . . , (xm, ym), we deﬁne

m∑

i=1

u ((x1, y1), . . . , (xm, ym)) =

qi (y1, . . . , ym) · xi ,

(4)
where all qi : Rm 7→ R are bounded functions with |qi| ≤
1, which are chosen along with m based on the properties
of the transfer function. In Section 2.4 and 3.1, we will see
examples for their choices. The vector u ∈ Rp is critical
due to the key observation below.

Lemma 1 Suppose the distribution of y in model (1) de-
pends on x through ⟨(cid:18)∗, x⟩ and we deﬁne accordingly

bi (z1, . . . , zm; (cid:18)∗) =
(5)
E [qi (y1, . . . , ym) |⟨(cid:18)∗, x1⟩ = z1, . . . , ⟨(cid:18)∗, xm⟩ = zm] .
With x being standard Gaussian N (0, I), u deﬁned in (4)
satisﬁes

E [u ((x1, y1), . . . , (xm, ym))] = β(cid:18)∗ ,

(6)

where β =
g1, . . . , gm are i.i.d. standard Gaussian.

E[bi (g1, . . . , gm; (cid:18)∗) · gi], and

∑

m
i=1

Note that Lemma 1 is true for all choices of qi, and the
proof is given in the supplement. This lemma presents an
insight towards the design of our estimator, that is, the di-
rection of (cid:18)∗ can be approximated if we have a good sense
about Eu. As we will see in the sequel, the scalar β plays
a key role in the estimation error bound, which can give us
clues to the choice of qi. We can assume w.l.o.g. that β ≥ 0
since we can ﬂip the sign of each qi.

The recovery analysis is built on the notion of Gaussian
width (Gordon, 1985), which is deﬁned for any A ⊆ Rp
as w(A) = E[supv∈A⟨g, v⟩], where g is a standard Gaus-
sian random vector. Roughly speaking, w(A) measures the
scaled width of set A averaged over each direction.

2.2. Generalized Estimator

Inspired by Lemma 1, we deﬁne the vector ^u for the ob-
served data {(xi, yi)}n
∑

i=1,

u ((xi1 , yi1), . . . , (xim, yim )) ,

^u =

(n − m)!
n!

1≤i1,...,im≤n
i1̸=...̸=im

(7)
which is an unbiased estimator of Eu, meaning that E^u =
Eu = β(cid:18)∗. When m = 2, we essentially have

^u =

1
n(n − 1)

∑

1≤i,j≤n
i̸=j

u ((xi, yi), (xj, yj))

(8)

Robust Structured Estimation with Single-Index Models

i.e., ^(cid:18) = ^u/∥^u∥2.

In fact, ^u can be treated as a vector version of U -statistics
with order m. Given ^u, a naive way to estimate (cid:18)∗ is
to simply normalize ^u,
In high-
dimensional setting, (cid:18)∗ is often structured, but the naive
estimator fails to take such information into account, which
would lead to large error. To incorporate the prior knowl-
edge on (cid:18)∗, we design two types of estimator, the con-
strained one and the regularized one.
Constrained Estimator: If we assume that (cid:18)∗ belongs to
some structured set K ⊆ Sp−1, then the estimation of (cid:18)∗ is
carried out via the constrained optimization

^(cid:18) = argmin
(cid:18)∈Rp

− ⟨^u, (cid:18)⟩

s.t. (cid:18) ∈ K .

(9)

Here the set K can be non-convex, as long as the optimiza-
tion can be solved globally. Since the objective function is
very simple, we can often end up with a global minimizer.
Similar estimator has been used in Plan et al. (2016), but
they only focused on speciﬁc ^u.

Regularized Estimator: If we assume that the structure of
(cid:18)∗ can be captured by certain norm ∥ · ∥, we may alterna-
tively use the regularized estimator to ﬁnd (cid:18)∗,

^(cid:18) = argmin
(cid:18)∈Rp

− ⟨^u, (cid:18)⟩ + λ∥(cid:18)∥ s.t.

∥(cid:18)∥2 ≤ 1 .

(10)

The optimization is convex, thus the global minimum is
always attained. Previously this estimator was used in 1-bit
CS scenario with L1 norm (Zhang et al., 2014).

2.3. Recovery Analysis
Regarding the constrained estimator, the recovery of (cid:18)∗ re-
lies on the geometry of ^(cid:18), which is described by
(cid:12)
(cid:12)
(cid:12) v = ^(cid:18) − (cid:18)∗, ^(cid:18) ∈ K

AK((cid:18)∗) = cone

} ∩

Sp−1

{

v

(11)
The set AK((cid:18)∗) essentially contains all possible direction-
s that error ^(cid:18) − (cid:18)∗ could lie in. The following theorem
characterizes the error of ^(cid:18).

Theorem 1 Suppose that
the optimization (9) can be
solved to global minimum. Then the following error bound
holds for the minimizer ^(cid:18) with probability at least 1 −
C ′′ exp

(
−w2 (AK((cid:18)∗))

)
,

(cid:13)
(cid:13)
(cid:13) ^(cid:18) − (cid:18)∗

(cid:13)
(cid:13)
(cid:13)

≤ Cκm 3
β

2

· w(AK((cid:18)∗)) + C ′
√
n

,

2

(12)

where κ is the sub-Gaussian norm of a standard Gaussian
random variable, and C, C ′, C ′′ are all absolute constant.

which implies that we should construct suitable qi such
that β is large according to its deﬁnition in Lemma 1. The
choice of qi further depends on the assumed property of
f ∗. Though dependency on m may prevent us from using
higher-order u, m is typically small in practice and can be
treated as constant.

For regularized estimator, we can similarly establish the re-
covery guarantee in terms of Gaussian width.

Theorem 2 Deﬁne the following set for any ρ > 1,

Aρ ((cid:18)∗) = cone

{

(cid:12)
(cid:12)
(cid:12) ∥v + (cid:18)∗∥ ≤ ∥(cid:18)∗∥ +

v

} ∩

∥v∥
ρ

Sp−1

√

(

If we set λ = ρ ∥^u − β(cid:18)∗∥∗ = O(ρm3/2w(B∥·∥)/
n)
and it satisﬁes λ < ∥^u∥∗, then with probability at least
1 − C ′ exp
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) ^(cid:18) − (cid:18)∗
(cid:13)
2

(
−w2
B∥·∥
≤ C(1 + ρ)κm 3
β

, ^(cid:18) in (10) satisﬁes

(cid:9) (Aρ((cid:18)∗)) · w

(
B∥·∥

))

√

n

)

·

,

2

where (cid:9) (Aρ((cid:18)∗)) = supv∈A(cid:26)((cid:18)(cid:3))
{v | ∥v∥ ≤ 1} is the unit ball of norm ∥ · ∥.

(13)
∥v∥ and B∥·∥ =

Remark: The geometry of the regularized estimator is s-
lightly different from the constrained one. Instead of hav-
ing AK((cid:18)∗), here the set Aρ((cid:18)∗) depends on the choice
of the regularization parameter λ. The same phenomenon
also appears in the (Banerjee et al., 2014). The geometric
measure (cid:9) (Aρ((cid:18)∗)) is called restricted norm compatibil-
ity, which is non-random. For many interesting cases, it is
easy to calculate (Negahban et al., 2012; Chen & Banerjee,
2015b).

2.4. Application to 1-bit CS

For 1-bit CS problem (2), the u deﬁned in (4) can be chosen
with m = 1 and qi = yi, ending up with

u ((x, y)) = yx and

^u =

yixi .

(14)

1
n

n∑

i=1

By such choice of u, the β deﬁned in Lemma 1 is sim-
ply β = E[f ∗(g)g] with g being standard Gaussian ran-
dom vector. Under reasonably mild noise, y is likely to
take the sign of the linear measurement, which means that
f ∗(g) should be close to 1 (or -1) if g is positive (or neg-
ative). Thus we expect f ∗(g)g to be positive most of time
and β to be large. Given the choice of u, we can spe-
cialize our generalized constrained/regularized estimator to
obtain previous results. If (cid:18)∗ is assumed to be s-sparse,
for constrained estimator, we can choose a straightforward
K = {(cid:18) | ∥(cid:18)∥0 ≤ s} ∩ Sp−1, which results in the k-support
norm estimator (Chen & Banerjee, 2015a),

Remark: Note that estimator is consistent as long as β ̸=
0. The error bound inversely depends on the scale of β,

^(cid:18)ks = argmin
(cid:18)∈Rp

− ⟨^u, (cid:18)⟩ s.t. ∥(cid:18)∥0 ≤ s, ∥(cid:18)∥2 = 1 (15)

Robust Structured Estimation with Single-Index Models

Though K is non-convex, the global minimizer can actually
be obtained in closed form,

{

^θks
j =

^uj / ∥|^u|↓
∥2 ,
0 , otherwise

1:s

if |^uj| is in |^u|↓

1:s

(16)

where |^u|↓ is the absolute-value counterpart of ^u with en-
tries sorted in descending order, and the subscript takes the
top s entries. Similarly if the regularized estimator is in-
stantiated with L1 norm ∥ · ∥1, we obtain the so-called pas-
sive algorithm introduced in Zhang et al. (2014),

^(cid:18)ps = argmin
(cid:18)∈Rp

− ⟨^u, (cid:18)⟩ + λ∥(cid:18)∥1 s.t. ∥(cid:18)∥2 ≤ 1 ,

(17)

whose solution is given by ^(cid:18)ps = S (^u, λ) /∥S (^u, λ) ∥2,
where S(·, ·) is the elementwise soft-thresholding operator,
Si(^u, λ) = max{sign(^ui)(|^ui|−λ), 0}. Based on Theorem
1 and 2, we can easily obtain the error bound for both k-
support norm estimator and passive algorithm.
Corollary 1 Assume that {(xi, yi)}n
i=1 follow 1-bit CS
model in (2) and ^u is given as (14). For any s-sparse (cid:18)∗,
with high probability, ^(cid:18) produced by both (15) and (17)
(i.e., ^(cid:18)ks and ^(cid:18)ps) satisfy

(cid:13)
(cid:13)
(cid:13) ^(cid:18) − (cid:18)∗

(cid:13)
(cid:13)
(cid:13)
2

≤ O

(√

)

s log p
n

The proof is included in the supplementary material.
The above result was shown by Slawski & Li (2015) and
Zhang et al. (2014), but their analyses do not consider the
√
general structure. Compared with O( 4
s log p/n) yield-
ed by the general result in Plan & Vershynin (2013), our
bound is much sharper.

3. A New Estimator for Monotone Transfer

In this section, we speciﬁcally study model (3). Here we
further assume that ~f is strictly increasing. What is worth
mentioning is that the estimator we develop here can be
applied to GLMs as well. To avoid the confusion with u
and ^u deﬁned previously, we instead use new notations h
and ^h respectively in this section.

3.1. Estimator with Second-Order ^h

Put in another way, r needs to satisfy the constraint that
ri > rj iff. yi > yj and ri < rj iff. yi < yj. To
move one step further, it is equivalent to sign(yi − yj) =
sign(ri −rj) = sign(⟨(cid:18)∗, xi −xj⟩+ϵi −ϵj) based on mod-
el assumption. Hence the information contained in sample
{(xi, yi)}n
i=1 can be interpreted from the perspective of 1-
bit CS, where sign(yi − yj) reﬂects the perturbed sign of
linear measurement ⟨(cid:18)∗, xi − xj⟩. Inspired by the u for
1-bit CS, we may choose m = 2 and deﬁne h, ^h as

h ((x1, y1), (x2, y2)) = sign(y1 − y2) · (x1 − x2) , (20)

h ((xi, yi), (xj, yj)) ,

(21)

^h =

1
n(n − 1)

∑

1≤i,j≤n
i̸=j

Given the deﬁnition of ^h, Lemma 1 directly implies the
following corollary.

Corollary 2 Suppose that (x1, y2) and (x2, y2) are gener-
ated by model (3), where x1, x2 follow Gaussian distribu-
tion N (0, I), and the noise ϵ1, ϵ2 are independent of x1, x2
and identically (but arbitrarily) distributed. Then the ex-
pectation of h ((x1, y1), (x2, y2)) satisﬁes

(18)

E [h ((x1, y1), (x2, y2))] =
[
sign

(

g + (ϵ1 − ϵ2)/

2β′(cid:18)∗ ,
√
)
2

]
.

· g

where β′ = Eg∼N (0,1)

(22)

√

√

2β′ serves as the role of β in Lem-
Remark: The scalar
ma 1, and β′ is always guaranteed to be strictly positive
regardless how the noise is distributed, which keeps (cid:18)∗ dis-
tinguishable all the time. To see this, let ξ = (ϵ1 − ϵ2)/
2.
Note that ξ is symmetric, thus εξ has the same distribution
as ξ, where ε is a Rademacher random variable. Therefore

√

β′ = E [sign (g + ξ) · g] = Eg,ξEε [sign (g + εξ) · g]
sign (g − ξ) + sign (g + ξ)
2

= EξEg

· g

]

[

Since g(g − ξ) + g(g + ξ) = 2g2 ≥ 0, it follows that
sign(g(g − ξ)) + sign(g(g + ξ)) = (sign(g − ξ) + sign(g +
ξ)) · sign(g) ≥ 0, thus (sign(g − ξ) + sign(g + ξ)) · g is
always nonnegative. Find a large enough M > 0 such that
P(|ξ| ≤ M ) = 0.5 > 0, and we have

To motivate the design of h, it is helpful to rewrite model
(3) by applying the inverse of ~f on both sides,

β′ = E [sign (g + ξ) · g] ≥ EξEg [|g| · I{|g| > |ξ|}]

~f −1(y) = ⟨(cid:18)∗, x⟩ + ϵ .

(19)

≥ 0.5Eg [|g| · I{|g| > M }] =

· P(|g| > M ) > 0 .

M
2

Note that the new formulation resembles the linear model
except that we have no access to the value of ~f −1(y). In-
stead, all we know about r = [ ~f −1(y1), . . . , ~f −1(yn)]T ∈
Rn is that it preserves the ordering of y = [y1, . . . , yn]T .

In the ideal noiseless case, β′ achieve its maximum,
β′
max = E[sign(g)g] = E[|g|] =
In the worst
case, if ϵ1 and ϵ2 are heavy-tailed and dominate g, then
√
)
β′ ≈ E
2

[
sign

(ϵ1 − ϵ2)/

≈ 0.

2/π.

· g

√

(

]

Robust Structured Estimation with Single-Index Models

Now we can instantiate the generalized estimator based on
^h. For example, if (cid:18)∗ is s-sparse, we estimate it by
− ⟨^h, (cid:18)⟩ s.t. ∥(cid:18)∥0 ≤ s, ∥(cid:18)∥2 = 1
(√

^(cid:18) = argmin
(cid:18)∈Rp

(23)

)

s log p/n

which enjoys O
error rate as shown in
Corollary 1. The regularized estimator can also be obtained
with the same ^h according to (17). The bottleneck of com-
puting ^(cid:18) lies in the calculation of ^h. A simple proposition
below enables us to get ^h in a fast manner.

Proposition 1 Given {(xi, yi)}n
tion of {1, . . . , n} such that yπ
we have

#
1

i=1, let π↓ be the permuta-
> . . . > yπ
> yπ
. Then

#
n

#
2

^h =

2
n(n − 1)

n∑

i=1

(n + 1 − 2i) · xπ

#
i

(24)

Remark: Based on the proposition above, ^h can be efﬁ-
ciently computed in O(np + n log n) time, i.e., O(n log n)
time for sorting y and O(np) time for the weighted sum of
all xi. This is a signiﬁcant improvement compared with the
the naive calculation using (21), which takes O(n2p) time.

3.2. Beyond Unstructured Sparsity

So far we have illustrated the Gaussian width based er-
ror bounds, viz (12) and (13), only through unstructured
sparsity of (cid:18)∗. Here we provide two more examples, non-
overlapping group sparsity and fused sparsity.

Non-Overlapping Group Sparsity: Suppose the coordi-
nates of (cid:18)∗ has been partitioned into K predeﬁned disjoint
groups G1, . . . , GK ⊆ {1, 2, . . . , p}, out of which only k
groups are non-zero. If we use the regularized estimator
∑
∥2, the optimal so-
with L2,1 norm ∥(cid:18)∥2,1 =
lution can be similarly obtained as (17), with elementwise
soft-thresholding replaced by the groupwise one. The re-
lated geometric measures that appears in (13) can be found
in Banerjee et al. (2014), which are given by

∥(cid:18)Gi

K
i=1

(cid:9) (Aρ((cid:18)∗)) ≤ O(
≤ O(

√

(
B∥·∥2;1

)

√

k)

√

w

G)

log K +

(26)
Fused Sparsity: (cid:18)∗ is said to be s-fused-sparse if the car-
dinality of the set F((cid:18)∗) = {1 ≤ i < p | θ∗
}
i
is smaller than s. If we resort to the constrained estimator
(9) with K = {(cid:18) | |F((cid:18))| ≤ s, ∥(cid:18)∥2 = 1}, the associ-
ated optimization can be solved by dynamic programming
(Bellman, 1961). The proposition below upper bounds the
corresponding Gaussian width w(AK((cid:18)∗)) in (12).

̸= θ∗

i+1

Proposition 2 For s-fused-sparse (cid:18)∗, the Gaussian width
of set AK((cid:18)∗) with K = {(cid:18) | |F ((cid:18))| ≤ s, ∥(cid:18)∥2 = 1}
satisﬁes

√

w(AK((cid:18)∗)) ≤ O(

s log p)

(27)

The proof can be found in (Slawski & Li, 2016), and we
provide a different one in supplementary material.

4. Lemmas and Proof Sketch of Theorem 1

Here we ﬁrst present the important technical lemmas that
will be used in the proof of Theorem 1. The ﬁrst one is the
Hoeffding-type inequality for sub-Gaussian U -statistics.
In the literature, most of the studies are centered around
bounded U -statistics, for which the celebrated concentra-
tion is established by Hoeffding (1963). Yet it is not easy
to locate the counterpart for sub-Gaussian case. Therefore
we provide the following result and attach a proof in the
supplementary material.

Lemma 2 (Concentration for sub-Gaussian U -statistics)
Deﬁne the U -statistic

Un,m(h) =

(n − m)!
n!

∑

1≤i1,...,im≤n
i1̸=i2̸=...̸=im

h (zi1, . . . , zim) (28)

with order m and kernel h : Rd×m 7→ R based on n in-
dependent copies of random vector z ∈ Rd, denoted by
z1, · · · , zn. If h(·, . . . , ·) is sub-Gaussian with ∥h∥ψ2
≤ κ,
then the following inequality holds for Un,m(h) with any
δ > 0,

P (|Un,m(h) − EUn,m(h)| > δ) ≤ 2 exp

−C

in which C is an absolute constant.

(

⌊

⌋

n
m

)

,

· δ2
κ2
(29)

As mentioned earlier in Section 1, generic chaining is the
key tool that our analysis relies on. Speciﬁcally we utilize
Theorem 2.2.27 from (Talagrand, 2014).

Lemma 3 (Generic chaining concentration) Given met-
ric space (T , s),
if an associated stochastic process
{Zt}t∈T has sub-Gaussian incremental, i.e., satisﬁes the
condition

(25)

P (|Zu − Zv| ≥ δ) ≤ C exp

, ∀ u, v ∈ T ,

)

(
− C ′δ2
s2(u, v)

then the following inequality holds

(

P

sup
u,v∈T

|Zu − Zv| ≥ C1 (γ2(T , s) + δ · diam (T , s))

≤ C2 exp

)

(
−δ2

,

(30)

)

(31)

where C, C ′, C1 and C2 are all absolute constants.

The deﬁnition of the above γ2-functional γ2(·, ·) is compli-
cated, and is not of great importance. We refer interested

Robust Structured Estimation with Single-Index Models

reader to the books, Talagrand (2005; 2014). Loosely s-
peaking, γ2(T , s) can be thought of as a measure for the
size of set T under metric s. What really matters is the
following relationship between γ2-functional and Gaussian
width. (see Theorem 2.4.1 in Talagrand (2014))

Lemma 4 (Majorizing measures theorem) For any set
T ⊆ Rp, the γ2-functional w.r.t. L2-metric and Gaussian
width satisfy the following inequality with an absolute con-
stant C0,

γ2 (T , ∥ · ∥2) ≤ C0 · w(T )

(32)

Equipped with these lemmas, we are ready to present the
proof sketch of Theorem 1. A complete proof is deferred
to the supplementary material.

Proof Sketch of Theorem 1: We use the shorthand nota-
tion AK for the set AK((cid:18)∗). As ^(cid:18) attains the global mini-
mum of (9), we have

⟨

⟩

⟨ ^(cid:18) − (cid:18)∗, ^u⟩ ≥ 0 ⇐⇒

^(cid:18) − (cid:18)∗,

− (cid:18)∗ + (cid:18)∗

≥ 0

^u
β

=⇒ ⟨ ^(cid:18), (cid:18)∗⟩ ≥ 1 − ∥ ^(cid:18) − (cid:18)∗∥2 ·

⟨

⟩

v,

^u
β

− (cid:18)∗

sup
v∈AK∪{0}

In order to bound the supremum above, we use the re-
sult from generic chaining. We deﬁne the stochastic
process {Zv = ⟨v, ^u/β − (cid:18)∗⟩}v∈AK∪{0}. First, we
need to check the process has sub-Gaussian incremental.
For simplicity, we denote u ((xi1 , yi1 ), . . . , (xim, yim)) by
ui1,...,im . By the deﬁnitions and properties of sub-Gaussian
norm (Vershynin, 2012), it is not difﬁcult to show that
∥⟨ui1,...,im, v − w⟩∥ψ2
≤ κm · ∥v − w∥2 for any v, w ∈
AK ∪ {0}. By Lemma 2, we have

P (|Zv − Zw| > δ) ≤ 2 exp

−C ′ ·

(

nβ2δ2
m3κ2 · ∥v − w∥2
2

)

.

Therefore we can conclude that {Zv} has sub-Gaussian
the metric s(v, w) , κm 3
2 · ∥v −
incremental w.r.t.
√
w∥2/β
n. Now applying Lemma 3 to {Zv} with a bit
calculation, we can obtain

(

P

sup
v∈AK∪{0}

2

√

|Zv| ≥ C1κm 3
n
β
))
(

(

)

+ 2δ

≤ C2 exp

−δ2

·

γ2 (AK ∪ {0}, ∥ · ∥2)

Using γ2 (AK ∪ {0}, ∥ · ∥2) ≤ C0 · w (AK ∪ {0}) implied
by Lemma 4 and taking δ = w (AK ∪ {0}), we get
⟩

⟨

sup
v∈AK∪{0}

v,

^u
β

− (cid:18)∗

≤ C3κm 3
β

2

· w (AK) + C4
√
n
)
−w2 (AK)

(

with probability at least 1 − C2 exp
. The in-
equality also uses the fact that w (AK ∪ {0}) ≤ w (AK) +

C4, which is a result of Lemma 2 in Maurer et al. (2014)
(See Lemma A in supplementary material). Lastly we turn
to the quantity ∥ ^(cid:18) − (cid:18)∗∥2,

√

∥ ^(cid:18) − (cid:18)∗∥2 ≤

2 − 2⟨ ^(cid:18), (cid:18)∗⟩ ≤ 2C3κm 3

2

· w (AK) + C4
√
n

.

β

We ﬁnish the proof by letting C = 2C3, C ′ = C4 and
C ′′ = C2.

5. Experiment
In the experiment, we focus on model (3) with sparse (cid:18)∗.
The problem dimension is ﬁxed as p = 1000, and the s-
parsity of (cid:18)∗ is set to 10. Essentially we generate our data
(x, y) from

y = ~f (⟨(cid:18)∗, x⟩ + ϵ) ,

where x ∼ N (0, I) and ϵ ∼ N (0, σ2). σ ranges from
0.3 to 1.5. We choose three monotonically increasing ~f ,
~f (z) = 1/(1 + exp(−z)) (which is bounded and Lips-
chitz), ~f (z) = z3 (which is unbounded and non-Lipschitz),
and ~f (z) = log(1 + exp(z)) (which is unbounded but
Lipschitz). The sample size n varies from 200 to 1000.
We use the estimator (23) in Section 3. The baselines
we compare with is the SILO and iSILO algorithm intro-
duced in (Ganti et al., 2015). SILO does not quite take
the monotonicity in account.
In fact, it is the special
case of our generalized constrained estimator which uses
the same choice of u as 1-bit CS. The original SILO use
the constraint set {(cid:18) | ∥(cid:18)∥1 ≤
s, ∥(cid:18)∥2 ≤ 1}, which
is computationally less efﬁcient and statistically no better
than K = {(cid:18) | ∥(cid:18)∥0 ≤ s} ∩ Sp−1 (Zhang et al., 2014;
Chen & Banerjee, 2015a). Hence we also use K in SILO
for a fair comparison. iSILO relies on a speciﬁc implemen-
tation of isotonic regression which explicitly restricts the
Lipschitz constant of ~f to be one. To ﬁt iSILO into our
setting, we remove the Lipschitzness constraint and perfor-
m the standard isotonic regression. Since the convergence
is not guaranteed for the iterative procedure of iSILO, the
number of its iterations is ﬁxed to 100. The best tuning
parameter of iSILO is obtained by grid search.

√

The experiment results are shown in Figure 1. Overall
the iSILO algorithm works well under small noise, while
our estimator has better performance when the variance of
noise increases. To better demonstrate the robustness of
our estimator to heavy-tailed noise, instead of Gaussian
noise, we sample ϵ from the Student’s t distribution with
degrees of freedom equal to 3. We repeat the experiments
for ~f (z) = z3, and obtain the plots in Figure 2. We can see
that the error of our estimator consistently decreases for all
choice of σ as n increases. For SILO and iSILO, the errors
are relatively large, and unable to shrink for large σ even
when more data are provided.

Robust Structured Estimation with Single-Index Models

(a) Error for ~f (z) = 1=(1 + exp((cid:0)z))

(b) Error for ~f (z) = log(1 + exp(z))

(c) Error for ~f (z) = z3

Figure1. Recovery error vs. sample size (a) Our estimator has similar performance compared with iSILO, both of which outperform
SILO by a large margin. (b) iSILO has smaller error when (cid:27) is small, while our estimator works better in high-noise regime (c) The
error of SILO is reduced compared with other ~f , but iSILO fails to give further improvement over SILO when (cid:27) is large. Our estimator
still outperforms them when (cid:27) (cid:21) 0:6.

Figure2. Recovery error vs. sample size, with ~f (z) = z3 under heavy-tailed noise

6. Conclusion

In this paper, we study the parameter estimation for the
high-dimensional single-index models. We propose two
classes of robust estimators, which generalize previous
works in two aspects. First we allow the diverse structure
(e.g., binary, monotone and etc.) of the transfer function,
which can help us customize the estimators. Secondly the
structure of the true parameter can be general, either en-
coded by a constraint or a norm. With limited assump-
tion on the noise, we can show that the estimation error can
be bounded by simple geometric measures under Gaussian

measurement, which subsumes the existing results for spe-
ciﬁc settings. The experiment results also validate our the-
oretical analyses.

Acknowledgements
We thank Sreangsu Acharyya for helpful discussions re-
lated to the paper. The research was supported by NS-
F grants IIS-1563950, IIS-1447566, IIS-1447574, IIS-
1422557, CCF-1451986, CNS- 1314560, IIS-0953274,
IIS-1029711, NASA grant NNX12AQ39A, and gifts from
Adobe, IBM, and Yahoo.

2004006008001000n00.511.5Recoveryerrorσ = 0.32004006008001000n00.511.5σ = 0.62004006008001000n00.511.5σ = 0.92004006008001000n00.511.5σ = 1.22004006008001000n00.511.5σ = 1.5our estimatorSILOiSILO2004006008001000n00.511.5Recoveryerrorσ = 0.32004006008001000n00.511.5σ = 0.62004006008001000n00.511.5σ = 0.92004006008001000n00.511.5σ = 1.22004006008001000n00.511.5σ = 1.5our estimatorSILOiSILO2004006008001000n00.511.5Recoveryerrorσ = 0.32004006008001000n00.511.5σ = 0.62004006008001000n00.511.5σ = 0.92004006008001000n00.511.5σ = 1.22004006008001000n00.511.5σ = 1.5our estimatorSILOiSILO2004006008001000n00.511.5Recoveryerrorσ = 0.32004006008001000n00.511.5σ = 0.62004006008001000n00.511.5σ = 0.92004006008001000n00.511.5σ = 1.22004006008001000n00.511.5σ = 1.5our estimatorSILOiSILORobust Structured Estimation with Single-Index Models

References

Alquier, P. and Biau, G. Sparse single-index model. Jour-
nal of Machine Learning Research, 14:243–280, 2013.

Amelunxen, D., Lotz, M., McCoy, M. B., and Tropp, J. A.
Living on the edge: Phase transitions in convex program-
Inform. Inference, 3(3):224–294,
s with random data.
2014.

Bach, F., Jenatton, R., Mairal, J., and Obozinski, G. Opti-
mization with sparsity-inducing penalties. Foundations
and Trends R⃝ in Machine Learning, 4(1):1–106, 2012.

Banerjee, A., Chen, S., Fazayeli, F., and Sivakumar, V. Es-
timation with norm regularization. In Advances in Neu-
ral Information Processing Systems (NIPS), 2014.

Bellman, R. On the approximation of curves by line seg-
ments using dynamic programming. Communications of
the ACM, 4(6):284, 1961.

Ganti, R., Rao, N., Willett, R. M, and Nowak, R. Learning
single index models in high dimensions. arXiv preprint
arXiv:1506.08910, 2015.

Gopi, S., Netrapalli, P., Jain, P., and Nori, A. One-bit com-
pressed sensing: Provable support and vector recovery.
In Proceedings of The 30th International Conference on
Machine Learning, 2013.

Gordon, Y. Some inequalities for gaussian processes and
applications. Israel Journal of Mathematics, 50(4):265–
289, 1985.

Hoeffding, W. Probability inequalities for sums of bounded
random variables. Journal of the American statistical
association, 58(301):13–30, 1963.

Horowitz, J. L. and Hardle, W. Direct semiparametric
estimation of single-index models with discrete covari-
ates. Journal of the American Statistical Association, 91
(436):1632–1640, 1996.

Bickel, P. J., Ritov, Y., and Tsybakov, A. B. Simultaneous
analysis of Lasso and Dantzig selector. The Annals of
Statistics, 37(4):1705–1732, 2009.

Ichimura, H.

Semiparametric least squares (sls) and
weighted sls estimation of single-index models. Journal
of Econometrics, 58:71–120, 1993.

Boufounos, P. T and Baraniuk, R. G. 1-bit compressive
In Information Sciences and Systems, 2008.

sensing.
CISS 2008. 42nd Annual Conference on, 2008.

Candes, E. and Tao, T. The Dantzig selector: statistical
estimation when p is much larger than n. The Annals of
Statistics, 35(6):2313–2351, 2007.

Chandrasekaran, V., Recht, B., Parrilo, P. A., and Willsky,
A. S. The convex geometry of linear inverse problems.
Foundations of Computational Mathematics, 12(6):805–
849, 2012.

Chatterjee, S., Chen, S., and Banerjee, A. Generalized
dantzig selector: Application to the k-support norm.
In Advances in Neural Information Processing System-
s (NIPS), 2014.

Chen, S. and Banerjee, A. One-bit compressed sensing
with the k-support norm. In International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS), 2015a.

Chen, S. and Banerjee, A. Structured estimation with atom-
ic norms: General bounds and applications. In Advances
in Neural Information Processing Systems, 2015b.

Chen, S. and Banerjee, A. Structured matrix recovery via
the generalized dantzig selector. In Advances in Neural
Information Processing Systems, 2016.

Dirksen, S. Dimensionality reduction with subgaussian
matrices: A uniﬁed theory. Foundations of Computa-
tional Mathematics, 16(5):1367–1396, 2016.

Jacques, L., Laska, J. N, Boufounos, P. T, and Baraniuk,
R. G. Robust 1-bit compressive sensing via binary sta-
ble embeddings of sparse vectors. IEEE Transactions on
Information Theory, 59(4):2082–2102, 2013.

Kakade, S., Shamir, O., Sindharan, K., and Tewari,
A. Learning exponential families in high-dimensions:
In Proceedings of the
Strong convexity and sparsity.
Thirteenth International Conference on Artiﬁcial Intel-
ligence and Statistics, 2010.

Kakade, S. M, Kanade, V., Shamir, O., and Kalai, A. Ef-
ﬁcient learning of generalized linear and single index
models with isotonic regression. In Advances in Neural
Information Processing Systems, 2011.

Kalai, A. T and Sastry, R. The isotron algorithm: High-

dimensional isotonic regression. In COLT, 2009.

Koltchinskii, V. Oracle Inequalities in Empirical Risk Min-
imization and Sparse Recovery Problems. Lecture Notes
in Mathematics. Springer Berlin Heidelberg, 2011.

Lee, A. J. U-Statistics: Theory and Practice. Taylor &

Francis, 1990.

Li, P. One scan 1-bit compressed sensing. In Proceedings
of the 19th International Conference on Artiﬁcial Intel-
ligence and Statistics, 2016.

Maurer, A., Pontil, M., and Romera-Paredes, B. An In-
equality with Applications to Structured Sparsity and
Multitask Dictionary Learning. In Conference on Learn-
ing Theory (COLT), 2014.

Robust Structured Estimation with Single-Index Models

Vershynin, R. Introduction to the non-asymptotic analysis
of random matrices. In Eldar, Y. and Kutyniok, G. (ed-
s.), Compressed Sensing, chapter 5, pp. 210–268. Cam-
bridge University Press, 2012.

Vershynin, R. Estimation in High Dimensions: A Geomet-
ric Perspective, pp. 3–66. Springer International Pub-
lishing, 2015.

Wainwright, M. J. Sharp thresholds for noisy and high-
dimensional recovery of sparsity using ℓ1-constrained
IEEE Transactions on
quadratic programming(Lasso).
Information Theory, 55:2183–2202, 2009.

Yang, Z., Wang, Z., Liu, H., Eldar, Y. C., and Zhang, T.
Sparse nonlinear regression: Parameter estimation under
nonconvexity. In Proceedings of the 33nd International
Conference on Machine Learning, 2016.

Yi, X., Wang, Z., Caramanis, C., and Liu, H. Optimal linear
estimation under unknown nonlinear transform. In Ad-
vances in Neural Information Processing Systems, 2015.

Zhang, L., Yi, J., and Jin, R. Efﬁcient algorithms for robust
one-bit compressive sensing. In Proceedings of the 31st
International Conference on Machine Learning (ICML-
14), 2014.

Zhu, R. and Gu, Q. Towards a Lower Sample Complexity
In Proceed-
for Robust One-bit Compressed Sensing.
ings of the 32nd International Conference on Machine
Learning, 2015.

McCullagh, P. Generalized linear models. European Jour-
nal of Operational Research, 16(3):285–292, 1984.

Negahban, S., Ravikumar, P., Wainwright, M. J., and Yu,
B. A uniﬁed framework for the analysis of regularized
M -estimators. Statistical Science, 27(4):538–557, 2012.

Neykov, M., Liu, J. S., and Cai, T. L1-regularized least
squares for support recovery of high dimensional single
index models with gaussian designs. J. Mach. Learn.
Res., 17(1):2976–3012, 2016.

Oymak, S. and Soltanolkotabi, M. Fast and reliable pa-
rameter estimation from nonlinear observations. arXiv
preprint arXiv:1610.07108, 2016.

Oymak, S., Thrampoulidis, C., and Hassibi, B.

The
squared-error of generalized lasso: A precise analysis.
In Communication, Control, and Computing (Allerton),
2013 51st Annual Allerton Conference on, 2013.

Plan, Y. and Vershynin, R. Robust 1-bit compressed sens-
ing and sparse logistic regression: A convex program-
ming approach. IEEE Transactions on Information The-
ory, 59(1):482–494, 2013.

Plan, Y., Vershynin, R., and Yudovina, E.

High-
dimensional estimation with geometric constraints. In-
formation and Inference, 2016.

Radchenko, P. High dimensional single index models.
Journal of Multivariate Analysis, 139:266–282, 2015.

Rao, N., Recht, B., and Nowak, R. Universal Measure-
ment Bounds for Structured Sparse Signal Recovery. In
International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), 2012.

Slawski, M. and Li, P. b-bit marginal regression. In Ad-
vances in Neural Information Processing Systems, 2015.

Slawski, M. and Li, P. Linear signal recovery from b-
bit-quantized linear measurements: precise analysis of
the trade-off between bit depth and number of measure-
ments. arXiv preprint arXiv:1607.02649, 2016.

Talagrand, M. The Generic Chaining. Springer, 2005.

Talagrand, M. Upper and Lower Bounds for Stochastic

Processes. Springer, 2014.

Tibshirani, R. Regression shrinkage and selection via the
Lasso. Journal of the Royal Statistical Society, Series B,
58(1):267–288, 1996.

Tropp, J. A. Convex recovery of a structured signal from
independent random linear measurements. In Sampling
Theory, a Renaissance. 2015.

