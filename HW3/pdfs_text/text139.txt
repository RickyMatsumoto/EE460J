Toward Efﬁcient and Accurate Covariance Matrix
Estimation on Compressed Data

Xixian Chen 1 2 Michael R. Lyu 1 2 Irwin King 1 2

Abstract
Estimating covariance matrices is a fundamen-
tal technique in various domains, most notably
in machine learning and signal processing. To
tackle the challenges of extensive communica-
tion costs, large storage capacity requirements,
and high processing time complexity when han-
dling massive high-dimensional and distributed
data, we propose an efﬁcient and accurate co-
variance matrix estimation method via data com-
pression. In contrast to previous data-oblivious
compression schemes, we leverage a data-aware
weighted sampling method to construct
low-
dimensional data for such estimation. We rig-
orously prove that our proposed estimator is un-
biased and requires smaller data to achieve the
same accuracy with specially designed sampling
distributions. Besides, we depict that the compu-
tational procedures in our algorithm are efﬁcient.
All achievements imply an improved tradeoff be-
tween the estimation accuracy and computational
costs. Finally, the extensive experiments on syn-
thetic and real-world datasets validate the supe-
rior property of our method and illustrate that it
signiﬁcantly outperforms the state-of-the-art al-
gorithms.

1. Introduction

Covariance matrices play a fundamental role in machine
learning and statistics owing to their capability to retain the
second-order information of data samples (Feller, 1966).
For example, Principal Component Analysis (PCA) along

1Shenzhen Research Institute, The Chinese Univer-
2Department of
sity of Hong Kong, Shenzhen, China.
Computer Science and Engineering, The Chinese Univer-
sity of Hong Kong, Shatin, N.T., Hong Kong.
Corre-
Xixian Chen <xxchen@cse.cuhk.edu.hk>,
spondence to:
Michael R. Lyu <lyu@cse.cuhk.edu.hk>,
Irwin King
<king@cse.cuhk.edu.hk>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

with its extensions (Zou et al., 2006), Linear Discrimi-
nant Analysis (LDA), and Quadratic Discriminant Analysis
(QDA) (Anzai, 2012) are powerful for dimension reduction
and denoising, which require the estimation of a covariance
matrix from a given collection of data points. Other promi-
nent examples include Generalized Least Squares (GLS)
regression that requires the estimation of the noise covari-
ance matrix (Kariya & Kurata, 2004), Independent Compo-
nent Analysis (ICA) that relies on pre-whitening based on
the covariance matrix (Hyv¨arinen et al., 2004), and Gen-
eralized Method of Moments (GMM) (Hansen, 1982) that
improves the effectiveness by a precise covariance matrix.

Many practical applications also rely on covariance ma-
trix directly (Bartz, 2016). In biology, gene relevance net-
works and gene association networks are straightforwardly
inferred from the covariance matrix (Butte et al., 2000;
Sch¨afer & Strimmer, 2005). In modern wireless commu-
nications, protocols optimize the bandwidth based on co-
variance estimates (Tulino & Verd´u, 2004). In array sig-
nal processing, the capon beamformer linearly combines
the sensors to minimize the noise in the signal, which is
closely related to the portfolio optimization on covariance
matrices (Abrahamsson et al., 2007). For policy learning
in the ﬁeld of robotics, it requires reliable estimates of the
covariance matrix between policy parameters (Deisenroth
et al., 2013).

Calculation of a covariance matrix usually requires enor-
mous computational resources in the form of communica-
tion and storage because large and high-dimensional data
are now routinely gathered at an exploding rate from many
distributed remote sites, such as sensor networks, surveil-
lance, and distributed databases (Haupt et al., 2008; Shi
et al., 2014; Ha & Barber, 2015). In particular, high com-
munication cost of transmitting the distributed data from
the remote sites to the fusion center (i.e., a destination to
conduct complex data analysis tasks) will require tremen-
dous bandwidth and power consumption (Srisooksai et al.,
2012; Abbasi-Daresari & Abouei, 2016). Formally, given
a data matrix X ∈ Rd×n with d features and n instances
collected from the remote sites, the covariance matrix is
computed in the fusion center by C (cid:44) 1
n XXT − ¯x¯xT ,
i=1 xi ∈ Rd (Feller, 1966). For simplicity
where ¯x = 1
n

(cid:80)n

Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data

of discussion, we temporarily assume the empirical mean is
zero, i.e., ¯x = 0. The covariance matrix can be written as
C = 1
n XXT consequently (Azizyan et al., 2015). Then, it
takes O(nd) communication burden to transmit data from
numerous remote sites to the fusion center to form the full
data set X, O(nd) storage in total to store X in remote
sites, and O(nd+d2) storage with O(nd2) time to calculate
C in the fusion center. When n, d (cid:29) 1, the overall cost is
prohibitively expensive for practical scenarios like wireless
sensors which have narrow transmission bandwidth, lim-
ited storage, and low power supply.

To tackle such computational challenges, compressed data
can be leveraged to estimate the covariance matrix, which
essentially has roots in compressed sensing. One solution
is to process each data point by multiplying it with a sin-
gle projection matrix S ∈ Rd×m whose entry follows the
Gaussian distribution N (0, 1
m ) (Mahoney, 2011). Thus,
storing ST X and the estimated covariance matrix requires
O(mn + d2) space in total, sending ST X to the fusion cen-
ter incurs a O(mn) communication cost, and calculating
ST X and the covariance matrix estimator 1
n SST XXT SST
takes O(mdn + m2n + m2d + md2) time. This method
substantially reduces all computational costs if m (cid:28) n, d.
Note that synchronizing only a seed between remote sites
and the fusion center allows pseudo-random number gen-
erators to reconstruct an identical S, which avoids sending
S directly and imposes a negligible computational burden.

However, the example solution has two critical drawbacks.
The ﬁrst is that the operations on the Gaussian matrix is
inefﬁcient. One could use a sparse projection matrix (Li
et al., 2006), structured matrix (Ailon & Chazelle, 2009) or
sampling matrix (Drineas et al., 2006b) to achieve a better
tradeoff between computational cost and estimation preci-
sion. The second problem is that applying a single projec-
tion matrix to all data points cannot consistently estimate
the covariance matrix, i.e., the estimator cannot converge
to the actual covariance matrix even if the sample size n
grows to inﬁnity with d ﬁxed. This issue is demonstrated
both theoretically and empirically in (Azizyan et al., 2015)
and also brieﬂy described in (Gleichman & Eldar, 2011;
Anaraki & Hughes, 2014; Anaraki & Becker, 2017).

In this paper, we thus adopt n distinct projection matrices
for n data vectors (Azizyan et al., 2015; Anaraki & Hughes,
2014; Anaraki & Becker, 2017; Anaraki, 2016) to achieve
consistent covariance matrix estimation, and construct a
speciﬁc sampling matrix to increase both its efﬁciency and
accuracy. On the whole, we do not make statistical assump-
tions on the distributed data X ∈ Rd×n with n, d (cid:29) 1,
nor do we impose structural assumptions on the covariance
matrix C such as being low-rank or sparse. Our goal is to
compress data and recover C efﬁciently and accurately, and
the contributions in our work are summarized as follows:

• First, in contrast to all existing methods (Azizyan
et al., 2015; Anaraki & Hughes, 2014; Anaraki &
Becker, 2017; Anaraki, 2016) that are based on data-
oblivious projection matrices, we propose to estimate
the covariance matrix based on the data compressed
by a weighted sampling scheme. This strategy is data-
aware with a capacity to explore the most important
entries. Hence, we require considerably fewer entries
to achieve an equal estimation accuracy.

• Second, we provide error analysis for the derived un-
biased covariance estimator, which rigorously demon-
strates that our method can compress data to a much
smaller volume than other methods. The proofs also
indicate our probability distribution is speciﬁcally de-
signed to render a covariance matrix estimation based
on the compressed data as accurate as possible.

• Third, we specify our method by an efﬁcient algorithm
whose computational complexity is superior to other
methods. By additionally considering the best tradeoff
between the estimation accuracy and the compression
ratio, our algorithm ultimately incurs a signiﬁcantly
lower computational cost than the other methods.

• Finally, we validate our method on both synthetic and
real-world datasets, which demonstrates a better per-
formance than the other methods.

The remainder of this paper is organized as follows. In Sec-
tion 2, we review the prior work. In Section 3, we present
our method along with theoretical analysis and emphasize
its achievements. In Section 4, we provide extensive empir-
ical results, and in Section 5 we conclude the whole work.

2. Related Work

i Si)−1(ST

i xi). Because Si(ST

There have been several investigations of ways to achieve
accurate covariance matrix estimation from the low-
dimensional compressed observations constructed by ap-
i=1 ∈ Rd×m
plying a distinct projection matrix {Si}n
i=1 ∈ Rd.
to each data vector {xi}n
The work
(Qi & Hughes, 2012) adopts a Gaussian matrix
of
to compress data via ST
i xi, and recovers them by
Si(ST
is a strictly
m-dimensional orthogonal projection drawn uniformly
at random,
it can capture the information of all en-
tries in each data vector uniformly and substantively.
Then, 1
i up to
n
a known scaling factor is expected to constitute accurate
and consistent covariance matrix estimation. This estima-
tor can be modiﬁed to an unbiased one, and its error analy-
sis is thoroughly provided in (Azizyan et al., 2015). How-
ever, a Gaussian matrix is dense and unstructured, which
imposes an extra computational burden. Also, many ma-
trix inversions take a considerable amount of time, and

i Si)−1ST

i Si)−1ST

i Si)−1ST
i

i=1 Si(ST

i Si(ST

i xixT

(cid:80)n

Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data

(cid:80)n

i xixT

i SiST
i

i=1 SiST

i xi ∈ Rd because SiST
i

the whole square matrix has to be loaded into the mem-
ory. Biased estimator 1
is thus pro-
n
posed in (Anaraki & Hughes, 2014) to improve the ef-
ﬁciency by avoiding matrix inversions and assigning Si
to be a sparse matrix. This method is less accurate be-
cause SiST
i approximates only an m-dimensional random
orthogonal projection. Its another disadvantage is that the
result only holds for data samples under statistical assump-
tions. Based on (Anaraki & Hughes, 2014), another study
proposes an unbiased estimator (Anaraki, 2016), but it still
adopts an unstructured sparse matrix that is insufﬁciently
computation-efﬁcient and fails to provide the error bounds
to characterize the estimation error versus the compression
ratio. Recently, sampling matrices Si ∈ Rd×m constructed
via uniform sampling without replacement have been em-
ployed (Anaraki & Becker, 2017). This approach is efﬁ-
cient, but it only results in poor accuracy if data are com-
pressed directly by ST
is an m-
dimensional orthogonal projection drawn only from d de-
terministic orthogonal spaces/coordinates, and the d − m
entries of each vector are removed. To avoid sacriﬁc-
ing much accuracy, use of the computationally efﬁcient
Hadamard matrix (Tropp, 2011) before sampling has also
been proposed in (Anaraki & Becker, 2017). It ﬂattens out
whole entries, particularly those with large magnitudes, to
all coordinates to ensure that poor uniform sampling with a
small sampling size still obtains some information among
all entries. However, the Hadamard matrix involves de-
terministic orthogonal projection and is unable to capture
the information uniformly in all coordinates of each vector,
which results in the need for numerous samples to achieve
sufﬁcient accuracy. (Anaraki & Becker, 2017) constitutes
the current state of the art in the tradeoff between the es-
timation accuracy and computational efﬁciency. Through-
out the paper, we group the foregoing representative meth-
ods into Gauss-Inverse (Azizyan et al., 2015; Qi & Hughes,
2012), Sparse (Anaraki & Hughes, 2014; Anaraki, 2016),
and UniSample-HD (Anaraki & Becker, 2017), and the un-
biased estimators produced by these methods are adopted
in the subsequent theoretical and empirical comparisons.

A number of other methods have been proposed to recover
covariance matrix from compressed data (Chen et al., 2013;
Bioucas-Dias et al., 2014; Dasarathy et al., 2015; Cai et al.,
2015). These methods are only applicable to low-rank,
sparse, or statistically-assumed covariance matrices.

Interesting work has also been done in the area of low-rank
matrix approximation via randomized techniques. In addi-
tion to simply embedding the data X into space spanned
by a single random projection matrix S, a representative
study (Halko et al., 2011) improves approximation accu-
racy by replacing the random projection matrix S with a
low-dimensional data-aware matrix XS(cid:48), where S(cid:48) is a ran-
dom projection matrix. However, X has to be low-rank,

and computing XS(cid:48) requires one extra pass through all en-
tries in X. It is not suitable for our settings, where we do
not impose structural assumptions on the covariance ma-
trix, nor do we fully observe all data. Moreover, (Azizyan
et al., 2015) demonstrates both theoretically and empiri-
cally that a single projection matrix for all data points can-
not consistently and accurately estimate the covariance ma-
trix. The problem also exist in (Wu et al., 2016; Mroueh
et al., 2016) aiming for a fast approximation of matrix prod-
ucts in a single pass, which only results in an inconsistent
covariance matrix estimation and suits the low-rank case.

Among randomized techniques, it is also worth brieﬂy
discussing sampling approaches in matrix approximation.
Literature in (Drineas et al., 2006a; Papailiopoulos et al.,
2014; Woodruff, 2014; Holodnak & Ipsen, 2015) proposes
to leverage column sampling in which the sampling proba-
bilities in the sampling matrix are either the column norms
or leverage scores. Other work (Woodruff, 2014; Achliop-
tas & Mcsherry, 2007; Achlioptas et al., 2013) performs
element-wise sampling on the entire matrix based on the
relative magnitudes over all data entries. These researches
employ different sampling distributions to sample entries
in a matrix. However, they have to observe all data fully
to calculate the sampling distributions, which also requires
one or more extra passes. In addition, their sampling proba-
bilities are designed for matrix approximation, which can-
not be trivially extended to covariance matrix estimation
because the exact covariance matrix in our setting cannot be
calculated in advance. Note that although the uniform sam-
pling in matrix approximation is a simple one-pass algo-
rithm, it performs poorly on many problems because usu-
ally there exists structural non-uniformity in the data which
has been veriﬁed in (Anaraki & Becker, 2017).

3. Our Approach

In this section, we ﬁrst introduce the deﬁnition and back-
ground to our overall work. We then justify and present our
method of data compression and covariance matrix estima-
tion, followed by the primary results and analysis.

3.1. Preliminaries

Let [k] denote a set of integers {1, 2, . . . , k}. Given a ma-
trix X ∈ Rd×n, for j ∈ [d], i ∈ [n], we let xi ∈ Rd denote
the i-th column of X, and xji denote the (j, i)-th element of
X or j-th element of xi. Let {Xt}k
t=1 denote the set of ma-
trices {X1, X2, . . . , Xk}, and xji,t denote the (j, i)-th ele-
ment of Xt. Let XT denote the transpose of X, and Tr(X)
denote its trace. Let |x| denote the absolute value of x. Let
(cid:107)X(cid:107)2 and (cid:107)X(cid:107)F denote the spectral norm and Frobenius
norm of X, respectively. Let (cid:107)x(cid:107)q = ((cid:80)d
j=1 |xj|q)1/q for
q ≥ 1 be the (cid:96)q-norm of x ∈ Rd. Let D(x) be a square

Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data

diagonal matrix with the elements of vector x on the main
diagonal, and D(X) also be a square diagonal matrix whose
main diagonal has only the main diagonal elements of X.

3.2. Method and Algorithm

As discussed previously, Gauss-Inverse (Azizyan et al.,
2015; Qi & Hughes, 2012) and Sparse (Anaraki & Hughes,
2014; Anaraki, 2016) suffer from deﬁciencies in either
computational efﬁciency or estimation accuracy, whereas
UniSample-HD (Anaraki & Becker, 2017) is less accurate
but offers a good tradeoff between estimation accuracy and
computational efﬁciency. We thus propose the adoption of
i=1 ∈ Rd×m to compress
weighted sampling matrices {Si}n
data via ST
i xi and then back-project the compressed data
into the original space via SiST
i xi. The recovered data
is then used for covariance matrix estimation as shown in
Eq. (1). Hence, a high computational efﬁciency is main-
tained. Although Si removes at least d−m entries from the
i-th vector, the remainders can be the most informative and
are retained. With the carefully designed sampling proba-
bilities, our unbiased estimator Ce performs as accurately
as or more accurately than its counterparts asymptotically
in terms of matrix spectral norm (cid:107)Ce − C(cid:107)2. Note we
have not quantiﬁed the error in any other entry-wise norm
(e.g., the Frobenius norm) that could be uninformative on
the quality of the approximate invariant subspace and un-
stable regarding the additive random error (Anaraki, 2016;
Achlioptas et al., 2013; Gittens, 2011).

Algorithm 1 The proposed algorithm.
Input:

Data X ∈ Rd×n, sampling size m, and 0 < α < 1.

Output:

Estimated covariance matrix Ce ∈ Rd×d.

1: Initialize Y ∈ Rm×n, T ∈ Rm×n, v ∈ Rn, and w ∈

Rn with 0.
2: for all i ∈ [n] do
3:

Load xi into memory, let vi = (cid:107)xi(cid:107)1 = (cid:80)d
and wi = (cid:107)xi(cid:107)2
for all j ∈ [m] do

2 = (cid:80)d

k=1 x2
ki

k=1 |xki|

4:

5:

Pick tji ∈ [d] with pki ≡ P(tji = k) = α |xki|
vi
(1 − α) x2
ki
wi

, and let yji = xtjii
6: Pass the compressed data Y, sampling indices T, v,

+

w, and α to the fusion center.

7: for all i ∈ [n] do
8:
9:

Initialize Si ∈ Rd×m and P ∈ Rd×n with 0
for all j ∈ [m] do

+ (1 − α)

, and stjij,i =

y2
ji
wi

10:

Let ptjii = α |yji|
vi
1√

mptjii

11: Compute Ce as deﬁned in Eq. (1) by using {Si}n

i=1,

T, P, and Y.

We here summarize our method in Algorithm 1. In a nut-
shell, we employ a weighted sampling that is able to ex-
plore the most important entries to reduce estimation error
(cid:107)Ce − C(cid:107)2. Steps 1 to 5 in our proposed algorithm show
how to compress distributed data in many remote sites.
In step 5, each entry is retained with probability propor-
tional to the combination of its relative absolute value and
square value, and such sampling probability is designed to
make (cid:107)Ce − C(cid:107)2 as small as possible. Step 6 shows the
communication procedure, and steps 7 to 11 reveal how to
construct an unbiased covariance matrix estimator in the
fusion center from compressed data. In many computing
cases, it is possible to manipulate vectors of length O(d)
in memory, and thus when compressing data via weighted
sampling, only one pass is required to move data from the
external space to memory. Hence, our algorithm is also
applicable to streaming data. For a covariance matrix de-
ﬁned as C = 1
n XXT − ¯x¯xT , we can exactly calculate
(cid:80)n
i=1 xi in the fusion center via ¯x = 1
¯x = 1
j=1 uj,
n
n
i=1 are from g (cid:28) n remote sites, and uj ∈ Rd
where {xi}n
is the summation of all data vectors in the j-th remote site
before being compressed. Doing so makes no deviation on
the following error analysis and imposes only a negligible
computational burden.

(cid:80)g

3.3. Primary Provable Results

i=1.

i=1 and Y is able to obtain {ST

In this part, we introduce the proposed covariance matrix
In Algorithm 1, we employ Y, T, v, and w
estimator.
to calculate {Si}n
It can be veriﬁed that using only
i xi}n
{Si}n
i=1. Thus, we
describe our estimator via {Si}n
i xi}n
i=1 and {ST
i=1 in the
following theorem, which shows our estimator is unbiased.
Theorem 1. Assume X ∈ Rd×n and the sampling size
2 ≤ m < d. Sample m entries from each xi ∈ Rd with re-
placement by running Algorithm 1. Let {pki}d
k=1 and Si ∈
Rd×m denote the sampling probabilities and sampling ma-
trix, respectively. Then, the unbiased estimator for the tar-
get covariance matrix C = 1
n XXT can
n
be recovered as

i=1 xixT

i = 1

(cid:80)n

Ce = (cid:98)C1 − (cid:98)C2,
(cid:80)n

(1)

where E [Ce] = C, (cid:98)C1 = m
m
and (cid:98)C2 =
nm−n
.
bki =

1
1+(m−1)pki

nm−n
D(SiST

(cid:80)n

i=1

i=1 SiST
i SiST

i SiST
i xixT
i ,
i )D(bi) with

i xixT

i xixT

i SiST

Note that at most m entries in each bi have to be calculated
because each SiST
i has at most m non-zero en-
tries in its diagonal. Now, having achieved the above unbi-
ased estimator Ce, we analyze its properties. We precisely
upper bound the estimation error for the original estimator
C in the matrix spectral norm.
Theorem 2. Given X ∈ Rd×n and the sampling size 2 ≤
m < d, let C and Ce be deﬁned as in Theorem 1. If the

Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data

sampling probabilities satisfy pki = α |xki|
ki
(cid:107)xi(cid:107)2
(cid:107)xi(cid:107)1
2
with 0 < α < 1 for all k ∈ [d] and i ∈ [n], then with
probability at least 1 − η − δ,

+(1−α) x2

(cid:114)

(cid:107)Ce − C(cid:107)2 ≤ log(

)

),

+

2R
3

2σ2 log(

2d
δ
(cid:104) 7(cid:107)xi(cid:107)2

2d
δ
η ) 14(cid:107)xi(cid:107)2
n + log2( 2nd
n2m2(1−α)2 + 4(cid:107)xi(cid:107)2
n2m3α2(1−α) + 9(cid:107)xi(cid:107)4
(cid:105)
1xix2
(cid:107)xi(cid:107)2
n2mα (cid:107)2.

+ (cid:107) (cid:80)n

1(cid:107)xi(cid:107)2

nmα2

i=1

2

2

1

2

i

(cid:104)

where R = maxi∈[n]
8(cid:107)xi(cid:107)4
σ2 = (cid:80)n
2
+ 2(cid:107)xi(cid:107)2

i=1
2(cid:107)xi(cid:107)2
1
n2m2α(1−α)

n2m(1−α)

(2)

(cid:105)
, and

A large R and σ2 work against the accuracy of Ce. Ac-
cordingly, our sampling probabilities are designed to make
R and σ2 as small as possible to improve the accuracy. In
the proof of Theorem 2, we also show that the selection of
|xki|q
q = 1, 2 in
k=1 |xki|q used for constructing the sampling
(cid:80)d
probability pki = α |xki|
is necessary and
(cid:107)xi(cid:107)1
sufﬁcient to make the error bound considerably tight.

+ (1 − α) x2

ki
(cid:107)xi(cid:107)2
2

Furthermore, α balances the performance by (cid:96)1-norm
and (cid:96)2-norm based sampling x2
based sampling |xki|
.
ki
(cid:107)xi(cid:107)2
(cid:107)xi(cid:107)1
2
(cid:96)2 sampling penalizes small entries more than (cid:96)1 sampling.
Hence (cid:96)2 sampling is more likely to select larger entries to
decrease error. However, as seen from the proof in the ap-
pendix, different from (cid:96)1 sampling, (cid:96)2 sampling is unstable
and sensitive to small entries, and it can make estimation
error incredibly high if extremely small entries are picked.
Hence, if α varies from 1 to 0, the estimation error will de-
crease and then increase, which is also empirically veriﬁed
in the appendix.

The error bound in Theorem 2 involves many data-
dependent quantities, whereas our primary interest lies in
studying the tradeoff between the computational efﬁciency
and estimation accuracy by employing weighted sampling
to compress data and estimate covariance matrix. To clar-
ify, we modify Theorem 2 and make the bound explicitly
dependent on n, d, and m with the constraint 2 ≤ m < d.
Corollary 1. Given X ∈ Rd×n and the sampling size 2 ≤
m < d, let C and Ce be created by Algorithm 1. Deﬁne
(cid:107)xi(cid:107)1
d, and (cid:107)xi(cid:107)2 ≤ τ for all
(cid:107)xi(cid:107)2
i ∈ [n]. Then, with probability at least 1 − η − δ we have

≤ ϕ with 1 ≤ ϕ ≤

√

(cid:107)Ce − C(cid:107)2 ≤ min{ (cid:101)O

f +

(cid:16)

(cid:114)

(cid:114) 1
n

τ 2ϕ
m
d(cid:107)C(cid:107)2
n
(cid:113) (cid:107)C(cid:107)2

+ τ

(cid:114) 1
nm

(cid:17)

,

+ τ 2

(cid:114)

(cid:17)

d(cid:107)C(cid:107)2
nm

}, (3)

(cid:16)

(cid:101)O

f +

τ ϕ
m

where f = τ 2
logarithmic factors on η, δ, m, n, d, and α.

n + τ 2ϕ2

nm + τ ϕ

nm , and (cid:101)O(·) hides the

The formulation above explores the fact
(cid:107)xi(cid:107)1/(cid:107)xi(cid:107)2 ≤

that 1 ≤
d by the Cauchy-Schwarz inequality.

√

√

Before proceeding, we make several remarks to make a
comparison with the following representative work: Gauss-
Inverse, UniSample-HD, and Sparse. The ﬁrst two methods
provide error analysis without assuming data distribution,
which is shown in (Azizyan et al., 2015; Anaraki & Becker,
2017) and illustrated in our appendix. In the following re-
marks, only our method is sensitive to ϕ, and we also em-
ploy the fact that 1
2 =
τ 2 to simplify all asymptotic bounds.

F ≤ (cid:107)C(cid:107)2 ≤ maxi∈[n] (cid:107)xi(cid:107)2

nd (cid:107)X(cid:107)2

m

m

(cid:17)

(cid:17)

, (cid:101)O

(cid:16) τ 2d

(cid:113) d
n

nm + τ d

nm + τ 2

(cid:113) (cid:107)C(cid:107)2
n

d indicates the error bound
Remark 1. Eq. (3) with ϕ =
for our estimator Ce in the worst case, where the magni-
tudes of each entry in all of the input data vectors are the
same (i.e., highly uniformly distributed). Even in this case,
nm +
our error bound has a leading term of order min{ (cid:101)O
(cid:113) d(cid:107)C(cid:107)2
(cid:16) τ 2d

}, which is the
τ
same as Gauss-Inverse ignoring logarithmic factors.
In
contrast, as the magnitudes of the entries in each data vec-
tor become uneven, ϕ gets smaller, leading to a tighter er-
ror bound than that in Gauss-Inverse. Furthermore, when
most of the entries in each vector xi have very low mag-
nitudes, the summation of these magnitudes will be com-
parable to a particular constant. This situation is typ-
ical because in practice only a limited number of fea-
tures in each input data dominate the learning perfor-
mance. Hence, ϕ turns to O(1), and Eq. (3) becomes
(cid:16) τ 2

min{ (cid:101)O
}, which is
tighter than the leading term of Gauss-Inverse by a factor
of at least (cid:112)d/m. As explained in the next section, Gauss-
Inverse also lacks computational efﬁciency.

n + τ 2(cid:113) 1
(cid:16) τ 2

(cid:113) d(cid:107)C(cid:107)2
nm

n + τ

, (cid:101)O

nm

(cid:17)

(cid:17)

Remark 2. As our target is to compress data to a smaller
m that is not comparable to d in practice, O(d − m)
can be approximately regarded as O(d). Then, the error
(cid:17)

of UniSample-HD is (cid:101)O
nm + τ
,
which is asymptotically worse than our bound. When n
is sufﬁciently large, the leading term of its error becomes

nm + τ 2d

(cid:113) 1
nm

(cid:113) d(cid:107)C(cid:107)2

(cid:16) τ 2d

m

(cid:16)

(cid:113) d(cid:107)C(cid:107)2

(cid:17)

(cid:113) 1
nm

nm + τ 2d

m

τ

(cid:101)O
, which can be weaker than the
leading term in our method by a factor of 1 to (cid:112)d/m when
ϕ =

d, and at least d/m when ϕ = O(1).

√

However, if m is sufﬁciently close to d, which is not mean-
ingful for practical usage, O(d − m) = O(1) will hold and

(cid:16) τ 2d

(cid:113) d(cid:107)C(cid:107)2

(cid:17)

nm +τ

nm +

(cid:113) d
nm

. This bound may slightly outperform ours by a

the error of UniSample-HD becomes (cid:101)O
τ 2
m
factor of (cid:112)d/m = O(1) when ϕ =
d, but is still worse
than ours when ϕ = O(1). These results also coincide with
the fact that UniSample-HD adopts uniform sampling with-
out replacement combined with the Hadamard matrix, but
we employ weighted sampling with replacement.

√

Remark 3. The Sparse method, which employs a sparse

Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data

matrix for each Si, is not sufﬁciently accurate as demon-
strated in our experiments. Moreover, there is no error anal-
ysis available for its unbiased estimator to characterize the
estimation error versus the compression ratio.

Thus far, we have not made statistical nor structural as-
sumptions concerning the input data or covariance matrix
to derive our provable results. However, motivated by (Az-
izyan et al., 2015), it is also straightforward to extend our
results to the statistical data and a low-rank covariance ma-
trix estimation. The derived results below are polynomially
equivalent to those in Gauss-Inverse (Azizyan et al., 2015).
Corollary 2 shows the (low-rank) covariance matrix esti-
mation on Gaussian data, and Corollary 3 indicates the de-
rived covariance estimator also guarantees the accuracy of
the principal components regarding the subspace learning.
Corollary 2. Given X ∈ Rd×n (2 ≤ d) and an unknown
population covariance matrix Cp ∈ Rd×d with each col-
umn vector xi ∈ Rd i.i.d. generated from the Gaussian dis-
tribution N (0, Cp). Let Ce be constructed by Algorithm 1
with the sampling size 2 ≤ m < d. Then, with probability
at least 1 − η − δ − ζ,

(cid:107)Ce − Cp(cid:107)2
(cid:107)Cp(cid:107)2

≤ (cid:101)O

(cid:16) d2
nm

+

d
m

(cid:114)

(cid:17)

;

d
n

(4)

Additionally, assuming rank(Cp)≤ r, with probability at
least 1 − η − δ − ζ we have

(cid:107)[Ce]r − Cp(cid:107)2
(cid:107)Cp(cid:107)2

≤ (cid:101)O

(cid:16) rd
nm

+

r
m

(cid:114)

(cid:114)

d
n

+

(cid:17)

,

rd
nm

(5)

where [Ce]r is the solution to minrank(A)≤r (cid:107)A − Ce(cid:107)2,
and (cid:101)O(·) hides the logarithmic factors on η, δ, ζ, m, n, d,
and α.
Corollary 3. Given X, d, m, Cp and Ce as deﬁned
k = (cid:80)k
i and (cid:98)(cid:81)
i=1 uiuT
in Corollary 2.
k =
(cid:80)k
i=1 and {ˆui}k
i=1 ˆui ˆuT
i=1 being the leading
k eigenvectors of Cp and Ce, respectively. Denote by λk
the k-th largest eigenvalue of Cp. Then, with probability at
least 1 − η − δ − ζ,

i with {ui}k

Let (cid:81)

(cid:107) (cid:98)(cid:81)

k − (cid:81)
(cid:107)Cp(cid:107)2

k (cid:107)2

≤

1
λk − λk+1

(cid:16) d2
nm

(cid:101)O

+

d
m

(cid:114)

(cid:17)

,

d
n

(6)

where the eigengap λk − λk+1 > 0 and (cid:101)O(·) hides the
logarithmic factors on η, δ, ζ, m, n, d, and α.

The proof details of all our theoretical results are rele-
gated to the appendix. We leverage the Matrix Bernstein
inequality (Tropp, 2015) and establish the error bound of
our proposed estimator on an arbitrary sampling probabil-
ity in order to determine which sampling probability brings
the best estimation accuracy. The employment of the Ma-
trix Bernstein inequality involves controlling the range and

variance of all zero-mean random matrices, whose deriva-
tions differ from those in (Azizyan et al., 2015; Anaraki
& Becker, 2017) because of different data compression
schemes. Moreover, to obtain the desired tight bound for
the range and variance, we precisely provide a group of
closed-form equalities or concentration inequalities for var-
ious quantities (see our proposed Lemma 1 and Lemma 2
along with their proofs in the appendix).

3.4. Computational Complexity

Recall that we have n data samples in the d-dimensional
space, and let m be the target compressed dimension. Re-
garding estimating C = 1
n XXT , the computational com-
parisons between our method and the representative base-
line methods are presented in Table 1, in which Stan-
dard method means that we compute C directly without
data compression. For the deﬁnition of covariance matrix
C = 1
n XXT − ¯x¯xT , extra computational costs (i.e., O(gd)
storage, O(gd) communication cost, and O(nd) time) must
be added to the last four compression methods in the table,
where g (cid:28) n is the number of the entire remote sites. All
detailed analysis is relegated to the appendix.

TG and TS in Table 1 represent
the time taken to
generate the Gaussian matrices and sparse matrices by
fast pseudo-random number generators like Mersenne
Twister (Matsumoto & Nishimura, 1998), which can be
enormous (Anaraki & Becker, 2017) and proportional to
nmd and nd2, respectively, up to certain small constants.
Hence, our method can be regarded as the most efﬁcient
when d is large. Furthermore, by using the smallest m to
obtain the same estimation accuracy as the other methods,
our approach incurs the least computational burden.

4. Empirical Studies

In this section, we empirically verify the properties of the
proposed method and demonstrate its superiority. We com-
pare its estimation accuracy with that of Gauss-Inverse,
Sparse, and UniSample-HD. We also report the time com-
parisons.

We run all algorithms on both synthetic and real-world
datasets whose largest dimension is around and below 105.
Such dimension is not very high in modern data analysis,
but this limitation is due to that reporting the estimation
error by calculating the spectral norm of a covariance ma-
trix with its size larger than 105 × 105 will take intolerable
amount of memory and time. The parameter selection on α
is deferred to the appendix, and we empirically set α = 0.9.
To allow a fair comparison of the time consumptions mea-
sured by FLOPS, we implement all algorithms in C++ and
run them in a single thread mode on a standard workstation
with Intel CPU@2.90GHz and 128GB RAM.

Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data

Method
Standard
Gauss-Inverse
Sparse

Storage
O(nd + d2)
O(nm + d2)
O(nm + d2)
UniSample-HD O(nm + d2)
O(nm + d2)

Table 1. Computational costs on the storage, communication, and time.
Communication
O(nd)
O(nm)
O(nm)
O(nm)
O(nm)

Time
O(nd2)
O(nmd + nm2d + nd2) + TG
O(d + nm2) + TS
O(nd log d + nm2)
O(nd + nm log d + nm2)

Our method

4.1. Experiments on Synthetic Datasets

To clearly examine the performance, we compare all meth-
i=1 ∈ R1024×20000,
ods on six synthetic datasets: {Xi}3
X4 ∈ R1024×200000, X5 ∈ R2048×200000, and X6 ∈
R65536×200000, which are generated based on the gener-
ative model (Liberty, 2013). Speciﬁcally, given a matrix
X ∈ Rd×n from such model, it is formally deﬁned as
X = UFG, where U ∈ Rd×k deﬁnes the signal column
space with UT U = Ik (k ≤ d), the square diagonal matrix
F ∈ Rk×k contains the diagonal entries fii = 1−(i−1)/k
that gives linearly diminishing signal singular values, and
G ∈ Rk×n is the signal coefﬁcient with gij ∼ N (0, 1) that
is the Gaussian distribution. We let k ≈ 0.005d, then set-
ting d = 1024 and n = 20000 completes the creation of
data X1. For X2, it is deﬁned as DX, where each entry in
the square diagonal matrix D is deﬁned by dii = 1/βi, and
βi is randomly sampled from the integer set [15]. Regard-
ing X3, it is constructed in the same way as X1 except that
F now becomes an identity matrix. Next, {Xi}6
i=4 follow
the same generation strategy of X2 except for the n and d.

Figure 1. Accuracy comparisons of covariance matrix estimation
on synthetic datasets. The estimation error is measured by
(cid:107)Ce − C(cid:107)2/(cid:107)C(cid:107)2 with Ce calculated by all compared methods,
and cf = m/d is the compression ratio.

In Figure 1, we plot the relative estimation error averaged
over ten runs with its standard deviation versus the naive
compression ratio cf = m/d. Note that a large cf is not
necessary for practical usage, and our aim is to compress
data to a smaller volume. In Figure 2, we report the running
time taken in both the compressing and recovering stages,
which preliminarily depicts the efﬁciency of the different
methods and indicates how much power should be spent in
the practical computation.

√

√

Generally, our method displays the least error and devi-
ation for all datasets and its error decreases dramatically
with an increase at a small cf. This observation indicates
that our method can achieve sufﬁcient estimation accuracy
by using substantially fewer data entries than the other
d), the magnitudes of the
methods. For X1 (ϕ = 0.81
data entries are highly uniformly distributed, and thus our
method can be regarded as uniform sampling with replace-
ment, which may perform slightly worse than UniSample-
HD and Gauss-Inverse if cf becomes large enough. Af-
ter allowing the magnitudes to vary within a moderately
d), our method consider-
larger range in X2 (ϕ = 0.55
ably outperforms the other three methods. Its improvement
comes from that only our method is sensitive to ϕ and a
smaller ϕ produces a tighter result, as demonstrated by Re-
marks 1 and 2. However, the error of each method in X3
√
(τ /(cid:112)(cid:107)C(cid:107)2 = 5.5, ϕ = 0.81
d) is larger than that in X1
√
(τ /(cid:112)(cid:107)C(cid:107)2 = 4.3, ϕ = 0.81
d), respectively. It is be-
cause of that almost all methods are sensitive to τ /(cid:112)(cid:107)C(cid:107)2,
and the error (cid:107)Ce − C(cid:107)2/(cid:107)C(cid:107)2 increases when τ /(cid:112)(cid:107)C(cid:107)2
rises. Such phenomenon is demonstrated via dividing nu-
merous error bounds in Remarks 1 and 2 by (cid:107)C(cid:107)2. Our
method also achieves the best performance in X4. Al-
though the ϕ and τ /(cid:112)(cid:107)C(cid:107)2 in X4 are approximately equal
with those in X2, yet the proved error bounds with Re-
marks 1 and 2 reveal that a larger n in X4 will lead to
smaller estimation errors given the same cf. Finally, our
method also achieves the best accuracy when the dimension
d increases in both X5 and X6. Besides, taking more data
(i.e., enlarging n) as suggested by X4 can be considered to
reduce the error in X5 and X6. Note that Gauss-Inverse
has not been run on X6 since it costs enormous time.

Figure 2. Time comparisons of covariance matrix estimation on
synthetic datasets. Rescaled time results from the running time
normalized by that spent in the Standard way of calculating C =
XXT /n without data compression, and it is plotted in log scale.

Turning to Gauss-Inverse, it becomes highly accurate when

m/d0.10.20.30.40.5Error00.050.10.150.20.25X1, d=1024 n=20000Gauss-InverseSparseUniSample-HDOur methodm/d0.10.20.30.40.5Error00.040.080.120.16X4, d=1024 n=200000Gauss-InverseSparseUniSample-HDOur methodm/d0.10.20.30.40.5Error00.10.20.30.40.5X2, d=1024 n=20000Gauss-InverseSparseUniSample-HDOur methodm/d0.10.20.30.40.5Error00.040.080.120.16X5, d=2048 n=200000Gauss-InverseSparseUniSample-HDOur methodm/d0.10.20.30.40.5Error00.10.20.30.40.5X3, d=1024 n=20000Gauss-InverseSparseUniSample-HDOur methodm/d0.10.20.30.40.5Error00.150.30.450.6X6, d=65536 n=200000SparseUniSample-HDOur methodm/d0.10.20.30.40.5Rescaled Time10-310-210-1100101102103X2, d=1024 n=20000Gauss-InverseSparseUniSample-HDOur methodStandardm/d0.10.20.30.40.5Rescaled Time10-310-210-1100101102103X4, d=1024 n=200000Gauss-InverseSparseUniSample-HDOur methodStandardm/d0.10.20.30.40.5Rescaled Time10-310-210-1100101102103X5, d=2048 n=200000Gauss-InverseSparseUniSample-HDOur methodStandardToward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data

cf increases but requires much more time than Standard
(see Figure 2) so that its usage might be ruled out in prac-
tice. However, Gauss-Inverse remains a good choice when
we are in urgent need of reducing the storage and commu-
nication burden. Sparse, which has no error analysis of its
unbiased estimator, generally performs less accurately than
the others but requires less time than Standard. UniSample-
HD is efﬁcient while it still consumes more time than our
method. Also, its accuracy is inferior to our method espe-
cially when cf is small. In conclusion, our method is ca-
pable of compressing data to a very small size while guar-
anteeing both estimation accuracy and computational efﬁ-
ciency.

4.2. Experiments on Real-world Datasets

In the second set of experiments, we use nine publicly
available real-world datasets (Chang & Lin, 2011; Blake
& Merz, 1998; Amsaleg, 2010), some of which are gath-
ered from many distributed sensors. Their statistics are
displayed in Figure 4. We again compare the estimation
accuracy of the proposed method against the other three
approaches. As can be seen from the ﬁgure, our method
consistently exhibits superior accuracy over all cf = m/d,
and its error decreases dramatically when cf grows. The
error of the other three methods also decreases with cf but
is still large at a small cf. Besides, our method enjoys the
least deviation. In summary, these results conﬁrm that our
method can compress data to the lowest volume with the
best accuracy, thereby substantially reducing storage, com-
munication, and processing time cost in practice.

Figure 3. Convergence rates of our method for the settings in
Corollaries 2 and 3.

√

As conﬁrmed in Figure 1, a large n beneﬁts the estimation
accuracy. Thus, we study its effect more quantitatively. We
conduct experiments following the settings as deﬁned in
Corollaries 2 and 3, and their results in Eqs. (4)-(6) clearly
show that the errors decay in 1/
n convergence rate if d (cid:28)
n. We run our method on another two synthetic datasets
t=7 ∈ Rd×n that follow the d-dimensional multivari-
{Xt}8
ate normal distribution N (0, Cpt), where the (i, j)-th ele-
ment of Cp7 ∈ Rd×d is 0.5|i−j|/50, and Cp8 ∈ Rd×d is
a low-rank matrix that satisﬁes minrank(A)≤r (cid:107)A − Cp7(cid:107)2.
We take d = 1000, r = 5, m/d = {0.02, 0.05, 0.15},
k = {5, 10, 15}, and vary n from 1000 to 100000. In Fig-
ure 3, the top three plots report the errors as deﬁned in the
LHS of Eqs. (4)-(6), respectively. Then, dividing such er-
rors by 1/

n obtains the bottom three plots accordingly.

√

√

The observation that the curves in plots (d)-(f) are roughly
ﬂat validates that the error bounds induced by our method
n convergence rate, which
decay rapidly with n in the 1/
coincides with Eqs. (4)-(6).
In addition to the fast error
convergence for the low-rank matrix Cp8, our method can
also obtain an increasingly better estimation accuracy for a
high-rank covariance matrix Cp7 if we enlarge n, which is
displayed in plot (a). Besides, considering the omitted plot
where the eigengap λk −λk+1 of Cp7 decreases with k, the
fact that the errors in plot (c) increase with k also coheres
with Eq. (6). To conclude, our method also adapts well to
the speciﬁc settings in Corollaries 2 and 3, and all induced
error bounds indeed satisfy a 1/

n convergence rate.

√

Figure 4. Accuracy comparisons of covariance matrix estimation
on real-world datasets.

5. Conclusion

In this paper, we describe a weighted sampling method for
accurate and efﬁcient calculation of an unbiased covari-
ance matrix estimator. The analysis demonstrates that our
method can employ a smaller data volume than the other
approaches to achieve an equal accuracy, and is highly efﬁ-
cient regarding the communication, storage, and processing
time. The empirical results of the algorithm’s application to
both synthetic and real-world datasets further support our
analysis and demonstrate its signiﬁcant improvements over
other state-of-the-art methods.

Compared with the sampling-with-replacement scheme in
this paper, we seek to make more achievements via a
sampling-without-replacement scheme in the future work.
Analyzing the corresponding unbiased estimator will pose
signiﬁcant technical challenges in this research direction.

n×104246810Error00.20.40.60.81(a), X7 X8X7, m/d=0.02X8, m/d=0.02X7, m/d=0.05X8, m/d=0.05X7, m/d=0.15X8, m/d=0.15n×104246810Rescaled Error08162432(d), X7 X8n×104246810Error00.20.40.60.8(b), X8m/d=0.02m/d=0.05m/d=0.15n×104246810Rescaled Error0510152025(e), X8n×104246810Error×10-302468(c), X7k=5, m/d=0.02k=10, m/d=0.02k=15, m/d=0.02k=5, m/d=0.05k=10, m/d=0.05k=15, m/d=0.05k=5, m/d=0.15k=10, m/d=0.15k=15, m/d=0.15n×104246810Rescaled Error00.20.40.60.81(f), X7m/d0.10.20.30.40.5Error00.050.10.150.20.250.3DailySports, d=5625 n=9120Gauss-InverseSparseUniSample-HDOur methodm/d0.10.20.30.40.5Error00.20.40.60.8Arcene, d=10000 n=800Gauss-InverseSparseUniSample-HDOur methodm/d0.10.20.30.40.5Error00.10.20.3Slice, d=384 n=53500Gauss-InverseSparseUniSample-HDOur methodm/d0.10.20.30.40.5Error00.020.040.060.08Cifar10, d=3072 n=60000Gauss-InverseSparseUniSample-HDOur methodm/d0.10.20.30.40.5Error00.010.020.030.040.05Gist1M, d=960 n=1000000Gauss-InverseSparseUniSample-HDOur methodm/d0.10.20.30.40.5Error00.050.10.150.2Mnist, d=780 n=70000Gauss-InverseSparseUniSample-HDOur methodm/d0.10.20.30.40.5Error00.10.20.30.4Isolet, d=617 n=7797Gauss-InverseSparseUniSample-HDOur methodm/d0.10.20.30.40.5Error00.10.20.30.40.5A9a, d=123 n=48842Gauss-InverseSparseUniSample-HDOur methodm/d0.10.20.30.40.5Error00.050.10.150.2UJIIndoorLoc, d=520 n=21048Gauss-InverseSparseUniSample-HDOur methodToward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data

Acknowledgments

We truly thank Akshay Krishnamurthy for the fruitful dis-
cussions and interpretations on (Azizyan et al., 2015). We
also thank Yuxin Su for the help on the experiments. The
work described in this paper was fully supported by the
National Natural Science Foundation of China (Project
No. 61332010), the Research Grants Council of the Hong
Kong Special Administrative Region, China ((No. CUHK
14208815 and No. CUHK 14234416 of the General Re-
search Fund), and 2015 Microsoft Research Asia Col-
laborative Research Program (Project No. FY16-RES-
THEME-005).

References

Abbasi-Daresari, S. and Abouei, J. Toward cluster-based
weighted compressive data aggregation in wireless sen-
sor networks. Ad Hoc Networks, 36:368–385, 2016.

Abrahamsson, R., Selen, Y., and Stoica, P. Enhanced co-
variance matrix estimators in adaptive beamforming. In
Acoustics, Speech and Signal Processing, 2007. ICASSP
2007. IEEE International Conference on, volume 2, pp.
II–969. IEEE, 2007.

Achlioptas, D. and Mcsherry, F. Fast computation of low-
rank matrix approximations. Proceedings of the annual
ACM symposium on Theory of computing, 54(2):9, 2007.

Achlioptas, D., Karnin, Z. S., and Liberty, E. Near-optimal
In Advances in
entrywise sampling for data matrices.
Neural Information Processing Systems, pp. 1565–1573,
2013.

Ailon, N. and Chazelle, B. The fast johnson-lindenstrauss
SIAM

transform and approximate nearest neighbors.
Journal on Computing, 39(1):302–322, 2009.

Amsaleg, L. Datasets for approximate nearest neighbor

search, 2010.

Anaraki, F. Estimation of the sample covariance matrix
IET Signal Process-

from compressive measurements.
ing, 2016.

Anaraki, F. and Becker, S. Preconditioned data sparsiﬁca-
tion for big data with applications to pca and k-means.
IEEE Transactions on Information Theory, 2017.

Anaraki, F. and Hughes, S. Memory and computation efﬁ-
In Pro-
cient pca via very sparse random projections.
ceedings of the 31st International Conference on Ma-
chine Learning (ICML-14), pp. 1341–1349, 2014.

Azizyan, M., Krishnamurthy, A., and Singh, A. Extreme
compressive sampling for covariance estimation. arXiv
preprint arXiv:1506.00898, 2015.

Bartz, D. Advances in high-dimensional covariance matrix

estimation. 2016.

Bioucas-Dias, J., Cohen, D., and Eldar, Y. C. Covalsa: Co-
variance estimation from compressive measurements us-
ing alternating minimization. In Signal Processing Con-
ference (EUSIPCO), 2014 Proceedings of the 22nd Eu-
ropean, pp. 999–1003. IEEE, 2014.

Blake, C.L. and Merz, C.J. UCI repository of machine

learning databases, 1998.

Butte, A. J., Tamayo, P., Slonim, D., Golub, T. R., and Ko-
hane, I. S. Discovering functional relationships between
rna expression and chemotherapeutic susceptibility us-
ing relevance networks. Proceedings of the National
Academy of Sciences, 97(22):12182–12186, 2000.

Cai, T. T., Zhang, A., et al. Rop: Matrix recovery via rank-
one projections. The Annals of Statistics, 43(1):102–138,
2015.

Chang, Chih-Chung and Lin, Chih-Jen. Libsvm: a library
for support vector machines. ACM Transactions on In-
telligent Systems and Technology (TIST), 2(3):27, 2011.

Chen, Y., Chi, Y., and Goldsmith, A. Exact and stable co-
variance estimation from quadratic sampling via convex
programming. 2013.

Dasarathy, G., Shah, P., Bhaskar, B. N., and Nowak, R. D.
Sketching sparse matrices, covariances, and graphs via
tensor products. Information Theory, IEEE Transactions
on, 61(3):1373–1388, 2015.

Deisenroth, M. P., Neumann, G., Peters, J., et al. A survey
on policy search for robotics. Foundations and Trends in
Robotics, 2(1-2):1–142, 2013.

Drineas, P., Kannan, R., and Mahoney, M. W. Fast monte
carlo algorithms for matrices i: Approximating matrix
multiplication. SIAM Journal on Computing, 36(1):132–
157, 2006a.

Drineas, P., Mahoney, M. W., and Muthukrishnan, S. Sub-
space sampling and relative-error matrix approximation.
In Approximation, Randomization, and Combinatorial
Optimization. 2006b.

Feller, W. introduction to probability theory and its appli-

cations. vol. ii.[an]. 1966.

Anzai, Y. Pattern Recognition & Machine Learning. Else-

Gittens, A. The spectral norm error of the naive nystrom

vier, 2012.

extension. arXiv preprint arXiv:1110.5305, 2011.

Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data

Gleichman, S. and Eldar, Y. C. Blind compressed sens-
ing. Information Theory, IEEE Transactions on, 57(10):
6958–6975, 2011.

Qi, H. and Hughes, S. M. Invariance of principal compo-
nents under low-dimensional random projection of the
data. In Image Processing. IEEE, 2012.

Ha, W. and Barber, R. F. Robust pca with compressed data.
In Advances in Neural Information Processing Systems,
pp. 1936–1944, 2015.

Sch¨afer, J. and Strimmer, K. An empirical bayes ap-
proach to inferring large-scale gene association net-
works. Bioinformatics, 21(6):754–764, 2005.

Shi, T., Tang, D., Xu, L., and Moscibroda, T. Correlated
In UAI, pp.

compressive sensing for networked data.
722–731, 2014.

Srisooksai, T., Keamarungsi, K., Lamsrichan, P., and
Araki, K. Practical data compression in wireless sensor
networks: A survey. Journal of Network and Computer
Applications, 35(1):37–59, 2012.

Tropp, J. A. Improved analysis of the subsampled random-
ized hadamard transform. Advances in Adaptive Data
Analysis, 3(01n02):115–126, 2011.

Tropp, J. A. An introduction to matrix concentration in-
equalities. Foundations and Trends in Machine Learn-
ing, 8(1-2):1–230, 2015.

Tulino, A. M. and Verd´u, S. Random matrix theory and
wireless communications, volume 1. Now Publishers
Inc, 2004.

Woodruff, D. P. Sketching as a tool for numerical linear

algebra. arXiv preprint arXiv:1411.4357, 2014.

Wu, S., Bhojanapalli, S., Sanghavi, S., and Dimakis, A.
In Advances In
Single pass pca of matrix products.
Neural Information Processing Systems, pp. 2577–2585,
2016.

Zou, H., Hastie, T., and Tibshirani, R. Sparse princi-
pal component analysis. Journal of computational and
graphical statistics, 15(2):265–286, 2006.

Halko, N., Martinsson, P., and Tropp, J. A. Finding struc-
ture with randomness: Probabilistic algorithms for con-
structing approximate matrix decompositions. SIAM re-
view, 53(2):217–288, 2011.

Hansen, L. P. Large sample properties of generalized
method of moments estimators. Econometrica: Journal
of the Econometric Society, pp. 1029–1054, 1982.

Haupt, J., Bajwa, W. U., Rabbat, M., and Nowak, R. Com-
pressed sensing for networked data. IEEE Signal Pro-
cessing Magazine, 25(2):92–101, 2008.

Holodnak, J. T. and Ipsen, I. C. Randomized approximation
of the gram matrix: Exact computation and probabilistic
bounds. SIAM Journal on Matrix Analysis and Applica-
tions, 36(1):110–137, 2015.

Hyv¨arinen, A., Karhunen, J., and Oja, E. Independent com-
ponent analysis, volume 46. John Wiley & Sons, 2004.

Kariya, T. and Kurata, H. Generalized least squares. John

Wiley & Sons, 2004.

Li, P., Hastie, T. J., and Church, K. W. Very sparse random
projections. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining. ACM, 2006.

Liberty, E. Simple and deterministic matrix sketching.
In Proceedings of the 19th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pp. 581–588. ACM, 2013.

Mahoney, M. Randomized algorithms for matrices and
data. Foundations and Trends in Machine Learning, 3
(2):123–224, 2011.

Matsumoto, M. and Nishimura, T. Mersenne twister:
a 623-dimensionally equidistributed uniform pseudo-
random number generator. ACM Transactions on Mod-
eling and Computer Simulation, 1998.

Mroueh, Y., Marcheret, E., and Goel, V. Co-occuring direc-
tions sketching for approximate matrix multiply. arXiv
preprint arXiv:1610.07686, 2016.

Papailiopoulos, D., Kyrillidis, A., and Boutsidis, C. Prov-
able deterministic leverage score sampling. In Proceed-
ings of the 20th ACM SIGKDD international conference
on Knowledge discovery and data mining, pp. 997–1006.
ACM, 2014.

