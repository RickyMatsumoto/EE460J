On the Sampling Problem for Kernel Quadrature

A. Appendix

This appendix complements the paper “On the sampling
problem for kernel quadrature”. Section A.1 discusses the
potential lack of robustness of greedy optimization meth-
ods, which motivated the development of SMC-KQ. Sec-
tions A.2 and A.3 discuss some of the theoretical aspects
of KQ, whilst Section A.4 and A.5 presents additional nu-
merical experiments and details for implementation. Fi-
nally, Section A.6 provides detailed pseudo-code for all al-
gorithms used in this paper.

A.1. Lack of Robustness of Optimisation Methods

To demonstrate the non-robustness to mis-speciﬁed ker-
nels, that is a feature of optimisation-based methods, we
considered integration against Π = N(0, 1) for func-
tions that can be approximated by the kernel k(x, x(cid:48)) =
exp(−(x − x(cid:48))2/(cid:96)2). An initial state x1 was ﬁxed at the
the state xn was cho-
origin and then for n = 2, 3, . . .
sen to minimise the error criterion en(w; {xj}n
j=1) given
the location of the {xj}n
j=1. This is known as ‘sequential
Bayesian quadrature’ (SBQ; Huszar and Duvenaud, 2012;
Gunter et al., 2014; Briol et al., 2015a). The kernel length
scale was ﬁxed at (cid:96) = 0.01 and we consider (as a thought
experiment, since it does not enter into our selection of
points) a more regular integrand, such as that shown in Fig.
5 (top). The location of the states {xj}n
j=1 obtained in this
manner are shown in Fig. 5 (bottom). It is clear that SBQ is
not an efﬁcient use of computation for integration of the in-
tegrand against N(0, 1). Of course, a bad choice of kernel
length scale parameter (cid:96) can in principle be alleviated by
kernel learning, but this will not be robust the case where n
is very small.

This example motivates sampling-based methods as an al-
ternative to optimisation-based methods. Future work will
be required to better understand when methods such as SBQ
can be reliable in the presence of unknown kernel parame-
ters, but this was beyond the scope of this work.

A.2. Additional Deﬁnitions

The space L2(Π) is deﬁned to be the set of Π-measurable
functions f : X → R such that the Lebesgue integral

(cid:90)

X

f 2 dΠ

exists and is ﬁnite.

For a multi-index α = (α1, . . . , αd) deﬁne |α| = α1 +
· · · + αd. The (standard) Sobolev space of order s ∈ N is
denoted

of

the
j=1), denoted SBQ, does not

criterion
Figure 5. Sequential minimisation
en(w; {xj}n
lead to adequate
placement of points {xj}n
j=1 when the kernel is mis-speciﬁed.
[Here the kernel length scale was ﬁxed to (cid:96) = 0.01. Selected
points xj are represented as red. For comparison, a collection of
draws from Π, as used in KQ, are shown as blue points.]

error

This space is equipped with norm

(cid:107)f (cid:107)Hs(Π) =

(cid:107)(∂x1)α1 . . . (∂xd)αd f (cid:107)2

L2(Π)

(cid:33)1/2

.

(cid:32)

(cid:88)

|α|≤s

Two normed spaces (F, (cid:107) · (cid:107)) and (F, (cid:107) · (cid:107)(cid:48)) are said to be
‘norm equivalent’ if there exists 0 < c < ∞ such that

c−1(cid:107)f (cid:107)(cid:48) ≤ (cid:107)f (cid:107) ≤ c(cid:107)f (cid:107)(cid:48)

for all f ∈ F.

A.3. Theoretical Results

A.3.1. PROOF OF THEOREM 1

Proof. From Thm. 11.13 in Wendland (2004) we have that
there exist constants 0 < ck < ∞, h0 > 0 such that

| ˆf (x) − f (x)| ≤ ckhs

n(cid:107)f (cid:107)H

(7)

for all x ∈ X , provided hn < h0, where

hn = sup
x∈X

min
i=1,...,n

(cid:107)x − xi(cid:107)2.

Hs(Π) = {f : X → R s.t.

(∂x1)α1 . . . (∂xd)αd f ∈ L2(Π) ∀ |α| ≤ s}.

Under the hypotheses, we can suppose that the determinis-
tic states x1, . . . , xm ensure hm < h0. Then Eqn. 7 holds

On the Sampling Problem for Kernel Quadrature

for all n > m, where the xm+1, . . . , xn are independent
draws from Π(cid:48). It follows that

Under the hypothesis on n, Prop. 1 of Bach (2015) estab-
lished that when x1, . . . , xn ∼ ΠB are independent, then

| ˆΠ(f ) − Π(f )| ≤ sup
x∈X
≤ ckhs

n(cid:107)f (cid:107)H.

| ˆf (x) − f (x)|

sup
(cid:107)f (cid:107)H≤1

inf
2≤ 4
(cid:107)β(cid:107)2
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

βi
πB(xi)1/2

ψ(·, xi) − f

≤ 4λ

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

L2(Π)

Next, Lem. 1 in Oates et al. (2016) establishes that, un-
der the present hypotheses on X and Π(cid:48), there exists 0 <
cΠ(cid:48),(cid:15) < ∞ such that

E[h2s

n ] ≤ cΠ(cid:48),(cid:15)m−2s/d+(cid:15)

for all (cid:15) > 0, where cΠ(cid:48),(cid:15) is independent of n.

Combining the above results produces

E[ ˆΠ(f ) − Π(f )]2 ≤ c2
E[h2s
k
kcΠ(cid:48),(cid:15)m−2s/d+(cid:15)(cid:107)f (cid:107)2
≤ c2
H

n ](cid:107)f (cid:107)2
H

as required, with ck,Π(cid:48),(cid:15) = ckc1/2
Π(cid:48),(cid:15).

A.3.2. PROOF OF THEOREM 2

Proof. The Cauchy-Schwarz result for kernel mean em-
beddings (Smola et al., 2007) gives

with probability at least 1−δ. Fixing the function f in Eqn.
9 leads to the statement that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

βi
πB(xi)1/2

inf
2≤ 4
(cid:107)β(cid:107)2
n

ψ(·, xi) −

ψ(·, x)Π(dx)

(cid:90)

X

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

L2(Π)

is at most 4λ with probability at least 1 − δ. The inﬁmum
over (cid:107)β(cid:107)2
2 ≤ 4/n can be replaced with an unconstrained
inﬁmum over Rn to obtain the weaker statement that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

inf
β∈Rn

βi
πB(xi)1/2

ψ(·, xi) −

ψ(·, x)Π(dx)

(cid:90)

X

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

L2(Π)

is at most 4λ with probability at least 1 − δ. Now, recall
from Sec. 2.1 that the KQ weights w are characterised
through the solution β∗ to this optimisation problem as
wi = β∗

i πB(xi)−1/2. It follows that

| ˆΠ(f ) − Π(f )|
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

wik(·, xi) −

(cid:90)

X

≤

k(·, x)Π(dx)

(cid:107)f (cid:107)H.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)H

(8)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

wiψ(·, xi) −

ψ(·, x)Π(dx)

≤ 4λ

(cid:90)

X

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

L2(Π)

Consider the ﬁrst term above. Since H is dense in L2(Π),
it follows that Σ1/2 (the unique positive self-adjoint square
root of Σ) is an isometry from L2(Π) to H. Now, since
k(·, x) ∈ H, there exists a unique element ψ(·, x) ∈
L2(Π) such that Σ1/2ψ(·, x) = k(·, x). Then we have that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

n
(cid:88)

i=1

n
(cid:88)

i=1

=

=

wik(·, xi) −

k(·, x)Π(dx)

wiΣ1/2ψ(·, xi) −

Σ1/2ψ(·, x)Π(dx)

(cid:90)

X

(cid:90)

X

(cid:90)

X

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)H

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)L2(Π)

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)H

wiψ(·, xi) −

ψ(·, x)Π(dx)

For f ∈ L2(Π), we have f ∈ H if and only if

f =

g(x)ψ(·, x)Π(dx)

(cid:90)

X

with probability at least 1 − δ. Combining this fact with
Eqn. 8 completes the proof.

A.3.3. ΠB FOR THE EXAMPLE OF FIGURE 1

In this section we consider scope to derive ΠB in closed-
form for the example of Fig. 1. The following will be used:
Proposition 1 (Prop. 1 in Shi et al. (2009)). Let X = R,
Π = N(µ, σ2) and k(x, x(cid:48)) = exp(−(x − x(cid:48))2/(cid:96)2). De-
ﬁne β = 4σ2/(cid:96)2 and denote the jth Hermite polynomial as
Hj(x). Then the eigenvalues µj and corresponding eigen-
functions ej of the integral operator Σ are

2
√

(cid:16)

×

β
√

(cid:17)j

(1 + β +

1 + 2β)

1 + β +

1 + 2β

(cid:115)

µj =

and

(9)

ej(x) =

(1 + 2β)1/8
(cid:112)2jj!

(cid:16)

exp

−

(x − µ)2
2σ2

√

1 + 2β − 1
2

(cid:17)

(cid:17)1/4 x − µ

(cid:17)

σ

×Hj

(cid:16)(cid:16) 1
4

+

β
2

for some g ∈ L2(Π), in which case (cid:107)f (cid:107)H is equal to the in-
ﬁmum of (cid:107)g(cid:107)L2(Π) under all such representations g. In par-
ticular, it follows that (cid:107)f (cid:107)H = 1 for the particular choice
with g(x) = 1 for all x ∈ X .

for j ∈ {0, 1, 2, . . . }.

On the Sampling Problem for Kernel Quadrature

Proposition 2 (Ex. 6.8 in Temme (1996), p.167). The bi-
linear generating function for Hermite polynomials is

∞
(cid:88)

j=0

tj
j!

Hj(x)Hj(z)

=

√

1
1 − 4t2

(cid:18)

exp

x2 −

(x − 2zt)2
1 − 4t2

(cid:19)

.

Proposition 3. For the example in Fig. 1 we have

πB(x; λ) ∝

exp(−x2)

1
1 + λ2j+1

1
2jj!

H 2
j

(cid:16)(cid:114) 3
2

(cid:17)

x

.

∞
(cid:88)

j=0

Proof. For the example of Fig. 1, in the notation of Prop.
1, we have µ = 0, σ = 1, (cid:96) = 1 and β = 4. Thus

µj =

(cid:17)j+1

(cid:16) 1
2
√

ej(x)2 =

3 exp(−x2)

1
2jj!

H 2
j

(cid:16)(cid:114) 3
2

(cid:17)

x

and so

πB(x; λ) ∝

(cid:88)

j

µj
µj + λ

e2
j (x)

∝ exp(−x2)

1
1 + λ2j+1

1
2jj!

H 2
j

(cid:16)(cid:114) 3
2

(cid:17)

x

∞
(cid:88)

j=0

as required.

To the best of our knowledge, the expression for ΠB in
Prop. 3 does not admit a closed form. This poses a prac-
tical challenge. However, some limited insight is available
through basic approximations:

• For large values of λ we have 1 + λ2j+1 ≈ λ2j+1 for

all j ∈ {0, 1, 2, . . . }, from which we obtain

πB(x; λ) ∝

∼ exp(−x2)

∞
(cid:88)

1
4jj!

H 2
j

(cid:16)(cid:114) 3
2

(cid:17)

x

j=0
∝ exp(−x2) exp(x2) = 1,

Figure 6. Numerical approximation of ΠB for the running illustra-
tion. Here the regularisation parameter was λ = 10−15.

1 + λ2j+1 ≈ 1. Thus we have a computable approxi-
mation

πB(x; λ) ∝

∼ exp(−x2)

1
2jj!

H 2
j

(cid:16)(cid:114) 3
2

(cid:17)

x

m
(cid:88)

j=0

where m = (cid:100)− log2(λ)(cid:101). Empirical results (not
shown) indicate that this is not a useful approximation
from a practical standpoint, since at ﬁnite m the tails
of the approximation are explosive (due to the use of
a polynomial basis).

The approximation method in Bach (2015) was also used
to obtain the numerical approximation to ΠB shown in Fig.
6. This appears to support the intuition that it is beneﬁcial
to over-sample from the tails of Π.

To ﬁnish, we remark that Prop. 3 implies that the integra-
tion error in this example scales as

√

µn ∼ 2−n/2

as n → ∞ when samples are drawn from ΠB. This agrees
with both intuition and empirical results that concern ap-
proximation with exponentiated quadratic kernels.

A.3.4. ADDITIONAL THEORETICAL MATERIAL

As mentioned in the Main Text,
en({xj}n

j=1) can be computed in closed form:

the worst-case error

en({xj}n

j=1)2 = Π ⊗ Π(k) − 2w(cid:62)Kz + w(cid:62)Kw

where the second step made use of Prop. 2. Thus
when large integration errors are tolerated, ΠB re-
quires that we take the states xi to be approximately
uniform over X (of course, this limiting distribution is
improper and serves only for illustration).

Here we have deﬁned
(cid:90) (cid:90)

Π ⊗ Π(k) =

k(x, x(cid:48)) Π ⊗ Π(dx × dx(cid:48))

X ×X

• For small values of λ, the series in Prop. 3 is dom-
inated by the ﬁrst m terms such that j < m if and
only if λ2j+1 < 1.
Indeed, for j ≤ m we have

where Π ⊗ Π is the product measure of Π with itself.

Next, we report a result which does not address KQ itself,
but considers importance sampling methods for integration

On the Sampling Problem for Kernel Quadrature

of functions in a Hilbert space. The following is due to
Plaskota et al. (2009); Hinrichs (2010) and we provide an
elementary proof of their result:

Theorem 3. The assumptions of Sec. 2.4 are taken to hold.
In addition, we assume that distributions Π, Π(cid:48) admit den-
sities π, π(cid:48). Introduce importance sampling estimators of
the form

ˆΠIS(f ) =

n
(cid:88)

1
n

f (xi)

π(xi)
π(cid:48)(xi)

,

i=1
where x1, . . . , xn ∼ Π(cid:48) are independent, and consider the
distribution Π(cid:48) that minimises

(cid:113)

sup
f ∈F

E[ ˆΠIS(f ) − Π(f )]2.

For F = {f } we have that Π(cid:48) is π(cid:48)(x) ∝ |f (x)|π(x),
while for F = {f ∈ H : (cid:107)f (cid:107)H ≤ 1} we have that Π(cid:48) is
π(cid:48)(x) ∝ (cid:112)k(x, x)π(x).

Proof. The ﬁrst result, for F = {f } is well-known; e.g.
Thm. 3.3.4 in Robert and Casella (2013).

For the second case, where F is the unit ball in H, we start
by establishing a (tight) upper bound for the supremum of
f 2 over f ∈ F:

|f (x)| = (cid:12)

(cid:12)
(cid:12)

(cid:12)(cid:104)f, k(·, x)(cid:105)H
≤ (cid:107)f (cid:107)H(cid:107)k(·, x)(cid:107)H
= (cid:107)f (cid:107)H

(cid:112)(cid:104)k(·, x), k(·, x)(cid:105)H
(cid:112)k(x, x)

= (cid:107)f (cid:107)H

where the inequality here is Cauchy-Schwarz. Squaring
both sides and taking the supremum over f ∈ F gives

sup
f ∈F

f (x)2 ≤ sup
f ∈F

(cid:107)f (cid:107)2

H k(x, x) = k(x, x).

(10)

This is in fact an equality, since for given x ∈ X we can
take f (x(cid:48)) = k(x(cid:48), x)/(cid:112)k(x, x) which has (cid:107)f (cid:107)H = 1
and f (x)2 = k(x, x).

Our objective is expressed as

(cid:113)

sup
f ∈F

E[ ˆΠIS(f ) − Π(f )]2 = sup
f ∈F

1
√
n

Std

(cid:16) f π

π(cid:48) ; Π(cid:48)(cid:17)

and since

Std

(cid:16) f π

π(cid:48) ; Π(cid:48)(cid:17)2

= Π(cid:48)(cid:16)(cid:16) f π
π(cid:48)

(cid:17)2(cid:17)

(cid:17)2

− Π(cid:48)(cid:16) f π
π(cid:48)

Combining Eqns. 10 and A.3.4, we have

Π(cid:48)(cid:16)(cid:16) f π
π(cid:48)

sup
f ∈F

(cid:17)2(cid:17)

≤ Π(cid:48)(cid:16)

sup
f ∈F

(cid:17)2(cid:17)

(cid:16) f π
π(cid:48)
(cid:16) π(·)
π(cid:48)(·)

(cid:17)2(cid:17)

= Π(cid:48)(cid:16)

k(·, ·)

As before, this is in fact an equality, as can be seen from
f (x) = (cid:112)k(x, x).

From Jensen’s inequality,

Π(cid:48)(cid:16)

k(·, ·)

(cid:17)2(cid:17)

(cid:16) π(·)
π(cid:48)(·)

(cid:16)

Π(cid:48)(cid:16)(cid:112)k(·, ·)

≥

(cid:17)(cid:17)2

(11)

π(·)
π(cid:48)(·)

= (cid:0)Π(cid:0)(cid:112)k(·, ·)(cid:1)(cid:1)2

.

Since the right hand side is independent of Π(cid:48), a choice of
Π(cid:48) for which Eqn. 11 is an equality must be a minimiser of
Eqn. A.3.4. It remains just to verify this fact for π(cid:48)(x) =
(cid:112)k(x, x)π(x)/C, where the normalising constant is C =
Π((cid:112)k(·, ·)). For this choice

Π(cid:48)(cid:16)

k(·, ·)

(cid:17)2(cid:17)

(cid:16) π(·)
π(cid:48)(·)

= Π(cid:48)(C 2)

= (Π((cid:112)k(·, ·)))2

as required.

A.4. Implementation of test(R < Rmin)

Here we provide details for how the criterion R < Rmin
was tested. The problem with the naive approach of com-
paring R estimated at ti−1 directly with R estimated at ti is
that Monte Carlo error can lead to an incorrect impression
that R is increasing, when it is in fact decreasing, and cause
the algorithm to terminate when estimation is poor (see Fig.
7 and note the jaggedness of the estimated R curve as a
function of inverse temperature t). Our solution was to ap-
ply a least-squares linear smoother to the estimates for R
over 5 consecutive temperatures. This approach, denoted
test, illustrated in Fig. 7, determines whether the gradi-
ent of the linear smoother is positive or negative, and in this
way we are able to provide robustness to Monte Carlo error
in the termination criterion. To be precise, the algorithm
requires at least 5 temperature evaluations before termina-
tion is considered (Fig. 7; left) and terminates when the
gradient of the linear smoother becomes positive for the
ﬁrst time (Fig. 7; right). The success of this strategy was
established in Fig. 9 later in the Appendix.

we thus aim to minimise

A.5. Experimental Results

(cid:17)2(cid:17)

Π(cid:48)(cid:16)(cid:16) f π
π(cid:48)

sup
f ∈F

over Π(cid:48) ∈ P(F · dΠ/dΠ(cid:48)). (Here F · dΠ/dΠ(cid:48) denotes the
set of functions of the form f · dΠ/dΠ(cid:48) such that f ∈ F.)

A.5.1. IMPLEMENTATION OF SIMULATION STUDY

Denote by N(x|µ, Σ) the p.d.f. of the multivariate Gaus-
sian distribution with mean µ and covariance Σ. Further-
more, we denote by Σσ the diagonal covariance matrix

On the Sampling Problem for Kernel Quadrature

Figure 7. Implementation of test(R < Rmin). A linear smoother (dashed line) was based on 5 consecutive (inverse) temperature
parameters ti−4, ti−3, ti−2, ti−1, ti. To begin it is required that 5 temperatures are considered (left panel). The algorithm terminates on
the ﬁrst occasion when the linear smoother takes a positive gradient (right panel).

with diagonal element σ2. Then elementary manipulation
of Gaussian densities produces:

k(x, y)

(cid:1)2

(cid:17)

:= exp
√

= (

(cid:16)

−

j=1

(cid:80)d

(cid:0)xj − yj
l2
πl)dφ(cid:0)x|y, Σl/
j=1(xj − yj)2

√

(cid:1)

2

2 (cid:80)d

∇lk(x, y)

:=

Π[k(·, x)]

:= (

Π ⊗ Π(k)

:= (

√

√

l3

k(x, y)
πl)dN(cid:0)x|0, Σσ + Σl/
πl)dN(cid:0)0|0, Σ√

2
2σ + Σl/

√

(cid:1)

(cid:1)

√

2

A.5.2. DEPENDENCE ON PARAMETERS FOR THE

SIMULATION STUDY

For the running illustration with f (x) = 1 + sin(x),
Π = N(0, 1), Π(cid:48) = N(0, σ2) and k(x, x(cid:48)) = exp(−(x −
x(cid:48))2/(cid:96)2), we explored how the RMSE of KQ depends on
the choice of both σ and (cid:96). Here we go beyond the re-
sults presented in Fig. 2, which considered ﬁxed n, to now
consider the simultaneous choice of both σ, (cid:96) for varying
n. Note that in these numerical experiments the kernel ma-
trix inverse K−1 was replaced with the regularised inverse
(K + λI)−1 that introduces a small ‘nugget’ term λ > 0
for stabilisation. Results, shown in Fig. 8, demonstrate two
principles that guided the methodological development in
this paper:

• Length scales (cid:96) that are ‘too small’ to learn from n
samples do not permit good approximations ˆf and
lead in practice to high RMSE. At the same time, if
(cid:96) is taken to be ‘too large’ then efﬁcient approxima-
tion at size n will also be sacriﬁced. This is of course
well understood from a theoretical perspective and is
borne out in our empirical results. These results moti-
vated extension of SMC-KQ to SMC-KQ-KL.

• In general the ‘sweet spot’, where σ and (cid:96) lead to min-
imal RMSE, is quite small. However, the problem of
optimal choice for σ and (cid:96) does not seem to become

more or less difﬁcult as n increases. This suggests
that a method for selection of σ (and possibly also of
(cid:96)) ought to be effective regardless of the number n of
states that will be used.

A.5.3. ADDITIONAL RESULTS FOR THE SIMULATION

STUDY

To understand whether the termination criterion of Sec. 3.5
was suitable (and, by extension, to examine the validity of
the convexity ansatz in Sec. 3.2), in Fig. 9 we presented
histograms for both estimated and actual optimal (inverse)
temperature parameter t∗. Results supported the use of the
criterion, in the form described above for test.

In Fig. 10 reports the dependence of performance on the
choice of initial distribution Π0. There was relatively lit-
tle inﬂuence on the RMSE obtained by the method for this
wide range of initial distribution, which supports the pur-
ported robustness of the method.

We also test the method on more complex integrands in Fig.
11: f (x) = 1 + sin(4πx) and f (x) = 1 + sin(8πx). These
are more challenging for KQ compared to the illustration
in the Main Text, since they are more difﬁcult to interpo-
late due to their higher periodicity. However, SMC-KQ still
manages to adapt to the complexity of the integrand and
performs as well as the best importance sampling distribu-
tion (σ = 2).

As an extension, we also study the robustness to the dimen-
sionality to the problem. In problem, we consider the gen-
eralisation of our main test function to f : Rd → R given
by f (x) = 1 + (cid:81)d
j=1 sin(2πxj). Notice that the integral
can still be computed analytically and equals 1. We present
results for d = 2 and d = 3 in Fig. 12. These two cases are
more challenging for both the KQ and SMC-KQ methods,
since the higher dimension implies a slower convergence
rate. Once again, we notice that SMC-KQ manages to adapt
to the complexity of the problem at hand, and provides im-
proved performance on simpler sampling distributions.

On the Sampling Problem for Kernel Quadrature

Figure 9. Histograms for the optimal (inverse) temperature pa-
rameter t∗. Left: Estimate of t∗ provided under the termination
criterion of Sec. 3.5. Right: Estimate of t∗ obtained by estimat-
ing R over a grid for t ∈ [0, 1] and returning the global minimum.
The similarity of these histograms is supportive of the convexity
ansatz in Sec. 3.2.

Figure 8. Example of Fig. 2, continued. Here we consider the
simultaneous choice of sampling standard deviation σ and kernel
length-scale (cid:96), reporting empirical estimates for the estimated root
mean square integration error (over M = 300 repetitions) in each
case for sample size (a) n = 25 (top), (b) n = 50 (middle) and
(c) n = 75 (bottom).

Finally, we considered replacing the independent samples
xj ∼ Π with samples drawn from a quasi-random point se-
quence. Fig. 13 reports results where draws from N(0, 1)
were produced based on a Halton quasi-random number
In this case, the performance is improved by
generator.
up to 10 orders of magnitude in MSE when the sampling is
done with respect to a range of tempered sampling distribu-
tion (here N(0, 32)). This suggests that a SQMC approach
(Gerber and Chopin, 2015) could provide further improve-
ment and this suggested for future work.

Figure 10. Comparison of the performance of SMC-KQ on the
running illustration of Figs. 1 and 2 for varying initial distribu-
tion Π0 = N(0, σ2).

A.5.4. IMPLEMENTATION OF STEIN’S METHOD

Following Oates et al. (2017) we considered the Stein op-
erator

S[f ](θ) := [∇θ + ∇ log π(θ)][f ](θ)

and denote the score function by uj(θ) = ∇θj log π(θ).
Here π is the p.d.f. for Π. Applying the Stein operator to
each argument of a base kernel kb, and adding a constant,
gives produces the new kernel:

k(θ, φ)

:= 1 +

d
(cid:88)

j=1

[∇θj ∇φj kb(θ, φ)

+uj(θ)∇φj kb(θ, φ)
+uj(φ)∇θj kb(θ, φ)
+uj(θ)uj(φ)kb(θ, φ)]

which we will use for our KQ estimator. Using integration
by parts, we can easily check that Π[k(·, θ)] = 1 and Π ⊗
Π(k) = 1. In this experiment, the base kernel was taken to
be Gaussian: kb(θ, φ) = exp(− (cid:80)d
j ). We

j=1(θj − φj)2/(cid:96)2

On the Sampling Problem for Kernel Quadrature

Figure 11. Performance of KQ and SMC-KQ on the integration
problem with f (x) = 1 + sin(4πx) (top) and f (x) = 1 +
sin(8πx) (bottom) integrated against N(0, 1). The SMC sam-
pler was initiated with a N(0, 82) distribution. The kernel used
was Gaussian with length scales (cid:96) = 0.25 (top) and (cid:96) = 0.15
(bottom) each chosen to reﬂect the complexity of the functions.

obtained the derivatives:

dk(θ, φ)
dθj
dk(θ, φ)
dφj

dk(θ, φ)
dθjdφj

2
(cid:96)2
j

2
(cid:96)2
j
(cid:0)2(cid:96)2

=

=

= −

(θj − φj)k(θ, φ)

(θj − φj)k(θ, φ)

j − 4(θj − φj)2(cid:1)
(cid:96)4
j

k(θ, φ)

Furthermore, we can obtain expressions for the score func-
tion for posterior densities as follows:

uj(θ) =

log π(θ) +

log π(y|θ).

d
dθj

d
dθj

A.6. Algorithms and Implementation

A.6.1. SMC SAMPLER

In Alg. 2 the standard SMC scheme is presented. Re-
sampling occurs when the effective sample size, (cid:107)w(cid:107)−2

2

Figure 12. Performance of KQ and SMC-KQ on the integration
problem with f (x) = 1 + (cid:81)d
j=1 sin(2πxj) integrated against a
N(0, I) distribution for d = 2 (top), d = 3 (middle) and d = 10
(bottom). The SMC sampler was initiated with a N(0, 82I) dis-
tribution. The kernel used was a (multivariate) Gaussian kernel
k(x, y) = exp(− (cid:80)d
j=1(xj − yj)2/(cid:96)2
j ) with the length scales
(cid:96)1 = · · · = (cid:96)d = 0.25 were used.

drops below a fraction ρ of the total number N of particles.
In this work we took ρ = 0.95 which is a common default.

On the Sampling Problem for Kernel Quadrature

Algorithm 3 Markov Iteration

function Markov(x, π, {(wj, xj)}N
input x (current state)
input π (density of invar. dist.)
x∗ ∼ q(x, x∗; {(wj, xj)}N

j=1) (propose)

j=1)

r ←

πi(x∗)q(x∗, x; {(wj, xj)}N
πi(x)q(x, x∗; {(wj, xj)}N

j=1)
j=1)

u ∼ Unif(0, 1)
if u < r then

x ← x∗ (accept)

end ifreturn x (next state)

Figure 13. Comparison between KQ with xj ∼ N(0, 1) indepen-
dent and KQ with xj = Φ−1(uj) where the {uj}n
j=1 are the ﬁrst
n terms in the Halton sequence and Φ is the standard Gaussian
cumulative density function.

Algorithm 2 Sequential Monte Carlo Iteration

j=1, ti, ti−1, ρ)
j=1 (particle approx. to Πi−1)

function SMC({(wj, xj)}N
input {(wj, xj)}N
input ti (next inverse-temperature)
input ti−1 (previous inverse-temperature)
input ρ (re-sample threshold)
w(cid:48)
w(cid:48) ← w(cid:48)/(cid:107)w(cid:48)(cid:107)1 (normalise weights)
if (cid:107)w(cid:48)(cid:107)−2

j ← wj × [π(xj)/π0(xj)]ti−ti−1 (∀j ∈ 1 : N )

2 < N · ρ then

j ← xa(j) (re-sample ∀j ∈ 1 : N )
j ← N −1 (reset weights ∀j ∈ 1 : N )

a ∼ Multinom(w(cid:48))
x(cid:48)
w(cid:48)
end if
x(cid:48)
j ∼ Markov(x(cid:48)
∈ 1 : N )
return {(w(cid:48)

j)}N

j, x(cid:48)

j; Πi, {(wj, xj)}N

j=1) (Markov update

j=1 (particle approx. to Πi)

Denote

q(x, ·; {(wj, xj)}N

j=1) = N(·; µ, Σ)

µ =

wjxj

N
(cid:88)

j=1

N
(cid:88)

j=1

Σ =

wj(xj − µ)(xj − µ)(cid:62).

A.6.2. CHOICE OF TEMPERATURE SCHEDULE

Following Zhou et al. (2016) we employed an adaptive tem-
perature schedule construction. This was based on the con-
ditional effective sample size of the SMC particle set, esti-
mated as follows:

Algorithm 4 Conditional Effective Sample Size

function CESS({(wj, xj)}N
j=1, t)
input {(wj, xj)}N
j=1 (particle approx. Πi−1)
input t (candidate next inverse-temperature)
zj ← [π(xj)/π0(xj)]ti−ti−1 (∀j ∈ 1 : N )
(cid:17)

(cid:17)2 (cid:14) (cid:16)(cid:80)N

(cid:16)(cid:80)N

E ← N
return E (est’d. cond. ESS)

j=1 wjzj

j=1 wjz2
j

The speciﬁc construction for the temperature schedule is
detailed in Alg. 5 below and makes use of a Sequential
Least Squares Programming algorithm:

Algorithm 5 Adaptive Temperature Iteration
j=1, ti−1, ρ, ∆)

j=1 (particle approx. Πi−1)

function temp({(wj, xj)}N
input {(wj, xj)}N
input ti−1 (current inverse-temperature)
input ρ (re-sample threshold)
input ∆ (max. grid size, default ∆ = 0.1)
t ← solve(CESS({(wj, xj)}N
(binary search in [ti−1, 1])
ti ← min{ti−1 + ∆, t} return ti (next
temperature)

j=1, t) = N · ρ)

inverse-

The above standard adaptive independence proposal was
used within a Metropolis-Hastings Markov transition:

A.6.3. TERMINATION CRITERION

For SMC-KQ we estimated an upper bound on the worst
case error in the unit ball of the Hilbert space H. This was
computed as follows, using a bootstrap algorithm:

On the Sampling Problem for Kernel Quadrature

Algorithm 6 Termination Criterion
function crit(Π, k, {xj}N
j=1)
input Π (target disn.)
input k (kernel)
input {xj}N
R2 ← 0
e0 ← (cid:82)(cid:82)
for m = 1,. . . ,M do

j=1 (collection of states)

X ×X k(x, x(cid:48))Π ⊗ Π(dx × dx(cid:48)) (in’l error)

j=1) (∀j ∈ 1 : n)

˜xj ∼ Unif({xj}N
zj ← (cid:82)
X k(·, ˜xj)dΠ (k’l mean eval. ∀j ∈ 1 : n)
Kj,j(cid:48) ← k( ˜xj, ˜xj(cid:48)) (kernel eval. ∀j, j(cid:48) ∈ 1 : n)
w ← zT K−1 (KQ weights)
e2
n ← w(cid:62)Kw − 2w(cid:62)z + e2
0
R2 ← R2 + e2

nM −1

end for
return R (est’d error)

A.6.4. KERNEL LEARNING

A generic approach to select kernel parameters is the max-
imum marginal likelihood method:

Algorithm 8 Parameter Update

j=1, kθ)

function kern-param(f , {xj}n
input f (integrand evals.)
input {xj}n
input kθ (parametric kernel)
θ(cid:48) ← arg minθ f (cid:62)K−1
(s.t. Kθ,j,j(cid:48) = kθ(xj, xj(cid:48))) return θ(cid:48) (optimal params)

θ f + log |Kθ| (numer. opt.)

j=1 (associated states)

A.6.5. IMPLEMENTATION OF SMC-KQ-KL

Our ﬁnal algorithm to present is the full implementation for
SMC-KQ-KL:

Note that this could be slightly improved using a weighted
bootstrap approach.

For SMC-KQ-KL an empirical upper bound on integration
error was estimated. This requires that the norm (cid:107)f (cid:107)H be
estimated, which was achieved as follows:

Algorithm 7 Termination Crit. + Kernel Learning

j=1)

function crit-KL(f, Π, k, {xj}N
input f (integrand)
input Π (target disn.)
input k (kernel)
input {xj}N
R2 ← 0
e0 ← (cid:82)(cid:82)
for m = 1,. . . ,M do

j=1 (collection of states)

X ×X k(x, x(cid:48))Π ⊗ Π(dx × dx(cid:48)) (in’l error)

j=1) (∀j ∈ 1 : n)

˜xj ∼ Unif({xj}N
fj ← f ( ˜xj) (function eval. ∀j ∈ 1 : n)
zj ← (cid:82)
X k(·, ˜xj)dΠ (k’l mean eval. ∀j ∈ 1 : n)
Kj,j(cid:48) ← k( ˜xj, ˜xj(cid:48)) (kernel eval. ∀j, j(cid:48) ∈ 1 : n)
w ← zT K−1 (KQ weights)
e2
n ← w(cid:62)Kw − 2w(cid:62)z + e2
0
R2 ← R2 + e2

nM −1

X k(·, xj)dΠ (kernel mean eval. ∀j ∈ 1 : n)

end for
zj ← (cid:82)
Kj,j(cid:48) ← k(xj, xj(cid:48)) (kernel eval. ∀j, j(cid:48) ∈ 1 : n)
w ← zT K−1 (KQ weights)
S2 ← R2 × w(cid:62)Kw return S (est’d error bound)

In Alg. 7 the literal interpretation, that f is re-evaluated
on values of xj which have been previously examined, is
In practice such function evaluations
clearly inefﬁcient.
were cached and then do not contribute further to the to-
tal number of function evaluations that are required in the
algorithm.

On the Sampling Problem for Kernel Quadrature

Algorithm 9 SMC for KQ with Kernel Learning
function SMC-KQ-KL(f, Π, kθ, Π0, ρ, n, N )
input f (integrand)
input Π (target disn.)
input kθ (parametric kernel)
input Π0 (reference disn.)
input ρ (re-sample threshold)
input n (num. func. evaluations)
input N (num. particles)
i ← 0; ti ← 0; Rmin ← ∞
x(cid:48)
w(cid:48)
θ(cid:48) ← kern-param(f, {x(cid:48)
j}n
R ← crit-KL(f, Π, kθ(cid:48), {x(cid:48)
while test(R < Rmin) and ti < 1 do

j ∼ Π0 (initialise states ∀j ∈ 1 : N )
j ← N −1 (initialise weights ∀j ∈ 1 : N )

j=1) (kernel params)
j}N
j=1) (est’d error)

j, x(cid:48)
j=1, ti−1) (next temp.)

j=1

j=1 ← SMC({(wj, xj)}N

j=1, ti, ti−1, ρ)

j=1 ← {(w(cid:48)

i ← i + 1; Rmin ← R; θ ← θ(cid:48)
{(wj, xj)}N
j)}N
ti ← temp({(wj, xj)}N
{(w(cid:48)
j)}N
(next particle approx.)
θ(cid:48) ← kern-param(f, {x(cid:48)
j}n
R ← crit-KL(f, Π, kθ(cid:48), {x(cid:48)

j, x(cid:48)

j=1) (kernel params)
j}N
j=1) (est’d error)

X kθ(·, xj)dΠ (kernel mean eval. ∀j ∈ 1 : n)

end while
fj ← f (xj) (function eval. ∀j ∈ 1 : n)
zj ← (cid:82)
Kj,j(cid:48) ← kθ(xj, xj(cid:48)) (kernel eval. ∀j, j(cid:48) ∈ 1 : n)
ˆΠ(f ) ← z(cid:62)K−1f (eval. KQ estimator) return ˆΠ(f )
(estimator)

As stated here, Alg. 9 is inefﬁcient as function evaluations
that are produced in the kern-param and crit-KL
components are not included in the KQ estimator ˆΠ(f ).
Thus a trivial modiﬁcation is to store all function evalua-
tions (fj, xj) that are produced and to include all of these
in the ultimate KQ estimator. This was the approach taken
in our experiments that involved SMC-KQ-KL. However,
since it is somewhat cumbersome to include in the pseudo-
code, we have not made this explicit in the notation. Our
reported results are on a per-function-evaluation basis and
so we do adjust for this detail in our reported comparisons.

