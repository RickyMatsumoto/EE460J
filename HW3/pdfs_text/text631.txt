Batched High-dimensional Bayesian Optimization
via Structural Kernel Learning

Zi Wang * 1 Chengtao Li * 1 Stefanie Jegelka 1 Pushmeet Kohli 2

Abstract

Optimization of high-dimensional black-box
functions is an extremely challenging problem.
While Bayesian optimization has emerged as
a popular approach for optimizing black-box
functions, its applicability has been limited to
low-dimensional problems due to its compu-
tational and statistical challenges arising from
high-dimensional settings. In this paper, we pro-
pose to tackle these challenges by (1) assuming
a latent additive structure in the function and in-
ferring it properly for more efﬁcient and effec-
tive BO, and (2) performing multiple evaluations
in parallel to reduce the number of iterations re-
quired by the method. Our novel approach learns
the latent structure with Gibbs sampling and con-
structs batched queries using determinantal point
processes. Experimental validations on both syn-
thetic and real-world functions demonstrate that
the proposed method outperforms the existing
state-of-the-art approaches.

1. Introduction

Optimization is one of the fundamental pillars of modern
machine learning. Considering that most modern machine
learning methods involve the solution of some optimization
problem, it is not surprising that many recent breakthroughs
in this area have been on the back of more effective tech-
niques for optimization. A case in point is deep learning,
whose rise has been mirrored by the development of nu-
merous techniques like batch normalization.

While modern algorithms have been shown to be very

*Equal contribution

1Computer Science and Artiﬁcial In-
Institute of Technol-
telligence Laboratory, Massachusetts
ogy, Massachusetts, USA 2DeepMind, London, UK. Cor-
respondence to: Zi Wang <ziw@csail.mit.edu>, Chengtao
Li <ctli@mit.edu>, Stefanie Jegelka <stefje@csail.mit.edu>,
Pushmeet Kohli <pushmeet@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

effective for convex optimization problems deﬁned over
continuous domains, the same cannot be stated for non-
convex optimization, which has generally been dominated
by stochastic techniques. During the last decade, Bayesian
optimization has emerged as a popular approach for opti-
mizing black-box functions. However, its applicability is
limited to low-dimensional problems because of computa-
tional and statistical challenges that arise from optimization
in high-dimensional settings.

In the past, these two problems have been addressed by
assuming a simpler underlying structure of the black-box
function. For instance, Djolonga et al. (2013) assume that
the function being optimized has a low-dimensional effec-
tive subspace, and learn this subspace via low-rank matrix
recovery. Similarly, Kandasamy et al. (2015) assume ad-
ditive structure of the function where different constituent
functions operate on disjoint low-dimensional subspaces.
The subspace decomposition can be partially optimized by
searching possible decompositions and choosing the one
with the highest GP marginal likelihood (treating the de-
composition as a hyper-parameter of the GP). Fully opti-
mizing the decomposition is, however, intractable. Li et al.
(2016) extended (Kandasamy et al., 2015) to functions with
a projected-additive structure, and approximate the projec-
tive matrix via projection pursuit with the assumption that
the projected subspaces have the same and known dimen-
sions. The aforementioned approaches share the computa-
tional challenge of learning the groups of decomposed sub-
spaces without assuming the dimensions of the subspaces
are known. Both (Kandasamy et al., 2015) and subse-
quently (Li et al., 2016) adapt the decomposition by maxi-
mizing the GP marginal likelihood every certain number of
iterations. However, such maximization is computationally
intractable due to the combinatorial nature of the partitions
of the feature space, which forces prior work to adopt ran-
domized search heuristics.

In this paper, we develop a new formulation of Bayesian
optimization specialized for high dimensions. One of the
key contributions of this work is a new formulation that
interprets prior work on high-dimensional Bayesian opti-
mization (HDBO) through the lens of structured kernels,
and places a prior on the kernel structure. Thereby, our

Batched High-dimensional Bayesian Optimization via Structural Kernel Learning

formulation enables simultaneous learning of the decom-
position of the function domain.

Prior work on latent decomposition of the feature space
considers the setting where exploration/evaluation is per-
formed once at a time. This approach makes Bayesian
optimization time-consuming for problems where a large
number of function evaluations need to be made, which is
the case for high dimensional problems. To overcome this
restriction, we extend our approach to a batched version
that allows multiple function evaluations to be performed
in parallel (Desautels et al., 2014; Gonz´alez et al., 2016;
Kathuria et al., 2016). Our second contribution is an ap-
proach to select the batch of evaluations for structured ker-
nel learning-based HDBO.

Other Related Work.
In the past half century, a series
of different acquisition functions was developed for se-
quential BO in relatively low dimensions (Kushner, 1964;
Mo˘ckus, 1974; Srinivas et al., 2012; Hennig & Schuler,
2012; Hern´andez-Lobato et al., 2014; Kawaguchi et al.,
2015; Wang et al., 2016a; Kawaguchi et al., 2016; Wang
& Jegelka, 2017). More recent developments address
high dimensional BO by making assumptions on the la-
tent structure of the function to be optimized, such as low-
dimensional structure (Wang et al., 2016b; Djolonga et al.,
2013) or additive structure of the function (Li et al., 2016;
Kandasamy et al., 2015). Duvenaud et al. (2013) explicitly
search over kernel structures.

While the aforementioned methods are sequential in na-
ture, the growth of computing power has motivated settings
where at once a batch of points is selected for observa-
tion (Contal et al., 2013; Desautels et al., 2014; Gonz´alez
et al., 2016; Snoek et al., 2012; Wang et al., 2017). For
example, the UCB-PE algorithm (Contal et al., 2013) ex-
ploits that the posterior variance of a Gaussian Process
is independent of the function mean.
It greedily selects
points with the highest posterior variance, and is able to up-
date the variances without observations in between selec-
tions. Similarly, B-UCB (Desautels et al., 2014) greedily
chooses points with the highest UCB score computed via
the out-dated function mean but up-to-date function vari-
ances. However, these methods may be too greedy in their
selection, resulting in points that lie far from an optimum.
More recently, Kathuria et al. (2016) tries to resolve this is-
sue by sampling the batch via a diversity-promoting distri-
bution for better randomized exploration, while Wang et al.
(2017) quantiﬁes the goodness of the batch with a submod-
ular surrogate function that trades off quality and diversity.

2. Background

Let f : X → R be an unknown function and we aim to
optimize it over a compact set X ⊆ RD. Within as few

function evaluations as possible, we want to ﬁnd

f (x∗) = max
x∈X

f (x).

Following (Kandasamy et al., 2015), we assume a latent de-
composition of the feature dimensions [D] = {1, . . . , D}
into disjoint subspaces, namely, (cid:83)M
m=1 Am = [D] and
Ai ∩ Aj = ∅ for all i (cid:54)= j, i, j ∈ [D]. Further, f can
be decomposed into the following additive form:

f (x) =

(cid:88)

fm(xAm ).

m∈[M ]

To make the problem tractable, we assume that each fm is
drawn independently from GP(0, k(m)) for all m ∈ [M ].
The resulting f will also be a sample from a GP: f ∼
GP(µ, k), where the priors are µ(x) = (cid:80)
m∈[M ] µm(xAm )
and k(x, x(cid:48)) = (cid:80)
m∈[M ] k(m)(xAm, x(cid:48)Am). Let Dn =
{(xt, yt)}n
t=1 be the data we observed from f , where yt ∼
N (f (xt), σ). The log data likelihood for Dn is

log p(Dn|{k(m), Am}m∈[M ])

(2.1)

= −

(yT(Kn + σ2I)−1y + log |Kn + σ2I| + n log 2π)

1
2

(cid:104)(cid:80)M

i

, xAm
j

where Kn =

m=1 k(m)(xAm
gram matrix associated with Dn, and y = [yt]t≤n are the
concatenated observed function values. Conditioned on the
observations Dn, we can infer the posterior mean and co-
variance function of the function component f (m) to be

is the

i≤n,j≤n

(cid:105)
)

n (xAm)T(Kn + σ2I)−1y,

µ(m)
n (xAm ) = k(m)
n (xAm , x(cid:48)Am ) = k(m)(xAm, x(cid:48)Am )
k(m)
− k(m)
n (xAm)T(Kn + σ2I)−1k(m)

n (x(cid:48)Am),

where k(m)

n (xAm ) = [k(m)(xAm

, xAm)]t≤n.

t

(cid:80)

We use regret to evaluate the BO algorithms, both in the
sequential and the batch selection case. For the sequential
selection, let ˜rt = maxx∈X f (x) − f (xt) denote the im-
mediate regret at iteration t. We are interested in both the
averaged cumulative regret RT = 1
t ˜rt and the simple
T
regret rT = mint≤T ˜rt for a total number of T iterations.
For batch evaluations, ˜rt = maxx∈X ,b∈[B] f (x) − f (xt,b)
denotes the immediate regret obtained by the batch at iter-
ation t. The averaged cumulative regret of the batch setting
is RT = 1
t ˜rt, and the simple regret rT = mint≤T ˜rt.
T
We use the averaged cumulative regret in the bandit setting,
where each evaluation of the function incurs a cost. If we
simply want to optimize the function, we use the simple
regret to capture the minimum gap between the best point
found and the global optimum of the black-box function f .
Note that the averaged cumulative regret upper bounds the
simple regret.

(cid:80)

Batched High-dimensional Bayesian Optimization via Structural Kernel Learning

3. Learning Additive Kernel Structure

We take a Bayesian view on the task of learning the latent
structure of the GP kernel. The decomposition of the input
space X will be learned simultaneously with optimization
as more and more data is observed. Our generative model
draws mixing proportions θ ∼ DIR(α). Each dimension
j is assigned to one out of M groups via the decomposi-
tion assignment variable zj ∼ MULTI(θ). The objective
function is then f (x) = (cid:80)M
m=1 fm(xAm ), where Am =
{j : zj = m} is the set of support dimensions for function
fm, and each fm is drawn from a Gaussian Process. Fi-
nally, given an input x, we observe y ∼ N (f (x), σ). Fig-
ure 1 illustrates the corresponding graphical model.

Given the observed data Dn = {(xt, yt)}n
t=1, we obtain a
posterior distribution over possible decompositions z (and
mixing proportions θ) that we will include later in the BO
process:

p(z, θ | Dn; α) ∝ p(Dn | z)p(z | θ)p(θ; α).

Marginalizing over θ yields the posterior distribution of the
decomposition assignment

p(z | Dn; α) ∝ p(Dn | z)

p(z | θ)p(θ; α) dθ

(cid:90)

∝ p(Dn | z)

Γ((cid:80)
Γ(D + (cid:80)

m αm)

m αm)

(cid:89)

m

Γ(|Am| + αm)
Γ(αm)

where p(Dn|z) is the data likelihood (2.1) for the additive
GP given a ﬁxed structure deﬁned by z. We learn the pos-
terior distribution for z via Gibbs sampling, choose the de-
composition among the samples that achieves the highest
data likelihood, and then proceed with BO. The Gibbs sam-
pler repeatedly draws coordinate assignments zj according
to

p(zj = m | z¬j, Dn; α) ∝ p(Dn | z)p(zj | z¬j)

∝ p(Dn | z)(|Am| + αm) ∝ eφm ,

where

1
2

1
2

φm = −

yT(K(zj =m)
n

+ σ2I)−1y

−

log |K(zj =m)
n

+ σ2I| + log(|Am| + αm)

and K(zj =m)
is the gram matrix associated with the ob-
n
servations Dn by setting zj = m. We can use the Gumbel
trick to efﬁciently sample from this categorical distribution.
Namely, we sample a vector of i.i.d standard Gumbel vari-
ables ωi of length M , and then choose the sampled decom-
position assignment zj = arg maxi≤M φi + ωi.

With a Dirichlet process, we could make the model non-
parametric and the number M of possible groups in the
decomposition inﬁnite. Given that we have a ﬁxed number
of input dimension D, we set M = D in practice.

Figure 1. Graphical model for the structured Gaussian process; η
is the hyperparameter of the GP kernel; z controls the decompo-
sition for the input space.

η

x

α

θ

z

d

f

y

4. Diverse Batch Sampling

In real-world applications where function evaluations
translate into time-intensive experiments, the typical se-
quential exploration strategy – observe one function value,
update the model, then select the next observation – is un-
desirable. Batched Bayesian Optimization (BBO) (Azimi
et al., 2010; Contal et al., 2013; Kathuria et al., 2016) in-
stead selects a batch of B observations to be made in par-
allel, then the model is updated with all simultaneously.

Extending this scenario to high dimensions, two questions
arise:
(1) the acquisition function is expensive to opti-
mize and (2), by itself, does not sufﬁciently account for
exploration. The additive kernel structure improves efﬁ-
ciency for (1). For batch selection (2), we need an efﬁcient
strategy that enourages observations that are both informa-
tive and non-redundant. Recent work (Contal et al., 2013;
Kathuria et al., 2016) selects a point that maximizes the ac-
quisition function, and adds additional batch points via a
diversity criterion. In high dimensions, this diverse selec-
tion becomes expensive. For example, if each dimension
has a ﬁnite number of possible values1, the cost of sampling
batch points via a Determinantal Point Process (DPP), as
proposed in (Kathuria et al., 2016), grows exponentially
with the number of dimensions. The same obstacle arises
with the approach by Contal et al. (2013), where points
are selected greedily. Thus, na¨ıve adoptions of these ap-
proaches in our setting would result in intractable algo-
rithms.
Instead, we propose a general approach that ex-
plicitly takes advantage of the structured kernel to enable
relevant, non-redundant high-dimensional batch selection.

We describe our approach for a single decomposition sam-
pled from the posterior; it extends to a distribution of de-
compositions by sampling a set of decompositions from the
posterior and then sampling points for each decomposition
individually. Given a decomposition z, we deﬁne a sepa-
rate Determinantal Point Process (DPP) on each group of
Am dimensions. A set S of points in the subspace R|Am|
is sampled with probability proportional to det(K(m)
n (S)),

1While we use this discrete categorical domain to illustrate the
batch setting, our proposed method is general and is applicable to
continuous box-constrained domains.

Batched High-dimensional Bayesian Optimization via Structural Kernel Learning

n

where K(m)
is the posterior covariance matrix of the m-
th group given n observations, and K(S) is the submatrix
of K with rows and columns indexed by S. Assuming the
group sizes are upper-bounded by some constant, sampling
from each such DPP individually implies an exponential
speedup compared to using the full kernel.

Sampling vs. Greedy Maximization The determinant
det(K(m)
n (S)) measures diversity, and hence the DPP as-
signs higher probability to diverse subsets S. An alternative
to sampling is to directly maximize the determinant. While
this is NP-hard, a greedy strategy gives an approximate so-
lution, and is used in (Kathuria et al., 2016), and in (Contal
et al., 2013) as Pure Exploration (PE). We too test this strat-
egy in the experiments. In the beginning, if the GP is not
approximating the function well, then greedy may perform
no better than a stochastic combination of coordinates, as
we observe in Fig. 6.

i

Sample Combination Now we have chosen a diverse
subset Xm = {x(m)
}i∈[B−1] ⊂ R|Am| of size (B − 1)
for each group Am. We need to combine these subspace
points to obtain B − 1 ﬁnal batch query points in RD.
A simple way to combine samples from each group is to
do it randomly without replacement, i.e., we sample one
x(m)
from each Xm uniformly randomly without replace-
i
ment, and combine the parts, one for each m ∈ [M ], to get
one sample in RD. We repeat this procedure until we have
(B − 1) points. This retains diversity across the batch of
samples, since the samples are diverse within each group
of features.

t

Besides this random combination, we can also combine
samples greedily. We deﬁne a quality function ψ(m)
for each group m ∈ [M ] at time t, and combine sam-
ples to maximize this quality function. Concretely, for
the ﬁrst point, we combine the maximizers x(m)
=
arg maxx(m)∈Xm ψ(m)
(x(m)) from each group. We re-
t
move those used parts, Xm ← Xm\{x(m)
∗ }, and repeat the
procedure until we have (B − 1) samples.
In each iter-
ation, the sample achieving the highest quality score gets
selected, while diversity is retained.

∗

Both selection strategies can be combined with a wide
range of existing quality and acquisition functions.

Add-UCB-DPP-BBO We illustrate the above frame-
work with GP-UCB (Srinivas et al., 2012) as both the ac-
quisition and quality functions. The Upper Conﬁdence
Bound (f (m)
)−
with parameter βt for group m at time t are
t σ(m)
t σ(m)

)+ and Lower Conﬁdence Bound (f (m)

t−1(x) + β1/2
t−1(x) − β1/2

)+(x) = µ(m)
)−(x) = µ(m)

(f (m)
t
(f (m)
t

(4.1)

(x),

(x);

t

t

t

t

and combine the expected value µ(m)
uncertainty β1/2
t σ(m)
tion and quality function ψ(m)
at time t.

with the
(x). We set both the acquisition func-
)+ for group m

t−1(x) of f (m)

to be (f (m)

t

t

t

t

To ensure that we select points with high acquisition func-
tion values, we follow (Contal et al., 2013; Kathuria et al.,
2016) and deﬁne a relevance region R(m)
for each group
m as

t

R(m)

t = {x ∈ Xm |

(cid:113)

µ(m)
t−1(x) + 2

β(m)
t+1 σ(m)

t−1(x) ≥ (y(m)

t

)•

(cid:27)

,

)• = maxx(m)∈Xm(f (m)

where (y(m)
t
use R(m)
full algorithm is shown in the appendix.

)−(x(m)). We then
t
as the ground set to sample with PE/DPP. The

t

5. Empirical Results

We empirically evaluate our approach in two parts: First,
we verify the effectiveness of using our Gibbs sam-
pling algorithm to learn the additive structure of the un-
known function, and then we test our batch BO for
high dimensional problems with the Gibbs sampler. Our
code is available at https://github.com/zi-w/
Structural-Kernel-Learning-for-HDBBO.

5.1. Effectiveness of Decomposition Learning

We ﬁrst probe the effectiveness of using the Gibbs sam-
pling method described in Section 3 to learn the decompo-
sition of the input space. More details of the experiments
including sensitivity analysis for α can be found in the ap-
pendix.

Recovering Decompositions First, we sample test func-
tions from a known additive Gaussian Process prior with
zero-mean and isotropic Gaussian kernel with bandwidth
= 0.1 and scale = 5 for each function component. For
D = 2, 5, 10, 20, 50, 100 input dimensions, we randomly
sample decomposition settings that have at least two groups
in the decomposition and at most 3 dimensions in each
group.

Table 1. Empirical posterior of any two dimensions correctly be-
ing grouped together by Gibbs sampling.

N

D
5
10
20
50
100

50

150

250

450

0.81 ± 0.28
0.21 ± 0.13
0.06 ± 0.06
0.02 ± 0.03
0.01 ± 0.01

0.91 ± 0.19
0.54 ± 0.25
0.11 ± 0.08
0.02 ± 0.02
0.01 ± 0.01

1.00 ± 0.03
0.68 ± 0.25
0.20 ± 0.12
0.03 ± 0.03
0.01 ± 0.01

1.00 ± 0.00
0.93 ± 0.15
0.71 ± 0.22
0.06 ± 0.04
0.02 ± 0.02

We set the burn-in period to be 50 iterations, and the total
number of iterations for Gibbs sampling to be 100. In Ta-
bles 1 and 2, we show two quantities that are closely related

Batched High-dimensional Bayesian Optimization via Structural Kernel Learning

Figure 2. The simple regrets (rt) and the averaged cumulative regrets (Rt) for setting input space decomposition with Known, NP, FP,
PL-1, PL-2, and Gibbs on 2, 10, 20, 50 dimensional synthetic additive functions. Gibbs achieved comparable results to Known.
Comparing PL-1 and PL-2 we can see that sampling more settings of decompositions did help to ﬁnd a better decomposition. But a
more principled way of learning the decomposition using Gibbs can achieve much better performance than PL-1 and PL-2.

2

the

i<j≤D

1zg

((cid:80)
reports

j ∧zi≡zj )/((cid:80)
of

to the learned empirical posterior of the decompositions
with different numbers of randomly sampled observed data
points (N ). Table 1 shows the probability of two dimen-
sions being correctly grouped together by Gibbs sampling
in each iteration of Gibbs sampling after the burn-in period,
1zi≡zj ).
namely,
i ≡zg
Table
dimen-
probability
sions being correctly separated in each iteration of
namely,
sampling after
Gibbs
((cid:80)
The re-
i (cid:54)=zg
sults show that
the more
accurate the learned decompositions are. They also sug-
gest that the Gibbs sampling procedure can converge to the
ground truth decomposition with enough data for relatively
small numbers of dimensions. The higher the dimension,
the more data we need to recover the true decomposition.

j ∧zi(cid:54)=zj )/((cid:80)
i<j≤D
the more data we observe,

the burn-in period,

i<j≤D
two

1zi(cid:54)=zj ).

1zg

i<j≤D

Effectiveness of Learning Decompositions for Bayesian
Optimization To verify the effectiveness of the learned
decomposition for Bayesian optimization, we tested on
2, 10, 20 and 50 dimensional functions sampled from a
zero-mean Add-GP with randomly sampled decomposi-

Table 2. Empirical posterior of any two dimensions correctly be-
ing separated by Gibbs sampling.

N

D
2
5
10
20
50
100

50

150

250

450

0.30 ± 0.46
0.87 ± 0.17
0.88 ± 0.05
0.94 ± 0.02
0.98 ± 0.00
0.99 ± 0.00

0.30 ± 0.46
0.80 ± 0.27
0.89 ± 0.06
0.94 ± 0.02
0.98 ± 0.00
0.99 ± 0.00

0.90 ± 0.30
0.60 ± 0.32
0.89 ± 0.07
0.94 ± 0.02
0.98 ± 0.01
0.99 ± 0.00

1.00 ± 0.00
0.50 ± 0.34
0.94 ± 0.07
0.97 ± 0.02
0.98 ± 0.01
0.99 ± 0.00

tion settings (at least two groups, at most 3 dimensions
in each group) and isotropic Gaussian kernel with band-
width = 0.1 and scale = 5. Each experiment was re-
peated 50 times. An example of a 2-dimensional func-
tion component is shown in the appendix. For Add-GP-
UCB, we used β(m)
= |Am| log 2t for lower dimensions
(D = 2, 5, 10), and β(m)
= |Am| log 2t/5 for higher di-
mensions (D = 20, 30, 50). We show parts of the results
on averaged cumulative regret and simple regret in Fig. 2,
and the rest in the appendix. We compare Add-GP-UCB
with known additive structure (Known), no partitions (NP),
fully partitioned with one dimension for each group (FP)

t

t

t200400rt-0.200.20.40.60.8D=2KnownNPFPPL-1PL-2Gibbst200400rt010203040D=10t200400rt0102030405060D=20t200400600rt020406080100120D=50t200400Rt00.20.40.60.81D=2t200400Rt5101520253035D=10t200400Rt102030405060D=20t200400600Rt20406080100120D=50(a)(b)(c)(d)(e)(f)(g)(h)Batched High-dimensional Bayesian Optimization via Structural Kernel Learning

and the following methods of learning the decomposition:
Gibbs sampling (Gibbs), randomly sampling the same
number of decompositions sampled by Gibbs and select
the one with the highest data likelihood (PL-1), randomly
sampling 5 decompositions and selecting the one with the
highest data likelihood (PL-2). For the latter two learning
methods are referred to as “partial learning” in (Kandasamy
et al., 2015). The learning of the decomposition is done ev-
ery 50 iterations. Fig. 3 shows the improvement of learning
decompositions with Gibbs over optimizing without par-
titions (NP).

Overall, the results show that Gibbs outperforms both of
the partial learning methods, and for higher dimensions,
Gibbs is sometimes even better than Known.
Interest-
ingly, similar results can be found in Fig. 3 (c) of (Kan-
dasamy et al., 2015), where different decompositions than
the ground truth may give better simple regret. We conjec-
ture that this is because Gibbs is able to explore more than
Known, for two reasons:

1. Empirically, Gibbs changes the decompositions
across iterations, especially in the beginning. With
ﬂuctuating partitions, even exploitation leads to mov-
ing around, because the supposedly “good” points are
inﬂuenced by the partition. The result is an implicit
“exploration” effect that is absent with a ﬁxed parti-
tion.

2. Gibbs sometimes merges “true” parts into larger
parts. The parameter βt in UCB depends on the size
of the part, |Am|(log 2t)/5 (as in (Kandasamy et al.,
2015)). Larger parts hence lead to larger βt and hence
more exploration.

Of course, more exploration is not always better, but
Gibbs was able to ﬁnd a good balance between explo-
ration and exploitation, which leads to better performance.
Our preliminary experiments indicate that one solution to
ensure that the ground truth decomposition produces the
best result is to tune βt. Hyperparameter selection (such as
choosing βt) for BO is, however, very challenging and an
active topic of research (e.g. (Wang et al., 2016a)).

Next, we test the decomposition learning algorithm on a
real-world function, which returns the distance between a
designated goal location and two objects being pushed by
two robot hands, whose trajectory is determined by 14 pa-
rameters specifying the location, rotation, velocity, mov-
ing direction etc. This function is implemented with a
physics engine, the Box2D simulator (Catto, 2011). We
use add-GP-UCB with different ways of setting the addi-
tive structure to tune the parameters for the robot hand so
as to push the object closer to the goal. The regrets are
shown in Fig. 4. We observe that the performance of learn-
ing the decomposition with Gibbs dominates all existing

Figure 3. Improvement made by learning the decomposition with
Gibbs over optimizing without partitions (NP). (a) averaged cu-
mulative regret; (b) simple regret. (c) averaged cumulative regret
normalized by function maximum; (d) simple regret normalized
by function maximum. Using decompositions learned by Gibbs
continues to outperform BO without Gibbs.

alternatives including partial learning. Since the function
we tested here is composed of the distance to two objects,
there could be some underlying additive structure for this
function in certain regions of the input space, e.g. when
the two robots hands are relatively distant from each other
so that one of the hands only impacts one of the objects.
Hence, it is possible for Gibbs to learn a good underlying
additive structure and perform effective BO with the struc-
tures it learned.

Figure 4. Simple regret of tuning the 14 parameters for a robot
pushing task. Learning decompositions with Gibbs is more ef-
fective than partial learning (PL-1, PL-2), no partitions (NP),
or fully partitioned (FP). Learning decompositions with Gibbs
helps BO to ﬁnd a better point for this tuning task.

1020304050D-20020406080RtImprovementrtImprovement-200204060801020304050-20020406080-2002040608010203040501020304050DDDRtImprovement %rtImprovement %(a)(b)(c)(d)t100200300400500rt2345678910NPFPPL-1PL-2GibbsBatched High-dimensional Bayesian Optimization via Structural Kernel Learning

Figure 5. Scaled simple regrets (rt) and scaled averaged cumulative regrets (Rt) on synthetic functions with various dimensions when
the ground truth decomposition is known. The batch sampling methods (Batch-UCB-PE, Batch-UCB-DPP, Batch-UCB-PE-Fnc
and Batch-UCB-DPP-Fnc) perform comparably well and outperform random sampling (Rand) by a large gap.

5.2. Diverse Batch Sampling

Next, we probe the effectiveness of batch BO in high di-
mensions. In particular, we compare variants of the Add-
UCB-DPP-BBO approach outlined in Section 4, and a
baseline:

• Rand: All batch points are chosen uniformly at ran-

dom from X .

• Batch-UCB-*: *∈ {PE, DPP}. All acquisition
functions are UCB (Eq. 4.1). Exploration is done via
PE or DPP with posterior covariance kernels for each
group. Combination is via sampling without replace-
ment.

• *-Fnc: *∈ {Batch-UCB-PE, Batch-UCB-DPP}.
All quality functions are also UCB’s, and combination
is done by maximizing the quality functions.

A direct application of existing batch selection methods is
very inefﬁcient in the high-dimensional settings where they
differ more, algorithmically, from our approach that ex-

ploits decompositions. Hence, we only compare to uniform
sampling as a baseline.

Effectiveness We tested on 2, 10, 20 and 50-dimensional
functions sampled the same way as in Section 5.1; we as-
sume the ground-truth decomposition of the feature space
is known. Since Rand performs the worst, we show rel-
ative averaged cumulative regret and simple regret of all
methods compared to Rand in Fig. 5. Results for absolute
values of regrets are shown in the appendix. Each exper-
iment was repeated for 20 times. For all experiments, we
set βm
t = |Am| log 2t and B = 10. All diverse batch sam-
pling methods perform comparably well and far better than
Rand, although there exist slight differences. While in
lower dimensions (D ∈ {2, 10}), Batch-UCB-PE-Fnc
in higher dimensions (D ∈
performs among the best,
{20, 50}), Batch-UCB-DPP-Fnc performs better than
(or comparable to) all other variants. We will see a larger
performance gap in later real-world experiments, showing
that biasing the combination towards higher quality func-
tions while retaining diversity across the batch of samples
provides a better exploration-exploitation trade-off.

(e)(f)(g)(h)t50100-1-0.8-0.6-0.4-0.20D=2t50100-0.8-0.6-0.4-0.20D=10t50100-0.8-0.6-0.4-0.20D=20t50100-0.5-0.4-0.3-0.2-0.10D=50t50100-8-6-4-20D=2t50100-1.2-1-0.8-0.6-0.4-0.20D=10t50100-1.2-1-0.8-0.6-0.4-0.20D=20t50100-0.8-0.6-0.4-0.20D=50RandBatch-UCB-PEBatch-UCB-DPPBatch-UCB-PE-FncBatch-UCB-DPP-Fnc(a)(b)(c)(d)log    rt10log    rt10log    rt10log    rt10log    Rt10log    Rt10log    Rt10log    Rt10Batched High-dimensional Bayesian Optimization via Structural Kernel Learning

For a real-data experiment, we tested the diverse batch sam-
pling algorithms for BBO on the Walker function which
returns the walking speed of a three-link planar bipedal
walker implemented in Matlab (Westervelt et al., 2007).
We tune 25 parameters that may inﬂuence the walking
speed, including 3 sets of 8 parameters for the ODE solver
and 1 parameter specifying the initial velocity of the stance
leg. We discretize each dimension into 40 points, resulting
in a function domain of |X | = 4025. This size is very inef-
ﬁcient for existing batch sampling techniques. We learn the
additive structure via Gibbs sampling and sample batches
of size B = 5. To further improve efﬁciency, we limit
the maximum size of each group to 2. The regrets for all
methods are shown in Fig. 6. Again, all diverse batch sam-
pling methods outperform Rand by a large gap. Moreover,
Batch-UCB-DPP-Fnc is a bit better than other variants,
suggesting that a selection by quality functions is useful.

Figure 6. The simple regrets (rt) of batch sampling meth-
ods on Walker data where B = 5.
Four diverse batch
(Batch-UCB-PE, Batch-UCB-DPP,
sampling methods
Batch-UCB-DPP-Fnc)
Batch-UCB-PE-Fnc
outperform random sampling (Rand) by a
large gap.
Batch-UCB-DPP-Fnc performs the best among the four
diverse batch sampling methods.

and

Batch Sizes Finally, we show how the batch size B af-
fects the performance of the proposed methods. We test
the algorithms on the 14-dimensional Robot dataset with
B ∈ {5, 10}. The regrets are shown in Fig. 4. With
larger batches, the differences between the batch selection
approaches become more pronounced.
In both settings,
Batch-UCB-DPP-Fnc performs a bit better than other
variants, in particular with larger batch sizes.

6. Conclusion

In this paper, we propose two novel solutions for high di-
mensional BO: inferring latent structure, and combining it
with batch Bayesian Optimization. The experimental re-
sults demonstrate that the proposed techniques are effec-
tive at optimizing high-dimensional black-box functions.

Figure 7. Simple regret when tuning the 14 parameters of a robot
pushing task with batch size 5 and 10. Learning decompositions
with Gibbs sampling and diverse batch sampling are employed
simultaneously. In general, Batch-UCB-DPP-Fnc performs a
bit better than the other four diverse batch sampling variants. The
gap increases with batch size.

Moreover, their gain over existing methods increases as the
dimensionality of the input grows. We believe that these
results have the potential to enable the increased use of
Bayesian optimization for challenging black-box optimiza-
tion problems in machine learning that typically involve a
large number of parameters.

Acknowledgements

We gratefully acknowledge support from NSF CAREER
award 1553284, NSF grants 1420927 and 1523767, from
ONR grant N00014-14-1-0486, and from ARO grant
W911NF1410433. We thank MIT Supercloud and the
Lincoln Laboratory Supercomputing Center for providing
computational resources. Any opinions, ﬁndings, and con-
clusions or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect the views
of our sponsors.

References

Azimi, Javad, Fern, Alan, and Fern, Xiaoli Z. Batch
Bayesian optimization via simulation matching.
In
Advances in Neural Information Processing Systems
(NIPS), 2010.

Catto, Erin. Box2D, a 2D physics engine for games.

http://box2d.org, 2011.

Contal, Emile, Buffoni, David, Robicquet, Alexandre, and
Vayatis, Nicolas. Parallel Gaussian process optimiza-
tion with upper conﬁdence bound and pure exploration.
In Joint European Conference on Machine Learning
and Knowledge Discovery in Databases, pp. 225–240.
Springer, 2013.

Desautels, Thomas, Krause, Andreas, and Burdick, Joel W.
Parallelizing exploration-exploitation tradeoffs in Gaus-

t102030405060rt6.577.588.599.5RandBatch-UCB-PEBatch-UCB-DPPBatch-UCB-PE-FncBatch-UCB-DPP-Fnct20406080100rt45678B=5Batch-UCB-PEBatch-UCB-DPPBatch-UCB-PE-FncBatch-UCB-DPP-Fnct1020304050rt45678B=10Batched High-dimensional Bayesian Optimization via Structural Kernel Learning

sian process bandit optimization. Journal of Machine
Learning Research, 2014.

Mo˘ckus, J. On Bayesian methods for seeking the ex-
In Optimization Techniques IFIP Technical

tremum.
Conference, 1974.

Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P.
Practical Bayesian optimization of machine learning al-
gorithms. In Advances in Neural Information Processing
Systems (NIPS), 2012.

Srinivas, Niranjan, Krause, Andreas, Kakade, Sham M, and
Seeger, Matthias W. Information-theoretic regret bounds
for Gaussian process optimization in the bandit setting.
IEEE Transactions on Information Theory, 2012.

Wang, Zi and Jegelka, Stefanie. Max-value entropy search
In International

for efﬁcient Bayesian optimization.
Conference on Machine Learning (ICML), 2017.

Wang, Zi, Zhou, Bolei, and Jegelka, Stefanie. Optimiza-
tion as estimation with Gaussian processes in bandit set-
tings. In International Conference on Artiﬁcial Intelli-
gence and Statistics (AISTATS), 2016a.

Wang, Zi, Jegelka, Stefanie, Kaelbling, Leslie Pack, and
P´erez, Tom´as Lozano. Focused model-learning and plan-
ning for non-Gaussian continuous state-action systems.
In International Conference on Robotics and Automa-
tion (ICRA), 2017.

Wang, Ziyu, Hutter, Frank, Zoghi, Masrour, Matheson,
David, and de Feitas, Nando. Bayesian optimization in a
billion dimensions via random embeddings. Journal of
Artiﬁcial Intelligence Research, 55:361–387, 2016b.

Westervelt, Eric R, Grizzle, Jessy W, Chevallereau, Chris-
tine, Choi, Jun Ho, and Morris, Benjamin. Feedback
control of dynamic bipedal robot locomotion, volume 28.
CRC press, 2007.

Djolonga, Josip, Krause, Andreas, and Cevher, Volkan.
In Ad-
Information Processing Systems

High-dimensional Gaussian process bandits.
vances in Neural
(NIPS), 2013.

Duvenaud, David, Lloyd, James Robert, Grosse, Roger,
Tenenbaum, Joshua B., and Ghahramani, Zoubin. Struc-
ture discovery in nonparametric regression through com-
positional kernel search. In International Conference on
Machine Learning (ICML), 2013.

Gonz´alez, Javier, Dai, Zhenwen, Hennig, Philipp, and
Lawrence, Neil D. Batch Bayesian optimization via lo-
cal penalization. International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2016.

Hennig, Philipp and Schuler, Christian J. Entropy search
for information-efﬁcient global optimization. Journal of
Machine Learning Research, 13:1809–1837, 2012.

Hern´andez-Lobato, Jos´e Miguel, Hoffman, Matthew W,
and Ghahramani, Zoubin. Predictive entropy search
for efﬁcient global optimization of black-box functions.
In Advances in Neural Information Processing Systems
(NIPS), 2014.

Kandasamy, Kirthevasan, Schneider, Jeff, and Poczos,
Barnabas. High dimensional Bayesian optimisation and
bandits via additive models. In International Conference
on Machine Learning (ICML), 2015.

Kathuria, Tarun, Deshpande, Amit, and Kohli, Pushmeet.
Batched Gaussian process bandit optimization via deter-
minantal point processes. In Advances in Neural Infor-
mation Processing Systems (NIPS), 2016.

Kawaguchi, Kenji, Kaelbling, Leslie Pack, and Lozano-
P´erez, Tom´as. Bayesian optimization with exponential
In Advances in Neural Information Pro-
convergence.
cessing Systems (NIPS), 2015.

Kawaguchi, Kenji, Maruyama, Yu, and Zheng, Xiaoyu.
Global continuous optimization with error bound and
fast convergence. Journal of Artiﬁcial Intelligence Re-
search, 56(1):153–195, 2016.

Kushner, Harold J. A new method of locating the maxi-
mum point of an arbitrary multipeak curve in the pres-
ence of noise. Journal of Fluids Engineering, 86(1):97–
106, 1964.

Li, Chun-Liang, Kandasamy, Kirthevasan, P´oczos,
Barnab´as, and Schneider, Jeff.
High dimensional
Bayesian optimization via restricted projection pursuit
models. In International Conference on Artiﬁcial Intel-
ligence and Statistics (AISTATS), 2016.

