7. Appendix

Minimize w.r.t ηj gives

Latent Feature Lasso

7.1. Comparison on Time Complexity

The proposed LatentLasso algorithm runs signiﬁcantly
faster than other methods in our experiments. For exam-
ple, on the Syn1 dataset (N=1000, D=1000, K=35), the
runtime of LatentLasso is 398s, while MCMC, Variational,
MF-Binary and BP-Means all take more than 10000s to ob-
tain their best results reported in the Figures (and the im-
plementation of Spectral Method we obtained from the au-
thors has memory requirement that restricts K¡14). On the
real data sets, we report only up to K=50 because most of
the compared methods already took one day to train.

The complexity of each algorithm can be summarized in
Table 2. The reason for the smaller runtime of LatentLasso
is due to the decoupling of factor N D from the factor re-
lated to K, where the factor O(N D) comes from the cost
of solving a MAX-CUT-like problem using the method of
(Boumal et al., 2016) or (Wang & Kolter, 2016), while the
factor O(K 2D) comes from the cost of solving a least-
square problem given by (11) with the maintenance cost
of Z T Z amortized.

7.2. Proof for Theorem 1

Let L(M ) be a smooth function such that ∇L(M ) is
Lipschitz-continuous with parameter β, that is,

L(M (cid:48)) − L(M ) − (cid:104)∇L(M ), M (cid:48) − M (cid:105) ≤

(cid:107)M (cid:48) − M (cid:107)2
F .

β
2

Then

∇jf (c) = zT

j ∇L(M )zj

is Lipschitz-continuous with parameter γ, which is of or-
der O(1) when loss function L(.) is an empirical average
normalized by N D.
Let A be the active set before adding ˆj. Consider the de-
scent amount produced by minimizing F (c) w.r.t.
the cˆj
given that 0 ∈ ∂jF (c) for all j ∈ A due to the subproblem
solved in the previous iteration. Let j = ˆj, for any ηj we
have

F (c + ηjej) − F (c)

µ∇j∗ f (c)ηj + λ|ηj| +

γ
2

η2
j

min
ηj

≤ min
ηj

= min

ηk:k /∈A

µ∇kf (c)ηk + λ|ηk|

+

≤ min

µ

ηk:k /∈A

∇kf (c)ηk + λ|ηk|

+

+ (1 − µ)λ

|ηk|

(cid:18)

(cid:88)

k /∈A

(cid:18)

(cid:88)

k /∈A
(cid:88)

k /∈A

(cid:19)

(cid:19)

γ
2

γ
2

(cid:32)

(cid:32)

(cid:88)

k /∈A

(cid:88)

k /∈A

(cid:33)2

|ηk|

(cid:33)2

|ηk|

where the last equality is justiﬁed later in Lemma 1. For
k ∈ A, we have

0 = min

µ

ηk:k∈A

(cid:88)

k∈A

(∇kf (c)ηk + λ|ck + ηk| − λ|ck|)

Combining cases for k /∈ A and k ∈ A, we can obtain a
global estimate of descent amount compared to some opti-
mal solution x∗ as follows

min
ηˆj

F (c + ηˆjeˆj) − F (c)

≤ min

µ

(cid:104)∇f (c), η(cid:105) + λ(cid:107)c + η(cid:107)1 − λ(cid:107)c(cid:107)1

(cid:19)

η
(cid:32)

+

γ
2

(cid:18)

(cid:88)

k /∈A

(cid:18)

(cid:33)2

|ηk|

+ (1 − µ)λ

|ηk|

(cid:88)

k /∈A

(cid:19)

≤ min

µ

F (c + η) − F (c)

+

η

(cid:32)

γ
2

(cid:88)

k /∈A

(cid:33)2

|ηk|

+ (1 − µ)λ

|ηk|

(cid:88)

k /∈A
(cid:18)

≤ min
α∈[0,1]

+ α(1 − µ)λ(cid:107)c∗(cid:107)1

µ

F (c + α(c∗ − c)) − F (c)

+

(cid:19)

αγ
2

(cid:107)c∗(cid:107)2
1

≤ min
α∈[0,1]

(cid:18)

(cid:19)

−αµ

F (c) − F (c∗)

+

α2γ
2

(cid:107)c∗(cid:107)2
1

+ α(1 − µ)λ(cid:107)c∗(cid:107)1.

It means we can always choose an α small enough to guar-
antee descent if

F (c) − F (c∗) >

(1 − µ)
µ

λ(cid:107)c∗(cid:107)1.

F (c) − F (c∗) ≥

2(1 − µ)
µ

λ(cid:107)c∗(cid:107)1,

(23)

(24)

F (c + ηjej) − F (c) ≤ ∇jf (c)ηj + λ|ηj| +

γ
2
≤ µ∇j∗ f (c)ηj + λ|ηj| +

η2
j
γ
2

η2
j

In addition, for

Latent Feature Lasso

Table 2: Comparison of Time Complexity. (T denotes number of iterations)

Methods
Time Complexity

MCMC
(N K 2D)T

Variational MF-Binary
(N K)2K
(N K 2D)T

BP-Means
(N K 3D)T N D + K 5log(K)

Spectral

LatentLasso
(N D + K 2D)T

we have

min
ηˆj

F (c + ηˆjeˆj) − F (c)

≤ min
α∈[0,1]

−

(cid:18)

αµ
2

F (c) − F (c∗)

+

(cid:19)

α2γ
2

(cid:107)c∗(cid:107)2
1.

Minimizing w.r.t. to α gives the convergence guarantee

Proof. Since supp(c∗) = A∗, and c∗ is optimal when
restricted on the support, we have (cid:104), c∗(cid:105) = 0 for some
∈ ∂F (c∗). And since F (c) is strongly convex on the sup-
port A∗ with parameter β, we have

F (0) − F (c∗) = F (0) − F (c∗) − (cid:104), 0 − c∗(cid:105)

F (ct) − F (c∗) ≤

2γ(cid:107)c∗(cid:107)2
1
µ2

1
t

.

which gives us

≥

(cid:107)c∗ − 0(cid:107)2
2,

β
2

(cid:107)c∗(cid:107)2

2 ≤

2(F (0) − F (c∗))
β

.

for any iterate with F (ct) − F (c∗) ≥ 2(1−µ)
Lemma 1.

µ λ(cid:107)c∗(cid:107)1.

min
ηj

µ∇j∗ f (c)ηj + λ|ηj| +

γ
2

η2
j

= min

ηk:k /∈A

(cid:18)

(cid:88)

k /∈A

µ∇kf (c)ηk + λ|ηk|

+

|ηk|

(cid:19)

(cid:32)

γ
2

(cid:88)

k /∈A

(25)

(cid:33)2

(26)

Proof. The minimization (34) is equivalent to

min
ηk:k /∈A

s.t.

(cid:19)

µ∇kf (c)ηk

(cid:33)2

|ηk|

≤ C1

|ηk| ≤ C2

(cid:18)

(cid:88)

k /∈A
(cid:32)

(cid:88)

k /∈A

(cid:88)

k /∈A

and therefore is equivalent to

min
ηk:k /∈A

s.t.

(cid:88)

µ

k /∈A

(cid:88)

k /∈A

∇kf (c)ηk

|ηk| ≤ min{

C1, C2}

(cid:112)

which is a linear objective subject to a convex set and thus
always has solution that lies on the corner point with only
one non-zero coordinate ηj∗ , which then gives the same
minimum as (33).

7.3. Proof of Theorem 2
Lemma 2. Let A∗ ∈ [ ¯K] be a support set and c∗ :=
arg minc:supp(c)=A∗ F (c∗). Suppose F (c) is strongly con-
vex on A∗ with parameter β. We have

Combining above with the fact for any c, (cid:107)c(cid:107)2
we obtain the result.

1 ≤ (cid:107)c(cid:107)0(cid:107)c(cid:107)2
2,

Since F (0) − F (c∗) ≤ 1
2N
(1) and (27), we have

(cid:80)N

i=1 y2

i ≤ 1 , from Theorem

F (cT ) − F (c∗) ≤

4γ(cid:107)c∗(cid:107)0
βµ2

(cid:18) 1
T

(cid:19)

+

2(1 − µ)λ
µ

for any c∗ := arg minc:supp(c)=A∗ F (c).

(cid:115)

.

2(cid:107)c∗(cid:107)0
β
(28)

7.4. Proof of Theorem 3

Before delving into the analysis of the Latent Feature Lasso
method, we ﬁrst investigate what one can achieve in terms
of the risk deﬁned in (1) if the combinatorial version of
objective is solved. Let

f (x; W ) := min

(cid:107)x − W T z(cid:107)2.

1
2

z∈{0,1}K

Suppose we can obtain solution ˆW to the following empir-
ical risk minimization problem:

ˆW :=

argmin
W ∈RK×D:(cid:107)W (cid:107)F ≤R

1
N

N
(cid:88)

i=1

f (xi; W ).

(29)

Then the following theorem holds.
Theorem 7. Let W ∗ be the minimizer of risk (1) and ˆW be
the empirical risk minimizer (29). Then

E[f (x; ˆW )] − E[f (x; W ∗)]

(cid:115)

≤

+

3
N

DK log(4R2KN )
2N

+

1
2N

log(

)

1
ρ

(cid:115)

(cid:107)c∗(cid:107)1 ≤

2(cid:107)c∗(cid:107)0(F (0) − F (c∗))
β

.

(27)

with probability 1 − ρ.

Latent Feature Lasso

erations of the greedy algorithm, we have

EN [f (x, DcW )] +

(cid:107)W (cid:107)2

F + λ(cid:107)c(cid:107)1

τ
2

(cid:33)

=

1
2N

N
(cid:88)

i=1

≤ F (c)

min
z∈{0,1}(cid:107)c(cid:107)0

(cid:107)xi − W T DT

c z(cid:107)2 +

(cid:107)W (cid:107)2

F + λ(cid:107)c(cid:107)1

τ
2

2(f (x, ˜W ) − f (x, W )) ≤ (cid:107)x − ˜W T z∗(cid:107)2 − (cid:107)x − W T z∗(cid:107)2
= z∗T (W − ˜W )x + (cid:104) ˜W ˜W T − W W T , z∗z∗T (cid:105)
≤ (cid:107)z∗(cid:107)2(cid:107)W − ˜W (cid:107)F + 2R(cid:107) ˜W − W (cid:107)F (cid:107)z∗(cid:107)2

2 ≤ 3RK(cid:107) ˜W − W (cid:107)F ,

τ
2

Proof Sketch. Let EN [f (x, W )] denote the empirical risk.
We have

E[f (x; ˆW )] − E[f (x; W ∗)]

(cid:32)

≤ 2

sup
W ∈RK×D:(cid:107)W (cid:107)F ≤R

|E[f (x; W )] − EN [f (x; W )]|

decomposition

(30)
≤
Then by introducing a δ-net N (δ)
, we have

from error
EN [f (x, W ∗)].
with covering number |N (δ)| = (cid:0) 4R
δ
(cid:107) ˜W − W (cid:107)F ≤ δ for some ˜W ∈ N (δ) and

and EN [f (x, ˆW )]

(cid:1)DK

(cid:12)
(cid:12)
E[f (x; ˜W )] − EN [f (x; ˜W )]
(cid:12)
(cid:12)

(cid:33)

≤ (cid:15)

(31)

(cid:19)DK

≥ 1 −

exp(−2N (cid:15)2).

(cid:32)

P

(cid:12)
(cid:12)
sup
(cid:12)
(cid:12)
˜W ∈N (δ)
(cid:18) 4R
δ

Then since

we have

sup
W :(cid:107)W (cid:107)F ≤R

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
E[f (x; W )] − EN [f (x; W )]
(cid:12)
(cid:12)

≤ (3RKδ) + sup

(cid:12)
(cid:12)
(cid:12)
(cid:12)
˜W ∈N (δ)

(cid:12)
(cid:12)
E[f (x; ˜W )] − EN [f (x; ˜W )]
(cid:12)
(cid:12)

≤ 3RKδ +

log(

) +

log(

)

4R
δ

1
2N

1
ρ

(cid:115)

DK
2N

(32)
with probability 1 − ρ. Choosing δ = 1/(RKN ) yields the
result.

Now we establish the proof of Theorem (3) for bounding
risk of the Latent Feature Lasso estimator.

Proof. Let Z ∗ ∈ arg minZ∈{0,1}NK
F and
S ∗ be the set of column index of Z with the same 0-1 pat-
terns to columns in Z ∗. Let c∗ be indicator vector with
k = 1, k ∈ S ∗ and c∗
c∗

k = 0, k /∈ S ∗. We have

N (cid:107)X −ZW ∗(cid:107)2

1

(34)
Combining (33), (34) and (28), we obtain a bound on the
bias and optimization error of the Latent Feature Lasso es-
timator

EN [f (x, DcW )] ≤ F (c) ≤ EN [f (x; W ∗)]

+

τ
2
(cid:124)

(cid:107)W ∗(cid:107)2 + λK
(cid:125)
(cid:123)(cid:122)
regularize bias

(cid:115)

+

(cid:19)

2γK
β

(cid:18) 1
T

+

2(1 − µ)K
µβ

(cid:124)

(cid:123)(cid:122)
optimization error

λ

(cid:125)

(35)
To bound the estimation error, notice that the matrix ˆW :=
DcW is ˆK × D with ˆK ≤ T . Furthermore, the descent
condition F (c) ≤ F (0) guarantees that

(cid:107)W (cid:107)2

F + λ(cid:107)c(cid:107)1 ≤

(cid:107)X − 0(cid:107)2 ≤ 1

1
N

F ≤ 1/τ , (cid:107)c(cid:107)1 ≤ 1/λ.

and thus (cid:107)W (cid:107)2
Let W(T, λ, τ ) := { ˆW ∈ (RT ×D) | (cid:107) ˆW (cid:107)F ≤ (cid:112)1/(λτ )}.
We have

E[f (x; ˆW )] − EN [f (x, ˆW )]

sup
(c,W )∈W(T,λ,τ )

(cid:115)

≤

DT log(4T N/(τ λ))
2N

+

1
2N

log(

)

1
ρ

with probability 1 − ρ through the same argument as in
the case of combinatorial objective (32). Combining the
above estimation error with the bias and optimization error
in (35), we have

E[f (x; W )] − E[f (x; W ∗)]
(cid:115)

≤

R2 + λK +

2γK
βT

+

2(1 − µ)K
µβ

λ

τ
2
(cid:115)

+

DT log(4T N/(τ λ))
2N

+

1
2N

log(

)

1
ρ

Choosing T = 2γK
DK
(cid:15)3 gives the result.

β ( 1

(cid:15) ), λ = τ = 1√

N

and N (cid:38) DT

(cid:15)2 =

F (¯c) ≤ F (c∗) ≤ EN [f (x; W ∗)] +

7.5. Proof of Theorem 4

τ
2

(cid:107)W ∗(cid:107)2

F + λ(cid:107)c∗(cid:107)1
(33)
F (c). Then let (c, W ) with

where ¯c ∈

argmin
c:supp(c)=S ∗

Proof. Since W ∗ is of rank K, we have span(Θ∗) =
span(Z ∗). Therefore, from condition 2,

supp(c) = ˆS be the output obtained from running T it-

span(Θ∗) ∩ {0, 1}N \ {0} = {Z ∗

:,j}K

j=1.

(36)

Latent Feature Lasso

For any (Z, W ) : ZW = Θ∗, we have Z ∈ span(Θ∗)
since Z = Θ∗V Σ−1U T where U ΣV T is the SVD of W
with Σ : K ×K. Then by (36) we know that Z = Z ∗. Then
it follows W = W ∗ since the linear system Θ∗ = Z ∗W
has unique solution for W .

7.6. Proof of Theorem 5

Proof. The solution of (21) satisﬁes

ZSWS = X = Z ∗W ∗.

Since WS has full row-rank, we have rank(ZS) =
rank(X) = rank(Z) = K by condition 1 in Theo-
rem 4. Then let WS = U ΣV T be the SVD of WS with
Σ : |S| × |S|, we have

where A∗ = (I − P )(cid:15) + (I − P )(ZSW ∗) where

P = ZS(Z T

S ZS + N τ I)−1Z T
S .

Let κn be the restricted strong convexity term deﬁned as :

κn := inf
∆∈C

{f (c∗ + ∆) − f (c∗) − (cid:104)∇f (c∗), ∆(cid:105)} ,

where C = {c|(cid:107)cSc(cid:107)1 ≤ 3(cid:107)cS(cid:107)1}. Then, if the regulariza-
tion parameter is set as λ ≥ ρn, we have the following
bound on the norm of the error ˆc − c∗:

(cid:107)ˆc − c∗(cid:107)2 ≤

ρn

√

κn

K ∗

.

ZS = XV Σ−1U T = Z ∗W ∗V Σ−1U T ∈ span(Z ∗).

7.9. Proof of Theorem 6

Then by condition 2 in Theorem 4, the columns of ZS can
only be in {Z ∗
j=1, which implies ZS equal to Z ∗ up to
a permutation. Then we know |S| = K and by Theorem 4
WS also equals W ∗ up to a permutation.

:,j}K

7.7. Proof of Theorem 8

Proof. By an application of Theorem1 of (Negahban et al.,
2009), for λ ≥ (cid:107)∇f (c∗)(cid:107)∞, we have the following bound
on the (cid:96)2 norm of ˆc − c∗:

Proof. Note that the optimization problem in Equation (22)
can be rewritten as:

argmin
Z∈{0,1}N

1

2N (cid:107)E + (Z ∗ − Z)(cid:107)2

2

= 1
2N

N
(cid:88)

i=1

argmin
Zi∈{0,1}

(Ei + (Z ∗

i − Zi))2

(38)

So, we have the following closed form expression for ˆZ:

(cid:107)ˆc − c∗(cid:107)2 ≤

√

K ∗λ
κn

,

ˆZi =

(cid:40)

1
0

if Z ∗
o.w

i + Ei ≥ 0.5

.

We now compute the probability that Z ∗

i (cid:54)= ˆZi:

P(Z ∗

i (cid:54)= ˆZi) = P(Ei ≥ 0.5) ∗ P(Z ∗

i = 0)

+P(Ei ≤ −0.5) ∗ P(Z ∗

i = 1)

≥ min{P(Ei ≥ 0.5), P(Ei ≤ −0.5)} ≥ c,

(39)
for some positive constant c. We now use the fact that
(cid:54)= ˆZi) to complete the proof of
E((Z ∗
the Lemma.

i − ˆZi)2) = P(Z ∗

i

where (cid:107)∇f (c∗)(cid:107)∞ is given by:

(cid:107)∇f (c∗)(cid:107)∞ = max

z∈{0,1}N

1
2N 2τ

(cid:107)A∗T z(cid:107)2
2,

where A∗ is deﬁned as:

A∗ =

(I − ZS(Z T

S ZS + N τ I)−1Z T

= (I − ZS(Z T

S ZS + N τ I)−1Z T

S )X
S )(ZSW ∗ + (cid:15))

(37)
S , it can be seen that

Given P = ZS(Z T
A∗ can be rewritten as :

S ZS + N τ I)−1Z T

A∗ = (I − P )(cid:15) + (I − P )(ZSW ∗).

7.8. ell2 error bounds on the coefﬁcient vector ˆc

Theorem 8. Let c∗ be the true underlying vector, with sup-
port S and sparsity K ∗. Let ˆc be the minimizer of F (c),
deﬁned in Equation (7). Deﬁne the noise-level term

ρn := max

z∈{0,1}N

1
2N 2τ

(cid:107)A∗T z(cid:107)2
2,

