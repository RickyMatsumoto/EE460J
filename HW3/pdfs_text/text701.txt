Theoretical Properties for Neural Networks with Weight Matrices of Low
Displacement Rank

Liang Zhao 1 Siyu Liao 1 Yanzhi Wang 2 Zhe Li 2 Jian Tang 2 Bo Yuan 1

Abstract

Recently low displacement rank (LDR) matri-
ces, or so-called structured matrices, have been
proposed to compress large-scale neural net-
works. Empirical results have shown that neu-
ral networks with weight matrices of LDR ma-
trices, referred as LDR neural networks, can
achieve signiﬁcant reduction in space and com-
putational complexity while retaining high accu-
racy. We formally study LDR matrices in deep
learning. First, we prove the universal approx-
imation property of LDR neural networks with
a mild condition on the displacement operators.
We then show that the error bounds of LDR neu-
ral networks are as efﬁcient as general neural net-
works with both single-layer and multiple-layer
structure. Finally, we propose back-propagation
based training algorithm for general LDR neural
networks.

1. Introduction

Neural networks, especially large-scale deep neural net-
works, have made remarkable success in various applica-
tions such as computer vision, natural language process-
ing, etc. (Krizhevsky et al., 2012)(Sutskever et al., 2014).
However, large-scale neural networks are both memory-
intensive and computation-intensive, thereby posing se-
vere challenges when deploying those large-scale neu-
ral network models on memory-constrained and energy-
constrained embedded devices. To overcome these limi-
tations, many studies and approaches, such as connection
pruning (Han et al., 2015)(Gong et al., 2014), low rank ap-
proximation (Denton et al., 2014)(Jaderberg et al., 2014),
sparsity regularization (Wen et al., 2016)(Liu et al., 2015)

Co-ﬁrst authors: Liang Zhao and Siyu Liao. 1The City Uni-
versity of New York, New York, New York, USA 2Syracuse Uni-
versity, Syracuse, New York, USA. Correspondence to: Bo Yuan
<byuan@ccny.cuny.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure 1. Examples of commonly used LDR (structured) matri-
ces, i.e., circulant, Cauchy, Toeplitz, Hankel, and Vandermonde
matrices.

etc., have been proposed to reduce the model size of large-
scale (deep) neural networks.

LDR Construction and LDR Neural Networks: Among
those efforts, low displacement rank (LDR) construction is
a type of structure-imposing technique for network model
reduction and computational complexity reduction. By reg-
ularizing the weight matrices of neural networks using the
format of LDR matrices (when weight matrices are square)
or the composition of multiple LDR matrices (when weight
matrices are non-square), a strong structure is naturally im-
posed to the construction of neural networks. Since an
LDR matrix typically requires O(n) independent param-
eters and exhibits fast matrix operation algorithms (Pan,
2001), an immense space for network model and compu-
tational complexity reduction can be enabled. Pioneering
work in this direction (Cheng et al., 2015)(Sindhwani et al.,
2015) applied special types of LDR matrices (structured
matrices), such as circulant matrices and Toeplitz matrices,
for weight representation. Other types of LDR matrices ex-
ist such as Cauchy matrices, Vandermonde matrices, etc.,
as shown in Figure 1.

Beneﬁts of LDR Neural Networks: Compared with other
types of network compression approaches, the LDR con-
struction shows several unique advantages. First, unlike
heuristic weight-pruning methods (Han et al., 2015)(Gong
et al., 2014) that produce irregular pruned networks, the
LDR construction approach always guarantees the strong
structure of the trained network, thereby avoiding the stor-

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

age space and computation time overhead incurred by the
complicated indexing process. Second, as a “train from
scratch” technique, LDR construction does not need ex-
tra re-training, and hence eliminating the additional com-
plexity to the training process. Third, the reduction in
space complexity and computational complexity by us-
ing the structured weight matrices are signiﬁcant. Differ-
ent from other network compression approaches that can
only provide a heuristic compression factor, the LDR con-
struction can enable the model reduction and computa-
tional complexity reduction in Big-O complexity: The stor-
age requirement is reduced from O(n2) to O(n), and the
computational complexity can be reduced from O(n2) to
O(n log n) or O(n log2 n) because of the existence of fast
matrix-vector multiplication algorithm (Pan, 2001)(Bini
et al., 1996) for LDR matrices. For example, when ap-
plying structured matrices to the fully-connected layers of
AlexNet using ImageNet dataset (Deng et al., 2009), the
storage requirement can be reduced by more than 4,000X
while incurring negligible degradation in overall accuracy
(Cheng et al., 2015).

Motivation of This Work: Because of its inherent
structure-imposing characteristic, convenient re-training-
free training process and unique capability of simultaneous
Big-O complexity reduction in storage and computation,
LDR construction is a promising approach to achieve high
compression ratio and high speedup for a broad category of
network models. However, since imposing the structure to
weight matrices results in substantial reduction of weight
storage from O(n2) to O(n), cautious researchers need to
know whether the neural networks with LDR construction,
referred to as LDR neural networks, will consistently yield
the similar accuracy as compared with the uncompressed
networks. Although (Cheng et al., 2015)(Sindhwani et al.,
2015) have already shown that using LDR construction still
results the same accuracy or minor degradation on vari-
ous datasets, such as ImageNet (Deng et al., 2009), CIFAR
(Krizhevsky & Hinton, 2009) etc., the theoretical analy-
sis, which can provide the mathematically solid proofs that
the LDR neural networks can converge to the same “effec-
tiveness” as the uncompressed neural networks, is still very
necessary in order to promote the wide application of LDR
neural networks for emerging and larger-scale applications.

Technical Preview and Contributions: To address the
above necessity, in this paper we study and provide a solid
theoretical foundation of LDR neural networks on the abil-
ity to approximate an arbitrary continuous function, the er-
ror bound for function approximation, applications on shal-
low and deep neural networks, etc. More speciﬁcally, the
main contributions of this paper include:

• We prove the universal approximation property for
LDR neural networks, which states that the LDR neu-

ral networks could approximate an arbitrary contin-
uous function with arbitrary accuracy given enough
parameters/neurons. In other words, the LDR neural
network will have the same “effectiveness” of classi-
cal neural networks without compression. This prop-
erty serves as the theoretical foundation of the poten-
tial broad applications of LDR neural networks.

• We show that, for LDR matrices deﬁned by O(n) pa-
rameters, the corresponding LDR neural networks are
still capable of achieving integrated squared error of
order O(1/n), which is identical to the error bound of
unstructured weight matrices-based neural networks,
thereby indicating that there is essentially no loss for
restricting to the weight matrices to LDR matrices.

• We develop a universal training process for LDR neu-
ral networks with computational complexity reduc-
tion compared with backward propagation process for
classical neural networks. The proposed algorithm is
the generalization of the training process in (Cheng
et al., 2015)(Sindhwani et al., 2015) that restricts the
structure of weight matrices to circulant matrices or
Toeplitz matrices.

Outline: The paper is outlined as follows. In Section 2 we
review the related work on this topic. Section 3 presents
necessary deﬁnitions and properties of matrix displacement
and LDR neural networks. The problem statement is also
presented in this section. In Section 4 we prove the uni-
versal approximation property for a broad family of LDR
neural networks. Section 5 addresses the approximation
potential (error bounds) with a limited amount of neurons
on shallow LDR neural networks and deep LDR neural net-
works, respectively. The proposed detailed procedure for
training general LDR neural networks are derived in Sec-
tion 6. Section 7 concludes the article.

2. Related Work

Universal Approximation & Error Bound Analysis: For
feedforward neural networks with one hidden layer, (Cy-
benko, 1989) and (Hornik et al., 1989) proved separately
the universal approximation property, which guarantees
that for any given continuous function or decision func-
tion and any error bound (cid:15) > 0, there always exists a
single-hidden layer neural network that approximates the
function within (cid:15) integrated error. However, this property
does not specify the number of neurons needed to construct
such a neural network. In practice, there must be a limit on
the maximum amount of neurons due to the computational
limit. Moreover, the magnitude of the coefﬁcients can be
neither too large nor too small. To address these issues for
general neural networks, (Hornik et al., 1989) proved that it
is sufﬁcient to approximate functions with weights and bi-

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

ases whose absolute values are bounded by a constant (de-
pending on the activation function). (Hornik, 1991) further
extended this result to an arbitrarily small bound. (Barron,
1993) showed that feedforward networks with one layer of
sigmoidal nonlinearities achieve an integrated squared er-
ror with order of O(1/n), where n is the number of neu-
rons.

More recently, several interesting results were published
on the approximation capabilities of deep neural networks.
(Delalleau & Bengio, 2011) have shown that there ex-
ist certain functions that can be approximated by three-
layer neural networks with a polynomial amount of neu-
rons, while two-layer neural networks require exponen-
tially larger amount to achieve the same error. (Montufar
et al., 2014) and (Telgarsky, 2016) have shown the expo-
nential increase of linear regions as neural networks grow
deeper. (Liang & Srikant, 2016) proved that with log(1/(cid:15))
layers, the neural network can achieve the error bound (cid:15) for
any continuous function with O(polylog((cid:15))) parameters in
each layer.

LDR Matrices in Neural Networks: (Cheng et al., 2015)
have analyzed the effectiveness of replacing conventional
weight matrices in fully-connected layers with circulant
matrices, which can reduce the time complexity from
O(n2) to O(n log n), and the space complexity from
O(n2) to O(n), respectively. (Sindhwani et al., 2015) have
demonstrated signiﬁcant beneﬁts of using Toeplitz-like ma-
trices to tackle the issue of large space and computation re-
quirement for neural networks training and inference. Ex-
periments show that the use of matrices with low displace-
ment rank offers superior tradeoffs between accuracy and
time/space complexity.

3. Preliminaries on LDR Matrices and Neural

Networks

3.1. Matrix Displacement

An n × n matrix M is called a structured matrix when it
has a low displacement rank γ (Pan, 2001). More precisely,
with the proper choice of operator matrices A and B, if the
Sylvester displacement

and the Stein displacement

∇A,B(M) := AM − MB

∆A,B(M) := M − AMB

(1)

(2)

of matrix M have a rank γ bounded by a value that is in-
dependent of the size of M, then matrix M is referred to
as a matrix with a low displacement rank (Pan, 2001). In
this paper we will call these matrices as LDR matrices.
Even a full-rank matrix may have small displacement rank
with appropriate choice of displacement operators (A, B).

Operator Matrix

A
Z1
Z1
Z0
diag(t)
diag(s) diag(t)

B
Z0
Z0
Z1
Z0

Structured
Rank of
Matrix M ∆A,B(M)
Circulant
Toeplitz
Henkel
Vandermonde
Cauchy

≤ 2
≤ 2
≤ 2
≤ 1
≤ 1

Table 1. Pairs of Displacement Operators and Associated Struc-
tured Matrices. Z0 and Z1 represent the 0-unit-circulant matrix
and the 1-unit-circulant matrix respectively, and vector s and t
denote vectors deﬁning Vandermonde and Cauchy matrices (cf.
(Sindhwani et al., 2015)).

Figure 1 illustrates a series of commonly used structured
matrices, including a circulant matrix, a Cauchy matrix, a
Toeplitz matrix, a Hankel matrix, and a Vandermonde ma-
trix, and Table 1 summarizes their displacement ranks and
corresponding displacement operators.

The general procedure of handling LDR matrices gener-
ally takes three steps: Compression, Computation with Dis-
placements, Decompression. Here compression means to
obtain a low-rank displacement of the matrices, and decom-
pression means to converting the results from displacement
computations to the answer to the original computational
problem. In particular, if one of the displacement operator
has the property that its power equals the identity matrix,
then one can use the following method to decompress di-
rectly:
Lemma 3.1. If A is an a-potent matrix (i.e., Aq = aI for
some positive integer q ≤ n), then

M =

(cid:104)

q−1
(cid:88)

k=0

Ak∆A,B(M)Bk(cid:105)

(I − aBq)−1.

(3)

Proof. See Corollary 4.3.7 in (Pan, 2001).

One of the most important characteristics of structured ma-
trices is their low number of independent variables. The
number of independent parameters is O(n) for an n-by-
n structured matrix instead of the order of n2, which in-
dicates that the storage complexity can be potentially re-
duced to O(n). Besides, the computational complexity for
many matrix operations, such as matrix-vector multiplica-
tion, matrix inversion, etc., can be signiﬁcantly reduced
when operating on the structured ones. The deﬁnition and
analysis of structured matrices have been generalized to the
case of n-by-m matrices where m (cid:54)= n, e.g., the block-
circulant matrices (Pan et al., 2015). Our application of
LDR matrices to neural networks would be the general n-
by-m weight matrices. For certain lemmas and theorems
such as Lemma 3.1, only the form on n × n square ma-
trices is needed for the derivation procedure in this paper.

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

So we omit the generalized form of such statements unless
necessary.

3.2. LDR Neural Networks

In this paper we study the viability of applying LDR matri-
ces in neural networks. Without loss of generality, we focus
on a feed-forward neural network with one fully-connected
(hidden) layer, which is similar network setup as (Cybenko,
1989). Here the input layer (with n neurons) and the hidden
layer (with kn neurons)1 are assumed to be fully connected
with a weight matrix W ∈ Rn×kn of displacement rank
at most r corresponding to displacement operators (A, B),
where r (cid:28) n. The domain for the input vector x is the n-
dimensional hypercube I n := [0, 1]n, and the output layer
only contains one neuron. The neural network can be ex-
pressed as:

y = GW,θ(x) =

αjσ(wj

T x + θj).

(4)

kn
(cid:88)

j=1

Here σ(·) is the activation function, wj ∈ Rn denotes the j-
th column of the weight matrix W, and αj, θj ∈ R for j =
1, ..., kn. When the weight matrix W = [w1|w2| · · · |wkn]
has a low-rank displacement, we call it an LDR neural net-
work. Matrix displacement techniques ensure that LDR
neural network has much lower space requirement and
higher computational speed comparing to classical neural
networks of the similar size.

3.3. Problem Statement

In this paper, we aim at providing theoretical support on the
accuracy of function approximation using LDR neural net-
works, which represents the “effectiveness” of LDR neu-
ral networks compared with the original neural networks.
Given a continuous function f (x) deﬁned on [0, 1]n, we
study the following tasks:

• For any (cid:15) > 0, ﬁnd an LDR weight matrix W so that

the function deﬁned by equation (4) satisﬁes

max
x∈[0,1]n

|f (x) − GW,θ(x)| < (cid:15).

(5)

• Fix a positive integer n, ﬁnd an upper bound (cid:15) so that
for any continuous function f (x) there exists a bias
vector θ and an LDR matrix with at most n rows sat-
isfying equation (5).

• Find a multi-layer LDR neural network that achieves

error bound (5) but with fewer parameters.

1Please note that this assumption does not sacriﬁce any gen-
erality because the n-by-m case can be transformed to n-by-kn
format with the nearest k using zero padding (Cheng et al., 2015).

The ﬁrst task is handled in Section 4, which is the universal
approximation property of LDR neural networks. It states
that the LDR neural networks could approximate an arbi-
trary continuous function arbitrarily well and is the under-
pinning of the widespread applications. The error bounds
for shallow and deep neural networks are derived in Sec-
tion 5. In addition, we derived explicit back-propagation
expressions for LDR neural networks in Section 6.

4. The Universal Approximation Property of

LDR Neural Networks

We call a family of matrices S to have representation
property if for any vector v ∈ Rn, there exists a matrix
M ∈ SA,B such that v is a column of M . Note that all
ﬁve types of LDR matrices shown in Fig. 1 have this repre-
sentation property because of their explicit pattern. In this
section we will prove that this property also holds for many
other LDR families. Based on this result, we are able to
prove the universal approximation property of neural net-
works utilizing only LDR matrices.

Theorem 4.1. Let A, B be two n × n non-singular diago-
nalizable matrices. Deﬁne Sr
A,B as the set of matrices M
such that ∆A,B(M) has rank at most r. Then the repre-
sentation property holds for Sr

A,B if A and B satisfy

i) Aq = aI for some positive integer q ≤ n and a scalar
a (cid:54)= 0; ii) (I − aBq) is nonsingular; iii) the eigenvalues of
B have distinguishable absolute values.

Proof. It sufﬁces to prove for the case r = 1, as increasing
r only provides more candidate matrices to choose from.
By the property of Stein displacement, any matrix M ∈ S
can be expressed in terms of A, B, and its displacement as
follows:

M =

Ak∆A,B(M)Bk(I − aBq)−1.

(6)

Next we express ∆A,B(M) as a product of two vectors
g · hT since it has rank 1. Also write A = Q−1ΛQ, where
Λ = diag(λ1, ..., λn) is a diagonal matrix generated by
the eigenvalues of A. Now deﬁne ej to be the j-th unit
column vector for j = 1, ..., n. Write

QMej =Q

Ak∆A,B(M)Bk(I − aBq)−1ej

=Q

(Q−1ΛQ)kghT Bk(I − aBq)−1ej

(7)

q−1
(cid:88)

k=0

q−1
(cid:88)

k=0

q−1
(cid:88)

k=0

=(cid:0)

q−1
(cid:88)

k=0

sh,jΛk(cid:1)Qg.

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

Here we use sh,j to denote the resulting scalar from ma-
trix product hT Bk(I − aBq)−1ej for k = 1, ..., n. Deﬁne
T := (I − aBq)−1.
In order to prove the theorem, we
need to show that there exists a vector h and an index k
such that the matrix (cid:80)q−1
k=0 sh,jΛk is nonsingular. In order
to distinguish scalar multiplication from matrix multiplica-
tion, we use notation a ◦ M to denote the multiplication of
a scalar value and a matrices whenever necessary. Rewrite
the expression as

sh,jΛk

q−1
(cid:88)

k=0

q−1
(cid:88)

k=0

q−1
(cid:88)

k=0

=

=

hT · (cid:0)BkTej ◦ diag(λk

1, ..., λk

n)(cid:1)

This implies that λi1η1, ..., λinηn are solutions to the equa-
tion

1 + x + x2 + · · · + xq−1 = 0.

(8)

By assumption of matrix B, η1, ..., ηk have different abso-
lute values, and so are λi1η1, ..., λi1η1, since all λk have
the same absolute value because Aq = aI. This fact sug-
gests that there are q distinguished solutions of equation
(8), which contradicts the fundamental theorem of algebra.
Thus it is incorrect to assume that matrix (cid:80)q−1
k=0 sh,jΛk is
singular for all h ∈ Rn. With this property proven, given
any vector v ∈ Rn, one can take the following procedure
to ﬁnd a matrix M ∈ S and a index j such that the j-th
column of M equals v:

i) Find a vector h and a index j such that matrix
(cid:80)q−1

k=0 sh,jΛk is non-singular;

diag(hT · Bk · T · [λk

1ej| · · · |λk

nej])

ii) By equation (7), ﬁnd

(cid:18)

=diag

hT ·

(cid:16)

q−1
(cid:88)

k=0

(cid:17)

BkTλk

1ej

, ..., hT ·

BkTλk

nej

(cid:17)(cid:19)
.

(cid:16)

q−1
(cid:88)

k=0

g :=Q−1(cid:0)

sh,jΛk(cid:1)−1

QTv;

q−1
(cid:88)

k=0

k=0 BTkλk

The diagonal matrix (cid:80)q−1
k=0 sh,jΛk is nonsingular if and
only if all of its diagonal entries are nonzero. Let bij de-
note the column vector (cid:80)q−1
i ej. Unless for ev-
ery j there is an index ij such that bij j = 0, we can al-
ways choose an appropriate vector h so that the resulting
diagonal matrix is nonsingular. Next we will show that
the former case is not possible using proof by contradic-
tion. Assume that there is a column bij j = 0 for every
j = 1, 2, · · · , n, we must have:

0 =[bi11|bi22| · · · |binn]

BkTλk
i1

e1| · · · |

BkTλk
in

en

(cid:105)

q−1
(cid:88)

k=0

BkT · diag(λk
i1

, ..., λk
in

).

(cid:104)

q−1
(cid:88)

=

k=0

=

q−1
(cid:88)

k=0

Since B is diagonalizable, we write B = P−1ΠP, where
Π = diag(η1, ..., ηn). Also we have T = (I − aBq)−1 =
P−1(I − aΠq)−1P. Then

0 =

BTkdiag(λk
i1

, ..., λk
in

)

= P−1

Πk(I − aΠq)−1diag(λk
i1

, ..., λk
in

(cid:21)
)

P

= P−1

diag

(cid:16)

(λi1 η1)k, ..., (λin ηn)k(cid:17)

(I − aΠq)−1P

= P−1diag

(λi1η1)k, ...,

(λin ηn)k(cid:17)

(I − aΠq)−1P.

(cid:16)

q−1
(cid:88)

k=0

q−1
(cid:88)

k=0

q−1
(cid:88)

k=0

(cid:20) q−1
(cid:88)

k=0

q−1
(cid:88)

k=0

iii) Construct M ∈ S with g and h by equation (6). Then
its j-th column will equal to v.

With the above construction, we have shown that for any
vector v ∈ Rn one can ﬁnd a matrix M ∈ S and a index j
such that the j-th column of M equals v, thus the theorem
is proved.

Our main goal of this section is to show that neural net-
works with many types of LDR matrices (LDR neural net-
works) can approximate continuous functions arbitrarily
well. In particular, we are going to show that Toeplitz ma-
trices and circulant matrices, as speciﬁc cases of LDR ma-
trices, have the same property. In order to do so, we need to
introduce the following deﬁnition of a discriminatory func-
tion and state one of its key property as Lemma 4.1.
Deﬁnition 4.1. A function σ(u) : R → R is called as
discriminatory if the zero measure is the only measure µ
that satisﬁes the following property:

(cid:90)

I n

σ(wTx + θ)dµ(x) = 0, ∀w ∈ Rn, θ ∈ R.

(9)

Lemma 4.1. [cf. (Cybenko, 1989)] Any bounded, measur-
able sigmoidal function is discriminatory.

Now we are ready to present the universal approximation
theorem of LDR neural networks with n-by-kn weight
matrix W:

Theorem 4.2 (Universal Approximation Theorem for LDR
Neural Networks). Let σ be any continuous discriminatory
function and Sr
A,B be a family of LDR matrices having
representation property. Then for any continuous function

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

f (x) deﬁned on I n and any (cid:15) > 0, there exists a function
G(x) in the form of equation (4) so that its weight matrix
consists of k submatrices from Sr

A,B and

max
x∈I n

|G(x) − f (x)| < (cid:15).

(10)

Proof. Denote the i-th n × n submatrix of W as Wi. Then
W can be written as

W = (cid:2)W1|W2|...|Wk

(cid:3).

(11)

Let SI n denote the set of all continuous functions deﬁned
on I n. Let UI n be the linear subspace of SI n that can be
expressed in form of equation (4) where W consists of k
sub-matrices with displacement rank at most r. We want to
show that UI n is dense in the set of all continuous functions
SI n .

Suppose not, by Hahn-Banach Theorem, there exists a
bounded linear functional L (cid:54)= 0 such that L( ¯U (I n)) = 0.
Moreover, By Riesz Representation Theorem, L can be
written as

L(h) =

h(x)dµ(x), ∀h ∈ S(I n),

(cid:90)

I n

for some measure µ.
Next we show that for any y ∈ Rn and θ ∈ R, the function
σ(yTx + θ) belongs to the set UI n , and thus we must have

(cid:90)

I n

σ(yTx + θ)dµ(x) = 0.

(12)

For any vector y ∈ Rn, Theorem 4.1 guarantees that there
exists an n×n LDR matrix M = [b1| · · · |bn] and an index
j such that bj = y. Now deﬁne a vector (α1, ..., αn) such
that αj = 1 and α1 = · · · = αn = 0. Also let the value
of all bias be θ. Then the LDR neural network function
becomes

n
(cid:88)

G(x) =

αiσ(bT

i x + θ)

i=1
=αjσ(bT

j x + θ) = σ(yTx + θ).

(13)

From the fact that L(G(x)) = 0, we derive that

0 =L(G(x))

(cid:90)

=

n
(cid:88)

I n

i=1

αiσ(bT

i x + θ) =

σ(yTx + θ)dµ(x).

(cid:90)

In

Theorem, which is obtained based on the assumption that
the set UI n of LDR neural network functions are not dense
in SI n . As this assumption is not true, we have the univer-
sal approximation property of LDR neural networks.

Reference work (Cheng et al., 2015), (Sindhwani et al.,
2015) have utilized a circulant matrix or a Toeplitz matrix
for weight representation in deep neural networks. Please
note that for the general case of n-by-m weight matrices,
either the more general Block-circulant matrices should
be utilized or padding extra columns or rows of zeroes
are needed (Cheng et al., 2015). Circulant matrices and
Topelitz matrices are both special form of LDR matrices,
and thus we could apply the above universal approxima-
tion property of LDR neural networks and provide theoret-
ical support for the use of circulant and Toeplitz matrices in
(Cheng et al., 2015), (Sindhwani et al., 2015). Moreover, it
is possible to consolidate the choice of parameters so that
a block-Toeplitz matrix also shows Toeplitz structure glob-
ally. Therefore we arrive at the following corollary.

Corollary 4.1. Any continuous function can be arbitrarily
approximated by neural networks constructed with Toeplitz
matrices or circulant matrices (with padding or using
Block-circulant matrices).

5. Error Bounds on LDR Neural Networks

With the universal approximation property proved, natu-
rally we seek ways to provide error bound estimates for
LDR neural networks. We are able to prove that for
LDR matrices deﬁned by O(n) parameters (n represents
the number of rows and has the same order as the num-
ber of columns), the corresponding structured neural net-
work is capable of achieving integrated squared error of
order O(1/n), where n is the number of parameters. This
result is asymptotically equivalent to Barron’s aforemen-
tioned result on general neural networks, indicating that
there is essentially no loss for restricting to LDR matrices.

The functions we would like to approximate are those who
are deﬁned on a n-dimensional ball Br = {x ∈ Rn : |x| ≤
r} such that (cid:82)
|x||f (x)|µ(dx) ≤ C, where µ is an arbi-
trary measure normalized so that µ(Br) = 1. Let’s call this
set ΓC,Br . (Barron, 1993) considered the following set of
bounded multiples of a sigmoidal function composed with
linear functions:

Br

Gσ = {ασ(yTx + θ) : |α| ≤ 2C, y ∈ Rn, θ ∈ R}. (14)

Since σ(t) is a discriminatory function by Lemma 4.1. We
can conclude that µ is the zero measure. As a result, the
function deﬁned as an integral with measure µ must be zero
for any input function h ∈ S(I n). The last statement con-
tradicts the property that L (cid:54)= 0 from the Hahn-Banach

He proved the following theorem:

Theorem 5.1 ((Barron, 1993)). For every function in
ΓC,Br , every sigmoidal function σ, every probability mea-
sure, and every k ≥ 1, there exists a linear combination of

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

sigmoidal functions fk(x) of the form

have the following equality

fk(x) =

αjσ(yT

j x + θj),

(15)

k
(cid:88)

j=1

such that

(cid:90)

Br

(f (x) − fk(x))2µ(dx) ≤

(16)

4r2C
k

.

Here yj ∈ Rn and θj ∈ R for every j = 1, 2, ..., N , More-
over, the coefﬁcients of the linear combination may be re-
stricted to satisfy (cid:80)k
j=1 |cj| ≤ 2rC.

Now we will show how to obtain a similar result for LDR
matrices. Fix operator (A, B) and deﬁne

(cid:110) kn
(cid:88)

Skn

σ =

αjσ(yT

j x + θj) : |αj| ≤ 2C, yj ∈ Rn,

j=1
θj ∈ R, j = 1, 2, ..., N,
and [y(i−1)n+1|y(i−1)n+2| · · · |yin]

is an LDR matrix, ∀i = 1, ..., k

(cid:111)
.

(17)
Moreover, let Gk
σ be the set of function that can be ex-
pressed as a sum of no more than k terms from Gσ. Deﬁne
(f (x) − g(x))2µ(dx). Theo-
the metric ||f −g||µ =
rem 5.1 essentially states that the minimal distance between
a function f ∈ ΓC,B and Gm
σ is asymptotically O(1/n).
The following lemma proves that Gk
σ is in fact contained in
Skn
σ .

(cid:113)(cid:82)

Br

Lemma 5.1. For any k ≥ 1, Gk

σ ⊂ Skn
σ .

Proof. Any function fk(x) ∈ Gk
form

σ can be written in the

fk(x) =

αjσ(yT

j x + θj).

(18)

k
(cid:88)

j=1

For each j = 1, ..., k, deﬁne a n × n LDR matrix Wj such
that one of its column is yj. Let tij be the i-th column
of Wj. Let ij correspond to the column index such that
tij = yj for all j. Now consider the following function

G(x) :=

βijσ(tT

ijx + θj)

k
(cid:88)

n
(cid:88)

j=1

i=1

βij jσ(tT

ijx + θj)

=

=

k
(cid:88)

j=1

k
(cid:88)

j=1

αjσ(yT

j x + θj) = fk(x).

Notice that the matrix W = [W1|W2| · · · |Wk] consists k
LDR submatrices. Thus fk(x) belongs to the set Skn
σ .

σ with Skn

By Lemma 5.1, we can replace Gk
σ in Theorem
5.1 and obtain the following error bound estimates on LDR
neural networks:
Theorem 5.2. For every disk Br ⊂ Rn, every function in
ΓC,Br , every sigmoidal function σ, every normalized mea-
sure µ, and every k ≥ 1, there exists neural network deﬁned
by a weight matrix consists of k LDR submatrices such that

(cid:90)

Br

(f (x) − fkn(x))2µ(dx) ≤

(20)

4r2C
k

.

Moreover, the coefﬁcients of the linear combination may be
restricted to satisfy (cid:80)N
k=1 |ck| ≤ 2rC.

Theorem 5.2 is the ﬁrst theoretical result that gives a gen-
eral error bound on LDR neural networks. Empirically,
(Cheng et al., 2015) reported that circulant neural net-
works are capable of achieving the same level of accu-
racy as AlexNet with more than 4,000X space saving on
fully-connected layers.
(Sindhwani et al., 2015) applied
Toeplitz-type LDR matrices to several benchmark image
classiﬁcation datasets, retaining the performance of state-
of-the-art models with very high compression ratio.

The next theorem naturally extended the result from (Liang
& Srikant, 2016) to LDR neural networks, indicating that
LDR neural networks can also beneﬁt a parameter reduc-
tion if one uses more than one layers. More precisely, we
have the following statement:
Theorem 5.3. Let f be a continuous function on [0, 1] and
is 2n + 1 times differentiable in (0, 1) for n = (cid:100)log 1
(cid:15) +
1](cid:101). If |f (k)(x)| ≤ k! holds for all x ∈ (0, 1) and k ∈
(cid:2)2n + 1(cid:3), then for any n × n matrices A and B satisfying
the conditions of Theorem 4.1, there exists a LDR neural
network GA,B(x) with O(log 1
(cid:15) ) binary
step units, O(log3 1

(cid:15) ) layers, O(log2 1
(cid:15) ) rectiﬁer linear units such that

G(x) :=

βijσ(tT

ijx + θj),

(19)

max
x∈[0,1]

|f (x) − GA,B(x)| < (cid:15).

k
(cid:88)

n
(cid:88)

j=1

i=1

where βij j equals αj, and βij = 0 if i (cid:54)= ij. Notice that we

Proof. The theorem with better bounds and without as-
sumption of being LDR neural network is proved in (Liang

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

y = σ(WT x + θ),

(21)

∂O
∂Hi

Bk

i (I − aBq

i )−1(

∂O
∂ ˆHik

)T

=

=

q−1
(cid:88)

k=0

q−1
(cid:88)

k=0

Bk

i (I − aBq

i )−1(

∂O
∂Wik

)T ˆGik.

& Srikant, 2016) as Theorem 4. For each binary step unit
or rectiﬁer linear unit in the construction of the general neu-
ral network, attach (n − 1) dummy units, and expand the
weights associated to this unit from a vector to an LDR
matrix based on Theorem 4.1. By doing so we need to ex-
pand the number units by a factor of order log 1
(cid:15) , and the
asymptotic bounds are relaxed accordingly.

6. Training LDR Neural Networks

In this section, we reformulate the gradient computation of
LDR neural networks. The computation for propagating
through a fully-connected layer can be written as

where σ(·) is the activation function, W ∈ Rn×kn is the
weight matrix, x ∈ Rn is input vector and θ ∈ Rkn is bias
vector. According to Equation (7), if Wi is an LDR matrix
with operators (Ai, Bi) satisfying conditions of Theorem
4.1, then it is essentially determined by two matrices Gi ∈
Rn×r, Hi ∈ Rn×r as

Wi =

(cid:104)

q−1
(cid:88)

k=0

Ak

i GiHT

i Bk
i

(cid:105)
(I − aBq

i )−1.

(22)

To ﬁt the back-propagation algorithm, our goal is to com-
pute derivatives ∂O
∂x for any objective func-
∂Gi
tion O = O(W1, . . . , Wk).

and ∂O

, ∂O
∂Hi

In general, given that a := WTx + θ, we can have:

∂O
∂W

= x(

∂O
∂a

)T ,

∂O
∂x

= W

∂O
∂a

,

∂O
∂θ

∂O
∂a

=

1.

(23)

where 1 is a column vector full of ones. Let ˆGik := Ak
ˆHik := HT
derivatives of

i Gi,
i )−1, and Wik := ˆGik ˆHik. The

can be computed as following:

i (I − aBq
∂O
∂Wik

i Bk

∂O
∂Wik

=

∂O
∂Wi

.

According to Equation (23), if we let a = Wik, W = ˆGT
ik
and x = ˆHik, then ∂O
and ∂O
∂ ˆHik
∂ ˆGik

can be derived as:

∂O
∂ ˆGik

= (cid:2) ∂O
∂ ˆGT
ik

(cid:3)T

= (cid:2) ˆHik

∂O
∂Wik

(cid:3)T

= (

∂O
∂Wik

)T ˆHT
ik,

∂O
∂ ˆHik

= ˆGT
ik

∂O
∂Wik

.

Similarly, let a = ˆGik, W = (Ak

i )T and x = Gi, then

(24)

(25)

(26)

∂O
∂Gi

can be derived as:

∂O
∂Gi

=

=

q−1
(cid:88)

k=0

q−1
(cid:88)

k=0

(Ak

i )T (

∂O
∂ ˆGik

)

∂O
∂Wik

(Ak

i )T (

)T ˆHT
ik.

Substituting with a = ˆHik, W = HT
aBq

i )−1, we have ∂O
∂Hi

derived as:

i and x = Bk

i (I −

(27)

(28)

which is equal to ∂O
∂Wi

and ∂O
In this way, derivatives ∂O
can be computed
∂Gi
∂Hi
given ∂O
. The essence of back-
∂Wik
propagation algorithm is to propagate gradients backward
from the layer with objective function to the input layer.
can be calculated from previous layer and ∂O
∂O
∂x will be
∂Wi
propagated to the next layer if necessary.

For practical use one may want to choose matrices Ai and
Bi with fast multiplication method such as diagonal ma-
trices, permutation matrices, banded matrices, etc. Then
the space complexity (the number of parameters for stor-
age) of Wi can be O(2n + 2nr) rather than O(n2) of
traditional dense matrix. The 2n is for Ai and Bi and
2nr is for Gi and Hi. The time complexity of WT
i x will
be O(q(3n + 2nr)) compared with O(n2) of dense ma-
trix. Particularly, when Wi is a structured matrix like the
Toeplitz matrix, the space complexity will be O(2n). This
is because the Toeplitz matrix is deﬁned by 2n parameters.
Moreover, its matrix-vector multiplication can be acceler-
ated by using Fast Fourier Transform (for Toeplitz and cir-
culant matrices), resulting in time complexity O(n log n).
In this way the back-propagation computation for the layer
can be done with near-linear time.

7. Conclusion

In this paper, we have proven that the universal approx-
imation property of LDR neural networks.
In addition,
we also theoretically show that the error bounds of LDR
neural networks are at least as efﬁcient as general unstruc-
tured neural network. Besides, we also develop the back-
propagation based training algorithm for universal LDR
neural networks. Our study provides the theoretical foun-
dation of the empirical success of LDR neural networks.

Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank

Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple

layers of features from tiny images. 2009.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in neural information processing
systems, pp. 1097–1105, 2012.

Liang, Shiyu and Srikant, R. Why deep neural networks?

arXiv preprint arXiv:1610.04161, 2016.

Liu, Baoyuan, Wang, Min, Foroosh, Hassan, Tappen, Mar-
shall, and Pensky, Marianna. Sparse convolutional neu-
ral networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 806–814,
2015.

Montufar, Guido F, Pascanu, Razvan, Cho, Kyunghyun,
and Bengio, Yoshua. On the number of linear regions
of deep neural networks. In Advances in neural infor-
mation processing systems, pp. 2924–2932, 2014.

Pan, Victor. Structured matrices and polynomials: uniﬁed
superfast algorithms. Springer Science & Business Me-
dia, 2001.

Pan, Victor Y, Svadlenka, John, and Zhao, Liang. Estimat-
ing the norms of random circulant and toeplitz matrices
and their inverses. Linear algebra and its applications,
468:197–210, 2015.

Sindhwani, Vikas, Sainath, Tara, and Kumar, Sanjiv. Struc-
In
tured transforms for small-footprint deep learning.
Advances in Neural Information Processing Systems, pp.
3088–3096, 2015.

Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V.

Se-
In
quence to sequence learning with neural networks.
Advances in neural information processing systems, pp.
3104–3112, 2014.

Telgarsky, Matus. Beneﬁts of depth in neural networks.

arXiv preprint arXiv:1602.04485, 2016.

Wen, Wei, Wu, Chunpeng, Wang, Yandan, Chen, Yiran,
and Li, Hai. Learning structured sparsity in deep neural
networks. In Advances in Neural Information Processing
Systems, pp. 2074–2082, 2016.

References

Barron, Andrew R. Universal approximation bounds for
superpositions of a sigmoidal function. IEEE Transac-
tions on Information theory, 39(3):930–945, 1993.

Bini, Dario, Pan, Victor, and Eberly, Wayne. Polynomial
and matrix computations volume 1: Fundamental algo-
rithms. SIAM Review, 38(1):161–164, 1996.

Cheng, Yu, Yu, Felix X, Feris, Rogerio S, Kumar, Sanjiv,
Choudhary, Alok, and Chang, Shi-Fu. An exploration of
parameter redundancy in deep networks with circulant
In Proceedings of the IEEE International
projections.
Conference on Computer Vision, pp. 2857–2865, 2015.

Cybenko, George. Approximation by superpositions of a
sigmoidal function. Mathematics of Control, Signals,
and Systems (MCSS), 2(4):303–314, 1989.

Delalleau, Olivier and Bengio, Yoshua. Shallow vs. deep
sum-product networks. In Advances in Neural Informa-
tion Processing Systems, pp. 666–674, 2011.

Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai,
and Fei-Fei, Li.
Imagenet: A large-scale hierarchical
image database. In Computer Vision and Pattern Recog-
nition, 2009. CVPR 2009. IEEE Conference on, pp. 248–
255. IEEE, 2009.

Denton, Emily L, Zaremba, Wojciech, Bruna, Joan, Le-
Cun, Yann, and Fergus, Rob. Exploiting linear structure
within convolutional networks for efﬁcient evaluation. In
Advances in Neural Information Processing Systems, pp.
1269–1277, 2014.

Gong, Yunchao, Liu, Liu, Yang, Ming, and Bourdev,
Lubomir. Compressing deep convolutional networks us-
ing vector quantization. arXiv preprint arXiv:1412.6115,
2014.

Han, Song, Mao, Huizi, and Dally, William J. Deep com-
pression: Compressing deep neural networks with prun-
ing, trained quantization and huffman coding. arXiv
preprint arXiv:1510.00149, 2015.

Hornik, Kurt. Approximation capabilities of multilayer
feedforward networks. Neural networks, 4(2):251–257,
1991.

Hornik, Kurt, Stinchcombe, Maxwell, and White, Halbert.
Multilayer feedforward networks are universal approxi-
mators. Neural networks, 2(5):359–366, 1989.

Jaderberg, Max, Vedaldi, Andrea, and Zisserman, Andrew.
Speeding up convolutional neural networks with low
rank expansions. arXiv preprint arXiv:1405.3866, 2014.

