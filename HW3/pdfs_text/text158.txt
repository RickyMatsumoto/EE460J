Random Feature Expansions for Deep Gaussian Processes

Kurt Cutajar 1 Edwin V. Bonilla 2 Pietro Michiardi 1 Maurizio Filippone 1

Abstract

The composition of multiple Gaussian Processes
as a Deep Gaussian Process (DGP) enables a deep
probabilistic nonparametric approach to ﬂexibly
tackle complex machine learning problems with
sound quantiﬁcation of uncertainty. Existing in-
ference approaches for DGP models have lim-
ited scalability and are notoriously cumbersome
to construct. In this work we introduce a novel
formulation of DGPs based on random feature
expansions that we train using stochastic varia-
tional inference. This yields a practical learn-
ing framework which signiﬁcantly advances the
state-of-the-art in inference for DGPs, and en-
ables accurate quantiﬁcation of uncertainty. We
extensively showcase the scalability and perfor-
mance of our proposal on several datasets with
up to 8 million observations, and various DGP ar-
chitectures with up to 30 hidden layers.

1. Introduction

Given their impressive performance on machine learn-
ing and pattern recognition tasks, Deep Neural Networks
(DNNs) have recently attracted a considerable deal of atten-
tion in several applied domains such as computer vision
and natural language processing; see, e.g., LeCun et al.
(2015) and references therein. Deep Gaussian Processes
(DGPs; Damianou & Lawrence, 2013) alleviate the out-
standing issue characterizing DNNs of having to specify
the number of units in hidden layers by implicitly working
with inﬁnite representations at each layer. From a gener-
ative perspective, DGPs transform the inputs using a cas-
cade of Gaussian Processes (GPs; Rasmussen & Williams,

1Department of Data Science, EURECOM, France
2School of Computer Science and Engineering, Univer-
Correspondence to:
sity of New South Wales, Australia.
Kurt Cutajar <kurt.cutajar@eurecom.fr>, Pietro Michiardi
<pietro.michiardi@eurecom.fr>, Edwin V. Bonilla Cuta-
jar <e.bonilla@unsw.edu.au>, Maurizio Filippone <maur-
izio.ﬁlippone@eurecom.fr>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

2006) such that the output of each layer of GPs forms the
input to the GPs at the next layer, effectively implementing
a deep probabilistic nonparametric model for compositions
of functions (Neal, 1996; Duvenaud et al., 2014).

Because of their probabilistic formulation, it is natural to
approach the learning of DGPs through Bayesian inference
techniques; however, the application of such techniques
to learn DGPs leads to various forms of intractability. A
number of contributions have been proposed to recover
tractability, extending or building upon the literature on ap-
proximate methods for GPs. Nevertheless, only few works
leverage one of the key features that arguably make DNNs
so successful, that is being scalable through the use of mini-
batch-based learning (Hensman & Lawrence, 2014; Dai
et al., 2016; Bui et al., 2016). Even among these works,
there does not seem to be an approach that is truly appli-
cable to large-scale problems, and practical beyond only a
few hidden layers.

In this paper, we develop a practical learning framework for
DGP models that signiﬁcantly improves the state-of-the-art
on those aspects. In particular, our proposal introduces two
sources of approximation to recover tractability, while (i)
scaling to large-scale problems, (ii) being able to work with
moderately deep architectures, and (iii) being able to accu-
rately quantify uncertainty. The ﬁrst is a model approxima-
tion, whereby the GPs at all layers are approximated using
random feature expansions (Rahimi & Recht, 2008); the
second approximation relies upon stochastic variational in-
ference to retain a probabilistic and scalable treatment of
the approximate DGP model.

We show that random feature expansions for DGP models
yield Bayesian DNNs with low-rank weight matrices, and
the expansion of different covariance functions results in
different DNN activation functions, namely trigonometric
for the Radial Basis Function (RBF) covariance, and Rec-
tiﬁed Linear Unit (ReLU) functions for the ARC-COSINE
covariance. In order to retain a probabilistic treatment of
the model we adapt the work on variational inference for
DNNs and variational autoencoders (Graves, 2011; Kingma
& Welling, 2014) using mini-batch-based stochastic gradi-
ent optimization, which can exploit GPU and distributed
computing. In this respect, we can view the probabilistic
treatment of DGPs approximated through random feature

Random Feature Expansions for Deep Gaussian Processes

expansions as a means to specify sensible and interpretable
priors for probabilistic DNNs. Furthermore, unlike popu-
lar inducing points-based approximations for DGPs, the re-
sulting learning framework does not involve any matrix de-
compositions in the size of the number of inducing points,
but only matrix products. We implement our model in Ten-
sorFlow (Abadi et al., 2015), which allows us to rely on
automatic differentiation to apply stochastic variational in-
ference.

Although having to select the appropriate number of ran-
dom features goes against the nonparametric formulation
favored in GP models, the level of approximation can be
tuned based on constraints on running time or hardware.
Most importantly, the random feature approximation en-
ables us to develop a learning framework for DGPs which
signiﬁcantly advances the state-of-the-art. We extensively
demonstrate the effectiveness of our proposal on a variety
of regression and classiﬁcation problems by comparing it
with DNNs and other state-of-the-art approaches to infer
DGPs. The results indicate that for a given DGP architec-
ture, our proposal is consistently faster at achieving better
generalization compared to the competitors. Another key
observation is that the proposed DGP outperforms DNNs
trained with dropout when quantifying uncertainty.

We focus part of the experiments on large-scale prob-
lems, such as MNIST8M digit classiﬁcation and the AIR-
LINE dataset, which contain over 8 and 5 million observa-
tions, respectively. Only very recently there have been at-
tempts to demonstrate performance of GP models on such
large data sets (Wilson et al., 2016; Krauth et al., 2016),
and our proposal is on par with these latest GP methods.
Furthermore, we obtain impressive results when employ-
ing our learning framework to DGPs with moderate depth
(few tens of layers) on the AIRLINE dataset. We are not
aware of any other DGP models having such depth that can
achieve comparable performance when applied to datasets
with millions of observations. Crucially, we obtain all these
results by running our algorithm on a single machine with-
out GPUs, but our proposal is designed to be able to exploit
GPU and distributed computing to signiﬁcantly accelerate
our deep probabilistic learning framework (see supplement
for experiments in distributed mode).

In summary, the most signiﬁcant contributions of this work
are as follows: (i) we propose a novel approximation of
DGPs based on random feature expansions that we study
in connection with DNNs; (ii) we demonstrate the ability
of our proposal to systematically outperform state-of-the-
art methods to carry out inference in DGP models, espe-
cially for large-scale problems and moderately deep archi-
tectures; (iii) we validate the superior quantiﬁcation of un-
certainty offered by DGPs compared to DNNs.

1.1. Related work

Following the original proposal of DGP models in Dami-
anou & Lawrence (2013), there have been several attempts
to extend GP inference techniques to DGPs. Notable ex-
amples include the extension of inducing point approxi-
mations (Hensman & Lawrence, 2014; Dai et al., 2016)
and Expectation Propagation (Bui et al., 2016). Sequen-
tial inference for training DGPs has also been investigated
in Wang et al. (2016). A recent example of a DGP “na-
tively” formulated as a variational model appears in Tran
et al. (2016). Our work is the ﬁrst to employ random fea-
ture expansions to approximate DGPs as DNNs. The expan-
sion of the squared exponential covariance for DGPs leads
to trigonometric DNNs, whose properties were studied in
Sopena et al. (1999). Meanwhile, the expansion of the arc-
cosine covariance is inspired by Cho & Saul (2009), and it
allows us to show that DGPs with such covariance can be
approximated with DNNs having ReLU activations.

The connection between DGPs and DNNs has been pointed
out in several papers, such as Neal (1996) and Duvenaud
et al. (2014), where pathologies with deep nets are investi-
gated. The approximate DGP model described in our work
becomes a DNN with low-rank weight matrices, which have
been used in, e.g., Novikov et al. (2015); Sainath et al.
(2013); Denil et al. (2013) as a regularization mechanism.
Dropout is another technique to speed-up training and im-
prove generalization of DNNs that has recently been linked
to variational inference (Gal & Ghahramani, 2016).

Random Fourier features for large scale kernel machines
were proposed in Rahimi & Recht (2008), and their ap-
plication to GPs appears in L´azaro-Gredilla et al. (2010).
In the case of squared exponential covariances, variational
learning of the posterior over the frequencies was proposed
in Gal & Turner (2015) to avoid potential overﬁtting caused
by optimizing these variables. These approaches are spe-
cial cases of our DGP model when using no hidden layers.

In our work, we learn the proposed approximate DGP model
using stochastic variational inference. Variational learning
for DNNs was ﬁrst proposed in Graves (2011), and later
extended to include the reparameterization trick to clamp
randomness in the computation of the gradient with respect
to the posterior over the weights (Kingma & Welling, 2014;
Rezende et al., 2014), and to include a Gaussian mixture
prior over the weights (Blundell et al., 2015).

2. Preliminaries

Consider a supervised learning scenario where a set of in-
put vectors X = [x1; : : : ; xn]⊤ is associated with a set of
(possibly multivariate) labels Y = [y1; : : : ; yn]⊤, where
xi 2 RDin and yi 2 RDout. We assume that there is an un-
derlying function fo(xi) characterizing a mapping from the

Random Feature Expansions for Deep Gaussian Processes

inputs to a latent representation, and that the labels are a re-
alization of some probabilistic process p(yiojfo(xi)) which
is based on this latent representation.

on random feature expansions for the radial basis function
(RBF) covariance and the ARC-COSINE covariance, which
we will use in our experiments.

In this work, we consider modeling the latent func-
tions using Deep Gaussian Processes (DGPs; Damianou &
Lawrence, 2013). Let variables in layer l be denoted by
the (l) superscript. In DGP models, the mapping between
inputs and labels is expressed as a composition of functions

(

)

f (x) =

f (Nh(cid:0)1) ◦ : : : ◦ f (0)

(x);

For the sake of clarity, we will present the covariances with-
out any explicit scaling of the features or the covariance it-
self. After explaining the random feature expansion associ-
ated with each covariance, we will generalize these results
in the context of DGPs to include scaling the covariance by
a factor (cid:27)2, and scaling the features for Automatic Rele-
vance Determination (ARD) (Mackay, 1994).

where each of the Nh layers is composed of a (possi-
bly transformed) multivariate Gaussian process (GP). For-
mally, a GP is a collection of random variables such that
any subset of these are jointly Gaussian distributed (Ras-
mussen & Williams, 2006). In GPs, the covariance between
variables at different inputs is modeled using the so-called
covariance function.

Given the relationship between GPs and single-layered neu-
ral networks with an inﬁnite number of hidden units (Neal,
1996), the DGP model has an obvious connection with
DNNs. In contrast to DNNs, where each of the hidden lay-
ers implements a parametric function of its inputs, in DGPs
these functions are assigned a GP prior, and are therefore
nonparametric. Furthermore, because of their probabilistic
formulation, it is natural to approach the learning of DGPs
through Bayesian inference techniques that lead to princi-
pled approaches for both determining the optimal settings
of architecture-dependent parameters, such as the number
of hidden layers, and quantiﬁcation of uncertainty.

While DGPs are attractive from a theoretical standpoint, in-
ference is extremely challenging. Denote by F (l) the set
of latent variables with entries f (l)
o (xi), and let
p(Y jF (Nh)) be the conditional likelihood. Learning and
making predictions with DGPs requires solving integrals
that are generally intractable. For example, computing the
marginal likelihood to optimize covariance parameters (cid:18)(l)
at all layers entails solving
∫

io = f (l)

(

)

(

p(Y jX; (cid:18)) =

p

p

F (Nh)jF (Nh(cid:0)1); (cid:18)(Nh(cid:0)1)

)

Y jF (Nh)
(

)

(cid:2) : : : (cid:2) p

F (1)jX; (cid:18)(0)

dF (Nh) : : : dF (1).

In the following section we use random feature approxi-
mations to the covariance function in order to develop a
scalable algorithm for inference in DGPs.

2.1. Random Feature Expansions for GPs

We start by describing how random feature expansions
can be used to approximate the covariance of a single GP
model. Such approximations have been considered previ-
ously, for example by Rahimi & Recht (2008) in the con-
text of non-probabilistic kernel machines. Here we focus

2.1.1. RADIAL BASIS FUNCTION COVARIANCE

A popular example of a covariance function, which we con-
sider here, is the Radial Basis Function (RBF) covariance

krbf (x; x′) = exp

∥x (cid:0) x′∥⊤

.

(1)

]

[

(cid:0) 1
2

Appealing to Bochner’s theorem, any continuous shift-
invariant normalized covariance function k(xi; xj) =
k(xi (cid:0) xj) is positive deﬁnite if and only if it can be rewrit-
ten as the Fourier transform of a non-negative measure
p(!) (Rahimi & Recht, 2008). Denoting the spectral fre-
(cid:0)1 and (cid:14) = xi (cid:0) xj,
quencies by !, while assigning (cid:19) =
in the case of the RBF covariance in equation (1), this
yields:

p

∫

krbf ((cid:14)) =

p(!) exp

(

)

(cid:19)(cid:14)⊤!

d!,

(2)

with a corresponding non-negative measure p(!) =
N (0; I). Because the covariance function and the non-
negative measures are real, we can drop the unnecessary
complex part of the argument of the expectation, keeping
cos((cid:14)⊤!) = cos((xi (cid:0) xj)⊤!) that can be rewritten as
cos(x⊤

j !) + sin(x⊤

i !) cos(x⊤

i !) sin(x⊤

j !).

The importance of the expansion above is that it allows us
to interpret the covariance function as an expectation that
can be estimated using Monte Carlo. Deﬁning z(xj!) =
[cos(x⊤!); sin(x⊤!)]⊤, the covariance function can be
therefore unbiasedly approximated as

krbf (xi; xj) (cid:25) 1
NRF

NRF∑

r=1

z(xij ~!r)⊤z(xjj ~!r),

(3)

with ~!r (cid:24) p(!). This has an important practical impli-
cation, as it provides the means to access an approximate
explicit representation of the mapping induced by the co-
variance function that, in the RBF case, is inﬁnite dimen-
sional (Shawe-Taylor & Cristianini, 2004). Various re-
sults have been established on the accuracy of the random
Fourier feature approximation; see, e.g., Rahimi & Recht
(2008).

Random Feature Expansions for Deep Gaussian Processes

X

(cid:8)(0)

F (1)

(cid:8)(1)

F (2)

Y

Ω(0)

W (0)

Ω(1)

W (1)

(cid:18)(0)

(cid:18)(1)

Figure1. The proposed DGP approximation. At each hidden layer
GPs are replaced by their two-layer weight-space approximation.
Random-features (cid:8)(l) are obtained using a weight matrix Ω(l).
This is followed by a linear transformation parameterized by
weights W (l). The prior over Ω(l) is determined by the covari-
ance parameters (cid:18)(l) of the original GPs.

2.1.2. ARC-COSINE COVARIANCE

We also consider the ARC-COSINE covariance of order p
))

(

(

arc (x; x′) =
k(p)

(∥x∥ ∥x′∥)p Jp

cos(cid:0)1

1
(cid:25)

x⊤x′
∥x∥∥x′∥

,

(4)

where we have deﬁned

Jp((cid:11)) = ((cid:0)1)p(sin (cid:11))2p+1

(

1
sin (cid:11)

@
@(cid:11)

)p (

)

.

(cid:25) (cid:0) (cid:11)
sin (cid:11)

Let H((cid:1)) be the Heaviside function. Following Cho & Saul
(2009), an integral representation of this covariance is:

∫

arc (x; x′) = 2
k(p)

H(!⊤x)

(
!⊤x

)p

(

H(!⊤x′)

!⊤x′

(cid:2) N (!j0; I)d!.

)p

(5)

This integral formulation immediately suggests a random
feature approximation for the ARC-COSINE covariance in
equation (4), noting that it can be seen as an expectation
of the product of the same function applied to the inputs to
the covariance. As before, this provides an approximate ex-
plicit representation of the mapping induced by the covari-
ance function. Interestingly, for the ARC-COSINE covari-
ance of order p = 1, this yields an approximation based on
popular Rectiﬁed Linear Unit (ReLU) functions. We note
that when p = 0 the resulting Heaviside activations are
unsuitable for our inference scheme, given that they yield
systematically zero gradients.

3. Random Feature Expansions for DGPs

In this section, we present our approximate formulation of
DGPs which, as we illustrate in the experiments, leads to
a practical learning algorithm for these deep probabilistic

nonparametric models. We propose to employ the random
feature expansion at each layer, and by doing so we ob-
tain an approximation to the original DGP model as a DNN
(Figure 1).

Assume that the GPs have zero mean, and deﬁne F (0) :=
X. Also, assume that the GP covariances at each layer
are parameterized through a set of parameters (cid:18)(l). The
parameter set (cid:18)(l) comprises the layer-wise GP marginal
variances ((cid:27)2)(l) and lengthscale parameters (cid:3)(l) =
diag((ℓ2

1)(l); : : : ; (ℓ2

)(l)).

D(l)
F

Considering a DGP with RBF covariances, taking a “weight-
space view” of the GPs at each layer, and extending the
results in the previous section, we have that

(cid:8)(l)

rbf =

√

((cid:27)2)(l)
N (l)
RF

(

[
cos

)

(

)]

F (l)Ω(l)

; sin

F (l)Ω(l)

,

(

)

(

)

(

(

)

(cid:3)(l)

and p

= N

W (l)
(cid:1)i

(6)
and F (l+1) = (cid:8)(l)
rbf W (l). At each layer, the priors over the
)(cid:0)1
Ω(l)
=
0;
weights are p
(cid:1)j
N (0; I). Each matrix Ω(l) has dimensions DF (l) (cid:2) N (l)
RF.
On the other hand, the weight matrices W (l) have dimen-
sions 2N (l)
(cid:2) DF (l+1) (weighting of sine and cosine ran-
RF
dom features), with the constraint that DF (Nh ) = Dout.
Similarly, considering a DGP with ARC-COSINE covari-
ances of order p = 1, the application of the random feature
approximation leads to DNNs with ReLU activations:

√

(cid:8)(l)

arc =

(

2((cid:27)2)(l)
N (l)
RF
)(cid:0)1

(

)

(

)

max

0; F (l)Ω(l)

,

(7)

0;

(cid:3)(l)

(cid:24) N

with Ω(l)
, which are cheaper to eval-
(cid:1)j
uate and differentiate than the trigonometric functions re-
quired in the RBF case. As in the RBF case, we allowed
the covariance and the features to be scaled by ((cid:27)2)(l) and
(cid:3)(l), respectively. The dimensions of the weight matrices
Ω(l) are the same as in the RBF case, but the dimensions of
the W (l) matrices are N (l)
RF

(cid:2) DF (l+1).

3.1. Low-rank weights in the resulting DNN

(

)

(cid:8)(l)W (l)Ω(l+1)

Our formulation of an approximate DGP using random
feature expansions reveals a close connection with DNNs.
In our formulation, the design matrices at each layer are
, where (cid:13)((cid:1)) denotes the
(cid:8)(l+1) = (cid:13)
element-wise application of covariance-dependent func-
tions, i.e., sine and cosine for the RBF covariance and ReLU
for the ARC-COSINE covariance. Instead, for the DNN case,
the design matrices are computed as (cid:8)(l+1) = g((cid:8)(l)Ω(l)),
where g((cid:1)) is a so-called activation function. In light of this,
we can view our approximate DGP model as a DNN. From a
probabilistic standpoint, we can interpret our approximate

Random Feature Expansions for Deep Gaussian Processes

DGP model as a DNN with speciﬁc Gaussian priors over
the Ω(l) weights controlled by the covariance parameters
(cid:18)(l), and standard Gaussian priors over the W (l) weights.
Covariance parameters act as hyper-priors over the weights
Ω(l), and the objective is to optimize these during training.

)

(

N (l)
RF

Another observation about the resulting DGP approxima-
tion is that, for a given layer l, the transformations given
by W (l) and Ω(l+1) are both linear.
If we collapsed
the two transformations into a single one, by introduc-
ing weights (cid:4)(l) = W (l)Ω(l+1), we would have to learn
O
weights at each layer, which is con-
siderably more than learning the two separate sets of
weights. As a result, we can view the proposed approxi-
mate DGP model as a way to impose a low-rank structure
on the weights of DNNs, which is a form of regularization
proposed in the literature of DNNs (Novikov et al., 2015;
Sainath et al., 2013; Denil et al., 2013).

(cid:2) N (l+1)
RF

3.2. Variational inference

∏

Nh(cid:0)1
l=0

In order to keep the notation uncluttered, let (cid:2) be the col-
lection of all covariance parameters (cid:18)(l) at all layers. Also,
consider the case of a DGP with ﬁxed spectral frequencies
Ω(l) collected into Ω, and let W be the collection of the
weight matrices W (l) at all layers. For W we have a prod-
uct of standard normal priors stemming from the approxi-
p(W (l)),
mation of the GPs at each layer p(W) =
and we propose to treat W using variational inference fol-
lowing Kingma & Welling (2014) and Graves (2011), and
optimize all covariance parameters (cid:2). We will consider Ω
to be ﬁxed here, but we will discuss alternative ways to treat
Ω in the next section. In the supplement we also assess the
quality of the variational approximation over W, with Ω
and (cid:2) ﬁxed, by comparing it with MCMC techniques.
The marginal likelihood p(Y jX; Ω; (cid:2)) involves intractable
integrals, but we can obtain a tractable lower bound using
variational inference. Deﬁning L = log [p(Y jX; Ω; (cid:2))]
and E = Eq(W) (log [p (Y jX; W; Ω; (cid:2))]), we obtain

L (cid:21) E (cid:0) DKL [q(W)∥p (W)] ,

(8)

where q(W) acts as an approximation to the posterior over
all the weights p(WjY; X; Ω; (cid:2)).

We are interested in optimizing q(W), i.e. ﬁnding an op-
timal approximate distribution over the parameters accord-
ing to the bound above. The ﬁrst term can be interpreted as
a model ﬁtting term, whereas the second as a regularization
term. In the case of a Gaussian distribution q(W) and a
Gaussian prior p(W), it is possible to compute the DKL
term analytically (see supplementary material), whereas
the remaining term needs to be estimated. Assume a Gaus-
sian approximating distribution that factorizes across layers

and weights:

∏

(

)

∏

(

)

q(W) =

q

W (l)
ij

=

N

m(l)

ij ; (s2)(l)

ij

.

(9)

ijl

ijl

The variational parameters are the mean and the variance
of each of the approximating factors m(l)
ij , and we
aim to optimize the lower bound with respect to these as
well as all covariance parameters (cid:2).

ij ; (s2)(l)

In the case of a likelihood that factorizes across observa-
tions, an interesting feature of the expression of the lower
bound is that it is amenable to fast stochastic optimization.
In particular, we derive a doubly-stochastic approximation
of the expectation in the lower bound as follows. First,
E can be rewritten as a sum over the input points, which
allows us to estimate it in an unbiased fashion using mini-
batches, selecting m points indexed by Im:

Eq(W)(log[p(ykjxk; W; Ω; (cid:2))]).

(10)

E (cid:25) n
m

∑

k2Im

Second, each of the elements of the sum can be estimated
using NMC Monte Carlo samples, yielding:

E (cid:25) n
m

∑

k2Im

1
NMC

NMC∑

r=1

log[p(ykjxk; ~Wr; Ω; (cid:2))], (11)

with ~Wr (cid:24) q(W). In order to facilitate the optimization,
we reparameterize the weights as follows:

( ~W (l)

r )ij = s(l)

ij ϵ(l)

rij + m(l)
ij .

(12)

By differentiating the lower bound with respect to (cid:2) and
the mean and variance of the approximate posterior over
W, we obtain an unbiased estimate of the gradient for the
lower bound. The reparameterization trick ensures that the
randomness in the computation of the expectation is ﬁxed
when applying stochastic gradient ascent moves to the pa-
rameters of q(W) and (cid:2) (Kingma & Welling, 2014). Au-
tomatic differentiation tools enabled us to compute stochas-
tic gradients automatically, which is why we opted to im-
plement our model in TensorFlow (Abadi et al., 2015).

3.3. Treatment of the spectral frequencies Ω

So far, we have assumed the spectral frequencies Ω to
be sampled from the prior and ﬁxed throughout, whereby
we employ the reparameterization trick to obtain Ω(l)
ij =
((cid:12)2)(l)
ij determined by the
. We then draw the

ij , with ((cid:12)2)(l)
ij and (cid:22)(l)
(
)(cid:0)1
(
= N
(cid:3)(l)

prior p
"(l)
rij’s and ﬁx them from the outset, such that covariance
parameters (cid:2) can be optimized along with q(W). We re-
fer to this variant as PRIOR-FIXED.

ij "(l)
rij + (cid:22)(l)
)
(
Ω(l)
(cid:1)j

0;

)

Random Feature Expansions for Deep Gaussian Processes

samples from the posterior over W (and Ω when treated
variationally) and given the mini-batch formulation, the
former costs O
RFNMC
, while the latter costs
RFD(l)
mN (l)

mD(l)
F N (l)
)
.

F NMC

O

)

(

(

Because of feature expansions and stochastic variational
inference, the resulting algorithm does not involve any
Cholesky decompositions. This is in sharp contrast with
stochastic variational inference using inducing-point ap-
proximations (see e.g. Dai et al., 2016; Bui et al., 2016),
where such operations could signiﬁcantly limit the number
of inducing points that can be employed.

4. Experiments

We evaluate our model by comparing it against relevant al-
ternatives for both regression and classiﬁcation, and assess
its performance when applied to large-scale datasets. We
also investigate the extent to which such deep compositions
continue to yield good performance when the number of
layers is signiﬁcantly increased.

4.1. Model Comparison

We primarily compare our model to the state-of-the-art
DGP inference method presented in the literature, namely
DGPs using expectation propagation (DGP-EP; Bui et al.,
2016). We originally intended to include results for the
variational auto-encoded DGP (Damianou & Lawrence,
2013); however, the results obtained using the available
code were not competitive with DGP-EP and we thus de-
cided to exclude them from the ﬁgures. We also omit-
ted DGP training using sequential inference (Wang et al.,
2016) given that we could not ﬁnd an implementation of
the method and, in any case, the performance reported in
the paper is inferior to more recent approaches. We also
compare against DNNs in order to present the results in a
wider context, and demonstrate that DGPs lead to better
quantiﬁcation of uncertainty. Finally, to substantiate the
beneﬁts of using a deep model, we compare against the
shallow sparse variational GP (Hensman et al., 2015b) im-
plemented in GPﬂow (Matthews et al., 2016).

We use the same experimental set-up for both regression
and classiﬁcation tasks using datasets from the UCI repos-
itory (Asuncion & Newman, 2007), for models having one
hidden layer. The results for architectures with two hid-
den layers are included in the supplementary material. The
speciﬁc conﬁgurations for each model are detailed below:

Figure2. Performance of different strategies for dealing with Ω as
a function of the number of random features. These can be ﬁxed
(PRIOR-FIXED), or treated variationally (with ﬁxed randomness
VAR-FIXED and resampled at each iteration VAR-RESAMPLED).

Inspired by previous work on random feature expansions
for GPs, we can think of alternative ways to treat these pa-
rameters, e.g., L´azaro-Gredilla et al. (2010); Gal & Turner
(2015). In particular, we study a variational treatment of
Ω; we refer the reader to the supplementary material for
details on the derivation of the lower bound in this case.
When being variational about Ω, we introduce an approxi-
mate posterior q(Ω) which also has a factorized form. We
use the reparameterization trick once again, but Ω are now
sampled from the posterior, which in general has different
mean and variances to the prior. We report two variations of
this treatment, namely VAR-FIXED and VAR-RESAMPLED.
In VAR-FIXED, we ﬁx "(l)
rij in computing Ω throughout the
learning of the model, whereas in VAR-RESAMPLED we re-
sample these at each iteration. We note that one can also be
variational about (cid:2), but we leave this for future work.

In Figure 2, we illustrate the differences between the strate-
gies discussed in this section; we report the accuracy of the
proposed one-layer DGP with RBF covariances with respect
to the number of random features on one of the datasets that
we consider in the experiment section (EEG dataset). For
PRIOR-FIXED, more random features result in a better ap-
proximation of the GP priors at each layer, and this results
in better generalization. When we resample Ω from the
approximate posterior (VAR-RESAMPLED), we notice that
the model quickly struggles with the optimization when in-
creasing the number of random features. We attribute this
to the fact that the factorized form of the posterior over Ω
and W is unable to capture posterior correlations between
the coefﬁcients for the random features and the weights
of the corresponding linearized model. Being determinis-
tic about the way spectral frequencies are computed (VAR-
FIXED) offers the best performance among the three learn-
ing strategies, and this is what we employ throughout the
rest of this paper.

3.4. Computational complexity

When estimating the lower bound, there are two main
operations performed at each layer, that is F (l)Ω(l) and
(cid:8)(l)W (l). Recalling that this matrix product is done for

DGP-RBF, DGP-ARC : In the proposed DGP with an RBF
kernel, we use 100 random features at every hidden layer
to construct a multivariate GP with D(l)
F = 3, and set the
batch size to m = 200. We initially only use a single

11:522:533:50:20:4log10(RFs)RMSE11:522:533:50:20:40:6log10(RFs)MNLLprior-fixedvar-fixedvar-resampledRandom Feature Expansions for Deep Gaussian Processes

REGRESSION

CLASSIFICATION

Powerplant
(n = 9568, d=4)

RMSE

Protein
(n = 45730, d=9)
RMSE

Spam
(n = 4061, d=57)

Error rate

EEG
(n = 14979, d=14)
Error rate

MNIST
(n = 60000, d=784)
Error rate

3:5

2

3:5

1

1:5

2

2:5

3

3:5

2:5
3
log10(sec)

MNLL

2:5

3

log10(sec)

MNLL

log10(sec)

MNLL

2:5

3

3:5

log10(sec)

MNLL

3:5

4
log10(sec)

4:5

MNLL

0:8

0:7

1:2

1:1

1

0:1

0:05

3

2

1

0

0:2

0:1

0

2

0:4

0:2

2:5
3
log10(sec)

2:5

3

log10(sec)

3:5

2

3:5

1

1:5

3

3:5

2

2:5

3

3:5

2

2:5

log10(sec)

log10(sec)

3:5

4
log10(sec)

4:5

0:2

0:15

0:1

0:05

0

3

3

2

1

0

3

0:5

0:4

0:3

0:2

2

1

0:5

0

2

Figure3. Progression of error rate (RMSE in the regression case) and MNLL over time for competing models. Results are shown for
conﬁgurations having 1 hidden layer, while the results for models having 2 such layers may be found in the supplementary material.

Monte Carlo sample, and halfway through the allocated
optimization time, this is then increased to 100 samples.
We employ the Adam optimizer (Kingma & Ba, 2015)
with a learning rate of 0:01, and in order to stabilize the op-
timization procedure, we ﬁx the parameters (cid:2) for 12; 000
iterations, before jointly optimizing all parameters. As dis-
cussed in Section 3.3, Ω are optimized variationally with
ﬁxed randomness. The same set-up is used for DGP-ARC,
the variation of our model using the ARC-COSINE kernel;

DGP-EP 1: For this technique, we use the same architec-
ture and optimizer as for DGP-RBF and DGP-ARC, a batch
size of 200 and 100 inducing points at each hidden layer.
For the classiﬁcation case, we use 100 samples for approx-
imating the Softmax likelihood;

DNN : We construct a DNN conﬁgured with a dropout rate
of 0:5 at each hidden layer in order to provide regular-
ization during training. In order to preserve a degree of
fairness, we set the number of hidden units in such a way
as to ensure that the number of weights to be optimized
match those in the DGP-RBF and DGP-ARC models when
the random features are taken to be ﬁxed.

We assess the performance of each model using the error
rate (RMSE in the regression case) and mean negative log-
likelihood (MNLL) on withheld test data. The results are
averaged over 3 folds for every dataset. The experiments
were launched on single nodes of a cluster of Intel Xeon
E5-2630 CPUs having 32 cores and 128GB RAM.

Figure 3 shows that DGP-RBF and DGP-ARC consistently
outperform competing techniques both in terms of con-
vergence speed and predictive accuracy. This is particu-

1Code obtained from:

github.com/thangbui/deepGP_approxEP

larly signiﬁcant for larger datasets where other techniques
take considerably longer to converge to a reasonable error
rate, although DGP-EP converges to superior MNLL for the
PROTEIN dataset. The results are also competitive (and
sometimes superior) to those obtained by the variational
GP (VAR-GP) in Hensman et al. (2015b). It is striking to
see how inferior uncertainty quantiﬁcation provided by the
DNN (which is inherently limited to the classiﬁcation case,
so no MNLL reported on regression datasets) is compared
to DGPs, despite the error rate being comparable.

By virtue of its higher dimensionality, larger conﬁgurations
were used for MNIST. For DGP-RBF and DGP-ARC, we use
500 random features, 50 GPs in the hidden layers, batch
size of 1000, and Adam with a 0:001 learning rate. Simi-
larly for DGP-EP, we use 500 inducing points, with the only
difference being a slightly smaller batch size to cater for is-
sues with memory requirements. Following Simard et al.
(2003), we employ 800 hidden units at each layer of the
DNN. The DGP-RBF peaks at 98:04% and 97:93% for 1
and 2 hidden layers respectively. It was observed that the
model performance degrades noticeably when more than
2 hidden layers are used (without feeding forward the in-
puts). This is in line with what is reported in the literature
on DNNs (Neal, 1996; Duvenaud et al., 2014). By simply
re-introducing the original inputs in the hidden layer, the
accuracy improves to 98:2% for the one hidden layer case.

Recent experiments on MNIST using a variational GP with
MCMC report overall accuracy of 98:04% (Hensman et al.,
2015a), while the AutoGP architecture has been shown
to give 98:45% accuracy (Krauth et al., 2016). Using a
ﬁner-tuned conﬁguration, DNNs were also shown to obtain
98:5% accuracy (Simard et al., 2003), whereas 98:6% has
been reported for SVMs (Sch¨olkopf, 1997). In view of this
wider scope of inference techniques, it can be conﬁrmed

dgp-rbfdgp-arcdgp-epdnnvar-gpRandom Feature Expansions for Deep Gaussian Processes

Table1. Performance of our proposal on large-scale datasets.

Dataset

Accuracy
ARC
RBF

MNLL

RBF

ARC

MNIST8M
AIRLINE

99:14% 99:04%
78:55% 72:76%

0:0454
0:4583

0:0465
0:5335

0:5

0:4

0:3

0:2

2

Error rate

MNLL

(cid:1)106

Neg. Lower Bound

0:6

0:55

0:5

0:45

2:7

2:6

3
4
log10(sec)

5

2

3
4
log10(sec)

5

2

10
20
Layers

30

that the results obtained using the proposed architecture
are comparable to the state-of-the-art, even if further ex-
tensions may be required for obtaining a proper edge. Note
that this comparison focuses on approaches without prepro-
cessing, and excludes convolutional neural nets.

4.2. Large-scale Datasets

One of the deﬁning characteristics of our model is the abil-
ity to scale up to large datasets without compromising on
performance and accuracy in quantifying uncertainty. As
a demonstrative example, we evaluate our model on two
large-scale problems which go beyond the scale of datasets
to which GPs and especially DGPs are typically applied.

We ﬁrst consider MNIST8M, which artiﬁcially extends the
original MNIST dataset to 8+ million observations. We
trained this model using the same conﬁguration described
for standard MNIST, and we obtained 99:14% accuracy
on the test set using one hidden layer. Given the size of
this dataset, there are only few reported results for other
GP models. Most notably, Krauth et al. (2016) recently
obtained 99:11% accuracy with the AutoGP framework,
which is comparable to the result obtained by our model.

Meanwhile, the AIRLINE dataset contains ﬂight informa-
tion for 5+ million US ﬂights. Following the procedure de-
scribed in Hensman et al. (2013) and Wilson et al. (2016),
we use this 8-dimensional dataset for classiﬁcation, where
the task is to determine whether a ﬂight has been delayed
or not. We construct the test set using the scripts provided
in Wilson et al. (2016), where 100; 000 data points are held-
out for testing. We construct our DGP models using 100
random features at each layer, and set the dimensionality
to DF (l) = 3. As shown in Table 1, our model works sig-
niﬁcantly better when using the RBF kernel. In addition,
the results are also directly comparable to those obtained
by Wilson et al. (2016), which reports accuracy and MNLL
of 78:1% and 0:457, respectively.

4.3. Model Depth

Finally, we assess the scalability of our model with respect
In
to additional hidden layers in the constructed model.
particular, we re-consider the AIRLINE dataset and evaluate
the performance of DGP-RBF models constructed using up
to 30 layers. In order to cater for the increased depth in the

Figure4. Left and center - Performance of our model on the AIR-
LINE dataset as function of time for different depths. The baseline
(SV-DKL) is taken from Wilson et al. (2016). Right - The box
plot of the negative lower bound, estimated over 100 mini-batches
of size 50; 000, conﬁrms this is a suitable objective for model se-
lection.

model, we feed-forward the original input to each hidden
layer, as suggested in Duvenaud et al. (2014).

Figure 4 reports the progression of error rate and MNLL
over time for different number of hidden layers, using the
results obtained in Wilson et al. (2016) as a baseline (re-
portedly obtained in about 3 hours). As expected, the
model takes longer to train as the number of layers in-
creases. However, the model converges to an optimal state
in every case in less than a couple of hours, with an im-
provement being noted in the case of 10 and 20 layers over
the shallower 2-layer model. The box plot within the same
ﬁgure indicates that the negative lower bound is a suitable
objective function for carrying out model selection.

5. Conclusions

In this work, we have proposed a novel formulation of
DGPs which exploits the approximation of covariance func-
tions using random features, as well as stochastic varia-
tional inference for preserving the probabilistic representa-
tion of a regular GP. We demonstrated how inference using
this model is not only faster, but also frequently superior
to other state-of-the-art methods, with particular empha-
sis on competing DGP models. The results obtained for
both the AIRLINE dataset and the MNIST8M digit recogni-
tion problem are particularly impressive since such large
datasets have been generally considered to be beyond the
computational scope of DGPs. We perceive this to be a
considerable step forward in the direction of scaling and
accelerating DGPs.

The results obtained on higher-dimensional datasets
strongly suggest that approximations such as Fastfood (Le
et al., 2013) could be instrumental in the interest of using
more random features. We are also currently investigat-
ing ways to mitigate the decline in performance observed
when optimizing Ω variationally with resampling. The ob-
tained results also encourage the extension of our model to
include convolutional layers suitable for computer vision
applications.

2layers10layers20layers30layersSV-DKLRandom Feature Expansions for Deep Gaussian Processes

ACKNOWLEDGEMENTS

MF gratefully acknowledges support from the AXA Re-
search Fund. PM was partially supported by the EU project
H2020-644182 “IOStack”.

References

Abadi, Mart´ın, Agarwal, Ashish, Barham, Paul, et al. Ten-
sorFlow: Large-scale machine learning on heteroge-
neous systems, 2015.

Asuncion, Arthur and Newman, David J. UCI machine

learning repository, 2007.

Blundell, Charles, Cornebise, Julien, Kavukcuoglu, Koray,
and Wierstra, Daan. Weight Uncertainty in Neural Net-
work. In Proceedings of the 32nd International Confer-
ence on Machine Learning, ICML 2015, Lille, France,
6-11 July 2015, volume 37 of JMLR Workshop and Con-
ference Proceedings, pp. 1613–1622. JMLR.org, 2015.

Bui, Thang D., Hern´andez-Lobato, Daniel, Hern´andez-
Lobato, Jos´e M., Li, Yingzhen, and Turner, Richard E.
Deep Gaussian Processes for Regression using Approx-
In Proceedings of the
imate Expectation Propagation.
33nd International Conference on Machine Learning,
ICML 2016, New York City, NY, USA, June 19-24, 2016,
volume 48 of JMLR Workshop and Conference Proceed-
ings, pp. 1472–1481. JMLR.org, 2016.

Cho, Youngmin and Saul, Lawrence K. Kernel methods for
deep learning. In Advances in Neural Information Pro-
cessing Systems 22: 23rd Annual Conference on Neural
Information Processing Systems 2009. Proceedings of a
meeting held 7-10 December 2009, Vancouver, British
Columbia, Canada., pp. 342–350, 2009.

Dai, Zhenwen, Damianou, Andreas, Gonz´alez, Javier, and
Lawrence, Neil. Variational auto-encoded deep Gaus-
In Proceedings of the Fourth Interna-
sian processes.
tional Conference on Learning Representations, ICLR
2016, San Juan, Puerto Rico, 2-4 May, 2016, 2016.

Damianou, Andreas C. and Lawrence, Neil D. Deep Gaus-
sian Processes. In Proceedings of the Sixteenth Interna-
tional Conference on Artiﬁcial Intelligence and Statis-
tics, AISTATS 2013, Scottsdale, AZ, USA, April 29 - May
1, 2013, volume 31 of JMLR Proceedings, pp. 207–215.
JMLR.org, 2013.

Denil, Misha, Shakibi, Babak, Dinh, Laurent, Ranzato,
Marc’Aurelio, and de Freitas, Nando. Predicting Param-
eters in Deep Learning. In Advances in Neural Informa-
tion Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Proceed-
ings of a meeting held December 5-8, 2013, Lake Tahoe,
Nevada, United States., pp. 2148–2156, 2013.

Duvenaud, David K., Rippel, Oren, Adams, Ryan P., and
Ghahramani, Zoubin. Avoiding pathologies in very deep
In Proceedings of the Seventeenth Interna-
networks.
tional Conference on Artiﬁcial Intelligence and Statis-
tics, AISTATS 2014, Reykjavik, Iceland, April 22-25,
2014, volume 33 of JMLR Workshop and Conference
Proceedings, pp. 202–210. JMLR.org, 2014.

Gal, Yarin and Ghahramani, Zoubin.

Dropout as a
Bayesian Approximation: Representing Model Uncer-
In Proceedings of the 33nd
tainty in Deep Learning.
International Conference on Machine Learning, ICML
2016, New York City, NY, USA, June 19-24, 2016, vol-
ume 48 of JMLR Workshop and Conference Proceed-
ings, pp. 1050–1059. JMLR.org, 2016.

Gal, Yarin and Turner, Richard. Improving the Gaussian
Process Sparse Spectrum Approximation by Represent-
ing Uncertainty in Frequency Inputs. In Proceedings of
the 32nd International Conference on Machine Learn-
ing, ICML 2015, Lille, France, 6-11 July 2015, vol-
ume 37 of JMLR Workshop and Conference Proceed-
ings, pp. 655–664. JMLR.org, 2015.

Graves, Alex. Practical Variational Inference for Neural
Networks. In Shawe-Taylor, J., Zemel, R. S., Bartlett,
P. L., Pereira, F., and Weinberger, K. Q. (eds.), Advances
in Neural Information Processing Systems 24, pp. 2348–
2356. Curran Associates, Inc., 2011.

Hensman, James and Lawrence, Neil D. Nested Varia-
tional Compression in Deep Gaussian Processes, De-
cember 2014.

Hensman, James, Fusi, Nicol´o, and Lawrence, Neil D.
Gaussian processes for big data. In Proceedings of the
Twenty-Ninth Conference on Uncertainty in Artiﬁcial In-
telligence, UAI 2013, Bellevue, WA, USA, August 11-15,
2013, 2013.

Hensman, James, de G. Matthews, Alexander G., Fil-
ippone, Maurizio, and Ghahramani, Zoubin. MCMC
In Ad-
for variationally sparse Gaussian processes.
vances in Neural Information Processing Systems 28:
Annual Conference on Neural Information Processing
Systems 2015, December 7-12, 2015, Montreal, Quebec,
Canada, pp. 1648–1656, 2015a.

Hensman, James, de G. Matthews, Alexander G., and
Ghahramani, Zoubin. Scalable variational Gaussian pro-
In Proceedings of the Eighteenth
cess classiﬁcation.
International Conference on Artiﬁcial Intelligence and
Statistics, AISTATS 2015, San Diego, California, USA,
May 9-12, 2015, pp. 351–360, 2015b.

Kingma, Diederik P. and Ba, Jimmy. Adam: A method
for stochastic optimization. In Proceedings of the Third

Random Feature Expansions for Deep Gaussian Processes

International Conference on Learning Representations,
ICLR 2015, San Diego, California, 7-9 May, 2015, 2015.

Kingma, Diederik P. and Welling, Max. Auto-Encoding
Variational Bayes. In Proceedings of the Second Inter-
national Conference on Learning Representations, ICLR
2014, Banff, Canada, April 14-16, 2014, 2014.

Krauth, Karl, Bonilla, Edwin V., Cutajar, Kurt, and Filip-
pone, Maurizio. AutoGP: Exploring the Capabilities and
Limitations of Gaussian Process Models. arXiv preprint
1610.05392, October 2016.

L´azaro-Gredilla, M., Quinonero-Candela, J., Rasmussen,
C. E., and Figueiras-Vidal, A. R. Sparse Spectrum Gaus-
sian Process Regression. Journal of Machine Learning
Research, 11:1865–1881, 2010.

Le, Quoc V., Sarls, Tams, and Smola, Alexander J. Fast-
food - computing Hilbert space expansions in loglinear
time. In ICML (3), volume 28 of JMLR Workshop and
Conference Proceedings, pp. 244–252. JMLR.org, 2013.

LeCun, Yann, Bengio, Yoshua, and Hinton, Geoffrey. Deep

learning. Nature, 521(7553):436–444, 2015.

Mackay, D. J. C. Bayesian methods for backpropagation
networks. In Domany, E., van Hemmen, J. L., and Schul-
ten, K. (eds.), Models of Neural Networks III, chapter 6,
pp. 211–254. Springer, 1994.

Matthews, Alexander G. de G., van der Wilk, Mark, Nick-
son, Tom, Fujii, Keisuke., Boukouvalas, Alexis, Le´on-
Villagr´a, Pablo, Ghahramani, Zoubin, and Hensman,
James. GPﬂow: A Gaussian process library using Ten-
sorFlow. arXiv preprint 1610.08733, October 2016.

Neal, Radford M. Bayesian Learning for Neural Networks
(Lecture Notes in Statistics). Springer, 1 edition, August
1996. ISBN 0387947248.

Novikov, Alexander, Podoprikhin, Dmitry, Osokin, Anton,
and Vetrov, Dmitry P. Tensorizing Neural Networks. In
Advances in Neural Information Processing Systems 28:
Annual Conference on Neural Information Processing
Systems 2015, December 7-12, 2015, Montreal, Quebec,
Canada, pp. 442–450, 2015.

Rahimi, Ali and Recht, Benjamin. Random Features for
Large-Scale Kernel Machines. In Platt, J. C., Koller, D.,
Singer, Y., and Roweis, S. T. (eds.), Advances in Neu-
ral Information Processing Systems 20, pp. 1177–1184.
Curran Associates, Inc., 2008.

Rasmussen, Carl E. and Williams, Christopher. Gaussian
Processes for Machine Learning. MIT Press, 2006.

Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,
Daan. Stochastic backpropagation and approximate in-
In Proceedings of
ference in deep generative models.
the 31th International Conference on Machine Learn-
ing, ICML 2014, Beijing, China, 21-26 June 2014, vol-
ume 32 of JMLR Workshop and Conference Proceed-
ings, pp. 1278–1286. JMLR.org, 2014.

Sainath, Tara N., Kingsbury, Brian, Sindhwani, Vikas,
Arisoy, Ebru, and Ramabhadran, Bhuvana. Low-rank
matrix factorization for Deep Neural Network training
with high-dimensional output targets. In IEEE Interna-
tional Conference on Acoustics, Speech and Signal Pro-
cessing, ICASSP 2013, Vancouver, BC, Canada, May 26-
31, 2013, pp. 6655–6659. IEEE, 2013. doi: 10.1109/
ICASSP.2013.6638949.

Sch¨olkopf, Bernhard. Support vector learning. PhD thesis,

Berlin Institute of Technology, 1997.

Shawe-Taylor, John and Cristianini, Nello. Kernel Methods
for Pattern Analysis. Cambridge University Press, New
York, NY, USA, 2004.

Simard, Patrice Y., Steinkraus, Dave, and Platt, John C.
Best Practices for Convolutional Neural Networks Ap-
In Proceedings
plied to Visual Document Analysis.
of the Seventh International Conference on Document
Analysis and Recognition - Volume 2, ICDAR ’03, Wash-
ington, DC, USA, 2003. IEEE Computer Society.

Sopena, J. M., Romero, E., and Alquezar, R. Neural net-
works with periodic and monotonic activation functions:
a comparative study in classiﬁcation problems. In Arti-
ﬁcial Neural Networks, 1999. ICANN 99. Ninth Interna-
tional Conference on (Conf. Publ. No. 470), volume 1,
1999. doi: 10.1049/cp:19991129.

Tran, Dustin, Ranganath, Rajesh, and Blei, David M. The
In Proceedings of the
Variational Gaussian Process.
Fourth International Conference on Learning Represen-
tations, ICLR 2016, San Juan, Puerto Rico, 2-4 May,
2016, 2016.

Wang, Yali, Brubaker, Marcus A., Chaib-draa, Brahim, and
Urtasun, Raquel. Sequential inference for deep Gaussian
process. In Proceedings of the 19th International Con-
ference on Artiﬁcial Intelligence and Statistics, AISTATS
2016, Cadiz, Spain, May 9-11, 2016, pp. 694–703, 2016.

Wilson, Andrew Gordon, Hu, Zhiting, Salakhutdinov, Rus-
lan, and Xing, Eric P. Stochastic variational deep kernel
In Advances in Neural Information Process-
learning.
ing Systems 29: Annual Conference on Neural Infor-
mation Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain, pp. 2586–2594, 2016.

