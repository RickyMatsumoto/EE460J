EfﬁcientDistributedLearningwithSparsityA.IllustrativeExamplesofGeneralSparseLearningProblemsInthissectionwediscussadditionalexamplesofhigh-dimensionalstatisticallearningproblemsforwhichTheorem6isapplicable.A.1.SparseLogisticRegressionForlogisticmodel,performingmaximumlikelihoodestimation(MLE)on(12)leadstothelogisticlossfunction‘pyji,xβ,xjiyq(cid:16)logp1(cid:0)expp(cid:1)yjixβ,xjiyqq.Forhigh-dimensionalproblems,whenweadda‘1regularization,weobtainthe‘1regularizedlogisticregressionmodel(Zhu&Hastie,2004,Wuetal.,2009):bβcentralize(cid:16)argminβ1mn‚jPrms‚iPrnslogp1(cid:0)expp(cid:1)yjixβ,xjiyqq(cid:0)λ||β||1.Thelogisticlossis14-smooth,andwealsoknowM(cid:16)14becauseofself-concordance(Zhang&Xiao,2015).LetLjpβq(cid:16)1n(cid:176)iPrnslogp1(cid:0)expp(cid:1)yjixβ,xjiyqq,(Negahbanetal.,2012)showedthatifxjiaredrawnfrommeanzerodistributionwithsub-Gaussiantails,thenL1pβqsatisﬁestherestrictedstrongcondition(5).Moreover,wehavethefollowingcontrolonthequantity(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1m(cid:176)jPrms∇Ljpβ(cid:6)q(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8.Lemma10.Thenwehavethefollowingupperboundholdsinprobabilityatleast1(cid:1)δ:(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1m‚jPrms∇Ljpβ(cid:6)q(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8(cid:192)||xji||8c2logpp{δqmn.Thefollowing‘1errorboundstatestheestimationerrorforlogisticregressionwith‘1regularization,whichwasestab-lished,forexample,in(vandeGeer,2008,Negahbanetal.,2012).Lemma11.Underthemodel(12),whenn¥p64{κqslogp,wehavethefollowingestimationerrorboundforbβ0holdswithprobabilityatleast1(cid:1)δ:||bβ0(cid:1)β(cid:6)||1(cid:192)sσXκc2logpnp{δqn.Withaboveanalysisforsparselogisticregressionmodelwithrandomdesign,wearereadytopresenttheresultsfortheestimationerrorboundwhichestablishedlocalexponentialconvergence.Corollary12.Undersparselogisticregressionmodelwithrandomdesign,andsetλt(cid:0)1as(9).IfthefollowingconditionholdsforsomeT¥0:||bβT(cid:1)β(cid:6)||1⁄4clogp2p{δqn.(13)Thenwithprobabilityatleast1(cid:1)2δ,wehavethefollowingestimationerrorboundforallt¥T:||bβt(cid:0)1(cid:1)β(cid:6)||1⁄1(cid:1)at(cid:1)T(cid:0)1n1(cid:1)an96sσσXκclogpp{δqmn(cid:0)4at(cid:1)T(cid:0)1nclogp2p{δqn,(14)||bβt(cid:0)1(cid:1)β(cid:6)||2⁄1(cid:1)at(cid:1)T(cid:0)1n1(cid:1)an4?sσσXκclogpp{δqmn(cid:0)4at(cid:1)Tnbnclogp2p{δqn,(15)wherean(cid:16)24sσσXκclogp2p{δqnandbn(cid:16)?sσσXκclogpnp{δqn.A.2.High-dimensionalGeneralizedLinearModelsTheresultsarereadilyextendabletootherhigh-dimensionalgeneralizedlinearmodels(McCullagh&Nelder,1989,vandeGeer,2008),wheretheresponsevariableyjiPYisdrawnfromthedistributionPpyji|xjiq9exp(cid:2)yjixxji,β(cid:6)y(cid:1)Φpxxji,β(cid:6)yqApσq(cid:10),EfﬁcientDistributedLearningwithSparsitywhereΦp(cid:4)qisalinkfunctionandApσqisascaleparameter.Undertherandomsubgaussiandesign,aslongasthelossfunctionhasLipschitzgradient,thenthealgorithmandcorrespondingestimationerrorboundandbeapplied.A.3.High-dimensionalGraphicalModelsTheresultscanalsobeusedforthedistributedunsupervisedlearningsettingwherethetaskistolearnasparsegraphicalstructurethatrepresentstheconditionalindependencebetweenvariables.WidelystudiedgraphicalmodelsareGaussiangraphicalmodels(Meinshausen&B¨uhlmann,2006,Yuan&Lin,2007)forcontinuousdataandIsinggraphicalmodels(Ravikumaretal.,2010)forbinaryobservations.Asshownin(Meinshausen&B¨uhlmann,2006,Ravikumaretal.,2010),thesemodelselectionproblemscanbereducedtosolvingparallel‘1regularizedlinearregressionandlogisticregressionproblems,respectively.Thustheapproachpresentedinthispapercanbereadilyapplicableforthesetasks.B.ProofsThesectioncontainsproofsofsometheoremsandlemmasstatedinthemainpaper.B.1.ProofofLemma8Proof.RecallthedeﬁnitionofeL1from(11).Wehave∇eL1pβ(cid:6),bβtq(cid:16)∇L1pβ(cid:6)q(cid:0)1m‚jPrms∇Ljpbβtq(cid:1)∇L1pbβtq(cid:16)1m‚jPrms∇Ljpβ(cid:6)q(cid:0)∇L1pβ(cid:6)q(cid:1)∇L1pbβtq(cid:1)(cid:4)(cid:5)1m‚jPrms∇Ljpβ(cid:6)q(cid:1)1m‚jPrms∇Ljpbβtq(cid:12)(cid:13).Usingthetriangleinequality(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)∇eL1pβ(cid:6),bβtq(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8⁄(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1m‚jPrms∇Ljpβ(cid:6)q(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8(cid:0)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)∇L1pβ(cid:6)q(cid:1)∇L1pbβtq(cid:1)(cid:4)(cid:5)1m‚jPrms∇Ljpβ(cid:6)q(cid:1)1m‚jPrms∇Ljpbβtq(cid:12)(cid:13)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8.Wefocusonboundingthesecondtermintheright-hand-sideinequalityabove.Letτji(cid:16)‘1pyji,xβ(cid:6),xjiyqanddeﬁnevjipbβtqPRp:vjipbβtq(cid:16)xjip‘1pyji,xβ(cid:6),xjiyq(cid:1)‘1pyji,xbβt,xjiyqq(cid:16)τjixjixTji(cid:1)bβt(cid:1)β(cid:6)(cid:9)(cid:0)xji‘3pyji,ujiq2pxbβt(cid:1)β(cid:6),xjiyq2whereujiisanumberbetweenxbβt,xjiyandxβ(cid:6),xjiy.Withthisnotation(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)∇L1pβ(cid:6)q(cid:1)∇L1pbβtq(cid:1)(cid:4)(cid:5)1m‚jPrms∇Ljpβ(cid:6)q(cid:1)1m‚jPrms∇Ljpbβtq(cid:12)(cid:13)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8⁄(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1n‚iPrnsv1ipbβtq(cid:1)1mn‚j‚ivjipbβtq(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8⁄(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1n‚iτ1ix1ixT1ipbβt(cid:1)β(cid:6)q(cid:1)1mn‚j‚iτjixjixTjipbβt(cid:1)β(cid:6)q(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8(cid:0)M(cid:4)(cid:2)maxj,i||xji||38(cid:10)(cid:4)||bβt(cid:1)β(cid:6)||21.EfﬁcientDistributedLearningwithSparsityTheﬁrsttermabovecanbefurtherupperboundedby(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1n‚jτ1ix1ixT1ipbβt(cid:1)β(cid:6)q(cid:1)1mn‚j‚iτjixjixTjipbβt(cid:1)β(cid:6)q(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8⁄(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1n‚jτ1ix1ixT1i(cid:1)1mn‚j‚iτjixjixTji(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8(cid:4)||bβt(cid:1)β(cid:6)||1.⁄(cid:4)(cid:5)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1n‚iPrnsτ1ix1ixT1i(cid:1)E(cid:16)τjixjixTji(cid:24)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8(cid:0)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1mn‚j‚iτjixjixTji(cid:1)E(cid:16)τjixjixTji(cid:24)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8(cid:12)(cid:13)(cid:4)||bβt(cid:1)β(cid:6)||1.UsingHoeffding’sinequalitytogetherwithaunionbound,wehavewithprobabilityatleast1(cid:1)δ,(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1n‚iPrnsτ1ix1ixT1i(cid:1)E(cid:16)τjixjixTji(cid:24)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8⁄L(cid:2)maxj,i||xji||28(cid:10)c2logp2p{δqn,and(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1mn‚j‚iτjixjixTji(cid:1)E(cid:16)τjixjixTji(cid:24)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8⁄L(cid:2)maxj,i||xji||28(cid:10)c2logp2p{δqmn.Combiningthebounds,theproofofthelemmaiscomplete.B.2.ProofofLemma9Proof.Theproofusesideaspresentedin(Negahbanetal.,2012).Bytriangleinequalitywehave||bβt(cid:0)1||1(cid:1)||β(cid:6)||1(cid:16)||β(cid:6)(cid:0)pbβt(cid:0)1(cid:1)β(cid:6)qSc(cid:0)pbβt(cid:0)1(cid:1)β(cid:6)qS||1(cid:1)||β(cid:6)||1¥||β(cid:6)(cid:0)pbβt(cid:0)1(cid:1)β(cid:6)qSc||1(cid:1)||pbβt(cid:0)1(cid:1)β(cid:6)qS||1(cid:1)||β(cid:6)||1(cid:16)||pbβt(cid:0)1(cid:1)β(cid:6)qSc||1(cid:1)||pbβt(cid:0)1(cid:1)β(cid:6)qS||1.Bytheoptimalityofbβt(cid:0)1for(4),wehaveeL1pbβt(cid:0)1,bβtq(cid:0)λt(cid:0)1||bβt(cid:0)1||1(cid:1)eL1pβ(cid:6),bβtq(cid:1)λt(cid:0)1||β(cid:6)||1⁄0.ThuseL1pbβt(cid:0)1,bβtq(cid:1)eL1pβ(cid:6),bβtq(cid:0)λt(cid:0)1p||pbβt(cid:0)1(cid:1)β(cid:6)qSc||1(cid:1)||pbβt(cid:0)1(cid:1)β(cid:6)qS||1q⁄0.BytheconvexityofeL1p(cid:4),bβtq,wefurtherhaveeL1pbβt(cid:0)1,bβtq(cid:1)eL1pβ(cid:6),bβtq¥x∇eL1pβ(cid:6),bβtq,bβt(cid:0)1(cid:1)β(cid:6)y.ThusbyH¨older’sinequality0¥x∇eL1pβ(cid:6),bβtq,bβt(cid:0)1(cid:1)β(cid:6)y(cid:0)λt(cid:0)1p||pbβt(cid:0)1(cid:1)β(cid:6)qSc||1(cid:1)||pbβt(cid:0)1(cid:1)β(cid:6)qS||1q¥(cid:1)||∇eL1pβ(cid:6),bβtq||8||bβt(cid:0)1(cid:1)β(cid:6)||1(cid:0)λt(cid:0)1p||pbβt(cid:0)1(cid:1)β(cid:6)qSc||1(cid:1)||pbβt(cid:0)1(cid:1)β(cid:6)qS||1q.Undertheassumptiononλt(cid:0)1wefurtherhave0¥(cid:1)λt(cid:0)12||bβt(cid:0)1(cid:1)β(cid:6)||1(cid:0)λt(cid:0)1p||pbβt(cid:0)1(cid:1)β(cid:6)qSc||1(cid:1)||pbβt(cid:0)1(cid:1)β(cid:6)qS||1q(cid:16)λt(cid:0)12||pbβt(cid:0)1(cid:1)β(cid:6)qSc||1(cid:1)3λt(cid:0)12||pbβt(cid:0)1(cid:1)β(cid:6)qS||1,whichcompletestheproof.EfﬁcientDistributedLearningwithSparsityB.3.ProofofTheorem6Proof.ForthetermeL1pbβt(cid:0)1,bβtq(cid:1)eL1pβ(cid:6),bβtqwehaveeL1pbβt(cid:0)1,bβtq(cid:1)eL1pβ(cid:6),bβtq(cid:16)L1pbβt(cid:0)1q(cid:0)C1m‚jPrms∇Ljpbβtq(cid:1)∇L1pbβtq,bβt(cid:0)1G(cid:1)L1pβ(cid:6)q(cid:1)C1m‚jPrms∇Ljpbβtq(cid:1)∇L1pbβtq,β(cid:6)G¥x∇L1pβ(cid:6)q,bβt(cid:0)1(cid:1)β(cid:6)y(cid:0)κ||bβt(cid:0)1(cid:1)β(cid:6)||22(cid:0)C1m‚jPrms∇Ljpbβtq(cid:1)∇L1pbβtq,bβt(cid:0)1G(cid:1)C1m‚jPrms∇Ljpbβtq(cid:1)∇L1pbβtq,β(cid:6)G(cid:16)C∇L1pβ(cid:6)q(cid:0)1m‚jPrms∇Ljpbβtq(cid:1)∇L1pbβtq,bβt(cid:0)1(cid:1)β(cid:6)G(cid:0)κ||bβt(cid:0)1(cid:1)β(cid:6)||22(cid:16)x∇eL1pβ(cid:6),bβtq,bβt(cid:0)1(cid:1)β(cid:6)y(cid:0)κ||bβt(cid:0)1(cid:1)β(cid:6)||22,wheretheﬁrstinequalityweusetherestrictedstrongconvexitycondition(5).Alsobytheoptimalityofbβt(cid:0)1for(4),wehaveeL1pbβt(cid:0)1,bβtq(cid:1)eL1pβ(cid:6),bβtq(cid:0)λt(cid:0)1||bβt(cid:0)1||1(cid:1)λt(cid:0)1||β(cid:6)||1⁄0.Combiningabovetwoinequalitiesweobtainwithprobabilityatleast1(cid:1)δ:λt(cid:0)1||β(cid:6)||1(cid:1)λt(cid:0)1||bβt(cid:0)1||1¥x∇eL1pβ(cid:6),bβtq,bβt(cid:0)1(cid:1)β(cid:6)y(cid:0)κ||bβt(cid:0)1(cid:1)β(cid:6)||22¥(cid:1)||∇eL1pβ(cid:6),bβtq||8||bβt(cid:0)1(cid:1)β(cid:6)||1(cid:0)κ||bβt(cid:0)1(cid:1)β(cid:6)||22¥(cid:1)λt(cid:0)12||bβt(cid:0)1(cid:1)β(cid:6)||1(cid:0)κ||bβt(cid:0)1(cid:1)β(cid:6)||22.Bytriangleinequalitythatλt(cid:0)1||bβt(cid:0)1(cid:1)β(cid:6)||1¥λt(cid:0)1||β(cid:6)||1(cid:1)λt(cid:0)1||bβt(cid:0)1||1,wehaveκ||bβt(cid:0)1(cid:1)β(cid:6)||22⁄3λt(cid:0)12||bβt(cid:0)1(cid:1)β(cid:6)||1(cid:16)3λt(cid:0)12p||pbβt(cid:0)1(cid:1)β(cid:6)qS||1(cid:0)||pbβt(cid:0)1(cid:1)β(cid:6)qSc||1q⁄3λt(cid:0)12p||pbβt(cid:0)1(cid:1)β(cid:6)qS||1(cid:0)3||pbβt(cid:0)1(cid:1)β(cid:6)qS||1q(cid:16)6λt(cid:0)1||pbβt(cid:0)1(cid:1)β(cid:6)qS||1⁄6?sλt(cid:0)1||pbβt(cid:0)1(cid:1)β(cid:6)qS||2⁄6?sλt(cid:0)1||bβt(cid:0)1(cid:1)β(cid:6)||2.Weget||bβt(cid:0)1(cid:1)β(cid:6)||2⁄6?sλt(cid:0)1κ.EfﬁcientDistributedLearningwithSparsitySubstituteλt(cid:0)1in(9)concludestheprooffor‘2estimationerrorbound.For||bβt(cid:0)1(cid:1)β(cid:6)||1,weknow||bβt(cid:0)1(cid:1)β(cid:6)||1⁄||pbβt(cid:0)1(cid:1)β(cid:6)qS||1(cid:0)||pbβt(cid:0)1(cid:1)β(cid:6)qSc||1⁄4||pbβt(cid:0)1(cid:1)β(cid:6)qS||1⁄4?s||pbβt(cid:0)1(cid:1)β(cid:6)qS||2⁄4?s||bβt(cid:0)1(cid:1)β(cid:6)||2⁄24sλt(cid:0)1κ,whichobtainsthedesiredbound.B.4.ProofofTheorem3Proof.Theorem3followsfromTheorem6afterweverifysomeconditions.First,itiseasytoseethatthequadraticlossL(cid:16)1,M(cid:16)0.UnderconditionsofTheorem,withprobability1(cid:1)δ,(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1m‚jPrms∇Ljpβ(cid:6)q(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8(cid:192)σσXclogpp{δqmn.ThisfollowsfromCorollary5.17ofVershynin(2012).Furthermore,withprobabilityatleast1(cid:1)δ,wehavemaxjPrms,iPrns||xji||8(cid:192)σXalogpmnp{δq.Finally,||bβ0(cid:1)β(cid:6)||1(cid:192)sσσXκclogpnp{δqn,withprobabilityatleast1(cid:1)δ(Wainwright,2009,Meinshausen&Yu,2009,Bickeletal.,2009).PluggingtheseboundsintoTheorem6completestheproof.B.5.ProofofCorollary7Proof.TheproofproceedsbyrecursivelyapplyingTheorem6andsumageometricsequence.Fornotationsimplicityleta(cid:16)48sκ(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1m‚jPrms∇Ljpβ(cid:6)q(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8,b(cid:16)(cid:3)48sLκ(cid:2)maxj,i||xji||28(cid:10)c4logp2p{δqn(cid:11),c(cid:16)48sMκ(cid:2)maxj,i||xji||38(cid:10).ByTheorem6wehave||bβt(cid:0)1(cid:1)β(cid:6)||1⁄a(cid:0)b||bβt(cid:1)β(cid:6)||1(cid:0)c||bβt(cid:1)β(cid:6)||21⁄a(cid:0)2b||bβt(cid:1)β(cid:6)||1⁄a(cid:0)2bpa(cid:0)2b||bβt(cid:1)1(cid:1)β(cid:6)||1q⁄...⁄at‚k(cid:16)0p2bqk(cid:0)p2bqt(cid:0)1||bβ0(cid:1)β(cid:6)||1.(cid:16)ap1(cid:1)p2bqt(cid:0)1q1(cid:1)2b(cid:0)p2bqt(cid:0)1||bβ0(cid:1)β(cid:6)||1,(16)whichcompletesthe‘1estimationerrorbound.For||bβt(cid:0)1(cid:1)β(cid:6)||2,weﬁrstuse(16)toobtain||bβt(cid:1)β(cid:6)||1⁄ap1(cid:1)p2bqtq1(cid:1)p2bq(cid:0)p2bqt||bβ0(cid:1)β(cid:6)||1.EfﬁcientDistributedLearningwithSparsityThenapplyTheorem6toobtainthat||bβt(cid:0)1(cid:1)β(cid:6)||2⁄a4?s(cid:0)p2bq4?s||bβt(cid:1)β(cid:6)||1⁄a4?s(cid:0)b4?s(cid:2)ap1(cid:1)p2bqtq1(cid:1)p2bq(cid:0)p2bqt||bβ0(cid:1)β(cid:6)||1(cid:10)(cid:16)14?s(cid:2)a(cid:0)app2bq(cid:1)p2bqt(cid:0)1q1(cid:1)p2bq(cid:10)(cid:0)p2bqt(cid:0)1||bβ0(cid:1)β(cid:6)||14?s(cid:16)ap1(cid:1)p2bqt(cid:0)1q4?sp1(cid:1)p2bqq(cid:0)p2bqt(cid:0)1||bβ0(cid:1)β(cid:6)||14?s,whichconcludestheproof.B.6.ProofofLemma10Proof.BythedeﬁnitionofLjpβq,wehave1m‚jPrms∇Ljpβ(cid:6)q(cid:16)1mn‚jPrms‚iPrnsxji(cid:2)yji(cid:1)yji1(cid:0)expp(cid:1)yjixβ,xjiyq(cid:10).ItiseasytocheckthatE(cid:18)yji(cid:1)yji1(cid:0)expp(cid:1)yjixβ,xjiyq(cid:26)(cid:16)0,and(cid:7)(cid:7)(cid:7)(cid:7)yji(cid:1)yji1(cid:0)expp(cid:1)(cid:1)yjixβ,xjiyq(cid:7)(cid:7)(cid:7)(cid:7)⁄1andthusE(cid:18)xji(cid:2)yji(cid:1)yji1(cid:0)expp(cid:1)yjixβ,xjiyq(cid:10)(cid:26)(cid:16)0,(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)xji(cid:2)yji(cid:1)yji1(cid:0)expp(cid:1)(cid:1)yjixβ,xjiyq(cid:10)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8⁄maxjip||xji||8q.ApplingAzuma-Hoeffdinginequality(Hoeffding,1963)andtheunionboundoverrpsleadstothedesiredbound.C.FullExperimentalResultsWerunthealgorithmsforbothdistributedregressionandclassiﬁcationproblems.Thealgorithmstobecomparedare:•Local:theﬁrstmachinejustsolvesarelated‘1regularizedproblem(lassoor‘1regularizedlogisticregression)withtheoptimalλ,andoutputsthesolution.Obviouslythisapproachiscommunicationfree.•Centralize:themastergathersalldatafromdifferentmachinestogether,andsolvesacentralized‘1regularizedlossminimizationproblemwiththeoptimalλ,andoutputsthesolution.Thisapproachiscommunicationexpensiveasalldataneedstobecommunicated,butitusuallygivesusthebestestimationandpredictionperformance.•ProxGD:thedistributedproximalgradientdescentisranonthe‘1regularizedobjective,whereweinitializedthestartingpointwiththeﬁrstmachine’ssolution.•Avg-Debias:themethodproposedinLeeetal.(2015b),withﬁnetunedregularizationandhardthresholdingparam-eters.Thisapproachonlyrequiresoneroundofcommunication,whereeachmachinesendsap-dimensionalvector.However,Avg-Debiasiscomputationallyprohibitivebecauseofthedebiasingoperation.•EDSL:theproposedefﬁcientdistributedsparselearningapproach,wheretheregularizationlevelateachiterationisﬁnetunedonaheldouttestdataset.C.1.SimulationsThefullexperimentalresultsplottedinFigure3andFigure4,withvarioussettingsofpn,p,m,sq,andconditionnumbers1{κ.Wehavethefollowingobservations:EfﬁcientDistributedLearningwithSparsityTable2.Listofreal-worlddatasetsusedintheexperiments.Name#Instances#FeaturesTaska9a48,842123Classiﬁcationconnect-467,557127Regressiondna2,000181Regressionmitface6,977362Classiﬁcationmnist1vs214,867785Classiﬁcationmnist60,000785Regressionmushrooms8,124113Classiﬁcationprotein17,766358Regressionspambase4,60157Classiﬁcationusps7,291257Regressionw8a64,700301Classiﬁcationyear51,63091Regression•TheAvg-DebiasapproachobtainedmuchbetterestimationerrorcomparedtoLocalafteroneroundofcommunicationandsometimesperformedquiteclosetoCentralize.However,inmostcases,thereisstillagapcomparedwithCentralize,especiallywhentheproblemisnotwell-conditionedorthenumberofmachinesmislarge.•Whentheproblemiswellconditioned(Σij(cid:16)0.5|i(cid:1)j|case),ProxGDconvergesreasonablyfast.However,itbecomesveryslowwhentheconditionnumberbecomesbad(Σij(cid:16)0.5|i(cid:1)j|{5case).Weexpecttoobserveasim-ilarphenomenonforotherﬁrst-orderdistributedoptimizationalgorithms,suchasacceleratedproximalgradientorADMM.•Astheorysuggests,EDSLobtainedasolutionthatiscompetitivewithAvg-Debiasafteroneroundofcommunication.TheestimationerrordecreasestomatchperformanceofCentralizewithinfewroundsofcommunications;typicallylessthan5,eventhoughthetheorysuggestsEDSLwillmatchtheperformanceofcentralizewithinOplogmqroundsofcommunication.C.2.Real-worldDataEvaluationInrealworlddataevaluationpresentedinSection5.2,thedatasetsarepubliclyavailablefromtheLIBSVMwebsite7andUCIMachineLearningRepository8.ThestatisticsofthesedatasetsaresummarizedinTable2,wheresomeofthemulti-classclassiﬁcationdatasetsareadoptedundertheregressionsettingwithsquaredlosses.TheresultsareplottedinFigure5whereforsomedatasetstheperformanceofAvg-Debiasissigniﬁcantlyworsethanothers(mostlybecausethedebiasingstepfails),thusweomittheseplots.TheplotsareshowninFigure5Wehavethefollowingobservations•Sincethereisnowell-speciﬁedmodelonthesedatasets,thecurvesbehavequitedifferentlyondifferentdatasets.However,alargegapbetweenthelocalandcentralizedprocedureisconsistentasthelateruses10timesmoredata.•Avg-Debiasoftenfailsontheserealdatasetsandperformsmuchworsethaninsimulations.Themainreasonmightbethattheassumptions,suchaswell-speciﬁedmodelorgeneralizedcoherencecondition,fail,thenAvg-Debiascantotallyfailandproducesolutionevenmuchworsethanthelocal.•ProxGDapproachstillconvergesslowlyinmostofthecases.•TheproposedEDSLisquiterobustonrealworlddatasets,andcanoutputasolutionwhichishighlycompetitivewiththecentralizedmodelwithinafewroundsofcommunications.•Thereexitsaslight“zig-zag”behaviorforEDSLapproachonsomedatasets.Forexample,onthemushroomsdataset,thepredictiveperformanceofEDSLisnotstable.7https://www.csie.ntu.edu.tw/cjlin/libsvmtools/datasets/8http://archive.ics.uci.edu/ml/EfﬁcientDistributedLearningwithSparsitym(cid:16)5m(cid:16)10m(cid:16)200123456789RoundsofCommunications0.150.220.290.360.430.5EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.120.240.360.480.6EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.070.210.350.490.630.77EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSLn(cid:16)200,p(cid:16)1000,s(cid:16)10,X(cid:18)Np0,Σq,Σij(cid:16)0.5|i(cid:1)j|.0123456789RoundsofCommunications0.230.310.390.470.550.63EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.20.310.420.530.640.75EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.10.280.460.640.821.0EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSLn(cid:16)200,p(cid:16)1000,s(cid:16)10,X(cid:18)Np0,Σq,Σij(cid:16)0.5|i(cid:1)j|{5.0123456789RoundsofCommunications0.10.160.220.280.340.4EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.070.140.210.280.350.42EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.030.120.210.30.390.48EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSLn(cid:16)500,p(cid:16)3000,s(cid:16)10,X(cid:18)Np0,Σq,Σij(cid:16)0.5|i(cid:1)j|.0123456789RoundsofCommunications0.150.230.310.390.470.55EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.10.190.280.370.460.55EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.070.170.270.370.470.57EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSLn(cid:16)500,p(cid:16)3000,s(cid:16)10,X(cid:18)Np0,Σq,Σij(cid:16)0.5|i(cid:1)j|{5.Figure3.Comparisonofvariousalgorithmsfordistributedsparseregression,1stand3rdrow:well-conditionedcases,2ndand4throw:ill-conditionedcases.EfﬁcientDistributedLearningwithSparsitym(cid:16)5m(cid:16)10m(cid:16)200123456789RoundsofCommunications0.50.660.820.981.141.3EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.40.620.841.061.281.5EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.30.580.861.141.421.7EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSLn(cid:16)500,p(cid:16)1000,s(cid:16)10,X(cid:18)Np0,Σq,Σij(cid:16)0.5|i(cid:1)j|.0123456789RoundsofCommunications0.550.740.931.121.311.5EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.450.70.951.21.451.7EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.350.660.971.281.591.9EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSLn(cid:16)500,p(cid:16)1000,s(cid:16)10,X(cid:18)Np0,Σq,Σij(cid:16)0.5|i(cid:1)j|{5.0123456789RoundsofCommunications0.450.580.710.840.971.1EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.350.520.690.861.031.2EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.250.480.710.941.171.4EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSLn(cid:16)1000,p(cid:16)3000,s(cid:16)10,X(cid:18)Np0,Σq,Σij(cid:16)0.5|i(cid:1)j|.0123456789RoundsofCommunications0.60.740.881.021.161.3EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.50.640.780.921.061.2EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.30.580.861.141.421.7EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSLn(cid:16)1000,p(cid:16)3000,s(cid:16)10,X(cid:18)Np0,Σq,Σij(cid:16)0.5|i(cid:1)j|{5.Figure4.Comparisonofvariousalgorithmsfordistributedsparseclassiﬁcation(logisticregression),1stand3rdrow:well-conditionedcases,2ndand4throw:ill-conditionedcases.EfﬁcientDistributedLearningwithSparsity0123456789RoundsofCommunications15.615.715.815.916.016.1ClassiﬁcationError(%)LocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications2.02.142.282.422.562.7ClassiﬁcationError(%)LocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications1.11.281.461.641.822.0ClassiﬁcationError(%)LocalProx-GDCentralizeAvg-DebiasEDSLa9aw8amnist1vs20123456789RoundsofCommunications0.030.0330.0360.0390.0420.045ClassiﬁcationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.0020.0060.010.0140.0180.022ClassiﬁcationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.10.1080.1160.1240.1320.14ClassiﬁcationErrorLocalProx-GDCentralizeAvg-DebiasEDSLmitfacemushroomsspambase0123456789RoundsofCommunications0.6770.67950.6820.68450.6870.68950.692NormalizedMSELocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.70.720.740.760.780.80.82NormalizedMSELocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.390.40.410.420.430.440.45NormalizedMSELocalProx-GDCentralizeEDSLconnect4proteinusps0123456789RoundsofCommunications0.40.440.480.520.560.6NormalizedMSELocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.40.410.420.430.440.450.460.470.48NormalizedMSELocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.9370.9380.9390.940.9410.942NormalizedMSELocalProx-GDCentralizeEDSLdnamnistyearFigure5.Comparisonofvariousapproachesfordistributedsparseregressionandclassiﬁcationonrealworlddatasets.(Avg-Debiasisomittedwhenitissigniﬁcantlyworsethanothers.)