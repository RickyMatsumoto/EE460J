Contextual Decision Processes with low Bellman rank are PAC-Learnable

Appendix

This appendix is organized as follows. We begin with describing the formal statements for the extensions of Theorem 1
along with necessary algorithmic modiﬁcations in Appendix A. We then provide formal statements and proofs for several
models with low Bellman rank introduced in Section 3 in Appendix B. In Appendix C, we provide the proofs of our main
results, including a more general version of Theorem 1 which also covers the aforementioned extensions. In Appendix D
we give the various technical lemmas required for our proofs and we present the main lower bound statements and proofs
in Appendix F.

We introduce four important extensions to the algorithm and analysis.

A. Extensions

A.1. Unknown Bellman Rank

The ﬁrst extension eliminates the need to know M in advance (note that Algorithm 1 requires M as an input parameter). A
simple procedure, as described in Algorithm 2, can guess the value of M by a doubling schedule and handle this situation
with no consequences to asymptotic sample complexity.8

Algorithm 2 GUESSM(F, ζ, (cid:15), δ)

1: for i = 1, 2, . . . do
M (cid:48) ← 2i.
2:
Call OLIVE(F, M (cid:48), (cid:15),
3:

5:
6:
7:
8: end for

end if

Return π.

4:

Terminate the subroutine when t > HM (cid:48) log
if a policy π is returned from OLIVE then

δ

i(i+1) ) with parameters speciﬁed on Theorem 1.
(cid:16) 6H

(cid:17)

√

M (cid:48)ζ
(cid:15)

/ log(5/3) in Line 4 (the for-loop).

Theorem 2. For any (cid:15), δ ∈ (0, 1), any Contextual Decision Process and function class F that admit a Bellman factoriza-
tion with parameters M, ζ, if we run GUESSM(F, (cid:15), δ), then with probability at least 1 − δ, GUESSM halts and returns a
policy which satisﬁes V ˆπ ≥ V (cid:63)

F − (cid:15), and the number of episodes required is at most

˜O

(cid:18) M 2H 3K
(cid:15)2

(cid:19)

log(N ζ/δ)

.

We give some intuition about the proof here, with details in Appendix E.1. In Algorithm 2, M (cid:48) is a guess for M which
grows exponentially. When M (cid:48) ≥ M , analysis of the main algorithm shows that OLIVE(F, M (cid:48), (cid:15),
i(i+1) ) terminates
and returns a near-optimal policy with high probability. The doubling schedule implies that the largest guess is at most
2M , which has negligible effect on the sample complexity. On the other hand, OLIVE may not explore effectively when
M (cid:48) < M , because not enough samples (chosen according to M (cid:48)) are used to estimate the average Bellman errors in
Line 13 of OLIVE. This worse accuracy does not guarantee sufﬁcient progress in learning.

δ

However, the high-probability guarantee that f (cid:63) is not eliminated is unaffected, because the threshold φ on Line 14 of
OLIVE is set in accordance with the sample size n speciﬁed in Theorem 1, regardless of M . Consequently, if the algorithm
ever terminates when M (cid:48) < M , we still get a near-optimal policy. When M (cid:48) < M the OLIVE subroutine may not
terminate, which the explicit termination on line 4 in Algorithm 2 addresses. Finally, by splitting the failure probability δ
appropriately among all guesses of M (cid:48), we obtain the same order of sample complexity as in Theorem 1.

8In Algorithm 2 we assume that ζ is ﬁxed. In the examples provided in Proposition 1, 2, and 3, however, ζ grows with M in the form
M (cid:48) and call OLIVE with ζ (cid:48) instead of ζ. As long as ζ is a polynomial term and

M . In this case, we can compute ζ (cid:48) = 2

√

√

of ζ = 2
non-decreasing in M the same analysis applies and Theorem 2 holds.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

A.2. Separation of Policy Class and V-value Class

So far, we have assumed that the agent has access to a class of Q-value functions F ⊂ X × A → [0, 1]. In this section, we
show the algorithm allows separate representations of policies and V-value functions.

For every f ∈ F, and any x ∈ X , a (cid:54)= πf (x), we note that the value of f (x, a) is not used by Algorithm 1, and changing
it to arbitrary values does not affect the execution of the algorithm as long as f (x, a) ≤ f (x, πf (x)) (so that πf does not
change). In other words, the algorithm only interacts with f in two forms:

1. f ’s greedy policy πf .

functions.9

2. A mapping gf : x (cid:55)→ f (x, πf (x)). We call such mappings V-value functions to contrast the previous use of Q-value

Hence, supplying F is equivalent to supplying the following space of (policy, V-value function) pairs:

{(cid:0)πf , gf

(cid:1) : f ∈ F}.

This observation provides further evidence that Deﬁnition 3 is signiﬁcantly less restrictive than standard realizability as-
sumptions. Validity of f means that (πf , gf ) obeys the Bellman Equations for Policy Evaluation (i.e., gf predicts the
long-term value of following πf ), as opposed to the more common Bellman Optimality Equations. In MDPs, there are
many ways to satisfy the policy evaluation equations at every state simultaneously, while Q(cid:63) is the only function that
satisﬁes all optimality equations.

More generally, instead of using a Q-value function class, we can run OLIVE with a policy space Π ⊂ X → A and a
V-value function class G ⊂ X → [0, 1] where we assemble (policy,V-value function) pairs by taking the Cartesian product
of Π and G. OLIVE can be run here with the understanding that each Q-value function f in OLIVE is associated with a
(π, g) pair, and the algorithm uses π instead of πf and g(x) instead of f (x, πf (x)). All the analysis applies directly with
this transformation, and the log |F| dependence in sample complexity is replaced by log |Π| + log |G|. Note also that the
deﬁnition of Bellman factorization also extends naturally to this case, where f in Equation 3 corresponds to a (π, g) pair
and f (cid:48) corresponds to a roll-in policy, π(cid:48).

A.3. Inﬁnite Hypothesis Classes

The arguments in Section 4 assume that |F| = N < ∞. However, almost all commonly used function approximators are
inﬁnite classes, which restricts the applicability of the algorithm. On the other hand, the size of the function class appears
in the analysis only through deviation bounds, so techniques from empirical process theory can be used to generalize the
results to inﬁnite classes. This section establishes parallel versions of those deviation bounds for function classes with
ﬁnite combinatorial dimensions, and together with the rest of the original analysis we can show the algorithm enjoys
similar guarantees when working with inﬁnite hypothesis classes.

Speciﬁcally, we consider the setting where Π and G are given (see Appendix A.2), and they are inﬁnite classes with
ﬁnite combinatorial dimensions. We assume that Π has ﬁnite Natarajan dimension (Deﬁnition 6), and G has ﬁnite pseudo
dimension (Deﬁnition 7). These two dimensions are standard extensions of VC-dimension to multi-class classiﬁcation and
regression respectively.

Deﬁnition 6 (Natarajan dimension (Natarajan, 1989)). Suppose X is a feature space and Y is a ﬁnite label space. Given
hypothesis class H ⊂ X → Y, its Natarajan dimension Ndim(H) is deﬁned as the maximum cardinality of a set A ⊆ X
that satisﬁes the following: there exists h1, h2 : A → Y such that (1) ∀x ∈ A, h1(x) (cid:54)= h2(x), and (2) ∀B ⊆ A, ∃h ∈ H
such that ∀x ∈ B, h(x) = h1(x) and ∀x ∈ A \ B, h(x) = h2(x).
Deﬁnition 7 (Pseudo dimension (Haussler, 1992)). Suppose X is a feature space. Given hypothesis class H ⊂ X → R,
its pseudo dimension Pdim(H) is deﬁned as Pdim(H) = VC-dim(H+), where H+ = {(x, ξ) (cid:55)→ 1[h(x) > ξ] : h ∈ H} ⊂
X × R → {0, 1}.

The deﬁnition of pseudo dimension relies on that of VC-dimension, whose deﬁnition and basic properties are recalled
in Appendix E.2. We state the ﬁnal sample complexity result here. Since the algorithm parameters are somewhat com-

9In the MDP setting, such functions are also known as state-value functions.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

plex expressions, we omit them in the theorem statement and provide speciﬁcation in the proof, which is deferred to
Appendix E.2.
Theorem 3. Let Π ⊂ X → A with Ndim(Π) ≤ dΠ < ∞ and G ⊂ X → [0, 1] with Pdim(G) ≤ dG < ∞. For
any (cid:15), δ ∈ (0, 1), any Contextual Decision Process with policy space Π and function space G that admits a Bellman
factorization with parameters M, ζ, if we run OLIVE with appropriate parameters, then with probability at least 1 − δ,
OLIVE halts and returns a policy ˆπ that satisﬁes V ˆπ ≥ V (cid:63)

F − (cid:15), and the number of episodes required is at most

˜O

(cid:18) M 2H 3K 2
(cid:15)2

(cid:16)

dΠ + dG + log(ζ/δ)

(cid:17)(cid:19)

.

(7)

Compared to Theorem 1, the sample complexity we get for inﬁnite hypothesis classes has two differences: (1) log N is
replaced by dΠ +dG, which is expected, based on the discussion in Appendix A.2, and (2) the dependence on K is quadratic
as opposed to linear. In fact, in the proof of Theorem 1, we exploited the low-variance property of importance weights in
Line 13 of OLIVE, and applied Bernstein’s inequality to avoid a factor of K. With inﬁnite hypothesis classes, the same
approach does not apply directly. However, this may only be a technical issue, and a more reﬁned analysis might recover
the linear dependence (e.g., using tools from Panchenko (2002)).

A.4. Approximate Validity and Approximate Bellman Rank

Recall that the sample-efﬁciency guarantee of OLIVE relies on two major assumptions:

• We assumed that F contains valid functions (Deﬁnition 3). In practice, however, it is hard to specify a function
class that contains strictly valid functions, as the notion of validity depends on the environment dynamics, which are
unknown. A much more realistic situation is that some functions in F satisfy validity only approximately.

• We assumed that the average Bellman errors have an exact low-rank factorization (Deﬁnition 5). While this is true
for a number of RL models (Section 3), it is worth keeping in mind that these are only models of the environments,
which are different from and only approximations to the real environments themselves. Therefore, it is more realistic
to assume that an approximate factorization exists when deﬁning Bellman factorization.

In this section, we show that the algorithmic ideas of OLIVE are indeed robust against both types of approximation errors,
and degrades gracefully as the two assumptions are violated. Below we introduce the approximate versions of Deﬁnition 3
and 5, give a slightly extended version of the algorithm, OLIVER (for Optimism-Led Iterative Value-function Elimination
with Robustness, see Algorithm 3), and state its sample complexity guarantee in Theorem 4.
Deﬁnition 8 (Approximate validity of f ). Given any CDP and function class F, we say f ∈ F is θ-valid if for any f (cid:48) ∈ F
and any h ∈ [H], |E(f, πf (cid:48), h)| ≤ θ.

The approximation error θ introduced in Deﬁnition 8 allows the algorithm to compete against a broader range of functions;
hence the notions of optimal function and value need to be re-deﬁned accordingly.
Deﬁnition 9. For a ﬁxed θ, deﬁne f (cid:63)

θ = argmaxf ∈F : f is θ-valid V πf , and V (cid:63)

F ,θ = V πf (cid:63)
θ .

By deﬁnition, V (cid:63)
F ,θ is non-decreasing in θ with Deﬁnition 3 being a special case where θ = 0. When θ > 0, we compete
against some functions that do not obey Bellman equations, breaking an essential element of value-based RL. As a con-
sequence, returning a policy with value close to V (cid:63)
F ,θ in a sample-efﬁcient manner is very challenging, so the value that
OLIVER can guarantee is suboptimal to V (cid:63)
Deﬁnition 10 (Approximate Bellman rank). We say that a CDP (X , A, H, P ) and F ⊂ X × A → [0, 1], admits a Bellman
factorization with Bellman rank M , norm parameter ζ, and approximation error η, if there exists νh : F → RM , ξh : F →
RM for each h ∈ [H], such that for any f, f (cid:48) ∈ F, h ∈ [H],

F ,θ by a term that is proportional to θ and does not diminish with more data.

|E(f, πf (cid:48), h) − (cid:104)νh(f (cid:48)), ξh(f )(cid:105)| ≤ η,

(8)

and (cid:107)νh(f (cid:48))(cid:107)2 · (cid:107)ξh(f )(cid:107)2 ≤ ζ < ∞.

A modiﬁed version of OLIVE that deals with these approximation errors, OLIVER, is speciﬁed in Algorithm 3. Here, we
use (cid:15) to denote the component of the suboptimality that diminishes as more data is collected, and the total suboptimality

Contextual Decision Processes with low Bellman rank are PAC-Learnable

√

M (θ + η) + η).

Algorithm 3 OLIVER (F, θ, M, ζ, η, (cid:15), δ)
1: Let (cid:15)(cid:48) = (cid:15) + 2H(3
2: Collect nest trajectories with actions taken in an arbitrary manner; save initial contexts {x(i)
3: Estimate the predicted value for each f ∈ F: ˆVf = 1
nest
4: F0 ← F.
5: for t = 1, 2, . . . do
6:

1 , πf (x(i)

i=1 f (x(i)

1 )).

(cid:80)nest

1 }nest
i=1.

Choose policy ft = argmaxf ∈Ft−1
Collect neval trajectories {(x(i)
1 , a(i)
Estimate ∀h ∈ [H],

ˆVf , πt = πft.
1, . . . , x(i)

1 , ri

H , a(i)

H , r(i)

H )}neval

i=1 by following πt (i.e. a(i)

h = πt(x(i)

h ) for all h, i).

˜E(ft, πt, h) =

(cid:104)
ft(x(i)

h , a(i)

h ) − r(i)

h − ft(x(i)

h+1, a(i)

(cid:105)
h+1)

.

(9)

1
neval

neval(cid:88)

i=1

if (cid:80)H

˜E(ft, πt, h) ≤ 5(cid:15)(cid:48)/8 then

h=1
Terminate and ouptut πt.

uniformly at random.

Estimate

end if
Pick any ht ∈ [H] for which ˜E(ft, πt, ht) ≥ 5(cid:15)(cid:48)/8H (One is guaranteed to exist).
h = πt(x(i)
Collect trajectories {(x(i)

i=1 where a(i)

1 , . . . , x(i)

H , a(i)

H , r(i)

1 , a(i)

1 , r(i)

H )}n

h ) for all h (cid:54)= ht and a(i)
ht

is drawn

7:
8:

9:
10:
11:
12:

13:

14:

(cid:98)E(f, πt, ht) =

n
(cid:88)

1[a(i)
ht

1
n

i=1

= πf (x(i)
ht
1/K

)]

(cid:16)

f (x(i)
ht

, a(i)
ht

) − r(i)
ht

− f (x(i)

ht+1, πf (x(i)

(cid:17)
ht+1))

.

15:

Learn

16: end for

(cid:110)

Ft =

f ∈ Ft−1 :

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤ φ + θ
(cid:12) (cid:98)E(f, πt, ht)

(cid:111)

.

that we can guarantee is (cid:15) plus a term proportional to θ and η (see Eq. (12) in Theorem 4). The algorithm is almost identical
to OLIVE except in two places: (1) it uses (cid:15)(cid:48) (deﬁned on Line 1) in the termination condition (Line 9) as opposed to (cid:15), and
(2) it uses a higher threshold that depends on θ in Eq. (11) to avoid eliminating θ-valid functions. The approximation and
sample complexity guarantees of OLIVER are stated in Theorem 4, with the proof deferred to Appendix D.

Theorem 4. For any (cid:15), δ ∈ (0, 1), any Contextual Decision Process and function class F that admit a Bellman factor-
ization with parameters M , ζ, and η, suppose we run OLIVER with any θ ∈ [0, 1], and nest, neval, n, φ as speciﬁed in
Theorem 1. Then with probability at least 1 − δ, OLIVER halts and returns a policy ˆπ which is at most

suboptimal compared to V (cid:63)

F ,θ deﬁned in Deﬁnition 9, and the number of episodes required is at most

√

(cid:15) + 8H

M (θ + η)

˜O

(cid:18) M 2H 3K
(cid:15)2

(cid:19)

log(N ζ/δ)

.

(10)

(11)

(12)

(13)

B. Models with Low Bellman Rank

B.1. Proof of Proposition 1

Let M = |S| and each element of νh(·) and ξh(·) be indexed by s ∈ S. We explicitly construct νh and ξh as follows: let
[νh(f (cid:48))]s = Pr [xh = (s, h) | a1:h−1 ∼ πf (cid:48)], and [ξh(f )]s = E (cid:2)f (xh, ah) − rh − f (xh+1, ah+1) (cid:12)
(cid:12) xh = (s, h), ah:h+1 ∼
(cid:3). In other words, νh(f (cid:48)) is the distribution over states induced by πf (cid:48) at time step h, and the s-th element of ξh is
πf
the traditional notion of Bellman error for state s. It is easy to verify that Eq. (3) holds. For the norm constraint, since

Contextual Decision Processes with low Bellman rank are PAC-Learnable
√

√

(cid:107)νh(·)(cid:107)1 = 1 and (cid:107)ξh(·)(cid:107)∞ ≤ 2, we have (cid:107)νh(·)(cid:107)2 ≤ 1 and (cid:107)ξh(·)(cid:107)2 ≤ 2
on the product of vector norms.

M , hence ζ = 2

M is a valid upper bound

B.2. Generalization of Li (2009)’s Setting

Li (2009, Section 8.2.3) considers the setting where the learner is given an abstraction φ that maps the large state space
S in an MDP to some ﬁnite abstract state space ¯S. | ¯S| is potentially much smaller than |S|, and it is guaranteed that Q(cid:63)
can be expressed as a function of (φ(s), a). Li shows that when delayed Q-learning is applied to this setting, the sample
complexity has polynomial dependence on | ¯S| with no direct dependence on |S|.

In the next proposition, we show that a similar setting for ﬁnite-horizon problems admits Bellman factorization with low
Bellman rank. In particular, we subsume Li’s setting by viewing it as a POMDP, where φ is a deterministic emission
process that maps hidden state s ∈ S to discrete observations φ(s) ∈ ¯S = O, and the candidate value functions are
reactive so they depend on φ(s) but not directly on s or any previous state. More generally, Proposition 6 claims that for
POMDPs with large hidden-state spaces and ﬁnite observation spaces, the Bellman rank is polynomial in the number of
observations if the function class is reactive.

Proposition 6 (A generalization of (Li, 2009)’s setting). Consider a POMDP introduced in Example 2 with |O| < ∞,
and assume that rewards can only take CR different discrete values.10 The CDP (X , A, H, P ) induced by letting X =
O × [H] and xh = (oh, h), with any F : X × A → [0, 1], admits a Bellman factorization with M = |O|2CRK and
ζ = 2|O|K

√

CR.

Proof. For any f, f (cid:48) ∈ F, h ∈ [H], let νh(f (cid:48)) and ξh(f ) be vectors of length |O|2CRK. Let the entry of νh(f (cid:48)) indexed
by (oh, ah, rh, oh+1) be

P [oh, rh, oh+1 | a1:h−1 ∼ πf (cid:48), ah],

interpreted as the following: conditioned on the fact that the ﬁrst h − 1 actions are chosen according to πf (cid:48), what is
the probability of seeing a particular tuple of (oh, rh, oh+1) when taking a particular action for ah? For ξh(f ), let the
corresponding entry be (with xh = (oh, h) and xh+1 = (oh+1, h + 1) as the corresponding contexts in the CDP)

1[ah = πf (xh)](cid:0)f (xh, ah) − rh − f (xh+1, πf (xh+1))(cid:1).

It is not hard to verify that E(f, πf (cid:48), h) = (cid:104)νh(f (cid:48)), ξh(f )(cid:105). Since ﬁxing ah to any non-adaptive choice of action induces a
valid distribution over (oh, rh, oh+1), we have (cid:107)νh(f (cid:48))(cid:107)1 = K and (cid:107)νh(f (cid:48))(cid:107)2 ≤ K. On the other hand, (cid:107)ξh(f )(cid:107)∞ ≤ 2
but the vector only has |O|2CR non-zero entries, so (cid:107)ξh(f )(cid:107)2 ≤ 2|O|

CR. Together the norm bound follows.

√

B.3. POMDP-like Models

Here we ﬁrst state the formal version of Proposition 2, and prove Propositions 2 and 3 together by studying a slightly more
general model (See Figure 1).

Proposition 7 (Formal version of Proposition 2). Consider an MDP introduced in Example 1. With a slight abuse of
notation let Γ denote its transition matrix of size |S × A| × |S|, whose element indexed by ((s, a), s(cid:48)) is Γ(s(cid:48)|s, a).
Assume that there are two row-stochastic matrices Γ(1) and Γ(2) with sizes |S × A| × M and M × |S| respectively,
such that Γ = Γ(1)Γ(2). Recall that we convert an MDP into a CDP by letting X = S × [H], xh = (sh, h). For any
F ⊂ X × A → [0, 1], this model admits a Bellman factorization with Bellman rank M and ζ = 2

M .

√

The model in Figure 1 that we use to prove Propositions 2 and 3 simultaneously behaves like a POMDP except that
both the transition function and the reward depends also on the observation, that is Γ : S × O × A → ∆(S) and
R : S × O × A → ∆([0, 1]). Clearly this model generalizes standard POMDPs, where the transition and reward are
both assumed to be independent of the current observation.

This model also generalizes the MDP with low-rank dynamics described in Proposition 2: if the future hidden-state is
independent of the current hidden-state conditioned on the observation (i.e., Γ(s(cid:48)|s, o, a) does not depend on s), the obser-
vations themselves become Markovian, and we can treat o as the observed state s in Proposition 7, and the hidden-state s

10The discrete reward assumption is made to simplify presentation and can be relaxed. For arbitrary rewards, we can always discretize

the reward distribution onto a grid of resolution CR, which incurs η = O(1/CR) approximation error in Deﬁnition 10.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

as the low-rank factor in Proposition 7 (see Figure 1). Hence, Proposition 2 follows as a special case of the analysis for
this more general model.

As in Proposition 3, we consider a class F reactive value functions. Observe that for the MDP with low rank dynamics,
this provides essentially no loss of generality, since the optimal value function is reactive.

Proposition 8. Let (X , A, H, P ) be the CDP induced by the model in Figure ?? which generalizes POMDPs, with X =
O × [H] and xh = (oh, h). Given any F : X × A → [0, 1], the Bellman rank M ≤ |S| with ζ = 2(cid:112)|S|.

Proof. For any f, f (cid:48) ∈ F, h ∈ [H], consider

which is how actions are chosen in the deﬁnition of E(f, πf (cid:48), h) (see Deﬁnition 2). Such a decision-making strategy
induces a distribution over the following set of variables

a1:h−1 ∼ πf (cid:48), ah:h+1 ∼ πf ,

(sh, oh, ah, rh, oh+1, ah+1).

We use µf,f (cid:48) to denote this distribution, and the subscript emphasizes its dependence on both f and f (cid:48). Note that the
marginal distribution of sh only depends on f (cid:48) and has no dependence on f , which we denote as µf (cid:48). Then, sampling from
µf,f (cid:48) is equivalent to the following sampling procedure: (recall that xh = (oh, h))

sh ∼ µf (cid:48), oh ∼ Dsh , ah = πf (xh), rh ∼ R(sh, oh, ah),
sh+1 ∼ Γ(sh, oh, ah), oh+1 ∼ Dsh+1, ah+1 = πf (xh+1).

That is, we ﬁrst sample sh from the marginal µf (cid:48), and then sample the remaining variables conditioned on sh. Notice
that once we condition on sh, the sampling of the remaining variable has no dependence on f (cid:48), so we denote the joint
distribution over the remaining variables (conditioned on the value of sh) µf |sh.

Finally, we express the factorization of E(f, πf (cid:48), h) as follows:

E(f, πf (cid:48), h) = Eµf,f (cid:48) [f (xh, ah) − rh − f (xh+1, ah+1)]
Eµf |sh

[f (xh, ah) − rh − f (xh+1, ah+1)]

µf (cid:48)(s) · Eµf |s[f (xh, ah) − rh − f (xh+1, ah+1)].

= Esh∼µf (cid:48)
(cid:88)
=

s∈S

We deﬁne νh(·) and ξh(·) explicitly with dimension M = |S|: given f and f (cid:48), we index the elements of νh(f (cid:48)) and those
of ξh(f ) by s ∈ S, and let [νh(f (cid:48))]s = µf (cid:48)(s), [ξh(f )]s = Eµf |s[f (xh, ah) − rh − f (xh+1, ah+1)]. ζ = 2
M follows
from the fact that (cid:107)νh(f (cid:48))(cid:107)1 = 1 and (cid:107)ξh(f )(cid:107)∞ ≤ 2.

√

B.4. Predictive State Representations

In this subsection we state and prove the formal version of Proposition 4. We ﬁrst recall the deﬁnitions and some basic
properties of PSRs, which can be found in Singh et al. (2004); Boots et al. (2011). Consider dynamical systems with
discrete and ﬁnite observation space O and action space A. Such systems can be fully speciﬁed by moment matrices
PT |H, where H is a set of histories (past events) and T is a set of tests (future events). Elements of T and H are sequences
of alternating actions and observations, and the entry of PT |H indexed by t ∈ T on the row and τ ∈ H on the column is
Pt|τ , the probability that the test t succeeds conditioned on a particular past τ . For example, if t = aoa(cid:48)o(cid:48), success of t
means seeing o and o(cid:48) in the next two steps after τ is observed, if interventions a and a(cid:48) were to be taken.

Among all such systems, we are concerned about those that have ﬁnite linear dimension, deﬁned as supT ,H rank(PT |H).
As an example, the linear dimension of a POMDP is bounded by the number of hidden-states. Systems with ﬁnite linear
dimension have many nice properties, which allow them to be expressed by compact models, namely PSRs. In particular,
ﬁxing any T and H such that rank(PT |H) is equal to the linear dimension (such (H, T ) are called core histories and core
tests), we have:

1. For any history τ ∈ (A × O)∗, the conditional predictions of core tests PT |{τ } (we also write PT |τ ) is always a state,

that is, a sufﬁcient statistics of history. This gives rise to the name “predictive state representation”.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

2. Based on PT |τ , the conditional prediction of any test t can be computed from a PSR model, parameterized by square
matrices {Bao} and a vector b∞ with dimension |T |. Letting t(i) be the i-th (action, observation) pair in t, and |t| be
the number of such pairs, the prediction rule is

And these parameters can be computed as

Pt|τ = b(cid:62)

∞Bt(|t|) · · · Bt(1)PT |τ .

Bao = PT ,ao,HP †

T ,H ,

∞ = P (cid:62)
b(cid:62)

H P †

T ,H

(14)

(15)

• PT ,H is a matrix whose element indexed by (t ∈ T , τ ∈ H) is Pτ t|∅, where τ t is the concatenation of τ and t

where

and ∅ is the null history.

• PH = P{∅},H.
• PT ,ao,H = PT ,Hao , where Hao = {τ ao : τ ∈ H}.

Now we are ready to state and prove the formal version of Proposition 4.

Proposition 9 (Formal version of Proposition 4). Consider a partially observable system with observation space O, and
the induced CDP (X , A, H, P ) with xh = (oh, h). To handle some subtleties, we assume that

1. |O| < ∞ (classical PSR results assume discrete observations).

2. o1 is deterministic (PSR trajectories always start with an action), and rh is a deterministic function of oh+1 (reward

is usually omitted or assumed to be part of the observation).

If the linear dimension of the original system is at most L, then with any F : X × A → [0, 1], this model admits a Bellman
factorization with M = LK. Assuming further that the PSR’s parameters are non-negative under some choice of core
histories and tests (H, T ) of size |H| = |T | = L, then we have ζ ≤ 2K 2L3
min, where σmin is the minimal non-zero
singular value of PT ,H.

L/σ3

√

Proof. For any f, f (cid:48) ∈ F, h ∈ [H], deﬁne

1. µf (cid:48),h as the distribution vector over (a1, o2, . . . , oh−1, ah−1) ∈ (A × O)h−2 × A induced by a1:h−1 ∼ πf (cid:48). (Recall

that o1 is deterministic.)

2. P2|h−1 as a moment matrix whose element with column index (oh, ah, oh+1) ∈ O × A × O and

row index (a1, o2, . . . , oh−1, ah−1) ∈ (A × O)h−2 × A is

P [oh, oh+1 (cid:107) ah−1, ah | a1, o2, . . . , oh−1].11

3. Ff,h as a vector whose element indexed by (oh, ah, oh+1) ∈ O × A × O is (recall that xh = (oh, h) and rh is function

1[ah (cid:54)= πf (xh)] (cid:0)f (xh, ah) − rh − f (xh+1, πf (xh+1))(cid:1).

of oh+1)

First we verify that

To show this, ﬁrst observe that µ(cid:62)

f (cid:48),hP2|h−1 is a row vector whose element indexed by (oh, ah, oh+1) is

E(f, πf (cid:48), h) = µ(cid:62)

f (cid:48),hP2|h−1Ff,h.

P [oh, oh+1 (cid:107) ah | a1:h−1 ∼ πf (cid:48)].

Multiplied by Ff,h, we further get

E[f (xh, ah) − rh − f (xh+1, πf (xh+1)) | a1:h−1 ∼ πf (cid:48), ah ∼ πf ] = E(f, πf (cid:48), h).

11PSR literature often emphasizes the intervention aspect of the actions in tests via the uses “(cid:107)” symbol; mathematically they can be

treated as the conditioning operator in most cases.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Next, we explicitly construct ξh(f ) and νh(f (cid:48)) by factorizing P2|h−1 = P1×P2, where both P1 and P2 have no dependence
on either f or f (cid:48). Recall that for PSRs, any history (a1, o2, . . . , oh−1) has sufﬁcient statistics PT |a1,o2,...,oh−1, that is a
vector of predictions over the selected core tests T conditioned on the observed history. P1 consists of row vectors of
length LK, and for the row indexed by (a1, o2, . . . , oh−1, ah−1) the vector is

where Pada(·) is a function that takes a L-dimensional vector, puts it in the a-th block of a vector of length LK, and ﬁlls
the remaining entries with 0.

We construct P2 to be a matrix whose column vector indexed by (oh, ah, oh+1) is

Padah−1

(cid:0)P (cid:62)

T |a1,o2,...,oh−1

(cid:1),

a(1),oh

ah,oh+1



B(cid:62)



B(cid:62)

B(cid:62)
. . .
B(cid:62)

a(K),oh

ah,oh+1



b∞

 ,

b∞

where A = {a(1), . . . , a(K)}. It is easy to verify that P2|h−1 = P1 × P2 by recalling the prediction rules of PSRs in
Eq. (14):

P [oh, oh+1 (cid:107) ah−1, ah | a1, o2, . . . , oh−1] = b(cid:62)
= P (cid:62)

∞Bah,oh+1Bah−1,ohPT |a1,o2,...,oh−1
(B(cid:62)

B(cid:62)

T |a1,o2,...,oh−1

ah−1,oh

ah,oh+1

b∞).

Given this factorization, we can write

E(f, πf (cid:48), h) = (µ(cid:62)

f (cid:48),hP1) × (P2Ff,h).

So we let νh(f (cid:48)) = P (cid:62)
1 µf (cid:48),h and ξh(f ) = P2Ff,h. It remains to be shown that we can bound their norms. Notice that the
entries of a state vector PT |(·) are predictions of probabilities, so (cid:107)P1(cid:107)∞ ≤ 1. Since µf (cid:48),h is a probability vector, its dot
√
product with every column in P1 is bounded by 1, hence (cid:107)νh(f (cid:48))(cid:107)2 ≤

LK.

At last, we consider bounding the norm of P2Ff,h. We upper bound each entry of P2Ff,h by providing an (cid:96)1 bound on
the row vectors of P2, and then applying the H¨older’s inequality with (cid:107)Ff,h(cid:107)∞ ≤ 2. Since we assumed that all model
parameters of the PSRs are non-negative, P2 is a non-negative matrix, and bounding the (cid:96)1 norm of its row vectors is
equivalent to bounding each entry of the vector P2 1, where 1 is an all-1 vector. This vector is equal to

P2 1 =





(cid:80)

(oh,ah,oh+1) B(cid:62)
a(1),oh
. . .
(oh,ah,oh+1) B(cid:62)

a(K),oh

(cid:80)

B(cid:62)

ah,oh+1



b∞

B(cid:62)

ah,oh+1

b∞



(cid:16)(cid:80)




(cid:16)(cid:80)

 =

B(cid:62)

oh

a(1),oh

(cid:17) (cid:16)(cid:80)

B(cid:62)

oh

a(K),oh

(cid:17) (cid:16)(cid:80)

ah,oh+1

(ah,oh+1) B(cid:62)
. . .
(ah,oh+1) B(cid:62)

ah,oh+1







.

(cid:17)

b∞

(cid:17)

b∞

(16)

Since we care about the (cid:96)∞ norm of this vector, we can bound the (cid:96)∞ norm of each component vector. Using the PSR
learning equations, we have

Bao =

PT ,ao,HP †

T ,H =

PT ,ao,H

P †

T ,H.

(cid:88)

a,o

(cid:88)

a,o

(cid:32)

(cid:88)

a,o

(cid:33)

Note that for any ﬁxed a = a(i), every entry of (cid:80)
o PT ,ao,H is the probability that the event t ∈ T happens after h ∈ H
happens with a one step delay in the middle, where a is intervened in that delayed time step. Such entries are predicted
probabilities of events, hence lie in [0, 1]. Consequently, (cid:107) (cid:80)
a,o PT ,ao,H(cid:107)∞ ≤ K, and we can upper bound the matrix (cid:96)2
norm by Frobenius norm: (cid:107) (cid:80)

a,o PT ,ao,H(cid:107)F ≤ KL. Hence,

a,o PT ,ao,H(cid:107)2 ≤ (cid:107) (cid:80)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

a,o

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

a,o

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:13)
(cid:13)P †
(cid:13)

·

T ,H

(cid:13)
(cid:13)
(cid:13)2

Bao

≤

PT ,ao,H

≤ KL/σmin.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Using a similar argument, for any ﬁxed a = a(i), (cid:107)(cid:80)
norm similarly:

o Bao(cid:107)2 ≤ L/σmin. We also recall the deﬁnition of b∞ and bound its

Finally, we have

(cid:107)b∞(cid:107)2 =

(cid:13)
(cid:13)P (cid:62)
(cid:13)

H P †

T ,H

(cid:13)
(cid:13)
(cid:13)2

√

≤

L/σmin.

(cid:107)P2 1(cid:107)∞ ≤ max
a∈A

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:32)

(cid:88)

oh

(cid:32)

(cid:88)

oh

(cid:88)

o

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:33) 


(cid:33) 


B(cid:62)

a,oh

B(cid:62)

a,oh

(ah,oh+1)

(cid:88)

(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:33) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(ah,oh+1)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

Bao

a,o

Bao





B(cid:62)

ah,oh+1

 b∞

B(cid:62)

ah,oh+1

 b∞

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

√

≤ max
a∈A

(cid:32)

≤

max
a∈A

(cid:107)b∞(cid:107)2 ≤ KL2

L/σ3

min.

(Eq. (16))

So each row of P2 has (cid:96)1 norm bounded by the above expression. Applying H¨older’s inequality we have each entry of
P2Ff,h bounded by 2KL2
L/σ3
min. Combined with the bound on
(cid:107)νh(f (cid:48))(cid:107)2 the proposition follows.

min, hence (cid:107)ξh(f )(cid:107)2 = (cid:107)P2Ff,h(cid:107)2 ≤ 2L3K

K/σ3

√

√

B.5. Linear Quadratic Regulators

In this subsection we prove that Linear Quadratic Regulators (LQR) (See e.g., Anderson & Moore (2007) for a standard
reference) admit Bellman factorization with low Bellman rank. We study a ﬁnite-horizon, discrete-time LQR, governed by
the equations:

x1 = (cid:15)0,

xh+1 = Axh + Bah + (cid:15)h,

and

ch = x(cid:62)

h Qxh + a(cid:62)

h ah + τh,

where xh ∈ Rd, ah ∈ RK and the noise variables are centered with E[(cid:15)h(cid:15)(cid:62)
h = σ2. We operate with costs
ch, and the goal is to minimize the cumulative cost. We assume that all parameters A, B, Σ, Q, σ2 are bounded in spectral
norm by some Θ ≥ 1, that λmin(B(cid:62)B) ≥ κ > 0, and that Q is strictly positive deﬁnite. Other formulations of LQR
replace a(cid:62)
h Rah for a positive deﬁnite matrix R, which can be accounted for by a change of variables.
Generalization to non-stationary parameters is straightforward.

h ah in the cost with a(cid:62)

h ] = Σ, and Eτ 2

This model describes an MDP with continuous state and action spaces, and the corresponding CDP has context space
Rd × [H], although we always explicitly write both parts of the context in this section. It is well known that in a discrete
time LQR, the optimal policy is a non-stationary linear policy π(cid:63)(x, h) = P(cid:63),hx (Anderson & Moore, 2007), where
P(cid:63),h ∈ RK×d is an h-dependent control matrix. Moreover, if all of the parameters are known to have spectral norm
bounded by Θ then the optimal policy has matrices with bounded spectral norm as well, as we will see in the proof.

The arguments for LQR use decoupled policy and value function classes as in Appendix A.2. We use a policy class and
value function class deﬁned below for parameters B1, B2, B3 that we set in the proof.

Π = {π (cid:126)P : π (cid:126)P (x, h) = Phx, (cid:126)P ∈

RK×d, (cid:107)Ph(cid:107)2 ≤ B1}

H
(cid:89)

i=1

G = {f(cid:126)Λ, (cid:126)O : f(cid:126)Λ, (cid:126)O(x, h) = x(cid:62)Λhx + Oh, (cid:126)Λ ∈

Rd×d, (cid:107)Λh(cid:107)2 ≤ B2, (cid:126)O ∈ RH , |Oh| ≤ B3}

H
(cid:89)

i=1

The policy class consists of linear non-stationary policies, while the value functions are nonstationary quadratics with
constant offset.

Proposition 10 (Formal version of Proposition 5). Consider an LQR under the assumptions above.

Let G be a class of non-stationary quadratic value functions with offsets and let Π be a class of linear non-stationary
policies, deﬁned above. Then, at level h, for any (π, g) pair and any roll-in policy π(cid:48) ∈ Π, the average Bellman error can

Contextual Decision Processes with low Bellman rank are PAC-Learnable

be written as

E(g, π, π(cid:48), h) = (cid:104)ξh(π, g), νh(π(cid:48))(cid:105),

where ν, ξ ∈ Rd2+1. If Π, G are deﬁned as above with bounds B1, B2, B3 and if all problem parameters have spectral
norm at most Θ, then

(cid:107)ξh(π, g)(cid:107)2
(cid:107)νh(π(cid:48))(cid:107)2

2 ≤ d (cid:0)B2 + Θ + B2
2 ≤ dH+1Θ(ΘB1)2H + 1.

1 − (Θ + ΘB1)2B2

(cid:1) + 4B2

3 + d2Θ2B2
2

Hence, the problem admits Bellman factorization with Bellman rank at most d2 + 1 and ζ that is exponential in H but
polynomial in all other parameters. Moreover, if we set B1, B2, B3 as,

B1 = Θ2/κ, B2 =

(cid:19)H

(cid:18) 6Θ6
κ2

Θ, B3 =

(cid:19)H

(cid:18) 6Θ6
κ2

dHΘ2,

then the optimal policy and value function belong to Π, G respectively.

We prove the proposition in several components. First, we study the relationship between policies and value functions,
showing that linear policies induce quadratic value functions. Then, we turn to the structure of the optimal policy, showing
that it is linear. Next, we derive bounds on the parameters B1, B2, B3, which ensure that the optimal policy and value
function belong to Π, G. Lastly, we demonstrate the Bellman factorization.

The next lemma derives a relationship between linear policies and quadratic value functions.
Lemma 2. If π is a linear non-stationary policy, πh(x) = Pπ,hx, then V π(x, h) = x(cid:62)Λπ,hx + Oπ,h where Λπ,h ∈ Rd×d
depends only on π and h and Oπ,h ∈ R. These parameters are deﬁned inductively by,

Λπ,H = Q + P (cid:62)
Λπ,h = Q + P (cid:62)
Oπ,h = tr(Λπ,h+1Σ) + Oπ,h+1,

π,H Pπ,H ,
π,hPπ,h + (A + BPπ,h)(cid:62)Λπ,h+1(A + BPπ,h)

Oπ,H = 0

where we recall that Σ is the covariance matrix of the (cid:15)h random variables.

Proof. The proof is by backward induction on h, starting from level H. Clearly,

V π(x, H) = x(cid:62)Qx + πH (x)(cid:62)πH (x) = x(cid:62)Qx + x(cid:62)P (cid:62)

π,H Pπ,H x (cid:44) x(cid:62)Λπ,H x,

so V π(·, H) is a quadratic function.

For the inductive step, consider level h and assume that for all x, V π(x, h + 1) = x(cid:62)Λπ,h+1x + Oπ,h+1. Then, expanding
deﬁnitions

V π(x, h) = x(cid:62)Qx + πh(x)(cid:62)πh(x) + Ex(cid:48)∼(x,πh(x))V π(x(cid:48), h + 1)

= x(cid:62)Qx + x(cid:62)P (cid:62)
= x(cid:62)Qx + x(cid:62)P (cid:62)
= x(cid:62)Qx + x(cid:62)P (cid:62)
= x(cid:62)Qx + x(cid:62)P (cid:62)
= x(cid:62)Qx + x(cid:62)P (cid:62)

(cid:2)(x(cid:48))(cid:62)Λπ,h+1(x(cid:48)) + Oπ,h+1

π,hPπ,hx + Ex(cid:48)∼(x,πh(x))
π,hPπ,hx + E(cid:15)h
π,hPπ,hx + E(cid:15)h
π,hPπ,hx + x(cid:62)(A + BPπ,h)(cid:62)Λπ,h+1(A + BPπ,h)x + E(cid:15)h (cid:15)(cid:62)
π,hPπ,hx + x(cid:62)(A + BPπ,h)(cid:62)Λπ,h+1(A + BPπ,h)x + tr(Λπ,h+1Σ) + Oπ,h+1.

(cid:2)(Ax + Bπh(x) + (cid:15)h)(cid:62)Λπ,h+1(Ax + Bπh(x) + (cid:15)h) + Oπ,h+1
(cid:2)(Ax + BPπ,hx + (cid:15)h)(cid:62)Λπ,h+1(Ax + BPπ,hx + (cid:15)h) + Oπ,h+1

h Λπ,h+1(cid:15)h + Oπ,h+1

(cid:3)

(cid:3)

(cid:3)

Thus, setting

Λπ,h = Q + P (cid:62)
Oπ,h = tr(Λπ,h+1Σ) + Oπ,h+1,

π,hPπ,h + (A + BPπ,h)(cid:62)Λπ,h+1(A + BPπ,h)

we have shown that V π(x, h) is a quadratic function of x.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

The next lemma shows that the optimal policy is linear.
Lemma 3. In an LQR, the optimal policy π(cid:63) is a non-stationary linear policy given by π(cid:63)(x, h) = P(cid:63),hx, with parameter
matrices P(cid:63),h ∈ RK×d at each level h. The optimal value function V (cid:63) is a non-stationary quadratic function given by
V (cid:63)(x, h) = x(cid:62)Λ(cid:63),hx + O(cid:63),h with parameter matrix Λ(cid:63),h ∈ Rd×d and offset O(cid:63),h ∈ R. The optimal parameters are
deﬁned recursively by,

O(cid:63),H = 0

Λ(cid:63),H = Q

P(cid:63),H = 0
P(cid:63),h = (I + B(cid:62)Λ(cid:63),h+1B)−1B(cid:62)Λ(cid:63),h+1A
Λ(cid:63),h = Q + P (cid:62)
O(cid:63),h = tr(Λ(cid:63),h+1Σ) + O(cid:63),h+1.

(cid:63),hP(cid:63),h + (A + BP(cid:63),h)(cid:62)Λ(cid:63),h+1(A + BP(cid:63),h)

Proof. We explicitly calculate the optimal policy π(cid:63) and demonstrate that it is linear. Then we instantiate these matrices in
Lemma 2 to compute the optimal value function.

For the optimal policy, we use backward induction on H. At the last level, we have,

π(cid:63)(x, H) = argmin

{x(cid:62)Qx + a(cid:62)a} = 0.

a

Recall that we are working with costs, so the optimal policy minimizes the expected cost. Thus P(cid:63),H = 0 ∈ RK×d and
π(cid:63)(x, H) is a linear function of x.

Plugging into Lemma 2 the value function has parameters

Λ(cid:63),H = Q,

O(cid:63),H = 0.

For the induction step, assume that π(cid:63)(x, h + 1) = P(cid:63),h+1x is linear and V (cid:63)(x, h + 1) is quadratic with parameter
Λ(cid:63),h+1 (cid:31) 0 and O(cid:63),h+1. We then have,

π(cid:63)(x, h) = argmin

x(cid:62)Qx + a(cid:62)a + Ex(cid:48)∼(x,a)V (cid:63)(x(cid:48), h + 1)

= argmin

x(cid:62)Qx + a(cid:62)a + E(cid:15)h(Ax + Ba + (cid:15)h)(cid:62)Λ(cid:63),h+1(Ax + Ba + (cid:15)h) + O(cid:63),h+1

= argmin

a(cid:62)(I + B(cid:62)Λ(cid:63),h+1B)a + 2(cid:104)Λ(cid:63),h+1Ax, Ba(cid:105).

a

a

a

This follows by applying deﬁnitions and eliminating terms that are independent of a. Since R, Λ(cid:63),h+1 (cid:31) 0 by assumption
and using the inductive hypothesis we can analytically minimize. Setting the derivative equal to zero gives,

Thus P(cid:63),h = (I + B(cid:62)Λ(cid:63),h+1B)−1B(cid:62)Λ(cid:63),h+1A.

a = (I + B(cid:62)Λ(cid:63),h+1B)−1B(cid:62)Λ(cid:63),h+1Ax.

As a consequence, we can now derive bounds on the policy and value function parameters. Recall that we assume that all
system parameters are bounded in spectral norm by Θ ≥ 1 and that (B(cid:62)B)−1 has minimum eigenvalue at least κ.
Corollary 1. With Θ and κ deﬁned above, we have

(cid:107)P(cid:63),h(cid:107)f ≤

(cid:107)Λ(cid:63),h(cid:107) ≤

Θ,

|O(cid:63),h| ≤ (H − h)

Θ2
κ

,

(cid:19)H−h

(cid:18) 6Θ6
κ2

(cid:18) 6Θ6
κ2

(cid:19)H−h

dΘ2.

Proof. Again we proceed by backward induction, using Lemma 3. Clearly (cid:107)P(cid:63),H (cid:107)F = 0, (cid:107)Λ(cid:63),H (cid:107)F ≤ Θ, |O(cid:63),H | = 0.

For the inductive step we can actually compute P(cid:63),h without any assumption on Λ(cid:63),h+1, except for the fact that it is
symmetric positive deﬁnite, which follows from Lemma 3. First, we consider just the matrix B(cid:62)Λ(cid:63),h+1A. Diagonalizing
Λ(cid:63),h+1 = U (cid:62)DU where U is orthonormal and D is diagonal, gives,

B(cid:62)Λ(cid:63),h+1A = (U B)(cid:62)D(U A) = (U B)(cid:62)D(U B)(B(cid:62)U (cid:62)U B)−1(U B)(cid:62)(U A)
= (U B)(cid:62)D(U B)(B(cid:62)B)−1B(cid:62)A = B(cid:62)Λ(cid:63),h+1ΠBA.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Here ΠB = B(B(cid:62)B)−1B(cid:62) is an orthogonal projection operator. This derivation uses the fact that since (U B)(cid:62)D has
rows in the column space of U B, we can right multiply by the projector onto U B. We also use that U (cid:62)U = I since U has
orthonormal rows and columns.

Thus, by the submultiplicative property of the spectral norm, we obtain

(cid:107)(I + B(cid:62)Λ(cid:63),h+1B)−1B(cid:62)Λ(cid:63),h+1A(cid:107)2 ≤ (cid:107)(I + B(cid:62)Λ(cid:63),h+1B)−1B(cid:62)Λ(cid:63),h+1B(cid:107)2(cid:107)(B(cid:62)B)−1B(cid:62)A(cid:107)2

≤ (cid:107)(B(cid:62)B)−1B(cid:62)A(cid:107)2 ≤ Θ2/κ.

Here κ is a lower bound on the minimum eigenvalue of B(cid:62)B.

Using this bound on (cid:107)P(cid:63),h(cid:107), we can now bound the optimal value function as

(cid:107)Λ(cid:63),h(cid:107) ≤ Θ + Θ4/κ2 + (Θ + Θ3/κ)2(cid:107)Λ(cid:63),h+1(cid:107) ≤ 6Θ6/κ2(cid:107)Λ(cid:63),h+1(cid:107).

The last bound uses the fact we apply a bound for (cid:107)Λ(cid:63),h+1(cid:107)2 that is larger than one, so the last term dominates. We also
use the inequalities Θ2/κ ≥ 1 and Θ ≥ 1. This recurrence yields

(cid:107)Λ(cid:63),h(cid:107)2 ≤

(cid:18) 6Θ6
κ2

(cid:19)H−h

Θ.

A naive upper bound on O(cid:63),h gives,

O(cid:63),h ≤ (cid:107)Λ(cid:63),h+1(cid:107) tr(Σ) + |O(cid:63),h+1| ≤ (H − h)

(cid:18) 6Θ6
κ2

(cid:19)H−h

dΘ2.

The ﬁnal component of the proposition is to demonstrate the Bellman factorization.

Proof of Proposition 10. Fix h and a value function g parametrized by matrices Λ and offset O at time h and Λ(cid:48), O(cid:48) at time
h + 1. Also ﬁx π which uses operator Pπ at time h.

E(π, g, π(cid:48), h) = Ex∼(π(cid:48),h)x(cid:62)Λx + O − x(cid:62)Qx − x(cid:62)P (cid:62)
= Ex∼(π(cid:48),h)x(cid:62)Λx + O − x(cid:62)Qx − x(cid:62)P (cid:62)
= tr (cid:2)(cid:0)Λ − Q − P (cid:62)

π Pπx − Ex(cid:48)∼(x,π(x))(x(cid:48))(cid:62)Λ(cid:48)x(cid:48) + O(cid:48)
π Pπx − E(cid:15)(Ax + BPπx + (cid:15))(cid:62)Λ(cid:48)(Ax + BPπx + (cid:15)) + O(cid:48)

π Pπ − (A + BPπ)(cid:62)Λ(cid:48)(A + BPπ)(cid:1) Ex∼(π(cid:48),h)xx(cid:62)(cid:3) + O − O(cid:48) − tr(ΛΣ).

π Pπ − (A + BPπ)(cid:62)Λ(cid:48)(A + BPπ)) in the ﬁrst d2 coordinates and
Thus we may write ξh(π, g) = vec(Λ − Q − P (cid:62)
O − O(cid:48) − tr(ΛΣ) in the last coordinate. We also write νh(π(cid:48)) = vec(Ex∼(π(cid:48),h)xx(cid:62)) in the ﬁrst d2 coordinates and 1 in
the last coordinate.

The norm bound on ξ is straightforward, since all terms in its decomposition have an exponential in H bound.

For ν, since the distribution is based on applying a bounded policy π(cid:48) at level h − 1 iteration, we can write x = A˜x +
BPπ(cid:48) ˜x + (cid:15) where ˜x is obtained by rolling in with π(cid:48) for h − 1 steps. If (π(cid:48), h − 1) denotes the distribution at the previous
level, this gives

(cid:107)Ex∼(π(cid:48),h)xx(cid:62)(cid:107)F ≤ (cid:107)Σ(cid:107)F + tr (cid:0)(A + BP )(cid:62)(A + BP )E˜x∼(π(cid:48),h−1) ˜x˜x(cid:62)(cid:1)

≤ (cid:107)Σ(cid:107)F + d(Θ + ΘB1)2(cid:107)E˜x∼(π(cid:48),h−1) ˜x˜x(cid:62)(cid:107)F .

Since at level one we have that the norm is at most (cid:107)Σ(cid:107)F , we obtain a recurrence which produces a bound at level h of

(cid:107)Ex∼(π(cid:48),h)xx(cid:62)(cid:107)F ≤ (cid:107)Σ(cid:107)F

di−1(Θ + ΘB1)2(i−1) ≤ (cid:107)Σ(cid:107)F HdH (ΘB1)2H ,

h
(cid:88)

i=1

if Θ, B1 ≥ 1, which is the regime of interest.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

C. Proofs of Main Results

In this section, we provide the main ideas as well as the key lemmas involved in proving Theorem 1. We also show how
the lemmas are assembled to prove the theorem. Detailed proofs of the lemmas are in Appendix D.

The proof follows an explore-or-terminate argument common to existing sample-efﬁcient RL algorithms. We argue that
the optimistic policy chosen in Line 5 of Algorithm 1 is either approximately optimal, or visits a context distribution under
which its associated value function has a large Bellman error. This implies that using this policy for exploration leads to
learning on a new context distribution. For sample efﬁciency, we then need to establish that this event cannot happen too
many times. This is done by leveraging the Bellman factorization of the process and arguing that the number of times an
(cid:15) sub-optimal policy is found can be no larger than ˜O(M H). Combining with the number of samples collected for every
sub-optimal policy, this immediately yields the PAC learning guarantee.

C.1. Key Lemmas for Theorem 1

We begin by recalling Lemma 1.
Lemma (Restatement of Lemma 1 from main text for convenience) With Vf = E[f (x1, πf (x1))], we have

Vf − V πf =

E(f, πf , h).

H
(cid:88)

h=1

The structure of this lemma is similar to many existing results in RL that upper-bound the loss of following an approximate
value function greedily using the function’s Bellman errors (e.g., Singh & Yee, 1994). However, most existing results are
inequalities that use max-norm relaxations to deal with mismatch in distributions; hence, they are likely to be loose. This
lemma, on the other hand, is an equality, thanks to the fact that we are comparing V πf to Vf , not V (cid:63). As the remaining
analysis shows, this simple equation allows us to relate policy loss (the LHS) with the average Bellman error (the RHS)
that we use to drive exploration. In particular, this lemma implies an explore-or-terminate behavior for the algorithm.
Lemma 4 (Optimism drives exploration). Suppose the estimates ˆVf and ˜E(ft, πt, h) in Line 2 and 7 always satisfy

| ˆVf − Vf | ≤ (cid:15)/8,

and

| ˜E(ft, πt, h) − E(ft, πt, h)| ≤

(cid:15)
8H

throughout the execution of the algorithm. Assume further that f (cid:63) is never eliminated. Then in any iteration t, one of the
following two statements holds:

(i) the algorithm does not terminate and

E(ft, πt, ht) ≥

(cid:15)
2H

,

(ii) the algorithm terminates and the output policy πt satisﬁes V πt ≥ V (cid:63)

F − (cid:15).

The lemma guarantees that the policy πt used at iteration t in OLIVE has sufﬁciently large Bellman error on at least one of
the levels, provided that the two conditions in Equation (18) are met. These conditions require that (1) we have reasonably
accurate value function estimates from Line 1, and (2) we collect enough samples in Line 6 to form reliable Bellman error
estimates under ft at each level h. The result of Theorem 1 can then be obtained using two further ingredients. First,
we need to make sure that the ﬁrst case in Lemma 4 does not happen too many times. Second, we need to collect enough
samples in Lines 1 and 6 to ensure the preconditions in Equation (18). We ﬁrst establish a bound on the number of iterations
using the Bellman rank of the problem, before moving on to sample complexity questions.
Lemma 5 (Iteration complexity). If (cid:98)E(f, πt, ht) in Line 13 always satisﬁes

| (cid:98)E(f, πt, ht) − E(f, πt, ht)| ≤ φ
throughout the execution of the algorithm (φ is the threshold in the elimination criterion), then f (cid:63) is never eliminated.
Furthermore, for any particular level h, if whenever ht = h we have
√

(20)

then the number of iterations that ht = h is at most M log

|E(ft, πt, ht)| ≥ 6

M φ,

(cid:17)

(cid:16) ζ
2φ

/ log 5
3 .

(17)

(18)

(19)

(21)

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Precondition (20) simply posits that we collect enough samples for reliable Bellman error estimation in Line 12. Intuitively,
since f (cid:63) has no Bellman error, this is sufﬁcient to ensure that it is never eliminated. Precondition (21) is naturally satisﬁed
by the exploration policies πt given Lemma 4, when φ is chosen appropriately according to (cid:15), M , and H. Given this, the
above lemma bounds the number of iterations at which we can ﬁnd a large Bellman error at any particular level.

The intuition behind this claim is most clear in the POMDP setting of Proposition 3. In this case, νh(f (cid:48)) in Deﬁnition 5
corresponds to the distribution over hidden states induced by πf (cid:48) at level h. At iteration t, the exploration policy πft induces
such a hidden-state distribution p = νh(ft) at the chosen level h = ht, which results in the elimination of all functions
that have large Bellman error on p. Thanks to the Bellman factorization, this corresponds to the elimination of all f with a
large |p(cid:62)ξh(f )|, where ξh(f ) is also deﬁned in Deﬁnition 5. In this case, it can be easily shown that ξh(f ) ∈ [−2, 2]M , so
the space of all such vectors {ξh(f ) : f ∈ F} at each level h is originally contained in an (cid:96)∞ ball in M dimensions with
radius 2, and, whenever ht = h, we intersect this set with two parallel halfspaces. Via a geometric argument adapted from
Todd (1982), we show that each such intersection reduces the volume of the space by a multiplicative factor of 3/5. We
also show that the volume is bounded from below, hence volume reduction cannot occur indeﬁnitely. Together, these two
facts lead to the iteration complexity in Lemma 5. The mathematical techniques used here are analogous to the analysis of
the Ellipsoid method in linear programming (see e.g. Bland et al. (1981)).

Finally, we need to ensure that the number of samples collected in each of Lines 1, 6, and 12 of OLIVE can be upper
bounded, which yields the overall PAC learning result in Theorem 1. The next three lemmas present precisely the deviation
bounds required for this argument. The ﬁrst two follow from simple applications of Hoeffding’s inequality.
Lemma 6 (Deviation bound for ˆVf ). With probability at least 1 − δ,

| ˆVf − Vf | ≤

(cid:114) 1
2nest

log

2N
δ

holds for all f ∈ F simultaneously. Hence, we can set nest ≥ 32

(cid:15)2 log 2N

δ

to guarantee that | ˆVf − Vf | ≤ (cid:15)/8.

This controls the number of samples required in Line 1.
Lemma 7 (Deviation bound for ˜E(ft, πt, h)). For any ﬁxed ft, with probability at least 1 − δ,

| (cid:98)E(ft, πt, h) − E(ft, πt, h)| ≤ 3

(cid:114) 1

2neval

log

2H
δ

holds for all h ∈ [H] simultaneously. Hence, we can set neval ≥ 288H 2
E(ft, πt, h)| ≤ (cid:15)

(cid:15)2

8H .

log 2H
δ

to guarantee that | ˜E(ft, πt, h) −

This lemma can be seen as the sample complexity at each iteration in Line 6. Note that no union bound over F is needed
here, since Line 6 only estimates the average Bellman error for a single function, which is ﬁxed before data is collected.
Finally, we bound the sample complexity of the learning step.
Lemma 8 (Deviation bound for (cid:98)E(f, πt, ht)). For any ﬁxed πt and ht, with probability at least 1 − δ,

| (cid:98)E(f, πt, ht) − E(f, πt, ht)| ≤

(cid:115)

8K log 2N
δ
n

+

2K log 2N
δ
n

holds for all f ∈ F simultaneously. Hence, we can set n ≥ 32K
as long as φ ≤ 4.

φ2 log 2N

δ

to guarantee that | (cid:98)E(f, πt, ht) − E(f, πt, ht)| ≤ φ

This lemma uses Bernstein’s inequality to exploit the small variance of the importance weighted estimates.

C.2. Proof of Theorem 1

Suppose the preconditions of Lemma 4 (Eq. (18)) and Lemma 5 (Eq. (20)) hold; we show them via concentration inequal-
ities later. Applying Lemma 4, in every iteration t before the algorithm terminates,

E(ft, πt, ht) ≥

= 6

M φ,

√

(cid:15)
2H

Contextual Decision Processes with low Bellman rank are PAC-Learnable

due to the choice of φ. For level h = ht, Eq. (21) is satisﬁed. According to Lemma 5, the event ht = h can happen at most
M log

3 times for every h ∈ [H]. Hence, the total number of iterations in the algorithm is at most

(cid:16) ζ
2φ

/ log 5

(cid:17)

HM log

/ log

= HM log

(cid:19)

(cid:18) ζ
2φ

5
3

(cid:32)

6H

M ζ

(cid:33)

√

(cid:15)

/ log

5
3

.

Now we are ready to apply the concentration inequalities to show that Eq. (18) and (20) hold with high probability. We
split the total failure probability δ among the following estimation events:

1. Estimation of ˆVf (Lemma 6; only once): δ/3.

2. Estimation of ˜E(ft, πt, h) (Lemma 7; every iteration): δ/

3HM log

(cid:16)

√

(cid:16) 6H

M ζ

(cid:17)

(cid:15)

/ log 5
3

(cid:17)

.

3. Estimation of (cid:98)E(f, πt, ht) (Lemma 8; every iteration): same as above.

Since these events happen in a particular sequence, the proof actually bounds the probability of these failure events condi-
tioned on all previous events succeeding. This imposes no technical challenge as fresh data is collected for every event, so
it effectively reduces to a standard union bound.

Applying Lemmas 6, 7, and 8 with the above failure probabilities, we can verify that the choices of nest, neval, and n in the
algorithm statement satisfy the preconditions of Lemmas 4 and 5. Finally, we upper bound the total number of episodes as

nest + neval · HM log

/ log

+ n · HM log

(cid:32)

6H

M ζ

(cid:33)

√

(cid:15)

5
3

= ˜O

(cid:18) log(N/δ)
(cid:15)2

+

M H 3
(cid:15)2

log(ζ/δ) +

M 2H 3K
(cid:15)2

(cid:19)

log(N ζ/δ)

= ˜O

(cid:19)

log(N ζ/δ)

.

(cid:32)

√

6H

M ζ

(cid:33)

5
3

/ log

(cid:15)
(cid:18) M 2H 3K
(cid:15)2

D. Auxiliary Proofs of the Main Lemmas

In this appendix we give the full proofs of the lemmas sketched in Appendix C. Note that OLIVER (Algorithm 3) with
parameters θ = 0 and η = 0 is precisely OLIVE, and the two analyses are identical. To avoid repetition, in this appendix
we analyze OLIVER (Algorithm 3) and prove the versions of the lemmas that can be used for Theorem 4. Readers can easily
F ,θ = V (cid:63)
recover the detailed proofs of the lemmas in Appendix C for OLIVE by letting θ = 0, η = 0, (cid:15)(cid:48) = (cid:15), f (cid:63)
F .
To facilitate understanding we break up the proofs into 3 parts. The main proofs appear in D.1, and two types of technical
lemmas are invoked from there: (1) a series of lemmas that adapt the work of Todd (1982) for the purpose, which are given
in D.2; (2) deviation bounds, which are given in D.3.

θ = f (cid:63), V (cid:63)

D.1. Main Proofs

Proof of Lemma 1. Recall from Deﬁnition 2 that the average Bellman errors are deﬁned as

E(f, π, h) = E (cid:2)f (xh, ah) − rh − f (xh+1, ah+1) (cid:12)

(cid:12) a1:h−1 ∼ π, ah:h+1 ∼ πf

(cid:3).

Expanding RHS of Eq. (17), we get

H
(cid:88)

h=1

E (cid:2)f (xh, ah) − rh − f (xh+1, ah+1) (cid:12)

(cid:12) a1:h−1 ∼ πf , ah:h+1 ∼ πf

(cid:3).

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Since all H expected values share the same distribution over trajectories, which is the one induced by a1:H ∼ πf , the
above expression is equal to

H
(cid:88)

h=1

= E

(cid:34) H
(cid:88)

(cid:16)

h=1

E (cid:2)f (xh, ah) − rh − f (xh+1, ah+1) (cid:12)

(cid:12) a1:H ∼ πf

(cid:3)

f (xh, ah) − rh − f (xh+1, ah+1)

(cid:12) a1:H ∼ πf

(cid:17) (cid:12)

(cid:35)

= E (cid:2)f (x1, πf (x1))(cid:3) − E (cid:2)rh

(cid:12)
(cid:12) a1:H ∼ πf

(cid:3) = Vf − V πf .

Lemma 9 (Optimism drives exploration, analog of Lemma 4). If the estimates ˆVf and ˜E(ft, πt, h) in Line 3 and 8 of
Algorithm 3 always satisfy

| ˆVf − Vf | ≤ (cid:15)(cid:48)/8,

| ˜E(ft, πt, h) − E(ft, πt, h)| ≤

(cid:15)(cid:48)
8H

throughout the execution of the algorithm (recall that (cid:15)(cid:48) is deﬁned on Line 1), and f (cid:63)
iteration t, either the algorithm does not terminate and

θ is never eliminated, then in any

or the algorithm terminates and the output policy πt satisﬁes V πt ≥ V (cid:63)

F ,θ − (cid:15)(cid:48) − Hθ.

E(ft, πt, ht) ≥

(cid:15)(cid:48)
2H

,

Proof. Eq. (23) follows directly from the termination criterion and Eq. (22). Suppose the algorithm terminates in iteration
t. Let fmax := argmaxf ∈Ft−1 Vf , and we have

V πt = Vft −

E(ft, πt, h)

H
(cid:88)

h=1

H
(cid:88)

h=1

≥ ˆVft −

˜E(ft, πt, h) − (cid:15)(cid:48)/4

≥ ˆVft − 7(cid:15)(cid:48)/8
≥ ˆVfmax − 7(cid:15)(cid:48)/8
≥ Vfmax − (cid:15)(cid:48) ≥ Vf (cid:63)
F ,θ − Hθ − (cid:15)(cid:48).
≥ V (cid:63)

θ

− (cid:15)(cid:48)

(Lemma 1)

(Eq. (22))

(termination criterion)
(ft is the maximizer of ˆVf )
θ is not eliminated)
(Lemma 1)

(f (cid:63)

The last inequality uses Lemma 1 on Vf (cid:63)
F ,θ, which is the value of policy πf (cid:63)
these two quantities to the average Bellman errors, which are upper bounded by θ at each level since f (cid:63)

and the deﬁnition of V (cid:63)

θ

θ

. Lemma 1 relates
θ is θ-valid.

Lemma 10 (Volumetric argument, analog of Lemma 5). If (cid:98)E(f, πt, ht) in Eq. (10) always satisﬁes

throughout the execution of the algorithm (φ is the threshold in the elimination criterion), then f (cid:63)
Furthermore, for any particular level h, if whenever ht = h, we have

θ is never eliminated.

| (cid:98)E(f, πt, ht) − E(f, πt, ht)| ≤ φ

|E(ft, πt, ht)| ≥ 3

M (2φ + θ + η) + η, ,

√

M log

/ log

ζ
2φ

5
3

.

then the number of iterations that ht = h is at most

(22)

(23)

(24)

(25)

(26)

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Proof. The ﬁrst claim that f (cid:63)
and the elimination threshold φ + θ. Below we prove the second claim.

θ is never eliminated follows directly from the fact |E(f (cid:63)

θ , πt, ht)| ≤ θ (Deﬁnition 8), Eq. (24),

For any particular level h, suppose i1 < · · · < iτ < · · · < iTh are the iteration indices with ht = h, {t : ht = h} ordered
from ﬁrst to last, and Th = |{t : ht = h}|. For convenience deﬁne i0 = 0. The goal is to prove an upper bound on Th.

Deﬁne notations:

• p1, . . . , pTh. pτ := νh(fiτ ) where νh(·) is given in Deﬁnition 10. Recall that fiτ is the optimistic function used for

exploration in iteration t = iτ .

• U(Fi0), U(Fi1 ), . . . , U(FiTh

). U(Fiτ ) = {ξh(f ) : f ∈ Fiτ } where ξh(f ) ∈ RM is given in Deﬁnition 10.

• Ψ = supf ∈F (cid:107)νh(f )(cid:107)2, and Φ = supf ∈F (cid:107)ξh(f )(cid:107)2. By Deﬁnition 10, Ψ · Φ ≤ ζ.

• V0, V1, . . . , VTh . V0 := {v : (cid:107)v(cid:107)2 ≤ Φ}, and Vτ := {v ∈ Vτ −1 : |p(cid:62)

τ v| ≤ 2φ + θ + η}.

• B0, B1, . . . , BTh . Bτ is a minimum volume enclosing ellipsoid (MVEE) of Vτ .

For every τ = 0, . . . , Th, we ﬁrst show that U(Fiτ ) ⊆ Vτ . When τ = 0 this is obvious. For τ ≥ 1, we have ∀f ∈ Fiτ ,

by the elimination criterion and Eq. (24). By Deﬁnition 10, this implies that, ∀v ∈ U(Fiτ ),

|E(f, πfiτ

, h)| ≤ 2φ + θ.

|p(cid:62)

τ v| ≤ 2φ + θ + η,

so U(Fiτ ) ⊆ Vτ .
Next we show that ∃v ∈ Vτ −1 such that |p(cid:62)
(implying that it survived) implies that this v can be chosen as

τ v| ≥ 3

√

M (2φ + θ + η). In fact, Eq. (25) and the fact that fit was chosen

v = ξh(fiτ ) ∈ U(Fiτ −1) ⊆ U(Fiτ −1) ⊆ Vτ −1.

(The ﬁrst “⊆” follows from the fact that Ft shrinks monotonically in Algorithm 3, since the learning steps between
t = iτ −1 + 1 and t = iτ − 1 on other levels can only eliminate functions.) We verify that this v satisﬁes the desired
property, given by Deﬁnition 10 and Eq. (25):

|p(cid:62)

τ v| = |(cid:104)νh(fiτ ), ξh(fiτ )(cid:105)| ≥ |E(fiτ , πiτ , h)| − η ≥ 3

M (2φ + θ + η).

√

Observing that Vt is centrally symmetric and consequently so is Bt (Todd & Yıldırım, 2007), we apply Lemma 11 and
√
Fact 4 with the variables set to d := M, B := Bτ −1, κ := 3

M (2φ + θ + η), γ := 2φ + θ + η. We obtain that

where B+ is the MVEE of V (cid:48)
2φ + θ + η} ⊆ V (cid:48)
have vol(Bτ ) ≤ vol(B+). Altogether we claim that

τ := {v ∈ Bτ −1 : |p(cid:62)

τ given that Vτ −1 ⊆ Bτ −1. Since B+ is an enclosing ellipsoid of V (cid:48)

τ v| ≤ 2φ + θ + η}. Note that Vτ = {v ∈ Vτ −1 : |p(cid:62)

τ v| ≤
τ , and Bτ is the MVEE of Vτ , we

vol(B+)
vol(Bt−1)

≤ 0.6,

vol(Bτ )
vol(Bτ −1)

≤ 0.6.

This result shows that the volume of Bτ shrinks exponentially with τ . To prove that Th is small, it sufﬁces to show that the
volume of B0 is not too large, and that of BTh is not too small. Let cM be the volume of Euclidean sphere with unit radius
in RM . By deﬁnition, vol(B0) = cM (Φ)M .

Contextual Decision Processes with low Bellman rank are PAC-Learnable

For vol(BTh), since (cid:107)pτ (cid:107)2 ≤ Ψ always holds, we can guarantee that






VT ⊇

q ∈ RM :

(cid:92)

|(cid:104)p, q(cid:105)| ≤ 2φ + θ + η

p∈RM :(cid:107)p(cid:107)2≤Ψ
⊇ (cid:8)q ∈ RM : (cid:107)q(cid:107)2 ≤ (2φ + θ + η)/Ψ(cid:9)
⊇ (cid:8)q ∈ RM : (cid:107)q(cid:107)2 ≤ 2φ/Ψ(cid:9) .






cM (2φ/Ψ)M
cM (Φ)M ≤

vol(BTh)
vol(B0)

=

Th(cid:89)

t=1

vol(Bt)
vol(Bt−1)

≤ 0.6Th .

Hence, vol(BTh ) ≥ cM (2φ/Ψ)M , and

Algebraic manipulations give

M log

≥ Th log

(cid:19)

(cid:18) ΨΦ
2φ

5
3

.

The second claim of the lemma statement follows by recalling that ΨΦ ≤ ζ.

D.2. Lemmas for the Volumetric Argument

(H¨older’s inequality)

We adapt the work of Todd (1982) to derive lemmas that we use in D.1. The main result of this section is Lemma 11. As
this section focuses on generic geometric results, we adopt notation more standard for these arguments unlike the notation
used in the rest of the paper.
Theorem 5 (Theorem 2 of Todd (1982)). Deﬁne E = {w ∈ Rd : w(cid:62)w ≤ 1} and Eβ = {w ∈ E : |e(cid:62)
0 < β ≤ d−1/2. The ellipsoid,

1 w| ≤ β} for

is a minimum volume enclosing ellipsoid (MVEE) for Eβ if

Fact 3. With E, E+, σ, ρ as in Theorem 5, we have

E+ = {w ∈ Rd | w(cid:62)(ρ(I − σe1e(cid:62)

1 ))−1w ≤ 1},

σ =

1 − dβ2
1 − β2

and

ρ =

d(1 − β2)
d − 1

.

Vol(E+)
Vol(E)

√

=

dβ

(cid:18) d

d − 1

(cid:19)(d−1)/2

(cid:0)1 − β2(cid:1)(d−1)/2

.

(27)

(28)

1 ) so that E+ = {w ∈ Rd : w(cid:62)G−1w ≤ 1}. Notice that E can be
Proof. For convenience, let us deﬁne G = ρ(I − σe1e(cid:62)
obtained from E+ by the afﬁne transformation v = G−1/2w, which means that if w ∈ E+ then v = G−1/2w ∈ E. Via
change of variables this implies that

The determinant is simply the product of the eigenvalues, which is easy to calculate since G is diagonal,

Vol(E+)
Vol(E)

= det(G1/2).

det(G1/2) = ρ(d−1)/2(ρ(1 − σ))1/2.

Plugging in the deﬁnitions of ρ, σ from Theorem 5 proves the statement.

Lemma 11. Consider a closed and bounded set V ⊂ Rd and a vector p ∈ Rd. Let B be any enclosing ellipsoid of V that
is centered at the origin, and we abuse the same symbol for the symmetric positive deﬁnite matrix that deﬁnes the ellipsoid,
i.e., B = {v ∈ Rd : v(cid:62)B−1v ≤ 1}. Suppose there exists v ∈ V with |p(cid:62)v| ≥ κ and deﬁne B+ as the minimum volume
enclosing ellipsoid of {v ∈ B : |p(cid:62)v| ≤ γ}. If γ/κ ≤ 1/

d, we have

√

vol(B+)
vol(B)

≤

√

d

γ
κ

d − 1

(cid:18) d

(cid:19)(d−1)/2 (cid:18)

1 −

γ2
κ2

(cid:19)(d−1)/2

.

(29)

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Proof. The ﬁrst claim is to prove a bound on p(cid:62)Bp.

κ ≤ |p(cid:62)v| = |p(cid:62)B1/2B−1/2v| ≤

p(cid:62)Bp

v(cid:62)B−1v ≤

p(cid:62)Bp.

(cid:112)

√

(cid:112)

The last inequality applies since v ∈ B so that v(cid:62)B−1v ≤ 1. Now we proceed to work with the ellipsoids, let L = {v :
|v(cid:62)p| ≤ γ}. Set B+ = M V EE(B (cid:84) L). We apply two translations of the coordinate system so that B gets mapped to
the unit ball and so that p gets mapped to αe1 (i.e. a scaled multiple of the ﬁrst standard basis vector). The ﬁrst translation
is done by setting w = B−1/2v where w is in the new coordinate system and v is in the old coordinate system. Let
p1 = B1/2p so that we can equivalently write L = {w : |w(cid:62)p1| ≤ γ}. The second translation maps p1 to αe1 via a
rotation matrix R such that RB1/2p = Rp1 = αe1. We also translate w to Rw but this doesn’t affect the now spherically
symmetric ellipsoid, so we do not change the variable names.
To summarize, after applying the scaling and the rotation, we are interested in M V EE(I (cid:84){w : |w(cid:62)e1| ≤ γ/α}) and
speciﬁcally, since volume ratios are invariant under afﬁne transformation, we have

Here I is the unit ball (i.e. the ellipsoid with identity matrix). Further applying Fact 3, we obtain

Vol(B+)
Vol(B)

=

Vol(M V EE(I (cid:84){w : |w(cid:62)e1| ≤ γ/α}))
Vol(I)

.

Vol(B+)
Vol(B)

=

√

d

γ
α

d − 1

(cid:18) d

(cid:19)(d−1)/2 (cid:18)

1 −

γ2
α2

(cid:19)(d−1)/2

.

It remains to lower bound α, which is immediate since

α = (cid:107)RB1/2p(cid:107)2 = (cid:107)B1/2p(cid:107)2 ≥ κ.

Substituting this lower bound on α completes the proof.

Fact 4. When γ/κ = 1
√
3

d

, the RHS of Eq. (29) is less than 0.6.

Proof. Plugging in the numbers, we have the RHS of Eq. (29) equal to

(cid:18) d

d − 1

1
3

9d − 1
9d

(cid:19)(d−1)/2

=

1 +

(cid:18)

1
3

8
9(d − 1)

(cid:19)9(d−1)/8 · 4/9

1
3

≤

exp(4/9) ≤ 0.52.

Here we used the fact that (1 + 1

x )x is monotonically increasing towards e on x ∈ [1, ∞).

D.3. Deviation Bounds

In this section we prove the deviation bounds. Note that the statement of the lemmas in this section, which are for OLIVER,
coincide with those stated in Appendix C for OLIVE. This is not surprising as the two algorithms draw data and estimate
quantities in the same way.
Lemma 12 (Deviation Bound for ˆVf ). With probability at least 1 − δ,

holds for all f ∈ F simultaneously. Hence, we can set nest ≥ 32

| ˆVf − Vf | ≤

log

(cid:114) 1
2nest
(cid:15)2 log 2N

δ

2N
δ
to guarantee that | ˆVf − Vf | ≤ (cid:15)/8.

Proof. The bound follows from a straight-forward application of Hoeffding’s inequality and the union bound, and we only
need to verify that the Vf is the expected value of the ˆVf , and the range of the random variables is [0, 1].
Lemma 13 (Deviation Bound for ˜E(ft, πt, h)). For any ﬁxed ft, with probability at least 1 − δ,

| ˜E(ft, πt, h) − E(ft, πt, h)| ≤ 3

(cid:114) 1

2neval

log

2H
δ

holds for all h ∈ [H] simultaneously. Hence, for any neval ≥ 288H 2
| ˜E(ft, πt, h) − E(ft, πt, h)| ≤ (cid:15)

(cid:15)2

8H .

log 2H

δ , with probability at least 1 − δ we have

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Proof. This bound is another straight-forward application of Hoeffding’s inequality and the union bound, except that the
random variables that go into the average have range [−1, 2], and we have to realize that ˜E(ft, πt, h) is an unbiased estimate
of E(ft, πt, h).

Lemma 14 (Deviation Bound for (cid:98)E(f, πt, ht)). For any ﬁxed πt and ht, with probability at least 1 − δ,

| (cid:98)E(f, πt, ht) − E(f, πt, ht)| ≤

(cid:115)

8K log 2N
δ
n

+

2K log 2N
δ
n

holds for all f ∈ F simultaneously. Hence, for any n ≥ 32K
| (cid:98)E(f, πt, ht) − E(f, πt, ht)| ≤ φ.

φ2 log 2N

δ and φ ≤ 4, with probability at least 1 − δ we have

Proof. We ﬁrst show that (cid:98)E(f, πt, ht) is an average of i.i.d. random variables with mean E(f, πt, ht). We use µ as a
shorthand for the distribution over trajectories induced by a1, . . . , aht−1 ∼ πt, ah ∼ unif(A), which is the distribu-
tion of data used to estimate (cid:98)E(f, πt, ht). On the other hand, let µ(cid:48) denote the distribution over trajectories induced by
a1, . . . , aht−1 ∼ πt, ah ∼ πf . The importance weight used in Eq. (10) essentially converts the distribution from µ to µ(cid:48),
hence the expected value of (cid:98)E(f, πt, ht) can be written as

Eµ [K1[ah = πf (xh)] (f (xh, ah) − rh − f (xh+1, πf (xh+1)))]

= Eµ(cid:48) [f (xh, ah) − rh − f (xh+1, πf (xh+1))] = E(f, πt, ht).

Now, we apply Bernstein’s inequality. We ﬁrst analyze the 2nd-moment of the random variable. Deﬁning
y(xh, ah, rh, xh+1) = f (xh, ah) − rh − f (xh+1, πf (xh+1)) ∈ [−2, 1], the 2nd-moment is
(K1[ah = πf (xh)]y(xh, ah, rh, xh+1))2(cid:105)
(cid:104)
(cid:104)
(Ky(xh, ah, rh, xh+1))2 (cid:12)

[ah = πf (xh)] · Eµ

[ah (cid:54)= πf (xh)] · 0

(cid:12) ah = πf (xh)

Eµ

(cid:105)

+ Pr
µ

= Pr
µ
1
K

≤

Eµ

(cid:2)K 2 · 4 (cid:12)

(cid:12) ah = πf (xh)(cid:3) = 4K.

Next we check the range of the centered random variable. The uncentered variable lies in [−2K, K], and the expected
value is in [−2, 1], so the centered variable lies in [−2K − 1, K + 2] ⊆ [−3K, 3K]. Applying Bernstein’s inequality, we
have with probability at least 1 − δ,

| (cid:98)E(f, πt, ht) − E(f, πt, ht)| ≤

2 Var [K1[ah = πf (xh)]y(xh, ah, rh, xh+1)] log 2N
δ
n

+

6K log 2N
δ
3n

(cid:115)

(cid:115)

≤

8K log 2N
δ
n

+

2K log 2N
δ
n

.

(variance is bounded by 2nd-moment)

As long as 2K log 2N
for n, which indeed guarantees that 2K log 2N

≤ 1, the above is bounded by 2

n

δ

δ

(cid:113) 8K log 2N
n
≤ 1 as φ ≤ 4.

δ

n

. The choice of n follows from solving 2

(cid:113) 8K log 2N
n

δ

= φ

E. Proofs of Extensions

E.1. Proof for Unknown Bellman Rank (Theorem 2)

Since we assign δ(cid:48) = δ

i(i+1) failure probability to the i-th call of Algorithm 2, the total failure probability is at most

∞
(cid:88)

i=1

δ
i(i + 1)

= δ

∞
(cid:88)

i=1

(cid:18) 1
i

(cid:19)

−

1
i + 1

= δ.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

So with probability at least 1 − δ, all high probability events in the analysis of OLIVE occur for every i = 1, 2, . . .. Note
that regardless of whether M (cid:48) < M , we never eliminate f (cid:63) according to Lemma 5. Hence Lemma 4 holds and whenever
the algorithm returns a policy it is near-optimal.

While the algorithm returns a near-optimal policy if it terminates, we still must prove that the algorithm terminates. Since
when M (cid:48) < M Eq. (21) and Lemma 10 do not apply, we cannot naively use arguments from the analysis of OLIVE.
However, we monitor the number of iterations that have passed in each execution to OLIVE and stop the subroutine when
the actual number of iterations exceeds the iteration complexity bound (Lemma 5) to prevent wasting more samples on the
wrong M (cid:48).

OLIVE is guaranteed to terminate within the sample complexity bound and output near-optimal policy when M ≤ M (cid:48).
Since M (cid:48) grows on a doubling schedule, for the ﬁrst M (cid:48) that satisﬁes M ≤ M (cid:48), we have M (cid:48) ≤ 2M and i ≤ log2 M + 1.
Hence, the total number of calls is bounded by log2 M + 1.

Finally, since the sample complexity bound in Theorem 1 is monotonically increasing in M and 1/δ and the schedule for
δ(cid:48) is decreasing, we can bound the total sample complexity by that of the last call to OLIVE multiplied by the number of
calls. The last call to OLIVE has M (cid:48) ≤ 2M , and δ(cid:48) = i(i+1)
, so the sample complexity bound is
only affected by factors that are at most logarithmic in the relevant parameters.

δ ≤ (log2 M +2)(log2 M +1)

δ

E.2. Proofs for Inﬁnite Hypothesis Classes

In this section we prove sample complexity guarantee for using inﬁnite hypothesis classes in Appendix A.3. Recall that
we are working with separated policy class Π and V-value function class G, and when running OLIVE any occurrence of
f ∈ F is replaced appropriately by (π, g) ∈ Π × G. For clarity, we use (π, g) instead of f in the derivations in this section.
We assume that the two function classes have ﬁnite Natarajan dimension and pseudo dimension respectively.

The key technical step for the sample complexity guarantee is to establish the necessary deviation bounds for inﬁnite
classes. Among these deviation bounds, the bound on ˜E((πt, gt), πt, h) (Lemma 7) does not involve union bound over F,
so it can be reused without modiﬁcation. The other two bounds need to be replaced by Lemma 15 and 16, stated below.
With these lemmas, Theorem 3 immediately follows simply by replacing the deviation bounds.

Deﬁnition 11. Deﬁne dΠ = max(Ndim(Π), 6), dG = max(Pdim(G), 6), and d = dΠ + dG.

Lemma 15. If

nest ≥

dG log

+ log(8e(dG + 1)) + log

(cid:18)

8192
(cid:15)2

128e
(cid:15)

(cid:19)

,

1
δ

(30)

then with probability at least 1 − δ, | ˆV(π,g) − V(π,g)| ≤ (cid:15)/8, ∀(π, g) ∈ Π × G.

We remark that both the estimate ˆV(π,g) and population quantity V(π,g) are independent of π in the separable case, and
hence the sample complexity is independent of dΠ.

Lemma 16. If

(cid:18)

n ≥

1152K 2
φ2

48eK
φ

(cid:16)

6d log (2eKd) log

+ log

8e(6d log (2eKd) + 1)

+ log

(31)

(cid:17)

(cid:19)

,

3
δ

then for any ﬁxed πt and ht, with probability at least 1 − δ,

| (cid:98)E((π, g), πt, ht) − E((π, g), πt, ht)| ≤ φ, ∀(π, g) ∈ Π × G.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Proof of Theorem 3. Set the algorithm parameters to:

φ =

neval =

n =

(cid:15)
√
12H
288H 2
(cid:15)2

,

M

log

(cid:16)

1152K 2
φ2

nest =

(cid:18)

8192
(cid:15)2

dG log
√

128e
(cid:15)
(cid:33)

(cid:32)

12H 2M log(6H

M ζ/(cid:15))

+ log(8e(dG + 1)) + log

(cid:19)

,

3
δ

(cid:16)

(cid:17)

6d log

2eK

log

+ log

8e(6d log (2eKd) + 1)

(cid:17)

,

(cid:16)

δ

48eK
φ

√

δ

+ log

18HM log(6H

M ζ/(cid:15))

(cid:17)

.

The rest of the proof is essentially the same as the proof of Theorem 1, and the sample complexity follows by noticing that
nest = ˜O( dG +log(1/δ)

) and n = ˜O(K 2(dΠ + dG + log(1/δ))/φ2).

(cid:15)2

Lemma 15 is a straight-forward application of Corollary 2 introduced in E.2.1 and are not proved separately.
In the
remainder of this section we prove Lemma 16. Before that, we review some standard deﬁnitions and results from statistical
learning theory.

E.2.1. DEFINITIONS AND BASIC LEMMAS

Notations X , x, n, d, ξ in this section are used according to conventions in the literature and may not share semantics with
the same symbols used elsewhere in this paper.

Deﬁnition 12 (VC-Dimension). Given hypothesis class H ⊂ X → {0, 1}, its VC-dimension VC-dim(H) is deﬁned as the
maximal cardinality of a set X = {x1, . . . , x|X|} ⊂ X that satisﬁes |HX | = 2|X| (or X is shattered by H), where HX is
the restriction of H to X, namely {(h(x1), . . . , h(x|X|)) : h ∈ H}.
Lemma 17 (Sauer’s Lemma). Given hypothesis class H ⊂ X → {0, 1} with d = VC-dim(H) < ∞, we have ∀X =
(x1, . . . , xn) ∈ X n,

Lemma 18 (Sauer’s Lemma for Natarajan dimension (Ben-David et al., 1992; Haussler & Long, 1995)). Given hypothesis
class H ⊂ X → Y with Ndim(H) ≤ d, we have ∀X = (x1, . . . , xn) ∈ X n,

|HX | ≤ (n + 1)d.

|HX | ≤

(cid:18) ne(K + 1)2
2d

(cid:19)d

,

where K = |Y|.
Deﬁnition 13 (Covering number). Given hypothesis class H ⊂ X → R, (cid:15) > 0, X = (x1, . . . , xn) ∈ X n, the covering
number N1(α, H, X) is deﬁned as the minimal cardinality of a set C ⊂ Rn, such that for any h ∈ H there exists
c = (c1, . . . , cn) ∈ C where 1
n
Lemma 19 (Bounding covering number by pseudo dimension (Haussler, 1995)). Given hypothesis class H ⊂ X → R
with Pdim(H) ≤ d, we have for any X ∈ X n,

i=1 |h(xi) − ci| ≤ α.

(cid:80)n

N1(α, H, X) ≤ e(d + 1)

(cid:19)d

.

(cid:18) 2e
α

Lemma 20 (Uniform deviation bound using covering number (Pollard, 2012); also see Devroye et al. (1996), Theorem
29.1). Let H ⊂ X → [0, b] be a hypothesis class, and (x1, . . . , xn) be i.i.d. samples drawn from some distribution
supported on X . For any α > 0,

(cid:40)

Pr

sup
h∈H

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
h(xi) − E[h(x1)]
(cid:12)
(cid:12)
(cid:12)

(cid:41)

> α

≤ 8 E (cid:2)N1

(cid:0)α/8, H, (x1, . . . , xn)(cid:1)(cid:3) exp

(cid:18)

−

nα2
128b2

(cid:19)

.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Corollary 2 (Uniform deviation bound using pseudo dimension). Suppose Pdim(H) ≤ d, then

(cid:40)

Pr

sup
h∈H

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
h(xi) − E[h(x1)]
(cid:12)
(cid:12)
(cid:12)

(cid:41)

> α

≤ 8e(d + 1)

(cid:18) 16e
α

(cid:19)d

(cid:18)

exp

−

(cid:19)

.

nα2
128b2

To guarantee that this probability is upper bounded by δ, it sufﬁces to have

n ≥

d log

+ log(8e(d + 1)) + log

(cid:18)

128
α2

16e
α

(cid:19)

.

1
δ

E.2.2. PROOF OF LEMMA 16

The idea is to establish deviation bounds for each of the three terms in the deﬁnition of (cid:98)E((π, g), πt, ht) (Line 13). Each
term takes the form of an importance weight multiplied by a real-valued function, and we ﬁrst show that the function
space formed by these products has bounded pseudo dimension. We state this supporting lemma in terms of an arbitrary
value-function class V which might operate on an input space X (cid:48) different from the context space X . In the sequel, we
instantiate V and X (cid:48) in the the lemma with speciﬁc choices to prove the desired results.
Lemma 21. Let Y be a label space with |Y| = K, let Π ⊆ X → Y be a function class with Natarajan dimension at most
dΠ ∈ [6, ∞), and let V ⊆ X (cid:48) → [0, 1] be a class with pseudo dimension at most dV ∈ [6, ∞). The hypothesis class H =
{(x, a, x(cid:48)) (cid:55)→ 1[a = π(x)]g(x(cid:48)) : π ∈ Π, g ∈ V} has pseudo dimension Pdim(H) ≤ 6(dΠ + dV ) log (2eK(dΠ + dV )).

Proof. Recall that Pdim(H) = VC-dim(H+), so it sufﬁces to show that for any
X = {(x1, a1, x(cid:48)
Note that since g(x) ∈ [0, 1] for all g, x

1, ξ1), . . . , (xd, ad, x(cid:48)

d, ξd)} ∈ (X ×A×X (cid:48)×R)d where d = 6(dΠ+dV ) log (2eK(dΠ + dV )), |H+

X | < 2d.

(cid:104)
H+ = {(x, a, x(cid:48), ξ) (cid:55)→ 1

1[a = π(x)]g(x(cid:48)) > ξ

(cid:105)

}

= {(x, a, x(cid:48), ξ) (cid:55)→ 1[ξ < 0] + 1[ξ ≥ 0] · 1[a = π(x)] · 1[g(x(cid:48)) > ξ]}

For points where ξi < 0, all hypotheses in H+ produce label 1, so without loss of generality we can assume that ξi ≥
0, i = 1, . . . , d.

With a slight abuse of notation, let ΠX denote the restriction of Π to the set of contexts {x1, . . . , xd} (actions and future
X denote the restriction of V + to
contexts (a1, x(cid:48)
{(x(cid:48)

d) are ignored since Π does not operate on them), and V +
X can be produced by the Cartesian product of ΠX and V +

1, ξ1), . . . , (x(cid:48)

X as follows:

X = {(1[a1 = α1]β1, . . . , 1[ad = αd]βd) : (α1, . . . , αd) ∈ ΠX , (β1, . . . , βd) ∈ V +

X }.

X | ≤ |ΠX | |V +

X |. Recall that Ndim(Π) ≤ dΠ and VC-dim(V +) = Pdim(V) ≤ dV . Applying Lemma 18 and

1), . . . , (ad, x(cid:48)
d, ξd)}. H+
H+

Therefore, |H+
17:

|H+

X | ≤

(cid:18) de(K + 1)2
2dΠ

(cid:19)dΠ

(d + 1)dV .

The logarithm of the RHS is

dΠ log

(cid:18) de(K + 1)2
2dΠ

(cid:19)

+ dV log(d + 1) < dΠ log(de(K + 1)2) + dV log(d + 1)

≤ dΠ log d + 2dΠ log(2eK) + dV log(d + 1) ≤ 2(dΠ + dV ) log(2eK) + (dΠ + dV ) log(2d).

It remains to be shown that this is less than log(2d) = d log 2. Note that

d log 2 > 3(dΠ + dV )(log(2eK) + log(dΠ + dV )),

so we only need to show that (dΠ + dV ) log(2d) ≤ (dΠ + dV ) log(2eK) + 3(dΠ + dV ) log(dΠ + dV ). Now

(dΠ + dV ) log(2d) = (dΠ + dV )

log(12(dΠ + dV )) + log log(2eK(dΠ + dV ))

(cid:16)

(cid:16)

(cid:17)

(cid:17)

≤ 2(dΠ + dV ) log(dΠ + dV ) + (dΠ + dV ) log
≤ 2(dΠ + dV ) log(dΠ + dV ) + (dΠ + dV )(cid:0) log(2eK) + log(dΠ + dV )(cid:1).

log(2eK) + log(dΠ + dV )

(dΠ + dV ≥ 12)

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Proof of Lemma 16. Recall that when we are given a policy class Π and separate V-value function class G, for every
π ∈ Π, g ∈ G, we instead estimate average Bellman error with

(cid:98)E((π, g), πt, ht) =

n
(cid:88)

1[a(i)
ht

1
n

i=1

= π(x(i)
ht
1/K

)]

(cid:16)

g(x(i)
ht

) − r(i)
ht

− g(x(i)

(cid:17)
ht+1)

.

So it sufﬁces to show that the averages of 1[a(i)
)]g(x(i)
ht+1) are
ht
ht
φ
3K -close to their expectations with probability at least 1 − δ/3, respectively. It turns out that, we can use Lemma 21 for
all the three terms. For the ﬁrst and the third terms, we apply Lemma 21 with V = G, X (cid:48) = X , and obtain the necessary
sample size directly from Corollary 2. For the second term, we apply Lemma 21 with V = {x (cid:55)→ x}, X (cid:48) = R. Note that
in this case V is a singleton with the only element being the identity function over R, so it is clear that Pdim(V) < 6 ≤ dG,
hence the sample size for the other two terms is also adequate for this term.

= π(x(i)
ht

= π(x(i)
ht

= π(x(i)
ht

), 1[a(i)
ht

, 1[a(i)
ht

)]g(x(i)

)]r(i)
ht

E.3. Proofs for OLIVER

Recall that the main lemmas for analyzing OLIVER have been proved in Appendix D.1, so below we directly prove Theo-
rem 4.

Proof of Theorem 4. Suppose the preconditions of Lemma 9 (Eq. (22)) and Lemma 10 (Eq. (24)) hold; we show them by
invoking the deviation bounds later. By Lemma 9, when the algorithm terminates, the value of the output policy is at least

Recall that (cid:15)(cid:48) = (cid:15) + 2H(3

M (θ + η) + η) (Line 1), so the suboptimality compared to V (cid:63)

F ,θ is at most

√

√

(cid:15) + 2H(3

M (θ + η) + η) + Hθ ≤ (cid:15) + 8H

M (θ + η),

F ,θ − (cid:15)(cid:48) − Hθ.
V (cid:63)

which establishes the suboptimality claim.

It remains to show the sample complexity bound. Applying Lemma 9, in every iteration t before the algorithm terminates,

E(ft, πt, ht) ≥

+ 3

M (θ + η) + η = 3

M (2φ + θ + η) + η,

(cid:15)(cid:48)
2H

=

(cid:15)
2H

√

thanks to the choice of φ and (cid:15)(cid:48). For level h = ht, Eq. (25) is satisﬁed. According to Lemma 10, the event ht = h can
3 times for every h ∈ [H]. Hence, the total number of iterations in the algorithm is at
happen at most M log
most

(cid:16) ζ
2φ

/ log 5

(cid:17)

HM log

/ log

= HM log

(cid:19)

(cid:18) ζ
2φ

5
3

(cid:32)

6H

M ζ

(cid:33)

√

(cid:15)

/ log

5
3

.

Now we are ready to apply the deviation bounds to show that Eq. (22) and 20 hold with high probability. We split the total
failure probability δ among the following events:

√

√

1. Estimation of ˆVf (Lemma 12; only once): δ/3.

2. Estimation of ˜E(ft, πt, h) (Lemma 13; every iteration): δ/

3HM log

(cid:16)

√

(cid:16) 6H

M ζ

(cid:17)

(cid:15)

/ log 5
3

(cid:17)

.

3. Estimation of (cid:98)E(f, πt, ht) (Lemma 14; every iteration): same as above.

Applying Lemma 12, 13, 14 with the above failure probabilities, the choices of nest, neval, n in the algorithm statement
satisfy the preconditions of Lemmas 9 and 10. In particular, the choice of nest and neval guarantee that | ˆVf − Vf | ≤ (cid:15)/8
and | ˜E(ft, πt, h) − E(ft, πt, h)| ≤ (cid:15)/(8H), which are tighter than needed as (cid:15) ≤ (cid:15)(cid:48) (only (cid:15)(cid:48)/8 and (cid:15)(cid:48)/(8H) are needed
respectively, but tightening these bounds does not improve the sample complexity signiﬁcantly, so we keep them the same
as in Theorem 1 for simplicity). The remaining calculation of sample complexity is exactly the same as in the proof of
Theorem 1.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

F. Lower Bounds

F.1. An Exponential Lower Bound

We include a result from Krishnamurthy et al. (2016) to formally show that, without making additional assumptions, the
sample complexity of value-based RL for CDPs as introduced in Section 2 has a lower bound of order K H .
Proposition 11 (Restatement of Proposition 2 in Krishnamurthy et al. (2016)). For any H, K ∈ N with K ≥ 2, and any
(cid:15) ∈ (0, (cid:112)1/8), there exists a family of ﬁnite-horizon MDPs with horizon H and |A| = K, and a function space F with
|F| = K H and a universal constant c, such that Q(cid:63) ∈ F for all MDP instances in the family, yet for any algorithm and
any T ≤ cK H /(cid:15)2, the probability that the algorithm outputs a policy ˆπ with V ˆπ ≥ V (cid:63) − (cid:15) after collecting T trajectories
is at most 2/3 when the problem instance is chosen from the family by an adversary.

Proof. The proof relies on the fact that CDPs include MDPs where the state space is arbitrarily large. Each instance of
the MDP family is a complete tree with branching factor K and depth H. Transition dynamics are deterministic, and only
leaf nodes have non-zero rewards. All leaves give Ber(1/2) rewards, except for one that gives Ber(1/2 + (cid:15)). Changing the
position of the most rewarding leaf node yields a family of K H MDP instances so collecting optimal Q-value functions
forms the desired function class F. Since F provides no information other than the fact that the true MDP lies in this
family, the problem is equivalent to identifying the best arm in a multi-arm bandit with K H arms, and the remaining
analysis follows exactly as in Krishnamurthy et al. (2016).

F.2. A Polynomial Lower Bound that Depends on Bellman Rank

In this section, we prove a new lower bound for layered episodic MDPs that meet the assumptions we make in this paper.

We ﬁrst recall some deﬁnitions. A layered episodic MDP is deﬁned by a time horizon H, a state space S, partitioned
into sets S1, . . . , SH , each of size at most M , and an action space A of size K. The system descriptor is replaced with a
transition function Γ that associates a distribution over states with each state action pair. More formally, for any sh ∈ Sh,
and a ∈ A, Γ(sh, a) ∈ ∆(Sh+1). The starting state is drawn from Γ1 ∈ ∆(S1), and all transitions from SH are terminal.

There is also a reward distribution R that associates a random reward with each state-action pair. We use r ∼ R(s, a) to
denote the random instantaneous reward for taking action a at state s. We assume that the cumulative reward (cid:80)H
h=1 rh ∈
[0, 1], where rh is the reward received at level h as in Assumption 1.

Observe that this process is a special case of the ﬁnite-horizon Contextual Decision Process and moreover, with the set of
all value functions F = (S ×A → [0, 1]), admits a Bellman factorization with Bellman rank at most M (by Proposition 1).
Thus the upper bounds for PAC learning apply directly to this setting.

We now state the lower bound.
). For any algorithm and any n ≤ cM KH/(cid:15)2, there exists a layered
Theorem 6. Fix M ≥ 4, H, K ≥ 2 and (cid:15) ∈ (0,
episodic MDP with H layers, M states per layer, and K actions, such that the probability that the algorithm outputs a
policy ˆπ with V (ˆπ) ≥ V (cid:63) − (cid:15) after collecting n trajectories is at most 11/12. Here c > 0 is a universal constant.

1
√
48

8

The result precludes a o(M KH/(cid:15)2) PAC-learning sample complexity bound since in this case the algorithm must fail
with constant probability. The result is similar in spirit to other lower bounds for PAC-learning MDPs (Dann & Brunskill,
2015; Krishnamurthy et al., 2016), but we are not aware of any lower bound that applies directly to the setting. There are
two main differences between this bound and the lower bound due to Dann & Brunskill (2015) for episodic MDPs. First,
that bound assumes that the total reward is in [0, H], so the H 2 dependence in the sample complexity is a consequence of
scaling the rewards. Second, that MDP is not layered, but instead has M total states shared across all layers. In contrast,
our process is layered with M distinct states per layer and total reward bounded in [0, 1]. Intuitively, the additional H
dependence arises simply from having M H total states.

At a high level, the proof is based on embedding Θ(M H) independent multi-arm bandit instances into a MDP and requiring
that the algorithm identify the best action in Ω(M H) of them to produce a near-optimal policy. By appealing to a sample
complexity lower bound for best arm identiﬁcation, this implies that the algorithm requires Ω(M HK/(cid:15)2) samples to
identify a near-optimal policy.

We rely on a fairly standard lower bound for best arm identiﬁcation. We reproduce the formal statement from Krishna-
murthy et al. (2016), although the proof is based on earlier lower bounds due to Auer et al. (2002).

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Proposition 12. For any K ≥ 2 and τ ≤ (cid:112)1/8 and any best arm identiﬁcation algorithm that produces an estimate ˆa,
there exists a multi-arm bandit problem for which the best arm a(cid:63) is τ better than all others, but P[ˆa (cid:54)= a(cid:63)] ≥ 1/3 unless
the number of samples T is at least K

72τ 2 .

In particular, the problem instance used in this lower bound is one where the best arm a(cid:63) has reward Ber(1/2 + (cid:15)), while
all other arms have reward Ber(1/2). Our construction embeds precisely these instances into the MDP.

Proof. We construct an MDP with M states per level, H levels, and K actions per state. At each level, we allocate
three special states, wh, gh, and bh, for “waiting”, “good”, and “bad.” The remaining M − 3 “bandit” states are denoted
sh,i, i ∈ [M − 3]. Each bandit state has an unknown optimal action a(cid:63)

h,i.

The dynamics are as follows.

• For waiting states wh, all actions are equivalent and with probability 1 − 1/H, they transition to the next waiting
state wh+1. With the remaining 1/H probability, they transition randomly to one of the bandit state sh+1,i so each
subsequent bandit state is visited with probability

1
H(M −3) .

• For bandit states sh,i, the optimal action a(cid:63)

h,i transitions to the good state gh+1 with probability 1/2 + τ and otherwise
to the bad state bh+1. All other actions transition to gh+1 and bh+1 with probability 1/2. Here τ is a parameter we set
toward the end of the proof.

• Good states always transition to the next good state and bad states always transition to bad states.

• The starting state is w1 with probability 1 − 1/H and s1,i with probability

1

H(M −3) for each i ∈ [M − 3].

The reward at all states except gH is zero, and the reward at gH is one. Clearly the optimal policy takes actions a(cid:63)
each bandit state, and takes arbitrary actions at the waiting, good, and bad states.

h,i for

This construction embeds H(M − 3) best arm identiﬁcation problems that are identical to the one used in Proposition 12
into the MDP. Moreover, these problems are independent in the sense that samples collected from one provides no infor-
mation about any others. Appealing to Proposition 12, for each bandit state (h, i), unless K
72τ 2 samples are collected from
that state, the learning algorithm fails to identify the optimal action a(cid:63)

h,i with probability at least 1/3.

After the execution of the algorithm, let B be the set of (h, i) pairs for which the algorithm identiﬁes the correct action.
Let C be the set of (h, i) pairs for which the algorithm collects fewer than K
72τ 2 samples. For a set S, we use SC to denote
the complement.

E[|B|] = E

1[ah,i = a(cid:63)

h,i]







(cid:88)

(h,i)

≤ ((M − 3)H − |C|) +

E1[ah,i = a(cid:63)

h,i]

≤ ((M − 3)H − |C|) +

|C| = (M − 3)H − |C|/3



(cid:88)

(h,i)∈C
2
3

The second inequality is based on Proposition 12. Now, by the pigeonhole principle, if n ≤ (M −3)H
algorithm can collect K

72τ 2 samples from at most half of the bandit problems. Thus |C| > (M − 3)H/2, which implies,

72τ 2 , then the

× K

2

By Markov’s inequality,

E[|B|] <

(M − 3)H

5
6

(cid:20)

P

11
12

|B| ≥

(M − 3)H

≤

(cid:21)

E[|B|]
11
12 (M − 3)H

<

5/6
11/12

= 10/11

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Thus with probability at least 1/11 we know that |B| ≤ 11
on 1/12 fraction of the bandit problems. Under this event, the suboptimality of the policy produced by the algorithm is,

12 (M − 3)H, so the algorithm failed to identify the optimal action

V (cid:63) − V (ˆπ) = P[visit BC] × τ = P[

visit (h, i)] × τ =

(cid:88)

P[visit (h, i)] × τ

(cid:91)

(h,i)∈BC

1
H(M − 3)

(1 − 1/H)h−1τ ≥

(cid:88)

(1 − 1/H)H τ

=

≥

(cid:88)

(cid:88)

(h,i)∈BC

(h,i)∈BC

1
H(M − 3)

1
4

τ ≥

H(M − 3)
12

(h,i)∈BC

1
H(M − 3)

(h,i)∈BC
1
H(M − 3)

1
4

τ =

τ
48

.

Here we use the fact that the probability of visiting a bandit state is independent of the policy and that the policy can only
visit one bandit state per episode, so the events are disjoint. Moreover, if we visit a bandit state for which the algorithm
failed to identify the optimal action, the difference in value is τ , since the optimal action visits the good state with τ more
probability than a suboptimal one. The remainder of the calculation uses the transition model, the fact that H ≥ 2, and
ﬁnally the fact that |B| ≤ 11
12 (M − 3)H. Setting τ = 48(cid:15) and using the requirement on τ gives a stricter requirement on (cid:15)
and proves the result.

