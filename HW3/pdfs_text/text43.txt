A Closer Look at Memorization in Deep Networks

Devansh Arpit * 1 2 Stanisław Jastrz˛ebski * 3 Nicolas Ballas * 1 2 David Krueger * 1 2 Emmanuel Bengio 4
Maxinder S. Kanwal 5 Tegan Maharaj 1 6 Asja Fischer 7 Aaron Courville 1 2 8 Yoshua Bengio 1 2 9
Simon Lacoste-Julien 1 2

Abstract

We examine the role of memorization in deep
learning, drawing connections to capacity, gen-
eralization, and adversarial robustness. While
deep networks are capable of memorizing noise
data, our results suggest that they tend to pri-
In our
oritize learning simple patterns ﬁrst.
experiments, we expose qualitative differences
in gradient-based optimization of deep neural
networks (DNNs) on noise vs. real data. We
also demonstrate that for appropriately tuned
explicit regularization (e.g., dropout) we can
degrade DNN training performance on noise
datasets without compromising generalization on
real data. Our analysis suggests that the notions
of effective capacity which are dataset indepen-
dent are unlikely to explain the generalization
performance of deep networks when trained with
gradient based methods because training data it-
self plays an important role in determining the
degree of memorization.

1. Introduction

The traditional view of generalization holds that a model
with sufﬁcient capacity (e.g. more parameters than training
examples) will be able to “memorize” each example, over-
ﬁtting the training set and yielding poor generalization to
validation and test sets (Goodfellow et al., 2016). Yet deep
neural networks (DNNs) often achieve excellent gener-
alization performance with massively over-parameterized
models. This phenomenon is not well-understood.

*Equal contribution 1Montréal Institute for Learning Algo-
rithms, Canada 2Université de Montréal, Canada 3Jagiellonian
University, Krakow, Poland 4McGill University, Canada
5University of California, Berkeley, USA 6Polytechnique
Montréal, Canada 7University of Bonn, Bonn, Germany
8CIFAR Fellow 9CIFAR Senior Fellow. Correspondence to:
<david.krueger@umontreal.ca>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

From a representation learning perspective, the general-
ization capabilities of DNNs are believed to stem from
their incorporation of good generic priors (see, e.g., Ben-
gio et al. (2009)). Lin & Tegmark (2016) further suggest
that the priors of deep learning are well suited to the phys-
ical world. But while the priors of deep learning may help
explain why DNNs learn to efﬁciently represent complex
real-world functions, they are not restrictive enough to rule
out memorization.

On the contrary, deep nets are known to be universal ap-
proximators, capable of representing arbitrarily complex
functions given sufﬁcient capacity (Cybenko, 1989; Hornik
et al., 1989). Furthermore, recent work has shown that the
expressiveness of DNNs grows exponentially with depth
(Montufar et al., 2014; Poole et al., 2016). These works,
however, only examine the representational capacity, that
is, the set of hypotheses a model is capable of expressing
via some value of its parameters.

Because DNN optimization is not well-understood, it is un-
clear which of these hypotheses can actually be reached by
gradient-based training (Bottou, 1998). In this sense, opti-
mization and generalization are entwined in DNNs. To ac-
count for this, we formalize a notion of the effective capac-
ity (EC) of a learning algorithm A (deﬁned by specifying
both the model and the training procedure, e.g.,“train the
LeNet architecture (LeCun et al., 1998) for 100 epochs us-
ing stochastic gradient descent (SGD) with a learning rate
of 0.01”) as the set of hypotheses which can be reached
by applying that learning algorithm on some dataset. For-
mally, using set-builder notation:

EC(A) = {h | ∃D such that h ∈ A(D)} ,

where A(D) represents the set of hypotheses that is reach-
able by A on a dataset D1.

One might suspect that DNNs effective capacity is sufﬁ-
ciently limited by gradient-based training and early stop-
ping to resolve the apparent paradox between DNNs’ excel-
lent generalization and their high representational capacity.
However, the experiments of Zhang et al. (2017) suggest
that this is not the case. They demonstrate that DNNs are

1 Since A can be stochastic, A(D) is a set.

A Closer Look at Memorization in Deep Networks

able to ﬁt pure noise without even needing substantially
longer training time. Thus even the effective capacity of
DNNs may be too large, from the point of view of tradi-
tional learning theory.

By demonstrating the ability of DNNs to “memorize” ran-
dom noise, Zhang et al. (2017) also raise the question
whether deep networks use similar memorization tactics on
real datasets.
Intuitively, a brute-force memorization ap-
proach to ﬁtting data does not capitalize on patterns shared
between training examples or features; the content of what
is memorized is irrelevant. A paradigmatic example of
a memorization algorithm is k-nearest neighbors (Fix &
Hodges Jr, 1951). Like Zhang et al. (2017), we do not
formally deﬁne memorization; rather, we investigate this
intuitive notion of memorization by training DNNs to ﬁt
random data.

Main Contributions

We operationalize the deﬁnition of “memorization” as the
behavior exhibited by DNNs trained on noise, and conduct
a series of experiments that contrast the learning dynamics
of DNNs on real vs. noise data. Thus, our analysis builds
on the work of Zhang et al. (2017) and further investigates
the role of memorization in DNNs.

Our ﬁndings are summarized as follows:

1. There are qualitative differences in DNN optimization
behavior on real data vs. noise. In other words, DNNs
do not just memorize real data (Section 3).

2. DNNs learn simple patterns ﬁrst, before memorizing
(Section 4).
In other words, DNN optimization is
content-aware, taking advantage of patterns shared by
multiple training examples.

3. Regularization techniques can differentially hinder
memorization in DNNs while preserving their ability
to learn about real data (Section 5).

2. Experiment Details

We perform experiments on MNIST (LeCun et al., 1998)
and CIFAR10 (Krizhevsky et al.) datasets. We investi-
gate two classes of models: 2-layer multi-layer percep-
trons (MLPs) with rectiﬁer linear units (ReLUs) on MNIST
and convolutional neural networks (CNNs) on CIFAR10.
If not stated otherwise, the MLPs have 4096 hidden units
per layer and are trained for 1000 epochs with SGD and
learning rate 0.01. The CNNs are a small Alexnet-style
CNN2 (as in Zhang et al. (2017)), and are trained using

2Input → Crop(2,2) → Conv(200,5,5) → BN → ReLU →
MaxPooling(3,3) → Conv(200,5,5) → BN→ ReLU→ MaxPool-

SGD with momentum=0.9 and learning rate of 0.01, sched-
uled to drop by half every 15 epochs.

Following Zhang et al. (2017), in many of our experiments
we replace either (some portion of) the labels (with random
labels), or the inputs (with i.i.d. Gaussian noise matching
the real dataset’s mean and variance) for some fraction of
the training set. We use randX and randY to denote datasets
with (100%, unless speciﬁed) noisy inputs and labels (re-
spectively).

3. Qualitative Differences of DNNs Trained

on Random vs. Real Data

Zhang et al. (2017) empirically demonstrated that DNNs
are capable of ﬁtting random data, which implicitly neces-
sitates some high degree of memorization. In this section,
we investigate whether DNNs employ similar memoriza-
tion strategy when trained on real data. In particular, our
experiments highlight some qualitative differences between
DNNs trained on real data vs. random data, supporting the
fact that DNNs do not use brute-force memorization to ﬁt
real datasets.

3.1. Easy Examples as Evidence of Patterns in Real

Data

A brute-force memorization approach to ﬁtting data should
apply equally well to different training examples. How-
ever, if a network is learning based on patterns in the data,
some examples may ﬁt these patterns better than others. We
show that such “easy examples” (as well as correspond-
ingly “hard examples”) are common in real, but not in
random, datasets. Speciﬁcally, for each setting (real data,
randX, randY), we train an MLP for a single epoch start-
ing from 100 different random initializations and shufﬂings
of the data. We ﬁnd that, for real data, many examples
are consistently classiﬁed (in)correctly after a single epoch,
suggesting that different examples are signiﬁcantly easier
or harder in this sense. For noise data, the difference be-
tween examples is much less, indicating that these exam-
ples are ﬁt (more) independently. Results are presented in
Figure 1.

For randX, apparent differences in difﬁculty are well mod-
eled as random Binomial noise. For randY, this is not the
case, indicating some use of shared patterns. Visualizing
ﬁrst-level features learned by a CNN supports this hypoth-
esis (Figure 2).

ing(3,3) → Dense(384) → BN → ReLU → Dense(192) → BN
→ ReLU → Dense(#classes) → Softmax. Here Crop(. , .) crops
height and width from both sides with respective values.

A Closer Look at Memorization in Deep Networks

We ﬁnd that for real data, only a subset of the training set
has high ¯gx, while for random data, ¯gx is high for virtually
all examples. We also ﬁnd a different behavior when each
example is given a unique class; in this scenario, the net-
work has to learn to identify each example uniquely, yet
still behaves differently when given real data than when
given random data as input.

We visualize (Figure 3) the spread of ¯gx as training pro-
gresses by computing the Gini coefﬁcient over x’s. The
Gini coefﬁcient (Gini, 1913) is a measure of the inequality
among values of a frequency distribution; a coefﬁcient of
0 means exact equality (i.e., all values are the same), while
a coefﬁcient of 1 means maximal inequality among values.
We observe that, when trained on real data, the network has
a high ¯gx for a few examples, while on random data the net-
work is sensitive to most examples. The difference between
the random data scenario, where we know the neural net-
work needs to do memorization, and the real data scenario,
where we’re trying to understand what happens, leads us to
believe that this measure is indeed sensitive to memoriza-
tion. Additionally, these results suggest that when being
trained on real data, the neural network probably does not
memorize, or at least not in the same manner it needs to for
random data.

1/T (cid:80)T

In addition to the different behaviors for real and random
data described above, we also consider a class speciﬁc loss-
sensitivity: ¯gi,j = E(x,y)
t |∂Lt(y = i)/∂xy=j|,
where Lt(y = i) is the term in the crossentropy sum cor-
responding to class i. We observe that the loss-sensitivity
w.r.t. class i for training examples of class j is higher when
i = j, but more spread out for real data (see Figure 4).
An interpretation of this is that for real data there are more
interesting cross-category patterns that can be learned than
for random data.

Figure 3 and 4 were obtained by training a fully-connected
network with 2 layers of 16 units on 1000 downscaled 14 ×
14 MNIST digits using SGD.

3.3. Capacity and Effective Capacity

In this section, we investigate the impact of capacity and
effective capacity on learning of datasets having different
amounts of random input data or random labels.

3.3.1. EFFECTS OF CAPACITY AND DATASET SIZE ON

VALIDATION PERFORMANCES

In a ﬁrst experiment, we study how overall model capac-
ity impacts the validation performances for datasets with
different amounts of noise. On MNIST, we found that the
optimal validation performance requires a higher capacity
model in the presence of noise examples (see Figure 5).
This trend was consistent for noise inputs on CIFAR10, but

Figure 1. Average (over 100 experiments) misclassiﬁcation rate
for each of 1000 examples after one epoch of training. This mea-
sure of an example’s difﬁculty is much more variable in real data.
We conjecture this is because the easier examples are explained
by some simple patterns, which are reliably learned within the
ﬁrst epoch of training. We include 1000 points samples from a
binomial distribution with n = 100 and p equal to the average
estimated P(correct) for randX, and note that this curve closely
resembles the randX curve, suggesting that random inputs are all
equally difﬁcult.

Figure 2. Filters from ﬁrst layer of network trained on CIFAR10
(left) and randY (right).

3.2. Loss-Sensitivity in Real vs. Random Data

To further investigate the difference between real and fully
random inputs, we propose a proxy measure of memoriza-
tion via gradients. Since we cannot measure quantitatively
how much each training sample x is memorized, we instead
measure the effect of each sample on the average loss. That
is, we measure the norm of the loss gradient with respect
to a previous example x after t SGD updates. Let Lt be the
loss after t updates; then the sensitivity measure is given by

gt
x = (cid:107)∂Lt/∂x(cid:107)1 .

The parameter update from training on x inﬂuences all fu-
ture Lt indirectly by changing the subsequent updates on
different training examples. We denote the average over gt
x
after T steps as ¯gx, and refer to it as loss-sensitivity. Note
that we only report (cid:96)1-norm results, but that results stay
very similar using (cid:96)2-norm and inﬁnity norm.

We compute gt
x by unrolling t SGD steps and applying
backpropagation over the unrolled computation graph, as
done by Maclaurin et al. (2015). Unlike Maclaurin et al.
(2015), we only use this procedure to compute gt
x, and do
not modify the training procedure in any way.

A Closer Look at Memorization in Deep Networks

Figure 3. Plots of the Gini coefﬁcient of ¯gx over examples x (see section 3.2) as training progresses, for a 1000-example real dataset
(14x14 MNIST) versus random data. On the left, Y is the normal class label; on the right, there are as many classes as examples, the
network has to learn to map each example to a unique class.

Figure 4. Plots of per-class gx (see previous ﬁgure; log scale), a
cell i, j represents the average |∂L(y = i)/∂xy=j|, i.e. the loss-
sensitivity of examples of class i w.r.t. training examples of class
j. Left is real data, right is random data.

we did not notice any relationship between capacity and
validation performance on random labels on CIFAR10.

This result contradicts the intuitions of traditional learning
theory, which suggest that capacity should be restricted, in
order to enforce the learning of (only) the most regular pat-
terns. Given that DNNs can perfectly ﬁt the training set in
any case, we hypothesize that that higher capacity allows
the network to ﬁt the noise examples in a way that does
not interfere with learning the real data. In contrast, if we
were simply to remove noise examples, yielding a smaller
(clean) dataset, a lower capacity model would be able to
achieve optimal performance.

3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON

TRAINING TIME

Our next experiment measures time-to-convergence, i.e.
how many epochs it takes to reach 100% training accu-
racy. Reducing the capacity or increasing the size of the
dataset slows down training as well for real as for noise

Figure 5. Performance as a function of capacity in 2-layer MLPs
trained on (noisy versions of) MNIST. For real data, performance
is already very close to maximal with 4096 hidden units, but when
there is noise in the dataset, higher capacity is needed.

data3. However, the effect is more severe for datasets con-
taining noise, as our experiments in this section show (see
Figure 6).

Effective capacity of a DNN can be increased by increas-
ing the representational capacity (e.g. adding more hidden
units) or training for longer. Thus, increasing the num-
ber of hidden units decreases the number of training iter-
ations needed to ﬁt the data, up to some limit. We ob-
serve stronger diminishing returns from increasing repre-
sentational capacity for real data, indicating that this limit
is lower, and a smaller representational capacity is sufﬁ-
cient, for real datasets.

Increasing the number of examples (keeping representa-
tional capacity ﬁxed) also increases the time needed to
memorize the training set.
In the limit, the representa-
tional capacity is simply insufﬁcient, and memorization is
not feasible. On the other hand, when the relationship be-
tween inputs and outputs is meaningful, new examples sim-

3 Regularization can also increase time-to-convergence; see

section 5.

103104105numberofSGDsteps(logscale)0.10.20.30.40.50.60.7Ginicoeﬃcientof¯gxdistributionrealdata50%realdatarandomdata103104numberofSGDsteps(logscale)0.160.180.200.220.24Ginicoeﬃcientof¯gxdistributionrealdatarandomdata0246802468024680246810−710−6A Closer Look at Memorization in Deep Networks

Figure 6. Time to convergence as a function of capacity with dataset size ﬁxed to 50000 (left), or dataset size with capacity ﬁxed to 4096
units (right). “Noise level” denotes to the proportion of training points whose inputs are replaced by Gaussian noise. Because of the
patterns underlying real data, having more capacity/data does not decrease/increase training time as much as it does for noise data.

ply give more (possibly redundant) clues as to what the in-
put → output mapping is. Thus, in the limit, an idealized
learner should be able to predict unseen examples perfectly,
absent noise. Our experiments demonstrate that time-to-
convergence is not only longer on noise data (as noted by
Zhang et al. (2017)), but also, increases substantially as a
function of dataset size, relative to real data. Following the
reasoning above, this suggests that our networks are learn-
ing to extract patterns in the data, rather than memorizing.

4. DNNs Learn Patterns First

This section aims at studying how the complexity of the hy-
potheses learned by DNNs evolve during training for real
data vs. noise data. To achieve this goal, we build on the
intuition that the number of different decision regions into
which an input space is partitioned reﬂects the complexity
of the learned hypothesis (Sokolic et al., 2016). This notion
is similar in spirit to the degree to which a function can scat-
ter random labels: a higher density of decision boundaries
in the data space allows more samples to be scattered.

Therefore, we estimate the complexity by measuring how
densely points on the data manifold are present around the
model’s decision boundaries. Intuitively, if we were to ran-
domly sample points from the data distribution, a smaller
fraction of points in the proximity of a decision boundary
suggests that the learned hypothesis is simpler.

4.1. Critical Sample Ratio (CSR)

Here we introduce the notion of a critical sample, which
we use to estimate the density of decision boundaries as
discussed above. Critical samples are a subset of a dataset
such that for each such sample x, there exists at least one
adversarial example ˆx in the proximity of x. Speciﬁcally,
consider a classiﬁcation network’s output vector f (x) =

(f1(x), . . . , fk(x)) ∈ Rk for a given input sample x ∈ Rn
from the data manifold. Formally we call a dataset sample
x a critical sample if there exists a point ˆx such that,

arg max

fi(x) (cid:54)= arg max

fj(ˆx)

i

j

(1)

s.t. (cid:107)x − ˆx(cid:107)∞ ≤ r

where r is a ﬁxed box size. As in recent work on adver-
sarial examples (Kurakin et al., 2016) the above deﬁnition
depends only on the predicted label arg maxi fi(x) of x,
and not the true label (as in earlier work on adversarial ex-
amples, such as Szegedy et al. (2013); Goodfellow et al.
(2014)).

Following the above argument relating complexity to deci-
sion boundaries, a higher number of critical samples indi-
cates a more complex hypothesis. Thus, we measure com-
plexity as the critical sample ratio (CSR), that is, the frac-
tion of data-points in a set |D| for which we can ﬁnd a crit-
ical sample: #critical samples

.

|D|

To identify whether a given data point x is a critical sam-
ples, we search for an adversarial sample ˆx within a box
of radius r. To perform this search, we propose using
Langevin dynamics applied to the fast gradient sign method
(FGSM, Goodfellow et al. (2014)) as shown in algorithm
14. We refer to this method as Langevin adversarial sample
search (LASS). While the FGSM search algorithm can get
stuck at a points with zero gradient, LASS explores the box
more thoroughly. Speciﬁcally, a problem with ﬁrst order
gradient search methods (like FGSM) is that there might
exist training points where the gradient is 0, but with a large
2nd derivative corresponding to a large change in prediction
in the neighborhood. The noise added by the LASS algo-
rithm during the search enables escaping from such points.

4In our experiments, we set α = 0.25, β = 0.2 and η is

samples from standard normal distribution.

A Closer Look at Memorization in Deep Networks

(a) Noise added on classiﬁcation inputs.

(b) Noise added on classiﬁcation labels.

Figure 7. Accuracy (left in each pair, solid is train, dotted is validation) and Critical sample ratios (right in each pair) for MNIST.

(a) Noise added on classiﬁcation inputs.

(b) Noise added on classiﬁcation labels.

Figure 8. Accuracy (left in each pair, solid is train, dotted is validation) and Critical sample ratios (right in each pair) for CIFAR10.

Algorithm 1 Langevin Adversarial Sample Search (LASS)
Require: x ∈ Rn, α, β, r, noise process η
Ensure: ˆx
1: converged = FALSE
2: ˜x ← x; ˆx ← ∅
3: while not converged or max iter reached do
4: ∆ = α · sign( ∂fk(x)
˜x ← ˜x + ∆
5:
for i ∈ [n] do
6:

∂x ) + β · η

(cid:26) xi + r · sign(˜xi − xi)

˜xi

if |˜xi − xi| > r
otherwise

end for
if arg maxi f (x) (cid:54)= arg maxi f (˜x) then

7:

˜xi ←

8:
9:
10:
11:
12:
13: end while

end if

converged = TRUE
ˆx ← ˜x

4.2. Critical Samples Throughout Training

We now show that the number of critical samples is much
higher for a deep network (speciﬁcally, a CNN) trained on
noise data compared with real data. To do so, we mea-

Figure 9. Critical sample ratio throughout training on CIFAR-10,
random input (randX), and random label (randY) datasets.

sure the number of critical samples in the validation set5,
throughout training6. Results are shown in Figure 9. A

5 We also measure the number of critical samples in the train-
ing sets. Since we train our models using log loss, training points
are pushed away from the decision boundary even after the net-
work learns to classify them correctly. This leads to an initial rise
and then fall of the number of critical samples in the training sets.
6We use a box size of 0.3, which is small enough in a 0-255
pixel scale to be unnoticeable by a human evaluator. Different
values for r were tested but did not change results qualitatively

A Closer Look at Memorization in Deep Networks

higher number of critical samples for models trained on
noise data compared with those trained on real data sug-
gests that the learned decision surface is more complex for
noise data (randX and randY). We also observe that the
CSR increases gradually with increasing number of epochs
and then stabilizes. This suggests that the networks learn
gradually more complex hypotheses during training for all
three datasets.

In our next experiment, we evaluate the performance and
critical sample ratio of datasets with 20% to 80% of the
training data replaced with either input or label noise. Re-
sults for MNIST and CIFAR-10 are shown in Figures 7
and 8, respectively. For both randX and randY datasets,
the CSR is higher for noisier datasets, reﬂecting the higher
level of complexity of the learned prediction function. The
ﬁnal and maximum validation accuracies are also both
lower for noisier datasets, indicating that the noise exam-
ples interfere somewhat with the networks ability to learn
about the real data.

More signiﬁcantly, for randY datasets (Figures 7(b) and
8(b)), the network achieves maximum accuracy on the val-
idation set before achieving high accuracy on the training
set. Thus the model ﬁrst learns the simple and general pat-
terns of the real data before ﬁtting the noise (which re-
sults in decreasing validation accuracy). Furthermore, as
the model moves from ﬁtting real data to ﬁtting noise, the
CSR greatly increases, indicating the need for more com-
plex hypotheses to explain the noise. Combining this result
with our results from Section 3.1, we conclude that real
data examples are easier to ﬁt than noise.

5. Effect of Regularization on Learning

Here we demonstrate the ability of regularization to de-
grade training performance on data with random labels,
while maintaining generalization performance on real data.
Zhang et al. (2017) argue that explicit regularizations are
not the main explanation of good generalization perfor-
mance, rather SGD based optimization is largely responsi-
ble for it. Our ﬁndings extend their claim and indicate that
explicit regularizations can substantially limit the speed of
memorization of noise data without signiﬁcantly impacting
learning on real data.

We compare the performance of CNNs trained on CIFAR-
10 and randY with the following regularizers: dropout
(with dropout rates in range 0-0.9), input dropout (range 0-
0.9), input Gaussian noise (with standard deviation in range
0-5), hidden Gaussian noise (range 0-0.3), weight decay
(range 0-1) and additionally dropout with adversarial train-
ing (with weighting factor in range 0.2-0.7 and dropout in

and lead to the same conclusions

Figure 10. Effect of different regularizers on train accuracy (on
noise dataset) vs. validation accuracy (on real dataset). Flatter
curves indicate that memorization (on noise) can be capped with-
out sacriﬁcing generalization (on real data).

rate range 0.03-0.5).7 We train a separate model for every
combination of dataset, regularization technique, and regu-
larization parameter.

The results are summarized in Figure 10. For each com-
bination of dataset and regularization technique, the ﬁnal
training accuracy on randY (x-axis) is plotted against the
best validation accuracy on CIFAR-10 from amongst the
models trained with different regularization parameters (y-
axis). Flat curves indicate that the corresponding regular-
ization technique can reduce memorization when applied
on random labeling, while resulting in the same valida-
tion accuracy on the clean validation set. Our results show
that different regularizers target memorization behavior to
different extent – dropout being the most effective. We
ﬁnd that dropout, especially coupled with adversarial train-
ing, is best at hindering memorization without reducing the
model’s ability to learn. Figure 11 additionally shows this
effect for selected experiments (i.e. selected hyperparame-
ter values) in terms of train loss.

6. Related Work

Our work builds on the experiments and challenges the in-
terpretations of Zhang et al. (2017). We make heavy use
of their methodology of studying DNN training in the con-
text of noise datasets. Zhang et al. (2017) show that DNNs
can perfectly ﬁt noise and thus that their generalization
ability cannot be explained through traditional statistical
learning theory (e.g., see (Vapnik & Vapnik, 1998; Bartlett
et al., 2005)). We agree with this ﬁnding, but show in ad-
dition that the degree of memorization and generalization
in DNNs depends not only on the architecture and training

7We perform adversarial training using critical samples found

by LASS algorithm with default parameters.

A Closer Look at Memorization in Deep Networks

the effect of noise samples on learning dynamics has a long
tradition (Bishop, 1995; An, 1996), we are the ﬁrst to ex-
amine relationships between the fraction of noise samples
and other attributes of the learning algorithm, namely: ca-
pacity, training time and dataset size.

Multiple techniques for analyzing the training of DNNs
have been proposed before, including looking at gener-
alization error, trajectory length evolution (Raghu et al.,
2016), analyzing Jacobians associated to different lay-
ers (Wang; Saxe et al., 2013), or the shape of the loss min-
ima found by SGD (Im et al., 2016; Chaudhari et al., 2016;
Keskar et al., 2016). Instead of measuring the sharpness
of the loss for the learned hypothesis, we investigate the
complexity of the learned hypothesis throughout training
and across different datasets and regularizers, as measured
by the critical sample ratio. Critical samples refer to real
data-points that have adversarial examples (Szegedy et al.,
2013; Goodfellow et al., 2014) nearby. Adversarial ex-
amples originally referred to imperceptibly perturbed data-
points that are conﬁdently misclassiﬁed.
(Miyato et al.,
2015) deﬁne virtual adversarial examples via changes in
the predictive distribution instead, thus extending the deﬁ-
nition to unlabeled data-points. Kurakin et al. (2016) rec-
ommend using this deﬁnition when training on adversarial
examples, and it is the deﬁnition we use.

Two contemporary works perform in-depth explorations of
topics related to our work. Bojanowski & Joulin (2017)
show that predicting random noise targets can yield state
of the art results in unsupervised learning, corroborating
our ﬁndings in Section 3.1, especially Figure 2. Koh &
Liang (2017) use inﬂuence functions to measure the impact
on parameter changes during training, as in our Section 3.2.
They explore several promising applications for this tech-
nique, including generation of adversarial training exam-
ples.

7. Conclusion

Our empirical exploration demonstrates qualitative differ-
ences in DNN optimization on noise vs. real data, all of
which support the claim that DNNs trained with SGD-
variants ﬁrst use patterns, not brute force memorization, to
ﬁt real data. However, since DNNs have the demonstrated
ability to ﬁt noise, it is unclear why they ﬁnd generaliz-
able solutions on real data; we believe that the deep learn-
ing priors including distributed and hierarchical represen-
tations likely play an important role. Our analysis suggests
that memorization and generalization in DNNs depend on
network architecture and optimization procedure, but also
on the data itself. We hope to encourage future research on
how properties of datasets inﬂuence the behavior of deep
learning algorithms, and suggest a data-dependent under-
standing of DNN capacity as a research goal.

Figure 11. Training curves for different regularization techniques
on random label (left) and real (right) data. The vertical ordering
of the curves is different for random labels than for real data, in-
dicating differences in the propensity of different regularizers to
slow-down memorization.

procedure (including explicit regularizations), but also on
the training data itself 8.

Another direction we investigate is the relationship be-
tween regularization and memorization.
Zhang et al.
(2017) argue that explicit and implicit regularizers (includ-
ing SGD) might not explain or limit shattering of random
data.
In this work we show that regularizers (especially
dropout) do control the speed at which DNNs memorize.
This is interesting since dropout is also known to prevent
catastrophic forgetting (Goodfellow et al., 2013) and thus
in general it seems to help DNNs retain patterns.

A number of arguments support the idea that SGD-based
learning imparts a regularization effect, especially with a
small batch size (Wilson & Martinez, 2003) or a small
number of epochs (Hardt et al., 2015). Previous work also
suggests that SGD prioritizes the learning of simple hy-
pothesis ﬁrst. Sjoberg et al. (1995) showed that, for linear
models, SGD ﬁrst learns models with small (cid:96)2 parameter
norm. More generally, the efﬁcacy of early stopping shows
that SGD ﬁrst learns simpler models (Yao et al., 2007). We
extend these results, showing that DNNs trained with SGD
learn patterns before memorizing, even in the presence of
noise examples.

Various previous works have analyzed explanations for the
generalization power of DNNs. Montavon et al. (2011) use
kernel methods to analyze the complexity of deep learn-
ing architectures, and ﬁnd that network priors (e.g. imple-
mented by the network structure of a CNN or MLP) con-
trol the speed of learning at each layer. Neyshabur et al.
(2014) note that the number of parameters does not con-
trol the effective capacity of a DNN, and that the reason
for DNNs’ generalization is unknown. We supplement this
result by showing how the impact of representational ca-
pacity changes with varying noise levels. While exploring

8We conclude the latter part based on experimental ﬁndings in

sections 3 and 4.2

A Closer Look at Memorization in Deep Networks

ACKNOWLEDGMENTS

We thank Akram Erraqabi, Jason Jo and Ian Goodfellow
for helpful discussions. SJ was supported by Grant No. DI
2014/016644 from Ministry of Science and Higher Edu-
cation, Poland. DA was supported by IVADO, CIFAR and
NSERC. EB was ﬁnancially supported by the Samsung Ad-
vanced Institute of Technology (SAIT). MSK and SJ were
supported by MILA during the course of this work. We
acknowledge the computing resources provided by Com-
puteCanada and CalculQuebec. Experiments were carried
out using Theano (Theano Development Team, 2016) and
Keras (Chollet et al., 2015).

References

Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron.
Deep Learning. MIT Press, 2016. http://www.
deeplearningbook.org.

Goodfellow, Ian J, Mirza, Mehdi, Xiao, Da, Courville,
Aaron, and Bengio, Yoshua. An empirical investigation
of catastrophic forgetting in gradient-based neural net-
works. arXiv preprint arXiv:1312.6211, 2013.

Goodfellow, Ian J, Shlens, Jonathon, and Szegedy, Chris-
tian. Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572, 2014.

Hardt, Moritz, Recht, Benjamin, and Singer, Yoram. Train
faster, generalize better: Stability of stochastic gradient
descent. arXiv preprint arXiv:1509.01240, 2015.

An, Guozhong. The effects of adding noise during back-
propagation training on a generalization performance.
Neural computation, 8(3):643–674, 1996.

Hornik, Kurt, Stinchcombe, Maxwell, and White, Halbert.
Multilayer feedforward networks are universal approxi-
mators. Neural networks, 2(5):359–366, 1989.

Bartlett, Peter L, Bousquet, Olivier, Mendelson, Shahar,
et al. Local rademacher complexities. The Annals of
Statistics, 33(4):1497–1537, 2005.

Im, Daniel Jiwoong, Tao, Michael, and Branson, Kristin.
An empirical analysis of deep network loss surfaces.
arXiv preprint arXiv:1612.04010, 2016.

Bengio, Yoshua et al. Learning deep architectures for ai.
Foundations and trends® in Machine Learning, 2(1):1–
127, 2009.

Bishop, Chris M. Training with noise is equivalent to
tikhonov regularization. Neural computation, 7(1):108–
116, 1995.

Bojanowski, P. and Joulin, A. Unsupervised Learning by

Predicting Noise. ArXiv e-prints, April 2017.

Bottou, Léon. Online learning and stochastic approxima-
tions. On-line learning in neural networks, 17(9):142,
1998.

Chaudhari, Pratik, Choromanska, Anna, Soatto, Ste-
Entropy-sgd: Biasing
arXiv preprint

fano, and LeCun, Yann.
gradient descent
arXiv:1611.01838, 2016.

into wide valleys.

Chollet, François et al. Keras. https://github.com/

fchollet/keras, 2015.

Cybenko, George. Approximation by superpositions of a
sigmoidal function. Mathematics of Control, Signals,
and Systems (MCSS), 2(4):303–314, 1989.

Fix, Evelyn and Hodges Jr, Joseph L.

Discrimina-
tory analysis-nonparametric discrimination: consistency
properties. Technical report, DTIC Document, 1951.

Gini, Corrado. Variabilita e mutabilita.
Royal Statistical Society, 76(3), 1913.

Journal of the

Keskar, Nitish Shirish, Mudigere, Dheevatsa, Nocedal,
Jorge, Smelyanskiy, Mikhail, and Tang, Ping Tak Pe-
ter. On large-batch training for deep learning: Gen-
arXiv preprint
eralization gap and sharp minima.
arXiv:1609.04836, 2016.

Koh, Pang Wei and Liang, Percy. Understanding black-
box predictions via inﬂuence functions. arXiv preprint
arXiv:1703.04730, 2017.

Krizhevsky, Alex, Nair, Vinod, and Hinton, Geof-
frey.
Cifar-10 (canadian institute for advanced re-
search). URL http://www.cs.toronto.edu/
~kriz/cifar.html.

Kurakin, Alexey, Goodfellow, Ian, and Bengio, Samy. Ad-
versarial examples in the physical world. arXiv preprint
arXiv:1607.02533, 2016.

LeCun, Yann, Cortes, Corinna, and Burges, Christo-
pher JC. The mnist database of handwritten digits, 1998.

Lin, Henry W and Tegmark, Max. Why does deep
arXiv preprint

and cheap learning work so well?
arXiv:1608.08225, 2016.

Maclaurin, Dougal, Duvenaud, David K, and Adams,
Ryan P. Gradient-based hyperparameter optimization
In ICML, pp. 2113–2122,
through reversible learning.
2015.

Miyato, Takeru, Maeda, Shin-ichi, Koyama, Masanori,
Nakae, Ken, and Ishii, Shin. Distributional smoothing
with virtual adversarial training. stat, 1050:25, 2015.

A Closer Look at Memorization in Deep Networks

Wang, Shengjie. Analysis of deep neural networks with the

extended data jacobian matrix.

Wilson, D Randall and Martinez, Tony R. The general in-
efﬁciency of batch training for gradient descent learning.
Neural Networks, 16(10):1429–1451, 2003.

Yao, Yuan, Rosasco, Lorenzo, and Caponnetto, Andrea. On
early stopping in gradient descent learning. Constructive
Approximation, 26(2):289–315, 2007.

Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Recht, Ben-
jamin, and Vinyals, Oriol. Understanding deep learning
requires rethinking generalization. International Confer-
ence on Learning Representations (ICLR), 2017.

Montavon, Grégoire, Braun, Mikio L., and Müller, Klaus-
Robert. Kernel analysis of deep networks. Journal of
Machine Learning Research, 12, 2011.

Montufar, Guido F, Pascanu, Razvan, Cho, Kyunghyun,
and Bengio, Yoshua. On the number of linear regions
of deep neural networks.
In Ghahramani, Z., Welling,
M., Cortes, C., Lawrence, N. D., and Weinberger, K. Q.
(eds.), Advances in Neural Information Processing Sys-
tems 27, pp. 2924–2932. Curran Associates, Inc., 2014.

Neyshabur, Behnam, Tomioka, Ryota, and Srebro, Nathan.
In search of the real inductive bias: On the role of im-
plicit regularization in deep learning. arXiv preprint
arXiv:1412.6614, 2014.

Poole, Ben, Lahiri, Subhaneil, Raghu, Maithreyi, Sohl-
Dickstein, Jascha, and Ganguli, Surya. Exponential
expressivity in deep neural networks through transient
chaos.
In Lee, D. D., Sugiyama, M., Luxburg, U. V.,
Guyon, I., and Garnett, R. (eds.), Advances in Neural In-
formation Processing Systems 29, pp. 3360–3368. Cur-
ran Associates, Inc., 2016.

Raghu, Maithra, Poole, Ben, Kleinberg, Jon, Ganguli,
Surya, and Sohl-Dickstein, Jascha. On the expres-
arXiv preprint
sive power of deep neural networks.
arXiv:1606.05336, 2016.

Saxe, Andrew M, McClelland, James L, and Ganguli,
Surya. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks. arXiv preprint
arXiv:1312.6120, 2013.

Sjoberg, J., Sjoeberg, J., Sjöberg, J., and Ljung, L. Over-
training, regularization and searching for a minimum,
with application to neural networks. International Jour-
nal of Control, 62:1391–1407, 1995.

Sokolic, Jure, Giryes, Raja, Sapiro, Guillermo, and Ro-
drigues, Miguel RD. Robust large margin deep neural
networks. arXiv preprint arXiv:1605.08254, 2016.

Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya,
Bruna, Joan, Erhan, Dumitru, Goodfellow, Ian J., and
Fergus, Rob.
Intriguing properties of neural networks.
CoRR, abs/1312.6199, 2013. URL http://arxiv.
org/abs/1312.6199.

Theano Development Team, and others. Theano: A Python
framework for fast computation of mathematical expres-
sions. arXiv e-prints, abs/1605.02688, May 2016.

Vapnik, Vladimir Naumovich and Vapnik, Vlamimir. Sta-
tistical learning theory, volume 1. Wiley New York,
1998.

