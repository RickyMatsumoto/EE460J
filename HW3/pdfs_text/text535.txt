Online Learning with Local Permutations and Delayed Feedback

A. Proofs

A.1. Analysis Of The Delayed Permuted Mirror Descent Algorithm

We will use throughout the proofs the well known Pythagorean Theorem for Bregman divergences, and the ’projection’
lemma that considers the projection step in the algorithm.
Lemma 1. Pythagorean Theorem for Bregman divergences
Let v be the projection of w onto a convex set W w.r.t Bregman divergence (cid:52)ψ: v = argminu∈W (cid:52)ψ (u, w), then:
(cid:52)ψ (u, w) ≥ (cid:52)ψ (u, v) + (cid:52)ψ (v, w)
Lemma 2. Projection Lemma
Let W be a closed convex set and let v be the projection of w onto W, namely,
v = argmin

(cid:107)x − w(cid:107)2. Then, for every u ∈ W, (cid:107)w − u(cid:107)2 − (cid:107)v − u(cid:107)2 ≥ 0

The following lemma gives a bound on the distance between two consequent predictions when using the Euclidean mirror
map:
Lemma 3. Let g ∈ Rn s.t. (cid:107)g(cid:107)2 < G, W a convex set, and η > 0 be ﬁxed. Let w ∈ W and w2 = w − η · g. Then, for
w(cid:48) = argmin

2, we have that (cid:107)w − w(cid:48)(cid:107) ≤ η · G

(cid:107)w2 − u(cid:107)2

x∈W

u∈W

Proof. From the projection lemma: (cid:107)w2 − w(cid:107)2
(cid:107)w2 − w(cid:107)2 = (cid:107)η · g(cid:107)2 ≤ η · G. and so we get: (cid:107)w(cid:48) − w(cid:107)2 ≤ (cid:107)w2 − w(cid:107)2 ≤ η · G

2 ≥ (cid:107)w(cid:48) − w(cid:107)2

2 and so: (cid:107)w2 − w(cid:107)2 ≥ (cid:107)w(cid:48) − w(cid:107)2. From deﬁnition:

We prove a modiﬁcation of Lemma 2 given in (Menache et al., 2014) in order to bound the distance between two consequent
predictions when using the negative entropy mirror map:
Lemma 4. Let g ∈ Rn s.t. (cid:107)g(cid:107)1 ≤ G for some G > 0 and let η > 0 be ﬁxed, with η < 1√
w in the n − simplex, if we deﬁne w(cid:48) to be the new distribution vector

. For any distribution vector

2·G

∀i ∈ {1, ..., n} , w(cid:48)

i =

wi · exp (−η · gi)
j=1 wj · exp (−η · gj)

(cid:80)n

Then (cid:107)w − w(cid:48)(cid:107)1 ≤ 3ηG

Proof. Since (cid:107)g(cid:107)∞ < G and η < 1√

we get that ∀i : |η · gi| < 1. We have that:

(cid:107)w − w(cid:48)(cid:107)1 =

|wi − w(cid:48)

i| =

wi ·

1 −

(cid:32)

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

exp (−η · gi)
j=1 wj · exp (−η · gj)

(cid:80)n

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2·G

n
(cid:88)

i=1

Since (cid:107)w(cid:107)1 = 1, we can apply Holder’s inequality, and upper bound the above by

Using the inequality 1 − x ≤ exp (−x) ≤ 1

1+x for all |x| ≤ 1, we know that

max
i

1 −

exp (−η · gi)
j=1 wj · exp (−η · gj)

(cid:80)n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 − η · gi ≤ exp (−η · gi) ≤

1
1 + η · gi

and since −ηG ≤ η · gi ≤ ηG we have that

and so we get:

1 − η · gi ≤ exp (−η · gi) ≤

1
1 + η · gi

≤

1
1 − ηG

1 −

1
1+ηgi
1 + ηG

≤ 1 −

exp (−η · gi)
j=1 wj · exp (−η · gj)

(cid:80)n

≤ 1 −

1 − η · gi
1
1−ηG

Online Learning with Local Permutations and Delayed Feedback

Using again the fact that −ηG ≤ η · gi ≤ ηG, we have

1 −

1
1−ηG
1 + ηG

≤ 1 −

exp (−η · gi)
j=1 wj · exp (−η · gj)

(cid:80)n

≤ 1 −

1 − ηG
1
1−ηG

=⇒

−η2G2
1 − η2G2 = 1 −

1

1 − η2G2 ≤ 1 −

exp (−η · gi)
j=1 wj · exp (−η · gj)

(cid:80)n

≤ 1 − (1 − ηG)2 = 2ηG + η2G2

Now, since ηG < 1, we get that:

and so we can conclude that

−η2G2
1 − η2G2 ≤ 1 −

exp (−η · gi)
j=1 wj · exp (−η · gj)

(cid:80)n

≤ 2ηG + ηG = 3ηG

max
i

1 −

exp (−η · gi)
j=1 wj · exp (−η · gj)

(cid:80)n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ max

i

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

−η2G2
1 − η2G2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

, |3ηG|

≤ max

(cid:18) ηG

(cid:19)
1 − η2G2 , 3ηG

i

Since η < 1√

, we get (η · G)2 < 1

2G

2 . Thus we get:

max
i

1 −

exp (−η · gi)
j=1 wj · exp (−η · gj)

(cid:80)n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ max

(2ηG, 3ηG) ≤ 3ηG

i

which gives us our desired bound.

With the above two lemmas in hand, we bound the distance between consequent predictors by cηG, where c is a different
constant in each mirror map: c = 1 for the euclidean case, and c = 3 for the negative entropy mirror map.
Note that both mapping are 1-strongly convex with respect to their respective norms. For other mappings with a different
strong convexity constant, one would need to scale the step sizes according to the strong convexity parameter in order to
get the bound.

A.1.1. PROOF OF THEOREM 1

We provide an upper bound on the regret of the algorithm, by competing against the best ﬁxed action in each one of the
sets of iterations- the ﬁrst τ iterations and the last M − τ iterations in each block. This is an upper bound on competing
against the best ﬁxed predictor in hindsight for the entire sequence. Formally, we bound:

(cid:34) T

(cid:88)

R(T ) = E

ft (wt) −

ft (w∗)

(cid:35)

T
(cid:88)

t=1

t=1

T
M −1
(cid:88)









≤ E

M ·i+τ
(cid:88)

ft (wt) − ft

(cid:0)w∗

f

(cid:1) +

ft (wt) − ft (w∗
s )





M ·(i+1)
(cid:88)





i=0

t=M ·i+1

t=M ·i+τ +1

where

w∗

f = argmin
w∈W

T
M −1
(cid:88)

M ·i+τ
(cid:88)

i=0

t=M ·i+1

ft (w) and w∗

s = argmin
w∈W

T
M −1
(cid:88)

M ·(i+1)
(cid:88)

i=0

t=M ·i+τ +1

ft (w)

where expectation is taken over the randomness of the algorithm.
(cid:16)

(cid:17)

The diameter of the domain W is bounded by B2, and so (cid:52)ψ
0) ≤ B2. We start with a
general derivation that will apply both for ws and for wf simultaneously. For the following derivation we use the notation
wj, wj+1 omitting the f, s superscript, for denoting subsequent updates of the predictor vector, whether it is ws or wf .

≤ B2 and (cid:52)ψ (w∗

s , ws

0

f , wf
w∗

Online Learning with Local Permutations and Delayed Feedback

Denote by gj the gradient used to update wj, i.e., ∇ψ

Looking at the update step in the algorithm, we have that gj = 1
η ·

(cid:17)

(cid:16)

wj+ 1

2

= ∇ψ (wj) − η · gj, and wj+1 = argmin
(cid:16)

(cid:17)(cid:17)

(cid:16)

w∈W
and thus:

∇ψ (wj) − ∇ψ

wj+ 1

2

(cid:16)

(cid:52)ψ

w, wj+ 1

2

(cid:17)

.

(cid:104)wj − w∗, gj(cid:105) =

(cid:68)
wj − w∗,

(cid:16)

∇ψ (wj) − ∇ψ

(cid:17)(cid:17)(cid:69)

(cid:16)

wj+ 1

2

=

(cid:52)ψ (w∗, wj) + (cid:52)ψ

(cid:16)

(cid:17)

wj, wj+ 1

2

− (cid:52)ψ

(cid:16)
w∗, wj+ 1

2

(cid:17)(cid:17)

1
η
1
η

·

·

(cid:16)

We now use the Pythagorean Theorem to get:

≤

(cid:16)

·

1
η

(cid:52)ψ (w∗, wj) + (cid:52)ψ

− (cid:52)ψ (w∗, wj+1) − (cid:52)ψ

(cid:16)

wj, wj+ 1

2

(cid:17)

(cid:16)

wj+1, wj+ 1

2

(cid:17)(cid:17)

When we sum terms for all updates of the predictor, wf or ws respectively, the terms (cid:52)ψ (w∗, wj) − (cid:52)ψ (w∗, wj+1) will
result in a telescopic sum, canceling all terms expect the ﬁrst and last. Thus we now concentrate on bounding the term:
(cid:52)ψ

− (cid:52)ψ

wj+1, wj+ 1

wj, wj+ 1

(cid:16)

(cid:17)

(cid:16)

(cid:17)

.

2
(cid:16)

(cid:17)

2

(cid:16)

(cid:17)

(cid:52)ψ

wj, wj+ 1

2

− (cid:52)ψ

wj+1, wj+ 1

2

= ψ (wj) − ψ (wj+1) −

wj − wj+1, ∇ψ

≤
ψ 1-strong convex

(cid:68)

wj − wj+1, ∇ψ (wj) − ∇ψ

(cid:17)(cid:69)

(cid:16)

wj+ 1

2

−

· (cid:107)wj − wj+1(cid:107)2

(cid:68)

1
2

(cid:17)(cid:69)

(cid:16)

wj+ 1

2

= (cid:104)wj − wj+1, η · gj(cid:105) −

· (cid:107)wj − wj+1(cid:107)2

≤ η · G · (cid:107)wj − wj+1(cid:107) −

· (cid:107)wj − wj+1(cid:107)2

1
2
1
2

≤

(η · G)2
2

where the last inequality stems from the fact that

(cid:107)wj − wj+1(cid:107) ·

(cid:16)

√
1√
2

− η·G√
2

(cid:17)2

≥ 0

We now continue with the analysis referring to wf and ws separately. Summing over j = τ + 1 to (cid:0) T
(these are the T
M τ iterations in which the ﬁrst sub-algorithm is in use), and from j = 1 to T
the T

M (M − τ ) iterations in which the second sub-algorithm is in use) we get:

M + 1(cid:1) · τ for wf
M · (M − τ ) for ws (these are

For wf :
( T
M +1)·τ
(cid:88)

(cid:68)
wf

j − w∗

f , gj

(cid:69)

j=τ +1

=

=

=

≤

M +1)·τ
( T
(cid:88)

j=τ +1

( T
M +1)·τ
(cid:88)

j=τ +1

M +1)·τ
( T
(cid:88)

j=τ +1

M +1)·τ
( T
(cid:88)

j=τ +1

1
η

1
η

1
η

·

·

≤

1
η

·

M +1)·τ
( T
(cid:88)

j=τ +1

(cid:68)
wf

j − w∗

f , ∇fT1(j−τ )

(cid:16)

wf

j−τ

(cid:17)(cid:69)

(cid:68)
wf

·

j − w∗
f ,

(cid:16)

∇ψ

(cid:17)

(cid:16)
wf
j

− ∇ψ

(cid:17)(cid:17)(cid:69)

(cid:16)

ws

j+ 1
2

(cid:16)

(cid:16)

(cid:52)ψ

w∗

f , wf

j

(cid:17)

+ (cid:52)ψ

j , wf
wf

j+ 1
2

− (cid:52)ψ

w∗

f , wf

j+ 1
2

(cid:17)(cid:17)

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

(cid:52)ψ

w∗

f , wf

j

(cid:17)

+ (cid:52)ψ

j , wf
wf

j+ 1
2

− (cid:52)ψ

w∗

f , wf

j+1

− (cid:52)ψ

(cid:17)

(cid:16)

(cid:17)(cid:17)

j+1, wf
wf

j+ 1
2

(cid:16)

(cid:52)ψ

w∗

f , wf

j

(cid:17)

(cid:16)

− (cid:52)ψ

w∗

f , wf

j+1

(cid:16)

(cid:52)ψ

j , wf
wf

j+ 1
2

(cid:17)

− (cid:52)ψ

(cid:16)

j+1, wf
wf

j+ 1
2

(cid:17)

(cid:17)

+

·

1
η

M +1)·τ
( T
(cid:88)

j=τ +1

Online Learning with Local Permutations and Delayed Feedback

=

· (cid:52)ψ

w∗

f , wf

τ +1

− (cid:52)ψ

(cid:16)

(cid:17)

(cid:18)

w∗

f , wf
M +1)·τ
( T

(cid:19)

+

·

1
η

M +1)·τ
( T
(cid:88)

j=τ +1

(cid:16)

(cid:52)ψ

j , wf
wf

j+ 1
2

(cid:17)

− (cid:52)ψ

(cid:16)
j+1, wf
wf

j+ 1
2

(cid:17)

1
η

1
ηf
1
ηf

≤

≤

For ws:

(cid:16)

· (cid:52)ψ

w∗

f , wf

τ +1

· B2 +

· τ ·

T
M

(cid:17)

+

1
ηf
ηf · G2
2

·

T
M

· τ ·

(ηf · G)2
2

T
M ·(M −τ )
(cid:88)

j=1

(cid:10)ws

j − w∗

s , gj

(cid:11)

(cid:10)ws

j − w∗

s , ∇fT2(j)

(cid:1)(cid:11)

(cid:0)ws

j

T
M ·(M −τ )
(cid:88)

j=1

T
M ·(M −τ )
(cid:88)

j=1

T
M ·(M −τ )
(cid:88)

j=1

T
M ·(M −τ )
(cid:88)

(cid:68)

(cid:16)

(cid:16)

·

·

·

1
η

1
η

1
η

j=1

T
M ·(M −τ )
(cid:88)

j=1

=

=

=

≤

=

≤

≤

1
η

1
η

1
ηs
1
ηs

ws

j − w∗
s ,

∇ψ (cid:0)ws

(cid:1) − ∇ψ

j

(cid:16)

(cid:17)(cid:17)(cid:69)

(cid:16)

ws

j+ 1
2

(cid:52)ψ

(cid:0)w∗

s , ws
j

(cid:1) + (cid:52)ψ

ws

j , ws

j+ 1
2

(cid:16)

− (cid:52)ψ

w∗

s , ws

j+ 1
2

(cid:17)(cid:17)

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(cid:52)ψ

(cid:0)w∗

s , ws
j

(cid:1) + (cid:52)ψ

ws

j , ws

j+ 1
2

− (cid:52)ψ

(cid:0)w∗

s , ws

j+1

(cid:1) − (cid:52)ψ

(cid:16)

ws

j+1, ws

j+ 1
2

(cid:17)(cid:17)

≤

·

(cid:52)ψ

(cid:0)w∗

s , ws
j

(cid:1) − (cid:52)ψ

(cid:0)w∗

s , ws

j+1

(cid:1) +

(cid:16)

· (cid:52)ψ

1
η

ws

j , ws

j+ 1
2

(cid:17)

− (cid:52)ψ

(cid:16)

ws

j+1, ws

j+ 1
2

(cid:17)

· (cid:52)ψ (w∗

s , ws

1) − (cid:52)ψ

(cid:16)

w∗

s , ws
M +1)·τ
( T

(cid:17)

+

·

1
η

(cid:16)

(cid:52)ψ

ws

j , ws

j+ 1
2

(cid:17)

− (cid:52)ψ

(cid:16)

ws

j+1, ws

j+ 1
2

(cid:17)

T
M ·(M −τ )
(cid:88)

j=1

· (cid:52)ψ (w∗

s , ws

1) +

· (M − τ ) ·

1
ηs

·

T
M
ηs · G2
2

· B2 +

· (M − τ ) ·

T
M

(ηs · G)2
2

We are after bounding the regret, which in itself is upper bounded by the sum of the regret accumulated by each sub-
algorithm, considering iterations in the ﬁrst τ and last M −τ per block separately, as mentioned above. Using the convexity
of ft for all t, we bound these terms:

i=0

t=M ·i+1

T
M −1
(cid:88)

M ·i+τ
(cid:88)

i=0

t=M ·i+1













≤ E

= E

T
M ·τ
(cid:88)

j=1

T
M −1
(cid:88)

M ·i+τ
(cid:88)

E

ft (wt) − ft

(cid:0)w∗

f

(cid:1) +



ft (wt) − ft (w∗
s )



(cid:10)wt − w∗

f , ∇ft (wt)(cid:11) +

(cid:104)wt − w∗

s , ∇ft (wt)(cid:105)



(cid:68)
wf

j − w∗

f , ∇fT1(j)

(cid:17)(cid:69)

(cid:16)

wf
j

+

(cid:10)ws

j − w∗

s , ∇fT2(j)+τ

(cid:1)(cid:11)

(cid:0)ws

j







T
M −1
(cid:88)

M ·(i+1)
(cid:88)

i=0

t=M ·i+τ +1

T
M −1
(cid:88)

M ·(i+1)
(cid:88)

i=0

t=M ·i+τ +1

T
M ·(M −τ )
(cid:88)

j=1

In the last equality of the above derivation, we simply replace notations, writing the gradient ∇ft (wt) in notation of T1

Online Learning with Local Permutations and Delayed Feedback

and T2. T1 contains all time points in the ﬁrst τ iterations of each block, and T2 contains all time points in the ﬁrst M − τ
iterations of each block.

that what we
(cid:10)ws

j − w∗
and
Note
(cid:80) T
(cid:1)(cid:11) for ws, which are not the terms we need to bound in order to get a regret
bound since they use the delayed gradient, and so we need to take a few more steps in order to be able to bound the regret.

M ·(M −τ )
j=1

s , ∇fT2(j)

f , ∇fT1(j−τ )(wf

for wf

j − w∗

bounded

j−τ )(cid:105)

(cid:0)ws

(cid:104)wf

have

far

so

j

is (cid:80)( T

M +1)·τ
j=τ +1

We begin with wf :

T
M −1
(cid:88)

M ·i+τ
(cid:88)

i=0

t=M ·i+1

(cid:10)wt − w∗

f , ∇ft (wt)(cid:11) =

(cid:68)
wf

j − w∗

f , ∇fT1(j)

(cid:17)(cid:69)

(cid:16)

wf
j

T
M ·τ
(cid:88)

j=1

T
M ·τ
(cid:88)

j=1

=

=

1
ηf

1
ηf

1
ηf

(cid:68)
wf

j+τ − w∗

f , ∇fT1(j)

(cid:16)

wf
j

(cid:17)(cid:69)

(cid:68)

+

j − wf
wf

j+τ , ∇fT1(j)

(cid:17)(cid:69)

(cid:16)

wf
j

M +1)·τ
( T
(cid:88)

(cid:68)
wf

j=τ +1

j − w∗

f , ∇fT1(j−τ )

(cid:16)

wf

j−τ

(cid:17)(cid:69)

(cid:68)
j−τ − wf
wf

+

j , ∇fT1(j−τ )

(cid:16)

wf

j−τ

(cid:17)(cid:69)

≤

· B2 +

· τ ·

(cid:68)
j−τ − wf
wf

j , ∇fT1(j−τ )

(cid:16)

wf

j−τ

(cid:17)(cid:69)

≤

· B2 +

· τ ·

(cid:107)wf

j−τ − wf

j (cid:107) · (cid:107)∇fT1(j−τ )

wf

j−τ

(cid:16)

(cid:17)

(cid:107)

T
M

T
M

T
M

ηf · G2
2

+

ηf · G2
2

+

M +1)·τ
( T
(cid:88)

j=τ +1

M +1)·τ
( T
(cid:88)

j=τ +1

ηf · G2
2

+

M +1)·τ
( T
(cid:88)

τ
(cid:88)

j=τ +1

i=1

≤

· B2 +

· τ ·

(cid:107)wf

j−i − wf

j−i+1(cid:107) · G

The last term in the above derivation, is the sum of differences between consecutive predictors. This difference, is deter-
mined by the mirror map in use, the step size ηf , and the bound over the norm of the gradient used in the update stage of the
algorithm, G. This is because every consecutive predictor is received by taking a gradient step from the previous predictor,
in the dual space, with a step size ηf , and projecting back to the primal space by use of the bregman divergence with the
speciﬁc mirror map in use. We denote the bound on this difference by Ψ(ηf ,G), i.e., ∀j, j + 1 : (cid:107)wf
j+1(cid:107) ≤ Ψ(ηf ,G).
Continuing our derivation, we have:

j − wf

≤

≤

1
ηf

1
ηf

· B2 +

· τ ·

T
M

T
M

ηf · G2
2

ηf · G2
2

+

+

· B2 +

· τ ·

· τ 2 · Ψ(ηf ,G) · G

( T
M +1)·τ
(cid:88)

τ
(cid:88)

j=τ +1

i=1

Ψ(ηf ,G) · G

T
M

1
ηf

Since this upper bound does not depend on the permutation,and holds for every sequence, it holds also in expectation, i.e.





E

T
M −1
(cid:88)

M ·i+τ
(cid:88)

i=0

t=M ·i+1



ft (wt) − ft

(cid:1)

(cid:0)w∗

f

 ≤

· B2 +

· τ ·

T
M

ηf · G2
2

+

T
M

· τ 2 · Ψ(ηf ,G) · G

We now turn to ws

T
M −1
(cid:88)

M ·(i+1)
(cid:88)

i=0

t=M ·i+τ +1

T
M −1
(cid:88)

M ·(i+1)
(cid:88)

≤

i=0

t=M ·i+τ +1

ft (wt) − ft (w∗
s )

(cid:104)wt − w∗

s , ∇ft (wt)(cid:105)

Online Learning with Local Permutations and Delayed Feedback

(cid:10)ws

j − w∗

s , ∇fT2(j)+τ

(cid:1)(cid:11)

(cid:0)ws

j

=

=

T
M ·(M −τ )
(cid:88)

j=1

T
M ·(M −τ )
(cid:88)

j=1

T
M ·(M −τ )
(cid:88)

j=1

T
M ·(M −τ )
(cid:88)

j=1

(cid:10)ws

j − w∗

s , ∇fT2(j)

(cid:0)ws

j

(cid:1)(cid:11) +

(cid:10)ws

j − w∗

s , ∇fT2(j)+τ

(cid:0)ws

j

(cid:1) − ∇fT2(j)

(cid:0)ws

j

(cid:1)(cid:11)

≤

· B2 +

· (M − τ ) ·

1
ηs

T
M

ηs · G2
2

+

(cid:10)ws

j − w∗

s , ∇fT2(j)+τ

(cid:0)ws

j

(cid:1) − ∇fT2(j)

(cid:0)ws

j

(cid:1)(cid:11)

j

j

(cid:0)ws

(cid:0)ws

j − w∗

s , ∇fT2(j)+τ

(cid:1)(cid:11) for any j.

(cid:1) − ∇fT2(j)

j only depends on gradients of time points: T2 (1) , T2 (2) , ..., T2 (j − 1).

We now look at the expression (cid:10)ws
We ﬁrst notice that for any j, ws
We also notice that given the functions received at these time points, i.e, given fT2(1), fT2(2), ..., fT2(j−1), ws
a random variable.
We have that for all j, T2 (j) and T2 (j) + τ are both time points that are part of the same M -sized block. Suppose we have
observed n functions of the block to which T2 (j) and T2 (j) + τ belong. All of these n functions are further in the past
than both T2 (j) and T2 (j) + τ , because of the delay of size τ . We have M − n functions in the block that have not been
observed yet, and since we performed a random permutation within each block, all remaining functions in the block have
the same expected value. Formally, given ws
j , the expected value of the current and delayed gradient are the same, since we
(cid:0)ws
(cid:0)ws
have: E[∇fT2(j)+τ
j ]. As mentioned above, this stems
i=1 ∇fT2(j)+i
from the random permutation we performed within the block - all M − n remaining functions (that were not observed yet
in this block) have an equal (uniform) probability of being in each location, and thus the expected value of the gradients is
equal. From the law of total expectation we have that

M −n · (cid:80)M −n

(cid:1) = E[∇fT2(j)

j is no longer

j ] = 1

(cid:1) |ws

(cid:1) |ws

(cid:0)ws

j

j

j

E[∇fT2(j)+τ

(cid:0)ws

j

(cid:1)] = E[E[∇fT2(j)+τ

(cid:0)ws

j

(cid:1) |ws

j ]] = E[E[∇fT2(j)

(cid:0)ws

j

(cid:1) |ws

j ]] = E[∇fT2(j)

(cid:0)ws

j

(cid:1)]

(cid:0)ws

(cid:1) − ∇fT2(j)

ans thus E[∇fT2(j)+τ
We get that E[(cid:10)ws
So we have that the upper bound on the expected regret of the time point in which we predict with ws is:

(cid:1)] = 0.
(cid:0)ws
(cid:1) − ∇fT2(j)

s , ∇fT2(j)+τ

(cid:1)(cid:11)] = 0

j − w∗

(cid:0)ws

(cid:0)ws

j

j

j

j





E

T
M −1
(cid:88)

M ·(i+1)
(cid:88)

i=0

t=M ·i+τ +1

ft (wt) − ft (w∗
s )

 ≤

· B2 +

· (M − τ ) ·



1
ηs

T
M

ηs · G2
2

Summing up the regret of the two sub-algorithms, we get:

(cid:34) T

(cid:88)

(cid:35)
ft (wt) − ft (w∗)

E

≤ E

t=1

T
M −1
(cid:88)

M ·i+τ
(cid:88)





i=0

t=M ·i+1

ft (wt) − ft

(cid:0)w∗

f

(cid:1) +

≤

B2
ηf

+ ηf ·

T τ
M

·

G2
2

+

T τ 2
M

· G · Ψ(ηf ,G) +

T
M −1
(cid:88)

M ·(i+1)
(cid:88)

i=0

t=M ·i+τ +1
B2
ηs

+ ηs ·



ft (wt) − ft (w∗
s )



T · (M − τ )
M

·

G2
2

which gives us the bound.
For Ψ(ηf ,G) ≤ c · ηf · G where c is some constant, choosing the step sizes, ηf , ηs optimally:

√

ηf =

(cid:113)

M
B ·
2 + c · τ (cid:1)
T · τ · (cid:0) 1

G ·

, ηs =

√

B ·

2M

G · (cid:112)T · (M − τ )

Online Learning with Local Permutations and Delayed Feedback

we get the bound:

(cid:34) T

(cid:88)

(cid:35)
ft (wt) − ft (w∗)

E

· B · G ·

+ c · τ +

· B · G ·

(cid:114) 1
2

(cid:114)

T · τ
M

1

(cid:113) 1

2 + cτ

(cid:114)

+

T · τ
M

· B · G ·

cτ

(cid:113) 1

2 + cτ

t=1
(cid:114)

T · τ
M

=

+

(cid:114)

≤ c ·

= O

2 · T · (M − τ )
M
T · τ
M

(cid:114)

· B · G ·

· B · G
(cid:114) 1
2

+ c · τ +

(cid:32)(cid:114)

(cid:114)

T · τ 2
M

+

T · (M − τ )
M

(cid:33)

(cid:114)

2 · T · (M − τ )
M

(cid:32)√

(cid:32)(cid:114)

= O

T ·

· B · G

(cid:33)(cid:33)

τ 2
M

+ 1

A.2. Lower Bound For Algorithms With No Permutation Power

Theorem 3. For every (possible randomized) algorithm A, there exists a choice of linear, 1-Lipschitz functions over
[−1, 1] ⊂ R, with τ a ﬁxed size delay of feedback, such that the expected regret of A after T rounds (with respect to the
algorithm’s randomness), is

E [RA (T )] = E

ft (wt) −

(cid:34) T

(cid:88)

t=1

(cid:35)
ft (w∗)

(cid:16)√

(cid:17)

τ T

= Ω

T
(cid:88)

t=1

, where w∗ = argmin
w∈W

T
(cid:88)

t=1

ft (w)

Proof. First, we note that in order to show that for every algorithm, there exists a choice of loss functions by an oblivious
adversary, such that the expected regret of the algorithm is bounded from below, it is enough to show that there exists a
distribution over loss function sequences such that for any algorithm, the expected regret is bounded from below, where
now expectation is taken over both the randomness of the algorithm and the randomness of the adversary. This is because
if there exists such a distribution over loss function sequences, then for any algorithm, there exists some sequence of loss
functions that can lead to a regret at least as high. To put it formally, if we mark E
the expectation over the randomness of
alg

the algorithm, and

the expectation over the randomness of the adversary, then:

E
f1,...,fT

∃ a (randomized) adversary s.t. ∀ algorithm A,

∀ algorithm A, ∃f1, ..., fT s.t. E
alg

[RA (T )] > Ω

τ T

E
f1,...,fT

E
alg

[RA (T )] > Ω
(cid:16)√

(cid:17)

(cid:16)√

(cid:17)

τ T

→

Thus, we prove the ﬁrst statement above, that immediately gives us the second statement which gives the lower bound.

We consider the setting where W = [−1, 1], and ∀t ∈ [1, T ] : ft (wt) = αt · wt where αt ∈ {1, −1}. We divide the T
rounds to blocks of size τ . αt is chosen in the following way: if αt is the ﬁrst α in the block, it is randomly picked, i.e,
Pr (α = ±1) = 1
2 . Following this random selection, the next τ − 1 α’s of the block will be identical to the ﬁrst α in it, so
that we now have a block of τ consecutive functions in which α is identical. We wish to lower bound the expected regret
of any algorithm in this setting.

Consider a sequence of predictions by the algorithm w1, w2, ..., wT . Denote by αi,j the j’th α in the i’th block, and
similarly for wi,j, fi,j. We denote the entire sequence of α’s by ¯α(1→T ), and the sequence of α’s until time point j in block
i by ¯α(1→i,j). Notice that wi,j is a function of the α’s that arrive up until time point i · τ + j − τ − 1. We denote these α’s
as ¯α(1→i,j−τ −1).

Then the expected sum of losses is:

(cid:35)

ft (wt)

= E

(cid:34) T

(cid:88)

E

t=1





T

τ(cid:88)

τ
(cid:88)

i=1

j=1





fi,j (wi,j)

Online Learning with Local Permutations and Delayed Feedback

=

=

=

=

=

=

=

T

τ(cid:88)

τ
(cid:88)

i=1

j=1

T

τ(cid:88)

τ
(cid:88)

i=1

j=1

T

τ(cid:88)

τ
(cid:88)

i=1

j=1

T

τ(cid:88)

τ
(cid:88)

i=1

j=1

T

τ(cid:88)

τ
(cid:88)

i=1

j=1

T

τ(cid:88)

τ
(cid:88)

i=1

j=1

T

τ(cid:88)

τ
(cid:88)

i=1

j=1

E [fi,j (wi,j)]

E ¯α(1→T ) [αi,j · wi,j]

E ¯α(1→i,j−τ −1)

(cid:2)E ¯α(i,j−τ→T )

(cid:2)αi,j · wi,j|¯α(1→i,j−τ −1)

(cid:3)(cid:3)

E ¯α(1→i,j−τ −1)

(cid:2)wi,j · E ¯α(i,j−τ→T )

(cid:2)αi,j|¯α(1→i,j−τ −1)

(cid:3)(cid:3)

E ¯α(1→i,j−τ −1)

(cid:2)wi,j · E ¯α(i,1→i,j)

(cid:2)αi,j|¯α(1→i,j−τ −1)

(cid:3)(cid:3)

E ¯α(1→i,j−τ −1)

(cid:2)wi,j · Eαi,1 [αi,1](cid:3)

E ¯α(1→i,j−τ −1)

(cid:20)
wi,j ·

(cid:18) 1
2

1
2

· 1 +

· (−1)

= 0

(cid:19)(cid:21)

The last equality is true because every ﬁrst α in any block has probability 1

We now continue to the expected sum of losses for the optimal choice of w∗ = argminw∈W
this setting, w∗ ∈ {+1, −1} and is with opposite sign to the majority of α’s in the sequence.

2 to be either +1 or −1.
(cid:16)(cid:80)T

(cid:17)
t=1 ft (w)

. Note that in

(cid:35)

(cid:34) T

(cid:88)

E

t=1

ft (w∗)

= E

fi,j (w∗)

T

τ(cid:88)

τ
(cid:88)

i=1

j=1









T

τ(cid:88)

i=1

= E

τ · αi,1 · w∗

= −τ · E



αi,1|





|

T

τ(cid:88)

i=1


 = E





T

τ(cid:88)

τ
(cid:88)





αi,j · w∗


 = τ · E

j=1

i=1


T

τ(cid:88)

i=1







αi,1 · w∗

Using Khintchine inequality we have that:

−τ · E

αi,1 · 1|

 ≤ −τ · C ·

12

 = −τ · C ·





|

T

τ(cid:88)

i=1



(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)





T

τ(cid:88)

i=1

(cid:114)

T
τ

(cid:16)√

(cid:17)

= −Ω

τ · T

where C is some constant.

Thus we get that for a sequence of length T the expected regret is:

(cid:34) T

(cid:88)

E

t=1

(cid:35)

ft (wt)

− E

(cid:34) T

(cid:88)

(cid:35)
ft (w∗)

(cid:16)√

(cid:17)

= Ω

τ · T

t=1

Online Learning with Local Permutations and Delayed Feedback

A.3. Proof of Theorem 2

Proof. First, we note that to show that for every algorithm, there exists a choice of loss functions by an oblivious adversary,
such that the expected regret of the algorithm is bounded from below, it is enough to show that there exists a distribution
over loss function sequences such that for any algorithm, the expected regret is bounded from below, where now expectation
is taken over both the randomness of the algorithm and the randomness of the adversary. This is because if there exists
such a distribution over loss function sequences, then for any algorithm, there exists some sequence of loss functions that
can lead to a regret at least as high. To put it formally, if we mark E
the expectation over the randomness of the algorithm,
alg

and

E
f1,...,fT

the expectation over the randomness of the adversary, then:

∃ a (randomized) adversary s.t. ∀ algorithm A,

∀ algorithm A, ∃f1, ..., fT s.t. E
alg

[RA (T )] > Ω

τ T

E
f1,...,fT

E
alg

[RA (T )] > Ω
(cid:16)√

(cid:17)

(cid:16)√

(cid:17)

τ T

→

Thus, we prove the ﬁrst statement above, that immediately gives us the second statement which is indeed our lower bound.

We consider the setting where W = [−1, 1], and ∀t ∈ [1, T ] : ft (wt) = αt · wt where αt ∈ {1, −1}. We start by
constructing our sequence of α’s. We divide the T iterations to blocks of size τ
3 . In each block, all α’s are identical, and are
chosen to be +1 or −1 w.p. 1
3 consecutive functions in which α is identical within each
block. Let M be a permutation window of size smaller than τ
3 and the sequence of α’s is
organized in blocks of size τ
3 , then even after permutation, the time difference between the ﬁrst and last time we encounter
an α is ≤ τ , which means we will not get the feedback from the ﬁrst time we encountered this α before encountering the
next one, and we will not be able to use it for correctly predicting α’s of this (original) block that arrive later. This is the
main idea that stands in the basis of this lower bound.

3 . We notice ﬁrst that since M < τ

2 . This choice gives us blocks of τ

Formally, consider a sequence of w1, w2, ..., wT chosen by the algorithm. Denote by αi,j the j’th α in the i’th block,
and similarly for wi,j, fi,j. We denote the entire sequence of α’s by ¯α(1→T ), and the sequence of α’s until time point
j in block i by ¯α(1→i,j). For simplicity we will denote βt as the α that was presented at time t, after permutation, i.e.
βt := ασ−1(()t). Notice that wi,j is a function of the β’s that arrive up until time point i · (cid:0) τ
(cid:1) + j − τ − 1. We denote these
β’s as ¯β(1→i,j−τ −1). I.e wi,j = g (cid:0) ¯β(1→i,j−τ −1)
Going back to our main idea of the construction, we can put it in this new terminology- since the delay is τ and the
permutation window is M < τ
3 , for any i, j, the ﬁrst time we encountered ασ−1(i,j) is less than τ iterations ago, and thus,
βi,j is independent of ¯β(1→i,j−τ −1), while wi,j is a function of it: wi,j = g (cid:0) ¯β(1→i,j−τ −1)
With this in hand, we look at the sum of losses of the predictions of the algorithm, w1, w2, ..., wT :

(cid:1) where g is some function.

(cid:1).

3

(cid:35)

ft (wt)

= E

(cid:34) T

(cid:88)

E

t=1





T/ τ
3(cid:88)

τ

3(cid:88)

i=1

j=1





fi,j (wi,j)

E [fi,j (wi,j)]

E ¯β(1→T )

[βi,j · wi,j]

=

=

=

=

=

T/ τ
3(cid:88)

τ

3(cid:88)

i=1

j=1

T/ τ
3(cid:88)

τ

3(cid:88)

i=1

j=1

T/ τ
3(cid:88)

τ

3(cid:88)

i=1

j=1

T/ τ
3(cid:88)

τ

3(cid:88)

i=1

j=1

T/ τ
3(cid:88)

τ

3(cid:88)

i=1

j=1

E ¯β(1→i,j−τ −1)

(cid:104)
E ¯β(i,j−τ→T )

(cid:2)βi,j · wi,j| ¯β(1→i,j−τ −1)

E ¯β(1→i,j−τ −1)

(cid:104)
wi,j · E ¯β(i,j−τ→T )

(cid:2)βi,j| ¯β(1→i,j−τ −1)

(cid:3)(cid:105)

(cid:3)(cid:105)

E ¯β(1→i,j−τ −1)

(cid:104)
wi,j · E ¯β(i,j−τ→T )

(cid:2)ασ−1(i,j)

(cid:3)(cid:105)

Online Learning with Local Permutations and Delayed Feedback

=

T/ τ
3(cid:88)

τ

3(cid:88)

i=1

j=1

E ¯β(1→i,j−τ −1)

(cid:20)
wi,j ·

(cid:18) 1
2

1
2

· 1 +

· (−1)

= 0

(cid:19)(cid:21)

where the last equality stems from the fact that βi,j = ασ−1(i,j) is equal to the expected value of the ﬁrst time we
encountered the α that corresponds to ασ−1(i,j), i.e, the ﬁrst α that came from the same block of ασ−1(i,j). This expectation
is 0 since we choose α = 1 or α = −1 with probability 1

2 for each block.

We now continue to the expected sum of losses for the optimal choice of w∗ = argminw∈W
. Note that
after permutation, the expected sum of losses of the optimal w remains the same since it is best predictor over the entire
sequence, and so for simplicity we look at the sequence of α’s as it is chosen initially. Also, in this setting, w∗ ∈ {+1, −1}
and is with opposite sign to the majority of α’s in the sequence.

t=1 ft (w)

(cid:16)(cid:80)T

(cid:17)

T/ τ
3(cid:88)

τ

3(cid:88)

i=1

j=1

T/ τ
3(cid:88)

i=1

τ
3











τ
3



T/ τ
3(cid:88)

i=1

τ
3

(cid:35)

(cid:34) T

(cid:88)

E

t=1

ft (w∗)

= E

fi,j (w∗)


 = E





T/ τ
3(cid:88)

τ

3(cid:88)

αi,j · w∗

= E

· αi,1 · w∗

 =

· E



αi,1 · w∗

i=1


j=1

T/ τ
3(cid:88)

i=1

τ
3





= −

· E

|

αi,1|











−

τ
3



· E

|

T/ τ
3(cid:88)

i=1

αi,1 · 1|

 ≤ −

· C ·

12

 = −

· C ·



(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)





T/ τ
3(cid:88)

i=1

(cid:19)

τ
3

(cid:115)

T
τ
3

= −Ω

· T

= −Ω

τ · T

(cid:16)√

(cid:17)

(cid:18)(cid:114) τ
3

Using Khintchine inequality we have that:

where C is some constant.

Thus we get that overall expected regret for any algorithm with permutation power M < τ

3 is:

(cid:34) T

(cid:88)

E

t=1

(cid:35)

(cid:35)

ft (wt)

− E

ft (w∗)

= Ω

(cid:16)√

(cid:17)

τ · T

(cid:34) T

(cid:88)

t=1

as in the adversarial case.

