Count-BasedExplorationwithNeuralDensityModelsA.PixelCNNHyper-parametersThePixelCNNmodelusedinthispaperisalightweightvariantoftheGatedPixelCNNintroducedin(vandenOordetal.,2016a).Itconsistsofa7×7maskedconvolution,followedbytworesidualblockswith1×1maskedconvolu-tionswith16featureplanes,andanother1×1maskedcon-volutionproducing64featuresplanes,whicharemappedbyaﬁnalmaskedconvolutiontotheoutputlogits.Inputsare42×42greyscaleimages,withpixelvaluesquantizedto8bins.Themodelistrainedcompletelyonline,fromthestreamofAtariframesexperiencedbyanagent.Optimizationisper-formedwiththe(uncentered)RMSPropoptimizer(Tiele-man&Hinton,2012)withmomentum0.9,decay0.95andepsilon10−4.B.MethodologyUnlessotherwisestated,allagentperformancegraphsinthispapershowtheagent’strainingperformance,measuredastheundiscountedper-episodereturn,averagedover1Menvironmentframesperdatapoint.Thealgorithm-comparisongraphsFig.6andFig.8showtherelativeimprovementofonealgorithmoveranotherintermsofarea-under-the-curve(AUC).Acomparisonbymaximumachievedscorewouldyieldsimilaroverallre-sults,butunderestimatetheadvantageintermsoflearningspeed(sampleefﬁciency)andstabilitythattheintrinsicallymotivatedandMMC-basedagentsshowoverthebaselines.C.ConvolutionalCTSInSection4wehaveseenthatDQN-PixelCNNoutper-formsDQN-CTSinmostofthe57Atarigames,byprovid-ingamoreimpactfulexplorationbonusinhardexplorationgames,aswellasamoregraceful(lessharmful)oneingameswherethelearningalgorithmdoesnotbeneﬁtfromtheadditionalcuriositysignal.OnemaywonderwhetherthisimprovementisduetothegenerallymoreexpressiveandaccuratedensitymodelPixelCNN,orsimplyitscon-volutionalnature,whichgivesitanadvantageingeneral-izationandsampleefﬁciencyoveramodelthatrepresentspixelprobabilitiesinacompletelylocation-dependentway.Toanswerthisquestion,wedevelopedaconvolutionalvari-antoftheCTSmodel.Thismodelhasasinglesetofpa-rametersconditioningapixel’svalueonitspredecessorssharedacrossallpixellocations,insteadofthelocation-dependentparametersintheregularCTS.InFig.14wecontrasttheperformanceofDQN,DQN-MC,DQN-CTS,DQN-ConvCTSandDQN-PixelCNNon6examplegames.WeﬁrstconsiderdenserewardgameslikeQ*BERTandZAXXON,wheremostimprovementcomesfromtheuseoftheMMC,andtheexplorationbonushurtsperformance.WeﬁndthatinfactconvolutionalCTSbehavesfairlysim-ilarlytoPixelCNN,leavingagentperformanceunaffected,whereasregularCTScausestheagenttotrainmoreslowlyorreachanearlierperformanceplateau.Onthesparsere-wardgames(GRAVITAR,PRIVATEEYE,VENTURE)how-ever,convolutionalCTSshowstobeasinferiortoPixel-CNNasthevanillaCTSvariant,failingtoachievethesig-niﬁcantimprovementsoverthebaselineagentspresentedinthispaper.Weconcludethatwhiletheconvolutionalaspectplaysaroleinthe’softer’natureofthePixelCNNmodelcom-paredtoitsCTScounterpart,italoneisinsufﬁcienttoexplainthemassiveexplorationboostthatthePixelCNN-derivedrewardprovidestotheDQNagent.Themoread-vancedmodel’saccuracyadvantagetranslatesintoamoretargetedandusefulcuriositysignalfortheagent,whichdistinguishesnovelfromwell-exploredstatesmoreclearlyandallowsformoreeffectiveexploration.D.TheHardestExplorationGamesTable1reproducesBellemareetal.(2016)’staxonomyofgamesavailablethroughtheALEaccordingtotheirexplo-rationdifﬁculty.“Human-Optimal”referstogameswhereDQN-likeagentsachievehuman-levelorhigherperfor-mance;“ScoreExploit”referstogameswhereagentsﬁndwaystoachievesuperhumanscores,withoutnecessarilyplayingthegameasahumanwould.“Sparse”and“Dense”rewardsarequalitativedescriptorsofthegame’srewardstructure.Seetheoriginalsourceforadditionaldetails.Table2comparespreviouslypublishedresultsonthe7hardexploration,sparserewardAtari2600gameswithresultsobtainedbyDQN-CTSandDQN-PixelCNN.Count-BasedExplorationwithNeuralDensityModelsFigure14.ComparisonofDQN,DQN-CTS,DQN-ConvCTSandDQN-PixelCNNtrainingperformance.EasyExplorationHardExplorationHuman-OptimalScoreExploitDenseRewardSparseRewardASSAULTASTERIXBEAMRIDERALIENFREEWAYASTEROIDSATLANTISKANGAROOAMIDARGRAVITARBATTLEZONEBERZERKKRULLBANKHEISTMONTEZUMA’SREVENGEBOWLINGBOXINGKUNG-FUMASTERFROSTBITEPITFALL!BREAKOUTCENTIPEDEROADRUNNERH.E.R.O.PRIVATEEYECHOPPERCMDCRAZYCLIMBERSEAQUESTMS.PAC-MANSOLARISDEFENDERDEMONATTACKUPNDOWNQ*BERTVENTUREDOUBLEDUNKENDUROTUTANKHAMSURROUNDFISHINGDERBYGOPHERWIZARDOFWORICEHOCKEYJAMESBONDZAXXONNAMETHISGAMEPHOENIXPONGRIVERRAIDROBOTANKSKIINGSPACEINVADERSSTARGUNNERTable1.AroughtaxonomyofAtari2600gamesaccordingtotheirexplorationdifﬁculty.DQNA3C-CTSPrior.DuelDQN-CTSDQN-PixelCNNFREEWAY30.830.4833.031.731.7GRAVITAR473.0238.68238.0498.3859.1MONTEZUMA’SREVENGE0.0273.700.03705.52514.3PITFALL!-286.1-259.090.00.00.0PRIVATEEYE146.799.32206.08358.715806.5SOLARIS3,482.82270.15133.42863.65501.5VENTURE163.00.0048.082.21356.25Table2.Comparisonwithpreviouslypublishedresultsonhardexploration,sparserewardgames.ThecomparedagentsareDQN(Mnihetal.,2015),A3C-CTS(“A3C+”in(Bellemareetal.,2016)),PrioritizedDuelingDQN(Wangetal.,2016),andthebasicversionsofDQN-CTSandDQN-PixelCNNfromSection4.Forouragentswereportthemaximumscoresachievedover150Mframesoftraining,averagedover3seeds.Count-BasedExplorationwithNeuralDensityModelsFigure15.TrainingcurvesofDQN,DQN-CTSandDQN-PixelCNNacrossall57Atarigames.Count-BasedExplorationwithNeuralDensityModelsFigure16.TrainingcurvesofDQN,DQN-PixelCNN,ReactorandReactor-PixelCNNacrossall57Atarigames.Count-BasedExplorationwithNeuralDensityModelsFigure17.TrainingcurvesofDQNandDQN-PixelCNN,eachwithandwithoutMMC,acrossall57Atarigames.