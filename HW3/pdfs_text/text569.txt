Safety-Aware Algorithms for Adversarial Contextual Bandit

Wen Sun 1 Debadeepta Dey 2 Ashish Kapoor 2

Abstract
In this work we study the safe sequential decision
making problem under the setting of adversar-
ial contextual bandits with sequential risk con-
straints. At each round, nature prepares a context,
a cost for each arm, and additionally a risk for
each arm. The learner leverages the context to
pull an arm and receives the corresponding cost
and risk associated with the pulled arm. In addi-
tion to minimizing the cumulative cost, for safety
purposes, the learner needs to make safe deci-
sions such that the average of the cumulative risk
from all pulled arms should not be larger than a
pre-deﬁned threshold. To address this problem,
we ﬁrst study online convex programming in the
full information setting where in each round the
learner receives an adversarial convex loss and a
convex constraint. We develop a meta algorithm
leveraging online mirror descent for the full in-
formation setting and then extend it to contextual
bandit with sequential risk constraints setting us-
ing expert advice. Our algorithms can achieve
near-optimal regret in terms of minimizing the
total cost, while successfully maintaining a sub-
linear growth of accumulative risk constraint vi-
olation. We support our theoretical results by
demonstrating our algorithm on a simple simu-
lated robotics reactive control task.

1. Introduction

The topic of Safe Sequential Decision Making recently has
received a lot of attention (Amodei et al., 2016) and ﬁnds its
importance in different applications ranging from robotics,
artiﬁcial intelligence and clinical trials. Designing reactive
controls for mobile robots requires reasoning and making
safe decisions so that robots can minimize the risk of dan-
gerous obstacle collision. In clinical trials the risk of the

1Robotics Institute, Carnegie Mellon University, USA
2Microsoft Research, Redmond, USA. Correspondence to: Wen
Sun <wensun@cs.cmu.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

side-effect of a new treatment must be taken into considera-
tion for patients’ safety. In general these applications will
require the risk (probability of collision, level of side-effect)
to be less than some safe-threshold. In this work we study
safe sequential decision making under the setting of adver-
sarial contextual bandits with sequential risk constraints.

The Contextual Bandits problem (Langford & Zhang, 2008)
is a classic framework for studying sequential decision mak-
ing with rich contextual information. In each round, given
the contextual information, the learner chooses an arm (i.e.,
a decision) to pull based on the history of the interaction
with the environment, and then receives the reward asso-
ciated with the pulled arm. For the special case where
contexts and rewards are i.i.d sampled from ﬁxed unknown
distributions, there exists an oracle-based computationally
efﬁcient algorithm (Agarwal et al., 2014) that achieves near-
optimal regret rate. For adversarial contexts and rewards,
EXP4 (Auer et al., 2002) and EXP4.P (Beygelzimer et al.,
2011) are state-of-the-art algorithms, which achieve near-
optimal regret rate, but are not computationally efﬁcient
in general. Recently, a few authors have started to incor-
porate global constraints into multi-armed bandit and con-
textual bandits problem where the goal of the learner is to
maximize the reward while satisfying a global constraint
to some degree. Previous work considered special cases
such as single resource budget constraint (Ding et al., 2013;
Madani et al., 2004) and multiple resources budget con-
straints (Badanidiyuru et al., 2013). Resourceful Contextual
Bandits (Badanidiyuru et al., 2014) ﬁrst introduced resource
budget constraint to contextual bandits. Later on, the au-
thors in (Agrawal et al., 2015) generalize the setting to a
setting with a global convex constraint and a concave ob-
jective. Recently (Agrawal & Devanur, 2015) introduce a
Upper Conﬁdence Bound (UCB) style algorithm for linear
contextual bandits with knapsack constraints. The settings
considered in these previous work mainly focused on the
stochastic case where contexts and rewards are i.i.d, and
the single constraint is pre-ﬁxed (i.e., time-independent,
non-adversarial).

We introduce sequential risk constraints in contextual ban-
dits: in each round, the environment prepares a context, a

They can be very computationally efﬁcient for special cases

of expert class (Beygelzimer et al., 2011)

Safety-Aware Algorithms for Adversarial Contextual Bandit

cost for each arm, and additionally a risk for each arm. The
learner pulls an arm and receives the cost and risk associated
with the pulled arm. Given a pre-deﬁned risk threshold, the
learner ideally needs to make safe decisions such that the
risk is no larger than the safe-threshold in every round, while
minimizing the cumulative cost simultaneously. Such adver-
sarial risk functions are common in real world applications:
when a robot is navigating in an unfamiliar environment, risk
(e.g., probability of being collision with unseen obstacles)
and reward of taking a particular action may dependent on
the robot’s current state and the environment (or the whole
history of states), while the sequential states visited by the
robot are unlikely to be i.i.d or even Markovian.

To address the adversarial contextual bandit with sequential
risk constraints problem, we ﬁrst study the problem of on-
line convex programming (OCP) with sequential constraints,
where at each round, the environment prepares a convex loss,
and additionally a convex constraint. The learner wants to
minimize its cumulative loss while satisfying the constraints
as best as possible. The online learning with constraints set-
ting is ﬁrst studied in (Mannor et al., 2009) in a two-player
game setting. Particularly the authors constructed a two-
player game where there exists a strategy for the adversary
such that among the strategies of the player that satisfy the
constraints on average, there is no strategy that can achieve
the no-regret property in terms of maximizing the player’s
reward. Later on (Mahdavi et al., 2012; Jenatton et al.,
2016) considered the online convex programming frame-
work where they introduced a pre-deﬁned global constraint
and designed algorithms that achieve no-regret property
on loss functions while maintaining the accumulative con-
straint violation grows sublinearly. Though the work in
(Mahdavi et al., 2012; Jenatton et al., 2016) did not consider
time-dependent, adversarial constraints, we ﬁnd that their
online gradient descent (OGD) (Zinkevich, 2003) based al-
gorithms are actually general enough to handle adversarial
time-dependent constraints. We ﬁrst present a family of
online learning algorithms based on Mirror Descent (OMD)
(Beck & Teboulle, 2003; Bubeck, 2015), which we show
achieves near-optimal regret rate with respect to loss and
maintains the growth of total constraint violation to be sub-
linear. With a speciﬁc design of a mirror map, our meta
algorithm reveals a similar algorithm shown in (Mahdavi
et al., 2012).

The mirror descent based algorithms in the full information
online learning setting also enables us to derive a Multiplica-
tive Weight (MW) update procedure by choosing negative
entropy as the mirror map. Note that MW based update pro-
cedure is important when extending to partial information
contextual bandit setting. The MW based update procedure

To be consistent to classic Online Convex Programming set-

ting, we consider minimizing cost

can ensure the regret is polylogarithmic in the number of ex-
perts , instead of polynomial in the number of experts from
using the OGD-based algorithms (Mahdavi et al., 2012;
Jenatton et al., 2016). Leveraging the MW update proce-
dure developed from the online learning setting, we present
algorithms called EXP4.R (EXP4 with Risk Constraints)
and EXP4.P.R (EXP4.P with Risk Constraints). EXP4.R
can achieve near optimal regret in terms of minimizing cost
while ensuring the average of the accumulative risk is no
larger than the pre-deﬁned threshold. For EXP4.P.R, we
further introduce a tradeoff parameter that shows how one
can trade between the risk violation and the regret of cost.

The rest of the paper is organized as follows. We introduce
necessary deﬁnitions and problem setup in Sec. 2. We then
deviate to the full information online learning setting where
we introduce sequential, adversarial convex constraints in
Sec. 3. In Sec. 4, we move to contextual bandits with risk
constraints setting to present and analyze the EXP4.R and
EXP4.P.R algorithm.

2. Preliminaries

2.1. Deﬁnitions

A function R(x) :
to some norm
↵

k·k
+ such that:

2R
R(x)

 

X!R

it is strongly convex with respect
if and only if there exists a constant

R(x0) +

R(x0)T (x

x0) +

r

 

↵
2 k

x

2.

x0k

 

Given a strongly convex function R(
gence DR(

) :

,

·

is deﬁned as follows:

), the Bregman diver-

·

·
DR(x, x0) = R(x)

X⇥X!R

R(x0)

 

  r

R(x0)T (x

x0).

 

·

)



2.2. Online Convex Programming with Constraints
d,
In each round, the learner makes a decision xt 2X ✓R
) and a convex
and then receives a convex loss function `t(
0. The learner suffers
constraint in the form of ft(
·
loss `t(x). The work in (Mahdavi et al., 2012) considers
a similar setting but with a known, pre-deﬁned global con-
straint. Instead of projecting the decision x back to the
), (Mah-
convex set induced by the global constraint f (
davi et al., 2012) introduces an algorithm that achieves no-
regret on loss while satisfying the global constrain in a
long-term perspective. Since exactly satisfying adversarial
constraint in every round is impossible, we also consider
constraint satisfaction in a long-term perspective. Formally,
xt}t made by the learner, we
for the sequence of decisions
{
T
t=1 ft(xt) as the cumulative constraint violation
deﬁne
and we want to control the growth of the cumulative con-
T
o(T ), so
t=1 ft(xt)
straint violation to be sublinear:
2
T
that for the long-term constraint 1
t=1 ft(x), we have
T
P
limT

T
t=1 ft(xt)

P

0.

·

1
T



P

!1

P

Safety-Aware Algorithms for Adversarial Contextual Bandit

We place one assumption on the decision set
that the decision set
we have x
: ft(x)

that can satisfy all constraints:
t

: we assume
is rich enough such that in hindsight,

2
(e.g., default conservative, safe

X
decisions ). We compete with the optimal decision x⇤
that minimizes the total loss in hindsight:

2X


x
{

2O

O

0,

=

˙=

X

X

8

;

}6

x⇤ = arg min
⇠O

x

`t(x).

T

t=1
X

(1)

{

x



O

O

P

0 ˙=

2X

T
t ft(x)

Though one probably would be interested in compet-
ing against the best decision from the set of decisions
:
that satisfy the constraints in average:
0
(1/T )
, in general it is impossible to com-
}
pete against the best decision in
0).
0 in hindsight (
The following proposition adapts the discrete 2-player game
from proposition 4 in (Mannor et al., 2009) for the OCP
with adversary constraints and shows the learner is unable
to compete against
0:
Proposition 2.1. There exists a decision set
of convex loss functions
vex constraints
decisions
strain as lim supt
ing against

, a sequence
X
`t(x)
, and a sequence of con-
}
{
0
, such that for any sequence of
}
, if it satisﬁes the long-term con-
t
0, then if compet-
i=1 fi(xi)
0, the regret grows at least linearly:


x1, ..., xt, ...
}
1
t

ft(x)

O⇢O

!1

O



{

{

P

lim sup

t

!1

`i(xi)

min
x
2O0

 

`i(x)

=⌦( t).

(2)

t

i=1
X

 

O

t

i=1
X

 

The proof of the proposition can be found in Sec. A in Ap-
pendix. Hence in the rest of the paper, we have to restrict to
. The average regret of loss R` and the average constraint

O
violation Rf are deﬁned as:

R` =

`t(xt)

`t(x⇤)], Rf =

ft(xt)].

T

1
T

[

t=1
X

T

 

t=1
X

T

1
T

[

t=1
X

)

B



D

ft(

2X

is bounded as
is bounded as
`t(

2R
+, the loss function is bounded as
X
+, the constraint is bounded

We will assume the decision set
+, x
DR(x1, x2)
maxx1,x22X
)
x
·
|
k 
k
2R
+ and
F
2R
the gradient of the loss and constraint is also bounded as
{krx`t(x)
max
k·k ⇤
k⇤
is the dual norm with respect to
. Note
the setting with a global constraint considered in (Mahdavi
et al., 2012) is a special case of our setting. Set ft = f ,
o(T ), by Jensen’s
where f is the global constraint. If Rf 2
T
inequality, we have f (
t=1 f (xt)/T =

o(T )/T

|
2R
+, where

krxft(x)

2R
deﬁned for

T
t=1 xt/T )

k⇤}

0, as T

k·k

|

G

X

|

·

,

!

.
! 1
P

P

2.3. Contextual Bandits with Risk Constraints

, a K-dimensional cost vector ct 2

Formally, at every time step t, the environment generates a
[0, 1]K,
context st 2S
[0, 1]K. The environment then reveals
and a risk vector rt 2
the context st to the learner, and the learner then proposes
 ([K]) over all arms. Fi-
a probability distribution pt 2
[K] according to
nally the learner samples an action at 2
pt and receives the cost and risk associated to the chosen
action: ct[at] and rt[at] (we denote c[i] as the i’th element
of vector c). The learner ideally wants to make a sequence
of decisions that has low accumulative cost and also satis-
ﬁes the constraint that related to the risk: pT
  where
 

[0, 1] is a pre-deﬁned threshold.

t rt 

2

2

⇧:

S!

⇡i}
{

⇡i(s)
{

We address this problem by leveraging experts’ advice.
N
Given the expert set ⇧ that consists of N experts
i=1,
where each expert ⇡
 ([K]), gives advice
by mapping from the context s to a probability distribu-
tion p over arms. The learner then properly combines
N
i=1 (e.g., compute the average
the experts’ advice
}
N
i=1 ⇡i(s)/N ) to generate a distribution over all arms.
With risk constraints, distributions over policies in ⇧ could
P
be strictly more powerful than any policy in ⇧ itself. We
aim to compete against this more powerful set. Given any
distribution w
 (⇧), the mixed policy resulting from w
can be regarded as: sample policy i according to w and then
sample an arm according to ⇡i(s). Though we do not place
any statistical assumptions (e.g., i.i.d) on the sequence of
cost vectors
, we assume the pol-
icy set ⇧ is rich enough to satisfy the following assumption:

and risk vectors

rt}
{

ct}

2

{

Assumption 2.2. The set of distributions from  (⇧) whose
mixed policies satisfy all risk constraints in expectation is
non-empty:

˙=

w
{

P

2

 (⇧) : Ei

w,j

⇠

⇠

⇡i(st)rt[j]

 ,

t

8

}6

=

.

;



Namely we assume that the distribution set  (⇧) is rich
enough such that there always exists at least one mixed pol-
icy that can satisfy all risk constraints in hindsight. Similar
to the full information setting, competing against the set of
mixed policies that satisfy the constraint on average, namely
T
,
t=1 Ei
P
}
is impossible in the partial information setting. Hence we
deﬁne the best mixed policy in hindsight as:

⇡i(st)rt[j]/T

 (⇧) :

0 =

P

w,j



w

2

 

{

⇠

⇠

w⇤ = arg min
2P

w

Ei

w,j

⇠

⇠

⇡i(st)ct[j].

(3)

T

t=1
X

For contextual bandits with sequential risk constraints, let
[K] be a ﬁnite set of K arms,
be the space of contexts.

T
Given a sequence of decisions
t=1 generated from some
{
algorithm, we deﬁne average regret and average constraint

at}

S

Safety-Aware Algorithms for Adversarial Contextual Bandit

violation as:

Rc =

ct[at]

Ei

⇠

w⇤,j

⇠

⇡i(st)ct[j]

,

⇤

T

T

1
T

1
T

t=1
X
T

⇥

t=1
X

⇥

 

t=1
X

(rt[at]

 )

.

 

⇤

Rr =

The goal is to either minimize Rc and Rr in high prob-
ability or minimize the expected version ¯Rc ˙=E[Rc] and
¯Rr ˙=E[Rr] where the expectation is over the randomness of
the algorithms.

3. Online Learning with Sequential

Constraints

The online learning with adversarial sequential constraints
setting is similar to the one considered in (Mannor et al.,
2009; Jenatton et al., 2016) except that they only have a
pre-deﬁned ﬁxed global constraint. However we ﬁnd that
their algorithms and analysis are general enough to extend
to the online learning with adversarial sequential constraints.
In (Mannor et al., 2009; Jenatton et al., 2016), the algo-
rithms introduce a Lagrangian dual parameter and perform
online gradient descent on x and online gradient ascent on
the dual parameter. Since in this work we are eventually
interested in reducing the contextual bandit problem to the
full information online learning setting, simply adopting the
OGD-based approaches from (Mannor et al., 2009; Jenat-
ton et al., 2016) will not give a near optimal regret bound.
Hence, developing the corresponding Multiplicative Weight
(MW) update procedure is essential for a successful reduc-
tion from adversarial contextual bandit to full information
online learning setting.

3.1. Algorithm

We use the same saddle-point convex concave formation
from (Mannor et al., 2009; Jenatton et al., 2016) to design a
composite loss function as:

Lt(x,  ) = `t(x) +  ft(x)

 

 µ
2

 2,

(4)

2R

+. Alg. 1 leverages online mirror descent
where  
(OMD) for updating the x (Line 6 and Line 7) and online
gradient ascent algorithm for updating   (Line 8). Note
2 as the regularization
that if we use the square norm
k
function R(x) in Alg. 1, we reveal the gradient descent
based update procedure from (Mahdavi et al., 2012).

x

k

Algorithm 1 OCP with Sequential Constraints via OMD
1: Input: Learning rate µ, mirror map R, parameter  

and  1 = 0.

(used in

Lt).

2: Initialize x1 2X
3: for t = 1 to T do
4:
5:
6:

Learner proposes xt.
Receive loss function `t and constraint ft.
Set ˜xt+1 such that
rxLt(xt,  t).
µ
Projection: xt+1 = arg minx
Update  t+1 = max

R(˜xt+1) =

2X
0,  t + µ

r

r

DR(x, ˜xt+1).
r Lt(xt,  t))

}

.

{

R(xt)

 

7:
8:
9: end for

the number of rounds T is given and we consider the asymp-
totic property of Alg. 1 when T is large enough.

{L

The algorithm should be really understood as running two
no-regret procedures: (1) Online Mirror Descent on the se-
}t with respect to x and (2) Online
(x,  t)
quence of loss
Gradient ascent on the sequence of loss
}t with
{L
respect to  . Instead of digging into the details of Online
Mirror Descent and Online Gradient ascent, our analysis
simply leverages the existing analysis of online mirror de-
scent and online gradient ascent and show how to combine
them to derive the regret bound and constraint violation
bound for Alg. 1.

(xt,  )

B

·
T (D2+G2/↵) and   = 2G2

Theorem 3.1. Let R(
Set µ =
`t(x), convex constraint ft(x)
that
;
the following property:

) be a ↵-strongly convex function.
↵ . For any convex loss
0, under the assumption
, the family of algorithms induced by Alg. 1 have

q

O6



=

R` 

O(1/pT ), Rf 

O(T  

1/4).

Proof Sketch of Theorem 3.1. Since the algorithm runs on-
line mirror descent on the sequence of loss
}t
with respect to x, using the existing results of online mirror
descent (e.g., Theorem 4.2 and Eq. 4.10 from (Bubeck,
2015)), we know that for the computed sequence

{Lt(x,  t)

xt}t:
{

T

t=1
X

(

Lt(xt,  t)

 L t(x,  t))

DR(x, x1)
µ

+

µ
2↵



krxLt(xt,  t)

,

2
k
⇤

(5)

T

t=1
X

3.2. Analysis of Alg. 1

Throughout our analysis, we assume the regularization func-
tion R(x) is ↵-strongly convex. For simplicity, we assume

2X

for any x
. Also, we know that the algorithm runs
{Lt(xt,  )
online gradient ascent on the sequence of loss
}t
with respect to  , using the existing analysis of online gra-
dient descent (Zinkevich, 2003), we have for the computed

Safety-Aware Algorithms for Adversarial Contextual Bandit

sequence of

 t}t:

{
T

Lt(xt,  )

t=1
X
1
µ



 2 +

µ
2

T

 

T

t=1
X
@

t=1
X

 

Lt(xt,  t)

Lt(wt,  t)
@ t

2

,

 

(6)

(7)

for any  

0.

 

2

t (xt) + 2 2µ2 2

Note that for (@
2f 2
krxLt(xt,  t)

k
krxLt(xt,  t)
k
2G2(1 +  2
t ),


⇤

Lt(xt,  t)/@ t)2 = (ft(xt)
, we also have:

2D2 +  2µ2 2

t 

 µ  t)2


t . Similarly for

 

2
⇤ 

2

kr

`t(xt)

2

k

⇤

+ 2

 tr

k

ft(xt)

2
k
⇤

where we ﬁrst used triangle inequality for
k⇤
+.
and then use the inequality of 2ab
Note that we also assumed that the norm of the gradients are
+.
bounded as max(
)
Now sum Inequality 5 and 6 together, we get:

krxLt(xt,  t)
a, b
2R
8

a2 + b2,

ft(xt)

`t(xt)

2R

kr

kr

k⇤

k⇤

G





,

µ(D2 +  2µ2 2
t )

 L t(x,  t)

Lt(xt,  )
2DR(x, x1) +  2
2µ

+

t
X



t
X
(1 +  2
t )

µG2
↵

+

t
X

=

2DR(x, x1) +  2
2µ

+ T µ(D2 +

G2
↵

)

+ µ( 2µ2 +

G2
↵

)

 2
t .

X

minx

with µ =

DR(x, x1)/(T (D2 + G2/↵)). To upper bound
t ft(xt), we ﬁrst observe that we can lower bound
p
T
2F T , where F is the
t=1 `t(xt)
P
2F T
). Replace
upper bound of `(
 
P
P
in Eq. 9, and set   = (
t ft(xt))/( µT + 1/µ) (here we
0, otherwise we prove the theorem),
t ft(xt)
assume
P
we can show that:
P

T
t=1 `t(x)

   
t `t(xt)

`t(x) by

P

 

 

 

·

ft(xt))2

DR(x, x1) + 2(D2 +

8G2
↵



T

(

t=1
X

+ T 3/2

8F 2G2/↵

G2
↵

)T

(10)

p

The RHS of the above inequality is dominated by the term
T 3/2
8F 2G2/↵ when T approaches to inﬁnity. Hence,
we get
p

T
t=1 ft(xt) = O(T 3/4).

P

x

2
As we can see that if we replace R(x) with
2 in Alg. 1,
k
we reveal a gradient descent based update procedure that is
almost identical to the one in (Mahdavi et al., 2012). When
x is restricted to a simplex, to derive the multiplicative
weight update procedure, we replace R(x) with the negative
i x[i] ln(x[i]) and we can achieve
entropy regularization
the following update steps for x:

k

xt+1[i] =

P
xt[i] exp(
µ
 
d
j=1 xt[j] exp(

rxLt(xt,  t)[i])
µ
 

rxLt(xt,  t)[j])

.

We refer readers to (Shalev-Shwartz, 2011; Bubeck, 2015)
for the derivation of the above equation.

P

(8)

4. Contextual Bandits With Risk Constraints

Substitute the form of

Lt into the above inequality, we have:

(`t(xt)

`t(x)) +

( ft(xt)

 tft(x))

t
X

+

 µ
2

 

t
X

 2
t  
G2
↵

t
X
 µT
 2
2

2DR(x, x1) +  2
2µ



+ T µ(D2 +

) + µ( 2µ2 +

 2
t .

(9)

Note that from our setting of µ and   we can verify that
 
t in the
above inequality.

 2µ2 + G2/↵, we can remove the term

t  2

 

t , to upper bound the regret on loss

Without the term
`t, let us set   = 0 and x = x⇤, we get:
P

t  2

(`t(xt)

`t(x⇤))

 

2DR(x, x1)
2µ



+ T µ(D2 + G2/↵)

 

G2
↵

)

t
X

P

When contexts, costs and risks are i.i.d sampled from some
unknown distribution, then our problem setting can be re-
garded as a special case of the setting of contextual bandit
with global objective and constraint (CBwRC) considered
in (Agrawal et al., 2015). In (Agrawal et al., 2015), the
algorithm also leverages Lagrangian dual variable. The dif-
ference is that in i.i.d setting the dual parameter is ﬁxed with
respect to the underlying distribution and hence it is possible
to estimate the dual variable. For instance one can uniformly
pull arms with a ﬁxed number of rounds at the beginning to
gather information for estimating the dual variable and then
use the estimated dual variable for all remaining rounds.
However in the adversarial setting, this nice trick will fail
since the costs and risks are possibly sampled from a chang-
ing distribution. We have to rely on OCP algorithms to keep
updating the dual variable to adapt to adversarial risks and
costs.

t
X
2



DR(x, x1)T (D2 + G2/↵) = O(pT ),

4.1. Algorithm

p
For simplicity we assumed T is large enough to be larger than

any given constant.

Our algorithm EXP4.R (EXP4 with Risk constraints)
(Alg. 2) extends the EXP4 (Auer et al., 2002) algorithm

Safety-Aware Algorithms for Adversarial Contextual Bandit

Algorithm 2 EXP4 with Risk Constraints (EXP4.R)
1: Input: Policy set ⇧.
2: Initialize w1 = [1/N, ..., 1/N ]T and  1 = 0.
3: for t = 1 to T do
4:
5:
6:
7:
8:
9:

Receive context st.
Query experts to get advice ⇡i(st),
Set pt =
Draw action at randomly from distribution pt.
Receive cost ct[at] and risk rt[at].
Set the cost vector ˆct 2
ˆrt 2

RK as follows: for all i

N
i=1 wt[i]⇡i(st).

[N ].

[K]

P

2

2

8

i

RK and the risk vector

ˆct[i] =

ct[i] (at = i)
pt[i]

,

ˆrt[i] =

rt[i] (at = i)
pt[i]

.

10:

For each expert j

[N ], set:

2

ˆyt[j] = ⇡j(st)T ˆct,

ˆzt[j] = ⇡j(st)T ˆrt.

11:

Compute wt+1, for i

[

⇧

]:

wt+1[i] =

|

|

2
wt[i] exp
 
⇧
|j=1 wt[j] exp
|

 

12:

Compute  t+1:

P

µ(ˆyt[i] +  t ˆzt[i])

µ(ˆyt[j] +  t ˆzt[j])

 

 

 

.

 

 t+1 = max

0,  t + µ(wT

{

t ˆzt  

 

 

 µ  t)

.

}

13: end for

to carefully incorporating the risk constraints for updating
the probability distribution w over all policies. At each
round, it ﬁrst uses the common trick of importance weight-
ing to form unbiased estimations of cost vector ˆc and risk
vector ˆr. Then the algorithm uses the unbiased estimations
of cost vector and risk vector to form unbiased estimations
of the cost ˆy[i] and risk ˆz[i] for each expert i. EXP4.R then
starts behaving differently than EXP4. EXP4.R introduces
a dual variable   and combine the cost and risk together
2  2. We then
as
use Alg. 1 with the negative entropy regularization as a
black box online learner to update the weight w and the dual
variable  .

Lt(w,  ) = wT ˆyt +  (wT ˆzt  

 )

 

 µ

The proposed algorithm EXP4.R in general is computation-
ally inefﬁcient since similar to EXP4 and EXP4.P, it needs
to maintain a probability distribution over the policy set.
Though there exist computationally efﬁcient algorithms for
stochastic contextual bandits and hybrid contextual bandits,
we are not aware of any computationally efﬁcient algorithm
for adversarial contextual bandits, even without risk con-
straints.

4.2. Analysis of EXP4.R

We provide a reduction based analysis for EXP4.R by
ﬁrst reducing EXP4.R to Alg. 1 with negative entropic
regularization. For the following analysis, let us deﬁne
yt[j] = ⇡j(st)T ct and zt[j] = ⇡j(st)T rt, which stand for
the expected cost and risk for policy j at round t.

 µ

 

 )

2  2.
Lt(w,  ) = wT ˆyt +  (wT ˆzt  
Let us deﬁne
The multiplicative weight update in Line 11 can be re-
garded as running Weighted Majority on the sequence of
}t, while the update rule for   in Line 12
{Lt(w,  t)
loss
can be regarded as running Online Gradient Ascent on the
{Lt(wt,  )
sequence of loss
}t. Directly applying the classic
analysis of Weighted Majority (Shalev-Shwartz, 2011) on
wt}t and the classic
the generated sequence of weights
analysis of OGD (Zinkevich, 2003) on the generated se-
 t}t, we get the following lemma:
quence of dual variables
{
i x[i] ln(x[i]) as
Lemma 4.1. With the negative entropy
the regularization function for R(x) in Alg. 1, running Alg. 1
on the sequence of linear loss functions `t(w) = wT ˆyt and
linear constraint ft(w) = wT ˆzt  

0, we have:

P



 

{

Lt(wt,  )

 

Lt(w,  t)



 2
µ

+

ln(

)

⇧
|
µ

|

T

t=1
X

T

t=1
X
⇧
|

|

T

+

µ
2

t=1 ⇣ 
X
+ (wT
t ˆzt  

i=1
X
 

 

wt[i](2ˆyt[i]2 + 2 2

t ˆzt[i]2)

 µ  t)2

.

⌘

 

(11)

We defer the proof of the above lemma to Appendix. The
EXP4.R algorithm has the following property:

Theorem 4.2. Set µ =
3K. Assume

=

ln(

⇧

)/(T (K + 4)) and   =

|

|

p

. EXP4.R has the following property:
P6
;
¯Rc = O(
¯Rr = O(T  
p

K ln(
|
1/4(K ln(

))1/4).

)/T ),

⇧

⇧

|

|

|



t ˆyt = ct[at]

Proof Sketch of Theorem 4.2. The proof consists of a com-
bination of the analysis of EXP4 and the analysis of The-
orem 3.1. We defer the full proof in Appendix C. We
ﬁrst present several known facts. First, we have wT
t ˆzt =
rt[at]

For Eat⇠
 )2

is
pt ˆyt = yt and Eat⇠

1 and wT
1.
pt (wT
 )2, we can show that: Eat⇠
t ˆzt  
2 + 2 2
4.


It
Eat⇠
also true that Eat⇠
pt
⇧
|i=1 wt[i]ˆzt[i]2
Eat⇠
|
Now take expectation with respect to the sequence of deci-

⇧
|i=1 wt[i]ˆyt[i]2
|
K.

to
pt ˆzt = zt.

that
It
is
K and

straightforward

t ˆzt  

pt (wT

show

also

P




pt

P

Safety-Aware Algorithms for Adversarial Contextual Bandit

The whole framework of the algorithm is similar to the
one of EXP4.R, with only one modiﬁcation. For notation
simplicity, let us deﬁne ˜xt[i] = ˆyt[i] +  t ˆzt[i]. Note that
˜xt[i] is an unbiased estimate of yt[i] +  tzt[i]. EXP4.P.R
modiﬁes EXP4.R by replacing the update procedure for
wt+1 in Line 11 in Alg. 2 with the following update step:

(12)

wt+1[i] =

wt[i] exp(
 
⇧
|j=1 wt[j] exp(
|

µ(˜xt[i]



 
µ(˜xt[j]

 


P
 

K
k=1

⇡i(st)[k]
pt[k]

))

K
k=1

⇡j (st)[k]
pt[k]

))

,

sions

at}t on LHS of Inequality 11:
{

E

at}t

{

Lt(wt,  )

 L t(w,  t)

i

T

t=1 h
X

T

t=1
X
+

⇥
 µ
2

 2
t

 µT
2

 2

 

⇤

=

Ect[at] +  (Ert[at]

 )

 

 

yT
t w

 t(zT

t w

 

 )

 

Now take the expectation with respect to a1, ..., aT on the
RHS of inequality 11, we can get:

E[RHS of Inequality 11]

+

ln

⇧
|
µ

|

 2
µ
T



t=1
X

+µT (K + 4) + µ(K +  2µ2)

 2
t .

(13)

Now we can chain Eq. 12 and 13 together and use the
same technique that we used in the analysis of Theorem 3.1.
Chain Eq. 12 and 13 together and set w to w⇤ and   = 0, it
is not hard to show that:

T

T

E

ct[at]

yT
t w⇤

 

t=1
X

 

t=1
X
2

 


|

|

p

where µ =
t(Ert[at]
(

P

T

 

p

ln(

⇧

)T (K + 4) = O(

T K ln(

⇧

)),

|

|

ln(

⇧

)/(T (K + 4)).

Set   =

 ))/( µT + 2/µ), we can get:

|

|

p

  = T  
⌫:
1

 

(

(Ert[at]

 ))2

 



(2 µT + 4/µ)

2T

t=1
X

+ 2

ln(

⇧

)T (K + 4)

|

|

⌘

⇣

(14)

Substitute µ =
equation, it is easy to verity that:

ln(

⇧

|

|

)/(T (K + 4)) back to the above

p

p

T

t=1
X

(

(Ert[at]

 ))2

O

T 3/2(K ln(

⇧

))1/2

.

(15)

 



|

|

 

 

4.3. Extension To High-Probability Bounds

The regret bound and constraint violation bound of EXP4.R
hold in expectation. In this section, we present an algo-
rithm named EXP4.P.R, which achieves high-probability
regret bound and constraint violation bound. The algo-
rithm EXP4.P.R, as indicated by its name, is built on the
well-known EXP4.P algorithm. In this section for the conve-
nience of analysis, without loss of generality, we are going to
assume that for any cost vector c and risk vector r, we have
c[i]

[K], and  

1, 0], r[i]

1, 0].

1, 0],

i

[

[

[

2

 



 

8

2

2

 

P

P

where  is a constant that will be deﬁned in the analy-
sis of EXP4.P.R. We refer readers to Appendix D for the
full version of EXP4.P.R. Essentially, similar to EXP3.P
to
and EXP4.P, we add an extra term

⇡j (st)[k]
K
k=1
pt[k]
˜xt[i]. Though ˜xt[i]
is not an unbi-
ased estimation of yt[i] +  tzt[i] anymore, as shown in
P
Lemma D.3 in Appendix D.2, it enables us to upper bound
t yt[i] +  tzt[i] with


 
⇡j (st)[k]
pt[k]

t ˜xt[i]

⇡j (st)[k]
pt[k]

using

K
k=1

K
k=1

P

 





high probability.
P
P
We show that EXP4.P.R has the following performance
guarantees:

P

 

Theorem 4.3. Assume
P6
ln(
⇧
)
(3K+4)T ,  =
|

2
, and
✏+1/2K, we have that with probability at least

(0, 1/2), ⌫
/⌫)

2
(1+T ✏) ln(
T K

(0, 1), set µ =

. For any ✏

q

q

=

;

⇧

|

|

|

Rc = O(

T ✏

 

1K ln(

⇧

/⌫)),

Rr = O(T  
p

✏/2

(16)

|
K ln(

|
⇧

)).

|

|

p

|

|

⇧

!

p

T K ln(

The above theorem introduces a trade-off between the regret
of cost and the constraint violation. As ✏
0, we can
see that the regret of cost approaches to the near-optimal
), but the average risk constrain violation
one
approaches to a constant. Based on speciﬁc applications,
one may set a speciﬁc ✏
(0, 0.5) to balance the regret and
the constraint violation. For instance, for ✏ = 1/3, one can
show that the cumulative regret is O(T 2/3
)) and
⇧
|
the average constraint violation is ˜O(T  
1/6). Note that if
p
one simply runs EXP4.R proposed in the previous section, it
is impossible to achieve the regret rate O(T 2/3
)) in
a high probability statement. As shown in (Auer et al., 2002),
for EXP4 the cumulative regret on the order of O(T 3/4) was
possible.

K ln(

ln(

p

⇧

2

|

|

|

The difﬁculty of achieving a high probability statement
with optimal cumulative regret O(
)) and cu-
mulative constraint violation rate ˜O(T 3/4) is from the La-
grangian dual variable  . The variance of ˆzt is propor-
tional to 1/pt[at]. With  t, the variance of  t ˆzt scales as

T K ln(

p

⇧

|

|

EXP4.R becomes the same as EXP4 when we set all risks and

  to zeros.

Safety-Aware Algorithms for Adversarial Contextual Bandit

(a) Environment

(b) Cost

(c) Risk

Figure 1. (a) Environment set up and a safe trajectory from the initial position to the goal region (green). (b,c) The performance of EXP4.R
under different values of risk thresholds (y-axis is in log scale).

|

|

 

 2
t /pt[at]. As we show in Lemma D.2 in Appendix D.2,
 t could be as large as
/( µ). Depending on the value
of  , µ,  t could be large, e.g., ⇥(pT ) if   is a constant
and µ =⇥(1 /pT ). Hence compared to EXP4.P, the La-
grangian dual variable in EXP4.P.R makes it more difﬁ-
cult to control the variance of ˜xt, which is an unbiased
estimation of yt[i] +  tzt[i]. This is exactly where the
trade-off ✏ comes from: we can tune the magnitude of   to
control the variance of ˜xt and further control the trade-off
between regret and risk violation . How to achieve total re-
gret O(
)) and cumulative constraint violation
O(T 3/4) in high probability is still an open problem.

T K ln(

⇧

|

|

p

5. Simulation

P

We test our algorithms on a simple synthetic robotics reac-
tive control task. The 2-D environment, shown in Fig. 1a is
set up as follows. We divide the environment into 9 cells
and each cell is associated by a waypoint (black dot or black
circle). The black dots are associated with risk 1 and stands
for dangerous areas while the circle dots are associated with
risk zeros (i.e., safe regions). The state of the agent is its
2-D position and for each state, we compute the RBF feature
s with respect 9 waypoints and then normalize the feature s
9
i=1 s[i] = 1. The risk for feature s is the inner
such that
product between s and the 9-dimension risk vector. The
d2) (we simply negate
reward of action a is max(0, d1  
the reward to fake a cost to feed to our algorithms), where
d1 stands for the old distance to goal and d2 is the new
distance to goal after taken action a (hence a non-linear
reward mapping). We designed 4 actions for the agent: it
can move up, right, left, and down with a ﬁxed small con-
stant distance. We design 49 experts where each expert ⇡i
is a 9 dimensional vector, where the element in each di-
mension belongs to the action set (Left,Right, Up, Down)
(namely each expert ⇡i suggests an action for every way-
point). Given a feature s, the suggestion ⇡i(s) is computed
k,⇡i[k]=i s[k]. Note that this is similar to the
as ⇡i(s)[i] =
setting consider in (Beygelzimer et al., 2011). Note that the
class of experts can be represented by a depth-9 tree with 4
branches. Hence computational efﬁcient weighted majority

P

algorithm implementation could be used here (Cesa-Bianchi
& Lugosi, 2006). But in this work, we temporarily lighten
computational burden by paralleling computing. We initial-
ize the weight over all experts uniformly and let the EXP4.R
algorithm picks decision for the agent. The agent executes
the decision, receives risk and cost. The agent is reset to the
initial position once it reaches the goal region or outside of
the map too much.

Fig. 1b and 1c show the performance of EXP4.R under
different values of risk threshold  . We observe that EXP4.R
can ensure that the average risk converges to the threshold
 , while keep decreasing the average cost simultaneously.
A lower risk threshold usually results a slower decrease in
the average cost, which is consistent to the theorems since
a smaller risk threshold leads to a smaller
. Compare to
the performance of EXP4, we see the EXP4 can offer faster
decrease rate for cost (as it can compete against the whole
policy class set), but it cannot control risk, e.g, it discovers
paths that cut through the middle high-risk cell.

P

6. Conclusion

We study safe sequential decision making problem under
the format of adversarial contextual bandits with adversarial
sequential risk constraints. We provide safety-aware algo-
rithms that can satisfy the long-term risk constraint while
achieve near-optimal regret in terms of minimizing costs.
The proposed two algorithm, EXP4.R and EXP4.P.R, are
built on the existing EXP4 and EXP4.P algorithms: EXP4.R
achieves near-optimal regret and satisﬁes the long-term con-
straint in expectation while EXP4.P.R achieves similar the-
oretical bounds with high probability, with a tradeoff that
can trade the constraint violation for regret and vice versa.
Same as EXP4 and EXP4.P, the computational complexity
of a simple implementation of our algorithms per step is
linear with respect to the size of the expert class. However
for expert class that has special structures such as trees (as
we used in our simulation) or graphs, one can usually design
efﬁcient implementation. We leave the efﬁcient implemen-
tation for real robotics control as a future work.

Safety-Aware Algorithms for Adversarial Contextual Bandit

Madani, Omid, Lizotte, Daniel J, and Greiner, Russell. The bud-
geted multi-armed bandit problem. In International Conference
on Computational Learning Theory, pp. 643–645. Springer,
2004.

Mahdavi, Mehrdad, Jin, Rong, and Yang, Tianbao. Trading re-
gret for efﬁciency: online convex optimization with long term
constraints. The Journal of Machine Learning Research, 13(1):
2503–2528, 2012.

Mannor, Shie, Tsitsiklis, John N, and Yu, Jia Yuan. Online learning
with sample path constraints. The Journal of Machine Learning
Research, 10:569–590, 2009.

Shalev-Shwartz, Shai. Online Learning and Online Convex Opti-
mization. Foundations and Trends in Machine Learning, 4(2):
107–194, 2011.

Zinkevich, Martin. Online Convex Programming and Generalized
Inﬁnitesimal Gradient Ascent. In International Conference on
Machine Learning (ICML 2003), pp. 421–422, 2003.

Acknowledgement

The research work was done during Wen Sun’s internship at
Microsoft Research, Redmond.

References
Agarwal, Alekh, Hsu, Daniel, Kale, Satyen, Langford, John, Li,
Lihong, and Schapire, Robert. Taming the monster: A fast
and simple algorithm for contextual bandits. In Proceedings of
The 31st International Conference on Machine Learning, pp.
1638–1646, 2014.

Agrawal, Shipra and Devanur, Nikhil R. Linear contextual bandits

with knapsacks. arXiv preprint arXiv:1507.06738, 2015.

Agrawal, Shipra, Devanur, Nikhil R, and Li, Lihong. Contextual
bandits with global constraints and objective. arXiv preprint
arXiv:1506.03374, 2015.

Amodei, Dario, Olah, Chris, Steinhardt, Jacob, Christiano, Paul,
Schulman, John, and Man´e, Dan. Concrete problems in ai safety.
arXiv preprint arXiv:1606.06565, 2016.

Auer, Peter, Cesa-Bianchi, Nicolo, Freund, Yoav, and Schapire,
Robert E. The nonstochastic multiarmed bandit problem. SIAM
Journal on Computing, 32(1):48–77, 2002.

Badanidiyuru, Ashwinkumar, Kleinberg, Robert, and Slivkins,
Aleksandrs. Bandits with knapsacks. In Foundations of Com-
puter Science (FOCS), 2013 IEEE 54th Annual Symposium on,
pp. 207–216. IEEE, 2013.

Badanidiyuru, Ashwinkumar, Langford, John, and Slivkins, Alek-
sandrs. Resourceful contextual bandits. In COLT, pp. 1109–
1134, 2014.

Beck, Amir and Teboulle, Marc. Mirror descent and nonlinear pro-
jected subgradient methods for convex optimization. Operations
Research Letters, 31(3):167–175, 2003.

Beygelzimer, Alina, Langford, John, Li, Lihong, Reyzin, Lev,
and Schapire, Robert E. Contextual bandit algorithms with
supervised learning guarantees. In AISTATS, pp. 19–26, 2011.

Bubeck, S´ebastien. Convex optimization: Algorithms and com-
in Machine Learning, 8

plexity. Foundations and Trends R
 
(3-4):231–357, 2015.

Bubeck, S´ebastien, Cesa-Bianchi, Nicol`o, et al. Regret analysis
of stochastic and nonstochastic multi-armed bandit problems.
Foundations and Trends R
in Machine Learning, 5(1):1–122,
 
2012.

Cesa-Bianchi, Nicolo and Lugosi, G´abor. Prediction, learning,

and games. Cambridge university press, 2006.

Ding, Wenkui, Qin, Tao, Zhang, Xu-Dong, and Liu, Tie-Yan.
Multi-armed bandit with budget constraint and variable costs.
In AAAI, 2013.

Jenatton, Rodolphe, Huang, Jim, and Archambeau, C´edric. Adap-
tive algorithms for online convex optimization with long-term
constraints. ICML, 2016.

Langford, John and Zhang, Tong. The epoch-greedy algorithm
for multi-armed bandits with side information. In Advances in
neural information processing systems, pp. 817–824, 2008.

