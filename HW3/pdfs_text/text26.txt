Input Convex Neural Networks: Supplementary Material

Brandon Amos Lei Xu J. Zico Kolter

A. Additional architectures

A.1. Convolutional architectures

Convolutions are important to many visual structured tasks.
We have left convolutions out to keep the prior ICNN no-
tation light by using matrix-vector operations. ICNNs can
be similarly created with convolutions because the convo-
lution is a linear operator.

The construction of convolutional layers in ICNNs depends
on the type of input and output space. If the input and out-
put space are similarly structured (e.g. both spatial), the
jth feature map of a convolutional PICNN layer i can be
deﬁned by

zj
i+1 = gi

zi ∗ W (z)

i,j + (Sx) ∗ W (x)

i,j + (Sy) ∗ W (y)

(cid:16)

(cid:17)

i,j + bi,j
(20)

where the convolution kernels W are the same size and S
scales the input and output to be the same size as the previ-
ous feature map, and were we omit some of the Hadamard
product terms that can appear above for simplicity of pre-
sentation.

If the input space is spatial, but the output space has another
structure (e.g. the simplex), the convolution over the output
space can be replaced by a matrix-vector operation, such as

zj
i+1 = gi

zi ∗ W (z)

i,j + (Sx) ∗ W (x)

i,j + B(y)

i,j y + bi,j

(cid:16)

(cid:17)

(21)

where the product B(y)

i,j y is a scalar.

B. Exact inference in ICNNs

Although it is not a practical approach for solving the opti-
mization tasks, we ﬁrst highlight the fact that the inference
problem for the networks presented above (where the non-
linear are either ReLU or linear units) can be posed as as
linear program. Speciﬁcally, considering the FICNN net-

work in (2) can be written as the optimization problem

zk

minimize
y,z1,...,zk
subject to zi+1 ≥ W (z)

zi + W (y)
i
i = 1, . . . , k − 1.

i y + bi,

zi ≥ 0,

i = 0, . . . , k − 1

(22)

This problem exactly replicates the equations of the
FICNN, with the exception that we have replaced ReLU
and the equality constraint between layers with a positiv-
ity constraint on the zi terms and an inequality. How-
ever, because we are minimizing the ﬁnal zk term, and
because each inequality constraint is convex, at the solu-
tion one of these constraints must be tight, i.e., (zi)j =
(W (z)
i y + bi)j or (zi)j = 0, which recovers the
i
ReLU non-linearity exactly. The exact same procedure can
be used to write to create an exact inference procedure for
the PICNN.

zi + W (y)

Although the LP formulation is appealing in its simplicity,
in practice these optimization problems will have a num-
ber of variables equal to the total number of activations in
the entire network. Furthermore, most LP solution meth-
ods to solve such problems require that we form and in-
vert structured matrices with blocks such as W T
i Wi — the
case for most interior-point methods (Wright, 1997) or even
approximate algorithms such as the alternating direction
method of multipliers (Boyd et al., 2011) — which are large
dense matrices or have structured forms such as non-cyclic
convolutions that are expensive to invert. Even incremental
approaches like the Simplex method require that we form
inverses of subsets of columns of these matrices, which are
additionally different for structured operations like convo-
lutions, and which overall still involve substantially more
computation than a single forward pass. Furthermore, such
solvers typically do not exploit the substantial effort that
has gone in to accelerating the forward and backward com-
putation passes for neural networks using hardware such
as GPUs. Thus, as a whole, these do not present a viable
option for optimizing the networks.

Input Convex Neural Networks: Supplementary Material

Algorithm 1 A typical bundle method to optimize f :
Rm×n → R over Rn for K iterations with a ﬁxed x and
initial starting point y1.

Algorithm 2 Our bundle entropy method to optimize f :
Rm × [0, 1]n → R over [0, 1]n for K iterations with a ﬁxed
x and initial starting point y1.

function BUNDLEMETHOD(f , x, y1, K)

function BUNDLEENTROPYMETHOD(f , x, y1, K)

k ← ∇yf (x, yk; θ)T

GT
hk ← f (x, yk; θ) − ∇yf (x, yk; θ)T yk
yk+1, tk+1 ← argminy∈Y,t {t | G1:ky+h1:k ≤

(cid:46) kth row of G

APPEND(G(cid:96), ∇yf (x, yk; θ)T )
APPEND(h(cid:96), f (x, yk; θ) − ∇yf (x, yk; θ)T yk)
ak ← LENGTH(G(cid:96))

(cid:46) The number of active

G(cid:96) ← [ ]
h(cid:96) ← [ ]
for k = 1, K do

G ← 0 ∈ RK×n
h ← 0 ∈ RK
for k = 1, K do

t1}

end for
return yK+1

end function

constraints.

Gk ← CONCAT(G(cid:96))∈ Rak×n
hk ← CONCAT(h(cid:96))∈ Rak
if ak = 1 then
λk ← 1

else

λk ← PROJNEWTONLOGISTIC(Gk, hk)

end if
yk+1 ← (1 + exp(GT
k λk))−1
DELETE(G(cid:96)[i] and h(cid:96)[i] where λi ≤ 0) (cid:46) Prune

inactive constraints.

end for
return yK+1

end function

E. Deep Q-learning with ICNNs

In Algorithm 3.

C. The bundle method for approximate

inference in ICNNs

We here review the basic bundle method (Smola et al.,
2008) that we build upon in our bundle entropy method.
The bundle method takes advantage of the fact that for a
convex objective, the ﬁrst-order approximation at any point
is a global underestimator of the function; this lets us main-
tain a piecewise linear lower bound on the function by
adding cutting planes formed by this ﬁrst order approxi-
mation, and then repeatedly optimizing this lower bound.
Speciﬁcally, the process follows the procedure shown in
Algorithm 1. Denoting the iterates of the algorithm as yk,
at each iteration of the algorithm, we compute the ﬁrst or-
der approximation to the function

f (x, yk; θ) + ∇yf (x, yk; θ)T (y − yk)

(23)

and update the next iteration by solving the optimization
problem

yk+1 := argmin

max
1≤i≤k

y∈Y

{f (x, yi; θ)+∇yf (x, yi; θ)T (y−yi)}.

(24)
A bit more concretely, the optimization problem can be
written via a set of linear inequality constraints

yk+1, tk+1 := argmin
y∈Y,t

{t | Gy + h ≤ t1}

(25)

where G ∈ Rk×n has rows equal to

gT
i = ∇yf (x, yi; θ)T

(26)

and h ∈ Rk has entries equal to

hi = f (x, yi; θ) − ∇yf (x, yi; θ)T yi.

(27)

D. Bundle Entropy Algorithm

In Algorithm 2.

Input Convex Neural Networks: Supplementary Material

Algorithm 3 Deep Q-learning with ICNNs. Opt-Alg
is a convex minimization algorithm such as gradient de-
scent or the bundle entropy method. ˜Qθ is the objective
the optimization algorithm solves.
In gradient descent,
˜Qθ(s, a) = Q(s, a|θ) and with the bundle entropy method,
˜Qθ(s, a) = Q(s, a|θ) + H(a).

Select a discount factor γ ∈ (0, 1) and moving average
factor τ ∈ (0, 1)
Initialize the ICNN −Q(s, a|θ) with target network pa-
rameters θ(cid:48) ← θ and a replay buffer R ← ∅
for each episode e = 1, E do

Initialize a random process N for action exploration
Receive initial observation state s1
for i = 1, I do

ai ← OPT-ALG(−Qθ, si, ai,0)+Ni (cid:46) For some

initial action ai,0

Execute ai and observe ri+1 and si+1
INSERT(R, (si, ai, si+1, ri+1))
Sample a random minibatch from the replay

buffer: RM ⊆ R

for (sm, am, s+

m, r+
m) ∈ RM do
m ← OPT-ALG(−Qθ(cid:48),s+
a+

m,a+

m,0) (cid:46) Uses the

target parameters θ(cid:48)
ym ← r+

m + γQ(s+

m, a+

m|θ(cid:48))

end for
Update θ with a gradient step to minimize L =
(cid:1)2

(cid:0) ˜Q(sm, am|θ) − ym

m
θ(cid:48) ← τ θ + (1 − τ )θ(cid:48)

(cid:46) Update the target

(cid:80)

1
|RM |

network.

end for

end for

F. Max-margin structured prediction

In the more traditional structured prediction setting, where
we do not aim to ﬁt the energy function directly but ﬁt
the predictions made by the system to some target out-
puts, there are different possibilities for learning the ICNN
parameters. One such method is based upon the max-
margin structured prediction framework (Tsochantaridis
et al., 2005; Taskar et al., 2005). Given some training ex-
ample (x, y(cid:63)), we would like to require that this example
has a joint energy that is lower than all other possible val-
ues for y. That is, we want the function ˜f to satisfy the
constraint

˜f (x, y(cid:63); θ) ≤ min

˜f (x, y; θ)

y

(28)

Unfortunately,
these conditions can be trivially ﬁt by
choosing a constant ˜f (although the entropy term allevi-
ates this problem slightly, we can still choose an approx-
imately constant function), so instead the max-margin ap-
proach adds a margin-scaling term that requires this gap to
be larger for y further from y(cid:63), as measured by some loss

function ∆(y, y(cid:63)). Additionally adding slack variables to
allow for potential violation of these constraints, we arrive
at the typical max-margin structured prediction optimiza-
tion problem

minimize
θ,ξ≥0

λ
2

(cid:107)θ(cid:107)2

2 +

m
(cid:88)

i=1

ξi

subject to ˜f (xi, yi; θ) ≤ min
y∈Y

(cid:17)
(cid:16) ˜f (xi, y; θ) − ∆(yi, y)

− ξi

(29)

As a simple example, for multiclass classiﬁcation tasks
where y(cid:63) denotes a “one-hot” encoding of examples, we
can use a multi-variate entropy term and let ∆(y, y(cid:63)) =
y(cid:63)T (1 − y).
Training requires solving this “loss-
augmented” inference problem, which is convex for suit-
able choices of the margin scaling term.

The optimization problem (29) is naturally still not con-
vex in θ, but can be solved via the subgradient method for
structured prediction (Ratliff et al., 2007). This algorithm
iteratively selects a training example xi, yi, then 1) solves
the optimization problem

y(cid:63) = argmin

f (xi, y; θ) − ∆(yi, y)

(30)

y∈Y

and 2) if the margin is violated, updates the network’s pa-
rameters according to the subgradient

θ := P+ [θ − α (λθ + ∇θf (xi, yi, θ) − ∇θf (xi, y(cid:63); θ))]
(31)
where P+ denotes the projection of W (z)
1:k−1 onto the non-
negative orthant. This method can be easily adapted to use
mini-batches instead of a single example per subgradient
step, and also adapted to alternative optimization methods
like AdaGrad (Duchi et al., 2011) or ADAM (Kingma &
Ba, 2014). Further, a fast approximate solution to y(cid:63) can
be used instead of the exact solution.

G. Proof of Proposition 3

Proof (of Proposition 3). We have by the chain rule that

∂(cid:96)
∂θ

=

∂(cid:96)
∂ ˆy

(cid:18) ∂ ˆy
∂G

∂G
∂θ

+

∂ ˆy
∂h

∂h
∂θ

(cid:19)

.

(32)

The challenging terms to compute in this equation are the
∂ ˆy
∂G and ∂ ˆy
∂h terms. These can be computed (although we
will ultimately not compute them explicitly, but just com-
pute the product of these matrices and other terms in the
Jacobian), by implicit differentiation of the KKT condi-
tions. Speciﬁcally, the KKT conditions of the bundle en-
tropy method (considering only the active constraints at the

Input Convex Neural Networks: Supplementary Material

solution) are given by

1 + log ˆy − log(1 − ˆy) + GT λ = 0
Gˆy + h − t1 = 0
1T λ = 1.

For simplicity of presentation, we consider ﬁrst the Jaco-
bian with respect to h. Taking differentials of these equa-
tions with respect to h gives

diag

(cid:18) 1
ˆy

+

1
1 − ˆy

(cid:19)

dy + GT dλ = 0

Gdy + dh − dt1 = 0
1T dλ = 0

H. State and action space sizes in the OpenAI

gym MuJoCo benchmarks.

(33)

(34)

Environment
InvertedPendulum-v1
InvertedDoublePendulum-v1
Reacher-v1
HalfCheetah-v1
Swimmer-v1
Hopper-v1
Walker2d-v1
Ant-v1
Humanoid-v1
HumanoidStandup-v1

# State
4
11
11
17
8
11
17
111
376
376

# Action
1
1
2
6
2
3
6
8
17
17

Table 4. State and action space sizes in the OpenAI gym MuJoCo
benchmarks.

I. Synthetic classiﬁcation examples

We begin with a simple example to illustrate the classi-
ﬁcation performance of a two-hidden-layer FICNN and
PICNN on two-dimensional binary classiﬁcation tasks
from the scikit-learn toolkit (Pedregosa et al., 2011). Fig-
ure 4 shows the classiﬁcation performance on the dataset.
The FICNN’s energy function which is fully convex in
X × Y jointly is able to capture complex, but sometimes
restrictive decision boundaries. The PICNN, which is non-
convex over X but convex over Y overcomes these restric-
tions and can capture more complex decision boundaries.

J. Multi-Label Classiﬁcation Training Plots

In Figure 5.

(37)

K. Image Completion

The losses are in Figure 6.

or in matrix form

diag






(cid:17)

1−ˆy

(cid:16) 1
ˆy + 1
G
0

GT
0
−1T










dy
dλ
dt

0
−1
0





 =





 .

0
−dh
0

(35)
To compute the Jacobian ∂ ˆy
∂h we can solve the system above
with the right hand side given by dh = I, and the resulting
dy term will be the corresponding Jacobian. However, in
our ultimate objective we always left-multiply the proper
terms in the above equation by ∂(cid:96)
∂ ˆy . Thus, we instead deﬁne

(cid:17)



−1 



(cid:16) 1







1−ˆy




cy
cλ
ct

diag

 =

ˆy + 1
G
0

∂ ˆy )T
−( ∂(cid:96)
0
0
(36)
and we have the the simple formula for the Jacobian prod-
uct

GT
0
−1T

0
−1
0










∂(cid:96)
∂ ˆy

∂ ˆy
∂h

= (cλ)T .

A similar set of operations taking differentials with respect
to G leads to the matrix equations






(cid:16) 1

diag

(cid:17)

1−ˆy

ˆy + 1
G
0

GT
0
−1T










dy
dλ
dt

0
−1
0





 =



−dGT λ
−dGy
0





(38)
and the corresponding Jacobian products / gradients are
given by

∂(cid:96)
∂ ˆy

∂ ˆy
∂G

= cyλT + ˆy(cλ)T .

(39)

Finally, using the deﬁnitions that

i = ∇yf (x, yi; θ)T , hi = f (x, yk; θ)−∇yf (x, yi; θ)T yi
gT
(40)

we recover the formula presented in the proposition.

Input Convex Neural Networks: Supplementary Material

Figure 4. FICNN (top) and PICNN (bottom) classiﬁcation of synthetic non-convex decision boundaries. Best viewed in color.

Figure 5. Training (blue) and test (red) macro-F1 score of a feedforward network (left) and PICNN (right) on the BibTeX multi-label
classiﬁcation dataset. The ﬁnal test F1 scores are 0.396 and 0.415, respectively. (Higher is better.)

Figure 6. Mean Squared Error (MSE) on the train (blue, rolling over 1 epoch) and test (red) images from Olivetti faces for PICNNs
trained with the bundle entropy method (left) and back optimization (center), and back optimization with the convexity constraint
relaxed (right). The minimum test MSEs are 833.0, 872.0, and 850.9, respectively.

