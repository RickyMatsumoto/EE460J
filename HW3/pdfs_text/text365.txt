Provably Optimal Algorithms for Generalized Linear Contextual Bandits

Lihong Li 1 Yu Lu 2 Dengyong Zhou 1

Abstract

Contextual bandits are widely used in Internet
services from news recommendation to adver-
tising, and to Web search. Generalized linear
models (logistical regression in particular) have
demonstrated stronger performance than linear
models in many applications where rewards are
binary. However, most theoretical analyses on
contextual bandits so far are on linear bandits. In
this work, we propose an upper conﬁdence bound
based algorithm for generalized linear contex-
tual bandits, which achieves an ˜O(
dT ) regret
over T rounds with d dimensional feature vec-
tors. This regret matches the minimax lower
bound, up to logarithmic terms, and improves on
d factor, assum-
the best previous result by a
ing the number of arms is ﬁxed. A key compo-
nent in our analysis is to establish a new, sharp
ﬁnite-sample conﬁdence bound for maximum-
likelihood estimates in generalized linear mod-
els, which may be of independent interest. We
also analyze a simpler upper conﬁdence bound
algorithm, which is useful in practice, and prove
it to have optimal regret for certain cases.

√

√

1. Introduction

Contextual bandit problems are originally motivated by ap-
plications in clinical trials (Woodroofe, 1979). When a
standard treatment and a new treatment are available for
a certain disease, the doctor needs to decide, in a sequetial
manner, which of them to use based on the patient’s proﬁles
such as age, general physical status or medicine history.
With the development of modern technologies, contextual
bandit problems have more applications, especially in web-
based recommendation, advertising and search (Agarwal

1Microsoft Research, Redmond, WA 98052 2Department
of Statistics, Yale University, New Haven, CT, USA. Cor-
Lihong Li <lihongli@microsoft.com>,
respondence
Zhou <den-
Yu
zho@microsoft.com>.

Lu <yu.lu@yale.edu>,

Dengyong

to:

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

et al., 2009; Li et al., 2010; 2012). In the problem of per-
sonalized news recommendation, the website must recom-
mend news articles that are most interesting to users that
visit the website. The problem is especially challenging
for breaking news, as little data are available to make good
prediction about user interest. A trade-off naturally occurs
in this kind of sequential decision making problems. One
needs to balance exploitation—choosing actions that per-
formed well in the past—and exploration—choosing ac-
tions that may potentially give better outcomes.

In this paper, we study the following stochastic, K-armed
contextual bandit problem. Suppose at each of the T
rounds, an agent is presented with a set of K actions, each
of which is associated with a context (a d-dimensional fea-
ture vector). By choosing an action based on the rewards
obtained from previous rounds and on the contexts, the
agent will receive a stochastic reward generated from some
unknown distribution conditioned on the context and the
chosen action. The goal of the agent is to maximize the
expected cumulative rewards over T rounds. The most
studied model in contextual bandits literature is the linear
model (Auer, 2002; Dani et al., 2008; Rusmevichientong
& Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al.,
2011), in which the expected rewards at each round is a lin-
ear combination of features in the context vector. The lin-
ear model is theoretically convenient to work with. How-
ever, in practice, we usually have binary rewards (click or
not, treatment working or not). Logistic regression model
based algorithms have been shown to have substantial im-
provements over linear models (Li et al., 2012). We there-
fore consider generalized linear models (GLM) in the con-
textual bandit setting, in which linear, logistic and probit
regression serve as three important special cases.

The celebrated work of Lai & Robbins (1985) ﬁrst intro-
duces the upper conﬁdence bound (UCB) approach to efﬁ-
cient exploration. Later, the idea of conﬁdence bound has
been successfully applied to many stochastic bandits prob-
lems, from K-arm bandits problems (Auer et al., 2002a;
Bubeck & Cesa-Bianchi, 2012) to linear bandits (Auer,
2002; Abbasi-Yadkori et al., 2011). UCB-type algorithms
are both efﬁcient and provable optimal in K-arm bandits
and K-armed linear bandits. However, most study are lim-
ited to the linear case. While some UCB-type algorithms
using GLMs perform well empirically (Li et al., 2012),

Generalized Linear Contextual Bandits

there is little theoretical study of them. A natural question
arises: can we ﬁnd an efﬁcient algorithm to achieve the op-
timal convergence rate for generalized linear bandits?

√

√

Our Contributions
In this paper, we propose a GLM
version of the UCB algorithm called SupCB-GLM that
achieves a regret over T rounds of order ˜O(
dT ). This rate
improves the state-of-the-art results of Filippi et al. (2010)
d factor, assuming the number of actions is ﬁxed.
by a
Moreover, it matches the GLM bandits problem’s mini-
max lower bound indicated by the linear bandits problem
and thus is optimal. SupCB-GLM is inspired by the semi-
nal work of Auer (2002), which introduced a technique to
construct independence samples in linear contextual ban-
dits. A key observation in proving this result is that the
(cid:96)2 conﬁdence ball of the unknown parameter is insufﬁcient
to calculate a sharp upper conﬁdence bound, yet what we
need is the conﬁdence interval in all directions. Thus, we
prove a ﬁnite sample normality type conﬁdence bound for
the maximum likelihood estimator of GLM. To the best of
our knowledge, this is the ﬁrst non-asymptotic normality
type result for the GLM and might be of its own theoretical
value. We also analyze a simple version of UCB algorithm
called UCB-GLM that is widely used in practice. We prove
it also achieves the optimal regret bound under a reason-
able assumption. These results shed light on explaining the
good empirical performance of GLM bandits in practice.

Related Work The study of GLM bandits problem goes
back at least to Sarkar (1991), who considered discounted
regrets rather than cumulative regerts. They prove that a
myopic rule without exploration is asymptotically optimal.
Recently, Filippi et al. (2010) study the same stochastic
GLM bandit problem considered here. They propose the
GLM-UCB algorithm, similar to our Algorithm 1, which
achieves a regret of ˜O(d
T ) after T rounds. However, as
we believe the optimal regret for stochastic GLM bandits
should be the same as linear case when the number of ac-
d term than the optimal
tions is small, their rates misses a
rates.

√

√

Another line of research focuses on using EXP-type al-
gorithms, which can be applied to almost any model
classes (Auer et al., 2002b). These algorithms, which
choose actions using a carefully randomized policy, use
importance sampling to reduce a bandit problem to its full-
information analogue. Later variants of the EXP4 algo-
rithm (Beygelzimer et al., 2010; Agarwal et al., 2014) give
an ˜O(
dKT ) regret that is near-optimal with respect to
T . However, these regret bounds have a
K dependence.
Moreover, these algorithms can be expensive to run: they
either have a computational complexity exponential in d for
our GLM case, or need to make a large number of calls to
a nontrivial optimization oracle.

√

√

Organization Section 2 introduces the generalized linear
bandit problem. Section 3 gives a brief review of the sta-
tistical properties of generalized linear model, and gives a
sharp non-asymptotic normality-type result for GLM pa-
rameter estimation which can be of independent value.
With this tool, Section 4 presents our algorithms and the
main theoretical results. Section 5 concludes the paper with
further discussions, including several open problems. All
proofs are given in the supplementary materials.

√

Notations For a vector x ∈ Rd, we use (cid:107)x(cid:107) to denote its
(cid:96)2- norm and x(cid:48) its transpose. Bd := {x ∈ Rd : (cid:107)x(cid:107) ≤ 1}
is the unit ball centered at the origin. The weighted (cid:96)2-norm
associated with a positive-deﬁnite matrix A is deﬁned by
x(cid:48)Ax. The minimum and maximum singular
(cid:107)x(cid:107)A :=
values of a matrix A are written as λmin(A) and (cid:107)A(cid:107), re-
spectively. The trace of a matrix A is tr (A). For two sym-
metric matrices A and B of the same dimensions, A (cid:23) B
means that A − B is positive semi-deﬁnite. For a real-
valued function f , we use ˙f and ¨f to denote its ﬁrst and
second derivatives. Finally, [n] := {1, 2, . . . , n}.

2. Problem Setting

We consider the stochastic K-armed contextual bandit
problem. Let T be the number of total rounds. At round
t, the agent observes a context consisting of a set of K
feature vectors, {xt,a | a ∈ [K]} ⊂ Rd, which is drawn
IID from an unknown distribution ν, with (cid:107)xt,a(cid:107) ≤ 1.
Each feature vector xt,a is associated with an unknown
stochastic reward yt,a ∈ [0, 1]. The agent selects one ac-
tion, denoted at, and observes the corresponding reward
yt,at. Finally, we make a regularity assumption about the
distribution ν:
there exists a constant σ0 > 0 such that
(cid:80)
λmin(E[ 1
K

t,a]) ≥ σ2
In this paper, we are concerned with the generalized linear
model, or GLM, in which there is an unknown θ∗ ∈ Rd and
a ﬁxed, strictly increasing link function µ : R → R such
that E[Y | X] = µ(X (cid:48)θ∗), where X is the chosen action’s
feature and Y the corresponding reward. One can verify
that linear and logistic models are special cases of GLM
with µ(x) = x and µ(x) = 1/(1 + e−x), respectively.

a∈[K] xt,ax(cid:48)

0 for all t.

The agent’s goal is to maximize the cumulative expected
rewards over T rounds. Suppose the agent takes action at
at round t. Then the agent’s strategy can be evaluated by
comparing its expected reward to the best expected reward.
To do so, deﬁne the optimal action at round t by a∗
t =
argmaxa∈[K] µ(x(cid:48)
t,aθ∗). Then, the agent’s total regret of
following strategy π can be expressed as follows

RT (π) :=

µ(x(cid:48)

t,a∗
t

θ∗) − µ(x(cid:48)

t,at

(cid:17)

.

θ∗)

T
(cid:88)

(cid:16)

t=1

Note that RT (π) is in general a random variable due to the

Generalized Linear Contextual Bandits

possible randomness in π. Denote by Xt = xt,at, Yt =
yt,at, and our model can be written as

Yt = µ(X (cid:48)

tθ∗) + (cid:15)t ,

(1)

where {(cid:15)t, t ∈ [T ]} are independent zero-mean noise.
Here, Xt is a random variable because the agent chooses
current action based on previous rewards. Formally, we as-
sume there is an increasing sequence of sigma ﬁelds {Fn}
such that (cid:15)t is Ft-measurable with E [ (cid:15)t | Ft−1 ] = 0.
An example of Fn will be the sigma-ﬁeld generated by
{X1, Y1, . . . , Xn, Yn}. Also, we assume the noise (cid:15)t is
sub-Gaussian with parameter σ, where σ is some positive,
universal constant; that is, for all t,

E (cid:2) eλ(cid:15)t | Ft−1

(cid:3) ≤ eλ2σ2/2.

(2)

In practice, when we have bounded reward Yt ∈ [0, 1], the
noise (cid:15)t is also bounded and hence satisﬁes (2) with some
appropriate σ value.
In addition to the boundedness as-
sumption on the rewards and feature vectors, we also need
the following assumption on the link function µ.
Assumption 1. κ := inf {(cid:107)x(cid:107)≤1, (cid:107)θ−θ∗(cid:107)≤1} ˙µ(x(cid:48)θ) > 0.

As we shall see in Section 3, the asymptotic normality of
maximum-likelihood estimates implies the necessity of this
assumption. Note that this assumption is weaker than As-
sumption 1 in Filippi et al. (2010), as it only requires to
control the local behavior of ˙µ(x(cid:48)θ) near θ∗.

Assumption 2. µ is twice differentiable. Its ﬁrst and sec-
ond order derivatives are upper-bounded by Lµ and Mµ,
respectively.

It can be veriﬁed that Assumption 2 holds for the logistic
link function, where we may choose Lµ = Mµ = 1/4.

3. Generalized Linear Models

To motivate the algorithms proposed in this paper, we ﬁrst
brieﬂy review the classical likelihood theory of generalized
linear models. In the canonical generalized linear model
(McCullagh & Nelder, 1989), the conditional distribution
of Y given X is from the exponential family, and its den-
sity, parameterized by θ ∈ Θ, can be written as

P(Y | X) = exp

(cid:26) Y X (cid:48)θ∗ − m(X (cid:48)θ∗)
g(η)

(cid:27)

+ h(Y, η)

. (3)

Here, η ∈ R+ is a known scale parameter; m, g and h
are three normalization functions mapping from R to R.
The exponential family (3) is a very broad family of distri-
butions including the Gaussian, binomial, Poisson, gamma
and inverse-Gaussian distributions. It follows from stan-
dard properties of exponential families (Brown, 1986) that
m is inﬁnitely differentiable satisfying ˙m(X (cid:48)θ∗) = E[ Y |

X ] = µ(X (cid:48)θ∗) and ¨m(X (cid:48)θ∗) = V(Y | X).
It can be
checked that the data generated from (3) automatically sat-
isﬁes the sub-Gaussian condition (2).

Suppose we have independent samples of Y1, Y2, . . . , Yn
condition on X1, X2, . . . , Xn. The log-likelihood function
of θ under model (3) is

log (cid:96)(θ) =

n
(cid:88)

(cid:20) YtX (cid:48)

tθ − m(X (cid:48)
v(η)

tθ)

(cid:21)

+ c(Yt, η)

t=1

1
v(η)

n
(cid:88)

t=1

=

[YtX (cid:48)

tθ − m(X (cid:48)

tθ)] + constant .

Consequently, the maximum likelihood estimate (MLE)
may be deﬁned by

ˆθn ∈ argmax

θ∈Θ

n
(cid:88)

t=1

[YtX (cid:48)

tθ − m(X (cid:48)

tθ)] .

From classical likelihood theory (Lehmann & Casella,
1998), we know that when the sample size n goes to inﬁn-
ity, the MLE ˆθn is asymptotically normal, that is, ˆθn−θ∗ →
θ∗ ), where Iθ = (cid:80)n
N (0, I −1
tθ)XtX (cid:48)
t=1 ˙µ(X (cid:48)
t is the Fisher
tθ∗) → 0, the asymp-
Information Matrix. Note that if ˙µ(X (cid:48)
totic variance of x(cid:48) ˆθ can go to inﬁnity for some x. This
suggests the necessity of Assumption 1.

As we will see later, the normality result is crucial in our
regret analysis of GLM bandits. However, to the best of
our knowledge, there is no non-asymptotic normality re-
sults of the MLE for GLM. In the following, we present a
ﬁnite-sample version of the classical asymptotic normality
results, which can be of independent interest.
Theorem 1. Deﬁne Vn = (cid:80)n
given. Furthermore, assume that

t, and let δ > 0 be

t=1 XtX (cid:48)

λmin(Vn) ≥

(cid:18)

µσ2

512M 2
κ4

d2 + log

(cid:19)

.

1
δ

Then, with probability at least 1 − 3δ,
likelihood estimator satisﬁes, for any x ∈ Rd, that

the maximum-

|x(cid:48)(ˆθn − θ∗)| ≤

3σ
κ

(cid:112)log(1/δ) (cid:107)x(cid:107)V −1

n

.

This theorem characterizes the behavior of MLE on every
direction. It implies that x(cid:48)(ˆθn − θ∗) has a sub-Gaussian
tail bound for any x ∈ Rd. It also provides a rigorous justi-
ﬁcation of the asymptotic upper conﬁdence bound derived
heuristically by Filippi et al. (2010, Section 4.2).

The proof of the theorem is given in the appendix. It con-
sists of two main steps, as is typical for proving normality-
type results of MLEs (Van der Vaart, 2000). We ﬁrst show
the n−1/2-consistency of ˆθ to θ∗. Then, by using a second-
order Taylor expansion or Newton-step, we can prove the
desired normality of ˆθ.

(4)

(5)

Generalized Linear Contextual Bandits

The condition (4) on λmin(Vn) is necessary for the consis-
tency of estimating linear models (Lai & Wei, 1982; Bickel
et al., 2009) and generalized linear models (Fahrmeir &
Kaufmann, 1985; Chen et al., 1999). It can be satisﬁed un-
der mild conditions such as the proposition below, which
will be useful for our analysis.
Proposition 1. Deﬁne Vn = (cid:80)n
t, where Xt is
drawn iid from some distribution ν with support in the unit
ball, Bd. Furthermore, let Σ := E[XtX (cid:48)
t] be the second
moment matrix, and B and δ > 0 be two positive con-
stants. Then, there exist positive, universal constants C1
and C2 such that λmin(Vn) ≥ B with probability at least
1 − δ, as long as

t=1 XtX (cid:48)

n ≥

(cid:32)

√

C1

d + C2

(cid:112)log(1/δ)

(cid:33)2

λmin(Σ)

+

2B
λmin(Σ)

.

Proof Sketch. We give a proof sketch here, and the full
proof is found in the appendix. In the following, for sim-
plicity, we will drop the subscript n when there is no ambi-
guity. Therefore, Vn is denoted V and so on. We will need a
technical lemma, which is an existing result in random ma-
trix theory. The version we presented here is adapted from
Equation (5.23) of Theorem 5.39 from Vershynin (2012).

Lemma 1. Let A ∈ Rn×d be a matrix whose rows Ai
are independent sub-Gaussian isotropic random vectors in
Rd with parameter σ, namely, E exp(x(cid:48)(Ai − EAi)) ≤
exp(σ2 (cid:107)x(cid:107)2 /2) for any x ∈ Rd. Then, there exist pos-
itive, universal constants C1 and C2 such that, for ev-
ery t ≥ 0, the following holds with probability at least
1 − 2 exp(−C2t2), where ε = σ2(C1
n):
(cid:13)
(cid:13)
(cid:13) 1
(cid:13) ≤ max{ε, ε2} .
n A(cid:48)A − Id

(cid:112)d/n + t/

√

t=1 ZtZ (cid:48)

Let X be a random vector drawn from the distribution
ν. Deﬁne Z := Σ−1/2X. Then Z is isotropic, namely,
E[ZZ (cid:48)] = Id. Deﬁne U = (cid:80)n
t = Σ−1/2V Σ−1/2.
From Lemma 1, we have that, for any t, with probability
at least 1 − 2 exp(−C2t2), λmin(U ) ≥ n − C1σ2
nd −
σ2t
n, where σ is the sub-Gaussian parameter of Z, which
is upper-bounded by (cid:13)
min (Σ) (see, e.g., Ver-
shynin (2012)). We thus can rewrite the above inequality
(which holds with probability 1 − δ as

(cid:13) = λ−1/2

(cid:13)Σ−1/2(cid:13)

√

√

λmin(U ) ≥ n − λ−1

min(Σ)

C1σ2

√

nd + t

√

(cid:17)

n

.

This implies the following lower bound:

λmin(V ) ≥ λmin(Σ)n − C1

nd − C2

(cid:112)n log(1/δ) .

Finally, simple calculations show that the last expression is
no less than B as long as n is no smaller than the expression
stated in the proposition, ﬁnishing the proof.

(cid:16)

√

4. Algorithms and Main Results

In this section, we are going to present two algorithms.
While the ﬁrst algorithm is computationally more efﬁcient,
the second algorithm has a provable optimal regret bound.

4.1. Algorithm UCB-GLM

The idea of upper conﬁdence bounds (UCB) is highly
effective in dealing with the exploration and exploita-
tion trade-off in many parametric bandit problems, includ-
ing K-arm bandits (Auer et al., 2002a) and linear ban-
dits (Abbasi-Yadkori et al., 2011; Auer, 2002; Chu et al.,
2011; Dani et al., 2008). For the generalized linear model
considered here, since µ is a strictly increasing function,
our goal is equivalent to choosing a ∈ [K] to maximize
t,aθ∗ at round t. Suppose ˆθt is our current estimator of θ∗
x(cid:48)
after round t. An exploitation action is to take the action
that maximizes the estimated mean value, while an explo-
ration action is to choose the one that has the largest vari-
ance. Thus, to balance exploitation and exploration, we
can simply choose the action that maximizes the sum of es-
timated mean and variance, which can be interpreted as an
ˆθt. This leads to the algo-
upper conﬁdence bound of x(cid:48)
t,a
rithm UCB-GLM (Algorithm 1).

Algorithm 1 UCB-GLM

Input: the total rounds T , tuning parameter τ and α.
Initialization: randomly choose at ∈ [K] for t ∈ [τ ], set
Vτ +1 = (cid:80)τ
i=1 XtX (cid:48)
t
For t = τ + 1, τ + 2, . . . , T do
1. Calculate the maximum-likelihood estimator ˆθt by

solving the equation

t−1
(cid:88)

i=1

(Yi − µ(X (cid:48)

iθ))Xi = 0

(6)

(cid:17)

2. Choose at = argmaxa∈[K]
3. Observe Yt, let Xt ← Xt,at, Vt+1 ← Vt + XtX (cid:48)
t

ˆθt + α (cid:107)Xt,a(cid:107)V −1

X (cid:48)

t,a

t

(cid:16)

End For

UCB-GLM take two parameters. At the initialization stage,
we randomly choose actions to ensure a unique solution
of (6). The choice of τ in the theorem statement follows
from Proposition 1 with B = 1. It should be noted that the
IID assumption about contextual (i.e., the distribution ν)
is only needed to ensure Vτ +1 is invertable (similar to the
ﬁrst phase in the algorithm of Filippi et al. (2010)); the rest
of our analysis does not depend on this stochastic assump-
tion. The same may be achieved by using regularization
(see, e.g., Abbasi-Yadkori et al. (2011)). Another tuning
parameter α is used to control the amount of exploration.
The larger the α is, the more exploration will be used.

As mentioned earlier, the feature vectors Xt depend on the

Generalized Linear Contextual Bandits

previous rewards. Consequently, the rewards {Yi, i ∈ [t]}
may not be independent given {Xi, i ∈ [t]}. We in-
stead use results on self-normalized martingales (Abbasi-
Yadkori et al., 2011), together with a ﬁnite-time normality
result like Theorem 1, to prove the next theorem.

(cid:113) d

2 log(1 + 2T /d) + log(1/δ) and τ = Cσ−2

Theorem 2. Fix any δ > 0. There exists a universal
constant C > 0, such that if we run UCB-GLM with
α = σ
0 (d +
κ
log(1/δ)), then, with probability at least 1 − 2δ, the regret
of the algorithm is upper bounded by
(cid:18) T
dδ

2Lµσd
κ

RT ≤ τ +

(cid:19) √

log

T .

√

√

The theorem shows an ˜O(d
T ) regret bound that is in-
dependent of K.
Indeed, this rate matches the minimax
lower bound up to logarithm factor for the inﬁnite actions
contextual bandit problems (Dani et al., 2008). By choos-
ing δ = 1/T and using the fact that RT ≤ T , this high-
probability result implies a bound on the expected regret:
E[RT ] = ˜O(d
T ). Our result improves the previous
log T factor.
regret bound of Filippi et al. (2010) by a
Moreover, the algorithm proposed in Filippi et al. (2010)
involves a projection step, which is computationally more
expensive comparing to UCB-GLM. Finally, this algorithm
works well in practice. We give a heuristic argument for its
strong performance in Section 5, under a speciﬁc condition
that sometimes are satisﬁed.

√

Proof of Theorem 2. We ﬁrst bound the one-step regret. To
t and ∆t = ˆθt − θ∗, where
do so, ﬁx t and let X ∗
t,aθ∗) is an optimal action at round
t ∈ arg maxa∈[K] µ(x(cid:48)
a∗
t. The selection of at in UCB-GLM implies

t = xt,a∗

(cid:104)X ∗

t , ˆθt(cid:105) + α (cid:107)X ∗

t (cid:107)V −1

t

≤ (cid:104)Xt, ˆθt(cid:105) + α (cid:107)Xt(cid:107)V −1

.

t

(7)

Then, we have

(cid:104)X ∗

t , θ∗(cid:105) − (cid:104)Xt, θ∗(cid:105) = (cid:104)X ∗
− α (cid:107)X ∗
≤ α (cid:107)Xt(cid:107)V −1
≤ α(cid:0) (cid:107)Xt(cid:107)V −1

− (cid:107)X ∗

t − Xt, ˆθt(cid:105) − (cid:104)X ∗
− (cid:104)X ∗
t (cid:107)V −1
(cid:1) + (cid:107)X ∗
t (cid:107)V −1

t − Xt, ∆t(cid:105)
t − Xt(cid:107)V −1

t

t

t

t

t − Xt, ˆθt − θ∗(cid:105)

(cid:107)∆t(cid:107)Vt

,

t

where the last inequality is due to Cauchy-Schwartz in-
equality. We have the following two lemmas to bound
(cid:107)∆t(cid:107)Vt
and (cid:107)Xt(cid:107)V −1
, respectively. Their proofs are de-
ferred to the appendix.

t

t=1 be a sequence in Rd satisfying
Lemma 2. Let {Xt}∞
(cid:107)Xt(cid:107) ≤ 1. Deﬁne X0 = 0 and Vt = (cid:80)t−1
s. Sup-
pose there is an integer m such that λmin(Vm+1) ≥ 1, then
for all n > 0,

s=0 XsX (cid:48)

m+n
(cid:88)

t=m+1

(cid:115)

(cid:107)Xt(cid:107)V −1

t

≤

2nd log

(cid:18) n + m
d

(cid:19)

.

Lemma 3. Suppose λmin(Vτ +1) ≥ 1. For any δ ∈
[1/T, 1), deﬁne event

E∆ :=

(cid:107)∆t(cid:107)Vt

≤

log(1 + 2t/d) + log(1/δ)

.

(cid:40)

(cid:114)

d
2

σ
κ

(cid:41)

Then, event E∆ holds for all t ≥ τ with probability at least
1 − δ.

We now choose α = σ
κ
event E∆ holds for all t ≥ τ , then,

(cid:113) d

2 log(1 + 2T /d) + log(1/δ). If

(cid:104)X ∗

(cid:16)

t , θ∗(cid:105) − (cid:104)Xt, θ∗(cid:105)
(cid:107)Xt(cid:107)V −1
≤ α
≤ 2α (cid:107)Xt(cid:107)V −1

t

t

.

− (cid:107)X ∗

t (cid:107)V −1

t

+ (cid:107)X ∗

t − Xt(cid:107)V −1

t

(cid:17)

Combining the above with Lemma 2 yields

(cid:0)(cid:104)X ∗

t , θ∗(cid:105) − (cid:104)Xt, θ∗(cid:105)(cid:1) ≤ 2α

2T d log

(cid:115)

T
(cid:88)

t=τ +1

(cid:19)

(cid:18) T
d
(cid:19) √

≤

2dσ
κ

log

(cid:18) T
dδ

T . (8)

Note that µ is an increasing Lipschitz function with Lips-
chitz constant Lµ and the µ function is bounded between
0 and 1. The regret of algorithm UCB-GLM can be upper
bounded as

RT =

µ ((cid:104)X ∗

t , θ∗(cid:105)) − µ ((cid:104)Xt, θ∗(cid:105))

(cid:17)

τ
(cid:88)

(cid:16)

t=1

+

T
(cid:88)

(cid:16)

t=τ +1

µ ((cid:104)X ∗

t , θ∗(cid:105)) − µ ((cid:104)Xt, θ∗(cid:105))

(cid:17)

≤ τ + Lµ

(cid:104)X ∗

t , θ∗(cid:105) − (cid:104)Xt, θ∗(cid:105)

(cid:17)

.

T
(cid:88)

(cid:16)

t=τ +1

The proof can be ﬁnished by applying (8) and the speciﬁed
value of τ to the bound above.

4.2. Algorithm SupCB-GLM

√

While the algorithm UCB-GLM performs sufﬁciently well
in practice (Li et al., 2012), it is unclear whether it can
dT log K), when K is
achieve the optimal rates of O(
ﬁxed and small. As mentioned in Section 4.1, the key
technical difﬁculty in analyzing UCB-GLM is the depen-
dence between samples. Inspired by a technique developed
by Auer (2002) to create independent samples for linear
contextual bandits, we propose another algorithm SupCB-
GLM (Algorithm 3), which uses algorithm CB-GLM (Al-
gorithm 2) as a sub-routine.

Generalized Linear Contextual Bandits

Algorithm 2 CB-GLM

Algorithm 3 SupCB-GLM

Input: parameter α, index set Ψ(t), and candidate set A.

Input: tuning parameter α, τ , the number of trials T .

1. Let ˆθt be the solution of

Initialization:

(cid:88)

i∈Ψ(t)

[Yi − µ(X (cid:48)

iθ)] Xi = 0

for t ∈ [τ ], randomly choose at ∈ [K].
Set S = (cid:98)log2 T (cid:99), F = {a1, · · · , aτ } and Ψ0 = Ψ1 =

2. Vt = (cid:80)

i∈Ψ(t) XiX (cid:48)
i

3. For a ∈ A, do

wt,a = α (cid:107)xt,a(cid:107)V −1

t

, mt,a = (cid:104)xt,a, ˆθt(cid:105)

End For

This algorithm also relies on the idea of conﬁdence bound
to do exploration. At round t, the algorithm screens the
candidate actions based on the value of w(s)
t,a through S
stages until an action is chosen. At stage s, we set the
conﬁdence level at stage s to be 2−s. If w(s)
t,a > 2−s for
some a, we need to do more exploration on xt,a and thus
we choose this action. Otherwise, the actions are ﬁltered
in step 2d such that the actions passed to the next stage are
close enough to the optimal action. Since all the widths
are smaller than 2−s, if m(s)
t,j − 2 · 2−s for some
j ∈ As, the action a can not be the optimal action. The ﬁl-
ter process terminates when we have already got accurate
estimate of all x(cid:48)
T level and we do not
need to do exploration. Thus in step 2c we just choose the
action that maximizes the estimated mean value.

t,aθ∗ up to the 1/

t,a < m(s)

√

Our algorithm is different from the algorithm SupLinRel
in Auer (2002) that we directly maximize the mean, rather
than the upper conﬁdence bound, in steps c and d. This
modiﬁcation leads to a simpler algorithm and a cleaner re-
gret analysis. Also, we would like to point out that, unlike
SpectralEliminator (Valko et al., 2014), the algorithm can
easily handle a changing action set.

The following result, adapted from Auer (2002, lemma 14),
shows how the algorithm SupCB-GLM will give us inde-
pendent samples. For the sake of completeness, we also
present its proof here.
Lemma 4. For all s ∈ [S] and t ∈ [T ], given {xi,ai, i ∈
Ψs(t)}, the rewards {yi,ai, i ∈ Ψs(t)} are independent
random variables.

· · · = ΨS = ∅.
For t = τ + 1, τ + 2, · · · , T do

1. Initialize A1 = [K] and s = 1.

2. While at =Null

a. Run CB-GLM with α and Ψs ∪ F to calculate

m(s)
b. If w(s)

t,a and w(s)

t,a for all a ∈ As.

t,a > 2−s for some a ∈ As,

set at = a, update Ψs = Ψs ∪ {t}

c. Else if w(s)

t,a ≤ 1/

T for all a ∈ As,

√

set at = argmax

t,a, update Ψ0 = Ψ0 ∪ {t}

m(s)

a∈As

d. Else if w(s)

t,a ≤ 2−s for all a ∈ As,

As+1 = {a ∈ As, m(s)

t,a ≥ max
j∈As

m(s)

t,j − 2 · 2−s},

s ← s + 1 .

End For

With Lemma 4, we are able to apply the non-asymptotic
normality result (5) and thus to prove our regret bound of
Algorithm SupCB-GLM.

√

Theorem 3. For any 0 < δ < 1, if we run the SupCB-GLM
(cid:112)2 log(T K/δ) for
algorithm with τ =
T ≥ T0 rounds, where
(cid:18) σ2

dT and α = 3σ
κ

(cid:27)(cid:19)

(cid:26)

T0 = Ω

κ4 max

d3,

log(T K/δ)
d

,

(9)

the regret of the algorithm is bounded as

RT ≤ 45(σLµ/κ)(cid:112)log T log(T K/δ) log(T /d)

√

dT ,

with probability at least 1 − δ. With δ = 1/T , we obtain

E[RT ] = O

(cid:16)

(log T )1.5(cid:112)dT log K

(cid:17)

.

Proof of Lemma 4. Since a trial t can only be added to
Ψs(t) in step 2b of algorithm SupCB-GLM, the event {t ∈
Ψs} only depends on the results of trials τ ∈ ∪σ<sΨσ(t)
and on w(s)
t,a , we know it only
depends on the feature vectors xi,ai, i ∈ Ψs(t) and on xt,i.
This implies the lemma.

t,a . From the deﬁnition of w(s)

√

The theorem demonstrates an ˜O(
dT log K) regret bound
for the algorithm SupCB-GLM. It has been proved in Chu
√
dT is the minimax lower
et al. (2011, Theorem 2) that
bound of the expected regret for K-armed linear bandits,
a special of the GLM bandits considered here. Therefore,

Generalized Linear Contextual Bandits

the regret of our SupCB-GLM algorithm is optimal up to
logarithm terms of T and K. To the best of our knowledge,
this is the ﬁrst algorithm which achieves the (near-)optimal
rate of GLM bandits.

It is worthwhile to compare Theorem 3 with the result in
Theorem 2. When K = o(2d) is small, the rate of SupCB-
√
d
GLM is faster, and we improve the previous rates by a
factor. Here, we give a brieﬂy illustration of how we get rid
d factor. Both in Theorem 2 and in Filippi
of the extra
et al. (2010), |x(cid:48)(ˆθn − θ∗)| is upper bounded by using the
Cauchy-Schwartz inequality,

√

|x(cid:48)(ˆθn − θ∗)| ≤ (cid:107)x(cid:107)V −1

n

ˆθn − θ∗(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Vn
(cid:13)

.

(10)

Lemma 3 in the supplementary material establishes that

ˆθn − θ∗(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Vn
(cid:13)

≤ C2

(cid:112)d log(T /δ).

√

d factor compared to (5). By
This will lead to an extra
using the Cauchy-Schwartz inequality (10), we only make
use of the fact that ˆθn is close to θ∗ in the (cid:96)2 sense. How-
ever, (5) tells us that actually ˆθn is close to θ∗ in every
direction. This is the reason why we are able to remove
d factor to achieve a near-optimal regret. It also
the extra
explains why the bound in Theorem 2 is tight when K is
large. As K goes large, it is likely there is a direction x for
which (10) is tight.

√

√

least 1 −
dT ). By Theorem 1 and union bound, we have

for some constant c with probability at
exp(−
the desired result under condition (9).

Lemma 6. Suppose that event EX holds, and that in round
t, the action at is chosen at stage st. Then, a∗
t ∈ As for all
s ≤ st. Furthermore, we have

θ∗)

t,at

µ(x(cid:48)

≤

θ∗) − µ(x(cid:48)
t,a∗
t
(cid:26) (8Lµ)/2st
√
(2Lµ)/

if at is selected in step 2b
T if at is selected in step 2c .

Deﬁne Vs,t = (cid:80)
(cid:88)

t∈Ψs(T ) XtX (cid:48)
(cid:88)

w(s)

t,at =

t∈Ψs(T )

t∈Ψs(T )

t, then by Lemma 2,

α(δ)(cid:107)xt,at(cid:107)V −1

s,t

≤ α(δ)(cid:112)2d log(T /d)|Ψs(n)| .

On the other hand, by the step 2b of SupCB-GLM,

(cid:88)

t∈Ψs(T )

w(s)

t,at ≥ 2−s|Ψs(T )|.

Combining the above two inequalities gives us

|Ψs(T )| ≤ 2sα(δ)(cid:112)2d log (T /d)|Ψs(T )|.

(12)

Let Ψ0 be the collection of trials such that at is chosen in
step 2c. Since we have chose S = log2 T , each t ∈ [τ +
1, T ] must be in one of Ψs and hence, {τ, τ + 1, . . . , T } =
Ψ0 ∪ (cid:0)∪S

s=1Ψs(T )(cid:1). If we set τ =

dT , we have

√

Proof of Theorem 3. To facilitate our proof, we ﬁrst
present two technical lemmas. Lemma 5 follows from
Lemma 4, Theorem 1, Theorem 5.39 of Vershynin (2012)
and a union bound. The proof of Lemma 6 is deferred to
the appendix.

Lemma 5. Fix δ > 0. Choose in SupCB-GLM τ =
and α = 3σ
κ
(9). Deﬁne the following event:

dT
(cid:112)2 log(T K/δ). Suppose T satisﬁes condition

√

τ
(cid:88)

(cid:16)

RT =

µ(x(cid:48)

t,a∗
t

θ∗) − µ(x(cid:48)

t,at

(cid:17)

θ∗)

µ(x(cid:48)

t,a∗
t

θ∗) − µ(x(cid:48)

t,at

(cid:17)

θ∗)

(cid:17)

θ∗)

≤ τ +

µ(x(cid:48)

t,a∗
t

θ∗) − µ(x(cid:48)

t,at

t=1

+

T
(cid:88)

(cid:16)

t=τ +1
(cid:88)

(cid:16)

t∈Ψ0

EX := {|m(s)

t,a − x(cid:48)

t,aθ∗| ≤ w(s)
t,a ,
∀t ∈ [τ + 1, T ], s ∈ [S], a ∈ [K]}

(11)

Then, event EX holds with probability at least 1 − δ.

Proof of Lemma 5. By Lemma 4, we have independent
samples now. Then to apply Theorem 1, the key is to
lower bound the minimum eigenvalue of Vt. Note that we
dT
randomly select the feature vectors at the ﬁrst τ =
rounds, that is, they are independent. Moreover, the fea-
ture vectors are bounded. Thus, X1, X2, . . . , Xτ are inde-
pendent sub-Gaussian with parameter 1.
It follows from
Proposition 1 that

√

λmin(Vt) ≥ λmin(Vτ ) ≥ c

dT

√

+

√

√

√

S
(cid:88)

(cid:88)

(cid:16)

s=1

t∈Ψs(T )

µ(x(cid:48)

t,a∗
t

θ∗) − µ(x(cid:48)

t,at

(cid:17)

θ∗)

Lµ · 23−s · |Ψs(T )|

≤

≤

dT + T ·

dT + 2Lµ

+

S
(cid:88)

s=1

2Lµ√
T
√

T
(cid:114)

S
(cid:88)

s=1
√

+8Lµα(δ)

2d log

|Ψs(T )|

T
d

dT + 2Lµ

T + 8Lµα(δ)(cid:112)2d log(T /d)
≤
√
≤ 45(σLµ/κ)(cid:112)log T log(T K/δ) log(T /d)

√

ST

dT ,

with probability at least 1 − δ. Here, the ﬁrst inequality is
due to the assumption that 0 ≤ µ ≤ 1. The second inequal-
ity is Lemma 6. The third inequality is the inequality (12)

Generalized Linear Contextual Bandits

and the fourth inequality is implied by Cauchy-Schwartz.
This completes the proof of the high-probability result.

follows that,

5. Discussions

In this paper, we propose two algorithms for K-armed ban-
dits with generalized linear models. While the ﬁrst algo-
rithm, UCB-GLM, achieves the optimal rate for the case
of inﬁnite number of arms, the second algorithm SupCB-
GLM is provable optimal for the case of ﬁnite number ac-
tions at each round. However, it remains open whether
UCB-GLM can achieve the optimal rate for small K.

T
(cid:88)

t=τ +1

1
(cid:112)λmin(Vt)

=

T
(cid:88)

t=τ +1

Θ(t−1/2) = O(

T ).

√

It should be cautioned that, since we do not know the distri-
bution of our feature vectors, we cannot assume the above
gap exists. It is therefore challenging to make the above
In fact, when studying the ARIMA
arguments rigorous.
model in time series, Lai & Wei (1982, Example 1) pro-
vide an example such that λmin(Vt) = O(log t).

5.1. A better regret bound for UCB-GLM

5.2. Open Questions

Computational efﬁcient algorithms. While UCB-GLM
and SupCB-GLM enjoy good theoretical properties, they
can be expensive in some applications. First, they require
inverting a d × d matrix in every step, a costly operation
when d is large. Second, at step t, the MLE is computed
using Θ(t) samples, meaning that the per-step complexity
grows at least linearly with t for a straightforward imple-
mentation of the algorithms. It is therefore interesting to
investigate more scalable alternatives. It is possible to use a
ﬁrst-order, iterative optimization procedure to amortize the
cost, analogous to the approach of Agarwal et al. (2014).

K-dependent lower bound. Currently, all
the lower
bound results on (generalized) linear bandits have no de-
pendence on K, the number of arms. The minimax lower
bound will be of particularly interest because all current
lower bound results assume that K ≤ d. Although it will
at most be a logarithm dependence on K, it is still a theo-
retically interesting question.

Randomized algorithms with optimal regret rate. As
opposed to the deterministic, UCB-style algorithms stud-
ied in this paper, randomized algorithms like EXP4 (Auer
et al., 2002b) and Thompson Sampling (Thompson, 1933)
have advantages in certain situations, for example, when re-
ward observations are delayed (Chapelle & Li, 2012). Re-
cently developed techniques for analyzing Bayes regret in
BLM bandits (Russo & Van Roy, 2014) may be useful to
analyze the cumulative regret considered here.

A key quantity in determining the regret of UCB-GLM is
the minimum eigenvalue of Vt.
If we make an addition
assumption on the minimum eigenvalue of Vt, we will be
able to prove an O(

dT ) regret bound for UCB-GLM.

√

Theorem 4. We run algorithm UCB-GLM with τ =
8σ2
κ2 d log T and α ≤ Lµσ/κ. For any δ ∈ [1/T, 1), sup-
pose there is an universal constant c such that

holds with probability at least 1 − δ, and

T
(cid:88)

t=τ +1

λ−1/2
min (Vt) ≤ c

√

T .

T = Ω

(cid:18) σR
κLµ

(cid:19)

d log2 T

.

(13)

(14)

Then, the regret of the algorithm is bounded by

RT ≤

CLµσ
κ

(cid:112)dT log(T /δ)

with probability at least 1 − 2δ, where C is a positive, uni-
versal constant.

This theorem provides some insights of why UCB-GLM
performs well in practice. Although the condition in (13) is
hard to check and may be violated in some cases, for exam-
ple, in K-armed bandits, we provide a heuristic argument
to justify this assumption in a range of problems. When
t is large enough, our estimator ˆθt is very close to θ∗. If
, θ∗(cid:105) and
we assume there is a positive gap between (cid:104)xt,a∗
(cid:104)xt,a, θ∗(cid:105) for all a (cid:54)= a∗
t , we will have at = a∗
t after, for
T steps. Since {xt,a, a ∈ [K]} are independent
example,
for t ∈ [T ], {xt,a∗
} are also independent samples. Then
Vt/t will be well-approximated by the covariance matrix of
xt,a∗
t , which we denote by Σ0. In many problem in prac-
tice, especially when features are dense, it is unlikely the
feature vector xt,a∗
t lies in a low-dimensional subspace of
Rd. It implies that Σ0 has full rank, and that we will have
λmin(Vt) = Θ(t · λmin(Σ0)) when t is large enough. It

√

t

t

Generalized Linear Contextual Bandits

References

Abbasi-Yadkori, Yasin, P´al, D´avid, and Szepesv´ari, Csaba.
Improved algorithms for linear stochastic bandits. In Ad-
vances in Neural Information Processing Systems 24, pp.
2312–2320, 2011.

Agarwal, Alekh, Hsu, Daniel, Kale, Satyen, Langford,
John, Li, Lihong, and Schapire, Robert E. Taming the
monster: A fast and simple algorithm for contextual ban-
dits. In Proceedings of the 31th International Conference
on Machine Learning (ICML), pp. 1638–1646, 2014.

Agarwal, Deepak, Chen, Bee-Chung, Elango, Pradheep,
Motgi, Nitin, Park, Seung-Taek, Ramakrishnan, Raghu,
Roy, Scott, and Zachariah, Joe. Online models for con-
In Advances in Neural Information
tent optimization.
Processing Systems 21, pp. 17–24, 2009.

Auer, Peter. Using conﬁdence bounds for exploitation-
exploration trade-offs. The Journal of Machine Learning
Research, 3:397–422, 2002.

Chu, Wei, Li, Lihong, Reyzin, Lev, and Schapire, Robert E.
Contextual bandits with linear payoff functions. In Pro-
ceedings of the 14th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS), pp. 208–214,
2011.

Dani, Varsha, Hayes, Thomas P, and Kakade, Sham M.
Stochastic linear optimization under bandit feedback. In
Proceedings of the 21st Annual Conference on Learning
Theory (COLT), pp. 355–366, 2008.

Fahrmeir, Ludwig and Kaufmann, Heinz. Consistency and
asymptotic normality of the maximum likelihood esti-
mator in generalized linear models. The Annals of Statis-
tics, 13(1):342–368, 1985.

Filippi, Sarah, Cappe, Olivier, Garivier, Aur´elien, and
Szepesv´ari, Csaba. Parametric bandits: The generalized
linear case. In Advances in Neural Information Process-
ing Systems 23, pp. 586–594, 2010.

Auer, Peter, Cesa-Bianchi, Nicolo, and Fischer, Paul.
Finite-time analysis of the multiarmed bandit problem.
Machine learning, 47(2-3):235–256, 2002a.

Lai, Tze Leung and Robbins, Herbert. Asymptotically ef-
ﬁcient adaptive allocation rules. Advances in Applied
Mathematics, 6(1):4–22, 1985.

Auer, Peter, Cesa-Bianchi, Nicolo, Freund, Yoav, and
Schapire, Robert E. The nonstochastic multiarmed ban-
dit problem. SIAM Journal on Computing, 32(1):48–77,
2002b.

Lai, Tze Leung and Wei, Ching Zong. Least squares esti-
mates in stochastic regression models with applications
to identiﬁcation and control of dynamic systems. The
Annals of Statistics, 10(1):154–166, 1982.

Beygelzimer, Alina, Langford, John, Li, Lihong, Reyzin,
Lev, and Schapire, Robert E. Contextual bandit algo-
rithms with supervised learning guarantees. In Proceed-
ings of the 14th International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), pp. 19–26, 2010.

Bickel, Peter J, Ritov, Ya’acov, and Tsybakov, Alexan-
dre B. Simultaneous analysis of Lasso and Dantzig se-
lector. The Annals of Statistics, 37(4):1705–1732, 2009.

Brown, Lawrence D. Fundamentals of Statistical Expo-
nential Families with Applications in Statistical Decision
Theory, volume 9 of Lecture Notes-Monograph Series.
Institute of Mathematical Statistics, 1986.

Bubeck, S´ebastien and Cesa-Bianchi, Nicolo. Regret anal-
ysis of stochastic and nonstochastic multi-armed bandit
problems. Foundations and Trends in Machine Learn-
ing, 5(1):1–122, 2012.

Chapelle, Olivier and Li, Lihong. An empirical evaluation
of Thompson sampling. In Advances in Neural Informa-
tion Processing Systems 24, pp. 2249–2257, 2012.

Chen, Kani, Hu, Inchi, and Ying, Zhiliang. Strong consis-
tency of maximum quasi-likelihood estimators in gener-
alized linear models with ﬁxed and adaptive designs. The
Annals of Statistics, 27(4):1155–1163, 1999.

Lehmann, Erich Leo and Casella, George. Theory of Point
Estimation, volume 31 of Springer Texts in Statistics.
Springer Science & Business Media, 1998.

Li, Lihong, Chu, Wei, Langford, John, and Schapire,
Robert E. A contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th
International Conference on World Wide Web (WWW),
pp. 661–670. ACM, 2010.

Li, Lihong, Chu, Wei, Langford, John, Moon, Taesup, and
Wang, Xuanhui. An unbiased ofﬂine evaluation of con-
textual bandit algorithms with generalized linear mod-
els. JMLR Workshop and Conference Proceedings, 26:
19–36, 2012.

McCullagh, Peter and Nelder, John A. Generalized Linear

Models, volume 37. CRC press, 1989.

Pollard, David. Empirical processes: Theory and applica-
tions. In NSF-CBMS regional conference series in prob-
ability and statistics, pp. i–86. JSTOR, 1990.

Rusmevichientong, Paat and Tsitsiklis, John N. Linearly
parameterized bandits. Mathematics of Operations Re-
search, 35(2):395–411, 2010.

Generalized Linear Contextual Bandits

Russo, Daniel and Van Roy, Benjamin. Learning to opti-
mize via posterior sampling. Mathematics of Operations
Research, 39(4):1221–1243, 2014.

Sarkar, Jyotirmoy. One-armed bandit problems with co-
variates. The Annals of Statistics, 19(4):1978–2002,
1991.

Thompson, William R. On the likelihood that one unknown
probability exceeds another in view of the evidence of
two samples. Biometrika, 25(3–4):285–294, 1933.

Valko, Michal, Munos, R´emi, Kveton, Branislav, and
Koc´ak, Tom´aˇs. Spectral bandits for smooth graph func-
tions. In Proceedings of the 31th International Confer-
ence on Machine Learning (ICML), pp. 46–54, 2014.

Van der Vaart, Aad W. Asymptotic Statistics, volume 3.

Cambridge university press, 2000.

Vershynin, Roman.

Introduction to the non-asymptotic
analysis of random matrices.
In Eldar, Yonina C. and
Kutyniok, Gitta (eds.), Compressed Sensing: Theory and
Applications, pp. 210–268. Cambridge University Press,
2012.

Woodroofe, Michael. A one-armed bandit problem with a
concomitant variable. Journal of the American Statisti-
cal Association, 74(368):799–806, 1979.

