Identify the Nash Equilibrium in Static Games with Random Payoffs

Yichi Zhou 1 Jialian Li 1 Jun Zhu 1

Abstract

We study the problem on how to learn the pure
Nash Equilibrium of a two-player zero-sum static
game with random payoffs under unknown dis-
tributions via efﬁcient payoff queries. We intro-
duce a multi-armed bandit model to this problem
due to its ability to ﬁnd the best arm efﬁciently
among random arms and propose two algorithms
for this problem—LUCB-G based on the conﬁ-
dence bounds and a racing algorithm based on
successive action elimination. We provide an
analysis on the sample complexity lower bound
when the Nash Equilibrium exists.

1. Introduction

We consider the static zero-sum game where two players
are involved with ﬁnite pure strategies. From game the-
ory, if both players use only pure strategies and the payoffs
are distinct from each other, at most one pure Nash Equi-
librium (NE) exists (Osborne & Rubinstein, 1994). We
concentrate on the setting where all payoffs are random
variables under some unknown distributions. Samples (or
queries) can be obtained by submitting pure strategies of
the two players and receiving the associated payoffs. Our
target is to answer the questions: (1) whether there is a pure
NE; and 2) how to identify it if exists using as few queries
as possible.

Our motivation for this problem comes from the need of
identifying NE in many practical competitive situations.
Since NE is a fundamental concept in game theory and
many other ﬁelds, the computational complexity needed for
NE is of much interest. However, in practice we are often
given access to the data generated from some practical phe-
nomena, rather than a clear rule for the payoffs of the game.
Hence, the empirical game-theoretic analysis (Wellman,
2006; Jordan et al., 2008) has received a lot of attention

1Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab
for Intell. Tech. & Systems, CBICR Center, Tsinghua University.
Correspondence to: Jun Zhu <dcszj@tsinghua.edu.cn>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

to estimate the practical games through simulation. In the
empirical modeling, pure-strategy proﬁles of players are
submitted to the game and we receive the associated pay-
offs. Fearnley et al. (2015) consider the process managed
in an online manner by algorithms and analyze the com-
plexity of these payoff-query algorithms. The main focus
is on whether the query methods can ﬁgure out mixed Nash
Equilibrium with only a fraction of proﬁles. Extensions
have been made to obtain query complexity on approximate
Nash Equilibrium (Babichenko, 2016), correlated equilib-
rium (Hart & Nisan, 2016), and well-supported approxi-
mate correlated equilibrium (Goldberg & Roth, 2016).

The above work is essentially a revealed-payoff search
model (Jordan et al., 2008), where payoffs are deterministic
and every proﬁle only needs to be queried at most once. We
concentrate on the noisy-payoff model (Jordan et al., 2008),
where the received payoff of a query is a sample of an un-
derlying distribution. This random payoff setting is more
realistic in practice, where randomness naturally arises be-
cause of incomplete information, noise, or other stochastic
factors in the world. A simple but well-known example is
the coin ﬂipping game, where two players throw a coin and
guess its landing upper side. Since the physical process of
coin landing can be determined by many noisy factors, the
payoffs to the two players, which depend on the landing
results, are random. This notable discrepancy leads to dif-
ferent algorithms and complexity bounds for the two mod-
els, since for noisy-payoff models, more queries are needed
for any proﬁle to get an estimated payoff near its expecta-
tion value with high probability and the additional compu-
tational cost can take a dominating role in complexity. Pre-
vious work has explored different methods for noisy pay-
off models, such as interleaving the samples (Walsh et al.,
2003) and using regression for payoffs (Vorobeychik et al.,
2007). This paper turns to bandit models, a relatively natu-
ral direction from the view of online learning, since query
methods themselves hold a sequential property. Put another
way, we select a strategy to submit based on previous ob-
servations at each round of query methods.

This task can be viewed as a variant of best arm iden-
tiﬁcation (BAI) in the literature of multi-armed bandits
(Jamieson & Nowak, 2014), where different pure strategy
proﬁles are regarded as arms. The classical BAI problem
is to identify which arm is the one with the highest mean.

Identify the Nash Equilibrium in Static Games with Random Payoffs

There are two basic settings for BAI problem—ﬁxed bud-
get and ﬁxed conﬁdence (Kaufmann et al., 2015). In this
paper, we focus on the ﬁxed conﬁdence setting, where the
purpose of an algorithm is to identify the best arm with a
ﬁxed probability by as few pulls (queries) as possible.

Contributions: We study both the sample complexity
lower bound and algorithms in the ﬁxed conﬁdence setting
for two-player zero-sum static games with random payoffs.

In Section 3, we discuss the sample complexity lower
bound for the case NE exists. Previous proofs on the lower
bound for BAI all rely on changes of distributions (Au-
dibert & Bubeck, 2010; Kaufmann et al., 2015; Mannor &
Tsitsiklis, 2004)—changes on a single arm can change the
best arm in the bandit model. For our problem, we prove
the lower bound for the arms in the same row or same col-
umn with the NE by similar techniques. However, this ap-
proach does not work for those arms that are in neither the
same row nor column with the NE, since changing the dis-
tribution of such an arm does not change the NE (details
are in Section 3). To get the lower bound on these arms,
we rephrase the arm selection as a hypothesis testing prob-
lem and use the minimax techniques for hypothesis test-
ing (Tsybakov, 2009) to get the bound.

There are two types of algorithms for the BAI prob-
lem in the ﬁxed conﬁdence setting (Jamieson & Nowak,
2014)—based on either conﬁdence bound (Kalyanakrish-
nan et al., 2012) or successive eliminations on suboptimal
arms (Maron & Moore, 1997). In Section 4, we propose
two corresponding algorithms to identify NE for our prob-
lem in the ﬁxed conﬁdence setting. The ﬁrst algorithm has
a provable bound on sample complexity, and we show that
the second one will stop in a ﬁnite number of time steps
with probability at least 1 − δ.

Related work: Garivier et al. (2016) also study the two-
player zero-sum game with random payoffs. They consider
the case that each player selects her strategy one-by-one,
while we focus on the case that both players select their
strategies simultaneously. Our setting is suitable for the
case that each player chooses actions independently. Many
games are static, such as the tai sai game where players
independently guess the range of the outputs of three dices.

Much work has been done on Nash Equilibrium.
Daskalakis et al. (2009) shows that it is computationally
hard to recognize exact Nash Equilibrium, even for the
simplest two-person game (Chen et al., 2009). As em-
pirical game-theoretical analysis (Wellman, 2006) is pro-
posed, Fearnley et al. (2015) studies the payoff-query algo-
rithms and considers the query complexity as a criterion for
computational complexity. Following work (Babichenko,
2016; Goldberg & Roth, 2016) has extended this criterion
to some other approximate equilibrium. Query bounds for

NE have also been given on speciﬁc games such as two-
strategy anonymous games (Goldberg & Turchetta, 2015)
and bimatrix games (Fearnley & Savani, 2016).

2. Preliminaries

2.1. Basic settings

We start by presenting the basic settings, notations and as-
sumptions that will be used in the sequel.

Two-player zero-sum static game with random payoffs:
A static game is a model in which all players choose their
strategies once and simultaneously. A two-player zero-sum
game involves two players 1 and 2, and each player chooses
her own strategy si from a strategy set Si, i ∈ {1, 2}. Af-
ter decisions are made, player 1 gets payoff rews1,s2 and
player 2 gets −rews1,s2. Each player tries to maximize her
payoff. The game can be represented by a m × n-matrix
where m = |S1|, n = |S2|. If rewi,j is a deterministic
value for any i, j, it is direct to identify the NE (i∗, j∗)
which has the minimum value in row i∗ and the maximum
value in column j∗. We consider the more practical games
with random payoffs, whose distributions are unknown.
This makes the identiﬁcation of NE difﬁcult and hence we
employ query methods to learn the NE empirically. In each
query, the algorithm generates a pure strategy (s1, s2), and
the environment returns the associated payoff. Our target
is to determine whether the Nash Equilibrium (NE) exists1
and what it is if it does exist with as few queries as possible.

Multi-armed bandits: In a bandit model, an agent is fac-
ing a set of actions (or arms), and needs to select one arm
to pull every time. In our case, an arm is speciﬁed by s ∈
[m] × [n], where [m] denotes the set {1, · · · , m}. Succes-
sive pulls of an arm (i, j) yield a sequence of observations
(or rewards) Y i,j
2 , · · · . A policy I = {It : t ∈ N+}
denotes a sequence of random variables, where the variable
It ∈ [m] × [n] indicates which arm to pull at time step t.

1 , Y i,j

The classical best arm identiﬁcation (BAI) problem is to
identify which arm is the one with the highest mean. There
are two basic settings for the BAI problem—ﬁxed budget
and ﬁxed conﬁdence (Kaufmann et al., 2015). In this paper,
we focus on the identiﬁcation of the NE of a static game or
detect its absence in the ﬁxed conﬁdence setting. That is,
we aim to identify the correct NE with probability at least
1 − δ with efﬁcient sampling, where δ ∈ (0, 1) is the con-
ﬁdence parameter. Algorithms satisfying this requirement
are known as δ-PAC algorithms (Kaufmann et al., 2015).

Following Kaufmann et al. (2015), a practical BAI al-
gorithm in the ﬁxed conﬁdence setting typically consists of:

• Policy: given a sequence of past observations, a

policy determines which arms to pull.

• Stopping rule: a stopping rule can be described as

1We focus on the NE of pure strategy. So NE may not exist.

Identify the Nash Equilibrium in Static Games with Random Payoffs

a series of observation sets Ft, t ∈ N+. When an
element o ∈ Ft is observed, the policy stops sampling.
• Recommendation rule: a recommendation rule is

usually to recommend the best arm.

As we shall see, in our problem the best arm may not exist,
when the sampling process stops, the recommendation rule
determines whether the NE exists or not, and if exists, it
determines which is the NE.

2.2. Basic assumptions and notations
Let Pi,j ∈ P be the underlying distribution of arm (i, j)
and P is a set of probability measures. For an arm s, we
use µs to denote the expectation of Ps. And let ¯µs(t) de-
note the empirical mean of s at time step t, we omit t for
simplicity when there is no ambiguity . Here, we consider
pulling an arm once as a time step, that is, time step t means
that we have pulled arms for t times. We use ¯Mt to denote
the empirical matrix with entry (i, j) representing the em-
pirical mean of Pi,j at time step t. For s = (s1, s2), we
deﬁne row(s) = {s(cid:48) = (s(cid:48)
1, s(cid:48)
1 = s1}, col(s) =
{s(cid:48) = (s(cid:48)
2) : s(cid:48)
1, s(cid:48)
2 = s2}, nei(s) = row(s) ∪ col(s), and
s[1] = s1, s[2] = s2.

2) : s(cid:48)

Let N E(M) denote the NE of matrix M.
Formally,
N E(M) = s if there is an arm s such that µs =
mins(cid:48)∈row(s) µs(cid:48) and µs = maxs(cid:48)∈col(s) µs(cid:48); otherwise if
there is no such arm, we denote N E(M) = none to show
that no Nash Equilibrium exists. Speciﬁcally, we use M
to denote the matrix whose entry (i, j) is the expectation
of distribution Pi,j. Our target is to identify N E(M). For
convenience, let s∗ = N E(M). Let Ri, Cj be the sets of
the arms corresponding to the i-th row and the j-th column
of M respectively. Our lower bound mainly focuses on the
case that the NE exists (i.e., N E(M) (cid:54)= none) and our al-
gorithms are δ-PAC. We assume that the expectations of the
arms are mutually different.

In the proof of the lower bound, it is natural to make P
abundant enough to include various continuous distribu-
tions while ruling out some extreme situations where dis-
tributions are not mutually absolutely continuous. So we
assume that P consists of parametric distributions continu-
ously parameterized by their means. This assumption has
been widely used in studying multi-armed bandits (Lai &
Robbins, 1985; Kaufmann et al., 2015).

Assumption 1. For all p, q ∈ P such that p (cid:54)= q, for all
α > 0:

• ∃q1 ∈ P, KL(p, q) < KL(p, q1) < KL(p, q) + α,

Eq1 > Eq > Ep.

Eq2 < Eq < Ep.

• ∃q2 ∈ P, KL(p, q) < KL(p, q2) < KL(p, q) + α,

Here KL(q, p) is the KL-divergence. Many distributions
are included in P, such as the broad class of one-parameter

exponential family distributions.

3. Lower bound

Let Nδ(s) denote the number of pulls on an arm s by a δ-
PAC algorithm. For arm s ∈ nei(s∗), we provide a lower
bound of Nδ(s) in Lemma 1, which is obtained by the
classical technique of changes of distributions (Kaufmann
et al., 2015; Lai & Robbins, 1985; Audibert & Bubeck,
2010) and Theorem 1 in Kaufmann et al. (2015).
Theorem 1. (Kaufmann et al., 2015). Let v and v(cid:48) be two
bandit models with K arms, such that for all a ∈ [K],
the distributions Pa and P(cid:48)
a are mutually absolutely con-
tinuous. For any almost-surely ﬁnite stopping time σ with
respect to Ft, we have

(cid:88)

a≤K

E[Nδ(sa)]KL(Pa, P(cid:48)

a) ≥ sup
E∈(Ft)

d(Pv(ε), Pv(cid:48)(E)),

where d(x, y) = x log(x/y) + (1 − x) log[(1 − x)/(1 − y)].
Lemma 1. Let s(cid:48) = arg mins∈nei(s∗)\s∗ KL(Ps∗ , Ps),
then the number of pulls on nei(s∗) of any δ-PAC algo-
rithm has a lower bound as follows:

(cid:88)

E[Nδ(s)] ≥

s∈nei(s∗)

(cid:18)

1
KL(Ps∗ , Ps(cid:48))
1
(cid:88)
KL(Ps, Ps∗ )

s∈nei(s∗)\s∗

+

(cid:19)

log

1
2.4δ

.

Proof. By Assumption 1, for all arms s ∈ nei(s∗), there
exists an alternative model, in which the only arm modiﬁed
is arm s, and the modiﬁed distribution P(cid:48)

s satisﬁes:
s) < KL(Ps, Ps∗ ) + α,

• KL(Ps, Ps∗ ) < KL(Ps, P(cid:48)

and EP(cid:48)

s < µs∗ for s ∈ row(s∗)\s∗

• KL(Ps, Ps∗ ) < KL(Ps, P(cid:48)

s) < KL(Ps, Ps∗ ) + α,

and EP(cid:48)

s > µs∗ for s ∈ col(s∗)\s∗

• KL(Ps∗ , Ps(cid:48)) < KL(Ps∗ , P(cid:48)

s∗ < µs(cid:48) for s(cid:48) ∈ col(s∗)\s∗ or EP(cid:48)

s∗ ) < KL(Ps∗ , Ps(cid:48)) + α,
s∗ > µs(cid:48)

and EP(cid:48)
for s(cid:48) ∈ row(s∗)\s∗.

In particular,

the NE for v(cid:48)

Denote the original bandit model by v and the modiﬁed
one by v(cid:48).
is no longer
s∗. Consider the event E : the recommendation rule
recommends s∗ as NE. Any δ-PAC algorithm satisﬁes
Pv(E) > 1 − δ and Pv(cid:48)(E) < δ, so by Theorem 1,
E[Nδ(s)]KL(Ps, P(cid:48)
2.4δ .
Hence we have:
E[Nδ(s)] ≥

s) ≥ d(Pv(E), Pv(cid:48)(E)) ≥ log 1
(cid:40) log 1/(2.4δ)

log 1/(2.4δ)
KL(Ps, P(cid:48)
s)
Let α → 0 and we complete the proof.

KL(Ps,Ps∗ )+α , s (cid:54)= s∗
s(cid:48) )+α , s = s∗

KL(Ps∗ ,P

log 1/(2.4δ)

≥

From the proof, we can see that the lower bound re-
lies on the fact that we can change the best arm (i.e.,
N E(M) in our case) by changing the distribution of a sin-
gle arm. However, this proof technique is not suitable for

Identify the Nash Equilibrium in Static Games with Random Payoffs

s /∈ nei(s∗) because the NE will not change no matter what
the distribution of an arm s /∈ nei(s∗) is.

In theory, we only need to pull s ∈ nei(s∗) to identify NE
because of the same reason (i.e., the distributions of arm
s /∈ nei(s∗) won’t change NE). In practice however a pol-
icy does not know which arm is in nei(s∗) in advance, so
it may make some pulls on s /∈ nei(s∗) before making
a sufﬁcient number of pulls on nei(s∗). So we can con-
sider the arm selection as a hypothesis testing problem, and
then use the lower bound techniques for the minimax risk
of hypothesis testing (See Chapter 2 in Tsybakov (2009)).
Speciﬁcally, our proof is based on the following lemma:

Lemma 2. Let P1, · · · , PK be probability distributions
supported on some set X , with Pi absolutely continuous
w.r.t P1. For any measurable function ψ : X → [K], we
have: K
(cid:88)

K
(cid:88)

Pk(ψ = k) ≥

exp{−

KL(P1, Pk)},

1
e

k=1

k=2

where Pk(ψ = k) := Pk({x : ψ(x) = k}) for clarity.

Ht ≥

e−∆(1 − e−t∆)
4e(1 − e−∆)

.

(1)

Ht ≥

Proof. With straight-forward computations, we have:
(cid:0)Ht(h1,1) + Ht(hi,j) + Ht(h1,j) + Ht(hi,1)(cid:1)
(cid:18) (cid:88)

1[It(cid:48) /∈ nei((1, 1)); h1,1]

=

1
4
1
4

t(cid:48)≤t

+ 1[It(cid:48) /∈ nei((i, j)); hi,j] + 1[It(cid:48) /∈ nei((1, j)); h1,j]
(cid:19)

+ 1[It(cid:48) /∈ nei((i, 1)); hi,1]

(cid:88)

(cid:18) (cid:88)

≥

1
4

t(cid:48)≤t

i(cid:48),j(cid:48)≥2
+ 1[It(cid:48) = (1, 1); hi,j] +

1[It(cid:48) = (i(cid:48), j(cid:48)); h1,1]

1[It(cid:48) = (1, j(cid:48)); hi,1]

(cid:88)

j(cid:48)≥2
(cid:19)
.

+

(cid:88)

i(cid:48)≥2

1[It(cid:48) = (i(cid:48), 1); h1,j]

Proof. This lemma is an extension of Lemma 2.6 in Tsy-
bakov (2009) from two distributions to multiple distribu-
tions. We put the proof in Appendix A.

Deﬁne P t
step t under the hypothesis ha,b, and deﬁne function

as the distribution of observations until time

ha,b

Now we show what the hypotheses to be tested in our prob-
lem are and how to apply Lemma 2. Without loss of gen-
erality, consider a game M with N E(M) = (1, 1). Let’s
consider a set of hypotheses: new games hi,j constructed
by swapping the i-th row with the ﬁrst row and the j-th col-
umn with the ﬁrst column of M. Obviously, these games
are essentially the same game up to permutation. We will
show the lower bound on the maximum number of pulls
among these hypotheses by arbitrary policies.

Formally, deﬁne fi,j(i(cid:48), j(cid:48)) = (i(cid:48)(cid:48), j(cid:48)(cid:48)) where i(cid:48)(cid:48) = i(cid:48) if
i(cid:48) /∈ {1, i} else i(cid:48)(cid:48) = 1 + i − i(cid:48) and j(cid:48)(cid:48) = j(cid:48) if j(cid:48) /∈ {1, j}
else j(cid:48)(cid:48) = 1 + j − j(cid:48). Let hi,j specify a game such that
the distribution of arm (i(cid:48), j(cid:48)) is Pfi,j (i(cid:48),j(cid:48)). Let Ht(hi,j)
be the sum of the expected number of pulls on arms s /∈
nei((i, j)) until time step t under hypothesis hi,j:

Ht(hi,j) :=

1[It(cid:48) /∈ nei((i, j)); hi,j],

(cid:88)

t(cid:48)≤t

g(i(cid:48), j(cid:48)) :=






(1, 1)

i(cid:48), j(cid:48) ≥ 2,

(1, j)

i(cid:48) ≥ 2, j(cid:48) = 1,

(i, 1)

j(cid:48) ≥ 2, i(cid:48) = 1,

(i, j)

i(cid:48) = j(cid:48) = 1.

i(cid:48),j(cid:48) = P t

Let P t
hg(i(cid:48) ,j(cid:48) )
selects arm s at time step t. We have:

. Consider events ev(t, s): policy

Ht ≥

P t(cid:48)
i(cid:48),j(cid:48)(ev(t(cid:48), (i(cid:48), j(cid:48))))

exp{−

KL(P t(cid:48)

1,1, P t(cid:48)

i(cid:48),j(cid:48))}

(cid:88)

i(cid:48),j(cid:48)

1
4

1
4e

1
4e

(cid:88)

(cid:88)

i(cid:48),j(cid:48)

t(cid:48)≤t
(cid:88)

t(cid:48)≤t

t
(cid:88)

t(cid:48)=1

≥

≥

=

exp{−t∆}

e−∆(1 − e−t∆)
4e(1 − e−∆)

.

and let Ht := maxi,j Ht(hi,j) be the maximum number
of pulls under any hypothesis. Then, theorem 2 shows the
lower bound of Ht.

≥ 2,

j(cid:48)≥2 M u(Pi,j(cid:48), P1,j(cid:48)) + (cid:80)i(cid:48)(cid:54)=i

Theorem 2. For any
let ∆ :=
i, j
(m − 1)(n − 1)(M u(Pi,j, P1,1) + M u(Pi,1, P1,j) +
(cid:80)j(cid:48)(cid:54)=j
i(cid:48)≥2 M u(Pi(cid:48),1, Pi(cid:48),j)) +
mM u(Pi,1, Pi,j)
where
Then,
M u(P1, P2)
we have the lower bound:

:= KL(P1, P2) + KL(P2, P1).

nM u(P1,j, Pi,j)

+

The second inequality is proven by Lemma 2. The third
is by the fact that let Pi,j(a, b) denote the distribution
of arm (a, b) under hypothesis hg(a,b),
then we have
i(cid:48),j(cid:48)) = (cid:80)
KL(P t
t(cid:48)≤t KL(P1,1(It(cid:48)), Pi(cid:48),j(cid:48)(It(cid:48)));
≤
and
(cid:80)
i(cid:48)(cid:48),j(cid:48)(cid:48) KL(P1,1(i(cid:48)(cid:48), j(cid:48)(cid:48)), Pi(cid:48),j(cid:48)(i(cid:48)(cid:48), j(cid:48)(cid:48))); summing over all
i(cid:48), j(cid:48), we get the third inequality.

that KL(P1,1(It(cid:48)), Pi(cid:48),j(cid:48)(It(cid:48)))

1,1, P t
note

We can get a different lower bound by choosing a different
i, j in Theorem 2, and take the maximum one. Though our

Identify the Nash Equilibrium in Static Games with Random Payoffs

lower bound is not on the expected number of pulls, it intu-
itively answers why the pulls on s /∈ nei(s∗) are unavoid-
able. Obviously, e−∆(1−e−t∆)
4e(1−e−∆) , which sug-
gests that there is a policy which pulls on s /∈ nei(s∗) for a
bounded number of times. This result inspires us to design
a policy with a bounded number of pulls on s /∈ nei(s∗),
as shown in Section 4.

4e(1−e−∆) ≤

e−∆

4. Algorithms

We now present two δ-PAC algorithms for our problem.
The ﬁrst one is inspired by LUCB (Kalyanakrishnan et al.,
2012) and UCB1 (Auer et al., 2002), while the second one
follows another line of BAI algorithms which are based on
the successive action eliminations (Even-Dar et al., 2006;
Maron & Moore, 1997).

4.1. LUCB-G

We ﬁrst present and analyze the LUCB-G (i.e., LUCB for
Game) algorithm, as illustrated in Alg. 1.

4.1.1. ALGORITHM
Informally, our problem can be divided into m + n bandit
tasks—m for identifying s∗
r(i) := arg mins∈Ri µs and n
for s∗
c (j) := arg maxs∈Cj µs. So in each round2, LUCB-
G can be divided into two stages. In the ﬁrst stage, it selects
two bandit tasks—a row and a column. Note that LUCB-G
r and s∗
tries to identify s∗
c after each round. If before round
γ, it identiﬁed ¯s∗
r(i) as the arm with minimum mean in Ri,
or identiﬁed ¯s∗
c (j) 3 as the arm with maximum mean in Cj,
then LUCB-G will not select row i or column j. That is to
say, we only select bandit models from the following rows
and columns at the γ-th round:

ar(γ) := {i ∈ [m] : ¯s∗
ac(γ) := {j ∈ [n] : ¯s∗

r(i) not identiﬁed until round γ.}
c (j) not identiﬁed until round γ.}

We’ll introduce how to identify these arms later in this
section.
In the second stage, we pull arms according
to past observations and some conﬁdence bound function
β : N+ × N+ → (0, ∞), which will be presented soon in
Section 4.1.2. Here we show our ﬁrst policy in Alg. 1.

We have a clock for each bandit task, and our conﬁdence
bounds rely on them. Deﬁne τr(i, t) as the set of all the
time steps t(cid:48) that satisfy the two requirements: (1) t(cid:48) < t;
(2) at the round when t(cid:48) takes place, row i is chosen, and at
least one line from 15 to 17 is executed. Similarly, deﬁne
τc(j, t) as the set of all the time steps t(cid:48) that satisfy the two
requirements: (1) t(cid:48) < t; (2) at the round when t(cid:48) takes
place, column j is chosen, and at least one line from 21 to
23 is executed.

The method of identifying ¯s∗

r(i) and ¯s∗

c (j) also relies on

the conﬁdence bound function β. For an arm s, deﬁne
L(s, u, t) = ¯µs(t(cid:48))−β(u, t), U (s, u, t) = ¯µs(t(cid:48))+β(u, t)4.
And let Ts(τ ) = {t : It = s, t ∈ τ }. Alg. 1 determines
r(i) and ¯s∗
¯s∗

c (j) at time step t as follows:

• If ∃s ∈ Ri,

for all s(cid:48) ∈ Ri\s, we have
U (s, |Ts(τ )|, |τ |) ≤ L(s(cid:48), |Ts(cid:48)(τ )|, |τ |), where
τ := τr(i, t), then Alg. 1 takes s as ¯s∗

r(i).

• If ∃s ∈ Cj,

for all s(cid:48) ∈ Cj\s, we have
L(s, |Ts(τ )|, |τ |) ≥ U (s(cid:48), |Ts(cid:48)(τ )|, |τ |), where
τ := τc(j, t), then Alg. 1 takes s as ¯s∗

c (j).

Now we introduce the stopping and recommendation rules
for Alg. 1.

Stopping and recommendation rules: The policy stops
and recommends N E as follows:

• If after round γ, there is an arm s, Alg. 1 takes it
c (s[2]). Then Alg. 1 stops and

r(s[1]) and ¯s∗
as ¯s∗
recommends s as the NE.
• Else if after round γ, ¯s∗

r(i) and ¯s∗

c (j) have all been
determined. Then the policy stops and the recommen-
dation rule determines that the underlying game does
not have a NE.

4.1.2. δ-PAC

We now show that Alg. 1 is a δ-PAC algorithm. Lemma
3 guarantees that if β(u, t) satisﬁes the requirements in
InEq. (2), then the probability that there is an arm violat-
ing its conﬁdence bounds is less than δ. Our choice of the
,5
conﬁdence bound function is β(u, t) =
which satisﬁes this requirement. And Theorem 3 is a sim-
ple application of Lemma 3 since if no arm violates its
conﬁdence bounds, the stopping and recommendation rules
won’t make mistakes.
Lemma 3. Let β(u, t) : N+ × N+ → (0, ∞) be a function
such that:

(cid:113) log(mnt4/4δ)
2u

∞
(cid:88)

t
(cid:88)

t=1

u=1

exp{−2uβ(u, t)2} ≤

(2)

δ
2K

.

Consider a bandit model v with K arms, for each arm,
there is a sequence (t1, u1), (t2, u2), · · · such that ti ≥
ui, ti+1 ≥ ti, ui+1 ≥ ui, and a sequence u(cid:48)
2, · · · such
that u(cid:48)
i ≥ ui, and then the probability that ∃s ∈ v, i such
that

1, u(cid:48)

|

1
u(cid:48)
i

u(cid:48)
i(cid:88)

i=1

Y s
i − µs| > β(ui, ti)

is less than δ.

4In this paper, we have t(cid:48) ≥ t. Thus in fact L and U are

functions of t(cid:48), but for convenience we omit the notation of t(cid:48).

5For convenience, let 1/0 = +∞. So if u = 0, β(u, t) =

2We pull arms for several times in each round.
3With a probability, ¯s∗

r(i) (cid:54)= s∗

r(i) or ¯s∗

c (j) (cid:54)= s∗

c (j).

+∞.

Identify the Nash Equilibrium in Static Games with Random Payoffs

Proof. The proof can be found in Appendix B.

Theorem 3. The probability of making mistakes by the rec-
ommendation and stopping rules of LUCB-G is at most δ.

Proof. If all arms don’t violate their conﬁdence bounds,
then it is easy to see that we determine NE correctly. As
our choice of β(u, t) satisﬁes the condition in InEq. (2),
we can use Lemma 3 to get the result.

4.1.3. SAMPLE COMPLEXITY
We analyze the sample complexity of Alg. 1 in this sec-
tion. We provide a proof of the sample complexity when
N E(M) (cid:54)= none here, and the sample complexity when
N E(M) = none is a straight-forward application of the
result in LUCB (Kalyanakrishnan et al., 2012).
For convenience, let Hr(i) = (cid:80)
s∈Ri\s∗
r (i))2
and Hc(j) = (cid:80)
1
c (j))2 . In the second
stage of each round, LUCB-G pulls arms similarly as
LUCB and UCB1. When the NE of the underlying game
exists, the pulls can be divided into three parts:

s∈Cj \s∗

(µs−µs∗

(µs−µs∗

c (j)

r (i)

1

• part1: Time steps t such that s∗ is the NE of ¯Mt.
• part2: Time steps t such that s (cid:54)= s∗ is the NE of ¯Mt.
• part3: Time steps t such that there is no NE of ¯Mt.

Therefore, the total sample complexity can be decomposed
as the summation of the bounds for the three parts.

We ﬁrst provide sample complexity bounds for the pulls in
part2 and part3, which appear because of the algorithm’s
misjudgments on which arm is N E(M) during training,
while deferring the bound for part1 to Lemma 6, which is
relatively standard.

time step t,
(cid:54)= arg mins(cid:48)∈row(s∗) ¯µs(cid:48)(t) or s∗

if part2 or part3 hap-
Obviously, at
then s∗
(cid:54)=
pens,
arg maxs(cid:48)∈col(s∗) ¯µs(cid:48)(t). Lemma 4 ensures that these
events will not happen with a high probability if Alg. 1 se-
lects row(s∗) and col(s∗) for a sufﬁciently large number
of times. The key idea is to use the UCB1 policy (line 20,
26) (Auer et al., 2002): considering column j, when the al-
gorithm chooses it in the ﬁrst stage, the algorithm pulls an
arm in Cj by UCB1. This ensures that with a high proba-
bility, the policy pulls a sufﬁciently large number of times
on s∗
c (j), and then by Hoeffding’s inequality, we get the
bound. A formal statement is in Lemma 4.

Lemma 4. Without loss of generality, consider column j,
let φ(γ) := 1[s∗
c (j) (cid:54)= arg maxs(cid:48)∈Cj ¯µs(cid:48) when Alg. 1 se-
lects column j for the γ-th time], let φ = (cid:80)∞
γ=1 φ(γ).
Then, the expectation of φ satisﬁes the inequality:

E[φ] − c1Hc(j)(log E[φ])2 − c2Hc(j) − c3 ≤ 0,

(3)

where c1, c2, c3 are positive constants.

Algorithm 1 LUCB-G
1: Input: distribution matrix M, conﬁdence δ
2: Pull all arms
3: Chr(i) = Chc(j) = 0 for i ∈ [m], j ∈ [n], t = m ∗ n
4: while Not stop do
5:
6:
7:
8:
9:

selr(i) = selc(j) = 0 for i ∈ [m], j ∈ [n]
if s = N E( ¯Mt) (cid:54)= none then
selr(s[1]) = selc(s[2]) = 1

else

Let ˆi := arg mini∈ar[γ] Chr(i)
Let ˆj := arg minj∈ar[γ] Chc(j)
selr(ˆi) = selc(ˆj) = 1

end if
if ∃ˆi ∈ [m], selr(ˆi) = 1 then
Chr(ˆi) = Chr(ˆi) + 1
Pull s1 := arg mins∈Rˆi

¯µs(t), t = t + 1
(cid:114)
2 log |τr(ˆi,t)|/3
|Ts(τr(ˆi,t))|

¯µs −

Pull arg mins∈Rˆi
Pull arg mins∈Rˆi\s1 L(s, |Ts(τr(ˆi, t))|, |τr(ˆi, t)|), t =
t + 1

, t = t + 1

end if
if ∃ˆj ∈ [n], selc(ˆj) = 1 then
Chc(ˆj) = Chc(ˆj) + 1
Pull s1 := arg maxs∈Cˆj

(cid:114)

¯µs(t), t = t + 1
2 log |τr(ˆj,t)|/3
|Ts(τr(ˆj,t))|

¯µs +

Pull arg maxs∈Cˆj
Pull arg maxs∈Cˆj \s1 U (s, |Ts(τr(ˆj, t)|), |τr(ˆj, t)|), t =
t + 1

, t = t+1

10:

11:
12:
13:
14:
15:

16:

17:

18:
19:
20:
21:

22:

23:

end if

24:
25: end while

If ξγ (cid:54)= s∗

c (j) − µξγ .

c (j)(tγ) ≤ µs∗

Proof. Let tγ denote the time step when Alg. 1 selects col-
umn j for the γ-th time and ξγ = arg maxs∈Cj ¯µs(tγ).
c (j), then we have
Let ∆γ = µs∗
¯µs∗
c (j) − ∆γ/2 or ¯µξγ (tγ) ≥ µξγ + ∆γ/2.
So let Set1(s) = {γ : ξγ = s}, Set2(s) = {γ ∈
Set1(s) : ¯µξγ (tγ) ≥ µξγ + ∆γ/2} and Set3(s) = {γ ∈
Set1(s) : ¯µs∗
c (j)(tγ) ≤ µs∗
c (j) − ∆γ/2}. With the above
c (j), we have E[|Set1(s)|] ≤
argument, for s ∈ Cj\s∗
E(|Set2(s)| + |Set3(s)|).

the policy pulls s for rounds γ ∈
Due to Alg. 1,
Set1(s). So by Hoeffding’s inequality, E|Set2(s)| ≤
(cid:80)∞
c (j) − µs)2.

c (j) − µs)/2)2} ≤ 2/(µs∗
Now consider Set3(s), which is computed as:

γ=1 exp{−2γ((µs∗

E|Set3(s)| = E

(cid:88)

1[¯µs∗

c (j)(tγ) ≤





γ∈Set1(s)

(µs∗

c (j) + µs)

2



]

 .

Let T (t) be the number of pulls on s∗
c (j) at line 22 in Alg. 1
at that time step t. By Hoeffding’s inequality and straight-

Identify the Nash Equilibrium in Static Games with Random Payoffs

forward computations, we have

Then, the following inequality holds:

E|Set3(s)| ≤E

(cid:104) (cid:88)

1[T (tγ) ≤

γ∈Set1(s)

γ
2

]

γ
2

]

+1[¯µs∗

c (j)(tγ) ≤

(cid:104) (cid:88)

≤E

2
1[T (tγ) ≤

(µs∗

c (j) + µs)

; T (tγ) ≥

(cid:105)
]

γ
2

γ∈Set1(s)

+ exp{−γ(





≤E

(cid:88)

γ∈Set1(s)

µs∗

c (j) − µs
2

(cid:105)
)2}

1[T (tγ) ≤

] +

γ
2

4
c (j) − µs)2

(µs∗

Note that Line 22 is the UCB1 policy proposed by Auer
et al. (2002). So by Theorem 1 in Auer et al. (2002) (A
slightly modiﬁcation on this theorem, see Appendix C),
we have E[γ(cid:48) − T (tγ(cid:48))] ≤ O(Hc(j) log γ(cid:48)). Then with
Markov inequality, we can get P [γ(cid:48) − T (tγ(cid:48)) ≥ γ(cid:48)/2] ≤
O(Hc(j) log γ(cid:48))
, that is, P [T (tγ(cid:48)) ≤ γ(cid:48)/2] ≤ O(Hc(j) log γ(cid:48))
.
γ(cid:48)

γ(cid:48)

So let Set3 = ∪s∈Cj \s∗

c (j)Set3(s), we have:

E[|Set3|] ≤O(Hc(j)) + E

(cid:88)

1[T (tγ) ≤













γ∈Set1

(cid:88)

γ∈Set1

|Set1|
(cid:88)

γ=1



]


γ
2





O(Hc(j) log γ)
γ

O(Hc(j)logγ)
γ





≤O(Hc(j)) + E

=O(Hc(j)) + E

E|Sw| ≤O

(cid:16)

(cid:88)
(

i

Λ(Hr(i)) +

Λ(Hc(j)))

(cid:88)

j

(cid:17)
+(m + n)(Λ(Hr(s∗[1])) + Λ(Hc(s∗[2])))

.

Proof. We put the proof in Appendix D.

The bound on part1 is based on the result of LUCB, as in
Lemma 6, which has almost the same result on the sample
complexity as policy LUCB.



 .

Lemma 6. Without loss of generality, considering column
j, suppose ¯s∗
c (j) is identiﬁed by Alg. 1 after being selected
for γc(j) rounds, then

(cid:18)

E[γc(j)] = O

Hc(j) log(

(cid:19)

.

Hc(j)
δ

)

Proof. The proof is the same as that of Theorem 6 in
(Kalyanakrishnan et al., 2012), except slightly changes on
description and constants. See Appendix E.

With the above results, we are ready to get our major result
on the sample complexity of LUCB-G, as in Theorem 4.
Theorem 4. When N E(M) (cid:54)= none, the sample complex-
ity of LUCB-G is:

(cid:18)

O

Hr(s∗[1]) log

Hr(s∗[1])
δ

+ Hc(s∗[2]) log

Hc(s∗[2])
δ

≤O(Hc(j)) + E (cid:2)O(Hc(j)(log |Set1|)2)(cid:3)
≤O(Hc(j) + Hc(j)(log E|Set1|)2)

+ E[|Sw|]

(cid:19)
,

where Set1 = ∪s∈Cj \s∗
c (j)Set1(s). The third inequality is
by simple integration, and the last inequality holds because
f (x) = (log x)2 is a concave function for x > e. Note that
φ = |Set1|. With E|Set1(s)| ≤ E(|Set2(s)| + |Set3(s)|),
we complete the proof.

It is noteworthy that although we do not have an analyti-
cal solution of E[φ] from InEq. (3), it is obvious that the
solution is bounded, that is, it will not diverge as δ → 0.
Then, we can get the sample complexity on part 2 and 3, as
in Lemma 5.
Lemma 5. Suppose s∗ = N E(M) (cid:54)= none, let Sw =
{Rounds γ such that Rs∗[1] or Cs∗[2] is not chosen by Alg. 1}.
Let Λ(a) be the maximum value among all solutions that
satisfy the following inequality (constants c1, c2, c3 are the
same as in Lemma 4):

x − c1a(log x)2 − c2a − c3 ≤ 0.

where E[|Sw|]
N E(M) = none, the sample complexity of LUCB-G is:

is bounded as in Lemma 5. When



O



(cid:88)

i

Hr(i) log

Hc(j) log

Hr(i)
δ

(cid:88)

+

j



 .

Hc(j)
δ

Proof. By Lemma 5 and Lemma 6, with straight-forward
computations, we get the complexity.

Note that the sample complexity of LUCB-G is optimal
within a constant gap if N E(M) (cid:54)= none. This is because
that E[|sw|] is bounded and for some family P, P1, P2 ∈ P,
the KL-divergence KL(P1, P2) has the same order as the
squared mean-difference (EP1 − EP2)2 (e.g., normal dis-
tributions with unit variances). Therefore, we can replace
the KL-divergence terms in Lemma 1 by the corresponding
squared mean-difference terms.

Identify the Nash Equilibrium in Static Games with Random Payoffs

Algorithm 2 Racing
1: Input: distribution matrix M, conﬁdence δ
2: γ = 1.
3: while Not stop do
4:
5:

Pull all arms except those have been eliminated.
For an active arm s, if ∃s(cid:48) ∈ row(s) : 2β1(γ) <
¯µs − ¯µs(cid:48) and ∃s(cid:48) ∈ col(s) : ¯µs(cid:48) − ¯µs > 2β1(γ), then
we eliminate s.
For all sequences of arms S = {s1, s2, · · · , s2k}, if
S satisﬁes ∀i ∈ N:
•

s2i+1 ∈ row(s2i), s2i+2 ∈ col(s2i+1).
all arms are eliminated in row(s2i), col(s2i+1)
except s ∈ S.

•

6:

•

•

¯µs2i+1 − ¯µs2i > 2β1(γ).
¯µs2i+1 − ¯µs2i+2 > 2β1(γ),

where sj := s(j−1)%(2k)+1, eliminate all arms in S.
γ = γ + 1.

7:
8: end while

4.2. A racing algorithm
Finally, we present another algorithm, along the line of rac-
ing algorithms for BAI (Even-Dar et al., 2006; Maron &
Moore, 1997). A racing algorithm maintains a set of active
arms, and during each round it samples all the active arms
and then eliminates some arms according to certain rules.

However, we cannot eliminate an arm when the algorithm
“knows” it cannot be the NE immediately. Consider a 2 × 2
game. Suppose an algorithm determines (1, 1) (cid:54)= ¯sr(1),
and eliminates it immediately. Then we cannot determine
whether (2, 1) is NE or not. Therefore, our racing al-
gorithm eliminates arm s only if Alg. 2 determines that
s /∈ {¯s∗
c (s[2]). Let
. Our racing algorithm is shown

β1(γ) =
in Alg. 2, whose stopping and recommendation rules are:

r(s[1], ¯s∗
(cid:113) log(cmnγ2/δ)
2γ

c (s[2]))} or ¯s∗

r(s[1]) (cid:54)= ¯s∗

• If only an arm is not eliminated after round γ, then

recommend this arm as NE.

• If all arms are eliminated after round γ, then the algo-

rithm determines N E = none.

As shown in Theorem 5, this algorithm is δ-PAC and it will
terminate in ﬁnite time step with probability at least 1 − δ.

Theorem 5. Alg. 2 is δ-PAC and will terminate in ﬁnite
time with probability at least 1 − δ.
Proof. We put the proof in Appendix F.

5. Experiments
We now empirically verify the sample complexity of our
algorithms. We choose a simple algorithm as our baseline
(denoted by ALL), which pulls all arms at each round un-
til stopping. The stopping and recommendation rules are

Pulls

5

4

ALL
Racing

LUCB-G

Pulls

5

4

3

ALL

Racing

3
−1 −3 −5 −7

δ

2
−1 −3 −5 −7

LUCB-G
δ

(a) ﬁrst-game

(b) ﬁrst-game

Pulls

4.5

5

4

ALL

Racing
LUCB-G

−1 −3 −5 −7

(c) second-game

δ

Figure 1. The results on two simulated games.

the same as LUCB-G, and the conﬁdence bound for this
baseline is slightly different, see Appendix G for details.

We evaluate on synthetic 5×5 games, where the payoffs are
all random Bernoulli variables. The ﬁrst game has a NE,
while the second game has no NE. The results are shown
in Fig. 1, where both axes are in log-scale with base 10.
The number of pulls needed for both games are shown in
Fig. 1(a) and Fig. 1(c) separately and our algorithms out-
perform the baseline (i.e., ALL). Fig. 1(b) shows the num-
ber of pulls on s /∈ nei(s∗) in the ﬁrst game and we can see
that this number is bounded, agreeing with our analysis.

6. Conclusions and Discussions

We analyze the two-player zero-sum static game with ran-
dom payoffs via efﬁcient sampling and give a lower bound
of the sample complexity in the case that the Nash Equilib-
rium (NE) exists. We then present two δ-PAC algorithms
to identify the NE. They follow two lines of algorithms for
the best arm identiﬁcation problem in the ﬁxed conﬁdence
setting. The sample complexity of the ﬁrst algorithm is op-
timal within a constant gap if NE exists.

As we cannot give an explicit form for the expectation
number of pulls wasting on arms in neither the row nor the
column of the NE, our lower bound can be loose to some
extent. It is worth of having a further study for tighter lower
bounds. Moreover, an analysis of the sample complexity in
the case that NE does not exist is still an open problem, and
we expect better work on it in the future.

Identify the Nash Equilibrium in Static Games with Random Payoffs

Acknowledgements

This work is supported by the National Basic Research
(973) Program of China (No. 2013CB329403), NSFC
Projects (Nos. 61620106010 and 61621136008), and the
Youth Top-notch Talent Support Program.

References

Audibert, Jean-Yves and Bubeck, S´ebastien. Best arm
In COLT-23th

identiﬁcation in multi-armed bandits.
Conference on Learning Theory-2010, pp. 13–p, 2010.

Auer, Peter, Cesa-Bianchi, Nicolo, and Fischer, Paul.
Finite-time analysis of the multiarmed bandit problem.
Machine learning, 47(2-3):235–256, 2002.

Babichenko, Yakov. Query complexity of approximate
nash equilibria. Journal of the ACM (JACM), 63(4):36,
2016.

Chen, Xi, Deng, Xiaotie, and Teng, Shang-Hua. Settling
the complexity of computing two-player nash equilibria.
Journal of the ACM (JACM), 56(3):14, 2009.

Daskalakis, Constantinos, Goldberg, Paul W, and Papadim-
itriou, Christos H. The complexity of computing a nash
equilibrium. SIAM Journal on Computing, 39(1):195–
259, 2009.

Even-Dar, Eyal, Mannor, Shie, and Mansour, Yishay.
Action elimination and stopping conditions for the
multi-armed bandit and reinforcement learning prob-
Journal of machine learning research, 7(Jun):
lems.
1079–1105, 2006.

Fearnley, John and Savani, Rahul. Finding approximate
nash equilibria of bimatrix games via payoff queries.
ACM Transactions on Economics and Computation
(TEAC), 4(4):25, 2016.

Fearnley, John, Gairing, Martin, Goldberg, Paul W, and
Savani, Rahul. Learning equilibria of games via pay-
off queries. Journal of Machine Learning Research, 16:
1305–1344, 2015.

Garivier, Aur´elien, Kaufmann, Emilie, and Koolen,
Wouter M. Maximin action identiﬁcation: A new ban-
dit framework for games. In 29th Annual Conference on
Learning Theory, pp. 1028–1050, 2016.

Goldberg, Paul W and Roth, Aaron. Bounds for the query
complexity of approximate equilibria. ACM Transac-
tions on Economics and Computation (TEAC), 4(4):24,
2016.

Goldberg, Paul W and Turchetta, Stefano. Query complex-
ity of approximate equilibria in anonymous games. In In-
ternational Conference on Web and Internet Economics,
pp. 357–369. Springer, 2015.

Hart, Sergiu and Nisan, Noam. The query complexity of
correlated equilibria. Games and Economic Behavior,
2016.

Jamieson, Kevin and Nowak, Robert. Best-arm identiﬁ-
cation algorithms for multi-armed bandits in the ﬁxed
conﬁdence setting. In Information Sciences and Systems
(CISS), 2014 48th Annual Conference on, pp. 1–6. IEEE,
2014.

Jordan, Patrick R, Vorobeychik, Yevgeniy, and Wellman,
Michael P. Searching for approximate equilibria in em-
In Proceedings of the 7th international
pirical games.
joint conference on Autonomous agents and multiagent
systems-Volume 2, pp. 1063–1070. International Foun-
dation for Autonomous Agents and Multiagent Systems,
2008.

Kalyanakrishnan, Shivaram, Tewari, Ambuj, Auer, Peter,
and Stone, Peter. Pac subset selection in stochastic multi-
armed bandits. In Proceedings of the 29th International
Conference on Machine Learning (ICML-12), pp. 655–
662, 2012.

Kaufmann, Emilie, Capp´e, Olivier, and Garivier, Aur´elien.
On the complexity of best arm identiﬁcation in multi-
armed bandit models. The Journal of Machine Learning
Research, 2015.

Lai, Tze Leung and Robbins, Herbert. Asymptotically ef-
ﬁcient adaptive allocation rules. Advances in applied
mathematics, 6(1):4–22, 1985.

Mannor, Shie and Tsitsiklis, John N. The sample com-
plexity of exploration in the multi-armed bandit prob-
lem. Journal of Machine Learning Research, 5(Jun):
623–648, 2004.

Maron, Oded and Moore, Andrew W. The racing algo-
rithm: Model selection for lazy learners. In Lazy learn-
ing, pp. 193–225. Springer, 1997.

Osborne, Martin J and Rubinstein, Ariel. A course in game

theory. MIT press, 1994.

Tsybakov, Alexandre B. Introduction to nonparametric es-

timation. Springer, 2009.

Vorobeychik, Yevgeniy, Wellman, Michael P, and Singh,
Satinder. Learning payoff functions in inﬁnite games.
Machine Learning, 67(1-2):145–168, 2007.

Identify the Nash Equilibrium in Static Games with Random Payoffs

Walsh, William E, Parkes, David C, and Das, Rajarshi.
Choosing samples to compute heuristic-strategy nash
In International Workshop on Agent-
equilibrium.
Mediated Electronic Commerce, pp. 109–123. Springer,
2003.

Wellman, Michael P. Methods for empirical game-theoretic
analysis. In Proceedings of the National Conference on
Artiﬁcial Intelligence, volume 21, pp. 1552. Menlo Park,
CA; Cambridge, MA; London; AAAI Press; MIT Press;
1999, 2006.

