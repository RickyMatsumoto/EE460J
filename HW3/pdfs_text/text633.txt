Tensor Decomposition via Simultaneous Power Iteration

Po-An Wang 1 Chi-Jen Lu 1

Abstract

Tensor decomposition is an important problem
with many applications across several disci-
plines, and a popular approach for this problem
is the tensor power method. However, previous
works with theoretical guarantee based on this
approach can only ﬁnd the top eigenvectors one
after one, unlike the case for matrices.
In this
paper, we show how to ﬁnd the eigenvectors si-
multaneously with the help of a new initialization
procedure. This allows us to achieve a better run-
ning time in the batch setting, as well as a lower
sample complexity in the streaming setting.

1. Introduction

Tensors have long been successfully used in several dis-
ciplines, including neuroscience, phylogenetics, statistics,
signal processing, computer vision, and data mining. They
are used to model multi-relational or multi-modal data, and
their decompositions often reveal some underlying struc-
tures behind the observed data. See (Kolda & Bader, 2009)
for a survey of such results. Recently, they have found
applications in machine learning, particularly for learning
various latent variable models (Anandkumar et al., 2012;
Chaganty & Liang, 2014; Anandkumar et al., 2014a).

One popular decomposition method in such applications
is the CP (Candecomp/Parafac) decomposition, which de-
composes the given tensor as a sum of rank-one compo-
nents. This is similar to the singular value decomposition
(SVD) of matrices, and a popular approach for SVD is the
power method, which is well-understood and has nice theo-
retical guarantee. As tensors can be seen as generalization
of matrices to higher orders, one would hope that a nat-
ural generalization of the power method to tensors could
inherit the success from the matrix case. However, the sit-
uation turns out to be much more complicated for tensors
the discussion in (Anandkumar et al., 2014a)),
(see e.g.

1Academia Sinica, Taiwan. Correspondence to: Po-An Wang

<poanwang@iis.sinica.edu.tw>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

and in fact several problems related to tensor decomposi-
tion are known to be NP-hard (Hillar & Lim, 2013). Nev-
ertheless, when the given tensor has some additional struc-
ture, the tensor decomposition problem becomes tractable
again. In particular, for tensors having orthogonal decom-
position, Anandkumar et al. (2014a) provided an efﬁcient
algorithm based on the tensor power method with theoreti-
cal guarantee. Still, as we will discuss later in Section 2, the
seemingly subtle change of going from matrices to tensors
makes some signiﬁcant differences for the power method.

The ﬁrst is that while the matrix power method can guaran-
tee that a randomly selected initial vector will almost surely
converge to the top singular vector, we have much less con-
trol of where the convergence goes in the tensor case. Con-
sequently, most previous works based on the tensor power
method with theoretical guarantee, such as (Anandkumar
et al., 2014a;b; Wang & Anandkumar, 2016), require much
more complicated procedures. In particular, they can only
ﬁnd the top k eigenvectors one by one, each time with the
power method applied to a modiﬁed tensor, deﬂated from
the original tensor according to the previously found vec-
tors. Moreover, to ﬁnd each vector, they need to sample
several initial vectors and apply the power method on all
of them, before selecting just one from them. In contrast,
algorithms for matrices such as (Mitliagkas et al., 2013;
Hardt & Price, 2014) are much simpler, as they can ﬁnd
the k vectors simultaneously by applying the power method
only on k random initial vectors. The second difference, on
the other hand, has a beneﬁcial effect, which allows the ten-
sor power method to converge exponentially faster than the
matrix one when starting from good initial vectors. Then a
natural question is: can we inherit the best of both worlds?
Namely, is it possible to have a simple algorithm which can
ﬁnd the k eigenvectors of a tensor simultaneously and con-
verge faster than that for matrices?

Our Results. As in previous works, we consider the
slightly harder scenario in which we only have access to
a noisy version of the tensor we want to decompose. This
arises in applications such as learning latent variable mod-
els, in which the tensor we have access to is obtained from
some empirical average of the observed data. Our main
contribution is to answer the above question afﬁrmatively.

First, we consider the batch setting in which we assume

Tensor Decomposition via Simultaneous Power Iteration

that the given noisy tensor is stored somewhere and can be
accessed whenever we want to. In this setting, we iden-
tify a sufﬁcient condition such that if we have k initial vec-
tors satisfying this condition, then we can apply the tensor
power method on them simultaneously, which will come
within some distance ε to the eigenvectors in O(log log 1
ε )
iterations, with parameters related to eigenvalues consid-
ered as constant. To apply such a result, we need an ef-
ﬁcient way to ﬁnd such initial vectors. We show how to
do this by choosing a good direction to project the tensor
down to a matrix while preserving the eigengaps, and then
applying the matrix power method for only a few iterations
just to obtain vectors meeting that sufﬁcient condition. The
number of iterations needed here is only O(log d), inde-
pendent of ε, where d is the dimension of the eigenvectors.

The result stated above is for orthogonal tensors. On the
other hand, it is known that an nonorthogonal tensor with
linearly independent eigenvectors can be converted into an
orthogonal one with the help of some whitening matrix.
However, previous works usually pay little attention on
how to ﬁnd such a whitening matrix efﬁciently. Accord-
ing to (Anandkumar et al., 2014a), one way is via SVD on
some second moment matrix, but doing this using the ma-
trix power method would take longer to converge compared
to the tensor power method which would then be applied on
the whitened tensor. Our second contribution is to provide
an efﬁcient way to ﬁnd a whitening matrix, by simply ap-
plying only one iteration of the matrix power method.

While most previous works on tensor decomposition focus
on the batch setting, storing even a tensor of order three re-
quires Ω(d3) space, which is infeasible for a large d. We
show to avoid this in the streaming setting, with a stream
of data arriving one at a time, which is the only source of
information about the tensor. We provide a streaming algo-
rithm using only O(kd) space, which is the smallest pos-
sible, just enough to store the k eigenvectors of dimension
d. To achieve an approximation error ε, the total number of
samples we need is O(kd log d + 1

ε2 log(d log 1

ε )).

Related Works There is a huge literature on tensor de-
composition, and it is beyond the scope of this paper to
give a comprehensive survey. Thus, we only compare our
results to the most related ones, particularly those based
on the power method. While different works may focus
on different aspects, we are most interested in understand-
ing how the error parameter ε affects various performance
measures, having in mind a small ε.

First, the batch algorithm of Anandkumar et al. (2014a),
using a better analysis in (Wang & Anandkumar, 2016),
runs in time about O((k2 log k)(log log 1
ε )), which can be
made to run in O(k(log log 1
ε )) iterations in parallel, while
ours are O(k log log 1
ε ), respectively. On

ε ) and O(log log 1

the other hand, one advantage of their algorithm is that its
running time does not depend on the eigengaps, while ours
has the dependence hidden above as some constant.

In the streaming setting, Wang & Anandkumar (2016)
provided an algorithm using O(dk log k) memory and
O( k
ε2 log( d
ε )) samples, while ours only uses O(dk) mem-
ory and O( 1
ε2 log(d log log 1
ε )) samples.1 Nevertheless, the
sample complexity of Wang & Anandkumar (2016) is also
independent of the eigengaps, while ours has the depen-
dence hidden above as a constant factor.

As one can see, our algorithms, which ﬁnd the k eigenvec-
tors simultaneously, allow us to save a factor of k in the
time complexity and the sample complexity, although our
bounds may become worse when the eigengaps are small.
Thus, our algorithms can be seen as new options for users
to choose from, depending on the data they are given.

Although not directed related, let us also compare to pre-
vious works on SVD. Two related ones, both based on the
simultaneous matrix power method, are the batch algorithm
of (Hardt & Price, 2014) which converges in O(log d
ε ) it-
erations, and the streaming algorithm of (Li et al., 2016)
which requires O( 1
ε )) samples. Both bounds
are worse than ours and also depend on the eigengaps.
Thus, although one approach for orthogonal tensor decom-
position is to reduce it to a matrix SVD problem, this does
not appear to result in better performance than ours.

ε2 log(d log 1

Finally, comparisons of the tensor power method with other
approaches can be found in works such as (Anandkumar
et al., 2014a; Wang & Anandkumar, 2016). For example,
the online SGD approach of (Ge et al., 2015) works only
for tensors of even orders and its sample complexity has a
poor dependency on the dimension d.

Organization of the paper. First, we provide some pre-
liminaries in Section 2. Then we present our batch algo-
rithm for orthogonal and symmetric tensors of order three
in Section 3, and then for general orthogonal tensors in Sec-
tion 4. In Section 5, we introduce our whitening procedure
for nonorthogonal but symmetry tensors. Finally, in Sec-
tion 6, we present our algorithm for the streaming setting.
Due to the space limitation, we will move all our proofs to
the appendix in the supplementary material.

2. Preliminaries

Let us ﬁrst introduce some notations and deﬁnitions which
we will use later. Let R denote the set of real numbers
and N the set of positive integers. Let N (0, 1) denote the
standard normal distribution with mean 0 and variance 1,

1We use a different input distribution from theirs. The bound

listed here is modiﬁed from theirs according to our distribution.

Tensor Decomposition via Simultaneous Power Iteration

(cid:107)A·x(cid:107)

and let N d(0, 1), for d ∈ N, denote the d-variate one which
has each of its d dimensions sampled independently from
N (0, 1). For d ∈ N, let [d] denote the set {1, . . . , d}. For
a vector x, let (cid:107)x(cid:107) denote its L2 norm. For d ∈ N, let Id
denote the d × d identity matrix. For a matrix A ∈ Rd×k,
let Ai, for i ∈ [k], denote its i-th column, and let Ai,j, for
j ∈ [d], be the j-th entry of Ai. Moreover, for a matrix A,
let A(cid:62) denote its transpose, and deﬁne its norm as (cid:107)A(cid:107) =
(cid:107)x(cid:107) , using the convention that 0
maxx∈Rk
Tensors are the focus of our paper, which can be seen as
generalization of matrices to higher orders. For simplic-
ity of presentation, we will use symmetric tensors of order
three as examples in the following deﬁnitions. A real ten-
sor T of order three can be seen as an three-dimensional
array in Rd×d×d, for some d ∈ N, with its (i, j, k)-th
entry denoted as Ti,j,k. For such a tensor T and three
matrices A ∈ Rd×m1, B ∈ Rd×m2 , C ∈ Rd×m3 , let
T (A, B, C) be the tensor in Rm1×m2×m3 , with its (a, b, c)-
th entry deﬁned as (cid:80)
i,j,k∈[d] Ti,j,kAa,iBb,jCc,k. The norm
of a tensor T we will use is the operator norm: (cid:107)T (cid:107) =
maxx,y,z∈Rd

0 = 0.

|T (x,y,z)|
(cid:107)x(cid:107)(cid:107)y(cid:107)(cid:107)z(cid:107) .

method, which works as follows. Suppose we are given
a d × d matrix M = (cid:80)
i∈[d] λi · ui ⊗ ui, with nonnega-
tive λ1 > λ2 ≥ · · · and orthonormal vectors u1, . . . , ud.
The power method starts with some q(0) = (cid:80)
i∈[d] ci · ui,
usually chosen randomly, and then repeatedly performs the
update q(t) = M · q(t−1), which results in

q(t) =

(cid:88)

(cid:16)

i q(t−1)(cid:17)
u(cid:62)

λi

· ui =

(cid:0)λt

ici

(cid:1) · ui.

(cid:88)

i∈[d]

i∈[d]

Note that for any i (cid:54)= 1, as λi < λ1, the coefﬁcient λt
ici will
soon become much smaller than the coefﬁcient λt
1c1 if c1
is not too small, which is likely to happen for a randomly
chosen q(0). This has the effect that after normalization,
q(t)/(cid:107)q(t)(cid:107) approaches u1 quickly.
Now consider a tensor T = (cid:80)
i∈[d] λi · ui ⊗ ui ⊗ ui,
with nonnegative λ1 > λ2 ≥ · · · and orthonormal
vectors u1, . . . , ud.
The tensor version of the power
method again starts from a randomly chosen q(0) =
(cid:80)
i∈[d] ci · ui, but now repeatedly performs the update

q(t) = T (Id, q(t−1), q(t−1)), which in turn results in

The tensor decomposition problem.
there is a tensor T with some unknown decomposition

In this problem,

q(t) =

(cid:88)

(cid:16)

i q(t−1)(cid:17)2
u(cid:62)

λi

· ui =

λ−1
i

(λici)2t

· ui.

(cid:88)

i∈[d]

i∈[d]

T =

λi · ui ⊗ ui ⊗ ui,

(cid:88)

i∈[d]

with λi ≥ 0 and ui ∈ Rd for any i ∈ [d]. Then given some
k ∈ [d] and ε ∈ (0, 1), our goal is to ﬁnd ˆλi and ˆui with

|ˆλi − λi| ≤ ε and (cid:107)ˆui − ui(cid:107) ≤ ε, for every i ∈ [k].

We will assume that

(cid:88)

i∈[d]

λi ≤ 1 and ∀i ∈ [k] : λi > λi+1.

(1)

As in previous works, we consider a slightly harder ver-
sion of the problem, in which we only have access to some
noisy version of T , instead of the noiseless T . We will
consider the following two settings. In the batch setting,
we have access to some ¯T = T + Φ for the whole time,
for some perturbation tensor Φ. In the streaming setting,
we have a stream of data points x1, x2, . . . arriving one by
one, which provide the only information we have about T ,
with each xτ ∈ Rd allowing us to compute some ¯Tτ with
mean E[ ¯T ] = T . In this streaming setting, we are particu-
larly interested in the case of a large d which prohibits one
to store a tensor of size d3 in memory.

Power Method: Matrices versus Tensors. Note that a
tensor of order two is just a matrix, and a popular ap-
proach for decomposing matrices is the so-called power

The coefﬁcient of each ui now has a different form from
the matrix case, and this leads to the following two effects.

First, one now has much less control on what q(t)/(cid:107)q(t)(cid:107)
converges to. In fact, it can converge to any ui (cid:54)= u1 if
ui has the largest value of λi|ci|, which happens with a
good probability if λi is not much smaller than λ1. Con-
sequently, to ﬁnd the top k vectors u1, . . . , uk, previous
works based on the power method all need much more
complicated procedures (Anandkumar et al., 2014a), com-
pared to those for matrices, as discussed in the introduction.

On the other hand, the different form of q(t) has the beneﬁ-
cial effect that the convergence is now exponentially faster
than in the matrix case. More precisely, if λi|ci| < λj|cj|,
than the gap between the coefﬁcients (λici)2t
and (λjcj)2t
is now ampliﬁed much faster. We will show how to inherit
this nice property of faster convergence but at the same time
avoid the difﬁculty discussed above.

3. Orthogonal and Symmetric Tensors of

Order Three

In this section, we focus on the special case in which the
tensors to be decomposed are orthogonal, symmetric, and
of order three. Formally, there is an underlying tensor

T =

λi · ui ⊗ ui ⊗ ui,

(cid:88)

i∈[d]

Tensor Decomposition via Simultaneous Power Iteration

Algorithm 1 Robust tensor power method

Input: Tensor ¯T ∈ Rd×d×d and parameters k, L, S, N .
Initialization Phase:
Sample w1, . . . , wL, Y (0)
Compute ¯w = 1
j∈[L]
L
Compute ¯M = ¯T (Id, Id, ¯w).
Factorize Y (0) as Z (0) · R(0) by QR decomposition.
for s = 1 to S do

, . . . , Y (0)
¯T (Id, wj, wj).

k ∼ N d(0, 1).

(cid:80)

1

Compute Y (s) = ¯M · Z (s−1).
Factorize Y (s) as Z (s) · R(s) by QR decomposition.

end for
Tensor Power Phase:
Let Q(0) = Z (S).
for t = 1 to N do
Compute Y (t)
), ∀j ∈ [k].
j
Factorize Y (t) as Q(t) · R(t) by QR decomposition.

j = ¯T (Id, Q(t−1)

, Q(t−1)
j

end for
Output: ˆuj = Q(N )

j

and ˆλj = ¯T (ˆuj, ˆuj, ˆuj), ∀j ∈ [k].

with orthonormal vectors ui’s and real λi’s satisfying the
condition (1). Then given k ∈ [d] and ε ∈ (0, 1), our goal
is to ﬁnd approximates to those λi and ui within distance ε,
but we only have access to some noisy tensor ¯T = T + Φ
for some symmetric perturbation tensor Φ.

Our algorithm is given in Algorithm 1, which consists of
two phases: the initialization phase and the tensor power
phase. The main phase is the tensor power phase, which
we will discuss in detail in Subsection 3.1. For our ten-
sor power phase to work, it needs to have a good starting
point. This is provided by the initialization phase, which
we will discuss in detail in Subsection 3.2. Through these
two subsections, we will prove Theorem 1 below, which
summarizes the performance of our algorithm, according
to the following parameters of the tensor:

γ = min
i∈[k]

i − λ2
λ2
λ2
i

i+1

and ∆ = min
i∈[k]

λi − λi+1
4

.

Theorem 1. Suppose ε ≤ λk
2 and the perturbation tensor
3d , α0∆2
has the bound (cid:107)Φ(cid:107) ≤ min{ ∆ε
, ∆
} for a small
√
k
enough constant α0. Then for some L = O( 1
γ2 log d), S =
O( 1
ε )), our Algorithm 1
with high probability will output ˆui and ˆλi with (cid:107)ˆui−ui(cid:107) ≤
ε and |ˆλi − λi| ≤ ε for every i ∈ [k].

γ log d), and N = O(log( 1

γ log 1

dk

√

2

Let us make some remarks about the theorem. First, the L
samples are used to compute ¯w and ¯M , which can be done
in a parallel way. Second, our parameter γ is related to a
λi−λi+1
parameter γ(cid:48) = mini∈[k]
used in (Hardt & Price,
λi
2014), and it is easy to verify that γ ≥ γ(cid:48). Thus, our algo-
γ log 1
rithm for tensors converges in O( 1
ε ))
γ(cid:48) log 1
rounds, which is faster than the O( 1
ε )

γ log d + log( 1
γ(cid:48) log d + 1

rounds of (Hardt & Price, 2014) for matrices. Note that
our dependence on the error parameter ε is exponentially
smaller than that of (Hardt & Price, 2014), which means
that for a small ε, we can decompose tensors much faster
than matrices. Finally, compared to previous works on
tensors, our convergence time, for a small ε is about
O(log log 1
ε ) while those in (Anandkumar et al., 2014a;
Wang & Anandkumar, 2016) are at least Ω(k log log 1

ε ).

3.1. Our Robust Tensor Power Method

1 , . . . , Q(t)

The tensor power phase of our Algorithm 1 is based on
our version of the tensor power method, which works as
follows. At each step t, we maintain a d × k matrix Q(t)
with columns Q(t)
k as our current estimators for
u1, . . . , uk, which is obtained by updating the previous es-
timators with the following two operations.
The main operation is to apply the noisy tensor ¯T on them
simultaneously to get a d × k matrix Y (t) with its j-th col-
umn computed as Y (t)
), which
equals

j = ¯T (Id, Q(t−1)

, Q(t−1)
j

j

(cid:88)

(cid:16)

λi

i∈[d]

(cid:17)2

i Q(t−1)
u(cid:62)

j

· ui + ˆΦ(t)
j ,

for ˆΦ(t)

j = Φ(Id, Q(t−1)

j

, Q(t−1)
j

). This implies that

∀i ∈ [d] : u(cid:62)

i Y (t)

j = λi

i Q(t−1)
u(cid:62)

j

+ u(cid:62)
i

ˆΦ(t)
j ,

(2)

(cid:16)

(cid:17)2

which shows the progress made by this operation.

The second operation is to orthogonalize Y (t) as

Y (t) = Q(t) · R(t),

by the QR decomposition via the Gram-Schmidt process,
1 , . . . , Q(t)
to obtain a d × k matrix Q(t) with columns Q(t)
k ,
which then become our new estimators. As we will show
in Lemma 1 below, given a small enough (cid:107)Φ(cid:107), if we start
with a full-rank Q(0), then each Q(t) also has full rank and
consists of orthonormal columns, and each R(t) is invert-
ible. Moreover, although we apply the QR decomposition
on the whole matrix Y (t) to obtain the matrix Q(t), it has
the effect that for any m ∈ [k], the ﬁrst m columns of Q(t)
can be seen as obtained from the ﬁrst m columns of Y (t)
by a QR decomposition. This property is needed in our
Lemma 1 and Theorem 2 below to guarantee the simulta-
neous convergence of Q(t)
i

to ui for every i ∈ [k].

Before stating Lemma 1 which guarantees the progress we
make at each step, let us prepare some notations ﬁrst. For
a d × k matrix A and some m ∈ [k], let A[m] denote the
d × m matrix containing the ﬁrst m columns of A. Let U
denote the d × k matrix with the target vector ui as its i’th
column. For a d × k matrix Q and some m ∈ [k], deﬁne

cosm(Q) = min
y∈Rm

(cid:13)
(cid:13)U (cid:62)
(cid:13)

[m] · Q[m] · y

(cid:13)
(cid:13)
(cid:13) /(cid:107)Q[m] · y(cid:107),

Tensor Decomposition via Simultaneous Power Iteration

which equals the cosine of the m’th principal angle be-
tween the column spaces of U[m] and Q[m], let sinm(Q) =
(cid:112)1 − cos2

m(Q), and let us use as the error measure

tanm(Q) = sinm(Q)/ cosm(Q).

More information about the principal angles can be found
in, e.g., (Golub & Van Loan, 1996). Then we have the
following lemma, which we prove in Appendix B.1.
Lemma 1. Fix any m ∈ [k] and t ≥ 0. Let ˆΦ(t)
the d × m matrix with ˆΦ(t)
, Q(t−1)
j
j’th column, and suppose

j = Φ(Id, Q(t−1)

[m] denote
) as its

j

(cid:13)
ˆΦ(t)
(cid:13)
(cid:13)
[m]

(cid:13)
(cid:13)
(cid:13) < ∆ · min

(cid:110)

β, cos2

(cid:111)
m(Q(t−1))

,

(3)

for some β > 0. Then for ρ = maxi∈[k]( λi+1
λi

) 1
4 , we have

tanm(Q(t)) ≤ max

β, max{β, ρ} · tan2

(cid:110)

(cid:111)
m(Q(t−1))

.

Observe that the guarantee provided by the lemma above
has a similar form as that in (Hardt & Price, 2014) for
matrices. The main difference is that here in the tensor
case, we have the error measure essentially squared af-
ter each step, which has the following two implications.
First, to guarantee that the error is indeed reduced, we need
tanm(Q(t−1)) to be small enough (say, less than one), un-
like in the matrix case. Next, if we indeed have a small
enough tanm(Q(t−1)), then the error can be reduced in a
much faster rate than in the matrix case. Another differ-
ence is that here we provide the guarantee for all the k sub-
matrices Q(t)
[m], for m ∈ [k], instead of just one matrix Q(t).
This allows us to show the simultaneous convergence of
each column Q(t)
to the target vector ui for every i ∈ [k],
i
as given in the following, which we prove in Appendix B.2.
Theorem 2. For any ε ∈ (0, λk
there exists some
N ≤ O(log( 1
γ log 1
ε )) such that the following holds. Sup-
pose the perturbation is bounded by (cid:107)Φ(cid:107) ≤ ∆ε
and we
√
k
start from some initial Q(0) with tanm Q(0) ≤ 1 for ev-
ery m ∈ [k]. Then for any t ≥ N , with ˆui = Q(t)
i
and ˆλi = ¯T (ˆui, ˆui, ˆui), we have (cid:107)ui − ˆui(cid:107) ≤ ε and
|λi − ˆλi| ≤ ε, for every i ∈ [k].

2 ),

2

Note that the convergence rate guaranteed by the theorem
above is exponentially faster than that in (Hardt & Price,
2014) for matrices, assuming that we indeed can have such
a good initial Q(0) to start with. In the next subsection, we
show how it can be found efﬁciently.

3.2. Initialization Procedure

Our approach for ﬁnding a good initialization is to project
the tensor down to a matrix and apply the matrix power

method for only a few steps just to make the tangents less
than one. Although we could continue applying the matrix
power method till reaching the much smaller target bound
ε, this would take exponentially longer than by switching
to the tensor power method as we actually do.

i∈[d] λi(u(cid:62)

As mentioned above, we would ﬁrst like to project the ten-
sor ¯T down to a matrix. A naive approach is to sample
a random vector ¯w and take the matrix ¯T (Id, Id, ¯w) ≈
T (Id, Id, ¯w) = (cid:80)
i ¯w) · ui ⊗ ui. However, this
may mess up the gaps between eigenvalues, which are
needed to guarantee the convergence rate of the matrix
power method. The reason is that as each u(cid:62)
i ¯w has mean
zero, the coefﬁcient λi(u(cid:62)
i ¯w) also has mean zero and thus
has a good chance of coming very close to others. To pre-
serve the gaps, we would like to have u(cid:62)
i+1 ¯w for
each i with high probability. To achieve this, let us ﬁrst
imagine sampling a random w ∈ Rd from N d(0, 1), and
computing the vector ¯w = ¯T (Id, w, w), which is close to
(cid:88)

i ¯w ≥ u(cid:62)

T (Id, w, w) =

λi(u(cid:62)

i w)2 · ui.

i∈[d]

Then one can show that for every i, E[(u(cid:62)
E[ ¯w] ≈ E[T (Id, w, w)] = (cid:80)

i∈[d] λiui, and

i w)2] = 1, so that

i E[ ¯w] ≈ λi > λi+1 ≈ u(cid:62)
u(cid:62)

i+1 E[ ¯w].

However, we want the gap-preserving guarantee to be in
high probability, instead in expectation. Thus we go further
by sampling not just one, but some number L of vectors
w1, . . . , wL independently from the distribution N d(0, 1),
and then taking the average

¯w =

1
L

(cid:88)

j∈[L]

¯T (Id, wj, wj).

(4)

The following lemma shows that such a ¯w is likely to have
u(cid:62)
i ¯w ≈ λi, which we prove in Appendix B.3.
Lemma 2. Suppose we have ¯T = T + Φ with (cid:107)Φ(cid:107) ≤ ∆
3d .
Then for some L ≤ O( 1
γ2 log d), the vector ¯w computed
according to (4) with high probability satisﬁes

(cid:12)
(cid:12)u(cid:62)

i ¯w − λi

(cid:12)
(cid:12) ≤

1
4

(λiγ + 2∆) for every i ∈ [k].

(5)

With this ¯w, we compute the matrix ¯M = ¯T (Id, Id, ¯w).
As shown by the following lemma, which we prove in Ap-
pendix B.4, ¯M is close to a matrix with ui’s as eigenvectors
and good gaps between eigenvalues.
Lemma 3. Suppose we have ¯T = T + Φ. Then for any ¯w
satisfying the condition (5) in Lemma 2, the matrix ¯M =
¯T (Id, Id, ¯w) can be decomposed as

¯M =

¯λi · ui ⊗ ui + ¯Φ,

(cid:88)

i∈[d]

Tensor Decomposition via Simultaneous Power Iteration

for some ¯λi’s with ¯λi − ¯λi+1 ≥ ∆2, for i ∈ [k], and ¯Φ =
Φ(Id, Id, ¯w) with (cid:107) ¯Φ(cid:107) ≤ 2(cid:107)Φ(cid:107).

With such a matrix ¯M , we next apply the matrix power
method of (Hardt & Price, 2014) to ﬁnd good approxi-
mates to its eigenvectors. More precisely, we sample an
initial matrix Y (0) ∈ Rd×k by choosing each of its col-
umn independently according to the distribution N d(0, 1),
and factorize it as Y (0) = Z (0) · R(0) by QR decomposi-
tion via the Gram-Schmidt process. Then at step s ≥ 1,
we multiply the previous estimate Z (s−1) by ¯M to obtain
Y (s) = ¯M · Z (s−1), factorize it as Y (s) = Z (s) · R(s)
by QR decomposition via the Gram-Schmidt process, and
then take the orthonormal Z (s) as the new estimate. The
following lemma shows the number of steps needed to ﬁnd
a good enough Z (s).
Lemma 4. Suppose we are given a matrix ¯M having the
decomposition described in Lemma 3, with (cid:107) ¯Φ(cid:107) ≤ 2α0∆2
dk
for a small enough constant α0. Then there exists some
S ≤ O( 1
γ log d) such that with high probability, we have
tanm(Z (s)) ≤ 1 for every m ∈ [k] whenever s ≥ S.

√

This together with the previous two lemmas guarantee that
given ¯T = T + Φ, with (cid:107)Φ(cid:107) ≤ min{ ∆
}, for a small
enough constant α0, we can obtain with high probability a
good Z (S) which can be used as the initial Q(0) for our
tensor power phase. Combining this with Theorem 2 in the
previous subsection, we then have our Theorem 1 given at
the beginning of the section.

3d , α0∆2

dk

√

4. General Orthogonal Tensors

In the previous section, we consider tensors which are or-
thogonal, symmetric, and of order three. In this section, we
show how to extend our results for general orthogonal ten-
sors, to deal with higher orders ﬁrst and then asymmetry.

4.1. Higher-Order Tensors

To handle orthogonal and symmetric tensors of any or-
der, only the initialization procedure needs to be modiﬁed.
First, for tensors of any odd order, a straightforward mod-
iﬁcation is as follows. Take for example a tensor of order
2r + 1. Now we simply compute

¯w =

1
L

(cid:88)

j∈[L]

¯T (Id, wj, . . . , wj),

with 2r copies of wj, and similarly to Lemma 2, one can
show that ¯w is likely to be close to the vector (cid:80)
i∈[d] λi · ui.
With such a vector ¯w, one can show that the matrix

¯M = ¯T (Id, Id, ¯w, . . . , ¯w),

as that in the previous section. Note that this approach has
the eigenvalues decreased exponentially in r. A different
approach avoiding this is to compute ¯M directly as

¯M =

1
L

(cid:88)

j∈[L]

(cid:0) ¯T (Id, Id, wj, . . . , wj)(cid:1)2

,

which is close to

(cid:88)

1
L

j∈[L]
1
L

=

(T (Id, Id, wj, . . . , wj))2

(cid:88)

(cid:88)

λ2
i

(cid:0)u(cid:62)

i wj

(cid:1)4r−2

· ui ⊗ ui

=

j∈[L]

(cid:88)

λ2
i

i∈[d]

i∈[d]
1
L

(cid:88)

j∈[L]

(cid:0)u(cid:62)

i wj

(cid:1)4r−2

· ui ⊗ ui.

Then one can again show that such a matrix ¯M is likely to
be close to the matrix (cid:80)
i∈[d] λ2

i · ui ⊗ ui.

To handle tensors of even orders,
the initialization is
slightly different but the idea is similar. Given a tensor of
order 2r, we again sample vectors w1, . . . , wL as before,
but now we compute the matrix directly as

¯M =

1
L

(cid:88)

j∈[L]

¯T (Id, Id, wj, . . . , wj),

with 2r − 2 copies of wj. As before, one can show that
the matrix ¯M is likely to be close to the matrix (cid:80)
i∈[d] λi ·
ui ⊗ ui. Then again we can apply the matrix power method
on ¯M and obtain a good initialization for the tensor power
method as before. Note that now the eigenvalues are no
longer squared, and the previous requirement on (cid:107)Φ(cid:107) can
be slightly relaxed, with the dependence on ∆2 being re-
placed by ∆.

4.2. Asymmetric Tensors

For simplicity of presentation, let us focus on the third or-
der case; the extension to higher orders is straightforward.
That is, now the underlying tensor has the form

T =

λi · ai ⊗ bi ⊗ ci,

(cid:88)

i∈[d]

with nonnegative λi’s satisfying the condition (1), together
with three sets of orthonormal vectors of ai’s, bi’s, and ci’s.
As before, we only have access to a noisy version ¯T of T .

The main modiﬁcation of our algorithm is again to the ini-
tialization procedure, but the idea is also similar. To ﬁnd a
good initial matrix A for ai’s, we sample w1, . . . , wL inde-
pendently from N d(0, 1), and now compute the matrix

with 2r − 1 copies of ¯w, is close to the matrix (cid:80)
i∈[d] λ2r
·
i
ui ⊗ ui, similarly to Lemma 3. Then the rest is the same

¯M =

1
L

(cid:88)

j∈[L]

(cid:0) ¯T (Id, Id, wj)(cid:1) (cid:0) ¯T (Id, Id, wj)(cid:1)(cid:62)

.

As before, it is not hard to show that

such that W (cid:62)M W = Ik. The reason is that

Tensor Decomposition via Simultaneous Power Iteration

¯M ≈

(cid:88)

(cid:88)

λ2
i

(cid:0)c(cid:62)

i wj

(cid:1)2

· ai ⊗ ai,

1
L

i∈[d]

j∈[L]

i∈[d] λ2

which is close to the matrix (cid:80)
i · ai ⊗ ai with high
probability. From the matrix ¯M , we can again apply the
matrix power method to ﬁnd a good initial matrix A. Sim-
ilarly, we can ﬁnd good initial matrices B and C for bi’s
and ci’s, respectively.

Next, with such matrices, we would like to apply the tensor
power method, which we modify as follows. Now at each
step t, we take previous estimates A(t−1), B(t−1), C (t−1),
= ¯T (Id, B(t−1)
and compute X (t)
=
¯T (A(t−1)
), Z (t)
, Id, C (t−1)
, Id), for
i
i
i ∈ [k], followed by orthonormalizing X (t), Y (t), Z (t)
to obtain the new estimates A(t), B(t), C (t) via QR-
decomposition. It is not hard to show that the resulting al-
gorithm has a similar convergence rate as our Algorithm 1.

, C (t−1)
i
, B(t−1)
i

i = ¯T (A(t−1)

), Y (t)
i

i

i

i

5. Nonorthogonal but Symmetric Tensors

In the previous section, we consider general orthogonal ten-
sors, which can be asymmetric. In this section, we con-
sider non-orthogonal tensors which are symmetric. We
remark that for some latent variable models such as the
multi-view model, the corresponding asymmetric tensors
can be converted into symmetric ones (Anandkumar et al.,
2014a), so that our result here can still be applied. For
simplicity of exposition, let us again focus on the case of
order three, so that the given tensor has the form T =
(cid:80)
i∈[d] λi ·vi ⊗vi ⊗vi, but the vectors vi’s are no longer as-
sumed to be orthogonal to each other. Still we assume them
to be linearly independent, and we again assume without
loss of generality that (cid:107)T (cid:107) ≤ 1 and (cid:107)vi(cid:107) = 1 for each i. In
addition, let us assume, as in previous works, that λj = 0
for j ≥ k + 1.2

Following (Anandkumar et al., 2014a), we would like to
whiten such a tensor T into an orthogonal one, so that we
can then apply our Algorithm 1. More precisely, our goal is
to ﬁnd a d × k matrix W such that the tensor T (W, W, W )
becomes orthogonal. As in (Anandkumar et al., 2014a),
assume that we also have available a matrix

M =

λi · vi ⊗ vi.3

(cid:88)

i∈[k]

Then for a whitening matrix, it sufﬁces to ﬁnd some W

2This assumption is not necessary. We assume it just to sim-
plify the ﬁrst step of our algorithm given below. Without it, we
can simply replace that step by the matrix power method used in
our Algorithm 1, which takes more steps but can still do the job.
3More generally, the weights λi in M are allowed to differ
from those in T , but for simplicity we assume they are the same.

Ik = W (cid:62)M W =

λi · (cid:0)W (cid:62)vi

(cid:1) ⊗ (cid:0)W (cid:62)vi

(cid:1) ,

(cid:88)

i∈[k]

√

which implies that the vectors
orthonormal. Then the tensor T (W, W, W ) equals

λiW (cid:62)vi, for i ∈ [k], are

λi · (W (cid:62)vi)⊗3 =

(cid:88)

i∈[k]

(cid:88)

i∈[k]

1
√
λi

(cid:16)(cid:112)

·

λiW (cid:62)vi

(cid:17)⊗3

,

which has an orthogonal decomposition.

According to (Anandkumar et al., 2014a), one way to
ﬁnd such a W is to do the spectral decomposition of M
as U ΛU (cid:62), with eigenvectors as columns of U , and let
W = U Λ− 1
2 . However, we will not take this approach,
because ﬁnding a good approximate to U by the matrix
power method would take longer to converge than the ten-
sor power method which we will later apply to the whitened
tensor. Our key observation is that it sufﬁces to ﬁnd a d × k
matrix Q such that the matrix P = Q(cid:62)M Q is invertible,
since we can then let W = QP − 1

2 and have

W (cid:62)M W = P − 1

2 Q(cid:62)M QP − 1

2 = Ik.

With such a W , the tensor T (W, W, W ) becomes orthogo-
nal, so that we can decompose it4 to obtain σi = 1√
and
λi
λiW (cid:62)vi, from which we can recover λi = 1
ui =
σ2
i
vi = σiQP 1
2 ui if Q has orthonormal columns.

and

√

As before, we consider a similar setting in which we only
have access to a noisy ¯M = M + ¯Φ, for some symmetric
perturbation matrix ¯Φ, in addition to the noisy tensor ¯T =
T +Φ. Then our algorithm for ﬁnding the whitening matrix
consists of the following two steps:

1. Sample a random matrix Z ∈ Rd×k with orthonormal
columns, compute ¯Y = ¯M Z, and factorize it as ¯Y =
Q ¯R by a QR decomposition.

2. Compute ¯P = Q(cid:62) ¯M Q and output ¯W = Q ¯P − 1

2 as

the whitening matrix.

We analyze our algorithm in the following. First note that
Q is computed in the same way as we compute Z (1) in
Algorithm 1, and with λk+1 = 0 we are likely to have
tank(Q) ≈ 0 so that the matrix P = Q(cid:62)M Q is invert-
ible. Formally, we have the following, which we prove in
Appendix C.1.
Lemma 5. Suppose (cid:107) ¯Φ(cid:107) ≤ α0λk√
for a small enough con-
dk
stant α0. Then with high probability we have σmax(P ) ≤
λ1 and σmin(P ) ≥ λk
2 .

4To apply our Algorithm 1, we need to scale it properly, say
λk/k to make its norm at most one.

by a factor of

√

Tensor Decomposition via Simultaneous Power Iteration

Next, with a small enough (cid:107) ¯Φ(cid:107), if P is invertible, then so
is ¯P , and moreover, we have ¯P − 1
2 . This is shown
in the following, which we prove in Appendix C.2.

2 ≈ P − 1

Lemma 6. Fix any (cid:15) ∈ (0, 1) and suppose we have
σmin(P ) ≥ 2(cid:15) and (cid:107) ¯Φ(cid:107) ≤ (cid:15). Then ¯P is invertible and
(cid:107) ¯P − 1
2 (cid:107) ≤ 2(cid:15)(σmin(P ))−2(σmax(P )) 1
2 .

2 − P − 1

Then, with a good ¯P − 1
2 , we can obtain a good ¯W and have
¯T ( ¯W , ¯W , ¯W ) close to T (W, W, W ) which has an orthog-
onal decomposition. This is shown in the following, which
we prove in Appendix C.3.
Theorem 3. Fix any ε ∈ (0, λk

4 ) and suppose we have
, λ3
k ε and (cid:107) ¯Φ(cid:107) ≤ α0ε min{ λk√
}, for a
(cid:107)Φ(cid:107) ≤ α0λ
k√
λ1
small enough constant α0. Then with high probability we
have (cid:107) ¯T ( ¯W , ¯W , ¯W ) − T (W, W, W )(cid:107) ≤ ε.

dk

3
2

6. Streaming setting

In the previous sections, we consider the batch setting in
which the tensor ¯T is assumed to be stored somewhere
which can be accessed whenever we want to. However,
storing such a tensor, say of order three, requires a space
complexity of Ω(d3), which becomes impractical even for
a moderate value of d. In this section, we study the possibil-
ity of achieving a space complexity of O(kd), which is the
least amount of memory needed just to store the k vectors
in Rd. More precisely, we consider the streaming setting,
in which there is a stream of vectors x1, x2, . . . arriving
one at a time. We assume that each vector x is sampled in-
dependently from some distribution over Rd, with (cid:107)x(cid:107) ≤ 1
and some function g : Rd → Rd×d×d such that

• E[g(x)] = T , and given x, u, v ∈ Rd, g(x)(Id, u, v)

can be computed in O(d) space.

Such a function g is known to exist for some latent vari-
able models (Ge et al., 2015; Wang & Anandkumar, 2016).
Given such a function, our algorithms in previous sections
can all be converted to work in the streaming setting using
O(kd) space. This is because all our operations involv-
ing tensors have the form ¯T (Id, u, v), for some u, v ∈ Rd,
which can be realized as

(cid:33)

(cid:32)

1
|J|

(cid:88)

t∈J

1
|J|

(cid:88)

t∈J

g(xt)

(Id, u, v) =

(g(xt) (Id, u, v)) ,

for a collection J of samples, with the righthand side above
clearly computable in O(kd) space.5 Then depending on
the distance we want between ¯T = 1
t∈J g(xt) and T ,
|J|

(cid:80)

5This also includes the initialization phase in which we now do
not store the matrix ¯M explicitly but instead replace the operation
¯M Zi by ¯T (Id, Zi, ¯w).

we can choose a proper size for J. In fact, to save the to-
tal number of samples, we can follow the approach of (Li
et al., 2016) by choosing different sizes in different itera-
tions of the matrix or tensor power method.

Following (Wang & Anandkumar, 2016), let us take the
speciﬁc case with g(x) = x ⊗ x ⊗ x as a concrete example
and focus on the orthogonal case studied in Section 3; it is
not hard to convert other algorithms of ours to the stream-
ing setting. One can show that in this speciﬁc case, we have
E[x] = (cid:80)
i∈[d] λiui so that there is a more efﬁcient way to
ﬁnd a vector ¯w for producing the matrix ¯M in the initializa-
tion phase. Formally, we have the following lemma, which
we prove in Appendix D.1.

Lemma 7. There is an algorithm using O(d) space and
O( log k
∆2 ) samples to ﬁnd some ¯w ∈ Rd satisfying the con-
dition (5) in Lemma 2 with high probability.

With such a vector ¯w, we can then use the streaming algo-
rithm of (Li et al., 2016) to ﬁnd a good initial matrix Z for
the later tensor power phase. Formally, we have the follow-
ing lemma, which we prove in Appendix D.2.

Lemma 8. Given ¯w from Lemma 7, we can use O(kd)
) samples to ﬁnd some Z ∈ Rd×k
space and O(
with tanm(Z) < 1, for any m ∈ [k], with high probability.

kd log d
γ
∆4γ

Having such a matrix Z, we can proceed to the tensor
power phase. Borrowing again the idea from (Li et al.,
2016), let us partition the incoming data into blocks of in-
creasing sizes, with the t’th block Jt used to carry out one
, Q(t−1)
tensor power iteration Y (t)
), for
i
i ∈ [k], of Algorithm 1, with ¯T (t) = 1
g(xτ ). In-
|Jt|
stead of preparing this ¯T (t) and then computing each Y (t)
we now go through |Jt| steps of updates:

i = ¯T (t)(Id, Q(t−1)

i
(cid:80)

τ ∈Jt

,

i

• For τ ∈ Jt do: Y (t)

i = Y (t)

i + 1

|Jt| (x(cid:62)

τ Q(t−1)

i

)2xτ .

The block sizes are chosen carefully to keep (cid:107) ¯T (t) − T (cid:107)
small enough so that we can have tanm(Q(t)) decreased in
a desirable rate. Here, we choose the parameters

βt = max

(cid:110)

ρ2t−1,

(cid:111)

ε
2

and |Jt| =

c0 log(dt)
∆2β2
t

,

(6)

for a large enough constant c0, to make the condition (3)
in Lemma 1 hold with high probability so that we have
tanm(Q(t)) ≤ βt. In Appendix D.3, we summarize our
algorithm and prove the following theorem.
Theorem 4. Given ε ∈ (0, λk
2 ), with high probability we
can ﬁnd ˆλi, ˆui with | ˆλi − λi|, (cid:107) ˆui − ui(cid:107) ≤ ε, for any i ∈
ε ))
[k], using O(kd) space and O(
)
samples.

∆4γ +

log(d log( 1

kd log d
γ

γ log 1

∆2γε2

Tensor Decomposition via Simultaneous Power Iteration

References

Anandkumar, Animashree, Hsu, Daniel J, and Kakade,
Sham M. A method of moments for mixture models and
hidden markov models. In COLT, volume 1, pp. 4, 2012.

Anandkumar, Animashree, Ge, Rong, Hsu, Daniel,
Kakade, Sham M, and Telgarsky, Matus. Tensor decom-
positions for learning latent variable models. Journal of
Machine Learning Research, 15(1):2773–2832, 2014a.

Anandkumar, Animashree, Ge, Rong, and Janzamin, Ma-
tensor decomposi-
arXiv preprint

jid.
tion via alternating rank-1 updates.
arXiv:1402.5180, 2014b.

Guaranteed non-orthogonal

Chaganty, Arun Tejasvi and Liang, Percy. Estimating
latent-variable graphical models using moments and
likelihoods. In ICML, pp. 1872–1880, 2014.

Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang. Es-
caping from saddle pointsonline stochastic gradient for
tensor decomposition. In Proceedings of The 28th Con-
ference on Learning Theory, pp. 797–842, 2015.

Golub, Gene H. and Van Loan, Charles F. Matrix Com-
John Hopkins University Press, 3rd edition,

putation.
1996.

Hardt, Moritz and Price, Eric. The noisy power method: A
meta algorithm with applications. In Advances in Neural
Information Processing Systems, pp. 2861–2869, 2014.

Hillar, Christopher J and Lim, Lek-Heng. Most tensor
problems are NP-hard. Journal of the ACM (JACM), 60
(6), 2013.

Kolda, Tamara G and Bader, Brett W. Tensor decompo-
sitions and applications. SIAM review, 51(3):455–500,
2009.

Li, Chun-Liang, Lin, Hsuan-Tien, and Lu, Chi-Jen. Rivalry
of two families of algorithms for memory-restricted
streaming PCA. In Proceedings of the 19th International
Conference on Artiﬁcial Intelligence and Statistics (AIS-
TATS), pp. 473–481, 2016.

Mitliagkas, Ioannis, Caramanis, Constantine, and Jain, Pra-
teek. Memory limited, streaming PCA. In Advances in
Neural Information Processing Systems, pp. 2886–2894,
2013.

Wang, Yining and Anandkumar, Anima. Online and
In Ad-
differentially-private tensor decomposition.
vances in Neural Information Processing Systems, pp.
3531–3539, 2016.

