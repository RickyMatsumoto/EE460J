Conditional Image Synthesis with Auxiliary Classiﬁer GANs: Appendix

Augustus Odena 1 Christopher Olah 1 Jonathon Shlens 1

1. Hyperparameters

We summarize hyperparameters used for the ImageNet
model in Table 1 and for the CIFAR-10 model in Table 2.

Conditional Image Synthesis with Auxiliary Classiﬁer GANs: Appendix

Operation Kernel

Strides

Feature maps BN? Dropout Nonlinearity

Gx(z) – 110 × 1 × 1 input

Linear N/A

Transposed Convolution
Transposed Convolution
Transposed Convolution
Transposed Convolution
D(x) – 128 × 128 × 3 input
Convolution
Convolution
Convolution
Convolution
Convolution
Convolution

5 × 5
5 × 5
5 × 5
5 × 5

3 × 3
3 × 3
3 × 3
3 × 3
3 × 3
3 × 3

Linear N/A

N/A
2 × 2
2 × 2
2 × 2
2 × 2

2 × 2
1 × 1
2 × 2
1 × 1
2 × 2
1 × 1
N/A

768
384
256
192
3

16
32
64
128
256
512
11

×
√
√
√

×

×
√
√
√
√
√

×

0.0
0.0
0.0
0.0
0.0

0.5
0.5
0.5
0.5
0.5
0.5
0.0

ReLU
ReLU
ReLU
ReLU
Tanh

Leaky ReLU
Leaky ReLU
Leaky ReLU
Leaky ReLU
Leaky ReLU
Leaky ReLU
Soft-Sigmoid

Optimizer Adam (α = 0.0002, β1 = 0.5, β2 = 0.999)
Batch size 100
Iterations 50000

Leaky ReLU slope 0.2

Weight, bias initialization Isotropic gaussian (µ = 0, σ = 0.02), Constant(0)

Table 1. ImageNet hyperparameters. A Soft-Sigmoid refers to an operation over K +1 output units where we apply a Softmax activation
to K of the units and a Sigmoid activation to the remaining unit. We also use activation noise in the discriminator as suggested in (?).

Conditional Image Synthesis with Auxiliary Classiﬁer GANs: Appendix

Operation Kernel

Strides

Feature maps BN? Dropout Nonlinearity

Gx(z) – 110 × 1 × 1 input

Linear N/A

Transposed Convolution
Transposed Convolution
Transposed Convolution
D(x) – 32 × 32 × 3 input
Convolution
Convolution
Convolution
Convolution
Convolution
Convolution

5 × 5
5 × 5
5 × 5

3 × 3
3 × 3
3 × 3
3 × 3
3 × 3
3 × 3

Linear N/A

N/A
2 × 2
2 × 2
2 × 2

2 × 2
1 × 1
2 × 2
1 × 1
2 × 2
1 × 1
N/A

384
192
96
3

16
32
64
128
256
512
11

×
√
√

×

×
√
√
√
√
√

×

0.0
0.0
0.0
0.0

0.5
0.5
0.5
0.5
0.5
0.5
0.0

ReLU
ReLU
ReLU
Tanh

Leaky ReLU
Leaky ReLU
Leaky ReLU
Leaky ReLU
Leaky ReLU
Leaky ReLU
Soft-Sigmoid

Generator Optimizer Adam (α = [0.0001, 0.0002, 0.0003], β1 = 0.5, β2 = 0.999)
Discriminator Optimizer Adam (α = [0.0001, 0.0002, 0.0003], β1 = 0.5, β2 = 0.999)

Batch size 100
Iterations 50000

Leaky ReLU slope 0.2

Activation noise standard deviation [0, 0.1, 0.2]

Weight, bias initialization Isotropic gaussian (µ = 0, σ = 0.02), Constant(0)

Table 2. CIFAR-10 hyperparameters. When a list is given for a hyperparameter it means that we performed a grid search using the
values in the list. For each set of hyperparameters, a single AC-GAN was trained on the whole CIFAR-10 dataset. For each AC-GAN
that was trained, we split up the samples into groups so that we could give some sense of the variance of the Inception Score. To the best
of our knowledge, this is identical to the analysis performed in (?).

