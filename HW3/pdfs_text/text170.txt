Supplement to “An Inﬁnite Hidden Markov Model with
Similarity-Biased Transitions”, ICML 2017

Colin Dawson, Chaofan Huang, Clayton Morrison

June 13, 2017

This supplement contains additional derivations to accompany the model deﬁnition and Gibbs updates for the
paper “An Inﬁnite Hidden Markov Model with Similariy-Biased Transitions”, published in ICML 2017. Section
1 concerns the derivation of the augmented data representation referred to as the “Markov Jump Process with
Failed Transitions” (MJP-FT). Section 2 ﬁlls in details for the Gibbs sampling steps to sample the rescaled HDP
used by the HDP-HMM-LT. Section 3 gives a derivation for the updates to the binary state vectors, θ, in the
version of the HDP-HMM-LT used in the cocktail party experiment. Finally, section 4 gives the details for the
Hamiltonian Monte Carlo update for (cid:96) in the version of the model used in the Bach chorale experiment.

1. Details of the Markov Jump Process with Failed Transitions Representation

We can gain stronger intuition, as well as simplify posterior inference, by re-casting the HDP-HMM-LT as a
continuous time Markov Jump Process where some of the attempts to jump from one state to another fail, and
where the failure probability increases as a function of the “distance” between the states.

Let φ be deﬁned as in the last section, and let β, θ and π be deﬁned as in the Normalized Gamma Process
representation of the ordinary HDP-HMM. That is,

β ∼ GEM(γ)
i.i.d∼ H
πjj(cid:48) | β, θ ∼ Gamma(αβj(cid:48), 1)

θj

(1)

(2)

(3)

Now suppose that when the process is in state j, jumps to state j(cid:48) are made at rate πjj(cid:48). This deﬁnes a continuous-
time Markov Process where the oﬀ-diagonal elements of the transition rate matrix are the oﬀ diagonal elements
of π. In addition, self-jumps are allowed, and occur with rate πjj. If we only observe the jumps and not the
durations between jumps, this is an ordinary Markov chain, whose transition matrix is obtained by appropriately
normalizing π. If we do not observe the jumps themselves, but instead an observation is generated once per
jump from a distribution that depends on the state being jumped to, then we have an ordinary HMM.

We modify this process as follows. Suppose that each jump attempt from state j to state j(cid:48) has a chance of failing,
which is an increasing function of the “distance” between the states. In particular, let the success probability be
φjj(cid:48) (recall that we assumed above that 0 ≤ φjj(cid:48) ≤ 1 for all j, j(cid:48)). Then, the rate of successful jumps from j to
j(cid:48) is πjj(cid:48)φjj(cid:48), and the corresponding rate of unsuccessful jump attempts is πjj(cid:48)(1 − φjj(cid:48)). To see this, denote by
Njj(cid:48) the total number of jump attempts to j(cid:48) in a unit interval of time spent in state j. Since we are assuming
the process is Markovian, the total number of attempts is Poisson(πjj(cid:48)) distributed. Conditioned on Njj(cid:48), njj(cid:48)
will be successful, where

njj(cid:48) | Njj(cid:48) ∼ Binom(Njj(cid:48), φjj(cid:48))

(4)

It is easy to show (and well known) that the marginal distribution of njj(cid:48) is Poisson(πjj(cid:48)φjj(cid:48)), and the marginal
distribution of ˜qjj(cid:48) := Njj(cid:48) − njj(cid:48) is Poisson(πjj(cid:48)(1 − φjj(cid:48))). The rate of successful jumps from state j overall is
then Tj := (cid:80)
Let t index jumps, so that zt indicates the tth state visited by the process (couting self-jumps as a new time
step). Given that the process is in state j at discretized time t − 1 (that is, zt−1 = j), it is a standard property of
1

j(cid:48) πjj(cid:48)φjj(cid:48).

Markov Processes that the probability that the ﬁrst successful jump is to state j(cid:48) (that is, zt = j(cid:48)) is proportional
to the rate of successful attempts to j(cid:48), which is πjj(cid:48)φjj(cid:48).

Let ˜ut indicate the time elapsed between the tth and and t − 1th successful jump (where we assume that the
ﬁrst observation occurs when the ﬁrst successful jump from a distinguished initial state is made). We have

˜ut | zt−1 ∼ Exp(Tzt−1)

where ˜ut is independent of zt.
During this period, there will be ˜qj(cid:48)t unsuccessful attempts to jump to state j(cid:48), where

˜qj(cid:48)t | zt−1 ∼ Poisson(˜utπzt−1j(cid:48)(1 − φzt−1j(cid:48)))

Deﬁne the following additional variables

Tj = {t | zt−1 = j}

qjj(cid:48) =

˜qj(cid:48)t

(cid:88)

t∈Tj
(cid:88)

t∈Tj

uj =

˜ut

and let Q = (qjj(cid:48))j,j(cid:48)≥1 be the matrix of unsuccessful jump attempt counts, and u = (uj)j≥1 be the vector of
the total times spent in each state.

Since each of the ˜ut with t ∈ Tj are i.i.d. Exp(Tj), we get the marginal distribution

by the standard property that sums of i.i.d. Exponential distributions has a Gamma distribution with shape
equal to the number of variates in the sum, and rate equal to the rate of the individual exponentials. Moreover,
since the ˜qj(cid:48)t with t ∈ Tj are Poisson distributed, the total number of failed attempts in the total duration uj is

uj | z, π, φ ind∼ Gamma(nj·, Tj)

qjj(cid:48)

ind∼ Poisson(ujπjj(cid:48)(1 − φjj(cid:48))).

Thus if we marginalize out the individual ˜ut and ˜qj(cid:48)t, we have a joint distribution over z, u, and Q, conditioned
on the transition rate matrix π and the success probability matrix φ, which is

p(z, u, Q | π, φ) =

p(zt | zt−1)

p(uj | z, π, φ)

p(qjj(cid:48) | ujπjj(cid:48), φjj(cid:48))

(cid:32) T
(cid:89)

(cid:32)

t=1

(cid:89)

t

=

(cid:33)

(cid:89)

j
(cid:33)

(cid:89)

j(cid:48)

πzt−1ztφzt−1zt
Tzt−1
(cid:89)

T nj·
j
Γ(nj·)

(cid:89)

j

unj·−1
j

e−Tj uj

×

e−uj πjj(cid:48) (1−φjj(cid:48) )u

qjj(cid:48)
j π

qjj(cid:48)
jj(cid:48) (1 − φjj(cid:48))qjj(cid:48) (qjj(cid:48)!)−1

(cid:89)

=

Γ(nj·)−1unj·+qj·−1
j

j(cid:48)

(cid:89)

×

j(cid:48)

j

j

π

njj(cid:48) +qjj(cid:48)
jj(cid:48)

φ

njj(cid:48)
jj(cid:48) (1 − φjj(cid:48))qjj(cid:48) e−πjj(cid:48) φjj(cid:48) uj e−πjj(cid:48) (1−φjj(cid:48) )uj (qjj(cid:48)!)−1

(cid:89)

=

Γ(nj·)−1unj·+qj·−1
j

π

njj(cid:48) +qjj(cid:48)
jj(cid:48)

φ

njj(cid:48)
jj(cid:48) (1 − φjj(cid:48))qjj(cid:48) e−πjj(cid:48) uj (qjj(cid:48)!)−1

(cid:89)

j(cid:48)

Setting aside terms that do not depend on π, we get the conditional likelihood function used in sampling π:

p(z, u, Q | π, φ) ∝

(cid:89)

(cid:89)

j

j(cid:48)

π

njj(cid:48) +qjj(cid:48)
jj(cid:48)

e−πjj(cid:48) uj

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

which, combined with the independent Gamma priors on π yields conditionally independent Gamma posteriors:

πjj(cid:48) | z, u, Q, β, α ind.∼ Gamma(αβj(cid:48) + njj(cid:48) + qjj(cid:48), 1 + uj)

2. Inference details for hyperparameters of the rescaled HDP

2.1. Sampling π, β, α and γ

The joint conditional over γ, α, β and π given the augmented data D = (z, u, Q, M, r, w) factors as

p(γ, α, β, π | D) = p(γ | D)p(α | D)p(β | γ, D)p(π | α, β, D)

We will derive these four factors in reverse order.

Sampling π The entries in π are conditionally independent given α and β, so we have the prior

p(π | β, α) =

Γ(αβj(cid:48))−1π

αβj(cid:48) −1
jj(cid:48)

exp(−πjj(cid:48)),

and the likelihood given {z, u, Q} given by (17). Combining these, we have

p(π, z, u, Q | β, α, φ) =

unj·+qj·−1
j

Γ(αβj(cid:48))−1π

αβj(cid:48) +njj(cid:48) +qjj(cid:48) −1
jj(cid:48)

(cid:89)

j(cid:48)

× e−(1+uj )πjj(cid:48) φ

njj(cid:48)
jj(cid:48) (1 − φjj(cid:48))qjj(cid:48) (qjj(cid:48)!)−1

Conditioning on everything except π, we get

p(π | Q, u, z, β, α) ∝

π

αβj(cid:48) +njj(cid:48) +qjj(cid:48) −1
jj(cid:48)

exp(−(1 + uj)πjj(cid:48))

(cid:89)

(cid:89)

j

j(cid:48)

(cid:89)

j

(cid:89)

(cid:89)

j

j(cid:48)

and thus we see that the πjj(cid:48) are conditionally independent given u, z and Q, and distributed according to

πjj(cid:48) | njj(cid:48), qjj(cid:48), βj(cid:48), α ind∼ Gamma(αβj(cid:48) + njj(cid:48) + qjj(cid:48), 1 + uj)

Sampling β Consider the conditional distribution of β having integrated out π. The prior density of β is

p(β | γ) =

Γ(γ)
Γ( γ
J )J

(cid:89)

γ
J −1
j

β

j

After integrating out π in (22), we have

p(z, u, Q | β, α, γ, φ) =

unjj(cid:48) +qjj(cid:48) −1(1 + uj)−(αβj(cid:48) +njj(cid:48) +qjj(cid:48) )

J
(cid:89)

j=1

u−1
j

J
(cid:89)

j(cid:48)=1

×

Γ(αβj(cid:48) + njj(cid:48) + qjj(cid:48))
Γ(αβj(cid:48))

φ

njj(cid:48)
jj(cid:48) (1 − φjj(cid:48))qjj(cid:48) (qjj(cid:48)!)−1

=

J
(cid:89)

j=1

Γ(nj·)−1u−1

j (1 + uj)−α

(cid:18) uj

(cid:19)nj·+qj·

1 + uj

×

J
(cid:89)

j(cid:48)=1

Γ(αβj(cid:48) + njj(cid:48) + qjj(cid:48))
Γ(αβj(cid:48))

φ

njj(cid:48)
jj(cid:48) (1 − φjj(cid:48))qjj(cid:48) (qjj(cid:48)!)−1

where we have used the fact that the βj sum to 1. Therefore

p(β | z, u, Q, α, γ) ∝

J
(cid:89)

j=1

γ
J −1
j

β

J
(cid:89)

j(cid:48)=1

Γ(αβj(cid:48) + njj(cid:48) + qjj(cid:48))
Γ(αβj(cid:48))

.

(19)

(20)

(21)

(22)

(23)

(24)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

Following (Teh et al., 2006), we can write the ratios of Gamma functions as polynomials in βj, as

p(β | z, u, Q, α, γ) ∝

s(njj(cid:48) + qjj(cid:48), mjj(cid:48))(αβj(cid:48))mjj(cid:48)

(32)

J
(cid:89)

j=1

γ
J −1
j

β

J
(cid:89)

njj(cid:48)
(cid:88)

j(cid:48)=1

mjj(cid:48) =1

where s(m, n) is an unsigned Stirling number of the ﬁrst kind, which is used to represent the number of permu-
tations of n elements such that there are m distinct cycles.

This admits an augmented data representation, where we introduce a random matrix M = (mjj(cid:48))1≤j,j(cid:48)≤J , whose
entries are conditionally independent given β, Q and z, with

p(mjj(cid:48) = m | βj(cid:48), α, njj(cid:48), qjj(cid:48)) =

(33)

s(njj(cid:48) + qjj(cid:48), m)αmβm
j(cid:48)

(cid:80)njj(cid:48) +qjj(cid:48)
m(cid:48)=0

s(njj(cid:48) + qjj(cid:48), m(cid:48))αm(cid:48)βm(cid:48)
j(cid:48)

for integer m ranging between 0 and njj(cid:48) + qjj(cid:48). Note that s(n, 0) = 0 if n > 0, s(0, 0) = 1, s(0, m) = 0 if m > 0,
and we have the recurrence relation s(n + 1, m) = ns(n, m) + s(n, m − 1), and so we could compute each of
these coeﬃcients explicitly; however, it is typically simpler and more computationally eﬃcient to sample from
this distribution by simulating the number of occupied tables in a Chinese Restaurant Process with n customers,
than it is to enumerate its probabilities.

For each mjj(cid:48) we simply draw njj(cid:48) assignments of customers to tables according to the Chinese Restaurant
Process and set mjj(cid:48) to be the number of distinct tables realized; that is, assign the ﬁrst customer to a table,
setting mjj(cid:48) to 1, and then, after n customers are assigned, assign the n + 1th customer to a new table with
probability αβj(cid:48)/(n+αβj(cid:48)), in which case we increment mjj(cid:48), and to an existing table with probability n/(n+α),
in which case we do not increment mjj(cid:48).

Then, we have joint distribution

p(β, M | z, u, Q, α, γ) ∝

s(njj(cid:48) + qjj(cid:48), mjj(cid:48))αmjj(cid:48) β

mjj(cid:48)
j(cid:48)

J
(cid:89)

j=1

γ
J −1
j

β

J
(cid:89)

j(cid:48)=1

which yields (32) when marginalized over M. Again discarding constants in β and regrouping yields

which is Dirichlet:

Sampling α and γ Assume that α and γ have Gamma priors, parameterized by shape, a and rate, b:

p(β | M, z, u, θ, α, γ) ∝

J
(cid:89)

j(cid:48)=1

γ
J +m·j(cid:48) −1
j(cid:48)

β

β | M, γ ∼ Dirichlet(

+ m·1, . . . ,

+ m·J )

γ
J

γ
J

p(α) =

αaα−1 exp(−bαα)

p(γ) =

γaγ−1 exp(−bγγ)

baα
α
Γ(aα)
baγ
γ
Γ(aγ)

Having integrated out π, we have

p(β, z, u, Q, M | α, γ, φ) =

Γ(γ)
Γ( γ

J )J αm··

J
(cid:89)

j=1

γ
J +m·j −1
j

β

Γ(nj·)−1u−1

j (1 + uj)−α

(cid:18) uj

(cid:19)nj·+qj·

1 + uj

×

s(njj(cid:48) + qjj(cid:48), mjj(cid:48))φ

njj(cid:48)
jj(cid:48) (1 − φjj(cid:48))qjj(cid:48) (qjj(cid:48)!)−1

J
(cid:89)

j(cid:48)=1

(34)

(35)

(36)

(37)

(38)

(39)

(40)

We can also integrate out β, to yield

p(z, u, Q, M | α, γ, φ) = αm··e− (cid:80)

j(cid:48)(cid:48) log(1+uj(cid:48)(cid:48) )α

Γ(γ)
Γ(γ + m··)
(cid:18) uj

u−1
j

1 + uj

(cid:19)nj·+qj·

(cid:89)

×

Γ( γ
Γ( γ

J + m·j)
J )Γ(nj·)

j

J
(cid:89)

j(cid:48)=1

×

s(njj(cid:48) + qjj(cid:48), mjj(cid:48))φ

njj(cid:48)
jj(cid:48) (1 − φjj(cid:48))qjj(cid:48) (qjj(cid:48)!)−1

demonstrating that α and γ are independent given φ and the augmented data, with

p(α | z, u, Q, M) ∝ αaα+m·· exp(−(bα +

log(1 + uj))α)

(cid:88)

j

(cid:88)

j

(cid:17)r

(cid:16) γ
J

and

So we see that

so that

and

which is to say

p(γ | z, u, Q, M) ∝ γaγ−1 exp(−bγγ)

Γ(γ) (cid:81)J
Γ( γ

j=1 Γ( γ
J )J Γ(γ + m··)

J + m·j)

α | z, u, Q, M ∼ Gamma(aα + m··, bα +

log(1 + uj))

p(rj(cid:48) = r | m·j(cid:48), γ) =

s(m·j(cid:48), r)

r = 1, . . . , m·j

p(w | m··γ) =

wγ−1(1 − w)m··−1

w ∈ (0, 1)

Γ( γ
J )
Γ( γ
J + m·j(cid:48))
Γ(γ + m··)
Γ(γ)Γ(m··)

p(γ, r, w | M) ∝ γaγ−1 exp(−bγγ)wγ−1(1 − w)m··−1

s(m·j(cid:48), rj(cid:48))

J
(cid:89)

j(cid:48)=1

(cid:17)rj(cid:48)

(cid:16) γ
J

p(γ | r, w) ∝ γaγ +r·−1 exp(−(bγ − log(w))γ),

γ | r, w, z, u, Q, M ∼ Gamma(aγ + r·, bγ − log(w))

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

To sample γ, we introduce a new set of auxiliary variables, r = (r1, . . . , rJ ) and w with the following distributions:

3. Derivation of η update in the Cocktail Party and Synthetic Data Experiments

In principle, η can have any distribution over binary vectors, but we will suppose for simplicity that it can be
factored into D independent coordinate-wise Bernoulli variates. Let µd be the Bernoulli parameter for the dth
coordinate.

The similarity function φjj(cid:48) is the Laplacian kernel:

φjj(cid:48) = Φ(ηj, ηj(cid:48)) = exp(−λdjj(cid:48))

(52)

where djj(cid:48)d = |ηjd − ηj(cid:48)d| is Hamming distance in the dth coordinate, djj(cid:48) := (cid:80)D
d=1 djj(cid:48) is the total Hamming
distance between ηj and ηj(cid:48), and λ ≥ 0 (if λ = 0, the φjj(cid:48) are identically 1, and so do not have any inﬂuence,
reducing the model to an ordinary HDP-HMM).

Let

φjj(cid:48)−d = exp(−λ(djj(cid:48) − djj(cid:48)d))

so that φjj(cid:48) = φjj(cid:48)−de−λdjj(cid:48) d .

Since the matrix φ is assumed to be symmetric, we have

p(z, Q | ηjd = 1, η \ ηjd)
p(z, Q | ηjd = 0, η \ ηjd)

∝

(cid:89)

j(cid:48)(cid:54)=j

e−λ(njj(cid:48) +nj(cid:48) j )|1−θj(cid:48)d|(1 − φjj(cid:48)−de−λ|1−θj(cid:48) d|)qjj(cid:48) +qj(cid:48) j
e−λ(njj(cid:48) +nj(cid:48)j )|θj(cid:48)d|(1 − φjj(cid:48)−de−λ|θj(cid:48) d|)qjj(cid:48) +qj(cid:48) j
j(cid:48) d (qjj(cid:48) +qj(cid:48) j )

θ

(cid:19)(−1)

= e−λ(cjd0−cjd1) (cid:89)

(cid:18) 1 − φjj(cid:48)−de−λ
1 − φjj(cid:48)−d

j(cid:48)(cid:54)=j

where cjd0 and cjd1 are the number of successful jumps to or from state j, to or from states with a 0 or 1,
respectively, in position d. That is,

cjd0 =

(cid:88)

{j(cid:48) | θj(cid:48)d=0}

(cid:88)

{j(cid:48) | θj(cid:48) d=1}

njj(cid:48) + nj(cid:48)j

cjd1 =

njj(cid:48) + nj(cid:48)j

(56)

Therefore, we can Gibbs sample ηjd from its conditional posterior Bernoulli distribution given the rest of η,
where we compute the Bernoulli parameter via the log-odds

log

(cid:18) p(ηjd = 1 | Y, z, Q, η \ ηjd)
p(ηjd = 0 | Y, z, Q, η \ ηjd)

(cid:19)

= log

(cid:18) p(ηjd = 1)p(z, Q | ηjd = 1, η \ ηjd)p(Y | z, ηjd = 1, η \ ηjd)
p(ηjd = 0)p(z, Q | ηjd = 0, η \ ηjd)p(Y | z, ηjd = 0, η \ ηjd)

(cid:19)

= log

(cid:18) µd

(cid:19)

1 − µd

+ (cjd1 − cjd0)λ +

(−1)θj(cid:48) d (qjj(cid:48) + qj(cid:48)j) log

(cid:88)

j(cid:48)(cid:54)=j

(cid:32) 1 − φ(−d)

jj(cid:48) e−λ

(cid:33)

1 − φ(−d)

jj(cid:48)

(cid:88)

+

{t | zt=j}

log

(cid:18) f (yt; ηjd = 1, ηj \ ηjd)
f (yt; ηjd = 0, ηj \ ηjd)

(cid:19)

Suppose also that the observed data Y consists of a T × K matrix, where the tth row yt = (yt1, . . . , ytK)T is a
K-dimensional feature vector associated with time t, and let W be a D × K weight matrix with kth column wk,
such that

for a suitable parametric function g. We assume for simplicity that g factors as

f (yt; ηj) = g(yt; WTηj)

g(yt; WTηj) =

gk(ytk; wk · ηj)

K
(cid:89)

k=1

Deﬁne xtk = wk · θzt, and x(−d)
coordinate removed. Then

tk = w−d

k

· θ−d
zt

, where θ−d

and w−d

j

k

are θj and wk, respectively, with the dth

log

(cid:18) f (yt; ηjd = 1, ηj \ ηjd)
f (yt; ηjd = 0, ηj \ ηjd)

(cid:19)

=

log

K
(cid:88)

k=1

(cid:32)

gk(ytk; x(−d)

tk + wdk)

(cid:33)

.

gk(ytk; x(−d)

)

tk

If gk(y; x) is a Normal density with mean x and unit variance, then

(cid:32)

log

gk(ytk; x(−d)

tk + wdk)

(cid:33)

gk(ytk; x(−d)

)

tk

= −wdk(ytk − x(−d)

tk +

wdk)

1
2

(53)

(54)

(55)

(57)

(58)

(59)

(60)

(61)

(62)

(63)

4. Derivation of HMC update for (cid:96) in the Bach Chorale Experiment

We have a set of states with parameters (cid:96)j, j = 1, . . . , J. In the previous version of the model, (cid:96)j was a binary
state vector on which both the similarities φjj(cid:48) and the emission distribution Fj depended. Here, we deﬁne the
latent locations (cid:96)j = ((cid:96)j1, (cid:96)jD) to be locations in RD, independent of the emission distributions, so that during
inference they are informed solely by the transitions.

We set

where djj(cid:48) is the Euclidean distance between (cid:96)j and (cid:96)j(cid:48); that is,

φjj(cid:48)((cid:96)j, (cid:96)j(cid:48)) = exp

−

(cid:18)

(cid:19)

λ
2

d2
jj(cid:48)

d2
jj(cid:48) =

((cid:96)jd − (cid:96)j(cid:48)d)2

(cid:88)

d

Since now (cid:96)j are continuous locations, we use Hamlitonian Monte Carlo (Duane et al., 1987; Neal et al., 2011) to
sample them jointly. HMC is a variation on Metropolis-Hastings algorithm which is designed to more eﬃciently
explore a high-dimensional continuous distribution by adopting a proposal distribution which incorporates an
auxiliary “momentum” variable to make it more likely that proposals will go in useful directions and improve
mixing compared to naive movement.

To do Hamiltonian Monte Carlo to sample from the conditional posterior of (cid:96) given z and Q, we need to compute
the gradient of the log posterior, which is just the sum of the gradient of the log prior and the gradient of the
log likelihood.

Assume independent and isotropic Gaussian priors on each (cid:96)j, so we have

where h(cid:96) is the prior precision which does not depend on d.

Then the log prior density, up to an additive constant c, is

p((cid:96)j) ∝ exp

−

(cid:32)

(cid:33)

(cid:96)2
jd

,

h(cid:96)
2

(cid:88)

d

log p((cid:96)j) = c −

h(cid:96)
2

(cid:88)

(cid:96)2
jd

d

The relevant log likelihood is the log of the probability of the z and Q variables given the φjj(cid:48). In particular, we
have

so that

L := p(z, Q | φ) ∝

(cid:89)

(cid:89)

j

j(cid:48)

φ

njj(cid:48)
jj(cid:48) (1 − φjj(cid:48))qjj(cid:48)

log L =

(njj(cid:48) log(φjj(cid:48)) + qjj(cid:48) log(1 − φjj(cid:48)))

(cid:88)

(cid:88)

j

j(cid:48)

The j, d coordinate of the gradient of the log prior is simply −h(cid:96)(cid:96)jd.

To get the j, d coordinate of the gradient of the log likelihood, we can apply the chain rule to terms as is
convenient. In particular,

∂L
∂(cid:96)jd

=

(cid:88)

(cid:88)

njj(cid:48)

j

j(cid:48)

∂ log(φjj(cid:48))
∂d2

jj(cid:48)

∂d2
jj(cid:48)
∂(cid:96)jd

(cid:88)

(cid:88)

+

qjj(cid:48)

j

j(cid:48)

∂ log(1 − φjj(cid:48))
∂(1 − φjj(cid:48))

∂(1 − φjj(cid:48))
∂d2

jj(cid:48)

∂d2
jj(cid:48)
∂(cid:96)jd

We have the following components:

= 2djj(cid:48)dI(j (cid:54)= j(cid:48))

∂ log(φjj(cid:48))
∂d2

jj(cid:48)
∂d2
jj(cid:48)
∂(cid:96)jd
∂ log(1 − φjj(cid:48))
∂(1 − φjj(cid:48))

∂(1 − φjj(cid:48))
∂d2

jj(cid:48)

= −

λ
2

=

=

1
1 − φjj(cid:48)
λ
2

φjj(cid:48)

which yields

∂L
∂(cid:96)jd

(cid:88)

(cid:88)

= −λ

njj(cid:48)djj(cid:48)dI(j (cid:54)= j(cid:48)) + λ

(cid:88)

(cid:88)

qjj(cid:48)djj(cid:48)d

φjj(cid:48)
1 − φjj(cid:48)

I(j (cid:54)= j)

= −λ

djj(cid:48)d

njj(cid:48) − qjj(cid:48)

j

j(cid:48)

(cid:88)

(j,j(cid:48)):j(cid:54)=j(cid:48)

(cid:18)

j(cid:48)
(cid:19)

j
φjj(cid:48)
1 − φjj(cid:48)

References

letters B, 195(2):216–222, 1987.

113–162, 2011.

Duane, Simon, Kennedy, Anthony D, Pendleton, Brian J, and Roweth, Duncan. Hybrid monte carlo. Physics

Neal, Radford M et al. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2:

Teh, Yee Whye, Jordan, Michael I, Beal, Matthew J, and Blei, David M. Hierarchical Dirichlet processes. Journal

of the American Statistical Association, 101(476), 2006.

