Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Supplementary Materials

A. Proof of Theorem 1

We ﬁrst recall the following lemma.
Lemma 1 (Lemma 1, (Gong et al., 2013)). Under Assumption 1.{3}. For any η > 0 and any x, y ∈ Rd such that
x = proxηg(y − η∇f (y)), one has that

Applying Lemma 1 with x = xk, y = yk, we obtain that

F (x) ≤ F (y) − ( 1

2η − L

2 )(cid:107)x − y(cid:107)2.

F (xk) ≤ F (yk) − ( 1

2η − L

2 )(cid:107)xk − yk(cid:107)2.

F (yk+1) ≤ F (xk) ≤ F (yk) ≤ F (xk−1).

Since η < 1
summary, for all k the following inequality holds:

L , it follows that F (xk) ≤ F (yk). Moreover, the update rule of APGnc guarantees that F (yk+1) ≤ F (xk). In

Combing further with the fact that F (xk), F (yk) ≥ inf F > −∞ for all k, we conclude that {F (xk)}, {F (yk)} converge
to the same limit F ∗, i.e.,

On the other hand, by induction we conclude from eq. (13) that for all k

lim
k→∞

F (xk) = lim
k→∞

F (yk) = F ∗.

F (yk) ≤ F (x0), F (xk) ≤ F (x0).

Combining with Assumption 1.1 that F has bounded sublevel set, we conclude that {xk} and {yk} are bounded and thus
have bounded limit points. Now combining eq. (12) and eq. (13) yields

which, after telescoping over k and letting k → ∞, becomes

2η − L
( 1

2 )(cid:107)yk − xk(cid:107)2 ≤ F (yk) − F (xk)

≤ F (yk) − F (yk+1),

∞
(cid:88)

k=1

( 1
2η − L

2 )(cid:107)yk − xk(cid:107)2 ≤ F (y1) − inf F < ∞.

This further implies that (cid:107)yk − xk(cid:107) → 0, and hence {xk} and {yk} share the same set of limit points Ω. Note that Ω is
closed (it is a set of limit points) and bounded, we conclude that Ω is compact in Rd.

By optimality condition of the proximal gradient step of APGnc, we obtain that

which further implies that

−∇f (yk) − 1
⇔ ∇f (xk) − ∇f (yk) − 1

(cid:124)

(cid:123)(cid:122)
uk

η (xk − yk) ∈ ∂g(xk)
∈ ∂F (xk),
η (xk − yk)
(cid:125)

(cid:107)uk(cid:107) = (cid:107)∇f (xk) − ∇f (yk) − 1
η )(cid:107)yk − xk(cid:107) → 0.

≤ (L + 1

η (xk − yk)(cid:107)

(cid:104)∇f (yk), xk − yk(cid:105) + 1

2η (cid:107)yk − xk(cid:107)2 + g(xk)

≤ (cid:104)∇f (yk), z(cid:48) − yk(cid:105) + 1

2η (cid:107)z(cid:48) − yk(cid:107)2 + g(z(cid:48)).

Consider any limit point z(cid:48) ∈ Ω, and w.l.o.g we write xk → z(cid:48), yk → z(cid:48) by restricting to a subsequence. By the deﬁnition
of the proximal map, the proximal gradient step of APGnc implies that

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Taking lim sup on both sides and note that xk − yk → 0, yk → z(cid:48), we obtain that lim supk→∞ g(xk) ≤ g(z(cid:48)). Since
g is lower semicontinuous and xk → z(cid:48), it follows that lim supk→∞ g(xk) ≥ g(z(cid:48)). Combining both inequalities, we
conclude that limk→∞ g(xk) = g(z(cid:48)). Note that the continuity of f yields limk→∞ f (xk) = f (z(cid:48)), we then conclude that
limk→∞ F (xk) = F (z(cid:48)). Since limk→∞ F (xk) = F ∗ by eq. (14), we conclude that

F (z(cid:48)) ≡ F ∗,

∀z(cid:48) ∈ Ω.

Hence, F remains constant on the compact set Ω. To this end, we have established xk → z(cid:48), F (xk) → F (z(cid:48)) and that
∂F (xk) (cid:51) uk → 0. Recall the deﬁnition of limiting sub-differential, we conclude that 0 ∈ ∂F (z(cid:48)) for all z(cid:48) ∈ Ω.

B. Proof of Theorem 2

Throughout the proof we assume that F (xk) (cid:54)= F ∗ for all k because otherwise the algorithm terminates and the conclusions
hold trivially. We also denote k0 as a sufﬁciently large positive integer.

Combining eq. (12) and eq. (13) yields that

Moreover, eq. (17) and eq. (18) imply that

F (xk+1) ≤ F (xk) − ( 1

2η − L

2 )(cid:107)yk+1 − xk+1(cid:107)2.

dist∂F (xk)(0) ≤ (L + 1

η )(cid:107)yk − xk(cid:107).

We have shown in Appendix A that F (xk) ↓ F ∗, and it is also clear that distΩ(xk) → 0. Thus, for any (cid:15), δ > 0 and all
k ≥ k0, we have

xk ∈ {x | distΩ(x) ≤ (cid:15), F ∗ < F (x) < F ∗ + δ}.

Since Ω is compact and F is constant on it, the uniformized KL property implies that for all k ≥ k0

Recall that rk := F (xk) − F ∗. Then eq. (23) is equivalent to

ϕ(cid:48)(F (xk) − F ∗)dist∂F (xk)(0) ≥ 1.

(20)

(21)

(22)

(23)

1 ≤ (cid:0)ϕ(cid:48) (rk) dist∂F (xk) (0)(cid:1)2
≤ (ϕ(cid:48) (rk))2 (cid:16) 1

(cid:17)2

(i)

(cid:107)yk − xk(cid:107)2

η + L
(cid:17)2
(cid:16) 1

η +L
2η − L

2

1

(ii)

≤ (ϕ(cid:48) (rk))2

[F (xk−1) − F (xk)]

≤ d1 (ϕ(cid:48) (rk))2 (rk−1 − rk) ,

where (i) is due to eq. (22), (ii) follows from eq. (21), and d1 =
ϕ(cid:48) (t) = ctθ−1. Thus the above inequality becomes

(cid:16) 1

η + L

(cid:17)2

(cid:16) 1
2η − L

2

/

(cid:17)

. Since ϕ (t) = c

θ tθ, we have that

It has been shown in (Frankel et al., 2015; Li & Lin, 2015) that sequence {rk} satisfying the above inductive property
converges to zero at different rates according to θ as stated in the theorem.

1 ≤ d1c2r2θ−2

k

(rk−1 − rk) .

(24)

C. Proof of Theorem 3

g non-convex, (cid:15)k = 0: In this setting, we ﬁrst prove the following inexact version of Lemma 1.
Lemma 2. Under Assumption 1.3. For any η > 0 and any x, y ∈ Rd such that x = proxηg(y − η(∇f (y) + e)), one has
that

F (x) ≤ F (y) + ( L

2 − 1

2η )(cid:107)x − y(cid:107)2 + (cid:107)x − y(cid:107)(cid:107)e(cid:107).

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Proof. By Assumption 1.3 we have that

Also, by the deﬁnition of proximal map, the proximal gradient step implies that

f (x) ≤ f (y) + (cid:104)x − y, ∇f (y)(cid:105) + L

2 (cid:107)x − y(cid:107)2.

g(x) + 1

2η (cid:107)x − y + η(∇f (y) + e)(cid:107)2 ≤ g(y) + 1

2η (cid:107)η(∇f (y) + e)(cid:107)2,

which, after simpliﬁcations becomes that

Combine the above two inequalities further gives that

g(x) ≤ g(y) − 1

2η (cid:107)x − y(cid:107)2 − (cid:104)x − y, (∇f (y) + e)(cid:105).

F (x) ≤ F (y) + ( L

2 − 1

2η )(cid:107)x − y(cid:107)2 + (cid:107)x − y(cid:107)(cid:107)e(cid:107).

Using Lemma 2 with x = xk, y = yk, e = ek and notice the fact that (cid:107)ek(cid:107) ≤ γ(cid:107)xk − yk(cid:107), we obtain that

F (xk) ≤ F (yk) + (γ + L

2 − 1

2η )(cid:107)xk − yk(cid:107)2.

Moreover, the optimality condition of the proximal gradient step with gradient error gives that By optimality condition of
the proximal gradient step of APGnc, we obtain that

which further implies that

∇f (xk) − ∇f (yk) − ek − 1

η (xk − yk) ∈ ∂F (xk),

dist∂F (xk)(0) ≤ (γ + L + 1

η )(cid:107)yk − xk(cid:107).

Notice that eq. (25) and eq. (26) are parallel to the key inequalities eq. (21) and eq. (22) in the analysis of exact APGnc.
Thus, by choosing η < 1
η + L + γ)2/( 1
2 − γ), all the statements in Theorem 1 remain
true and the convergence rates in Theorem 2 remain the same order with a worse constant.

2γ+L and redeﬁning d1 = ( 1

2η − L

g convex: We ﬁrst present the following lemma.
Lemma 3. For any x, v ∈ Rd, let u(cid:48) ∈ ∂(cid:15)g(x) such that ∇f (x) + u(cid:48) has minimal norm. Denote ξ := dist∂g(x)(u(cid:48)), then
we have

Proof. We observe the following

dist∂F (x)(0) ≤ dist∇f (x)+∂(cid:15)g(x)(0) + ξ.

dist∂F (x)(0) = min

(cid:107)∇f (x) + u(cid:107)

u∈∂g(x)

= min

u∈∂g(x)

(cid:107)∇f (x) + u(cid:48) + u − u(cid:48)(cid:107), ∀u(cid:48) ∈ ∂(cid:15)g(x)

≤ (cid:107)∇f (x) + u(cid:48)(cid:107) + min

(cid:107)u − u(cid:48)(cid:107), ∀u(cid:48) ∈ ∂(cid:15)g(x)

u∈∂g(x)

≤ min
u(cid:48)∈∂(cid:15)

g(x)(cid:107)∇f (x) + u(cid:48)(cid:107) + ξ

= dist∇f (x)+∂(cid:15)g(x)(0) + ξ.

Recall that we have two inexactness, i.e., xk = prox(cid:15)k
Lemma 2 and notice that (cid:15)k ≤ δ(cid:107)xk − yk(cid:107)2, we can obtain that

ηg(yk − η(∇f (yk) + ek)). Following a proof similar to that of

F (xk) ≤ F (yk) + (γ + L
≤ F (yk) + (γ(cid:48) + L

2 − 1
2 − 1

2η )(cid:107)xk − yk(cid:107)2 + (cid:15)k
2η )(cid:107)xk − yk(cid:107)2

(25)

(26)

(27)

(28)

(29)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

for some γ(cid:48) > γ > 0. Since g is convex, by Lemma 2 in (Schmidt et al., 2011) one can exhibit vk with (cid:107)vk(cid:107) ≤
such that

√

2η(cid:15)k

This implies that

1
η [yk − xk − η(∇f (yk) + ek) − vk] ∈ ∂(cid:15)k g(xk).

dist∇f (xk)+∂(cid:15)k g(xk)(0) ≤ (γ + 1

η + L)(cid:107)xk − yk(cid:107) +

(cid:113) 2(cid:15)k
η .

Apply Lemma 3 and notice that (cid:15)k ≤ δ(cid:107)xk − yk(cid:107)2, ξk ≤ λ(cid:107)xk − yk(cid:107), we obtain that

dist∂F (xk)(0) ≤ (γ(cid:48) +

+ L)(cid:107)xk − yk(cid:107)

1
η

for some γ(cid:48) > γ > 0. Now eq. (29) and eq. (30) are parallel to the key inequalities eq. (21) and eq. (22) in the analysis
of exact APGnc. Thus, by choosing η < 1
2 − γ(cid:48)), all the statements in
Theorem 1 remain true and the convergence rates in Theorem 2 remain the same order with a worse constant.

2γ(cid:48)+L and redeﬁning d1 = ( 1

η + L + γ(cid:48))2/( 1

2η − L

D. Proof of Theorem 4

We ﬁrst deﬁne the following quantities for the convenience of the proof.

ct = ct+1(1 + 1
k := E (cid:2)F (xt
Rt
¯xt+1
k = proxηg(xt

m ) + ηL2
k) + ct(cid:107)xt

2 , cm = 0,
k(cid:107)2(cid:3) ,
k − x0
k − η∇f (xt
k)).

Rt+1

k ≤ Rt

k +

L − 1
2η

(cid:16)

(cid:17)

E (cid:2)(cid:107)¯xt+1

k − xt

k(cid:107)2(cid:3) .

Note that ¯xt+1
implementation of the algorithm. Then it has been shown in the proof of Theorem 5 of (Reddi et al., 2016b) that

is a reference sequence introduced for the convenience of analysis, and is not being computed in the

k

Telescoping eq. (34) over t from t = 1 to t = m − 1, we obtain

E[F (xm

k )] ≤ E

k) + c1(cid:107)¯x1

k − x0

k(cid:107)2 +

(cid:34)
F (¯x1

(cid:17)

L − 1
2η

(cid:107)¯xt+1

k − xt

k(cid:107)2

.

(cid:35)

m−1
(cid:88)

(cid:16)

t=1

Following from eq. (31), a simple induction shows that ct ≤ ηL2m. Setting η < 1
eq. (35) further implies that

2L and recalling that F (yk) ≤ F (xm

k−1).,

E[F (yk+1)] ≤ E[F (xm

k )] ≤ E[F (¯x1

k)] + ηL2mE[(cid:107)¯x1

k − x0

k(cid:107)2].

Now telescoping eq. (34) again over t from t = 0 to t = m − 1 and applying eq. (36), we obtain

E[F (xm

k )] ≤ E[F (yk)] +

(L − 1

2η )E (cid:2)(cid:107)¯xt+1

k − xt

k(cid:107)2(cid:3) .

m−1
(cid:88)

t=0

Combining all the above facts, we conclude that for η < 1
2L

E[F (yk)] ≤ E[F (yk−1)] ≤ . . . ≤ F (y0).

Since E[F (·)] is bounded below, E[F (yk)] decreases to a ﬁnite limit, say, F ∗. Deﬁne rk = E [F (yk) − F ∗], and assume
rk > 0 for all k (since otherwise rk = 0 and the algorithm terminates). Applying the KŁ property with θ = 1/2, we obtain

Setting x = ¯x1

k, we further obtain

1

c (F (x) − F ∗)

1

2 ≤ dist∂F (x)(0).

1

c2 (F (¯x1

k) − F ∗) ≤ dist2

∂F (¯x1

k)(0) ≤

L + 1
η

(cid:107)¯x1

k − yk(cid:107)2,

(cid:16)

(cid:17)2

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

where the last inequality is due to eq. (33). Taking expectation over both sides and using eq. (36), we obtain

1

c2 E[F (xm

k ) − F ∗] − ηL2m

c2 E (cid:2)(cid:107)¯x1

k − x0

k(cid:107)2(cid:3) ≤

(cid:16)

L + 1
η

(cid:17)2

E (cid:2)(cid:107)¯x1

k − yk(cid:107)2(cid:3) .

Noting that x0

k = yk and EF (yk+1) ≤ EF (xm

k ), we then rearrange the above inequality and obtain

1

c2 E[F (yk+1) − F ∗] ≤ 1

c2 E[F (xm

k ) − F ∗] ≤

L + 1
η

(cid:20)(cid:16)

(cid:17)2

(cid:21)

+ ηL2m
c2

E (cid:2)(cid:107)¯x1

k − yk(cid:107)2(cid:3)

≤

(L+ 1

+ ηL2m
c2

η )2
1
2η −L

(E[F (yk)] − E[F (yk+1)]) ,

which can be further rewritten as

where d =

η )2
c2(L+ 1
1
2η −L

+ηL2m

. Then a simple induction yields that

rk+1 ≤ d (rk − rk+1) ,

rk+1 ≤

(cid:17)k+1

(cid:16) d
d+1

(F (y0) − F ∗) .

E. Proof of Theorem 5

We ﬁrst introduce some auxiliary lemmas.
Lemma 4. Consider the convex function g and x, y ∈ Rd such that y = prox(cid:15)
(cid:107)i(cid:107) ≤

2η(cid:15) that satisﬁes the following inequality for all z ∈ Rd.

√

ηg(x) for some (cid:15) > 0. Then, there exists

g(y) + 1

2η (cid:107)y − x(cid:107)2 ≤ g(z) + 1

2η (cid:107)z − x(cid:107)2 − 1

2η (cid:107)y − z(cid:107)2 + (cid:104)y − z, 1

η i(cid:105) + (cid:15).

Proof. By Lemma 2 in (Schmidt et al., 2011), there exists (cid:107)i(cid:107) ≤

2η(cid:15) such that

√

Then, the deﬁnition of (cid:15)-subdifferential implies that

1
η (x − y − i) ∈ ∂(cid:15)g(y).

g(z) − g(y) ≥ (cid:104)z − y, ∂(cid:15)g(y)(cid:105) − (cid:15) = (cid:104)z − y, 1

η (x − y − i)(cid:105) − (cid:15), ∀ z ∈ Rd.

The desired result follows by rearranging the above inequality.

Lemma 5. Consider the convex function g and x, y, d ∈ Rd such that y = prox(cid:15)
exists (cid:107)i(cid:107) ≤

2η(cid:15) that satisﬁes the following inequality for all z ∈ Rd.

√

ηg(x − ηd) for some (cid:15) > 0. Then, there

g(y) = (cid:104)y − z, d − 1

η i(cid:105) ≤ g(z) + 1

2η

(cid:2)(cid:107)z − x(cid:107)2 − (cid:107)y − z(cid:107)2 − (cid:107)y − x(cid:107)2(cid:3) + (cid:15).

Proof. By Lemma 4, we obtain the following inequality for all z ∈ Rd.

g(y) + (cid:104)y − x, d(cid:105) + 1

2η (cid:107)y − x(cid:107)2 +
2η (cid:107)z − x + ηd(cid:107)2 − 1

(cid:107)d(cid:107)2

η
2

≤ g(z) + 1

= g(z) + (cid:104)z − x, d(cid:105) 1

2η (cid:107)z − x(cid:107)2 + η

2 (cid:107)d(cid:107)2 − 1

2η (cid:107)y − z(cid:107)2 + (cid:104)y − z, 1

η i(cid:105) + (cid:15)
2η (cid:107)y − z(cid:107)2 + (cid:104)y − z, 1

η i(cid:105) + (cid:15).

The desired result follows by rearranging the above inequality.

Lemma 6. Consider the convex function g and x, y, d ∈ Rd such that y = prox(cid:15)
exists (cid:107)i(cid:107) ≤

2η(cid:15) that satisﬁes the following inequality for all z ∈ Rd.

√

ηg(x − ηd) for some (cid:15) > 0. Then, there

F (y) + (cid:104)y − z, d − 1

η i − ∇f (x)(cid:105) ≤ F (z) +

(cid:107)y − x(cid:107)2 +

2 + 1

2η

(cid:107)z − x(cid:107)2 − 1

2η (cid:107)y − z(cid:107)2 + (cid:15).

(51)

(cid:16) L

2 − 1

2η

(cid:17)

(cid:16) L

(cid:17)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Proof. By Lipschitz continuity of ∇f , we obtain

f (y) ≤ f (x) + (cid:104)∇f (x), y − x(cid:105) + L
f (x) ≤ f (z) + (cid:104)∇f (x), x − z(cid:105) + L

2 (cid:107)y − x(cid:107)2,
2 (cid:107)x − z(cid:107)2.

Adding the above inequalities together yields

f (y) ≤ f (z) + (cid:104)∇f (x), y − z(cid:105) + L
2

(cid:2)(cid:107)y − x(cid:107)2 + (cid:107)z − x(cid:107)2(cid:3) .

Combining with Lemma 5, we then obtain the desired result.

Recall the reference sequence ¯xt+1
d = ∇f (xt

k − η∇f (xt
k) and taking expectation on both sides, we obtain

k = proxηg(xt

k)). Applying Lemma 6 with (cid:15) = 0, y = ¯xt+1

k

, z = xt

k, and

E[F (¯xt+1

)] ≤ E

k

F (xt

k) +

2 − 1

2η

(cid:107)¯xt+1
k

) − xt

k(cid:107)2 − 1

2η (cid:107)¯xt+1

k − xt

(cid:104)

(cid:16) L

(cid:17)

k(cid:107)2(cid:105)

.

Similarly, applying Lemma 6 with (cid:15) = (cid:15)t

k, y = xt+1

k

, z = ¯xt+1

, d = vt

k

k and taking expectation on both sides, we obtain

E[F (xt+1

k

(cid:104)

)] ≤ E
(cid:16) L

+

F (¯xt+1
k
(cid:17)

2 − 1

2η

) + (cid:104)xt+1

k − ¯xt+1

k

(cid:107)xt+1

k − xt

k(cid:107)2 +

, ∇f (cid:0)xt
(cid:1) − vt
k
(cid:17)
(cid:16) L

2 + 1

2η

k + 1

η ik(cid:105)

(cid:107)¯xt+1

k − xt

k(cid:107)2 − 1

2η (cid:107)¯xt+1

k − xt+1

k (cid:107)2 + (cid:15)t

k

(cid:105)

.

(56)

Adding eq. (55) and eq. (56) together yields

E[F (xt+1

)] ≤ E

k

F (xt

k) +

L − 1
2η

(cid:107)¯xt+1

k − xt

k(cid:107)2 +

2 − 1

2η

(cid:107)xt+1

k − xt

k(cid:107)2 − 1

2η (cid:107)¯xt+1

k − xt+1

k (cid:107)2 + T

(57)

(cid:104)

(cid:16)

(cid:17)

(cid:16) L

(cid:17)

(cid:105)

where T = (cid:104)xt+1

k − ¯xt+1

k

, ∇f (xt

k) − vt

k + ik

η (cid:105) + (cid:15)t

E[T ] ≤ 1
2η

E (cid:2)(cid:107)xt+1

k − xt+1

≤ 1
2η
≤ 1
2η

E (cid:2)(cid:107)xt+1
E (cid:2)(cid:107)xt+1

k − xt+1
k − xt+1

(cid:104)

E

k. Now we bound E[T ] as follows.
η (cid:107)2(cid:105)
k + ik
k(cid:107)2(cid:3) + ηE
k(cid:107)2(cid:3) + 3(cid:15)t
k.

k (cid:107)2(cid:3) + η
(cid:107)∇f (cid:0)xt
k (cid:107)2(cid:3) + ηE (cid:2)(cid:107)∇f (cid:0)xt
k (cid:107)2(cid:3) + ηE (cid:2)(cid:107)∇f (cid:0)xt

(cid:1) − vt
(cid:1) − vt

(cid:1) − vt

k

k

k

2

+ (cid:15)t
k
(cid:104)

η (cid:107)2(cid:105)
(cid:107) ik

+ (cid:15)t
k

By Lemma 3 of (Reddi et al., 2016b), it holds that E (cid:2)(cid:107)∇f (xt
above inequality, we further obtain that

k) − vt

k(cid:107)2(cid:3) ≤ L2E (cid:2)(cid:107)xt

k − x0

k(cid:107)2(cid:3) . Combining with the

E[T ] ≤ 1
2η

E (cid:2)(cid:107)xt+1

k − xt+1

k (cid:107)2(cid:3) + ηL2E (cid:2)(cid:107)xt

k − x0

k(cid:107)2(cid:3) + 3(cid:15)t
k.

Substituting the above result into eq. (57), we obtain

E[F (xt+1

)] ≤ E

k

F (xt

k) +

L − 1
2η

(cid:107)¯xt+1

k − xt

k(cid:107)2 +

(cid:104)

(cid:16)

(cid:17)

(cid:16) L

2 − 1

2η

(cid:17)

(cid:107)xt+1

k − xt

k(cid:107)2 + ηL2(cid:107)xt

k − x0

k(cid:107)2 + 3(cid:15)t
k

(cid:105)

.

(62)

Recalling that Rt
Rt+1
k

as

k := E (cid:2)F (xt

k) + ct(cid:107)xt

k − x0

k(cid:107)2(cid:3) , where ct = ηL2 (1+β)m−t−1

β

with β > 0. Then, we can upper bound

k =E (cid:2)F (xt+1
Rt+1
=E (cid:2)F (xt+1

k

k
(cid:104)
F (xt+1
k
(cid:104)
F (xt

≤E

≤E

(cid:16)

) + ct+1(cid:107)xt+1
(cid:0)(cid:107)xt+1
) + ct+1
(cid:16)

k − xt
k − xt
(cid:17)

k − x0
k + xt
k(cid:107)2 + (cid:107)xt
k − xt

(cid:107)xt+1

) + ct+1

1 + 1
β
(cid:17)

k) +

(cid:107)¯xt+1
+ (cid:2)ct+1 (1 + β) + ηL2(cid:3) (cid:107)xt

L − 1
2η

k − xt
k − x0

k(cid:107)2 +
k(cid:107)2 + 3(cid:15)t
k

k − xt

k(cid:107)2 + 2(cid:104)xt+1

k(cid:107)2(cid:3)
k − x0
k(cid:107)2 + ct+1 (1 + β) (cid:107)xt
(cid:104)
ct+1
(cid:3) .

1 + 1
β

+ L

(cid:16)

(cid:17)

k(cid:105)(cid:1)(cid:3)

k, xt
k − x0
(cid:105)

k − x0
k(cid:107)2(cid:105)
(cid:107)xt+1

2 − 1

2η

k − xt

k(cid:107)2

(52)

(53)

(54)

(55)

(58)

(59)

(60)

(61)

(63)

(64)

(65)

(66)

(67)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Setting β = 1/m in ct and observe that

which further implies that

ct = ηL2 (1+β)m−t−1

β

= ηL2m (cid:0)(1 + β)m−t − 1(cid:1) ≤ ηL2m (e − 1) ≤ 2ηL2m,

(cid:16)

(cid:17)

ct+1

1 + 1
β

+ L

2 ≤ 2ηL2m(1 + m) ≤ 4ηL2m2 + L

2 = 4ρLm2 + L

2 ≤ 1
2η .

Also note that ct = ct+1(1 + β) + ηL2. Collecting all these facts, Rt+1

can be further upper bounded by

k

Rt+1

k ≤ Rt

k + E

L − 1
2η

(cid:107)¯xt+1

k − xt

k(cid:107)2 + 3(cid:15)t
k

(cid:104)(cid:16)

(cid:17)

(cid:105)

.

Telescoping eq. (70) from t = 1 to t = m − 1, we obtain

(cid:34)

E[F (xm

k )] ≤ E

F (¯x1

k) + c1(cid:107)¯x1

k − x0

k(cid:107)2 +

m−1
(cid:88)

(cid:16)

t=1

(cid:17)

L − 1
2η

(cid:107)¯xt+1

k − xt

k(cid:107)2 +

(cid:35)

3(cid:15)t
k

.

m−1
(cid:88)

t=1

Again, telescoping eq. (70) from t = 0 to t = m − 1 we obtain

E[F (yk+1)] ≤ E[F (xm

k )] ≤ E[F (yk)] +

(L − 1

2η )E (cid:2)(cid:107)¯xt+1

k − xt

k(cid:107)2(cid:3) + 3

E (cid:2)(cid:15)t

(cid:3) .

k

(72)

m−1
(cid:88)

t=0

m−1
(cid:88)

t=0

m−1
(cid:80)
t=0

Assume

that 3

m−1
(cid:80)
t=0

k − xt
m−1
(cid:80)
t=0

E (cid:2)(cid:107)¯xt+1

k(cid:107)2(cid:3) > 0, because otherwise the algorithm is terminated. Assume that there exists α > 0 such

E [(cid:15)t

k] ≤ α

E (cid:2)(cid:107)¯xt+1

k − xt

k(cid:107)2(cid:3) and 1

2η − L − α > 0. Then eq. (72) further implies that

E[F (yk+1)] ≤ E[F (xm

k )] ≤ E[F (yk)] +

(L − 1

2η + α)E (cid:2)(cid:107)¯xt+1

k − xt

k(cid:107)2(cid:3) .

(73)

m−1
(cid:88)

t=0

That is, we have E[F (yk)] ≤ E[F (yk−1)] ≤ . . . ≤ F (y0), and hence E[F (yk)] ↓ F ∗. We can further upper bound
eq. (71) as

E[F (xm

k )] ≤E

F (¯x1

k) + c1(cid:107)¯x1

k − x0

k(cid:107)2 +

m−1
(cid:88)

(cid:16)

(cid:17)

L − 1
2η

(cid:107)¯xt+1

k − xt

k(cid:107)2 +

(cid:34)

(cid:34)

(cid:34)
F (¯x1

≤E

k) + c1(cid:107)¯x1

k − x0

k(cid:107)2 −

L − 1
2η

(cid:107)¯x1

k − x0

k(cid:107)2 +

L − 1
2η

(cid:107)¯xt+1

k − xt

k(cid:107)2 +

t=1

(cid:16)

(cid:17)

(cid:35)

3(cid:15)t
k

m−1
(cid:88)

t=0

≤E

F (¯x1

k) +

(cid:16)
c1 + 1
2η

(cid:17)

(cid:107)¯x1

k − x0

k(cid:107)2 +

m−1
(cid:88)

(cid:16)

L − 1

2η + α

(cid:107)¯xt+1

k − xt

k(cid:107)2

(cid:35)

≤E (cid:2)F (¯x1

k)(cid:3) + E

(cid:104)(cid:16)

2ηL2m + 1
2η

(cid:17)

t=0
k − x0

k(cid:107)2(cid:105)

.

(cid:107)¯x1

(cid:35)

3(cid:15)t
k

m−1
(cid:88)

t=1

m−1
(cid:88)

(cid:16)

(cid:17)

t=0

(cid:17)

Deﬁne rk = E [F (yk) − F ∗], and suppose rk > 0 for all k (otherwise the algorithm terminates in ﬁnite steps). Applying
the KŁ condition with θ = 1/2, we obtain

Setting x = ¯x1

k, we obtain

1

c (F (x) − F ∗)

1

2 ≤ dist∂F (x)(0).

1
c2 (F (¯x1

k) − F ∗) ≤ dist2

∂F (¯x1

k)(0) ≤

(cid:16)

L + 1
η

(cid:17)2

(cid:107)¯x1

k − yk(cid:107)2.

(68)

(69)

(70)

(71)

(74)

(75)

(76)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Taking expectation on both sides and using the result from eq. (74), we obtain

E[F (xm

k ) − F ∗] −

2ηL2m+ 1
2η
c2

E (cid:2)(cid:107) ¯xk

1 − x0

k(cid:107)2(cid:3) ≤

(cid:16)

L + 1
η

(cid:17)2

E (cid:2)(cid:107)¯x1

k − yk(cid:107)2(cid:3) .

1
c2

Note that x0

k = yk. Then rearranging the above inequality yields
(cid:20)(cid:16)

1

c2 E[F (yk+1) − F ∗] ≤

1
c2 E[F (xm

k ) − F ∗] ≤

L + 1
η

(cid:17)2

+

2ηL2m+ 1
2η
c2

(cid:21)

E (cid:2)(cid:107)¯x1

k − yk(cid:107)2(cid:3)

which can be rewritten as rk+1 ≤ d (rk − rk+1) with d =

. Then, induction yields that

(L+ 1

≤

+

2ηL2m+ 1
η )2
2η
c2
1
2η −L−α

c2(L+ 1

+2ηL2m+ 1
2η

η )2
1
2η −L−α

(E[F (yk)] − E[F (yk+1)]) ,

rk+1 ≤

d
d + 1

rk ≤

(cid:18) d

(cid:19)k+1

d + 1

(F (y0) − F ∗) .

(77)

(78)

(79)

(80)

