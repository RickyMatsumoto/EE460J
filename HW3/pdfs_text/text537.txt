Orthogonalized ALS: A Theoretically Principled Tensor Decomposition
Algorithm for Practical Use

Vatsal Sharan 1 Gregory Valiant 1

Abstract

The popular Alternating Least Squares (ALS) al-
gorithm for tensor decomposition is efﬁcient and
easy to implement, but often converges to poor
local optima—particularly when the weights of
the factors are non-uniform. We propose a mod-
iﬁcation of the ALS approach that is as efﬁ-
cient as standard ALS, but provably recovers
the true factors with random initialization un-
der standard incoherence assumptions on the fac-
tors of the tensor. We demonstrate the signif-
icant practical superiority of our approach over
traditional ALS for a variety of tasks on syn-
thetic data—including tensor factorization on ex-
act, noisy and over-complete tensors, as well as
tensor completion—and for computing word em-
beddings from a third-order word tri-occurrence
tensor.

1. Introduction

From a theoretical perspective, tensor methods have be-
come an incredibly useful and versatile tool for learning
a wide array of popular models, including topic model-
ing (Anandkumar et al., 2012), mixtures of Gaussians (Ge
et al., 2015), community detection (Anandkumar et al.,
2014a),
learning graphical models with guarantees via
the method of moments (Anandkumar et al., 2014b; Cha-
ganty & Liang, 2014) and reinforcement learning (Az-
izzadenesheli et al., 2016). The key property of ten-
sors that enables these applications is that tensors have a
unique decomposition (decomposition here refers to the
most commonly used CANDECOMP/PARAFAC or CP
decomposition), under mild conditions on the factor ma-
trices (Kruskal, 1977); for example, tensors have a unique
decomposition whenever the factor matrices are full rank.
As tensor methods naturally model three-way (or higher-
order) relationships, it is not too optimistic to hope that

1Stanford University, USA. Correspondence to: Vatsal Sharan

<vsharan@stanford.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

their practical utility will only increase, with the rise of
multi-modal measurements (e.g. measurements taken by
“Internet of Things” devices) and the numerous practical
applications involving high order dependencies, such as
those encountered in natural language processing or ge-
nomic settings.
In fact, we are already seeing exciting
applications of tensor methods for analysis of high-order
spatiotemporal data (Yu & Liu, 2016), health data analysis
(Wang et al., 2015a) and bioinformatics (Colombo & Vlas-
sis, 2015). Nevertheless, to truly realize the practical im-
pact that the current theory of tensor methods portends, we
require better algorithms for computing decompositions—
practically efﬁcient algorithms that are both capable of
scaling to large (and possibly sparse) tensors, and are ro-
bust to noise and deviations from the idealized “low-rank”
assumptions.

As tensor decomposition is NP-Hard in the worst-case
(Hillar & Lim, 2013; H˚astad, 1990), one cannot hope for
algorithms which always produce the correct factorization.
Despite this worst-case impossibility, accurate decompo-
sitions can be efﬁciently computed in many practical set-
tings. Early work from the 1970’s (Leurgans et al., 1993;
Harshman, 1970) established a simple algorithm for com-
puting the tensor decomposition (in the noiseless setting)
provided that the factor matrices are full rank. This ap-
proach, based on an eigendecomposition, is very sensitive
to noise in the tensor (as we also show in our experiments),
and does not scale well for large, sparse tensors.

Since this early work, much progress has been made. Nev-
ertheless, many of the tensor decomposition algorithms
hitherto proposed and employed have strong provable suc-
cess guarantees but are computationally expensive (though
still polynomial time)—either requiring an expensive ini-
tialization phase, being unable to leverage the sparsity of
the input tensor, or not being efﬁciently parallelizable. On
the other hand, there are also approaches which are efﬁ-
cient to implement, but which fail to compute an accurate
decomposition in many natural settings. The Alternating
Least Squares (ALS) algorithm (either with random initial-
ization or more complicated initializations) falls in this lat-
ter category and is, by far, the most widely employed de-
composition algorithm despite its often poor performance

Orthgonalized ALS for Tensor Decomposition

and propensity for getting stuck in local optima (which we
demonstrate on both synthetic data and real NLP data).

In this paper we propose an alternative decomposition algo-
rithm, “Orthogonalized Alternating Least Squares” (Orth-
ALS) which has strong theoretical guarantees, and seems to
signiﬁcantly outperform the most commonly used existing
approaches in practice on both real and synthetic data, for a
number of tasks related to tensor decomposition. This algo-
rithm is a simple modiﬁcation of the ALS algorithm to pe-
riodically “orthogonalize” the estimates of the factors. In-
tuitively, this periodic orthogonalization prevents multiple
recovered factors from “chasing after” the same true fac-
tors, allowing for the avoidance of local optima and more
rapid convergence to the true factors.

From the practical side, our algorithm enjoys all the bene-
ﬁts of standard ALS, namely simplicity and computational
efﬁciency/scalability, particularly for very large yet sparse
tensors, and noise robustness. Additionally, the speed of
convergence and quality of the recovered factors are sub-
stantially better than standard ALS, even when ALS is ini-
tialized using the more expensive SVD initialization. As
we show, on synthetic low-rank tensors, our algorithm con-
sistently recovers the true factors, while standard ALS of-
ten falters in local optima and fails both in recovering the
true factors and in recovering an accurate low-rank approx-
imation to the original tensor. We also applied Orth-ALS to
a large 3-tensor of word co-occurrences to compute “word
embeddings”.1 The embedding produced by our Orth-ALS
algorithm is signiﬁcantly better than that produced by stan-
dard ALS, as we quantify via a near 30% better perfor-
mance of the resulting word embeddings across standard
NLP datasets that test the ability of the embeddings to an-
swer basic analogy tasks (i.e. “puppy is to dog as kitten
?”) and semantic word-similarity tasks. Together,
is to
these results support our optimism that with better decom-
position algorithms, tensor methods will become an indis-
pensable, widely-used data analysis tool in the near future.

Beyond the practical beneﬁts of Orth-ALS, we also con-
sider its theoretical properties. We show that Orth-ALS
provably recovers all factors under random initialization
for worst-case tensors as long as the tensor satisﬁes an in-
coherence property (which translates to the factors of the
tensors having small correlation with each other), which is
satisﬁed by random tensors with rank k = o(d0.25) where
d is the dimension of the tensor. This requirement that k =
o(d0.25) is signiﬁcantly worse than the best known prov-
able recovery guarantees for polynomial-time algorithms

1Word embeddings are vector representations of words, which
can then be used as features for higher-level machine learning.
Word embeddings have rapidly become the backbone of many
downstream natural language processing tasks (see e.g. (Mikolov
et al., 2013b)).

on random tensors—the recent work Ma et al. (2016) suc-
ceeds even in the over-complete setting with k = o(d1.5).
Nevertheless, our experiments support our belief that this
shortcoming is more a property of our analysis than the al-
gorithm itself. Additionally, for many practical settings,
particularly natural language tasks, the rank of the recov-
ered tensor is typically signiﬁcantly sublinear in the dimen-
sionality of the space, and the beneﬁts of an extremely ef-
ﬁcient and simple algorithm might outweigh limitations on
the required rank for provable recovery.

Finally, as a consequence of our analysis technique for
proving convergence of Orth-ALS, we also improve the
known guarantees for another popular tensor decomposi-
tion algorithm—the tensor power method. We show that
the tensor power method with random initialization con-
verges to one of the factors with small residual error for
rank k = o(d), where d is the dimension. We also show that
the convergence rate is quadratic in the dimension. Anand-
kumar et al. (2014c) had previously shown local conver-
gence of the tensor power method with a linear conver-
gence rate (and also showed global convergence via a SVD-
based initialization scheme, obtaining the ﬁrst guarantees
for the tensor power method in non-orthogonal settings).
Our new results, particularly global convergence from ran-
dom initialization, provide some deeper insights into the
behavior of this popular algorithm.

The rest of the paper is organized as follows—Section 2
states the notation. In Section 3 we discuss related work.
Section 4 introduces Orth-ALS, and states the convergence
guarantees. We state our convergence results for the tensor
power method in Section 4.2. The experimental results, on
both synthetic data and the NLP tasks are discussed in Sec-
tion 5. Proof details have been deferred to the Appendix.

2. Notation

We state our algorithm and results for 3rd order tensors,
and believe the algorithm and analysis techniques should
extend easily to higher dimensions. Given a 3rd order ten-
sor T ∈ Rd×d×d our task is to decompose the tensor into its
factor matrices A, B and C: T = (cid:80)
i∈[k] wiAi ⊗ Bi ⊗ Ci,
where Ai denotes the ith column of a matrix A. Here
wi ∈ R, Ai, Bi, Ci ∈ Rd and ⊗ denotes the tensor prod-
if a, b, c ∈ Rd then a ⊗ b ⊗ c ∈ Rd×d×d and
uct:
(a ⊗ b ⊗ c)ijk = aibjck. We will refer to wi as the weight
of the factor {Ai, Bi, Ci}. This is also known as CP de-
composition. We refer to the dimension of the tensor by d
and denote its rank by k. We refer to different dimensions
of a tensor as the modes of the tensor.

We denote T(n) as the mode n matricization of the tensor,
which is the ﬂattening of the tensor along the nth direction
obtained by stacking all the matrix slices together. For ex-
ample T(1) denotes ﬂattening of a tensor T ∈ Rn×m×p to

Orthgonalized ALS for Tensor Decomposition

a (n × mp) matrix. We denote the Khatri-Rao product of
two matrices A and B as (A (cid:12) B)i = (Ai ⊗ Bi)(1), where
(Ai ⊗ Bi)(1) denotes the ﬂattening of the matrix Ai ⊗ Bi
into a row vector. For any tensor T and vectors a, b, c, we
also deﬁne T (a, b, c) = (cid:80)
i,j,k Tijkaibjck. Throughout,
we say f (n) = ˜O(g(n)) if f (n) = O(g(n)) up to poly-
logarithmic factors.

Though all algorithms in the paper extend to asymmetric
tensors, we prove convergence results under the symmet-
ric setting where A = B = C. Similar to other works
(Tang & Shah, 2015; Anandkumar et al., 2014c; Ma et al.,
2016), our guarantees depend on the incoherence of the
factor matrices (cmax), deﬁned to be the maximum cor-
relation in absolute value between any two factors, i.e.
cmax = maxi(cid:54)=j |AT
i Aj|. This serves as a natural assump-
tion to simplify the problem as it is NP-Hard in the worst
case. Also, tensors with randomly drawn factors satisfy
cmax ≤ ˜O(1/
3. Background and Related Work

d), and our results hold for such tensors.

√

We begin the section with a brief discussion of related work
on tensor decomposition. We then review the ALS algo-
rithm and the tensor power method and discuss their basic
properties. Our proposed tensor decomposition algorithm,
Orth-ALS, builds on these algorithms.

3.1. Related Work on Tensor Decomposition

Though it is not possible for us to do justice to the substan-
tial body of work on tensor decomposition, we will review
three families of algorithms which are distinct from alter-
nating minimization approaches such as ALS and the ten-
sor power method. Many algorithms have been proposed
for guaranteed decomposition of orthogonal tensors, we
refer the reader to Anandkumar et al. (2014b); Kolda &
Mayo (2011); Comon et al. (2009); Zhang & Golub (2001).
However, obtaining guaranteed recovery of non-orthogonal
tensors using algorithms for orthogonal tensors requires
converting the tensor into an orthogonal form (known as
whitening) which is ill conditioned in high dimensions (Le
et al., 2011; Souloumiac, 2009), and is computationally
the most expensive step (Huang et al., 2013). Another
very interesting line of work on tensor decompositions is
to use simultaneous diagonalization and higher order SVD
(Colombo & Vlassis, 2016; Kuleshov et al., 2015; De Lath-
auwer, 2006) but these are not as computationally efﬁcient
as alternating minimization2. Recently, there has been in-

2De Lathauwer (2006) prove unique recovery under very gen-
eral conditions, but their algorithm is quite complex and requires
solving a linear system of size O(d4), which is prohibitive for
large tensors. We ran the simultaneous diagonalization algorithm
of Kuleshov et al. (2015) on a dimension 100, rank 30 tensor; and
the algorithm needed around 30 minutes to run, whereas Orth-
ALS converges in less than 5 seconds.

triguing work on provably decomposing random tensors
using the sum-of-squares approach (Ma et al., 2016; Hop-
kins et al., 2016; Tang & Shah, 2015; Ge & Ma, 2015).
Ma et al. (2016) show that a sum-of-squares based relax-
ation can decompose highly overcomplete random tensors
of rank up to o(d1.5). Though these results establish the
polynomial learnability of the problem, they are unfortu-
nately not practical.

Very recently, there has been exciting work on scalable ten-
sor decomposition algorithms using ideas such as sketch-
ing (Song et al., 2016; Wang et al., 2015b) and contraction
of tensor problems to matrix problems (Shah et al., 2015).
Also worth noting are recent approaches to speedup ALS
via sampling and randomized least squares (Battaglino
et al., 2017; Cheng et al., 2016; Papalexakis et al., 2012).

3.2. Alternating Least Squares (ALS)

ALS is the most widely used algorithm for tensor decompo-
sition and has been described as the “workhorse” for tensor
decomposition (Kolda & Bader, 2009). The algorithm is
conceptually very simple: if the goal is to recover a rank-k
tensor, ALS maintains a rank-k decomposition speciﬁed by
three sets of d × k dimensional matrices { ˆA, ˆB, ˆC} corre-
sponding to the three modes of the tensor. ALS will it-
eratively ﬁx two of the three modes, say ˆA and ˆB, and
then update ˆC by solving a least-squared regression prob-
lem to ﬁnd the best approximation to the underlying tensor
T having factors ˆA and ˆB in the ﬁrst two modes, namely
ˆCnew = arg minC(cid:48) (cid:107)T − ˆA⊗ ˆB ⊗ ˆC (cid:48)(cid:107)2. ALS will then con-
tinue to iteratively ﬁx two of the three modes, and update
the other mode via solving the associated least-squares re-
gression problem. These updates continue until some stop-
ping condition is satisﬁed—typically when the squared er-
ror of the approximation is no longer decreasing, or when a
ﬁxed number of iterations have elapsed. The factors used in
ALS are either chosen uniformly at random, or via a more
expensive initialization scheme such as SVD based initial-
ization (Anandkumar et al., 2014c).
In the SVD based
scheme, the factors are initialized to be the singular vec-
tors of a random projection of the tensor onto a matrix.

The main advantages of the ALS approach, which have led
to its widespread use in practice are its conceptual simplic-
ity, noise robustness and computational efﬁciency given its
graceful handling of sparse tensors and ease of paralleliza-
tion. There are several publicly available optimized pack-
ages implementing ALS, such as Kossaiﬁ et al. (2016);
Vervliet et al.; Bader et al. (2012); Bader & Kolda (2007);
Smith & Karypis; Huang et al. (2014); Kang et al. (2012).

Despite the advantages, ALS does not have any global
convergence guarantees and can get stuck in local optima
(Comon et al., 2009; Kolda & Bader, 2009), even under
very realistic settings. For example, consider a setting
where the weights wi for the factors {Ai, Bi, Ci} decay

Orthgonalized ALS for Tensor Decomposition

according to a power-law, hence the ﬁrst few factors have
much larger weight than the others. As we show in the ex-
periments (see Fig. 2), ALS fails to recover the low-weight
factors. Intuitively, this is because multiple recovered fac-
tors will be chasing after the same high weight factor, lead-
ing to a bad local optima.

3.3. Tensor Power Method

The tensor power method is a special case of ALS that only
computes a rank-1 approximation. The procedure is then
repeated multiple times to recover different factors. The
factors recovered in different iterations of the algorithm are
then clustered to determine the set of unique factors. Dif-
ferent initialization strategies have been proposed for the
tensor power method. Anandkumar et al. (2014c) showed
that the tensor power method converges locally (i.e.
for
a suitably chosen initialization) for random tensors with
rank o(d1.5). They also showed that a SVD based ini-
tialization strategy gives good starting points and used this
to prove global convergence for random tensors with rank
O(d). However, the SVD based initialization strategy can
be computationally expensive, and our experiments suggest
that even SVD initialization fails in the setting where the
weights decay according to a power-law (see Fig. 2).

In this work, we prove global convergence guarantees with
random initializations for the tensor power method for ran-
dom and worst-case incoherent tensors. Our results also
demonstrate how, with random initialization, the tensor
power method converges to the factor having the largest
product of weight times the correlation of the factor with
the random initialization vector. This explains the difﬁculty
of using random initialization to recover factors with small
weight. For example, if one factor has weight less than a
1/c fraction of the weight of, say, the heaviest k/2 factors,
then with high probability we would require at least kΘ(c2)
random initializations to recover this factor. This is because
the correlation between random vectors in high dimensions
is approximately distributed as a Normal random variable
and if k/2+1 samples are drawn from the standard Normal
distribution, the probability that one particular sample is at
least a factor of c larger than the other k/2 other samples
scales as roughly k−Θ(c2).

4. The Algorithm: Orthogonalized

Alternating Least Squares (Orth-ALS)

In this section we introduce Orth-ALS, which combines
the computational beneﬁts of standard ALS and the prov-
able recovery of the tensor power method, while avoiding
the difﬁculties faced by both when factors have different
weights. Orth-ALS is a simple modiﬁcation of standard
ALS that adds an orthogonalization step before each set of
ALS steps. We describe the algorithm below. Note that
steps 4-6 are just the solution to the least squares problem

expressed in compact tensor notation, for instance step 4
can be equivalently stated as X = arg minC(cid:48) (cid:107)T − ˆA ⊗
ˆB ⊗ ˆC (cid:48)(cid:107)2. Similarly, step 9 is the least squares estimate of
the weight wi of each rank-1 component ˆAi ⊗ ˆBi ⊗ ˆCi.
Algorithm 1 Orthogonalized ALS (Orth-ALS)

Input: Tensor T ∈ Rd×d×d, number of iterations N .
1: Initialize each column of ˆA, ˆB and ˆC ∈ Rd×k uni-

formly from the unit sphere

2: for t = 1 : N do
3:

Find QR decomposition of ˆA, set ˆA = Q. Orthogo-
nalize ˆB and ˆC analogously.

6:

4: X = T(1)( ˆC (cid:12) ˆB)
Y = T(2)( ˆC (cid:12) ˆA)
5:
Z = T(3)( ˆB (cid:12) ˆA)
Normalize X, Y, Z and store results in ˆA, ˆB, ˆC

7:
8: end for
9: Estimate weights ˆwi = T ( ˆAi, ˆBi, ˆCi), ∀ i ∈ [k].
10: return ˆA, ˆB, ˆC, ˆw

To get some intuition for why the orthogonalization makes
sense, let us consider the more intuitive matrix factorization
problem, where the goal is to compute the eigenvectors of
a matrix. Subspace iteration is a straightforward extension
of the matrix power method to recover all eigenvectors at
once. In subspace iteration, the matrix of eigenvector es-
timates is orthogonalized before each power method step
(by projecting the second eigenvector estimate orthogonal
to the ﬁrst one and so on), because otherwise all the vectors
would converge to the dominant eigenvector. For the case
of tensors, the vectors would not all necessarily converge
to the dominant factor if the initialization is good, but with
high probability a random initialization would drive many
factors towards the larger weight factors. The orthogonal-
ization step is a natural modiﬁcation which forces the esti-
mates to converge to different factors, even if some factors
are much larger than the others. It is worth stressing that
the orthogonalization step does not force the ﬁnal recov-
ered factors to be orthogonal (because the ALS step follows
the orthogonalization step) and in general the factors out-
put will not be orthogonal (which is essential for accurately
recovering the factors).

From a computational perspective, adding the orthogonal-
ization step does not add to the computational cost as the
least squares updates in step 4-6 of Algorithm 1 involve an
extra pseudoinverse term for standard ALS, which evalu-
ates to identity for Orth-ALS and does not have to be com-
puted. The cost of orthogonalization is O(k2d), while the
cost of computing the pseudoinverse is also O(k2d). We
also observe signiﬁcant speedups in terms of the number of
iterations required for convergence for Orth-ALS as com-
pared to standard ALS in our simulations on random ten-
sors (see the experiments in Section 5).

Orthgonalized ALS for Tensor Decomposition

Variants of Orthogonalized ALS. Several other modiﬁ-
cations to the simple orthogonalization step also seem nat-
ural. Particularly for low-dimensional settings, in practice
we found that it is useful to carry out orthogonalization
for a few steps and then continue with standard ALS up-
dates until convergence (we call this variant Hybrid-ALS).
Hybrid-ALS also gracefully reverts to standard ALS in set-
tings where the factors are highly correlated and orthogo-
nalization is not helpful.

4.1. Performance Guarantees

We now state the formal guarantees on the performance of
Orthogonalized ALS. The speciﬁc variant of Orthogonal-
ized ALS that our theorems apply to is a slight modiﬁcation
of Algorithm 1, and differs in that there is a periodic (every
log k steps) re-randomization of the factors for which our
analysis has not yet guaranteed convergence. In our prac-
tical implementations, we observe that all factors seem to
converge within this ﬁrst log k steps, and hence the subse-
quent re-randomization is unnecessary.

i=1 wiAi ⊗ Ai ⊗ Ai. Let cmax = maxi(cid:54)=j |AT

Theorem 1. Consider a d-dimensional rank k tensor T =
(cid:80)k
i Aj| be
the incoherence between the true factors and γ = wmax
wmin
be the ratio of the largest and smallest weight. Assume
γcmax ≤ o(k−2), and the estimates of the factors are ini-
tialized randomly from the unit sphere. Provided that, at
the i(log k + log log d)th step of the algorithm the esti-
mates for all but the ﬁrst i factors are re-randomized, then
with high probability the orthogonalized ALS updates con-
verge to the true factors in O(k(log k + log log d)) steps,
and the error at convergence satisﬁes (up to relabelling)
(cid:107) Ai − ˆAi (cid:107)2
2≤ O(γk max{c2
| ≤
O(max{cmax, 1/d}), for all i.

max, 1/d2}) and |1 − ˆwi
wi

√

Theorem 1 immediately gives convergence guarantees for
random low rank tensors. For random d dimensional ten-
sors, cmax = O(1/
d); therefore Orth-ALS converges
globally with random initialization whenever k = o(d0.25).
If the tensor has rank much smaller than the dimension,
then our analysis can tolerate signiﬁcantly higher correla-
tion between the factors. In the Appendix, we also prove
Theorem 1 for the special and easy case of orthogonal ten-
sors, which nevertheless highlights the key proof ideas.

for random tensors, provided an SVD based initialization
is employed.

Theorem 2. Consider a d-dimensional rank k tensor T =
(cid:80)k
i=1 wiAi ⊗ Ai ⊗ Ai with the factors Ai sampled uni-
formly from the d-dimensional sphere. Deﬁne γ = wmax
wmin
to be the ratio of the largest and smallest weight. As-
sume k ≤ o(d) and γ ≤ polylog(d). If the initialization
x0 ∈ Rd is chosen uniformly from the unit sphere, then with
high probability the tensor power method updates converge
to one of the true factors (say A1) in O(log log d) steps,
and the error at convergence satisﬁes (cid:107) A1 − ˆA1 (cid:107)2 ≤
˜O(1/
d). Also, the estimate of the weight ˆw1 satisﬁes
| ≤ ˜O(1/
|1 − ˆw1
w1

d).

√

√

Theorem 2 provides guarantees for random tensors, but it
is natural to ask if there are deterministic conditions on the
tensors which guarantee global convergence of the tensor
power method. Our analysis also allows us to obtain a clean
characterization for global convergence of the tensor power
method updates for worst-case tensors in terms of the inco-
herence of the factor matrix—

Theorem 3. Consider a d-dimensional rank k tensor T =
(cid:80)k
i=1 wiAi ⊗ Ai ⊗ Ai. Let cmax = maxi(cid:54)=j |AT
i Aj|
and γ = wmax
be the ratio of the largest and small-
wmin
est weight, and assume γcmax ≤ o(k−2).
If the ini-
tialization x0 ∈ Rd is chosen uniformly from the unit
sphere, then with high probability the tensor power method
updates converge to one of the true factors (say A1) in
O(log k + log log d) steps, and the error at convergence
satisﬁes (cid:107) A1 − ˆA1 (cid:107)2
max, 1/d2}) and
|1 − ˆw1
w1

| ≤ O(max{cmax, 1/d}).

2 ≤ O(γk max{c2

5. Experiments

We compare the performance of Orth-ALS, standard ALS
(with random and SVD initialization), the tensor power
method, and the classical eigendecomposition approach,
through experiments on low rank tensor recovery in a few
different parameter regimes, on a overcomplete tensor de-
composition task and a tensor completion task. We also
compare the factorization of Orth-ALS and standard ALS
on a large real-world tensor of word tri-occurrence based
on the 1.5 billion word English Wikipedia corpus.3

4.2. New Guarantees for the Tensor Power Method

5.1. Experiments on Random Tensors

As a consequence of our analysis of the orthogonalized
ALS algorithm, we also prove new guarantees on the ten-
sor power method. As these may be of independent interest
because of the wide use of the tensor power method, we
summarize them in this section. We show a quadratic rate
of convergence (in O(log log d) steps) with random initial-
ization for random tensors having rank k = o(d). This
contrasts with the analysis of Anandkumar et al. (2014c)
who showed a linear rate of convergence (O(log d) steps)

Recovering low rank tensors: We explore the abilities
of Orth-ALS, standard ALS, and the tensor power method
(TPM), to recover a low rank (rank k) tensor that has been
constructed by independently drawing each of the k factors
independently and uniformly at random from the d dimen-
sional unit spherical shell. We consider several different

3MATLAB, Python and C code for Orth-ALS and Hybrid-
http://web.stanford.edu/

is

ALS
available
˜vsharan/orth-als.html

at

Orthgonalized ALS for Tensor Decomposition

combinations of the dimension, d, and rank, k. We also
consider both the setting where all of the factors are equally
weighted, as well as the practically relevant setting where
the factor weights decay geometrically, and consider the
setting where independent Gaussian noise has been added
to the low-rank tensor.

In addition to random initialization for standard ALS and
the TPM, we also explore SVD based initialization (Anand-
kumar et al., 2014c) where the factors are initialized via
SVD of a projection of the tensor onto a matrix. We also
test the classical technique for tensor decomposition via si-
multaneous diagonalization (Leurgans et al., 1993; Harsh-
man, 1970) (also known as Jennrich’s algorithm, we refer
to it as Sim-Diag), which ﬁrst performs two random pro-
jections of the tensor, and then recovers the factors by an
eigenvalue decomposition of the projected matrices. This
gives guaranteed recovery when the tensors are noiseless
and factors are linearly independent, but is extremely un-
stable to perturbations.

We evaluate the performance in two respects: 1) the ability
of the algorithms to recover a low-rank tensor that is close
to the input tensor, and 2) the ability of the algorithms to
recover accurate approximations of many of the true fac-
tors. Fig. 1 depicts the performance via the ﬁrst metric.
We evaluate the performance in terms of the discrepancy
between the input low-rank tensor, and the low-rank tensor
recovered by the algorithms, quantiﬁed via the ratio of the
Frobenius norm of the residual, to the Frobenius norm of
the actual tensor: (cid:107)T − ˆT (cid:107)F
, where ˆT is the recovered tensor.
(cid:107)T (cid:107)F
Since the true tensor has rank k, the inability of an algo-
rithm to drive this error to zero indicates the presence of
local optima. Fig. 1 depicts the performance of Orth-ALS,
standard ALS with random initialization and the hybrid al-
gorithm that performs Orth-ALS for the ﬁrst ﬁve iterations
before reverting to standard ALS (Hybrid-ALS). Tests are
conducted in both the setting where factor weights are uni-
form, as well as a geometric spacing, where the ratio of the
largest factor weight to the smallest is 100. Fig. 1 shows
that Hybrid ALS and Orth-ALS have much faster conver-
gence and ﬁnd a signiﬁcantly better ﬁt than standard ALS.

Fig. 2 quantiﬁes the performance of the algorithms in terms
of the number of the original factors that the algorithms
accurately recover. We use standard ALS, Orth-ALS (Al-
gorithm 1), Hybrid-ALS, TPM with random initialization
(TPM), ALS with SVD initialization (ALS-SVD), TPM
with SVD initialization (TPM-SVD) and the simultaneous
diagonalization approach (Sim-Diag). We run TPM and
SVD-TPM with 100 different initializations and ﬁnd a rank
k = 30 decomposition for ALS, ALS-SVD, Orth-ALS,
Hybrid-ALS and Sim-Diag. We repeat the experiment (by
sampling a new tensor) 10 times. We perform this evalu-
ation in both the setting where we receive an actual low-

rank tensor as input, as well as the setting where each en-
try Tijk of the low-rank tensor has been perturbed by in-
dependent Gaussian noise of standard deviation equal to
0.05Tijk. We can see that Orth-ALS and Hybrid-ALS per-
form signiﬁcantly better than the other algorithms and are
able to recover all factors in the noiseless case even when
the weights are highly skewed. Note that the reason the
Hybrid-ALS and Orth-ALS fail to recover all factors in the
noisy case when the weights are highly skewed is that the
magnitude of the noise essentially swamps the contribution
from the smallest weight factors.

Recovering over-complete tensors: Overcomplete ten-
sors are tensors with rank higher than the dimension, and
have found numerous theoretical applications in learning
latent variable models (Anandkumar et al., 2015). Even
though orthogonalization cannot be directly applied to the
setting where the rank is more than the dimension (as the
factors can no longer be orthogonalized), we explore a de-
ﬂation based approach in this setting. Given a tensor T with
dimension d = 50 and rank r > d, we ﬁnd a rank d decom-
position T1 of T , subtract T1 from T , and then compute a
rank d decomposition of T1 to recover the next set of d fac-
tors. We repeat this process to recover subsequent factors.
After every set of d factors has been estimated, we also re-
ﬁne the factor estimates of all factors estimated so far by
running an additional ALS step using the current estimates
of the extracted factors as the initialization. Fig. 3a plots
the number of factors recovered when this deﬂation based
approach is applied to a dimension d = 50 tensor with a
mild power low distribution on weights. We can see that
Hybrid-ALS is successful at recovering tensors even in the
overcomplete setup, and gives an improvement over ALS.

Tensor completion: We also test the utility of orthogonal-
ization on a tensor completion task, where the goal is to
recover a large missing fraction of the entries. Fig. 3b sug-
gests Hybrid-ALS gives considerable improvements over
standard ALS. Further examining the utility of orthogo-
nalization in this important setting, in theory and practice,
would be an interesting direction.

5.2. Learning Word Embeddings via Tensor

Factorization

A word embedding is a vector representation of words
which preserves some of the syntactic and semantic rela-
tionships in the language. Current methods for learning
word embeddings implicitly (Mikolov et al., 2013b; Levy
& Goldberg, 2014) or explicitly (Pennington et al., 2014)
factorize some matrix derived from the matrix of word co-
occurrences M , where Mij denotes how often word i ap-
pears with word j. We explore tensor methods for learning
word embeddings, and contrast the performance of stan-
dard ALS and Orthogonalized ALS on standard tasks.

Orthgonalized ALS for Tensor Decomposition

(a) k = 30, d = 100,
uniform weights

(b) k = 30, d = 100,

wmax
wmin

= 100

(c) k = 100, d = 1000,
uniform weights

(d) k = 100, d = 1000,
= 100

wmax
wmin

Figure 1. Plot of the normalized discrepancy between the recovered rank k tensor ˆT and the true tensor T : (cid:107)T − ˆT (cid:107)F
, as a function of the
(cid:107)T (cid:107)F
iteration. In all settings, the Orth-ALS and the hybrid algorithm drive this discrepancy nearly to zero, with the performance of Orth-ALS
improving for the higher dimensional cases, whereas standard ALS algorithm has slower convergence and gets stuck in bad local optima.

(a) Noiseless case, ratio of weights equals wmax
wmin

(b) Noisy case, ratio of weights equals wmax
wmin

Figure 2. Average number of factors recovered by different algorithms for different values of wmax
, the ratio of the maximum factor
wmin
weight to minimum factor weight (with the weights spaced geometrically), along with error bars for the standard deviation in the number
of factors recovered, across independent trials. The true rank k = 30, and the dimension d = 100. We say a factor {Ai, Bi, Ci} of the
tensor T is successfully recovered if there exists at least one recovered factor { ˆAj, ˆBj, ˆCj} with correlation at least 0.9 in all modes.
Orth-ALS and Hybrid-ALS recover all factors in almost all settings, whereas ALS and the tensor power method struggle when the
weights are skewed, even with the more expensive SVD based initialization.

(a) No. of factors recovered for a dimension d = 50
tensor with varying rank. The weights of the factors
are geometrically spaced and the ratio of the weights
between every pair of consecutive factors is 1.05

(b) Average error on the missing entries for tensor
completion where each entry is sampled with proba-
bility p, on a 100 different runs with each setting of p.
The tensor has dimension d = 50 and rank k = 10.

Figure 3. Experiments on overcomplete tensors and tensor completion. Even though our theoretical guarantees do not apply to these
settings, we see that orthogonalization leads to signiﬁcantly better performance over standard ALS.

10203000.10.20.30.40.50.6IterationNormalized Error  Hybrid−ALSOrth−ALSALS10203000.10.20.30.40.50.6IterationNormalized Error  Hybrid−ALSOrth−ALSALS10203000.10.20.30.40.50.6IterationNormalized Error  Hybrid−ALSOrth−ALSALS10203000.10.20.30.40.50.6IterationNormalized Error  Hybrid−ALSOrth−ALSALS1101001000051015202530Ratio of weightsNo of factors recovered  Hybrid−ALS/Orth−ALS/Sim−DiagALS−SVDALSTPM−SVDTPM1101001000051015202530Ratio of weightsNo of factors recovered  Hybrid−ALS/Orth−ALSALS−SVDALSTPM−SVDTPMSim−Diag1001502002503003504000100200300400RankNo. of factors recovered  RankALSHybrid−ALS0.020.060.10.20.30.4Sampling probability (p)0%2.5%5.0%7.5%10%12.5%Normalized MSEALSHybrid-ALSOrthgonalized ALS for Tensor Decomposition

Methodology. We used the English Wikipedia as our cor-
pus, with 1.5 billion words. We constructed a word co-
occurrence tensor T of the 10,000 most frequent words,
where the entry Tijk denotes the number of times the words
i, j and k appear in a sliding window of length w across the
corpus. We consider two different window lengths, w = 3
and w = 5. Before factoring the tensor, we apply the
non-linear element-wise scaling f (x) = log(1 + x) to the
tensor. This scaling is known to perform well in practice
for co-occurrence matrices (Pennington et al., 2014), and
makes some intuitive sense in light of the Zipﬁan distribu-
tion of word frequencies. Following the application of this
element-wise nonlinearity, we recover a rank 100 approxi-
mation of the tensor using Orth-ALS or ALS.

We concatenate the (three) recovered factor matrices into
one matrix and normalize the rows. The ith row of this
matrix is then the embedding for the ith word. We test the
quality of these embeddings on two tasks aimed at measur-
ing the syntactic and semantic structure captured by these
word embeddings.

We also evaluated the performance of matrix SVD based
methods on the task. For this, we built the co-occurrence
matrix M with a sliding window of length w over the cor-
pus. We applied the same non-linear element-wise scaling
and performed a rank 100 SVD, and set the word embed-
dings to be the singular vectors after row normalization.

It is worth highlighting some implementation details for
our experiments, as they indicate the practical efﬁciency
and scalability inherited by Orth-ALS from standard ALS.
Our experiments were run on a cluster with 8 cores and
48 GB of RAM memory per core. Most of the runtime
was spent in reading the tensor, the runtime for Orth-ALS
was around 80 minutes, with 60 minutes spent in reading
the tensor (the runtime for standard ALS was around 100
minutes because it took longer to converge). Since storing
a dense representation of the 10,000×10,000×10,000 ten-
sor is too expensive, we use an optimized ALS solver for
sparse tensors (Smith & Karypis; 2015) which also has an
efﬁcient parallel implementation.

Evaluation: Similarity and Analogy Tasks. We eval-
uated the quality of the recovered word embeddings pro-
duced by the various methods via their performance on
two different NLP tasks for which standard, human-labeled
data exists: estimating the similarity between a pair of
words, and completing word analogies.

The word similarity tasks (Bruni et al., 2012; Finkelstein
et al., 2001) contain word pairs along with human assigned
similarity scores, and the objective is to maximize the cor-
relation between the similarity in the embeddings of the
two words (according to a similarity metric such as the dot
product) and human judged similarity.

Algorithm

Similarity tasks Analogy tasks

Standard ALS, w = 3
Standard ALS, w = 5

Orth-ALS, w = 3
Orth-ALS, w = 5

Matrix methods, w = 3
Matrix methods, w = 5

0.50
0.50

0.59
0.60

0.68
0.67

30.92%
37.38%

40.00%
46.37%

53.29%
57.40%

Table 1. Results for word analogy and word similarity tasks for
different window lengths w over which the co-occurrences are
counted. The embeddings recovered by Orth-ALS are signif-
icantly better than those recovered by standard ALS. Despite
this, embeddings derived from word co-occurrences using ma-
trix SVD still outperform the tensor embeddings, and we are un-
sure whether this is due to the relative sparsity of the tensor, sub-
optimal element-wise scaling (i.e. the f (x) = log(1+x) function
applied to the counts), or something more fundamental.

The word analogy tasks (Mikolov et al., 2013a;c) present
questions of the form “a is to a∗ as b is to
?” (e.g. “Paris
?”). We ﬁnd the answer to “a
is to France as Rome is to
is to a∗ as b is to b∗” by ﬁnding the word whose embedding
is the closest to wa∗ − wa + wb in cosine similarity, where
wa denotes the embedding of the word a.

Results. The performances are summarized in the Table
1. The use of Orth-ALS rather than standard ALS leads to
signiﬁcant improvement in the quality of the embeddings
as judged by the similarity and analogy tasks. However,
the matrix SVD method still outperforms the tensor based
methods. We believe that it is possible that better tensor
based approaches (e.g. using better renormalization, addi-
tional data, or some other tensor rather than the symmet-
ric tri-occurrence tensor) or a combination of tensor and
matrix based methods can actually improve the quality of
word embeddings, and is an interesting research direction.
Alternatively, it is possible that natural language does not
contain sufﬁciently rich higher-order dependencies among
words that appear close together, beyond the co-occurrence
structure, to truly leverage the power of tensor methods.
Or, perhaps, the two tasks we evaluated on—similarity and
analogy tasks—do not require this higher order.
In any
case, investigating these possibilities seems worthwhile.

6. Conclusion

Our results suggest the theoretical and practical beneﬁts of
Orthogonalized ALS, versus standard ALS. An interesting
direction for future work would be to more thoroughly ex-
amine the practical and theoretical utility of orthogonal-
ization for other tensor-related tasks, such as tensor com-
pletion. Additionally, its seems worthwhile to investigate
Orthogonalized ALS or Hybrid ALS in more application-
speciﬁc domains, such as natural language processing.

Orthgonalized ALS for Tensor Decomposition

References

Anandkumar, Animashree, Liu, Yi-kai, Hsu, Daniel J, Fos-
ter, Dean P, and Kakade, Sham M. A spectral algorithm
for latent dirichlet allocation. In Advances in Neural In-
formation Processing Systems, pp. 917–925, 2012.

Anandkumar, Animashree, Ge, Rong, Hsu, Daniel, and
Kakade, Sham M. A tensor approach to learning mixed
membership community models. The Journal of Ma-
chine Learning Research, 15(1):2239–2312, 2014a.

Anandkumar, Animashree, Ge, Rong, Hsu, Daniel,
Kakade, Sham M, and Telgarsky, Matus. Tensor decom-
positions for learning latent variable models. Journal of
Machine Learning Research, 15(1):2773–2832, 2014b.

Anandkumar, Animashree, Ge, Rong, and Janzamin, Ma-
tensor decomposi-
arXiv preprint

jid.
tion via alternating rank-1 updates.
arXiv:1402.5180, 2014c.

Guaranteed non-orthogonal

Anandkumar, Animashree, Ge, Rong, and Janzamin, Ma-
Learning overcomplete latent variable models
In Proceedings of The 28th

jid.
through tensor methods.
Conference on Learning Theory, pp. 36–112, 2015.

Azizzadenesheli, Kamyar, Lazaric, Alessandro,

and
Anandkumar, Animashree. Reinforcement learning of
POMDPs using spectral methods. In 29th Annual Con-
ference on Learning Theory, pp. 193–256, 2016.

Bader, Brett W. and Kolda, Tamara G. Efﬁcient MATLAB
computations with sparse and factored tensors. SIAM
Journal on Scientiﬁc Computing, 30(1), December 2007.

Bader, Brett W., Kolda, Tamara G., et al. Matlab tensor
toolbox version 2.5. Available online, January 2012.

Battaglino, Casey, Ballard, Grey, and Kolda, Tamara G. A
practical randomized CP tensor decomposition. arXiv
preprint arXiv:1701.06600, 2017.

Bruni, Elia, Boleda, Gemma, Baroni, Marco, and Tran,
Nam-Khanh. Distributional semantics in technicolor. In
Proceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics, 2012.

Chaganty, Arun Tejasvi and Liang, Percy. Estimating
latent-variable graphical models using moments and
likelihoods. In ICML, pp. 1872–1880, 2014.

Cheng, Dehua, Peng, Richard, Liu, Yan, and Perros,
Ioakeim. SPALS: Fast alternating least squares via im-
plicit leverage scores sampling. In Advances In Neural
Information Processing Systems, pp. 721–729, 2016.

Colombo, Nicolo and Vlassis, Nikos. FastMotif: spectral
sequence motif discovery. Bioinformatics, 31(16), 2015.

Colombo, Nicolo and Vlassis, Nikos. Tensor decomposi-
tion via joint matrix schur decomposition. In Proceed-
ings of The 33rd International Conference on Machine
Learning, pp. 2820–2828, 2016.

Comon, Pierre, Luciani, Xavier,

and De Almeida,
Andr´e LF. Tensor decompositions, alternating least
squares and other tales. Journal of chemometrics, 23
(7-8):393–405, 2009.

De Lathauwer, Lieven. A link between the canonical de-
composition in multilinear algebra and simultaneous ma-
trix diagonalization. SIAM journal on Matrix Analysis
and Applications, 28(3):642–666, 2006.

Duembgen, Lutz. Bounding standard gaussian tail proba-

bilities. arXiv preprint arXiv:1012.2063, 2010.

Finkelstein, Lev, Gabrilovich, Evgeniy, Matias, Yossi,
Rivlin, Ehud, Solan, Zach, Wolfman, Gadi, and Ruppin,
Eytan. Placing search in context: The concept revisited.
In Proceedings of the 10th international conference on
World Wide Web, pp. 406–414. ACM, 2001.

Ge, Rong and Ma, Tengyu. Decomposing overcomplete
3rd order tensors using sum-of-squares algorithms. Ap-
proximation, Randomization, and Combinatorial Opti-
mization. Algorithms and Techniques, pp. 829, 2015.

Ge, Rong, Huang, Qingqing, and Kakade, Sham M. Learn-
ing mixtures of gaussians in high dimensions. In Pro-
ceedings of the Forty-Seventh Annual ACM on Sympo-
sium on Theory of Computing, pp. 761–770. ACM, 2015.

Harshman, Richard A. Foundations of the parafac proce-
dure: Models and conditions for an” explanatory” multi-
modal factor analysis. 1970.

H˚astad, Johan. Tensor rank is NP-Complete. Journal of

Algorithms, 11(4):644–654, 1990.

Hillar, Christopher J and Lim, Lek-Heng. Most tensor
problems are NP-Hard. Journal of the ACM, 60(6), 2013.

Hopkins, Samuel B, Schramm, Tselil, Shi, Jonathan, and
Fast spectral algorithms from sum-
Steurer, David.
tensor decomposition and planted
of-squares proofs:
sparse vectors. In Proceedings of the 48th Annual ACM
SIGACT Symposium on Theory of Computing, 2016.

Huang, Furong, Niranjan, UN, Hakeem, Moham-
mad Umar, and Anandkumar, Animashree. Fast detec-
tion of overlapping communities via online tensor meth-
ods. arXiv preprint arXiv:1309.0787, 2013.

Huang, Furong, Matusevych, Sergiy, Anandkumar, Anima,
Karampatziakis, Nikos, and Mineiro, Paul. Distributed
latent dirichlet allocation via tensor factorization.
In
NIPS Optimization Workshop, 2014.

Orthgonalized ALS for Tensor Decomposition

Kang, U, Papalexakis, Evangelos, Harpale, Abhay, and
Faloutsos, Christos. Gigatensor: scaling tensor analysis
up by 100 times-algorithms and discoveries. In Proceed-
ings of the 18th ACM SIGKDD international conference
on Knowledge discovery and data mining. ACM, 2012.

Papalexakis, Evangelos E, Faloutsos, Christos,

and
Sidiropoulos, Nicholas D. Parcube: Sparse paralleliz-
able tensor decompositions. In Joint European Confer-
ence on Machine Learning and Knowledge Discovery in
Databases, pp. 521–536. Springer, 2012.

Kolda, Tamara G and Bader, Brett W. Tensor decomposi-

tions and applications. SIAM review, 51(3), 2009.

Kolda, Tamara G and Mayo, Jackson R. Shifted power
method for computing tensor eigenpairs. SIAM Journal
on Matrix Analysis and Applications, 32(4), 2011.

Kossaiﬁ, Jean, Panagakis, Yannis, and Pantic, Maja. Ten-
arXiv preprint

sorly: Tensor learning in python.
arXiv:1610.09555, 2016.

Kruskal, Joseph B. Three-way arrays: rank and uniqueness
of trilinear decompositions, with application to arith-
metic complexity and statistics. Linear algebra and its
applications, 18(2):95–138, 1977.

Kuleshov, Volodymyr, Chaganty, Arun Tejasvi, and Liang,
Percy. Tensor factorization via matrix factorization. In
AISTATS, 2015.

Le, Quoc V, Karpenko, Alexandre, Ngiam, Jiquan, and Ng,
ICA with reconstruction cost for efﬁcient
Andrew Y.
In Advances in Neural
overcomplete feature learning.
Information Processing Systems, pp. 1017–1025, 2011.

Leurgans, SE, Ross, RT, and Abel, RB. A decomposition
for three-way arrays. SIAM Journal on Matrix Analysis
and Applications, 14(4):1064–1083, 1993.

Levy, Omer and Goldberg, Yoav. Neural word embedding
as implicit matrix factorization. In Advances in Neural
Information Processing Systems, pp. 2177–2185, 2014.

Jonathan,

Ma, Tengyu, Shi,

and Steurer, David.
Polynomial-time tensor decompositions with sum-of-
squares. In Foundations of Computer Science (FOCS),
2016 IEEE 57th Annual Symposium on, 2016.

Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jef-
frey. Efﬁcient estimation of word representations in vec-
tor space. arXiv preprint arXiv:1301.3781, 2013a.

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed representations of
In Ad-
words and phrases and their compositionality.
vances in Neural Information Processing Systems, pp.
3111–3119, 2013b.

Pennington,

Jeffrey, Socher, Richard, and Manning,
Christopher D. Glove: Global vectors for word repre-
In Empirical Methods in Natural Language
sentation.
Processing (EMNLP), pp. 1532–1543, 2014.

Shah, Parikshit, Rao, Nikhil, and Tang, Gongguo. Sparse
and low-rank tensor decomposition. In Advances in Neu-
ral Information Processing Systems, 2015.

Smith, Shaden and Karypis, George. SPLATT: The Sur-

prisingly ParalleL spArse Tensor Toolkit.

Smith, Shaden and Karypis, George. DMS: Distributed
sparse tensor factorization with alternating least squares.
Technical report, 2015.

Song, Zhao, Woodruff, David, and Zhang, Huan. Sublinear
time orthogonal tensor decomposition. In Advances in
Neural Information Processing Systems, 2016.

Souloumiac, Antoine.

Is non-
Joint diagonalization:
In 3rd
orthogonal always preferable to orthogonal?
IEEE International Workshop on Computational Ad-
vances in Multi-Sensor Adaptive Processing, 2009.

Tang, Gongguo and Shah, Parikshit. Guaranteed tensor de-
In Proceedings of
composition: A moment approach.
the 32nd International Conference on Machine Learn-
ing (ICML-15), pp. 1491–1500, 2015.

Vervliet, N., Debals, O., Sorber, L., Van Barel, M., and
De Lathauwer, L. Tensorlab 3.0, Mar. . Available online.

Wang, Yichen, Chen, Robert, Ghosh, Joydeep, Denny,
Joshua C, Kho, Abel, Chen, You, Malin, Bradley A, and
Sun, Jimeng. Rubik: Knowledge guided tensor factor-
ization and completion for health data analytics. In Pro-
ceedings of the 21th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining, pp.
1265–1274. ACM, 2015a.

Wang, Yining, Tung, Hsiao-Yu, Smola, Alexander J, and
Anandkumar, Anima. Fast and guaranteed tensor de-
In Advances in Neural In-
composition via sketching.
formation Processing Systems, pp. 991–999, 2015b.

Yu, Rose and Liu, Yan. Learning from multiway data: Sim-
In Proceedings of
ple and efﬁcient tensor regression.
the 33nd International Conference on Machine Learn-
ing (ICML-16), pp. 238–247, 2016.

Mikolov, Tomas, Yih, Wen-tau, and Zweig, Geoffrey. Lin-
guistic regularities in continuous space word representa-
tions. In HLT-NAACL, pp. 746–751, 2013c.

Zhang, Tong and Golub, Gene H. Rank-one approximation
to high order tensors. SIAM Journal on Matrix Analysis
and Applications, 23(2):534–550, 2001.

A. Proof Overview: the Orthogonal Tensor Case

Orthgonalized ALS for Tensor Decomposition

In this section, we will consider Orthogonalized ALS for the special case when the factors matrix of the tensor is an
orthogonal matrix. Although this special case is easy and numerous algorithms provably work in this setting, it will serve
to highlight the high level analysis approach that we apply to the more general settings.

The analysis of Orth-ALS hinges on an analysis of the tensor power method. For completeness we describe the tensor
power method in Algorithm 2. We will go through some preliminaries for our analysis of the tensor power method. Let
the iterate of the tensor power method at time t be Zt. The tensor power method update equations can be written as (refer
to (Anandkumar et al., 2014c))

Zt =

(cid:80)k
(cid:107) (cid:80)k

i=1 wi(cid:104)Zt−1, Ai(cid:105)2Ai
i=1 wi(cid:104)Zt−1, Ai(cid:105)2Ai (cid:107)2

(A.1)

Eq. A.1 is just the tensor analog of the matrix power method updates. For tensors, the updates are quadratic in the previous
inner products, in contrast to matrices where the updates are linear in the inner products in the previous step.

0 ∈ Rd uniformly from the unit sphere or using the SVD based method

5:

0 , y(τ )

0 , z(τ )

Algorithm 2 Tensor power method to recover all factors (Anandkumar et al., 2014c)
Input: Tensor T ∈ Rd×d×d, number of initializations L, number of iterations N .
1: for τ = 1 : L do
Initialize x(τ )
2:
for t = 1 : N do
3:
4:

Rank-1 ALS/Power method updates-x(τ )
Rank-1 ALS/Power method updates-y(τ )
Rank-1 ALS/Power method updates-z(τ )
t+1, zτ
Normalize xτ

t+1 = T(1)(ˆz(τ )
t+1 = T(2)(ˆz(τ )
t+1 = T(3)(ˆy(τ )
t+1, ˆyτ

t (cid:12) ˆy(τ )
t (cid:12) ˆx(τ )
t (cid:12) ˆx(τ )
)
t
t+1, ˆzτ
t+1.

t+1 and store results in ˆxτ

end for
Estimate weights- ˆw(τ ) = T (ˆx(τ )

6:
7:
8:
9:
10: end for
11: Cluster set {( ˆw(τ ), ˆx(τ )
12: return the centers {( ˆwi, ˆai, ˆbi, ˆci), i ∈ [k]} of the k clusters as the estimates

N ), τ ∈ [L]} into k clusters.

N , ˆz(τ )
N )

N , ˆz(τ )

N , ˆy(τ )

N , ˆy(τ )

t+1, yτ

)

)

t

t

Observe from Algorithm 1 (Orth-ALS) that the ALS steps in step 4-6 have the same form as tensor power method updates,
but on the orthogonalized factors. This is the key idea we use in our analysis of Orth-ALS. Note that the ﬁrst factor
estimate is never affected by the orthogonalization, hence the updates for the ﬁrst estimated factor have exactly the same
form as the tensor power method updates, as this factor is unaffected by orthogonalization. The subsequent factors have
an orthogonalization step before every tensor power method step. This ensures that they never have high correlation with
the factors which have already been recovered, as they are projected orthogonal to the recovered factors before each ALS
step. We then use the incoherence of the factors to argue that orthogonalization does not signiﬁcantly affect the updates of
the factors which have not been recovered so far, while ensuring that the factors which have already been recovered always
have a small correlation.

Note that Eq. A.1 is invariant with respect to multiplying the weights of all the factors by some constant. Hence for ease of
exposition, we assume that all the weights lie in the interval [1, γ], where γ = wmax
. We also deﬁne η = max{cmax, 1/d}.
wmin
Proposition 1. Consider a d-dimensional rank k tensor T = (cid:80)k
i=1 wiAi⊗Ai⊗Ai where the factor matrix A is orthogonal.
Deﬁne γ = wmax
to be the ratio of the largest and smallest weight. If the initial estimates for all the factors are initialized
wmin
randomly from the unit sphere and the factors {Aj, j ≥ i + 1} are re-randomized after i(log k + log log d) steps where i is
an integer, then with high probability the orthogonalized ALS updates converge to the true factors in O(k(log k+log log d))
steps, and the error at convergence satisﬁes (cid:107) Ai − ˆAi (cid:107)2

| ≤ O(1/d) for all i.

2≤ O(γk/d2}) and |1 − ˆwi
wi

Proof. Without loss of generality, we assume that the ith recovered factor converges to the ith true factor. As mentioned
earlier, the iterations for the ﬁrst factor are the usual tensor power method updates and are unaffected by the remaining
factors. Therefore to show that orthogonalized ALS recovers the ﬁrst factor, we only need to analyze the tensor method
updates. We show that the tensor power method with random initialization converges in O(log k + log log d) steps with

Orthgonalized ALS for Tensor Decomposition

probability at least (1 − 1/k1+(cid:15)), for any (cid:15) > 0. Hence this implies that Orth-ALS correctly recovers the ﬁrst factor in
O(log k + log log d) steps with probability at least (1 − 1/k1+(cid:15)), for any (cid:15) > 0.

The main idea of our proof of convergence of the tensor power method is the following – with decent probability, there is
some separation between the correlations of the factors with the random initialization. By the tensor power method updates
(Eq. A.1), this gap is ampliﬁed at every stage. We analyze the updates for all the factors together by a simple recursion.
We then show that this recursion converges in in O(log k + log log d) steps.

Let Zt be the iterate of the tensor power method updates at time t. Without loss of generality, we will be proving con-
vergence to the ﬁrst factor A1. Let ai,t be the correlation of the ith factor Ai with Zt, i.e. ai,t = (cid:104)Ai, Zt(cid:105) (note that this
should technically be called the inner product, but we refer to it as the correlation). We will refer to wiai,t as the weighted
correlation of the ith factor.

The ﬁrst step of the proof is that with decent probability, there is some separation between the weighted correlation of the
factors with the initial random estimate. This is Lemma 1.

Lemma 1. If γkcmax ≤ 1/k1+(cid:15) for some (cid:15) > 0, then with probability at least
∀ i (cid:54)= arg maxi |wiai,0|.

(cid:16)

1 − log5 k
k1+(cid:15)

(cid:17)

,

|wiai,0|

maxi |wiai,0| ≤ 1 − 5/k1+(cid:15)

The proof of Lemma 1 is a bit technical, but relies on basic concentration inequalities for Gaussians. Then using Eq. A.1
the correlation at the end of the (t + 1)th time step is given by

where κt = (cid:107) (cid:80)k

i=1 wi(cid:104)Zt−1, Ai(cid:105)2 (cid:107)2 is the normalizing factor at the tth time step.

Because the estimate is normalized at the end of the updates, we only care about the ratio of the correlations of the
factors with the estimate rather than the magnitude of the correlations themselves. Hence, it is convenient to normalize all
the correlations by the correlation of the largest factor and normalize all the weights by the weight of the largest factor.
Therefore, let ˆai,t = ai,t
a1,t

. The new update equation for the ratio of correlations ˆai,t is-

and ˆwi = wi
w1

Our goal is to show that ai,t becomes small for all i (cid:54)= 1 in O(log k + log log d) steps. Instead of separately analyzing the
different ai,t for different factors Ai, we upper bound ai,t for all i via a simple recursion. Consider the recursion,

ai,t+1 = wia2

i,t/κt

ˆai,t+1 = ˆwiˆa2
i,t

(cid:12)
(cid:12)
(cid:12) ˆwiˆai,0

(cid:12)
(cid:12)
(cid:12)

β0 = max
i(cid:54)=1
βt+1 = β2
t

(A.2)

(A.3)

(A.4)

We claim that | ˆwiˆai,t| ≤ βt for all t and i (cid:54)= 1. By Eq. A.3, this is true for t = 0 by deﬁnition. We prove our claim via
induction. Assume that | ˆwiˆai,t| ≤ βt for t = p. Note that by Eq. A.2, ˆwiˆai,p+1 = ˆw2
p+1 for
all i (cid:54)= 1. Hence | ˆwiˆai,t| ≤ βt for all t and i (cid:54)= 1. Note that as the weights lie in the interval [1, γ], ˆai,t ≤ βt.

i,p. Therefore wiˆai,p+1 ≤ β2

i ˆa2

To show convergence, we will now analyze the recursion in Eq. A.3. We will show that βt becomes sufﬁciently small in
O(log k + log log d) steps. Note that βt = (β0)2t
and β0 ≤ 1 − 5/k1+(cid:15). Therefore βt ≤ 0.1 for t = 2 log k. In another
log log d steps, βt ≤ 1/d. Hence βt ≤ 1/d in O(log k + log log d) steps. As βt is an upper bound for the correlation of all
but the ﬁrst factor, hence |ˆai,t| ≤ 1/d for all i (cid:54)= 1 in O(log k + log log d) steps.

To ﬁnish the proof of convergence for the tensor power method, we need to show that the estimate Zt is close to A1 if it
has small correlation with every factor other than A1. Lemma 2 shows that if the ratio of the correlation of every other
factor with A1 is small, then the residual (cid:96)2 error in estimating A1 is also small.

Lemma 2. Let γkcmax ≤ 1/k1+(cid:15). Without loss of generality assume convergence to the ﬁrst factor A1. Deﬁne
ˆai,t = | ai,t
If ˆai,t ≤ 2η ∀ i (cid:54)= 1,
a1,t
then (cid:107) A1 − ˆA1 (cid:107)2
2≤ O(γkη2) the relative error in the
estimation of the weight w1 is at most O(η).

2 ≤ 10γkη2 in the subsequent iteration. Also, if (cid:107) A1 − ˆA1 (cid:107)2

|- the ratio of the correlation of the ith and 1st factor with the iterate at time t.

Orthgonalized ALS for Tensor Decomposition

Using Lemma 2, it follows that the estimate ˆA1 and ˆw1 for the factor A1 satisﬁes (cid:107) A1 − ˆA1 (cid:107)2
O(1/d). Hence we have shown that Orth-ALS correctly recovers the ﬁrst factor.

2≤ 10γk/d2 and |1 − ˆw1
w1

| ≤

We now prove that Orth-ALS also recovers the remaining factors. The proof proceeds by induction. We have already
shown that the base case is correct and the algorithm recovers the ﬁrst factor. We next show that if the ﬁrst (m − 1) factors
have converged, then the mth factor converges in O(log k + log log d) steps with failure probability at most ˜O(1/k1+(cid:15)).
The main idea is that as the factors have small correlation with each other, hence orthogonalization does not affect the
factors which have not been recovered but ensures that the mth estimate never has high correlation with the factors which
have already been recovered. Recall that we assume without loss of generality that the ith recovered factor Xi converges
to the ith true factor, hence Xi = Ai + ˆ∆i for i < m, where (cid:107) ˆ∆i (cid:107)2 ≤ 10γk/d2. This is our induction hypothesis,
which is true for the base case as we just showed that the tensor power method updates converge with residual error at most
10γk/d2.

Let Xm,t denote the mth factor estimate at time t and let Ym denote it’s value at convergence. We will ﬁrst calculate the
effect of the orthogonalization step on the correlation between the factors and the estimate Xm,t. Let { ¯Xi, i < m} denote
an orthogonal basis for {Xi, i < m}. The basis { ¯Xi, i < m} is calculated via QR decomposition, and can be recursively
written down as follows,

j<i
Note that the estimate Xm,t is projected orthogonal to this basis. Deﬁne ¯Xm,t as this orthogonal projection, which can be
written down as follows –

(A.5)

¯Xi =

Xi − (cid:80)
(cid:107) Xi − (cid:80)

j<i

¯X T
j Xi ¯Xj
j Xi ¯Xj (cid:107)2
¯X T

¯Xm,t = ¯Xm,t −

¯X T

j Xm,t ¯Xj

(cid:88)

j<m

In the QR decomposition algorithm ¯Xm,t is also normalized to have unit norm but we will ignore the normalization of
Xm,t in our analysis because as before we only consider ratios of correlations of the true factors with ¯Xm,t, which is
unaffected by normalization.
We will now analyze the orthogonal basis { ¯Xi, i < m}. The key idea is that the orthogonal basis { ¯Xi, i < m} is close to
the original factors {Ai, i < m} as the factors are incoherent. Lemma 3 proves this claim.

Lemma 3. Consider a stage of the Orthogonalized ALS iterations when the ﬁrst (m − 1) factors have converged. Without
loss of generality let Xi = Ai + ˆ∆i, i < m, where(cid:107) ˆ∆i (cid:107)2 ≤ 10γkη2. Let { ¯Xi, i < m} denote an orthogonal basis for
{Xi, i < m} calculated using Eq. A.5. Then,

1. ¯Xi = Ai + ∆i, ∀ i < m and (cid:107) ∆j (cid:107)2 ≤ 10kη.

2. |AT

j ∆i| ≤ 3η, ∀ i < m, j < i.

3. |AT

j ∆i| ≤ 20γkη2, ∀ i < m, j > i.

Using Lemma 3, we will ﬁnd the effect of orthogonalization on the correlations of the factors with the iterate Xm,t. At a
high level, we need to show that the iterations for the factors {Ai, i ≥ m} are not much affected by the orthogonalization,
while the correlations of the factors {Ai, i < m} with the estimate Xm,t are ensured to be small. Lemma 3 is the key tool
to prove this, as it shows that the orthogonalized basis is close to the true factors.
We will now analyze the inner product between ¯Xm,t and factor Ai. This is given by-

As earlier, we normalize all the correlations by the correlation of the largest factor, let ¯ai,t be the ratio of the correlations
of Ai and Am with the orthogonalized estimate ¯Xm,t at time t. We can write ¯ai,t as-

AT
i

¯Xm,t = AT

i Xm,t −

X T

m,t

¯XjAT
i

¯Xj

(cid:88)

j<m

¯ai,t =

i Xm,t − (cid:80)
AT
mXm,t − (cid:80)
AT

j<m X T
j<m X T

m,t

m,t

¯XjAT
i
¯XjAT
m

¯Xj
¯Xj

We can multiply both sides by ˆwi and substitute ¯Xj from Lemma 3 and then rewrite as follows-

Orthgonalized ALS for Tensor Decomposition

ˆwi¯ai,t =

i Xm,t − (cid:80)
ˆwiAT
mXm,t − (cid:80)
AT

j<m ˆwiX T
j<m X T

m,t(Aj + ∆j)AT
m∆j

m,t(Aj + ∆j)AT

i ∆j

We divide the numerator and denominator by AT

mXm,t to derive an expression in terms of the ratios of correlations. Let

δi,t =

X T
X T

m,t∆j
m,tAm

.

ˆwi¯ai,t =

ˆwiˆai,t − (cid:80)
1 − (cid:80)

j<m( ˆwiˆaj,t + ˆwiδi,t)AT
j<m(ˆaj,t + δi,t)AT
m∆j

i ∆j

We now need to show ˆwi¯ai,t is small for all i < m and is close to ˆwiai,t, the weighted correlation before orthogonalization,
for all i > m. Lemma 4 proves this, and shows that the weighted correlation of factors which have not yet been recovered,
{Ai, i ≥ m}, is not much affected by orthogonalization, but the factors which have already been recovered. {Ai, i < m},
are ensured to be small after the orthogonalization step.

Lemma 4. Let | ˆwiˆai,t| ≤ βt ∀ i (cid:54)= m at the end of the tth iteration. Let ¯ai,t be the ratio of the correlation of the ith and
the mth factor with Xm,t, the iterate at time t after the orthogonalization step. Then,

1. | ˆwi¯ai,t| ≤ βt(1 + 1/k1+(cid:15))), ∀ i > m.

2. | ˆwi¯ai,t| ≤ 50γkηβt, ∀ i < m.

We are now ready to analyze the Orth-ALS updates for the mth factor. First, we argue about the initialization step.
Lemma 4 shows that an orthogonalization step performed after a random initialization ensures that the factors which have
already been recovered have small correlation with the orthogonalized initialization. This is where we need a periodic
re-randomization of the factors which have not converged so far.

Lemma 5. Let Xm,0 be initialized randomly and the result be projected orthogonal to the (m − 1) previously estimated
factors, let these be {Xi, i < m} without loss of generality. Then arg maxi |wiai,0| ≥ m with high probability. Also, with
(cid:12)
(cid:12)
(cid:12) ≤ 1 − 4/k1+(cid:15) ∀ i (cid:54)= arg maxi |wiai,0| after the orthogonalization
failure probability at most
step.

wi¯ai,0
maxi{wi¯ai,0}

1 − log5 k
k1+(cid:15)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

(cid:17)

,

Lemma 5 shows that with high probability, the initialization for the mth recovered factor has the largest weighted corre-
lation with a factor which has not been recovered so far after the orthogonalization step. It also shows that the separation
condition in Lemma 1 is satisﬁed for all remaining factors with probability (1 − log5 k/k1+(cid:15)).

Now, we combine the effects of the tensor power method step and the orthogonalization step for subsequent iterations to
show that that Xm,t converges to Am. Consider a tensor power method step followed by an orthogonalization step. By our
previous argument about the convergence of the tensor power method, if | ˆwiˆai,t−1| ≤ βt−1 i (cid:54)= m at some time (t − 1),
then | ˆwiˆai,t| ≤ β2
t−1 for i (cid:54)= m after a tensor power method step. Lemma 4 shows that the correlation of all factors
other than the mth factor is still small after the orthogonalization step, if it was small before. Combining the effect of the
t−1(1 + 1/k1+(cid:15))
orthogonalization step via Lemma 4, if | ˆwiˆai,t−1| ≤ βt−1 i (cid:54)= m for some time (t − 1), then | ˆwiˆai,t| ≤ β2
for i (cid:54)= m after both the tensor power method and the orthogonalization steps. By also using Lemma 5 for the initialization,
can now write the updated combined recursion analogous to Eq. A.3 and Eq. A.4, but which combines the effect of the
tensor power method step and the orthogonalization step.

(cid:12)
(cid:12)
β0 = max
(cid:12) ˆwiˆai,0
i(cid:54)=1
t (1 + 1/k1+(cid:15))
βt+1 = β2

(cid:12)
(cid:12)
(cid:12)

(A.6)

(A.7)

By the previous argument, |wi¯ai,t| ≤ βt. Note that β0 ≤ 1 − 4/k1+(cid:15) by Lemma 5. By expanding the recursion A.7, βt =
(β0(1+1/k1+(cid:15)))2t
. Hence βt ≤ 1/d in 2 log k+log log d steps as was the case for the analysis for the tensor power method.
This shows that the correlation of the estimate Xm,t with all factors other than Am becomes small in 2 log k + log log d.

Orthgonalized ALS for Tensor Decomposition

We now again use Lemma 2 to argue that this implies that the recovery error is small, i.e. (cid:107) Am − ˆAm (cid:107)2
|1 − ˆwm
wm

| ≤ O(1/d).

2≤ 10γk/d2 and

Hence we have shown that if the ﬁrst (m−1) factors have converged to Xi = Ai+ ˆ∆i where (cid:107) ˆ∆i (cid:107)2 ≤ 10γk/d2, ∀ i < m
then the mth factor converges to Xm = Am+ ˆ∆m where (cid:107) ˆ∆m (cid:107)2 ≤ 10γk/d2 in O(log k+log log d) steps with probability
at least

. This proves the induction hypothesis.

(cid:16)

(cid:17)

1 − log5 k
k1+(cid:15)

We can now do a union bound to argue that each factor converges with (cid:96)2 error at most O(γk/d2) in O(k(log k+log log d))
with overall failure probability at most ˜O(1/k−(cid:15)), (cid:15) > 0. This ﬁnishes the proof of convergence of Orth-ALS for the special
case of orthogonal tensors.

B. Global convergence of the tensor power method for incoherent tensors

In this section, we will analyze the tensor power method updates for worst-case incoherent tensors. This is a necessary
step before analyzing Orth-ALS, because as was pointed out in the proof of convergence of Orth-ALS in the orthogonal
tensor case, analyzing Orth-ALS updates reduces to analyzing a perturbed version of the tensor power method updates.
Our convergence results for the tensor power method are interesting independent of Orth-ALS though, as they prove global
convergence under random initialization. The proof idea is similar to the proof of convergence of the tensor power method
in the orthogonal case, but we now need to analyze the cross-terms which come in because the factors are no longer
orthogonal.
Theorem 3. Consider a d-dimensional rank k tensor T = (cid:80)k
i Aj| and
γ = wmax
If the initialization x0 ∈
wmin
Rd is chosen uniformly from the unit sphere, then with high probability the tensor power method updates converge to
one of the true factors (say A1) in O(log k + log log d) steps, and the error at convergence satisﬁes (cid:107) A1 − ˆA1 (cid:107)2
2 ≤
O(γk max{c2

be the ratio of the largest and smallest weight, and assume γcmax ≤ o(k−2).

i=1 wiAi ⊗ Ai ⊗ Ai. Let cmax = maxi(cid:54)=j |AT

| ≤ O(max{cmax, 1/d}).

max, 1/d2}) and |1 − ˆw1
w1

Proof. Without loss of generality, we will prove convergence to the ﬁrst factor A1. The proof is similar in spirit to the
proof of convergence of the tensor power method in the orthogonal case in Section A.

As in the orthogonal case, Lemma 1 states that with high probability there is some separation between the weighted
correlation of the largest and second largest factors.

Lemma 1. If γkcmax ≤ 1/k1+(cid:15) for some (cid:15) > 0, then with probability at least
∀ i (cid:54)= arg maxi |wiai,0|.

(cid:16)

1 − log5 k
k1+(cid:15)

(cid:17)

,

|wiai,0|

maxi |wiai,0| ≤ 1 − 5/k1+(cid:15)

We normalize all the correlations by the correlation of the largest factor, let ˆai,t+1 = ai,t
a1,t
the weight of the largest factor, ˆwi = wi
w1

. The new update equations in terms of the ratio of correlations ˆai,t become-

and normalize all the weights by

ˆai,t+1 =

ˆwiˆa2

i,t + ci,1 + (cid:80)
1 + (cid:80)

j:j(cid:54)={i,1} ci,j ˆwj ˆa2
j,t

j:j(cid:54)=1 c1,j ˆwj ˆa2
j,t

Notice that we have cross terms in Eq. B.1 as compared to Eq. A.2 in the orthogonal case, due to the correlation ci,j
between the factors being non-zero. The goal of the analysis for the non-orthogonal case is to bound these cross-terms
using the incoherence between the factors.

As in the orthogonal case, we will analyze all the correlations ˆai,t via a single recursion. We deﬁne βt in the non-orthogonal
case keeping in mind the cross-terms because of the correlations between the factors being non-zero.

(cid:12)
(cid:12)
(cid:12)wiˆai,0
β0 = max
i(cid:54)=1
βt+1 = γcmax + β2

(cid:12)
(cid:12)
(cid:12)
t + 3γkcmaxβ2
t

We now show that |wiˆai,t| ≤ βt, ∀ i (cid:54)= 1 and all t.

(B.1)

(B.2)

(B.3)

Lemma 6. If | ˆwiˆai,m| ≤ βm for some time m and for all i (cid:54)= 1, then at time (m + 1) for all i (cid:54)= 1,

Orthgonalized ALS for Tensor Decomposition

1. | ˆwiˆai,m+1| ≤ βm+1.

2. |ˆai,m+1 − ci,1| ≤ 2β2
m

Proof. Note that by Lemma 7, βt < 1 ∀ t =⇒ ˆwiˆa2
we can write,

i,m ≤ 1. Therefore (cid:80)

j |ci,j ˆwj ˆa2

j,m| ≤ kcmax ≤ 1/k1+(cid:15) ∀ i. Hence

1 + (cid:80)

j:j(cid:54)=1 c1,j ˆwj ˆa2

j,m

= 1 −

c1,j ˆwj ˆa2

j,m + (cid:15)1

(cid:88)

j:j(cid:54)=1

1

(cid:80)

(cid:12)
(cid:12)
(cid:12)

where (cid:15)1 is the residual term, and |(cid:15)1| ≤
ˆai,m+1 as-

j:j(cid:54)=1 c1,j ˆwj ˆa2

j,m

≤ k2c2

max ≤ 1/k2. We can now rewrite the updates for

2

(cid:12)
(cid:12)
(cid:12)

(cid:16)

ˆai,m+1 =

ci,1 + ˆwiˆa2

i,m +

(cid:88)

ci,j ˆwj ˆa2

j,m

(cid:17)(cid:16)

1 −

(cid:88)

c1,j ˆwj ˆa2

j,m + (cid:15)1

(cid:17)

j:j(cid:54)={i,1}

j:j(cid:54)=1

Let ρi,m = ci,1 + ˆwiˆa2

i,m + (cid:80)

j:j(cid:54)={i,1} ci,j ˆwj ˆa2

j,m. We can write,

ˆai,m+1 = ci,1 + ˆwiˆa2

i,m +

ci,j ˆwj ˆa2

j,m − ρi,m

c1,j ˆwj ˆa2

j,m + ρi,m(cid:15)1

=⇒

(cid:12)
(cid:12)
(cid:12)ˆai,m+1

(cid:12)
(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12)ci,1

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12) ˆwiˆa2
(cid:12)

i,m

ci,j ˆwj ˆa2

j,m

c1,j ˆwj ˆa2

j,m

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)ρi,m(cid:15)1

(cid:12)
(cid:12)
(cid:12)

(cid:88)

j:j(cid:54)=1

(cid:88)

j:j(cid:54)={i,1}
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)

(cid:88)

j:j(cid:54)={i,1}

(cid:88)

j:j(cid:54)=1
(cid:12)
(cid:12)
(cid:12)ρi,m

(cid:12)
(cid:12)
(cid:12) +

(B.4)

(B.5)

We claim that ρi,m ≤ 1. We verify this as follows,

ρi,m = ci,1 + ˆwiˆa2

i,m +

(cid:88)

ci,j ˆwj ˆa2

j,m

=⇒

(cid:12)
(cid:12)
(cid:12)ρi,m

(cid:12)
(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12)ci,1

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12) ˆwiˆa2
(cid:12)

i,m

j:j(cid:54)={i,1}
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)

(cid:88)

ci,j ˆwj ˆa2

j,m

(cid:12)
(cid:12)
(cid:12)

=⇒

(cid:12)
(cid:12)
(cid:12) ˆwiρi,m

(cid:12)
(cid:12)
(cid:12) ≤ γ

(cid:12)
(cid:12)
(cid:12)ci,1

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12) ˆw2
(cid:12)

i ˆa2

i,m

(cid:12)
(cid:12)
(cid:12) +

ci,j ˆwi ˆwj ˆa2

j,m

(cid:12)
(cid:12)
(cid:12)

j:j(cid:54)={i,1}
(cid:12)
(cid:12)
(cid:12)

(cid:88)

j:j(cid:54)={i,1}

≤ γcmax + β2
≤ βm+1 ≤ 1

m + γkcmaxβ2
m

=⇒

(cid:12)
(cid:12)
(cid:12)ρi,m

(cid:12)
(cid:12)
(cid:12) ≤ 1

where we used the fact that the weights lie in the interval [1, γ]. Hence |ρi,m| ≤ 1. Therefore, by Eq. B.5,

(cid:12)
(cid:12)
(cid:12)ˆai,m+1

(cid:12)
(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12)ci,1

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12) ˆwiˆa2
(cid:12)

i,m

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)

(cid:88)

ci,j ˆwj ˆa2

j,m

(cid:88)

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)

c1,j ˆwj ˆa2

j,m

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)(cid:15)1

(cid:12)
(cid:12)
(cid:12)

=⇒

(cid:12)
(cid:12)
(cid:12) ˆwiˆai,m+1

(cid:12)
(cid:12)
(cid:12) ≤ γ

(cid:12)
(cid:12)
(cid:12)ci,1

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12) ˆw2
(cid:12)

i ˆa2

i,m

(cid:12)
(cid:12)
(cid:12) +

ci,j ˆwi ˆwj ˆa2

j,m

c1,j ˆwi ˆwj ˆa2

j,m

(cid:12)
(cid:12)
(cid:12) + γ

(cid:12)
(cid:12)
(cid:12)(cid:15)1

(cid:12)
(cid:12)
(cid:12)

j:j(cid:54)={i,1}
(cid:12)
(cid:12)
(cid:12)

(cid:88)

j:j(cid:54)={i,1}

j:j(cid:54)=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)

(cid:88)

j:j(cid:54)=1

≤ γcmax + β2

m + 3γkcmaxβ2

m = βm+1

(B.6)

To show that |ˆai,m+1 − ci,1| ≤ 2β2

m we use Eq. B.4 and repeat the steps used to show that | ˆwiˆai,m+1| ≤ βm+1 ∀ t.

By using induction and Lemma 6, the iterates at all time t satisfy the following properties, for all i (cid:54)= 1,

Orthgonalized ALS for Tensor Decomposition

1. | ˆwiˆai,t| ≤ βt ∀ t.

2. |ˆai,t − ci,1| ≤ 2β2

t−1

This allows us to analyze the iterations of βt instead of keeping track of the different ai,t. We will now analyze the recursion
for βt. The following Lemma shows that βt becomes sufﬁciently small in O(log k + log log d) steps.

Lemma 7. βt ≤ 3γη ∀ t ≥ O(log k + log log d), also βt < 1 ∀ t

Proof. We divide the updates into three stages.

1. 0.1 ≤ βt ≤ 1 − 5/k1+(cid:15):

As βt ≥ 0.1, therefore kβ2

t ≥ 1 in this regime and hence γcmax ≤ γkcmaxβ2

t , and we can write-

We claim that βt < 0.1 for t = O(log d). To verify, note that-

βt+1 = γcmax + β2
βt+1 ≤ β2

t + 4γkcmaxβ2
t

t + 3γkcmaxβ2
t

βt ≤ (β0(1 + 4γkcmax))2t

(cid:16)

(cid:16)

≤

≤

(1 − 5/k1+(cid:15))(1 + 1/k1+(cid:15))
1 − 1/k1+(cid:15)(cid:17)2t

(cid:17)2t

where we used the fact that γkcmax ≤ 1/k1+(cid:15). Note that (1 − 1/k1+(cid:15))2t
this regime for at most 2 log k steps.

≤ 0.1 for t = 2 log k and hence we stay in

√

2.

γη ≤ βt ≤ 0.1 :

For notational convenience, we restart t from 0 in this stage. Because γcmax ≤ γη ≤ β2
t
3γkcmaxβ2

t as γkcmax ≤ 1/k1+(cid:15), we can write-

t ≤ 0.1β2

in this regime and

We claim that βt <

γη for t = O(log log γη−1). To verify, note that-

βt+1 = γcmax + β2

t + 3γkcmaxβ2
t
t ≤ 2.5β2
t

t + 0.3β2

≤ β2

t + β2

βt ≤ (2.5βt)2t

≤ (0.25)2t

√

√

Note that (0.25)2t
steps. As η−1 = O(d), this stage continues for at most O(log log d) steps.

≤

γη for t = O(log log(γη)−1) and hence we stay in this stage for at most O(log log(γη)−1)

3. Note that in the next step, βt ≤ γcmax + 1.1γη ≤ 3γη. This is again because 3γkcmaxβ2

t ≤ 0.1β2

t and βt ≤

the end of the previous stage.

(B.7)

(B.8)

√

γη at

Hence βt ≤ 3γη for t = O(log log d+log k). By Lemma 6, |ˆai,t −ci,1| ≤ 18γ2η2, i (cid:54)= 1. Hence |ˆai,t| ≤ 2η. By Lemma 2,
the error at convergence satisﬁes (cid:107) A1 − ˆA1 (cid:107)2 ≤ 10γkη2 and the estimate of the weight ˆw1 satisﬁes |1− ˆw1
w1

| ≤ O(η).

C. Global convergence of the tensor power method for random tensors

Orthgonalized ALS for Tensor Decomposition

The previous section gives global convergence guarantees for the tensor power method for incoherent tensors. Applying
Theorem 3 to a tensor whose factors are chosen uniformly at random, we can say that the tensor power method converges
with random initialization whenever the rank k = o(d0.25). Theorem 3 also proves a linear convergence rate. However,
this is quite suboptimal for random tensors. In this section, we use the randomness in the tensor to get much stronger
convergence results.

The techniques used in this section are very different from the rest of the paper. Instead of recursively analyzing the tensor
power method updates by showing that the algorithm makes progress at every step by boosting its correlation with some
ﬁxed factor, we directly express the correlation of the factors with the estimate Zτ after a ﬁxed number of τ = O(log log d)
time steps in terms of the initial correlations of the factors with the random initialization. This allows us to then skillfully
use the randomness in the factors to get strong results. The difﬁculty with the recursive approach is that all the randomness
in the tensor is “lost” after just one tensor power method update, i.e. the correlations of different factors with the estimate
are no longer independent of each other, which makes the analysis much more difﬁcult.
Theorem 2. Consider a d-dimensional rank k tensor T = (cid:80)k
i=1 wiAi ⊗ Ai ⊗ Ai with the factors Ai sampled uniformly
from the d-dimensional sphere. Deﬁne γ = wmax
to be the ratio of the largest and smallest weight. Assume k ≤ o(d)
wmin
and γ ≤ polylog(d). If the initialization x0 ∈ Rd is chosen uniformly from the unit sphere, then with high probability
the tensor power method updates converge to one of the true factors (say A1) in O(log log d) steps, and the error at
convergence satisﬁes (cid:107) A1 − ˆA1 (cid:107)2 ≤ ˜O(1/

d). Also, the estimate of the weight ˆw1 satisﬁes |1 − ˆw1
w1

| ≤ ˜O(1/

d).

√

√

Proof. Without loss of generality, we will prove convergence to the ﬁrst factor A1. Let τ = 5 log log d2. As before, deﬁne
ai,t = (cid:104)Ai, Zt(cid:105) where Zt is the iterate at time t. For the analysis of the tensor power method updates for random tensors
we ignore the normalization step of the updates, till the last iteration. It is easy to see that this makes no difference in the
analysis, though in practice it is important to normalize after every step to prevent the vectors from becoming too small
and causing numerical errors. Recall that the update equations for ai,t for any t are-

ai,t = wia2

i,t−1 + ci,1w1a2

1,t−1 +

(cid:88)

ci,jwja2

j,t−1

j:j(cid:54)={i,1}

(C.1)

and the iterate Xτ +1 at time τ + 1 can be written as

Zτ +1 = w1a2

1,τ A1 +

wia2

i,τ Ai

(cid:88)

i(cid:54)=1

On expanding w1a2
ατ = |w1a1,0|2τ
most log−1 d. We can write (1/ατ )w1a2

/w1. Let ∆τ = (1/ατ ) (cid:80)

1,τ recursively using Eq. C.1, one of the terms that appears in the expansion is (w1a1,0)2τ

i(cid:54)=1 wia2

i,τ Ai. We show that (cid:107) ∆τ (cid:107)2 ≤ ˜O(1/
1,τ , the coefﬁcient for ﬁrst factor A1 normalized by ατ , as follows

√

/w1. Deﬁne
d) with failure probability at

w1a2
1,τ
ατ

=

(w1a1,0)2τ
w1ατ
(cid:16)
1
ατ

+

(cid:16)

1
ατ

w1a2

1,τ −
(w1a1,0)2τ
w1

(cid:17)

= 1 +

w1a2

1,τ −

(w1a1,0)2τ
w1

(cid:17)

Let λτ = 1
ατ

(cid:16)

w1a2

1,τ − (w1a1,0)2τ

w1

(cid:17)

. Let Z (cid:48)

τ +1 = Zτ +1/ατ . We can write Z (cid:48)

τ +1 as

Z (cid:48)

τ +1 = (1 + λτ )A1 + ∆τ

Note that
We can bound (cid:107) ˜Zτ +1 − A1 (cid:107)2 as follows using the triangle inequality,

. Let

= Zτ +1
(cid:107)Zτ +1(cid:107)2

Z(cid:48)
τ +1
(cid:107)Z(cid:48)
τ +1(cid:107)2

Z(cid:48)
τ +1
(cid:107)Z(cid:48)
τ +1(cid:107)2

= ˜Zτ +1. We desire to show that the residual (cid:107) ˜Zτ +1 − A1 (cid:107)2 ≤ ˜O(1/

d).

√

(cid:107) ˜Zτ +1 − A1 (cid:107)2 ≤

(cid:12)
(cid:12)
(cid:12)

1 + λτ
(cid:107) (1 + λτ )A1 + ∆τ (cid:107)2

(cid:12)
(cid:12)
(cid:12) +
− 1

(cid:107) ∆τ (cid:107)2
(cid:107) (1 + λτ )A1 + ∆τ (cid:107)2

If (cid:107) ∆τ (cid:107)2 ≤ ˜O(1/

d) and |λτ | ≤ d−(cid:15) then,

√

Orthgonalized ALS for Tensor Decomposition

(cid:107) ˜Zτ +1 − A1 (cid:107)2 ≤

(cid:12)
(cid:12)
(cid:12)

1
(cid:107) A1 + ∆τ /(1 + λτ ) (cid:107)2
2(cid:107) ∆τ (cid:107)2
1 − |λτ |

(cid:107) ∆τ (cid:107)2
1 − |λτ | − (cid:107) ∆τ (cid:107)2

(cid:12)
(cid:12)
(cid:12) +
− 1

+

(cid:107) ∆τ (cid:107)2
1 − |λτ | − (cid:107) ∆τ (cid:107)2
√

≤ ˜O(1/

d)

If (cid:107) ˜Zτ +1 − A1 (cid:107)2 ≤ ˜O(1/

d) then, by Lemma 2, the estimate of the weight ˆw1 satisﬁes |1 − ˆw1
w1

√

√

| ≤ ˜O(1/

d).

Hence we will show that (cid:107) ∆τ (cid:107)2 ≤ ˜O(1/
We can write (cid:15)τ as

d) and |λτ | ≤ d−(cid:15) with failure probability at most log−1 d. Let (cid:15)τ = (cid:107) ∆τ (cid:107)2

2.

≤

√

We can also write λ2

τ as follows-

(cid:15)τ = (cid:107) ∆τ (cid:107)2

2 =

(1/α2

τ )wiwja2

i,τ a2

j,τ ci,j

(cid:88)

i(cid:54)=1,j(cid:54)=1

τ = (1/α2
λ2

τ )w2
1

a2
1,τ −

(cid:16)

(w1a1,0)2τ
w2
1

(cid:17)2

Note that λτ has the same form as (cid:15)τ with the restriction that i = j = 1 and the (w1a1,0)2τ
removed.

w2
1

in the expansion of a2

1,τ is

Our approach will be to recursively expand the a2
at time 0), the correlation between factors ci,j and the weights wi. We use the recursion Eq. C.1 to do this.

i,τ terms to express (cid:15)τ and λτ only in terms of ai,0 (the initial correlations

We ﬁrst consider the expansion of a2
correlations of the factors with the iterate at the (t − 1)st time step as follows using recursion Eq. C.1-

i,τ for any i using recursion Eq. C.1. a2

i,t can be written as a weighted sum of

(cid:16)

a2
i,t =

wia2

i,t−1 +

ci,jwja2

j,t−1

(cid:17)2

(cid:88)

j(cid:54)=i

(cid:88)

=

j,k

wjwkci,jci,ka2

j,t−1a2

k,t−1

(C.2)

(C.3)

Each term in the summation corresponds to two choices for the terms at time (t − 1), the j and k variables. Continuing
this recursive expansion for τ time steps, we can represent each monomial in the expansion by a complete binary tree with
depth τ . We label a node of the binary tree as j if it corresponds to factor Aj. For ease of exposition, we will consider
the initialization Z0 as the 0th factor for the graph representation, hence ci,0 = ai,0. The root of the tree is labeled as i as
it corresponds to the factor Ai. The descendants of the root i are labelled as j and k if a2
j,τ −1 and
a2
k,τ −1 using recursion Eq. C.1. The process is repeated at any step of the recursion, by expanding a2
and a2

l,t−1 for some k and l. Refer to Fig. 4 for an example of a monomial and it’s binary tree representation.

i,τ is expanded into a2

j,t in terms of a2

k,t−1

Given any complete binary tree B, the monomial associated with the tree can be written down recursively. We write down
the procedure for ﬁnding the monomial corresponding to a binary tree B explicitly in Algorithm 3 for clarity.

Therefore, by successively using Eq. C.1, we expand wia2
i,1 in terms of the correlations of the factors with the random
initialization Z0 (the a2
j,0 factors) and deﬁne a complete binary tree Bf for every monomial f in the expansion. We also
deﬁne a graph Gf for the monomial f by coalescing nodes of the binary tree having the same label and removing self-loops.
We allow more than one edge between two nodes.

For any monomial f in the expansion of (1/α2
j,τ ci,j in (cid:15)t, we construct two binary trees corresponding to
the expansion of wia2
j,τ . We construct the graph Gf by adding an edge between the roots of the two binary
trees (this corresponds to the ci,j term) and then coalescing nodes of the new graph having the same label and removing
self-loops, while allowing multiple edges between two nodes. The same procedure is followed for the expansion of λ2
τ ,
with the difference that now i = j = 1, and the (w1a1,0)2τ

term in the expansion of a2

i,τ and wja2

τ )wiwja2

i,τ a2

1,τ is removed.

w2
1

Orthgonalized ALS for Tensor Decomposition

Figure 4. Example of a monomial
ci,jci,jcj,lcj,mci,kck,nck,owiwjwlwmwkwnwoa2

in the expansion of wia2
i,2
m,0a2

n,0a2

l,0a2

o,0

represented as a binary tree.

The monomial

is

Algorithm 3 Finding monomial f from binary tree B
Input: Binary tree B, root u
monomial(B, u)
1: while u is not a leaf do
2:
3:
4:
5:
6:
7:
8: end while
9: return f

Set i to be the factor corresponding to u
Set v to be the left child of u, set j to be the factor corresponding to v
Set w to be the right child of u, set j to be the factor corresponding to w
f = f wi
f = f ci,jmonomial(B, v)
f = f ci,jmonomial(B, w)

C.1. Choosing a suitable basis for the factors

Without loss of generality, assume that the ﬁrst (n − 1) factors are present in Gf , for some n. The (n − 1) vectors
corresponding to the (n − 1) factors and the initialization Z0 span a n dimensional subspace. We will choose a particular
basis {vi}, i ∈ [n] for the n dimensional subspace and express the factors with respect to that basis. v1 = Z0, and vi
is unit vector along the projection of Ai−1 orthogonal to {Aj, j < i − 1}. In terms of this basis, Z0 = (1, 0, · · · , 0).
Let the 1st factor A1 have component x1,1 along the ﬁrst coordinate axis and u1,2 along the second coordinate axis.
Note that x1,1 is distributed as ˜x1,1
i=2 ˜y2
1,i
r1
1,1 + ˜u2
˜x2
1,2. Here ˜y1,i ∼ N (0, 1/d) and v1 is uniform on {−1, +1}. Similarly, the 2nd factor A2 has

and u1,2 is distributed as ˜u1,2
r1

where ˜x1,1 ∼ N (0, 1/d), ˜u1,2 ∼ v1

and r1 =

(cid:113)(cid:80)d

(cid:113)

components (x2,1, x2,2, u2,3) ∼

along the ﬁrst three coordinate axes. Here ˜x2,1, ˜x2,2 ∼ N (0, 1/d) and

(cid:113)(cid:80)d

˜u2,3 ∼ v2
continue this projection for all subsequent factors.

2,i and r2 =

i=3 ˜y2

(cid:16) ˜x2,1
r2
(cid:113)

(cid:17)

, ˜x2,2
, ˜u2,3
r2
r2
˜x2
2,1 + ˜x2
2,2 + ˜u2

2,3, where ˜y2,i ∼ N (0, 1/d) and v2 is uniform on {−1, +1}. We

We ﬁrst prove a Lemma that bounds the magnitude of the projection of any factor along the basis vectors.

Lemma 8. The projection of n factors along the basis deﬁned above has the following properties-

1. 1 − 1

d0.25 ≤ r2

i ≤ 1 + 1

d0.25 ∀ i ∈ [n] with failure probability at most 2ne−

d/8.

√

2. |˜xi,j| ≤ log5 d/

√

d for all valid i, j (i.e. for all j < i, i ∈ [n]) with failure probability at most n( 1

d )log8 d.

Proof. The proof relies on basic concentration inequalities.

1. Consider the vector (xi,1, · · · , ui,i+1, 0 · · · , 0) corresponding to factor i. The squared scaling factor r2
i,i+1 ∼ ˜y2

i is distributed
i,1 + · · · + ˜x2
i,i+1 + · · · + ˜y2
i,d, the ˜yi,j are independent N (0, 1/d) random
i is the sum of squares of independent zero mean Gaussian random variables each having variance 1/d,
i is a χ2 random variable with d degrees of freedom. We use the following tail bound on a χ2
i = dr2

i ∼ (˜x2
as r2
variables. r2
and hence x2

i,i+1), where ˜u2

i,i + ˜u2

Orthgonalized ALS for Tensor Decomposition

random variable x with d degrees of freedom (the bound follows from the sub-exponential property of the χ2 random
variable)

P[|x2 − d| ≥ dt] ≤ 2e−dt2/8

Choosing t = d−0.25, P[|x2
|r2

√
i − d| ≥ d0.75] ≤ 2e−
d0.25 ∀ i ∈ [n] with failure probability at most 2ne−

i − 1| ≤ 1

√

d/8.

d/8. Therefore P[|ri − 1| ≥ d−0.25] ≤ 2e−

d/8. By a union bound,

√

2. The bound follows directly from basic Gaussian tail bounds (refer to Eq. E.1) and a union bound.

Note that as τ = 5 log log d2, the total number of nodes of the binary tree corresponding to a monomial is at most
2τ +1 = 2 log5 d2. As each monomial corresponds to two binary trees, the number of number in the graph Gf can be at
most 4 log5 d2. Let N = 4 log5 d2. We can now use a union bound to argue that the properties of the factors in Lemma 8
hold with high probability for any set of N factors. We deﬁne β0 = max

and βt = β2t

0 for any t.

(cid:110)(cid:12)
(cid:12)
(cid:12)

wixi,1
w1x1,1

(cid:12)
(cid:111)
(cid:12)
(cid:12), i (cid:54)= 1

Lemma 9. Consider the projection of any set of N = 4 log5 d2 factors. With failure probability at most 1/dlog d, |xi,j| ≤
2 log3 d/
d for all valid i, j (i.e. for all j < i, i ∈ [N ]). Also, with failure probability at most 1/ log d, β0 ≤ 1 − 1/ log4 k.

√

Proof. Using Lemma 8 and a union bound, |˜xi,j| ≤ (log d)5/
failure probability at most N ( 1
d/8 ≤ 2N ( 1
whenever ˜xi,j ≤ log5 d/
by doing a union bound over all possible sets of N factors, |xi,j| ≤ 2 log5 d/
most 2dN N ( 1

d0.25 ∀ i ∈ [N ] with
√
d
d0.25 . Therefore, as the total number of sets of N factors is at most kN ≤ dN ,
d for all valid i, j with failure probability at

d )log8 d. As xi,j = ˜xi,j/ri, therefore xi,j ≤ 2 log5 d/

d for all valid i, j and |r2

d )log8 d + 2N e−

d and ri ≥ 1 − 1

i − 1| ≤ 1

√

√

√

d )log8 d ≤ 1/dlog d.

√

Using Lemma 14, with failure probability at most 1/ log d,

[k] with failure probability at most 2ke−
for all i (cid:54)= 1.

d/8, therefore with failure probability at most 2/ log d,

(cid:12)
(cid:12)
(cid:12)

wi ˜xi,1
w1 ˜x1,1

(cid:12)
(cid:12) ≤ 1 − 1/ log5 k for all i (cid:54)= 1. As |r2
i − 1| ≤ 1
(cid:12)
d0.25 ∀ i ∈
(cid:12)
(cid:12) ≤ 1−0.5/ log5 k
(cid:12)

wixi,1
w1x1,1

(cid:12)
(cid:12)
(cid:12)

√

Let E be the event that for any projection of up to N factors |xi,j| ≤ 2 log3 d/
d for all valid i, j (i.e. for all j < i, i ∈ [n])
and β0 ≤ 1 − 1/ log4 k. By Lemma 9, probability of the event E is at least (1 − 3/ log d). We condition on the event E
for the rest of the proof.

√

C.2. Characterizing when the monomial has non-zero expectation

Let f2 refer to the product of all a2
terms in f not present in f2, hence f = f1f2. Let G(cid:48)
initialization X0 and all it’s edges from Gf . Note that G(cid:48)
of the binary tree.

i,0 terms, all the weights wi for any i appearing in f and 1/α2

τ . Let f1 refer to all the
f be the graph obtained by removing the node corresponding to the
f is a connected graph, as the 0th factors only appears in the leaves

As the ci,j terms are inner products between the factors, we can write ci,j in terms of the co-ordinates of the vectors Ai and
Aj, in terms of the basis we described previously. Note that ai,0 = xi,1 hence there is only one term in the inner product
ai,0. f1 is a product of the cross-correlation terms ci,j, hence it can be written as the summation of a product of a choices
of coordinate for every ci,j term. Let the terms obtained on rewriting f1 in terms of the coordinates of the vectors be gi,
hence f1 = (cid:80)K

i=1 gi.

Lemma 10. f has non-zero expectation only if Gf is Eulerian. Also, every term gi having non-zero expectation corre-
sponds to choosing a split of G(cid:48)
f into a disjoint union of cycles and then choosing a single coordinate for all inner products
ci,j which are part of a particular cycle.

Proof. We claim that every node in Gf must have even degree for f to have non-zero expectation. To verify, consider any
node j which has odd degree. Note that the 0th node corresponding to the initialization X0 always has even degree, hence

Orthgonalized ALS for Tensor Decomposition

j (cid:54)= 0. E[f] is the expectation of the product of all correlation terms ci,j and ai,0 appearing in the monomial. Each inner
product ci,j involves a xi,t term or ui,t term for some coordinate t. Hence if node i has odd degree, then there is at least
some t such that xi,t or ui,t is raised to an odd power. Note that the sign of xi,t or ui,t is an independent zero mean random
variable, hence the expectation evaluates to 0 in this case. Hence every node in Gf must have even degree for f to have
non-zero expectation. By Euler’s theorem every node in a graph has even degree if and only if the graph is Eulerian (there
exists a trail in the graph which uses every edge exactly once and returns to its starting point). Also, an Eulerian graph can
be written as a disjoint union of cycles (Veblen’s theorem).

f is also Eulerian and can be written as a disjoint union of cycles as every node has an even number of edges to node 0

G(cid:48)
and hence removal of these edges preserves the Eulerian property.

We now prove the second part of the Lemma, that every term gi having non-zero expectation corresponds to choosing a
split of G(cid:48)
f into a disjoint union of cycles and then choosing a single coordinate for all inner products ci,j which are part of
a particular cycle. To verify this, let’s start at any node i and consider it’s inner product with a neighbor j. Say we choose
coordinate t for the inner product ci,j which leads to a xi,txj,t term in gi. To ensure that gi has non-zero expectation,
xj,t must appear in the term an even number of times (as the sign of xj,t is an independent zero mean random variable).
Hence the coordinate t must be chosen in the inner product of node j with some neighbor of j. By repeating this argument,
there must exist a cycle C with node i such that the coordinate t is chosen for all correlation terms in that cycle C. We
then repeat the process on the graph obtained by removing the edges corresponding to cycle C from G(cid:48)
f . Hence every gi
term having non-zero expectation corresponds to choosing a split of G(cid:48)
f into a disjoint union of cycles and then choosing
a single coordinate for all inner products ci,j which are part of a particular cycle.

1 = (cid:80)

i:E[gi](cid:54)=0 gi and f (cid:48) = f (cid:48)

1f2. We claim that E[f ] = E[f (cid:48)]. Consider any term gi, such that E[gi] = 0. We
We let f (cid:48)
claim that E[gif2] also equals 0, hence E[f ] = E[f (cid:48)]. This is because if gi has zero expectation, then there is some xi,t
term raised to an odd power, as otherwise the expectation is non-zero. But, as all terms are raised to an even power in f2,
the xi,t term is also raised to an odd power in gif2, which implies that E[gif2] = 0. This veriﬁes the claim that E[gif2] = 0
if E[gi] = 0.

C.3. Bounding expected value of monomial

f into some disjoint union of cycles. Say we split G(cid:48)

We are now ready to bound the expected value of f. Note that E[f] = 0 if Gf is not Eulerian. If Gf and hence G(cid:48)
f are Eu-
lerian, split G(cid:48)
f into p cycles {C1, C2, · · · , Cp} with m1, m2, · · · , mp
edges. Let D(Cj) refer to the choice of coordinate D(Cj) for cycle Cj. Let g(∪jCj(D(Cj))) be the term in the expansion
of f corresponding to a split of Gf into cycles {C1, C2, · · · , Cp} and the choice of coordinate D(Cj) for cycle Cj. We
also deﬁne h(Cj(D(Cj))) as the product of terms corresponding to cycle Cj and the choice of coordinate D(Cj) for the
cycle Cj. Note that g(∪jCj(D(Cj))) = Πp

j=1h(Cj(D(Cj))). We can write

g(∪jCj(D(Cj)))] = Πjh(Cj(D(Cj)))

h(Cj(D(Cj))) is the product of the square of the D(Cj)-th co-ordinate of all the factors appearing in the cycle Cj.
Conditioned on the event E, there is only one factor having a component greater than log5 d/
d in absolute value along
the D(Cj)-th co-ordinate axis, hence

√

Hence, conditioned on event E, we can bound g(∪jCj(D(Cj))) as-

h(Cj(D(Cj))) ≤

(log10 d)m1−1
dm1−1

g(∪jCj(D(Cj))) ≤

(log10 d)m/2
dm−p

Let c(Gf ) be the largest p such that G(cid:48)
f can be decomposed into a union of p disjoint cycles. There can be at most m/2
disjoint cycles in G(cid:48)
f as there are m edges, therefore c(Gf ) ≤ m/2. Each edge can be placed in one of the total number
f into a disjoint union of cycles is at most (m/2)m. Also,
of possible cycles, hence the total number of ways of splitting G(cid:48)
there are n possible choices for a coordinate for each cycle, hence there are at most n(m/2) terms corresponding to the same
split of G(cid:48)
f into a disjoint union of cycles. Hence for any particular monomial f , the number of possible gi terms having

Orthgonalized ALS for Tensor Decomposition

non-zero expectation is at most (m/2)mn(m/2). Note that m ≤ 2 log5 d2 as the graph G(cid:48)
f is constructed by collapsing the
two binary trees corresponding to monomial f . Each binary tree has depth τ = 5 log log d2, hence the number of edges is
at most 2 log5 d2. Hence the total number of edges in graph G(cid:48)
f is at most 4 log5 d2 ≤ log6 d. Hence we can bound E|E[f]
as-

E|E[f] ≤ f (cid:48) ≤ (m/2)mn(m/2) (log10 d)m/2
dm−c(Gf )

f2 ≤

(log10 d)5m/2
dm−c(Gf )

f2

(C.4)

(cid:110)(cid:12)
(cid:12)
(cid:12)

wixi,1
w1x1,1

(cid:111)

(cid:12)
(cid:12)
(cid:12)

over all nodes i ∈ G(cid:48)

f . Clearly θ ≤ β0 if node 1 is not in G and
We will now bound the f2 term. Let θ = max
is at most 1 otherwise. We will consider the representation of the monomial f as two complete binary trees. Recall that
the leaves of the binary tree correspond to the 0th factor. Each pair of leaves having the factor i as their parent corresponds
to a a2
i,0 term. We will pair every leaf node with it’s successor, regarding the binary tree as a binary search tree. Note that
the left child of any node has the same node as it’s successor. Let the right child of the node with factor i have a node with
factor j as it’s successor. We group the wi term due to the successor of the left child and wj term due to the successor of
the right child together with the a2
i,0 term by γ(wiai,0)2 whenever j (cid:54)= i and by (wiai,0)2
when j = i. If all the edges from the successor to the leaf are self-loops of the form cj,j, then j = i. Note that the paths of
all leaves of a binary tree to their successor are disjoint, hence each cross-correlation term ci,j, i (cid:54)= j can lead to at most
one leaf with j (cid:54)= i. The number of cross-correlation terms equals m, the number of edges in the graph G(cid:48)
f . Recall that
ατ = |w1a1,0|2τ
i,0 terms normalized by ατ is at most γmθ2τ
.

/w1. Therefore the product of all the wi and a2

i,0 term. We bound the wjwia2

As an example, consider the monomial f = w2
1(c1,2)4(a1,0)8. The binary tree Bf corresponding to f is given in
Fig. 6. Both binary trees are the same in this case. The graph Gf obtained by coalescing the two binary trees is given
in Fig. 5.

2w4

1. Projecting factors onto suitable basis: We can write the initialization Z0 as the vector (1, 0 · · · , 0). We write the
factor A1 as (x1,1, u1,2, 0, · · · , 0). Similarly, the 2nd factor A2 has components (x2,1, x2,2, u2,3). Using Lemma
9, max{|x1,1|, |x2,1|, |x2,2|} ≤ log5 d/

√

d.

2. Writing expectation of f as product of expectation of cycles: Let f1 = (c1,2)4. Let f2 = (a1,0)8 = (x1,0)8.
f can be expanded by choosing a coordinate corresponding to each c1,2 term, and then summing across all
choices. Let the terms obtained on rewriting f1 in terms of the co-ordinates of the factors A1 and A2 be gi,
hence f = (cid:80)K
j=1 gi. By Lemma 10, every term gi having non-zero expectation corresponds to choosing a split
of G(cid:48)
f into a disjoint union of cycles and then choosing a single coordinate for all inner products ci,j which are
part of a particular cycle. Say we split G(cid:48)
f into the union of cycles C1 and C2 where C1 and C2 are 2 edge
cycles between node 1 and node 2. Say we choose the 2nd coordinate for both the cycles C1 and C2. Following
the notation of subsection C.3, D(C1) = D(C2) = 2 and g(C1(2) ∪ C2(2)) is the term in the expansion of f
corresponding to split of G(cid:48)
f into cycles C1 and C2 and then choosing the second coordinate for both cycles.
1,2 ≤ log10 d/d2, again following the notation of subsection
g(C1(2) ∪ C2(2)) = h(C1(2))h(c2(2)) = x4
C.3. Recalling the deﬁnition of c(Gf ) be the largest p such that G(cid:48)
f can be decomposed into a union of p disjoint
cycles, for our example, c(Gf ) = 2. As each edge can be placed in one of the two possible cycles and there are 4
edges, the total number of ways of splitting G(cid:48)
f into a disjoint union of cycles is at most 24. There are 2 possible
choices for coordinates for each cycle as we have two factors. Hence we can bound f (cid:48) and E[f ] as -

1,2u4

E[f] ≤ f (cid:48) ≤ 2442 (log10 d)2

f2 ≤

d2

(log10 d)10
d2

f2

(C.5)

Orthgonalized ALS for Tensor Decomposition

Figure 5. Graph Gf for f = w2

2w4

1(c1,2)4(a1,0)8

Figure 6. Binary tree Bf for f = w2

2w4

1(c1,2)4(a1,0)8 (both binary trees for f are the same)

We are now ready to bound (cid:15)τ . We will divide (cid:15)τ into 2 sets of monomials and bound each one of them separately-

1. All monomials with root nodes i and j and with either no path from node i to node 1 or no path from node j to node

1. We call this set S1.

j to node 1. We call this set S2.

2. All monomials with root nodes i and j and at least two paths from node i to node 1 and at least two paths from node

Note that the number of paths between two nodes in the graph G(cid:48)
f is always even if f has non-zero expectation, as G(cid:48)
f
is Eulerian in that case. We need to relate the number of nodes and edges of an Eulerian graph for the rest of the proof,
Lemma 11 provides a simple bound.

Lemma 11. For any connected Eulerian graph G, let N be the number of nodes and M be the number of edges. Consider
any decomposition of G into a edge-disjoint set of p cycles. Then, N ≤ M − p + 1. Moreover, if G has four edge-disjoint
paths between a pair of nodes then N ≤ M − p.

We ﬁrst consider the set S1. As there are no paths from node i to node 1 or from node j to node 1, therefore θ ≤ β0 for
at least one of the binary trees. Therefore f2 ≤ γmβ2τ
0 = γmβτ . For any graph Gf with n nodes, there can be at most
kn ≤ dn monomials having a graph isomorphic to Gf as their representation. By Lemma 11, n ≤ m − c(Gf ) + 1. The
total number of graphs with n nodes and m edges is be at most (n2)m. As the graph G(cid:48)
f is connected, n ≤ m. Note that
the number of edges can be at most 4 log5 d2 ≤ log6 d. Hence we can bound the contribution of all monomials in the set
S1 as follows-

(cid:88)

E[f ] ≤

f :f ∈S1

km−c(Gf )+1 (m2)m(log10 d)5m/2γm

βt

dm−c(Gf )

log6 d
(cid:88)

m=0

log6 d
(cid:88)

m=0

1
d

1
d

log6 d
(cid:88)

m=0
log6 d
(cid:88)

m=0

≤

≤

≤

km−c(Gf )+1 (γ log55 d)m
dm−c(Gf )

1
d2

(γ log55 d)m(cid:16) k
d

(cid:17)m−c(Gf )

(γ log55 d)m(cid:16) 1
d(cid:15)

(cid:17)m/2

≤

1
d

∞
(cid:88)

m=0

(cid:16) γ log55 d
d0.5(cid:15)

(cid:17)m

≤

2
d

We next consider the set S2. For any graph Gf with n nodes with at least one of the nodes corresponding to factor A1,
there can be at most nkn−1 ≤ ndn−1 monomials having a graph isomorphic to Gf as their representation as there are n
possible positions to place the factor A1 and at most dn−1 ways to label the remaining nodes. We claim that by Lemma 11,
n ≤ m − c(Gf ). This is because there are two paths from node i to node 1 and two paths from node j to node 1. Note that

Orthgonalized ALS for Tensor Decomposition

there is always an edge between nodes i and j, as we connect the roots of the binary trees by an edge. Hence there are at
least three edge-disjoint paths between nodes i and j. But there cannot be an odd number of edge-disjoint paths between 2
nodes in an Eulerian graph, hence there must be at least four edge-disjoint paths between nodes i and j. Hence by Lemma
11, n ≤ m − c(Gf ). Also, note that the number of edges m ≥ 4 for monomials in S2 as there are two paths from node
i to node 1 and two paths from node j to node 1. Hence we can bound the contribution of all monomials in the set S2 as
follows-

(cid:88)

E[f ] ≤

f :f ∈S2

km−c(Gf )−1 m(m2)m(log10 d)5m/2γm

dm−c(Gf )

km−c(Gf )−1 (γ log55 d)m
dm−c(Gf )

(γ log55 d)m(cid:16) k
d

(cid:17)m−c(Gf )−1

log6 d
(cid:88)

m=4

log6 d
(cid:88)

m=4

1
d

1
d

1
d

log6 d
(cid:88)

m=4
log6 d
(cid:88)

m=4
log6 d
(cid:88)

m=4

≤

≤

≤

≤

(γ log55 d)m(cid:16) 1
d(cid:15)

(cid:17)m/2−1

≤

1
d

log6 d
(cid:88)

m=4

(γ log55 d)m(cid:16) 1
d(cid:15)

(cid:17)m/10

(cid:16) γ log55 d
d0.1(cid:15)

(cid:17)m

≤

1
d

∞
(cid:88)

m=4

(cid:16) γ log55 d
d0.1(cid:15)

(cid:17)m

≤

2
d

λτ is composed of monomials with at least one correlation (c1,i) term for i (cid:54)= 1. Also, all graphs for monomials corre-
sponding to the expansion of λτ must include a node with label A1. As before, for any graph Gf with n nodes with at least
one of the nodes corresponding to factor A1, there can be at most nkn−1 ≤ ndn−1 monomials having a graph isomorphic
to Gf as their representation. By Lemma 11, n ≤ m − c(Gf ) + 1. Hence we can bound λτ as follows,

E[λτ ] ≤

km−c(Gf ) m(m2)m(log10 d)5m/2γm

dm−c(Gf )

log6 d
(cid:88)

m=1
log6 d
(cid:88)

m=1
log6 d
(cid:88)

m=1
log6 d
(cid:88)

m=1
∞
(cid:88)

≤

≤

≤

≤

km−c(Gf ) (γ log55 d)m
dm−c(Gf )

(γ log55 d)m(cid:16) k
d

(cid:17)m−c(Gf )

(γ log55 d)m(cid:16) 1
d(cid:15)

(cid:17)m/2

≤

log6 d
(cid:88)

m=1

(cid:16) γ log55 d
d0.5(cid:15)

(cid:17)m

(cid:16) γ log55 d
d0.5(cid:15)

(cid:17)m

≤

1
d(cid:15)(cid:48)

(cid:104)
(cid:105)
P
(cid:15)τ ≥ log d/d
λτ ≥ log d/d(cid:15)(cid:48)(cid:105)

(cid:104)
P

≤ 4/log2 d

≤ 1/log2 d

√

m=1
for some (cid:15)(cid:48) > 0. We now use Markov’s inequality to get high probability guarantees

Hence we have shown that (cid:107) ∆τ (cid:107)2 ≤ ˜O(1/

d) and |λτ | ≤ d−(cid:15) with failure probability at most log−1 d.

D. Proof of convergence for Orth-ALS

The proof of convergence of Orth-ALS for incoherent tensors mirrors the proof for orthogonal tensors in Section A. For
clarity, we will try to stick to the proof for the orthogonal case as far possible, while also providing proofs for intermediate
Lemmas which were stated without proof in Section A.

Orthgonalized ALS for Tensor Decomposition

Theorem 1. Consider a d-dimensional rank k tensor T = (cid:80)k
i=1 wiAi ⊗ Ai ⊗ Ai. Let cmax = maxi(cid:54)=j |AT
i Aj| be the
incoherence between the true factors and γ = wmax
be the ratio of the largest and smallest weight. Assume γcmax ≤
wmin
o(k−2), and the estimates of the factors are initialized randomly from the unit sphere. Provided that, at the i(log k +
log log d)th step of the algorithm the estimates for all but the ﬁrst i factors are re-randomized, then with high probability
the orthogonalized ALS updates converge to the true factors in O(k(log k + log log d)) steps, and the error at convergence
satisﬁes (up to relabelling) (cid:107) Ai − ˆAi (cid:107)2

| ≤ O(max{cmax, 1/d}), for all i.

2≤ O(γk max{c2

max, 1/d2}) and |1 − ˆwi
wi

Proof. Without loss of generality, we assume that the ith recovered factor converges to the ith true factor. Note that
the iterations for the ﬁrst factor are the usual tensor power method updates and are unaffected by the remaining factors.
Hence by Theorem 3, Orth-ALS correctly recovers the ﬁrst factor O(log k + log log d) steps with probability at least
(1 − log5 k/k1+(cid:15)), for any (cid:15) > 0.

We now prove that Orth-ALS also recovers the remaining factors. The proof proceeds by induction. We have already
shown that the base case is correct and the algorithm recovers the ﬁrst factor. We next show that if the ﬁrst (m − 1) factors
have converged, then the mth factor converges in O(log k + log log d) steps with failure probability at most ˜O(1/k1+(cid:15)).
The main idea is that as the factors have small correlation with each other, hence orthogonalization does not affect the
factors which have not been recovered but ensures that the mth estimate never has high correlation with the factors which
have already been recovered. Recall that we assume without loss of generality that the ith recovered factor Xi converges
to the ith true factor, hence Xi = Ai + ˆ∆i for i < m, where (cid:107) ˆ∆i (cid:107)2 ≤ 10γkη2. This is our induction hypothesis, which
is true for the base case as by Theorem 3 the tensor power method updates converge with residual error at most 10γkη2.

Let Xm,t denote the mth factor estimate at time t and let Ym denote it’s value at convergence. We will ﬁrst calculate the
effect of the orthogonalization step on the correlation between the factors and the estimate Xm,t. Let { ¯Xi, i < m} denote
an orthogonal basis for {Xi, i < m}. The basis { ¯Xi, i < m} is calculated via QR decomposition, and can be written down
as follows,

j<i
Note that the estimate Xm,t is projected orthogonal to this basis. Deﬁne ¯Xm,t as this orthogonal projection, which can be
written down as follows –

¯Xi =

Xi − (cid:80)
(cid:107) Xi − (cid:80)

j<i

¯X T
j Xi ¯Xj
¯X T
j Xi ¯Xj (cid:107)2

¯Xm,t = ¯Xm,t −

¯X T

j Xm,t ¯Xj

(cid:88)

j<m

In the QR decomposition algorithm ¯Xm,t is also normalized to have unit norm but we will ignore the normalization of
Xm,t in our analysis because as before we only consider ratios of correlations of the true factors with ¯Xm,t, which is
unaffected by normalization.
We will now analyze the orthogonal basis { ¯Xi, i < m}. The key idea is that the orthogonal basis { ¯Xi, i < m} is close to
the original factors {Ai, i < m} as the factors are incoherent. Lemma 3 proves this claim.

Lemma 3. Consider a stage of the Orthogonalized ALS iterations when the ﬁrst (m − 1) factors have converged. Without
loss of generality let Xi = Ai + ˆ∆i, i < m, where(cid:107) ˆ∆i (cid:107)2 ≤ 10γkη2. Let { ¯Xi, i < m} denote an orthogonal basis for
{Xi, i < m} calculated using Eq. A.5. Then,

1. ¯Xi = Ai + ∆i, ∀ i < m and (cid:107) ∆j (cid:107)2 ≤ 10kη.
2. |AT

j ∆i| ≤ 3η, ∀ i < m, j < i.

3. |AT

j ∆i| ≤ 20γkη2, ∀ i < m, j > i.

Proof. We argue the result by induction. As the ﬁrst estimate converges to A1 + ˆ∆1 where (cid:107) ˆ∆1 (cid:107)2 ≤ 10γkη2, the base
case is correct. Assume that the result is true for the ﬁrst p − 1 vectors in the basis. After orthogonalization, the pth basis
vector has the following form-

¯Xp =

(Ap + ˆ∆p) −

((Ap + ˆ∆p)T ¯Xj) ¯Xj

(cid:17)

(cid:16)

1
κ

(cid:88)

j<p

Orthgonalized ALS for Tensor Decomposition

where κ is the normalizing factor which ensures (cid:107) ¯Xp (cid:107)2 = 1. Deﬁne µp,j = (AT
the induction hypothesis and |AT

p Aj| ≤ η by deﬁnition of η, |µp,j| ≤ 2η. Using the induction hypothesis, we can write

p (Aj + ∆j). As |AT

p ∆j| ≤ 20γkη2 by

κ ¯Xp = Ap −

AT

p (Aj + ∆j)

(Aj + ∆j) + ˆ∆p −

(cid:17)

(cid:88)

(cid:16) ˆ∆T

k<p

p (Aj + ∆j)

(Aj + ∆j)

(cid:17)

(cid:88)

(cid:16)

j<p

(cid:88)

j<p

= Ap −

µp,j(Aj + ∆j) + ˆ∆(cid:15)

where ˆ∆(cid:15) = ˆ∆p − (cid:80)
k<p
(cid:107) ˆ∆(cid:15) (cid:107)2 ≤ (cid:107) ˆ∆p (cid:107)2 ≤ 10γkη2. We can write-

p (Aj + ∆j)

(cid:16) ˆ∆T

(cid:17)

(Aj + ∆j). As ˆ∆(cid:15) is a projection of ˆ∆p orthogonal to the basis { ¯Xi, i < p},

κ ¯Xp = Ap −

(cid:88)

µp,jAj −

µp,j∆j + ˆ∆(cid:15)

(cid:88)

j<p

j<p
= Ap + ∆(cid:48)
p

where ∆(cid:48)

p = − (cid:80)

j<p µp,jAj − (cid:80)

j<p µp,j∆j + ˆ∆(cid:15). We bound (cid:107) ∆(cid:48)

p (cid:107)2 as follows-

(cid:107) ∆(cid:48)

p (cid:107)2 ≤

(cid:107) µp,jAj (cid:107)2 +

(cid:107) µp,j∆j (cid:107)2 + (cid:107) ˆ∆(cid:15) (cid:107)2

(cid:88)

j<p

(cid:88)

j<p

≤ 2kη + 20k2η2 + 10γkη2 ≤ 3kη

Note that κ = (cid:107) Ap + ∆(cid:48)
Therefore we can rewrite ¯Xp as-

p (cid:107)2 =⇒ 1 − 3kη ≤ κ ≤ 1 + 3kη by the triangle inequality. Hence 1 − 3kη ≤ 1/κ ≤ 1 + 6kη.

¯Xp =

1
κ

(Ap + ∆(cid:48)
p)
1
κ

= Ap + (1 −

)Ap +

= Ap + c1Ap + c2∆(cid:48)
p
= Ap + ∆p

1
κ

∆(cid:48)
p

where c1 = (1 − 1
(cid:107) ∆p (cid:107)2 ≤ 10kη.

κ ), c2 = 1

κ and ∆p = c1Ap + c2∆(cid:48)

p. Note that |c1| ≤ 6kη and 1 − 3kη ≤ c2 ≤ 1 + 6kη. Hence

We now show that |AT

i ∆p| ≤ 3η, i < p,

∆p = c1Ap + c2

−

µp,jAj −

(cid:16)

(cid:88)

µp,j∆j + ˆ∆(cid:15)

(cid:17)

(cid:88)

j<p

(cid:88)

j<p,j(cid:54)=i

j<p
(cid:12)
(cid:12)
(cid:12)

(cid:88)

j<p

=⇒

i ∆p| =

(cid:12)
(cid:12)AT
(cid:12)

(cid:12)
(cid:12)c1AT
(cid:12)

i Ap

(cid:12)
(cid:12)
(cid:12) + c2

µp,jAT

i Aj −

µp,jAT

i ∆j − µp,iAT

i ∆i + AT
i

ˆ∆(cid:15)

(cid:12)
(cid:12)
(cid:12)

Finally, we show that |AT

≤ 6kη2 + (1 + 6kη)(2η(1 + kη) + 6kη2 + 20kη2 + 10γkη2)
≤ 3η

i ∆p| ≤ 20γkη2, i > p,
(cid:12)
(cid:12)
(cid:12)AT
(cid:12)
(cid:12)
(cid:12) + c2

i ∆p| = c1

(cid:12)
(cid:12)AT
(cid:12)

i Ap

(cid:12)
(cid:12)
(cid:12)

(cid:88)

j<p

µp,jAT

i Aj −

µp,jAT

i ∆j + AT
i

ˆ∆(cid:15)

(cid:12)
(cid:12)
(cid:12)

(cid:88)

j<p

≤ 6kη2 + (1 + 6kη)(2kη2 + 40γk2η3 + 10γkη2)
≤ 20γkη2

Orthgonalized ALS for Tensor Decomposition

Using Lemma 3, we will ﬁnd the effect of orthogonalization on the correlations of the factors with the iterate Xm,t. At a
high level, we need to show that the iterations for the factors {Ai, i ≥ m} are not much affected by the orthogonalization,
while the correlations of the factors {Ai, i < m} with the estimate Xm,t are ensured to be small. Lemma 3 is the key tool
to prove this, as it shows that the orthogonalized basis is close to the true factors.
We will now analyze the inner product between ¯Xm,t and factor Ai. This is given by-

As earlier, we normalize all the correlations by the correlation of the largest factor, let ¯ai,t be the ratio of the correlations
of Ai and Am with the orthogonalized estimate ¯Xm,t at time t. We can write ¯ai,t as-

AT
i

¯Xm,t = AT

i Xm,t −

X T

m,t

¯XjAT
i

¯Xj

(cid:88)

j<m

¯ai,t =

i Xm,t − (cid:80)
AT
mXm,t − (cid:80)
AT

j<m X T
j<m X T

m,t

m,t

¯XjAT
i
¯XjAT
m

¯Xj
¯Xj

We can multiply both sides by ˆwi and substitute ¯Xj from Lemma 3 and then rewrite as follows-

ˆwi¯ai,t =

i Xm,t − (cid:80)
ˆwiAT
mXm,t − (cid:80)
AT

j<m ˆwiX T
j<m X T

m,t(Aj + ∆j)AT
m∆j

m,t(Aj + ∆j)AT

i ∆j

We divide the numerator and denominator by AT

mXm,t to derive an expression in terms of the ratios of correlations. Let

δi,t =

X T
X T

m,t∆j
m,tAm

.

ˆwi¯ai,t =

ˆwiˆai,t − (cid:80)
1 − (cid:80)

j<m( ˆwiˆaj,t + ˆwiδi,t)AT
j<m(ˆaj,t + δi,t)AT
m∆j

i ∆j

Lemma 12 upper bounds δi,t.
Lemma 12. Let | ˆwiˆai,t−1| ≤ βt−1 ∀ i (cid:54)= m and some time (t − 1). Also, let βt ≤ γη + β2
δi,t ≤ 40γkηβt.

t−1. Then for all i < m,

Proof. By the power method updates Xm,t =
correlation of the largest factor Am, hence the normalizing factor (cid:107) (cid:80)
We use Lemma 3 to bound |AT
i ∆j|. Hence,

(cid:80)
(cid:107)(cid:80)

i wiλiAi
i wiλiAi(cid:107)2

where λi = a2

i,t−1. Note that δi,j is normalized by the
i wiλiAi (cid:107)2 does not matter and we will ignore it.

(cid:12)
(cid:12)
(cid:12)

X T
X T

m,t∆j
m,tAm

(cid:12)
(cid:12)
(cid:12) ≤

(cid:80)
(cid:80)

|AT

i,t−1|AT
i,t−1AT

i ˆwiˆa2
i ˆwiˆa2
m∆j| + (cid:80)

i ∆j|
i Am
i(cid:54)=j,m ˆwiˆa2
1 + (cid:80)
t−1 + 10kηβ2

i,t−1|AT
i(cid:54)=m ci,m ˆwiˆa2

t−1

i,t−1

i ∆i| + ˆwiˆa2

j,t−1|AT

j ∆j|

=

≤

≤

20γkη2 + 3kηβ2

1 − γkηβ2
t−1)

t−1

kη(20γη + 13β2

1 − 0.5

≤ 40γkηβt

We now need to show ˆwi¯ai,t is small for all i < m and is close to ˆwiai,t, the weighted correlation before orthogonalization,
for all i > m. Lemma 4 proves this, and shows that the weighted correlation of factors which have not yet been recovered,
{Ai, i ≥ m}, is not much affected by orthogonalization, but the factors which have already been recovered. {Ai, i < m},
are ensured to be small after the orthogonalization step.

Orthgonalized ALS for Tensor Decomposition

Lemma 4. Let | ˆwiˆai,t| ≤ βt ∀ i (cid:54)= m at the end of the tth iteration. Let ¯ai,t be the ratio of the correlation of the ith and
the mth factor with Xm,t, the iterate at time t after the orthogonalization step. Then,

1. | ˆwi¯ai,t| ≤ βt(1 + 1/k1+(cid:15))), ∀ i > m.

2. | ˆwi¯ai,t| ≤ 50γkηβt, ∀ i < m.

Proof. We can bound ¯ai,t for all i ≥ m as-

(cid:12)
(cid:12)
(cid:12) ˆwi¯ai,t

(cid:12)
(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12) ˆwiˆai,t

(cid:12)
(cid:12)
(cid:12) ˆwiˆai,t

(cid:12)
(cid:12)
(cid:12) ˆwiˆai,t

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:80)
(cid:12)
1 −
(cid:12)
(cid:12)
(cid:12) + (cid:80)
(cid:12)
1 − (cid:80)
(cid:12)
(cid:12) + (cid:80)
(cid:12)
1 − (cid:80)

(cid:80)

j<m( ˆwiˆaj,t + ˆwiδj,t)AT

(cid:12)
(cid:12)
i (Aj + ∆j)
(cid:12)
(cid:12)
(cid:12)
m(Aj + ∆j)
(cid:12)
(cid:12)
(cid:12)
(cid:12)AT
m(Aj + ∆j)
(cid:12)
(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)AT
i (Aj + ∆j)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
m(Aj + ∆j)
(cid:12)

(cid:12)
(cid:12)
i (Aj + ∆j)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

j<m(ˆaj,t + δj,t)AT

j<m

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)( ˆwiˆaj,t + ˆwiδj,t)
(cid:12)
j<m
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)AT
(cid:12)(ˆaj,t + δj,t)
(cid:12)
(cid:12)
(cid:16)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ˆwiδj,t
(cid:12) ˆwiˆaj,t
(cid:12)
(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:16)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)AT
(cid:12) +
(cid:12)ˆaj,t
(cid:12)

(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)δj,t

j<m

j<m

≤

≤

(cid:12)
(cid:12)
(cid:12) ˆwi¯ai,t

1 + 8γkη
1 − 4kηβt

(cid:12)
(cid:12)
(cid:12) ≤ βt
≤ βt(1 + 8γkη)(1 + 8kηβt)
≤ βt(1 + 20γkη)
≤ βt(1 + 1/k1+(cid:15))

Note that | ˆwiˆai,t| ≤ βt, | ˆwiˆaj,t| ≤ γβt and | ˆwiδj,t| ≤ 40γkηβt. Also, |AT
can write,

i (Aj + ∆j)| ≤ 4η using Lemma 3. Hence we

Similarly, we can bound ¯ai,t for all i < m as-

j<m,j(cid:54)=i( ˆwiˆaj,t + ˆwiδj,t)AT

i (Aj + ∆j)

ˆwi¯ai,t =

ˆwiˆai,t − (cid:80)
1 − (cid:80)

j<m( ˆwiˆaj,t + ˆwiδj,t)AT
j<m(ˆaj,t + δj,t)AT

i (Aj + ∆j)

m(Aj + ∆j)
i (Ai + ∆i) − (cid:80)

ˆwiˆai,t − ( ˆwiˆai,t + ˆwiδj,t)AT
1 − (cid:80)

ˆwiδj,tAT

i (Ai + ∆i) − (cid:80)
1 − (cid:80)

j<m(ˆaj,t + δj,t)AT
j<m,j(cid:54)=i( ˆwiˆaj,t + ˆwiδj,t)AT

m(Aj + ∆j)

i (Aj + ∆j)

(cid:12)
(cid:12)
(cid:12) ˆwiδj,t

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)AT

j<m(ˆaj,t + δj,t)AT
(cid:12)
(cid:12) + (cid:80)
(cid:12)
(cid:16)(cid:12)
(cid:12)
(cid:12)ˆaj,t

(cid:16)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ˆwiˆaj,t
(cid:12) +
(cid:12)
(cid:12)
(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)AT
(cid:12)δj,t
(cid:12)

m(Aj + ∆j)
(cid:12)
(cid:12)
(cid:12) ˆwiδj,t
m(Aj + ∆j)

j(cid:54)=i
(cid:12)
(cid:12)
(cid:12) +

(cid:17)(cid:12)
(cid:12)
(cid:12)AT
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

j,m

i (Ai + ∆i)
1 − (cid:80)

(cid:12)
(cid:12)
i (Aj + ∆j)
(cid:12)

=

=

≤

≤

40γkηβt + 8γkηβt
1 − 4kηβt

≤ 50γkηβt

where we have again used the relations | ˆwiˆai,t| ≤ βt, | ˆwiˆaj,t| ≤ γβt, | ˆwiδj,t| ≤ 40γkηβt and |AT

i (Aj + ∆j)| ≤ 4η.

We are now ready to analyze the Orth-ALS updates for the mth factor. First, we argue about the initialization step. Lemma
4 shows that an orthogonalization step performed after the initialization ensures that the factors which have already been
recovered have small correlation with the orthogonalized initialization –

Orthgonalized ALS for Tensor Decomposition

Lemma 5. Let Xm,0 be initialized randomly and the result be projected orthogonal to the (m − 1) previously estimated
factors, let these be {Xi, i < m} without loss of generality. Then arg maxi |wiai,0| ≥ m with high probability. Also, with
(cid:12)
(cid:12)
(cid:12) ≤ 1 − 4/k1+(cid:15) ∀ i (cid:54)= arg maxi |wiai,0| after the orthogonalization
failure probability at most
step.

wi¯ai,0
maxi{wi¯ai,0}

1 − log5 k
k1+(cid:15)

(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:16)

,

Proof. We ﬁrst show that arg maxi |wiai,0| ≥ m. From Lemma 4, the ratio of the weighted correlation of all factors
{Ai, i < m} with the random initialization and the weighted correlation of all factors {Ai, i ≥ m} with the random
initialization is shrunk by a factor of O(k−(1+(cid:15))) after the orthogonalization step. Hence no factor {Ai, i < m} will have
maximum weighted correlation after the orthogonalization step.

(cid:16)

Lemma 1 can now be applied on all remaining factors, to get the initialization condition. Without loss of generality,
assume that arg max |wiai,0| = m. Consider the set of factors {Ai, m ≤ i ≤ n}. From Lemma 1, with prob-
(cid:12)
(cid:12)
(cid:12) ≤ 1 − 5/k1+(cid:15), (cid:15) > 0 ∀ i (cid:54)= 1. Applying Lemma 4 once more, | ˆwi¯ai,t| ≤
ability at least
(cid:17)
,

βt(1 + 1/k1+(cid:15)), ∀ i > m. Therefore combining Lemma 1 and Lemma 4, with failure probability at most
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ 1 − 4/k1+(cid:15) ∀ i (cid:54)= m after the orthogonalization step.

1 − log5 k
k1+(cid:15)

1 − log5 k
k1+(cid:15)

wi¯ai,0
wm¯am,0

wiai,0
wmam,0

(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:16)

,

Lemma 5 shows that with high probability, the initialization for the mth recovered factor has the largest weighted corre-
lation with a factor which has not been recovered so far after the orthogonalization step. It also shows that the separation
condition in Lemma 1 is satisﬁed for all remaining factors with probability (1 − log5 k/k1+(cid:15)).

Now, we combine the effects of the tensor power method step and the orthogonalization step for subsequent iterations to
show that that Xm,t converges to Am. Consider a tensor power method step followed by an orthogonalization step. By
Lemma 6, if | ˆwiˆai,t−1| ≤ βt−1 i (cid:54)= m at some time (t − 1), then | ˆwiˆai,t| ≤ (γcmax + β2
t−1) for i (cid:54)= m
after a tensor power method step. Lemma 4 shows that the correlation of all factors other than the mth factors is still small
after the orthogonalization step if it was small before. Combining the effect of the orthogonalization step via Lemma 4, if
| ˆwiˆai,t−1| ≤ βt−1 i (cid:54)= m for some time (t − 1), then | ˆwiˆai,t| ≤ (γcmax + β2
t−1)(1 + 1/k1+(cid:15)) for i (cid:54)= m
after both the tensor power method and the orthogonalization steps. By also using Lemma 5 for the initialization, can now
write the updated combined recursion analogous to Eq. B.2 and Eq. B.2, but which combines the effect of the tensor power
method step and the orthogonalization step.

t−1 + 3γkcmaxβ2

t−1 + 3γkcmaxβ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)wiˆai,0
β0 = max
(cid:12)
i(cid:54)=1
t + 3γkcmaxβ2
βt+1 = (γcmax + β2

t )(1 + 1/k1+(cid:15))

(D.1)

(D.2)

By the previous argument, |wi¯ai,t| ≤ βt. Note that β0 ≤ 1 − 4/k1+(cid:15) by Lemma 5
Lemma 13. βt ≤ 3γcmax ∀ t ≥ O(log k + log log d), also βt < 1 − 1/k1+(cid:15) ∀ t

Proof. The proof is very similar to the proof for Lemma 7. We divide the updates into three stages.

1. 0.1 ≤ βt ≤ 1 − 4/k1+(cid:15):

As βt ≥ 0.1 therefore kβ2

t ≥ 1 in this regime and hence γcmax ≤ γkβ2

t cmax, and we can write-

We claim that βt < 0.1 for t = 2 log k. To verify, note that-

βt+1 = (γcmax + β2
βt+1 ≤ (β2

t + 4γkcmaxβ2

t )(1 + 1/k1+(cid:15))

t + 3γkcmaxβ2

t )(1 + 1/k1+(cid:15))

βt ≤ (β0(1 + 4γ2kcmax)(1 + 1/k1+(cid:15)))2t

(cid:16)

(cid:16)

≤

≤

(1 − 4/k1+(cid:15))(1 + 1/k1+(cid:15))(1 + 1/k1+(cid:15))
1 − 1/k1+(cid:15)(cid:17)2t

(cid:17)2t

Orthgonalized ALS for Tensor Decomposition

This follows because γkcmax ≤ 1/k1+(cid:15). Note that (1 − 1/k1+(cid:15))2t
regime for at most 2 log k steps.

≤ 0.1 for t = 2 log k and hence we stay in this

√

2.

γη ≤ βt ≤ 0.1 :

For notational convenience, we restart t from 0 in this stage. Because γcmax ≤ γη ≤ β2
t
3γkβ2

t as γkcmax ≤ 1/k1+(cid:15), we can write-

t cmax ≤ 0.1β2

in this regime and

βt+1 = (γcmax + β2

t + 4γkcmax β2

t

)(1 + 1/k1+(cid:15))

t + β2

t + 0.1β2

t )(1 + 1/k1+(cid:15))

≤ (β2
≤ 2.5β2
t

We claim that βt <

γη for t = O(log log(γη)−1). To verify, note that-

βt ≤ (2.5(1 + O(log−2 k))βt1)2t

≤ (0.25)2t

√

√

Note that (0.25)2t
steps. As η−1 = O(d), this stage continues for at most O(log log d) steps.

≤

γη for t = O(log log(γη)−1) and hence we stay in this stage for at most O(log log(γη)−1)

3. Note that in the next step, βt ≤ (γcmax + 1.1γη)(1 + 1/k1+(cid:15)) ≤ 3γη. This is again because 3γ2kβ2

t η ≤ 0.1β2

t and

√

βt ≤

γη at the end of the previous stage.

Therefore βt ≤ 3γη for some t = O(log log d + log k). By Lemma 6, |ˆai,t − ci,1| ≤ 18γ2η2, i (cid:54)= 1. Hence |ˆai,t| ≤ 2η.
By Lemma 2, the error at convergence satisﬁes (cid:107) Am − ˆAm (cid:107)2
2≤ 10γkη2 and the estimate of the weight ¯wm satisﬁes
|1 − ˆwm
wm

| ≤ O(η).

Hence we have shown that if the ﬁrst (m−1) factors have converged to Xi = Ai+ ˆ∆i where (cid:107) ˆ∆i (cid:107)2 ≤ 10γk/d2, ∀ i < m
then the mth factor converges to Xm = Am+ ˆ∆m where (cid:107) ˆ∆m (cid:107)2 ≤ 10γk/d2 in O(log k+log log d) steps with probability
at least

. This proves the induction hypothesis.

(cid:16)

(cid:17)

1 − log5 k
k1+(cid:15)

We can now do a union bound to argue that each factor converges with (cid:96)2 error at most O(γk/d2) in O(log k + log log d)
with overall failure probability at most ˜O(1/k−(cid:15)), (cid:15) > 0.

E. Proof of additional Lemmas

In this section, we will prove the initialization condition which we used at several points in the proof of convergence of
the tensor power method and Orth-ALS updates. We also provide the proof for a few Lemmas whose proofs were omitted
earlier.
Lemma 1. If γkcmax ≤ 1/k1+(cid:15) for some (cid:15) > 0, then with probability at least
∀ i (cid:54)= arg maxi |wiai,0|.

maxi |wiai,0| ≤ 1 − 5/k1+(cid:15)

1 − log5 k
k1+(cid:15)

|wiai,0|

(cid:17)

(cid:16)

,

Proof. Without loss of generality, assume arg maxi |wiai,0| = 1. We will ﬁrst express all factors in terms of a par-
ticular choice of orthonormal basis vectors {vi}, i ∈ [k]. v1 = A1, and vi is unit vector along the projection of Ai
orthogonal to {Aj}, j < i. In terms of this basis, A1 = (1, 0, · · · , 0), let A2 = (x1,2, u2,2, 0 · · · , 0) and in general
Ai = (xi,1, xi,2, · · · , xi,i−1, ui,i, 0, · · · , 0). We will show that |xi,j| ≤ O(cmax) for all valid i, j i.e. for all j < i, i ∈ [k].

We claim that |xi,j| ≤ cmax(1 + jcmax) for all valid i, j. We prove this via induction on j.
It is clear that xi,1 ≤
cmax(1+cmax) for all valid i as (cid:104)Ai, A1(cid:105) ≤ cmax, i (cid:54)= 1. The induction step is that xi,j ≤ cmax(1+pcmax) for all valid i and
j ≤ p. We show that this implies that xi,p+1 ≤ cmax(1+(p+1)cmax) for all valid i. Note that |(cid:104)Ai, Ap+1(cid:105)| ≤ cmax ∀ i ≥ p

therefore,

Orthgonalized ALS for Tensor Decomposition

|up+1,p+1xi,p+1| ≤ cmax +

max(1 + icmax)2
c2

p
(cid:88)

i=1

p
(cid:88)

≤ cmax + c2

max

(1 + 4icmax)

≤ cmax + pc2

i=1
max + 4p2c3

max

From the induction hypothesis, |up+1,p+1| ≥ 1 − 2kc2
(cid:80)
max. Hence,

max =⇒ |up+1,p+1| ≥ 1 − 2kc2

i,j ≤ 4c2

j x2

max. This is because |xi,j| ≤ cmax(1 + jcmax) ≤ 2cmax =⇒

max + 4p2c3
|xi,p+1| ≤ (cmax + pc2
max + 4p2c3
≤ (cmax + pc2
≤ cmax + pc2
max + 4k2c3
≤ cmax(1 + (p + 1)cmax)

max)(1 − 2kc2
max)(1 + 4kc2
max + 4kc3

max)−1
max)
max + 4k2c4

max + 16k3c5

max

Therefore |xi,j| ≤ cmax(1 + jcmax) ≤ 2cmax for all valid i, j. Let the random initialization be (t1/r, t2/r, · · · , tk/r)
(cid:12)
1 − 10 log4 k
where ti ∼ N (0, 1/d) and r = (cid:80)
(cid:12)
(cid:12) ≤
,
(cid:12)
(cid:12)
1 − 10/k1+(cid:15), (cid:15) > 0 ∀ i (cid:54)= 1. We claim that
(cid:12) ≤ 1 − 10/k1+(cid:15), (cid:15) >
0 ∀ i (cid:54)= 1. This follows because-

i . Let ui = witi. By Lemma 14 with probability at least
(cid:12)
(cid:12)
(cid:12) ≤ 1 − 5/k1+(cid:15), (cid:15) > 0 ∀ i (cid:54)= 1 whenever

wiai,0
w1a1,0

witi
w1t1

i t2

k1+(cid:15)

ti
t1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:16)

=

wiai,0
w1a1,0
(cid:12)
wiai,0
(cid:12)
(cid:12) ≤
w1a1,0

(cid:12)
(cid:12)
(cid:12)

witiui,i + (cid:80)

j<i xi,jwjtj

wi
wj

(cid:12)
(cid:12)
(cid:12)

witi
w1t1

(cid:12)
(cid:12)
(cid:12) +

w1t1

(cid:88)

2cmax

(cid:12)
(cid:12)
(cid:12)

witi
w1t1

(cid:12)
(cid:12)
(cid:12)γ

=⇒

j<i
≤ 1 − 10/k1+(cid:15) + 2γkcmax
≤ 1 − 10/k1+(cid:15) + 1/k1+(cid:15)
≤ 1 − 5/k1+(cid:15)

Lemma 14. Let ui ∼ N (0, w2
1 − log4 k
|wiui|
at least

(cid:16)

(cid:17)

,

h

maxi |wiui| ≤ 1 − 1/h for all i (cid:54)= arg maxi |wiai,0|.

i ), i ∈ [k] be independent Gaussian random variables. For log4 k ≤ h ≤ k2, with probability

Proof. We refer to the pdf of ui by fi(x). Without loss of generality, assume arg maxi |wiai,0| = 1. As we are only
interested in the ratio of the absolute value of random variables {ui}, we will assume without loss of generality that the
standard deviations or the weights wi have been scaled such that wi ≥ 1. We will use the following tail bound on the
standard Gaussian random variable x (refer to (Duembgen, 2010))-

Let κ be some variable which satisﬁes the following relation–

Let m = maxi ui. As the ui are independent,

e−t2/2
√
2π

√

4
4 + t2 + t

≤ P[|x| > t] ≤

2e−t2/2
√
2π
t

1 −

3 log k
k

≤

(cid:80)k

i=1

P[|ui| ≤ κ]

k

≤ 1 −

2 log k
k

P[m > κ] = 1 − ΠiP[|ui| ≤ κ]

(E.1)

(E.2)

By the AM-GM inequality-

Orthgonalized ALS for Tensor Decomposition

ΠiP[|ui| ≤ κ] ≤

(cid:16) (cid:80)k

i=1

P[|ui| ≤ κ]

(cid:17)k

(cid:16)

≤

1 −

k
2 log k
k

(cid:17)k

≤

1
k2

Hence with failure probability at most 1/k2 the maximum is at least κ.

Instead of drawing k samples from the k distributions corresponding to the k factors, we ﬁrst draw the maximum m from
the distribution of the maximum of the k samples, and then draw the remaining samples conditioned on the maximum
being m. We have shown that m > κ with high probability. We condition on the maximum m > κ. We now show that
with high probability no sample lies in the range [m(1 − 1/h), m)], given that the maximum is at least κ. After drawing
the maximum from its distribution, we will draw samples from the distributions corresponding to all the k factors even
though one of the factors would already be the maximum m. Clearly this can only increase the probability of a sample
lying in the interval [m(1 − 1/h), m], and as we only want an upper bound this is ﬁne. Let the conditional pdf of the
ith random variable ui conditioned on the maximum being m be gi|m(x). Conditioned on the maximum being m, all
remaining samples are at most m and hence gi|m(x) = fi(x)/P[|ui| ≤ m] for all x ≤ m and is 0 otherwise. We will now
upper bound 1/P[|ui| ≤ m]. We rely on the following observation about the distribution of a standard Normal random
variable x-

The bound for t ∈ [0, 1] follows from the concavity of the Gaussian cumulative distribution function for t > 0, the bound
for t > 1 is easily veriﬁed. Using this, we can write

We will now ﬁnd a upper bound on fi(m(1 − 1/k)), the pdf of the samples at m(1 − 1/h). Let ti = m
wi

. Using Eq. E.1-

P[|x| ≤ t] ≥

(cid:40)

0.5t
0.5

t ∈ [0, 1]
t > 1

P[|ui| ≤ m] ≥ 0.5 min

(cid:111)

, 1

(cid:110) m
wi

fi(m) ≤

=⇒ gi|m(m) ≤

(cid:88)

=⇒

gi|m(m) ≤

i:ti≤log k

P[|ui| ≥ m]

(cid:16)(cid:112)4 + t2

(cid:17)

i + ti
4wi

(2ti + 2)P[|ui| ≥ κ]
4wiP[|ui| ≤ m]
(2ti + 2)P[|ui| ≥ κ]
(cid:111)
2wi min
(ti + 1)P[|ui| ≥ κ]
min{m, wi}
(cid:88)

(cid:110) m
wi

, 1

≤

≤

(ti + 1)P[|ui| ≥ κ]
m/ log k

i:ti≤log k
2 log2 k (cid:80)
P[|ui| ≥ κ]
i
m

≤

≤ 6 log3 k/m

(E.3)

where we used Eq. E.2 in the last step. We will now relate gi|m(m) and gi|m(m(1 − 1/h)). We can write,

Orthgonalized ALS for Tensor Decomposition

gi|m(m(1 − 1/h)) =

(1 − 2/h + 1/k2+2(cid:15))

− m2
2w2
i
√
2πwiP[|ui| ≤ m]

(cid:17)

exp

(cid:16)

(cid:16)

(cid:17)

et2

i /h

exp
√

≤

(cid:16)

=

√

− t2

i (1 − 2/h)/2
2πwiP[|ui| ≤ m]
i /2
(cid:17)
2πwiP[|ui| ≤ m]
i /h

e−t2

= gi|m(m)et2
(cid:88)

gi|m(m)et2

i /h

i:ti≤log k
≤ 20 log3 k/m

(cid:88)

=⇒

i:ti≤log k

gi|m(m(1 − 1/h)) ≤

where we used Eq. E.3 in the last step. For all i : ti > log k, we can write,

gi|m(m(1 − 1/h)) ≤

√

Therefore,

Hence the probability of a sample lying in the interval [m(1 − 1/h), m] can be bounded by-

P[∪(ui ∈ [m(1 − 1/h), m])] ≤

P[ui ∈ [m(1 − 1/h), m]]

e−t2
i (1/2−1/h)
2πwiP[|ui| ≤ m]
i /3

2e−t2

≤

√

i /3

2π min{m, wi}
2e−t2
√
2πm/ti
i /3

2tie−t2
√
2πm

≤

≤

≤ 1/(k2m)

(cid:88)

i:ti>log k
(cid:88)

=⇒

i

gi|m(m(1 − 1/h)) ≤ 1/(km)

gi|m(m(1 − 1/h)) ≤ 25 log3 k/m

gi|m(m(1 − 1/h))

m
h

(cid:88)

gi|m(m(1 − 1/h))

(cid:88)

i
(cid:88)

i
m
h

≤

=

≤

i
25 log3 k
h

Hence with probability at least (1 − 1
in the interval [m(1 − 1/h), m].

k2 )(1 − 25 log3 k

h

) = 1 − log4 k

h

the maximum is greater than κ and there are no samples

Lemma 11. For any connected Eulerian graph G, let N be the number of nodes and M be the number of edges. Consider
any decomposition of G into a edge-disjoint set of p cycles. Then, N ≤ M − p + 1. Moreover, if G has four edge-disjoint
paths between a pair of nodes then N ≤ M − p.

Orthgonalized ALS for Tensor Decomposition

Proof. Consider any decomposition of G into a disjoint set of p cycles C1 ∪ C2 · · · ∪ Cp. We will consider the number of
unique nodes in C1 ∪ C2 · · · ∪ Ct for t ≤ p. Let N (C1 ∪ C2 · · · ∪ Ct) be the number of unique nodes in C1 ∪ C2 · · · ∪ Ct.
Similarly, let M (C1 ∪ C2 · · · ∪ Ct) be the number of edges in C1 ∪ C2 · · · ∪ Ct. Note that N (C1) ≤ M (C1). There
must be a cycle in C2 · · · ∪ Cp with at least one node common to C1 as G is connected. Assume C2 has this property.
Then N (C1 ∪ C2) ≤ M (C1) + M (C2) − 1 as C1 and C2 have one node in common. Repeating this argument, at any
stage when we have selected t cycles, there must be a cycle which has not been selected yet but has a node common to the
selected cycles. Hence N (C1 ∪ C2 · · · ∪ Cp) ≤ (cid:80)p
1 M (Ci) − p + 1.

To prove the second part of the Lemma, we claim that if N = M − p + 1 for some decomposition C of G into a disjoint
set of p cycles, then there cannot be more than two disjoint paths between any pair of nodes. By our previous argument, if
N = M − p + 1 then for any union S of connected cycles, any cycle not in S can have at most one node common with the
nodes in S. Note that the number of disjoint set of paths between any pair u and v must be even as the graph is Eulerian.
Assume for the sake of contradiction that there are at least four disjoint paths between two nodes u and v. Consider any set
of cycles S in C which cover any two of the disjoint paths. Say that P is some path which is not covered by S. Then, there
must exist nodes s and t such that s and t are present in C but a segment of the path P is not present in C. We claim that
this implies that for some union S (cid:48) of cycles such that there is some cycle having two nodes common with S (cid:48). To verify
this, we simply add cycles to S to grow our subgraph from node s till it reaches node t. At some point, there must be a
cycle with two nodes common to the cycles already selected, because we have to reach the node t which has already been
included.

Lemma 2. Let γkcmax ≤ 1/k1+(cid:15). Without loss of generality assume convergence to the ﬁrst factor A1. Deﬁne
ˆai,t = | ai,t
If ˆai,t ≤ 2η ∀ i (cid:54)= 1,
a1,t
then (cid:107) A1 − ˆA1 (cid:107)2
2≤ O(γkη2) the relative error in the
estimation of the weight w1 is at most O(η).

2 ≤ 10γkη2 in the subsequent iteration. Also, if (cid:107) A1 − ˆA1 (cid:107)2

|- the ratio of the correlation of the ith and 1st factor with the iterate at time t.

Proof. Consider any step τ of the power iterations at the end of which |ˆai,τ | ≤ 2η ∀ i (cid:54)= 1. Let the ﬁrst (largest) factor
have true correlation a1,τ with the iterate at this time step. Consider the next tensor power method update. From the update
formula, the result Zτ +1 is-

Let κ =

w1a2
1,τ
1,τ ˆa2
i=1 w1 ˆwia2

(cid:107)(cid:80)k

i,τ Ai(cid:107)2

. Hence the estimate at the end of the mth iteration is-

Denote (cid:80)
unit norm. As (cid:107) Xτ +1 (cid:107)2 = 1, κ = 1/(cid:107) A1 + ˆ∆1 (cid:107)2. From the triangle inequality,

i,τ Ai = ˆ∆1. Note that (cid:107) ˆ∆1 (cid:107)2 ≤ 4γkη2 as |ˆai,τ | ≤ 2η =⇒ ˆwiˆa2

i(cid:54)=1 ˆwiˆa2

i,τ ≤ 4γη2 and the factors Ai have

Zτ +1 =

(cid:80)k
(cid:107) (cid:80)k

i=1 w1 ˆwia2
i=1 w1 ˆwia2

1,τ ˆa2
1,τ ˆa2

i,τ Ai
i,τ Ai (cid:107)2

Xτ +1 = κ

ˆwiˆa2

i,τ Ai

k
(cid:88)

i=1

= κ(A1 +

ˆwiˆa2

i,τ Ai)

(cid:88)

i(cid:54)=1

1 − (cid:107) ˆ∆1 (cid:107)2 ≤ (cid:107) A1 + ˆ∆1 (cid:107)2 ≤ 1 + (cid:107) ˆ∆1 (cid:107)2

=⇒ 1 − 4γkη2 ≤ (cid:107) A1 + ˆ∆1 (cid:107)2 ≤ 1 + 4γkη2

1

=⇒

1 + 4γkη2 ≤ κ ≤

1
1 − 4γkη2
=⇒ 1 − 4γkη2 ≤ κ ≤ 1 + 5γkη2

We can now write the error (cid:107) A1 − Xτ +1 (cid:107)2 as-

Orthgonalized ALS for Tensor Decomposition

(cid:107) A1 − Xτ +1 (cid:107)2 = (cid:107) A1 − κ(A1 + ˆ∆1) (cid:107)2
= (cid:107) A1(1 − κ) + κ ˆ∆1 (cid:107)2
≤ (cid:107) A1(1 − κ) (cid:107)2 + κ(cid:107) ˆ∆1 (cid:107)2
≤ 5γkη2 + 4γkη2(1 + 5γkη2)
≤ 10γkη2

We also show that the error in estimating the weight w1 of factor A1 is small once we have good estimate of the factor.

¯w1 =

wi(cid:104) ˆA1, Ai(cid:105)3

=

wi(cid:104)A1 + ˆ∆1, Ai(cid:105)3

(cid:88)

i
(cid:88)

i

≤ 3w1η + 8

(cid:88)

wiη3

i
≤ 3w1η + 8wikη3 ≤ 4w1η

=⇒ |w1 − ¯w1| ≤

(cid:12)
(cid:12)w1(cid:104)A1 + ˆ∆1, A1(cid:105)3 − w1
(cid:12)

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)

wi(cid:104)A1 + ˆ∆1, Ai(cid:105)3(cid:12)
(cid:12)
(cid:12)

(cid:88)

i(cid:54)=1

(E.4)

=⇒

(cid:12)
(cid:12)
(cid:12)1 −

¯w1
w1

(cid:12)
(cid:12)
(cid:12) ≤ O(η)

where Eq. E.4 follows as |AT
i

ˆ∆1| ≤ (cid:107) ˆ∆1 (cid:107)2 ≤ η.

