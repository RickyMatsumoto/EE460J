Supplementary materials for
Stochastic modiﬁed equations and the dynamics
of stochastic gradient algorithms

A Modiﬁed equations in the numerical analysis of PDEs

The method of modiﬁed equations is widely applied in ﬁnite difference methods in
numerical solution of PDEs Hirt (1968); Noh & Protter (1960); Daly (1963); Warming
& Hyett (1974). In this section, we brieﬂy demonstrate this classical method. Consider
the one dimensional transport equation

∂u
∂t

= c

∂u
∂x

where u : [0, T ] × [0, L] → R represents a density of some material in [0, L] and
c > 0 is the transport velocity. It is well-known that the simple forward-time-central-
space differencing leads to instability for all discretization step-sizes (LeVeque, 2002).
Instead, more sophisticated differencing schemes must be used.

We set time and space discretization steps to ∆t and ∆x and denote u(n∆t, j∆x) =
Un,j for 1 ≤ n ≤ N and 1 ≤ j ≤ J. The simplest scheme that can exhibit stability is
the upwind scheme (Courant et al., 1952), where we approximate (1) by the difference
equation

Un+1,j = Un,j + ∆t

(cid:18)
c+ Un,j+1 − Un,j
∆x

+ c− Un,j − Un,j−1

(cid:19)

,

∆x

where c+ = max(c, 0) + c− = min(c, 0). The idea is to now approximate this differ-
ence scheme by another continuous PDE, that is not equal to the original equation (1)
for non-zero ∆x, ∆t. This can be done by Taylor expanding each term in (2) around
u(t, x) = Un,j. Simplifying and truncating to leading term in ∆t, ∆x, we obtain the
modiﬁed equation

∂u
∂t

∂u
∂x

1
2

− c

=

c∆x(1 − r)

∂2u
∂x2 ,

where r = c∆t/∆x is the Courant-Friedrichs-Lewy (CFL) number (Courant et al.,
1952). Notice that in the limit ∆t, ∆x → 0 with r ﬁxed, one recovers the original

(1)

(2)

(3)

1

transport equation, but for ﬁnite step sizes, the upwind scheme is really described by
the modiﬁed equation (3). In other words, this truncated equation describes the leading
order, non-trivial behavior of the ﬁnite difference scheme.

From the modiﬁed equation (3), one can immediately deduce a number of interest-
ing properties. First, the error from the upwind scheme is diffusive in nature, due to
the presence of the second order spatial derivative on the right hand side. Second, we
observe that if the CFL number r is greater than 1, then the coefﬁcient for the diffu-
sive term becomes negative and this results in instability. This is the well-known CFL
condition. This places a fundamental limit on the spatial resolution for ﬁxed tempo-
ral resolution with regards to the stability of the algorithm. Lastly, the error term is
proportional to ∆x for ﬁxed r, thus it may be considered a ﬁrst order method.

Now, another possible proposal for discretizing (1) is the Lax-Wendroff (LW) scheme (Lax

& Wendroff, 1960):

Un+1,j = Un,j + ∆t

(cid:18)

c

Un,j+1 − Un,j−1
2∆x

− c∆t

Un,j+1 − 2Un,j + Un,j−1
2∆x2

(cid:19)

,

(4)

whose modiﬁed equation is

∂u
∂t

∂u
∂x

1
6

− c

=

c∆x2(r2 − 1)

∂3u
∂x3 .

(5)

Comparing with (3), we observe that the LW scheme error is of higher order (∆x2), but
at the cost of introducing dispersive, instead of diffusive errors due to the presence of
the third derivative. These ﬁndings are in excellent agreement with the actual behavior
of their respective discrete numerical schemes (Warming & Hyett, 1974).

We stress here that if we simply took the trivial leading order, the right hand sides of
(3) and (5) disappear and vital information, including stability, accuracy and the nature
of the error term will be lost. The ability to capture the effective dynamical behavior of
ﬁnite difference schemes is the key strength of the modiﬁed equations approach, which
has become the primary tool in analyzing and improving ﬁnite difference algorithms.
The goal of our work is to extend this approach to analyze stochastic algorithms.

B Summary of SDE terminologies and results

Here, we summarize various SDE terminologies and results we have used throughout
the main paper and also subsequent derivations. A particular important result is the
Itˆo formula (Sec. B.2), which is used throughout this work for deriving moment equa-
tions. For a thorough reference on the subject of stochastic calculus and SDEs, we
suggest Oksendal (2013).

B.1 Stochastic differential equations

Let T > 0. An Itˆo stochastic differential equation on the interval [0, T ] is an equation
of the form

dXt = b(Xt, t)dt + σ(Xt, t)dWt,

X0 = x0,

(6)

2

where Xt ∈ Rd, b : Rd × [0, T ] → Rd, σ : Rd × [0, T ] → Rd×l and Wt is a l-
dimensional Wiener process, or Brownian motion. This is a mathematically sound way
of expressing the intuitive notion of SDEs being ODEs plus noise:

˙Xt = b(Xt, t) + “noise”

The equation (6) is really a “shorthand” for the integral equation

Xt − x0 =

b(Xs, s)ds +

σ(Xs, s)dWs.

(cid:90) t

0

(cid:90) t

0

The last integral is deﬁned in the Itˆo sense, i.e.

(cid:90) t

0

FsdWs := lim
n→∞

(cid:88)

[si−1,si]∈πn

Fsi−1(Wsi − Wsi−1 ),

(7)

(8)

(9)

where πn is a sequence of n-partitions of [0, t] and the limit represents convergence in
probability. In (6), b is known as the drift, and σ is known as the diffusion matrix. When
they satisfy Lipschitz conditions, one can show that (6) (or (8)) has a unique strong
solution (Oksendal (2013), Chapter 5). For our purposes in this paper, we consider the
special case where b, σ do not depend on time and we set d = l so that σ is a square
matrix.

To perform calculus, we need an important result that generalizes the notion of

chain rule to the stochastic setting.

B.2

Itˆo formula

Itˆo formula, also known as Itˆo’s lemma, is the extension of the chain rule of ordinary
calculus to the stochastic setting. Let φ ∈ C 2,1(Rd × [0, T ]) and let Xt be a stochastic
process satisfying the SDE (6), and thus (8). Then, the stochastic process φ(Xt, t) is
again an Itˆo process satisfying

dφ(Xt, t) =

∇φ(Xt, t)T b(Xt, t) +

Tr[σ(t, Xt)T Hφ(t, Xt)σ(t, Xt)]

dt

(cid:18)

(cid:20)
∂tφ(Xt, t) +
+ (cid:2)∇φ(Xt, t)T σ(Xt, t)(cid:3) dWt,

1
2

(cid:19)(cid:21)

(10)

where ∇ denotes gradient with respect to the ﬁrst argument and Hφ denotes the Hes-
sian, i.e. Hφ(ij) = ∂2φ/∂x(i)∂x(j). The formula (10) is the Itˆo formula. If φ is not
a scalar but a vector, then each of its component satisfy (10). Note that if σ = 0, this
reduces to the chain rule of ordinary calculus.

B.3 The Ornstein-Uhlenbeck process

An important solvable SDE is the Ornstein-Uhlenbeck (OU) process Uhlenbeck &
Ornstein (1930). Consider d = 1, b(x, t) = θ(ξ − x) and σ(x, t) = σ > 0, with θ > 0,
σ > 0 and ξ ∈ R. Then we have the SDE

dXt = θ(ξ − Xt)dt + σdWt,

X0 = x0.

(11)

3

To solve this equation, we change variables x (cid:55)→ φ(x, t) = xeθt. Applying Itˆo formula,
we have

dφ(Xt, t) = θξeθtdt + σeθtdWt,

which we can integrate from 0 to T to get

Xt = x0e−θt + ξ(1 − e−θt) + σ

e−θ(t−s)dWs.

(cid:90) t

0

This is a path-wise solution to the SDE (11). To infer distributional properties, we
do not require such precise solutions.
In fact, we only need the distribution of the
random variable Xt at any ﬁxed time t ∈ [0, T ]. Observe that Xt is really a Gaussian
process, since the integrand in the Wiener integral is deterministic. Hence, we need
only calculate its moments. Taking expectation on (13), we get

EXt = x0e−θt + ξ(1 − e−θt).

(14)

To obtain the covariance function, we see that

E(Xt − EXt)(Xs − EXs) = σ2E

eθ(u−s)dWu

eθ(v−s)dWv

.

(15)

(cid:20)(cid:90) t

0

(cid:90) t

0

(cid:21)

This can be evaluated by using Itˆo’s isometry, which says that for any Wt adapted
process φt, ψt, we have

(cid:20)(cid:90) t

E

0

φudWu

ψvdWv

= E

(cid:21)

(cid:20)(cid:90) t

(cid:21)
φsψsds

.

(cid:90) t

0

0

We get, for s ≤ t

cov(Xs, Xt) =

(cid:16)

e−θ|t−s| + e−θ|t+s|(cid:17)

,

σ2
2θ

and in particular, for ﬁxed t ∈ [0, T ], we have

Var(Xt) =

(1 − e−2θt).

σ2
2θ

(12)

(13)

(16)

(17)

(18)

Hence, we have

(cid:18)

Xt ∼ N

x0e−θt + ξ(1 − e−θt),

(1 − e−2θt)

.

(19)

σ2
2θ

(cid:19)

In Sec. 3.1 in the main paper, the solution of the SME is the OU process with θ =
2(1 + η), ξ = 0, σ = 2

√

η. Making these substitutions, we obtain
1 − e−4(1+η)t(cid:17)(cid:19)

x0e−2(1+η)t,

(cid:18)

(cid:16)

.

η
1 + η

Xt ∼ N

4

B.4 Numerical solution of SDEs

Unfortunately, most SDEs are not amenable to exact solutions. Often, we resort to
numerical methods. The simplest method is the Euler-Maruyama method. This extends
the Euler method for ODEs to SDEs. Fix a time discretization size δ > 0 and deﬁne
˜Xk = Xkδ, then we can iterate the ﬁnite difference equation

˜Xk+1 = ˜Xk + δb( ˜Xk, kδ) + σ( ˜Xk, kδ)(W(k+1)δ − Wkδ).

(20)

By deﬁnition, W(k+1)δ − Wkδ ∼ N (0, δI), and are independent for each k. Here, I is
the identity matrix. Hence, we have the Euler-Maruyama scheme

˜Xk+1 = ˜Xk + δb( ˜Xk, kδ) +

δσ( ˜Xk, kδ)Zk,

(21)

√

where Zk

i.i.d.∼ N (0, I).

One can show that the Euler-Maruyama method (21) is a ﬁrst order weak approx-
imation (c.f. Def. 1 in main paper) to the SDE (6). However, it is only a order 1/2
scheme in the strong sense (Kloeden & Platen, 2011), i.e.

E|Xkδ − ˜Xkδ| < Cδ

1
2 .

(22)

With more sophisticated methods, one can design higher order schemes (both in the
strong and weak sense), see Milstein (1986).

B.5 Stochastic asymptotic expansion

Besides numerics, if there exists small parameters in the SDE, we can proceed with
stochastic asymptotic expansions Freidlin et al. (2012). This is the case for the SME,
which has a small η1/2 multiplied to the noise term. Let us consider a time-homogeneous
SDE of the form

dX (cid:15)

t = b(X (cid:15)

t )dt + (cid:15)σ(X (cid:15)

t )dWt

where (cid:15) (cid:28) 1. The idea is to follow standard asymptotic analysis and write X (cid:15)
asymptotic series

t as an

X (cid:15)

t = X0,t + (cid:15)X1,t + (cid:15)2X2,t + . . . .

We substitute (24) into (23) and assuming smoothness of b and σ, we expand

to get

b(cid:15)(X (cid:15)
σ(X (cid:15)

t ) = b(X0,t) + (cid:15)∇b(X0,t)X1,t + O((cid:15)2)
t ) = σ(X0,t) + (cid:15)∇σ(X0,t)X1,t + O((cid:15)2)

dX0,t = b(X0,t)dt,
dX1,t = ∇b(X0,t)X1,tdt + σ(X0,t)dWt,

...

5

(23)

(24)

(25)

(26)

and X0,0 = x0, X1,0 = 0. In general, the equation for Xi,t are linear stochastic differ-
ential equations with time-dependent coefﬁcients depending on {X0,t, X1,t, . . . , Xi−1,t}
and the initial conditions are X0,0 = x0, Xi,0 = 0 for all i ≥ 1. Hence, the asymptotic
equations can be solved sequentially to obtain an estimate of Xt to arbitrary order in (cid:15).
The equations for higher order terms become messy quickly, but they are always linear
in the unknown, as long as all the previous equations are solved. For more details on
stochastic asymptotic expansions, the reader is referred to Freidlin et al. (2012).

B.6 Asymptotics of the SME

η,
We now derive the ﬁrst two asymptotic equations of the SME. we take (cid:15) =
b = −∇f (O(η) term can be ignored for ﬁrst two terms) and σ = Σ1/2. Then, (26)
becomes

dX0,t = −∇f (X0,t)dt,

dX1,t = −Hf (X0,t)X1,tdt + Σ(X0,t)

2 dWt,

1

where Hf(ij) = ∂(i)∂(j)f is the Hessian of f .

In the following analysis, we shall assume that the truncated series approximation

ˆXt = X0,t +

ηX1,t,

√

where X0,t, X1,t satisfy (27) and (28), describes the leading order stochastic dynamics
of the SGD. Now, let us analyze the asymptotic equations in detail. First, we assume
that the ODE (27) has a unique solution X0,t, t ≥ 0 with X0,0 = x0. This is true if for
example, ∇f is locally Lipschitz. Next, let us deﬁne the non-random functions

Ht = Hf (X0,t),
Σt = Σ(X0,t).

Both H and σ are d×d matrices for each t. Then, (28) becomes the time-inhomogeneous
linear SDE

dX1,t = −HtX1,t + Σ

(31)
with X1,0 = 0. Since the drift is linear and the diffusion matrix is constant (i.e. inde-
pendent of X1,t), X1,t is a Gaussian process. Hence we need only calculate its mean
and covariance using Itˆo formula (see B.2). We have

t dWt,

1
2

and the covariance matrix St = Cov(X1,t) satisﬁes the differential equation

EX1,t = 0,

d
dt

St = −StHt − HtSt + Σt,

with S0 = 0. This equation is a linearized version of the Riccati equation and there are
simple closed-form solutions under special conditions, e.g. d = 1 or Ht is constant.

Hence, we conclude that the asymptotic approximation ˆXt is a Gaussian process

with distribution

ˆXt ∼ N (X0,t, ηSt),
(34)
where X0,t solves the ODE (27) and St solves the ODE (33), with Ht, Σt given by (30).

√

(27)

(28)

(29)

(30)

(32)

(33)

6

Remark 1. At this point, it is important to discuss the validity of the asymptotic ap-
proximation (34), and the SME approximation (35) in general. What we prove in Sec. C
and is shown in Freidlin et al. (2012) is that for ﬁxed T , we can take η = η(T ) small
enough so that the SME and its asymptotic expansion is a good approximation of the
distribution of the SGD iterates. What we did not prove is that for ﬁxed η, the approx-
imations hold for arbitrary T . In particular, it is not hard to construct systems where
for ﬁxed η, both the SME and the asymptotic expansion fails when T is large enough.
To prove the second general statement requires further assumptions, particularly on
the distribution of fi’s. This is out of the scope of the current work.

C Formal Statement and proof of Thm. 1

Theorem 1 (Stochastic modiﬁed equations). Let α ∈ {1, 2}, 0 < η < 1, T > 0 and
set N = (cid:98)T /η(cid:99). Let xk ∈ R, 0 ≤ k ≤ N denote a sequence of SGD iterations deﬁned
by (2). Deﬁne Xt ∈ Rd as the stochastic process satisfying the SDE

dXt = −∇(f (Xt) +

(α − 1)η|∇f (Xt)|2)dt + (ηΣ(Xt))

1

2 dWt

(35)

1
4

X0 = x0 and Σ(x) = 1
n

i=1(∇f (x) − ∇fi(x))(∇f (x) − ∇fi(x))T .

(cid:80)n

Fix some test function g ∈ G (c.f. Def. 1 in main paper). Suppose further that the

following conditions are met:

(i) ∇f, ∇fi satisfy a Lipschitz condition: there exists L > 0 such that

|∇f (x) − ∇f (y)| +

|∇fi(x) − ∇fi(y)| ≤ L|x − y|.

(ii) f, fi and its partial derivatives up to order 7 belong to G.

(iii) ∇f, ∇fi satisfy a growth condition: there exists M > 0 such that

|∇f (x)| +

|∇fi(x)| ≤ M (1 + |x|).

(iv) g and its partial derivatives up to order 6 belong to G.

Then, there exists a constant C > 0 independent of η such that for all k = 0, 1, . . . , N ,
we have

|Eg(Xkη) − Eg(xk)| ≤ Cηα.

That is, the equation (35) is an order α weak approximation of the SGD iterations.

The basic idea of the proof is similar to the classical approach in proving weak con-
vergence of discretization schemes of SDEs outlined in the seminal papers by Milstein
(Milstein (1975, 1979, 1986, 1995)). The main difference is that we wish to establish
that the continuous SME is an approximation of the discrete SGD, instead of the other

n
(cid:88)

i=1

n
(cid:88)

i=1

7

way round, which is the case dealt by classical approximation theorems of SDEs with
ﬁnite difference schemes. In the following, we ﬁrst show that a one-step approximation
has order ηα+1 error, and then deduce, using the general result in Milstein (1986), that
the overall global error is of order ηα.

It is well known that a second order weak convergence discretization scheme for
a SDE is not trivial. The classical Euler-Maruyama scheme, as well as the Milstein
scheme are both ﬁrst order weak approximations. However, in our case the problem
simpliﬁes signiﬁcantly. This is because the noise we are trying to model is small, so
that from the outset, we may assume that b(x) = O(1) but σ(x) = O(η1/2), i.e. we
set σ(x) = η1/2 ˜σ(x) where ˜σ = O(1) and deduce the appropriate expansions. For
brevity, in the following we will drop the tilde and simply denote the noise term of the
SDE by η1/2σ.

In the subsequent proofs we will make repeated use of Taylor expansions in powers
of η. To simplify presentation, we introduce the shorthand that whenever we write
O(ηα), we mean that there exists a function K(x) ∈ G (c.f. Def. 1 in main text) such
that the error terms are bounded by K(x)ηα. For example, we write

b(x + η) = b0(x) + ηb1(x) + O(η2)

to mean: there exists K ∈ G such that

|b(x + η) − b0(x) − ηb1(x)| ≤ K(x)η2.

(36)

(37)

These results can be deduced easily using Taylor’s theorem with a variety of forms of
the remainder, e.g. Lagrange form. We omit such routine calculations. We also denote
the partial derivative with respect to x(i) by ∂(i).

First, let us prove a lemma regarding moments of SDEs with small noise.

Lemma 1. Let 0 < η < 1. Consider a stochastic process Xt, t ≥ 0 satisfying the SDE

dXt = b(Xt) + η

2 σ(Xt)dWt

1

(38)

with X0 = x ∈ Rd and b, σ together with their derivatives belong to G. Deﬁne the
one-step difference ∆ = Xη − x, then we have

(i) E∆(i) = b(i)η + 1

2 [(cid:80)d

j=1 b(j)∂(j)b(i)]η2 + O(η3).

(ii) E∆(i)∆(j) = [b(i)b(j) + σσT

(ij)]η2 + O(η3).

(iii) E (cid:81)s

j=1 ∆(ij ) = O(η3) for all s ≥ 3, ij = 1, . . . , d.

All functions above are evaluated at x.

Proof. One way to establish (i)-(iii) is to employ the Ito-Taylor expansion (see Kloeden
& Platen (2011), Chapter 5) on the random variable Xη around x and calculating the
moments. Here, we will employ instead the method of semigroup expansions (see Hille
& Phillips (1996), Chapter XI), which works directly on expectation functions. The

8

generator of the stochastic process (38) is the operator L acting on sufﬁciently smooth
functions φ : Rd → R, and is deﬁned by

Lφ =

b(i)∂(i)φ +

σσT

(ij)∂(i)∂(j)φ,

(39)

d
(cid:88)

i=1

1
2

η2

d
(cid:88)

i,j=1

A classical result on semigroup expansions (Hille & Phillips (1996), Chapter XI) states
that if φ and its derivatives up to order 6 belong to G, then

Eφ(Xη) = φ(x) + Lφ(x)η +

L2φ(x)η2 + O(η3).

1
2

Now, let t ∈ Rd and consider the moment-generating function (MGF)

M (t) = Eet·∆.

To ensure its existence we may instead set t to be purely imaginary, i.e. t = is where s
is real. Then, (41) is known as the characteristic function (CF). The important property
we make use of is that the moments of ∆ are found by differentiating the MGF (or CF)
with respect to t. In fact, we have

(40)

(41)

(42)

s
(cid:89)

E

j=1

∆(ij ) =

∂sM (t)
j=1 ∂t(ij )

(cid:81)s

(cid:12)
(cid:12)
(cid:12)
(cid:12)t=0

,

where ij = 1, . . . , d. We now expand M (t) in powers of η using formula (40). We get,

M (t) =1 +

b(i)t(i)η +

b(i)t(j)∂(i)b(j)η2









d
(cid:88)

i=1





1
2

d
(cid:88)

i=1

1
2

d
(cid:88)

i,j=1

1
2

d
(cid:88)

i,j=1

+

η2(

b(i)t(i))2 +

σσT

(ij)t(i)t(j)


 + O(η3).

(43)

All functions are again evaluated at x. Finally, we apply formula (42) to deduce (i)-
(iii).

Next, we have an equivalent result for one SGD iteration.

Lemma 2. Let 0 < η < 1. Consider xk, k ≥ 0 satisfying the SGD iterations

with x0 = x ∈ Rd. Deﬁne the one-step difference ¯∆ = x1 − x, then we have

xk+1 = xk − η∇fγk (xk)

(44)

(i) E ¯∆(i) = −∂(i)f η
(ii) E ¯∆(i)
(iii) E (cid:81)s

j=1

¯∆(j) = ∂(i)f ∂(j)f η2 + Σ(ij)η2.

¯∆a(ij ) = O(η3) for all s ≥ 3, ij = 1, . . . , d.

9

where Σ = 1
n

(cid:80)n

i=1(∇f − ∇fi)(∇f − ∇fi)T . All functions above are evaluated at x.

Proof. From deﬁnition (44) and the deﬁnition of Σ, the results are immediate.

Now, we will need a key result linking one step approximations to global approx-
imations due to Milstein. We reproduce the theorem, tailored to our problem, below.
The more general statement can be found in Milstein (1986).

Theorem 2 (Milstein, 1986). Let α be a positive integer and let the assumptions in
Theorem 1 hold. If in addition there exists K1, K2 ∈ G so that

|E

s
(cid:89)

j=1

s
(cid:89)

j=1

∆(ij ) − E

¯∆(ij )| ≤ K1(x)ηα+1,

for s = 1, 2, . . . , 2α + 1 and

Then, there exists a constant C so that for all k = 0, 1, . . . , N we have

2α+2
(cid:89)

E

j=1

| ¯∆(ij )| ≤ K2(x)ηα+1.

|Eg(Xkη) − Eg(xk)| ≤ Cηα

Proof. See Milstein (1986), Theorem 2 and Lemma 5.

Proof of Theorem 1

We are now ready to prove theorem 1 by checking the conditions in theorem 2 with
α = 1, 2. The second condition is implied by Lemma 2. The ﬁrst condition is implied
by Lemma 1 and Lemma 2 with the choice

b(x) = −∇(f (x) +

η(α − 1)|∇f (x))|2,

1
4

σ(x) = Σ(x)

1
2 .

To illustrate our approximation result, let us calculate, using Monte-Carlo simula-

tions, the weak error of the SME approximation

Ew = |Eg(XN η) − Eg(xN )|,

(45)

for α = 1, 2 v.s. η for different f, fi and generic polynomial test functions g. The
results are shown in Fig. 1. We see that we have order α weak convergence, even when
some conditions of the above theorem are not satisﬁed (Fig. 1(b)).

10

(a)

(b)

Figure 1: Weak error Ew, as deﬁned in (45) with α = 1, 2, v.s. learning rate η for two
different choices of f, fi. All errors are averaged over 1012 samples of SGD trajectories
up to T = 1.0. The initial condition is x0 = 1. The SMEs moments are solved exactly
since they involve linear drifts. (a) Quadratic objective with n = 2, fi = (x − γi)2
where γi ∈ {±1}. The total objective is f (x) = x2 + 1. The test function is g(x) =
x + x2 + x3. (b) Non-convex fi’s with n = 2, fi(x) = (x − γi)2 + γix3 where
γi ∈ {±1}. The total objective is the same f (x) = x2 + 1. We chose g(x) = x so that
Eg(XT ) has closed form solution. Note that for this choice of fi, the condition (iii) of
Theorem 1 is not satisﬁed. Nevertheless, in both cases, we observe that the weak error
decreases with η like Ew ∼ ηα.

Remark 2. From above, we also observe that if we pick b(x) = −∇f (x) and σ(x)
to be any function in G (and its sufﬁciently high derivatives are also in G), then we
have matching moments up to order η2 and hence we can conclude that for this choice,
the resulting SDE is a ﬁrst order weak approximation of the SGD, with |Eg(Xkη) −
Eg(xk)| ≤ Cη, k = 0, 1, . . . , N . In particular, the deterministic gradient ﬂow is a ﬁrst
order weak approximation of the SGD. Hence, just like traditional modiﬁed equations,
our SME (35) (α = 2) is the next order approximation of the underlying algorithm.

However, we stress that for our ﬁrst order SME with α = 1, i.e.

the choice
b = −∇f and σ = Σ 1
2 , the fact that we did not the improve the order of weak con-
vergence from the deterministic gradient ﬂow does not mean that this is a equally bad
approximation. The constant C in the weak error depends on the choice of Σ and in
fact, it can be shown empirically that with this choice, we do have lower weak error
|Eg(Xkη) − Eg(xk)|, but the order of convergence of the weak error as η → 0 is the
same. An analytical justiﬁcation must then rely on using the Itˆo-Taylor expansion to
obtain precise estimates for the factor C (see e.g. Talay & Tubaro (1990)). This is
beyond the scope of the current paper.

Remark 3. The Lipschitz condition (i) is to ensure that the SME has a unique strong
solution with uniformly bounded moments Milstein (1986). If we allow weak solutions
and establish uniform boundedness of moments by other means (more assumptions on
the growth and direction of ∇f for large x), then condition (i) is expected to be relaxed
although the technical details will be tedious.

Condition (iii) in Theorem 1 appears to be the most stringent one and in fact it may
limit applications to problems with objectives that have more than quadratic growth.

11

10-210-1100η10110-110-310-5Ewα=1α=2Slope=1Slope=210-210-1100η10110-110-310-5Ewα=1α=2Slope=1Slope=2However, closer inspection tells us it can also be relaxed. For example, if there exists
an invariant distribution that concentrates on a compact subset of Rd then as η → 0,
xk’s would be bounded with high probability, and hence for large x we may replace
f, fi with a version that satisﬁes the growth condition in (iii). Further work is needed
to make this precise but we can already see in Fig. 1(b) that we have quadratic weak
convergence even when (iii) is not satisﬁed.

Remark 4. The regularity conditions on f and g in Theorem 1 are inherited from The-
orem 2 in Milstein (1986). For smooth objectives, polynomial growth conditions are
usually not restrictive. Still, with care, these should be relaxed since in our case the
small noise helps to reduce the number of terms containing higher derivatives in var-
ious Taylor and Itˆo-Taylor expansions. Proving a more general version of Theorem 1
will be left as future work.

D Derivation of SMEs

In this section, we include more detailed derivations of the SMEs used in the main
paper. For brevity, we do not include rigorous proofs of approximation statements for
SGD variants in Sec. D.2 and D.3, but only heuristic justiﬁcations. Proving rigorous
statements for these approximations can be done by modifying the proof of Thm. 1.

D.1 SME for the simple quadratic example

We start with the example in Sec. 3.1 of the main paper. Let n = 2, d = 1 and set
f (x) = x2 + 1 with f1(x) = (x − 1)2 and f2(x) = (x + 1)2. The SGD iterations picks
at random between f1 and f2 and performs descent with their respective gradients.
Recall that the (second order) SME is given by

dXt = −(f (cid:48)(Xt) +

1
2
Now, f (cid:48)(x) = 2x, f (cid:48)(cid:48)(x) = 2 and

ηf (cid:48)(Xt)f (cid:48)(cid:48)(Xt))dt + (ηΣ(Xt))

1

2 dWt.

(46)

and hence the SME is

Σ(x) =

(f (cid:48)

i (x) − fi(x))2 = 4

1
2

2
(cid:88)

i=1

dXt = −2(1 + η)Xtdt + 2

ηdWt.

√

D.2 SME for learning rate adjustment

The SGD iterations with learning rate adjustment is

xk+1 = xk − ηukf (cid:48)(xk),

(47)

(48)

(49)

where uk ∈ [0, 1] is the learning rate adjustment factor. η is the maximum allowed
learning rate. There are two reasons we introduce this hyper-parameter. First, gradients

12

cannot be arbitrarily large since that will cause instabilities. Second, the SME is only
an approximation of the SGD for small learning rates, and so it is hard to justify the
approximation for large η.

In this case, deriving the corresponding SME is extremely simple. Notice that we

can deﬁne gi,k(xk) = ukfi(xk), gk = ukf (xk). Then, the iterations above is simply

xk+1 = xk − ηg(cid:48)

γk,k(xk),

whose (ﬁrst order) SME is by Thm. 1

dXt = −g(cid:48)(Xt)dt + (ηu2

t Σ(Xt))

2 dWt.

1

And hence the SME for SGD with learning rate adjustments is

dXt = −utf (cid:48)(Xt)dt + ut(ηΣ(Xt))

1

2 dWt.

D.3 SME for SGD with momentum

First let us consider the constant momentum parameter case. The SGD with momentum
is the paired update

vk+1 = µvk − ηf (cid:48)
γk
xk+1 = xk + vk+1.

(xk),

To derive and SME, notice that we can write the above as

vk+1 = vk + η

xk+1 = xk + η

(cid:18)

−

1 − µ
η
(cid:18) vk+1
η

(cid:19)

.

(cid:19)

vk − f (cid:48)(xk)

+ η (cid:0)f (cid:48)(xk) − f (cid:48)
γk

(xk)(cid:1) ,

Recall that since we are looking at ﬁrst order weak approximations, it is sufﬁcient to
compare to the Euler-Maruyama discretization (Sec. B.4). We observe that the above
can be seen as an Euler-Maruyama discretization of the coupled SDE

dVt = (−

Vt − f (cid:48)(Xt))dt + (ηΣ(Xt))

1

2 dWt,

with the usual choice of Σ(x). Hence, this is the ﬁrst order SME for the SGD with
momentum having a constant momentum parameter µ. For time-varying momentum
parameter, we just replace µ by µt to get

dVt = (−

Vt − f (cid:48)(Xt))dt + (ηΣ(Xt))

1

2 dWt,

1 − µ
η

dXt =

Vtdt,

1
η

1 − µt
η

dXt =

Vtdt.

1
η

13

(50)

(51)

(52)

(53)

(54)

(55)

(56)

E Solution of optimal control problems

E.1 Brief introduction to optimal control

We ﬁrst introduce some basic terminologies and results on optimal control theory to
pave way for our solutions to optimal control problems for the learning rate and mo-
mentum parameter. For simplicity, we restrict to one state dimension (d = 1), but
similar equations hold for multiple dimensions. For a more thorough introduction
to optimal control theory and calculus of variations, we refer the reader to Liberzon
(2012).

Let t ∈ [0, T ] and consider the ODE

d
dt

zt = Φ(zt, ut),

(57)

where zt, ut ∈ R and Φ : R × R → R. The variable zt describes the evolution of some
state and ut is the control variable, which can affect the state dynamics. Consider the
control problem of minimizing the cost functional

C(u) =

L(zs, us)ds + G(zT )

(58)

(cid:90) T

0

with respect to ut, subject to zt satisfying the ODE (57) with prescribed initial con-
dition z0 ∈ R. The function L is known as the running cost and G is the terminal
cost. Usually, we also specify some control set U ⊂ R so that we only consider
u : [0, T ] → U . The full control problem reads

min
u:[0,T ]→U

C(u) subject to (57).

(59)

Note that additional path constraints can also be added and (57) can also be made time-
inhomogeneous, but for our purposes it is sufﬁcient to consider the above form.

There are two principal ways of solving optimal control problems: either dynamic
programming through the Hamilton-Jacobi-Bellman (HJB) equation (Bellman, 1956),
or using the Pontryagin’s maximum principle (PMP) (Pontryagin, 1987). In this sec-
tion, we will only discuss the HJB method as this is the one we employ to solve the
relevant control problems in this paper.

E.2 Dynamic programming and the HJB equation

The ﬁrst way to solve (59) is through the dynamic programming principle. For t ∈
[0, T ] and z ∈ R, deﬁne the value function

V (z, t) = min

L(zs, us)ds + G(zT ),

(cid:90) T

u:[t,T ]→U

t

subject to

zs = Φ(zs, us),

s ∈ [t, T ],

d
ds
z(t) = z.

14

(60)

Notice that if there exists a solution to (59), then the value of the minimum cost is
V (z0, 0). The dynamic programming principle allows us to derive a recursion on the
function V , in the form of a partial differential equation (PDE)

∂tV (z, t) + min
u∈U

{∂zV (z, t)Φ(z, u) + L(z, u)} = 0,

V (T, z) = G(z).

(61)

This is known as the Hamilton-Jacobi-Bellman equation (HJB). Note that this PDE is
solved backwards in time. The derivation of this PDE can be found in most references
on optimal control, e.g. in Liberzon (2012). The main idea is the dynamic program-
ming principle: for any t the [t, T ]-portion of the optimal trajectory must again be
optimal.

After solving the HJB (61), we can then obtain the optimal control u∗

t as function

of the state process zt and t, given by

u∗
t = arg min

u∈U

{∂zV (zt, t)Φ(zt, u) + L(zt, u)}.

(62)

In some cases, we ﬁnd that the optimal control is independent of time and is strictly of
a feed-back control law, i.e.

t = u∗(zt)
u∗
(63)
for some function u∗ : R → U . This is the case for the problems considered in this
paper. With the optimal control found in (62), we can then substitute ut = u∗
t in (57)
to obtain the optimally controlled process z∗
t .
In summary, to solve the optimal control problem (59), we ﬁrst solve the HJB
PDE (61), and then solve for the optimal control (62), and lastly (if necessary) solve
the optimally controlled state process by substituting the solution of (62) into (57).
Sometimes, the optimal control (62) can be solved without fully solving the HJB for
V , e.g. when L = 0 and one can infer the sign of ∂V . This is the case for the two
control problems we encounter in this paper. The solution to (62) is the most important
for all practical purposes since it gives a way to adjust the control parameters on-the-ﬂy,
especially when we have a feed back control law.

E.3 Solution of the learning rate control problem

Now, let us apply the HJB equations (Sec. E.2) to solve the learning rate control prob-
lem. Recall from Sec. 4.1.2 that we wish to solve

min
u:[0,T ]→[0,1]

mT subject to

mt = −2autmt +

aηΣu2
t ,

d
dt

m0 =

1
2

1
2
a(x0 − b)2.

(64)

15

This is of the form (59) with Φ(m, u) = −2aum + aηΣu2/2, L(m, u) = 0 and
G(m) = m. Thus, we write the HJB equation

∂tV (m, t) + min
u∈[0,1]

{∂mV (m, t)[−2aum +

aηΣu2]} = 0,

1
2

V (m, T ) = m.

(65)

First, it’s not hard to see that for a > 0, ∂mV ≥ 0 for all m, t. This is because, the
lower the m, the closer we are to the optimum and hence the minimum cost achievable
in the same time interval [t, T ] should be less. Similarly,∂mV ≥ 0 holds for a < 0
if one reverses all previous statements (in this case m is negative). Hence, we can
calculate the minimum

1
2

aηΣu2},

{−2aum +

u∗ = arg min
u∈[0,1]
(cid:40)
1
min(1, 2m
ηΣ )
Notice that this solution is a feed-back control policy. We can now substitute ut = u∗
t
where

a ≤ 0,
a > 0.

(66)

=

u∗
t =

(cid:40)
1
min(1, 2mt
ηΣ )

a ≤ 0,
a > 0.

into the ODE in (64) to obtain

m∗

t =

(cid:40)

m0e−2at + 1
ηΣ
2+2a(t−t∗)

4 ηΣ(1 − e−2at)

a ≤ 0 or t < t∗,
a > 0 and t ≥ t∗.

where

t∗ =

log

1
2a

(cid:18) 4m0
ηΣ

(cid:19)

− 1

And therefore, we get from (67) the effective annealing schedule

(cid:40)

1

u∗
t =

1
1+a(t−t∗)

a ≤ 0 or t ≥ t∗,
a > 0 and t > t∗,

E.4 Solution of the momentum parameter control problem

We shall consider the case a > 0, since for a ≤ 0 the optimal control is trivially µt = 1.
The momentum parameter control problem is

(67)

(68)

(69)

(70)

(71)

min
µ:[0,T ]→[0,1]

mT subject to

d
dt

mt = Rλ(µt)(mt − m∞(µt)),

m0 =

a(x0 − b)2,

1
2

16

where

λ(µ) = −

(1 − µ) − (cid:112)(1 − µ)2 − 4aη
η

,

m∞(µ) =

ηΣ
4(1 − µ)

.

(72)

This is of the form (59) with Φ(m, µ) = Rλ(µ)(m − m∞(µ)), L(m, u) = 0 and
G(m) = m. The HJB equation is

∂tV (m, t) + min
µ∈[0,1]

{∂mV (m, t)[Rλ(µ)(m − m∞(µ))]} = 0,

V (m, T ) = m.

(73)

Again, it is easy to see that ∂mV (m, t) ≥ 0 for all m, t and so
µ∗ = arg min
µ∈[0,1]

{Rλ(µ)(m − m∞(µ))}

This minimization problem has no closed form solution. However, observe that Rλ(µ) ≤
aη). Now, if µ > µopt, we have
0 and is minimized at µ = µopt = max(0, 1 − 2
Rλ(µ) = −(1 − µ)/η and so Rλ(µ)(m − m∞(µ)) is increasing in µ for µ > µopt
(one can check this by differentiation and showing that the derivative is always posi-
tive). Hence, µ∗ ≤ µopt and it is sufﬁcient to consider µ ∈ [0, µopt] in the minimization
problem (74).

√

Next, observe that m − m∞(µ) is decreasing in µ and negative if

m <

ηΣ
4(1 − µ)

.

µ > 1 −

ηΣ
4m

.

µ∗ ≤ 1 −

ηΣ
4m

.

or

At the same time, Rλ(µ) is negative and decreasing for µ ∈ [0, µopt]. Thus, the product
Rλ(µ)(m − m∞(µ)) is positive and increasing for 1 − ηΣ
4m < µ < µopt and hence we
must have

Note that this is only a bound, but for small η, we can take this as an approximation of
µ∗, so long as it is less than µopt. Hence, we arrive at

µ∗

t =

(cid:40)
1
min(µopt, max(0, 1 − ηΣ
4mt

))

a ≤ 0,
a > 0.

One can of course follow the steps in Sec. E.3 to calculate m∗
t in the
form of an annealing schedule. We omit these calculations since they are not relevant
to applications.

t and hence µ∗

F Numerical experiments

In this section, we provide model and algorithmic details for the various numerical ex-
periments considered in the main paper, as well as a brief description of the commonly
applied adaptive learning rate methods that we compare the cSGD algorithm with.

17

(74)

(75)

(76)

(77)

(78)

F.1 Model details

In Sec. 4 from the main paper, we consider three separate models for two datasets.

M0: fully connected NN on MNIST

The ﬁrst dataset we consider the MNIST dataset (LeCun et al., 1998), which involves
computer recognition of 60000 28 × 28 pixel pictures of handwritten digits. We split
it into 55000 training samples and 5000 test samples. Our inference model is a fully
connected neural network with one hidden layer. For a input batch K of pixel data
(ﬂattened into a vector) z ∈ R784×K, we deﬁne the model

y = softmax(W2hR(W1z + b1) + b2),

where the activation function hR is the commonly used Rectiﬁed Linear Unit (ReLU)

hR(z)(ij) = max(z(ij), 0).

The ﬁrst layer weights and biases are W1 ∈ R784×10 and b1 ∈ R10 and the second
layer weights and biases are W2 ∈ R10×10 and b2 ∈ R10. These constitute the trainable
parameters. The softmax function is deﬁned as

(79)

(80)

(81)

softmax(z)(ij) =

exp(−z(ij))
k exp(−z(kj))

.

(cid:80)

The output tensors y ∈ R10×K is compared to a batch of one-hot target labels ˆy with
the cross-entropy loss

C(y, ˆy) = −

ˆy(ij) log y(ij).

(82)

1
10K

(cid:88)

i,j

Lastly, we use (cid:96)2 regularization so that the minimization problem is

min
W1,b1,W2,b2

C(y, ˆy) +

λW,i(cid:107)Wi(cid:107)2

2 +

λb,i(cid:107)bi(cid:107)2
2,

(83)

2
(cid:88)

i=1

2
(cid:88)

i=1

Each regularization strength λ is set to be 1 divided by the dimension of the trainable
parameter.

C0: fully connected NN on CIFAR-10

The CIFAR-10 dataset (Krizhevsky & Hinton, 2009) consists of 60000 small 32 × 32
pixels of RGB natural images belonging to ten separate classes. We split the dataset
into 50000 training samples and 10000 test samples. Our ﬁrst model for this dataset is
a deeper fully connected neural network

y = softmax(W3hT (W2hT (W1z + b1) + b2) + b3),

(84)

18

where we use a tanh activation function between the hidden layers

hT (z)(ij) = tanh(z(ij)).

(85)

The layers have width 3071,500,300,10. That is, the trainable parameters have dimen-
sions W1 ∈ R3071×500, b1 ∈ R500, W2 ∈ R500×300, b2 ∈ R300, W3 ∈ R300×10, b3 ∈
R10. We use the same soft-max output, cross-entropy loss and (cid:96)2 regularization as as
before.

C1: convolutional NN on CIFAR-10

Our last experiment is a convolutional neural network on the same CIFAR-10 dataset.
We use four convolution layers consisting of convolution,batch-normalization,ReLU,max-
pooling. Convolution ﬁlter size is 5 × 5, with uniform stride 1 and padding 2. Output
channels of convolution layers are {96,128,256,64}. The pooling size is 2 × 2 with
stride 2. The output layers consist of two fully connected layers of width {1024,256}
and drop-out rate 0.5. (cid:96)2 regularization is introduced as a weight decay with decay
parameter 5e-3.

F.2 Adagrad and Adam

Here, we write down for completeness the iteration rules of Adagrad (Duchi et al.,
2011), and Adam (Kingma & Ba, 2015) optimizers, which are commonly applied tools
to tune the learning rate. For more details and background, the reader should consult
the respective references.

Adagrad. The Adagrad modiﬁcation to the SGD reads

x(i),k+1 = xk,(i) −

∂(i)fγk (xk),

(86)

η
(cid:112)Gk,(i)

where Gk,(i) is the running sum of gradients ∂(i)fγl (xl) for l = 0, . . . , k − 1. The
tunable hyper-parameters are the learning rate η and the initial accumulator value G0.
In this paper we consider only the learning rate hyper-parameter as this is equivalent to
setting the initial accumulator to a common constant across all dimensions.

Adam. The Adam method has similar ideas to momentum. It keeps the exponential

moving averages

m(i),k+1 = β1mk,(i) + (1 − β1)∂(i)fγk (xk),
v(i),k+1 = β2vk,(i) + (1 − β2)[∂(i)fγk (xk)]2.

Next, set,

(87)

(88)

ˆmk,(i) =

ˆvk,(i) =

mk,(i)
1 − βk
1
vk,(i)
1 − βk
2

,

.

19

Finally, the Adam update is

x(i),k+1 = xk,(i) −

ˆmk,(i).

(89)

η
(cid:112)ˆvk,(i)

The hyper-parameters are the learning rate η and the EMA decay parameters β1, β2.

Note that for both methods above, one can also introduce a regularization term (cid:15) to

the denominator to prevent numerical instabilities.

F.3

Implementation of cSGD

Recall from Sec. 4.1 that the optimal control solution for learning rate control of the
quadratic objective f (x) = 1

2 a(x − b)2 is given by

(90)

(91)

(92)

The idea is to perform a local quadratic approximation

u∗
t =

(cid:40)
1
min(1, 2mt
ηΣ )

a ≤ 0,
a > 0.

f (x) ≈

a(i)(x(i) − b(i))2.

1
2

d
(cid:88)

i=1

∂(i)f (x) ≈ a(i)(x(i) − b(i)).

This is equivalent to a local linear approximation of the gradient, i.e. for i = 1, 2, . . . , d

This effectively decouples the control problems of d identical one-dimensional control
problems, so that we may apply (90) element-wise. We note that this approximation
is only assumed to hold locally and the parameters must be updated. There are many
ways to do this. Our approach uses linear regression on-the-ﬂy via exponential moving
averages (EMA). For each trainable dimension i, we maintain the following exponen-
tial averages

gk+1,(i) = βk,(i)gk,(i) + (1 − βk,(i))f (cid:48)
γk
k,(i) + (1 − βk,(i))f (cid:48)
k+1,(i) = βk,(i)g2
g2
γk
xk+1,(i) = βk,(i)xk,(i) + (1 − βk,(i))xk,(i),
x2
k+1,(i) = βk,(i)x2
k,(i),
gxk+1,(i) = βk,(i)gxk,(i) + (1 − βk,(i))xk,(i)f (cid:48)
γk

k,(i) + (1 − βk,(i))x2

(xk,(i)),
(xk,(i))2,

(xk,(i)).

(93)

The decay parameter βk,(i) controls the effective averaging window size. In practice,
we should adjust βk,(i) so that it is small when variations are large, and vice versa. This
ensures that our local approximations adapts to the changing landscapes. Since local
variations is related to the gradient, we use the following heuristic

βk+1,(i) = βmin + (βmax − βmin)

(94)

g2

k,(i) − g2

k,(i)

.

g2

k,(i)

20

Algorithm 1 controlled SGD (cSGD)

Hyper-parameters: η, u0
Initialize x0; β0,(i) = 0.9 ∀i
for k = 0 to (#iterations − 1) do

Compute sample gradient ∇fγk (xk)
for i = 1 to d do

Update EMA using (93)
Compute ak,(i), bk,(i), Σk,(i) using (95)
Compute u∗
k,(i) using (96)
k,(i) − g2
βk+1,(i) = (g2
uk+1,(i) = βk,(i)uk,(i) + (1 − βk,(i))u∗
xk+1,(i) = xk,(i) − ηuk,(i)∇fγk (xk)(i)

k,(i))/g2

k,(i) and clip

k,(i)

end for

end for

which is similar to the one employed in Schaul et al. (2013) for maintaining EMAs.
The additional clipping to the range [βmin, βmax] is to make sure that there are enough
samples to calculate meaningful regressions, and at the same time prevent too large de-
cay values where the contribution of new samples vanish. In the applications presented
in this paper, we usually set βmin = 0.9 and βmax = 0.999, but results are generally
insensitive to these values.

With the EMAs (93), we compute ak,(i) by the ordinary-least-squares formula and

Σk,(i) as the variance of the gradients:

ak,(i) =

,

x2

k,(i)

gxk,(i) − gk,(i)xk,(i)
k,(i) − x2
gk,(i)
ak,(i)
k,(i),

k,(i) − g2

,

bk,(i) = xk,(i) −

Σk,(i) = g2

This allows us to estimate the policy (90) as

(cid:40)1

u∗
k,(i) =

min(1, ak,(i)(xk,(i)−bk,(i))2

)

ηΣk,(i)

ak,(i) ≤ 0,
ak,(i) > 0.

for i = 1, 2, . . . , d. Since our averages are from exponentially averaged sources, we
should also update our learning rate policy in the same way:

uk+1,(i) = βk,(i)uk,(i) + (1 − βk,(i))u∗

k,(i)

The algorithm is summarized in Alg. 1

(95)

(96)

(97)

21

Algorithm 2 controlled momentum SGD (cMSGD)

Hyper-parameters: η, µ0
Initialize x0, v0; β0,(i) = 0.9 ∀i
for k = 0 to (#iterations − 1) do

Compute sample gradient ∇fγk (xk)
for i = 1 to d do

Update EMA using (93)
Compute ak,(i), bk,(i), Σk,(i) using (95)
Compute µ∗
k,(i) using (99)
k,(i) − g2
βk+1,(i) = (g2
µk+1,(i) = βk,(i)µk,(i) + (1 − βk,(i))µ∗
vk+1,(i) = µk,(i)vk,(i) − η∇fγk (xk)(i)
xk+1,(i) = xk,(i) + vk+1,(i)

k,(i))/g2

k,(i) and clip

k,(i)

end for

end for

F.4

Implementation of cMSGD

We wish to apply the momentum parameter control

µ∗

t =

(cid:40)
1
min(µopt, max(0, 1 − ηΣ
4mt

))

a ≤ 0,
a > 0,

where µopt = max{0, 1 − 2
aη}. We proceed in the same way as in Sec. F.3 by
keeping the relevant EMA averages and performing linear regression on the ﬂy. The
only difference is the application of the momentum parameter adjustment, which is

√

µ∗
k,(i) =


1

min[max(0, 1 − 2
max(0, 1 −



√

ak,(i)η),
ηΣk,(i)
2ak,(i)(xk,(i)−bk,(i))2 )]

ak,(i) ≤ 0,

ak,(i) > 0,

(98)

(99)

The algorithm is summarized in Alg. 2.

F.5 Training accuracy for C1

For completeness we also provide in Fig. 2 the training accuracies of C1 with various
hyper-parameter choices and methods tested in this work. These complements the
plots of test accuracies in Fig. 3,5,6 in the main paper. We see that cSGD and cMSGD
display the same robustness in terms of test and training accuracies.

References

Bellman, Richard. Dynamic programming and Lagrange multipliers. Proceedings of

the National Academy of Sciences, 42(10):767–769, 1956.

22

Courant, Richard, Isaacson, Eugene, and Rees, Mina. On the solution of nonlinear
hyperbolic differential equations by ﬁnite differences. Communications on Pure and
Applied Mathematics, 5(3):243–255, 1952.

Daly, Bart J. The stability properties of a coupled pair of non-linear partial difference

equations. Mathematics of Computation, 17(84):346–360, 1963.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online
learning and stochastic optimization. Journal of Machine Learning Research, 12
(Jul):2121–2159, 2011.

Freidlin, Mark I, Sz¨ucs, Joseph, and Wentzell, Alexander D. Random perturbations of

dynamical systems, volume 260. Springer Science & Business Media, 2012.

Hille, Einar and Phillips, Ralph Saul. Functional analysis and semi-groups, volume 31.

American Mathematical Soc., 1996.

Hirt, CW. Heuristic stability theory for ﬁnite-difference equations. Journal of Compu-

tational Physics, 2(4):339–355, 1968.

Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. ICLR,

2015.

images. 2009.

Kloeden, P. E. and Platen, E. Numerical Solution of Stochastic Differential Equations.

Springer, New York, corrected edition, June 2011.

Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny

Lax, Peter and Wendroff, Burton. Systems of conservation laws. Communications on

Pure and Applied mathematics, 13(2):217–237, 1960.

LeCun, Yann, Cortes, Corinna, and Burges, Christopher JC. The mnist dataset of

handwritten digits. URL http://yann. lecun. com/exdb/mnist, 1998.

LeVeque, Randall J. Finite volume methods for hyperbolic problems, volume 31. Cam-

bridge university press, 2002.

Liberzon, Daniel. Calculus of variations and optimal control theory: a concise intro-

duction. Princeton University Press, 2012.

Milstein, GN. Approximate integration of stochastic differential equations. Theory of

Probability & Its Applications, 19(3):557–562, 1975.

Milstein, GN. A method of second-order accuracy integration of stochastic differential

equations. Theory of Probability & Its Applications, 23(2):396–401, 1979.

Milstein, GN. Weak approximation of solutions of systems of stochastic differential

equations. Theory of Probability & Its Applications, 30(4):750–766, 1986.

Milstein, GN. Numerical integration of stochastic differential equations, volume 313.

Springer Science & Business Media, 1995.

23

Noh, WF and Protter, MH. Difference methods and the equations of hydrodynamics.
Technical report, California. Univ., Livermore. Lawrence Radiation Lab., 1960.

Oksendal, Bernt. Stochastic differential equations: an introduction with applications.

Springer Science & Business Media, 2013.

Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC Press,

1987.

Schaul, Tom, Zhang, Sixin, and LeCun, Yann. No more pesky learning rates. In ICML

(3), volume 28, pp. 343–351, 2013.

Talay, Denis and Tubaro, Luciano. Expansion of the global error for numerical schemes
solving stochastic differential equations. Stochastic analysis and applications, 8(4):
483–509, 1990.

Uhlenbeck, George E and Ornstein, Leonard S. On the theory of the Brownian motion.

Physical review, 36(5):823, 1930.

Warming, RF and Hyett, BJ. The modiﬁed equation approach to the stability and
accuracy analysis of ﬁnite-difference methods. Journal of computational physics,
14(2):159–179, 1974.

24

(a) C1, Learning rate adjustments (c.f. main paper Fig . 3)

(b) C1, Momentum adjustments (c.f. main paper Fig . 5)

(c) C1, Learning rate sensitivity (c.f. main paper Fig . 6)

Figure 2: Training accuracies for various methods and hyper-parameter choices. The
set-up is the same as in the main paper, Fig. 3,5,6 except that we plot training accuracy
instead of test accuracy. The qualitative observation is the same: cSGD and cMSGD
are generally robust to changing parameters and models.

25

050100150Epoch0.00.20.40.60.81.0Train acccSGDηu01e-21e-15e-1050100150EpochAdagradη5e-15e-21e-2050100150EpochAdamη5e-21e-25e-4050100150Epoch0.00.20.40.60.81.0Train acccMSGDµ00.950.90.999050100150EpochMSGDµ0.9990.990.95050100150EpochMSGD-Aµmax0.9990.990.9050100150Epoch0.00.20.40.60.81.0Train acccMSGDη2e-11e-12e-2050100150EpochMSGDη1e-11e-21e-3050100150EpochMSGD-Aη1e-11e-21e-3