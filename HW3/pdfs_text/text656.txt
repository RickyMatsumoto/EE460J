Learning Latent Space Models with Angular Constraints

Pengtao Xie 1 2 Yuntian Deng 3 Yi Zhou 4 Abhimanu Kumar 5 Yaoliang Yu 6 James Zou 7 Eric P. Xing 2

Abstract

The large model capacity of latent space mod-
els (LSMs) enables them to achieve great per-
formance on various applications, but meanwhile
renders LSMs to be prone to overﬁtting. Several
recent studies investigate a new type of regular-
ization approach, which encourages components
in LSMs to be diverse, for the sake of alleviating
overﬁtting. While they have shown promising
empirical effectiveness, in theory why larger “di-
versity” results in less overﬁtting is still unclear.
To bridge this gap, we propose a new diversity-
promoting approach that is both theoretically an-
alyzable and empirically effective. Speciﬁcally,
we use near-orthogonality to characterize “di-
versity” and impose angular constraints (ACs)
on the components of LSMs to promote diver-
sity. A generalization error analysis shows that
larger diversity results in smaller estimation er-
ror and larger approximation error. An efﬁcient
ADMM algorithm is developed to solve the con-
strained LSM problems. Experiments demon-
strate that ACs improve generalization perfor-
mance of LSMs and outperform other diversity-
promoting approaches.

1. Introduction

Latent space models (LSMs), such as sparse coding (Ol-
shausen & Field, 1997), topic models (Blei et al., 2003)
and neural networks, are widely used in machine learning
to extract hidden patterns and learn latent representations
of data. An LSM consists of a set of components. Each
component aims at capturing one latent pattern and is pa-

1Machine Learning Department, Carnegie Mellon University
2Petuum Inc. 3School of Engineering and Applied Sciences, Har-
vard University 4College of Engineering and Computer Science,
Syracuse University 5Groupon Inc. 6School of Computer Science,
University of Waterloo 7Department of Biomedical Data Science,
Stanford University. Correspondence to: Pengtao Xie <peng-
taox@cs.cmu.edu>, Eric P. Xing <eric.xing@petuum.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

rameterized by a weight vector. For instance, in a topic
model (Blei et al., 2003), the components are referred to
as topics, aiming at discovering the semantics underlying
documents. Each topic is associated with a weight vector.
The modeling power of LSMs can be very large when the
number of components is large and the dimension of weight
vectors is high. For instance, in the LightLDA (Yuan et al.,
2015) topic model, the number of topics is 1 million and
the dimension of topic vector is 50000, resulting in a topic
matrix with 50 billion parameters. The vast model capac-
ity of LSMs enables them to ﬂexibly adapt to the complex
patterns underlying data and achieve great predictive per-
formance therefrom.

While highly expressive, LSMs are prone to overﬁtting,
because of their large amount of model parameters. A
key ingredient to successfully train LSMs is regularization,
which imposes certain control over the model parameters
to reduce model complexity and improve the generaliza-
tion performance on unseen data. Many regularizers have
been proposed, including (cid:96)2 regularization, (cid:96)1 regulariza-
tion (Tibshirani, 1996), nuclear norm (Recht et al., 2010),
Dropout (Srivastava et al., 2014) and so on.

Recently, a new type of regularization approaches (Yu
et al., 2011; Zou & Adams, 2012a; Xie et al., 2015; 2016a;
Rodr´ıguez et al., 2016), which aim at encouraging the
weight vectors of components in LSMs to be “diverse”,
are emerging. Zou & Adams (2012b) apply Determinantal
Point Process (Kulesza & Taskar, 2012) to encourage the
topic vectors in LDA to be “diverse”. Bao et al. (2013) de-
velop a softmax regularizer to promote incoherence among
hidden units in neural network. Xie et al. (2015) propose
an angle-based regularizer to “diversify” the weight vec-
tors in Restricted Boltzmann Machine (RBM). While these
approaches have demonstrated promising effectiveness on
a wide range of empirical studies, in theory how they re-
duce overﬁtting is still unclear. One intuitive explanation
could be: promoting diversity imposes a structural con-
straint on model parameters, which reduces the model ca-
pacity of LSMs and therefore alleviates overﬁtting. How-
ever, how to make this formal is challenging. In this pa-
per, we aim to bridge this gap, by proposing a diversity-
promoting approach that is both empirically effective and
theoretically analyzable. We use near-orthogonality to rep-
resent “diversity” and propose to learn LSMs with angular

Learning Latent Space Models with Angular Constraints

constraints (ACs) where the angle between components is
constrained to be close to π
2 , which hence encourages the
components to be close to orthogonal (therefore “diverse”).
Using sparse coding and neural network as study cases, we
analyze how ACs affect the generalization performance of
these two LSMs. The analysis shows that the more close
to π
2 the angles are, the smaller the estimation error is and
the larger the approximation error is. The best tradeoffs
of these two errors can be explored by properly tuning the
angles. We develop an alternating direction method of mul-
tipliers (ADMM) (Boyd et al., 2011) algorithm to solve
the angle-constrained LSM (AC-LSM) problems. In var-
ious experiments, we demonstrate that ACs improve the
generalization performance of LSMs and outperform other
diversity-promoting regularization approaches.

The major contributions of this work are:

• We propose a new approach to promote diversity
in LSMs, by imposing angular constraints (ACs) on
components, for the sake of alleviating overﬁtting.
• We perform theoretical analysis on how ACs affect the
generalization error of two exemplar LSMs: sparse
coding and neural networks.

• We develop an efﬁcient ADMM algorithm to solve the

AC-LSM problems.

• Empirical evaluation demonstrates that ACs are very
effective in reducing overﬁtting and outperforms other
diversity-promoting approaches.

The rest of the paper is organized as follows. Section
2 reviews related works. Second 3 introduces the angle-
constrained LSMs and Section 4 gives the theoretical anal-
ysis. Section 5 presents experimental results and Section 6
concludes the paper.

2. Related Works

Diversity-promoting learning of latent space models has
been widely studied recently. Ramirez et al. (2010) de-
ﬁne a regularizer based on squared Frobenius norm to en-
courage the dictionary in sparse coding to be incoherent.
Zou & Adams (2012a) use the determinantal point process
(DPP) (Kulesza & Taskar, 2012) to encourage the location
vectors in Gaussian mixture model (GMM) and topics in la-
tent Dirichlet allocation (Blei et al., 2003) to be “diverse”.
Given m vectors {wj}m
j=1, DPP is deﬁned as log det(G).
G is a kernel matrix where Gij = k(wi, wj) and k(·, ·) is a
kernel function. det(G) is the volume of the parallelepiped
formed by {φ(wj)}m
j=1, where φ(·) denotes the reproduc-
ing kernel feature map associated with kernel k. Vectors
with larger volume are considered to be more diverse since
they are more spread out. Xie (2015) develop an angle-
based regularizer to encourage the weight vectors of hidden
units in restricted Boltzmann machine to be close to orthog-
onal. The non-obtuse angle between each pair of weight

vectors is measured and the regularizer is deﬁned as the
mean of these angles minus their variance. A larger mean
encourages the vectors to have larger angles overall and a
smaller variance encourages the vectors to be evenly dif-
ferent from each other. Xie (2015) apply this regularizer to
encourage the projection vectors in distance metric learning
to be diverse and show that promoting diversity can reduce
model size without sacriﬁcing modeling power. Besides
frequentist-style regularization, diversity-promoting learn-
ing is also investigated in Bayesian learning where the com-
ponents are random variables. Affandi et al. (2013) apply
DPP as a repulsive prior to encourage the location vectors
in GMM to be far apart. Xie et al. (2016a) propose a mu-
tual angular prior that has an inductive bias towards vectors
having larger angles.

In the literature of neural networks, many works have stud-
ied the “diversiﬁcation” of hidden units. Le et al. (2010)
apply a strict-orthogonality constraint over the weight pa-
rameters to make the hidden units uncorrelated (therefore
“diverse”). In practice, this hard constraint might be too
restrictive and hurts performance, as we will conﬁrm in ex-
periments. Bao et al. (2013) propose a softmax regular-
izer to encourage the weight vectors of hidden units to have
small cosine similarity. Cogswell et al. (2015) propose to
decorrelate hidden activations by minimizing their covari-
ance. In convolutional neural networks (CNNs) where the
number of activations is much larger than that of weight
parameters, this regularizer is computationally prohibitive
since it is deﬁned over activations rather than weights.
Henaff et al. (2016) perform a study to show that random
orthogonal initialization of the weight matrices in recur-
rent neural networks improves its ability to perform long-
memory tasks. Xiong et al. (2016) propose a structured
decorrelation constraint which groups hidden units and en-
courages units within the same group to have strong con-
nections during the training procedure and forces units in
different groups to learn nonredundant representations by
minimizing the cross-covariance between them. Rodr´ıguez
et al. (2016) show that regularizing negatively correlated
features inhibits effective diversity and propose a solution
which locally enforces feature orthogonality. Chen et al.
(2017) propose a group orthogonal CNN which leverages
side information to learn diverse feature representations.
Mao et al. (2017) impose a stochastic decorrelation con-
straint based on covariance to reduce the co-adaptation of
hidden units. Xie et al. (2017) deﬁne a kernel-based regu-
larizer to promote diversity and analyze how it affects the
generalize performance of neural networks (NNs). It is un-
clear how to generalize the analysis to other LSMs.

Diversity-promoting learning has been investigated in non-
LSM models as well. In multi-class classiﬁcation, Malkin
& Bilmes (2008) encourage the coefﬁcient vectors of dif-
ferent classes to be diverse by maximizing the determi-

Learning Latent Space Models with Angular Constraints

nant of the covariance matrix of the coefﬁcient vectors. In
classiﬁers ensemble, Yu et al. (2011) develop a regularizer
to encourage the coefﬁcient vectors of support vector ma-
chines (SVMs) to have small cosine similarity and analyze
how this regularizer affects the generalization performance.
The analysis is speciﬁc to SVM ensemble. It is unclear how
to generalize it to latent space models.

(denoted by A) by minimizing the following objective
(cid:80)n
function: L(W, A) = 1
2 +
2
(cid:80)m
j=1 (cid:107)wj(cid:107)2
λ1
j=1 |αij|1) + λ2
2. Applying ACs to the
basis vectors, we obtain the following AC-SC problem:

i=1((cid:107)xi − (cid:80)m

j=1 αijwj(cid:107)2

(cid:80)m

minW,A L(W, A)
s.t.

1 ≤ i < j ≤ m,

|wi·wj |
(cid:107)wi(cid:107)2(cid:107)wj (cid:107)2

≤ τ

(2)

3. Methods

In this section, we propose Angle-Constrained Latent
Space Models (AC-LSMs) and present an ADMM algo-
rithm to solve them.

3.1. Latent Space Models with Angular Constraints

An LSM consists of m components and these components
are parameterized by vectors W = {wj}m
j=1. Let L(W)
denote the objective function of this LSM. Similar to (Bao
et al., 2013; Xie et al., 2015; Rodr´ıguez et al., 2016), we use
angle to characterize diversity: the components are consid-
ered to be more diverse if they are close to being orthogo-
nal, i.e., their angles are close to π
2 . To encourage this, we
require the absolute value of cosine similarity between each
pair of components to be less than a small value τ , which
leads to the following angle-constrained LSM (AC-LSM)
problem

L(W)

min
W
s.t.

1 ≤ i < j ≤ m,

|wi·wj |
(cid:107)wi(cid:107)2(cid:107)wj (cid:107)2

≤ τ

(1)

The parameter τ controls the level of near-orthogonality
(or diversity). A smaller τ indicates that the vectors are
more close to being orthogonality, and hence are more di-
verse. As will be shown later, representing diversity using
the angular constraints facilitates theoretical analysis and is
empirically effective as well.

3.2. Case Studies

In this section, we apply the ACs to two LSMs.
Sparse Coding Given a set of data samples {xi}n
i=1,
where x ∈ Rd, sparse coding (SC) (Olshausen & Field,
1997) aims to use a set of “basis” vectors (referred to as
dictionary) W = {wj}m
j=1 to reconstruct the data sam-
ples. Each data sample x is reconstructed by taking a sparse
linear combination of the basis vectors x ≈ (cid:80)m
j=1 αjwj
where {αj}m
j=1 are the linear coefﬁcients (referred to as
sparse codes) and most of them are zero. The recon-
struction error is measured using the squared (cid:96)2 norm
(cid:107)x − (cid:80)m
2. To achieve sparsity among the coefﬁ-
cients, (cid:96)1-regularization is utilized: (cid:80)m
j=1 |αj|1. To avoid
the degenerated case where most coefﬁcients are zero and
the basis vectors are of large magnitude, (cid:96)2-regularization
is applied to the basis vectors: (cid:107)wj(cid:107)2
2. Putting these
pieces together, we learn the basis vectors and sparse codes

j=1 αjwj(cid:107)2

In a neural network (NN) with L hid-
Neural Networks
den layers, each hidden layer l is equipped with m(l) units
and each unit i is connected with all units in layer l − 1.
Hidden unit i at layer l is parameterized by a weight vector
w(l)
. These hidden units aim at capturing latent features
i
underlying data. Applying ACs to the weight vectors of
hidden units, we obtain the following AC-NN problem

L(W)

min
W

s.t.

∀ 1 ≤ l ≤ L, 1 ≤ i < j ≤ m(l),

·w(l)
|w(l)
j |
i
i (cid:107)2(cid:107)w(l)
(cid:107)w(l)
j (cid:107)2

≤ τ

where W denotes weight vectors in all layers and L(W) is
the objective function of this NN.

3.3. Algorithm

In this section, we develop an ADMM-based algorithm to
solve the AC-LSM problem. To make it amenable for op-
timization, we ﬁrst factorize each weight vector w into its
(cid:96)2 norm g = (cid:107)w(cid:107)2 and direction (cid:101)w = w
. Under such a
factorization, w can be reparameterized as w = g (cid:101)w, where
g > 0 and (cid:107) (cid:101)w(cid:107)2 = 1. Then the problem deﬁned in Eq.(1)
can be transformed into

(cid:107)w(cid:107)2

(3)

(4)

L( (cid:102)W, G)

min
(cid:102)W,G
s.t.

∀j, gj ≥ 0, (cid:107) (cid:101)wj(cid:107)2 = 1
∀i (cid:54)= j, | (cid:101)wi · (cid:101)wj| ≤ τ

j=1 and G = {gj}m
where (cid:102)W = { (cid:101)wj}m
j=1. We solve this
new problem by alternating between (cid:102)W and G. Fixing (cid:102)W,
the problem deﬁned over G is: minG L(G) s.t. ∀j, gj ≥
0, which can be solved using projected gradient descent.
Fixing G, the sub-problem deﬁned over (cid:102)W is

min
(cid:102)W
s.t.

L( (cid:102)W)
∀j, (cid:107) (cid:101)wj(cid:107)2 = 1
∀i (cid:54)= j, | (cid:101)wi · (cid:101)wj| ≤ τ

which we solve using an ADMM algorithm. There are
R = m(m − 1) pairwise constraints | (cid:101)wi · (cid:101)wj| ≤ τ . For
the r-th constraint, let p(r) and q(r) be the index of the
ﬁrst and second vector respectively, i.e., the r-th constraint
is | (cid:101)wp(r) · (cid:101)wq(r)| ≤ τ . First, we introduce auxiliary vari-
ables {v(r)
r=1, to rewrite the problem in

r=1 and {v(r)

2 }R

1 }R

Learning Latent Space Models with Angular Constraints

Eq.(4) into an equivalent form. For each pairwise con-
straint: | (cid:101)wp(r) · (cid:101)wq(r)| ≤ τ , we introduce two auxiliary
1 , (cid:101)wq(r) = v(r)
2 , and let (cid:101)wp(r) = v(r)
vectors v(r)
2 ,
1
(cid:107)v(r)
2 (cid:107)2 = 1, |v(r)
2 | ≤ τ . To this end,
1
we obtain the following problem

and v(r)
1 (cid:107)2 = 1, (cid:107)v(r)

· v(r)

L( (cid:102)W)

min
(cid:102)W,V
s.t.

∀j, (cid:107) (cid:101)wj(cid:107)2 = 1
∀r, (cid:101)wp(r) = v(r)
∀r, (cid:107)v(r)

1 , (cid:101)wq(r) = v(r)

2

1 (cid:107)2 = 1, (cid:107)v(r)

2 (cid:107)2 = 1, |v(r)

1

· v(r)

2 | ≤ τ

where V = {(v(r)
2 )}R
Then we deﬁne the
augmented Lagrangian, with Lagrange multipliers Y =
{(y(r)

r=1 and parameter ρ

1 , v(r)

1 , y(r)

2 )}R

r=1.

min
(cid:102)W,V,Y

R
(cid:80)
r=1

L( (cid:102)W) +

(y(r)
1

· ( (cid:101)wp(r) − v(r)
1 )
· ( (cid:101)wq(r) − v(r)
2 (cid:107) (cid:101)wp(r) − v(r)
2 ) + ρ
2 (cid:107)2
2)

2 (cid:107) (cid:101)wq(r) − v(r)

+y(r)
2
+ ρ

1 (cid:107)2
2

s.t.

∀j, (cid:107) (cid:101)wj(cid:107)2 = 1
∀r, (cid:107)v(r)

1 (cid:107)2 = 1, (cid:107)v(r)

2 (cid:107)2 = 1, |v(r)

1

· v(r)

2 | ≤ τ

which can be solved by alternating between (cid:102)W, V, Y.

Solve (cid:102)W The sub-problem deﬁned over (cid:102)W is

min
(cid:102)W

L( (cid:102)W) +

R
(cid:80)
r=1

(y(r)
1
2 (cid:107) (cid:101)wp(r) − v(r)

+ ρ

1 (cid:107)2

· (cid:101)wp(r) + y(r)
2 + ρ

· (cid:101)wq(r)
2 (cid:107) (cid:101)wq(r) − v(r)
2 (cid:107)2
2)

2

(5)

s.t.

∀j, (cid:107) (cid:101)wj(cid:107)2 = 1

For sparse coding, we solve this sub-problem using coor-
dinate descent. At each iteration, we update (cid:101)wj by ﬁxing
the other variables. Please refer to the supplements for de-
tails. For neural network, this sub-problem can be solved
using projected gradient descent which iteratively performs
the following three steps: (1) compute the gradient of (cid:101)wj
using backpropagation; (2) perform a gradient descent up-
date of (cid:101)wj; (3) project each vector onto the unit sphere:
(cid:101)wj ← (cid:101)wj/(cid:107) (cid:101)wj(cid:107)2.

Solve v(r)

1 , v(r)

2

The corresponding sub-problem is

min
1 ,v(r)
v(r)

2

−y(r)
1

· v(r)

1 − y(r)
2 (cid:107) (cid:101)wp(r) − v(r)

1 (cid:107)2

2

+ ρ

· v(r)
2

2 + ρ

2 (cid:107) (cid:101)wq(r) − v(r)

2 (cid:107)2
2

s.t.

(cid:107)v(r)
v(r)
1

1 (cid:107)2 = 1, (cid:107)v(r)

· v(r)

2 ≤ τ, −v(r)

1

2 (cid:107)2 = 1,
· v(r)

2 ≤ τ

Let γ1, γ2, λ1 ≥ 0, λ2 ≥ 0 be the KKT multipliers associ-
ated with the four constraints in this sub-problem. Accord-
ing to the KKT conditions, we have

−y(r)

1 +ρ(v(r)

1 − (cid:101)wp(r))+2γ1v(r)

1 +(λ1−λ2)v(r)

2 = 0 (6)

−y(r)

2 +ρ(v(r)

2 − (cid:101)wq(r))+2γ2v(r)

2 +(λ1−λ2)v(r)

1 = 0 (7)

We solve these two equations by examining four cases.

1 = y(r)

Case 1 First, we assume λ1 = 0, λ2 = 0, then (ρ +
2γ1)v(r)
1 + ρ (cid:101)wp(r) and (ρ + 2γ2)v(r)
2 +
ρ (cid:101)wq(r). According to the primal feasibility (cid:107)v(r)
1 (cid:107)2 = 1
and (cid:107)v(r)

2 = y(r)

2 (cid:107)2 = 1, we know

v(r)
1 =

y(r)
1 + ρ (cid:101)wp(r)
(cid:107)y(r)
1 + ρ (cid:101)wp(r)(cid:107)2

, v(r)

2 =

y(r)
2 + ρ (cid:101)wq(r)
(cid:107)y(r)
2 + ρ (cid:101)wq(r)(cid:107)2

Then we check whether the constraint |v(r)
1
satisﬁed. If so, then v(r)
1

· v(r)
2 | ≤ τ is
are the optimal solution.

and v(r)
2

Case 2 We assume λ1 > 0 and λ2 = 0, then

(ρ + 2γ1)v(r)

1 + λ1v(r)

2 = y(r)

1 + ρ (cid:101)wp(r)

(ρ + 2γ2)v(r)

2 + λ1v(r)

1 = y(r)

2 + ρ (cid:101)wq(r)

(8)

(9)

According to the complementary slackness condition, we
know v(r)
2 = τ . For the vectors on both sides of
1
Eq.(8), taking the square of their (cid:96)2 norm, we get

· v(r)

(ρ+2γ1)2 +λ2

1 +2(ρ+2γ1)λ1τ = (cid:107)y(r)

1 +ρ (cid:101)wp(r)(cid:107)2

2 (10)

Similarly, from Eq.(9), we get

(ρ+2γ2)2 +λ2

1 +2(ρ+2γ2)λ1τ = (cid:107)y(r)

2 +ρ (cid:101)wq(r)(cid:107)2

2 (11)

Taking the inner product of the two vectors on the left hand
sides of Eq.(8,9), and that on the right hand sides, we get

(2ρ + 2γ1 + 2γ2)λ1 + ((ρ + 2γ1)(ρ + 2γ2) + λ2
= (y(r)

1 + ρ (cid:101)wp(r))(cid:62)(y(r)

2 + ρ (cid:101)wq(r))

1)τ

(12)
Solving the system of equations consisting of Eq.(10-12),
we obtain the optimal values of γ1, γ2 and λ1. Plugging
them into Eq.(8) and Eq.(9), we obtain a solution of v(r)
1
and v(r)
2 . Then we check whether this solution satisﬁes
· v(r)
−v(r)
1

2 ≤ τ . If so, this is an optimal solution.

In Case 3, we discuss λ1 = 0, λ2 > 0.
In Case 4, we
discuss λ1 > 0, λ2 > 0. The corresponding problems can
be solved in a similar way as Case 2. Please refer to the
supplements for details.

Learning Latent Space Models with Angular Constraints

Solve y(r)

1 , y(r)

2 We simply perform the following:

1 = y(r)
y(r)

2 = y(r)
y(r)

1 + ρ( (cid:101)wp(r) − v(r)
1 )
2 + ρ( (cid:101)wq(r) − v(r)
2 )

empirical risk minimizer. We aim to analyze the general-
ization error L( ˆf ) of the empirical risk minimizer ˆf . L( ˆf )
can be decomposed into L( ˆf ) = L( ˆf ) − L(f ∗) + L(f ∗),
where L( ˆf ) − L(f ∗) is the estimation error and L(f ∗) is
the approximation error.

(13)

(14)

Compared with a vanilla backpropagation algorithm, the
major extra cost in this ADMM algorithm comes from solv-
ing the R = m(m − 1) pairs of vectors {v(r)
r=1.
Solving each pair incurs O(m) cost. The R pairs bring in
a total cost of O(m3). Such a cubic cost is also incurred in
other decorrelation methods such as (Le et al., 2010; Bao
et al., 2013). In practice, m is typically less than 1000. This
O(m3) cost does not substantially bottleneck computation,
as we will validate in experiments.

1 , v(r)

2 }R

4. Analysis

In this section, we discuss how the parameter τ which con-
trols the level of diversity affects the generalization perfor-
mance of sparse coding and neural network.

4.1. Sparse Coding

Following (Vainsencher et al., 2011), we assume the data
example x ∈ Rd and basis vector w ∈ Rd are both of unit
length, and the linear coefﬁcient vector a ∈ Rm is at most
k sparse, i.e., (cid:107)a(cid:107)0 ≤ k. The estimation error of dictionary
W is deﬁned as

L(W) = Ex∼p∗ [min(cid:107)a(cid:107)0≤k (cid:107)x −

ajwj(cid:107)2]. (15)

(cid:88)m

j=1

(cid:80)n

Let (cid:101)L(W) = 1
j=1 ajwj(cid:107)2 be
n
the empirical reconstruction error on n samples. We have
the following theorem.

i=1 min(cid:107)a(cid:107)0≤k (cid:107)xi − (cid:80)m

Theorem 1 Assume τ < 1

k , with probability at least 1 − δ:

L(W) ≤ (cid:101)L(W)+

(cid:115)

√

nk
1−kτ

dm ln 4
2n

(cid:114)

+

ln 1/δ
2n

+

(cid:114) 4
n

Note that the right hand side is an increasing function w.r.t
τ . As expected, a smaller τ (implying more diversity)
would induce a lower estimation error bound.

4.2. Neural Network

The generalization error of a hypothesis f repre-
sented with a neural network is deﬁned as L(f ) =
E(x,y)∼p∗ [(cid:96)(f (x), y)], where p∗ is the distribution of input-
output pair (x, y) and (cid:96)(·, ·) is the loss function. The train-
ing error is ˆL(f ) = 1
i=1 (cid:96)(f (x(i)), y(i)), where n is
n
the number of training samples. Let f ∗ ∈ argminf ∈F L(f )
be the true risk minimizer and ˆf ∈ argminf ∈F
ˆL(f ) be the

(cid:80)n

For simplicity, we start with a “simple” fully connected net-
work with one hidden layer of m units, used for univariate
regression (one output unit) with squared loss. Analysis
for more complicated fully connected NNs with multiple
hidden layers can be achieved in a straightforward way by
cascading our analysis for this “simple” fully connected
NN. Let x ∈ Rd be the input vector and y be the response
value. For simplicity, we assume max{(cid:107)x(cid:107)2, |y|} ≤ 1. Let
wj ∈ Rd be the weights connecting the j-th hidden unit
with input units, with (cid:107)wj(cid:107)2 ≤ C.

Let α be a vector where αj is the weight connecting hid-
den unit j to the output unit, with (cid:107)α(cid:107)2 ≤ B. We as-
sume the activation function h(t) applied on the hidden
units is Lipschitz continuous with constant L. Commonly
used activation functions such as rectiﬁed linear h(t) =
max(0, t), tanh h(t) = (et − e−t)/(et + e−t), and sig-
moid h(t) = 1/(1 + e−t) are all Lipschitz continuous with
L = 1, 1, 0.25, respectively. Consider the hypothesis set

F = {x (cid:55)→

αjh(w(cid:62)

j x) | (cid:107)α(cid:107)2 ≤ B, (cid:107)wj(cid:107)2 ≤ C,

m
(cid:88)

j=1

∀i (cid:54)= j, |wi · wj| ≤ τ (cid:107)wi(cid:107)2(cid:107)wj(cid:107)2}.

The estimation error given in Theorem 2 below indicates
how well the algorithm is able to learn from the samples.

Theorem 2 Let the activation function h be L-Lipschitz
continuous and the loss (cid:96)(ˆy, y) = 1
2 (ˆy − y)2. Then, with
probability at least 1 − δ:

L( ˆf )−L(f ∗) ≤

γ2(cid:112)2 ln(4/δ) + 4γB(2CL + |h(0)|)
√

√

m

(17)

n

√

. (16)

where γ = 1 + BCL(cid:112)(m − 1)τ + 1 +

mB|h(0)|.

Note that γ, hence the above bound on estimation error, de-
creases as τ becomes smaller. The bound goes to zero as n
(sample size) goes to inﬁnite. The inverse square root de-
pendence on n matches existing results (Bartlett & Mendel-
son, 2003). We note that it is straightforward to extend our
bound to any bounded Lipschitz continuous loss (cid:96).

The approximation error indicates how capable the hypoth-
esis set F is to approximate a target function g = E[y|x],
where the error is measured by minf ∈F (cid:107)f − g(cid:107)L2 and
L2 = (cid:82) (f (x) − g(x))2P (dx). Following (Bar-
(cid:107)f − g(cid:107)2
ron, 1993), we assume the target function g satisﬁes certain
smoothness condition that is expressed in the ﬁrst moment
of its Fourier representation: (cid:82) (cid:107)ω(cid:107)2|˜g(ω)|dω ≤ B/2

Learning Latent Space Models with Angular Constraints

where ˜g(ω) is the Fourier representation of g(x). For such
function the following theorem states its approximation er-
ror. (In order to derive explicit constants we restrict h to be
the sigmoid function, but other Lipschitz continuous acti-
vation function can be similarly handled.)

π
2 −θ
θ (cid:99) + 1), where
Theorem 3 Let C > 1, m ≤ 2((cid:98)
θ = arccos(τ ), and h(t) = 1/(1 + e−t). Then, there is
a function f ∈ F such that

(cid:107)f − g(cid:107)L2 ≤ B( 1√

m + 1+2 ln C
√

C

)+

+ 2

mBC sin( min(3mθ,π)

).

(18)

2

This theorem implies that whether to use the angular con-
straint (AC) or not has a signiﬁcant inﬂuence on the ap-
proximate error bound: without using AC (τ = 1), the
bound is a decreasing function of m (the number of hid-
den units); using AC (τ < 1), the bound increases with
m. This striking phrase-change indicates the impact of AC.
Given a ﬁxed m, the bound decreases with τ , implying that
a stronger regularization (smaller τ ) incurs larger approxi-
mation error. When τ = 1, the second term in the bound
vanishes and the bound is reduced to the one in (Barron,
1993), which is a decreasing function of m (and C, the up-
per bound on the weights). When τ < 1, the second term
π
2 −θ
θ (cid:99)+1). This
increases with m up to the upper bound 2((cid:98)
is because a larger number of hidden units bear a larger
difﬁculty in satisfying the pairwise ACs, which causes the
function space F to shrink rapidly; accordingly, the ap-
proximation power of F decreases quickly.

The analysis in the two theorems shows that τ incurs a
tradeoff between the estimation error and the approxima-
tion error: decreasing τ reduces the estimation error and
enlarges the approximation error. Since the generalization
error is the sum of the estimation error and the approxi-
mation error, τ has an optimal value to yield the minimal
generalization error.

5. Experiments

5.1. Sparse Coding

In this section, we present experimental results. Due to
space limit, we put some results into supplements.

Following (Yang et al., 2009), we applied sparse coding
for image feature learning. We used three datasets in the
experiments: Scenes-15 (Lazebnik et al., 2006), Caltech-
256 (Grifﬁn et al., 2007) and UIUC-Sport (Li & Fei-Fei,
2007). For each dataset, ﬁve random train/test splits are
performed and the results are averaged over the ﬁve runs.
We extract pixel-level dense SIFT (Lowe, 2004) features
where the step size and patch size are 8 and 16 respec-
tively. On top of the SIFT features, we use sparse cod-
ing methods to learn a dictionary and represent each SIFT

Scene
83.6 ± 0.2
SC
DCM-SC 85.4 ± 0.5
84.8 ± 0.6
CS-SC
84.6 ± 0.3
DPP-SC
85.5 ± 0.1
IC-SC
86.1 ± 0.5
MA-SC
86.5 ± 0.7
AC-SC

Caltech
42.3 ± 0.4
44.7 ± 0.8
45.4 ± 0.5
43.5 ± 0.3
43.9 ± 0.7
45.6 ± 0.4
46.1 ± 0.3

Sports
87.4 ± 0.5
89.6 ± 0.1
88.3 ± 0.3
88.1 ± 0.2
90.2 ± 0.7
89.7 ± 0.4
90.9 ± 0.3

Table 1. Classiﬁcation accuracy (%) on three datasets.

feature into a sparse code. To obtain image-level features,
we apply max-pooling (Yang et al., 2009) and spatial pyra-
mid matching (Lazebnik et al., 2006; Yang et al., 2009)
over the pixel-level sparse codes. Then a linear SVM is
applied to classify the images. We compare with other
diversity-promoting regularizers including determinant of
covariance matrix (DCM) (Malkin & Bilmes, 2008), co-
sine similarity (CS) (Yu et al., 2011), determinantal point
process (DPP) (Kulesza & Taskar, 2012; Zou & Adams,
2012b), InCoherence (IC) (Bao et al., 2013) and mutual
angles (MA) (Xie et al., 2015). We use 5-fold cross val-
idation to tune τ in {0.3, 0.4, · · · , 1} and the number of
basis vectors in {50, 100, 200, · · · , 500}. The parameter ρ
in ADMM is set to 1.

Table 1 shows the classiﬁcation accuracy on three datasets,
from which we can see that compared with unregularized
SC, AC-SC greatly improves performance. For example,
on the Sports dataset, AC improves the accuracy from
87.4% to 90.9%. This suggests that AC is effective in
reducing overﬁtting and improving generalization perfor-
mance. Compared with other diversity-promoting regular-
izers, AC achieves better performance, demonstrating its
better efﬁcacy in promoting diversity.

5.2. Neural Networks

We evaluate AC on three types of neural networks: fully-
connected NN (FNN) for phone recognition (Hinton et al.,
2012), CNN for image classiﬁcation (Krizhevsky et al.,
2012), and RNN for question answering (Seo et al., 2017).
In the main paper, we report results on four datasets:
TIMIT1, CIFAR-102, CNN (Hermann et al., 2015), Daily
Mail (Hermann et al., 2015). Please refer to the supple-
ments for results on other datasets.

FNN for Phone Recognition The TIMIT dataset con-
tains a total of 6300 sentences (5.4 hours), divided into
a training set (462 speakers), a validation set (50 speak-
ers) and a core test set (24 speakers). We used the Kaldi
(Povey et al., 2011) toolkit to train the monophone system
which was utilized to do forced alignment and to get la-
bels for speech frames. The toolkit was also utilized to
preprocess the data into log-ﬁlter banks. Among meth-
ods based on FNN, Karel’s recipe in Kaldi achieves state

1https://catalog.ldc.upenn.edu/LDC93S1
2https://www.cs.toronto.edu/ kriz/cifar.html

Learning Latent Space Models with Angular Constraints

Network
Segmental NN (Abdel-Hamid et al., 2013)
MCRBM (Dahl et al., 2010)
DSR (Tang et al., 2015)
Rectiﬁer NN (T´oth, 2013)
DBN (Srivastava et al., 2014)
Shallow CNN (Ba & Caruana, 2014)
Structured DNN (Yang et al., 2016)
Posterior Modeling (Prabhavalkar et al., 2013)
Kaldi
CS-Kaldi
IC-Kaldi
MA-Kaldi
DC-Kaldi
AC-Kaldi
CTC (Graves et al., 2013)
RNN Transducer (Graves et al., 2013)
Attention RNN (Chorowski et al., 2015)
CTC+SCRF (Lu et al., 2017)
Segmental RNN (Lu et al., 2016)
RNNDrop (Moon et al., 2015)
CNN (T´oth, 2014)

Error
21.9
20.5
19.9
19.8
19.7
19.5
18.8
18.5
18.53
18.48
18.46
18.51
18.50
18.41
18.4
17.7
17.6
17.4
17.3
16.9
16.7

Table 2. Phone error rate (%) on the TIMIT test set.

of the art performance. We apply AC to the FNN in this
recipe. The inputs of the FNN are the FMLLR (Gales,
1998) features of the neighboring 21 frames, which are
mean centered and normalized to have unit variance. The
number of hidden layers is 4. Each layer has 1024 hid-
den units. Stochastic gradient descent (SGD) is used to
train the network. The learning rate is set to 0.008. We
compare with four diversity-promoting regularizers: CS,
IC, MA and DeCorrelation (DC) (Cogswell et al., 2015).
The regularization parameter in these methods are tuned in
{10−6, 10−5, · · · , 105}. The β parameter in IC is set to 1.

Table 2 shows state of the art phone error rate (PER) on the
TIMIT core test set. Methods in the ﬁrst panel are mostly
based on FNN, which perform less well than Kaldi. Meth-
ods in the third panel are all based on RNNs which in gen-
eral perform better than FNN since they are able to capture
the temporal structure in speech data. In the second panel,
we compare AC with other diversity-promoting regulariz-
ers. Without regularization, the error is 18.53%. With AC,
the error is reduced to 18.41%, which is very close to a
strong RNN-based baseline Connectionist Temporal Clas-
siﬁcation (CTC) (Graves et al., 2013). Besides, AC outper-
forms other regularizers.

CNN for Image Classiﬁcation The CIFAR-10 dataset
contains 32x32 color images from 10 categories, with
50,000 images for training and 10,000 for testing. We used
5000 training images as the validation set to tune hyperpa-
rameters. The data is augmented by ﬁrst zero-padding the
images with 4 pixels on each side, then randomly cropping

Network
Maxout (Goodfellow et al., 2013)
NiN (Lin et al., 2013)
DSN (Lee et al., 2015)
Highway Network (Srivastava et al., 2015)
All-CNN (Springenberg et al., 2014)
ResNet (He et al., 2016)
ELU-Network (Clevert et al., 2015)
LSUV (Mishkin & Matas, 2015)
Fract. Max-Pooling (Graham, 2014)
WideResNet (Huang et al., 2016)
CS-WideResNet
IC-WideResNet
MA-WideResNet
DC-WideResNet
OP-WideResNet
AC-WideResNet
ResNeXt (Xie et al., 2016b)
PyramidNet (Huang et al., 2016)
DenseNet (Huang et al., 2016)
PyramidSepDrop (Yamada et al., 2016)

Error
9.38
8.81
7.97
7.60
7.25
6.61
6.55
5.84
4.50
3.89
3.81
3.85
3.68
3.77
3.69
3.63
3.58
3.48
3.46
3.31

Table 3. Classiﬁcation error (%) on CIFAR-10 test set

the padded images to reproduce 32x32 images. We apply
AC to wide residual network (WideResNet) (Zagoruyko
& Komodakis, 2016) where the depth is set to 28 and
the width is set to 10. SGD is used for training, with
epoch number 200, initial learning rate 0.1, minibatch size
128, Nesterov momentum 0.9, dropout probability 0.3 and
weight decay 0.0005. The learning rate is dropped by 0.2
at 60, 120 and 160 epochs. The performance is the me-
dian of 5 runs. We compare with CS, IC, MA, DC and
an Orthogonality-Promoting (OP) regularizer (Rodr´ıguez
et al., 2016).

Table 3 shows state of the art classiﬁcation error on the test
set. Compared with the unregularized WideResNet which
achieves an error of 3.89%, applying AC reduces the error
to 3.63%. AC achieves lower error than other regularizers.

LSTM for Question Answering We apply AC to long
short-term memory (LSTM) (Hochreiter & Schmidhuber,
1997) network, which is a type of RNN. Given the input xt
at timestamp t, LSTM produces a hidden state ht based on
the following transition equations:

it = σ(W(i)xt + U(i)ht−1 + b(i))
ft = σ(W(f )xt + U(f )ht−1 + b(f ))
ot = σ(W(o)xt + U(o)ht−1 + b(o))
ct = it (cid:12) tanh(W(c)xt + U(c)ht−1 + b(c)) + ft (cid:12) ct−1
ht = ot (cid:12) tanh(ct)

where Ws are Us are gate-speciﬁc weight matrices. On
the row vectors of each weight matrix, the AC is applied.
The LSTM is used for a question answering (QA) task on
two datasets: CNN and DailyMail (Hermann et al., 2015),

Learning Latent Space Models with Angular Constraints

Hermann et al. (2015)
Hill et al. (2015)
Kadlec et al. (2016)
Kobayashi et al. (2016)
Sordoni et al. (2016)
Trischler et al. (2016)
Chen et al. (2016)
Cui et al. (2016)
Shen et al. (2016)
BIDAF
CS-BIDAF
IC-BIDAF
MA-BIDAF
DC-BIDAF
AC-BIDAF
Dhingra et al. (2016)
Dhingra et al. (2017)

CNN

Dev
61.6
63.4
68.6
71.3
72.6
73.4
73.8
73.1
72.9
76.31
76.43
76.41
76.49
76.35
76.62
77.9
79.2

Test
63.0
6.8
69.5
72.9
73.3
74.0
73.6
74.4
74.7
76.94
77.10
77.21
77.09
77.15
77.23
77.9
78.6

DailyMail
Test
Dev
69.0
70.5
-
-
73.9
75.0
-
-
-
-
-
-
76.6
77.6
-
-
76.6
77.6
79.63
80.33
79.71
80.37
79.83
80.49
79.74
80.42
79.67
80.38
79.88
80.65
80.9
81.5
–
–

Table 4. Accuracy (%) on the two QA datasets

each containing a training, development and test set with
300k/4k/3k and 879k/65k/53k examples respectively. Each
example consists of a passage, a question and an answer.
The question is a cloze-style task where an entity is re-
placed by a placeholder and the goal is to infer this miss-
ing entity (answer) from all the possible entities appear-
ing in the passage. The LSTM architecture and exper-
imental settings follow the Bidirectional Attention Flow
(BIDAF) (Seo et al., 2017) model, which consists of the
following layers: character embedding, word embedding,
contextual embedding, attention ﬂow, modeling and out-
put. LSTM is applied in the contextual embedding and
modeling layer. Character embedding is based on one-
dimensional convolutional neural network, where the num-
ber of ﬁlters is set to 100 and the width of receptive ﬁeld
is set to 5. In LSTM, the size of hidden state is set to 100.
Optimization is based on AdaDelta (Zeiler, 2012), where
the minibatch size and initial learning rate are set to 48 and
0.5. The model is trained for 8 epochs. Dropout
(Sri-
vastava et al., 2014) with probability 0.2 is applied. We
compare with four diversity promoting regularizers: CS,
IC, MA and DC.

Table 4 shows state of the art accuracy on the two datasets.
As can be seen, after applying AC to BIDAF, the accu-
racy is improved from 76.94% to 77.23% on the CNN test
set and from 79.63% to 79.88% on the DailyMail test set.
Among the diversity-promoting regularizers, AC achieves
the highest accuracy.

5.3. Sensitivity to Parameter τ

In the theoretical analysis presented in Section 4, we have
shown that the parameter τ which controls the level of near-

Figure 1. Phone error rate on TIMIT, under varying τ

No regularization
CS
IC
MA
DC
OP
AC

TIMIT CIFAR-10 CNN
69.4
6.3
74.8
6.8
76.1
6.7
78.6
7.0
82.9
7.6
–
6.8
79.7
7.1

1.1
1.2
1.2
1.3
1.5
–
1.3

Table 5. Average runtime (hours)

orthogonality (or diversity) incurs a tradeoff between esti-
mation error and approximation error. In this section, we
provide an empirical veriﬁcation, using FNN on TIMIT as
a study case. Figure 1 shows how the phone error rates vary
on the TIMIT core test set. As can be seen, the lowest test
error is achieved under a moderate τ (= 0.75). Either a
smaller or a larger τ degrades the performance. This em-
pirical observation is aligned with the theoretical analysis
that the best generalization performance is achieved under
a properly chosen τ . When τ is close to 0, the hidden units
are close to orthogonality, which yields much poorer per-
formance. This conﬁrms that the strict-orthogonality con-
straint proposed by (Le et al., 2010) is too restrictive and is
less favorable than a “soft” regularization approach.

5.4. Computational Time

We compare the computational time of neural networks un-
der different regularizers. Table 5 shows the total runtime
time of FNNs on TIMIT and CNNs on CIFAR-10 with a
single GTX TITAN X GPU, and the runtime of LSTM net-
works on the CNN dataset with 2 TITAN X GPUs. Com-
pared with no regularization, AC incurs a 18.2% extra time
on TIMIT, 12.7% on CIFAR-10 and 14.8% on CNN. The
runtime of AC is comparable to that under other diversity-
promoting regularizers.

6. Conclusions

In this paper, we propose Angled-Constrained Latent Space
Models (AC-LSMs) that aim at promoting diversity among
components in LSMs for the sake of alleviating overﬁt-
ting. Compared with previous diversity-promoting meth-
ods, AC has two beneﬁts. First, it is theoretically analyz-
able: the generalization error analysis shows that larger di-
versity leads to smaller estimation error and larger approx-
imation error. Second, it is empirically effective, as vali-
dated in various experiments.

18.418.4518.518.5518.618.6518.718.7518.80.550.60.650.70.750.80.850.90.951Phoneerrorrate𝜏Learning Latent Space Models with Angular Constraints

Acknowledgements

We would like to thank the anonymous reviewers for
the suggestions and comments that help to improve
this work a lot, and thank Yajie Miao for helping
with some of the experiments. P.X and E.X are sup-
ported by National Institutes of Health P30DA035778,
R01GM114311, National Science Foundation IIS1617583,
DARPA FA872105C0003 and Pennsylvania Department of
Health BD4BH4100070287.

References

Abdel-Hamid, Ossama, Deng, Li, Yu, Dong, and Jiang,
Hui. Deep segmental neural networks for speech recog-
nition. INTERSPEECH, 2013.

Affandi, Raja Haﬁz, Fox, Emily, and Taskar, Ben. Approx-
imate inference in continuous determinantal processes.
In Advances in Neural Information Processing Systems,
pp. 1430–1438, 2013.

Ba, Jimmy and Caruana, Rich. Do deep nets really need to
be deep? In Advances in neural information processing
systems, pp. 2654–2662, 2014.

Bao, Yebo, Jiang, Hui, Dai, Lirong, and Liu, Cong. Inco-
herent training of deep neural networks to de-correlate
bottleneck features for speech recognition. In Acoustics,
Speech and Signal Processing (ICASSP), 2013 IEEE In-
ternational Conference on, pp. 6980–6984. IEEE, 2013.

Barron, Andrew R. Universal approximation bounds for
superpositions of a sigmoidal function. Information The-
ory, IEEE Transactions on, 1993.

Bartlett, Peter L and Mendelson, Shahar. Rademacher and
gaussian complexities: Risk bounds and structural re-
sults. Journal of Machine Learning Research, 3:463–
482, 2003.

Blei, David M, Ng, Andrew Y, and Jordan, Michael I. La-
tent dirichlet allocation. the Journal of machine Learn-
ing research, 2003.

Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein,
J. Distributed optimization and statistical learning via
alternating direction method of multipliers. Foundations
and Trends R(cid:13) in Machine Learning, 2011.

Chen, Danqi, Bolton, Jason, and Manning, Christopher D.
A thorough examination of the cnn/daily mail reading
comprehension task. arXiv preprint arXiv:1606.02858,
2016.

Chorowski, Jan K, Bahdanau, Dzmitry, Serdyuk, Dmitriy,
Cho, Kyunghyun, and Bengio, Yoshua. Attention-based
models for speech recognition. In Advances in Neural
Information Processing Systems, pp. 577–585, 2015.

Clevert, Djork-Arn´e, Unterthiner, Thomas, and Hochre-
iter, Sepp.
Fast and accurate deep network learn-
ing by exponential linear units (elus). arXiv preprint
arXiv:1511.07289, 2015.

Cogswell, M., Ahmed, F., Girshick, R., Zitnick, L., and Ba-
tra, D. Reducing overﬁtting in deep networks by decor-
relating representations. ICLR, 2015.

Cui, Yiming, Chen, Zhipeng, Wei, Si, Wang, Shijin, Liu,
Ting, and Hu, Guoping. Attention-over-attention neu-
ral networks for reading comprehension. arXiv preprint
arXiv:1607.04423, 2016.

Dahl, George, Mohamed, Abdel-rahman, Hinton, Ge-
offrey E, et al.
Phone recognition with the mean-
covariance restricted boltzmann machine. In Advances
in neural information processing systems, pp. 469–477,
2010.

Dhingra, Bhuwan, Liu, Hanxiao, Cohen, William W, and
Salakhutdinov, Ruslan. Gated-attention readers for text
comprehension. arXiv preprint arXiv:1606.01549, 2016.

Dhingra, Bhuwan, Yang, Zhilin, Cohen, William W, and
Salakhutdinov, Ruslan. Linguistic knowledge as mem-
arXiv preprint
ory for recurrent neural networks.
arXiv:1703.02620, 2017.

Gales, Mark JF. Maximum likelihood linear transforma-
tions for hmm-based speech recognition. Computer
speech & language, 12(2):75–98, 1998.

Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi,
Courville, Aaron, and Bengio, Yoshua. Maxout net-
works. ICML, 2013.

Graham, Benjamin.

Fractional max-pooling.

arXiv

preprint arXiv:1412.6071, 2014.

Graves, Alex, Mohamed, Abdel-rahman, and Hinton, Ge-
offrey. Speech recognition with deep recurrent neu-
ral networks. In Acoustics, speech and signal process-
ing (icassp), 2013 ieee international conference on, pp.
6645–6649. IEEE, 2013.

Grifﬁn, Gregory, Holub, Alex, and Perona, Pietro. Caltech-

256 object category dataset. 2007.

Chen, Yunpeng, Jin, Xiaojie, Feng, Jiashi, and Yan,
Training group orthogonal neural net-
arXiv preprint

Shuicheng.
works with privileged information.
arXiv:1701.06772, 2017.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, pp. 770–778, 2016.

Learning Latent Space Models with Angular Constraints

Henaff, Mikael, Szlam, Arthur, and LeCun, Yann. Or-
thogonal rnns and long-memory tasks. arXiv preprint
arXiv:1602.06662, 2016.

Hermann, Karl Moritz, Kocisky, Tomas, Grefenstette, Ed-
ward, Espeholt, Lasse, Kay, Will, Suleyman, Mustafa,
and Blunsom, Phil. Teaching machines to read and com-
prehend. In Advances in Neural Information Processing
Systems, pp. 1693–1701, 2015.

Hill, Felix, Bordes, Antoine, Chopra, Sumit, and Weston,
Jason. The goldilocks principle: Reading children’s
ICLR,
books with explicit memory representations.
2015.

Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E,
Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, An-
drew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath,
Tara N, et al. Deep neural networks for acoustic mod-
eling in speech recognition: The shared views of four
research groups. Signal Processing Magazine, IEEE,
2012.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-
term memory. Neural computation, 9(8):1735–1780,
1997.

Huang, Gao, Liu, Zhuang, Weinberger, Kilian Q, and
van der Maaten, Laurens. Densely connected convolu-
tional networks. arXiv preprint arXiv:1608.06993, 2016.

Kadlec, Rudolf, Schmid, Martin, Bajgar, Ondrej, and
Kleindienst, Jan. Text understanding with the attention
sum reader network. ACL, 2016.

Kobayashi, Sosuke, Tian, Ran, Okazaki, Naoaki, and
Inui, Kentaro. Dynamic entity representation with max-
In Proceedings of
pooling improves machine reading.
NAACL-HLT, pp. 850–855, 2016.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in neural information processing
systems, 2012.

Kulesza, Alex and Taskar, Ben. Determinantal point pro-
cesses for machine learning. Foundations and Trends in
Machine Learning, 2012.

Lazebnik, Svetlana, Schmid, Cordelia, and Ponce, Jean.
Beyond bags of features: Spatial pyramid matching for
recognizing scene categories. In CVPR, 2006.

Le, Quoc V, Ngiam, Jiquan, Chen, Zhenghao, Chia, Daniel,
Koh, Pang Wei, and Ng, Andrew Y. Tiled convolu-
tional neural networks. In Proceedings of the 23rd Inter-
national Conference on Neural Information Processing
Systems, pp. 1279–1287. Curran Associates Inc., 2010.

Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick, Zhang,
Zhengyou, and Tu, Zhuowen. Deeply-supervised nets.
In Artiﬁcial Intelligence and Statistics, pp. 562–570,
2015.

Li, Li-Jia and Fei-Fei, Li. What, where and who? classify-

ing events by scene recognition. In ICCV, 2007.

Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in

network. arXiv preprint arXiv:1312.4400, 2013.

Lowe, David G. Distinctive image features from scale-

invariant keypoints. IJCV, 2004.

Lu, Liang, Kong, Lingpeng, Dyer, Chris, Smith, Noah A,
and Renals, Steve.
Segmental recurrent neural net-
works for end-to-end speech recognition. arXiv preprint
arXiv:1603.00223, 2016.

Lu, Liang, Kong, Lingpeng, Dyer, Chris, and Smith,
Noah A. Multi-task learning with ctc and seg-
arXiv preprint
mental crf for speech recognition.
arXiv:1702.06378, 2017.

Malkin, Jonathan and Bilmes, Jeff. Ratio semi-deﬁnite
classiﬁers. In Acoustics, Speech and Signal Processing,
2008. ICASSP 2008. IEEE International Conference on,
pp. 4113–4116. IEEE, 2008.

Mao, Fengling, Xiong, Wei, Du, Bo, and Zhang, Lefei.
Stochastic decorrelation constraint regularized auto-
encoder for visual recognition. In International Confer-
ence on Multimedia Modeling, pp. 368–380. Springer,
2017.

Mishkin, Dmytro and Matas, Jiri. All you need is a good

init. arXiv preprint arXiv:1511.06422, 2015.

Moon, Taesup, Choi, Heeyoul, Lee, Hoshik, and Song,
Inchul. Rnndrop: A novel dropout for rnns in asr. In Au-
tomatic Speech Recognition and Understanding (ASRU),
2015 IEEE Workshop on, pp. 65–70. IEEE, 2015.

Olshausen, Bruno A and Field, David J. Sparse coding with
an overcomplete basis set: A strategy employed by v1?
Vision research, 37(23):3311–3325, 1997.

Povey, Daniel, Ghoshal, Arnab, Boulianne, Gilles, Bur-
get, Lukas, Glembek, Ondrej, Goel, Nagendra, Hanne-
mann, Mirko, Motlicek, Petr, Qian, Yanmin, Schwarz,
Petr, et al. The kaldi speech recognition toolkit. In IEEE
2011 workshop on automatic speech recognition and un-
derstanding, number EPFL-CONF-192584. IEEE Signal
Processing Society, 2011.

Prabhavalkar, Rohit, Sainath, Tara N, Nahamoo, David,
Ramabhadran, Bhuvana, and Kanevsky, Dimitri. An
evaluation of posterior modeling techniques for phonetic

Learning Latent Space Models with Angular Constraints

recognition. In Acoustics, Speech and Signal Processing
(ICASSP), 2013 IEEE International Conference on, pp.
7165–7169. IEEE, 2013.

Ramirez,

Ignacio, Sprechmann, Pablo,

and Sapiro,
Guillermo. Classiﬁcation and clustering via dictionary
learning with structured incoherence and shared fea-
In Computer Vision and Pattern Recognition
tures.
(CVPR), 2010 IEEE Conference on, pp. 3501–3508.
IEEE, 2010.

Recht, Benjamin, Fazel, Maryam, and Parrilo, Pablo A.
Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM review,
52(3):471–501, 2010.

Rodr´ıguez, Pau, Gonz`alez, Jordi, Cucurull, Guillem, Gon-
faus, Josep M, and Roca, Xavier. Regularizing cnns
with locally constrained decorrelations. arXiv preprint
arXiv:1611.01967, 2016.

Seo, Minjoon, Kembhavi, Aniruddha, Farhadi, Ali, and
Hajishirzi, Hannaneh. Bidirectional attention ﬂow for
machine comprehension. ICLR, 2017.

Shen, Yelong, Huang, Po-Sen, Gao, Jianfeng, and Chen,
Weizhu. Reasonet: Learning to stop reading in machine
comprehension. arXiv preprint arXiv:1609.05284, 2016.

Sordoni, Alessandro, Bachman, Philip, Trischler, Adam,
Iterative alternating neu-
arXiv preprint

and Bengio, Yoshua.
ral attention for machine reading.
arXiv:1606.02245, 2016.

Springenberg, Jost Tobias, Dosovitskiy, Alexey, Brox,
Striving for sim-
arXiv preprint

Thomas, and Riedmiller, Martin.
plicity: The all convolutional net.
arXiv:1412.6806, 2014.

Srivastava, Nitish, Hinton, Geoffrey E, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:
a simple way to prevent neural networks from overﬁt-
Journal of Machine Learning Research, 15(1):
ting.
1929–1958, 2014.

Srivastava, Rupesh Kumar, Greff, Klaus, and Schmid-
arXiv preprint

huber, J¨urgen. Highway networks.
arXiv:1505.00387, 2015.

Tang, Hao, Wang, Weiran, Gimpel, Kevin, and Livescu,
Karen. Discriminative segmental cascades for feature-
rich phone recognition. In Automatic Speech Recogni-
tion and Understanding (ASRU), 2015 IEEE Workshop
on, pp. 561–568. IEEE, 2015.

Tibshirani, Robert. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society. Series
B (Methodological), pp. 267–288, 1996.

T´oth, L´aszl´o. Phone recognition with deep sparse rectiﬁer
neural networks. In Acoustics, Speech and Signal Pro-
cessing (ICASSP), 2013 IEEE International Conference
on, pp. 6985–6989. IEEE, 2013.

T´oth, L´aszl´o. Combining time-and frequency-domain con-
volution in convolutional neural network-based phone
In Acoustics, Speech and Signal Process-
recognition.
ing (ICASSP), 2014 IEEE International Conference on,
pp. 190–194. IEEE, 2014.

Trischler, Adam, Ye, Zheng, Yuan, Xingdi, and Sule-
man, Kaheer. Natural language comprehension with the
epireader. arXiv preprint arXiv:1606.02270, 2016.

Vainsencher, Daniel, Mannor, Shie, and Bruckstein, Al-
fred M. The sample complexity of dictionary learning.
Journal of Machine Learning Research, 12(Nov):3259–
3281, 2011.

Xie, Bo, Liang, Yingyu, and Song, Le. Diversity leads to

generalization in neural networks. AISTATS, 2017.

Xie, Pengtao. Learning compact and effective distance
metrics with diversity regularization. In ECML, 2015.

Xie, Pengtao, Deng, Yuntian, and Xing, Eric P. Diversify-
ing restricted boltzmann machine for document model-
ing. In ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining, 2015.

Xie, Pengtao, Zhu, Jun, and Xing, Eric. Diversity-
promoting bayesian learning of latent variable models.
In Proceedings of The 33rd International Conference on
Machine Learning, pp. 59–68, 2016a.

Xie, Saining, Girshick, Ross, Doll´ar, Piotr, Tu, Zhuowen,
transfor-
arXiv preprint

and He, Kaiming.
mations for deep neural networks.
arXiv:1611.05431, 2016b.

Aggregated residual

Xiong, Wei, Du, Bo, Zhang, Lefei, Hu, Ruimin, and Tao,
Dacheng. Regularizing deep convolutional neural net-
works with a structured decorrelation constraint. In Data
Mining (ICDM), 2016 IEEE 16th International Confer-
ence on, pp. 519–528. IEEE, 2016.

Yamada, Yoshihiro,

Iwamura, Masakazu, and Kise,
Koichi. Deep pyramidal residual networks with sepa-
rated stochastic depth. arXiv preprint arXiv:1612.01230,
2016.

Yang, J, Ragni, Anton, Gales, Mark JF, and Knill, Kate M.
Log-linear system combination using structured support
vector machines. In Proceedings of the Annual Confer-
ence of the International Speech Communication Associ-
ation, INTERSPEECH, volume 8, pp. 1898–1902, 2016.

Learning Latent Space Models with Angular Constraints

Yang, Jianchao, Yu, Kai, Gong, Yihong, and Huang,
Thomas. Linear spatial pyramid matching using sparse
coding for image classiﬁcation. In CVPR, 2009.

Yu, Yang, Li, Yu-Feng, and Zhou, Zhi-Hua. Diversity reg-

ularized machine. In IJCAI, 2011.

Yuan, Jinhui, Gao, Fei, Ho, Qirong, Dai, Wei, Wei, Jin-
liang, Zheng, Xun, Xing, Eric Po, Liu, Tie-Yan, and Ma,
Wei-Ying. Lightlda: Big topic models on modest com-
puter clusters. In Proceedings of the 24th International
Conference on World Wide Web, pp. 1351–1361. ACM,
2015.

Zagoruyko, Sergey and Komodakis, Nikos. Wide residual

networks. arXiv preprint arXiv:1605.07146, 2016.

Zeiler, Matthew D. Adadelta: an adaptive learning rate

method. arXiv preprint arXiv:1212.5701, 2012.

Zou, James Y. and Adams, Ryan P. Priors for diversity in

generative latent variable models. In NIPS, 2012a.

Zou, James Y and Adams, Ryan P. Priors for diversity in
generative latent variable models. In Advances in Neural
Information Processing Systems, pp. 2996–3004, 2012b.

