Sparse + Group-Sparse Dirty Models: Statistical Guarantees without
Unreasonable Conditions and a Case for Non-Convexity

Eunho Yang 1 2 Aur´elie C. Lozano 3

Abstract

Imposing sparse + group-sparse superposition
structures in high-dimensional parameter estima-
tion is known to provide ﬂexible regularization
that is more realistic for many real-world prob-
lems. For example, such a superposition en-
ables partially-shared support sets in multi-task
learning, thereby striking the right balance be-
tween parameter overlap across tasks and task
speciﬁcity. Existing theoretical results on esti-
mation consistency, however, are problematic as
they require too stringent an assumption: the in-
coherence between sparse and group-sparse su-
perposed components. In this paper, we ﬁll the
gap between the practical success and subopti-
mal analysis of sparse + group-sparse models,
by providing the ﬁrst consistency results that do
not require unrealistic assumptions. We also
study non-convex counterparts of sparse + group-
sparse models. Interestingly, we show that these
are guaranteed to recover the true support set
under much milder conditions and with smaller
sample size than convex models, which might be
critical in practical applications as illustrated by
our experiments.

1. Introduction

We consider high-dimensional statistical models where the
ambient dimension p is much larger than the number of
observations n. Under such high-dimensional scaling, it
is still possible to obtain consistent estimators by impos-
ing low-dimensional structural constraints upon the statis-
in compressed sens-
tical models, such as sparsity (e.g.

1School of Computing, KAIST, Daejeon, South Ko-
rea 2AItrics, Seoul, South Korea 3IBM T.J. Watson Re-
search Center, Yorktown Heights, NY, USA. Correspondence
to: Eunho Yang <eunhoy@kaist.ac.kr>, Aurelie C. Lozano
<aclozano@us.ibm.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ing (Baraniuk, 2007) and Lasso (Tibshirani, 1996)), low-
rank structure (Recht et al., 2007; Negahban & Wainwright,
2010), sparse graphical model structure (Friedman et al.,
2007; Ravikumar et al., 2008), and sparse additive struc-
ture for non-parametric models (Ravikumar et al., 2009). A
widely used approach to structured learning is via speciﬁc
regularization functions. For instance, (cid:96)1-regularization
is employed for sparse models (Tibshirani, 1996), (cid:96)1/(cid:96)q
norms for group sparsity (Yuan & Lin, 2006), and nuclear
norm for low-rank matrix-structured models (Cand`es &
Tao, 2010). Much attention has been devoted to the study
of these structured norms and their theoretical properties.

Such a “clean” regularization approach, however, might be
too stringent in practice. For instance in linear regression,
a blend of element-wise sparsity and group-sparsity might
be more appropriate than a purely sparse or purely group-
sparse solution. In multitask learning, while some parame-
ters might be shared across tasks, others might only be rel-
evant to a subset of tasks or a single task. To overcome
this limitation, a line of work on so-called dirty models
has emerged, which addresses this caveat by “mixing and
matching” different structures. One basic approach con-
sists in decomposing the model parameters as a sum of
two components, each penalized separately: one compo-
nent captures the common structure across tasks and the
other task-speciﬁc characteristics (Jalali et al., 2010; Gong
et al., 2012). For instance the dirty model in Jalali et al.
(2010) employs (cid:96)1,1 and (cid:96)1,∞ regularizers to the two com-
ponents. Chandrasekaran et al. (2011) consider the prob-
lem of recovering unknown low-rank and sparse matrices,
given the sum of their sum, with application such as opti-
cal imaging systems. Robust principal component analysis
and related extensions (Cand`es et al., 2011; Agarwal et al.,
2012; Hsu et al., 2011) estimate a covariance matrix that is
the sum of a low-rank matrix and a structured (e.g. sparse,
column sparse) matrix.

A general framework for studying dirty models was re-
cently proposed in Yang & Ravikumar (2013), which
bridges and extends several analyses for speciﬁc pairs
of superposition structures and speciﬁc statistical mod-
els (e.g., Jalali et al. (2010); Chandrasekaran et al. (2011);
Cand`es et al. (2011); Agarwal et al. (2012); Hsu et al.

Sparse + Group Sparse Dirty Models

(2011)) Speciﬁcally, this framework applies to a general
class of M -estimators employing a so-called hybrid reg-
ularization function, which is the inﬁmal convolution of
weighted regularization functions, one for each structural
component. This formulation is equivalent to an M -
estimator that combines a loss function applied to the sum
of multiple parameter vectors (one per structural compo-
nent) and a weighted sum of regularization functions (one
per parameter vector).

For the sparse + group sparse decomposition, however
existing analyses are highly problematic. The key weak-
ness is that they require some form of structural incoher-
ence condition which captures the interaction between the
different structured components. While such a structural
incoherence is a reasonable assumption for e.g.
sparse
+ low rank superposition, it is what too stringent for the
sparse+group sparse case because the two structures are
completely coherent for this case! This yields a key mo-
tivating question for this paper: Under the sparse + group
sparse setting, can we bypass structural incoherence con-
ditions and yet obtain tight error bounds?

In this paper we provide a positive answer by developing
a novel proof technique. Prior analyses require ‘local’ re-
stricted strong convexity conditions (RSC): one condition
for the sparse component and one for the group sparse
component. The use of structural incoherence between
sparse and group sparse components in then needed to
show ‘global’ RSC for the vector concatenating sparse and
group sparse components. To avoid the need for structural
incoherence, we use RSC in the summed space directly
(namely for the summed sparse + group-sparse structure).
However, this brings in a new issue: in this case, the dirty
regularizer for the parameter vector is not decomposable.
To circumvent this issue, our key ingredient is to introduce
“surrogate” sparse and group sparse components depend-
ing on our estimators such that i) their sum equals the sum
of the true parameter components and ii) corresponding er-
ror vectors are decomposable even though the regularizer
itself is not decomposable. Using the decomposability of
error vectors, we are then able to show (cid:96)2 consistency for
general loss functions.

As an additional key contribution of this paper, we con-
sider the extension of sparse+group sparse dirty models to
non-convex regularizers, and show their (cid:96)∞ consistency.
Interestingly, these models are guaranteed to recover the
true support set under much milder conditions and with
smaller sample size than convex models.
In particular,
our (cid:96)∞ consistency results require neither incoherence in
the loss function nor structural incoherence between sparse
and group sparse parameters. We illustrate the practical
impact of this superior theoretical results with simulation
experiments.

The remainder of this paper is organized as follows. In Sec-
tion 2 we review sparse+group-sparse dirty models with
convex penalties and introduce their non-convex counter-
parts.
In Section 3 we discuss the incoherence assump-
tion required by prior analyses and explain why such an
assumption is unreasonable. Section 4 introduces the key
ingredient of our novel proof technique. Section 5 presents
the convergence bounds for models with convex penalties.
Those for non-convex penalties are stated in Section 6. Fi-
nally, simulation experiments are provided in Section 7 to
illustrate the remarkable practical advantage of non-convex
penalties, agreeing with their superior convergence rates.

2. Sparse + Group-Sparse Dirty Models:

Setup and Formulations

Consider a data collection Z = {Z1, . . . , Zn}, where each
element is drawn independently from distribution P, and a
loss function L(· ; Z) : Ω → R where L(θ ; Z) measures
the goodness of ﬁt of parameter θ ∈ Ω to the given data
collection Z. Typically Ω = Rp (parameters are vectors)
or Rp×r (parameters are matrices). Assume there are some
known groups G = {G1, . . . , Gq} that partition the param-
eter index set: Gi ∩ Gj = φ and ∪q

g=1Gg = {1, . . . , p}.

We aim at
unique minimizer of
argminθ∈Ω

EZ[L(θ; Z)] in cases where

recovering parameter θ∗ which is
θ∗

the population risk:

the
:=

θ∗ = α∗ + β∗,

(1)

where α∗ is a sparse component and β∗ is a group-sparse
component obeying the group structure G. For that purpose,
we focus on regularized M -estimators under a dirty learn-
ing setting that combines sparsity and group-sparsity. We
consider both convex and non-convex regularizers as fol-
lows.

2.1. Dirty models with convex regularizers

We focus on regularized M -estimators of the form

minimize
α,β

L(α + β; Z) + λ1(cid:107)α(cid:107)1 + λ2(cid:107)β(cid:107)1,a,

(2)

function L(· ; Z)
is possibly non-
where the loss
convex. Here, given known parameter groups G =
{G1, G2, . . . , Gq},
the group regularizer is deﬁned as
(cid:107)β(cid:107)1,a := (cid:80)q
t=1 (cid:107)βGt(cid:107)a for a ≥ 2, where βGt denotes
the parameter subset in group Gt. The constant a deter-
mines how the elements within each group are combined.

We provide examples for the popular settings of linear re-
gression and inverse covariance estimation.

Linear regression. Consider the standard linear model
y = Xθ∗ + w where y ∈ Rn is the observation vector, θ∗

is the true regression parameter which is the sum of sparse
α∗ and group sparse β∗, X ∈ Rn×p is the design matrix,
and w ∈ Rn is the observation noise. The “dirty” regular-
ized least squares solves

minimize
α,β∈Rp

1
2n

(cid:107)y − X(cid:0)α + β)(cid:107)2

2 + λ1(cid:107)α(cid:107)1 + λ2(cid:107)β(cid:107)1,a

where groups are deﬁned within a (single) parameter vector
space via β. The formulation can be seamlessly extended
to cover the dirty multitask learning setting of Jalali et al.
(2010):

minimize
α,β∈Rp×m

m
(cid:88)

k=1

1
2n

(cid:13)
(cid:13)y(k) − X (k)(cid:0)[α + β](·,k)

(cid:1)(cid:13)
2
(cid:13)
2

(4)

+λ1(cid:107)α(cid:107)1 + λ2(cid:107)β(cid:107)1,∞

where we have m related tasks in columns: α, β ∈ Rp×m,
and the groups can be deﬁned across tasks in rows. E.g. for
predictor j, β(j,1), . . . , β(j,m) belong to the same group.
Here, [α + β](·,k) indicates k-th column of matrix input
α + β.

Graphical Model Estimation. Another key example is
a modiﬁed graphical Lasso where the goal is to estimate
the structure of the underlying graphs representing condi-
tional independences across variables. Assume that there
are some known set of edge groups and that the true pa-
rameter Θ∗ has only a small number of active edge groups
plus some individual edges. To recover Θ∗ we solve
trace(cid:0)(S + B) (cid:98)Σ(cid:1) − log det(S + B)

(5)

minimize
S+B(cid:31)0

+λ1(cid:107)S(cid:107)1 + λ2(cid:107)B(cid:107)1,a

where (cid:98)Σ is the sample covariance matrix and regulariz-
ers are applied to off-diagonal entries of S and B. As
done in (4) for the linear model, the formulation (5) can be
seamlessly extended to the multitask setting where we wish
to estimate multiple precision matrices jointly, encourag-
ing similar structure while allowing for some discrepancy
across them. This estimator is discussed in Hara & Washio
(2013).

Equivalent Program. As shown in Yang & Ravikumar
(2013), the formulation (2) can be rewritten as:

Sparse + Group Sparse Dirty Models

2.2. Dirty models with non-convex regularizers

In this paper, we introduce and study estimators of the form

minimize
α,β

L(α + β) + ρλ1

(cid:0)α(cid:1) + φλ2,a

(cid:0)β).

(8)

(3)

Here ρλ1(·) is any regularizer inducing sparsity beyond the
(cid:96)1-norm (note that the notation encapsulates the regulariza-
tion parameter λ1 itself within the regularizer) satisfying
the following conditions (Loh & Wainwright, 2014):

(C1) ρλ1 (0) = 0 and is symmetric.
For t > 0,
ρλ1(t) is non-decreasing but ρλ1 (t)/t is non-increasing
in t. Besides, ρλ1 (t) is differentiable for t (cid:54)= 0 with
2 t2 is convex for
limt→0+ ρ(cid:48)
λ1
some µ > 0.

(t) = λ1, and is ρλ1 (t) + µ

(C2) There exists some scalar γ ∈ (0, ∞) such that
ρ(cid:48)
λ1

(t) = 0 when t ≥ γλ1.

Following the notation of Loh & Wainwright (2014), we
call ρλ1(·) µ-amenable if it satisﬁes (C1) and (µ, γ)-
amenable if it additionally satisﬁes (C2).
The popu-
lar non-convex regularizers SCAD (Fan & Li, 2001) and
MCP (Zhang, 2010) are both (µ, γ)-amenable (Loh &
Wainwright, 2014).

The regularizer φλ2,a(·) a non-convex counterpart of the
group regularizer λ2(cid:107) · (cid:107)1,a employed in (2) where we use
ρλ2(·) instead of λ2(cid:107) · (cid:107)1, over groups:

φλ2,a(β) := ρλ2 (G(β))

where G(β) := ((cid:107)βG1(cid:107)a, . . . , (cid:107)βGq (cid:107)a)(cid:62). Example of
non-convex regularizers include the Group-SCAD and
Group-MCP penalties where SCAD and MCP penalties are
respectively used on the norm of each group.

Remarkably, the proof techniques developed in this paper
make it possible to provide not only (cid:96)2-error bounds under
milder conditions than prior work on convex problem (2),
but also support set recovery guarantees for non-convex
one (8). In fact we shall see that dirty models with non-
convex regularizers (8) enjoy strictly better statistical guar-
antees than their convex counterpart (2), with practical con-
sequences.

minimize
θ

L(θ; Z) + (cid:107)θ(cid:107)λ

(6)

3. Structural Incoherence: essential in prior
work, yet an unreasonable assumption

where (cid:107)θ(cid:107)λ is the inﬁmal convolution of two regularizers

(cid:110)

(cid:107)θ(cid:107)λ := inf
α,β

λ1(cid:107)α(cid:107)1 + λ2(cid:107)β(cid:107)1,a : α + β = θ

. (7)

(cid:111)

It is known that (cid:107) · (cid:107)λ is a norm and its dual is deﬁned
as (cid:107)θ(cid:107)∗
λ := max{(cid:107)θ(cid:107)∞/λ1, (cid:107)θ(cid:107)∞,a∗ /λ2} where 1/a +
1/a∗ = 1 so that (cid:107) · (cid:107)∞,a∗ is the dual norm of (cid:107) · (cid:107)1,a (see
Yang & Ravikumar (2013) for details).

As our starting point, we focus on the case of convex
dirty models in (2) or equivalently in (6). A key ingre-
dient for showing statistical guarantees of regularized M-
estimators is the decomposability of regularizer (Negahban
et al., 2012). However, considering the form of regularizer
in (6), it is not obvious to ﬁnd the model space and its or-
thogonal complement with which we could directly derive

Sparse + Group Sparse Dirty Models

error bounds with optimal rates. To circumvent this prob-
lem, Yang & Ravikumar (2013) utilize the decomposability
of each component separately, but this requires restricted
strong convexity (RSC) to hold jointly for all component
parameters. In order to have the “joint” RSC property from
“local” RSC with respect to each individual component,
Yang & Ravikumar (2013) assume a structural incoherence
condition. Even if the loss function is strongly convex with
respect to each component, such incoherence across com-
ponents is essential for the joint RSC due to the linearity
across components. To see this more clearly, suppose we
have the function z2 for z ∈ R, which is obviously strongly
convex. If we assume, however, that z is the sum of two
components x, y ∈ R, then one can immediately see that
(x + y)2 is not strongly convex jointly in x and y because
x and y are completely coherent in this one dimensional
example.

The problem is that the structural incoherence condition for
the (cid:96)1 +(cid:96)1,a setting is way too restrictive because the sparse
and the group-sparse structures essentially share the same
model and its orthogonal spaces1. In order to see this, we
consider the popular linear model setting (3) for example.
Let s∗ (and b∗) be the support set of true sparse (group-
sparse) component and sc be its complement. Further-
more, [ 1
n X (cid:62)X](sc∩b∗) denotes the projection of the sam-
ple covariance onto sc ∩ b∗-coordinate space (j-th coordi-
nate becomes zero if j /∈ sc ∩ b∗). Projections on other
spaces are deﬁned similarly. Then, the structural incoher-
ence condition for joint RSC can be reduced as: for all
(s, b) ∈ {(sc, b∗), (s∗, bc), (sc, bc)},

σmax

(cid:0)[ 1

n X (cid:62)X](s∩b)

(cid:1) ≤ Cκ1

(9)

where σmax(·) is the maximum singular value of a ma-
trix, κ1 is the curvature of (restricted) eigenvalue condi-
tion, and C is some ﬁxed constant. Informally, this con-
dition requires the maximum singular value of sample co-
variance (modulo the projection onto the true model and
its orthogonal space) to be smaller than its minimum sin-
gular value (Note that for linear models, the curvature pa-
rameter of the eigenvalue condition is related to the mini-
mum singular value of the sample covariance). This con-
dition can be easily shown to fail in many cases. For in-
stance consider the popular setting where the design ma-
trix X is a set of samples from Gaussian ensemble with
covariance Σ, and the true parameter is the sum of group
sparse + a single nonzero component as depicted in Fig-
ure 1. Then, the incoherence condition in (9) implies
maxi,j |[ 1
n X (cid:62)X]ij| ≤ 1/128σmin(Σ), which can be easily
violated in many natural setting of Σ because the minimum
eigenvalue of Σ is smaller than the maximum element of Σ

1Note that the sparse + group sparse setting is outstanding.
The structural incohence assumption makes sense in other dirty
models settings, e.g. sparse + low rank dirty models.

Figure 1. Example illustrating why the incoherence condition re-
quired by previous work fails to hold.

by the Rayleigh quotient.

This naturally leads to the following question:

Can we provide tight error bounds for the problem (2) not
requiring the joint RSC across individual structures and
hence bypassing the incoherence condition?

4. Our key strategy: Constructing surrogate

components that are always decomposable.

In order to address the above question, our key proof tech-
nique is to establish the decomposability between two com-
ponents of error vectors, by making the target components
dependent of our estimation. Consider arbitrary target pa-
rameter θ∗ such that θ∗ = α∗ + β∗. Note that we do not
impose additional constraints on deﬁning the sparse com-
ponent α∗ and the group sparse component β∗, hence the
possible combination of (α∗, β∗) is not unique. As we
will see later, we provide estimation error bounds that de-
pend on the selection of (α∗, β∗)–more precisely on the
sparsity level of α∗ and the group sparsity level of β∗. In
that sense our theorems provide sets of estimation bounds.
However, it is important to note that we still do not need to
worry about the identiﬁability between structures, because
we only care about the (cid:96)2 and (cid:96)∞ error rates of the ﬁnal
or “summed” estimator (we do not recover (nor care about)
the individual components).

Suppose we compute (cid:101)θ from the program (6) where (cid:101)α and
(cid:101)β are minimizing its dirty regularizer (7). Then, rather than
directly deriving error bounds of (cid:101)θ − θ∗ from (cid:101)α − α∗ and
(cid:101)β − β∗, which are not decomposable, we introduce an ad-
ditional set of vectors, ¯α, ¯β and ¯θ from the following rules:

1. If α∗

j = β∗

j = 0, then ¯αj = ¯βj := 0.

j (cid:54)= 0 and β∗

j = 0, then ¯βj := (cid:101)βj, and ¯αj :=

2. If α∗
θ∗
j − (cid:101)βj.

3. If β∗

j (cid:54)= 0, then ¯αj := (cid:101)αj and ¯βj := θ∗

j − (cid:101)αj.

4. ¯θ is deﬁned as the sum of ¯α and ¯β.

Sparse + Group Sparse Dirty Models

(a) θ∗

(b) α∗

(c) β∗

(d) (cid:101)α

(e) (cid:101)β

(f) (cid:101)α − α∗

(g) (cid:101)β − β∗

(h) ¯α

(i) ¯β

(j) (cid:101)α − ¯α

(k) (cid:101)β − ¯β

Figure 2. Example of constructing surrogate target parameters given (b) α∗, (c) β∗, (d) (cid:101)α and (e) (cid:101)β via transformation T (·). Error
vectors based on surrogates are decomposable (see text for details).

By construction, ¯θ is same as θ∗, but ¯α has different spar-
sity pattern and values from α∗, depending on (cid:101)α. The same
holds for ¯β as well. We denote the above transformation as
( ¯α, ¯β) := T (α∗, β∗; (cid:101)α, (cid:101)β).
It turns out that the error vectors computed based on the
surrogate ¯α and ¯β are always decomposable as described
in the following proposition, and the consequence of this
decomposability plays a key role for showing i) (cid:96)2-error
bounds without incoherence condition and ii) support set
recovery guarantee for non-convex (cid:96)1 + (cid:96)1,a dirty regular-
izers (with faster estimation rates than for convex dirty reg-
ularizers).

Proposition 1. Consider any local optimum (cid:101)θ of con-
vex or non-convex dirty models, and corresponding ¯θ :=
T (α∗, β∗; (cid:101)α, (cid:101)β). Then, the error vectors for individual
components, ∆ := (cid:101)α − ¯α and Γ := (cid:101)β − ¯β, are decom-
posable in the sense that (cid:12)
(cid:12)
(cid:12) = |∆j| + |Γj| for all
(cid:12)[∆ + Γ]j
j, and the overall (cid:96)2 error (cid:107) (cid:101)θ − θ∗(cid:107)2 is lower bounded as
follows:

(cid:107) (cid:101)θ − θ∗(cid:107)2

2 ≥ (cid:107)∆(cid:107)2

2 + (cid:107)Γ(cid:107)2
2 .

(10)

Moreover, let s∗ := supp(α∗) (the support set of α∗), ¯s :=
supp(α∗) ∪ supp( ¯α) and U := supp(α∗) ∪ supp(β∗).
Similarly, we also deﬁne b∗ := supp(β∗) and ¯b :=
supp( ¯β). Then, by construction of T , s∗ ⊆ ¯s ⊆ U and
b∗ ⊆ ¯b ⊆ U . However, it is always guaranteed that

∆s∗ = ∆¯s = ∆U

and Γb∗ = Γ¯b = ΓU

(11)

where ∆s∗ represents the projection of ∆ onto the s∗-
coordinate space; that is, [∆s∗ ]j is ∆j if j ∈ s∗, and 0
otherwise.

Illustrative example. Figure 2 describes an example:
consider a 5 × 3 matrix parameter with 5 known groups in
rows. Suppose (i) the target parameter is given by (a), (ii)
we deﬁne (b) and (c) as the sparse and group sparse compo-
nents of θ∗, and (iii) the minimizer of program (6) are com-
puted as in (d) and (e), respectively for (cid:101)α and (cid:101)β. Then, (f)
and (g) show the error vectors for sparse and group-sparse
components, which are not decomposable ((10) does not
hold for (f) and (g)). On the other hand, for ¯α in (h) and ¯β
in (i) derived from T (·), we can verify that surrogate error
vectors (shown in (j) and (k)) are decomposable; at every
position, (cid:101)α − ¯α and (cid:101)β − ¯β are sign consistent (or at least
one of them is zero).

5. Statistical Guarantees of Models with

Convex Regularizers

Throughout our analysis, we assume that the loss function
L(·) is twice differentiable and and satisﬁes the restricted
strong convexity condition

(RSC) For any vector θ1, θ2 ∈ Rp, the loss function L(·)
satisﬁes

(cid:10)∇L(θ1 + θ2) − ∇L(θ1), θ2

(cid:11)

≥

(cid:26) κ1(cid:107)θ2(cid:107)2

2 − τ1(cid:107)θ2(cid:107)2
η ,
κ2(cid:107)θ2(cid:107)2 − τ2(cid:107)θ2(cid:107)η ,

for all (cid:107)θ2(cid:107)2 ≤ 1 , (12)
for all (cid:107)θ2(cid:107)2 ≥ 1 . (13)

RSC of the loss is also used to guarantee (cid:96)2-consistency
(Negahban et al., 2012; Loh & Wainwright, 2015) or (cid:96)∞-
consistency (Loh & Wainwright, 2014) of “clean” struc-
turally constrained problems (i.e. problems with a single

311121001010000Theta^*000000001010000a^*311121000000000b^*1.900.201.100.10100.90.20.100a_hat0.80.80.81.11.11.10.2-0.20.2000000b_hat1.900.201.100.1000-0.10.20.100Del^*-2.2-0.2-0.20.1-0.90.10.2-0.20.2000000Gam^*1.900.201.10000.8010000a_bar1.110.810.91000.2000000b_bar0000000.100.20-0.10.20.100Del^bar-0.3-0.200.10.20.10.2-0.20000000Gam^barSparse + Group Sparse Dirty Models

structure). Note that there are slight variations in the def-
inition of RSC conditions in the literature. Here we adopt
the form with tolerance terms in Loh & Wainwright (2014;
2015), to allow for a wide class of non quadratic and/or
non-convex loss functions. We will show that RSC with
tolerance in dirty norm holds with high probability under
the popular setting of Gaussian ensembles, as an example.

For the analysis, we consider a slight modiﬁcation of the
program (6):

w is independent sub-Gaussian with parameter σ. Now
suppose that in (14) we set a := 2 (where a is the parame-
ter for the group norm both for (cid:107)θ(cid:107)η and (cid:107)θ(cid:107)λ), r constant
(r only depends on Σ and σ), λ1 = η1 := 8σ(cid:112)log p/n
and λ2 = η2 := 8σ((cid:112)m/n + (cid:112)log q/n) for q groups and
maximum group size m (maxg=1,...,q |Gg|). Suppose that
θ∗ is feasible to program (14) with these settings. Then
with probability at least 1 − c1 exp(−c2nλ2
s) − c3/q2, any
local optimum (cid:101)θ satisﬁes

minimize
(cid:107)θ(cid:107)η≤r

L(θ) + (cid:107)θ(cid:107)λ

(14)

(cid:107) (cid:101)θ − θ∗(cid:107)2 ≤

24σ
κ1

max

(cid:26)(cid:114)

s log p
n

,

(cid:114) sGm
n

+

(cid:114)

sG log q
n

(cid:27)

.

where L is possibly non-convex, but satisﬁes (RSC). The
additional constraint (cid:107)θ(cid:107)η ≤ r also involves the dirty norm
(7) but with a different parameter vector η. This constraint
is a safety radius commonly used for analyzing non-convex
problems to ensure that the global minimum exists (see e.g.
Loh & Wainwright (2014; 2015)). In practice, we can dis-
regard this additional constraint.
Theorem 1. Consider the dirty model
for problem
(14) where L(·)
satis-
the restricted strong convexity (RSC). Suppose
ﬁes
that θ∗ is feasible and the regularization parameters
are set so that λ1 ≥ 4(cid:107)∇L(θ∗)(cid:107)∞ and λ2 ≥
4(cid:107)∇L(θ∗)(cid:107)∞,a∗ .
Suppose furthermore that r ≤
(cid:9). Then, any lo-
min (cid:8) κ2
,
4τ2
cal optimum (cid:101)θ of (14) is guaranteed to be (cid:96)2 consistent:

5 max{λ1/η1,λ2/η2} , λ1
8τ1η1

is possibly non-convex but

, λ2
8τ1η2

κ2

(cid:13)
(cid:13) (cid:101)θ − θ∗(cid:13)

(cid:13)2 ≤

3
κ1

max (cid:8)λ1

√

s , λ2

√

(cid:9)

sG

(15)

where s is the number of nonzero elements in α∗ and sG is
the number of nonzero groups in β∗.

error bound in (15)

Remarks. The
scales with
(n, p, s, sG) at the same rate as previous analysis (Yang &
Ravikumar, 2013) for the sparse plus group sparse setting,
which required a much stringent incoherence condition,
It is also instructive to note
as we already discussed.
that Theorem 1 holds for any combination of (α∗, β∗)
such that α∗ + β∗ = θ∗, but different views of (α∗, β∗)
constructing θ∗ give different bounds depending on
sparsity/group sparsity levels of (α∗, β∗) (i.e. s and sG).
In this sense, Theorem 1 provides a set of (cid:96)2 estimation
upper bounds.

Linear model and modiﬁed graphical Lasso.
In the fol-
lowing corollaries, we apply Theorem 1 to the linear model
(3) and the modiﬁed graphical Lasso problem (5) and de-
rive their corresponding (cid:96)2 estimation bounds.
Corollary 1. Consider the linear model (3). Assume that
(i) each row Xi of the observation matrix X is indepen-
dently sampled from N (0, Σ), (ii) X is (group) column nor-
malized by scaling as in (Negahban et al., 2012), and (iii)

where κ1 is some constant depending on Σ.
Corollary 2. Consider the modiﬁed graphical Lasso (5) to
estimate inverse covariance Θ∗. Suppose we set the pa-
(cid:12)
(cid:12)
(cid:12)(cid:98)Σij − (Θ∗)−1
rameters of (14) as λ1 = η1 := 4 maxi(cid:54)=j
(cid:12),
ij
(cid:13)
(cid:13)a∗ and r ≤
:= 4 maxg∈G
λ2 = η2
1
5(|||Θ∗|||2+1)2 where ||| · |||2 is the spectral norm of the ma-
trix and a ≥ 2. In addition, assume that Θ∗ is feasible for
this problem. Then, any local optimum (cid:101)Θ satisﬁes
(cid:107) (cid:101)Θ − Θ∗(cid:107)F ≤ 3(|||Θ∗|||2 + 1)2 max (cid:8)λ1

(cid:13)
(cid:13)(cid:98)Σg − (Θ∗)−1
g

s , λ2

sG

√

√

(cid:9) .
(16)

Remark. Since (cid:107)θ(cid:107)η scales with 1√
n for the speciﬁed
values of η, the constraint (cid:107)θ(cid:107)η ≤ r gets milder as n in-
creases. It is also important to note that this constraint is
no more stringent than those of non-convex analyses with
a single regularizer (Loh & Wainwright, 2015; 2014): their
constraints can be written as η1(cid:107)θ(cid:107)1 ≤ r (since η1 (cid:16)
(cid:112)log p/n for linear models for example.) in our notation,
which directly implies (cid:107)θ(cid:107)η ≤ r since (cid:107)θ(cid:107)η ≤ η1(cid:107)θ(cid:107)1 by
the deﬁnition of (cid:107) · (cid:107)η.

6. Statistical Guarantees of Models with

Non-convex Penalties

A natural extension of (14) is to incorporate non-convex
regularizers that have some advantages such as unbiased-
ness. For that purpose, in this section we consider the fol-
lowing formulation

minimize
(cid:107)θ(cid:107)η≤r

L(θ) + R(θ; λ)

(17)

(cid:0)α(cid:1) + φλ2,a

(cid:0)β(cid:1) : α + β =
where R(θ; λ) = inf α,β{ρλ1
θ}. While the (cid:96)2 analysis in Theorem 1 can be extended
to non-convex regularizers following proof techniques re-
cently developed in Loh & Wainwright (2015), using non-
convex unbiased regularizers has no beneﬁt in terms of
asymptotic convergence rates of (cid:96)2 estimation errors. In-
stead, we here investigate the (cid:96)∞-norm bound and related
support set recovery guarantees where non-convex unbi-
ased regularizers help. To derive (cid:96)∞ bounds, we use the

Sparse + Group Sparse Dirty Models

, λ2
η2

}) ≤ 1.

primal-dual witness method described in the supplemen-
tary materials.
Theorem 2. Consider the dirty program with non-
convex penalties in (17), under (RSC). Suppose 2r(τ2 +
2 max{ λ1
for some
Also suppose that
η1
δ ∈ (cid:2) max{ 4rτ1η1
}, 1(cid:3), the strict dual feasibility
of primal-dual witness holds. Then, any stationary point
(cid:101)θ of (17) is supported by U (recall U := supp(α∗) ∪
supp(β∗)) if the number of samples is large enough to
sG}2 < c for some constant c de-
satisfy max{λ1
pending only on κ1, τ1 and δ.

, 4rτ1η2
λ2

s, λ2

√

√

λ1

As in Theorem 1, the decomposability in (10) and (11) with
respect to the surrogates ¯α and ¯β, plays a crucial role in
establishing the support set recovery guarantee of any local
optimum in Theorem 2.

Based on Theorem 2, we can derive the (cid:96)∞ bounds follow-
ing the standard steps in (Loh & Wainwright, 2014):
Corollary 3. Suppose the assumptions in Theorem 2 hold.
Then,

1. If κ1−µ

(cid:0) max{η1

√

√

sG}(cid:1)2

2 ≥ τ1

holds for large
s, η2
enough sample size n, the program (17) has a unique
stationary point (cid:98)θ, speciﬁed by the construction of the
primal dual witness.

2. Letting (cid:98)Q := (cid:82) 1

0 ∇2L(cid:0)θ∗ + t( (cid:98)θ − θ∗)(cid:1)dt, it holds
that (cid:107) (cid:98)θ − θ∗(cid:107)∞ ≤ (cid:13)
(cid:13)
(cid:1)−1
(cid:0)
(cid:13)∞ +
(cid:13)
(cid:12)
(cid:1)−1(cid:12)
(cid:12)
(cid:12)
(cid:12)
min{λ1, λ2}(cid:12)
(cid:0)
(cid:12)∞ where ||| · |||∞ denotes a
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
matrix induced norm (maximum absolute row sum).

∇L(θ∗)U

(cid:98)QU U

(cid:98)QU U

3. Moreover,

if ρ is (µ, γ)-amenable, and the mini-
mum absolute value θ∗
min := minj |θ∗
j | is lower-
(cid:13)
min ≥ (cid:13)
(cid:0)
bounded by θ∗
(cid:13)∞ +
(cid:13)
(cid:12)
(cid:12)
(cid:1)−1(cid:12)
(cid:12)
(cid:12)
min{λ1, λ2}(cid:12)
(cid:0)
(cid:12)∞ + 2 max{λ1, λ2}γ.
(cid:98)QU U
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Then,
the error bound in the statement 2 is
reduced to tighter bound as (cid:107) (cid:98)θ − θ∗(cid:107)∞ ≤
(cid:13)
(cid:0)
(cid:13)

∇L(θ∗)U

∇L(θ∗)U

(cid:98)QU U

(cid:98)QU U

(cid:1)−1

(cid:1)−1

(cid:13)
(cid:13)∞.

Multi-task high-dimensional
linear regression. We
consider the multi-task high-dimensional linear regression,
as a concrete example of using non-convex dirty regulariz-
ers. This is the counterpart of model (4) which uses convex
dirty regularizer. In the following corollary, we analyze the
sparsistency of dirty multi-task linear regression with non-
convex regularizers:

minimize
Θ∈Rp×ms.t.(cid:107)Θ(cid:107)η≤r

m
(cid:88)

k=1

1
2n

(cid:13)
(cid:13)y(k) − X (k)Θ(·,k)

(cid:13)
2
2 + R(Θ; λ)
(cid:13)

(18)
(cid:0)β(cid:1) : α + β =
where R(Θ; λ) = inf α,β{ρλ1
Θ}. Now, we derive a corollary for this particular non-
convex dirty model.

(cid:0)α(cid:1) + φλ2,a

Corollary 4. Consider the multitask regression model.
Suppose that for each task, design matrix X (k) is a zero-
mean Gaussian ensemble and is column normalized, w(k)
is independent sub-Gaussian with parameter σ. Now sup-
pose we set parameters of (18) as a := ∞, r constant (only
depends on Σ and σ.), λ1 := c1σ(cid:112)log(pm)/n, λ2 :=
c2σ(cid:112)(log p + m log 2)/n, and Θ∗ is feasible to program
(14) with these settings. Then, for any local optimum
(cid:101)Θ, with probability at least 1 − c1 exp(−c2 log(pm)) −
c3 exp (cid:0) − c4(log p + m log 2)(cid:1) (which is approaching to 1)
for some positive constants c1 − c4,

1. supp( (cid:101)Θ) ⊆ supp(Θ∗),

2. if additionally the regularizer ρλ is (µ, γ)-amenable
with µ < λmin(Σ) (the minimum eigenvalue of Σ)
(cid:1) > 0, then
and Cmin := mink=1,...,m λmin
supp( (cid:101)Θ) = supp(Θ∗) and the element-wise difference
is bounded as follows:

(cid:0)Σ(k)

UkUk

||| (cid:101)Θ − Θ∗|||max := max
i,j

|[ (cid:101)Θ − Θ∗]i,j| ≤ σ

(cid:115)

100 log(pm)
nCmin

provided that θ∗
maxk=1,...,m |||(Σ(k)

min ≥ σ

UkUk

(cid:113) 100 log(pm)
nCmin

+ min{λ1, λ2}

)−1|||∞ + 2 max{λ1, λ2}γ.

In order to highlight

Remark.
the beneﬁt of using
(µ, γ)-amenable regularizers, we brieﬂy compare the re-
sult of Corollary 4 with that of (cid:96)1 + (cid:96)1,a case in
in (Jalali
(Jalali et al., 2010).
et al., 2010) requires the incoherence on X (speciﬁcally,
(cid:0)Σ(k)
maxj∈U c (cid:80)m
(cid:13)1 < 1), but it also
j,Uk
n term in ||| (cid:101)Θ − Θ∗|||max bound.
has an additional
the required λ1 and λ2 there can converge
Moreover,
√

(cid:13)
(cid:13)Σ(k)
j,Uk
sλ1
Cmin

Not only the result

(cid:1)−1(cid:13)

k=1

√

to zero more slowly: λ1 (cid:16)

√

log(pm)
√

n−

s log(pm)

and λ2 (cid:16)

√

√

m(m+log p)
√

n−

sm(m+log p)

.

7. Experiments

To illustrate the practical consequences of the superior sta-
tistical guarantees of models with non-convex penalties,
we perform experiments on both simulated and real-world
data and compare convex and non-convex dirty models for
sparse + group-sparse structures.

Simulated data. We consider multitask regression prob-
lems with m = 10 tasks and p = 260 variables for settings
of parameters (s, sG) ∈ {(2p/10, p/20), (p/10, p/10)} with
respectively less / more support overlap across tasks (recall
s and sG are the number of nonzero elements in α∗ and
the number of nonzero groups in β∗, respectively). The

Sparse + Group Sparse Dirty Models

Figure 3. (cid:96)∞-error for comparison methods for varying sample size n. Left: less sharing across tasks. Right: more sharing across tasks

rows of the design matrices X are sampled i.i.d. from a
zero-mean Gaussian distribution with correlation of 0.2 be-
tween feature pairs. For each set of parameters (s, sG),
we generate 100 instances of the problem where for each
instance the non-zero entries of the true model parame-
ter matrix are i.i.d. zero-mean Gaussian to agree with s
and sG. Gaussian error with standard deviation of 4 is
added to each observation. For varying sample size n we
measure the (cid:96)∞ error of parameters estimated by (i) con-
vex dirty model (Jalali et al., 2010), (ii) non-convex dirty
model with SCAD + Group-SCAD penalty, and (iii) non-
convex dirty model with MCP + Group-MCP penalty. We
also evaluate the following baselines: Lasso, MCP, SCAD,
Group-Lasso, Group-MCP and Group-SCAD 2. The regu-
larization parameters of each method are tuned via 5-folds
cross-validation. The results are presented in Figure 3 (To
avoid cluttering the graphs, we do not display standard er-
rors as these are much lower than the gaps between the
pertinent groups of methods, and we only display the best
group of baselines for each setting). As can be seen from
the ﬁgure, dirty models with non-convex penalties enjoy
superior performance over their counterparts with convex
penalties as a function of the sample size.
In terms of
computational cost, (Group) coordinate descent steps for
(group) lasso, (group) MCP and (group) SCAD all have
simple closed-form expressions (Huang et al., 2012), sim-
ilarly for proximal-based approaches. We noticed that for
a wide range of (λ1, λ2), non-convex procedures took less
time and converged faster (See supplements). As future
work it would be interesting to study their theoretical nu-
merical convergence rates.

Real data analysis. We consider the problem of pre-
dicting biological activities of molecules given features
extracted from their chemical structures. We ana-
lyze three biological activity datasets from the “molec-

2Our theorem on (cid:96)∞ consistency requires the sample size to
be larger than the maximum of two terms, which precludes from
presenting graphs with curve alignment across p (by rescaling the
x-axis with a control parameter as in Jalali et al. (2010)).

ular activity challenge” (http://www.kaggle.com/
c/MerckActivity). Speciﬁcally we consider multitask
regression with three tasks corresponding to predicting the
raw value (− log(IC50)) of three different types of biolog-
ical activities : ‘binding to cannabinoid receptor 1’, ‘inhi-
bition of dipeptidyl peptidase 4’ and ‘time dependent 3A4
inhibitions’. For each task we used n = 200 observations
with p = 3000 molecular features. We consider 20 random
data splits into training and validation sets, using 2/3 of the
data for tranining and 1/3 for validation, and report the av-
erage R2 over these random splits. As shown in table1,
dirty models outperformed “clean” models suggesting the
importance to strike a balanc e between task speciﬁcity and
sharing for this data. Non-convex dirty models achieved
the best R2, which illustrate their capability as a valuable
tool for high-dimensional data analysis.

Table 1. Average R2 for comparison methods on molecular activ-
ity data

Method
Lasso
SCAD
MCP
GLasso
GSCAD
GMCP
Convex DM (Lasso/GLasso)
Nonconvex DM (SCAD/GSCAD)
Nonconvex DM (MCP/GMCP)

R2
0.36 ± 0.03
0.37 ± 0.03
0.36 ± 0.04
0.35 ± 0.03
0.37 ± 0.03
0.38 ± 0.03
0.43 ± 0.04
0.49 ± 0.04
0.49 ± 0.03

8. Concluding Remarks

This paper ﬁnally resolved the outstanding case of sparse +
group-sparse dirty models with convex penalties: we pro-
vided the ﬁrst satisfactory consistency results that do not
require implausible assumptions, thereby fully justifying
their practical success. In addition we proposed and stud-
ied dirty models with non-convex penalties and showed that
they enjoy superior theoretical guarantees that translate into
signiﬁcant practical impact. An interesting direction for
future work is to investigate whether our proof technique
might be applicable to other dirty models and beyond.

0.20.40.60.81.00.20.30.40.50.60.7n/perrorLassoSCADMCPConvex DM (Lasso/GLasso)Non−convex DM (SCAD/G−SCAD)Non−convex DM (MCP/G−MCP)0.20.40.60.81.00.20.40.60.8n/perrorGLassoGSCADGMCPConvex DM (Lasso/GLasso)Non−convex DM (SCAD/G−SCAD)Non−convex DM (MCP/G−MCP)Sparse + Group Sparse Dirty Models

Acknowledgments

E.Y. acknowledges
the support of MSIP/NRF (Na-
tional Research Foundation of Korea) via NRF-
2016R1A5A1012966
for
Information & Communications Technology Promo-
tion of Korea) via ICT R&D program 2016-0-00563,
2017-0-00537.

and MSIP/IITP (Institute

References

Agarwal, A., Negahban, S., and Wainwright, M. J. Noisy
matrix decomposition via convex relaxation: Optimal
rates in high dimensions. The Annals of Statistics, 40
(2):1171–1197, 2012.

Baraniuk, R. Compressive sensing. IEEE Signal Process-

ing Magazine, 24(4):118–121, 2007.

Cand`es, E. J. and Tao, T. The power of convex relaxation:
Information Theory,

Near-optimal matrix completion.
IEEE Transactions on, 56(5):2053–2080, 2010.

Cand`es, E. J., Li, X., Ma, Y., and Wright, J. Robust prin-
cipal component analysis? Journal of the ACM, 58(3),
May 2011.

Jalali, A., Ravikumar, P., Sanghavi, S., and Ruan, C. A
dirty model for multi-task learning. In Neur. Info. Proc.
Sys. (NIPS), 23, 2010.

Loh, P. and Wainwright, M. J. Support recovery without
incoherence: A case for nonconvex regularization. Arxiv
preprint arXiv:1412.5632, 2014.

Loh, P. and Wainwright, M. J. Regularized m-estimators
with nonconvexity: Statistical and algorithmic theory for
local optima. Journal of Machine Learning Research
(JMLR), 16:559–616, 2015.

Negahban, S. and Wainwright, M. J. Estimation of (near)
low-rank matrices with noise and high-dimensional scal-
ing. In Inter. Conf. on Machine learning (ICML), 2010.

Negahban, S., Ravikumar, P., Wainwright, M. J., and Yu,
B. A uniﬁed framework for high-dimensional analysis
of M-estimators with decomposable regularizers. Statis-
tical Science, 27(4):538–557, 2012.

Raskutti, G., Wainwright, M. J., and Yu, B. Restricted
eigenvalue properties for correlated gaussian designs.
Journal of Machine Learning Research (JMLR), 99:
2241–2259, 2010.

Chandrasekaran, V., Sanghavi, S., Parrilo, P. A., and Will-
sky, A. S. Rank-sparsity incoherence for matrix decom-
position. SIAM Journal on Optimization, 21(2):572–
596, 2011.

Ravikumar, P., Wainwright, M. J., Raskutti, G., and Yu,
B. Model selection in gaussian graphical models: High-
dimensional consistency of (cid:96)1-regularized mle. In Neur.
Info. Proc. Sys. (NIPS), 21, 2008.

Fan, J. and Li, R. Variable selection via non-concave pe-
nalized likelihood and its oracle properties. Jour. Amer.
Stat. Ass., 96(456):1348–1360, December 2001.

Friedman, J., Hastie, T., and Tibshirani, R. Sparse inverse
covariance estimation with the graphical Lasso. Bio-
statistics, 2007.

Gong, P., Ye, J., and Zhang, C. Multi-stage multi-task fea-
ture learning. In Pereira, F., Burges, C. J. C., Bottou, L.,
and Weinberger, K. Q. (eds.), Advances in Neural Infor-
mation Processing Systems 25, pp. 1988–1996. 2012.

Hara, S. and Washio, T. Learning a common substructure
of multiple graphical gaussian models. Neural Networks,
38:23–38, 2013.

Hsu, D., Kakade, S. M., and Zhang, T. Robust matrix de-
composition with sparse corruptions. Information The-
ory, IEEE Transactions on, 57(11):7221–7234, 2011.

Huang, J., Breheny, P., and Ma, S. A selective re-
view of group selection in high-dimensional mod-
els. Statist. Sci., 27(4):481–499, 11 2012. doi: 10.
1214/12-STS392. URL http://dx.doi.org/10.
1214/12-STS392.

Ravikumar, P., Lafferty, J., Liu, H., and Wasserman, L.
Sparse additive models. Journal of the Royal Statisti-
cal Society: Series B (Statistical Methodology) (JRSSB),
(5):1009–1030, 2009.

Recht, B., Fazel, M., and Parrilo, P. A. Guaranteed
minimum-rank solutions of linear matrix equations via
nuclear norm minimization. Allerton Conference 07,
Allerton House, Illinois, 2007.

Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society, Series B,
58(1):267–288, 1996.

Wainwright, M. J. Sharp thresholds for high-dimensional
and noisy sparsity recovery using (cid:96)1-constrained
quadratic programming (Lasso). IEEE Trans. Informa-
tion Theory, 55:2183–2202, May 2009.

Yang, E. and Ravikumar, P. Dirty statistical models.

In

Neur. Info. Proc. Sys. (NIPS), 26, 2013.

Yang, E., Ravikumar, P., Allen, G. I., and Liu, Z. Graph-
ical models via univariate exponential family distribu-
tions. Journal of Machine Learning Research (JMLR),
16:3813–3847, 2015.

Sparse + Group Sparse Dirty Models

Yuan, M. and Lin, Y. Model selection and estimation in
regression with grouped variables. Journal of the Royal
Statistical Society B, 1(68):49, 2006.

Zhang, C. H. Nearly unbiased variable selection under min-
imax concave penalty. Annals of Statistics, 38(8):894–
942, 2010.

