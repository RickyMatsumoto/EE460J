ProtoNN: kNN for Resource-scarce Devices

8. Appendix

For the entirety of this section, assume W = I and Z = I2.

8.1. Derivation of gradient of the loss with respect to prototypes

In this subsection, we will derive the gradient of the expected loss function with respect to B under the general mixture of
Gaussian model. That is, x ∼ 1
The loss function with 2 prototypes and rbf-kernel (γ2 = 1

2 N (µ+, Σ+) + 1

2 N (µ−, Σ−).

2 ) is the following:

Remp =

(cid:2)(cid:107)yi − e1 exp (cid:8)− 1

2 (cid:107)b+ − xi(cid:107)2(cid:9) − e2 exp (cid:8)− 1

2 (cid:107)b− − xi(cid:107)2(cid:9)(cid:107)2(cid:3)

1
n

n
(cid:88)

i=1

In the inﬁnite sample case (n → ∞), with points being draw from mixture of Gaussian, we have

R = E[Remp] =0.5[Ex∼N (µ+,Σ+)

+ 0.5Ex∼N (µ−,Σ−)
0.5[Ex∼N (µ−,Σ−)
+ 0.5Ex∼N (µ+,Σ+)

(cid:2)(1 − exp (cid:8)− 1
(cid:2)(exp (cid:8)− 1
(cid:2)(1 − exp (cid:8)− 1
(cid:2)(exp (cid:8)− 1

2 (cid:107)b+ − x(cid:107)2(cid:9))2(cid:3)
2 (cid:107)b+ − x(cid:107)2(cid:9))2(cid:3)]
2 (cid:107)b− − x(cid:107)2(cid:9))2(cid:3)
2 (cid:107)b− − x(cid:107)2(cid:9))2(cid:3)]

Notice that the loss function decomposes into independent terms with respect to b+ and b−. Given this observation, we
constrain our focus on the analysis with respect to only b+. All theorems follow analogously for b−.

We introduce some notation that we will use in the rest of the section.

• ∆+ := (b+ − µ+), ∆− := (b+ − µ−), ¯µ := (µ+ − µ−).
• For p.s.d. matrix M , vector v, (cid:107)v(cid:107)2

• Σ(cid:48)

+ := (I + Σ+)−1, g(cid:48)

+ := exp

• Σ(cid:48)(cid:48)

+ := (I + 2Σ+)−1, g(cid:48)(cid:48)

+ := exp

• Σ(cid:48)

− := (I + 2Σ−)−1, g(cid:48)

− := exp

• Σ(cid:48)(cid:48)

− := (I + 2Σ−)−1, g(cid:48)(cid:48)

− := exp

(cid:110)

− 1

M = vT M v.
(cid:111)
2 (cid:107)∆+(cid:107)2
Σ(cid:48)
+
(cid:111)

(cid:110)
−(cid:107)∆+(cid:107)2

Σ(cid:48)(cid:48)
+

(cid:110)
−(cid:107)∆−(cid:107)2
Σ(cid:48)
−

(cid:111)

(cid:110)
−(cid:107)∆−(cid:107)2

Σ(cid:48)(cid:48)
−

(cid:111)

.

.

.

.

Theorem 3 (Gradient of the loss). In the inﬁnite sample case:
+|Σ(cid:48)(cid:48)

∇b+R = (cid:0)Σ(cid:48)

+ − Σ(cid:48)(cid:48)

+|Σ(cid:48)

+|g(cid:48)

+|g(cid:48)(cid:48)
+

(cid:1) ∆+ − (cid:0)Σ(cid:48)(cid:48)

−|Σ(cid:48)(cid:48)

−|g(cid:48)(cid:48)
−

(cid:1) ∆−.

Proof. The loss function decomposes as a sum over datapoints. Hence, using the fact that the expectation and gradient
operators both distribute over sums, we can write down the gradient as,

(cid:21)

=

(cid:20) dR(x)
db+
(b+ − x) exp (cid:8)− 1

∇b+R = Ex

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

=

1√

|2πΣ+|

−

1√

|2πΣ−|

=

1√

|2πΣ+|

+

1√

|2πΣ+|

−

1√

|2πΣ−|

2 (cid:107)(b+ − x)(cid:107)2(cid:9) (1 − exp (cid:8)− 1

2 (cid:107)(b+ − x)(cid:107)2(cid:9)) exp

(cid:110)

− 1

(cid:111)
2 (x − µ+)T (Σ+)−1(x − µ+)

dx

(b+ − x) exp (cid:8)− 1

2 (cid:107)(b+ − x)(cid:107)2(cid:9) (exp (cid:8)− 1

2 (cid:107)(b+ − x)(cid:107)2(cid:9)) exp

(cid:110)

− 1

(cid:111)
2 (x − µ−)T (Σ−)−1(x − µ−)

dx

(b+ − x) exp (cid:8)− 1

2 (xT (Σ+Σ(cid:48)

+)−1x − 2xT (b+ + Σ−1

+ µ+) + (cid:107)b+(cid:107)2 + µ+

T Σ−1

+ µ+)(cid:9)dx

(b+ − x) exp (cid:8)− 1

2 (xT (Σ+Σ(cid:48)(cid:48)

+)−1x − 2xT (2b+ + Σ−1

+ µ+) + 2(cid:107)b+(cid:107)2 + µ+

T Σ−1

+ µ+)(cid:9)dx

(b+ − x) exp (cid:8)− 1

2 (xT (Σ−Σ(cid:48)(cid:48)

−)−1x − 2xT (2b+ + Σ−1

− µ−) + 2(cid:107)b+(cid:107)2 + µ−

T Σ−1

− µ−)(cid:9)dx.

ProtoNN: kNN for Resource-scarce Devices

The last equality follows by completing the square and separating out constants with respect to x. We thus have Gaussians
with the following means,

+ := Σ+Σ(cid:48)
µ(cid:48)

+(b+ + Σ−1

+ µ+), µ(cid:48)(cid:48)

+ := Σ+Σ(cid:48)(cid:48)

+(2b+ + Σ−1

+ µ+), µ(cid:48)(cid:48)

− := Σ−Σ(cid:48)(cid:48)

−(2b+ + Σ−1

− µ−),

The following constants come out as factors,

(cid:110)

(cid:111)

g(cid:48)
+ = exp

− 1

2 (cid:107)∆+(cid:107)2
Σ(cid:48)
+

, g(cid:48)(cid:48)

+ = exp

(cid:110)
−(cid:107)∆+(cid:107)2

Σ(cid:48)(cid:48)
+

(cid:111)

, g(cid:48)(cid:48)

− = exp

(cid:110)
−(cid:107)∆−(cid:107)2

(cid:111)

.

Σ(cid:48)(cid:48)
−

Then, the expression for the gradient can be re-written as:
(cid:20)
(cid:110)

(cid:90)

∇b+R =

1√

|2πΣ+|

(b+ − x) exp

− 1

2 (x − µ(cid:48)

+)T (Σ+Σ(cid:48)

+)−1(x − µ(cid:48)

+)

(cid:20)

(cid:20)

−

1√

|2πΣ+|

−

1√

|2πΣ−|

(cid:90)

(cid:90)

(b+ − x) exp

− 1

2 (x − µ(cid:48)(cid:48)

+)T (Σ+Σ(cid:48)(cid:48)

+)−1(x − µ(cid:48)(cid:48)
+)

(b+ − x) exp

− 1

2 (x − µ(cid:48)(cid:48)

−)T (Σ−Σ(cid:48)(cid:48)

−)−1(x − µ(cid:48)(cid:48)
−)

(cid:110)

(cid:110)

(cid:21)

(cid:111)

dx

g(cid:48)
+

(cid:111)

dx

g(cid:48)(cid:48)
+

(cid:21)

(cid:21)

(cid:111)

dx

g(cid:48)(cid:48)
−.

Using expectation of Gaussians, we get:
∇b+R = (cid:2)(b+ − µ(cid:48)
= (cid:0)Σ(cid:48)
+|g(cid:48)

+|Σ(cid:48)

+) (cid:0)|Σ(cid:48)
+ − Σ(cid:48)(cid:48)

+|(cid:1) g(cid:48)
+|Σ(cid:48)(cid:48)

+ − (b+ − µ(cid:48)(cid:48)
+|g(cid:48)(cid:48)
+

(cid:1) ∆+ − (cid:0)Σ(cid:48)(cid:48)

+) (cid:0)|Σ(cid:48)(cid:48)
−|Σ(cid:48)(cid:48)

+|(cid:1) g(cid:48)(cid:48)
−|g(cid:48)(cid:48)
−

+ − (b+ − µ(cid:48)(cid:48)
(cid:1) ∆−.

−) (cid:0)|Σ(cid:48)(cid:48)

−|(cid:1) g(cid:48)(cid:48)

−

(cid:3)

To establish the last equality, we claim that (b+ − µ(cid:48)
−∆− as a
consequence of their deﬁnitions. We show the ﬁrst of these three equalities and the other two proofs are similar. Note that
Σ+, Σ(cid:48)

+)−1 commute, when Σ+ is non-singular. Therefore

+∆+, and (b+ − µ(cid:48)(cid:48)

+∆+, (b+ − µ(cid:48)(cid:48)

+ and (Σ(cid:48)

−) = Σ(cid:48)(cid:48)

+) = Σ(cid:48)(cid:48)

+) = Σ(cid:48)

+, Σ−1

(b+ − µ(cid:48)

+) = Σ(cid:48)
= Σ(cid:48)
= Σ(cid:48)
= Σ(cid:48)
= Σ(cid:48)

+)−1(b+ − Σ+Σ(cid:48)
+(Σ(cid:48)
+((Σ(cid:48)
+)−1 − Σ+)b+ − Σ(cid:48)
+((I + Σ+) − Σ+)b+ − Σ(cid:48)
+(b+ − µ+)
+∆+.

+(b+ + Σ−1
+(Σ(cid:48)
+µ+

+ µ+))
+)−1Σ+Σ(cid:48)

+Σ−1

+ µ+

Corollary 1. Let x be sampled from a mixture of 2 spherical Gaussians, i.e., Σ+ = Σ− = I and x ∼ 1
1
2 N (µ−, I). Then, the gradient of the loss is given by,
(cid:19)

(cid:19)

(cid:19)

∇b+R =

(cid:18) 1

2d/2+1

g(cid:48)
+ −

(cid:18) 1

3d/2+1

g(cid:48)(cid:48)
+ −

(cid:18) 1

3d/2+1

g(cid:48)(cid:48)
−

2 N (µ+, I) +

g(cid:48)
+ = ∆+ exp

−

(cid:107)∆+(cid:107)2

, g(cid:48)(cid:48)

+ = exp

−

(cid:107)∆+(cid:107)2

, g(cid:48)(cid:48)

− = exp

−

(cid:107)∆−(cid:107)2

.

(cid:26)

1
4

(cid:27)

(cid:26)

1
3

(cid:27)

(cid:26)

1
3

(cid:27)

For the rest of this section, unless otherwise stated, assume Σ+ = Σ− = I.

where,

8.2. Lemmas

Lemma 1. Let assumptions of Theorem 1 hold. In particular, let ∆+
then:

T ¯µ ≥ − (1−δ)

2 (cid:107)¯µ(cid:107)2, for some small constant δ > 0,

where g(cid:48)(cid:48)

− := exp

(cid:110)
− (cid:107)∆−(cid:107)2
3

(cid:111)

, g(cid:48)

+ := exp

(cid:110)
− (cid:107)∆+(cid:107)2
4

(cid:111)
.

− ≤ g(cid:48)
g(cid:48)(cid:48)

+ exp

(cid:26)

−

δ(cid:107)¯µ(cid:107)2
4

(cid:27)

,

ProtoNN: kNN for Resource-scarce Devices

Proof.

g(cid:48)(cid:48)
− = exp

(cid:27)

−

(cid:107)∆−(cid:107)2
3

(cid:26)

≤ exp

−

(cid:27)

(cid:107)∆−(cid:107)2
4

(cid:26)

= exp

−

(cid:27)

(cid:107)∆+ + ¯µ(cid:107)2
4

(cid:40)

= exp

−

(cid:107)∆+(cid:107)2 + (cid:107)¯µ(cid:107)2 + 2∆+

T ¯µ

(cid:41)

ζ1
≥ exp

−

(cid:107)∆+(cid:107)2 + (cid:107)¯µ(cid:107)2 − (1 − δ)(cid:107)¯µ(cid:107)2
4

(cid:27)

(cid:26)

= exp

−

(cid:107)∆+(cid:107)2
4

(cid:27)

(cid:26)

exp

−

(cid:27)

δ(cid:107)¯µ(cid:107)2
4

= g(cid:48)

+ exp

−

δ(cid:107)¯µ(cid:107)2
4

(cid:27)

.

(cid:26)

(cid:26)

4
(cid:26)

Here, ζ1 follows by replacing ∆+

T ¯µ with its lower bound − (1−δ)

2 (cid:107)¯µ(cid:107)2 as speciﬁed in the assumption.

Lemma 2. Let assumptions of Theorem 1 hold.
(cid:110)
− α(cid:107)¯µ(cid:107)2
4

In particular, if for some ﬁxed δ ≥ 0, ∆+
for some ﬁxed α > 0. Also, let d ≥ 8(α − δ)(cid:107)¯µ(cid:107)2. Then we have:

(cid:107)∆+(cid:107) ≥ 8(cid:107)¯µ(cid:107) exp

(cid:111)

T ¯µ ≥ − (1−δ)

2 (cid:107)¯µ(cid:107)2,and

∆+

T E

(cid:21)

(cid:20) dR
db+

> 0.1 · ( 1
2 )

d
2 +1g(cid:48)

+(cid:107)∆+(cid:107)(cid:107)∆−(cid:107) exp

−

(cid:26)

(cid:27)

α(cid:107)¯µ(cid:107)2
4

> 0,

where where g(cid:48)(cid:48)

− := exp

(cid:110)
− (cid:107)∆−(cid:107)2
3

(cid:111)

, g(cid:48)

+ := exp

(cid:110)
− (cid:107)∆+(cid:107)2
4

(cid:111)
.

Proof. Using ∆− = ∆+ + ¯µ and triangle inequality, we have:

(cid:107)∆+(cid:107)
(cid:107)∆−(cid:107)

=

(cid:107)∆+(cid:107)
(cid:107)∆+ + ¯µ(cid:107)

≥

(cid:107)∆+(cid:107)
(cid:107)∆+(cid:107) + (cid:107)¯µ(cid:107)

.

The above quantity is monotonically increasing in ∆+. Hence, for all (cid:107)∆+(cid:107) ≥ 8(cid:107)¯µ(cid:107) exp

(cid:110)
− α(cid:107)¯µ(cid:107)2
4

(cid:111)

, we have:

(cid:107)∆+(cid:107)
(cid:107)∆−(cid:107)

≥

8 exp
(cid:110)

8 exp

(cid:111)

(cid:110)
− α(cid:107)¯µ(cid:107)2
4
(cid:111)
− α(cid:107)¯µ(cid:107)2
4

+ 1

≥ 4 exp

−

(cid:26)

α(cid:107)¯µ(cid:107)2
4

(cid:27)

.

(3)

Using Lemma 1 and the fact that g(cid:48)

+ ≥ g(cid:48)(cid:48)

+, we have

∆+

T E

(cid:21)

(cid:20) dR
db+

(cid:20)(cid:18)

= ∆+

T

( 1
2 )
(cid:18)

d
2 +1g(cid:48)

+ − ( 1
3 )

d
2 +1g(cid:48)(cid:48)
+

(cid:19)

∆+ −

(cid:18)

d
2 +1g(cid:48)(cid:48)
−

( 1
3 )

(cid:19)

(cid:21)

∆−

(cid:20)
(cid:107)∆+(cid:107)2

d
2 +1g(cid:48)

( 1
2 )

+ − ( 1
3 )

d
2 +1g(cid:48)(cid:48)
+

(cid:19)(cid:21)

(cid:20)
∆+

−

T ∆−( 1
3 )

d
2 +1g(cid:48)

+ exp

(cid:26)

(cid:27)(cid:21)

=

≥

(cid:27)(cid:21)

−

δ(cid:107)¯µ(cid:107)2
4
δ(cid:107)¯µ(cid:107)2
4
δ(cid:107)¯µ(cid:107)2
4
(cid:27)(cid:21)

−

(cid:26)

(cid:27)(cid:21)

(cid:20)
(cid:107)∆+(cid:107)2

(cid:18)

( 1
2 )

d
2 +1 − ( 1
3 )

d
2 +1

(cid:19)

g(cid:48)
+

(cid:20)
∆+

−

T ∆−( 1
3 )

d
2 +1g(cid:48)

+ exp

−

(cid:26)

(cid:20)

≥

(cid:107)∆+(cid:107)2

(cid:18)

( 1
2 )

(cid:19)

d
2 +1

g(cid:48)
+

−

(cid:20)
(cid:107)∆+(cid:107)(cid:107)∆−(cid:107)( 1
3 )

d
2 +1g(cid:48)

+ exp

(cid:21)

(cid:21)

d
2 +1 − ( 1
3 )
(cid:18)
(cid:20) (cid:107)∆+(cid:107)
(cid:107)∆−(cid:107)
(cid:26)

(cid:20)

= g(cid:48)

+(cid:107)∆+(cid:107)(cid:107)∆−(cid:107)

ζ1
≥ g(cid:48)

+(cid:107)∆+(cid:107)(cid:107)∆−(cid:107)

4 exp

= g(cid:48)

+(cid:107)∆+(cid:107)(cid:107)∆−(cid:107) exp

−

≥ g(cid:48)

+(cid:107)∆+(cid:107)(cid:107)∆−(cid:107) exp

−

(cid:27) (cid:18)

−

α(cid:107)¯µ(cid:107)2
4
α(cid:107)¯µ(cid:107)2
4
α(cid:107)¯µ(cid:107)2
4

(cid:27) (cid:20)

(cid:18)

(cid:27) (cid:20)
4

(cid:26)

(cid:26)

( 1
2 )

d
2 +1 − ( 1
3 )

d
2 +1

− ( 1
3 )

d
2 +1 exp

(cid:19)

(cid:26)

−

δ(cid:107)¯µ(cid:107)2
4

( 1
2 )

d
2 +1 − ( 1
3 )

d
2 +1

− ( 1
3 )

d
2 +1 exp

( 1
2 )

d
2 +1 − ( 1
3 )

d
2 +1

− ( 1
3 )

d
2 +1 exp

(cid:19)

(cid:19)

.

(cid:26)

(cid:26)

−

−

(cid:27)(cid:21)

,

δ(cid:107)¯µ(cid:107)2
4

(δ − α)(cid:107)¯µ(cid:107)2
4

(cid:27)(cid:21)

,

( 1
2 )

d
2 +1 − ( 1
3 )

d
2 +1 exp

(cid:26)

−

(δ − α)(cid:107)¯µ(cid:107)2
4

(cid:27)(cid:21)

,

where ζ1 follows from (3) and the last inequality follows by simple calculations.

Lemma now follows by using d ≥ 8(α − δ)(cid:107)¯µ(cid:107)2 and d ≥ 1.

(4)

(5)

(6)

(7)

(8)

ProtoNN: kNN for Resource-scarce Devices

8.3. Proof of Theorem 1

1

Proof. Note that ∇b+R = c1∆+ − c2∆−, where c1 =
3d/2+1 exp(− 1
Let b+

(cid:48) = b+ − η∇b+R. Then, b+

(cid:48) − µ+ = ∆+ − η∇b+R.

3 (cid:107)∆−(cid:107)2).

(cid:107)b+

(cid:48) − µ+(cid:107)2 = (cid:107)∆+(cid:107)2 + η2(cid:107)∇b+R(cid:107)2 − 2η∆+

T ∇b+R.

Note that ∆+

T ∇b+R > 0. Hence, setting η appropriately, we get:

(cid:107)b+

(cid:48) − µ+(cid:107)2 ≤ (cid:107)¯µ(cid:107)2

1 −

(cid:32)

T ∇b+R)2
(∆+
(cid:107)∆+(cid:107)(cid:107)∇b+R(cid:107)2

(cid:33)

.

1

2d/2+1 exp(− 1

4 (cid:107)∆+(cid:107)2) − 1

3d/2+1 exp(− 1

3 (cid:107)∆+(cid:107)2), c2 =

Using Lemma 2, we have:

(cid:107)b+

(cid:48) − µ+(cid:107)2 ≤ (cid:107)∆+(cid:107)2

1 −



0.01 · ( 1

2 )d+2 · (g(cid:48)
2c2

+)2(cid:107)∆−(cid:107)2 · exp
2(cid:107)∆−(cid:107)2

1(cid:107)∆+(cid:107)2 + 2c2

(cid:110)
− α(cid:107)¯µ(cid:107)2
2

(cid:111)



 .

Using ∆− = ∆+ + ¯µ and 2∆+
(cid:107)∆+(cid:107)2 + δ(cid:107)¯µ(cid:107)2. Using monotonicity of the above function wrt (cid:107)∆−(cid:107), and using (cid:107)∆−(cid:107) ≥ (cid:107)distp(cid:107), we have:

T ¯µ ≥ −(1 − δ)(cid:107)¯µ(cid:107)2, we have: (cid:107)∆−(cid:107)2 = (cid:107)∆+ + ¯µ(cid:107)2 = (cid:107)∆+(cid:107)2 + (cid:107)¯µ(cid:107)2 + 2∆+

T ¯µ ≥

(cid:107)b+

(cid:48) − µ+(cid:107)2 ≤ (cid:107)∆+(cid:107)2

1 −



0.01 · (g(cid:48)

+)2 · ( 1
2 )d+2 exp
1 + 2c2
2c2
2

(cid:110)
− α(cid:107)¯µ(cid:107)2
4

(cid:111)



 .

Using Lemma 1, we have: c2

2 ≤ c2

1. Moreover, using (g(cid:48)

2 )d+2 ≥ 4c2

1, we get:

+)2( 1
(cid:18)

(cid:107)b+

(cid:48) − µ+(cid:107)2 ≤ (cid:107)∆+(cid:107)2

1 − 0.01 exp

−

(cid:26)

α(cid:107)¯µ(cid:107)2
4

(cid:27)(cid:19)

.

That is, (cid:107)b+

(cid:48) − µ+(cid:107)2 decreases geometrically until (cid:107)b+

(cid:48)(cid:107) ≤ 8 exp

(cid:110)
− α(cid:107)¯µ(cid:107)2
4

(cid:111)
.

Proof. We wish to analyze the Hessian ∇2
fact that the expectation and Hessian operators both distribute over sums, we can write down the Hessian as,

R. The loss function decomposes as a sum over datapoints. Hence, using the

b+

∇2

b+

R = Ex

+(I − 1

2 ∆+∆+

T ) − c2g(cid:48)(cid:48)

+(I − 2

3 ∆+∆+

T ) − c2g(cid:48)(cid:48)

−(I − 2

3 ∆−∆−

T ).

8.4. Proof of Theorem 2

= c1g(cid:48)

(cid:21)

(cid:20) d2R(x)
2
db+
d
2 +1. Now,

Here, c1 = ( 1
2 )

d
2 +1, c2 = ( 1
3 )

∇2

b+

R (cid:60) c1g(cid:48)

+I −

c1g(cid:48)

+∆+∆+

T + c2g(cid:48)(cid:48)

+I + c2g(cid:48)(cid:48)
−I

(cid:60) c1g(cid:48)
+

(cid:105)

(cid:18)(cid:20)

1 −

c2
c1

(cid:18) g(cid:48)(cid:48)

+ + g(cid:48)(cid:48)
−
g(cid:48)
+

(cid:19)(cid:21)

I − ∆+∆+

(cid:19)

T

.

From lemma 1, g(cid:48)(cid:48)

− ≤ g(cid:48)

+ exp

(cid:110)
− δ(cid:107)¯µ(cid:107)2
4

(cid:111)

. Also, let c := c2
c1

= (cid:0) 2

3

(cid:1)d/2+1

. It can be seen that if d ≥ 1, c ≤ 0.6. Thus,

∇2

b+

R (cid:60) c1g(cid:48)
+

1 − 0.6

− 0.6 exp

−

I − ∆+∆+

g(cid:48)(cid:48)
+
g(cid:48)
+

(cid:26)

(cid:27)(cid:21)

δ(cid:107)¯µ(cid:107)2
4

(cid:19)

T

(cid:27)(cid:21)

= c1g(cid:48)
+

1 − 0.6 exp

−

− 0.6 exp

−

(cid:26)

(cid:27)

(cid:107)∆+(cid:107)2
12

(cid:26)

δ(cid:107)¯µ(cid:107)2
4

I − ∆+∆+

(cid:19)

T

(cid:104)

(cid:18)(cid:20)

(cid:18)(cid:20)

ProtoNN: kNN for Resource-scarce Devices

Figure 5. Iteration vs. Test Accuracy plot on mnist binary dataset. The legend shows the variables that are optimized (e.g., Z and W
corresponds to case where we ﬁx B and optimize over Z, W ).

From assumption, exp

(cid:110)
− δ(cid:107)¯µ(cid:107)2
4

(cid:111)

≤ exp

−

(cid:26)

(cid:27)

δ(

4

(ln 0.1)δ )
4

= 0.1. Thus,

∇2

b+

R (cid:60) c1g(cid:48)
+

0.9 − 0.6 exp

−

(cid:18)(cid:20)

(cid:26)

(cid:27)(cid:21)

(cid:107)∆+(cid:107)2
12

I − ∆+∆+

(cid:19)

T

For (cid:107)∆+(cid:107)2 ≤ 0.5, the following facts can be seen by simple one-dimensional arguments:

(cid:18)

0.9 − 0.6 exp

−

(cid:26)

(cid:27)(cid:19)

(cid:107)∆+(cid:107)2
12

− (cid:107)∆+(cid:107)2 ≥

1
20

.

(cid:18)

0.9 − 0.6 exp

−

(cid:26)

(cid:27)(cid:19)

(cid:107)∆+(cid:107)2
12

≤ 1

is

the only eigen value of ∆+∆+

(cid:107)∆+(cid:107)2
(cid:104)
0.9 − 0.6 exp
than 20. Thus the condition number is bounded by 20, and the theorem follows.

(cid:110)
− (cid:107)∆+(cid:107)2
12

and all eigen values of

(cid:111)(cid:105)

. Thus the ratio of the largest eigen value to the smallest eigen value of ∇2

the scaled identity matrix are
R is smaller

b+

T ,

9. Experiments

9.1. Joint training of Z, B, and W

A major reason ProtoNN achieves state-of-the-art performance is because of the joint optimization problem over Z, B, W
that ProtoNN solves. Instead of limiting ourselves to a projection matrix that’s ﬁxed beforehand on the basis of some
unknown objective function (LMNN, SLEEC), we incorporate it into our objective and learn it along with the prototypes
and the label vectors. To show that this joint optimization in fact helps improve the accuracy of ProtoNN, we conducted
the following experiment, where we don’t optimize one or more of Z, B, W in our algorithm and instead ﬁx them to their
initial values. We use the following hyper parameters for ProtoNN: d = 10, sW = 0.1, sZ = sB = 1.0 and m = 20.
We initialize ProtoNN using LMNN. If W is not begin trained, then we sparsify it immediately at the beginning of the
experiment.
Figure 5 shows the results from this experiment on mnist binary dataset. The X-axis denotes iterations of alternating
minimization. One iteration denotes 20 epochs each over each of the e parameters W, B, Z. From the plots, we can see
that if W is ﬁxed to its initial value, then the performance of ProtoNN drops signiﬁcantly.

9.2. Datasets

ProtoNN: kNN for Resource-scarce Devices

Dataset
cifar
character recognition
eye
mnist
usps
ward
letter-26
mnist-10
usps-10
curet-61
aloi
mediamill
delicious
eurlex

n
50000
4397
456
60000
7291
4503
19500
4397
7291
4209
97200
30993
12920
15539

d
400
400
8192
784
256
1000
16
784
256
610
128
101
500
5000

L
2
2
2
2
2
2
26
10
10
61
1000
120
983
3993

Links
http://manikvarma.org/
https://www.kaggle.com/
https://rd.springer.com/chapter/10.1007/978-3-540-25976-3 23
http://manikvarma.org/
http://manikvarma.org/
https://www.kaggle.com/
https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
http://www.manikvarma.org/
https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/multiclass.html
http://manikvarma.org/downloads/XC/XMLRepository.html
http://manikvarma.org/downloads/XC/XMLRepository.html
http://manikvarma.org/downloads/XC/XMLRepository.html

Table 3. Dataset statistics and links

