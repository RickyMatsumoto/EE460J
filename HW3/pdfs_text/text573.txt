meProp: Sparsiﬁed Back Propagation for Accelerated Deep Learning
with Reduced Overﬁtting

Xu Sun 1 2 Xuancheng Ren 1 2 Shuming Ma 1 2 Houfeng Wang 1 2

Abstract
We propose a simple yet effective technique for
neural network learning. The forward propaga-
tion is computed as usual. In back propagation,
only a small subset of the full gradient is com-
puted to update the model parameters. The gra-
dient vectors are sparsiﬁed in such a way that
only the top-k elements (in terms of magnitude)
are kept. As a result, only k rows or columns
(depending on the layout) of the weight matrix
are modiﬁed, leading to a linear reduction (k di-
vided by the vector dimension) in the computati-
onal cost. Surprisingly, experimental results de-
monstrate that we can update only 1–4% of the
weights at each back propagation pass. This does
not result in a larger number of training iterati-
ons. More interestingly, the accuracy of the re-
sulting models is actually improved rather than
degraded, and a detailed analysis is given.

1. Introduction

Neural network learning is typically slow, where back pro-
pagation usually dominates the computational cost during
the learning process. Back propagation entails a high com-
putational cost because it needs to compute full gradients
and update all model parameters in each learning step. It
is not uncommon for a neural network to have a massive
number of model parameters.

In this study, we propose a minimal effort back propaga-
tion method, which we call meProp, for neural network
learning. The idea is that we compute only a very small
but critical portion of the gradient information, and update
only the corresponding minimal portion of the parameters
in each learning step. This leads to sparsiﬁed gradients,

1School of Electronics Engineering and Computer Science,
Peking University, China 2MOE Key Laboratory of Computati-
onal Linguistics, Peking University, China. Correspondence to:
Xu Sun <xusun@pku.edu.cn>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

such that only highly relevant parameters are updated and
other parameters stay untouched. The sparsiﬁed back pro-
pagation leads to a linear reduction in the computational
cost.

To realize our approach, we need to answer two questions.
The ﬁrst question is how to ﬁnd the highly relevant subset
of the parameters from the current sample in stochastic le-
arning. We propose a top-k search method to ﬁnd the most
important parameters.
Interestingly, experimental results
demonstrate that we can update only 1–4% of the weights
at each back propagation pass. This does not result in a
larger number of training iterations. The proposed method
is general-purpose and it is independent of speciﬁc models
and speciﬁc optimizers (e.g., Adam and AdaGrad).

The second question is whether or not this minimal effort
back propagation strategy will hurt the accuracy of the trai-
ned models. We show that our strategy does not degrade
the accuracy of the trained model, even when a very small
portion of the parameters is updated. More interestingly,
our experimental results reveal that our strategy actually
improves the model accuracy in most cases. Based on our
experiments, we ﬁnd that it is probably because the mini-
mal effort update does not modify weakly relevant para-
meters in each update, which makes overﬁtting less likely,
similar to the dropout effect.

The contributions of this work are as follows:

• We propose a sparsiﬁed back propagation technique
for neural network learning, in which only a small
subset of the full gradient is computed to update the
model parameters. Experimental results demonstrate
that we can update only 1–4% of the weights at each
back propagation pass. This does not result in a larger
number of training iterations.

• Surprisingly, our experimental results reveal that the
accuracy of the resulting models is actually impro-
ved, rather than degraded. We demonstrate this effect
by conducting experiments on different deep learning
models (LSTM and MLP), various optimization met-
hods (Adam and AdaGrad), and diverse tasks (natural
language processing and image recognition).

Sparsiﬁed Back Propagation for Accelerated Deep Learning with Reduced Overﬁtting

Figure 1. An illustration of meProp.

2. Proposed Method

We propose a simple yet effective technique for neural net-
work learning. The forward propagation is computed as
usual. During back propagation, only a small subset of the
full gradient is computed to update the model parameters.
The gradient vectors are “quantized” so that only the top-k
components in terms of magnitude are kept. We ﬁrst pre-
sent the proposed method and then describe the implemen-
tation details.

2.1. meProp

Forward propagation of neural network models, including
feedforward neural networks, RNN, LSTM, consists of li-
near transformations and non-linear transformations. For
simplicity, we take a computation unit with one linear
transformation and one non-linear transformation as an ex-
ample:

y = W x

z = σ(y)

(1)

(2)

where W ∈ Rn×m, x ∈ Rm, y ∈ Rn, z ∈ Rn, m is the
dimension of the input vector, n is the dimension of the out-
put vector, and σ is a non-linear function (e.g., relu, tanh,
and sigmoid). During back propagation, we need to com-
pute the gradient of the parameter matrix W and the input
vector x:

(1 ≤ i ≤ n, 1 ≤ j ≤ m)

(3)

∂z
∂Wij

= σ

(cid:48)

ixT
j

∂z
∂xi

(cid:88)

=

W T

ij σ

(cid:48)
j

j

That

The proposed meProp uses approximate gradients by
keeping only top-k elements based on the magnitude va-
lues.
is, only the top-k elements with the lar-
gest absolute values are kept. For example, suppose a
vector v = (cid:104)1, 2, 3, −4(cid:105), then top2(v) = (cid:104)0, 0, 3, −4(cid:105).
We denote the indices of vector σ
(y)’s top-k values as
{t1, t2, ..., tk}(1 ≤ k ≤ n), and the approximate gradient
of the parameter matrix W and input vector x is:

(cid:48)

if

i ∈ {t1, t2, ..., tk} else 0

(5)

∂z
∂Wij

← σ

(cid:48)

ixT
j

∂z
∂xi

(cid:88)

←

W T

ij σ

(cid:48)
j

j

if j ∈ {t1, t2, ..., tk} else 0 (6)

As a result, only k rows or columns (depending on the la-
yout) of the weight matrix are modiﬁed, leading to a linear
reduction (k divided by the vector dimension) in the com-
putational cost.

Figure 1 is an illustration of meProp for a single computa-
tion unit of neural models. The original back propagation
uses the full gradient of the output vectors to compute the
gradient of the parameters. The proposed method selects
the top-k values of the gradient of the output vector, and
backpropagates the loss through the corresponding subset
of the total model parameters.

As for a complete neural network framework with a loss L,
the original back propagation computes the gradient of the
parameter matrix W as:

∂L
∂W

=

∂L
∂y

·

∂y
∂W

∂L
∂x

=

∂y
∂x

·

∂L
∂y

(7)

(8)

(1 ≤ j ≤ n, 1 ≤ i ≤ m)

(4)

while the gradient of the input vector x is:

(cid:48)

i ∈ Rn means ∂zi
∂yi

where σ
. We can see that the computati-
onal cost of back propagation is directly proportional to the
dimension of output vector n.

The proposed meProp selects top-k elements of the gra-
dient ∂L
∂y to approximate the original gradient, and passes

Sparsiﬁed Back Propagation for Accelerated Deep Learning with Reduced Overﬁtting

Figure 2. An illustration of the computational ﬂow of meProp.

them through the gradient computation graph according to
the chain rule. Hence, the gradient of W goes to:

while the gradient of the vector x is:

∂L
∂W

← topk(

∂L
∂y

) ·

∂y
∂W

∂L
∂x

∂y
∂x

←

· topk(

∂L
∂y

)

(9)

(10)

Figure 2 shows an illustration of the computational ﬂow of
meProp. The forward propagation is the same as traditio-
nal forward propagation, which computes the output vec-
tor via a matrix multiplication operation between two input
tensors. The original back propagation computes the full
gradient for the input vector and the weight matrix. For me-
Prop, back propagation computes an approximate gradient
by keeping top-k values of the backward ﬂowed gradient
and masking the remaining values to 0.

Figure 3 further shows the computational ﬂow of meProp
for the mini-batch case.

2.2. Implementation

We have coded two neural network models, including an
LSTM model for part-of-speech (POS) tagging, and a feed-
forward NN model (MLP) for transition-based dependency

parsing and MNIST image recognition. We use the optimi-
zers with automatically adaptive learning rates, including
Adam (Kingma & Ba, 2014) and AdaGrad (Duchi et al.,
2011). In our implementation, we make no modiﬁcation to
the optimizers, although there are many zero elements in
the gradients.

Most of the experiments on CPU are conducted on the fra-
mework coded in C# on our own. This framework builds a
dynamic computation graph of the model for each sample,
making it suitable for data in variable lengths. A typical
training procedure contains four parts: building the compu-
tation graph, forward propagation, back propagation, and
parameter update. We also have an implementation based
on the PyTorch framework for GPU based experiments.

2.2.1. WHERE TO APPLY MEPROP

The proposed method aims to reduce the complexity of the
back propagation by reducing the elements in the compu-
tationally intensive operations. In our preliminary observa-
tions, matrix-matrix or matrix-vector multiplication consu-
med more than 90% of the time of back propagation. In our
implementation, we apply meProp only to the back propa-
gation from the output of the multiplication to its inputs.
For other element-wise operations (e.g., activation functi-
ons), the original back propagation procedure is kept, be-

Sparsiﬁed Back Propagation for Accelerated Deep Learning with Reduced Overﬁtting

Figure 3. An illustration of the computational ﬂow of meProp on a mini-batch learning setting.

cause those operations are already fast enough compared
with matrix-matrix or matrix-vector multiplication operati-
ons.

If there are multiple hidden layers, the top-k sparsiﬁcation
needs to be applied to every hidden layer, because the spar-
siﬁed gradient will again be dense from one layer to anot-
her. That is, in meProp the gradients are sparsiﬁed with a
top-k operation at the output of every hidden layer.

While we apply meProp to all hidden layers using the same
k of top-k, usually the k for the output layer could be diffe-
rent from the k for the hidden layers, because the output
layer typically has a very different dimension compared
with the hidden layers. For example, there are 10 tags in
the MNIST task, so the dimension of the output layer is
10, and we use an MLP with the hidden dimension of 500.
Thus, the best k for the output layer could be different from
that of the hidden layers.

2.2.2. CHOICE OF TOP-k ALGORITHMS

Instead of sorting the entire vector, we use the well-known
min-heap based top-k selection method, which is slightly
changed to focus on memory reuse. The algorithm has a
time complexity of O(n log k) and a space complexity of
O(k).

3. Related Work

Riedmiller and Braun (1993) proposed a direct adaptive
method for fast learning, which performs a local adap-
tation of the weight update according to the behavior of
the error function. Tollenaere (1990) also proposed an
adaptive acceleration strategy for back propagation. Dro-
pout (Srivastava et al., 2014) is proposed to improve trai-
ning speed and reduce the risk of overﬁtting. Sparse co-
ding is a class of unsupervised methods for learning sets of
over-complete bases to represent data efﬁciently (Olshau-
sen & Field, 1996). Ranzato et al. (2006) proposed a sparse
autoencoder model for learning sparse over-complete fea-
tures. The proposed method is quite different compared
with those prior studies on back propagation, dropout, and
sparse coding.

The sampled-output-loss methods (Jean et al., 2015) are
limited to the softmax layer (output layer) and are only
based on random sampling, while our method does not
have those limitations. The sparsely-gated mixture-of-
experts (Shazeer et al., 2017) only sparsiﬁes the mixture-
of-experts gated layer and it is limited to the speciﬁc set-
ting of mixture-of-experts, while our method does not have
those limitations. There are also prior studies focusing
on reducing the communication cost in distributed sys-
tems (Seide et al., 2014; Dryden et al., 2016), by quanti-

Sparsiﬁed Back Propagation for Accelerated Deep Learning with Reduced Overﬁtting

Table 1. Results based on LSTM/MLP models and AdaGrad/Adam optimizers. Time means averaged time per iteration. Iter means the
number of iterations to reach the optimal score on development data. The model of this iteration is then used to obtain the test score.

POS-Tag (AdaGrad)
LSTM (h=500)
meProp (k=5)
Parsing (AdaGrad)
MLP (h=500)
meProp (k=20)
MNIST (AdaGrad)
MLP (h=500)
meProp (k=10)
POS-Tag (Adam)
LSTM (h=500)
meProp (k=5)
Parsing (Adam)
MLP (h=500)
meProp (k=20)
MNIST (Adam)
MLP (h=500)
meProp (k=20)

Iter Backprop time (s) Dev Acc (%)

4
4

17,534.4

253.2 (69.2x)

96.89
97.18 (+0.29)
Iter Backprop time (s) Dev UAS (%)
11
8

89.07
89.17 (+0.10)
Iter Backprop time (s) Dev Acc (%)

492.3 (18.1x)

8,899.7

171.0

8
98.20
98.20 (+0.00)
16
Iter Backprop time (s) Dev Acc (%)

4.1 (41.7x)

2
5

16,167.3

247.2 (65.4x)

97.18
97.14 (-0.04)
Iter Backprop time (s) Dev UAS (%)
14
6

90.12
90.02 (-0.10)
Iter Backprop time (s) Dev Acc (%)
17
15

98.32
98.22 (-0.10)

488.7 (18.6x)

7.9 (21.4x)

9,077.7

169.5

Test Acc (%)
96.93
97.25 (+0.32)
Test UAS (%)
88.92
88.95 (+0.03)
Test Acc (%)
97.52
98.00 (+0.48)
Test Acc (%)
97.07
97.12 (+0.05)
Test UAS (%)
89.84
90.01 (+0.17)
Test Acc (%)
97.82
98.01 (+0.19)

Table 2. Overall forward propagation time vs. overall back propa-
gation time. Time means averaged time per iteration. FP means
forward propagation. BP means back propagation. Ov. time me-
ans overall training time (FP + BP).

7,334s 16,522s
7,362s

POS-Tag (Adam) Ov. FP time Ov. BP time
LSTM (h=500)
meProp (k=5)
Parsing (Adam) Ov. FP time Ov. BP time
MLP (h=500)
meProp (k=20)
MNIST (Adam) Ov. FP time Ov. BP time
MLP (h=500)
meProp (k=20)

3,906s
4,002s

69s
68s

9,114s

171s

540s (30.5x)

513s (17.7x)

9s (18.4x)

Ov. time
23,856s
7,903s
Ov. time
13,020s
4,516s
Ov. time
240s
77s

zing each value of the gradient from 32-bit ﬂoat to only
1-bit. Those settings are also different from ours.

4. Experiments

To demonstrate that
the proposed method is general-
purpose, we perform experiments on different models
(LSTM/MLP), various training methods (Adam/AdaGrad),
and diverse tasks.

Part-of-Speech Tagging (POS-Tag): We use the standard
benchmark dataset in prior work (Collins, 2002), which is
derived from the Penn Treebank corpus, and use sections
0-18 of the Wall Street Journal (WSJ) for training (38,219
examples), and sections 22-24 for testing (5,462 examples).
The evaluation metric is per-word accuracy. A popular mo-
del for this task is the LSTM model (Hochreiter & Schmid-
huber, 1997),1 which is used as our baseline.

Transition-based Dependency Parsing (Parsing): Follo-
wing prior work, we use English Penn TreeBank (PTB)
(Marcus et al., 1993) for evaluation. We follow the stan-
dard split of the corpus and use sections 2-21 as the trai-
ning set (39,832 sentences, 1,900,056 transition exam-
ples),2 section 22 as the development set (1,700 sentences,
80,234 transition examples) and section 23 as the ﬁnal test
set (2,416 sentences, 113,368 transition examples). The
evaluation metric is unlabeled attachment score (UAS). We
implement a parser using MLP following Chen and Man-
ning (2014), which is used as our baseline.

MNIST Image Recognition (MNIST): We use the
MNIST handwritten digit dataset (LeCun et al., 1998) for
evaluation. MNIST consists of 60,000 28×28 pixel trai-
ning images and additional 10,000 test examples. Each
image contains a single numerical digit (0-9). We select
the ﬁrst 5,000 images of the training images as the deve-
lopment set and the rest as the training set. The evaluation
metric is per-image accuracy. We use the MLP model as
the baseline.

4.1. Experimental Settings

We set the dimension of the hidden layers to 500 for all
the tasks. For POS-Tag, the input dimension is 1 (word) ×
50 (dim per word) + 7 (features) × 20 (dim per feature) =
190, and the output dimension is 45. For Parsing, the input
dimension is 48 (features) × 50 (dim per feature) = 2400,
and the output dimension is 25. For MNIST, the input di-
mension is 28 (pixels per row) × 28 (pixels per column) ×
1 (dim per pixel) = 784, and the output dimension is 10.
As discussed in Section 2, the optimal k of top-k for the

2A transition example consists of a parsing context and its op-

1In this work, we use the bi-directional LSTM (Bi-LSTM) as

the implementation of LSTM.

timal transition action.

Sparsiﬁed Back Propagation for Accelerated Deep Learning with Reduced Overﬁtting

Figure 4. Accuracy vs. meProp’s backprop ratio (left). Results of top-k meProp vs. random meProp (middle). Results of top-k meProp
vs. baseline with the hidden dimension h (right).

output layer could be different from the hidden layers, be-
cause their dimensions could be very different. For Parsing
and MNIST, we ﬁnd using the same k for the output and the
hidden layers works well, and we simply do so. For anot-
her task, POS-Tag, we ﬁnd the the output layer should use
a different k from the hidden layers. For simplicity, we do
not apply meProp to the output layer for POS-Tag, because
in this task we ﬁnd the computational cost of the output
layer is almost negligible compared with other layers.

The hyper-parameters are tuned based on the development
data. For the Adam optimization method, we ﬁnd the
default hyper-parameters work well on development sets,
the learning rate α = 0.001, and
which are as follows:
β1 = 0.9, β2 = 0.999, (cid:15) = 1×10−8. For the AdaGrad lear-
ner, the learning rate is set to α = 0.01, 0.01, 0.1 for POS-
Tag, Parsing, and MNIST, respectively, and (cid:15) = 1 × 10−6.
The experiments on CPU are conducted on a computer with
the INTEL(R) Xeon(R) 3.0GHz CPU. The experiments on
GPU are conducted on NVIDIA GeForce GTX 1080.

4.2. Experimental Results

In this experiment, the LSTM is based on one hidden layer
and the MLP is based on two hidden layers (experiments
on more hidden layers will be presented later). We conduct
experiments on different optimization methods, including
AdaGrad and Adam. Since meProp is applied to the li-
near transformations (which entail the major computational
cost), we report the linear transformation related backprop
time as Backprop Time. It does not include non-linear acti-
vations, which usually have only less than 2% computatio-
nal cost. The total time of back propagation, including non-
linear activations, is reported as Overall Backprop Time.
Based on the development set and prior work, we set the
mini-batch size to 1 (sentence), 10,000 (transition exam-
ples), and 10 (images) for POS-Tag, Parsing, and MNIST,
respectively. Using 10,000 transition examples for Parsing
follows Chen and Manning (2014).

Table 1 shows the results based on different models and

different optimization methods. In the table, meProp me-
ans applying meProp to the corresponding baseline model,
h = 500 means that the hidden layer dimension is 500, and
k = 20 means that meProp uses top-20 elements (among
500 in total) for back propagation. Note that, for fair com-
parisons, all experiments are ﬁrst conducted on the deve-
lopment data and the test data is not observable. Then,
the optimal number of iterations is decided based on the
optimal score on development data, and the model of this
iteration is used upon the test data to obtain the test scores.

As we can see, applying meProp can substantially speed
up the back propagation. It provides a linear reduction in
the computational cost. Surprisingly, results demonstrate
that we can update only 1–4% of the weights at each back
propagation pass. This does not result in a larger number
of training iterations. More surprisingly, the accuracy of
the resulting models is actually improved rather than de-
creased. The main reason could be that the minimal effort
update does not modify weakly relevant parameters, which
makes overﬁtting less likely, similar to the dropout effect.

Table 2 shows the overall forward propagation time, the
overall back propagation time, and the training time by
summing up forward and backward propagation time. As
we can see, back propagation has the major computational
cost in training LSTM/MLP.

The results are consistent among AdaGrad and Adam. The
results demonstrate that meProp is independent of speci-
ﬁc optimization methods. For simplicity, in the following
experiments the optimizer is based on Adam.

4.3. Varying Backprop Ratio

In Figure 4 (left), we vary the k of top-k meProp to com-
pare the test accuracy on different ratios of meProp back-
prop. For example, when k=5, it means that the backprop
ratio is 5/500=1%. The optimizer is Adam. As we can see,
meProp achieves consistently better accuracy than the ba-
seline. The best test accuracy of meProp, 98.15% (+0.33),

02040608010097.697.797.897.99898.198.2Backprop Ratio (%)Accuracy (%)MNIST: Reduce Overfitting  mePropMLP05101586889092949698100Backprop Ratio (%)Accuracy (%)MNIST: Topk vs Random  Topk mePropRandom meProp02468109192939495969798Backprop/Hidden Ratio (%)Accuracy (%)MNIST: Change h/k  mePropMLPSparsiﬁed Back Propagation for Accelerated Deep Learning with Reduced Overﬁtting

Table 3. Results based on the same k and h.
POS-Tag (Adam)
LSTM (h=5)
meProp (k=5)
Parsing (Adam)
MLP (h=20)
meProp (k=20)
MNIST (Adam)
MLP (h=20)
meProp (k=20)

Test Acc (%)
96.40
97.12 (+0.72)
Test UAS (%)
88.37
90.01 (+1.64)
Test Acc (%)
95.77
98.01 (+2.24)

Iter
7
5
Iter
18
6
Iter
15
17

Table 4. Adding the dropout technique.

POS-Tag (Adam) Dropout
0.5
LSTM (h=500)
0.5
meProp (k=20)
Parsing (Adam)
Dropout
0.5
MLP (h=500)
0.5
meProp (k=40)
MNIST (Adam)
Dropout
0.2
MLP (h=500)
0.2
meProp (k=25)

Test Acc (%)
97.20
97.31 (+0.11)
Test UAS (%)
91.53
91.99 (+0.46)
Test Acc (%)
98.09
98.32 (+0.23)

is actually better than the one reported in Table 1.

4.4. Top-k vs. Random

It will be interesting to check the role of top-k elements.
Figure 4 (middle) shows the results of top-k meProp vs.
random meProp. The random meProp means that random
elements (instead of top-k ones) are selected for back pro-
pagation. As we can see, the top-k version works better
than the random version. It suggests that top-k elements
contain the most important information of the gradients.

2

3

4

5

2

5

Table 5. Varying the number of hidden layers on the MNIST task.
The optimizer is Adam. Layers: the number of hidden layers.

Layers Method

MLP (h=500)
meProp (k=25)
MLP (h=500)
meProp (k=25)
MLP (h=500)
meProp (k=25)
MLP (h=500)
meProp (k=25)

Test Acc (%)
98.10
98.20 (+0.10)
98.21
98.37 (+0.16)
98.10
98.15 (+0.05)
98.05
98.21 (+0.16)

Table 6. Results of simple uniﬁed top-k meProp based on a whole
mini-batch (i.e., uniﬁed sparse patterns). The optimizer is Adam.
Mini-batch Size is 50.

Layers Method

MLP (h=500)
meProp (k=30)
MLP (h=500)
meProp (k=50)

Test Acc (%)
97.97
98.08 (+0.11)
98.09
98.36 (+0.27)

4.6. Adding Dropout

Since we have observed that meProp can reduce overﬁtting
of deep learning, a natural question is that if meProp is re-
ducing the same type of overﬁtting risk as dropout. Thus,
we use development data to ﬁnd a proper value of the dro-
pout rate on those tasks, and then further add meProp to
check if further improvement is possible.

Table 4 shows the results. As we can see, meProp can
achieve further improvement over dropout. In particular,
meProp has an improvement of 0.46 UAS on Parsing. The
results suggest that the type of overﬁtting that meProp re-
duces is probably different from that of dropout. Thus, a
model should be able to take advantage of both meProp
and dropout to reduce overﬁtting.

4.5. Varying Hidden Dimension

4.7. Adding More Hidden Layers

We still have a question: does the top-k meProp work well
simply because the original model does not require that big
dimension of the hidden layers? For example, the meProp
(topk=5) works simply because the LSTM works well with
the hidden dimension of 5, and there is no need to use the
hidden dimension of 500. To examine this, we perform
experiments on using the same hidden dimension as k, and
the results are shown in Table 3. As we can see, however,
the results of the small hidden dimensions are much worse
than those of meProp.

In addition, Figure 4 (right) shows more detailed curves
by varying the value of k. In the ﬁgure, different k gives
different backprop ratio for meProp and different hidden
dimension ratio for LSTM/MLP. As we can see, the ans-
wer to that question is negative: meProp does not rely on
redundant hidden layer elements.

Another question is whether or not meProp relies on shal-
low models with only a few hidden layers. To answer this
question, we also perform experiments on more hidden lay-
ers, from 2 hidden layers to 5 hidden layers. We ﬁnd setting
the dropout rate to 0.1 works well for most cases of diffe-
rent numbers of layers. For simplicity of comparison, we
set the same dropout rate to 0.1 in this experiment. Table 5
shows that adding the number of hidden layers does not
hurt the performance of meProp.

4.8. Speedup on GPU

For implementing meProp on GPU, the simplest solution
is to treat the entire mini-batch as a “big training exam-
ple”, where the top-k operation is based on the averaged va-
lues of all examples in the mini-batch. In this way, the big
sparse matrix of the mini-batch will have consistent sparse

Sparsiﬁed Back Propagation for Accelerated Deep Learning with Reduced Overﬁtting

Table 7. Acceleration results on the matrix multiplication synthe-
tic data using GPU. The batch size is 1024.

Method
Baseline (h=8192)
meProp (k=8)
meProp (k=16)
meProp (k=32)
meProp (k=64)
meProp (k=128)
meProp (k=256)
meProp (k=512)

Backprop time (ms)

308.00

8.37 (36.8x)
9.16 (33.6x)
11.20 (27.5x)
14.38 (21.4x)
21.28 (14.5x)
38.57 (8.0x)
69.95 (4.4x)

Table 8. Acceleration results on MNIST using GPU.

Overall backprop time (ms)

Method
MLP (h=8192)
meProp (k=8)
meProp (k=16)
meProp (k=32)
meProp (k=64)
meProp (k=128)
meProp (k=256)
meProp (k=512)

17,696.2

1,501.5 (11.8x)
1,542.8 (11.5x)
1,656.9 (10.7x)
1,828.3 (9.7x)
2,200.0 (8.0x)
3,149.6 (5.6x)
4,874.1 (3.6x)

patterns among examples, and this consistent sparse matrix
can be transformed into a small dense matrix by removing
the zero values. We call this implementation as simple uni-
ﬁed top-k. This experiment is based on PyTorch.

Despite its simplicity, Table 6 shows the good performance
of this implementation, which is based on the mini-batch
size of 50. We also ﬁnd the speedup on GPU is less sig-
niﬁcant when the hidden dimension is low. The reason
is that our GPU’s computational power is not fully consu-
med by the baseline (with small hidden layers), so that the
normal back propagation is already fast enough, making it
hard for meProp to achieve substantial speedup. For ex-
ample, supposing a GPU can ﬁnish 1000 operations in one
cycle, there could be no speed difference between a met-
hod with 100 and a method with 10 operations. Indeed, we
ﬁnd MLP (h=64) and MLP (h=512) have almost the same
GPU speed even on forward propagation (i.e., without me-
Prop), while theoretically there should be an 8x difference.
With GPU, the forward propagation time of MLP (h=64)
and MLP (h=512) is 572ms and 644ms, respectively. This
provides evidence for our hypothesis that our GPU is not
fully consumed with the small hidden dimensions.

Thus, the speedup test on GPU is more meaningful for the
heavy models, such that the baseline can at least fully con-
sume the GPU’s computational power. To check this, we
test the GPU speedup on synthetic data of matrix multi-
plication with a larger hidden dimension. Indeed, Table 7
shows that meProp achieves much higher speed than the
traditional backprop with the large hidden dimension. Furt-
hermore, we test the GPU speedup on MLP with the large
hidden dimension (Dryden et al., 2016). Table 8 shows that

meProp also has substantial GPU speedup on MNIST with
the large hidden dimension. In this experiment, the speedup
is based on Overall Backprop Time (see the prior deﬁni-
tion). Those results demonstrate that meProp can achieve
good speedup on GPU when it is applied to heavy models.

Finally, there are potentially other implementation choices
of meProp on GPU. For example, another natural solution
is to use a big sparse matrix to represent the sparsiﬁed gra-
dient of the output of a mini-batch. Then, the sparse matrix
multiplication library can be used to accelerate the com-
putation. This could be an interesting direction of future
work.

4.9. Related Systems on the Tasks

The POS tagging task is a well-known benchmark task,
with the accuracy reports from 97.2% to 97.4% (Toutanova
et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al.,
2011; Collobert et al., 2011; Huang et al., 2015). Our met-
hod achieves 97.31% (Table 4).

For the transition-based dependency parsing task, existing
approaches typically can achieve the UAS score from 91.4
to 91.5 (Zhang & Clark, 2008; Nivre et al., 2007; Huang &
Sagae, 2010). As one of the most popular transition-based
parsers, MaltParser (Nivre et al., 2007) has 91.5 UAS. Chen
and Manning (2014) achieves 92.0 UAS using neural net-
works. Our method achieves 91.99 UAS (Table 4).

For MNIST, the MLP based approaches can achieve 98–
99% accuracy, often around 98.3% (LeCun et al., 1998; Si-
mard et al., 2003; Ciresan et al., 2010). Our method achie-
ves 98.37% (Table 5). With the help from convolutional
layers and other techniques, the accuracy can be improved
to over 99% (Jarrett et al., 2009; Ciresan et al., 2012). Our
method can also be improved with those additional techni-
ques, which, however, are not the focus of this paper.

5. Conclusions

The back propagation in deep learning tries to modify all
parameters in each stochastic update, which is inefﬁcient
and may even lead to overﬁtting due to unnecessary mo-
diﬁcation of many weakly relevant parameters. We pro-
pose a minimal effort back propagation method (meProp),
in which we compute only a very small but critical portion
of the gradient, and modify only the corresponding small
portion of the parameters in each update. This leads to very
sparsiﬁed gradients to modify only highly relevant parame-
ters for the given training sample. The proposed meProp
is independent of the optimization method. Experiments
show that meProp can reduce the computational cost of
back propagation by one to two orders of magnitude via
updating only 1–4% parameters, and yet improve the mo-
del accuracy in most cases.

Sparsiﬁed Back Propagation for Accelerated Deep Learning with Reduced Overﬁtting

Acknowledgements

The authors would like to thank the anonymous revie-
wers for insightful comments and suggestions on this pa-
per. This work was supported in part by National Natural
Science Foundation of China (No. 61673028), National
High Technology Research and Development Program of
China (863 Program, No. 2015AA015404), and an Okawa
Research Grant (2016).

References

Chen, Danqi and Manning, Christopher D. A fast and accu-
rate dependency parser using neural networks. In Pro-
ceedings of EMNLP’14, pp. 740–750, 2014.

Ciresan, Dan C., Meier, Ueli, Gambardella, Luca Maria,
and Schmidhuber, J¨urgen. Deep, big, simple neural nets
for handwritten digit recognition. Neural Computation,
22(12):3207–3220, 2010.

Ciresan, Dan C., Meier, Ueli, and Schmidhuber, J¨urgen.
Multi-column deep neural networks for image classiﬁca-
tion. In Proceedings of CVPR’12, pp. 3642–3649, 2012.

Collins, Michael. Discriminative training methods for hid-
den markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of EMNLP’02, pp.
1–8, 2002.

Collobert, Ronan, Weston, Jason, Bottou, L´eon, Karlen,
Michael, Kavukcuoglu, Koray, and Kuksa, Pavel P. Na-
tural language processing (almost) from scratch. Journal
of Machine Learning Research, 12:2493–2537, 2011.

Dryden, Nikoli, Moon, Tim, Jacobs, Sam Ade, and Es-
sen, Brian Van. Communication quantization for data-
parallel training of deep neural networks. In Proceedings
of the 2nd Workshop on Machine Learning in HPC En-
vironments, pp. 1–8, 2016.

Duchi, John C., Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research,
12:2121–2159, 2011.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-
term memory. Neural computation, 9(8):1735–1780,
1997.

Huang, Liang and Sagae, Kenji. Dynamic programming
In Proceedings of

for linear-time incremental parsing.
ACL’10, pp. 1077–1086, 2010.

Jarrett,

Kevin,

Kavukcuoglu,

Ranzato,
Marc’Aurelio, and LeCun, Yann. What is the best
multi-stage architecture for object recognition?
In
Proceeding of ICCV’09, pp. 2146–2153, 2009.

Koray,

Jean, S´ebastien, Cho, KyungHyun, Memisevic, Roland,
and Bengio, Yoshua. On using very large target voca-
bulary for neural machine translation. In Proceedings of
ACL/IJCNLP’15, pp. 1–10, 2015.

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for
stochastic optimization. CoRR, abs/1412.6980, 2014.

LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document re-
cognition. Proceedings of the IEEE, 86(11):2278–2324,
1998.

Marcus, Mitchell P, Marcinkiewicz, Mary Ann, and San-
torini, Beatrice. Building a large annotated corpus of
english: The penn treebank. Computational linguistics,
19(2):313–330, 1993.

Nivre, Joakim, Hall, Johan, Nilsson, Jens, Chanev, Atanas,
Eryigit, G¨ulsen, K¨ubler, Sandra, Marinov, Svetoslav, and
Marsi, Erwin. Maltparser: A language-independent sy-
stem for data-driven dependency parsing. Natural Lan-
guage Engineering, 13(2):95–135, 2007.

Olshausen, Bruno A and Field, David J. Natural image
statistics and efﬁcient coding. Network: Computation in
Neural Systems, 7(2):333–339, 1996.

Ranzato, Marc’Aurelio, Poultney, Christopher S., Chopra,
Sumit, and LeCun, Yann. Efﬁcient learning of sparse re-
presentations with an energy-based model. In NIPS’06,
pp. 1137–1144, 2006.

Riedmiller, Martin and Braun, Heinrich. A direct adaptive
method for faster backpropagation learning: The rprop
algorithm. In Proceedings of IEEE International Confe-
rence on Neural Networks 1993, pp. 586–591, 1993.

Seide, Frank, Fu, Hao, Droppo, Jasha, Li, Gang, and Yu,
Dong. 1-bit stochastic gradient descent and its applica-
tion to data-parallel distributed training of speech dnns.
In Proceedings of INTERSPEECH’14, pp. 1058–1062,
2014.

Shazeer, Noam, Mirhoseini, Azalia, Maziarz, Krzysz-
tof, Davis, Andy, Le, Quoc V., Hinton, Geoffrey E.,
and Dean, Jeff. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. CoRR,
abs/1701.06538, 2017.

Huang, Zhiheng, Xu, Wei, and Yu, Kai.

Bidirecti-
onal lstm-crf models for sequence tagging. CoRR,
abs/1508.01991, 2015.

Shen, Libin, Satta, Giorgio, and Joshi, Aravind K. Gui-
ded learning for bidirectional sequence classiﬁcation. In
Proceedings of ACL’07, pp. 760–767, 2007.

Sparsiﬁed Back Propagation for Accelerated Deep Learning with Reduced Overﬁtting

Simard, Patrice Y., Steinkraus, Dave, and Platt, John C.
Best practices for convolutional neural networks applied
to visual document analysis. In Proceedings of ICDR’03,
pp. 958–962, 2003.

Srivastava, Nitish, Hinton, Geoffrey E., Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:
a simple way to prevent neural networks from overﬁt-
Journal of Machine Learning Research, 15(1):
ting.
1929–1958, 2014.

Sun, Xu. Structure regularization for structured prediction.

In NIPS’14, pp. 2402–2410. 2014.

Tollenaere, Tom. Supersab: fast adaptive back propaga-
tion with good scaling properties. Neural networks, 3
(5):561–573, 1990.

Toutanova, Kristina, Klein, Dan, Manning, Christopher D.,
and Singer, Yoram. Feature-rich part-of-speech tagging
In Proceedings of
with a cyclic dependency network.
HLT-NAACL’03, pp. 173–180, 2003.

Tsuruoka, Yoshimasa, Miyao, Yusuke, and Kazama,
Jun’ichi. Learning with lookahead: Can history-based
models rival globally optimized models? In Proceedings
of CoNLL’11, pp. 238–246, 2011.

Zhang, Yue and Clark, Stephen. A tale of two par-
sers:
Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings of
EMNLP’08, pp. 562–571, 2008.

