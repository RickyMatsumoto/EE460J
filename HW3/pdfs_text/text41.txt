Generalization and Equilibrium in Generative Adversarial Nets (GANs)

Sanjeev Arora 1 Rong Ge 2 Yingyu Liang 1 Tengyu Ma 1 Yi Zhang 1

Abstract

It is shown that training of generative adversar-
ial network (GAN) may not have good gener-
alization properties; e.g., training may appear
successful but the trained distribution may be
far from target distribution in standard metrics.
However, generalization does occur for a weaker
metric called neural net distance. It is also shown
that an approximate pure equilibrium exists in the
discriminator/generator game for a natural train-
ing objective (Wasserstein) when generator ca-
pacity and training set sizes are moderate. This
existence of equilibrium inspires MIX+GAN pro-
tocol, which can be combined with any existing
GAN training, and empirically shown to improve
some of them.

1. Introduction

Generative Adversarial Networks (GANs) (Goodfellow
et al., 2014) have become one of the dominant methods for
ﬁtting generative models to complicated real-life data, and
even found unusual uses such as designing good crypto-
graphic primitives (Abadi & Andersen, 2016). See a survey
by (Goodfellow, 2016). Various novel architectures and
training objectives were introduced to address perceived
shortcomings of the original idea, leading to more stable
training and more realistic generative models in practice
(Odena et al., 2016; Huang et al., 2017; Radford
(see
et al., 2016; Tolstikhin et al., 2017; Salimans et al., 2016;
Jiwoong Im et al., 2016; Durugkar et al., 2016) and the ref-
erence therein).

The goal is to train a generator deep net whose input is
a standard Gaussian, and whose output is a sample from
on Rd, which has to be close to some
some distribution
D
Dreal (which could be, say, real-life im-
target distribution
1Princeton Univer-
sity, Princeton NJ 2Duke University, Durham NC. Corre-
spondence to: Rong Ge <rongge@cs.duke.edu>, Yi Zhang
<y.zhang@cs.princeton.edu>.

Authors listed in alphabetical order.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure 1. Probability density

real with many peaks and valleys

D

D

D

D

or

Dreal and

ages represented using raw pixels). The training uses sam-
ples from
Dreal and together with the generator net also
trains a discriminator deep net trying to maximise its ability
to distinguish between samples from
. So long
as the discriminator is successful at this task with nonzero
probability, its success can be used to generate a feedback
(using backpropagation) to the generator, thus improving
its distribution
. Training is continued until the generator
wins, meaning that the discriminator can do no better than
random guessing when deciding whether or not a particular
Dreal. This basic iterative frame-
sample came from
work has been tried with many training objectives; see Sec-
tion 2. But it has been unclear what to conclude when the
generator wins this game: is
Dreal in some met-
D
ric? One seems to need some extension of generalization
theory that would imply such a conclusion. The hurdle is
that distribution
Dreal could be complicated and may have
many peaks and valleys; see Figure 1. The number of peaks
(modes) may even be exponential in d. (Recall the curse of
dimensionality: in d dimensions there are exp(d) directions
whose pairwise angle exceeds say ⇡/3, and each could be
the site of a peak.) Whereas the number of samples from
for that matter) used in the training is
Dreal (and from
a lot fewer, and thus may not reﬂect most of the peaks and
Dreal.
valleys of

close to

D

A standard analysis due to (Goodfellow et al., 2014) shows
that when the discriminator capacity (= number of parame-
ters) and number of samples is “large enough”, then a win
by the generator implies that
Dreal (see
D
Section 2). But the discussion in the previous paragraph
raises the possibility that “sufﬁciently large” in this analy-
sis may need to be exp(d).

is very close to

Another open theoretical issue is whether an equilibrium
always exists in this game between generator and discrim-

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

inator. Just as a zero gradient is a necessary condition for
standard optimization to halt, the corresponding necessary
condition in a two-player game is an equilibrium. Con-
ceivably some of the instability often observed while train-
ing GANs could just arise because of lack of equilibrium.
(Recently Arjovsky et al. (2017) suggest that using their
Wasserstein objective in practice reduces instability, but
we still lack proof of existence of an equilibrium.) Stan-
dard game theory is of no help here because we need a so-
called pure equilibrium, and simple counter-examples such
as rock/paper/scissors show that it doesn’t exist in general1.

1.1. Our Contributions

We formally deﬁne generalization for GANs in Section 3
and show that for previously studied notions of distance
between distributions, generalization is not guaranteed
In fact we show that the generator can win
(Lemma 1).
Dreal are arbitrarily far in any standard
and
even when
metric.

D

However, we can guarantee some weaker notion of gener-
alization by introducing a new metric on distributions, the
neural net distance. We show that generalization does hap-
pen with moderate number of training examples (i.e., when
the generator wins, the two distributions must be close in
neural net distance). However, this weaker metric comes at
a cost: it can be near-zero even when the trained and target
distributions are very far (Section 3.1)

To explore the existence of equilibria we turn in Section 4
to inﬁnite mixtures of generator deep nets. These are
clearly vastly more expressive than a single generator net:
e.g., a standard result in bayesian nonparametrics says that
every probability density is closely approximable by an in-
ﬁnite mixture of Gaussians (Ghosh et al., 2003). Thus un-
surprisingly, an inﬁnite mixture should win the game. We
then prove rigorously that even a ﬁnite mixture of fairly
reasonable size can closely approximate the performance
of the inﬁnite mixture (Theorem 4.2).

This insight also allows us to show for a natural GAN set-
ting with Wasserstein objective there exists an approximate
equilibrium that is pure. (Roughly speaking, an approxi-
mate equilibrium is one in which neither of the players can
gain much by deviating from their strategies.)

This existence proof for an approximate equilibrium unfor-
tunately involves a quadratic blowup in the “size” of the
generator (which is still better than the naive exponential
blowup one might expect). Improving this is left for future
theoretical work. But we propose a heuristic approxima-
tion to the mixture idea to introduce a new framework for

1Such counterexamples are easily turned into toy GAN sce-
narios with generator and discriminator having ﬁnite capacity, and
the game lacks a pure equilibrium. See supplementary material.

training that we call MIX+GAN. It can be added on top of
any existing GAN training procedure, including those that
use divergence objectives. Experiments in Section 6 show
that for several previous techniques, MIX+GAN stabilizes
the training, and in some cases improves the performance.

2. Preliminaries

Notations. Throughout the paper we use d for the dimen-
sion of samples, and p for the number of parameters in the
generator/discriminator. In Section 3 we use m for number
of samples.

!

2U }

Gu, u
{

Generators and discriminators. Let
Rp) denote the class of generators, where Gu is a
(
U⇢
function — which is often a neural network in practice —
Rd indexed by u that denotes the parameters
from R`
denotes the possible ranges
of the generators. Here
of the parameters and without
loss of generality we
is a subset of the unit ball2. The generator Gu
assume
DGu as follows: generate h from
deﬁnes a distribution
`-dimensional spherical Gaussian distribution and then
apply Gu on h and generate a sample x = Gu(h) of the
DGu. We drop the subscript u in
distribution
DGu when
it’s clear from context.

U

U

2V}

Dv, v
{

denote the class of discriminators, where
Let
Dv is function from Rd to [0, 1] and v is the parameters of
Dv. The value Dv(x) is usually interpreted as the probabil-
ity that the sample x comes from the real distribution
Dreal
(as opposed to the generated distribution

DG).

Gu0 (h)

Gu(h)
k

We assume Gu and Dv are L-Lipschitz with respect to their
and any input h, we
parameters. That is, for all u, u0
(similar for D).
u
have
k
Notice, this is distinct from the assumption (which we
will also sometimes make) that functions Gu, Dv are Lip-
schitz: that focuses on the change in function value when
we change x, while keeping u, v ﬁxed3.

2U
u0
 

k 

 

L

k

Objective functions. The standard GAN training (Good-
fellow et al., 2014) consists of training parameters u, v so
as to optimize an objective function:

min
u
2U

max
v
2V

E
⇠Dreal

x

[log Dv(x)] + E

[log(1

Dv(x))].

x

⇠DGu

 

(1)
Intuitively, this says that the discriminator Dv should give
high values Dv(x) to the real samples and low values
Dv(x) to the generated examples. The log function was
suggested because of its interpretation as the likelihood,

2Otherwise we can scale the parameter properly by changing

the parameterization.

3Both Lipschitz parameters can be exponential in the number
of layers in the neural net, however our Theorems only depend on
the log of the Lipschitz parameters

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

and it also has a nice information-theoretic interpretation
described below. However, in practice it can cause prob-
lems since log x
0. The objective still
as x
makes intuitive sense if we replace log by any monotone
function   : [0, 1]

R, which yields the objective:

!  1

!

!
[ (Dv(x))]+ E

min
u
2U

max
v
2V

E
⇠Dreal

x

[ (1

Dv(x))]. (2)

x

⇠DGu

 

Dreal and

We call function   the measuring function. It should be
DG are the same distribu-
concave so that when
tion, the best strategy for the discriminator is just to output
1/2 and the optimal value is 2 (1/2). In later proofs, we
will require   to be bounded and Lipschitz. Indeed, in prac-
 )x) (which
tice training often uses  (x) = log(  + (1
takes values in [log  , 0] and is 1/ -Lipschitz) and the re-
cently proposed Wasserstein GAN (Arjovsky et al., 2017)
objective uses  (x) = x.

 

P

Dreal
1
m

to estimate the value Ex

The objective func-
Training with ﬁnite samples.
tion (2) assumes we have inﬁnite number of samples
⇠Dreal [ (Dv(x))].
from
With ﬁnite training examples x1, . . . , xm ⇠D real,
m
i=1[ (Dv(xi))] to estimate the quantity
one uses
⇠Dreal [ (Dv(x))]. We call the distribution that gives
Ex
1/m probability to each of the xi’s the empirical version
of the real distribution. Similarly, one can use a empirical
⇠DGu [ (1
version to estimate Ex
Standard interpretation via distance between distribu-
tions. Towards analyzing GANs, researchers have assumed
access to inﬁnite number of examples and that the discrim-
inator is chosen optimally within some large class of func-
tions that contain all possible neural nets. This often al-
lows computing analytically the optimal discriminator and
therefore removing the maximum operation from the ob-
jective (2), which leads to some interpretation of how and
in what sense the resulting distribution
DG is close to the
Dreal.
true distribution

Dv(x))].

 

!

Preal(x)

Using the original objective function (1), then the optimal
choice among all the possible functions from Rd
(0, 1)
is D(x) =
Preal(x)+PG(x) , as shown in (Goodfellow et al.,
2014). Here Preal(x) is the density of x in the real dis-
tribution, and PG(x) is the density of x in the distribu-
tion generated by generator G. Using this discriminator —
though it’s computationally infeasible to obtain it — one
can show that the minimization problem over the genera-
tor correspond to minimizing the Jensen-Shannon (JS) di-
Dreal and the gen-
vergence between the true distribution
erative distribution
DG. Recall that for two distributions
µ and ⌫, the JS divergence is deﬁned by dJS(µ, ⌫) =
µ+⌫
1
2 )).
2 (KL(µ
Other measuring functions   and choice of discrimi-
nator class leads to different distance function between
(Ar-
distribution other than JS divergence. Notably,

µ+⌫
2 ) + KL(⌫

k

k

jovsky et al., 2017) shows that when  (t) = t, and
the discriminator is chosen among all 1-Lipschitz func-
tions, maxing out
the generator is
the discriminator,
attempting to minimize the Wasserstein distance be-
Dreal and Du(h). Recall that Wasserstein dis-
tween
tance between µ and ⌫ is deﬁned as dW (µ, ⌫) =
µ[D(x)]
supD is 1-Lipschitz |

⌫[D(x)]

Ex

Ex

 

⇠

⇠

|

.

3. Generalization Theory for GANs

The above interpretation of GANs in terms of minimizing
distance (such as JS divergence and Wasserstein distance)
between the real distribution and the generated distribution
relies on two crucial assumptions: (i) very expressive class
of discriminators such as the set of all bounded discrimina-
tor or the set of all 1-Lipschitz discriminators, and (ii) very
large number of examples to compute/estimate the objec-
tive (1) or (2). Neither assumption holds in practice, and
we will show next that this greatly affects the generaliza-
tion ability, a notion we introduce in Section 3.1.

3.1. Deﬁnition of Generalization

Our deﬁnition is motivated from supervised classiﬁcation,
where training is said to generalize if the training and test
error closely track each other. (Since the purpose of GANs
training is to learn a distribution, one could also consider
a stronger deﬁnition of successful training, as discussed in
Section 3.4.)
Let x1, . . . , xm be the training examples, and let ˆ
Dreal
denote the uniform distribution over x1, . . . , xm. Simi-
larly, let Gu(h1), . . . , Gu(hr) be a set of r examples from
In the training of GANs,
the generated distribution
DG.
[ (Dv(x))] to approximate
one implicitly uses Ex
ˆ
Dreal
⇠Dreal [ (Dv(x))]. Inspired by the observa-
the quantity Ex
tion that the training objective of GANs and its variants is
) between
to minimize some distance (or divergence) d(
DG using ﬁnite samples, we deﬁne the general-
Dreal and
ization of GANs as follows:
Deﬁnition 1. A divergence or distance d(
) between dis-
·
·
tributions generalizes with m training examples and error
" if for the learnt distribution
DG, the following holds with
high probability,

⇠

·

,

·

,

d(

Dreal,

DG)

 

Dreal, ˆ
d( ˆ

DG)

"



(3)

 
 
 
 
where ˆ
Dreal is an empirical version of the true distribution
 
 
(with m samples) and ˆ
DG is an empirical version of the
generated distribution with polynomial number of samples.

In words, generalization in GANs means that the popula-
tion distance between the true and generated distribution is
close to the empirical distance between the empirical dis-
tributions. Our target is to make the former distance small,

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

whereas the latter one is what we can access and minimize
in practice. The deﬁnition allows only polynomial num-
ber of samples from the generated distribution because the
training algorithm should run in polynomial time.

3.2. JS Divergence and Wasserstein don’t Generalize

As a warm-up, we show that JS divergence and Wasserstein
distance don’t generalize with any polynomial number of
examples because the population distance (divergence) is
not reﬂected by the empirical distance.

Lemma 1. Let µ be uniform Gaussian distributions
d I) and ˆµ be an empirical versions of µ with m ex-
1.1.

N
amples. Then we have dJS(µ, ˆµ) = log 2, dW (µ, ˆµ)

(0, 1

 

There are two consequences of Lemma 1. First, consider
DG = µ. Then we have that
the situation where
Dreal, ˆ
DG) > 1 as long as
Dreal,
dW (
we have polynomial number of examples. This violates the
generalization deﬁnition equation (3).

Dreal =
DG) = 0 but dW ( ˆ

DG = ˆ

Dreal =
Dreal = µ and
Second, consider the case
ˆµ, that is,
DG memorizes all of the training examples in
ˆ
Dreal. In this case, since
DG is a discrete distribution with
ﬁnite supports, with enough (polynomial) examples, in ˆ
DG,
effectively we also have that ˆ
DG ⇡D G. Therefore, we
have that dW ( ˆ
DG) >
0 whereas dW (
1. In other words, with any polynomial number of exam-
ples, it’s possible to overﬁt to the training examples using
Wasserstein distance. The same argument also applies to
JS divergence. See supplementary material for the formal
proof.

Dreal, ˆ

Dreal,

DG)

⇡

Notice,
this result does not contradict the experiments
of (Arjovsky et al., 2017) since they actually use not
Wasserstein distance but a surrogate distance that does gen-
eralize, as we show next.

3.3. Generalization Bounds for Neural Net Distance

Dreal and

DG is the
Which distance measure between
GAN objective actually minimizing and can we analyze
its generalization performance? Towards answering these
questions in full generality (given multiple GANs objec-
tives) we consider the following general distance measure
that uniﬁes JS divergence, Wasserstein distance, and the
neural net distance that we deﬁne later in this section.

F

-distance). Let

Deﬁnition 2 (
be a class of functions
from Rd to [0, 1] and   be a concave measuring function.
-divergence with respect to   between two dis-
Then the
tributions µ and ⌫ supported on Rd is deﬁned as

F

F

d

, (µ, ⌫) = sup

F

[ (D(x))] + E
⌫
⇠

x

[ (1

D(x))]

 

E
µ
x
2F  
⇠
 
2 (1/2)
 
 

D

 

 
 
 
 

,  is a distance function 4
When  (t) = t, we have that d
, and with slightly abuse of notation we write it simply as

F

E
µ
x
⇠

 

E
⌫
x
⇠

d

(µ, ⌫) = sup

[D(x)]

[D(x)]

.

F

D

2F  
 
 
Example 1. When  (t) = log(t) and
 
all functions from Rd to [0, 1]
, we have that d
{
}
F
same as JS divergence. When  (t) = t and
all 1-Lipschitz functions from Rd to [0, 1]
, then d
}
{
the Wasserstein distance.

 
 
 
=
 
F
,  is the
=
,  is

F
F

F

F

F

F

F

F
, ( ˆ

Dreal, ˆ
( ˆ

Example 2. Suppose
is a set of neural networks and
 (t) = log t, then original GAN objective function is
Dreal, ˆ
equivalent to minG d

DG) .
is the set of neural networks, and  (t) = t, then
Suppose
the objective function used empirically in (Arjovsky et al.,
2017) is equivalent to minG d

DG) .
GANs training uses
to be a class of neural nets with a
bound p on the number of parameters. We then informally
refer to d
as the neural net distance. The next theorem
establishes generalization in the sense of equation (3) does
hold for it (with a uniform convergence) . We assume that
 ,  ] and that it
the measuring function takes values in [
is L -Lipschitz. Further,
Dv, v
is the class
of discriminators that is L-Lipschitz with respect to the pa-
rameters v. As usual, we use p to denote the number of
parameters in v.
Theorem 3.1. In the setting of previous paragraph, let µ, ⌫
be two distributions and ˆµ, ˆ⌫ be empirical versions with at
least m samples each. There is a universal constant c such
that when m
, we have with probability
at least 1

p) over the randomness of ˆµ and ˆ⌫,

cp 2 log(LL p/✏)
✏2

 
2V}

 
exp(

=

F

{

 

 
, (ˆµ, ˆ⌫)

d

|

F

d

, (µ, ⌫)

✏.

|

 

F

See supplementary material for the proof. The intuition is
that there aren’t too many distinct discriminators, and thus
given enough samples the expectation over the empirical
distribution converges to the expectation over the true dis-
tribution for all discriminators.

Theorem 3.1 shows that the neural network divergence (and
neural network distance) has a much better generalization
properties than Jensen-Shannon divergence or Wasserstein
distance.
If the GAN successfully minimized the neural
network divergence between the empirical distributions,
that is, d( ˆ
DG), then we know the neural network
divergence d(
DG) between the distributions
Dreal
and
DG is also small. It is possible to change the proof to
also show that this generalization continues to hold at every
iteration of the training. See supplementary material.

Dreal, ˆ
Dreal,

4Technically it is a pseudometric. This is also known as inte-

gral probability metrics(M¨uller, 1997).

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

3.4. Generalization vs Diversity

Since the ﬁnal goal of GANs training is to learn a distri-
bution, it is worth understanding that though weak general-
ization in the sense of Section 3.3 is guaranteed, it comes
with a cost (albeit a necessary one). For JS divergence and
Wasserstein distance, when the distance between two dis-
tributions µ, ⌫ is small, it is safe to conclude that the distri-
butions µ and ⌫ are almost the same. However, the neural
net distance dN N (µ, ⌫) can be small even if µ, ⌫ are not
very close. As a simple Corollary of Lemma 3.1, we ob-
tain:

Corollary 3.1 (Low-capacity discriminators cannot detect
lack of diversity). Let ˆµ be the empirical version of distri-
bution µ with m samples. There is a some universal con-
stant c such that when m
, we have that
with probability at least 1

cp 2 log(LL p/✏)
✏2
p), d

, (µ, ˆµ)

exp(

✏.

 

F



 
 

That is, the neural network distance for nets with p parame-
ters cannot distinguish between a distribution µ and a distri-
bution with support ˜O(p/✏2). In fact the proof still works
if the disriminator is allowed to take many more samples
from µ; the reason they don’t help is that its capacity is
limited to p.

4. Expressive Power and Equilibrium

D

and

Section 3 clariﬁed the notion of generalization for GANs:
namely, neural-net divergence between the generated dis-
Dreal on the empirical samples closely
tribution
tracks the divergence on the full distribution (i.e., unseen
samples). But this doesn’t explain why in practice the gen-
erator usually “wins” so that the discriminator is unable to
do much better than random guessing at the end. In other
words, was it sheer luck that so many real-life distributions
Dreal turned out to be close in neural-net distance to a dis-
tribution produced by a fairly compact neural net? This
section suggests no luck may be needed.

The explanation starts with a thought experiment. Imag-
ine allowing a much more powerful generator, namely, an
inﬁnite mixture of deep nets, each of size p. So long as
the deep net class is capable of generating simple gaus-
sians, such mixtures are quite powerful, since a classical
result says that an inﬁnite mixtures of simple gaussians can
closely approximate
Dreal. Thus an inﬁnite mixture of
deep net generators will “win” the GAN game, not only
against a discriminator that is a small deep net but also
against more powerful discriminators (e.g., any Lipschitz
function).

The next stage in the thought experiment is to imagine a
much less powerful generator, which is a mix of only a few
deep nets, not inﬁnitely many. Simple counterexamples
will not closely approxi-
show that now the distribution

D

Dreal with respect to natural metrics like `p.
mate arbitrary
Nevertheless, could the generator still win the GAN game
against a deep net of bounded capacity (i.e., the deep net is
unable to distinguish

and

Dreal)? We show it can.

D

INFORMAL THEOREM: If the discriminator is a deep net
with p parameters, then a mixture of ˜O(p log(p/✏)/✏2) gen-
that the discrim-
erator nets can produce a distribution
D
inator will be unable to distinguish from
Dreal with prob-
ability more than ✏. (Here ˜O(
) notation hides some nui-
sance factors.)

·

This informal theorem is also a component of our re-
sult below about the existence of an approximate pure
equilibrium. With current technique this existence result
seems sensitive to the measuring function  , and works for
 (x) = x (i.e., Wasserstein GAN). For other   we only
show existence of mixed equilibria with small mixtures.

4.1. General  : Mixed Equilibrium

For general measuring function   we can only show the
existence of a mixed equilibrium, where we allow the dis-
criminator and generator to be ﬁnite mixtures of deep nets.

For a class of generators
criminators
of the game between generator and discriminator

and a class of dis-
{
, we can deﬁne the payoff F (u, v)

Dv, v

2U }

2V}

Gu, u

{

F (u, v) = E

[ (Dv(x))] + E

[ (1

Dv(x)))].

(4)

x

⇠Dreal

x

⇠DG

 

Of course as we discussed in the previous section, in prac-
tice these expectations should be with respect to the em-
pirical distributions. Our discussions in this section does
not depend on the distributions
Dh, so we deﬁne
F (u, v) this way for simplicity.

Dreal and

The well-known min-max theorem (v. Neumann, 1928) in
game theory shows if both players are allowed to play
mixed strategies then the game has a min-max solution. A
mixed strategy for the generator is just a distribution
Du
supported on
, and one for discriminator is a distribution
U
Dv supported on
Theorem 4.1
V ,
v, Eu

V
(vonNeumann). There
and a pair of mixed strategies

exists
Su,Sv)
(
⇠Sv [F (u, v)]
 

⇠Su [F (u, v)]

value
s.t.
V.

u, Ev

V ,



8

8

.

Su,

Note that this equilibrium involves both parties announc-
ing their strategies
Sv at the start, such that neither will
have any incentive to change their strategy after studying
the opponent’s strategy. The payoff is generated by the gen-
erator ﬁrst sample u
⇠D h, and then generate an
example x = Gu(h). Therefore, the mixed generator is just
a linear mixture of generators. The discriminator will ﬁrst
⇠S v, and then output Dv(x). Note that in gen-
sample v
eral this is very different from a discriminator D that out-
⇠Sv [Dv(x)], because the measuring function   is in
puts Ev

⇠S u, h

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

general nonlinear. In particular, the correct payoff function
for a mixture of discriminator is:

[F (u, v)]

E
v
⇠Sv
= E
x
⇠Dreal
v
⇠Sv

[ (Dv(x))] + E

[ (1

Dv(Gu(h)))].

h
⇠Dh
v
⇠Sv

 

Of course, this equilibrium involving an inﬁnite mixture
makes little sense in practice. We show that (as is folk-
lore in game theory (Lipton & Young, 1994)) that we can
approximate this min-max solution with mixture of ﬁnitely
many generators and discriminators. More precisely we
deﬁne ✏-approximate equilibrium:

Su,
Deﬁnition 3. A pair of mixed strategies (
Sv) is an ✏-
approximate equilibrium, if for some value V
v
,
8
2V
⇠Sv [F (u, v)]
V + ✏;
Eu
Su,
V
pair is called an ✏-approximate pure equilibrium.

8
 
Sv are pure strategies, then this

⇠Su [F (u, v)]
 

✏. If the strategies

, Ev

2U



u

Suppose   is L -Lipschitz and bounded in [
 ,  ], the
generator and discriminators are L-Lipschitz with respect
to the parameters and L0-Lipschitz with respect to inputs, in
this setting we can formalize the above Informal Theorem
as follows:

 

✏2

Theorem 4.2. In the settings above, there is a univer-
sal constant C > 0 such that for any ✏,
there exists
T = C 2p log(LL0L ·
p/✏)
generators Gu1 , . . . , GuT and T
discriminators Dv1 , . . . , DvT , let
Su be a uniform distri-
Sv be a uniform distribution on vi, then
bution on ui and
(
Sv) is an ✏-approximate equilibrium. Furthermore, in
Su,
this equilibrium the generator “wins,” meaning discrimi-
nators cannot do better than random guessing.

The proof uses a standard probabilistic argument and ep-
silon net argument to show that if we sample T generators
and discriminators from inﬁnite mixture, they form an ap-
proximate equilibrium with high probability. For the sec-
ond part, we use the fact that every distribution can be ap-
proximated by inﬁnite mixture of Gaussians, so the gener-
ator must be able to approximate the real distribution
Dreal
and win. Therefore indeed a mixture of ˜O(p) generators
can achieve an ✏-approximate equilibrium. See supplemen-
tary material for details.

In the special case of  (x) = x (Wasserstein GAN), we
show that a mixture of generator/discriminator is equiva-
lent to a specially designed, larger generator/discriminator,
therefore an approximate pure equilibrium exists. See sup-
plementary material for more details.

Theorem 4.3. Suppose the generator and discriminator
are both k-layer neural networks (k
2) with p param-
eters, and the last layer uses ReLU activation function. In
the setting of Theorem 4.2, when  (x) = x there exists

 

 2p2 log(LL0L ·

k + 1-layer neural networks of generators G and discrim-
inator D with O
parameters, such
✏2
that there exists an ✏-approximate pure equilibrium. Fur-
thermore, if the generator is capable of generating a Gaus-
sian then the value V = 1.

p/✏)

⇣

⌘

5. MIX+GANs

Theorem 4.2 show that using a mixture of (not too many)
generators and discriminators guarantees existence of ap-
proximate equilibrium. This suggests that using a mixture
may lead to more stable training.

}


2

and T discriminators

Of course, it is impractical to use very large mixtures, so
we propose MIX+GAN: use a mixture of T components,
where T is as large as allowed by size of GPU memory
5). Namely, train a mixture of T genera-
(usually T
[T ]
Gui , i
)
tors
}
{
which share the same network architecture but have their
own trainable parameters. Maintaining a mixture means
of course maintaining a weight wui for the generator Gui
which corresponds to the probability of selecting the output
of Gui . These weights are also updated via backpropaga-
tion. This heuristic can be combined with existing meth-
ods like DCGAN (Radford et al., 2016), WASSERSTEIN-
GAN (Arjovsky et al., 2017) etc., giving us new training
methods MIX+DCGAN, MIX+WASSERSTEINGAN etc.

Dvi , i
{

[T ]

2

We use exponentiated gradient (Kivinen & Warmuth,
1997): store the log-probabilities
, and then
}
obtain the weights by applying soft-max function on them:
wui =

↵ui , i
{

[T ].

[T ]

2

i

e↵ui
k=1 e↵uk ,
T

2

P

Note that our algorithm is maintaining weights on different
generators and discriminators. This is very different from
the idea of boosting where weights are maintained on sam-
ples. ADAGAN (Tolstikhin et al., 2017) uses ideas similar
to boosting and maintains weights on training examples.

Given payoff function F , training MIX+GAN boils down
to optimizing:

min
ui}
↵ui }
,
{
{
= min
ui}

{

{

,

vj }

{

max
,
↵vj }
{
max
,
{

vj }

{

↵ui }

F (ui, vj)

i,j

[T ]

E
2

wui wvj F (ui, vj).

↵vj }

[T ]
Xi,j
2

Here the payoff function is the same as Equation (4). We
use both measuring functions  (x) = log x (for origi-
nal GAN) and  (x) = x (for WASSERSTEINGAN). In
our experiments we alternatively update generators’ and
discriminators’ parameters as well as their corresponding
log-probabilities using ADAM (Kingma & Ba, 2015), with
learning rate lr = 0.0001.

Empirically, it is observed that some components of the
mixture tend to collapse and their weights diminish during

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

the training. To encourage full use of the mixture capac-
ity, we add to the training objective an entropy regularizer
that discourages the weights being far away from uniform:
T
i=1(log(wui ) + log(wvi )).
Rent(

) =

1
T

,
wui }
{

wvi }
{

 

6. Experiments

P

In this section, we ﬁrst explore the qualitative bene-
ﬁts of our method on image generation tasks: MNIST
dataset (LeCun et al., 1998) of hand-written digits and the
CelebA (Liu et al., 2015) dataset of human faces. Then
for more quantitative evaluation we use the CIFAR-10
dataset (Krizhevsky & Hinton, 2009) and use the Inception
Score introduced in (Salimans et al., 2016). MNIST con-
28-sized images of hand-written
tains 60,000 labeled 28
108-sized images
digits, CelebA contains over 200K 108
64 pixels for our
of human faces (we crop the center 64
32-
experiments), and CIFAR-10 has 60,000 labeled 32
⇥
sized RGB natural images which fall into 10 categories.

⇥
⇥

⇥

To reinforce the point that this technique works out of the
box, no extensive hyper-parameter search or tuning is nec-
essary. Please refer to our code for experimental setup. 5

6.1. Qualitative Results

The DCGAN architecture (Radford et al., 2016) uses deep
convolutional nets as generators and discriminators. We
trained MIX+DCGAN on MNIST and CelebA using the
authors’ code as a black box, and compared visual qualities
of generated images to those by DCGAN.

Results on MNIST is shown in Figure 2. In this experi-
ment, the baseline DCGAN consists of a pair of a genera-
tor and a discriminator, which are 5-layer deconvoluitonal
neural networks, and are conditioned on image labels. Our
MIX+DCGAN model consists of a mixture of such DC-
GANs so that it has 3 generators and 3 discriminators. We
observe that our method produces somewhat cleaner digits
than the baseline (note the fuzziness in the latter).

Results on CelebA dataset are also in Figure 2, using the
same architecture as for MNIST, except the models are not
conditioned on image labels anymore. Again, our method
generates more faithful and more diverse samples than the
baseline. Note that one may need to zoom in to fully per-
ceive the difference, since both the two datasets are rather
easy for DCGAN.

6.2. Quantitative Results

Now we turn to quantitative measurement using Inception
Score. Our method is applied to DCGAN and WASSER-
STEINGAN (Arjovsky et al., 2017), and throughout, mix-

5Related code is public online at https://github.com/

PrincetonML/MIX-plus-GANs.git

Figure 2. MNIST and CelebA Samples. Digits and Faces gener-
ated from (a) MIX+DCGAN. (b) DCGAN.

tures of 5 generators and 5 discriminators are used. At ﬁrst
sight the comparison DCGAN v.s. MIX+DCGAN seems
unfair because the latter uses 5 times the capacity of the for-
mer, with corresponding penalty in running time per epoch.
To address this, we also compare our method with larger
versions of DCGAN with roughly the same number of pa-
rameters, and we found the former is consistently better
than the later, as detailed below.

To construct MIX+DCGAN, we build on top of the DC-
GAN trained with losses proposed by Huang et al. (2017),
which is the best variant so far without improved train-
ing techniques. The same hyper-parameters are used for
fair comparison. See (Huang et al., 2017) for more de-
tails. Similarly, for the MIX+WASSERSTEINGAN, the
base GAN is identical to that proposed by Arjovsky et al.
(2017) using their hyper-parameter scheme.

For a quantitative comparison, inception score is calculated
for each model, using 50,000 freshly generated samples
that are not used in training. To sample a single image from
our MIX+ models, we ﬁrst select a generator from the mix-
wui }
ture according to their assigned weights
, and then
{
draw a sample from the selected generator.

Table 1 shows the results on the CIFAR-10 dataset. We ﬁnd
that, simply by applying our method to the baseline mod-
els, our MIX+ models achieve 7.72 v.s. 7.16 on DCGAN,
and 4.04 v.s. 3.82 on WASSERSTEINGAN. To conﬁrm that
the superiority of MIX+ models is not solely due to more
parameters, we also tested a DCGAN model with 5 times
many parameters (roughly the same number of parameters

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

Table 1. Inception Scores on CIFAR-10. Mixture of DCGANs
achieves higher score than any single-component DCGAN does.
All models except for WASSERSTEINGAN variants are trained
with labels.

Method
SteinGAN (Wang & Liu, 2016)
Improved GAN (Salimans et al., 2016)
AC-GAN (Odena et al., 2016)
S-GAN (best variant in (Huang et al., 2017))
DCGAN (as reported in (Wang & Liu, 2016))
DCGAN (best variant in (Huang et al., 2017))
DCGAN (5x size)
MIX+DCGAN (with 5 components)
WASSERSTEINGAN
MIX+WASSERSTEINGAN (with 5 components)
Real data

±
±
±

Score
6.35
0.07
0.07
0.12
7.37
0.10
0.07
0.09
0.06
0.07
0.12

±
±
±
±
±
±

8.09

8.25
8.59

7.16
7.34
7.72
3.82
4.04
11.24

Figure 3. Training Curve of MIX+DCGAN v.s. DCGAN (In-
ception Score). MIX+DCGAN is consistently higher than DC-
GAN.

as a 5-component MIX+DCGAN), which is tuned using a
grid search over 27 sets of hyper-parameters (learning rates,
dropout rates, and regularization weights). It gets only 7.34
(labeled as ”5x size” in Table 1), which is lower than that
of a MIX+DCGAN. It is unclear how to apply MIX+ to
S-GANs. We tried mixtures of the upper and bottom gen-
erators separately, resulting in worse scores somehow. We
leave that for future exploration.

Figure 3 shows how Inception Scores evolve during train-
ing. MIX+DCGAN outperforms DCGAN throughout the
entire training process, showing that it makes effective use
of the capacity.

Arjovsky et al. (2017) shows that (approximated) Wasser-
stein loss, which is the neural network divergence by
our deﬁnition, is meaningful because it correlates well
with visual quality of generated samples.
Figure 4
shows the training dynamics of neural network divergence
of MIX+WASSERSTEINGAN v.s. WASSERSTEINGAN,
which clearly indicates our method is capable of achiev-
ing a much lower divergence as well as of improving the
visual quality of generated samples.

Figure 4. Training Curve
v.s. WASSERSTEINGAN
MIX+WASSERSTEINGAN is better
loss drops less smoothly, which needs further investigation.

of MIX+WASSERSTEINGAN
Objective).
the end but

(Wasserstein
towards

7. Conclusions

The notion of generalization for GANs has been clariﬁed
by introducing a new notion of distance between distribu-
tions, the neural net distance. (Whereas popular distances
such as Wasserstein and JS may not generalize.) Assum-
ing the visual cortex also is a deep net (or some network of
moderate capacity) generalization with respect to this met-
ric is in principle sufﬁcient to make the ﬁnal samples look
realistic to humans, even if the GAN doesn’t actually learn
the true distribution.

One issue raised by our analysis is that the current GANs
objectives cannot even enforce that the synthetic distribu-
tion has high diversity (Section 3.4). Furthermore this can-
not be ﬁxed by simply providing the discriminator with
more training examples. Possibly some other change to the
GANs setup are needed.

The paper also made progress another unexplained issue
about GANs, by showing that a pure approximate equilib-
rium exists for a certain natural training objective (Wasser-
stein) and in which the generator wins the game. No as-
sumption about the target distribution

Dreal is needed.

Suspecting that a pure equilibrium may not exist for all ob-
jectives, we recommend in practice our MIX+GAN proto-
col using a small mixture of discriminators and generators.
Our experiments show it improves the quality of several
existing GAN training methods.

Finally, existence of an equilibrium does not imply that a
simple algorithm (in this case, backpropagation) would ﬁnd
it easily. Understanding convergence remains wide open.

Acknowledgements

This paper was done in part while the authors were hosted
by Simons Institute. We thank Moritz Hardt, Kunal Talwar,
Luca Trevisan, and the referees for useful comments. This
research was supported by NSF, Ofﬁce of Naval Research,
and the Simons Foundation.

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

M¨uller, Alfred. Integral probability metrics and their gen-
erating classes of functions. Advances in Applied Prob-
ability, 29(02):429–443, 1997.

Odena, Augustus, Olah, Christopher, and Shlens, Jonathon.
Conditional image synthesis with auxiliary classiﬁer
gans. arXiv preprint arXiv:1610.09585, 2016.

Radford, Alec, Metz, Luke, and Chintala, Soumith. Unsu-
pervised representation learning with deep convolutional
generative adversarial networks. In International Con-
ference on Learning Representations, 2016.

Salimans, Tim, Goodfellow, Ian, Zaremba, Wojciech, Che-
ung, Vicki, Radford, Alec, and Chen, Xi. Improved tech-
niques for training gans. In Advances in Neural Informa-
tion Processing Systems, 2016.

Tolstikhin, Ilya, Gelly, Sylvain, Bousquet, Olivier, Simon-
Gabriel, Carl-Johann, and Sch¨olkopf, Bernhard. Ada-
arXiv preprint
gan: Boosting generative models.
arXiv:1701.02386, 2017.

v. Neumann, J. Zur theorie der gesellschaftsspiele. Mathe-

matische annalen, 100(1):295–320, 1928.

Wang, Dilin and Liu, Qiang. Learning to draw samples:
With application to amortized mle for generative adver-
sarial learning. Technical report, 2016.

References

Abadi, Mart´ın and Andersen, David G. Learning to pro-
tect communications with adversarial neural cryptogra-
phy. arXiv preprint arXiv:1610.06918, 2016.

Arjovsky, Martin, Chintala, Soumith, and Bottou, L´eon.
arXiv preprint arXiv:1701.07875,

Wasserstein gan.
2017.

Durugkar, I., Gemp, I., and Mahadevan, S. Generative
Multi-Adversarial Networks. ArXiv e-prints, November
2016.

Ghosh, Jayanta K, Ghosh, RVJK, and Ramamoorthi, RV.

Bayesian nonparametrics. Technical report, 2003.

Goodfellow, Ian. Nips 2016 tutorial: Generative adversar-
ial networks. arXiv preprint arXiv:1701.00160, 2016.

Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,
Bing, Warde-Farley, David, Ozair, Sherjil, Courville,
Aaron, and Bengio, Yoshua. Generative adversarial nets.
In Advances in neural information processing systems,
pp. 2672–2680, 2014.

Huang, Xun, Li, Yixuan, Poursaeed, Omid, Hopcroft, John,
and Belongie, Serge. Stacked generative adversarial
networks. In Computer Vision and Patter Recognition,
2017.

Jiwoong Im, D., Ma, H., Dongjoo Kim, C., and Taylor, G.
Generative Adversarial Parallelization. ArXiv e-prints,
December 2016.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. In International Conference on
Learning Representations, 2015.

Kivinen, Jyrki and Warmuth, Manfred K. Exponentiated
gradient versus gradient descent for linear predictors. In-
formation and Computation, 132(1):1–63, 1997.

Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple
layers of features from tiny images. Technical report,
2009.

LeCun, Yann, Cortes, Corinna, and Burges, Christo-
pher JC. The mnist database of handwritten digits, 1998.

Lipton, Richard J and Young, Neal E. Simple strategies for
large zero-sum games with applications to complexity
theory. In Proceedings of the twenty-sixth annual ACM
symposium on Theory of computing, pp. 734–740. ACM,
1994.

Liu, Ziwei, Luo, Ping, Wang, Xiaogang, and Tang, Xiaoou.
Deep learning face attributes in the wild. In Proceedings
of the IEEE International Conference on Computer Vi-
sion, pp. 3730–3738, 2015.

