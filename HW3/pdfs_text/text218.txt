No Spurious Local Minima in Nonconvex Low Rank Problems:
A Uniﬁed Geometric Analysis

Rong Ge 1 Chi Jin 2 Yi Zheng 1

Abstract
In this paper we develop a new framework that
captures the common landscape underlying the
common non-convex low-rank matrix problems
including matrix sensing, matrix completion and
robust PCA. In particular, we show for all above
problems (including asymmetric cases): 1) all lo-
cal minima are also globally optimal; 2) no high-
order saddle points exists. These results explain
why simple algorithms such as stochastic gradi-
ent descent have global converge, and efﬁciently
optimize these non-convex objective functions in
practice. Our framework connects and simpliﬁes
the existing analyses on optimization landscapes
for matrix sensing and symmetric matrix com-
pletion. The framework naturally leads to new
results for asymmetric matrix completion and ro-
bust PCA.

1. Introduction

Non-convex optimization is one of the most powerful tools
in machine learning. Many popular approaches, from tra-
ditional ones such as matrix factorization (Hotelling, 1933)
to modern deep learning (Bengio, 2009) rely on optimiz-
ing non-convex functions. In practice, these functions are
optimized using simple algorithms such as alternating min-
imization or gradient descent. Why such simple algorithms
work is still a mystery for many important problems.

One way to understand the success of non-convex opti-
mization is to study the optimization landscape: for the ob-
jective function, where are the possible locations of global
optima, local optima and saddle points. Recently, a line
of works showed that several natural problems including
tensor decomposition (Ge et al., 2015), dictionary learn-
ing (Sun et al., 2015a), matrix sensing (Bhojanapalli et al.,

Authors listed alphabetically. 1Duke University, Durham NC
2UC Berkeley, Berkeley CA. Correspondence to: Rong Ge
<rongge@cs.duke.edu>, Chi Jin <chijin@cs.berkeley.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

2016; Park et al., 2016) and matrix completion (Ge et al.,
2016) have well-behaved optimization landscape: all local
optima are also globally optimal. Combined with recent
results (e.g. Ge et al. (2015); Carmon et al. (2016); Agar-
wal et al. (2016); Jin et al. (2017)) that are guaranteed to
ﬁnd a local minimum for many non-convex functions, such
problems can be efﬁciently solved by basic optimization
algorithms such as stochastic gradient descent.

In this paper we focus on optimization problems that look
for low rank matrices using partial or corrupted obser-
vations. Such problems are studied extensively (Fazel,
2002; Rennie & Srebro, 2005; Cand`es & Recht, 2009) and
has many applications in recommendation systems (Koren,
2009), see survey by Davenport & Romberg (2016). These
optimization problems can be formalized as follows:

f (M),

(1)

min
M∈Rd1×d2
s.t.

rank(M) = r.

Here M is an d1 × d2 matrix and f is a convex function
of M. The non-convexity of this problem stems from the
low rank constraint. Several interesting problems, such
as matrix sensing (Recht et al., 2010), matrix completion
(Cand`es & Recht, 2009) and robust PCA (Cand`es et al.,
2011) can all be framed as optimization problems of this
form(see Section 3).

In practice, Burer & Monteiro (2003) heuristic is often
used – replace M with an explicit low rank representation
M = UV(cid:62), where U ∈ Rd1×r and V ∈ Rd2×r. The new
optimization problem becomes

min
U∈Rd1×r,V∈Rd2×r

f (UV(cid:62)) + Q(U, V).

(2)

Here Q(U, V) is a (optional) regularizer. Despite the ob-
jective being non-convex, for all the problems mentioned
above, simple iterative updates from random or even arbi-
trary initial point ﬁnd the optimal solution in practice. It is
then natural to ask: Can we characterize the similarities
between the optimization landscape of these problems?
We show this is indeed possible:

Theorem 1 (informal). The objective function of matrix
sensing, matrix completion and robust PCA have similar

No Spurious Local Minima in Nonconvex Low Rank Problems

optimization landscape. In particular, for all these prob-
lems, 1) all local minima are also globally optimal; 2) any
saddle point has at least one strictly negative eigenvalue in
its Hessian.

More precise theorem statements appear in Section 3. Note
that there were several cases (matrix sensing (Bhojanapalli
et al., 2016; Park et al., 2016), symmetric matrix comple-
tion (Ge et al., 2016)) where similar results on the opti-
mization landscape were known. However the techniques
in previous works are tailored to the speciﬁc problems and
hard to generalize. Our framework captures and simpli-
ﬁes all these previous results, and also gives new results on
asymmetric matrix completion and robust PCA.

The key observation in our analysis is that for matrix
sensing, matrix completion, and robust PCA (when ﬁxing
sparse estimate), function f (in Equation (1)) is a quadratic
function over the matrix M. Hence the Hessian H of f with
respect to M is a constant. More importantly, the Hessian
H in all above problems has similar properties (that it ap-
proximately preserves norm, similar to the RIP properties
used in matrix sensing (Recht et al., 2010)), which allows
their optimization landscapes to be characterized in a uni-
ﬁed way. Speciﬁcally, our framework gives principled way
of deﬁning a direction of improvement for all points that are
not globally optimal.

Another crucial property of our framework is the interac-
tion between the regularizer and the Hessian H. Intuitively,
the regularizer makes sure the solution is in a nice region B
(e.g. set of incoherent matrices for matrix completion), and
only within B the Hessian has the norm preserving prop-
erty. On the other hand, regularizer should not be too large
to severely distort the landscape. This interaction is crucial
for matrix completion, and is also very useful in handling
noise and perturbations. In Section 4, we discuss ideas re-
quired to apply this framework to matrix sensing, matrix
completion and robust PCA.

Using this framework, we also give a way to reduce
asymmetric matrix problems to symmetric PSD problems
(where the desired matrix is of the form UU(cid:62)). See Sec-
tion 5 for more details.

In addition to the results of no spurious local minima,
our framework also implies that any saddle point has at
least one strictly negative eigenvalue in its Hessian. For-
mally, we proved all above problems satisfy a robust ver-
sion of this claim — strict saddle property (see Deﬁni-
tion 2), which is one of crucial sufﬁcient conditions to ad-
mit efﬁcient optimization algorithms, and thus following
corollary:

Corollary 2 (informal). For matrix sensing, matrix com-
pletion and robust PCA, simple local search algorithms can
ﬁnd the desired low rank matrix UV(cid:62) = M(cid:63) from an ar-

bitrary starting point in polynomial time with high proba-
bility.

Several algorithms, including many variants of gradient de-
scent Ge et al. (2015); Carmon et al. (2016); Agarwal et al.
(2016); Jin et al. (2017) are known to converge to a local
optimum for strict-saddle functions, and hence can be ap-
plied to the problems discussed in this paper. There are
some technicalities in the exact guarantees, which we defer
to supplementary material.

For simplicity, we present most results in the noiseless set-
ting, but our results can also be generalized to handle noise.
See supplementary material for details.

1.1. Related Works

The landscape of low rank matrix problems have recently
received a lot of attention. Ge et al. (2016) showed sym-
metric matrix completion has no spurious local minimum.
At the same time, Bhojanapalli et al. (2016) proved simi-
lar result for symmetric matrix sensing. Park et al. (2016)
extended the matrix sensing result to asymmetric case. All
of these works guarantee global convergence to the correct
solution.

There has been a lot of work on the local convergence
analysis for various algorithms and problems. For ma-
trix sensing or matrix completion, the works (Keshavan
et al., 2010a;b; Hardt & Wootters, 2014; Hardt, 2014; Jain
et al., 2013; Chen & Wainwright, 2015; Sun & Luo, 2015;
Zhao et al., 2015; Zheng & Lafferty, 2016; Tu et al., 2015)
showed that given a good enough initialization, many sim-
ple local search algorithms, including gradient descent and
alternating least squares, succeed. Particularly, several
works (e.g. Sun & Luo (2015); Zheng & Lafferty (2016))
accomplished this by showing a geometric property which
is very similar to strong convexity holds in the neighbor-
hood of optimal solution. For robust PCA, there are also
many analysis for local convergence (Lin et al., 2010; Ne-
trapalli et al., 2014; Yi et al., 2016; Zhang et al., 2017).

Several works also try to unify the analysis for similar prob-
lems. Bhojanapalli et al. (2015) gave a framework for local
analysis for these low rank problems. Belkin et al. (2014)
showed a framework of learning basis functions, which
generalizes tensor decompositions. Their techniques imply
the optimization landscape for all such problems are very
similar. For problems looking for a symmetric PSD matrix,
Li & Tang (2016) showed for objective similar to (2) (but in
the symmetric setting), restricted smoothness/strong con-
vexity on the function f sufﬁces for local analysis. How-
ever, their framework does not address the interaction be-
tween regularizer and the function f , hence cannot be di-
rectly applied to problems such as matrix completion or
robust PCA.

No Spurious Local Minima in Nonconvex Low Rank Problems

Organization We will ﬁrst introduce notations and ba-
sic optimality conditions in Section 2. Then Section 3 in-
troduces the problems and our results. For simplicity, we
present our framework for the symmetric case in Section 4,
and brieﬂy discuss how to reduce asymmetric problem to
symmetric problem in Section 5. For clean presentation,
many proofs are deferred to supplementary material.

2. Preliminaries

In this section we introduce notations and basic optimality
conditions.

2.1. Notations

r be the condition number.

We use bold letters for matrices and vectors. For a vector v
we use (cid:107)v(cid:107) to denote its (cid:96)2 norm. For a matrix M we use
(cid:107)M(cid:107) to denote its spectral norm, and (cid:107)M(cid:107)F to denote its
Frobenius norm. For vectors we use (cid:104)u, v(cid:105) to denote inner-
product, and for matrices we use (cid:104)M, N(cid:105) = (cid:80)
i,j MijNij
to denote the trace of MN(cid:62). We will always use M(cid:63) to
denote the optimal low rank solution. Further, we use σ(cid:63)
1
to denote its largest singular value, σ(cid:63)
r to denote its r-th
singular value and κ(cid:63) = σ(cid:63)

1/σ(cid:63)
We use ∇f to denote the gradient and ∇2f to denote its
Hessian. Since function f can often be applied to both M
(as in (1)) and U, V (as in (2)), we use ∇f (M) to denote
gradient with respect to M and ∇f (U, V) to denote gradi-
ent with respect to U, V. Similar notation is used for Hes-
sian. The Hessian ∇2f (M) is a crucial object in our frame-
work. It can be interpreted as a linear operator on matrices.
This linear operator can be viewed as a d1d2 × d1d2 matrix
(or (cid:0)d+1
(cid:1) matrix in the symmetric case) that ap-
plies to the vectorized version of matrices. We use the nota-
tion M : H : N to denote the quadratic form (cid:104)M, H(N)(cid:105).
Similarly, the Hessian of objective (2) is a linear operator
on a pair of matrices U, V, which we usually denote as
∇2f (U, V).

(cid:1) × (cid:0)d+1

2

2

2.2. Optimality Conditions

Local Optimality Suppose we are optimizing a function
f (x) with no constraints on x. In order for a point x to be
a local minimum, it must satisfy the ﬁrst and second order
necessary conditions. That is, we must have ∇f (x) = 0
and ∇2f (x) (cid:23) 0.
Deﬁnition 1 (Optimality Condition). Suppose x is a local
minimum of f (x), then we have

∇f (x) = 0, ∇2f (x) (cid:23) 0.

saddle property, which is a quantitative version of the op-
timality conditions, and can lead to efﬁcient algorithms to
ﬁnd local minima.

Deﬁnition 2. We say function f (·) is (θ, γ, ζ)-strict sad-
dle. That is, for any x, at least one of followings holds:

1. (cid:107)∇f (x)(cid:107) ≥ θ.
2. λmin(∇2f (x)) ≤ −γ.
3. x is ζ-close to X (cid:63) – the set of local minima.

Intuitively, this deﬁnition says for any point x, it either vi-
olates one of the optimality conditions signiﬁcantly (ﬁrst
two cases), or is close to a local minima. Note that ζ and θ
are often closely related. For a function with strict-saddle
property, it is possible to efﬁciently ﬁnd a point near a local
minimum.

Local vs. Global However, of course ﬁnding a local min-
imum is not sufﬁcient in many case. In this paper we are
also going to prove that all local minima are also globally
optimal, and they correspond to the desired solutions.

3. Low Rank Problems and Our Results

In this section we introduce matrix sensing, matrix comple-
tion and robust PCA. For each problem we give the results
obtained by our framework. The proof ideas are illustrated
later in Sections 4 and 5.

3.1. Matrix Sensing

Matrix sensing (Recht et al., 2010) is a generalization of
compressed sensing (Candes et al., 2006).
In the matrix
sensing problem, there is an unknown low rank matrix
M(cid:63) ∈ Rd1×d2. We make linear observations on this ma-
let A1, A2, ..., Am ∈ Rd1×d2 be m sensing matri-
trix:
ces, the algorithm is given {Ai}’s and the corresponding
bi = (cid:104)Ai, M(cid:63)(cid:105). The goal is now to ﬁnd the unknown ma-
trix M(cid:63). In order to ﬁnd M(cid:63), we need to solve the follow-
ing nonconvex optimization problem

min
M∈Rd1×d2 ,rank(M)=r

f (M) =

1
2m

m
(cid:88)

i=1

((cid:104)M, Ai(cid:105) − bi)2.

We can transform this constraint problem to an uncon-
straint problem by expressing M as M = UV(cid:62) where
U ∈ Rd1×r and V ∈ Rd2×r. We also need an additional
regularizer (common for all asymmetric problems):

Intuitively, if one of these conditions is violated, then it
is possible to ﬁnd a direction that decreases the function
value. (Ge et al., 2015) characterized the following strict-

min
U,V

1
2m

m
(cid:88)

i=1

((cid:104)UV(cid:62), Ai(cid:105) − bi)2 +

(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2
F .

1
8

(3)

No Spurious Local Minima in Nonconvex Low Rank Problems

The regularizer has been widely used in previous works
(Zheng & Lafferty, 2016; Park et al., 2016). In Section 5 we
show how this regularizer can be viewed as a way to deal
with the additional invariants in asymmetric case, and re-
duce the asymmetric case to the symmetric case. A crucial
concept in standard sensing literature is Restrict Isometry
Property (RIP), which is deﬁned as follows:
Deﬁnition 3. A group of sensing matrices {A1, .., Am}
satisﬁes the (r, δ)-RIP condition, if for every matrix M of
rank at most r,

A well-known problem in matrix completion is that when
the true matrix M(cid:63) is very sparse, then we are very likely
to observe only 0 entries, and has no chance to learn the
other entries of M(cid:63). To avoid this case, previous works
have assumed following incoherence condition:
Deﬁnition 4. A rank r matrix M ∈ Rd1×d2 is µ-
incoherent, if for the rank-r SVD XDY(cid:62) of M, we have
for all i ∈ [d1], j ∈ [d2]

(cid:107)e(cid:62)

i X(cid:107) ≤ (cid:112)µr/d2,

(cid:107)e(cid:62)

j Y(cid:107) ≤ (cid:112)µr/d1.

(1 − δ)(cid:107)M(cid:107)2

F ≤

(cid:104)Ai, M(cid:105)2 ≤ (1 + δ)(cid:107)M(cid:107)2
F .

1
m

m
(cid:88)

i=1

(cid:80)m

Intuitively, RIP says operator 1
i=1(cid:104)Ai, ·(cid:105)2 approxi-
m
mately perserve norms for all low rank matrices. When
the sensing matrices are chosen to be i.i.d. matrices with
independent Gaussian entries, if m ≥ c(d1 + d2)r for large
enough constant c, the sensing matrices satisfy the (2r, 1
20 )-
RIP condition (Candes & Plan, 2011). Using our frame-
work we can show:
Theorem 3. When measurements {Ai} satisfy (2r, 1
20 )-
RIP, for matrix sensing objective (3) we have 1) all lo-
cal minima satisfy UV(cid:62) = M(cid:63) 2) the function is
((cid:15), Ω(σ(cid:63)

))-strict saddle.

r ), O( (cid:15)
σ(cid:63)
r

This in particular says 1) no spurious local minima existsl;
2) whenever at some point (U, V) so that the gradient is
small and the Hessian does not have signiﬁcant negative
eigenvalue, then the distance to global optimal (see Deﬁni-
tion 6 and Deﬁnition 7) is guaranteed to be small. Such a
point can be found efﬁciently (see supplementary material).

3.2. Matrix Completion

Matrix completion is a popular technique in recommenda-
tion systems and collaborative ﬁltering (Koren, 2009; Ren-
nie & Srebro, 2005). In this problem, again we have an
unknown low rank matrix M(cid:63). We observe each entry
of the matrix M(cid:63) independently with probability p. Let
Ω ⊂ [d1] × [d2] be a set of observed entries. For any matrix
M, we use MΩ to denote the matrix whose entries outside
of Ω are set to 0. That is, [MΩ]i,j = Mi,j if (i, j) ∈ Ω,
and [MΩ]i,j = 0 otherwise. We further use (cid:107)M(cid:107)Ω to de-
note (cid:107)MΩ(cid:107)F . Matrix completion can be viewed as a spe-
cial case of matrix sensing, where the sensing matrices only
have one nonzero entry. However such matrices do not sat-
isfy the RIP condition.

In order to solve matrix completion, we try to optimize the
following:

min
M∈Rd1×d2 ,rank(M)=r

1
2p

(cid:107)M − M(cid:63)(cid:107)2
Ω.

We assume the unknown optimal low rank matrix M(cid:63) is
µ-incoherent.

(cid:80)d2

In the non-convex program, we try to make sure the
decomposition UV(cid:62) is also incoherent by adding a
+ +
regularizer Q(U, V) = λ1
λ2
+. Here λ1, λ2, α1, α2 are param-
eters that we choose later, (x)+ = max{x, 0}. Using this
regularizer, we can now transform the objective function to
the unconstraint form

i U(cid:107) − α1)4

j V(cid:107) − α2)4

j=1((cid:107)e(cid:62)

i=1((cid:107)e(cid:62)

(cid:80)d1

min
U,V

(cid:107)UV(cid:62) − M(cid:63)(cid:107)2
Ω

1
2p
1
8

+

(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2

F + Q(U, V).

(4)

Using the framework, we can show following:

1 = Θ( µrσ(cid:63)

min{d1,d2} ), choose α2

Theorem 4. Let d = max{d1, d2}, when sample rate p ≥
2 = Θ( µrσ(cid:63)
Ω( µ4r6(κ(cid:63))6 log d
)
and λ1 = Θ( d1
µrκ(cid:63) ). With probability at
least 1 − 1/poly(d), for Objective Function (4) we have 1)
all local minima satisfy UV(cid:62) = M(cid:63) 2) The objective is
((cid:15), Ω(σ(cid:63)

))-strict saddle for polynomially small (cid:15).

µrκ(cid:63) ), λ2 = Θ( d2

), α2

d1

d2

1

1

r ), O( (cid:15)
σ(cid:63)
r

3.3. Robust PCA

Robust PCA (Cand`es et al., 2011) is a generalization to
the standard Principled Component Analysis.
In Robust
PCA, we are given an observation matrix Mo, which is an
true underlying matrix M(cid:63) corrupted by a sparse noise S(cid:63)
(Mo = M(cid:63) + S(cid:63)). In some sense the goal is to decom-
pose the matrix M into these two components. There are
many models on how many entries can be perturbed, and
how they are distributed. In this paper we work in the set-
ting where M(cid:63) is µ-incoherent, and the rows/columns of
S(cid:63) can have at most α-fraction non-zero entries.

In order to express robust PCA as an optimization problem,
we need constraints on both M and S:

min

(cid:107)M + S − Mo(cid:107)2
F .

(5)

1
2

s.t. rank(M) ≤ r, S is sparse.

No Spurious Local Minima in Nonconvex Low Rank Problems

There can be several ways to specify the sparsity of S. In
this paper we restrict attention to the set Sα which is the set
of matrices that have at most α-fraction non-zero entries in
each column/row, and entries have absolute value at most
2 µrσ(cid:63)
1√
d1d2

.

Assuming the true sparse matrix S(cid:63) is in Sα. Note that the
inﬁnite norm requirement on S(cid:63) is without loss of gener-
ality, because by incoherence M(cid:63) cannot have entries with
absolute value more than µrσ(cid:63)
. Any entry larger than that
1√
d1d2
is obviously in the support of S(cid:63) and can be truncated.

In objective function, we allow S to be γ times denser (in
Sγα) where γ is a parameter we choose later. Now the
constraint optimization problem can be tranformed to the
unconstraint problem

min
U,V

f (U, V) +

(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2
F ,

(6)

1
8

min
U∈Rn×r

1
2

f (U, V) := min
S∈Sγα

1
2

(cid:107)UV(cid:62) + S − Mo(cid:107)2
F .

Of course, we can also think of this as a joint minimization
problem of U, V, S. However we choose to present it this
way in order to allow extension of the strict-saddle condi-
tion. Since f (U, V) is not twice-differetiable w.r.t U, V,
it does not admit Hessian matrix, so we use the following
generalized version of strict-saddle

Deﬁnition 5. We say function f (·) is (θ, γ, ζ)-pseudo
strict saddle if for any x, at least one of followings holds:

1. (cid:107)∇f (x)(cid:107) ≥ θ.

2. ∃gx(·) so that ∀y, gx(y) ≥ f (y); gx(x) = f (x);

λmin(∇2gx(x)) ≤ −γ.

3. x is ζ-close to X (cid:63) – the set of local minima.

Note that in this deﬁnition, the upperbound in 2 can be
viewed as similar to the idea of subgradient. For func-
tions with non-differentiable points, subgradient is deﬁned
so that it still offers a lowerbound for the function. In our
case this is very similar – although Hessian is not deﬁned,
we can use a smooth function that upperbounds the current
function (upper-bound is required for minimization). In the
case of robust PCA the upperbound is obtained by a ﬁxed
S. Using this formalization we can prove

Theorem 5. There is an absolute constant c > 0, if γ >
c, and γα · µr · (κ(cid:63))5 ≤ 1
c holds, for objective function
Eq.(6) we have 1) all local minima satisﬁes UV(cid:62) = M(cid:63);
2) objective function is ((cid:15), Ω(σ(cid:63)
))-pseudo strict
saddle for polynomially small (cid:15).

√
r ), O( (cid:15)

σ(cid:63)
r

κ(cid:63)

4. Framework for Symmetric Positive Deﬁnite

Problems

In this section we describe our framework in the simpler
setting where the desired matrix is positive semideﬁnite. In
particular, suppose the true matrix M(cid:63) we are looking for
can be written as M(cid:63) = U(cid:63)(U(cid:63))(cid:62) where U(cid:63) ∈ Rd×r. For
objective functions that is quadratic over M, we denote its
Hessian as H and we can write the objective as

min

M∈Rd×d

sym ,rank(M)=r

1
2

(M − M(cid:63)) : H : (M − M(cid:63)),

(7)

We call this objective function f (M). Via Burer-Monteiro
factorization, the corresponding unconstraint optimization
problem, with regularization Q can be written as

(UU(cid:62)−M(cid:63)) : H : (UU(cid:62)−M(cid:63))+Q(U). (8)

In this section, we also denote f (U) as objective function
with respect to parameter U, abuse the notation of f (M)
previously deﬁned over M.

Direction of Improvement The optimality condition
(Deﬁnition 1) implies if the gradient is non-zero, or if we
can ﬁnd a negative direction of the Hessian (that is a di-
rection v, so that v(cid:62)∇2f (x)v < 0), then the point is not
a local minimum. A common technique in characterizing
the optimization landscape is therefore trying to explicitly
ﬁnd this negative direction. We call this the direction of
improvement. Different works (Bhojanapalli et al., 2016;
Ge et al., 2016) have chosen very different directions of
improvement.

In our framework, we show it sufﬁces to choose a single
direction ∆ as the direction of improvement. Intuitively,
this direction should bring us close to the true solution U(cid:63)
from the current point U. Due to rotational symmetry (U
and UR behave the same for the objective if R is a rotation
matrix), we need to carefully deﬁne the difference between
U and U(cid:63).
Deﬁnition 6. Given matrices U, U(cid:63) ∈ Rd×r, deﬁne their
difference ∆ = U − U(cid:63)R, where R ∈ Rr×r is chosen as
R = argminZ(cid:62)Z=ZZ(cid:62)=I (cid:107)U − U(cid:63)Z(cid:107)2
F .

Note that this deﬁnition tries to “align” U and U(cid:63) before
taking their difference, and therefore is invariant under ro-
tations. In particular, this deﬁnition has the nice property
that as long as M = UU(cid:62) is close to M(cid:63) = U(cid:63)(U(cid:63))(cid:62),
we have ∆ is small (we defer the proof to Appendix):
Lemma 6. Given matrices U, U(cid:63) ∈ Rd×r, let M = UU(cid:62)
and M(cid:63) = U(cid:63)(U(cid:63))(cid:62), and let ∆ be deﬁned as in Deﬁ-
F ≤ 2(cid:107)M − M(cid:63)(cid:107)2
nition 6, then we have (cid:107)∆∆(cid:62)(cid:107)2
F , and
1
r (cid:107)∆(cid:107)2
σ(cid:63)
(cid:107)M − M(cid:63)(cid:107)2
F .
2−1)

F ≤

2(

√

No Spurious Local Minima in Nonconvex Low Rank Problems

Now we can state the main Lemma:

Lemma 7 (Main). For the objective (8), let ∆ be deﬁned as
in Deﬁnition 6 and M = UU(cid:62). Then, for any U ∈ Rd×r,
we have

∆ : ∇2f (U) : ∆ = ∆∆(cid:62) : H : ∆∆(cid:62)

− 3(M − M(cid:63)) : H : (M − M(cid:63))
+ 4(cid:104)∇f (U), ∆(cid:105) + [∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)]
(9)

To see why this lemma is useful, let us look at the simplest
case where Q(U) = 0 and H is identity. In this case, if
gradient is zero, by Eq. (9)

∆ : ∇2f (U) : ∆ = (cid:107)∆∆(cid:107)2

F − 3(cid:107)M − M(cid:63)(cid:107)2
F

By Lemma 6 this is no more than −(cid:107)M − M(cid:63)(cid:107)2
F . There-
fore, all stationary point with M (cid:54)= M∗ must be saddle
points, and we immediately conclude all local minimum
satisﬁes UU(cid:62) = M(cid:63)!

Interaction with Regularizer For problems such as ma-
trix completion, the Hessian H does not preserve the norm
for all low rank matrices. In these cases we need to use
additional regularizer. In particular, conceptually we need
the following steps:

1. Show that the regularizer Q ensures for any U such

that ∇f (U) = 0, U ∈ B for some set B.

2. Show that whenever U ∈ B, the Hessian operator H
behaves similarly as identity: for some c > 0 we have:
∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63)) <
−c(cid:107)∆(cid:107)2
F .

3. Show that the regularizer does not contribute a large
positive term to ∆ : ∇2f (U) : ∆. This means
we show an upperbound for 4(cid:104)∇f (U), ∆(cid:105) + [∆ :
∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)].

Interestingly, these steps are not just useful for handling
regularizers. Any deviation to the original model (such as
noise, or if the optimal matrix is not exactly low rank) can
be viewed as an additional “regularizer” function Q(U)
and argued in the same framework. See supplementary ma-
terial for more details.

4.1. Matrix Sensing

Recall that matrices {Ai : i = 1, 2, ..., m} are known sens-
ing matrices, and bi = (cid:104)Ai, M(cid:63)(cid:105) is the result of i-th ob-
servation. The intended solution is the unknown low rank
matrix M(cid:63) = U(cid:63)(U(cid:63))(cid:62). For any low rank matrix M, the
Hessian operator satisﬁes

M : H : M =

(cid:104)Ai, M(cid:105)2.

m
(cid:88)

i=1

Therefore if the sensing matrices satisfy the RIP property
(Deﬁnition 3), the Hessian operator is close to identity for
all low rank matrices! In the symmetric case there is no
regularizer, so the landscape for symmetric matrix sensing
follows immediately from our main Lemma 7.
Theorem 8. When measurement {Ai} satisﬁes (2r, 1
10 )-
RIP, for matrix sensing objective (10) we have 1) all lo-
cal minima U satisfy UU(cid:62) = M(cid:63); 2) the function is
((cid:15), Ω(σ(cid:63)

))-strict saddle.

r ), O( (cid:15)
σ(cid:63)
r

Proof. For point U with small gradient
(cid:107)∇f (U)(cid:107)F ≤ (cid:15), by (2r, δ2r)-RIP property:

satisfying

∆ : ∇2f (U) : ∆ ≤(1 + δ2r)(cid:107)∆∆(cid:62)(cid:107)2
F

− 3(1 − δ2r)(cid:107)M − M(cid:63)(cid:107)2
≤ − (1 − 5δ2r)(cid:107)M − M(cid:63)(cid:107)2
≤ − 0.4σ(cid:63)
r (cid:107)∆(cid:107)2
F + 4(cid:15)(cid:107)∆(cid:107)F

F + 4(cid:15)(cid:107)∆(cid:107)F
F + 4(cid:15)(cid:107)∆(cid:107)F

inequality is due to Lemma 6 that
The second last
F ≤ 2(cid:107)M − M(cid:63)(cid:107)2
(cid:107)∆∆(cid:62)(cid:107)2
F, and last inequality is due to
δ2r = 1
10 and second part of Lemma 6. This means if U
is not close to U(cid:63), that is, if (cid:107)∆(cid:107)F ≥ 20(cid:15)
, we have ∆ :
σ(cid:63)
r
∇2f (U) : ∆ ≤ −0.2σ(cid:63)
F. This proves ((cid:15), 0.2σ(cid:63)
)-
strict saddle property. Take (cid:15) = 0, we know all stationary
points with (cid:107)∆(cid:107)F (cid:54)= 0 are saddle points. This means all
local minima are global minima (satisfying UU(cid:62) = M(cid:63)),
which ﬁnishes the proof.

r (cid:107)∆(cid:107)2

r , 20(cid:15)
σ(cid:63)
r

4.2. Matrix Completion

For matrix completion, we need to ensure the incoherence
condition (Deﬁnition 4). In order to do that, we add a reg-
ularizer Q(U) that penalize the objective function when
some row of U is too large. We choose the same regu-
larizer as (Ge et al., 2016): Q(U) = λ (cid:80)d
i=1((cid:107)Ui(cid:107) − α)4
+.
The objective is then

min
U∈Rd×r

1
2p

(cid:107)M(cid:63) − UU(cid:62)(cid:107)2

Ω + Q(U).

(11)

Matrix sensing is the ideal setting for this framework. For
symmetric matrix sensing, the objective function is

min
U∈Rd×r

1
2m

m
(cid:88)

i=1

((cid:104)Ai, UU(cid:62)(cid:105) − bi)2.

(10)

Using our framework, we ﬁrst need to show that the regu-
larizer ensures all rows of U are small (step 1).

Lemma 9. There exists an absolute constant c, when sam-
ple rate p ≥ Ω( µr
d ) and λ =

d log d), α2 = Θ( µrσ(cid:63)

1

No Spurious Local Minima in Nonconvex Low Rank Problems

Θ( d
µrκ(cid:63) ), we have for any points U with (cid:107)∇f (U)(cid:107)F ≤ (cid:15)
for polynomially small (cid:15), with probability at least 1 −
1/poly(d):

We assume S(cid:63) ∈ Sα, the objective can be written as

min
U

f (U), wheref (U) := min
S∈Sγα

(cid:107)UU(cid:62) + S − Mo(cid:107)2
F .

1
2

max
i

(cid:107)e(cid:62)

i U(cid:107)2 ≤ O

(cid:18) (µr)1.5κ(cid:63)σ(cid:63)
1
d

(cid:19)

This is a slightly stronger version of Lemma 4.7 in (Ge
et al., 2016). Next we show under this regularizer, we can
still select the direction ∆, and the ﬁrst part of Equation (9)
is signiﬁcantly negative when ∆ is large (step 2):
Lemma 10. When sample rate p ≥ Ω( µ3r4(κ(cid:63))4 log d
), by
d
choosing α2 = Θ( µrσ(cid:63)
d ) and λ = Θ( d
µrκ(cid:63) ) with probabil-
ity at least 1 − 1/poly(d), for all U with (cid:107)∇f (U)(cid:107)F ≤ (cid:15)
for polynomially small (cid:15) we have

1

∆∆(cid:62) : H : ∆∆(cid:62)−3(M−M(cid:63)) : H : (M−M(cid:63)) ≤ −0.3σ(cid:63)

r (cid:107)∆(cid:107)2
F

This lemma follows from several standard concentration in-
equalities, and is made possible because of the incoherence
bound we proved in the previous lemma.

Finally we show the additional regularizer related term in
Equation (9) is bounded (step 3).
Lemma 11. By choosing α2 = Θ( µrσ(cid:63)
O(σ(cid:63)

d ) and λα2 ≤

r ), we have:

1

1
4

[∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)] ≤ 0.1σ(cid:63)

r (cid:107)∆(cid:107)2
F

Combining these three lemmas, it is easy to see
Theorem 12. When sample rate p ≥ Ω( µ3r4(κ(cid:63))4 log d
), by
choosing α2 = Θ( µrσ(cid:63)
µrκ(cid:63) ). Then with
probability at least 1 − 1/poly(d), for matrix completion
objective (11) we have 1) all local minima satisfy UU(cid:62) =
M(cid:63) 2) the function is ((cid:15), Ω(σ(cid:63)
))-strict saddle for
polynomially small (cid:15).

d ) and λ = Θ( d

r ), O( (cid:15)
σ(cid:63)
r

d

1

Notice that our proof is different from (Ge et al., 2016), as
we focus on the direction ∆ for both ﬁrst and second order
conditions while they need to select different directions for
the Hessian. The framework allowed us to get a simpler
proof, generalize to asymmetric case and also improved the
dependencies on rank.

4.3. Robust PCA

In the robust PCA problem, for any given matrix M the
objective function try to ﬁnd the optimal sparse perturba-
tion S.
In the symmetric PSD case, recall we observe
Mo = M(cid:63) +S(cid:63), we deﬁne the set Sα to be the set of matri-
ces whose rows/columns have at most α-fraction nonzero
entries, and entries are bounded by 2 µrσ(cid:63)
d . Note the pro-
jection onto set Sα be computed in polynomial time (using
a max ﬂow algorithm).

1

(12)

Here γ is a slack parameter that we choose later.

Note that now the objective function f (U) is not quadratic,
so we cannot use the framework directly. However, if we
ﬁx S, then fS(U) := 1
F is a quadratic
function with Hessian equal to identity. We can still apply
our framework to this function. In this case, since the Hes-
sian is identity for all matrices, we can skip the ﬁrst step.
The problem becomes a matrix factorization problem:

2 (cid:107)UU(cid:62) + S − Mo(cid:107)2

min
U∈Rd×r

1
2

(cid:107)A − UU(cid:62)(cid:107)2
F .

(13)

The difference here is that the matrix A (which is M(cid:63) +
S(cid:63) − S) is not equal to M(cid:63) and is in general not low rank.
We can use the framework to analyze this problem (and
treat the residue A − M(cid:63) as the “regularizer” Q(U)).
Lemma 13. Let A ∈ Rd×d be a symmetric PSD matrix,
and matrix factorization objective to be:

f (U) = (cid:107)UU(cid:62) − A(cid:107)2
F

where σr(A) ≥ 15σr+1(A). then 1) all local minima sat-
isﬁes UU(cid:62) = Pr(A) (best rank-r approximation), 2) ob-
jective is ((cid:15), Ω(σ(cid:63)

))-strict saddle.

r ), O( (cid:15)
σ(cid:63)
r

To deal with the case S not ﬁxed (but as minimizer
of Eq.(12)), we let U†(U†)(cid:62) be the best
rank r-
approximation of M(cid:63) + S(cid:63) − S. The next lemma shows
when U is close to U† up to some rotation, U will actually
be already close to U(cid:63) up to some rotation.
Lemma 14. There is an absolute constant c, assume γ > c,
and γα · µr · (κ(cid:63))5 ≤ 1
c . Let U†(U†)(cid:62) be the best rank r-
approximation of M(cid:63) +S(cid:63) −S, where S is the minimizer as
in Eq.(12). Assume minR(cid:62)R=RR(cid:62)=I (cid:107)U − U†R(cid:107)F ≤ (cid:15).
κ(cid:63))
Let ∆ be deﬁned as in Deﬁnition 6, then (cid:107)∆(cid:107)F ≤ O((cid:15)
for polynomially small (cid:15).

√

The proof of Lemma 14 is inspired by Yi et al. (2016)
and uses the property of the optimally chosen sparse set
S. Combining these two lemmas we get our main result:
Theorem 15. There is an absolute constant c, if γ > c, and
γα · µr · (κ(cid:63))5 ≤ 1
c holds, for objective function Eq.(12) we
have 1) all local minima satisﬁes UU(cid:62) = M(cid:63); 2) objec-
tive function is ((cid:15), Ω(σ(cid:63)
))-pseudo strict saddle
for polynomially small (cid:15).

√
r ), O( (cid:15)

σ(cid:63)
r

κ(cid:63)

5. Handling Asymmetric Matrices

In this section we show how to reduce problems on asym-
metric matrices to problems on symmetric PSD matrices.

No Spurious Local Minima in Nonconvex Low Rank Problems

Let M(cid:63) = U(cid:63)V(cid:63)(cid:62), and M = UV(cid:62), and objective func-
tion:

Lemma 16. For the objective (16), let ∆, N, N(cid:63) be deﬁned
as in Deﬁnition 7. Then, for any W ∈ R(d1+d2)×r, we have

f (U, V) = 2(M − M(cid:63)) : H0 : (M − M(cid:63))

+

(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2

F + Q0(U, V)

(14)

1
2

Note this is a scaled version of objectives introduced in
Sec.3 (multiplied by 4), and scaling will not change the
property of local minima, global minima and saddle points.

We view the problem as if it is trying to ﬁnd a (d1 + d2) × r
matrix, whose ﬁrst d1 rows are equal to U, and last d2 rows
are equal to V.
Deﬁnition 7. Suppose M(cid:63) is the optimal solution, and
its SVD is X(cid:63)D(cid:63)Y(cid:63)(cid:62). Let U(cid:63) = X(cid:63)(D(cid:63)) 1
2 , V(cid:63) =
Y(cid:63)(D(cid:63)) 1
2 , M = UV(cid:62) is the current point, we reduce the
problem into a symmetric case using following notations.

W =

, W(cid:63) =

, N = WW(cid:62), N(cid:63) = W(cid:63)W(cid:63)(cid:62)

(cid:19)

(cid:18)U
V

(cid:19)

(cid:18)U(cid:63)
V(cid:63)

(15)
Further, ∆ is deﬁned to be the difference between W and
W(cid:63) up to rotation as in Deﬁnition 6.

We will also transform the Hessian operators to operate on
(d1 + d2) × r matrices. In particular, deﬁne Hessian H1, G
such that for all W we have:

N : H1 : N = M : H0 : M

N : G : N = (cid:107)U(cid:62)U − V(cid:62)V(cid:107)2
F

Now, let Q(W) = Q(U, V), and we can rewrite the ob-
jective function f (W) as

1
2

[(N − N(cid:63)) : 4H1 : (N − N(cid:63)) + N : G : N] + Q(W)
(16)

We know H0 perserves the norm of low rank matrices M.
To reduce asymmetric problems to symmetric problem, in-
tuitively, we also hope H0 to approximately preserve the
norm of N. However this is impossible as by deﬁnition,
H0 only acts on M, which is the off-diagonal blocks of N.
We can expect N : H0 : N to be close to the norm of
UV(cid:62), but for all matrices U, V with the same UV(cid:62), the
matrix N can have very different norms. The easiest exam-
ple is to consider U = diag(1/(cid:15), (cid:15)) and V = diag((cid:15), 1/(cid:15)):
while UV(cid:62) = I no matter what (cid:15) is, the norm of N is
of order 1/(cid:15)2 and can change drastically. The regularizer
is exactly there to handle this case: the Hessian G of the
regularizer will be related to the norm of the diagonal com-
ponents, therefore allowing the full Hessian H = 4H1 + G
to still be approximately identity.

Now we can formalize the reduction as the following main
Lemma:

∆ : ∇2f (W) : ∆ ≤ ∆∆(cid:62) : H : ∆∆(cid:62)
− 3(N − N(cid:63)) : H : (N − N(cid:63)) + 4(cid:104)∇f (W), ∆(cid:105)
+ [∆ : ∇2Q(W) : ∆ − 4(cid:104)∇Q(W), ∆(cid:105)]

(17)

where H = 4H1 + G. Further, if H0 satisﬁes M : H0 :
M ∈ (1 ± δ)(cid:107)M(cid:107)2
F for some matrix M = UV(cid:62), let W
and N be deﬁned as in (15), then N : H : N ∈ (1 ±
2δ)(cid:107)N(cid:107)2
F .

Intuitively, this lemma shows the same direction of im-
provement works as before, and the regularizer is exactly
what it requires to maintain the norm-preserving property
of the Hessian.

The proofs are deferred to supplementary material.

6. Conclusions

In this paper we give a framework that explains the recent
success in understanding optimization landscape for low
rank matrix problems. Our framework connects and sim-
pliﬁes the existing proofs, and generalizes to new settings
such as asymmetric matrix completion and robust PCA.
The key observation is when the Hessian operator preserves
the norm of certain matrices, one can use the same direc-
tions of improvement to prove similar optimization land-
scape. We show the regularizer 1
F is
exactly what it requires to maintain this norm preserving
property in the asymmetric case.Our analysis also allows
the interaction between regularizer and Hessian to handle
difﬁcult settings such as.

4 (cid:107)U(cid:62)U − V(cid:62)V(cid:107)2

For low rank matrix problems, there are generalizations
such as weighted matrix factorization(Li et al., 2016) and
1-bit matrix sensing(Davenport et al., 2014) where the Hes-
sian operator may behave differently as the settings we can
analyze. How to characterize the optimization landscape in
these settings is still an open problem.

In order to get general ways of understanding optimization
landscapes for more generally, there are still many open
problems. In particular, how can we decide whether two
problems are similar enough to share the same optimization
landscape? A minimum requirement is that the non-convex
problem should have the same symmetry structure – the set
of equivalent global optimum should be the same. In this
work, we show if the problems come from convex objective
functions with similar Hessian properties, then they have
the same optimization landscape. We hope this serves as a
ﬁrst step towards general tools for understanding optimiza-
tion landscape for groups of problems.

No Spurious Local Minima in Nonconvex Low Rank Problems

References

Agarwal, Naman, Allen-Zhu, Zeyuan, Bullins, Brian,
Hazan, Elad, and Ma, Tengyu. Finding approximate lo-
cal minima for nonconvex optimization in linear time.
arXiv preprint arXiv:1611.01146, 2016.

Belkin, Mikhail, Rademacher, Luis, and Voss, James. Ba-
sis learning as an algorithmic primitive. arXiv preprint
arXiv:1411.1420, 2014.

Bengio, Yoshua. Learning deep architectures for AI. Foun-
dations and trends R(cid:13) in Machine Learning, 2(1):1–127,
2009.

Bhojanapalli, Srinadh, Kyrillidis, Anastasios, and Sang-
havi, Sujay. Dropping convexity for faster semi-deﬁnite
optimization. arXiv:1509.03917, 2015.

Bhojanapalli, Srinadh, Neyshabur, Behnam, and Srebro,
Nathan. Global optimality of local search for low
rank matrix recovery. arXiv preprint arXiv:1605.07221,
2016.

Burer, Samuel and Monteiro, Renato DC. A nonlinear pro-
gramming algorithm for solving semideﬁnite programs
via low-rank factorization. Mathematical Programming,
95(2):329–357, 2003.

Candes, Emmanuel J and Plan, Yaniv. Tight oracle inequal-
ities for low-rank matrix recovery from a minimal num-
ber of noisy random measurements. IEEE Transactions
on Information Theory, 57(4):2342–2359, 2011.

Cand`es, Emmanuel J and Recht, Benjamin. Exact ma-
trix completion via convex optimization. Foundations
of Computational mathematics, 9(6):717–772, 2009.

Candes, Emmanuel J, Romberg, Justin K, and Tao, Ter-
ence. Stable signal recovery from incomplete and in-
accurate measurements. Communications on pure and
applied mathematics, 59(8):1207–1223, 2006.

Cand`es, Emmanuel J, Li, Xiaodong, Ma, Yi, and Wright,
John. Robust principal component analysis? Journal of
the ACM (JACM), 58(3):11, 2011.

Carmon, Yair, Duchi, John C, Hinder, Oliver, and Sidford,
Aaron. Accelerated methods for non-convex optimiza-
tion. arXiv preprint arXiv:1611.00756, 2016.

Chen, Yudong and Wainwright, Martin J.

Fast low-
rank estimation by projected gradient descent: General
statistical and algorithmic guarantees. arXiv preprint
arXiv:1509.03025, 2015.

Davenport, Mark A, Plan, Yaniv, van den Berg, Ewout, and
Wootters, Mary. 1-bit matrix completion. Information
and Inference, 3(3):189–223, 2014.

Fazel, Maryam. Matrix rank minimization with applica-
tions. PhD thesis, PhD thesis, Stanford University, 2002.

Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang. Es-
caping from saddle points—online stochastic gradient
for tensor decomposition. arXiv:1503.02101, 2015.

Ge, Rong, Lee, Jason D, and Ma, Tengyu. Matrix com-
pletion has no spurious local minimum. In Advances in
Neural Information Processing Systems, pp. 2973–2981,
2016.

Hardt, Moritz. Understanding alternating minimization for

matrix completion. In FOCS 2014. IEEE, 2014.

Hardt, Moritz and Wootters, Mary. Fast matrix completion
without the condition number. In COLT 2014, pp. 638–
678, 2014.

Hotelling, Harold. Analysis of a complex of statistical vari-
ables into principal components. Journal of educational
psychology, 24(6):417, 1933.

Jain, Prateek, Netrapalli, Praneeth, and Sanghavi, Su-
jay. Low-rank matrix completion using alternating min-
imization. In Proceedings of the forty-ﬁfth annual ACM
symposium on Theory of computing, pp. 665–674. ACM,
2013.

Jin, Chi, Ge, Rong, Netrapalli, Praneeth, Kakade, Sham M,
and Jordan, Michael I. How to escape saddle points efﬁ-
ciently. arXiv preprint arXiv:1703.00887, 2017.

Keshavan, Raghunandan H, Montanari, Andrea, and Oh,
In-
Sewoong. Matrix completion from a few entries.
formation Theory, IEEE Transactions on, 56(6):2980–
2998, 2010a.

Keshavan, Raghunandan H, Montanari, Andrea, and Oh,
Sewoong. Matrix completion from noisy entries. The
Journal of Machine Learning Research, 11:2057–2078,
2010b.

Koren, Yehuda. The bellkor solution to the netﬂix grand

prize. Netﬂix prize documentation, 81, 2009.

Li, Qiuwei and Tang, Gongguo. The nonconvex geometry
of low-rank matrix optimizations with general objective
functions. arXiv preprint arXiv:1611.03060, 2016.

Davenport, Mark A and Romberg, Justin. An overview of
low-rank matrix recovery from incomplete observations.
IEEE Journal of Selected Topics in Signal Processing,
10(4):608–622, 2016.

Li, Yuanzhi, Liang, Yingyu, and Risteski, Andrej. Re-
covery guarantee of weighted low-rank approxima-
arXiv preprint
tion via alternating minimization.
arXiv:1602.02262, 2016.

No Spurious Local Minima in Nonconvex Low Rank Problems

Lin, Zhouchen, Chen, Minming, and Ma, Yi. The aug-
mented lagrange multiplier method for exact recov-
arXiv preprint
ery of corrupted low-rank matrices.
arXiv:1009.5055, 2010.

Zhao, Tuo, Wang, Zhaoran, and Liu, Han. A nonconvex
optimization framework for low rank matrix estimation.
In Advances in Neural Information Processing Systems,
pp. 559–567, 2015.

Zheng, Qinqing and Lafferty, John. Convergence analysis
for rectangular matrix completion using burer-monteiro
arXiv preprint
factorization and gradient descent.
arXiv:1605.07051, 2016.

Nesterov, Yurii and Polyak, Boris T. Cubic regularization
of Newton method and its global performance. Mathe-
matical Programming, 108(1):177–205, 2006.

Netrapalli, Praneeth, Niranjan, UN, Sanghavi, Sujay,
Anandkumar, Animashree, and Jain, Prateek. Non-
convex robust pca. In Advances in Neural Information
Processing Systems, pp. 1107–1115, 2014.

Park, Dohyung, Kyrillidis, Anastasios, Caramanis, Con-
stantine, and Sanghavi, Sujay.
Non-square matrix
sensing without spurious local minima via the burer-
monteiro approach. arXiv preprint arXiv:1609.03240,
2016.

Recht, Benjamin, Fazel, Maryam, and Parrilo, Pablo A.
Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM review,
52(3):471–501, 2010.

Rennie, Jasson DM and Srebro, Nathan. Fast maximum
margin matrix factorization for collaborative prediction.
In Proceedings of the 22nd international conference on
Machine learning, pp. 713–719. ACM, 2005.

Sun, Ju, Qu, Qing, and Wright, John. Complete dictionary
recovery over the sphere I: Overview and the geometric
picture. arXiv:1511.03607, 2015a.

Sun, Ju, Qu, Qing, and Wright, John. When are nonconvex
problems not scary? arXiv preprint arXiv:1510.06096,
2015b.

Sun, Ruoyu and Luo, Zhi-Quan. Guaranteed matrix com-
In Foundations
pletion via nonconvex factorization.
of Computer Science (FOCS), 2015 IEEE 56th Annual
Symposium on, pp. 270–289. IEEE, 2015.

Tu, Stephen, Boczar, Ross, Soltanolkotabi, Mahdi, and
Recht, Benjamin. Low-rank solutions of linear ma-
arXiv preprint
trix equations via procrustes ﬂow.
arXiv:1507.03566, 2015.

Yi, Xinyang, Park, Dohyung, Chen, Yudong, and Carama-
nis, Constantine. Fast algorithms for robust pca via gra-
In Advances in neural information pro-
dient descent.
cessing systems, pp. 4152–4160, 2016.

Zhang, Xiao, Wang, Lingxiao, and Gu, Quanquan. A non-
convex free lunch for low-rank plus sparse matrix recov-
ery. arXiv preprint arXiv:1702.06525, 2017.

