Differentially Private Submodular Maximization:
Data Summarization in Disguise
(Full version)

Marko Mitrovic, Mark Bun, Andreas Krause, Amin Karbasi

June 12, 2017

Abstract

How can we extract representative features from a dataset containing sensitive personal informa-
tion, while providing individual-level privacy guarantees? Many data summarization applications are
captured by the general framework of submodular maximization. As a consequence, a wide range of
efﬁcient approximation algorithms for submodular maximization have been developed. However, when
such applications involve sensitive data about individuals, their privacy concerns are not automatically
addressed by these algorithms.

To remedy this problem, we propose a general and systematic study of differentially private submod-
ular maximization. We present privacy-preserving algorithms for both monotone and non-monotone
submodular maximization under cardinality, matroid, and p-extendible system constraints, with guar-
antees that are competitive with optimal solutions. Along the way, we analyze a new algorithm for
non-monotone submodular maximization under a cardinality constraint, which is the ﬁrst (even non-
privately) to achieve a constant approximation ratio with a linear number of function evaluations. We
additionally provide two concrete experiments to validate the efﬁcacy of these algorithms. In the ﬁrst
experiment, we privately solve the facility location problem using a dataset of Uber pickup locations
in Manhattan.
In the second experiment, we perform private submodular maximization of a mutual
information measure to select features relevant to classifying patients by diabetes status.

1 Introduction

A set function f : 2V → R is said to be submodular if for all sets S ⊆ T ⊆ V and every element v ∈ V we
have f (S ∪ {v}) − f (S) ≥ f (T ∪ {v}) − f (T ). That is, the marginal contribution of any element v to the
value of the function f (S) diminishes as the input set S increases. The theory of submodular maximization
uniﬁes and generalizes diverse problems in combinatorial optimization, including the Max-Cover, Max-Cut,
and Facility Location problems. In turn, this theory has recently found numerous applications to problems
in machine learning, data science, and artiﬁcial intelligence. A few such applications include exemplar-
based clustering (Krause & Gomes, 2010), feature selection for classiﬁcation (Krause & Guestrin, 2005),
document and corpus summarization (Lin & Bilmes, 2011; Kirchhoff & Bilmes, 2014; Sipos et al., 2012),
crowd teaching (Singla et al., 2014), and inﬂuence maximization in social networks (Kempe et al., 2003).

Some of the most compelling use cases for these applications concern sensitive data about individuals
(Mirzasoleiman et al., 2016a,b). As a running example, let us consider the speciﬁc problem of determining
which of a collection of features (e.g. age, height, weight, etc.) are most relevant to a binary classiﬁcation
task (e.g. predicting whether an individual is likely to have diabetes). In this problem, a sensitive training

1

set takes the form D = {(xi, yi)}n
i=1 where each individual i’s data consists of features xi,1, . . . , xi,m
together with a class label yi. The goal is to identify a small subset S ⊆ [m] of features which can then be
used to build a good classiﬁer for y. Many techniques exist for feature selection, including one based on
maximizing a submodular function which captures the mutual information between a subset of features and
the class label of interest (Krause & Guestrin, 2005). However, for both legal (e.g. compliance with HIPAA
regulations) and ethical reasons, it is important that the selection of relevant features does not compromise
the privacy of any individual who has contributed to the training data set. Unfortunately, the theory of
submodular maximization does not in itself accommodate such privacy concerns.

To this end, we propose a systematic study of differentially private submodular maximization to enable
these applications based on submodular maximization, while provably guaranteeing individual-level privacy.
The notion of differential privacy (Dwork et al., 2006) offers a strong protection of individual-level privacy.
Nevertheless, differential privacy has been shown to permit useful data analysis and machine learning tasks.
In a nutshell, the deﬁnition formalizes a guarantee that no individual’s data should have too signiﬁcant an
effect on the outcome of a computation. We provide the formal deﬁnition in Section 2. Such a privacy
guarantee is obtained through the introduction of random noise, so private submodular maximization is
conceptually related to the problem of submodular maximization in the presence of noise (Cheraghchi,
2012; Hassidim & Singer, 2016).

In this work, we study the following problem under various conditions on the submodular objective
function f (monotone vs. non-monotone), and various choices of the constraint C (cardinality, matroid, or
p-extendible system).

Problem 1.1. Given a sensitive dataset D associated to a submodular function fD : 2V → R: Find a subset
S ∈ C ⊂ 2V that approximately maximizes fD(S) in a manner that guarantees differential privacy with
respect to the input dataset D.

An important special case of this problem was studied in prior work of Gupta et al. (2010). They con-
sidered the “combinatorial public projects” problem (Papadimitriou et al., 2008), where given a dataset
D = (x1, . . . , xn), the function fD takes the particular form fD(S) = 1
i=1 fxi(S) for monotone sub-
n
modular functions fxi : 2V → [0, 1], and is to be maximized subject to a cardinality constraint |S| ≤ k. We
call functions of this form decomposable. They presented a simple greedy algorithm, which will be central
to our work, together with a tailored analysis which achieves strong accuracy guarantees in this special case.
However, there are many cases of Problem 1.1 which do not fall into the combinatorial public projects
framework. For some problems, including feature selection via mutual information, the submodular function
fD of interest depends on the dataset D in ways much more complicated than averaging functions associated
to each individual. The focus of our work is on understanding Problem 1.1 in circumstances which capture
a broader class of useful applications and constraints in machine learning. We summarize our speciﬁc
contributions in Section 1.2.

(cid:80)n

1.1 The greedy paradigm

Even without concern for privacy, the problem of submodular maximization poses computational challenges.
In particular, exact submodular maximization subject to a cardinality constraint is NP-hard. One of the
principal approaches to designing efﬁcient approximation algorithms is to use a greedy strategy (Nemhauser
et al., 1978). Consider the problem of maximizing a set function f (S) subject to the cardinality constraint
|S| ≤ k. In each of rounds i = 1, . . . , k, the basic greedy algorithm constructs Si from Si−1 by adding the
element vi ∈ (V \ Si−1) which maximizes the marginal gain f (Si−1 ∪ {vi}) − f (Si−1). Nemhauser et al.

2

Comb. Pub. Proj.

(cid:0)1 − 1

e

Monotone

Non-monotone

Cardinality
(cid:17)
(cid:16) k log |V |
n

(cid:1) OPT −O
(cid:0)1 − 1
(cid:0)1 − 1

e

1
e

e

(Gupta et al., 2010)
(cid:17)

(cid:1) OPT −O
(cid:1) OPT −O

(cid:16) k3/2 log |V |
n
(cid:16) k3/2 log |V |
n

(cid:17)

Matroid

1
2 OPT −O
1
2 OPT −O

(cid:17)

(cid:16) k log |V |
n
(cid:16) k3/2 log |V |
n

(cid:17)

–

p-Extendible

(cid:17)

(cid:16) k log |V |
n
(cid:16) k3/2 log |V |
n

(cid:17)

1
p+1 OPT −O
1
p+1 OPT −O
–

Table 1: Guarantees of expected solution quality for privately maximizing a sensitivity-(1/n) submodular
function fD. The parameter k represents either a cardinality constraint, or the size of the set returned (for
matroid or p-extendible system constraints). Full expressions with explicit dependencies on differential
privacy parameters ε, δ appear in the body of the paper.

(1978) famously showed that this algorithm yields a (1 − 1/e)-approximation to the optimal value of f (S)
whenever f is a monotone submodular function.

In the combinatorial public projects setting, Gupta et al. (2010) showed how to make the greedy algo-
rithm compatible with differential privacy by randomizing the procedure for selecting each vi. This selection
procedure is speciﬁed by the differentially private exponential mechanism of McSherry & Talwar (2007),
which (probabilistically) guarantees that the vi selected in each round is almost as good as the true marginal
gain maximizer. Remarkably, Gupta et al. (2010) show that the cumulative privacy guarantee of the result-
ing randomized greedy algorithm is not much worse than that of a single run of the exponential mechanism.
This analysis is highly tailored to the structure of the combinatorial public projects problem. However,
replacing this tailored analysis with the more generic “advanced composition theorem” for differential pri-
vacy (Dwork et al., 2010), one still obtains useful results for the more general class of “low-sensitivity”
submodular functions.

1.2 Our contributions

Table 1 summarizes the approximation guarantees we obtain for Problem 1.1 under increasingly more gen-
eral classes of submodular functions fD (read top to bottom), and increasingly more general types of con-
straints (read left to right). In each entry, OPT denotes the value of the optimal non-private solution. Below
we draw attention to a few particular contributions, including some that are not expressed in Table 1.

Non-monotone objective functions. Submodular maximization for non-monotone functions is signiﬁ-
cantly more challenging than it is for monotone objectives. In particular, the basic greedy algorithm of
Nemahauser et al. fails, and cannot guarantee any constant-factor approximation. Several works (Feldman
et al., 2017; Mirzasoleiman et al., 2016a; Buchbinder et al., 2014; Feldman et al., 2011) have identiﬁed vari-
ations of the greedy algorithm that do yield constant-factor approximations for non-monotone objectives.
However, it is not clear how to modify any of these algorithms to accommodate differential privacy.

Our starting point is instead the “stochastic greedy” algorithm of Mirzasoleiman et al. (2015), which
was originally designed to perform monotone submodular maximization in linear time. Drawing ideas
from Buchbinder et al. (2014), we give a new analysis of the stochastic greedy algorithm to show that it also
gives a 1
e (1 − 1/e)-approximation for non-monotone submodular functions. To our knowledge, this is the
ﬁrst algorithm making exactly |V | function evaluations which achieves a constant-factor approximation for
either monotone or non-monotone objectives. Moreover, it is immediately clear how to use the exponential
mechanism to make this algorithm differentially private.

This phenomenon is analogous to how stochastic variants of gradient descent are more naturally suited to

3

providing differential privacy than their deterministic counterparts (Song et al., 2013; Bassily et al., 2014).
That is, our results illustrate how techniques for making algorithms fast are also helpful in making them
privacy-preserving.

General constraints. While a cardinality constraint is perhaps the most natural to place on a submodular
maximization problem, some machine learning problems, e.g. personalized data summarization (Mirza-
soleiman et al., 2016a), require the use of more general types of constraints. For instance, one may wish
to maximize a submodular function f (S) subject to S ∈ I for an arbitrary matroid I, or subject to S be-
ing contained in an intersection of p matroids (more generally, a p-extendible system). For these types of
constraints, the greedy algorithm still yields a constant factor approximation for monotone objective func-
tions (Fisher et al., 1978; Jenkyns, 1976; C˘alinescu et al., 2011). We show in this work that the analysis
provided by C˘alinescu et al. (2011) for matroids and p-extendible families can be adapted to handle addi-
tional error introduced for differential privacy.

General selection procedures. For worst-case datasets, the exponential mechanism is optimal within each
round of private maximization. However, it may be sub-optimal for datasets enjoying additional structural
properties. Fortunately, the greedy framework we use is ﬂexible with regard to the choice of the selection
procedure. For instance, one can replace the exponential mechanism in a black-box manner with the “large
margin mechanism” of Chaudhuri et al. (2014) to obtain error bounds that replace the explicit dependence
on log |V | in Table 1 with a term that may be signiﬁcantly smaller for real datasets. We give a slightly
simpliﬁed analysis of the large margin mechanism, and present it in a manner suitable for greedy algorithms
which access the same data set multiple times. (These guarantees are more complicated, but spelled out in
Section 5.) For submodular functions exhibiting additional structure, one may also be able to perform each
maximization step with the “choosing mechanism” of Beimel et al. (2016) and Bun et al. (2015).

2 Preliminaries

Let V be ﬁnite set which we will refer to as the ground set and let X be a ﬁnite set which we will refer to as
the data universe. A dataset is an n-tuple D = (x1, . . . , xn) ∈ X n. Suppose each dataset D is associated
to a set function fD : 2V → R. The manner in which fD depends on D will be application-speciﬁc, but it is
assumed that the association between D and fD is public information.

Deﬁnition 2.1. A set function fD : 2V → R is submodular if for all sets S ⊆ T ⊆ V and every element
v ∈ V , we have fD(S ∪ {v}) − fD(S) ≥ fD(T ∪ {v}) − fD(T ).

Moreover, If fD(S) ≤ fD(T ) whenever S ⊆ T , we say fD is monotone. If for every dataset D =
(x1, . . . , xn), the function fD = 1
i=1 fxi for monotone submodular functions fxi : 2V → [0, λ], we say
n
fD is λ-decomposable. The problem of maximizing a decomposable submodular function was considered
as the “combinatorial public projects problem” by Papadimitriou et al. (2008).

(cid:80)n

We are interested in the problem of approximately maximizing a submodular function subject to differ-
ential privacy. The deﬁnition of differential privacy relies on the notion of neighboring datasets, which are
simply tuples D, D(cid:48) ∈ X n that differ in at most one entry. If D, D(cid:48) are neighboring, we write D ∼ D(cid:48).

Deﬁnition 2.2. A randomized algorithm M : X n → R satisﬁes (ε, δ)-differential privacy if for all measur-
able sets T ⊆ R and all neighboring datasets D ∼ D(cid:48),

Pr[M (D) ∈ T ] ≤ eε Pr[M (D(cid:48)) ∈ T ] + δ.

4

Differentially private algorithms must be calibrated to the sensitivity of the function of interest with

respect to small changes in the input dataset, deﬁned formally as follows.

Deﬁnition 2.3. The sensitivity of a set function fD : 2V → R (depending on a dataset D) with respect to a
constraint C ⊆ 2V is deﬁned as

max
D∼D(cid:48)

max
S∈C

|fD(S) − fD(cid:48)(S)|.

Composition of Differential Privacy. The analyses of our algorithms rely crucially on composition theo-
rems for differential privacy. For a sequence of privacy parameters {(εi, δi)}k
i=1, we informally refer to the
k-fold adaptive composition of (εi, δi)-differentially private algorithms as the output of a mechanism M ∗
that behaves as follows on an input D: In each of rounds i = 1, . . . , k, the algorithm M ∗ selects an (εi, δi)-
differentially private algorithm Mi possibly depending on the previous outcomes M1(D), . . . , Mi(D) (but
not directly on the sensitive dataset D itself), and releases Mi(D). For a formal treatment of adaptive
composition, see (Dwork et al., 2010; Dwork & Roth, 2014).

Theorem 2.4. (Dwork & Lei, 2009; Dwork et al., 2010; Bun & Steinke, 2016) The k-fold adaptive compo-
sition of (ε0, δ0)-differentially private algorithms satisﬁes (ε, δ)-differential privacy where

1. ε = kε0 and δ = kδ0.

(Basic Composition).

2. ε = 1

2 kε2

0 + (cid:112)2 log(1/δ(cid:48))ε0 and δ = δ(cid:48) + kδ, for any δ(cid:48) > 0.

(Advanced Composition)

Exponential Mechanism. The exponential mechanism (McSherry & Talwar, 2007) is a general primitive
for solving discrete optimization problems. Let q : V × X n → R be a “quality” function measuring how
good a solution v ∈ V is with respect to a dataset D ∈ X n. We say a quality function q has sensitivity λ if
for all v ∈ V and all neighboring datasets D ∼ D(cid:48), we have |q(v, D) − q(v, D(cid:48))| ≤ λ.

Proposition 2.5. Let ε > 0 and let q : V ×X n be a quality function with sensitivity λ. Deﬁne the exponential
mechanism as the algorithm which selects every v ∈ V with probability proportional to exp(εq(v, D)/2λ).

• The exponential mechanism provides (ε, 0)-differential privacy.

• For every D ∈ X n,

2λ · ln |V |
ε
where ˆv is the output of the exponential mechanism on dataset D.

E[q(ˆv, D)] ≥ max
v∈V

q(v, D) −

,

The privacy guarantee and a “with high probability” utility guarantee of the exponential mechanism are
due to McSherry & Talwar (2007). A simple proof of the utility guarantee in expectation appears in (Bassily
et al., 2016).

3 Monotone Submodular Maximization

In this section, we present a variant of the basic greedy algorithm which will enable maximization of mono-
tone submodular functions. This algorithm simply replaces each greedy selection step with a privacy-
preserving selection algorithm denoted O. The selection function O takes as input a quality function
q : U × X n → R and a dataset D, as well as privacy parameters ε0, δ0, and outputs an element u ∈ U .

5

We begin in the simplest case of monotone submodular maximization with a cardinality constraint (Algo-
rithm 1). The algorithm for more general constraints appears in Section 3.1.

Algorithm 1 was already studied by Gupta et al. (2010) in the special case where fD is decomposable,
and O is the exponential mechanism. We generalize their result to the much broader class of low-sensitivity
monotone submodular functions.

Algorithm 1 Diff. Private Greedy (Cardinality) GO
Input: Submodular function fD : 2V → R, dataset D, cardinality constraint k, privacy parameters ε0, δ0
Output: Size k subset of V

1. Initialize S0 = ∅

2. For i = 1, . . . , k:

3. Return Sk

• Deﬁne qi : (V \ Si−1) × X n → R via qi(v, ˜D) = f ˜D(Si−1 ∪ {v}) − f ˜D(Si−1)
• Compute vi ←R O(qi, D; ε0, δ0)
• Update Si ← (Si−1 ∪ {vi})

Theorem 3.1. (Gupta et al., 2010) Suppose fD : 2V → R is λ-decomposable (cf. Deﬁnition 2.1). Let δ > 0
and let ε0 ≥ 0 be such that ε = 2 · ε0 · (e − 1) ln(3e/δ) ≤ 1. Then instantiating Algorithm 1 with O = EM
and parameter ε0 > 0 provides (ε, δ)-differential privacy.

Moreover, for every D ∈ X n,

where Sk ←R GEM(D).

E [fD(Sk)] ≥

1 −

OPT −

(cid:18)

(cid:19)

1
e

2λk ln |V |
ε0

Unfortunately, the privacy analysis of Theorem 3.1 makes essential use of the decomposability of fD,
and does not directly generalize to arbitrary submodular functions of low-sensitivity. Replacing the privacy
analysis of Gupta et al. (2010) with the Composition Theorem 2.4 instead gives

Theorem 3.2. Suppose fD : 2V → R is monotone and has sensitivity λ. Then instantiating Algorithm 1
with O = EM and parameter ε0 > 0 provides (ε = kε0, δ = 0)-differential privacy. It also provides
(ε, δ)-differential privacy for every δ > 0 with ε = kε2

0/2 + ε0 · (cid:112)2k ln(1/δ).

Moreover, for every D ∈ X n,

where Sk ←R GEM(D).

E [fD(Sk)] ≥

1 −

OPT −

(cid:18)

(cid:19)

1
e

2λk ln |V |
ε0

Proof. The privacy guarantee of Theorem 3.2 follows immediately from the (ε, 0)-differential privacy of
the exponential mechanism, together with Theorem 2.4.

To simplify notation in the utility proofs in this paper, we suppress the dependence of the submodular
function of interest on D, i.e. we write f = fD. We also introduce the notation fS(T ) = f (S ∪ T ) − f (S)
to denote the marginal gain by adding T to the set S.

6

To argue that the algorithm achieves good utility, recall that in each step i, the exponential mechanism

E[fSi−1(vi)] ≥ max

fSi−1(v) − α

v∈V \Si−1

(1)

guarantees a solution vi with

where α = 2λ · ln |V |/ε.

Let S∗ denote any set of size k with f (S∗) = OPT. Below, let us condition on having obtained some

set Si−1 of elements after the ﬁrst i − 1 iterations of our algorithm. Then

E[fSi(vi)] = max

fSi−1(v) − α

(by Condition (1))

v∈V \Si−1
(cid:32)

(cid:88)

1
k

v∈S∗

(cid:33)

fSi−1(v)

− α

f (S∗ ∪ Si−1) − f (Si−1)
k

− α

OPT −f (Si−1)
k

− α

≥

≥

≥

(by submodularity of f )

(by monotonicity of f )

We now unﬁx from conditioning on having obtained a speciﬁc Si−1 by taking the expectation over all

choices of such a set. This gives

E[fSi−1(vi)] ≥

OPT −E[f (Si−1)]
k

− α

Rearranging yields

OPT −E[f (Si)] ≤

1 −

(OPT −E[f (Si−1)]) + α

(cid:18)

(cid:19)

1
k

Recursively applying this bound yields

Hence, we conclude

OPT −E[f (Si)] ≤

1 −

(OPT −E[f (S0)]) +

i−1
(cid:88)

(cid:18)

j=0

1 −

· α

(cid:19)j

1
k

≤

1 −

OPT +α.

(cid:18)

(cid:18)

(cid:19)i

(cid:19)i

1
k

1
k

(cid:34)

(cid:18)

E[f (Sk)] ≥

1 −

1 −

OPT −α

(cid:18)

(cid:19)k(cid:35)

1
k

(cid:19)

1
e

≥

1 −

OPT −α.

7

3.1 Matroid and p-Extendible System Constraints

We now show how to extend Algorithm 1 to privately maximize monotone submodular functions subject to
more general constraints. To start, we review the deﬁnition of a p-extendible system. Consider a ground set
V and a non-empty downward-closed family of subsets I ⊆ 2V (i.e. if T ∈ I, then S ∈ I for every S ⊆ T ).
Such an I is called a family of independent sets. The pair (V, I) is said to be a p-extendible system (Mestre,
2006) if for all S ⊂ T ∈ I, and v ∈ V such that S ∪ {v} ∈ I, there exists a set Z ⊆ (T \ S) such that
|Z| ≤ p and (T \ Z) ∪ {v} ∈ I. Let r(I) denote the size of the largest independent set in I.

The deﬁnition of a matroid coincides with that of a 1-extendible system (with rank r(I)). For p ≥
2, the notion of a p-extendible system strictly generalizes that of an intersection of p matroids. A slight
modiﬁcation of Algorithm 1 gives a uniﬁed algorithm for privately maximizing a monotone submodular
function subject to matroid and p-extendible system constraints, presented as Algorithm 2.

We obtain analogues of the results presented for cardinality constraints.

Theorem 3.3. Suppose fD : 2V → R is λ-decomposable (cf. Deﬁnition 2.1). Let δ > 0 and let ε0 ≥ 0 be
such that ε = 2 · ε0 · (e − 1) ln(3e/δ) ≤ 1. Then instantiating Algorithm 2 with O = EM and parameter
ε0 > 0 provides (ε, δ)-differential privacy. Moreover, for every D ∈ X n,

E [fD(S)] ≥

· OPT −

1
p + 1

p
p + 1

(cid:18) 2λr(I) ln |V |
ε0

(cid:19)

where S ←R GEM(D).

Algorithm 2 Differentially Private Greedy (p-system) GO
Input: Submodular function fD : 2V → R, dataset D, p-extendible family (V, I), privacy parameters ε0, δ0
Output: Maximal independent subset of V

1. Initialize S = ∅

2. While S ∈ I is not maximal:

• Deﬁne q : (V \ S) × X n → R via q(v, ˜D) = f ˜D(S ∪ {v}) − f ˜D(S)
• Compute vi ←R O(q, D; ε0, δ0)
• Update S ← (S ∪ {vi})

3. Return S

Proof. The privacy guarantee of Theorem 3.3 follows from Gupta et al. (2010).

In our proof of utility, we again suppress the dataset D, and use the notation fS(T ) to denote f (S ∪

T ) − f (S). Our proof applies to any greedy algorithm that, in each round i, selects an item vi with

E (cid:2)fSi−1(vi)(cid:3) ≥

max
v:Si−1∪{v}∈I

fSi−1(vi) − α

(2)

for some error term α > 0.

We follow the proof outlined by C˘alinescu et al. (2011). Fix an optimal solution O ∈ I, i.e. f (O) =
(If

OPT. Let S1, . . . , Sr be any sequence representing the output of the algorithm, where r = r(I).

8

the algorithm terminates in an earlier round k < r, then extend its output by setting Si = Sk for each
i = k + 1, . . . , r.) To such a sequence, we deﬁne a partition O1, . . . , Or of O via the following algorithm.

Algorithm 3 Partition construction algorithm
Input: Optimal solution O, sets S1, . . . , Sr
Output: A partition O1, O2, . . . , Or of O

1. Initialize T0 = O

2. For i = 1, 2, . . . , r:

(b) Set Ti = Ti−1\Oi

3. Return O1, O2, . . . , Or

(a) If vi ∈ Ti−1, set Oi = {vi};

Else, let Oi ⊆ Ti−1 be the smallest subset s.t. ((Si−1 ∪ Ti−1) \ Oi) ∪ {vi} ∈ I

To see that O1, . . . , Or is indeed a partition, observe that Si ∪ Ti ∈ I and Si ∩ Ti = ∅ for every i.
Therefore, it must be the case that Tr = ∅, since Sr ∪ Tr ∈ I and Sr is maximal when the algorithm
terminates. Hence, the disjoint sets O1, . . . , Or do in fact exhaust O.
Lemma 3.4. For every i = 1, . . . , r, we have E (cid:2)fSi−1(vi)(cid:3) ≥ 1

E (cid:2)fSi−1(Oi)(cid:3) − α.

p

Before proving Lemma 3.4, we show how to use it to complete the proof of Threorem 3.3. Recursively

applying the lemma shows that for every i,

E [f (Si)] ≥

E (cid:2)fSj−1(Oj)(cid:3) − iα.

1
p

i
(cid:88)

j=1

Hence, we obtain

E [f (Sr)] ≥

E (cid:2)fSi−1(Oi)(cid:3) − rα

r
(cid:88)

i=1
r
(cid:88)

1
p

1
p

1
p
1
p

≥

≥

≥

E [fSr (Oi)] − rα

i=1
E [fSr (O)] − rα

(by submodularity)

(by linearity of expectation and submodularity)

(f (O) − E [f (Sr)]) − rα.

(by monotonicity)

Rearranging gives the desired result E [f (Sr)] ≥ 1

p+1 f (O) − p

p+1 rα.

Proof of Lemma 3.4. The partition construction algorithm that every set Oi satisﬁes |Oi| ≤ p; this follows
from the deﬁnition of p-extendibility and the fact that Si−1 ∪ {vi} ∈ I. Moreover, any element in Oi is a
candidate for inclusion in Si, since Si−1 ∪ {v} ∈ I for every v ∈ Oi.

9

It is also clear from the partition construction that for each v ∈ Oi, we have Si−1 ∪ {v} ∈ I. Below,
ﬁx a choice of i and condition on the algorithm’s history up to iteration i − 1. This ﬁxes choices of the sets
S1, . . . , Si−1, as well as T1, . . . , Ti and O1, . . . , Oi.

Then since E (cid:2)fSi−1(vi)(cid:3) ≥ fSi−1(v) − α for every v ∈ Oi, we have

E (cid:2)fSi−1(vi)(cid:3) ≥

fSi−1(Oi) − α

(by submodularity)

1
|Oi|
1
p

≥

fSi−1(Oi) − α.

Taking the expectation over the conditioned event gives the asserted result.

Theorem 3.5. Suppose fD : 2V → R has sensitivity λ. Then instantiating Algorithm 2 with O = EM
and parameter ε0 > 0 provides (ε = r(I)ε0, δ = 0)-differential privacy. It also provides (ε, δ)-differential
privacy for every δ > 0 with ε = r(I)ε2/2 + ε · (cid:112)2r(I) ln(1/δ).

Moreover, for every D ∈ X n,

E [fD(S)] ≥

· OPT −

1
p + 1

p
p + 1

(cid:18) 2λr(I) ln |V |
ε0

(cid:19)

where S ←R GEM(D).

Proof. The proof of privacy follows from Theorem 2.4. The proof of utility is identical to that of the proof
of Theorem 3.3.

4 Non-Monotone Submodular Maximization

We now consider the problem of privately maximizing an arbitrary, possibly non-monotone, submodular
function under a cardinality constraint. In general, the greedy algorithm presented in Section 3 fails to give
any constant-factor approximation. Instead, our algorithm in this section will be based on the “stochastic
greedy” algorithm ﬁrst studied by Mirzasoleiman et al. (2015). In each round, the stochastic greedy algo-
rithm ﬁrst subsamples a random 1
k · ln(1/α) fraction of the ground set for some α > 0, and then greedily
selects the item from this subsample that maximizes marginal gain. Mirzasoleiman et al. (2015) showed that
for a monotone objective function f , this algorithm provides a (1 − 1/e − α)-approximation to the optimal
solution. Their original motivation was to improve the running time of the greedy algorithm: from O(|V |·k)
evaluations of the objective function to linear O(|V | · ln(1/α)).

Unfortunately, the stochastic greedy algorithm does not provide any approximation guarantee for non-
monotone submodular functions. Buchbinder et al. (2014) instead proposed a “random greedy” algorithm
that, in each iteration, randomly selects one of the k elements with the highest marginal gain. Buchbinder
et al. (2014) showed that the random greedy algorithm achieves a 1/e approximation to the optimal solution
(in expectation), using k|V | function evaluations. However, it is not clear how to adapt this algorithm to
accommodate differential privacy, since its analysis has a brittle dependence on the sampling procedure.

We make two main contributions to the analysis of the stochastic greedy and random greedy algorithms.
First, we show that running the stochastic greedy algorithm on an exact 1
k fraction of the ground set per
iteration still gives a (0.468)-approximation for monotone objectives, and moreover, gives a 1
e (1 − 1/e)-
approximation even for non-monotone objectives. Note that this algorithm evaluates the objective func-
tion on only |V | elements, and still provides a constant factor approximation guarantee. This makes our

10

“subsample-greedy” algorithm the fastest algorithm for maximizing a general submodular function subject
to a cardinality constraint (albeit with slightly worse approximation guarantees). Second, we show that the
guarantees of this algorithm are robust to using a randomized greedy selection procedure (e.g. the exponen-
tial or large margin mechanism), and hence it can be adapted to ensure differential privacy.

We present the subsample-greedy algorithm as Algorithm 4 below. Assume that V is augmented by
enough “dummy elements” to ensure that |V |/k is an integer; each dummy element u is deﬁned so that
fD(S ∪ {u}) = fD(S) for every set S. We also explicitly account for an additional set U of k dummy
elements, and ensure that at least one appears in every subsample.

Algorithm 4 Diff. Private “Subsample-Greedy” SGO
Input: Submodular function fD : 2V → R, dataset D, cardinality constraint k, privacy parameters ε0, δ0
Output: Size k subset of V

1. Initialize S0 = ∅, dummy elements U = {u1, . . . , uk}

2. For i = 1, . . . , k:

• Sample Vi ⊂ V a uniformly random subset of size |V |/k and ui a random dummy element
• Deﬁne qi : (Vi ∪ {ui}) × X n → R via qi(v, ˜D) = f ˜D(Si−1 ∪ {v}) − f ˜D(Si−1)
• Compute vi ←R O(qi, D; ε0, δ0)
• Update Si ← (Si−1 ∪ {vi})

3. Return Sk with all dummy elements removed

Theorem 4.1. Suppose fD : 2V → R has sensitivity λ. Then instantiating Algorithm 4 with O = EM
provides (ε, δ)-differential privacy, and for every D ∈ X n,

E [fD(S)] ≥

1 −

OPT −

(cid:18)

1
e

(cid:19)

1
e

2λk ln |V |
ε

where S ←R SGEM(D). Moreover, if fD is monotone, then
1 − e−(1−1/e)(cid:17)

E [fD(S)] ≥

(cid:16)

OPT −

2λk ln |V |
ε

≈ 0.468 OPT −

2λk ln |V |
ε

.

The guarantees of Theorem 4.1 are of interest even without privacy. Letting MAX denote the selection
procedure which simply outputs the true maximizer (equivalently, which runs the exponential mechanism
with ε0 = +∞), we obtain the following non-private algorithm for maximizing a submodular function fD:

Corollary 4.2. Let fD : 2V → R be any submodular function. Instantiating Algorithm 4 with O = MAX
gives

E [fD(S)] ≥

1 −

OPT

(cid:18)

1
e

(cid:19)

1
e

where S ←R SGMAX(D). Moreover, if fD is monotone, then

E [fD(S)] ≥

(cid:16)

1 − e−(1−1/e)(cid:17)

OPT ≈ 0.468 OPT .

11

4.1 Proof of Theorem 4.1

The analysis below will work generally for any random selection procedure guaranteeing that in every round
i = 1, . . . , k,

E (cid:2)fSi−1(vi)(cid:3) ≥ max

fSi−1(v) − α

v∈(Vi∪{ui})

for some parameter α > 0. We begin by ﬁxing an optimal solution S∗ with f (S∗) = OPT.

Claim 4.3 ((Buchbinder et al., 2014, Observation 3.2)). For every i = 0, . . . , k, we have E [f (S∗ ∪ Si)] ≥
(1 − 1/k)i · OPT.

Proof. For every iteration i = 1, . . . k, the subsampling step ensures that every element in V ∪ U is selected
for inclusion in Si with probability at most 1/k. Hence, for every i = 0, 1, . . . , k, each element is included
in Si with probability at most 1 − (1 − 1/k)i. Deﬁne g : 2V → R by g(S) = g(S∗ ∪ S). Then g is a
submodular function, and

E [f (S∗ ∪ Si)] = E [g(Si \ S∗)] ≥ (1 − 1/k)ig(∅) = (1 − 1/k)i OPT .

The inequality here follows from the fact that for any set T and any random subset T (cid:48) ⊆ T that includes
every element of T with probability p, we have E [g(T (cid:48))] ≥ (1 − p) · g(∅) + p · g(T ) for any submodular
function g Feige et al. (2007).

Claim 4.4.

E[fSi−1(vi)] ≥

1 −

(cid:18)

(cid:19)

1
e

(cid:18) E[f (S∗ ∪ Si−1)] − E[f (Si−1)]
k

·

(cid:19)

− α.

Proof. Begin by conditioning on a ﬁxed choice of the set Si−1. Let M ⊆ (V ∪ U ) denote a set of k items
which maximizes the quantity (cid:80)
v∈M fSi−1(v). That is, M consists of the k items in (V ∪ U ) which result
in the largest marginal gain for fSi−1.

Let G denote the event that the subsampled set Vi ∪ {ui} contains at least one element in M . Observe

that even if G does not occur, we have

We claim moreover that

E (cid:2)fSi−1(vi)|G(cid:3) ≥ fSi−1(ui) − α ≥ −α.

E (cid:2)fSi−1(vi)|G(cid:3) ≥

fSi−1(v) − α.

1
k

(cid:88)

v∈M

(3)

(4)

To see this, sort the items in V ∪U as v(1), . . . , v(m), v(m+1), . . . , v(m+k), where m = |V | and fSi−1(v(j)) ≥
fSi−1(v(j+1)) for every j = 1, . . . , m + k − 1. Break ties in such a way that M = {v(1), . . . , v(k)}, and that
there is some t ∈ {0, . . . , k} such that v(1), . . . , v(t) ∈ V and v(t+1), . . . , v(k) ∈ U (that is, real elements
come before dummy elements).

Let Aj denote the event that j is the smallest index such that v(j) ∈ Vi ∪ {ui}. Then the events

A1, . . . , Am+k are mutually exclusive and exhaustive. Moreover, by the deﬁnition of G, we have (cid:80)k
Pr[G].

j=1 Pr[Aj] =

12

It is easy to see that

Pr[A1] =

Pr[Aj] =

Pr[Aj] =

1
k
(cid:1)
(cid:0) m−j
m/k−1
(cid:1)
(cid:0) m
m/k
(cid:1)
(cid:0)m−t
m/k
(cid:1) ·
(cid:0) m
m/k

(cid:19)j−t−1

(cid:18)

·

1 −

1
k

1
k

j = 2, . . . , t,

j = t + 1, . . . , k.

Moreover, Pr[Aj] is a decreasing function j = 1, . . . , k. Hence, Pr[Aj|G] = Pr[Aj]/ Pr[G] is a decreasing
function of j = 1, . . . , k as well. Moreover, Pr[A1|G] ≥ Pr[A1] = 1/k. This allows us to calculate

E (cid:2)fSi−1(vi)|G(cid:3) =

E (cid:2)fSi−1(vi)|Aj

(cid:3) · Pr[Aj|G]

k
(cid:88)

j=1

≥

=

1
k

1
k

j=1
(cid:88)

v∈M

k
(cid:88)

(cid:16)

fSi−1(v(j)) − α

(cid:17)

fSi−1(v) − α.

(by Chebyshev’s sum inequality)

This establishes the claimed inequality (4).

To estimate E (cid:2)fSi−1(vi)|G(cid:3), it remains to calculate Pr[G]. Suppose M consists of t elements from V

and k − t dummy elements from U . Then

t
k

= 1 −

Pr[G] = 1 − Pr[M ∩ (Vi ∪ {ui}) = ∅]
(cid:1)
(cid:0)m−t
m/k
(cid:1) ·
(cid:0) m
m/k
(m − (m/k))(m − (m/k) − 1) . . . (m − (m/k) − t + 1)
m(m − 1) . . . (m − t + 1)
(cid:19)
1
k

m
m − t + 1

m
m − 1

= 1 −

= 1 −

(cid:19) (cid:18)

1 −

1 −

1 −

1
k

1
k

. . .

(cid:19)

(cid:18)

(cid:18)

·

·

·

t
k

·

t
k

(cid:19)t

·

t
k

(cid:18)

1 −

1
k
te−t/k
k

≥ 1 −

≥ 1 −

≥ 1 −

1
e

,

13

where the last inequality follows from the fact that the function r(x) = xe−x is maximized at x = 1, where
it takes the value 1/e.

Let M (cid:48) be the set containing S∗ \ Si−1 together with enough dummy elements to have size exactly k.

We conclude that

E (cid:2)fSi−1(vi)(cid:3) = Pr[G] · E (cid:2)fSi−1(vi)|G(cid:3) + (1 − Pr[G]) · E (cid:2)fSi−1(vi)| ¯G(cid:3)

≥

1 −

fSi−1(v) − α

−

· α

(by (4) and (3))

(cid:33)

1
e

(cid:33)

fSi−1(v)

− α

(cid:19) (cid:32)

(cid:19) (cid:32)

1
k

1
k

(cid:88)

v∈M

(cid:88)

v∈M (cid:48)

(cid:18)

(cid:18)

(cid:18)

≥

1 −

≥

1 −

1
e

1
e

1
e

(cid:19) (cid:18) f (S∗ ∪ Si−1) − f (Si−1)

(cid:19)

k

− α.

(by submodularity)

(by deﬁnition of M )

Unconditioning from Si−1 by taking the expectation over its choice proves the claim.

Proof of Theorem 4.1. Let f be any (possibly non-monotone) submodular function. We show by induction
that for every i = 0, . . . , k, we have

(cid:18)

E [f (Si)] ≥

1 −

(cid:19)

1
e

·

i
k

(cid:18)

·

1 −

1
k

(cid:19)i−1

· OPT −iα.

(5)

This clearly holds for the base case of i = 0. Assuming it holds in iteration i − 1, we calculate

E[f (Si)] = E[f (Si−1)] + E[fvi(Si−1)]

≥ E[f (Si−1)] +

1 −

≥ E[f (Si−1)] +

1 −

(cid:18)

(cid:18)

1
e

1
e

(cid:19) (cid:18) E[f (S∗ ∪ Si−1)] − E[f (Si−1)]

(cid:19)

(cid:19) (cid:32)

(1 − 1

k )i−1 OPT −E[f (Si−1)]

− α

(cid:33)

− α

=

1 −

E[f (Si−1)] +

1 −

·

1 −

·

· OPT −α

(cid:18)

(cid:19)

(cid:18)

(cid:19)i−1

k

k

1
k

≥

1 −

1 −

·

1 −

· OPT +

1 −

·

(cid:19)

1
e

·

i − 1
k

(cid:19)i−2

1
k

1
k
(cid:18)

1
e
(cid:18)

(cid:18)

(cid:18)

(cid:18)

(cid:19)

(cid:19) (cid:18)

1
k

1
k

1
e

=

1 −

(cid:19)

·

i
k

(cid:18)

·

1 −

1
k

(cid:19)i−1

· OPT −iα.

Hence, in iteration k, we have

(by Claim 4.4)

(by Claim 4.3)

(cid:19)

(cid:18)

1
e

(cid:19)i−1

1
k

1 −

1
k
(by the inductive hypothesis)

· OPT −iα

·

(cid:18)

(cid:19)

(cid:18)

(cid:19)k−1

E [f (Sk)] ≥

1 −

·

1 −

1
k

1
e

· OPT −kα ≥

1 −

· OPT −kα

(cid:18)

(cid:19)

1
e

·

1
e

as we wanted to show.

Now we consider the special case where f is monotone. In this case, we have

E[fvi(Si−1)] ≥

1 −

(cid:19) (cid:18) E[f (S∗ ∪ Si−1)] − E[f (Si−1)]

(cid:19)

− α

(by Claim 4.4)

≥

1 −

(cid:19) (cid:18) OPT −E[f (Si−1)]

(cid:19)

− α

(by monotonicity).

(cid:18)

(cid:18)

1
e
1
e

k

k

14

Rearranging gives us

OPT −E [f (Si)] ≤

1 −

· (OPT −E [f (Si−1)]) + α.

(cid:18)

(cid:19)

(1 − 1/e)
k

Recursively applying this bound yields

OPT −E[f (Si)] ≤

1 −

(OPT −E[f (S0)]) +

(cid:18)

(cid:18)

(cid:19)i

(cid:19)i

(1 − 1/e)
k

(1 − 1/e)
k

≤

1 −

OPT +iα.

i−1
(cid:88)

(cid:18)

1 −

j=0

(cid:19)j

α

1
k

Hence, we conclude

E[f (Sk)] ≥

(cid:19)k(cid:35)

(cid:34)

(cid:16)

(cid:18)

1 −

1 −

(1 − 1/e)
k
1 − e−(1−1/e)(cid:17)

≥

OPT −kα.

OPT −kα

5 The Large Margin Mechanism

The accuracy guarantee of the exponential mechanism can be pessimistic on datasets where q(·, D) exhibits
additional structure. For example, suppose that when the elements of V are sorted so that q(v1, D) ≥
q(v2, D) ≥ · · · ≥ q(v|V |, D), there exists an (cid:96) such that q(v1, D) (cid:29) q(v(cid:96)+1, D). Then only the top (cid:96)
ground set items are relevant to the optimization problem, so running the exponential mechanism on these
should maintain differential privacy, but with error proportional to ln (cid:96) rather than to ln |V |. The large
margin mechanism of Chaudhuri et al. (2014), like the exponential mechanism, generically solves discrete
optimization problems. However, it automatically leverages this additional margin structure whenever it
exists. Asymptotically, the error guarantee of the large margin mechanism is always at most that of the
exponential mechanism, but can be much smaller when the data exhibits a margin for small (cid:96).

Formally, given a quality function q : V × X n → R and parameters (cid:96) ∈ N, γ > 0, a dataset D satisﬁes

the ((cid:96), γ)-margin condition if q(v(cid:96)+1, D) < q(v1, D) − γ.

For each (cid:96) = 1, . . . , |V |, deﬁne

(cid:18)

g(cid:96) = λ ·

3 +

G(cid:96) =

8λ ln(2/δ)
ε

(cid:19)

4 ln(2(cid:96)/δ)
ε
16λ ln(7(cid:96)2/δ)
ε

+

+ g(cid:96).

Recall that the Laplace distribution Lap(b) is speciﬁed by the density function 1
sample Z ∼ Lap(b) obeys the tail bound Pr[Z > t] = 1

2 exp(−t/b) for all t > 0.

2b exp(−|x|/b).

, and a

Proposition 5.1. Let ε, δ > 0. Consider the large margin mechanism described in Algorithm 5. Then

• Algorithm LMM is (ε, δ)-differentially private.

15

Algorithm 5 Large Margin Mechanism (LMM)
Input: Quality function q : V × X n → R, dataset D, privacy parameters ε, δ > 0
Output: Item ˆv ∈ V

1. Sort the elements of V so that q(v1, D) ≥ · · · ≥ q(v|V |, D)

2. Let m = q(v1, D) + Z for Z ∼ Lap(8λ/ε)

3. For (cid:96) = 1, . . . , |V |:

• Sample Z(cid:96) ∼ Lap(16λ/ε)
• If m − q(v(cid:96)+1, D) > G(cid:96) + Z(cid:96): Report (cid:96) and break

4. Return ˆv ∈ {v1, . . . , v(cid:96)} sampled w.p. ∝ exp(εq(vi, D)/4λ).

• Suppose D ∈ X n satisﬁes the ((cid:96), γ)-margin condition for

for some β > 0. Then there exists an event E with Pr[E] ≥ 1 − β such that

γ =

24λ ln(1/β)
ε

+ G(cid:96)

E[q(ˆv, D)|E] ≥ OPT −

4λ · ln (cid:96)
ε

,

where ˆv is the output of LMM(D).

Our presentation of Algorithm 5 differs slightly from that of Chaudhuri et al. (2014). Namely, we
simplify the choice of the noisy maximum m, and redistribute the algorithm’s use of the privacy budget ε
with an eye toward better performance in applications. Because of these small changes, we sketch the proof
of Proposition 5.1 for completeness.

Privacy Analysis of Proposition 5.1. Algorithm 5 can be thought of as releasing two items in stages: First,
the margin parameter (cid:96) in Step 3, and second, the item ˆv sampled via the exponential mechanism in Step 4.
We ﬁrst claim that releasing the margin parameter (cid:96) guarantees (ε/2, 0)-differential privacy. This follows
because Steps 2 and 3 taken together are an instantiation of the “AboveThreshold” algorithm, as presented
by Dwork and Roth (Dwork & Roth, 2014, Theorem 3.23), with respect to the sensitivity-(2λ) functions
q(v1, D) − q(v(cid:96)+1, D). Denote the output (cid:96) of the algorithm at Step 3 by S(D).

We now establish that Step 4 provides differential privacy. Following Chaudhuri et al. (2014), we let
A((cid:96), D) capture the behavior of the algorithm in Step 4, where on receiving (cid:96) from Step 3, it samples from
the exponential mechanism on the top (cid:96) elements. They proved the following lemma about A((cid:96), D):

Lemma 5.2 ((Chaudhuri et al., 2014, Lemma 5)). If D satisﬁes the ((cid:96), γ)-margin condition with

for some δ(cid:48) > 0, then for every neighbor D(cid:48) ∼ D and any T ⊆ V , we have

Pr[A((cid:96), D) ∈ T ] ≤ eε/2 Pr[A((cid:96), D(cid:48)) ∈ T ] + δ(cid:48).

(cid:18)

γ ≥ 2λ

1 +

(cid:19)

2 ln((cid:96)/δ(cid:48))
ε

16

Now ﬁx neighboring datasets D ∼ D(cid:48). Let L denote the set of (cid:96) for which q(v1, D) − q(v(cid:96)+1, D) ≥ g(cid:96).
By deﬁnition, if (cid:96) = S(D) ∈ L, then D indeed satisﬁes the ((cid:96), g(cid:96))-margin condition. Moreover, by tail
bounds on the Laplace distribution,

Pr[S(D) /∈ L] ≤ Pr (cid:2)Z > 8λ ln(2/δ)/ε ∨ (cid:0)∃(cid:96) ∈ {1, . . . , |V |} : Z(cid:96) < −16λ ln(7(cid:96)2/δ)/ε(cid:1)(cid:3)

|V |
(cid:88)

(cid:96)=1

6δ
4π2(cid:96)2

≤

+

δ
4

δ
2

≤

.

Hence, we have that for any T ⊆ V ,

(cid:88)

(cid:96)∈L
(cid:88)

(cid:96)∈L
(cid:88)

(cid:96)∈L

≤

≤

≤ eε Pr[LMM(D(cid:48)) ∈ T ] + δ

This completes the privacy proof of Proposition 5.1.

Pr[LMM(D) ∈ T ] ≤

Pr[LMM(D) ∈ T |S(D) = (cid:96)] · Pr[S(D) = (cid:96)] + Pr[S(D) /∈ L]

Pr[LMM(D) ∈ T |S(D) = (cid:96)] · eε/2 Pr[S(D(cid:48)) = (cid:96)] +

δ
2

(eε/2 Pr[LMM(D(cid:48)) ∈ T |S(D(cid:48)) = (cid:96)] + e−ε/2 δ
2

) · eε/2 Pr[S(D(cid:48)) = (cid:96)] +

by Lemma 5.2

δ
2

Utility Analysis of Proposition 5.1. Suppose D satisﬁes the ((cid:96), β)-margin condition with

γ ≥

24λ ln(1/β)
ε

+ G(cid:96),

for some β > 0. By the tail bound for the Laplace distribution and a union bound, we have that with
probability at least 1 − β,

Let E be the event where this occurs. If E occurs, then indeed we have

Z ≥

ln

and Z(cid:96) ≤

8
λ

1
β

16
λ

ln

1
β

.

and hence Step 3 terminates outputting some (cid:96)(cid:48) ≤ (cid:96). By Proposition 2.5, it follows that

(q(v1, D) + Z) − q(v(cid:96)+1, D) > G(cid:96) + Z(cid:96),

E[q(ˆv, D)|E] ≥ OPT −

4λ · ln (cid:96)
ε

.

Replacing the exponential mechanism with the large margin mechanism gives analogues of our results
for monotone submodular maximization with a cardinality constraint, monotone submodular maximization
over a p-extendible system, and non-monotone submodular maximization with a cardinality constraint:

17

Theorem 5.3. Suppose fD : 2V → R is monotone and has sensitivity λ. Then instantiating Algorithm 1
with O = LMM and parameters ε0, δ0 = 0 provides (kε0, kδ0)-differential privacy.
It also provides
(ε, δ(cid:48) + kδ0)-differential privacy for every δ(cid:48) > 0 with ε = kε2/2 + ε · (cid:112)2k ln(1/δ(cid:48)).
Moreover, for every D ∈ X n, there exists an event E with Pr[E] ≥ 1 − β such that

1
e
where Sk ←R GLMM(D), and D satisﬁes the ((cid:96)i, γi)-margin condition with respect to every function of the
form qi(v, D) = fD( ˆSi−1 ∪ {v}) − fD( ˆSi−1), with γi = 24λ ln(k/β)/ε + G(cid:96)i.

E [fD(Sk)|E] ≥

4λ ln (cid:96)i
ε0

OPT −

1 −

i=1

(cid:18)

(cid:19)

k
(cid:88)

Theorem 5.4. Instantiating Algorithm 2 with O = LMM under all of the conditions of Theorem 5.3 gives
the same privacy guarantee (replacing k with r(I)) and gives

E [fD(S)|E] ≥

· OPT −

1
p + 1

r(I)
(cid:88)

i=1

4λ ln (cid:96)i
ε0

.

Theorem 5.5. Instantiating Algorithm 4 with O = LMM under all of the conditions of Theorem 5.3 gives
the same privacy guarantee and gives

Moreover, if fD is monotone, then

E [fD(Sk)|E] ≥

1 −

OPT −

(cid:18)

1
e

(cid:19)

1
e

k
(cid:88)

i=1

4λ ln (cid:96)i
ε0

.

E [fD(Sk)|E] ≥ 0.468 OPT −

k
(cid:88)

i=1

4λ ln (cid:96)i
ε0

.

6 Experimental Results

In this section we describe two concrete applications of our mechanisms.

6.1 Location Privacy

We analyze a dataset of 10,000 Uber pickups in Manhattan in April 2014 (UberDataset). Each individual
entry in the dataset consists of the longitude and latitude coordinates of the pickup location. We want to
use this dataset to select k public locations as waiting spots for idle Uber drivers, while also guaranteeing
differential privacy for the passengers whose locations appear in this dataset.1 We consider two different
public sets of locations L:

• LP opular is a set of 33 popular locations in Manhattan.

• LGrid is a set of 33 locations spread evenly across Manhattan in a grid-like manner.

We deﬁne a utility function M (i, j) to be the normalized Manhattan distance between a pickup location
i and the waiting location j. That is, if pickup location i is located at coordinates (i1, i2) and the waiting

location j is located at coordinates (j1, j2), then M (i, j) =
, where m = 0.266 is sim-
ply the Manhattan distance between the two furthest spread apart points in Manhattan. This normalization

|i1 − j1| + |i2 − j2|
m

1Under the assumption that each pickup corresponds to a unique individual.

18

(a) LP opular

(b) LGrid

(c) LP opular

(d) LGrid

(e) LP opular:
greedy

Non-private

(f) LP opular: EM-based greedy

(g) LGrid: Non-
private greedy

(h) LGrid: EM-
based greedy

Figure 1: (a) and (b) show utility for various cardinalities (k). (c) and (d) ﬁx k = 3 and show utility for
various privacy parameters ((cid:15)). Utility is normalized to be between 0 and 1. (e) - (h) shows a representative
top 3 set under various settings.

19

ensures that 0 ≤ M (i, j) ≤ 1, for all i, j. In order to make sure we have a maximization problem, we deﬁne
min
the following objective function: fD(S) = n −
j∈S

M (i, j), where n = |D| = 10000.

(cid:88)

i∈D

Observation 6.1. The function fD is λ-decomposable for λ = 1 (and hence has sensitivity 1).

This form of objective function is known to be monotone submodular and so we can use the greedy
algorithms studied in this paper. We use (cid:15) = 0.1 and δ = 2−20. For our settings of parameters, “basic
composition” outperforms “advanced composition,” so the privacy budget of (cid:15) = 0.1 is split equally across
the k iterations, meaning the mechanism at each iteration uses (cid:15)0 = (cid:15)
k . Our ﬁgures plot the average utility
across 100 simulations.

From Figures 1(a) and (b) we see that the results for both LP opular and LGrid are relatively similar and
unsurprising. The non-private greedy algorithm achieves the highest utility, but both the exponential mecha-
nism (EM)-based greedy and large margin mechanism (LMM)-based greedy algorithms exhibit comparable
utility while preserving a high level of privacy. Interestingly, we also see that the utilities of the EM-based
and LMM-based algorithms are almost identical for both LP opular and LGrid. This indicates that our mech-
anisms are actually selecting good locations, rather than just getting lucky because there are a lot of good
locations to choose from.

(a) Graphical model of Naive Bayes

(b) Expected mutual information

(c) Representative top 3 features

Figure 2: Privately selecting health features, from national health examination surveys, that correlate most
with diabetes.

Figures 1(c) and (d) show how the utility of the EM-based and LMM-based algorithms vary with the
privacy parameter (cid:15). We can also think of this as varying the dataset size for a ﬁxed (cid:15). We ﬁx k = 3 and
take the average of 100 simulations for each value of (cid:15). We see that even for very small (cid:15), our algorithms
outperform fully random selection. As (cid:15) increases, so does the utility. It is not shown in this ﬁgure, but
varying δ has very little effect.

From Figures 1(e) - (h), we see that the both the non-private and private algorithms select public lo-
cations that are relatively close to each other. For example, for the LP opular set of locations, the Empire
State Building is close to the New York Public Library, the Soho Grand Hotel is close to NYU, and the
Grand Army Plaza is close to the UN Headquarters. As a result, the private mechanisms manage to achieve
comparable utility, while also masking the users’ exact locations.

The theory described in Section 5 suggests that, at least asymptotically, the large margin mechanism-
based algorithm should outperform the exponential mechanism-based algorithm. However, in our experi-
ments, we ﬁnd that the large margin mechanism is generally only able to ﬁnd a margin in the ﬁrst iteration
of the greedy algorithm. This is because the threshold for ﬁnding a margin depends only on (cid:15), δ, and n and

20

DiabetesAsthmaVigorous ExerciseTaking InsulinHigh Blood PressureInsulinInsulinInsulinOverweightOverweightGenderHigh Blood PressureBlood TransfusionVigorous ExerciseNon-Private GreedyExponential MechanismLarge Margin Mechanismthus it stays the same across all k iterations. On the other hand, the marginal gain at each iteration drops very
quickly, so the mechanism fails to ﬁnd a margin and thus samples from all remaining locations. However,
since the large margin mechanism spends half of its privacy budget to try to ﬁnd a margin, the sampling
step gives slightly worse guarantees than does the plain exponential mechanism, thus giving us the slightly
weaker results we see in the ﬁgures.

6.2 Feature Selection Privacy

We analyze a dataset created from a combination of National Health Examination Surveys ranging from
2007 to 2014 (NHANESDataset). There are n = 23, 876 individuals in the dataset with information on
whether or not they have diabetes, along with m = 23 other potentially related binary health features. Our
goal is to privately select k of these features that provide as much information about the diabetes class
variable as possible.

More speciﬁcally, our goal is to maximize the mutual information between Y and XS, where Y is a
binary random variable indicating whether or not an individual has diabetes and XS is a random variable
that represents a set S of k binary health features. Mutual information takes the form:
(cid:88)

(cid:88)

(cid:19)

I(Y ; X) =

p(x, y) log2

y∈Y

x∈X

(cid:18) p(x, y)
p(x)p(y)

.

Under the Naive Bayes assumption, we suppose the joint distribution on (Y, X1, . . . , Xk) takes the form

k
(cid:89)

i=1

p(y, x1, . . . , xk) = p(y)

p(xi | y). Therefore, we can easily specify the entire probability distribution by

ﬁnding each p(xi | y). We estimate each p(xi | y) by counting frequencies in the dataset.

Our goal is to choose a size k subset S of the features in order to maximize fD(S) = I(Y ; XS). Mutual
information (under the Naive Bayes assumption) for feature selection is known to be monotone submodular
in S (Krause & Guestrin, 2005), and thus we can apply the greedy algorithms described in this paper.

Claim 6.2. In iteration i of the greedy algorithm, the sensitivity of fD(S) is (2i+1) log2(n)

.

n

We run 1,000 simulations with (cid:15) = 1.0 and δ = 2−20. As we can see from Figure 2(b), our private
mechanisms maintain a comparable utility relative to the non-private algorithm. We also observe an inter-
esting phenomenon where the expected utility obtained by our mechanism is not necessarily monotonically
increasing with the number of features selected. This is an artifact of the fact that if we are selecting k
features, then composition requires us to divide (cid:15) so that each iteration uses privacy budget (cid:15)
k . This is prob-
lematic for this particular application because there happens to be one feature (insulin administration) that
has much higher value than the rest. Therefore, the reduced probability of picking this single best feature
(as a result of the lower privacy parameter (cid:15)

k ) is not compensated for by selecting more features.

From Figure 2(c), we see that both the private and non-private mechanisms generally select insulin
administration as the top feature. However, while all three of the top features selected by the non-private
algorithm are clearly related to diabetes, the non-private mechanisms tend to select one feature (in our case,
gender or having received a blood transfusion) that may not be quite as relevant.

7 Conclusion

We have presented a general framework for maximizing submodular functions while guaranteeing differen-
tial privacy. Our results demonstrate that simple and ﬂexible greedy algorithms can preserve privacy while

21

achieving competitive guarantees for a variety of submodular maximization problems: for all functions
under cardinality constraints, as well as for monotone functions under matroid and p-extendible system con-
straints. Via our motivation to identify algorithms that could be made differentially private, we discovered
a non-monotone submodular maximization algorithm that achieves guarantees that are novel even without
concern for privacy. Finally, our experiments show that our algorithms are indeed competitive with their
non-private counterparts.

Acknowledgments. This work was supported by DARPA Young Faculty Award (D16AP00046), Simons-
Berkeley fellowship, and ERC StG 307036. This work was done in part while Amin Karbasi and Andreas
Krause were visiting the Simons Institute for the Theory of Computing.

References

Bassily, Raef, Smith, Adam D., and Thakurta, Abhradeep. Private empirical risk minimization: Efﬁcient

algorithms and tight error bounds. In FOCS, pp. 464–473, 2014.

Bassily, Raef, Nissim, Kobbi, Smith, Adam D., Steinke, Thomas, Stemmer, Uri, and Ullman, Jonathan.

Algorithmic stability for adaptive data analysis. In STOC, pp. 1046–1059, 2016.

Beimel, Amos, Nissim, Kobbi, and Stemmer, Uri. Private learning and sanitization: Pure vs. approximate

differential privacy. Theory of Computing, 12(1):1–61, 2016.

Buchbinder, Niv, Feldman, Moran, Naor, Joseph, and Schwartz, Roy. Submodular maximization with

cardinality constraints. In SODA, pp. 1433–1452, 2014.

Bun, Mark and Steinke, Thomas. Concentrated differential privacy: Simpliﬁcations, extensions, and lower

bounds. In TCC, pp. 635–658, 2016.

Bun, Mark, Nissim, Kobbi, Stemmer, Uri, and Vadhan, Salil P. Differentially private release and learning of

threshold functions. In FOCS, pp. 634–649, 2015.

C˘alinescu, Gruia, Chekuri, Chandra, P´al, Martin, and Vondr´ak, Jan. Maximizing a monotone submodular

function subject to a matroid constraint. SIAM Journal on Computing, 2011.

Chaudhuri, Kamalika, Hsu, Daniel J., and Song, Shuang. The large margin mechanism for differentially

private maximization. In NIPS, pp. 1287–1295, 2014.

Cheraghchi, Mahdi, et al. Submodular functions are noise stable. In SODA, 2012.

Dwork, Cynthia and Lei, Jing. Differential privacy and robust statistics. In STOC, pp. 371–380, 2009.

Dwork, Cynthia and Roth, Aaron. The algorithmic foundations of differential privacy. Foundations and

Trends in Theoretical Computer Science, 9(3-4):211–407, 2014.

Dwork, Cynthia, McSherry, Frank, Nissim, Kobbi, and Smith, Adam D. Calibrating noise to sensitivity in

private data analysis. In TCC, pp. 265–284, 2006.

Dwork, Cynthia, Rothblum, Guy N., and Vadhan, Salil P. Boosting and differential privacy. In FOCS, pp.

51–60, 2010.

Feige, U., Mirrokni, V., and Vondrak, J. Maximizing non-monotone submodular functions. In FOCS, 2007.

22

Feldman, Moran, Naor, Joseph, and Schwartz, Roy. A uniﬁed continuous greedy algorithm for submodular

maximization. In FOCS, 2011.

Feldman, Moran, Harshaw, Christopher, and Karbasi, Amin. Greed is good: Near-optimal submodular

maximization via greedy optimization. In COLT, 2017.

Fisher, Marshall L., Nemhauser, George L., and Wolsey, Laurence A. An analysis of approximations for

maximizing submodular set functions - II. Mathematical Programming Study, (8), 1978.

Gupta, Anupam, Ligett, Katrina, McSherry, Frank, Roth, Aaron, and Talwar, Kunal. Differentially private

combinatorial optimization. In SODA, pp. 1106–1125, 2010.

Hassidim, Avinatan and Singer, Yaron. Submodular optimization under noise. CoRR, abs/1601.03095,

2016. URL http://arxiv.org/abs/1601.03095.

Jenkyns, T. A. The efﬁcacy of the “greedy” algorithm. In South Eastern Conference on Combinatorics,

Graph Theory and Computing, 1976.

Kempe, David, Kleinberg, Jon, and Tardos, ´Eva. Maximizing the spread of inﬂuence through a social

network. In KDD, 2003.

EMNLP, 2014.

2005.

2007.

Kirchhoff, Katrin and Bilmes, Jeff. Submodularity for data selection in statistical machine translation. In

Krause, A. and Guestrin, C. Near-optimal nonmyopic value of information in graphical models. In UAI,

Krause, Andreas and Gomes, Ryan G. Budgeted nonparametric learning from data streams. In ICML, 2010.

Lin, Hui and Bilmes, Jeff. A class of submodular functions for document summarization. In ACL, 2011.

McSherry, Frank and Talwar, Kunal. Mechanism design via differential privacy.

In FOCS, pp. 94–103,

Mestre, Juli´an. Greedy in approximation algorithms. In ESA, pp. 528–539, 2006.

Mirzasoleiman, Baharan, Badanidiyuru, Ashwinkumar, Karbasi, Amin, Vondrak, Jan, and Krause, Andreas.

Lazier than lazy greedy. In AAAI, 2015.

Mirzasoleiman, Baharan, Badanidiyuru, Ashwinkumar, and Karbasi, Amin. Fast constrained submodular

maximization: Personalized data summarization. In ICML, 2016a.

Mirzasoleiman, Baharan, Zadimoghaddam, Morteza, and Karbasi, Amin. Fast distributed submodular cover:

Public-private data summarization. In NIPS, 2016b.

Nemhauser, George L., Wolsey, Laurence A., and Fisher, Marshall L. An analysis of approximations for

maximizing submodular set functions - I. Mathematical Programming, 1978.

NHANESDataset. National health and nutrition examination survey (2007 - 2014). URL https://wwwn.

cdc.gov/nchs/nhanes/default.aspx.

23

Papadimitriou, Christos H., Schapira, Michael, and Singer, Yaron. On the hardness of being truthful. In

FOCS, pp. 250–259, 2008.

Singla, Adish, Bogunovic, Ilija, Bart´ok, G´abor, Karbasi, Amin, and Krause, Andreas. Near-optimally teach-

ing the crowd to classify. In ICML, 2014.

Sipos, Ruben, Swaminathan, Adith, Shivaswamy, Pannaga, and Joachims, Thorsten. Temporal corpus sum-

marization using submodular word coverage. In CIKM, 2012.

Song, Shuang, Chaudhuri, Kamalika, and Sarwate, Anand D. Stochastic gradient descent with differentially

private updates. In GlobalSIP, pp. 245–248, 2013.

UberDataset.

Uber pickups

in new york city.

URL https://www.kaggle.com/

fivethirtyeight/uber-pickups-in-new-york-city.

24

