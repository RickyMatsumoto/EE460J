Semi-Supervised Classiﬁcation
Based on Classiﬁcation from Positive and Unlabeled Data

Tomoya Sakai 1 2 Marthinus Christoffel du Plessis Gang Niu 1 Masashi Sugiyama 2 1

Abstract
Most of the semi-supervised classiﬁcation meth-
ods developed so far use unlabeled data for reg-
ularization purposes under particular distribu-
tional assumptions such as the cluster assump-
tion. In contrast, recently developed methods of
classiﬁcation from positive and unlabeled data
(PU classiﬁcation) use unlabeled data for risk
evaluation, i.e., label information is directly ex-
tracted from unlabeled data.
In this paper, we
extend PU classiﬁcation to also incorporate neg-
ative data and propose a novel semi-supervised
classiﬁcation approach. We establish general-
ization error bounds for our novel methods and
show that the bounds decrease with respect to
the number of unlabeled data without the distri-
butional assumptions that are required in existing
semi-supervised classiﬁcation methods. Through
experiments, we demonstrate the usefulness of
the proposed methods.

1. Introduction

Collecting a large amount of labeled data is a critical bottle-
neck in real-world machine learning applications due to the
laborious manual annotation.
In contrast, unlabeled data
can often be collected automatically and abundantly, e.g.,
by a web crawler. This has led to the development of vari-
ous semi-supervised classiﬁcation algorithms over the past
decades.

To leverage unlabeled data in training, most of the exist-
ing semi-supervised classiﬁcation methods rely on partic-
ular assumptions on the data distribution (Chapelle et al.,
2006). For example, the manifold assumption supposes that
samples are distributed on a low-dimensional manifold in
the data space (Belkin et al., 2006). In the existing frame-
work, such a distributional assumption is encoded as a reg-

1The University of Tokyo, Japan 2RIKEN, Japan. Correspon-

dence to: Tomoya Sakai <sakai@ms.k.u-tokyo.ac.jp>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ularizer for training a classiﬁer and biases the classiﬁer to-
ward a better one under the assumption. However, if such a
distributional assumption contradicts the data distribution,
the bias behaves adversely, and the performance of the ob-
tained classiﬁer becomes worse than the one obtained with
supervised classiﬁcation (Cozman et al., 2003; Sokolovska
et al., 2008; Li & Zhou, 2015; Krijthe & Loog, 2017).

Recently, classiﬁcation from positive and unlabeled data
(PU classiﬁcation) has been gathering growing attention
(Elkan & Noto, 2008; du Plessis et al., 2014; 2015; Jain
et al., 2016), which trains a classiﬁer only from positive and
unlabeled data without negative data. In PU classiﬁcation,
the unbiased risk estimators proposed in du Plessis et al.
(2014; 2015) utilize unlabeled data for risk evaluation, im-
plying that label information is directly extracted from un-
labeled data without restrictive distributional assumptions,
unlike existing semi-supervised classiﬁcation methods that
utilize unlabeled data for regularization. Furthermore, the-
oretical analysis (Niu et al., 2016) showed that PU classi-
ﬁcation (or its counterpart, NU classiﬁcation, classiﬁcation
from negative and unlabeled data) is likely to outperform
classiﬁcation from positive and negative data (PN classi-
ﬁcation, i.e., ordinary supervised classiﬁcation) depending
on the number of positive, negative, and unlabeled sam-
ples. It is thus naturally expected that combining PN, PU,
and NU classiﬁcation can be a promising approach to semi-
supervised classiﬁcation without restrictive distributional
assumptions.

In this paper, we propose a novel semi-supervised classiﬁ-
cation approach by considering convex combinations of the
risk functions of PN, PU, and NU classiﬁcation. Without
any distributional assumption, we theoretically show that
the conﬁdence term of the generalization error bounds de-
creases at the optimal parametric rate with respect to the
number of positive, negative, and unlabeled samples, and
the variance of the proposed risk estimator is almost always
smaller than the plain PN risk function given an inﬁnite
number of unlabeled samples. Through experiments, we
analyze the behavior of the proposed approach and demon-
strate the usefulness of the proposed semi-supervised clas-
siﬁcation methods.

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

2. Background

2.3. PU Classiﬁcation

In this section, we ﬁrst introduce the notation commonly
used in this paper and review the formulations of PN, PU,
and NU classiﬁcation.

2.1. Notation

Rd and y

Let random variables x
be
equipped with probability density p(x, y), where d is a pos-
itive integer. Let us consider a binary classiﬁcation problem
from x to y, given three sets of samples called the posi-
tive (P), negative (N), and unlabeled (U) data:

∈ {

1
}

+1,

−

∈

XP :=
XN :=
XU :=

xP
i }
{
xN
i }
{
xU
i }
{

nP
i=1

nN
i=1

nU
i=1

i.i.d.
∼
i.i.d.
∼
i.i.d.
∼

pP(x) := p(x

y = +1),

|

pN(x) := p(x

y =

1),

|
p(x) := θPpP(x) + θNpN(x),

−

where

θP := p(y = +1),

θN := p(y =

1)

−

are the class-prior probabilities for the positive and negative
classes such that θP + θN = 1.
Let g : Rd
R be an arbitrary real-valued decision
function for binary classiﬁcation, and classiﬁcation is per-
R be a loss func-
formed based on its sign. Let ℓ : R
tion such that ℓ(m) generally takes a small value for large
margin m = yg(x). Let RP(g), RN(g), RU,P(g), and
RU,N(g) be the risks of classiﬁer g under loss ℓ:

→

→

RP(g) := EP[ℓ(g(x))],

RN(g) := EN[ℓ(
RU,P(g) := EU[ℓ(g(x))], RU,N(g) := EU[ℓ(

g(x))],
g(x))],

−
−

where EP, EN, and EU denote the expectations over
pP(x), pN(x), and p(x), respectively. Since we do not
have any samples from p(x, y),
the true risk R(g) =
Ep(x,y)[ℓ(yg(x))], which we want to minimize, should be
recovered without using p(x, y) as shown below.

2.2. PN Classiﬁcation

In standard supervised classiﬁcation (PN classiﬁcation), we
have both positive and negative data, i.e., fully labeled data.
The goal of PN classiﬁcation is to train a classiﬁer using
labeled data.

The risk in PN classiﬁcation (the PN risk) is deﬁned as

In PU classiﬁcation, we do not have labeled data for the
negative class, but we can use unlabeled data drawn from
marginal density p(x). The goal of PU classiﬁcation is
to train a classiﬁer using only positive and unlabeled data.
The basic approach to PU classiﬁcation is to discriminate P
and U data (Elkan & Noto, 2008). However, naively clas-
sifying P and U data causes a bias.

To address this problem, du Plessis et al. (2014; 2015) pro-
posed a risk equivalent to the PN risk but where pN(x) is
not included. The key idea is to utilize unlabeled data to
evaluate the risk for negative samples in the PN risk. Re-
placing the second term in Eq. (1) with1

θN EN[ℓ(

g(x))] = EU[ℓ(

g(x))]

θP EP[ℓ(

g(x))],

−

−

−

−

we obtain the risk in PU classiﬁcation (the PU risk) as

RPU(g) := θP EP[
= θPRC

ℓ(g(x))] + EU[ℓ(

g(x))]

−

P(g) + RU,N(g),

e

where RC
P(g) := EP[
is a composite loss function.

ℓ(g(x))] and

ℓ(m) = ℓ(m)

ℓ(

m)

−

−

Non-Convex Approach:

If the loss function satisﬁes

e

e

(2)

(3)

ℓ(m) + ℓ(

m) = 1,

−

the composite loss function becomes
We thus obtain the non-convex PU risk as

ℓ(m) = 2ℓ(m)

1.

−

RN-PU(g) := 2θPRP(g) + RU,N(g)

θP.

(4)

e

−

This formulation can be seen as cost-sensitive classiﬁcation
of P and U data with weight 2θP (du Plessis et al., 2014).

The ramp loss used in the robust support vector machine
(Collobert et al., 2006),

ℓR(m) :=

max(0, min(2, 1

m)),

(5)

−

1
2

satisﬁes the condition (3). However,
the use of the
ramp loss (and any other losses that satisfy the condi-
tion (3)) yields a non-convex optimization problem, which
may be solved locally by the concave-convex procedure
(CCCP) (Yuille & Rangarajan, 2002; Collobert et al., 2006;
du Plessis et al., 2014).

RPN(g) := θP EP[ℓ(g(x))] + θN EN[ℓ(

g(x))]

= θPRP(g) + θNRN(g),

−

(1)

Convex Approach:
satisﬁes

If a convex surrogate loss function

which is equal to R(g), but p(x, y) is not included. If we
use the hinge loss function ℓH(m) := max(0, 1
m), the
PN risk coincides with the risk of the support vector ma-
chine (Vapnik, 1995).

−

ℓ(m)

ℓ(

m) =

m,

−

−

−

(6)

1The equation comes from the deﬁnition of the marginal den-

sity p(x) = θPpP(x) + θNpN(x).

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

the composite loss function becomes a linear function
m (see Table 1 in du Plessis et al., 2015). We
ℓ(m) =
thus obtain the convex PU risk as
e

−

RC-PU(g) := θPRL

P(g) + RU,N(g),

P(g) := EP[

where RL
ℓLin(m) :=
mization problem that can be solved efﬁciently.

g(x)] is the risk with the linear loss
m. This formulation yields the convex opti-

−

−

2.4. NU Classiﬁcation

As a mirror of PU classiﬁcation, we can consider NU clas-
siﬁcation. The risk in NU classiﬁcation (the NU risk) is
given by

RNU(g) := θN EN[
= θNRC

N(g) + RU,P(g),

−

e

ℓ(

g(x))] + EU[ℓ(g(x))]

N(g) := EN[

where RC
g(x))] is the risk function with
the composite loss. Similarly to PU classiﬁcation, the non-
convex and convex NU risks are expressed as
e

ℓ(

−

RN-NU(g) := 2θNRN(g) + RU,P(g)
RC-NU(g) := θNRL
N(g) + RU,P(g),

θN,

−

(7)

(8)

where RL

N(g) := EN[g(x)] is the risk with the linear loss.

3. Semi-Supervised Classiﬁcation Based on

PN, PU, and NU Classiﬁcation

In this section, we propose semi-supervised classiﬁcation
methods based on PN, PU, and NU classiﬁcation.

3.1. PUNU Classiﬁcation

A naive idea to build a semi-supervised classiﬁer is to com-
bine the PU and NU risks. For γ
[0, 1], let us consider a
linear combination of the PU and NU risks:

∈

Rγ

PUNU(g) := (1

−

γ)RPU(g) + γRNU(g).

We refer to this combined method as PUNU classiﬁcation.

If we use a loss function satisfying the condition (3), the
non-convex PUNU risk Rγ
N-PUNU(g) can be expressed as

Rγ

N-PUNU(g) = 2(1

−

γ)θPRP(g) + 2γθNRN(g)

g(x)) + γℓ(g(x))]

+ EU[(1
(1

γ)ℓ(
−
γ)θP −

−

−

−
γθN.

Here, R1/2
N-PUNU(g) agrees with RPN(g) due to the condi-
tion (3). Thus, when γ = 1/2, PUNU classiﬁcation is re-
duced to ordinary PN classiﬁcation.

On the other hand, γ = 1/2 is still effective when the con-
dition (6) is satisﬁed. Its risk Rγ
C-PUNU(g) can be expressed
as

Rγ

C-PUNU(g) = (1

γ)θPRL

P(g) + γθNRL
γ)ℓ(g(x)) + γℓ(

N(g)

−
+ EU[(1

g(x))].

−

−
γ)ℓ(g(x)) + γℓ(

Here, (1
loss function for unlabeled samples with weight γ.

g(x)) can be regarded as a

−

−

When γ = 1/2, unlabeled samples incur the same loss for
the positive and negative classes. On the other hand, when
0 < γ < 1/2, a smaller loss is incurred for the negative
class than the positive class. Thus, unlabeled samples tend
to be classiﬁed into the negative class. The opposite is true
when 1/2 < γ < 1.

3.2. PNU Classiﬁcation

Another possibility of using PU and NU classiﬁcation in
semi-supervised classiﬁcation is to combine the PN and
PU/NU risks. For γ
[0, 1], let us consider linear com-
binations of the PN and PU/NU risks:

∈

Rγ
Rγ

PNPU(g) := (1
PNNU(g) := (1

−
−

γ)RPN(g) + γRPU(g),
γ)RPN(g) + γRNU(g).

In practice, we combine PNPU and PNNU classiﬁcation
and adaptively choose one of them with a new trade-off
parameter η

1, 1] as

[

∈

−

Rη

PNU(g) :=

Rη
PNPU(g)
−η
PNNU(g)
R

(

0),
(η
≥
(η < 0).

We refer to the combined method as PNU classiﬁcation.
1, 0, +1 corre-
Clearly, PNU classiﬁcation with η =
sponds to NU, PN, and PU classiﬁcation. As η gets
large/small, the effect of the positive/negative classes is
more emphasized.

−

In the theoretical analyses in Section 4, we denote the
combinations of the PN risk with the non-convex PU/NU
risks by Rγ
N-PNNU, and that with the convex
PU/NU risks by Rγ

N-PNPU and Rγ

C-PNPU and Rγ

C-PNNU.

3.3. Practical Implementation

We have so far only considered the true risks R (with
respect to the expectations over true data distributions).
When a classiﬁer is trained from samples in practice, we
use the empirical risks
R where the expectations are re-
placed with corresponding sample averages.

b

More speciﬁcally, in the theoretical analysis in Section 4
and experiments in Section 5, we use a linear-in-parameter
j=1 wjφj(x) = w⊤φ(x),
model given by g(x) =
where ⊤ denotes the transpose, b is the number of basis
P

b

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

functions, w = (w1, . . . , wb)⊤ is a parameter vector, and
φ(x) = (φ1(x), . . . , φb(x))⊤ is a basis function vector.
The parameter vector w is learned in order to minimize the
ℓ2-regularized empirical risk:

R(g) + λw⊤w,

min
w

where λ

0 is the regularization parameter.

b

≥

4. Theoretical Analyses

In this section, we theoretically analyze the behavior of
the empirical versions of the proposed semi-supervised
classiﬁcation methods. We ﬁrst derive generalization er-
ror bounds and then discuss variance reduction. Finally,
we discuss whether PUNU or PNU classiﬁcation is more
promising. All proofs can be found in Appendix A.

4.1. Generalization Error Bounds

G
=

Let

be a function class of bounded hyperplanes:

h

G

w

k ≤

k ≤

i | k

Cw,

w, φ(x)

φ(x)
k

g(x) =
{

,
Cφ}
where Cw and Cφ are certain positive constants. Since ℓ2-
regularization is always included, we can naturally assume
that the empirical risk minimizer g belongs to a certain
.
G
sign(m))/2 the zero-one loss
Denote by ℓ0-1(m) = (1
and I(g) = Ep(x,y)[ℓ0-1(yg(x))] the risk of g for binary
classiﬁcation, i.e., the generalization error of g. In the fol-
lowing, we study upper bounds of I(g) holding uniformly
. We respectively focus on the (scaled) ramp
for all g
and squared losses for the non-convex and convex methods
due to limited space. Similar results can be obtained with a
little more effort if other eligible losses are used. For con-
venience, we deﬁne a function as

∈ G

−

χ(cP, cN, cU) = cPθP/√nP + cNθN/√nN + cU/√nU.

≤

that
2R(g). Note

Non-Convex Methods: A key observation is
2ℓR(m), and consequently I(g)
ℓ0-1(m)
that by deﬁnition we have
N-PUNU(g) = Rγ

N-PNPU(g) = Rγ
The theorem below can be proven using the Rademacher
analysis (see, for example, Mohri et al., 2012; Ledoux &
Talagrand, 1991).

N-PNNU(g) = R(g).

Rγ

≤

Theorem 1 Let ℓR(m) be the loss for deﬁning the empir-
ical risks. For any δ > 0, the following inequalities hold
separately with probability at least 1

δ for all g

:

I(g)

I(g)

I(g)

Rγ
N-PUNU(g) + Cw,φ,δ ·
2
Rγ
N-PNPU(g) + Cw,φ,δ ·
2
b
Rγ
N-PNNU(g) + Cw,φ,δ ·
2
b

≤

≤

≤

where Cw,φ,δ = 2CwCφ +

b

χ(1

−
2 ln(3/δ).

−

∈ G

χ(2

2γ, 2γ,

2γ

−
χ(1 + γ, 1

|
−
γ, γ),

),

1
|

−
γ, 1 + γ, γ),

p

Theorem 1 guarantees that when ℓR(m) is used, I(g) can
be bounded from above by two times the empirical risks,
Rγ
Rγ
i.e., 2
N-PNNU(g), plus
the corresponding conﬁdence terms of order
b

N-PNPU(g), and 2

N-PUNU(g), 2

Rγ

b

b

Op(1/√nP + 1/√nN + 1/√nU).
Since nP, nN, and nU can increase independently, this is al-
ready the optimal convergence rate without any additional
assumption (Vapnik, 1998; Mendelson, 2008).

Convex Methods: Analogously, we have ℓ0-1(m)
≤
4ℓS(m) for the squared loss. However, it is too loose when
0. Fortunately, we do not have to use ℓS(m) if we
m
|
work on the generalization error rather than the estimation
error. To this end, we deﬁne the truncated (scaled) squared
loss ℓTS(m) as

| ≫

ℓTS(m) =

ℓS(m)
ℓ0-1(m)/4

(

1,

0 < m
≤
otherwise,

4ℓTS(m) is much tighter. For ℓTS(m),
so that ℓ0-1(m)
RC-PU(g) and RC-NU(g) need to be redeﬁned as follows
(see du Plessis et al., 2015):

≤

RC-PU(g) := θPR
RC-NU(g) := θNR

′
P(g) + RU,N(g),
′
N(g) + RU,P(g),

P(g) and R′

where R′
w.r.t. the composite loss
The condition
=
−
e
but the equivalence is not lost; indeed, we still have

N(g) are simply RP(g) and RN(g)
m).
m means the loss of convexity,

ℓTS(m) = ℓTS(m)

ℓTS(m)

ℓTS(

−

−

e

Rγ

C-PUNU(g) = Rγ

C-PNPU(g) = Rγ

C-PNNU(g) = R(g).

Theorem 2 Let ℓTS(m) be the loss for deﬁning the empir-
ical risks (where RC-PU(g) and RC-NU(g) are redeﬁned).
For any δ > 0, the following inequalities hold separately
with probability at least 1

δ for all g

:

−

∈ G

I(g)

I(g)

I(g)

Rγ
4
Rγ
4
b
Rγ
4
b

≤

≤

≤

C-PUNU(g) + C
C-PNPU(g) + C
C-PNNU(g) + C

′
w,φ,δ ·
′
w,φ,δ ·
′
w,φ,δ ·

χ(1

−
χ(1, 1

χ(1

−

γ, γ, 1),

γ, γ),

−
γ, 1, γ),

where C ′

w,φ,δ = 4CwCφ +

b

2 ln(4/δ).

p
Theorem 2 ensures that when ℓTS(m) is used (for evalu-
ating the empirical risks rather than learning the empirical
risk minimizers), I(g) can be bounded from above by four
times the empirical risks plus conﬁdence terms in the op-
timal parametric rate. As ℓTS(m)
ℓS(m), Theorem 2 is
valid (but weaker) if all empirical risks are w.r.t. ℓS(m).

≤

6
Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

4.2. Variance Reduction

RPN(g), i.e., whether

Our empirical risk estimators proposed in Section 3 are all
unbiased. The next question is whether their variance can
be smaller than that of
XU can help
reduce the variance in estimating R(g). To answer this
question, pick any g of interest. For simplicity, we assume
, to illustrate the maximum variance reduc-
that nU → ∞
tion that could be achieved. Due to limited space, we only
focus on the non-convex methods.

b

Similarly to RP(g) and RN(g), let σ2
corresponding variance:

P(g) and σ2

N(g) be the

σ2
P(g) := VarP[ℓ(g(x))],

σ2
N(g) := VarN[ℓ(

g(x))],

−

where VarP and VarN denote the variance over pP(x)
and pN(x). Moreover, denote by ψP = θ2
P(g)/nP
and ψN = θ2
N(g)/nN for short, and let Var be the
variance over pP(xP
1 )
nN )
p(xU
1 )

pN(xN
1 )

pN(xN

pP(xP

p(xU

Nσ2

Pσ2

nU).

nP )

· · ·

· · ·

·

·

· · ·

Theorem 3 Assume nU → ∞
Rγ
γN-PUNU = argmin

Var[

γ

. For any ﬁxed g, let

N-PUNU(g)] =

ψP
ψP + ψN

. (9)

Then, we
ther, Var[
γ
γ

Rγ
(2γN-PUNU −
∈
(1/2, 2γN-PUNU −
b
∈

b
have
γN-PUNU
N-PUNU(g)] < Var[

Fur-
for all
1/2, 1/2) if ψP < ψN, or for all
1/2) if ψP > ψN.2
b

[0, 1].
RPN(g)]

∈

b

Rγ

Theorem 3 guarantees that the variance is always reduced
by
N-PUNU(g) if γ is close to γN-PUNU, which is optimal
for variance reduction. The interval of such good γ val-
ues has the length min
. In
/(ψP + ψN), 1/2
ψN|
ψP −
}
3ψN, the length is 1/2.
ψN or ψP ≥
particular, if 3ψP ≤
Theorem 4 Assume nU → ∞
Rγ
γN-PNPU= argmin

. For any ﬁxed g, let

N-PNPU(g)] =

, (10)

Var[

{|

γN-PNNU= argmin

Var[

b
Rγ

N-PNNU(g)] =

. (11)

b
[0, 1] if ψP ≤
ψN. Additionally, Var[

Then, we have γN-PNPU ∈
[0, 1] if ψP ≥
Var[
RPN(g)]
Rγ
ψN, or Var[
b
(0, 2γN-PNNU) if ψP > ψN.
b

for all γ
N-PNNU(g)] < Var[

∈

ψN or γN-PNNU ∈
Rγ
N-PNPU(g)] <
(0, 2γN-PNPU) if ψP <
RPN(g)] for all γ

b

∈

b

Theorem 4 implies that the variance of
duced by either

Rγ
N-PNPU(g) if ψP ≤

RPN(g) is re-
N-PNNU(g)

Rγ

2Being ﬁxed means g is determined before seeing the data for
evaluating the empirical risk. For example, if g is trained by some
learning method, and the empirical risk is subsequently evaluated
on the validation/test data, g is regarded as ﬁxed in the evaluation.

b

b

ψN or
b

ψP
ψN −
ψP + ψN
ψN
ψP −
ψP + ψN

γ

γ

ψN, where γ should be close to γN-PNPU or
if ψP ≥
γN-PNNU. The range of such good γ values is of length
. In particular, if 3ψP ≤
2
min
ψP −
/(ψP + ψN), 1
}
{
|
Rγ
ψN,
(0, 1) can reduce the vari-
N-PNPU(g) given any γ
3ψN,
ance, and if ψP ≥
(0, 1)
can reduce the variance.

N-PNNU(g) given any γ

ψN|

Rγ

∈

∈

b

b
Rγ

b

b

N-PUNU(g),

As a corollary of Theorems 3 and 4,
the minimum
Rγ
variance achievable by
N-PNPU(g), and
Rγ
N-PNNU(g) at their optimal γN-PUNU, γN-PNPU, and
γN-PNNU is exactly the same, namely, 4ψPψN/(ψP + ψN).
Rγ
b
Nevertheless,
N-PNNU(g) have a much
Rγ
wider range of nice γ values than
b

N-PNPU(g) and

If we further assume that σP(g) = σN(g), the condition in
b
ψN will
Theorems 3 and 4 as to whether ψP ≤
be independent of g. Also, it will coincide with the condi-
tion in Theorem 7 in Niu et al. (2016) where the minimizers
of

ψN or ψP ≥

RNU(g) are compared.

N-PUNU(g).

RPU(g) and

RPN(g),

Rγ

b

b

b

b

−

A ﬁnal remark is that learning is uninvolved in Theorems 3
and 4, such that ℓ(m) can be any loss that satisﬁes ℓ(m) +
m) = 1, and g can be any ﬁxed decision function. For
ℓ(
instance, we may adopt ℓ0-1(m) and pick some g resulted
from some other learning methods. As a consequence, the
variance of
IPN(g) over the validation data can be reduced,
and then the cross-validation should be more stable, given
that nU is sufﬁciently large. Therefore, even without being
minimized, our proposed risk estimators are themselves of
practical importance.

b

4.3. PUNU vs. PNU Classiﬁcation

We discuss here which approach, PUNU or PNU classiﬁ-
cation, is more promising according to state-of-the-art the-
oretical comparisons (Niu et al., 2016), which are based on
estimation error bounds.

b

b

b

b

gPU, and

gPU) (R(

gPN) < R(

gPN) < R(

gNU be the minimizers of

Let
gPN,
RPN(g),
Let αPU,PN :=
RNU(g), respectively.
RPU(g), and
b
(θP/√nP + 1/√nU)/(θN/√nN) and αNU,PN
:=
(θN/√nN + 1/√nU)/(θP/√nP). The ﬁnite-sample com-
b
parisons state that if αPU,PN > 1 (αNU,PN > 1), PN clas-
siﬁcation is more promising than PU (NU) classiﬁcation,
gNU)); otherwise
i.e., R(
PU (NU) classiﬁcation is more promising than PN classiﬁ-
cation (cf. Section 3.2 in Niu et al., 2016).
b
b

b
Suppose that nU is not sufﬁciently large against nP and nN.
According to the ﬁnite-sample comparisons, PN classiﬁca-
tion is most promising, and either PU or NU classiﬁcation
is the second best, i.e., R(
gNU)
gPU). On the other hand,
or R(
if nU is sufﬁciently large (nU → ∞
, which is faster
b
), we have the asymptotic compar-
than nP, nN → ∞
b
b
isons: α∗
NU,PN =

PU,PN = limnP,nN,nU→∞ αPU,PN, α∗

gNU) < R(

gPN) < R(

gPU) < R(

gPN) < R(

b

b

b

b

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

b

b

gPN) < R(

gPU) < R(

NU,PN < 1, R(

α∗
limnP,nN,nU→∞ αNU,PN, and α∗
NU,PN = 1. From
PU,PN ·
the last equation, if α∗
PU,PN < 1, then α∗
NU,PN > 1, imply-
ing that PU (PN) classiﬁcation is more promising than PN
(NU) classiﬁcation, i.e., R(
gNU).
Similarly, when α∗
gNU) <
gPU) (cf. Section 3.3 in Niu et al., 2016).
gPN) < R(
R(
b

PU,PN > 1 and α∗
b
In real-world applications, since we do not know whether
the number of unlabeled samples is sufﬁciently large or not,
a practical approach is to combine the best methods in both
the ﬁnite-sample and asymptotic cases. PNU classiﬁcation
is the combination of the best methods in both cases, but
PUNU classiﬁcation is not.
In addition, PUNU classiﬁ-
cation includes the worst one in its combination in both
cases. From this viewpoint, PNU classiﬁcation would be
more promising than PUNU classiﬁcation, as demonstrated
in the experiments shown in the next section.

b

b

5. Experiments

In this section, we ﬁrst numerically analyze the proposed
approach and then compare the proposed semi-supervised
classiﬁcation methods against existing methods. All ex-
periments were carried out using a PC equipped with two
2.60GHz Intel® Xeon® E5-2640 v3 CPUs.

5.1. Experimental Analyses

Here, we numerically analyze the behavior of our proposed
approach. Due to limited space, we show results on two out
of six data sets and move the rest to Appendix C.

x

−

−k

n
i=1 =

n
i=1 wi exp(
wi}
{

Common Setup: As a classiﬁer, we use the Gaussian
2/(2σ2)),
kernel model: g(x) =
xik
n
i=1 are the parameters,
where n = nP + nN,
P
xi}
XP ∪XN, and σ > 0 is the Gaussian bandwidth.
{
The bandwidth candidates are
} ×
n
xi −
i,j=1). The classiﬁer trained by mini-
median(
k
mizing the empirical PN risk is denoted by
gPN. The num-
ber of labeled samples for training is 20, where the class-
prior was 0.5. In all experiments, we used the squared loss
for training. We note that the class-prior of test data was
the same as that of unlabeled data.

1/8, 1/4, 1/2, 1, 3/2, 2
{

xjk

b

Variance Reduction in Practice: Here, we numerically
investigate how many unlabeled samples are sufﬁcient in
practice such that the variance of the empirical PNU risk
is smaller than that of the PN risk: Var[
PNU(g)] <
RPN(g)] given a ﬁxed classiﬁer g.
Var[

Rη

b

b
As the ﬁxed classiﬁer, we used the classiﬁer
gPN, where
the hyperparameters were determined by ﬁve-fold cross-
validation. To compute the variance of the empirical PN
gPN)], we
and PNU risks, Var[
P = 10 positive, nV
repeatedly drew additional nV
N = 10
b

gPN)] and Var[

b
PNU(

RPN(

Rη

b

b

b

e
c
n
a
i
r
a
V

f
o

o
i
t
a
R

1.4

1.2

1

0.8

0.6

θP = 0.3
θP = 0.5
θP = 0.7

e
c
n
a
i
r
a
V

f
o

o
i
t
a
R

1.4

1.2

1

0.8

0.6

0.4

0

50

100 150 200 250 300

0

50

100 150 200 250 300

nv
U

nv
U

(a) Phoneme (d = 5)

(b) Magic (d = 10)

b
Rη

PNU(bgPN)]/ Var[

Figure1. Average and standard error of
the ratio between
the variance of empirical PNU risk and that of PN risk,
b
RPN(bgPN)], as a function of the number
Var[
of unlabeled samples over 100 trials. Although the variance re-
duction is proved for an inﬁnite number of samples, it can be ob-
served with a ﬁnite number of samples.

negative, and nV
U unlabeled samples from the rest of the
data set. The additional samples were also used for ap-
proximating
gPN) to compute η, i.e., γ in
σN(
Eqs.(10) and (11).

gPN) and

σP(

b

b

b

b

b

Rη

RPN(

PNU(

gPN)]/ Var[

Figure 1 shows the ratio between the variance of
the empirical PNU risk and that of
the PN risk,
gPN)]. The number of unla-
Var[
beled samples for validation nV
U increases from 10 to 300.
We see that with a rather small number of unlabeled sam-
b
ples, the ratio becomes less than 1. That is, the variance
of the empirical PNU risk becomes smaller than that of the
PN risk. This implies that although the variance reduction
is proved for an inﬁnite number of unlabeled samples, it can
be observed under a ﬁnite number of samples in practice.

b

b

≈

Compared to when θP = 0.3 and 0.7, the effect of variance
reduction is small when θP = 0.5. This is because if we
nN and θP = 0.5,
σN(g), when nP ≈
assume σP(g)
ψN.
we have γN-PNPU ≈
γN-PNNU ≈
See Theorem 4). That is, the PNU risk is dominated by
Rη
the PN risk, implying that Var[
RPN(g)].
Note that the class-prior is not the only factor for vari-
nN, and
ance reduction; for example, if θP = 0.5, nP ≫
b
ψN)
σP(g)
and the variance reduction will be large.

b
σN(g), then γN-PNPU 6≈

0 (because ψP ≪

0 (because ψP ≈

PNU(g)]

Var[

≈

≈

PNU Risk in Validation: As discussed in Section 4, the
empirical PNU risk will be a reliable validation score due
to its having smaller variance than the empirical PN risk.
We show here that the empirical PNU risk is a promising
alternative to a validation score.

To focus on the effect of validation scores only, we trained
two classiﬁers by using the same risk, e.g, the empirical
PN risk. We then tune the classiﬁers with the empirical
gPNU
PN and PNU risks denoted by
PN , respectively.
The number of validation samples was the same as in the
previous experiment.

gPN
PN and

b

b

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

θP = 0.3
θP = 0.5
θP = 0.7

e
t
a
R
n
o
i
t
a
c
ﬁ

i
s
s
a
l
c
s
i

M

f
o

o
i
t
a
R

1.04

1.02

1

0.98

0.96

e
t
a
R
n
o
i
t
a
c
ﬁ

i
s
s
a
l
c
s
i

M

f
o

o
i
t
a
R

1.04

1.02

1

0.98

0.96

0.94

103

102

101

100
10−1

]
.
c
e
s
[

i

e
m
T
n
o
i
t
a
t
u
p
m
o
C

0

50

100 150 200 250 300

0

50

100 150 200 250 300

nv
U

nv
U

(a) Phoneme (d = 5)

(b) Magic (d = 10)

Figure2. Average and standard error of the ratio between the mis-
classiﬁcation rates of bgPNU
PN as a function of unlabeled
samples over 1000 trials. In many cases, the ratio becomes less
than 1, implying that the PNU risk is a promising alternative to
the standard PN risk in validation if unlabeled data are available.

PN and bgPN

PNU

PUNU

ER

LapSVM

SMIR

WellSVM

S4VM

Banana
Phoneme

Magic

Image

Susy

ijcnn1

Waveform
German

g50c

Spambase
covtype

phishing
Splice

a9a

Coil2

w8a

Figure3. Average computation time over 50 trials for benchmark
data sets when nL = 50.

b

b

gPNU
PN and that of

gPNU
PN achieves better performance than

Figure 2 shows the ratio between the misclassiﬁcation rate
gPN
of
PN. The number of unlabeled sam-
ples for validation increases from 10 to 300. With a rather
small number of unlabeled samples, the ratio becomes less
gPN
than 1, i.e.,
PN.
gPNU
PN improved sub-
In particular, when θP = 0.3 and 0.7,
stantially; the large improvement tends to give the large
variance reduction (cf. Figure 1). This result shows that
the use of the empirical PNU risk for validation improved
the classiﬁcation performance given a relatively large size
of unlabeled data.

b

b

b

5.2. Comparison with Existing Methods

Next, we numerically compare the proposed methods
against existing semi-supervised classiﬁcation methods.

Common Setup: We compare our methods against ﬁve
conventional semi-supervised classiﬁcation methods: en-
tropy regularization (ER) (Grandvalet & Bengio, 2004),
the Laplacian support vector machine (LapSVM) (Belkin
et al., 2006; Melacci & Belkin, 2011), squared-loss mu-
tual information regularization (SMIR) (Niu et al., 2013),
the weakly labeled support vector machine (WellSVM) (Li
et al., 2013), and the safe semi-supervised support vector
machine (S4VM) (Li & Zhou, 2015).

Among the proposed methods, PNU classiﬁcation and

Table1. Average and standard error of the misclassiﬁcation rates
of each method over 50 trials for benchmark data sets. Boldface
numbers denote the best and comparable methods in terms of av-
erage misclassiﬁcations rate according to a t-test at a signiﬁcance
level of 5%. The bottom row gives the number of best/comparable
cases of each method.

ER

PNU

PUNU

d = 21

ijcnn1
d = 22

Susy
d = 18

Magic
d = 10

Image
d = 18

German
d = 20

Phoneme
d = 5

Data set nL
Banana
d = 2

LapSVM SMIR WellSVM S4VM
10 30.1 (1.0) 32.1 (1.1) 35.8 (1.0) 36.9 (1.0) 37.7 (1.1) 41.8 (0.6) 45.3 (1.0)
50 19.0 (0.6) 26.4 (1.2) 20.6 (0.7) 21.3 (0.7) 21.1 (1.0) 42.6 (0.5) 38.7 (0.9)
10 32.5 (0.8) 33.5 (1.0) 33.4 (1.2) 36.5 (1.5) 36.4 (1.2) 28.4 (0.6) 33.7 (1.4)
50 28.1 (0.5) 32.8 (0.9) 27.8 (0.6) 27.0 (0.8) 28.6 (1.0) 26.8 (0.4) 25.1 (0.2)
10 31.7 (0.8) 34.1 (0.9) 34.2 (1.1) 37.9 (1.3) 36.0 (1.2) 30.1 (0.8) 33.3 (0.9)
50 29.9 (0.8) 33.4 (0.9) 30.9 (0.5) 31.0 (0.9) 30.8 (0.9) 28.8 (0.8) 29.2 (0.4)
10 29.8 (0.9) 31.7 (0.8) 33.7 (1.1) 36.6 (1.2) 36.7 (1.2) 34.7 (1.1) 35.9 (1.0)
50 20.7 (0.8) 26.6 (1.1) 20.8 (0.8) 20.3 (1.0) 20.9 (0.9) 27.2 (1.0) 23.2 (0.7)
10 44.6 (0.6) 45.0 (0.6) 47.7 (0.4) 48.2 (0.4) 45.1 (0.7) 48.0 (0.3) 46.8 (0.3)
50 38.9 (0.6) 41.5 (0.6) 37.9 (0.7) 43.1 (0.6) 43.9 (0.8) 43.8 (0.7) 42.1 (0.4)
10 40.8 (0.9) 42.4 (0.7) 43.6 (0.9) 45.9 (0.7) 46.2 (0.8) 42.4 (0.8) 42.0 (0.7)
50 36.2 (0.8) 39.0 (0.8) 38.9 (0.6) 40.6 (0.6) 38.4 (1.1) 38.5 (1.0) 34.9 (0.5)
Waveform 10 17.4 (0.6) 18.0 (0.9) 18.5 (0.6) 24.9 (1.4) 18.0 (1.0) 16.7 (0.6) 20.8 (0.8)
50 16.3 (0.6) 23.7 (1.2) 14.2 (0.4) 18.1 (0.8) 15.4 (0.6) 15.5 (0.5) 15.3 (0.3)
10 43.6 (0.6) 40.3 (1.0) 49.7 (0.1) 49.2 (0.3) 44.0 (1.0) 45.9 (0.7) 49.3 (0.8)
50 34.5 (0.8) 37.1 (0.9) 35.5 (0.8) 33.4 (1.1) 49.4 (0.3) 46.2 (0.8) 48.6 (0.4)
10 11.4 (0.6) 12.5 (0.6) 23.3 (2.3) 39.8 (1.6) 21.9 (1.3) 6.6 (0.4) 27.0 (1.4)
8.7 (0.4) 22.5 (1.5) 10.6 (0.6) 7.4 (0.4) 12.1 (0.5)
50 12.5 (1.1) 10.1 (0.6)
10 46.2 (0.4) 46.0 (0.4) 46.0 (0.5) 47.1 (0.5) 47.9 (0.5) 46.9 (0.6) 46.4 (0.4)
covtype
50 41.3 (0.5) 42.3 (0.5) 41.0 (0.4) 41.5 (0.5) 46.2 (0.8) 43.6 (0.6) 40.8 (0.4)
d = 54
Spambase 10 27.2 (0.9) 28.1 (1.1) 31.8 (1.4) 39.7 (1.4) 30.9 (1.3) 23.8 (0.8) 36.1 (1.5)
50 23.4 (1.0) 26.6 (1.0) 22.1 (0.7) 28.5 (1.3) 20.9 (0.5) 19.1 (0.4) 24.5 (0.9)
d = 57
10 38.3 (0.8) 39.3 (0.8) 43.9 (0.8) 47.9 (0.5) 41.6 (0.7) 42.0 (1.0) 42.4 (0.6)
50 30.6 (0.8) 34.7 (0.9) 30.9 (0.8) 38.8 (1.0) 30.6 (0.9) 40.9 (0.8) 35.9 (0.7)
10 24.2 (1.2) 25.8 (1.0) 27.3 (1.6) 37.2 (1.6) 27.6 (1.6) 27.5 (1.4) 31.7 (1.3)
50 15.8 (0.6) 18.3 (0.8) 15.4 (0.5) 21.1 (1.3) 14.7 (0.8) 17.2 (0.7) 16.7 (0.8)
10 31.4 (0.9) 31.3 (1.0) 34.3 (1.2) 41.0 (1.1) 37.3 (1.3) 33.1 (1.2) 34.3 (1.2)
50 27.9 (0.6) 29.9 (0.8) 28.6 (0.7) 33.3 (1.0) 26.9 (0.7) 28.9 (0.8) 26.2 (0.4)
10 38.7 (0.8) 40.1 (0.8) 42.8 (0.7) 43.9 (0.8) 43.2 (0.8) 39.1 (0.9) 44.0 (0.8)
50 23.2 (0.6) 30.5 (0.9) 23.6 (0.9) 22.8 (0.9) 25.1 (0.9) 22.6 (0.8) 25.4 (0.8)
10 35.9 (0.9) 33.6 (1.0) 41.6 (1.0) 46.6 (0.8) 39.4 (0.9) 42.1 (0.8) 43.0 (0.8)
50 28.1 (0.7) 27.6 (0.6) 27.0 (0.9) 38.7 (0.8) 28.0 (0.9) 33.7 (0.8) 35.2 (1.0)

phishing
d = 68

w8a
d = 300

Coil2
d = 241

a9a
d = 83

Splice
d = 60

g50c
d = 50

#Best/Comp.

23

13

11

4

9

13

7

PUNU classiﬁcation with the squared loss were tested.3

Data Sets: We used sixteen benchmark data sets taken
from the UCI Machine Learning Repository (Lichman,
2013), the Semi-Supervised Learning book (Chapelle et al.,
2006), the LIBSVM (Chang & Lin, 2011), the ELENA
Project,4 and a paper by Chapelle & Zien (2005).5 Each
feature was scaled to [0, 1]. Similarly to the setting in Sec-
tion 5.1, we used the Gaussian kernel model for all meth-
n
xi}
XP ∪ XN ∪ XU, where
ods. The training data is
i=1 =
{
n = nP + nN + nU. We selected all hyper-parameters with
validation samples of size 20 (nV
N = 10). For train-
ing, we drew nL labeled and nU = 300 unlabeled samples.
The class-prior of labeled data was set at 0.7 and that of un-
labeled samples was set at θP = 0.5 that were assumed to
be known. In practice, the class-prior, θP, can be estimated

P = nV

3In preliminary experiments, we tested other loss functions
such as the ramp and logistic losses and concluded that the dif-
ference in loss functions did not provide noticeable difference.

4https://www.elen.ucl.ac.be/neural-
nets/Research/Projects/ELENA/elena.htm
5http://olivier.chapelle.cc/lds/

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

PNU

ER

LapSVM

Table2. Average and standard error of misclassiﬁcation rates over
30 trials for the Places 205 data set. Boldface numbers denote the
best and comparable methods in terms of the average misclassiﬁ-
cation rate according to a t-test at a signiﬁcance level of 5%.

Data set

nU

θP

PNU

ER

LapSVM

SMIR

WellSVM

b
θP

Arts

Deserts

Fields

Stadiums

Platforms

Temples

1000 0.50 0.49 (0.01) 27.4 (1.3) 26.6 (0.5) 26.1 (0.7) 40.1 (3.9) 27.5 (0.5)
5000 0.50 0.50 (0.01) 24.8 (0.6) 26.1 (0.5) 26.1 (0.4) 30.1 (1.6)
N/A
10000 0.50 0.52 (0.01) 25.6 (0.7) 25.4 (0.5) 25.5 (0.6)
N/A
N/A
1000 0.73 0.67 (0.01) 13.0 (0.5) 15.3 (0.6) 16.7 (0.8) 17.2 (0.8) 18.2 (0.7)
5000 0.73 0.67 (0.01) 13.4 (0.4) 13.3 (0.5) 16.6 (0.6) 24.4 (0.6)
N/A
10000 0.73 0.68 (0.01) 13.3 (0.5) 13.7 (0.6) 16.8 (0.8)
N/A
N/A
1000 0.65 0.57 (0.01) 22.4 (1.0) 26.2 (1.0) 26.6 (1.3) 28.2 (1.1) 26.6 (0.8)
5000 0.65 0.57 (0.01) 20.6 (0.5) 22.6 (0.6) 24.7 (0.8) 29.6 (1.2)
N/A
10000 0.65 0.57 (0.01) 21.6 (0.6) 22.5 (0.6) 25.0 (0.9)
N/A
N/A
1000 0.50 0.50 (0.01) 11.4 (0.4) 11.5 (0.5) 12.5 (0.5) 17.4 (3.6) 11.7 (0.4)
5000 0.50 0.50 (0.01) 11.0 (0.5) 10.9 (0.3) 11.1 (0.3) 13.4 (0.7)
N/A
10000 0.50 0.51 (0.00) 10.7 (0.3) 10.9 (0.3) 11.2 (0.2)
N/A
N/A
1000 0.27 0.33 (0.01) 21.8 (0.5) 23.9 (0.6) 24.1 (0.5) 30.1 (2.3) 26.2 (0.8)
5000 0.27 0.34 (0.01) 23.3 (0.8) 24.4 (0.7) 24.9 (0.7) 26.6 (0.3)
N/A
10000 0.27 0.34 (0.01) 21.4 (0.5) 24.3 (0.6) 24.8 (0.5)
N/A
N/A
1000 0.55 0.51 (0.01) 43.9 (0.7) 43.9 (0.6) 43.4 (0.6) 50.7 (1.6) 44.3 (0.5)
5000 0.55 0.54 (0.01) 43.4 (0.9) 43.0 (0.6) 43.1 (1.0) 43.6 (0.7)
N/A
10000 0.55 0.50 (0.01) 45.2 (0.8) 44.4 (0.8) 44.2 (0.7)
N/A
N/A

by methods proposed, e.g., by Blanchard et al. (2010), Ra-
maswamy et al. (2016), or Kawakubo et al. (2016).

Table 1 lists the average and standard error of the mis-
classiﬁcation rates over 50 trials and the number of
best/comparable performances of each method in the bot-
tom row. The superior performance of PNU classiﬁcation
over PUNU classiﬁcation agrees well with the discussion
in Section 4.3. With the g50c data set, which well sat-
isﬁes the low-density separation principle, the WellSVM
achieved the best performance. However, in the Banana
data set, where the two classes are highly overlapped, the
performance of WellSVM was worse than the other meth-
ods. In contrast, PNU classiﬁcation achieved consistently
better/comparable performance and its performance did
not degenerate considerably across data sets. These re-
sults show that the idea of using PU classiﬁcation in semi-
supervised classiﬁcation is promising.

Figure 3 plots the computation time, which shows that the
fastest computation was achieved using the proposed meth-
ods with the square loss.

Image Classiﬁcation: Finally, we used the Places 205
data set (Zhou et al., 2014), which contains 2.5 million im-
ages in 205 scene classes. We used a 4096-dimensional fea-
ture vector extracted from each image by AlexNet under the
framework of Caffe,6 which is available on the project web-
site7. We chose two similar scenes to construct binary clas-
siﬁcation tasks (see the description of data sets in Appendix
B.3). We drew 100 labeled and nU unlabeled samples from
each task; the class-prior of labeled and unlabeled data
were respectively set at 0.5 and θP = mP/(mP + mN),
where mP and mN respectively denote the number of total
samples in positive and negative scenes. We used a linear

6http://caffe.berkeleyvision.org/
7http://places.csail.mit.edu/

]
.
c
e
s
[

i

e
m
T
n
o
i
t
a
t
u
p
m
o
C

104

103

102

Arts

Deserts

Fields

Stadiums Platforms Temples

Figure4. Average computation time over 30 trials for the Places
205 data set when nU = 10000.

classiﬁer g(x) = w⊤x + w0, where w is the weight vector
and w0 is the offset (in the SMIR, the linear kernel model
is used; see Niu et al. (2013) for details).

We selected hyper-parameters in PNU classiﬁcation by ap-
plying ﬁve-fold cross-validation with respect to R ¯η
PNU(g)
with the zero-one loss, where ¯η was set at Eq.(10) or
Eq.(11) with σP(g) = σN(g). The class-prior p(y =
+1) = θP was estimated using the method based on en-
ergy distance minimization (Kawakubo et al., 2016).

Table 2 lists the average and standard error of the misclas-
siﬁcation rates over 30 trials, where methods taking more
than 2 hours were omitted and indicated as N/A. The results
show that PNU classiﬁcation was most effective. The av-
erage computation times are shown in Figure 4, revealing
again that PNU classiﬁcation was the fastest method.

6. Conclusions

In this paper, we proposed a novel semi-supervised clas-
siﬁcation approach based on classiﬁcation from positive
and unlabeled data. Unlike most of the conventional meth-
ods, our approach does not require strong assumptions on
the data distribution such as the cluster assumption. We
theoretically analyzed the variance of risk estimators and
showed that unlabeled data help reduce the variance with-
out the conventional distributional assumptions. We also
established generalization error bounds and showed that
the conﬁdence term decreases with respect to the num-
ber of positive, negative, and unlabeled samples without
the conventional distributional assumptions in the optimal
parametric order. We experimentally analyzed the behavior
of the proposed methods and demonstrated that one of the
proposed methods, termed PNU classiﬁcation, was most
effective in terms of both classiﬁcation accuracy and com-
putational efﬁciency. It was recently pointed out that PU
classiﬁcation can behave undesirably for very ﬂexible mod-
els and a modiﬁed PU risk has been proposed (Kiryo et al.,
2017). Our future work is to develop a semi-supervised
classiﬁcation method based on the modiﬁed PU classiﬁca-
tion.

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

Acknowledgements

TS was supported by JSPS KAKENHI 15J09111. GN was
supported by the JST CREST program and Microsoft Re-
search Asia. MCdP and MS were supported by the JST
CREST program.

References

Belkin, M., Niyogi, P., and Sindhwani, V. Manifold reg-
ularization: A geometric framework for learning from
labeled and unlabeled examples. Journal of Machine
Learning Research, 7:2399–2434, 2006.

Kiryo, R., Niu, G., du Plessis, M. C., and Sugiyama, M.
Positive-unlabeled learning with non-negative risk esti-
mator. arXiv preprint arXiv:1703.00593, 2017.

Krijthe, J. H. and Loog, M. Robust semi-supervised least
squares classiﬁcation by implicit constraints. Pattern
Recognition, 63:115–126, 2017.

Ledoux, M. and Talagrand, M. Probability in Banach
Spaces: Isoperimetry and Processes. Springer, 1991.

Li, Y.-F. and Zhou, Z.-H. Towards making unlabeled data
never hurt. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 37(1):175–188, 2015.

Blanchard, G., Lee, G., and Scott, C. Semi-supervised nov-
elty detection. Journal of Machine Learning Research,
11:2973–3009, 2010.

Li, Y.-F., Tsang, I. W., Kwok, J. T., and Zhou, Z.-H. Con-
vex and scalable weakly labeled SVMs. Journal of Ma-
chine Learning Research, 14(1):2151–2188, 2013.

Chang, C.-C. and Lin, C.-J. LIBSVM: A library for sup-
port vector machines. ACM Transactions on Intelli-
gent Systems and Technology, 2:27:1–27:27, 2011. Soft-
ware available at http://www.csie.ntu.edu.
tw/~cjlin/libsvm.

Chapelle, O. and Zien, A. Semi-supervised classiﬁcation
by low density separation. In AISTATS, pp. 57–64, 2005.

Chapelle, O., Schölkopf, B., and Zien, A. (eds.). Semi-

Supervised Learning. MIT Press, 2006.

Collobert, R., Sinz, F., Weston, J., and Bottou, L. Trading
convexity for scalability. In ICML, pp. 201–208, 2006.

Cozman, F. G., Cohen, I., and Cirelo, M. C.

supervised learning of mixture models.
99–106, 2003.

Semi-
In ICML, pp.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Melacci, S. and Belkin, M. Laplacian support vector ma-
chines trained in the primal. Journal of Machine Learn-
ing Research, 12:1149–1184, 2011.

Mendelson, S. Lower bounds for the empirical minimiza-
tion algorithm. IEEE Transactions on Information The-
ory, 54(8):3797–3803, 2008.

Mohri, M., Rostamizadeh, A., and Talwalkar, A. Founda-

tions of Machine Learning. MIT Press, 2012.

Niu, G., Jitkrittum, W., Dai, B., Hachiya, H., and
Sugiyama, M. Squared-loss mutual information regular-
ization: A novel information-theoretic approach to semi-
In ICML, volume 28, pp. 10–18,
supervised learning.
2013.

du Plessis, M. C., Niu, G., and Sugiyama, M. Analysis of
learning from positive and unlabeled data. In NIPS, pp.
703–711, 2014.

Niu, G., du Plessis, M. C., Sakai, T., Ma, Y., and Sugiyama,
M. Theoretical comparisons of positive-unlabeled learn-
ing against positive-negative learning. In NIPS, 2016.

du Plessis, M. C., Niu, G., and Sugiyama, M. Convex for-
mulation for learning from positive and unlabeled data.
In ICML, volume 37, pp. 1386–1394, 2015.

Ramaswamy, H. G., Scott, C., and Tewari, A. Mixture pro-
portion estimation via kernel embedding of distributions.
In ICML, 2016.

Elkan, C. and Noto, K. Learning classiﬁers from only posi-
tive and unlabeled data. In SIGKDD, pp. 213–220, 2008.

Grandvalet, Y. and Bengio, Y. Semi-supervised learning by
entropy minimization. In NIPS, pp. 529–536, 2004.

Jain, S., White, M., and Radivojac, P. Estimating the class
prior and posterior from noisy positives and unlabeled
data. In NIPS, 2016.

Kawakubo, H., du Plessis, M. C., and Sugiyama, M. Com-
putationally efﬁcient class-prior estimation under class
balance change using energy distance. IEICE Transac-
tions on Information and Systems, E99-D(1):176–186,
2016.

Sokolovska, N., Cappé, O., and Yvon, F. The asymptotics
of semi-supervised learning in discriminative probabilis-
tic models. In ICML, pp. 984–991, 2008.

Vapnik, V. N. Statistical Learning Theory. John Wiley &

Sons, 1998.

Vapnik, V.N. The Nature of Statistical Learning Theory.

Springer-Verlag, New York, NY, USA, 1995.

Yuille, A. L. and Rangarajan, A. The concave-convex pro-

cedure (CCCP). In NIPS, pp. 1033–1040, 2002.

Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., and Oliva,
A. Learning deep features for scene recognition using
places database. In NIPS, pp. 487–495, 2014.

