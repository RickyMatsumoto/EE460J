Selective Inference for Sparse High-Order Interaction Models

Shinya Suzumura 1 Kazuya Nakagawa 1 Yuta Umezu 1 Koji Tsuda 2 3 Ichiro Takeuchi 1 3

Abstract

Finding statistically signiﬁcant high-order inter-
actions in predictive modeling is important but
challenging task because the possible number of
high-order interactions is extremely large (e.g.,
> 1017).
In this paper we study feature se-
lection and statistical inference for sparse high-
order interaction models. Our main contribution
is to extend recently developed selective infer-
ence framework for linear models to high-order
interaction models by developing a novel algo-
rithm for efﬁciently characterizing the selection
event for the selective inference of high-order in-
teractions. We demonstrate the effectiveness of
the proposed algorithm by applying it to an HIV
drug response prediction problem.

1. Introduction

Finding statistically reliable high-order interaction features
in predictive modeling has been important challenging task.
For example, in a biomedical study, co-occurrence of mul-
tiple mutations in multiple genes may have a signiﬁcant in-
ﬂuence on a response to a drug even if occurrence of single
mutation in each of these genes has no inﬂuence (Mano-
lio & Collins, 2006; Cordell, 2009). A major challenge
in prediction modeling with high-order interaction features
is the exponentially expanded feature space. If one has a
dataset with d original variables and takes into account in-
(cid:1)
(cid:0)d
teractions up to order r, the model has D := (cid:80)r
ρ
features (e.g., for d = 10, 000, r = 5, D > 1017). Unless
both d and r are fairly small, D is extremely large. Fea-
ture selection and statistical inference in such an extremely
high-dimensional model are challenging both computation-
ally and statistically.

ρ=1

A common approach to high-dimensional modeling is to
consider a sparse model, i.e., a model only with a selected

1Nagoya Institute of Technology, Nagoya, Japan 2University
of Tokyo, Tokyo, Japan 3RIKEN, Tokyo, Japan. Correspondence
to: Ichiro Takeuchi <takeuchi.ichiro@nitech.ac.jp>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure 1. Example of the tree structure among high-order interac-
tion features when d = 4 and r = 3.

subset of features. In the past two decades, considerable
amount of studies have been done on sparse modeling and
feature selection in high-dimensional models.
In these
studies, a variety of feature selection algorithms such as
marginal screening (Fan & Lv, 2008), orthogonal match-
ing pursuit (Pati et al., 1993), LASSO (Tibshirani, 1996),
and their various extensions have been developed. On the
other hand, statistical inference for sparse models (hypoth-
esis testing or conﬁdence interval computation of the ﬁt-
ted coefﬁcients) have not been deeply studied until very
recently. The main challenge in statistical inference of
sparse models is that, if the data is used for selecting a
subset of features, this selection event must be taken into
account in the following inference stage. Otherwise, the
inference results are distorted by so-called selection bias,
and false positive errors cannot be controlled at desired
levels. This problem is refereed to as selective inference
or post selection inference (Benjamini & Yekutieli, 2005;
Benjamini et al., 2009; Berk et al., 2013). After the seminal
work by Lee et al. (2016), signiﬁcant progress has been re-
cently made on selective inference for sparse linear models
(Fithian et al., 2014b; Lee & Taylor, 2014; Fithian et al.,
2015; Tian & Taylor, 2015; Taylor & Tibshirani, 2016;
Yang et al., 2016; Barber & Cand`es, 2016).

In this paper, we study feature selection and statistical in-
ference for sparse high-order interaction models. Unfortu-
nately, neither existing feature selection methods nor ex-
isting selective inference methods can be applied to sparse
high-order interaction models because the computational
costs of these existing methods at least linearly depend on
the number of features D. The main contribution in this
paper is to develop computationally efﬁcient algorithms for
these two tasks when the original variables are represented
in [0, 1]d. Our main idea is to exploit the underlying tree
structure of high-order interaction features as depicted in

Selective Inference for Sparse High-Order Interaction Models

Figure 1.
In feature selection tasks, it allows us to efﬁ-
ciently identify interaction features that have no chance to
be selected. In statistical inference tasks, it allows us to ef-
ﬁciently identify interaction features that do not affect the
results of the selective inference.

We demonstrate the effectiveness of the proposed methods
through numerical experiments both on synthetic and real
datasets.
In the latter, we apply the proposed method to
HIV dataset in (Rhee et al., 2003), where the goal is to iden-
tify statistically signiﬁcant high-order interactions of mul-
tiple gene mutations that are signiﬁcantly associated with
HIV drug responses.

Related works and our contributions Methods for efﬁ-
ciently ﬁnding high-order interaction features and properly
evaluating their statistical signiﬁcances have long been de-
sired in many scientiﬁc studies.

In the past decade, feature selection for interaction mod-
els has been studied in the context of sparse learning (Choi
et al., 2010; Hao & Zhang, 2014; Bien et al., 2013). None
of these works have a special computational trick for han-
dling exponentially large number of interaction features,
which makes their empirical evaluations restricted up-to
second order interactions. One commonly used heuristic
in the context of interaction modeling is to introduce a
prior knowledge such as strong heredity assumption where,
e.g., an interaction term z1z2 would be selected only when
both of z1 and z2 are selected. Such a heuristic restric-
tion is helpful for reducing the number of interaction terms
to be considered. However, in many scientiﬁc studies, re-
searchers are primarily interested in ﬁnding interactions
even when their main effects alone do not have any associa-
tion with the response. The idea of considering a tree struc-
ture among interaction features has been commonly used n
data mining literature (Kudo et al., 2005; Saigo et al., 2006;
Nakagawa et al., 2016). However, it is difﬁcult to properly
assess the statistical signiﬁcances of the selected features
by these mining techniques.

One traditional approach to assessing the statistical signif-
icances of selected features is multiple testing correction
(MTC). In the context of DNA microarray studies, many
MTC procedures for high-dimensional data have been pro-
posed (Tusher et al., 2001; Dudoit et al., 2003). An MTC
approach for statistical evaluation of high-order interac-
tion features was recently studied in (Terada et al., 2013;
Llinares-L´opez et al., 2015). A main drawback of MTC is
that they are highly conservative when the number of candi-
date features increases. Another common approach is data-
splitting (DS). (Fithian et al., 2014a). In DS approach, we
split the data into two subsets, and use one for feature se-
lection and another for statistical inference, which enables
us to remove the selection bias. However, performances

of DS approach is clearly weak both in selection and in-
ference stages because only a part of the available data is
used in each stage. In addition, it is quite annoying that
different set of features would be selected if data is splitted
differently. Recently, much attention has been paid to se-
lective inference for sparse linear models. The basic idea
of selective inference is to make inferences conditional on
a feature selection event. Lee et al. (2016) recently pro-
posed a practical selective inference framework for a class
of feature selection algorithms.

The main contribution in this paper is to extend the selec-
tive inference framework into sparse high-order interaction
models by introducing novel computational algorithms. To
the best of our knowledge, there are no other existing works
for sparse high-order interaction models in which the sta-
tistical signiﬁcances of the ﬁtted coefﬁcients are properly
evaluated in non-asymptotic sense.

Notations We use the following notations in the re-
mainder. For any natural number n, we deﬁne [n]
:=
{1, . . . , n}. A vector and a matrix is denoted such as
v ∈ Rn and M ∈ Rn
m, respectively. The index func-
tion is written as 1{z} which returns 1 if z is true, and 0
otherwise. The sign function is written as sgn(z) which
returns 1 if z ≥ 0, and −1 otherwise. An n × n identity
matrix is denoted as In.

×

2. Preliminaries

2.1. Problem setup

Consider a regression problem with a response Y ∈ R and
d-dimensional original covariates z = [z1, . . . , zd](cid:62) by the
following high-order interaction model up to r-th order

(cid:88)

Y =

αj1zj1 +

(cid:88)

αj1,j2zj1zj2

j1

[d]

∈

+ · · · +

(j1,j2)
j1

[d]
∈
=j2

×

[d]

(cid:88)

(j1,...,jr)

j1

=...

∈
=jr

[d]r

αj1,...,jr zj1 · · · zjr + ε,

(1)

where αs are the coefﬁcients and ε is a random noise. We
assume that each original covariate zj, j ∈ [d] is deﬁned in
a domain [0, 1]. Here, values 1 and 0 respectively might
be interpreted as the existence and the non-existence of
a certain property, and values between them indicate the
“degree” of existence. High-order interaction features thus
represent co-existence of multiple properties. For example,
if we are interested in interactions among age, body mass
index (BMI), and a mutation in a certain gene, we may code

(cid:54)
(cid:54)
(cid:54)
Selective Inference for Sparse High-Order Interaction Models

some covariates as

zj1 :=






1
(BMI − 15)/(30 − 15)
0

if BMI > 30,
if BMI ∈ [15, 30],
if BMI < 15,

zj2 := 1{mutation in the gene}.

that, if the ﬁrst selection stage is described as a linear se-
lection event, then exact statistical inference of the ﬁtted
coefﬁcients conditional on the selection event can be done.

Consider a linear regression model y = Xβ∗ + ε, where
β∗ ∈ RD is the true coefﬁcients and ε is distributed ac-
cording to N(0, σ2I) with known variance σ2.

Then, e.g., an interaction term zj1 zj2 represents the co-
existence of high BMI and a mutation in the gene.

∈

[r]

(cid:0)d
ρ

The high-order interaction model Eq.(1) has in total D :=
(cid:1) features. Let us write the mapping from the
(cid:80)
ρ
original covariates z := [z1, . . . , zd](cid:62) ∈ Rd to the high-
order interaction features x := [x1, . . . , xD](cid:62) ∈ RD as
φ : [0, 1]d → [0, 1]D, z (cid:55)→ x,, i.e.,

x := φ(z) = [z1, . . . , zd, z1z2, . . . , zd

. . . , z1 ···zk, . . . , zd

−

1zd,
r+1 ···zd](cid:62)

−

Then, the high-order interaction model Eq.(1) is simply
written as a D-dimensional linear model

y = β(cid:62)x = β1x1 + · · · + βDxD,

where β1, . . . , βD are D coefﬁcients corresponding to
αj1, . . . , αj1,...,jr in Eq.(1). Since a high-order interaction
feature is a product of original covariates deﬁned in [0, 1],
the range of each feature xj, j ∈ [D] is also [0, 1].
The original training set is denoted as {(zi, yi) ∈ [0, 1]d ×
R}i
[n], while the expanded training set is written as
∈
{(xi, yi) ∈ [0, 1]D × R}i
[n]. The latter is also denoted as
D × Rn where each row of X is xi ∈ Rd
(X, y) ∈ [0, 1]n
and each element of y is yi. Furthermore, the j-th column
j, j ∈ [D]. We denote the pseudo
of X is written as x
·
inverse of X as X + := (X (cid:62)X)−

1X (cid:62).

×

∈

Our goal is to identify statistically signiﬁcant high-order in-
teraction terms that have large impacts on the response Y
by identifying regression coefﬁcients αs which are signif-
icantly deviated from zero. Unfortunately, since the num-
ber of coefﬁcients αs to be ﬁtted would be far greater than
the sample size n, traditional least-square estimation the-
ory cannot be used for making statistical inferences on the
ﬁtted model. We thus consider ﬁrst to perform feature se-
lection and then to make statistical inference only for the
selected features based on selective inference approach.

2.2. Selective inference for sparse linear models

In this section, we brieﬂy review the selective inference
framework for sparse linear models developed by Lee et al.
(2016). Selective inference is developed for two stage
methods, where a subset of features is selected in the ﬁrst
stage, and inferences are made only on the selected features
in the second stage. A key ﬁnding by Lee et al. (2016) is

Feature selection stage Suppose that, in the ﬁrst feature
selection stage, a subset of features S ⊆ [D] are selected.
The selective inference framework in Lee et al. (2016) can
be applied to feature selection algorithms whose selection
process can be characterized by a set of linear inequalities
in the form of Ay ≤ b with a certain matrix A and a cer-
tain vector b that do not depend on y. This type of selec-
tion event is called a linear selection event. In the selective
inference framework, inferences are made conditional on
the selection event. It means that, in the case of a linear
selection event, we only care about the cases where y is
observed in a polytope Pol(S) := {y ∈ Rn | Ay ≤ b}.
In Lee & Taylor (2014) and Lee et al. (2016), marginal
screening, OMP and LASSO are shown to be linear selec-
tion events, indicating that the selective inference frame-
work can be applied to statistical testing of the selected
features by these algorithms.

Statistical inference stage Consider a hypothesis testing
for the j-th selected feature in S

H0,j : β∗S,j = 0 vs. H1,j : β∗S,j (cid:54)= 0.

(2)

The least-square estimator of the linear model only with the
selected features S is written as ˆβS = (X (cid:62)S XS)−
If we consider the case where S is NOT selected from the
data, i.e., independent of y, then, under the null hypothesis
H0, the sampling distribution of each ﬁtted coefﬁcient is

1X (cid:62)S y.

ˆβS,j ∼ N(0, σ2

S,j), where σ2

S,j := σ2(X (cid:62)S XS)−

1
jj .

(3)

For two-sided test at level α, if the critical values (cid:96)α/2 and
uα/2 are chosen to be the lower and the upper α/2 points
of the sampling distribution in Eq.(3), then the type I error
at level α is controlled as

Pr( ˆβS,j /∈ [(cid:96)α/2, uα/2]) ≤ α

(4)

On the other hand, when S is selected from the data as
we consider here, we would like to control the following
selective type I error

Pr( ˆβS,j /∈ [(cid:96)(S,j)
=Pr( ˆβS,j /∈ [(cid:96)(S,j)

α/2 , u(S,j)
α/2 , u(S,j)

α/2 ] | {S is selected})
α/2 ] | y ∈ Pol(S)) ≤ α (5)

by appropriately selecting the adjusted critical values (cid:96)(S,j)
α/2
and u(S,j)
α/2 , where the selection event {S is selected} is

Selective Inference for Sparse High-Order Interaction Models

L(S, j) = ˆβS,j + θL(X (cid:62)S XS)−

1
jj ,

(7a)

3.1. MS for interaction models

written as y ∈ Pol(S) in the case of a linear selection
event. Lee et al. (2016) derived how to compute these
adjusted critical values as formally stated in the following
lemma.

Lemma 1. If the critical values are computed as

α/2 := (F [L(S,j),U (S,j)]
(cid:96)(S,j)
0,σ2
α/2 := (F [L(S,j),U (S,j)]
u(S,j)
0,σ2

S,j

S,j

)−

1(α/2),

)−

1(1 − α/2),

(6a)

(6b)

then the selective type I error is controlled as in Eq. (5),
where F [L,U ]
is the cumulative distribution function of a
µ,σ2
truncated Normal distribution TN(µ, σ2, L, U ), i.e.,

F [L,U ]
µ,σ2 (x) =

Φ((x − µ)/σ) − Φ((L − µ)/σ)
Φ((U − µ)/σ) − Φ((L − µ)/σ)

,

and the truncation points are obtained, by using the ob-
served ˆβS,j and y, as

where θL := min
R

θ s.t. y + θ(X +

S )(cid:62)ej ∈ Pol(S),

U (S, j) = ˆβS,j + θU (X (cid:62)S XS)−

1
jj ,

(7b)

where θU := max
R

θ s.t. y + θ(X +

S )(cid:62)ej ∈ Pol(S).

θ

∈

θ

∈

The proof of Lemma 1 is is presented in Appendix A al-
though it is easily proved by using the results in Lee et al.
(2016). See Lee et al. (2016) for more general statement
about the selective inference framework.

Eq.(7) indicates that the truncation points are obtained by
considering the interval where the test statistic ˆβS,j can
move within the polyhedron Pol(S). Figure 2 schemati-
cally illustrates that, when we make inferences conditional
on a linear selection event S, the sampling distribution is
deﬁned within the polytope Pol(S), and it follows a trun-
cated normal distribution when y is normally distributed.

Unfortunately, we cannot directly apply this selective infer-
ence framework to high-order interaction models because
the polytope Pol(S) is characterized by extremely large
number of linear inequalities, and the optimization prob-
lems in Eq.(7) are hard to solve.

Figure 2. An illustration of polyhedral lemma. The polyhedron
represents the selection event and truncation points can be com-
puted by optimizing θ along with the direction (X +
S )(cid:62)ej. In ad-
dition, critical values can be obtained by computing Eq.(6), and
the red region shows the rejection region of the test in Eq.(2).

Consider selecting the top k interaction features from all
the D interaction features that have marginal strong cor-
relations with the response. Noting that each feature is
deﬁned in [0, 1] and the value indicates (the degree of)
the existence of a certain property, we consider a score
j y, j ∈ [D] for each of the D features, and select the top
x(cid:62)
·
k features according to their absolute scores |x(cid:62)
j y|. We de-
·
note the index set of the selected k features by S, and that
of the unselected ¯k := D − k features by ¯S := [D] \ S.

Since D is extremely large, we cannot compute the score
for each interaction feature. We exploit the tree structure
among interaction patterns as depicted in Figure 1.

Deﬁnition 2. (Descendant features) For each j ∈ [D], let
Des(j) ⊆ [D] be the set of features corresponding to the
descendant nodes in the tree including j itself.

j, j ∈ [D],
Lemma 3. Consider an interaction feature x
·
whose indices are represented in a tree structure as de-
picted in Figure1. Then, for any node j ∈ [D] in the tree,

|x

˜jy| ≤ max
·

(cid:88)

(cid:88)

xijyi, −

xijyi

(8)

i:yi>0

i:yi<0











3. Feature selection for interaction models

for all ˜j ∈ Des(j).

In this section, we present two feature selection algorithms
for high-order interaction models. Since the number of fea-
tures D is extremely large, existing feature selection algo-
rithms for linear models cannot be directly applied to inter-
action models. In this paper, we study marginal screening
(MS) and orthogonal matching pursuit (OMP) as examples
of feature selection algorithms.

The proof of Lemma 3 is presented in Appendix A.
˜j, (j, ˜j) ∈
Lemma 3 tells that, for a descendant feature x
S × Des(j), an upper bound of the absolute score |x(cid:62)
y|
˜j
·
can be computed based on its parent feature x

·

j.
·

We note that this simple upper bound has been used in some
data mining studies such as Saigo et al. (2006); Kudo et al.

yL(S,j)U(S,j)`(S,j)↵/2u(S,j)↵/2✓(X+S)>ejSelective Inference for Sparse High-Order Interaction Models

(2004); Nakagawa et al. (2016). When we search over the
tree, if the upper bound in Eq.(8) is smaller than the cur-
rent k-th largest score at a certain node j, then we can quit
searching over its descendant nodes ˜j ∈ Des(j).

As pointed out in Lee & Taylor (2014), feature selection
processes of marginal screening is a linear selection event,
i.e., characterized by a set of linear constraints. The event
that k features in S are selected, and ¯k features in ¯S are
j(cid:48)y|, ∀ (j, j(cid:48)) ∈
j y| ≥ |x(cid:62)
not selected is rephrased as |x(cid:62)
·
·
S × ¯S. Let sj := sgn(x(cid:62)
j y), j ∈ S. Then, the above
·
feature selection event is rewritten with the sign constraints
of the selected features by the following 2k¯k+k constraints

(−sjx
·
(−sjx

j − x

j + x
·

j(cid:48))(cid:62)y ≤ 0, ∀ (j, j(cid:48)) ∈ S × ¯S,
·
j(cid:48))(cid:62)y ≤ 0, ∀ (j, j(cid:48)) ∈ S × ¯S,
·
j y ≤ 0, ∀ j ∈ S.
−sjx(cid:62)
·

(9a)

(9b)

(9c)

×

These constraints are written as Ay ≤ 0 with a matrix
A ∈ R(2k¯k+k)
n. Unfortunately, ﬁnding θmin and θmax
by naively solving the optimization problems in Eq.(7) is
computationally difﬁcult because the polyhedron Pol(S) is
characterized by the extremely large number of constraints.
For example, when d = 10, 000, r = 5, k = 10, the
number of linear inequalities that deﬁnes the polyhedron
Pol(S) is 2k¯k + k > 1019.

3.2. OMP for interaction models

Orthogonal matching pursuit (OMP) is a well-known iter-
ative feature selection method (Pati et al., 1993). At each
iteration, the most correlated feature with the residual of
the current model which is ﬁtted via least-squares method
by using the features selected in earlier steps.

∈

[h]

ˆβSh,(j)x

Consider again selecting k interaction features by OMP.
Let [(1), . . . , (h)] be the sequence of the indices of the se-
lected features from step 1 to step h for h ∈ [k], and de-
ﬁne Sh := {(1), . . . , (h)}. Before step h + 1, we have
j, j ∈ Sh. Using these h fea-
already selected h features x
·
tures, the current n-dimensional model output is written as
(cid:80)
(j), where the coefﬁcients ˆβSh,(j), j ∈ [h]
j
·
are estimated by least-squares method. Denoting by ΓSh
the n×h matrix whose j-th column is x
(j), the least square
·
estimates are written as ˆβSh := [ ˆβSh,(1), . . . , ˆβSh,(h)](cid:62) =
(ΓSh)+y. Then, at the h + 1 step, we consider the corre-
ˆβSh and
lation between the residual vector rh := y − ΓSh
j(cid:48) for j(cid:48) ∈ ¯Sh, and ﬁnd the one that maximizes
a feature x
·
the absolute correlation |x(cid:62)
j(cid:48)rh| among them. Here, since
·
the number of remaining features | ¯Sh| = D − h is still
extremely large, it is hard to compute all these D − h cor-
relations. To overcome this difﬁculty, we can simply use
Lemma 3 just by replacing y with the current residual rh.
˜j, ˜j ∈ Des(j), an
Speciﬁcally, for a descendant feature x

·

rh| is given as

upper bound of |x(cid:62)
˜j
·



˜j rh| ≤ max
|x(cid:62)
·

(cid:88)



i:rh,i>0

xijrh,i, −

xijrh,i

.

(cid:88)

i:rh,i<0






At each iteration, when we search over the tree, if the upper
bound is smaller than the current largest correlation, then,
in the same way as the case of MS, we can quit searching
over its descendant nodes j(cid:48) ∈ Des(j).

It is also pointed out in Lee & Taylor (2014) that a feature
selection process of OMP is linear selection event. At step
h, the event that the (h)-th feature is selected is formulated
j(cid:48)rh|, for all j(cid:48) ∈ ¯Sh. Let PSh := In −
as |x(cid:62)
·
ΓSh Γ+
. Then, the above selection event is rewritten as a
Sh
set of linear inequalities with respect to y

(h)rh| ≥ |x(cid:62)
·

(−s(h)x
(−s(h)x

j(cid:48))(cid:62)PSh y ≤ 0, ∀ j(cid:48) ∈ ¯Sh,
(h) − x
·
·
j(cid:48))(cid:62)PSh y ≤ 0, ∀ j(cid:48) ∈ ¯Sh,
(h) + x
·
·
(h)PSh y ≤ 0,
−s(h)x(cid:62)
·

(10a)

(10b)

(10c)

where s(h) = sgn(x(cid:62)
(h)rh). By combining all the linear
·
selection events in k steps, the entire selection event of
the OMP is characterized by (cid:80)
[k](2(D − h) + 1) lin-
h
ear inequalities in Rn. In practice, it is computationally in-
tractable to handle these extremely large number of linear
inequalities.

∈

4. Selective inference for interaction models

In this section, we present an efﬁcient selective inference
algorithm for high-order interaction models, which is our
main contribution.

The discussion in §3 suggests that it would be hard to com-
pute critical values for selective inference in Eq.(6) because
the selection event y ∈ Pol(S) is characterized by ex-
tremely large number of inequalities. Our basic idea for ad-
dressing this computational difﬁculty is to note that most of
the inequalities actually do not affect the results of the se-
lective inference, and a large portion of them can be identi-
ﬁed by exploiting the anti-monotonicity properties deﬁned
in the tree structure among high-order interaction features.

4.1. Marginal screening

We consider k trees for each of the k selected features.
Each tree consists of a set of nodes corresponding to each of
the non-selected features j(cid:48) ∈ ¯S. For a pair (j, j(cid:48)) ∈ S × ¯S,
the j(cid:48)-th node in the j-th tree corresponds to the linear in-
equalities Eqs.(9a) and (9b). When we search over these k
trees, we introduce a novel pruning strategy by deriving a
condition such that, if the j(cid:48)-th node in the j-th tree satisﬁes
certain conditions, then all the (j, ˜j(cid:48))-th inequalities for all

Selective Inference for Sparse High-Order Interaction Models

˜j(cid:48) ∈ Desj(j(cid:48)) are guaranteed to be irrelevant to the selec-
tive inference results because they do not affect the optimal
solutions in Eq.(7), where we deﬁne Desj(j(cid:48)) be all the
features corresponding to the descendant node of j(cid:48) in the
j-th tree.
Lemma 4. Let η := (X +
mization problems in (7) are respectively written as

S )(cid:62)ej. The solutions of the opti-

θL = − min{θ(a)
θU = − max{θ(a)

L , θ(b)
U , θ(b)

L , θ(c)
L },
U , θ(c)
U },

where

θ(a)
L :=

min
S
∈
(sj x·j +x·j(cid:48) )(cid:62)η>0

(j,j(cid:48))

¯S,

×

θ(a)
U :=

max
S
×
∈
(sj x·j +x·j(cid:48) )(cid:62)η<0

(j,j(cid:48))

¯S,

θ(b)
L :=

θ(b)
U :=

min
¯S,
S
∈
×
x·j(cid:48) )(cid:62)η>0

(j,j(cid:48))

(sj x·j

−

(j,j(cid:48))

max
¯S,
S
∈
×
x·j(cid:48) )(cid:62)η<0

(sj x·j

−

(sjx
(sjx
·

j + x
·
j + x
·

j(cid:48))(cid:62)y
·
j(cid:48))(cid:62)η

(sjx
(sjx
·

j + x
·
j + x
·

j(cid:48))(cid:62)y
·
j(cid:48))(cid:62)η

j − x
(sjx
·
j − x
(sjx
·

j(cid:48))(cid:62)y
·
j(cid:48))(cid:62)η
·

j − x
(sjx
j(cid:48))(cid:62)y
·
·
j − x
j(cid:48))(cid:62)η
(sjx
·
·

,

,

,

,

(11a)

(11b)

(11c)

(11d)

θ(c)
L := min
S,
j
∈
sj x(cid:62)
·j η>0

sjx(cid:62)
j y
·
sjx(cid:62)
j η
·

, θ(c)

U := max
j
S,
∈
sj x(cid:62)
·j η<0

sjx(cid:62)
j y
·
sjx(cid:62)
j η
·

.

The proof of Lemma 4 is presented in Appendix A.
Lemma 5. For any triplet (j, j(cid:48), ˜j(cid:48)) ∈ S × ¯S × Desj(j(cid:48)),

L(a)

j y +
E := sjx(cid:62)
·

U (a)

j y +
E := sjx(cid:62)
·

L(a)

j η +
D := sjx(cid:62)
·

U (a)

j η +
D := sjx(cid:62)
·

L(b)

j y −
E := sjx(cid:62)
·

U (b)

j y −
E := sjx(cid:62)
·

L(b)

j η −
D := sjx(cid:62)
·

U (b)

j η −
D := sjx(cid:62)
·

(cid:88)

i:yi<0
(cid:88)

i:yi>0
(cid:88)

i:ηi<0
(cid:88)

i:ηi>0
(cid:88)

i:yi>0
(cid:88)

i:yi<0
(cid:88)

i:ηi>0
(cid:88)

i:ηi<0

xij(cid:48)yi ≤ (sjx

j + x
·

˜j(cid:48))(cid:62)y, (12a)
·

xij(cid:48)yi ≥ (sjx

j + x
·

˜j(cid:48))(cid:62)y, (12b)
·

xij(cid:48)ηi ≤ (sjx

j + x
·

˜j(cid:48))(cid:62)η, (12c)
·

xij(cid:48)ηi ≥ (sjx

j + x
·

˜j(cid:48))(cid:62)η, (12d)
·

xij(cid:48)yi ≤ (sjx
·

j − x

˜j(cid:48))(cid:62)y, (12e)
·

xij(cid:48)yi ≥ (sjx
·

j − x

˜j(cid:48))(cid:62)y,
·

(12f)

xij(cid:48)ηi ≤ (sjx

j − x
·

˜j(cid:48))(cid:62)η, (12g)
·

xij(cid:48)ηi ≥ (sjx

j − x
·

˜j(cid:48))(cid:62)η. (12h)
·

The proof of Lemma 5 is presented in Appendix A.

Theorem 6. (i) Consider solving the optimization problem
in Eq.(11a), and let ˆθ(a)
L be the current optimal solution,
i.e., we know that the optimal θ(a)
L is at least no greater
than ˆθ(a)

L . If

{U (a)

D < 0} ∪ {L(a)
∪ {L(a)

D > 0, L(a)
D > 0, L(a)

E < 0, L(a)
E > 0, L(a)

E /L(a)
E /U (a)

D > ˆθ(a)
L }
D > ˆθ(a)
L }

is true, then the (j, ˜j(cid:48))-th constraint in Eq. (9a) for any
(j, j(cid:48), ˜j(cid:48)) ∈ S × ¯S × Desj(j(cid:48)) does not affect the optimal
solution in Eq.(11a).

(ii) Next, consider solving the optimization problem in
Eq.(11c), and let ˆθ(b)

L be the current optimal solution. If

{U (b)

D < 0} ∪ {L(b)
∪ {L(b)

D > 0, L(b)
D > 0, L(b)

E < 0, L(b)
E > 0, L(b)

E /L(b)
E /U (b)

D > ˆθ(b)
L }
D > ˆθ(b)
L }

is true, then the (j, ˜j(cid:48))-th constraint in Eq. (9b) for any
(j, j(cid:48), ˜j(cid:48)) ∈ S × ¯S × Desj(j(cid:48)) does not affect the optimal
solution in Eq.(11c).

(iii) Furthermore, consider solving the optimization prob-
lem in Eq.(11b), and let ˆθ(a)
U be the current optimal solu-
tion. If

{L(a)

D > 0} ∪ {U (a)
∪ {U (a)

D < 0, L(a)
D < 0, L(a)

E < 0, L(a)
E > 0, L(a)

E /U (a)
E /L(a)

D < ˆθ(a)
U }
D < ˆθ(a)
U }

is true, then the (j, ˜j(cid:48))-th constraint in Eq. (9a) for any
(j, j(cid:48), ˜j(cid:48)) ∈ S × ¯S × Desj(j(cid:48)) does not affect the optimal
solution in Eq.(11b).

(iv) Finally, consider solving the optimization problem in
Eq.(11d), and let ˆθ(b)

U be the current optimal solution. If

{L(b)

D > 0} ∪ {U (b)
∪ {U (b)

D > 0, L(b)
D > 0, L(b)

E < 0, L(b)
E < 0, L(b)

E /U (b)
E /L(b)

D < ˆθ(b)
U }
D < ˆθ(b)
U }

is true, then the (j, ˜j(cid:48))-th constraint in Eq. (9b) for any
(j, j(cid:48), ˜j(cid:48)) ∈ S × ¯S × Desj(j(cid:48)) does not affect the optimal
solution in Eq.(11d).

The proof of Theorem 6 is presented in Appendix. Note
that all the conditions in Theorem 6 can be checked at the
j(cid:48)-th node in each tree. If the conditions are satisﬁed as
the j(cid:48)-th node, then one can skip searching over its sub-
tree. It allows us to perform selective inference for high-
order interaction models even the number of constraints
that deﬁnes the selection event is extremely large. As we
demonstrate in the experiment section, these pruning con-
ditions are quite effective in practice. For example, we can
perform selective inference for an interaction models with
d = 10, 000, r = 5, k = 10 in a few seconds.

Selective Inference for Sparse High-Order Interaction Models

4.2. Orthogonal matching pursuit (OMP)

As we discuss in the previous section, the selection event
at each iteration of OMP has same form as MS. Therefore,
we can derive similar pruning conditions as in Theorem 6
for OMP. Due to the space limitation, we deffer the corre-
sponding lemma and the theorem for OMP in Appendix B.

5. Experiments

We demonstrate the performance of the selective inference
for high-order sparse interaction models by numerical ex-
periments on synthetic datasets and a real dataset.

5.1. Experiments on synthetic datasets

First, we compared selective inference (select) with
naive (naive) and data-splitting (split) on synthetic
datasets. In naive, the critical values of the selected k
features were naively computed without any selection bias
correction mechanisms as in Eq. (4). In split, the dataset
was ﬁrst divided into two equally sized sets, and one of
them was used for selection stage, and the other was used
for inference stage. Note that the errors controlled by these
methods are individual false positive rate for each of the
selected features (although naive actually cannot control
it), we applied Bonferroni correction within the k selected
features, i.e., we reject the hypothesis in Eq. (2) with the
signiﬁcance level α/k where α = 0.05, and we refer this
error as family-wise false positive rates (FW-FPRs).

The synthetic dataset was generated as follows. In the ex-
periments for comparing FW-FPRs, we generated the train-
ing instances (zi, yi) ∈ [0, 1]d×R independently at random
for each i ∈ [n]. The original covariates zi were randomly
generated so that it contains d(1 − ζ) 1s on average, where
ζ ∈ [0, 1] is an experimental parameter for representing
the sparsity of the dataset, while the response yi was ran-
domly generated from a Normal distribution N (0, σ2). In
the experiments for comparing true positive rates (TPRs)
the response yi was randomly generated from a Normal
distribution N (µ(X), σ2I), where, for each row of µ(X)
is deﬁned as µ(zi) = 2z1z2z3 in the experiments for MS,
µ(zi) = 0.5z1 − 2z2z3 + 3z4z5z6 in the experiments for
OMP. We investigated the performances by changing var-
ious experimental parameters. We set the baseline param-
eters as n = 100, d = 100, k = 5, r = 5, α = 0.05,
σ = 0.5, and ζ = 0.6.

5.1.1. FALSE POSITIVE RATES

Figure 3 shows the FW-FPRs when varying the number of
transactions n ∈ {50, 100, . . . , 250}, the number of orig-
inal covariates d ∈ {50, 100, . . . , 250}. In all cases, the
FW-FPRs of naive were far greater than the desired sig-
niﬁcance level α = 0.05, indicating that the selection bias

(a) n ∈ {50, . . . , 250}

(b) d ∈ {50, . . . , 250}

(c) n ∈ {50, . . . , 250}

(d) d ∈ {50, . . . , 250}

Figure 3. False positive rates (FPRs).

(a) n ∈ {50, . . . , 250}

(b) d ∈ {50, . . . , 250}

MS

OMP

MS

OMP

(c) n ∈ {50, . . . , 250}

(d) d ∈ {50, . . . , 250}

Figure 4. True positive rates (TPRs).

is harmful. The FW-FPRs of the other two approaches
select and split were successfully controlled.

5.1.2. TRUE POSITIVE RATES

Figure 4 shows the TPRs of select and split (we omit
naive because it cannot control FPRs). Here, TPRs are
deﬁned as the probability of ﬁnding truly correlated inter-
In all the setups, the TPRs of select
action features.
were much greater than split. Note that the perfor-
mances of split would be worse than select both in
the selection and the inference stages. The risk of failing to
select truly correlated features in split would be higher
than select because only half of the data would be used
in the selection stage. Similarly, the statistical power in the
inference stage in split would be smaller than select
because the sample size is smaller.

 0 0.05 0.150250100 150 200 The number of instances n 0.95 1 1.05 1.1Family-wise error (FW-FPRs)naiveselectsplit 0 0.05 0.150100150200250The number of features d 0.95 1 1.05 1.1Family-wise error (FW-FPRs)naiveselectsplit 0 0.05 0.150250100 150 200 The number of instances n 0.95 1 1.05 1.1Family-wise error (FW-FPRs)naiveselectsplit 0 0.05 0.150100150200250The number of features d 0.95 1 1.05 1.1Family-wise error (FW-FPRs)naiveselectsplit 0 0.2 0.4 0.6 0.8 150250True positive rates (TPRs)100 150 200The number of instances nselectsplit 0 0.2 0.4 0.6 0.8 150100150200250True positive rates (TPRs)The number of features dselectsplit 0 0.2 0.4 0.6 0.8 150250True positive rates (TPRs)100 150 200The number of instances nselectsplit 0 0.2 0.4 0.6 0.8 150100150200250True positive  rates (TPRs)The number of features dselectsplitSelective Inference for Sparse High-Order Interaction Models

Table 1. Computation times [sec]

with computational trick

MS

ζ = 0.8

4.68
1.74
3.38
2.33
5.04

10−2
×
10−1
×
10−1
×
100
×
100
×
ζ = 0.8

4.40
5.06
1.23
1.53
3.70

×
×
×
×
×

10−2
10−1
100
101
101

ζ = 0.9

1.80
9.07
1.54
6.61
1.55

10−2
×
10−2
×
10−1
×
10−1
×
100
×
ζ = 0.9

1.77
1.64
3.74
2.88
6.16

×
×
×
×
×

10−2
10−1
10−1
100
100

n
100
500
1000
5000
10000
d
100
500
1000
5000
10000

without computational trick
ζ = 0.9
ζ = 0.8

102
102
102
103
103

1.37
1.80
2.65
1.05
2.06

×
×
×
×
×
ζ = 0.8

1.47

102

×
1 day
1 day
1 day
1 day

≥
≥
≥
≥

102
102
102
102
102

1.31
1.36
1.41
2.57
5.12

×
×
×
×
×
ζ = 0.9

1.31

102

×
1 day
1 day
1 day
1 day

≥
≥
≥
≥

with computational trick

OMP

ζ = 0.8

2.33
1.01
3.18
6.20
1.24

10−1
×
100
×
100
×
101
×
102
×
ζ = 0.8

10−1
101
102

2.41
3.52
3.01

×
×
×
1 day
1 day

≥
≥

ζ = 0.9

5.85
3.74
7.27
3.48
9.00

10−2
×
10−1
×
10−1
×
100
×
100
×
ζ = 0.9

6.02
9.83
1.66
1.92
5.98

×
×
×
×
×

10−2
100
102
103
104

without computational trick
ζ = 0.9
ζ = 0.8

102
103
103
104
104

8.83
1.33
2.15
1.00
1.98

×
×
×
×
×
ζ = 0.8

8.86

102

×
1 day
1 day
1 day
1 day

≥
≥
≥
≥

102
102
102
103
103

8.28
8.60
9.07
2.05
4.63

×
×
×
×
×
ζ = 0.9

8.20

102

×
1 day
1 day
1 day
1 day

≥
≥
≥
≥

Database (Rhee et al., 2003). The goal here is to ﬁnd statis-
tically signiﬁcant high-order interactions of multiple muta-
tions (up to r = 5 order interactions) that are highly as-
sociated with the drug resistances. We selected k = 30
features, and evaluated the statistical signiﬁcances of these
features by the selective inference framework. Table 2
shows the numbers of 1st, 2nd, 3rd and 4th order inter-
actions that were statistically signiﬁcant after Bonferroni
correction, i.e., signiﬁcance level is set to be α/k with
α = 0.05. (there were no statistically signiﬁcant 5th or-
der interactions).

Figure 5 shows the degree of signiﬁcances in the form of
adjusted p-values after Bonferroni correction in increasing
order on idv and d4t datasets by MS and OMP scenario,
respectively. These results indicate that the selective infer-
ence approach could successfully identify statistically sig-
niﬁcant high-order interactions of multiple mutations.

(a) idv dataset (MS)

Table 2. The numbers of signiﬁcant high-order interactions of
multiple mutations in HIV datasets.

MS

Data 1st 2nd 3rd 4th Time[s]

NNRTI (d = 371)

OMP
1st 2nd 3rd 4th Time[s]

dlv(n = 732)
efv(n = 734)
nvp(n = 746)

3tc(n = 633)
abc(n = 628)
azt(n = 630)
d4t(n = 630)
ddi(n = 632)
tdf(n = 353)

apv(n = 768)
atv(n = 329)
idv(n = 827)
lpv(n = 517)
nfv(n = 844)
rtv(n = 795)
sqv(n = 826)

1

4

1
5
2
4
2

3
1
1
4
5
5
1

1

2
13
5
11
1

6
3
6
4
7
7
3

7
3
6

1
2
3
1
1
2
2

NRTI (d = 348)

2
1
1

PI (d = 225)

.495
.732
.774

.257
.238
.231
.215
.234
.230

.188
.150
.437
.275
.455
.183
.623

2
5
8

4
9
5
7
6
3

9
3
9
11
15
10
7

3

1

1

1

1
1

18.0
13.7
17.4

15.1
11.7
17.5
13.7
12.1
26.4

6.5
5.0
6.2
6.1
5.8
5.6
7.8

5.1.3. COMPUTATIONAL EFFICIENCY

Table 1 shows the computation times in seconds for the se-
lective inference approach with and without the computa-
tional tricks described in §4 for various values of the num-
ber of transactions n ∈ {100, . . . , 10, 000}, the number of
original covariates d ∈ {100, . . . , 10, 000}, and the spar-
sity rates ζ ∈ {0.8, 0.9} (we terminated the search if the
time exceeds 1 day). It can be observed from the table that,
if we use the computational trick, the selective inferences
can be conducted with reasonable computational costs ex-
cept for d ≥ 5, 000 and ζ = 0.8 cases with OMP. When the
computational trick was not used, the cost was extremely
large. Especially when the number of original covariates d
is larger than 100, we could not complete the search within
1 day. From the results, we conclude that computational
trick described in §4 is indispensable for selective infer-
ences for sparse high-order interaction models.

5.2. Application to HIV drug resistance data

We applied the selective inference approach to HIV-1 se-
quence data obtained from Stanford HIV Drug Resistance

Figure 5. The list of Bonferroni-adjusted selective p-values of
k = 30 selected high-order interactions of multiple mutations
on two HIV datasets.

(b) d4t dataset (OMP)

 1e-09 1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 110I,63P,71V10I,71V63P,71V54V,90M71V,90M63P,71V,90M54V,63P,71V71V54V,71V54V,63P54V10I,54V36I,71V71V,82A63P,82A10I82A54V,82A10I,63P63P10I,90M90M10I,63P,90M63P,90M84V10I,46I46I46I,63P46I,90M46I,63P,90MAdjusted p-value 1e-45 1e-40 1e-35 1e-30 1e-25 1e-20 1e-15 1e-10 1e-05 1210W41L151M67N122E215Y118I,210W41L,210W,215Y103N,210W,215Y75M41L,122E,135T69i65R215F41L,43E,67N,210W,215Y41L,215Y,219R68G,122E70R,219E67N,123E41L,215F67N,69D,122E,208Y70R,219Q122E,190A62V35T122E,228H75I,77L70R41L,142V67N,207EAdjusted p-valueSelective Inference for Sparse High-Order Interaction Models

Acknowledgements

This work was partially supported by MEXT KAKENHI
JST CREST (JPMJCR1302,
(17H00758, 16H06538),
JPMJCR1502), RIKEN Center
for Advanced Intelli-
gence Project, and JST support program for starting up
innovation-hub on materials research by information inte-
gration initiative.

References

Barber, Rina Foygel and Cand`es, Emmanuel J. A knock-
off ﬁlter for high-dimensional selective inference. arXiv
preprint arXiv:1602.03574, 2016.

Benjamini, Yoav and Yekutieli, Daniel. False discovery
rate–adjusted multiple conﬁdence intervals for selected
parameters. Journal of the American Statistical Associ-
ation, 100(469):71–81, 2005.

Benjamini, Yoav, Heller, Ruth, and Yekutieli, Daniel. Se-
lective inference in complex research. Philosophical
Transactions of the Royal Society of London A: Mathe-
matical, Physical and Engineering Sciences, 367(1906):
4255–4271, 2009.

Berk, Richard, Brown, Lawrence, Buja, Andreas, Zhang,
Kai, and Zhao, Linda. Valid post-selection inference.
The Annals of Statistics, 41(2):802–837, 2013.

Bien, J., Taylor, J. E., and Tibshirani, R. A LASSO for hi-
erarchical interactions. Journal of The Royal Statistical
Society B, 41:1111–1141, 2013.

Choi, N.H., Li, W., and Zhu, J. Variable selection with the
strong heredity constraint and its oracle property. Jour-
nal of the American Statistical Association, 105:354–
364, 2010.

Cordell, Heather J. Detecting gene–gene interactions that
underlie human diseases. Nature Reviews Genetics, 10
(6):392–404, 2009.

Dudoit, Sandrine, Shaffer, Juliet Popper, and Boldrick, Jen-
nifer C. Multiple hypothesis testing in microarray exper-
iments. Statistical Science, pp. 71–103, 2003.

Fan, J. and Lv, J. Sure independence screening for ultra-
high dimensional feature space. Journal of The Royal
Statistical Society B, 70:849–911, 2008.

Fithian, William, Sun, Dennis, and Taylor, Jonathan. Op-
timal inference after model selection. arXiv preprint
arXiv:1410.2597, 2014a.

Fithian, William, Sun, Dennis, and Taylor, Jonathan. Op-
arXiv preprint

timal inference after model selection.
arXiv:1410.2597, 2014b.

Fithian, William, Taylor, Jonathan, Tibshirani, Robert, and
Tibshirani, Ryan. Selective sequential model selection.
arXiv preprint arXiv:1512.02565, 2015.

Hao, Ning and Zhang, Hao Helen.

Interaction screening
for ultrahigh-dimensional data. Journal of the American
Statistical Association, 109(507):1285–1301, 2014.

Kudo, T., Maeda, E., and Matsumoto, Y. An application of
boosting to graph classiﬁcation. In Advances in Neural
Information Processing Systems, 2005.

Kudo, Taku, Maeda, Eisaku, and Matsumoto, Yuji. An ap-
plication of boosting to graph classiﬁcation. In Advances
in neural information processing systems, pp. 729–736,
2004.

Lee, Jason D and Taylor, Jonathan E. Exact post model
selection inference for marginal screening. In Advances
in Neural Information Processing Systems, pp. 136–144,
2014.

Lee, Jason D, Sun, Dennis L, Sun, Yuekai, Taylor,
Jonathan E, et al. Exact post-selection inference, with
application to the lasso. The Annals of Statistics, 44(3):
907–927, 2016.

Llinares-L´opez, Felipe, Sugiyama, Mahito, Papaxanthos,
Laetitia, and Borgwardt, Karsten. Fast and memory-
efﬁcient signiﬁcant pattern mining via permutation test-
In Proceedings of the 21th ACM SIGKDD Inter-
ing.
national Conference on Knowledge Discovery and Data
Mining, pp. 725–734. ACM, 2015.

Manolio, Teri A and Collins, Francis S. Genes, environ-
ment, health, and disease: facing up to complexity. Hu-
man heredity, 63(2):63–66, 2006.

Nakagawa, Kazuya, Suzumura, Shinya, Karasuyama,
Masayuki, Tsuda, Koji, and Takeuchi, Ichiro. Safe pat-
tern pruning: An efﬁcient approach for predictive pattern
mining. In Proceedings of the 22nd ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data
Mining, pp. 1785–1794. ACM, 2016.

Pati, Yagyensh Chandra, Rezaiifar, Ramin, and Krish-
naprasad, PS. Orthogonal matching pursuit: Recursive
function approximation with applications to wavelet de-
composition. In Signals, Systems and Computers, 1993.
1993 Conference Record of The Twenty-Seventh Asilo-
mar Conference on, pp. 40–44. IEEE, 1993.

Rhee, Soo-Yon, Gonzales, Matthew J, Kantor, Rami, Betts,
Bradley J, Ravela, Jaideep, and Shafer, Robert W. Hu-
man immunodeﬁciency virus reverse transcriptase and
protease sequence database. Nucleic acids research, 31
(1):298–303, 2003.

Selective Inference for Sparse High-Order Interaction Models

Saigo, H., Uno, T., and Tsuda, K. Mining complex geno-
typic features for predicting hiv-1 drug resistance. Bioin-
formatics, 24:2455—2462, 2006.

Taylor, Jonathan and Tibshirani, Robert. Post-selection
arXiv

inference for l1-penalized likelihood models.
preprint arXiv:1602.07358, 2016.

Terada, Aika, Okada-Hatakeyama, Mariko, Tsuda, Koji,
and Sese, Jun. Statistical signiﬁcance of combinatorial
regulations. Proceedings of the National Academy of
Sciences, 110(32):12996–13001, 2013.

Tian, Xiaoying and Taylor, Jonathan. Asymptotics of selec-
tive inference. arXiv preprint arXiv:1501.03588, 2015.

Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society, Series B,
58:267–288, 1996.

Tusher, Virginia Goss, Tibshirani, Robert, and Chu,
Gilbert. Signiﬁcance analysis of microarrays applied to
the ionizing radiation response. Proceedings of the Na-
tional Academy of Sciences, 98(9):5116–5121, 2001.

Yang, Fan, Barber, Rina Foygel, Jain, Prateek, and Lafferty,
John. Selective inference for group-sparse linear models.
arXiv preprint arXiv:1607.08211, 2016.

