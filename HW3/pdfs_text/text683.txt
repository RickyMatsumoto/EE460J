Combined Group and Exclusive Sparsity for Deep Neural Networks

Jaehong Yoon 1 Sung Ju Hwang 1 2

Abstract

The number of parameters in a deep neural net-
work is usually very large, which helps with its
learning capacity but also hinders its scalabil-
ity and practicality due to memory/time inefﬁ-
ciency and overﬁtting. To resolve this issue,
we propose a sparsity regularization method that
exploits both positive and negative correlations
among the features to enforce the network to be
sparse, and at the same time remove any redun-
dancies among the features to fully utilize the
capacity of the network. Speciﬁcally, we pro-
pose to use an exclusive sparsity regularization
based on (1, 2)-norm, which promotes competi-
tion for features between different weights, thus
enforcing them to ﬁt to disjoint sets of features.
We further combine the exclusive sparsity with
the group sparsity based on (2, 1)-norm, to pro-
mote both sharing and competition for features
in training of a deep neural network. We vali-
date our method on multiple public datasets, and
the results show that our method can obtain more
compact and efﬁcient networks while also im-
proving the performance over the base networks
with full weights, as opposed to existing sparsity
regularizations that often obtain efﬁciency at the
expense of prediction accuracy.

1. Introduction

Deep neural networks have shown tremendous success
in recent years, achieving near-human performances on
tasks such as visual recognition (Krizhevsky et al., 2012;
Szegedy et al., 2015; He et al., 2016). One of the key fac-
tors in this success of deep network is its expressive power,
which is made possible by multiple layers of non-linear
transformations. However, this expressive power comes
at a cost: increased number of parameters. Due to large

1UNIST, Ulsan, South Korea 2AITrics, Seoul, South Korea.

Correspondence to: Sung Ju Hwang <sjhwang@unist.ac.kr>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

number of parameters, deep networks require large amount
of memory and computation power to train. Further, large
number of parameters also mean that the model is highly
susceptible to overﬁtting as well, if trained with insufﬁ-
cient data. To resolve such issues, researchers have sought
ways to make the model more compact and lightweight by
parameter reduction, via model compression (Ba & Caru-
ana, 2014; Hinton et al., 2014), or removing unnecessary
weights either by pruning (Reed, 1993; Han et al., 2015)
and (cid:96)1-regularization (Collins & Kohli, 2014). However,
one of the main problems of these methods is that they of-
ten achieve such efﬁciency at the expense of accuracy.

How can we then obtain a compact deep network without
sacriﬁcing the prediction accuracy? One way to achieve
this goal is better utilizing the capacity of the network, by
reducing redundancies in the model parameters. In the op-
timal case, the weights at each layer will be fully orthog-
onal to each other, and thus forming an orthogonal basis
set. However, since this is a difﬁcult constraint to sat-
isfy, in practice, such constraint is given only at the ini-
tialization stage (Saxe et al., 2014), or enforced implicitly
through regularizations such as dropout (Srivastava et al.,
2014) that prevents feature co-adaption. Contrary to these
existing approaches, we propose to impose an explicit reg-
ularization to reduce redundancies. Our idea is to enforce
network weights at each layer to ﬁt to different sets of input
features as much as possible. This exclusive feature learn-
ing is implemented by the exclusive sparsity regularization
based on (1, 2)-norm (Zhou et al., 2010; Kong et al., 2014),
which basically promotes network weights at each layer to
compete for few meaningful features from the lower layer.

However, it is not practical nor desirable to restrict each
weight to be completely disjoint from others as some fea-
tures still need to be shared. For example, if the lower-
layer feature is a wheel, and the upper layer weights are
features describing car and bicycle respectively, then the
two upper layer weights should share the common feature
that describes the wheel. Thus, we also allow for sharing
of some important features, by introducing an additional
group sparsity regularizer based on (2, 1)-norm and com-
bine the two regularization terms, balancing their effect at
each layer of the network to adjust the degree of feature
sharing and competition.

Combined Group and Exclusive Sparsity for Deep Neural Networks

Our combined regularizer can be applied to all layers
of a generic deep neural network, including plain fully-
connected feedforward networks and convolutional net-
works. We validate our regularized network on four public
datasets with different base networks, on which it achieves
a compact, lighter model while achieving superior perfor-
mance over networks trained with other sparsity-inducing
regularizers, sometimes obtaining even better accuracy
than the full model. As an example, on CIFAR-10 dataset,
our network obtains 2.17% accuracy improvements while
using 13.72% less number of parameters and 35.67% less
ﬂoating point operations. Further empirical analysis shows
that exclusive sparsity helps the network to converge faster
to a given error rate, and learn less redundant features.

2. Related Work

Sparsity for deep neural networks Obtaining compact
deep networks by removing unnecessary weights, is a long-
studied topic in deep learning research. The most sim-
plest yet popular weight removal method is to prune out
weak weights by simple thresholding (Reed, 1993; Han
et al., 2015). Another way to induce sparsity on weights
is by (cid:96)1-regularization (Tibshirani, 1994). Collins & Kohli
(2014) applied the (cid:96)1-regularization to convolutional neu-
ral networks, demonstrating that it can obtain a compact,
memory-efﬁcient network at the expense of small reduc-
tion in the prediction accuracy. Few recent work applied
group sparsity (Yuan & Lin, 2006) regularization to deep
networks, as it has a number of nice properties. By remov-
ing an entire feature group, group sparsity can automati-
cally decide the number of neurons (Alvarez & Salzmann,
2016). Further, if applied between the weights at different
layers, it can also be used to decide optimal number of lay-
ers to use for the given network (Wen et al., 2016). In terms
of efﬁciency, structured sparsity using (2, 1)-norm exhibits
better data locality than the regular sparsity, and results in
larger speedups (Wen et al., 2016; Alvarez & Salzmann,
2016). We also employ the group sparsity in our com-
bined regularizer, but we mainly group the features across
multiple ﬁlters, to promote feature sharing among the ﬁl-
ters. While all the previously introduced models do help
reduce number of parameters and result in certain amount
of speedups, such memory and time efﬁciency is mostly
obtained at the expense of reduced accuracy. Our com-
bined group and exclusive sparsity regularization, on the
other hand, do not degenerate performance, since its aim in
learning sparse weights/features is in removing redundancy
to better utilize the network capacity.

Exclusive feature learning There exists quite a number
of work on imposing exclusivity among the learned model
parameters/features. One popular way is to enforce orthog-
onality, as this will minimize the dependency and redun-

dancy among the variables that are being regularized. Or-
thogonality at initialization stage has been much studied in
the deep learning context (Saxe et al., 2014), as in such
a non-convex optimization setting this can lead to conver-
gence to a better local optimum. Zhou et al. (2011) en-
forced orthogonality via explicit dot product regulariza-
tion to make the parameters for parent-level and child-
level classiﬁers in a hierarchical classiﬁer to be orthog-
onal. However, the orthogonal regularizer is non-convex
and does not scale well, since it scales quadratically to the
number of participating vectors. Another way to enforce
exclusivity is through (1, 2)-norm, which is basically the
2-norm over 1-norm groups, that results in promoting spar-
sity across different vectors. The (1, 2)-norm is ﬁrst pro-
posed in (Zhou et al., 2010), where it is used to promote
competitions among the models jointly learned in a multi-
task learning framework. A similar regularizer was used
in (Hwang et al., 2011) in a metric learning setting, with
an additional (cid:96)1-regularization that helps learn discrimina-
tive features for each metric. Kong et al. (2014) general-
ized the (1, 2)-norm to be used with arbitrary objective and
handle overlapping groups. In deep learning context, Goo
et al. (2016) proposed a difference pooling technique that
has a similar motivation to exclusive lasso, which subtracts
the common superclass level feature map from the class-
speciﬁc feature maps to learn class-exclusive features for
ﬁne-grained classiﬁcation. In all existing models, exclusiv-
ity is applied only at the class-level, and application of the
exclusivity regularization to weights at any layers of deep
networks through (1, 2)-norm, has not yet been explored.
Further, our regularizer is a combined term of both group
and exclusive lasso which allows sharing of important fea-
tures while making each weight to be as different as possi-
ble, rather than purely exclusive feature learning that is im-
practical. The regularizer proposed in (Kim & Xing, 2010)
is similar to ours, which proposes a weighted (2, 1)-norm
that has a similar effect of varying the degree of competi-
tion and grouping, although our regularizer is more explicit
in its effect and optimization.

3. Approach

Our main objective is to implement a sparse deep neu-
ral network with signiﬁcantly less number of parameters
than what the original non-sparse network has, which at
the same time obtains comparable or even better perfor-
mance to the original model. The training objective for a
generic (deep) neural network for classiﬁcation1 is given as
follows:

min
{W (l)}

L({W (l)}, D) + λ

Ω(W (l))

(1)

L
(cid:88)

l=1

1While our method is generic and can be also applied to re-

gression, we only consider the classiﬁcation task for simplicity.

Combined Group and Exclusive Sparsity for Deep Neural Networks

Here, D = {xi, yi}N
i=1 is a training dataset with N in-
stances where xi ∈ Rd is a d-dimensional input feature
and yi ∈ {1, . . . , K} is its class label which is one of the
K classes, {W (l)} is the set of weights across all layers,
L(W ) is the loss parameterized by W , L is the total num-
ber of layers, W (l) is the weight matrix (or tensor) for
layer l, Ω(W (l)) is some regularization term on the net-
work weights at layer l, and λ is the regularization param-
eter that balances the loss with the regularization.

The usual and the most often used regularization term is the
2-norm: Ω(W (l)) = (cid:107)W (l)(cid:107)2
2, which is also called as the
(cid:96)2-regularizer. The regularization has an effect of adding
a bias term to reduce variance of the model, which in turn
results in a lower generalization error.

However, since our goal is in obtaining a sparse model
where large portion of W (l) is zeroed out, we want
Ω(W (l)) to be a sparsity-inducing regularizer. The most
common regularizer for promoting sparsity is the 1-norm:

Ω(W (l)) = (cid:107)W (l)(cid:107)1

(2)

This 1-norm regularization results in obtaining a sparse
weight matrix, since it requires the solution to be found at
the corner of the 1-norm ball, thus eliminating unnecessary
elements. The element-wise sparsity can be helpful when
most of the features are irrelevant to the learning objective,
as in the data-driven approaches. However, as aforemen-
tioned, when applied to a deep network it usually results
in slight accuracy reduction. Further, element-wise spar-
sity, while achieving a memory-efﬁcient model, usually do
not result in meaningful speedups in practical network ar-
chitectures such as CNNs, since the bottleneck is in the
convolutional operations that do not reduce much when the
number of ﬁlters stays the same (Wen et al., 2016).

Group sparsity, on the other hand, can help reduce the in-
trinsic complexity of the model by eliminating a neuron
or a convolutional ﬁlter as a whole, and thus can help ob-
tain practical speedups in deep neural networks (Wen et al.,
2016; Alvarez & Salzmann, 2016). The group sparsity reg-
ularization is deﬁned as follows:

Ω(W (l)) =

(cid:107)W (l)

g (cid:107)2 =

(cid:88)

g

(cid:115)

(cid:88)

(cid:88)

g

i

2

w(l)
g,i

(3)

g

where g ∈ G is a weight group, W (l)
is the weight matrix
(or a vector) for group g that is deﬁned on W (l), and wg,i
is a weight at index i, for group g. Since (cid:96)2-norm has the
grouping effect that results in similar weights for correlated
features, this will result in complete elimination of some
groups, thus removing some input neurons (See Figure 3,
(a)). This has an effect of automatically deciding how many
neurons to use at each layer.

(a) Group Sparsity

(b) Exclusive Sparsity

Figure 1. Illustration of the regularizers: (a) When grouping
weights from the the same input neuron into each group, the group
sparsity has an effect of completely removing some neurons that
are not shared across different weights (highlighted in red). (b)
Exclusive sparsity, on the other hand, does not result in removal
of any input neurons, but rather it makes each upper layer unit to
select from a set of lower-layer units, that is disjoint from the sets
used by other units.

Still, this group sparsity does not maximally utilize the ca-
pacity of the network since there still could be redundancy
among the features that are selected. Thus, we propose
to apply a sparsity-inducing regularization that obtains a
sparse network weight matrix, while also minimizing the
redundancy among network weights for better utilization
of the network capacity.

3.1. Exclusive Sparsity Regularization for Deep Neural

Networks

Exclusive sparsity, or exclusive lasso was ﬁrst introduced
in (Zhou et al., 2010) in multi-task learning context. The
main idea in the work is to enforce the model parameters
for different tasks to compete for features, instead of shar-
ing features as suggested by previous work on multi-task
learning that leverages group lasso. When the task is a
classiﬁcation task, this makes sense since the objective is
to differentiate between classes which can be achieved by
identifying discriminative feature for each class.

The generic exclusive sparsity regularization is deﬁned as
follows:

Ω(W (l)) =

(cid:107)W (l)

g (cid:107)2

1 =

1
2

(cid:88)

g

(cid:32)

(cid:88)

(cid:88)

(cid:33)2

|w(l)
g,i|

(4)

1
2

g

i

where w(l)
g,i is the ith instance of the submatrix (or the vec-
tor) W (l)
g . This norm is often called as (1, 2)-norm, and
is basically the 2-norm over 1-norm groups. The spar-
sity is now enforced within each group, as opposed to
the group sparsity regularizer which promotes inter-group
sparsity. Applying 2-norm over these 1-norm groups will
result in even weights among the groups; that is, all groups
should have similar number of non-sparse weights, and
thus no group can have large number of non-sparse weight.
In (Zhou et al., 2010), Wg is deﬁned to be the model pa-
rameter for multiple tasks on the same feature, in which
case the (1,2)-norm enforces each task predictor to ﬁt to

Combined Group and Exclusive Sparsity for Deep Neural Networks

(a) Group Sparsity - Filter

(b) Group Sparsity - Feature

(c) Exclusive Sparsity

Figure 2. Illustration of the effect of each regularizer on convoultional ﬁlters. (a) Group sparsity, when each group is deﬁned as a
ﬁlter, can result in complete elimination of some ﬁlters that are not shared among multiple high-level ﬁlters (Wen et al., 2016; Alvarez &
Salzmann, 2016). (b) Group sparsity, when applied across ﬁlters for the same feature, will remove certain spatial features as a whole. (c)
Exclusive sparsity enforces each convolutional ﬁlter to learn features that are as different as possible, by promoting competition among
the ﬁlters for the same spatial feature.

few features that are most useful for it. Exclusive spar-
sity can be straightforwardly applied to fully connected lay-
ers of a deep network, by grouping network weights from
the same neuron at each layer into one group and apply-
ing (1, 2)-norm on these groups (See Figure 1(b)). This
will enforce each output neuron to compete for input neu-
rons, which will result in learning largely disparate network
weights at each layer.

Exclusive sparsity on convolutional ﬁlters For convo-
lutional layers of a convolutional neural network, exclu-
sive sparsity can be applied in the same manner as in fully
connected layers, where we apply Eq. 4 on the convolu-
tional ﬁlters, while deﬁning each group g as the same fea-
ture across multiple convolutional ﬁlters. Figure 2(c) il-
lustrates the feature groups and effect of exclusive sparsity
on the convolutional ﬁlters. This will enforce the convolu-
tional ﬁlters to be as different as possible from each other,
removing any redundancies between them.

3.2. Combined Group & Exclusive Sparsity

Regularization

As mentioned earlier, our main intuition is that there are
varying degree of sharing and exclusivity among different
features. Exclusivity alone cannot result in learning an op-
timal set of features, since some features need to be shared
across multiple higher-level features. Thus we need to al-
low for some degree of sharing across the features, while
still making each weight to be sufﬁciently different in or-
der for each feature to be meaningful. How can we then
come up with a regularizer that can achieve the two seem-
ingly conﬂicting goals?

In tree guided group lasso (Kim & Xing, 2010), each pair
of weights are given different degree of sharing and com-
petition based on the similarity between the tasks given by
a taxonomy, which can be either semantically deﬁned or
obtained through clustering, through a regularization simi-
lar to an elastic-net formulation. While this model can be

applied at the ﬁnal softmax layer, on the softmax weight
for each class, such taxonomy does not exist for the inter-
mediate level network weights, and it is also not efﬁcient to
obtain them through clustering or other means.

Thus we propose to simply combine the group sparsity and
the exclusive sparsity together, which will result in a simi-
lar effect, where network weights exhibit certain degree of
sharing if they are correlated, but are learned to be different
on other parts that are not shared. Our combined group and
exclusive lasso regularizer is given as follows:

Ω(W (l)) =

(1 − µl)(cid:107)W (l)

g (cid:107)2 +

(cid:107)W (l)

g (cid:107)2
1

(5)

(cid:19)

µl
2

(cid:18)

(cid:88)

g

where λ is the parameter that decides the entire regulariza-
tion effect, W l is the weight matrix for lth layer, and µl
is the parameter for balancing the sharing and competition
term at each layer.

Then how should we set the balancing term µl at each
layer? One simple solution is to set all µl to be a single
constant, but a better way is to set them differently at each
layer, based on the degree of sharing and competition re-
quired at each layer. At lower layers, features will be quite
generic and might need to be shared across all high-level
neurons for accurate expression of the input data, wheareas
at the top layer softmax weights, it would be better to have
the weights to select features as disjoint as possible for bet-
ter discriminativity. Thus, we set µl = m + (1 − 2m) l
L−1 ,
to reﬂect such intuition, where L is a total number of all
layers, l ∈ {0, ..., L − 1} is an index of each layer, and
0 ≤ m ≤ 1 is the lowest parameter value for the exclusive
sparsity term. If m = 0, the regularizer reduces to (2, 1)-
norm regularizer with µ1 = 0 at the lowest layer, while at
the topmost softmax layer, the regularizer is an (1, 2)-norm
regularizer µ1 = 1.

3.3. Numerical Optimization

Our regularized learning objective can be solved using
proximal gradient descent, which is often used for opti-
mizing objectives formed as a combination of both smooth

Combined Group and Exclusive Sparsity for Deep Neural Networks

and non-smooth terms. The proximal gradient algorithm
for regularized objective ﬁrst obtains the intermediate so-
lution Wt+ 1
by taking a gradient step using the gradient
computed on the loss only, and then optimize for the regu-
larization term while performing Euclidean projection of it
to the solution space, as in the following formulation:

2

min
Wt+1

Ω(Wt+1) +

1
2λs

(cid:107)Wt+1 − Wt+ 1

(cid:107)2
2

2

(6)

where Wt+1 is the variable to obtain after the current itera-
tion, λ is the regularization parameter, and s is the step size.
When Ω(Wt+1) is a group sparsity regularizer or an exclu-
sive sparsity regularizer, the above problem has a closed-
form solution.

The solution, or the proximal operator for the group spar-
sity regularizer, proxGL(W ) is given as follows:

plied at the network weights for all layers, excluding the
bias term. All models are implemented and experimented
using Tensorﬂow (Abadi et al., 2016) framework 2.

Baselines and our models We compare our regularized
networks against relevant baselines.

1) (cid:96)2. The network trained with (cid:96)2-regularization.

2) (cid:96)1. The network trained with (cid:96)1-regularization, which
has elementwise sparse network weights.

3) Group Sparsity-Filter. The network regularized with
(cid:96)2,1-norm on the weights, which groups each convolutional
ﬁlter as a group at convolutional layers. This network is an
implementation of the model in (Wen et al., 2016).

4) Group Sparsity-Feature. The network that uses the
same (cid:96)2,1-regularization as in 3), but with each group de-
ﬁned as the same feature at different ﬁlters.

proxGL(W) =

1 −

(cid:18)

(cid:19)

λ
(cid:107)wg(cid:107)2

wg,i

+

(7)

5) Exclusive Sparsity. This is the network whose weights
at each layer are regularized with (cid:96)1,2-norm only.

for all g and i, where g is each group, and i is an element
of in each group. The proximal operator for the exclusive
sparsity regularizer, proxEL(W), is obtained as follows:

6) Combined Group and Exclusive Sparsity. The net-
work regularized with our combined structured sparsity on
the weights. The combination weight that balances both
regularizations are dynamically set at each layer.

proxEL(W) =

1 −

(cid:18)

= sign(wg,i)((cid:12)

(cid:12)wg,i

(cid:19)

wg,i

λ(cid:107)wg(cid:107)1
|wg,i|
(cid:12)
(cid:12) − λ(cid:107)wg(cid:107)1)+

+

for all g and i. The combined regularizer can be optimized
simply by applying the two proximal operators in a row at
each gradient step, after updating the variable with the loss-
based gradient. Algorithm 1 describes the proximal gradi-
ent algorithm for optimizing our regularized objective.

Algorithm 1 Stochastic Proximal Gradient Algorithm for
Combined (2,1)- and (1,2)- regularization
Input: W, λ, µ, mini-batch size b, learning rate η

t − ηst

Initialize W, t
while Some predeﬁned stopping criterion is satisﬁed do
Randomly select b samples from p ∈ {1, 2, ..., n},
for each layer l, do
:= W(l)

W(l)
t+ 1
2
parameter with the gradient of a non-regularized objective
W(l)
) (cid:46) Apply proxGL in Eq. 7
t+ 1
t+1 = proxEL(W (l)
W(l)
t+ 1
end for
end while

= proxGL(W(l)
t+ 1
2
) (cid:46) Apply proxEL in Eq. 8

p (cid:53)fp(W(l)
t )

(cid:46) Update the

2 ,GL

2 ,GL

(cid:80)

b

4. Experiment

We perform all experiments with convolutional neural net-
work as the base network model. The regularization is ap-

(8)

Datasets and base networks We validate our method on
four public datasets for classiﬁcation, with four different
convolutional networks.

1) MNIST. This dataset contains 70, 000 28 × 28 grayscale
images of handwritten digits for training example images,
where there is 6, 000 training instances and 1, 000 test in-
stances per class. As for the base network, we use a simple
convolutional neural network with two convolutional layers
and two fully connected layers.

2) CIFAR-10. This dataset consists of 60, 000 images
sized 32 × 32, from ten animal and vehicle classes (air-
plane, automobile, bird, cat, deer, dog, frog, horse, ship,
and truck). For each class, there are 5, 000 images for train-
ing and 1, 000 images for test. For the base network, we
use LeNet (Lecun et al., 1998), that has two convolutional
layers followed by three fully connected layers.

3) CIFAR-100. This dataset also consists of 60, 000 im-
ages of 32 × 32 pixels as in CIFAR-10, but has 100 generic
object classes instead of 10. For each class, 500 images
are used for training and 100 images are used for test. For
the base network, we use a variant of Wide Residual Net-
work (Zagoruyko & Komodakis, 2016), which has 16 lay-
ers with the widening factor of k = 10.

4) ImageNet-1K. This is the dataset for 2012 ImageNet

2Codes available at https://github.com/jaehong-yoon93/CGES

Combined Group and Exclusive Sparsity for Deep Neural Networks

(a) MNIST, Accuracy / # of Parameters

(b) CIFAR-10, Accuracy / # of Parameters

(c) CIFAR-100, Accuracy / # of Parameters

(d) MNIST, Accuracy / FLOP

(e) CIFAR-10, Accuracy / FLOP

(f) CIFAR-100, Accuracy / FLOP

Figure 3. Accuracy-efﬁciency trade-off. We report the accuracy over number of parameters, and accuracy over FLOP to see how each
sparsity-inducing regularization at various sparsity range affect the model accuracy. The reported results are average model accuracy
over three runs (with random weight initialization), and the errorbars denote standard errors for 95% conﬁdence interval. L2 and L1 are
the networks trained with (cid:96)2-regularization, (cid:96)1-regularization, GS-ﬁlter and GS-feature are ﬁlter-wise and feature-wise group sparsity
respectively, ES is our exclusive sparsity regularizer, and CGES is our proposed combined group and exclusive sparsity regularizer.

Large Scale Visual Recognition Challange (Deng et al.,
2009) that consists of 1, 281, 167 images from 1, 000
generic object categories. For evaluation, we used the val-
idation set that consists of 50, 000 images, following the
standard procedure. For the base network, we used an im-
plementation of AlexNet (Krizhevsky et al., 2012).

For MNIST and CIFAR-10 experiment, we train all net-
works from the scratch; for CIFAR-100, and ImageNet-
1K experiment where we use larger networks (WRN and
AlexNet) we ﬁne-tune the network from the (cid:96)2- regular-
ized networks, since training them from scratch takes pro-
hibitively long time.

4.1. Quantitative analysis

We ﬁrst validate whether our sparsity-inducing regular-
izations result in better accuracy-efﬁciency trade-off com-
pared to baseline methods, by measuring the prediction ac-
curacy over number of parameters, and number of ﬂoating
point operations (FLOP) for each method.

Figure 3 shows the prediction accuracy of the different
models over number of parameters/FLOP, obtained by dif-
ferentiating the sparsity-inducing regularization parameter
for each method. As expected, (cid:96)1-regularization greatly re-
duces the number of parameters, while maintaining a sim-
ilar performance to the original model. The group spar-
sity regularization in general performs worse than (cid:96)1, but

achieves better accuracy in certain sparsity ranges. The ex-
clusive sparsity improves the performance over the base (cid:96)2-
regularization model in low-sparsity range which is espe-
cially well shown in CIFAR-10 result, but degenerates per-
formance as the sparsity increases. We attribute this to the
fact that exclusive sparsity aims to make each weight/ﬁlter
to ﬁt to completely disjoint sets of low-level features, which
is unrealistic as features may need to ﬁt to the same set of
low-level features for accurate representation.

Finally, our combined group and exclusive sparsity,
which allows for certain degree of sharing between the
weights/features while enforcing exclusivity, achieves the
best accuracy/parameter trade-off, achieving similar or bet-
ter performance gain to the exclusive sparsity while also
greatly reducing the number of parameters. Fig 3(a) shows
the results on the MNIST dataset, on which our CGES ob-
tains no accuracy reduction, using 36.48% less number of
parameters and 14.46% less computation. On CIFAR-10
dataset, CGES improves the classiﬁcation accuracy over
the (cid:96)2 baselines by 2.17%, using 13.72% less number of
parameters using 35.67% less FLOP. CGES obtains slight
accuracy reduction of 1.15% on CIFAR-100 dataset, using
only 51.22% of parameters and 42.77% less FLOP.

On ImageNet (Table 1), CGES obtains similar or slightly
worse performance to the full network while using 60% −
68% of its parameters, while (cid:96)1 shows noticeable perfor-
mance degeneration at the same sparsity level.

0.10.20.30.40.50.60.70.80.910.940.950.960.970.980.991Percentage of Parameters used (%)Accuracy  L2L1GS−filterGS−featureESCGES0.30.40.50.60.70.80.910.750.760.770.780.790.80.81Percentage of Parameters used (%)Accuracy  L2L1GS−filterGS−featureESCGES0.50.60.70.80.910.740.750.760.770.780.79Percentage of Parameters used (%)Accuracy  L2L1GS−filterGS−featureESCGES0.10.20.30.40.50.60.70.80.910.940.950.960.970.980.991Flop (x)Accuracy  L2L1GS−filterGS−featureESCGES0.30.40.50.60.70.80.910.760.770.780.790.80.81Flop (x)Accuracy  L2L1GS−filterGS−featureESCGES0.50.60.70.80.910.740.750.760.770.780.79Flop (x)Accuracy  L2L1GS−filterGS−featureESCGESCombined Group and Exclusive Sparsity for Deep Neural Networks

(a) Convergence speed

(b) Conv vs. FC layers

(c) Effect of µl

Figure 4. Further Analysis of the exclusive sparsity on CIFAR-10 dataset. (a) Convergence speed: Networks regularized with ES (Light
Blue) or CGES (Dark Blue) converge fastest to a given error rate, compared to (cid:96)2. (b) Effect of exclusive sparsity at different types
of layers: The network regularized with exclusive sparsity at all layers performed better with higher sparsity, compared to models that
used ES only at convolutional, or fully connected layers. (c) Effect of µl: ES-increasing is our combined regularizer, where exclusivity
increases with network layer l. For ES-constant, we set µl = 0.5 at all layers.

Table 1. Accuracy-efﬁciency trade-off on the Imagenet dataset.
Model Accuracy % Params. Accuracy % Params.

L2
L1
ES
CGES

59.89%
57.55%
57.99%
58.56%

100.0%
60.39%
60.41%
60.66%

-
58.45%
58.89%
59.25%

-
66.75%
67.37%
67.24%

Table 2. Performance of CGES coupled with iterative pruning.
The reported results are averages over 3 runs and standard errors
for 95% conﬁdence interval.
Model
L2 (Full Network)
Han et al. (2015)
CGES

98.71 ± 0.03% 76.37 ± 0.42%
99.16 ± 0.03% 78.97 ± 0.41%

CIFAR-10
78.15%

MNIST
99.20%

Iterative pruning Iterative pruning (Han et al., 2015) is
another effective method for obtaining a sparse network
while maintaining high accuracy. As iterative pruning is
orthogonal to our method, we can couple the two methods
to obtain even better performance per number of parame-
ters used; speciﬁcally, we replace the usual weight decay
regularizer used in (Han et al., 2015) with our CGES reg-
ularizer. We report the accuracy of this combined model
on MNIST and CIFAR-10 dataset, when using 10% of the
parameters of the full network (Table 2). The results show
that CGES coupled with iterative pruning obtains similar or
even better results to the original model using only a frac-
tion of the parameters, signiﬁcantly outperforming the base
pruning model which suffers substantial accuracy loss.

Convergence speed We further analyze the empirical
convergence rate of our regularized network, since it will
be impractical if the regularized network requires much
longer iterations to reach the same accuracy.
Interest-
ingly, we empirically found that our exclusive sparsity reg-
ularizer also helps network achieve the same error using
much fewer iterations (Figure 4(a)), compared to base (cid:96)2-
regularization. This faster convergence agrees with the ob-
servations in (Saxe et al., 2014), where networks whose
weights are initialized as random orthgonal matrices con-

verged faster than networks with random Gaussian initial-
ization.

Convolutional vs.
fully connected layers To see how
much effect our combined regularizer has on different types
of layers, we experiment applying the model only to the
fully connected layer, or convolutional layers, while apply-
ing usual (cid:96)2-regularizer to other layers. Figure 4(b) shows
the result of this experiment, where we plot the accuracy
over percentage of parameters used, for models that applies
ES only to fully connected layers, only to convolutional
layers, and both. We observe improvements on all mod-
els, which shows the effectiveness of the exclusive spar-
sity regularizer to all types of network weights. Further,
ES results in larger improvements on convolutional layers,
which makes sense since lower-layer features are more im-
portant as they are more generic across different classes,
than the features learned at fully connected layers. How-
ever, conv layers obtained the best accuracy at low-sparsity
range, since strict enforcement of exclusivity hurts the rep-
resentational power of the features, whereas FC layers ob-
tained improvements even on high-sparsity range; this may
be because loss of expressiveness could be compensated by
better discriminativity of the features at high level.

Sharing vs. Competing for Features We further ex-
plore how varying the degree of sharing and competition
affect the accuracy and efﬁciency of the model, by experi-
menting with different conﬁgurations of µl in Eq. 5 at each
layer. We report the results in Figure 4(c). Speciﬁcally,
we test two different approaches to balance the degree of
sharing and competition at each layer. The ﬁrst model,
ES-Increasing, is the actual combination we have used in
our method which increases the effect of exclusive spar-
sity with increasing l. This model reﬂects our intuition
that competition will help at high layers, while sharing will
help more at lower layers. The second model, ES-Constant
combines the two terms with µl = 0.5 throughout all lay-
ers. We observe that ES-Increasing works better than ES-

020406080100120203040506070Iteration (K)Error (%)  L2ESCGES0.880.90.920.940.960.9810.780.7850.790.7950.80.8050.81Percentage of Parameters used (%)Accuracy  L2ES−convES−fcES−full0.20.40.60.810.740.760.780.8Percentage of Parameters used (%)Accuracy  L2ES−IncreasingES−ConstantCombined Group and Exclusive Sparsity for Deep Neural Networks

(a) (cid:96)2-regularization

(b) (cid:96)1-regularization

(c) Group sparsity

(d) Exclusive sparsity

(e) CGES

Figure 5. Visualizations of the last fully connected layer weights on the CIFAR-10 dataset. These ﬁgures show the weight of ﬁrst 50
weights out of 192 weights. The rows are output units for each class and the columns are features.

(a) (cid:96)2, Sparsity: 0.00 %

(b) (cid:96)1, Sparsity: 71.3 %

(c) GS-Filter, Sparsity: 54.7 % (d) CGES, Sparsity: 73.5 %

Figure 6. Visualizaiton of the 1st convolution layer ﬁlters from the network trained on CIFAR-10 dataset. (a) (cid:96)2-regularization results
in smooth non-sparse ﬁlters. (b) (cid:96)1-regualrization results in ﬁlters that are elementwise sparse. (c) GS-Filter results in complete removal
of some ﬁlters. (d) CGES obtains sharper ﬁlters with some spatial features completely zeroed out, from competition among the ﬁlters.

Constant across all sparsity ranges, which shows that our
scheme of increasing exclusivity at higher layers indeed
helps improve the model performance.

4.2. Qualitative analysis

For further qualitative analysis, we visualize the weights
and convolutional ﬁlters obtained using the baselines and
our methods.

Figure 5 visualizes the weights of the softmax layer for dif-
ferent regularization methods, from the network trained on
the CIFAR-10 dataset. Each row is the softmax parame-
ter for each class. (cid:96)2 and (cid:96)1 work as expected, resulting
in non-sparse and elementwise sparse weights. The group
sparsity regularizer results in the total elimination of certain
features that are not shared across multiple classes. The
exclusive sparsity regularizer, when used on its own, re-
sults in disjoint feature selection for each class. However,
when combined with the group lasso, it allows certain de-
gree of feature reuse, while still obtaining parameters that
are largely disparate across classes.

To show that such effect is not conﬁned to the fully con-
nected layer, we also visualize the convolutional ﬁlters in
the ﬁrst convolutional layer of the network trained on the
CIFAR-10 dataset, in Figure 6. We observe that the com-
bined group and exclusive sparsity regularizer results in ﬁl-
ters that are much sharper than the ones that are obtained by
(cid:96)1 or group sparsity regularization, with some spatial fea-
tures dropped altogether from the competition with other
ﬁlters. Further, there is less redundancy among the ﬁlters,

unlike the ﬁlters learned by other regularization methods.
Note that we set the exclusivity factor µ1 = 0.8 just for
visualization purpose, since our weighting scheme will set
µ1 as a low value in the ﬁrst convolutional layer.

5. Conclusion

In this work, we proposed a novel regularizer for generic
deep neural networks that effectively utilizes the capacity
of the network, by exploiting the sharing and competing
relationships among different network weights. Speciﬁ-
cally, we propose to use an exclusive sparsity regularization
based on (1, 2)-norm on the network weights, along with
group sparsity regularization using (2, 1)-norm, such that
exclusive sparsity enforces the network weights to use in-
put neurons that are as different as possible from the other
weights, while the group sparsity allows for some degree
of sharing among them, as it is impossible to make the net-
work weights to ﬁt to completely disjoint set of features.
We validate our method on some public datasets for both
the accuracy and efﬁciency against other sparsity-inducing
regularizers, and the results show that our combined regu-
larizer helps obtain even better performance than the orig-
inal full network, while signiﬁcantly reducing the memory
and computation requirements.

Acknowledgements This work was supported by Ba-
sic Science Research Program through the National
Research Foundation of Korea (NRF) funded by the
Ministry of Science,
ICT & Future Planning (NRF-
2016M3C4A7952634), and UAV-technology development

Combined Group and Exclusive Sparsity for Deep Neural Networks

program through the National Research Foundation of Ko-
rea (NRF) funded by the Korea Aerospace Research Insti-
tute (NRF-2016M1B3A1A01937742).

Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
based Learning Applied to Document Recognition. Pro-
ceedings of the IEEE, 86(11):2278–2324, 1998.

References

Abadi, Mart´ın, Agarwal, Ashish, Barham, Paul, Brevdo,
Eugene, Chen, Zhifeng, Citro, Craig, Corrado, Greg S,
Davis, Andy, Dean, Jeffrey, Devin, Matthieu, et al. Ten-
sorﬂow: Large-scale Machine Learning on Heteroge-
neous Distributed Systems. arXiv:1603.04467, 2016.

Alvarez, Jose M and Salzmann, Mathieu. Learning the
number of neurons in deep networks. In NIPS. 2016.

Ba, Jimmy and Caruana, Rich. Do deep nets really need to

be deep? In NIPS, 2014.

Collins, Maxwell D and Kohli, Pushmeet. Memory
bounded deep convolutional networks. arXiv preprint
arXiv:1412.1442, 2014.

Reed, R. Pruning algorithms-a survey. IEEE Transactions
ISSN

on Neural Networks, 4(5):740–747, Sep 1993.
1045-9227. doi: 10.1109/72.248452.

Saxe, Andrew M., McClelland, James L., and Ganguli,
Surya. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks. In ICLR, 2014.

Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A
simple way to prevent neural networks from overﬁtting.
Journal of Machine Learning Research, 15:1929–1958,
2014.

Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,
Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du-
mitru, Vanhoucke, Vincent, and Rabinovich, Andrew.
Going Deeper with Convolutions. In CVPR, 2015.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and ei,
L. Fei-F˙ Imagenet: A Large-Scale Hierarchical Image
Database. In CVPR, 2009.

Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society, Series B,
58:267–288, 1994.

Goo, Wonjoon, Kim, Juyong, Kim, Gunhee, and Hwang,
Sung Ju. Taxonomy-Regularized Semantic Deep Con-
volutional Neural Networks. In ECCV, 2016.

Wen, Wei, Wu, Chunpeng, Wang, Yandan, Chen, Yiran,
and Li, Hai. Learning structured sparsity in deep neural
networks. In NIPS, pp. 2074–2082. 2016.

Han, Song, Pool, Jeff, Tran, John, and Dally, William.
Learning both weights and connections for efﬁcient neu-
ral network. In NIPS. 2015.

Yuan, Ming and Lin, Yi. Model selection and estimation in
regression with grouped variables. Journal of the Royal
Statistical Society, Series B, 68:49–67, 2006.

Zagoruyko, Sergey and Komodakis, Nikos. Wide residual

networks. In BMVC, 2016.

Zhou, D., Xiao, L., and Wu, M. Hierarchical Classiﬁcation

via Orthogonal Transfer. In ICML, 2011.

Zhou, Yang, Jin, Rong, and Hoi, Steven C. H. Exclusive
lasso for multi-task feature selection. Journal of Ma-
chine Learning Research, 9:988–995, 2010.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep residual learning for image recognition. In
CVPR, 2016.

Hinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff. Distilling
the knowledge in a neural network. In NIPS 2014 Deep
Learning Workshop, 2014.

Hwang, Sung Ju, Grauman, Kristen, and Sha, Fei. Learning
a tree of metrics with disjoint visual features. In NIPS,
2011.

Kim, S. and Xing, E. P. Tree-guided group lasso for multi-
In ICML, pp.

task regression with structured sparsity.
543–550, 2010.

Kong, Deguang, Fujimaki, Ryohei, Liu, Ji, Nie, Feiping,
and Ding, Chris. Exclusive feature learning on arbitrary
structures via (cid:96)1, 2-norm. In NIPS. 2014.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
ImageNet Classiﬁcation with Deep Convolutional Neu-
ral Networks. In NIPS, 2012.

