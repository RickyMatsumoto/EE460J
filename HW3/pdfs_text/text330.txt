Supplementary File for Paper
SplitNet: Learning to Semantically Split Deep Networks
for Parameter Reduction and Model Parallelization

Juyong Kim * 1 Yookoon Park * 1 Gunhee Kim 1 Sung Ju Hwang 2 3

0 and 1. The experiments are done on CIFAR-100 and use
WRN-16-8 as the base network. K = 5 and all other hy-
perparameters are same.

1. Reduced Gradient Method

Given the following optimization J with sum-to-one con-
straint:

RM J(d)

mind
subject to d (cid:62) 0,

∈

(1)

dm = 1,

m

(cid:88)

The reduced gradient of J with respect to d is deﬁned as
follows(Rakotomamonjy et al., 2008):

(∇redJ)m =

∂J
∂dµ

if dm = 0 and
∂J
−
∂dm
if dm > 0 and
m (cid:54)= µ

> 0

,

∂J
∂dµ

)

if m = µ

0






∂J
∂dµ
∂J
∂dν −

(

∂J
∂dm −

−

=µ
ν
dν >0
(cid:80)

(2)
where µ is the index of the largest component of vector
d. Then we can apply (stochastic) gradient descent to op-
γ∇redJ, where γ is the step size.
timize J with d
The maximal admissible size of γ is the value that make
any component of d zero.

←

−

d

Since the constraint is given as
g pgi = 1 in our problem,
the reduced gradients method is applied to each group of

p
∗

i}

{

and

q
{
∗

i}

separately.

(cid:80)

1.1. Convergence of Group Assignments

As mentioned in the paper, the direct optimization of group
assignment variables with reduced gradients yields faster
convergence than optimization via softmax reparametriza-
tion. Figure 1 shows the distribution plots, which are pro-
vided by TensorFlow, of class-to-group assignments using
two methods. Despite starting with lower variance, when
the distribution of group assignment variables diverged to

*Equal contribution 1Seoul National University, Seoul, South
Korea 2UNIST, Ulsan, South Korea 3AITrics, Seoul, South Korea.
Correspondence to: Sung Ju Hwang <sjhwang@unist.ac.kr>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

(a) Softmax Reparameteriza-
tion

(b) Reduced Gradient Method

Figure 1. Comparison of Convergence of Group Assignments.
Distribution plot of class to group assignments of SplitNet (a)
with softmax reparametrization and (b) reduced gradient method.
Note that the x-axis scale of two graphs is different.

2. Group Weight Regularization

Here we attach the full version of the Figure 2 in the pa-
per(Figure 2), illustrating each term of the group weight
regularization. As illustrated in the right of the ﬁgure, two
terms of the regularization RW (W , P , Q) are (cid:96)2,1-norm
of inter group connections in row and column direction re-
spectively.

3. Deep Split in Residual Networks

Our method can be further extended to residual net-
works (He et al., 2016) by sharing group assignments over
the nodes connected with shortcut connections. Consider
a residual building block where a shortcut connection by-
passes two convolutional layers. Let W (l1), W (l2) denote
g , ql2
the weights of the convolutional layer and pl1
g
denote corresponding group assignment vectors for each
layer with q(l1)
. Since the shortcut identity map-
ping connects the input nodes of the ﬁrst convolutional
layer to the output nodes of the second convolutional layer,
the grouping of these nodes should be shared: p(l1)
g =
q(l2)
g

g = p(l2)

g , pl2

g , ql1

g

.

(cid:54)
Learning to Semantically Split Deep Networks for Improved Scalability and Parallelization

Figure 2. Group Assignment and Group Weight Regularization(Full Figure). (Left) An example of group assignment with G = 3.
Colors indicate groups. Each row of matrix P , Q is group assignment vectors for group g: pg, qg. (Center) Visualization of matrix
(I − Pg)W Qg. The group assignment vectors work as soft indicators for inter-group connections. As the groupings converge, (cid:96)2,1-
norm is concentrated on inter-group connections. (Right) Group weight regularization RW (W , P , Q) when group assignments are well
partitioned. Horizontal and vertical bars painted in each color indicate the parts of W that represent inter-group connections from/to the
corresponding group. These colored parts of W are penalized to be near to zero.

4. Experimental Details

4.1. Models Description

Models for CIFAR-100. We use the WRN-16-8(i.e. a
depth of 16 and a widening factor k = 8), as our base
network. This network consists of initial conv layer with
16 ﬁlters, followed by 6 residual blocks each including
3 conv layers and last fc layer. The number of ﬁlters
2
in residual blocks are 128, 256, 512, respectively. Refer to
(Zagoruyko & Komodakis, 2016) for details of the WRN
structure. We explore four split settings on the structure of
WRN-16-8: FC Split, Shallow Split, Deep Split, and Hier-
archical Split.

∼

We experiment with three split depth settings: 1, 6, 11.
The depth 6 and 11 roughly correspond to one-third and
two-thirds depth of the WRN-16-8. FC Split splits the last
softmax fc layer with setting the number of groups to G =
4. Shallow Split has the depth of 6, including 5 conv layers
and the fc layer with G = 2. Deep Split has the split depth
of 11, including 10 conv layers and the fc layer with G = 2.
We use G = 2 for Shallow and Deep Split, mainly because
using a large G may substantially cut down the network
capacity when recursively splitting multiple layers. Finally,
Hierarchical Split is a hierarchical version of Deep Split:
ﬁrst 5 layers are split into two supergroups and then the
network branches into 4 subgroups for later 6 layers. This
results in tree-structured layer networks.

We use weight decay 0.0005 in FC Split and Shallow Split
as in baseline. For Deep Split and Hierarchical Split we
observe that using the weight decay 0.0001 produces better
groupings. However, this causes the network to overﬁt and
we apply dropout to prevent overﬁtting. In addition, we ex-
pect that dropout will help the splitting process as dropout
prevents co-adaptation of neurons(Srivastava et al., 2014).
We report the test error of baseline and SplitNet variants
using the same setting. For data augmentation, we apply
random croping and horizontal ﬂipping. Batch normaliza-
tion is included in all networks. For the CIFAR-100 exper-

iments, we reparameterized group assignment variables in
the softmax form.

We hypothesize that the degradation of the deep split is due
to insufﬁcient capacity caused by splitting layers, and ex-
periment increasing the capacity of the splitted layers by
allowing overlap of the nodes in each layers. Speciﬁcally,
we allow some nodes to be duplicated and assigned to mul-
tiple groups when the network is splitted. Here we regard
group assignment for a node as fuzzy if there are multiple
groups that the node is assigned to, with values greather
than some threshold, say 0.1. During training, we freeze
group assignment matrix of a layer if only less than 20% of
nodes in the layer remains fuzzy. Then, when splitting the
network, these fuzzy nodes are duplicated and distributed
to corresponding groups. The resulting SplitNet with over-
lap has a larger capacity than a standard SplitNet.

The results of the overlap experiments for three SplitNets
on WRN-16-8 are shown in the table 1(marked as overlap).
In the case of all three splits, the error rate is reduced while
increasing the parameter and FLOPs. Note that although
the parameter reductions are smaller, the networks are still
parallelizable easily.

For comparison with other works in the direction of param-
eter reduction, we conduct an experiment on WRN-16-8
using SSL(Wen et al., 2016) which uses the (2,1)-norm to
induce structured sparsity. Our Shallow Split (23.96%) out-
performs SSL(1e-3) (24.15%), while signiﬁcantly reducing
the number of parameters (32.54%) as opposed to 9.40% of
SSL.

Models for ImageNet-1K. We test two baseline networks,
AlexNet(Krizhevsky et al., 2012) and ResNet-18x2, a vari-
ant of ResNet-18(He et al., 2016). First, for AlexNet, we
evaluate various split settings from conv4 to fc8 layer, and
ﬁnetune SplitNets from those grouping. The number of
splits in the result table indicates the split in fc6, fc7 and
fc8 layer, respectively. For instance, the 2-4-8 split means
that fc6 layer is split into 2 groups and fc7, fc8 layers are
split in 4, 8 groups, respectively. All supergroups at each

P2RG⇥DQ2RG⇥KW2RD⇥KKDXgXi||((I Pg)WQg)i⇤||2+XgXi||((I Pg)WQg)⇤j||2pgqgW||((I Pg)WQg)i⇤||2iDKXgXi||((I Pg)WQg)i⇤||2+XgXi||((I Pg)WQg)⇤j||2Learning to Semantically Split Deep Networks for Improved Scalability and Parallelization

Table 1. Comparison of Parameter/Computation Reduction and Test Errors on CIFAR-100.

NETWORK
WRN-16-8
FC SPLIT
SHALLOW SPLIT
DEEP SPLIT (DROPOUT)
HIER. SPLIT (DROPOUT)
FC SPLIT (OVERLAP)
SHALLOW SPLIT (OVERLAP)
DEEP SPLIT (OVERLAP)
SSL(1E-3) (WEN ET AL., 2016)
SSL(3E-3) (WEN ET AL., 2016)
SSL(5E-3) (WEN ET AL., 2016)

PARAMS(106) % REDUCED
0.0
0.35
32.54
46.39
62.58
0.32
21.67
27.02
9.40
32.38
59.31

11.0
11.0
7.42
5.90
4.12
11.0
8.62
8.03
9.97
7.44
4.48

FLOPS(109) % REDUCED
0.0
0.0
14.63
31.97
39.29
0.0
9.77
19.52
12.64
33.14
48.71

3.10
3.10
2.64
2.11
1.88
3.10
2.79
2.49
2.70
2.07
1.60

TEST ERROR(%)
24.28
24.26
23.96
24.66
24.80
24.04
23.74
24.10
24.15
24.49
24.65

∼

3 subgroups at the consecutive layer, so
layer include 2
that the numbers of branches in the hierarchical tree are as
even as possible. For deeper split, we split 5 layers from
conv4 to the last fc8 with G = 2 in 2

5 split experiment.

×

ResNet-18x2 is a over-parameterized network that doubles
the number of convolutional ﬁlters of ResNet-18. It con-
sists of the initial conv layer with 128 ﬁlters, 8 residual
blocks, each including 2
1024
channels, and the last fc layer. We optimize the grouping
in various levels of splits, from conv4-1 layer at the deepest
split, and ﬁnetune the SplitNet. The number of splits in the
result table indicates the split in conv4-1&2, conv5-1&2,
and the last fc layer, each of which has 5, 5, and 1 split
depth, respectively.

3 conv layers with 128

∼

∼

We also test SplitNet-Random, where we split fc layers
randomly, and remove inter-group connections while keep-
ing the parameters of remaining intra-group connections.
We then ﬁnetune the SplitNet-Random models for 10
20
epochs. For these two ImageNet-1K experiments, we di-
rectly optimized group assignment variables, pgi and qgi,
using reduced gradient algorithm (Rakotomamonjy et al.,
2008).

∼

5. Effect of Number of Groups

We experiment how setting the number of groups G affects
Shallow SplitNet on CIFAR-100, and report the results in
Table 2. Setting G = 2 produces the lowest test error. In-
terestingly, the test error is lower with G = 4 than G = 3,
even with less parameters. Moreover, Shallow SplitNet us-
ing G = 4 still outperforms the baseline with only 51.36%
of parameters. This result strongly supports the motivation
of this research, in that often it is important to ﬁnd the opti-
mal split structure for a given network, in terms of both test
accuracy and memory/computation.

6. Training Time Parallelization of SplitNet

Training SplitNet mainly consists of two stages: 1) learning
groupings and block-diagonal weight matrices, 2) Splitting

Table 2. Effect of G on Shallow SplitNet on CIFAR-100

MODEL

BASELINE
SHALLOW SPLIT
SHALLOW SPLIT
SHALLOW SPLIT

G

2
3
4

PARAMS
REDUCED (%)
0.0
32.44
43.09
48.66

TEST ERROR (%)

24.28
23.94
25.14
24.10

the network and ﬁnetuning. The second stage can be eas-
ily model-parallelized as in test time parallelization. For
shared lower layers, gradients computed at each processor
may be averaged for weight update. Training time paral-
lelization of the ﬁrst stage is also implementable, although
the networks are not as clean-cut as at test time. We can
simply start with an arbitrary assignment of the weight sub-
matrices in the early stage, and as the network is trained
we can rearrange the network weights to be disjoint across
multiple nodes. However, training-time parallelization is
not yet implemented, which requires further research.

References

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep Residual Learning for Image Recognition. In
CVPR, 2016.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
ImageNet Classiﬁcation with Deep Convolutional Neu-
ral Networks. In NIPS, 2012.

Rakotomamonjy, Alain, Bach, Francis R, Canu, St´ephane,
and Grandvalet, Yves. SimpleMKL. JMLR, 9:2491–
2521, 2008.

Srivastava, Nitish, Hinton, Geoffrey E, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:
a Simple Way to Prevent Neural Networks from Over-
ﬁtting. Journal of Machine Learning Research, 15(1):
1929–1958, 2014.

Wen, Wei, Wu, Chunpeng, Wang, Yandan, Chen, Yiran,
and Li, Hai. Learning Structured Sparsity in Deep Neu-
ral Networks. In NIPS. 2016.

Learning to Semantically Split Deep Networks for Improved Scalability and Parallelization

Zagoruyko, Sergey and Komodakis, Nikos. Wide Residual

Networks. In BMVC, 2016.

