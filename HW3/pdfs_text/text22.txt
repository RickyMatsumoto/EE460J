Near-OptimalDesignofExperimentsviaRegretMinimizationZeyuanAllen-Zhu*1YuanzhiLi*2AartiSingh*3YiningWang*3AbstractWeconsidercomputationallytractablemethodsfortheexperimentaldesignproblem,wherekoutofndesignpointsofdimensionpareselectedsothatcertainoptimalitycriteriaareapproxi-matelysatisﬁed.Ouralgorithmﬁndsa(1+ε)-approximateoptimaldesignwhenkisalinearfunctionofp;incontrast,existingresultsrequirektobesuper-linearinp.Ouralgorithmalsohan-dlesallpopularoptimalitycriteria,whileexistingonesonlyhandleoneortwosuchcriteria.Nu-mericalresultsonsyntheticandreal-worldde-signproblemsverifythepracticaleffectivenessoftheproposedalgorithm.1.IntroductionExperimentaldesignisanimportantprobleminstatisticsandmachinelearningresearch(Pukelsheim,2006).Con-sideralinearregressionmodely=Xβ0+w,(1)whereX∈Rn×pisapoolofndesignpoints,yisthere-sponsevector,β0isap-dimensionalunknownregressionmodelandwisavectorofi.i.d.noisevariablessatisfyingEwi=0andEw2i<∞.Theexperimentaldesignprob-lemistoselectasmallsubsetofrows(i.e.,designpoints)XSfromthedesignpoolXsothatthestatisticalpowerofestimatingβ0ismaximizedfromnoisyresponseySontheselecteddesignsXS.Asanexample,consideramaterialsynthesisapplicationwherepisthenumberofvariables(e.g.,temperature,pres-sure,duration)thatarehypothesizedtoaffectthequalityofthesynthesizedmaterialandnisthetotalnumberofcom-binationsofdifferentparametersofexperimentalcondi-tions.Asexperimentsareexpensiveandtime-consuming,*Authornameslistedinalphabeticorder.1MicrosoftResearch,Redmond,USA2PrincetonUniversity,Princeton,USA3CarnegieMellonUniversity,Pittsburgh,USA.Correspondenceto:YiningWang<yiningwa@cs.cmu.edu>.Proceedingsofthe34thInternationalConferenceonMachineLearning,Sydney,Australia,PMLR70,2017.Copyright2017bytheauthor(s).onewishestoselectk(cid:28)nexperimentalsettingsfromXthatarethemoststatisticallyefﬁcientforestablishingamodelthatconnectsexperimentalparameterswithsynthe-sizedmaterialquality,y.Theexperimentaldesignproblemisalsorelatedtomanymachinelearningtasks,suchaslin-earbandits(Deshpande&Montanari,2012;Huangetal.,2016),diversitysampling(Kulesza&Taskar,2012)andactivelearning(Maetal.,2013;Chaudhurietal.,2015;Hazan&Karnin,2015;Balcan&Long,2013;Wang&Singh,2016).Sincestatisticalefﬁciencycanbemeasuredinvariousways,thereexistanumberofoptimalitycriteriatoguidetheselectionofexperiments.WereviewsomeoptimalitycriteriainSec.2andinterestedreadersarereferredtoSec.6of(Pukelsheim,2006)foracomprehensivereview.Typically,anoptimalitycriterionisafunctionf:S+p→Rthatmapsfromthep-dimensionalpositivedeﬁniteconetoarealnumber.Theexperimentaldesignproblemcanthenbeformulatedasacombinatorialoptimizationproblem:S∗(k)=argminS∈S(n,k)f(X>SXS),(2)whereSiseitherasetoramulti-setofsizek,andXS∈Rk×pisformedbystackingtherowsofXthatareinS.TheconstraintsetS1/2(n,k)isdeﬁnedasfollows:1.Withreplacement:S1(n,k)={Smulti-set:S⊆[n],|S|≤k}.Underthissetting,XSmaycontaindu-plicaterowsofthedesignpoolX;2.Withoutreplacement:S2(n,k)={Sstandardset:S⊆[n],|S|≤k}.Underthissetting,XSonlycon-tainsdistinctrowsofthedesignpoolX.The“withreplacement”settingisclassicalinstatisticslit-erature,wherethemultiplemeasurementsinywithrespecttothesamedesignpointleadtodifferentvalueswithstatis-ticallyindependentnoise.The“withoutreplacement”set-ting,ontheotherhand,ismorerelevantinmachinelearn-ingapplications,becauselabelsarenotlikelytochangeifthesamedatapoint(e.g.,thesameimage)isconsideredtwice.Finally,itisworthpointingoutthatthe“withre-placement”settingiseasier,becauseitcanbereduced(inpolynomialtime)tothe“withoutreplacement”settingbyreplicatingeachrowofXforktimes.Formanypopularchoicesoff,theexactoptimizationNear-optimaldesignofexperimentsviaregretminimizationTable1.Comparisonwithexistingresultsoncomputationallyefﬁcientexperimentaldesign.AnalgorithmproducesasubsetˆSofsizek,andtheapproximationratioisdeﬁnedasf(X>ˆSXˆS)/minS∈Sb(n,r)f(X>SXS)forr≤k.TOPTdenotesthetimecomplexityforsolvingthecontinuousconvexoptimizationprobleminEq.(6).Withoutreplacement?Deterministic?TimecomplexityConstraints†CriteriaApprox.ratioPipageroundingYesYesTOPT+O(n2p2)k=r≥pD,T(1−1/e)−1(Bouhtouetal.,2010)YesNoTOPT+O(n)k=r≥pD(n/k)1/p(Avron&Boutsidis,2013)YesYesO(n2p2)k=r≥pAn−p+1k−p+1(Wangetal.,2016)NoNoTOPT+O(nk)k=Ω(r),andr=Ω((plogp)/ε2)A,V1+ε(Wangetal.,2016)YesYesTOPT+O(p6)k=r=Ω(p2/ε)A,V1+εThispaper—Eq.(3)NoYesTOPT+˜O(nkp2)k=r=Ω(p/ε2)A,D,T,E,V,G1+εThispaper—Eq.(4)YesYesTOPT+˜O(nkp2)k=r>2pA,D,T,E,V,GO(1)Thispaper—Eq.(5)YesYesTOPT+˜O(nkp2)k=Ω(r),r≥p/ε2A,D,T,E,V,G1+ε†In˜O(·)and˜Ω(·)wehidelogarithmicdependencyovern,pandk.probleminEq.(2)isNP-hard(C¸ivril&Magdon-Ismail,2009;ˇCern`y&Hlad´ık,2012).Inthispaper,weproposeacomputationallytractablealgorithmthatapproximatelycomputesEq.(2)forawiderangeofoptimalitycriteria,andunderveryweakconditionsonn,kandp.Belowisourmaintheorem:Theorem1.1.Supposeb∈{1,2},n>k>pandletf:S+p→Rbearegularoptimalitycriterion(cf.Deﬁnition2.1).Thereexistsapolynomial-timealgorithmthatoutputsˆS∈Sb(n,k)foranyinputmatrixX∈Rn×pwithfullcolumnrank,andˆSsatisﬁesthefollowing:1.Forb=1(withreplacement),thereexistsanabsoluteconstantC0≤32suchthat,foranyε∈(0,1),ifk≥C0p/ε2thenf(X>ˆSXˆS)≤(1+ε)·minS∈S1(n,k)f(X>SXS).(3)2.Forb=2(withoutreplacement)andanyξ>2,thereexistsconstantC1(ξ)>0dependingonlyonξsuchthat,ifk≥ξpthenf(X>ˆSXˆS)≤C1(ξ)·minS∈S2(n,k)f(X>SXS).(4)Moreover,forξ≥4wehaveC1(ξ)≤32.3.Forb=2(withoutreplacement)andanyε∈(0,1/2),ifk,rsatisfyk≥4(1+7ε)randr≥p/ε2,thenf(X>ˆSXˆS)≤(1+ε)·minS∈S2(n,r)f(X>SXS).(5)WeinterpretthesigniﬁcanceofTheorem1.1asfollows.•Underaverymildconditionofk>2p,ourpolynomial-timealgorithmﬁndsasetˆS⊂[n]ofsizek,withob-jectivevaluef(X>ˆSXˆS)beingatmostO(1)aconstanttimestheoptimum.SeeEq.(4).•Ifreplacement(b=1)orover-sampling(k>r)isallowed,theapproximationratiocanbetightenedto1+εforarbitrarilysmallε>0.SeeEq.(3)and(5).•Inallofthethreecases,weonlyrequirektogrowlin-earlyinp.Recallthatk≥pisnecessarytoensurethesingularityofX>ˆSXˆS.Incontrast,nopolynomial-timealgorithmhasachievedO(1)approximationintheregimek=O(p)fornon-submodularoptimalitycri-teria(e.g.,A-andV-optimality)underthewithoutre-placementsetting.•Ouralgorithmworksforanyregularoptimalitycri-terion.Tothebestofourknowledge,noknownpolynomial-timealgorithmcanachievea(1+ε)ap-proximationfortheD-andT-optimalitycriteria,orevenanO(1)approximationfortheE-andG-optimalitycri-teria.SeeTable1foracomparison.ThekeyideabehindourproofofTheorem1.1isare-gretminimizationcharacterizationoftheleasteigenvalueofpositivesemideﬁnite(PSD)matrices.Similarideasweredevelopedin(Allen-Zhuetal.,2015;Silvaetal.,2016)toconstructefﬁcientalgorithmsforlinear-sizedgraphsparsi-ﬁers.Inthispaperweadopttheregretminimizationframe-workandpresentnovelpotentialfunctionanalysisforthespeciﬁcapplicationofexperimentaldesign.1.1.NotationsS+pisthepositivedeﬁniteconeofp×pmatrices:ap×psymmetricmatrixAbelongstoS+pifandonlyifv>Av>0forallv∈Rp\{0}.Forsymmetricmatri-cesAandB,wewriteA(cid:22)Bifv>(A−B)v≥0forallv∈Rp.TheinnerproducthA,BiisdeﬁnedashA,Bi=tr(B>A)=Ppi,j=1AijBij.WeusekAk2=supv∈Rp\{0}kAvk2/kvk2todenotethespectralnorm,andkAkF=qPpi,j=1A2ij=phA,AitodenoteNear-optimaldesignofexperimentsviaregretminimizationtheFrobeniusnormofA.ForA(cid:23)0,wewriteB=A1/2astheuniqueB(cid:23)0thatsatisﬁesB2=A.ForadesignpoolX∈Rn×p,weusexi∈Rptodenotethei-throwofX.Weuseσmin(A)fortheleast(smallest)singularvalueofaPSDmatrixA.1.2.RelatedworkExperimentaldesignisanoldtopicinstatisticsresearch(Pukelsheim,2006;Fedorov,1972).Computationallyefﬁ-cientexperimentaldesignalgorithms(withprovableguar-antees)are,however,alessstudiedﬁeld.Inthecaseofsub-modularoptimalitycriteria(e.g.,D-andT-optimality),theclassicalpipageroundingmethod(Ageev&Sviridenko,2004;Horeletal.,2014;Ravietal.,2016)combinedwithsemi-deﬁniteprogrammingresultsincomputationallyef-ﬁcientalgorithmsthatenjoyaconstantapproximationra-tio.Bouhtouetal.(2010)improvestheapproximationra-tiowhenkisverycloseton.Deshpande&Rademacher(2010);Lietal.(2017)consideredpolynomial-timealgo-rithmsforsamplingfromaD-optimalitycriterion.Thesealgorithmsarenotapplicabletonon-submodularcriteria,suchasA-,V-,E-orG-optimality.FortheparticularA-optimalitycriterion,(Avron&Bout-sidis,2013)proposedagreedyalgorithmwithanapprox-imationratioofO(n/k)withrespecttof(X>X).Itwasshownthatintheworstcasemin|S|≤kf(X>SXS)≈O(n/k)·f(X>X)andhencetheboundistight.How-ever,forgeneraldesignpoolmin|S|≤kf(X>SXS)couldbefarsmallerthanO(n/k)·f(X>X),makingthetheoreti-calresultspowerlessinsuchscenarios.Wangetal.(2016)consideredavariantofthegreedymethodandshowedanapproximationratioquadraticindesigndimensionpandindependentofpoolsizen.Wangetal.(2016)derivedalgorithmsbasedoneffectivere-sistancesampling(Spielman&Srivastava,2011)thatattain(1+ε)approximationratioifk=Ω(plogp/ε2)andrep-etitionsofdesignpointsareallowed.Thealgorithmfunda-mentallyreliesonthecapabilityof“re-weighting”(repeat-ing)designpointsandcannotbeadaptedtothemoregen-eral“withoutreplacement”setting.Naivesamplingbasedmethodswereconsideredin(Wangetal.,2016;Chaudhurietal.,2015;Dhillonetal.,2013),whichalsoachieve(1+ε)approximationbutrequiresthesubsetsizektobemuchlargerthantheconditionnumberofX.Arelatedhoweverdifferenttopicislow-rankmatrixcol-umnsubsetselectionandCURapproximation,whichseekscolumnsubsetCandrowsubsetRsuchthatkX−CC†XkFand/orkX−CURkFareminimized(Drineasetal.,2008;Boutsidis&Woodruff,2014;Wang&Singh,2015b;Drineas&Mahoney,2005;Wang&Zhang,2013;Wang&Singh,2015a).Theseproblemsareunsupervisedinnatureanddonotingeneralcorrespondtostatisticalpropertiesundersupervisedregressionsettings.Pilanci&Wainwright(2016);Raskutti&Mahoney(2014);Woodruff(2014)consideredfastmethodsforsolvingordinaryleastsquares(OLS)problems.Theyarecomputationallyori-entedandtypicallyrequireknowledgeofthefullresponsevectory,whichisdifferentfromtheexperimentaldesignproblem.2.RegularcriteriaandcontinuousrelaxationWestartwiththedeﬁnitionofregularoptimalitycriteria:Deﬁnition2.1(Regularcriteria).Anoptimalitycriterionf:S+p→Risregularifitsatisﬁesthefollowigproperties:1.Convexity:1f(λA+(1−λ)B)≤λf(A)+(1−λ)f(B)forallλ∈[0,1]andA,B∈S+p;2.Monotonicity:IfA(cid:22)Bthenf(A)≥f(B);3.Reciprocalmultiplicity:f(tA)=t−1f(A)forallt>0andA∈S+p.Almostalloptimalitycriteriausedintheexperimentalde-signliteratureareregular.Belowwelistafewpopu-larexamples;theirstatisticalimplicationscanbefoundin(Pukelsheim,2006):-A-optimality(Average):fA(Σ)=1ptr(Σ−1);-D-optimality(Determinant):fD(Σ)=(det|Σ|)−1p;-T-optimality(Trace):fT(Σ)=p/tr(Σ);-E-optimality(Eigenvalue):fE(Σ)=kΣ−1k2;-V-optimality(Variance):fV(Σ)=1ntr(XΣ−1X>);-G-optimality:fG(Σ)=maxdiag(XΣ−1X>).The(A-,D-,T-,E-)criteriaconcernestimatesofregres-sioncoefﬁcientsandthe(V-,G-)criteriaareaboutin-samplepredictions.Allcriterialistedaboveareregular.NotethatforD-optimalitytheproxyfunctiongD(Σ)=−logdet(Σ)isconsideredtosatisfytheconvexityprop-erty.Inaddition,bythestandardarithmeticinequalitywehavethatfT≤fD≤fA≤fEandthatfV≤fG.AlthoughexactoptimizationofthecombinatorialproblemEq.(2)isintractable,itisneverthelesseasytosolveacon-tinuousrelaxationofEq.(2)giventheconvexitypropertyinDeﬁnition2.1.Weconsiderthefollowingcontinuousoptimizationproblem:π∗(b)=argminπ=(π1,···,πn)f nXi=1πixix>i!,(6)s.t.π≥0,kπk1≤r,I[b=2]·kπk∞≤1.1Thispropertycouldberelaxedtoallowaproxyfunctiong:S+p→Rbeingconvex,whereg(A)≤g(B)⇔f(A)≤f(B).Near-optimaldesignofexperimentsviaregretminimizationThekπk1≤rconstraintmakessureonlyrrowsofXare“selected”,wherer≤kisaparameterthatcontrolsthedegreeofoversampling.The0≤πi≤1constrainten-forcesthateachrowofXis“selected”atmostonceandisonlyapplicabletothewithoutreplacementsetting(b=2).Eq.(6)isarelaxationoftheoriginalcombinatorialprob-lemEq.(2),whichweformalizebelow:Fact2.1.Forb∈{1,2}wehavef(Pni=1π∗i(b)xix>i)≤minS∈Sb(n,r)f(X>SXS)Inaddition,becauseofthemonotonicitypropertyoffthesumconstraintmustbind:Fact2.2.Forb∈{1,2}itholdsthatPni=1π∗i(b)=r.ProofsofFacts2.1and2.2arestraightforwardandareplacedinthesupplementarymaterial.BoththeobjectivefunctionandtheconstraintsetinEq.(6)areconvex,andhenceitcanbeefﬁcientlysolvedtoglobaloptimalitybyconventionalconvexoptimizational-gorithms.Inparticular,fordifferentiablefwesuggestthefollowingprojectedgradientdescent(PGD)procedure:π(t+1)=PC(cid:16)π(t)−γt∇f(π(t))(cid:17),(7)wherePC(x)=argminy∈Ckx−yk2istheprojectionoperatorontothefeasiblesetC={π∈Rp:π≥0,kπk1≤r,I[b=2]·kπk∞≤1}and{γt}t≥1>0isasequenceofstepsizestypicallychosenbybacktrackinglinesearch.Whenfisnotdifferentiableeverywhere,pro-jectedsubgradientdescentcouldbeusedwitheithercon-stantordiminishingstepsizes.Wedeferdetailedgradientcomputationstothesupplementarymaterial.Itwasshownin(Wangetal.,2016;Suetal.,2012)thattheprojectionop-eratorPC(x)couldbeefﬁcientlycomputeduptoprecisionδinO(nlog(kxk∞/δ))operations.3.SparsiﬁcationviaregretminimizationTheoptimalsolutionπ∗ofEq.(6)doesnotnaturallyleadtoavalidapproximationofthecombinatorialprobleminEq.(2),becausethenumberofnon-zerocomponentsinπ∗mayfarexceedk.Theprimaryfocusofthissectionistodesignefﬁcientalgorithmsthatsparsifytheoptimalsolu-tionπ∗intos∈[k]n(withreplacement)ors∈{0,1}n(withoutreplacement),whileatthesametimeboundingtheincreaseintheobjective.Duetothemonotonicityandreciprocalmultiplicityprop-ertiesoff,itsufﬁcestoﬁndasparsiﬁersthatsatisﬁes nXi=1sixix>i!(cid:23)τ· nXi=1π∗ixix>i!(8)forsomeconstantτ∈(0,1).ByDeﬁnition2.1,Eq.(8)immediatelyimpliesf(Pni=1sixix>i)≤τ−1f(Pni=1π∗ixix>i).Thekeyideabehindouralgorithmisaregret-minimizationinterpretationoftheleasteigen-valueofapositivedeﬁnitematrix,whicharisesfromrecentprogressinthespectralgraphsparsiﬁcationliterature(Silvaetal.,2016;Allen-Zhuetal.,2015).Intherestofthissection,weadoptthenotationthatΠ=diag(π∗)andS=diag(s),bothbeingn×nnon-negativediagonalmatrices.WealsouseItodenotetheidentityma-trix,whosedimensionshouldbeclearfromthecontext.3.1.ThewhiteningtrickConsiderthelineartransformxi7→(XΠX>)−1/2xi=:˜xi.ItiseasytoverifythatPni=1π∗i˜xi˜x>i=I.Suchatransformisusuallyreferredtoaswhitening,becausethesamplecovarianceofthetransformeddataistheidentitymatrix.DeﬁneW=Pni=1si˜xi˜x>i.Wethenhavethefollowing:Proposition3.1.Forτ>0,W(cid:23)τIifandonlyif(Pni=1sixix>i)(cid:23)τ(Pni=1π∗ixix>i).Proof.ThepropositionholdsbecauseW(cid:23)τIifandonlyif(XΠX>)1/2W(XΠX>)1/2(cid:23)τXΠX>,andthat(XΠX>)1/2W(XΠX>)1/2=XSX>.Proposition3.1showsthat,withoutlossofgenerality,wemayassumePni=1π∗ixix>i=XΠX>=I.ThequestionofprovingW=XSX>(cid:23)τIisthenreducedtolowerboundingthesmallesteigenvalueofW.RecallthatWcanbewrittenasasumofrank-1PSDma-tricesW=Pkt=1Ft,whereFt=xix>iforsomei∈[n].InthenextsectionwegiveanovelcharacterizationoftheleasteigenvalueofWfromaregretminimizationperspec-tive.TheproblemoflowerboundingtheleasteigenvalueofWcanthenbereducedtoboundingtheregretofapar-ticularFollow-The-Regularized-Leader(FTRL)algorithm,whichisamucheasiertaskasFTRLadmitsclosed-formsolutions.3.2.SmallesteigenvalueasregretminimizationWeﬁrstreviewtheconceptofregretminimizationinaclas-sicallinearbanditsetting.Let∆p={A∈Rp×p:A(cid:23)0,tr(A)=1}beanactionspacethatconsistsofposi-tivesemi-deﬁnitematricesofdimensionpandunittracenorm.Considerthelinearbanditproblem,whichoperatesinkiterations.Atiterationt,theplayerchoosesanactionAt∈∆p;afterwards,a“reference”actionFt(cid:23)0isob-servedandthelosshFt,Atiisincurred.Theobjectiveoftheplayeristominimizehis/herregret:R({At}kt=1):=kXt=1hFt,Ati−infU∈∆pkXt=1hFt,Ui,Near-optimaldesignofexperimentsviaregretminimizationwhichisthe“excessloss”of{At}kt=1comparedtothesingleoptimalactionU∈∆pinhindsight,knowingallthereferenceactions{Ft}kt=1.ApopularalgorithmforregretminimizationisFollow-The-Regularized-Leader(FTRL),alsoknowntobeequivalenttoMirrorDescent(MD)(McMahan,2011),whichsolvesforAt=argminA∈∆p(w(A)+α·t−1X‘=1hF‘,Ai).(9)Herew(A)isaregularizationtermandα>0isaparam-eterthatbalancesmodelﬁttingandregularization.Fortheproofofourpurposeweadoptthe‘1/2-regularizerw(A)=−2tr(A1/2)introducedin(Allen-Zhuetal.,2015),whichleadstotheclosed-formsolutionAt= ctI+αt−1X‘=1F‘!−2,(10)wherect∈RistheuniqueconstantthatensuresAt∈∆p.Thefollowinglemmafrom(Allen-Zhuetal.,2015)boundstheregretofFTRLusingtheparticular‘1/2-regularizer:Lemma3.1(Theorem3.2of(Allen-Zhuetal.,2015),specializedto‘1/2-regularization).Supposeα>0,rank(Ft)=1andlet{At}kt=1beFTRLsolutionsdeﬁnedinEq.(10).IfαhFt,A1/2ti>−1forallt,thenR({At}kt=1):=kXt=1hFt,Ati−infU∈∆pkXt=1hFt,Ui≤αkXt=1hFt,AtihFt,A1/2ti1+αhFt,A1/2ti+2√pα.NowconsidereachFt=xitx>ittobetheouterproductofadesignpointselectedfromthedesignpoolX.Oneremark-ableconsequenceofLemma3.1isthat,inordertolowerboundthesmallesteigenvalueofPkt=1Ft,whichbydeﬁ-nitionisinfU∈∆phPkt=1Ft,Ui,itsufﬁcestolowerboundPkt=1hFt,Ati.BecauseAtadmitsclosed-formexpres-sioninEq.(10),choosingasequenceof{Ft}kt=1withlargePkt=1hFt,Atibecomesamuchmoremanageableanalyt-icaltask,whichweshallformalizeinthenextsection.3.3.ProofofTheorem1.1Re-organizingtermsinLemma3.1weobtaininfU∈∆pkXt=1hFt,Ui≥kXt=1hFt,Ati1+αhFt,A1/2ti−2√pα.(11)Theknear-optimaldesignpointsareselectedinasequen-tialmanner.LetΛt∈Sb(n,t)bethesetofselectedde-signpointsatorpriortoiterationt(Λ0=∅),anddeﬁneFt=xitx>it,whereitisthedesignpointselectedatitera-tiont.DeﬁnealsoΛt=Pt‘=1F‘=Pi∈Λtxix>i.Weﬁrstconsiderthewithreplacementsettingb=1.Lemma3.2.SupposePni=1π∗ixix>i=Iwhereπ∗i≥0andPni=1π∗i=r.Thenfor1≤t≤kwehavethatmaxi∈[n]hxix>i,Ati1+αhxix>i,A1/2ti≥1r+α√p.Proof.Recallthattr(At)=1andPni=1π∗ixix>i=I.Subsequently,Pni=1π∗ihxix>i,Ati=1.Ontheotherhand,wehavethatPni=1π∗i(1+αhxix>i,A1/2ti)=Pni=1π∗i+α·tr(A1/2t)(a)≤r+α·tr(A1/2t)(b)≤r+α√p.Here(a)isduetotheoptimizationconstraintthatkπ∗k1≤r,and(b)isbecausetr(A1/2t)=kσ(A1/2t)k1≤√pkσ(A1/2t)k2=√ppkσ(At)k1=√pptr(At)=√p,whereσ(·)isthevectorofalleigenvaluesofaPSDmatrix.Combiningbothinequalitieswehavethatmaxi∈[n]hxix>i,Ati1+αhxix>i,A1/2ti≥Pni=1π∗ihxix>i,AtiPni=1π∗i(1+αhxix>i,A1/2ti),wheretheright-handsideislowerboundedby1/(r+α√p).Letit=argmaxi∈[n]hxix>i,Ati1+αhxix>i,A1/2tibethedesignpointselectedatiterationt.CombiningEq.(11)andLemma3.2,Λk=Xi∈Λkxix>i(cid:23)(cid:18)kr+α√p−2√pα(cid:19)I.(12)ToproveEq.(3),setα=8√p/ε.Becausek=r≥C0p/ε2,wehavethatkr+α√p−2√pα≥11+8ε/C0−ε4.WithC0=32theright-handsideislowerboundedby1−ε/2.Eq.(3)isthusprovedbecause(1−ε/2)−1≤1+ε.Wenextconsiderthewithoutreplacementsettingb=2.Lemma3.3.Fixarbitraryβ∈(0,1]andsupposePni=1π∗ixix>i=Iwhereπ∗i∈[0,β]andPni=1π∗i=r.Thenforall1≤t≤k,maxi/∈Λt−1hxix>i,Ati1+αhxix>i,A1/2ti≥1−βσmin(Λt−1)−√p/αr+α√p.Proof.Ononehand,wehavePi/∈Λt−1π∗ihxix>i,Ati(a)≥hAt,I−βΛt−1i(b)=1−trh(αΛt−1+ctI)−2βΛt−1i=1+βctα−βαtrh(αΛt−1+ctI)−1i=1+βctα−tr(A1/2t)α(c)≥1+βctα−√pα.Here(a)isduetoPni=1π∗ixix>i=Iandπ∗i∈[0,β];(b)isduetohAt,Ii=tr(At)=1and(c)isprovedintheproofofLemma3.2.BecauseαΛt−1+ctI(cid:31)0,weconcludethatct≥−ασmin(Λt−1)andthereforePi/∈Λt−1π∗ihxix>i,Ati≥1−βσmin(Λt−1)−√p/α.Ontheotherhand,Pi/∈Λt−1π∗i(1+αhxix>i,A1/2ti)≤Near-optimaldesignofexperimentsviaregretminimizationr+α√pbythesameargumentasintheproofofLemma3.2.Subsequently,maxi/∈Λt−1hxix>i,Ati1+αhxix>i,A1/2ti≥Pi/∈Λt−1π∗ihxix>i,AtiPi/∈Λt−1π∗i(1+αhxix>i,A1/2ti)≥1−βσmin(Λt−1)−√p/αr+α√p.Letit=argmaxi/∈Λt−1hxix>i,Ati1+αhxix>i,A1/2ti.CombiningEq.(11)andLemma3.3withβ=1,wehavethatΛk(cid:23) kXt=11−κt−√p/αr+α√p−2√pα!I,(13)whereκt:=σmin(Λt).WearenowreadytoproveEqs.(4,5)inTheorem1.1.ProofofEq.(4).NotethatΛk(cid:23)supu>0min(cid:26)u,1−u−√p/αr+α√p·k−2√pα(cid:27)I.(14)Eq.(14)canbeprovedbyacaseanalysis:ifu≤κtforsome1≤t≤kthenσmin(Λk)≥σmin(Λt−1)≥u;otherwise1−κt−√p/α≥1−u−√p/αforall1≤t≤k.Supposek=r≥ξpforsomeξ>2.andletα=ν√p,u=(1−2/ξ)ν−3ν(2+ν/ξ),whereν>1issomeparametertobespeciﬁedlater.Eq.(14)thenyieldsΛk(cid:23)(1−2/ξ)ν−3ν(2+ν/ξ)I.Becauseξ>2,itispossibletoselectν>0suchthatC1(ξ)−1=(1−2/ξ)ν−3ν(2+ν/ξ)>0.Finally,forξ≥4andν=8wehaveC1(ξ)−1≥1/32.Eq.(4)isthusproved.ProofofEq.(5).Letβ∈(0,1)beaparametertobespec-iﬁedlater,anddeﬁneΣ∗β:=Pπ∗i≥βπ∗ixix>iand¯Σ∗β:=I−Σ∗β=Pπ∗i<βπ∗ixix>i.LetˆSbeconstructedsuchthatitincludesallpointsinS∗β:={i:π∗i≥β},plustheresultingsetbyrunningAlgorithm1ontheremainingweightssmallerthanβ,withsubsetsizek−k0=k−|S∗β|.Deﬁneα=2√p/ε,r0:=Pπ∗i≥βπ∗i,˜k:=k−k0and˜r:=r−r0+α√p=r−r0+2p/ε.LetΛ=Pi∈ˆSxix>ibethesamplecovarianceoftheselectedsubset.BythedeﬁnitionofˆSandLemma3.3,togetherwiththewhiten-ingtrick(Sec.3.1)on¯Σ∗β,wehaveΛ(cid:23)Σ∗β+supu>0minnu,(1−βu−ε/2)˜k/˜r−εo¯Σ∗β(cid:23)supu>0minnu,(1−βu−ε/2)˜k/˜r−εoI,wherethesecondlineholdsbecauseΣ∗β+¯Σ∗β=Iandu≤1.Nowsetβ=0.5andnotethatk0≤r0/β≤2r0bydeﬁnitionofS∗β.Subsequently,r≥p/ε2andk≥4(1+7ε)rforε∈(0,1/2)impliesthat˜k˜r≥1+2ε(1−ε/2)(1−β),whichyieldsu≥1−ε/2andhencef(X>ˆSXˆS)≤(1+ε)f(X>S∗XS∗).Eq.(5)isthusproved.Algorithm1Near-optimalexperimentaldesign1:Input:designpoolX∈Rn×p,budgetparametersk≥r≥p,algorithmicparameterα>0.2:SolvetheconvexoptimizationproblemEq.(6)withparameters;Letπ∗betheoptimalsolution;3:Whitening:X←X(X>diag(π∗)X)−1/2;4:Initialization:Λ0=∅;5:fort=1tokdo6:ct←FINDCONSTANT(Pi∈Λt−1xix>i,α);7:At←(ctI+Pi∈Λt−1xix>i)−2;8:Ifb=1thenΓt=[n];elseΓt=[n]\Λt−1;9:it←argmaxi∈Γthxix>i,Ati1+αhxix>i,A1/2ti;10:Λt=Λt−1∪{it};11:endfor12:Output:ˆS=Λk.Algorithm2FINDCONSTANT(Z,α)1:Initialization:c‘=−σmin(Z),cu=√p;(cid:15)=10−9;2:while|c‘−cu|>(cid:15)do3:¯c←(c‘+cu)/2;4:Iftr[(¯cI+Z)−2]>1thenc‘←¯c;elsecu←¯c;5:endwhile6:Output:c=(c‘+cu)/2.OurproofofTheorem1.1isconstructiveandyieldsacom-putationallyefﬁcientiterativealgorithmwhichﬁndssub-setˆS∈Sb(n,k)thatsatisﬁestheapproximationresultsinTheorem1.1.InAlgorithm1wegiveapseducodedescrip-tionofthealgorithm,whichmakesuseofabinarysearchroutine(Algorithm2)thatﬁndstheuniqueconstantctforwhichtr(At)=tr[(ctI+Pi∈Λt−1xix>i)−2]=1.NotethatforEq.(5)tobevalid,itisnecessarytorunAlgorithm2ontheremainingsetofπ∗afterincludingallpointsxiwithπ∗i≥1/2inˆS.4.ExtensiontogeneralizedlinearmodelsTheexperimentalalgorithmpresentedinthispapercouldbeeasilyextendedbeyondthelinearregressionmodel.ForthispurposeweconsidertheGeneralizedLinearModel(GLM),whichassumesthaty|xi.i.d.∼p(y|x>β0),wherep(·|·)isaknowndistributionandβ0isanunknownp-dimensionalregressionmodel.Examplesincludethelo-gisticregressionmodelp(y=1|x)=exp(x>β0)1+exp(x>β0),thePossioncountmodelp(yi=y|x)=exp(yx>β0−e−x>β0)y!,andmanyothers.LetS∈Sb(n,k)bethesetofselecteddesignpointsfromX.Undertheclassicalstatisticsregime,Near-optimaldesignofexperimentsviaregretminimizationTable2.Simulationresultsonsyntheticdataofsizen=1000andk=50.Uniformsamplingandweightedsamplingarerunfor50independenttrialsandthemedianobjectiveisreported.“Inf”meansthesamplecovarianceX>SXSdoesnotbelongtoS+p.k=2p=100k=3p=150fAfDfTfEfVfGfAfDfTfEfVfGUNIFORMSAMPLING34.297.252.05349.4101.4381.424.616.402.03196.273.7219.1WEIGHTEDSAMPLING23.424.57InfInf60.22202.611.184.260.96Inf46.20119.5FEDOROV’SEXCHANGE23.175.521.15172.944.43117.712.264.651.22173.773.97101.8(runningtime/secs)4.6262442284888893296282311360<111478ALGORITHM112.554.721.1953.5250.4790.7711.904.601.2741.5345.9780.94(runningtime/secs)<1<1<1<1<1<1<2<2<2<2<2<2k=5p=250k=10p=500UNIFORMSAMPLING20.025.822.00137.160.2155.217.575.512.02103.952.93123.5WEIGHTEDSAMPLING10.364.231.14Inf41.9190.6111.224.531.4452.7543.0480.74FEDOROV’SEXCHANGE11.705.841.38116.153.14133.6712.135.521.65108.445.0599.07(runningtime/secs)441<135225521961152100<1575<1151526804ALGORITHM111.144.671.3836.6745.676.2011.604.771.5649.2745.1481.78(runningtime/secs)<2<2<2<2<2<2<5<5<5<5<5<5themaximumlikelihood(ML)estimatorˆβML=argminβPi∈Slogp(yi|x>iβ)isasymptoticallyefﬁcient,anditsasymptoticvarianceequalstheFisher’sinformationI(XS;β0):=Xi∈SEy|x>iβ0(cid:20)−∂2logp(y|xi;β0)∂β∂β>(cid:21)ηi=x>iβ0=Xi∈SEy|ηi(cid:20)−∂2logp(y|ηi)∂η2i(cid:21)·xix>i.Herethesecondequalityisduetothesufﬁciencyofx>iβ0inaGLM.Notethatforthelinearregressionmodely=Xβ0+w,theMLestimatoristheordinaryleastsquares(OLS)ˆβ=(X>SXS)−1XSySanditsFisher’sinformationequalsthesamplecovarianceX>SXS.Theexperimentaldesignproblemcanthenbeformalizedasfollows:2minS∈Sb(n,k)f(I(XS;β0))=minS∈Sb(n,k)f Xi∈Sziz>i!;(15)zi=s−Ey|ηi(cid:20)−∂2logp(yi|ηi)∂η2i(cid:21),ηi=x>iβ0.Supposeˇβisa“pilot”estimateofβ0,obtainedfromauni-formlysampleddesignsubsetS1.Anear-optimaldesignsetS2canthenbeconstructedbyminimizingEq.(15)us-ingˇηi=x>iˇβ.Suchanapproachwasadoptedinsequen-tialdesignandactivelearningforMLestimators(Chaud-hurietal.,2015;Khurietal.,2006);however,withouralgorithmthequalityofS2isgreatlyimproved.2UnderverymildconditionsE[−∂2logp∂η2]=E[(∂logp∂η)2]isnon-negative(VanderVaart,2000).5.NumericalresultsWecomparetheproposedmethodwithseveralbaselinemethodsonbothsyntheticandreal-worlddatasets.Weonlyconsidertheharder“withoutreplacement”setting,whereeachrowofXcanbeselectedatmostonce.5.1.MethodsandtheirimplementationWecompareouralgorithmwiththreesimpleheuristicmethodsthatapplytoalloptimalitycriteria:1.Uniformsampling:ˆSissampleduniformlyatrandomwithoutreplacementfromthedesignpoolX;2.Weightedsampling:ﬁrsttheoptimalsolutionπ∗ofEq.(6)iscomputedwithr=k;afterwards,ˆSissam-pledwithoutreplacementaccordingtothedistributionspeciﬁedbyπ∗/k.Recallthat(Wangetal.,2016)provedthatweightedsamplingworkswhenkissuf-ﬁcientlylargecomparedtop(cf.Table1).33.Fedorov’sexchange(Miller&Nguyen,1994):theal-gorithmstartswitharandomsubsetS0∈Sb(n,k)anditerativelyexchangestwocoordinatesi∈S0,j/∈S0suchthattheobjectiveisminimizedaftertheexchange.Thealgorithmterminatesifnosuchexchangecanre-ducetheobjective,orTiterationsarereached.AllalgorithmsareimplementedinMATLAB,exceptfortheFedorov’sexchangealgorithm,whichisimplementedinCduetoefﬁciencyconcerns.WealsoapplytheSherman-Morrisonformula(A+λuu>)−1=A−1+λA−1uu>A−11+λu>A−1uandthematrixdeterminantlemmadet(A+λuu>)=3Fact2.2ensuresthatπ∗/kisavalidprobabilitydistribution.Near-optimaldesignofexperimentsviaregretminimizationTable3.ResultsontheMinnesotawindspeeddataset(n=2642,p=15,k=30).MSEisdeﬁnedasq1nky−Vˆβk22.fVMSEfGMSEUNIFORMSAMPLING94.11.1030931.34WEIGHTEDSAMPLING21.40.8924511.13FEDOROV’SEXCHANGE10.00.8629.20.78(runningtime/secs)15-1857-ALGORITHM110.80.7229.20.76(runningtime/secs)<1-<1-FULL-SAMPLEOLS-0.55-0.55(1+λu>A−1u>)det(A)toacceleratecomputationsofrank-1updatesofmatrixinverseanddeterminant.Foruni-formsamplingandweightedsampling,wereporttheme-dianobjectiveof50indpendenttrials.WeonlyreporttheobjectiveforonetrialofFedorov’sexchangemethodduetotimeconstraints.ThemaximumnumberofiterationsTforFedorov’sexchangeissetatT=100.Wealwayssetk=rintheoptimizationproblemEq.(6),anddetailsofsolvingEq.(6)areplacedintheappendix.InAlgorithm1wesetα=10;oursimiluationssuggestthatthealgorithmisnotsensitivetoα.5.2.SyntheticdataWesynthesizea1000×50designpoolXasfollows:X=(cid:20)XA0500×250500×25XB(cid:21).XAisa500×25randomGaussianmatrix,re-scaledsothattheeigenvaluesofX>AXAsatisfyaquadraticdecay:σj(X>AXA)∝j−2;XBisa500×25Gaussianmatrixwithi.i.d.standardNormalvariables.BothXAandXBhavecomparableFrobeniusnorm.InTable2wereportresultsonall6optimalitycriteria(fA,fD,fT,fE,fV,fG)fork∈{2p,3p,5p,10p}.Wealsoreporttherunningtime(measuredinseconds)ofAlgo-rithm1andtheFedorov’sexchangealgorithm.Theothertwosamplingbasedalgorithmsareveryefﬁcientandal-waysterminatewithinonesecond.Weobservethatoural-gorithmhasthebestperformanceforfEandfG,whilestillachievingcomparableresultsfortheotheroptimalitycrite-ria.Itisalsorobustwhenkissmallcomparedtop,whilesamplingbasedmethodsoccasionallyproducedesignsthatarenotevenfullrank.Finally,Algorithm1iscomputation-allyefﬁcientandterminateswithinsecondsforallsettings.5.3.TheMinnesotawindspeeddatasetTheMinnesotawinddatasetcollectswindspeedinforma-tionacrossn=2642locationsinMinnesota,USAforaperiodof24months(forthepurposeofthisexperiment,weonlyusewindspeeddataforonemonth).The2642loca-tionsareconnectedwith3304bi-directionalroads,whichformann×nsparseunweightedundirectedgraphG.LetL=diag(d)?Gbethen×nLaplacianofG,wheredisavectorofnodedegrees,andletV∈Rn×pbeanorthonor-maleigenbasiscorrespondingtothesmallestpeigenvaluesofL.(Chenetal.,2015)showsthattherelativelysmoothwindspeedsignaly∈Rncanbewellapproximatedbyusingonlyp=15graphLaplacianbasis.InTable3wecomparethemean-squareerror(MSE)forpredictiononthefulldesignpoolV:MSE=q1nky−Vˆβk22.Becausetheobjectiveispredictionbased,weonlyconsiderthetwopredictionrelatedcriteria:fV(Σ)=tr(VΣ−1V>)andfG(Σ)=maxdiag(VΣ−1V>).Thesubsetsizekissetask=2p=30,whichismuchsmallerthann=2642.Weob-servethatAlgorithm1consistentlyoutperformstheotherheuristicmethods,andissoefﬁcientthatitsrunningtimeisnegligible.Itisalsointerestingthatbyusingk=30samplesAlgorithm1alreadyachievesanMSEthatiscom-parabletotheOLSontheentiren=2642designpool.6.ConcludingremarksandopenquestionsWeproposedacomputationallyefﬁcientalgorithmthatap-proximatelycomputesoptimalsolutionsfortheexperimen-taldesignproblem,withnear-optimalrequirementonk(i.e.,thenumberofexperimentstochoose).Inparticular,weobtainedaconstantapproximationundertheveryweakconditionk>2p,anda(1+ε)approximationifreplace-mentorover-samplingisallowed.Ouralgorithmworksforallregularoptimalitycriteria.Animportantopenquestionistoachieve(1+ε)relativeapproximationratiounderthe“propersampling”regimek=r,orthe“slightover-sampling”regimek=(1+δ)r,forthewithoutreplacementmodel.Itwasshownin(Wangetal.,2016)thatasimplegreedymethodachieves(1+ε)approximationratioforA-andV-optimalityprovidedthatk=Ω(p2/ε).Whethersuchanalysiscanbeextendedtootheroptimalitycriteriaandwhetherthep2termcanbefurtherreducedtoanearlinearfunctionofpremainopen.Anotherpracticalquestionistodevelopfast-convergingoptimizationmethodsforthecontinuousprobleminEq.(6),especiallyforcriteriathatarenotdifferentiablesuchastheE-andG-optimality,wheresubgradientmeth-odshaveveryslowconvergencerate.AcknowledgementThisworkissupportedbyNSFgrantsCAREERIIS-1252412andCCF-1563918.WethankAdamsWeiYuforprovidinganefﬁcientimplemen-tationoftheprojectionstep,andotherusefuldiscussions.Near-optimaldesignofexperimentsviaregretminimizationReferencesAgeev,AlexanderAandSviridenko,MaximI.Pipagerounding:Anewmethodofconstructingalgorithmswithprovenperformanceguarantee.JournalofCom-binatorialOptimization,8(3):307–328,2004.Allen-Zhu,Zeyuan,Liao,Zhenyu,andOrecchia,Lorenzo.Spectralsparsiﬁcationandregretminimizationbeyondmatrixmultiplicativeupdates.InProceedingsofAnnualSymposiumontheTheoryofComputing(STOC),2015.Avron,HaimandBoutsidis,Christos.Fastersubsetselec-tionformatricesandapplications.SIAMJournalonMa-trixAnalysisandApplications,34(4):1464–1499,2013.Balcan,Maria-FlorinaandLong,PhilipM.Activeandpassivelearningoflinearseparatorsunderlog-concavedistributions.InProceedingsofAnnualConferenceonLearningTheory(COLT),2013.Bouhtou,Mustapha,Gaubert,Stephane,andSagnol,Guil-laume.Submodularityandrandomizedroundingtech-niquesforoptimalexperimentaldesign.ElectronicNotesinDiscreteMathematics,36:679–686,2010.Boutsidis,ChristosandWoodruff,DavidP.OptimalCURmatrixdecompositions.InProceedingsofAnnualSym-posiumontheTheoryofComputing(STOC),2014.ˇCern`y,MichalandHlad´ık,Milan.TwocomplexityresultsonC-optimalityinexperimentaldesign.ComputationalOptimizationandApplications,51(3):1397–1408,2012.Chaudhuri,Kamalika,Kakade,Sham,Netrapalli,Praneeth,andSanghavi,Sujay.Convergenceratesofactivelearn-ingformaximumlikelihoodestimation.InProceedingsofAdvancesinNeuralInformationProcessingSystems(NIPS),2015.Chen,Siheng,Varma,Rohan,Singh,Aarti,andKovaˇcevi´c,Jelena.Signalrepresentationsongraphs:Toolsandap-plications.arXivpreprintarXiv:1512.05406,2015.C¸ivril,AliandMagdon-Ismail,Malik.Onselectingamax-imumvolumesub-matrixofamatrixandrelatedprob-lems.TheoreticalComputerScience,410(47-49):4801–4811,2009.Deshpande,AmitandRademacher,Luis.Efﬁcientvolumesamplingforrow/columnsubsetselection.InProceed-ingsofAnnualConferenceonFoundationsofComputerScience(FOCS),2010.Deshpande,YashandMontanari,Andrea.Linearbanditsinhighdimensionandrecommendationsystems.InPro-ceedingsofAnnualAllertonConferenceonCommunica-tion,Control,andComputing(Allerton),2012.Dhillon,Paramveer,Lu,Yichao,Foster,DeanP,andUngar,Lyle.Newsubsamplingalgorithmsforfastleastsquaresregression.InProceedingsofAdvancesinNeuralInfor-mationProcessingSystems(NIPS),2013.Drineas,PetrosandMahoney,MichaelW.OntheNystr¨ommethodforapproximatingagrammatrixforimprovedkernel-basedlearning.JournalofMachineLearningRe-search,6(12):2153–2175,2005.Drineas,Petros,Mahoney,MichaelW,andMuthukrishnan,S.Relative-errorCURmatrixdecompositions.SIAMJournalonMatrixAnalysisandApplications,30(2):844–881,2008.Fedorov,ValeriiVadimovich.Theoryofoptimalexperi-ments.Elsevier,1972.Hazan,EladandKarnin,Zohar.Hard-marginactivelinearregression.InProceedingsofInternationalConferenceonMachineLearning(ICML),2015.Horel,Thibaut,Ioannidis,Stratis,andMuthukrishnan,S.Budgetfeasiblemechanismsforexperimentaldesign.InProceedingsofLatinAmericanSymposiumonTheoreti-calInformatics(LATIN),2014.Huang,Ruitong,Lattimore,Tor,Gy¨orgy,Andr´as,andSzepesv´ari,Csaba.Followingtheleaderandfastratesinlinearprediction:Curvedconstraintsetsandotherregu-larities.InProceedingsofAdvancesinNeuralInforma-tionProcessingSystems(NIPS),2016.Khuri,Andre,Mukherjee,Bhramar,Sinha,Bikas,andGhosh,Malay.Designissuesforgeneralizedlinearmod-els:areview.StatisticalScience,21(3):376–399,2006.Kulesza,AlexandTaskar,Ben.Determinantalpointpro-cessesformachinelearning.FoundationsandTrendsR(cid:13)inMachineLearning,5(2–3):123–286,2012.Li,Chengtao,Jegelka,Stefanie,andSra,Suvrit.Polyno-mialtimealgorithmsfordualvolumesampling.arXivpreprintarXiv:1703.02674,2017.Ma,Yifei,Garnett,Roman,andSchneider,Jeff.σ-optimalityforactivelearningonGaussianrandomﬁelds.InProceedingsofAdvancesinNeuralInformationPro-cessingSystems(NIPS),2013.McMahan,HBrendan.Follow-the-regularized-leaderandmirrordescent:EquivalencetheoremsandL1regular-ization.InProcedingsofInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS),2011.Miller,AlanandNguyen,Nam-Ky.AFedorovexchangealgorithmford-optimaldesign.JournaloftheRoyalSta-tisticalSociety,SeriesC(AppliedStatistics),43(4):669–677,1994.Near-optimaldesignofexperimentsviaregretminimizationPilanci,MertandWainwright,MartinJ.IterativeHes-siansketch:Fastandaccuratesolutionapproximationforconstrainedleast-squares.JournalofMachineLearn-ingResearch,17(53):1–38,2016.Pukelsheim,Friedrich.Optimaldesignofexperiments.SIAM,2006.Raskutti,GarveshandMahoney,Michael.Astatisticalperspectiveonrandomizedsketchingforordinaryleast-squares.arXivpreprintarXiv:1406.5986,2014.Ravi,SathyaN,Ithapu,VamsiK,Johnson,SterlingC,andSingh,Vikas.Experimentaldesignonabudgetforsparselinearmodelsandapplications.InProceedingsofInternationalConferenceonMachineLearning(ICML),2016.Silva,MarcelK,Harvey,NicholasJA,andSato,Cris-tianeM.Sparsesumsofpositivesemideﬁnitematrices.ACMTransactionsonAlgorithms,12(1):9,2016.Spielman,DanielAandSrivastava,Nikhil.Graphsparsi-ﬁcationbyeffectiveresistances.SIAMJournalonCom-puting,40(6):1913–1926,2011.Su,Hao,Yu,AdamsWei,andLi,Fei-Fei.Efﬁcienteu-clideanprojectionsontotheintersectionofnormballs.InProceedingsofInternationalConferenceonMachineLearning(ICML),2012.VanderVaart,AadW.Asymptoticstatistics,volume3.Cambridgeuniversitypress,2000.Wang,ShusenandZhang,Zhihua.ImprovingCURmatrixdecompositionandtheNystr¨omapproximationviaadap-tivesampling.JournalofMachineLearningResearch,14(1):2729–2769,2013.Wang,YiningandSingh,Aarti.Anempiricalcompari-sonofsamplingtechniquesformatrixcolumnsubsetse-lection.InProceedingsofAnnualAllertonConferenceonCommunication,Control,andComputing(Allerton),2015a.Wang,YiningandSingh,Aarti.Provablycorrectactivesamplingalgorithmsformatrixcolumnsubsetselectionwithmissingdata.arXivpreprintarXiv:1505.04343,2015b.Wang,YiningandSingh,Aarti.Noise-adaptivemargin-basedactivelearningandlowerboundsundertsybakovnoisecondition.InProceedingsofAAAIConferenceonArtiﬁcialIntelligence(AAAI),2016.Wang,Yining,Yu,WeiAdams,andSingh,Aarti.Oncom-putationallytractableselectionofexperimentsinregres-sionmodels.arXivpreprints:arXiv:1601.02068,2016.Woodruff,DavidP.Sketchingasatoolfornumericallin-earalgebra.FoundationsandTrendsR(cid:13)inTheoreticalComputerScience,10(1–2):1–157,2014.