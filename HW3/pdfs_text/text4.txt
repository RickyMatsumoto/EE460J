A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric
Properties of Discrete Distributions

Jayadev Acharya 1 Hirakendu Das 2 Alon Orlitsky 3 Ananda Theertha Suresh 4

Abstract

Symmetric distribution properties such as sup-
port size, support coverage, entropy, and prox-
imity to uniformity, arise in many applications.
Recently, researchers applied different estima-
tors and analysis tools to derive asymptotically
sample-optimal approximations for each of these
properties. We show that a single, simple, plug-in
estimator—proﬁle maximum likelihood (PML)–
is sample competitive for all symmetric proper-
ties, and in particular is asymptotically sample-
optimal for all the above properties.

1. Introduction

1.1. Symmetric distribution properties

Let ∆ def= {(p1, . . . ,pk) : pi ≥ 0, (cid:80)k
i=1 pi = 1, 1 ≤ k ≤ ∞}
denote the collection of all discrete distributions over ﬁnite
or inﬁnite support. A distribution property is a mapping
f : ∆ → R. It is symmetric if it remains unchanged under
relabeling of domain symbols, namely if it is determined
by just the probability multiset {p1, p2, . . . ,pk}. Many
important properties are symmetric. For example:

Support size S(p) = |{x : p(x) > 0}|, plays an important
role in population and vocabulary estimation.
Support coverage Sm(p) = (cid:80)
x(1−(1−p(x))m), the ex-
pected number of elements observed in m samples, arises
in ecological and biological studies, e.g., (Colwell et al.,
2012).
Shannon entropy H(p) = (cid:80)
p(x) , central to
information theory (Cover & Thomas, 2006), has numerous

x p(x) log 1

*Equal contribution 1Cornell University, Ithaca, NY 2Yahoo
Inc!, Sunnyvale, CA 3University of California, San Diego
4Google Research.
Jayadev Acharya
Correspondence to:
<acharya@cornell.edu>, Hirakendu Das <hdas@yahoo-
inc.com>, Alon Orlitsky <alon@ucsd.edu>, Ananda Theertha
Suresh <theertha@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

applications.
Distance to uniform (cid:107)p−u(cid:107)1 = (cid:80)
x |p(x)−1/|X ||, where
u is the uniform distribution over the domain X of p. This
distance measure appears in the error of hypothesis testing,
and the uniform distribution is arguably one of the com-
monest discrete distributions.

1.2. Distribution estimation

Considerable research, over many years, has focused on es-
timating distribution properties. In the common setting, an
unknown underlying distribution p ∈ ∆ generates n inde-
pendent samples X n def= X1, , . . . ,Xn, and the objective is
to estimate a given property f (p) as accurately as possible.

Speciﬁcally, an estimator for a distribution p over X is a
function ˆf : X n → R mapping observed samples to a
property estimate. The sample complexity of ˆf is the small-
est number of samples it requires to estimate a property f
with accuracy ε and conﬁdence probability δ, for all distri-
butions in a collection P ⊆ ∆,

ˆf (f, P, δ, ε) def=

C

(cid:110)

min

n : p(|f (p) − ˆf (X n)| ≥ ε) ≤ δ ∀p ∈ P

(cid:111)
.

The sample complexity of estimating f is the lowest sam-
ple complexity of any estimator,

C ∗(f, P, δ, ε) = min
ˆf

ˆf (f, P, δ, ε).

C

By taking the median of about log 1
δ independent estima-
tors, the error rate can be driven down from a constant to
δ. Therefore, the sample complexity depends on δ only
through a factor of at most log 1
δ . For simplicity, we there-
fore abbreviate C ˆf (f, P, 1/3, ε) by C ˆf (f, P, ε).

1.3. Result summary

Recent research has shown that while simple estimators for
the aforementioned properties require sample size n pro-
portional to the support size k, more sophisticated tech-
niques need only a sub-linear sample size n = Θ(k/ log k).

A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

However, each of the problems was approximated via dif-
ferent estimators and analysis techniques, that for some
properties were rather complex.

Motivated by the principle of maximum likelihood, we
show that a single, simple, plug-in estimator—proﬁle max-
imum likelihood (PML) (Orlitsky et al., 2004b)— is com-
petitive for estimating any symmetric property. Its sample
complexity is at most quadratically worse than that of any
estimator.

√

Speciﬁcally, we show that if a symmetric property can be
estimated using n samples with conﬁdence δ, then the PML
plug-in estimator can estimate it using as many samples
n. While this increase may seem high,
with conﬁdence δ·e
note that it is sub-exponential. We show that if a property
has an estimator that has a small bounded difference con-
stant (how much the estimator changes when we change
one sample), then the error probability reduces exponen-
tially with n (Please see Section 7.1). Combined, these two
facts imply that for properties with locally-smooth estima-
tors, the PML plug-in estimator is optimal up to a constant:
C PML = Θ(C ∗). We then show that all the above proper-
ties have locally-smooth estimators, hence they can be es-
timated by the PML plug-in estimator with up to a constant
factor more than the optimal number of samples.

1.4. Outline

The rest of the paper is organized as follows. In Section 2
we describe existing results and those shown in this paper.
In Section 3 we formally deﬁne the quantities involved and
state the results. In Section 4 we deﬁne proﬁles and PML.
In Section 5, we outline the new approach. In Section 6,
we demonstrate auxiliary results for maximum likelihood
estimators. In Section 7, we outline how we apply maxi-
mum likelihood to support, support coverage, entropy, and
uniformity. In Section 8, we provide the details for sup-
port, and support coverage and in the appendix we outline
results for distance to uniformity and entropy.

2. Previous and New Results

2.1. Previous Results

Plug-in estimation is a general approach for estimating dis-
tribution properties. It uses the samples X n to ﬁnd an ap-
proximation ˆp of p, and lets f (ˆp) estimate f (p).

One of the most common distribution estimators, dating
back to Fisher is maximum likelihood, that for clarity we
call sequence maximum likelihood (SML) (Aldrich, 1997).
To any sample xn it assigns the distribution p that maxi-
mizes p(xn). The SML estimate is exceedingly simple to
def= Nx(xn) of symbol x is the
derive. The multiplicity Nx
number of times it appears in the sequence xn. The empiri-

cal frequency estimator assigns to each symbol x, the frac-
tion ˆp(x) def= Nx/n of times it appears in the sample xn.
For example, if x7 =bananas, empirical frequency would
assign ˆp(a) = 3/7, ˆp(n) = 2/7, and ˆp(b) = ˆp(s) = 1/7.
It can be readily shown that SML is exactly the empirical
frequency estimator.

While the SML plug-in estimator performs well in the
limit of many samples, sophisticated techniques have re-
cently yielded more accurate estimators for several impor-
tant symmetric properties.

Support size. With ﬁnitely many samples, S(p) cannot
be estimated to any accuracy as many symbols with ar-
bitrarily small probability may not be observed. Mo-
tivated by databases, where each entry appears at least
once, (Raskhodnikova et al., 2009) considered distributions
whose non-zero probabilities are at least 1
k ,

∆≥ 1

k

def= {p ∈ ∆ : p(x) ∈ {0} ∪ [1/k, 1]} ,

and estimated the normalized support ˜S(p) def= S(p)/k.
It can be shown that C SML( ˜S(p), ∆≥ 1
ε ).
Yet (Valiant & Valiant, 2011a; Wu & Yang, 2015) showed
(cid:16) k
that C ∗( ˜S(p), ∆≥ 1
log k · log2 1

, ε) = Θ(k log 1

, ε) = Θ

(cid:17)

ε

.

k

k

def= Sm(p)/m.

Support coverage. Here too we consider the normalized
coverage ˜Sm(p)
(Good & Toulmin,
1956) proposed the Good Toulmin (GT) estimator that
achieves C GT( ˜Sm(p), ∆, ε) = m/2. Recently, (Orlit-
sky et al., 2016) derived a simple estimator showing that
C ∗( ˜Sm(p), ∆, ε) = Θ( m
log m · log 1
ε ). (Zou et al., 2016)
derived a more complex estimator with similar dependence
on m but worse dependence on ε.

Shannon entropy. Since elements with arbitrarily small
probability can contribute to an arbitrarily high entropy,
H(p) cannot be estimated over aribtrary support with
ﬁnitely many samples. Therefore researchers are mostly
interested in estimating entropy of distributions with sup-
port size at most k.

∆k

def= {p ∈ ∆ : S(p) ≤ k}.

It can be shown that C SML(H(p), ∆k, ε) = Θ( k
ε ) (Panin-
ski, 2003). Moreover, (Paninski, 2003) showed that
C ∗(H(p), ∆k, ε) is sublinear in k, (Valiant & Valiant,
2011a) showed that the optimal dependence on k is k/ log k
and (Wu & Yang, 2016; Jiao et al., 2015) obtained the
optimal dependence on both k, and ε, and showed that
(cid:16) k
log k · 1
C ∗(H(p), ∆k, ε) = Θ

(cid:17)

ε

.

Distance to uniform. (Valiant & Valiant, 2011b) showed
that C ∗((cid:107)p − u(cid:107)1, ∆k, ε) = O
, and (Jiao et al.,
2016) showed that this bound is tight.

(cid:16) k
log k · 1
ε2

(cid:17)

A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

These results are summarized in Table 1.

Other properties were considered as well.
(Bar-Yossef
et al., 2001; Acharya et al., 2015; Caferov et al., 2015;
Obremski & Skorski, 2017) estimated R´enyi entropy
and (Bu et al., 2016) estimated KL divergence. (Canonne,
2015) surveyed testing whether distributions have certain
properties, and (Jiao et al., 2014) studied the performance
of SML estimators for several properties. Closest to this
work in terms of approach and techniques are (Acharya
et al., 2011; 2012; 2013a;b; Valiant & Valiant, 2013; Orlit-
sky & Suresh, 2015) that design algorithms whose sample
complexity is provably close to the best possible regardless
of the domain size.

2.2. Proﬁle Maximum Likelihood

Symmetric distribution properties do not depend on the
symbol labels. They are determined by a simple sufﬁcient
statistic: the number of elements appearing any given num-
ber of times. The proﬁle of a sequence X n, denoted ϕ(X n)
is the multiset of the multiplicities of all the symbols ap-
pearing in X n. For example, ϕ(a b r a c a d a b r a) =
{1, 1, 2, 2, 5}, as two symbols appearing once, two ap-
pearing twice, and one symbol appearing ﬁve times, re-
moving the association of the individual symbols with the
multiplicities. Proﬁles are also referred to as histograms
of histograms (Batu et al., 2000), histogram order statis-
tics (Paninski, 2003), and ﬁngerprints (Valiant & Valiant,
2011a).

Motivated by the principle of maximum likelihood, (Orlit-
sky et al., 2004b; 2017b) discarded the symbol labels, and
considered the proﬁle maximum likelihood (PML) distribu-
tion that maximizes the probability of the observed proﬁle.

A number of PML properties were established. (Orlitsky
et al., 2004b; 2005) proved PML’s existence, consistency,
and some of its properties. (Orlitsky et al., 2004d; 2005;
Orlitsky & Pan, 2009; Pan et al., 2009) described addi-
tional properties and derived the PML distributions of sev-
eral short and simple proﬁles. (Orlitsky et al., 2017b;c) pro-
vide a uniﬁed review of several of these results. (Anevski
et al., 2013) contains a combination of previously-known
and new results. A related distribution-estimation approach
is described in (Orlitsky et al., 2004c; 2003).

Several approaches were taken to computing the PML
distribution.
Algebraic computation was considered
in (Acharya et al., 2010). A combination of the EM and
MCMC algorithms have shown excellent results for calcu-
lating the PML distribution and applying it to support-size
estimation (Orlitsky et al., 2004a; 2006; Pan, 2012) and a
summary of some of the results appears in (Orlitsky et al.,
2017a). (Vontobel, 2012; 2014) derived the Bethe approxi-
mation of these algorithms.

Following the ﬁrst draft of this work, (Vatedka & Vonto-
bel, 2016) showed that both theoretically and empirically
plug-in estimators obtained from the PML estimate yield
good estimates for symmetric functionals of Markov distri-
butions.

2.3. New Results

We show that replacing the SML plug-in estimator by PML
yields a uniﬁed estimator that, like the best results shown
via specialized techniques developed, is optimal.

Theorem 1. There is a uniﬁed approach based on PML
distribution that achieves the optimal sample complexity
for the problems of estimating the entropy, support, sup-
port coverage, and distance to uniformity.

We prove in Corollary 1 that the PML approach is com-
petitive with respect to any symmetric property. For sym-
metric properties, these results are perhaps a justiﬁcation of
Fisher’s thoughts on Maximum Likelihood:

“Of course nobody has been able to prove that maximum
likelihood estimates are best under all circumstances. Maximum
likelihood estimates computed with all the information available
may turn out to be inconsistent. Throwing away a substantial part
of the information may render them consistent.”

R. A. Fisher’s thoughts on Maximum Likelihood (Le Cam, 1979).

To prove these PML guarantees, we establish two results
that are of interest on their own right.

• With n samples, PML estimates any symmetric prop-
erty of p with essentially the same accuracy, and at
most e3
n times the error, of any other estimator. This
follows by combining Theorem 3 with Lemma 1.

√

• For a large class of symmetric properties, including
all those mentioned above, if there is an estimator that
uses n samples, and has an error probability 1/3, we
design an estimator using O(n) samples, whose er-
ror probability is nearly exponential in n. We remark
that this decay is much faster than applying the median
trick. This result follows by combining McDiarmid’s
inequality with Lemma 2.

Combined, these results prove that PML plug-in estimators
are sample-optimal.

We also introduce the notion of β-approximate ML distri-
butions, described in Deﬁnition 1. These distributions are
more relaxed version of PML, hence may be more easily
computed, yet they provide essentially the same perfor-
mance guarantees.

A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Property name
Entropy
Support size
Support coverage
Distance to u

P
f (p)
∆k
H(p)
˜S(p)
∆≥ 1
˜Sm(p)
∆
(cid:107)p − u(cid:107)1 ∆X

k

C SML
k
ε
k log 1
ε
m
k
ε2

C ∗
k
log k

1
ε

k

log k log2 1
log m log 1

m

ε

ε

k
log k

1
ε2

PML
Theorem 5 and Section 8.1
Theorem 5 and Section 8.2
Theorem 5 and Section A
Theorem 5 and Section A

Table 1. Estimation complexity for various properties up to a constant factor. For all properties shown, PML achieves the best known
results up to a constant factor. The details of where the optimal sample complexity was derived for each problem is discussed in
Section 2.1.

3. Formal Deﬁnitions and Results

In the past, different sophisticated estimators were used for
every property in Table 1. We show that the simple plug-in
estimator that uses any PML approximation ˜p, has optimal
performance guarantees for all these properties.

In the next theorem, assume n is at least the optimal sample
complexity of estimating entropy, support, support cover-
age, and distance to uniformity (given in Table 1) respec-
tively.
Theorem 2. For all ε > c/n0.2, any plug-in exp (−
approximate PML ˜p satisﬁes,

n)-

√

Entropy

Support size

C ˜p(H(p), ∆k, ε) (cid:16) C ∗(H(p), ∆k, ε),

C ˜p(S(p)/k, ∆≥ 1

, ε) (cid:16) C ∗(S(p)/k, ∆≥ 1

, ε),

k

k

Support coverage

Distance to uniformity

C ˜p(Sm(p)/m, ∆, ε) (cid:16) C ∗(Sm(p)/m, ∆, ε),

Lemma 1 ((Hardy & Ramanujan, 1918)).
exp(3

n).

√

|Φn| ≤

For a distribution p, the probability of a proﬁle ϕ is deﬁned
as

p(ϕ) def=

(cid:88)

p(X n),

X n:ϕ(X n)=ϕ

(cid:81)n

X n:ϕ(X n)=ϕ

the probability of observing a sequence with proﬁle ϕ. Un-
der i.i.d. sampling, p(ϕ) = (cid:80)
i=1 p(Xi).
For example, the probability of observing a sequence with
proﬁle ϕ = {1, 2} is the probability of observing a se-
quence with one symbol appearing once, and one symbol
appearing twice. A sequence with a symbol x appearing
twice and y appearing once (e.g., x y x) has probability
p(x)2p(y). Appropriately normalized, for any p, the prob-
ability of the proﬁle {1, 2} is

p({1, 2}) =

(cid:88)

n
(cid:89)

p(Xi) =

ϕ(X n)={1,2}

i=1

(cid:19) (cid:88)
(cid:18)3
1

a(cid:54)=b∈X

p(a)2p(b),

(1)

where the normalization factor is independent of p. The
summation is a monomial symmetric polynomial in the
probability values. See (Pan, 2012) for more examples.

C ˜p((cid:107)p − u(cid:107)1, ∆X , ε) (cid:16) C ∗((cid:107)p − u(cid:107)1, ∆k, ε).

4.2. PML Estimation Scheme

4. PML: Proﬁle Maximum Likelihood

4.1. Preliminaries

For a sequence X n, recall that the multilplicity Nx is the
number of times x appears in X n. Discarding, the labels,
proﬁle of a sequence (Orlitsky et al., 2004b) is deﬁned be-
low. Let Φn be all proﬁles of length-n sequences. Then,
Φ4 = {{1, 1, 1, 1}, {1, 1, 2}, {1, 3}, {2, 2}, {4}}. In par-
ticular, a proﬁle of a length-n sequence is an unordered
partition of n. Therefore, |Φn|, the number of proﬁles
of length-n sequences is equal to the partition number of
n. Then, by the Hardy-Ramanujan bounds on the partition
number,

Recall that pXn is the distribution maximizing the proba-
bility of X n. Similarly, deﬁne (Orlitsky et al., 2004b):

pϕ

def= max
p∈P

p(ϕ)

as the distribution in P that maximizes the probability of
observing a sequence with proﬁle ϕ.

For example, for ϕ = {1, 2}. For P = ∆k, from (1),

pϕ = arg max
p∈∆k

(cid:88)

a(cid:54)=b

p(a)2p(b).

Note that in contrast, SML only maximizes one term of this
expression.

For a, b > 0, denote a (cid:46) b or b (cid:38) a if for some universal

constant c, a/b ≤ c. Denote a (cid:16) b if both a (cid:46) b and a (cid:38) b.

We give two examples from the table in (Orlitsky et al.,
2004b) to distinguish between SML and PML distributions,

A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

and also show an instance where PML outputs distributions
with a larger support than those appearing in the sample.
Example 1. Let X = {a, b, . . . , z}. Suppose X n = x y x,
then the SML distribution is (2/3, 1/3). However, the dis-
tribution in ∆ that maximizes the probability of the proﬁle
ϕ(x y x) = {1, 2} is (1/2, 1/2). Another example, illus-
trating the power of PML to predict new symbols is X n =
a b a c, with proﬁle ϕ(a b a c) = {1, 1, 2}. The SML distri-
bution is (1/2, 1/4, 1/4), but the PML is a uniform distri-
bution over 5 elements, namely (1/5, 1/5, 1/5, 1/5, 1/5).

Suppose we want to estimate a symmetric property f (p)
of an unknown distribution p ∈ P given n independent
samples. Our high level approach using PML is described
below.

Input: Class of distributions P, symmetric function
f (·), sample X n

1. Compute pϕ : arg maxp∈P p(ϕ(X n)).

2. Output f (pϕ).

There are a few advantages of this approach (as is true with
any plug-in approach): (i) the computation of PML is ag-
nostic to the function f at hand, (ii) there are no parameters
to be tuned, (iii) techniques such as Poisson sampling or
median tricks are not necessary, (iv) well motivated by the
maximum-likelihood principle.

Comparison to the linear-programming plug-in estima-
tor (Valiant & Valiant, 2011a). Our approach is per-
haps closest in ﬂavor to the plug-in estimator of (Valiant &
Valiant, 2011a). Indeed, as mentioned in (Valiant, 2012),
their linear-programming estimator is motivated by the
question of estimating the PML. Their result was the ﬁrst
estimator to provide sample complexity bounds in terms
of the alphabet size, and accuracy the problems of entropy
and support estimation. Before we explain the differences
of the two approaches, we brieﬂy explain their approach.

Deﬁne, ϕµ(X n) to be the number of elements that appear
µ times. For example, when X n = a b r a c a d a b r a,
ϕ1 = 2, ϕ2 = 2, and ϕ5 = 1. (Valiant & Valiant, 2011a)
design a linear program that uses SML for high values of
µ, and formulate a linear program to ﬁnd a distribution for
which E[ϕµ]’s are close to the observed ϕµ’s. They then
plug-in this estimate to estimate the property. On the other
hand, our approach, by the nature of ML principle, tries to
ﬁnd the distribution that best explains the entire proﬁle of
the observed data, not just some partial characteristics. It
therefore has the potential to estimate any symmetric prop-
erty and estimate the distribution closely in any distance
measures, competitive with the best possible. For exam-
ple, the guarantees of the linear program approach are sub-
optimal in terms of the desired accuracy ε. For entropy

estimation the optimal dependence is 1
ε , whereas (Valiant
& Valiant, 2011a) yields 1
ε2 . This is more prominent for
support size and support coverage, which have optimal
dependence of polylog( 1
ε ), whereas (Valiant & Valiant,
2011a) gives a 1
ε2 dependence. Besides, we analyze the
ﬁrst method proposed for estimating symmetric properties,
designed from the ﬁrst principles, and show that in fact it
is competitive with the optimal estimators for various prob-
lems.

5. Proof Outline

Our arguments have two components.
In Section 6 we
prove a general result for the performance of plug-in es-
timation via maximum likelihood approaches.
Let P be a class of distributions over Z, and f : P → R be
a function. For z ∈ Z, let

pz

def= arg max
p∈P

p(z)

be the maximum-likelihood estimator of z in P. Upon ob-
serving z, f (pz ) is the ML estimator of f . In Theorem 4,
we show that if there is an estimator that achieves error
probability δ, then the ML estimator has an error probabil-
ity at most δ|Z|. We note that variations of this result in the
asymptotic statistics were studied before (see (Lehmann &
Casella, 1998)). Our contribution is to use these results in
the context of symmetric properties and show sample com-
plexity bounds in the non-asymptotic regime.

We emphasize that, throughout this paper Z will be the set
of proﬁles of length n, and P will be distributions induced
over proﬁles by length-n i.i.d. samples. Therefore, we
have |Z| = |Φn|. By Lemma 1, if there is a proﬁle based
estimator with error probability δ, then the PML approach
n). Such argu-
will have error probability at most δ exp(3
ments were used in hypothesis testing to show the existence
of competitive testing algorithms for fundamental statisti-
cal problems (Acharya et al., 2011; 2012).

√

At its face value this seems like a weak result. Our second
key step is to prove that for the properties we are interested,
it is possible to obtain very sharp guarantees. For example,
we show that if we can estimate the entropy to an accuracy
±ε with error probability 1/3 using n samples, then we can
estimate the entropy to accuracy ±2ε with error probability
exp(−n0.9) using only 2n samples. Using this sharp con-
centration, the new error probability term dominates |Φn|,
and we obtain our results. The arguments for sharp con-
centration are based on modiﬁcations to existing estimators
and a new analysis. Most of these results are technical and
are in the appendix.

A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

6. Maximum Likelihood Property Estimation

We establish performance guarantees of ML property es-
timation in a general set-up. Recall that P is a collection
of distributions over Z, and f : P → R. Given a sam-
ple Z from an unknown p ∈ P, we want to estimate f (p).
The maximum likelihood approach is the following two-
step procedure.

1. Find pZ = arg maxp∈P p(Z).

2. Output f (pZ ).

We bound the performance of this approach in the follow-
ing theorem.
Theorem 3. Suppose there is an estimator ˆf : Z → R,
such that for any p, and Z ∼ p,

Pr

(cid:16)(cid:12)
(cid:12)f (p) − ˆf (Z)
(cid:12)

(cid:12)
(cid:17)
(cid:12)
(cid:12) > ε

< δ,

then

Pr (|f (p) − f (pZ )| > 2ε) ≤ δ · |Z| .

(2)

(3)

Proof. Consider symbols with p(z) ≥ δ and p(z) < δ
separately. A distribution p with p(z) ≥ δ outputs z with
probability at least δ. For (2) to hold, we must have,
(cid:12)
(cid:12)
(cid:12)f (p) − ˆf (z)
(cid:12)
(cid:12)
(cid:12) < ε. By the deﬁnition of ML, pz (z) ≥
(cid:12)
(cid:12)
p(z) ≥ δ, and for (2) to hold for pz ,
(cid:12) < ε.
By the triangle inequality, for all such z,
(cid:12)
(cid:12)f (p) − ˆf (z)
(cid:12)

(cid:12)
(cid:12)f (pz ) − ˆf (z)
(cid:12)

(cid:12)
(cid:12)f (pz ) − ˆf (z)
(cid:12)

|f (p) − f (pz )| ≤

(cid:12)
(cid:12)
(cid:12) ≤ 2ε.

(cid:12)
(cid:12)
(cid:12) +

Thus if p(z) ≥ δ, then PML satisﬁes the required guar-
antee with zero probability of error, and any error occurs
only when p(z) < δ. We bound this probability as follows.
When Z ∼ p,

Pr (p(Z) < δ) ≤

p(z) < δ · |Z| .

(cid:88)

z∈Z:p(z)<δ

For some problems, it might be easier to just approximate
the ML, instead of ﬁnding it exactly. We deﬁne an approx-
imation ML as follows:
Deﬁnition 1 (β-approximate ML). Let β ≤ 1. For Z ∈ Z,
˜pZ ∈ P is a β-approximate ML distribution if ˜pz(z) ≥
β · pz (z). When Z is proﬁles of length-n, a β-approximate
PML is a distribution ˜pϕ such that ˜pϕ(ϕ) ≥ β · pϕ(ϕ).

The next result proves guarantees for any β-approximate
ML estimator.
Theorem 4. Suppose there exists an estimator satisfy-
ing (2). For any p ∈ P and Z ∼ p, any β-approximate
ML ˜pZ satisﬁes:

Pr (|f (p) − f (˜pZ)| > 2ε) ≤ δ · |Z|/β.

The proof is very similar to the previous theorem and is
presented in the Appendix B.

6.1. Competitiveness of ML via Median Trick

Suppose for a property f (p), there is an estimator with
sample complexity n that achieves an accuracy ±ε with
probability of error at most 1/3. The standard method to
boost the error probability is the median trick: (i) Obtain
O(log(1/δ)) independent estimates using O(n log(1/δ))
independent samples. (ii) Output the median of these es-
timates. This is an ε-accurate estimator of f (p) with error
probability at most δ. By deﬁnition, estimators are a map-
ping from the samples to R. However, in many applications
the estimators map from a much smaller (some sufﬁcient
statistic) of the samples. Denote by Zn the space consist-
ing of all sufﬁcient statistics that the estimator uses. For
example, estimators for symmetric properties, such as en-
tropy typically use the proﬁle of the sequence, and hence
Zn = Φn. Using the median-trick, we get the following
result.
Corollary 1. Let ˆf : Zn → R be an estimator of f (p) with
accuracy ε and error-probability 1/3. The ML estimator
achieves accuracy 2ε with probability at least 2/3 using

(cid:26)

min

n(cid:48) :

(cid:27)

n(cid:48)
20 log(3Zn(cid:48))

> n samples.

Proof. Since n is the number of samples to get error prob-
ability 1/3, by the Chernoff bound, the error after n(cid:48) sam-
ples is at most exp(−(n(cid:48)/(20n))). Therefore,
the er-
ror probability of the ML estimator for accuracy 2ε is at
most exp(−(n(cid:48)/(20n)))Zn(cid:48), which we desire to be at most
1/3.

√

For estimators that use the proﬁle of sequences, |Φn| <
exp(3
n). Plugging this in the previous result shows that
the PML based approach has a sample complexity of at
most O(n2). This result holds for all symmetric proper-
ties, independent of ε, and the alphabet size k. For the
problems mentioned earlier, something much better in pos-
sible, namely the PML approach is optimal up to constant
factors.

7. Sample optimality of PML

7.1. Sharp Concentration for Some Properties

To obtain sample-optimality guarantees for PML, we need
to drive the error probability down much faster than the
median trick. We achieve this by using McDiarmid’s in-
equality stated below. Let ˆf : X ∗ → R. Suppose ˆf gets
n independent samples X n from an unknown distribution.
j changed ˆf by
Moreover, changing one of the Xj to any X (cid:48)

A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

at most c∗. Then McDiarmid’s inequality (bounded differ-
ence inequality, (Boucheron et al., 2013)) states that,

coverage, and distance to uniformity to an accuracy of 4ε
with probability at least 1 − exp(−

n).

√

Pr

(cid:16)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:17)
ˆf (X n) − E[ ˆf (X n)]
(cid:12)
(cid:12) > t

≤ 2 exp

−

. (4)

(cid:18)

(cid:19)

2t2
nc2
∗

This inequality can be used to show strong error probability
bounds for many problems. We mention a simple applica-
tion for estimating discrete distributions.

Example 2. It is well known (Devroye & Lugosi, 2001)
that SML requires Θ(k/ε2) samples to estimate p in (cid:96)1 dis-
tance with probability at least 2/3. In this case, ˆf (X n) =
(cid:80)
(cid:12), and therefore c∗ is at most 2/n. Using
x
McDiarmid’s inequality, it follows that SML has an error
probability of δ = 2 exp(−k/2), while still using Θ(k/ε2)
samples.

n − p(x)(cid:12)
(cid:12)
(cid:12) Nx

def=

Let Bn be the bias of an estimator ˆf (X n) of f (p), namely
Bn

(cid:12)
(cid:12)
(cid:12)f (p) − E[ ˆf (X n)]
(cid:12)
(cid:12)
(cid:12) . By the triangle inequality,
(cid:12)
(cid:12)
(cid:12)f (p) − ˆf (X n)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)f (p) − E[ ˆf (X n)]
(cid:12)
(cid:12)
(cid:12) +

(cid:12)
ˆf (X n) − E[ ˆf (X n)]
(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
(cid:12)

= Bn +

(cid:12)
(cid:12)
ˆf (X n) − E[ ˆf (X n)]
(cid:12)
(cid:12)
(cid:12) .
(cid:12)

Plugging this in (4),

Pr

(cid:12)
(cid:16)(cid:12)
(cid:12)f (p) − ˆf (X n)]
(cid:12)
(cid:12)
(cid:12) > t + Bn

(cid:17)

≤ 2 exp

−

.

(5)

(cid:18)

(cid:19)

2t2
nc2
∗

With this in hand, we need to show that c∗ can be bounded
for estimators for the properties we consider. In particular,
we will show that

Lemma 2. Let α > 0 be a ﬁxed constant. For entropy,
support, support coverage, and distance to uniformity there
exist proﬁle based estimators that use the optimal number
of samples (given in Table 1), have bias ε and if we change
any of the samples, changes by at most c · nα
n , where c is a
positive constant.

We prove this lemma by proposing several modiﬁcations to
the existing sample-optimal estimators. The modiﬁed esti-
mators will preserve the sample complexity up to constant
factors and also have a small c∗. The proof details are given
in the appendix.

Using (5) with Lemma 2,

Proof. Let α = 0.05. By Lemma 2, for each property of
interest, there are estimators based on the proﬁles of the
samples such that using near-optimal number of samples,
they have bias ε and maximum change if we change any of
the samples is at most c(cid:48)nα/n for some constant c(cid:48). Hence,
by McDiarmid’s inequality, an accuracy of 2ε is achieved
with probability at least 1−2 exp
. Now
suppose ˜p is any β-approximate PML distribution. Then by
Theorem 4

−2ε2n1−2α/c(cid:48)2(cid:17)

(cid:16)

Pr (|f (p)−f (˜p)| > 4ε) <

δ · |Φn|
β

2 exp(−2ε2n1−2α/c(cid:48)2 + 3

n)

√

≤

√

≤ exp(−

n),

β

where in the last step we used ε2n1−2α (cid:38) c(cid:48)√
exp(−

n).

√

n, and β >

8. Support and Support Coverage

We analyze both support coverage and the support estima-
tion via a single approach. We ﬁrst start with support cover-
age. Recall that the goal is to estimate Sm(p), the expected
number of distinct symbols that we see after observing m
samples from p. By the linearity of expectation,

Sm(p) =

E[INx(Xm)>0] =

(1 − (1 − p(x))m) .

(cid:88)

x∈X

(cid:88)

x∈X

The problem is closely related to the support coverage
problem (Orlitsky et al., 2016), where the goal is to esti-
mate Ut(X n), the number of new distinct symbols that we
observe in n · t additional samples. Hence

Sm(p) = E

ϕi

+ E[Ut],

(cid:35)

(cid:34) n
(cid:88)

i=1

where t = (m − n)/n. We show that the modiﬁcation of
an estimator in (Orlitsky et al., 2016) is also near-optimal
and satisﬁes conditions in Lemma 2. We propose to use the
following estimator

ˆSm(p) =

ϕi + U SGT

t

(X n),

n
(cid:88)

i=1

Theorem 5. Let n be the optimal sample complexity of esti-
mating entropy, support, support coverage and distance to
uniformity (given in table 1) and c be a large positive con-
stant. Let ε ≥ c/n0.2, then any for any β > exp (−
n),
the β-PML estimator estimates entropy, support, support

√

i=1 ϕi(−t)i Pr(Z ≥ i) and Z is a
where U SGT
Poisson random variable with mean r and t = (m − n)/n.

(X n) = (cid:80)n

t

The above theorem also works for any ε (cid:38) 1/n0.25−η for

any η > 0

A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

We remark that the proof also holds for Binomial smoothed
random variables as discussed in (Orlitsky et al., 2016).

8.2. Support Estimator

The next lemma bounds the bias of the estimator.

0 ≤ S(p) − Sm(p) =

(1 − p(x))m

E[ϕi] + E[U SGT

t

(X n)] −

(1 − (1 − p(x))m)

(cid:88)

x∈X

n
(cid:88)

=

i=1
= E[U SGT

t

(cid:88)

x∈X

(X n)] −

((1 − p(x))n − (1 − p(x))m) .

for all ε > 12nα/n.

Recall that the quantity of interest in support estimation is
˜S(p), which we wish to estimate to an accuracy of ε.

Proof of Lemma 2 for support. Note that we are interested
in distributions with all the non zero probabilities are at
least 1/k. We propose to estimate ˜S(p) using ˆSm(p)/k,
for m = k log 3
ε , then by
ε .
Lemma 3, the maximum coefﬁcient of ˆSm(p)/k is at most
k e m
n log 3
2
ε is at most
kα/k < nα/n.

If we choose r = log 3

α log(k/21/α) log2 3

ε , which for n ≥

k

To bound the bias, note that for this choice of m

(cid:88)

x
(cid:88)

x

≤

e−mp(x) ≤ ke− log 3

ε =

kε
3

.

Similarly, by Lemma 4,

1
|E[ ˆSm(p)] − S(p)|
k
1
k
1
k

1
k
(2 + 2er(t−1) + ke−r) +

|E[ ˆSm(p)] − Sm(p)| +

≤

≤

|S(p) − Sm(p)|

ε
3

≤ ε,

9. Discussion and Future Directions

We studied estimation of symmetric properties of discrete
distributions using the principle of maximum likelihood,
and proved optimality of this approach for a number of
problems. A number of directions are of interest. We be-
lieve that the lower bound requirement on ε is perhaps an
artifact of our proof technique, and that the PML based ap-
proach is indeed optimal for all ranges of ε. Approximation
algorithms for estimating the PML distributions would be
a fruitful direction to pursue. Given our results, approxi-
mations stronger than exp(−ε2n) would be very interest-
ing. In the particular case when the desired accuracy is a
constant, even an exponential approximation would be suf-
ﬁcient for many properties. We plan to apply the heuris-
tics proposed by (Vontobel, 2012) for various problems we
consider, and compare with the state of the art provable
methods.

We need to bound the maximum coefﬁcient and the bias to
apply Lemma 2. We ﬁrst bound the maximum coefﬁcient
of this estimator.

Lemma 3. For all n ≤ m/2, the maximum coefﬁcient of
ˆSm(p) is at most 1 + er(t−1).

Proof. For any i, the coefﬁcient of ϕi is 1 + (−t)i Pr(Z ≥
i). It can be upper bounded as 1 + (cid:80)t
= 1 +
er(t−1).

e−r(rt)i
i!

i=0

Lemma 4. For all n ≤ m/2, the bias of the estimator is
bounded by

|E[ ˆSm(p)] − Sm(p)| ≤ 2 + 2er(t−1) + min(m, S(p))e−r.

Proof. As before let t = (m − n)/n.

E[ ˆSm(p)] − Sm(p)

Hence by Lemma 8 and Corollary 2, in (Orlitsky et al.,
2016), we get

|E[ ˆSm(p)] − Sm(p)| ≤ 2+2er(t−1) + min(m, S(p))e−r.

Using the above two lemmas we prove results for both the
observed support coverage and support estimator.

8.1. Support Coverage Estimator

Recall that the quantity of interest in support coverage es-
timation is Sm(p)/m, which we wish to estimate to an ac-
curacy of ε.

Proof of Lemma 2 for observed. If we choose r = log 3
ε ,
then by Lemma 3, the maximum coefﬁcient of ˆSm(p)/m
is at most 2
is at
most nα/m < nα/n. Similarly, by Lemma 4,

ε , which for m ≤ α n log(n/21/α)

m e m

n log 3

log(3/ε)

1
m

|E[ ˆSm(p)] − Sm(p)| ≤

(2 + 2er(t−1) + me−r) ≤ ε,

1
m

for all ε > 6nα/n.

A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Acknowledgements

The authors thank the reviewers for valuable feedback and
the NSF for support through grants CIF-1564355, CIF-
1619448, CRII-CIF-1657471, and a Cornell University
start-up grant. Jayadev Acharya thanks Clement Canonne,
Jiantao Jiao, and Pascal Vontobel for interesting discus-
sions.

References

Acharya, Jayadev, Das, Hirakendu, Mohimani, Hosein, Or-
litsky, Alon, and Pan, Shengjun. Exact calculation of
pattern probabilities. In ISIT, pp. 1498 –1502, 2010.

Bu, Yuheng, Zou, Shaofeng, Liang, Yingbin, and Veer-
avalli, Venugopal V. Estimation of KL divergence be-
tween large-alphabet distributions. In ISIT, 2016.

Caferov, Cafer, Kaya, Barıs¸, ODonnell, Ryan, and Say,
AC Cem. Optimal bounds for estimating entropy with
In International Symposium on Mathe-
pmf queries.
matical Foundations of Computer Science, pp. 187–198.
Springer, 2015.

Cai, T Tony, Low, Mark G, et al. Testing composite hy-
potheses, hermite polynomials and optimal estimation of
a nonsmooth functional. The Annals of Statistics, 39(2):
1012–1041, 2011.

Acharya, Jayadev, Das, Hirakendu, Jafarpour, Ashkan, Or-
litsky, Alon, and Pan, Shengjun. Competitive closeness
testing. COLT, 19:47–68, 2011.

Canonne, Cl´ement L. A survey on distribution testing:
Your data is big. but is it blue? Electronic Colloquium
on Computational Complexity (ECCC), 22:63, 2015.

Acharya, Jayadev, Das, Hirakendu, Jafarpour, Ashkan,
Suresh,
Competitive classiﬁcation and

Orlitsky, Alon,
Ananda Theertha.
closeness testing. In COLT, 2012.

Shengjun,

Pan,

and

Acharya, Jayadev, Jafarpour, Ashkan, Orlitsky, Alon, and
Suresh, Ananda Theertha. Optimal probability estima-
tion with applications to prediction and classiﬁcation. In
COLT, 2013a.

Acharya, Jayadev, Jafarpour, Ashkan, Orlitsky, Alon, and
Suresh, Ananda Theertha. A competitive test for unifor-
mity of monotone distributions. In AISTATS, 2013b.

Acharya,

Orlitsky,

Jayadev,
Ananda Theertha,
complexity of estimating R´enyi entropy.
2015.

and Tyagi, Himanshu.

Alon,

Suresh,
The
In SODA,

Aldrich, John. R.a. ﬁsher and the making of maximum like-
lihood 1912-1922. Statistical Science, 12(3):162–176,
09 1997.

Anevski, Dragi, Gill, Richard D, and Zohren, Stefan. Esti-
mating a probability mass function with unknown labels.
arXiv preprint arXiv:1312.1200, 2013.

Bar-Yossef, Ziv, Kumar, Ravi, and Sivakumar, D. Sampling
algorithms: lower bounds and applications. In Sympo-
sium on Theory of computing, pp. 266–275. ACM, 2001.

Batu, Tugkan, Fortnow, Lance, Rubinfeld, Ronitt, Smith,
Warren D., and White, Patrick. Testing that distributions
are close. In FOCS, pp. 259–269, 2000.

Boucheron, S., Lugosi, G., and Massart, P. Concentration
Inequalities: A Nonasymptotic Theory of Independence.
OUP Oxford, 2013.

Colwell, Robert K, Chao, Anne, Gotelli, Nicholas J,
Lin, Shang-Yi, Mao, Chang Xuan, Chazdon, Robin L,
and Longino, John T. Models and estimators linking
individual-based and sample-based rarefaction, extrapo-
lation and comparison of assemblages. Journal of plant
ecology, 5(1):3–21, 2012.

Cover, Thomas M. and Thomas, Joy A. Elements of infor-

mation theory (2. ed.). Wiley, 2006.

Devroye, Luc and Lugosi, G´abor. Combinatorial methods

in density estimation. Springer, 2001.

Good, IJ and Toulmin, GH. The number of new species,
and the increase in population coverage, when a sample
is increased. Biometrika, 43(1-2):45–63, 1956.

Hardy, Godfrey H and Ramanujan, Srinivasa. Asymptotic
formulaæ in combinatory analysis. Proceedings of the
London Mathematical Society, 2(1):75–115, 1918.

Jiao, Jiantao, Venkat, Kartik, Han, Yanjun, and Weiss-
man, Tsachy. Maximum likelihood estimation of
arXiv preprint
functionals of discrete distributions.
arXiv:1406.6959, 2014.

Jiao, Jiantao, Venkat, Kartik, Han, Yanjun, and Weissman,
Tsachy. Minimax estimation of functionals of discrete
distributions. IEEE Transactions on Information Theory,
61(5):2835–2885, 2015.

Jiao, Jiantao, Han, Yanjun, and Weissman, Tsachy. Mini-
max estimation of the L1 distance. In ISIT, pp. 750–754,
2016.

Le Cam, Lucien Marie. Maximum likelihood: an introduc-

tion. JSTOR, 1979.

A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Lehmann, Erich Leo and Casella, George. Theory of point
estimation, volume 31. Springer Science & Business
Media, 1998.

Obremski, Maciej and Skorski, Maciej. Renyi entropy es-

timation revisited. In APPROX, 2017.

Orlitsky, A., Pan, S., Sajama, Santhanam, P., Viswanathan,
K., and Zhang, J. Pattern maximum likelihood: Compu-
tation and experiments. Arxiv, 2017a.

Orlitsky, Alon and Pan, Shengjun. The maximum likeli-
hood probability of skewed patterns. In ISIT, 2009.

Orlitsky, Alon and Suresh, Ananda Theertha. Competitive
In
distribution estimation: Why is good-turing good.
NIPS, pp. 2143–2151, 2015.

Orlitsky, Alon, Santhanam, Narayana P., and Zhang, Junan.
Always good turing: Asymptotically optimal probability
estimation. In FOCS, 2003.

Orlitsky, Alon, Sajama, S, Santhanam, NP, Viswanathan,
K, and Zhang, Junan. Algorithms for modeling distribu-
tions over large alphabets. In ISIT, 2004a.

Orlitsky, Alon, Santhanam, Narayana P., Viswanathan, Kr-
ishnamurthy, and Zhang, Junan. On modeling proﬁles
instead of values. In UAI, 2004b.

Orlitsky, Alon, Santhanam, Narayana P, and Zhang, Ju-
nan. Universal compression of memoryless sources over
unknown alphabets. IEEE Transactions on Information
Theory, 50(7):1469–1481, 2004c.

Orlitsky,

Alon,

Santhanam,

Narayana

Viswanathan, Krishna, and Zhang,
(size) and order in distribution modeling. 2004d.

Junan.

Prasad,
Low

Orlitsky, Alon, Santhanam, Narayana, Viswanathan, Kr-
ishnamurthy, and Zhang, Junan. Convergence of pro-
ﬁle based estimators. In Proceedings of the 2005 IEEE
International Symposium on Information Theory (ISIT),
pp. 1843 –1847, 2005.

Orlitsky,

Alon,

Santhanam,

Narayana
Viswanathan, Krishna, and Zhang, Junan.
retical and experimental
probabilities. In Information Theory Workshop, 2006.

Prasad,
Theo-
results on modeling low

Orlitsky, Alon, Suresh, Ananda Theertha, and Wu, Yihong.
Optimal prediction of the number of unseen species.
Proceedings of the National Academy of Sciences, 2016.
doi: 10.1073/pnas.1607774113.

Orlitsky, Alon, Santhanam, Narayana, Viswanathan, Krish-
namurthy, and Zhang, Junan. On estimating the proba-
bility multiset part ii: Properties of the pattern maximum
likelihood estimator. Arxiv, 2017c.

Pan, Shengjun. On the theory and application of pattern
maximum likelihood. PhD thesis, UC San Diego, 2012.

Pan, Shengjun, Acyarya, Jayadev, and Orlitsky, Alon. The
maximum likelihood probability of unique-singleton,
ternary, and length-7 patterns. pp. 1135–1139, 2009.

Paninski, Liam. Estimation of entropy and mutual infor-
mation. Neural computation, 15(6):1191–1253, 2003.

Raskhodnikova, Sofya, Ron, Dana, Shpilka, Amir, and
Smith, Adam. Strong lower bounds for approximating
distribution support size and the distinct elements prob-
lem. SIAM Journal on Computing, 39(3):813–842, 2009.

Timan, A. F. Theory of Approximation of Functions of a

Real Variable. Pergamon Press, 1963.

Valiant, Gregory and Valiant, Paul. Estimating the un-
seen: an n/log(n)-sample estimator for entropy and sup-
port size, shown optimal via new clts. In STOC, 2011a.

Valiant, Gregory and Valiant, Paul. The power of linear

estimators. In FOCS, pp. 403–412. IEEE, 2011b.

Valiant, Gregory and Valiant, Paul.

Instance-by-instance
optimal identity testing. Electronic Colloquium on Com-
putational Complexity (ECCC), 20:111, 2013.

Valiant, Gregory John. Algorithmic approaches to statis-
tical questions. PhD thesis, University of California,
Berkeley, 2012.

Vatedka, Shashank and Vontobel, Pascal O. Pattern max-
imum likelihood estimation of ﬁnite-state discrete-time
markov chains. In ISIT, 2016.

Vontobel, Pascal O. The bethe approximation of the pat-
tern maximum likelihood distribution. In IEEE ISIT, pp.
2012–2016, 2012.

Vontobel, Pascal O. The bethe and sinkhorn approxima-
tions of the pattern maximum likelihood estimate and
their connections to the valiant-valiant estimate. In In-
formation Theory and Applications Workshop, ITA, pp.
1–10, 2014.

Wu, Yihong and Yang, Pengkun. Chebyshev polynomials,
moment matching, and optimal estimation of the unseen.
CoRR, abs/1504.01227, 2015.

Orlitsky, Alon, Santhanam, Narayana, Viswanathan, Krish-
namurthy, and Zhang, Junan. On estimating the proba-
bility multiset part i: The pattern maximum likelihood
approach. Arxiv, 2017b.

Wu, Yihong and Yang, Pengkun. Minimax rates of en-
tropy estimation on large alphabets via best polynomial
approximation. IEEE Trans. Information Theory, 62(6):
3702–3720, 2016.

A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Zou, James, Valiant, Gregory, Valiant, Paul, Karczewski,
Konrad, Chan, Siu On, Samocha, Kaitlin, Lek, Monkol,
Sunyaev, Shamil, Daly, Mark, and MacArthur, Daniel G.
Quantifying unobserved protein-coding variants in hu-
man populations provides a roadmap for large-scale se-
quencing projects. Nature Communications, 7:13293 EP,
10, 2016.

white

A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

A. Entropy and Distance to Uniformity

A.1. Entropy

The known optimal estimators for entropy and distance to
uniformity both depend on the best polynomial approx-
imation of the corresponding functions and the splitting
trick (Wu & Yang, 2016; Jiao et al., 2015). Building on
their techniques, we show that a slight modiﬁcation of their
estimators satisfy conditions in Lemma 2. Both these func-
tions can be written as functionals of the form:

f (p) =

g(p(x)),

(cid:88)

x

where g(y) = −y log y for entropy and g(y) = (cid:12)
uniformity.

(cid:12)y − 1
k

(cid:12)
(cid:12) for

Both (Wu & Yang, 2016; Jiao et al., 2015) ﬁrst approximate
g(y) with PL,g(y) polynomial of some degree L. Clearly
a larger degree implies a smaller bias/approximation error,
but estimating a higher degree polynomial also implies a
larger statistical estimation error. Therefore, the approach
is the following:

PL,g(p(x)) = (cid:80)L

• For small values of p(x), we estimate the polynomial
i=1 bi · (p(x))i.
• For large values of p(x) we simply use the empirical

estimator for g(p(x)).

However, it is not a priori known which symbols have high
probability and which have low probability. Hence, they
both assume that they receive 2n samples from p. They
then divide them into two set of samples, X
n, and
x, and Nx be the number of appear-
X1, . . . , Xn. Let N
ances of symbol x in the ﬁrst and second half respectively.
They propose to use the estimator of the following form:

1, . . . , X

(cid:48)

(cid:48)

(cid:48)

(cid:40)

(cid:41)

(cid:41)

ˆg(X 2n

1 ) = max

min

gx, fmax

, 0

.

(cid:40)

(cid:88)

x

where fmax is the maximum value of the property f and

gx =






(cid:48)

GL,g(Nx), for N
(cid:1) , for N
g (cid:0) Nx
(cid:1) + gn, for N
g (cid:0) Nx

n

(cid:48)

(cid:48)

n

x < c2 log n, and Nx < c1 log n,

x < c2 log n, and Nx ≥ c1 log n,

x ≥ c2 log n,

i=1 biN i

where gn is the ﬁrst order bias correction term for g,
GL,g(Nx) = (cid:80)L
x/ni is the unbiased estimator for
PL,g, and c1 and c2 are two constants which we decide
later. We remark that unlike previous works, we set gx to 0
x to ensure that c∗ is bounded.
for some values of Nx and N (cid:48)
The following lemma bounds c∗ for any such estimator ˆg.
Lemma 5. For any estimator ˆg deﬁned as above, changing
any one of the values changes the estimator by at most
(cid:19)

(cid:19)

(cid:18)

Lg
n

, g

(cid:18) c1 log(n)
n

, gn

,

8 max

eL2/n max |bi|,

where Lg = n maxi∈N |g(i/n) − g((i − 1)/n)|.

The following lemma is adapted from Proposition 4 in (Wu
& Yang, 2016) where we make the constants explicit.

Lemma 6. Let gn = 1/(2n) and α > 0. Suppose c1 =
2c2, and c2 > 35, Further suppose that n3 (cid:16) 16c1
>
log k · log n. There exists a polynomial approximation of
log n
−y log y with degree L = 0.25α, over [0, c1
n ] such that
maxi |bi| ≤ nα/n and the bias of the entropy estimator is
(cid:17)
at most O

α2 + 1
c2

(cid:16)(cid:16) c1

(cid:17)

(cid:17)

.

α2 + 1
c2

+ 1
n3.9

k
n log n

Proof. Our estimator is similar to that of (Wu & Yang,
2016; Jiao et al., 2016) except for the case when N
x <
c2 log n, and Nx > c1 log n. For any p(x), and N
x and Nx
both distributed Bin(np(x)). By the Chernoff bounds for
binomial distributions, the probability of this event can be
bounded by,

(cid:48)

(cid:48)

(cid:16)

(cid:48)

max
p(x)

Pr

N

x < c2 log n, Nx > 2c2 log n

≤

(cid:17)

1
√
n0.1

2c2

≤

1
n4.9 .

Therefore, the additional bias the modiﬁcation introduces
is at most k log k/n4.9 which is smaller than the bias term
of (Wu & Yang, 2016; Jiao et al., 2016).

The largest coefﬁcient can be bounded by using that the
best polynomial approximation of degree L of x log x in
the interval [0, 1] has all coefﬁcients at most 23L. There-
fore, the largest change we have (after appropriately nor-
malizing) is the largest value of bi which is

23LeL2/n
n

.

For L = 0.25α log n, this is at most na
n .

The proof of Lemma 2 for entropy follows from the above
(cid:17)
lemma and Lemma 5 and by substituting n = O
.

(cid:16) k
log k

1
ε

A.2. Distance to Uniformity

We state the following result stated in (Jiao et al., 2016).

Lemma 7. Let c1 > 2c2, c2 = 35. There is an estimator
for distance to uniformity that changes by at most nα/n
when a sample is changed, and the bias of the estimator is
at most O( 1
α

(cid:113) c1 log n

k·n ).

Proof. Estimating the distance to uniformity has two re-
gions based on N (cid:48)

x and Nx.

Case 1: 1
tor deﬁned in the last section for g(x) = |x − 1/k|.

k < c2 log n/n.

In this case, we use the estima-

A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

k > c2 log n/n.

Case 2: 1
In this case, we have a slight
change to the conditions under which we use various esti-
mators.

(cid:113) c2 log n

kn , & (cid:12)

(cid:12)Nx − 1
k

(cid:12)
(cid:12) <

(cid:113) c1 log n
kn :

function,

the indicator
(cid:12)
(cid:111)
(cid:12)
(cid:12) > ε

where I
is
(cid:110)(cid:12)
(cid:12)f (˜pz) − ˆf (z)
I
(cid:12)
(cid:12)
(cid:12)
(cid:12)f (˜pz) − ˆf (z)
(cid:12)
(cid:12)
(cid:12) < ε. By an identical reasoning, since
(cid:12)
(cid:12)
(cid:12)f (p) − ˆf (z)
(cid:12)
(cid:12)
(cid:12) < ε. By the trian-
p(z) > δ/β, we have
gle inequality,

and therefore,
This implies that

= 0.

(cid:113) c2 log n

kn , & (cid:12)

(cid:12)Nx − 1
k

(cid:12)
(cid:12) <

(cid:113) c1 log n
kn :

|f (p) − f (˜pz)| ≤

(cid:12)
(cid:12)f (p) − ˆf (z)
(cid:12)

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)f (˜pz) − ˆf (z)
(cid:12)
(cid:12)
(cid:12) < 2ε.

• For

(cid:48)

(cid:12)
(cid:12)
(cid:12)N

(cid:12)
x − 1
(cid:12)
(cid:12) <
k
gx = GL,g(Nx),

• For

(cid:48)

(cid:12)
(cid:12)
(cid:12)N
gx = 0,

x − 1
k

(cid:12)
(cid:12)
(cid:12) <

• For

(cid:48)

(cid:12)
(cid:12)
(cid:12)N
gx = (cid:0) Nx

x − 1
k
(cid:1) .

n

(cid:12)
(cid:12)
(cid:12) ≥

(cid:113) c2 log n
kn

:

Thus if p(z) ≥ δ/β, then PML satisﬁes the required guar-
antee with zero probability of error, and any error occurs
only when p(z) < δ/β. We bound this probability as fol-
lows. When Z ∼ p,

Pr (p(Z) ≤ δ/β) ≤

p(z) ≤ δ · |Z| /β.

(cid:88)

z∈Z:p(z)<δ/β

The estimator proposed in (Jiao et al., 2016) is slightly dif-
ferent, assigning GL,g(Nx) for the ﬁrst two cases. We
design the second case to bound the maximum devia-
tion. The bias of their estimator was shown to be at most
O
, which can be shown by using Equation
Equation 7.2.2 of (Timan, 1963)

(cid:113) log n

(cid:16) 1
L

k·n log n

(cid:17)

E|x−τ |,L,[0,1] ≤ O

(6)

(cid:32) (cid:112)τ (1 − τ )
L

(cid:33)

.

By our choice of c1, c2, our modiﬁcation changes the bias
by at most 1/n4 < ε2.

To bound the largest deviation, we use the fact from Lemma
2 in (Cai et al., 2011) that the largest coefﬁcient of the best
degree-L polynomial approximation of |x| in [−1, 1] has
all coefﬁcients at most 23L. Similar argument as with en-
tropy yields that after appropriate normalization, the largest
difference in estimation will be at most nα/n.

The proof of Lemma 2 for entropy follows from the
above lemma and Lemma 5 and by substituting n =
O

(cid:17)
.

(cid:16) k
log k

1
ε2

B. Proof of Approximate ML Performance

Proof. We consider symbols such that p(z) ≥ δ/β and
p(z) < δ/β separately. For an z with p(z) ≥ δ/β, by the
deﬁnition of f (pZ ),

˜pz(z) ≥ pz (z)β ≥ p(z)β ≥ δ.

Applying (2) to ˜pz, we have for Z ∼ ˜pz,

δ > Pr

(cid:12)
(cid:16)(cid:12)
(cid:17)
(cid:12)f (˜pz) − ˆf (Z)
(cid:12)
(cid:12)
(cid:12) > ε
(cid:12)
(cid:110)(cid:12)
(cid:111)
(cid:12)f (˜pz) − ˆf (z)
≥ ˜pz(z) · I
(cid:12)
(cid:12)
(cid:12) > ε
(cid:12)
(cid:110)(cid:12)
(cid:111)
(cid:12)f (˜pz) − ˆf (z)
(cid:12)
(cid:12)
(cid:12) > ε

≥ δ · I

,

