Just Sort It! A Simple and Effective Approach to Active Preference Learning
Supplementary Material

Lucas Maystre 1 Matthias Grossglauser 1

The supplementary material consists of three parts. In Sec-
tion A, we present formal proofs for the results given in
Section 3 of the main text. In Section B we show that in the
Poisson model, Ω(n) comparison outcomes are necessary to
discriminate between two neighboring items. In Section C,
we present ﬁgures that complement the ones presented in
the experimental evaluation (Section 4 of the main text).

A Proofs

Section A.1 contains the proofs of Lemmas 2 and 3. Sec-
tion A.2 presents the proof for our result on the displacement
of the output of a single call to Quicksort (Theorem 1), and
Section A.3 that of our result on the displacement of the
Copeland aggregation of multiple outputs.

A.1 Lemmas 2 and 3

We start by brieﬂy presenting a result from graph theory
that will be useful in the proof of Lemma 2. A tournament
is a directed graph obtained by assigning a direction to
every edge of a complete graph. The score sequence of a
tournament is deﬁned as the nondecreasing sequence of the
vertices’ outdegrees. The following proposition is due to
Landau (1953).
Proposition 1. Let (s1, . . . , sn) with 0 ≤ s1 ≤ · · · ≤ sn
be the score sequence of a tournament on n vertices. Then,

k − 1
2

≤ sk ≤

n + k − 2
2

∀ k ∈ [n].

We use a tournament on n vertices to represent the outcome
of a comparison between each pair of items. In particular,
we represent the outcome i ≺ j by an edge (i, j). In this
case, the outdegree of a vertex i corresponds to the number
of items which “won” in a comparison against i. Note that
the comparison outcomes do not need to be transitive, i.e.,
the tournament can contain cycles.

1School of Computer and Communication Sciences, EPFL,
Lausanne, Switzerland. Correspondence to: Lucas Maystre <lu-
cas.maystre@epﬂ.ch>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

The proof of Lemma 2 is adapted from standard results on
Quicksort, see, e.g., Dubhashi & Panconesi (2009, Section
3.3.3). These results are based on the fact that it is likely
that the random choice of pivot leads to a well-balanced
partition into subsets L and R. In our setting, the comparison
outcomes do not need to be consistent with an ordering of
the items, therefore we cannot use the standard argument
based on the pivot’s rank. Instead, we use the tournament
representation of the comparison outcomes and analyze the
pivot’s out-degree (using Proposition 1) to ensure that the
partition is balanced often enough.

Proof of Lemma 2. We show that the maximum call depth
of Quicksort is at most ⌈48 log n⌉ with high probability. The
statement follows by noticing that at most n comparisons
are used at each level of the call tree.

By Lemma 1, Quicksort samples a comparison outcome
for each pair of items at most once. Therefore, we can rep-
resent these (a priori unobserved) pairwise outcomes as a
tournament T = ([n], A). At each step of the recursion, we
select a pivot p uniformly at random in the set V (line 3),
and compare it to the rest of the items in the set (line 5). Let
TV denote the subgraph of T induced by V . Given that the
comparison outcomes follow from the edges of the tourna-
ment, L is equal to the set of incoming neighbors of p in
TV . (Correspondingly, R is equal to the set of the outgoing
neighbors.) Hence, the outdegree of p in TV determines how
balanced the partition is. The probability that the outdegree
of p lies in the middle half of the score sequence is 1/2, and
if it does, Proposition 1 tells us that

|V | − 7
8

≤ outdeg(p) ≤

7|V | − 5
8

.

In this case, at the end of the partition |L| and |R| are of
size at most 7|V |/8, and in at most log8/7(n) ≤ 8 log n
such partitions we get to a subset of size one and match the
terminating case. Even though we do not select the pivot
in the middle half every time, it is unlikely that more than
c · 8 log n recursions are needed (for some small constant c)
to select the pivot in the middle range at least 8 log n times.
Let zd i.i.d ∼ Bern(1/2) be the indicator variable for the
event “the pivot is selected in the middle half at level of

A Simple and Effective Approach to Active Preference Learning

recursion d”. Using a Chernoff bound, we have

⌈48 log n⌉

P





Xd=1

zd ≤ 8 log n

≤

1
n2 ,





i.e., the depth of a leaf in the call tree is at most ⌈48 log n⌉
with probability at least 1 − 1/n2. As there are at most n
leaves in the tree, the maximum depth is bounded by the
same value with probability at least 1 − 1/n.

In order to prove Lemma 3, we introduce some additional
notation. For any σ ∈ Sn and V ⊆ [n], let σV : V →
{1, . . . , |V |} be the ordering induced by σ on V . We gener-
alize the deﬁnition of displacement as

∆V (σ, τ ) =

|σV (i) − τV (i)|.

Xi∈V

For conciseness, we use the shorthand ∆V (σ)
where id is the identity permutation.

.
= ∆V (σ, id),

Proof of Lemma 3. Denote by V the collection of working
sets that were used as input to one of the recursive calls to
Quicksort. For V ∈ V, let EV be the set of pairs sampled by
Quicksort to partition V and which results in an error. Note
that EV ∩ EV ′ = ∅ for V 6= V ′, and that
V EV = E.
We will show that for all V ∈ V,

∆V (σ) ≤ ∆L(σ) + ∆R(σ) + 2

S
|i − j|,

(1)

X(i,j)∈EV

where L and R are the two sets obtained at the end of the
partition operation. The lemma follows by taking V = [n]
and recursively bounding ∆L(σ) and ∆R(σ).

Consider the partition operation on V , with pivot p, resulting
in partitions L and R. Let ˜σ be the ordering on V that
a) ranks L at the bottom, p in the middle and R at the top,
and b) matches the identity permutation on L and R, i.e.,
∆L(˜σ) = ∆R(˜σ) = 0. In a sense, ˜σ is the ordering that
would be obtained if there were no further errors in the
remaining recursive calls. Using the triangle inequality, we
have that

∆V (σ) ≤ ∆V (σ, ˜σ) + ∆V (˜σ).

By deﬁnition of ˜σ, we have that

∆V (σ, ˜σ) = ∆L(σ, ˜σ) + ∆R(σ, ˜σ)

= ∆L(σ) + ∆R(σ),

(2)

(3)

where the ﬁrst equality follows from a), and the second
follows from b).
Finally, we bound ∆V (˜σ). Let E−
p}, and similarly E+

V = {(p, i) ∈ EV : i <
V = {(p, i) ∈ EV : i > p}. Without

t = 0

∆V (˜σ)

t = 1

∆V (˜σ)

t = 2

∆V (˜σ)

t = 3

∆V (˜σ)

1 2 3 4 5 6 7 8 9

0

0

0

0

0

0

0

0

0 → 0

1 2 4 6 5 3 7 8 9

0

0

1

2

0

3

0

0

0 → 6

1 4 6 8 5 2 3 7 9

0

2

3

4

0

4

4

1

0 → 18

1 4 6 8 9 5 2 3 7

0

2

3

4

4

1

5

5

1 → 25

Figure 4. Illustration of the decomposition of ∆V (˜σ) into contribu-
tions of individual errors over a sequence of steps. In this example,
V = {1, . . . , 9}, p = 5 and there are ﬁve errors. At step t = 1, we
process the errors (5, 3) and (5, 6); at step t = 2, we process the
errors (5, 2) and (5, 8), and ﬁnally, at step t = 3, we process the
error (5, 9). The shifts caused by an error are highlighted in red and
green. In this example, ∆V (˜σ) = 25 < 2 P(i,j)∈EV |i−j| = 26.

V | ≤ |E+

loss of generality, we can assume that V consists of con-
.
= |E−
V |. We proceed
secutive integers, and that κ
as follows: starting from the ranking idV , we progressively
incorporate errors into the ranking, ending with ˜σ once all
errors have been treated. To understand the impact of each
error on ∆V (˜σ), we look at errors in the following speciﬁc
sequence.

1. At steps t = 1, . . . , κ, we consider the t-th “smallest”
errors in E−
V . That is, we process (p, i) ∈
E−
V such that |p − i| and |p − i′|,
respectively, are smallest among errors not yet treated.

V and E+
V and (p, i′) ∈ E+

2. At steps t = κ+1, . . . , |E+

V |, we process the remaining
V , once again in increasing order of distance

errors in E+
to p.

Figure 4 illustrates the state of the ranking at different steps
on a concrete example. We start with the ﬁrst case, i.e.,
t ≤ κ. The effect of the errors (p, i) and (p, i′) on ∆V (˜σ)
is as follows.

• All items j < i and j > i′ are not affected by the two

errors: their position remains the same.

• The position of the pivot p remains the same, as the

two errors balance out.

• Item i is shifted by |p − i| + 1 positions to the right,
just right of p. Similary, item i′ is shifted by |p − i′| + 1
positions to the left, just left of p.

• The |p − i| − 1 items that are between p (excluded)
and i are shifted by 1 position to the left. Similarly, the

A Simple and Effective Approach to Active Preference Learning

|p − i′| − 1 items that are between p and i′ are shifted
by 1 position to the right.

Hence, the two errors contribute 2(|p − i| + |p − i′|) towards
∆V (˜σ). Now consider the second case, when t > κ. The
effect of an error (p, i) is as follows.

Next, we bound their covariance. Note that the random vari-
ables {zij} are in general not unconditionally independent.
They become independent only when conditioned on θ.
Lemma 5. For any 1 ≤ i < j ≤ n and any 1 ≤ u < v ≤ n,
let A = {i .. j −1} and B = {u .. v−1}.

• All items j > i and all the items on the left of p are not
affected by the error: their position remains the same.

• The (at most) |p−i| items that are between p (included)

and i are shifted by 1 position to the right.

• Item i is shifted by at most |p − i| positions to the left,

just left of p.

As a result, the error contributes at most 2|p − i| to the
displacement. Adding up the contributions of all the errors,
it follows that

∆V (˜σ) ≤ 2

|i − j|.

(4)

X(i,j)∈EV

Combining (3) and (4) using (2) we obtain (1), which con-
cludes the proof.

A.2 Theorem 1

From now on, we focus on parameters drawn from a Poisson
process of rate λ, as described in (1) in the main text. We
consider a worst-case scenario and assume that Quicksort
samples a comparison outcome for every pair of items. Let
zij be the indicator random variable of the event “the com-
parison between i and j resulted in an error”. By Lemma 3,
we have

∆(σ) ≤ 2

|i − j|zij

(5)

i<j
X
In the following, we will bound some of the statistical prop-
erties of the random variables {zij}. We start with a lemma
that bounds their mean.
Lemma 4. For any 1 ≤ i < j ≤ n,

E [zij] ≤

j−i

.

λ
λ + 1

(cid:18)

(cid:19)

Proof. Let dij = θi − θj be the (random) distance be-
tween items i and j. This distance is a sum of k = j − i
independent exponential random variables, and therefore
dij ∼ Gamma(k, λ). The comparison outcome is gener-
ated as per the BT model; conditioned on the distance dij,
the random variable zij is a Bernoulli trial with probability
[1 + exp(dij)]−1. Therefore, we have that

E [zij] ≤ E [exp(−dij)] =

k

λ
λ + 1

(cid:18)

(cid:19)

Cov [zij, zuv] ≤

0

(cid:18)

(cid:18)






j−i

λ
λ + 1

λ + 1
λ + 2

(cid:19)

(cid:19)

j−i+v−u

if A ∩ B = ∅,

if A = B,

otherwise.

Proof. If A and B are disjoint, the distances dij and duv are
independent random variables. Conditioned on the distances,
the comparison outcomes are independent Bernoulli trials,
and we conclude that zij and zuv are independent. In the two
remaining cases, we bound E [zijzuv] ≥ Cov [zij, zuv]. If
A = B, then zij = zuv and we have

E [zijzuv] = E

z2
ij

= E [zij]

(cid:3)

(cid:2)
and we apply Lemma 4. Finally, if A and B are neither
equivalent nor disjoint, the two comparison outcomes are
independent Bernoulli trials conditioned on the distances
dij and duv, but the distances are not independent. Consider
the case where i < u < j < v. Even though dij and duv
are dependent, the distances diu, duj, djv are independent
Gamma random variables of rate λ and shape u − i, j − u
and v − j, respectively, and

E [zijzuv] ≤ E [exp{−(diu + duj) − (duj + djv)}]
u−i

j−u

v−j

λ
λ + 2

λ
λ + 1

(cid:19)

(cid:18)
j−i+v−u

(cid:19)

(cid:18)

(cid:19)

λ
λ + 1

λ + 1
λ + 2

=

≤

(cid:18)

(cid:18)

(cid:19)

The other cases are treated analogously.

Lemmas 4 and 5 will be useful in proving the ﬁrst part
of Theorem 1. For the second part, we need a result from
Ailon (2008), which characterizes the pairwise marginals
of the distribution over rankings induced by Quicksort with
comparisons sampled from a BT model.

Theorem 3 (Ailon, 2008, Theorem 4.1). Let σ be the out-
put of Quicksort using comparison outcomes sampled from
BT(θ). Then, for any i, j ∈ [n],

P [σ(i) < σ(j) | θ] = p(i ≺ j | θ)

Note that the result is non-trivial as i and j might not have
been directly compared to each other: their relative position
might have been deduced by transitivity from other compar-
ison outcomes. We are now ready to prove Theorem 1.

A Simple and Effective Approach to Active Preference Learning

= 2nλ(λ + 1).

A.3 Theorem 2

Proof of Theorem 1. We begin with the ﬁrst part of the the-
orem, which bounds the displacement ∆(σ). For clarity
of exposition, we use the notation zi→k instead of zij if
j = i + k. Using (5) and Lemma 4, we can bound the
expected displacement as

n−1

n−i

E [∆] ≤

2kE [zi→k]

i=1
Xk=1
X
∞
2k

≤ n

Xk=1

k

λ
λ + 1

(cid:18)

(cid:19)

In a similar way, using Lemma 5, we can bound the variance
of the displacement as

Var [∆] ≤

4k2Var [zi→k]

n−1

n−i

i=1
X

Xk=1
n−1

+ 2

n−i

i+k

n−u

2k

λ
λ + 1

i=1
X
∞

Xk=1

u=i+1
X

Xℓ=1

k

≤ n

4k2

Xk=1
+ 2n

∞

(cid:18)

2k2

(cid:18)
Xk=1
≤ 1500n(λ5 + 1).

(cid:19)
λ + 1
λ + 2

2ℓCov [zi→k, zu→ℓ]

k

∞

·

2ℓ

(cid:19)

Xℓ=1

(cid:18)

λ + 1
λ + 2

ℓ

(cid:19)

Combining the bounds for the mean and the variance with
Chebyshev’s inequality, we have that

P

∆(σ) ≥ 50n(λ2 + 1)

≤ λ/n,

which concludes the proof of the ﬁrst part of the claim.

(cid:2)

(cid:3)

The second part of the theorem bounds the maximum dis-
placement for any single item. We start by showing that
with high probability, there is no pair of items separated by
at least O(λ log n) positions that is “ﬂipped” in the output
of Quicksort. Let i and j be two items such that i < j and
let k = |i − j|. Then dij ∼ Gamma(k, λ), and using a
Chernoff bound we obtain

P [dij ≤ k/(eλ)] ≤ exp(−k/e).

If k ≥ 3(λ + 1)e log n, we ﬁnd that

P [dij ≤ k/(eλ)] ≤ P [dij ≤ 3 log n] ≤ n−3.

(6)

Using the fact that the pairwise marginals of Quicksort
match the pairwise comparison outcome probabilities (The-
orem 3), we ﬁnd

P [σ(j) < σ(i)] = p(j ≺ i)

≤ exp(−3 log n) = n−3.

(7)

(cid:1)

Combining (6) and (7), and using a union bound over the
n
pairs, we see that with probability 1 − 1/n there is no
k
pairs of items (i, j) separated by at least 3(λ + 1)e log n
(cid:0)
position with i < j but σ(j) < σ(i). Finally, suppose that
there is an i such |σ(i) − i| = k. Without loss of generality,
we can assume that i < σ(i). This means that there are k
items larger than i that are on the left of i in σ. In particular,
there is an item j > i such that |i − j| ≥ k and σ(j) < σ(i).
This concludes the proof.

In order to prove Theorem 2, we ﬁrst need a basic result
on the order statistics of exponential random variables. Let
x1, . . . , xn, be i.i.d. exponential random variables of rate λ.
Let x(1), . . . , x(n) be their order statistics, i.e., the random
variables arranged in increasing order. Then,

x(i) =

i

j=1
X

1
n − j + 1

yj,

(8)

where y1, . . . , yn are i.i.d. exponential random variables of
rate λ (see, e.g., Arnold et al., 2008, Section 4.6).

Proof of Theorem 2. We consider the order statistics of the
n − 1 i.i.d. exponential random variables x1, . . . , xn−1
which deﬁne the distances between neighboring items. Let
ˆn = ⌈n/ log2 n⌉, and denote by B ⊂ [n] the set of items
at both ends of x(1), . . . , x(ˆn−1). These “bad” items are
close to their nearest neighbor, and we simply invoke Theo-
rem 1 to claim that each of these items is shifted by at most
O(λ log n) positions with high probability. Consider now
the “good” items, i.e., those in G = [n] \ B. Using (8) and
for n large enough,

P

x(ˆn) ≤ 1/(eλ log2 n)

(cid:2)

≤ P

(cid:3)

yj/n ≤ 1/(eλ log2 n)

ˆn





j=1
X

≤ exp(−ˆn/e) ≤ 1/n.





The second-to-last inequality follows from a Chernoff bound
similar to that used in the proof of Theorem 1. Therefore,
with high probability all items in G are at distance larger
than c/(λ log2 n) from their nearest neighbor.
We will now show that after m = O(λ2 log5 n) runs of
Quicksort, ˆσ(i) = i with high probability for all i ∈ G.
Let i ∈ G, j ∈ [n] be a pair of items, and without loss
of generality assume that i < j. Let tk be the indicator
random variable for the event “σ(i) < σ(j) in the k-th
run of Quicksort”, and let p = P [tk = 1]. Then, using

A Simple and Effective Approach to Active Preference Learning

Theorem 3,

×106

4.5
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0

t
n
e
m
e
c
a
l
p
s
i
D

p −

1
2

1 − exp(−dij)
2[1 + exp(−dij)]

= p(i ≺ j) −

=

1
2
1 − exp[−1/(eλ log2 n)]
4

≥

≥

1
8eλ log2 n

with high probability. In the last inequality, we used the
fact that 1 − e−x ≥ x/2 for x ∈ [0, 1]. The random vari-
ables t1, . . . , tm are independent Bernoulli trials, and using
a Chernoff bound we obtain

P [ˆσ(j) < ˆσ(i)] = P

tk ≤ n/2

m

"

Xk=1

≤ exp[−2m(p − 1/2)2] ≤ exp

−

#

(cid:20)

m
32e2λ2 log4 n

.

(cid:21)

With m = 96e2λ2 log5 n, we have P [ˆσ(j) < ˆσ(i)] ≤ n−3,
and using a union bound we see that with probability 1 −
1/n we have ˆσ(i) = i for all i ∈ G. Therefore, the total
displacement is

∆(ˆσ) =

|ˆσ(i) − i| ≤ |B| · 3(λ + 1)e log n

Xi∈B

= O(λn/ log n).

This concludes the proof.

Quicksort
uncertainty
random
Mergeort

0.1

0.2

0.3

0.5

0.7
0.4
Number of comparisons

0.6

0.8

0.9

1.0

×106

Figure 5. Results on the GIFGIF dataset. The experiment is re-
peated 10 times, and we report the mean and the standard deviation.
The variant of uncertainty sampling performs extremely poorly.

the correct decision is

m

P

"

i=1
X

zi ≤ m/2

≤

P [zi = 0]m

m/2

m
k

#

Xk=1 (cid:18)

(cid:19)

m/2

· 2−m

m
k

Xk=1 (cid:18)

(cid:19)

≤ exp

m
2λn − 1

(cid:21)

(cid:20)

=

exp

1
2

m
2λn − 1

.

(cid:21)

(cid:20)

Therefore, if m = o(λn) the probability of making a mis-
take is bounded from below by a positive constant.

B Discriminating the Closest Items

C Additional Figures

The distance between the two closest items is dmin =
mini|θi+i − θi| = mini xi, i.e., the minimum of n − 1
independent exponential random variables of rate λ. There-
fore, dmin ∼ Exp((n−1)λ), and for n ≥ 2 with probability
at least 1 − e−1/2 ≈ 0.39 we have dmin ≤ (λn)−1. Sup-
pose that we compare the two closest items m times, and
let zi be the indicator random variable for the event “the
outcome of the i-th comparison is incorrect”. Assuming that
dmin ≤ (λn)−1 and that λn ≥ 1/2,

P [zi = 0] ≤

≤

1
2 − 1/(λn)

1
1 + exp[−1/(λn)]
1
2
1
2

1
2λn − 1

1
2λn − 1

exp

1 +

(cid:18)

·

,

(cid:20)

(cid:21)

(cid:19)

=

≤

In this section, we present a few additional ﬁgures that
complement the ones presented in Section 4 of the main
text.

Figure 5 presents the results on the GIFGIF dataset including
a variant of uncertainty sampling. This variant samples, at
each iteration, n − 1 comparisons consisting of adjacent
pairs in the ranking ˆθ. This strategy performs surprisingly
poorly.

Figure 6 presents results on synthetic datasets with n = 200
and λ ∈ {1, 2, 5, 10}. For the reader’s convenience, we
plot every graph on both a linear and a logarithmic scale.
Unsurprisingly, the gains of adaptive sampling are greater
when the noise is smaller.

References

where we used the inequality ex ≥ 1+x twice. Given the m
comparison outcomes, we use a majority vote to decide the
relative order of the two items. The probability of making

Ailon, N. Reconciling Real Scores with Binary Compar-
In Ad-
isons: A Uniﬁed Logistic Model for Ranking.
vances in Neural Information Processing Systems 21,
Vancouver, BC, Canada, 2008.

A Simple and Effective Approach to Active Preference Learning

Arnold, B. C., Balakrishnan, N., and Nagaraja, H. N. A First

Course in Order Statistics. SIAM, 2008.

Dubhashi, D. P. and Panconesi, A. Concentration of Mea-
sure for the Analysis of Randomized Algorithms. Cam-
bridge University Press, 2009.

Landau, H. G. On Dominance Relations and the Struc-
ture of Animal Societies: III The Condition for a Score
Structure. Bulletin of Mathematical Biophysics, 15(2):
143–148, 1953.

A Simple and Effective Approach to Active Preference Learning

λ = 1λ = 1λ = 1λ = 1λ = 1λ = 1

λ = 1, log scale
λ = 1, log scale
λ = 1, log scale
λ = 1, log scale
λ = 1, log scale
λ = 1, log scale

uncertainty
entropy
KL-div
Mergesort
Quicksort
random

uncertainty
entropy
KL-div
Mergesort
Quicksort
random

uncertainty
entropy
KL-div
Mergesort
Quicksort
random

uncertainty
entropy
KL-div
Mergesort
Quicksort
random

104

103

102

104

103

102

104

103

104

103

λ = 2λ = 2λ = 2λ = 2λ = 2λ = 2

λ = 2, log scale
λ = 2, log scale
λ = 2, log scale
λ = 2, log scale
λ = 2, log scale
λ = 2, log scale

λ = 5λ = 5λ = 5λ = 5λ = 5λ = 5

λ = 5, log scale
λ = 5, log scale
λ = 5, log scale
λ = 5, log scale
λ = 5, log scale
λ = 5, log scale

λ = 10
λ = 10
λ = 10
λ = 10
λ = 10
λ = 10

λ = 10, log scale
λ = 10, log scale
λ = 10, log scale
λ = 10, log scale
λ = 10, log scale
λ = 10, log scale

t
n
e
m
e
c
a
l
p
s
i
D

t
n
e
m
e
c
a
l
p
s
i
D

t
n
e
m
e
c
a
l
p
s
i
D

t
n
e
m
e
c
a
l
p
s
i
D

8000

7000

6000

5000

4000

3000

2000

1000

0

8000

7000

6000

5000

4000

3000

2000

1000

0

8000

7000

6000

5000

4000

3000

2000

1000

0

8000

7000

6000

5000

4000

3000

2000

1000

0

0

500

1000

3000

3500

4000

0

500

1000

3000

3500

4000

1500

2000
Number of comparisons c

2500

1500

2000
Number of comparisons c

2500

Figure 6. Results on synthetic datasets for n = 200 and increasing values of λ. Every experiment is repeated 10 times, and we report the
mean and the standard deviation.

