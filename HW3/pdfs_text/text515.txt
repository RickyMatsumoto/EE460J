Asymmetric Tri-training for Unsupervised Domain Adaptation

Kuniaki Saito 1 Yoshitaka Ushiku 1 Tatsuya Harada 1 2

Abstract

dataset (Girshick et al., 2014; Long et al., 2015a).

It is important to apply models trained on a large
number of labeled samples to different domains
because collecting many labeled samples in var-
ious domains is expensive. To learn discrimi-
native representations for the target domain, we
assume that artiﬁcially labeling the target sam-
ples can result in a good representation. Tri-
training leverages three classiﬁers equally to pro-
vide pseudo-labels to unlabeled samples; how-
ever, the method does not assume labeling sam-
ples generated from a different domain. In this
paper, we propose the use of an asymmetric tri-
training method for unsupervised domain adap-
tation, where we assign pseudo-labels to unla-
beled samples and train the neural networks as
In our work, we use
if they are true labels.
three networks asymmetrically, and by asymmet-
ric, we mean that two networks are used to la-
bel unlabeled target samples, and one network
is trained by the pseudo-labeled samples to ob-
tain target-discriminative representations. Our
proposed method was shown to achieve a state-
of-the-art performance on the benchmark digit
recognition datasets for domain adaptation.

1. Inroduction

in-
With the development of deep neural networks,
(CNN)
cluding deep convolutional neural networks
(Krizhevsky et al., 2012), the ability to recognize images
and languages has improved dramatically. Training deep-
layered networks using a large number of labeled samples
enables us to correctly categorize samples in diverse do-
mains. In addition, the transfer learning of a CNN has been
utilized in many studies. For object detection or segmenta-
tion, we can transfer the knowledge of a CNN trained using
a large-scale dataset by ﬁne-tuning it on a relatively small

1The University of Tokyo, Tokyo,

Japan 2RIKEN,
Japan. Correspondence to: Kuniaki Saito <k-saito@mi.t.u-
tokyo.ac.jp>, Yoshitaka Ushiku <ushiku@mi.t.u-tokyo.ac.jp>,
Tatsuya Harada <harada@mi.t.u-tokyo.ac.jp>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

One of the problems inherent to neural networks is that, al-
though such networks perform well on samples generated
from the same distribution as the training samples, they
may ﬁnd it difﬁcult to correctly recognize samples from
different distributions at the test time. An example of this
is images collected from the Internet, which may come in
abundance and are fully labeled. Such images have a distri-
bution that differs from images taken from a camera. Thus,
a classiﬁer that performs well on various domains is im-
portant for practical use. To realize such a classiﬁer, it is
necessary to learn domain-invariantly discriminative repre-
sentations. However, acquiring such representations is not
easy because it is often difﬁcult to collect a large number
of labeled samples, and because samples from different do-
mains have domain-speciﬁc characteristics.

In unsupervised domain adaptation, we try to train a clas-
siﬁer that works well on a target domain under the condi-
tion that we are provided labeled source samples and un-
labeled target samples during training. Most of the pre-
viously developed deep domain adaptation methods oper-
ate mainly under the assumption that the adaptation can be
realized by matching the distribution of features from dif-
ferent domains. These methods have been aimed at ob-
taining domain-invariant features by minimizing the di-
vergence between domains, as well as a category loss on
the source domain (Ganin & Lempitsky, 2014; Long et al.,
2015b; 2016). However, as shown in (Ben-David et al.,
2010), if a classiﬁer that works well on both the source and
the target domains does not exist, we theoretically cannot
expect a discriminative classiﬁer to be applicable to the tar-
get domain. That is, even if the distributions are matched
with the non-discriminative representations, the classiﬁer
may not work well on the target domain. Because the di-
rect learning discriminative representations for the target
domain, in the absence of target labels, is considered very
difﬁcult, we propose assigning pseudo-labels to the target
samples and training the target-speciﬁc networks as if they
were true labels.

Co-training and tri-training (Zhou & Li, 2005) leverage
multiple classiﬁers to artiﬁcially label unlabeled samples
and retrain the classiﬁers. However, such methods do not
assume labeling samples from different domains. Because

Asymmetric Tri-training for Unsupervised Domain Adaptation

Figure1. Outline of our model. We assign pseudo-labels to unla-
beled target samples based on the predictions from two classiﬁers
trained on the source samples.

our goal is to classify unlabeled target samples that have
different characteristics from labeled source samples, we
propose the use of asymmetric tri-training for unsupervised
domain adaptation. By asymmetric, we mean that we as-
sign different roles to three different classiﬁers.

In this paper, we propose a novel tri-training method for
unsupervised domain adaptation, where we assign pseudo-
labels to unlabeled samples, and train the neural networks
utilizing these samples. As described in Fig. 1, two net-
works are used to label unlabeled target samples, and the
remaining network is trained using the pseudo-labeled tar-
get samples. We evaluated our method using digit clas-
siﬁcation tasks, trafﬁc sign classiﬁcation tasks, and senti-
ment analysis tasks using the Amazon Review dataset, and
demonstrated its state-of-the-art performance for nearly all
of the conducted experiments. In particular, for the adapta-
tion scenario, MNIST!SVHN, our method outperformed
other methods by more than 10%.
2. Related Work
A number of previous methods have attempted to realize
adaptation by measuring the divergence between different
domains (Ganin & Lempitsky, 2014; Long et al., 2015b;
Li et al., 2016). Such methods are based on the theory pro-
posed in (Ben-David et al., 2010), which states that the ex-
pected loss for a target domain is bounded by three terms:
(i) the expected loss for the source domain, (ii) the domain
divergence between the source and target, and (iii) the min-
imum value of a shared expected loss. A shared expected
loss indicates the sum of the loss on the source and target
domains. Because the third term, which is usually con-
sidered to be very low, cannot be evaluated when labeled
target samples are absent, most methods attempt to mini-
mize the ﬁrst and second terms. With regard to the train-
ing of deep architectures, the maximum mean discrepancy
(MMD), or the loss of a domain classiﬁer network, is uti-
lized to measure the divergence corresponding to the sec-
ond term (Gretton et al., 2012; Ganin & Lempitsky, 2014;
Long et al., 2015b; 2016; Bousmalis et al., 2016). How-
ever, the third term is very important in training a CNN,
which simultaneously extracts and recognizes the repre-
sentations. The third term can easily become large when
the representations are not discriminative for the target do-

main. Therefore, we focus on how to learn the target-
discriminative representations to consider the third term.
In (Long et al., 2016), the focus was on this point, and a
target-speciﬁc classiﬁer was constructed using a residual
network structure. Differing from their method, we con-
structed a target-speciﬁc network by providing artiﬁcially
labeled target samples.

Several transductive methods use a similarity of features
to provide labels for unlabeled samples (Rohrbach et al.,
2013; Khamis & Lampert, 2014). For unsupervised do-
main adaptation, in (Sener et al., 2016), a method was pro-
posed to learn the labeling metrics by utilizing the k-nearest
neighbors between unlabeled target samples and labeled
source samples. In contrast to this method, our method ex-
plicitly and simply backpropagates the category loss for the
target samples based on pseudo-labeled samples.

Many methods have proposed giving pseudo-labels to un-
labeled samples by utilizing the predictions of a classiﬁer
and retraining it, including pseudo-labeled samples, a pro-
cess called self-training. The underlying assumption of
self-training is that one ’s own high-conﬁdence predictions
are correct (Zhu, 2005). As the predictions are mostly
correct, utilizing samples with high conﬁdence will fur-
ther improve the performance of the classiﬁer. Co-training
utilizes two classiﬁers, which have different views on one
sample, to provide pseudo-labels (Blum & Mitchell, 1998;
Tanha et al., 2011). The unlabeled samples are then added
to the training set if at least one classiﬁer is conﬁdent
regarding the predictions. The generalization capability
of co-training is theoretically ensured (Balcan et al., 2004;
Dasgupta et al., 2001) under certain assumptions, and ap-
plied to various tasks (Wan, 2009; Levin et al., 2003). In
(Chen et al., 2011), the idea of co-training was incorpo-
rated into domain adaptation. Similar to co-training, tri-
training uses the output of three different classiﬁers to
provide pseudo-labels to unlabeled samples (Zhou & Li,
2005). Tri-training does not require partitioning features
into different views; instead, tri-training initializes each
classiﬁer in a different manner. However, tri-training does
not assume that the unlabeled samples follow different dis-
tributions from those the labeled ones are generated from.
Hence, we developed a tri-training method for domain
adaptation that utilizes three classiﬁers asymmetrically.

In (Lee, 2013), the effects of pseudo-labels on a neural net-
work were investigated. The authors argued that the effect
of training a classiﬁer using pseudo-labels is equivalent to
entropy regularization, thus leading to a low-density sep-
aration between classes. In our experiments, we observed
that the target samples are separated in hidden features.

3. Method
In this section, we provide details of the proposed model
for domain adaptation. We aim to construct a target-

!"#$$%&%’()*!"#$$%&%’(+*,#-’"’./$01(2’/$#34"’$*!"#$$%&%’()*!"#$$%&%’(+*56"#-’"’./7#(8’7/$#34"’$*!"#$$/9*!"#$$/9*!"#$$%&%’(/7*:$’1.0;"#-’"’./7#(8’7/$#34"’$*Asymmetric Tri-training for Unsupervised Domain Adaptation

weights of F1 and F2, which are ﬁrst applied to the feature
F (xi). With this constraint, each network will learn from
different features. The objective for the learning of F1 and
F2 is deﬁned as

E((cid:18)F ; (cid:18)F1 ; (cid:18)F2 ) =

n∑
[
Ly(F1 ◦ F (xi)); yi)

1
n

i=1

]
+ Ly(F2 ◦ (F (xi)); yi)

+ (cid:21)jW1

T W2j
(1)

where Ly denotes the standard softmax cross-entropy loss
function. We determined the trade-off parameter (cid:21) based
on a validation split.

3.2. Learning Procedure and Labeling Method

samples will provide

Pseudo-labeled target
target-
discriminative information to the network. However,
because they certainly contain false labels, we have to pick
up reliable pseudo-labels, which our labeling and learning
method is aimed at realizing.

The entire training procedure of the network is shown in
Algorithm 1. First, we train the entire network using the
source training set Xs. Here, F1 and F2 are optimized
through Eq. (1), and Ft is trained based on a standard cat-
egory loss. After training on Xs, to provide pseudo-labels,
we use the predictions of F1 and F2, namely, ^y1; ^y2 ob-
tained from xk. When C1 and C2 denote the class that has
the maximum predicted probability for ^y1; ^y2, we assign a
pseudo-label to xk if the following two conditions are sat-
isﬁed. First, we require C1 = C2 to provide pseudo-labels,
which means the two different classiﬁers agree with the
prediction. The second requirement is that the maximiz-
ing probability of ^y1 or ^y2 exceed the threshold parameter,
which we set as 0.9 or 0.95 in the experiment. We suppose
that unless one of the two classiﬁers is conﬁdent of the pre-
diction, the prediction is not reliable. If the two require-
ments are satisﬁed,
l.
To prevent an overﬁtting to the pseudo-labels, we resample
the candidate for labeling the samples in each step. We set
the number of initial candidates Ninit to 5,000. We grad-
ually increase the number of candidates Nt = K=20 (cid:3) n,
where n denotes the number of all target samples, and K
denotes the number of steps; in addition, we set the maxi-
mum number of pseudo-labeled candidates to 40,000. We
set K to 30 in the experiments. After the pseudo-labeled
training set Xt
l is composed, F; F1, and F2 are updated
based on the objective in Eq. (1) for the labeled training
set L = Xs [ Xt
l. Then, F and Ft are simply optimized
based on the category loss for Xt

xk; yk = C1 = C2

is added to Xt

(

)

l.

Discriminative representations will be learned by con-
structing a target-speciﬁc network trained only on the target
samples. However, if only noisy pseudo-labeled samples
are used for the training, the network may not learn any

Figure2. The proposed method includes a shared feature extrac-
tor (F ), classiﬁers for labeled samples (F1 and F2) that learn from
labeled source samples, and newly labeled target samples. In ad-
dition, a target-speciﬁc classiﬁer (Ft) learns from pseudo-labeled
target samples. Our method ﬁrst trains networks from only la-
beled source samples, and then labels the target samples based on
the output of F1 and F2. We train all architectures using these
samples under the assumption that they are correctly labeled.

speciﬁc network by utilizing pseudo-labeled target sam-
ples. Simultaneously, we expect two labeling networks to
acquire target-discriminative representations and gradually
increase the accuracy on the target domain.

Our proposed network structure is shown in Fig. 2. Here, F
denotes a network that outputs shared features from among
three different networks, and F1 and F2 classify the fea-
tures generated from F . Their predictions are utilized to
provide pseudo-labels. The classiﬁer Ft classiﬁes features
generated from F , which is a target-speciﬁc network. Here,
F1 and F2 learn from the source and pseudo-labeled target
samples, and Ft learns only from the pseudo-labeled tar-
get samples. The shared network F learns from all gradi-
ents from F1; F2, and Ft. Without such a shared network,
another option for the network architecture is training the
three networks separately, although this is inefﬁcient in
terms of training and implementation. Furthermore, by
building a shared network, F , F1, and F2 can also harness
the target-discriminative representations learned through
the feedback from Ft.

The set of source samples is deﬁned as
}mt
Xs, the unlabeled target set is
}nt
pseudo-labeled target set is

(xi)
(xi; ^yi)

{

{

i=1

i=1

3.1. Loss for Multiview Features Network

{

}ms

i=1

(cid:24)
(xi; yi)
(cid:24) Xt, and the
(cid:24) Xt

l.

In existing studies (Chen et al., 2011) on co-training for do-
main adaptation, the given features are divided into sepa-
rate parts, and considered to be different views.

Because we aim to label the target samples with high ac-
curacy, we expect F1 and F2 to classify the samples based
on different viewpoints. Therefore, we make a constraint
for the weights of F1 and F2 to make their inputs differ-
T W2j to the cost
ent from each other. We add the term jW1
function, where W1 and W2 denote fully connected layer

!"!!#!!$!%&!$’!()*+,*+,(--.-/01234-/56!+4/,*+-.-!/41708+594+47-$52:4$-/56!+4/,"#$%&!;!<"!<#!<$!=>!%&!#’!?>!%&!"’!?>!%&!#’!=>!%&!"’!=>!’(!’(!=-.-@/41708+594+-A02-$52:4$-/56!+4!?-.-%594+-A02-/01234-/56!+4!<!B9C43$DE4-A02-35$4:02?-+0//!()*+,<"!’<#-.-%594+DF:-F4$G02H/!<$-.-*52:4$-/!43DID3-F4$G02H!<-.-(J5247-F4$G02H!Asymmetric Tri-training for Unsupervised Domain Adaptation

Algorithm 1 iter denotes the iteration of the training.
The function Labeling indicates the labeling method. We
assign pseudo-labels to samples when the predictions of
F1 and F2 agree, and at least one of them is conﬁdent of
their predictions.

}m
i=1, Xt =

{

(xj)

}n
j=1

{

(xi; ti)

Input: data
Xs =
Xt
for j = 1 to iter do

l = ∅

Train F; F1; F2; Ft with a mini-batch from the training
set S
end for
Nt = Ninit
Xt
L = Xs [ Xt
l
for K steps do

l = Labeling(F; F1; F2,Xt, Nt)

for j = 1 to iter do

Train F; F1; F2 with mini-batch from training set L
Train F; Ft with mini-batch from training set Xt

l

end for
Xt
Xt
L = Xs [ Xt

l

l = ∅, Nt = K=20 (cid:3) n
l = Labeling(F; F1; F2,Xt, Nt)

end for

useful representations. We then use both the source sam-
ples and pseudo-labeled samples for the training of F; F1,
and F2 to ensure the accuracy. In addition, as the learn-
ing proceeds, F will learn target-discriminative representa-
tions, resulting in an improvement in accuracy for F1 and
F2. This cycle will gradually enhance the accuracy in the
target domain.

3.3. Batch Normalization for Domain Adaptation

Batch normalization (BN) (Ioffe & Szegedy, 2015), which
whitens the output of the hidden layer in a CNN, is an ef-
fective technique for accelerating the training speed and
enhancing the accuracy of the model. In addition, in do-
main adaptation, whitening the output of the hidden layer
is effective in improving the performance, and makes the
distribution in different domains similar (Sun et al., 2016;
Li et al., 2016).

The input samples of F1 and F2 include both pseudo-
labeled target samples and source samples. Introducing BN
will be useful for matching the distribution and improving
the performance. We add BN layers to F; F1 and F2, which
we detail in our supplementary material.

4. Analysis
In this section, we provide a theoretical analysis to our ap-
proach. First, we provide insight into existing theory, and
then introduce a simple expansion of the theory related to

our method. The distribution of the source samples is de-
noted as S; that of the target samples, as T ; and that of the
pseudo-labeled target samples, as Tl.

In (Ben-David et al., 2010), an equation was introduced
showing that the upper bound of the expected error in the
target domain depends on three terms, which include the
divergence between different domains and the error of an
ideal joint hypothesis. The divergence between the source
and target domains, H∆H-distance, is deﬁned as follows:

dH∆H(S; T )

= 2

sup
(h;h′)2H2

(cid:12)
(cid:12)
(cid:12) E
x(cid:24)S

[h(x) ̸= h′(x)] (cid:0) E
x(cid:24)T

(cid:12)
(cid:12)
[h(x) ̸= h′(x)]
(cid:12)

This distance is frequently used to measure the adaptability
between different domains.

joint hypothesis

(
RS (h) + RT (h)

is deﬁned as h(cid:3) =
The ideal
)
arg min
, and its corresponding error is
h2H
C = RS(h(cid:3)) + RT (h(cid:3)), where R denotes the expected
error for each hypothesis. The theorem is as follows.

Theorem 1. (Ben-David et al., 2010)
Let H be the hypothesis class. Given two different do-
mains, S and T , we have

8h 2 H; RT (h) (cid:20) RS (h) +

dH∆H(S; T ) + C

(2)

1
2

This theorem indicates that the expected error on the tar-
get domain is upper bounded by three terms: the expected
error on the source domain, the domain divergence mea-
sured by the disagreement of the hypothesis, and the er-
ror of the ideal joint hypothesis.
In an existing work
(Ganin & Lempitsky, 2014; Long et al., 2015b), C was dis-
regarded because it was considered to be negligible. If we
are provided with ﬁxed features, we do not need to consider
this term because it is also ﬁxed. However, if we assume
that xs (cid:24) S and xt (cid:24) T are obtained from the last fully
connected layer of the deep models, we should note that C
is determined based on the output of the layer, as well as
the necessity of considering this term.

We consider the pseudo-labeled target sample distributions
Tl given false labels at a ratio of (cid:26). The shared error of h(cid:3)
on S; Tl is denoted as C ′. The following inequality then
holds:

8h 2 H; RT (h) (cid:20) RS(h) +

dH∆H(S; T ) + C

1
2
dH∆H(S; T ) + C ′ + (cid:26)

(3)

(cid:20) RS (h) +

1
2
We show a simple derivation of the inequality in the Sup-
plementary materials section.
In Theorem 1, we cannot
measure C in the absence of labeled target samples. We
can evaluate and minimize it approximately using pseudo-
labels. Furthermore, when we consider the second term
on the right-hand side, our method is expected to reduce

Asymmetric Tri-training for Unsupervised Domain Adaptation

this term. This term intuitively denotes the discrepancy be-
tween different domains in the disagreement of two clas-
siﬁers. If we regard h and h′ as F1 and F2, respectively,
[h(x) ̸= h′(x)] should be very low because the train-
E
x(cid:24)S
ing is based on the same labeled samples. Moreover, for the
[h(x) ̸= h′(x)] is expected to be low, al-
same reason, E
x(cid:24)T
though we use the training set Xt
l instead of the genuine
labeled target samples. Thus, our method considers both
the second and third terms in Theorem 1.
5. Experiment and Evaluation
We conducted extensive evaluations of our method on im-
age datasets and a sentiment analysis dataset. We evaluated
the accuracy of the target-speciﬁc networks.

Visual Domain Adaptation For visual domain adaptation,
we conducted our evaluation on the digit and trafﬁc sign
datasets. The digit datasets include MNIST (LeCun et al.,
1998), MNIST-M (Ganin & Lempitsky, 2014), Street
View House Numbers (SVHN) (Netzer et al., 2011), and
Synthetic Digits (SYN DIGITS)
(Ganin & Lempitsky,
2014). We further evaluated our method on trafﬁc sign
datasets including Synthetic Trafﬁc Signs (SYN SIGNS)
(Moiseev et al., 2013) and the German Trafﬁc Sign Recog-
nition Benchmark (Stallkamp et al., 2011) (GTSRB). In to-
tal, ﬁve adaptation scenarios were evaluated during this
experiment. Because the datasets used for evaluation are
varied in previous studies, we extensively evaluated our
method using these ﬁve scenarios.

Many previous studies have evaluated the ﬁne-tuning of
pretrained networks using ImageNet. This protocol as-
sumes the existence of another source domain. In our work,
we want to evaluate a situation in which we have access to
only a single source domain and a single target domain.

Adaptation in Amazon Reviews To investigate its be-
havior on the language datasets, we evaluated our method
on the Amazon Review dataset
(Blitzer et al., 2006)
through the same preprocessing used by (Chen et al., 2011;
Ganin et al., 2016). The dataset contains reviews on four
types of products: books, DVDs, electronics, and kitchen
appliances. We evaluated our method under 12 domain
adaptation scenarios. The results are shown in Table 1.

Baseline Methods We compared our method with ﬁve
methods for unsupervised domain adaptation,
includ-
ing state-of-the art methods in visual domain adapta-
tion: Maximum Mean Discrepancy (MMD) (Long et al.,
2015b), Domain Adversarial Neural Network (DANN)
(Ganin & Lempitsky, 2014), Deep Reconstruction Clas-
siﬁcation Network (DRCN) (Ghifary et al., 2016), Do-
main Separation Network (DSN) (Bousmalis et al., 2016),
and k-Nearest Neighbor based adaptation (kNN-Ad)
(Sener et al., 2016). We cited the results of MMD from
In addition, we compared our
(Bousmalis et al., 2016).

method with CNN trained only on the source samples. We
compared our method with Variational Fair AutoEncoder
(VFAE) (Louizos et al., 2015) and DANN (Ganin et al.,
2016) in our experiment on the Amazon Review dataset.

5.1. Implementation Detail
In our experiments on the image datasets, we employed the
architecture of CNN used in (Ganin & Lempitsky, 2014).
For a fair comparison, we separated the network at the
hidden layer from which (Ganin & Lempitsky, 2014) con-
structed discriminator networks. Therefore, when con-
sidering a single classiﬁer, for example, F1 ◦ F , the ar-
chitecture is identical to a previous work. We also fol-
lowed (Ganin & Lempitsky, 2014) with the other protocols.
Based on a validation, we set the threshold value for the
labeling method as 0.95 in MNIST$SVHN. In other sce-
narios, we set it as 0.9. We used MomentumSGD for opti-
mization, and set the momentum as 0:9, whereas the learn-
ing rate was set 0.01. (cid:21) was set to 0.01 for all scenarios
based on our validation. In the Supplementary materials
section, we provide details of the network architecture and
the hyper-parameters.

For our experiments on the Amazon Review dataset, we
used a similar architecture to that used in (Ganin et al.,
2016): with the sigmoid activated, one dense hidden layer
with 50 hidden units, and a softmax output. We extended its
architecture to our method similarly to that of the CNN. (cid:21)
was set to 0.001 based on a validation. Because the input is
sparse, we used Adagrad (Duchi et al., 2011) for optimiza-
tion. We repeated this evaluation ten times, and reported
the mean accuracy.

5.2. Experimental Result

In Tables 1 and 3, we show the main results of our experi-
ments. When training only using source samples, the effect
of the BN is not clear, as shown in the Tables 1. However,
for most of the image recognition experiments, the effect
of the BN with our method is clear; at the same time, the
effect of our method is also clear when we do not use a BN
in the network architecture compared to the Source Only
method. The effect of the weight constraint is not obvious
in other than MNIST!SVHN. This result indicates that we
can obtain sufﬁciently different classiﬁers when initializing
the layer parameters differently.
MNIST!MNIST-M First, we evaluated the adaptation
between the hand-written digit dataset, MNIST, and its
transformed dataset, MNIST-M. MNIST-M was composed
by merging clips of a background from the BSDS500
datasets (Arbelaez et al., 2011). A patch was randomly
taken from the images in BSDS500, and merged with the
MNIST digits. From 59,001 target training samples, we
randomly selected 1,000 labeled target samples as a valida-
tion split and tuned the hyper-parameters.

Asymmetric Tri-training for Unsupervised Domain Adaptation

SOURCE MNIST

SVHN

MNIST

SYN DIGITS

SYN SIGNS

METHOD

TARGET MNIST-M MNIST

SVHN

SVHN

GTSRB

Source Only w/o BN
Source Only with BN
MMD (Long et al., 2015b)
DANN (Ganin & Lempitsky, 2014)
DRCN (Ghifary et al., 2016)
DSN (Bousmalis et al., 2016)
kNN-Ad (Sener et al., 2016)

Ours w/o BN
Ours w/o weight constraint ((cid:21) = 0)
Ours

59.1(56.6)
57.1
76.9
81.5
-
83.2
86.7

68.1(59.2)
70.1
71.1
71.1
82.0
82.7
78.8

37.2(30.5)
34.9
-
35.7
40.1
-
40.3

85.3
94.2
94.0

79.8
86.0
85.8

39.8
49.7
52.8

84.1(86.7)
85.5
88.0
90.3
-
91.2
-

93.1
92.4
92.9

79.2(79.0)
75.7
91.1
88.7
-
93.1
-

96.2
94.0
96.2

Table1. Results of the visual domain adaptation experiment on digit and trafﬁc sign datasets. In every setting, our method outperforms
other methods by a large margin. In the source-only results, we show the results reported in (Bousmalis et al., 2016) and (Ghifary et al.,
2016) in parentheses.

MNIST!MNIST-M: last pooling layer

MNIST!SVHN: last shared hidden layer

(a) Non-adapted

(b) Adapted

(c) Non-adapted

(d) Adapted

Figure3. We conﬁrmed the effects our method through a visualization of the learned representations using t-distributed stochastic
neighbor embedding (t-SNE) (Maaten & Hinton, 2008). The red points are the target samples, and the blue points are the source
samples. (a), (c) The case in which only source samples are used for training. (b), (d) Adaptation using our proposed method. In both
scenarios, MNIST!SVHN and MNIST!MNIST-M, we can see that the target samples are more dispersed through adaptation.

Our method outperformed the other existing method by
about 7%. Visualization of the features in the last pool-
ing layer is shown in Fig. 3(a)(b). We observed that the
red target samples are more dispersed when adaptation is
achieved. A comparison of the accuracy between the actual
labeling accuracy on the target samples during the training
and the test accuracy is shown in Fig. 4. The test accuracy
is very low initially, but as the steps increase, the accuracy
becomes closer to that of the labeling accuracy. With this
adaptation, we can clearly see that the actual labeling accu-
racy gradually improves with the accuracy of the network.
SVHN$MNIST We increased the gap between distribu-
tions during this experiment. We evaluated the adaptation
between SVHN (Netzer et al., 2011) and MNIST in a ten-
class classiﬁcation problem. SVHN and MNIST have dis-
tinct appearances, and thus this adaptation is a challeng-
ing scenario, particularly in MNIST!SVHN. The images
in SVHN are colored, and some contain multiple digits.
Therefore, a classiﬁer trained on SVHN is expected to per-
form well on MNIST, but the reverse is not true. MNIST
does not include any samples containing multiple digits,

and most of the samples are centered in the images, and
thus adaptation from MNIST to SVHN is rather difﬁcult.
In both settings, we use 1,000 labeled target samples to ﬁnd
the optimal hyperparameters.

In Fig.

We evaluated our method under both adaptation scenar-
ios and achieved a state-of-the-art performance for both
datasets. In particular, for the adaptation MNIST!SVHN,
our method outperformed the other methods by more
than 10%.
the representations in
3(c)(d),
MNIST!SVHN are visualized. Although the distributions
seem to be separated between domains, the red SVHN sam-
ples become more discriminative when using our method
compared with non-adapted embedding. A comparison be-
tween the actual labeling method accuracy and the testing
accuracy is also shown in Fig. 4(b)(c). In this ﬁgure, it can
be seen that the labeling accuracy rapidly decreases during
the initial adaptation stage. On the other hand, the test-
ing accuracy continues to improve, and ﬁnally exceeds the
labeling accuracy. There are two questions regarding this
interesting phenomenon. The ﬁrst is why does the label-
ing method continue to decrease despite the increase in the

Asymmetric Tri-training for Unsupervised Domain Adaptation

(a) MNIST!MNIST-M

(b) SVHN!MNIST

(c) MNIST!SVHN

(d) SYNDIGITS!SVHN

(e) SYNSIGNS!GTSRB

(f) Comparision of accuracy of three net-
works on SVHN!MNIST

(g) A-distance in MNIST!MNIST-
M

Figure4. (a) (cid:24) (e): Comparison of the actual accuracy of the pseudo-labels and the learned network accuracy during training. The blue
curve indicates the pseudo-label accuracy, and the red curve is the learned network accuracy. Note that the labeling accuracy is computed
using (the number of correctly labeled samples)/(the number of labeled samples). The green curve shows the number of labeled target
samples in each step. (f): Comparison of the accuracy of the three networks in our model. The accuracy of the three networks improved
almost simultaneously. (g): Comparison of the A-distance of the different methods. Our model slightly reduced the divergence of the
domain compared with the source-only trained CNN.

testing accuracy? Target samples given pseudo-labels al-
ways include mistakenly labeled samples, whereas those
given no labels are ignored in our method. Therefore, an
error will be reinforced in the target samples included in
the training set. The second question is why does the test
accuracy continue to increase despite the lower labeling ac-
curacy? The assumed reason is that the network already
acquires target discriminative representations during this
phase, which can improve the accuracy when using source
samples and correctly labeled target samples.

In Fig. 4(f), we show a comparison of the accuracy of
the three networks F1; F2, and Ft in SVHN!MNIST. The
accuracy of these networks is nearly the same during ev-
ery step. The same situation was observed for the other
scenarios. Based on this result, we can state that target-
discriminative representations are shared in three networks.
SYN DIGITS!SVHN With this experiment, we aimed
to address a common adaptation scenario from synthetic
images to real images. The datasets of synthetic num-
bers (Ganin & Lempitsky, 2014) consist of 500,000 im-
ages generated from Windows fonts by varying the text,
positioning, orientation, background and stroke colors, and
the amount of blur. We used 479,400 source samples and
73,257 target samples for training, and 26,032 target sam-
ples for testing. In addition, we used 1,000 SVHN samples
as the validation set.

Our method also outperformed the other methods during
this experiment. With this experiment, the effect of BN
is not clear as compared with the other scenarios. The
domain gap is considered small in this scenario, as the
In
performance of the source-only classiﬁer illustrates.
Fig. 4(d), although the labeling accuracy decreases, the
accuracy of the learned network prediction improves, as in
MNIST$SVHN.
SYN SIGNS!GTSRB This setting is similar to the pre-
vious one, adaptation from synthetic images to real im-
ages, but we have a larger number of classes, namely, 43
classes instead of ten. We used the SYN SIGNS dataset
(Ganin & Lempitsky, 2014) for the source, and the GTSRB
dataset (Stallkamp et al., 2011) for the target, which con-
sist of real images of trafﬁc signs. We randomly selected
31,367 samples for the target training samples and eval-
uated the accuracy on the remaining samples. A total of
3,000 labeled target samples were used for validation.

Under this scenario, our method outperformed the other
methods, which indicates that our method is effective for
the adaptation from synthesized images to real images with
diverse classes. As shown in Fig. 4(e), the same ten-
dency as in MNIST$SVHN was observed for this adap-
tation scenario.

Gradient Stop Experiment We evaluated the effects of a
target-speciﬁc network using our method. We stopped the

051015202530Number of steps0.40.50.60.70.80.91Accuracy00.511.522.533.544.5Number of samples!104Accuracy of labeling methodAccuracy of learned networkNumber of labeled samples051015202530Number of steps0.70.750.80.850.90.95Accuracy00.511.522.533.544.5Number of samples!104Accuracy of labeling methodAccuracy of learned networkNumber of labeled samples051015202530Number of steps0.40.450.50.550.60.650.70.750.8Accuracy00.511.522.533.544.5Number of samples!104Accuracy of labeling methodAccuracy of learned networkNumber of labeled samples051015202530Number of steps0.840.860.880.90.920.94Accuracy00.511.522.533.544.5Number of samples!104Accuracy of labeling methodAccuracy of learned networkNumber of labeled samples051015202530Number of steps0.750.80.850.90.951Accuracy00.511.522.533.544.5Number of samples!104Accuracy of labeling methodAccuracy of learned networkNumber of labeled samples051015202530Number of steps0.650.70.750.80.850.9AccuracyAccuracy of Target NetworkAccuracy of Network1Accuracy of Network2Asymmetric Tri-training for Unsupervised Domain Adaptation

Gradient stop branch
Ft
MNIST!MNIST-M 56.4
47.7
SYN SIGNS!GTSRB 96.5

MNIST!SVHN

F1; F2

None

95.4
47.5
93.1

94.0
52.8
96.2

Table2. Results of gradient stop experiment. When stopping the
gradients from Ft, we did not use backward gradients from Ft
to F , and F only learned from F1 and F2. When stopping the
gradients from F1 and F2, we did not use backward gradients
from F1 and F2 on F , and F learned from Ft. None denotes our
proposed method, in which we backwarded all gradients from all
branches to F .

theoretical

gradient from the upper layer networks F1; F2, and Ft to
examine the effect on Ft. Table 2 shows three scenarios,
including the case in which we stopped the gradients from
F1; F2, and Ft.
In the experiment on MNIST!MNIST-M, we assumed
that only the backpropagation from F1 and F2 cannot con-
struct discriminative representations for the target samples,
and conﬁrmed the effect of Ft. For the adaptation on
MNIST!SVHN, the best performance was realized when
F received all gradients from the upper networks. Back-
warding all gradients ensures both target-speciﬁc discrim-
inative representations in difﬁcult adaptations.
In SYN
SIGNS!GTSRB, backwarding only from Ft results in the
worst performance because these domains are similar, and
noisy pseudo-labeled samples worsen the performance.
A-distance Based
on
the
in
the A-distance is usually
(Ben-David et al., 2010),
used as a measure of domain discrepancy. The method
of estimating the empirical A-distance is simple: We
train a classiﬁer to classify a domain from each domains’
feature. The approximate distance is then calculated as
^dA = 2(1 (cid:0) 2ϵ), where ϵ is a generalization error of the
classiﬁer. We compared our method with the distribution
matching methods, DANN and MMD. We calculated
the distance using the last pooling layer features. We
followed the implementation of DANN (Ganin et al.,
2016) for the training. For MMD training, we followed
the implementation in (Bousmalis et al., 2016).
In Fig.
4(g), the A-distance calculated from each CNN feature is
shown. We used a linear SVM to calculate the distance.
From this graph, we can see that our method clearly
reduces the A-distance compared with the CNN trained on
only the source samples. In addition, when comparing the
distribution matching methods against our own, although
the former reduce the A-distance much more, our method
shows a superior performance as shown in Table 1.

results

Semi-supervised domain adaptation We evaluated our
model in a semi-supervised domain adaptation setting on
MNIST!SVHN. We randomly selected the labeled target
samples for each class, and reported the mean accuracy for

Source!Target

VFAE DANN Our method

books!dvd
books!electronics
books!kitchen
dvd!books
dvd!electronics
dvd!kitchen
electronics!books
electronics!dvd
electronics!kitchen
kitchen!books
kitchen!dvd
kitchen!electronics

79.9
79.2
81.6
75.5
78.6
82.2
72.7
76.5
85.0
72.0
73.3
83.8

78.4
73.3
77.9
72.3
75.4
78.3
71.1
73.8
85.4
70.9
74.0
84.3

80.7
79.8
82.5
73.2
77.0
82.5
73.2
72.9
86.9
72.5
74.9
84.6

Table3. Amazon Reviews experimental results. The accuracy
(%) of the proposed method is shown with the result of VFAE
(Louizos et al., 2015) and DANN (Ganin et al., 2016).

ten experiments. The resulting accuracy was 58% on aver-
age when using ten labeled target samples per class. We can
see the effectiveness of our method in a semi-supervised
setting. A detailed explanation of this is given in our Sup-
plementary materials section.

Amazon Reviews The reviews were encoded in 5,000 di-
mensional vectors of bag-of-word unigrams and bigrams
with binary labels. Negative labels were attached to the
samples if they were ranked with 1 to 3 stars. Positive la-
bels were attached if they were ranked with 4 or 5 stars. We
used 2,000 labeled source samples and 2,000 unlabeled tar-
get samples for the training, and between 3,000 and 6,000
samples for the testing. We used 200 labeled target samples
for validation.

Based on the results in Table 3, our method per-
formed better than VFAE (Louizos et al., 2015) and DANN
(Ganin et al., 2016) in nine out of twelve settings. Our
method was shown to be effective in learning a shallow net-
work on different domains.
6. Conclusion
In this paper, we proposed a novel asymmetric tri-training
method for unsupervised domain adaptation, which is im-
plemented in a simple manner. We aimed at learning dis-
criminative representations by utilizing pseudo-labels as-
signed to unlabeled target samples. We utilized three clas-
siﬁers, two networks assigned pseudo-labels to unlabeled
target samples, and the remaining network, which learned
from them. We evaluated our method regarding both do-
main adaptation for a visual recognition and a sentiment
analysis, and the results show that we outperformed all
other methods. In particular, our method outperformed the
other methods by more than 10% for MNIST!SVHN.
7. Acknowledgement
This work was partially funded by the ImPACT Program of
the Council for Science, Technology, and Innovation (Cab-
inet Ofﬁce, Government of Japan), and was partially sup-
ported by CREST, JST.

Asymmetric Tri-training for Unsupervised Domain Adaptation

References

Arbelaez, Pablo, Maire, Michael, Fowlkes, Charless, and
Malik, Jitendra. Contour detection and hierarchical im-
age segmentation. PAMI, 33(5):898–916, 2011.

Balcan, Maria-Florina, Blum, Avrim, and Yang, Ke. Co-
training and expansion: Towards bridging theory and
practice. In NIPS, 2004.

Ben-David, Shai, Blitzer, John, Crammer, Koby, Kulesza,
Alex, Pereira, Fernando, and Vaughan, Jennifer Wort-
man. A theory of learning from different domains. Ma-
chine learning, 79(1-2):151–175, 2010.

Blitzer,

John, McDonald, Ryan,

and Pereira, Fer-
nando. Domain adaptation with structural correspon-
dence learning. In EMNLP, 2006.

Blum, Avrim and Mitchell, Tom. Combining labeled and

unlabeled data with co-training. In COLT, 1998.

Ioffe, Sergey and Szegedy, Christian. Batch normalization:
Accelerating deep network training by reducing internal
covariate shift. arXiv:1502.03167, 2015.

Khamis, Sameh and Lampert, Christoph H. Coconut:
In
Co-classiﬁcation with output space regularization.
BMVC, 2014.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In NIPS, 2012.

LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998.

Lee, Dong-Hyun. Pseudo-label: The simple and efﬁcient
semi-supervised learning method for deep neural net-
works. In ICML workshop on Challenges in Represen-
tation Learning, 2013.

Bousmalis, Konstantinos, Trigeorgis, George, Silberman,
Nathan, Krishnan, Dilip, and Erhan, Dumitru. Domain
separation networks. In NIPS, 2016.

Levin, Anat, Viola, Paul A, and Freund, Yoav. Unsuper-
vised improvement of visual detectors using co-training.
In ICCV, 2003.

Chen, Minmin, Weinberger, Kilian Q, and Blitzer, John.

Co-training for domain adaptation. In NIPS, 2011.

Dasgupta, Sanjoy, Littman, Michael L, and McAllester,
David. Pac generalization bounds for co-training.
In
NIPS, 2001.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. JMLR, 12(7):2121–2159, 2011.

Ganin, Yaroslav and Lempitsky, Victor. Unsupervised do-
main adaptation by backpropagation. In ICML, 2014.

Ganin, Yaroslav, Ustinova, Evgeniya, Ajakan, Hana, Ger-
main, Pascal, Larochelle, Hugo, Laviolette, Franc¸ois,
Marchand, Mario, and Lempitsky, Victor. Domain-
adversarial training of neural networks. JMLR, 17(59):
1–35, 2016.

Ghifary, Muhammad, Kleijn, W Bastiaan, Zhang, Mengjie,
Balduzzi, David, and Li, Wen. Deep reconstruction-
classiﬁcation networks for unsupervised domain adap-
tation. In ECCV, 2016.

Girshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik,
Jitendra. Rich feature hierarchies for accurate object de-
tection and semantic segmentation. In CVPR, 2014.

Gretton, Arthur, Borgwardt, Karsten M, Rasch, Malte J,
Sch¨olkopf, Bernhard, and Smola, Alexander. A kernel
two-sample test. JMLR, 13(3):723–773, 2012.

Li, Yanghao, Wang, Naiyan, Shi, Jianping, Liu, Jiaying,
and Hou, Xiaodi. Revisiting batch normalization for
practical domain adaptation. arXiv:1603.04779, 2016.

Long, Jonathan, Shelhamer, Evan, and Darrell, Trevor.
Fully convolutional networks for semantic segmentation.
In CVPR, 2015a.

Long, Mingsheng, Cao, Yue, Wang, Jianmin, and Jordan,
Michael I. Learning transferable features with deep
adaptation networks. In ICML, 2015b.

Long, Mingsheng, Zhu, Han, Wang, Jianmin, and Jordan,
Michael I. Unsupervised domain adaptation with resid-
ual transfer networks. In NIPS, 2016.

Louizos, Christos, Swersky, Kevin, Li, Yujia, Welling,
Max, and Zemel, Richard. The variational fair autoen-
coder. arXiv:1511.00830, 2015.

Maaten, Laurens van der and Hinton, Geoffrey. Visualizing

data using t-sne. JMLR, 9(11):2579–2605, 2008.

Moiseev, Boris, Konev, Artem, Chigorin, Alexander, and
Konushin, Anton. Evaluation of trafﬁc sign recogni-
tion methods trained on synthetically generated data. In
ACIVS, 2013.

Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco,
Alessandro, Wu, Bo, and Ng, Andrew Y. Reading dig-
its in natural images with unsupervised feature learning.
In NIPS workshop on deep learning and unsupervised
feature learning, 2011.

Asymmetric Tri-training for Unsupervised Domain Adaptation

Rohrbach, Marcus, Ebert, Sandra, and Schiele, Bernt.
In NIPS,

Transfer learning in a transductive setting.
2013.

Sener, Ozan, Song, Hyun Oh, Saxena, Ashutosh, and
Savarese, Silvio. Learning transferrable representations
for unsupervised domain adaptation. In NIPS, 2016.

Stallkamp, Johannes, Schlipsing, Marc, Salmen, Jan, and
Igel, Christian. The german trafﬁc sign recognition
benchmark: a multi-class classiﬁcation competition. In
IJCNN, 2011.

Sun, Baochen, Feng, Jiashi, and Saenko, Kate. Return of
frustratingly easy domain adaptation. In AAAI, 2016.

Tanha, Jafar, van Someren, Maarten, and Afsarmanesh,
Hamideh. Ensemble based co-training. In BNAIC, 2011.

Wan, Xiaojun. Co-training for cross-lingual sentiment clas-

siﬁcation. In ACL, 2009.

Zhou, Zhi-Hua and Li, Ming. Tri-training: Exploiting un-
labeled data using three classiﬁers. TKDE, 17(11):1529–
1541, 2005.

Zhu, Xiaojin.

Semi-supervised learning literature sur-
vey. Technical report, University of Wisconsin-Madison,
2005.

