Neural Networks and Rational Functions

Matus Telgarsky 1

Abstract
Neural networks and rational functions efﬁ-
ciently approximate each other.
In more de-
tail, it is shown here that for any ReLU net-
there exists a rational function of de-
work,
gree O(poly log(1/(cid:15))) which is (cid:15)-close, and sim-
ilarly for any rational function there exists a
ReLU network of size O(poly log(1/(cid:15))) which
is (cid:15)-close. By contrast, polynomials need de-
gree Ω(poly(1/(cid:15))) to approximate even a single
ReLU. When converting a ReLU network to a
rational function as above, the hidden constants
depend exponentially on the number of layers,
which is shown to be tight; in other words, a com-
positional representation can be beneﬁcial even
for rational functions.

1. Overview

Signiﬁcant effort has been invested in characterizing the
functions that can be efﬁciently approximated by neural
networks. The goal of the present work is to characterize
neural networks more ﬁnely by ﬁnding a class of functions
which is not only well-approximated by neural networks,
but also well-approximates neural networks.

The function class investigated here is the class of rational
functions: functions represented as the ratio of two poly-
nomials, where the denominator is a strictly positive poly-
nomial. For simplicity, the neural networks are taken to
always use ReLU activation σr(x) := max{0, x}; for a re-
view of neural networks and their terminology, the reader
is directed to Section 1.4. For the sake of brevity, a network
with ReLU activations is simply called a ReLU network.

1.1. Main results

The main theorem here states that ReLU networks and ra-
tional functions approximate each other well in the sense

1University of Illinois, Urbana-Champaign; work completed
while visiting the Simons Institute. Correspondence to: your
friend <mjt@illinois.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure 1. Rational, polynomial, and ReLU network ﬁt to “spike”,
a function which is 1/x along [1/4, 1] and 0 elsewhere.

that (cid:15)-approximating one class with the other requires a
representation whose size is polynomial in ln(1 / (cid:15)), rather
than being polynomial in 1/(cid:15).
Theorem 1.1.

1. Let (cid:15) ∈ (0, 1] and nonnegative inte-
[0, 1]d → [−1, +1] and
ger k be given. Let p :
q : [0, 1]d → [2−k, 1] be polynomials of degree ≤ r,
each with ≤ s monomials. Then there exists a function
f : [0, 1]d → R, representable as a ReLU network of
size (number of nodes)

(cid:16)

O

k7 ln(1 / (cid:15))3

such that

+ min

srk ln(sr / (cid:15)), sdk2 ln(dsr / (cid:15))2(cid:111) (cid:17)
(cid:110)
,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
x∈[0,1]d

f (x) −

≤ (cid:15).

p(x)
q(x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2. Let (cid:15) ∈ (0, 1] be given. Consider a ReLU network
f : [−1, +1]d → R with at most m nodes in each
of at most k layers, where each node computes z (cid:55)→
σr(a(cid:62)z + b) where the pair (a, b) (possibly distinct
across nodes) satisﬁes (cid:107)a(cid:107)1 + |b| ≤ 1. Then there ex-
ists a rational function g : [−1, +1]d → R with degree
(maximum degree of numerator and denominator)

(cid:16)

ln(k/(cid:15))kmk(cid:17)

O

such that

sup
x∈[−1,+1]d

(cid:12)f (x) − g(x)(cid:12)
(cid:12)

(cid:12) ≤ (cid:15).

−1.00−0.75−0.50−0.250.000.250.500.751.0001234spikeratpolynetNeural networks and rational functions

Perhaps the main wrinkle is the appearance of mk when
approximating neural networks by rational functions. The
following theorem shows that this dependence is tight.

Theorem 1.2. Let any integer k ≥ 3 be given. There ex-
ists a function f : R → R computed by a ReLU network
with 2k layers, each with ≤ 2 nodes, such that any ratio-
nal function g : R → R with ≤ 2k−2 total terms in the
numerator and denominator must satisfy

(cid:90)

[0,1]

|f (x) − g(x)| dx ≥

1
64

.

Note that this statement implies the desired difﬁculty of ap-
proximation, since a gap in the above integral (L1) distance
implies a gap in the earlier uniform distance (L∞), and
furthermore an r-degree rational function necessarily has
≤ 2r + 2 total terms in its numerator and denominator.

As a ﬁnal piece of the story, note that the conversion be-
tween rational functions and ReLU networks is more seam-
less if instead one converts to rational networks, meaning
neural networks where each activation function is a rational
function.
Lemma 1.3. Let a ReLU network f : [−1, +1]d → R
be given as in Theorem 1.1, meaning f has at most l layers
and each node computes z (cid:55)→ σr(a(cid:62)z + b) where where the
pair (a, b) (possibly distinct across nodes) satisﬁes (cid:107)a(cid:107)1 +
|b| ≤ 1. Then there exists a rational function R of degree
O(ln(l/(cid:15))2) so that replacing each σr in f with R yields a
function g : [−1, +1]d → R with

sup
x∈[−1,+1]d

|f (x) − g(x)| ≤ (cid:15).

Combining Theorem 1.2 and Lemma 1.3 yields an intrigu-
ing corollary.

Corollary 1.4. For every k ≥ 3, there exists a function
f : R → R computed by a rational network with O(k)
layers and O(k) total nodes, each node invoking a rational
activation of degree O(k), such that every rational function
g : R → R with less than 2k−2 total terms in the numerator
and denominator satisﬁes

(cid:90)

[0,1]

|f (x) − g(x)| dx ≥

1
128

.

Figure 2. Polynomial and rational ﬁt to the threshold function.

1.2. Auxiliary results

The ﬁrst thing to stress is that Theorem 1.1 is impossible
with polynomials: namely, while it is true that ReLU net-
works can efﬁciently approximate polynomials (Yarotsky,
2016; Safran & Shamir, 2016; Liang & Srikant, 2017), on
the other hand polynomials require degree Ω(poly(1/(cid:15))),
rather than O(poly(ln(1/(cid:15)))),
to approximate a single
ReLU, or equivalently the absolute value function (Petru-
shev & Popov, 1987, Chapter 4, Page 73).

Another point of interest is the depth needed when convert-
ing a rational function to a ReLU network. Theorem 1.1
is impossible if the depth is o(ln(1/(cid:15))): speciﬁcally, it is
impossible to approximate the degree 1 rational function
x (cid:55)→ 1/x with size O(ln(1/(cid:15))) but depth o(ln(1/(cid:15))).
Proposition 1.5. Set f (x) := 1/x, the reciprocal map. For
any (cid:15) > 0 and ReLU network g : R → R with l layers and
m < (27648(cid:15))−1/(2l)/2 nodes,

(cid:90)

[1/2,3/4]

|f (x) − g(x)| dx > (cid:15).

Lastly, the implementation of division in a ReLU network
requires a few steps, arguably the most interesting being
a “continuous switch statement”, which computes recipro-
cals differently based on the magnitude of the input. The
ability to compute switch statements appears to be a fairly
foundational operation available to neural networks and ra-
tional functions (Petrushev & Popov, 1987, Theorem 5.2),
but is not available to polynomials (since otherwise they
could approximate the ReLU).

1.3. Related work

The hard-to-approximate function f is a rational network
which has a description of size O(k2). Despite this, at-
tempting to approximate it with a rational function of the
usual form requires a description of size Ω(2k). Said an-
other way: even for rational functions, there is a beneﬁt to
a neural network representation!

The results of the present work follow a long line of work
on the representation power of neural networks and re-
lated functions. The ability of ReLU networks to ﬁt con-
tinuous functions was no doubt proved many times, but
it appears the earliest reference is to Lebesgue (Newman,
1964, Page 1), though of course results of this type are usu-

−1.00−0.75−0.50−0.250.000.250.500.751.000.00.20.40.60.81.0threshratpolyNeural networks and rational functions

ally given much more contemporary attribution (Cybenko,
1989). More recently, it has been shown that certain func-
tion classes only admit succinct representations with many
layers (Telgarsky, 2015). This has been followed by proofs
showing the possibility for a depth 3 function to require ex-
ponentially many nodes when rewritten with 2 layers (El-
dan & Shamir, 2016). There are also a variety of other
result giving the ability of ReLU networks to approximate
various function classes (Cohen et al., 2016; Poggio et al.,
2017).

Most recently, a variety of works pointed out neural net-
works can approximate polynomials, and thus smooth
functions essentially by Taylor’s theorem (Yarotsky, 2016;
Safran & Shamir, 2016; Liang & Srikant, 2017). This
somewhat motivates this present work, since polynomials
can not in turn approximate neural networks with a depen-
dence O(poly log(1/(cid:15))): they require degree Ω(1/(cid:15)) even
for a single ReLU.

Rational functions are extensively studied in the classical
approximation theory literature (Lorentz et al., 1996; Petru-
shev & Popov, 1987). This literature draws close con-
nections between rational functions and splines (piecewise
polynomial functions), a connection which has been used
in the machine learning literature to draw further connec-
tions to neural networks (Williamson & Bartlett, 1991). It
is in this approximation theory literature that one can ﬁnd
the following astonishing fact: not only is it possible to ap-
proximate the absolute value function (and thus the ReLU)
over [−1, +1] to accuracy (cid:15) > 0 with a rational function of
degree O(ln(1/(cid:15))2) (Newman, 1964), but moreover the op-
timal rate is known (Petrushev & Popov, 1987; Zolotarev,
1877)! These results form the basis of those results here
which show that rational functions can approximate ReLU
networks. (Approximation theory results also provide other
functions (and types of neural networks) which rational
functions can approximate well, but the present work will
stick to the ReLU for simplicity.)

An ICML reviewer revealed prior work which was embar-
rassingly overlooked by the author:
it has been known,
since decades ago (Beame et al., 1986), that neural net-
works using threshold nonlinearities (i.e., the map x (cid:55)→
1[x ≥ 0]) can approximate division, and moreover the
proof is similar to the proof of part 1 of Theorem 1.1!
Moreover, other work on threshold networks invoked
Newman polynomials to prove lower bound about linear
threshold networks (Paturi & Saks, 1994). Together this
suggests that not only the connections between rational
functions and neural networks are tight (and somewhat
known/unsurprising), but also that threshold networks and
ReLU networks have perhaps more similarities than what is
suggested by the differing VC dimension bounds, approxi-
mation results, and algorithmic results (Goel et al., 2017).

Figure 3. Newman polynomials of degree 5, 9, 13.

1.4. Further notation

Here is a brief description of the sorts of neural networks
used in this work. Neural networks represent computation
as a directed graph, where nodes consume the outputs of
their parents, apply a computation to them, and pass the
resulting value onward.
In the present work, nodes take
their parents’ outputs z and compute σr(a(cid:62)z + b), where
a is a vector, b is a scalar, and σr(x) := max{0, x}; an-
other popular choice of nonlineary is the sigmoid x (cid:55)→
(1 + exp(−x))−1. The graphs in the present work are
acyclic and connected with a single node lacking children
designated as the univariate output, but the literature con-
tains many variations on all of these choices.
As stated previously, a rational function f : Rd → R is
ratio of two polynomials. Following conventions in the ap-
proximation theory literature (Lorentz et al., 1996), the de-
nominator polynomial will always be strictly positive. The
degree of a rational function is the maximum of the degrees
of its numerator and denominator.

2. Approximating ReLU networks with

rational functions

This section will develop the proofs of part 2 of Theo-
rem 1.1, Theorem 1.2, Lemma 1.3, and Corollary 1.4.

2.1. Newman polynomials

The starting point is a seminal result in the theory of ra-
tional functions (Zolotarev, 1877; Newman, 1964): there
exists a rational function of degree O(ln(1/(cid:15))2) which can
approximate the absolute value function along [−1, +1] to
accuracy (cid:15) > 0. This in turn gives a way to approximate
the ReLU, since

σr(x) = max{0, x} =

(2.1)

x + |x|
2

.

The construction here uses the Newman polynomials (New-

−1.00−0.75−0.50−0.250.000.250.500.751.000246810125913Neural networks and rational functions

man, 1964): given an integer r, deﬁne

Nr(x) :=

(x + exp(−i/

r)).

√

r−1
(cid:89)

i=1

The Newman polynomials N5, N9, and N13 are depicted
in Figure 3. Typical polynomials in approximation theory,
for instance the Chebyshev polynomials, have very active
oscillations; in comparison, the Newman polynomials look
a little funny, lying close to 0 over [−1, 0], and quickly in-
creasing monotonically over [0, 1]. The seminal result of
Newman (1964) is that

(cid:12)
(cid:12)
(cid:12)
sup
(cid:12)
(cid:12)
|x|≤1

|x| − x

(cid:18) Nr(x) − Nr(−x)
Nr(x) + Nr(−x)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

≤ 3 exp(−

r)/2.

Thanks to this bound and eq. (2.1), it follows that the ReLU
can be approximated to accuracy (cid:15) > 0 by rational func-
tions of degree O(ln(1/(cid:15))2).

(Some basics on Newman polynomials, as needed in the
present work, can be found in Appendix A.1.)

2.2. Proof of Lemma 1.3

Now that a single ReLU can be easily converted to a ra-
tional function, the next task is to replace every ReLU in
a ReLU network with a rational function, and compute
the approximation error. This is precisely the statement of
Lemma 1.3.

The proof of Lemma 1.3 is an induction on layers, with
full details relegated to the appendix. The key computa-
tion, however, is as follows. Let R(x) denote a rational
approximation to σr. Fix a layer i + 1, and let H(x) de-
note the multi-valued mapping computed by layer i, and
let HR(x) denote the mapping obtained by replacing each
σr in H with R. Fix any node in layer i + 1, and let
x (cid:55)→ σr(a(cid:62)H(x) + b) denote its output as a function of
the input. Then

(cid:12)
(cid:12)σr(a(cid:62)H(x) + b) − R(a(cid:62)HR(x) + b)
(cid:12)
≤

(cid:12)
(cid:12)
(cid:12)σr(a(cid:62)H(x) + b) − σr(a(cid:62)HR(x) + b)
(cid:12)
(cid:12)
(cid:12)
(cid:123)(cid:122)
(cid:125)
(cid:124)
♥

(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)σr(a(cid:62)HR(x) + b) − R(a(cid:62)HR(x) + b)
(cid:12)
(cid:12)
(cid:12)
(cid:123)(cid:122)
(cid:125)
(cid:124)
♣

.

For the ﬁrst term ♥, note since σr is 1-Lipschitz and by
H¨older’s inequality that

♥ ≤

(cid:12)
(cid:12)a(cid:62)(H(x) − HR(x))
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ (cid:107)a(cid:107)1(cid:107)H(x) − HR(x)(cid:107)∞,

meaning this term has been reduced to the inductive hy-
pothesis since (cid:107)a(cid:107)1 ≤ 1. For the second term ♣, if

Figure 4. Polynomial and rational ﬁt to ∆.

a(cid:62)HR(x) + b can be shown to lie in [−1, +1] (which is
another easy induction), then ♣ is just the error between R
and σr on the same input.

2.3. Proof of part 2 of Theorem 1.1

It is now easy to ﬁnd a rational function that approximates
a neural network, and to then bound its size. The ﬁrst step,
via Lemma 1.3, is to replace each σr with a rational func-
tion R of low degree (this last bit using Newman polynomi-
als). The second step is to inductively collapse the network
into a single rational function. The reason for the depen-
dence on the number of nodes m is that, unlike polyno-
mials, summing rational functions involves an increase in
degree:

p1(x)
q1(x)

+

p1(x)
q2(x)

=

p1(x)q2(x) + p2(x)q1(x)
q1(x)q2(x)

.

2.4. Proof of Theorem 1.2

The ﬁnal interesting bit is to show that the dependence on
ml in part 2 of Theorem 1.1 (where m is the number of
nodes and l is the number of layers) is tight.

Recall the “triangle function”

∆(x) :=






2x
x ∈ [0, 1/2],
2(1 − x) x ∈ (1/2, 1],
0

otherwise.

The k-fold composition ∆k is a piecewise afﬁne function
with 2k−1 regularly spaced peaks (Telgarsky, 2015). This
function was demonstrated to be inapproximable by shal-
low networks of subexponential size, and now it can be
shown to be a hard case for rational approximation as well.

Consider the horizontal line through y = 1/2. The func-
tion ∆k will cross this line 2k times. Now consider a ra-
tional function f (x) = p(x)/q(x). The set of points where
f (x) = 1/2 corresponds to points where 2p(x)−q(x) = 0.

0.00.20.40.60.81.00.00.20.40.60.81.0∆ratpolyNeural networks and rational functions

3.1. Proving part 1 of Theorem 1.1

Figure 5. Polynomial and rational ﬁt to σr.

A poor estimate for the number of zeros is simply the de-
gree of 2p−q, however, since f is univariate, a stronger tool
becomes available: by Descartes’ rule of signs, the number
of zeros in f − 1/2 is upper bounded by the number of
terms in 2p − q.

3. Approximating rational functions with

ReLU networks

This section will develop the proof of part 1 of Theo-
rem 1.1, as well as the tightness result in Proposition 1.5

To establish part 1 of Theorem 1.1, the ﬁrst step is to ap-
proximate polynomials with ReLU networks, and the sec-
ond is to then approximate the division operation.

The representation of polynomials will be based upon con-
structions due to Yarotsky (2016). The starting point is the
following approximation of the squaring function.
Lemma 3.1 ((Yarotsky, 2016)). Let any (cid:15) > 0 be given.
There exists f : x → [0, 1], represented as a ReLU
network with O(ln(1/(cid:15))) nodes and layers, such that
supx∈[0,1] |f (x) − x2| ≤ (cid:15) and f (0) = 0.

Yarotsky’s proof is beautiful and deserves mention. The
approximation of x2 is the function fk, deﬁned as

fk(x) := x −

k
(cid:88)

i=1

∆i(x)
4i

,

where ∆ is the triangle map from Section 2. For every
k, fk is a convex, piecewise-afﬁne interpolation between
points along the graph of x2; going from k to k + 1 does
not adjust any of these interpolation points, but adds a new
set of O(2k) interpolation points.

Once squaring is in place, multiplication comes via the po-
larization identity xy = ((x + y)2 − x2 − y2)/2.
Lemma 3.2 ((Yarotsky, 2016)). Let any (cid:15) > 0 and B ≥ 1
be given. There exists g(x, y) : [0, B]2 → [0, B2], rep-
resented by a ReLU network with O(ln(B/(cid:15)) nodes and
layers, with

sup
x,y∈[0,1]

|g(x, y) − xy| ≤ (cid:15)

and g(x, y) = 0 if x = 0 or y = 0.

Next, it follows that ReLU networks can efﬁciently approx-
imate exponentiation thanks to repeated squaring.
Lemma 3.3. Let (cid:15) ∈ (0, 1] and positive integer y be given.
There exists h : [0, 1] → [0, 1], represented by a ReLU
network with O(ln(y/(cid:15))2) nodes and layers, with
(cid:12)h(x) − xy(cid:12)
(cid:12)

(cid:12) ≤ (cid:15)

sup
x,y∈[0,1]

With multiplication and exponentiation, a representation
result for polynomials follows.

Let p

:
Lemma 3.4. Let (cid:15) ∈ (0, 1] be given.
[0, 1]d → [−1, +1] denote a polynomial with ≤ s
monomials, each with degree ≤ r and scalar coef-
ﬁcient within [−1, +1].
Then there exists a function
[0, 1]d → [−1, +1] computed by a network of
q
size O (cid:0)min{sr ln(sr/(cid:15)), sd ln(dsr/(cid:15))2}(cid:1), which satisﬁes
supx∈[0,1]d |p(x) − q(x)| ≤ (cid:15).

:

The remainder of the proof now focuses on the division op-
eration. Since multiplication has been handled, it sufﬁces
to compute a single reciprocal.

Lemma 3.5. Let (cid:15) ∈ (0, 1] and nonnegative integer k be
given. There exists a ReLU network q : [2−k, 1] → [1, 2k],
of size O(k2 ln(1/(cid:15))2) and depth O(k4 ln(1/(cid:15))3) such that

(cid:12)
(cid:12)
sup
(cid:12)
(cid:12)
x∈[2−k,1]

q(x) −

≤ (cid:15).

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
x

This proof relies on two tricks. The ﬁrst is to observe, for
x ∈ (0, 1], that

1
x

=

1
1 − (1 − x)

=

(cid:88)

i≥0

(1 − x)i.

Thanks to the earlier development of exponentiation, trun-
cating this summation gives an expression easily approxi-
mate by a neural network as follows.

Lemma 3.6. Let 0 < a ≤ b and (cid:15) > 0 be given. Then there
exists a ReLU network q : R → R with O(ln(1/(a(cid:15)))2)
layers and O((b/a) ln(1/(a(cid:15)))3) nodes satisfying

(cid:12)
(cid:12)
sup
(cid:12)
(cid:12)
x∈[a,b]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
x

q(x) −

≤ 2(cid:15).

Unfortunately, Lemma 3.6 differs from the desired state-
ment Lemma 3.6: inverting inputs lying within [2−k, 1] re-
quires O(2k ln(1/(cid:15))2) nodes rather than O(k4 ln(1/(cid:15))3)!

−1.00−0.75−0.50−0.250.000.250.500.751.000.00.20.40.60.81.0ReLUratpolyNeural networks and rational functions

To obtain a good estimate with only O(ln(1/(cid:15))) terms of
the summation, it is necessary for the input to be x bounded
below by a positive constant (not depending on k). This
leads to the second trick (which was also used by Beame
et al. (1986)!).

Consider, for positive constant c > 0, the expression

1
x

=

c
1 − (1 − cx)

= c

(1 − cx)i.

(cid:88)

i≥0

If x is small, choosing a larger c will cause this summation
to converge more quickly. Thus, to compute 1/x accurately
over a wide range of inputs, the solution here is to multi-
plex approximations of the truncated sum for many choices
of c. In order to only rely on the value of one of them, it
is possible to encode a large “switch” style statement in a
neural network. Notably, rational functions can also repre-
sentat switch statements (Petrushev & Popov, 1987, The-
orem 5.2), however polynomials can not (otherwise they
could approximate the ReLU more efﬁciently, seeing as it
is a switch statement of 0 (a degree 0 polynomial) and x (a
degree 1 polynomial).
Lemma 3.7. Let (cid:15) > 0, B ≥ 1, reals a0 ≤ a1 ≤ · · · ≤
an ≤ an+1 and a function f : [a0, an+1] → R be given.
Moreover, suppose for i ∈ {1, . . . , n}, there exists a ReLU
network gi : R → R of size ≤ mi and depth ≤ ki with
gi ∈ [0, B] along [ai−1, ai+1] and

sup
x∈[ai−1,ai+1]

|gi(x) − f | ≤ (cid:15).

Then there exists a function g : R → R computed by a
ReLU network of size O (cid:0)n ln(B/(cid:15)) + (cid:80)
(cid:1) and depth
O (cid:0)ln(B/(cid:15)) + maxi ki

(cid:1) satisfying

i mi

sup
x∈[a1,an]

|g(x) − f (x)| ≤ 3(cid:15).

3.2. Proof of Proposition 1.5

It remains to show that shallow networks have a hard time
approximating the reciprocal map x (cid:55)→ 1/x.

This proof uses the same scheme as various proofs in (Tel-
garsky, 2016), which was also followed in more recent
works (Yarotsky, 2016; Safran & Shamir, 2016): the idea
is to ﬁrst upper bound the number of afﬁne pieces in ReLU
networks of a certain size, and then to point out that each
linear segment must make substantial error on a curved
function, namely 1/x.

The proof is fairly brute force, and thus relegated to the
appendices.

4. Summary of ﬁgures

Throughout this work, a number of ﬁgures were presented
to show not only the astonishing approximation properties

Figure 6. Polynomial and rational ﬁt to ∆3.

of rational functions, but also the higher ﬁdelity approxi-
mation achieved by both ReLU networks and rational func-
tions as compared with polynomials. Of course, this is only
a qualitative demonstration, but still lends some intuition.

In all these demonstrations, rational functions and polyno-
mials have degree 9 unless otherwise marked. ReLU net-
works have two hidden layers each with 3 nodes. This is
not exactly apples to apples (e.g., the rational function has
twice as many parameters as the polynomial), but still rea-
sonable as most of the approximation literature ﬁxes poly-
nomial and rational degrees in comparisons.

Figure 1 shows the ability of all three classes to approx-
imate a truncated reciprocal. Both rational functions and
ReLU networks have the ability to form “switch state-
ments” that let them approximate different functions on dif-
ferent intervals with low complexity (Petrushev & Popov,
1987, Theorem 5.2). Polynomials lack this ability; they can
not even approximate the ReLU well, despite it being low
degree polynomials on two separate intervals.

Figure 2 shows that rational functions can ﬁt the threshold
function errily well; the particular rational function used
here is based on using Newman polynomials to approxi-
mate (1 + |x|/x)/2 (Newman, 1964).

Figure 3 shows Newman polynomials N5, N9, N13. As dis-
cussed in the text, they are unlike orthogonal polynomials,
and are used in all rational function approximations except
Figure 1, which used a least squares ﬁt.

Figure 4 shows that rational functions (via the Newman
polynomials) ﬁt ∆ very well, whereas polynomials have
trouble. These errors degrade sharply after recursing,
namely when approximating ∆3 as in Figure 6.

Figure 5 shows how polynomials and rational functions ﬁt
the ReLU, where the ReLU representation, based on New-
man polynomials, is the one used in the proofs here. De-
spite the apparent slow convergence of polynomials in this
regime, the polynomial ﬁt is still quite respectable.

0.00.20.40.60.81.00.00.20.40.60.81.0∆3ratpolyNeural networks and rational functions

Liang, Shiyu and Srikant, R. Why deep neural networks

for function approximation? In ICLR, 2017.

Lorentz, G. G., Golitschek, Manfred von, and Makovoz,
Yuly. Constructive approximation : advanced problems.
Springer, 1996.

Newman, D. J. Rational approximation to |x|. Michigan

Math. J., 11(1):11–14, 03 1964.

Paturi, Ramamohan and Saks, Michael E. Approximating
Inf. Comput.,

threshold circuits by rational functions.
112(2):257–272, 1994.

Petrushev, P. P. Penco Petrov and Popov, Vasil A. Rational
approximation of real functions. Encyclopedia of mathe-
matics and its applications. Cambridge University Press,
1987.

Poggio, Tomaso, Mhaskar, Hrushikesh, Rosasco, Lorenzo,
Miranda, Brando, and Liao, Qianli. Why and when can
deep – but not shallow – networks avoid the curse of
dimensionality: a review. 2017. arXiv:1611.00740
[cs.LG].

Safran, Itay and Shamir, Ohad. Depth separation in relu
networks for approximating smooth non-linear func-
tions. 2016. arXiv:1610.09887 [cs.LG].

Telgarsky, Matus. Representation beneﬁts of deep feed-
forward networks. 2015. arXiv:1509.08101v2
[cs.LG].

Telgarsky, Matus. Beneﬁts of depth in neural networks. In

COLT, 2016.

Williamson, Robert C. and Bartlett, Peter L. Splines, ratio-

nal functions and neural networks. In NIPS, 1991.

Yarotsky, Dmitry. Error bounds for approximations with
2016. arXiv:1610.01145

deep relu networks.
[cs.LG].

Zolotarev, E.I. Application of elliptic functions to the prob-
lem of the functions of the least and most deviation from
zero. Transactions Russian Acad. Scai., pp. 221, 1877.

5. Open problems

There are many next steps for this and related results.

1. Can rational functions, or some other approximating
class, be used to more tightly bound the generaliza-
tion properties of neural networks? Notably, the VC
dimension of sigmoid networks uses a conversion to
polynomials (Anthony & Bartlett, 1999).

2. Can rational functions, or some other approximating
class, be used to design algorithms for training neural
networks? It does not seem easy to design reasonable
algorithms for minimization over rational functions;
if this is fundamental and moreover in contrast with
neural networks, it suggests an algorithmic beneﬁt of
neural networks.

3. Can rational functions, or some other approximating
class, give a sufﬁciently reﬁned complexity estimate
of neural networks which can then be turned into a
regularization scheme for neural networks?

Acknowledgements

The author thanks Adam Klivans and Suvrit Sra for stim-
ulating conversations. Adam Klivans and the author both
thank Almare Gelato Italiano, in downtown Berkeley, for
necessitating further stimulating conversations, but now on
the topic of health and exercise. Lastly, the author thanks
the University of Illinois, Urbana-Champaign, and the Si-
mons Institute in Berkeley, for ﬁnancial support during this
work.

References

Anthony, Martin and Bartlett, Peter L. Neural Network
Learning: Theoretical Foundations. Cambridge Univer-
sity Press, 1999.

Beame, Paul, Cook, Stephen A., and Hoover, H. James.
Log depth circuits for division and related problems.
SIAM Journal on Computing, 15(4):994–1003, 1986.

Cohen, Nadav, Sharir, Or, and Shashua, Amnon. On the
expressive power of deep learning: A tensor analysis.
2016. COLT.

Cybenko, George. Approximation by superpositions of a
sigmoidal function. Mathematics of Control, Signals and
Systems, 2(4):303–314, 1989.

Eldan, Ronen and Shamir, Ohad. The power of depth for

feedforward neural networks. In COLT, 2016.

Goel, Surbhi, Kanade, Varun, Klivans, Adam, and Thaler,
Justin. Reliably learning the relu in polynomial time. In
COLT, 2017.

A. Deferred material from Section 2

Neural networks and rational functions

This section collects technical material omitted from Section 2. The ﬁrst step is to ﬁll in some missing details regarding
Newman polynomials.

A.1. Newman polynomials

Deﬁne the Newman polynomial (Newman, 1964)

Nr(x) :=

(x + αi
r)

where αr := exp(−1/

r).

√

r−1
(cid:89)

i=1

(A.1)

r > 0. Otherwise x > 0, and note for any i ∈ {1, . . . , r − 1} that

Deﬁne Ar(x), the Newman approximation to |x|, as

Ar(x) := x

(cid:18) Nr(x) − Nr(−x)
Nr(x) + Nr(−x)

(cid:19)

.

Lemma A.2 (Newman (1964)). Suppose r ≥ 5.

• Nr(x) + Nr(−x) > 0; in particular, Ar is well-deﬁned over R.

• Given any b ≥ 1,

(cid:12)bAr(x/b) − |x|(cid:12)
(cid:12)

(cid:12) ≤ 3b exp(−

r).

√

sup
x∈[−b,+b]

Proof.

• If x = 0, then Nr(−x) = Nr(x) = (cid:81)r−1
r − x < αi
r + x,
r < x + αi
r.

r| = αi
r| = x − αi

r] means |x − αi

– x ∈ (0, αi
– x > αi

i=1 αi

r means |x − αi
r| < x + αi

r, and

Together, |x − αi

Nr(x) =

(x + αi

r) >

|x − αi

r| =

= |Nr(−x)|.

r−1
(cid:89)

i=1

r−1
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

r−1
(cid:89)

i=1

(cid:12)
(cid:12)
(cid:12)
(x − αi
r)
(cid:12)
(cid:12)
(cid:12)

Since Nr(x) > 0 when x > 0, thus Nr(x) + Nr(−x) > Nr(x) − |Nr(x)| = 0.

Lastly, the case x < 0 follows from the case x > 0 since x (cid:55)→ Nr(x) + Nr(−x) is even.

• For any x ∈ [−b, +b],

||x| − bAr(x/b)| =

(cid:12)
(cid:12)

(cid:12)b (cid:0)|x/b| − Ar(x/b)(cid:1)(cid:12)

(cid:12) = b (cid:12)
(cid:12)

(cid:12)x/b − Ar(x/b)(cid:12)

(cid:12) ≤ 3b exp(−

r),

√

where the last step was proved by Newman (Lorentz et al., 1996, Theorem 7.3.1).

Finally, deﬁne

˜Rr,b(x) := Rr(x; b) :=
√

(cid:15)r,b := 3 exp(−

r)/2,

x + bAr(x/b)
2

,

Rr,b(x) := (1 − 2(cid:15)r,b) ˜Rr,b(x) + b(cid:15)r,b.

Lemma A.3. If r ≥ 5 and b ≥ 1 and (cid:15)r,b ≤ 1/2, then Rr,b is a degree-r rational function over R, and

Neural networks and rational functions

(cid:12)
(cid:12)
(cid:12)σr(x) − ˜Rr,b(x)
(cid:12)
(cid:12)
(cid:12) ≤ b(cid:15)r,b,
sup
x∈[−b,+b]
(cid:12)σr(x) − Rr,b(x)(cid:12)
(cid:12)

(cid:12) ≤ 3b(cid:15)r,b.

sup
x∈[−b,+b]

If (cid:15)r,b ≤ 1, then Rr,b ∈ [0, b] along [−b, +b].

Proof. Let r, b be given, and for simplicity omit the various subscripts. The denominator of ˜R is positive over R by
Lemma A.2. Now ﬁx x ∈ [−b, +b]. Using the second part of Lemma A.2,

(cid:12)
(cid:12)
(cid:12)σr(x) − ˜R(x)
(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

x + |x|
2

−

x + bA(x/b)
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

1
2

(cid:12)|x| − bA(x/b)(cid:12)
(cid:12)

(cid:12) ≤ 3b exp(−

r)/2 = b(cid:15).

√

Next, note that ˜R ∈ [−b(cid:15), b(1 + (cid:15))]:

˜R(x) ≤ σr(x) + b(cid:15) ≤ b(1 + (cid:15)),

˜R(x) ≥ σr(x) − b(cid:15) ≥ −b(cid:15).

Thus

Moreover

(cid:12)σr(x) − R(x)(cid:12)
(cid:12)

(cid:12) ≤

(cid:12)
(cid:12)σr(x) − ˜R(x)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
˜R(x) − R(x)
(cid:12)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)
˜R(x) − b/2
(cid:12)
(cid:12)

≤ b(cid:15) + 0 + 2(cid:15)

≤ 3b(cid:15).

R(x) = (1 − 2(cid:15)) ˜R(x) + b(cid:15) ≥ (1 − 2(cid:15))(−b(cid:15)) + b(cid:15) ≥ 0,
R(x) ≤ (1 − 2(cid:15))b(1 + (cid:15)) + b(cid:15) ≤ b.

A.2. Remaining deferred proofs

The details of converting a ReLU network into a rational network are as follows.
Lemma A.4. Let f : Rd → R be represented by a ReLU network with ≤ l layers, and with each node computing a map
z (cid:55)→ σr(a(cid:62)z + b) where (cid:107)a(cid:107)1 + |b| ≤ 1. Then for every (cid:15) > 0 there exists a function g : Rd → R with |g(x) − f (x)| ≤ (cid:15)
for (cid:107)x(cid:107)∞ ≤ 1 where g is obtained from f by replacing each ReLU with an r-rational function with r = O(ln(1/(cid:15))2).

Proof of Lemma 1.3. This construction will use the Newman-based approximation R := Rr,b to σr with degree
O(ln(l/(cid:15))2). By Lemma A.3, this degree sufﬁces to guarantee R(x) ∈ [0, 1] and |R(x) − σr(x)| ≤ (cid:15)/l for |x| ≤ 1.

First note, by induction on layers, that the output of every node has absolute value at most 1. The base case is the
inputs themselves, and thus the statement holds by the assumption (cid:107)x(cid:107)∞ ≤ 1. In the inductive step, consider any node
z (cid:55)→ R(a(cid:62)z + b), where z is the multivariate input to this node. By the inductive hypothesis, (cid:107)z(cid:107)∞ ≤ 1, thus

|a(cid:62)z + b| ≤ (cid:107)a(cid:107)1(cid:107)z(cid:107)∞ + |b| ≤ 1.

As such, R(a(cid:62)z + b) ∈ [0, 1].
It remains to prove the error bound. For any node, if h : Rd → R denote the function (of the input x) compute by this node,
then let hR denote the function obtained by replacing all ReLUs with R. It will be shown that every node in layer i has
|hR(x) − h(x)| ≤ i(cid:15)/l when (cid:107)x(cid:107)∞ ≤ 1. The base case is the inputs themselves, and thus there is no approximation error,
meaning the bound holds with error 0 ≤ 1 · (cid:15)/l. Now consider any node in layer i + 1 with i ≥ 0, and suppose the claim
holds for nodes in layers i and lower. For convenience, let H denote the multivalued map computed by the previous layer,

and HR denote the multivalued map obtained by replacing all activations in earlier layers with R. Since σr is 1-Lipschitz,
and since the earlier boundedness property grants

Neural networks and rational functions

(cid:12)
(cid:12)a(cid:62)HR(x) + b
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ (cid:107)a(cid:107)1(cid:107)HR(x)(cid:107)∞ + |b| ≤ 1,

then

|h(x) − hR(x)| =

≤

(cid:12)
(cid:12)
(cid:12)σr(a(cid:62)H(x) + b) − R(a(cid:62)HR(x) + b)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)σr(a(cid:62)H(x) + b) − σr(a(cid:62)HR(x) + b)
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)a(cid:62)H(x) − a(cid:62)HR(x)
(cid:12)
≤ (cid:107)a(cid:107)1(cid:107)H − HR(cid:107)∞ + (cid:15)/l
≤ (i + 1)(cid:15)/l.

(cid:12)
(cid:12)
(cid:12) + (cid:15)/l

≤

(cid:12)
(cid:12)σr(a(cid:62)HR(x) + b) − R(a(cid:62)HR(x) + b)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Next, collapsing a rational network down into a single rational function is proved as follows.
Lemma A.5. Let f : Rd → R be a rational network with ≤ m nodes in each of ≤ l layers, and the activation function
has degree r. Then the rational function obtained by collapsing f has degree at most (rm)l.

Proof. Throughout this proof, let R denote the rational activation function at each node, and write R(x) = p(x)/q(x)
where p and q are polynomials of degree at most r. The proof establishes, by induction on layers, that the nodes of layer i
compute rational functions of degree at most (rm)i. The base case is layer 1, where each node computes a rational function
of degree r ≤ rm. For the case of layer i > 1, ﬁx any node, and denote its computation by h(x) = R((cid:80)n
j=1 ajgj(x) + b),
where n ≤ m and gj = pj/qj is a rational function of degree at most (rm)i−1. Note



deg



(cid:88)

j

ajpj(x)
qj(x)



+ b

 = deg

(cid:32) b (cid:81)

j qj(x) + (cid:80)
j ajpj(x) (cid:81)
(cid:81)
j qj(x)

k(cid:54)=j qk(x)

(cid:33)

≤ m(mr)i−1.

the map f := (cid:80)
is univariate, its numerator p and denominator q have the form p(x) := (cid:80)
fact that q > 0,

j ajgj + b is rational of degree m(mr)i−1. Let pf and qf denote its numerator and denominator. Since R
j≤r djxj. Thus, using the

j≤r cjxj and q(x) (cid:80)

deg(h(x)) = deg(R(f (x))) = deg

j≤r cj(pf (x)/qf (x))j
j≤r dj(pf (x)/qf (x))j

(cid:32)

qf (x)r
qf (x)r

(cid:33)


= deg

j≤r cjpf (x)jqf (x))r−j
j≤r djpf (x)jqf (x))r−j

(cid:33)

≤ rm(rm)i−1 = (rm)i.



(cid:80)



(cid:80)

(cid:32) (cid:80)
(cid:80)

The proof of part 2 of Theorem 1.1 now follows by combining Lemmas 1.3, A.3 and A.5.

The last piece is a slighly more detailed account of Theorem 1.2.

Proof of Theorem 1.2. Let ∆ : R → R denote the triangle function from (Telgarsky, 2015):

∆(x) :=






x ∈ [0, 1/2],
2x
2(1 − x) x ∈ (1/2, 1],
0

otherwise.

Neural networks and rational functions

Deﬁne the target function f = ∆k, which as in (Telgarsky, 2015) has 2k regular-spaced crossings of 0 along [0, 1], and can
be written as a network with 2k layers, each with ≤ 2 nodes.

Next consider the rational function g. As in the text, it is necessary to count the zeros of g − 1/2 (the case g = 1/2 is
trivial). Writing g = p/q, equivalently this means the zeros of 2p − q. Since p and q together have ≤ 2k−2 terms, by
Descartes’ rule of signs, g crosses 1/2 at most 2k−2 times along (0, 1]. Therefore, following a similar calculation to the
proof in (Telgarsky, 2016, Proof of Theorem 1.1),

(cid:90)

(0,1]

|f (x) − g(x)| dx ≥

1 −

(cid:32)

1
32

(cid:33)

2(2k−2)
2k

=

1
64

.

B. Deferred material from Section 3

B.1. Towards the proof of part 1 of Theorem 1.1

To start, the lemmas due to Yarotsky are slightly adjusted to clip the range to [0, 1].

Proof of Lemma 3.1. Inspecting Yarotsky’s proof, the construction provides g(x) with g(0) = 0 and supx∈[0,1] |g(x) −
x2| ≤ (cid:15). To provide the desired f , it sufﬁces to deﬁne f (x) = σr(g(x)) − σr(g(x) − 1).

Proof of Lemma 3.2. First suppose B = 1, let f be as in Lemma 3.1 at resolution (cid:15)/8, and deﬁne h via the polarization
identity (as in Yarotsky’s proof):

(where x/2 appears since f has domain [0, 1]2). Since f (0) = 0,

h(x, y) = 2(f (x/2 + y/2) − f (x/2) − f (y/2))

h(x, 0) = 2(f (x/2) − f (x/2) − 0) = 0,

h(0, y) = 2(f (y/2) − 0 − f (y/2)) = 0.

Moreover, for any x, y ∈ [0, 1]

h(x, y) − xy ≤ 2

(x/2 + y/2)2 + (cid:15)/8 − x2/4 + (cid:15)/8 − y2/4 + (cid:15)/8

− xy ≤ xy + (cid:15),

h(x, y) − xy ≥ 2

(x/2 + y/2)2 − (cid:15)/8 − x2/4 − (cid:15)/8 − y2/4 − (cid:15)/8

− xy ≤ xy − (cid:15).

(cid:16)

(cid:16)

(cid:17)

(cid:17)

Finally, set ˜g(x, y) := σr(h(x, y)) − σr(h(x, y) − 1), which preserves the other properties.

Now consider the case B ≥ 1, and set g(x, y) = B2g(x/B, y/B). Then (x, y) ∈ [0, B]2 implies

(cid:12)
(cid:12)g(x, y) − xy(cid:12)

(cid:12) = B2(cid:12)

(cid:12)˜g(x/B, y/B) − (x/B)(y/B)(cid:12)

(cid:12) ≤ (cid:15)B2,

and g ∈ [0, B2] over [0, B]2 since ˜g ∈ [0, 1] over [0, 1]2.

The full details for the proof of fast exponentiation are as follows.

Proof of Lemma 3.3. This proof constructs a network implementing the russian peasant algorithm for exponentiation:

1. Set v := 1.

2. For b ∈ bits-ltr(y) (the bits of y from left to right):

(a) Set v := v2.
(b) If b = 1, set v := vx.

Neural networks and rational functions

For example,

x101012 = ((((12 · x)2)2 · x)2)2 · x = x24

· x22

· x.

The two lines in the inner loop will use the squaring function f from Lemma 3.1 and the multiplication function g from
Lemma 3.2, each with accuracy c(cid:15) where c := 1/y2. At the end, the network returns σr(v) − σr(v − 1) to ensure the output
lies in [0, 1]; this procedure can not increase the error. Since the loop is invoke O(ln(y)) times and each inner loop requires
a network of size O(ln(1/(c(cid:15)))) = O(ln(y/(cid:15))), the full network has size O(ln(y/(cid:15))2).

It remains to show that the network computes a function h which satisﬁes

h(x) = xy.

|v − xzj | ≤ z2

j c(cid:15).

Let zj denote the integer corresponding to the ﬁrst j bits of y when read left-to-right; it will be shown by induction (on the
bits of y from left to right) that, at the end of the jth invocation of the loop,

This sufﬁces to establish the claim since then |v − xy| ≤ y2c(cid:15) = (cid:15).

For the base case, consider j = 0; then v = 1 = xz0 = x0 as desired. For the inductive step, let w denote v at the end of
the previous iteration, whereby the inductive hypothesis grants

The error after the approximate squaring step can be upper bounded as

|w − xzj−1| ≤ z2

j−1c(cid:15).

The reverse inequality is proved analogously, thus

If the bit b in this iteration is 0, then 2zj−1 = zj and the proof for this loop iteration is complete. Otherwise b = 1, and

f (w) − x2zj−1 ≤

(cid:16)

f (w) − w2(cid:17)
(cid:16)

≤ c(cid:15) +

(xzj−1 + z2

j−1c(cid:15) + z4
j−1c(cid:15) + z2

≤ c(cid:15) + 2z2
≤ c(cid:15) + 2z2
≤ (2zj−1)2c(cid:15).

(cid:16)

+

w2 − x2zj−1

(cid:17)

(cid:17)

j−1c(cid:15))2 − x2zj−1
j−1c2(cid:15)2
j−1c(cid:15)

(cid:12)
(cid:12)f (w) − x2zj−1
(cid:12)

(cid:12)
(cid:12) ≤ (2zj−1)2c(cid:15).
(cid:12)

v − xzj = g(f (w), x) − x2zj−1+b
≤ xf (w) + c(cid:15) − x2zj−1+b

(cid:16)

≤

(2zj−1)2 + 1

c(cid:15)

(cid:17)

≤ (zj)2c(cid:15).

The proof of the reverse inequality is analogous, which establishes the desired error bound on v for this loop iteration.

Using the preceding exponentiation lemma, the proof of polynomial approximation is as follows.

Proof of Lemma 3.4. It will be shown momentarily that a single monomial term can be approximating to accuracy (cid:15)/s with
a network of size O (cid:0)min{r ln(sr/(cid:15)), d ln(dsr/(cid:15))2}(cid:1). This implies the result by summing ≤ s monomials comprising a
polynomial, along with their errors.

For a single monomial, here are two constructions.

Neural networks and rational functions

• One approach is to product together ≤ r individual variables (and lastly multiple by a ﬁxed scalar coefﬁcient), with
no concern of the multiplicities of individual variables. To this end, let (y1, . . . , yk) with s ≤ r denote coordinates
of the input variable so that (cid:81)k
i=1 yi is the desired multinomial. Let g denote multiplication with error (cid:15)0 := (cid:15)/(rs)
as provided by Lemma 3.2. The network will compute αgi(y), where α ∈ [−1, +1] is the scalar coefﬁcient on the
monomial, and gi is recursively deﬁned as

It is established by induction that

g1(y) = y1,

gi+1(y) := f (yi+1, gi(y))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i
(cid:89)

j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

gi(y) −

yj

≤ j(cid:15)0

The base case is immediate since g1(y) = y1 = (cid:81)1

j=1 yj. For the inductive step,

gi+1(y) −

yj = f (yi+1, gi(y)) −

yj ≤ yi+1gi(y) + (cid:15)0 −

yj ≤ yi+1(i(cid:15)0 +

yj) + (cid:15)0 −

yj ≤ (i + 1)(cid:15)0,

i+1
(cid:89)

j=1

i+1
(cid:89)

j=1

i+1
(cid:89)

j=1

i
(cid:89)

j=1

i+1
(cid:89)

j=1

and the reverse inequality is proved analogously.

• Alternatively, the network uses the fast exponentiation routine from Lemma 3.3, and then multiplies together the terms
for individual coordinates. In particular, the exponentiation for each coordinate with accuracy (cid:15)1 := (cid:15)/(ds) requires
a network of size O(ln(r/(cid:15)1)2). By an analysis similar to the preceding construction, multiplying ≤ d such networks
will result in a network approximating the monomial with error (cid:15)/s and size O(d ln(r/(cid:15)1)2).

Next, the proof that ReLU networks can efﬁciently compute reciprocals, namely Lemma 3.5. As stated in the text, it is
ﬁrst necessary to establish Lemma 3.6, which gives computes reciprocals at a choice of magnitude, and then Lemma 3.7,
which combines these circuits across scales.

Proof of Lemma 3.7. For each i ∈ {1, . . . , n}, deﬁne the function

pi(z) :=






z−ai−1
ai−ai−1
ai+1−z
ai+1−ai
0

z ∈ [ai−1, ai],
z ∈ (ai, ai+1],
otherwise.

The functions (pi)n

i=1 have the following properties.

• Each pi can be represented by a ReLU network with three nodes in 2 layers.

• For any x ∈ [a1, an], there exists j ∈ {1, . . . , n} so that i ∈ {j, j + 1} implies pi(x) ≥ 0 and i (cid:54)∈ {j, j + 1} implies

pi(x) = 0. Indeed, it sufﬁces to let j be the smallest element of {1, . . . , n − 1}. satisfying x ∈ [aj, aj+1].

• For any x ∈ [a1, an], (cid:80)n

i=1 pi(x) = 1.

The family (pi)n
necessarily consecutive, are nonzero at any point in the interval.

i=1 thus forms a partition of unity over [a1, an], moreover with the property that at most two elements,

Let h : [0, B]2 → [0, B] be a uniform (cid:15)-approximation via ReLU networks to the multiplication map (x, y) (cid:55)→ xy; by
Lemma 3.2, h has O(ln(B/(cid:15))) nodes and layers, and moreover the multiplication is exact when either input is 0. Finally,
deﬁne g : R → R as

g(x) :=

h(pi(x), gi(x)).

n
(cid:88)

i=1

Neural networks and rational functions

By construction, g is a ReLU network with O(ln(B/(cid:15)) + maxi ki) layers and O(n ln(B/(cid:15)) + (cid:80)

i mi) nodes.

remains to check the approximation properties of g.

It
min (cid:8)j ∈ {1, n − 1} : x ∈ [aj, aj+1](cid:9). Then

Let x ∈ [a1, an] be given, and set j

:=

(cid:12)f (x) − g(x)(cid:12)
(cid:12)

(cid:12) = (cid:12)
≤ (cid:12)

(cid:12)f (x) − h(pj(x), gj(x)) − h(pj+1(x), gj+1(x))(cid:12)
(cid:12)
(cid:12)f (x) − pj(x)gj(x) − pj+1(x)gj+1(x)(cid:12)
(cid:12)
(cid:12)pj+1(x)gj+1(x) − h(pj+1(x), gj+1(x))(cid:12)
(cid:12) +(cid:12)
(cid:12)pj(x)gj(x) − h(pj(x), gj(x))(cid:12)
(cid:12)
(cid:12)f (x) − gj(x)(cid:12)

(cid:12)f (x) − gj+1(x)(cid:12)

(cid:12) + pj+1(x)(cid:12)

(cid:12) + (cid:15) + (cid:15)

+(cid:12)
≤ pj(x)(cid:12)
≤ pj(x)(cid:15) + pj+1(x)(cid:15) + 2(cid:15).

Proof of Lemma 3.6. Set c := 1/b and r := (cid:100)b ln(1/((cid:15)a))/a(cid:101) and (cid:15)0 := (cid:15)/(r2c). For i ∈ {0, . . . , r}, let hi : [0, 1] → [0, 1]
denote a ReLU network (cid:15)0-approximation to the map x (cid:55)→ xi; by Lemma 3.3, hi has O(ln(1/(cid:15)0)2 nodes and layers. Deﬁne
q : [0, 1] → R as

q(x) := c

hi(1 − cx).

r
(cid:88)

i=0

By construction, q is a ReLU network with O(r ln(1/(cid:15)0)2) nodes and O(ln(1/(cid:15)0)2) layers.

For the approximation property of q, let x ∈ [a, b] be given, and note

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
x

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

r
(cid:88)

i=0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

r
(cid:88)

i=0

q(x) −

≤

q(x) − c

(1 − cx)i

+

c

(1 − cx)i −

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
x

r
(cid:12)
(cid:88)
(cid:12)

≤ c

(cid:12)hi(1 − cx) − (1 − cx)i(cid:12)
(cid:12)
(cid:12) +

(1 − cx)i −

c
1 − (1 − cx)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

c

r
(cid:88)

i=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞
(cid:88)

i=0

≤ (cid:15) +

c

(1 − cx)i − c

(1 − cx)i

i=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

r
(cid:88)

i=0

∞
(cid:88)

= (cid:15) + c

(1 − cx)i

i=r+1
c(1 − cx)r+1)
1 − (1 − cx)
exp(−cx(r + 1))
x

exp(−car)
a

= (cid:15) +

≤ (cid:15) +

≤ (cid:15) +

≤ (cid:15) + (cid:15).

Proof of Lemma 3.5. Set (cid:15)0 := (cid:15)/3. For i ∈ {1, . . . , k}, Let ˜qi denote the ReLU network (cid:15)0-approximation to 1/x
along [2−i, 2−i+1]; by Lemma 3.6, ˜qi has O(k2 ln(1/(cid:15))2) layers and O(k3 ln(1/(cid:15))3) nodes. Furthermore, set qi
:=
max{2i, min{0, ˜qi}}, which has the same approximation and size properties of ˜qi. Applying Lemma 3.7 with B := 2k
i=1, it follows that there exists q : R → R which
and reals ai := 2i−k−1 for i ∈ {0, . . . , k + 2} and functions (qi)k
(cid:15)-approximates 1/x along [2−k, 1] with size O(k2 ln(1/(cid:15)) + k4 ln(1/(cid:15))3) and depth O(k ln(1/(cid:15)) + k2 ln(1/(cid:15))2).

Putting the pieces together gives the proof of the second part of the main theorem.

Neural networks and rational functions

Proof of part 1 of Theorem 1.1. Deﬁne (cid:15)0 := (cid:15)/22k+3, and use Lemmas 3.2, 3.4 and 3.5 to choose ReLU network ap-
proximations fp and fq to p and q at resolution (cid:15)0, as well as ReLU network f for multiplication along [0, 1]2 and g to
approximate x (cid:55)→ 1/x along [2−k−1, 1], again at resolution (cid:15)0. The desired network will compute the function h, deﬁned
as

h(x) := 2k+1f (fp(x), 2−k−1g(fq(x))).

Combining the size bounds from the preceding lemmas, h itself has size bound

(cid:18)

O

min

(cid:110)

sr ln(sr/(cid:15)0), sd ln(dsr/(cid:15)0)2(cid:111)(cid:19)

+ O (cid:0)ln(1/(cid:15)0)(cid:1) + O

(cid:16)

k4 ln(1/(cid:15)0)3(cid:17)

(cid:18)

= O

min

srk ln(sr/(cid:15)), sdk2 ln(dsr/(cid:15))2(cid:111)
(cid:110)

+ k7 ln(1/(cid:15))3

.

(cid:19)

Before verifying the approximation guarantee upon h, it is necessary to verify that the inputs to f and g are of the correct
magnitude, so that Lemmas 3.2 and 3.5 may be applied. Note ﬁrstly that g(fq(x)) ∈ [1, 2k+1], since q(x) ∈ [2−k, 1]
implies fq(x) ∈ [2−k − (cid:15)0, 1] ⊆ [2−k−1, 1]. Thus 2−k−1g(fq(x)) ∈ [0, 1], and so both arguments to f within the
deﬁnition of h are within [0, 1]. Consequently, the approximation guarantees of Lemmas 3.2, 3.4 and 3.5 all hold, whereby

h(x) −

= 2k+1f

fp(x), 2−k−1g(fq(x))

−

(cid:16)

p(x)
q(x)

(cid:17)

p(x)
q(x)

≤ fp(x)g(fq(x)) −

+ 2k+1(cid:15)0

p(x)
q(x)

≤

≤

≤

=

−

−

p(x)
q(x)

p(x)
q(x)

+ fp(x)(cid:15)0 + 2k+1(cid:15)0

+ fp(x)(cid:15)0 + 2k+1(cid:15)0

fp(x)
fq(x)
p(x) + (cid:15)0
q(x) − (cid:15)0
p(x)q(x) + q(x)(cid:15)0 − p(x)q(x) + p(x)(cid:15)0
q(x)(q(x) − (cid:15)0)
p(x)(cid:15)0
q(x)(q(x) − (cid:15)0)

(cid:15)0
q(x) − (cid:15)0

+

≤ 2k+1(cid:15)0 + 22k+1(cid:15)0 + fp(x)(cid:15)0 + 2k+1(cid:15)0
≤ (cid:15).

+ fp(x)(cid:15)0 + 2k+1(cid:15)0

+ fp(x)(cid:15)0 + 2k+1(cid:15)0

The proof of the reverse inequality is analogous.

B.2. Proof of Proposition 1.5

Proof of Proposition 1.5. By (Telgarsky, 2015, Lemma 2.1), a ReLU network g with at most m nodes in each of at most
l layers computes a function which is afﬁne along intervals forming a partition of R of cardinality at most N (cid:48) ≤ (2m)l.
Further subdivide this collection of intervals at any point where g intersects f (x) = 1/x; since f is convex and g is
afﬁne within each existing piece of the subdivision, then the number of intervals is at most three times as large as before.
Together, the total number of intervals N (cid:48)(cid:48) now satisﬁes N (cid:48)(cid:48) ≤ 3(2m)l. Finally, intersect the family of intervals with
[1/2, 3/4], obtaining a ﬁnal number of intervals N ≤ 3(2m)l.

Let (U1, . . . , UN ) denote this ﬁnal partition of [1/2, 3/4], and let (δ1, . . . , δN ) denote the corresponding interval lengths.
Let S ⊆ {1, . . . , N } index the subcollection of intervals with length at least 1/(8N ), meaning S := {j ∈ {1, . . . , N } :
δj ≥ 1/(8N )}. Then

(cid:88)

j∈S

1
4

(cid:88)

j(cid:54)∈S

δj =

−

δj >

−

1
4

N
8N

=

1
8

.

Consider now any interval Uj with endpoints {a, b}. Since 1/2 ≤ a < b ≤ 3/4, then f satisﬁes 128/27 ≤ f (cid:48)(cid:48) ≤ 16. In
order to control the difference between f and g along Uj, consider two cases: either f ≥ g along this interval, or f ≤ g
along this interval (these are the only two cases due to the subdivisions above).

Neural networks and rational functions

• If f ≥ g, then g can be taken to be a tangent to f at some point along the interval [a, b] (otherwise, the distance can
always be only decreased by moving g up to be a tangent). Consequently, g(x) := f (c) + f (cid:48)(c)(x − c) for some
c ∈ [a, b], and by convexity and since f (cid:48)(cid:48) ≥ 128/27 over this interval,

(cid:90) b

a

|f (x) − g(x)| dx ≥ min
c∈[a,b]

(f (c) + f (cid:48)(c)(x − c) + f (cid:48)(cid:48)(b)(x − c)2/2) − (f (c) + f (cid:48)(c)(x − c))

dx

(cid:17)

(cid:90) b

(cid:16)

a
(cid:90) b

a

(cid:32)

≥

=

=

64
min
81
α∈[0,1]
16(b − a)3
81

.

= min
c∈[a,b]

f (cid:48)(cid:48)(b)(x − c)2/2 dx

64
27

min
c∈[a,b]

(b − c)3 − (a − c)3
3

(cid:33)

.

(cid:16)

(α(b − a))3 + ((1 − α)(b − a))3(cid:17)

• On the other hand, if g ≥ f , then g passes above the secant line h between (a, f (a)) and (b, f (b)). The area between
f and g is at least the area between f and h, and this latter area is bounded above by a triangle of width (b − a) and
height

f (a) + f (b)
2

− f (cid:0)(a + b)/2(cid:1) =

(cid:19)

−

1
a + b

+

1
b

1
2

(cid:18) 1
a
1
2ab(a + b)
1
2ab(a + b)
3/2
4

.

=

≥

≥

(cid:0)b(a + b) + a(a + b) − ab(cid:1)

(cid:0)b(a + b) + a(a + b) − ab(cid:1)

Combining this with b − a ≤ 1/4, the triangle has area at least 3(b − a)/16 ≥ 3(b − a)3.

Combining these two cases and summing across the intervals of S (where j ∈ Sj implies δj ≥ 1/(8N )),

(cid:90)

[1/2,3/4]

|f (x) − g(x)| dx ≥

|f (x) − g(x)| dx

(cid:90)

(cid:88)

Uj

δ3
j
6

j∈S

(cid:88)

j∈S

1
6(8N )2

(cid:88)

j∈S

δj

1
27648(2m)2l .

≥

≥

≥

If m < (27648(cid:15))−1/(2l)/2, then

(cid:90)

[1/2,3/4]

|f (x) − g(x)| dx ≥

1

27648(2m)2l > (cid:15).

