Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

A. Additional Proofs

A.1. Proof of Thm. 2

This proof bears resemblance to the proof provided in Eldan & Shamir (2016)[Lemma 10], albeit once approximating
(cid:107)x(cid:107)2
2, the following construction takes a slightly different route. For completeness, we also state assumptions 1 and 2 from
Eldan & Shamir (2016):

Assumption 1. Given the activation function σ, there is a constant cσ ≥ 1 (depending only on σ) such that the following
holds: For any L-Lipschitz function f : R → R which is constant outside a bounded interval [−R, R], and for any δ, there
RL
exist scalars a, {αi, βi, γi}w
δ , such that the function

i=1, where w ≤ cσ

satisﬁes

h(x) = a +

αi · σ(βix − γi)

w
(cid:88)

i=1

|f (x) − h(x)| ≤ δ.

sup
x∈R

As discussed in Eldan & Shamir (2016), this assumption is satisﬁed by ReLU, sigmoid, threshold, and more generally all
standard activation functions we are familiar with.

Assumption 2. The activation function σ is (Lebesgue) measurable and satisﬁes

for all x ∈ R and for some constants C, α > 0.

Proof. Consider the 4-Lipschitz function

which is constant outside [−2, 2], as well as the function

|σ (x)| ≤ C (1 + |x|α)

l (x) = min (cid:8)x2, 4(cid:9) ,

on Rd. Applying assumption 1, we obtain a function ˜l(x) having the form a + (cid:80)w

i=1 αiσ (βix − γi) so that

and where the width parameter w is at most 8cσd

. Consequently, the function

δ

can be expressed in the form a + (cid:80)w

, yielding an approximation satisfying

We now invoke assumption 1 again to approximate the 1-Lipschitz function

(cid:96) (x) =

l(xi) =

min (cid:8)x2

i , 4(cid:9)

d
(cid:88)

i=1

d
(cid:88)

i=1

(cid:12)
˜l (x) − l (x)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤

δ
d

,

sup
x∈R

˜(cid:96) (x) =

˜l (xi)

d
(cid:88)

i=1

i=1 αiσ (βix − γi) where w ≤ 8cσd2
(cid:12)
(cid:12)
(cid:12)

(cid:12)
˜(cid:96) (x) − (cid:96) (x)
(cid:12)
(cid:12) ≤ δ.

δ

sup
x∈Rd

f (x) =






x < −0.5

0
x + 0.5 x ∈ [−0.5, 0.5]
1

x > 0.5

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

and obtain an approximation ˜f (x) = ˜a + (cid:80) ˜w

i=1 ˜αiσ

(cid:16) ˜βix − ˜γi

(cid:17)

satisfying

(cid:12)
(cid:12)
(cid:12)

(cid:12)
˜f (x) − f (x)
(cid:12)
(cid:12) ≤

sup
x∈R

(cid:114)

δ
2

where ˜w ≤ cσ

(cid:112)1/2δ.

Now consider the composition ˜f ◦

cµ · ˜(cid:96) − cµ

(cid:16)

(cid:17)

, where cµ > 0 is to be determined later. This composition has the form

w
(cid:88)

i=1



w
(cid:88)

j=1

a +

uiσ



vi,jσ ((cid:104)wi,j, x(cid:105) + bi,j) + ci





for appropriate scalars a, ui, ci, vi,j, bi,j and vectors wi,j, and where w is at most max
left to bound the approximation error obtained by ˜f ◦

. Deﬁne for any (cid:15) > 0,

cµ · ˜(cid:96) − cµ

(cid:17)

(cid:16)

(cid:110)

8cσd2/δ, cσ

(cid:112)1/2δ

(cid:111)

. It is now

Since µ is continuous, there exists (cid:15) > 0 such that

(cid:110)

R(cid:15) =

x ∈ Rd : 1 − (cid:15) ≤ (cid:107)x(cid:107)2

2 ≤ 1 + (cid:15)

(cid:111)

.

(cid:90)

R(cid:15)

µ (x) dx ≤

δ
4

.

Now, for any x ∈ Rd such that 1 + (cid:15) ≤ (cid:107)x(cid:107)2

2 we have

˜(cid:96) (x) ≥ (cid:96) (x) − δ = min

(cid:110)

(cid:111)

(cid:107)x(cid:107)2

2 , 4

− δ ≥ 1 + (cid:15) − δ.

Assuming δ < (cid:15)/2, we have the above is at least

1 + (cid:15)/2.

Taking cµ = 1/(cid:15), we get

and thus

cµ · ˜(cid:96) (x) − cµ = cµ ·

(cid:17)
(cid:16)˜(cid:96) (x) − 1

≥

cµ(cid:15)
2

= 0.5,

(cid:32)

(cid:114)

(cid:114)

(cid:33)

˜f

(cid:16)

(cid:17)

cµ · ˜(cid:96) (x) − cµ

δ
2
2. A similar argument shows that for any x ∈ R satisfying (cid:107)x(cid:107)2

, 1 +

1 −

δ
2

∈

,

2 ≤ 1 − (cid:15) we have

(cid:16)

˜f

cµ · ˜(cid:96) (x) − cµ

∈

−

(cid:17)

(cid:32)

(cid:114)

(cid:114)

(cid:33)

δ
2

,

δ
2

.

For any x ∈ Rd satisfying 1 + (cid:15) ≤ (cid:107)x(cid:107)2

Combining both Eq. (7) and Eq. (8) we obtain

(6)

(7)

(8)

cµ · ˜(cid:96) (x) − cµ

− 1 ((cid:107)x(cid:107)2 ≤ 1)

µ (x) dx

(cid:17)

(cid:17)

(cid:17)2

(cid:17)2

=

cµ · ˜(cid:96) (x) − cµ

− 1 ((cid:107)x(cid:107)2 ≤ 1)

µ (x) dx

(cid:16)

(cid:16) ˜f

cµ · ˜(cid:96) (x) − cµ

(cid:17)

− 1 ((cid:107)x(cid:107)2 ≤ 1)

µ (x) dx

(cid:17)2

≤

2µ (x) dx +

µ (x) dx

Rd\R(cid:15)

(cid:90)

δ
4

Rd\R(cid:15)

(cid:90)

(cid:90)

Rd

R(cid:15)

(cid:16)

(cid:16) ˜f

(cid:16)

(cid:16) ˜f
(cid:90)

+

R(cid:15)

(cid:90)

δ
2

δ
2

≤

+

= δ,

Where the ﬁrst summand in the penultimate inequality is justiﬁed due to ˜f being bounded in the interval
(cid:104)
−(cid:112)δ/2, 1 + (cid:112)δ/2
2, and the second summand justiﬁed due to Equations
(7) and (8), concluding the proof of the lemma.

by Eq. (6), and assuming 1 + (cid:112)δ/2 ≤

√

(cid:105)

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

A.2. Proof of Thm. 3

Consider an input distribution of the form

x = srv,

where v is drawn from a certain distribution on the unit L1 sphere {x : (cid:107)x(cid:107)1 = 1} to be speciﬁed later, and s is uniformly
distributed on [1, 1 + (cid:15)].

Let

F (x) =

aj [(cid:104)wj, x(cid:105) + bj]+

N
(cid:88)

j=1

be a 2-layer ReLU network of width N , such that with respect to the distribution above,

Ex

(cid:2)(f ((cid:107)x(cid:107)1) − F (x))2(cid:3) = Ev

(cid:2)Es

(cid:2)(f (sr) − F (srv))2(cid:12)

(cid:12)v(cid:3)(cid:3) ≤

By Markov’s inequality, this implies

1
2
(cid:12)v(cid:3) ≤ δ(cid:15) only if ˜fN
By the assumption on f , and the fact that s is uniform on [1, 1 + (cid:15)], we have that Es
is not a linear function on the line between rv and (1 + (cid:15))rv. In other words, this line must be crossed by the hyperplane
{x : (cid:104)wj, x(cid:105) + bj = 0} for some neuron j. Thus, we must have

(cid:2)f (sr) − F (srv)2(cid:12)

(cid:2)f (sr) − F (srv)2(cid:12)

(cid:12)v(cid:3) ≤ δ(cid:15)

(cid:1) ≥

(cid:0)Es

Prv

.

Prv (∃j ∈ {1, . . . , N }, s ∈ [1, 1 + (cid:15)] s.t. (cid:104)wj, srv(cid:105) + bj = 0) ≥

(9)

The left hand side equals

δ(cid:15)
2

.

1
2

.

Prv (∃j ∈ {1 . . . N }, s ∈ [1, 1 + (cid:15)] s.t. (cid:104)wj, v(cid:105) = −bj/sr)

= Prv

∃j ∈ {1 . . . N } s.t. (cid:104)wj, v(cid:105) between −

and −

(cid:18)

(cid:18)

≤ Prv

∃j ∈ {1 . . . N } s.t. (cid:104)wj, v(cid:105) betwen −

and − (1 − (cid:15))

≤

Prv

(cid:104)wj, v(cid:105) between −

and − (1 − (cid:15))

bj
r

(cid:18)

N
(cid:88)

j=1

(cid:18)

≤ N ·

sup
w∈Rd,b∈R

Prv

(cid:104)w, v(cid:105) between −

and − (1 − (cid:15))

b
r

(cid:19)

b
r

(cid:19)

bj
(1 + (cid:15))r
bj
r

(cid:19)

bj
r

bj
r

(cid:19)

bj
r

= N ·

sup
w∈Rd,b∈R

Prv

b

(cid:18)(cid:28) −rw

(cid:29)

(cid:19)

, v

∈ [1 − (cid:15), 1]

= N · sup
w∈Rd

Prv ((cid:104)w, v(cid:105) ∈ [1 − (cid:15), 1]) ,

where in the ﬁrst inequality we used the fact that
union bound. Combining these inequalities with Eq. (9), we get that

1
1+(cid:15) ≥ 1 − (cid:15) for all (cid:15) ∈ (0, 1), and in the second inequality we used a

N ≥

1
supw Prv ((cid:104)w, v(cid:105) ∈ [1 − (cid:15), 1])

.

As a result, to prove the theorem, it is enough to construct a distribution for v on the on the unit L1 ball, such that for any
w,

Prv ((cid:104)w, v(cid:105) ∈ [1 − (cid:15), 1]) ≤ ˜O((cid:15) + exp(−Ω(d)))

(10)

By the inequality above, we would then get that N = ˜Ω(min{1/(cid:15), exp(Ω(d))}).

Speciﬁcally, consider a distribution over v deﬁned as follows: First, we sample σ ∈ {−1, +1}d uniformly at random, and
n ∈ Rd from a standard Gaussian distribution, and deﬁne

ˆv =

σ + cd

I −

(cid:18)

(cid:19)

(cid:19)

n

,

σσ(cid:62)

1
d

(cid:18)

1
d

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

where cd > 0 is a parameter dependent on d to be determined later. It is easily veriﬁed that (cid:104)σ/d, ˆv(cid:105) = (cid:104)σ/d, σ/d(cid:105)
independent of n, hence ˆv lies on the hyperplane containing the facet of the L1 ball on which σ/d resides. Calling this
facet Fσ, we deﬁne v to have the same distribution as ˆv, conditioned on ˆv ∈ Fσ.

We begin by arguing that

To see this, let A = {x : (cid:104)w, x(cid:105) ∈ [1 − (cid:15), 1]}, and note that the left hand side equals

Prv ((cid:104)w, v(cid:105) ∈ [1 − (cid:15), 1]) ≤ 2 · Prˆv ((cid:104)w, ˆv(cid:105) ∈ [1 − (cid:15), 1]) .

(11)

Pr(v ∈ A) = Eσ[Pr(v ∈ A|σ)] = Eσ [Pr (ˆv ∈ A|σ, ˆv ∈ Fσ)]

= Eσ

(cid:20) Pr(ˆv ∈ A ∩ Fσ|σ)
Pr(ˆv ∈ Fσ|σ)

(cid:21)

≤

1
minσ Pr(ˆv ∈ Fσ|σ)

Eσ [Pr(ˆv ∈ A|σ)]

=

Pr(ˆv ∈ A)
minσ Pr(ˆv ∈ Fσ|σ)

.

Therefore, to prove Eq. (11), it is enough to prove that Pr(ˆv ∈ Fσ|σ) ≥ 1/2 for any σ. As shown earlier, ˆv lies on the
hyperplane containing Fσ, the facet of the L1 ball in which σ/d resides. Thus, ˆv can be outside Fσ, only if at least one
of its coordinates has a different sign than σ. By deﬁnition of ˆv, this can only happen if (cid:13)
(cid:13)∞ ≥ 1. The
probability of this event (over the random draw of n) equals

(cid:13)cd(I − σσ(cid:62)/d)n(cid:13)

(cid:18)

Pr

max
j∈{1...d}

(cid:18)

(cid:12)
(cid:12)
cd
(cid:12)
(cid:12)

1
d

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

nj −

(cid:104)σ, n(cid:105) σj

≥ 1

= Pr

max
j∈{1...d}

nj − σj ·

(cid:19)

(cid:32)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
d

d
(cid:88)

i=1

σini

≥

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

.

1
cd

Since σi ∈ {−1, 1} for all i, the event on the right hand side can only occur if |nj| ≥ 1/2cd for some j. Recalling that
each nj has a standard Gaussian distribution, this probability can be upper bounded by

(cid:18)

Pr

max
j∈{1...d}

|nj| ≥

(cid:19)

1
2cd

(cid:18)

(cid:19)

1
2cd

(cid:18)

(cid:19)

1
2cd

≤ d · Pr

|n1| ≥

= 2d · Pr

n1 ≥

≤ 2d · exp

−

(cid:18)

(cid:19)

,

1
4c2
d

where we used a union bound and a standard Gaussian tail bound. Thus, by picking

(cid:115)

cd =

1
4 log(4d)

,

we can ensure that the probability is at most 1/2, hence proving that Pr(ˆv ∈ Fσ|σ) ≥ 1/2 and validating Eq. (11).

With Eq. (11) in hand, we now turn to upper bound

Pr ((cid:104)w, ˆv(cid:105) ∈ [1 − (cid:15), 1]) = Pr

(cid:28)

(cid:18) 1
d

(cid:18)

(cid:19)

(cid:29)

(cid:19)

w, σ + cd

I −

σσ(cid:62)

n

∈ [1 − (cid:15), 1]

.

1
d

By the equation above, we have that conditioned on σ, the distribution of (cid:104)w, ˆv(cid:105) is Gaussian with mean (cid:104)σ, w(cid:105) /d and
variance

(cid:18)

c2
d2 · w(cid:62)
d

1
d

(cid:19)2

I −

σσ(cid:62)

w =

(cid:107)w(cid:107)2 −

(cid:32)

c2
d
d2 ·

(cid:33)

(cid:104)w, σ(cid:105)2
d

=

(cid:18) cd (cid:107)w(cid:107)
d

(cid:19)2

(cid:32)

·

1 −

(cid:28) w
(cid:107)w(cid:107)

1
d

(cid:29)2(cid:33)

, σ

.

By Hoeffding’s inequality, we have that for any t > 0,
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:104)σ, w(cid:105)
d

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

Prσ

> t ·

(cid:19)

(cid:107)w(cid:107)
d

≤ 2 exp(−2t2)

and

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:28) w
(cid:107)w(cid:107)

, σ

>

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:114)

(cid:33)

d
2

Prσ

≤ 2 exp(−d).

This means that with probability at least 1 − 2 exp(−d) − 2 exp(−2t2) over the choice of σ, the distribution of (cid:104)w, ˆv(cid:105)

(conditioned on σ) is Gaussian with mean bounded in absolute value by t (cid:107)w(cid:107) /d, and variance of at least
(cid:0)1 − 1

. To continue, we utilize the following lemma:

(cid:1) = 1

(cid:17)2

(cid:16) cd(cid:107)w(cid:107)
d

2

d · d

2

(cid:16) cd(cid:107)w(cid:107)
d

(cid:17)2

·

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

Lemma 1. Let n be a Gaussian random variable on R with mean µ and variance v2 for some v > 0. Then for any
(cid:15) ∈ (0, 1),

Pr (n ∈ [1 − (cid:15), 1]) ≤

· max

1,

(cid:114) 2
π

(cid:26)

(cid:27)

|µ|
v

·

(cid:15)
1 − (cid:15)

.

Proof. Since the probability can only increase if we replace the mean µ by |µ|, we will assume without loss of generality
that µ ≥ 0.

By deﬁnition of a Gaussian distribution, and using the easily-veriﬁed fact that exp(−z2) ≤ min{1, 1/z} for all z ≥ 0, the
probability equals

√

1
2πv2

(cid:90) 1

1−(cid:15)

(cid:18)

exp

−

(cid:19)

(x − µ)2
v2

dx ≤

(cid:26)

√

(cid:15)
2πv2
v
|x − µ|

(cid:27)

· max
x∈[1−(cid:15),1]

min

1,

· max
x∈[1−(cid:15),1]

· max
x∈[1−(cid:15),1]

1
max{v, |x − µ|}
max{1, µ
v }
max{µ, |x − µ|}

=

=

≤

=

≤

√

(cid:15)
2πv2
(cid:15)
√
2π
(cid:15)
√
2π

· max
x∈[1−(cid:15),1]

exp

−

(cid:18)

(cid:19)

(x − µ)2
v2

=

(cid:15)
√
2π

· max

,

(cid:27)

min

· max
x∈[1−(cid:15),1]

(cid:26) 1
1
|x − µ|
v
max{1, µ
v }
max{1, µ
v } · max{v, |x − µ|}
max{1, µ
v }
max{µ, minx∈[1−(cid:15),1] |x − µ|}

x∈[1−(cid:15),1]

·

(cid:15)
√
2π
(cid:15)
√
2π

A simple case analysis reveals that the denominator is at least2 1−(cid:15)

2 , from which the result follows.

Using this lemma and the previous observations, we get that with probability at least 1 − 2 exp(−d) − 2 exp(−2t2) over
the choice of σ,

Pr((cid:104)w, ˆv(cid:105) ∈ [1 − (cid:15), 1]|σ) ≤

· max

1,

(cid:114) 2
π
(cid:114) 2
π

(cid:26)

(cid:26)

t (cid:107)w(cid:107) /d
√

(cid:27)

·

(cid:15)
1 − (cid:15)

2d

cd (cid:107)w(cid:107) /
(cid:27)

t
√

cd

2

·

(cid:15)
1 − (cid:15)

.

=

· max

1,

Letting E be the event that σ is such that this inequality is satisﬁed (and noting that its probability of non-occurence is at
most 2 exp(−d) + 2 exp(−2t2)), we get overall that

Pr((cid:104)w, ˆv(cid:105) ∈ [1 − (cid:15), 1]) = Pr(E) · Pr((cid:104)w, ˆv(cid:105) ∈ [1 − (cid:15), 1]|E) + Pr(¬E) · Pr((cid:104)w, ˆv(cid:105) ∈ [1 − (cid:15), 1]|¬E)

≤ 1 · Pr((cid:104)w, ˆv(cid:105) ∈ [1 − (cid:15), 1]|E) + Pr(¬E) · 1

(cid:114) 2
π

≤

· max

1,

(cid:26)

(cid:27)

t
√

cd

2

·

(cid:15)
1 − (cid:15)

+ 2 exp(−d) + 2 exp(−2t2).

Recalling Eq. (11) and the deﬁnition of cd, we get that

Pr((cid:104)w, v(cid:105) ∈ [1 − (cid:15), 1]) ≤

(cid:114) 8
π

· max

(cid:111)
(cid:110)
1, t · (cid:112)2 log(4d)

·

(cid:15)
1 − (cid:15)

+ 2 exp(−d) + 2 exp(−2t2).

Picking t =

(cid:113) 1

2 log (cid:0) 1−(cid:15)

(cid:15)

(cid:1), we get the bound

(cid:32)(cid:114) 8
π

(cid:40)

(cid:115)

(cid:41)

(cid:33)

(cid:19)

(cid:18) 1 − (cid:15)
(cid:15)

(cid:15)
1 − (cid:15)

· max

1,

log

log(4d)

+ 2

·

+ 2 exp(−d) = ˜O ((cid:15) + exp(−d)) .

This justiﬁes Eq. (10), from which the result follows.

2If µ ∈ [1 − (cid:15), 1], then we get max{µ, 0} = µ ≥ 1 − (cid:15). If µ > 1, we get max{µ, µ − 1} > 1 ≥ 1 − (cid:15). If µ < 1 − (cid:15), we get

max{µ, 1 − (cid:15) − µ} ≥ (1 − (cid:15))/2.

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

A.3. Proof of Thm. 4

The proof rests largely on the following key result:
Theorem 7. Let Gn be the family of piece-wise linear functions on the domain [0, 1] comprised of at most n linear segments.
n be the family of piece-wise linear functions on the domain [0, 1]d, with the property that for any g ∈ Gd
Let Gd
n and any
afﬁne transformation h : R → Rd, g ◦ h ∈ Gn. Suppose f : [0, 1]d → R is C 2. Then for all λ > 0

(cid:90)

inf
g∈Gd
n

[0,1]d

(f − g)2dµd ≥

c · λ2 · σλ(f )5
n4

,

where c = 5

4096 .

Thm. 7 establishes that the error of a piece-wise linear approximation of a C 2 function cannot decay faster than quartically
in the number of linear segments of any one-dimensional projection of the approximating function. Note that this result is
stronger than a bound in terms of the total number of linear regions in Rd, since that number can be exponentially higher
(in the dimension) than n as deﬁned in the theorem.

Before proving Thm. 7, let us explain how we can use it to prove Thm. 4. To that end, we use the result in Telgarsky (2016,
Lemma 3.2), of which the following is an immediate corollary:
Corollary 2. Let N d
maximal width m. Then

m,l denote the family of ReLU neural networks receiving input of dimension d and having depth l and

N d

m,l ⊆ Gd

(2m)l .

Combining this corollary with Thm. 7, the result follows. The remainder of this subsection will be devoted to proving
Thm. 7.

A.3.1. SOME TECHNICAL TOOLS

Deﬁnition 2. Let Pi denote the ith Legendre Polynomial given by Rodrigues’ formula:

These polynomials are useful for the following analysis since they obey the orthogonality relationship

Pi (x) =

1
2ii!

di
dxi

(cid:104)(cid:0)x2 − 1(cid:1)i(cid:105)

.

(cid:90) 1

−1

Pi (x) Pj (x) dx =

2
2i + 1

δij.

Since we are interested in approximations on small intervals where the approximating function is linear, we use the change
of variables x = 2
[a, a + (cid:96)] with respect to the L2 norm. The ﬁrst few polynomials of this family are given by

(cid:96) a − 1 to obtain an orthogonal family

of shifted Legendre polynomials on the interval

(cid:96) t − 2

(cid:110) ˜Pi

(cid:111)∞

i=1

˜P1 (x) =

˜P0 (x) = 1
2
(cid:96)
6
(cid:96)2 x2 −

˜P2 (x) =

x −

(cid:19)

a + 1

(cid:18) 2
(cid:96)
(cid:18) 12a

(cid:96)2 +

(cid:19)

6
(cid:96)

x +

(cid:18) 6a2

(cid:96)2 +

6a
(cid:96)

(cid:19)

+ 1

.

The shifted Legendre polynomials obey the orthogonality relationship

a
Deﬁnition 3. We deﬁne the Fourier-Legendre series of a function f : [a, a + (cid:96)] → R to be

(cid:90) a+(cid:96)

˜Pi (x) ˜Pj (x) dx =

(cid:96)
2i + 1

δij.

f (x) =

˜ai ˜Pi (x) ,

∞
(cid:88)

i=0

(12)

(13)

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

where the Fourier-Legendre Coefﬁcients ˜ai are given by

Theorem 8. A generalization of Parseval’s identity yields

˜ai =

(cid:90) a+(cid:96)

2i + 1
(cid:96)

a

˜Pi (x) f (x) dx.

(cid:107)f (cid:107)2
L2

= (cid:96)

∞
(cid:88)

i=0

˜a2
i
2i + 1

.

Deﬁnition 4. A function f is λ-strongly convex if for all w, u and α ∈ (0, 1),

f (αw + (1 − α)u) ≤ αf (w) + (1 − α)f (u) −

λ
2

α(1 − α) (cid:107)w − u(cid:107)2
2 .

A function is λ-strongly concave, if −f is λ-strongly convex.

A.3.2. ONE-DIMENSIONAL LOWER BOUNDS

We begin by proving two useful lemmas; the ﬁrst will allow us to compute the error of a linear approximation of one-
dimensional functions on arbitrary intervals, and the second will allow us to infer bounds on the entire domain of approxi-
mation, from the lower bounds we have on small intervals where the approximating function is linear.
Lemma 2. Let f ∈ C 2. Then the error of the optimal linear approximation of f denoted P f on the interval [a, a + (cid:96)]
satisﬁes

(cid:107)f − P f (cid:107)2
L2

= (cid:96)

∞
(cid:88)

i=2

˜a2
i
2i + 1

.

P f = ˜a0 ˜P0 (x) + ˜a1 ˜P1 (x) ,

Proof. A standard result on Legendre polynomials is that given any function f on the interval [a, a + (cid:96)], the best linear
approximation (w.r.t. the L2 norm) is given by

where ˜P0, ˜P1 are the shifted Legendre polynomials of degree 0 and 1 respectively, and ˜a0, ˜a1 are the ﬁrst two Fourier-
Legendre coefﬁcients of f as deﬁned in Eq. (3). The square of the error obtained by this approximation is therefore

(14)

(15)

(cid:107)f − P f (cid:107)2 = (cid:107)f (cid:107)2 − 2 (cid:104)f, P f (cid:105) + (cid:107)P f (cid:107)2

(cid:32) ∞
(cid:88)

i=0

˜a2
i
2i + 1

(cid:18)

− 2

˜a2
0 +

(cid:19)

˜a2
1
3

+ ˜a2

0 +

(cid:33)

˜a2
1
3

= (cid:96)

= (cid:96)

∞
(cid:88)

i=2

˜a2
i
2i + 1

.

Where in the second equality we used the orthogonality relationship from Eq. (13), and the generalized Parseval’s identity
from Eq. (14).

Lemma 3. Suppose f : [0, 1] → R satisﬁes (cid:107)f − P f (cid:107)2
L2
[0, 1]. Then

≥ c(cid:96)5 for some constant c > 0, and on any interval [a, a + (cid:96)] ⊆

(cid:90) 1

0

inf
g∈Gn

(f − g)2dµ ≥

c
n4 .

Proof. Let g ∈ Gn be some function, let a0 = 0, a1, . . . , an−1, an = 1 denote its partition into segments of length
(cid:96)j = aj − aj−1, where g is linear when restricted to any interval [aj−1, aj], and let gj, j = 1, . . . , n denote the linear

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

restriction of g to the interval [aj−1, aj]. Then

(cid:90) 1

0

(f − g)2 dµ =

(f − gj)2 dµ

n
(cid:88)

(cid:90) aj

aj−1

≥

c(cid:96)5
j

j=1
n
(cid:88)

j=1
n
(cid:88)

j=1

= c

(cid:96)5
j .



1/p 



1/q

|xjyj| ≤

|xj|p



|yj|q



.





n
(cid:88)

j=1



n
(cid:88)

j=1

n
(cid:88)

j=1

Now, recall H¨older’s sum inequality which states that for any p, q satisfying 1

p + 1

q = 1 we have

Plugging in xj = (cid:96)j, yj = 1 ∀j ∈ {1, . . . , n} we have

and using the equalities (cid:80)n

j=1 |(cid:96)j| = 1 and p

q = p − 1 we get that

Plugging the inequality from Eq. (17) with p = 5 in Eq. (16) yields

n
(cid:88)

j=1





n
(cid:88)

j=1

|(cid:96)j| ≤

|(cid:96)j|p



n1/q,



1/p

1
np−1 ≤

n
(cid:88)

j=1

|(cid:96)j|p .

(cid:90) 1

0

(p − g)2 dµ ≥

c
n4 ,

(cid:90) 1

0

inf
g∈Gn

(p − g)2dµ ≥

p2
2
180n4 .

p (x) =

˜ai ˜Pi (x) ,

2
(cid:88)

i=0

(cid:107)p − P p(cid:107)2 =

p2
2(cid:96)5
180

.

concluding the proof of the lemma.

Our ﬁrst lower bound for approximation using piece-wise linear functions is for non-linear target functions of the simplest
kind. Namely, we obtain lower bounds on quadratic functions.

Theorem 9. If Gn is the family of piece-wise linear functions with at most n linear segments in the interval [0, 1], then for
any quadratic function p(x) = p2x2 + p1x + p0, we have

Proof. Observe that since p is a degree 2 polynomial, we have that its coefﬁcients satisfy ˜ai = 0 ∀i ≥ 3, so from Lemma 2
its optimal approximation error equals ˜a2
2(cid:96)
5 . Computing ˜a2 can be done directly from the equation

Which gives ˜a2 = p2(cid:96)2

6 due to Eq. (12). This implies that

(16)

(17)

(18)

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

Note that for quadratic functions, the optimal error is dependent solely on the length of the interval. Using Lemma 3 with
c = p2

2

180 we get

(cid:90) 1

0

(p − g)2 dµ ≥

p2
2
180n4 ,

concluding the proof of the theorem.

Computing a lower bound for quadratic functions is made easy since the bound on any interval [a, a + (cid:96)] depends on (cid:96) but
not on a. This is not the case in general, as can be seen by observing monomials of high degree k. As k grows, xk on
the interval [0, 0.5] converges rapidly to 0, whereas on (cid:2)1 − 1
, which
indicates that indeed a lower bound for xk will depend on a.

k , 1(cid:3) its second derivative is lower bounded by k(k−1)

4

For non-quadratic functions, however, we now show that a lower bound can be derived under the assumption of strong
convexity (or strong concavity) in [0, 1].

Theorem 10. Suppose f : [0, 1] → R is C 2 and either λ-strongly convex or λ-strongly concave. Then

(cid:90) 1

0

inf
g∈Gn

(f − g)2dµ ≥ cλ2n−4,

(19)

where c > 0 is a universal constant.

Proof. We ﬁrst stress that an analogous assumption to λ-strong convexity would be that f is λ-strongly concave, since the
same bound can be derived under concavity by simply applying the theorem to the additive inverse of f , and observing
that the additive inverse of any piece-wise linear approximation of f is in itself, of course, a piece-wise linear function. For
this reason from now on we shall use the convexity assumption, but will also refer without loss of generality to concave
functions.

As in the previous proof, we ﬁrst prove a bound on intervals of length (cid:96) and then generalize for the unit interval. From
Lemma 2, it sufﬁces that we lower bound ˜a2 (although this might not give the tightest lower bound in terms of constants,
it is possible to show that it does give a tight bound over all C 2 functions). We compute

using the change of variables t = 2

(cid:96) x − 2

(cid:96) a − 1, dt = 2

(cid:96) dx, we get the above equals

˜a2 =

=

5
(cid:96)

5
(cid:96)

(cid:90) a+(cid:96)

a
(cid:90) a+(cid:96)

a

˜P2 (x) f (x) dx

P2

(cid:18) 2
(cid:96)

2
(cid:96)

x −

a − 1

f (x) dx,

P2 (t) f

t +

+ a

dt

(cid:18) (cid:96)
2

(cid:96)
2

=

(cid:0)3t2 − 1(cid:1) f

t +

+ a

dt.

(cid:18) (cid:96)
2

(cid:96)
2

(cid:19)

5
2

5
4

(cid:90) 1

−1
(cid:90) 1

−1

(cid:19)

(cid:19)

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

We now integrate by parts twice, taking the anti-derivative of the polynomial to obtain

−

5(cid:96)
8

(cid:90) 1

−1

(cid:0)t3 − t(cid:1) f (cid:48)

t +

+ a

dt

(cid:18) (cid:96)
2

(cid:96)
2

(cid:19)

5
4

5
4

5(cid:96)
8

5(cid:96)
8

=

=

=

(cid:90) 1

−1

(cid:0)3t2 − 1(cid:1) f

(cid:20)
(cid:0)t3 − t(cid:1) f

(cid:18) (cid:96)
2

(cid:18) (cid:96)
2

t +

(cid:19)

dt

+ a

(cid:96)
2
(cid:19)(cid:21)1

t +

+ a

(cid:0)t − t3(cid:1) f (cid:48)

t +

+ a

dt

(cid:96)
2
(cid:18) (cid:96)
2
(cid:18) (cid:96)
2

−1
(cid:19)

(cid:96)
2

(cid:96)
2

(cid:19)(cid:21)1

t +

+ a

(cid:90) 1

−1
(cid:20)(cid:18) t2
2

−

5(cid:96)2
16

−

t4
4
(cid:90) 1

−1

(cid:19)

f (cid:48)

(cid:18) t2
2

(cid:19)

t +

+ a

dt

−

(cid:19)

t4
4

f (cid:48)(cid:48)

(cid:18) (cid:96)
2
(cid:90) 1

−1
(cid:96)
2
(cid:18) t2
2

But since t2

2 − t4

=

(f (cid:48) (a + (cid:96)) − f (cid:48) (a)) −

5(cid:96)
32
(cid:3) ∀t ∈ [−1, 1] and since f (cid:48)(cid:48) > 0 due to strong convexity, we have that
4 ∈ (cid:2)0, 1
(cid:19)
(cid:90) 1

+ a

(cid:90) 1

t +

f (cid:48)(cid:48)

(cid:96)
2

dt.

−

(cid:19)

(cid:19)

−1

4

5(cid:96)2
16

t4
4

(cid:18) (cid:96)
2

(cid:19)

(cid:19)

(cid:18) t2
2

−

t4
4

f (cid:48)(cid:48)

(cid:18) (cid:96)
2

−1

(cid:96)
2

t +

+ a

dt ≤

1
4

f (cid:48)(cid:48)

(cid:18) (cid:96)
2

−1

(cid:96)
2

t +

+ a

dt.

Plugging this inequality in Eq. (20) yields

˜a2 ≥

(f (cid:48) (a + (cid:96)) − f (cid:48) (a)) −

(cid:90) 1

−1

f (cid:48)(cid:48)

(cid:18) (cid:96)
2

(cid:19)

t +

+ a

dt

(cid:96)
2

=

(f (cid:48) (a + (cid:96)) − f (cid:48) (a)) −

(f (cid:48) (a + (cid:96)) − f (cid:48) (a))

5(cid:96)2
64
5(cid:96)2
64

5(cid:96)
32

5(cid:96)
32
(cid:18)

=

1 −

(f (cid:48) (a + (cid:96)) − f (cid:48) (a)) ,

(cid:19) 5(cid:96)
32

(cid:96)
2

but (cid:96) ≤ 1, so the above is at least

5(cid:96)
64

(f (cid:48) (a + (cid:96)) − f (cid:48) (a)) .

By Lagrange‘s intermediate value theorem, there exists some ξ ∈ [a, a + (cid:96)] such that f (cid:48) (a + (cid:96)) − f (cid:48) (a) = (cid:96)f (cid:48)(cid:48) (ξ), so
Eq. (21) is at least

(20)

(21)

and by using the strong convexity of f again, we get that

Lemma 2 now gives

Finally, by using Lemma 3 we conclude

5(cid:96)2
64

f (cid:48)(cid:48) (ξ) ,

˜a2 ≥

5λ(cid:96)2
64

.

(cid:107)f − P f (cid:107)2 = (cid:96)

∞
(cid:88)

˜a2
i
2i + 1

i=2
˜a2
2
5
5λ2(cid:96)5
4096

.

≥ (cid:96)

≥

(cid:90) 1

0

inf
g∈Gn

(f − g)2dµ ≥

5λ2
4096n4 .

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

We now derive a general lower bound for functions f : [0, 1] → R.

Theorem 11. Suppose f : [0, 1] → R is C 2. Then for any λ > 0

(cid:90) 1

0

inf
g∈Gn

(f − g)2dµ ≥

c · λ2 · σλ(f )5
n4

.

Proof. First, observe that if f is λ-strongly convex on [a, b], then f ((b − a) x + a) is λ (b − a)2-strongly convex on [0, 1]
since ∀x ∈ [0, 1],

∂
∂x2 f ((b − a) x + a) = (b − a)2 f (cid:48)(cid:48) ((b − a) x + a) ≥ λ (b − a)2 .

Now, we use the change of variables x = (b − a) t + a, dx = (b − a) dt

(cid:90) b

a

inf
g∈Gn

(f (x) − g(x))2dx

(cid:90) 1

0
(cid:90) 1

= inf
g∈Gn

(b − a)

(b − a)

= inf
g∈Gn
0
c · λ2 · (b − a)5
n4

≥

,

(f ((b − a) t + a) − g ((b − a) t + a))2 dt

(f ((b − a) t + a) − g (t))2 dt

(22)

where the inequality follows from an application of Thm. 10. Back to the theorem statement, if σλ = 0 then the bound
trivially holds, therefore assume λ > 0 such that σλ > 0. Since f is strongly convex on a set of measure σλ > 0, the
theorem follows by applying the inequality from Eq. (22).

A.3.3. MULTI-DIMENSIONAL LOWER BOUNDS

We now move to generalize the bounds in the previous subsection to general dimension d. Namely, we can now turn to
proving Thm. 7.

Proof of Thm. 7. Analogously to the proof of Thm. 11, we identify a neighborhood of f in which the restriction of f to a
line in a certain direction is non-linear. We then integrate along all lines in that direction and use the result of Thm. 11 to
establish the lower bound.

Before we can prove the theorem, we need to assert that indeed there exists a set having a strictly positive measure where
f has strong curvature along a certain direction. Assuming f is not piece-wise linear; namely, we have some x0 ∈ [0, 1]d
such that H (f ) (x0) (cid:54)= 0. Since H (f ) is continuous, we have that the function hv (x) = v(cid:62)H (f ) (x) v is continuous
and there exists a direction v ∈ Sd−1 where without loss of generality hv (x0) > 0. Thus, we have an open neighborhood
containing x0 where restricting f to the direction v forms a strongly convex function, which implies that indeed σλ > 0
for small enough λ > 0.

We now integrate the approximation error on f in the neighborhood U along the direction v. Compute

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

inf
g∈Gd
n

[0,1]d

(f − g)2dµd

u:(cid:104)u,v(cid:105)=0

β:(u+βv)∈[0,1]d

(cid:90)

(cid:90)

(f − g)2 dµ1dµd−1

(f − g)2 dµ1dµd−1

u:(cid:104)u,v(cid:105)=0

β:(u+βv)∈U

(µ1 ({β : (u + βv) ∈ U }))5

5λ2
4096n4 dµd−1

|µ1 ({β : (u + βv) ∈ U })|5 dµd−1

µ1 ({β : (u + βv) ∈ U }) dµd−1

(cid:33)5

u:(cid:104)u,v(cid:105)=0
(cid:32)(cid:90)

u:(cid:104)u,v(cid:105)=0

(cid:90)

(cid:90)

(cid:90)

= inf
g∈Gd
n

≥ inf
g∈Gd
n
(cid:90)

≥

=

u:(cid:104)u,v(cid:105)=0
(cid:90)
5λ2
4096n4

≥

5λ2
4096n4

=

5λ2σ5
λ
4096n4 ,

where in the second inequality we used Thm. 11 and in the third inequality we used Jensen‘s inequality with respect to the
convex function x (cid:55)→ |x|5.

A.4. Proof of Thm. 6

We begin by monitoring the rate of growth of the error when performing either an addition or a multiplication. Suppose
that the given input ˜a, ˜b is of distance at most δ > 0 from the desired target values a, b, i.e., |a − ˜a| ≤ δ, |b − ˜b| ≤ δ. Then
for addition we have

(cid:12)
(cid:12)
(cid:12)(a + b) −

(cid:16)

˜a + ˜b

(cid:17)(cid:12)
(cid:12)
(cid:12) ≤ |a − ˜a| +

(cid:12)
(cid:12)
(cid:12)b − ˜b
(cid:12)
(cid:12)
(cid:12) ≤ 2δ,

and for multiplication we compute the product error estimation

(cid:12)
(cid:12)
(cid:12)˜a · ˜b − a · b
(cid:12)
(cid:12)
(cid:12) ≤ |(a + δ) · (b + δ) − a · b|
= (cid:12)

(cid:12)δ (a + b) + δ2(cid:12)
(cid:12) .

(23)

Now, we have bounded the error of approximating the product of two numbers which we only have approximations of, but
since the computation of the product itself cannot be done with perfect accuracy using ReLU networks, we need to suffer
the error of approximating a product, as shown in Thm. 5. We add the error of approximating the product of ˜a · ˜b, which
we may assume is at most δ (assuming Θ (log2 (M/δ)) bits are used for the product, since each intermediate computation
is bounded in the interval [−M, M ]). Overall, we get an error bound of

(cid:12)
(cid:12)δ (a + b) + δ2 + δ(cid:12)

(cid:12) ≤ 3M δ.

From this, we see that at each stage the error grows by at most a multiplicative factor of 3M . After t operations, and with
an initial estimation error of δ, we have that the error is bounded by (3M )t−1 δ. Choosing δ ≤ (3M )1−t (cid:15) to guarantee
approximation (cid:15), we have from Thm. 5 that each operation will require at most
(cid:32)

(cid:33)(cid:39)

(cid:38)

width and

+ 13 ≤ c

log

+ t log (M )

4

log

M (3M )t−1
(cid:15)

(cid:38)

(cid:32)

2

log

M (3M )t−1
(cid:15)

(cid:33)(cid:39)

(cid:18)

(cid:18)

(cid:19)

(cid:18) 1
(cid:15)

(cid:19)

(cid:18) 1
(cid:15)

+ 9 ≤ c

log

+ t log (M )

(cid:19)

(cid:19)

depth for some universal c > 0. Composing the networks performing each operation, we arrive at a total network width
and depth of at most

(cid:18)

c

t log

(cid:19)

(cid:18) 1
(cid:15)

(cid:19)

+ t2 log (M )

.

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

Now, our target function is approximated to accuracy (cid:15) by a function which our network approximates to the same accuracy
(cid:15), for a total approximation error of the target function by our network of 2(cid:15).

B. L1 Ball Indicator Experiment

In this section, we run a similar experiment to the one presented in 3.1, this time with respect to indicators of L1 balls.
For this experiment, we sampled 5 · 105 data instances uniformly at random from the 14-dimensional L1 unit sphere
(i.e. each instance is of dimension 15). We then scaled the norm of each instance independently by a scaler chosen
uniformly from the interval [0, 2]. To each instance, we associated a target value computed according to the target function
f (x) = 1 ((cid:107)x(cid:107)1 ≤ 1). Another 5 · 104 examples were generated in a similar manner and used as a validation set.
We trained 5 ReLU networks on this dataset:

• One 3-layer network, with a ﬁrst hidden layer of size 100, a second hidden layer of size 20, and a linear output neuron.

• Four 2-layer networks, with hidden layer of sizes 100, 200, 400 and 800, and a linear output neuron.

Training was performed with backpropagation, using the TensorFlow library. We used the squared loss (cid:96)(y, y(cid:48)) = (y −y(cid:48))2
and batches of size 100. For all networks, we chose a momentum parameter of 0.95, and a learning rate starting at 0.1,
decaying by a multiplicative factor of 0.95 every 1000 batches, and stopping at 10−4.

The results are presented in Fig. 3. Like the L2 ball experiment, we see that adding depth when learning such functions is
much more helpful than increasing width. In fact, here the improvement by increased width is hardly noticeable, and the
width 400 network actually obtained a slightly better error than the width 800 network. In contrast, the 3-layer network
converged to a signiﬁcantly better solution.

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

Figure 3. The L1 experiment results, depicting the network’s root mean square error over the training set (top) and validation set (bottom),
as a function of the number of batches processed. Best viewed in color.

Batch number (x1000)020406080100120140160180200RMSE (training set)0.020.0250.030.0350.040.0450.050.0550.060.0650.073-layer, width 1002-layer, width 1002-layer, width 2002-layer, width 4002-layer, width 800Batch number (x1000)020406080100120140160180200RMSE (validation set)0.020.0250.030.0350.040.0450.050.0550.060.0650.073-layer, width 1002-layer, width 1002-layer, width 2002-layer, width 4002-layer, width 800