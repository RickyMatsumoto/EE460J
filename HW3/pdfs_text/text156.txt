AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

Corinna Cortes 1 Xavier Gonzalvo 1 Vitaly Kuznetsov 1 Mehryar Mohri 2 1 Scott Yang 2

Abstract

We present new algorithms for adaptively learn-
ing artiﬁcial neural networks. Our algorithms
(ADANET) adaptively learn both the structure
They are
of the network and its weights.
based on a solid theoretical analysis, including
data-dependent generalization guarantees that we
prove and discuss in detail. We report the re-
sults of large-scale experiments with one of our
algorithms on several binary classiﬁcation tasks
extracted from the CIFAR-10 dataset and on the
Criteo dataset. The results demonstrate that our
algorithm can automatically learn network struc-
tures with very competitive performance accura-
cies when compared with those achieved by neu-
ral networks found by standard approaches.

1. Introduction

Multi-layer artiﬁcial neural networks form a powerful
learning model which has helped achieve a remarkable per-
formance in several applications in recent years. Repre-
senting the input through increasingly more abstract lay-
ers of feature representation has shown to be very ef-
fective in natural language processing, image captioning,
speech recognition and several other areas (Krizhevsky
et al., 2012; Sutskever et al., 2014). However, despite the
compelling arguments for adopting multi-layer neural net-
works as a general template for tackling learning problems,
training these models and designing the right network for a
given task has raised several theoretical questions and faced
numerous practical challenges.

A critical step in learning a large multi-layer neural net-
work for a speciﬁc task is the choice of its architecture,
which includes the number of layers and the number of
units within each layer. Standard training methods for neu-
ral networks return a model admitting precisely the number

1Google Research, New York, NY, USA; 2Courant Institute
of Mathematical Sciences, New York, NY, USA. Correspondence
to: Vitaly Kuznetsov <vitalyk@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

of layers and units speciﬁed since there needs to be at least
one path through the network for the hypothesis to be non-
trivial. Single weights can be pruned (Han et al., 2015)
via a technique originally termed Optimal Brain Damage
(LeCun et al., 1990), but the global architecture remains
unchanged. Thus, this imposes a stringent lower bound on
the complexity of the model, which may not match that
of the learning task considered: complex networks trained
with insufﬁcient data may be prone to overﬁtting and, in
reverse, simpler architectures may not sufﬁce to achieve an
adequate performance.

This places a considerable burden on the user who is left
with the requirement to specify an architecture with the
right complexity, which is often a difﬁcult task even with a
signiﬁcant level of experience and domain knowledge. As a
result, the choice of the network is typically left to a hyper-
parameter search using a validation set. This search space
can quickly become exorbitantly large (Szegedy et al.,
2015; He et al., 2015) and large-scale hyperparameter tun-
ing to ﬁnd an effective network architecture often waste-
ful of data, time, and resources (e.g. grid search, random
search (Bergstra et al., 2011)).

This paper seeks precisely to address some of these issues.
We present a theoretical analysis of the problem of learn-
ing simultaneously both the network architecture and its
parameters. To the best of our knowledge, our results are
the ﬁrst generalization bounds for the problem of structural
learning of neural networks. These general guarantees can
guide the design of a variety of different algorithms for
learning in this setting. We describe in detail two such al-
gorithms (ADANET algorithms) that directly beneﬁt from
our theory.

Rather than enforcing a pre-speciﬁed architecture and
thus a ﬁxed network complexity, our ADANET algorithms
adaptively learn the appropriate network architecture for
a learning task. Starting from a simple linear model, our
algorithms incrementally augment the network with more
units and additional layers, as needed. The choice of the
additional subnetworks depends on their complexity and is
directly guided by our learning guarantees. Remarkably,
the optimization problems for both of our algorithms turn
out to be strongly convex and thus guaranteed to admit a
unique global solution.

AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

sents a function denoted by hk,j (before composition with
an activation function). Let X denote the input space and
for any x ∈ X, let Ψ(x) ∈ Rn0 denote the corresponding
feature vector. Then, the family of functions deﬁned by the
ﬁrst layer functions h1,j, j ∈ [n1], is the following:
(cid:111)
,

x (cid:55)→ u · Ψ(x) : u ∈ Rn0, (cid:107)u(cid:107)p ≤ Λ1,0

H1 =

(1)

(cid:110)

where p ≥ 1 deﬁnes an lp-norm and Λ1,0 ≥ 0 is a hyperpa-
rameter on the weights connecting layer 0 and layer 1. The
family of functions hk,j, j ∈ [nk], in a higher layer k > 1
is then deﬁned as follows:

(cid:26)

Hk =

x (cid:55)→

k−1
(cid:88)

s=1

us · (ϕs ◦ hs)(x) :

us ∈ Rns, (cid:107)us(cid:107)p ≤ Λk,s, hk,s ∈ Hs

(2)

(cid:27)

,

where, for each unit function hk,s, us in (2) denotes the
vector of weights for connections from that unit to a lower
layer s < k. The Λk,ss are non-negative hyperparameters
and ϕs ◦ hs abusively denotes a coordinate-wise compo-
sition: ϕs ◦ hs = (ϕs ◦ hs,1, . . . , ϕs ◦ hs,ns ). The ϕss
are assumed to be 1-Lipschitz activation functions. In par-
ticular, they can be chosen to be the Rectiﬁed Linear Unit
function (ReLU function) x (cid:55)→ max{0, x}, or the sigmoid
function x (cid:55)→ 1
1+e−x . The choice of the parameter p ≥ 1
determines the sparsity of the network and the complexity
of the hypothesis sets Hk.

For the networks we consider, the output unit can be con-
nected to all intermediate units, which therefore deﬁnes a
function f as follows:

f =

wk,jhk,j =

wk · hk,

(3)

l
(cid:88)

nk(cid:88)

k=1

j=1

l
(cid:88)

k=1

where hk = [hk,1, . . . , hk,nk ](cid:62)∈ Hnk
and wk ∈ Rnk is
k
the vector of connection weights to units of layer k. Ob-
serve that, for us = 0 for s < k − 1 and wk = 0 for k < l,
our architectures coincides with standard multi-layer feed-
forward ones.
We will denote by F the family of functions f deﬁned by
(3) with the absolute value of the weights summing to one:

F =

wk · hk : hk ∈ Hnk
k ,

(cid:107)wk(cid:107)1 = 1

l
(cid:88)

k=1

(cid:41)
.

(cid:40) l

(cid:88)

k=1

Let (cid:101)Hk denote the union of Hk and its reﬂection, (cid:101)Hk =
Hk ∪ (−Hk), and let H denote the union of the families
(cid:101)Hk: H = (cid:83)l
k=1 (cid:101)Hk. Then, F coincides with the convex
hull of H: F = conv(H).
For any k ∈ [l], we will also consider the family H∗
k de-
rived from Hk by setting Λk,s = 0 for s < k − 1, which

Figure 1. An example of a general network architecture: the out-
put layer (in green) is connected to all of the hidden units as well
as some input units. Some hidden units (in red and yellow) are
connected not only to the units in the layer directly below, but
also to units at other levels.

The paper is organized as follows. In Appendix A, we give
a detailed discussion of previous work related to this topic.
Section 2 describes the general network architecture and
therefore the hypothesis set that we consider. Section 3
provides a formal description of our learning scenario. In
Section 4, we prove strong generalization guarantees for
learning in this setting, which help guide the design of the
algorithm described in Section 5, as well as a variant de-
scribed in Appendix C. We report the results of our experi-
ments with ADANET in Section 6.

2. Network architecture

In this section, we describe the general network architec-
ture we consider for feedforward neural networks, thereby
also deﬁning our hypothesis set. To simplify the presenta-
tion, we restrict our attention to the case of binary classiﬁ-
cation. However, all our results can be straightforwardly
extended to multi-class classiﬁcation, including the net-
work architecture, by augmenting the number of output
units, and, our generalization guarantees, by using existing
multi-class counterparts of the binary classiﬁcation ensem-
ble margin bounds we use.

A common model for feedforward neural networks is the
multi-layer architecture where units in each layer are only
connected to those in the layer below. We will consider
more general architectures where a unit can be connected
to units in any of the layers below, as illustrated by Fig-
ure 1. In particular, the output unit in our network architec-
tures can be connected to any other unit. These more gen-
eral architectures include as special cases standard multi-
layer networks (by zeroing out appropriate connections) as
well as somewhat more exotic ones (He et al., 2015; Huang
et al., 2016). In fact, our deﬁnition covers any architecture
that can be represented as a directed acyclic graph (DAG).

More formally, the artiﬁcial neural networks we consider
are deﬁned as follows. Let l denote the number of interme-
diate layers in the network and nk the maximum number of
units in layer k ∈ [l]. Each unit j ∈ [nk] in layer k repre-

AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

k = H∗

k) and H∗ = ∪l

corresponds to units connected only to the layer below. We
similarly deﬁne (cid:101)H∗
H∗
k ∪ (−H∗
k,
and deﬁne F∗ as the convex hull F∗ = conv(H∗). Note
that the architecture corresponding to the family of func-
tions F∗ is still more general than standard feedforward
neural network architectures since the output unit can be
connected to units in different layers.

k=1

3. Learning problem

We consider the standard supervised learning scenario and
assume that training and test points are drawn i.i.d. accord-
ing to some distribution D over X × {−1, +1} and denote
by S = ((x1, y1), . . . , (xm, ym)) a training sample of size
m drawn according to Dm.
For a function f taking values in R, we denote by R(f ) =
E(x,y)∼D[1yf (x)≤0] its generalization error and, for any
ρ > 0, by (cid:98)RS,ρ(f ) its empirical margin error on the sample
S: (cid:98)RS,ρ(f ) = 1
m

i=1 1yif (xi)≤ρ.

(cid:80)m

The learning problem consists of using the training sam-
ple S to determine a function f deﬁned by (3) with small
generalization error R(f ). For an accurate predictor f , we
expect many of the weights to be zero and the correspond-
ing architecture to be quite sparse, with fewer than nk units
at layer k and relatively few non-zero connections. In that
sense, learning an accurate function f implies also learning
the underlying architecture.

In the next section, we present data-dependent learning
bounds for this problem that will help guide the design of
our algorithms.

4. Generalization bounds

Our
the
learning bounds are expressed in terms of
Rademacher complexities of the hypothesis sets Hk. The
empirical Rademacher complexity of a hypothesis set G for
a sample S is denoted by (cid:98)RS(G) and deﬁned as follows:
(cid:34)

(cid:35)

(cid:98)RS(G) =

1
m

E
σ

m
(cid:88)

i=1

sup
h∈G

σih(xi)

,

where σ = (σ1, . . . , σm), with σis independent uniformly
distributed random variables taking values in {−1, +1}.
Its Rademacher complexity is deﬁned by Rm(G) =
ES∼Dm[ (cid:98)RS(G)]. These are data-dependent complexity
measures that lead to ﬁner learning guarantees (Koltchin-
skii & Panchenko, 2002; Bartlett & Mendelson, 2002).
As pointed out earlier, the family of functions F is the con-
vex hull of H. Thus, generalization bounds for ensemble
methods can be used to analyze learning with F. In particu-
lar, we can leverage the recent margin-based learning guar-
antees of Cortes et al. (2014), which are ﬁner than those

that can be derived via a standard Rademacher complex-
ity analysis (Koltchinskii & Panchenko, 2002), and which
admit an explicit dependency on the mixture weights wk
deﬁning the ensemble function f . That leads to the follow-
ing learning guarantee.

Theorem 1 (Learning bound). Fix ρ > 0. Then, for any
δ > 0, with probability at least 1 − δ over the draw of a
sample S of size m from Dm, the following inequality holds
for all f = (cid:80)l

k=1 wk · hk ∈ F:

R(f ) ≤ (cid:98)RS,ρ(f ) +

4
ρ

l
(cid:88)

k=1

(cid:13)
(cid:13)wk

(cid:13)
(cid:13)1Rm( (cid:101)Hk) +

(cid:114)

2
ρ

log l
m

+ C(ρ, l, m, δ),

where C(ρ, l, m, δ) =

(cid:16) 1
ρ

(cid:113) log l
m

(cid:17)

.

(cid:101)O

(cid:113)(cid:6) 4

ρ2 log( ρ2m

log l )(cid:7) log l

m + log(2/δ)

2m =

The proof of this result, as well as that of all other
main theorems are given in Appendix B. The bound of
the theorem can be generalized to hold uniformly for all
ρ ∈ (0, 1], at the price of an additional term of the form
(cid:112)log log2(2/ρ)/m using standard techniques (Koltchin-
skii & Panchenko, 2002).

Observe that the bound of the theorem depends only log-
arithmically on the depth of the network l. But, perhaps
more remarkably, the complexity term of the bound is a
(cid:107)wk(cid:107)1-weighted average of the complexities of the layer
hypothesis sets Hk, where the weights are precisely those
deﬁning the network, or the function f . This suggests that
a function f with a small empirical margin error and a deep
architecture beneﬁts nevertheless from a strong generaliza-
tion guarantee, if it allocates more weights to lower layer
units and less to higher ones. Of course, when the weights
are sparse, that will imply an architecture with relatively
fewer units or connections at higher layers than at lower
ones. The bound of the theorem further gives a quantita-
tive guide for apportioning the weights, depending on the
Rademacher complexities of the layer hypothesis sets.

This data-dependent learning guarantee will serve as a
foundation for the design of our structural learning algo-
rithms in Section 5 and Appendix C. However, to fully
exploit it, the Rademacher complexity measures need to
be made explicit. One advantage of these data-dependent
measures is that they can be estimated from data, which
can lead to more informative bounds. Alternatively, we can
derive useful upper bounds for these measures which can
be more conveniently used in our algorithms. The next re-
sults in this section provide precisely such upper bounds,
thereby leading to a more explicit generalization bound.

We will denote by q the conjugate of p, that is 1
and deﬁne r∞ = maxi∈[1,m] (cid:107)Ψ(xi)(cid:107)∞.

p + 1

q = 1,

AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

Our ﬁrst result gives an upper bound on the Rademacher
complexity of Hk in terms of the Rademacher complexity
of other layer families.

Lemma 1. For any k > 1, the empirical Rademacher
complexity of Hk for a sample S of size m can be upper-
bounded as follows in terms of those of Hss with s < k:

(cid:98)RS(Hk) ≤ 2

1
q

Λk,s n

s (cid:98)RS(Hs).

k−1
(cid:88)

s=1

For the family H∗
k, which is directly relevant to many of
our experiments, the following more explicit upper bound
can be derived, using Lemma 1.
Lemma 2. Let Λk = (cid:81)k
s=1 ns−1.
Then, for any k ≥ 1, the empirical Rademacher complexity
of H∗
k for a sample S of size m can be upper bounded as
follows:

s=1 2Λs,s−1 and Nk = (cid:81)k

(cid:98)RS(H∗

1
q
k) ≤ r∞ΛkN
k

(cid:114)

log(2n0)
2m

.

Note that Nk, which is the product of the number of units
in layers below k, can be large. This suggests that values of
p closer to one, that is larger values of q, could be more
helpful to control complexity in such cases. More gen-
erally, similar explicit upper bounds can be given for the
Rademacher complexities of subfamilies of Hk with units
connected only to layers k, k − 1, . . . , k − d, with d ﬁxed,
d < k. Combining Lemma 2 with Theorem 1 helps derive
the following explicit learning guarantee for feedforward
neural networks with an output unit connected to all the
other units.

s=1 4Λs,s−1 and Nk = (cid:81)k

Corollary 1 (Explicit learning bound). Fix ρ > 0. Let
Λk = (cid:81)k
s=1 ns−1. Then, for any
δ > 0, with probability at least 1 − δ over the draw of a
sample S of size m from Dm, the following inequality holds
for all f = (cid:80)l

k=1 wk · hk ∈ F∗:

R(f ) ≤ (cid:98)RS,ρ(f ) +

2
ρ

l
(cid:88)

k=1

(cid:20)

(cid:13)
(cid:13)wk

(cid:13)
(cid:13)1

r∞ΛkN

(cid:114)

1
q
k

(cid:21)

2 log(2n0)
m

(cid:114)

+

2
ρ

log l
m

+ C(ρ, l, m, δ),

where C(ρ, l, m, δ) =
(cid:17)

log l )(cid:7) log l
, and where r∞ = ES∼Dm[r∞].

ρ2 log( ρ2m

(cid:113) log l
m

(cid:16) 1
ρ

(cid:101)O

(cid:113)(cid:6) 4

m + log( 2
δ )
2m =

The learning bound of Corollary 1 is a ﬁner guarantee than
previous ones by Bartlett (1998), Neyshabur et al. (2015),
or Sun et al. (2016). This is because it explicitly differenti-
ates between the weights of different layers while previous
bounds treat all weights indiscriminately. This is crucial

to the design of algorithmic design since the network com-
plexity no longer needs to grow exponentially as a function
of depth. Our bounds are also more general and apply to
more other network architectures, such as those introduced
in (He et al., 2015; Huang et al., 2016).

5. Algorithm

This section describes our algorithm, ADANET, for adap-
tive learning of neural networks. ADANET adaptively
grows the structure of a neural network, balancing model
complexity with empirical risk minimization. We also de-
scribe in detail in Appendix C another variant of ADANET
which admits some favorable properties.

Let x (cid:55)→ Φ(−x) be a non-increasing convex function
upper-bounding the zero-one loss, x (cid:55)→ 1x≤0, such that Φ
is differentiable over R and Φ(cid:48)(x) (cid:54)= 0 for all x. This surro-
gate loss Φ may be, for instance, the exponential function
Φ(x) = ex as in AdaBoost (Freund & Schapire, 1997), or
the logistic function, Φ(x) = log(1 + ex) as in logistic
regression.

5.1. Objective function

Let {h1, . . . , hN } be a subset of H∗. In the most general
case, N is inﬁnite. However, as discussed later, in practice,
the search is limited to a ﬁnite set. For any j ∈ [N ], we
will denote by rj the Rademacher complexity of the family
Hkj that contains hj: rj = Rm(Hkj ).
ADANET seeks to ﬁnd a function f = (cid:80)N
j=1 wjhj ∈
F∗ (or neural network) that directly minimizes the data-
dependent generalization bound of Corollary 1. This leads
to the following objective function:

F (w) =

Φ

1 − yi

wjhj

+

Γj|wj|,

(4)

(cid:16)

1
m

m
(cid:88)

i=1

N
(cid:88)

j=1

(cid:17)

N
(cid:88)

j=1

where w ∈ RN and Γj = λrj + β, with λ ≥ 0 and
β ≥ 0 hyperparameters. The objective function (4) is a
convex function of w. It is the sum of a convex surrogate
of the empirical error and a regularization term, which is a
weighted-l1 penalty containing two sub-terms: a standard
norm-1 regularization which admits β as a hyperparame-
ter, and a term that discriminates the functions hj based on
their complexity.

The optimization problem consisting of minimizing the ob-
jective function F in (4) is deﬁned over a very large space
of base functions hj. ADANET consists of applying coor-
dinate descent to (4). In that sense, our algorithm is similar
to the DeepBoost algorithm of Cortes et al. (2014). How-
ever, unlike DeepBoost, which combines decision trees,
ADANET learns a deep neural network, which requires new
methods for constructing and searching the space of func-

AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

(a)

(b)

Figure 2. Illustration of the algorithm’s incremental construction
of a neural network. The input layer is indicated in blue, the out-
put layer in green. Units in the yellow block are added at the ﬁrst
iteration while units in purple are added at the second iteration.
Two candidate extensions of the architecture are considered at the
the third iteration (shown in red): (a) a two-layer extension; (b)
a three-layer extension. Here, a line between two blocks of units
indicates that these blocks are fully-connected.
tions hj. Both of these aspects differ signiﬁcantly from the
decision tree framework. In particular, the search is par-
ticularly challenging. In fact, the main difference between
the algorithm presented in this section and the variant de-
scribed in Appendix C is the way new candidates hj are
examined at each iteration.

5.2. Description

We start with an informal description of ADANET. Let
B ≥ 1 be a ﬁxed parameter determining the number of
units per layer of a candidate subnetwork. The algorithm
proceeds in T iterations. Let lt−1 denote the depth of the
neural network constructed before the start of the t-th itera-
tion. At iteration t, the algorithm selects one of the follow-
ing two options:

1. augmenting the current neural network with a subnet-
work with the same depth as that of the current network
h ∈ H∗B
, with B units per layer. Each unit in layer k of
lt−1
this subnetwork may have connections to existing units in
layer k − 1 of ADANET in addition to connections to units
in layer k − 1 of the subnetwork.

2. augmenting the current neural network with a deeper
subnetwork h(cid:48) ∈ H∗B
, with depth lt−1 + 1. The set of
lt−1
connections allowed is deﬁned in the same way as for h.

The option selected is the one leading to the best reduction
of the current value of the objective function, which de-
pends both on the empirical error and the complexity of the
subnetwork added, which is penalized differently in these
two options.

Figure 2 illustrates this construction and the two options

just described. An important aspect of our algorithm is that
the units of a subnetwork learned at a previous iteration
(say h1,1 in Figure 2) can serve as input to a deeper subnet-
work added later (for example h2,2 or h2,3 in the Figure).
Thus, the deeper subnetworks added later can take advan-
tage of the embeddings that were learned at the previous
iterations. The algorithm terminates after T rounds or if
the ADANET architecture can no longer be extended to im-
prove the objective (4).

More formally, ADANET is a boosting-style algorithm that
applies (block) coordinate descent to (4). At each iteration
of block coordinate descent, descent coordinates h (base
learners in the boosting literature) are selected from the
space of functions H∗. These coordinates correspond to
the direction of the largest decrease in (4). Once these co-
ordinates are determined, an optimal step size in each of
these directions is chosen, which is accomplished by solv-
ing an appropriate convex optimization problem.

Note that, in general, the search for the optimal descent
coordinate in an inﬁnite-dimensional space or even in ﬁ-
nite but large sets such as that of all decision trees of some
large depth may be intractable, and it is common to resort
to a heuristic search (weak learning algorithm) that returns
δ-optimal coordinates. For instance, in the case of boosting
with trees one often grows trees according to some partic-
ular heuristic (Freund & Schapire, 1997).

We denote the ADANET model after t − 1 rounds by
ft−1, which is parameterized by wt−1. Let hk,t−1 de-
note the vector of outputs of units in the k-th layer of the
ADANET model, lt−1 be the depth of the ADANET archi-
tecture, nk,t−1 be the number of units in k-th layer after
t − 1 rounds. At round t, we select descent coordinates
by considering two candidate subnetworks h ∈ (cid:101)H∗
and
h(cid:48) ∈ (cid:101)H∗
lt−1+1 that are generated by a weak learning algo-
rithm WEAKLEARNER. Some choices for this algorithm in
our setting are described below. Once we obtain h and h(cid:48),
we select one of these vectors of units, as well as a vector of
weights w ∈ RB, so that the result yields the best improve-
ment in (4). This is equivalent to minimizing the following
objective function over w ∈ RB and u ∈ {h, h(cid:48)}:

lt−1

Ft(w, u) =

Φ

1 − yift−1(xi) − yiw · u(xi)

(cid:17)

(cid:16)

1
m

m
(cid:88)

i=1

+ Γu(cid:107)w(cid:107)1,

(5)

(cid:0)Hlt−1

(cid:1) if u =
if

In other words,

where Γu = λru + β and ru is Rm
h and Rm
minw Ft(w, h) ≤ minw Ft(w, h(cid:48)), then

(cid:1) otherwise.

(cid:0)Hlt−1+1

w∗ = argmin
w∈RB

Ft(w, h), ht = h

AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

ADANET(S = ((xi, yi)m

i=1)

f0 ← 0
for t ← 1 to T do

1
2
3
4
5
6
7
8
9
10
11
12 return fT

h, h(cid:48) ← WEAKLEARNER(cid:0)S, ft−1
w ← MINIMIZE(cid:0)Ft(w, h)(cid:1)
w(cid:48) ← MINIMIZE(cid:0)Ft(w, h(cid:48))(cid:1)
if Ft(w, h(cid:48)) ≤ Ft(w(cid:48), h(cid:48)) then

(cid:1)

ht ← h
else ht ← h(cid:48)
if F (wt−1 + w∗) < F (wt−1) then

ft ← ft−1 + w∗ · ht

else return ft−1

Figure 3. Pseudocode of the ADANET algorithm. On line 3 two
candidate subnetworks are generated (e.g. randomly or by solving
(6)). On lines 3 and 4, (5) is solved for each of these candidates.
On lines 5-7 the best subnetwork is selected and on lines 9-11
termination condition is checked.

and otherwise

w∗ = argmin
w∈RB

Ft(w, h(cid:48)), ht = h(cid:48)

If F (wt−1 + w∗) < F (wt−1) then we set ft = ft−1 +
w∗ · ht and otherwise we terminate the algorithm.

There are many different choices for the WEAKLEARNER
algorithm. For instance, one may generate a large number
of random networks and select the one that optimizes (5).
Another option is to directly minimize (5) or its regularized
version:

(cid:101)Ft(w, h) =

Φ

1−yift−1(xi)−yiw · h(xi)

(cid:17)

(cid:16)

1
m

m
(cid:88)

i=1

+ R(w, h),

(6)

over both w and h. Here R(w, h) is a regularization term
that, for instance, can be used to enforce that (cid:107)us(cid:107)p ≤ Λk,s
in (2). Note that, in general, (6) is a non-convex objective.
However, we do not rely on ﬁnding a global solution to
the corresponding optimization problem. In fact, standard
guarantees for regularized boosting only require that each
h that is added to the model decreases the objective by a
constant amount (i.e. it satisﬁes δ-optimality condition) for
a boosting algorithm to converge (R¨atsch et al., 2001; Luo
& Tseng, 1992).

Furthermore, the algorithm that we present in Appendix C
uses a weak-learning algorithm that solves a convex sub-
problem at each step and that additionally has a closed-
form solution. This comes at the cost of a more restricted
search space for ﬁnding a descent coordinate at each step
of the algorithm.

We conclude this section by observing that in our descrip-
tion of ADANET we have ﬁxed B for all iterations and
only two candidate subnetworks are considered at each
step. Our approach easily extends to an arbitrary number
of candidate subnetworks (for instance of different depth l)
as well as varying number of units per layer B. Further-
more, selecting an optimal subnetwork among the candi-
dates is easily parallelizable allowing for efﬁcient and ef-
fective search for optimal descent directions. We also note
that the choice of subnetworks need not be restricted to
standard feedforward architectures and more exotic choices
can be employed including the ones in (He et al., 2015;
Huang et al., 2016).
In our experiments we will restrict
attention to simple feedforward subnetworks.

6. Experiments

In this section we present the results of our experiments
with ADANET. Some additional experimental results are
given in Appendix D and further implementation details
presented in Appendix E.

6.1. CIFAR-10

In our ﬁrst set of experiments, we used the CIFAR-10
dataset (Krizhevsky, 2009). This dataset consists of 60,000
images evenly categorized in 10 different classes. To
reduce the problem to binary classiﬁcation, we consid-
ered ﬁve pairs of classes: deer-truck, deer-horse,
automobile-truck, cat-dog, dog-horse. Raw
images have been pre-processed to obtain color histograms
and histogram of gradient features. The result is 154 real
valued features with ranges in [0, 1].

We compared ADANET to standard feedforward neural
networks (NN) and logistic regression (LR) models. Note
that convolutional neural networks are often a more nat-
ural choice for image classiﬁcation problems such as
CIFAR-10. However, the goal of our experiments was not
to obtain state-of-the-art results for this particular task, but
a proof-of-concept showing that our structural learning ap-
proach can be very competitive with traditional approaches
for ﬁnding efﬁcient architectures and training correspond-
ing networks.

Our ADANET algorithm requires the knowledge of com-
plexities rj, which, in some cases, can be estimated from
data.
In our experiments, we used the upper bound of
Lemma 2. Our algorithm admits a number of hyperparam-
eters: regularization hyperparameters λ, β, number of units
B in each layer of new subnetworks that are used to extend
the model at each iteration, and a bound Λk on weights u
in each unit. As discussed in Section 5, there are different
approaches to ﬁnding candidate subnetworks in each itera-
tion. In our experiments, we searched for candidate subnet-

AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

Table 1. Experimental results for ADANET, NN, LR and NN-GP for different pairs of labels in CIFAR-10. Boldfaced results are
statistically signiﬁcant at a 5% conﬁdence level.

Label pair

ADANET

LR

NN

NN-GP

deer-truck
deer-horse
automobile-truck
cat-dog
dog-horse

0.9372 ± 0.0082
0.8430 ± 0.0076
0.8461 ± 0.0069
0.6924 ± 0.0129
0.8350 ± 0.0089

0.8997 ± 0.0066
0.7685 ± 0.0119
0.7976 ± 0.0076
0.6664 ± 0.0099
0.7968 ± 0.0128

0.9213 ± 0.0065
0.8055 ± 0.0178
0.8063 ± 0.0064
0.6595 ± 0.0141
0.8066 ± 0.0087

0.9220 ± 0.0069
0.8060 ± 0.0181
0.8056 ± 0.0138
0.6607 ± 0.0097
0.8087 ± 0.0109

works by minimizing (6) with R = 0. This also requires
a learning rate hyperparameter η. These hyperparamers
have been optimized over the following ranges: λ ∈
{0, 10−8, 10−7, 10−6, 10−5, 10−4}, B ∈ {100, 150, 250},
η ∈ {10−4, 10−3, 10−2, 10−1}. We have used a single Λk
for all k > 1 optimized over {1.0, 1.005, 1.01, 1.1, 1.2}.
For simplicity, we chose β = 0.

Neural network models also admit a learning rate η
and a regularization coefﬁcient λ as hyperparameters, as
well as the number of hidden layers l and the num-
ber of units n in each hidden layer.
The range of
η was the same as for ADANET and we varied l in
{1, 2, 3}, n in {100, 150, 512, 1024, 2048} and λ ∈
{0, 10−5, 10−4, 10−3, 10−2, 10−1}. Logistic regression
only admits as hyperparameters η and λ which were opti-
mized over the same ranges. Note that the total number of
hyperparameter settings for ADANET and standard neural
networks is exactly the same. Furthermore, the same holds
for the number of hyperparameters that determine the re-
sulting architecture of the model: Λ and B for ADANET
and l and n for neural network models. Observe that, while
a particular setting of l and n determines a ﬁxed architec-
ture, Λ and B parameterize a structural learning procedure
that may result in a different architecture depending on the
data.

In addition to the grid search procedure, we have con-
ducted a hyperparameter optimization for neural net-
works using Gaussian process bandits (NN-GP), which
is a sophisticated Bayesian non-parametric method for
response-surface modeling in conjunction with a bandit
Instead of operating on
algorithm (Snoek et al., 2012).
a pre-speciﬁed grid,
this allows one to search for hy-
perparameters in a given range. We used the following
ranges: λ ∈ [10−5, 1], η ∈ [10−5, 1], l ∈ [1, 3] and
n ∈ [100, 2048]. This algorithm was run for 500 trials,
which is more than the number of hyperparameter settings
considered by ADANET and NN. Observe that this search
procedure can also be applied to our algorithm but we chose
not to use it in this set of experiments to further demonstrate
competitiveness of our structural learning approach.

In all our experiments, we use ReLu as the activation func-

tions. NN, NN-GP and LR are trained using stochastic
gradient method with batch size of 100 and maximum of
10,000 iterations. The same conﬁguration is used for solv-
ing (6). We use T = 30 for ADANET in all our experi-
ments although in most cases algorithm terminates after 10
rounds.

In each of the experiments, we used standard 10-fold cross-
validation for performance evaluation and model selection.
In particular, the dataset was randomly partitioned into 10
folds, and each algorithm was run 10 times, with a different
assignment of folds to the training set, validation set and
test set for each run. Speciﬁcally, for each i ∈ {0, . . . , 9},
fold i was used for testing, fold i + 1 (mod 10) was used
for validation, and the remaining folds were used for train-
ing. For each setting of the parameters, we computed
the average validation error across the 10 folds, and se-
lected the parameter setting with maximum average accu-
racy across validation folds. We report the average accu-
racy (and standard deviations) of the selected hyperparam-
eter setting across test folds in Table 1.

Our results show that ADANET outperforms other meth-
ods on each of the datasets. The average architectures
for all label pairs are provided in Table 2. Note that NN
and NN-GP always select a one-layer architecture. The
architectures selected by ADANET also typically admit a
single layer, with fewer nodes than those selected by NN
and NN-GP. However, for the more challenging problem
cat-dog, ADANET opts for a more complex model with
two layers, which results in a better performance. This fur-
ther illustrates how our approach helps learn network ar-
chitectures in an adaptive fashion, based on the complexity
of the task.

As discussed in Section 5, different heuristics can be used
to generate candidate subnetworks on each iteration of
ADANET. In a second set of experiments, we varied the
objective function (6), as well as the domain over which
it is optimized. This allowed us to study the sensitiv-
ity of ADANET to the choice of a heuristic used to gen-
In particular, we consid-
erate candidate subnetworks.
ered the following variants of ADANET. ADANET.R uses
R(w, h) = Γh(cid:107)w(cid:107)1 as a regularization term in (6). As

AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

Table 2. Average number of units in each layer.

Table 4. Experimental results for Criteo dataset.

Label pair

ADANET

NN NN-GP

Algorithm Accuracy

1st layer 2nd layer

deer-truck
990
deer-horse
1475
automobile-truck 2000
cat-dog
1800
dog-horse
1600

0
0
0
25
0

2048 1050
2048
488
2048 1595
512
155
2048 1273

Table 3. Experimental results for different variants of ADANET,
for the deer-truck label pair in CIFAR-10.

Algorithm

Accuracy (± std. dev.)

ADANET.SD
ADANET.R
ADANET.P
ADANET.D

0.9309 ± 0.0069
0.9336 ± 0.0075
0.9321 ± 0.0065
0.9376 ± 0.0080

the ADANET architecture grows, each new subnetwork is
connected to all the previous subnetworks, which signiﬁ-
cantly increases the number of connections in the network
and the overall complexity of the model. ADANET.P and
ADANET.D are restricting connections to existing subnet-
works in different ways. ADANET.P connects each new
subnetwork only to the subnetwork that was added on the
previous iteration. ADANET.D uses dropout on the con-
nections to previously added subnetworks. Finally, while
ADANET is based on the upper bounds on the Rademacher
complexities of Lemma 2, ADANET.SD uses instead stan-
dard deviations of the outputs of the last hidden layer on
the training data as surrogates for Rademacher complex-
ities. The advantage of using this data-dependent mea-
sure of complexity is that it eliminates the hyperparame-
ter Λ, thereby reducing the hyperparameter search space.
We report the average accuracies across test folds for the
deer-truck pair in Table 3.

6.2. Criteo Click Rate Prediction

We also compared ADANET to NN on the Criteo Click
Rate Prediction dataset (https://www.kaggle.com/c/
criteo-display-ad-challenge). This dataset con-
sists of 7 days of data where each instance is an impression
and a binary label (clicked or not clicked). Each impres-
sion admits 13 count features and 26 categorical features.
Count features have been transformed by taking the natu-
ral logarithm. The values of categorical features appear-
ing less than 100 times are replaced by 0. The rest of the
values are then converted to integers, which are then used
as keys to look up embeddings (that are trained together
with each model). If the number of possible values for a
feature x is d(x), then the embedding dimension is set to

ADANET
NN

0.7846
0.7811

6d(f )1/4 for d(f ) > 25. Otherwise, the embedding di-
mension is d(f ). Missing feature values are set to 0. We
split the labeled set provided in the link above into train-
ing, validation and test sets.1 Our training set covered the
ﬁrst 5 days of data (32,743,299 instances) and the valida-
tion and test sets consisted of 1 day (6,548,659 instances).
Gaussian processes bandits were used to ﬁnd the best hy-
perparameter settings on validation set both for ADANET
and NN. For ADANET we optimized over the following
hyperparameter ranges: B ∈ {125, 256, 512}, Λ ∈ [1, 1.5],
η ∈ [10−4, 10−1], λ ∈ [10−12, 10−4]. For NN the ranges
were as follows: l ∈ [1, 6], n ∈ {250, 512, 1024, 2048},
η ∈ [10−5, 10−1], λ ∈ [10−6, 10−1]. We trained NNs
for 100,000 iterations using mini-batch stochastic gradient
method with batch size of 512. The same conﬁguration was
used at each iteration of ADANET to solve (6). The max-
imum number of hyperparameter trials was 2,000 for both
methods. The results are presented in Table 4. In this exper-
iment, NN chooses an architecture with four hidden layers
and 512 units in each hidden layer. Remarkably, ADANET
achieves a better accuracy with an architecture consisting
of single layer with just 512 nodes. While the difference
in performance appears to be small, it is in fact statistically
signiﬁcant in this challenging task.

7. Conclusion

We presented a new framework and algorithms for adap-
tively learning artiﬁcial neural networks. Our algorithm,
ADANET, beneﬁts from strong theoretical guarantees. It
simultaneously learns a neural network architecture and its
parameters, by balancing a trade-off between model com-
plexity and empirical risk minimization. We reported fa-
vorable experimental results demonstrating that our algo-
rithm is able to learn network architectures that perform
better than those found via a grid search. Our techniques
are general and can be applied to other neural network ar-
chitectures such as CNNs and RNNs.

Acknowledgments

The work of M. Mohri and that of S. Yang were partly
funded by NSF awards IIS-1117591 and CCF-1535987.

1The test set available from this link does not include ground

truth labels and therefore could be used in our experiments.

AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

References

Alvarez, Jose M and Salzmann, Mathieu. Learning the
number of neurons in deep networks. In NIPS, 2016.

Arora, Sanjeev, Bhaskara, Aditya, Ge, Rong, and Ma,
Tengyu. Provable bounds for learning some deep rep-
resentations. In ICML, pp. 584–592, 2014.

Arora, Sanjeev, Liang, Yingyu, and Ma, Tengyu. Why are
deep nets reversible: A simple theory, with implications
for training. arXiv:1511.05653, 2015.

Han, Hong-Gui and Qiao, Jun-Fei. A structure optimisation
algorithm for feedforward neural network construction.
Neurocomputing, 99:347–357, 2013.

Han, Song, Pool, Jeff, Tran, John, and Dally, William J.
Learning both weights and connections for efﬁcient neu-
ral networks. In NIPS, 2015.

Hardt, Moritz, Recht, Benjamin, and Singer, Yoram. Train
faster, generalize better: Stability of stochastic gradient
descent. arXiv:1509.01240, 2015.

Baker, Bowen, Gupta, Otkrist, Naik, Nikhil, and Raskar,
Ramesh. Designing neural network architectures using
reinforcement learning. CoRR, 2016.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep residual learning for image recognition.
CoRR, abs/1512.03385, 2015.

Bartlett, Peter L. The sample complexity of pattern classi-
ﬁcation with neural networks: the size of the weights is
more important than the size of the network. Information
Theory, IEEE Transactions on, 44(2), 1998.

Bartlett, Peter L. and Mendelson, Shahar. Rademacher and
Gaussian complexities: Risk bounds and structural re-
sults. JMLR, 3, 2002.

Bergstra, James S, Bardenet, R´emi, Bengio, Yoshua, and
K´egl, Bal´azs. Algorithms for hyper-parameter optimiza-
tion. In NIPS, pp. 2546–2554, 2011.

Chen, Tianqi, Goodfellow, Ian J., and Shlens, Jonathon.
Net2net: Accelerating learning via knowledge transfer.
CoRR, 2015.

Choromanska, Anna, Henaff, Mikael, Mathieu, Michael,
Arous, G´erard Ben, and LeCun, Yann. The loss surfaces
of multilayer networks. arXiv:1412.0233, 2014.

Cohen, Nadav, Sharir, Or, and Shashua, Amnon. On the ex-
pressive power of deep learning: a tensor analysis. arXiv,
2015.

Cortes, Corinna, Mohri, Mehryar, and Syed, Umar. Deep

boosting. In ICML, pp. 1179 – 1187, 2014.

Daniely, Amit, Frostig, Roy, and Singer, Yoram. Toward
deeper understanding of neural networks: The power of
initialization and a dual view on expressivity. In NIPS,
2016.

Eldan, Ronen and Shamir, Ohad. The power of depth for
feedforward neural networks. arXiv:1512.03965, 2015.

Freund, Yoav and Schapire, Robert E. A decision-theoretic
generalization of on-line learning and an application to
boosting. Journal of Computer System Sciences, 55(1):
119–139, 1997.

Ha, David, Dai, Andrew M., and Le, Quoc V. Hypernet-

works. CoRR, 2016.

Huang, Gao, Liu, Zhuang, and Weinberger, Kilian Q.
CoRR,

Densely connected convolutional networks.
2016.

Islam, Md. Monirul, Yao, Xin, and Murase, Kazuyuki.
A constructive algorithm for training cooperative neural
network ensembles. IEEE Trans. Neural Networks, 14
(4):820–834, 2003.

Islam, Md. Monirul, Sattar, Md. Abdus, Amin, Md. Faijul,
Yao, Xin, and Murase, Kazuyuki. A new adaptive merg-
ing and growing algorithm for designing artiﬁcial neural
networks. IEEE Trans. Systems, Man, and Cybernetics,
Part B, 39(3):705–722, 2009.

Janzamin, Majid, Sedghi, Hanie, and Anandkumar, Anima.
Generalization bounds for neural networks through ten-
sor factorization. arXiv:1506.08473, 2015.

Kawaguchi, Kenji. Deep learning without poor local min-

ima. In NIPS, 2016.

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for
stochastic optimization. CoRR, abs/1412.6980, 2014.

Koltchinskii, Vladmir and Panchenko, Dmitry. Empiri-
cal margin distributions and bounding the generalization
error of combined classiﬁers. Annals of Statistics, 30,
2002.

Kotani, Manabu, Kajiki, Akihiro, and Akazawa, Kenzo.
A structural learning algorithm for multi-layered neural
In International Conference on Neural Net-
networks.
works, volume 2, pp. 1105–1110. IEEE, 1997.

Krizhevsky, Alex. Learning multiple layers of features
from tiny images. Master’s thesis, University of Toronto,
2009.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In NIPS, pp. 1097–1105, 2012.

AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P.
Practical Bayesian Optimization of Machine Learning
Algorithms. In Pereira, F., Burges, C. J. C., Bottou, L.,
and Weinberger, K. Q. (eds.), NIPS, pp. 2951–2959. Cur-
ran Associates, Inc., 2012.

Sun, Shizhao, Chen, Wei, Wang, Liwei, Liu, Xiaoguang,
and Liu, Tie-Yan. On the depth of deep neural networks:
A theoretical view. In AAAI, 2016.

Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence
In NIPS,

to sequence learning with neural networks.
2014.

Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,
Pierre, Reed, Scott E., Anguelov, Dragomir, Erhan, Du-
mitru, Vanhoucke, Vincent, and Rabinovich, Andrew.
Going deeper with convolutions. In CVPR, 2015.

Telgarsky, Matus. Beneﬁts of depth in neural networks. In

COLT, 2016.

Zhang, Saizheng, Wu, Yuhuai, Che, Tong, Lin, Zhouhan,
Memisevic, Roland, Salakhutdinov, Ruslan, and Bengio,
Yoshua. Architectural complexity measures of recurrent
neural networks. CoRR, 2016.

Zhang, Yuchen, Lee, Jason D, and Jordan, Michael I. (cid:96) 1-
regularized neural networks are improperly learnable in
polynomial time. arXiv:1510.03528, 2015.

Zoph, Barret and Le, Quoc V. Neural architecture search

with reinforcement learning. CoRR, 2016.

Kuznetsov, Vitaly, Mohri, Mehryar, and Syed, Umar.

Multi-class deep boosting. In NIPS, 2014.

Kwok, Tin-Yau and Yeung, Dit-Yan. Constructive algo-
rithms for structure learning in feedforward neural net-
IEEE Transactions on
works for regression problems.
Neural Networks, 8(3):630–645, 1997.

LeCun, Yann, Denker, John S., and Solla, Sara A. Optimal

brain damage. In NIPS, 1990.

Lehtokangas, Mikko. Modelling with constructive back-
propagation. Neural Networks, 12(4):707–716, 1999.

Leung, Frank HF, Lam, Hak-Keung, Ling, Sai-Ho, and
Tam, Peter KS. Tuning of the structure and parame-
ters of a neural network using an improved genetic al-
gorithm. IEEE Transactions on Neural Networks, 14(1):
79–88, 2003.

Lian, Xiangru, Huang, Yijun, Li, Yuncheng, and Liu, Ji.
Asynchronous parallel stochastic gradient for nonconvex
optimization. In NIPS, pp. 2719–2727, 2015.

Livni, Roi, Shalev-Shwartz, Shai, and Shamir, Ohad. On
the computational efﬁciency of training neural networks.
In NIPS, pp. 855–863, 2014.

Luo, Zhi-Quan and Tseng, Paul. On the convergence of co-
ordinate descent method for convex differentiable mini-
mization. Journal of Optimization Theory and Applica-
tions, 72(1):7 – 35, 1992.

Ma, Liying and Khorasani, Khashayar. A new strategy
for adaptively constructing multilayer feedforward neu-
ral networks. Neurocomputing, 51:361–385, 2003.

Narasimha, Pramod L, Delashmit, Walter H, Manry,
Michael T, Li, Jiang, and Maldonado, Francisco. An
integrated growing-pruning method for feedforward net-
work training. Neurocomputing, 71(13):2831–2847,
2008.

Neyshabur, Behnam, Tomioka, Ryota, and Srebro, Nathan.
In
Norm-based capacity control in neural networks.
COLT, 2015.

R¨atsch, Gunnar, Mika, Sebastian, and Warmuth, Man-
fred K. On the convergence of leveraging. In NIPS, pp.
487–494, 2001.

Sagun, Levent, Guney, V Ugur, Arous, Gerard Ben, and
LeCun, Yann. Explorations on high dimensional land-
scapes. arXiv:1412.6615, 2014.

Saxena, Shreyas and Verbeek, Jakob. Convolutional neural

fabrics. CoRR, abs/1606.02492, 2016.

