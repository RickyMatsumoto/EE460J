Graph-based Isometry Invariant Representation Learning:
Supplementary material

Renata Khasanova 1 Pascal Frossard 1

In the supplementary material, we present in more de-
tail the training procedure of the TIGraNet and deﬁne the
partial derivatives, required to compute back-propagation
through the newly introduced layers. We also provide a
more complete analysis of the network behavior with addi-
tional experiments on the small MNIST-012 dataset.

1. Training

1.1. Back-propagation details

We use supervised learning and train our network so that
it maximizes the log-probability of estimating the correct
class of training samples via logistic regression. Overall,
we need to compute the values of the parameters in each
convolutional and in fully-connected layers. The other lay-
ers do not have any parameter to be estimated. We train
the network using a classical back-propagation algorithm
and learn the parameters using ADAM stochastic optimiza-
tion (Kingma & Ba, 2014).

We provide more details here about the computation that
are speciﬁc to our new architecture. We refer the reader
to (Rumelhart et al., 1988) for more details about the over-
all training procedure. The back-propagation in the spec-
tral convolutional layer is performed by evaluating the par-
tial derivatives with respect to the parameters α : α ∈
RKl−1×M of the spectral ﬁlters, and to the parameters
β : β ∈ RKl−1 of the feature map construction. The partial
derivatives read

∂E
∂αl

i,m

=

Kl−1
(cid:88)

k=0

βl
k

(cid:104)
Lm|N l−1

i

(cid:105)

yl−1
k

∂E
∂zl
i

,

∂E
∂βl
j

=

M
(cid:88)

m=0

(cid:104)

αl

i,m

Lm|N l−1

i

(cid:105)

yl−1
j

∂E
∂zl
i

,

(1)

(2)

1Ecole Polytechnique F´ed´erale de Lausanne (EPFL), Lau-
sanne, Switzerland. Correspondence to: Renata Khasanova
<renata.khasanova@epﬂ.ch>,
<pas-
cal.frossard@epﬂ.ch>.

Frossard

Pascal

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

where E is the negative log-likelihood cost function, zl
i =
yl
i is the output feature map of layer l, Kl−1 denotes the
number of feature maps at the previous layer of the net-
work, M is the polynomial degree of the convolutional ﬁl-
ter and L is the Laplacian matrix. Then, we further need to
compute the partial derivatives with respect to the previous
feature maps as follows

∂E
∂yl−1
j

= βl
j

M
(cid:88)

m=0

αl

i,m

(cid:104)
Lm|N l−1

i

(cid:105) ∂E
∂zl
i

.

(3)

Our new dynamic pooling layers, as well as our statistical
layer do not have parameters to be trained. Similarly to the
max-pooling operator our dynamic pooling layer permits
back-propagation through the active nodes since the gra-
dient is 0 for the non-selected nodes and not zero for the
chosen ones. Further, the statistical layer back-propagates
the gradients as follows:

∂E
∂ti,k

=

1
N

∂E
∂µi,k

,

∂E
∂ti,k

=

2(N − 1)
N 2

N
(cid:88)

i=1

(ti,k − µi,k)

∂E
∂σ2
i,k

,

where µi,k, σ2
i,k are the inputs to the ﬁrst fully-connected
layer and the outputs of the statistical layer. The derivatives
∂E/∂ ˜zi are then computed as:

(4)

(5)

(6)

∂E
∂ ˜zi

=

Kmax(cid:88)

k=0

∂E
∂ti,k

∂ti,k
∂ ˜zi

,

where ∂ti,k/∂ ˜zi are simply the derivatives of Chebyshev
polynomials (Shuman et al., 2011) with maximum order
Kmax. Please note that we use the non-linear absolute
function |ti,k| before statistical layer, therefore, the gradi-
ent at ti,k = 0 is not deﬁned. In practice, however, we set
it to 0, which gives us a nice property of encouraging some
feature map values to be 0 and favors sparsity.

Finally, the parameters of the fully-connected layers are
trained in a classical way, similarly to the training of
fully-connected layers in ConvNet architectures (Rumel-
hart et al., 1988).

Graph-based Isometry Invariant Representation Learning: Supplementary material

1.2. Filter initialization

The initialization of the system may have some inﬂuence
on the actual values of the parameters after training. We
have chosen to initialize the parameters αl
i,m of our spectral
convolutional ﬁlters so that the different ﬁlters uniformly
cover the full spectral domain. We ﬁrst create a set of Z
overlapping rectangular functions w(λ, ai, bi)

w(λ, ai, bi) =

(cid:40)
1
0

if ai < λ < bi,
otherwise.

(7)

The non-zero regions for all functions have the same size,
and the set of functions covers the full spectrum of the nor-
malized laplacian L, i.e., [0, 2]. We ﬁnally approximate
each of these rectangular functions by a M -order polyno-
mial, which produces a set of initial coefﬁcients αl
i,m that
are used to deﬁne the initial version of the spectral ﬁlter F l
i .

Finally, the initial values of the parameters β in the spectral
convolutional layer are distributed uniformly in [0, 1] and
those of the parameters in the fully-connected layers are
selected uniformly in [−1, 1].

2. TIGraNet Analysis

We analyze the performance of our new architecture on the
MNIST-012 dataset. We ﬁrst give some examples of fea-
ture maps that are produced by our network. We then illus-
trate the spectral kernels learned by our system, and discuss
the inﬂuence of dynamic pooling.

We ﬁrst conﬁrm the transformation invariant properties of
our architecture. Even though our classiﬁer is trained on
images without any transformations, it is able to correctly
classify rotated images in the test set, since our spectral
convolutional layer learns ﬁlters that are equivariant to iso-
metric transformations. We illustrate this in Fig. 1, which
depicts several examples of feature maps y2
i from the sec-
ond spectral convolutional layer for randomly rotated input
digits in the test set. Each row of Fig. 1 corresponds to im-
ages of a different digit, and we see that the corresponding
feature maps are very close to each other (up to the image
rotation) even when the rotation angle is quite large. This
conﬁrms that our architecture is able to learn features that
are preserved with rotation, even if the training has been
performed on non-transformed images. Despite important
similarities in feature maps of rotated digits, one may how-
ever observe some slightly different values for the intensity.
This can be explained by the fact that rotated versions of the
input images may differ a bit from the original images due
to interpolation artifacts.

Fig. 2 then shows the spectral representation of the ker-
nels learned for the ﬁrst two spectral convolutional layers

Figure 1. Feature maps from the second spectral convolutional
layer for test images that are rotated versions of an image of the
digit ‘2’. The predicted label for each of the images is further
shown in the right bottom corner of each image.

a)

b)

Figure 2. Sample trained ﬁlters in the spectral domain for (a)
ﬁrst and (b) second convolutional layers. Different colors repre-
sent different ﬁlters on each of the layers.

of our network. As expected, the network learns ﬁlters that
are quite different from each other in the spectral domain
but that altogether cover the full spectrum. They permit
to efﬁciently combine information in the different bands of
frequency in the spectral representation of the input sig-
nal. Generally, the ﬁlters in the upper spectral convolu-
tional layers are more diverse and represent more compli-
cated features than those for the lower ones.

Finally, we look at the inﬂuence of the new dynamic pool-
ing layers in our architecture. Recall that dynamic pooling
is used to reduce the network complexity and to focus on
the representative parts of the input signal. Fig. 3 depicts
the intermediate feature maps of the network for sample
test images. We can see that after each pooling operation

Graph-based Isometry Invariant Representation Learning: Supplementary material

Figure 3. Feature maps after pooling Each row shows different
digits. The left most column depicts the original images, while
the other columns show the features maps after dynamic pooling
at the ﬁrst, second and third layers respectively. The degree of the
polynomial ﬁlters has been set to M = 3 for each layer in this
experiment.

the signal is getting more and more sparse, while preserv-
ing the structure of the data that is important for discrim-
inating images in different classes. That shows that our
dynamic pooling operator is able to retain the important in-
formation in the feature maps constructed by the spectral
convolutional layers.

References

Kingma, D. and Ba, J. Adam: A method for stochastic

optimization. arXiv Preprint, 2014.

Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learn-
ing representations by back-propagating errors. Cogni-
tive modeling, 5(3):1, 1988.

Shuman, D. I., Vandergheynst, P., and Frossard, P. Cheby-
shev polynomial approximation for distributed signal
processing. In IEEE International Conference on Dis-
tributed Computing in Sensor Systems, pp. 1–8, 2011.

