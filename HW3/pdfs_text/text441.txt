Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

Mahesh Chandra Mukkamala 1 2 Matthias Hein 1

Abstract

√

Adaptive gradient methods have become recently
very popular,
in particular as they have been
shown to be useful in the training of deep neu-
In this paper we have analyzed
ral networks.
RMSProp, originally proposed for the training
of deep neural networks, in the context of on-
T -type re-
line convex optimization and show
gret bounds. Moreover, we propose two vari-
ants SC-Adagrad and SC-RMSProp for which
we show logarithmic regret bounds for strongly
convex functions. Finally, we demonstrate in the
experiments that these new variants outperform
other adaptive gradient techniques or stochastic
gradient descent in the optimization of strongly
convex functions as well as in training of deep
neural networks.

1. Introduction

There has recently been a lot of work on adaptive gradient
algorithms such as Adagrad (Duchi et al., 2011), RMSProp
(Hinton et al., 2012), ADADELTA (Zeiler, 2012), and
Adam (Kingma & Bai, 2015). The original idea of Ada-
grad to have a parameter speciﬁc learning rate by analyz-
ing the gradients observed during the optimization turned
out to be useful not only in online convex optimization
but also for training deep neural networks. The original
analysis of Adagrad (Duchi et al., 2011) was limited to the
case of all convex functions for which it obtained a data-
dependent regret bound of order O(
T ) which is known
to be optimal (Hazan, 2016) for this class. However, a lot
of learning problems have more structure in the sense that
one optimizes over the restricted class of strongly convex
functions. It has been shown in (Hazan et al., 2007) that
one can achieve much better logarithmic regret bounds for
the class of strongly convex functions.

√

1Department of Mathematics and Computer Science, Saarland
University, Germany 2IMPRS-CS, Max Planck Institute for In-
formatics, Saarbr¨ucken, Germany . Correspondence to: Mahesh
Chandra Mukkamala <mmahesh.chandra873@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

√

The goal of this paper is twofold. First, we propose SC-
Adagrad which is a variant of Adagrad adapted to the
strongly convex case. We show that SC-Adagrad achieves
a logarithmic regret bound for the case of strongly convex
functions, which is data-dependent. It is known that such
bounds can be much better in practice than data indepen-
dent bounds (Hazan et al., 2007),(McMahan, 2014). Sec-
ond, we analyze RMSProp which has become one of the
standard methods to train neural networks beyond stochas-
tic gradient descent. We show that under some condi-
tions on the weighting scheme of RMSProp, this algorithm
T ) regret bound. In fact, it
achieves a data-dependent O(
turns out that RMSProp contains Adagrad as a special case
for a particular choice of the weighting scheme. Up to our
knowledge this is the ﬁrst theoretical result justifying the
usage of RMSProp in online convex optimization and thus
can at least be seen as theoretical support for its usage in
deep learning. Similarly, we then propose the variant SC-
RMSProp for which we also show a data-dependent loga-
rithmic regret bound similar to SC-Adagrad for the class of
strongly convex functions. Interestingly, SC-Adagrad has
been discussed in (Ruder, 2016), where it is said that “it
does not to work”. The reason for this is that SC-Adagrad
comes along with a damping factor which prevents poten-
tially large steps in the beginning of the iterations. How-
ever, as our analysis shows this damping factor has to be
rather large initially to prevent large steps and should be
then monotonically decreasing as a function of the itera-
tions in order to stay adaptive. Finally, we show in experi-
ments on three datasets that the new methods are compet-
itive or outperform other adaptive gradient techniques as
well as stochastic gradient descent for strongly convex op-
timization problem in terms of regret and training objective
but also perform very well in the training of deep neural
networks, where we show results for different networks and
datasets.

2. Problem Statement

We ﬁrst need some technical statements and notation and
then introduce the online convex optimization problem.

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

2.1. Notation and Technical Statements

We denote by [T ] the set {1, . . . , T }. Let A ∈ Rd×d be a
symmetric, positive deﬁnite matrix. We denote as

We assume that the adversarial can choose from the class
of convex functions on C, for some parts we will specialize
this to the set of strongly convex functions.

(cid:104)x, y(cid:105)A = (cid:104)x, Ay(cid:105) =

Aijxiyj,

(cid:107)x(cid:107)A =

(cid:104)x, x(cid:105)A

(cid:113)

d
(cid:88)

i,j=1

Deﬁnition 2.1 Let C be a convex set. We say that a func-
tion f : C → R is µ-strongly convex, if there exists µ ∈ Rd
with µi > 0 for i = 1, . . . , d such that for all x, y ∈ C,

Note that the standard Euclidean inner product becomes
(cid:104)x, y(cid:105) = (cid:80)
i xiyi = (cid:104)x, y(cid:105)I While we use here the gen-
eral notation for matrices for comparison to the literature.
All positive deﬁnite matrices A in this paper will be diago-
nal matrices, so that the computational effort for computing
inner products and norms is still linear in d. The Cauchy-
Schwarz inequality becomes, (cid:104)x, y(cid:105)A ≤ (cid:107)x(cid:107)A (cid:107)y(cid:107)A . We
further introduce the element-wise product a(cid:12)b of two vec-
tors. Let a, b ∈ Rd, then (a (cid:12) b)i = aibi for i = 1, . . . , d.
Let A ∈ Rd×d be a symmetric, positive deﬁnite matrix,
z ∈ Rd and C ⊂ Rd a convex set. Then we deﬁne the
weighted projection P A

C (z) of z onto the set C as

P A

C (z) = arg min

(cid:107)x − z(cid:107)2

A .

x∈C

(1)

It is well-known that the weighted projection is unique and
non-expansive.

Lemma 2.1 Let A ∈ Rd×d be a symmetric, positive deﬁ-
nite matrix and C ⊂ Rd be a convex set. Then

(cid:13)
(cid:13)P A

C (z) − P A

C (y)(cid:13)

(cid:13)A ≤ (cid:107)z − y(cid:107)A .

Lemma 2.2 For any symmetric, positive semi-deﬁnite ma-
trix A ∈ Rd×d we have

(cid:104)x, Ax(cid:105) ≤ λmax(A) (cid:104)x, x(cid:105) ≤ tr(A) (cid:104)x, x(cid:105)

(2)

where λmax(A) is the maximum eigenvalue of matrix A
and tr(A) denotes the trace of matrix A .

2.2. Problem Statement

In this paper we analyze the online convex optimization
setting, that is we have a convex set C and at each round we
get access to a (sub)-gradient of some continuous convex
function ft : C → R. At the t-th iterate we predict θt ∈ C
and suffer a loss ft(θt). The goal is to perform well with
respect to the optimal decision in hindsight deﬁned as

The adversarial regret at time T ∈ N is then given as

θ∗ = arg min

ft(θ).

θ∈C

T
(cid:88)

t=1

R(T ) =

(ft(θt) − ft(θ∗)).

T
(cid:88)

t=1

f (y) ≥ f (x) + (cid:104)∇f (x), y − x(cid:105) + (cid:107)y − x(cid:107)2

diag(µ)

= f (x) + (cid:104)∇f (x), y − x(cid:105) +

µi(yi − xi)2.

d
(cid:88)

i=1

Let ζ = mini=1,...,d µi, then this function is ζ-strongly con-
vex (in the usual sense), that is

f (y) ≥ f (x) + (cid:104)∇f (x), y − x(cid:105) + ζ (cid:107)x − y(cid:107)2 .

Note that the difference between our notion of component-
wise strong convexity and the usual deﬁnition of strong
convexity is indicated by the bold font versus normal font.
We have two assumptions:

• A1: It holds supt≥1 (cid:107)gt(cid:107)2 ≤ G which implies the
existence of a constant G∞ such that supt≥1 (cid:107)gt(cid:107)∞ ≤
G∞.

• A2:

It holds supt≥1 (cid:107)θt − θ∗(cid:107)2 ≤ D which im-
plies the existence of a constant D∞ such that
supt≥1 (cid:107)θt − θ∗(cid:107)∞ ≤ D∞.

√

One of the ﬁrst methods which achieves the optimal regret
T ) for convex problems is online projected
bound of O(
gradient descent (Zinkevich, 2003), deﬁned as

θt+1 = PC(θt − αtgt)

(3)

where αt = α√
is the step-size scheme and gt is a (sub)-
t
gradient of ft at θt. With αt = α
t , online projected gradi-
ent descent method achieves the optimal O(log(T )) regret
bound for strongly-convex problems (Hazan et al., 2007).
We consider Adagrad in the next subsection which is one of
the popular adaptive alternative to online projected gradient
descent.

2.3. Adagrad for convex problems

In this section we brieﬂy recall the main result for the Ada-
grad. The algorithm for Adagrad is given in Algorithm 1. If
the adversarial is allowed to choose from the set of all pos-
sible convex functions on C ⊂ Rd, then Adagrad achieves
√
T ) as shown in (Duchi et al.,
the regret bound of order O(
2011). This regret bound is known to be optimal for this
class, see e.g.
(Hazan, 2016). For better comparison to
our results for RMSProp, we recall the result from (Duchi

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

Algorithm 1 Adagrad

Input: θ1 ∈ C , δ > 0, v0 = 0 ∈ Rd
for t = 1 to T do
gt ∈ ∂ft(θt)
vt = vt−1 + (gt (cid:12) gt)
√
vt) + δI
At = diag(
(cid:0)θt − αA−1
θt+1 = P At
C

t gt

(cid:1)

end for

et al., 2011) in our notation. For this purpose, we introduce
the notation, g1:T,i = (g1,i, g2,i, .., gT,i)T , where gt,i is the
i-th component of the gradient gt ∈ Rd of the function ft
evaluated at θt.

Theorem 2.1 (Duchi et al., 2011) Let Assumptions A1, A2
hold and let θt be the sequence generated by Adagrad in
Algorithm 1, where gt ∈ ∂ft(θt) and ft : C → R is an
arbitrary convex function, then for stepsize α > 0 the regret
is upper bounded as

R(T ) ≤

(cid:107)g1:T,i(cid:107)2 + α

(cid:107)g1:T,i(cid:107)2 .

D2
∞
2α

d
(cid:88)

i=1

d
(cid:88)

i=1

The effective step-length of Adagrad is on the order of α√
t
This can be seen as follows; ﬁrst note that vT,i = (cid:80)T
t=1 g2
t,i
1√
and thus (At)−1 is a diagonal matrix with entries
vt,i+δ .
Then one has

.

α(A−1

T )ii =

α

(cid:113)(cid:80)T

t=1 g2

t,i + δ
1

=

α
√
T

(cid:113) 1
T

(cid:80)T

t=1 g2

t,i + δ√
T

(4)

Thus an alternative point of view of Adagrad, is that it has a
decaying stepsize α√
but now the correction term becomes
t
the running average of the squared derivatives plus a van-
ishing damping term. However, the effective stepsize has
to decay faster to get a logarithmic regret bound for the
strongly convex case. This is what we analyze in the next
section, where we propose SC-Adagrad for strongly convex
functions.

and set it as a diagonal matrix with entries
one has

1
vt,i+δt

. Then

α(A−1

T )ii =

α
t=1 g2
t,i + δt

(cid:80)T

=

α
T

1
t=1 g2

(cid:80)T

1
T

t,i + δT
T

.

(5)

Again, we have in the denominator a running average of
the observed gradients and a decaying damping factor. In
this way, we get an effective stepsize of order O( 1
T ) in SC-
Adagrad. The formal method is presented in Algorithm
2. As just derived the only difference of Adagrad and SC-
Adagrad is the deﬁnition of the diagonal matrix At. Note

Algorithm 2 SC-Adagrad

Input: θ1 ∈ C , δ0 > 0, v0 = 0 ∈ Rd
for t = 1 to T do
gt ∈ ∂ft(θt)
vt = vt−1 + (gt (cid:12) gt)
Choose 0 < δt ≤ δt−1 element wise
At = diag(vt) + diag(δt)
(cid:0)θt − αA−1
θt+1 = P At
t gt
C

(cid:1)

end for

also that we have deﬁned the damping factor δt as a func-
tion of t which is also different from standard Adagrad.
The constant δ in Adagrad is mainly introduced due to nu-
merical reasons in order to avoid problems when gt,i is
very small for some components in the ﬁrst iterations and
is typically chosen quite small e.g. δ = 10−8. For SC-
Adagrad the situation is different. If the ﬁrst components
g1,i, g2,i, . . . are very small, say of order (cid:15), then the update
which can become extremely large if δt is chosen
is
to be small. This would make the method very unstable
and would lead to huge constants in the bounds. This is
probably why in (Ruder, 2016), the modiﬁcation of Ada-
grad where one “drops the square-root” did not work. A
good choice of δt should be initially roughly on the order
of 1 and it should decay as vt,i = (cid:80)T
t,i starts to grow.
This is why we propose to use

t=1 g2

(cid:15)
(cid:15)2+δt

δt,i = ξ2e−ξ1vt,i,

i = 1, . . . , d,

3. Strongly convex Adagrad (SC-Adagrad)

The modiﬁcation SC-Adagrad of Adagrad which we pro-
pose in the following can be motivated by the observation
that the online projected gradient descent (Hazan et al.,
2007) uses stepsizes of order α = O( 1
T ) in order to achieve
the logarithmic regret bound for strongly convex functions.
In analogy with the derivation in the previous section, we
still have vT,i = (cid:80)T
t,i. But now we modify (At)−1

t=1 g2

for ξ1 > 0, ξ2 > 0 as a potential decay scheme as it satis-
ﬁes both properties for sufﬁciently large ξ1 and ξ2 chosen
on the order of 1. Also, one can achieve a constant de-
cay scheme for ξ1 = 0 , ξ2 > 0. We will come back to
this choice after the proof. In the following we provide the
regret analysis of SC-Adagrad and show that the optimal
logarithmic regret bound can be achieved. However, as it is
data-dependent it is typically signiﬁcantly better in practice
than data-independent bounds.

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

3.1. Analysis

For any two matrices A, B ∈ Rd×d, we use the notation
• to denote the inner product i.e A • B = (cid:80)
j AijBij.
i
Note that A • B = tr(AT B).

(cid:80)

Lemma 3.1 [Lemma 12 (Hazan et al., 2007)] Let A, B be
positive deﬁnite matrices, let A (cid:23) B (cid:31) 0 then

A−1 • (A − B) ≤ log

(6)

(cid:17)

(cid:16) |A|
|B|

where |A| denotes the determinant of the matrix A

Lemma 3.2 Let Assumptions A1, A2 hold, then for T ≥
1 and At, δt as deﬁned in the SC-Adagrad algorithm we
have,

T
(cid:88)

t=1

(cid:10)gt, A−1

t gt

(cid:11) ≤

(cid:32)

log

d
(cid:88)

i=1

(cid:33)

(cid:107)g1:T,i(cid:107)2 + δT,i
δ1,i

d
(cid:88)

T
(cid:88)

−

i=1

t=2

δt,i − δt−1,i
(cid:107)g1:t,i(cid:107)2 + δt,i

Theorem 3.1 Let Assumptions A1, A2 hold and let θt be
the sequence generated by the SC-Adagrad in Algorithm
2, where gt ∈ ∂ft(θt) and ft : C → R is an arbitrary
µ-strongly convex function (µ ∈ Rd
+) where the stepsize
fulﬁlls α ≥ maxi=1,...,d
. Furthermore, let δt > 0 and
δt,i ≤ δt−1,i∀t ∈ [T ], ∀i ∈ [d], then the regret of SC-
Adagrad can be upper bounded for T ≥ 1 as

G2
∞
2µi

R(T ) ≤

D2

∞ tr(diag(δ1))
2α

+

α
2

d
(cid:88)

i=1

log

(cid:16) (cid:107)g1:T,i(cid:107)2 + δT,i
δ1,i

(cid:17)

+

1
2

d
(cid:88)

i=1

inf
t∈[T ]

(cid:16) (θt,i − θ∗

i )2

α

−

α
(cid:107)g1:t,i(cid:107)2 + δt,i

(cid:17)

For constant δt i.e δt,i = δ > 0 ∀t ∈ [T ] and ∀i ∈ [d] then
the regret of SC-Adagrad is upper bounded as

R(T ) ≤

D2

∞dδ
2α

+

α
2

d
(cid:88)

i=1

log

(cid:16) (cid:107)g1:T,i(cid:107)2 + δ
δ

(cid:17)

(7)

For ζ-strongly convex function choosing α ≥ G2
tain the above mentioned regret bounds.

∞

2ζ we ob-

Note that the ﬁrst and the last term in the regret bound
can be upper bounded by constants. Only the second term
depends on T . Note that (cid:107)g1:T,i(cid:107)2 ≤ T G2 and as δt is
monotonically decreasing, the second term is on the order
of O(log(T )) and thus we have a logarithmic regret bound.
As the bound is data-dependent, in the sense that it depends
on the observed sequence of gradients, it is much tighter
than a data-independent bound.

The bound includes also the case of a non-decaying damp-
ing factor δt = δ = ξ2 (ξ1 = 0). While a rather large
constant damping factor can work well, we have noticed
that the best results are obtained with the decay scheme

δt,i = ξ2e−ξ1vt,i,

i = 1, . . . , d.

where ξ1 > 0 , ξ2 > 0 , which is what we use in the ex-
periments. Note that this decay scheme for ξ1, ξ2 > 0 is
adaptive to the speciﬁc dimension and thus increases the
adaptivity of the overall algorithm. For completeness we
also give the bound specialized for this decay scheme.

Corollary 3.1 In the setting of Theorem 3.1 choose δt,i =
ξ2e−ξ1vt,i for i = 1, . . . , d for some ξ1 > 0, ξ2 > 0 . Then
the regret of SC-Adagrad can be upper bounded for T ≥ 1
as

R(T ) ≤

dD2
∞ξ2
2α

−

α
2

log(ξ2e−ξ1G2

∞ )

(cid:16)

(cid:107)g1:T,i(cid:107)2 + ξ2 e−ξ1(cid:107)g1:T ,i(cid:107)2 (cid:17)

log

αξ1ξ2
2(cid:0) log(ξ2 ξ1) + 1(cid:1)

d
(cid:88)

i=1

(cid:16)

1 − e−ξ1(cid:107)g1:T ,i(cid:107)2 (cid:17)

α
2

d
(cid:88)

i=1

+

+

Unfortunately, it is not obvious that the regret bound for our
decaying damping factor is better than the one of a constant
damping factor. Note, however that the third term in the re-
gret bound of Theorem 3.1 can be negative. It thus remains
an interesting question for future work, if there exists an
optimal decay scheme which provably works better than
any constant one.

RMSProp is one of the most popular adaptive gradient
algorithms used for the training of deep neural networks
(Schaul et al., 2014; Dauphin et al., 2015; Daniel et al.,
2016; Schmidhuber, 2015). It has been used frequently in
computer vision (Karpathy & Fei-Fei, 2016) e.g.
to train
the latest InceptionV4 network (Szegedy et al., 2016a;b).
Note that RMSProp outperformed other adaptive methods
like Adagrad order Adadelta as well as SGD with momen-
tum in a large number of tests in (Schaul et al., 2014). It
has been argued that if the changes in the parameter update
are approximately Gaussian distributed, then the matrix At
can be seen as a preconditioner which approximates the di-
agonal of the Hessian (Daniel et al., 2016). However, it is
fair to say that despite its huge empirical success in prac-
tice and some ﬁrst analysis in the literature, there is so far
no rigorous theoretical analysis of RMSProp. We will an-
alyze RMSProp given in Algorithm 3 in the framework of
of online convex optimization.

(δT,i − δ1,i)

4. RMSProp and SC-RMSProp

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

Algorithm 3 RMSProp

Input: θ1 ∈ C , δ > 0, α > 0, v0 = 0 ∈ Rd
for t = 1 to T do
gt ∈ ∂ft(θt)
vt = βtvt−1 + (1 − βt)(gt (cid:12) gt)
Set (cid:15)t = δ√
t
At = diag(
θt+1 = P At
C

and αt = α√
t
√
vt) + (cid:15)tI
(cid:0)θt − αtA−1

t gt

(cid:1)

end for

First, we will show that RMSProp reduces to Adagrad for
a certain choice of its parameters. Second, we will prove
T ) sim-
for the general convex case a regret bound of O(
ilar to the bound given in Theorem 2.1. It turns out that
the convergence analysis requires that in the update of the
weighted cumulative squared gradients (vt) , it has to hold

√

1 −

≤ βt ≤ 1 −

1
t

γ
t

,

for some 0 < γ ≤ 1. This is in contrast to the original
suggestion of (Hinton et al., 2012) to choose βt = 0.9.
It will turn out later in the experiments that the constant
choice of βt leads sometimes to divergence of the sequence,
whereas the choice derived from our theoretical analysis
always leads to a convergent scheme even when applied to
deep neural networks. Thus we think that the analysis in
the following is not only interesting for the convex case but
can give valuable hints how the parameters of RMSProp
should be chosen in deep learning.

Before we start the regret analysis we want to discuss the
sequence vt in more detail. Using the recursive deﬁnition
of vt, we get the closed form expression

vt,i =

(1 − βj)

t
(cid:88)

j=1

t
(cid:89)

k=j+1

βkg2

j,i.

(cid:81)t

1
j=1
j
product

k=j+1

ones

k−1

k g2

j,i,
gets

With βt = 1− 1
using
and
(cid:81)t
k−1

t one gets, vt,i = (cid:80)t
telescoping
the
k = j
(cid:80)t
j=1 g2

t and thus

j,i.

k=j+1

vt,i = 1
t
If one uses additionally the stepsize scheme αt = α√
t
(cid:15)t = δ√
, then we recover the update scheme of Adagrad,
T
see (4), as a particular case of RMSProp. We are not aware
of that this correspondence of Adagrad and RMSProp has
been observed before.

and

The proof of the regret bound for RMSProp relies on the
following lemma.

for t > 1 suppose (cid:112)(t − 1)(cid:15)t−1 ≤

t(cid:15)t, then

√

T
(cid:88)

t=1

√

g2
t,i
t vt,i +

≤

√

t(cid:15)t

2(2 − γ)
γ

(cid:16)(cid:112)T vT,i +

√

(cid:17)

.

T (cid:15)T

t ≤ βt ≤ 1 − γ

Corollary 4.1 Let Assumptions A1, A2 hold and suppose
that 1 − 1
t for some 0 < γ ≤ 1, and
t ≥ 1. Also for t > 1 suppose (cid:112)(t − 1)(cid:15)t−1 ≤
t(cid:15)t, and
set αt = α√
t

, then

√

T
(cid:88)

t=1

αt
2

(cid:10)gt, A−1

t gt

(cid:11) ≤

α(2 − γ)
γ

d
(cid:88)

i=1

(cid:16)(cid:112)T vT,i +

√

(cid:17)

T (cid:15)T

With the help of Lemma 4.1 and Corollary 4.1 we can now
state the regret bound for RMSProp.

Theorem 4.1 Let Assumptions A1, A2 hold and let θt be
the sequence generated by RMSProp in Algorithm 3, where
gt ∈ ∂ft(θt) and ft : C → R is an arbitrary convex func-
t ≤ βt ≤ 1− γ
for some α > 0 and 1− 1
tion and αt = α√
t
for some 0 < γ ≤ 1. Also for t > 1 let (cid:112)(t − 1)(cid:15)t−1 ≤
√
t(cid:15)t, then the regret of RMSProp can be upper bounded

t

for T ≥ 1 as

R(T ) ≤

(cid:16) D2
∞
2α

+

α(2 − γ)
γ

(cid:17) d
(cid:88)

(cid:16)(cid:112)T vT,i +

√

(cid:17)

T (cid:15)T

i=1

t , that is γ = 1, and (cid:15)t = δ

Note that for βt = 1 − 1
T where
RMSProp corresponds to Adagrad we recover the regret
bound of Adagrad in the convex case, see Theorem 2.1, up
to the damping factor. Note that in this case

(cid:112)T vT,i =

g2
j,i = (cid:107)g1:T,i(cid:107)2 .

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

j=1

4.1. SC-RMSProp

Similar to the extension of Adagrad to SC-Adagrad, we
present in this section SC-RMSProp which achieves a log-
arithmic regret bound.

Note that again there exist choices for the parameters of
SC-RMSProp such that it reduces to SC-Adagrad. The cor-
respondence is given by the choice

βt = 1 −

, αt =

(cid:15)t =

α
t

,

δt
t

,

1
t

for which again it follows vt,i = 1
j,i with the same
t
argument as for RMSProp. Please see Equation (5) for the
correspondence. Moreover, with the same argument as for
SC-Adagrad we use a decay scheme for the damping factor

j=1 g2

(cid:80)t

Lemma 4.1 Let Assumptions A1 and A2 and suppose that
1 − 1
t for some 0 < γ ≤ 1, and t ≥ 1. Also

t ≤ βt ≤ 1 − γ

(cid:15)t,i = ξ2

e−ξ1 t vt,i
t

,

i = 1, . . . , d. for ξ1 ≥ 0 , ξ2 > 0

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

Algorithm 4 SC-RMSProp

Input: θ1 ∈ C , δ0 = 1 , v0 = 0 ∈ Rd
for t = 1 to T do
gt ∈ ∂ft(θt)
vt = βtvt−1 + (1 − βt)(gt (cid:12) gt)
Set (cid:15)t = δt
At = diag(vt + (cid:15)t)
θt+1 = P At
C

(cid:0)θt − αtA−1

t gt

(cid:1)

end for

t where δt,i ≤ δt−1,i for i ∈ [d] and αt = α

t

+

α
2γ

d
(cid:88)

i=1

(1 − γ)(1 + log T )
inf j∈[1,T ] jvj,i + j(cid:15)j,i

Note that the regret bound reduces for γ = 1 to that of SC-
Adagrad. For 0 < γ < 1 a comparison between the bounds
is not straightforward as the vt,i terms cannot be compared.
It is an interesting future research question whether it is
possible to show that one scheme is better than the other
one potentially dependent on the problem characteristics.

The analysis of SC-RMSProp is along the lines of SC-
Adagrad with some overhead due to the structure of vt.

5. Experiments

t , 1 − 1

t ≤ βt ≤ 1 − γ

Lemma 4.2 Let αt = α
deﬁned in SC-RMSProp, then it holds for all T ≥ 1,
(cid:16) T (cid:0)vT,i + (cid:15)T,i
(cid:15)1,i

(cid:10)gt, A−1

α
2γ

αt
2

t gt

(cid:11) ≤

d
(cid:88)

T
(cid:88)

log

(cid:1)

(cid:17)

t and At as

t=1

i=1

+

+

α
2γ

α
2γ

T
(cid:88)

d
(cid:88)

t=2

i=1

d
(cid:88)

i=1

−t(cid:15)t,i + (t − 1)(cid:15)t−1,i
tvt,i + t(cid:15)t,i

(1 − γ)(1 + log T )
inf j∈[1,T ] jvj,i + j(cid:15)j,i

Note that for γ = 1 and the choice (cid:15)t = δt
the result of Lemma 3.2.

t this reduces to

Lemma 4.3 Let (cid:15)t ≤ 1
1 ≥ γ > 0. Then it holds,

t and 1 − 1

t ≤ βt ≤ 1 − γ

t for some

T
(cid:88)

t=2

(cid:16)

α
2

(tAt)−1 •

(cid:16) −diag((cid:15)t) + βtdiag((cid:15)t−1)
(1 − βt)

(cid:17)(cid:17)

≤

α
2γ

T
(cid:88)

d
(cid:88)

t=2

i=1

−t(cid:15)t,i + (t − 1)(cid:15)t−1,i
tvt,i + t(cid:15)t,i

+

α
2γ

d
(cid:88)

i=1

(1 − γ)(1 + log T )
inf j∈[1,T ] jvj,i + j(cid:15)j,i

Theorem 4.2 Let Assumptions A1, A2 hold and let θt be
the sequence generated by SC-RMSProp in Algorithm 4,
where gt ∈ ∂ft(θt) and ft : C → R is an arbitrary µ-
strongly convex function (µ ∈ Rd
t for some
α ≥ (2−γ)G2
t for some 0 <
∞
2 mini µi
γ ≤ 1. Furthermore, set (cid:15)t = δt
t and assume 1 ≥ δt,i > 0
and δt,i ≤ δt−1,i∀t ∈ [T ], ∀i ∈ [d], then the regret of SC-
RMSProp can be upper bounded for T ≥ 1 as

t ≤ βt ≤ 1 − γ

+) with αt = α

and 1 − 1

R(T ) ≤

D2

∞ tr(diag(δ1))
2α

+

α
2γ

d
(cid:88)

i=1

log

(cid:16) T vT,i + δT,i
δ1,i

(cid:17)

+

1
2

d
(cid:88)

i=1

inf
t∈[T ]

(cid:16) (θt,i − θ∗

i )2

α

−

α
γ(tvt,i + t(cid:15)t,i)

(cid:17)

(δT,i − δ1,i)

(cid:17)

(cid:16) f (xt)−p∗
p∗

The idea of the experiments is to show that the proposed
algorithms are useful for standard learning problems in
both online and batch settings. We are aware of the fact
that in the strongly convex case online to batch conver-
sion is not tight (Hazan & Kale, 2014), however that does
not necessarily imply that the algorithms behave gener-
ally suboptimal. We compare all algorithms for a strongly
convex problem and present relative suboptimality plots,
, where p∗ is the global optimum, as well
log10
as separate regret plots, where we compare to the best op-
timal parameter in hindsight for the fraction of training
points seen so far. On the other hand RMSProp was origi-
nally developed by (Hinton et al., 2012) for usage in deep
learning. As discussed before the ﬁxed choice of βt is
not allowed if one wants to get the optimal O(
T ) regret
bound in the convex case. Thus we think it is of interest to
the deep learning community, if the insights from the con-
vex optimization case transfer to deep learning. Moreover,
Adagrad and RMSProp are heavily used in deep learning
and thus it is interesting to compare their counterparts SC-
Adagrad and SC-RMSProp developed for the strongly con-
vex case also in deep learning. The supplementary material
contains additional experiments on various neural network
models.

√

Datasets: We use three datasets where it is easy, difﬁ-
cult and very difﬁcult to achieve good test performance,
just in order to see if this inﬂuences the performance. For
this purpose we use MNIST (60000 training samples, 10
classes), CIFAR10 (50000 training samples, 10 classes)
and CIFAR100 (50000 training samples, 100 classes). We
refer to (Krizhevsky, 2009) for more details on the CIFAR
datasets.

Algorithms: We compare 1) Stochastic Gradient Descent
(SGD) (Bottou, 2010) with O(1/t) decaying step-size for
the strongly convex problems and for non-convex problems
we use a constant learning rate, 2) Adam (Kingma & Bai,
2015) , is used with step size decay of αt = α√
for strongly
t
convex problems and for non-convex problems we use a
constant step-size. 3) Adagrad, see Algorithm 1, remains
the same for strongly convex problems and non-convex

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

(a) CIFAR10

(b) CIFAR100

(c) MNIST

Figure 1. Relative Suboptimality vs Number of Epoch for L2-Regularized Softmax Regression

(a) CIFAR10

(b) CIFAR100

(c) MNIST

Figure 2. Regret (log scale) vs Dataset Proportion for Online L2-Regularized Softmax Regression

problems. 4) RMSProp as proposed in (Hinton et al., 2012)
is used for both strongly convex problems and non-convex
problems with βt = 0.9 ∀t ≥ 1. 5) RMSProp (Ours) is
and βt = 1 − γ
used with step-size decay of αt = α√
t .
t
In order that the parameter range is similar to the original
RMSProp ((Hinton et al., 2012)) we ﬁx as γ = 0.9 for all
experiment (note that for γ = 1 RMSProp (Ours) is equiv-
alent to Adagrad), 6) SC-RMSProp is used with stepsize
αt = α
t and γ = 0.9 as RMSProp (Ours) 7) SC-Adagrad
is used with a constant stepsize α. The decaying damp-
ing factor for both SC-Adagrad and SC-RMSProp is used
with ξ1 = 0.1, ξ2 = 1 for convex problems and we use
ξ1 = 0.1, ξ2 = 0.1 for non-convex deep learning prob-
lems. Finally, the numerical stability parameter δ used in
Adagrad, Adam, RMSProp is set to 10−8 as it is typically
recommended for these algorithms.

Setup: Note that all methods have only one varying pa-
rameter: the stepsize α which we choose from the set of
{1, 0.1, 0.01, 0.001, 0.0001} for all experiments. By this
setup no method has an advantage just because it has more
hyperparameters over which it can optimize. The optimal
rate is always chosen for each algorithm separately so that
one achieves either best training objective or best test per-
formance after a ﬁxed number of epochs.

yi

(cid:33)

i=1

log

+ λ

1
m

K
(cid:88)

m
(cid:88)

xi+byi

j xi+bj

(cid:107)θk(cid:107)2

J(θ) = −

Strongly Convex Case - Softmax Regression: Given the
training data (xi, yi)i∈[m] and let yi ∈ [K]. we ﬁt a lin-
ear model with cross entropy loss and use as regularization
the squared Euclidean norm of the weight parameters. The
objective is then given as
(cid:32)

eθT
(cid:80)K
j=1 eθT
All methods are initialized with zero weights. The regu-
larization parameter was chosen so that one achieves the
best prediction performance on the test set. The results are
shown in Figure 1. We also conduct experiments in an on-
line setting, where we restrict the number of iterations to
the number of training samples. Here for all the algorithms,
we choose the stepsize resulting in best regret value at the
end. We plot the Regret ( in log scale ) vs dataset propor-
tion seen, and as expected SC-Adagrad and SC-RMSProp
outperform all the other methods across all the considered
datasets. Also, RMSProp (Ours) has a lower regret values
than the original RMSProp as shown in Figure 2.

k=1

Convolutional Neural Networks: Here we test a 4-layer
CNN with two convolutional (32 ﬁlters of size 3 × 3) and
one fully connected layer (128 hidden units followed by
0.5 dropout). The activation function is ReLU and after

150100150200Epoch10−1100 Relative Sub-optimality (log scale) SGDAdamAdagradRMSPropRMSProp (Ours)SC-RMSPropSC-Adagrad150100150200Epoch10−1100 Relative Sub-optimality (log scale) SGDAdamAdagradRMSPropRMSProp (Ours)SC-RMSPropSC-Adagrad150100150200Epoch10−210−1100 Relative Sub-optimality (log scale) SGDAdamAdagradRMSPropRMSProp (Ours)SC-RMSPropSC-Adagrad0.10.20.30.40.50.60.70.80.91.0Dataset proportion104105 Regret (log scale)SGDAdamAdagradRMSPropRMSProp (Ours)SC-RMSPropSC-Adagrad0.10.20.30.40.50.60.70.80.91.0Dataset proportion104105 Regret (log scale)SGDAdamAdagradRMSPropRMSProp (Ours)SC-RMSPropSC-Adagrad0.10.20.30.40.50.60.70.80.91.0Dataset proportion103104 Regret (log scale)SGDAdamAdagradRMSPropRMSProp (Ours)SC-RMSPropSC-AdagradVariants of RMSProp and Adagrad with Logarithmic Regret Bounds

(a) CIFAR10

(b) CIFAR100

(c) MNIST

Figure 3. Test Accuracy vs Number of Epochs for 4-layer CNN

(a) Training Objective

(b) Test Accuracy

Figure 4. Plots of ResNet-18 on CIFAR10 dataset

the last convolutional layer we use max-pooling over a 2 ×
2 window and 0.25 dropout. The ﬁnal layer is a softmax
layer and the ﬁnal objective is cross-entropy loss. This is
a pretty simple standard architecture and we use it for all
datasets. The results are shown in Figure 3. Both RMSProp
(Ours) and SC-Adagrad perform better than all the other
methods in terms of test accuracy for CIFAR10 dataset. On
both CIFAR100 and MNIST datasets SC-RMSProp is very
competitive.

Residual Network: We also conduct experiments for
ResNet-18 network proposed in (He et al., 2016a) where
the residual blocks are used with modiﬁcations proposed
in (He et al., 2016b) on CIFAR10 dataset. We report the
results in Figures 4. SC-Adagrad, SC-RMSProp and RM-
SProp (Ours) have the best performance in terms of test
Accuracy and RMSProp (Ours) has the best performance
in terms of training objective along with Adagrad.

We also test all the algorithms on a simple 3-layer Mul-
tilayer perceptron which we include in the supplemen-
tary material. Given these experiments, we think that SC-
RMSProp and SC-Adagrad are valuable new adaptive gra-
dient techniques for deep learning.

6. Conclusion

We have analyzed RMSProp originally proposed in the
deep learning community in the framework of online con-
vex optimization. We show that the conditions for conver-
gence of RMSProp for the convex case are different than
what is used by (Hinton et al., 2012) and that this leads
to better performance in practice. We also propose vari-
ants SC-Adagrad and SC-RMSProp which achieve loga-
rithmic regret bounds for the strongly convex case. More-
over, they perform very well for different network models
and datasets and thus they are an interesting alternative to
existing adaptive gradient schemes. In the future we want
to explore why these algorithms perform so well in deep
learning tasks even though they have been designed for the
strongly convex case.

Acknowledgements

We would like to thank Shweta Mahajan and all the review-
ers for their insightful comments.

150100150200Epoch0.600.620.640.660.680.700.72 Test Accuracy SGDAdamAdagradRMSPropRMSProp (Ours)SC-RMSPropSC-Adagrad150100150200Epoch0.260.280.300.320.340.360.380.40 Test Accuracy SGDAdamAdagradRMSPropRMSProp (Ours)SC-RMSPropSC-Adagrad150100150200Epoch0.9800.9820.9840.9860.9880.9900.9920.994 Test Accuracy SGDAdamAdagradRMSPropRMSProp (Ours)SC-RMSPropSC-Adagrad1100200300400Epoch0.10.20.30.40.50.6 Training Objective SGDAdamAdagradRMSPropRMSProp (Ours)SC-RMSPropSC-Adagrad1100200300400Epoch0.700.720.740.760.780.800.820.840.86 Test AccuracySGDAdamAdagradRMSPropRMSProp (Ours)SC-RMSPropSC-AdagradVariants of RMSProp and Adagrad with Logarithmic Regret Bounds

Krizhevsky, A. Learning multiple layers of features from
tiny images. Technical report, University of Toronto,
2009.

McMahan, H Brendan. A survey of algorithms and
arXiv preprint

analysis for adaptive online learning.
arXiv:1403.3465, 2014.

Ruder, S. An overview of gradient descent optimization

algorithms. preprint, arXiv:1609.04747v1, 2016.

Schaul, T., Antonoglou, I., and Silver, D. Unit tests for

stochastic optimization. In ICLR, 2014.

Schmidhuber, J. Deep learning in neural networks: An

overview. Neural Networks, 61:85 – 117, 2015.

Srivastava, Nitish, Hinton, Geoffrey E, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:
a simple way to prevent neural networks from overﬁt-
Journal of Machine Learning Research, 15(1):
ting.
1929–1958, 2014.

Szegedy, C., Ioffe, S., and Vanhoucke, V.

Inception-v4,
inception-resnet and the impact of residual connections
on learning. In ICLR Workshop, 2016a.

Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wo-
jna, Z. Rethinking the inception architecture for com-
puter vision. In CVPR, 2016b.

Zeiler, M. D. ADADELTA: An adaptive learning rate

method. preprint, arXiv:1212.5701v1, 2012.

Zinkevich, M. Online convex programming and general-

ized inﬁnitesimal gradient ascent. In ICML, 2003.

References

Bottou, L. Large-scale machine learning with stochastic
gradient descent. In Proceedings of COMPSTAT’2010,
pp. 177–186. Springer, 2010.

Daniel, C., Taylor, J., and Nowozin, S. Learning step size
controllers for robust neural network training. In AAAI,
2016.

Dauphin, Y., de Vries, H., and Bengio, Y. Equilibrated
adaptive learning rates for non-convex optimization. In
NIPS, 2015.

Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient
methods for online learning and stochastic optimization.
Journal of Machine Learning Research, 12:2121–2159,
2011.

Hazan, E.

Introduction to online convex optimization.
Foundations and Trends in Optimization, 2:157–325,
2016.

Hazan, E. and Kale, S. Beyond the regret minimization bar-
rier: optimal algorithms for stochastic strongly-convex
optimization. Journal of Machine Learning Research,
15(1):2489–2512, 2014.

Hazan, E., Agarwal, A., and Kale, S. Logarithmic re-
gret algorithms for online convex optimization. Machine
Learning, 69(2-3):169–192, 2007.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, pp. 770–778, 2016a.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Identity mappings in deep residual networks. In
European Conference on Computer Vision, pp. 630–645.
Springer, 2016b.

Hinton, G., Srivastava, N., and Swersky, K. Lecture 6d
- a separate, adaptive learning rate for each connection.
Slides of Lecture Neural Networks for Machine Learn-
ing, 2012.

Karpathy, A. and Fei-Fei, L. Deep visual-semantic align-
In CVPR,

ments for generating image descriptions.
2016.

Kingma, D. P. and Bai, J. L. Adam: a method for stochastic

optimization. ICLR, 2015.

Koushik, Jayanth and Hayashi, Hiroaki.

stochastic gradient descent with feedback.
preprint arXiv:1611.01505, 2016.

Improving
arXiv

