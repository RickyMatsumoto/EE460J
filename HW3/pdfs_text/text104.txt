Clustering High Dimensional Dynamic Data Streams

A Proofs of Section 3
Proof of Lemma 2.2. Fix an i and consider a grid Gi. For each center zj, denote Xj,α the indicator random variable for the
event that the distance to the boundary in dimension α of grid Gi is at most ∆/(2i+1d). Since in each dimension, if the
center is close to a boundary, it contributes a factor at most 2 to the total number of close cells. It follows that the number
of cells that have distance at most ∆/(2i+1d) to zj is at most,
(cid:80)d

N = 2

α=1 Xj,α .

Deﬁning Yj,α = 2Xj,α, we obtain,

E[N ] = E

Yj,α

=

E[Yj,α].

(cid:34) d
(cid:89)

α=1

(cid:35)

d
(cid:89)

α=1

We have that P r[Xj,α = 1] ≤ 1/d and so we get,

E[Yj,α] ≤ E [1 + Xj,α] = 1 + E[Xj,α] ≤ 1 + 1/d.

Thus E(N ) = (cid:81)d
α=1 E[Yj,α] ≤ (1+1/d)d ≤ e. Thus the expected number of center cells is at most (1+1/d)d|Z| ≤ e|Z|.
By Markov’s inequality, the probability that we have more than e|Z|(L+1)/ρ center cells in each grid is at most ρ/(L+1).
By a union bound, the probability that in any grid we have more than e|Z|(L + 1)/ρ center cells is at most ρ.

Proof of Lemma 3.4. Let L(cid:48) = L + 1. Note that for each point p ∈ P , |d(ci
, Z))/πi and A = (cid:80)
ˆA = (cid:80)
p, Z) − d(ci+1

p, Z) − d(ci+1

p∈∪{C∈C}(d(ci

p∈S(d(ci

p

p

p, Z) − d(ci−1

, Z)| ≤ ∆
, Z)). We have that E( ˆA) = A. Let

d/2i. Denote

p

√

√

where Ip∈S is the indicator function that p ∈ S. Then we have that V ar(Xp) ≤ ∆2d/(4iπi) and b := maxp |Xp| ≤
∆

d/(2iπi). By Bernstein’s inequality,

Xp := Ip∈S(d(ci

p, Z) − d(ci+1

p

, Z))/πi,

By setting t = (cid:15)OPT/L(cid:48), we have that

(cid:104)

(cid:105)
| ˆA − A| > t

P r

≤ 2e

−

t2
2|P |∆2d/(4iπi)+2bt/3

− 3×2i−1t2πi
(βOPT+ t
3 )∆

√

d .

≤ 2e

(cid:20)
| ˆA − A| >

P r

(cid:21)

(cid:15)OPT
L(cid:48)

≤ 2e− ln 2L(cid:48)∆dk

ρ ≤

ρ
L(cid:48)∆dk .

(7)

(8)

Thus with probability 1−ρ/(L(cid:48)∆dk), ˆA is an (cid:15)OPT/L(cid:48) additive approximation to the sum (cid:80)

p∈P d(ci

p, Z)−d(ci+1

p

, Z).

B Proof of Theorem 3.6
Before we prove this theorem, we ﬁrst present Lemma B.1 and Lemma B.2. In Algorithm 1, for each level i ∈ [0, L], let Hi
be the set of cells in Gi whose frequencies are returned by HEAVY-HITTER in the RetrieveFrequency procedure.
For each C ∈ Hi, let (cid:99)|C| be the returned frequency of C. Let H (cid:48)
i be the set of cells in Gi whose frequencies are returned by
a K-set in the RetrieveFrequency procedure. Then Hi and H (cid:48)
Lemma B.1. Let L(cid:48) = L + 1. Fix (cid:15), ρ ∈ (0, 1/2). Let Z ∗ ⊂ [∆]d be a set of optimal k-centers for the k-median problem of
the input point set. For each i ∈ [0, L], if at most ekL(cid:48)/ρ cells C in Gi satisfy d(C, Z ∗) ≤ ∆/(2i+1d), then with probability
1 − ρ/L(cid:48), the following two statements hold:

i are complements in Gi.

1.

(cid:12)
(cid:80)
(cid:12)
(cid:12)
2. (cid:80)

C∈H (cid:48)
i

((cid:99)|C| − |C|) (cid:0)d(c(C), Z) − d(c(CP ), Z)(cid:1)(cid:12)
(cid:12) ≤ (cid:15)OPT
(cid:12)

2L(cid:48)

C∈Hi

for every Z ⊂ [∆]d.

|C|diam(C) ≤ βOPT for β = 3d3/2

Proof of Lemma B.1. Let L(cid:48) = L + 1. Fix a value i ∈ [0, L] and then W = ∆/2i is the width of a cell in Gi. Since at most
ekL(cid:48)/ρ cells in Gi satisfy d(C, Z ∗) ≤ W/(2d), then of the remaining cells, at most 2kL(cid:48)/ρ cells can contain more than
ρdOPT/(W kL(cid:48)) points. This is because each such cells contribute at least ρdOPT
to the cost which sums to
W kL(cid:48)
OPT. Therefore, at most (e + 2)L(cid:48)k/ρ cells contain more than ρdOPT/(W kL(cid:48)) points.
The number of cells in grid Gi is at most N = (1 + 2i)d (and perhaps as few as 2id, depending on the random vector
v), so HEAVY-HITTER receives cells of at most N types. Enumerating all cells C ∈ Gi such that |Cj| ≥ |Cj+1|, deﬁne
fj = |Cj|. Algorithm 1 sets k(cid:48) = (e + 2)L(cid:48)k/ρ, and the additive error of the estimator of fi of HEAVY-HITTER is given

2d = ρOPT

2kL(cid:48)

W

Clustering High Dimensional Dynamic Data Streams

(cid:113)(cid:80)N

√

(cid:113)

j=k(cid:48)+1 f 2

j . We know that for all j > k(cid:48) the value fj ≤ ρdOPT/(W kL(cid:48)). Moreover, the sum (cid:80)N

by (cid:15)(cid:48)
j=k(cid:48)+1 fj ≤
2dOPT/W because each point is at distance at least W/(2d) to a point of Z ∗. Under these two restraints, the grouping of
maximal error is with fj = ρdOPT/(W kL(cid:48)) for k(cid:48) < j ≤ k(cid:48) + 2kL(cid:48)/ρ and fj = 0 for j > k(cid:48) + 2kL(cid:48)/ρ. Then the additive
error becomes (cid:15)(cid:48)(cid:112)2ρ/(kL(cid:48))dOPT/W .
The error from a single cell Cj is at most |fj − ˆfj|
dW , and HEAVY-HITTER gaurantees with probability 1 − δ that
|fj − ˆfj| ≤ (cid:15)(cid:48)(cid:112)2ρ/(kL(cid:48))dOPT/W for every j. Therefore to ensure total error over all k(cid:48) cells is bounded by (cid:15)OPT/(2L(cid:48)),
we set (cid:15)(cid:48) ≤ (cid:15)
8(2+e)2kd3L(cid:48)3 . Setting δ = ρ/L(cid:48), the above bound holds with probability at least 1 − ρ/L(cid:48).
For the second claim, we must bound (cid:80)
|C|. Hi consists of the top k(cid:48) cells when ordered by value of ˆfj. This
may differ from the top k(cid:48) cells when ordered by value of fj, but if j and j(cid:48) change orders between these two orderings
then |fj − fj(cid:48)| ≤ 2(cid:15)(cid:48)(cid:112)2ρ/(kL(cid:48))dOPT/W . Since the sum may swap up to k(cid:48) indices, the difference is bounded by
2k(cid:48)(cid:15)(cid:48)(cid:112)2ρ/(kL(cid:48))dOPT/W . By setting (cid:15)(cid:48) ≤
8(2+e)2dkL(cid:48) , we can ensure that the difference is at most dOPT/W . We
know that (cid:80)N
dW . Therefore
(cid:80)

j=k(cid:48)+1 fj ≤ 2dOPT/W , and so (cid:80)

|C| ≤ 3dOPT/W . For all cells C ∈ Gi, diam(C) =

C∈H (cid:48)
i

C∈H (cid:48)
i

(cid:113)

√

ρ

ρ

|C|diam(C) ≤ 3d3/2OPT.

C∈H (cid:48)
i

Lemma B.2. Let L(cid:48) = L + 1. In Algorithm 1, ﬁxing (cid:15), ρ ∈ (0, 1/2), o ∈ O and i ∈ [0, L], if OPT/2 ≤ o ≤ OPT, then
with probability 1 − ρ/(L(cid:48)∆kd), at most (2+e)L(cid:48)k

ρ cells of Gi contain a point of Si,o.

+ 24d4L(cid:48)3k
(cid:15)2

ln 1

ρ

Proof. Similar to the proof of Lemma B.1, there are at most k(cid:48) = (2+e)L(cid:48)k/ρ cells C in Gi that satisfy |C| ≥ ρdOPT/(W k)
and/or d(C, Z ∗) ≤ W/(2d). Considering the other cells, together they contain at most 2dOPT/W points. So by a Chernoff
bound, with probability 1 − ρ/(L(cid:48)∆kd) at most O(2dπi,oOPT/(W ρ)) ≤ 24d4L(cid:48)3k ln 1
ρ /(cid:15)2 points are sampled. The claim
follows since each non-empty cell must contain at least one point.

Proof of Theorem 3.6. Let L(cid:48) = L + 1. W.l.o.g. we assume ρ ≥ ∆−d, since otherwise we store the entire set of points and
the theorem is proved. By Lemma 2.2, with probability at least 1 − ρ, for every level i ∈ [0, L], at most ekL(cid:48)/ρ cells C in
Gi satisfy d(C, Z ∗) ≤ ∆/(2i+1d). Conditioning on this event, we will show 1) in the query phase, if o∗ ≤ OPT, then with
probability at least 1 − 4ρ, S is the desired coreset; 2) there exists o ≤ OPT in the guesses O = {1, 2, 4, . . . , ∆d+1} such
that with probability 1 − 4ρ, none of the K-set structures return Nil. 1) and 2) guarantee the correctness of the algorithm.
Note that one can always rescale ρ to ρ/9 to achieve the correct probability bound. Finally, we will bound the space, update
time and query time of the algorithm.
To show 1), we ﬁrst note that the coreset size is at most O(KL) as desired. Then by Lemma 3.2, we only need to show
that with probability at least 1 − 4ρ, for any k-set Z ⊂ [∆]d and any level i ∈ [−1, L],

|cost(Gi, Z) − (cid:100)cost(Gi, Z)| ≤

(cid:15)OPT
L(cid:48)

,

where the value of each (cid:99)|C| is returned by RetrieveFrequency. For each level i, we denote Ci as the set of cells
that gets frequency from a HEAVY-HITTER instances in the RetrieveFrequency procedure, and Si = {p ∈ C :
C (cid:54)∈ Ci, ho∗,i(p) = 1} be the set of points sampled in the rest of cells. Since KSo∗,i does not return Fail, then for each
C ∈ Gi\Ci, (cid:99)|C| = |Si ∩ C|/πi(o∗). Fix a k-set Z ⊂ [∆]d, we rewrite the cost as,
(d(ci

(cid:99)|C| (cid:0)d(c(C), Z) − d(c(CP ), Z)(cid:1) +

, Z)))/πi(o∗),

p, Z) − d(ci−1

(cid:100)cost(Gi, Z) =

(cid:88)

(cid:88)

p

C∈Ci

p∈Si

where CP is the parent cell of C in grid Gi−1. By Lemma B.1 we have that, with probability at least 1 − ρ/L(cid:48), for every
Z ⊂ [∆]d,

(cid:99)|C| (cid:0)d(c(C), Z) − d(c(CP ), Z)(cid:1) −

|C| (cid:0)d(c(C), Z) − d(c(CP ), Z)(cid:1)

≤

|C|diam(C) ≤ 3d3/2OPT. Conditioning on this event, by Lemma 3.4, with probability at least 1 −

and that, (cid:80)
ρ/(L(cid:48)∆kd),

C∈Gi\Ci

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
C∈Ci

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
p∈Si

(cid:88)

C∈Ci

C∈Gi\Ci

(d(ci

p, Z) − d(ci−1

p

, Z)))/πi −

(cid:88)

|C| (cid:0)d(c(C), Z) − d(c(CP ), Z)(cid:1)

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:15)OPT
2L(cid:48)

,

(cid:15)OPT
2L(cid:48)

.

By a union bound, we show with probability at least 1 − 4ρ, for any k-set Z ⊂ [∆]d and any level i ∈ [−1, L],

Clustering High Dimensional Dynamic Data Streams

|cost(Gi, Z) − (cid:100)cost(Gi, Z)| ≤

(cid:15)OPT
L(cid:48)

,

as desired.
To show 2), we will consider some OPT/2 ≤ o ≤ OPT. By Lemma B.2 with probablity at least 1 − ρ/∆kd, the total
number of cells occupied by sample points in each level is upper bounded by K = (2+e)L(cid:48)k
ρ . Thus by the
guarantee of the K-Set structure, with probability at least 1 − ρ, none of the KSo,0, KSo,1 . . . , KSo,L will return Fail.
The memory requirement of the algorithm is determined by the L instances of HEAVY-HITTER and the dL2 instances
of K-set. By Theorem 3.5, each instance of HEAVY-HITTER requires O (cid:0)(k(cid:48) + 1
δ log m(cid:1) bits of space. Here
N ≤ (1 + ∆/W )d ≤ ∆d and m is the maximum number of elements active in the stream. Since we require that at most
one point exists at each location at the same time, we have that m ≤ N . The parameters are set to k(cid:48) = (2 + e)Lk/ρ,
(cid:15)(cid:48) =
bits. For each
K-Set data structure, it requires

, and δ = ρ/L. This translates to a space bound of O

ρ
8(2+e)2kd3L3

+ 24d4L(cid:48)3k
(cid:15)2

dL + log 1
ρ

(cid:17) d4L5k
ρ(cid:15)2

(cid:15)(cid:48)2 ) log N

ln 1

(cid:113)

(cid:16)

(cid:17)

(cid:16)

(cid:15)

ρ

O(KdL log(KL/ρ)) = O

(cid:18) d5L4k

(cid:15)2 +

(cid:19)

dkL2
ρ

log

dkL
(cid:15)ρ

(cid:16) d6L6k

(cid:17)

ρ

(cid:15)2 + d2kL4

log dkL
bits of space. In total, there are O(dL2) K-Set instances and thus all K-Set instances cost O
(cid:15)ρ
bits of space. By the same argument as in the ofﬂine case, the last paragraph of the proof of Theorem 3.3, the size of the
coreset is at most O((k(cid:48) + K)L) = O(d4kL4(cid:15)−2 + kL2/ρ) points. Finally, to derandomize the fully random functions, we
use Nissan’s pseudorandom generator (Nisan, 1992) in a similar way used in (Indyk, 2000b). But our pseudo-random bits
only need to fool the sampling part of the algorithm rather than whole algorithm. We consider an augmented streaming al-
gorithm A that does exactly the same as in CoreSet but with all the HEAVY-HITTER operations removed. Thus all K-set
instances will have identical distribution with the ones in algorithm CoreSet. A uses O(KdL log(KL/ρ)) bits of space.
To fool this algorithm, using Nissan’s pseudo-random generator, the length of random seed to generate the hash functions
(cid:16)(cid:16) d7kL7
we need is of size O(KdL log(KL/ρ) log(|O|∆d)) = O
. This random seed is thus sufﬁcient
(cid:17)
ρ(cid:15) + d5kL6

to be used in Algorithm CoreSet. Thus the total space used in the algorithm is O
bits.
Regarding the update time, for the HEAVY-HITTER operations, it requires O(L log N ) = O(dL2) time. For the K-
set operations, it requires |O|LO(log(KL/ρ)) = dL2 log(dkL/(ρ(cid:15))) time. The de-randomized hash operation takes
O(dL) more time per update. The ﬁnal query time is dominated by the HEAVY-HITTER data structure, which requires
poly(d, k, L, 1/(cid:15)) time.

log dkL
ρ(cid:15)
(cid:16)(cid:16) d7kL7

(cid:15)2 + d3kL5

(cid:15)2 + d3kL5

log dkL

(cid:15)2ρ

(cid:17)

(cid:17)

(cid:17)

ρ

ρ

C Full Construction of Positively Weighted Coreset
In this section, we will introduce a modiﬁcation to our previous coreset construction, which leads to a coreset with all
positively weighted points. The high level idea is as follows. When considering the estimate of the number of points in
a cell, the estimate is only accurate when it truly contains a large number of points. However, in the construction of the
previous section, we sample from each cell of each level, even though some of the cells contain a single point. For those
cells, we cannot adjust their weights from negative to positive, since doing so would introduce large error. In this section,
we introduce an ending level to each point. In other words, the number of points of a cell is estimated by sampling only if
it contains many points. Thus, the estimates will be accurate enough and allow us to rectify the weights to be all positive.
This section is organized as follows. We reformulate the telescope sum in Subsection 4.1, provide a different construction
(still with negative weights) in Subsection 4.2, modify our different construction to output non-negative weights in Sub-
section 4.3, and move this construction into to the streaming setting in Subsection 4.4. For simplicity of presentation, we
will use λ1, λ2, . . . to denote some ﬁxed positive universal constants.
C.1 Reformulation of the Telescope Sum
Deﬁnition C.1. A heavy cell identiﬁcation scheme H is a map H : G → {heavy, non-heavy} such that, h(C−1) =heavy
and for cell C ∈ Gi for i ∈ [0, L]

1. if |C| ≥ 2iρdOPT

k(L+1)∆ then H(C) = heavy;

2. If H(C) = non-heavy, then H(C(cid:48)) = non-heavy for every subsell C(cid:48) of C.

3. For every cell C in level L, H(C) = non-heavy.

Clustering High Dimensional Dynamic Data Streams

4. For each i ∈ [0, L], |{C ∈ Gi : H(C) = heavy}| ≤ λ1kL

, where λ1 ≤ 10 is a positive universal constant.

ρ

The output for a cell not speciﬁed by the above conditions can be arbitrary. We call a cell heavy if it is identiﬁed heavy
by H. Note that a heavy cell does not necessarily contain a large number of points, but the total number of these cells is
always bounded.

In the sequel, heavy cells are deﬁned by an arbitrary ﬁxed identiﬁcation scheme unless otherwise speciﬁed.

Deﬁnition C.2. Fix a heavy cell identiﬁcation scheme H. For level i ∈ [−1, L], let C(p, i) ∈ Gi be the cell in Gi containing
p. The ending level l(p) of a point p ∈ P is the largest level i such that H(C(p, i)) =heavy, and H(C(p, i+1)) =non-heavy.

Note that the ending level is uniquely deﬁned if a heavy cell identiﬁcation scheme is ﬁxed. We now rewrite the telescope
sum for p as follows,

p =

(cid:0)ci

p − ci−1
p

(cid:1) + cL

p − cl(p)

p

,

l(p)
(cid:88)

i=0

p = 0 and cL
(cid:0)d(ci

p = p. For arbitrary k-centers Z ⊂ [∆]d, we write,
p, Z) − d(ci−1

where c−1
d(p, Z) = (cid:80)l(p)
i=0
Let Pl be all the points with ending level l(p) = l. We now present the following lemmas.
Lemma C.3. Let Pi be the set of points with ending level i. Let Z ∗ ⊂ [∆]d be a set of optimal k-centers for the k-
median problem of the input point set. Assume that for each i ∈ [−1, L], at most ek(L + 1)/ρ cells C in Gi satisfy
d(C, Z ∗) ≤ ∆/(2i+1d). Then

p , Z) − d(cl(p)

, Z)(cid:1) + d(cL

, Z) + d(0, Z)

p

p

√
∆
2i ≤ λ2d3/2OPT,

d

|Pi| ·

where λ2 > 0 is a universal constant.

Before we prove this lemma, we ﬁrst introduce the following lemmas to bound the cells with a large number of points.
Lemma C.4. Assume that for each i ∈ [0, L], at most ek(L + 1)/ρ cells C in Gi satisfy d(C, Z ∗) ≤ ∆/(2i+1d). Then for
any r > 0 there are at most (e+2r)k(L+1)

cells that satisfy |C| ≥ 2iρdOPT
rk(L+1)∆ .

ρ

Proof of Lemma C.4. Let L(cid:48) = L + 1. Fix a value i ∈ [0, L] and then W = ∆/2i is the width of a cell in Gi. Since at most
ekL/ρ cells in Gi satisfy d(C, Z ∗) ≤ W/(2d), then of the remaining cells, each contribute at least ρdOPT
2rkL(cid:48) to the
rW kL(cid:48)
cost, and the cost of these cells is at most OPT. Therefore there can be at most 2rL(cid:48)k/ρ cells such that d(C, Z ∗) > W/(2d).
Along with the at most ekL(cid:48)/ρ cells (by the assumption) such that d(C, Z ∗) ≤ W/(2d), there are at most (e + 2r)L(cid:48)k/ρ
cells that contain at least ρdOPT/(rW kL(cid:48)) points.

2d = ρOPT

W

Lemma C.5. Assume that for each i ∈ [0, L], at most ek(L + 1)/ρ cells C in Gi satisfy d(C, Z ∗) ≤ ∆/(2i+1d). Then for
i ∈ [−1, L], the points of Pi can be partitioned to at most k(cid:48) = 2(e+6)k(L+1)
groups, G1, G2, . . . , Gk(cid:48), such that for each
ρ
j ∈ [k(cid:48)], there exists a C ∈ Gi, such that Gj ∈ C, |Gj| < 5 2i−1ρdOPT
k(L+1)∆ .

(cid:104) 2i−1ρdOPT

Proof of Lemma C.5. Let L(cid:48) = L + 1. For each heavy cell in Gi, if the number of points falling into its non-heavy subcells
(in Gi+1) is less than 2i−1ρdOPT
kL(cid:48)∆ , we group all these subcells into a single group. Let the groups formed this way be called
type I, and by Property 4 of Deﬁnition 4.1 there are at most (e + 4)kL(cid:48)/ρ type I groups.
For each of the remaining heavy cells in Gi, we group its subcells into groups such that each group contains a number
. This can be done since each non-heavy subcell contains less than
of points in the interval
2i+1ρdOPT
kL(cid:48)∆

kL(cid:48)∆ , 5 2i−1ρdOPT
points, and the total number of points contained in them is at least 2i−1ρdOPT

(otherwise we
would have formed a type I group). Let the groups formed this way be called type II. By the assumption of of Lemma C.3,
at most ekL(cid:48)
2i+2d from an optimal center of Z ∗. Since each type II group
contains at least 2i−1ρdOPT
points, by the same argument as in Lemma C.4, the number of type II groups further than
distance ∆

ρ of these non-heavy subcells are within distance ∆

2i+2d from an optimal center is at most 8kL(cid:48)

= 4 2i−1ρdOPT

kL(cid:48)∆

kL(cid:48)∆

kL(cid:48)∆

kL(cid:48)∆

(cid:17)

ρ . We conclude that,
(e + 4)kL(cid:48)
ρ

(e + 8)kL(cid:48)
ρ

+

.

k(cid:48) ≤

Clustering High Dimensional Dynamic Data Streams

Proof of Lemma C.3. Let L(cid:48) = L+1. Fix a value i ∈ [−1, L] and then W = ∆/2i is the upper bound of the width of a cell
√
in Gi. Let G1, G2, . . . Gk(cid:48) be group of points satisfying Lemma C.5. Thus, (cid:80)
d
2i ≤
λ(cid:48)kL(cid:48)
2i ≤ λ2d3/2OPT for some universal constants λ(cid:48) and λ2.
ρ

· 2i+1ρdOPT
kL(cid:48)∆

2i+1ρdOPT
kL(cid:48)∆

2i ≤ (cid:80)

j∈[k(cid:48)]

· ∆

p∈Pl

√

∆

∆

√

d

d

Proof of Proposition C.10. First notice that the weighted set satisﬁes the about condition is an (cid:15)-coreset. If we replace each
(cid:99)|Ci| by the exact number of points in |Ci|, then the new weighted set is an ((cid:15)/2)-coreset. For each C ∈ G, let bC be the new
value returned by the algorithm, and bq is the new value of a point q ∈ S. The error of the cost introduced is at most,

L
(cid:88)





A =

(cid:88)

|(cid:99)|C| − bC| +

i=0

C∈Gi: heavy

(cid:88)

p∈Si−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18) 1
πi−1


(cid:19)(cid:12)
(cid:12)
(cid:12)

(cid:12)

√

∆

2i

d

.

− bp

By the procedure, the new value of a cell is always smaller than its original value, thus
√
d
∆
2i =

(cid:99)|C| − bC +

(cid:18) 1
πi−1

L
(cid:88)

− bp

A =

(cid:88)

(cid:88)









(cid:19)

i=0

C∈Gi: heavy

p∈Si−1

L
(cid:88)

i=0

gi,

where

Let

Then



gi =



(cid:88)

(cid:99)|C| − bC +

(cid:88)

C∈Gi:heavy

p∈Si−1



√

∆

d

.

2i

− bp



1
πi−1



fi =



(cid:88)

(cid:12)
(cid:12)
(cid:12)|C| − (cid:99)|C|

(cid:12)
(cid:12)
(cid:12) +

(cid:88)

C∈Gi:heavy

C(cid:48)∈Gi−1:heavy

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|Si−1 ∩ C(cid:48)|
πi−1

(cid:12)
(cid:12)
− |Pi−1 ∩ C(cid:48)|
(cid:12)
(cid:12)





√

∆

2i

d

.

Thus fi ≤ (cid:15)OPT/L by choosing appropriate λ6.
(cid:12)
(cid:12)bC − (cid:80)
(cid:12)

C∈Gi+1:heavy (cid:99)|C| − |Si∩C|

(cid:12)
(cid:12)
(cid:12). Then,

πi

Now consider heavy cell C ∈ Gi,

let sC =

sC =

bC − (cid:99)|C| + (cid:99)|C| − |C| −

(cid:88)

( (cid:99)|C(cid:48)| − |C(cid:48)|) −

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)bC − (cid:99)|C|

≤

C(cid:48)∈Gi+1:heavy

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)(cid:99)|C| − |C|

(cid:12)
(cid:12)
(cid:12) +

(cid:88)

C(cid:48)∈Gi+1:heavy

(cid:12)
(cid:12)
(cid:12) (cid:99)|C(cid:48)| − |C(cid:48)|
(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|Si ∩ C|
πi

(cid:18) |Si ∩ C|
πi

− |Pi ∩ C|

(cid:12)
(cid:12)
(cid:19)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
− |Pi ∩ C|
(cid:12)
(cid:12)

.

(cid:88)

gi =

C∈Gi−1

√
∆
d
2i +





sC

(cid:88)

p∈Si−1

1
πi−1



√
d
∆
2i ≤

1
2

− bp



gi−1 +

fi−1 + fi.

1
2

gi ≤ fi + 3

2j−ifj, and

gi ≤

fi(1 +

fi ≤ 4(cid:15)OPT.

L
(cid:88)

i=0

L
(cid:88)

i=0

i
(cid:88)

j=1

3
2j ) ≤ 4

L
(cid:88)

i=1

i−1
(cid:88)

j=0

Since g−1 = f−1 = 0, thus

(9)

(10)

Remark C.6. The multiset of centers of heavy cells with each assigned a weight of the number of points in the cell is a
O(d3/2)-coreset. This can be easily seen by removing the term of d(cL
, Z) from Equation (C.1) together with
Lemma C.3, which bounds the error introduced by this operation.

p , Z) − d(cl(p)

p

C.2 The New Construction (with arbitrary weights)
For these heavy cells, we use HEAVY-HITTER algorithms to obtain accurate estimates of the number of points in these
cells, thus providing a heavy cell identiﬁcation scheme. For the non-heavy cells, we only need to sample points from the
bottom level, GL, but with a different probability for points with different ending levels. We present the following lemma
that governs the correctness of sampling from the last level.
√

Lemma C.7. If a set of points Pi ⊂ P satisﬁes |Pi|∆

d/(2i) ≤ βOPT for some β ≥ 2(cid:15)/(3(L + 1)), let Si be an

Clustering High Dimensional Dynamic Data Streams

independent sample from Pi such that p ∈ Si with probability
√

πi ≥ min

(cid:32)

3a(L + 1)2∆

dβ

2i(cid:15)2o

ln

2∆kd(L + 1)
ρ

, 1

(cid:33)

where 0 < o ≤ aOPT for some a > 0. Then for a ﬁxed set Z ⊂ [∆]d, with probability at least 1 − ρ/((L + 1)Deltakd),
| (cid:80)
p, Z) − d(p, Z))| ≤ (cid:15)OPT
L+1 .

p, Z) − d(p, Z))/πi − (cid:80)

(d(ci

(d(ci

p∈Pi

p∈Si

Proof. The proof is identical to that of Lemma 3.4.

Lemma C.8. Consider a set of sets {Pi}L
For each i ∈ [0, L], let Si be an independent sample from Pi with sampling probability

i=0 which satisﬁes |Pi|∆

d/(2i) ≤ βρ

k(L+1) OPT for some β ≥ (cid:15)/(3(L + 1)).

where 0 < o ≤ aOPT for some a > 0, then with probability at least 1 − δ,

√

√

πi ≥ min

(cid:32)

4aβk(L + 1)3∆

d

2i(cid:15)2ρo

(cid:33)

log

, 1

2
δ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|Si ∩ Pi|
πi

− |Pi|

√
∆
d
2i ≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:15)ρOPT
k(L + 1)2 .

Proof of Lemma C.8. The proof is simply by Bernstein inequality. Let t = 2i(cid:15)ρOPT
that V ar(Xp) ≤ 1/πi and b := maxp |Xp| ≤ 1/πi. By Bernstein’s inequality, for any j ∈ [k(cid:48)],

dk(L+1)2∆

√

, Xp := Ip∈Si/πi, then we have

P r

(cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)

|Pj ∩ Si|
πi

(cid:12)
(cid:12)
− |Pj|
(cid:12)
(cid:12)

(cid:21)

> t

≤ 2e−

2|C|/πi+2bt/3 ≤ δ.

t2

We now describe the new construction. This essentially has the same gaurantee as the simpler construction from the
previous section, however the beneﬁt here is that (as shown in the next subsection) it can be modiﬁed to output only
positive weights. In the following paragraph, the estimations (cid:99)|C| are given as a blackbox. In proposition C.9 we specify the
conditions these estimations must satisfy.
Non-Negatively Weighted Construction Fix an arbitrary heavy cell identiﬁcation scheme H. Let Pl be all the points
with ending level l(p) = l. For each heavy cell C, let (cid:99)|C| be an estimation of number of points of |C|, we also call
(cid:99)|C| the value of cell C. For each non-heavy cell C(cid:48), let (cid:99)|C(cid:48)| = 0. Let S be a set samples of P constructed as follows:
S = S−1 ∪ S0 ∪ S1, ∪ . . . ∪ SL, where Sl is a set of i.i.d samples from Pl with probability πl. Here πl for l ∈ [−1, L]
where λ3 > 0 and λ4 > 0 are universal
is redeﬁned as,πl = min
constants. Our coreset S is composed by all the sampled points in S and the cell centers of heavy cells, with each point p
assigned a weight 1/πl(p) and for each cell center c of a heavy cell C ∈ Gi, the weight is,

(cid:16) λ3d2∆L2
2l(cid:15)2o

+ λ4d2kL3∆
2i(cid:15)2ρo

(cid:16) 2L∆dk
ρ

log 30kL2

log

, 1

(cid:17)

(cid:17)

ρ2

wt(c) = (cid:99)|C| −

(cid:88)

(cid:99)|C(cid:48)| −

|Si ∩ C|
πi

.

C(cid:48):C(cid:48)∈Gi+1,C(cid:48)⊂C,
C(cid:48) is heavy

(11)

For each non-heavy cell C except for those in the bottom level, wt(c(C)) = 0. The weight of each point from S is the value
of the corresponding cell in the bottom level.
We now state the following proposition for a coreset construction, which immediately serves as an ofﬂine coreset construc-
tion.
Proposition C.9. Let H be an arbitrary heavy cell identiﬁcation scheme. Fix Ω(∆−d) ≤ ρ < 1 and for each heavy C ∈ Gi
in level i, (cid:99)|C| is an estimation of number of points in C with additive error at most
kL∆ , where λ5 > 0 is a
universal constant. Let Sl be the set of i.i.d. samples of Pl with probability πl(o). If 0 < o ≤ OPT, then with probability
at least 1 − 4ρ, for every k-set Z ⊂ [∆]d,

λ5Ld3/2 · 2iρdOPT

(cid:15)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

q∈S

wt(q)d(q, Z) −

≤ (cid:15)OPT.

(cid:12)
(cid:12)
(cid:12)
d(p, Z)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

p∈P

And the coreset size |S| is

Clustering High Dimensional Dynamic Data Streams

(cid:18)

O

(cid:20) d3L4k
(cid:15)2

d +

log

1
ρ

kL
ρ

(cid:19) OPT
o

(cid:21)

.

Proof of Proposition C.9. Fix a k-set Z ⊂ [∆]d. First notice that,

(cid:100)cost(Z) =

wt(q)d(q, Z)

(cid:88)

q∈S












L−1
(cid:88)

=

L−1
(cid:88)

=

(cid:88)

(cid:88)

i=−1

C∈Gi:C heavy

i=−1

C∈Gi:C heavy








(cid:99)|C| −

(cid:88)

C(cid:48):C(cid:48)∈Gi+1,C(cid:48)⊂C,
C(cid:48) is heavy

(cid:99)|C(cid:48)| −

|Si ∩ C|
πi

d(c(C), Z) +








(cid:88)

p∈Si

d(p, Z)
πi

(cid:99)|C|(d(c(C), Z) − d(c(CP ), Z)) +

(12)

(cid:88)

d(p, Z) − d(ci

p, Z)

p∈Si

πi



 ,

where we denote d(c(CP
cost of Z as

−1), Z) = 0 for convenience. Let cost(Z) = (cid:80)

p∈P d(p, Z). Note that we can also write the true



cost(Z) =

|C|(d(c(C), Z) − d(c(CP , Z))) +

d(p, Z) − d(ci

p, Z)

 .

L−1
(cid:88)





(cid:88)

i=−1

C∈Gi:C heavy








(cid:88)

p∈Pi

We have that,

where

and

For A2, let

(cid:100)cost(Z) − cost(Z) = A1 + A2,

L−1
(cid:88)





A1 =

(cid:88)

i=−1

C∈Gi:C heavy

((cid:99)|C| − |C|)(d(c(C), Z) − d(c(CP , Z)))

L−1
(cid:88)





A2 =

i=−1

p∈Si

(cid:88)

d(p, Z) − d(ci

p, Z)

πi

(cid:88)

−

p∈Pi

d(p, Z) − d(ci

p, Z)

 .







Let Z ∗ ⊂ [∆]d be a set of optimal k-centers for the k-median problem of the input point set. By Lemma 2.2, with
probability at most 1 − ρ, for each i ∈ [0, L], if at most ek(L + 1)/ρ cells C in Gi satisfy d(C, Z ∗) ≤ ∆/(2i+1d).
Conditioning on this event, we have that, by Lemma C.4 there are at most k(cid:48) = O
heavy cells per level. Since for

(cid:17)

(cid:16) kL
ρ

each C ∈ Gi,

(cid:12)
(cid:12)
(cid:12)(cid:99)|C| − |C|

(cid:12)
(cid:12)
(cid:12) ≤

(cid:15)

λ5Ld3/2 · 2iρdOPT

kL∆ , by choosing appropriate constant λ5 > 0 we have

L−1
(cid:88)

(cid:88)

|A1| ≤

i=−1

C∈Gi:C heavy

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
((cid:99)|C| − |C|)(d(c(C), Z) − d(c(CP , Z)))
(cid:12)
(cid:12)
(cid:12)

≤ L · k(cid:48) ·

(cid:15)
λ5Ld3/2

·

2iρdOPT
kL∆

·

√
∆
d
2i ≤

(cid:15)OPT
2

.





A2i =

(cid:88)

d(p, Z) − d(ci

p, Z)

p∈Si

πi

√

(cid:88)

−

p∈Pi



d(p, Z) − d(ci

p, Z)

 .

(13)

By Lemma C.3, for each i ∈ [−1, L − 1], |Pi|∆
constants, with probability at least 1 − ρ/(L + 1)∆dk, |A2i| ≤ (cid:15)OPT
for every level i, and every k-set Z ⊂ [∆]d, |A2i| ≤ (cid:15)OPT
1 − 3ρ, |A1 + A2| ≤ (cid:15)OPT for any k-set Z ⊂ [∆]d.
The coreset size is the number of heavy cells plus the number of sampled points. The number of heavy cells is O(kL2/ρ).

d/(2i) = λ2(d3/2OPT). Thus by Lemma C.7, and choosing appropriate
2(L+1) . By the union bound, with probability at least 1 − ρ,
2(L+1) . Thus |A2| ≤ (cid:15)OPT/2. In total, with probability at least

The expected number of sampled points per level is at most,

By a Chernoff bound, with probability at least 1 − ρ/∆dk, for every level i ∈ [0, L], the number of sampled points is,

Clustering High Dimensional Dynamic Data Streams

|Si| = O

(cid:18) d4L3k

(cid:15)2 +

d3L2k
(cid:15)2ρ

log

(cid:18) kL
ρ

(cid:19)(cid:19) OPT

|Si| ≤ O

(cid:18) d4L3k

(cid:15)2 +

d3L3k
ρ(cid:15)2

log

(cid:18) kL
ρ

(cid:19)(cid:19) OPT

|S| ≤ O

(cid:18)

(cid:20) d3L4k
(cid:15)2

d +

log

1
ρ

kL
ρ

(cid:19) OPT
o

(cid:21)

.

.

.

o

o

Thus the size of the coreset S is,

C.3 Ensuring Non-Negative Weights
In this section, we will provide a procedure to rectify all the weights for the coreset constructed in the last sub-section. The
idea is similar to the method used in (Indyk & Price, 2011). The procedure is shown in Algorithm 4.3.
Proposition C.10. Let S be a weighted set constructed using the Non-Negatively Weighted Construction, i.e. each heavy
cell C has value (cid:99)|C| and the set of sampled points S = S−1 ∪ S0 . . . ∪ SL with each point in Sl has weight 1/πl. If for each
heavy cell C ∈ Gi, |(cid:99)|C| − |C|| ≤
kL∆ for some universal constant λ6 > 0 and for each i ∈ [−1, L] and any
k-set Z ⊂ [∆]d,

λ6Ld3/2 · 2iρdOPT

(cid:15)

and

wt(p)(d(ci

p, Z) − d(p, Z)) −

(d(ci

p, Z) − d(p, Z))

(cid:88)

p∈Pi

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

p∈S

≤

(cid:15)OPT
2L

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

|Si ∩ C|
πi

(cid:12)
(cid:12)
− |Pi ∩ C|
(cid:12)
(cid:12)

√
d
∆
2i ≤

(cid:15)OPT
L

.

(cid:88)

C∈Gi: heavy

Then on input (cid:100)|C1|, (cid:100)|C2|, . . . , (cid:100)|Ck(cid:48)| and S, where k(cid:48) is the number of heavy cells, the coreset output by Algorithm 4.3 is a
(4(cid:15))-coreset.

C.4 The Streaming Algorithm
C.4.1 SAMPLING FROM SPARSE CELLS
For the streaming algorithm, we can still use HEAVY-HITTER algorithms to ﬁnd the heavy cells. The major challenge
is to do the sampling for each point from its ending level. We do this using a combination of hash functions and K-Set.
In Algorithm C.4.1, we provide a procedure that recovers the set of points from cells with a small number of points and
ignore all the heavy cells. The guarantee is,
Theorem C.11. Given as input a set of dynamically updating streaming points P ⊂ [N ], a set of mutually disjoint cells
C ⊂ [M ], whose union covers the region of P . Algorithm C.4.1 outputs all the points in cells with less than β points or
output Fail. If with the promise that at most α cells from C contain a point of P , then the algorithm outputs Fail with
probability at most δ. The algorithm uses O(αβ(log(M β) + log N ) log N log(log N αβ/δ) log(αβ/δ)) bits in the worst
case.

The high level idea of this algorithm is to hash the original set of points to a universe of smaller size. For cells with less
points, the collision rate is much smaller than cells with more points. To recover one bit of a point, we update that bit and
the cell ID and also its hash tag to the K-Set-data structure. If there are no other points with hash values colliding with
this point, the count of that point is simply 1. If this is the case, we immediately recover the bit. By repeating the above
procedure once for each bit, we can successfully recover the set of points with no colliding hash tags. For those points with
colliding hash tags, we simply ignore them. Each point has a constant probability to collide with another point, thus not be
in the output. By running the whole procedure O(log(αβ/(cid:15))) times in parallel, we reduce the probability to roughly (cid:15) for
each point in cells with less than β points. To formally prove Theorem C.11, we ﬁrst prove the following lemma, which is
the guarantee of Algorithm C.4.1.
Lemma C.12. Given input a set of dynamically updating streaming points P ⊂ [N ], a set of mutually disjoint cells
C ⊂ [M ], whose union covers the region of P . Algorithm C.4.1 outputs a set of points in cells with less than β points

Clustering High Dimensional Dynamic Data Streams

or output Fail. If with the promise that at most α cells from C contain a point of P , then the algorithm outputs Fail
with probability at most δ. Conditioning on the event that the algorithm does not output Fail, each point p from cell
with less than β points is in the output with marginal probability at least 0.9. The algorithm uses O(αβ(log(M β) +
log N ) log N log(log N αβ/δ)) bits in the worst case.

Proof. We prove this lemma by showing that (a) if a point p ∈ P contained in cell C, with |C ∩ P | ≤ β, then with
probability at least 0.99, there are no other points p(cid:48) ∈ C ∩ P with H(p) = H(p(cid:48)), (b) conditioning on the event that the
algorithm does not output Fail, then for any cell C ∈ C, if a point p ∈ C such that no other points in C ∩ P has the same
hash value H(p), then p is in the output and (c) the algorithm outputs Fail with probability at most δ. The correctness of
the algorithm follows by (a), (b) and (c).
To show (a), consider any cell C ∈ C with |C| ≤ β, let p ∈ C with hash value H(p). Since H is 2-wise independent, the
expected number of other points hashed to the same hash value H(p) is at most β/U = 1/100. By Markov’s inequality,
with probability at least 0.99, no other point in C is hashed to H(p).
To show (b), notice that if the algorithm does not output Fail, then for a given cell C, let c be its ID, and pj be the j-th
bit of point p. Then (c, h, pj) has 1 count and (c, h, 1 − pj) has 0 count for each j ∈ [t], where t = (cid:100)log N (cid:101). Thus we can
uniquely recover each bit of point p, hence the point p.
For (c), since there are at most α cells, there are at most 2αU = O(αβ) many different updates for each KS structure.
Therefore, with probability at most δ
t , a single KS instance outputs Fail. By the union bound, with probability at least
1 − δ, no KS instance outputs Fail.
Finally, the space usage is dominated by the KS data structures. Since the input data to KS is from universe [M ] ×
[U ] × {0, 1}, each KS structure uses space O(αβ(log(M β) + log N ) log(tαβ/δ)) bits of memory, the total space is
O(αβ(log(M β) + log N ) log N log(tαβ/δ)).

Proof of Theorem C.11. Each instance of SparseCellsSingle fails with probability at most δ/(4A), where A is the
number independent SparseCellsSingle instances. By the union bound, with probability at least 1 − δ/4, none of
them output Fail. Conditioning on this event, the random bits of the hash functions of each SparseCellsSingle in-
stance are independent, thus by Lemma C.12 a ﬁxed point p ∈ C with |C| ≤ β is in the output with probability at least
10− log 4αβ
δ ≤ δ/(4αβ). Since there are at most αβ points in cells with less than β points, by the union bound we conclude
that with probability at least 1 − δ/4, every point in cells with less than β points is in the output set S. In sum, with
probability at least 1 − δ/2, S contains all the desired points.
The other KS instance outputs Fail with probability at most δ/2. Thus if T is not Fail, then T contains the exact number
of points of each cell. If any desired point is not in S, then |C| > |C ∩ S|, we output Fail. This happens with probability
at most δ under the gaurantee of the KSstructure.
Since each SparseCellsSingle instance uses O(αβ(log(M β) + log N ) log N log(tAαβ/δ)) bits of space, the ﬁnal
space of the algorithm is O(αβ(log(M β) + log N ) log N log(tαβ/δ) log(αβ/δ)).

Algorithm 4 SparseCells(N, M, α, β, δ): input the point sets P ⊂ [N ] and set of cells C ⊂ [M ] such that at most α
cells containing a point, output the set of points in cells with less than β points.
Let A ← log 4αβ
δ ;
Let R1, R2, . . . , RA be the results of independent instances of SparseCellsSingle(N, M, α, β, δ/(4A)) running in
parallel;
Let T be the results of another parallel KS structure with space parameter α and error δ/2 and with input as the cell IDs of
points in P ;
if any of the data structures returns Fail:

/*T returns the exact counts of each cell*/

return Fail;

Let S ← R1 ∪ R2 ∪ . . . RA;
if ∃ set C ∈ T with |C| ≤ β and |C| (cid:54)= |C ∩ S|:

return Fail;

return S;

C.4.2 THE ALGORITHM
With the construction of algorithm SparseCells, we now have all the tools for the streaming coreset construction. The
streaming algorithm is composed by O(L) levels of HEAVY-HITTER instances, which serve as a heavy cell identiﬁer and

Clustering High Dimensional Dynamic Data Streams

Algorithm 5 SparseCellsSingle(N, M, α, β, δ): input the point sets P ⊂ [N ] and set of cells C ⊂ [M ] such that at
most α cells containing a point, output the set of points in cells with less than β points.

Initization:
U ← 100β .
t ← (cid:100)log N (cid:101);
H : [N ] → [U ], 2-wise independent;
K-Set structures KS1, KS2, . . . KSt with space parameter 2αU and probability δ
t ;
Update(p, op):
c ← cell ID of p;
for i ∈ [t]:

/*op ∈ {Insert, Delete}*/

/*A point p is represented as (p1, p2, . . . , pt)*/;

pi ← the i-th bit of point p;
KSi.update((c, H(p), pi), op);

Query:
if for any i ∈ [t], KSi returns Fail:

return Fail;

S ← ∅;
for each (c, h, p1) in the output of KS1:
if (c, h, pj) /∈ KSj for some j ∈ [t]:

/*A checking step, may not happen at all*/;

return Fail;

Let s(c, h, pj) be the counts of (c, h, pj) in KSj;
if s(c, h, pj) = 1 and s(c, h, 1 − pj) = 0 for each j ∈ [t]:

p ← (p1, p2, . . . , pt);
S ← S ∪ {p};

return S

by O(L) levels of SparseCells instances, which sample the points from their ending levels. The full algorithm is stated
in Algorithm 6. The guarantee of the algorithm is stated in the following theorem.

Theorem C.13. Fix (cid:15), ρ ∈ (0, 1/2), positive integers k and ∆, Algorithm 6 makes a single pass over the streaming
point set P ⊂ [∆]d, outputs a weighted set S with non-negative weights for each point, such that with probability at
least 0.99, S is an (cid:15)-coreset for k-median of size O
, where L = log ∆. The algorithm uses

d + 1

(cid:17)(cid:105)

(cid:16)

ρ log kL

ρ

(cid:104) d7L7k
(cid:15)2

(cid:16)

ρdL + 1

bits in the worst case. For each update of the input, the algo-
O
rithm needs poly (d, 1/(cid:15), L, log k) time to process and outputs the coreset in time poly(d, k, L, 1/(cid:15), 1/ρ, log k) after one
pass of the stream.

ρ(cid:15) + L)

ρ(cid:15) (log log dkL

ρ log2 dkL

(cid:104) d3L4k
(cid:15)2
(cid:105)
log2 dkL
ρ(cid:15)

(cid:17)

Proof. W.l.o.g. assume ρ ≥ ∆−d, since otherwise we can store the entire set of points. In the sequel, we will prove the
theorem with parameter O(ρ) and O((cid:15)). It translates to ρ and (cid:15) directly by scaling and with losing at most a constant factor
in space and time bounds. By Lemma 2.2, with probability at least 1 − ρ, for every level i ∈ [0, L], at most ekL/ρ cells C
in Gi satisfy d(C, Z ∗) ≤ ∆/(2i+1d). We condition on this event for the following proof.
We ﬁrst show that the HEAVY-HITTER instances faithfully implement a heavy cell identiﬁcation scheme. First note that
with probability at least 1 − ρ, all HEAVY-HITTER instances succeed. Conditioning on this event for the following proof.
(cid:113) ρ
λ7kd3L3 and k(cid:48) = λ8kL/ρ, for appropriate positive universal
As shown in the proof of Lemma B.1, by setting (cid:15)(cid:48) = (cid:15)
kL∆ for some universal constant λ9, which
constants λ7, λ8, then the additive error to each cell is at most
matches the requirement of Proposition C.10. For each cell C with at least 2iρdOPT/(k(L + 1)∆) points, by Lemma C.4
it must be in the top (e + 2)k(L + 1)/ρ cells. For each cell C(cid:48) with at least 2i−1ρdOPT/(k(L + 1)∆) points, it must be in
2idOPT
the top (e + 4)k(L + 1)/ρ cells. Since the additive error is
k(L+1)∆ . Thus C is in the output
of the HEAVY-HITTER instances, since otherwise (cid:99)|C| ≤ 1
k(L+1)∆ contradicts the error bound
2

λ7d3/2(L+1) · 2idOPT
2idOPT
k(L+1)∆ +

k(L+1)∆ (cid:28) 1
λ7d3/2(L+1) · 2idOPT

λ9d3/2L · 2idOPT

2

(cid:15)

(cid:15)

(cid:15)

Clustering High Dimensional Dynamic Data Streams

(by choosing sufﬁciently large λ7). Thus the algorithm faithfully implements a heavy cell identiﬁcation scheme.
Now we show that if there exists an o ≤ OPT such that no instance of SparseCells outputs Fail, then the result is a
desired O((cid:15))-coreset. This follows by Proposition C.9 and Proposition C.10. Then we note that the coreset size is upper
bounded by O
Next we show that there exists an OPT/2 ≤ o∗ ≤ OPT that with probability at least 1 − ρ, no SparseCells in-
stance SCo∗,i outputs Fail. By Chernoff bound, with probability at least 1 − O(ρ), as also shown in the proof
ρ log kL
of Proposition C.9, per level at most O
cells is occupied by a point. And at most

(cid:104) d3L4k
(cid:15)2

(cid:104) d3L4k
(cid:15)2

(cid:17) OPT
o

ρ log kL

as desired.

d + 1

d + 1

(cid:17)(cid:105)

(cid:16)

(cid:16)

(cid:105)

ρ

ρ

(cid:16)

(cid:104) d3L2
(cid:15)2

ρd + log kL

ρ + ρ

kL log L

ρ

O
fails with probability at most O(ρ/(dL)), with probability at least 1 − O(ρ), no nstance SCo∗,i fails.
Lastly, we bound the space usage and update/query time. For the HEAVY-HITTER instances, the total space used
bits, analogous to the proof of Theorem 3.6. Each SparseCells instance uses space
is O

points is sampled per light cell. Conditioned on this fact and that each instances

(cid:16)

O
bits.
As a same argument in the proof of Theorem 3.6, the cost of de-randomization introduce an additional dL factor. Thus,
the ﬁnal space bound is O
bits. The query time and update time is similar to that of

ρ(cid:15) . The total space bound is O

, where r = log dkL

(cid:17)(cid:105)

(cid:16)

ρ

ρ

ρdL + r2(log r+L)

(cid:104) d6L6r2k
(cid:15)2

(cid:16)

(cid:17)(cid:105)

dL + log 1
ρ
(cid:16)

(cid:17) d4L5k
ρ(cid:15)2
ρdL + r2(log r+L)

(cid:104) d5L4r2k
(cid:15)2

(cid:17)(cid:105)

(cid:17)(cid:105)

Theorem 3.6 thus poly

(cid:104) d7L7r2k
(cid:15)2
(cid:15) , 1
ρ , k

(cid:17)

(cid:16)

d, L, 1

ρdL + r2(log r+L)
and poly (cid:0)d, L, 1

ρ

(cid:15) , log k(cid:1).

D Synthetic Dataset

Figure 2. 65536 points are drawn from a Gaussian Mixture distribution. The contours illustrate the PDF function.

Clustering High Dimensional Dynamic Data Streams

√

Algorithm 6 PositiveCoreSet(S, k, ρ, (cid:15)): construct a (cid:15)-coreset for dynamic stream S.
Initization:
Initialize a grid structure;
O ← {1, 2, 4, . . . ,
(cid:104) d3L4k
(cid:15)2

(cid:104) d3L2
(cid:15)2
For each o ∈ O and i ∈ [0, L], construct fully independent hash function ho,i
P rho,i(ho,i[q] = 1) = πi(o); initialize SparseCells(∆d, (1 + 2i)d, α, β, O(ρ/(dL))) instances SCo,i;
Initialize HEAVY-HITTER(∆d, 10Lk/ρ, (cid:15)(cid:48), ρ/L) instances, HH0, HH1, . . . , HHL−1, one for a level;

(cid:16) 2L∆dk
+ λ4d2kL3∆
2i(cid:15)2ρo
ρ
(cid:113) ρ
λ7kd3L3 ; m ← 0;
:

d∆d+1}; L ← (cid:100)log ∆(cid:101); πi(o) ← min

(cid:16) λ3d2∆L2
2l(cid:15)2o
(cid:17)(cid:105)

ρd + log kL

kL log L

ρ log kL

ρ + ρ

, (cid:15)(cid:48) ← (cid:15)

, β ← O

α ← O

d + 1

log

(cid:17)(cid:105)

(cid:16)

(cid:16)

(cid:17)

ρ

ρ

log 30kL2

, 1

ρ2

(cid:17)

;

[∆]d → {0, 1} with

Update (S):
for each update (op, q) ∈ S:

/*op ∈ {Insert, Delete}*/

/*Insert: +1, Delete:−1*/

m ← m ± 1;
for each i ∈ [0, L]:

ci
q ← the center of the cell contains q at level i;
HHi.update(op, ci
for each o ∈ [O]:

q);

if ho,i(q) == 1:

SCo,i.update(op, ci

q);

Query:
Let o∗ be the smallest o such that no instance of SCo,0, SCo,1, . . . , SCo,L returns Fail;
S ← {};
C−1 ← the cell of the entire space [∆]d;
for i ∈ [0, L − 1]:

(cid:91)|C−1| ← m;

Ci ← HHi.query().top((e + 4)(L + 1)k/ρ);
Remove cells C from Ci if CP (C) (cid:54)∈ Ci−1, where CP (C) is the parent cell of C in level i − 1;
Bi ← SCo∗,i.query();
Si ← {p ∈ Bi : C(p, i − 1) ∈ Ci−1 AND C(p, i) (cid:54)∈ Ci};
Each point in Si receives weight 1/πi(o∗);
S ← S ∪ Si;
i∈[0,L] |Ci|;

k(cid:48) ← (cid:80)
Let {C1, C2, . . . Ck(cid:48)} = ∪i∈[0,L]Ci ∪ {C−1} be the set of heavy cells;
Let

be the estimated frequency of each cell;

(cid:100)|C1|, (cid:100)|C2|, . . . (cid:100)|Ck(cid:48)|

(cid:111)

(cid:110)

R ← RectifyWeights((cid:100)|C1|, (cid:100)|C2|, . . . (cid:100)|Ck(cid:48)|, S);
return R.

