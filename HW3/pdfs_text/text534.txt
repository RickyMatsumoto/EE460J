Online Learning with Local Permutations and Delayed Feedback

Ohad Shamir * 1 Liran Szlak * 1

Abstract
We propose an Online Learning with Local Per-
mutations (OLLP) setting, in which the learner
is allowed to slightly permute the order of the
loss functions generated by an adversary. On one
hand, this models natural situations where the ex-
act order of the learner’s responses is not crucial,
and on the other hand, might allow better learn-
ing and regret performance, by mitigating highly
adversarial loss sequences. Also, with random
permutations, this can be seen as a setting in-
terpolating between adversarial and stochastic
losses. In this paper, we consider the applicabil-
ity of this setting to convex online learning with
delayed feedback, in which the feedback on the
prediction made in round t arrives with some de-
lay τ . With such delayed feedback, the best pos-
sible regret bound is well-known to be O(
τ T ).
We prove that by being able to permute losses by
a distance of at most M (for M ≥ τ ), the regret
√
T (1 + (cid:112)τ 2/M )), us-
can be improved to O(
ing a Mirror-Descent based algorithm which can
be applied for both Euclidean and non-Euclidean
geometries. We also prove a lower bound, show-
ing that for M < τ /3, it is impossible to improve
the standard O(
τ T ) regret bound by more than
constant factors. Finally, we provide some ex-
periments validating the performance of our al-
gorithm.

√

√

1. Introduction

Online learning is traditionally posed as a repeated game
where the learner has to provide predictions on an arbi-
trary sequence of loss functions, possibly even generated
adversarially. Although it is often possible to devise algo-
rithms with non-trivial regret guarantees, these have to cope
with arbitrary loss sequences, which makes them conserva-

*Equal contribution
Israel.

hovot,
ran.szlak@weizmann.ac.il>.

1Weizmann Institute of Science, Re-
Liran Szlak <li-

Correspondence to:

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

tive and in some cases inferior to algorithms not tailored to
cope with worst-case behavior. Indeed, an emerging line of
work considers how better online learning can be obtained
on “easy” data, which satisﬁes some additional assump-
tions. Some examples include losses which are sampled
i.i.d. from some distribution, change slowly in time, have
a consistently best-performing predictor across time, have
some predictable structure, mix adversarial and stochastic
(Sani et al., 2014; Karnin and Anava,
losses, etc.
2016; Bubeck and Slivkins, 2012; Seldin and Slivkins,
2014; Hazan and Kale, 2010; Chiang et al., 2012; Stein-
hardt and Liang, 2014; Hazan and Kale, 2011; Rakhlin and
Sridharan, 2013; Seldin and Slivkins, 2014)).

(e.g.

In this paper, we take a related but different direction:
Rather than explicitly excluding highly adversarial loss se-
quences, we consider how slightly perturbing them can mit-
igate their worst-case behavior, and lead to improved per-
formance. Conceptually, this resembles smoothed analy-
sis (Spielman and Teng, 2004), in which one considers the
worst-case performance of some algorithm, after perform-
ing some perturbation to their input. The idea is that if the
worst-case instances are isolated and brittle, then a pertur-
bation will lead to easier instances, and better reﬂect the
attainable performance in practice.

Speciﬁcally, we propose a setting, in which the learner
is allowed to slightly reorder the sequence of losses gen-
erated by an adversary: Assuming the adversary chooses
losses h1, . . . , hT , and before any losses are revealed, the
learner may choose a permutation σ on {1, . . . , T }, sat-
isfying maxt |t − σ(t)| ≤ M for some parameter M ,
and then play a standard online learning game on losses
hσ(1), . . . , hσ(T ). We denote this as the Online Learning
with Local Permutations (OLLP) setting. Here, M controls
the amount of power given to the learner: M = 0 means
that no reordering is performed, and the setting is equiv-
alent to standard adversarial online learning. At the other
extreme, M = T means that the learner can reorder the
losses arbitrarily. For example, the learner may choose to
order the losses uniformly at random, making it a quasi-
stochastic setting (the only difference compared to i.i.d.
losses is that they are sampled without-replacement rather
than with-replacement).

We argue that allowing the learner some ﬂexibility in the

Online Learning with Local Permutations and Delayed Feedback

order of responses is a natural assumption. For example,
when the learner needs to provide rapid predictions on a
high-frequency stream of examples, it is often immaterial
if the predictions are not provided in the exact same order at
which the examples arrived. Indeed, by buffering examples
for a few rounds before being answered, one can simulate
the local permutations discussed earlier.

We believe that this setting can be useful in various on-
line learning problems, where it is natural to change a bit
the order of the loss functions. In this paper, we focus on
one well-known problem, namely online learning with de-
layed feedback.
In this case, rather than being provided
with the loss function immediately after prediction is made,
the learner only receives the loss function after a certain
number τ ≥ 1 of rounds. This naturally models situa-
tions where the feedback comes much more slowly than
the required frequency of predictions: To give a concrete
example, consider a web advertisement problem, where an
algorithm picks an ad to display, and then receives a feed-
back from the user in the form of a click. It is likely that
the algorithm will be required to choose ads for new users
while still waiting for the feedback from the previous user.
Web advertisement, answering user queries and many other
regimes where the feedback come from a user, are all rele-
vant to this setting. It is also plausible that the distribution
of examples in the above scenarios is not stochastic neither
is it adversarial, and thus the proposed setting of Online
Learning With Local Permutation is relevant to these cases.

√

For convex online learning with delayed feedback, in a
standard adversarial setting, it is known that the attain-
τ T ), and this is also the
able regret is on the order of O(
best possible in the worst case (Weinberger and Ordentlich,
2002; Mesterharm, 2005; Langford et al., 2009; Joulani
et al., 2013; Quanrud and Khashabi, 2015). On the other
hand, in a stochastic setting where the losses are sampled
i.i.d. from some distribution, (Agarwal and Duchi, 2011)
show that the attainable regret is much better, on the order
of O(
T +τ ). This gap between the worst-case adversarial
setting, and the milder i.i.d. setting, hints that this problem
is a good ﬁt for our OLLP framework.

√

Thus, in this paper, we focus on online learning with feed-
back delayed up to τ rounds, in the OLLP framework
where the learner is allowed to locally permute the loss
functions (up to a distance of M ). First, we devise an algo-
rithm, denoted as Delayed Permuted Mirror Descent, and
prove that it achieves an expected regret bound of order
O((cid:112)T (τ 2/M + 1)) assuming M ≥ τ . As M increases
compared to τ , this regret bound interpolates between the
T regret,
standard adversarial
typical of i.i.d. losses. As its name implies, the algorithm
is based on the well-known online mirror descent (OMD)
algorithm (see (Hazan et al., 2016; Shalev-Shwartz et al.,

τ T regret, and a milder

√

√

2012)), and works in the same generality, involving both
Euclidean and non-Euclidean geometries. The algorithm
is based on dividing the entire sequence of functions into
blocks of size M and performing a random permutation
within each block. Then, two copies of OMD are ran
on different parts of each block, with appropriate param-
eter settings. A careful analysis, mixing adversarial and
stochastic elements, leads to the regret bound.

In addition, we provide a lower bound complementing our
upper bound analysis, showing that when M is signiﬁ-
cantly smaller than τ (speciﬁcally, τ /3), then even with
local permutations, it is impossible to obtain a worse-case
τ T ), matching (up to constants) the
regret better than Ω(
attainable regret in the standard adversarial setting where
no permutations are allowed. Finally, we provide some ex-
periments validating the performance of our algorithm.

√

The rest of the paper is organized as follows: in section 2
we formally deﬁne the Online Learning with Local Per-
mutation setting, section 3 describes the Delayed Permuted
Mirror Descent algorithm and outlines its regret analysis,
section 4 discusses a lower bound for the delayed setting
with limited permutation power, section 5 shows experi-
ments, and ﬁnally section 6 provides concluding remarks,
discussion, and open questions. Appendix A contains most
of the proofs.

2. Setting and Notation

Convex Online Learning. Convex online learning is posed
as a repeated game between a learner and an adversary
(assumed to be oblivious in this paper). First, the ad-
versary chooses T convex losses h1, . . . , hT which are
functions from a convex set W to R. At each iteration
t ∈ {1, 2, . . . , T }, the learner makes a prediction wt, and
suffers a loss of ht (wt). To simplify the presentation, we
use the same notation ∇ht(w) to denote either a gradient
of ht at w (if the loss is differentiable) or a subgradient at
w otherwise, and refer to it in both cases as a gradient. We
assume that both w ∈ W and the gradients of any func-
tion ht in any point w ∈ W are bounded w.r.t. some norm:
Given a norm (cid:107) · (cid:107) with a dual norm (cid:107) · (cid:107)∗, we assume that
the diameter of the space W is bounded by B2 and that
∀w ∈ W, ∀h ∈ {h1, h2, ..., hT } : (cid:107)∇h (w) (cid:107)∗ ≤ G. The
purpose of the learner is to minimize her (expected) regret,
i.e.

R(T ) = E

ht (wt) −

(cid:34) T

(cid:88)

t=1

(cid:35)
ht (w∗)

where w∗ = argmin
w∈W

ht (w)

T
(cid:88)

t=1

T
(cid:88)

t=1

where the expectation is with respect to the possible ran-
domness of the algorithm.

Online Learning with Local Permutations and Delayed Feedback

Learning with Local Permutations. In this paper, we in-
troduce and study a variant of this standard setting, which
gives the learner a bit more power, by allowing her to
slightly modify the order in which the losses are pro-
cessed,
thus potentially avoiding highly adversarial but
brittle loss constructions. We denote this setting as the
Online Learning with Local Permutations (OLLP) setting.
Formally, letting M be a permutation window parame-
ter, the learner is allowed (at the beginning of the game,
and before any losses are revealed) to permute h1, . . . , hT
to hσ−1(1), . . . , hσ−1(T ), where σ is a permutation from
the set P erm := {σ : ∀t, |σ (t) − t| ≤ M }. After this
permutation is performed, the learner is presented with
the permuted sequence as in the standard online learn-
ing setting, with the same regret as before. To sim-
plify notation, we let ft = hσ−1(t), so the learner is
presented with the loss sequence f1, . . . , fT , and the re-
gret is the same as the standard regret, i.e. R(T ) =
E
. Note that if M = 0
then we are in the fully adversarial setting (no permuta-
tion is allowed). At the other extreme, if M = T and σ
is chosen uniformly at random, then we are in a stochastic
setting, with a uniform distribution over the set of functions
chosen by the adversary (note that this is close but differs a
bit from a setting of i.i.d. losses). In between, as M varies,
we get an interpolation between these two settings.

t=1 ft(wt) − (cid:80)T

(cid:105)
t=1 ft(w∗)

(cid:104)(cid:80)T

Learning with Delayed Feedback. The OLLP setting can
be useful in many applications, and can potentially lead to
improved regret bounds for various tasks, compared to the
standard adversarial online learning. In this paper, we focus
on studying its applicability to the task of learning from
delayed feedback.

Whereas in standard online learning, the learner gets to ob-
serve the loss ft immediately at the end of iteration t, here
we assume that at round t, she only gets to observe ft−τ for
some delay parameter τ < T (and if t < τ , no feedback
is received). For simplicity, we focus on the case where
τ is ﬁxed, independent of t, although our results can be
easily generalized (as discussed in subsection 3.3). We em-
phasize that this is distinct from another delayed feedback
scenario sometimes studied in the literature (Agarwal and
Duchi, 2011; Langford et al., 2009), where rather than re-
ceiving ft−τ the learner only receives a (sub)gradient of
ft−τ at wt−τ . This is a more difﬁcult setting, which is rel-
evant for instance when the delay is due to the time it takes
to compute the gradient.

3. Algorithm and Analysis

Our algorithmic approach builds on the well-established
online mirror descent framework. Thus, we begin with a
short reminder of the Online Mirror Descent algorithm. For

a more extensive explanation refer to (Hazan et al., 2016).
Readers who are familiar with the algorithm are invited to
skip to Subsection 3.1.

The online mirror descent algorithm is a generalization of
online gradient descent, which can handle non-Euclidean
geometries. The general idea is the following: we start
with some point wt ∈ W, where W is our primal space.
We then map this point to the dual space using a (strictly
convex and continuously differentiable) mirror map ψ, i.e.
∇ψ (wt) ∈ W ∗, then perform the gradient update in the
dual space, and ﬁnally map the resulting new point back
to our primal space W again, i.e. we want to ﬁnd a point
wt+1 ∈ W s.t. ∇ψ (wt+1) = ∇ψ (wt) − η · gt where gt
denotes the gradient. Denoting by wt+ 1
the point satisfy-
ing ∇ψ(wt+ 1
) = ∇ψ (wt) − η · gt, it can be shown that
= (∇ψ∗) (∇ψ (wt) − η · gt), where ψ∗ is the dual
wt+ 1
function of ψ. This point, wt+ 1
, might lie outside our hy-
pothesis class W, and thus we might need to project it back
to our space W. We use the Bregman divergence associated
to ψ to do this:

2

2

2

2

wt+1 = argmin

(cid:52)ψ(w, wt+ 1

),

2

w∈W

where the Bregman divergence ∆ψ is deﬁned as

(cid:52)ψ (x, y) = ψ (x) − ψ (y) − (cid:104)∇ψ(y), x − y(cid:105).

Speciﬁc choices of the mirror map ψ leads to speciﬁc in-
stantiations of the algorithms for various geometries. Per-
haps the simplest example is ψ (x) = 1
2 (cid:107)x(cid:107)2
2, with associ-
ated Bregman divergence (cid:52)ψ (x, y) = 1
2 · (cid:107)x − y(cid:107)2. This
leads us to the standard and well-known online gradient de-
scent algorithm, where wt+1 is the Euclidean projection on
the set W of

wt − η · gt.

Another example is the negative entropy mirror map
ψ (x) = (cid:80)n
i=1 xi · log (xi), which is 1-strongly con-
vex with respect to the 1-norm on the simplex W =
(cid:8)x ∈ Rn
i=1 xi = 1(cid:9). In that case, the resulting algo-
rithm is the well-known multiplicative updates algorithm,
where

+ : (cid:80)n

wt+1,i = wt,i · exp(−ηgt,i)/

wt,i · exp(−ηgt,j).

n
(cid:88)

j=1

Instead of the 1-norm on the simplex, one can also consider
arbitrary p-norms, and take ψ(x) = 1
q, where q is the
dual norm (satisfying 1/p + 1/q = 1).

2 · (cid:107)x(cid:107)2

3.1. The Delayed Permuted Mirror Descent Algorithm

Before describing the algorithm, we note that we will focus
here on the case where the permutation window parameter

Online Learning with Local Permutations and Delayed Feedback

M is larger than the delay parameter τ . If M < τ , then our
τ T ) ob-
regret bound is generally no better than the O(
tainable by a standard algorithm without any permutations,
and this is actually tight as shown in Section 4.

√

We now turn to present our algorithm, denoted as The De-
layed Permuted Mirror Descent algorithm (see algorithm 1
below as well as ﬁgure 1 for a graphical illustration). First,
the algorithm splits the time horizon T into M consecu-
tive blocks, and performs a uniformly random permutation
on the loss functions within each block. Then, it runs two
online mirror descent algorithms in parallel, and uses the
delayed gradients in order to update two separate predic-
tors – wf and ws, where wf is used for prediction in the
ﬁrst τ rounds of each block, and ws is used for prediction
in the remaining M − τ rounds (here, f stands for “ﬁrst”
and s stands for “second”). The algorithm maintaining ws
crucially relies on the fact that the gradient of any two func-
tions in a block (at some point w) is equal, in expectation
over the random permutation within each block. This al-
lows us to avoid most of the cost incurred by delays within
each block, since the expected gradient of a delayed func-
tion and the current function are equal. A complicating fac-
tor is that at the ﬁrst τ rounds of each block, no losses from
the current block has been revealed so far. To tackle this,
we use another algorithm (maintaining wf ), speciﬁcally to
deal with the losses at the beginning of each block. This
algorithm does not beneﬁt from the random permutation,
and its regret scales the same as standard adversarial on-
line learning with delayed feedback. However, as the block
size M increases, the proportion of losses handled by wf
decreases, and hence its inﬂuence on the overall regret di-
minishes.

The above refers to how the blocks are divided for pur-
poses of prediction. For purposes of updating the predictor
of each algorithm, we need to use the blocks a bit differ-
ently. Speciﬁcally, we let T1 and T2 be two sets of indices.
T1 includes all indices from the ﬁrst τ time points of ev-
ery block, and is used to update wf . T2 includes the ﬁrst
M −τ indices of every block, and is used to update ws (see
ﬁgure 1). Perhaps surprisingly, note that T1 and T2 are not
disjoint, and their union does not cover all of {1, . . . , T }.
The reason is that due to the random permutation in each
block, the second algorithm only needs to update on some
of the loss functions in each block, in order to obtain an
expected regret bound on all the losses it predicts on.

3.2. Analysis

The regret analysis of the Delayed Permuted Mirror De-
scent algorithm is based on a separate analysis of each
of the two mirror descent sub-algorithms, where in the
ﬁrst sub-algorithm the delay parameter τ enters multiplica-
tively, but doesn’t play a signiﬁcant role in the regret of

Algorithm 1 Delayed Permuted Mirror Descent

1 = 0, jf = js = 1

Input: M , ηf , ηs
Init: wf
1 = 0, ws
Divide T to consecutive blocks of size M , and permute
the losses uniformly at random within each block. Let
f1, . . . , fT denote the resulting permuted losses.
for t = 1..., T do

if t ∈ ﬁrst τ rounds of the block then

Predict using wf
jf
Receive a loss function from τ places back:
ft−M = fT1(jf −τ ).
If none exists (in the ﬁrst τ
iterations), take the 0 function.
Compute: ∇fT1(jf −τ )
Update: wf

wf

jf −τ

=

(cid:16)

(cid:17)

jf + 1
2
(cid:16)

(cid:16)

(∇ψ∗)

∇ψ

(cid:17)

wf
jf

Project: wjf +1 = argmin

− ηf ∇fT1(jf −τ )
w, wf

(cid:52)ψ

(cid:16)

w∈W

jf + 1
2

(cid:17)(cid:17)

(cid:16)

wf

jf −τ
(cid:17)

jf = jf + 1

else

Predict using ws
js
Receive a loss function from τ places back: ft−τ =
fT2(js)
Compute: ∇ft−τ
Update: ws
=
js+ 1
2
(cid:1) − ηs · ∇fT2(js)
(∇ψ∗) (cid:0)∇ψ (cid:0)ws
js
Project: wjs+1 = argmin

(cid:0)ws
js
w, ws

(cid:1) = ∇fT2(js)

(cid:0)ws
js

(cid:0)ws
js

(cid:1)(cid:1)
(cid:17)

(cid:52)ψ

(cid:16)

(cid:1)

js+ 1
2

w∈W

js = js + 1

end if
end for

the second sub-algorithm (which utilizes the stochastic na-
ture of the permutations). Combining the regret bound of
the two sub-algorithms, and using the fact that the portion
of losses predicted by the second algorithm increases with
M , leads to an overall regret bound improving in M .

In the proof, to analyze the effect of delay, we need a
bound on the distance between any two consequent pre-
dictors wt, wt+1 generated by the sub-algorithm. This de-
pends on the mirror map and Bregman divergence used for
the update, and we currently do not have a bound holding
in full generality. Instead, we let Ψ(ηf ,G) be some upper
bound on (cid:107)wt+1 − wt(cid:107), where the update is using step-size
ηf and gradients of norm ≤ G. Using Ψ(ηf ,G) we prove a
general bound for all mirror maps. In Lemmas 3 and 4 in
Appendix A.1, we show that for two common mirror maps
(corresponding to online gradient descent and multiplica-
tive weights), Ψ(ηf ,G) ≤ c · ηf G for some numerical con-
stant c, leading to a regret bound of O((cid:112)T (τ 2/M + 1)).
Also, we prove theorem 1 for 1-strongly convex mirror

Online Learning with Local Permutations and Delayed Feedback

Figure 1. Scheme of predictions and updates of both parallel algorithms (best viewed in color; see text for details). Top color bars mark
which iterations are in T1 (purple lines) and which are in T2 (green lines). Top timeline shows which predictor, wf or ws, is used to
predict in each iteration. Middle timeline shows where gradients for updating wf come from (ﬁrst τ iterations of the previous block),
and lower timeline shows where gradients for updating ws come from (ﬁrst M − τ iterations of the same block, each gradient from
exactly τ rounds back).

maps, although it can be generalized to any λ-strongly con-
vex mirror map by scaling.

Theorem 1. Given a norm (cid:107) · (cid:107), suppose that we run the
Delayed Permuted Mirror Descent algorithm using a mir-
ror map ψ which is 1-strongly convex w.r.t. (cid:107) · (cid:107), over a
domain W with diameter B2 w.r.t the bregman divergence
of ψ: ∀w, v ∈ W : (cid:52)ψ(w, v) ≤ B2, and such that the
(sub)-gradient g of each loss function on any w ∈ W sat-
isﬁes (cid:107)g(cid:107)∗ ≤ G (where (cid:107) · (cid:107)∗ is the dual norm of (cid:107) · (cid:107)).
Then the expected regret, given a delay parameter τ and
step sizes ηf , ηs satisﬁes:

(cid:34) T

(cid:88)

(cid:35)
ft (wt) − ft (w∗)

E

≤

t=1

B2
ηf

+ ηf ·

T τ
M

·

G2
2

+

T τ 2
M

+ ηs ·

· G · Ψ(ηf ,G) +

T · (M − τ )
M

·

G2
2

B2
ηs

Furthermore, if Ψ(ηf ,G) ≤ c · ηf G for some constant c,
, the regret
, ηs =
and ηf =

B·
√

√

√

(cid:113)

2M
T ·(M −τ )

G·

M
B·
2 +c·τ)
T ·τ ·( 1

G·
is bounded by

· BG

+ c · τ +

(cid:114)
c

T τ
M
(cid:32)√

= O

T ·

(cid:114) 1
2
(cid:32)(cid:114)

(cid:114)

(cid:33)(cid:33)

τ 2
M

+ 1

2T (M − τ )
M

· BG

√

τ T ).

When M = O(τ ), this bound is O(
similar to
the standard adversarial learning case. However, as M in-
creases, the regret gradually improves to O(
T +τ ), which
is the regret attainable in a purely stochastic setting with
i.i.d. losses. The full proof can be found in appendix A.1.1,
and we sketch below the main ideas.

√

f , ∇ft(wf
s , ∇ft (ws

t − w∗
t − w∗
f and w∗

First, using the deﬁnition of regret, we show that it is
enough to upper-bound the regret of each of the two sub-
algorithms separately. Then, by a standard convexity ar-
gument, we reduce this to bounding sums of terms of the
form E[(cid:104)wf
t )(cid:105)] for the ﬁrst sub-algorithm,
and E [(cid:104)ws
t )(cid:105)] for the second sub-algorithm
(where w∗
s are the best ﬁxed points in hindsight
for the losses predicted on by the ﬁrst and second sub-
algorithms, respectively, and where for simplicity we as-
sume the losses are differentiable).
In contrast, we can
use the standard analysis of mirror descent, using de-
layed gradients, to get a bound for the somewhat differ-
f , ∇ft−τ (wf
ent terms E[(cid:104)wf
t − w∗
t−τ )(cid:105)] for the ﬁrst sub-
algorithm, and E [(cid:104)ws
t − w∗
t )(cid:105)] for the second
sub-algorithm. Thus, it remains to bridge between these
terms.

s , ∇ft−τ (ws

Starting with the second sub-algorithm, we note that since
we performed a random permutation within each block, the
expected value of all loss functions within a block (in ex-
pectation over the block, and evaluated at a ﬁxed point) is
equal. Moreover, at any time point, the predictor ws main-
tained by the second sub-algorithm does not depend on the

 𝑻𝟏 𝑻𝟐 1 T . . . 𝝉 𝝉 𝝉 Predictions Updates for 𝒘𝒇 𝒘𝒇 𝒘𝒇 𝒘𝒇 𝒘𝒔 𝒘𝒔 𝒘𝒔 Block 1 Block 2 Block 3 T 1 M M M 𝝉 𝝉 𝝉 𝝉 𝝉 𝝉 Updates for 𝒘𝒔 . . . . . . T 1 Online Learning with Local Permutations and Delayed Feedback

delayed nor the current loss function. Therefore, condi-
tioned on ws
t , and in expectation over the random permuta-
tion in the block, we have that

E[∇ft(ws

t )] = E[ft−τ (ws

t )]

from which it can be shown that

E [(cid:104)ws

t − w∗

s , ∇ft(ws

t )(cid:105)] = E [(cid:104)ws

t − w∗

s , ∇ft−τ (ws

t )(cid:105)]

Thus, up to a negligible factor having to do with the ﬁrst
few rounds of the game, the second sub-algorithm’s ex-
pected regret does not suffer from the delayed feedback.

For the ﬁrst sub-algorithm, we perform an analysis which
does not rely on the random permutation.
Speciﬁ-
cally, we ﬁrst show that since we care just about the
it is sufﬁcient to bound the differ-
sum of the losses,
ence between E[(cid:104)wf
t − w∗
t+τ −
f , ∇ft(wf
w∗
t )(cid:105)]. Using Cauchy-Shwartz, this difference
can be upper bounded by (cid:107)wf
t )(cid:107), which
in turn is at most c · τ · ηf · G2 using our assumptions on the
gradients of the losses and the distance between consecu-
tive predictors produced by the ﬁrst sub-algorithm.

t )(cid:105)] and E[(cid:104)wf

t+τ (cid:107)·(cid:107)∇ft(wf

f , ∇ft(wf

t −wf

Overall, we get two regret bounds, one for each sub-
algorithm. The regret of the ﬁrst sub-algorithm scales
with τ , similar to the no-permutation setting, but the sub-
algorithm handles only a small fraction of the iterations (the
ﬁrst τ in every block of size M ). In the rest of the iterations,
where we use the second sub-algorithm, we get a bound
that resembles more the stochastic case, without such de-
pendence on τ . Combining the two, the result stated in
Theorem 1 follows.

3.3. Handling Variable Delay Size

So far, we discussed a setting where the feedback arrives
with a ﬁxed delay of size τ . However, in many situations
the feedback might arrive with a variable delay size τt at
any iteration t, which may raise a few issues.
First, feedback might arrive in an asynchronous fashion,
causing us to update our predictor using gradients from
time points further in past after already using more recent
gradients. This complicates the analysis of the algorithm.
A second, algorithmic problem, is that we could also possi-
bly receive multiple feedbacks simultaneously, or no feed-
back at all, in certain iterations, since the delay is of vari-
able size. One simple solution is to use buffering and re-
duce the problem to a constant delay setting. Speciﬁcally,
we assume that all delays are bounded by some maximal
delay size τ . We would like to use one gradient to up-
date our predictor at every iteration (this is mainly for ease
of analysis, practically one could update the predictor with
multiple loss functions in a single iteration).
In order to
achieve this, we can use a buffer to store loss functions that

were received but have not been used to update the predic-
tors yet. We deﬁne Gradf and Grads, two buffers that
will contain gradients from time points in T1 or T2, corre-
spondingly. Each buffer is of size τ . If we denote by Ft
the set of function that have arrived in time t, we can sim-
ply store loss functions that have arrived asynchronously
in the buffers deﬁned above, sort them in ascending order,
and take the delayed loss function from exactly τ iterations
back in the update step. This loss function must be in the
appropriate buffer since the maximal delay size is τ . From
this moment on, the algorithm can proceed as usual and its
analysis still applies.

4. Lower Bound

In this section, we give a lower bound in the setting where
M < τ
3 with all feedback having delay of exactly τ . We
will show that for this case, the regret bound cannot be im-
proved by more than a constant factor over the bound of
the adversarial online learning problem with a ﬁxed delay
(cid:17)
of size τ , namely Ω
for a sequence of length T .
We hypothesize that this regret bound also cannot be sig-
niﬁcantly improved for any M = O(τ ) (and not just τ /3).
However, proving this remains an open problem.

(cid:16)√

τ T

Theorem 2. For every (possible randomized) algorithm A
with a permutation window of size M ≤ τ
3 , there exists a
choice of linear, 1-Lipschitz functions over [−1, 1] ⊂ R,
such that the expected regret of A after T rounds (with re-
spect to the algorithm’s randomness), is

(cid:34) T

(cid:88)

E

t=1

ft (wt) −

T
(cid:88)

t=1

(cid:35)
ft (w∗)

(cid:16)√

(cid:17)

τ T

= Ω

where w∗ = argmin
w∈W

T
(cid:88)

t=1

ft (w)

√

For completeness, we we also provide in appendix A.2 a
proof that when M = 0 (i.e. no permutations allowed),
then the worst-case regret is no better than Ω(
τ T ). This
is of course a special case of Theorem 2, but applies to
the standard adversarial online setting (without any local
permutations), and the proof is simpler. The proof sketch
for the setting where no permutation is allowed was already
provided in (Langford et al., 2009), and our contribution is
in providing a full formal proof.

√

The proof in the case where M = 0 is based on linear
losses of the form ft = αt · wt over [−1, +1], where
αt ∈ {−1, +1}. Without permutations, it is possible to
τ T ) lower bound by dividing the T iterations
prove a Ω(
into blocks of size τ , where the α values of all losses at
each block is the same and randomly chosen to equal either
+1 or −1. Since the learner does not obtain any informa-

Online Learning with Local Permutations and Delayed Feedback

tion about this value until the block is over, this reduces to
adversarial online learning over T /τ rounds, where the re-
gret at each round scales linearly with τ , and overall regret
at least Ω(τ (cid:112)T /τ ) = Ω(
τ T ).

√

In the proof of theorem 2, we show that by using a similar
construction, even with permutations, having a permutation
window less than τ /3 still means that the α values would
still be unknown until all loss functions of the block are
processed, leading to the same lower bound up to constants.

The formal proof appears in the appendix, but can be
sketched as follows: ﬁrst, we divide the T iterations into
blocks of size τ /3. Loss functions within each block are
identical, of the form ft = αt · wt, and the value of α per
block is chosen uniformly at random from {−1, +1}, as
before. Since here, the permutation window M is smaller
than τ /3, then even after permutation, the time difference
between the ﬁrst and last time we encounter an α that orig-
inated from a single block is less than τ . This means that
by the time we get any information on the α in a given
block, the algorithm already had to process all the losses
in the block, which leads to the same difﬁculty as the no-
permutation setting. Speciﬁcally, since the predictors cho-
sen by the algorithm when handling the losses of the block
do not depend on the α value in that block, and that α is
chosen randomly, we get that the expected loss of the al-
gorithm at any time point t equals 0. Thus, the cumulative
loss across the entire loss sequence is also 0. In contrast,
for w∗, the optimal predictor in hindsight over the entire
sequence, we can prove an expected accumulated loss of
τ T ) after T iterations, using Khintchine inequality
−Ω(
and the fact that the α’s were randomly chosen per block.
This leads us to a lower bound of expected regret of order
√
τ T , for any algorithm with a local permutation window

√

of size M < τ /3.

5. Experiments

We consider the adversarial setting described in section 4,
where an adversary chooses a sequence of functions such
that every τ functions are identical, creating blocks of size
τ of identical loss functions, of the form ft(wt) = αt · wt
where αt is chosen randomly in {−1, +1} for each block.
In all experiments we use T = 105 rounds, a delay pa-
rameter of τ = 200, set our step sizes according to the
theoretical analysis, and report the mean regret value over
1000 repetitions of the experiments.

In our ﬁrst experiment, we considered the behavior of our
Delayed Permuted Mirror Descent algorithm, for window
sizes M > τ , ranging from τ + 1 to T . In this experiment,
we chose the α values randomly, while ensuring a gap of
200 between the number of blocks with +1 values and the
number of blocks with −1 values (this ensures that the op-

timal w∗ is a sufﬁciently strong competitor, since otherwise
the setting is too “easy” and the algorithm can attain neg-
ative regret in some situations). The results are shown in
Figures 2 and 3, where the ﬁrst ﬁgure presents the accumu-
lated regret of our algorithm over time, whereas the second
ﬁgure presents the overall regret after T rounds, as a func-
tion of the window size M .

When applying our algorithm in this setting with differ-
ent values of M > τ , ranging from M = τ + 1 and up
to M = T , we get a regret that scales from the order of
the adversarial bound to the order of the stochastic bound
depending on the window size, as expected by our analy-
sis. For all window sizes greater than 5 · τ , we get a regret
that is in the order of the stochastic bound - this is not sur-
prising, since after the permutation we get a sequence of
functions that is very close to an i.i.d. sequence, in which
T ) regret
case any algorithm can be shown to achieve O(
in expectation. Note that this performance is better than
that predicted by our theoretical analysis, which implies an
T ) behavior only when M ≥ Ω(τ 2). It is an open
O(
and interesting question whether it means that our analysis
can be improved, or whether there is a harder construction
leading to a tighter lower bound.

√

√

In our second experiment, we demonstrate the brittleness
of the lower bound construction for standard online learn-
ing with delayed feedback, focusing on the M < τ regime.
Speciﬁcally, we create loss functions with blocks as before
(where following the lower bound construction, the α val-
ues in each block of size τ = 200 is chosen uniformly at
random). Then, we perform a random permutation over
consecutive windows of size M (ranging from M = 0 up
to M = 9
10 τ ). Finally, we run standard
Online Gradient Descent with delayed gradients (and ﬁxed
step size 1/
T ), on the permuted losses. The results are
presented in Figure 4.

10 τ in intervals of 1

√

For window sizes M < τ
2 we see that the regret is close to
the adversarial bound, whereas as we increase the window
size the regret decreases towards the stochastic bound. This
experiment evidently shows that this hardness construction
is indeed brittle, and easily breaks in the face of local per-
mutations, even for window sizes M < τ .

6. Discussion

We presented the OLLP setting, where a learner can locally
permute the sequence of examples from which she learns.
This setting can potentially allow for improved learning in
many problems, where the worst-case regret is based on
highly adversarial yet brittle constructions. In this paper,
we focused on the problem of learning from delayed feed-
back in the OLLP setting, and showed how it is possible to
improve the regret by allowing local permutations. Also,

Online Learning with Local Permutations and Delayed Feedback

Figure 2. Regret of the Delayed Permuted Mirror Descent algorithm, with local permutation in window sizes ranging from M = τ + 1
√
to M = T . A pink ∗ indicates the order of the stochastic bound (
T + τ ), and a red ∗ indicates the order of the adversarial bound
√
(

τ T ). Regret is averaged over 1000 repetitions. Best viewed in color.

Figure 3. Regret of the Delayed Permuted Mirror Descent algo-
rithm for different window sizes, after T = 105 iterations, with
local permutation window sizes ranging from M = τ + 1 to
M = T . Red dashed line (top) indicates the order of the adver-
τ T ) and green dashed line (bottom) indicates the
sarial bound (
T + τ ). Regret is averaged over
order of the stochastic bound (
1000 repetitions, error bars indicate standard error of the mean.
Best viewed in color.

√

√

Figure 4. Regret of the standard Online Gradient Descent algo-
rithm, in a adversarialy designed setting as described in 4, and
with local permutation in window sizes ranging from M = 0 to
M = 9
10 τ . Red dashed line (top) indicates the order of the adver-
τ T ) and green dashed line (bottom) indicates the
sarial bound (
T + τ ). Regret is averaged over
order of the stochastic bound (
1000 repetitions, error bars indicate standard error of the mean.
Best viewed in color.

√

√

we proved a lower bound in the situation where the per-
mutation window is signiﬁcantly smaller than the feedback
delay, and showed that in this case, permutations cannot
allow for a better regret bound than the standard adversar-
ial setting. We also provided some experiments, demon-
strating the power of the setting as well as the feasibility
of the proposed algorithm. An interesting open question
is what minimal permutation size allows non-trivial regret
improvement, and whether our upper bound in Theorem 1
is tight. As suggested by our empirical experiments, it
is possible that even small local permutations are enough
to break highly adversarial sequences and improve perfor-
mance in otherwise worst-case scenarios. Another interest-

ing direction is to extend our results to a partial feedback
(i.e. bandit) setting. Finally, it would be interesting to study
other cases where local permutations allow us to interpolate
between fully adversarial and more benign online learning
scenarios.

Acknowledgements

OS is supported in part by an FP7 Marie Curie CIG grant,
the Intel ICRI-CI Institute, and Israel Science Foundation
grant 425/13. LS is an ISEF fellow.

Online Learning with Local Permutations and Delayed Feedback

References

Alekh Agarwal and John C Duchi. Distributed delayed
In Advances in Neural Infor-

stochastic optimization.
mation Processing Systems, pages 873–881, 2011.

S´ebastien Bubeck and Aleksandrs Slivkins. The best of
both worlds: Stochastic and adversarial bandits.
In
COLT, pages 42–1, 2012.

Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad
Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu.
Online optimization with gradual variations. In COLT,
pages 6–1, 2012.

Elad Hazan and Satyen Kale. Extracting certainty from un-
certainty: Regret bounded by variation in costs. Machine
learning, 80(2-3):165–188, 2010.

Yevgeny Seldin and Aleksandrs Slivkins. One practical al-
gorithm for both stochastic and adversarial bandits. In
Proceedings of the 31st International Conference on Ma-
chine Learning (ICML-14), pages 1287–1295, 2014.

Shai Shalev-Shwartz et al. Online learning and online con-
vex optimization. Foundations and Trends R(cid:13) in Machine
Learning, 4(2):107–194, 2012.

Daniel A Spielman and Shang-Hua Teng. Smoothed anal-
ysis of algorithms: Why the simplex algorithm usually
takes polynomial time. Journal of the ACM (JACM), 51
(3):385–463, 2004.

Jacob Steinhardt and Percy Liang. Adaptivity and opti-
mism: An improved exponentiated gradient algorithm.
In ICML, pages 1593–1601, 2014.

Elad Hazan and Satyen Kale. Better algorithms for be-
nign bandits. Journal of Machine Learning Research, 12
(Apr):1287–1311, 2011.

Marcelo J Weinberger and Erik Ordentlich. On delayed
prediction of individual sequences. IEEE Transactions
on Information Theory, 48(7):1959–1976, 2002.

Elad Hazan et al. Introduction to online convex optimiza-
tion. Foundations and Trends R(cid:13) in Optimization, 2(3-4):
157–325, 2016.

Pooria Joulani, Andr´as Gy¨orgy, and Csaba Szepesv´ari. On-
In ICML (3),

line learning under delayed feedback.
pages 1453–1461, 2013.

Zohar S Karnin and Oren Anava. Multi-armed bandits:
Competing with optimal sequences. In NIPS, 2016.

John Langford, Alexander Smola, and Martin Zinkevich.
Slow learners are fast. arXiv preprint arXiv:0911.0491,
2009.

Ishai Menache, Ohad Shamir, and Navendu Jain. On-
demand, spot, or both: Dynamic resource allocation for
executing batch jobs in the cloud. In 11th International
Conference on Autonomic Computing (ICAC 14), pages
177–187, 2014.

Chris Mesterharm. On-line learning with delayed label
In International Conference on Algorithmic

feedback.
Learning Theory, pages 399–413. Springer, 2005.

Kent Quanrud and Daniel Khashabi. Online learning with
adversarial delays. In Advances in Neural Information
Processing Systems, pages 1270–1278, 2015.

Alexander Rakhlin and Karthik Sridharan. Online learning
with predictable sequences. In COLT, pages 993–1019,
2013.

Amir Sani, Gergely Neu, and Alessandro Lazaric. Exploit-
In Advances in
ing easy data in online optimization.
Neural Information Processing Systems, pages 810–818,
2014.

