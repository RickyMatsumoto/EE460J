Evaluating Bayesian Models with Posterior Dispersion Indices

Alp Kucukelbir 1 Yixin Wang 1 David M. Blei 1

Abstract

Probabilistic modeling is cyclical: we specify a
model, infer its posterior, and evaluate its per-
formance. Evaluation drives the cycle, as we re-
vise our model based on how it performs. This
requires a metric. Traditionally, predictive accu-
racy prevails. Yet, predictive accuracy does not
tell the whole story. We propose to evaluate a
model through posterior dispersion. The idea is
to analyze how each datapoint fares in relation to
posterior uncertainty around the hidden structure.
This highlights datapoints the model struggles
to explain and provides complimentary insight
to datapoints with low predictive accuracy. We
present a family of posterior dispersion indices
(PDI) that capture this idea. We show how a PDI
identiﬁes patterns of model mismatch in three real
data examples: voting preferences, supermarket
shopping, and population genetics.

1. Introduction

Probabilistic modeling is a ﬂexible approach to analyzing
structured data. Three steps deﬁne the approach. First, we
specify a model; this captures our structural assumptions
about the data. Then, we infer the hidden structure; this
means computing (or approximating) the posterior. Last, we
evaluate the model; this helps build better models down the
road (Blei, 2014).

How do we evaluate models? Decades of reﬂection have led
to deep and varied forays into model checking, comparison,
and criticism (Gelman et al., 2013). But a common theme
permeates all approaches to model evaluation: the desire to
generalize well.

In machine learning, we traditionally use two complemen-
tary tools: predictive accuracy and cross-validation. Pre-
dictive accuracy is the target evaluation metric. Cross-

1Columbia University, New York City, USA. Correspondence

to: Alp Kucukelbir <alp@cs.columbia.edu>.

Proceedings of the 34th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

validation captures a notion of generalization and justiﬁes
holding out data. This simple combination has fueled the
development of myriad probabilistic models (Bishop, 2006;
Murphy, 2012).

Does predictive accuracy tell the whole story? The predic-
tive accuracy of an observation is its per-datapoint likeli-
hood averaged over the posterior. In this sense, predictive
accuracy reports a mean value for each datapoint; it ignores
how each per-datapoint likelihood varies with respect to the
posterior.

Main idea. We propose to evaluate probabilistic models
through the idea of posterior dispersion, analyzing how each
datapoint fares in relation to posterior uncertainty around
the hidden structure. To capture this, we propose a family of
posterior dispersion indices (PDI). These are per-datapoint
quantities, each a variance to mean ratio of the datapoint’s
likelihood with respect to the posterior. A PDI highlights
observations whose likelihoods exhibit the most uncertainty
under the posterior.

Consider a model p.x; (cid:18)/ and the likelihood of a datapoint
p.xn j (cid:18)/. It depends on some hidden structure (cid:18) that we
seek to infer. Since (cid:18) is random, we can view the likelihood
of each datapoint as a random variable. Predictive accu-
racy reports the average likelihood of each xn with respect
to the posterior p.(cid:18) j x/. But this ignores how the likeli-
hood changes under the posterior. How can we capture this
uncertainty and compare datapoints to each other?

To answer this, we appeal to various forms of dispersion,
such as the variance of the likelihood under the posterior.
We propose a family of dispersion indices in Section 2.2;
they have the following form:

PDI D variance of likelihood under posterior
mean of likelihood under posterior
V(cid:18)jxŒp.xn j (cid:18)/(cid:141)
E(cid:18)jxŒp.xn j (cid:18)/(cid:141)

D

:

PDIs compliment predictive accuracy. Here is a mental
picture. Consider a nuclear power plant where we monitor
the temperature of a pool of water. We train a probabilistic
model; the posterior represents our uncertainty around some
safe temperature, say 80 degrees. Suppose we receive a high
measurement thigh (Figure 1). Its likelihood varies rapidly

Evaluating Bayesian Models with Posterior Dispersion Indices

y
t
i
s
n
e
D

y
t
i
s
n
e
D

t

80

80

t

posterior
likelihood of thigh measurement
likelihood of tzero measurement

Figure 1. A mental picture of how PDI identify different types of mismatch. While both measurements (thigh and tzero) exhibit low
predictive accuracy, a PDI differentiates them by also considering how their per-datapoint likelihoods vary under the posterior. See text for
more details.

across plausible posterior values for t. This datapoint is
reasonably well modeled, but is sensitive to the posterior.
Now imagine the thermostat breaks and we receive a zero
measurement tzero. This zero datapoint is poorly modeled.

Both datapoints may have similar predictive accuracy val-
ues under the model. (See Section 2.3 for how this can
occur.) But the high measurement is different than the zero
measurement. A PDI differentiates these measurements by
considering not only their predictive accuracy scores, but
also how their per-datapoint likelihoods vary with respect
to the posterior.

Section 3 presents an empirical study of model mismatch in
three real-world examples: voting preferences, supermarket
shopping, and population genetics. In each case, a PDI
provides insight beyond predictive accuracy and highlights
potential directions for improvement.

Related work. This paper relates to a constellation of ideas
from statistical model criticism.

The ﬁrst connection is to analysis of variance: PDI bears
similarity to ANOVA, which is a frequentist approach to
evaluating explanatory variables in linear regression (Davi-
son, 2003). Gelman et al. (1996) cemented the idea of
studying predictive accuracy of probabilistic models at the
data level; Vehtari et al. (2012) and Betancourt (2015) give
up-to-date overviews of these ideas. PDIs add to this body
of work by considering the variance of each datapoint in
context of its predictive accuracy.

The second connection is to model comparison. Recent
research, such as Gelman et al. (2014), Vehtari et al. (2014)
and Piironen & Vehtari (2015), explore the relationship
between cross validation and information criteria, such as
the widely applicable information criterion (WAIC) (Watan-
abe, 2010; Vehtari et al., 2016). WAIC offers an intuitive
connection to cross validation (Vehtari & Lampinen, 2002;
Watanabe, 2015); we draw inspiration from it in this paper
too. However our focus is on evaluation at the datapoint
level, not at the dataset level. In this sense, PDIs and infor-
mation criteria are complimentary tools.

chine learning community. Gretton et al. (2007) and
Chwialkowski et al. (2016) developed effective kernel-based
methods for independence and goodness-of-ﬁt tests. Re-
cently, Lloyd & Ghahramani (2015) visualized smooth re-
gions of data space that the model fails to explain. In con-
trast, we focus directly on the datapoints, which can live in
high-dimensional spaces that may be difﬁcult to visualize.

A ﬁnal connection is to scoring rules. While the literature on
scoring rules originally focused on probability forecasting
(Winkler, 1996; Dawid, 2006), recent advances draw new
connections to decision theory, divergence measures, and
information theory (Dawid & Musio, 2014). We discuss
how PDIs ﬁt into this picture in the conclusion.

2. Posterior Dispersion Indices

A posterior dispersion index (PDI) highlights datapoints
that exhibit the most uncertainty with respect to the hidden
structure of a model. Here is the road map for this section.
A small case study illustrates how a PDI gives more insight
beyond predictive accuracy. Deﬁnitions, theory, and an-
other small analysis give further insight; a straightforward
algorithm leads into the empirical study.

2.1. 44% outliers?

Hayden (2005) considers the number of days each U.S. pres-
ident stayed in ofﬁce; he submits that 44% of presidents may
be outliers. Figure 2 plots the data. One-term presidents
stay in ofﬁce for around 1460 days; two-term presidents
approximately double that. Yet many presidents deviate
from this “two bump” trend.

A reasonable model for such data is a mixture of negative
binomial distributions.1 Consider the parameterization of
the negative binomial with mean (cid:22) and variance (cid:22) C (cid:22)2=(cid:30).
Posit gamma priors on the (non-negative) latent variables.
Set the prior on (cid:22) to match the mean and variance of the
data (Robbins, 1964). Choose an uninformative prior on (cid:30).
Three mixtures make sense: two for the typical trends and
one for the rest.

The third connection is to a body of work from the ma-

1 A Poisson likelihood is too underdispersed.

Evaluating Bayesian Models with Posterior Dispersion Indices

e
c
ﬁ
f
o

n
i

s
y
a
D

2921

1460

3
Y

kD1

3
Y

kD1

0

Washington
VanBuren
Monroe
Adams
Adams
Madison
Jackson
Harrison
Jefferson

Tyler

Taylor
Lincoln
Buchanan
Filmore
Pierce
Johnson
Polk

Hayes
McKinley
Roosevelt
Cleveland
Cleveland
Harrison
Grant
Garﬁeld
Arthur

Harding
Coolidge
Kennedy
Hoover
Truman
Eisenhower
Roosevelt
Wilson
Johnson
Taft

Nixon

Reagan
Carter
Ford

Clinton
Bush

Bush

Figure 2. The number of days each U.S. president stayed in ofﬁce. Typical durations are easy to identify; gray lines indicate one- and
two-term stays. Appendix A presents numerical values.

The complete model is

p.(cid:25)/ D Dirichlet.(cid:25) I ˛ D .1; 1; 1//

p.(cid:22)/ D

Gam.(cid:22)k I mean and variance

matched to that of data/

p.(cid:30)/ D

Gam.(cid:30)k I a D 1; ˇ D 0:01/

2.2. Deﬁnitions

the high measurement in the nuclear plant example in the
introduction.

This case study suggests that predictive probability does
not tell the entire story. Datapoints can exhibit low predic-
tive accuracy in different ways. We now turn to a formal
deﬁnition of PDIs.

Let x D fxngN
1 be a dataset with N observations. A prob-
abilistic model has two parts. The ﬁrst is the likelihood,
p.xn j (cid:18)/. It relates an observation xn to hidden patterns
described by a set latent random variables (cid:18). If the observa-
tions are independent and identically distributed, the likeli-
hood of the dataset factorizes as p.x j (cid:18)/ D Q
n p.xn j (cid:18)/.
The second is the prior density, p.(cid:18)/.
It captures the
structure we expect from the hidden patterns. Combin-
ing the likelihood and the prior gives the joint density
p.x; (cid:18)/ D p.x j (cid:18)/p.(cid:18)/. Conditioning on observed data
gives the posterior density, p.(cid:18) j x/.

Treat the likelihood of each datapoint as a function of (cid:18). To
evaluate the model, we analyze how each datapoint fares in
relation to the posterior density. Consider these expectations
and variances with respect to the posterior,

(cid:22).n/ D E(cid:18)jxŒp.xn j (cid:18)/(cid:141)
(cid:22)log.n/ D E(cid:18)jxŒlog p.xn j (cid:18)/(cid:141)
(cid:27) 2.n/ D V(cid:18)jxŒp.xn j (cid:18)/(cid:141)
log.n/ D V(cid:18)jxŒlog p.xn j (cid:18)/(cid:141):
(cid:27) 2

(1)

Each includes the likelihood in a slightly different fash-
ion. The ﬁrst expectation is a familiar object: (cid:22).n/ is the
posterior predictive density.

A PDI is a ratio of these variances to expectations. Taking
the ratio calibrates this quantity for each datapoint. Recall
the mental picture from the introduction. The variance of

p.xn j (cid:25); (cid:22); (cid:30)/ D

(cid:25)kNB.xn I (cid:22)k; (cid:30)k/:

3
X

kD1

gives

this model

posterior mean

esti-
Fitting
mates b(cid:22) D .1461; 2896; 1578/ with
corresponding
b(cid:30) D .470; 509; 1.3/. The ﬁrst two clusters describe the
two typical term durations, while the third (highlighted in
bold red) is a dispersed negative binomial that attempts to
describe the rest of the data.

We compute a PDI (deﬁned in Section 2.2) and the posterior
predictive density for each president p.xn j x/. Figure 3
compares both metrics and sorts the presidents according to
the PDI.

Some presidents are clear outliers: Harrison [31: natural
death], Roosevelt [4452: four terms], and Garﬁeld
[199: assassinated]. However, there are three presidents
with worse predictive accuracy than Harrison: Coolidge,
Nixon, and Johnson. A PDI differentiates Harrison from
these three because his likelihood is varying rapidly with
respect to the dispersed negative binomial cluster.

This PDI also calls attention to McKinley [1655:
assassinated] and Arthur [1260:
succeeded
Garfield], because they are close to the sharp negative
binomial cluster at 1460 but not close enough to have good
predictive accuracy. They are datapoints whose likelihoods
are rapidly changing with respect to a peaked posterior, like

Evaluating Bayesian Models with Posterior Dispersion Indices

PDI

log p.xn j x/

(cid:0)0:05

(cid:0)0:1

x
e
d
n
i

n
o
i
s
r
e
s
p
i
D

y
t
i
s
n
e
d

g
o
L

(cid:0)7

(cid:0)9

(cid:0)11

e
c
ﬁ
f
o

n
i

s
y
a
D

2921

1460

0

McKinley
Harrison
Roosevelt

Reagan
Washington
Truman
Clinton
Garﬁeld
Arthur
Eisenhower
Roosevelt

Monroe
Madison
Wilson
Jefferson
Bush

Taylor
Filmore
Jackson
Grant

Harding
Coolidge
Kennedy
Ford

Johnson
Johnson
Nixon

Tyler
Hoover
Lincoln
Adams

Buchanan
Pierce
Polk

Hayes
VanBuren
Adams
Cleveland
Harrison

Taft

Carter
Cleveland

Bush

Figure 3. PDI and log predictive accuracy of each president under a mixture of three negative binomials model. Presidents sorted by PDI.
The closer to zero, the better. (Code in supplement.)

the likelihood under the posterior highlights potential model
mismatch; dividing by the mean calibrates this spread to its
predictive accuracy.

Calibration puts all datapoints on a common scale. Imagine
a binary classiﬁcation problem where each datapoint yn
lives in f0; 1g. The variances of the zero measurements may
be numerically quite different than the one measurements;
considering the mean renders these values comparable.

Related ratios also appear in classical statistics under a va-
riety of forms, such as indices of dispersion (Hoel, 1943),
coefﬁcients of variation (Koopmans et al., 1964), or the
Fano factor (Fano, 1947). They all quantify dispersion of
samples from a random process. PDIs extend these ideas by
connecting to the posterior density of a probabilistic model.

2015). WAPDI has two advantages; both are practically mo-
tivated. First, we hope the reader is computing an estimate
of generalization error. Gelman et al. (2014) suggests WAIC
because it is easy to compute and designed for common
machine learning models (Watanabe, 2010). Computing
WAIC gives WAPDI for free. Second, the variance is a
second-order moment calculation; using the log likelihood
gives numerical stability to the computation.
(More on
computation in Section 2.4.)

WAPDI compares the variance of the log likelihood to the
log posterior predictive. This gives insight into how the like-
lihood of a datapoint fares under the posterior distribution
of the hidden patterns. We now study this in more detail.

2.3. Intuition: not all predictive probabilities are

In this paper, we study a particular PDI, called the widely
applicable posterior dispersion index (WAPDI),

created equal

WAPDI.n/ D

(cid:27) 2
log.n/
log (cid:22).n/

:

Its form and name comes from the widely applicable in-
formation criterion WAIC D (cid:0) 1
log.n/:
N
WAIC measures generalization error;
it asymptotically
equates to leave-one-one cross validation (Watanabe, 2010;

n log (cid:22).n/ C (cid:27) 2

P

The posterior predictive density is an expectation,
E(cid:18)jxŒp.xnew j (cid:18)/(cid:141) D R p.xnew j (cid:18)/p.(cid:18) j x/ d(cid:18): Expecta-
tions are integrals: areas under a curve. Different likelihood
and posterior combinations can lead to similar integrals.

A toy model illustrates this. Consider a gamma likelihood
with ﬁxed shape, and place a gamma prior on the rate. The

Evaluating Bayesian Models with Posterior Dispersion Indices

y
t
i
s
n
e
D

p.x2 j x/ (cid:25)

p.x1 j x/

posterior p.ˇjx/
p.x1 D 0:727 j ˇ/
p.x2 D 15 j ˇ/

ˇ

0

0:5

1

1:5

Figure 4. Not all predictive probabilities are created equal. The translucent curves are the two likelihoods multiplied by the posterior
(cropped). The posterior predictives p.x1 j x/ and p.x2 j x/ for each datapoint is the area under the curve. While both datapoints have the
same predictive accuracy, the likelihood for x2 has higher variance under the posterior; x2 is more sensitive to the spread of the posterior
than x1. WAPDI captures this effect. (Code in supplement.)

model is

p.ˇ/ D Gam.ˇ I a0 D 1; b0 D 1/;

p.x j ˇ/ D

Gam.xn I a D 5; ˇ/;

N
Y

nD1

n xn/.

which gives the posterior p.ˇ j x/ D Gam.ˇ I a D a0 C
5N; b D b0 C P
Now simulate a dataset of size N D 10 with ˇ D 1; the
data have mean a=ˇ D 5. Now consider an outlier at 15. We
can ﬁnd another x value with essentially the same predictive
accuracy

log p.x1 D 0:727 j x/ D (cid:0)5:633433;
log p.x2 D 15 j x/ D (cid:0)5:633428:

Yet their WAPDI values differ by an order of magnitude

WAPDI.x1 D 0:727/ D (cid:0)0:067;
WAPDI.x2 D 15/ D (cid:0)0:229:
In this case, WAPDI highlights x2 D 15 as a more severe
outlier than x1 D 0:727, even though they have the same
predictive accuracy. What does that mean? Figure 4 depicts
the difference.

The following lemma explains how WAPDI measures this
effect. (Proof in Appendix B.)

Lemma 1 If log p.xn j (cid:18)/ is at least twice differentiable
and the posterior p.(cid:18) j x/ has ﬁnite ﬁrst and second mo-
ments, then a second-order Taylor approximation gives

WAPDI.n/ (cid:25)

(cid:0)log p0.xn j E(cid:18)jxŒ(cid:18)(cid:141)/(cid:1)2 V(cid:18)jxŒ(cid:18)(cid:141)
log E(cid:18)jxŒp.xn j (cid:18)/(cid:141)

:

(2)

Corollary 1 WAPDI highlights datapoints whose likeli-
hood is rapidly changing at the posterior mean estimate
of the latent variables. (V(cid:18)jxŒ(cid:18)(cid:141) is constant across n.)

Looking back at Figure 4, the likelihood p.x2 D 15 j ˇ/
indeed changes rapidly under the posterior. WAPDI reports
the ratio of this rate-of-change to the area under the curve.
In this speciﬁc example, only the numerator matters, since
the denominator is effectively the same for both datapoints.

Corollary 2 Equation (2) is zero if and only if the posterior
mean coincides with the maximum likelihood estimate of (cid:18)
for datapoint xn. (V(cid:18)jxŒ(cid:18)(cid:141) is positive for ﬁnite N .)

For most interesting models, we do not expect such a coin-
cidence. However, in practice, we ﬁnd WAPDI to be close
to zero for datapoints that match the model well. With that,
we now turn to computation.

2.4. Computation

Calculating WAPDI is straightforward. The only require-
ment are samples from the posterior. This is precisely the
output of an Markov chain Monte Carlo (MCMC) sampling
algorithm. (We used the no-U-turn sampler (Hoffman &
Gelman, 2014) for the analyses above.) Other inference
procedures, such as variational inference, give an analytic
approximation to the posterior (Jordan et al., 1999; Blei
et al., 2016). Drawing samples from an approximate pos-
terior also works. (We use this approach for the empirical
study in Section 3.)

Equipped with S samples from the posterior, Monte Carlo
integration (Robert & Casella, 1999) gives unbiased esti-
mates of the quantities in Equation (1). The variance of
these estimates decreases as O.1=S/; we assume S is suf-
ﬁciently large to cover the posterior (Gelman et al., 2014).
We default to S D 1000 in our experiments. Algorithm 1
summarizes these steps.

Evaluating Bayesian Models with Posterior Dispersion Indices

Algorithm 1: Calculating WAPDI.

1 , model p.x; (cid:18)/.

Input: Data x D fxngN
Output: WAPDI for each datapoint xn.
Draw S samples f(cid:18)gS
p.(cid:18) j x/.

1 from posterior (approximation)

for n in 1, 2, . . . , N do

Estimate log (cid:22).n/; (cid:27) 2

log.n/ from samples f(cid:18)gS
1 .

Store and return

WAPDI.n/ D

(cid:27) 2
log.n/
log (cid:22).n/

:

end

3. Experimental Study

We now explore three real data examples using modern
machine learning models: voting preferences, supermarket
shopping, and population genetics.

3.1. Voting preferences: a hierarchical logistic

regression model

In 1988, CBS conducted a U.S. nation-wide survey of voting
preferences. Citizens indicated their preference towards the
Democratic or Republican presidential candidate. Each indi-
vidual also declared their gender, age, race, education level,
and the state they live in; 11 566 individuals participated.

Gelman & Hill (2006) study this data through a hierarchical
logistic regression model. They begin by modeling gender,
race, and state; the state variable has a hierarchical prior.
This model is easy to ﬁt using automatic differentiation
variational inference (ADVI) within Stan (Kucukelbir et al.,
2015; Carpenter et al., 2015). (Model and inference details
in Appendix C.)

R

R R R

VOTE R
SEX F
F
RACE B
B
STATE WA WA NY WI NY NY NY NY MA MA

F
F
F
B B B

F
B

F
B

F
B

F
B

F
B

R

R

R

R

R

Table 1. Lowest predictive accuracy.

D
F

VOTE D
SEX F

D
D D D D
F M M M M M
RACE W W W W W W W W W W
STATE WY WY WY WY WY WY WY DC DC NV

D
F

D
F

D

Tables 1 and 2 show the individuals with the lowest predic-
tive accuracy and WAPDI. The nation-wide trend predicts
that females (F) who identify as black (B) have a strong pref-
erence to vote democratic (D); predictive accuracy identiﬁes
the few individuals who defy this trend. However, there is
not much to do with this information; the model identiﬁes a
nation-wide trend that correctly describes most female black
voters. In contrast, WAPDI points to parts of the dataset
that the model fails to describe; these are datapoints that we
might try to explain better with a revised model.

Most of the individuals with poor WAPDI live in Wyoming,
the District of Columbia, and Nevada. We focus on
Wyoming and Nevada. The average WAPDI for Wyoming
and Nevada are (cid:0)0:057 and (cid:0)0:041; these are baselines that
we seek to improve. (The closer to zero, the better.)

Consider expanding the model by modeling age. Introduc-
ing age into the model with a hierarchical prior reveals
that older voters tend to vote Republican. This helps ex-
plain Wyoming voters; their average WAPDI improves from
(cid:0)0:057 to (cid:0)0:04; however Nevada’s average WAPDI re-
mains unchanged. This means that Nevada’s voters may
not follow the national age-dependent trend. Now consider
removing age and introducing education in a similar way.
Education helps explain voters from both states; the average
WAPDI for Wyoming and Nevada improve to (cid:0)0:041 and
(cid:0)0:029.

WAPDI thus captures interesting datapoints beyond what
predictive accuracy reports. As expected, predictive accu-
racy still highlights the same female black voters in both
expanded models; WAPDI illustrates a deeper way to evalu-
ate this model.

3.2. Supermarket shopping: a hierarchical Poisson

factorization model

Market research ﬁrm IRi hosts an anonymized dataset of
customer shopping behavior at U.S. supermarkets (Bron-
nenberg et al., 2008). The dataset tracks 136 584 “checkout”
sessions; each session contains a basket of purchased items.
An inventory of 7 903 items range across categories such as
carbonated beverages, toiletries, and yogurt.

What items do customers tend to purchase together? To
study this, consider a hierarchical Poisson factorization
(HPF) model (Gopalan et al., 2015). HPF models the quan-
tities of items purchased in each session with a Poisson
likelihood; its rate is an inner product between a session’s
preferences (cid:18) s and the item attributes ˇ. Hierarchical priors
on (cid:18) and ˇ simultaneously promote sparsity, while account-
ing for variation in session size and item popularity. Some
sessions contain only a few items; others are large purchases.
(Model and inference details in Appendix D.)

Table 2. Worst WAPDI values.

A 20-dimensional HPF model discovers intuitive trends. A

Evaluating Bayesian Models with Posterior Dispersion Indices

few stand out. Snack-craving customers like to buy Doritos
tortilla chips along with Lay’s potato chips. Morning birds
typically pair Cheerios cereal with 2% skim milk. Yoplait
fans tend to purchase many different ﬂavors at the same
time. Tables 3 and 4 show the top ﬁve items in two of these
twenty trends.

WAPDI identiﬁes a valuable aspect of the data that the HPF
struggles to capture: sessions with spiked activity. This is a
concrete direction for model revision.

3.3. Population genetics: a mixed membership model

Do all people who live nearby have similar genomes? Not
necessarily. Population genetics considers how individuals
exhibit ancestral patterns of mutations. Begin with N indi-
viduals and L locations on the genome. For each location,
report whether each individual reveals a mutation. This
gives an (N (cid:2) L) dataset x where xnl 2 f0; 1; 2; 3g. (We
assume two speciﬁc forms of mutation; 3 encodes a missing
observation.)

Mixed membership models offer a way to study this
(Pritchard et al., 2000). Assume K ancestral populations (cid:30);
these are the mutation probabilities of each location. Each
individual mixes these populations with weights (cid:18); these are
the mixing proportions. Place a beta prior on the mutation
probabilities and a Dirichlet prior on the mixing proportions.
We study a dataset of N D 324 individuals from four geo-
graphic locations and focus on L D 13 928 locations on the
genome. Figure 5 shows how these individuals mix K D 3
ancestral populations. (Data, model, and inference details
in Appendix E.)

WAPDI reveals three interesting patterns of mismatch here.
First, individuals with poor WAPDI values have many miss-
ing observations; the worst 10% of WAPDI have 1 344 miss-
ing values, in contrast to 563 for the lowest 10% of pre-
dictive scores. We may consider directly modeling these
missing observations.

Second, ASW has two individuals with poor WAPDI; their
mutation patterns are outliers within the group. While the
average individual reveals 272 mutations away from the
median genome, these individuals show 410 and 383 muta-
tions. This points to potential mishaps while gathering or
pre-processing the data.

Last, MEX exhibits good predictive accuracy, yet poor
WAPDI values compared to other groups. Based on pre-
dictive accuracy, we may happily accept these patterns. Yet
WAPDI highlights a serious issue with the inferred popu-
lations. The blue and red populations are almost twice as
correlated across genes (0:58) as the other possible com-
binations (0:24 and 0:2). In other words, the blue and red
populations represent similar patterns of mutations at the
same locations. These populations, as they stand, are not
necessarily interpretable. Revising the model to penalize
correlation may be a direction worth pursuing.

Item Description

Category

Brand A: 2% skim milk milk rfg skim/lowfat
cold cereal
Cheerios: cereal
Diet Coke: soda
carbonated beverages
Brand B: 2% skim milk milk rfg skim/lowfat
Brand C: 2% skim milk milk rfg skim/lowfat

Table 3. Morning bird trend.

Item Description

Yoplait: raspberry ﬂavor
Yoplait: peach ﬂavor
Yoplait: strawberry ﬂavor
Yoplait: blueberry ﬂavor
Yoplait: blackberry ﬂavor

Category

yogurt rfg
yogurt rfg
yogurt rfg
yogurt rfg
yogurt rfg

Table 4. Yoplait fan trend.

Sessions where a customer purchases many items from dif-
ferent categories have low predictive accuracy. This makes
sense as these customers do not exhibit a trend; mathe-
matically, there is no combination of item attributes ˇ that
explain buying items from disparate categories. For exam-
ple, the session with the lowest predictive accuracy contains
117 items ranging from coffee to hot dogs.

WAPDI highlights a different aspect of the HPF model. Ses-
sions with poor WAPDI contain similar items but exhibit
many purchases of a single item. Table 5 shows an exam-
ple of a session where a customer purchased 14 blackberry
Yoplait yogurts, but only a few of the other ﬂavors.

Item Description

Quantity

Yoplait: blackberry ﬂavor
Yoplait: strawberry ﬂavor
Yoplait: raspberry ﬂavor
Yoplait: peach ﬂavor
Yoplait: cherry ﬂavor
Yoplait: mango ﬂavor

14
2
2
1
1
1

Table 5. A session with poor WAPDI value.

This indicates that the Poisson likelihood assumption may
not be ﬂexible enough to model customer purchasing be-
havior. Perhaps a negative binomial likelihood could model
this kind of spiked activity better. Another option might
be to keep the Poisson likelihood but increase the hierar-
chy of the probabilistic model; this approach may identify
item attributes that explain such purchases. In either case,

Evaluating Bayesian Models with Posterior Dispersion Indices

ASW

CEU

MEX

YRI

s
e
r
u
t
x
i
M

1

0
0

(cid:0)0:1

(cid:0)0:2

(cid:0)5
(cid:0)6
(cid:0)7
(cid:0)8

(cid:1)104

WAPDI

log p.xn j x/

Figure 5. Individuals of African ancestry in southwest U.S. (ASW) and Mexican ancestry in Los Angeles (MEX) exhibit a mixture of two
populations. In contrast, Utah residents with European ancestry (CEU) and members of the Yoruba group in Nigeria (YRI) are mostly
uniform.

4. Discussion

A posterior dispersion index (PDI) identiﬁes informative
forms of model mismatch that compliments predictive ac-
curacy. By highlighting which datapoints exhibit the most
uncertainty under the posterior, a PDI offers a new perspec-
tive into evaluating probabilistic models. Here, we show
how one particular PDI, the widely applicable posterior dis-
persion index (WAPDI), reveals promising directions for
model improvement across a range of models and applica-
tions.

The choice of WAPDI is practically motivated; it comes
for free as part of the calculation of WAIC. This highlights
how PDIs are complimentary to tools such as predictive
accuracy, cross validation, and information criteria. While
PDIs and predictive accuracy assess model mismatch at the
datapoint level, cross validation and information criteria
indicate model mismatch at the dataset level.

PDIs provide a relative comparison of datapoints with re-
spect to a model. Can PDIs be thresholded to identify “prob-
lematic” datapoints? One approach in this direction draws
inspiration from posterior predictive checks (PPCs) (Rubin,
1984; Gelman et al., 1996). A PPC works by hallucinating
data from the posterior predictive and comparing properties
of the hallucinated data to the observed dataset. Comparing
PDI values in this way could lead to a meaningful way of
thresholding PDIs.

There are several research directions. One is to extend the

notion of a PDI to non-exchangeable data. Another is to
leverage the bootstrap to extend this idea beyond proba-
bilistic models. Computationally, ideas from importance
sampling could reduce the variance of PDI computations for
very high dimensional models.

A promising direction is to study PDIs under the viewpoint
of scoring rules (Dawid & Musio, 2014). Understanding
the decision theoretic properties of a PDI as a loss function
could lead to alternative objectives for inference.

Finally, we end on a reminder that PDIs are simply another
tool in the statistician’s toolbox. The design and criticism of
probabilistic models is still a careful, manual craft. While
good tools can help, an overarching obstacle remains to
pursue their adoption by practitioners. To this end, making
these tools easier to use and more automatic can only help.

Acknowledgments

We thank Dustin Tran, Maja Rudolph, David Mimno, Aki
Vehtari, Josh Vogelstein, and Rajesh Ranganath for their
insightful comments. This work is supported by NSF
IIS-1247664, ONR N00014-11-1-0651, DARPA PPAML
FA8750-14-2-0009, DARPA SIMPLEX N66001-15-C-
4032, and the Alfred P. Sloan Foundation.

Evaluating Bayesian Models with Posterior Dispersion Indices

References

Betancourt, Michael. A uniﬁed treatment of predictive
model comparison. arXiv preprint arXiv:1506.02273,
2015.

Bishop, Christopher M. Pattern Recognition and Machine

Learning. Springer New York, 2006.

Blei, David M. Build, compute, critique, repeat: Data
analysis with latent variable models. Annual Review of
Statistics and Its Application, 1:203–232, 2014.

Blei, David M, Kucukelbir, Alp, and McAuliffe, Jon D.
Variational inference: a review for statisticians. arXiv
preprint arXiv:1601.00670, 2016.

Bronnenberg, Bart J, Kruger, Michael W, and Mela, Carl F.
The IRi marketing data set. Marketing Science, 27(4),
2008.

Carpenter, Bob, Gelman, Andrew, Hoffman, Matt, Lee,
Daniel, Goodrich, Ben, Betancourt, Michael, Brubaker,
Marcus A, Guo, Jiqiang, Li, Peter, and Riddell, Allen.
Stan: a probabilistic programming language. Journal of
Statistical Software, 2015.

Chwialkowski, Kacper, Strathmann, Heiko, and Gretton,
Arthur. A kernel test of goodness of ﬁt. arXiv preprint
arXiv:1602.02964, 2016.

Davison, Anthony Christopher. Statistical models. Cam-

bridge University Press, 2003.

Dawid, A. P. Probability Forecasting. John Wiley & Sons,

Inc., 2006. ISBN 9780471667193.

Dawid, Alexander Philip and Musio, Monica. Theory and
applications of proper scoring rules. Metron, 72(2):169–
183, 2014.

Fano, Ugo.

Ionization yield of radiations II. Physical

Review, 72(1):26, 1947.

Gelman, Andrew and Hill, Jennifer. Data analysis using re-
gression and multilevel/hierarchical models. Cambridge
University Press, 2006.

Gelman, Andrew, Meng, Xiao-Li, and Stern, Hal. Poste-
rior predictive assessment of model ﬁtness via realized
discrepancies. Statistica Sinica, 6(4):733–760, 1996.

Gopalan, Prem, Hofman, Jake M, and Blei, David M. Scal-
able recommendation with hierarchical Poisson factoriza-
tion. UAI, 2015.

Gretton, Arthur, Fukumizu, Kenji, Teo, Choon H, Song,
Le, Schölkopf, Bernhard, and Smola, Alex J. A kernel
statistical test of independence. NIPS, 2007.

Hayden, Robert W. A dataset that is 44% outliers. J Stat

Educ, 13(1), 2005.

Hoel, Paul G. On indices of dispersion. The Annals of

Mathematical Statistics, 14(2):155–162, 1943.

Hoffman, Matthew D and Gelman, Andrew. The No-U-Turn
sampler. Journal of Machine Learning Research, 15(1):
1593–1623, 2014.

Jordan, Michael I, Ghahramani, Zoubin, Jaakkola, Tommi S,
and Saul, Lawrence K. An introduction to variational
methods for graphical models. Machine Learning, 37(2):
183–233, 1999.

Koopmans, Lambert H, Owen, Donald B, and Rosenblatt,
JI. Conﬁdence intervals for the coefﬁcient of variation
for the normal and log normal distributions. Biometrika,
51(1/2):25–32, 1964.

Kucukelbir, Alp, Ranganath, Rajesh, Gelman, Andrew, and
Blei, David M. Automatic variational inference in Stan.
NIPS, 2015.

Lloyd, James R and Ghahramani, Zoubin. Statistical model
criticism using kernel two sample tests. NIPS, 2015.

Murphy, Kevin P. Machine Learning: a Probabilistic Per-

spective. MIT Press, 2012.

Piironen, Juho and Vehtari, Aki. Comparison of Bayesian
predictive methods for model selection. Statistics and
Computing, pp. 1–25, 2015.

Pritchard, Jonathan K, Stephens, Matthew, and Donnelly,
Peter. Inference of population structure using multilocus
genotype data. Genetics, 155(2):945–959, 2000.

Robbins, Herbert. The empirical Bayes approach to statisti-
cal decision problems. Annals of Mathematical Statistics,
1964.

Robert, Christian P and Casella, George. Monte Carlo

statistical methods. Springer, 1999.

Gelman, Andrew, Carlin, John B, Stern, Hal S, Dunson,
David B, Vehtari, Aki, and Rubin, Donald B. Bayesian
Data Analysis. CRC Press, 2013.

Rubin, Donald B. Bayesianly justiﬁable and relevant fre-
quency calculations for the applied statistician. The An-
nals of Statistics, 12(4):1151–1172, 1984.

Gelman, Andrew, Hwang, Jessica, and Vehtari, Aki. Un-
derstanding predictive information criteria for Bayesian
models. Statistics and Computing, 24(6):997–1016, 2014.

Vehtari, Aki and Lampinen, Jouko. Bayesian model assess-
ment and comparison using cross-validation predictive
densities. Neural Computation, 14(10):2439–2468, 2002.

Evaluating Bayesian Models with Posterior Dispersion Indices

Vehtari, Aki, Ojanen, Janne, et al. A survey of Bayesian
predictive methods for model assessment, selection and
comparison. Statistics Surveys, 6:142–228, 2012.

Vehtari, Aki, Tolvanen, Ville, Mononen, Tommi, and
Winther, Ole. Bayesian leave-one-out cross-validation ap-
proximations for Gaussian latent variable models. arXiv
preprint arXiv:1412.7461, 2014.

Vehtari, Aki, Gelman, Andrew, and Gabry, Jonah. Practical
Bayesian model evaluation using leave-one-out cross-
validation and WAIC. arXiv preprint arXiv:1507.04544,
2016.

Watanabe, Sumio. Asymptotic equivalence of Bayes cross
validation and widely applicable information criterion in
singular learning theory. Journal of Machine Learning
Research, 11:3571–3594, 2010.

Watanabe, Sumio. Bayesian cross validation and WAIC
for predictive prior design in regular asymptotic theory.
arXiv preprint arXiv:1503.07970, 2015.

Winkler, Robert L. Scoring rules and the evaluation of

probabilities. Test, 5(1):1–60, 1996.

