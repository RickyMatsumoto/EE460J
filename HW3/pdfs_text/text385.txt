Learning Inﬁnite Layer Networks Without the Kernel Trick

Roi Livni 1 Daniel Carmon 2 Amir Globerson 2

Abstract
Inﬁnite Layer Networks (ILN) have been pro-
posed as an architecture that mimics neural net-
works while enjoying some of the advantages
ILN are networks that in-
of kernel methods.
tegrate over inﬁnitely many nodes within a sin-
gle hidden layer.
It has been demonstrated by
several authors that the problem of learning ILN
can be reduced to the kernel trick, implying that
whenever a certain integral can be computed an-
alytically they are efﬁciently learnable.
In this
work we give an online algorithm for ILN, which
avoids the kernel trick assumption. More gen-
erally and of independent interest, we show that
kernel methods in general can be exploited even
when the kernel cannot be efﬁciently computed
but can only be estimated via sampling. We pro-
vide a regret analysis for our algorithm, showing
that it matches the sample complexity of methods
which have access to kernel values. Thus, our
method is the ﬁrst to demonstrate that the kernel
trick is not necessary, as such, and random fea-
tures sufﬁce to obtain comparable performance.

1. Introduction

With the increasing success of highly non-convex and com-
plex learning architectures such as neural networks, there
is an increasing effort to further understand and explain the
limits of training such hierarchical structures.

Recently there have been attempts to draw mathematical
insight from kernel methods in order to better understand
deep learning, as well as come up with new computation-
ally learnable architectures. One such line of work consists
of learning classiﬁers that are linear functions of a very
large or inﬁnite collection of non-linear functions (Bach,

1University

of Princeton,

Princeton, New Jersey,
Correspon-
Roi Livni <rlivni@cs.princeton.edu>, Daniel
Globerson

USA 2Tel-Aviv University, Tel-Aviv,
dence to:
Carmon <carmonda@mail.tau.ac.il>,
<gamir@mail.tau.ac.il>.

Israel.

Amir

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

2014; Daniely et al., 2016; Cho & Saul, 2009; Heinemann
et al., 2016; Williams, 1997). Such models can be inter-
preted as a neural network with inﬁnitely many nodes in a
hidden layer, and we thus refer to them as “Inﬁnite Layer
Networks” (ILN). They are of course also related to kernel
based classiﬁers, as will be discussed later.

A target function in an ILN class will be of the form:

(cid:90)

x →

ψ(x; w)f (w)dµ(w),

(1)

Here ψ is some function of the input x and parameters w,
and dµ(w) is a prior over the parameter space. For exam-
ple, ψ(x; w) can be a single sigmoidal neuron or a com-
plete convolutional network. The integral can be thought
of as an inﬁnite sum over all such possible networks, and
f (w) can be thought of as an inﬁnite output weight vector
to be trained.

A Standard 1–hidden layer network with a ﬁnite set of units
can be obtained from the above formalism as follows. First,
choose ψ(x; w) = σ(x · w) where σ is an activation func-
tion (e.g., sigmoid or relu). Next, set dµ(w) to be a discrete
measure over a ﬁnite set w1, . . . , wd.1 In this case, the inte-
gral results in a network with d hidden units, and the func-
tion f is the linear weights of the output layer. Namely:

x →

f (wi) · σ(x · wi).

1
d

d
(cid:88)

i=1

The main challenge when training 1–hidden layer networks
is of course to ﬁnd the w1, . . . , wd on which we wish to
support our distribution. It is known (Livni et al., 2014),
that due to hardness of learning intersection of halfspaces
(Klivans & Sherstov, 2006; Daniely et al., 2014), 1–hidden
layer neural networks are computationally hard for a wide
class of activation functions. Therefore, as the last exam-
ple illustrates, the choice of µ is indeed crucial for perfor-
mance.

For a ﬁxed prior µ, the class of ILN functions is highly
expressive, since f can be chosen to approximate any 1-
hidden layer architecture to arbitrary precision (by setting
f to delta functions around the weights of the network, as

1In δ function notation dµ(w) = 1
d

(cid:80)d

i=1 δ(w − wi)dw

Learning Inﬁnite Layer Networks Without the Kernel Trick

we did above for µ). However, this expressiveness comes
at a cost. As argued in Heinemann et al. (2016), ILN will
generalize well when there is a large probability mass of w
parameters that attain a small loss.

The key observation that makes certain ILN tractable to
learn is that Eq. 1 is a linear functional in f . In that sense it
is a linear classiﬁer and enjoys the rich theory and algorith-
mic toolbox for such classiﬁers. In particular, one can use
the fact that linear classiﬁers can be learned via the kernel
trick in a batch (Cortes & Vapnik, 1995) as well as online
settings (Kivinen et al., 2004). In other words, we can re-
duce learning ILN to the problem of computing the kernel
function between two examples. Speciﬁcally the problem
reduces to computing integrals of the following form:

k(x1, x2) =

ψ(x1; w) · ψ(x2; w)dµ(w)

(cid:90)

= E

¯w∼µ

[ψ(x1; ¯w) · ψ(x2; ¯w)] .

(2)

(3)

In this work we extend this result to the case where no
closed form kernel is available, and thus the kernel trick
is not directly applicable. We thus turn our attention to the
setting where features (i.e., w vectors) can be randomly
sampled.
In this setting, our main result shows that for
the squared loss, we can efﬁciently learn the above class.
Moreover, we can surprisingly do this with a computational
cost comparable to that of methods that have access to the
closed form kernel k(x1, x2).

The observation we begin with is that sampling random
features (i.e., w above), leads to an unbiased estimate of
the kernel in Eq. 2. Thus, if for example, we ignore com-
plexity issues and can sample inﬁnitely many w’s, it is not
surprising that we can avoid the need for exact computa-
tion of the kernel. However, our results provide a much
stronger and practical result. Given T training samples,
T ) (see
the lower bound on achievable accuracy is O(1/
Shamir, 2015). We show that we can in fact achieve this
rate, using ˜O(T 2) calls2 to the random feature generator.
For comparison, note that O(T 2) is the size of the kernel
matrix, and is thus likely to be the cost of any algorithm
that uses an explicit kernel matrix, where one is available.
As we discuss later, our approach improves on previous
random features based learning (Dai et al., 2014; Rahimi &
Recht, 2009) in terms of sample/computational complexity,
and expressiveness.

√

2. Problem Setup

We consider algorithms that learn a mapping from input
instances x ∈ X to labels y ∈ Y. We focus on the re-
gression case where Y is the interval [−1, 1]. Our starting
point is a class of feature functions ψ(w; x) : Ω × X → R,

2We use ˜O notation to suppress logarithmic factors

parametrized by vectors w ∈ Ω. The functions ψ(w; x)
may contain highly complex non linearities, such as multi-
layer networks consisting of convolution and pooling lay-
ers. Our only assumption on ψ(w; x) is that for all w ∈ Ω
and x ∈ X it holds that |ψ(w; x)| < 1.

Given a distribution µ on Ω, we denote by L2(Ω, µ) the
class of square integrable functions over Ω.

(cid:26)

(cid:90)

(cid:27)

L2(Ω, µ) =

f :

f 2(w)dµ(w) < ∞

.

We will use functions f ∈ L2(Ω, µ) as mixture weights
over the class Ω, where each f naturally deﬁnes a new re-
gression function from x to R as follows:
(cid:90)

x →

ψ(w; x)f (w)dµ(w).

(4)

Our key algorithmic assumption is that the learner can ef-
ﬁciently sample random w according to the distribution µ.
Denote the time to generate one such sample by ρ.

In what follows it will be simpler to express the integrals
as scalar products. Deﬁne the following scalar product on
functions f ∈ L2(Ω, µ).

(cid:90)

(cid:104)f, g(cid:105) =

f (w)g(w)dµ(w)

(5)

We denote the corresponding (cid:96)2 norm by (cid:107)f (cid:107) = (cid:112)(cid:104)f, f (cid:105).
Also, given features x denote by Φ(x) the function in
L2(Ω, µ) given by Φ(x)[w] = ψ(w; x). The regres-
sion functions we are considering are then of the form
x → (cid:104)f, Φ(x)(cid:105).

A subclass of norm bounded elements in L2(Ω, µ) induces
a natural subclass of regression functions. Namely, we con-
sider the following class:

HB

µ = {x → (cid:104)f, Φ(x)(cid:105) : (cid:107)f (cid:107) < B} .

Our ultimate goal is to output a predictor f ∈ L2(Ω, µ) that
is competitive, in terms of prediction, with the best target
function in the class HB
µ .

We will consider an online setting, and use it to derive gen-
eralization bounds via standard online to batch conversion.
In our setting, at each round a learner chooses a target func-
tion ft ∈ L2(Ω, µ) and an adversary then reveals a sample
xt and label yt. The learner then incurs a loss of

(cid:96)t(ft) =

((cid:104)ft, Φ(xt)(cid:105) − yt)2 .

(6)

1
2

The use of squared loss might seem restrictive if one is in-
terested in classiﬁcation. However, L2 loss is common by
now in classiﬁcation with support vector machines and ker-
nel methods since (Suykens & Vandewalle, 1999; Suykens

Learning Inﬁnite Layer Networks Without the Kernel Trick

et al., 2002). More recently Zhang et al. (2016) showed that
when using a large number of features regression achieves
performance comparable to the corresponding linear clas-
siﬁers (see Section 5 therein).

The objective of the learner is to minimize her T round
regret w.r.t norm bounded elements in L2(Ω, µ). Namely:

else

Algorithm 2: EST SCALAR PROD

Data: α, x1:t−1, x, m
Result: Estimated scalar product E
if α = ¯0 then
Set E = 0

T
(cid:88)

t=1

(cid:96)t(ft) − min
f ∗∈HB
µ

T
(cid:88)

t=1

(cid:96)t(f ∗).

(7)

In the statistical setting we assume that the sequence S =
{(xi, yi)}T
i=1 is generated IID according to some unknown
distribution P. We then deﬁne the expected loss of a pre-
dictor as

L(f ) = E

(x,y)∼P

(cid:20) 1
2

((cid:104)f, Φ(x)(cid:105) − y)2

.

(8)

(cid:21)

3. Main Results

Theorem 1 states our result for the online model. The cor-
responding result for the statistical setting is given in Corol-
lary 1. We will elaborate on the structure of the Algorithm
later, but ﬁrst provide the main result.

Algorithm 1: The SHRINKING GRADIENT algo-
rithm.

Data: T, B > 1, η, m
Result: Weights α(1), . . . , α(T +1) ∈ RT . Functions

i Φ(xi);

ft ∈ L2(Ω, µ) deﬁned as
ft = (cid:80)t
i=1 α(t)
Initialize α(1) = ¯0 ∈ RT ;
for t = 1, . . . , T do
Observe xt, yt;
Set Et =
EST SCALAR PROD(α(t), x1:t−1, xt, m);
if |Et| < 16B then
α(t+1) = α(t);
α(t+1)
t

= −η(yt − Et);

else

α(t+1) = 1

4 α(t);

for k=1.. . . ,m do

Sample i from the distribution q(i) = |αi|
(cid:80) |αi|
;
Sample parameter ¯w from µ. Set
E(k) = sgn(αi)ψ(xi; ¯w)ψ(x; ¯w);
k=1 E(k)

(cid:80)m

Set E = (cid:107)α(cid:107)1
m

2. The run-time of the algorithm is ˜O (cid:0)ρB4T 2(cid:1).3

3. For each t = 1 . . . T and a new test example x, we can
with probability ≥ 1 − δ estimate (cid:104)ft, Φ(x)(cid:105) within
accuracy (cid:15)0 by running Algorithm 2 with parameters
α(t), {xi}t
log 1/δ). The re-
sulting running time for a test point is then O(ρm).

i=1, , x and m = O( B4T
(cid:15)2
0

We next turn to the statistical setting, where we provide
bounds on the expected performance. Following standard
online to batch conversion and Theorem 1 we can obtain
the following Corollary (e.g., see Shalev-Shwartz, 2011):

Corollary 1 (Statistical Setting). The following holds for
any (cid:15) > 0. Run Algorithm 1 as in Theorem 1, with T =
O( B2
t=1, be an IID sample drawn
(cid:80) ft.
from some unknown distribution P. Let fS = 1
T
Then the expected loss satisﬁes:

(cid:15)2 ). Let S = {(xt, yt)}T

E
S∼P

[L(fS)] < inf

L(f ∗) + (cid:15).

f ∗∈HB
µ

The runtime of the algorithm, as well as estimation time on
a test example are as deﬁned in Theorem 1.

Proofs of the results are provided in Section 5.1 and the
appendix.

4. Related Work

Theorem 1. Run Algorithm 1 with parameters T , B ≥ 1,
η = B√
T

and m = O (cid:0)B4T log (BT )(cid:1). Then:

1. For every sequence of squared losses (cid:96)1, . . . , (cid:96)T ob-
served by the algorithm we have for f1, . . . , fT :

(cid:34) T

(cid:88)

E

t=1

(cid:96)t(ft) − min
f ∗∈HB
µ

T
(cid:88)

t=1

(cid:35)
(cid:96)t(f ∗)

√

= O(B

T )

Learning with random features can be traced to the early
days of learning (Minsky & Papert, 1988), and inﬁnite
networks have also been introduced more than 20 years
ago (Williams, 1997; Hornik, 1993). More recent works
have considered learning neural nets (also multi-layer) with
inﬁnite hidden units using the kernel trick (Cho & Saul,
2009; Deng et al., 2012; Hazan & Jaakkola, 2015; Heine-
mann et al., 2016). These works take a similar approach

3Ignoring logarithmic factors in B and T .

Learning Inﬁnite Layer Networks Without the Kernel Trick

to ours but focus on computing the kernel for certain fea-
ture classes in order to invoke the kernel trick. Our work
in contrast avoids using the kernel trick and applies to any
feature class that can be randomly generated. All the above
works are part of a broader effort of trying to circumvent
hardness in deep learning by mimicking deep nets through
kernels (Mairal et al., 2014; Bouvrie et al., 2009; Bo et al.,
2011; 2010), and developing general duality between neu-
ral networks and kernels (Daniely et al., 2016).

From a different perspective the relation between random
features and kernels has been noted by Rahimi & Recht
(2007) who show how to represent translation invariant ker-
nels in terms of random features. This idea has been further
studied (Bach, 2015; Kar & Karnick, 2012) for other ker-
nels as well. The focus of these works is mainly to allow
scaling down of the feature space and representation of the
ﬁnal output classiﬁer.

Dai et al. (2014) focus on tractability of large scale kernel
methods, and their proposed doubly stochastic algorithm
can also be used for learning with random features as we
have here. In Dai et al. (2014) the objective considered is
of the regularized form: γ
2 (cid:107)f (cid:107)2 + R(f ), with a correspond-
ing sample complexity of O(1/(γ2(cid:15)2)) samples needed to
achieve (cid:15) approximation with respect to the risk of the op-
timum of the regularized objective.

To relate the above results to ours, we begin by emphasiz-
ing that the bound in (Dai et al., 2014) holds for ﬁxed γ, and
refers to optimization of the regularized objective. Our ob-
jective is to minimize the risk R(f ) which is the expected
squared loss, for which we need to choose γ = O( (cid:15)
B2 ) in
order to attain accuracy (cid:15) (Sridharan et al., 2009). Plugging
this γ into the generalization bound in Dai et al. (2014) we
obtain that the algorithm in Dai et al. (2014) needs O( B4
(cid:15)4 )
samples to compete with the optimal target function in the
B-ball. Our algorithm needs O( B2
(cid:15)2 ) examples which is
considerably better. We note that their method does ex-
tend to a larger class of losses, whereas our is restricted to
the quadratic loss.

In Rahimi & Recht (2009),
the authors consider em-
bedding the domain into the feature space x →
[ψ(w1; x), . . . , ψ(wm; x)], where wi are IID random vari-
ables sampled according to some prior µ(w).
They
show that with O( B2 log 1/δ
) random features estimated on
O( B2 log 1/δ
(cid:15)2
(cid:26)

) samples they can compete with the class:

(cid:15)2

(cid:90)

(cid:27)

=

x →

ψ(w; x)f (w)dµ(w) : |f (w)| ≤ B

HB

µ max

Our algorithm relates to the mean square error cost func-
tion which does not meet the condition in Rahimi & Recht
(2009), and is hence formally incomparable. Yet we can
invoke our algorithm to compete against a larger class

of target functions. Our main result shows that Algo-
rithm 1, using ˜O( B8
(cid:15)4 ) estimated features and using O( B2
(cid:15)2 )
samples will, in expectation, output a predictor that is (cid:15)
close to the best in HB
µ . Note that |f (w)| < B implies
Ew∼µ(f 2(w)) < B2. Hence HB
µ . Note how-
ever, that the number of estimated features (as a function of
B) is worse in our case.

⊆ HB

µ max

Our approach to the problem is to consider learning with
a noisy estimate of the kernel. A related setting was stud-
ied in Cesa-Bianchi et al. (2011b), where the authors con-
sidered learning with kernels when the data is corrupted.
Noise in the data and noise in the scalar product estima-
tion are not equivalent when there is non-linearity in the
kernel space embedding. There is also extensive research
on linear regression with actively chosen attributes (Cesa-
Bianchi et al., 2011a; Hazan & Koren, 2012). The conver-
gence rates and complexity of the algorithms are dimension
dependent. It would be interesting to see if their method
can be extended from ﬁnite set of attributes to a continuum
set of attributes.

5. Algorithm

We next turn to present Algorithm 1, from which our main
result is derived. The algorithm is similar in spirit to Online
Gradient Descent (OGD) (Zinkevich, 2003), but with some
important modiﬁcations that are necessary for our analysis.

We ﬁrst introduce the problem in the terminology of online
convex optimization, as in Zinkevich (2003). At iteration t
our algorithm outputs a hypothesis ft. It then receives as
feedback (xt, yt), and suffers a loss (cid:96)t(ft) as in Eq. 6. The
objective of the algorithm is to minimize the regret against
a benchmark of B-bounded functions, as in Eq. 7.

A classic approach to the problem is to exploit the OGD
algorithm. Its simplest version would be to update ft+1 →
ft − η∇t where η is a step size, and ∇t is the gradient of
the loss w.r.t. f at ft. In our case, ∇t is given by:

∇t = ((cid:104)ft, Φ(xt)(cid:105) − yt) Φ(xt)

(9)

Applying this update would also result in a function ft =
(cid:80)t
i=1 αiΦ(xt) as we have in Algorithm 1 (but with differ-
ent αi from ours). However, in our setting this update is
not applicable since the scalar product (cid:104)ft, Φ(xt)(cid:105) is not
available. One alternative is to use a stochastic unbiased
estimate of the gradient that we denote by ¯∇t. This in-
duces an update step ft+1 → ft − η ¯∇t. One can show that
OGD with such an estimated gradient enjoys the following
upper bound on the regret E [(cid:80) (cid:96)t(ft) − (cid:96)t(f ∗)] for every
(cid:107)f ∗(cid:107) ≤ B (e.g., see Shalev-Shwartz, 2011):

B2
η

+ η

T
(cid:88)

i=1

E (cid:2)(cid:107)∇t(cid:107)2(cid:3) + η

V (cid:2) ¯∇t

(cid:3) ,

(10)

T
(cid:88)

i=1

Learning Inﬁnite Layer Networks Without the Kernel Trick

where V (cid:2) ¯∇t
(cid:3) = E (cid:2)(cid:107) ¯∇t − ∇t(cid:107)2(cid:3). We can bound the
ﬁrst two terms using standard techniques applicable for the
squared loss (e.g., see Zhang, 2004; Srebro et al., 2010).
The third term depends on our choice of gradient estimate.
There are various choices for such an estimate, and we use
a version which facilitates our analysis, as explained below.

i=1 α(t)

Assume that at iteration t, our function ft is given by ft =
(cid:80)t
i Φ(xt). We now want to use sampling to obtain
an unbiased estimate of (cid:104)ft, Φ(xt)(cid:105). This will be done via
a two step sampling procedure, as described in Algorithm
2. First, sample an index i ∈ [1, . . . , t] by sampling ac-
cording to the distribution q(i) ∝ |α(t)
|. Next, for the cho-
i
sen i, sample ¯w according to µ, and use ψ(x; ¯w)ψ(xi; ¯w)
to construct an estimate of (cid:104)Φ(xi), Φ(xt)(cid:105). The resulting
unbiased estimate of (cid:104)Φ(xi), Φ(xt)(cid:105) is denoted by Et and
given by:

Et =

(cid:107)α(t)(cid:107)1
m

m
(cid:88)

i=1

sgn(α(t)

i )ψ(xi; ¯w)ψ(xt; ¯w)

(11)

The corresponding unbiased gradient estimate is:

¯∇t = (Et − yt) xt

(12)

The variance of ¯∇ affects the convergence rate and depends
on both (cid:107)α(cid:107)1 and the number of estimations m. We wish to
maintain m = O(T ) estimations per round, while achiev-
ing O(

T ) regret.

√

To effectively regularize (cid:107)α(cid:107)1, we modify the OGD algo-
rithm so that whenever Et is larger then 16B, we do not
perform the usual update. Instead, we perform a shrinking
step that divides α(t) (and hence ft) by 4. Treating B as
constant, this guarantees that (cid:107)α(cid:107)1 = O(ηT ), and in turn
Var( ¯∇t) = O( η2T 2
T ), we have that
m ). Setting η = O(1/
m = O(T ) estimations are sufﬁcient.

√

The rationale for the shrinkage is that whenever Et is large,
it indicates that ft is “far away” from the B-ball, and a
shrinkage step, similar to projection, brings ft closer to the
optimal element in the B-ball. However, due to stochastic-
ity, the shrinkage step does add a further term to the regret
bound that we would need to take care of.

5.1. Analysis

In what follows we analyze the regret for Algorithm 1, and
provide a high level proof of Theorem 1. The appendix
provides the necessary lemmas and a more detailed proof.
We begin by modifying the regret bound for OGD in Eq. 10
to accommodate for steps that differ from the standard gra-
dient update, such as shrinkage. We use the following no-
tation for the regret at iteration t:

Rt(f ∗) = E

(cid:34) T

(cid:88)

(cid:35)
(cid:96)t(ft) − (cid:96)t(f ∗)

(13)

t=1

Lemma 1. Let (cid:96)1, . . . , (cid:96)T be an arbitrary sequence of con-
vex loss functions, and let f1, . . . , fT be random vectors,
produced by an online algorithm. Assume (cid:107)fi(cid:107) ≤ BT for
all i ≤ T . For each t let ¯∇t be an unbiased estimator of
∇(cid:96)t(ft). Denote ˆft = ft−1 − η ¯∇t−1 and let
(cid:104)
(cid:107)ft − f ∗(cid:107) > (cid:107) ˆft − f ∗(cid:107)

Pt(f ∗) = P

(14)

(cid:105)

.

For every (cid:107)f ∗(cid:107) ≤ B it holds that :

Rt(f ∗) ≤

E (cid:2)(cid:107)∇t(cid:107)2(cid:3) + η

V (cid:2) ¯∇t

(cid:3) +

T
(cid:88)

t=1

T
(cid:88)

+ η

t=1
(BT + B)2
η

B2
η

T
(cid:88)

t=1

E [Pt(f ∗)]

(15)

See Appendix B.1 for proof of the lemma. As discussed
earlier, the ﬁrst three terms on the RHS are the standard
bound for OGD from Eq. 10. Note that in the standard
OGD it holds that ft = ˆft, and therefore Pt(f ∗) = 0 and
the last term disappears.

The third term will be bounded by controlling (cid:107)α(cid:107)1. The
last term Pt(f ∗) is a penalty that results from updates that
stir ft away from the standard update step ˆft. This will
indeed happen for the shrinkage step. The next lemma
bounds this term. See Appendix B.2 for proof.

Lemma 2. Run Algorithm 1 with parameters T , B ≥ 1
and η < 1/8. Let ¯∇t be the unbiased estimator of ∇(cid:96)t(ft)
of the form ¯∇t = (Et − yt)Φ(xt). Denote ˆft = ft − η ¯∇t
and deﬁne Pt(f ∗) as in Eq. 14. Then:

Pt(f ∗) ≤ 2 exp

(cid:19)

(cid:18)

−

m
(3ηt)2

The following lemma (see Appendix B.3 for proof) bounds
the second and third terms of Eq. 15.

Lemma 3. Consider the setting as in Lemma 2. Then
V (cid:2) ¯∇t

and E (cid:2)(cid:107)∇t(cid:107)2(cid:3) ≤ 2E [(cid:96)t(ft)].

(cid:3) ≤ ((16B+1)ηt)2

m

Proof of Theorem 1 Combining Lemmas 1, 2 and 3 and
rearranging we get:

(1 − 2η)E [Rt(f ∗)] ≤

+ 2η

(cid:96)t(f ∗) + (16)

T
(cid:88)

B2
η

t=1
(BT + B)2
η

T
(cid:88)

t=1

Pt(f ∗)

η

((16B + 1)ηT )2T
m

+

To bound the second term in Eq. 16 we note that:

min
(cid:107)f ∗(cid:107)<B

T
(cid:88)

t=1

T
(cid:88)

t=1

(cid:96)t(f ∗) ≤

(cid:96)t(0) ≤ T.

(17)

Learning Inﬁnite Layer Networks Without the Kernel Trick

sampled from a standard Gaussian. We furthermore clip
negative values to zero, in order to make the data sparser
and more challenging for feature sampling. Next a weight
vector a ∈ RD is chosen as a random sparse linear combi-
nation of the training points. This is done in order for the
true function to be in the corresponding RKHS. Finally, the
training set is labeled using yi = a · xi.

During training we do not assume that the algorithms have
access to x. Rather they can uniformly sample coordi-
nates from it, which mimics our setting of random features.
For the experiment we take D = 550, 600, . . . , 800 and
T = 200. All algorithms perform one pass over the data,
to emulate the online regret setting. The results shown in
Figure 1 show that our method indeed achieves a lower loss
while working with the same feature budget.

We next set η and m as in the statement of the theorem.
Namely: η = B
, and m = ((16B+1)B)2T log γ, where
√
T

2

(cid:16) ((16B+1)ηT +B)2)
η2

γ = max
. This choice of m implies
, e
that m > ((16B + 1)ηT )2, and hence the third term in
Eq. 16 is upper bounded by T .

(cid:17)

Next we have that m > (3ηt)2 log γ for every t, and by the
bound on BT we have that γ > (B+BT )2
. Taken together
with Lemma 2 we have that:

η2

(BT + B)2
η

T
(cid:88)

t=1

Pt(f ∗) ≤ ηT.

(18)

The above bounds imply that:

(1 − 2η)E [Rt(f ∗)] ≤

+ 2ηT + ηT + ηT

B2
η

Finally by choice of η, and dividing both sides by (1 − 2η)
we obtain the desired result.

6. Experiments

In this section we provide a toy experiment to compare
our Shrinking Gradient algorithm to other random fea-
In particular, we consider the fol-
ture based methods.
lowing three algorithms: Fixed-Random: Sample a set of
r features w1, . . . , wr and evaluate these on all the train
and test points. Namely, all x points will be evaluated
on the same features. This is the standard random fea-
tures approach proposed in Rahimi & Recht (2007; 2009).
Doubly Stochastic Gradient Descent (Dai et al., 2014):
Here each training point x samples k features w1, . . . , wk.
These features will from that point on be used for evaluat-
ing dot products with x. Thus, different x points will use
different features. Shrinking Gradient: This is the ap-
proach proposed here in Section 3. Namely, each training
point x samples m features in order to calculate the dot
product with the current regression function.

In comparing the algorithms we choose r, k, m so that the
same overall number of features is calculated. For all meth-
ods we explored different initial step sizes and schedules
for changing the step size.

The key question in comparing the three algorithms is how
well they use a given budget of random features. To ex-
plore this we perform an experiments to simulate the high
dimensional feature case. We consider vectors x ∈ RD,
where a random feature w corresponds to a uniform choice
of coordinate w in x. We work in the regime where D is
large in the sense that D > T , where T is the size of the
training data. Thus random sampling of T features will
not reveal all coordinates of x. The training set is gener-
ated as follows. First, a training set x1, . . . , xT ∈ RD is

Figure 1. Comparison of three random feature methods. See Sec-
tion 6 for details.

7. Discussion

We presented a new online algorithm that employs kernels
implicitly but avoids the kernel trick assumption. Namely,
the algorithm can be invoked even when one has access to
only estimations of the scalar product. The problem was
motivated by kernels resulting from neural nets, but it can
of course be applied to any scalar product of the form we
described. As an example of an interesting extension, con-
sider a setting where a learner can observe an unbiased esti-
mate of a coordinate in a kernel matrix, or alternatively the
scalar product between any two observations. Our results
imply that in this setting the above rates are applicable, and
at least for the square loss, having no access to the true
values in the kernel matrix is not necessarily prohibitive
during training.

The results show that with sample size T we can achieve
error of O( B√
). As demonstrated in Shamir (2015) these
T

550600650700750800input dimension6080100120140160180200220240lossFixed-RandomDoubly-StochShrink-GradLearning Inﬁnite Layer Networks Without the Kernel Trick

rates are optimal, even when the scalar product is com-
putable. To achieve this rate our algorithm needs to per-
form ˜O(B4T 2) scalar product estimations. When the
scalar product can be computed, existing kernelized algo-
rithms need to observe a ﬁxed proportion of the kernel ma-
trix, hence they observe order of Ω(T 2) scalar products.
In Cesa-Bianchi et al. (2015) it was shown that when the
scalar product can be computed exactly, one would need
access to at least Ω(T ) entries to the kernel matrix. It is
still an open problem whether one has to access Ω(T 2) en-
tries when the kernel can be computed exactly. However,
as we show here, for ﬁxed B even if the kernel can only be
estimated ˜O(T 2) estimations are enough. It would be inter-
esting to further investigate and improve the performance
of our algorithm in terms of the norm bound B.

To summarize, we have shown that the kernel trick is not
strictly necessary in terms of sample complexity. Instead,
simply sampling random features via our proposed algo-
rithm results in a similar sample complexity. Recent empir-
ical results by Zhang et al. (2016) show that using a large
number of random features and regression comes close to
the performance of the ﬁrst successful multilayer CNNs
(Krizhevsky et al., 2012) on CIFAR-10. Although deep
learning architectures still substantially outperform random
features, it is conceivable that with the right choice of
random features, and scalable learning algorithms like we
present here, considerable improvement in performance is
possible.

A. Estimation Concentration Bounds

In this section we provide concentration bounds for the es-
timation procedure in Algorithm 2.
Lemma 4. Run Algorithm 2 with α and, {xi}T
i=1, x, and
m. Let f = (cid:80) αiΦ(xi). Assume that |ψ(x; w)| < 1 for
all w and x. Let E be the output of Algorithm 2. Then E
is an unbiased estimator for (cid:104)f, Φ(x)(cid:105) and:

P [|E − (cid:104)f, Φ(x)(cid:105)| > (cid:15)] ≤ exp

−

(19)

(cid:18)

(cid:19)

m(cid:15)2
(cid:107)α(cid:107)2
1

Proof. Consider the random variables (cid:107)α(cid:107)1E(k) (where
E(k)
is as deﬁned in Algorithm 2) and note that
they are IID. One can show that E (cid:2)(cid:107)α(cid:107)1E(k)(cid:3) =
(cid:80) αiE [ψ(xi; w)ψ(x; w)] = (cid:104)f, Φ(x)(cid:105). By the bound on
ψ(x; w) we have that (cid:12)
(cid:12)(cid:107)α(cid:107)1E(k)(cid:12)
(cid:12) < (cid:107)α(cid:107)1 with probability
(cid:80) E(k) the result follows directly from
1. Since E = 1
m
Hoeffding’s inequality.

Next, we bound the α(t) coeffcients and obtain a concen-
tration bound for the estimated dot product Et.
Lemma 5. The α(t) obtained in Algorithm 1 satisﬁes:

As a corollary of this and Lemma 4 we have that the func-
tion ft satisﬁes:

P [|Et − (cid:104)ft, Φ(xt)(cid:105)| > (cid:15)] ≤ exp

−

(cid:18)

(cid:15)2m
((16B + 1)ηt)2

(cid:19)

(20)

Proof. We prove the statement by induction. We separate
into two cases, depending on whether the shrinkage step
was performed or not.

If |Et| ≥ 16B the algorithm sets α(t+1) = 1

4 α(t), and:

(cid:107)α(t+1)(cid:107)1 =

(cid:107)α(t)(cid:107)1 ≤ (16B + 1)η(t + 1)

1
4

If |Et| < 16B the gradient update is performed. Since
|yt| ≤ 1 we have that |Et − yt| < 16B + 1 and:

(cid:107)α(t+1)(cid:107)1 ≤ (cid:107)α(t)(cid:107)1 + η|Et − yi| ≤ (16B + 1)η(t + 1).

B. Proofs of Lemmas

B.1. Proof of Lemma 1

First, by convexity we have that

2((cid:96)t(ft) − (cid:96)t(f ∗)) ≤ 2 (cid:104)∇t, ft − f ∗(cid:105) .

(21)

Next we upper bound (cid:104)∇t, ft − f ∗(cid:105). Denote by E the event
(cid:107)ft+1 − f ∗(cid:107) > (cid:107) ˆft+1 − f ∗(cid:107). Note that:

(cid:104)

(cid:107) ˆft+1 − f ∗(cid:107)2(cid:105)

+

E (cid:2)(cid:107)ft+1 − f ∗(cid:107)2(cid:3) ≤ E
E (cid:2)(cid:107)ft+1 − f ∗(cid:107)2(cid:12)
≤ E

(cid:107) ˆft+1 − f ∗(cid:107)2(cid:105)
(cid:104)

(cid:12)E(cid:3) · Pt+1(f ∗)

+ (B + BT )2Pt+1(f ∗)

Plugging in ˆft+1 = ft − η ¯∇t, summing over t and using
Eq. 21 and E (cid:2)(cid:107) ¯∇t(cid:107)2(cid:3) = E (cid:2)(cid:107)∇t(cid:107)2(cid:3) + V (cid:2) ¯∇t
(cid:3), we obtain
the desired result.

B.2. Proof for Lemma 2

To prove the bound in the lemma, we ﬁrst bound the event
Pt(f ∗) w.r.t to two possible events:
Lemma 6. Consider the setting as in Lemma 2. Run Algo-
rithm 1 and for each t consider the following two events:

• E t

1 : |Et| > 16B and |Et| > 1

4η (cid:107)ft(cid:107).

• E t

2 : |Et| > 16B and (cid:107)ft(cid:107) < 8B.

(cid:107)α(t)(cid:107)1 ≤ (16B + 1)ηt.

For every (cid:107)f ∗(cid:107) < B we have that Pt(f ∗) < P [E t

1 ∪ E t
2].

Learning Inﬁnite Layer Networks Without the Kernel Trick

Proof. Denote the event |Et| > 16B by E t
does not happen, then ft = ˆft. Hence trivially

0. Note that if E t
0

Which implies Et < 1
happen. We conclude that if E1 and not E2 then:

4η (cid:107)f (cid:107), and we get that E1 did not

Pt(f ∗) = P

(cid:107)ft − f ∗(cid:107) > (cid:107) ˆft − f ∗(cid:107) ∧ E t
0

(cid:104)

(cid:105)

|Et − (cid:104)ft, Φ(xt)(cid:105)| ≥ (

− 1)8B.

1
4η

We will assume that: (1) |Et| > 16B., (2) |Et| < 1
4η (cid:107)ft(cid:107).,
(3) (cid:107)ft(cid:107) > 8B. We then show (cid:107)ft+1−f ∗(cid:107) ≤ (cid:107) ˆft+1−f ∗(cid:107).
In other words, we will show that if E t
f ∗(cid:107) > (cid:107) ˆft+1 − f ∗(cid:107), then either E t
will conclude the proof.

0 happens and (cid:107)ft+1−
2 or E t
1 happened. This

Fix t, note that since |ψ(x; w)| < 1 we have that (cid:107)Φ(x)(cid:107) <
1. We then have:

(cid:107) ˆft+1(cid:107) = (cid:107)ft − η(Et − y)Φ(xt)(cid:107)

(22)

≥ (cid:107)ft(cid:107) − η|Et| − η ≥

(cid:107)ft(cid:107) − η

3
4

Since 1
leading to:

4η − 1 > 1 we have that: |Et − (cid:104)ft, Φ(xt)(cid:105)| ≥ 8B,

P [E1 ∩ E c

2] ≤ P [|Et − (cid:104)ft, Φ(xt)(cid:105)| ≥ 8B] .

(23)

A bound for P [E2]: If |Et| > 16B and (cid:107)ft(cid:107) < 8B then
by normalization of Φ(xt) we have that (cid:104)ft, Φ(xt)(cid:105) < 8B
and trivially we have that |Et − (cid:104)ft, Φ(xt)(cid:105)| ≥ 8B, and
therefore:

P [E2] ≤ P [|Et − (cid:104)ft, Φ(xt)(cid:105)| ≥ 8B] .

(24)

Taking Eq. 23 and Eq. 24 we have that

P [E2 ∪ E1] ≤ 2P [|Et − (cid:104)ft, Φ(xt)(cid:105)| ≥ 8B] .

(25)

where the last inequality is due to assumption (2) above.
We therefore have the following for every (cid:107)f ∗(cid:107) < B:

By Lemma 5 we have that:

(cid:107) ˆft+1 − f ∗(cid:107) ≥

(cid:107)ft(cid:107) − η − B

3
4

P (|Et − (cid:104)ft, Φ(xt)(cid:105)|) > 8B) <

m(8B)2

exp(−

((16B + 1)ηt)2 ) < exp

(cid:19)

(cid:18)

−

m
(3ηt)2

On the other hand, if ft+1 (cid:54)= ˆft+1 then by construction of
the algorithm ft+1 = 1

4 ft:

Taking the above upper bounds together with Lemma 6 we
can prove Lemma 2.

(cid:107)ft+1 − f ∗(cid:107) ≤ (cid:107)ft+1(cid:107) + (cid:107)f ∗(cid:107) ≤

(cid:107)ft(cid:107)
4

+ B.

B.3. Proof of Lemma 3

Next note that η < 2B and assumption (3) states (cid:107)ft(cid:107) >
8B. Therefore: 1
2 (cid:107)ft(cid:107) > 4B > η + 2B, and:

(cid:107) ˆft+1 − f ∗(cid:107) ≥

(cid:107)ft(cid:107) − η − B

3
4
1
4
1
4

=

≥

(cid:107)ft(cid:107) +

(cid:107)ft(cid:107) − η − 2B

+ B

(cid:19)

(cid:18) 1
2

(cid:107)ft(cid:107) + B ≥ (cid:107)ft+1 − f ∗(cid:107)

Next we upper bound P [E t
perscript t is dropped.
A bound for P [E1 ∩ E c

2]: Assume that

1 ∪ E t

2]. In what follows the su-

Begin by noting that since (cid:107)Φ(x)(cid:107) < 1, it follows from the
deﬁnitions of ∇, ¯∇ that V (cid:2) ¯∇t
(cid:3) = E (cid:2)(cid:107) ¯∇t − ∇t(cid:107)2(cid:3) and
therefore

V (cid:2) ¯∇t

(cid:3) ≤ E

(cid:104)

(Et − (cid:104)ft, Φ(xt)(cid:105))2(cid:105)

= V [Et]

By construction (see Algorithm 2) we have that:
(cid:104)
(cid:107)α(t)(cid:107)2

1ψ(xi; w)ψ(xt; w)

V [Et] =

V

(cid:105)

1
m

where the index i is sampled as in Algorithm 2, and
ψ(xi; w)ψ(xt; w) is bounded by 1. By Lemma 5 we have
that

V [Et] ≤
.
This provides the required bound on V (cid:2) ¯∇t
we have that

((16B + 1)ηt)2
m

(cid:3). Additionally,

|Et − (cid:104)ft, Φ(xt)(cid:105)| < (

− 1)8B.

(cid:107)∇t(cid:107)2 = ((cid:104)ft, Φ(xt)(cid:105) − yt)2(cid:107)Φ(xt)(cid:107)2 ≤ 2(cid:96)t(ft)

1
4η

We assume T is sufﬁciently large and η < 1
8 . We have
1
4η − 1 > 1. Since we assume E2 did not happen we must
have (cid:107)ft(cid:107) > 8B and |Et − (cid:104)ft, Φ(xt)(cid:105)| < ( 1
4η − 1)(cid:107)f (cid:107),
and therefore:

Et − (cid:107)f (cid:107) < |Et − (cid:104)ft, Φ(xt)(cid:105)| < (

− 1)(cid:107)f (cid:107).

1
4η

and the result follows by taking expectation.

Acknowledgements The authors would like to thank
Tomer Koren for helpful discussions. Roi Livni was sup-
ported by funding from Eric and Wendy Schmidt Fund
for Strategic Innovation. This work was supported by
the Blavatnik Computer Science Research Fund, the Intel
Collaborative Research Institute for Computational Intelli-
gence (ICRI-CI), and an ISF Centers of Excellence grant.

Learning Inﬁnite Layer Networks Without the Kernel Trick

References

Bach, Francis. Breaking the curse of dimensionality with convex

neural networks. arXiv preprint arXiv:1412.8690, 2014.

Bach, Francis. On the equivalence between kernel quadra-
arXiv preprint

ture rules and random feature expansions.
arXiv:1502.06800, 2015.

Bo, Liefeng, Ren, Xiaofeng, and Fox, Dieter. Kernel descriptors
for visual recognition. In Advances in neural information pro-
cessing systems, pp. 244–252, 2010.

Bo, Liefeng, Lai, Kevin, Ren, Xiaofeng, and Fox, Dieter. Object
recognition with hierarchical kernel descriptors. In Computer
Vision and Pattern Recognition (CVPR), 2011 IEEE Confer-
ence on, pp. 1729–1736. IEEE, 2011.

Bouvrie, Jake, Rosasco, Lorenzo, and Poggio, Tomaso. On in-
variance in hierarchical models. In Advances in Neural Infor-
mation Processing Systems, pp. 162–170, 2009.

Cesa-Bianchi, Nicolo, Shalev-Shwartz, Shai, and Shamir, Ohad.
Efﬁcient learning with partially observed attributes. The Jour-
nal of Machine Learning Research, 12:2857–2878, 2011a.

Cesa-Bianchi, Nicolo, Shalev-Shwartz, Shai, and Shamir, Ohad.
Information Theory, IEEE

Online learning of noisy data.
Transactions on, 57(12):7907–7931, 2011b.

Cesa-Bianchi, Nicol`o, Mansour, Yishay, and Shamir, Ohad. On
the complexity of learning with kernels. In Proceedings of The
28th Conference on Learning Theory, pp. 297–325, 2015.

Cho, Youngmin and Saul, Lawrence K. Kernel methods for deep
In Advances in neural information processing sys-

learning.
tems, pp. 342–350, 2009.

Cortes, Corinna and Vapnik, Vladimir. Support-vector networks.

Machine learning, 20(3):273–297, 1995.

Dai, Bo, Xie, Bo, He, Niao, Liang, Yingyu, Raj, Anant, Balcan,
Maria-Florina F, and Song, Le. Scalable kernel methods via
doubly stochastic gradients. In Advances in Neural Informa-
tion Processing Systems, pp. 3041–3049, 2014.

Daniely, Amit, Linial, Nati, and Shalev-Shwartz, Shai. From aver-
age case complexity to improper learning complexity. In Pro-
ceedings of the 46th Annual ACM Symposium on Theory of
Computing, pp. 441–448. ACM, 2014.

Daniely, Amit, Frostig, Roy, and Singer, Yoram. Toward deeper
understanding of neural networks: The power of initialization
and a dual view on expressivity. In Lee, D. D., Sugiyama, M.,
Luxburg, U. V., Guyon, I., and Garnett, R. (eds.), Advances
in Neural Information Processing Systems 29, pp. 2253–2261.
Curran Associates, Inc., 2016.

Deng, Li, Tur, Gokhan, He, Xiaodong, and Hakkani-Tur, Dilek.
Use of kernel deep convex networks and end-to-end learning
for spoken language understanding. In Spoken Language Tech-
nology Workshop (SLT), 2012 IEEE, pp. 210–215. IEEE, 2012.

Hazan, Tamir and Jaakkola, Tommi. Steps toward deep ker-
arXiv preprint

nel methods from inﬁnite neural networks.
arXiv:1508.05133, 2015.

Heinemann, Uri, Livni, Roi, Eban, Elad, Elidan, Gal, and Glober-
son, Amir. Improper deep kernels. In Proceedings of the 19th
International Conference on Artiﬁcial Intelligence and Statis-
tics, pp. 1159–1167, 2016.

Hornik, Kurt. Some new results on neural network approxima-

tion. Neural Networks, 6(8):1069–1072, 1993.

Kar, Purushottam and Karnick, Harish. Random feature maps for
dot product kernels. In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 583–591, 2012.

Kivinen, Jyrki, Smola, Alexander J, and Williamson, Robert C.
Online learning with kernels. IEEE transactions on signal pro-
cessing, 52(8):2165–2176, 2004.

Klivans, Adam R and Sherstov, Alexander A. Cryptographic
hardness for learning intersections of halfspaces. In Founda-
tions of Computer Science, 2006. FOCS’06. 47th Annual IEEE
Symposium on, pp. 553–562. IEEE, 2006.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Ima-
genet classiﬁcation with deep convolutional neural networks.
In Advances in neural information processing systems, pp.
1097–1105, 2012.

Livni, Roi, Shalev-Shwartz, Shai, and Shamir, Ohad. On the com-
putational efﬁciency of training neural networks. In Advances
in Neural Information Processing Systems, pp. 855–863, 2014.

Mairal, Julien, Koniusz, Piotr, Harchaoui, Zaid, and Schmid,
Cordelia. Convolutional kernel networks. In Advances in Neu-
ral Information Processing Systems, pp. 2627–2635, 2014.

Minsky, Marvin and Papert, Seymour. Perceptrons: an introduc-
tion to computational geometry (expanded edition), 1988.

Rahimi, Ali and Recht, Benjamin. Random features for large-
scale kernel machines. In Advances in neural information pro-
cessing systems, pp. 1177–1184, 2007.

Rahimi, Ali and Recht, Benjamin. Weighted sums of random
kitchen sinks: Replacing minimization with randomization in
In Advances in neural information processing sys-
learning.
tems, pp. 1313–1320, 2009.

Shalev-Shwartz, Shai. Online learning and online convex opti-
mization. Foundations and Trends in Machine Learning, 4(2):
107–194, 2011.

Shamir, Ohad. The sample complexity of learning linear predic-
tors with the squared loss. Journal of Machine Learning Re-
search, 16(Dec):3475–3486, 2015.

Srebro, Nathan, Sridharan, Karthik, and Tewari, Ambuj. Smooth-
ness, low noise and fast rates. In Advances in neural informa-
tion processing systems, pp. 2199–2207, 2010.

Sridharan, Karthik, Shalev-Shwartz, Shai, and Srebro, Nathan.
In Advances in Neural

Fast rates for regularized objectives.
Information Processing Systems, pp. 1545–1552, 2009.

Hazan, Elad and Koren, Tomer. Linear regression with limited ob-
servation. In Proceedings of the 29th International Conference
on Machine Learning (ICML-12), pp. 807–814, 2012.

Suykens, Johan AK and Vandewalle, Joos. Least squares sup-
port vector machine classiﬁers. Neural processing letters, 9
(3):293–300, 1999.

Learning Inﬁnite Layer Networks Without the Kernel Trick

Suykens, Johan AK, Van Gestel, Tony, and De Brabanter, Jos.
Least squares support vector machines. World Scientiﬁc, 2002.

Williams, Christopher. Computing with inﬁnite networks. Ad-
vances in neural information processing systems, pp. 295–301,
1997.

Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Recht, Benjamin,
and Vinyals, Oriol. Understanding deep learning requires
rethinking generalization. arXiv preprint arXiv:1611.03530,
2016.

Zhang, Tong. Solving large scale linear prediction problems using
stochastic gradient descent algorithms. In Proceedings of the
twenty-ﬁrst international conference on Machine learning, pp.
116. ACM, 2004.

Zinkevich, Martin. Online convex programming and generalized
inﬁnitesimal gradient ascent. In Machine Learning, Proceed-
ings of the Twentieth International Conference, pp. 928–936,
2003.

