The Price of Differential Privacy for Online Learning
(with Supplementary Material)

Naman Agarwal 1 Karan Singh 1

Abstract

√

We design differentially private algorithms for
the problem of online linear optimization in the
full information and bandit settings with optimal
˜O(
T )1 regret bounds. In the full-information
setting, our results demonstrate that ε-differential
privacy may be ensured for free – in particular,
(cid:1). For
the regret bounds scale as O(
bandit linear optimization, and as a special case,
for non-stochastic multi-armed bandits, the pro-
(cid:17)
posed algorithm achieves a regret of ˜O
,
while the previously known best regret bound
was ˜O

T ) + ˜O (cid:0) 1

(cid:16) 1
ε

√

√

(cid:17)

T

ε

.

(cid:16) 1
ε T 2

3

1. Introduction

In the paradigm of online learning, a learning algorithm
makes a sequence of predictions given the (possibly in-
complete) knowledge of the correct answers for the past
queries.
In contrast to statistical learning, online learn-
ing algorithms typically offer distribution-free guarantees.
Consequently, online learning algorithms are well suited
to dynamic and adversarial environments, where real-time
learning from changing data is essential making them ubiq-
uitous in practical applications such as servicing search ad-
vertisements. In these settings often these algorithms inter-
act with sensitive user data, making privacy a natural con-
cern for these algorithms. A natural notion of privacy in
such settings is differential privacy (Dwork et al., 2006)
which ensures that the outputs of an algorithm are indis-
tinguishable in the case when a user’s data is present as
opposed to when it is absent in a dataset.

In this paper, we design differentially private algorithms for

*Equal contribution

1Computer Science, Princeton Uni-
versity, Princeton, NJ, USA. Correspondence
Na-
man Agarwal <namana@cs.princeton.edu>, Karan Singh
<karans@cs.princeton.edu>.

to:

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1Here the ˜O(·) notation hides polylog(T ) factors.

online linear optimization with near-optimal regret, both in
the full information and partial information (bandit) set-
tings. This result improves the known best regret bounds
for a number of important online learning problems – in-
cluding prediction from expert advice and non-stochastic
multi-armed bandits.

1.1. Full-Information Setting: Privacy for Free

ε

T

(cid:17)

(cid:17)

√

(cid:16)√

(cid:16) 1
ε

+ ˜O (cid:0) 1

For the full-information setting where the algorithm gets
to see the complete loss vector every round, we design
ε-differentially private algorithms with regret bounds that
(cid:1) (Theorem 3.1), partially resolv-
scale as O
ing an open question to improve the previously best known
bound of O
posed in (Smith & Thakurta, 2013).
A decomposition of the bound on the regret bound of this
form implies that when ε ≥ 1√
, the regret incurred by
T
the differentially private algorithm matches the optimal re-
gret in the non-private setting, i.e. differential privacy is
free. Moreover even when ε ≤ 1√
, our results guarantee
T
a sub-constant regret per round in contrast to the vacuous
constant regret per round guaranteed by existing results.

T

√

Concretely, consider the case of online linear optimization
over the cube, with unit l∞-norm-bounded loss vectors. In
this setting, (Smith & Thakurta, 2013) achieves a regret
bound of O( 1
N T ), which is meaningful only if T ≥ N
ε2 .
ε
Our theorems imply a regret bound of ˜O(
ε ). This
is an improvement on the previous bound regardless of the
value of ε. Furthermore, when T is between N
ε2 , the
previous bounds are vacuous whereas our results are still
meaningful. Note that the above arguments show an im-
provement over existing results even for moderate value of
ε. Indeed, when ε is very small, the magnitude of improve-
ments are more pronounced.

ε and N

N T + N

√

Beyond the separation between T and ε, the key point of
our analysis is that we obtain bounds for general regular-
ization based algorithms which adapt to the geometry of
the underlying problem optimally, unlike the previous algo-
rithms (Smith & Thakurta, 2013) which utilizes euclidean
regularization. This allows our results to get rid of a poly-
T term) in some cases.
nomial dependence on N (in the

√

The Price of Differential Privacy for Online Learning

Online linear optimization over the sphere and prediction
with expert advice are notable examples.

We summarize our results in Table 1.1.

1.2. Bandits: Reduction to the Non-private Setting

In the partial-information (bandit) setting, the online learn-
ing algorithm only gets to observe the loss of the predic-
tion it prescribed. We outline a reduction technique that
translates a non-private bandit algorithm to a differentially
private bandit algorithm, while retaining the ˜O(
T ) de-
pendency of the regret bound on the number of rounds of
play (Theorem 4.5). This allows us to derive the ﬁrst ε-
differentially private algorithm for bandit linear optimiza-
tion achieving ˜O(
T ) regret, using the algorithm for the
non-private setting from (Abernethy et al., 2012). This an-
swers a question from (Smith & Thakurta, 2013) asking if
˜O(
T ) regret is attainable for differentially private linear
bandits .

√

√

√

An important case of the general bandit linear optimization
framework is the non-stochastic multi-armed bandits prob-
lem(Bubeck et al., 2012b), with applications for website
optimization, personalized medicine, advertisement place-
ment and recommendation systems. Here, we propose
an ε-differentially private algorithm which enjoys a re-
gret of ˜O( 1
N T log N ) (Theorem 4.1), improving on the
ε
previously best attainable regret of ˜O( 1
3 )(Smith &
Thakurta, 2013).

ε N T 2

√

We summarize our results in Table 1.2.

1.3. Related Work

The problem of differentially private online learning was
ﬁrst considered in (Dwork et al., 2010), albeit guarantee-
ing privacy in a weaker setting – ensuring the privacy of
the individual entries of the loss vectors.
(Dwork et al.,
2010) also introduced the tree-based aggregation scheme
for releasing the cumulative sums of vectors in a differen-
tially private manner, while ensuring that the total amount
of noise added for each cumulative sum is only poly-
logarithmically dependent on the number of vectors. The
stronger notion of privacy protecting entire loss vectors was
ﬁrst studied in (Jain et al., 2012), where gradient-based al-
gorithms were proposed that achieve (ε, δ)-differntial pri-
vacy and regret bounds of ˜O
(Smith &
Thakurta, 2013) proposed a modiﬁcation of Follow-the-
ε log2.5 T (cid:1)
Approximate-Leader template to achieve ˜O (cid:0) 1
regret for strongly convex loss functions, implying a regret
bound of ˜O
for general convex functions. In addi-
tion, they also demonstrated that under bandit feedback, it
(cid:17)
is possible to obtain regret bounds that scale as ˜O
.
(Dwork et al., 2014a; Jain & Thakurta, 2014) proved that

(cid:16) 1
ε T 2

T log 1
δ

(cid:16) 1
ε

(cid:16) 1
ε

√

√

(cid:17)

(cid:17)

T

.

3

ε

√

in the special case of prediction with expert advice setting,
T log N (cid:1). While
it is possible to achieve a regret of O (cid:0) 1
most algorithms for differentially private online learning
are based on the regularization template, (Dwork et al.,
2014b) used a perturbation-based algorithm to guarantee
(ε, δ)-differential privacy for the problem of online PCA.
(Tossou & Dimitrakakis, 2016) showed that it is possible
to design ε-differentially private algorithms for the stochas-
tic multi-armed bandit problem with a separation of ε, T
for the regret bound. Recently, an independent work due
to (Tossou & Dimitrakakis, 2017), which we were made
aware of after the ﬁrst manuscript, also demonstrated a
˜O
regret bound in the non-stochastic multi-armed
bandits setting. We match their results (Theorem 4.1), as
well as provide a generalization to arbitrary convex sets
(Theorem 4.5).

(cid:16) 1
ε

√

(cid:17)

T

1.4. Overview of Our Techniques

Information Setting: We consider

Full
the two
well known paradigms for online learning, Folllow-the-
Regularized-Leader (FTRL) and Folllow-the-Perturbed-
Leader (FTPL). In both cases, we ensure differential pri-
vacy by restricting the mode of access to the inputs (the
loss vectors). In particular, the algorithm can only retrieve
estimates of the loss vectors released by a tree based ag-
gregation protocol (Algorithm 2) which is a slight modiﬁ-
cation of the protocol used in (Jain et al., 2012; Smith &
Thakurta, 2013). We outline a tighter analysis of the regret
minimization framework by crucially observing that in case
of linear losses, the expected regret of an algorithm that
injects identically (though not necessarily independently)
distributed noise per step is the same as one that injects a
single copy of the noise at the very start of the algorithm.

The regret analysis of Follow-the-Leader based algorithm
involves two components, a bias term due to the regular-
ization and a stability term which bounds the change in the
output of the algorithm per step.
In the analysis due to
(Smith & Thakurta, 2013), the stability term is affected by
the variance of the noise as it changes from step to step.
However in our analysis, since we treat the noise to have
been sampled just once, the stability analysis does not fac-
tor in the variance and the magnitude of the noise essen-
tially appears as an additive term in the bias.

Bandit Feedback: In the bandit feedback setting, we show
a general reduction that takes a non-private algorithm and
outputs a private algorithm (Algorithm 4). Our key ob-
servation here (presented as Lemma 4.3) is that on linear
functions, in expectation the regret of an algorithm on a
noisy sequence of loss vectors is the same as its regret on
the original loss sequence as long as noise is zero mean.
We now bound the regret on the noisy sequence by condi-
tioning out the case when the noise can be large and us-

The Price of Differential Privacy for Online Learning

FUNCTION CLASS (N DIMENSIONS)

OUR REGRET BOUND

NON-

PREVIOUS
KNOWN REGRET

BEST

PREDICTION WITH EXPERT ADVICE

(cid:16) √

˜O

T log N
ε

(cid:17)

(cid:16)√

O

T log N + N log N log2 T

(cid:17)

O(

T log N )

BEST
PRIVATE
REGRET
√

√

O(

T )

√

O(

N T )

√

O(

T )

ε

(cid:17)

(cid:16)√

O

T + N log2 T

ε

(cid:16)√

O

N T + N log2 T

ε

(cid:17)

(cid:16)√

O

T + log2 T

ε

(cid:17)

ONLINE LINEAR OPTIMIZATION OVER
THE SPHERE

ONLINE LINEAR OPTIMIZATION OVER
THE CUBE

ONLINE LINEAR OPTIMIZATION

˜O

˜O

(cid:16) √

(cid:16) √

(cid:17)

(cid:17)

N T
ε

N T
ε

˜O

(cid:16) √
T
ε

(cid:17)

Table 1. Summary of our results in the full-information setting. In the last row we suppress the constants depending upon X , Y.

FUNCTION CLASS (N DIMENSIONS)

OUR REGRET BOUND

PREVIOUS
KNOWN REGRET

BEST

BANDIT LINEAR OPTIMIZATION

NON-STOCHASTIC
BANDITS

MULT-ARMED

(cid:18)

(cid:19)

T

2
3
ε

˜O

(cid:18)

˜O

N T
ε

(cid:19)

2
3

˜O

(cid:16) √
T
ε

(cid:17)

(cid:16) √

˜O

T N log N
ε

(cid:17)

NON-

BEST
PRIVATE
REGRET

√

O(

T )

√

O(

N T )

Table 2. Summary of our results in the bandit setting. In the ﬁrst row we suppress the speciﬁc constants depending upon X , Y.

ing exploration techniques from (Bubeck et al., 2012a) and
(Abernethy et al., 2008).

properties of the speciﬁc decision set X and loss set Y.
(See (Hazan et al., 2016) for a survey of results.)

2. Model and Preliminaries

This section introduces the model of online (linear) learn-
ing, the distinction between full and partial feedback sce-
narios, and the notion of differential privacy in this model.

Full-Information Setting: Online linear optimization
(Hazan et al., 2016; Shalev-Shwartz, 2011) involves re-
peated decision making over T rounds of play. At the be-
ginning of every round (say round t), the algorithm chooses
a point in xt ∈ X , where X ⊆ RN is a (compact) convex
set. Subsequently, it observes the loss lt ∈ Y ⊆ RN and
suffers a loss of (cid:104)lt, xt(cid:105). The success of such an algorithm,
across T rounds of play, is measured though regret, which
is deﬁned as

Regret = E

(cid:104)lt, xt(cid:105) − min
x∈K

(cid:20) T

(cid:88)

t=1

T
(cid:88)

(cid:21)
(cid:104)lt, x(cid:105)

t=1

where the expectation is over the randomness of the algo-
In particular, achieving a sub-linear regret (o(T ))
rithm.
corresponds to doing almost as good (averaging across T
rounds) as the ﬁxed decision with the least loss in hind-
sight. In the non-private setting, a number of algorithms
have been devised to achieve O(
T ) regret, with addi-
tional dependencies on other parameters dependent on the

√

Following are three important instantiations of the above
framework.

• Prediction with Expert Advice: Here the underlying
decision set is the simplex X = ∆N = {x ∈ Rn :
xi ≥ 0, (cid:80)n
i=1 xi = 1} and the loss vectors are con-
strained to the unit cube Y = {lt ∈ RN : (cid:107)lt(cid:107)∞ ≤ 1}.

• OLO over the Sphere: Here the underlying decision is
the euclidean ball X = {x ∈ Rn : (cid:107)x(cid:107)2 ≤ 1} and the
loss vectors are constrained to the unit euclidean ball
Y = {lt ∈ RN : (cid:107)lt(cid:107)2 ≤ 1}.

• OLO over the Cube: The decision is the unit cube
X = {x ∈ Rn : (cid:107)x(cid:107)∞ ≤ 1}, while the loss vectors
are constrained to the set Y = {lt ∈ RN : (cid:107)lt(cid:107)1 ≤ 1}.

Partial-Information Setting: In the setting of bandit feed-
back, the critical difference is that the algorithm only gets
to observe the value (cid:104)lt, xt(cid:105), in contrast to the complete
loss vector lt ∈ RN as in the full information scenario.
Therefore, the only feedback the algorithm receives is the
value of the loss it incurs for the decision it takes. This
makes designing algorithms for this feedback model chal-
lenging. Nevertheless for the general problem of bandit
linear optimization, (Abernethy et al., 2008) introduced a

The Price of Differential Privacy for Online Learning

computationally efﬁcient algorithm that achieves an opti-
T ) on the
mal dependence of the incurred regret of O(
number of rounds of play. The non-stochastic multi-armed
bandit (Auer et al., 2002) problem is the bandit version of
the prediction with expert advice framework.

√

Differential Privacy: Differential Privacy (Dwork et al.,
2006) is a rigorous framework for establishing guarantees
on privacy loss, that admits a number of desirable prop-
erties such as graceful degradation of guarantees under
composition and robustness to linkage acts (Dwork et al.,
2014a).

Deﬁnition 2.1 ((ε, δ)-Differential Privacy). A randomized
online learning algorithm A on the action set X and the
loss set Y is (ε, δ)-differentially private if for any two se-
quence of loss vectors L = (l1, . . . lT ) ⊆ Y T and L(cid:48) =
T ) ⊆ Y T differing in at most one vector – that is to
(l(cid:48)
t – for all S ⊆ X T ,
say ∃t0 ∈ [T ], ∀t ∈ [T ] − {t0}, lt = l(cid:48)
it holds that

1, . . . l(cid:48)

P(A(L) ∈ S) ≤ eεP(A(L(cid:48)) ∈ S) + δ

Remark 2.2. The above deﬁnition of Differential Privacy
is speciﬁc to the online learning scenario in the sense that
it assumes the change of a complete loss vector. This has
been the standard notion considered earlier in (Jain et al.,
2012; Smith & Thakurta, 2013). Note that the deﬁnition
entails that the entire sequence of predictions produced by
the algorithm is differentially private.

:

lt ∈
Notation: We deﬁne (cid:107)Y(cid:107)p = max{(cid:107)lt(cid:107)p
Y}, (cid:107)X (cid:107)p = max{(cid:107)x(cid:107)p
: x ∈ X }, and M =
maxl∈Y,x∈X |(cid:104)l, x(cid:105)|, where (cid:107) · (cid:107)p is the lp norm. By
Holder’s inequality, it is easy to see that M ≤ (cid:107)Y(cid:107)p(cid:107)X (cid:107)q
for all p, q ≥ 1 with 1
q = 1. We deﬁne the distri-
bution LapN (λ) to be the distribution over RN such that
each coordinate is drawn independently from the Laplace
distribution with parameter λ.

p + 1

3. Full-Information Setting: Privacy for Free

In this section, we describe an algorithmic template (Algo-
rithm 1) for differentially private online linear optimiza-
tion, based on Follow-the-Regularized-Leader scheme.
Subsequently, we outline the noise injection scheme (Al-
gorithm 2), based on the Tree-based Aggregation Protocol
(Dwork et al., 2010), used as a subroutine by Algorithm 1
to ensure input differential privacy. The following is our
main theorem in this setting.

Theorem 3.1. Algorithm 1 when run with D = LapN (λ)
where λ = (cid:107)Y(cid:107)1 log T
, regularization R(x), decision set
X and loss vectors l1, . . . lt, the regret of Algorithm 1 is

ε

bounded by

(cid:118)
(cid:117)
(cid:117)
(cid:116)DR

Regret ≤

where

T
(cid:88)

t=1

(cid:20)

max
x∈X

((cid:107)lt(cid:107)∗

∇2R(x))2 + DLap

DLap = EZ∼D(cid:48)

max
x∈X

(cid:104)Z, x(cid:105) − min
x∈X

(cid:104)Z, x(cid:105)

(cid:21)

DR = max
x∈X

R(x) − min
x∈X

R(x)

and D(cid:48) is the distribution induced by the sum of (cid:100)log T (cid:101)
independent samples from D, (cid:107) · (cid:107)∗
∇2R(x) represents the
dual of the norm with respect to the hessian of R. Moreover,
the algorithm is ε-differentially private, i.e.
the sequence
of predictions produced (xt : t ∈ [T ]) is ε-differentially
private.

Algorithm 1 FTRL Template for OLO
input Noise distribution D, Regularization R(x)
1: Initialize an empty binary tree B to compute differen-

s=1 ls.

independently from D.

tially private estimates of (cid:80)t
0, . . . n(cid:100)log T (cid:101)
2: Sample n1
0
3: ˜L0 ← (cid:80)(cid:100)log T (cid:101)
ni
0.
4: for t = 1 to T do
5:

i=1

Choose xt = argminx∈X
Observe lt ∈ Y, and suffer a loss of (cid:104)lt, xt(cid:105).
( ˜Lt, B) ← TreeBasedAgg(lt, B, t, D, T ).

(cid:16)

(cid:17)
η(cid:104)x, ˜Lt−1(cid:105) + R(x)

.

6:
7:
8: end for

The above theorem leads to following corollary where we
show the bounds obtained in speciﬁc instantiations of on-
line linear optimization.
Corollary 3.2. Substituting the choices of λ, R(x) listed
below, we specify the regret bounds in each case.

1. Prediction with Expert Advice:
and R(x) = (cid:80)N
(cid:32)

N log T
ε

i=1 xi log(xi),

Regret ≤ O

(cid:112)T log N +

(cid:33)

N log2 T log N
ε

Choosing λ =

2. OLO over the Sphere Choosing λ =

R(x) = (cid:107)x(cid:107)2
2

√

N log T
ε

and

Regret ≤ O

T +

(cid:32)√

(cid:33)

N log2 T
ε

3. OLO over the Cube With λ = log T
ε

and R(x) = (cid:107)x(cid:107)2
2

(cid:32)√

Regret ≤ O

N T +

(cid:33)

N log2 T
ε

The Price of Differential Privacy for Online Learning

Algorithm 2 TreeBasedAgg(lt, B, t, D, T )
input Loss vector lt, Binary tree B, Round t, Noise distri-

bution D, Time horizon T

1: ( ˜L(cid:48)

t, B) ← PrivateSum(lt, B, t, D, T ) – Algo-
rithm 5 ((Jain et al., 2012)) with the noise added at
each node – be it internal or leaf – sampled indepen-
dently from the distribution D.

B that can compute (cid:80)t

2: st ← the binary representation of t as a string.
3: Find the minimum set S of already populated nodes in
s=1 ls.
4: Deﬁne Q = |S| ≤ (cid:100)log T (cid:101). Deﬁne rt = (cid:100)log T (cid:101) − Q.
5: Sample n1
6: ˜Lt ← ˜L(cid:48)
output ( ˜Lt, B).

t , . . . nrt
t
t + (cid:80)rt
i=1 ni
t.

independently from D.

3.1. Proof of Theorem 3.1

We ﬁrst prove the privacy guarantee, and then prove the
claimed bound on the regret. For the analysis, we deﬁne
the random variable Zt to be the net amount of noise in-
jected by the TreeBasedAggregation (Algorithm 2) on the
true partial sums. Formally, Zt is the difference between
cumulative sum of loss vectors and its differentially private
estimate used as input to the arg-min oracle.

Zt = ˜Lt −

t
(cid:88)

i=1

li

Further, let D(cid:48) be the distribution induced by summing of
(cid:100)log T (cid:101) independent samples from D.

Privacy : To make formal claims about the quality of
privacy, we ensure input differential privacy for the algo-
rithm – that is, we ensure that the entire sequence of par-
tial sums of the loss vectors ((cid:80)t
s=1 ls : t ∈ [T ]) is ε-
differentially private. Since the outputs of Algorithm 1 are
strictly determined by the preﬁx sum estimates produced
by TreeBasedAgg, by the post-processing theorem, this
certiﬁes that the entire sequence of choices made by the
algorithm (across all T rounds of play) (xt : t ∈ [T ]) is ε-
differentially private. We modify the standard Tree-based
Aggregation protocol to make sure that the noise on each
output (partial sum) is distributed identically (though not
necessarily independently) across time. While this modiﬁ-
cation is not essential for ensuring privacy, it simpliﬁes the
regret analysis.

Lemma 3.3 (Privacy Guarantees with Laplacian Noise).
Choose any λ ≥ (cid:107)Y(cid:107)1 log T
. When Algorithm 2 A(D, T )
is run with D = LapN (λ), the following claims hold true:

ε

• Privacy: The sequence ( ˜Lt

:

t ∈ [T ]) is ε-

differentially private.

• Distribution: ∀t ∈ [T ], Zt ∼ (cid:80)(cid:100)log T (cid:101)

ni, where

i=1

each ni is independently sampled from LapN (λ).

Proof. By Theorem 9 ((Jain et al., 2012)), we have that the
sequence ( ˜L(cid:48)
t : t ∈ [T ]) is ε-differentially private. Now the
sequence ( ˜Lt : t ∈ [T ]) is ε-differentially private because
differential privacy is immune to post-processing(Dwork
et al., 2014a).

Note that the PrivateSum algorithm adds exactly |S| in-
dependent draws from the distribution D to (cid:80)t
s=1 ls, where
S is the minimum set of already populated nodes in the tree
that can compute the required preﬁx sum. Due to Line 6 in
Algorithm 2, it is made certain that every preﬁx sum re-
leased is a sum of the true preﬁx sum and (cid:100)log T (cid:101) indepen-
dent draws from D.

Regret Analysis: In this section, we show that for lin-
ear loss functions any instantiation of the Follow-the-
Regularized-Leader algorithm can be made differentially
private with an additive loss in regret.
Theorem 3.4. For any noise distribution D, regularization
R(x), decision set X and loss vectors l1, . . . lt, the regret
of Algorithm 1 is bounded by

Regret ≤

max
x∈X

((cid:107)lt(cid:107)∗

∇2R(x))2 + DD(cid:48)

(cid:118)
(cid:117)
(cid:117)
(cid:116)DR

T
(cid:88)

t=1

where DD(cid:48) = EZ∼D(cid:48) [maxx∈X (cid:104)Z, x(cid:105) − minx∈X (cid:104)Z, x(cid:105)],
DR = maxx∈X R(x) − minx∈X R(x), and (cid:107) · (cid:107)∗
∇2R(x)
represents the dual of the norm with respect to the hessian
of R.

Proof. To analyze the regret suffered by Algorithm 1, we
consider an alternative algorithm that performs a one-shot
noise injection – this alternate algorithm may not be dif-
ferentially private. The observation here is that the alter-
nate algorithm and Algorithm 1 suffer the same loss in ex-
pectation and therefore the same expected regret which we
bound in the analysis below.

Consider the following alternate algorithm which instead
of sampling noise Zt at each step instead samples noise at
the beginning of the algorithm and plays with respect to
that. Formally consider the sequence of iterates ˆxt deﬁned
as follows. Let Z ∼ D.

ˆx1 (cid:44) x1,

ˆxt (cid:44) argminx∈X η(cid:104)x, Z +

li(cid:105) + R(x)

We have that

EZ1...ZT ∼D

(cid:104)lt, xt(cid:105)

= EZ∼D

(cid:104)lt, ˆxt(cid:105)

(1)

(cid:35)

(cid:35)

(cid:34) T

(cid:88)

t=1

To see the above equation note that EZt∼D [(cid:104)lt, ˆxt(cid:105)] =
EZ∼D [(cid:104)lt, xt(cid:105)] since x, ˆxt have the same distribution.

(cid:88)

i

(cid:34) T

(cid:88)

t=1

The Price of Differential Privacy for Online Learning

Therefore it is sufﬁcient to bound the regret of the sequence
ˆx1 . . . ˆxt. The key idea now is to notice that the addition
of one shot noise does not affect the stability term of the
FTRL analysis and therefore the effect of the noise need
not be paid at every time step. Our proof will follow the
standard template of using the FTL-BTL (Kalai & Vem-
pala, 2005) lemma and then bounding the stability term in
the standard way. Formally deﬁne the augmented series of
loss functions by deﬁning

l0(x) =

R(x) + (cid:104)Z, x(cid:105)

1
η

where Z ∼ D is a sample. Now invoking the Follow the
Leader, Be the Leader Lemma (Lemma 5.3, (Hazan et al.,
2016)) we get that for any ﬁxed u ∈ X

T
(cid:88)

t=0

T
(cid:88)

t=0

lt(u) ≥

lt(ˆxt+1)

Therefore we can conclude that

[lt(ˆxt) − lt(u)]

(2)

≤

[lt(ˆxt) − lt(ˆxt+1)] + l0(u) − l0(ˆx1)

≤

[lt(ˆxt) − lt(ˆxt+1)] +

DR + DZ

(3)

1
η

where DZ (cid:44) maxx∈X ((cid:104)Z, x(cid:105)) − minx∈X ((cid:104)Z, x(cid:105)) There-
fore we now need to bound the stability term lt(ˆxt) −
lt(ˆxt+1). Now, the regret bound follows from the standard
analysis for the stability term in the FTRL scheme (see for
instance (Hazan et al., 2016)). Notice that the bound only
depends on the change in the cumulative loss per step i.e.
η ((cid:80)
t lt + Z), for which the change is the loss vector ηlt+1
across time steps. Therefore we get that

lt(ˆxt) − lt(ˆxt+1) ≤ max
x∈X

(cid:107)ηlt(cid:107)2

η∇−2R(x)

(4)

Combining Equations (1), (3), (4) we get the regret bound
in Theorem 3.4.

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

3.2. Regret Bounds for FTPL

In this section, we outline algorithms based on the Follow-
the-Perturbed-Leader template(Kalai & Vempala, 2005).
FTPL-based algorithms ensure low-regret by perturbing the
cumulative sum of loss vectors with noise from a suit-
ably chosen distribution. We show that the noise added in
the process of FTPL is sufﬁcient to ensure differential pri-
vacy. More concretely, using the regret guarantees due to

√

ε log 1

T )+ ˜O( 1

(Abernethy et al., 2014), for the full-information setting,
we establish that the regret guarantees obtained scale as
δ ). While Theorem 3.5 is valid for all in-
O(
T )
stances of online linear optimization and achieves O(
regret, it yields sub-optimal dependence on the dimension
of the problem. The advantage of FTPL-based approaches
over FTRL is that FTPL performs linear optimization over
the decision set every round, which is possibly computa-
tionally less expensive than solving a convex program ev-
ery round, as FTRL requires.

√

Algorithm 3 FTPL Template for OLO – A(D, T ) on the
action set X , the loss set Y.
1: Initialize an empty binary tree B to compute differen-

s=1 ls.

independently from D.

i=1

tially private estimates of (cid:80)t
0, . . . n(cid:100)log T (cid:101)
2: Sample n1
0
3: ˜L0 ← (cid:80)(cid:100)log T (cid:101)
ni
0.
4: for t = 1 to T do
5:
6:
7:
8: end for

Choose xt = argminx∈X (cid:104)x, ˜Lt−1(cid:105).
Observe the loss vector lt ∈ Y, and suffer (cid:104)lt, xt(cid:105).
( ˜Lt, B) ← TreeBasedAgg(lt, B, t, D, T ).

Theorem 3.5 (FTPL: Online Linear Optimization). Let
(cid:107)X (cid:107)2 = supx∈X (cid:107)x(cid:107)2 and (cid:107)Y(cid:107)2 = suplt∈Y (cid:107)lt(cid:107)2. Choos-
(cid:113) T√
log T log log T
δ } and
ing σ = max{(cid:107)Y(cid:107)2
D = N (0, σ2IN ), we have that RegretA(D,T )(T ) is

N log T

N
ε

√

,

(cid:32)

1

√

O

N

4 (cid:107)X (cid:107)2(cid:107)Y(cid:107)2

T +

N (cid:107)X (cid:107)2
ε

log1.5 T log

(cid:33)

log T
δ

Moreover the algorithm is ε-differentially private.

The proof of the theorem is deferred to the appendix.

4. Differentially Private Multi-Armed Bandits

In this section, we state our main results regarding bandit
linear optimization, the algorithms that achieve it and prove
the associated regret bounds. The following is our main
theorem concerning non-stochastic multi-armed bandits.

Theorem 4.1 (Differentially Private Multi-Armed Ban-
dits). Fix loss vectors (l1 . . . lT ) such that (cid:107)lt(cid:107)∞ ≤ 1.
When Algorithm 4 is run with parameters D = LapN (λ)
where λ = 1
ε and algorithm A = Algorithm 5 with
(cid:113)
log N
2N T (1+2λ2 log N T ) ,
the following parameters: η =
γ = ηN (cid:112)1 + 2λ2 log N T and the exploration distribu-
tion µ(i) = 1
N . The regret of the Algorithm 4 is

(cid:18) √

O

N T log T log N
ε

(cid:19)

Moreover, Algorithm 4 is ε-differentially private

The Price of Differential Privacy for Online Learning

Bandit Feedback: Reduction to the Non-private Setting

We begin by describing an algorithmic reduction that takes
as input a non-private bandit algorithm and translates it into
an ε-differentially private bandit algorithm. The reduction
works in a straight-forward manner by adding the requisite
magnitude of Laplace noise to ensure differential privacy.
For the rest of this section, for ease of exposition we will
assume that both T and N are sufﬁciently large.

Algorithm 4 A(cid:48)(A, D) – Reduction to the Non-private Set-
ting for Bandit Feedback
Input: Online Algorithm A, Noise Distribution D.
1: for t = 0 to T do
2:
3:
4:
5:
6: end for

Receive ˜xt ∈ X from A and output ˜xt.
Receive a loss value (cid:104)lt, ˜xt(cid:105) from the adversary.
Sample Zt ∼ D.
Forward (cid:104)lt ˜xt(cid:105) + (cid:104)Zt, ˜xt(cid:105) as input to A.

N

N . . . 1

(cid:1) ∈ RN .

Algorithm 5 EXP2 with exploration µ
Input: learning rate η; mixing coefﬁcient γ; distribution µ
1: q1 = (cid:0) 1
2: for t = 1,2 . . . T do
3:
4:

Let pt = (1 − γ)qt + γµ and play it ∼ pt
Estimate loss vector lt by ˜lt = P +
(cid:3)
(cid:2)eieT
Pt = Ei∼pt
i
Update the exponential weights,

t eiteT
it

lt, with

5:

qt+1(i) =

e−η(cid:104)ei,˜lt(cid:105)qt(i)
i(cid:48) e−η(cid:104)ei(cid:48) ,˜lt(cid:105)qt(i(cid:48))

(cid:80)

6: end for

The following Lemma characterizes the conditions under
which Algorithm 4 is ε differentially private
Lemma 4.2 (Privacy Guarantees). Assume that each
is in the set Y ⊆ RN ,
such that
loss vector lt
maxt,l∈Y | (cid:104)l,˜xt(cid:105)
| ≤ B. For D = LapN (λ) where λ = B
ε ,
(cid:107)˜xt(cid:107)∞
the sequence of outputs ( ˜xt : t ∈ [T ]) produced by the
Algorithm A(cid:48)(A, D) is ε-differentially private.

The following lemma charaterizes the regret of Algorithm
4.
In particular we show that the regret of Algorithm 4
is, in expectation, same as that of the regret of the input
algorithm A on a perturbed version of loss vectors.
Lemma 4.3 (Noisy Online Optimization). Consider a loss
sequence (l1 . . . lT ) and a convex set X . Deﬁne a perturbed
version of the sequence as random vectors (˜lt : t ∈ [T ])
as ˜lt = lt + Zt where Zt is a random vector such that
{Z1, . . . Zt} are independent and E[Zt] = 0 for all t ∈ [T ].

Let A be a full information (or bandit) online algorithm
which outputs a sequence (˜xt ∈ X : t ∈ [T ]) and takes as

input ˜lt (respectively (cid:104)˜lt, ˜xt(cid:105)) at time t. Let x∗ ∈ K be a
ﬁxed point in the convex set. Then we have that

(cid:34)
EA

(cid:34) T

(cid:88)

E{Zt}

((cid:104)lt, ˜xt(cid:105) − (cid:104)lt, x∗(cid:105))

(cid:35)(cid:35)

(cid:34)

= E{Zt}

EA

(cid:16)

(cid:17)
(cid:104)˜lt.˜xt(cid:105) − (cid:104)˜lt, x∗(cid:105)

(cid:35)(cid:35)

t=1

(cid:34) T

(cid:88)

t=1

We provide the proof of Lemma 4.2 and defer the proof of
Lemma 4.3 to the Appendix Section B.

Proof of Lemma 4.2. Consider a pair of sequence of loss
vectors that differ at exactly one time step – say L =
(l1, . . . lt0 . . . , lT ) and L(cid:48) = (l1, . . . , l(cid:48)
, . . . lT ). Since
t0
the prediction of produced by the algorithm at time step
any time t can only depend on the loss vectors in the past
(l1, . . . lt−1), it is clear that the distribution of the output
of the algorithm for the ﬁrst t0 rounds (˜x1, . . . ˜xt0) is unal-
tered. We claim that ∀I ⊆ R, it holds that

P((cid:104)lt0 + Zt0 , ˜xt0 (cid:105) ∈ I) ≤ eεP((cid:104)l(cid:48)
t0

+ Zt0, ˜xt0(cid:105) ∈ I)

Before we justify the claim, let us see how this implies that
desired statement. To see this, note that conditioned on the
value fed to the inner algorithm A at time t0, the distri-
bution of all outputs produced by the algorithm are com-
pletely determined since the feedback to the algorithm at
other time steps (discounting t0) stays the same (in distri-
bution). By the above discussion, it is sufﬁcient to demon-
strate ε-differential privacy for each input fed (as feedback)
to the algorithm A.

t

t

t

t

˜xi

= lF ict
t

)i = (cid:104)lt, ˜xt(cid:105)

= 0 ∈ RN . Else, deﬁne lF ict

For the sake of analysis, deﬁne lF ict
as follows. If ˜xt = 0,
∈ RN to be such
deﬁne lF ict
that (lF ict
if and only if i = argmaxi∈[d]|˜xi|
and 0 otherwise, where argmax breaks ties arbitrarily. De-
ﬁne ˜lF ict
, ˜xt(cid:105) =
t
(cid:104)lt ˜xt(cid:105) + (cid:104)Zt, ˜xt(cid:105).
It sufﬁces to establish that each ˜lF ict
is ε-differentially
private. To argue for this, note that Laplace mechanism
(Dwork et al., 2014a) ensures the same, since the l1 norm
of ˜lF ict
t

+ Zt. Now note that (cid:104)˜lF ict

is bounded by B.

t

t

4.1. Proof of Theorem 4.1
Privacy: Note that since maxt,l∈Y | (cid:104)l,˜xt(cid:105)
| ≤ (cid:107)Y(cid:107)∞ ≤ 1
(cid:107)˜xt(cid:107)∞
as ˜xt ∈ {ei : i ∈ [N ]}. Therefore by Lemma 4.2, setting
λ = 1

ε is sufﬁcient to ensure ε-differential privacy.

Regret Analysis: For the purpose of analysis we deﬁne the
following pseudo loss vectors.

˜lt = lt + Zt

The Price of Differential Privacy for Online Learning

where by deﬁnition Zt ∼ LapN (λ). The following follows
from Fact C.1 proved in the appendix.

which holds by the choice of these parameters. Finally

P(∃t (cid:107)Zt(cid:107)2

∞ ≥ 10λ2 log2 N T ) ≤

(5)

≤ E{Zt}

+ η

N (cid:107)˜lt(cid:107)2

∞ + 2T γ

P((cid:107)Zt(cid:107)2

∞ ≥ 10λ2 log2 N T ) ≤

Taking a union bound, we have

1
T 2

1
T

To bound the norm of the loss we deﬁne the event F (cid:44)
∞ ≥ 10λ2 log2 N T }. We have from (5) that
{∃t : (cid:107)Zt(cid:107)2
P(F ) ≤ 1

T . We now have that

E[Regret] ≤ E[Regret| ¯F ] + P(F )E[Regret|F ]

Since the regret is always bounded by T we get that the
second term above is at most 1. Therefore we will concern
ourselves with bounding the ﬁrst term above. Note that
Zt remains independent and symmetric even when condi-
tioned on the event ¯F . Moreover the following statements
also hold.

∀t E[Zt| ¯F ] = 0

∀t E[(cid:107)Zt(cid:107)2

∞| ¯F ] ≤ 10λ2 log2 N T

(6)

(7)

Equation (6) follows by noting that Zt remains symmet-
ric around the origin even after conditioning. It can now
be seen that Lemma 4.3 still applies even when the noise
is sampled from LapN (λ) conditioned under the event ¯F
(due to Equation 6). Therefore we have that

E[Regret| ¯F ] = E{Zt}

(cid:16)

(cid:17)
(cid:104)˜lt, ˜xt(cid:105) − (cid:104)˜lt, x∗(cid:105)

(cid:34)
EA

(cid:34) T

(cid:88)

t=1

(cid:35)

¯F

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(8)

To bound the above quantity we make use of the following
lemma which is a specialization of Theorem 1 in (Bubeck
et al., 2012a) to the case of multi-armed bandits.

Lemma 4.4 (Regret Guarantee for Algorithm 5). If η is
such that η|(cid:104)ei, ˜lt(cid:105)| ≤ 1, we have that the regret of Algo-
rithm 5 is bounded by

Regret ≤ 2γT +

log N
η

+ ηE

(cid:20) (cid:88)

(cid:88)

(cid:21)

pt(i)(cid:104)ei, ˜lt(cid:105)2

t

i

Now note that due to the conditioning (cid:107)Zt(cid:107)2
10λ2 log2 N T and therefore we have that

∞ ≤

maxt,x∈∆N |(cid:104)Zt, x(cid:105)| ≤ 4λ log N T.

It can be seen that the condition η|(cid:104)ei, ˜lt(cid:105)| ≤ 1 in Theorem
4.4 is satisﬁed for exploration µ(i) = 1
N and under the
condition ¯F as long as

ηN (1 + 4λ log N T ) ≤ γ

E[Regret| ¯F ]

= E{Zt}

EA

(cid:16)

(cid:17)
(cid:104)˜lt, ˜xt(cid:105) − (cid:104)˜lt, x∗(cid:105)

(cid:34) T

(cid:88)

t=1

(cid:34)

(cid:34)

(cid:34)

log N
η

log N
η

T
(cid:88)

t=1

T
(cid:88)

t=1

(cid:35)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)

¯F

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

¯F

+ 2η

N ((cid:107)lt(cid:107)2

∞ + (cid:107)Zt(cid:107)2

∞) + 2T γ

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

¯F

≤ E{Zt}

≤

log N
η
(cid:18)(cid:113)

(cid:18) √

≤ O

+ 2ηT N (1 + λ2 log2 N T ) + 2T γ

≤ O

T N log N (1 + λ2 log2 N T )

(cid:19)

N T log T log N
ε

(cid:19)

4.2. Differentially Private Bandit Linear Optimization

In this section we prove a general result about bandit linear
optimization over general convex sets, the proof of which
is deferred to the appendix.
Theorem 4.5 (Bandit Linear Optimization). Let X ⊆ RN
be a convex set. Fix loss vectors (l1, . . . lT ) such that
maxt,x∈X |(cid:104)lt, x(cid:105)| ≤ M . We have that Algorithm 4 when
run with parameters D = LapN (λ) (with λ = (cid:107)Y(cid:107)1
)
and algorithm A = SCRiBLe(Abernethy et al., 2012)
2) we have
with step parameter η =
the following guarantees that the regret of the algorithm is
bounded by

ν log T
2N 2T (M 2+λ2N (cid:107)X (cid:107)2

(cid:113)

ε

(cid:32)

O

(cid:112)T log T

(cid:115)

(cid:18)

N 2ν

M 2 +

(cid:19)(cid:33)

N (cid:107)X (cid:107)2
2(cid:107)Y(cid:107)2
1
ε2

where ν is the self-concordance parameter of the convex
body X . Moreover the algorithm is ε-differentially private.

5. Conclusion

In this work, we demonstrate that ensuring differential pri-
vacy leads to only a constant additive increase in the in-
curred regret for online linear optimization in the full feed-
back setting. We also show nearly optimal bounds (in terms
of T) in the bandit feedback setting. Multiple avenues for
future research arise, including extending our bandit re-
sults to other challenging partial-information models such
as semi-bandit, combinatorial bandit and contextual ban-
dits. Another important unresolved question is whether it
is possible to achieve an additive separation in ε, T in the
adversarial bandit setting.

The Price of Differential Privacy for Online Learning

Jain, Prateek, Kothari, Pravesh, and Thakurta, Abhradeep.
In COLT, vol-

Differentially private online learning.
ume 23, pp. 24–1, 2012.

Kalai, Adam and Vempala, Santosh. Efﬁcient algorithms
for online decision problems. Journal of Computer and
System Sciences, 71(3):291–307, 2005.

Shalev-Shwartz, Shai. Online learning and online con-
vex optimization. Foundations and Trends in Machine
Learning, 4(2):107–194, 2011.

Smith, Adam and Thakurta, Abhradeep Guha.

(nearly)
optimal algorithms for private online learning in full-
information and bandit settings. In Advances in Neural
Information Processing Systems, pp. 2733–2741, 2013.

Tossou, Aristide and Dimitrakakis, Christos. Algorithms
for differentially private multi-armed bandits. In AAAI
2016, 2016.

Tossou, Aristide C. Y. and Dimitrakakis, Christos. Achiev-
ing privacy in the adversarial multi-armed bandit.
In
14th International Conference on Artiﬁcial Intelligence
(AAAI 2017), 2017.

References

Abernethy, Jacob, Hazan, Elad, and Rakhlin, Alexander.
Competing in the dark: An efﬁcient algorithm for bandit
linear optimization. In COLT, pp. 263–274, 2008.

Abernethy, Jacob, Lee, Chansoo, Sinha, Abhinav, and
Tewari, Ambuj. Online linear optimization via smooth-
ing. In COLT, pp. 807–823, 2014.

Abernethy, Jacob D, Hazan, Elad, and Rakhlin, Alexan-
der. Interior-point methods for full-information and ban-
dit online learning. IEEE Transactions on Information
Theory, 58(7):4164–4175, 2012.

Auer, Peter, Cesa-Bianchi, Nicolo, Freund, Yoav, and
Schapire, Robert E. The nonstochastic multiarmed ban-
dit problem. SIAM journal on computing, 32(1):48–77,
2002.

Bubeck, S´ebastien, Cesa-Bianchi, Nicolo, Kakade,
and
Sham M, Mannor, Shie, Srebro, Nathan,
Williamson, Robert C.
Towards minimax policies
for online linear optimization with bandit feedback. In
COLT, volume 23, 2012a.

Bubeck, S´ebastien, Cesa-Bianchi, Nicolo, et al. Regret
analysis of stochastic and nonstochastic multi-armed
bandit problems. Foundations and Trends R(cid:13) in Machine
Learning, 5(1):1–122, 2012b.

Dwork, Cynthia, McSherry, Frank, Nissim, Kobbi, and
Smith, Adam. Calibrating noise to sensitivity in private
In Theory of Cryptography Conference,
data analysis.
pp. 265–284. Springer, 2006.

Dwork, Cynthia, Naor, Moni, Pitassi, Toniann, and Roth-
blum, Guy N. Differential privacy under continual obser-
vation. In Proceedings of the forty-second ACM sympo-
sium on Theory of computing, pp. 715–724. ACM, 2010.

Dwork, Cynthia, Roth, Aaron, et al. The algorithmic
foundations of differential privacy. Foundations and
Trends R(cid:13) in Theoretical Computer Science, 9(3–4):211–
407, 2014a.

Dwork, Cynthia, Talwar, Kunal, Thakurta, Abhradeep, and
Zhang, Li. Analyze gauss: optimal bounds for privacy-
preserving principal component analysis. In Proceedings
of the 46th Annual ACM Symposium on Theory of Com-
puting, pp. 11–20. ACM, 2014b.

Hazan, Elad et al. Introduction to online convex optimiza-
tion. Foundations and Trends R(cid:13) in Optimization, 2(3-4):
157–325, 2016.

Jain, Prateek and Thakurta, Abhradeep G. (near) dimension
independent risk bounds for differentially private learn-
ing. In Proceedings of the 31st International Conference
on Machine Learning (ICML-14), pp. 476–484, 2014.

A. Proofs for FTPL (Theorem 3.5)

Now we have that

The Price of Differential Privacy for Online Learning

Theorem A.1 (Privacy Guarantees with Gaussian Noise).
Choose any σ2 ≥ (cid:107)Y(cid:107)2
. When Algorithm
ε2
2 A(D, T ) is run with D = N (0, σ2IN ), the following
claims hold true:

log2 T log2 log T
δ

2

• Privacy: The sequence ( ˜Lt

: t ∈ [T ]) is (ε, δ)-

differentially private.

• Distribution:

It holds that ∀t ∈ [T ], ˜Lt ∼

N ((cid:80)t

s=1 ls, (cid:100)log T (cid:101)σ2IN ).

Proof. By Theorem 9 ((Jain et al., 2012)), we have that
the sequence ( ˜L(cid:48)
t : t ∈ [T ]) is (ε, δ)-differentially private.
Now the sequence ( ˜Lt : t ∈ [T ]) is (ε, δ)-differentially
private because differential privacy is immune to post-
processing (Dwork et al., 2014a).

Note that the PrivateSum algorithm adds exactly |S| in-
dependent draws from the distribution D to (cid:80)t
s=1 ls, where
S is the minimum set of already populated nodes in the tree
that can compute the required preﬁx sum. Due to Line 6, it
is made certain that every preﬁx sum released is a sum of
the preﬁx sum and (cid:100)log T (cid:101) independent draws from D.

Proof of Theorem 3.5. As a consequence of Corollary 4
((Abernethy et al., 2014)) and Lemma 13 ((Abernethy
et al., 2014)), it is true that

RegretA(D,T )(T ) ≤ σ(cid:112)N log T (cid:107)X (cid:107)2 +

T (cid:107)X (cid:107)2(cid:107)Y(cid:107)2
2
√
2σ

log T

Substituting the proposed value of σ, we obtain the stated
claim.

B. Noisy OCO Theorem Proof

We now give the proof of Lemma 4.3.

(cid:34)

(cid:34) T

(cid:88)

E{Zt}

EA

(cid:104)lt − ˜lt, ˜xt(cid:105)

(cid:35)(cid:35)

− E{Zt}

EA

(cid:104)lt − ˜lt, x∗(cid:105)

(cid:34)

t=1

(cid:34) T

(cid:88)

t=1

=EA

E{Zt}

(cid:104)

(cid:105)
(cid:104)lT − ˜lt, ˜xt − x∗(cid:105)

=EA

(cid:104)E [Zt] , E[˜xt − x∗](cid:105)

(cid:35)

(cid:34) T

(cid:88)

t=1
(cid:34) T

(cid:88)

t=1

(cid:35)(cid:35)

(cid:35)

=0

The ﬁrst inequality follows because the randomness of the
algorithm does not depend on Zt. The second equality fol-
lows because xt, being a function of the loss vectors of the
previous round, is independent of the noise Zt and the third
equality follows from the fact that E[Zt] = 0.

C. Facts about Norms of Laplace Vectors

The following simple fact follows for Laplacian distribu-
tions. The general versions follow immediately.
Fact C.1. Let Z ∼ LapN (λ), then we have that

P((cid:107)Z(cid:107)2

∞ ≥ 10λ2 log2 T N ) ≤

Proof. We will show that for a ﬁxed i ∈ [N ]

P(Z(i)2 ≥ 10λ2 log2 T N ) ≤

1
T 2

1
N T 2

The proof then follows by a simple union bound. Note
that it is enough to bound the probability that P (Y ≥
√
10λ log T N ) where Y is distributed as an exponential
random variable with parameter λ. This is bounded by
T 2N . A simple union bound ﬁnishes the proof now.

1

Proof. The proof of the lemma is a straightforward calcu-
lation. First note that

(cid:34)

(cid:34) T

(cid:88)

E{Zt}

EA

((cid:104)lt, ˜xt(cid:105) − (cid:104)lt, x∗(cid:105))

(cid:35)(cid:35)

= E{Zt}

EA

(cid:16)

(cid:17)
(cid:104)˜lt, ˜xt(cid:105) − (cid:104)˜lt, x∗(cid:105)

t=1
(cid:34)

(cid:34) T

(cid:88)

t=1

(cid:35)(cid:35)

(cid:34)

(cid:34)

(cid:34) T

(cid:88)

t=1

(cid:35)(cid:35)

(cid:34) T

(cid:88)

t=1

+E{Zt}

EA

(cid:104)lt − ˜lt, ˜xt(cid:105)

−E{Zt}

EA

(cid:104)lt − ˜lt, x∗(cid:105)

D. Proof for Bandit Linear Optimization

Proof of Theorem 4.5. We prove the privacy guarantees
ﬁrst, followed by the regret bound.
Privacy: Note that maxt,l∈Y | (cid:104)l,˜xt(cid:105)
(cid:107)˜xt(cid:107)∞
inequality. Therefore, with λ = (cid:107)Y(cid:107)1
sures ε-differential privacy due to Lemma 4.2.

| ≤ (cid:107)Y(cid:107)1 by Holder’s
, D = LapN (λ) en-

ε

(cid:35)(cid:35)
Regret: For the purpose of analysis we deﬁne the follow-
ing pseudo loss vectors ˜lt = lt + Zt, where by deﬁnition
Z ∼ LapN (λ). Further note that the following (which is

The Price of Differential Privacy for Online Learning

a direct consequence of Fact C.1 proved in the appendix)
holds for Zt ∼ LapN (λ)

It can now be veriﬁed that η ≤
can apply Theorem D.1 obtain that

1

4N (M +L) . Therefore we

P((cid:107)Zt(cid:107)2

2 ≥ 10λ2N log2 N T ) ≤

E[Regret| ¯F ]

(cid:34)

(cid:34) T

(cid:88)

= E{Zt}

EA

(cid:16)

(cid:17)
(cid:104)˜lt, ˜xt(cid:105) − (cid:104)˜lt, x∗(cid:105)

(cid:35)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)

¯F

(cid:88)

t=1
N 2|(cid:104)˜lt, ˜xt(cid:105)|2| ¯F

(cid:105)

(cid:104)

2η

≤ E{Zt}

(cid:20)
νη−1 log T + 2(M + L)

+ E{Zt}
(cid:104)

≤ E{Zt}

2η

(cid:88)

N 2(|(cid:104)lt, ˜xt(cid:105)|2 + (cid:107)Zt(cid:107)2

+ E{Zt}

(cid:20)
νη−1 log T + 2(M + L)

(cid:21)

¯F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
2)| ¯F
2(cid:107)˜xt(cid:107)2
(cid:12)
(cid:21)
(cid:12)
(cid:12)
(cid:12)
2) + νη−1 log T

¯F

(cid:105)

≤ 2ηT N 2(M 2 + λ2N (cid:107)X (cid:107)2

(cid:113)

+ 8λ(cid:107)X (cid:107)2
(cid:18)
(cid:112)T log T

≤ O

N log2 N T + 2M
(cid:113)

N 2ν(M 2 + λ2N (cid:107)X (cid:107)2
2)

(cid:19)

1
T 2

1
T

Therefore taking a union bound we get that

P(∃t : (cid:107)Zt(cid:107)2

2 ≥ 10λ2N log2 N T ) ≤

(9)

We condition on the following event F = {∃t : (cid:107)Zt(cid:107)2
10λ2N log2 N T }. We have from (9) that P(F ) ≤ 1
now have that

2 ≥
T . We

E[Regret] ≤ E[Regret| ¯F ] + P(F )E[Regret|F ]

Since the regret is always bounded by T we get that the
second term above is at most 1. Therefore we will concern
ourselves with bounding the ﬁrst term above. For the rest of
the section we will assume the conditioning on the event ¯F .
First note that since the noise vectors Zt were independent
to begin with they still remain conditionally independent
even when conditioned on the event ¯F . The following two
statements can be seen to follow easily.

∀t E[Zt|F ] = 0

∀t E[(cid:107)Zt(cid:107)2

2| ¯F ] ≤ 10λ2N log2 N T

(10)

(11)

It follows from Equation 10 that Lemma 4.3 applies even
when the noise is sampled from LapN (λ) conditioned on
the event ¯F . Therefore we have that

(cid:34)

(cid:88)

(cid:34) T

E[Regret| ¯F ] = E{Zt}

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(12)
Now note that since due to the conditioning (cid:107)Zt(cid:107)2 ≤
10λ2N log2 N T and therefore we have that

(cid:17)
(cid:16)
(cid:104)˜lt, ˜xt(cid:105) − (cid:104)˜lt, x∗(cid:105)

EA

t=1

¯F

(cid:35)

L (cid:44) maxt,x∈X |(cid:104)Zt, x(cid:105)| ≤ 4λ(cid:107)X (cid:107)2

N log2 N T .

(cid:113)

We now use the following theorem which is a simple re-
statement of Theorem 5.1 in (Abernethy et al., 2012).

Theorem D.1 (Theorem 5.1 in (Abernethy et al., 2012)).
Fix a loss sequence l1, . . . lT . Let X be a compact convex
set and R be a ν-self concordant barrier on X . Assume
maxt,x∈X |(cid:104)lt, x(cid:105)| ≤ M . Setting η ≤ 1
4NM then the regret
of the SCRiBLe algorithm is bounded by

RegretSCRiBLe ≤ 2η

N (cid:104)lt, xt(cid:105)2 + νη−1 log T + 2M

(cid:88)

where (xt ∈ X : t ∈ [T ]) is the sequence of points played
by the algorithm.

