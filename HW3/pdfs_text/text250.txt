Efﬁcient Regret Minimization in Non-Convex Games

Elad Hazan 1 Karan Singh 1 Cyril Zhang 1

Abstract

We consider regret minimization in repeated
games with non-convex loss functions. Minimiz-
ing the standard notion of regret is computation-
ally intractable. Thus, we deﬁne a natural no-
tion of regret which permits efﬁcient optimiza-
tion and generalizes ofﬂine guarantees for con-
vergence to an approximate local optimum. We
give gradient-based methods that achieve optimal
regret, which in turn guarantee convergence to
equilibrium in this framework.

1. Introduction

Repeated games with non-convex utility functions serve to
model many natural settings, such as multiplayer games
with risk-averse players and adversarial (e.g. GAN) train-
ing. However, standard regret minimization and equilibria
computation with general non-convex losses are computa-
tionally hard. This paper studies computationally tractable
notions of regret minimization and equilibria in non-convex
repeated games.

Efﬁcient online learning algorithms are intimately con-
nected to convexity. This connection is natural, since in
a very broad sense1, convexity captures efﬁcient computa-
tion in continuous mathematical optimization.

The recent success of non-convex learning models, no-
tably deep neural networks, motivates the need for efﬁ-
cient non-convex optimization. Since the latter is NP-hard
in general, efﬁcient optimization methods for non-convex
optimization are designed to ﬁnd local minima of varied
quality. Stochastic gradient-based methods for non-convex
optimization are currently state-of-the-art in training non-
convex machines.

1Computer Science,
to:

spondence
Karan
<cyril.zhang@princeton.edu>.

Princeton University.

Corre-
Elad Hazan <ehazan@princeton.edu>,
Zhang

Cyril

Singh <karans@princeton.edu>,

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1Though by no means exclusively, PCA and other spectral

methods being a notable exception.

In this paper we investigate the generalization of the non-
convex statistical, or batch, learning model to online learn-
ing. The main question we ask is what kind of guaran-
tees can be obtained efﬁciently in an adversarial non-convex
scenario.

After brieﬂy discussing why standard regret is not a suit-
able metric of performance, we introduce and motivate lo-
cal regret, a surrogate for regret to the non-convex world.
We then proceed to give efﬁcient algorithms for non-
convex online learning with optimal guarantees for this new
objective. In analogy with the convex setting, we discuss
the way our framework captures the ofﬂine and stochastic
cases. In the ﬁnal section, we describe a game-theoretic so-
lution concept which is intuitively appealing, and, in con-
trast to Nash equilibria, efﬁciently attainable by simple al-
gorithms.

1.1. Related work

The ﬁeld of online learning is by now rich with a diverse
set of algorithms for extremely general scenarios, see e.g.
(Cesa-Bianchi & Lugosi, 2006). For bounded cost func-
tions over a bounded domain, it is well known that versions
of the multiplicative weights method gives near-optimal re-
gret bounds (Cover, 1991; Vovk, 1990; Arora et al., 2012).

Despite the tremendous generality in terms of prediction,
the multiplicative weights method in its various forms
yields only exponential-time algorithms for these general
scenarios. This is inevitable, since regret minimization im-
plies optimization, and general non-convex optimization is
NP-hard. Convex forms of regret minimization have dom-
inated the learning literature in recent years due to the fact
that they allow for efﬁcient optimization, see e.g. (Hazan,
2016; Shalev-Shwartz, 2011).

Non-convex mathematical optimization algorithms typi-
cally ﬁnd a local optimum. For smooth optimization,
gradient-based methods are known to ﬁnd a point with gra-
dient of squared norm at most ε in O( 1
ε ) iterations (Nes-
terov, 2004).2 A rate of O( 1
ε2 ) is known for stochastic

2We note here that we measure the squared norm of the
gradient, since it is more compatible with convex optimization.
The mathematical optimization literature sometimes measures the
norm of the gradient without squaring it.

Efﬁcient Regret Minimization in Non-Convex Games

gradient descent (Ghadimi & Lan, 2013). Further accel-
erations in terms of the dimension are possible via adaptive
regularization (Duchi et al., 2011).

bounded, local information may provide no information
about the location of a stationary point. This motivates us
to reﬁne our search criteria.

Recently, stochastic second-order methods have been con-
sidered, which enable even better guarantees for non-
convex optimization: not only is the gradient at the point
returned small, but the Hessian is also guaranteed to be
close to positive semideﬁnite (i.e.
the objective function
is locally almost-convex), see e.g. (Erdogdu & Montanari,
2015; Carmon et al., 2016; Agarwal et al., 2016a;b).

The relationship between regret minimization and learning
in games has been considered in both the machine learning
literature, starting with (Freund & Schapire, 1997), and the
game theory literature by (Hart & Mas-Colell, 2000). Mo-
tivated by (Hart & Mas-Colell, 2000), (Blum & Mansour,
2005) study reductions from internal to external regret, and
(Hazan & Kale, 2007) relate the computational efﬁciency
of these reductions to ﬁxed point computations.

2. Setting

We begin by introducing the setting of online non-convex
optimization, which is modeled as a game between a
learner and an adversary. During each iteration t, the
learner is tasked with predicting xt from K ⊆ Rn, a con-
vex decision set. Concurrently, the adversary chooses a loss
function ft : K → R; the learner then observes ft(x) (via
access to a ﬁrst-order oracle) and suffers a loss of ft(xt).
This procedure of play is repeated across T rounds.

The performance of the learner is measured through its re-
gret, which is deﬁned as a function of the loss sequence
f1, . . . , fT and the sequence of online decisions x1, . . . , xT
made by the learner. We discuss our choice of regret mea-
sure at length in Section 2.2.

Throughout this paper, we assume the following standard
regularity conditions:

Assumption 2.1. We assume the following is true for each
loss function ft:

(i) ft is bounded: |ft(x)| ≤ M.

(ii) ft is L-Lipschitz: |ft(x) − ft(y)| ≤ L(cid:107)x − y(cid:107).

(iii) ft is β-smooth (has a β-Lipschitz gradient):

Consider, for example, the function sketched in Figure 1.
In this construction, deﬁned on the hypercube in Rn, the
unique point with a vanishing gradient is a hidden valley,
and gradients outside this valley are all identical. Clearly,
it is hopeless in an information-theoretic sense to ﬁnd this
point efﬁciently: the number of value or gradient evalua-
tions of this function must be exp(Ω(n)) to discover the
valley.

Figure 1. A difﬁcult “needle in a haystack” case for constrained
non-convex optimization. Left: A function with a hidden val-
ley, with small gradients shown in yellow. Right: Regions with
small projected gradient for the same function. For smaller η,
only points near the valley and bottom-left corner have small pro-
jected gradient.

To circumvent such inherently difﬁcult and degenerate
cases, we relax our conditions, and try to ﬁnd a vanish-
ing projected gradient. In this section, we introduce this
notion formally, and motivate it as a natural quantity of in-
terest to capture the search for local minima in constrained
non-convex optimization.
Deﬁnition 2.2 (Projected gradient). Let f : K → R be
a differentiable function on a closed (but not necessarily
bounded) convex set K ⊆ Rn. Let η > 0. We deﬁne
∇K,ηf : K → Rn, the (K, η)-projected gradient of f , by

∇K,ηf (x) def=

(x − ΠK [x − η∇f (x)]) ,

1
η

where ΠK[·] denotes the orthogonal projection onto K.

This can be viewed as a surrogate for the gradient which
ensures that the gradient descent step always lies within K,
by transforming it into a projected gradient descent step.
Indeed, one can verify by deﬁnition that

(cid:107)∇ft(x) − ∇ft(y)(cid:107) ≤ β(cid:107)x − y(cid:107).

x − η∇K,η(x) = ΠK [x − η∇f (x)] .

2.1. Projected Gradients and Constrained Non-Convex

In particular, when K = Rn,

Optimization

In constrained non-convex optimization, minimizing the
gradient presents difﬁcult computational challenges.
In
general, even when objective functions are smooth and

∇K,ηf (x) =

(x − x + η∇f (x)) = ∇f (x),

1
η

and we retrieve the usual gradient at all x.

Efﬁcient Regret Minimization in Non-Convex Games

We ﬁrst note that there always exists a point with vanishing
projected gradient.
Proposition 2.3. Let K be a compact convex set, and sup-
pose f : K → R satisﬁes Assumption 2.1. Then, there
exists some point x∗ ∈ K for which

∇K,ηf (x∗) = 0.

particular,

In
fact
(cid:107)∇K,ηf (x)(cid:107) ≤ (cid:107)∇f (x)(cid:107).

this

immediately

implies

that

As we demonstrate later, looking for a small projected gra-
dient becomes a feasible task. In Figure 1 above, such a
point exists on the boundary of K, even when there is no
“hidden valley” at all.

Proof. Consider the map g : K → K, deﬁned by

2.2. A Local Regret Measure

g(x) def= x − η∇K,ηf (x) = ΠK [x − η∇f (x)] .

This is a composition of continuous functions (noting that
the smoothness assumption implies that ∇f is continuous),
and is therefore continuous. Thus g satisﬁes the conditions
for Brouwer’s ﬁxed point theorem, implying that there ex-
ists some x∗ ∈ K for which g(x∗) = x∗. At this point, the
projected gradient vanishes.

In the limit where η(cid:107)∇f (x)(cid:107) is inﬁnitesimally small, the
projected gradient is equal to the gradient in the interior of
K; on the boundary of K, it is the gradient with its outward-
facing component removed. This exactly captures the ﬁrst-
order condition for a local minimum.

The ﬁnal property that we note here is that an approximate
local minimum, as measured by a small projected gradient,
is robust with respect to small perturbations.
Proposition 2.4. Let x be any point in K ⊆ Rn, and let f, g
be differentiable functions K → R. Then, for any η > 0,

(cid:107)∇K,η[f + g](x)(cid:107) ≤ (cid:107)∇K,ηf (x)(cid:107) + (cid:107)∇g(x)(cid:107).

In the well-established framework of online convex opti-
mization, numerous algorithms can efﬁciently achieve op-
timal regret, in the sense of converging in terms of average
loss towards the best ﬁxed decision in hindsight. That is,
for any u ∈ K, one can play iterates x1, . . . , xT such that

1
T

T
(cid:88)

i=1

[ft(xt) − ft(u)] = o(1).

Unfortunately, even in the ofﬂine case, it is too ambitious
to converge towards a global minimizer in hindsight.
In
the existing literature, it is usual to state convergence guar-
antees towards an ε-approximate stationary point – that is,
there exists some iterate xt for which (cid:107)∇f (xt)(cid:107)2 ≤ ε. As
discussed in the previous section, the projected gradient is
a natural analogue for the constrained case.

In light of the computational intractability of direct ana-
logues of convex regret, we introduce local regret, a new
notion of regret which quantiﬁes the objective of predict-
ing points with small gradients on average. The remainder
of this paper discusses the motivating roles of this quantity.

Proof. Let u = x + η∇f (x), and v = u + η∇g(x). Deﬁne
their respective projections u(cid:48) = ΠK [u] , v(cid:48) = ΠK [v], so
that u(cid:48) = x − η∇K,ηf (x) and v(cid:48) = x − η∇K,η[f + g](x).
We ﬁrst show that (cid:107)u(cid:48) − v(cid:48)(cid:107) ≤ (cid:107)u − v(cid:107).

Throughout this paper, for convenience, we will use the
following notation to denote the sliding-window time av-
erage of functions f , parametrized by some window size
1 ≤ w ≤ T :

By the generalized Pythagorean theorem for convex sets,
we have both (cid:104)u(cid:48) − v(cid:48), v − v(cid:48)(cid:105) ≤ 0 and (cid:104)v(cid:48) − u(cid:48), u − u(cid:48)(cid:105) ≤
0. Summing these, we get

(cid:104)u(cid:48) − v(cid:48), u(cid:48) − v(cid:48) − (u − v)(cid:105) ≤ 0

=⇒ (cid:107)u(cid:48) − v(cid:48)(cid:107)2 ≤ (cid:104)u(cid:48) − v(cid:48), u − v(cid:105)

≤ (cid:107)u(cid:48) − v(cid:48)(cid:107) · (cid:107)u − v(cid:107),

as claimed. Finally, by the triangle inequality, we have

(cid:107)∇K,η[f + g](x)(cid:107) − (cid:107)∇K,ηf (x)(cid:107)

≤ (cid:107)∇K,η[f + g](x) − ∇K,ηf (x)(cid:107)

=

(cid:107)u(cid:48) − v(cid:48)(cid:107)

1
η
1
η

≤

(cid:107)u − v(cid:107) = (cid:107)∇g(x)(cid:107),

as required.

Ft,w(x) def=

ft−i(x).

1
w

w−1
(cid:88)

i=0

For simplicity of notation, we deﬁne ft(x) to be identically
zero for all t ≤ 0. We deﬁne local regret below:

Deﬁnition 2.5 (Local regret). Fix some η > 0. Deﬁne the
w-local regret of an online algorithm as

Rw(T ) def=

(cid:107)∇K,ηFt,w(xt)(cid:107)2,

T
(cid:88)

t=1

When the window size w is understood by context, we omit
the parameter, writing simply local regret as well as Ft(x).

We turn to the ﬁrst motivating perspective on local regret.
When an algorithm incurs local regret sublinear in T , a ran-
domly selected iterate has a small time-averaged gradient
in expectation:

Efﬁcient Regret Minimization in Non-Convex Games

Proposition 2.6. Let x1, . . . , xT be the iterates produced
by an algorithm for online non-convex optimization which
incurs a local regret of Rw(T ). Then,

ft(x), a bound on local regret truly captures a guarantee of
playing points with small gradients.

Et∼Unif([T ])

(cid:2)(cid:107)∇K,ηFt,w(xt)(cid:107)2(cid:3) ≤

Rw(T )
T

.

3. An Efﬁcient Non-Convex Regret

Minimization Algorithm

This generalizes typical convergence results for the gradi-
ent in ofﬂine non-convex optimization; we discuss concrete
reductions in Section 4.

2.3. Why Smoothing is Necessary

In this section, we show that for any online algorithm, an
adversarial sequence of loss functions can force the local
(cid:1). This demon-
regret incurred to scale with T as Ω (cid:0) T
w2
strates the need for a time-smoothed performance measure
in our setting, and justiﬁes our choice of larger values of
the window size w in the sections that follow.
Theorem 2.7. Deﬁne K = [−1, 1]. For any T ≥ 1,
1 ≤ w ≤ T , and η ≤ 1, there exists a distribution D on
0-smooth, 1-bounded cost functions f1, . . . , fT on K such
that for any online algorithm, when run on this sequence of
functions,

ED [Rw(T )] ≥

1
4w

(cid:22) T
2w

(cid:23)

.

Proof. We begin by partitioning the T rounds of play into
(cid:98) T
2w (cid:99) repeated segments, each of length 2w.
For the ﬁrst half of the ﬁrst segment (t = 1, . . . , w), the
adversary declares that

• For odd t, select ft(x) i.i.d. as follows:
(cid:40)

ft(x) :=

−x, with probability 1
2
with probability 1
x,
2

• For even t, ft(x) := −ft−1(x).

During the second half (t = w + 1, . . . , 2w), the adver-
sary sets all ft(x) = 0. This construction is repeated (cid:98) T
2w (cid:99)
times, padding the ﬁnal T mod 2w costs arbitrarily with
ft(x) = 0.

By this construction, at each round t at which ft(x) is
drawn randomly, we have Ft,w(x) = ft(x)/w. Further-
more, for any xt played by the algorithm, |ft(xt)| = 1 with
probability at least 1
2w2 .
The claim now follows from the fact that there are at least
2 of these rounds per segment, and exactly (cid:4) T
(cid:5) segments
w
in total.

2 . so that E (cid:2)(cid:107)∇K,ηFt,w(xt)(cid:107)2(cid:3) ≥ 1

2w

We further note that the notion of time-smoothing cap-
tures non-convex online optimization under limited con-
cept drift: in online learning problems where Ft,w(x) ≈

Our approach, as given in Algorithm 1, is to play follow-
the-leader iterates, approximated to a suitable tolerance
using projected gradient descent. We show that
this
method efﬁciently achieves an optimal local regret bound
of O (cid:0) T
w2

(cid:1), taking O (T w) iterations.

Algorithm 1 Time-smoothed online gradient descent
1: Input: window size w ≥ 1, learning rate 0 < η < β
2 ,

tolerance δ > 0, a convex body K ⊆ Rn.

2: Set x1 ∈ K arbitrarily.
3: for t = 1, . . . , T do
4:
5:
6:
7:
8:
9: end for

end while

Predict xt. Observe the cost function ft : K → R.
Initialize xt+1 := xt.
while (cid:107)∇K,ηFt,w(xt+1)(cid:107) > δ/w do

Update xt+1 := xt+1 − η∇K,ηFt,w(xt+1).

Theorem 3.1. Let f1, . . . , fT be the sequence of loss func-
tions presented to Algorithm 1, satisfying Assumption 2.1.
Then:

(i) The w-local regret incurred satisﬁes

Rw(T ) ≤ (δ + 2L)2 T
w2 .

(ii) The total number of gradient steps τ taken by Algo-

rithm 1 satisﬁes

τ ≤

M
(cid:16)
η − βη2
2

δ2

(cid:17) · (cid:0)2T w + w2(cid:1) .

Proof of (i). We note that Algorithm 1 will only play an
iterate xt if (cid:107)∇K,ηFt−1,w(cid:107) ≤ δ/w. (Note that at t = 1,
Ft−1,w is zero.) Let ht(x) = 1
w (ft(x) − ft−w(x)), which
is 2L
w -Lipschitz. Then, for each 1 ≤ t ≤ T we have a
bound on each cost

(cid:107)∇K,ηFt,w(xt)(cid:107)2 = (cid:107)∇K,η [Ft,w−1 + ht(x)] (xt)(cid:107)2
≤ ((cid:107)∇K,ηFt,w−1(cid:107) + (cid:107)∇ht(xt)(cid:107))2

≤

(cid:18) δ
w

+

2L
w

(cid:19)2

=

(δ + 2L)2
w2

,

where the ﬁrst inequality follows from Proposition 2.4.
Summing over all t gives the desired result.

Efﬁcient Regret Minimization in Non-Convex Games

Proof of (ii). First, we require an additional property of the
projected gradient.
Lemma 3.2. Let K ∈ Rn be a closed convex set, and let
η > 0. Suppose f : K → R is differentiable. Then, for any
x ∈ R,

(cid:104)∇f (x), ∇K,ηf (x)(cid:105) ≥ (cid:107)∇K,ηf (x)(cid:107)2.

Proof. Let u = x − η∇f (x) and u(cid:48) = ΠK [u]. Then,

(cid:104)∇f (x), ∇K,ηf (x)(cid:105)
η2

−

(cid:107)∇K,ηf (x)(cid:107)2
η2

= (cid:104)u − x, u(cid:48) − x(cid:105) − (cid:104)u(cid:48) − x, u(cid:48) − x(cid:105)
= (cid:104)u − u(cid:48), u(cid:48) − x(cid:105) ≥ 0,

where the last
Pythagorean theorem.

inequality follows by the generalized

For 2 ≤ t ≤ T , let τt be the number of gradient steps taken
in the outer loop at iteration t − 1, in order to compute the
iterate xt. For convenience, deﬁne τ1 = 0. We establish a
progress lemma during each gradient descent epoch:

Lemma 3.3. For any 2 ≤ t ≤ T ,

Using Lemma 3.3, we have

FT (xT ) ≤

2M T
w

−

(cid:17)

δ2

(cid:16)

η − βη2
2
w2

·

T
(cid:88)

t=1

τt,

whence

τ =

τt ≤

T
(cid:88)

t=1

as claimed.

(cid:16)

w2
η − βη2
2

(cid:16)

M
η − βη2
2

δ2

δ2

≤

(cid:18) 2M T
w

(cid:17) ·

(cid:19)

− FT (xT )

(cid:17) · (cid:0)2T w + w2(cid:1) ,

Setting η = 1/β and δ = L gives the asymptotically op-
timal local regret bound, with O(T w) time-averaged gra-
dient steps (and thus O(T w2) individual gradient oracle
calls). We further note that in the case where K = Rn,
one can replace the gradient descent subroutine (the inner
loop) with non-convex SVRG (Allen-Zhu & Hazan, 2016),
achieving a complexity of O(T w5/3) gradient oracle calls.

Ft−1(xt) − Ft−1(xt−1) ≤ −τt

η −

(cid:18)

βη2
2

(cid:19) δ2
w2 .

4. Implications for Ofﬂine and Stochastic

Non-Convex Optimization

Proof. Consider a single iterate z of the inner loop, and
the next iterate z(cid:48)
:= z − η∇K,ηFt−1(z). We have, by
β-smoothness of Ft−1,

Ft−1(z(cid:48)) − Ft−1(z) ≤ (cid:104)∇Ft−1(z), z(cid:48) − z(cid:105) +

(cid:107)z(cid:48) − z(cid:107)2

= −η (cid:104)∇Ft−1(z), ∇K,ηFt−1(z)(cid:105) +

(cid:107)∇K,ηFt−1(z)(cid:107)2.

β
2

βη2
2

In this section, we discuss the ways in which our online
framework generalizes the ofﬂine and stochastic versions
of non-convex optimization – that any algorithm achieving
a small value of Rw(T ) efﬁciently ﬁnds a point with small
gradient in these settings. For convenience, for 1 ≤ t ≤
t(cid:48) ≤ T , we denote by D[t,t(cid:48)] the uniform distribution on
time steps t through t(cid:48) inclusive.

Thus, by Lemma 3.2,

4.1. Ofﬂine non-convex optimization

Ft−1(z(cid:48)) − Ft−1(z) ≤ −

η −

(cid:107)∇K,ηFt−1(z)(cid:107)2.

(cid:18)

(cid:19)

βη2
2

The algorithm only takes projected gradient steps when
(cid:107)∇K,ηFt−1(z)(cid:107) ≥ δ/w. Summing across all τt consec-
utive iterations in the epoch yields the claim.

To complete the proof of the theorem, we write the tele-
scopic sum (understanding F0(x0) = 0):

FT (xT ) =

Ft(xt) − Ft−1(xt−1)

T
(cid:88)

t=1

=

≤

T
(cid:88)

t=1

T
(cid:88)

t=2

Ft−1(xt) − Ft−1(xt−1) + ft(xt) − ft−w(xt)

[Ft−1(xt) − Ft−1(xt−1)] +

2M T
w

.

For ofﬂine optimization on a ﬁxed non-convex function
f : K → R, we demonstrate that a bound on local re-
gret translates to convergence. In particular, using Algo-
rithm 1 one ﬁnds a point x ∈ K with (cid:107)∇K,ηf (x)(cid:107)2 ≤ ε
while making O (cid:0) 1
(cid:1) calls to the gradient oracle, matching
the best known result for the convergence of gradient-based
methods.
Corollary 4.1. Let f : K → R satisfy Assumption 2.1.
When online algorithm A is run on a sequence of T identi-
cal loss functions f (x), it holds that for any 1 ≤ w < T ,

ε

Et∼D[w,T ] (cid:107)∇K,ηf (xt)(cid:107)2 ≤

Rw(A)
T − w

.

In particular, Algorithm 1, with parameter choices T =
2w, η = 1

β , δ = L, and w = (δ + 2L)

ε , yields

(cid:113) 2

Et∼Dw,T (cid:107)∇K,ηf (xt)(cid:107)2 ≤ ε.

Efﬁcient Regret Minimization in Non-Convex Games

Furthermore, the algorithm makes O (cid:0) 1
dient oracle in total.

ε

(cid:1) calls to the gra-

Proof. Since ft(x) = f (x) for all t,
it follows that
Ft,w(x) = f (x) for all t ≥ w. As a consequence, we
have

Et∼Dw,t(cid:107)∇K,ηf (xt)(cid:107)2 ≤

(cid:107)∇K,ηf (xt)(cid:107)2

1
T − w

T
(cid:88)

t=1

≤

Rw(A)
T − w

.

With the stated choice of parameters, Theorem 3.1 guaran-
tees that

Et∼Dw,t (cid:107)∇K,ηf (xt)(cid:107)2 ≤

ε
2

·

T
T − w

= ε.

Also, since the loss functions are identical, the execution
of line 7 of Algorithm 1 requires exactly one call to the
gradient oracle at each iteration. This entails that the total
number of gradient oracle calls made in the execution is
O(T w + w2) = O( 1

ε ).

4.2. Stochastic non-convex optimization

We examine the way in which our online framework cap-
tures stochastic non-convex optimization of a ﬁxed func-
tion f : Rn → R, in which an algorithm has access to a
noisy stochastic gradient oracle (cid:102)∇f (x). We note that the
reduction here will only apply in the unconstrained case;
it becomes challenging to reason about the projected gra-
dient under noisy information. From a local regret bound,
we recover a stochastic algorithm with oracle complexity
O
. We note that this black-box reduction recovers an
optimal convergence rate in terms of ε, but not σ2.

(cid:16) σ4
ε2

(cid:17)

In the setting, the algorithm must operate on the noisy es-
timates of the gradient as the feedback. In particular, for
any ft that the adversary chooses, the learning algorithm
is supplied with a stochastic gradient oracle for ft. The
discussion in the preceding sections may be viewed as a
special case of this setting with σ = 0. We list the assump-
tions we make on the stochastic gradient oracle, which are
standard:

Assumption 4.2. We assume that each call to the stochas-
tic gradient oracle yields an i.i.d. random vector (cid:102)∇f (x)
with the following properties:

(i) Unbiased: E

(cid:104)

(cid:105)

(cid:102)∇f (x)

= ∇f (x).

When an online algorithm incurs small local regret in ex-
pectation, it has a convergence guarantee in ofﬂine stochas-
tic non-convex optimization:

Proposition 4.3. Let 1 ≤ w < T . Suppose that on-
line algorithm A is run on a sequence of T identical
loss functions f (x) satisfying Assumption 2.1, with iden-
tical stochastic gradient oracles satisfying Assumption 4.2.
Sample t ∼ D[w,T ]. Then, over the randomness of t and
the oracles,

E (cid:2)(cid:107)∇f (xt)(cid:107)2(cid:3) ≤

E [Rw(A)]
T − w

.

Proof. Observe that

Et∼D[w,T ]

(cid:2)(cid:107)∇f (xt)(cid:107)2(cid:3) ≤

(cid:80)T

t=1 (cid:107)∇f (xt)(cid:107)2
T − w

≤

Rw(A)
T − w

.

The claim follows by taking the expectation of both sides,
over the randomness of the oracles.

For a concrete online-to-stochastic reduction, we consider
Algorithm 2, which exhibits such a bound on expected lo-
cal regret.

Algorithm 2 Time-smoothed online gradient descent with
stochastic gradient oracles
1: Input: learning rate η > 0, window size w ≥ 1.
2: Set x1 = 0 ∈ Rn arbitrarily.
3: for t = 1, . . . , T do
4:
5:
6: end for

Predict xt. Observe the cost function ft : Rn → R.
Update xt+1 := xt − η
w

i=0 (cid:102)∇f t−i(xt).

(cid:80)w−1

Theorem 4.4. Let f1, . . . , ft satisfy Assumption 2.1. Then,
Algorithm 2, with access to stochastic gradient oracles
{ (cid:102)∇f t(xt)} satisfying Assumption 4.2, and a choice of η =
1
β , guarantees

E [Rw(T )] ≤ (cid:0)8βM + σ2(cid:1) T
w

.

Furthermore, Algorithm 2 makes a total of O(T w) calls to
the stochastic gradient oracles.

Using this expected local regret bound in Proposition 4.3,
we obtain the reduction claimed at the beginning of the sec-
tion:

Corollary 4.5. Algorithm 2, with parameter choices w =
12M β+2σ2
ε

, T = 2w, and η = 1

β , yields
E (cid:2)(cid:107)∇f (xt)(cid:107)2(cid:3) ≤ ε.
(cid:16) σ4
ε2

(cid:17)

(ii) Bounded variance: E

(cid:104)

(cid:107) (cid:102)∇f (x) − ∇f (x)(cid:107)2(cid:105)

≤ σ2.

Furthermore, the algorithm makes O
dient oracle calls in total.

stochastic gra-

Efﬁcient Regret Minimization in Non-Convex Games

5. An Efﬁcient Algorithm with Second-Order

Guarantees

We note that by modifying Algorithm 1 to exploit second-
order information, our online algorithm can be improved to
play approximate ﬁrst-order critical points which are also
locally almost convex. This entails replacing the gradient
descent epochs with a cubic-regularized Newton method
(Nesterov & Polyak, 2006; Agarwal et al., 2016a).

In this setting, we assume that we have access to each ft
through a value, gradient, and Hessian oracle. That is,
once we have observed ft, we can obtain ft(x), ∇ft(x),
and ∇2ft(x) for any x. Let MinEig(A) be the minimum
(eigenvalue, eigenvector) pair for matrix A. As is standard
for ofﬂine second-order algorithms, we must add the fol-
lowing additional smoothness restriction:

Assumption 5.1. ft is twice differentiable and has an L2-
Lipschitz Hessian:

Algorithm 3 Time-smoothed online Newton method
1: Input: window size w ≥ 1, tolerance δ > 0.
2: Set x1 ∈ K arbitrarily.
3: for t = 1, . . . , T do
4:
5:
6:
7:

Predict xt. Observe the cost function ft : Rn → R.
Initialize xt+1 := xt.
while Φt(xt+1) > δ3/w3 do
Update xt+1 := xt+1 − 1
β ∇Ft,w(xt+1).
Let (λ, v) := MinEig (cid:0)∇2Ft,w(xt+1)(cid:1).
if λ < 0 then

Flip the sign of v so that (cid:104)v, ∇Ft,w(xt+1)(cid:105) ≤ 0.
Compute yt+1 := xt + 2λ
L2
if Ft,w(yt+1) < Ft,w(xt+1) then

v.

Set xt+1 := yt+1.

8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end for

end if

end if
end while

(cid:107)∇2f (x) − ∇2f (y)(cid:107) ≤ L2(cid:107)x − y(cid:107).

Proof of (i). For each 1 ≤ t ≤ T , we have

Additionally, we consider only the unconstrained case
where K = Rn; the second-order optimality condition is
irrelevant when the gradient does not vanish at the bound-
ary of K.

The second-order Algorithm 3 uses the same approach as
in Algorithm 1, but terminates each epoch under a stronger
approximate-optimality condition. We deﬁne

Φt(x) := max

(cid:107)∇Ft(x)(cid:107)2, −

· λmin(∇2Ft(x))3

,

(cid:26)

4β
3L2
2

the quantity (cid:80)T

t=1 Φt(xt) is termwise lower
so that
bounded by the costs in Rw(T ), but penalizes local con-
cavity.

We characterize the convergence and oracle complexity
properties of this algorithm:

Theorem 5.2. Let f1, . . . , fT be the sequence of loss func-
tions presented to Algorithm 3, satisfying Assumptions 2.1
and 5.1. Choose δ = β. Then, for some constants C1, C2
in terms of M, L, β, L2:

(i) The iterates {xt} produced by Algorithm 3 satisfy

T
(cid:88)

t=1

Φt(xt) ≤ C1 ·

T
w2 .

(ii) The total number of iterations τ of the inner loop taken

by Algorithm 3 satisﬁes

τ ≤ C2 · T w2.

Φt−1(xt) ≤

δ3
w3 .

Let ht(x) := 1
w -Lipschitz and 2β
2L

w -smooth,

w (ft(x) − ft−w(x)). Then, since ht(x) is

Φt(xt) = max

(cid:107)∇Ft−1(xt) + ∇ht(xt)(cid:107)2,

(cid:110)

(cid:27)

−

· λmin(∇2Ft(xt) + ∇2ht(xt))3(cid:111)

4β
3L2
2
(cid:40)(cid:18) δ3/2
w3/2

≤ max

(cid:19)2

+

2L
w

,

4β
3L2
2

(cid:18) δ
w

·

+

2β
w

(cid:19)3(cid:41)

,

which is bounded by C1/w2, for some C1(M, L, β, L2
2).
The claim follows by summing this inequality across all
1 ≤ t ≤ T .

Proof of (ii). We ﬁrst show the following progress lemma:

Lemma 5.3. Let z, z(cid:48) be two consecutive iterates of the
inner loop in Algorithm 3 during round t. Then,

Ft(z(cid:48)) − Ft(z) ≤ −

Φt(z)
2β

.

Proof. Let u denote the step z(cid:48) − z. Let g := ∇Ft(z),
H := ∇2Ft(z), and (λ, v) := MinEig(H).

Suppose that at time t, the algorithm takes a gradient step,
so that u = g/β. Then, by second-order smoothness of Ft,
we have

Ft(z(cid:48)) − Ft(z) ≤ (cid:104)g, u(cid:105) +

(cid:107)u(cid:107)2 = −

(cid:107)g(cid:107)2.

β
2

1
2β

Efﬁcient Regret Minimization in Non-Convex Games

Supposing instead that the algorithm takes a second-order
step, so that u = ± 2λ
v (whichever sign makes (cid:104)g, u(cid:105) ≤ 0),
L2
the third-order smoothness of Ft implies

(cid:8)(x1

t−j, . . . , xk

t−j)(cid:9)w−1

if, for every player i ∈ [k],

j=0
(cid:34) (cid:80)w−1
j=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇K,η

(cid:35)

˜fi,t−j
w

(xi
t)

≤ ε,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ft(z(cid:48)) − Ft(z) ≤ (cid:104)g, u(cid:105) +

uT Htu +

(cid:107)u(cid:107)3

where

L2
6

L2
6

= (cid:104)g, u(cid:105) +

(cid:107)u(cid:107)2 +

(cid:107)u(cid:107)3

≤

2λ3
3L2
2

=

·

4βλ3
3L2
2

.

1
2
λ
2
1
2β

The lemma follows due to the fact that the algorithm takes
the step that gives a smaller value of Ft(z(cid:48)).

The rest of the proof follows the same structure as that for
part (ii) of Theorem 3.1: summing Lemma 5.3 implies a
statement analogous to Lemma 3.3, which we telescope
over all epochs. For sake of completeness, we give the
proof in the appendix.

6. A Solution Concept for Non-Convex Games

Finally, we discuss an application of our regret minimiza-
tion framework to learning in k-player T -round iterated
games with smooth, non-convex payoff functions. Suppose
that each player i ∈ [k] has a ﬁxed decision set Ki ⊂ Rn,
and a ﬁxed payoff function fi : K → R satisﬁes Assump-
tion 2.1 as before. Here, K denotes the Cartesian product
of the decision sets Ki: each payoff function is deﬁned in
terms of the choices made by every player.

In such a game, it is natural to consider the setting where
players will only consider small local deviations from their
strategies. This is a natural setting, which models risk aver-
sion. This setting lends itself to the notion of a local equi-
librium, to replace the stronger condition of Nash equilib-
rium: a joint strategy in which no player encounters a large
gradient on her utility. However, ﬁnding an approximate
local equilibrium in this sense remains computationally in-
tractable when the utility functions are non-convex.

Using the idea time-smoothing, we formulate a tractable re-
laxed notion of local equilibrium, deﬁned over some time
window w. Intuitively, this deﬁnition captures a state of
an iterated game in which each player examines the past
w actions played, and no player can make small deviations
to improve the average performance of her play against her
opponents’ historical play. We formulate this solution con-
cept as follows:

Deﬁnition 6.1 (Smoothed local equilibrium). Fix some
η > 0, w ≥ 1. Let (cid:8)fi(x1, . . . , xk) : K → R(cid:9)k
i=1 be
the payoff functions for a k-player iterated game. A
joint strategy (x1
t ) is an ε-approximate (η, w)-
smoothed local equilibrium with respect to past iterates

t , . . . , xk

˜fi,t(cid:48)(x) def= fi(x1

t(cid:48), . . . , xi−1

t(cid:48)

, x, xi+1

, . . . , xk

t(cid:48)).

t(cid:48)

To achieve such an equilibrium efﬁciently, we use Algo-
rithm 4, which runs k copies of any online algorithm that
achieves a w-local regret bound for some η > 0.

Algorithm 4 Time-smoothed game simulation
1: Input: convex decision sets K1, . . . , Kk ⊆ Rn, payoff
functions fi : (K1, . . . , Kk) → R, online algorithm A,
window size 1 ≤ w < T .

2: Initialize k copies (A1, . . . , Ak) of A with window

size w, where each Ai plays on decision set Ki.

3: for t = 1, . . . , T do
4:
5:

Each Ai outputs xi
t.
Show each Ai the online loss function

fi,t(x) := −fi(x1

t , . . . , xi−1

t

, x, xi+1
t

, . . . , xk

t ).

6: end for

We show this meta-algorithm yields a subsequence of iter-
ates that satisfy our solution concept, with error parameter
dependent on the local regret guarantees of each player:
Theorem 6.2. For some t such that w ≤ t ≤ T , the
joint strategy (x1
t ) produced by Algorithm 4 is an
ε-approximate (η, w)-smoothed local equilibrium with re-
spect to (cid:8)(x1

t , . . . , xk

, where

t−j, . . . , xk

j=0

t−j)(cid:9)t−1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

k
(cid:88)

i=1

ε =

Rw,Ai(T )
T − w

.

The proof, which we give in the appendix, follows from
the same method as for the reductions in Section 4, after
summing the regret bounds from each Ai.

7. Concluding Remarks

We have described how to extend the theory of online learn-
ing to non-convex loss functions, while permitting efﬁcient
algorithms. Our deﬁnitions give rise to efﬁcient online and
stochastic non-convex optimization algorithms that con-
verge to local optima of ﬁrst and second order. We give
a game theoretic solution concept which we call local equi-
librium, which, in contrast to existing solution concepts
such as Nash equilibrium, is efﬁciently attainable in any
non-convex game.

Efﬁcient Regret Minimization in Non-Convex Games

Ghadimi, Saeed and Lan, Guanghui. Stochastic ﬁrst-and
zeroth-order methods for nonconvex stochastic program-
ming. SIAM Journal on Optimization, 23(4):2341–2368,
2013.

Hart, Sergiu and Mas-Colell, Andreu. A simple adaptive
procedure leading to correlated equilibrium. Economet-
rica, 68(5):1127–1150, 2000.

Hazan, Elad.

Introduction to online convex optimiza-
Foundations and Trends in Optimization, 2
doi: 10.
URL http://dx.doi.org/

tion.
(3-4):157–325, 2016.
1561/2400000013.
10.1561/2400000013.

ISSN 2167-3888.

Hazan, Elad and Kale, Satyen. Computational equivalence
of ﬁxed points and no regret algorithms, and conver-
gence to equilibria. In Platt, J.c., Koller, D., Singer, Y.,
and Roweis, S. (eds.), Advances in Neural Information
Processing Systems 20, pp. 625–632. MIT Press, Cam-
bridge, MA, 2007. URL http://books.nips.cc/
papers/files/nips20/NIPS2007_0695.pdf.

Nesterov, Yurii. Introductory lectures on convex optimiza-
tion, volume 87. Springer Science & Business Media,
2004.

Nesterov, Yurii and Polyak, Boris T. Cubic regularization
of newton method and its global performance. Mathe-
matical Programming, 108(1):177–205, 2006.

Shalev-Shwartz, Shai. Online learning and online con-
vex optimization. Foundations and Trends in Machine
Learning, 4(2):107–194, 2011.

Vovk, Volodimir G. Aggregating strategies. In Proceedings
of the Third Annual Workshop on Computational Learn-
ing Theory, COLT ’90, pp. 371–386, 1990.

Acknowledgements

This material is based upon work supported by the National
Science Foundation under Grant No. 1523815. The authors
gratefully acknowledge Naman Agarwal, Brian Bullins,
Matt Weinberg, and Yi Zhang for helpful discussions.

References

Agarwal, Naman, Allen-Zhu, Zeyuan, Bullins, Brian,
Hazan, Elad, and Ma, Tengyu. Finding approximate lo-
cal minima for nonconvex optimization in linear time.
arXiv preprint arXiv:1611.01146, 2016a.

Agarwal, Naman, Bullins, Brian, and Hazan, Elad. Sec-
ond order stochastic optimization for machine learning
in linear time. arXiv preprint arXiv:1602.03943, 2016b.

Allen-Zhu, Zeyuan and Hazan, Elad. Variance reduction
In Proceedings of
for faster non-convex optimization.
The 33rd International Conference on Machine Learn-
ing, pp. 699–707, 2016.

Arora, Sanjeev, Hazan, Elad, and Kale, Satyen. The mul-
tiplicative weights update method: a meta-algorithm
Theory of Computing, 8(6):
and applications.
121–164, 2012.
10.4086/toc.2012.v008a006.
URL http://www.theoryofcomputing.org/
articles/v008a006.

doi:

Blum, A. and Mansour, Y. From external to internal regret.

In COLT, pp. 621–636, 2005.

Carmon, Yair, Duchi, John C., Hinder, Oliver, and Sidford,
Aaron. Accelerated methods for non-convex optimiza-
tion. arXiv preprint 1611.00756, 2016.

Cesa-Bianchi, Nicol`o and Lugosi, G´abor.

Prediction,
Learning, and Games. Cambridge University Press,
2006.

Cover, Thomas. Universal portfolios. Math. Finance, 1(1):

1–19, 1991.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. The Journal of Machine Learning Re-
search, 12:2121–2159, 2011.

Erdogdu, Murat A and Montanari, Andrea. Convergence
rates of sub-sampled newton methods. In Advances in
Neural Information Processing Systems, pp. 3034–3042,
2015.

Freund, Yoav and Schapire, Robert E. A decision-theoretic
generalization of on-line learning and an application to
boosting. J. Comput. Syst. Sci., 55(1):119–139, August
1997.

