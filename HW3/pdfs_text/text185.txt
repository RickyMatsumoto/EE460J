Sharp Minima Can Generalize For Deep Nets
Supplementary Material

Laurent Dinh 1 Razvan Pascanu 2 Samy Bengio 3 Yoshua Bengio 1 4

A Radial transformations

We show an elementary transformation to locally perturb the
geometry of a ﬁnite-dimensional vector space and therefore
affect the relative ﬂatness between a ﬁnite number minima,
at least in terms of spectral norm of the Hessian. We deﬁne
the function:

∀δ > 0, ∀ρ ∈]0, δ[,∀(r, ˆr) ∈ R+×]0, δ[,

ψ(r, ˆr, δ, ρ) = 1(cid:0)r /∈ [0, δ](cid:1) r + 1(cid:0)r ∈ [0, ˆr](cid:1) ρ
+ 1(cid:0)r ∈]ˆr, δ](cid:1) (cid:16)
r − δ
ˆr − δ
ψ(cid:48)(r, ˆr, δ, ρ) = 1(cid:0)r /∈ [0, δ](cid:1) + 1(cid:0)r ∈ [0, ˆr](cid:1) ρ
ˆr

(ρ − δ)

r
ˆr

(cid:17)

+ δ

+ 1(cid:0)r ∈]ˆr, δ](cid:1) ρ − δ
ˆr − δ

For a parameter ˆθ ∈ Θ and δ > 0, ρ ∈]0, δ[, ˆr ∈]0, δ[,
inspired by the radial ﬂows (Rezende & Mohamed, 2015)
in we can deﬁne the radial transformations

ψ

(cid:17)

(cid:16)
(cid:107)θ − ˆθ(cid:107)2, ˆr, δ, ρ
(cid:107)θ − ˆθ(cid:107)2

(cid:0)θ − ˆθ(cid:1) + ˆθ

∀θ ∈ Θ, g−1(θ) =

with Jacobian

∀θ ∈ Θ, (∇g−1)(θ) = ψ(cid:48)(r, ˆr, δ, ρ) In

− 1(cid:0)r ∈]ˆr, δ](cid:1) δ(ˆr − ρ)
r3(ˆr − δ)
+ 1(cid:0)r ∈]ˆr, δ](cid:1) δ(ˆr − ρ)
r(ˆr − δ)

In,

(θ − ˆθ)T (θ − ˆθ)

with r = (cid:107)θ − ˆθ(cid:107)2.

First, we can observe in Figure 1 that these transformations
are purely local: they only have an effect inside the ball

1Universit´e of Montr´eal, Montr´eal, Canada 2DeepMind, Lon-
don, United Kingdom 3Google Brain, Mountain View, United
States 4CIFAR Senior Fellow. Correspondence to: Laurent Dinh
<laurent.dinh@umontreal.ca>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

(a) ψ(r, ˆr, δ, ρ)

(b) g−1(θ)

Figure 1: An example of a radial transformation on a 2-
dimensional space. We can see that only the area in blue
and red, i.e. inside B2(ˆθ, δ), are affected. Best seen with
colors.

B2(ˆθ, δ). Through these transformations, you can arbitrarily
perturb the ranking between several minima in terms of
ﬂatness as described in subsection 5.1.

Sharp Minima Can Generalize For Deep Nets

B Considering the bias parameter

When we consider the bias parameter for a one (hidden)
layer neural network, the non-negative homogeneity prop-
erty translates into

Without loss of generality, we consider (∇θ1 L)(θ1, θ2) (cid:54)= 0.
Then the limit of the norm

(cid:107)(∇L)(αθ1, α−1θ2)(cid:107)2

2 = α−2(cid:107)(∇θ1L)(θ1, θ2)(cid:107)2
2
+ α2(cid:107)(∇θ2 L)(θ1, θ2)(cid:107)2
2

y = φrect(x · θ1 + b1) · θ2 + b2

= φrect(x · αθ1 + αb1) · α−1θ2 + b2,

of the gradient goes to +∞ as α goes to 0. Therefore, L is
not Lipschitz continuous.

which results in conclusions similar to section 4.

For a deeper rectiﬁed neural network, this property results
in

This result can be generalized to several other models con-
taining a one-hidden layer rectiﬁed neural network, includ-
ing deeper rectiﬁed networks.

y = φrect

φrect

(cid:0) · · · φrect(x · θ1 + b1) · · · (cid:1) · θK−1 + bK−1

(cid:16)

(cid:17)

D Euclidean distance and input

· θK + bK
(cid:16)

= φrect

φrect

(cid:0) · · · φrect(x · α1θ1 + α1b1) · · · (cid:1)

· αK−1θK−1 +

αkbK−1

· αKθK + bK

(cid:17)

K−1
(cid:89)

k=1

for (cid:81)K
values of the Hessian that can be arbitrarily inﬂuenced.

k=1 αk = 1. This can decrease the amount of eigen-

C Rectiﬁed neural network and

Lipschitz continuity

Relative to recent works (Hardt et al., 2016; Gonen &
Shalev-Shwartz, 2017) assuming Lipschitz continuity of the
loss function to derive uniform stability bound, we make
the following observation:

Theorem 1. For a one-hidden layer rectiﬁed neural network
of the form

y = φrect(x · θ1) · θ2,

if L is not constant, then it is not Lipschitz continuous.

Proof. Since a Lipschitz function is necessarily absolutely
continuous, we will consider the cases where L is absolutely
continuous. First, if L has zero gradient almost everywhere,
then L is constant.

Now, if there is a point θ with non-zero gradient, then by
writing

Dα =

(∇L)(θ1, θ2) = [(∇θ1L)(θ1, θ2)
(∇θ2L)(θ1, θ2)],

we have

(∇L)(αθ1, α−1θ2) = [α−1(∇θ1L)(θ1, θ2)
α(∇θ2L)(θ1, θ2)].

representation

A natural consequence of subsectio 5.2 is that metrics rely-
ing on Euclidean metric like mean square error or Earth-
mover distance will rank very differently models depending
on the input representation chosen. Therefore, the choice
of input representation is critical when ranking different
models based on these metrics. Indeed, bijective transfor-
mations as simple as feature standardization or whitening
can change the metric signiﬁcantly.

On the contrary, ranking resulting from metrics like f-
divergence and log-likelihood are not perturbed by bijective
transformations because of the change of variables formula.

E Eigenspectrum of Hessian

In section 4.2, we show how to manipulate the spectral ra-
dius and trace of the Hessian as a notion of sharpness. In
However, some notion of sharpness might take into account
the entire eigenspectrum of the Hessian as opposed to its
largest eigenvalue, for instance, Chaudhari et al. (2017) de-
scribe the notion of wide valleys, allowing the presence of
very few large eigenvalues. We can generalize the trans-
formations between observationally equivalent parameters
to deeper neural networks with K − 1 hidden layers: for
αk > 0, Tα : (θk)k≤K (cid:55)→ (αkθk)k∈K with (cid:81)K
k=1 αk = 1.
If we deﬁne








In1
α−1
1
0
...
0

0
In2
α−1
2
...
0

0
· · ·
0
· · ·
...
. . .
InK
· · · α−1
K








then the ﬁrst and second derivatives at Tα(θ) will be
(∇L)(cid:0)Tα(θ)(cid:1) =(∇L)(θ)Dα
(∇2L)(cid:0)Tα(θ)(cid:1) =Dα(∇2L)(θ)Dα.

We will show to which extent you can increase several
eigenvalues of (∇2L)(cid:0)Tα(θ)(cid:1) by varying α.

Chaudhari, Pratik, Choromanska, Anna, Soatto, Stefano, Le-
Cun, Yann, Baldassi, Carlo, Borgs, Christian, Chayes, Jen-
nifer, Sagun, Levent, and Zecchina, Riccardo. Entropy-sgd:
In ICLR’2017,
Biasing gradient descent into wide valleys.
arXiv:1611.01838, 2017.

Gonen, Alon and Shalev-Shwartz, Shai. Fast rates for empirical
risk minimization of strict saddle problems. arXiv preprint
arXiv:1701.04271, 2017.

Hardt, Moritz, Recht, Ben, and Singer, Yoram. Train faster, gener-
alize better: Stability of stochastic gradient descent. In Balcan,
Maria-Florina and Weinberger, Kilian Q. (eds.), Proceedings
of the 33nd International Conference on Machine Learning,
ICML 2016, New York City, NY, USA, June 19-24, 2016, vol-
ume 48 of JMLR Workshop and Conference Proceedings, pp.
1225–1234. JMLR.org, 2016. URL http://jmlr.org/
proceedings/papers/v48/hardt16.html.

Klyachko, Alexander A. Random walks on symmetric spaces
and inequalities for matrix spectra. Linear Algebra and its
Applications, 319(1-3):37–59, 2000.

Rezende, Danilo Jimenez and Mohamed, Shakir. Variational in-
ference with normalizing ﬂows. In Bach, Francis R. and Blei,
David M. (eds.), Proceedings of the 32nd International Confer-
ence on Machine Learning, ICML 2015, Lille, France, 6-11 July
2015, volume 37 of JMLR Workshop and Conference Proceed-
ings, pp. 1530–1538. JMLR.org, 2015. URL http://jmlr.
org/proceedings/papers/v37/rezende15.html.

Sagun, Levent, Bottou, L´eon, and LeCun, Yann. Singularity of
the hessian in deep learning. arXiv preprint arXiv:1611.07476,
2016.

Sharp Minima Can Generalize For Deep Nets

Deﬁnition 1. For each n × n matrix A, we deﬁne the vector
λ(A) of sorted singular values of A with their multiplicity
λ1(A) ≥ λ2(A) ≥ · · · ≥ λn(A).

References

If A is symmetric positive semi-deﬁnite, λ(A) is also the
vector of its sorted eigenvalues.

Theorem 2. For a (K − 1)-hidden layer rectiﬁed neural
network of the form

y = φrect(φrect(· · · φrect(x · θ1) · · · ) · θK−1) · θK,

and critical point θ = (θk)k≤K being a minimum for L,
such that (∇2L)(θ) has rank r = rank(cid:0)(∇2L)(θ)(cid:1), ∀M >
eigenvalues are
0, ∃α > 0 such that
greater than M .

r − mink≤K(nk)

(cid:16)

(cid:17)

√

√

M the principal
Proof. For simplicity, we will note
square root of a symmetric positive-semideﬁnite matrix
M . The eigenvalues of
M are the square root of the
eigenvalues of M and are its singular values. By deﬁni-
tion, the singular values of (cid:112)(∇2L)(θ)Dα are the square
root of the eigenvalues of Dα(∇2L)(θ)Dα. Without loss
of generality, we consider mink≤K(nk) = nK and choose
∀k < K, αk = β−1 and αK = βK−1. Since Dα and
(cid:112)(∇2L)(θ) are positive symmetric semi-deﬁnite matrices,
we can apply the multiplicative Horn inequalities (Klyachko,
2000) on singular values of the product (cid:112)(∇2L)(θ)Dα:

∀i ≤ n,j ≤ (n − nK),
λi+j−n

(cid:0)(∇2L)(θ)D2

α

(cid:1) ≥ λi

(cid:0)(∇2L)(θ)(cid:1)β2.

By choosing β >

(cid:114)

λr

M

(cid:0)(∇2L)(θ)(cid:1) , since we have
(cid:0)(∇2L)(θ)(cid:1) > 0 we can

(cid:0)(∇2L)(θ)(cid:1) ≥ λr

∀i ≤ r, λi
conclude that

∀i ≤ (r − nK),
(cid:0)(∇2L)(θ)D2

α

λi

(cid:1) ≥ λi+nk
≥ λr

(cid:0)(∇2L)(θ)(cid:1)β2
(cid:0)(∇2L)(θ)(cid:1)β2 > M.

(cid:17)

(cid:16)

r − mink≤K(nk)

It means that there exists an observationally equivalent pa-
rameter with at least
arbitrarily large
eigenvalues. Since Sagun et al. (2016) seems to suggests that
rank deﬁciency in the Hessian is due to over-parametrization
(cid:17)
of the model, one could conjecture that
can be high for thin and deep neural networks, resulting in
a majority of large eigenvalues. Therefore, it would still
be possible to obtain an equivalent parameter with large
Hessian eigenvalues, i.e. sharp in multiple directions.

r − mink≤K(nk)

(cid:16)

