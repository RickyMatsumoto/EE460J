Prox-PDA: The Proximal Primal-Dual Algorithm for Fast Distributed
Nonconvex Optimization and Learning Over Networks

Mingyi Hong 1 Davood Hajinezhad 1 Ming-Min Zhao 2

Abstract
In this paper we consider nonconvex optimiza-
tion and learning over a network of distributed
nodes. We develop a Proximal Primal-Dual Al-
gorithm (Prox-PDA), which enables the network
nodes to distributedly and collectively compute
the set of ﬁrst-order stationary solutions in a
global sublinear manner [with a rate of O(1/r),
where r is the iteration counter]. To the best
of our knowledge, this is the ﬁrst algorithm that
enables distributed nonconvex optimization with
global sublinear rate guarantees. Our numerical
experiments also demonstrate the effectiveness
of the proposed algorithm.

1. Introduction

We consider the following optimization problem

min
z∈RM

g(z) :=

fi(z),

(1)

N
(cid:88)

i=1

where each fi, i ∈ {1, · · · , N } := [N ] is a nonconvex cost
function, and we assume that it is smooth and has Lipschitz
continuous gradient.

Such a ﬁnite sum problem plays a central role in machine
learning and signal/information processing (Cevher et al.,
2014; Hong et al., 2016). In particular, in the class of em-
pirical risk minimization (ERM) problem, z represents the
feature vectors to be learned, and each fi can represent: 1)
a mini-batch of (possibly nonconvex) loss functions model-
ing data ﬁdelity (Antoniadis et al., 2009); 2) nonconvex ac-
tivation functions of neural networks (Allen-Zhu & Hazan,

of

Industrial

of
Iowa State University, Ames,

1Department
and Manufacturing Sys-
IA, USA
tems Engineering,
2College
and Electronic Engi-
neering, Zhejiang University, China.
Correspondence
to: Mingyi Hong <mingyi@iastate.edu>, Davood Ha-
jinezhad <dhaji@iastate.edu>, Ming-Min Zhao <zmm-
black@zju.edu.cn>.

Information Science

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

2016); 3) nonconvex utility functions used in applications
such as resource allocation (Bjornson & Jorswieck, 2013).
Recently, a number of works in machine learning commu-
nity have been focused on designing fast algorithms for
solving problem (1) in centralized setting; e.g., SAG (De-
fazio et al., 2014), SAGA (Schmidt et al., 2013), and SVRG
(Johnson & Zhang, 2013) for convex problems, and (Reddi
et al., 2016; Allen-Zhu & Hazan, 2016; Hajinezhad et al.,
2016b; Rahimpour et al., 2016) for nonconvex problems.

In this work, we are interested in designing algorithms
that solve problem (1) in a distributed manner. In partic-
ular, we focus on the scenario where each fi (or equiva-
lently, each subset of data points in the ERM problem) is
available locally at a given computing node i ∈ [N ], and
the nodes are connected via a network. Clearly, such dis-
tributed optimization and learning scenario is important for
machine learning, because in contemporary applications
such as document topic modeling and/or social network
data analysis, oftentimes data corporas are stored in geo-
graphically distributed locations without any central con-
troller managing the entire network of nodes; see (Forero
et al., 2010; Yan et al., 2013; Rahmani & Atia, 2015; Aybat
& Hamedani, 2016).

Related Works. Distributed convex optimization and
learning has been thoroughly investigated in the literature.
In (Nedic & Ozdaglar, 2009b), the authors propose a dis-
tributed subgradient algorithm (DSG), which allows the
agents to jointly optimize problem (1). Subsequently, many
variants of DSG have been proposed, either with special as-
sumptions on the underlying graph, or having additional
structures of the problem; see, e.g., (Lobel & Ozdaglar,
2011; Lobel et al., 2011; Nedic & Olshevsky, 2015). The
r) under cer-
rate of convergence for DSG is O(log(r)/
tain diminishing stepsize rules. Recently, a number of al-
gorithms such as the exact ﬁrst-order algorithm (EXTRA)
(Shi et al., 2014) and DLM (Ling et al., 2015) have been
proposed, which use constant stepsize and achieve faster
O(1/r) rate for convex problems. Recent works that ap-
ply distributed optimization algorithms to machine learn-
ing applications include (Scardapane et al., 2016; Aybat &
Hamedani, 2016; Scardapane & Lorenzo, 2016).

√

On the other hand,

there has been little work for dis-

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

tributed optimization and learning when the objective func-
tion involves nonconvex problems. A dual subgradient
method has been proposed in (Zhu & Martinez, 2010),
which relaxes the exact consensus constraint. In (Bianchi
& Jakubowicz, 2013) a stochastic projection algorithm us-
ing diminishing stepsizes has been proposed. An ADMM
based algorithm has been presented in (Hong et al., 2014;
Hajinezhad & Hong, 2015) for a special type of prob-
lem called global consensus, where all distributed nodes
are directly connected to a central controller. Utilizing
certain convexiﬁcation decomposition technique the au-
thors of (Lorenzo & Scutari, 2016) designed an algorithm
named NEXT, which converges to the set of stationary so-
lutions when using diminishing stepsizes. To the best of
our knowledge, no multi agent distributed algorithm is able
to guarantee global sublinear convergence rate for problem
(1).

Our Contributions. In this work, we propose a proximal
primal-dual algorithm (Prox-PDA) for problem (1) over an
undirected connected network. We show that Prox-PDA
converges to the set of stationary solutions of problem (1)
(satisfying the ﬁrst-order optimality condition) in a glob-
ally sublinear manner. We also show that Prox-PDA can
be extended in several directions to improve its practical
performance. To the best of our knowledge, this is the ﬁrst
algorithm that is capable of achieving global sublinear con-
vergence rate for distributed non-convex optimization.

Further, our work reveals an interesting connection be-
tween the primal-dual based algorithm Prox-PDA and the
primal-only fast distributed algorithms such as EXTRA
(Shi et al., 2014). Such new insight of the connection be-
tween primal-dual and primal-only algorithms could be of
independent interest for the optimization community. Fi-
nally, we generalize the theory for Prox-PDA based al-
gorithms to a challenging distributed matrix factorization
problem.

2. System Model

Deﬁne a graph G := {V, E}, where V and E are the
node and edge sets; Let |V| = N and |E| = E. Each
node v ∈ V represents an agent in the network, and each
edge eij = (i, j) ∈ E indicates that node i and j are
neighbors; see Fig.1(Left). Assume that each node i can
only communicate with its immediate neighbors, deﬁned
as Ni := {j | (i, j) ∈ V}, with |Ni| = di. The distributed
version of problem (1) is given as below

N
(cid:88)

i=1

min
xi∈RM

f (x) :=

fi(xi), s.t. xi = xj, ∀ (i, j) ∈ E.

(2)

Clearly the above problem is equivalent to (1) as long as G
is connected. For notational simplicity, deﬁne x := {xi} ∈
RNM ×1, and Q := N × M .

˜A =



1 −1
0
1


1
0

0
0







.

0
0
0 −1
0 −1
1 −1

Figure 1. (Left) An undirected Connected Network,
(Right) Incidence Matrix.

To proceed, let us introduce a few useful quantities related
to graph G.
• The incidence matrix ˜A ∈ RE×N is a matrix with entires
˜A(k, i) = 1 and ˜A(k, j) = −1 if k = (i, j) ∈ E with j > i,
and all the rest of the entries being zero. For example, for
the network in Fig.1 (Left); the incidence matrix is given in
Fig.1 (Right). Deﬁne the extended incidence matrix as

A := ˜A ⊗ IM ∈ REM ×Q,

(3)

where ⊗ denotes the Kronecker product.
• The Degree matrix ˜D ∈ RN ×N is given by ˜D :=
diag[d1, · · · , dN ]; Let D := ˜D ⊗ IM ∈ RQ×Q.

• The signed and the signless Laplacian matrices (denoted
as L− and L+ respectively), are given below

L− := A(cid:62)A ∈ RQ×Q, L+ := 2D − A(cid:62)A ∈ RQ×Q.

(4)

Using the above notations, one can verify that problem (2)
can be written in the following compact form

min
x∈RQ

f (x),

s.t. Ax = 0.

(5)

3. The Prox-PDA Algorithm

The proposed algorithm builds upon the classical aug-
mented Lagrangian (AL) method (Bertsekas, 1982; Powell,
1969). Let us deﬁne the AL function for (5) as

Lβ(x, µ) = f (x) + (cid:104)µ, Ax(cid:105) +

(cid:107)Ax(cid:107)2,

(6)

β
2

where µ ∈ RQ is the dual variable; β > 0 is a penalty
parameter. Let B ∈ RQ×Q be some arbitrary matrix to be
determined shortly. Then the proposed algorithm is given
in the table below (Algorithm 1).
In Prox-PDA, the primal iteration (7a) minimizes the aug-
mented Lagrangian plus a proximal term β
BT B.
We emphasize that the proximal term is critical in both the
algorithm implementation and the analysis. It is used to en-
sure the following key properties:
(1). The primal problem is strongly convex;
(2). The primal problem is decomposable over different
network nodes, hence distributedly implementable.
To see the ﬁrst point, suppose BT B is chosen such that
AT A + BT B (cid:23) IQ, and that f (x) has Lipschitz gradient.
Then by a result in (Zlobec, 2005)[Theorem 2.1], we know

2 (cid:107)x − xr(cid:107)2

e12e24e14e34N1N2N4N3Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

Algorithm 1 The Prox-PDA Algorithm
1: At iteration 0, initialize µ0 = 0 and x0 ∈ RQ.
2: At each iteration r + 1, update variables by:
β
2

f (x) + (cid:104)µr, Ax(cid:105) +

(cid:107)Ax(cid:107)2

xr+1 = arg min
x∈RQ
β
2

+

µr+1 = µr + βAxr+1.

(cid:107)x − xr(cid:107)2

BT B;

(7a)

(7b)

that there exists β > 0 large enough such that the objec-
tive function of (7a) is strongly convex. To see the second
point, Let B := |A|, where the absolute value is taken for
each component of A. It can be veriﬁed that BT B = L+,
and step (7a) becomes

xr+1 = arg min

fi(xi) + (cid:104)µr, Ax(cid:105)

+

xT L−x +

(x − xr)T L+(x − xr)

β
2

N
(cid:88)

i=1

x

β
2

N
(cid:88)

i=1

= arg min

x

fi(xi) + (cid:104)µr, Ax(cid:105) + βxT Dx − βxT L+xr.

Clearly this problem is separable over the nodes, therefore
it can be solved completely distributedly.
4. The Convergence Analysis

In this section we provide convergence analysis for Algo-
rithm 1. The key in the analysis is the construction of a
novel potential function, which decreases at every iteration
of the algorithm.
In particular, the constructed potential
function is a conic combination of the AL function and the
size of the violation of the consensus constraint, therefore it
measures the progress of both the primal and dual updates.

We ﬁrst state our main assumptions below.

[A1.] The function f (x) is differentiable and has Lipschitz
continuous gradient, i.e.,

(cid:107)∇f (x) − ∇f (x)(cid:107) ≤ L(cid:107)x − y(cid:107),

∀ x, y ∈ RQ.

Further assume that AT A + BT B (cid:23) IQ.

[A2.] There exists a constant δ > 0 such that

∃ f > −∞,

s.t. f (x) +

(cid:107)Ax(cid:107)2 ≥ f , ∀ x ∈ RQ.

δ
2

Without loss of generality we will assume that f = 0.
Below we provide a few nonconvex smooth functions that
satisfy our assumptions, all of which are commonly used
as activation functions for neural networks.
• The sigmoid function sigmoid(x) = 1
• The arctan and tanh function.
• The logit function logit(x) = ex

1+e−x .

ex+1 .

4.1. The Analysis Steps

Below we provide the analysis of Prox-PDA. First we pro-
vide a bound on the size of the constraint violation using
a quantity related to the primal iterates. Let σmin denotes
the smallest non-zero eigenvalue of AT A, and we deﬁne
wr := (xr+1 − xr) − (xr − xr−1) for notational simplicity.
We have the following result.
Lemma 1 Suppose Assumptions [A1] and [A2] are satis-
ﬁed. Then the following is true for Prox-PDA

(cid:107)µr+1 − µr(cid:107)2

1
β

≤

2L2
βσmin

(cid:13)xr − xr+1(cid:13)
(cid:13)
2
(cid:13)

+

2β
σmin

(cid:107)BT Bwr(cid:107)2.

(8)

Then we bound the descent of the AL function.
Lemma 2 Suppose Assumptions [A1] and [A2] are satis-
ﬁed. Then the following is true for Algorithm 1
2β(cid:107)BT B(cid:107)
Lβ(xr+1, µr+1) − Lβ(xr, µr) ≤
σmin

(cid:107)wr(cid:107)2

BT B

−

(cid:18) β − L
2

−

2L2
βσmin

(cid:19)

(cid:107)xr+1 − xr(cid:107)2.

(9)

A key observation from Lemma 2 is that no matter how
large β is, the rhs of (9) cannot be made negative. This
observation suggests that in contrast to (Hong et al., 2014;
Hajinezhad et al., 2016a) the augmented Lagrangian alone
cannot serve as the potential function for Prox-PDA. In
search for an appropriate potential function, we need a new
object that is decreasing in the order of β (cid:107)wr(cid:107)2
BT B. The
following lemma shows that the descent of the sum of the
constraint violation and the proximal term has the desired
property.

Lemma 3 Suppose Assumption [A1] is satisﬁed. Then the
following is true

(cid:0)(cid:107)Axr+1(cid:107)2 + (cid:107)xr+1 − xr(cid:107)2

β
2
≤ L(cid:107)xr+1 − xr(cid:107)2 +

(cid:1)

BT B

β
2

(cid:0)(cid:107)Axr(cid:107)2 + (cid:107)xr − xr−1(cid:107)2

(cid:1)

BT B

−

β
2

(cid:0)(cid:107)wr(cid:107)2

BT B + (cid:107)A(xr+1 − xr)(cid:107)2(cid:1) .

(10)

is

BT B

interesting to observe that
(cid:1),

It
the new object,
β/2 (cid:0)(cid:107)Axr+1(cid:107)2 + (cid:107)xr+1 − xr(cid:107)2
increases
in
L(cid:107)xr+1 − xr(cid:107)2 and decreases in β/2(cid:107)wr(cid:107)2
BT B, while the
AL behaves in an opposite manner (cf. Lemma 2). More
importantly, in our new object, the constant in front of
(cid:107)xr+1 − xr(cid:107)2 is independent of β. Although neither of
these two objects decreases by itself, quite surprisingly, a
proper conic combination of these two objects decreases at
every iteration of Prox-PDA. To precisely state the claim,
let us deﬁne the potential function for Algorithm 1 as

Pc,β(xr+1, xr, µr+1) := Lβ(xr+1, µr+1)
(cid:1) ,
(cid:0)(cid:107)Axr+1(cid:107)2 + (cid:107)xr+1 − xr(cid:107)2

+

BT B

cβ
2

(11)

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

where c > 0 is some constant to be determined later. We
have the following result.

Lemma 4 Suppose the assumptions made in Lemmas 1 –
3 are satisﬁed. Then we have the following

Pc,β(xr+1, xr, µr+1) ≤ Pc,β(xr, xr−1, µr)

−

−

(cid:18) β − L
2
(cid:18) cβ
2

−

−

(cid:19)

− cL

2L2
βσmin
2β(cid:107)BT B(cid:107)F
σmin

(cid:19)

(cid:107)xr+1 − xr(cid:107)2

(cid:107)wr(cid:107)2

BT B .

(12)

From the above analysis, it is easy to see that as long as c
and β are sufﬁciently large, the potential function decreases
at each iteration of Prox-PDA. Below we derive the precise
bounds for c and β. First, a sufﬁcient condition for c is
given below (note, that δ > 0 is deﬁned in Assumption
[A2])

c ≥ max

(cid:26) δ
L

,

4(cid:107)BT B(cid:107)F
σmin

(cid:27)

.

(13)

Second, for any given c, we need β to satisfy β−L
2L2
βσmin

− cL > 0, which implies the following

2 −

(cid:115)



L
2



 .

16L2
σmin

β >

2c + 1 +

(2c + 1)2 +

(14)

We conclude that if both (13) and (14) are satisﬁed, then
the potential function Pc,β(xr+1, xr, µr+1) decreases at
every iteration.

Our next step shows that by using the particular choices of
c and β in (13) and (14), the constructed potential function
is lower bounded.

Lemma 5 Suppose [A1] - [A2] are satisﬁed, and (c, β)
are chosen according to (13) and (14). Then the follow-
ing statement holds true

∃ P > −∞ s.t. Pc,β(xr+1, xr, µr+1) ≥ P , ∀ r > 0.
Now we are ready to present the main result of this section.
To this end, deﬁne Q(xr+1, µr) as the optimality gap of
problem (5), given by

Q(xr+1, µr) := (cid:107)∇xLβ(xr+1, µr)(cid:107)2 + (cid:107)Axr+1(cid:107)2.

(15)

It is easy to see that Q(xr+1, µr) → 0 implies that any
limit point (x∗, µ∗), if it exists, is a KKT point of (5) that
satisﬁes the following conditions

0 = ∇f (x∗) + AT µ∗, Ax∗ = 0.

(16)

In the following we show that the gap Q(·) not only de-
creases to zero, but does so in a sublinear manner.

Theorem 1 Suppose Assumption A and the conditions (13)
and (14) are satisﬁed. Then we have
• Eventual Consensus:

lim
r→∞

µr+1 − µr → 0,

lim
r→∞

Axr → 0.

• Convergence to Stationary Points: Every limit point of
the iterates {xr, µr} generated by Algorithm 1 converges
to a KKT point of problem (5). Further, Q(xr+1, µr) → 0.
• Sublinear Convergence Rate: For any given ϕ > 0,
let us deﬁne T to be the ﬁrst time that the optimality gap
reaches below ϕ, i.e.,

T := arg min

Q(xr+1, µr) ≤ ϕ.

r

Then for some ν > 0, we have ϕ ≤ ν
optimality gap Q(xr+1, µr) converges sublinearly.

T −1 . That is, the

5. Variants of Prox-PDA

In this section, we discuss two important extensions of the
Prox-PDA, one allows the x-problem (7a) to be solved
inexactly, while the second allows the use of increasing
penalty parameter β. In many practical applications, ex-
actly minimizing the augmented Lagrangian may not be
easy. Therefore, we propose the proximal gradient primal-
dual algorithm (Prox-GPDA), whose main steps are given
below

xr+1 = arg min
x∈RQ

(cid:104)∇f (xr), x − xr(cid:105) + (cid:104)µr, Ax(cid:105)

+

(cid:107)Ax(cid:107)2 +

(cid:107)x − xr(cid:107)2

BT B;

β
2

β
2

µr+1 = µr + βAxr+1.

(17)

(18)

The analysis of this algorithm follows similar steps as that
for Prox-PDA. For detailed discussion see (Hong, 2016).

Our second variant do not require to explicitly compute the
bound for β given in (14). Indeed, the bounds on β de-
rived in the previous sections are the worst case bounds,
and algorithms that use stepsizes that strictly satisfy such
bounds may be slow at the beginning. In practice, one may
prefer to start with a small penalty parameter and gradu-
ally increase it. We denote this algorithm by Prox-PDA-IP,
whose main steps are given below

xr+1 = arg min
x∈RQ

f (x) + (cid:104)µr, Ax(cid:105)

+

βr+1
2

βr+1
2
µr+1 = µr + βr+1Axr+1.

(cid:107)Ax(cid:107)2 +

(cid:107)x − xr(cid:107)2

BT B;

(19)

(20)

Note that one can also replace f (x) in (19) by (cid:104)∇f (xr), x−
xr(cid:105) to obtain a similar variant for Prox-GPDA denoted by
Prox-GPDA-IP. Throughout this section we will still as-

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

sume that Assumption A holds true. Further, we will as-
sume that βr satisﬁes the following conditions

∇f (xr) − ∇f (xr−1) + βL−xr + 2βD(xr+1 − xr)
− βL+(xr − xr−1) = 0,

∞
(cid:88)

1
βr → 0,

1
βr = ∞, βr+1 ≥ βr,
(βr+1 − βr) < κ, for some ﬁnite κ > 0.

r=1

max
r

(21)

Also without loss of generality we will assume that

where we have used the fact that AT (µr − µr−1) =
βAT Axr = βL−xr. Rearranging terms, we have

xr+1 = xr −

D−1 (cid:0)∇f (xr) − ∇f (xr−1)(cid:1)

1
2β

BT B (cid:31) 0,

and (cid:107)BT B(cid:107)F > 1.

(22)

D−1(L+ − L−)xr −

D−1L+xr−1

Note that this is always possible, by adding an identity
matrix to BT B if necessary.

The analysis for Prox-PDA-IP is long and technical, there-
fore we refer the readers to (Hong, 2016). The key step is
to construct a new potential function, given below

Pβr+1,c(xr+1, xr, µr+1) = Lβr+1(xr+1, µr+1)

+

cβr+1βr
2

(cid:107)Axr+1(cid:107)2 +

(cid:107)xr − xr+1(cid:107)2

BT B.

cβr+1βr
2

The insight here is that
in order to achieve the de-
sired descent, in the potential function the coefﬁcients for
(cid:107)xr+1 − xr(cid:107)2
BT B and (cid:107)Axr+1(cid:107)2 should be proportional to
O (cid:0)(βr)2(cid:1). We have the following theorem regarding to the
convergence of Prox-PDA-IP.

Theorem 2 Suppose Assumption A and (21) are satisﬁed.
Suppose that B is selected such that (22) holds true. Then
the following hold for Prox-PDA-IP

• Eventual Consensus:

lim
r→∞

µr+1 − µr → 0,

lim
r→∞

Axr → 0.

• Convergence to KKT Points: Every limit point of the
iterates {xr, µr} generated by Prox-PDA-IP converges to
a KKT point of problem (5). Further, Q(xr+1, µr) → 0.

1
2

+

1
2
= xr −

1
2β

−

(I + W )xr−1,

1
2

D−1 (cid:0)∇f (xr) − ∇f (xr−1)(cid:1) + W xr

(23)

where in the last equality we have deﬁned the weight ma-
trix W := 1
2 D−1(L+ − L−), which is a row stochastic
matrix.

Iteration (23) has the same form as the EXTRA algorithm
given in (Shi et al., 2014), therefore we can conclude that
EXTRA is a special case of Prox-GPDA. Moreover, by ap-
pealing to our analysis in Section 5, it readily follows that
EXTRA works for the nonconvex distributed optimization
problem as well.

We remark that each node i can distributedly implement
iteration (23) by performing the following

i = xr
xr+1

i −

1
2βdi

i ) − ∇fi(xr−1

i

)(cid:1)

(cid:88)

+

j∈N (i)

1
di

xr
j −

1
2



j∈N (i)

1
di



j + xr−1
xr−1

i

 .

(24)

(cid:0)∇fi(xr


(cid:88)

Clearly, at iteration r + 1, besides the local gradient in-
formation, node i only needs the aggregated information
from its neighbors, (cid:80)
j∈N (i) xr
j . Therefore the algorithm
is distributedly implementable.

6. Connections and Discussions

7. Distributed Matrix Factorization

In this section we present an interesting observation which
establishes links between the so-called EXTRA algorithm
(Shi et al., 2014) (developed for distributed, but convex op-
timization) and the Prox-GPDA.

Speciﬁcally, the optimality condition of the x-update step
(17) is given by

∇f (xr) + AT (µr + βAxr+1) + β(BT B(xr+1 − xr)) = 0.

Utilizing the fact that AT A = L−, BT B = L+ and L+ +
L− = 2D, we have

∇f (xr) + AT µr + 2βDxr+1 − βL+xr = 0.

Subtracting the same equation evaluated at the previous
iteration, we obtain

In this section we study a variant of the Prox-PDA/Prox-
PDA-IP for the following distributed matrix factorization
problem (Ling et al., 2012)
1
(cid:107)XY − Z(cid:107)2
2

F + η(cid:107)X(cid:107)2

F + h(Y )

min
X,Y

(25)

=

N
(cid:88)

i=1

1
2

s.t. (cid:107)yi(cid:107)2 ≤ τ, ∀ i

(cid:107)Xyi − zi(cid:107)2 + γ(cid:107)X(cid:107)2

F + hi(yi),

where X ∈ RM ×K, Y ∈ RK×N ; for each i, yi ∈ RK
consists of one column of Y ; Z ∈ RM ×N is some known
matrix; h(Y ) := (cid:80)N
i=1 hi(yi) is some convex but possibly
nonsmooth penalization term; η > 0 is some given con-
stant; for notation simplicity we have deﬁned γ := 1/N η.

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

It is easy to extend the above formulation to the case where
Y and Z both have N P columns, and each yi and zi con-
sists of P columns of Y and Z respectively.

Algorithm 2 Prox-PDA for Distr. Matrix Factorization
1: At iteration 0, initialize Ω0 = 0, and X 0, y0
2: At each iteration r + 1, update variables by:

We assume that h(Y ) is lower bounded over dom (h). One
application of problem (25) is the distributed sparse dic-
tionary learning problem where X is the dictionary to be
learned, each zi is a training data sample, and each yi is the
sparse coefﬁcient corresponding to the particular training
sample zi. The constraint (cid:107)yi(cid:107)2 ≤ τ simply says that the
size of the coefﬁcient must be bounded.

Consider a distributed scenario where N agents form a
graph {V, E}, each having a column of Y . We reformu-
late problem (25) as
(cid:18) 1
2

(cid:107)Xiyi − zi(cid:107)2 + hi(yi) + γ(cid:107)Xi(cid:107)2
F

min
{Xi},{yi}

N
(cid:88)

(cid:19)

i=1

s.t. (cid:107)yi(cid:107)2 ≤ τ, ∀ i Xi = Xj, ∀ (i, j) ∈ E.

the variables Xi, and deﬁne X :=
Let us stack all
[X1; X2; · · · ; XN ] ∈ RNM ×K. Deﬁne the block signed
incidence matrix as A = ˜A ⊗ IM ∈ REM ×NM , where
A is the standard graph incidence matrix. Deﬁne the block
signless incidence matrix B ∈ REM ×NM similarly. If the
graph is connected, then the condition AX = 0 implies
network-wide consensus. We formulate the distributed ma-
trix factorization problem as

min
{Xi},{yi}

:=

N
(cid:88)

i=1

(cid:18) 1
2

f (X, Y ) + h(Y )

(cid:107)Xiyi − zi(cid:107)2 + γ(cid:107)Xi(cid:107)2

F + hi(yi)

(cid:19)

Clearly the above problem does not satisfy Assumption
A, because the objective function is not smooth, and nei-
ther ∇Xf (X, Y ) nor ∇Y f (X, Y ) is Lipschitz continuous.
The latter fact poses signiﬁcant difﬁculty in algorithm de-
velopment and analysis. Deﬁne the block-signed/signless
Laplacians as

L− = AT A, L+ = BT B.

The AL function for the above problem is given by

Lβ(X, Y, Ω) =

(cid:107)Xiyi − zi(cid:107)2 + γ(cid:107)Xi(cid:107)2

F + hi(yi)

(cid:18) 1
2

N
(cid:88)

i=1
β
2

+ (cid:104)Ω, AX(cid:105) +

(cid:104)AX, AX(cid:105),

(27)

(cid:19)

(28)

where Ω := {Ωe} ∈ REM ×K is the matrix of the dual
variable, with Ωe ∈ RM ×K being the dual variable for the
consensus constraint on link e, i.e, Xi = Xj, e = (i, j).

Let us generalize Algorithm 1 for distributed matrix fac-
torization given in Algorithm 2. In Algorithm 2 we have
introduced a sequence {θr
i ≥ 0} which measures the size

i = (cid:107)X r
θr

i yr

∀ i;

i − zi(cid:107)2,
1
2

(cid:107)yi(cid:107)2≤τ

yr+1
i = arg min

(cid:107)X r

i yi − zi(cid:107)2 + hi(yi)

(cid:107)yi − yr

i (cid:107)2,

∀ i;

X r+1 = arg min
X

f (X, Y r+1) + (cid:104)Ωr, AX(cid:105)

+

(cid:104)AX, AX(cid:105) +

(cid:104)B(X − X r), B(X − X r)(cid:105);

β
2

Ωr+1 = Ωr + βAX r+1.

+

θr
i
2

β
2

(29a)

(29b)

(29c)

(29d)

of the local factorization error. We note that including the
proximal term θr
i (cid:107)2 is the key to achieve conver-
gence for Algorithm 2.

2 (cid:107)yi − yr

i

that

Second,

Let us comment on the distributed implementation of
the y subproblem
the algorithm.
First note that
is,
is naturally distributed to each node,
(29b)
information is needed to perform the up-
only local
the X subproblem (29c) can also be
date.
decomposed into N subproblems, one for each node.
let us examine the terms in
To be more precise,
the term f (X, Y r+1) =
(29c) one by one.
(cid:1), hence
(cid:80)N
it is decomposable. Second, the term (cid:104)Ωr, AX(cid:105) can be
expressed as

First,
i − zi(cid:107)2 + hi(yr+1

(cid:0) 1
2 (cid:107)Xiyr+1

) + γ(cid:107)Xi(cid:107)2
F

i=1

i

N
(cid:88)

(cid:88)

(cid:104)Ωr

e, Xi(cid:105) −

(cid:88)

(cid:104)Ωr

e, Xi(cid:105)

i=1

e∈U (i)

e∈H(i)

where the sets U (i) and H(i) are deﬁned as

U (i) := {e | e = (i, j) ∈ E, i ≥ j},

H(i) := {e | e = (i, j) ∈ E, j ≥ i}.

Similarly, we have

(cid:104)BX r, BX(cid:105) =

(cid:42)

N
(cid:88)

i=1

Xi, diX r

i +

(cid:88)

X r
j

j∈N (i)

(cid:43)

β
2

((cid:104)AX, AX(cid:105) + (cid:104)BX, BX(cid:105))

= β(cid:104)DX, X(cid:105) = β

di(cid:107)Xi(cid:107)2
F ,

N
(cid:88)

i=1

where D := ˜D ⊗ IM ∈ RNM ×NM with ˜D being the
degree matrix. It is easy to see that the X subproblem (29c)
is separable over the distributed agents. Finally, one can
verify that the Ω update step (29d) can be implemented by
each edge e ∈ E as follows
e + β (cid:0)X r+1

e = (i, j), i ≥ j.

i − X r+1

e = Ωr

Ωr+1

(cid:1) ,

j

s.t. (cid:107)yi(cid:107)2 ≤ τ, ∀ i AX = 0.

(26)

(cid:104)Ωr, AX(cid:105) =

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

To show convergence rate of the algorithm, we need the
following deﬁnition

gradually increase d to ﬁnd an appropriate β that satisﬁes
the ﬁrst three conditions.

Q(X r+1, Y r+1, Ωr) := β(cid:107)AX r+1(cid:107)2 + (cid:107)[Z r+1

; Z r+1
2

](cid:107)2,

1

where we have deﬁned
Z r+1
1
Z r+1
2

:= ∇X Lβ(X r+1, Y r+1, Ωr);
:= Y r+1

− proxh+ι(Y)

(cid:2)Y r+1 − ∇Y

(cid:0)Lβ(X r+1, Y r+1, Ωr) − h(Y )(cid:1)(cid:3) .

i

(cid:83)

:=
In the above expression, we have used Y
(cid:8)(cid:107)yi(cid:107)2 ≤ τ (cid:9) to denote the feasible set of Y , and used
ι(Y) to denote the indicator function of such set. Similarly
as in Section 4, we can show that Q(X r+1, Y r+1, Ωr) →
0 implies that every limit point of (X r+1, Y r+1, Ωr) is a
KKT point of problem (26).

Next we present the main convergence analysis for Algo-
rithm 2. The proof is long and technical, therefore we refer
the readers to (Hong, 2016).

Theorem 3 Consider using Algorithm 2 to solve the dis-
tributed matrix factorization problem (26). Suppose that
h(Y ) is lower bounded over dom h(x), and that the penalty
parameter β, together with two positive constants c and d,
satisﬁes the following conditions
cd
2

β + 2γ
2

> 0,

−

−

−

8τ
σminβ

cτ
d

−

> 0,

(30)

8(τ 2 + 4γ2)
βσmin
8
c
σminβ
d
2β(cid:107)BT B(cid:107)
σmin

> 0.

> 0,

−

1
2

−

1
2
cβ
2

−

Then in the limit, consensus will be achieved, i.e.,

lim
r→∞

(cid:107)X r

i − X r

j (cid:107) = 0,

∀ (i, j) ∈ E.

Further, the sequences {X r+1} and {Ωr+1} are both
bounded, and every limit point generated by Algorithm 2
is a KKT point of problem (25).

Additionally, Algorithm 2 converges sublinearly. Speciﬁ-
cally, for any given ϕ > 0, deﬁne T to be the ﬁrst time that
the optimality gap reaches below ϕ, i.e.,

T := arg min

Q(X r+1, Y r+1, Ωr) ≤ ϕ.

r

T −1 .

Then for some constant ν > 0 we have ϕ ≤ ν
We can see that it is always possible to ﬁnd the tuple
{β, c, d > 0} that satisﬁes (30): c can be solely deter-
mined by the last inequality; for ﬁxed c, the constant d
needs to be chosen large enough such that 1/2 − c
d > 0
and 1/2 − cτ
d > 0 are satisﬁed. After c and d are ﬁxed,
one can always choose β large enough to satisfy the ﬁrst
three conditions. In practice, we typically prefer to choose
β as small as possible to improve the convergence speed.
Therefore empirically one can start with (for some small
ν > 0): c = 4(cid:107)BT B(cid:107)
d = max{4, 2cτ }, and then

+ ν,

σmin

We remark that Algorithm 2 can be extended to the case
with increasing penalty. Due to the space limitation we
omit the details here.

8. Numerical Results
In this section, we demonstrate the performance of the pro-
posed algorithms. All experiments are performed in Matlab
(2016b) on a desktop with an Intel Core(TM) i5-4690 CPU
(3.50 GHz) and 8GB RAM running Windows 7.

8.1. Distributed Binary Classiﬁcation

In this subsection, we study the problem of binary classiﬁ-
cation using nonconvex regularizers in the mini-bach setup
i.e. each node stores b (batch size) data points, and each
component function is given by

fi(xi) =

log(1 + exp(−yijxT

i vij)) +

1
N b

(cid:20) b

(cid:88)

j=1

M
(cid:88)

k=1

(cid:21)

λαx2
i,k
1 + αx2

i,k

where vij ∈ RM and yij ∈ {1, −1} are the feature vector
and the label for the jth date point in ith agent (Antoniadis
et al., 2009). We use the parameter settings of λ = 0.001,
α = 1 and M = 10. We randomly generated 100, 000 data
points and distribute them into N = 20 nodes (i.e. b =
5000). We use the optimality gap (opt-gap) and constraint
violation (con-vio), displayed below, to measure the quality
of the solution generated by different algorithms

opt-gap :=

∇fi(zi)

+ (cid:107)Ax(cid:107)2, con-vio = (cid:107)Ax(cid:107)2.

(cid:13)
(cid:13)
(cid:13)
(cid:13)

N
(cid:88)

i=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

We compare the the Prox-GPDA, and Prox-GPDA-IP
with the distributed subgradient (DSG) method (Nedic &
Ozdaglar, 2009a;b) (which is only known to work for con-
vex cases) and the Push-sum algorithm (Tatarenko & Touri,
2015). The performance of all three algorithms in terms of
the consensus error and the optimality gap (averaged over
30 problem instances) are presented in Fig. 2. The penalty
parameter for Prox-GPDA is chosen such that satisfy (14),
and βr for Prox-GPDA-IP is set as 0.05 log(r), the step-
sizes of the DSG algorithm and the Push-sum algorithm are
chosen as 1/0.05 log(r) and 1/r, respectively. Note that
these parameters are tuned for each algorithm to achieve
the best results. All Algorithms will stop after 1000 itera-
tions. It can be observed that the Prox-GPDA with constant
stepsize outperforms other algorithms. The Push-sum algo-
rithm does not seem to converge within 1000 iterations.

To see more results, we compare different algorithms with
different number of agents in the network (N ). The prob-
lem and the algorithms setup are set as the previous case.
We measure the optimality gap as well as the constraint
violation and the results are respectively reported in Ta-
ble 1 and Table 2. In the tables Alg1, Alg2, Alg3, Alg4

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

Figure 2. Results for the matrix factorization problem.

Figure 3. Results for the matrix factorization problem.

Table 1. Optimality Gap for different Algorithms
Alg2
2.4e-22
5.0e-9
5.1e-8
2.9e-7
4.2e-6

Alg3 Alg4
2.79
1.34
0.42
0.04
0.20
0.008
0.21
0.007
0.40
0.005

Alg1
5.1e-36
4.7e-32
2.3e-21
1.3e-12
5.5e-10

N
10
20
30
40
50

Table 2. Constraint Violation for different Algorithms
Alg2
3.4e-27
3.7e-16
7.8e-15
2.1e-14
2.2e-12

Alg3 Alg4
0.65
0.35
0.40
0.02
0.18
0.01
0.20
0.03
0.12
0.01

Alg1
1.3e-36
1.2e-34
2.3e-24
2.2e-16
2.2e-14

N
10
20
30
40
50

denote Prox-GPDA, Prox-GPDA-IP, DGS, and Push-sum
algorithms respectively. As seen, the performance of the
proposed algorithms (Alg1, Alg2) are signiﬁcantly better
than DGS and Push-Sum.

8.2. Distributed Matrix Factorization

In this section we consider the distributed matrix factor-
ization problem (25). The training data is constructed

by randomly extracting 300 overlapping patches from the
512 × 512 image of barbara.png, each with size 16 × 16
pixels. Each of the extracted patch is vectorized, result-
ing a training data set Z of size 256 × 300. We consider
a network of N = 10 agents, and the columns of Z are
evenly distributed among the agents (each having P = 30
columns). We compare Prox-PDA-IP with the EXTRA-
AO algorithm proposed in (H.-T. Wai & Scaglione, 2015).
Note that the EXTRA-AO is also designed for a similar
distributed matrix factorization problem and it works well
in practice. However, it does not have formal convergence
proof. We initialize both algorithms with X being the 2D
discrete cosine transform (DCT) matrix. We set γ = 0.05,
τ = 105 and β = 0.001r, and the results are averaged
over 30 problem instances. The stepsizes of the EXTRA-
AO is set as αAO = 0.03 and βAO = 0.002. In Fig. 3,
we compare the performance of the proposed Prox-PDA-
IP and the EXTRA-AO. It can be observed that our pro-
posed algorithm converges faster than the EXTRA-AO. We
have observed that the EXTRA-AO does have reasonably
good practical performance, however it lacks formal con-
vergence proof.

Acknowledgment

The authors supported by National Natural Science Foun-
dation (Grant No. CCF-1526078).

0200400600800100010-4010-3010-2010-1010010100200400600800100010-4010-3010-2010-101001010010002000300040005000Iterationnumber10-810-610-410-2100102104OptimalityGapProx-PDA-IPEXTRA-AO010002000300040005000IterationNumber10-1010-51001051010ConsensusErrorProx-PDA-IPEXTRA-AOProximal Primal Dual Algorithm for Distributed Nonconvex Optimization

References

Allen-Zhu, Z. and Hazan, E. Variance Reduction for Faster Non-
Convex Optimization. In Proceedings of the 33rd International
Conference on Machine Learning, ICML, 2016.

Antoniadis, A., Gijbels, I., and Nikolova, M. Penalized likelihood
regression for generalized linear models with non-quadratic
penalties. Annals of the Institute of Statistical Mathematics,
63(3):585–615, 2009.

Aybat, N-S. and Hamedani, E-Y. A primal-dual method for conic
constrained distributed optimization problems. Advances in
Neural Information Processing Systems, 2016.

Bertsekas, D. P. Constrained Optimization and Lagrange Multi-

plier Method. Academic Press, 1982.

Bianchi, P. and Jakubowicz, J. Convergence of a multi-agent pro-
jected stochastic gradient algorithm for non-convex optimiza-
tion. IEEE Transactions on Automatic Control, 58(2):391–405,
2013.

Bjornson, E. and Jorswieck, E. Optimal resource allocation in co-
ordinated multi-cell systems. Foundations and Trends in Com-
munications and Information Theory, 9, 2013.

Cevher, V., Becker, S., and Schmidt, M. Convex optimization
for big data: Scalable, randomized, and parallel algorithms for
big data analytics. IEEE Signal Processing Magazine, 31(5):
32–43, 2014.

Defazio, A., Bach, F., and Lacoste-Julien, S. Saga: A fast incre-
mental gradient method with support for non-strongly convex
composite objectives. In The Proceeding of NIPS, 2014.

Forero, P. A., Cano, A., and Giannakis, G. B. Consensus-
based distributed support vector machines. Journal of Machine
Learning Research, 11(May):1663–1707, 2010.

H.-T. Wai, T.-H. Chang and Scaglione, A. A consensus-based
decentralized algorithm for non-convex optimization with ap-
plication to dictionary learning. In the Proceedings of the IEEE
ICASSP, 2015.

Hajinezhad, D. and Hong, M. Nonconvex alternating direction
method of multipliers for distributed sparse principal compo-
nent analysis. In IEEE Global Conference on Signal and Infor-
mation Processing (GlobalSIP). IEEE, 2015.

Hajinezhad, D., Chang, T-H., Wang, X., Shi, Q., , and Hong, M.
Nonnegative matrix factorization using admm: Algorithm and
In IEEE International Conference on
convergence analysis.
Acoustics, Speech and Signal Processing (ICASSP), 2016a.

Hajinezhad, D., Hong, M., Zhao, T., and Wang, Z. Nestt: A
nonconvex primal-dual splitting method for distributed and
In Advances in Neural Information
stochastic optimization.
Processing Systems 29, pp. 3215–3223. 2016b.

Hong, M. Decomposing linearly constrained nonconvex problems
by a proximal primal dual approach: Algorithms, convergence,
and applications. arXiv preprint arXiv:1604.00543, 2016.

Hong, M., Luo, Z.-Q., and Razaviyayn, M. Convergence analysis
of alternating direction method of multipliers for a family of
nonconvex problems. 2014.
technical report, University of
Minnesota.

Hong, M., Razaviyayn, M., Luo, Z.-Q., and Pang, J.-S. A uni-
ﬁed algorithmic framework for block-structured optimization
involving big data. IEEE Signal Processing Magazine, 33(1):
57–77, 2016.

Johnson, R. and Zhang, T. Accelerating stochastic gradient de-
scent using predictive variance reduction. In the Proceedings
of the Neural Information Processing (NIPS). 2013.

Ling, Q., Xu, Y., Yin, W., and Wen, Z. Decentralized low-rank
matrix completion. In 2012 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 2925–
2928, March 2012.

Ling, Q., Shi, W., Wu, G., and Ribeiro, A. DLM: Decentralized
IEEE
linearized alternating direction method of multipliers.
Transactions on Signal Processing, 63(15):4051–4064, Aug
2015.

Lobel, I. and Ozdaglar, A. Distributed subgradient methods for
convex optimization over random networks. Automatic Con-
trol, IEEE Transactions on, 56(6):1291–1306, 2011.

Lobel, I., Ozdaglar, A., and Feijer, D. Distributed multi-agent op-
timization with state-dependent communication. Mathematical
Programming, 129(2):255–284, 2011.

Lorenzo, P. Di and Scutari, G. Next: In-network nonconvex opti-
mization. IEEE Transactions on Signal and Information Pro-
cessing over Networks, 2(2):120–136, 2016.

Nedic, A. and Olshevsky, A. Distributed optimization over time-
varying directed graphs. IEEE Transactions on Automatic Con-
trol, 60(3):601–615, 2015.

Nedic, A. and Ozdaglar, A. Cooperative distributed multi-agent
optimization. In Convex Optimization in Signal Processing and
Communications. Cambridge University Press, 2009a.

Nedic, A. and Ozdaglar, A. Distributed subgradient methods for
IEEE Transactions on Automatic

multi-agent optimization.
Control, 54(1):48–61, 2009b.

Powell, M. M. D. An efﬁcient method for nonlinear constraints
in minimization problems. In Optimization. Academic Press,
1969.

Rahimpour, Alireza, Taalimi, Ali, Luo, Jiajia, and Qi, Hairong.
In
Distributed object recognition in smart camera networks.
IEEE International Conference on Image Processing, Phoenix,
Arizona, USA. IEEE, 2016.

Rahmani, M. and Atia, G. A decentralized approach to robust sub-
space recovery. In 2015 53rd Annual Allerton Conference on
Communication, Control, and Computing (Allerton), pp. 802–
807. IEEE, 2015.

Reddi, S., Sra, S., Poczos, B., and Smola, A.

Fast incre-
mental method for nonconvex optimization. arXiv preprint
arXiv:1603.06159, 2016.

Scardapane, S. and Lorenzo, P. Di. A framework for parallel
and distributed training of neural networks. arXiv preprint
arXiv:1610.07448, 2016.

Scardapane, S., Fierimonte, R., Lorenzo, P. Di, Panella, M., and
Uncini, A. Distributed semi-supervised support vector ma-
chines. Neural Networks, 80:43–52, 2016.

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

Schmidt, M., Roux, N. Le, and Bach., F. Minimizing ﬁnite sums
with the stochastic average gradient. 2013. Technical report,
INRIA.

Shi, W., Ling, Q., Wu, G., and Yin, W. Extra: An exact ﬁrst-order
SIAM

algorithm for decentralized consensus optimization.
Journal on Optimization, 25(2):944–966, 2014.

Tatarenko, T. and Touri, B. Non-convex distributed optimization.

2015. arXiv Preprint: arXiv:1512.00895.

Yan, F., Sundaram, S., Vishwanathan, S. V. N., and Qi, Y. Dis-
tributed autonomous online learning: Regrets and intrinsic
privacy-preserving properties. IEEE Transactions on Knowl-
edge and Data Engineering, 25(11):2483–2493, 2013.

Zhu, M. and Martinez, S. An approximate dual subgradient algo-
rithm for multi-agent non-convex optimization. In 49th IEEE
Conference on Decision and Control (CDC), pp. 7487–7492,
2010.

Zlobec, S. On the liu-ﬂoudas convexiﬁcation of smooth programs.

Journal of Global Optimization, 32(3):401–407, 2005.

