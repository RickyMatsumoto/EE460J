When can Multi-Site Datasets be Pooled for Regression?
Hypothesis Tests, (cid:96)2-consistency and Neuroscience Applications
(Supplementary Material)

Hao Henry Zhou 1 Yilin Zhang 1 Vamsi K. Ithapu 1 Sterling C. Johnson 1 2 Grace Wahba 1 Vikas Singh 1

In this supplementary document we ﬁrst present the proofs of all the technical results in Section 2 and 3 of the main paper.
We then expand upon the Section 4 and present extra experiments to strengthen the evaluations from the main paper.

Remarks on transformations in pre-processing step: For all i ∈ {1, ..., k}, after applying the transformation (shift
correction), we pool (Xi, yi) together to estimate β∗. Note that in general the transformation (shift correction) should not
depend on the responses yi, otherwise we get a dependence on the noise. To see this, notice that yi = Xiβi + (cid:15)i where
Xi is the transformed set of features. But when the transformation depends on yi, then Xi will also depend on (cid:15)i, which
causes a poor estimation of β∗ (and βi). In situations where the transformations must involve yi, a sensible strategy is to
separate each site’s dataset into two parts, where one part from each site is used to learn the transformation, and the other
part (after applying the learned transformation) is used for pooling towards β∗ estimation and conducting our hypothesis
test.

1. Proof of Section 2

We now provide the proofs of the results presented in the main paper.

Theorem 2.1. τi = σ1
σi

achieve the smallest variance in ˆβ.

Proof. The choice of τi leads to weighted least squares, which is known to be the best linear unbiased estimator (BLUE)
under uncorrelated heteroscedastic errors. The variance of ˆβ is equivalent to the case when ∆βi = 0. In the latter case,
BLUE condition holds and setting τi to the above value achieves lowest variance. The equivalence between variances
under two cases completes the proof.

Lemma 2.2. For multi-site model, we have

≤ (cid:107)( ˆΣk

(cid:107)Biasβ(cid:107)2
2
(cid:107)G−1/2∆β(cid:107)2
2
(cid:13)
(cid:13)(n1 ˆΣ1)−1 − (n1 ˆΣ1 + ˆΣk
(cid:13)

2 (n1 ˆΣ1)−1 ˆΣk
2 )−1(cid:13)
(cid:13)
(cid:13)∗

V arβ = σ2
1

1 )−2( ˆΣk

.

2 + ˆΣk

2 )(cid:107)∗,

Proof. The estimation from single site model is unbiased, and it has the following variance.

V ar1 = tr((X T

1 X1)−1)σ2

1 = tr((n1 ˆΣ1)−1)σ2

1

1University of Wisconsin-Madison 2William S. Middleton Memorial Veteran’s Affairs Hospital. Correspondence to: Hao Zhou

<hzhou@stat.wisc.edu>, Vikas Singh <vsingh@biostat.wisc.edu>.

Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

(1)

(2)

(3)

When can Multi-Site Datasets be Pooled for Regression?

The estimation error from multi-sites model has the following closed form expression

ˆβ − β∗ =

























+

X1
τ2X2
..
τkXk

X1
τ2X2
..
τkXk



T 











T 









X1
τ2X2
..
τkXk

−1

















τ2X2
..
τkXk



T 





τ2X2(∆β2)
..
τkXk(∆βk)









−1 













X1
τ2X2
..
τkXk

X1
τ2X2
..
τkXk



T 















(cid:15)1
τ2(cid:15)2
..
τk(cid:15)k

First term in the summation from (4) is bias, while second term is variance. We can see that our choice of τi = σ1
σi
heteroscedastic errors issue among sites. We further simplify bias and variance terms, and obtain

resolves

V ar2 = tr((n1 ˆΣ1 +

niτ 2
i

ˆΣi)−1)σ2
1

k
(cid:88)

i=2

The reduced variance statement is proved. For the bias term, it is equivalent as shown below.













X1
τ2X2
..
τkXk



T 









X1
τ2X2
..
τkXk

−1

















τ2X2
..
τkXk



T 





τ2X2
0
0

0
τ3X3
0

...
...
...

0
0
τkXk


 G1/2






G−1/2





∆β2
..
∆βk










A one step Cauchy Schwartz inequality is then applied. Then our ﬁnal proof is to show (cid:107)..(cid:107)2

F on













X1
τ2X2
..
τkXk



T 









X1
τ2X2
..
τkXk

−1

















τ2X2
..
τkXk



T 





τ2X2
0
0

0
τ3X3
0

...
...
...

0
0
τkXk


 G1/2

is equal to right side of the bias relaxation in (1).
It is easy to see that (cid:107)A(cid:107)2
in (1). Let the other part in (7) be L. We have

F = (cid:107)AT A(cid:107)∗. Based on this, we can see the ﬁrst term of matrix inverse contributes the ( ˆΣk

1)−2

LLT =





2 X T
τ 2
2 X2
..
k X T
τ 2
k Xk


T





G



2 X T
τ 2
2 X2
..
k X T
τ 2
k Xk





 =



n2τ 2
2
..
nkτ 2
k

ˆΣ2

ˆΣk

T






G







n2τ 2
2
..
nkτ 2
k

ˆΣ2

ˆΣk

After some manipulations, this becomes ( ˆΣk

2(n1 ˆΣ1)−1 ˆΣk

2 + ˆΣk

2). The bias part is proved.

Theorem 2.3. a): The multi-sites model has smaller MSE of ˆβ than single-site model whenever

b): Further, we have the following test statistic,

H0 :

(cid:13)
(cid:13)G−1/2∆β
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

≤ σ2
1.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

G−1/2∆ ˆβ
σ1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

∼ χ2

(k−1)∗p

(cid:32)(cid:13)
(cid:13)
(cid:13)
(cid:13)

G−1/2∆β
σ1

(cid:33)

,

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

where (cid:107)G−1/2∆β/σ1(cid:107)2 is called a “condition value”.

(4)

(5)

(6)

(7)

(8)

(9)

(10)

When can Multi-Site Datasets be Pooled for Regression?

Proof. (a): Based on Lemma 2.2, the theorem is proved when right side in (9) is replaced by

(cid:13)
(cid:13)(n1 ˆΣ1)−1 − (n1 ˆΣ1 + ˆΣk
(cid:13)
1)−2( ˆΣk

2(n1 ˆΣ1)−1 ˆΣk

σ2
1
(cid:107)( ˆΣk

2)−1(cid:13)
(cid:13)
(cid:13)∗
2)(cid:107)∗

2 + ˆΣk

We ﬁrst calculate the numerator

σ2
1

(cid:13)
(cid:13)(n1 ˆΣ1)−1 − (n1 ˆΣ1 + ˆΣk
(cid:13)

= σ2
1

(n1 ˆΣ1)−1(n1 ˆΣ1 + ˆΣk

2) − I

(n1 ˆΣ1 + ˆΣk

(cid:105)

2)−1(cid:13)
(cid:13)
(cid:13)∗

= σ2
1

(cid:13)
(cid:13)(n1 ˆΣ1)−1 ˆΣk
(cid:13)

2)−1(cid:13)
(cid:13)
(cid:13)∗
2)−1(cid:13)
2(n1 ˆΣ1 + ˆΣk
(cid:13)
(cid:13)∗

(cid:13)
(cid:104)
(cid:13)
(cid:13)

The denominator is then given by

1)−2( ˆΣk
(cid:107)( ˆΣk
Remember ˆΣk
= (cid:107)(( ˆΣk

2(n1 ˆΣ1)−1 ˆΣk
1 = ˆΣk

2)(cid:107)∗ = (cid:107)( ˆΣk
2 + n1 ˆΣ1, , we continue

2 + ˆΣk

2 + n1 ˆΣ1)−1(n1 ˆΣ1)−1 ˆΣk

2)(cid:107)∗ = (cid:107)((n1 ˆΣ1)−1 ˆΣk

2(n1 ˆΣ1 + ˆΣk

2)−1)(cid:107)∗

1)−2(( ˆΣk

2 + n1 ˆΣ1)(n1 ˆΣ1)−1 ˆΣk

2)(cid:107)∗

The last step uses the property of (cid:107)..(cid:107)∗ norm. The proof is completed by noticing the simpliﬁed form of numerator and
denominator. It is clear now that the right side in (9) is exactly σ2
1.
1G is the covariance matrix of ∆ ˆβ. We have

(b): First, we show σ2

cov(∆ ˆβi, ∆ ˆβj) = cov( ˆβi, ˆβj) − cov( ˆβi, ˆβ1) − cov( ˆβ1, ˆβj) + cov( ˆβ1, ˆβ1)
Since each site is independent from other site, we have
cov(∆ ˆβi, ∆ ˆβj) = cov( ˆβ1, ˆβ1) = σ2
1(n1 ˆΣ1)−1for i (cid:54)= j
cov(∆ ˆβi, ∆ ˆβi) = cov( ˆβi, ˆβi) + cov( ˆβ1, ˆβ1) = σ2

1((n1 ˆΣ1)−1 + (ni(σ2

1/σ2

i ) ˆΣi)−1) = σ2

1((n1 ˆΣ1)−1 + (niτ 2

i

ˆΣi)−1)

∆ ˆβ follows Gaussian distribution since it is a linear transformation of Gaussian distribution. It’s expectation is ∆β since
each ˆβi is an unbiased estimator. Hence, we have

∆ ˆβ ∼ N (∆β, σ2

1G)

This distribution result, and noticing the connection between Gaussian and non-central χ2 distributions completes the
proof.

Corollary 2.4. For the case where we have two participating sites, the condition (9) from Theorem 2.3 reduces to

H0 : ∆βT ((n1 ˆΣ1)−1 + (n2τ 2
2

ˆΣ2)−1)−1∆β ≤ σ2
1.

Proof. The proof is follows by noticing the form of G when k = 2.
Theorem 2.5. Analysis in Section 2.1 holds for β in model with Z confounding features, when we replace ˆΣi with

˜Σi = ˆΣxxi − ˆΣxzi ( ˆΣzzi)−1 ˆΣzxi.

Proof. Deﬁne γT = (γT

1 , ..., γT

k ), X T

all = (cid:0)X T

1 , τ2X T

2 , ..., τkX T
k

(cid:1), Zall = Diag (Z1, τ2Z2, ..., τkZk). We have

(cid:18) ˆβ
ˆγ

(cid:19)

(cid:18) β∗
γ∗

−

(cid:19)

=

(cid:18) X T
Z T

allXall X T
allXall Z T

allZall
allZall

(cid:19)−1 (cid:18) X T
all
Z T
all

(cid:18) X T
Z T

allXall X T
allXall Z T

allZall
allZall

(cid:19)−1 (cid:18) X T
all
Z T
all

(cid:19)













(cid:15)1
τ2(cid:15)2
..
τk(cid:15)k

(cid:19)







0
τ2X2(∆β2)
..
τkXk(∆βk)







+

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

When can Multi-Site Datasets be Pooled for Regression?

Using sub-matrix inverse property, we obtain

(cid:18) X T
Z T

allXall X T
allXall Z T

allZall
allZall

(cid:19)−1 (cid:18) X T
all
Z T
all

(cid:19)

=

(cid:18) ( ˜X T
all
( ˜Z T
all

˜Xall)−1 ˜X T
all
˜Zall)−1 ˜Z T
all

(cid:19)

We then have

Deﬁne

Hence, we have

˜Zall = (I − Xall(X T

allXall)−1X T

all)Zall

˜Xall = (I − Zall(Z T

allZall)−1Z T

all)Xall =







(I − Z1(Z T
(I − Z2(Z T

1 Z1)−1Z T
2 Z2)−1Z T

1 )X1
2 )X2

..

(I − Zk(Z T

k Zk)−1Z T

k )Xk







HZi = (I − Zi(Z T

i Zi)−1Z T
i )

ˆβ − β∗ = ( ˜X T
all

˜Xall)−1 ˜X T
all

+ ( ˜X T
all

˜Xall)−1 ˜X T
all







0
τ2X2(∆β2)
..
τkXk(∆βk)













(cid:15)1
τ2(cid:15)2
..
τk(cid:15)k







= ( ˜X T
all

˜Xall)−1

τi ˜X T

i Xi(∆βi) + ( ˜X T

all

˜Xall)−1 ˜X T
all

k
(cid:88)

i=2







(cid:15)1
τ2(cid:15)2
..
τk(cid:15)k







We also observe that

(29)
Therefore, we can apply our previous results to a subset of parameters if we replace Xi by ˜Xi. Since our results only
depend on ˆΣi, we only need to replace it by

i Xi = X T

i HZiXi = X T

i H 2
Zi

Xi = ˜X T
i

˜X T

˜Xi

1
ni

1
ni

˜X T
i

˜Xi =

X T

i HZiXi = ˆΣxxi − ˆΣxzi( ˆΣzzi)−1 ˆΣzxi

This proves the theorem.

2. Proof of Section 3

Deﬁnition 3.1. The m-sparse minimal and maximal eigenvalues of C, denoted by φmin(m) and φmax(m), are

We ﬁrst list down the two key theorem statements that we prove in this section.
Theorem 3.2. Let 0 ≤ α ≤ 0.4. Assume there exist constants 0 ≤ ρmin ≤ ρmax ≤ ∞ such that

min
ν:(cid:107)ν(cid:107)0≤(cid:100)m(cid:101)

νT Cν
νT ν

and

max
ν:(cid:107)ν(cid:107)0≤(cid:100)m(cid:101)

νT Cν
νT ν

(cid:32)

(cid:18)

lim inf
n→∞

φmin

sp

1 +

(cid:19)2(cid:33)

2α
1 − 2α

≥ ρmin, and

lim sup
n→∞

φmax(sp + min{

ni, kp}) ≤ ρmax.

k
(cid:88)

i=1

Then, for λ ∝ σ(cid:112)¯n log(kp), there exists a constant ω > 0 such that, with probability converging to 1 for n → ∞,

where ¯s = {(1 − α)

√

sp + α(cid:112)sh/k}2, σ is the noise level.

1
k

(cid:107) ˆBλ − B∗(cid:107)2

F ≤ ωσ2 ¯s log(kp)

,

¯n

(24)

(25)

(26)

(27)

(28)

(30)

(31)

(32)

(33)

When can Multi-Site Datasets be Pooled for Regression?

Theorem 3.3. Let 0.4 ≤ ˜α ≤ 1. Assume there exist constants 0 ≤ ρmin ≤ ρmax ≤ ∞ such that

(cid:32)

(cid:18)

lim inf
n→∞

φmin

sh

1 +

(cid:19)2(cid:33)

(1 − ˜α)
˜α

≥ kmin, and

lim sup
n→∞

φmax(sh + min{

ni, kp}) ≤ kmax.

k
(cid:88)

i=1

1
k

(cid:107) ˆBλ − B∗(cid:107)2

F ≤ ωσ2 ˜s log(kp)

,

¯n

Then, for ˜λ ∝ σ(cid:112)¯n log(kp), there exists a ω > 0 such that, with probability converging to 1 for n → ∞, we have

with ˜s = {(1 − ˜α)(cid:112)sp/k + ˜α(cid:112)sh/k}2 instead of ¯s.

√

√

k when the sparsity patterns across sites share few of the features.
Comment about Theorem 3.3: We do not penalize by
To see this, ﬁrst observe that when sparsity patterns are similar, most of the groups we have are non-sparse, and the effects
k is close to |a1|+...+|ak| whenever
of
|a1|, ..., |ak| are close. However when sparsity patterns across sites share few features only, most of the groups are going
to be sparse. For these groups, we should use (cid:107)βj(cid:107)2, because in this setting (cid:112)a2
1 + 0 + ... + 0 is close to |a1| + 0 + ... + 0.

k(cid:107)βj(cid:107)2 and (cid:107)βj(cid:107)1 have the same scale. This is simply because,

1 + ... + a2

k(cid:112)a2

√

3.1. Proof of Theorem 3.2:

We follow the proof procedure from Lasso (Meinshausen & Yu, 2009) and group Lasso (Liu & Zhang, 2009) results. Let
Bλ be the estimator under the absence of noise, i.e., Bλ = ˆBλ,0, where ˆBλ,ξ is deﬁned as in (37). The (cid:96)2-distance can
then be bounded by (cid:107) ˆBλ − B∗(cid:107)2
F + 2(cid:107)Bλ − B∗(cid:107)2
F . The ﬁrst term on the right-hand side represents the
variance of the estimation, while the second term represents the bias. The bias contribution follows directly from Lemma
3.4 below, and the variance bound term follows from Lemma 3.9.

F ≤ 2(cid:107) ˆBλ − Bλ(cid:107)2

De-noised response. For 0 < ξ < 1, we deﬁne a de-noised version of the response variable as follows,

We can regulate the amount of noise with the parameter ξ.

Yi(ξ) = Xiβi + ξ(cid:15)i

For ξ = 0, only the signal is retained. The original observations with the full amount of noise are recovered for ξ = 1.
Now consider for 0 ≤ ξ ≤ 1 the estimator ˆBλ,ξ,

ˆBλ,ξ = arg min
B

(cid:107)Yi(ξ) − Xiβi(cid:107)2

2 + λΛ(B)

Λ(B) = (1 − α)

(cid:107)βj(cid:107)2 + α

(cid:107)βj(cid:107)1

p
(cid:88)

j=1

k
(cid:88)

i=1
√

k

p
(cid:88)

j=1

The ordinary sparse multi-site Lasso estimate is recovered under the full amount of noise so that ˆBλ,1 = ˆBλ. Using
the notation from the previous results, we have ˆBλ,0 = Bλ, for the estimate in the absence of noise. The deﬁnition of
the de-noised version of the sparse multi-site Lasso estimator will be helpful for the proof as it allows to characterize the
variance of the estimator.

3.1.1. PART I OF PROOF – DEALING WITH BIAS

Let P∗ be the set of nonzero groups of B∗, i.e., P∗ = {j : βj (cid:54)= 0}. The cardinality of P∗ is denoted by sp. For each j
in P∗, let Hj be the set of nonzero elements of βj, i.e., Hj = {i : βj
i (cid:54)= 0}. The number of all nonzero elements of B is
denoted by sh. For the following, let Bλ be the estimator ˆBλ,0 with no noise (as deﬁned in (37)). For each λ, the solution
Bλ can be written as Bλ = B∗ + Γλ. We deﬁne γj and γi to be j-th column and i-th row of Γ. γ is the transpose of the
unfolded vector of Γ by row. Denote λ2 = λ(1 − α) and η = α

1−α . Then

Γλ = arg min

f (Γ)

Γ

(34)

(35)

(36)

(37)

(38)

When can Multi-Site Datasets be Pooled for Regression?

The function f (Γ) is given by

f (Γ) = ¯nγT Cγ + λ2

K(cid:107)γj(cid:107)2 + η(cid:107)γj(cid:107)1) +

K((cid:107)βj + γj(cid:107)2 − (cid:107)βj(cid:107)2)

+






(cid:88)

√

(

j∈P C
∗

√

(cid:88)

j∈P∗
(cid:41)

(cid:40)

(cid:88)

λ2

j∈P∗

η((cid:107)βj
Hj

+ γj
Hj

(cid:107)1 − (cid:107)βjHj)(cid:107) +

η(cid:107)γjH C

j (cid:107)1

(cid:88)

j∈P∗






The matrix Γλ is the bias of the sparse multi-site Lasso estimator. We derive ﬁrst a bound on the Frobenius norm of Γλ.
Lemma 3.4. Assume conditions in Theorem3.2. The Frobenius norm of Γλ is then bounded for sufﬁciently large values of
¯n, given a constant ω1 > 0, by

(cid:107)Γλ(cid:107)2

F ≤ ω1σ2 k¯s log(kp)

¯n

Proof. f (Γ) = 0 whenever Γ = 0 following the deﬁnition from (39). For the true solution Γλ, it follows hence that
f (Γλ) ≤ 0. For notational simplicity, we drop the super-script λ from here on. Using γT Cγ ≥ 0, we have






(cid:88)

√

(

j∈P C
∗

k(cid:107)γj(cid:107)2) +

(η(cid:107)γj(cid:107)1) +

(cid:88)

j∈P C
∗

(cid:88)

j∈P∗

η(cid:107)γj

HC
j

(cid:107)1

≤






(cid:40)

√

(cid:88)

j∈P∗

k(cid:107)γj(cid:107)2 +

η(cid:107)γj
Hj

(cid:107)1

(cid:88)

j∈P∗

(cid:41)

Since |P∗| = sp, (cid:80)
using (41),

j∈P∗

|Hj| = sh. It follows that (cid:80)

(cid:107)γj(cid:107)2 ≤

j∈P∗

(cid:107)γj
Hj

(cid:107)1 ≤

j∈P∗

√

sh(cid:107)γ(cid:107)2, and hence,

√

sp(cid:107)γ(cid:107)2, (cid:80)
√

Λ(Γ) ≤ 2{(1 − α)(cid:112)ksp + α

sh}(cid:107)γ(cid:107)2 = 2

k¯s(cid:107)γ(cid:107)2

Using f (Γ) ≤ 0 again and (42), it follows that

√

√

¯nγT Cγ ≤ 2λ

k¯s(cid:107)γ(cid:107)2

Now consider γT Cγ. Bounding this term from below and plugging the result into (42) will yield the desired upper bound
on the Frobenius norm of Γ. Let (cid:107)γ(1)(cid:107) ≥ (cid:107)γ(2)(cid:107) ≥ ... ≥ (cid:107)γ(p)(cid:107) be the ordered columns of Γ. Let un for n ∈ N be a
sequence of positive integers, to be chosen later, and deﬁne U = {j : (cid:107)γj(cid:107)2 ≥ (cid:107)γ(un)(cid:107)2}. Deﬁne γ(U ) and γ(U C) by
setting γj(U ) = γj1{i /∈ U } and γj(U C) = γj1{i ∈ U }, followed by unfolding Γ. Then quantity γT Cγ can be written
2, where a := ¯n−1/2Xγ(U ), b := ¯n−1/2Xγ(U C), X = DIAG(X1, ..., Xk). Then
as γT Cγ = (cid:107)a + b(cid:107)2

γT Cγ = (cid:107)a + b(cid:107)2
Before proceeding, we need to bound the norm (cid:107)γ(U C)(cid:107)2 as a function of un. Assume l = (cid:80)p
every j = 1, ..., p that (cid:107)γ(j)(cid:107)2 ≤ l/j. Hence,

2 ≥ ((cid:107)a(cid:107)2 − (cid:107)b(cid:107)2)2

j=1 (cid:107)γj(cid:107)2. It holds for

Therefore, we have

Based on (42), Λ(Γ) = (1 − α)

j=1 (cid:107)γj(cid:107)2 + α(cid:107)γ(cid:107)1, and (46), it follows that

√

k (cid:80)p

(cid:107)γ(U C)(cid:107)2

2 ≤ (

(cid:107)γj(cid:107)2)2

p
(cid:88)

j=1

p
(cid:88)

1
j2

j=un+1

(cid:107)γ(U C)(cid:107)2 ≤

(cid:107)γj(cid:107)2

≤ (cid:107)γ(cid:107)1

p
(cid:88)

j=1

(cid:114) 1
un

(cid:114) 1
un

(cid:107)γ(U C)(cid:107)2

2 ≤ 4(cid:107)γ(cid:107)2
2

(cid:32)






1
un

√

k¯s
√

(1 − α)

k + α

(cid:33)2




(cid:107)a(cid:107)2

2 = (cid:107)γ(U )T Cγ(U )(cid:107)2
(cid:40)
(cid:32)

φmin(un)(cid:107)γ(cid:107)2
2

1 − 4

2 ≥ φmin(un)(cid:107)γ(U )(cid:107)2
1
un

(1 − α)

k + α

k¯s
√

(cid:18)

2 ≥
(cid:19)2(cid:41)(cid:33)

By deﬁnition, since γ(U ) has only un nonzero groups,

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

When can Multi-Site Datasets be Pooled for Regression?

i=1 φi

i Xi. Then
min(m) because of block structure. Since we have un nonzero groups, instead of arbitrary kun
min(un) instead of φmin(kun). This is the one place

min(m) to be m-sparse of ¯n−1X T

Here we explain why we obtain φmin(un) instead of φmin(kun). We denote φi
φmin(m) = mink
nonzero elements, we obtain a higher value φmin(un) = mink
where we consider the block structure of multi-site design.
As γ(U C) has at most min{(cid:80)k
design,

i=1 ni, kp} nonzero groups, using again (47), (42) and the block structure of multi-site

i=1 φi

(cid:107)b(cid:107)2

2 ≤ 4φmax(min{

ni, kp})(cid:107)γ(cid:107)2
2

(cid:40)

(cid:32)

1
un

√

k¯s
√

(cid:33)2(cid:41)

(1 − α)

k + α

Using (49), (48) and (44), along with φmax(min{(cid:80)k

i=1 ni, kp}) ≥ φmin(un),

γT Cγ ≥ φmin(un)(cid:107)γ(cid:107)2

2 ×

1 − 4



φmax(min{(cid:80)k

i=1 ni, kp})

φmin(un)

(cid:40)

1
un

(

√

k¯s
√

(cid:41)


)2

(1 − α)

k + α

Using conditions in Theorem 3.2 and setting un =

(cid:16)

√

k¯s
√

(1−α)

k+α

(cid:17)2

, it follows that

k
(cid:88)

i=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

Using this result together with (43), which says that γT Cγ ≤ 2¯n−1λ

k¯s(cid:107)γ(cid:107)2, we have the following for large ¯n,

γT Cγ ≥ ρmin

1 − 4

(cid:18)

(cid:19)

(cid:107)γ(cid:107)2
2

(cid:114) ρmax
ρmin
√

(cid:107)Γ(cid:107)2

F = (cid:107)γ(cid:107)2

2 ≤

1
√
ρminρmax)2

λ2k¯s
¯n2

(ρmin − 4

The proof of Lemma 3.4 is completed by noticing λ in Theorem 3.2.

3.1.2. PART II OF PROOF – DEALING WITH VARIANCE

The proof for the variance part is two-fold. We ﬁrst derive a bound on the variance, which is a function of the number of
nonzero groups. We then bound the number of nonzero groups, taking into account the bound on the bias derived above.

Variance of restricted OLS: Before considering the sparse multi-site Lasso estimator, a trivial bound is shown for the
variance of a restricted OLS estimation. For every subset ψ ⊂ {1, , p}, we use it to select a subset of columns from design
matrix Xi for task i. These columns form a matrix Xiψ. Deﬁne Xψ = DIAG(X1ψ, X2ψ, ..., Xkψ), and the restricted
OLS-estimator with the noise vector (cid:15)T = ((cid:15)1, ..., (cid:15)k)T is

ˆθψ = (X T

ψ Xψ)−1X T
ψ (cid:15)

The (cid:96)2-norm of this estimator can be bounded.

Lemma 3.5. Let mp be a sequence with mp = o(¯n) and mp → ∞ for ¯n → ∞. It holds with probability converging to 1
for n → ∞

max
ψ:|ψ|≤mp

(cid:107)ˆθψ(cid:107)2

2 ≤

2 log kp
¯n

kmp
φ2
min(mp)

σ2

Proof. We refer the readers to Lemma 3 in (Meinshausen & Yu, 2009) and Lemma 3 in (Liu & Zhang, 2009) for the
proof. Here, we again use block design structure of multi-site problem, the same as in (48), to obtain φmin(mp) instead of
φmin(kmp).

The variance of the sparse multi-site Lasso estimator can be bounded by the variance of restricted OLS estimators, using
bounds on the number of active groups.

(49)

(50)

(51)

(52)

(53)

(54)

When can Multi-Site Datasets be Pooled for Regression?

Lemma 3.6. If, for a ﬁxed value of λ, the number of nonzero groups of de-noised estimators ˆBλ,ξ is for every 0 ≤ ξ ≤ 1
bounded by m, then

sup
0≤ξ≤1

(cid:107) ˆBλ,0 − ˆBλ,ξ(cid:107)2

F ≤ C max

ψ:|ψ|≤m

(cid:107)ˆθψ(cid:107)2
2

with C as a generic constant.

Proof. We refer the readers to Lemma 4 and Lemma 5 in (Liu & Zhang, 2009) for the proof.

λ,ξ be the set of variables in nonzero groups of the de-noised estimator ˆBλ,ξ. Deﬁne mp to be the largest number of

Let AP
nonzero groups over all values of 0 ≤ ξ ≤ 1. Then we have kmp = sup0≤ξ≤1 |AP
Lemma 3.7. Given 0 ≤ α ≤ 0.5, we have

λ,ξ|.

(55)

(56)

|AP

λ,ξ|λ2(1 − 2α)2 ≤ (cid:107)2X T
Ap

(Y − X ˆβλ,ξ)(cid:107)2
2

λ,ξ

where we deﬁned before that X = DIAG(X1, ..., Xk), Y T = (Y T
ˆBλ,ξ by rows. XAp

is Xψ when ψ = Ap
λ,ξ

λ,ξ

1 , ..., Y T

k ). ˆβλ,ξ is the transpose of unfolded vector of

Proof. The conditions for the solution of sparse multi-site Lasso are presented in (Simon et al., 2013). We use ˆβ rather
than ˆβλ,ξ for notational simplicity in this proof. We continue to use our notation ˆβj to refer the j-th column (here it is a
group) of ˆB, and ˆβj
i to be the j-th
column of Xi for task i. In other words, we allow for (k − 1)p number of 0 in X j
i .

i to refer the i-th element (task) in ˆβj. We deﬁne X j = DIAG(X j

k) and X j

1 , ..., X j

T

− 2X j
i

(Y − X ˆβ) + λ

α

+ (1 − α)

= 0, when ˆβj

i (cid:54)= 0, ˆβj (cid:54)= 0,

(cid:40)

ˆβj
i
(cid:107) ˆβj
i (cid:107)2

(cid:41)

ˆβj
i
(cid:107) ˆβj(cid:107)2/

√

k

(Y − X ˆβ) + λ(1 − α)

= λαvj

i , with (cid:107)vj

i (cid:107)2 ≤ 1, when ˆβj

i = 0, ˆβj (cid:54)= 0,

(57)

T

− 2X j
i

(cid:13)
(cid:13)−2X j T
(cid:13)

ˆβj
i
(cid:107) ˆβj(cid:107)2/

√

k

(Y − X ˆβ)

≤ λ

k, when ˆβj = 0.

√

(cid:13)
(cid:13)
(cid:13)2

λ,ξ = {j ∈ 1, 2, ..., p|group j is active for ˆBλ,ξ}. For each j in DP

i s from X j, would form a matrix X j

Let DP
Their corresponding columns X j
of all ˆβj

i = 0. Their corresponding columns X j

i s from X j, would form a matrix X j

λ,ξ, we deﬁne ˆβj
∗. For each j in DP

∗ to be the vector of all ˆβj
λ,ξ, we deﬁne ˆβj
∗C . Then, from (57),

i (cid:54)= 0.
∗C to be the vector

DP
λ,ξ
(cid:88)

j=1

T

(cid:107)2X j
∗

(Y − X ˆβ)(cid:107)2

2 ≥ λ2(1 − α)2k

(58)

DP
λ,ξ
(cid:88)

j=1

(cid:107) ˆβj
∗(cid:107)2
2
(cid:107) ˆβj(cid:107)2
2

Based on the fact that (cid:107)a + b(cid:107)2

2 ≥ ((cid:107)a(cid:107)2 − (cid:107)b(cid:107)2)2

DP
λ,ξ
(cid:88)

j=1

=

≥

(cid:40)

DP
λ,ξ
(cid:88)

(cid:40)

j=1

DP
λ,ξ
(cid:88)

j=1

T

(cid:107)2X j
∗C

(Y − X ˆβ)(cid:107)2

2 ≥

(cid:32)

DP
λ,ξ
(cid:88)

j=1

λ(1 − α)

k

√

(cid:107) ˆβj
∗C (cid:107)2
(cid:107) ˆβj(cid:107)2

(cid:33)2

− λα(cid:107)vj

∗C (cid:107)2

λ2(1 − α)2k

+ λ2α2(cid:107)vj

∗C (cid:107)2

2 − 2λ2α(1 − α)

(cid:107) ˆβj
∗C (cid:107)2
2
(cid:107) ˆβj(cid:107)2
2

(cid:107) ˆβj
∗C (cid:107)2
2
(cid:107) ˆβj(cid:107)2
2

λ,ξ

DP
(cid:88)

j=1

(cid:107) ˆβj
∗C (cid:107)2
2
(cid:107) ˆβj(cid:107)2
2

λ2(1 − α)2k

+ λ2α2(cid:107)vj

∗C (cid:107)2

2 − λ2α(1 − α)

k

+ (cid:107)vj

∗C (cid:107)2

2

= λ2(1 − α)(1 − 2α)k

− λ2α(1 − 2α)

(cid:107)vj

∗C (cid:107)2

2

λ,ξ

DP
(cid:88)

j=1

√

k

(cid:107) ˆβj
∗C (cid:107)2
(cid:107) ˆβj(cid:107)2

(cid:41)

(cid:107)vj

∗C (cid:107)2

(cid:34)

(cid:107) ˆβj
∗C (cid:107)2
2
(cid:107) ˆβj(cid:107)2
2

(cid:35)(cid:41)

(59)

When can Multi-Site Datasets be Pooled for Regression?

Based on (58) and (59), we have

(cid:107)2X T
Ap

λ,ξ

(Y − X ˆβ)(cid:107)2

2 =

(cid:107)2X j T

(Y − X ˆβ)(cid:107)2

2 =

T

(cid:107)2X j
∗

(Y − X ˆβ)(cid:107)2

2 +

T

(cid:107)2X j
∗C

(Y − X ˆβ)(cid:107)2
2

(60)

DP
λ,ξ
(cid:88)

j=1

DP
λ,ξ
(cid:88)

j=1

(cid:107) ˆβj
∗(cid:107)2
2
(cid:107) ˆβj(cid:107)2
2

DP
λ,ξ
(cid:88)

j=1

DP
λ,ξ
(cid:88)

j=1

(cid:107) ˆβj
∗C (cid:107)2
2
(cid:107) ˆβj(cid:107)2
2

DP
λ,ξ
(cid:88)

j=1

DP
λ,ξ
(cid:88)

j=1

≥ λ2(1 − α)2k

+ λ2(1 − α)(1 − 2α)k

− λ2α(1 − 2α)

(cid:107)vj

∗C (cid:107)2

2

≥ λ2(1 − α)(1 − 2α)k

j=1
≥ λ2(1 − α)(1 − 2α)k|DP
= λ2(1 − 2α)2k|DP

λ,ξ| = λ2(1 − 2α)2|AP

λ,ξ|

DP
λ,ξ
(cid:88)

2

∗(cid:107)2

(cid:107) ˆβj

∗C (cid:107)2

2 + (cid:107) ˆβj
(cid:107) ˆβj(cid:107)2
2
λ,ξ| − λ2α(1 − 2α)k|DP

λ,ξ|

− λ2α(1 − 2α)

(cid:107)vj

∗C (cid:107)2

2

DP
λ,ξ
(cid:88)

j=1

The next lemma provides an asymptotic upper bound on the number of selected variables, the proof of which is similar to
Lemma 5 in (Meinshausen & Yu, 2009).

Lemma 3.8. Assume conditions in Theorem 3.2, with probability converging to 1 for n → ∞,

sup
0≤ξ≤1

|AP

λ,ξ| ≤

(cid:26)(cid:18)

1 +

(cid:19)

α
1 − 2α

(cid:112)ksp +

(cid:27)2

α
1 − 2α

√

sh

Proof. Based on Lemma 3.7,

(1 − 2α)2kmp = (1 − 2α)2 sup
0≤ξ≤1

|AP

λ,ξ| ≤

sup
0≤ξ≤1

(cid:107)2X T
Ap

λ,ξ

(Y − X ˆβλ,ξ)(cid:107)2
2

1
λ2

We decompose the right side into two parts and then have

(1 − 2α)2kmp ≤

(cid:32)

1
λ

sup
0≤ξ≤1

(cid:107)2X T
Ap

λ,ξ

X(β∗ − ˆβλ,ξ)(cid:107)2 +

1
λ

sup
0≤ξ≤1

(cid:107)2X T
Ap

λ,ξ

(cid:15)(cid:107)2

(cid:33)2

Similarly, we know from proof in Lemma 3.5 that

Based on the deﬁnition of λ, there exists a constant (cid:36)1 > 0, such that

sup
0≤ξ≤1

(cid:107)2X T
Ap

λ,ξ

(cid:15)(cid:107)2

2 ≤ 2kmp log(kp)σ2 ¯n

sup0≤ξ≤1(cid:107)2X T
Ap

(cid:15)(cid:107)2
2

λ,ξ

λ2

≤ (cid:36)2

1kmp

Therefore, we have

(1 − 2α)2kmp ≤

(cid:32)

1
λ

sup
0≤ξ≤1

(cid:107)2X T
Ap

λ,ξ

X(β∗ − ˆβλ,ξ)(cid:107)2 + (cid:36)1

(cid:112)kmp

(cid:33)2

Deﬁne F P

λ,ξ = {i : β∗

i (cid:54)= 0} ∪ AP

λ,ξ. Based on the block trick we used in proof of Lemma 3.4,

(cid:107)X T
Ap

λ,ξ

X(β∗ − ˆβλ,ξ)(cid:107)2

2 ≤ (cid:107)X T
F p

λ,ξ

XF p

λ,ξ

(β∗ − ˆβλ,ξ)(cid:107)2

2 ≤ ¯n2φ2

max(sp + min{

ni, kp})(cid:107)β∗ − ˆβλ,ξ(cid:107)2
2

(71)

k
(cid:88)

i=1

From the assumption on φmax(sp + min{(cid:80)k

i=1 ni, kp}), we know

(cid:107)X T
Ap

λ,ξ

X(β∗ − ˆβλ,ξ)(cid:107)2

2 ≤ ¯n2ρ2

max(cid:107)β∗ − ˆβλ,ξ(cid:107)2

2

(61)

(62)

(63)

(64)

(65)

(66)

(67)

(68)

(69)

(70)

(72)

When can Multi-Site Datasets be Pooled for Regression?

Therefore, we have

(1 − 2α)2kmp ≤

¯nρmax sup
0≤ξ≤1

(cid:107)β∗ − ˆβλ,ξ(cid:107)2 + (cid:36)1

(cid:112)kmp

(cid:33)2

(cid:32)

(cid:32)

2
λ

2
λ

≤

¯nρmax(cid:107)β∗ − ˆβλ,0(cid:107)2 +

2
λ

¯nρmax sup
0≤ξ≤1

(cid:107) ˆβλ,0 − ˆβλ,ξ(cid:107)2 + (cid:36)1

(cid:112)kmp

(cid:33)2

Because β is the unfolded vector of B, actually sup0≤ξ≤1 (cid:107) ˆβλ,0 − ˆβλ,ξ(cid:107)2 = sup0≤ξ≤1 (cid:107) ˆBλ,0 − ˆBλ,ξ(cid:107)F . From Lemmas
3.5 and 3.6, deﬁnition of λ and the assumption on φmin, we obtain the bound

4¯n2ρ2
λ2

max

sup
0≤ξ≤1

(cid:107) ˆβλ,0 − ˆβλ,ξ(cid:107)2

2 ≤ C

4¯n2ρ2
λ2

max

2 log(kp)
¯n

kmp
φ2
min(mp)

σ2 ≤ (cid:36)2

2kmp

Here, (cid:36)2 > 0 is a constant. We deﬁne (cid:36) = (cid:36)1 + (cid:36)2. Now, we obtain

(1 − 2α)2kmp ≤

¯nρmax(cid:107)β∗ − ˆβλ,0(cid:107)2 + (cid:36)(cid:112)kmp

(cid:19)2

(cid:18) 2
λ

By setting the constant term in λ large enough, we can have (cid:36)/(1 − 2α) ≤ 5(cid:36) ≤ 0.026, and hence

kmp ≤ (18/17.5)2(2ρmax)2 ¯n2(cid:107)β∗ − ˆβλ,0(cid:107)2

2

(1 − 2α)2λ2 ≤

(cid:26)(cid:18)

1 +

(cid:19)

α
1 − 2α

(cid:112)ksp +

(cid:27)2

α
1 − 2α

√

sh

The last inequality is obtained by plugging in Lemma 3.4. The constant can be 1 by setting the constant term in λ large
enough.

Follow from Lemmas 3.5,3.6, and 3.8, the next lemma bounds the variance part of the sparse multi-sites Lasso estimator:

Lemma 3.9. Assume conditions in Theorem3.2, there exists a constant ω2 > 0, with probability converging to 1 for
n → ∞,

(cid:107)Bλ − ˆBλ,1(cid:107)2

F = (cid:107) ˆBλ,0 − ˆBλ,1(cid:107)2

F ≤ ω2σ2 k¯s log(kp)

¯n

Proof. We have deﬁned Bλ as the estimator ˆBλ,0 with no noise before Lemma 3.4.
Based on Lemmas 3.5 and 3.6

(cid:107) ˆBλ,0 − ˆBλ,1(cid:107)2

F ≤

2 log kp
¯n

kmp
φ2
min(mp)

σ2

Based on Lemma 3.8, assumption on φmin and 0 ≤ α ≤ 0.4,

(cid:107) ˆBλ,0 − ˆBλ,1(cid:107)2

F ≤

2 log kp
¯n

kmp
φ2
min(mp)

σ2 ≤ ω2σ2 k¯s log(kp)

¯n

The lemma 3.4 and 3.9 together complete the proof of Theorem 3.2

3.2. Proof of Theorem 3.3:

The proof is similar to that of Theorem 3.2. Recall that in this case, however, we do not penalize
Hence, we have the following result about bias contribution of Theorem 3.3.
Lemma 3.10. Assume conditions in Theorem 3.3. The Frobenius norm of Γλ is then bounded for sufﬁciently large values
of ¯n, given a constant ω1 > 0, by

k on group penalty.

√

(cid:107)Γλ(cid:107)2

F ≤ ω1σ2 k˜s log(kp)

¯n

(73)

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

When can Multi-Site Datasets be Pooled for Regression?

Proof. The proof procedure is same as Lemma 3.4. But instead of (42), we now have

Λ(Γλ) ≤ 2{(1 − ˜α)

sp + ˜α

sh}(cid:107)γλ(cid:107)2 = 2

k˜s(cid:107)γλ(cid:107)2

√

√

√

because we do not have
˜α(cid:112)sh/k}2, instead of ¯s = {(1 − ˜α)

√

sp + ˜α(cid:112)sh/k}2.

√

k penalization on group penalty. Hence, in Lemma 3.10, we have ˜s = {(1 − ˜α)(cid:112)sp/k +

For restricted OLS estimation, we redeﬁne few things here. For every subset ψ ⊂ {1, ..., kp} with |ψ| ≤ (cid:80)k
i=1 ni, we
deﬁne Xψ to be the combination of columns from design matrix X, where X = DIAG(X1, X2, ..., Xk). The restricted
OLS-estimator of the noise vector (cid:15)T = ((cid:15)1, ..., (cid:15)k)T is then given by,

ˆθψ = (X T

ψ Xψ)−1X T
ψ (cid:15)

For the variance contribution, the proof is similar to that of Theorem 3.2. We present the required Lemmas for Theorem
3.3 here.

Lemma 3.11. Let mn be a sequence with mn = o(k¯n) and mn → ∞ for ¯n → ∞. It holds with probability converging to
1 for n → ∞

Lemma 3.12. If, for a ﬁxed value of λ, the number of active variables of de-noised estimators ˆBλ,ξ is for every 0 ≤ ξ ≤ 1
bounded by m, then

max
ψ:|ψ|≤mn

(cid:107)ˆθψ(cid:107)2

2 ≤

2 log kp
¯n

mn
φ2
min(mn)

σ2

sup
0≤ξ≤1

(cid:107) ˆBλ,0 − ˆBλ,ξ(cid:107)2

F ≤ C max

ψ:|ψ|≤m

(cid:107)ˆθψ(cid:107)2
2

with C as a generic constant.

λ,ξ be the set of active variables of the de-noised estimator ˆBλ,ξ. Let mn to be the largest number of active variables

Let A1
over all values of 0 ≤ ξ ≤ 1. Then we have mn = sup0≤ξ≤1 |A1
Lemma 3.13. For any 0 ≤ ˜α ≤ 1, we have

λ,ξ|.

where we deﬁned before that X = DIAG(X1, ..., Xk), Y T = (Y T
ˆBλ,ξ by rows. XA1
Lemma 3.14. Assume conditions in Theorem 3.3, with probability converging to 1 for n → ∞,

is Xψ when ψ = A1

1 , ..., Y T

λ,ξ

λ,ξ

k ). ˆβλ,ξ is the transpose of unfolded vector of

|A1

λ,ξ|λ2 ˜α2 ≤ (cid:107)2X T
A1

λ,ξ

(Y − X ˆβλ,ξ)(cid:107)2
2

sup
0≤ξ≤1

|A1

λ,ξ| ≤

(cid:26)√

sh +

(cid:27)2

1 − ˜α
˜α

√

sp

Lemma 3.15. Assume conditions in Theorem3.3, there exists a constant ω2 > 0, with probability converging to 1 for
n → ∞,

(cid:107)Bλ − ˆBλ,1(cid:107)2

F = (cid:107) ˆBλ,0 − ˆBλ,1(cid:107)2

F ≤ ω2σ2 k˜s log(kp)

¯n

Lemma 3.10 and Lemma 3.15 complete the proof of Theorem 3.3

(82)

(83)

(84)

(85)

(86)

(87)

(88)

When can Multi-Site Datasets be Pooled for Regression?

3. Extra set of simulations (corresponding to Section 4.1 in the main paper)

3.1. Hypothesis Test Simulation when p = 6

(a)

(c)

(b)

(d)

Figure 1. The ﬁgure is similar to the simulations done in Figure 3 (which is also the one Figure 3 in the main paper). However, here the
dimension p of β is 6 instead of 3. (a,c) are MSE of ˆβ and the corresponding acceptance rate of our hypothesis test (from Section2.1).
(b,d) are MSE of ˆβ and ˆγ1 and the corresponding acceptance rate (from Section2.2). These are based on 100 bootstrap repetitions. The
solid line in (c,d) represents the point where the condition from Theorem 2.3 is equal to 1. The dotted line is when MSE of ˆβ is the same
for single-site and multi-site models.

3.2. Sparse Multi-Sites Lasso Simulation

Table 1. Add multi-sites Lasso on Lasso.

α

0

0.05

0.95

0.97 (OUR)

1

CDR
CDV
CDG

0.1423
78
5

0.1463
78
5

0.2747
75
3

0.2863
75
3

0.2955
73
1

We report correctly discovered number of active variables (CDV), ratio of CDV and total number of discovered variables
(CDR), and correctly discovered number of always-active features (CDG).
From Table1 and Table 2 we see that our chosen α helps sparse multi-sites Lasso to discover more or preserve always-active
features. The number and rate of correctly discovered number of active variables given by our chosen α are also among
the best.

2.55.07.5456789101112Sample Size (log2 scale)Square Root of MSEMSESingle siteTwo sites255075456789101112Sample Size (log2 scale)Square Root of MSEMSEb  single siteb  two sitesg1  single siteg1  two sitesSufficient ConditionSame MSE0.000.250.500.75456789101112Sample Size (log2 scale)Acceptance RateSufficient ConditionSame MSE0.000.250.500.751.00456789101112Sample Size (log2 scale)Acceptance RateWhen can Multi-Site Datasets be Pooled for Regression?

Table 2. Add Lasso on multi-sites Lasso.

α

0

0.05

0.25 (OUR)

0.95

1

CDR
CDV
CDG

0.2292
80
16

0.2381
80
16

0.2453
79
15

0.2841
75
11

0.2885
73
11

3.3. Figure Examples for Choosing α

(a)

(b)

Figure 2. These plots show that site-active set from simultaneous inference provides information of always-active features (which is then
used to choose the hyper-parameters α an λ). In (a), we add Lasso on multi-sties Lasso, and α = 0.25 is chosen. Similarly, in Figure(b),
we add multi-sites Lasso on Lasso, and α = 0.97 is chosen.
We here point out a caveat about our choice of α when sparsity patterns share few features and always-active features exist.
In this setting, we do want to discover more always-active features. Hence, we decrease α from 1 and stop at the point
where we just select one more always-active feature. In other words, we choose the α left to the one described in main
body.

101214160.000.250.500.751.00anumber of always−active featuresTypeCorrect discovered(truth)Correct discovered(estimate)2460.000.250.500.751.00anumber of always−active featuresTypeCorrect discovered(truth)Correct discovered(estimate)4. Longer version of Section 4 from the main paper

When can Multi-Site Datasets be Pooled for Regression?

(a)

(c)

(b)

(d)

Figure 3. (a,c) are MSE of ˆβ and the corresponding acceptance rate of our hypothesis test (from Section2.1). (b,d) are MSE of ˆβ and ˆγ1
and the corresponding acceptance rate from Section 2.2). These are based on 100 bootstrap repetitions. The solid line in (c,d) represents
the point where the condition from Theorem 2.3 is equal to 1. The dotted line is when MSE of ˆβ is the same for single-site and multi-site
models.
We now provide few more details about the different curves observed in Figure 3, beyond what is reported in the main paper
due to space constraint. First, we check whether the gap between the sufﬁcient condition (from Theorem 2.3) and the point
where single-site and multi-site models have same MSE is small. The solid lines in Figure 3(c,d) correspond to the point
where the condition value deﬁned in Theorem 2.3 is equal to 1. The dotted lines (where condition value is approximately
3.3) are the points where the MSE of multi-site model starts to increase above the MSE of single-site one. In other words,
to the left of the dotted lines that MSE of ˆβ from multi-sites model is smaller than single-site model. To the right of these
lines it is larger. We see that the gap is reasonably small. We then check the type I error of our hypothesis test. On the
left side of solid lines, the sufﬁcient condition holds and our hypothesis test accepts the combination with high rate around
95%, i.e., the type I error is well-controlled. Further, the power of our hypothesis test is evident when MSE of ˆβ from
multi-sites model is worse than single-site model. Though our sufﬁcient condition is conservative for the combination, by
noticing that χ2 test is progressive, our test has a high power on the right side of dotted line. In the regime between the two
lines, the multi-sites model has slightly better MSE of ˆβ compared to single-site model, and our hypothesis test accepts the
combination with high rate.

00.1250.250.511.52456789101112Sample Size (log2 scale)Square Root of MSEMSESingle siteTwo sites00.1250.250.5161116456789101112Sample Size (log2 scale)Square Root of MSEMSEb  single siteb  two sitesg1  single siteg1  two sitesSufficient ConditionSame MSE0.000.250.500.751.00456789101112Sample Size (log2 scale)Acceptance RateSufficient ConditionSame MSE0.000.250.500.751.00456789101112Sample Size (log2 scale)Acceptance RateWhen can Multi-Site Datasets be Pooled for Regression?

(a)

(b)

Figure 4. (a) shows the solution path of λ when sparsity patterns share few features across sites, and group Lasso penalty is added to
balance Lasso penalty. (b) shows the alternate regime where sparsity patters are similar. The (cid:96)2 loss is plotted, based on 10-fold cross
validation.

4.1. AD dataset details

The two datasets we use are – an open-source Alzheimer’s Disease Neuroimage Initiative (ADNI) dataset, and a local
dataset (ADlocal). ADNI is an open consortium with the goal of understanding AD related cognitive decline, and in the
process, develop clinical interventions aimed at delaying the disease onset. ADlocal corresponds to a recent (smaller)
initiative local study for the AD related decline. We used 318 samples from ADNI and 156 samples from ADlocal. The
input variables are 8 Cerebrospinal ﬂuid (CSF) protein levels, and the response is hippocampus volume. The CSF proteins
are “1-38-Tr”, “1-40-Tr”, “1-42-Tr”, “NFL”, ”AB42”, “htau”, “ptau181”, and “Neurogranin”. The two datasets have
different age and diagnosis distributions, and hence, we subsample 81 samples from either of sites to control age and
diagnosis variation. Using these 81 samples from each dataset, we perform domain adaptation (using a maximum mean
discrepancy objective as a measure of distance between the two marginals) and transform CSF proteins from ADlocal to
match ADNI. The transformed data are then used to evaluate our proposed framework. The results in Figure 5 are already
explained in the main body.

1.52.02.53.00.010.020.030.040.05lSquare Root of Cross−Validation Errora = 0 (group lasso)a = 0.05 (fixed choice)a = 0.95 (fixed choice)a = 0.97 (our method)a = 1 (lasso)1.31.41.51.61.70.010.020.030.040.05lSquare Root of Cross−Validation Errora = 0 (group lasso)a = 0.05 (fixed choice)a = 0.25 (our method)a = 0.95 (fixed choice)a = 1 (lasso)When can Multi-Site Datasets be Pooled for Regression?

(a)

(c)

(b)

(d)

Figure 5. Evaluating combined models. (a,c) In this ﬁrst setting x-axis represents number/fraction of ADNI labeled samples used in
training along with ADlocal labeled data. The dotted line in (a) is where the sample sizes of ADNI and ADlocal in training datasets
match. y-axis shows square root of mean prediction error (computed on the remaining unused ADNI data) scaled by estimated noise
level in ADNI responses. Error bars give 95% conﬁdence interval. (c) shows the acceptance rate of our hypothesis test. (b,d) show the
same evaluations for the alternate setting where equal number of ADNI and ADlocal samples are used for training.

References

Liu, Han and Zhang, Jian. Estimation consistency of the group lasso and its applications. In AISTATS, pp. 376–383, 2009.

Meinshausen, Nicolai and Yu, Bin. Lasso-type recovery of sparse representations for high-dimensional data. The Annals

of Statistics, pp. 246–270, 2009.

Simon, Noah, Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert. A sparse-group lasso. Journal of Computational

and Graphical Statistics, 22(2):231–245, 2013.

Same sample sizes1.051.101.151.21.2532(10%)64(20%)95(30%)127(40%)159(50%)191(60%)223(70%)254(80%)286(90%)Sample size of labeled ADNISquare root of MPEMethodlocal (before) + ADNIlocal (after) + ADNIADNI1.051.101.1551(16%)76(24%)102(32%)127(40%)Sample sizeSquare Root of MPEMethodlocal (before) + ADNIlocal (after) + ADNIADNIPower increases0.000.250.500.751.0032(10%)64(20%)95(30%)127(40%)159(50%)191(60%)223(70%)254(80%)286(90%)Sample size of labeled ADNIAcceptance RateMethodlocal (before) + ADNIlocal (after) + ADNIPower increases0.250.500.751.0051(16%)76(24%)102(32%)127(40%)Sample sizeAcceptance RateMethodlocal (before) + ADNIlocal (after) + ADNI