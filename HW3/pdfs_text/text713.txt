Supplementary Material for Stochastic Adaptive Quasi-Newton
Methods for Minimizing Expected Values

Wenbo Gao 1 Donald Goldfarb 1 Chaoxu Zhou 1

]

A. The Empirical Process Framework

We use the framework developed by Goldfarb, Iyengar, and Zhou (2017) for proving convergence of stochastic
algorithms. These results originate in empirical process theory (W. van der Vaart & Wellner, 1996). The problem
to be minimized has the form

We require the following assumptions on F, f for our analysis:

F (x) = Eξf (x, ξ)

min
x

Assumptions:

respect to x satisﬁes

1. There exist constants L ≥ (cid:96) > 0 such that for every x ∈ Rn and every realization of ξ, the Hessian of f with

(cid:96)I (cid:22) ∇2

xf (x, ξ) (cid:22) LI

That is, f (x, ξ) is strongly convex for all ξ, with the eigenvalues of ∇2
(cid:96) and L, respectively.

xf (x, ξ) bounded below and above by

2. Fk(x) is standard self-concordant for every possible sampling ξ1, . . . , ξmk .

3. There exist compact sets D0 and D with x∗ ∈ D and D0 ⊆ D, such that if x0 is chosen in D0, then for all
k=0 produced by

possible realizations of the samples ξ1, . . . , ξmk for every k, the sequence of iterates {xk}∞
the algorithm is contained within D. We write D = sup{(cid:107)x − y(cid:107) : x, y ∈ D} for the diameter of D.

Furthermore, we assume that the objective values and gradients are bounded:

u = sup

f (x, ξ) < ∞

l = inf
ξ

inf
x∈D

f (x, ξ) > −∞

γ = sup

(cid:107)∇f (x, ξ)(cid:107) < ∞

sup
x∈D

sup
x∈D

ξ

ξ

The key theorem of this framework is a concentration bound which limits the divergence of Fk(x) from F (x).
Theorem A.1 (Corollary 1, (Goldfarb et al., 2017)). For any δ > 0 and 0 < (cid:15) < min{D, δ
|Fk(x) − F (x)| ≥ δ) ≤ 2nn/2 Dn

2L }, we have

−

(cid:21)

(cid:20)

(cid:15)n exp

mk(δ − 2L(cid:15))2
2(u − l)2

P(sup
x∈D

1Department of Industrial Engineering and Operations Research, Columbia University. Correspondence to: Chaoxu

Zhou <cz2364@columbia.edu>.

Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright
2017 by the author(s).

Consequently, if mk ≥ 3, then we have

Stochastic Adaptive Quasi-Newton Methods

and

where C is the constant

E sup
x∈D

|Fk(x) − F (x)| ≤ C

E|Fk(x∗

k) − F (x∗)| ≤ C

(cid:114) log mk
mk

(cid:114) log mk
mk

C = 4(|u| + |l|)nn/2Dn exp

−n

log

(cid:20)

(cid:18)

(cid:19)(cid:21)

u − l
√
2L
2

√

+ (u − l)

n + 1

(x) are also uniformly bounded for x ∈ D. Hence, we can apply Theorem A.1 to each of the n entries

We will also use Theorem A.1 to bound gk and Gk. Assumption 1 implies that the partial derivatives ∂F
∂xi
and ∂2F
∂xixj
∂F
of the gradient, and each of the n2 entries
∂xi
n2 + n inequalities, we obtain the following concentration inequality for the sampled gradients and Hessians:
Corollary A.2. For any δ > 0 and 0 < (cid:15) < min{D, δ
2L },

of the Hessian. Taking a union bound over the resulting

∂2F
∂xi∂xj

(x)

P(sup
x∈D

(cid:107)Gk(x) − G(x)(cid:107) > δ or sup
x∈D

(cid:107)gk(x) − g(x)(cid:107) > δ) ≤ C1(cid:15)−n exp (cid:2)−C2mk(δ − C3(cid:15))2(cid:3)

where C1, C2, C3 are constants depending only on F .

Recall the deﬁnitions of δk, ρk, αk, and ηk above.
Hessians are those of the empirical objective function. That is to say, ρk = gT
gk and Gk are the gradient and Hessian of Fk.

In our analysis of stochastic methods, the gradients and
dT
k Gkdk, where

k Hkgk and δk =

(cid:113)

We say that a constant c is global if it depends only on the properties of the function F , and is completely
independent of the realization of the samples ξ1, . . . , ξmk .

For convenience, we state again the main result for adaptive step sizes:
Theorem A.3 (Lemma 4.1, (Gao & Goldfarb, 2016)). Let ρk = ∇f (xk)T Hk∇f (xk). If αk is chosen to be
αk = ρk
δ2
k

, then

f (xk + tkdk) ≤ f (xk) − ω(ηk)

where ηk = ρk
δk

and ω is the function ω(z) = z − log(1 + z).

B. Convergence of SA-GD

The SA-GD method corresponds to Hk = I. More generally, we may assume that the sequence of matrices Hk
has bounded eigenvalues.

λI (cid:22) Hk (cid:22) ΛI

for all Hk

(1)

Since this condition is satisﬁed for L-BFGS, the following results also apply to L-BFGS (with slightly diﬀerent
constants).

Theorem B.1. Let (cid:15) > 0 be ﬁxed. At each iteration, we draw m i.i.d samples ξ1, . . . , ξm, where the size of m
satisﬁes

and C is the constant in Theorem A.1 and r = 1 − λ2(cid:96)3/2

. Then we have

√
(

(cid:96)+γ)Λ2L

log m
m

≤

(cid:18) 1 − r
4C

(cid:19)2

(cid:15)2

EF (xk+1) − F (x∗) ≤ (cid:15)

when k = log((cid:15)−12(u − l))/ log r.

Stochastic Adaptive Quasi-Newton Methods

For matrices Hk with bounded eigenvalues, ηk can readily be bounded in terms of the empirical gradients, and
the sequence {ηk}∞
k=0 is bounded.
Theorem B.2. There exists a global constant Γ = γ√
(cid:96)
for all k.

such that ηk ≤ Γ for all k. Furthermore, ηk ≥ λ
√
Λ

(cid:107)gk(cid:107)

L

Proof. By Assumption 1 (strong convexity), Gk satisﬁes (cid:96)I (cid:22) Gk (cid:22) LI. Thus, from the deﬁnition of ηk, we have

By Assumption 3, we ﬁnd that (cid:107)gk(cid:107) = (cid:107)gk(xk)(cid:107) ≤ γ. Hence, we may take Γ = γ√
(cid:96)

. We also ﬁnd that

ηk =

(cid:113)

gT
k Hkgk
gT
k HkGkHkgk

≤

(cid:107)gk(cid:107)(cid:107)Hkgk(cid:107)
√
(cid:96)(cid:107)Hkgk(cid:107)

=

(cid:107)gk(cid:107)

1
√
(cid:96)

ηk =

(cid:113)

gT
k Hkgk
gT
k HkGkHkgk

≥

λ
√

Λ

L

(cid:107)gk(cid:107)

Lemma B.3. The empirical objective function Fk(x) satisﬁes

Fk(xk+1) − Fk(x∗

k) ≤ r(Fk(xk) − Fk(x∗

k))

for the global constant r = 1 − (cid:96)

(1+Γ)L < 1.

Proof. Observe that the function ω(z) satisﬁes ω(z) ≥ 1
strongly convex function Fk satisﬁes (cid:107)gk(x)(cid:107)2 ≥ 2(cid:96)(Fk(x) − Fk(x∗
that

2 (1 + Γ)−1z2 for all z ∈ [0, Γ]. Also, recall that the
k)). By Theorem A.3 and Theorem B.2, we ﬁnd

Fk(xk+1) − Fk(x∗

k) ≤ Fk(xk) − Fk(x∗

k) − ω(ηk) ≤ Fk(xk) − Fk(x∗

k) −

1
2
1
2

(1 + Γ)−1η2
k
(1 + Γ)−1 λ2
Λ2L

(cid:107)gk(cid:107)2

(Fk(xk) − Fk(x∗

k))

≤ Fk(xk) − Fk(x∗

(cid:18)

≤

1 −

λ2(cid:96)
(1 + Γ)Λ2L

k) −
(cid:19)

Thus, we may take r = 1 − λ2(cid:96)

(1+Γ)Λ2L . For SA-GD in particular, λ = Λ = 1, so r = 1 − (cid:96)

(1+Γ)L .

We are now ready to prove Theorem B.1.

Proof. By Lemma B.3, we calculate that

Fk(xk+1) − Fk(x∗

k) ≤ r(Fk(xk) − Fk(x∗

k))

= r(Fk−1(xk) − Fk−1(x∗

k−1))

+ r(Fk(xk) − F (xk) − Fk−1(xk) + F (xk))
k−1) − F (x∗) − Fk(x∗
+ r(Fk−1(x∗
k−1))

≤ r(Fk−1(xk) − Fk−1(x∗

k) + F (x∗))

+ r(sup
x∈D
+ r(|Fk(x∗

|Fk(x) − F (x)| + sup
x∈D

|Fk−1(x) − F (x)|)

k) − F (x∗)| + |Fk−1(x∗

k−1) − F (x∗)|)

Stochastic Adaptive Quasi-Newton Methods

By iterating this expansion, we ﬁnd that

Fk(xk+1) − Fk(x∗

k) ≤ rk(F0(x1) − F0(x∗

0))

+

+

k
(cid:88)

j=1

k
(cid:88)

j=1

rj(sup
x∈D

|Fk+1−j(x) − F (x)| + sup
x∈D

|Fk−j(x) − F (x)|)

rj(|Fk+1−j(x∗

k+1−j) − F (x∗)| + |Fk−j(x∗

k−j) − F (x∗)|)

Decompose Fk(xk+1) − Fk(x∗

k) as

Fk(xk+1) − Fk(x∗

k) = F (xk+1) − F (x∗) + [Fk(xk+1) − F (xk+1)] + [F (x∗) − Fk(x∗

k)]

We can move the terms in square brackets to the right hand side, and upper bound them, to obtain

F (xk+1) − F (x∗) ≤ rk(F0(x1) − F0(x∗

0))

|Fk(x) − F (x)|

+ sup
x∈D

k
(cid:88)

+

j=1
+ |Fk(x∗
k
(cid:88)

+

j=1

k) − F (x∗)|

rj(sup
x∈D

|Fk+1−j(x) − F (x)| + sup
x∈D

|Fk−j(x) − F (x)|)

(2)

rj(|Fk+1−j(x∗

k+1−j) − F (x∗)| + |Fk−j(x∗

k−j) − F (x∗)|)

Suppose that we draw a constant number of samples mk = m at each iteration. Taking expectations on both
sides of equation (2) and applying the concentration bound of Theorem A.1, we obtain

EF (xk+1) − F (x∗) ≤ rk(u − l) + 2C

(cid:114)

log m
m

k
(cid:88)

j=0

rj

(cid:114)

2C
1 − r

log m
m

≤ rk(u − l) +

In order to obtain an (cid:15)-optimal solution, we may use suﬃciently large samples, and take suﬃciently many
iterations, so that

rk(u − l) ≤
(cid:114)

log m
m

≤

2C
1 − r

(cid:15)
2
(cid:15)
2

This yields the given bounds on m and k in Theorem B.1.

In particular, it suﬃces to take m = O((cid:15)−2 log (cid:15)−1) and k = O(log (cid:15)−1).

C. Convergence of SA-BFGS

Our goal in this section is to prove that SA-BFGS converges superlinearly with probability 1.
Theorem C.1. Suppose that we draw mk samples on the k-th step, where m−1
k
Then SA-BFGS converges to the optimal solution x∗ almost surely.

converges R-superlinearly to 0.

Our arguments closely follow the proofs given in (Powell, 1976) and (Griewank & Toint, 1982) for the deterministic
BFGS method.

Stochastic Adaptive Quasi-Newton Methods

Along the way, we will also consider the behavior of SA-BFGS when (cid:15)-optimality suﬃces, and mk is held constant.
Note that the results preceding Lemma C.10 do not depend on any particular choice of sample sizes mk.

We introduce the following assumption in this section:

4. The Hessian G(x) is Lipschitz continuous with constant LH .

The adaptive step size is known to satisfy the Armijo-Wolfe conditions in the deterministic setting. A similar
property holds for the empirical objective functions.
Theorem C.2 (Theorem 6.2, (Gao & Goldfarb, 2016)). The adaptive step size tk satisﬁes the Armijo condition
for α = 1

2 , for the empirical objective function Fk(x).

Recall that the SA-BFGS algorithm performs a BFGS update at step k only if tk satisﬁes the Wolfe condition.
If tk does not satisfy the Wolfe condition, then we take a SA-GD step instead. In this case, the direction is −gk
and the step size is the adaptive step size for SA-GD.

We use q(j) to denote the the index of the j-th BFGS step, or equivalently, the index at which the j-th BFGS
update is performed. The steps {q(j)}∞
j=1 where we perform BFGS updates will be referred to as update times.
Later on, we will see that if mk grows at a suﬃcient rate, then all q(j) exist with probability 1.

The following technical lemma is used in the analysis of BFGS; it can also be found in (Byrd et al., 1987) and
(Powell, 1976).
Lemma C.3. Let k = q(j) be an update time. Let Gk = (cid:82) 1
the vectors −gk and sk. Then

0 Gk(xk + τ sk)dτ , and let θk denote the angle between

1. yk = Gksk, and sT

k yk ≤ L(cid:107)sk(cid:107)2.

2. (cid:107)sk(cid:107) ≤ 1

(cid:96) (cid:107)gk(cid:107) cos θk

3. If the Wolfe condition is satisﬁed on step k, then (cid:104)yk, sk(cid:105) ≥ (1 − β)(cid:104)−gk, sk(cid:105) and (cid:107)sk(cid:107) ≥ (1−β)

L (cid:107)gk(cid:107) cos θk.

Proof. The ﬁrst statement follows from the deﬁnition yk = gk(xk+1) − gk(xk). Since Gk(x) (cid:22) LI for all x, we
also have Gk (cid:22) LI, and hence sT

k Gksk ≤ L(cid:107)sk(cid:107)2.

k yk = sT

The second statement follows from the Armijo condition (Theorem C.2) and Taylor’s theorem. Let x be a point
on the line [xk, xk+1] with Fk(xk+1) = Fk(xk) + (cid:104)gk, sk(cid:105) + 1
2 (cid:104)gk, sk(cid:105), we
have 1
k Gk(x)sk ≥ 1

k Gk(x)sk. Since Fk(xk+1) − Fk(xk) ≤ 1

2 m(cid:107)sk(cid:107)2 as desired.

2 (cid:104)−gk, sk(cid:105) ≥ 1

2 sT

2 sT

The Wolfe condition implies that (cid:104)yk, sk(cid:105) = (cid:104)gk(xk+1) − gk(xk), sk(cid:105) ≥ (1 − β)(cid:104)−gk, sk(cid:105). Writing (cid:104)−gk, sk(cid:105) =
(cid:107)gk(cid:107)(cid:107)sk(cid:107) cos θk, we have L(cid:107)sk(cid:107)2 ≥ (1 − β)(cid:107)gk(cid:107)(cid:107)sk(cid:107) cos θk, which gives the last statement.

The next result is the key technical lemma in proving that SA-BFGS converges R-linearly. Its proof is identical
to the deterministic case (Powell, 1976).
Lemma C.4. There exists a global constant c such that

Proof. By considering the BFGS update formula, we have

Recall from Lemma C.3 that yj = Gjsj. Therefore, writing zj = G

1/2
j sj, we have

k
(cid:89)

j=1

(cid:107)gq(j)(cid:107)2
(cid:104)−gq(j), sq(j)(cid:105)

≤ ck

Tr(Bj+1) = Tr(Bj) −

sT
j B2
j sj
sT
j Bjsj

+

yT
j yj
sT
j yj

yT
j yj
sT
j yj

=

zT
j Gjzj
zT
j zj

≤ L

Stochastic Adaptive Quasi-Newton Methods

where the last inequality follows from Assumption 1. Let c1 = Tr(B0) + kL. The BFGS formula implies that
Tr(Bq(k+1)) ≤ Tr(B0) + kL ≤ c1k, and since Bq(k+1) is positive deﬁnite, we also have

Observe that sT
GM) inequality,

j B2

j sj = t2

j (cid:107)gj(cid:107)2 and that sT

j Bjsj = tj(cid:104)−gj, sj(cid:105). By the arithmetic mean-geometric mean (AM-

(3)

Next, we use the recursive formula for the determinant:

k
(cid:88)

j=1

sT
q(j)B2
q(j)sq(j)
sT
q(j)Bq(j)sq(j)

≤ Tr(B0) + kL ≤ c1k

k
(cid:89)

j=1

tq(j)(cid:107)gq(j)(cid:107)2
(cid:104)−gq(j), sq(j)(cid:105)

≤ ck
1

det(Bj+1) =

det(Bj)

yT
j sj
sT
j Bjsj

Since the Wolfe condition is satisﬁed, we have

Therefore,

j sj = (gj(xj+1) − gj(xj))T sj ≥ (1 − β)(cid:104)−gj, sj(cid:105)
yT

det(Bq(k+1)) ≥ det(B0)

k
(cid:89)

j=1

1 − β
tq(j)

By the AM-GM inequality applied to the eigenvalues of Bq(k+1), we ﬁnd that det(Bq(k+1)) ≤ (c1k/n)n ≤ ck
global constant c2. Hence, (cid:81)k
we ﬁnd that

2. Multiplying this together with inequality (3), and taking c = c1

1−β
tq(j)

≤ ck

j=1

2 for a
,

(1−β)c2

k
(cid:89)

j=1

(cid:107)gq(j)(cid:107)2
(cid:104)−gq(j), sq(j)(cid:105)

≤ ck

as desired.

Lemma C.5. At least 1
Lemma C.4.

2 k of the angles θq(1), . . . , θq(k) satisfy cos2 θq(j) > ((cid:96)/c)2, where c is the constant of

Proof. By Lemma C.3, (cid:107)sj(cid:107) ≤ 1

(cid:96) (cid:107)gj(cid:107) cos θj. Substituting this in Lemma C.4 yields

ck ≥

k
(cid:89)

j=1

(cid:107)gq(j)(cid:107)2
(cid:104)−gq(j), sq(j)(cid:105)

≥

k
(cid:89)

j=1

(cid:96)
cos2 θq(j)

= (cid:96)k+1

k
(cid:89)

j=1

1
cos2 θq(j)

Hence, (cid:81)k

j=1 cos2 θq(j) ≥ ((cid:96)/c)k. It follows that at least 1

2 k of the angles must satisfy cos2 θq(j) ≥ ((cid:96)/c)2.

We can proceed to show that stochastic adaptive BFGS converges R-linearly. The argument proceeds by showing
that if k is not an update time, then SA-BFGS inherits the Q-linear convergence rate of SA-GD, and if k = q(j),
then we can measure the decrement with Lemma C.4.

Lemma C.6. If k is not an update time, then

Fk(xk+1) − Fk(x∗

k) ≤ r(Fk(xk) − Fk(x∗

k))

where r = 1 − (cid:96)3/2

√
(

(cid:96)+γ)L

.

Proof. This follows from Lemma B.3 for SA-GD.

Stochastic Adaptive Quasi-Newton Methods

Lemma C.7. Let k = q(j). Then

Fk(xk+1) − Fk(x∗

k) ≤ (cid:0)1 − (1 − β)(cid:96)L−1 cos2 θk

(cid:1) (Fk(xk) − Fk(x∗

k))

Proof. Since the adaptive step size tk satisﬁes the Armijo condition for α = 1

2 , we have

Fk(xk+1) − Fk(xk) ≤

(cid:104)gk, sk(cid:105) = −

(cid:107)gk(cid:107)(cid:107)sk(cid:107) cos θk

1
2

Using Lemma C.3, we rewrite (cid:107)sk(cid:107) in terms of (cid:107)gk(cid:107), cos θk to obtain

Fk(xk+1) − Fk(xk) ≤ −

(1 − β)L−1(cid:107)gk(cid:107)2 cos2 θk

Since (cid:107)gk(cid:107)2 ≥ 2(cid:96)(Fk(xk) − Fk(x∗

k)), we rearrange to obtain

Fk(xk+1) − Fk(x∗

k) ≤ (1 − (1 − β)(cid:96)L−1 cos2 θk)(Fk(xk) − Fk(x∗

k))

1
2

1
2

Theorem C.8. Suppose that we draw samples of size mk at step k, where m−1
k
With probability 1, SA-BFGS converges R-linearly.

converges superlinearly to 0.

Proof. Let ν = max{1 − (1 − β)(cid:96)L−1((cid:96)/c)2, r} < 1. Let I1(k) be the 0-1 indicator variable for the event that
k is a BFGS update time, and let I2(k) be the indicator for the event that k is a BFGS update time and
cos2 θk ≥ ((cid:96)/c)2. Combining Lemma C.6 and Lemma C.7 by using these indicator variables, we have

Fk(xk+1) − Fk(x∗

k) ≤ (1 − (1 − β)(cid:96)L−1 cos2 θk)I1(k)r1−I1(k)(Fk(xk) − Fk(x∗
k))
≤ (1 − (1 − β)(cid:96)L−1((cid:96)/c)2)I2(k)r1−I1(k)(Fk(xk) − Fk(x∗
k))
≤ νI2(k)+1−I1(k)(Fk(xk) − Fk(x∗

k))

For any t ≤ k, let b(t) = (cid:80)t
Therefore

j=0

I1(j). Rewritten with indicators, Lemma C.5 states that (cid:80)t

I2(j) ≥ 1

2 b(t).

j=0

k
(cid:88)

j=0

(I2(j) + 1 − I1(j)) ≥ k −

1
2

b

Deﬁne I3(k) = I2(k) + 1 − I1(k). Iterating the above expansion, we have

Fk(xk+1) − Fk(x∗

k) ≤ νI3(k)(Fk(xk) − Fk(x∗

k))

≤ νI3(k)(Fk−1(xk) − Fk−1(x∗
I3(i)(F0(x0) − F0(x∗

≤ ν

(cid:80)k

i=0

k−1) + (Fk(xk) − Fk−1(xk)) + (Fk−1(x∗
0))

k−1) − Fk(x∗

k))

(cid:80)k

i=j

ν

I3(i)[sup
x∈D

|Fj(x) − F (x)| + sup
x∈D

|Fj−1(x) − F (x)|]

(cid:80)k

i=j

ν

I3(i)[|Fj(x∗

j ) − F (x∗)| + |Fj−1(x∗

j−1) − F (x∗)|]

≤ νk−b/2(F0(x0) − F0(x∗
(cid:88)

0))

νk−b/2−j(sup
x∈D

|Fj(x) − F (x)| + |Fj(x∗

j ) − F (x∗)|)

+

+

k
(cid:88)

j=1

k
(cid:88)

j=1

+ 2

+ 2

0≤j≤k−b/2

k
(cid:88)

(sup
x∈D

j>k−b/2

|Fj(x) − F (x)| + |Fj(x∗

j ) − F (x∗)|)

Stochastic Adaptive Quasi-Newton Methods

In the last inequality, we have simply split the sums into two sums, one running over the indices 0 ≤ j ≤ k − b/2
and the other over k − b/2 < j ≤ k. Writing the left side as

Fk(xk+1) − Fk(x∗

k) = F (xk+1) − F (x∗) + (Fk(xk+1) − F (xk+1)) + (F (x∗) − Fk(x∗

k))

we can move terms to the right to obtain

F (xk+1) − F (x∗) ≤ νk−b/2(F0(x0) − F0(x∗

0))

|Fk(x) − F (x)| + |Fk(x∗

k) − F (x∗)|

+ sup
x∈D

+ 2

(cid:88)

0≤j≤k−b/2

k
(cid:88)

+ 2

j>k−b/2

(sup
x∈D

νk−b/2−j(sup
x∈D

|Fj(x) − F (x)| + |Fj(x∗

j ) − F (x∗)|)

|Fj(x) − F (x)| + |Fj(x∗

j ) − F (x∗)|)

Taking expectations, and applying Theorem A.1 on the right, we have

EF (xk+1) − F (x∗) ≤ νk−b/2(u − l) + 4C

(cid:88)

νk−b/2−j

0≤j≤k−b/2

(cid:115)

log mj
mj

+ 4C

(cid:115)

k
(cid:88)

log mj
mj

j>k−b/2

(4)

(cid:113) log mj
Our choice of mj satisﬁes mj = Ω(ν−2j), so
mj
of νk−b/2, we may ﬁnd a global constant φ, with 1 > φ > ν, and a global constant c3, such that

= O(νj√

j). Hence, by bounding each term with a multiple

Clearly b ≤ k, and thus we ﬁnd that

EF (xk+1) − F (x∗) ≤ c3φk−b/2

EF (xk+1) − F (x∗) ≤ c3φk/2

Now, ﬁx any constant ϕ with φ < ϕ < 1. By Markov’s inequality,

P(F (xk) − F (x∗) ≥ ϕk/2) ≤

E(F (xk) − F (x∗))
ϕk/2

≤ c3

(cid:18) φ
ϕ

(cid:19)k/2

Since (cid:80)∞
k=0

(cid:17)k/2

(cid:16) φ
ϕ

< ∞, the Borel-Cantelli Lemma implies that the sequence of events Ak with

Ak = {F (xk) − F (x∗) > ϕk/2}

occurs ﬁnitely often with probability 1. Therefore, with probability 1, SA-BFGS converges R-linearly.

Before proceeding further, let us digress brieﬂy to consider the behavior of SA-BFGS when we are satisﬁed with
an (cid:15)-optimal solution, and wish to hold the number of samples constant.
Lemma C.9. Let (cid:15) > 0. Suppose we draw m i.i.d samples at each step, where m = O((cid:15)2(log (cid:15)−1)3). Then
SA-BFGS converges in expectation to an (cid:15)-optimal solution after k steps, where k = O((cid:15)−1).

Proof. Note that equation (4) in the proof of Theorem C.8 holds in the absence of any assumptions on the sample
sizes mk. Suppose that we take mk = m. Then we have

EF (xk+1) − F (x∗) ≤ νk−b/2(u − l) + 4C

(cid:88)

νk−b/2−j

≤ νk/2(u − l) + 4C

(cid:114)

0≤j≤k−b/2
(cid:18) 1

log m
m

1 − ν

(cid:19)

+ k/2

(cid:115)

log mj
mj

+ 4C

(cid:115)

k
(cid:88)

log mj
mj

j>k−b/2

Therefore, in order to obtain an (cid:15)-optimal solution from SA-BFGS, we may take

Stochastic Adaptive Quasi-Newton Methods

νk/2(u − l) ≤

(cid:114)

4C
1 − r

log m
m

(cid:18) 1

1 − ν

(cid:19)

+ k/2

≤

(cid:15)
2
(cid:15)
2

Thus, it suﬃces to take k = log((cid:15)−12(u − l))/ log ν. Substituting this value of k into the second inequality, we
see that it suﬃces to take m = O((cid:15)2(log (cid:15)−1)3).

We now concern ourselves with R-superlinear convergence to the true optimal solution. Henceforth, we assume
that the sample sizes grow so that m−1
converges R-superlinearly to 0.
Lemma C.10. We have (cid:80)∞

k=0 ω(ηk) < ∞ with probability 1. In particular, ηk → 0 almost surely.

k

Proof. By Theorem A.3, we ﬁnd that

Fk(xk+1) ≤ Fk(xk) − ω(ηk)

= Fk−1(xk) + (Fk(xk) − Fk−1(xk)) − ω(ηk)

≤ F0(x0) +

(Fj(xj) − Fj−1(xj)) −

ω(ηj)

k
(cid:88)

j=1

k
(cid:88)

j=1

sup
x∈D

k
(cid:88)

j=1

∞
(cid:88)

j=1

sup
x∈D

sup
x∈D

≤ F0(x0) +

|Fj(x) − Fj−1(x)| −

ω(ηj)

≤ F0(x0) + 2

|Fj(x) − F (x)| −

ω(ηj)

≤ F0(x0) + 2

|Fj(x) − F (x)| −

ω(ηj)

k
(cid:88)

j=0

k
(cid:88)

j=0

k
(cid:88)

j=0

k
(cid:88)

j=0

EY =

∞
(cid:88)

j=1

E sup
x∈D

|Fj(x) − F (x)| ≤ C

(cid:115)

∞
(cid:88)

j=1

log mj
mj

Let Y = (cid:80)∞

j=1 supx∈D |Fj(x) − F (x)|. By the monotone convergence theorem and Theorem A.1, we have

By our choice of mj, the latter sum is ﬁnite. This implies that P(Y < ∞) = 1. Since Fk(x) is bounded below
on D by Assumption 3, we necessarily have (cid:80)∞
k=0 ω(ηk) < ∞ whenever Y < ∞. Thus ηk → 0 with probability
1.

Theorem C.11. Fix any β < 1. With probability 1, there exists a ﬁnite index k0 such that the Wolfe condition
is satisﬁed for all k ≥ k0.

Proof. This follows from Theorem 6.3 in (Gao & Goldfarb, 2016), for any realization of the empirical objective
functions F0, F1, . . . such that ηk → 0. By Lemma C.10, the event ηk → 0 occurs with probability 1.

In particular, this implies that with probability 1, there exists a ﬁnite time k0 after which every step is a BFGS
step, and BFGS updates are always performed.
Corollary C.12. With probability 1, we have (cid:80)∞

k=0 (cid:107)xk − x∗(cid:107) < ∞.

Proof. This follows from Theorem C.8. Let {xk}∞
for all k ≥ k0, for some index k0. Since F (x) is strongly convex,

k=0 be any instance of the algorithm where F (xk) ≤ F (x∗)+ϕk/2

for all k ≥ k0. Hence (cid:80)∞

k=0 (cid:107)xk − x∗(cid:107) < ∞. By Theorem C.8, this occurs with probability 1.

(cid:107)xk − x∗(cid:107) ≤

(F (xk) − F (x∗)) ≤

ϕk/2

2
(cid:96)

2
(cid:96)

Stochastic Adaptive Quasi-Newton Methods

Let us deﬁne ek = max{(cid:107)xk − x∗(cid:107), (cid:107)xk+1 − x∗(cid:107)}. Corollary C.12 implies that (cid:80)∞
Next, we perform a detailed analysis of the evolution of Hk+1. By applying Corollary A.2, we can use a modiﬁed
form of the classical argument ((Griewank & Toint, 1982)) on a path-by-path basis.
Corollary C.13. Let σk = m−2/5
ω < 1 such that

. By taking δ = σk in Corollary A.2, we can ﬁnd global constants c4 and

k=0 ek < ∞.

k

P(sup
x∈D

(cid:107)Gk(x) − G(x)(cid:107) > σk or sup
x∈D

(cid:107)gk(x) − g(x)(cid:107) > σk) ≤ c4ωk

Hence, with probability 1, there exists an index k0 such that for all k ≥ k0, we have both sup
x∈D

(cid:107)Gk(x)−G(x)(cid:107) < σk

and sup
x∈D

(cid:107)gk(x) − g(x)(cid:107) < σk.

By construction, {σk} converges to 0 at a R-superlinear rate.

Proof. The ﬁrst part follows by Corollary A.2. Taking (cid:15) = δ

2L+1 , our probability bound is

P(sup
x∈D

(cid:107)Gk(x) − G(x)(cid:107) > σk or sup
x∈D

(cid:107)gk(x) − g(x)(cid:107) > σk) ≤ C1 exp(

n log mk − C2(1 −

2
5

C3
2L + 1

)2m1/5
k )

Since m1/5
k
log mk
follows immediately from the Borel-Cantelli Lemma.

→ 0 and mk = Ω(k5) by construction, we can ﬁnd the desired ω < 1. The second statement then

Let Ω denote the space of paths where (cid:80)∞
k=0 ek < ∞ and for some k0, supx∈D (cid:107)Gk(x) − G(x)(cid:107) ≤ σk and
supx∈D (cid:107)gk(x) − g(x)(cid:107) ≤ σk for all k ≥ k0. By Corollary C.12 and Corollary C.13, P(Ω) = 1. Henceforth, we
restrict our analysis to the paths belonging to Ω.

The BFGS algorithm is invariant under a linear change of variables, so without loss of generality, we may assume
that G(x∗) = I. This corresponds to the change of variables (cid:101)F (y) = F (G(x∗)−1/2y), y = G(x∗)1/2x. Deﬁne two
‘hypothetical’ updates:

(cid:98)Bk+1 = Bk −

(cid:101)Bk+1 = Bk −

k Bk

BksksT
sT
k Bksk
BksksT
sT
k Bksk

k Bk

+

+

k Gk(x∗)

Gk(x∗)sksT
sT
k Gk(x∗)sk
G(x∗)sksT
k G(x∗)
sT
k G(x∗)sk

(cid:107) (cid:101)Bk+1 − I(cid:107)2

F ≤ (cid:107)Bk − I(cid:107)2
F

(cid:107) (cid:101)Hk+1 − I(cid:107)2

F ≤ (cid:107)Hk − I(cid:107)2
F

Lemma C.14. We have

and

and

Proof. For brevity, we write s = sk, B = Bk, H = Hk. By a routine calculation (see §4 of (Griewank & Toint,
1982)), we have

(cid:107) (cid:101)Bk+1 − I(cid:107)2

F − (cid:107)Bk+1 − I(cid:107)2

F = −

1 −

(cid:34)(cid:18)

(cid:32)

(cid:19)2

sT B2s
sT Bs

+ 2

sT B3s
sT Bs

−

(cid:18) sT B2s
sT Bs

(cid:19)2(cid:33)(cid:35)

(cid:107) (cid:101)Hk+1 − I(cid:107)2

F − (cid:107)Hk+1 − I(cid:107)2

F = −

1 −

(cid:34)(cid:18)

(cid:32)

(cid:19)2

sT Hs
sT s

+ 2

sT H 2s
sT s

−

(cid:18) sT Hs
sT s

(cid:19)2(cid:33)(cid:35)

The Cauchy-Schwarz inequality implies that the latter terms in the brackets are non-positive, which gives the
desired result.

Stochastic Adaptive Quasi-Newton Methods

Lemma C.15. Every path in Ω satisﬁes

and

(cid:107)Bk+1 − (cid:101)Bk+1(cid:107) ≤ O(ek + σk)

(cid:107)Hk+1 − (cid:101)Hk+1(cid:107) ≤ ((cid:107)Hk − I(cid:107) + 1)O(ek + σk)

Proof. We again write s = sk, y = yk, B = Bk, H = Hk for brevity.

We can bound the diﬀerence (cid:107)Bk+1 − (cid:98)Bk+1(cid:107), as both updates are performed with sampled gradients, and then
use Corollary C.13 to bound (cid:107) (cid:98)Bk+1 − (cid:101)Bk+1}.
Take ∆ = Gk(x∗)s − y. By Lemma C.3, we can write y = Gk((cid:98)x)s for some (cid:98)x on the line segment [xk, xk+1], and
we deduce that:

1. (cid:96)(cid:107)s(cid:107)2 ≤ yT s ≤ L(cid:107)s(cid:107)2

2. (cid:107)∆(cid:107) ≤ LH ek(cid:107)s(cid:107).

3. yT ∆

sT y ≤ LLH ek

Hence, writing

1

sT y+∆T s = 1

(cid:107)Bk+1 − (cid:98)Bk+1(cid:107) =

−

sT y − yT ∆
sT y+yT ∆ , we have
(cid:13)
yyT
(cid:13)
(cid:13)
sT y
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
≤ O(ek)

(y + ∆)(y + ∆)T
(y + ∆)T s
y∆T + ∆yT + ∆∆T
sT y

−

=

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
yT ∆(yyT + y∆T + ∆yT + ∆∆T )
sT y + yT ∆

Next, write (cid:98)y = Gk(x∗)s and (cid:101)y = G(x∗)s. Since our path lies in Ω, we know that (cid:107)Gk(x∗) − G(x∗)(cid:107) ≤ σk. Let
∆ = (cid:98)y − (cid:101)y, so (cid:107)∆(cid:107) ≤ σk(cid:107)s(cid:107), and perform the same calculation as above to obtain

(cid:107) (cid:98)Bk+1 − (cid:101)Bk+1(cid:107) =

(cid:13)
− (cid:101)y∆T + ∆(cid:101)yT + ∆∆T
(cid:13)
(cid:13)
sT
(cid:13)
≤ O(σk)

(cid:101)y

+ (cid:101)yT ∆((cid:101)y(cid:101)yT + (cid:101)y∆T + ∆(cid:101)yT + ∆∆T )

sT

(cid:101)y + (cid:101)yT ∆

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Hence, (cid:107)Bk+1 − (cid:101)Bk+1(cid:107) ≤ O(ek + σk).

A similar calculation holds for H.

(cid:107)Hk+1 − (cid:98)Hk+1(cid:107) = (cid:107)

ssT
(y + ∆)T s

−

ssT
sT y

+

+

(cid:18) s(y + ∆)T
(y + ∆)T s

−

(cid:19)

syT
sT y

H + H

(cid:18) (y + ∆)sT
(y + ∆)T s

−

(cid:19)

ysT
sT y

s(y + ∆)T H(y + ∆)sT
((y + ∆)T s)2

−

syT HysT
(sT y)2 (cid:107)

It is elementary, though tedious, to verify that

sT y ≤ O(ek) and that the other terms are bounded by
O((cid:107)H(cid:107)ek). The same calculation shows that (cid:107) (cid:98)Hk+1 − (cid:101)Hk+1(cid:107) ≤ O(σk +(cid:107)H(cid:107)σk). Thus, we have (cid:107)Hk+1 − (cid:101)Hk+1(cid:107) ≤
((cid:107)Hk − I(cid:107) + 1)O(ek + σk).

ssT

(y+∆)T s − ssT

Corollary C.16. By Lemma C.15, Lemma C.14, and the triangle inequality,

(cid:107)Bk+1 − I(cid:107) ≤ (cid:107)Bk+1 − (cid:101)Bk+1(cid:107) + (cid:107) (cid:101)Bk+1 − I(cid:107) ≤ (cid:107)Bk − I(cid:107) + O(ek + σk)

and

(cid:107)Hk+1 − I(cid:107) ≤ (cid:107)Hk+1 − (cid:101)Hk+1(cid:107) + (cid:107) (cid:101)Hk+1 − I(cid:107) ≤ ((cid:107)Hk − I(cid:107) + 1)O(ek + σk)

Stochastic Adaptive Quasi-Newton Methods

A lemma of Griewank and Toint shows that this forces the convergence of {(cid:107)Bk − I(cid:107)} and {(cid:107)Hk − I(cid:107)}.
Lemma C.17 (Lemma 3.3 of (Griewank & Toint, 1982)). Let {φk} and {δk} be sequences of non-negative
numbers such that φk+1 ≤ (1 + δk)φk + δk and (cid:80)∞

k=1 δk < ∞. Then {φk} converges.

In our case, we take δk = ek + σk, as (cid:80)∞
Following §4 of (Griewank & Toint, 1982), our previous results yield the Dennis-Mor´e ((Dennis Jr. & Mor´e,
1974)) condition:

k=0(ek + σk) < ∞ by Corollary C.12 and Corollary C.13.

lim
k→∞

(cid:107)(Bk − I)sk(cid:107)
(cid:107)sk(cid:107)

= 0

It only remains to show that this implies R-superlinear convergence in the stochastic setting. Since I = G(x∗),
we have

(cid:107)Bksk − G(x∗)sk(cid:107) = (cid:107) − gk − G(x∗)sk + gk(xk+1) − gk(xk+1)(cid:107)

= (cid:107)gk(xk+1) − gk − G(x∗)sk − gk(xk+1)(cid:107)

= (cid:107)

(Gk(xk + τ sk) − G(x∗))skdτ − gk(xk+1)(cid:107)

(cid:90) 1

0
(cid:90) 1

0

= (cid:107)

(G(xk + τ sk) − G(x∗))skdτ +

(Gk(x + τ sk) − G(xk + τ sk))skdτ − gk(xk+1)(cid:107)

≥ (cid:107)gk(xk+1)(cid:107) − (LH ek + σk)(cid:107)sk(cid:107)

(cid:90) 1

0

and therefore (cid:107)gk(xk+1)(cid:107)
therefore

(cid:107)sk(cid:107) → 0. By Assumption 1, the empirical objective function Fk(x) is strongly convex, and

(cid:107)gk(xk+1)(cid:107)
(cid:107)sk(cid:107)

≥

|(cid:107)gk(xk+1) − gk(x∗)(cid:107) − (cid:107)gk(x∗) − g(x∗)(cid:107)|
(cid:107)xk+1 − x∗(cid:107) + (cid:107)xk − x∗(cid:107)

To complete the analysis, let ak = (cid:107)gk+1(cid:107)
(cid:107)sk(cid:107) , bk = (cid:107)gk(x∗) − g(x∗)(cid:107), and zk = (cid:107)xk − x∗(cid:107). Our above results show
that ak → 0, and bk ≤ σk tends to 0 R-superlinearly. For convenience, we assume without loss of generality that
{bk} converges Q-superlinearly, by replacing {bk} by the Q-superlinear sequence bounding σk if necessary.

Rearrange inequality (5) to obtain

(cid:96)zk+1 = (cid:96)(cid:107)xk+1 − x∗(cid:107) ≤ (cid:107)gk(xk+1) − gk(x∗)(cid:107) ≤ ak(zk+1 + zk) + bk

Eventually, ak < 1

2 (cid:96), as ak → 0. Beyond that point, we ﬁnd that

zk+1 ≤

zk + bk ≤

akzk + bk

ak
(cid:96) − ak

2
(cid:96)

(5)

(6)

Let ck = max{akzk, bk}. Clearly zk+1 ≤ (2 + 2
are two cases to consider. If ck+1 = ak+1zk+1, then

(cid:96) )ck, so it suﬃces to prove that {ck} converges superlinearly. There

ck+1
ck

=

ak+1zk+1
ck

≤ ak+1

=

2 +

ak+1

(cid:18)

(cid:96) )ck

(2 + 2
ck

(cid:19)

2
(cid:96)

and ak → 0. Otherwise, if ck+1 = bk+1, then

ck+1
ck

=

bk+1
ck

≤

bk+1
bk

and by construction, {bk} converges to 0 superlinearly, so bk+1
bk

→ 0.

This proves that zk converges R-superlinearly, and completes the proof of Theorem C.1.

D. Additional Experiments

Stochastic Adaptive Quasi-Newton Methods

To complement the numerical experiments for general stochastic optimization problems, we provide additional
results for ERM (empirical risk minimization) problems. We compare all the algorithms in section 8 on ridge
regression problems, that is,

min
w∈Rp

1
n

n
(cid:88)

i=1

(yi − Xiβ)2 + λ(cid:107)w(cid:107)2
2,

where we set n = 106, Xi ∼ N (0, Σ(ρ)), Σ(ρ) = (1 − ρ2)Ip + ρ2J (here J is the all-ones matrix), β is a ﬁxed p
dimensional vector and λ = 1. We test problems of size p = 100, 500 and ρ = 0, 0.5, 0.9. From the ﬁgures, we

may draw similar conclusions as to those in section 6 for the methods that use an adaptive step length. One

CPU time(s)00.20.40.60.811.21.41.61.82log(f(xk) - f(x*))-10123456ρ = 0, p = 100S-GDSA-GDSA-GD-ISA-BFGSSA-BFGS-ISA-BFGS-GDR-S-GD-CR-S-GD-VCPU time(s)0102030405060log(f(xk) - f(x*))-2-1012345678ρ = 0, p = 500S-GDSA-GDSA-GD-ISA-BFGSSA-BFGS-ISA-BFGS-GDR-S-GD-CR-S-GD-VCPU time(s)00.20.40.60.811.21.41.61.82log(f(xk) - f(x*))-2-10123456ρ = 0.5, p = 100S-GDSA-GDSA-GD-ISA-BFGSSA-BFGS-ISA-BFGS-GDR-S-GD-CR-S-GD-VCPU time(s)0102030405060log(f(xk) - f(x*))-1012345678ρ = 0.5, p = 500S-GDSA-GDSA-GD-ISA-BFGSSA-BFGS-ISA-BFGS-GDR-S-GD-CR-S-GD-VCPU time(s)00.20.40.60.811.21.41.61.82log(f(xk) - f(x*))-1012345ρ = 0.9, p = 100S-GDSA-GDSA-GD-ISA-BFGSSA-BFGS-ISA-BFGS-GDR-S-GD-CR-S-GD-VCPU time(s)0102030405060log(f(xk) - f(x*))-2-1012345678ρ = 0.9, p = 500S-GDSA-GDSA-GD-ISA-BFGSSA-BFGS-ISA-BFGS-GDR-S-GD-CR-S-GD-VStochastic Adaptive Quasi-Newton Methods

interesting ﬁnding in this set of experiments is that the robust SGD methods do not work well especially for
p = 500.

References

Byrd, Richard H., Nocedal, Jorge, and Yuan, Ya-Xiang. Global convergence of a class of quasi-Newton methods

on convex problems. Siam. J. Numer. Anal., (5):1171–1190, 1987.

Dennis Jr., John E. and Mor´e, Jorge J. Characterization of superlinear convergence and its application to

quasi-Newton methods. Math. Comp., 28(106):549–560, 1974.

Gao, Wenbo and Goldfarb, Donald. Quasi-Newton methods: Superlinear convergence without line search for

self-concordant functions. in review. arXiv:1612.06965, 2016.

Goldfarb, Donald, Iyengar, Garud, and Zhou, Chaoxu. Linear convergence of stochastic Frank-Wolfe variants.
In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics, pp. 1066–1074,
2017.

Griewank, Andreas and Toint, Philippe L. Local convergence analysis for partitioned quasi-Newton updates.

Numer. Math., 39:429–448, 1982.

Powell, Michael J. D. Some global convergence properties of a variable metric algorithm for minimization
without exact line searches. In Cottle, Richard and Lemke, C.E. (eds.), Nonlinear Programming, volume IX.
SIAM-AMS Proceedings, 1976.

W. van der Vaart, Aad. and Wellner, Jon A. Weak Convergence and Empirical Processes. Springer New York,

1996.

