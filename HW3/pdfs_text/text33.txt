A Simple Multi-Class Boosting Framework
with Theoretical Guarantees and Empirical Proﬁciency

Ron Appel 1 Pietro Perona 1

Abstract
There is a need for simple yet accurate white-box
learning systems that train quickly and with lit-
tle data. To this end, we showcase REBEL, a
multi-class boosting method, and present a novel
family of weak learners called localized similar-
ities. Our framework provably minimizes the
training error of any dataset at an exponential
rate. We carry out experiments on a variety of
synthetic and real datasets, demonstrating a con-
sistent tendency to avoid overﬁtting. We eval-
uate our method on MNIST and standard UCI
datasets against other state-of-the-art methods,
showing the empirical proﬁciency of our method.

1. Motivation

The past couple of years have seen vast improvements in
the performance of machine learning algorithms. Deep
Nets of varying architectures reach almost (if not better
than) human performance in many domains (LeCun et al.,
2015). A key strength of these systems is their ability to
transform the data using complex feature representations
to facilitate classiﬁcation. However, there are several con-
siderable drawbacks to employing such networks.

A ﬁrst drawback is that validating through many architec-
tures, each of which may have millions of parameters, re-
quires a lot of data and time. In many ﬁelds (e.g. pathology
of not-so-common diseases, expert curation of esoteric sub-
jects, etc.), gathering large amounts of data is expensive
or even impossible (Yu et al., 2015). Autonomous robots
that need to learn on the ﬂy may not be able to afford the
large amount of processing power or time required to prop-
erly train more complex networks simply due to their hard-
ware constraints. Moreover, most potential users (e.g. non-
machine-learning scientists, small business owners, hobby-

1Caltech, Pasadena, USA. Correspondence to:

Ron
Pietro Perona <per-

Appel <appel@vision.caltech.edu>,
ona@vision.caltech.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

(a) Old: Decision Stumps

(b) New: Localized Similarities

Figure1. (a) The typical decision stumps commonly used in
boosting lead to classiﬁcation boundaries that are axis aligned
and not representative of the data. Although these methods can
achieve perfect training accuracy, it is apparent that they heavily
overﬁt. (b) Our method uses localized similarities, a novel family
of simple weak learners (see Sec. 5.1). Paired with a procedure
that provably guarantees exponential loss minimization, our clas-
siﬁers focus on smooth, well-generalizing boundaries.

ists, etc.) may not have the expertise or artistry required to
hypothesize a set of appropriate models.

A second drawback is that the complex representations
achieved by these networks are difﬁcult to interpret and to
analyze. For many riskier applications (e.g. self-driving
cars, robotic surgeries, military drones, etc.), a machine
should only run autonomously if it is able to explain its ev-
ery decision and action. Further, when used towards the sci-
entiﬁc analysis of phenomena (e.g. understanding animal
behavior, weather patterns, ﬁnancial market trends, etc.),
the goal is to extract a causal interpretation of the system in
question; hence, to be useful, a machine should be able to
provide a clear explanation of its internal logic.

For these reasons, it is desirable to have a simple white-box
machine learning system that is able to train quickly and
with little data. With these constraints in mind, we show-
case a multi-class boosting algorithm called REBEL and
a novel family of weak learners called similarity stumps,
leading to much better generalization than decision stumps,
as shown in Fig. 1. Our proposed framework is simple, efﬁ-
cient, and is able to perfectly train on any dataset (i.e. fully
minimize the training error in a ﬁnite number of iterations).

A Simple Multi-class Boosting Framework

The main contributions of our work are as follows:

1. a simple multi-class boosting framework using local-

ized similarities as weak learners (see Sec. 3)

2. a proof that the training error is fully minimized within

a ﬁnite number of iterations (see Sec. 5)

3. a procedure for selecting an adequate learner at each

iteration (see Sec. 5.2)

4. empirical demonstrations of state-of-the-art results on

a range of datasets (see Sec. 7)

2. Background

Boosting is a fairly mature method, originally formulated
for binary classiﬁcation (e.g. AdaBoost and similar vari-
ants) (Schapire, 1990; Freund, 1995; Freund & Schapire,
1996). Multi-class classiﬁcation is more complex than its
binary counterpart, however, many advances have been
made in both performance and theory in the context of
boosting. Since weak learners come in two ﬂavors, bi-
nary and multi-class, two corresponding families of boost-
ing methods have been explored.

The clever combination of multiple binary weak learn-
ers can result in a multi-class prediction. AdaBoost.MH
reduces the K-class problem into a single binary prob-
lem with a K-fold augmented dataset (Schapire & Singer,
1999). AdaBoost.MO and similar methods reduce the
K-class problem into C one-versus-all binary problems
using Error-Correcting Output Codes to select the ﬁnal
hypothesized class (Allwein et al., 2001; Sun et al., 2005;
Li, 2006). More recently, CD-MCBoost and CW-Boost
return a K-dimensional vector of class scores, focusing
each iteration on a (binary) problem of improving the
margin of one class at a time (Saberian & Vasconcelos,
2011; Shen & Hao, 2011). REBEL also returns a vector of
class scores, increasing the margin between dynamically-
selected binary groupings of the K classes at each iteration
(Appel et al., 2016).

When multi-class weak learners are acceptable (and avail-
able), a reduction to binary problems is unnecessary. Ad-
aBoost.M1 is a straightforward extension of its binary
counterpart (Freund & Schapire, 1996). AdaBoost.M2 and
AdaBoost.MR make use of a K-fold augmented dataset
to estimate output label probabilities or rankings for a
given input (Freund & Schapire, 1996; Schapire & Singer,
1999). More recent methods such as SAMME, AOSO-
LogitBoost, and GD-MCBoost are based on linear com-
binations of a ﬁxed set of codewords, outputting K-
dimensional score vectors (Zhu et al., 2009; Sun et al.,
2011; Saberian & Vasconcelos, 2011).

In the noteworthy paper “A Theory of Multiclass Boosting”
(Mukherjee & Schapire, 2010), many of the existing boost-
ing methods were shown to be inadequate at training; either

because they require their weak learners to be too strong,
or because their loss functions are unable to deal with
some training data conﬁgurations. (Mukherjee & Schapire,
2010) outline the appropriate Weak Learning Condition
that a boosting algorithm must require of its weak learn-
ers in order to guarantee training convergence. However,
no method is prescribed with which to ﬁnd an adequate set
of weak learners.

The goal of our work is to propose a multi-class boosting
framework with a simple family of binary weak learners
that guarantee training convergence and are easily inter-
pretable. Using REBEL (Appel et al., 2016) as the multi-
class boosting method, our framework is meant to be as
straightforward as possible so that it is accessible and prac-
tical to more users; outlining it in Sec. 3 below.

3. Our Framework

In this section, we deﬁne our notation, introduce our boost-
ing framework, and describe our training procedure.

Notation
scalars (regular), vectors (bold): x, x ≡ [x1, x2, ...]
constant vectors:
indicator vector:
logical indicator function:
inner product:
element-wise multiplication:
element-wise function:

0 ≡ [0,0, ...], 1 ≡ [1,1, ...]
δδδk (0 with a 1 in the kth entry)
1(LOGICAL EXPRESSION) ∈ {0,1}
hx, vi
x ⊙ v
F[x] ≡ [F(x1), F(x2), ...]

Y

∈

∈

X

≡ {

Rd, y

1,2, ...,K

In the multi-class classiﬁcation setting, a datapoint is repre-
sented as a feature vector x and is associated with a class
label y. Each point is comprised of d features and belongs
to one of K classes: x

⊆
A good classiﬁer reduces the training error while gener-
alizing well to potentially-unseen data. We use REBEL
(Appel et al., 2016) due to its support for binary weak learn-
ers, its mathematical simplicity (i.e.
closed-form solu-
tion to loss minimization), and its strong empirical perfor-
mance. REBEL returns a vector-valued output H, the sum
of T
pairs, where
{
ft : X
→ {±

weak learner f, accumulation vector a
}

and at ∈

RK
:

}

1

}

T

ft(x) at

H(x)

≡

t=1
X
The hypothesized class is simply the index of the maximal
entry in H:

F(x)

arg max

y∈Y {h

H(x), δδδyi}

≡

The average misclassiﬁcation error ε can be expressed as:

ε

≡

1
N

N

n=1
X

1(F(xn)

= yn)

(1)

6
A Simple Multi-class Boosting Framework

REBEL uses an exponential loss function to upper-bound
the average training misclassiﬁcation error:

4. Binarizing Multi-Class Data

ε

≤ L ≡

exp[yn⊙H(xn)], 1

(2)

i

1
2N

N

n=1h
X

1
−

2 δδδyn (i.e. all +1s with a −1 in the yth

where: yn ≡
Being a greedy, additive model, all previously-trained pa-
rameters are ﬁxed and each iteration amounts to jointly op-
timizing a new weak learner f and accumulation vector a.
To this end, the loss at iteration I+1 can be expressed as:

n index)

LI+1 =

1
N

N

n=1h
X

where: wn ≡

1
2

wn, exp[f(xn) yn ⊙a]

(3)

i

exp[yn⊙HI(xn)]

Given a weak learner f, we deﬁne true and false (i.e. correct
and incorrect) multi-class weight sums (sT

f and sF

f ) as:

sT
f ≡

1
N

N

n=1
X

1[f(xn)yn < 0]⊙wn,

1[f(xn)yn > 0]⊙wn

sF
f ≡

1
N

thus:

f +sF
sT

f =

wn,

sT
f −

sF
f =

f(xn) wn⊙ yn

N

1
N

n=1
X

Using these weight sums, the loss can be simpliﬁed to:

N

n=1
X

N

1
N

n=1
X

At each iteration, the ﬁrst step in determining an adequate
weak learner is binarizing the data, i.e. assigning a tempo-
rary binary label to each data point by placing it into one of
two groups. The following manipulations result in a proce-
dure for binarizing datapoints given their boosting weights.

Eq. 5 can be upper-bounded as follows:

U

∗

f = 2
L

h
p

sT
f

⊙sF

f , 1

sT
f +sF

f , 1

i −

i ≤ h

1
2 −

2

1
2 −

x

x,

∀

since:

x(1

x)

−

≤

p
By expanding sT

sF
f ,

U

f ±

(cid:17)

(cid:16)
is expressed as a squared norm:

f ]2
sF
[sT
f −
f +sF
[sT
f ]
}|

1
2

z
D

using: x =

, 1
{
E(6)
sT
sT+sF

N

1
N

n=1
P

=

U

h

*

f(xn) wn⊙yn

2

N

1
N

wn

i

n=1
h
P
where: un ≡

1
√N

, 1

i

=

+

f(xn) un

2

(cid:13)
(cid:13)
(cid:13)
(7)

N

n=1
X

(cid:13)
(cid:13)
(cid:13)
wn⊙ yn
N

wn

r

n=1
P

Eq. 7 can be written as a product of matrices by stacking
all of the un as column vectors of a K
N matrix U and
deﬁning f as a row vector with elements f(xn):

×

⊤

⊤

= f [U

U] f

U

⊤

N

[wn]2

wn

i

n=1
h
P

N

LI+1

≡ Lf ≡ h

sT
f , exp[

a]

+

−

i

sF
f , exp[a]
h

i

(4)

Note that the trace of U

U can be lower-bounded:

In this form, it is easily shown that with the optimal accu-
mulation vector a∗, the loss has an explicit expression:

a∗ =

ln[sT
f ]

ln[sF
f ]

−

∴

∗

f = 2
L

sT
f

⊙ sF

f , 1

i

(5)

(cid:1)

h
p

1
2

(cid:0)

At each iteration, growing decision trees requires an ex-
haustive search through a pool of decision stumps (which
is tractable but time-consuming), storing the binary learner
that best reduces the multi-class loss in Eq. 5. In some sit-
uations, axis-aligned trees are simply unable to reduce the
loss any further, thereby stalling the training procedure.

Our proposed framework circumvents such situations. At
each iteration, instead of exhaustively searching for an ad-
equate learner, we ﬁrst determine an appropriate “binariza-
tion” of the multi-class data (i.e. a separation of the K-class
data into two distinct groups) and then ﬁnd a weak learner
with a guaranteed reduction in loss, foregoing the need for
an exhaustive search.

⊤

tr(U

U) =

N

n=1k
X

2 =

unk

n=1
P
N

*

N

, 1

+ ≥

1
N 2

N

n=1h
X

wn, 1

i

since by Jensen’s inequality:

x2
n ≥

1
N

N

(cid:16)

n=1
X

2

xn
(cid:17)

n=1
X

⊤

U has N (not-necessarily unique) non-
Furthermore, U
negative eigenvalues, each associated with an independent
eigenvector. Let ˆvn be the eigenvector corresponding to the
nth largest eigenvalue λn. Hence, f can be decomposed as:

f =

f ,ˆv1
h

i

ˆv1 +

f ,ˆvni

ˆvn

(8)

∴

= λ1

f ,ˆv1
h

2 +
i

U

2

λnh

f ,ˆvni

λ1

f ,ˆv1
h

2
i

≥

N

n=2h
X

N

n=2
X

A Simple Multi-class Boosting Framework

Since the trace of a matrix is equal to the sum of its eigen-
values and U
U has at most K non-zero eigenvalues (λ1
being the largest), hence:

⊤

λ1

≥

1
K

⊤

tr(U

U)

0
L
KN

≥

since:

wn, 1

=

i

0
L

1
N

N

n=1h
X

Based on this formulation, binarization is achieved by set-
ting the binarized class bn of each sample n as the sign of
ˆv1, δδδni
its corresponding element in ˆv1: bn ≡
)
sign(
h
Accordingly, if b is the vector with elements bn, then:

b,ˆv1
h

sign[ˆv1],ˆv1
h

2 =
2 =
i
≥
i
(please refer to supplement for proof)

2
i

, 1

ˆv1

h|

|

1

(10)

Finally, by combining Eq. 6, Eq. 9, and Eq. 10, with perfect
binarized classiﬁcation (i.e. when the binary weak learner
perfectly classiﬁes the binarized data), the loss ratio at any
iteration is bounded by:

Lf ∗
L

1

0 ≤

−

1
2KN

In general, there is no guarantee that any weak learner can
achieve perfect binarized classiﬁcation.
In the following
section, we show that with the ability to isolate any single
point in space (i.e. to classify an inner point as +1 and all
outer points as

1), the loss decreases exponentially.

−

5. Isolating Points

Assume that we have a weak learner fi that can isolate a
single point xi in the input space X. Accordingly, denote
1s with a +1 in the ith entry,
fi = 2 δδδi −
−
corresponding to classiﬁcation using the isolating learner
4, then for any unit vector ˆv
fi(xn). If N

1 as a vector of

RN :

≥

∈

max

i {h

fi,ˆv

2
i

} ≥

4
N

(please refer to supplement for proof)

Combining Eq. 6, Eq. 9, and Eq. 11, the loss ratio at each
iteration is upper-bounded as follows:

mini{Lfi}
0
L

1

≤

−

2
KN 2

Before the ﬁrst iteration, the initial loss
0 = K/2. Each
iteration decreases the loss exponentially. Since the train-
ing error is discrete and is upper bounded by the loss (as

L

in Eq. 2), our framework attains minimal training error on
any1 training set after a ﬁnite number of iterations:

deﬁne: T0

(9)

ln(2/KN )
ln

2
KN 2

1

−

≡ &

(cid:24)

' ≈
(cid:1)
2
KN 2

T

(cid:17)

(cid:0)
K
2

1

−

(cid:16)

KN 2
2

ln

KN
2

(cid:16)

(cid:17)(cid:25)

<

1
N ⇒

ε = 0

∴ T

T0

≥

⇒

Although this bound is too weak to be of practical use, it
is a bound nonetheless (and can likely be improved). In
the following section, we specify a suitable family of weak
learners with the ability to isolate single points.

5.1. One/Two-Point Localized Similarities

Classical decision stumps compare a single feature to a
threshold, outputting +1 or
1. Instead, our proposed fam-
−
ily of weak learners (called localized similarities) compare
points in the input space using a similarity measure. Due
to its simplicity and effectiveness, we use negative squared
2 as a measure of similarity
xjk
Euclidean distance
between points xi and xj . A localized similarity has two
modes of operation:

xi−

−k

1. In one-point mode, given an anchor xi and a threshold
τ , the input space is classiﬁed as positive if it is more
similar to xi than τ , and negative otherwise; ranging
between +1 and

1:

−

fi(x)

≡

τ
τ +

− k
k

xi −
xi −

x
k
x
k

2

2

2. In two-point mode, given supports xi and xj , the input
space is classiﬁed as positive if it is more similar to
xi than to xj (and vice-versa), with maximal absolute
activations around xi and xj; falling off radially away
from the midpoint m:

fij(x)

d, x
h
4 +
k

m
i
−
m
x
−
k

4

k

≡

4

d

k

(11)

where: d

1
2

≡

[xi −

xj]

and: m

[xi +xj]

1
2

≡

One-point mode enables the isolation of any single data-
point, guaranteeing a baseline reduction in loss. However,
it essentially leads to pure memorization of the training
data; mimicking a nearest-neighbor classiﬁer. Two-point
mode adds the capability to generalize better by provid-
ing margin-style functionality. The combination of these

1 There may be situations in which multiple samples belong-
ing to different classes are coincident in the input space. These
cases can be dealt with (before or during training) either by as-
signing all such points as a special “mixed” class (to be dealt with
at a later stage), or by setting the class labels of all coincident
points to the single label that minimizes the error.

A Simple Multi-class Boosting Framework

two modes enables the ﬂexibility to tackle a wide range of
classiﬁcation problems. Furthermore, in either mode, the
functionality of a localized similarity is easily interpretable:
“which of these ﬁxed training points is a given query point
more similar to?”

5.2. Finding Adequate Localized Similarities

Given a dataset with N samples, there are about N 2 pos-
sible localized similarities. The following procedure efﬁ-
ciently selects an adequate localized similarity:

0. Using Eq. 5, calculate the base loss

1 for the homoge-
neous stump f1 (i.e. the one-point stump with any xi
, classifying all points as +1).
and τ

L

1. Compute the eigenvector ˆv1 (as in Eq. 8); label the

≡ ∞

points based on their binarized class labels bn.

2. Find the optimal isolating localized similarity fi (i.e.
with xi and appropriate τ , classifying point i as +1 and
all other points as

1).
3. Using Eq. 5, calculate the corresponding loss

Li. Of the
two stumps f1 and fi, store the one with smaller loss as
best-so-far.

−

4. Find point xj most similar2 to xi among points of the

opposite binarized class:

xj = arg min

bj =−bi{k

xi −

xjk

2

}

5. Calculate the loss achieved using the two-point local-
ized similarity fij. If it outperforms the previous best,
store the newer learner and update the best-so-far loss.
6. Find all points that are similar enough to xj and remove
them from consideration for the remainder of the cur-
rent iteration. In our implementation, we remove all xn
for which:

fij(xn)

fij(xj )/2

≤
If all points have been removed, return the best-so-far
localized similarity; otherwise, loop back to step 4.

Upon completion of this procedure, the best-so-far local-
ized similarity is guaranteed to lead to an adequate reduc-
tion in loss, based on the derivation in Sec. 4 above.

6. Generalization Experiments

Our boosting method provably reduces the loss well after
the training error is minimized. In this section, we demon-
strate that the continual reduction in loss serves only to im-
prove the decision boundaries and not to overﬁt the data.

We generated 2-dimensional synthetic datasets in order to
better visualize and get an intuition for what the classiﬁers

2 “most similar” need not be exact; approximate nearest-

neighbors also works with negligible differences.

Figure2. A 500-point 2-dimensional synthetic dataset with a
(2/3, 1/3) split of train data (left plot) to test data (right plot).
Background shading corresponds to the hypothesized class using
our framework.

1

0.8

0.6

0.4

0.2

 

s
s
o
L
g
n
n
a
r
T

i

i

 
,
r
o
r
r

 

E
e
v
i
t

l

a
e
R

0
100

log

 Loss [-20.6]

10

Train Error [0.0%]
Test Error  [0.0%]

 
s
s
o
L
 
g
n
n
a
r
T

i

i

10-10

10-20

10-30

40

0.6

]
]
]
]
]

%
%
%
%
%

[
[
[
[
[
 
 
 
 
 
r
r
r
r
r
o
o
o
o
o
r
r
r
r
r
r
r
r
r
r

E
E
E
E
E

 
 
 
 
 
t
t
t
t
t
s
s
s
s
s
e
e
e
e
e
T
T
T
T
T

0.4

0.2

0

102

101
102
Iteration [max = 1000]

103

Figure3. A plot of training loss, training error, and test error as
a classiﬁer is trained for 1000 iterations. Note that the test error
does not increase even after the training error drops to zero. The
lower inset is a zoomed-in plot of the train and test error, the upper
inset is a plot of training loss using a log-scaled y-axis; both inset
plots are congruous with the original x-axis.

are doing. The results shown in this chapter are based on
a dataset composed of 500 points belonging to one of three
classes in a spiral formation, with a (2/3, 1/3) train/test
split. Fig. 2 shows the hypothesized class using a classiﬁer
trained for 1000 iterations.

training (left) and test
Our classiﬁer achieves perfect
classiﬁcation (right), producing a visually simple well-
generalizing contour around the points. Training curves
are given in Fig. 3, tracking the loss and classiﬁcation er-
rors per training iteration. Note that the test error does not
increase even after the training error drops to zero.

The following experiments explore the functionality of our
framework (i.e. REBEL using localized similarities) in two
scenarios that commonly arise in practice: (1) varying spar-
sity of training data, and, (2) varying amounts of mislabeled
training data.

A Simple Multi-class Boosting Framework

(a4) Train Data

(b4) Test Data

(c4) Training on 4/5 of the data (267 points)

(a1) Train Data

(b1) Test Data

(c1) ∼1% mislabeled data (4 points)

102

103

102

103

 
s
s
o
L
 
g
n
n
a
r
T

i

i

10-50

40
4

10-100

]
]
]
]
]

%
%
%
%
%

[
[
[
[
[
 
 
 
 
 
r
r
r
r
r
o
o
o
o
o
r
r
r
r
r
r
r
r
r
r

E
E
E
E
E

 
 
 
 
 
t
t
t
t
t
s
s
s
s
s
e
e
e
e
e
T
T
T
T
T

3

2

1

0

log

 Loss [-96.8]

10

Train Error [0.0%]
Test Error  [0.0%]

101

102

103

Iteration [max = 5000]

log

 Loss [-15.0]

10

Train Error [0.0%]
Test Error  [3.6%]

log

 Loss [-22.7]

10

Train Error [0.0%]
Test Error  [5.4%]

101

102

Iteration [max = 2000]

103

 
s
s
o
L
 
g
n
n
a
r
T

i

i

10-10

10-20

40

15

]
]
]
]
]

%
%
%
%
%

[
[
[
[
[
 
 
 
 
 
r
r
r
r
r
o
o
o
o
o
r
r
r
r
r
r
r
r
r
r

E
E
E
E
E

 
 
 
 
 
t
t
t
t
t
s
s
s
s
s
e
e
e
e
e
T
T
T
T
T

10

5

0

 
s
s
o
L
 
g
n
n
a
r
T

i

i

10-10

10-20

10-30

]
]
]
]
]

%
%
%
%
%

[
[
[
[
[
 
 
 
 
 
r
r
r
r
r
o
o
o
o
o
r
r
r
r
r
r
r
r
r
r

E
E
E
E
E

 
 
 
 
 
t
t
t
t
t
s
s
s
s
s
e
e
e
e
e
T
T
T
T
T

6

4

2

0

 
s
s
o
L
 
g
n
n
a
r
T

i

i

10-50

10-100

200

]
]
]
]
]

%
%
%
%
%

[
[
[
[
[
 
 
 
 
 
r
r
r
r
r
o
o
o
o
o
r
r
r
r
r
r
r
r
r
r

E
E
E
E
E

 
 
 
 
 
t
t
t
t
t
s
s
s
s
s
e
e
e
e
e
T
T
T
T
T

40

30

20

10

0

1

0.8

0.6

0.4

0.2

s
s
o
L

 

i

g
n
n
a
r
T

i

 
,
r
o
r
r

 

E
e
v
i
t

l

a
e
R

0
100

1

0.8

0.6

0.4

0.2

s
s
o
L

 

i

g
n
n
a
r
T

i

 
,
r
o
r
r

 

E
e
v
i
t

l

a
e
R

0
100

1

0.8

0.6

0.4

0.2

s
s
o
L

 

i

g
n
n
a
r
T

i

 
,
r
o
r
r

 

E
e
v
i
t

l

a
e
R

0
100

1

0.8

0.6

0.4

0.2

s
s
o
L
 
g
n
n
a
r
T

i

i

 
,
r
o
r
r

E
 
e
v
i
t
a
e
R

l

0
100

log

 Loss [-44.2]

10

Train Error [0.0%]
Test Error  [1.2%]

101

102

Iteration [max = 2000]

103

log

 Loss [-22.5]

10

Train Error [0.0%]
Test Error  [1.8%]

 
s
s
o
L
 
g
n
n
a
r
T

i

i

10-50

10-100

10-150

30

10

]
]
]
]
]

%
%
%
%
%

[
[
[
[
[
 
 
 
 
 
r
r
r
r
r
o
o
o
o
o
r
r
r
r
r
r
r
r
r
r

E
E
E
E
E

 
 
 
 
 
t
t
t
t
t
s
s
s
s
s
e
e
e
e
e
T
T
T
T
T

5

0

log

 Loss [-116.5]

10

Train Error [  0.0%]
Test Error  [  2.0%]

101

102

103

Iteration [max = 5000]

 
s
s
o
L
 
g
n
n
a
r
10-50T

i

i

50
3
%
%
%
%
%

]
]
]
]

]

[
[
[
[
[
 
 
 
 
 
r
r
r
r
r
2
o
o
o
o
o
r
r
r
r
r
r
r
r
r
r

E
E
E
E
E
1
 
 
 
 
 
t
t
t
t
t
s
s
s
s
s
e
e
e
e
e
T
T
T
T
T
0

 
s
s
o
L
 
g
n
n
a
r
T

i

i

40
4

10-10

10-20

10-30

]
]
]
]
]

%
%
%
%
%

[
[
[
[
[
 
 
 
 
 
r
r
r
r
r
o
o
o
o
o
r
r
r
r
r
r
r
r
r
r

E
E
E
E
E

 
 
 
 
 
t
t
t
t
t
s
s
s
s
s
e
e
e
e
e
T
T
T
T
T

3

2

1

0

 
s
s
o
L
 
g
n
n
a
r
T

i

i

10-20

10-40

10-60

50

]

%

]
]
]
]

20
%
%
%
%
[
[
[
[
15
 
 
 
 
r
r
r
r
o
o
o
o
r
r
r
r
10
r
r
r
r
E
E
E
E

[
 
r
o
r
r

E

 
t
s
e
T

 
 
 
 
t
t
t
t
5
s
s
s
s
e
e
e
e
T
T
T
T
0

1

0.8

0.6

0.4

0.2

s
s
o
L

 

i

g
n
n
a
r
T

i

 
,
r
o
r
r

 

E
e
v
i
t

l

a
e
R

0
100

1

0.8

0.6

0.4

0.2

s
s
o
L

 

i

g
n
n
a
r
T

i

 
,
r
o
r
r

 

E
e
v
i
t

l

a
e
R

0
100

1

0.8

0.6

0.4

0.2

s
s
o
L

 

i

g
n
n
a
r
T

i

 
,
r
o
r
r

 

E
e
v
i
t

l

a
e
R

0
100

1

0.8

0.6

0.4

0.2

s
s
o
L
 
g
n
n
a
r
T

i

i

 
,
r
o
r
r

E
 
e
v
i
t
a
e
R

l

0
100

(a3) Train Data

(b3) Test Data

(c3) Training on 3/5 of the data (200 points)

(a2) Train Data

(b2) Test Data

(c2) ∼3% mislabeled data (10 points)

102

102

(a2) Train Data

(b2) Test Data

(c2) Training on 2/5 of the data (133 points)

(a3) Train Data

(b3) Test Data

(c3) ∼10% mislabeled data (32 points)

101
102
Iteration [max = 1000]

103

101
102
Iteration [max = 1000]

103

102

103

102

70
8

103

(a1) Train Data

(b1) Test Data

(c1) Training on 1/5 of the data (67 points)

(a4) Train Data

(b4) Test Data

(c4) ∼30% mislabeled data (97 points)

102

103

103

log

 Loss [  -58.9]

10

Train Error [  0.0%]
Test Error  [14.0%]

101

102

Iteration [max = 2000]

103

log

 Loss [  -67.6]

10

Train Error [  0.0%]
Test Error  [31.1%]

101

102
Iteration [max = 10000]

103

104

Figure4. Classiﬁcation boundaries (a,b), and training curves (c)
when a classiﬁer is trained on varying amounts of data. Stars are
correctly-classiﬁed, circles are misclassiﬁed. In all cases, the test
error is fairly stable once reaching its minimum.

Figure5. Classiﬁcation boundaries (a,b), and training curves (c)
when a classiﬁer is trained on varying fractions of mislabeled data.
In all cases, the test error is fairly stable once reaching its mini-
mum. Even with 30% mislabeled data, the classiﬁcation bound-
aries are reasonable given the training labels.

6.1. Sparse Training Data

In this section of experiments, classiﬁers were trained using
varying amounts of data, from 4/5 to 1/5 of the total train-
ing set. Fig. 4 shows the classiﬁcation boundaries learned
by the classiﬁer (ai,bi), and the training curves (ci). In all
cases, the boundaries seem to aptly ﬁt (and not overﬁt) the
training data (i.e. being satisﬁed with isolated patches with-
out overzealously trying to connect points of the same class
together). This is more rigorously observed from the train-
ing curves; the test error does not increase after reaching its
minimum (for hundreds of iterations).

6.2. Mislabeled Training Data

In this section of experiments, classiﬁers were trained with
varying fractions of mislabeled data; from 1% to 30% of
the training set. Fig. 5 shows the classiﬁcation boundaries
(ai,bi) and the training curves (ci). All classiﬁers seem to
degenerate gracefully, isolating rogue points and otherwise
maintaining smooth boundaries. Even the classiﬁer trained
on 30% mislabeled data (which we would consider to be
unreasonably noisy) is able to maintain smooth boundaries.

In all cases, the training curves still show that the test error
is fairly stable once reaching its minimum value. Moreover,
test errors approximately equal the fraction of mislabeled
data, further validating the generalization of our method.

6.3. Real Data

that

Although the above observations are promising, they could
result from the fact
the synthetic datasets are 2-
dimensional.
In order to rule out this possibility, we
performed similar experiments on several UCI datasets
(Bache & Lichman, 2013) of varying input dimensionali-
ties (from 9 to 617). From the training curves in Fig. 6,
we observe that once the test errors saturate, they no longer
increase, even after hundreds of iterations.

In Fig. 7, we plot the training losses on a log-scaled y-axis.
The linear trend signiﬁes an exponential decrease in loss
per iteration. Our proven bound predicts a much slower (ex-
ponential) rate than the actual trend observed during train-
ing. Note that within the initial
10% of the iterations, the
loss drops at an even faster rate, after which it settles down

∼

A Simple Multi-class Boosting Framework

GLASS (9-dimensional)

PENDIGIT (16-dimensional)

GLASS
(d = 9, K = 6, N = 53)

PENDIGIT
(d = 16, K = 10, N = 7494)

102

70
8

103

Proven bound
Loss
Exponential Fit

Proven bound
Loss
Exponential Fit

log

 Loss [-222.7]

10

Train Error [  0.0%]
Test Error  [27.0%]

log

 Loss [-10.2]

10

Train Error [0.0%]
Test Error  [1.8%]

101

102
Iteration [max = 10000]

103

104

0
100

101

102

Iteration [max = 2000]

103

OPTDIGIT (64-dimensional)

ISOLET (617-dimensional)

1

0.8

0.6

0.4

0.2

s
s
o
L

 

i

g
n
n
a
r
T

i

 
,
r
o
r
r

 

E
e
v
i
t

l

a
e
R

0
100

1

0.8

0.6

0.4

0.2

s
s
o
L

 

i

g
n
n
a
r
T

i

 
,
r
o
r
r

 

E
e
v
i
t

l

a
e
R

0
100

 
s
s
o
L
 
g
n
n
a
r
T

i

i

10-100

10-200

10-300

200

30

]
]
]
]
]

%
%
%
%
%

[
[
[
[
[
 
 
 
 
 
r
r
r
r
r
o
o
o
o
o
r
r
r
r
r
r
r
r
r
r

E
E
E
E
E

 
 
 
 
 
t
t
t
t
t
s
s
s
s
s
e
e
e
e
e
T
T
T
T
T

25

~~~

0

 
s
s
o
L
 
g
n
n
a
r
T

i

i

10-20

10-40

100
6

]
]
]
]

]

%
%
%
%

%

[
[
[
[
 
 
 
 
r
r
r
r
o
o
o
o
r
r
r
r
r
r
r
r

[
 
r
o
r
r

E
E
E
E

E

 
 
 
 
t
t
t
t
s
s
s
s
e
e
e
e
T
T
T
T

 
t
s
e
T

4

2

0

103

103

1

0.8

0.6

0.4

0.2

s
s
o
L

 

i

g
n
n
a
r
T

i

 
,
r
o
r
r

 

E
e
v
i
t

l

a
e
R

1

0.8

0.6

0.4

0.2

s
s
o
L

 

i

g
n
n
a
r
T

i

 
,
r
o
r
r

 

E
e
v
i
t

l

a
e
R

 
s
s
o
L
 
g
n
n
a
r
T

i

i

10-5

10-10

10-15

]
]
]
]
]

%
%
%
%
%

[
[
[
[
[
 
 
 
 
 
r
r
r
r
r
o
o
o
o
o
r
r
r
r
r
r
r
r
r
r

E
E
E
E
E

 
 
 
 
 
t
t
t
t
t
s
s
s
s
s
e
e
e
e
e
T
T
T
T
T

6

4

2

0

 
s
s
o
L
 
g
n
n
a
r
T

i

i

10-5

10-10

10-15

200
8

]
]
]
]

]

%
%
%
%
%

[
[
[
[
 
 
 
 
r
r
r
r
o
o
o
o
r
r
r
r
r
r
r
r

[
 
r
o
r
r

E
E
E
E

E

 
 
 
 
t
t
t
t
s
s
s
s
e
e
e
e
T
T
T
T

 
t
s
e
T

6

4

2

0

103

4000

5000
Iteration Number

6000

0

1000

2000

3000

7000

8000

9000 10000

0

200

400

600

1400

1600

1800

2000

800

1000
Iteration Number

1200

0

200

400

600

800

1000

50

100

150

200

log

 Loss [-34.7]

10

Train Error [0.0%]
Test Error  [2.3%]

log

 Loss [-10.6]

10

Train Error [0.0%]
Test Error  [3.5%]

101

102
Iteration [max = 10000]

103

104

0
100

101

102
Iteration [max = 10000]

103

104

OPTDIGIT
(d = 64, K = 10, N = 3823)

ISOLET
(d = 617, K = 26, N = 6238)

Proven bound
Loss
Exponential Fit

Proven bound
Loss
Exponential Fit

Figure6. Training curves for classiﬁers trained on UCI datasets
with a range of dimensionalities.
In all cases, the test error is
stable once it reaches its minimum.

105

Initial Iterations

Initial Iterations

Initial Iterations

Initial Iterations

1050

100

10-50

 

s
s
o
L
g
n
n
a
r
T

i

i

10-100

1020

10-150

100

10-20

10-200

10-40

10-250

105

100

10-5

s
s
o
L
 
g
n
n
a
r
T

i

i

10-10

10-15

10-20

10-25

100

10-30

10-35

10-5

0

102

100

10-2

10-4

s
s
o
L
 
g
n
n
a
r
T

i

i

101

10-6

100

10-1

10-8

10-2

0

10-10

s
s
o
L
 
g
n
n
a
r
T

i

i

102

100

10-2

10-4

10-6

10-8

102

100

10-2

0

10-10

to a seemingly-constant rate of exponential decay. We have
not yet determined the characteristics (i.e. the theoretically
justiﬁed rates) of these observed trends, and relegate this
endeavor to future work.

7. Comparison with Other Methods

In Sec. 5 we proved that our framework adheres to the-
oretical guarantees, and in Sec. 6 above, we showed
that it has promising empirical properties.
In this sec-
tion, we compete against several state-of-the-art boost-
ing baselines. Speciﬁcally, we compared 1-vs-All Ad-
aBoost and AdaBoost.MH (Schapire & Singer, 1999),
AdaBoost.ECC (Dietterich & Bakiri, 1995), Struct-Boost
(Shen et al., 2014), CW-Boost (Shen & Hao, 2011), AOSO-
LogitBoost (Sun et al., 2011), REBEL (Appel et al., 2016)
using shallow decision trees, REBEL using only 1-point
(isolating) similarities, and our full framework, REBEL us-
ing 2-point localized similarities.

r
o
r
r
E

 
t
n
e
c
r
e
P

40

30

20

10

0

Based on the same experimental setup as in (Shen et al.,
2014; Appel et al., 2016), competing methods are trained
to a maximum of 200 decision stumps. For each dataset,
ﬁve random splits are generated, with 50% of the samples
for training, 25% for validation (i.e. for setting hyperparam-
eters where needed), and the remaining 25% for testing.

REBEL using localized similarities is the most accurate
method on ﬁve of the six datasets tested.
In the Vowel
dataset, it achieves almost half of the error as the next best
method. Note that although our framework uses REBEL as
its boosting method, the localized similarities add an extra
edge, beating REBEL with decision trees in all runs.

Further, when limited to only using 1-point (i.e. isolating)
localized similarities, the performance is extremely poor,
validating the need for 2-point localized similarities as pre-
scribed in Sec. 5.2. Overall, these results demonstrate the

200

400

600

800

1000

200

400

600

800

1000

0

1000

2000

3000

7000

8000

9000 10000

0

1000

2000

3000

7000

8000

9000 10000

4000

5000
Iteration Number

6000

4000

5000
Iteration Number

6000

Figure7. Training losses for classiﬁers trained on UCI datasets.
The linear trend (visualized using a log-scaled y-axis) signiﬁes
an exponential decrease in loss, albeit at a much faster rate than
established by our proven bound.

[0 0 1] Ada 1vsAll
[0 0 0] Ada.MH
[0 1 0] Ada.ECC
[0 2 0] Struct-Boost
[2 1 1] CW-Boost
[0 0 0] A0S0-Logit
[0 3 2] RBL-Stump
[0 0 0] RBL-Iso.Sim
[5 0 0] RBL-Loc.Sim

7
.
1
3

3
.
2
3

7
.
2
3

8
.
5
3

4
.
5
3

2
.
4
3

4
.
0
3

9
.
5
3

*
4
.
7
2

1
.
1
2

8
.
8
1

6
.
0
2

5
.
7
1

4
.
2
2

6
.
0
2

4
.
7
1

3
.
2
6

*
5
.
9

1
.
5
1

7
.
2
1

8
.
2
1

1
.
2
1

1
.
1
1

4
.
5
1

7
.
0
1

3
.
2
5

*
6
.
0
1

0
.
1
1

4
.
3
1

8
.
5
1

5
.
2
1

*
3
.
9

5
.
2
1

5
.
0
1

1
.
7
8

*
3
.
9

1
.
7

4
.
7

4
.
8

9
.
6

GLASS

VOWEL

LANDSAT

MNIST

PENDIGITS

SEGMENT

2
.
3

5
.
2

8
.
2
1

*
2
.
1

8
.
2
3

7
.
3

9
.
2

9
.
2

*
5
.
2

7
.
7

6
.
4

3
.
3

6
.
5

7
.
9
6

Figure8. Test errors of various state-of-the-art and baseline
classiﬁcation methods on MNIST and several UCI datasets.
REBEL using localized similarities (shown in yellow) is the best-
performing method on all but one of the datasets shown. When
constrained to use only 1-point (isolating) similarities (shown in
red), the resulting classiﬁer is completely inadequate.

ability of our framework to produce easily interpretable
classiﬁers that are also empirically proﬁcient.

7.1. Comparison with Neural Networks and SVMs

Complex neural networks are able to achieve remarkable
performance on large datasets, but they require an amount
of training data proportional to their complexity.
In the
regime of small to medium amounts of data (within which
UCI and MNIST datasets belong, i.e. 10 < N < 106 train-
ing samples), such networks cannot be too complex. Ac-
cordingly, in Fig. 9, we compare our method against fully-
connected neural networks.

A Simple Multi-class Boosting Framework

]

%

[
 
r
o
r
r

E

 
t
s
e
T
 
e
g
a
r
e
v
A

100

30

10

3

1

0.3

0.1

0.03

G
G
G

G
S
V
P
L
A
O
I
M
C

53

Method
[2/10] NN 
[2/10] SVM 
[8/10] Ours
Dataset
GLASS
SHUTTLE
VOWEL
PENDIGIT
LETTER
LANDSAT
OPTDIGIT
ISOLET
MNIST
CUB200

VV
V

C

C
C

A
AA

O
O
O

I

I
I

P

PP

L
L

L

M

M
M

S

S

S

528

7494
6238
5594
Number of Training Samples (N)

4435
3823

16000

43500

60000

Figure9. Comparison of our method versus Neural Networks and
Support Vector Machines on ten datasets of varying sizes and dif-
ﬁculties. Our method is the most accurate on almost all datasets.

d

−

−

−

−

−

−

−

−

−

2d

4d

4K

4K

K], [d

Four neural networks were implemented, each having one
of the following architectures: [d
K],
−
K], where d is the number
K], [d
[d
2K
of input dimensions and K is the number of output classes.
Only the one with the best test error is shown in the plot. A
multi-class SVM (Chang & Lin, 2011) was validated using
a 5
6 parameter sweep for C and γ. Our method was run
until the training loss fell below 1/N . Overall, REBEL us-
ing localized similarities achieves the best results on eight
of the ten datasets, decisively marking it as the method of
choice for this range of data.

×

8. Discussion

In Sec. 6, we observed that our classiﬁers tend to smoothen
the decision boundaries in the iterations beyond zero train-
ing error. In Fig. 10, we see that this is not the case with
the typically-used axis-aligned decision stumps. Why does
this happen with our framework?

Figure10. The contrasted difference between overtraining using
(a)
(a) classical decision stumps and (b) localized similarities.
leads to massive overﬁtting of the training data, whereas (b) leads
to smoothening of the decision boundaries.

Firstly, we note that the largest-margin boundary between
two points is the hyperplane that bisects them. Every two-
point localized similarity acts as such a bisector. There-
fore, it is not surprising that with only a pool of localized
similarities, a classiﬁer should have what it needs to place
good boundaries. Further, not all pairs need to be separated
(since many neighboring points belong to the same class);
N 2 possible learners
hence, only a small subset of the
will ever need to be selected.

∼

Secondly, we note that if some point (either an outlier or an
unfortunately-placed point) continues to increase in weight
until it can no-longer be ignored, it can simply be isolated
and individually dealt with using a one-point localized sim-
ilarity, there is no need to combine it with other “innocent-
bystander” points. This phenomenon is observed in the mis-
labeled training experiments in Sec. 6.2.

Together, the two types of localized similarities comple-
ment each other. With the guarantee that every step reduces
the loss, each iteration focuses on either further smoothen-
ing out an existing boundary, or reducing the weight of a
single unﬁt point.

9. Conclusions

We have presented a novel framework for multi-class boost-
ing that makes use of a simple family of weak learners
called localized similarities. Each of these learners has a
clearly understandable functionality; a test of similarity be-
tween a query point and some pre-deﬁned samples.

We have proven that the framework adheres to theoretical
guarantees: the training loss is minimized at an exponen-
tial rate, and since the loss upper-bounds the training error
(which can only assume discrete values), our framework is
therefore able to achieve maximal accuracy on any dataset.

We further explored some of the empirical properties of
our framework, noting that the combination of localized
similarities and guaranteed loss reduction tend to lead to a
non-overﬁtting regime, in which the classiﬁer focuses on
smoothing-out its decision boundaries. Finally, we com-
pare our method against several state-of-the-art methods,
outperforming all of the methods in most of the datasets.

Altogether, we believe that we have achieved our goal of
presenting a simple multi-class boosting framework with
theoretical guarantees and empirical proﬁciency.

Acknowledgements

The authors would like to thank anonymous reviewers for
their feedback and Google Inc. and the Ofﬁce of Naval
Research MURI N00014-10-1-0933 for funding this work.

A Simple Multi-class Boosting Framework

References

Allwein, E. L., Schapire, R. E., and Singer, Y. Reducing
multiclass to binary: a unifying approach for margin clas-
siﬁers. JMLR, 2001.

Appel, R., Burgos-Artizzu, X. P., and Perona, P. Improved
multi-class cost-sensitive boosting via estimation of the
minimum-risk class. arXiv, (1607.03547), 2016.
and Lichman, M.

Bache, K.

UCI machine
URL

2013.
learning repository (uc
http://archive.ics.uci.edu/ml.

irvine),

Chang, C. and Lin, C. LIBSVM: A library for support vec-
tor machines. Transactions on Intelligent Systems and
Technology, 2011.

Dietterich, T. G. and Bakiri, G. Solving multiclass learn-
ing problems via error-correcting output codes. arXiv,
(9501101), 1995.

Freund, Y. Boosting a weak learning algorithm by majority.

Information and Computation, 1995.

Freund, Y. and Schapire, R. E. Experiments with a new
boosting algorithm. In Machine Learning International
Workshop, 1996.

LeCun, Y., Bengio, Y., and Hinton, G. E. Deep learning.

Nature Research, 2015.

Li, L. Multiclass boosting with repartitioning. In ICML,

2006.

Mukherjee, I. and Schapire, R. E. A theory of multiclass

boosting. In NIPS, 2010.

Saberian, M. and Vasconcelos, N. Multiclass boosting:

Theory and algorithms. In NIPS, 2011.

Schapire, R. E. The strength of weak learnability. Machine

Learning, 1990.

Schapire, R. E. and Singer, Y.

Improved boosting algo-
rithms using conﬁdence-rated predictions. In Conference
on Computational Learning Theory, 1999.

Shen, C. and Hao, Z. A direct formulation for totally-

corrective multi-class boosting. In CVPR, 2011.

Shen, G., Lin, G., and van den Hengel, A. Structboost:
Boosting methods for predicting structured output vari-
ables. PAMI, 2014.

Sun, P., Reid, M. D., and Zhou, J. Aoso-logitboost:
Adaptive one-vs-one logitboost for multi-class problem.
arXiv, (1110.3907), 2011.

Sun, Y., Todorovic, S., Li, J., and Wu, D. Unifying
the error-correcting and output-code adaboost within the
margin framework. In ICML, 2005.

Yu, F., Zhang, Y., Song, S., Seff, A., and Xiao, J. LSUN:
construction of a large-scale image dataset using deep
learning with humans in the loop. arXiv, (1506.03365),
2015.

Zhu, J., Zou, H., Rosset, S., and Hastie, T. Multi-class

adaboost. Statistics and its Interface, 2009.

