Conditional Image Synthesis with Auxiliary Classiﬁer GANs

Augustus Odena 1 Christopher Olah 1 Jonathon Shlens 1

Abstract

In this paper we introduce new methods for the
improved training of generative adversarial net-
works (GANs) for image synthesis. We con-
struct a variant of GANs employing label condi-
tioning that results in 128 × 128 resolution im-
age samples exhibiting global coherence. We
expand on previous work for image quality as-
sessment to provide two new analyses for assess-
ing the discriminability and diversity of samples
from class-conditional image synthesis models.
These analyses demonstrate that high resolution
samples provide class information not present in
low resolution samples. Across 1000 ImageNet
classes, 128 × 128 samples are more than twice
as discriminable as artiﬁcially resized 32 × 32
samples. In addition, 84.7% of the classes have
samples exhibiting diversity comparable to real
ImageNet data.

1. Introduction

Characterizing the structure of natural images has been a
rich research endeavor. Natural images obey intrinsic in-
variances and exhibit multi-scale statistical structures that
have historically been difﬁcult to quantify (Simoncelli &
Olshausen, 2001). Recent advances in machine learning
offer an opportunity to substantially improve the quality of
image models. Improved image models advance the state-
of-the-art in image denoising (Ball´e et al., 2015), compres-
sion (Toderici et al., 2016), in-painting (van den Oord et al.,
2016a), and super-resolution (Ledig et al., 2016). Bet-
ter models of natural images also improve performance in
semi-supervised learning tasks (Kingma et al., 2014; Sprin-
genberg, 2015; Odena, 2016; Salimans et al., 2016) and re-
inforcement learning problems (Blundell et al., 2016).

One method for understanding natural image statistics is to
build a system that synthesizes images de novo. There are

1Google Brain. Correspondence to: Augustus Odena <augus-

tusodena@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

several promising approaches for building image synthe-
sis models. Variational autoencoders (VAEs) maximize a
variational lower bound on the log-likelihood of the train-
ing data (Kingma & Welling, 2013; Rezende et al., 2014).
VAEs are straightforward to train but introduce potentially
restrictive assumptions about the approximate posterior
distribution (but see (Rezende & Mohamed, 2015; Kingma
et al., 2016)). Autoregressive models dispense with latent
variables and directly model the conditional distribution
over pixels (van den Oord et al., 2016a;b). These models
produce convincing samples but are costly to sample from
and do not provide a latent representation. Invertible den-
sity estimators transform latent variables directly using a
series of parameterized functions constrained to be invert-
ible (Dinh et al., 2016). This technique allows for exact
log-likelihood computation and exact inference, but the in-
vertibility constraint is restrictive.

Generative adversarial networks (GANs) offer a distinct
and promising approach that focuses on a game-theoretic
formulation for training an image synthesis model (Good-
fellow et al., 2014). Recent work has shown that GANs can
produce convincing image samples on datasets with low
variability and low resolution (Denton et al., 2015; Radford
et al., 2015). However, GANs struggle to generate glob-
ally coherent, high resolution samples - particularly from
datasets with high variability. Moreover, a theoretical un-
derstanding of GANs is an on-going research topic (Uehara
et al., 2016; Mohamed & Lakshminarayanan, 2016).

In this work we demonstrate that that adding more structure
to the GAN latent space along with a specialized cost func-
tion results in higher quality samples. We exhibit 128×128
pixel samples from all classes of the ImageNet dataset
(Russakovsky et al., 2015) with increased global coherence
(Figure 1). Importantly, we demonstrate quantitatively that
our high resolution samples are not just naive resizings of
low resolution samples. In particular, downsampling our
128 × 128 samples to 32 × 32 leads to a 50% decrease in
visual discriminability. We also introduce a new metric for
assessing the variability across image samples and employ
this metric to demonstrate that our synthesized images ex-
hibit diversity comparable to training data for a large frac-
tion (84.7%) of ImageNet classes. In more detail, this work
is the ﬁrst to:

Conditional Image Synthesis with Auxiliary Classiﬁer GANs

monarch butterfly

goldfinch

daisy

redshank

grey whale

128 resolution samples from 5 classes taken from an AC-GAN trained on the ImageNet dataset. Note that the classes
Figure 1. 128
shown have been selected to highlight the success of the model and are not representative. Samples from all ImageNet classes are linked
later in the text.

×

•

•

•

•

•

•

Demonstrate an image synthesis model for all 1000
ImageNet classes at a 128x128 spatial resolution (or
any spatial resolution - see Section 3).

Measure how much an image synthesis model actually
uses its output resolution (Section 4.1).

Measure perceptual variability and ’collapsing’ be-
havior in a GAN with a fast, easy-to-compute metric
(Section 4.2).

Highlight that a high number of classes is what makes
ImageNet synthesis difﬁcult for GANs and provide an
explicit solution (Section 4.6).

Demonstrate experimentally that GANs that perform
well perceptually are not those that memorize a small
number of examples (Section 4.3).

Achieve state of the art on the Inception score metric
when trained on CIFAR-10 without using any of the
techniques from (Salimans et al., 2016) (Section 4.4).

2. Background

A generative adversarial network (GAN) consists of two
neural networks trained in opposition to one another. The
generator G takes as input a random noise vector z and
outputs an image Xf ake = G(z). The discriminator D
receives as input either a training image or a synthesized
image from the generator and outputs a probability distri-
bution P (S
X) = D(X) over possible image sources.
The discriminator is trained to maximize the log-likelihood
it assigns to the correct source:

|

L = E[log P (S = real

Xreal)]+
E[log P (S = f ake

|

Xf ake)]

(1)

|

The generator is trained to minimize the second term in
Equation 1.

The basic GAN framework can be augmented using side
information. One strategy is to supply both the generator
and discriminator with class labels in order to produce class
conditional samples (Mirza & Osindero, 2014). Class con-
ditional synthesis can signiﬁcantly improve the quality of
generated samples (van den Oord et al., 2016b). Richer side
information such as image captions and bounding box lo-
calizations may improve sample quality further (Reed et al.,
2016a;b).

Instead of feeding side information to the discriminator,
one can task the discriminator with reconstructing side in-
formation. This is done by modifying the discriminator to
contain an auxiliary decoder network1 that outputs the class
label for the training data (Odena, 2016; Salimans et al.,
2016) or a subset of the latent variables from which the
samples are generated (Chen et al., 2016). Forcing a model
to perform additional tasks is known to improve perfor-
mance on the original task (e.g.
(Sutskever et al., 2014;
Szegedy et al., 2014; Ramsundar et al., 2016)). In addi-
tion, an auxiliary decoder could leverage pre-trained dis-
criminators (e.g.
image classiﬁers) for further improving
the synthesized images (Nguyen et al., 2016). Motivated
by these considerations, we introduce a model that com-
bines both strategies for leveraging side information. That
is, the model proposed below is class conditional, but with
an auxiliary decoder that is tasked with reconstructing class
labels.

1 Alternatively, one can force the discriminator to work with
the joint distribution (X, z) and train a separate inference network
that computes q(z
X) (Dumoulin et al., 2016; Donahue et al.,
2016).

|

Conditional Image Synthesis with Auxiliary Classiﬁer GANs

3. AC-GANs

We propose a variant of the GAN architecture which we
call an auxiliary classiﬁer GAN (or AC-GAN). In the AC-
GAN, every generated sample has a corresponding class la-
bel, c ∼ pc in addition to the noise z. G uses both to gener-
ate images Xf ake = G(c, z). The discriminator gives both
a probability distribution over sources and a probability dis-
tribution over the class labels, P (S | X), P (C | X) =
D(X). The objective function has two parts:
the log-
likelihood of the correct source, LS, and the log-likelihood
of the correct class, LC.

LS = E[log P (S = real | Xreal)]+

E[log P (S = f ake | Xf ake)]

(2)

LC = E[log P (C = c | Xreal)]+

E[log P (C = c | Xf ake)]

(3)

D is trained to maximize LS + LC while G is trained to
maximize LC − LS. AC-GANs learn a representation for
z that is independent of class label (e.g. (Kingma et al.,
2014)).

Structurally, this model is not tremendously different from
existing models. However, this modiﬁcation to the stan-
dard GAN formulation produces excellent results and ap-
pears to stabilize training. Moreover, we consider the AC-
GAN model to be only part of the technical contributions of
this work, along with our proposed methods for measuring
the extent to which a model makes use of its given output
resolution, methods for measuring perceptual variability of
samples from the model, and a thorough experimental ana-
lyis of a generative model of images that creates 128 × 128
samples from all 1000 ImageNet classes.

Early experiments demonstrated that increasing the num-
ber of classes trained on while holding the model ﬁxed de-
creased the quality of the model outputs. The structure of
the AC-GAN model permits separating large datasets into
subsets by class and training a generator and discriminator
for each subset. All ImageNet experiments are conducted
using an ensemble of 100 AC-GANs, each trained on a 10-
class split.

4. Results

We train several AC-GAN models on the ImageNet data
set (Russakovsky et al., 2015). Broadly speaking, the ar-
chitecture of the generator G is a series of ‘deconvolution’
layers that transform the noise z and class c into an image
(Odena et al., 2016). We train two variants of the model ar-
chitecture for generating images at 128 × 128 and 64 × 64

spatial resolutions. The discriminator D is a deep convo-
lutional neural network with a Leaky ReLU nonlinearity
(Maas et al., 2013). As mentioned earlier, we ﬁnd that re-
ducing the variability introduced by all 1000 classes of Im-
ageNet signiﬁcantly improves the quality of training. We
train 100 AC-GAN models – each on images from just 10
classes – for 50000 mini-batches of size 100.

Evaluating the quality of image synthesis models is chal-
lenging due to the variety of probabilistic criteria (Theis
et al., 2015) and the lack of a perceptually meaningful im-
age similarity metric. Nonetheless, in later sections we at-
tempt to measure the quality of the AC-GAN by building
several ad-hoc measures for image sample discriminabil-
ity and diversity. Our hope is that this work might provide
quantitative measures that may be used to aid training and
subsequent development of image synthesis models.

4.1. Generating High Resolution Images Improves

Discriminability

Building a class-conditional image synthesis model neces-
sitates measuring the extent to which synthesized images
appear to belong to the intended class. In particular, we
would like to know that a high resolution sample is not just
a naive resizing of a low resolution sample. Consider a
simple experiment: pretend there exists a model that syn-
thesizes 32 × 32 images. One can trivially increase the
resolution of synthesized images by performing bilinear in-
terpolation. This would yield higher resolution images, but
these images would just be blurry versions of the low res-
olution images that are not discriminable. Hence, the goal
of an image synthesis model is not simply to produce high
resolution images, but to produce high resolution images
that are more discriminable than low resolution images.

To measure discriminability, we feed synthesized images
to a pre-trained Inception network (Szegedy et al., 2015)
and report the fraction of the samples for which the In-
ception network assigned the correct label2. We calculate
this accuracy measure on a series of real and synthesized
images which have had their spatial resolution artiﬁcially
decreased by bilinear interpolation (Figure 2, top panels).
Note that as the spatial resolution is decreased, the accuracy
decreases - indicating that resulting images contain less
class information (Figure 2, scores below top panels). We
summarized this ﬁnding across all 1000 ImageNet classes
for the ImageNet training data (black), a 128 × 128 reso-

2 One could also use the Inception score (Salimans et al.,
2016), but our method has several advantages: accuracy ﬁg-
ures are easier to interpret than exponentiated KL-divergences;
accuracy may be assessed for individual classes; accuracy
measures whether a class-conditional model generated sam-
ples from the intended class. To compute the Inception ac-
curacy, we modiﬁed a version of Inception-v3 supplied in
https://github.com/openai/improved-gan/.

Conditional Image Synthesis with Auxiliary Classiﬁer GANs

16 x 16

32 x 32

 64 x 64

128 x 128

256 x 256

Real

Fake

0%

7%

 62%

94%

94%

0%

0%

 42%

76%

76%

×

Figure 2. Generating high resolution images improves discriminability. Top: Training data and synthesized images from the zebra
class resized to a lower spatial resolution (indicated above) and subsequently artiﬁcially resized to the original resolution (128
128 for
64 for the blue line). Inception accuracy is shown below the corresponding images. Bottom Left: Summary
the red and black lines; 64
128 models. Error bar
of accuracies across varying spatial resolutions for training data and image samples from 64
measures standard deviation across 10 subsets of images. Dashed lines highlight the accuracy at the output spatial resolution of the
model. The training data (clipped) achieves accuracies of 24%, 54%, 81% and 81% at resolutions of 32, 64, 128, and 256 respectively.
Bottom Right: Comparison of accuracy scores at 128
32 spatial resolutions (x and y axis, respectively). Each point
represents an ImageNet class. 84.4% of the classes are below the line of equality. The green dot corresponds to the zebra class. We
256 as a sanity check to demonstrate that simply increasing the number
also artiﬁcially resized 128
×
of pixels will not increase discriminability.

64 images to 256

64 and 128

128 and 32

128 and 64

×

×

×

×

×

×

×

64 resolution AC-GAN
lution AC-GAN (red) and a 64
(blue) in Figure 2 (bottom, left). The black curve (clipped)
provides an upper-bound on the discriminability of real im-
ages.

×

±

2.0% with samples resized to 64

The goal of this analysis is to show that synthesizing higher
resolution images leads to increased discriminability. The
2.0%
128 model achieves an accuracy of 10.1%
128
±
×
versus 7.0%
64 and
32. In other
2.0% with samples resized to 32
5.0%
words, downsizing the outputs of the AC-GAN to 32
32
and 64
64 decreases visual discriminability by 50% and
38% respectively. Furthermore, 84.4% of the ImageNet
classes have higher accuracy at 128
32
(Figure 2, bottom left).

128 than at 32

×

×

×

×

±

×

×

×

We performed the same analysis on an AC-GAN trained
64 spatial resolution. This model achieved less dis-
to 64
128 AC-GAN model. Accuracies
criminability than a 128
64 spatial reso-
from the 64
64 model plateau at a 64
lution consistent with previous results. Finally, the 64
64

×

×

×

×

resolution model achieves less discriminability at 64 spatial
resolution than the 128

128 model.

×

To the best of our knowledge, this work is the ﬁrst that at-
tempts to measure the extent to which an image synthesis
model is ‘making use of its given output resolution’, and in
fact is the ﬁrst work to consider the issue at all. We con-
sider this an important contribution, on par with propos-
ing a model that synthesizes images from all 1000 Ima-
geNet classes. We note that the proposed method can be
applied to any image synthesis model for which a measure
of ‘sample quality’ can be constructed. In fact, this method
(broadly deﬁned) can be applied to any type of synthesis
model, as long as there is an easily computable notion of
sample quality and some method for ‘reducing resolution’.
In particular, we expect that a similar procecure can be car-
ried out for audio synthesis.

Conditional Image Synthesis with Auxiliary Classiﬁer GANs

4.2. Measuring the Diversity of Generated Images

hot dog

MS-SSIM = 0.11

promontory

MS-SSIM = 0.29

green apple

MS-SSIM = 0.41

artichoke
MS-SSIM = 0.90

An image synthesis model is not very interesting if it only
outputs one image. Indeed, a well-known failure mode of
GANs is that the generator will collapse and output a single
prototype that maximally fools the discriminator (Goodfel-
low et al., 2014; Salimans et al., 2016). A class-conditional
model of images is not very interesting if it only outputs
one image per class. The Inception accuracy can not mea-
sure whether a model has collapsed. A model that simply
memorized one example from each ImageNet class would
do very well by this metric. Thus, we seek a complemen-
tary metric to explicitly evaluate the intra-class perceptual
diversity of samples generated by the AC-GAN.

Several methods exist for quantitatively evaluating image
similarity by attempting to predict human perceptual sim-
ilarity judgements. The most successful of these is multi-
scale structural similarity (MS-SSIM) (Wang et al., 2004b;
Ma et al., 2016). MS-SSIM is a multi-scale variant of
a well-characterized perceptual similarity metric that at-
tempts to discount aspects of an image that are not impor-
tant for human perception (Wang et al., 2004a). MS-SSIM
values range between 0.0 and 1.0; higher MS-SSIM values
correspond to perceptually more similar images.

As a proxy for image diversity, we measure the MS-
SSIM scores between 100 randomly chosen pairs of images
within a given class. Samples from classes that have higher
diversity result in lower mean MS-SSIM scores (Figure
3, left columns); samples from classes with lower diver-
sity have higher mean MS-SSIM scores (Figure 3, right
columns). Training images from the ImageNet training
data contain a variety of mean MS-SSIM scores across the
classes indicating the variability of image diversity in Ima-
geNet classes (Figure 4, x-axis). Note that the highest mean
MS-SSIM score (indicating the least variability) is 0.25 for
the training data.

We calculate the mean MS-SSIM score for all 1000 Ima-
geNet classes generated by the AC-GAN model. We track
this value during training to identify whether the generator
has collapsed (Figure 5, red curve). We also employ this
metric to compare the diversity of the training images to
the samples from the GAN model after training has com-
pleted. Figure 4 plots the mean MS-SSIM values for image
samples and training data broken up by class. The blue line
is the line of equality. Out of the 1000 classes, we ﬁnd that
847 have mean sample MS-SSIM scores below that of the
maximum MS-SSIM for the training data. In other words,
84.7% of classes have sample variability that exceeds that
of the least variable class from the ImageNet training data.

There are two points related to the MS-SSIM metric and
our use of it that merit extra attention. The ﬁrst point is
that we are ‘abusing’ the metric: it was originally intended

i

d
e
z
s
e
h
t
n
y
s

l

a
e
r

MS-SSIM = 0.05

MS-SSIM = 0.15

MS-SSIM = 0.08

MS-SSIM = 0.04

Figure 3. Examples of different MS-SSIM scores. The top and
bottom rows contain AC-GAN samples and training data, respec-
tively.

to be used for measuring the quality of image compres-
sion algorithms using a reference ‘original image’. We in-
stead use it on two potentially unrelated images. We be-
lieve that this is acceptable for the following reasons: First:
visual inspection seems to indicate that the metric makes
sense - pairs with higher MS-SSIM do seem more similar
than pairs with lower MS-SSIM. Second: we restrict com-
parisons to images synthesized using the same class label.
This restricts use of MS-SSIM to situations more similar to
those in which it is typically used (it is not important which
image is the reference). Third: the metric is not ‘saturated’
for our use-case. If most scores were around 0, then we
would be more concerned about the applicability of MS-
SSIM. Finally: The fact that training data achieves more
variability by this metric (as expected) is itself evidence
that the metric is working as intended.

The second point is that the MS-SSIM metric is not in-
tended as a proxy for the entropy of the generator distribu-
tion in pixel space, but as a measure of perceptual diversity
of the outputs. The entropy of the generator output distri-
bution is hard to compute and pairwise MS-SSIM scores
would not be a good proxy. Even if it were easy to com-
pute, we argue that it would still be useful to have a separate
measure of perceptual diversity. To see why, consider that
the generator entropy will be sensitive to trivial changes
in contrast as well as changes in the semantic content of
the outputs. In many applications, we don’t care about this
contribution to the entropy, and it is useful to consider mea-
sures that attempt to ignore changes to an image that we
consider ‘perceptually meaningless’, hence the use of MS-
SSIM.

4.3. Generated Images are both Diverse and

Discriminable

We have presented quantitative metrics demonstrating that
AC-GAN samples may be diverse and discriminable but
we have yet to examine how these metrics interact. Fig-
ure 6 shows the joint distribution of Inception accuracies

Conditional Image Synthesis with Auxiliary Classiﬁer GANs

Figure 4. Comparison of the mean MS-SSIM scores between
pairs of images within a given class for ImageNet training data
and samples from the GAN (blue line is equality). The horizontal
red line marks the maximum MS-SSIM value (for training data)
across all ImageNet classes. Each point is an individual class. The
mean score across the training data and the samples was 0.05 and
0.18 respectively. The mean standard deviation of scores across
the training data and the samples was 0.06 and 0.08 respectively.
Scores below the red line (84.7% of classes) arise from classes
where GAN training largely succeeded.

−

and MS-SSIM scores across all classes. Inception accuracy
and MS-SSIM are anti-correlated (r2 =
0.16). In fact,
74% of the classes with low diversity (MS-SSIM
0.25)
contain Inception accuracies
1%. Conversely, 78% of
classes with high diversity (MS-SSIM < 0.25) have In-
ception accuracies that exceed 1%.
In comparison, the
Inception-v3 model achieves 78.8% accuracy on average
across all 1000 classes (Szegedy et al., 2015). A fraction
of the classes AC-GAN samples reach this level of accu-
racy. This indicates opportunity for future image synthesis
models.

≥

≤

These results suggest that GANs that drop modes are most
likely to produce low quality images. This stands in con-
trast to a popular hypothesis about GANs, which is that
they achieve high sample quality at the expense of variabil-
ity. We hope that these ﬁndings can help structure further
investigation into the reasons for differing sample quality
between GANs and other image synthesis models.

4.4. Comparison to Previous Results

Previous quantitative results for image synthesis mod-
els trained on ImageNet are reported in terms of log-
likelihood (van den Oord et al., 2016a;b). Log-likelihood is
a coarse and potentially inaccurate measure of sample qual-
ity (Theis et al., 2015). Instead we compare with previous
state-of-the-art results on CIFAR-10 using a lower spatial
32). Following the procedure in (Salimans
resolution (32

×

Figure 5. Intra-class MS-SSIM for selected ImageNet classes
throughout a training run. Classes that successfully train (black
lines) tend to have decreasing mean MS-SSIM scores. Classes
for which the generator ‘collapses’ (red line) will have increasing
mean MS-SSIM scores.

×

et al., 2016), we compute the Inception score3 for 50000
samples from an AC-GAN with resolution (32
32), split
into 10 groups at random. We also compute the Inception
score for 25000 extra samples, split into 5 groups at ran-
dom. We select the best model based on the ﬁrst score and
report the second score. Performing a grid search across
27 hyperparameter conﬁgurations, we are able to achieve a
score of 8.25
±
0.07 (Salimans et al., 2016). Moreover, we accomplish this
without employing any of the new techniques introduced
in that work (i.e. virtual batch normalization, minibatch
discrimination, and label smoothing).

0.07 compared to state of the art 8.09

±

This provides additional evidence that AC-GANs are effec-
tive even without the beneﬁt of class splitting. See Figure 7
for a qualitative comparison of samples from an AC-GAN
and samples from the model in (Salimans et al., 2016).

4.5. Searching for Signatures of Overﬁtting

One possibility that must be investigated is that the AC-
GAN has overﬁt on the training data. As a ﬁrst check that
the network does not memorize the training data, we iden-
tify the nearest neighbors of image samples in the training
data measured by L1 distance in pixel space (Figure 8).
The nearest neighbors from the training data do not resem-
ble the corresponding samples. This provides evidence that
the AC-GAN is not merely memorizing the training data.

3

The

Inception
x)

by
is
p(y))]) where x is a particular im-
exp (Ex[DKL(p(y
age, p(y
x) is the conditional output distribution over the classes
in a pre-trained Inception network (Szegedy et al., 2014) given x,
and p(y) is the marginal distribution over the classes.

given

score

||

|

|

Conditional Image Synthesis with Auxiliary Classiﬁer GANs

−

Figure 6. Inception accuracy vs MS-SSIM for all 1000 ImageNet
classes (r2 =
0.16). Each data point represents the mean MS-
SSIM value for samples from one class. As in Figure 4, the
red line marks the maximum MS-SSIM value (for training data)
across all ImageNet classes. Samples from AC-GAN models do
not achieve variability at the expense of discriminability.

Figure 8. Nearest neighbor analysis. (Top) Samples from a single
ImageNet class.
(Bottom) Corresponding nearest neighbor (L1
distance) in training data for each sample.

the AC-GAN with z ﬁxed but altering the class label corre-
sponds to generating samples with the same ‘style’ across
multiple classes (Kingma et al., 2014). Figure 9 (Bottom)
shows samples from 8 bird classes. Elements of the same
row have the same z. Although the class changes for each
column, elements of the global structure (e.g. position, lay-
out, background) are preserved, indicating that AC-GAN
can represent certain types of ‘compositionality’.

Figure 7. Samples generated from the ImageNet dataset. (Left)
Samples generated from the model in (Salimans et al., 2016).
(Right) Randomly chosen samples generated from an AC-GAN.
AC-GAN samples possess global coherence absent from the sam-
ples of the earlier model.

A more sophisticated method for understanding the degree
of overﬁtting in a model is to explore that model’s latent
space by interpolation. In an overﬁt model one might ob-
serve discrete transitions in the interpolated images and re-
gions in latent space that do not correspond to meaning-
ful images (Bengio et al., 2012; Radford et al., 2015; Dinh
et al., 2016). Figure 9 (Top) highlights interpolations in the
latent space between several image samples. Notably, the
generator learned that certain combinations of dimensions
correspond to semantically meaningful features (e.g. size
of the arch, length of a bird’s beak) and there are no discrete
transitions or ‘holes’ in the latent space.

A second method for exploring the latent space of the AC-
GAN is to exploit the structure of the model. The AC-
GAN factorizes its representation into class information
and a class-independent latent representation z. Sampling

Figure 9. (Top) Latent space interpolations for selected ImageNet
classes. Left-most and right-columns show three pairs of image
samples - each pair from a distinct class. Intermediate columns
highlight linear interpolations in the latent space between these
three pairs of images. (Bottom) Class-independent information
contains global structure about the synthesized image. Each col-
umn is a distinct bird class while each row corresponds to a ﬁxed
latent code z.

Conditional Image Synthesis with Auxiliary Classiﬁer GANs

4.6. Measuring the Effect of Class Splits on Image

Sample Quality.

Class conditional image synthesis affords the opportunity
to divide up a dataset based on image label. In our ﬁnal
model we divide 1000 ImageNet classes across 100 AC-
GAN models. In this section we describe experiments that
highlight the beneﬁt of cutting down the diversity of classes
for training an AC-GAN. We employed an ordering of the
labels and divided it into contiguous groups of 10. This
ordering can be seen in the following section, where we
display samples from all 1000 classes. Two aspects of the
split merit discussion: the number of classes per split and
the intra-split diversity. We ﬁnd that training a ﬁxed model
on more classes harms the model’s ability to produce com-
pelling samples (Figure 10). Performance on larger splits
can be improved by giving the model more parameters.
However, using a small split is not sufﬁcient to achieve
good performance. We were unable to train a GAN (Good-
fellow et al., 2014) to converge reliably even for a split size
of 1. This raises the question of whether it is easier to train
a model on a diverse set of classes than on a similar set of
classes: We were unable to ﬁnd conclusive evidence that
the selection of classes in a split signiﬁcantly affects sam-
ple quality.

tivity to class count that is well-supported experimentally.
We can only note that, since the failure case that occurs
when the class count is increased is ‘generator collapse’, it
seems plausible that general methods for addressing ‘gen-
erator collapse’ could also address this sensitivity.

4.7. Samples from all 1000 ImageNet Classes

We also generate 10 samples from each of the 1000 Ima-
geNet classes, hosted here. As far as we are aware, no other
image synthesis work has included a similar analysis.

5. Discussion

This work introduced the AC-GAN architecture and
demonstrated that AC-GANs can generate globally coher-
ent ImageNet samples. We provided a new quantitative
metric for image discriminability as a function of spatial
resolution. Using this metric we demonstrated that our
samples are more discriminable than those from a model
that generates lower resolution images and performs a
naive resize operation. We also analyzed the diversity of
our samples with respect to the training data and provided
some evidence that the image samples from the majority of
classes are comparable in diversity to ImageNet data.

×

Several directions exist for building upon this work. Much
work needs to be done to improve the visual discriminabil-
ity of the 128
128 resolution model. Although some
synthesized image classes exhibit high Inception accura-
cies, the average Inception accuracy of the model (10.1%
±
2.0%) is still far below real training data at 81%. One im-
mediate opportunity for addressing this is to augment the
discriminator with a pre-trained model to perform addi-
tional supervised tasks (e.g.
image segmentation, (Ron-
neberger et al., 2015)).

Improving the reliability of GAN training is an ongoing
research topic. Only 84.7% of the ImageNet classes ex-
hibited diversity comparable to real training data. Training
stability was vastly aided by dividing up 1000 ImageNet
classes across 100 AC-GAN models. Building a single
model that could generate samples from all 1000 classes
would be an important step forward.

Image synthesis models provide a unique opportunity for
performing semi-supervised learning: these models build
a rich prior over natural image statistics that can be lever-
aged by classiﬁers to improve predictions on datasets for
which few labels exist. The AC-GAN model can perform
semi-supervised learning by ignoring the component of the
loss arising from class labels when a label is unavailable
for a given training image. Interestingly, prior work sug-
gests that achieving good sample quality might be inde-
pendent of success in semi-supervised learning (Salimans
et al., 2016).

Figure 10. Mean pairwise MS-SSIM values for 10 ImageNet
classes plotted against the number of ImageNet classes used dur-
ing training. We ﬁx everything except the number of classes
trained on, using values from 10 to 100. We only report the MS-
SSIM values for the ﬁrst 10 classes to keep the scores comparable.
MS-SSIM quickly goes above 0.25 (the red line) as the class count
increases. These scores were computed using 9 random restarts
per class count, using the same number of training steps for each
model. Since we have observed that generators do not recover
from the collapse phase, the use of a ﬁxed number of training
steps seems justiﬁed in this case.

We don’t have a hypothesis about what causes this sensi-

Conditional Image Synthesis with Auxiliary Classiﬁer GANs

References

Ball´e, Johannes, Laparra, Valero, and Simoncelli, Eero P.
Density modeling of images using a generalized normal-
ization transformation. CoRR, abs/1511.06281, 2015.
URL http://arxiv.org/abs/1511.06281.

Bengio, Yoshua, Mesnil, Gr´egoire, Dauphin, Yann, and
Rifai, Salah. Better mixing via deep representations.
CoRR, abs/1207.4404, 2012. URL http://arxiv.
org/abs/1207.4404.

Blundell, C., Uria, B., Pritzel, A., Li, Y., Ruderman,
A., Leibo, J. Z, Rae, J., Wierstra, D., and Hassabis,
D. Model-Free Episodic Control. ArXiv e-prints, June
2016.

Chen, X., Duan, Y., Houthooft, R., Schulman,

J.,
Sutskever, I., and Abbeel, P.
InfoGAN: Interpretable
Representation Learning by Information Maximizing
Generative Adversarial Nets. ArXiv e-prints, June 2016.

Denton, Emily L., Chintala, Soumith, Szlam, Arthur, and
Fergus, Robert. Deep generative image models using
a laplacian pyramid of adversarial networks. CoRR,
abs/1506.05751, 2015. URL http://arxiv.org/
abs/1506.05751.

Dinh, Laurent, Sohl-Dickstein,

Jascha, and Bengio,
Samy. Density estimation using real NVP. CoRR,
abs/1605.08803, 2016. URL http://arxiv.org/
abs/1605.08803.

Donahue, J., Kr¨ahenb¨uhl, P., and Darrell, T. Adversarial

Feature Learning. ArXiv e-prints, May 2016.

Dumoulin, V., Belghazi, I., Poole, B., Lamb, A., Arjovsky,
M., Mastropietro, O., and Courville, A. Adversarially
Learned Inference. ArXiv e-prints, June 2016.

Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y. Generative Adversarial Networks. ArXiv e-prints,
June 2014.

Kingma, D. P and Welling, M. Auto-Encoding Variational

Bayes. ArXiv e-prints, December 2013.

Ledig, C., Theis, L., Huszar, F., Caballero, J., Aitken,
A., Tejani, A., Totz, J., Wang, Z., and Shi, W. Photo-
Realistic Single Image Super-Resolution Using a Gen-
erative Adversarial Network. ArXiv e-prints, September
2016.

Ma, Kede, Wu, Qingbo, Wang, Zhou, Duanmu, Zhengfang,
Yong, Hongwei, Li, Hongliang, and Zhang, Lei. Group
mad competition - a new methodology to compare objec-
tive image quality models. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June
2016.

Maas, Andrew, Hannun, Awni, and Ng, Andrew. Rectiﬁer
nonlinearities improve neural network acoustic models.
In Proceedings of The 33rd International Conference on
Machine Learning, 2013.

Mirza, Mehdi and Osindero, Simon. Conditional genera-
tive adversarial nets. CoRR, abs/1411.1784, 2014. URL
http://arxiv.org/abs/1411.1784.

Mohamed, Shakir and Lakshminarayanan, Balaji. Learn-
arXiv preprint

ing in implicit generative models.
arXiv:1610.03483, 2016.

Nguyen, Anh Mai, Dosovitskiy, Alexey, Yosinski, Jason,
Brox, Thomas, and Clune, Jeff. Synthesizing the pre-
ferred inputs for neurons in neural networks via deep
generator networks. CoRR, abs/1605.09304, 2016. URL
http://arxiv.org/abs/1605.09304.

Odena, A. Semi-Supervised Learning with Generative Ad-

versarial Networks. ArXiv e-prints, June 2016.

Odena, Augustus, Dumoulin, Vincent,

and Olah,
Deconvolution and checkerboard artifacts.

Chris.
http://distill.pub/2016/deconv-checkerboard/, 2016.

Radford, Alec, Metz, Luke, and Chintala, Soumith.
Unsupervised representation learning with deep con-
CoRR,
volutional generative adversarial networks.
abs/1511.06434, 2015. URL http://arxiv.org/
abs/1511.06434.

Kingma, Diederik P., Rezende, Danilo Jimenez, Mohamed,
Shakir, and Welling, Max. Semi-supervised learning
with deep generative models. CoRR, abs/1406.5298,
URL http://arxiv.org/abs/1406.
2014.
5298.

Ramsundar, Bharath, Kearnes, Steven, Riley, Patrick, Web-
ster, Dale, Konerding, David, and Pande, Vijay. Mas-
In Pro-
sively multitask networks for drug discovery.
ceedings of The 33rd International Conference on Ma-
chine Learning, 2016.

Kingma, Diederik P., Salimans, Tim, and Welling, Max.
Improving variational inference with inverse autoregres-
sive ﬂow. CoRR, abs/1606.04934, 2016. URL http:
//arxiv.org/abs/1606.04934.

Reed, Scott, Akata, Zeynep, Mohan, Santosh, Tenka,
Samuel, Schiele, Bernt, and Lee, Honglak. Learn-
arXiv preprint
ing what and where to draw.
arXiv:1610.02454, 2016a.

Conditional Image Synthesis with Auxiliary Classiﬁer GANs

Reed, Scott, Akata, Zeynep, Yan, Xinchen, Logeswaran,
Lajanugen, Schiele, Bernt, and Lee, Honglak. Gener-
In Proceed-
ative adversarial text-to-image synthesis.
ings of The 33rd International Conference on Machine
Learning, 2016b.

Toderici, George, Vincent, Damien,

Johnston, Nick,
Hwang, Sung Jin, Minnen, David, Shor, Joel, and Cov-
ell, Michele. Full resolution image compression with re-
current neural networks. CoRR, abs/1608.05148, 2016.
URL http://arxiv.org/abs/1608.05148.

Uehara, M., Sato, I., Suzuki, M., Nakayama, K., and Mat-
suo, Y. Generative Adversarial Nets from a Density
Ratio Estimation Perspective. ArXiv e-prints, October
2016.

van

den Oord, A¨aron, Kalchbrenner, Nal,

Pixel
Kavukcuoglu, Koray.
CoRR, abs/1601.06759, 2016a.
works.
http://arxiv.org/abs/1601.06759.

and
recurrent neural net-
URL

van den Oord, A¨aron, Kalchbrenner, Nal, Vinyals, Oriol,
Espeholt, Lasse, Graves, Alex, and Kavukcuoglu, Ko-
ray. Conditional image generation with pixelcnn de-
coders. CoRR, abs/1606.05328, 2016b. URL http:
//arxiv.org/abs/1606.05328.

Wang, Zhou, Bovik, Alan C, Sheikh, Hamid R, and Si-
moncelli, Eero P. Image quality assessment: from error
visibility to structural similarity. IEEE transactions on
image processing, 13(4):600–612, 2004a.

Wang, Zhou, Simoncelli, Eero P, and Bovik, Alan C. Multi-
scale structural similarity for image quality assessment.
In Signals, Systems and Computers, 2004. Conference
Record of the Thirty-Seventh Asilomar Conference on,
volume 2, pp. 1398–1402. Ieee, 2004b.

Rezende, D. and Mohamed, S. Variational Inference with

Normalizing Flows. ArXiv e-prints, May 2015.

Rezende, D., Mohamed, S., and Wierstra, D. Stochas-
tic Backpropagation and Approximate Inference in Deep
Generative Models. ArXiv e-prints, January 2014.

Ronneberger, Olaf, Fischer, Philipp, and Brox, Thomas. U-
net: Convolutional networks for biomedical image seg-
mentation. CoRR, abs/1505.04597, 2015. URL http:
//arxiv.org/abs/1505.04597.

Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan,
Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpa-
thy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg,
Alexander C., and Fei-Fei, Li.
ImageNet Large Scale
Visual Recognition Challenge. International Journal of
Computer Vision (IJCV), 115(3):211–252, 2015. doi:
10.1007/s11263-015-0816-y.

Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V.,
Improved Techniques for

Radford, A., and Chen, X.
Training GANs. ArXiv e-prints, June 2016.

Simoncelli, Eero and Olshausen, Bruno. Natural image
statistics and neural representation. Annual Review of
Neuroscience, 24:1193–1216, 2001.

Springenberg, J. T. Unsupervised and Semi-supervised
Learning with Categorical Generative Adversarial Net-
works. ArXiv e-prints, November 2015.

Sutskever, Ilya, Vinyals, Oriol, and V., Le Quoc. Sequence
In Neural

to sequence learning with neural networks.
Information Processing Systems, 2014.

Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,
Pierre, Reed, Scott E., Anguelov, Dragomir, Erhan,
Dumitru, Vanhoucke, Vincent, and Rabinovich, An-
CoRR,
Going deeper with convolutions.
drew.
abs/1409.4842, 2014. URL http://arxiv.org/
abs/1409.4842.

Szegedy, Christian, Vanhoucke, Vincent, Ioffe, Sergey,
Shlens, Jonathon, and Wojna, Zbigniew. Rethinking
the inception architecture for computer vision. CoRR,
abs/1512.00567, 2015. URL http://arxiv.org/
abs/1512.00567.

Theis, L., van den Oord, A., and Bethge, M. A note on the
evaluation of generative models. ArXiv e-prints, Novem-
ber 2015.

