Composing Tree Graphical Models with Persistent Homology Features
for Clustering Mixed-Type Data

Xiuyan Ni 1 Novi Quadrianto 2 3 Yusu Wang 4 Chao Chen 1

Abstract
Clustering data with both continuous and dis-
crete attributes is a challenging task. Existing
methods often lack a principled probabilistic for-
mulation.
In this paper, we propose a cluster-
ing method based on a tree-structured graphi-
cal model to describe the generation process of
mixed-type data. Our tree-structured model fac-
torizes into a product of pairwise interactions,
and thus localizes the interaction between feature
variables of different types. To provide a robust
clustering method based on the tree-model, we
adopt a topographical view and compute peaks of
the density function and their attractive basins for
clustering. Furthermore, we leverage the theory
from topology data analysis to adaptively merge
trivial peaks into large ones in order to achieve
meaningful clusterings. Our method outperforms
state-of-the-art methods on mixed-type data.

1. Introduction

Clustering is one of the most widely used techniques
in data analysis (Xu & Wunsch, 2005; Jain, 2010). De-
spite a rich literature on pure continuous data or pure cat-
egorical data, the clustering problem remains challenging
for mixed-type data, i.e., data with both types of attributes
(Everitt et al., 2001). Mixed-type data are ubiquitous in
real world domains, e.g., social science, biomedicine and
ﬁnance, where categorical attributes often describe demo-
graphic information or questionnaire responses, and con-
tinuous attributes often correspond to quantitative measure-
ments. However, only a very limited number of clustering
methods have been proposed for such data (Everitt et al.,
2001; Huang, 1998). The major challenge is the lack of a
good geometric intuition of data on the mixed-type domain;

1City University of New York (CUNY), New York, USA
2University of Sussex, Falmer, United Kingdom 3National Re-
search University Higher School of Economics, Moscow, Rus-
sia 4Ohio State University, Columbus, USA. Correspondence to:
Chao Chen <chao.chen.cchen@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

such intuition is the important basis of many successful
geometric clustering methods, e.g., k-means (MacQueen
et al., 1967), Ward’s method (Ward Jr, 1963), DBSCAN
(Ester et al., 1996), to name a few. In practice, what usu-
ally being done is to convert mixed-type data to either pure
continuous or pure categorical domain, and subsequently
use existing geometric clustering methods. A metric for di-
rectly dealing with mixed-type data is also available, based
on Gower’s coefﬁcient (1971). The uptake of geometric
clustering methods is mostly driven by their lightweight
computational requirements. However, these methods lack
a well justiﬁed underlying probabilistic model, are sensi-
tive to the choice of underlying metric, and do not give a
principled answer to the fundamental question of required
number of clusters for the data at hand.

In this paper, we propose a probabilistic clustering
method for mixed-type data, which admits at least four
attractive properties. First, our probabilistic method goes
beyond the widely-adopted class conditional independence
assumption of feature variables, e.g., as in the latent class
model (McCutcheon, 1987). Second, our method is based
on the global topographical features, i.e., peaks and moun-
tains, of the density function, rather than the distances be-
tween data points. The argument for topographical features
is to sidestep a premature speciﬁcation of the metric space
in which our mixed-type data will achieve the best group-
ing. Third, our method is able to utilize a persistent homol-
ogy theory to automatically determine the number of clus-
ters in the data. Fourth, the proposed method can be easily
parallelized to achieve a competitive running time with re-
spect to many lightweight geometric clustering methods.

From the modeling perspective, we compose tree
graphical models with topographical features to achieve a
probabilistic mixed-type clustering model. Graphical mod-
els provide a way of factorizing a joint probability distri-
bution into a product of local interactions. These local
interactions capture dependency among feature variables.
While a Bayesian network or a Markov random ﬁeld can
be built with a set of nodes representing each feature vari-
able. The graph structure and parameter estimation can be
computationally expensive. By constraining the graph to
be a tree, the structure and parameter can be learned efﬁ-
ciently. Other than computational beneﬁts, tree-structured

Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data

graphical models also provide a modeling elegance; with a
tree structure, we have a factorization that explicitly corre-
sponds to empirical univariate and bivariate marginal dis-
tributions. For the bivariate distributions, we can then
adapt the product kernel density estimation (Scott, 2015)
to capture interaction between continuous-continuous vari-
ables, between categorical-categorical variables, and be-
tween categorical-continuous variables.

Having modeled the data generation process via a tree
graphical model, we are left with ﬁnding a robust approach
for assigning each data point to its cluster. To achieve
this, we adopt a topological perspective, namely, we view
a probability distribution as a terrain function, called the
density landscape, and capture its topographical features
as the basis for deﬁning clusters. The topographical fea-
tures include modes (peaks) and their attractive basins. For
high-dimension and sparse data, it is natural to have many
modes. To avoid over-segmentation of the data and genera-
tion of many clusters with only few members, we employ a
persistent homology theory (Edelsbrunner & Harer, 2010)
to measure the saliency of all modes and merge the triv-
ial ones. Our principled method for clustering mixed-type
data respects the underlying topographical features of the
density landscape and achieves competitive performance
on real data.

1.1. Related Work

Clustering has been extensively studied in machine
learning and data mining. Many comprehensive surveys
have been produced detailing the landscape of clustering
problems and models. Here we will review related work
in the context of geometric versus probabilistic clustering
methods for mixed-type data and clustering methods that
rely on topographical features such as modes and their at-
tractive basins.

Geometric clustering methods A straightforward ap-
proach for mixed-type data clustering is to map them into
either pure continuous or pure categorical domains before
applying a standard clustering method. A metric based
on Gower’s coefﬁcient (Gower, 1971) has been proposed
for mixed-type data, which rescales the difference in all
dimensions, continuous or categorical, and take the aver-
age. One can apply any distance based method using these
metrics. However, all these methods are heuristic; there is
no good justiﬁcation for the underlying geometric intuition
of these methods on such a counter-intuitive metric space,
despite some successful stories in practice. For exam-
ple, K-Prototypes algorithm (Huang, 1998) uses a weighted
sum of the Euclidean distance and Hamming distance and
adopts the K-Means method (Faber, 1994), which itera-
tively ﬁnds the mean of each cluster and re-associates data
to different clusters. When the data is pure categorical,
the method is called K-Modes (Huang, 1997). Chiu et
al. (2001) proposed a hierarchical clustering method, in

which distance between clusters are measured using their
log-likelihood, which treat continuous and categorical do-
main separately.

Probabilistic clustering methods Graphical models have
been applied to clustering before. Zhang (2004) proposed a
latent tree model, i.e., a Bayesian tree whose leaf nodes cor-
respond to all observed dimensions and internal nodes are
latent variables determining different clusters. Such tree
structure can be learned using efﬁcient algorithms (Chen
et al., 2012; Liu et al., 2015). However, this method is only
restricted to categorical data. Lee & Hastie (2015) pro-
posed a loopy graphical model to model mixed-type data.
Their model reduces to a discrete Markov random ﬁeld
when all attributes are categorical, and a Gaussian graph-
ical model when all attributes are continuous. Parame-
ters are learned using pseudo-likelihood estimation (Besag,
1975) and edges are selected using group sparsity penalties
(Yuan & Lin, 2006; Huang & Zhang, 2010). However, an
efﬁcient inference model is missing in order to apply such
model to clustering.

Clustering by mode-seeking The density landscape has
been exploited before to extract global properties of the
data and to achieve better clustering quality. Mode-seeking
methods, i.e., associating data to modes representing clus-
ters, have been proposed before in continuous domain
(Cheng, 1995; Comaniciu & Meer, 2002b). But such
methods rely on a kernel density estimation, which suffers
from the curse of dimensionality and thus do not scale to
high dimensions (Wasserman, 2013, chap. 20). Chen &
Quadrianto (2016) proposed a mode-seeking method for
categorical data clustering. However, their method tends
to produce trivial modes/clusters and thus over-segments
the data, mainly due to the lack a principled way to merge
modes into clusters of proper size.

Persistent homology for merging clusters In recent
years, novel approaches have been proposed to merge
modes/clusters based on the topographical landscape of the
density function. Chazal et al. (2013) used topological per-
sistence to guide the merging of data into clusters. Their
method, although theoretically sound, relies on a k-nearest
neighbor graph of the data and a given density function,
e.g., a kernel density estimation (Silverman, 1986) or a dis-
tance from measure (Chazal et al., 2011). This method
assumes that the data is a high quality sample of the do-
main and the k-nearest neighbor graph faithfully captures
the topographical characteristics of the distribution. How-
ever, this condition is often too strong to assume in practice,
where most datasets are relatively sparse.
In this paper,
we propose to start with mode-seeking, and leverage these
modes and the gradient paths as a more accurate account of
the density landscape. Our idea proves to be a better solu-
tion and a good complement to the theoretical tool. We also
refer to other topological and geometrical studies into the

Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data

global structures of hierarchical clustering (Eldridge et al.,
2015; Carlsson & M´emoli, 2010).

3. Method

2. Background

A probabilistic graphical model (Koller & Friedman,
2009) consists of a set of inter-dependent random variables
X = (X1, . . . , XD), a potential function f , and a graph
G = (V, E). Each element in the node set V represents
one random variable from X. The edges represents the
dependence relations between pairs of variables. There
are two different kinds of variables in our setting: con-
tinuous ones and discrete ones variables. For simpliﬁca-
tion, we assume each discrete variable takes discrete values
Xi ∈ L = {1, . . . , L}. In this paper, we use discrete and
categorical interchangeably and focus on non-ordinal dis-
crete variables, although ordinal discrete variables are of
interest in practice as well. In our setting, only Hamming
distance can be used for discrete variables.

A value assignment

to all random variables x =
(x1, . . . , xD) is called a conﬁguration. A potential func-
tion f : x → R assigns to each conﬁguration a real value,
which is inversely proportional to the logarithm of the prob-
ability distribution, p(x) = exp(−f (x) − A), where A is
the log-partition function. In this paper, we focus on tree
structured graphical models, represented by T = (V, E).
For a tree model, the probability and potential of a conﬁg-
uration can be factorized into a product (Bach & Jordan,
2003):

p(x) =

(cid:89)

(i,j)∈E

p(xi, xj)
p(xi)p(xj)

(cid:89)

k∈V

p(xk),

(2.1)

where p(xi, xj) is the bivariate marginal density of the vari-
able Xi and Xj, and p(xk) is the univariate marginal den-
sity of the variable Xk.

When the true distribution can be represented by a tree,
we can use the algorithm by Chow & Liu (1968) to recon-
struct the tree model. First, we compute the mutual infor-
mation between all pairs of variables:

M Iij =

p(xi, xj) log

(cid:90)

xi,xj

p(xi, xj)
p(xi)p(xj)

dxidxj,

using empirical univariate and bivariate marginals. The in-
tegral is replaced by sum when Xi and Xj have discrete
values. Next, we compute the maximum spanning tree of
a complete graph with D nodes, using the mutual informa-
tion as edge weights. The computed tree is the desired tree
model with the optimal KL-divergence from the true tree
distribution (Liu et al., 2011). More details of the selection
of the models for univariate and bivariate densities will be
given in Section 3.

Our method ﬁrst estimates the underlying probabilistic
density function from given data. We choose tree-models
as they strike a elegant balance between computational ef-
ﬁciency and ﬂexibility of the model. Next, we propose
to cluster data based on the density landscape: associat-
ing data with modes/peaks of the density, and merge them
based on advanced persistent homology theory. First, we
formalize the deﬁnition of modes in the mixed-type do-
main. Then we present algorithms for modes-seeking (Sec-
tion 3.2) and for modes-merging (Section 3.3).

We ﬁrst formalize what a mode is in a D-dimensional
mixed-type data domain. Our deﬁnition is not restricted to
the underlying model. Denote by Id and Ic the index sets
of discrete- and continuous-valued random variables. De-
note by distH (x, x(cid:48)) the Hamming distance between x and
x(cid:48) within the discrete dimensions, and distL2(x, x(cid:48)) the L2
distance within the continuous dimensions. We call a dis-
crete neighborhood of x with radius δ > 0 as all elements
with no more than δ Hamming distance and zero Euclidean
distance from x, formally,

N d

δ (x) = {x(cid:48) | distd(x, x(cid:48)) ≤ δ ∧ distc(x, x(cid:48)) = 0}.

Similarly, we deﬁne a continuous neighborhood of x with
radius (cid:15) > 0 as

N c

(cid:15) (x) = {x(cid:48) | distd(x, x(cid:48)) = 0 ∧ distc(x, x(cid:48)) ≤ (cid:15)}.

Given a probability density function, p(X), a mode is a
local maximum in both the continuous neighborhood and
discrete neighborhood, formally:
Deﬁnition 1 (Modes). A point x ∈ X is a mode if and only
if there exists positive numbers (cid:15) > 0 and δ > 0 such that
(1) p(x) ≥ p(x(cid:48)) for any x(cid:48) ∈ N c
(cid:15) (x); and (2) p(x) ≥
p(x(cid:48)) for any x(cid:48) ∈ N d
It sufﬁces to use the smallest positive integer for the dis-
crete neighborhood, δ = 1. In this paper, we focus on a
tree-structured graphical model. Next, we describe our tree
model in details within the mixed-type setting.

δ (x).

3.1. Instantiating the Tree Model

We formalize the univariate and bivariate marginal den-
sities p(xi) and p(xi, xj) in the tree model (Eq. (2.1)). We
assume a set of N data {y1, y2, · · · , yN } is given. For
discrete dimensions, we use Multinoulli distribution with
Dirichlet prior α = 1, ∀i, j ∈ Id:

p(xi) =

, with Nxi =

Nxi + 1
N + L

Nxi,xj + 1
N + L2 ,

p(xi, xj) =

N
(cid:88)
n=1(cid:74)

yn
i = xi

,

(cid:75)

with Nxi,xj =

i = xi ∧ yn
yn

j = xj

.

(cid:75)

N
(cid:88)
n=1(cid:74)

Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data

For continuous variables, we use one-dimensional kernel
density estimation for univariate density, and product ker-
nel (Scott, 2015) for univariate and bivariate marginal den-
sity. Formally, ∀i, j ∈ Ic,

p(xi) =

Kh1i (yn

i − xi), and

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

p(xi, xj) =

(cid:8)Kh2i(yn

i − xi) Kh2j (yn

j − xj)(cid:9) ,

(3.1)

(cid:17)

(cid:16)

2πh exp

We use a one-dimensional Gaussian kernel, denoted as
Kh(z) = 1√
Following standard non-
.
parametric statistics literature (Fan & Gijbels, 1996; Tsy-
bakov, 2009), the kernel bandwidths for univariate and bi-
variate density are chosen as

− z2
2h2

hti = 1.06·min

(cid:26)

σ∗
i ,

i,0.75 − q∗
q∗
1.34

(cid:27)

i,0.25

·N − 1

2β+t , t = 1, 2,

i , q∗

i,0.75 and q∗

where σ∗
i,0.25 are the standard deviation, the
75% and 25% sample quantiles of Xi, respectively. The
variable β is the order of the kernel (Fan & Gijbels, 1996)
and is set to 2 by default.

The choice of a product kernel is justiﬁed by two rea-
sons. First, a product kernel reduces to the product of one-
dimensional kernels, which are more reliable that a direct
2D kernel density estimation. Second, the product kernel
proves to be convenient to be adopt to bivariate densities
for variables with mixed-type as follows. For a mixed-type
pair of variables, (Xi, Xj), i ∈ Ic, j ∈ Id, we take the
limit of h2i to zero in the product kernel formula (Equation
(3.1)). The ﬁrst kernel becomes the Dirac-delta function,
leading to the following bivariate marginal

p(xi, xj) =

1
N

N
(cid:88)

n=1

(cid:8)
(cid:74)

yn
j = xj

Kh2i(yn

i − xi)(cid:9) .

(cid:75)

Building the tree model. Using these empirical univariate
and bivariate marginal densities, we estimate all pairwise
mutual information, and then compute the tree (V, E) using
the Chow-Liu algorithm. Plugging the univariate and bi-
variate marginal densities into Eq. (2.1), we have the com-
plete density distribution (the tree model). Next, we present
our algorithm for ﬁnding the modes over the density land-
scape of the computed model.

neighborhood N d
δ (x), with δ = 1. The two procedures
have to be taken alternatively in order to continue increas-
ing the probability until a mode is reached.

Our algorithm starts at each data, s, iteratively walks to
a nearby point with bigger probability until convergence.
The ﬁnal position is the mode of interest and will be asso-
ciated with the data, s. For ease of computation, we use the
potential function f (x) instead of the probability density
function:

f (x) = −

log p(xi, xj) −

(1 − di) log p(xi),

(cid:88)

(i,j)∈E

(cid:88)

i∈V

(3.2)
in which di is the degree of node i in the tree. It is easy to
verify that p(x) ∝ −f (x). Therefore, modes of p(x) are
the local minima of f (x), following the same deﬁnition in
Def. 1. We follow the aforementioned iterative procedure,
except at each step, we ﬁnd a nearby point with smaller
potential.

At each step of the algorithm, we ﬁrst update all dis-
crete variables until no better elements exist within the dis-
crete neighborhood N d
δ (x) with δ = 1. Next, we update
all continuous variables using gradient descent, until the
gradient of f at continuous dimensions ∇cf becomes zero.
Our main algorithm is summarized in Alg. 1.

Algorithm 1 Mode-Seeking Algorithm

1: Input: Data D = {si | i = 1, · · · , N }; a potential

function f .

2: Output: A set of modes, M; mode indices associated

to each data {ci | i = 1, · · · , N }

x ← argminz∈N d

1 (x) f (z)

repeat

x ← si
repeat

3: M ← ∅
4: for i = 1 to N do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: end for

until x converges
if x /∈ M then

until x converges
repeat

x ← x − η∇cf
until x converges

M ← M ∪ {x}

end if
ci ← the index of x in M

3.2. Mode-Seeking Algorithm

Our algorithm assigns each data to a mode via a gradi-
ent ascent procedure. For a mixed-domain, a gradient is not
well deﬁned. Following the deﬁnition of modes (Def. 1),
we formulate a gradient step as an optimization within ei-
ther the continuous neighborhood N c
(cid:15) (x) or the discrete

Here η is the stepsize. The best neighbor within Ham-
1 (x) f (z), can be computed
ming distance one, argminz∈N d
using dynamic programming. This can be achieved by
directly adapting the algorithm by (Chen & Quadrianto,
2016).

It remains to compute the gradient of f in the contin-

Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data

uous domain, ∇cf . For each continuous variable i ∈ Ic,
relevant terms in the energy function (Eq. (3.2)) can be di-
vided into three groups, the univariate term, the bivariate
terms with a continuous neighbor, j ∈ Ic, and the bivari-
ate terms with a discrete neighbor, j ∈ Id. Treating them
differently, the partial derivative:

∂f (x)
∂xi

= −(1 − di)

j∈Ic:(i,j)∈E

(cid:88)

(cid:88)

−

−

k∈Id:(i,j)∈E

(cid:80)N

n=1 Kh1i(yn
(cid:80)N

i − xi) yn
i −xi
h2
1i
i − xi)
i − xi) Kh2j (yn

n=1 Kh1i(yn

(cid:80)N

n=1 Kh2i (yn
(cid:80)N

n=1 Kh2i (yn

j − xj) yn
i −xi
h2
2i
i − xi) Kh2j (yn
j − xj)
yn
i −xi
h2
2i

yn
k = xk

(cid:80)N

n=1 Kh2i(yn
(cid:80)N

i − xi)
(cid:74)
n=1 Kh2i(yn
i − xi)

(cid:75)

(3.3)

Algorithm 2 Merging Data Using Topological Persistence
1: Input: (cid:98)G = ((cid:98)V, (cid:98)E), density function p : (cid:98)V → R+,

persistence threshold τ

2: Output: Clusters C
3: C ← ∅
4: Sort elements in (cid:98)V according to the density function

values, so that p(vi) ≥ p(vi+1), ∀vi, vi+1 ∈ (cid:98)V.

nbd ← {vj | (vi, vj) ∈ (cid:98)E ∧ j < i}
// neighbors of vi with smaller indices (bigger p)
if nbd = ∅ then

create a new cluster c = {vi}
birth(c) ← p(vi)
C ← C ∪ {c}

Cnbd ← all clusters containing nodes in nbd
cmax ← argmaxc∈Cnbd birth(c)
for all c ∈ Cnbd and c (cid:54)= cmax do

persistence(c) ← birth(c) − p(vi)
if persistence(c) < τ then
// merge c into cmax
cmax ← cmax ∪ c
C ← C\{c}

else

5: for i = 1 to |(cid:98)V| do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
end if
25:
26: end for

end if
end for
// assign vi to cmax
cmax ← cmax ∪ {vi}

3.3. Merging Clusters Using Topological Persistence

The modes computed in Alg. 1 provide a clustering of
the data. However, in practice, the data is often relatively
sparse. In such cases, the method tends to produce a large

Figure 1. Left: the density landscape with three modes (red). The
z axis is the density function. The persistence (length of the blue
vertical bars, π1, π2, π3) measure their saliency. The function val-
ues of the ends of these bars are the birth and death times (e.g., b1
and d1 for the left mode). The global maxima has inﬁnite persis-
tence, π3 = +∞. Right: an illustration of the merging hierarchy
of clusters, when π1 > τ > π2. The second mode is merged
into the third one when the ﬁrst mode remains separated. The
ﬁrst mode remains separated as its persistence is bigger than the
threshold. Two clusters remain after merging. Although in this
example, the domain is pure continous, we believe that the intu-
ition carries to the mixed domain.

number of modes, and thus over-segments the data into
small clusters. There are ways to merge these small clus-
ters (Ward Jr, 1963; Day & Edelsbrunner, 1984). But they
rely on a distance metric to measure similarities between
clusters. Instead, we propose a principled approach that is
only based on the density landscape, i.e., the topographical
features such as peaks, ridges, valleys. Our method is built
on the theory of persistent homology. We focus on zero-
dimensional topological structures in this paper, although
the theory is much more general.

Persistence of modes. We estimate the saliency of a peak
(mode) using its “relative height”, namely, the difference
between its height and the level at which its basin of attrac-
tion meets the one of another higher mode. Formally, we
ﬁlter the domain using a function value threshold t from
+∞ to −∞. As t decreases, we monitor the topologi-
cal changes of the progressively growing superlevel set,
X t = {x ∈ X | p(x) ≥ t}, that is, the domain whose
probability density value is no smaller than t. Each mode
attributes to the birth of a new connected component in the
superlevel set and the component is killed when it meets
another component created by a higher mode. The den-
sity value of the creating mode and the density value of the
point at which the two components meet (called a saddle)
are called the birth and death times, and their difference,
called the persistence, measures the saliency of this mode.
See Figure 1 for an illustration.

The merging of connected components as we decrease
the threshold t provides a natural way to merge modes;
when two connected components meet, we merge them if
one of them has ≤ τ persistence (Figure 1). This gives us a
principled way to merge modes. Based on the convergence
of tree-model estimation (Liu et al., 2011) and the stability

p(x)π1π2π3b1d1Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data

of persistent homology (Cohen-Steiner et al., 2007), this
method is guaranteed to be robust to noise and L∞ pertur-
bation of the density function.

Sample-based persistence computation. Given a dense
uniform sampling of the whole domain X , we can trust
these samples will describe the density landscape faithfully.
In practice, however, a uniform sampling will have expo-
nential size to the dimension. Chazal et al. (2013) used the
k-nearest neighbor graph of the input data, D, assuming
they are good samples from the density function. However,
in practice, the data is often relatively sparse and cannot
represent the landscape well enough to produce a high qual-
ity mode-merging hierarchy. In fact, it is very likely that the
modes are not included in the data and thus the birth time
(as well as the persistence) will be under-estimated. See
Figure 2(left) for an illustration.

In this paper, we propose to compute persistence based
on all points we encountered during the mode-seeking pro-
cedure. In Algorithm 1, we collect the point x computed
after each iteration (after line 12). The gradient step also
provides a natural edge connecting these points. This tree
structured graph give us a high-quality description of the at-
tractive basin of each mode. This provides us a well-suited
underlying graph describing the density landscape. See
Figure 2(right). Finally, to ensure the graph is fully con-
nected, and the space between modes are well described,
we add edges (green edges) connecting points from neigh-
boring attractive basins, as well as the lowest point along
these edges (green markers). Note that this is the only time
when the distance metric plays a role in our model. We use
a sum of the Hamming distance and Euclidean distance.

Algorithm. Given a graph (cid:98)G = ((cid:98)V, (cid:98)E), in which each
node is assigned a probability density, we compute the
persistence-based merge tree as follows. Sort all nodes in
decreasing order of their density function values. Add them
into the superlevel set one-by-one. To add a node vi, we
check whether it is adjacent to any nodes that have been
included. If not, vi, which must be a mode itself, creates a
new connected component with the birth time p(vi). If vi is
connected to multiple existing connected components, we
keep the one with the earliest birth time, cmax, and merge
some others into cmax. In particular, for each other adja-
cent connected component, we check whether its life length
so far is less than τ . The ones with ≤ τ life length will be
merged into cmax. We add vi into the connected compo-
nent cmax See Figure 3 for an illustration. See Alg. 2 for
the pseudocode.

4. Experiments

We compare our methods with existing clustering
methods on several real world mixed-type datasets from
UCI repository (Lichman, 2013): Contraceptive Method
Choice dataset (CMC), Credit Approval dataset (CRX),

German Credit Approval (German), and Statlog Heart Dis-
ease dataset (Heart). See the table below for more details.
All datasets have 60% to 70% of the features being discrete.
Table 1. Datasets

# of samples Dimension

Data
CMC
Heart
CRX
German

1473
297
653
1000

9
13
15
20

# of clusters
3
5
2
2

Our method can be straightforwardly parallelized. We
run the mode-seeking for all data points (the for-loop in
Alg. 1) in parallel. On average, the mode-seeking of a
single data takes 6 gradient ascent steps and 5.87 seconds.
On a cluster with 48 cores, our program ﬁnishes within 3
minutes for any of the datasets. If running in a sequential
manner, the time will be linear to the dataset size. After
all data are processed, we collect all relevant points and
run a persistence-based merging sequentially. This step
takes less than 20 seconds for any of the datasets. The
persistence-based merging depends on a threshold τ .
It
is hard to select a universal one due to the large varia-
Instead, we choose the τ for each
tion among datasets.
dataset so that the desired the nubmer of clusters remain af-
ter merging. This is a fair comparison; all clustering meth-
ods we compare with use an oracle number of clusters. We
empirically set the parameter δ to one. Using a bigger δ
hurts the performance as it would try to ‘smooth’ the land-
scape in the categorical domain.

All methods can be grouped into ﬁve different groups,
based on the underlying domain and the approach. The ﬁrst
group assumes a continuous domain and an Euclidean met-
ric. We project the mixed-type data into the continuous do-
main and directly apply such methods, including k-means
(Faber, 1994), Afﬁnity Propagation (Frey & Dueck, 2007),
Mean Shift (Cheng, 1995; Comaniciu & Meer, 2002a),
Spectral Clustering (Kamvar et al., 2003), Ward’s algo-
rithm (Ward Jr, 1963), Agglomerative clustering (Day &
Edelsbrunner, 1984) and DBSCAN (Ester et al., 1996).

The second group are methods designed for pure cat-
egorical domain, e.g., K-Modes (Huang, 1997), ROCK
(Guha et al., 1999), mixture of multinoulli (latent class
analysis) (McCutcheon, 1987). We convert mixed-type
data into categorical data by thresholding continuous val-
ues at the median. We also include Afﬁnity Propaga-
tion, Spectral Clustering and DBSCAN in this group; these
methods can be applied to any distance metrics. We com-
pute pairwise Hamming distance between data as the input
of these three methods.

For the third group, we use these three methods, but us-
ing a distance matrix based on Gower’s coefﬁcient (Gower,
1971), which was designed speciﬁcally for mixed-domain.
The fourth group uses a simply sum of the Euclidean dis-
tance (restricted to continuous dimensions) and Hamming
distance (restricted to categorical dimensions). A good rep-

Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data

Figure 2. Comparison of the topological methods: (Chazal et al., 2013) v.s. ours. The data is sampled from a mixture of three 2D
Gaussian functions. Left: k-nearest neighbor graph with k = 2 on the original data (red markers), plotted on the density function. Next
to it is the merging tree of clusters. Since the three components are disconnected, we cannot accurately estimate a proper merging time
between them. We also consider the middle and upper-right mode equally salient, as we underestimates the birth time of the upper-right
mode using the probability of one of its data point. Right: our method using the gradient steps as edges (blue) and explicit edges
connecting adjacent attractive basins (green), as well as more points collected during the procedure. Capturing the actual modes gives
us an accurate estimation of the birth time of each basin, furthermore, the lowest point along the green edge (purple marker) gives us
a good estimation of the saddle points and thus the death time of each connected component. In the illustration, we use τ = ∞ so all
three modes merged into a single cluster. The goal is to show that our method better captures the merging tree compared with (Chazal
et al., 2013). The domain is R2. But we believe that the intuition carries to the high-dimensional mixed domain.

methods. For all methods requiring random initializations,
we run each one for 10 times and take the average per-
formance. When necessary, we provide a true number of
clusters as an oracle. The cells with N/A correspond to
the cases when the program crashes. It is most likely be-
cause the Gower’s coefﬁcient and Hamming distance does
not give us a well-conditioned distance matrix for the spec-
tral clustering method.

Figure 3. Adding a new node vi into the superlevel set. Compo-
nent c3 is merged into cmax as its life length (l3 = birth(c3) −
p(vi)) is smaller than τ . The ﬁrst component c1 remains separated
after vi as its life length l1 is bigger than τ .

resentative in such group is K-Prototypes (Huang, 1998).
We again applied the three methods (Afﬁnity, Spectral and
DBSCAN) on this new metric.

In the last group, we compare our method and a few
other topological methods. We compare to the method us-
ing only modes for clustering. This is essentially an adap-
tation of (Chen & Quadrianto, 2016) to the mixed-type do-
main. We also compare to (Chazal et al., 2013) by comput-
ing the persistence on the k-nearest neighbor graph, using
our tree-model as the underlying density estimation. Fi-
nally, we also show the result of our method.

The results are listed in Table 2. We use the Adjusted
Mutual Information (AMI) (Vinh et al., 2010) and Adjusted
Rand Score (ARS) (Hubert & Arabie, 1985) to evaluate all

Discussion. Our method outperforms most methods from
all other four groups, using different types of metrics. We
also observe that a few methods based on pure categori-
cal domain are quite competitive. Similarly, K-prototype,
a popular tool for mixed-type data, has good performance
on some data. Outperforming other topological methods
(modes only and persistence only) demonstrate the signiﬁ-
cance of our contribution.

Our current experiments assume the correct number of
clusters is given. It is possible to prove that with sufﬁcient
samples and the correct threshold τ , the persistence-based
clustering can ﬁnd the correct number of cluster and the
right clustering for most data points in a sense similar to
the elegant result in (Chazal et al., 2013). A closely related
theoretical result is in (Eldridge et al., 2015), which shows
that the hierarchical clustering tree constructed by a similar
merging procedure is consistent for points sampled from a
nice density distribution over RD.

vip(v)l3l1τcmaxc1c3Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data

Table 2. Results on Real Datasets

Adjusted Random Score (ARS)

CRX

German

Adjusted Mutual Information (AMI)
German
CMC Heart

CRX

CMC

0.017
0.001
-0.004
-0.002
0.019
-0.013
-0.014

KMeans
Afﬁnity
MeanShift
Spectral
Ward
Agglomerative
DBSCAN

Heart
Continuous Domain (Euclidean Metric)
0.142
0.000
0.012
0.123
0.030
0.026
0.075
Categorical Domain (Hamming Metric)
0.108
0.000
0.127
0.024
-0.003
0.000

Kmodes
ROCK
Mixture (LCA)
Afﬁnity
Spectral
DBSCAN

0.020
0.000
0.010
0.003
N/A
0.000

Mixed Domain (Gower’s Coefﬁcient)

0.005
0.014
0.004
-0.006
0.001
0.001
0.001

0.230
0.001
0.006
0.022
-0.001
0.000

0.001
N/A
0.000

0.042
-0.022
0.000

Afﬁnity
Spectral
DBSCAN

0.034
-0.001
0.000
Summed Distance Metric (Euclidean + Hamming)
0.030
0.002
-0.010
0.003

Afﬁnity
Spectral
DBSCAN
K Prototype

0.000
0.002
0.000
0.066

0.006
N/A
0.000
0.026

0.016
0.003
-0.007
0.003
-0.026
0.006
-0.038

0.020
0.000
0.068
0.002
0.000
0.000

0.002
0.000
0.000

0.000
0.003
0.000
0.052

Topological Methods
Modes Only
Persistence Only
Ours (Modes + Pers.)

0.000
-0.001
0.026

0.000
0.083
0.285

0.000
-0.004
0.318

0.000
-0.005
0.040

0.029
0.013
0.006
0.004
0.031
0.012
0.016

0.017
0.005
0.025
0.018
N/A
0.000

0.013
N/A
0.000

0.013
N/A
0.000
0.030

0.008
0.000
0.038

0.164
0.000
0.001
0.158
0.166
0.013
0.023

0.125
0.002
0.109
0.077
-0.010
0.000

0.091
0.004
0.000

0.091
0.003
0.000
0.040

0.000
0.028
0.183

0.020
0.051
0.001
0.032
0.002
0.000
0.044

0.178
0.011
0.022
0.086
0.000
0.000

0.096
0.006
0.000

0.096
0.004
0.019
0.006

0.000
0.004
0.230

0.003
0.015
0.000
-0.001
0.008
0.000
0.011

0.017
0.000
0.016
0.017
0.000
0.000

0.016
0.000
0.000

0.016
0.001
0.000
0.010

0.000
0.001
0.006

5. Conclusions

In this paper, we propose a probabilistic clustering
method for mixed-type data. We design a tree-structured
graphical model for the mixed-type domain. We also de-
velop methods based on a topographical view of the den-
sity landscape. We design algorithms to capture modes of
the density landscape and merge trivial modes based on the
theory of persistent homology.

Acknowledgments. XN and CC have been partly funded
by the grant PSC-CUNY 69844-00 47. NQ has been partly
funded by the Russian Academic Excellence Project ‘5-
100’. YW has been partly supported by the grant NSF
DMS-1547357. The authors gratefully acknowledge use
of the services and facilities of CUNY Queens Colleges
Center for Computational Infrastructure for the Sciences
(CCIS).

References
Bach, Francis R and Jordan, Michael I. Beyond indepen-
dent components: trees and clusters. The Journal of Ma-
chine Learning Research, 4:1205–1233, 2003.

Besag, Julian. Statistical analysis of non-lattice data. The

statistician, pp. 179–195, 1975.

Carlsson, Gunnar and M´emoli, Facundo. Characteriza-
tion, stability and convergence of hierarchical clustering
methods. Journal of Machine Learning Research, 11:
1425–1470, 2010.

Chazal, Fr´ed´eric, Cohen-Steiner, David, and M´erigot,
Quentin. Geometric inference for measures based on
distance functions. Foundations of Computational Math-
ematics, 11(6):733–751, 2011.

Chazal, Fr´ed´eric, Guibas, Leonidas J, Oudot, Steve Y, and
Skraba, Primoz. Persistence-based clustering in Rieman-

Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data

nian manifolds. Journal of the ACM (JACM), 60(6):41,
2013.

Chen, Chao and Quadrianto, Novi. Clustering high dimen-
sional categorical data via topographical features. In In-
ternational Conference on Machine Learning (ICML),
2016.

Chen, Tao, Zhang, Nevin L, Liu, Tengfei, Poon, Kin Man,
and Wang, Yi. Model-based multidimensional clustering
of categorical data. Artiﬁcial Intelligence, 176(1):2246–
2269, 2012.

Cheng, Yizong. Mean shift, mode seeking, and clustering.
IEEE Transactions on Pattern Analysis and Machine In-
telligence, 17(8):790–799, 1995.

Chiu, Tom, Fang, DongPing, Chen, John, Wang, Yao, and
Jeris, Christopher. A robust and scalable clustering algo-
rithm for mixed type attributes in large database environ-
ment. In Proceedings of the seventh ACM SIGKDD in-
ternational conference on knowledge discovery and data
mining, pp. 263–268. ACM, 2001.

Chow, C and Liu, C. Approximating discrete probability
distributions with dependence trees. IEEE Transactions
on Information Theory, 14(3):462–467, 1968.

Cohen-Steiner, David, Edelsbrunner, Herbert, and Harer,
John. Stability of persistence diagrams. Discrete & Com-
putational Geometry, 37(1):103–120, 2007.

Comaniciu, D. and Meer, P. Mean shift: A robust approach
toward feature space analysis. Pattern Analysis and Ma-
chine Intelligence, IEEE Transactions on (PAMI), 24(5):
603–619, 2002a.

Comaniciu, Dorin and Meer, Peter. Mean shift: A robust
approach toward feature space analysis. Pattern Analysis
and Machine Intelligence, IEEE Transactions on, 24(5):
603–619, 2002b.

Day, William HE and Edelsbrunner, Herbert. Efﬁcient al-
gorithms for agglomerative hierarchical clustering meth-
ods. Journal of classiﬁcation, 1(1):7–24, 1984.

Edelsbrunner, Herbert and Harer, John. Computational

Topology: an Introduction. AMS, 2010.

Eldridge, Justin, Belkin, Mikhail, and Wang, Yusu. Be-
yond hartigan consistency: Merge distortion metric for
hierarchical clustering. In COLT, pp. 588–606, 2015.

Everitt, Brian S., Landau, Sabine, Leese, Morven, and
Stahl, Daniel. Cluster Analysis (5th Edition). Wiley,
2001.

Faber, Vance. Clustering and the continuous k-means algo-

rithm. Los Alamos Science, 22(138144.21), 1994.

Fan, Jianqing and Gijbels, Irene. Local polynomial mod-
elling and its applications: monographs on statistics and
applied probability 66, volume 66. CRC Press, 1996.

Frey, Brendan J and Dueck, Delbert. Clustering by passing
messages between data points. Science, 315(5814):972–
976, 2007.

Gower, John C. A general coefﬁcient of similarity and
some of its properties. Biometrics, pp. 857–871, 1971.

Guha, Sudipto, Rastogi, Rajeev, and Shim, Kyuseok.
ROCK: A robust clustering algorithm for categorical at-
tributes. In International Conference on Data Engineer-
ing (ICDE), pp. 512–521, 1999.

Huang, Junzhou and Zhang, Tong. The beneﬁt of group
sparsity. The Annals of Statistics, 38(4):1978–2004,
2010.

Huang, Zhexue. A fast clustering algorithm to cluster very
large categorical data sets in data mining. In DMKD, pp.
0, 1997.

Huang, Zhexue. Extensions to the k-means algorithm for
clustering large data sets with categorical values. Data
mining and knowledge discovery, 2(3):283–304, 1998.

Hubert, Lawrence and Arabie, Phipps. Comparing parti-
tions. Journal of classiﬁcation, 2(1):193–218, 1985.

Jain, Anil K. Data clustering: 50 years beyond k-means.

Pattern recognition letters, 31(8):651–666, 2010.

Kamvar, Sepandar D, Klein, Dan, and Manning, Christo-
pher D. Spectral learning. In Proceedings of the 18th
international joint conference on Artiﬁcial intelligence,
pp. 561–566. Morgan Kaufmann Publishers Inc., 2003.

Koller, Daphne and Friedman, Nir. Probabilistic graphical
models: principles and techniques. MIT press, 2009.

Lee, Jason D and Hastie, Trevor J. Learning the structure
of mixed graphical models. Journal of Computational
and Graphical Statistics, 24(1):230–253, 2015.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Ester, Martin, Kriegel, Hans-Peter, Sander, J¨org, Xu, Xi-
aowei, et al. A density-based algorithm for discovering
In Kdd,
clusters in large spatial databases with noise.
volume 96, pp. 226–231, 1996.

Liu, Han, Xu, Min, Gu, Haijie, Gupta, Anupam, Lafferty,
John, and Wasserman, Larry. Forest density estima-
tion. Journal of Machine Learning Research, 12:907–
951, 2011.

Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data

Liu, Teng-Fei, Zhang, Nevin L, Chen, Peixian, Liu,
April Hua, Poon, Leonard KM, and Wang, Yi. Greedy
learning of latent tree models for multidimensional clus-
tering. Machine learning, 98(1-2):301–330, 2015.

MacQueen, James et al. Some methods for classiﬁcation
In Proceed-
and analysis of multivariate observations.
ings of the ﬁfth Berkeley symposium on mathematical
statistics and probability, volume 1, pp. 281–297. Oak-
land, CA, USA., 1967.

McCutcheon, Allan L. Latent class analysis. Number 64.

Sage, 1987.

Scott, David W. Multivariate density estimation: theory,
practice, and visualization. John Wiley & Sons, 2015.

Silverman, Bernard W. Density estimation for statistics and

data analysis, volume 26. CRC press, 1986.

Tsybakov, Alexandre B.

Introduction to nonparametric
estimation. revised and extended from the 2004 french
original. translated by vladimir zaiats, 2009.

Vinh, Nguyen Xuan, Epps, Julien, and Bailey, James. In-
formation theoretic measures for clusterings compari-
son: Variants, properties, normalization and correction
for chance. Journal of Machine Learning Research, 11
(Oct):2837–2854, 2010.

Ward Jr, Joe H. Hierarchical grouping to optimize an ob-
jective function. Journal of the American statistical as-
sociation, 58(301):236–244, 1963.

Wasserman, Larry. All of statistics: a concise course in sta-
tistical inference. Springer Science & Business Media,
2013.

Xu, Rui and Wunsch, Donald. Survey of clustering algo-
IEEE Transactions on neural networks, 16(3):

rithms.
645–678, 2005.

Yuan, Ming and Lin, Yi. Model selection and estimation in
regression with grouped variables. Journal of the Royal
Statistical Society: Series B (Statistical Methodology),
68(1):49–67, 2006.

Zhang, Nevin L. Hierarchical latent class models for cluster
analysis. The Journal of Machine Learning Research, 5:
697–723, 2004.

