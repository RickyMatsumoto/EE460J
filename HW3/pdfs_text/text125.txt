Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

A. Proofs of Main Results

In this section we present proofs of the results from Section 5.

A.1. Proof of Theorem 5.5 and Corollaries

We ﬁrst demonstrate how we decompose the estimation error in the unbiased Lasso Granger estimator (cid:98)θu into the sum of
two components:

(cid:98)θu − θ∗ = (cid:98)θ +

M (cid:101)X(cid:62)( (cid:101)Xθ∗ + (cid:15) − (cid:101)X (cid:98)θ) − θ∗

1
T − p

M (cid:101)X(cid:62)(cid:15) +

M (cid:101)X(cid:62) (cid:101)X(θ∗ − (cid:98)θ) − (θ∗ − (cid:98)θ)

1
T − p

M (cid:101)X(cid:62)(cid:15) + (M (cid:101)Σn − I)(θ∗ − (cid:98)θ).

=

=

1
T − p
1
T − p

√

Letting Z = M (cid:101)X(cid:62)(cid:15)/

T − p and ∆ =

T − p(M (cid:101)Σn − I)(θ∗ − (cid:98)θ), we have

√

(cid:112)T − p( (cid:98)θu − θ∗) = Z + ∆.

(A.1)

Note that, clearly, E[Z] = 0. Thus, ∆ encapsulates the bias in (cid:98)θu. We divide this proof into two parts. We ﬁrst establish in
Lemma A.1 that (cid:98)θu is an asymptotically unbiased estimator of θ∗ by proving that (cid:107)∆(cid:107)∞ = o(1). We then proceed to prove
in Lemma A.2 that Z is asymptotically normally distributed.

Lemma A.1. Suppose Assumptions 5.3 and 5.4 are satisﬁed. Let s0 = supp( (cid:98)θ) (cid:16) (
(cid:112)log(pd)/(T − p). Then (cid:107)∆(cid:107)∞ = o(1), where ∆ =

T − p(M (cid:101)Σn − I)(θ∗ − (cid:98)θ).

√

√

T − p)/ log(pd) and µ (cid:16)

The proof of Lemma A.1, presented in Appendix B.1, uses H˝older’s inequality to decompose (cid:107)∆(cid:107)1 into the product
√
T − p(cid:107)M (cid:101)Σn − I(cid:107)∞ · (cid:107)θ∗ − (cid:98)θ(cid:107)1. We bound (cid:107)M (cid:101)Σn − I(cid:107)∞ by constructing a martingale difference sequence (see
Deﬁnition G.1 in Appendix G) and applying a Bernstein-type inequality (Lemma F.8) to this sequence. We then bound
(cid:107)θ∗ − (cid:98)θ(cid:107)1 via a standard argument for Lasso-type estimators that relies on the restricted eigenvalue condition. We amend
this argument to work in our non-i.i.d. setting by appealing to martingale theory and present a restricted eigenvalue condition
for martingale difference sequences in Appendix F.

Lemma A.2. Suppose Assumptions 5.3 and 5.4 are satisﬁed. Let s0 = supp( (cid:98)θ) (cid:16) (
(cid:112)log(pd)/(T − p). Then we have Z/(σ[M (cid:101)ΣnM(cid:62)]1/2) = M (cid:101)X(cid:62)(cid:15)/(σ

√

T − p[M (cid:101)ΣnM(cid:62)]1/2) D−→ N (0, Ipd×pd).

T − p)/ log(pd) and µ (cid:16)

√

The proof of Lemma A.2, deferred to Appendix B.2, relies on constructing a martingale difference sequence equal to
Z/(σ[M (cid:101)ΣnM(cid:62)]1/2), and applying the Martingale Central Limit Theorem (Hall & Heyde, 1980).

Having established Lemmas A.1 and A.2, we are now ready to present a proof of Theorem 5.5.

Proof of Theorem 5.5. We write the estimation error of the unbiased Lasso Granger estimator as

(cid:112)T − p( (cid:98)θu − θ∗) = Z + ∆.

√

√

T − p(M (cid:101)Σn − I)(θ∗ − (cid:98)θ). Then, by Lemma A.1 we have that ∆ P−→ 0. By
where Z = M (cid:101)X(cid:62)(cid:15)/
Lemma A.2, we have that Z/(σ[M (cid:101)ΣnM(cid:62)]1/2) D−→ N (0, I). Therefore, by the Slutsky Theorem (Van der Vaart, 2000), we
have that

T − p( (cid:98)θu − θ∗)/(σ[M (cid:101)ΣnM(cid:62)]1/2) D−→ N (0, Ipd×pd), as desired.

T − p and ∆ =

√

Theorem 5.5 allows us to demonstrate the asymptotic validity of the conﬁdence intervals we present in Corollary 5.6 as
follows:

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Proof of Corollary 5.6. By Theorem 5.5, the asymptotic normality of (cid:98)θu

i implies

(cid:18)

P

θ∗
i ∈ Ii

(cid:19)

lim
T −p→∞

= lim

T −p→∞

(cid:19)

(cid:19)

i ≤ δ(α, T − p)

− lim

T −p→∞

i − θ∗
(cid:98)θu

i ≤ −δ(α, T − p)

(cid:18)

P

(cid:19)

(cid:18)

P

(cid:98)θu
i − θ∗
(cid:18) √
P

i − θ∗
T − p((cid:98)θu
i )
σ[M (cid:101)ΣnM(cid:62)]1/2
i,i
(cid:18) √
T − p((cid:98)θu
i − θ∗
i )
P
σ[M (cid:101)ΣnM(cid:62)]1/2

i,i

= lim

T −p→∞

− lim

T −p→∞

= 1 − α.

≤ Φ−1(1 − α/2)

≤ −Φ−1(1 − α/2)

(cid:19)

(A.2)

In a similar manner, Theorem 5.5 also permits us to prove the several desirable properties of hypothesis test ΨZ(α), which
we introduce in (4.7), that we present in Corollary 5.7.

Proof of Corollary 5.7. By (4.3), we have

P(ΨZ(α) = 1|H i

0) = P(−|(cid:99)Zi| < zα/2) = α

where zα/2 = Φ−1(α/2), since (cid:99)Zi converges in distribution to the standard normal distribution. Similarly, for any u ∈ (0, 1),
we see that

P(Pi < u) = P(2(1 − Φ(|(cid:99)Zi|)) < u) = P

Φ(|(cid:99)Zi|) > 1 −

(cid:18)

(cid:19)

u
2

(cid:18)

(cid:18)

= P

|(cid:99)Zi| > Φ−1

1 −

(cid:19)(cid:19) (T −p)→∞
u
2

−−−−−−−→ u

since, again, (cid:99)Zi converges in distribution to the standard normal distribution.

In Section 4.1, we claim that the Scaled Lasso noise estimator (Sun & Zhang, 2012) (cid:98)σ, as given by (4.4), is a consistent
estimator of the true noise level σ. We note that while Sun & Zhang (2012) prove the consistency in the i.i.d. case, (cid:98)σ is
nevertheless still consistent in our non-i.i.d. case as well. This result follows directly from Theorem 1 in Sun & Zhang
(2012), which we paraphrase in the following lemma.
Lemma A.3. Let ( (cid:98)θ(λ), (cid:98)σ(λ)) be the Scaled Lasso estimator from (4.4) and λ = 8Cσ(cid:112)log(pd)/(T − p). Furthermore,
let the assumptions of Theorem 5.5 hold. Then,

P

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:98)σ(λ)
σ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

− 1

> (cid:15)

→ 0,

for all (cid:15) > 0 as (T − p, pd) → ∞.

We present a proof of this lemma in Appendix B.3.

A.2. Proof of Theorem 5.9

We ﬁrst present a property and three lemmas that will allow us to prove Theorem 5.9. For ease of presentation, let
G(t) = 2(1 − Φ(t)) and G−1(t) = Φ−1(1 − t/2).
Property 1. Recall that (cid:101)Σ = [σi,j] ∈ Rpd×pd is the true covariance matrix of our design matrix (cid:101)X. Now let ρi,j =
σi,iσj,j, and for δ, (cid:15) > 0, let B(δ) = {(i, j)||ρi,j| ≥ δ, i (cid:54)= j} and A((cid:15)) = B([log(pd)]−2−(cid:15)). By a similar argument
σi,j/
to that made by Liu et al. (2013b), since we assume that supp( (cid:98)θ) (cid:16)

T − p/ log(pd) in Theorem 5.5, we have

√

√

(cid:88)

(i,j)∈A((cid:15))

(pd)a1 = O((pd)2/(log(pd)2),

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

where a1 = 2|ρi,j|/(1 + |ρi,j|) + δ.

Property 1 amounts to a sparsity assumption on the true covariance matrix and allows us to cope with the correlation among
test statistics.
Lemma A.4. Suppose Assumptions 5.3, 5.4, the conditions of Theorem 5.9, and Property 1 are satisﬁed. Then we can
bound (cid:98)ν from (4.9) as follows:

where xpd = G−1(ypd/(pd)), and ypd is a sequence such that ypd

pd
−→ ∞ and ypd = o(pd).

P(0 ≤ (cid:98)ν ≤ xpd) → 1

Lemma A.4 bounds (cid:98)ν with high probability. We use this bound in the following lemma:
Lemma A.5. Suppose Assumptions 5.3, 5.4, and Property 1 are satisﬁed. Then for xpd as deﬁned in Lemma A.4, we have:

Lemma A.6. Suppose Assumptions 5.3 and 5.4 are satisﬁed. Then we have:

sup
0≤ν≤xpd

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)

i∈H0

1(|(cid:99)Zi| ≥ ν)

|H0|G(ν)

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)

P−→ 0.

(pd)G((cid:98)ν)

max{(cid:80)

1≤j≤pd

1(|(cid:99)Zi ≥ (cid:98)ν), 1}

= α.

In the interest of clarity, we defer the proofs of Lemmas A.4, A.5, and A.6 to Appendices B.4, B.5, and B.6, respectively.

The proof of Theorem 5.9 proceeds in three parts. We ﬁrst bound (cid:98)ν with high probability in Lemma A.4, and then prove in
Lemma A.5 that for any ν within those bounds

The result of Theorem 5.9 then follows naturally by Lemma B.6 .

Proof of Theorem 5.9. By Lemma A.4, P(0 ≤ (cid:98)ν ≤ xpd) → 1. Then by Lemma A.5, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)

i∈H0

1(|(cid:99)Zi| ≥ (cid:98)ν)

|H0|G((cid:98)ν)

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)

≤ sup

0≤ν≤xpd

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)

i∈H0

1(|(cid:99)Zi| ≥ ν)

|H0|G(ν)

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)

P−→ 0.

Thus, we have

(cid:80)

i∈H0

1(|(cid:99)Zi| ≥ (cid:98)ν)

P−→ 1.

|H0|G((cid:98)ν)

(cid:80)

i∈H0

1(|(cid:99)Zi| ≥ (cid:98)ν)

P−→ 1.

|H0|G((cid:98)ν)

From this result, we see that by the deﬁnition of FDP(ν),

FDP((cid:98)ν)
α|H0|/(pd)

=

(pd) (cid:80)
α|H0| max{(cid:80)

i∈H0

1(|(cid:99)Zi| ≥ (cid:98)ν)

P−→

1≤j≤pd

1(|(cid:99)Zi ≥ (cid:98)ν), 1}

α|H0| max{(cid:80)

1≤j≤pd

1(|(cid:99)Zi ≥ (cid:98)ν), 1}

(pd)|H0|G((cid:98)ν)

.

(A.3)

(pd)G((cid:98)ν)

max{(cid:80)

1≤j≤pd

1(|(cid:99)Zi ≥ (cid:98)ν), 1}

= α.

(A.4)

By Lemma A.6,

Thus, by (A.3) and (A.4)

as (T − p, pd) −→ ∞. This result clearly then implies that

as (T − p, pd) −→ ∞, as desired.

FDP((cid:98)ν)
α|H0|/(pd)

P−→ 1,

FDR((cid:98)ν)
α|H0|/(pd)

P−→ 1,

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

B. Proofs of Technical Lemmas in Appendix A

In this section we present the proofs of technical lemmas introduced in Appendix A.

B.1. Proof of Lemma A.1

We ﬁrst present two auxiliary lemmas that we will use in the proof of Lemma A.1.
Lemma B.1. If Assumption 5.3 holds and we additionally assume that the rows of (cid:101)X (cid:101)Σ−1/2 are sub-Gaussian with
sub-Gaussian norm of κ = (cid:107) (cid:101)Σ−1/2 (cid:102)Xi(cid:107)ψ2, then

(cid:107)M (cid:101)Σn − I(cid:107)∞ ≤ a

(cid:115)

log(pd)
,
T − p

a2Cmin
24e2κ4Cmax

(cid:107)θ∗ − (cid:98)θ(cid:107)1 ≤

12λs0
κ(cid:96)

,

holds with probability at least 1 − 2(pd)−c2, where c2 =

− 2 and a is some constant.

The proof of Lemma B.1, which we defer to Appendix C.1, relies constructing a martingale difference sequence (see
Deﬁnition G.1 in Appendix G) and applying a Bernstein-type inequality (Lemma F.8) to this sequence.
Lemma B.2. Let λ = 8Cσ(cid:112)log(pd)/(T − p) for some constant C. Then

with probability at least

1 − b1 exp[−b2σ2 log(pd)] − 2 exp(−c2

0c2

1c2ω2(B)) − L exp

− 4

(cid:20)

(ω(A))2
α2

(cid:21)
,

1
T − p

2 is the restricted minimum eigenvalue of (cid:101)Σn restricted to A ⊆ Spd−1 (the
where λmin( (cid:101)Σn|A) = inf u∈A
unit sphere in Rpd space), B = {(cid:101)u : (cid:101)u = (cid:101)Σ1/2u/(cid:107) (cid:101)Σ1/2u(cid:107)2, u ∈ A} is the normalized set of A, α = diam(A) =
supu,v∈A d(u, v) = supu,v∈A (cid:107)u − v(cid:107)2, ω(A) is the Gaussian width of set A as deﬁned in Deﬁnition F.5, and
κ(cid:96), b1, b2, c0, c1, c2, L > 0 are constants.

(cid:107) (cid:101)Xu(cid:107)2

The proof of Lemma B.2, which we present in Appendix C.2, relies on the restricted eigenvalue condition to bound (cid:107)θ∗ − (cid:98)θ(cid:107)1
with high probability.

We now present a proof of Lemma A.1.

Proof of Lemma A.1. H˝older’s inequality allows us to decompose (cid:107)∆(cid:107)1 as follows:

(cid:107)∆(cid:107)∞ ≤ (cid:107)∆(cid:107)1 ≤ (cid:112)T − p(cid:107)M (cid:101)Σn − I(cid:107)∞ · (cid:107)θ∗ − (cid:98)θ(cid:107)1.

We now bound (cid:107)M (cid:101)Σn − I(cid:107)∞ and (cid:107)θ∗ − (cid:98)θ(cid:107)1 separately. By Lemma B.1, we ﬁnd that (cid:107)M (cid:101)Σn − I(cid:107)∞ ≤ a(cid:112)log(pd)/T − p
with high probability . Additionally, by Lemma B.2, (cid:107)θ∗ − (cid:98)θ(cid:107)1 ≤ 12λs0/κ(cid:96) with high probability

Combining these two high-probability bounds yields the following result:

(cid:112)T − p(cid:107)M (cid:101)Σn − I(cid:107)∞ · (cid:107)θ∗ − (cid:98)θ(cid:107)1 < as0

96σ
κ(cid:96)

·

log(pd)
√
T − p

,

(B.1)

√

√

T − p). Recall that by assumption, s0 (cid:16)

T − p/ log(pd).

with high probability. Thus, by (B.1), (cid:107)∆(cid:107)∞ = o(s0 log(pd)/
Therefore, (cid:107)∆(cid:107)∞ = o(1).

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

B.2. Proof of Lemma A.2

We ﬁrst present an auxiliary lemma that we will use in the proof of Lemma A.2.
Lemma B.3 (Lindeberg condition from Hall & Heyde (1980)). Denote the martingale difference sequence ζi,t =
1((cid:107)ζt| ≥ δ)|Ft−1] →
((cid:15)t (cid:102)X (cid:62)
0.

i (cid:101)Σnmi]1/2) and ﬁltration Ft = σ( (cid:102)X1, . . . , (cid:102)Xt, (cid:15)1, . . . , (cid:15)t). Then, (cid:80)T

t mi)/(σ[m(cid:62)

E[ζ 2
i,t

t=p+1

In the interest of clarity, we defer the proof of Lemma B.3 to Appendix C.3.

Proof of Lemma A.2. To prove this lemma, we need to show that

Note that

(cid:99)Zi
σ[M (cid:101)ΣnM(cid:62)]1/2

i,i

=

m(cid:62)
i (cid:101)X(cid:62)(cid:15)
i (cid:101)Σnmi]1/2

σ[m(cid:62)

D−→ N (0, 1).

(cid:99)Zi = m(cid:62)

i (cid:101)X(cid:62)(cid:15) = (m(cid:62)

i (cid:101)X(cid:62)(cid:15))(cid:62) = (cid:15)(cid:62) (cid:101)Xmi =

(cid:15)t (cid:102)X (cid:62)

t mi.

T
(cid:88)

t=p+1

Now deﬁne ﬁltration Ft = σ( (cid:102)X1, . . . , (cid:102)Xt, (cid:15)1, . . . , (cid:15)t). By (3.1), (cid:15)t is independent of Ft−1 and conditionally independent
of (cid:102)Xt given Ft−1. Furthermore, by (4.2), (cid:15)t is conditionally independent mi given Ft−1. Therefore,

where the last equality follows since (cid:15)t∼N (0, σ2). Thus,

E[(cid:15)t (cid:102)X (cid:62)

t mi|Ft−1] = E[(cid:15)t|Ft−1] · E[ (cid:102)X (cid:62)

t mi|Ft−1]

= E[(cid:15)t] · E[ (cid:102)X (cid:62)
= 0,

t mi|Ft−1]

{ζi,t}T

t=p+1 =

(cid:26)

(cid:15)t (cid:102)X (cid:62)
t mi
i (cid:101)Σnmi]1/2

σ[m(cid:62)

(cid:27)T

,

t=p+1

is a martingale difference sequence by Deﬁnition G.1 in Appendix G.

i (cid:101)Σnmi]1/2) = (cid:80)T

Since (cid:99)Zi/(σ[m(cid:62)
t=p+1 ζi,t, if we can show that we can apply the Martingale Central Limit Theorem
(MCLT) (Hall & Heyde, 1980) to ζi,t, the result of this lemma will follow. To demonstrate that we can apply the MCLT to
ζi,t, we must prove that the Lindeberg condition holds for this sequence. By Lemma B.3, the Lindeberg condition holds for
ζi,t. Therefore, by the MCLT

T
(cid:88)

t=p+1

ζi,t =

(cid:99)Zi

D−→ N (0, 1),

σ[m(cid:62)

i (cid:101)Σnmi]1/2

as desired.

B.3. Proof of Lemma A.3

We present a variation on the argument made in the Proof of Theorem 1 in Sun & Zhang (2012). The proof of Lemma A.3
requires the following two supporting lemmas from Sun & Zhang (2012), which in turn necessitate introducing some new
notation. Denote the penalized least-squares loss function L(θ) = (2(T − p))−1(cid:107)Y − (cid:101)Xθ(cid:107)2
2 + λ(cid:107)θ(cid:107)1. We distinguish L(·)
from the Scaled Lasso loss function from (4.4), which we denote Lλ(θ, σ) = (2(T − p))−1(cid:107)Y − (cid:101)Xθ(cid:107)2
2 + σ/2 + λ(cid:107)θ(cid:107)1.
As Sun & Zhang (2012) note, θ is a critical point of L if and only if it satisﬁes:

(cid:40)

(cid:102)X·,j(Y − (cid:101)Xθ)/(T − p) = λsign(θj), θj (cid:54)= 0
(cid:102)X·,j(Y − (cid:101)Xθ)/(T − p) ∈ [−λ, λ], θj (cid:54)= 0

(B.2)

(B.3)

(B.4)

(B.5)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

where (cid:102)X·,j is the j-th column of the design matrix (cid:101)X. Importantly, Sun & Zhang (2012) note that B.2 is the Karush-Kuhn-
Tucker (KKT) condition for the minimization of L(·) when L(·) is convex in θ. This property will prove important in the
discussion of Lemma B.3 below. We now present the ﬁrst of two supporting lemmas for the proof of Lemma A.3.
Lemma B.4 (Proposition 1 from Sun & Zhang (2012)). Let (cid:98)θ = (cid:98)θ(λ) be a solution path of B.2 and λ0 = λ/σ =
8C(cid:112)log(pd)/(T − p). Then the loss function Lλ(·, ·) is jointly convex in (θ, σ). Furthermore, the derivative of Lλ(·, ·)
with respect to σ is

∂
∂t

Lλ0( (cid:98)θ(tλ0), t) =

−

1
2

(cid:107)Y − (cid:101)Xθ(tλ0)(cid:107)2
2
2(T − p)t2

.

Lemma B.4 does not rely on the i.i.d.-ness of the rows of (cid:101)X, and so we refer readers to the proof of Proposition 1 in Sun &
Zhang (2012) for a proof of Lemma B.4.

The second supporting lemma requires additional notation from Sun & Zhang (2012). Let η(λ, ξ, w, Q) be a prediction
error bound for the estimation of θ∗ via the Scaled Lasso. Let w ∈ Rpd Q ⊂ 1 . . . , pd, and

η(λ, ξ, w, Q) = (cid:107) (cid:101)X(θ∗ − w)(cid:107)2

2/(T − p) + 2λ(cid:107)wQc(cid:107)1(2 − 1(w = θ, Q = ∅)) +

4ξ2λ2|Q|
,
(ξ + 1)2κ(ξ, Q)

where vS = [vi]i∈S) ∈ R|S| and

κ(ξ, Q) = min

: u ∈ E(ξ, Q), u (cid:54)= 0

,

(cid:26) |Q|1/2(cid:107) (cid:101)Xu(cid:107)2
T − p

(cid:107)uQ(cid:107)1

√

(cid:27)

√

where E(ξ, Q) = {u : (cid:107)uQc(cid:107)1 ≤ ξ(cid:107)uQ(cid:107)1}. Now let σ∗ = (cid:107)Y − (cid:101)Xθ∗(cid:107)2/
T − p, which Sun & Zhang (2012)
call the oracle estimator for σ. Based on these deﬁnitions, let the minimum prediction error bound be η∗(λ, ξ) =
inf w,Q η(λ, ξ, w, Q) and deﬁne the following related quantity τ0 = η1/2
(σ∗λ0, ξ)/σ∗, where recall that λ0 = λ/σ =
8C(cid:112)log(pd)/(T − p). As Sun & Zhang (2012) note, since in (3.2) (cid:15) in a Gaussian random vector, σ∗ is the maximum
likelihood estimator for σ when θ is known. Thus, in the proof of Lemma A.3, we bound the quantity (cid:98)σ/σ∗ − 1 by τ0 in
order to prove the consistency of (cid:98)σ. However, we ﬁrst require the following intermediate result.
Lemma B.5 (Theorem 4 from Sun & Zhang (2012)). Let (cid:98)θ(λ) minimize L(·), ξ > 0, and deﬁne η∗(λ, ξ) =
1)1/2)(cid:9) (not to be confused with η∗(λ, ξ) deﬁned above).
minQ
If (cid:107) (cid:101)Xtop(Y − (cid:101)Xθ∗)(cid:107)∞/(T − p) ≤ λ(ξ − 1)/(ξ + 1), then

(cid:8)(1/2)(η(λ, ξ, θ∗, Q) + (η(λ, ξ, θ∗, Q) − 16λ2(cid:107)θQc(cid:107)2

∗

(cid:107) (cid:101)X( (cid:98)θ − θ∗)(cid:107)2

2/(T − p) ≤ min (cid:8)η∗(λ, ξ), η∗(λ, ξ)(cid:9).

The proof of Lemma B.3 follows directly from the proof of Theorem 4 from Sun & Zhang (2012). The only point of
contention in that proof is that B.2 must be the KKT condition for the minimization of L(·), which as we state above requires
that L(·) is convex in theta. In Lemma C.1 in Appendix C.2, we prove that the restricted eigenvalue condition holds with
high probability for the sample covariance matrix (cid:101)Σn. As we demonstrate in the proof of Lemma B.2 in Appendix C.2,
the restricted eigenvalue condition implies that the unpenalized least-squares loss function (2(T − p))−1(cid:107)Y − (cid:101)Xθ(cid:107)2
2 is
strictly convex for all θ such that the error vector θ − θ∗ falls in the error cone E(3, S), where S is the support of θ∗. Since
E(3, S) actually encompasses all possible error vectors, a property we prove in the proof of Lemma B.2, we see that the
unpenalized loss function is convex with respect to θ. Since the derivative of penalty term λ(cid:107)θ(cid:107)1 with respect to θ is a
strictly positive vector, given that the unpenalized loss function is convex in θ, so is the pealized loss function. Thus, B.2 is
the KKT condition for the minimization of L(·), and Lemma follows immediately from the proof of Theorem 4 in Sun &
Zhang (2012). We refer the reader to that paper for the full proof.

We are now ready to present the proof of Lemma A.3.

Proof of Lemma A.3. We present an amended version of the Proof of Theorem 1 from Sun & Zhang (2012). Let z∗ =
(cid:107) (cid:101)X(cid:62)(Y − (cid:101)Xθ∗)/(T − p)(cid:107)∞/σ∗. Without loss of generality, assume τ0 < 1 and let t ≥ σ∗(1 − τ0) and let λ1 = tλ0,
where λ0 is as deﬁned above. Now note that

z∗σ∗ ≤ σ∗(1 − τ0)λ0

ξ − 1
ξ + 1

≤ tλ0

= λ1

ξ − 1
ξ + 1

ξ − 1
.
ξ + 1

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Then by this inequality, the deﬁnition of σ∗, the Cauchy-Schwarz inequality, and Lemma B.3, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:107)Y − (cid:101)X (cid:98)θ(λ1)(cid:107)2
T − p

√

− σ∗

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:107) (cid:101)X( (cid:98)θ(λ1) − θ∗)(cid:107)2
T − p

√

≤ η1/2
∗

(λ1, ξ).

(B.6)

Now observe that B.3 in Lemma B.4 yields

2t2 ∂
∂t

Lλ0( (cid:98)θ(tλ0), t) = t2 −

(cid:107)Y − (cid:101)X (cid:98)θ(tλ0)(cid:107)2
2
(T − p)

≤ t2 − (σ∗)2(1 − τ0)2,

where the last inequality follows from B.6 and the deﬁnition of τ0, which implies η1/2
(λ1, ξ) ≤ σ∗τ0 for t < σ∗, since
λ1 = tλ0. Note that when t = σ∗(1 − τ0), the expression on the right-hand side of the last inequality equals zero. Note
further that Lλ(·, ·) is strictly convex in σ. Then the negativity of 2t2∂/(∂t)Lλ0 ( (cid:98)θ(tλ0), t) implies that (cid:98)σ ≥ σ∗(1 − τ0).
On the other hand, at t = σ∗/(1 − τ0) > σ∗, we have

∗

t2 −

(cid:107)Y − (cid:101)X (cid:98)θ(tλ0)(cid:107)2
2
(T − p)

≥ t2 − (σ + tτ0)2 ≥ 0,

since for t > σ∗, we have η1/2
Therefore,

∗

(λ1, ξ) ≤ tτ0. This result implies σ∗ ≥ (cid:98)σ(1 − τ0) by the strict convexity of Lλ(·, ·) in σ.

(B.7)

As Sun & Zhang (2012) argue, η1/2

∗

(λ1, ξ)/σ → 0 as (T − p, pd) → ∞. Thus,

(cid:18)

1 − (cid:98)σ

max

σ∗, 1 −

(cid:19)

σ∗
(cid:98)σ

≤ τ0.

P

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:98)σ(λ)
σ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

− 1

> (cid:15)

→ 0,

for all (cid:15) > 0 as (T − p, pd) → ∞.

B.4. Proof of Lemma A.4

We will need the following lemma, which is a modiﬁed version on Lemma 6.1 from Liu et al. (2013b) to prove Lemma A.4.
n), and E[(cid:107)ξi(cid:107)bpr+2+(cid:15)
Lemma B.6. Let ξ1, . . . , ξn ∈ Rp have mean zero. Suppose that p ≤ nr, log(p) = o(
] ≤ ∞, for
r, b, (cid:15) > 0. Furthermore, assume that (cid:107) Cov(ξi) − Ip×p(cid:107)2 ≤ C(log(p))−2−γ, where Cov(ξi) = E[(1/n) (cid:80)n
i ] and
γ > 0. Deﬁne (cid:107) · (cid:107)min as (cid:107)v(cid:107)min = min1≤i≤p{|vi|}. Then,

i=1 ξiξ(cid:62)

√

2

sup
√

0≤t≤b

log(p)

P((cid:107) (cid:80)n

i=1 ξi(cid:107)min ≥ t
(G(t))p

√

n)

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ C(log(p))−1−γ1 ,

where γ1 = min{γ, 1/2}.

√

Note that we make the assumptions p ≤ nr, r > 0 and log(p) = o(
n), in the statement of Theorem 5.9. The former
assumption is clearly satisﬁed in our setting, as we can have r > 1. As noted by Liu & Luo (2014), given that p ≤ nr, for
r > 0, our assumption 5.4 of (cid:101)X having bounded sub-Gaussian rows is equivalent to E[(cid:107)ξi(cid:107)bpr+2+(cid:15)
] ≤ ∞, for b, (cid:15) > 0.
Additionally, the assumption of (cid:107) Cov(ξi) − Ip×p(cid:107)2 ≤ C(log(p)−2−γ is satisﬁed by Property 1, the sparsity property of the
covariance matrix. Whereas Liu et al. (2013b) prove Lemma B.6 for the i.i.d. case, we present a proof that draws on the
Bernstein inequality for martingale difference sequences (Lemma F.8) to prove this lemma when ξ1, . . . , ξn ∈ Rp are not
independent. We defer the proof of Lemma B.6 to Appendix C.4. Having established Lemma B.6, we now present the proof
of Lemma A.4.

2

Proof of Lemma A.4. By Lemma B.6, we know that

max
1≤i≤pd

sup
√

0≤ν≤4

log(pd)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

P((cid:99)Zi ≥ ν)
G(ν)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

− 1

≤ C(log(pd))−1−γ1 ,

(B.8)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

for positive constants C and γ1. Then by (B.8), we have that P(|(cid:99)Zi| ≥ (cid:112)2 log(pd)) → 1 for all i ∈ B =
{i||θ∗

) ≥ (cid:112)c log(pd)/(T − p)}. Thus,

i |/(σ (cid:101)Σ−1/2

i,i

(cid:80)

i∈B

P(|(cid:99)Zi| ≥ (cid:112)2 log(pd))

P−→ 1,

|B|

(B.9)

since we assumed by Assumption 5.8 that |B| = (cid:80)
of true alternatives were ﬁxed, this convergence would clearly not occur. Now note that by Markov’s inequality

) ≥ (cid:112)c log(pd)/(T − p)} −→ ∞. If the number

i |/(σ (cid:101)Σ−1/2

1{|θ∗

i∈H1

i,i

(cid:18) (cid:88)

P

(cid:19)
1{|(cid:99)Zi| ≥ (cid:112)2 log(pd))} ≥ |B|

≤

i∈B

(cid:20)

E

(cid:80)

i∈B

(cid:21)
1{|(cid:99)Zi| ≥ (cid:112)2 log(pd))}

|B|
P(|(cid:99)Zi| ≥ (cid:112)2 log(pd)))

|B|

(cid:80)

i∈B

=

P−→ 1,

where the convergence follows by (B.9). Therefore, since (cid:80)

1{|(cid:99)Zi| ≥ (cid:112)2 log(pd))} ≤ |B|, we have

i∈B

(cid:80)

i∈B

1{|(cid:99)Zi| ≥ (cid:112)2 log(pd))}

P−→ 1.

|B|

This line implies that for 0 ≤ (cid:98)ν ≤ xpd, our FDR control procedure will correctly identify all true positives that meet a
certain minimum signal condition. The result of this lemma then follows from the deﬁnition of (cid:98)ν Section 4.2.

B.5. Proof of Lemma A.5

We will need the following lemma, which is a modiﬁed version on Lemma 6.2 from Liu et al. (2013b) to prove Lemma A.5.
Lemma B.7. Let η1, . . . , ηn have mean zero, where ηt = (ηt,1, ηt,2)(cid:62). Suppose that p ≤ nr, log(p) = o(
n), and
E[(cid:107)ξi(cid:107)bpr+2+(cid:15)
] ≤ ∞, for r, b, (cid:15) > 0. Furthermore, assume that V [ηt,1] = V [ηt,2] and |Cov(ηt,1, ηt,2)| ≤ δ, for some
2
0 ≤ δ ≤ 1. Then

√

P

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

(cid:19)

ηi,1

≥ t

n,

ηi,2

≥ t

n

≤ C(t + 1)−2 exp[−t2/(1 + δ)],

for 0 ≤ t ≤ b log(2), where C depends only on b, r, (cid:15), δ.

The proof of Lemma B.7 follows almost exactly the Proof of Lemma 6.2 in Liu et al. (2013b). The only difference is that
whereas Lemma 6.2 in Liu et al. (2013b) requires i.i.d. ηt vectors in order to cite the Proof of Lemma 6.1 in Liu et al.
(2013b), we do not require i.i.d. ηt vectors and instead appeal to the proof of Lemma B.6. We refer the reader to Liu et al.
(2013b) for more details. Having established Lemma B.7, we now present the proof of Lemma A.5.

Proof of Lemma A.5. Let b0 < b1 < . . . < bk and νi = G−1(bi), where b0 = ypd/(pd), bi = ypd/(pd) + y2/3
/(pd),
k = [log((pd − ypd)/y2/3
pd )]1/δ, and 0 < δ < 1. Then we have G(νi)/G(νi+1) = 1 + o(1) for all 0 ≤ i ≤ k, and
ν0/(cid:112)2 log(pd/ypd) = 1 + o(1). One can easily verify that 0 ≤ j ≤ k ↔ 0 ≤ ν ≤ ypd. So we see that to prove this lemma
it sufﬁces to show that

pd eiδ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

max
0≤j≤k

(cid:80)

i∈H0

[1(|(cid:99)Zi| ≥ νj) − G(νj)]

|H0|G(νj)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

P−→ 0.

Observe that for all (cid:15) > 0,

(cid:18)

P

(cid:12)
(cid:12)
(cid:12)
(cid:12)

max
0≤j≤k

(cid:80)

i∈H0

[1(|(cid:99)Zi| ≥ νj) − G(νj)]

|H0|G(νj)

(cid:19)

(cid:18)

≥ (cid:15)

≤ P

(cid:12)
(cid:12)
(cid:12)
(cid:12)

max
0≤j≤k

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:80)

(cid:80)

i∈H0

[1(|(cid:99)Zi| ≥ νj) − G(νj)]

|H0|G(νj)
[1(|(cid:99)Zi| ≥ νj) − G(νj)]

i∈H0

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:15)
2

.

≥

(cid:19)

(cid:15)
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

|H0|G(νj)

≤

k
(cid:88)

j=0

P

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(B.10)

(B.11)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Now let

Note that

(cid:80)

i∈H0

[1(|(cid:99)Zi| ≥ ν) − P(|(cid:99)Zi| ≥ ν)]

.

I(ν) =

|H0|G(ν)

(cid:80)

i∈H0

[1(|(cid:99)Zi| ≥ ν) − P(|(cid:99)Zi| ≥ ν)]

(cid:80)

i∈H0

[1(|(cid:99)Zi| ≥ νj) − G(νj)]

P−→

I(ν) =

|H0|G(νj)

,

|H0|G(ν)

by (B.8) in the proof of Lemma A.4 in Appendix B.4. Clearly, E[I(ν)] = 0, so if we can show that V [I(ν)] = E[I 2(ν)] → 0,
we will have that I(ν) P−→ 0. By (B.11), this convergence will prove (B.10). We now decompose V [I(ν)] = E[I 2(ν)] as
follows:

E[I 2(ν)] =

(cid:80)

i∈H0

[P(|(cid:99)Zi| ≥ ν) − P2(|(cid:99)Zi| ≥ ν)]

|H0|2G2(ν)

(cid:80)

+

i,j∈H0,i(cid:54)=j[P(|(cid:99)Zi| ≥ ν, |(cid:99)Zj| ≥ ν) − P(|(cid:99)Zi| ≥ ν)P(|(cid:99)Zj| ≥ ν)]
|H0|2G2(ν)

≤

C|H0|G(ν)
(|H0|G(ν))2 +

1
G2(ν)|H0|2

(cid:88)

(i,j)∈A((cid:15))

P(|(cid:99)Zi| ≥ ν, |(cid:99)Zj| ≥ ν) +

1
|H0|2

(cid:88)

i,j∈H0

(cid:84) A((cid:15))c

(cid:18) P(|(cid:99)Zi| ≥ ν, |(cid:99)Zj| ≥ ν)
G2(ν)

(cid:19)
,

− 1

where the equality follows by direct computation, and the inequality holds by (B.8) in the proof of Lemma A.4 in Appendix
B.4. If we let

and

then (B.12) yields

Furthermore, Lemma B.7 yields

I1,1(ν) =

1
G2(ν)|H0|2

(cid:88)

(i,j)∈A((cid:15))

P(|(cid:99)Zi| ≥ ν, |(cid:99)Zj| ≥ ν),

I1,2(ν) =

1
|H0|2

(cid:88)

i,j∈H0

(cid:84) A((cid:15))c

(cid:18) P(|(cid:99)Zi| ≥ ν, |(cid:99)Zj| ≥ ν)
G2(ν)

(cid:19)

− 1

,

E[I 2(ν)] ≤

C
|H0|G(ν)

+ I1,1(ν) + I1,2(ν).

|I1,2(ν)| ≤ C(log(pd))−1−δ.

P(|(cid:99)Zi| ≥ ν, |(cid:99)Zj| ≥ ν) ≤ C exp

(cid:20)

−ν2
1 + |ρi,j| + δ1

(cid:21)
,

Applying Lemma B.6 to I1,2(ν) yields the following result for all 0 ≤ t ≤ (cid:112)2 log(pd) for some δ > 0:

for all (i, j) ∈ A((cid:15)) and i, j ∈ H0, where δ1, C > 0. Lastly, the proceeding result follows from (B.15) and Property 1

Then by (F.2), (B.14), and (B.16), we have that

I1,1(ν) ≤ C(log(pd))−2.

k
(cid:88)

j=0

E[I(νj)]2 ≤ Ck[(log(pd))1−δ + (log(pd))−2] + C

(pdG(νj))−1

k
(cid:88)

j=0

(B.12)

(B.13)

(B.14)

(B.15)

(B.16)

≤ C

k
(cid:88)

j=0

1
ypd + y2/3

pd ejδ

+ o(1)

= o(1).

The second inequality holds by the deﬁnition of the sequence νi and since k = o(log(pd)). The third inequality follows
since 1/(ypd + y2/3
) = o(1/ypd) = o(1/(pd)). Our desired result (B.10) follows naturally from this last set of
inequalities.

pd ejδ

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

B.6. Proof of Lemma A.6

We present a variation on the argument made in the Proof of Theorem 3.1 in Liu et al. (2013b).

Proof of Lemma A.6. Since by the deﬁnition of (cid:98)ν in 4.9, (cid:98)ν is the inﬁmum of all values ν > 0 such that FDP(ν) ≤ α, for
ν < (cid:98)ν

Using the asymptotic normality of (cid:99)Zi, which holds by Theorem 5.5, we can approximate (cid:80)
yielding

1(|(cid:99)Zi| ≥ ν) by (pd)G(ν),

i∈H0

for ν < (cid:98)ν. Note that (pd)G(ν)/ max{(cid:80)

1≤j≤pd

1(|(cid:99)Zi ≥ ν), 1} is decreasing in ν. Then by letting ν approach (cid:98)ν, we obtain

Now to prove the reverse bound, we note that the deﬁnition of inﬁmum implies the existence of a sequence νk, where
νk ≥ (cid:98)ν and νk

k→∞−−−−→ (cid:98)ν. Since νk ≥ (cid:98)ν, we have

(cid:80)

i∈H0

1(|(cid:99)Zi| ≥ ν)

max{(cid:80)

1≤j≤pd

1(|(cid:99)Zi ≥ ν), 1}

> α.

(pd)G(ν)

max{(cid:80)

1≤j≤pd

1(|(cid:99)Zi ≥ ν), 1}

> α,

(pd)G((cid:98)ν)

max{(cid:80)

1≤j≤pd

1(|(cid:99)Zi ≥ (cid:98)ν), 1}

≥ α.

(pd)G(νk)

max{(cid:80)

1≤j≤pd

1(|(cid:99)Zi ≥ νk), 1}

≤ α.

(pd)G((cid:98)ν)

max{(cid:80)

1≤j≤pd

1(|(cid:99)Zi ≥ (cid:98)ν), 1}

≤ α.

(pd)G((cid:98)ν)

max{(cid:80)

1≤j≤pd

1(|(cid:99)Zi ≥ (cid:98)ν), 1}

= α,

Thus, by letting νk → (cid:98)ν, we see that

Therefore, by (B.17) and (B.18)

as desired.

(B.17)

(B.18)

C. Proofs of Auxiliary Lemmas in Appendix B

In this section we present the proofs of auxiliary lemmas introduced in Appendix B.

C.1. Proof of Lemma B.1

Here we present a modiﬁed version of the proof for Theorem 7.(b) from Javanmard & Montanari (2014).

Proof of Lemma B.1. Clearly (cid:107)M (cid:101)Σn − I(cid:107)∞ ≤ (cid:107) (cid:101)Σ−1 (cid:101)Σn − I(cid:107)∞. Let X t = (cid:101)Σ−1/2 (cid:102)Xt, where (cid:102)Xt is as deﬁned in Section
3. Now deﬁne Z ∈ Rpd×pd as follows:

Z = (cid:101)Σ−1 (cid:101)Σn − I =

(cid:101)Σ−1 (cid:102)Xt (cid:102)X (cid:62)

t − I

=

1
T − p

T
(cid:88)

(cid:18)

t=p+1

(cid:19)

1
T − p

T
(cid:88)

(cid:18)

t=p+1

(cid:101)Σ−1/2X tX

(cid:62)
t (cid:101)Σ1/2 − I

(cid:19)

.

For any given pair 1 ≤ i, j ≤ pd, denote γ(ij)
j,· , X t(cid:105) − δi,j, where p + 1 ≤ t ≤ T , and
i,·
δi,j represents the Kronecker delta: δi,j = 1(i = j). Let Ft be the ﬁltration Ft = σ(X 1, . . . , X t). Then note that

, X t(cid:105) · (cid:104) (cid:101)Σ1/2

= (cid:104) (cid:101)Σ−1/2

t

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

|Ft−1] = 0, so by Deﬁnition G.1 in Appendix G, γ(ij)

t=p+1 γ(ij)

t

forms a martingale difference sequence. Note further that
. Thus, to bound (cid:107) (cid:101)Σ−1 (cid:101)Σn − I(cid:107)∞, and by transitivity (cid:107)M (cid:101)Σn − I(cid:107)∞, we need only bound

t

E[γ(ij)
t
Zi,j = (T − p)−1 (cid:80)T
Zi,j.

We refer the reader to Deﬁnition G.2 in Appendix G for the deﬁnition of the sub-exponential norm. By Remark 5.18
(Centering) from Vershynin (2012), we can bound the sub-exponential norm of γ(ij)

as follows:

t

(cid:107)γ(ij)
t

(cid:107)ψ1 ≤ 2(cid:107)(cid:104) (cid:101)Σ−1/2

i,·

, X t(cid:105) · (cid:104) (cid:101)Σ1/2

j,· , X t(cid:105)(cid:107)ψ1.

(C.1)

Now note that, as shown by Javanmard & Montanari (2014) we can bound the sub-exponential norm of the product of two
random variables X and Y by:

(cid:107)XY (cid:107)ψ1 ≤ sup
q≥1

(cid:18)

(cid:20)
|XY |q

E

q−1

(cid:21)(cid:19)1/q

(cid:18)

(cid:20)
|X|2q

E

q−1

(cid:21)(cid:19)1/2q(cid:18)

(cid:20)
|Y |2q

E

(cid:21)(cid:19)1/2q

≤ sup
q≥1
(cid:18)

≤ 2

sup
r≥2

≤ 2(cid:107)X(cid:107)ψ2 · (cid:107)Y (cid:107)ψ2.

(cid:18)

(cid:20)
|X|r

E

r−1/2

(cid:21)(cid:19)1/r(cid:19)(cid:18)

(cid:18)

(cid:20)
|Y |r

E

r−1/2

(cid:21)(cid:19)1/r(cid:19)

sup
r≥2

Therefore, by (C.1)

and by assumption

(cid:107)γ(ij)
t

(cid:107)ψ1 ≤ 2(cid:107)(cid:104) (cid:101)Σ−1/2

i,·

, X t(cid:105) · (cid:104) (cid:101)Σ1/2

j,· , X t(cid:105)(cid:107)ψ1 ≤ 2(cid:107)(cid:104) (cid:101)Σ−1/2

i,·

, X t(cid:105)(cid:107)ψ2 · (cid:107)(cid:104) (cid:101)Σ1/2

j,· , X t(cid:105)(cid:107)ψ2,

2(cid:107)(cid:104) (cid:101)Σ−1/2

i,·

, X t(cid:105)(cid:107)ψ2 · (cid:107)(cid:104) (cid:101)Σ1/2

j,· , X t(cid:105)(cid:107)ψ2 ≤ 2(cid:107) (cid:101)Σ−1/2

i,·

(cid:107)ψ2 · (cid:107) (cid:101)Σ1/2

j,· (cid:107)ψ2κ2 ≤ 2κ2

(cid:115)

Cmax
Cmin

.

Thus, if we let κ(cid:48) = 2κ2(cid:112)Cmax/Cmin, then (cid:107)γ(ij)
apply the Bernstein inequality for martingale difference sequences (Lemma F.8 in Appendix F) to obtain:

(cid:107)ψ1 ≤ κ(cid:48). Now, since γ(ij)

t

t

is a martingale difference sequence, we can

(cid:18) 1
P

(cid:12)
(cid:12)
(cid:12)
T − p
(cid:12)

T
(cid:88)

t=p+1

γ(ij)
t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

(cid:20)

≥ (cid:15)

≤ 2 exp

−

T − p
6

min

(cid:18)(cid:18) (cid:15)
eκ(cid:48)

(cid:19)2

(cid:19)(cid:21)

.

,

(cid:15)
eκ(cid:48)

Let (cid:15) = a(cid:112)log(pd)/(T − p), and assume that T − p ≥ (a/(eκ(cid:48)))2 log(pd) so that ((cid:15)/(eκ(cid:48)))2 ≤ ((cid:15)/(eκ(cid:48))) ≤ 1. Then,

(cid:32)

P

1
T − p

(cid:12)
(cid:12)
(cid:12)
(cid:12)

T
(cid:88)

t=p+1

γ(ij)
t

≥ a

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:115)

log(pd)
T − p

(cid:33)

= P

(cid:32)(cid:12)
(cid:12)
Zi,j
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:115)

≥ a

log(pd)
T − p

(cid:33)

≤ 2(pd)−a2/(6e2κ(cid:48)2)
= 2(pd)−(a2Cmin)/(24e2κ4Cmax).

Taking the union over all (pd)2 pairs and letting c2 = (a2Cmin)/(24e2κ4Cmax) − 2 yields the result:



P

(cid:107)M (cid:101)Σn − I(cid:107)∞ ≤ a

(cid:115)

log(pd)
T − p


 ≥ 1 − 2(pd)−c2.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

C.2. Proof of Lemma B.2

The proof of Lemma B.2 relies on two lemmas. First, the following lemma asserts that the restricted eigenvalue condition
(RE condition) holds true for our design matrix (cid:101)X. As we will see in the proof of Lemma B.2 below, the restricted eigenvalue
condition implies the restricted strong convexity condition when the loss function is the least squares loss function. This
property will prove instrumental in bounding (cid:107) (cid:98)θ − θ∗(cid:107)1.
Lemma C.1. Under Assumptions 5.3 and 5.4, we have

with probability at least

inf
(cid:98)θ−θ∗∈Er

1
T − p

(cid:107) (cid:101)X( (cid:98)θ − θ∗)(cid:107)2

2 > 0

1 − 2 exp(−c2

0c2

1c2ω2(B)) − L exp

− 4

(cid:20)

(ω(A))2
α2

(cid:21)
,

2 is the restricted minimum eigenvalue of (cid:101)Σn restricted to A ⊆ Spd−1
where where λmin( (cid:101)Σn|A) = inf u∈A
(the unit sphere in Rpd space), B = {(cid:101)u : (cid:101)u = (cid:101)Σ1/2u/(cid:107) (cid:101)Σ1/2u(cid:107)2, u ∈ A} is the normalized set of A, α = diam(A) =
supu,v∈A d(u, v) = supu,v∈A (cid:107)u − v(cid:107)2, and c0, c1, c2, L > 0 are constants.

(cid:107) (cid:101)Xu(cid:107)2

1
T − p

The RE condition has been studied extensively in the setting where the rows of the design matrix are independent. However,
since the rows of the design matrix are dependent in this setting, we must appeal to martingale theory to prove the RE
condition. We construct a martingale difference sequence equal to 1/(T − p)(cid:107) (cid:101)X( (cid:98)θ − θ∗)(cid:107)2
2, and then bound the minimum
restricted eigenvalue of that sequence using the results from Appendix F. We defer the proof of Lemma C.1 to Appendix
D.1.

Second, the following lemma establishes with high-probability a property of the regularization parameter λ.
Lemma C.2. Denote the least-squares loss function L(θ) = (2(T − p))−1(cid:107)Y − (cid:101)Xθ(cid:107)2
λ = 8Cσ(cid:112)log(pd)/(T − p) for some constant C, then

2. If Assumption 5.4 holds and

holds with probability at least 1 − b1 exp[−b2σ2 log(pd)], for constants b1 and b2.

λ ≥ 2(cid:107)∇L(θ∗)(cid:107)∞,

We defer the proof of Lemma C.2 to Appendix D.2.

We now present a proof of Lemma B.2.

Proof of Lemma B.2. To prove this lemma, we ﬁrst recall the form of the biased Lasso Granger estimator from (3.3):

(cid:98)θ = arg min

θ

1
2(T − p)

(cid:107)Y − (cid:101)Xθ(cid:107)2

2 + λ(cid:107)θ(cid:107)1.

L( (cid:98)θ) + λ(cid:107) (cid:98)θ(cid:107)1 ≤ L(θ∗) + λ(cid:107)θ∗(cid:107)1.

For brevity, we denote the loss function L(θ) = (2(T − p))−1(cid:107)Y − (cid:101)Xθ(cid:107)2
yields the following inequality:

2. We immediately realize that the optimality of (cid:98)θ

To establish a lower bound on L( (cid:98)θ), we will appeal to the restricted strong convexity condition (RSC condition), which
provides a lower bound on the ﬁrst-degree Taylor approximation of L( (cid:98)θ):

δL( (cid:98)θ) := L( (cid:98)θ) − L(θ∗) − (cid:104)∇L(θ∗), (cid:98)θ − θ∗(cid:105) ≥ κ(cid:96)(cid:107) (cid:98)θ − θ∗(cid:107)2

2 > 0,

for some constant κ(cid:48)
we obtain

(cid:96). As noted by Negahban et al. (2012), when L(·) is the least-squares loss function, as it is in our setting,

δL( (cid:98)θ) =

(cid:107) (cid:101)X( (cid:98)θ − θ∗)(cid:107)2

2 ≥ κ(cid:48)

(cid:96)(cid:107) (cid:98)θ − θ∗(cid:107)2
2.

1
T − p

(C.2)

(C.3)

(C.4)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Note that

inf
(cid:98)θ−θ∗∈Er

1
T − p

(cid:107) (cid:101)X( (cid:98)θ − θ∗)(cid:107)2

2 = λmin

(cid:101)X(cid:62) (cid:101)X|Er

= λmin( (cid:101)Σn|Er),

(C.5)

(cid:18) 1

T − p

(cid:19)

where Er is the set the error vector (cid:98)θ − θ∗ can fall in, and λmin( (cid:101)Σn|Er) is the minimum restricted eigenvalue of the
sample covariance matrix. So we see that when L(·) is the least-squares loss function, the restricted strong convexity
condition collapses into the RE condition. The RE condition holds for (cid:101)X with high probability by Lemma C.1. Thus,
λmin( (cid:101)Σn|Er) > 0, and so the RSC condition holds. Then, rearranging (C.3), we see that

L( (cid:98)θ) ≥ L(θ∗) + (cid:104)∇L(θ∗), (cid:98)θ − θ∗(cid:105) +

(cid:107) (cid:98)θ − θ∗(cid:107)2
2,

(C.6)

κ(cid:96)
2

where κ(cid:96) = 2κ(cid:48)
(cid:96).

As a consequence of (C.2) and (C.6), we have

L(θ∗) + (cid:104)∇L(θ∗), (cid:98)θ − θ∗(cid:105) + λ(cid:107) (cid:98)θ(cid:107)1 ≤ L( (cid:98)θ) + λ(cid:107) (cid:98)θ(cid:107)1 ≤ L(θ∗) + λ(cid:107)θ∗(cid:107)1.

Furthermore, since (cid:104)∇L(θ∗), (cid:98)θ − θ∗(cid:105) ≤ (cid:107)∇L(θ∗)(cid:107)∞ · (cid:107) (cid:98)θ − θ∗(cid:107)1 by Holder’s inequality, we achieve the following result:

−(cid:107)∇L(θ∗)(cid:107)∞ · (cid:107) (cid:98)θ − θ∗(cid:107)1 + λ(cid:107) (cid:98)θ(cid:107)1 ≤ λ(cid:107)θ∗(cid:107)1.

(C.7)

We apply Lemma C.2 to establish that λ ≥ 2(cid:107)∇L(θ)(cid:107)∞ with probability at least 1 − b1 exp[−b2σ2 log(pd)], for constants
b1 and b2. Thus, since λ ≥ 2(cid:107)∇L(θ∗)(cid:107)∞ with high probability, (C.7) implies

−

1
λ(cid:107) (cid:98)θ − θ∗(cid:107)1 + λ(cid:107) (cid:98)θ(cid:107)1 ≤ λ(cid:107)θ∗(cid:107)1.
2

Now denote S to be the support of θ∗, so that θ∗ = θ∗

S and θ∗

Sc = 0. Then, based on the previous inequality, we have

−

1
λ(cid:107)( (cid:98)θ − θ∗)S(cid:107)1 −
2

1
2

λ(cid:107)( (cid:98)θ − θ∗)Sc(cid:107)1 + λ(cid:107) (cid:98)θS(cid:107)1 + λ(cid:107) (cid:98)θSc(cid:107)1 ≤ λ(cid:107)θ∗

S(cid:107)1.

Rearranging these terms yields:

−

λ(cid:107)( (cid:98)θ − θ∗)S(cid:107)1 −

1
2

1
λ(cid:107)( (cid:98)θ − θ∗)Sc(cid:107)1 + λ(cid:107)( (cid:98)θ − θ∗)Sc(cid:107)1 ≤ λ(cid:107)θ∗
2

S(cid:107)1 − λ(cid:107) (cid:98)θS(cid:107)1

≤ λ(cid:107)( (cid:98)θ − θ∗)S(cid:107)1,

where the second inequality follows by the triangle inequality. Rearranging terms once more produces the following result:

λ(cid:107)( (cid:98)θ − θ∗)Sc(cid:107)1 ≤ 3λ(cid:107)( (cid:98)θ − θ∗)S(cid:107)1.

(C.8)

We use (C.8) to bound λ(cid:107) (cid:98)θ − θ∗(cid:107)1. Note that (C.2) and (C.6) together imply

−

1
λ(cid:107) (cid:98)θ − θ∗(cid:107)1 + λ(cid:107) (cid:98)θ(cid:107)1 +
2

κ(cid:96)
2

(cid:107) (cid:98)θ − θ∗(cid:107)2

2 ≤ λ(cid:107)θ∗(cid:107)1,

(cid:107) (cid:98)θ − θ∗(cid:107)2

2 ≤ λ(cid:107)θ∗(cid:107)1 − λ(cid:107) (cid:98)θ(cid:107)1 +

λ(cid:107) (cid:98)θ − θ∗(cid:107)1.

1
2

and thus,

Applying the triangle inequality yields

κ(cid:96)
2

κ(cid:96)
2

(cid:107) (cid:98)θ − θ∗(cid:107)2

2 ≤ λ(cid:107) (cid:98)θ − θ∗(cid:107)1 +

1
λ(cid:107) (cid:98)θ − θ∗(cid:107)1
2

=

3
λ(cid:107)( (cid:98)θ − θ∗)(cid:107)1.
2

(C.9)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Note that by (C.8), we have (cid:107) (cid:98)θ − θ∗(cid:107)1 ≤ 4(cid:107)( (cid:98)θ − θ∗)S(cid:107)1. Substituting this result into (C.9) allows us to conclude that

(cid:107) (cid:98)θ − θ∗(cid:107)2

2 ≤ 6λ(cid:107)( (cid:98)θ − θ∗)S(cid:107)1 ≤ 6λ

s0(cid:107)( (cid:98)θ − θ∗)S(cid:107)2 ≤ 6λ

s0(cid:107) (cid:98)θ − θ∗(cid:107)2.

√

√

κ(cid:96)
2

√

Thus, (cid:107) (cid:98)θ − θ∗(cid:107)2 ≤ 12λ

s0/κ(cid:96), which offers us the result of this lemma:

(cid:107) (cid:98)θ − θ∗(cid:107)1 ≤

s0(cid:107) (cid:98)θ − θ∗(cid:107)2 ≤

√

12λs0
κ(cid:96)

.

We note that this result holds with high probability by Lemma C.2.

C.3. Proof of Lemma B.3

Here we verify that the Lindeberg condition (Hall & Heyde, 1980) holds for ζi,t.

Proof of Lemma B.3. Note that for some random variable Q ∼ N (0, 1), C = [((cid:15)t (cid:102)X (cid:62)
positive, ﬁxed constant δ > 0, and t ≥ p:

t mi)2/(σ2[m(cid:62)

i (cid:101)Σnmi])]−1/2, some

E[ζ 2
t

1((cid:107)ζt| ≥ δ)|Ft−1] =

((cid:15)t (cid:102)X (cid:62)

t mi)2E[Q2 1(|Q| > δC)]
.
i (cid:101)Σnmi]

σ2[m(cid:62)

(C.10)

By the properties of the truncated standard normal, we can see that E[Q2|Q > c] = 1 + c(φ(c)/Φ(c)), where φ(c) is the
PDF of the standard normal. Thus,

E[Q2 1(Q > c)] = 2Φ(c)

1 + c

= 2(Φ(c) + cφ(c)) ≤ 2φ(c)

(cid:18)

(cid:19)

φ(c)
Φ(c)

(cid:18) c + c2
c

(cid:19)

.

Now the proceeding inequality follows from the union bound and a standard bound on the normal CDF:

max
p+1≤t≤T

|(cid:15)t (cid:102)X (cid:62)

t mi| > v,

with probability at most 2(T − p) exp[−v2/(2m(cid:62)
following bound:

i (cid:101)Σmi)]. If we let v = 2

log(T − p)m(cid:62)

i (cid:101)Σmi, the we obtain the

(cid:113)

max
p+1≤t≤T

|(cid:15)t (cid:102)X (cid:62)

t mi| ≤ 2

log(T − p)m(cid:62)

i (cid:101)Σmi,

(cid:113)

with probability at least 1 − 2/(T − p). Returning to (C.10), we now see that for D = (cid:112)(T − p)/(4 log(T − p):

E[ζ 2
t

1((cid:107) zetat(cid:107) ≥ δ)|Ft−1] ≤

8 log(T − P )φ(δD)((δD)−1 + δD)
T − p

,

since φ(z)(z−1 + z) is a decreasing function. Therefore, if we sum over all t, we attain

T
(cid:88)

t=p+1

E[ζ 2
t

1((cid:107)ζt| ≥ δ)|Ft−1] ≤ 8 log(T − P )φ(δD)((δD)−1 + δD) → 0,

which demonstrates that the Lindeberg condition does indeed hold for ζt.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

C.4. Proof of Lemma B.6

We present a modiﬁed version of the proof of Lemma 6.1 from Liu et al. (2013b).

Proof of Lemma B.6. Deﬁne ﬁltration Ft = σ(ξ1, . . . , ξt). For 1 ≤≤ p, let:

√

(cid:98)ξt = ξt 1{(cid:107)ξt(cid:107)2 ≤
(cid:101)ξt = ξt − (cid:98)ξt.

n/(log(p))4} − E[ξt 1{(cid:107)ξt(cid:107)2 ≤

n/(log(p))4}|Ft],

√

Note that by Deﬁnition G.1, (cid:98)ξi forms a martingale difference sequence (MDS). Now we have by the triangle inequality

P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

ξt

(cid:13)
(cid:13)
(cid:13)
(cid:13)min

√

(cid:19)

≥ t

n

≤P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

√

√

(cid:98)ξt ≥ t

n −

n/(log(p))2

+ P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

√

(cid:101)ξt ≥

n/(log(p))2

P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

ξt

(cid:13)
(cid:13)
(cid:13)
(cid:13)min

√

(cid:19)

≥ t

n

≥P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

√

√

(cid:98)ξt ≥ t

n +

n/(log(p))2

− P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

√

(cid:101)ξt ≥

n/(log(p))2

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)min
(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)min

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)min
(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)min

,

.

√

n
(cid:88)

t=1

E[ξt 1{(cid:107)ξt(cid:107)2 ≤

n/(log(p))4}|Ft] = o(

n/(log(p))2).

√

√

By our assumption that log(p) = o(

n), we have that,

(C.11)

(C.12)

(C.13)

So by our assumption that E[(cid:107)ξt(cid:107)bpr+2+(cid:15)

2

] ≤ ∞, for r, b, (cid:15) > 0, the previous line implies that:

P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

√

(cid:101)ξt ≥

n/(log(p))2

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)min

≤ nP( max
1≤t≤n

(cid:107)ξt(cid:107) ≥

√

n/(log(p))4) ≤ C(log(p))−3/2(G(t))p,

for 0 ≤ t ≤ b(cid:112)log(p). Thus, by (C.13) and (C.12), we obtain:

P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

ξt

(cid:13)
(cid:13)
(cid:13)
(cid:13)min

√

(cid:19)

≥ t

n

≥P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

√

√

(cid:98)ξt ≥ t

n +

n/(log(p))2

− C(log(p))−3/2(G(t))p,

P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

ξt

(cid:13)
(cid:13)
(cid:13)
(cid:13)min

√

(cid:19)

≥ t

n

≤P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

√

√

(cid:98)ξt ≥ t

n −

n/(log(p))2

+ C(log(p))−3/2(G(t))p,

for constant C > 0 Therefore, it sufﬁces to prove

sup
√

0≤t≤b

log(p)

P((cid:107) (cid:80)n

(cid:12)
(cid:12)
(cid:12)
(cid:12)

t=1 (cid:98)ξt(cid:107)min ≥ (t ± (log(p)−2)
(G(t))p

√

n)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

− 1

≤ C(log(p))−1−γ1

(C.14)

in order to prove this lemma. To prove this line, we appeal to Theorem 1.1 from Zaitsev (1987). The original version of
Theorem 1.1 requires i.i.d. vectors only in order to leverage the Bernstein inequality. However, since in this application (cid:98)ξt
form a MDS, the proof of Theorem 1.1 holds by our Bernstein inequality for MDS (Lemma F.8). In the interest of clarity,

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)min

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)min

and

and

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

since Theorem 1.1 requires introducing a signiﬁcant amount of new material from probability theory, we refer the reader to
Zaitsev (1987) for more details. Thus, by application of Theorem 1.1 from Zaitsev (1987) to (cid:98)ξt, we obtain

≥ (t + (log(p)−2)

n

≤ P((cid:107)W (cid:107)min ≥ t − 2(log(p))−2) + c1,p exp[−c2,d(log(p))2],

P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

(cid:98)ξt

(cid:13)
(cid:13)
(cid:13)
(cid:13)min

P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

(cid:98)ξt

(cid:13)
(cid:13)
(cid:13)
(cid:13)min

√

(cid:19)

√

(cid:19)

≥ (t − (log(p)−2)

n

≤ P((cid:107)W (cid:107)min ≥ t + 2(log(p))−2) − c1,p exp[−c2,d(log(p))2],

where c1,p, c2,p > 0 are constants that depend only on p and W ∼ N

0, Cov

. By our assumption that

(cid:18)

(cid:18)

(cid:80)n

t=1 (cid:98)ξi/

√

n

(cid:19)(cid:19)

(cid:107) Cov(ξi) − Ip×p(cid:107)2 ≤ C(log(p)−2−γ, one can easily show that,

P((cid:107)W (cid:107)min ≥ t − 2(log(p))−2) ≤ (1 + C(log(p))−1−γ1)(G(t))p,

P((cid:107)W (cid:107)min ≥ t + 2(log(p))−2) ≤ (1 − C(log(p))−1−γ1)(G(t))p,
for 0 ≤ t ≤ b(cid:112)log(p). Since for 0 ≤ t ≤ b(cid:112)log(p) we have c1,p exp[−c2,p(log(p))2] ≤ C(log(p))−1−γ1(G(t))p, the
following holds:

≥ (t − (log(p)−2)

n

≤ (1 + C(log(p))−1−γ1)(G(t))p,

(C.15)

and

and

and

P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

(cid:98)ξt

(cid:13)
(cid:13)
(cid:13)
(cid:13)min

P

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

t=1

(cid:98)ξt

(cid:13)
(cid:13)
(cid:13)
(cid:13)min

√

(cid:19)

√

(cid:19)

≥ (t + (log(p)−2)

n

≤ (1 − C(log(p))−1−γ1)(G(t))p,

(C.16)

for 0 ≤ t ≤ b(cid:112)log(p). Therefore, (C.15) and (C.16) imply (C.14), which concludes the proof.

D. Proofs of Supporting Lemmas in Appendix C

In this section we present proofs for the supporting Lemmas of Lemma B.2.

D.1. Proof of Lemma C.1

We present a variation of the proof of Theorem 5 from Johnson et al. (2016). This proof will rely on lower bounding

1
T − p

(cid:98)θ−θ∗∈Er

(cid:107) (cid:101)X( (cid:98)θ − θ∗)(cid:107)2

2 by inf u∈A 1/(T − p) (cid:80)T

inf
We lower bound inf u∈A 1/(T −p) (cid:80)T
µt, u(cid:105) in Lemma D.2 below.
Lemma D.1. Let Zt = (cid:102)Xt − µt, for µt = E[ (cid:102)Xt|Ft−1], where Ft is the ﬁltration Ft = σ( (cid:102)Xp+1, . . . , (cid:102)Xt), and Z =
[Zp+1, . . . , ZT ](cid:62). Furthermore, let E[Z(cid:62)Z/(T − p)] = ΣZ and (cid:107)Σ−1/2

t=p+1(cid:104) (cid:102)Xt −µt, u(cid:105)2 in Lemma D.1 and upper bound supu∈A 2/(T −p) (cid:80)T

t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105)2 − supu∈A 2/(T − p) (cid:80)T

t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105).

t=p+1(cid:104) (cid:102)Xt −

Z Zt(cid:107)ψ2 ≤ κ(cid:48). Then,

(cid:18)

P

1
T − p

inf
u∈A

T
(cid:88)

t=p+1

(cid:104) (cid:102)Xt − µt, u(cid:105)2 ≥ λmin(ΣZ|A)

1 −

≥ 1 − 2 exp(−c2

0c2

1c2ω2(B))

(cid:18)

2c0c1κ(cid:48)2ω(B)
T − p

√

(cid:19)

(cid:19)

> 0

where λmin(Σ|A) = inf u∈A u(cid:62)Σu is the restricted minimum eigenvalue of Σ restricted to A ⊆ Spd−1 (the unit sphere in
Rpd space), and B = {(cid:101)u : (cid:101)u = Σ1/2u/(cid:107)Σ1/2u(cid:107)2, u ∈ A} is the normalized set of A.
The proof of Lemma D.1 relies on the restricted eigenvalue condition for martingale difference sequences from Appendix F.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Lemma D.2. Let µt = E[ (cid:102)Xt|Ft−1], where Ft is the ﬁltration Ft = σ( (cid:102)Xp+1, . . . , (cid:102)Xt), A ⊆ Spd−1 (the unit sphere in
Rpd space), and α = diam(A) = supu,v∈A d(u, v). Then,

(cid:18)

P

2
T − p

sup
u∈A

T
(cid:88)

t=p+1

(cid:104) (cid:102)Xt − µt, u(cid:105) ≤ 0

≥ 1 − L exp

− 4

(cid:19)

(cid:20)

(ω(A))2
α2

(cid:21)
.

The proof of Lemma D.2 employs a generic chaining argument (Talagrand, 2006). We defer the proofs of Lemmas D.1 and
D.2 to Appendicies E.1 and E.2. We now present the proof of Lemma C.1.

Proof of Lemma C.1. We seek to prove that,

From Negahban et al. (2012), we note that the error set Er is actually a cone, and that the magnitude of the error vector
(cid:98)θ − θ∗ in (D.1) does not matter, only the direction does. Thus, we consider set A = Spd−1 ∩ Er and reformulate our
problem as

inf
(cid:98)θ−θ∗∈Er

1
T − p

(cid:107) (cid:101)X( (cid:98)θ − θ∗)(cid:107)2

2 > 0.

1
T − p

inf
u∈A

(cid:107) (cid:101)Xu(cid:107)2

2 > 0.

(D.1)

(D.2)

It sufﬁces to prove (D.2) to prove the result of this lemma.

We now construct a martingale difference sequence that we will bound in order to prove (D.2). Let µt = E[ (cid:102)Xt|Ft−1],
where Ft is the ﬁltration Ft = σ( (cid:102)Xp+1, . . . , (cid:102)Xt), so that by Deﬁnition G.1 in Appendix G, (cid:102)Xt − µt forms a martingale
difference sequence (MDS). Then we have,

1
T − p

(cid:107) (cid:101)Xu(cid:107)2

2 =

T
(cid:88)

(cid:104) (cid:102)Xt, u(cid:105)

( (cid:102)X (cid:62)

t u)2 − 2( (cid:102)X (cid:62)

t u)(µ(cid:62)

t u) + 2( (cid:102)X (cid:62)

t u)(µ(cid:62)

t u) − (µ(cid:62)

t u)2 + (µ(cid:62)

t u)2

(cid:19)

( (cid:102)X (cid:62)

t u − µ(cid:62)

t u)2 − (µ(cid:62)

t u)2 + 2( (cid:102)X (cid:62)

t u)(µ(cid:62)

t u)

(cid:19)

.

Distributing the summation in the last line then yields:

1
T − p

(cid:107) (cid:101)Xu(cid:107)2

2 =

(cid:104) (cid:102)Xt − µt, u(cid:105)2 −

(cid:104)µt, u(cid:105)2 +

(cid:104) (cid:102)Xt, u(cid:105)(cid:104)µt, u(cid:105)

(cid:104) (cid:102)Xt − µt, u(cid:105)2 +

(cid:104)µt, u(cid:105)2 +

(cid:104) (cid:102)Xt − µt, u(cid:105)(cid:104)µt, u(cid:105)

2
T − p

2
T − p

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

1
T − p

1
T − p

2
T − p

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

(cid:104) (cid:102)Xt − µt, u(cid:105)2 +

(cid:104) (cid:102)Xt − µt, u(cid:105)(cid:104)µt, u(cid:105).

Hence,

1
T − p

inf
u∈A

(cid:107) (cid:101)Xu(cid:107)2

2 ≥ inf
u∈A

(cid:104) (cid:102)Xt − µt, u(cid:105)2 + inf
u∈A

(cid:104) (cid:102)Xt − µt, u(cid:105)(cid:104)µt, u(cid:105)

(D.3)

1
T − p

1
T − p

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

2
T − p

2
T − p

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

≥ inf
u∈A

(cid:104) (cid:102)Xt − µt, u(cid:105)2 − sup
u∈A

(cid:104) (cid:102)Xt − µt, u(cid:105),

(D.4)

1
T − p

1
T − p

1
T − p

=

=

t=p+1

T
(cid:88)

(cid:18)

t=p+1

T
(cid:88)

(cid:18)

t=p+1

1
T − p

1
T − p

1
T − p

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

T
(cid:88)

t=p+1

=

≥

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

where the second inequality holds since we can scale the design matrix to fall in the L2 unit ball so that (cid:104)µt, u(cid:105) <= 1.
Thus, to prove (D.2), we must lower bound inf u∈A 1/(T − p) (cid:80)T
t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105)2 and upper bound supu∈A 2/(T −
t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105). We bound inf u∈A 1/(T − p) (cid:80)T
p) (cid:80)T
t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105)2 with Lemma D.1, and supu∈A 2/(T −
p) (cid:80)T
t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105) with Lemma D.2. Thus, by application of Lemmas D.1 and D.2 to (D.3), we have that

1
T − p

inf
u∈A

(cid:107) (cid:101)Xu(cid:107)2

2 ≥ inf
u∈A

1
T − p

T
(cid:88)

t=p+1
(cid:18)

(cid:104) (cid:102)Xt − µt, u(cid:105)2 − sup
u∈A

2c0c1κ(cid:48)2ω(B)
T − p

√

(cid:19)

> 0,

≥ λmin(ΣZ|A)

1 −

2
T − p

T
(cid:88)

t=p+1

(cid:104) (cid:102)Xt − µt, u(cid:105)

1 − 2 exp(−c2

0c2

1c2ω2(B)) − L exp

− 4

(cid:20)

(ω(A))2
α2

(cid:21)
.

Thus, (cid:101)X satisﬁes that restricted eigenvalue condition with high probability.

with probability at least

D.2. Proof of Lemma C.2

Here we prove Lemma C.2.

Proof of Lemma C.2. To prove this lemma, we ﬁrst note that 2(cid:107)∇L(θ∗)(cid:107)∞ = 4
. Let (cid:15) be the error
vector from (3.2), so (cid:15) = Y − (cid:101)Xθ∗. Then Assumption 5.4 implies that (cid:15) (3.2) is sub-Gaussian. Note that since Assumption
5.4 ensures that (cid:107) (cid:102)Xi(cid:107)ψ2 ≤ κ, for i ∈ {1, 2, . . . , T − p}, we can scale the columns of any (cid:102)X so that (cid:107) (cid:101)X·,j(cid:107)2/
T − p ≤ 1,
for 1 ≤ k ≤ pd. Then the sub-Gaussian tails of (cid:15) guarantee that for all t > 0

√

(cid:13)
(cid:13)
(cid:13)(T − p)−1 (cid:101)X(cid:62)(cid:15)

(cid:13)
(cid:13)
(cid:13)∞

with probability at least 1 − 2 exp[−(T − p)t2/(2σ2)]. Bounding over all pd columns yields:

1
T − p

(cid:107)(cid:104) (cid:101)X, (cid:15)(cid:105)(cid:107)2 < t,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T − p

(cid:101)X(cid:62)(cid:15)

< t,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

holds with probability at least 1 − 2 exp[−(T − p)t2/(2σ2) + log(pd)]. Setting t = 2σ(cid:112)log(pd)/(T − p) allows us to
conclude that

λ = 8σ

(cid:115)

log(pd)
T − p

≥ 2(cid:107)∇L(θ∗)(cid:107)∞ = 4

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T − p

(cid:101)X(cid:62)(cid:15)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

,

holds with probability at least 1 − b1 exp[−b2σ2 log(pd)].

E. Proofs of Supporting Lemmas in Appendix D

In this section, we present proofs of the supporting lemmas for Lemma C.1.

E.1. Proof of Lemma D.1
We seek to bound inf u∈A 1/(T − p) (cid:80)T
t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105)2. Since { (cid:102)Xt − µt} is a martingale difference sequence (MDS)
by Deﬁnition G.1 in Appendix G, we will appeal to the restricted eigenvalue condition for MDS that we present in Theorem
F.6 in Appendix F. We reproduce Theorem F.6 here for the convenience of the reader:

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Theorem E.1. Let X = (X1, · · · , Xn)(cid:62) be a n × d design matrix whose anisotropic sub-Gaussian rows form a vector
valued martingale difference sequence. Let E[X(cid:62)X/n] = Σ and (cid:107)Σ−1/2Xi(cid:107)ψ2 ≤ κ. Then for absolute constants
c0, c1, c2 > 0, with probability at least 1 − 2 exp(−c2

1c2ω2(B)), we have

0c2

(cid:18)

λmin(Σ|A)

1 −

(cid:19)

2c0c1κ2ω(B)
√
n

≤ inf
u∈A

1
n

(cid:107)Xu(cid:107)2
2,

where λmin(Σ|A) = inf u∈A u(cid:62)Σu is the restricted minimum eigenvalues of Σ restricted to A ⊆ Sd−1 (the unit sphere in
Rd space), and B = {(cid:101)u : (cid:101)u = Σ1/2u/(cid:107)Σ1/2u(cid:107)2, u ∈ A} is the normalized set of A.

Proof of Lemma D.1. Let Zt = (cid:102)Xt − µt for µt = E[ (cid:102)Xt|Ft−1], where Ft is the ﬁltration Ft = σ( (cid:102)Xp+1, . . . , (cid:102)Xt), as
deﬁned in Appendix D.1. Then let Z = [Zp+1, . . . , ZT ](cid:62). Clearly the rows of Z form a vector-values MDS, and by
Assumption 5.4, they are sub-Gaussian as well. The deﬁnition of (cid:101)X in Section 3 implies that the rows or Z are anisotropic.
Let E[Z(cid:62)Z/(T − p)] = ΣZ be the true covariance matrix of Z, and (cid:107)Σ−1/2
Z Zt(cid:107)ψ2 ≤ κ(cid:48). Then by Theorem F.6 we have
that,

1
T − p

inf
u∈A

T
(cid:88)

t=p+1

(cid:104) (cid:102)Xt − µt, u(cid:105)2 = inf
u∈A

(cid:107)Zu(cid:107)2

2 ≥ λmin(ΣZ|A)

1 −

1
n

(cid:18)

2c0c1κ(cid:48)2ω(B)
T − p

√

(cid:19)
,

with high probability. Thus, to prove Lemma D.1, it sufﬁces to demonstrate that λmin(ΣZ|A), the restricted minimum
eigenvalue of ΣZ, is positive. In this endeavor, we will draw upon the argument made by Johnson et al. (2016) in a similar
context.
Let Bpd
2 (x, (cid:15)) be a L2 ball centered at x with radius (cid:15). Clearly, we can scale the orignial design matrix (cid:101)X so that its rows fall
in a L2 unit ball, in which case the rows of Z fall in a L2 unit ball centered at the origin. So then the set, A = { (cid:102)Xt − µt},
where (cid:102)Xt is drawn from the aforementioned L2 ball, is a subset of the L2 ball centered at the origin. Now note that by
deﬁnition λmin(ΣZ|A) ≥ λmin(ΣZ), where λmin(ΣZ) is the unrestricted minimum eigenvalue of ΣZ. So it sufﬁces to
show that λmin(ΣZ) > 0. By way of contradiction, assume that λmin(ΣZ) = 0. Then let the eigendecomposition of ΣZ
be ΣZ = QΛQ−1, where Q = [v1, . . . , vpd] has the eigenvectors of ΣZ for columns, and Λ = diag(λi)pd
i=1 is a diagonal
matrix of the eigenvalues of ΣZ in descending order. Observe that λpd = 0 implies

since vpd is the eigenvector corresponding to the minimum eigenvalue of 0. Let Avpd = {a ∈ A|(cid:104)a, vpd(cid:105) = 0}. Then
clearly, (E.1) implies,

Ea∼A[(cid:104)a, vpd(cid:105)] = 0,

P(a ∈ Avpd ) = 1.

(E.1)

(E.2)

Note that by (E.2), since there is no probability density outside Avpd , this density is thus concentrated on a subspace of Rpd.
Here we have a contradiction, since the span of Avpd is Rpd. Therefore, λmin(ΣZ|A) ≥ λmin(ΣZ) > 0, and we have

1
T − p

inf
u∈A

T
(cid:88)

t=p+1

(cid:104) (cid:102)Xt − µt, u(cid:105)2 = inf
u∈A

(cid:107)Zu(cid:107)2

2 ≥ λmin(ΣZ|A)

1 −

1
n

(cid:18)

2c0c1κ(cid:48)2ω(B)
T − p

√

(cid:19)

> 0,

with probability at least 1 − 2 exp(−c2

0c2

1c2ω2(B)), as desired.

E.2. Proof of Lemma D.2

In this section we present a proof for Lemma D.2. This proof will make a generic chaining argument, and will thus rely on
the following standard lemmas from Talagrand (2006) and Talagrand (2014).
Lemma E.2 (Theorem 2.1.5 from Talagrand (2006)). Consider two processes {Xt}t∈A and {Yt}t∈A, indexed by the same
set A. Assume {Xt}t∈A is Gaussian, and {Yt}t∈A satisﬁes the condition:

∀δ > 0, ∀u, v ∈ A, P(|Yu − Yv| > δ) ≤ 2 exp

(cid:18) − δ2

d(u, v)2

(cid:19)
,

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

where d(u, v) is the distance metric associated with Xt (we assume d(u, v) = (cid:107)u − v(cid:107)2). Then, for some constant L,

Lemma E.3 (Lemma 1.2.8 from Talagrand (2006)). If the process {Xt}t∈A is symmetric, then

E[ sup
u,v∈A

|Yu − Yv|] ≤ LE[sup
v∈A

Xv]]

E[ sup
u,v∈A

|Xu − Xv|] = 2E[sup
u∈A

Xu]].

Lemma E.4 (Theorem 2.2.27 from Talagrand (2014)). Let {Xt}t∈A be a process that satisﬁes Lemma E.2. Then for any
δ > 0 and constant L,

(cid:18)

P

sup
u,v∈A

|Xu − Xv| ≥ L(γ2(A, d(u, v)) + δα)

≤ L exp(−δ2),

(cid:19)

where α = diam(A) = supu,v∈A d(u, v), and γ2(·, ·) is the γ2-functional deﬁned in F.4.
Lemma E.5 (Theorem 2.2.27 from Talagrand (2014)). For constant L,

1
L

γ2(A, d(u, v)) ≤ E sup
u∈A

Xu ≤ Lγ2(A, d(u, v)).

Having presented these lemmas, we now give the proof of Lemma D.2.

Proof of Lemma D.2. We seek to bound supu∈A

t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105). Recall that by Assumption 5.4, the sub-

Gaussian norm of each row (cid:102)Xt is bounded by κ, which implies that (cid:107) (cid:102)Xt − µt(cid:107)ψ2 ≤ κ. Thus, (cid:102)Xt − µt forms a sub-Gaussian
bounded MDS, and so we can apply the Azuma-Hoeffding inequality to obtain

2
T − p

(cid:80)T

(cid:18)

P

√

(cid:12)
1
(cid:12)
(cid:12)
T − p
(cid:12)

T
(cid:88)

t=p+1

(cid:104) (cid:102)Xt − µt, u(cid:105)

≥ (cid:15)

≤ 2 exp

(cid:20) − (cid:15)2
2(cid:107)u(cid:107)2

2κ2

(cid:21)
,

for any u ∈ A. Then for u, v ∈ A, we have that

(cid:18)

P

√

1
T − p

(cid:12)
(cid:12)
(cid:12)
(cid:12)

T
(cid:88)

t=p+1

(cid:104) (cid:102)Xt − µt, u − v(cid:105)

≥ (cid:15)

≤ 2 exp

(cid:20)

− (cid:15)2

2(cid:107)u − v(cid:107)2

2κ2

(cid:21)

.

(cid:19)

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

We now make a generic chaining argument to bound supu∈A 1/

t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105) with high probability.

√

T − p (cid:80)T

√

Let Z = [( (cid:102)Xp+1−µp+1), . . . , ( (cid:102)XT −µT )](cid:62). We ﬁrst note that by (E.3), the process Zu = (cid:104)Z, u(cid:105) = 1/
T − p(cid:104) (cid:102)Xt−µt, u(cid:105)
has sub-Gaussian concentration. Similarly, by (E.4), Zu − Zv is a sub-Gaussian process ∀u, v ∈ A. We now bound
E[supu∈A 1/
t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105)] in terms of the Gaussian width of set A (see Deﬁnition F.5), and then prove
that supu∈A 1/

T − p (cid:80)T
√

T − p (cid:80)T

To bound E[supu∈A 1/
the Gaussian width of A. Thus, by Lemma E.2 and (E.4), we achieve the following bound for some constant L:

t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105)], we appeal to Lemma E.2. In our setting, E[supv∈A Xv] = ω(A),

t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105) concentrates around its expectation with high probability.
T − p (cid:80)T

√

√

Note that by (E.3), the process Zu is symmetric, and so Lemma E.3 applies. Thus, by Lemma E.3 and (E.5), we have that,

E[ sup
u,v∈A

|Zu − Zv|] ≤ Lκω(A).

E[ sup
u,v∈A

|Zu − Zv|] = 2E[sup
u∈A

Zu]] ≤ Lκω(A).

(E.3)

(E.4)

(E.5)

(E.6)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Thus, the deﬁnition of Zu, we can bound E[supu∈A 2/

√

T − p (cid:80)T

t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105)] as follows:

(cid:20)
2E

2
T − p

sup
u∈A

T
(cid:88)

(cid:21)
(cid:104) (cid:102)Xt − µt, u(cid:105)

= 2E[sup
u∈A

Zu]] ≤ Lκω(A).

t=p+1

√

Having bound the expectation of E[supu∈A 1/
seek to bound supu∈A 1/
to lemma E.4. In this setting, d(u, v) = (cid:107)u − v(cid:107)2. Lemma E.4 motivates the following result:

t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105)] in terms of the Gaussian width of A, we now
t=p+1(cid:104) (cid:102)Xt − µt, u(cid:105) around its expectation with high-probability. To do so, we appeal

T − p (cid:80)T

√

T − p (cid:80)T

P( sup
u,v∈A

|Zu − Zv| ≥ L(γ2(A, d(u, v)) + δα) = P( sup
u,v∈A

|Zu − Zv| ≥ L(γ2(A, d(u, v)) + (cid:15)),

(E.7)

where the right-hand side of the inequality follows since, as Taylor et al. (2014) notes, γ2(A, d(u, v)) ≥ α from the
deﬁnition of the γ2 functional. We now bound γ2(A, d(u, v)) with Lemma E.5. So by Lemma E.5 and (E.5),

P( sup
u,v∈A

|Zu − Zv| ≥ L(γ2(A, d(u, v)) + (cid:15)) ≤ P( sup
u,v∈A

|Zu − Zv| ≥ E[ sup
u,v∈A
|Zu|] + (cid:15)),

|Zu| ≥ 2E[sup
u∈A

= P(sup
u∈A

|Zu − Zv|] + (cid:15))

where the equality follows from Lemma E.3. Now substituting in the deﬁnition of Zu, and applying (E.6) and Lemma E.4,
we have:

(cid:18)

P

√

1
T − p

sup
u∈A

T
(cid:88)

t=p+1

(cid:104) (cid:102)Xt − µt, u(cid:105) ≥ 2Lκω(A) + (cid:15)

≤ L exp

−

(cid:19)

(cid:20)

(cid:18) (cid:15)

(cid:19)2(cid:21)
.

Lκα

T − p, multiplying by 2, and letting (cid:15) = −2Lκαω(A), we achieve the following bound on the

Dividing through by
desired quantity

√

(cid:18)

P

2
T − p

sup
u∈A

T
(cid:88)

t=p+1

(cid:104) (cid:102)Xt − µt, u(cid:105) ≥ 0

≤ L exp

− 4

(cid:19)

(cid:20)

(ω(A))2
α2

(cid:21)
.

(E.8)

F. Restricted Eigenvalue Condition for Martingale Difference Sequences

In this section, we prove that under mild conditions, the restricted eigenvalue condition will hold for martingale difference
sequences (MDS), which we deﬁne in Deﬁnition G.1 in Appendix G. We ﬁrst present the following deﬁnitions:
Deﬁnition F.1. An isotropic design matrix is one for which the covariance matrix of each row Σ = E[XiX T
Deﬁnition F.2. An anisotropic design matrix has rows with a general covariance matrix Σ = E[XiX T
sponding isotropic rows X i = XiΣ−1/2.
Deﬁnition F.3. For a ﬁnite set A ⊂ T , denote the cardinality of A by |A|. An admissible sequence of T is a collection of
subsets of T, {Ts : s ≥ 0}, such that for every s ≥ 1, |Ts| = 22s
Deﬁnition F.4. (Talagrand, 2006) For a metric space (T, d) and k = 1, 2, deﬁne

i ] = I.
i ], but with corre-

and |T0| = 1.

where d(t, Ts) is the distance between the set Ts and t, and the inﬁmum is taken with respect to all admissible sequences of
T . In cases where the metric is clear from the context, we will denote the γk functional by γk(T ).
Deﬁnition F.5. (Gordon, 1988; Chandrasekaran et al., 2012) The Gaussian width of a set A ∈ Rp is

γk(T, d) = inf sup
t∈T

2s/kd(t, Ts),

∞
(cid:88)

s=0

ω(A) = sup
u∈A

E[(cid:104)g, u(cid:105)],

where we take the expectation over random vector g ∼ N (0, Ip×p). The Gaussian width is a measure of the size of set A.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

We now present the restricted eigenvalue condition for MDS.
Theorem F.6. Let X = (X1, · · · , Xn)(cid:62) be a n × d design matrix whose anisotropic sub-Gaussian rows form a vector
valued martingale difference sequence. Let E[X(cid:62)X/n] = Σ and (cid:107)Σ−1/2Xi(cid:107)ψ2 ≤ κ. Then for absolute constants
c0, c1, c2 > 0, with probability at least 1 − 2 exp(−c2

1c2ω2(B)), we have

0c2

(cid:18)

λmin(Σ|A)

1 −

(cid:19)

2c0c1κ2ω(B)
√
n

≤ inf
u∈A

1
n

(cid:107)Xu(cid:107)2
2,

where λmin(Σ|A) = inf u∈A u(cid:62)Σu is the restricted minimum eigenvalues of Σ restricted to A ⊆ Sd−1 (the unit sphere in
Rd space), and B = {(cid:101)u : (cid:101)u = Σ1/2u/(cid:107)Σ1/2u(cid:107)2, u ∈ A} is the normalized set of A.
In the proof of this theorem, we will use the following lemma, which is a MDS version of the sub-Gaussian concentration in
Mendelson et al. (2007).
Lemma F.7. (Mendelson et al., 2007) Let (Ω, µ) be a probability space, and F ⊂ SL2 be a set of functions, where
SL2 := {f : (cid:107)f (cid:107)L2 = 1} is the unit sphere in L2(µ) space. Assume that diam(F, (cid:107) · (cid:107)ψ2) = α. Then, for any θ > 0 and
n ≥ 1 satisfying

we have with probability at least 1 − exp(−c2nθ2/α4) that

c1αγ2(F, (cid:107) · (cid:107)ψ2 ) ≤ θ

n,

√

(cid:12)
(cid:12)
(cid:12)

1
n

k
(cid:88)

i=1

sup
f ∈F

f 2(Xi) − E[f 2]

(cid:12)
(cid:12)
(cid:12) ≤ θ,

where c1, c2 are absolute constants.

The detailed proof of Lemma F.7 can be found in Mendelson et al. (2007). We outline the proof of this lemma in Appendix
F.1.

Proof of Theorem F.6. We will apply Lemma F.7 to this proof. First, we deﬁne the following class of functions:

(cid:26)

F =

fu : u ∈ A, fu(·) =

√

(cid:27)

(cid:104)·, u(cid:105)

.

1
u(cid:62)Σu

(F.1)

We need to verify that F ⊂ SL2. In fact, for any fu ∈ F , we have

(cid:107)fu(X)(cid:107)L2 =

EX [(cid:104)X, u(cid:105)2] =

EX [u(cid:62)X(cid:62)Xu] = 1.

1
u(cid:62)Σu

1
u(cid:62)Σu

Note that

diam(F, (cid:107) · (cid:107)ψ2) = sup

(cid:107)fu − fv(cid:107)ψ2 ≤ 2 sup
fu∈F

(cid:107)fu(cid:107)ψ2 .

fu,fv∈F

In order to bound the diameter of F according to (cid:107) · (cid:107)ψ2, we only need to get a bound on the following term

sup
fu∈F

(cid:107)fu(cid:107)ψ2 = sup
u∈A

(cid:13)
(cid:13)
(cid:13)
(cid:13)

√

1
u(cid:62)Σu

(cid:104)X, u(cid:105)

(cid:13)
(cid:13)
(cid:13)
(cid:13)ψ2

(cid:28)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

= sup
u∈A

Σ−1/2X,

Σ1/2u
(cid:107)Σ1/2u(cid:107)2

(cid:29)(cid:13)
(cid:13)
(cid:13)
(cid:13)ψ2

.

Thus, we have supfu∈F (cid:107)fu(cid:107)ψ2 ≤ (cid:107)Σ−1/2X |ψ2 ≤ κ. By similar argument, we have

(cid:107)fu − fv(cid:107)ψ2 =

Σ−1/2X,

(cid:28)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Σ1/2u
(cid:107)Σ1/2u(cid:107)2

−

Σ1/2v
(cid:107)Σ1/2v(cid:107)2

(cid:29)(cid:13)
(cid:13)
(cid:13)
(cid:13)ψ2

≤ κ

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Σ1/2u
(cid:107)Σ1/2u(cid:107)2

−

Σ1/2v
(cid:107)Σ1/2v(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

.

By deﬁnition, we also have

(cid:107)fu − fv(cid:107)L2 = E

Σ−1/2X,

(cid:20)(cid:28)

Σ1/2u
(cid:107)Σ1/2u(cid:107)2

−

Σ1/2v
(cid:107)Σ1/2v(cid:107)2

(cid:29)2(cid:21)

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Σ1/2u
(cid:107)Σ1/2u(cid:107)2

−

Σ1/2v
(cid:107)Σ1/2v(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

This equality immediately implies (cid:107)fu − fv(cid:107)ψ2 ≤ κ(cid:107)fu − fv(cid:107)L2. Then the γ2-functional in Lemma F.7 can be bounded as

γ2(F ∩ SL2 , (cid:107) · (cid:107)ψ2 ) ≤ κγ2(F ∩ SL2 , (cid:107) · (cid:107)L2 ) ≤ κc0ω(B),

where the last inequality is due to the majorizing measure theorem in Talagrand (2006), B := {(cid:101)u : (cid:101)u =
Σ1/2u/(cid:107)Σ1/2u(cid:107)2, u ∈ A} is the normalized set of A and c0 > 0 is an absolute constant. Now we choose the parameter θ
in Theorem F.7 as

Therefore, we have with probability at least 1 − exp(−c2

0c2

1c2ω(B)2/4) that

θ =

2c0c1κ2ω(B)
√
n

≥

2c1κγ2(F, (cid:107) · (cid:107)ψ2)
√
n

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

1
u(cid:62)Σu

sup
u∈A

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:104)Xi, u(cid:105)2 − 1
(cid:12)
(cid:12)

≤

2c0c1κ2ω(B)
√
n

,

(cid:18)

1 −

1
n

1
u(cid:62)Σu

(cid:19)

(cid:107)Xu(cid:107)2
2

≤

2c0c1κ2ω(B)
√
n

,

sup
u∈A

where c0, c1, c2 are absolute constants and B := {(cid:101)u : (cid:101)u = Σ1/2u/(cid:107)Σ1/2u(cid:107)2, u ∈ A}. It follows that

and

Thus we obtain that

1 −

2c0c1κ2ω(B)
√
n

≤ inf
u∈A

1
n

1
u(cid:62)Σu

(cid:107)Xu(cid:107)2

2 ≤

1
λmin(Σ|A)

inf
u∈A

1
n

(cid:107)Xu(cid:107)2
2.

(cid:18)

λmin(Σ|A)

1 −

(cid:19)

2c0c1κ2ω(B)
√
n

≤ inf
u∈A

1
n

(cid:107)Xu(cid:107)2
2

holds with probability at least 1 − exp(−c2

0c2

1c2ω(B)2/4), with c0, c1, c2 being absolute constants.

F.1. Sketch of Proof of Lemma F.7

Here we lay the outline of the proof for Lemma F.7, and show that it can be extended to bounded martingale difference
sequence. The only difference in the proof for our MDS version of this lemma from the independent case is the Bernstein
inequality. Whereas the original result leveraged the canonical Bernstein inequality, we here use the following MDS version
of the Bernstein inequality:

Lemma F.8 (Bernstein-Type Inequality for Martingale Difference Sequences). Let X1, . . . , Xn form a sub-exponential
Martingale Difference Sequence (MDS) such that max1≤i≤n (cid:107)Xi(cid:107)ψ1 ≤ κ. Here (cid:107) · (cid:107)ψ1 is the sub-Exponential norm deﬁned
in Deﬁnition G.2 in Appendix G. Then

P

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

(cid:20)

aiXi

≥ t

≤ 2 exp

− C min

(cid:26) t2

κ2(cid:107)a(cid:107)2,

(cid:27)(cid:21)
,

t
κ(cid:107)a(cid:107)∞

where C is a constant.

We defer the proof of Lemma F.8 to Appendix F.2. With the Bernstein inequality for MDS, we can now outline the proof of
Lemma F.7.

Let X1, . . . , Xn be a bounded martingale difference sequence. We ﬁrst deﬁne the empircal processes

Zf = 1/n

f 2(Xi) − E[f 2]

Wf =

1/n

(cid:18)

f 2(Xi)

(cid:19)2

.

n
(cid:88)

i=1

n
(cid:88)

i=1

The following lemma from Mendelson et al. (2007) can be easily obtained from Lemma F.8.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Lemma F.9. There exists an absolute constant c1 > 0 for which the following holds. Let F ⊂ SL2 , α = diam(F, (cid:107) · (cid:107)ψ2 )
and set n ≥ 1. For every f, g ∈ F and every u ≥ 2 we have

Also, for every u > 0,

and

P(Wf −g ≥ u(cid:107)f − g(cid:107)ψ2 ) ≤ exp(−c1nu2).

P(|Zf − Zg| ≥ uα(cid:107)f − g(cid:107)ψ2) ≤ exp(−c1nu2),

P(|Zf | ≥ uα2) ≤ 2 exp(−c1nu2).

The following two lemmas from Mendelson et al. (2007) hold in our setting since they do not require i.i.d. observations.
Lemma F.10. There exists an absolute constant C for which the following holds. Let F ⊂ SL2 , α = diam(F, (cid:107) · (cid:107)ψ2) and
n ≥ 1. There is F (cid:48) ⊂ F such that |F (cid:48)| ≤ 4n and with probability at least 1 − exp(−n), we have, for every f ∈ F ,

Wf −πF (cid:48) (f ) ≤

Cγ2(F, (cid:107) · (cid:107)ψ2 )
√
,
n

where πF (cid:48)(f ) is a nearest point to f in F (cid:48) with respect to the ψ2 metric.
Lemma F.11. There exist absolute constants C and c(cid:48) > 0 for which the following holds. Let F ⊂ SL2 and α =
diam(F, (cid:107) · (cid:107)ψ2). Let n ≥ 1 and F (cid:48) ⊂ F such that |F (cid:48)| ≤ 4n. Then for every w > 0,

with probability at least 1 − 3 exp(−c(cid:48)n min(w, w2)).

|Zf | ≤ Cα

sup
f ∈F (cid:48)

γ2(F, (cid:107) · (cid:107)ψ2)
√
n

+ α2w,

Based on the above lemmas, the rest of proof of Theorem F.7 is stated as the proof of Theorem 1.4 in Mendelson et al.
(2007).

F.2. Proof of Berstein Inequality for MDS (Lemma F.8)

We ﬁrst present the following lemma from Vershynin (2012).
Lemma F.12 (Lemma 5.15 from Vershynin (2012)). If X is a sub-exponential random variable such that E[X] = 0, then
for every t such that |t| ≤ c/(cid:107)X(cid:107)ψ1, we have

E[exp(tX)] ≤ exp(Ct2(cid:107)X(cid:107)ψ1],

where C, c > 0 are constants.

We now prove the Berstein Inequality for MDS.

Proof of Lemma F.8. We begin by bounding the moment-generating function of (cid:80)n

i aiXi as follows:

(cid:20)

E

(cid:18)

n
(cid:88)

exp

t ·

(cid:19)(cid:21)

(cid:20) n
(cid:89)

(cid:21)
exp[taiXi]

= E

aiXi

i

i
(cid:20)

(cid:20)
= E

E

exp[tanXn]

exp[taiXi]

n−1
(cid:89)

i

(cid:12)
(cid:12)
X1, . . . , Xn−1
(cid:12)
(cid:12)

(cid:21)(cid:21)

(cid:20)

(cid:20)
= E

E

(cid:12)
(cid:12)
exp[tanXn]
(cid:12)
(cid:12)

X1, . . . , Xn−1

(cid:21)

· E

(cid:20) n−1
(cid:89)

(cid:12)
(cid:12)
exp[taiXi]
(cid:12)
(cid:12)

i

(cid:21)(cid:21)

X1, . . . , Xn−1

(cid:20)
≤ E

exp(Ct2a2

nκ2) · E

(cid:20) n−1
(cid:89)

i

exp[taiXi]

X1, . . . , Xn−1

,

(cid:21)(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

where the second inequality holds by the Law of Iterated Expectations. Furthermore, the last inequality holds by Lemma
F.12 since E[Xn|X1, . . . , Xn−1] = E[Xn|Fn−1] = 0 by the deﬁnition of a MDS (recall Deﬁnition G.1 in Appendix G). By
iteratively repeating this process, we obtain

(F.2)

(F.3)

(F.4)

(cid:20)

E

(cid:18)

n
(cid:88)

exp

t ·

(cid:19)(cid:21)

(cid:20) n
(cid:89)

≤ E

aiXi

(cid:21)

exp(Ct2a2

i κ2)

i

= exp

Ct2

i
(cid:18)

(cid:18)

(cid:19)

i κ2
a2

n
(cid:88)

i

(cid:19)
.

= exp

Ct2(cid:107)a(cid:107)2

2κ2

Now note that by the Chernoff bound, for all λ such that |λ| ≤ C/(cid:107)a(cid:107)∞ we have

(cid:18) n
(cid:88)

P

i

(cid:19)

(cid:18)

(cid:18)

n
(cid:88)

(cid:19)

aiXi ≥ t

= P

exp

λ

aiXi

≥ exp(λt)

(cid:19)

i
i aiXi]]

E[exp[λ (cid:80)n
exp[λt]
exp[Cλ2(cid:107)a(cid:107)2

2κ2]

≤

≤

exp[λt]
= exp[Cλ2(cid:107)a(cid:107)2

2κ2 − λt],

where the last inequality holds by F.2. Now if we let λ = min{t/(2C(cid:107)a(cid:107)2
t/(2C(cid:107)a(cid:107)2

2κ2) < c/((cid:107)a(cid:107)∞κ), then

2κ2), c/((cid:107)a(cid:107)∞κ)}, then we see that if

(cid:18) n
(cid:88)

P

i

(cid:19)

aiXi ≥ t

≤ exp

(cid:20)

t2
4C(cid:107)a(cid:107)2

2κ2 −

(cid:21)

t2
2κ2C(cid:107)a(cid:107)2
2

= exp

(cid:20) − t2
4C(cid:107)a(cid:107)2

2κ2

(cid:21)
.

Similarly, if c/((cid:107)a(cid:107)∞κ) < t/(2C(cid:107)a(cid:107)2

2κ2), then

(cid:18) n
(cid:88)

P

i

(cid:19)

aiXi ≥ t

≤ exp

(cid:20) Cc(cid:107)a(cid:107)2
2κ
(cid:107)a(cid:107)∞

·

c
(cid:107)a(cid:107)∞κ

−

(cid:21)

ct
(cid:107)a(cid:107)∞κ

(cid:20)

≤ exp

−

ct
2(cid:107)a(cid:107)∞κ

(cid:21)
,

where the second inequality holds since Cc(cid:107)a(cid:107)2

2κ/(cid:107)a(cid:107)∞ ≤ t/2. Combining F.3 and F.4 yields

(cid:18) n
(cid:88)

P

i

(cid:19)

(cid:18)

aiXi ≥ t

≤ exp

min

(cid:26) − t2
4C(cid:107)a(cid:107)2

2κ2,

− ct
2(cid:107)a(cid:107)∞κ

(cid:27)(cid:19)

.

Note that we can repeat this process and replace each Xi with −Xi to obtain this same bound for P(− (cid:80)n
lemma then follows as a result of these two bounds.

i aiXi ≥ t). This

In this section we present deﬁnitions used in the Appendix sections.
Deﬁnition G.1. A stochastic process {ξt} is a martingale difference sequence with respect to ﬁltration Ft if:

G. Auxiliary Deﬁnitions

1. ξt is Ft-measurable, and

2. E[ξt|Ft−1] = 0.

Deﬁnition G.2. The sub-Exponential norm of a random scalar variable X , (cid:107)X(cid:107)ψ1, is:

(cid:107)X(cid:107)ψ1 = sup
q≥1

(cid:18)

(cid:20)
|X|q

E

q−1

(cid:21)(cid:19)1/q

.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

The sub-Exponential norm of a random vector X ∈ Rn is:

(cid:107)X(cid:107)ψ1 = sup

(cid:107)(cid:104)X, u(cid:105)(cid:107)ψ1 ,

u∈Sn−1

where Sn−1 is the unit sphere in Rn space.

