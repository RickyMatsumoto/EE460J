Conditional Accelerated Lazy Stochastic Gradient Descent

Guanghui Lan * 1 Sebastian Pokutta * 1 Yi Zhou * 1 Daniel Zink * 1

Abstract

In this work we introduce a conditional acceler-
ated lazy stochastic gradient descent algorithm
with optimal number of calls to a stochastic ﬁrst-
order oracle and convergence rate O( 1
ε2 ) improv-
ing over the projection-free, Online Frank-Wolfe
based stochastic gradient descent of (Hazan and
Kale, 2012) with convergence rate O( 1

ε4 ).

1. Introduction

The conditional gradient method (also known as: Frank-
Wolfe algorithm) proposed in (Frank and Wolfe, 1956),
gained much popularity in recent years due to its simple
projection-free scheme and fast practical convergence rates.
We consider the basic convex programming (CP) problem

f ∗ := min
x∈X

f (x),

(1)

∃
y

where X
smooth convex function such that

Rn is a closed convex set and f : X

L > 0,

⊆

→

R is a

f (cid:48)(y)

f (cid:48)(x)

,

(cid:107)

(cid:107)

∀

L

−

≤

−

X.

∗
(cid:107)

x, y

x
(cid:107)
The classic conditional gradient (CG) method solves (1) iter-
atively by minimizing a series of linear approximations of f
over the feasible set X. More speciﬁcally, given xk−1
X
at the k-th iteration, it updates xk according to the following
steps:

(2)

∈

∈

1) Call

the ﬁrst-order

(FO) oracle

to compute

(f (xk−1), f (cid:48)(xk−1)) and set pk = f (cid:48)(xk−1).

2) Call the linear optimization (LO) oracle to compute

yk

argminx∈X (cid:104)

.
pk, x
(cid:105)

∈

(3)

3) Set xk = (1

λk)xk−1 + λkyk for some λk

[0, 1].

−

∈

*Equal contribution

1ISyE, Georgia Institute of Tech-
nology, Atlanta, GA. Correspondence to: Daniel Zink
<daniel.zink@gatech.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

Compared to most other ﬁrst-order methods, such as e.g.,
gradient descent algorithms and accelerated gradient algo-
rithms (Nesterov, 1983; 2004), the CG method is compu-
tationally cheaper in some cases, since it only requires the
solution of a linear optimization subproblem (3) rather than
an often costly projection onto the feasible region X.

There has been extensive and fruitful research on the gen-
eral class of linear-optimization-based convex program-
ming (LCP) methods (which covers the CG method and
its variants) and their application in machine learning (e.g.,
(Ahipasaoglu and Todd, 2013; Bach et al., 2012; Beck and
Teboulle, 2004; Cox et al., 2013; Clarkson, 2010; Freund
and Grigas, 2013; Hazan, 2008; Harchaoui et al., 2012;
Jaggi, 2011; 2013; Jaggi and Sulovský, 2010; Luss and
Teboulle, 2013; Shen et al., 2012; Hazan and Kale, 2012;
Lan, 2013; Lan and Zhou, 2014; Braun et al., 2016)). It
should be noted that even the computational cost for LO
oracle to solve the linear optimization subproblem (3) is
high for some complex feasible regions. Recently, several
approaches have been considered to address this issue. Jaggi
demonstrated practical speed up for the CG method by ap-
proximately solving (3) in (Jaggi, 2013). Braun, Pokutta,
and Zink in (Braun et al., 2016) proposed a class of mod-
iﬁed CG methods, namely the lazy conditional gradient
(LCG) algorithms, which calls a weak separation oracle
rather than solving the linear subproblem (3) in the classical
CG method. In fact, the weak separation oracle is com-
putationally more efﬁcient than approximate minimization
used in (Jaggi, 2013), at the expense of not providing any
guarantee for function value improvement with respect to
(3). Furthermore, as shown in (Jaggi, 2013; Lan, 2013), the
total number of iterations for the LCP methods to ﬁnd an
X, s.t. f (¯x)
(cid:15)-solution of (1) (i.e., a point ¯x
(cid:15))
(1/(cid:15)), which is not improvable
cannot be smaller than
even when the objective function f is strongly convex.
Improved complexity results can only be obtained under
stronger assumptions on the LO oracle or the feasible set
(see, e.g., (Garber and Hazan, 2013; Lan, 2013)). However,
(1/(cid:15)) bound does not preclude the existence of more
the
efﬁcient LCP algorithms for solving (1). Lan and Zhou in
(Lan and Zhou, 2014) proposed a class of conditional gra-
dient sliding methods (CGS), which signiﬁcantly improve
the complexity bounds in terms of the number of gradient
evaluations while maintaining optimal complexity bounds

f ∗

O

O

−

≤

∈

Conditional Accelerated Lazy Stochastic Gradient Descent

for the LO oracle calls required by the LCP methods.

Contributions

Inspired by (Braun et al., 2016) and (Lan and Zhou, 2014),
in this paper we focus on a class of modiﬁed LCP methods
that require only improving solutions for a certain sepa-
ration problem rather than solving the linear optimization
subproblem (3) explicitly through LO oracle calls while
simultaneously minimizing the number of gradient evalua-
tions when performing weak separation over the feasible set
X. At ﬁrst these two objectives seem to be incompatible as
(Braun et al., 2016) give up the dual guarantee to simplify
the oracle, while the dual guarantee of CG iterations is at the
core of the analysis in (Lan and Zhou, 2014). We overcome
this impasse by carefully modifying both techniques.

O

O

) = F (
·

It should be mentioned that Hazan and Kale in (Hazan
and Kale, 2012) proposed the online Frank-Wolfe (OFW)
(1/(cid:15)4) rate of convergence for
algorithm, which obtains
stochastic problems. Indeed, if we consider the objective
function f (x) := E[F (x, ξ)] for stochastic optimization,
the OFW method can be applied to solve (1) by viewing the
iteratively observed function ft as the current realization of
, ξt). Without
the true objective function f , i.e., ft(
·
re-evaluating the (sub)gradients at the updated points, the
(T −1/4) bound for any (smooth or non-
OFW obtains
smooth) objective functions (see Theorem 4.4 in (Hazan and
(1/(cid:15)4) rate of convergence
Kale, 2012)), which implies
O
in terms of the number of (sub)gradient evaluations for
stochastic optimization. However, we can show that our
(1/(cid:15)2)) rate
proposed algorithm obtains
O
of convergence for smooth (resp., non-smooth) stochastic
problems, which is much better than the convergence rate of
the OFW method. We would like to stress that the stochastic
optimization bound in (Hazan and Kale, 2012, Theorem 4.1)
(1/(cid:15)2), requires to re-evaluate
which gives a guarantee of
all gradients at the current iterate and as such the number
of gradient evaluations required grows quadratically in t.
Moreover, Hazan and Luo (2016) proposed two methods for
solving the special case of Problem (1) of the form

(1/(cid:15)) (resp.,

O

O

min
x∈X

f (x) = min
x∈X

1
m

m
(cid:88)

i=1

fi(x),

O

which allows for a potentially smaller number of SFO eval-
(1/ε2), the lower bound for the general
uations than
problem. The two methods Stochastic Variance-Reduced
Frank-Wolfe (SVRF) and Stochastic Variance-Reduced Con-
ditional Gradient Sliding (STORC) are obtained by applying
the variance reduction idea of Johnson and Zhang (2013)
and Mahdavi et al. (2013) to the CG method and the Stochas-
tic CGS method respectively. Both algorithms however need
a certain number of exact (or full) gradient evaluations lead-
ing to a potentially undesirable dependence on the number
of examples m.

Our main contributions can be brieﬂy summarized as fol-
lows. We consider stochastic smooth optimization, where
we have only access unbiased estimators of the gradients of
f via a stochastic ﬁrst-order (SFO) oracle. By incorporat-
ing a modiﬁed LCG procedure (Braun et al., 2016) into a
modiﬁed CGS method (Lan and Zhou, 2014) we obtain a
new conditional accelerated lazy stochastic gradient descent
algorithm (CALSGD) and we show that the number of calls
to the weak separation oracle can be optimally bounded by
(1/(cid:15)2) on the total
O
number of calls to the SFO oracle can be maintained. In
addition, if the exact gradients of f can be accessed by a
FO oracle, the latter bound can be signiﬁcantly improved to
(1/√(cid:15)). In order to achieve the above we will present a
O
modiﬁed lazy conditional gradient method, and show that
the total number of iterations (or calls to the weak separation
(1/(cid:15)) under
oracle) performed by it can be bounded by
a stronger termination criterion, i.e., the primal-dual gap
function.

(1/(cid:15)), while the optimal bound of

O

O

O

O

(log 1/(cid:15)) (resp.,

We also consider strongly convex and smooth functions and
show that without enforcing any stronger assumptions on the
weak separation oracle or the feasible set X, the total num-
ber of calls to the FO (resp., SFO) oracle can be optimally
(1/(cid:15))) for variants of the
bounded by
proposed method to solve deterministic (resp., stochastic)
strongly convex and smooth problems. Furthermore, we
also generalize the proposed algorithms to solve an impor-
tant class of non-smooth convex programming problems
with a saddle point structure. By adaptively approximat-
ing the original non-smooth problem via a class of smooth
functions, we are able to show that the deterministic version
(1/(cid:15)) num-
of CALSGD can obtain an (cid:15)-solution within
(1/(cid:15)2) number of
ber of linear operator evaluations and
calls to the weak separation oracle, respectively. The former
(1/(cid:15)2) for non-smooth stochastic
bound will increase to
optimization.

O

O

O

Finally, we demonstrate practical speed ups of CALSGD
through preliminary numerical experiments for the video
co-localization problem, the structured regression problem
and quadratic optimization over the standard spectrahedron;
an extensive study is beyond the scope of this paper and
left for future work. In all cases we report a substantial
improvements in performance.

In the main body of the paper we focus on the stochastic
smooth case; several other results and their proofs have been
relegated to the Supplementary Material.

1.1. Notation and Terminology

Rn be a convex compact set, and

Let X
X be the
norm associated with the inner product in Rn. For the sake

(cid:107) · (cid:107)

⊆

Conditional Accelerated Lazy Stochastic Gradient Descent

of simplicity, we often skip the subscript in the norm
We deﬁne the diameter of the set X as

X .

(cid:107) · (cid:107)

DX

DX,(cid:107)·(cid:107) := max

x

x,y∈X (cid:107)

−

y

.
(cid:107)

≡

(4)

(cid:107) · (cid:107)

, we denote its conjugate by

. For a linear operator A : Rn
s, x
(cid:105)

→
to denote its operator norm deﬁned as

For a given norm
max(cid:107)x(cid:107)≤1(cid:104)
A
use
(cid:107)
(cid:107)
max(cid:107)x(cid:107)≤1 (cid:107)
denote its linear approximation at x by

s
∗ =
(cid:107)
(cid:107)
Rm, we
A
:=
(cid:107)
R be a convex function, we

. Let f : X

Ax

→

(cid:107)

(cid:107)

lf (x; y) := f (x) +

f (cid:48)(x), y
(cid:104)

x

.

(cid:105)

−

(5)

Clearly, if f satisﬁes (2), then

≤

f (y)

lf (x; y) + L
2 (cid:107)
Notice that the constant L in (2) and (6) depends on
.
(cid:107) · (cid:107)
Moreover, we say f is smooth with curvature at most C, if

2,
(cid:107)

x, y

(6)

X.

−

∈

x

∀

y

f (y)

lf (x; y) + C
2 ,

x, y

X.

(7)

≤

∈
It is clear that if X is bounded, we have C
X . In
the following we also use R++ to denote the set of strictly
positive reals.

LD2

≤

∀

2. Conditional Accelerated Lazy Stochastic

Gradient Descent

We now present a new method for stochastic gradient de-
scent that is based on the stochastic conditional gradient
sliding (SCGS) method and the parameter-free lazy condi-
tional gradient (LCG) procedure from Section 2.2, which
we refer to as the Conditional Accelerated Lazy Stochastic
Gradient Descent (CALSGD) method.

We consider the stochastic optimization problem:

f ∗ := min
x∈X{

f (x) = Eξ[F (x, ξ)]
,
}

(8)

where f (x) is a smooth convex function satisfying (2).

2.1. The Algorithm

Throughout this section, we assume that there exists a
stochastic ﬁrst-order (SFO) oracle, which for a search point
X outputs a stochastic gradient F (cid:48)(zk, ξk), s.t.
zk

∈

E [F (cid:48)(zk, ξk)] = f (cid:48)(zk),
σ2.
f (cid:48)(zk)
(cid:107)

≤

2
∗

(cid:3)

E (cid:2)
F (cid:48)(zk, ξk)
(cid:107)

−
If σ = 0, the stochastic gradient F (cid:48)(zk, ξk) is the exact
gradient at point zk, i.e., F (cid:48)(zk, ξk) = f (cid:48)(zk).

Our algorithmic framework is inspired by the SCGS method
by (Lan and Zhou, 2014). However, instead of applying
the classic CG method to solve the projection subproblem

appearing in the accelerated gradient (AG) method, the
CALSGD method utilizes a modiﬁed parameter-free LCG
algorithm (see Section 2.2) to approximately solve the sub-
problem ψ(x) deﬁned in (16) and skips the computations
of the stochastic gradient F (cid:48)(z, ξ) from time to time when
performing weak separation over the feasible region X. The
main advantages of our method are that it does not solve
a traditional projection problem and achieves the optimal
bounds on the number of calls to the SFO and LOsepX
oracles (see Oracle 1 in Subsection 2.2) for solving problem
(1)-(8). To the authors’ best knowledge, no such algorithms
have been developed before in the literature; we present the
algorithm below in Algorithm 1.

Algorithm 1 Conditional Accelerated Lazy Stochastic Gra-
dient Descent (CALSGD)
Input: Initial point x0
separation oracle accuracy α
R++, γk
Let βk
be given and set y0 = x0.
for k = 1, 2, . . . , N do

X, iteration limit N , and weak

1.
[0, 1], and ηk

R+, k = 1, 2, . . .,

≥

∈

∈

∈

∈

−
(cid:80)Bk

γk)yk−1 + γkxk−1,
j=1F (cid:48)(zk, ξk,j),

zk = (1
gk = 1
Bk
xk = LCG(gk, βk, xk−1, α, ηk),
yk = (1

γk)yk−1 + γkxk,

−

(11)

(12)

(13)

(14)

where F (cid:48)(zk, ξk,j), j = 1, . . . , Bk, are stochastic gradi-
ents computed by the SFO at zk.
end for
Output: yN .

We hasten to make some observations about the CALSGD
method. Firstly, we apply mini-batches to estimate the
gradient at point zk, where the parameter
denotes the
batch sizes used to compute gk. It can be easily seen from
(9), (10), and (12) that

Bk
{

}

E[gk

f (cid:48)(zk)] = 0 and E[

−

gk
(cid:107)

−

f (cid:48)(zk)

2
∗]
(cid:107)

≤

σ2
Bk

, (15)

and hence gk is an unbiased estimator of f (cid:48)(zk). In fact,
letting SBk = (cid:80)Bk
f (cid:48)(zk)), from (9) and
(10), by induction, we have

j=1(F (cid:48)(zk, ξk,j)

−

(9)

(10)

E (cid:2)

SBk (cid:107)
(cid:107)

2
∗

(cid:3)

f (cid:48)(zk)
2
∗
(cid:107)
2
f (cid:48)(zk)
∗
−
(cid:107)
f (cid:48)(zk)
]
(cid:105)

−
F (cid:48)(zk, ξk,Bk )

2
∗ +
(cid:107)

(cid:3) = E (cid:2)
SBk−1 + F (cid:48)(zk, ξk,Bk )
(cid:107)
= E (cid:2)
SBk−1
(cid:107)
(cid:107)
SBk−1, F (cid:48)(zk, ξk,Bk )
+2
(cid:104)
= E (cid:2)
2
SBk−1
∗
(cid:107)
(cid:107)
+ E (cid:2)
F (cid:48)(zk, ξk,Bk )
(cid:107)
E (cid:2)
= (cid:80)Bk
(cid:107)

−
F (cid:48)(zk, ξk,j)

j=1

(cid:3)

(cid:3)

2
f (cid:48)(zk)
∗
(cid:107)
2
f (cid:48)(zk)
∗
(cid:107)

−

(cid:3)

−

Bkσ2,

≤

Conditional Accelerated Lazy Stochastic Gradient Descent

(cid:80)Bk

which together with the fact
1
j=1 [F (cid:48)(zk, ξk,j)
Bk
second relationship in (15).

−

f (cid:48)(zk)] = 1
Bk

that gk

f (cid:48)(zk) =
SBk , implies the

−

Secondly, in view of the SCGS method in (Lan and Zhou,
2014), xk obtained in (13) should be an approximate solu-
tion to the gradient sliding subproblem

(cid:110)

min
x∈X

ψk(x) :=

gk, x
(cid:105)

(cid:104)

+ βk

x
2 (cid:107)

−

xk−1

2(cid:111)
(cid:107)

,

(16)

such that for some ηk

0 we have

≥
gk + βk(xk
(cid:104)

ψ(cid:48)
(cid:104)

(cid:105)

x

x

∈

−

−

=

−

(cid:105) ≤

xk−1), xk

k(xk), xk

ηk,
(17)
for all x
X. If we solve the subproblem (16) exactly
(i.e., ηk = 0), then CALSGD will reduce to the acceler-
ated stochastic approximation method by (Lan, 2009; 2012).
However, by employing the LCG procedure (see Proce-
dure 1 in Subsection 2.2), we only need to use a weak
separation oracle, but still maintaining the optimal bounds
on stochastic ﬁrst-order oracle as in (Lan, 2009; 2012; Lan
and Zhou, 2014).

Thirdly, observe that the CALSGD method so far is concep-
tual only as we have not yet speciﬁed the LCG procedure
βk
. We will
and the parameters
}
come back to this issue after introducing the LCG procedure
and establishing its main convergence properties.

Bk
{

, and

,
}

,
}

γk

ηk

{

}

{

{

2.2. The Parameter-free Lazy Conditional Gradient

Procedure

The classical CG method is a well-known projection-free
algorithm, which requires only the solution of a linear op-
timization subproblem (3) rather than the projection over
X per iteration. Therefore, it has computational advantages
over many other ﬁrst-order methods when projection over
X being costly. The LCG procedure presented in this sub-
section, a modiﬁcation of the vanilla LCG method in (Braun
et al., 2016), goes several steps further than CG and even
vanilla LCG method. Firstly, it replaces LO oracle by a
weaker separation oracle LOsep, which is no harder than
linear optimization and often much simpler. Secondly, it
uses a stronger termination criterion, the Frank-Wolfe gap
(cf. (18)), than vanilla LCG method. Finally, it maintains
the same order of convergence rate as the CG and the vanilla
LCG method.

We present the LOsep oracle in Oracle 1 below.

Oracle 1 Weak Separation Oracle LOsepP (c, x, Φ, α)
Input: c

Rn linear objective, x

P point, α

1 accu-

racy, Φ > 0 objective value;

∈

∈

≥

Output: y

P vertex with either (1) cT (x

y) > Φ/α,

or (2) y = argmaxy∈P cT (x

∈

z)

−

≤

Φ.

−

∈

Observe that the oracle has two output modes. In particular,
Oracle 1 ﬁrst veriﬁes whether there exists an improving
point y
P with the required guarantee and if so it out-
puts this point, which we refer it as a positive call. If no
such point exists the oracle certiﬁes this by providing the
maximizer y, which then also provides a new duality gap.
We refer to this case as a negative call. The computational
advantages of this oracle are that it can reuse previously
seen solutions y if they satisfy the improvement condition
and even if LO oracle has to be called, the optimization can
be terminated early once the improvement condition is satis-
ﬁed. Finally, the parameter α allows to only approximately
satisfy the improvement condition making separation even
easier; in our applications we set the parameter α slightly
larger than 1.

We present the LCG procedure based on (Braun et al., 2016)
below. We adapted the parameter-free version to remove
any dependence on hard to estimate parameters. For any
smooth convex function φ, we deﬁne its duality gap as

gapφ,X (x)

gapφ(x) := max

φ(x)T (x

y).

(18)

≡

y∈X ∇

−

Clearly, by convexity the duality gap is an upper bound on
f (x∗)
f (x). Given any accuracy parameter η
0, the
LCG procedure solves minx∈X φ(x) approximately with
η.
accuracy η, i.e., it outputs a point ¯u

X, s.t. gapφ(¯u)

−

≥

∈

≤

Procedure 1 Parameter-free Lazy Conditional Gradients
(LCG) procedure
Input: access to gradients of smooth convex function φ,
X vertex, LOsepX weak linear separation oracle,

1, duality gap bound η

X with bounded duality gap, i.e., gapφ(¯u)

≤

u1
accuracy α

∈

≥

∈

vt
←
if not

Output: ¯u
η
1: Φ0
←
2: for t = 1 to T
3:
4:
5:
6:
7:
8:
9:

end if

else
Φt
end if
10:
λt
11:
←
ut+1
12:
13: end for

←

←

(1

maxu∈X

φ(u1)T (u1
∇
1 do

u)

−

LOsepX (

−
∇
φ(ut)T (ut
if Φt−1 = η then
return ¯u = ut

∇

−

φ(ut), xt, Φt−1, α)

vt) > Φt−1/α then

max

(cid:110) Φt−1
2

(cid:111)

, η

{Update Φt}

argmin φ((1

λt)ut + λtvt)

−
λt)ut + λtvt

−

The LCG procedure is a parameter-free algorithm. Note
that while line search can be expensive in general, for our
subproblems, function evaluation is very cheap. The al-
gorithm needs only one LO oracle call to estimate the
initial functional value gap at Line 1. Alternatively, this

Conditional Accelerated Lazy Stochastic Gradient Descent

Φt
{

can be also done approximately via binary search with
LOsep. The algorithm maintains a sequence,
, that
provides valid upper bounds for the functional value gap
at the current iterate, i.e., φ(ut)
2Φt−1 (see The-
orem 5.1 of (Braun et al., 2016)), and it halves the value
of Φt only when the current oracle call is negative. Finally,
our LCG procedure exits at Line 5 whenever LOsepX re-
turns a negative call and Φt−1 = η, which ensures that
gapφ(¯u) = maxy∈X

φ(¯u), ¯u

φ∗

η.

−

≤

}

y

(cid:104)∇

−

(cid:105) ≤

Theorem 2.1 below provides a bound for the total num-
ber of iterations (or calls to the LOsepX oracle) that the
X with
LCG procedure requires to generate a point ¯u
gapφ(¯u)

η.

∈

X such that
Theorem 2.1. Procedure 1 returns a point ¯u
the duality gap at point ¯u is bounded by η, i.e., gapφ(¯u)
η.
Furthermore, the total number of iterations T (and hence
LOsepX calls) performed by Procedure 1 is at most

≤

∈

≤

(cid:40)

κ + 8α2Cφ
η + 2,
κ + 4α + 4α2Cφ

T

≤

η + 2,

η < αCφ;
αCφ,
η

≥

(19)

with κ := 4α

(cid:108)
log Φ0
αCφ

(cid:109)

+ log Φ0
η .

Proof. From the observations above, it is clear that the du-
ality gap at the output point ¯u is bounded by η.

Also observe that the procedure calls LOsepX once per
iteration. In order to demonstrate the bound in (19), we split
the LCG procedure into two phases, and bound the number
of iterations separately for each phase. Let Cφ denote the
curvature of the smooth convex function φ.

We say Procedure 1 is in the ﬁrst phase whenever Φt−1 > η.
In view of Theorem 5.1 in (Braun et al., 2016), it is clear that
the number of iterations in the ﬁrst phase can be bounded as

4α

(cid:108)
log Φ0
αCφ

(cid:109)

+ 4α2Cφ

η + log Φ0
η .

T1

≤

η. Again
Procedure 1 enters the second phase when Φt−1
with the argumentation in Theorem 5.1 in (Braun et al.,
2016), we obtain that the total number of positive calls in
this phase can be bounded by 4α2Cφ
, if η < αCφ, or by 4α
η
αCφ. Moreover, the procedure exits whenever the
if η
current LOsepX oracle call is a negative call. Hence, the
number of iterations in the second phase can be bounded by

≥

≤

(cid:40) 4α2Cφ

η + 1,

≤

4α + 1,

T2

η < αCφ;
αCφ.
η

≥

Thus, our bound in (19) can be obtained from the above two
bounds plus one more LO oracle call at Line 1.

2.3. The Convergence Properties of CALSGD

This subsection is devoted to establishing the main con-
vergence properties of the CALSGD method. Since the
algorithm is stochastic, we will establish the convergence
results for ﬁnding a stochastic (cid:15)-solution, i.e., a point ¯x
X
s.t. E[f (¯x)
(cid:15). We ﬁrst state a simple technical
result from (Lan and Zhou, 2014, Lemma 2.1) that we will
use.

f (x∗)]

−

≤

∈

Lemma 2.2. Let wt
let us denote

∈

(0, 1], t = 1, 2, . . ., be given. Also

Wt :=

(cid:40)
1
(1

wt)Wt−1

−

t = 1
2.
t

≥

Suppose that Wt > 0 for all t
δt

t≥0 satisﬁes
}

{

≥

2 and that the sequence

δt

(1

wt)δt−1 + Bt,

t = 1, 2, . . . .

≤
Then for any 1

−
l

k, we have

≤

≤

δk

Wk

≤

(cid:16) 1−wl
Wl

δl−1 + (cid:80)k

i=l

Bi
Wi

(cid:17)

.

Theorem 2.3 describes the main convergence properties of
the CALSGD method (cf. Algorithm 1). The proof of this
theorem can be found in the Supplementary material A.

Theorem 2.3. Let Γk be deﬁned as follows,

Γk :=

(cid:40)
1
Γk−1(1

k = 1
2.

≥

γk) k

−

(20)

Suppose that
satisfy

βk
{

}

and

γk

{

}

in the CALSGD algorithm

γ1 = 1 and Lγk

βk, k

1.

(21)

≤

≥

a) If

βkγk
Γk ≥

βk−1γk−1
Γk−1

, k

2,

≥

(22)

then under assumptions (9) and (10), we have

E [f (yk)

f (x∗)]

−

βkγk
2 D2

X

≤

+ Γk

k
(cid:88)

i=1

(cid:104) ηiγi
Γi

+

γiσ2
2ΓiBi(βi−Lγi)

(cid:105)

,

where x∗ is an arbitrary optimal solution of (8) and
DX is deﬁned in (4).

b) If

βkγk
Γk ≤

βk−1γk−1
Γk−1

, k

2,

≥

(rather than (36)) is satisﬁed, then the result in part a)
holds by replacing βkγkD2
2 in
the ﬁrst term of the RHS of (37).

X with β1Γk

x∗

x0

−

(cid:107)

(cid:107)

(23)

(24)

Conditional Accelerated Lazy Stochastic Gradient Descent

c) Under the assumptions in part a) or b), the number of
inner iterations performed at the k-th outer iterations
is bounded by

bound in (28) then immediately follows from this observa-
tion and the fact that the number of calls to the SFO oracle
is bounded by

Tk =

(cid:40)

κ + 8α2βkD2
+ 2,
ηk
κ + 4α + 4α2βkD2

X

X

ηk

+ 2,

with κ := 4α

log

(cid:108)

(cid:109)

Φk
0
αβkD2
X

+ log Φk
0
ηk

.

ηk < αβkD2
αβkD2
ηk

≥

X ;
X ,
(25)

,
}

,
}

ηk
{

γk
{

Bk
{

, and
}

Now we provide two different sets of parameters
βk
, which lead to optimal com-
}
{
plexity bounds on the number of calls to the SFO and
LOsepX oracles.
Corollary 2.4. Suppose that
βk
{
in the CALSGD method are set to

, and
}

Bk
{

ηk
{

,
}

γk

}

{

}

,

βk = 4L

k+2 , γk = 3

and Bk =

k+2 , ηk = LD2
1,
, k

(cid:109)

X

k(k+1) ,

(cid:108) σ2(k+2)3
L2D2
X

≥

and we assume
tion x∗ of (8). Under assumptions (9) and (10), we have

is bounded for any optimal solu-

(cid:107)

(cid:107)

f (cid:48)(x∗)

Φk

0 =

(cid:80)N

k=1Bk

(cid:80)N

k=1

σ2(k+2)3
L2D2
X

+ N

σ2(N +3)4
4L2D2
X

+ N.

≤

≤

We now provide a good estimation for Φk
0 (cf. Line 1 in
LCG procedure) at the k-th outer iteration. In view of the
deﬁnition of Φk

) (cf. (16)), we have,

0 and ψ(
·

Φk

0 =

ψ(cid:48)
(cid:104)

k(xk−1), xk−1

x

(cid:105)

−

=

gk, xk−1
(cid:104)

.

x
(cid:105)

−

Moreover, let Ak :=
∗
shev’s inequality and (15), we obtain,

f (cid:48)(zk)
(cid:107)

gk

−

(cid:107)

≥

(cid:113) N σ2
ΛBk

, by Cheby-

Prob

Ak
{

} ≤

E[(cid:107)gk−f (cid:48)(zk)|2

∗]ΛBk

N σ2

Λ
N ,

∀

≤

Λ < 1, k

1,

≥

which implies that Prob
Λ. Hence, by
k=1
{
Cauchy-Schwarz and triangle inequalities, we have with
probability 1

} ≤

Λ,

−

1

(cid:84)N

¯Ak

(26)

−
f (cid:48)(zk), xk−1

+

f (cid:48)(zk)
(cid:107)
(cid:19)

−
N σ2
ΛBk

gk
(cid:104)
(cid:18)(cid:113)

(cid:18)(cid:113)

≤

≤

x

+

f (cid:48)(zk), xk−1
(cid:105)
(cid:104)
f (cid:48)(x∗)

−
f (cid:48)(x∗)
(cid:107)

∗ +
(cid:107)

∗
(cid:107)

−

−

x
(cid:105)}
(cid:19)

DX

N
Λk3 + 1

LD2

X +

f (cid:48)(x∗)

(cid:107)

∗DX ,
(cid:107)

(31)

where the last inequality follows from (6) and (26).

(28)

Note that we always have ηk < αβkD2
X . Therefore, it
follows from the bound in (39), (26), and (31) that the total
number of inner iterations can be bounded by

X

≤

−

6LD2
X

f (x∗)]

E [f (yk)

(k+2)2 + 9LD2

2(k+1)(k+2) ,

1.
(27)
As a consequence, the total number of calls to the SFO and
LOsepX oracles performed by the CALSGD method for
ﬁnding a stochastic (cid:15)-solution of (1), respectively, can be
bounded by

≥

∀

k

(cid:26)(cid:113)

LD2
X

(cid:15) + σ2D2

(cid:15)2

X

(cid:27)

,

O

and

O

(cid:26)(cid:113)

LD2
X
(cid:15)

log LD2

Λ(cid:15) + LD2

X

(cid:15)

X

(cid:27)

with probability 1

Λ.

−

(29)

Proof. It can be easily seen from (26) that (35) holds. Also
note that by (26), we have

Γk =

6
k(k+1)(k+2) ,

(30)

and hence

βkγk
Γk

= 2Lk(k+1)
k+2

,

which implies that (36) holds. It can also be easily checked
from (30) and (26) that

(cid:80)k

i=1

ηiγi
Γi ≤

kLD2
X
2

, (cid:80)k

i=1

γi

ΓiBi(βi−Lγi) ≤

kLD2
2σ2 .
X

Using the bound in (37), we obtain (27), which implies that
the total number of outer iterations N can be bounded by
under the assumptions (9) and (10). The

(cid:16)(cid:112)LD2

(cid:17)
X /(cid:15)

O

(cid:80)N

k=1Tk

≤

(cid:80)N

k=1

(cid:16)

(cid:104)

4α

Φk
0
αβkD2
X

(cid:17)

+ 1

+ log Φk
0
ηk

log
(cid:105)

+ 2

+ 8α2βkD2
ηk

X

N
(cid:88)

(cid:20)
5α log

(cid:18)

2k2

(cid:18)(cid:113)

N

Λk3 + 1 + (cid:107)f (cid:48)(x∗)(cid:107)∗

LDX

(cid:19)(cid:19)

≤

k=1

+32α2k(cid:3) + (4α + 2)N

(N log N 2

Λ + N 2 + N ),

=

O

which implies that our bound in (29).

We now provide a slightly improved complexity bound on
the number of calls to the SFO oracle which depends on
the distance from the initial point to the set of optimal so-
lutions, rather than the diameter DX . In order to obtain
x∗
this improvement, we need to estimate D0
(cid:107)
and to ﬁx the number of iterations N in advance. This
result will play an important role for the analysis of the
CALSGD method to solve strongly convex problems (see
Supplementary Material C.1).

≥ (cid:107)

x0

−

Conditional Accelerated Lazy Stochastic Gradient Descent

Corollary 2.5. Suppose that there exists an estimate D0
s.t.
DX . Also assume that the outer
iteration limit N

1 is given. If

x0
(cid:107)

(cid:107) ≤

D0

x∗

−

≤

≥
k , γk = 2
βk = 3L

and Bk =

k+1 , ηk = 2LD2
N k ,
(cid:109)

0

(cid:108) σ2N (k+1)2
L2D2
0

, k

1.

≥

(32)

Under assumptions (9) and (10),

E [f (yN )

f (x∗)]

8LD2
N (N +1) ,
0

N

1.

−

≤
As a consequence, the total number of calls to the SFO and
LOsepX oracles performed by the CALSGD method for
ﬁnding a stochastic (cid:15)-solution of (1), respectively, can be
bounded by

≥

∀

(cid:26)(cid:113)

LD2
0

(cid:15) + σ2D2

(cid:15)2

0

(cid:27)

,

O

(33)

and (29).

Proof. The proof is similar to Corollary 2.4, and hence
details are skipped.

It should be pointed out that the complexity bound for the
number of calls to the LOsep oracle in (29) is established
with probability 1
Λ. However, the probability parameter
Λ only appears in the non-dominant term.

−

3. Experimental Results

We present preliminary experimental results showing the
performance of CALSGD compared to OFW for stochastic
optimization. As examples we use the video co-localization
problem, which can be solved by quadratic programming
over a path polytope, different structured regression prob-
lems, and quadratic programming over the standard spec-
trahedron. In all cases we use objective functions of the
Rm×n, i.e., m examples over a
form
feasible region of dimension n. For comparability we use a
batch size of 128 for all algorithms to compute each gradi-
ent and the full matrix A for the actual objective function
values. All graphs show the function value using a logscale
on the vertical axis.

2, with A
(cid:107)

Ax

−

∈

(cid:107)

b

In Figure 1 we compare the performance of three algorithms:
CALSGD, SCGS and OFW. As described above SCGS is
the non-lazy counterpart of CALSGD. In the four graphs
of Figure 1 we report the objective function value over the
number of iterations, the wall clock time in seconds, the
number of calls to the linear oracle, and the number of
gradient evaluations in that order. In all these measures, our
proposed algorithms outperform OFW by multiple orders
of magnitude. As expected in number of iterations and
number of gradient evaluations both versions CALSGD and
SCGS perform equally well, however in wall clock time and

Figure 1. Performance of CALSGD and its non-lazy variant SCGS
on a structured regression problem compared to OFW. The fea-
sible region is the ﬂow-based formulation of the convex hull of
Hamiltonian cycles on 9 nodes and has dimension n = 162.

in the number of calls to the linear oracle we observe the
advantage of the weaker LOsep oracle over LO.

In Figure 5 and 4 we show the performance of CALSGD
on one video co-localization instance and one semi-deﬁnite
convex programming instance. Due to space limitations
we only report the function value over the number of iter-
ations and wall clock time in seconds; see Supplementary
Material D for a detailed analysis as well as more examples.

Implementation details. Finally, we provide details of
the implementation of LOsep. In the case of the structured
regression problems and the quadratic optimizations over
the path polytope instances, we used Gurobi as a solver
and included callbacks to terminate whenever the required
improvement (given by Φt−1) is reached; our approach is
one out of many and other approaches could have been used
equally well. If the solver does not ﬁnd a good enough
solution, it returns with a lower bound on the Wolfe gap,
which we use to update Φt. In the case of convex program-
X (cid:60)
ming over the feasible region Sn :=
{
0 and tr(X) = 1
, we compute a maximal eigenvector of
}
the gradient (which is a matrix in this case) and use the rank-
1 factor of the maximal eigenvector, which is an optimal
point. In this case, there is no early termination.

Rn×n

X

∈

|

However, in all cases, we use caching, i.e., we store all
previously seen points and check if any of them satisﬁes
the improvement guarantee. If that is the case we do not
call Gurobi or the maximal eigenvector routine. The size of
the cache is very small in all experiments; alternatively one
could use cache strategies such as e.g., k-paging.

0150300450600Iterations10−310−210−1100101102FunctionvalueCALSGDSCGSOFW0150300450Wallclocktime10−310−210−1100101102FunctionvalueCALSGDSCGSOFW0100200300400500600LPcalls10−310−210−1100101102FunctionvalueCALSGDSCGSOFW0100200300400500600Gradientevaluations10−310−210−1100101102FunctionvalueCALSGDSCGSOFWConditional Accelerated Lazy Stochastic Gradient Descent

Figure 2. Two small video co-localization instances. On the
left: road_paths_01_DC_a instance (n = 29682 and m =
10000). On the right: road_paths_01_DC_b instance (n =
29682 and m = 10000). Observe a signiﬁcant difference in func-
tion value of multiple orders of magnitude after a few seconds.

Figure 4. Performance of CALSGD and OFW on a medium sized
convex programming instance with feasible region Sn := {X ∈
Rn×n | X (cid:60) 0, tr(X) = 1} with n = 100 and m = 10000.
Similar to the results before in both iterations and wall clock time
our method performs better.

Figure 5. Performance of CALSGD compared to OFW on a small
video co-localization instance. The dimension of the underlying
path polytope is n = 29682, the time limit is 50 seconds. Our
algorithm performs signiﬁcantly better both in number of iterations
as well as in wall clock time.

Figure 3. Two medium sized video co-localization instances. On
the left: road_paths_02_DE_a instance (n = 119520 and
m = 10000). On the right: road_paths_02_DE_b instance
(n = 119520 and m = 10000). Similar results as in Figure 2.

Figure 6. Structured regression problem over the convex hull of
all Hamiltonian cycles of a graph on 11 nodes (n = 242) on the
left and 12 nodes (n = 288) on the right. We used a density of
d = 0.6 for A and m = 10000. On both instances CALSGD
achieves lower values much faster, both in number of iterations as
well as in wall clock time.

010203040Iterations102103104105106107108FunctionvalueCALSGDOFW081624Iterations102103104105106107108FunctionvalueCALSGDOFW0153045Wallclocktime102103104105106107108FunctionvalueCALSGDOFW0153045Wallclocktime102103104105106107108FunctionvalueCALSGDOFW0102030Iterations103104105106107108109FunctionvalueCALSGDOFW081624Iterations103104105106107108109FunctionvalueCALSGDOFW050100150200Wallclocktime103104105106107108109FunctionvalueCALSGDOFW050100150200Wallclocktime103104105106107108109FunctionvalueCALSGDOFW050010001500Iterations10−410−310−210−1100101102103FunctionvalueCALSGDOFW0255075100Wallclocktime10−410−310−210−1100101102103FunctionvalueCALSGDOFW010203040Iterations102103104105106107108FunctionvalueCALSGDOFW0153045Wallclocktime102103104105106107108FunctionvalueCALSGDOFW0255075100Iterations106107FunctionvalueCALSGDOFW0204060Iterations106107FunctionvalueCALSGDOFW01000200030004000Wallclocktime106107FunctionvalueCALSGDOFW01000200030004000Wallclocktime106107FunctionvalueCALSGDOFWConditional Accelerated Lazy Stochastic Gradient Descent

Acknowledgements

We would to thank Elad Hazan for providing references.
Research reported in this paper was partially supported by
NSF CAREER award CMMI-1452463.

References

S. Ahipasaoglu and M. Todd. A Modiﬁed Frank-Wolfe Al-
gorithm for Computing Minimum-Area Enclosing Ellip-
soidal Cylinders: Theory and Algorithms. Computational
Geometry, 46:494–519, 2013.

F. Bach, S. Lacoste-Julien, and G. Obozinski. On the equiv-
alence between herding and conditional gradient algo-
rithms. In the 29th International Conference on Machine
Learning, 2012.

A. Beck and M. Teboulle. A conditional gradient method
with linear rate of convergence for solving convex linear
systems. Math. Methods Oper. Res., 59:235–247, 2004.

G. Braun, S. Pokutta, and D. Zink. Lazifying conditional
gradient algorithms. arXiv preprint arXiv:1610.05120,
2016.

Y. Chen, G. Lan, and Y. Ouyang. Optimal primal-dual
methods for a class of saddle point problems. SIAM
Journal on Optimization, 24(4):1779–1814, 2014.

K. L. Clarkson. Coresets, sparse greedy approximation, and
the frank-wolfe algorithm. ACM Trans. Algorithms, 6(4):
63:1–63:30, Sept. 2010.

B. Cox, A. Juditsky, and A. S. Nemirovski. Dual subgradient
algorithms for large-scale nonsmooth learning problems.
Manuscript, School of ISyE, Georgia Tech, Atlanta, GA,
30332, USA, 2013. submitted to Mathematical Program-
ming, Series B.

M. Frank and P. Wolfe. An algorithm for quadratic program-
ming. Naval Research Logistics Quarterly, 3:95–110,
1956.

R. M. Freund and P. Grigas. New Analysis and Results for
the Frank-Wolfe Method. ArXiv e-prints, July 2013.

D. Garber and E. Hazan. A Linearly Convergent Condi-
tional Gradient Algorithm with Applications to Online
and Stochastic Optimization. ArXiv e-prints, Jan 2013.

S. Ghadimi and G. Lan. Optimal stochastic approximation
algorithms for strongly convex stochastic composite op-
timization, I: a generic algorithmic framework. SIAM
Journal on Optimization, 22:1469–1492, 2012.

S. Ghadimi and G. Lan. Optimal stochastic approximation
algorithms for strongly convex stochastic composite opti-
mization, II: shrinking procedures and optimal algorithms.
SIAM Journal on Optimization, 23:2061–2089, 2013.

Gurobi Optimization. Gurobi optimizer reference man-
ual version 6.5, 2016. URL https://www.gurobi.
com/documentation/6.5/refman/.

Z. Harchaoui, A. Juditsky, and A. S. Nemirovski. Condi-
tional gradient algorithms for machine learning. NIPS
OPT workshop, 2012.

E. Hazan. Sparse approximate solutions to semideﬁnite
programs. In E. Laber, C. Bornstein, L. Nogueira, and
L. Faria, editors, LATIN 2008: Theoretical Informat-
ics, volume 4957 of Lecture Notes in Computer Science,
pages 306–316. Springer Berlin Heidelberg, 2008. ISBN
978-3-540-78772-3.

E. Hazan and S. Kale. Projection-free online learning. arXiv

preprint arXiv:1206.4657, 2012.

E. Hazan and H. Luo. Variance-reduced and projection-
free stochastic optimization. In Proceedings of The 33rd
International Conference on Machine Learning, pages
1263–1271, 2016.

M. Jaggi.

Sparse Convex Optimization Methods for
Machine Learning. PhD thesis, ETH Zürich, 2011.
http://dx.doi.org/10.3929/ethz-a-007050453.

M. Jaggi. Revisiting frank-wolfe: Projection-free sparse
convex optimization. In the 30th International Confer-
ence on Machine Learning, 2013.

M. Jaggi and M. Sulovský. A simple algorithm for nuclear
norm regularized problems. In the 27th International
Conference on Machine Learning, 2010.

R. Johnson and T. Zhang. Accelerating stochastic gradient
descent using predictive variance reduction. In Advances
in Neural Information Processing Systems, pages 315–
323, 2013.

A. Joulin, K. Tang, and L. Fei-Fei. Efﬁcient image and
video co-localization with frank-wolfe algorithm. In Eu-
ropean Conference on Computer Vision, pages 253–268.
Springer, 2014.

G. Lan. Convex optimization under inexact ﬁrst-order in-
formation. Ph.D. dissertation, School of Industrial and
Systems Engineering, Georgia Institute of Technology,
Atlanta, GA 30332, USA, 2009.

G. Lan. An optimal method for stochastic composite opti-
mization. Mathematical Programming, 133(1):365–397,
2012.

G. Lan. The complexity of large-scale convex programming
under a linear optimization oracle. Technical Report,
2013. Available on http://www.optimization-online.org/.

Conditional Accelerated Lazy Stochastic Gradient Descent

G. Lan and Y. Zhou. Conditional gradient sliding for convex
optimization. Optimization-Online preprint (4605), 2014.

R. Luss and M. Teboulle. Conditional gradient algorithms
for rank one matrix approximations with a sparsity con-
straint. SIAM Review, 55:65–98, 2013.

M. Mahdavi, L. Zhang, and R. Jin. Mixed optimization for
smooth functions. In Advances in Neural Information
Processing Systems, pages 674–682, 2013.

Y. E. Nesterov. A method for unconstrained convex mini-
mization problem with the rate of convergence O(1/k2).
Doklady AN SSSR, 269:543–547, 1983.

Y. E. Nesterov. Introductory Lectures on Convex Optimiza-
tion: a basic course. Kluwer Academic Publishers, Mas-
sachusetts, 2004.

Y. E. Nesterov. Smooth minimization of nonsmooth func-
tions. Mathematical Programming, 103:127–152, 2005.

C. Shen, J. Kim, L. Wang, and A. van den Hengel. Posi-
tive semideﬁnite metric learning using boosting-like al-
gorithms. Journal of Machine Learning Research, 13:
1007–1036, 2012.

Conditional Accelerated Lazy Stochastic Gradient Descent

A. Omitted proofs

Theorem A.1. Let Γk be deﬁned as follows,

Γk :=

(cid:40)
1
Γk−1(1

k = 1
2.

γk) k

(34)

≥

−
in the CALSGD algorithm

Suppose that
satisfy

βk
{

}

and

γk

{

}

γ1 = 1 and Lγk

βk, k

1.

(35)

≤

≥

f (yk)

a) If

βkγk
Γk ≥

βk−1γk−1
Γk−1

, k

2,

≥

(36)

then under assumptions (9) and (10), we have

which implies that

xk

1
2 (cid:107)

−

xk−1

2 = 1

(cid:107)

xk−1
xk−1
xk−1

−

−

−
gk, x

2 (cid:107)
− (cid:104)
1
2 (cid:107)
+ 1

βk (cid:104)

2
x
−
(cid:107)
xk, xk
2
x
(cid:107)
xk

−

−

(cid:105)

1
xk
2 (cid:107)
x
(cid:105)
−
1
xk
2 (cid:107)
+ ηk
.
βk

≤

x

2
(cid:107)

−

x

2
(cid:107)

−

Combing the above two relations, we have

≤

(1
−
+ βkγk
2
+ ηkγk

= (1

−
+ βkγk
2
+ ηkγk

γk)f (yk−1) + γklf (zk, xk) + γk
2

x

(cid:2)
xk−1
(cid:107)
−
(cid:107)
γk(βk−Lγk)
2

xk

x

2(cid:3)
(cid:107)
−
xk−1

2

− (cid:107)
xk

(cid:107)

−

−

(cid:107)
γk)f (yk−1) + γklf (zk, x) + γk
2(cid:3)
2
(cid:107)
−
xk−1

(cid:2)
xk−1
(cid:107)
−
(cid:107)
γk(βk−Lγk)
2

− (cid:107)
xk

xk

2.

x

x

(cid:107)
Using the above inequality and the fact that

−

−

(cid:107)

gk, x
(cid:104)

−

xk

(cid:105)

δk, x
(cid:104)

−

xk

(cid:105)

δk,x
(cid:104)
=

(cid:105) −

−

xk
δk, x
(cid:104)
+

(βk−Lγk)
2
xk−1

xk
(cid:107)

−

xk−1

2
(cid:107)

−
δk, xk−1
(cid:104)
δk, x

−
xk−1

(βk−Lγk)
2

(cid:105)
xk
(cid:105) −
+ (cid:107)δk(cid:107)2
2(βk−Lγk) ,

∗

(cid:105)

≤ (cid:104)
−
we obtain for all x

X,

∈

xk
(cid:107)

−

xk−1

2
(cid:107)

f (yk)

≤

(1
−
+ βkγk
2

+ γk

(cid:104)

γk)f (yk−1) + γkf (x) + ηkγk
2(cid:3)
2
xk
− (cid:107)
(cid:107)
(cid:107)
−
+ γk(cid:107)δk(cid:107)2
2(βk−Lγk) .
∗

(cid:2)
xk−1
(cid:107)
δk, x

−
xk−1

x

x

−

(cid:105)

(40)

Subtracting f (x) from both sides of (40) and applying
Lemma 2.2, we have

f (yk)

f (x)

Γk(1

γ1) [f (y0)

f (x)] + Γk

−

≤

−

−

k
(cid:88)

i=1

ηiγi
Γi

+ Γk

+ Γk

k
(cid:88)

i=1

k
(cid:88)

i=1

βiγi
2Γi

(cid:2)
xk−1
(cid:107)

2

x
(cid:107)

−

xk

− (cid:107)

−

x

2(cid:3)
(cid:107)

γi
Γi

(cid:104)
(cid:104)

δi, x

xi−1

−

(cid:105)

+ (cid:107)δi(cid:107)2

∗

2(βi−Lγi)

(cid:105)

.

(41)

Also observe that

(cid:80)k

βiγi
i=1
Γi
= β1γ1
Γ1 (cid:107)
+ (cid:80)k

(
(cid:107)
x0

xi−1

−
2
x
−
(cid:107)
(cid:16) βiγi

i=2

Γi −
X + (cid:80)k

i=2

β1γ1
Γ1

D2

−

x

xi

xk
(cid:17)

− (cid:107)

2
(cid:107)
βkγk
Γk (cid:107)
βi−1γi−1
Γi−1
(cid:16) βiγi

Γi −

x

−

(cid:107)
x

2)
2
−
(cid:107)
x
xi−1
−
(cid:107)
(cid:17)
βi−1γi−1
Γi−1

≤

2
(cid:107)
D2

X = βkγk
Γk

D2

X ,

E [f (yk)

f (x∗)]

−

βkγk
2 D2

X

≤

+ Γk

k
(cid:88)

i=1

(cid:104) ηiγi
Γi

+

γiσ2
2ΓiBi(βi−Lγi)

(cid:105)

,

(37)

where x∗ is an arbitrary optimal solution of (8) and
DX is deﬁned in (4).

b) If

βkγk
Γk ≤

βk−1γk−1
Γk−1
(rather than (36)) is satisﬁed, then the result in part a)
holds by replacing βkγkD2
2 in
the ﬁrst term of the RHS of (37).

X with β1Γk

(38)

, k

x∗

x0

2,

≥

−

(cid:107)

(cid:107)

c) Under the assumptions in part a) or b), the number of
inner iterations performed at the k-th outer iterations
is bounded by

Tk =

(cid:40)

κ + 8α2βkD2
+ 2,
ηk
κ + 4α + 4α2βkD2

X

X

ηk

+ 2,

with κ := 4α

log

(cid:108)

(cid:109)

Φk
0
αβkD2
X

+ log Φk
0
ηk

.

ηk < αβkD2
αβkD2
ηk

≥

X ;
X ,
(39)

Proof. Let us denote δk,j = F (cid:48)(zk, ξk,j)
gk
view of (6), (11) and (14), we have

f (cid:48)(zk) = (cid:80)Bk

≡
j=1 δk,j/Bk. We ﬁrst show part a). In

f (cid:48)(zk) and δk

−

−

≤
= (1
−
+ Lγ2
k
2 (cid:107)
= (1
−
+ βkγk

f (yk)

lf (zk; yk) + L
2 (cid:107)

yk

zk

2
(cid:107)

−
γk)lf (zk; yk−1) + γklf (zk; xk)

2

(cid:107)

−

xk−1

xk
γk)f (yk−1) + γklf (zk; xk)
γk(βk−Lγk)
2
2

xk−1

xk

2 (cid:107)

−

(cid:107)
(cid:107)
where the last inequality follows from the convexity of f (
·
Also observe that by (17), we have

−

−

(cid:107)

).

xk

xk−1

2,

gk + βk(xk

xk−1), xk

x

ηk,

−

−

(cid:105) ≤

(cid:104)

x

∀

∈

X,

Conditional Accelerated Lazy Stochastic Gradient Descent

where the inequality follows from the third assumption in
(36) and the deﬁnition of DX in (4).

such that for all x

X

Therefore, from the above two relations and the fact that
γ1 = 1, we can conclude that

f (yk)

f (x)

−

X + Γk

βkγk
2 D2
j=1B−1

≤
+(cid:80)Bi

i

(cid:104)

(cid:80)k

i=1

γi
Γi

ηi + (cid:107)δi(cid:107)2

∗

2(βi−Lγi)

δi,j, x

xi−1

(cid:104)

−

(cid:105)

.

(cid:105)

(42)

Note that by our assumptions on SFO, the random variables
δi,j are independent of the search point xi−1 and hence
E[
] = 0. In addition, relation (15) implies
−
(cid:105)
(cid:104)
that E[
2
σ2/Bi. Using the previous two observa-
∗]
(cid:107)
tions and taking expectation on both sides of (42) (with
x = x∗) we obtain (37).

δi,j, x∗
δi
(cid:107)

xi−1

≤

Similarly, Part b) follows from (41), the assumption that
γ1 = 1, and the fact that

(cid:80)k

i=1

βiγi
Γi
β1γ1
Γ1 (cid:107)

(

xi−1
(cid:107)
x0

x

−

−
2
(cid:107)

x

−

− (cid:107)

2
(cid:107)
βkγk
Γk (cid:107)

xi

xk

x

2)
(cid:107)
2
x
(cid:107)

−

−

≤

β1

x0
(cid:107)

−

≤

x

2,
(cid:107)
(43)

due to the assumptions in (35) and (38).

Let Φk
0 denote the initial bound obtained in Line 1 of the
LCG procedure at the k-th outer iteration. The result in
Part c) follows immediately from (19) and the fact that
Cψk = βkD2
X .

B. Deterministic CALSGD

Our goal in this section is to present a deterministic version
of CALSGD, which we refer to as CALGD. Instead of
calling the SFO oracle to compute the stochastic gradients,
we assume that we have access to the exact gradients of
f . Therefore, the CALGD method calls the FO oracle to
obtain the exact gradients f (cid:48)(zk) at the k-th outer iteration.

The CALGD method is formally described as follows.

Algorithm 2 The conditional accelerated lazy gradient de-
scent (CALGD) method

This algorithm is the same as Algorithm 1 except that
steps (12) and (13) are replaced by

xk = LCG(f (cid:48)(zk), βk, xk−1, α, ηk).

(44)

Similarly to the stochastic case, we can easily see that xk
obtained in (44) is an approximate solution for the gradient
sliding subproblem

(cid:26)

min
x∈X

ψk(x) :=

f (cid:48)(zk), x
(cid:105)
(cid:104)

+

βk
2 (cid:107)

x

−

xk−1

(cid:107)

(cid:27)

2

(45)

ψ(cid:48)

k(xk), xk

x

−

(cid:105)

f (cid:48)(zk)+βk(xk
(cid:104)

−

(cid:104)

xk−1), xk

x

−

(cid:105) ≤

ηk,
(46)

∈
=

for some ηk

0.

≥

Theorem B.1 describes the main convergence properties of
the above CALGD method.
Theorem B.1. Let Γk be deﬁned as in (34). Suppose that
βk

in the CALGD algorithm satisfy (35).

and

{

}

γk
{

}

a) If (36) is satisﬁed, then for any x

X and k

1,

∈
X + Γk

≥
.

−

f (yk)

f (x∗)

βkγk
2 D2
where x∗ is an arbitrary optimal solution of (1) and
DX is deﬁned in (4).

ηiγi
Γi

(47)

i=1

≤

(cid:80)k

b) If (38) (rather than (36)) is satisﬁed, then for any x

X and k

f (yk)

−

1,

≥
f (x∗)

β1Γk

x0

x∗

2 + Γk
(cid:107)

−

2 (cid:107)

≤

(cid:80)k

i=1

∈

ηiγi
.
Γi
(48)

c) Under the assumptions in either part a) or b), the
number of inner iterations performed at the k-th outer
iteration can be bounded by (39).

Proof. Since the convergence results stated in Theo-
rem 2.3 cover the deterministic case when we set δk,j =
F (cid:48)(zk, ξk,j)
0, Part a) immediately follows from
−
(37) with σ = 0. Similarly, Part b) follows from (41), (43)
and δi = (cid:80)Bi
j=1δi,j = 0. The proof of Part c) is exactly the
same as that of Theorem 2.3.c).

f (cid:48)(zk)

≡

{

}

ηk

,
}

γk
{

, and
}

Clearly, there exist various options to specify the parameters
βk
so as to guarantee the convergence
{
of the CALGD method. In the following corollaries, we
provide two different parameter settings for
, and
}
ηk
, which lead to optimal complexity bounds on the total
{
number of calls to the FO and LOsep oracles for smooth
convex optimization.

γk
{

,
}

βk

{

}

Corollary B.2. If
method are set to

βk
{

,
}

γk

, and

ηk

in the CALGD

{

}

{

}

βk = 3L

k+1 , γk = 3

k+2 , and ηk = LD2

X

k(k+1) ,

1,
(49)
is bounded for any optimal

≥

∀

k

and we assume that
solution x∗ of (1), then for any k

f (cid:48)(x∗)

(cid:107)

(cid:107)

f (yk)

f (x∗)

(50)

−

≤
As a consequence, the total number of calls to the FO and
LOsep oracles performed by the CALGD method for ﬁnding
and
an (cid:15)-solution of (1) can be bounded by

(cid:16)(cid:112)LD2

(cid:17)
X /(cid:15)

O

(cid:0)LD2

X /(cid:15)(cid:1) respectively.

O

1,

≥
15LD2
2(k+1)(k+2) .
X

Conditional Accelerated Lazy Stochastic Gradient Descent

Proof. It can be easily seen from (49) that (35) holds, Γk is
given by (30), and

problems

C. Generalizations to other optimization

βkγk
Γk

=

9L
(k+1)(k+2)

k(k+1)(k+2)
6

= 3Lk
2 ,

which implies that (36) is satisﬁed. It then follows from
Theorem B.1.a), (49), and (30) that

f (yk)

f (x∗)

−

9LD2
X

2(k+1)(k+2) +

≤
= 15LD2

2(k+1)(k+2) ,

X

6
k(k+1)(k+2)

(cid:80)k

i=1

ηiγi
Γi

which implies that the total number of outer iterations per-
formed by the CALGD method for ﬁnding an (cid:15)-solution can
be bounded by N =

15LD2

(cid:112)

X /(2(cid:15)).

We ﬁrst provide a valid upper bound for Φk
0 deﬁned in Line 1
when the CALGD method enters the LCG procedure at the
k-th outer iteration. In view of the deﬁnitions of Φk
0 and ψ(
)
·
at Line 1 and (45), respectively, we have, for any k
1,

Φk

0 =

x

ψ(cid:48)
k(xk−1), xk−1
(cid:104)
−
f (cid:48)(x∗)
f (cid:48)(zk)
(
(cid:107)
LD2
X +

−
f (cid:48)(x∗)
(cid:107)

(cid:107)
DX ,
(cid:107)

(cid:105)
+

≤

≤

f (cid:48)(zk), xk−1
=
(cid:104)
f (cid:48)(x∗)
xk−1
)
(cid:107)
(cid:107)
(cid:107)

≥
x

−

(cid:105)

x

−

(cid:107)
(51)

where the ﬁrst inequality follows from Cauchy-Schwarz and
the triangle inequality, and the second inequality follows
from (2) and (4). Note that we always have ηk < αβkD2
X .
X /(cid:15))
Therefore, similar to the stochastic case, our
bound immediately follows from the above relation, (39),
and (49).

(LD2

O

As before in the stochastic case, we can slightly improve
the complexity bound on the calls to the FO oracle in terms
of the dependence on DX .
Corollary B.3. Suppose that there exists an estimate D0
and that the outer iteration limit N
x0
(cid:107)
If

≥
1 is given.

x∗

−

≥

(cid:107)

βk = 2L

k , γk = 2

k+1 , ηk = 2LD2
N k ,

0

for k

1, then

≥

f (yN )

f (x∗)

−

≤

6LD2
N (N +1) .
0

As a consequence, the total number of calls to the FO and
LOsep oracles performed by the CALGD method for ﬁnding
an (cid:15)-solution of (1) can be bound by

(52)

(53)

(54)

(cid:18)

(cid:113)

(cid:19)

D0

L
(cid:15)

O

(cid:17)

(cid:16) LD2
(cid:15)

X

and

O

respectively.

Proof. The proof is similar to Corollary B.2, and hence
omitted.

We generalize the CALGD and CALSGD methods to solve
two other classes of problems frequently seen in machine
learning. In particular, we discuss the CALGD method with
a restarting technique for solving smooth and strongly con-
vex problems in Subsection C.1, and in Subsection C.3 we
extend the CALGD method to solve a special class of non-
smooth problems. Discussions for the similar extensions for
CALSGD method can be found in Subsection C.2 and C.4.

C.1. Strongly convex optimization

In this subsection, we assume that the objective function f
is not only smooth (i.e., (6) holds), but also strongly convex,
that is,

µ > 0 s.t.

∃

f (y)

f (x)

−

− (cid:104)

f (cid:48)(x), y

x

−

(cid:105) ≥

y

µ
2 (cid:107)

x

2,
(cid:107)

−

x, y

∀

X.
∈
(55)

For simplicity, we ﬁrst establish the convergence results
for the deterministic case, i.e., we have access to the exact
gradients of the objective function f .

The shrinking conditional gradient method in (Lan, 2013)
needs to make additional assumptions on the LO oracle to
obtain a linear rate of convergence. However, we will show
now that CALGD (relying on the vanilla weak separation
oracle) can obtain a linear rate of convergence in terms of
X /(cid:15)) rate
the number of calls to the FO oracle and
of convergence in the total number of calls to the LOsep
oracle. In view of the lower complexity bound established
for the LO oracle to solve strongly convex problems in
(Jaggi, 2013; Lan, 2013), our bound for the LOsep oracle
is not improvable.

(LD2

O

We are now ready to formally describe the CALGD method
for solving strongly convex problems, which is obtained by
properly restarting the CALGD method (Algorithm 2).

Conditional Accelerated Lazy Stochastic Gradient Descent

Algorithm 3 The CALGD method for strongly convex prob-
lems

Therefore, the total number of calls to the LOsep oracle can
be bounded by

Input: Initial point p0
satisfying f (p0)
−
for s = 1, 2, . . . do

∈
f (x∗)
≤

X and an estimate δ0 > 0
δ0.

Call the CALGD method in Algorithm 2 with input

x0 = ps−1 and N =

(cid:108)

(cid:113) 6L
2
µ

(cid:109)

,

(56)

and parameters

βk = 2L

k , γk = 2

k+1 , and ηk = ηs,k := 8Lδ02−s
µN k ,
(57)

and let ps be its output solution.

end for

(cid:112)

In Algorithm 3, we restart the CALGD method for smooth
iterations.
optimization (i.e., Algorithm 2) every
We call each loop iteration a phase of the above CALGD
ηk
decrease by a factor of 2 as
algorithm. Observe that
}
{
s increments by 1, while
βk
remain the same.
and
}
{
The following theorem shows the convergence of the above
variant of the CALGD method.

6L/µ
(cid:101)

γk
{

2

}

(cid:100)

Theorem C.1. Assume (55) holds and let
by Algorithm 3. Then,

ps

{

}

be generated

f (ps)

f (x∗)

δ02−s,

s

0.

−

≤
As a consequence, the total number of calls to the FO and
LOsep oracles performed by this algorithm for ﬁnding an
(cid:15)-solution of problem (1) can be bounded by

≥

(cid:110)(cid:113) L
µ

O

(cid:6)log2 max (cid:0)1, δ0

(cid:15)

(cid:1)(cid:7)(cid:111)

and

O

(cid:110) LD2
(cid:15)

X

(cid:111)

,

(58)

respectively.

Proof. Denote the total number of phases performed by
CALGD method to obtain an (cid:15)-solution of (1) by S. In view
of the complexity results obtained in Theorem 2.5 in (Lan
and Zhou, 2014), we conclude that

The total number of calls to the FO oracle performed by
Algorithm 3 is clearly bounded by N S, which immediately
implies our ﬁrst result in (58).

Now, let Ts,k denote the number of calls to the LOsep
oracle required at the k-th outer iteration in the s-th phase.
It follows from Theorem B.1.c), (51), and (57) that

Ts,k

≤ O

(cid:17)

(cid:16) βkD2
X
ηs,k

=

(cid:16) µD2
X 2sN
δ0

(cid:17)

.

O

(cid:80)S

(cid:80)N

s=1

k=1Ts,k

(cid:80)S

(cid:80)N

(cid:16) µD2
X 2sN
δ0

(cid:17)

≤
=

=

O

O

s=1
(cid:16) µD2
X N 2
δ0
(cid:16) µD2
X N 2
δ0

k=1O
(cid:80)S
2S+1(cid:17)

s=12s(cid:17)

=

(cid:16) µD2
X N 2
(cid:15)

(cid:17)

,

O

which implies our second bound in (58) due to the deﬁni-
tions of N and S in (56) and (59), respectively.

In view of classic complexity theory for convex optimiza-
tion, the bound on the total number of calls to the FO oracle
(cf. ﬁrst bound in (58)) is optimal for strongly convex op-
timization. Moreover, in view of the complexity results
established in (Lan, 2013) and the fact that the LOsep or-
acle is weaker than the LO oracle, the bound on the total
number of calls to the LOsep oracle (cf. second bound in
(58)) is not improvable either.

C.2. Strongly convex stochastic optimization

Similarly to the deterministic case we present an optimal
algorithm for solving stochastic smooth and strongly convex
problems.

Algorithm 4 The CALSGD method for solving strongly
convex problems

Input: Initial point p0
satisfying f (p0)
−
for s = 1, 2, . . . do

∈
f (x∗)
≤

X and an estimate δ0 > 0
δ0.

Call the CALSGD method in Algorithm 1 with input

x0 = ps−1 and N =

(cid:108)

(cid:113) 2L
4
µ

(cid:109)

,

(60)

and parameters

βk = 3L

k , γk = 2

k+1 , ηk = ηs,k := 8Lδ02−s
µN k ,

and Bk = Bs,k :=

(cid:108) µσ2N (k+1)2
4L2δ02−s

(cid:109)

,

(61)

The main convergence properties of Algorithm 4 are as
follows.

Theorem C.2. Assume that (55) holds and let
erated by Algorithm 4. Then,

ps
{

}

be gen-

E[f (ps)

f (x∗)]

δ02−s,

−

≤

s

0.

≥

S = (cid:6)log2 max (cid:0)1, δ0

(cid:1)(cid:7) .

(cid:15)

(59)

and let ps be its output solution.

end for

Conditional Accelerated Lazy Stochastic Gradient Descent

As a consequence, the total number of calls to the SFO and
LOsep oracles performed by this algorithm for ﬁnding a
stochastic (cid:15)-solution of problem (1)-(8) can be bounded by

(cid:110) σ2

µ(cid:15) +

(cid:113) L
µ

(cid:6)log2 max (cid:0)1, δ0

(cid:15)

(cid:1)(cid:7)(cid:111)

,

(62)

C.3. Non-smooth optimization: Saddle point problems

For the sake of simplicity, we consider the deterministic
case, i.e., the problem of interest is an important class of
saddle point problems with f given in the form of

O

and

O
respectively.

(cid:111)

(cid:110) LD2
(cid:15)

X

, with probability 1

Λ,

(63)

−

Proof. In view of Corollary 2.5, and Theorem 3.4 in (Lan
and Zhou, 2014), the total number of phases, S, performed
by CALSGD method to ﬁnd a stochastic (cid:15)-solution of prob-
lem (1)-(8) is bounded by (59). Since the number of outer
iterations in each phase is at most N , the total number of
calls to the SFO oracle is bounded by

(cid:80)S

(cid:80)N

k=1Bk

s=1

(cid:80)S

(cid:80)N

s=1

k=1

(cid:16) µσ2N (k+1)2

4L2δ02−s + 1

(cid:17)

µσ2N (N +1)3
12L2δ0
µσ2N (N +1)3
3L2(cid:15)

(cid:80)S

s=12s + SN

+ SN.

≤

≤

≤

≤

Moreover, similar to (31), we obtain a good estimator for
Φs,k

0 , for any 0 < Λ

1

(cid:18)(cid:113)

(cid:19)

Φs,k

0 ≤

4SL2δ0
Λµk22s + 1

LD2

X +

f (cid:48)(x∗)

(cid:107)

∗DX ,
(cid:107)

−

Λ. Let Ts,k denote the number of calls
with probability 1
to the LOsep oracle required at the k-th outer iteration in the
s-th phase of the CALSGD method. It follows from Theo-
rem 2.3.c), the above relation, and (61) that with probability
1

Λ,

−

Ts,k

≤ O

(cid:16)

log Φs,k
0
ηs,k

+ βkD2
X
ηs,k

(cid:17)

=

(cid:16) µD2
X 2sN
δ0

(cid:17)

O

holds. Therefore, the total number of calls to the LOsep
oracle is bounded by

(cid:80)S

(cid:80)N

s=1

k=1Ts,k

(cid:17)

(cid:16) µD2
X 2sN
δ0
s=12s(cid:17)

(cid:80)S

(cid:80)S

(cid:80)N

s=1
(cid:16)

k=1O
X N 2δ−1
µD2
0
(cid:17)
(cid:16) µD2
X N 2
(cid:15)

,

≤
=

=

O

O

which implies the bound in (63), due to the deﬁnitions of N
and S in (60) and (59), respectively.

f (x) = max
y∈Y

(cid:110)
Ax, y
(cid:104)

(cid:105) −

ˆf (y)

(cid:111)

,

(64)

∈

→

Rm denotes a linear operator, Y

Rm is
where A : Rn
→
a convex compact set, and ˆf : Y
R is a simple convex
function. Since the objective function f is non-smooth, we
cannot directly apply the CALGD method presented in the
previous section. However, as shown by Nesterov (Nesterov,
2005), the function f (
) in (64) can be closely approximated
·
by a class of smooth convex functions. More speciﬁcally,
R be a given strongly convex function with
let ω : Y
strongly convex modulus σω > 0, i.e.,

→

ω(y)

ω(x) +

ω(cid:48)(x), y
(cid:104)

x

+ σω

y

2 (cid:107)

x

2,
(cid:107)

≥

∈
−
and let us denote cω := argminy∈Y ω(y), W (y) := ω(y)
ω(cω)

ω(cω), y

and

cω

−

∀

(cid:105)

x, y

Y,

−

− (cid:104)∇

−
(cid:105)
2
Y,W := max
y∈Y

D

W (y).

It can be easily seen that

cω

y
(cid:107)
and hence that

−

2

(cid:107)

≤

2
σω

W (y)

2
σω D

2
Y,W ,

≤

y
∀

∈

Y,

y1
(cid:107)

−

y2

2
(cid:107)

≤

4
σω D

2
Y,W ,

∀

y1, y2

Y.

In view of these relations, the function f (
closely approximated by

∈
) in (64) can be
·

fτ (x) := max
y∈Y

(cid:110)
Ax, y
(cid:104)

(cid:105) −

ˆf (y)

τ [W (y)

−

− D

(cid:111)

.

2
Y,W ]

(65)

In particular, for any τ

0,

f (x)

fτ (x)

≤

≤

2
Y,W ,

D

x

∀

∈

X.

≥
f (x) + τ

Moreover, Nesterov (Nesterov, 2005) shows that fτ (
) is
·
differentiable and its gradients are Lipschitz continuous with
the Lipschitz constant given by

τ := (cid:107)A(cid:107)2
τ σω

.

L

(66)

According to Theorem C.2, the total number of calls to the
(1/(cid:15)), which is optimal in view
SFO oracle is bounded by
of the classic complexity theory for strongly convex opti-
mization (see (Ghadimi and Lan, 2012; 2013)). Moreover,
the total number of calls to the LOsep oracle is bounded by
(1/(cid:15)), which is the same bound as for the CALGD method
O
for strongly convex optimization and hence not improvable.

O

Throughout this subsection, we assume that the feasible
region Y and the function ˆf are simple enough, so that the
subproblem in (65) is easy to solve. Therefore, the major
computational cost for gradient calculations of fτ lie in the
evaluations of the linear operator A and its adjoint operator
AT . We are now ready to present a variant of the CALGD
method, which can achieve optimal bounds on the number

Conditional Accelerated Lazy Stochastic Gradient Descent

of calls to the LOsep oracle and the number of evaluations
of the linear operators A and AT .

Moreover, it follows from (51), (39), (69), (70) and (66) that
the total number of calls to the LOsep oracle is bounded by

Algorithm 5 The CALGD method for solving saddle point
problems

This algorithm is the same as Algorithm 2 except that
(44) is replaced by

xk = LCG(f (cid:48)

τk (zk), βk, xk−1, α, ηk),

(67)

for some τk

0.

≥

In Theorem C.3 we state the main convergence properties
of this modiﬁed CALGD method to solve the saddle point
problem in (1)-(64).

Theorem C.3. Suppose that τ1
assume that
γk
{
Lτk deﬁned in (66)) and (36). Then, for all k

0. Also
satisfy (35) (with L replaced by

βk
{

and

. . .

τ2

1,

≥

≥

≥

}

}

≥
(cid:0)ηi + τi

−

≤

(cid:80)k

f (yk)

f (x∗)

X + Γk

βkγk
2 D2

(cid:1) ,
(68)
where x∗ is an arbitrary optimal solution of (1)-(64). More-
over, the number of inner iterations performed at the k-th
outer iteration is bounded by (39).

2
Y,W

γi
Γi

i=1

D

Proof. The proof is similar to Theorem 4.1 in (Lan and
Zhou, 2014), and hence omitted.

,

}

{

}

{

ηk

γk

,
}

, and

βk
{

We now provide two different sets of parameter settings
τk
for
which can guarantee the
{
optimal convergence of the above variant of the CALGD
method for saddle point optimization. Speciﬁcally, Corol-
lary C.4 gives a static setting for parameter
under the
}
{
assumption that the outer iteration limit N
1 is given,
while a dynamic setting is provided in Corollary C.5.

τk

≥

}

Corollary C.4. Assume the outer iteration limit N
given. If

≥

1 is

τk

τ = 2(cid:107)A(cid:107)DX
DY,W

σωN , k
√

1,

(69)

≥
used in Algorithm 5 are set to

≡
,
, and
γk
}
{
}
{
3Lτk
k+1 , γk = 3

ηk

}

and

βk
{

βk =

k+2 , and ηk =

Lτk D2
k2

X

, k

1, (70)

≥

then the number of linear operator evaluations (for A and
AT ) and the number of calls to the LOsep oracle performed
by Algorithm 5 for ﬁnding an (cid:15)-solution of problem (1)-(64),
respectively, is bounded by

(cid:110) (cid:107)A(cid:107)DX DY,W
√

(cid:111)

σω(cid:15)

O

and

O

(cid:110) (cid:107)A(cid:107)2D2

X D2

Y,W

(cid:111)

σω(cid:15)2

.

(71)

Proof. In view of the result in Corollary 4.2 of (Lan and
Zhou, 2014), our ﬁrst bound in (71) immediately follows.

(cid:80)N

k=1Tk

(cid:80)N

≤
= (cid:80)N

k=1O

k=1O

(cid:17)

X

(cid:16) βkD2
ηk
(cid:16) Lτk D2
k+1

X

(cid:17)

k2
Lτk D2

X

=

(N 2),

O

which implies our second bound in (71).

Corollary C.5. Suppose that parameter

is now set to

τk
{

}

τk = 2(cid:107)A(cid:107)DX
DY,W

σωk , k

√

1,

≥

(72)

,

}

ηk
{

βk
{

, and
}

and the parameters
used in Algo-
γk
{
rithm 5 are set as in (70). Then, the number of linear opera-
tor evaluations (for A and AT ) and the number of calls to
the LOsep oracle performed by Algorithm 5 for ﬁnding an
(cid:15)-solution of problem (1)-(64) is bounded by the two bounds
as given in (71) respectively.

}

Proof. The proof is similar to the Corollary C.4, and hence
omitted.

In view of the discussions in (Chen et al., 2014), the obtained
bound on the total number of operator evaluations (cf. ﬁrst
bound in (71)) is not improvable for solving the saddle
point problems in (1)-(64). Moreover, according to (Lan,
2013) and the fact that the LOsep oracle is weaker than LO
(1/(cid:15)2) bound on the total number of calls to
oracle, the
the LOsep is not improvable.

O

C.4. Non-smooth stochastic optimization: stochastic

saddle point problems

In this subsection, we brieﬂy discuss stochastic saddle point
problems, i.e., only stochastic gradients of fτ (cf. (65)) are
available. In particular, we consider the situation when the
original objective function f in (1) is given by

(cid:20)

f (x) = E

max
y∈Y (cid:104)

Aξx, y

(cid:105) −

(cid:21)
ˆf (y, ξ)

,

(73)

where ˆf (
·
Aξ is a random linear operator such that

, ξ) is simple concave function for all ξ

∈

Ξ and

2(cid:3)
(cid:107)
We can solve this stochastic saddle point problem by replac-
ing (67) with

E (cid:2)
Aξ
(cid:107)

L2
A

(74)

≤

xk = LCG(gk, xk−1, βk, ηk),

(cid:80)Bk

j=1 F (cid:48)
1. By properly specifying

where gk = 1
Bk
Bk
Bk
{
evaluations (for Aξ and AT

0 and
, and
≥
, we can show that the number of linear operator
}
ξ ) and the number of calls to

τk (zk, ξk,j) for some τk
βk
,
}

≥
τk
{

ηk
{

}

{

}

,

Conditional Accelerated Lazy Stochastic Gradient Descent

the LOsep oracle performed by this variant of CALSGD
method for ﬁnding a stochastic (cid:15)-solution of problem (1)-
(73) is bounded by

of all Hamiltonian cycles of graphs of different size. In
Figure 8 the polytopes are the standard formulation of the
cut problem and the Birkhoff polytope.

D.3. Convex optimization over spectrahedra

In this section we consider instances of the problem of
ﬁnding the minimum of a convex function over the standard
spectrahedron, which is deﬁned as

Sn :=

X
{

Rn×n

X (cid:60) 0, tr(X) = 1
.
}

∈

|
In this case the linear minimization problem for an objective
function C is solved by computing an eigenvector for the
C. We use the same method to
largest eigenvalue of
implement LOsepSn .
We show results on three different sized instances, in Fig-
ure 9 for n = 50, Figure 10 for n = 100 and in Figure 11
for n = 150.

−

and

(cid:110) L2

AD2

X D2
σω(cid:15)2

Y,W

(cid:111)

,

(cid:110) L2

AD2

X D2
σω(cid:15)2

Y,W

(cid:111)

O

O

with probability 1
Λ respectively. This result can be
proved by combining the techniques in Section 2 and those
in Theorem C.3. However, we skip the details of these
developments for the sake of simplicity.

−

D. Experimental results

b

(cid:107)

−

×

Ax

We now provide additional experimental results in this sec-
tion. The setup of the problems is as described in Section 3,
2 as the objective function where A is a
i.e., we use
(cid:107)
n matrix with n being the dimension of the underlying
m
feasible region and m being the number of examples. In
each example we use a density parameter d specifying the
fraction of non-zero entries in A. We compute b = Ax∗
with some feasible point x∗ so that in all examples the op-
timal value is 0. Albeit the theoretical number of samples
Bk (see Equation (26)) needed for CALSGD we use a batch
size of 128 for each gradient computation in all algorithms
for comparability. The function values that we report are
calculated using the full matrix A, however since they are
not used by either algorithm, each algorithm has only the
information provided by the 128 examples sampled in that
speciﬁc round.

We implemented all algorithms using Python 2.7 using
Gurobi 7.0 (Gurobi Optimization, 2016) as the solver
for our linear models.

D.1. Video co-localization

Video co-localization is the problem of identifying and ob-
ject over multiple frames of a video. As shown by (Joulin
et al., 2014) this problem can be solved by quadratic pro-
gramming over a path/ﬂow polytope. In Figures 2, 3 and
7 we show that our algorithm CALSGD performs signiﬁ-
cantly better than OFW on this type of instances. We use
path polytopes available at http://lime.cs.elte.
hu/~kpeter/data/mcf/road/. The non-zero en-
tries of A in this section are chosen uniformly from [0, 1]
and the density parameter we used is d = 0.8.

D.2. Structured regression

For our structured regression instances we solve the objec-
2 as described before over different
tive function
polytopes. In Figure 6 the feasible region is the convex hull

Ax
(cid:107)

b
(cid:107)

−

Conditional Accelerated Lazy Stochastic Gradient Descent

Figure 7. Two large video co-localization instances. On the left: road_paths_03_NH_a instance (n = 262958 and m = 10000).
On the right: road_paths_03_NH_b instance (n = 262958 and m = 10000). CALSGD has a better performance in both, iterations
and wall clock time.

081624Iterations1031041051061071081091010FunctionvalueCALSGDOFW081624Iterations1031041051061071081091010FunctionvalueCALSGDOFW0150300450Wallclocktime1031041051061071081091010FunctionvalueCALSGDOFW0150300450Wallclocktime1031041051061071081091010FunctionvalueCALSGDOFWConditional Accelerated Lazy Stochastic Gradient Descent

Figure 8. Structured regression problem over the cut polytope for a graph on 23 vertices on the left and the Birkboff polytope containing
all doubly stochastic matrices of size 100 × 100 on the right. In both cases we used m = 10000 rows for the matrix A, on the left a
density of d = 0.6 and on the right d = 0.8. In both cases the number of iterations computed in the given time between CALSGD and
OFW is quite signiﬁcant, however in both cases CALSGD achieves better function values in the smaller number of iterations. In the
example of the Birkhoff polytope it almost looks like as if OFW converges suboptimally, however this is due to the large number of
iterations required: the convergence rate of OFW as shown by (Hazan and Kale, 2012) is O(T −1/4), so if we compute the improvement
with logarithmic scale, from, e.g., iteration 1500 to iteration 4500, we get −1/4(log(1500) − log(4500)) ≈ 0.12 (the constants hidden
in the O-notation get canceled due to the logarithm and the difference) and therefore indeed ﬁts to the observation on the graph.

080016002400Iterations10−1100101102103104105FunctionvalueCALSGDOFW0150030004500Iterations10−1100101102FunctionvalueCALSGDOFW02000400060008000Wallclocktime10−1100101102103104105FunctionvalueCALSGDOFW0150300450Wallclocktime10−1100101102FunctionvalueCALSGDOFWConditional Accelerated Lazy Stochastic Gradient Descent

Figure 9. Quadratic optimization over the standard spectrahedron of size n = 50. On the left we use m = 10000 on the right m = 20000.
In both cases CALSGD performs better than OFW both in iterations as well as in wall clock time. As described in Figure 8 the impression
of suboptimal convergence of OFW can be explained by the very high number of iterations required.

02000400060008000Iterations10−410−310−210−1100101102103FunctionvalueCALSGDOFW01000200030004000Iterations10−310−210−1100101102103FunctionvalueCALSGDOFW0255075100Wallclocktime10−410−310−210−1100101102103FunctionvalueCALSGDOFW0255075100Wallclocktime10−310−210−1100101102103FunctionvalueCALSGDOFWConditional Accelerated Lazy Stochastic Gradient Descent

Figure 10. Medium sized quadratic optimization over the standard spectrahedron (n = 100). Again we chose m = 10000 on the left and
m = 20000 on the right. The CALSGD method achieves values multiple orders of magnitude better within the given time window.

050010001500Iterations10−410−310−210−1100101102103FunctionvalueCALSGDOFW02505007501000Iterations10−410−310−210−1100101102103FunctionvalueCALSGDOFW0255075100Wallclocktime10−410−310−210−1100101102103FunctionvalueCALSGDOFW0255075100Wallclocktime10−410−310−210−1100101102103FunctionvalueCALSGDOFWConditional Accelerated Lazy Stochastic Gradient Descent

Figure 11. Large quadratic optimization over the standard spectrahedron (n = 150), with m = 10000 on the left and m = 20000 on the
right. The behaviour and the achieved objective function values are very similar to the medium size instances in Figure 10.

0200400600800Iterations10−410−310−210−1100101102103FunctionvalueCALSGDOFW0100200300400Iterations10−410−310−210−1100101102103FunctionvalueCALSGDOFW0255075100Wallclocktime10−410−310−210−1100101102103FunctionvalueCALSGDOFW0255075100Wallclocktime10−410−310−210−1100101102103FunctionvalueCALSGDOFW