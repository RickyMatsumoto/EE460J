Supplementary Material for
Learning Continuous Semantic Representations of Symbolic Expressions

Miltiadis Allamanis 1 Pankajan Chanthirasegaran 2 Pushmeet Kohli 3 Charles Sutton 2 4

1. Synthetic Expression Datasets

Table 1 and Table 2 are sample expressions within an equiv-
alence class for the two types of datasets we consider.

2. Detailed Evaluation

Figure 1 presents the detailed evaluation for our k-NN met-
ric for each dataset. Figure 2 shows the detailed evaluation
when using models trained on simpler datasets but tested
on more complex ones, essentially evaluating the learned
compositionality of the models. Figure 4 show how the
performance varies across the datasets based on their char-
acteristics. As expected as the number of variables increase,
the performance worsens (Figure 4a) and expressions with
more complex operators tend to have worse performance
(Figure 4b). The results for UNSEENEQCLASS look very
similar and are not plotted here.

3. Model Hyperparameters

The optimized hyperparameters are detailed in Table 3.
All hyperparameters were optimized using the Spearmint
(Snoek et al., 2012) Bayesian optimization package. The
same range of values was used for all common model hy-
perparameters.

References

Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. Practi-
cal Bayesian optimization of machine learning algorithms.
In NIPS, 2012.

Socher, Richard, Huval, Brody, Manning, Christopher D,
and Ng, Andrew Y. Semantic compositionality through
recursive matrix-vector spaces. In EMNLP, 2012.

Work started when M. Allamanis was at Edinburgh. This work
was done while P. Kohli was at Microsoft. 1Microsoft Research,
Cambridge, UK 2University of Edinburgh, UK 3DeepMind, Lon-
don, UK 4The Alan Turing Institute, London, UK. Correspondence
to: Miltiadis Allamanis <t-mialla@microsoft.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

Learning Continuous Semantic Representations of Symbolic Expressions

(¬a) ∧ (¬b)

(¬a ∧ ¬c) ∨ (¬b ∧ a ∧ c) ∨ (¬c ∧ b)

(¬a) ∧ b ∧ c

a¬((¬a) ⇒ ((¬a) ∧ b))
¬((b ∨ (¬(¬a))) ∨ b)
(¬a) ⊕ ((a ∨ b) ⊕ a)
(b ⇒ (b ⇒ a)) ∧ (¬a)
((¬a) ⇒ b) ⇒ (a ⊕ a)

False

(a ⊕ a) ∧ (c ⇒ c)
(¬b) ∧ (¬(b ⇒ a))
b ∧ ((a ∨ a) ⊕ a)
((¬b) ∧ b) ⊕ (a ⊕ a)
c ∧ ((¬(a ⇒ a)) ∧ c)

BOOL8

c ⊕ (((¬a) ⇒ a) ⇒ b)
¬((b ⊕ (b ∨ a)) ⊕ c)
¬((¬(b ∨ (¬a))) ⊕ c)
((b ∨ a) ⊕ (¬b)) ⊕ c)
(¬((b ⊕ a) ∧ a)) ⊕ c

(¬a) ∧ (¬b) ∨ (∧c)

(a ⇒ (¬c)) ⊕ (a ∨ b)
(a ⇒ (c ⊕ b)) ⊕ b
b ⊕ (a ⇒ (b ⊕ c))
(b ∨ a) ⊕ (x ⇒ (¬a))
b ⊕ ((¬a) ∨ (c ⊕ b))

Table 1. Sample of BOOL8 data.

¬((¬b) ∨ ((¬c) ∨ a))
((a ∨ b) ∧ c) ∧ (¬a)
(¬((¬(¬b)) ⇒ a)) ∧ c
(c ∧ (c ⇒ (¬a))) ∧ b
b ∧ (¬(b ∧ (c ⇒ a)))

¬a ∨ b

a ⇒ ((b ∧ (¬c)) ∨ b)
¬(¬((b ∨ a) ⇒ b))
(¬a) ⊕ (¬(b ⇒ (¬a)))
b ∨ (¬((¬b) ∧ a))
¬((a ⇒ (a ⊕ b)) ∧ a)

−a − c

POLY8
c2

(b − a) − (c + b)
b − (c + (b + a))
a − ((a + a) + c)
(a − (a + a)) − c
(b − b) − (a + c)

(c · c) + (b − b)
((c · c) − c) + c
((b + c) − b) · c
c · (c − (a − a))
c · c

b2c2

(b · b) · (c · c)
c · (c · (b · b))
(c · b) · (b · c)
((c · b) · c) · b
((c · c) · b) · b

c

b · c

b − c

c − ((c − c) · a)
c − ((a − a) · c)
((a − a) · b) + c
(c + a) − a
(a · (c − c)) + c

(c − (b − b)) · b
(b − (c − c)) · c
(b − b) + (b · c)
c · ((b − c) + c)
(b · c) + (c − c)

(a − (a + c)) + b
(a − c) − (a − b)
(b − (c + c)) + c
(b − (c − a)) − a
b − ((a − a) + c)

Table 2. Sample of POLY8 data.

Learning Continuous Semantic Representations of Symbolic Expressions

(a) SEENEQCLASS evaluation using model trained on the respective training set.

(b) UNSEENEQCLASS evaluation using model trained on the respective training set.

Figure 1. Evaluation of scorex (y axis) for x = 1, . . . , 15. on the respective SEENEQCLASS and UNSEENEQCLASS where each model
has been trained on. The markers are shown every ﬁve ticks of the x-axis to make the graph more clear. TREENN refers to the model of
Socher et al. (2012).

Model

EQNET

Table 3. Hyperparameters used in this work.

Hyperparameters
learning rate 10−2.1, rmsprop ρ = 0.88, momentum 0.88, minibatch size 900, repre-
sentation size D = 64, autoencoder size M = 8, autoencoder noise κ = 0.61, gradient
clipping 1.82, initial parameter standard deviation 10−2.05, dropout rate .11, hidden
layer size 8, ν = 4, curriculum initial tree size 6.96, curriculum step per epoch 2.72,
objective margin m = 0.5

1-layer-TREENN learning rate 10−3.5, rmsprop ρ = 0.6, momentum 0.01, minibatch size 650, represen-
tation size D = 64, gradient clipping 3.6, initial parameter standard deviation 10−1.28,
dropout 0.0, curriculum initial tree size 2.8, curriculum step per epoch 2.4, objective
margin m = 2.41

GRU

2-layer-TREENN learning rate 10−3.5, rmsprop ρ = 0.9, momentum 0.95, minibatch size 1000, repre-
sentation size D = 64, gradient clipping 5, initial parameter standard deviation 10−4,
dropout 0.0, hidden layer size 16, curriculum initial tree size 6.5, curriculum step per
epoch 2.25, objective margin m = 0.62
learning rate 10−2.31, rmsprop ρ = 0.90, momentum 0.66, minibatch size 100, rep-
resentation size D = 64, gradient clipping 0.87, token embedding size 128, initial
parameter standard deviation 10−1, dropout rate 0.26
learning rate 10−2.9, rmsprop ρ = 0.99, momentum 0.85, minibatch size 500, represen-
tation size D = 64, gradient clipping 0.70, token embedding size 64, RNN parameter
weights initialization standard deviation 10−4, embedding weight initialization standard
deviation 10−3, dropout 0.0, stack count 40

StackRNN

510SIMPBOOL10510BOOL5510BOOL8510BOOL10510SIMPBOOLL5510BOOLL5510SIMPPOLY8510SIMPPOLY10510ONEV-POLY10510ONEV-POLY13510POLY55100.00.20.40.60.81.0POLY8510SIMPBOOL10510BOOL5510BOOL8510BOOL10510SIMPBOOLL5510BOOLL5510SIMPPOLY8510SIMPPOLY10510ONEV-POLY10510ONEV-POLY13510POLY55100.00.20.40.60.81.0POLY8Learning Continuous Semantic Representations of Symbolic Expressions

(a) SEENEQCLASS evaluation using model trained on simpler datasets. Caption is “model trained on”→“Test dataset”.

(b) Evaluation of compositionality. UNSEENEQCLASS evaluation using model trained on simpler datasets. Caption is “model trained
on”→“Test dataset”.

Figure 2. Evaluation of compositionality. Evaluation of scorex (y axis) for x = 1, . . . , 15. The markers are shown every ﬁve ticks of the
x-axis to make the graph more clear. TREENN refers to the model of Socher et al. (2012).

(a) SEENEQCLASS

(b) UNSEENEQCLASS

Figure 3. Receiver operating characteristic (ROC) curves averaged across datasets.

510BOOLL5→BOOL8510BOOL5→BOOL8510BOOL5→BOOL10510POLY8→SIMPPOLY8510SIMPPOLY5→SIMPPOLY10510SIMPPOLY8→SIMPPOLY10510POLY5→SIMPPOLY10510POLY8→SIMPPOLY10510ONEV-POLY10→ONEV-POLY13510POLY8→ONEV-POLY135100.00.20.40.60.81.0POLY5→POLY8510BOOLL5→BOOL8510BOOL5→BOOL8510BOOL5→BOOL10510POLY8→SIMPPOLY8510SIMPPOLY5→SIMPPOLY10510SIMPPOLY8→SIMPPOLY10510POLY5→SIMPPOLY10510POLY8→SIMPPOLY10510ONEV-POLY10→ONEV-POLY13510POLY8→ONEV-POLY135100.00.20.40.60.81.0POLY5→POLY8tf-idfGRUStackRNNTreeNN-1LayerTreeNN-2LayerEqNet0.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRatetf-idfGRUStackRNNTreeNN-1LayerTreeNN-2LayerEqNet0.00.20.40.60.81.0FalsePositiveRate0.00.20.40.60.81.0TruePositiveRatetf-idfGRUStackRNNTreeNN-1LayerTreeNN-2LayerEqNetLearning Continuous Semantic Representations of Symbolic Expressions

(a) Performance vs. Number of Variables

(b) Performance vs. Operator Complexity

Figure 4. EQNET performance on SEENEQCLASS for various dataset characteristics

510k0.20.40.60.81.0scorek1Var3Vars10Vars510k0.20.40.60.81.0scorekSimpleAll