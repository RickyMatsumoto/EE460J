Uniform Convergence Rates for Kernel Density Estimation

Heinrich Jiang 1

Abstract

Kernel density estimation (KDE) is a popular
nonparametric density estimation method. We
(1) derive ﬁnite-sample high-probability density
estimation bounds for multivariate KDE under
mild density assumptions which hold uniformly
in x ∈ Rd and bandwidth matrices. We ap-
ply these results to (2) mode, (3) density level
set, and (4) class probability estimation and at-
tain optimal rates up to logarithmic factors. We
then (5) provide an extension of our results un-
der the manifold hypothesis. Finally, we (6) give
uniform convergence results for local intrinsic di-
mension estimation.

1. Introduction

KDE (Rosenblatt, 1956; Parzen, 1962) is a foundational as-
pect of nonparametric statistics. It is a powerful method to
estimate the probability density function of a random vari-
able. Moreover, it is simple to compute and has played
a signiﬁcant role in a very wide range of practical appli-
cations. Its convergence properties have been studied for a
long time with most of the work dedicated to its asymptotic
behavior or mean-squared risk (Tsybakov, 2008). How-
ever, there is still a surprising amount not yet fully under-
stood about its convergence behavior.
In this paper, we
focus on the uniform ﬁnite-sample facet of KDE conver-
gence theory. We handle the multivariate KDE setting in
Rd which allows a d × d bandwidth matrix H. This gen-
eralizes the scalar bandwidth h > 0 i.e. H = h2I. Such
a generalization is signiﬁcant to multivariate statistics e.g.
Silverman (1986); Simonoff (1996).

Our work begins by using VC-based Bernstein-type uni-
form convergence bounds to attain ﬁnite-sample rates for
a ﬁxed unknown density f over Rd (Theorem 1). These
bounds hold with high-probability under general assump-
tions on f and the kernel i.e. we only require f to be

1Google.

Correspondence to: Heinrich Jiang <hein-

rich.jiang@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

bounded as well as decay assumptions on the kernel func-
tions. Moreover, these bounds hold uniformly over Rd and
bandwidth matrices H.

We then show the versatility of our results by applying it to
the related areas of KDE rates under (cid:96)∞, mode estimation,
density level-set estimation, and class probability estima-
tion. We then extend our analysis to the manifold setting.
Finally, we provide uniform ﬁnite-sample results for local
intrinsic dimension estimation. Each of these contributions
are signiﬁcant on their own.

2. Contributions and Related Works

ˆfh|∞.

(cid:96)∞ bounds for KDE: It must ﬁrst be noted that bounding
| (cid:98)fh − f |∞ where (cid:98)fh is the KDE of f for scalar h > 0
is a more difﬁcult problem than for example bounding the
mean-squared error Ef [( (cid:98)fh − f )2]. Gine & Guillon (2002)
and Einmahl & Mason (2005) give asymptotic convergence
results on KDE for | ˆfh − Ef
In their work about
density clustering, Rinaldo & Wasserman (2010) extends
the results of the former to obtain high-probability ﬁnite-
sample bounds. This is to our knowledge the strongest and
most general uniform ﬁnite-sample result about KDE thus
far.
We show a general bound of form | (cid:98)fh(x) − f (x)| (cid:46) (cid:15)x +
(cid:112)log n/nhd where (cid:15)x is a function of the kernel and the
smoothness of f at x which holds with probability 1 − 1/n
uniformly over x ∈ Rd and h (Theorem 1). An almost
direct consequence is that if we take f to be α-H¨older con-
tinuous then under the optimal choice for h ≈ n−1/(2α+d),
we have | ˆfh − f |∞ (cid:46) n−α/(2α+d) with probability 1 − 1/n
(Theorem 2). This matches the known lower bound (Tsy-
bakov, 2008).

When comparing our ﬁnite-sample results to that of Ri-
naldo & Wasserman (2010), there are a few notable differ-
ences. Our results hold uniformly across bandwidths and
the probability that the bounds hold are independent of the
bandwidth (in fact, holds with probability 1 − 1/n). Our
results also extends to general bandwidth matrix H.

This can be signiﬁcant to analyze KDE-based procedures
with adaptive bandwidths– i.e. when the bandwidths
change depending on the region. Then the need for bounds
which hold simultaneously over bandwidth choices be-

Uniform Convergence Rates for Kernel Density Estimation

comes clear. Such an example includes adaptive or vari-
able KDE (Terrell & Scott, 1992; Botev et al., 2010) which
extends KDE to bandwidths that vary over the data space.

Thus our result for uniform ﬁnite-sample KDE bounds can
be seen as a reﬁnement to existing results.

Mode estimation Estimating the modes of a distribution
has a long history e.g. Parzen (1962); Chernoff (1964);
Eddy (1980); Silverman (1981); Cheng (1995); Abraham
et al. (2004); Li et al. (2007); Dasgupta & Kpotufe (2014);
Genovese et al. (2015); Jiang & Kpotufe (2017). The
modes can be viewed as the central tendancies of a distri-
bution and this line of work has played a signiﬁcant role in
areas such as clustering, image segmentation, and anomaly
detection.

Much of
the early work focused on the estimator
argmaxx∈Rd (cid:98)fh(x). While many useful insights have come
from studying this, it is difﬁcult to algorithmically com-
pute. Abraham et al. (2004) turned to the simple estima-
tor argmaxx∈X (cid:98)fh(x) and showed that it behaves asymp-
totically as argmaxx∈Rd (cid:98)fh(x) where X is the data. In this
paper, we show that this estimator is actually a rate-optimal
estimator of the mode under ﬁnite samples with appropri-
ate bandwidth choice. This would not have been possible
without the appropriate bounds on KDE. This approach is
similar to that of Dasgupta & Kpotufe (2014), who apply
their k-NN density estimation bounds to show that the k-
NN analogue of the estimator is rate-optimal.

Another approach to mode estimation that must be noted
is mean-shift (Fukunaga & Hostetler, 1975; Cheng, 1995;
Comaniciu & Meer, 2002; Arias-Castro et al., 2016), which
is a popular clustering algorithm amongst practitioners
based on performing a gradient-ascent of the KDE. Its theo-
retical analysis however is still far from complete; the difﬁ-
culty comes from analyzing KDE’s ability to estimate gra-
dients. Here we are focused on density estimation rather
than density derivative estimation so our results do not ap-
pear immediately applicable to mean-shift.

Density level-set estimation The problem of density-level
set estimation has been extensively studied e.g. Carmichael
et al. (1968); Hartigan (1975); Cuevas & Fraiman (1997);
Tsybakov (1997); Cadre (2006); Rigollet & Vert (2009);
Singh et al. (2009); Rinaldo & Wasserman (2010); Stein-
It involves estimating {x :
wart (2011); Jiang (2017).
f (x) ≥ λ} for some λ > 0 and density f based on sam-
ples drawn from f . This turns out to be one of the earliest
and still currently most popular means of modeling clusters
in the context of density-based clustering. The level-sets
also inﬂuenced much of the work on hierarchical cluster-
ing (Chaudhuri & Dasgupta, 2010).

Naturally, we must use some density estimator to get a han-
dle on λ. It turns out that in order to obtain the most gen-

eral uniform recovery bounds (e.g. ﬁnite-sample Hausdorff
rates (Singh et al., 2009)), one also needs similar uniform
density estimation bounds. The strongest known results
thus far use density estimators that are often impractical
(e.g. histogram density estimator) in order to obtain these
theoretical rates over a practical one such as KDE. Much
of the work, especially ones using more practical density
estimators have focused on bounding metrics such as sym-
metric set difference, which are computed as an expecta-
tion over f . This is considerably weaker than the Haus-
dorff metric, which imposes a uniform guarantee over each
estimated point and each point in the level-set.

We show that a simple KDE-based estimator is consistent
under the Hausdorff metric; moreoever when the band-
width is appropriately chosen, it attains the minimax op-
timal rate established by Tsybakov (1997).

Class probability estimation Class probability estimation
involves estimating the probability distribution over a set
of classes for a given input.
In other words, it is an ap-
proach to classiﬁcation which involves ﬁrst estimating the
marginal density f (Y |X) (where X is the observation and
Y is its category) and then choosing the category with high-
est probability. This density-based approach to classiﬁca-
tion has been studied in many places under nonparamet-
ric assumptions. e.g. Rigollet (2007); Chaudhuri et al.
(2009). However, there are still aspects about its conver-
gence properties that haven’t been fully understood. In the
current work, we give uniform rates on the approximation
of f (Y |X). Much of the related work assume the binary
classiﬁcation case and derive a hard classiﬁer based on the
marginal density and compare the risk between that and
the Bayes-optimal classiﬁer. Our work differs in that we
give uniform bounds on the recovery of the marginal den-
sity, which is a considerably stronger notion of consistency.
This is important in situations where a worst-case bound on
classiﬁer performance is required.

Density Estimation on Manifolds Density estimation on
manifolds has received much less attention than the full-
dimensional counterpart. However, understanding density
estimation in situations where the intrinsic dimension can
be much lower than the ambient dimension is becoming
ever more important: modern systems are able to capture
data at an increasing resolution while the number of de-
grees of freedom stays relatively constant. One of the lim-
iting aspects of density-based approaches is their perfor-
mance in high dimensions. It takes an exponential in di-
mension number of samples to estimate the density – this
is the so-called curse of dimensionality. Here we give re-
sults whose rates of convergence depend on the dimension
of the manifold dM compared to a much higher ambient di-
mension d; thus the convergence properties become much
more attractive under the manifold hypothesis.

Uniform Convergence Rates for Kernel Density Estimation

Local Intrinsic Dimension Estimation Many learning al-
gorithms require the intrinsic dimension as an input in order
to take advantage of the lower dimensional structure that
arises. There has been much work on estimating the intrin-
sic dimension of the data given ﬁnite samples e.g. (K´egl,
2003). However, the more interesting problem of estimat-
ing the local intrinsic dimension has received much less at-
tention. The bulk of the work in this area e.g. (Costa et al.,
2005; Houle, 2013; Amsaleg et al., 2015) provide inter-
esting estimators, but are unable to establish strong ﬁnite-
sample guarantees under nonparametric assumptions.
In
this paper, we consider a simple notion of local intrinsic
dimension based on the doubling dimension and utilize a
simple estimator. We then give a uniform ﬁnite-sample
convergence result for the estimator under nonparametric
assumptions. To the best of our knowledge, this is perhaps
the strongest ﬁnite-sample result obtained this far for this
problem.

3. Background and Setup

Deﬁnition 1. Let f be a probability density over Rd with
corresponding distribution F. Let X = {X1, ..., Xn} be n
i.i.d. samples drawn from it and let Fn denote the empirical
distribution w.r.t. X. i.e. Fn(A) = 1
n

i=1 1{Xi ∈ A}.

(cid:80)n

We only require that f is bounded.

Assumption 1. ||f ||∞ < ∞.
Deﬁnition 2. Deﬁne kernel function K : Rd → R≥0 where
R≥0 denotes the non-negative real numbers such that

(cid:90)

Rd

K(u)du = 1.

We make the following mild regularity assumptions on K.

There

(Spherically
Assumption 2.
increasing)
exists
k : R≥0 → R≥0 such that K(u) = k(|u|) for u ∈ Rd.
Assumption 3.
ρ, Cρ, t0 > 0 such that for t > t0,

Symmetric
non-increasing

(Exponential Decays) There

and

non-
function

exists

k(t) ≤ Cρ · exp(−tρ).

Remark 1. These assumptions allow the popular kernels
such as Gaussian, Exponential, Silverman, uniform, trian-
gular, tricube, Cosine, and Epanechnikov.

Assumption 3 implies the next result which will be useful
later on. The proof is elementary and is omitted.

Lemma 1. For all m > 0, we have

(cid:90)

Rd

K(u)|u|mdu < ∞.

Deﬁnition 3 (Bandwidth matrix). H is a valid bandwidth
matrix if it is a positive deﬁnite and symmetric d×d matrix.
H0 is a unit bandwidth matrix if it is a valid bandwidth
matrix and |H0| = 1.

Let σ1(H0) ≥ · · · ≥ σd(H0) > 0 be the eigenvalues of
H0.
Remark 2. In the scalar bandwidth case, H0 = I.
Remark 3. It will be useful later on that if H = h2H0
where H0 is a unit bandwidth, then for u ∈ Rd,

(cid:112)σd(H0) · h · |u| ≤ |H1/2u| ≤ (cid:112)σ1(H0) · h · |u|.

Deﬁnition 4 (Kernel Density Estimation). Given a kernel
K and h > 0 and H0, the KDE for H := h2H0 is given
by

(cid:98)fH(x) :=

· |H|−d/2

1
n

(cid:16)

(cid:17)
H−1/2(x − Xi)

K

=

1
n · hd

n
(cid:88)

i=1

K

H0

−1/2(x − Xi)

(cid:33)

.

h

n
(cid:88)

i=1
(cid:32)

4. Uniform Convergence Bounds

The following is a paraphrase of Bousquet et al. (2004),
which was given in Chaudhuri & Dasgupta (2010).
Lemma 2. Let G be a class of functions from X to {0, 1}
with VC dimension d < ∞, and F a probability distri-
bution on X. Let E denote expectation with respect to F.
Suppose n points are drawn independently at random from
F; let En denote expectation with respect to this sample.
Then with probability at least 1 − 1/n, the following holds
for all g ∈ G:

− min{βn

(cid:112)Eng, β2
n + βn
≤ Eg − Eng ≤ min{β2
where βn ≥ (cid:112)4(d + 3) log 2n/n.

(cid:112)Eg}
n + βn

(cid:112)Eng, βn

(cid:112)Eg},

Chaudhuri & Dasgupta (2010) takes G to be the indicator
functions over balls. Dasgupta & Kpotufe (2014) uses this
to provide similar bounds for the k-NN density estimator
as in this paper. Here, we extend this idea to ellipsoids
by taking G = B (the indicator functions over ellipsoids),
which has VC dimension (d2 + 3d)/2 as determined by
Akama & Irie (2011).
Lemma 3. Deﬁne ellipsoid BH0(x, r) := {x(cid:48) ∈ Rd :
−1/2(x − x(cid:48))| ≤ r}, and B := {BH0 (x, r) : x ∈
|H0
Rd, r > 0, H0 is a unit bandwidth}. With probability at
least 1 − 1/n, the following holds uniformly for every
B ∈ B and γ ≥ 0:

F(B) ≥ γ ⇒ Fn(B) ≥ γ − βn
F(B) ≤ γ ⇒ Fn(B) ≤ γ + βn

√

√

γ − β2
n,
γ + β2
n,

Uniform Convergence Rates for Kernel Density Estimation

where βn = 8d(cid:112)log n/n.
Remark 4. We could have alternatively used a ﬁxed con-
ﬁdence δ so that our results would hold with probability at
least 1 − δ. This would only require a modiﬁcation of βn
(e.g. by taking βn = 4d(cid:112)2(log n + log(1/δ))/n). In this
paper, we have simply taken δ = 1/n.

5. KDE Bound

Deﬁne the following which characterizes how much the
density can respectively decrease and increase from x in
B(x, r).
Deﬁnition 5.

ˇux(r) := f (x) − inf

f (x(cid:48)).

x(cid:48)∈B(x,r)

ˆux(r) := sup

f (x(cid:48)) − f (x).

x(cid:48)∈B(x,r)

The ﬁrst are general upper and lower bounds for (cid:98)fH.
Theorem 1. [Uniform Upper and Lower Bounds for (cid:98)fH]
Let vd be the volume of the unit ball in Rd. Then the follow-
ing holds uniformly in x ∈ Rd, (cid:15) > 0, unit bandwidths H0,
and h > (log n/n)1/d with probability at least 1 − 1/n.
Let H := h2H0.

(cid:98)fH(x) > f (x) − (cid:15) − C

log n
n · hd ,
Rd K(u) · ˇux(h|u|/(cid:112)σd(H0))du < (cid:15), and

if (cid:82)

(cid:114)

(cid:98)fH(x) < f (x) + (cid:15) + C

(cid:114)

log n
n · hd ,

√

√

(cid:0)(cid:82) ∞

0 k(t) · td/2dt + 1(cid:1) + 64d2 · k(0).

Rd K(u) · ˆux(h|u|/(cid:112)σd(H0))du < (cid:15), where C =

if (cid:82)
8d(cid:112)·vd · ||f ||∞
σd) and
Remark 5. The conditions on ˇux(h|u|/
σd) can be interpreted as a bound on
ˆux(h|u|/
their expectations over the probability measure K (i.e.
(cid:82)
Rd K(u)du = 1). These conditions can be satisﬁed by
taking h sufﬁciently small.
Remark 6. The parameter (cid:15) allows us the amount of slack
in the estimation errors. This is useful in a few aspects.
Oftentimes, we don’t require tight bounds, especially when
reasoning about low density regions thus having a large (cid:15)
allows us to satisfy the conditions more easily. In the case
that we want tight bounds, the additive error controlled by
the pointwise smoothness of the density can be encoded in
(cid:15), so to not require global smoothness assumptions.
Remark 7. Besides the ||f ||∞ factor, the value of C at the
end of the theorem statement is a quantity which can be
known without any a priori knowledge of f . We can bound
||f ||∞ in terms of known quantities given smoothness as-
sumptions near argmaxxf (x). This is used in later results
where knowing a value of C is important.

∞
(cid:88)

j=0

∞
(cid:88)

j=0

In order to prove Theorem 1, we ﬁrst deﬁne the following
two functions which serve to approximate K as a step-wise
linear combination of uniform kernels.
Deﬁnition 6. Let ∆ > 0.

K∆(u) :=

(k(j∆) − k((j + 1)∆)) · 1 {|u| < j∆} ,

K∆(u) :=

(k(j∆) − k((j + 1)∆)) · 1 {|u| < (j + 1)∆} .

Then it is clear that the following holds for all ∆ > 0.

K∆(u) ≤ K(u) ≤ K∆(u).

The next Lemma is useful in computing the expectations of
functions over the kernel measure.

Lemma 4. Suppose g is an integrable function over R≥0
and let vd denote the volume of a unit ball in Rd. Then

(cid:90)

Rd

K(u)g(|u|)du = vd ·

k(t) · tdg(|u|)du.

(cid:90) ∞

0

Proof of Lemma 4. Let Sd = 2πd/2/Γ(d/2) denote the
surface area of the unit ball in Rd.
(cid:90)

(cid:90) ∞

K(u)g(|u|)du = Sd

k(t) · td−1 · g(|t|)dt

Rd

=

Sd
d

(cid:90) ∞

0

0

(cid:90) ∞

0

(k(t) · g(|t|))tddt = vd

(k(t) · g(|t|))tddt,

where the second last equality follows from integration by
parts and the last follows from the fact that vd = Sd/d.

The following follows immediately from Lemma 4.

Corollary 1.

(cid:90)

(cid:90)

Rd

Rd

K(u)ˇux(h|u|)du = vd ·

k(t) · td · ˇux(ht)dt,

K(u)ˆux(h|u|)du = vd ·

k(t) · td · ˆux(ht)dt.

(cid:90) ∞

0
(cid:90) ∞

0

Proof of Theorem 1. Assume that the event that Lemma 3
holds, which occurs with probability at least 1 − 1/n. We
ﬁrst show the lower bound for (cid:98)fH(x). Deﬁne

(cid:98)f∆,H(x) :=

1
n · hd

n
(cid:88)

i=1

(cid:32)

K∆

H0

−1/2(x − Xi)

(cid:33)

.

h

It is clear that (cid:98)fH(x) ≥ (cid:98)f∆,H(x) for all x ∈ Rd. Let us use
the following shorthand ∆k,j := k(j∆). We have

(cid:98)f∆,H(x) =

(∆k,j − ∆k,j+1) · Fn (BH0 (x, jh∆)) .

1
hd

∞
(cid:88)

j=0

Uniform Convergence Rates for Kernel Density Estimation

− vd

(∆k,j − ∆k,j+1)(j∆)d · ˇux

(cid:32)

(cid:33)

jh∆
(cid:112)σd(H0)

Fn (BH0 (x, jh/m))
≤ vd(jh∆)dFj + βn(jh∆)d/2(cid:112)vd · Fj + β2
n.

(∆k,j − ∆k,j+1)(j∆)d/2

Using this, we now have

We next get a handle on each Fn (BH0 (x, jh∆)). We have

F (BH0 (x, jh∆)) ≥ vd · (jh∆)d · Fj,
where Fj := max{0, f (x) − ˇux(jh∆/(cid:112)σd(H0))}. Thus,
by Lemma 3, we have

Fn (BH0(x, jh∆))
≥ vd · (jh∆)d · Fj − βn
≥ vd · (jh∆)d · Fj − βn

√

vd · (jh∆)d/2 · (cid:112)Fj − β2
(cid:112)vd · ||f ||∞ · (jh∆)d/2 − β2
n.

n

(∆k,j − ∆k,j+1)(j∆)d · f (x)

Therefore,

(cid:98)f∆,h(x)
∞
(cid:88)

≥ vd

j=0

∞
(cid:88)

j=0

βn

−

(cid:112)vd · ||f ||∞
hd/2

·

∞
(cid:88)

j=0

− β2
n

k(0)
hd .

We handle each term separately. For the ﬁrst term, we have

lim
∆→0

vd

∞
(cid:88)

j=0
(cid:90) ∞

= vd

0

(∆k,j − ∆k,j+1)(j∆)d

k(t)tddt = 1.

where the last equality follows from Lemma 4. Next, we
have

(∆k,j − ∆k,j+1)(j∆)d · ˇux

(cid:32)

(cid:33)

jh∆
(cid:112)σd(H0)

k(t) · td · ˇux(th/(cid:112)σd(H0))dt < (cid:15).

lim
∆→0

vd

∞
(cid:88)

j=0
(cid:90) ∞

= vd

0

Finally, we have

lim
∆→0

∞
(cid:88)

j=0
(cid:90) ∞

=

0

(∆k,j − ∆k,j+1)(j∆)d/2

k(t) · td/2dt < ∞.

Thus, taking ∆ → 0 we get

(cid:98)fH(x) ≥ f (x) − (cid:15) −

βn

(cid:112)vd · ||f ||∞
hd/2

·

(cid:90) ∞

0

k(t) · td/2dt

− β2
n

k(0)
hd .

This gives us the lower bound. Next we derive an upper
bound. Let us redeﬁne

(cid:98)f∆,H(x) :=

1
n · hd

n
(cid:88)

i=1

Km

(cid:18) x − Xi
h

(cid:19)

.

It is clear that (cid:98)fH(x) ≤ (cid:98)f∆,H(x) for all x ∈ Rd and

(cid:98)f∆,H(x)
∞
(cid:88)

=

1
hd

j=0

(∆k,j − ∆k,j+1) · Fn (BH0(x, (j + 1)h∆)) .

We next get a handle on each Fn (BH0 (x, jh∆)). We have
F(BH0(x, jh∆)) ≤ vd · (jh∆)d · Fj
where Fj = min{||f ||∞, f (x) + ˆu(jh∆/(cid:112)σd(H0))}.
Thus by Lemma 3 we have

(cid:98)f∆,H(x)
∞
(cid:88)

≤ vd

j=0

∞
(cid:88)

j=0

(∆k,j − ∆k,j+1)((j + 1)(cid:15))d · f (x)

+ vd

(∆k,j − ∆k,j+1)((j + 1)∆)d · ˆux

(cid:32)

(cid:33)

(j + 1)h∆
(cid:112)σd(H0)

(∆k,j − ∆k,j+1)((j + 1)∆)d/2

βn

+

(cid:112)vd · ||f ||∞
hd/2

·

∞
(cid:88)

j=0

+ β2
n

k(0)
hd .

We proceed the same way as the other direction. Thus tak-
ing ∆ → 0 we get

(cid:98)f∆,H(x) ≤ f (x) + (cid:15) +

βn

(cid:112)vd · ||f ||∞
hd/2

·

(cid:90) ∞

0

k(t) · td/2dt

+ β2
n

k(0)
hd .

The result follows.

6. Sup-norm Bounds for KDE

Theorem 2. [(cid:96)∞ bound for α-H¨older continuous func-
|f (x) − f (x(cid:48))| ≤
tions] If f is H¨older-continuous (i.e.
Cα|x − x|α for x, x(cid:48) ∈ Rd and 0 < α ≤ 1), then there
exists positive constant C (cid:48) ≡ C (cid:48)(C, Cα, α, K) such that
the following holds with probability at least 1 − 1/n uni-
formly in h > (log n/n)1/d and unit bandwidths H0. Let
H := h2H0.

| (cid:98)fH(x) − f (x)| < C (cid:48) ·

sup
x∈Rd

(cid:32)

hα
σd(H0)α/2

+

(cid:114)

(cid:33)

.

log n
n · hd

Uniform Convergence Rates for Kernel Density Estimation

Remark 8. Taking h = n−1/(2α+d) in the above r.h.s. op-
timizes the rates to n−α/(2α+d) (ignoring log factors).
Remark 9. We can attain similar results (although not
uniform in bandwidth) by a straightforward application
of Theorem 3.1 of Sriperumbudur & Steinwart (2012) or
Proposition 9 of Rinaldo & Wasserman (2010).

7. Mode Estimation Results

The goal of this section is to utilize the KDE to estimate
the mode of a uni-modal distribution from its samples. We
borrow the estimator from Abraham et al. (2004)

ˆx := argmaxx∈X (cid:98)fH(x),

where H := h2I.

We adopt the mode estimation framework assumptions
from Dasgupta & Kpotufe (2014) which are summarized
below.
Deﬁnition 7. x0 is a mode of f if f (x) < f (x0) for all
x ∈ B(x0, r)\{x0} for some r > 0.
Assumption 4.

• f has a single mode x0.

• f is twice differentiable in a neighborhood around x0.

• f has a negative-deﬁnite Hessian at x0.

These assumptions lead to the following.
Lemma 5 ((Dasgupta & Kpotufe, 2014)). Let f satisfy As-
sumption 4. Then there exists ˆC, ˇC, r0, λ > 0 such that the
following holds.

ˇC · |x0 − x|2 ≤ f (x0) − f (x) ≤ ˆC · |x0 − x|2

for all x ∈ Ax where A0 is a connected component of {x :
f (x) ≥ λ} and A0 contains B(x0, r0).

We obtain the following result for the estimation error of ˆx.
Theorem 3. Suppose that Assumptions 1, 2, 3, 4 hold.
Choose h such that (log n)2/ρ · h → 0 and log n/(nhd) →
0 as n → ∞. Then, for n sufﬁciently large depending on
d, ||f ||∞, K, ˆC, ˇC, r0 the following holds with probability
least 1 − 1/n.

|ˆx − x0|2 ≤ max

(log n)4/ρ · h2, 17 · C

(cid:40)

32 ˆC
ˇC

(cid:114)

(cid:41)

.

log n
n · hd

Remark 10. Taking h = n−1/(4+d) optimizes the above
expression so that |ˆx − x0| (cid:46) n−1/(4+d) (ignoring log fac-
tors) which matches the lower bound rate for mode estima-
tion as established in Tsybakov (1990).
Remark 11. This result can be extended to multi-modal
distributions as done by Dasgupta & Kpotufe (2014) by us-
ing the connected components of nearest neighbor graphs
at appropriate empirical density levels to isolate the modes
away from each other.

8. Density Level Set Estimation Results

In this section, we estimate the density level set Lf (λ) :=
{x : f (x) ≥ λ} where λ > 0 is given. We make the fol-
lowing standard regularity assumptions e.g. (Singh et al.,
2009). To simplify the analysis, let us take H = h2I. It is
clear that the results that follow can be extended to arbitrary
H0.
Assumption 5 (β-regularity). Let 0 < β < ∞. There ex-
ists 0 < λ0 < λ and ˇCβ, ˆCβ, ¯r > 0 such that the following
holds for x ∈ Lf (λ0)\Lf (λ).

ˇCβ · d(x, Lf (λ))β ≤ λ − f (x) ≤ ˆCβ · d(x, Lf (λ))β,
where d(x, A) := inf x(cid:48)∈A{|x − x(cid:48)|}. and B(Lf (λ), ¯r) ⊆
Lf (λ0) where B(A, r) := {x : d(x, A) ≤ r}.

Then we consider following estimator.

(cid:40)

(cid:98)Lf :=

x ∈ X : (cid:98)fH(x) > λ − ˜C

(cid:114)

(cid:41)

.

log n
n · hd

where ˜C is obtained by taking C and replacing the ||f ||∞
factor by 1+5 maxx∈Xn0 (cid:98)fH(x) where Xn0 is a ﬁxed sam-
ple of size n0. Then, ˜C can be viewed as a constant w.r.t.
n and can be known without any a priori knowledge of f
while ensuring that ˜C ≥ max{1, 2C}.

We use the following Hausdorff metric.
Deﬁnition 8 (Hausdorff Distance).

dH (A, B) := max{sup
x∈A

d(x, B), sup
x∈B

d(x, A)}.

Theorem 4. Suppose that Assumptions 1, 2, 3, 5 hold and
that f is α-H¨older continuous for some 0 < α ≤ 1. Choose
h such that (log n)2/ρ · h → 0 and log n/(nhd) → 0
as n → ∞. Then, for n sufﬁciently large depending on
d, C, ˜C, K, ˆCβ, ˇCβ, β, ¯r the following holds with probabil-
ity least 1 − 1/n.

dH ((cid:98)Lf , Lf (λ)) ≤ C (cid:48)(cid:48)

(log n)2/ρ · h +

(cid:32)

(cid:18) log n
n · hd

(cid:19)1/(2β)(cid:33)

,

where C (cid:48)(cid:48) ≡ C (cid:48)(cid:48)(C, ˜C, ˆCβ, ˇCβ, ˜C, β).
Remark 12. Choosing h = n−β/(2β+d) gives us a density-
level set estimation rate of O(n−1/(2β+d)). This matches
the lower bound (ignoring log factors) determined by Tsy-
bakov (1997).
Remark 13. This result can be extended so that we can re-
cover each component separately (i.e. identify which points
correspond to which connected components of Lf (λ)).
Similar to the mode estimation result, this can be done us-
ing nearest neighbor graphs at the appropriate level to iso-
late the connected components of Lf (λ) away from each
other. This has been done extensively in the related area of
cluster tree estimation e.g. (Chaudhuri & Dasgupta, 2010).

Uniform Convergence Rates for Kernel Density Estimation

Remark 14. The global α-H¨older continuous assumption
is not required and is only here for simplicity. Smoothness
in a neighborhood around a maximizer of f is sufﬁcient so
that for n0 large enough, ˜C ≥ 2C.

works e.g. Audibert et al. (2007); Chaudhuri & Dasgupta
(2014). Note that misclassiﬁcation rate for a hard classiﬁer
is a slightly different but very related to what is done here,
which is directly estimating the marginal density.

9. Class Probability Estimation Results

10. Extension to Manifolds

We consider the setting where we have observations from
compact subset X ⊂ Rd and labels y ∈ {1, ..., L}. Given
a label y, an instance x ∈ Rd has density fy(x) where
the uniform measure on Rd.
fy is w.r.t.
Instance-label
pairs (X, Y ) are thus drawn according to a mixture distri-
bution where Y is chosen from {1, ..., L} with correspond-
ing probabilities π1, ..., πL (i.e. (cid:80)L
j=1 πj = 1) and then X
is chosen according to fY .

Then given x ∈ X , we can deﬁne the marginal distribution
as follows.

g(x) := (g1(x), ..., gL(x)),

gy(x) := f (Y = y|X = x) =

πyfy(x)
j πjfj(x)

.

(cid:80)

The goal of class probability estimation is to learn g based
on samples (x1, y1), ..., (xn, yn). We deﬁne our estimator
naturally as follows. Let (cid:98)fh,y be the KDE of fy w.r.t.
to
bandwidth matrix H = h2I.

ˆgh(x) := (ˆgh,1(x), ..., ˆgh,L(x)),

ˆgh,y(x) :=

and ˆπy :=

1[y = yi].

ˆπy (cid:98)fh,y(x)
j ˆπj (cid:98)fh,j(x)

(cid:80)

1
n

n
(cid:88)

j=1

We make the following regularity assumption on fy.
Assumption 6.
{1, ..., L} and x ∈ Rd we have

(α-H¨older densities) For each y ∈

|fy(x) − fy(x(cid:48))| ≤ Cα|x − x(cid:48)|α,

where 0 < α ≤ 1.

We state the result below:

Theorem 5. Suppose that Assumptions 1, 2, 3, 6 hold.
Then for n sufﬁciently large depending on miny πy, there
exists positive constants C (cid:48)(cid:48) ≡ C (cid:48)(cid:48)(L, C, Cα, α, K) and
˜C ≡ ˜C(miny πy, L) such that the following holds with
probability at least 1− ˜C/n uniformly in h > (log n/n)1/d.

||ˆgh(x) − g(x)||∞ ≤ C (cid:48)(cid:48) ·

hα +

sup
x∈Rd

(cid:32)

(cid:114)

(cid:33)

.

log n
n · hd

We make the following regularity assumptions which are
standard among works in manifold learning e.g. (Baraniuk
& Wakin, 2009; Genovese et al., 2012; Balakrishnan et al.,
2013).

Assumption 7. F is supported on M where:

• M is a dM -dimensional smooth compact Riemannian
manifold without boundary embedded in compact sub-
set X ⊆ RD.

• The volume of M is bounded above by a constant.

• M has condition number 1/τ , which controls the cur-

vature and prevents self-intersection.

Let f be the density of F with respect to the uniform mea-
sure on M .

In this section, we assume that our density estimator is w.r.t.
to dM instead of the ambient dimension d.

(cid:98)fH(x) :=

1
n · hdM

(cid:32)

n
(cid:88)

i=1

K

H0

−1/2(x − Xi)

(cid:33)

.

h

Remark 16. It is then the case that we must know the in-
trinsic dimension dM . There are numerous known tech-
niques for doing so e.g.
(Kegl, 2002; Levina & Bickel,
2004; Hein & Audibert, 2005; Farahmand et al., 2007).

Next, we need the following guarantee on the volume of
the intersection of a Euclidean ball and M ; this is required
to get a handle on the true mass of the ball under F in
later arguments. The upper and lower bounds follow from
Chazal (2013) and Lemma 5.3 of Niyogi et al. (2008). The
proof can be found in (Jiang, 2017).

Lemma 6 (Ball Volume). If 0 < r < min{τ /4dM , 1/τ },
and x ∈ M then

1 − τ 2r2 ≤

voldM (B(x, r) ∩ M )
vdM rdM

≤ 1 + 4dM r/τ,

where voldM is the volume w.r.t.
M .

the uniform measure on

We then give analogues to Theorem 1 and Theorem 2.

Remark 15. This corresponds to an optimized rate of
(cid:101)O(n−α/(2α+d)). This matches the lower bounds up to
log factors for misclassiﬁcation as established in related

[Manifold

Theorem 6.
and Lower Bounds
CM (dM , d, K, ||f ||∞, τ ) such that

Case Uniform Upper
for (cid:98)fH] There exists CM ≡
the following holds

Uniform Convergence Rates for Kernel Density Estimation

uniformly in x ∈ M , (cid:15) > 0, unit bandwidths H0, and
h > (log n/n)1/dM with probability at least 1 − 1/n. Let
H := h2H0.

(cid:98)fH(x) > f (x) − (cid:15) − CM

(cid:32)

(cid:114)

h2 +

(cid:33)

,

log n
n · hdM

if (cid:82)

Rd K(u) · ˇux(h|u|/(cid:112)σd(H0))du < (cid:15), and

(cid:98)fH(x) < f (x) + (cid:15) + CM

h +

(cid:32)

(cid:114)

(cid:33)

,

log n
n · hdM

Rd K(u) · ˆux(h|u|/(cid:112)σd(H0))du < (cid:15).

if (cid:82)
Remark 17. The extra h2 and h term in the lower and
upper bounds respectively come from the approximation of
the volume of the full-dimensional balls w.r.t. the uniform
measure on M in Lemma 6.

Proof Sketch of Theorem 6. The proof mirrors that of the
full dimensional case so we only highlight the differences.
For the lower bound, instead of

F (BH0(x, jhδ)) ≥ vd · (jhδ)d · Fj,

we have

11. Local Intrinsic Dimension Estimation

In this section, we only assume a distribution F on Rd
whose support is deﬁned as X := {x ∈ Rd : F(B(x, h)) >
0 ∀h > 0} and X is assumed to be compact. We use the
following notion of intrinsic dimension, which is based on
the doubling dimension and adapted from previous works
such as Houle (2013).

Deﬁnition 9. For x ∈ X , deﬁne the following local intrin-
sic dimension wherever the quantity exists

ID(x) := lim
h→0

log2

(cid:18) F(B(x, 2h))
F(B(x, h))

(cid:19)

.

We can then deﬁne our estimator of local intrinsic dimen-
sion at x ∈ X as follows:

(cid:99)IDn,h(x) := log2

(cid:18) Fn(B(x, 2h))
Fn(B(x, h))

(cid:19)

.

The following is a uniform convergence result
(cid:99)IDn,h(x).
Theorem 8. Deﬁne the following

for

IDh(x) := log2

(cid:18) F(B(x, 2h))
F(B(x, h))

(cid:19)

.

F (BH0 (x, jhδ)) ≥ vdM (jhδ)dM Fj(1 − τ 2(jhδ)2)
= vdM (jhδ)dM Fj − hdM +2vdM τ 2||f ||∞ (jδ)dM +2 .

Suppose
1
10 inf x(cid:48)∈X
with probability at least 1 − 1/n uniformly in x ∈ X .

that h > 0 and n satisfy βn
(cid:112)F(B(x(cid:48), h)).

<
Then the following holds

The ﬁrst term can be treated in the same way as before,
while the second term contributes in an extra term with an
h2 factor after taking the total summation.

|(cid:99)IDn,h(x) − IDh(x)| ≤

6βn
(cid:112)F(x(cid:48), 2h)

.

inf x(cid:48)∈X

For the upper bound, instead of

F(BH0 (x, jhδ)) ≤ vd · (jhδ)d · Fj,

we have

F(BH0 (x, jhδ)) ≤ vdM · (jhδ)dM · Fj(1 + 4dM (jhδ)/τ ).

Similary, this contributes an extra term with an h factor
after taking the total summation.

Theorem 7. [Manifold Case (cid:96)∞ bound for α-H¨older con-
tinuous functions] If f is H¨older-continuous (i.e. |f (x) −
f (x(cid:48))| ≤ Cα|x − x|α for x, x(cid:48) ∈ Rd with 0 <
α ≤ 1),
M ≡
C (cid:48)
M (||f ||∞, Cα, α, K, τ, dM , d, σd(H0)) such that the fol-
lowing holds with probability at least 1 − 1/n uniformly in
h satisfying (log n/n)1/dM < h < 1.

then there exists positive constant C (cid:48)

| (cid:98)fH(x) − f (x)| < C (cid:48)

M ·

hα +

sup
x∈M

(cid:32)

(cid:114)

(cid:33)

.

log n
n · hdM

Remark 18. The r.h.s. goes to 0 as n → ∞. More-
over, if IDh(x) converges to ID(x) uniformly in x ∈ X ,
then simultaneously taking h → 0 and n → ∞ such that

(cid:112)F(x(cid:48), 2h)

(cid:17)−1

inf x(cid:48)∈X

→ 0 gives us a ﬁnite-sample
βn·
uniform convergence rate for local intrinsic dimension es-
timation.

(cid:16)

If we assume a global

Remark 19.
mension d0 and a density,
1
10 inf x(cid:48)∈X
and the r.h.s. of the bound is on the order of

intrinsic di-
the condition βn <
(cid:112)F(B(x(cid:48), h)) can be interpreted as log n
nhd0 → 0
(cid:113) log n
nhd0 .

In fact, this result is similar to the uniform convergence
results for the KDE for estimating the smoothed density.

(cid:18)(cid:113) log n

(cid:19)

e.g. | (cid:98)fh − fh|∞ = O

when (ignoring some log

nhd
factors) nhd → ∞ where fh is the density convolved with
the uniform kernel with bandwidth h. It is interesting that
an analogous result comes up when estimating the intrinsic
dimension with our notion of smoothed ID.

Uniform Convergence Rates for Kernel Density Estimation

References

Abraham, C., Biau, G., and Cadre., B. On the asymptotic
properties of a simple estimate of the mode. ESAIM:
Probability and Statistics, 2004.

Akama, Yohji and Irie, Kei. Vc dimension of ellipsoids.

arxiv, 2011.

Amsaleg, Laurent, Chelly, Oussama, Furon, Teddy, Girard,
St´ephane, Houle, Michael E, Kawarabayashi, Ken-ichi,
and Nett, Michael. Estimating local intrinsic dimension-
ality. In Proceedings of the 21th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data
Mining, pp. 29–38. ACM, 2015.

Arias-Castro, Ery, Mason, David, and Pelletier, Bruno. On
the estimation of the gradient lines of a density and the
consistency of the mean-shift algorithm. Journal of Ma-
chine Learning Research, 2016.

Audibert, Jean-Yves, Tsybakov, Alexandre B, et al. Fast
learning rates for plug-in classiﬁers. The Annals of statis-
tics, 35(2):608–633, 2007.

Balakrishnan, S., Narayanan, S., Rinaldo, A., Singh, A.,
and Wasserman, L. Cluster trees on manifolds. In Ad-
vances in Neural Information Processing Systems, pp.
2679–2687, 2013.

Baraniuk, Richard G and Wakin, Michael B. Random pro-
jections of smooth manifolds. Foundations of computa-
tional mathematics, 9(1):51–77, 2009.

Botev, Zdravko I, Grotowski, Joseph F, Kroese, Dirk P,
et al. Kernel density estimation via diffusion. The Annals
of Statistics, 38(5):2916–2957, 2010.

Bousquet, O., Boucheron, S., and Lugosi, G. Introduction
to statistical learning theory. Lecture Notes in Artiﬁcial
Intelligence, 2004.

Cadre, Benoit. Kernel estimation of density level sets.

Journal of Multivariate Analysis, 2006.

Carmichael, J., George, G., and Julius, R. Finding natural

clusters. Systematic Zoology, 1968.

Chaudhuri, K. and Dasgupta, S. Rates for convergence for
the cluster tree. Advances in Neural Information Pro-
cessing Systems, 2010.

Chaudhuri, Kamalika and Dasgupta, Sanjoy. Rates of con-
vergence for nearest neighbor classiﬁcation. In Advances
in Neural Information Processing Systems, pp. 3437–
3445, 2014.

Chaudhuri, Probal, Ghosh, Anil K, and Oja, Hannu. Clas-
siﬁcation based on hybridization of parametric and non-
IEEE transactions on pattern
parametric classiﬁers.
analysis and machine intelligence, 31(7):1153–1164,
2009.

Chazal, F. An upper bound for the volume of geodesic balls

in submanifolds of euclidean spaces. 2013.

Cheng, Y. Mean shift, mode seeking, and clustering. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 1995.

Chernoff, Herman. Estimation of the mode. Annals of the

Institute of Statistical Mathematics, 1964.

Comaniciu, D and Meer, P. Mean shift: A robust approach
IEEE Transactions on

toward feature space analysis.
Pattern Analysis and Machine Intelligence, 2002.

Costa, Jose A, Girotra, Abhishek, and Hero, AO. Esti-
mating local intrinsic dimension with k-nearest neighbor
graphs. In Statistical Signal Processing, 2005 IEEE/SP
13th Workshop on, pp. 417–422. IEEE, 2005.

Cuevas, A. and Fraiman, R. A plug-in approach to support

estimation. Annals of Statistics, 1997.

Dasgupta, S. and Kpotufe, S. Optimal rates for k-nn density
and mode estimation. Advances in Neural Information
Processing Systems, 2014.

Eddy, William F. Optimum kernel estimators of the mode.

Annals of Statistics, 1980.

Einmahl, Uwe and Mason, David M. Uniform in band-
width consistency of kernel-type function estimators.
Annals of Statistics, 2005.

Farahmand, A., Szepesvari, C., and Audibert, J. Manifold-

adaptive dimension estimation. ICML, 2007.

Fukunaga and Hostetler. The estimation of the gradient of
a density function, with applications in pattern recogni-
tion. IEEE Transactions on Information Theory, 1975.

Genovese,

Christopher,

Perone-Paciﬁco, Marco,
Verdinelli, Isabella, and Wasserman, Larry. Mini-
max manifold estimation. Journal of machine learning
research, 13(May):1263–1291, 2012.

Genovese, Christopher R., Verdinelli, Marco Perone-
Paciﬁcoand Isabella, and Wasserman, Larry. Non-
parametric inference for density modes. Series B Sta-
tistical Methodology, 2015.

Gine and Guillon. Rates of strong uniform consistency
for multivariate kernel density estimators. Ann. Inst. H.
Poincare Probab. Statist., 2002.

Uniform Convergence Rates for Kernel Density Estimation

Hartigan, J. Clustering algorithms. Wiley, 1975.

Smoothing Methods in Statistics.

Simonoff, Jeffrey S.
Springer, 1996.

Singh, Aarti, Scott, Clayton, and Nowak, Robert. Adap-
tive hausdorff estimation of density level sets. Annals of
Statistics, 2009.

Sriperumbudur, Bharath K. and Steinwart, Ingo. Consis-
tency and rates for clustering with dbscan. AISTATS,
2012.

Steinwart, I. Adaptive density level set clustering. 24th

Annual Conference on Learning Theory, 2011.

Terrell, George R and Scott, David W. Variable kernel den-
sity estimation. The Annals of Statistics, pp. 1236–1265,
1992.

Tsybakov, A. Recursive estimation of the mode of a mul-
tivariate distribution. Problemy Peredachi Informatsii,
1990.

Tsybakov, A.

Introduction to nonparametric estimation.

Springer, 2008.

Tsybakov, A. B. On non-parametric estimation of density

level sets. Annals of Statistics, 1997.

Hein, Matthias and Audibert, Jean-Yves. Intrinsic dimen-
sionality estimation of submanifolds in rd. ICML, 2005.

Houle, Michael E. Dimensionality, discriminability, den-
In Data Mining Work-
sity and distance distributions.
shops (ICDMW), 2013 IEEE 13th International Confer-
ence on, pp. 468–473. IEEE, 2013.

Jiang, Heinrich. Density level set estimation on mani-
folds with dbscan. International Conference on Machine
Learning (ICML), 2017.

Jiang, Heinrich and Kpotufe, Samory. Modal-set estima-
tion with an application to clustering. AISTATS, 2017.

Kegl, Balazs. Intrinsic dimension estimation using packing

numbers. NIPS, 2002.

K´egl, Bal´azs. Intrinsic dimension estimation using packing
numbers. In Advances in neural information processing
systems, pp. 697–704, 2003.

Levina, Elizaveta and Bickel, Peter J. Maximum likelihood

estimation of intrinsic dimension. NIPS, 2004.

Li, J., Ray, S., and Lindsay, B. A nonparametric statistical
approach to clustering via mode identiﬁcation. Journal
of Machine Learning Research, 2007.

Niyogi, P., Smale, S., and Weinberger, S. Finding the ho-
mology of submanifolds with high conﬁdence from ran-
dom samples. Discrete and Computational Geometry,
2008.

Parzen, Emanual. On estimation of a probability density
function and mode. Annals of mathematical statistics,
1962.

Rigollet, P. and Vert, R. Fast rates for plug-in estimators of
density level sets. Bernoulli, 15(4):1154–1178, 2009.

Rigollet, Philippe. Generalization error bounds in semi-
supervised classiﬁcation under the cluster assumption.
Journal of Machine Learning Research, 8(Jul):1369–
1392, 2007.

Rinaldo, A. and Wasserman, L. Generalized density clus-

tering. Annals of Statistics, 2010.

Rosenblatt, M. Remarks on some nonparametric estimates

of a density function. Ann. Math. Statist., 1956.

Silverman, B. Density Estimation for Statistics and Data

Analysis. CRC Press, 1986.

Silverman, B. W. Using kernel density estimates to investi-
gate multimodality. Journal of the Royal Statistical So-
ciety. Series B (Methodological), 1981.

