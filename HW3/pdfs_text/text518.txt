Supplementary Material for “Semi-Supervised Classiﬁcation
Based on Classiﬁcation from Positive and Unlabeled Data”

Tomoya Sakai 1 2 Marthinus Christoffel du Plessis Gang Niu 1 Masashi Sugiyama 2 1

A. Proofs of Theorems

In this section, we give the proofs of Theorems in Section 4.

A.1. Proof of Theorem 1

Recall that

Rγ

N-PUNU(g) = (1
= (2
N-PNPU(g) = (1

Rγ

γ)RN-PU(g) + γRN-NU(g)
2γ)θPRP(g) + 2γθNRN(g) + (1
γ)RPN(g) + γRN-PU(g)

−

= (1 + γ)θPRP(g) + (1

γ)θNRN(g) + γRU,N(g) + Const,

Rγ

N-PNNU(g) = (1
= (1

γ)RPN(g) + γRN-NU(g)
γ)θPRP(g) + (1 + γ)θNRN(g) + γRU,P(g) + Const.

−

γ)RU,N(g) + γRU,P(g) + Const,

−

−
−

−

−

b

Let
RP(g),
lemma is needed:

RN(g),

b

b

b

RU,P(g) and

RU,N(g) be the empirical risks. In order to prove Theorem 1, the following concentration

Lemma 1 For any δ > 0, we have these uniform deviation bounds with probability at least 1

δ/3:

−

supg∈G(RP(g)

RP(g))

supg∈G(RN(g)

b
RN(g))

−

−

b

supg∈G(RU,P(g)

RU,P(g))

supg∈G(RU,N(g)

b
RU,N(g))

−

−

b

CwCφ
√nP

CwCφ
√nN

CwCφ
√nU

CwCφ
√nU

+

+

+

+

ln(3/δ)
2nP

,

ln(3/δ)
2nN

,

ln(3/δ)
2nU

,

ln(3/δ)
2nU

.

s

s

s

s

≤

≤

≤

≤

All inequalities in Lemma 1 are from the basic uniform deviation bound using the Rademacher complexity (Mohri et al.,
2012), Talagrand’s contraction lemma (Ledoux & Talagrand, 1991), as well as the fact that the Lipschitz constant of ℓR is
1/2. For these reasons, the detailed proof of Lemma 1 is omitted.

1The University of Tokyo, Japan 2RIKEN, Japan. Correspondence to: Tomoya Sakai <sakai@ms.k.u-tokyo.ac.jp>.

Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

Supplementary Material for “Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data”

Consider Rγ

N-PNPU(g). It is clear that

supg∈G(Rγ

N-PNPU(g)
(1 + γ)θP supg∈G(RP(g)

−

Rγ

≤

b

−

N-PNPU(g))

RP(g)) + (1

γ)θN supg∈G(RN(g)

RN(g)) + γ supg∈G(RU,N(g)

RU,N(g)).

Therefore, by applying Lemma 1, for any δ > 0, it holds with probability at least 1

δ that

b

−

supg∈G(Rγ

N-PNPU(g)

Rγ

N-PNPU(g))

1
2

≤

Cw,φ,δ

χ(1 + γ, 1

γ, γ).

−

−

b

·

−

−

−

b

Since I(g)

2Rγ

b
N-PNPU, with the same probability,

≤

supg∈G(I(g)

Rγ
2

N-PNPU(g))

−

Cw,φ,δ

χ(1 + γ, 1

γ, γ).

≤

·

−

Similarly, supg∈G(I(g)
Finally, Rγ
we can know RU,P(g) + RU,N(g) = 1 and then

N-PNNU(g))

−

≤

b

Rγ
2

b
Cw,φ,δ

χ(1

·

−

N-PUNU(g) is slightly more involved, for that there are both RU,P(g) and RU,N(g). From ℓR(m) + ℓR(

m) = 1,

−

γ, 1 + γ, γ) with probability at least 1

δ.

−

(1

γ)RU,N(g) + γRU,P(g) =

(2γ
(1

1)RU,P(g) + Const

1/2,
−
2γ)RU,N(g) + Const γ < 1/2.

≥

γ

(

−

As a result, supg∈G(I(g)

Rγ
2

N-PUNU(g))

Cw,φ,δ

χ(2

2γ, 2γ,

2γ

) with probability at least 1

δ.

≤

·

−

1
|

−

|

−

−

−

A.2. Proof of Theorem 2

b

In fact,

and after plugging this ℓTS(m) into

ℓTS(m),



ℓTS(m) = 


1/4
(m
0

−

1)2/4

m
0,
≤
0 < m
m > 1,

1,

≤

ℓTS(m) = ℓTS(m)
e

ℓTS(

m)

−

−

e

(m + 1)2/4
1)2/4
1/4

−

m

1,
≤ −
1 < m

−
0 < m
m > 1.

≤

0,

≤
1,

1/4
1/4
= 

(m
−


−
−
1/4

It is easy to see that ℓTS(m) and

ℓTS(m) are Lipschitz continuous with the same Lipschitz constant 1/2.

Next, recall that

e

Rγ

C-PUNU(g) = (1
= (1
C-PNPU(g) = (1
= (1
C-PNNU(g) = (1
= (1

Rγ

Rγ

γ)RC-PU(g) + γRC-NU(g)
γ)θPR′
P(g) + γθNR′
γ)RPN(g) + γRC-PU(g)
γ)θPRP(g) + (1
γ)RPN(g) + γRC-NU(g)
γ)θPRP(g) + (1

−

−

−
−

−
−

−
−

N(g) + (1

γ)RU,N(g) + γRU,P(g),

−

γ)θNRN(g) + γθPR′

P(g) + γRU,N(g),

γ)θNRN(g) + γθNR′

N(g) + γRU,P(g).

Let
RP(g),
lemma is needed:

RN(g),

b

b

b

b

b

b

RU,P(g),

RU,N(g),

R′

P(g) and

R′

N(g) be the empirical risks. Again, the following concentration

Supplementary Material for “Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data”

Lemma 2 For any δ > 0, we have these uniform deviation bounds with probability at least 1

δ/4:

−

supg∈G(RP(g)

RP(g))

supg∈G(RN(g)

b
RN(g))

−

−

b

supg∈G(RU,P(g)

RU,P(g))

supg∈G(RU,N(g)

b
RU,N(g))

supg∈G(R′

P(g)

R′

P(g))

supg∈G(R′

N(g)

b
R′

N(g))

b

−

−

b

−

−

CwCφ
√nP

CwCφ
√nN

CwCφ
√nU

CwCφ
√nU

CwCφ
√nP

CwCφ
√nN

+

+

+

+

+

+

ln(4/δ)
32nP

,

ln(4/δ)
32nN

,

ln(4/δ)
32nU

,

ln(4/δ)
32nU

,

ln(4/δ)
8nP

,

ln(4/δ)
8nN

.

s

s

s

s

s

s

≤

≤

≤

≤

≤

≤

1/4

ℓTS(m)

The detailed proof of Lemma 2 is omitted for the same reason as Lemma 1. The difference is due to that 0
and
1/8 in the square root for RP(g), RN(g), RU,P(g), RU,N(g).
Consider Rγ

C-PUNU(g). By applying Lemma 2, for any δ > 0, it holds with probability at least 1

1/4 whereas 0

1 just like 0

ℓ0-1(m)

ℓR(m)

≤

≤

≤

≤

≤

−

≤

e

δ that

1/4
1. For convenience, we will relax 1/32 to

ℓTS(m)

≤

≤

−

supg∈G(Rγ

C-PUNU(g)

Rγ

C-PUNU(g))

1
4

C ′

≤

w,φ,δ ·

χ(1

γ, γ, 1).

−

Since I(g)

4Rγ

C-PUNU, with the same probability,

b

≤

≤
The other two generalization error bounds can be proven similarly.

−

supg∈G(I(g)

Rγ
4

C-PUNU(g))

C ′

w,φ,δ ·

χ(1

γ, γ, 1).

−

−

b

A.3. Proofs of Theorems 3 and 4

Note that g is independent of the data for evaluating
σ2
P(g)/nP and VarN[

RN(g)] = σ2

,

Rγ
N(g)/nN. When nU
→ ∞
b
γ)2θ2
P VarP[
γ)2ψP + 4γ2ψN

N-PUNU(g)] = 4(1
= 4(1
−
= 4(ψP + ψN)γ2

−

b

b

N-PUNU(g), since it is ﬁxed in the evaluation. Thus, VarP[

RP(g)] =

Var[

Rγ

b

RP(g)] + 4γ2θ2

N VarN[

RN(g)]

b

8ψPγ + 4ψP,

−

b

[0, 1]. All other claims in Theorem 3 follow from that Var[

N-PUNU(g)] is quadratic
RPN(g)] at γ = 1/2, and that γN-PUNU < 1/2 if ψP < ψN or γN-PUNU > 1/2 if

Rγ

b

and it is obvious that γN-PUNU
Rγ
in γ, that Var[
ψP > ψN.

N-PUNU(g)] = Var[

∈

Likewise, when nU

b

,

→ ∞

b

Var[

Var[

N-PNPU(g)] = (1 + γ)2ψP + (1
Rγ
Rγ
N-PNNU(g)] = (1
b

γ)2ψN,
γ)2ψP + (1 + γ)2ψN,

−

−
ψN. The rest of proof of Theorem 4 is analogous to that of

and γN-PNPU
Theorem 3.

≥

0 if ψP

ψN or γN-PNNU

b

0 if ψP

≥

≥

≤

Supplementary Material for “Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data”

B. Experimental Setting

Here, we summarized the experimental settings.

B.1. Implementation in Our Experiments

We implemented the ER by ourselves, and for the other methods, we used the codes available at the authors’ websites:

• LapSVM: http://www.dii.unisi.it/~melacci/lapsvmp/

• SMIR: http://www.ms.k.u-tokyo.ac.jp/software/SMIR.zip

• WellSVM: http://lamda.nju.edu.cn/code_WellSVM.ashx

• S4VM: http://lamda.nju.edu.cn/files/s4vm.rar.

Note that we modiﬁed the original code of the S4VM for transductive learning to inductive learning according to Li &
Zhou (2015).

B.2. Parameter Candidates in Our Experiments

The regularization parameters for all the methods were chosen from
, except the regularization
}
parameter of the SMIR for the squared loss mutual information (SMI) and that of the S4VM for labeled data. The number
. The
of nearest-neighbors to construct Laplacian matrix for the LapSVM was chosen from the candidates
5, 6, . . . , 10
}
{
combination parameter η of PNU classiﬁcation was chosen from
, and γ of PUNU classiﬁcation was
0.9, . . . , 1
}
. We chose these hyper-parameters by ﬁve-fold cross-validation. The parameter for the ℓ2-
chosen from
0, 0.05, . . . , 1
}
{
regularizer of the SMIR is set at γS/(n
mink∈{±1} p(y = k)) + 0.001, where γS is the regularization parameter for the
SMI. The regularization parameter of the S4VM for the labeled data is set at 1. The other parameters were set at the default
values.

10−5, 10−4, . . . , 102
{

{−

−

1,

·

B.3. Data Set Description of Image Classiﬁcation Data Set

Table 1 is the description of the data sets used in the image classiﬁcation experiment.

Table1. The description of the data set used in the image classiﬁcation experiment.

Data set

Data sources

#Samples

Arts

Deserts

Fields

Stadiums

Platforms

Temples

Art Gallery
vs.
Art Studio

Desert Sand
vs.
Desert Vegetation

Field Wild
vs.
Field Cultivated

Stadium Baseball
vs.
Stadium Football

Subway Station
vs.
Train Station

(mP = 15000)

(mN = 15000)

(mP = 15000)

(mN = 5556)

(mP = 15000)

(mN = 8117)

(mP = 15000)

(mN = 15000)

(mP = 5597)

(mN = 15000)

Temple East Asia
vs.
Temple South Asia (mN = 7178)

(mP = 8691)

Supplementary Material for “Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data”

C. Supplementary Results for Experimental Analyses

Figure 1 and Figure 2 respectively show the results of variance reduction and comparison of validation scores. The details
of experimental setting and the interpretation of results can be found in Section 5.1.

0

50

100 150 200 250 300

0

50

100

200

250

300

0

50

100 150 200 250 300

nv
U

150
nv
U

nv
U

(a) Banana (d = 2)

(b) Waveform (d = 21)

(c) Splice (d = 60)

Figure1. Average and standard error of the ratio between the variance of the empirical PNU risk and that of the PN risk,
b
RPN(bgPN)], as a function of the number of unlabeled samples over 100 trials. Although the variance reduction
Var[
is proved for an inﬁnite number of samples, it can be observed with a ﬁnite number of samples.

PNU(bgPN)]/ Var[

b
Rη

e
c
n
a
i
r
a
V

f
o

o
i
t
a
R

1.5

2

1

0.5

e
t
a
R
n
o
i
t
a
c
ﬁ

i
s
s
a
l
c
s
i

M

f
o

o
i
t
a
R

1.05

1

0.95

θP = 0.3
θP = 0.5
θP = 0.7

e
c
n
a
i
r
a
V

f
o

o
i
t
a
R

4

3

2

1

0

θP = 0.3
θP = 0.5
θP = 0.7

e
t
a
R
n
o
i
t
a
c
ﬁ

i
s
s
a
l
c
s
i

M

f
o

o
i
t
a
R

1.1

1.05

1

0.95

0.9

e
c
n
a
i
r
a
V

f
o

o
i
t
a
R

1.6

1.4

1.2

1

0.8

0.6

e
t
a
R
n
o
i
t
a
c
ﬁ

i
s
s
a
l
c
s
i

M

f
o

o
i
t
a
R

1.04

1.02

1

0.98

0.96

0

50

100 150 200 250 300

0

50

100 150 200 250 300

0

50

100 150 200 250 300

nv
U

nv
U

nv
U

(a) Banana (d = 2)

(b) Waveform (d = 21)

(c) Splice (d = 60)

Figure2. Average and standard error of the ratio between the misclassiﬁcation rate of bgPNU
PN as a function of unlabeled
samples over 1000 trials. In many cases, the ratio becomes less than 1 or at worst almost 1, implying that the PNU risk is a promising
alternative to the standard PN risk in validation if unlabeled data is available.

PN and that of bgPN

Supplementary Material for “Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data”

D. Magniﬁed Versions of Experimental Results

Here, we show magniﬁed versions of the experimental results in Section 5.

ER

PNU

PUNU

Image
d = 18

Magic
d = 10

Susy
d = 18

Phoneme
d = 5

LapSVM SMIR WellSVM

Data set nL
Banana
d = 2

Table2. Magniﬁed version of Table 1: Average and standard error of the misclassiﬁcation rates of each method over 50 trials for
benchmark data sets. Boldface numbers denote the best and comparable methods in terms of average misclassiﬁcations rate according
to a t-test at a signiﬁcance level of 5%. The bottom row gives the number of best/comparable cases of each method.
S4VM
10 30.1 (1.0) 32.1 (1.1) 35.8 (1.0) 36.9 (1.0) 37.7 (1.1) 41.8 (0.6) 45.3 (1.0)
50 19.0 (0.6) 26.4 (1.2) 20.6 (0.7) 21.3 (0.7) 21.1 (1.0) 42.6 (0.5) 38.7 (0.9)
10 32.5 (0.8) 33.5 (1.0) 33.4 (1.2) 36.5 (1.5) 36.4 (1.2) 28.4 (0.6) 33.7 (1.4)
50 28.1 (0.5) 32.8 (0.9) 27.8 (0.6) 27.0 (0.8) 28.6 (1.0) 26.8 (0.4) 25.1 (0.2)
10 31.7 (0.8) 34.1 (0.9) 34.2 (1.1) 37.9 (1.3) 36.0 (1.2) 30.1 (0.8) 33.3 (0.9)
50 29.9 (0.8) 33.4 (0.9) 30.9 (0.5) 31.0 (0.9) 30.8 (0.9) 28.8 (0.8) 29.2 (0.4)
10 29.8 (0.9) 31.7 (0.8) 33.7 (1.1) 36.6 (1.2) 36.7 (1.2) 34.7 (1.1) 35.9 (1.0)
50 20.7 (0.8) 26.6 (1.1) 20.8 (0.8) 20.3 (1.0) 20.9 (0.9) 27.2 (1.0) 23.2 (0.7)
10 44.6 (0.6) 45.0 (0.6) 47.7 (0.4) 48.2 (0.4) 45.1 (0.7) 48.0 (0.3) 46.8 (0.3)
50 38.9 (0.6) 41.5 (0.6) 37.9 (0.7) 43.1 (0.6) 43.9 (0.8) 43.8 (0.7) 42.1 (0.4)
10 40.8 (0.9) 42.4 (0.7) 43.6 (0.9) 45.9 (0.7) 46.2 (0.8) 42.4 (0.8) 42.0 (0.7)
German
50 36.2 (0.8) 39.0 (0.8) 38.9 (0.6) 40.6 (0.6) 38.4 (1.1) 38.5 (1.0) 34.9 (0.5)
d = 20
Waveform 10 17.4 (0.6) 18.0 (0.9) 18.5 (0.6) 24.9 (1.4) 18.0 (1.0) 16.7 (0.6) 20.8 (0.8)
50 16.3 (0.6) 23.7 (1.2) 14.2 (0.4) 18.1 (0.8) 15.4 (0.6) 15.5 (0.5) 15.3 (0.3)
d = 21
10 43.6 (0.6) 40.3 (1.0) 49.7 (0.1) 49.2 (0.3) 44.0 (1.0) 45.9 (0.7) 49.3 (0.8)
50 34.5 (0.8) 37.1 (0.9) 35.5 (0.8) 33.4 (1.1) 49.4 (0.3) 46.2 (0.8) 48.6 (0.4)
10 11.4 (0.6) 12.5 (0.6) 23.3 (2.3) 39.8 (1.6) 21.9 (1.3) 6.6 (0.4) 27.0 (1.4)
8.7 (0.4) 22.5 (1.5) 10.6 (0.6) 7.4 (0.4) 12.1 (0.5)
50 12.5 (1.1) 10.1 (0.6)
10 46.2 (0.4) 46.0 (0.4) 46.0 (0.5) 47.1 (0.5) 47.9 (0.5) 46.9 (0.6) 46.4 (0.4)
covtype
50 41.3 (0.5) 42.3 (0.5) 41.0 (0.4) 41.5 (0.5) 46.2 (0.8) 43.6 (0.6) 40.8 (0.4)
d = 54
Spambase 10 27.2 (0.9) 28.1 (1.1) 31.8 (1.4) 39.7 (1.4) 30.9 (1.3) 23.8 (0.8) 36.1 (1.5)
50 23.4 (1.0) 26.6 (1.0) 22.1 (0.7) 28.5 (1.3) 20.9 (0.5) 19.1 (0.4) 24.5 (0.9)
d = 57
10 38.3 (0.8) 39.3 (0.8) 43.9 (0.8) 47.9 (0.5) 41.6 (0.7) 42.0 (1.0) 42.4 (0.6)
50 30.6 (0.8) 34.7 (0.9) 30.9 (0.8) 38.8 (1.0) 30.6 (0.9) 40.9 (0.8) 35.9 (0.7)
10 24.2 (1.2) 25.8 (1.0) 27.3 (1.6) 37.2 (1.6) 27.6 (1.6) 27.5 (1.4) 31.7 (1.3)
50 15.8 (0.6) 18.3 (0.8) 15.4 (0.5) 21.1 (1.3) 14.7 (0.8) 17.2 (0.7) 16.7 (0.8)
10 31.4 (0.9) 31.3 (1.0) 34.3 (1.2) 41.0 (1.1) 37.3 (1.3) 33.1 (1.2) 34.3 (1.2)
50 27.9 (0.6) 29.9 (0.8) 28.6 (0.7) 33.3 (1.0) 26.9 (0.7) 28.9 (0.8) 26.2 (0.4)
10 38.7 (0.8) 40.1 (0.8) 42.8 (0.7) 43.9 (0.8) 43.2 (0.8) 39.1 (0.9) 44.0 (0.8)
50 23.2 (0.6) 30.5 (0.9) 23.6 (0.9) 22.8 (0.9) 25.1 (0.9) 22.6 (0.8) 25.4 (0.8)
10 35.9 (0.9) 33.6 (1.0) 41.6 (1.0) 46.6 (0.8) 39.4 (0.9) 42.1 (0.8) 43.0 (0.8)
50 28.1 (0.7) 27.6 (0.6) 27.0 (0.9) 38.7 (0.8) 28.0 (0.9) 33.7 (0.8) 35.2 (1.0)

phishing
d = 68

w8a
d = 300

Coil2
d = 241

ijcnn1
d = 22

g50c
d = 50

Splice
d = 60

a9a
d = 83

#Best/Comp.

23

13

11

4

9

13

7

Supplementary Material for “Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data”

PNU

PUNU

ER

LapSVM

SMIR

WellSVM

S4VM

103

102

101

100
10−1

]
.
c
e
s
[

i

e
m
T
n
o
i
t
a
t
u
p
m
o
C

Banana
Phoneme

Magic

Image

Susy

ijcnn1

Waveform
German

g50c

covtype
Spambase

phishing
Splice

a9a

Coil2

w8a

Figure3. Magniﬁed version of Figure 3: Average computation time over 50 trials for benchmark data sets when nL = 50.

Table3. Magniﬁed version of Table 2: Average and standard error of misclassiﬁcation rates over 30 trials for the Places 205 data set.
Boldface numbers denote the best and comparable methods in terms of the average misclassiﬁcation rate according to a t-test at a
signiﬁcance level of 5%.

Data set

Arts

Deserts

Fields

Stadiums

Platforms

Temples

b
θP

θP

PNU

LapSVM

SMIR WellSVM
ER
nU
1000 0.50 0.49 (0.01) 27.4 (1.3) 26.6 (0.5) 26.1 (0.7) 40.1 (3.9) 27.5 (0.5)
5000 0.50 0.50 (0.01) 24.8 (0.6) 26.1 (0.5) 26.1 (0.4) 30.1 (1.6)
N/A
10000 0.50 0.52 (0.01) 25.6 (0.7) 25.4 (0.5) 25.5 (0.6)
N/A
N/A
1000 0.73 0.67 (0.01) 13.0 (0.5) 15.3 (0.6) 16.7 (0.8) 17.2 (0.8) 18.2 (0.7)
5000 0.73 0.67 (0.01) 13.4 (0.4) 13.3 (0.5) 16.6 (0.6) 24.4 (0.6)
N/A
10000 0.73 0.68 (0.01) 13.3 (0.5) 13.7 (0.6) 16.8 (0.8)
N/A
N/A
1000 0.65 0.57 (0.01) 22.4 (1.0) 26.2 (1.0) 26.6 (1.3) 28.2 (1.1) 26.6 (0.8)
5000 0.65 0.57 (0.01) 20.6 (0.5) 22.6 (0.6) 24.7 (0.8) 29.6 (1.2)
N/A
10000 0.65 0.57 (0.01) 21.6 (0.6) 22.5 (0.6) 25.0 (0.9)
N/A
N/A
1000 0.50 0.50 (0.01) 11.4 (0.4) 11.5 (0.5) 12.5 (0.5) 17.4 (3.6) 11.7 (0.4)
5000 0.50 0.50 (0.01) 11.0 (0.5) 10.9 (0.3) 11.1 (0.3) 13.4 (0.7)
N/A
10000 0.50 0.51 (0.00) 10.7 (0.3) 10.9 (0.3) 11.2 (0.2)
N/A
N/A
1000 0.27 0.33 (0.01) 21.8 (0.5) 23.9 (0.6) 24.1 (0.5) 30.1 (2.3) 26.2 (0.8)
5000 0.27 0.34 (0.01) 23.3 (0.8) 24.4 (0.7) 24.9 (0.7) 26.6 (0.3)
N/A
10000 0.27 0.34 (0.01) 21.4 (0.5) 24.3 (0.6) 24.8 (0.5)
N/A
N/A
1000 0.55 0.51 (0.01) 43.9 (0.7) 43.9 (0.6) 43.4 (0.6) 50.7 (1.6) 44.3 (0.5)
5000 0.55 0.54 (0.01) 43.4 (0.9) 43.0 (0.6) 43.1 (1.0) 43.6 (0.7)
N/A
10000 0.55 0.50 (0.01) 45.2 (0.8) 44.4 (0.8) 44.2 (0.7)
N/A
N/A

Supplementary Material for “Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data”

References

Ledoux, M. and Talagrand, M. Probability in Banach Spaces: Isoperimetry and Processes. Springer, 1991.

Li, Y.-F. and Zhou, Z.-H. Towards making unlabeled data never hurt. IEEE Transactions on Pattern Analysis and Machine

Intelligence, 37(1):175–188, 2015.

Mohri, M., Rostamizadeh, A., and Talwalkar, A. Foundations of Machine Learning. MIT Press, 2012.

