Projection-free Distributed Online Learning in Networks: Appendix

A. Proof of Lemma 4

Note that, for the right side, we have the following identity

First, the base case of induction is true for t = 1 since by
deﬁnition we have

It then follows that
√

√

Proof. Combining Lemma 2 and Lemma 3, we can obtain
the concrete recursion

t,iD2
ht+1,i ≤ (1 − σt,i)ht,i + σ2
√
n + 1)L(cid:112)ht+1,i.

+ ηi(

1 + σ2(P )
1 − σ2(P )

As the parameters ηi and σt,i are chosen such that
n + 1)L(cid:112)ht+1,i ≤ σ2
ηi( 1+σ2(P )
t,iD2, we can then ob-
1−σ2(P )
tain the following compact recursion

√

ht+1,i ≤ (1 − σt,i)ht,i + 2D2σ2

t,i.

Now based on this recursion, we can prove the bound in the
lemma by induction.

h1,i = F1,i(xi(1)) − F1,i(x∗

i (1))
= (cid:107)xi(1) − x1(1)(cid:107)2 − (cid:107)x∗
≤ 2D2
≤ 4D2σ1,i.

i (1) − x1(1)(cid:107)2

Second, assuming that the bound is true for t, we now show
that it also holds for t + 1:

ht+1,i ≤ (1 − σt,i)ht,i + 2D2σ2
t,i

≤ 4D2σt,i(1 − σt,i) + 2D2σ2
t,i
= 4D2σt,i(1 − σt,i +

)

σt,i
2

= 4D2σt,i(1 −

≤ 4D2σt+1,i.

σt,i
2

)

√

1
t + 1

=

√

t
t + 1

.

1
√
t

√

Thus, dividing both sides by the common 1√
t
following equivalent inequality

, we reach the

1 −

≤

√

1
√
2

t

√

t
t + 1

.

By rewriting, we have

1 −

≤ 1 −

1
√
2

t

√

√

t

.

t + 1 −
√
t + 1

t + 1 −
√
t + 1
√

t

≤

√

1
√
2

t

.

Multiplying

t + 1

t in both sides, we obtain

√
(

√

√

t + 1 −

t)

t ≤

√

t + 1
2

,

which is equivalent to the following
√

(cid:112)

t2 + t ≤

t + 1
2

+ t.

Squaring both sides, we have

t2 + t ≤ t2 +

√

+ t

t + 1.

t + 1
4

The last inequality follows from the deﬁnition of σt,i,
which can be proved in the following section.

B. Proof of the last inequality in Lemma 4

For the sequence σt,i = 1√
t
inequality holds

, t = 1, 2, · · · , T , the following

σt,i(1 −

) ≤ σt+1,i.

σt,i
2

Proof. The inequality we need to prove is

1
√
t

(1 −

) ≤

√

1
√

2

t

1
t + 1

.

Clearly, this inequality holds for any t = 1, · · · , T , since

t ≤

+ t

t + 1.

√

t + 1
4

C. Proof of Lemma 6

Proof. We adopt the same notations used in the proof of
Lemma 3. From there, we have

zi(t) =

P t−r−1

ij

gj(r).

t−1
(cid:88)

n
(cid:88)

r=1

j=1

Projection-free Distributed Online Learning in Networks: Appendix

To proceed, we ﬁrst introduce another auxiliary sequence
which are composed of the averages of the subgradients
over all nodes i at each iteration

we have

(cid:107)zi(t) − ¯z(t)(cid:107) ≤ L

=

Pijzj(t) + ¯g(t)

D. Proof of Lemma 7

¯g(t) =

gi(t).

1
n

n
(cid:88)

i=1

Then we can show that the averaged dual variable ¯z(t)
evolves in a quite simple way

¯z(t + 1) =

Pijzj(t) + gi(t))

1
n

1
n

n
(cid:88)

n
(cid:88)

(

i=1
n
(cid:88)

j=1
n
(cid:88)

j=1

i=1

= ¯z(t) + ¯g(t).

The last equation follows from the doubly stochastic prop-
erty of matrix P . Based on the above recursion, we can
easily deduce that

¯z(t) =

¯g(r) =

t−1
(cid:88)

r=1

1
n

t−1
(cid:88)

n
(cid:88)

r=1

j=1

gj(r).

Hence,

zi(t) − ¯z(t) =

t−1
(cid:88)

n
(cid:88)

r=1

j=1

(P t−r−1
ij

−

)gj(r).

1
n

Then using the fact that (cid:107)gi(t)(cid:107) ≤ L, and the properties of
norm functions and matrices, we obtain

(cid:107)zi(t) − ¯z(t)(cid:107)

t−1
(cid:88)

n
(cid:88)

r=1

j=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
t−1
(cid:88)

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

r=1

j=1

=

≤

(P t−r−1
ij

−

(cid:13)
(cid:13)
(cid:13)
)gj(r)
(cid:13)
(cid:13)
(cid:13)

1
n

P t−r−1

ij

−

(cid:13)
(cid:13)gj(r)(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

≤ L

= L

t−1
(cid:88)

r=1

t−1
(cid:88)

r=1

(cid:13)
(cid:13)P t−r−1
i

− 1/n(cid:13)
(cid:13)1

(cid:13)
(cid:13)P t−r−1ei − 1/n(cid:13)
(cid:13)1.

t−1
(cid:88)

r=1

σ2(P )t−r−1√
√

n

nL

=

≤

(1 − σ2(P )t−1)
1 − σ2(P )
nL
1 − σ2(P )

√

.

The above equation and the last inequality follow respec-
tively from the summation formula of geometric series and
the fact that σ2(P ) < 1 when P is a doubly stochastic ma-
trix (Berman & Plemmons, 1979).

Proof. According to (Hosseini et al., 2013), the D-ODA
algorithm with parameters α(t) applied to loss functions
that are L-Lipschitz with respect to a general norm attains
the following regret bound

Ra

T (xi,x) ≤

L2
2

α(t) +

ψ(x)

1
α(T )

T −1
(cid:88)

t=1

T
(cid:88)

t=1

+ L

α(t)(cid:107)zi(t) − ¯z(t)(cid:107)∗

+

2L
n

T
(cid:88)

t=1

n
(cid:88)

j=1

α(t)

(cid:107)zj(t) − ¯z(t)(cid:107)∗,

where (cid:107)·(cid:107)∗ denotes the corresponding dual norm.
Note that the norm we utilize is the L2 norm and its
dual norm is itself. Thus we can apply the bound for
(cid:107)zi(t) − ¯z(t)(cid:107) in Lemma 6 here. Combining it with the

fact that

α(t) ≤

α(t),

the fact that ψ(x) =

T −1
(cid:80)
t=1

T
(cid:80)
t=1

(cid:107)x − x1(1)(cid:107)2 ≤ D2 and setting α(t) = η yields the stated
regret bound in the lemma.

E. Veriﬁcation of the validity of ηi

Proof. As ηi =

√

2(

n+1+(

(1−σ2(P ))D

√

n−1)σ2(P ))LT 3/4 , we have
D(cid:112)ht+1,i
2T 3/4

n + 1)L(cid:112)ht+1,i =

.

ηi(

1 + σ2(P )
1 − σ2(P )

√

By Lemma 4 and deﬁnition of σt,i, we have

ht+1,i ≤ 4D2σt+1,i ≤ 4D2σt,i.

Since the following inequality holds for any non-negative
integer s

(cid:107)P sei − 1/n(cid:107)1 ≤ σ2(P )s√

n,

It then follows that
D(cid:112)ht+1,i
2T 3/4

≤

σ1/2
t,i
T 3/4

D2.

Projection-free Distributed Online Learning in Networks: Appendix

We thus only need to verify that the following inequality
holds for any t = 1, · · · , T

This clearly holds since for any t = 1, · · · , T

σ1/2
t,i
T 3/4

D2 ≤ σ2

t,iD2.

1
T 3/4

≤ σ3/2

t,i =

1
t3/4

.

Thus, the choice of ηi satisﬁes the constraint required in
Lemma 4.

References

Berman, Abraham and Plemmons, Robert J. Nonnegative
Matrices in the Mathematical Sciences. Academic Press,
1979.

Hosseini, Saghar, Chapman, Airlie, and Mesbahi, Mehran.
Online distributed optimization via dual averaging.
In
IEEE Conference on Decision and Control, pp. 1484–
1489. IEEE, 2013.

