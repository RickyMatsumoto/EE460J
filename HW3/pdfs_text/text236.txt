Reinforcement Learning with Deep Energy-Based Policies

Tuomas Haarnoja * 1 Haoran Tang * 2 Pieter Abbeel 1 3 4 Sergey Levine 1

Abstract

We propose a method for learning expressive
energy-based policies for continuous states and
actions, which has been feasible only in tabular
domains before. We apply our method to learn-
ing maximum entropy policies, resulting into a
new algorithm, called soft Q-learning, that ex-
presses the optimal policy via a Boltzmann dis-
tribution. We use the recently proposed amor-
tized Stein variational gradient descent to learn
a stochastic sampling network that approximates
samples from this distribution. The beneﬁts of
the proposed algorithm include improved explo-
ration and compositionality that allows transfer-
ring skills between tasks, which we conﬁrm in
simulated experiments with swimming and walk-
ing robots. We also draw a connection to actor-
critic methods, which can be viewed perform-
ing approximate inference on the corresponding
energy-based model.

1. Introduction

Deep reinforcement learning (deep RL) has emerged as a
promising direction for autonomous acquisition of com-
plex behaviors (Mnih et al., 2015; Silver et al., 2016), due
to its ability to process complex sensory input (Jaderberg
et al., 2016) and to acquire elaborate behavior skills using
general-purpose neural network representations (Levine
et al., 2016). Deep reinforcement learning methods can
be used to optimize deterministic (Lillicrap et al., 2015)
and stochastic (Schulman et al., 2015a; Mnih et al., 2016)
policies. However, most deep RL methods operate on the
conventional deterministic notion of optimality, where the
optimal solution, at least under full observability, is always
a deterministic policy (Sutton & Barto, 1998). Although

*Equal contribution

1UC Berkeley, Department of Elec-
2UC Berke-
trical Engineering and Computer Sciences
4International
3OpenAI
ley, Department of Mathematics
Computer Science Institute.
Hao-
ran Tang <hrtang@math.berkeley.edu>, Tuomas Haarnoja
<haarnoja@berkeley.edu>.

Correspondence to:

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

stochastic policies are desirable for exploration, this ex-
ploration is typically attained heuristically, for example by
injecting noise (Silver et al., 2014; Lillicrap et al., 2015;
Mnih et al., 2015) or initializing a stochastic policy with
high entropy (Kakade, 2002; Schulman et al., 2015a; Mnih
et al., 2016).

In some cases, we might actually prefer to learn stochastic
behaviors. In this paper, we explore two potential reasons
for this: exploration in the presence of multimodal objec-
tives, and compositionality attained via pretraining. Other
beneﬁts include robustness in the face of uncertain dynam-
ics (Ziebart, 2010), imitation learning (Ziebart et al., 2008),
and improved convergence and computational properties
(Gu et al., 2016a). Multi-modality also has application in
real robot tasks, as demonstrated in (Daniel et al., 2012).
However, in order to learn such policies, we must deﬁne an
objective that promotes stochasticity.

In which cases is a stochastic policy actually the optimal
solution? As discussed in prior work, a stochastic policy
emerges as the optimal answer when we consider the con-
nection between optimal control and probabilistic inference
(Todorov, 2008). While there are multiple instantiations of
this framework, they typically include the cost or reward
function as an additional factor in a factor graph, and in-
fer the optimal conditional distribution over actions condi-
tioned on states. The solution can be shown to optimize
an entropy-augmented reinforcement learning objective or
to correspond to the solution to a maximum entropy learn-
ing problem (Toussaint, 2009). Intuitively, framing control
as inference produces policies that aim to capture not only
the single deterministic behavior that has the lowest cost,
but the entire range of low-cost behaviors, explicitly max-
imizing the entropy of the corresponding policy. Instead
of learning the best way to perform the task, the result-
ing policies try to learn all of the ways of performing the
task. It should now be apparent why such policies might
be preferred: if we can learn all of the ways that a given
task might be performed, the resulting policy can serve as
a good initialization for ﬁnetuning to a more speciﬁc be-
havior (e.g. ﬁrst learning all the ways that a robot could
move forward, and then using this as an initialization to
learn separate running and bounding skills); a better explo-
ration mechanism for seeking out the best mode in a multi-
modal reward landscape; and a more robust behavior in the

Reinforcement Learning with Deep Energy-Based Policies

face of adversarial perturbations, where the ability to per-
form the same task in multiple different ways can provide
the agent with more options to recover from perturbations.

Unfortunately, solving such maximum entropy stochastic
policy learning problems in the general case is challeng-
ing. A number of methods have been proposed, includ-
ing Z-learning (Todorov, 2007), maximum entropy inverse
RL (Ziebart et al., 2008), approximate inference using mes-
sage passing (Toussaint, 2009), Ψ-learning (Rawlik et al.,
2012), and G-learning (Fox et al., 2016), as well as more
recent proposals in deep RL such as PGQ (O’Donoghue
et al., 2016), but these generally operate either on simple
tabular representations, which are difﬁcult to apply to con-
tinuous or high-dimensional domains, or employ a simple
parametric representation of the policy distribution, such
as a conditional Gaussian. Therefore, although the policy
is optimized to perform the desired skill in many different
ways, the resulting distribution is typically very limited in
terms of its representational power, even if the parameters
of that distribution are represented by an expressive func-
tion approximator, such as a neural network.

How can we extend the framework of maximum entropy
policy search to arbitrary policy distributions? In this pa-
per, we borrow an idea from energy-based models, which in
turn reveals an intriguing connection between Q-learning,
actor-critic algorithms, and probabilistic inference. In our
method, we formulate a stochastic policy as a (condi-
tional) energy-based model (EBM), with the energy func-
tion corresponding to the “soft” Q-function obtained when
In high-
optimizing the maximum entropy objective.
dimensional continuous spaces, sampling from this policy,
just as with any general EBM, becomes intractable. We
borrow from the recent literature on EBMs to devise an ap-
proximate sampling procedure based on training a separate
sampling network, which is optimized to produce unbiased
samples from the policy EBM. This sampling network can
then be used both for updating the EBM and for action se-
lection. In the parlance of reinforcement learning, the sam-
pling network is the actor in an actor-critic algorithm. This
reveals an intriguing connection: entropy regularized actor-
critic algorithms can be viewed as approximate Q-learning
methods, with the actor serving the role of an approximate
sampler from an intractable posterior. We explore this con-
nection further in the paper, and in the course of this discuss
connections to popular deep RL methods such as determin-
istic policy gradient (DPG) (Silver et al., 2014; Lillicrap
et al., 2015), normalized advantage functions (NAF) (Gu
et al., 2016b), and PGQ (O’Donoghue et al., 2016).

The principal contribution of this work is a tractable,
efﬁcient algorithm for optimizing arbitrary multimodal
stochastic policies represented by energy-based models, as
well as a discussion that relates this method to other recent

algorithms in RL and probabilistic inference. In our experi-
mental evaluation, we explore two potential applications of
our approach. First, we demonstrate improved exploration
performance in tasks with multi-modal reward landscapes,
where conventional deterministic or unimodal methods are
at high risk of falling into suboptimal local optima. Second,
we explore how our method can be used to provide a degree
of compositionality in reinforcement learning by showing
that stochastic energy-based policies can serve as a much
better initialization for learning new skills than either ran-
dom policies or policies pretrained with conventional max-
imum reward objectives.

2. Preliminaries

In this section, we will deﬁne the reinforcement learning
problem that we are addressing and brieﬂy summarize the
maximum entropy policy search objective. We will also
present a few useful identities that we will build on in our
algorithm, which will be presented in Section 3.

2.1. Maximum Entropy Reinforcement Learning

We will address learning of maximum entropy policies with
approximate inference for reinforcement learning in con-
tinuous action spaces. Our reinforcement learning prob-
lem can be deﬁned as policy search in an inﬁnite-horizon
Markov decision process (MDP), which consists of the tu-
ple (S, A, ps, r), The state space S and action space A are
assumed to be continuous, and the state transition probabil-
ity ps : S × S × A → [0, ∞) represents the probability
density of the next state st+1 ∈ S given the current state
st ∈ S and action at ∈ A. The environment emits a re-
ward r : S × A → [rmin, rmax] on each transition, which
we will abbreviate as rt (cid:44) r(st, at) to simplify notation.
We will also use ρπ(st) and ρπ(st, at) to denote the state
and state-action marginals of the trajectory distribution in-
duced by a policy π(at|st).

Our goal is to learn a policy π(at|st). We can deﬁne the
standard reinforcement learning objective in terms of the
above quantities as
π∗
std = arg max

E(st,at)∼ρπ [r(st, at)] .

(cid:88)

(1)

π

t

Maximum entropy RL augments the reward with an en-
tropy term, such that the optimal policy aims to maximize
its entropy at each visited state:

(2)
E(st,at)∼ρπ [r(st, at)+αH(π( · |st))] ,
π∗
MaxEnt = arg maxπ
where α is an optional but convenient parameter that can
be used to determine the relative importance of entropy and
reward.1 Optimization problems of this type have been ex-
plored in a number of prior works (Kappen, 2005; Todorov,

(cid:80)
t

1In principle, 1/α can be folded into the reward function,
eliminating the need for an explicit multiplier, but in practice, it is
often convenient to keep α as a hyperparameter.

Reinforcement Learning with Deep Energy-Based Policies

2007; Ziebart et al., 2008), which are covered in more de-
tail in Section 4. Note that this objective differs qualita-
tively from the behavior of Boltzmann exploration (Sal-
lans & Hinton, 2004) and PGQ (O’Donoghue et al., 2016),
which greedily maximize entropy at the current time step,
but do not explicitly optimize for policies that aim to reach
states where they will have high entropy in the future. This
distinction is crucial, since the maximum entropy objective
can be shown to maximize the entropy of the entire trajec-
tory distribution for the policy π, while the greedy Boltz-
mann exploration approach does not (Ziebart et al., 2008;
Levine & Abbeel, 2014). As we will discuss in Section 5,
this maximum entropy formulation has a number of bene-
ﬁts, such as improved exploration in multimodal problems
and better pretraining for later adaptation.

If we wish to extend either the conventional or the maxi-
mum entropy RL objective to inﬁnite horizon problems, it
is convenient to also introduce a discount factor γ to ensure
that the sum of expected rewards (and entropies) is ﬁnite.
In the context of policy search algorithms, the use of a dis-
count factor is actually a somewhat nuanced choice, and
writing down the precise objective that is optimized when
using the discount factor is non-trivial (Thomas, 2014). We
defer the full derivation of the discounted objective to Ap-
pendix A, since it is unwieldy to write out explicitly, but
we will use the discount γ in the following derivations and
in our ﬁnal algorithm.

2.2. Soft Value Functions and Energy-Based Models

Optimizing the maximum entropy objective in (2) provides
us with a framework for training stochastic policies, but we
must still choose a representation for these policies. The
choices in prior work include discrete multinomial distri-
butions (O’Donoghue et al., 2016) and Gaussian distribu-
tions (Rawlik et al., 2012). However, if we want to use a
very general class of distributions that can represent com-
plex, multimodal behaviors, we can instead opt for using
general energy-based policies of the form

π(at|st) ∝ exp (−E(st, at)) ,

(3)

where E is an energy function that could be represented,
If we use a
for example, by a deep neural network.
universal function approximator for E, we can represent
any distribution π(at|st). There is a close connection
between such energy-based models and soft versions of
value functions and Q-functions, where we set E(st, at) =
− 1
α Qsoft(st, at) and use the following theorem:
Theorem 1. Let the soft Q-function be deﬁned by

Q∗

soft(st, at) = rt+
(cid:34) ∞
(cid:88)

E(st+1,... )∼ρπ

l=1

(4)
(cid:35)
MaxEnt( · |st+l)))

,

γl (rt+l +αH (π∗

and soft value function by

V ∗
soft(st) = α log

exp

Q∗

soft(st, a(cid:48))

da(cid:48).

(5)

(cid:90)

A

(cid:18) 1
α

(cid:19)

Then the optimal policy for (2) is given by

MaxEnt(at|st) = exp(cid:0) 1
π∗

α (Q∗

soft(st, at)−V ∗

soft(st))(cid:1) .

(6)

Proof. See Appendix A.1 as well as (Ziebart, 2010).

Theorem 1 connects the maximum entropy objective in (2)
and energy-based models, where 1
α Qsoft(st, at) acts as the
negative energy, and 1
α Vsoft(st) serves as the log-partition
function. As with the standard Q-function and value func-
tion, we can relate the Q-function to the value function at a
future state via a soft Bellman equation:

Theorem 2. The soft Q-function in (4) satisﬁes the soft
Bellman equation

Q∗

soft(st, at) = rt + γ Est+1∼ps [V ∗

soft(st+1)] ,

(7)

where the soft value function V ∗

soft is given by (5).

Proof. See Appendix A.2, as well as (Ziebart, 2010).

The soft Bellman equation is a generalization of the con-
ventional (hard) equation, where we can recover the more
standard equation as α → 0, which causes (5) to approach
a hard maximum over the actions. In the next section, we
will discuss how we can use these identities to derive a
Q-learning style algorithm for learning maximum entropy
policies, and how we can make this practical for arbitrary
Q-function representations via an approximate inference
procedure.

3. Training Expressive Energy-Based Models

via Soft Q-Learning

In this section, we will present our proposed reinforcement
learning algorithm, which is based on the soft Q-function
described in the previous section, but can be implemented
via a tractable stochastic gradient descent procedure with
approximate sampling. We will ﬁrst describe the general
case of soft Q-learning, and then present the inference pro-
cedure that makes it tractable to use with deep neural net-
work representations in high-dimensional continuous state
and action spaces. In the process, we will relate this Q-
learning procedure to inference in energy-based models
and actor-critic algorithms.

3.1. Soft Q-Iteration
We can obtain a solution to (7) by iteratively updating esti-
mates of V ∗
soft and Q∗
soft. This leads to a ﬁxed-point itera-
tion that resembles Q-iteration:

Theorem 3. Soft Q-iteration. Let Qsoft( · , · ) and Vsoft( · )
α Qsoft( · , a(cid:48))(cid:1) da(cid:48) <
be bounded and assume that (cid:82)

A exp (cid:0) 1

Reinforcement Learning with Deep Energy-Based Policies

soft < ∞ exists. Then the ﬁxed-point itera-

∞ and that Q∗
tion
Qsoft(st, at) ← rt +γ Est+1∼ps [Vsoft(st+1)] , ∀st, at

(8)

Vsoft(st) ← α log

exp

Qsoft(st, a(cid:48))

da(cid:48), ∀st (9)

(cid:90)

(cid:18)1
α

(cid:19)

converges to Q∗

A
soft and V ∗
soft, respectively.

Proof. See Appendix A.2 as well as (Fox et al., 2016).

We refer to the updates in (8) and (9) as the soft Bellman
backup operator that acts on the soft value function, and
denote it by T . The maximum entropy policy in (6) can
then be recovered by iteratively applying this operator un-
til convergence. However, there are several practicalities
that need to be considered in order to make use of the algo-
rithm. First, the soft Bellman backup cannot be performed
exactly in continuous or large state and action spaces, and
second, sampling from the energy-based model in (6) is in-
tractable in general. We will address these challenges in
the following sections.

3.2. Soft Q-Learning

This section discusses how the Bellman backup in The-
orem 3 can be implemented in a practical algorithm that
uses a ﬁnite set of samples from the environment, resulting
in a method similar to Q-learning. Since the soft Bellman
backup is a contraction (see Appendix A.2), the optimal
value function is the ﬁxed point of the Bellman backup,
and we can ﬁnd it by optimizing for a Q-function for which
the soft Bellman error |T Q − Q| is minimized at all states
and actions. While this procedure is still intractable due to
the integral in (9) and the inﬁnite set of all states and ac-
tions, we can express it as a stochastic optimization, which
leads to a stochastic gradient descent update procedure. We
will model the soft Q-function with a function approxima-
tor with parameters θ and denote it as Qθ

soft(st, at).

To convert Theorem 3 into a stochastic optimization
problem, we ﬁrst express the soft value function in terms
of an expectation via importance sampling:

soft(st) = α log Eqa(cid:48)
V θ

(cid:34)

exp (cid:0) 1

soft(st, a(cid:48))(cid:1)

(cid:35)

α Qθ
qa(cid:48)(a(cid:48))

,

(10)

where qa(cid:48) can be an arbitrary distribution over the action
space. Second, by noting the identity g1(x) = g2(x) ∀x ∈
(cid:2)(g1(x) − g2(x))2(cid:3) = 0, where q can be any
X ⇔ Ex∼q
strictly positive density function on X, we can express the
soft Q-iteration in an equivalent form as minimizing
(cid:17)2(cid:21)
(cid:16) ˆQ¯θ

JQ(θ) = Est∼qst ,at∼qat

soft(st, at)−Qθ

soft(st, at)

, (11)

1
2

(cid:20)

soft(st, at) = rt + γEst+1∼ps [V ¯θ

where qst , qat are positive over S and A respectively,
ˆQ¯θ
soft(st+1)] is a target Q-
value, with V ¯θ
soft(st+1) given by (10) and θ being replaced
by the target parameters, ¯θ.

α Qθ

This stochastic optimization problem can be solved ap-
proximately using stochastic gradient descent using sam-
pled states and actions. While the sampling distribu-
tions qst and qat can be arbitrary, we typically use real
samples from rollouts of the current policy π(at|st) ∝
soft(st, at)(cid:1). For qa(cid:48) we have more options. A
exp (cid:0) 1
convenient choice is a uniform distribution. However, this
choice can scale poorly to high dimensions. A better choice
is to use the current policy, which produces an unbiased
estimate of the soft value as can be conﬁrmed by substi-
tution. This overall procedure yields an iterative approach
that optimizes over the Q-values, which we summarize in
Section 3.4.

α Qθ

spaces, we

in continuous

still need a
However,
tractable way to sample from the policy π(at|st) ∝
soft(st, at)(cid:1), both to take on-policy actions and,
exp (cid:0) 1
if so desired, to generate action samples for estimating
the soft value function. Since the form of the policy is so
general, sampling from it is intractable. We will therefore
use an approximate sampling procedure, as discussed in
the following section.

3.3. Approximate Sampling and Stein Variational

Gradient Descent (SVGD)

In this section we describe how we can approximately sam-
ple from the soft Q-function. Existing approaches that sam-
ple from energy-based distributions generally fall into two
categories: methods that use Markov chain Monte Carlo
(MCMC) based sampling (Sallans & Hinton, 2004), and
methods that learn a stochastic sampling network trained
to output approximate samples from the target distribution
(Zhao et al., 2016; Kim & Bengio, 2016). Since sampling
via MCMC is not tractable when the inference must be
performed online (e.g. when executing a policy), we will
use a sampling network based on Stein variational gradi-
ent descent (SVGD) (Liu & Wang, 2016) and amortized
SVGD (Wang & Liu, 2016). Amortized SVGD has several
intriguing properties: First, it provides us with a stochas-
tic sampling network that we can query for extremely fast
sample generation. Second, it can be shown to converge
to an accurate estimate of the posterior distribution of an
EBM. Third, the resulting algorithm, as we will show later,
strongly resembles actor-critic algorithm, which provides
for a simple and computationally efﬁcient implementation
and sheds light on the connection between our algorithm
and prior actor-critic methods.

Formally, we want to learn a state-conditioned stochastic
neural network at = f φ(ξ; st), parametrized by φ, that
maps noise samples ξ drawn from a normal Gaussian, or
other arbitrary distribution, into unbiased action samples
from the target EBM corresponding to Qθ
soft. We denote
the induced distribution of the actions as πφ(at|st), and we
want to ﬁnd parameters φ so that the induced distribution

Reinforcement Learning with Deep Energy-Based Policies

approximates the energy-based distribution in terms of the
KL divergence
Jπ(φ; st) =

(12)

(cid:18)

DKL

πφ( · |st)

exp

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:18) 1
α

(cid:0)Qθ

soft(st, · ) − V θ
soft

(cid:19)(cid:19)

(cid:1)

.

Suppose we “perturb” a set of independent samples a(i)
t =
f φ(ξ(i); st) in appropriate directions ∆f φ(ξ(i); st), the in-
duced KL divergence can be reduced. Stein variational gra-
dient descent (Liu & Wang, 2016) provides the most greedy
directions as a functional

∆f φ( · ; st) = Eat∼πφ

κ(at, f φ( · ; st))∇a(cid:48)Qθ

(cid:104)

soft(st, a(cid:48))(cid:12)
(cid:105)

(cid:12)a(cid:48)=at
(13)

+ α ∇a(cid:48)κ(a(cid:48), f φ( · ; st))(cid:12)

(cid:12)a(cid:48)=at

,

where κ is a kernel function (typically Gaussian, see de-
tails in Appendix D.1). To be precise, ∆f φ is the optimal
direction in the reproducing kernel Hilbert space of κ, and
is thus not strictly speaking the gradient of (12), but it turns
out that we can set ∂Jπ
∝ ∆f φ as explained in (Wang &
∂at
Liu, 2016). With this assumption, we can use the chain
rule and backpropagate the Stein variational gradient into
the policy network according to

∂Jπ(φ; st)
∂φ

(cid:20)
∆f φ(ξ; st)

∝ Eξ

∂f φ(ξ; st)
∂φ

(cid:21)

,

(14)

and use any gradient-based optimization method to learn
the optimal sampling network parameters. The sampling
network f φ can be viewed as an actor in an actor-critic al-
gorithm. We will discuss this connection in Section 4, but
ﬁrst we will summarize our complete maximum entropy
policy learning algorithm.

3.4. Algorithm Summary

To summarize, we propose the soft Q-learning algorithm
for learning maximum entropy policies in continuous do-
mains. The algorithm proceeds by alternating between col-
lecting new experience from the environment, and updating
the soft Q-function and sampling network parameters. The
experience is stored in a replay memory buffer D as stan-
dard in deep Q-learning (Mnih et al., 2013), and the pa-
rameters are updated using random minibatches from this
memory. The soft Q-function updates use a delayed ver-
sion of the target values (Mnih et al., 2015). For opti-
mization, we use the ADAM (Kingma & Ba, 2015) opti-
mizer and empirical estimates of the gradients, which we
denote by ˆ∇. The exact formulae used to compute the gra-
dient estimates is deferred to Appendix C, which also dis-
cusses other implementation details, but we summarize an
overview of soft Q-learning in Algorithm 1.

4. Related Work

Maximum entropy policies emerge as the solution when
we cast optimal control as probabilistic inference. In the

Algorithm 1 Soft Q-learning

θ, φ ∼ some initialization distributions.
Assign target parameters: ¯θ ← θ, ¯φ ← φ.
D ← empty replay memory.
for each epoch do
for each t do

Collect experience
Sample an action for st using f φ:
at ← f φ(ξ; st) where ξ ∼ N (0, I).
Sample next state from the environment:
st+1 ∼ ps(st+1|st, at).
Save the new experience in the replay memory:
D ← D ∪ {(st, at, r(st, at), st+1)} .

, s(i)

, r(i)
t

, a(i)
t

i=0 ∼ D.

Sample a minibatch from the replay memory
{(s(i)
t+1)}N
t
Update the soft Q-function parameters
j=0 ∼ qa(cid:48) for each s(i)
Sample {a(i,j)}M
t+1.
Compute empirical soft values ˆV ¯θ
soft(s(i)
t+1) in (10).
Compute empirical gradient ˆ∇θJQ of (11).
Update θ according to ˆ∇θJQ using ADAM.
Update policy
Sample {ξ(i,j)}M
Compute actions a(i,j)
Compute ∆f φ using empirical estimate of (13).
Compute empiricial estimate of (14): ˆ∇φJπ.
Update φ according to ˆ∇φJπ using ADAM.

j=0 ∼ N (0, I) for each s(i)
t = f φ(ξ(i,j), s(i)
t ).

.

t

end for
if epoch mod update interval = 0 then

Update target parameters: ¯θ ← θ, ¯φ ← φ.

end if
end for

case of linear-quadratic systems, the mean of the maxi-
mum entropy policy is exactly the optimal deterministic
policy (Todorov, 2008), which has been exploited to con-
struct practical path planning methods based on iterative
linearization and probabilistic inference techniques (Tous-
saint, 2009). In discrete state spaces, the maximum entropy
policy can be obtained exactly. This has been explored in
the context of linearly solvable MDPs (Todorov, 2007) and,
in the case of inverse reinforcement learning, MaxEnt IRL
(Ziebart et al., 2008). In continuous systems and contin-
uous time, path integral control studies maximum entropy
policies and maximum entropy planning (Kappen, 2005).
In contrast to these prior methods, our work is focused on
extending the maximum entropy policy search framework
to high-dimensional continuous spaces and highly multi-
modal objectives, via expressive general-purpose energy
functions represented by deep neural networks. A num-
ber of related methods have also used maximum entropy
policy optimization as an intermediate step for optimizing
policies under a standard expected reward objective (Pe-

Reinforcement Learning with Deep Energy-Based Policies

ters et al., 2010; Neumann, 2011; Rawlik et al., 2012; Fox
et al., 2016). Among these, the work of Rawlik et al. (2012)
resembles ours in that it also makes use of a temporal dif-
ference style update to a soft Q-function. However, unlike
this prior work, we focus on general-purpose energy func-
tions with approximate sampling, rather than analytically
normalizable distributions. A recent work (Liu et al., 2017)
also considers an entropy regularized objective, though the
entropy is on policy parameters, not on sampled actions.
Thus the resulting policy may not represent an arbitrar-
ily complex multi-modal distribution with a single param-
eter. The form of our sampler resembles the stochastic
networks proposed in recent work on hierarchical learn-
ing (Florensa et al., 2017). However this prior work uses
a task-speciﬁc reward bonus system to encourage stochas-
tic behavior, while our approach is derived from optimizing
a general maximum entropy objective.

A closely related concept to maximum entropy policies is
Boltzmann exploration, which uses the exponential of the
standard Q-function as the probability of an action (Kael-
bling et al., 1996). A number of prior works have also ex-
plored representing policies as energy-based models, with
the Q-value obtained from an energy model such as a re-
stricted Boltzmann machine (RBM) (Sallans & Hinton,
2004; Elfwing et al., 2010; Otsuka et al., 2010; Heess
et al., 2012). Although these methods are closely related,
they have not, to our knowledge, been extended to the
case of deep network models, have not made extensive use
of approximate inference techniques, and have not been
demonstrated on the complex continuous tasks. More re-
cently, O’Donoghue et al. (2016) drew a connection be-
tween Boltzmann exploration and entropy-regularized pol-
icy gradient, though in a theoretical framework that differs
from maximum entropy policy search: unlike the full max-
imum entropy framework, the approach of O’Donoghue
et al. (2016) only optimizes for maximizing entropy at the
current time step, rather than planning for visiting future
states where entropy will be further maximized. This prior
method also does not demonstrate learning complex multi-
modal policies in continuous action spaces.

Although we motivate our method as Q-learning, its struc-
It is particu-
ture resembles an actor-critic algorithm.
larly instructive to observe the connection between our ap-
proach and the deep deterministic policy gradient method
(DDPG) (Lillicrap et al., 2015), which updates a Q-
function critic according to (hard) Bellman updates, and
then backpropagates the Q-value gradient into the actor,
similarly to NFQCA (Hafner & Riedmiller, 2011). Our ac-
tor update differs only in the addition of the κ term. Indeed,
without this term, our actor would estimate a maximum a
posteriori (MAP) action, rather than capturing the entire
EBM distribution. This suggests an intriguing connection
between our method and DDPG: if we simply modify the

DDPG critic updates to estimate soft Q-values, we recover
the MAP variant of our method. Furthermore, this con-
nection allows us to cast DDPG as simply an approximate
Q-learning method, where the actor serves the role of an
approximate maximizer. This helps explain the good per-
formance of DDPG on off-policy data. We can also make
a connection between our method and policy gradients. In
Appendix B, we show that the policy gradient for a policy
represented as an energy-based model closely corresponds
to the update in soft Q-learning. Similar derivation is pre-
sented in a concurrent work (Schulman et al., 2017).

5. Experiments
Our experiments aim to answer the following questions:
(1) Does our soft Q-learning method accurately capture a
multi-modal policy distribution? (2) Can soft Q-learning
with energy-based policies aid exploration for complex
tasks that require tracking multiple modes? (3) Can a max-
imum entropy policy serve as a good initialization for ﬁne-
tuning on different tasks, when compared to pretraining
with a standard deterministic objective? We compare our
algorithm to DDPG (Lillicrap et al., 2015), which has been
shown to achieve better sample efﬁciency on the contin-
uous control problems that we consider than other recent
techniques such as REINFORCE (Williams, 1992), TRPO
(Schulman et al., 2015a), and A3C (Mnih et al., 2016). This
comparison is particularly interesting since, as discussed
in Section 4, DDPG closely corresponds to a deterministic
maximum a posteriori variant of our method. The detailed
experimental setup can be found in Appendix D. Videos
of all experiments2 and example source code3 are available
online.

5.1. Didactic Example: Multi-Goal Environment

In order to verify that amortized SVGD can correctly
draw samples from energy-based policies of the form
exp (cid:0)Qθ
soft(s, a)(cid:1), and that our complete algorithm can suc-
cessful learn to represent multi-modal behavior, we de-
signed a simple “multi-goal” environment, in which the
agent is a 2D point mass trying to reach one of four sym-
metrically placed goals. The reward is deﬁned as a mixture
of Gaussians, with means placed at the goal positions. An
optimal strategy is to go to an arbitrary goal, and the op-
timal maximum entropy policy should be able to choose
each of the four goals at random. The ﬁnal policy obtained
with our method is illustrated in Figure 1. The Q-values in-
deed have complex shapes, being unimodal at s = (−2, 0),
convex at s = (0, 0), and bimodal at s = (2.5, 2.5). The
stochastic policy samples actions closely following the en-
ergy landscape, hence learning diverse trajectories that lead
to all four goals.
In comparison, a policy trained with
DDPG randomly commits to a single goal.

2https://sites.google.com/view/softqlearning/home
3https://github.com/haarnoja/softqlearning

Reinforcement Learning with Deep Energy-Based Policies

Figure 1. Illustration of the 2D multi-goal environment. Left: tra-
jectories from a policy learned with our method (solid blue lines).
The x and y axes correspond to 2D positions (states). The agent
is initialized at the origin. The goals are depicted as red dots,
and the level curves show the reward. Right: Q-values at three
selected states, depicted by level curves (red: high values, blue:
low values). The x and y axes correspond to 2D velocity (actions)
bounded between -1 and 1. Actions sampled from the policy are
shown as blue stars. Note that, in regions (e.g. (2.5, 2.5)) between
the goals, the method chooses multimodal actions.

5.2. Learning Multi-Modal Policies for Exploration

Though not all environments have a clear multi-modal
reward landscape as in the “multi-goal” example, multi-
modality is prevalent in a variety of tasks. For example,
a chess player might try various strategies before settling
on one that seems most effective, and an agent navigating a
maze may need to try various paths before ﬁnding the exit.
During the learning process, it is often best to keep trying
multiple available options until the agent is conﬁdent that
one of them is the best (similar to a bandit problem (Lai
& Robbins, 1985)). However, deep RL algorithms for con-
tinuous control typically use unimodal action distributions,
which are not well suited to capture such multi-modality.
As a consequence, such algorithms may prematurely com-
mit to one mode and converge to suboptimal behavior.

To evaluate how maximum entropy policies might aid ex-
ploration, we constructed simulated continuous control en-
vironments where tracking multiple modes is important
for success. The ﬁrst experiment uses a simulated swim-
ming snake (see Figure 2), which receives a reward equal
to its speed along the x-axis, either forward or backward.
However, once the swimmer swims far enough forward, it
crosses a “ﬁnish line” and receives a larger reward. There-
fore, the best learning strategy is to explore in both direc-
tions until the bonus reward is discovered, and then com-
mit to swimming forward. As illustrated in Figure 6 in
Appendix D.3, our method is able to recover this strategy,
keeping track of both modes until the ﬁnish line is discov-
ered. All stochastic policies eventually commit to swim-

(a) Swimming snake

(b) Quadrupedal robot

Figure 2. Simulated robots used in our experiments.

(a) Swimmer (higher is better) (b) Quadruped (lower is better)

Figure 3. Comparison of soft Q-learning and DDPG on the swim-
mer snake task and the quadrupedal robot maze task. (a) Shows
the maximum traveled forward distance since the beginning of
training for several runs of each algorithm; there is a large re-
ward after crossing the ﬁnish line.
(b) Shows our method was
able to reach a low distance to the goal faster and more consis-
tently. The different lines show the minimum distance to the goal
since the beginning of training. For both domains, all runs of our
method cross the threshold line, acquiring the more optimal strat-
egy, while some runs of DDPG do not.

ming forward. The deterministic DDPG method shown in
the comparison commits to a mode prematurely, with only
80% of the policies converging on a forward motion, and
20% choosing the suboptimal backward mode.

The second experiment studies a more complex task with a
continuous range of equally good options prior to discov-
ery of a sparse reward goal.
In this task, a quadrupedal
3D robot (adapted from Schulman et al. (2015b)) needs to
ﬁnd a path through a maze to a target position (see Fig-
ure 2). The reward function is a Gaussian centered at the
target. The agent may choose either the upper or lower pas-
sage, which appear identical at ﬁrst, but the upper passage
is blocked by a barrier. Similar to the swimmer experi-
ment, the optimal strategy requires exploring both direc-
tions and choosing the better one. Figure 3(b) compares
the performance of DDPG and our method. The curves
show the minimum distance to the target achieved so far
and the threshold equals the minimum possible distance if
the robot chooses the upper passage. Therefore, successful
exploration means reaching below the threshold. All poli-
cies trained with our method manage to succeed, while only
60% policies trained with DDPG converge to choosing the
lower passage.

Reinforcement Learning with Deep Energy-Based Policies

(a)

(b)

(c)

(d)

Figure 4. Quadrupedal robot (a) was trained to walk in random di-
rections in an empty pretraining environment (details in Figure 7,
see Appendix D.3), and then ﬁnetuned on a variety of tasks, in-
cluding a wide (b), narrow (c), and U-shaped hallway (d).

5.3. Accelerating Training on Complex Tasks with

Pretrained Maximum Entropy Policies

A standard way to accelerate deep neural network train-
ing is task-speciﬁc initialization (Goodfellow et al., 2016),
where a network trained for one task is used as initializa-
tion for another task. The ﬁrst task might be something
highly general, such as classifying a large image dataset,
while the second task might be more speciﬁc, such as ﬁne-
grained classiﬁcation with a small dataset. Pretraining has
also been explored in the context of RL (Shelhamer et al.,
2016). However, in RL, near-optimal policies are often
near-deterministic, which makes them poor initializers for
new tasks.
In this section, we explore how our energy-
based policies can be trained with fairly broad objectives
to produce an initializer for more quickly learning more
speciﬁc tasks.

We demonstrate this on a variant of the
quadrupedal robot task. The pretraining
phase involves learning to locomote in
an arbitrary direction, with a reward that
simply equals the speed of the center of
mass. The resulting policy moves the
agent quickly to an randomly chosen direction. An over-
head plot of the center of mass traces is shown above to
illustrate this. This pretraining is similar in some ways to
recent work on modulated controllers (Heess et al., 2016)
and hierarchical models (Florensa et al., 2017). However,
in contrast to these prior works, we do not require any task-
speciﬁc high-level goal generator or reward.

Figure 4 also shows a variety of test environments that we
used to ﬁnetune the running policy for a speciﬁc task. In
the hallway environments, the agent receives the same re-
ward, but the walls block sideways motion, so the optimal
solution requires learning to run in a particular direction.
Narrow hallways require choosing a more speciﬁc direc-
tion, but also allow the agent to use the walls to funnel
itself. The U-shaped maze requires the agent to learn a
curved trajectory in order to arrive at the target, with the
reward given by a Gaussian bump at the target location.

As illustrated in Figure 7 in Appendix D.3, the pretrained
policy explores the space extensively and in all directions.
This gives a good initialization for the policy, allowing it to

Figure 5. Performance in the downstream task with ﬁne-tuning
(MaxEnt) or training from scratch (DDPG). The x-axis shows the
training iterations. The y-axis shows the average discounted re-
turn. Solid lines are average values over 10 random seeds. Shaded
regions correspond to one standard deviation.

learn the behaviors in the test environments more quickly
than training a policy with DDPG from a random initializa-
tion, as shown in Figure 5. We also evaluated an alternative
pretraining method based on deterministic policies learned
with DDPG. However, deterministic pretraining chooses
an arbitrary but consistent direction in the training envi-
ronment, providing a poor initialization for ﬁnetuning to a
speciﬁc task, as shown in the results plots.

6. Discussion and Future Work
We presented a method for learning stochastic energy-
based policies with approximate inference via Stein vari-
ational gradient descent (SVGD). Our approach can be
viewed as a type of soft Q-learning method, with the ad-
ditional contribution of using approximate inference to ob-
tain complex multimodal policies. The sampling network
trained as part of SVGD can also be viewed as tking the role
of an actor in an actor-critic algorithm. Our experimental
results show that our method can effectively capture com-
plex multi-modal behavior on problems ranging from toy
point mass tasks to complex torque control of simulated
walking and swimming robots. The applications of train-
ing such stochastic policies include improved exploration
in the case of multimodal objectives and compositionality
via pretraining general-purpose stochastic policies that can
then be efﬁciently ﬁnetuned into task-speciﬁc behaviors.

While our work explores some potential applications of
energy-based policies with approximate inference, an ex-
citing avenue for future work would be to further study
their capability to represent complex behavioral repertoires
and their potential for composability. In the context of lin-
early solvable MDPs, several prior works have shown that
policies trained for different tasks can be composed to cre-
ate new optimal policies (Da Silva et al., 2009; Todorov,
2009). While these prior works have only explored simple,
tractable representations, our method could be used to ex-
tend these results to complex and highly multi-modal deep
neural network models, making them suitable for compos-
able control of complex high-dimensional systems, such as
humanoid robots. This composability could be used in fu-
ture work to create a huge variety of near-optimal skills
from a collection of energy-based policy building blocks.

Reinforcement Learning with Deep Energy-Based Policies

7. Acknowledgements

We thank Qiang Liu for insightful discussion of SVGD,
and thank Vitchyr Pong and Shane Gu for help with im-
plementing DDPG. Haoran Tang and Tuomas Haarnoja are
supported by Berkeley Deep Drive.

References

Da Silva, M., Durand, F., and Popovi´c, J. Linear Bellman
combination for control of character animation. ACM
Trans. on Graphs, 28(3):82, 2009.

Daniel, C., Neumann, G., and Peters, J. Hierarchical rel-
ative entropy policy search. In AISTATS, pp. 273–281,
2012.

Elfwing, S., Otsuka, M., Uchibe, E., and Doya, K. Free-
energy based reinforcement learning for vision-based
navigation with high-dimensional sensory inputs. In Int.
Conf. on Neural Information Processing, pp. 215–222.
Springer, 2010.

Florensa, C., Duan, Y., and P., Abbeel. Stochastic neural
networks for hierarchical reinforcement learning. In Int.
Conf. on Learning Representations, 2017.

Fox, R., Pakman, A., and Tishby, N. Taming the noise
in reinforcement learning via soft updates. In Conf. on
Uncertainty in Artiﬁcial Intelligence, 2016.

Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron.
Deep learning. chapter 8.7.4. MIT Press, 2016. http:
//www.deeplearningbook.org.

Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and
Levine, S. Q-prop: Sample-efﬁcient policy gradient with
an off-policy critic. arXiv preprint arXiv:1611.02247,
2016a.

Gu, S., Lillicrap, T., Sutskever, I., and Levine, S. Contin-
uous deep Q-learning with model-based acceleration. In
Int. Conf. on Machine Learning, pp. 2829–2838, 2016b.

Hafner, R. and Riedmiller, M. Reinforcement learning in
feedback control. Machine Learning, 84(1-2):137–169,
2011.

Heess, N., Silver, D., and Teh, Y. W. Actor-critic reinforce-
ment learning with energy-based policies. In Workshop
on Reinforcement Learning, pp. 43. Citeseer, 2012.

Heess, N., Wayne, G., Tassa, Y., Lillicrap, T., Riedmiller,
M., and Silver, D. Learning and transfer of modulated
locomotor controllers. arXiv preprint arXiv:1610.05182,
2016.

Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T.,
Leibo, J. Z., Silver, D., and Kavukcuoglu, K. Reinforce-
ment learning with unsupervised auxiliary tasks. arXiv
preprint arXiv:1611.05397, 2016.

Kaelbling, L. P., Littman, M. L., and Moore, A. W. Rein-
forcement learning: A survey. Journal of artiﬁcial intel-
ligence research, 4:237–285, 1996.

Kakade, S. A natural policy gradient. Advances in Neural
Information Processing Systems, 2:1531–1538, 2002.

Kappen, H. J. Path integrals and symmetry breaking for
optimal control theory. Journal of Statistical Mechanics:
Theory And Experiment, 2005(11):P11011, 2005.

Kim, T. and Bengio, Y. Deep directed generative models
with energy-based probability estimation. arXiv preprint
arXiv:1606.03439, 2016.

Kingma, D. and Ba, J. Adam: A method for stochastic

optimization. 2015.

Lai, T. L. and Robbins, H. Asymptotically efﬁcient adap-
tive allocation rules. Advances in Applied Mathematics,
6(1):4–22, 1985.

Levine, S. and Abbeel, P. Learning neural network policies
with guided policy search under unknown dynamics. In
Advances in Neural Information Processing Systems, pp.
1071–1079, 2014.

Levine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-end
training of deep visuomotor policies. Journal of Machine
Learning Research, 17(39):1–40, 2016.

Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.

Liu, Q. and Wang, D. Stein variational gradient descent:
A general purpose bayesian inference algorithm. In Ad-
vances In Neural Information Processing Systems, pp.
2370–2378, 2016.

Liu, Y., Ramachandran, P., Liu, Q., and Peng,

J.
arXiv preprint

Stein variational policy gradient.
arXiv:1704.02399, 2017.

Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,
Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing
atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A, Ve-
ness, J., Bellemare, M. G., Graves, A., Riedmiller, M.,
Fidjeland, A. K., Ostrovski, G., et al. Human-level con-
trol through deep reinforcement learning. Nature, 518
(7540):529–533, 2015.

Reinforcement Learning with Deep Energy-Based Policies

Sutton, R. S. and Barto, A. G. Reinforcement learning: An
introduction, volume 1. MIT press Cambridge, 1998.

Thomas, P. Bias in natural actor-critic algorithms. In Int.

Conf. on Machine Learning, pp. 441–448, 2014.

Todorov, E. Linearly-solvable Markov decision problems.
In Advances in Neural Information Processing Systems,
pp. 1369–1376. MIT Press, 2007.

Todorov, E. General duality between optimal control and
estimation. In IEEE Conf. on Decision and Control, pp.
4286–4292. IEEE, 2008.

Todorov, E. Compositionality of optimal control laws. In
Advances in Neural Information Processing Systems, pp.
1856–1864, 2009.

Toussaint, M. Robot trajectory optimization using approx-
imate inference. In Int. Conf. on Machine Learning, pp.
1049–1056. ACM, 2009.

Uhlenbeck, G. E. and Ornstein, L. S. On the theory of the
brownian motion. Physical review, 36(5):823, 1930.

Wang, D. and Liu, Q. Learning to draw samples: With
application to amortized mle for generative adversarial
learning. arXiv preprint arXiv:1611.01722, 2016.

Williams, Ronald J. Simple statistical gradient-following
learning.

algorithms for connectionist reinforcement
Machine learning, 8(3-4):229–256, 1992.

Zhao,

J., Mathieu, M., and LeCun, Y.

Energy-
based generative adversarial network. arXiv preprint
arXiv:1609.03126, 2016.

Ziebart, B. D. Modeling purposeful adaptive behavior with
the principle of maximum causal entropy. PhD thesis,
2010.

Ziebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K.
In
Maximum entropy inverse reinforcement learning.
AAAI Conference on Artiﬁcial Intelligence, pp. 1433–
1438, 2008.

Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
T. P., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-
chronous methods for deep reinforcement learning.
In
Int. Conf. on Machine Learning, 2016.

Neumann, G. Variational inference for policy search in
changing situations. In Int. Conf. on Machine Learning,
pp. 817–824, 2011.

O’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih,
V. PGQ: Combining policy gradient and Q-learning.
arXiv preprint arXiv:1611.01626, 2016.

Otsuka, M., Yoshimoto, J., and Doya, K. Free-energy-
based reinforcement learning in a partially observable
environment. In ESANN, 2010.

Peters, J., M¨ulling, K., and Altun, Y. Relative entropy pol-
icy search. In AAAI Conf. on Artiﬁcial Intelligence, pp.
1607–1612, 2010.

Rawlik, K., Toussaint, M., and Vijayakumar, S. On
stochastic optimal control and reinforcement learning by
approximate inference. Proceedings of Robotics: Sci-
ence and Systems VIII, 2012.

Sallans, B. and Hinton, G. E. Reinforcement learning with
factored states and actions. Journal of Machine Learning
Research, 5(Aug):1063–1088, 2004.

Schulman, J., Levine, S., Abbeel, P., Jordan, M. I., and
Moritz, P. Trust region policy optimization. In Int. Conf
on Machine Learning, pp. 1889–1897, 2015a.

Schulman, J., Moritz, P., Levine, S., Jordan, M., and
Abbeel, P. High-dimensional continuous control us-
ing generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015b.

Schulman, J., Abbeel, P., and Chen, X. Equivalence
arXiv

between policy gradients and soft Q-learning.
preprint arXiv:1704.06440, 2017.

Shelhamer, E., Mahmoudieh, P., Argus, M., and Darrell, T.
Loss is its own reward: Self-supervision for reinforce-
ment learning. arXiv preprint arXiv:1612.07307, 2016.

Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D.,
and Riedmiller, M. Deterministic policy gradient algo-
rithms. In Int. Conf on Machine Learning, 2014.

Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,
van den Driessche, G., Schrittwieser, J., Antonoglou, I.,
Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe,
D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap,
T., Leach, M., Kavukcuoglu, K., Graepel, T., and Has-
sabis, D. Mastering the game of go with deep neural
networks and tree search. Nature, 529(7587):484–489,
Jan 2016. ISSN 0028-0836. Article.

