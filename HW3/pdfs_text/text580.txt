Selective Inference for Sparse High-Order Interaction Models

A. Proofs

A.1. Proof of Lamma 1

Proof. In the following, we abbreviate j in Lemma 1 for
the simplicity of the notation unless there is no confusion,
and prove the lemma in slightly general case of V[y] = Σ.
To prove the lemma, we ﬁrst state the polyhedral lemma in
Lee et al. (2016) as follows:

Lemma 7 (Polyhedral Lemma; (Lee et al., 2016)). Sup-
1 for any
pose y ∼ N(µ, Σ). Let c = Ση(η(cid:62)Ση)−
η ∈ Rn, and let z = (In − cη(cid:62))y. Then we have

Pol(S) = {y ∈ Rn | Ay ≤ b}

(cid:26)

=

y ∈ Rn

(cid:12)
(cid:12)
(cid:12)
(cid:12)

L(S, z) ≤ η(cid:62)y ≤ U (S, z),
N (S, z) ≥ 0

(cid:27)

,

where

L(S, z) = max

j:(Ac)j <0

U (S, z) = min

j:(Ac)j >0

,

bj − (Az)j
(Ac)j
bj − (Az)j
(Ac)j

(13a)

(13b)

and N (S, z) = maxj:(Ac)j =0 bj − (Az)j.
(L(S, z), U (S, z), N (S, z)) is independent of η(cid:62)y.

In addition,

The polyhedral lemma allows us to construct a pivotal
quantity as a truncated normal distribution, that is, for any
z, we have

[F [L(S,z),U (S,z)]

0,η(cid:62)Ση

(η(cid:62)y)|y ∈ Pol(S)] ∼ Unif(0, 1),

(14)

where Unif(0, 1) denotes the standard (continuous) uni-
form distribution. In fact, by letting z0 be an arbitrary real-
ization of z, one can see that

[η(cid:62)y | y ∈ Pol(S), z = z0]
d
= [η(cid:62)y | L(S, z0) ≤ η(cid:62)y ≤ U (S, z0)]
∼ TN(η(cid:62)µ, η(cid:62)Ση, L(S, z0), U (S, z0)),

where d
= denotes the equality of random variables in distri-
bution. Therefore, probability integral transformation im-
plies

[F [L(S,z),U (S,z)]
η(cid:62)µ,η(cid:62)Ση

(η(cid:62)y) | y ∈ Pol(S), z = z0]

In the following, let us denote (S, z) by S for shorthand.
The remaining is to show that truncation points in Eqs.(13)
are equivalent to

θ s.t. y + θΣη ∈ Pol(S)

(15a)

(15b)

θ s.t. y + θΣη ∈ Pol(S),

respectively. Simple calculation shows that, for any θ ∈ R,
we have

L(S) = η(cid:62)y + θLη(cid:62)Ση

where θL = min
R

θ

∈

and

U (S) = η(cid:62)y + θU η(cid:62)Ση

where θU = max
R

θ

∈

y + θΣη ∈ Pol(S)
⇔ A(y + θΣη) ≤ b
⇔ θ · AΣη ≤ b − Ay.






⇔

θ ≤ (b − Ay)j/(AΣη)j,
θ ≥ (b − Ay)j/(AΣη)j,
0 ≤ (b − Ay)j,

(AΣη)j > 0
(AΣη)j < 0
(AΣη)j = 0

.

On the other hand, by the deﬁnition of c and z in Lemma
7, it is easy to see that

L(S) = η(cid:62)y + η(cid:62)Ση max

j:(AΣη)j <0

(b − Ay)j
(AΣη)j

Therefore, for each j such that (AΣη)j < 0, we have

max
j:(AΣη)j <0

(b − Ay)j
(AΣη)j

≤ θ

and thus the minimum possible feasible θ would be

θL = min{θ ∈ R | y + θΣη ∈ Pol(S)}

= max

j:(AΣη)j <0

(b − Ay)j
(AΣη)j

.

Similarly, we see that the equivalency of U (S).

To complete the proof, let us consider a Gaussian random
variable y with mean Xβ∗ and covariance matrix σ2In
with some constant σ2. We can choose η = (X +
S )(cid:62)ej for
testing the null hypothesis H0,j : β∗S,j = 0 for each j ∈ S,
since η(cid:62)y reduces to the j-th element of an ordinary least
square estimator for the selected model, and in this case,
η(cid:62)Ση reduces to

has a uniform distribution Unif(0, 1) for any z0. By inte-
grating out z0, the pivotal quantity Eq.(14) holds. In ad-
dition, an lower α-percentile of the distribution can be ob-
tained as

S = σ2(cid:107)η(cid:107)2 = σ2(X (cid:62)S XS)−
σ2

1
jj .

Then the critical values are computed as

qα = (F [L(S,z),U (S,z)]

η(cid:62)µ,η(cid:62)Ση

)−

1(α).

α/2 = qα/2 = (F [L(S),U (S)]
(cid:96)S

)−

1(α/2)

0,σ2
S

Selective Inference for Sparse High-Order Interaction Models

uS
α/2 = q1

α/2 = (F [L(S),U (S)]
0,σ2
S

−

)−

1(1 − α/2),

respectively. From the above argument, there are no matter
to compute the truncation points in Eqs.(15) based on the
observations. In this case, Eqs.(15) can be written as

1
L(S) = η(cid:62)y + θLσ2(X (cid:62)S XS)−
jj
where θL = min
R

θ s.t. y + θσ2(X +

S )(cid:62)ej ∈ Pol(S)

1
U (S) = η(cid:62)y + θU σ2(X (cid:62)S XS)−
jj
where θU = max
R

θ s.t. y + θσ2(X +

S )(cid:62)ej ∈ Pol(S),

respectively, but we can ignore the scaling factor σ2 be-
cause

min{θ ∈ Rn | y + θ(X +
= min{σ2θ ∈ Rn | y + θσ2(X +

S )(cid:62)(cid:62)ej ∈ Pol(S)}

S )(cid:62)ej ∈ Pol(S)}

θ

∈

θ

∈

and

and

and

max{θ ∈ Rn | y + θ(X +
= max{σ2θ ∈ Rn | y + θσ2(X +

S )(cid:62)ej ∈ Pol(S)}

S )(cid:62)ej ∈ Pol(S)}.

A.2. Proof of Lamma 3

Proof. Since xij ∈ [0, 1], for any pair (j, ˜j) such that ˜j ∈
Des(j), xj ≥ x˜j holds. Then,

A.3. Proof of Lemma 4

Proof. In MS, from Eq.(9), the constraint y +θη ∈ Pol(S)
is written as

(−sjx
j − xj(cid:48))(cid:62)(y + θη) ≤ 0
·
−(sjx
j + x
j(cid:48))(cid:62)y
·
·
j(cid:48))(cid:62)η
j + x
(sjx
·
·

≤ θ if (sjx

⇔

j + x
·

j(cid:48))(cid:62)η > 0,
·

and

−(sjx
j(cid:48))(cid:62)y
j + x
·
·
j(cid:48))(cid:62)η
j + x
(sjx
·
·

≥ θ if (sjx

j + x
·

j(cid:48))(cid:62)η < 0.
·

j + xj(cid:48))(cid:62)(y + θη) ≤ 0
(−sjx
·
−(sjx
j − x
j(cid:48))(cid:62)y
·
·
j − x
j(cid:48))(cid:62)η
(sjx
·
·

≤ θ if (sjx

⇔

j − x
·

j(cid:48))(cid:62)η > 0
·

and

j − x
−(sjx
j(cid:48))(cid:62)y
·
·
j − x
j(cid:48))(cid:62)η
(sjx
·
·

≥ θ if (sjx

j − x
·

j(cid:48))(cid:62)η < 0.
·

≤ θ if sjx(cid:62)
j η > 0
·

j (y + θη) ≤ 0
− sjx(cid:62)
·
−sjx(cid:62)
j y
·
j η
sjx(cid:62)
·
−sjx(cid:62)
j y
·
sjx(cid:62)
j η
·

≥ θ if sjx(cid:62)
j η < 0
·

⇔

and

for all (j, j(cid:48)) ∈ S × ¯S. The conditions in Eqs.(16a), (16c),
and (16e) suggests that −θL must be at least smaller than
θ(a)
L in Eq.(11a), θ(b)
L in the second
last inequality in Eq.(11), respectively. Therefore, we have

L in Eq.(11c), and θ(c)

θL = − min{θ(a)

L , θ(b)

L , θ(c)

L }.

(cid:4)

Similarly, the conditions in Eqs.(16b), (16d), and (16f) im-
ply that

θL = − max{θ(a)

U , θ(b)

U , θ(c)

U }.

(16a)

(16b)

(16c)

(16d)

(16e)

(16f)

(cid:4)

˜j y| = |
|x(cid:62)
·

(cid:88)

i:yi>0



≤ max

xi˜jyi +

xi˜jyi|

(cid:88)

i:yi<0

(cid:88)

(cid:88)

xi˜jyi, −

xi˜jyi

i:yi>0

i:yi<0







≤ max

(cid:88)

xijyi, −

xijyi

.

(cid:88)

i:yi>0

i:yi<0

A.4. Proof of Lemma 5










(cid:4)

Proof. First, note that 0 ≤ xi˜j(cid:48) ≤ xij(cid:48) ≤ 1 for any
(j, j(cid:48), ˜j(cid:48)) ∈ S × ¯S × Desj(j(cid:48)). We ﬁrst prove Eq.(12a).

(sjx

j + x
·

˜j(cid:48))(cid:62)y = sjx(cid:62)
·
·

j y +

xi˜j(cid:48)yi +

xi˜j(cid:48)yi

(cid:88)

i:yi<0

(cid:88)

i:yi>0
(cid:88)

i:yi<0
(cid:88)

i:yi<0

≥ sjx(cid:62)
·

j y +

≥ sjx(cid:62)
·

j y +

xi˜j(cid:48)yi.

xij(cid:48)yi = L(a)
E ,

Selective Inference for Sparse High-Order Interaction Models

which proves the ﬁrst line. Next, we prove Eq.(12b).

B. Selectivxe inference for OMP

(sjx

j + x
·

j y +
˜j(cid:48))(cid:62)y = sjx(cid:62)
·
·

xi˜j(cid:48)yi +

xi˜j(cid:48)yi

(cid:88)

i:yi<0

Lemma 8. Let η := (X +)(cid:62)ej. The solutions of the opti-
mization problems in (7) are respectively written as

(cid:88)

i:yi>0
(cid:88)

i:yi>0
(cid:88)

i:yi>0

≤ sjx(cid:62)
j y +
·

≤ sjx(cid:62)
j y +
·

xi˜j(cid:48)yi.

xij(cid:48)yi = U (a)
E ,

which proves the second line. Eqs.
proved similarly.

(12c) to (12h) are
(cid:4)

A.5. Proof of Theorem 6

Proof. First, we prove (i). For any (j, j(cid:48), ˜j(cid:48)) ∈ S × ¯S ×
Desj(j(cid:48)), by using Lemma 5 directly, a lower and an upper
bound of sjx(cid:62)
˜j(cid:48)y can be obtained as
j y + x(cid:62)
·
·

L(a)

E ≤ sjx(cid:62)
·

j y + x(cid:62)
·

˜j(cid:48)y ≤ U (a)

E

(17)

where

Similarly, a lower and an upper bound of sjx(cid:62)
˜j(cid:48)η
j η + x(cid:62)
·
·
can be also obtained as

L(a)

D ≤ sjx(cid:62)
·

j η + x(cid:62)
·

˜j(cid:48)η ≤ U (a)

D

(18)

From Eq.(18), we have

U (a)

˜j(cid:48))(cid:62)η < 0
·

D < 0 ⇒ (sjx

j + x
·
for all (j, ˜j(cid:48)) ∈ S × Desj(j(cid:48)). It means that the (j, ˜j(cid:48))-th
constraint does not affect the solution of the optimization
problem in Eq.(11a). Now, we consider the case of U (a)
D >
0. If L(a)

D > 0, the value

(sjx
j + x
·
j + x
(sjx
·

j(cid:48))(cid:62)y
·
j(cid:48))(cid:62)η
·

E /U (a)

D when L(a)

can be bounded below by L(a)
D when L(a)
E /L(a)
L(a)
small values if L(a)
timal solution ˆθ(a)
solution of the optimization problem Eq.(11a), if

E > 0, and
E < 0, while the value can take any
D < 0. As a result, for the current op-
L , (j, j(cid:48))-th constraint does not affect the

D > 0, L(a)
L(a)

E > 0 and

> ˆθ(a)
L ,

or

D > 0, L(a)
L(a)

E < 0 and

> ˆθ(a)
L ,

because L(a)
prove (ii) – (iv) by the same argument.

D > 0 implies U (a)

D > 0. Similarly, we can
(cid:4)

L(a)
E
U (a)
D

L(a)
E
L(a)
D

θL = − min{θ(a)
θU = − max{θ(a)

L , θ(b)
U , θ(b)

L , θ(c)
L },
U , θ(c)
U },

θ(a)
L :=

min
[k], j(cid:48)
∈
(s(h)x·(h)+x·j(cid:48) )(cid:62)PSh η>0

¯Sh,

∈

h

j(cid:48))(cid:62)PShy
(s(h)x
(h) + x
·
·
j(cid:48))(cid:62)PSh η
(h) + x
(s(h)x
·
·

,

θ(b)
L :=

min
[k], j(cid:48)
∈

¯Sh,
x·j(cid:48) )(cid:62)PSh η>0

h
(s(h)x·(h)−

∈

(s(h)x
(s(h)x

(h) − x
j(cid:48))(cid:62)PSh y
·
·
(h) − x
j(cid:48))(cid:62)PSh η
·
·

,

θ(c)
L :=

min
h
[k],
∈
s(h)x(cid:62)
·(h)PSh η>0

(h)PShy
s(h)x(cid:62)
·
(h)PSh η
s(h)x(cid:62)
·

,

θ(a)
U :=

max
[k], j(cid:48)
∈
(s(h)x·(h)+x·j(cid:48) )(cid:62)PSh η<0

¯Sh,

∈

h

j(cid:48))(cid:62)PShy
(s(h)x
(h) + x
·
·
j(cid:48))(cid:62)PSh η
(h) + x
(s(h)x
·
·

,

θ(b)
U :=

max
[k], j(cid:48)
∈

¯Sh,
x·j(cid:48) )(cid:62)PSh η<0

h
(s(h)x·(h)−

∈

(s(h)x
(s(h)x

(h) − x
j(cid:48))(cid:62)PSh y
·
·
(h) − x
j(cid:48))(cid:62)PSh η
·
·

,

(19a)

(19b)

(19c)

(19d)

(19e)

(19f)

θ(c)
U :=

max
[k],
h
∈
s(h)x(cid:62)
·(h)PSh η<0

(h)PShy
s(h)x(cid:62)
·
(h)PSh η
s(h)x(cid:62)
·

.

Selective Inference for Sparse High-Order Interaction Models

Lemma 9. For any h ∈ [k] and (j(cid:48), ˜j(cid:48)) ∈ ¯Sh ×Des(h)(j(cid:48)),

(iii) Furthermore, consider solving the optimization prob-
lem in Eq.(19d), and let ˆθ(a)
U be the current optimal solu-
tion. If

{L(a)

D > 0} ∪ {U (a)
∪ {U (a)

D < 0, L(a)
D < 0, L(a)

E < 0, L(a)
E > 0, L(a)

E /U (a)
E /L(a)

D > ˆθ(a)
U }
D > ˆθ(a)
U }

is true, then the ˜j(cid:48)-th constraint in Eq. (10a) for any h ∈ [k]
and (j(cid:48), ˜j(cid:48)) ∈ ¯Sh × Des(h)(j(cid:48)) does not affect the optimal
solution in Eq.(19d).

(iv) Finally, consider solving the optimization problem in
Eq.(19e), and let ˆθ(b)

U be the current optimal solution. If

{L(b)

D > 0} ∪ {U (b)
∪ {U (b)

D < 0, L(b)
D < 0, L(b)

E < 0, L(b)
E > 0, L(b)

E /U (b)
E /L(b)

D > ˆθ(b)
U }
D > ˆθ(b)
U }

xij(cid:48)[PSh y]i

i:[PSh y]i>0

is true, then the (˜j(cid:48)-th constraint in Eq. (10b) for any
h ∈ [k] and (j(cid:48), ˜j(cid:48)) ∈ ¯Sh × Des(h)(j(cid:48)) does not affect
the optimal solution in Eq.(19e).

L(a)

(h)PSh y +
E := s(h)x(cid:62)
·

(cid:88)

xij(cid:48)[PSh y]i

i:[PSh y]i<0

≤ (s(h)x

˜j(cid:48))(cid:62)PSh y,
(h) + x
·
·
(cid:88)
(h)PSh y +
E := s(h)x(cid:62)
·

U (a)

i:[PSh y]i>0

xij(cid:48)[PSh y]i

L(a)

≥ (s(h)x

˜j(cid:48))(cid:62)PSh y,
(h) + x
·
·
(cid:88)
(h)η +
D := s(h)x(cid:62)
·

≤ (s(h)x

i:[PSh η]i<0
˜j(cid:48))(cid:62)PSh η,
(h) + x
·
·
(cid:88)
(h)η +
D := s(h)x(cid:62)
·

≥ (s(h)x

i:[PSh η]i>0
˜j(cid:48))(cid:62)PSh η,
(h) + x
·
·
(cid:88)
(h)PSh y −
E := s(h)x(cid:62)
·

L(b)

U (a)

xij(cid:48)[PShη]i

xij(cid:48)[PShη]i

≤ (s(h)x

(h) − x
˜j(cid:48))(cid:62)PSh y,
·
·
(cid:88)
(h)PSh y −
E := s(h)x(cid:62)
·

U (b)

i:[PSh y]i<0

xij(cid:48)[PSh y]i

L(b)

≥ (s(h)x

(h) − x
˜j(cid:48))(cid:62)PSh y,
·
·
(cid:88)
(h)η −
D := s(h)x(cid:62)
·

≤ (s(h)x

i:[PSh η]i>0
(h) − x
˜j(cid:48))(cid:62)PSh η,
·
·
(cid:88)
(h)η −
D := s(h)x(cid:62)
·

U (b)

≥ (s(h)x

i:[PSh η]i<0
(h) − x
˜j(cid:48))(cid:62)PSh η.
·
·

xij(cid:48)[PShη]i

xij(cid:48)[PShη]i

Theorem 10. (i) Consider solving the optimization prob-
lem in Eq.(19a), and let ˆθ(a)
L be the current optimal solu-
tion, i.e., we know that the optimal θ(a)
L is at least no greater
than ˆθ(a)

{U (a)

L . If
D < 0} ∪ {L(a)
∪ {L(a)

D > 0, L(a)
D > 0, L(a)

E < 0, L(a)
E > 0, L(a)

E /L(a)
E /U (a)

D > ˆθ(a)
L }
D > ˆθ(a)
L }

is true, then the ˜j(cid:48)-th constraint in Eq. (10a) for any h ∈ [k]
and (j(cid:48), ˜j(cid:48)) ∈ ¯Sh × Des(h)(j(cid:48)) does not affect the optimal
solution in Eq.(19a).

(ii) Next, consider solving the optimization problem in
Eq.(19b), and let ˆθ(b)
D < 0} ∪ {L(b)
∪ {L(b)

L be the current optimal solution. If
D < ˆθ(b)
E < 0, L(b)
D > 0, L(b)
L }
D < ˆθ(b)
E > 0, L(b)
D > 0, L(b)
L }

E /L(b)
E /U (b)

{U (b)

is true, then the ˜j(cid:48)-th constraint in Eq. (10b) for any h ∈ [k]
and (j(cid:48), ˜j(cid:48)) ∈ ¯Sh × Des(h)(j(cid:48)) does not affect the optimal
solution in Eq.(19b).

