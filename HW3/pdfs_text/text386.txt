Deep Transfer Learning with Joint Adaptation Networks

Mingsheng Long 1 Han Zhu 1 Jianmin Wang 1 Michael I. Jordan 2

Abstract

Deep networks have been successfully applied to
learn transferable features for adapting models
from a source domain to a different target domain.
In this paper, we present joint adaptation networks
(JAN), which learn a transfer network by aligning
the joint distributions of multiple domain-speciﬁc
layers across domains based on a joint maximum
mean discrepancy (JMMD) criterion. Adversarial
training strategy is adopted to maximize JMMD
such that the distributions of the source and target
domains are made more distinguishable. Learning
can be performed by stochastic gradient descent
with the gradients computed by back-propagation
in linear-time. Experiments testify that our model
yields state of the art results on standard datasets.

1. Introduction

Deep networks have signiﬁcantly improved the state of the
arts for diverse machine learning problems and applications.
Unfortunately, the impressive performance gains come only
when massive amounts of labeled data are available for
supervised learning. Since manual labeling of sufﬁcient
training data for diverse application domains on-the-ﬂy is
often prohibitive, for a target task short of labeled data,
there is strong motivation to build effective learners that can
leverage rich labeled data from a different source domain.
However, this learning paradigm suffers from the shift in
data distributions across different domains, which poses a
major obstacle in adapting predictive models for the target
task (Quionero-Candela et al., 2009; Pan & Yang, 2010).

Learning a discriminative model in the presence of the shift
between training and test distributions is known as transfer
learning or domain adaptation (Pan & Yang, 2010). Previous

1Key Lab for Information System Security, MOE; Tsinghua Na-
tional Lab for Information Science and Technology (TNList); NEL-
BDS; School of Software, Tsinghua University, Beijing 100084,
China 2University of California, Berkeley, Berkeley 94720. Corre-
spondence to: Mingsheng Long <mingsheng@tsinghua.edu.cn>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

shallow transfer learning methods bridge the source and tar-
get domains by learning invariant feature representations or
estimating instance importance without using target labels
(Huang et al., 2006; Pan et al., 2011; Gong et al., 2013). Re-
cent deep transfer learning methods leverage deep networks
to learn more transferable representations by embedding
domain adaptation in the pipeline of deep learning, which
can simultaneously disentangle the explanatory factors of
variations behind data and match the marginal distributions
across domains (Tzeng et al., 2014; 2015; Long et al., 2015;
2016; Ganin & Lempitsky, 2015; Bousmalis et al., 2016).

Transfer learning becomes more challenging when domains
may change by the joint distributions of input features and
output labels, which is a common scenario in practical ap-
plications. First, deep networks generally learn the complex
function from input features to output labels via multilayer
feature transformation and abstraction. Second, deep fea-
tures in standard CNNs eventually transition from general to
speciﬁc along the network, and the transferability of features
and classiﬁers decreases when the cross-domain discrepancy
increases (Yosinski et al., 2014). Consequently, after feed-
forwarding the source and target domain data through deep
networks for multilayer feature abstraction, the shifts in the
joint distributions of input features and output labels still
linger in the network activations of multiple domain-speciﬁc
higher layers. Thus we can use the joint distributions of the
activations in these domain-speciﬁc layers to approximately
reason about the original joint distributions, which should
be matched across domains to enable domain adaptation. To
date, this problem has not been addressed in deep networks.

In this paper, we present Joint Adaptation Networks (JAN)
to align the joint distributions of multiple domain-speciﬁc
layers across domains for unsupervised domain adaptation.
JAN largely extends the ability of deep adaptation networks
(Long et al., 2015) to reason about the joint distributions
as mentioned above, while keeping the training procedure
even simpler. Speciﬁcally, JAN admits a simple transfer
pipeline, which processes the source and target domain data
by convolutional neural networks (CNN) and then aligns
the joint distributions of activations in multiple task-speciﬁc
layers. To learn parameters and enable alignment, we derive
joint maximum mean discrepancy (JMMD), which measures
the Hilbert-Schmidt norm between kernel mean embedding
of empirical joint distributions of source and target data.

Deep Transfer Learning with Joint Adaptation Networks

Thanks to a linear-time unbiased estimate of JMMD, we can
easily draw a mini-batch of samples to estimate the JMMD
criterion, and implement it efﬁciently via back-propagation.
We further maximize JMMD using adversarial training strat-
egy such that the distributions of source and target domains
are made more distinguishable. Empirical study shows that
our models yield state of the art results on standard datasets.

the problem tractable (Zhang et al., 2013). As it is not easy
to justify which components of the joint distribution are
changing in practice, our work is transparent to diverse sce-
narios by directly manipulating the joint distribution without
assumptions on the marginal and conditional distributions.
Furthermore, it remains unclear how to account for the shift
in joint distributions within the regime of deep architectures.

2. Related Work

Transfer learning (Pan & Yang, 2010) aims to build learning
machines that generalize across different domains following
different probability distributions (Sugiyama et al., 2008;
Pan et al., 2011; Duan et al., 2012; Gong et al., 2013; Zhang
et al., 2013). Transfer learning ﬁnds wide applications in
computer vision (Saenko et al., 2010; Gopalan et al., 2011;
Gong et al., 2012; Hoffman et al., 2014) and natural lan-
guage processing (Collobert et al., 2011; Glorot et al., 2011).

The main technical problem of transfer learning is how
to reduce the shifts in data distributions across domains.
Most existing methods learn a shallow representation model
by which domain discrepancy is minimized, which cannot
suppress domain-speciﬁc exploratory factors of variations.
Deep networks learn abstract representations that disentan-
gle the explanatory factors of variations behind data (Bengio
et al., 2013) and extract transferable factors underlying dif-
ferent populations (Glorot et al., 2011; Oquab et al., 2013),
which can only reduce, but not remove, the cross-domain
discrepancy (Yosinski et al., 2014). Recent work on deep
domain adaptation embeds domain-adaptation modules into
deep networks to boost transfer performance (Tzeng et al.,
2014; 2015; 2017; Ganin & Lempitsky, 2015; Long et al.,
2015; 2016). These methods mainly correct the shifts in
marginal distributions, assuming conditional distributions
remain unchanged after the marginal distribution adaptation.

Transfer learning will become more challenging as domains
may change by the joint distributions P (X, Y) of input fea-
tures X and output labels Y. The distribution shifts may
stem from the marginal distributions P (X) (a.k.a. covari-
ate shift (Huang et al., 2006; Sugiyama et al., 2008)), the
conditional distributions P (Y|X) (a.k.a. conditional shift
(Zhang et al., 2013)), or both (a.k.a. dataset shift (Quionero-
Candela et al., 2009)). Another line of work (Zhang et al.,
2013; Wang & Schneider, 2014) correct both target and con-
ditional shifts based on the theory of kernel embedding of
conditional distributions (Song et al., 2009; 2010; Sriperum-
budur et al., 2010). Since the target labels are unavailable,
adaptation is performed by minimizing the discrepancy be-
tween marginal distributions instead of conditional distri-
butions. In general, the presence of conditional shift leads
to an ill-posed problem, and an additional assumption that
the conditional distribution may only change under location-
scale transformations on X is commonly imposed to make

3. Preliminary

3.1. Hilbert Space Embedding

We begin by providing an overview of Hilbert space embed-
dings of distributions, where each distribution is represented
by an element in a reproducing kernel Hilbert space (RKHS).
Denote by X a random variable with domain Ω and distribu-
tion P (X), and by x the instantiations of X. A reproducing
kernel Hilbert space (RKHS) H on Ω endowed by a kernel
k (x, x(cid:48)) is a Hilbert space of functions f : Ω (cid:55)→ R with
inner product (cid:104)·, ·(cid:105)H. Its element k (x, ·) satisﬁes the repro-
ducing property: (cid:104)f (·) , k (x, ·)(cid:105)H = f (x). Alternatively,
k (x, ·) can be viewed as an (inﬁnite-dimensional) implicit
feature map φ (x) where k (x, x(cid:48)) = (cid:104)φ (x) , φ (x(cid:48))(cid:105)H. Ker-
nel functions can be deﬁned on vector space, graphs, time
series and structured objects to handle diverse applications.
The kernel embedding represents a probability distribution
P by an element in RKHS endowed by a kernel k (Smola
et al., 2007; Sriperumbudur et al., 2010; Gretton et al., 2012)

µX (P ) (cid:44) EX [φ (X)] =

φ (x) dP (x),

(1)

(cid:90)

Ω

where the distribution is mapped to the expected feature map,
i.e. to a point in the RKHS, given that EX [k (x, x(cid:48))] (cid:54) ∞.
The mean embedding µX has the property that the expecta-
tion of any RKHS function f can be evaluated as an inner
(cid:44) EX [f (X)] , ∀f ∈ H. This kind
product in H, (cid:104)µX, f (cid:105)H
of kernel mean embedding provides us a nonparametric per-
spective on manipulating distributions by drawing samples
from them. We will require a characteristic kernel k such
that the kernel embedding µX (P ) is injective, and that the
embedding of distributions into inﬁnite-dimensional feature
spaces can preserve all of the statistical features of arbitrary
distributions, which removes the necessity of density estima-
tion of P . This technique has been widely applied in many
tasks, including feature extraction, density estimation and
two-sample test (Smola et al., 2007; Gretton et al., 2012).

While the true distribution P (X) is rarely accessible, we
can estimate its embedding using a ﬁnite sample (Gretton
et al., 2012). Given a sample DX = {x1, . . . , xn} of size n
drawn i.i.d. from P (X), the empirical kernel embedding is

(cid:98)µX =

φ (xi).

1
n

n
(cid:88)

i=1

(2)

Deep Transfer Learning with Joint Adaptation Networks

(cid:90)

=

×m

(cid:96)=1Ω(cid:96)

CX1:m(P ) (cid:44) EX1:m

This empirical estimate converges to its population counter-
part in RKHS norm (cid:107)µX − (cid:98)µX(cid:107)H with a rate of O(n− 1
2 ).
Kernel embeddings can be readily generalized to joint distri-
butions of two or more variables using tensor product feature
spaces (Song et al., 2009; 2010; Song & Dai, 2013). A joint
distribution P of variables X1, . . . , Xm can be embedded
into an m-th order tensor product feature space ⊗m
(cid:96)=1H(cid:96) by
(cid:2)⊗m

(cid:96)=1φ(cid:96) (cid:0)X(cid:96)(cid:1)(cid:3)
(cid:96)=1φ(cid:96) (cid:0)x(cid:96)(cid:1)(cid:1) dP (cid:0)x1, . . . , xm(cid:1),
(cid:0)⊗m
(3)
where X1:m denotes the set of m variables {X1, . . . , Xm}
(cid:96)=1Ω(cid:96) = Ω1 × . . . × Ωm, φ(cid:96) is the feature
on domain ×m
map endowed with kernel k(cid:96) in RKHS H(cid:96) for variable X(cid:96),
(cid:96)=1φ(cid:96) (cid:0)x(cid:96)(cid:1) = φ1 (cid:0)x1(cid:1)⊗. . .⊗φm (xm) is the feature map
⊗m
in the tensor product Hilbert space, where the inner product
(cid:96)=1 k(cid:96)(x(cid:96), x(cid:48)(cid:96)).
satisﬁes (cid:104)⊗m
The joint embeddings can be viewed as an uncentered cross-
covariance operator CX1:m by the standard equivalence be-
tween tensor and linear map (Song et al., 2010). That is,
given a set of functions f 1, . . . , f m, their covariance can be
(cid:11).
computed by EX1:m
When the true distribution P (X1, . . . , Xm) is unknown, we
can estimate its embedding using a ﬁnite sample (Song et al.,
2013). Given a sample DX1:m = {x1:m
n } of size
n drawn i.i.d. from P (X1, . . . , Xm), the empirical joint
embedding (the cross-covariance operator) is estimated as

(cid:96)=1 f (cid:96)(X(cid:96))(cid:3) = (cid:10)⊗m

(cid:96)=1φ(cid:96)(x(cid:48)(cid:96))(cid:105) = (cid:81)m

(cid:96)=1φ(cid:96)(x(cid:96)), ⊗m

(cid:96)=1f (cid:96), CX1:m

, . . . , x1:m

(cid:2)(cid:81)m

1

(cid:98)CX1:m =

(cid:96)=1φ(cid:96) (cid:0)x(cid:96)
⊗m

i

(cid:1).

(4)

1
n

n
(cid:88)

i=1

This empirical estimate converges to its population counter-
part with a similar convergence rate as marginal embedding.

3.2. Maximum Mean Discrepancy

1, . . . , xt
nt

1, . . . , xs
ns

} and DXt = {xt

Let DXs = {xs
} be
the sets of samples from distributions P (Xs) and Q(Xt),
respectively. Maximum Mean Discrepancy (MMD) (Gret-
ton et al., 2012) is a kernel two-sample test which rejects or
accepts the null hypothesis P = Q based on the observed
samples. The basic idea behind MMD is that if the generat-
ing distributions are identical, all the statistics are the same.
Formally, MMD deﬁnes the following difference measure:
(cid:2)f (cid:0)Xt(cid:1)(cid:3)(cid:1) , (5)

(cid:0)EXs [f (Xs)] − EXt

DH (P, Q) (cid:44) sup
f ∈H

where H is a class of functions. It is shown that the class
of functions in an universal RKHS H is rich enough to
distinguish any two distributions and MMD is expressed as
the distance between their mean embeddings: DH (P, Q) =
(cid:107)µXs (P ) − µXt (Q)(cid:107)2
H. The main theoretical result is that
P = Q if and only if DH (P, Q) = 0 (Gretton et al., 2012).

In practice, an estimate of the MMD compares the square
distance between the empirical kernel mean embeddings as
ns(cid:88)

ns(cid:88)

(cid:98)DH (P, Q) =

k (cid:0)xs

i , xs
j

(cid:1)

1
n2
s

1
n2
t

i=1

j=1

nt(cid:88)

nt(cid:88)

i=1

j=1

+

−

2
nsnt

ns(cid:88)

nt(cid:88)

i=1

j=1

k (cid:0)xs

i , xt
j

(cid:1),

k (cid:0)xt

i, xt
j

(cid:1)

(6)

where (cid:98)DH (P, Q) is an unbiased estimator of DH (P, Q).

4. Joint Adaptation Networks

i , ys

In unsupervised domain adaptation, we are given a source
i )}ns
domain Ds = {(xs
i=1 of ns labeled examples and
j}nt
a target domain Dt = {xt
j=1 of nt unlabeled examples.
The source domain and target domain are sampled from
joint distributions P (Xs, Ys) and Q(Xt, Yt) respectively,
P (cid:54)= Q. The goal of this paper is to design a deep neural
network y = f (x) which formally reduces the shifts in
the joint distributions across domains and enables learning
both transferable features and classiﬁers, such that the target
risk Rt (f ) = E(x,y)∼Q [f (x) (cid:54)= y] can be minimized by
jointly minimizing the source risk and domain discrepancy.

Recent studies reveal that deep networks (Bengio et al.,
2013) can learn more transferable representations than tra-
ditional hand-crafted features (Oquab et al., 2013; Yosinski
et al., 2014). The favorable transferability of deep features
leads to several state of the art deep transfer learning meth-
ods (Ganin & Lempitsky, 2015; Tzeng et al., 2015; Long
et al., 2015; 2016). This paper also tackles unsupervised
domain adaptation by learning transferable features using
deep neural networks. We extend deep convolutional neural
networks (CNNs), including AlexNet (Krizhevsky et al.,
2012) and ResNet (He et al., 2016), to novel joint adaptation
networks (JANs) as shown in Figure 1. The empirical error
of CNN classiﬁer f (x) on source domain labeled data Ds is

min
f

1
ns

ns(cid:88)

i=1

J (f (xs

i ) , ys

i ),

(7)

where J(·, ·) is the cross-entropy loss function. Based on the
quantiﬁcation study of feature transferability in deep con-
volutional networks (Yosinski et al., 2014), convolutional
layers can learn generic features that are transferable across
domains (Yosinski et al., 2014). Thus we opt to ﬁne-tune
the features of convolutional layers when transferring pre-
trained deep models from source domain to target domain.

However, the literature ﬁndings also reveal that the deep
features can reduce, but not remove, the cross-domain distri-
bution discrepancy (Yosinski et al., 2014; Long et al., 2015;

Deep Transfer Learning with Joint Adaptation Networks

(a) Joint Adaptation Network (JAN)

(b) Adversarial Joint Adaptation Network (JAN-A)

Figure 1. The architectures of Joint Adaptation Network (JAN) (a) and its adversarial version (JAN-A) (b). Since deep features eventually
transition from general to speciﬁc along the network, activations in multiple domain-speciﬁc layers L are not safely transferable. And the
joint distributions of the activations P (Zs1, . . . , Zs|L|) and Q(Zt1, . . . , Zt|L|) in these layers should be adapted by JMMD minimization.

2016). The deep features in standard CNNs must eventually
transition from general to speciﬁc along the network, and the
transferability of features and classiﬁers decreases when the
cross-domain discrepancy increases (Yosinski et al., 2014).
In other words, even feed-forwarding the source and target
domain data through the deep network for multilayer feature
abstraction, the shifts in the joint distributions P (Xs, Ys)
and Q(Xt, Yt) still linger in the activations Z1, . . . , Z|L| of
the higher network layers L. Taking AlexNet (Krizhevsky
et al., 2012) as an example, the activations in the higher fully-
connected layers L = {f c6, f c7, f c8} are not safely trans-
ferable for domain adaptation (Yosinski et al., 2014). Note
that the shift in the feature distributions P (Xs) and Q(Xt)
mainly lingers in the feature layers f c6, f c7 while the shift
in the label distributions P (Ys) and Q(Yt) mainly lingers
in the classiﬁer layer f c8. Thus we can use the joint distribu-
tions of the activations in layers L, i.e. P (Zs1, . . . , Zs|L|)
and Q(Zt1, . . . , Zt|L|) as good surrogates of the original
joint distributions P (Xs, Ys) and Q(Xt, Yt), respectively.
To enable unsupervised domain adaptation, we should ﬁnd
a way to match P (Zs1, . . . , Zs|L|) and Q(Zt1, . . . , Zt|L|).

4.1. Joint Maximum Mean Discrepancy

Many existing methods address transfer learning by bound-
ing the target error with the source error plus a discrepancy
between the marginal distributions P (Xs) and Q(Xt) of
the source and target domains (Ben-David et al., 2010). The
Maximum Mean Discrepancy (MMD) (Gretton et al., 2012),
as a kernel two-sample test statistic, has been widely ap-
plied to measure the discrepancy in marginal distributions
P (Xs) and Q(Xt) (Tzeng et al., 2014; Long et al., 2015;
2016). To date MMD has not been used to measure the
discrepancy in joint distributions P (Zs1, . . . , Zs|L|) and
Q(Zt1, . . . , Zt|L|), possibly because MMD has not been di-
rectly deﬁned for joint distributions by (Gretton et al., 2012)
while in conventional shallow domain adaptation methods
the joint distributions are not easy to manipulate and match.

Q(Zt1, . . . , Zt|L|). The resulting measure is called Joint
Maximum Mean Discrepancy (JMMD), which is deﬁned as

DL (P, Q) (cid:44) (cid:107)CZs,1:|L| (P ) − CZt,1:|L| (Q)(cid:107)2

(cid:96)=1H(cid:96) . (8)
⊗|L|

Based on the virtue of the kernel two-sample test theory
(Gretton et al., 2012), we will have P (Zs1, . . . , Zs|L|) =
Q(Zt1, . . . , Zt|L|) if and only if DL(P, Q) = 0. Given
source domain Ds of ns labeled points and target domain
Dt of nt unlabeled points drawn i.i.d. from P and Q respec-
tively, the deep networks will generate activations in layers
L as {(zs1
i=1 and {(zt1
j=1. The
empirical estimate of DL(P, Q) is computed as the squared
distance between the empirical kernel mean embeddings as

i , . . . , zs|L|

j , . . . , zt|L|

)}ns

)}nt

j

i

(cid:98)DL (P, Q) =

k(cid:96) (cid:0)zs(cid:96)

i , zs(cid:96)
j

(cid:1)

ns(cid:88)

ns(cid:88)

(cid:89)

i=1

j=1

(cid:96)∈L

nt(cid:88)

nt(cid:88)

(cid:89)

1
n2
s

1
n2
t

i=1

j=1

(cid:96)∈L

ns(cid:88)

nt(cid:88)

(cid:89)

i=1

j=1

(cid:96)∈L

2
nsnt

+

−

k(cid:96) (cid:0)zt(cid:96)

i , zt(cid:96)
j

(cid:1)

(9)

k(cid:96) (cid:0)zs(cid:96)

i , zt(cid:96)
j

(cid:1).

i , z(cid:96)

Remark: Taking a close look on the objectives of MMD (6)
and JMMD (9), we can ﬁnd some interesting connections.
The difference is that, for the activations Z(cid:96) in each layer (cid:96) ∈
L, instead of putting uniform weights on the kernel function
k(cid:96)(z(cid:96)
j) as in MMD, JMMD applies non-uniform weights,
reﬂecting the inﬂuence of other variables in other layers
L\(cid:96). This captures the full interactions between different
variables in the joint distributions P (Zs1, . . . , Zs|L|) and
Q(Zt1, . . . , Zt|L|), which is crucial for domain adaptation.
All previous deep transfer learning methods (Tzeng et al.,
2014; Long et al., 2015; Ganin & Lempitsky, 2015; Tzeng
et al., 2015; Long et al., 2016) have not addressed this issue.

4.2. Joint Adaptation Networks

Following the virtue of MMD (5), we use the Hilbert space
embeddings of joint distributions (3) to measure the dis-
crepancy of two joint distributions P (Zs1, . . . , Zs|L|) and

Denote by L the domain-speciﬁc layers where the activa-
tions are not safely transferable. We will formally reduce
the discrepancy in the joint distributions of the activations

XsXtZt|L|Zs|L|Zs1Zt1YsYtJMMD✖✖tiedtiedφ1φ1φLφLAlexNetVGGnetGoogLeNetResNet……XsXtZt|L|Zs|L|Zs1Zt1YsYtJMMD✖✖tiedtiedθθθθθθθθφ1φ1φLφLAlexNetVGGnetGoogLeNetResNet……Deep Transfer Learning with Joint Adaptation Networks

in layers L, i.e. P (Zs1, . . . , Zs|L|) and Q(Zt1, . . . , Zt|L|).
Note that the features in the lower layers of the network
are transferable and hence will not require a further distri-
bution matching. By integrating the JMMD (9) over the
domain-speciﬁc layers L into the CNN error (7), the joint
distributions are matched end-to-end with network training,

network as shown in Figure 1(b). We maximize JMMD with
respect to these new parameters θ to approach the virtue of
the original MMD (5), that is, maximizing the test power of
JMMD such that distributions of source and target domains
are made more distinguishable (Sriperumbudur et al., 2009).
This leads to a new adversarial joint adaptation network as

min
f

1
ns

ns(cid:88)

i=1

J (f (xs

i ) , ys

i ) + λ (cid:98)DL (P, Q) ,

(10)

J (f (xs

i ) , ys

i ) + λ (cid:98)DL (P, Q; θ) .

(12)

min
f

max
θ

1
ns

ns(cid:88)

i=1

where λ > 0 is a tradeoff parameter of the JMMD penalty.
As shown in Figure 1(a), we set L = {f c6, f c7, f c8} for
the JAN model based on AlexNet (last three layers) while
we set L = {pool5, f c} for the JAN model based on ResNet
(last two layers), as these layers are tailored to task-speciﬁc
structures, which are not safely transferable and should be
jointly adapted by minimizing CNN error and JMMD (9).

A limitation of JMMD (9) is its quadratic complexity, which
is inefﬁcient for scalable deep transfer learning. Motivated
by the unbiased estimate of MMD (Gretton et al., 2012), we
derive a similar linear-time estimate of JMMD as follows,

(cid:98)DL (P, Q) =

k(cid:96)(zs(cid:96)

2i−1, zs(cid:96)

2i) +

k(cid:96)(zt(cid:96)

2i−1, zt(cid:96)
2i)

2
n

2
n

(cid:32)

n/2
(cid:88)

(cid:89)

i=1

(cid:96)∈L

(cid:32)

n/2
(cid:88)

(cid:89)

i=1

(cid:96)∈L

(cid:89)

(cid:96)∈L

(cid:89)

(cid:96)∈L

−

k(cid:96)(zs(cid:96)

2i−1, zt(cid:96)

2i) +

k(cid:96)(zt(cid:96)

2i−1, zs(cid:96)
2i)

,

(11)
where n = ns. This linear-time estimate well ﬁts the mini-
batch stochastic gradient descent (SGD) algorithm. In each
mini-batch, we sample the same number of source points
and target points to eliminate the bias caused by domain size.
This enables our models to scale linearly to large samples.

4.3. Adversarial Training for Optimal MMD

The MMD deﬁned using the RKHS (6) has the advantage of
not requiring a separate network to approximately maximize
the original deﬁnition of MMD (5). But the original MMD
(5) reveals that, in order to maximize the test power such
that any two distributions can be distinguishable, we require
the class of functions f ∈ H to be rich enough. Although
(Gretton et al., 2012) shows that an universal RKHS is rich
enough, such kernel-based MMD may suffer from vanishing
gradients for low-bandwidth kernels. Moreover, it may be
possible that some widely-used kernels are unable to capture
very complex distances in high dimensional spaces such as
natural images (Reddi et al., 2015; Arjovsky et al., 2017).

To circumvent the issues of vanishing gradients and non-rich
function class of kernel-based MMD (6), we are enlightened
by the original MMD (5) which ﬁts the adversarial training
in GANs (Goodfellow et al., 2014). We add multiple fully-
connected layers parametrized by θ to the proposed JMMD
(9) to make the function class of JMMD richer using neural

Learning deep features by minimizing this more powerful
JMMD, intuitively any shift in the joint distributions will be
more easily identiﬁed by JMMD and then adapted by CNN.

Remark: This version of JAN shares the idea of domain-
adversarial training with (Ganin & Lempitsky, 2015), but
differs in that we use the JMMD as the domain adversary
while (Ganin & Lempitsky, 2015) uses logistic regression.
As pointed out in a very recent study (Arjovsky et al., 2017),
our JMMD-adversarial network can be trained more easily.

5. Experiments

(cid:33)

(cid:33)

We evaluate the joint adaptation networks with state of the
art transfer learning and deep learning methods. Codes and
datasets are available at http://github.com/thuml.

5.1. Setup

Ofﬁce-31 (Saenko et al., 2010) is a standard benchmark for
domain adaptation in computer vision, comprising 4,652
images and 31 categories collected from three distinct do-
mains: Amazon (A), which contains images downloaded
from amazon.com, Webcam (W) and DSLR (D), which
contain images respectively taken by web camera and dig-
ital SLR camera under different settings. We evaluate all
methods across three transfer tasks A → W, D → W and W
→ D, which are widely adopted by previous deep transfer
learning methods (Tzeng et al., 2014; Ganin & Lempitsky,
2015), and another three transfer tasks A → D, D → A and
W → A as in (Long et al., 2015; 2016; Tzeng et al., 2015).

ImageCLEF-DA1 is a benchmark dataset for ImageCLEF
2014 domain adaptation challenge, which is organized by
selecting the 12 common categories shared by the follow-
ing three public datasets, each is considered as a domain:
Caltech-256 (C), ImageNet ILSVRC 2012 (I), and Pascal
VOC 2012 (P). There are 50 images in each category and
600 images in each domain. We use all domain combina-
tions and build 6 transfer tasks: I → P, P → I, I → C, C
→ I, C → P, and P → C. Different from Ofﬁce-31 where
different domains are of different sizes, the three domains
in ImageCLEF-DA are of equal size, which makes it a good
complement to Ofﬁce-31 for more controlled experiments.

1http://imageclef.org/2014/adaptation

Deep Transfer Learning with Joint Adaptation Networks

We compare with conventional and state of the art transfer
learning and deep learning methods: Transfer Component
Analysis (TCA) (Pan et al., 2011), Geodesic Flow Kernel
(GFK) (Gong et al., 2012), Convolutional Neural Networks
AlexNet (Krizhevsky et al., 2012) and ResNet (He et al.,
2016), Deep Domain Confusion (DDC) (Tzeng et al., 2014),
Deep Adaptation Network (DAN) (Long et al., 2015), Re-
verse Gradient (RevGrad) (Ganin & Lempitsky, 2015), and
Residual Transfer Network (RTN) (Long et al., 2016). TCA
is a transfer learning method based on MMD-regularized
Kernel PCA. GFK is a manifold learning method that inter-
polates across an inﬁnite number of intermediate subspaces
to bridge domains. DDC is the ﬁrst method that maximizes
domain invariance by regularizing the adaptation layer of
AlexNet using linear-kernel MMD (Gretton et al., 2012).
DAN learns transferable features by embedding deep fea-
tures of multiple task-speciﬁc layers to reproducing kernel
Hilbert spaces (RKHSs) and matching different distributions
optimally using multi-kernel MMD. RevGrad improves do-
main adaptation by making the source and target domains
indistinguishable for a domain discriminator by adversarial
training. RTN jointly learns transferable features and adap-
tive classiﬁers by deep residual learning (He et al., 2016).

We examine the inﬂuence of deep representations for do-
main adaptation by employing the breakthrough AlexNet
(Krizhevsky et al., 2012) and the state of the art ResNet (He
et al., 2016) for learning transferable deep representations.
For AlexNet, we follow DeCAF (Donahue et al., 2014) and
use the activations of layer f c7 as image representation. For
ResNet (50 layers), we use the activations of the last feature
layer pool5 as image representation. We follow standard
evaluation protocols for unsupervised domain adaptation
(Long et al., 2015; Ganin & Lempitsky, 2015). For both
Ofﬁce-31 and ImageCLEF-DA datasets, we use all labeled
source examples and all unlabeled target examples. We
compare the average classiﬁcation accuracy of each method
on three random experiments, and report the standard error
of the classiﬁcation accuracies by different experiments of
the same transfer task. We perform model selection by tun-
ing hyper-parameters using transfer cross-validation (Zhong
et al., 2010). For MMD-based methods and JAN, we adopt
Gaussian kernel with bandwidth set to median pairwise
squared distances on the training data (Gretton et al., 2012).

We implement all deep methods based on the Caffe frame-
work, and ﬁne-tune from Caffe-provided models of AlexNet
(Krizhevsky et al., 2012) and ResNet (He et al., 2016), both
are pre-trained on the ImageNet 2012 dataset. We ﬁne-tune
all convolutional and pooling layers and train the classiﬁer
layer via back propagation. Since the classiﬁer is trained
from scratch, we set its learning rate to be 10 times that
of the other layers. We use mini-batch stochastic gradient
descent (SGD) with momentum of 0.9 and the learning rate
annealing strategy in RevGrad (Ganin & Lempitsky, 2015):

the learning rate is not selected by a grid search due to high
computational cost—it is adjusted during SGD using the
η0
following formula: ηp =
(1+αp)β , where p is the training
progress linearly changing from 0 to 1, η0 = 0.01, α = 10
and β = 0.75, which is optimized to promote convergence
and low error on the source domain. To suppress noisy acti-
vations at the early stages of training, instead of ﬁxing the
adaptation factor λ, we gradually change it from 0 to 1 by a
progressive schedule: λp =
1+exp(−γp) − 1, and γ = 10 is
ﬁxed throughout experiments (Ganin & Lempitsky, 2015).
This progressive strategy signiﬁcantly stabilizes parameter
sensitivity and eases model selection for JAN and JAN-A.

2

5.2. Results

The classiﬁcation accuracy results on the Ofﬁce-31 dataset
for unsupervised domain adaptation based on AlexNet and
ResNet are shown in Table 1. As fair comparison with identi-
cal evaluation setting, the results of DAN (Long et al., 2015),
RevGrad (Ganin & Lempitsky, 2015), and RTN (Long et al.,
2016) are directly reported from their published papers. The
proposed JAN models outperform all comparison methods
on most transfer tasks. It is noteworthy that JANs promote
the classiﬁcation accuracies substantially on hard transfer
tasks, e.g. D → A and W → A, where the source and target
domains are substantially different and the source domain
is smaller than the target domain, and produce comparable
classiﬁcation accuracies on easy transfer tasks, D → W and
W → D, where the source and target domains are similar
(Saenko et al., 2010). The encouraging results highlight the
key importance of joint distribution adaptation in deep neu-
ral networks, and suggest that JANs are able to learn more
transferable representations for effective domain adaptation.

The results reveal several interesting observations. (1) Stan-
dard deep learning methods either outperform (AlexNet) or
underperform (ResNet) traditional shallow transfer learning
methods (TCA and GFK) using deep features (AlexNet-fc7
and ResNet-pool5) as input. And traditional shallow trans-
fer learning methods perform better with more transferable
deep features extracted by ResNet. This conﬁrms the current
practice that deep networks learn abstract feature represen-
tations, which can only reduce, but not remove, the domain
discrepancy (Yosinski et al., 2014). (2) Deep transfer learn-
ing methods substantially outperform both standard deep
learning methods and traditional shallow transfer learning
methods. This validates that reducing the domain discrep-
ancy by embedding domain-adaptation modules into deep
networks (DDC, DAN, RevGrad, and RTN) can learn more
transferable features. (3) The JAN models outperform pre-
vious methods by large margins and set new state of the art
record. Different from all previous deep transfer learning
methods that only adapt the marginal distributions based
on independent feature layers (one layer for RevGrad and
multilayer for DAN and RTN), JAN adapts the joint distribu-

Deep Transfer Learning with Joint Adaptation Networks

Table 1. Classiﬁcation accuracy (%) on Ofﬁce-31 dataset for unsupervised domain adaptation (AlexNet and ResNet)

Method
AlexNet (Krizhevsky et al., 2012)
TCA (Pan et al., 2011)
GFK (Gong et al., 2012)
DDC (Tzeng et al., 2014)
DAN (Long et al., 2015)
RTN (Long et al., 2016)
RevGrad (Ganin & Lempitsky, 2015)
JAN (ours)
JAN-A (ours)
ResNet (He et al., 2016)
TCA (Pan et al., 2011)
GFK (Gong et al., 2012)
DDC (Tzeng et al., 2014)
DAN (Long et al., 2015)
RTN (Long et al., 2016)
RevGrad (Ganin & Lempitsky, 2015)
JAN (ours)
JAN-A (ours)

A → W
61.6±0.5
61.0±0.0
60.4±0.0
61.8±0.4
68.5±0.5
73.3±0.3
73.0±0.5
74.9±0.3
75.2±0.4
68.4±0.2
72.7±0.0
72.8±0.0
75.6±0.2
80.5±0.4
84.5±0.2
82.0±0.4
85.4±0.3
86.0±0.4

D → W
95.4±0.3
93.2±0.0
95.6±0.0
95.0±0.5
96.0±0.3
96.8±0.2
96.4±0.3
96.6±0.2
96.6±0.2
96.7±0.1
96.7±0.0
95.0±0.0
96.0±0.2
97.1±0.2
96.8±0.1
96.9±0.2
97.4±0.2
96.7±0.3

W → D
99.0±0.2
95.2±0.0
95.0±0.0
98.5±0.4
99.0±0.3
99.6±0.1
99.2±0.3
99.5±0.2
99.6±0.1
99.3±0.1
99.6±0.0
98.2±0.0
98.2±0.1
99.6±0.1
99.4±0.1
99.1±0.1
99.8±0.2
99.7±0.1

A → D
63.8±0.5
60.8±0.0
60.6±0.0
64.4±0.3
67.0±0.4
71.0±0.2
72.3±0.3
71.8±0.2
72.8±0.3
68.9±0.2
74.1±0.0
74.5±0.0
76.5±0.3
78.6±0.2
77.5±0.3
79.7±0.4
84.7±0.3
85.1±0.4

D → A
51.1±0.6
51.6±0.0
52.4±0.0
52.1±0.6
54.0±0.5
50.5±0.3
53.4±0.4
58.3±0.3
57.5±0.2
62.5±0.3
61.7±0.0
63.4±0.0
62.2±0.4
63.6±0.3
66.2±0.2
68.2±0.4
68.6±0.3
69.2±0.4

W → A
49.8±0.4
50.9±0.0
48.1±0.0
52.2±0.4
53.1±0.5
51.0±0.1
51.2±0.5
55.0±0.4
56.3±0.2
60.7±0.3
60.9±0.0
61.0±0.0
61.5±0.5
62.8±0.2
64.8±0.3
67.4±0.5
70.0±0.4
70.7±0.5

Table 2. Classiﬁcation accuracy (%) on ImageCLEF-DA for unsupervised domain adaptation (AlexNet and ResNet)

Method
AlexNet (Krizhevsky et al., 2012)
DAN (Long et al., 2015)
RTN (Long et al., 2016)
JAN (ours)
ResNet (He et al., 2016)
DAN (Long et al., 2015)
RTN (Long et al., 2016)
JAN (ours)

I → P
66.2±0.2
67.3±0.2
67.4±0.3
67.2±0.5
74.8±0.3
74.5±0.4
74.6±0.3
76.8±0.4

P → I
70.0±0.2
80.5±0.3
81.3±0.3
82.8±0.4
83.9±0.1
82.2±0.2
85.8±0.1
88.0±0.2

I → C
84.3±0.2
87.7±0.3
89.5±0.4
91.3±0.5
91.5±0.3
92.8±0.2
94.3±0.1
94.7±0.2

C → I
71.3±0.4
76.0±0.3
78.0±0.2
80.0±0.5
78.0±0.2
86.3±0.4
85.9±0.3
89.5±0.3

C → P
59.3±0.5
61.6±0.3
62.0±0.2
63.5±0.4
65.5±0.3
69.2±0.4
71.7±0.3
74.2±0.3

P → C
84.5±0.3
88.4±0.2
89.1±0.1
91.0±0.4
91.2±0.3
89.8±0.4
91.2±0.4
91.7±0.3

Avg
70.1
68.8
68.7
70.6
72.9
73.7
74.3
76.0
76.3
76.1
77.6
77.5
78.3
80.4
81.6
82.2
84.3
84.6

Avg
73.9
76.9
77.9
79.3
80.7
82.5
83.9
85.8

tions of network activations in all domain-speciﬁc layers to
fully correct the shifts in joint distributions across domains.
Although both JAN and DAN (Long et al., 2015) adapt mul-
tiple domain-speciﬁc layers, the improvement from DAN to
JAN is crucial for the domain adaptation performance: JAN
uses a JMMD penalty to reduce the shift in the joint distribu-
tions of multiple task-speciﬁc layers, which reﬂects the shift
in the joint distributions of input features and output labels;
DAN needs multiple MMD penalties, each independently
reducing the shift in the marginal distribution of each layer,
assuming feature layers and classiﬁer layer are independent.

By going from AlexNet to extremely deep ResNet, we can
attain a more in-depth understanding of feature transferabil-
ity. (1) ResNet-based methods outperform AlexNet-based
methods by large margins. This validates that very deep
convolutional networks, e.g. VGGnet (Simonyan & Zisser-
man, 2015), GoogLeNet (Szegedy et al., 2015), and ResNet,
not only learn better representations for general vision tasks
but also learn more transferable representations for domain
adaptation. (2) The JAN models signiﬁcantly outperform
ResNet-based methods, revealing that even very deep net-
works can only reduce, but not remove, the domain discrep-
ancy. (3) The boost of JAN over ResNet is more signiﬁcant
than the improvement of JAN over AlexNet. This implies

that JAN can beneﬁt from more transferable representations.

The great aspect of JAN is that via the kernel trick there is
no need to train a separate network to maximize the MMD
criterion (5) for the ball of a RKHS. However, this has the
disadvantage that some kernels used in practice are unsuit-
able for capturing very complex distances in high dimen-
sional spaces such as natural images (Arjovsky et al., 2017).
The JAN-A model signiﬁcantly outperforms the previous do-
main adversarial deep network (Ganin & Lempitsky, 2015).
The improvement from JAN to JAN-A also demonstrates the
beneﬁt of adversarial training for optimizing the JMMD in
a richer function class. By maximizing the JMMD criterion
with respect to a separate network, JAN-A can maximize the
distinguishability of source and target distributions. Adapt-
ing domains against deep features where their distributions
maximally differ, we can enhance the feature transferability.

The three domains in ImageCLEF-DA are more balanced
than those of Ofﬁce-31. With these more balanced transfer
tasks, we are expecting to testify whether transfer learning
improves when domain sizes do not change. The classiﬁca-
tion accuracy results based on both AlexNet and ResNet are
shown in Table 2. The JAN models outperform comparison
methods on most transfer tasks, but by less improvements.
This means the difference in domain sizes may cause shift.

Deep Transfer Learning with Joint Adaptation Networks

(a) DAN: Source=A

(b) DAN: Target=W

(c) JAN: Source=A

(d) JAN: Target=W

Figure 2. The t-SNE visualization of network activations (ResNet) generated by DAN (a)(b) and JAN (c)(d), respectively.

(a) A-distance

(b) JMMD

(c) Accuracy w.r.t. λ

(d) Convergence

Figure 3. Analysis: (a) A-distance; (b) JMMD; (c) parameter sensitivity of λ; (d) convergence (dashed lines show best baseline results).

5.3. Analysis

Feature Visualization: We visualize in Figures 2(a)–2(d)
the network activations of task A → W learned by DAN and
JAN respectively using t-SNE embeddings (Donahue et al.,
2014). Compared with the activations given by DAN in Fig-
ure 2(a)–2(b), the activations given by JAN in Figures 2(c)–
2(d) show that the target categories are discriminated much
more clearly by the JAN source classiﬁer. This suggests that
the adaptation of joint distributions of multilayer activations
is a powerful approach to unsupervised domain adaptation.

Distribution Discrepancy: The theory of domain adapta-
tion (Ben-David et al., 2010; Mansour et al., 2009) suggests
A-distance as a measure of distribution discrepancy, which,
together with the source risk, will bound the target risk. The
proxy A-distance is deﬁned as dA = 2 (1 − 2(cid:15)), where (cid:15)
is the generalization error of a classiﬁer (e.g. kernel SVM)
trained on the binary problem of discriminating the source
and target. Figure 3(a) shows dA on tasks A → W, W → D
with features of CNN, DAN, and JAN. We observe that dA
using JAN features is much smaller than dA using CNN and
DAN features, which suggests that JAN features can close
the cross-domain gap more effectively. As domains W and
D are very similar, dA of task W → D is much smaller than
that of A → W, which explains better accuracy of W → D.

A limitation of the A-distance is that it cannot measure the
cross-domain discrepancy of joint distributions, which is
addressed by the proposed JMMD (9). We compute JMMD
(9) across domains using CNN, DAN and JAN activations
respectively, based on the features in f c7 and ground-truth
labels in f c8 (the target labels are not used for model train-
ing). Figure 3(b) shows that JMMD using JAN activations is
much smaller than JMMD using CNN and DAN activations,

which validates that JANs successfully reduce the shifts in
joint distributions to learn more transferable representations.

Parameter Sensitivity: We check the sensitivity of JMMD
parameter λ, i.e. the maximum value of the relative weight
for JMMD. Figure 3(c) demonstrates the transfer accuracy
of JAN based on AlexNet and ResNet respectively, by vary-
ing λ ∈ {0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1} on task A → W.
The accuracy of JAN ﬁrst increases and then decreases as
λ varies and shows a bell-shaped curve. This conﬁrms the
motivation of deep learning and joint distribution adaptation,
as a proper trade-off between them enhance transferability.

Convergence Performance: As JAN and JAN-A involve
adversarial training procedures, we testify their convergence
performance. Figure 3(d) demonstrates the test errors of
different methods on task A → W, which suggests that JAN
converges fastest due to nonparametric JMMD while JAN-A
has similar convergence speed as RevGrad with signiﬁcantly
improved accuracy in the whole procedure of convergence.

6. Conclusion

This paper presented a novel approach to deep transfer learn-
ing, which enables end-to-end learning of transferable repre-
sentations. Unlike previous methods that match the marginal
distributions of features across domains, the proposed ap-
proach reduces the shift in joint distributions of the network
activations of multiple task-speciﬁc layers, which approxi-
mates the shift in the joint distributions of input features and
output labels. The discrepancy between joint distributions
can be computed by embedding the joint distributions in a
tensor-product Hilbert space, which can be scaled linearly
to large samples and be implemented in most deep networks.
Experiments testiﬁed the efﬁcacy of the proposed approach.

A->WW->DTransfer Task1  1.21.41.61.82  2.2A-DistanceCNN (AlexNet)DAN (AlexNet)JAN (AlexNet)A->WW->DTransfer Task00.020.040.06JMMDCNN (AlexNet)DAN (AlexNet)JAN (AlexNet)0.010.020.050.1 0.2 0.5 1   60708090Accuracy (%)AW (AlexNet)AW (ResNet)0.10.51  1.52  Number of Iterations (104)0.10.20.30.4Test ErrorRevGrad (ResNet)JAN (ResNet)JAN-A (ResNet)Deep Transfer Learning with Joint Adaptation Networks

Acknowledgments

We thank Zhangjie Cao for conducting part of experiments.
This work was supported by NSFC (61502265, 61325008),
National Key R&D Program of China (2016YFB1000701,
2015BAF32B01), and Tsinghua TNList Lab Key Projects.

References

Arjovsky, Martin, Chintala, Soumith, and Bottou, Léon.
Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.

Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A.,
Pereira, F., and Vaughan, J. W. A theory of learning from
different domains. Machine Learning, 79(1-2):151–175,
2010.

Bengio, Y., Courville, A., and Vincent, P. Representation
learning: A review and new perspectives. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence
(TPAMI), 35(8):1798–1828, 2013.

Bousmalis, Konstantinos, Trigeorgis, George, Silberman,
Nathan, Krishnan, Dilip, and Erhan, Dumitru. Domain
separation networks. In Advances in Neural Information
Processing Systems (NIPS), pp. 343–351, 2016.

Collobert, R., Weston, J., Bottou, L., Karlen, M.,
Kavukcuoglu, K., and Kuksa, P. Natural language pro-
cessing (almost) from scratch. Journal of Machine Learn-
ing Research (JMLR), 12:2493–2537, 2011.

Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N.,
Tzeng, E., and Darrell, T. Decaf: A deep convolutional
activation feature for generic visual recognition. In In-
ternational Conference on Machine Learning (ICML),
2014.

Duan, L., Tsang, I. W., and Xu, D. Domain transfer multiple
kernel learning. IEEE Transactions on Pattern Analysis
and Machine Intelligence (TPAMI), 34(3):465–479, 2012.

Ganin, Y. and Lempitsky, V. Unsupervised domain adapta-
tion by backpropagation. In International Conference on
Machine Learning (ICML), 2015.

Glorot, X., Bordes, A., and Bengio, Y. Domain adaptation
for large-scale sentiment classiﬁcation: A deep learn-
ing approach. In International Conference on Machine
Learning (ICML), 2011.

International Conference on Machine Learning (ICML),
2013.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y. Generative adversarial nets. In Advances in Neural
Information Processing Systems (NIPS), 2014.

Gopalan, R., Li, R., and Chellappa, R. Domain adapta-
tion for object recognition: An unsupervised approach.
In IEEE International Conference on Computer Vision
(ICCV), 2011.

Gretton, A., Borgwardt, K., Rasch, M., Schölkopf, B., and
Smola, A. A kernel two-sample test. Journal of Machine
Learning Research (JMLR), 13:723–773, 2012.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
learning for image recognition. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2016.

Hoffman, J., Guadarrama, S., Tzeng, E., Hu, R., Donahue,
J., Girshick, R., Darrell, T., and Saenko, K. LSDA: Large
scale detection through adaptation. In Advances in Neural
Information Processing Systems (NIPS), 2014.

Huang, J., Smola, A. J., Gretton, A., Borgwardt, K. M., and
Schölkopf, B. Correcting sample selection bias by unla-
beled data. In Advances in Neural Information Processing
Systems (NIPS), 2006.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
In Advances in Neural Information Processing Systems
(NIPS), 2012.

Long, Mingsheng, Cao, Yue, Wang, Jianmin, and Jordan,
Michael I. Learning transferable features with deep adap-
tation networks. In International Conference on Machine
Learning (ICML), 2015.

Long, Mingsheng, Zhu, Han, Wang, Jianmin, and Jordan,
Michael I. Unsupervised domain adaptation with residual
transfer networks. In Advances in Neural Information
Processing Systems (NIPS), pp. 136–144, 2016.

Mansour, Y., Mohri, M., and Rostamizadeh, A. Domain
adaptation: Learning bounds and algorithms. In Confer-
ence on Computational Learning Theory (COLT), 2009.

Gong, B., Shi, Y., Sha, F., and Grauman, K. Geodesic
ﬂow kernel for unsupervised domain adaptation. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2012.

Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning
and transferring mid-level image representations using
convolutional neural networks. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2013.

Gong, B., Grauman, K., and Sha, F. Connecting the
dots with landmarks: Discriminatively learning domain-
invariant features for unsupervised domain adaptation. In

Pan, S. J. and Yang, Q. A survey on transfer learning.
IEEE Transactions on Knowledge and Data Engineering
(TKDE), 22(10):1345–1359, 2010.

Deep Transfer Learning with Joint Adaptation Networks

Hilbert space embeddings and metrics on probability mea-
sures. Journal of Machine Learning Research (JMLR),
11(Apr):1517–1561, 2010.

Sugiyama, M., Nakajima, S., Kashima, H., Buenau, P. V.,
and Kawanabe, M. Direct importance estimation with
model selection and its application to covariate shift adap-
tation. In Advances in Neural Information Processing
Systems (NIPS), 2008.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,
A. Going deeper with convolutions. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR),
2015.

Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., and Darrell,
T. Deep domain confusion: Maximizing for domain
invariance. CoRR, abs/1412.3474, 2014.

Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., and Darrell,
T. Simultaneous deep transfer across domains and tasks.
In IEEE International Conference on Computer Vision
(ICCV), 2015.

Tzeng, Eric, Hoffman, Judy, Saenko, Kate, and Darrell,
Trevor. Adversarial discriminative domain adaptation.
arXiv preprint arXiv:1702.05464, 2017.

Wang, X. and Schneider, J. Flexible transfer learning under
support and model shift. In Advances in Neural Informa-
tion Processing Systems (NIPS), 2014.

Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How
transferable are features in deep neural networks? In Ad-
vances in Neural Information Processing Systems (NIPS),
2014.

Zhang, K., Schölkopf, B., Muandet, K., and Wang, Z. Do-
main adaptation under target and conditional shift. In
International Conference on Machine Learning (ICML),
2013.

Zhong, E., Fan, W., Yang, Q., Verscheure, O., and Ren,
J. Cross validation framework to choose amongst mod-
els and datasets for transfer learning. In Joint European
Conference on Machine Learning and Knowledge Discov-
ery in Databases (ECML/PKDD), pp. 547–562. Springer,
2010.

Pan, S. J., Tsang, I. W., Kwok, J. T., and Yang, Q. Do-
main adaptation via transfer component analysis. IEEE
Transactions on Neural Networks (TNN), 22(2):199–210,
2011.

Quionero-Candela, J., Sugiyama, M., Schwaighofer, A., and
Lawrence, N. D. Dataset shift in machine learning. The
MIT Press, 2009.

Reddi, Sashank J, Ramdas, Aaditya, Póczos, Barnabás,
Singh, Aarti, and Wasserman, Larry A. On the high
dimensional power of a linear-time two sample test un-
der mean-shift alternatives. In Artiﬁcial Intelligence and
Statistics Conference (AISTATS), 2015.

Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting
visual category models to new domains. In European
Conference on Computer Vision (ECCV), 2010.

Simonyan, K. and Zisserman, A. Very deep convolutional
networks for large-scale image recognition. In Interna-
tional Conference on Learning Representations (ICLR),
2015 (arXiv:1409.1556v6), 2015.

Smola, Alex, Gretton, Arthur, Song, Le, and Schölkopf,
Bernhard. A hilbert space embedding for distributions.
In International Conference on Algorithmic Learning
Theory (ALT), pp. 13–31. Springer, 2007.

Song, L., Huang, J., Smola, A., and Fukumizu, K. Hilbert
space embeddings of conditional distributions with appli-
cations to dynamical systems. In International Confer-
ence on Machine Learning (ICML), 2009.

Song, Le and Dai, Bo. Robust low rank kernel embeddings
of multivariate distributions. In Advances in Neural In-
formation Processing Systems (NIPS), pp. 3228–3236,
2013.

Song, Le, Boots, Byron, Siddiqi, Sajid M, Gordon, Geof-
frey J, and Smola, Alex. Hilbert space embeddings of
hidden markov models. In International Conference on
Machine Learning (ICML), 2010.

Song, Le, Fukumizu, Kenji, and Gretton, Arthur. Kernel
embeddings of conditional distributions: A uniﬁed kernel
framework for nonparametric inference in graphical mod-
els. IEEE Signal Processing Magazine, 30(4):98–111,
2013.

Sriperumbudur, B. K., Fukumizu, K., Gretton, A., Lanckriet,
G., and Schölkopf, B. Kernel choice and classiﬁability for
rkhs embeddings of probability distributions. In Advances
in Neural Information Processing Systems (NIPS), 2009.

Sriperumbudur, Bharath K, Gretton, Arthur, Fukumizu,
Kenji, Schölkopf, Bernhard, and Lanckriet, Gert RG.

