Supplementary Material

Proof of Theorem

The ideal

We introduce the derivation of theorem of the main pa-
joint hypothesis is deﬁned as h(cid:3) =
per.
)
(
RS (h(cid:3)) + RT (h(cid:3))
arg min
, and its corresponding error
h2H
is C = RS (h(cid:3)) + RT (h(cid:3)), where R denotes the expected
error on each hypothesis.

We consider the pseudo-labeled target samples set Tl =
}mt
{
(xi; ^yi)
i=1 given false labels at the ratio of (cid:26). The dis-
tribution of the source samples is denoted as S; that of the
target samples, as T ; and that of the pseudo-labeled tar-
get samples, as Tl. The minimum shared error on S; Tl is
denoted as C ′. Then, the following inequality holds:
2 dH∆H(SX; TX) + C
8h 2 H; RT (h) (cid:20) RS (h) + 1

(cid:20) RS(h) + 1

2 dH∆H(SX; TX) + C ′ + (cid:26)

Proof. The probabiliy of false labels in the pseudo-labeled
set Tl is (cid:26). When we consider 0-1 loss function for l, the
difference between the error based on the true labeled set
and pseudo-labeled set is

jl(h(xi); yi) (cid:0) l(h(xi); ^yi)j =

{

1
0

yi ̸= ^yi
yi = ^yi

Then, the difference in the expected error is,

E[jl(h(xi); yi) (cid:0) l(h(xi); ^yi)j] (cid:20) jRTl (h) (cid:0) RT (h)j (cid:20) (cid:26)

From the characteritic of the loss function, the triangle in-
equality will hold, then

RS (h) + RT (h) = RS (h) + RT (h) (cid:0) RTl(h) + RTl (h)
(cid:20) RS (h) + RTl(h) + jRTl (h) (cid:0) RT (h)j
(cid:20) RS (h) + RTl(h) + (cid:26)

From this result, the main inequality holds.

CNN Architectures and training detail

Four types of architectures are used for our method, which
is based on (Ganin & Lempitsky, 2014). The network
topology is shown in Figs 2, 3 and 4. The other hyper-
parameters are decided on the validation splits. In the all

Figure1. The behavior of our model when increasing the number
of steps up to 100. Our model achieves accuracy of about 97%.

scenarios, the learning rate is set to 0.01. In the initial train-
ing step, The batchsize is set as 128. After the initial step,
the batchsize for training Ft; F is set as 128, the batchsize
for training F1; F2; F is set as 64 in all scenarios.
In MNIST!MNIST-M, the dropout rate used in the ex-
periment is 0.2 for training Ft, 0.5 for training F1; F2.
The number of iteraions per one step is set 2000.
In
MNIST!SVHN, we did not use dropout. We decreased
learning rate to 0.001 after step 10. The number of iteraions
per one step is set 3000. In SVHN!MNIST, the dropout
rate used in the experiment is 0.5. The number of it-
eraions per one step is set 3000. In SYNDIGITS!SVHN,
the dropout rate used in the experiment is 0.5. The
number of
In
SYNSIGNS!GTSRB, the dropout rate used in the exper-
iment is 0.5. The number of iteraions per one step is set
5000.

iteraions per one step is set 5000.

Semi-supervised domain adaptation
experiments

In semi-supervised domain adaptation in MNIST!SVHN,
we used the same architecture we used in the unsupervised
setting. For the ﬁrst step of training, we trained all net-
works solely on source samples. We add randomly selected
labeled target samples into pseudo-labeled target training
sets. Other hyperparameters are the same as the ones used
in unsupervised settings.

020406080100Number of steps0.50.60.70.80.91Accuracy00.511.522.533.544.5Number of samples!104Accuracy of labeling methodAccuracy of learned networkNumber of labeled samplesSubmission and Formatting Instructions for ICML 2017

Supplementary experiments on
MNIST!MNIST-M

We observe the behavior of our model when increasing the
number of steps up to one hundred. We show the result in
Fig. 1. Our model’s accuracy gets about 97%. In our main
experiments, we set the number of steps thirty, but from this
experiment, further improvements can be expected when
the number of steps is increased.

References

Ganin, Yaroslav and Lempitsky, Victor. Unsupervised do-
main adaptation by backpropagation. In ICML, 2014.

Submission and Formatting Instructions for ICML 2017

Figure2. The architecture used for MNIST!MNIST-M. We added BN layer in the last convolution layer and FC layers in F1; F2. We
also used dropout in our experiment.

Figure3. The architecture used for training SVHN. In MNIST!SVHN, we added a BN layer in the last FC layer in F .
In
SVHN!MNIST, SYN Digits$SVHN, we added BN layer in the last convolution layer in F and FC layers in F1,F2 and also used
dropout.

Figure4. The architecture used in the adaptation Synthetic Signs!GTSRB. We added a BN layer after the last convolution layer in F
and also used dropout.

conv 5x5x32 ReLU!max-pool 2x2  2x2 stride conv 5x5x48 ReLU!FC 100 units ReLU!FC 10 units Softmax!F2: Labeling Network2!Ft : Target-specific network!F1: Labeling Network1!F: Shared Network!FC 100 units ReLU!FC 10 units Softmax!FC 100 units ReLU!FC 10 units Softmax!FC 100 units ReLU!FC 100 units ReLU!FC 100 units ReLU!max-pool 2x2  2x2 stride FC 2048 units ReLU!FC 10 units Softmax!F2: Labeling Network2!Ft : Target-specific network!F1: Labeling Network1!F: Shared Network!FC 2048 units ReLU!FC 10 units Softmax!FC 2048 units ReLU!FC 10 units Softmax!conv 5x5x64 ReLU!conv 5x5x64 ReLU!max-pool 3x3  2x2 stride conv 5x5x128 ReLU!FC 3072 units ReLU!max-pool 3x3  2x2 stride conv 5x5x96 ReLU!Conv 3x3x144 ReLU!FC 43 units Softmax!FC 512 units ReLU!FC 43 units Softmax!FC 43 units Softmax!F2: Labeling Network1!Ft : Target-specific network!F1: Labeling Network1!F: Shared Network!conv 5x5x256 ReLU!FC 512 units ReLU!FC 512 units ReLU!max-pool 2x2  2x2 stride max-pool 2x2  2x2 stride max-pool 2x2  2x2 stride 