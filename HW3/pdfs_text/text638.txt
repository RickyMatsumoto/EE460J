ExploitingStrongConvexityfromDatawithPrimal-DualFirst-OrderAlgorithmsJialeiWang1LinXiao2AbstractWeconsiderempiricalriskminimizationoflin-earpredictorswithconvexlossfunctions.Suchproblemscanbereformulatedasconvex-concavesaddlepointproblemsandsolvedbyprimal-dualﬁrst-orderalgorithms.However,primal-dualal-gorithmsoftenrequireexplicitstronglyconvexregularizationinordertoobtainfastlinearcon-vergence,andtherequireddualproximalmap-pingmaynotadmitclosed-formorefﬁcientso-lution.Inthispaper,wedevelopbothbatchandrandomizedprimal-dualalgorithmsthatcanex-ploitstrongconvexityfromdataadaptivelyandarecapableofachievinglinearconvergenceevenwithoutregularization.Wealsopresentdual-freevariantsofadaptiveprimal-dualalgorithmsthatdonotneedthedualproximalmapping,whichareespeciallysuitableforlogisticregression.1.IntroductionWeconsidertheproblemofregularizedempiricalriskmin-imization(ERM)oflinearpredictors.Leta1,...,an∈Rdbethefeaturevectorsofndatasamples,φi:R→RbeaconvexlossfunctionassociatedwiththelinearpredictionaTix,fori=1,...,n,andg:Rd→Rbeaconvexregu-larizationfunctionforthepredictorx∈Rd.ERMamountstosolvingthefollowingconvexoptimizationproblem:minx∈RdnP(x)def=1nPni=1φi(aTix)+g(x)o.(1)Thisformulationcoversmanywell-knownclassiﬁcationandregressionproblems.Forexample,logisticregres-sionisobtainedbysettingφi(z)=log(1+exp(−biz))wherebi∈{±1}.Forlinearregressionproblems,theloss1DepartmentofComputerScience,TheUniversityofChicago,Chicago,Illinois60637,USA.2MicrosoftResearch,Redmond,Washington98052,USA.Correspondenceto:JialeiWang<jialei@uchicago.edu>,LinXiao<lin.xiao@microsoft.com>.Proceedingsofthe34thInternationalConferenceonMachineLearning,Sydney,Australia,PMLR70,2017.Copyright2017bytheauthor(s).functionisφi(z)=(1/2)(z−bi)2,andwegetridgere-gressionwithg(x)=(λ/2)kxk22andtheelasticnetwithg(x)=λ1kxk1+(λ2/2)kxk22.LetA=[a1,...,an]Tbethenbyddatamatrix.Through-outthispaper,wemakethefollowingassumptions:Assumption1.Thefunctionsφi,gandmatrixAsatisfy:•Eachφiisδ-stronglyconvexand1/γ-smoothwhereγ>0andδ≥0,andγδ≤1;•gisλ-stronglyconvexwhereλ≥0;•λ+δµ2>0,whereµ=pλmin(ATA).ThestrongconvexityandsmoothnessmentionedabovearewithrespecttothestandardEuclideannorm,denotedaskxk=√xTx.(See,e.g.,Nesterov(2004,Sections2.1.1and2.1.3)fortheexactdeﬁnitions.)Weallowδ=0,whichsimplymeansφiisconvex.LetR=maxi{kaik}andassumingλ>0,thenR2/(γλ)isapopulardeﬁnitionofconditionnumberforanalyzingcomplexitiesofdifferentalgorithms.ThelastconditionabovemeansthattheprimalobjectivefunctionP(x)isstronglyconvex,evenifλ=0.Therehavebeenextensiveresearchactivitiesinrecentyearsondevelopingefﬁcientlyalgorithmsforsolvingprob-lem(1).Abroadclassofrandomizedalgorithmsthatex-ploittheﬁnitesumstructureintheERMproblemhaveemergedasverycompetitivebothintermsoftheoreticalcomplexityandpracticalperformance.Theycanbeputintothreecategories:primal,dual,andprimal-dual.PrimalrandomizedalgorithmsworkwiththeERMprob-lem(1)directly.Theyaremodernversionsofran-domizedincrementalgradientmethods(e.g.,Bertsekas,2012;Nedic&Bertsekas,2001)equippedwithvariancereductiontechniques.Eachiterationofsuchalgo-rithmsonlyprocessonedatapointaiwithcomplexityO(d).TheyincludesSAG(Rouxetal.,2012),SAGA(Defazioetal.,2014),andSVRG(Johnson&Zhang,2013;Xiao&Zhang,2014),whichallachievetheitera-tioncomplexityO(cid:0)(n+R2/(γλ))log(1/ǫ)(cid:1)toﬁndanǫ-optimalsolution.Infact,theyarecapableofexploitingthestrongconvexityfromdata,meaningthattheconditionnumberR2/(γλ)inthecomplexitycanbereplacedbythemorefavorableoneR2/(γ(λ+δµ2/n)).Thisimprovementcanbeachievedwithoutexplicitknowledgeofµfromdata.ExploitingStrongConvexityfromDatawithPrimal-DualFirst-OrderAlgorithmsDualalgorithmssolveFencheldualof(1)bymaximizingD(y)def=1nPni=1−φ∗i(yi)−g∗(cid:0)−1nPni=1yiai(cid:1)(2)usingrandomizedcoordinateascentalgorithms.(Hereφ∗iandg∗denotestheconjugatefunctionsofφiandg.)TheyincludeSDCA(Shalev-Shwartz&Zhang,2013),Nesterov(2012)andRicht´arik&Tak´aˇc(2014).TheyhavethesamecomplexityO(cid:0)(n+R2/(γλ))log(1/ǫ)(cid:1),butcannotex-ploitstrongconvexity,ifany(whenδµ2>0),fromdata.Primal-dualalgorithmssolvetheconvex-concavesaddlepointproblemminxmaxyL(x,y)whereL(x,y)def=1nPni=1(cid:0)yihai,xi−φ∗i(yi)(cid:1)+g(x).(3)Inparticular,SPDC(Zhang&Xiao,2015)achievesanacceleratedlinearconvergenceratewithiterationcom-plexityO(cid:0)(n+√nR/√γλ)log(1/ǫ)(cid:1),whichisbetterthantheaforementionednon-acceleratedcomplexitywhenR2/(γλ)>n.Lan&Zhou(2015)developeddual-freevariantsofacceleratedprimal-dualalgorithms,butwith-outconsideringthelinearpredictorstructureinERM.Balamurugan&Bach(2016)extendedSVRGandSAGAtosolvingsaddlepointproblems.Acceleratedprimalanddualrandomizedalgorithmshavealsobeendeveloped.Nesterov(2012),Fercoq&Richt´arik(2015)andLinetal.(2015b)developedacceleratedcoordi-nategradientalgorithms,whichcanbeappliedtosolvethedualproblem(2).Allen-Zhu(2016)developedanacceler-atedvariantofSVRG.AccelerationcanalsobeobtainedusingtheCatalystframework(Linetal.,2015a).TheyallachievethesameO(cid:0)(n+√nR/√γλ)log(1/ǫ)(cid:1)com-plexity.Acommonfeatureofacceleratedalgorithmsisthattheyrequiregoodestimateofthestrongconvexityparam-eter.ThismakeshardforthemtoexploitstrongconvexityfromdatabecausetheminimumsingularvalueµofthedatamatrixAisveryhardtoestimateingeneral.Inthispaper,weshowthatprimal-dualalgorithmsareca-pableofexploitingstrongconvexityfromdataifthealgo-rithmparameters(suchasstepsizes)aresetappropriately.Whiletheseoptimalsettingdependsontheknowledgeoftheconvexityparameterµfromthedata,wedevelopadap-tivevariantsofprimal-dualalgorithmsthatcantunethepa-rameterautomatically.Suchadaptiveschemesrelycriti-callyonthecapabilityofevaluatingtheprimal-dualopti-malitygapsbyprimal-dualalgorithms.Amajordisadvantageofprimal-dualalgorithmsisthattherequireddualproximalmappingmaynotadmitclosed-formorefﬁcientsolution.WefollowtheapproachofLan&Zhou(2015)toderivedual-freevariantsoftheprimal-dualalgorithmscustomizedforERMproblemswiththelinearpredictorstructure,andshowthattheycanalsoexploitstrongconvexityfromdatawithcorrectchoicesofparametersorusinganadaptationscheme.Algorithm1BatchPrimal-Dual(BPD)Algorithminput:parametersτ,σ,θ,initialpoint(˜x(0)=x(0),y(0))fort=0,1,2,...doy(t+1)=proxσf∗(cid:0)y(t)+σA˜x(t)(cid:1)x(t+1)=proxτg(cid:0)x(t)−τATy(t+1)(cid:1)˜x(t+1)=x(t+1)+θ(x(t+1)−x(t))endfor2.Batchprimal-dualalgorithmsWeﬁrststudybatchprimal-dualalgorithms,byconsideringa“batch”versionoftheERMproblem(1),minx∈Rd(cid:8)P(x)def=f(Ax)+g(x)(cid:9).(4)whereA∈Rn×d.Wemakethefollowingassumptions:Assumption2.Thefunctionsf,gandmatrixAsatisfy:•fisδ-stronglyconvexand1/γ-smoothwhereγ>0andδ≥0,andγδ≤1;•gisλ-stronglyconvexwhereλ≥0;•λ+δµ2>0,whereµ=pλmin(ATA).Usingconjugatefunctions,wecanderivethedualof(4)asmaxy∈Rn(cid:8)D(y)def=−f∗(y)−g∗(−ATy)(cid:9),(5)andtheconvex-concavesaddlepointformulationisminx∈Rdmaxy∈Rn(cid:8)L(x,y)def=g(x)+yTAx−f∗(y)(cid:9).(6)Weconsidertheprimal-dualﬁrst-orderalgorithmproposedbyChambolle&Pock(2011;2016)forsolvingthesaddlepointproblem(6),giveninAlgorithm1,whereproxψ(·),foranyconvexfunctionψ:Rn∪{∞},isdeﬁnedasproxψ(β)=argminα∈Rn(cid:0)ψ(α)+(1/2)kα−βk2(cid:1).Assumingthatfissmoothandgisstronglyconvex,Chambolle&Pock(2011;2016)showedthatAlgorithm1achievesacceleratedlinearconvergencerateifλ>0.However,theydidnotconsiderthecasewhereadditionalorthesolesourceofstrongconvexitycomesfromf(Ax).Inthefollowingtheorem,weshowhowtosettheparame-tersτ,σandθtoexploitbothsourcesofstrongconvexitytoachievefastlinearconvergence.Theorem1.SupposeAssumption2holdsand(x⋆,y⋆)istheuniquesaddlepointofLdeﬁnedin(6).LetL=kAk=pλmax(ATA).IfwesettheparametersinAlgorithm1asσ=1Lqλ+δµ2γ,τ=1Lqγλ+δµ2,(7)andθ=max{θx,θy}whereθx=(cid:16)1−δ(δ+2σ)µ2L2(cid:17)11+τλ,θy=11+σγ/2,(8)ExploitingStrongConvexityfromDatawithPrimal-DualFirst-OrderAlgorithmsthenwehave(cid:0)12τ+λ2(cid:1)kx(t)−x⋆k2+γ4ky(t)−y⋆k2≤θtC,L(x(t),y⋆)−L(x⋆,y(t))≤θtC,whereC=(cid:0)12τ+λ2(cid:1)kx(0)−x⋆k2+(cid:0)12σ+γ4(cid:1)ky(0)−y⋆k2.TheproofofTheorem1isgiveninAppendicesBandC.Herewegiveadetailedanalysisoftheconvergencerate.Substitutingσandτin(7)intotheexpressionsforθyandθxin(8),andassumingγ(λ+δµ2)≪L2,wehaveθx≈1−γδµ2L2(cid:16)2√γ(λ+δµ2)L+γδ(cid:17)−1−λLqγλ+δµ2,θy=11+√γ(λ+δµ2)/(2L)≈1−√γ(λ+δµ2)2L.SincetheoverallconditionnumberoftheproblemisL2γ(λ+δµ2),itisclearthatθyisanacceleratedconvergencerate.Nextweexamineθxintwospecialcases.Thecaseofδµ2=0butλ>0.Inthiscase,wehaveτ=1Lpγλandσ=1Lqλγ,andthusθx=11+√γλ/L≈1−√γλL,θy=11+√γλ/(2L)≈1−√γλ2L.Thereforewehaveθ=max{θx,θy}≈1−√λγ2L.Thisindeedisanacceleratedconvergencerate,recoveringtheresultofChambolle&Pock(2011;2016).Thecaseofλ=0butδµ2>0.Inthiscase,wehaveτ=1Lµpγδandσ=µLqδγ,andθx=1−γδµ2L2·12√γδµ/L+γδ,θy≈1−√γδµ2L.Noticethat1γδL2µ2istheconditionnumberoff(Ax).Nextweassumeµ≪Landexaminehowθxvarieswithγδ.•Ifγδ≈µ2L2,meaningfisbadlyconditioned,thenθx≈1−γδµ2L2·13√γδµ/L=1−√γδµ3L.Becausetheoverallconditionnumberis1γδL2µ2,thisisanacceleratedlinearrate,andsoisθ=max{θx,θy}.•Ifγδ≈µL,meaningfismildlyconditioned,thenθx≈1−µ3L312(µ/L)3/2+µ/L≈1−µ2L2.Thisrepresentsahalf-acceleratedrate,becausetheoverallconditionnumberis1γδL2µ2≈L3µ3.•Ifγδ=1,i.e.,fisasimplequadraticfunction,thenθx≈1−µ2L212µ/L+1≈1−µ2L2.Thisratedoesnothaveacceleration,becausetheover-allconditionnumberis1γδL2µ2≈L2µ2.Algorithm2AdaptiveBatchPrimal-Dual(Ada-BPD)input:problemconstantsλ,γ,δ,Landˆµ>0,initialpoint(x(0),y(0)),andadaptationperiodT.Computeσ,τ,andθasin(7)and(8)usingµ=ˆµfort=0,1,2,...doy(t+1)=proxσf∗(cid:0)y(t)+σA˜x(t)(cid:1)x(t+1)=proxτg(cid:0)x(t)−τATy(t+1)(cid:1)˜x(t+1)=x(t+1)+θ(x(t+1)−x(t))ifmod(t+1,T)==0then(σ,τ,θ)=BPD-Adapt(cid:0){P(s),D(s)}t+1s=t−T(cid:1)endifendforAlgorithm3BPD-Adapt(simpleheuristic)input:previousestimateˆµ,adaptionperiodT,primalanddualobjectivevalues{P(s),D(s)}ts=t−TifP(t)−D(t)<θT(P(t−T)−D(t−T))thenˆµ:=√2ˆµelseˆµ:=ˆµ/√2endifComputeσ,τ,andθasin(7)and(8)usingµ=ˆµoutput:newparameters(σ,τ,θ)Insummary,theextentofaccelerationinthedominatingfactorθx(whichdeterminesθ)dependsontherelativesizeofγδandµ2/L2,i.e.,therelativeconditioningbetweenthefunctionfandthematrixA.Ingeneral,wehavefullaccelerationifγδ≤µ2/L2.Thetheorypredictsthattheaccelerationdegradesasthefunctionfgetsbettercondi-tioned.However,inournumericalexperiments,weoftenobserveaccelerationevenifγδgetscloserto1.AsexplainedinChambolle&Pock(2011),Algorithm1isequivalenttoapreconditionedADMM.Deng&Yin(2016)characterizedvariousconditionsforADMMtoob-tainlinearconvergence,butdidnotderivetheconvergencerateforthecaseweconsiderinthispaper.2.1.Adaptivebatchprimal-dualalgorithmsInpractice,itisoftenveryhardtoobtainagoodesti-mateoftheproblem-dependentconstants,especiallyµ=pλmin(ATA),inordertoapplythealgorithmicparame-tersspeciﬁedinTheorem1.Hereweexploreheuristicsthatcanenableadaptivetuningofsuchparameters,whichoftenleadtomuchimprovedperformanceinpractice.AkeyobservationisthattheconvergencerateoftheBPDalgorithmchangesmonotonicallywiththeoverallconvex-ityparameterλ+δµ2,regardlessoftheextentofacceler-ation.Inotherwords,thelargerλ+δµ2is,thefastertheconvergence.Therefore,ifwecanmonitortheprogressofExploitingStrongConvexityfromDatawithPrimal-DualFirst-OrderAlgorithmsAlgorithm4BPD-Adapt(robustheuristic)input:previousrateestimateρ>0,∆=δˆµ2,periodT,constantsc<1andc>1,and{P(s),D(s)}ts=t−TComputenewrateestimateˆρ=P(t)−D(t)P(t−T)−D(t−T)ifˆρ≤cρthen∆:=2∆,ρ:=ˆρelseifˆρ≥cρthen∆:=∆/2,ρ:=ˆρelse∆:=∆endifσ=1Lqλ+∆γ,τ=1Lqγλ+∆Computeθusing(8)orsetθ=1output:newparameters(σ,τ,θ)theconvergenceandcompareitwiththepredictedconver-gencerateinTheorem1,thenwecanadjusttheestimatedparameterstoexploitstrongconvexityfromdata.Morespeciﬁcally,iftheobservedconvergencerateisslowerthanthepredictedrate,thenweshouldreducetheestimateofµ;otherwiseweshouldincreaseµforfasterconvergence.WeformalizetheabovereasoninginAlgorithm2(calledAda-BPD).Thisalgorithmmaintainsanestimateˆµofthetrueconstantµ,andadjustiteveryTiterations.WeuseP(t)andD(t)torepresenttheprimalanddualobjectivevaluesatP(x(t))andD(y(t)),respectively.WegivetwoimplementationsofthetuningprocedureBPD-Adapt:Al-gorithm3isasimpleheuristicfortuningtheestimateˆµ,wheretheincreasinganddecreasingfactor√2canbechangedtoothervalueslargerthan1.Algorithm4isamorerobustheuristic.Itdoesnotrelyonthespeciﬁccon-vergencerateθestablishedinTheorem1.Instead,itsimplycomparesthecurrentestimateofobjectivereductionrateˆρwiththepreviousestimateρ.Italsospeciﬁesanon-tuningrangeofchangesinρ,speciﬁedbytheinterval[c,c].Thecapabilityofaccessingboththeprimalanddualob-jectivevaluesallowsprimal-dualalgorithmstohavegoodestimateoftheconvergencerate,whichenableseffectivetuningheuristics.Automatictuningofprimal-dualalgo-rithmshavealsobeenstudiedby,e.g.,Malitsky&Pock(2016)andGoldsteinetal.(2013),butwithdifferentgoals.3.Randomizedprimal-dualalgorithmInthissection,wecomebacktotheERMproblemandcon-sideritssaddle-pointformulationin(3).Duetoitsﬁnitesumstructureinthedualvariablesyi,wecandeveloperan-domizedalgorithmstoexploitstrongconvexityfromdata.Inparticular,weextendthestochasticprimal-dualcoordi-nate(SPDC)algorithmbyZhang&Xiao(2015).SPDCisAlgorithm5AdaptiveSPDC(Ada-SPDC)input:parametersσ,τ,θ>0,initialpoint(x(0),y(0)),andadaptationperiodT.Set˜x(0)=x(0)fort=0,1,2,...dopickk∈{1,...,n}uniformlyatrandomfori∈{1,...,n}doifi==ktheny(t+1)k=proxσφ∗k(cid:16)y(t)k+σaTk˜x(t)(cid:17)elsey(t+1)i=y(t)iendifendforx(t+1)=proxτg(cid:16)x(t)−τ(cid:0)u(t)+(y(t+1)k−y(t)k)ak(cid:1)(cid:17)u(t+1)=u(t)+1n(y(t+1)k−y(t)k)ak˜x(t+1)=x(t+1)+θ(x(t+1)−x(t))ifmod(t+1,T·n)=0then(τ,σ,θ)=SPDC-Adapt(cid:0){P(t−sn),D(t−sn)}Ts=0(cid:1)endifendforaspecialcaseoftheAda-SPDCalgorithminAlgorithm5,bysettingtheadaptionperiodT=∞(noadaption).ThefollowingtheoremisprovedinAppendixE.Theorem2.SupposeAssumption1holds.Let(x⋆,y⋆)bethesaddlepointofthefunctionLdeﬁnedin(3),andR=max{ka1k,...,kank}.IfwesetT=∞inAlgorithm5(noadaption)andletτ=14Rqγnλ+δµ2,σ=14Rqnλ+δµ2γ,(9)andθ=max{θx,θy}whereθx=(cid:16)1−τσδµ22n(σ+4δ)(cid:17)11+τλ,θy=1+((n−1)/n)σγ/21+σγ/2,(10)thenwehave(cid:0)12τ+λ2(cid:1)E(cid:2)kx(t)−x⋆k2(cid:3)+γ4E(cid:2)ky(t)−y⋆k2(cid:3)≤θtC,E(cid:2)L(x(t),y⋆)−L(x⋆,y(t))(cid:3)≤θtC,whereC=(cid:0)12τ+λ2(cid:1)kx(0)−x⋆k2+(cid:0)12σ+γ4(cid:1)ky(0)−y⋆k2.TheexpectationE[·]istakenwithrespecttothehistoryofrandomindicesdrawnateachiteration.Belowwegiveadetaileddiscussionontheexpectedcon-vergencerateestablishedinTheorem2.Thecasesofσµ2=0butλ>0.Inthiscasewehaveτ=14Rpγnλandσ=14Rqnλγ,andθx=11+τλ=1−11+4R√n/(λγ),θy=1+((n−1)/n)σγ/21+σγ/2=1−1n+8R√n/(λγ).ExploitingStrongConvexityfromDatawithPrimal-DualFirst-OrderAlgorithmsHenceθ=θy.Theserecovertheparametersandconver-gencerateofthestandardSPDC(Zhang&Xiao,2015).Thecasesofσµ2>0butλ=0.Inthiscasewehaveτ=14Rµpγδandσ=µ4Rqδγ,andθx=1−τσδµ22n(σ+4δ)=1−γδµ232nR2·1√γδµ/(4R)+4γδ.θy=1−1n+8nR/(µ√γδ)≈1−√γδµ8nR(cid:16)1+√γδµ8R(cid:17)−1.SincetheobjectiveisR2/γ-smoothandδµ2/n-stronglyconvex,θyisanacceleratedrateif√γδµ8R≪1(otherwiseθy≈1−1n).Forθx,weconsiderdifferentsituations:•Ifµ≥R,thenwehaveθx≈1−√γδµnR,whichisanacceleratedrate.Soisθ=max{θx,θy}.•Ifµ<Randγδ≈µ2R2,thenθx≈1−√γδµnR,whichrepresentsanacceleratedrate.Theiterationcomplex-ityofSPDCiseO(nRµ√γδ),whichisbetterthanthatofSVRGinthiscase,whichiseO(nR2γδµ2).•Ifµ<Randγδ≈µR,thenwegetθx≈1−µ2nR2.Thisisahalf-acceleratedrate,becauseinthiscaseSVRGrequireseO(nR3µ3)iterations,versuseO(nR2µ2)forSPDC.•Ifµ<Randγδ≈1,meaningtheφi’sarewellconditioned,thenwegetθx≈1−γδµ2nR2≈1−µ2nR2,whichisanon-acceleratedrate.ThecorrespondingiterationcomplexityisthesameasSVRG.3.1.ParameteradaptationforSPDCTheSPDC-AdaptprocedurecalledinAlgorithm5followsthesamelogicsasthebatchadaptionschemesinAlgo-rithms3and4,andweomitthedetailshere.OnethingweemphasizehereisthattheadaptationperiodTisintermsofepochs,ornumberofpassesoverthedata.Inaddition,weonlycomputetheprimalanddualobjectivevaluesaf-tereachpassoreveryfewpasses,becausecomputingthemexactlyusuallyneedtotakeafullpassofthedata.Unlikethebatchcasewherethedualitygapdecreasesmonotonically,thedualitygapforrandomizedalgorithmscanﬂuctuatewildly.SoinsteadofusingonlythetwoendvaluesP(t−Tn)−D(t−Tn)andP(t)−D(t),wecanusemorepointstoestimatetheconvergenceratethroughalinearregression.Supposetheprimal-dualobjectivevaluesforthelastT+1passesare(P(0),D(0)),(P(1),D(1)),...,(P(T),D(T)),andweneedtoestimateρ(rateperpass)suchthatP(t)−D(t)≈ρt(cid:0)P(0)−D(0)(cid:1),t=1,...,T.Wecanturnitintoalinearregressionproblemaftertakinglogarithmandobtaintheestimateˆρthroughlog(ˆρ)=112+22+···+T2PTt=1tlogP(t)−D(t)P(0)−D(0).Algorithm6Dual-FreeBPDAlgorithminput:parametersσ,τ,θ>0,initialpoint(x(0),y(0))Set˜x(0)=x(0)andv(0)=(f∗)′(y(0))fort=0,1,2,...dov(t+1)=v(t)+σA˜x(t)1+σ,y(t+1)=f′(v(t+1))x(t+1)=proxτg(cid:0)x(t)−τATy(t+1)(cid:1)˜x(t+1)=x(t+1)+θ(x(t+1)−x(t))endfor4.Dual-freePrimal-dualalgorithmsComparedwithprimalalgorithms,onemajordisadvantageofprimal-dualalgorithmsistherequirementofcomputingtheproximalmappingofthedualfunctionf∗orφ∗i,whichmaynotadmitclosed-formedsolutionorefﬁcientcomputa-tion.Thisisespeciallythecaseforlogisticregression,oneofthemostpopularlossfunctionsusedinclassiﬁcation.Lan&Zhou(2015)developed“dual-free”variantsofprimal-dualalgorithmsthatavoidcomputingthedualprox-imalmapping.TheirmaintechniqueistoreplacetheEu-clideandistanceinthedualproximalmappingwithaBreg-mandivergencedeﬁnedovertheduallossfunctionitself.Weshowhowtoapplythisapproachtosolvethestruc-turedERMproblemsconsideredinthispaper.Theycanalsoexploitstrongconvexityfromdataifthealgorithmicparametersaresetappropriatelyoradaptedautomatically.4.1.Dual-freeBPDalgorithmFirst,weconsiderthebatchsetting.Wereplacethedualproximalmapping(computingy(t+1))inAlgorithm1withy(t+1)=argminy(cid:8)f∗(y)−yTA˜x(t)+1σD(y,y(t))(cid:9),(11)whereDistheBregmandivergenceofastrictlyconvexkernelfunctionh,deﬁnedasDh(y,y(t))=h(y)−h(y(t))−h∇h(y(t)),y−y(t)i.Algorithm1isobtainedintheEuclideansettingwithh(y)=12kyk2andD(y,y(t))=12ky−y(t)k2.Hereweusef∗asthekernelfunction,andshowthatitallowsustocom-putey(t+1)in(11)veryefﬁciently.Thefollowinglemmaexplainsthedetails(Cf.Lan&Zhou,2015,Lemma1).Lemma1.Letthekernelh≡f∗intheBregmandiver-genceD.Ifweconstructasequenceofvectors{v(t)}suchthatv(0)=(f∗)′(y(0))andforallt≥0,v(t+1)=v(t)+σA˜x(t)1+σ,(12)thenthesolutiontoproblem(11)isy(t+1)=f′(v(t+1)).Proof.Supposev(t)=(f∗)′(y(t))(truefort=0),thenD(y,y(t))=f∗(y)−f∗(y(t))−v(t)T(y−y(t)).ExploitingStrongConvexityfromDatawithPrimal-DualFirst-OrderAlgorithmsThesolutionto(11)canbewrittenasy(t+1)=argminynf∗(y)−yTA˜x(t)+1σ(cid:0)f∗(y)−v(t)Ty(cid:1)o=argminyn(cid:0)1+1σ(cid:1)f∗(y)−(cid:0)A˜x(t)+1σv(t)(cid:1)Tyo=argmaxyn(cid:16)v(t)+σA˜x(t)1+σ(cid:17)Ty−f∗(y)o=argmaxynv(t+1)Ty−f∗(y)o=f′(v(t+1)),whereinthelastequalityweusedthepropertyofconjugatefunctionwhenfisstronglyconvexandsmooth.Moreover,v(t+1)=(f′)−1(y(t+1))=(f∗)′(y(t+1)),whichcompletestheproof.AccordingtoLemma1,weonlyneedtoprovideinitialpointssuchthatv(0)=(f∗)′(y(0))iseasytocompute.Wedonotneedtocompute(f∗)′(y(t))directlyforanyt>0,becauseitiscanbeupdatedasv(t)in(12).Consequently,wecanupdatey(t)intheBPDalgorithmusingthegradientf′(v(t)),withouttheneedofdualproximalmapping.Theresultingdual-freealgorithmisgiveninAlgorithm6.Theorem3.SupposeAssumption2holdsandlet(x⋆,y⋆)betheuniquesaddlepointofLdeﬁnedin(6).IfwesettheparametersinAlgorithm6asτ=1Lqγλ+δµ2,σ=1Lpγ(λ+δµ2),(13)andθ=max{θx,θy}whereθx=(cid:16)1−τσδµ2(4+2σ)(cid:17)11+τλ,θy=11+σ/2,(14)thenwehave(cid:0)12τ+λ2(cid:1)kx(t)−x⋆k2+12D(y⋆,y(t))≤θtC,L(x(t),y⋆)−L(x⋆,y(t))≤θtC,whereC=(cid:0)12τ+λ2(cid:1)kx(0)−x⋆k2+(cid:0)1σ+12(cid:1)D(y⋆,y(0)).Theorem3isprovedinAppendicesBandD.Assumingγ(λ+δµ2)≪L2,wehaveθx≈1−γδµ216L2−λ2Lqγλ+δµ2,θy≈1−√γ(λ+δµ2)4L.Again,wegaininsightsbyconsiderthespecialcases:•Ifδµ2=0andλ>0,thenθy≈1−√γλ4Landθx≈1−√γλ2L.Soθ=max{θx,θy}isanacceleratedrate.•Ifδµ2>0andλ=0,thenθy≈1−√γδµ24Landθx≈1−γδµ216L2.Thusθ=max{θx,θy}≈1−γδµ216L2isnotaccelerated.Thisconclusiondoesnotdependsontherelativesizesofγδandµ2/L2,anditisthemajordifferencefromtheEuclideancaseinSection2.Algorithm7AdaptiveDual-FreeSPDC(ADF-SPDC)input:parametersσ,τ,θ>0,initialpoint(x(0),y(0)),andadaptationperiodT.Set˜x(0)=x(0)andv(0)i=(φ∗i)′(y(0)i)fori=1,...,nfort=0,1,2,...dopickk∈{1,...,n}uniformlyatrandomfori∈{1,...,n}doifi==kthenv(t+1)k=v(t)k+σaTk˜x(t)1+σ,y(t+1)k=φ′k(v(t+1)k)elsev(t+1)i=v(t)i,y(t+1)i=y(t)iendifendforx(t+1)=proxτg(cid:16)x(t)−τ(cid:0)u(t)+(y(t+1)k−y(t)k)ak(cid:1)(cid:17)u(t+1)=u(t)+1n(y(t+1)k−y(t)k)ak˜x(t+1)=x(t+1)+θ(x(t+1)−x(t))ifmod(t+1,T·n)=0then(τ,σ,θ)=SPDC-Adapt(cid:0){P(t−sn),D(t−sn)}Ts=0(cid:1)endifendforIfbothδµ2>0andλ>0,thentheextentofaccelerationdependsontheirrelativesize.Ifλisonthesameorderasδµ2orlarger,thenacceleratedrateisobtained.Ifλismuchsmallerthanδµ2,thenthetheorypredictsnoacceleration.4.2.Dual-freeSPDCalgorithmFollowingthesameapproach,wecanderiveanAdap-tiveDual-FreeSPDCalgorithm,giveninAlgorithm7.Onrelatedwork,Shalev-Shwartz&Zhang(2016)and(Shalev-Shwartz,2016)introduceddual-freeSDCA.Thefollowingtheoremcharacterizesthechoiceofalgorith-micparametersthatcanexploitstrongconvexityfromdatatoachievelinearconvergence(proofgiveninAppendixF).Theorem4.SupposeAssumption1holds.Let(x⋆,y⋆)bethesaddlepointofLin(3)andR=max{ka1k,...,kank}.IfwesetT=∞inAlgorithm7(nonadaption)andletσ=14Rpγ(nλ+δµ2),τ=14Rqγnλ+δµ2,(15)andθ=max{θx,θy}whereθx=(cid:16)1−τσδµ2n(4+2σ)(cid:17)11+τλ,θy=1+((n−1)/n)σ/21+σ/2,(16)thenwehave(cid:0)12τ+λ2(cid:1)E(cid:2)kx(t)−x⋆k2(cid:3)+γ4E(cid:2)D(y⋆,y(t))(cid:3)≤θtC,E(cid:2)L(x(t),y⋆)−L(x⋆,y(t))(cid:3)≤θtC,whereC=(cid:0)12τ+λ2(cid:1)kx(0)−x⋆k2+(cid:0)1σ+12(cid:1)D(y⋆,y(0)).ExploitingStrongConvexityfromDatawithPrimal-DualFirst-OrderAlgorithms02040608010-1010-5NumberofpassesAda-BPDOpt-BPDBPDPrimalAGPrimaloptimalitygap010020030040050010-1010-5Numberofpasses0200400600800100010-1010-5Numberofpassessynthetic1,λ=1/nsynthetic1,λ=10−2/nsynthetic1,λ=10−4/nFigure1.Comparisonofbatchprimal-dualalgorithmsforaridgeregressionproblemwithn=5000andd=3000.02040608010-1010-5100NumberofpassesADF-SPDCDF-SPDCSPDCSVRGSAGAKatyushaPrimaloptimalitygap010020030040010-1010-5100Numberofpasses0200400600800100010-1010-5100Numberofpassescpuact,λ=1/ncpuact,λ=10−2/ncpuact,λ=10−4/nFigure2.Comparisonofrandomizedalgorithmsforridgeregressionproblems.NowwediscusstheresultsofTheorem4infurtherdetails.Thecasesofσµ2=0butλ>0.Inthiscasewehaveτ=14Rpγnλandσ=14R√nγλ,andθx=1−11+4R√n/(λγ),θy=1−1n+8R√n/(λγ).TherateisthesameasforSPDCinZhang&Xiao(2015).Thecasesofσµ2>0butλ=0.Inthiscasewehaveτ=14Rµpγδandσ=µ4R√δγ,thusθx=1−τσδµ22n(σ+4)=1−γδµ232nR2·1√γδµ/(4R)+4,θy=1+((n−1)/n)σ/21+σ/2=1−1n+8nR/(µ√γδ).WenotethattheprimalfunctionnowisR2/γ-smoothandδµ2/n-stronglyconvex.Wediscussthefollowingcases:•If√γδµ>R,thenwehaveθx≈1−√γδµ8nRandθy≈1−1n.Thereforeθ=max{θx,θy}≈1−1n.•Otherwise,wehaveθx≈1−γδµ264nR2andθyisofthesameorder.Thisisnotanacceleratedrate,andwehavethesameiterationcomplexityasSVRG.Finally,wegiveconcreteexamplesofhowtocomputetheinitialpointsy(0)andv(0)suchthatv(0)i=(φ∗i)′(y(0)i).•Forsquaredloss,φi(α)=12(α−bi)2andφ∗i(β)=12β2+biβ.Sov(0)i=(φ∗i)′(y(0)i)=y(0)i+bi.•Forlogisticregression,wehavebi∈{1,−1}andφi(α)=log(1+e−biα).Theconjugatefunctionisφ∗i(β)=(−biβ)log(−biβ)+(1+biβ)log(1+biβ)ifbiβ∈[−1,0]and+∞otherwise.Wecanchoosey(0)i=−12biandv(0)i=0suchthatv(0)i=(φ∗i)′(y(0)i).Forlogisticregression,wehaveδ=0overthefulldo-mainofφi.However,eachφiislocallystronglyconvexinboundeddomain(Bach,2014):ifz∈[−B,B],thenweknowδ=minzφi′′(z)≥exp(−B)/4.ThereforeitiswellsuitableforanadaptationschemesimilartoAlgo-rithm4thatdonotrequireknowledgeofeitherδorµ.5.PreliminaryexperimentsWepresentpreliminaryexperimentstodemonstratetheef-fectivenessofourproposedalgorithms.First,weconsiderbatchprimal-dualalgorithmsforridgeregressionoverasyntheticdataset.ThedatamatrixAhassizesn=5000andd=3000,anditsentriesaresampledfrommul-tivariatenormaldistributionwithmeanzeroandcovari-ancematrixΣij=2|i−j|/2.WenormalizealldatasetsExploitingStrongConvexityfromDatawithPrimal-DualFirst-OrderAlgorithms02040608010-1010-5NumberofpassesADF-SPDCDF-SPDCSPDCSVRGSAGAKatyushaPrimaloptimalitygap0200400600800100010-1010-5Numberofpasses0200400600800100010-1010-5100Numberofpassessynthetic2,λ=1/nsynthetic2,λ=10−2/nsynthetic2,λ=10−4/n02040608010-1010-5NumberofpassesADF-SPDCDF-SPDCSPDCSVRGSAGAKatyushaPrimaloptimalitygap010020030040050010-1010-5Numberofpasses010020030040050010-5100Numberofpassesrcv1,λ=1/nrcv1,λ=10−2/nrcv1,λ=10−4/nFigure3.Comparisonofrandomizedalgorithmsforlogisticregressionproblems.suchthatai=ai/(maxjkajk),toensurethemaximumnormofthedatapointsis1.Weuseℓ2-regularizationg(x)=(λ/2)kxk2withthreechoicesofparameterλ:1/n,10−2/nand10−4/n,whichrepresentthestrong,medium,andweaklevelsofregularization,respectively.Figure1showstheperformanceoffourdifferentalgo-rithms:theprimalacceleratedgradient(PrimalAG)algo-rithm(Nesterov,2004)usingλasstrongconvexityparam-eter,theBPDalgorithm(Algorithm1)thatusesthesameλandµ2δ=0,theoptimalBPDalgorithm(Opt-BPD)thatusesµ2δ=λmin(ATA)n≈0.022ncomputedfromdata,andtheAda-BPDalgorithm(Algorithm2)withtherobustadaptationheuristic(Algorithm4)withT=10,c=0.95andc=1.5.Asexpected,theperformanceofPrimal-AGisverysimilartothatofBPD,andOpt-BPDhasthefastestconvergence.TheAda-BPDalgorithmcanpartiallyexploitstrongconvexityfromdatawithoutknowledgeofµ.NextwecompareDF-SPDC(Algorithm5withoutadap-tion)andADF-SPDC(Algorithm7withadaption)againstseveralstate-of-the-artrandomizedalgorithmsforERM:SVRG(Johnson&Zhang,2013),SAGA(Defazioetal.,2014)Katyusha(Allen-Zhu,2016)andthestandardSPDCmethod(Zhang&Xiao,2015).ForSVRGandKatyusha(anacceleratedvariantofSVRG),wechoosethevariancereductionperiodasm=2n.Thestepsizesofallal-gorithmsaresetastheiroriginalpapersuggested.ForAda-SPDCandADF-SPDC,weusetherobustadaptationschemewithT=10,c=0.95andc=1.5.Weﬁrstcomparetheserandomizedalgorithmsforridgere-gressionoverthecpuactdatafromtheLibSVMwebsite(https://www.csie.ntu.edu.tw/˜cjlin/libsvm/).TheresultsareshowninFigure2.Withrelativelystrongregularizationλ=1/n,allmethodsperformsimilarlyaspredictedbytheory.Whenλbecomessmaller,thenon-acceleratedalgorithms(SVRGandSAGA)automaticallyexploitstrongconvexityfromdata,sotheybecomefasterthanthenon-adaptiveacceleratedmethods(Katyusha,SPDCandDF-SPDC).Theadaptiveacceleratedmethod,ADF-SPDC,hasthefastestconvergence.Thisindicatesthatourtheoreticalresults,whichpredictnoaccelerationinthiscase,maybefurtherimproved.Finallywecomparethesealgorithmsforlogisticregres-siononthercv1dataset(fromLibSVMwebsite)andan-othersyntheticdatasetwithn=5000andd=500,generatedsimilarlyasbeforebutwithcovariancematrixΣij=2|i−j|/100.ForthestandardSPDC,wecomputethecoordinate-wisedualproximalmappingusingafewstepsofscalarNewton’smethodtohighprecision.Thedual-freeSPDCalgorithmsonlyusegradientsofthelogisticfunction.TheresultsarepresentedinFigure3.Forbothdatasets,thestrongconvexityfromdataisveryweak,andtheacceleratedalgorithmsperformsbetter.ExploitingStrongConvexityfromDatawithPrimal-DualFirst-OrderAlgorithmsReferencesAllen-Zhu,Zeyuan.Katyusha:Acceleratedvariancere-ductionforfastersgd.ArXive-print1603.05953,2016.Bach,Francis.Adaptivityofaveragedstochasticgradientdescenttolocalstrongconvexityforlogisticregression.JournalofMachineLearningResearch,15(1):595–627,2014.Balamurugan,PalaniappanandBach,Francis.Stochasticvariancereductionmethodsforsaddle-pointproblems.InAdvancesinNeuralInformationProcessingSystems(NIPS)29,pp.1416–1424,2016.Bertsekas,DimitriP.Incrementalgradient,subgradient,andproximalmethodsforconvexoptimization:Asur-vey.InSra,Suvrit,Nowozin,Sebastian,andWright,StephenJ.(eds.),OptimizationforMachineLearning,chapter4,pp.85–120.MITPress,2012.Chambolle,AntoninandPock,Thomas.Aﬁrst-orderprimal-dualalgorithmforconvexproblemswithappli-cationstoimaging.JournalofMathematicalImagingandVision,40(1):120–145,2011.Chambolle,AntoninandPock,Thomas.Ontheergodicconvergenceratesofaﬁrst-orderprimal–dualalgorithm.MathematicalProgramming,SeriesA,159:253–287,2016.Defazio,Aaron,Bach,Francis,andLacoste-Julien,Simon.Saga:Afastincrementalgradientmethodwithsupportfornon-stronglyconvexcompositeobjectives.InAd-vancesinNeuralInformationProcessingSystems,pp.1646–1654,2014.Deng,WeiandYin,Wotao.Ontheglobalandlinearcon-vergenceofthegeneralizedalternatingdirectionmethodofmultipliers.JournalofScientiﬁcComputing,66(3):889–916,2016.Fercoq,OliverandRicht´arik,Peter.Accelerated,parallel,andproximalcoordinatedescent.SIAMJournalonOp-timization,25(4):1997–2023,2015.Goldstein,Tom,Li,Min,Yuan,Xiaoming,Esser,Ernie,andBaraniuk,Richard.Adaptiveprimal-dualhybridgra-dientmethodsforsaddle-pointproblems.arXivpreprintarXiv:1305.0546,2013.Johnson,RieandZhang,Tong.Acceleratingstochasticgradientdescentusingpredictivevariancereduction.InAdvancesinNeuralInformationProcessingSystems,pp.315–323,2013.Lan,GuanghuiandZhou,Yi.Anoptimalrandom-izedincrementalgradientmethod.arXivpreprintarXiv:1507.02000,2015.Lin,Hongzhou,Mairal,Julien,andHarchaoui,Zaid.Auniversalcatalystforﬁrst-orderoptimization.InAd-vancesinNeuralInformationProcessingSystems,pp.3384–3392,2015a.Lin,Qihang,Lu,Zhaosong,andXiao,Lin.Anacceler-atedrandomizedproximalcoordinategradientmethodanditsapplicationtoregularizedempiricalriskmini-mization.SIAMJournalonOptimization,25(4):2244–2273,2015b.Malitsky,YuraandPock,Thomas.Aﬁrst-orderprimal-dualalgorithmwithlinesearch.arXivpreprintarXiv:1608.08883,2016.Nedic,AngeliaandBertsekas,DimitriP.Incrementalsubgradientmethodsfornondifferentiableoptimization.SIAMJournalonOptimization,12(1):109–138,2001.Nesterov,Yurii.IntroductoryLecturesonConvexOpti-mization:ABasicCourse.Kluwer,Boston,2004.Nesterov,Yurii.Efﬁciencyofcoordinatedescentmethodsonhuge-scaleoptimizationproblems.SIAMJournalonOptimization,22(2):341–362,2012.Richt´arik,PeterandTak´aˇc,Martin.Iterationcomplexityofrandomizedblock-coordinatedescentmethodsformin-imizingacompositefunction.MathematicalProgram-ming,144(1-2):1–38,2014.Roux,NicolasL,Schmidt,Mark,andBach,Francis.Astochasticgradientmethodwithanexponentialconver-gencerateforﬁnitetrainingsets.InAdvancesinNeuralInformationProcessingSystems,pp.2663–2671,2012.Shalev-Shwartz,Shai.SDCAwithoutduality,regulariza-tion,andindividualconvexity.InProceedingsofThe33rdInternationalConferenceonMachineLearning,pp.747–754,2016.Shalev-Shwartz,ShaiandZhang,Tong.Stochasticdualco-ordinateascentmethodsforregularizedlossminimiza-tion.JournalofMachineLearningResearch,14(Feb):567–599,2013.Shalev-Shwartz,ShaiandZhang,Tong.Acceleratedproxi-malstochasticdualcoordinateascentforregularizedlossminimization.MathematicalProgramming,155(1-2):105–145,2016.Xiao,LinandZhang,Tong.Aproximalstochasticgradi-entmethodwithprogressivevariancereduction.SIAMJournalonOptimization,24(4):2057–2075,2014.Zhang,YuchenandXiao,Lin.Stochasticprimal-dualco-ordinatemethodforregularizedempiricalriskminimiza-tion.InProceedingsofThe32ndInternationalConfer-enceonMachineLearning,pp.353–361,2015.