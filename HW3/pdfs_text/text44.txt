An Alternative Softmax Operator for Reinforcement Learning

Kavosh Asadi 1 Michael L. Littman 1

Abstract

A softmax operator applied to a set of values
acts somewhat like the maximization function
and somewhat like an average.
In sequential
softmax is often used in
decision making,
settings where it is necessary to maximize utility
but also to hedge against problems that arise
from putting all of one’s weight behind a single
The Boltzmann
maximum utility decision.
softmax operator is the most commonly used
softmax operator in this setting, but we show
that this operator is prone to misbehavior.
In
this work, we study a differentiable softmax
operator
is a
non-expansion ensuring a convergent behavior in
learning and planning. We introduce a variant
of SARSA algorithm that, by utilizing the new
operator, computes a Boltzmann policy with
a state-dependent temperature parameter. We
show that the algorithm is convergent and that it
performs favorably in practice.

that, among other properties,

1. Introduction

tension in decision making
There is a fundamental
between choosing the action that has highest expected
utility and avoiding “starving” the other actions. The issue
the exploration–exploitation
arises in the context of
decision
dilemma
problems (Sutton, 1990), and when interpreting observed
decisions (Baker et al., 2007).

non-stationary

(Thrun,

1992),

In reinforcement learning, an approach to addressing the
tension is the use of softmax operators for value-function
optimization, and softmax policies for action selection.
Examples include value-based methods such as SARSA
(Rummery & Niranjan, 1994) or expected SARSA (Sutton
& Barto, 1998; Van Seijen et al., 2009), and policy-search
methods such as REINFORCE (Williams, 1992).

1Brown University, USA. Correspondence to: Kavosh Asadi

<kavosh@brown.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

An ideal softmax operator is a parameterized set of
operators that:

1. has parameter settings that allow it to approximate
maximization arbitrarily accurately to perform
reward-seeking behavior;

2. is a non-expansion for all parameter settings ensuring

convergence to a unique ﬁxed point;

3. is differentiable to make it possible to improve via

gradient-based optimization; and

4. avoids the starvation of non-maximizing actions.

Let X = x1, . . . , xn be a vector of values. We deﬁne the
following operators:

max(X) = max

xi ,

i∈{1,...,n}

mean(X) =

n

xi ,

1
n

i=1
(cid:88)
eps(cid:15)(X) = (cid:15) mean(X) + (1

(cid:15)) max(X) ,

boltzβ(X) =

−
n
i=1 xi eβxi
n
i=1 eβxi

.

(cid:80)

The ﬁrst operator, max(X),
a
(cid:80)
non-expansion (Littman & Szepesv´ari, 1996). However,
it
and ignores
non-maximizing selections (Property 4).

is non-differentiable

is known to be

(Property 3),

The next operator, mean(X), computes the average of its
inputs. It is differentiable and, like any operator that takes a
ﬁxed convex combination of its inputs, is a non-expansion.
However, it does not allow for maximization (Property 1).

The third operator eps(cid:15)(X), commonly referred to as
epsilon greedy (Sutton & Barto, 1998),
interpolates
between max and mean. The operator is a non-expansion,
because it is a convex combination of two non-expansion
operators. But it is non-differentiable (Property 3).

The Boltzmann operator boltzβ(X) is differentiable.
, and mean as β
also approximates max as β
→
0. However, it is not a non-expansion (Property 2), and
therefore, prone to misbehavior as will be shown in the next
section.

→ ∞

It

An Alternative Softmax Operator for Reinforcement Learning

two actions, and
Figure 1. A simple MDP with two states,
γ = 0.98 . The use of a Boltzmann softmax policy is not sound
in this simple domain.

In the following section, we provide a simple example
illustrating why the non-expansion property is important,
especially in the context of planning and on-policy
learning. We then present a new softmax operator
is a
that
non-expansion. We prove several critical properties of this
new operator, introduce a new softmax policy, and present
empirical results.

to the Boltzmann operator yet

is similar

2. Boltzmann Misbehaves

We ﬁrst show that boltzβ can lead to problematic behavior.
To this end, we ran SARSA with Boltzmann softmax policy
(Algorithm 1) on the MDP shown in Figure 1. The edges
are labeled with a transition probability (unsigned) and a
reward number (signed). Also, state s2 is a terminal state,
so we only consider two action values, namely ˆQ(s1, a)
and ˆQ(s2, b). Recall that the Boltzmann softmax policy
assigns the following probability to each action:

s) =

π(a
|

eβ ˆQ(s,a)
a eβ ˆQ(s,a)

.

(cid:80)

Algorithm 1 SARSA with Boltzmann softmax policy

Input: initial ˆQ(s, a)
for each episode do

s
∀

a

, α, and β

∈ S ∀

∈ A

Boltzmann with parameter β

Initialize s
a
∼
repeat
Take action a, observe r, s(cid:48)
(cid:48)
a
∼
ˆQ(s, a)

ˆQ(s, a) + α

Boltzmann with parameter β

r + γ ˆQ(s(cid:48), a(cid:48))

←
, a

s

(cid:48)
(cid:48)
a
s
until s is terminal

←

←

end for

(cid:104)

−

ˆQ(s, a)
(cid:105)

In Figure 2, we plot state–action value estimates at the end
of each episode of a single run (smoothed by averaging
over ten consecutive points). We set α = .1 and β = 16.55.
The value estimates are unstable.

Figure 2. Values estimated by SARSA with Boltzmann softmax.
The algorithm never achieves stable values.

SARSA is known to converge in the tabular setting using
(cid:15)-greedy exploration (Littman & Szepesv´ari, 1996), under
decreasing exploration (Singh et al., 2000), and to a region
in the function-approximation setting (Gordon, 2001).
There are also variants of the SARSA update rule that
converge more generally (Perkins & Precup, 2002; Baird
& Moore, 1999; Van Seijen et al., 2009). However, this
example is the ﬁrst, to our knowledge, to show that SARSA
fails to converge in the tabular setting with Boltzmann
policy. The next section provides background for our
analysis of the example.

3. Background

:

,

R

(cid:104)S

,
A

, where

A Markov decision process (Puterman, 1994), or MDP,
is speciﬁed by the tuple
is the
,
, γ
(cid:105)
is the set of actions. The functions
set of states and
[0, 1] denote the

R
reward and transition dynamics of the MDP. Finally, γ
∈
[0, 1), the discount rate, determines the relative importance
of immediate reward as opposed to the rewards received in
the future.

S × A × S →

S × A →

A
R and

P

P

S

:

A typical approach to ﬁnding a good policy is to estimate
how good it
is to be in a particular state—the state
value function. The value of a particular state s given
a policy π and initial action a is written Qπ(s, a). We
deﬁne the optimal value of a state–action pair Q(cid:63)(s, a) =
maxπ Qπ(s, a). It is possible to deﬁne Q(cid:63)(s, a) recursively
and as a function of the optimal value of the other
state–action pairs:

Q(cid:63)(s, a) =

(s, a) +

(s, a, s(cid:48)) max

Q(cid:63)(s(cid:48), a(cid:48)) .

a(cid:48)

R

γ

P

(cid:88)s(cid:48)∈S

Bellman equations, such as the above, are at the core
of many reinforcement-learning algorithms such as Value
Iteration (Bellman, 1957). The algorithm computes the

S10.340.66a+0.122b0.010.99+0.033S2episode numberAn Alternative Softmax Operator for Reinforcement Learning

ˆQ(s, a)

(s, a)+γ

← R

γ

P

(cid:88)s(cid:48)∈S

(cid:79)a(cid:48)

(s, a, s(cid:48))

ˆQ(s(cid:48), a(cid:48)). (1)

This matches the GVI update (1) when

Figure 3. max is a non-expansion under the inﬁnity norm.

value of the best policy in an iterative fashion:

ˆQ(s, a)

(s, a) + γ

(s, a, s(cid:48)) max

ˆQ(s(cid:48), a(cid:48)).

← R

P

a(cid:48)

(cid:88)s(cid:48)∈S
Regardless of its initial value, ˆQ will converge to Q∗.

Littman & Szepesv´ari (1996) generalized this algorithm by
replacing the max operator by any arbitrary operator
,
resulting in the generalized value iteration (GVI) algorithm
(cid:78)
with the following update rule:

Algorithm 2 GVI algorithm
Input: initial ˆQ(s, a)
repeat
diff
←
for each s

s
∀

do

0

a

∈ S ∀

∈ A

and δ

+

∈ R

do
∈ A
ˆQ(s, a)

∈ S
for each a
Qcopy ←
ˆQ(s, a)
←
+ γ
(cid:80)
P
max

diff
end for

←

s(cid:48)∈S R
(s, a, s(cid:48))
diff,

(cid:8)

(s, a, s(cid:48))

ˆQ(s(cid:48), .)

Qcopy −
(cid:78)

|

ˆQ(s, a)
|

(cid:9)

end for
until diff < δ

ˆQ(s, a)

Crucially, convergence of GVI to a unique ﬁxed point
follows if operator
is a non-expansion with respect to
the inﬁnity norm:

(cid:78)
ˆQ(cid:48)(s, a)
ˆQ(cid:48)(s, a)
a
(cid:12)
(cid:12)
(cid:12)
(cid:79)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
for any ˆQ, ˆQ(cid:48) and s. As mentioned earlier, the max
operator is known to be a non-expansion, as illustrated in
Figure 3. mean and eps(cid:15) operators are also non-expansions.
Therefore, each of these operators can play the role of
in
GVI, resulting in convergence to the corresponding unique

ˆQ(s, a)
(cid:12)
(cid:12)
(cid:12)

max
a

a
(cid:79)

−

−

≤

,

(cid:78)

Figure 4. Fixed points of GVI under boltzβ for varying β. Two
distinct ﬁxed points (red and blue) co-exist for a range of β.

ﬁxed point. However, the Boltzmann softmax operator,
boltzβ, is not a non-expansion (Littman, 1996). Note that
we can relate GVI to SARSA by observing that SARSA’s
update is a stochastic implementation of GVI’s update.
Under a Boltzmann softmax policy π, the target of the
(expected) SARSA update is the following:

r + γ ˆQ(s(cid:48), a(cid:48))

s, a

=

E
π

(cid:2)

(s, a) + γ

R

(s, a, s(cid:48))

(cid:3)
P

(cid:12)
(cid:12)
(cid:88)s(cid:48)∈S

(cid:88)a(cid:48)∈A

π(a(cid:48)

s(cid:48)) ˆQ(s(cid:48), a(cid:48))
|

.

boltzβ

ˆQ(s(cid:48),·)
(cid:123)(cid:122)
(cid:0)
(cid:1)
= boltzβ.

(cid:124)

(cid:125)

4. Boltzmann Has Multiple Fixed Points

(cid:78)

Although it has been known for a long time that the
Boltzmann operator is not a non-expansion (Littman,
1996), we are not aware of a published example of an
MDP for which two distinct ﬁxed points exist. The MDP
presented in Figure 1 is the ﬁrst example where, as shown
in Figure 4, GVI under boltzβ has two distinct ﬁxed points.
We also show,
in Figure 5, a vector ﬁeld visualizing
GVI updates under boltzβ=16.55. The updates can move
the current estimates farther from the ﬁxed points. The
behavior of SARSA (Figure 2) results from the algorithm
stochastically bouncing back and forth between the two
ﬁxed points. When the learning algorithm performs a
sequence of noisy updates, it moves from a ﬁxed point to
the other. As we will show later, planning will also progress
extremely slowly near the ﬁxed points. The lack of the
non-expansion property leads to multiple ﬁxed points and
ultimately a misbehavior in learning and planning.

5. Mellowmax and its Properties

We advocate for an alternative softmax operator deﬁned as
follows:

mmω(X) =

log( 1
n

n
i=1 eωxi)
ω
(cid:80)

,

which can be viewed as a particular instantiation of the
quasi-arithmetic mean (Beliakov et al., 2016). It can also

   maxaQ1(s,a) maxaQ2(s,a)   maxa   Q1(s,a) Q2(s,a)   55An Alternative Softmax Operator for Reinforcement Learning

=

=

eω∆i∗

n
i=1 eωyi

log

n
i=1 eωyi
(cid:80)
log(eω∆i∗ )/ω
=
(cid:80)

/ω

∆i∗

(cid:12)
= max
(cid:12)

i

xi −

yi

,

allowing us to conclude that mellowmax is a non-expansion
under the inﬁnity norm.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

5.2. Maximization

Mellowmax includes parameter settings that allow for
maximization (Property 1) as well as for minimization. In
particular, as ω goes to inﬁnity, mmω acts like max.

Let m = max(X) and let W =
. Note that W
1, . . . , n
{
values (“winners”) in X. Then:

}}|

≥

∈
1 is the number of maximum

|{

i
xi = m
|

lim
ω→∞

mmω(X) = lim
ω→∞

= lim
ω→∞

= lim
ω→∞

= lim
ω→∞

log( 1
n

log( 1

log( 1

n
i=1 eωxi)
ω
(cid:80)
n eωm

n

i=1 eω(xi−m))
ω
(cid:80)
n eωmW )
ω

log(eωm)

log(n) + log(W )

−

ω

log(n) + log(W )
ω

= m + lim
ω→∞

−

= m = max(X) .

is,

That
the operator acts more and more like pure
maximization as the value of ω is increased. Conversely,
as ω goes to

, the operator approaches the minimum.

−∞

5.3. Derivatives

We can take the derivative of mellowmax with respect to
each one of the arguments xi and for any non-zero ω:

∂mmω(X)
∂xi

=

eωxi
n
i=1 eωxi ≥

0 .

Note that the operator is non-decreasing in each component
of X.

(cid:80)

Moreover, we can take the derivative of mellowmax with
n
respect to ω. We deﬁne nω(X) = log( 1
i=1 eωxi) and
n
dω(X) = ω. Then:

∂nω(X)
∂ω

=

n
i=1 xieωxi
n
i=1 eωxi

and so:

(cid:80)
(cid:80)

(cid:80)
∂dω(X)
∂ω

and

= 1 ,

∂mmω(X)
∂ω

=

∂nω(X)

∂ω dω(X)

nω(X) ∂dω(X)

∂ω

,

−
dω(X)2

ensuring differentiablity of the operator (Property 3).

Figure 5. A vector ﬁeld showing GVI updates under boltzβ=16.55.
Fixed points are marked in black. For some points, such as the
large blue point, updates can move the current estimates farther
from the ﬁxed points. Also, for points that lie in between the two
ﬁxed-points, progress is extremely slow.

be derived from information theoretical principles as a way
of regularizing policies with a cost function deﬁned by KL
divergence (Todorov, 2006; Rubin et al., 2012; Fox et al.,
2016). Note that the operator has previously been utilized
in other areas, such as power engineering (Safak, 1993).

We show that mmω, which we refer to as mellowmax, has
the desired properties and that it compares quite favorably
to boltzβ in practice.

5.1. Mellowmax is a Non-Expansion

We prove that mmω is a non-expansion (Property 2), and
therefore, GVI and SARSA under mmω are guaranteed to
converge to a unique ﬁxed point.

yi for i

Let X = x1, . . . , xn and Y = y1, . . . , yn be two vectors
of values. Let ∆i = xi −
be the
difference of the ith components of the two vectors. Also,
let i∗ be the index with the maximum component-wise
difference, i∗ = argmaxi ∆i. For simplicity, we assume
that i∗ is unique and ω > 0. Also, without loss of
generality, we assume that xi∗

0. It follows that:

1, . . . , n

∈ {

yi∗

}

−

≥

(cid:12)
eωxi)/ω
(cid:12)

log(

−

1
n

eωyi)/ω

n

i=1
(cid:88)

(cid:12)
(cid:12)

mmω(X)

mmω(Y)
n

−

(cid:12)
(cid:12)

=

log(

1
n

1
n
1
n

i=1
(cid:88)
n
i=1 eωxi
n
i=1 eωyi

(cid:80)
n
i=1 eω
(cid:80)

yi+∆i

/ω

(cid:12)
(cid:12)
/ω

n
i=1 eωyi
(cid:0)

(cid:1)

yi+∆i∗

(cid:80)
n
i=1 eω
(cid:80)

n
i=1 eωyi
(cid:0)

(cid:1)

(cid:12)
(cid:12)
/ω

(cid:12)
(cid:12)

=

log

=

log

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤

log

(cid:80)

(cid:80)

An Alternative Softmax Operator for Reinforcement Learning

5.4. Averaging

multipliers. Here, the Lagrangian is:

Because of the division by ω in the deﬁnition of mmω,
the parameter ω cannot be set to zero. However, we can
examine the behavior of mmω as ω approaches zero and
show that the operator computes an average in the limit.

Since both the numerator and denominator go to zero as ω
goes to zero, we will use L’Hˆopital’s rule and the derivative
given in the previous section to derive the value in the limit:

(cid:88)a∈A
s)
π(a
|

−

1

L(π, λ1, λ2) =

λ1

−

λ2

−

(cid:0) (cid:88)a∈A

(cid:16) (cid:88)a∈A

π(a
|

π(a

s) log
|

s)
π(a
|

(cid:0)

(cid:1)

s) ˆQ(s, a)

mmω

ˆQ(s, .)

.

(cid:1)

−

(cid:0)

(cid:1)(cid:17)

lim
ω→0

mmω(X)

=

L’Hˆopital
=

n
i=1 eωxi)
ω
(cid:80)
n
i=1 xieωxi
n
i=1 eωxi

log( 1
n

lim
ω→0

lim
ω→0

1
n

1
n

i=1
(cid:88)

1
(cid:80)
n
(cid:80)

n

=

xi = mean(X) .

That is, as ω gets closer to zero, mmω(X) approaches the
mean of the values in X.

6. Maximum Entropy Mellowmax Policy

As described, mmω computes a value for a list of
numbers somewhere between its minimum and maximum.
However, it is often useful to actually provide a probability
distribution over the actions such that (1) a non-zero
probability mass is assigned to each action, and (2) the
resulting expected value equals the computed value. Such
a probability distribution can then be used for action
selection in algorithms such as SARSA.

In this section, we address the problem of identifying
such a probability distribution as a maximum entropy
problem—over all distributions that satisfy the properties
above,
information
entropy (Cover & Thomas, 2006; Peters et al., 2010).
We formally deﬁne the maximum entropy mellowmax
policy of a state s as:

that maximizes

pick the one

πmm(s) = argmin

subject to

s) log
π(a
|

s)

π(a
|

(2)

(cid:0)

(cid:88)a∈A
s) ˆQ(s, a) = mmω( ˆQ(s, .))
a∈A π(a
|
0
s)
|
≥
s) = 1 .
a∈A π(a
|

(cid:1)

π

π(a
(cid:80)

(cid:80)

(cid:110)

Note that this optimization problem is convex and can be
solved reliably using any numerical convex optimization
library.

Taking the partial derivative of the Lagrangian with respect
s) and setting them to zero, we obtain:
to each π(a
|

∂L
∂π(a
|

s)

= log

π(a
|

s)

+1

λ1−

−

λ2 ˆQ(s, a) = 0

a

∀

∈ A

.

(cid:0)

(cid:1)

|A|

These
equations,
constraints in (2), form
+ 2 variables π(a
s)
|A|
|
multipliers λ1 and λ2.

together with the two linear
+ 2 equations to constrain the
|A|
and the two Lagrangian
a
∀

∈ A

Solving this system of equations, the probability of taking
an action under the maximum entropy mellowmax policy
has the form:

πmm(a
|

s) =

eβ ˆQ(s,a)
a∈A eβ ˆQ(s,a)

a

∀

∈ A

,

where β is a value for which:

(cid:80)

−

ˆQ(s,a)−mmω ˆQ(s,.)

eβ

ˆQ(s, a)

mmω ˆQ(s, .)

= 0 .

(cid:1)

(cid:0)

(cid:1)(cid:0)

→ ∞

→ −∞

(cid:88)a∈A
The argument for the existence of a unique root is simple.
As β
the term corresponding to the best action
dominates, and so, the function is positive. Conversely,
as β
the term corresponding to the action with
lowest utility dominates, and so the function is negative.
Finally, by taking the derivative, it is clear that the function
is monotonically increasing, allowing us to conclude that
there exists only a single root. Therefore, we can ﬁnd β
easily using any root-ﬁnding algorithm. In particular, we
use Brent’s method (Brent, 2013) available in the Numpy
library of Python.

This policy has the same form as Boltzmann softmax, but
with a parameter β whose value depends indirectly on
ω. This mathematical form arose not from the structure
of mmω, but from maximizing the entropy. One way to
view the use of the mellowmax operator, then, is as a form
of Boltzmann policy with a temperature parameter chosen
adaptively in each state to ensure that the non-expansion
property holds.

One way of ﬁnding the solution, which leads to an
interesting policy form, is to use the method of Lagrange

Finally, note that the SARSA update under the maximum
entropy mellowmax policy could be thought of as a

An Alternative Softmax Operator for Reinforcement Learning

Figure 7. Number of iterations before termination of GVI on the
example MDP. GVI under mmω outperforms the alternatives.

experiment is a policy gradient experiment where a deep
neural network, with a softmax output layer, is used to
directly represent the policy.

Figure 6. GVI updates under mmω=16.55. The ﬁxed point is
unique, and all updates move quickly toward the ﬁxed point.

stochastic implementation of the GVI update under the
mmω operator:

7.1. Random MDPs

E
πmm

r + γ ˆQ(s(cid:48), a(cid:48))

s, a

=

(s, a, s(cid:48)) + γ

(s, a, s(cid:48))

(cid:12)
(cid:12)

(cid:3)
P

πmm(a(cid:48)

s(cid:48)) ˆQ(s(cid:48), a(cid:48))

|

(cid:2)
(cid:88)s(cid:48)∈S

R

(cid:88)a(cid:48)∈A

(cid:124)

mmω

ˆQ(s(cid:48),.)
(cid:123)(cid:122)
(cid:0)

(cid:1)

due to the ﬁrst constraint of the convex optimization
problem (2). Because mellowmax is a non-expansion,
SARSA with the maximum entropy mellowmax policy is
guaranteed to converge to a unique ﬁxed point. Note also
that, similar to other variants of SARSA, the algorithm
simply bootstraps using the value of the next state while
implementing the new policy.

7. Experiments on MDPs

We observed that in practice computing mellowmax can
yield overﬂow if the exponentiated values are large. In this
case, we can safely shift the values by a constant before
exponentiating them due to the following equality:

log( 1
n

n
i=1 eωxi)
ω
(cid:80)

log( 1
n

= c +

n

i=1 eω(xi−c))
ω

.

(cid:80)

A value of c = maxi xi usually avoids overﬂow.

We repeat the experiment from Figure 5 for mellowmax
with ω = 16.55 to get a vector ﬁeld. The result, presented
in Figure 6, show a rapid and steady convergence towards
the unique ﬁxed point. As a result, GVI under mmω can
terminate signiﬁcantly faster than GVI under boltzβ, as
illustrated in Figure 7.

three additional experiments.

The ﬁrst
We present
investigates the behavior of GVI with the
experiment
softmax operators on randomly generated MDPs. The
second experiment evaluates the softmax policies when
used in SARSA with a tabular representation. The last

(cid:3)

(cid:125)

|S|

from

from

2, 3, ..., 10
}
{

The example in Figure 1 was created carefully by hand. It
is interesting to know whether such examples are likely to
be encountered naturally. To this end, we constructed 200
and
MDPs as follows: We sampled
uniformly at random. We initialized
2, 3, 4, 5
|A|
}
{
the transition probabilities by sampling uniformly from
[0, .01]. We then added to each entry, with probability 0.5,
Gaussian noise with mean 1 and variance 0.1. We next
added, with probability 0.1, Gaussian noise with mean 100
and variance 1. Finally, we normalized the raw values to
ensure that we get a transition matrix. We did a similar
process for rewards, with the difference that we divided
each entry by the maximum entry and multiplied by 0.5
to ensure that Rmax = 0.5 .

We measured the failure rate of GVI under boltzβ and
mmω by stopping GVI when it did not terminate in 1000
iterations. We also computed the average number of
iterations needed before termination. A summary of results
is presented in the table below. Mellowmax outperforms
Boltzmann based on the three measures provided below.

no

MDPs,
terminate
8 of 200
0

MDPs, > 1
ﬁxed points
3 of 200
0

average
iterations
231.65
201.32

boltzβ
mmω

7.2. Multi-passenger Taxi Domain

We evaluated SARSA on the multi-passenger taxi domain
introduced by Dearden et al. (1998). (See Figure 8.)

One challenging aspect of this domain is that it admits
many locally optimal policies.
Exploration needs
to be set carefully to avoid either over-exploring
or under-exploring the state space.
Note also that
Boltzmann softmax performs
remarkably well on
outperforming sophisticated Bayesian
this domain,

An Alternative Softmax Operator for Reinforcement Learning

Figure 8. Multi-passenger taxi domain. The discount rate γ is
0.99. Reward is +1 for delivering one passenger, +3 for two
passengers, and +15 for three passengers. Reward is zero for all
the other transitions. Here F , S, and D denote passengers, start
state, and destination respectively.

reinforcement-learning algorithms (Dearden et al., 1998).
As shown in Figure 9, SARSA with the epsilon-greedy
policy performs poorly.
In fact, in our experiment, the
algorithm rarely was able to deliver all the passengers.
However, SARSA with Boltzmann softmax and SARSA
with the maximum entropy mellowmax policy achieved
signiﬁcantly higher average reward. Maximum entropy
mellowmax policy is no worse than Boltzmann softmax,
here, suggesting that the greater stability does not come at
the expense of less effective exploration.

7.3. Lunar Lander Domain

In this section, we evaluate the use of the maximum entropy
mellowmax policy in the context of a policy-gradient
algorithm.
Speciﬁcally, we represent a policy by a
neural network (discussed below) that maps from states
to probabilities over actions. A common choice for the
activation function of the last
layer is the Boltzmann
softmax policy. In contrast, we can use maximum entropy
mellowmax policy, presented in Section 6, by treating the
inputs of the activation function as ˆQ values.

We used the lunar lander domain, from OpenAI Gym
(Brockman et al., 2016) as our benchmark. A screenshot
of the domain is presented in Figure 10. This domain has
a continuous state space with 8 dimensions, namely x-y
coordinates, x-y velocities, angle and angular velocities,
and leg-touchdown sensors. There are 4 discrete actions to
control 3 engines. The reward is +100 for a safe landing in
100 for a crash. There is a small
the designated area, and
shaping reward for approaching the landing area. Using the
engines results in a negative reward. An episode ﬁnishes
when the spacecraft crashes or lands. Solving the domain
is deﬁned as maintaining mean episode return higher than
200 in 100 consecutive episodes.

−

The policy in our experiment is represented by a neural
network with a hidden layer comprised of 16 units with
RELU activation functions, followed by a second layer
with 16 units and softmax activation functions. We used
REINFORCE to train the network. A batch episode size

Figure 9. Comparison on the multi-passenger
taxi domain.
Results are shown for different values of (cid:15), β, and ω. For each
setting, the learning rate is optimized. Results are averaged over
25 independent runs, each consisting of 300000 time steps.

Figure 10. A screenshot of the lunar lander domain.

of 10 was used, as we had stability issues with smaller
episode batch sizes. We used the Adam algorithm (Kingma
& Ba, 2014) with α = 0.005 and the other parameters as
suggested by the paper. We used Keras (Chollet, 2015)
and Theano (Team et al., 2016) to implement the neural
network architecture. For each softmax policy, we present
in Figure 11 the learning curves for different values of
their free parameter. We further plot average return over
all 40000 episodes. Mellowmax outperforms Boltzmann at
its peak.

8. Related Work

Softmax operators play an important role in sequential
decision-making algorithms.

In model-free reinforcement learning, they can help strike

FFSFDAn Alternative Softmax Operator for Reinforcement Learning

(Ng & Russell, 2000),

Algorithms for inverse reinforcement learning (IRL), the
problem of inferring reward functions from observed
behavior
frequently use a
Boltzmann operator to avoid assigning zero probability
to non-optimal actions and hence assessing an observed
sequence as impossible. Such methods include Bayesian
IRL (Ramachandran & Amir, 2007), natural gradient
IRL (Neu & Szepesv´ari, 2007), and maximum likelihood
IRL (Babes et al., 2011). Given the recursive nature of
value deﬁned in these problems, mellowmax could be a
more stable and efﬁcient choice.

In linearly solvable MDPs (Todorov, 2006), an operator
similar to mellowmax emerges when using an alternative
characterization for cost of action selection in MDPs.
Inspired by this work Fox et al. (2016) introduced an
off-policy G-learning algorithm that uses the operator to
perform value-function updates.
Instead of performing
off-policy updates, we introduced a convergent variant
of SARSA with Boltzmann policy and a state-dependent
temperature parameter. This is in contrast to Fox et al.
(2016) where an epsilon greedy behavior policy is used.

9. Conclusion and Future Work

We proposed the mellowmax operator as an alternative
to the Boltzmann softmax operator. We showed that
mellowmax has several desirable properties and that it
works favorably in practice.
Arguably, mellowmax
could be used in place of Boltzmann throughout
reinforcement-learning research.

to analyze the ﬁxed point
A future direction is
reinforcement-learning, and game-playing
of planning,
algorithms when using the mellowmax operators.
In
particular, an interesting analysis could be one that bounds
the sub-optimality of the ﬁxed points found by GVI.

An important future work is to expand the scope of our
theoretical understanding to the more general function
in which the state space or the
approximation setting,
action space is large and abstraction techniques are used.
Note that the importance of non-expansion in the function
approximation case is well-established. (Gordon, 1995)

Finally, due to the convexity of mellowmax (Boyd &
Vandenberghe, 2004),
in a
gradient-based algorithm in the context of sequential
decision making.
IRL is a natural candidate given the
popularity of softmax in this setting.

is compelling to use it

it

10. Acknowledgments

The authors gratefully acknowledge the assistance of
George D. Konidaris, as well as anonymous ICML
reviewers for their outstanding feedback.

Figure 11. Comparison of Boltzmann (top) and maximum
entropy mellowmax (middle) in Lunar Lander. Mean return over
all episodes (bottom). Results are 400-run averages.

a balance between exploration (mean) and exploitation
Decision rules based on epsilon-greedy and
(max).
Boltzmann softmax, while very simple, often perform
surprisingly well in practice, even outperforming more
advanced exploration techniques (Kuleshov & Precup,
2014) that require signiﬁcant approximation for complex
domains. When learning “on policy”,
exploration
steps can (Rummery & Niranjan, 1994) and perhaps
should (John, 1994) become part of the value-estimation
process itself. On-policy algorithms like SARSA can be
made to converge to optimal behavior in the limit when the
exploration rate and the update operator is gradually moved
toward max (Singh et al., 2000). Our use of softmax in
learning updates reﬂects this point of view and shows that
the value-sensitive behavior of Boltzmann exploration can
be maintained even as updates are made stable.

Analyses of the behavior of human subjects in choice
Sometimes
experiments very frequently use softmax.
referred to in the literature as logit choice (Stahl &
Wilson, 1994),
it forms an important part of the most
accurate predictor of human decisions in normal-form
games (Wright & Leyton-Brown, 2010), quantal level-k
reasoning (QLk). Softmax-based ﬁxed points play a crucial
role in this work. As such, mellowmax could potentially
make a good replacement.

An Alternative Softmax Operator for Reinforcement Learning

References

Babes, Monica, Marivate, Vukosi N., Littman, Michael L.,
and Subramanian, Kaushik. Apprenticeship learning
In International Conference
about multiple intentions.
on Machine Learning, pp. 897–904, 2011.

Baird, Leemon and Moore, Andrew W. Gradient descent
In Advances in
for general reinforcement learning.
Neural Information Processing Systems, pp. 968–974,
1999.

Baker, Chris L, Tenenbaum,

Joshua B, and Saxe,
Rebecca R. Goal inference as inverse planning.
In
Proceedings of the 29th Annual Meeting of the Cognitive
Science Society, 2007.

Beliakov, Gleb, Sola, Humberto Bustince, and S´anchez,
A Practical Guide to Averaging

Tomasa Calvo.
Functions. Springer, 2016.

Bellman, Richard. A Markovian decision process. Journal
of Mathematics and Mechanics, 6(5):679–684, 1957.

Boyd, S.P. and Vandenberghe, L. Convex optimization.

Cambridge University Press, 2004.

Brent, Richard P. Algorithms for minimization without

derivatives. Courier Corporation, 2013.

Brockman, Greg, Cheung, Vicki, Pettersson, Ludwig,
Schneider, Jonas, Schulman, John, Tang, Jie, and
Zaremba, Wojciech. Openai gym, 2016.

Chollet, Franc¸ois. Keras. https://github.com/

fchollet/keras, 2015.

Cover, T.M. and Thomas, J.A. Elements of Information

Theory. John Wiley and Sons, 2006.

Dearden, Richard, Friedman, Nir, and Russell, Stuart.
Bayesian Q-learning. In Fifteenth National Conference
on Artiﬁcial Intelligence (AAAI), pp. 761–768, 1998.

Fox, Roy, Pakman, Ari, and Tishby, Naftali. Taming
the noise in reinforcement learning via soft updates.
the Thirty-Second Conference on
In Proceedings of
Uncertainty in Artiﬁcial Intelligence, pp. 202–211.
AUAI Press, 2016.

Gordon, Geoffrey J.

Stable function approximation
the
in dynamic programming.
twelfth international conference on machine learning,
pp. 261–268, 1995.

In Proceedings of

Gordon, Geoffrey J.

learning with
function approximation converges to a region, 2001.
Unpublished.

Reinforcement

John, George H. When the best move isn’t optimal:
In Proceedings of the
Q-learning with exploration.
Twelfth National Conference on Artiﬁcial Intelligence,
pp. 1464, Seattle, WA, 1994.

Kingma, Diederik and Ba,

Jimmy.
method for stochastic optimization.
arXiv:1412.6980, 2014.

Adam:

A
arXiv preprint

Kuleshov, Volodymyr and Precup, Doina. Algorithms
arXiv preprint

for multi-armed bandit problems.
arXiv:1402.6028, 2014.

Littman, Michael L. and Szepesv´ari, Csaba.

A
generalized reinforcement-learning model: Convergence
and applications. In Saitta, Lorenza (ed.), Proceedings
of the Thirteenth International Conference on Machine
Learning, pp. 310–318, 1996.

Littman, Michael Lederman. Algorithms for Sequential
Decision Making. PhD thesis, Department of Computer
Science, Brown University, February 1996.
Also
Technical Report CS-96-09.

Neu, Gergely and Szepesv´ari, Csaba. Apprenticeship
learning and

learning using inverse reinforcement
gradient methods. In UAI, 2007.

Ng, Andrew Y. and Russell, Stuart. Algorithms for inverse
reinforcement learning. In International Conference on
Machine Learning, pp. 663–670, 2000.

Perkins, Theodore J and Precup, Doina. A convergent form
of approximate policy iteration. In Advances in Neural
Information Processing Systems, pp. 1595–1602, 2002.

Peters, Jan, M¨ulling, Katharina, and Altun, Yasemin.
Relative entropy policy search. In AAAI. Atlanta, 2010.

Puterman,

Martin

Decision
Processes—Discrete Stochastic Dynamic Programming.
John Wiley & Sons, Inc., New York, NY, 1994.

Markov

L.

Ramachandran, Deepak and Amir, Eyal. Bayesian inverse

reinforcement learning. In IJCAI, 2007.

Rubin, Jonathan, Shamir, Ohad, and Tishby, Naftali.
In Decision
Trading value and information in mdps.
Making with Imperfect Decision Makers, pp. 57–74.
Springer, 2012.

Rummery, G. A. and Niranjan, M. On-line Q-learning
Technical Report
systems.
using connectionist
CUED/F-INFENG/TR 166, Cambridge University
Engineering Department, 1994.

Safak, Aysel.

Statistical analysis of the power sum
IEEE
of multiple correlated log-normal components.
Transactions on Vehicular Technology, 42(1):58–61,
1993.

An Alternative Softmax Operator for Reinforcement Learning

Singh, Satinder, Jaakkola, Tommi, Littman, Michael L.,
and Szepesv´ari, Csaba.
Convergence results for
single-step on-policy reinforcement-learning algorithms.
Machine Learning, 39:287–308, 2000.

Stahl, Dale O. and Wilson, Paul W.

Experimental
evidence on players’ models of other players. Journal of
Economic Behavior and Organization, 25(3):309–327,
1994.

Sutton, Richard S.

Integrated architectures for learning,
planning, and reacting based on approximating dynamic
the Seventh
programming.
International Conference on Machine Learning, pp.
216–224, Austin, TX, 1990. Morgan Kaufmann.

In Proceedings of

Sutton, Richard S. and Barto, Andrew G. Reinforcement

Learning: An Introduction. The MIT Press, 1998.

Team, The Theano Development, Al-Rfou, Rami, Alain,
Guillaume, Almahairi, Amjad, Angermueller, Christof,
Bahdanau, Dzmitry, Ballas, Nicolas, Bastien, Fr´ed´eric,
Bayer, Justin, Belikov, Anatoly, et al. Theano: A
python framework for fast computation of mathematical
expressions. arXiv preprint arXiv:1605.02688, 2016.

Thrun, Sebastian B. The role of exploration in learning
In White, David A. and Sofge, Donald A.
control.
(eds.), Handbook of Intelligent Control: Neural, Fuzzy,
and Adaptive Approaches, pp. 527–559. Van Nostrand
Reinhold, New York, NY, 1992.

Todorov, Emanuel. Linearly-solvable markov decision

problems. In NIPS, pp. 1369–1376, 2006.

Van Seijen, Harm, Van Hasselt, Hado, Whiteson, Shimon,
A theoretical and empirical
and Wiering, Marco.
In 2009 IEEE Symposium
analysis of Expected Sarsa.
on Adaptive Dynamic Programming and Reinforcement
Learning, pp. 177–184. IEEE, 2009.

Williams, Ronald J. Simple statistical gradient-following
learning.

algorithms for connectionist reinforcement
Machine Learning, 8(3):229–256, 1992.

Wright, James R. and Leyton-Brown, Kevin. Beyond
equilibrium: Predicting human behavior in normal-form
games. In AAAI, 2010.

