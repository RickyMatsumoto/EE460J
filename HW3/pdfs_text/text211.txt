Local-to-Global Bayesian Network Structure Learning

Tian Gao 1 Kshitij Fadnis 1 Murray Campbell 1

Abstract

We introduce a new local-to-global structure
learning algorithm, called graph growing struc-
ture learning (GGSL), to learn Bayesian network
(BN) structures. GGSL starts at a (random) node
and then gradually expands the learned structure
through a series of local learning steps. At each
local learning step, the proposed algorithm only
needs to revisit a subset of the learned nodes,
consisting of the local neighborhood of a tar-
get, and therefore improves on both memory and
time efÔ¨Åciency compared to traditional global
structure learning approaches. GGSL also im-
proves on the existing local-to-global learning
approaches by removing the need for conÔ¨Çict-
resolving AND-rules, and achieves better learn-
ing accuracy. We provide theoretical analysis
for the local learning step, and show that GGSL
outperforms existing algorithms on benchmark
datasets. Overall, GGSL demonstrates a novel
direction to scale up BN structure learning while
limiting accuracy loss.

1. Introduction

Bayesian networks have been used in classiÔ¨Åcation (Alif-
eris et al., 2010), feature selection (Gao et al., 2015), latent
variable discovery (Lazic et al., 2013; Gao & Ji, 2016a),
and knowledge discovery (Spirtes et al., 1999; Gao & Ji,
2015) in various domains (Ott et al., 2004). However, due
to its NP-hard nature (Chickering et al., 2012), exact BN
structure learning on directed acyclic graphs (DAG) faces
scalability issues.

In this paper, we consider a local-to-global approach to
learn the Bayesian network structure, starting from the lo-
cal graph structure of one node and then gradually ex-
panding the graph based on already learned structures.

1IBM Thomas J. Watson Research Center, Yorktown
Heights, NY 10598 USA. Correspondence to: Tian Gao
<tgao@us.ibm.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

The predominant exact score-based structure learning al-
gorithms adopt the global approach and focus on bet-
ter scoring criteria (Acid et al., 2005; Brenner & Son-
tag, 2013) or more efÔ¨Åcient search procedures (Chicker-
ing, 2002; Koivisto & Sood, 2004; Silander & Myllymaki,
2006; Jaakkola et al., 2010; Cussens, 2011; Yuan & Mal-
one, 2013) to navigate the intractable search space of pos-
sible directed acyclic graphs over all the variables present.
Despite such progress, the practical usage of these algo-
rithms is still limited when there are large numbers of vari-
ables. Many approximations (Scanagatta et al., 2015), con-
straints (de Campos et al., 2009; Chen et al., 2016), and
assumptions (Nie et al., 2014) are utilized to alleviate time
and memory complexity.

Instead of searching the entire DAG space for all the vari-
ables at the same time, the local-to-global approach lim-
its the size of the space by learning a local structure with
only a limited number of variables. These variables con-
sists of potential candidates for local structures, usually de-
Ô¨Åned by the parent-child (PC) or the Markov Blanket (MB)
(Pearl, 1988) set of a target node in a DAG. Many local
learning algorithms (Koller & Sahami, 1996; Tsamardi-
nos et al., 2003; Fu & Desmarais, 2008) iteratively query
new variables to update and learn the local structure, ei-
ther PC, MB set or both, for one speciÔ¨Åc target variable.
Many constraint-based and score-based local learning al-
gorithms have been proposed and shown to have promis-
ing performances in practice (Aliferis et al., 2010). Using
learned local structures, some prior works have proposed to
combine the local structures for the global structure. Sev-
eral works (Margaritis & Thrun, 1999; Pellet & Ellisseeff,
2008) have proposed algorithms to identify MBs of ev-
ery node in the graph Ô¨Årst, and then connect the MBs in
a maximally consistent way to learn the global structure
of a BN. Both constraint-based (Tsamardinos et al., 2006)
and score-based local-to-global structure learning methods
(Niinimaki & Parviainen, 2012) have been proposed. This
local-to-global approach has the beneÔ¨Åt of improving the
exact structure learning efÔ¨Åciency, as at each step only a
small number of variables are expected to be used, although
the accuracy has not been very competitive.

We aim to improve the accuracy of the local-to-global ap-
proach and propose a new local-to-global structure learning
algorithm. The algorithm starts the local learning at one

variable Ô¨Årst, and then iteratively applies the local learn-
ing procedure to its neighbors and so on, gradually ex-
panding the learned graph with minimal repeated learning.
GGSL efÔ¨Åciently grows local graphs to global graphs with-
out considering the traditional the AND-rule, or a consis-
tency check on learned local neighborhoods. It uses only
the necessary variables for each local learning step to learn
and resolve any possible conÔ¨Çicts among local structures,
hence improving efÔ¨Åciency over global learning algorithms
and improving accuracy over existing local-to-global algo-
rithms with the AND-rule.

Notation: We use capital letters (such as X, Y ) to represent
variables, small letters (such as x, y) to represent values
of variables, and bold letters (such as V, MB) to represent
variable sets. |V| represents the size of a set V. X ‚ä•‚ä• Y and
X ‚ä•\‚ä• Y represent independence and dependence between
X and Y , respectively.

2. Technical Preliminaries

Let V denote a set of random variables. A Bayesian Net-
work for V is represented by a pair (G, Œ∏). The network
structure G is a directed acyclic graph with nodes corre-
sponding to the random variables in V. If a directed edge
exists from node X to node Y in G, X is a parent of Y
and Y is a child of X. The parameters Œ∏ indicate the con-
ditional probability distribution of each node X ‚àà V given
its parents. Moreover, let a path between two nodes X and
Y in G be any sequence of nodes between them such that
any successive nodes are connected by a directed edge, and
no node appears in the sequence twice. A directed path
of a DAG is a path with nodes (V1, ..., Vn) such that, for
1 ‚â§ i < n, Vi is a parent of Vi+1. If there is a directed path
from X to Y , then X is an ancestor of Y and Y is a de-
scendant of X. If X and Y have a common child and they
are not adjacent, X and Y are spouses of each other. Three
nodes X, Y , and Z form a V-structure if node Y has two
incoming edges from X and Z, forming X ‚Üí Y ‚Üê Z,
and X is not adjacent to Z. Y is a collider if Y has two in-
coming edges from X and Z in a path. A path J from node
X to Y is blocked by a set of nodes Z, if any of following
holds true: 1) There is a non-collider node in J belonging
to Z. 2) There is a collider node C on J such that neither C
nor any of its descendants belong to Z. Otherwise, J from
X to Y is unblocked or active.

The Local Markov Condition (Pearl, 1988) states a node
in a BN is independent of its non-descendant nodes, given
its parents. It enables the recovery of a distribution P (in
term of independence relationships) from a known DAG
G. A DAG G and a joint distribution P are faithful to each
other if all and only the conditional independencies true in
P are entailed by G (Pearl, 1988). The faithfulness condi-
tion enables us to recover a DAG G from a distribution P

Local-to-Global BN Structure Learning

to completely characterize P.

A Markov Blanket of a target variable T , MBT , is the
minimal set of nodes conditioned on which all other nodes
are independent of T , denoted as X ‚ä•‚ä• T |MBT , ‚àÄX ‚àà
{V \ T \ MBT }. Given independently and identically dis-
tributed (i.i.d.) samples D from an unknown distribution
P, represented by a faithful but unknown DAG G0 to P,
local structure learning is to Ô¨Ånd the PC or MB of a target
node in G0. To avoid symbol confusion, we use G to repre-
sent any learned DAG and use G0 to represent the ground
truth DAG. Under the faithfulness assumption between G0
and P, the PC and MB of a target node is uniquely iden-
tiÔ¨Åable (Pearl, 1988). For example, in Figure 1a, nodes A
and D form PCB. MBB contains its parent node A, its
child D, and its spouse C. All other nodes E, F , and H
are independent of B, given MBB, due to blocked paths.

Figure 1. Sample Bayesian Network. a) Dark node B is the target
node and the shaded nodes A, C, and D are the Markov Blanket
of B. b) A learned DAG is shown, where the dashed edges can
have different orientations while still being Markov Equivalent to
the DAG in a).

Score-based structure learning algorithms rely on some
score criteria s to learn a best-Ô¨Åtting DAG G for data D.
Score s(G, D) of a BN DAG structure G measures the
goodness of Ô¨Åt of G on D. Let G be any BN structure
and G(cid:48) be the same structure as G but with an edge from
a node T to a node X. Let PaG
X be the parent set of X in
G. Score s is locally consistent if, as the size of the data
D goes to inÔ¨Ånity, the following two properties hold true:
1) if X ‚ä•\‚ä• T |PaG
X , then s(G, D) < s(G(cid:48), D), and 2) if
X ‚ä•‚ä• T |PaG
X , then s(G, D) > s(G(cid:48), D). In addition, s is
score equivalent if Markov equivalent DAGs have the same
score. s is decomposable if it is a sum of each node‚Äôs indi-
vidual score that depends on only this node and its parents.
Commonly used Bayesian score criteria, such as BDeu, are
decomposable, consistent, locally consistent (Chickering,
2002), and score equivalent (Heckerman et al., 1995). We
assume the Markov condition, faithfulness condition, and
the inÔ¨Ånite data size hold in the theoretical analysis part of

ADEBFHCADEBFHC(ùëé)(ùëè)Local-to-Global BN Structure Learning

the paper.

Lastly, one of the main concepts in the topology-based MB
algorithms is the symmetry constraint, or the AND-rule.

Lemma 1. AND-Rule. For a node X to be adjacent to T
in G, both of the following statements hold true: X must
be in the PC set of T and T must be in the PC set of X, i.e.,
X ‚àà PCG

T and T ‚àà PCG
X .

The local-to-global BN structure learning algorithms gen-
erally use the AND-rule to enforce consistency between
different learned local structures to obtain a global DAG
(Margaritis & Thrun, 1999). Local structure learning algo-
rithms also employ it to guarantee soundness (Niinimaki &
Parviainen, 2012).

3. Local-to-Global BN Structure Learning

We will Ô¨Årst introduce local BN structure learning and pro-
vide some new theoretical guarantees, then propose a novel
procedure to expand the local graph to the global graph,
including some consistency guarantee and the proposed
GGSL algorithm.

3.1. Local Structure Learning

learning approach Ô¨Årst uses

lo-
The local-to-global
cal structure learning algorithms, either constraint-based
(Tsamardinos et al., 2006) or score-based (Niinimaki &
Parviainen, 2012), to discover the PC set or the Markov
Blanket of the target. The arguably state-of-art algorithms
to Ô¨Ånd the local structure of Bayesian network use a score-
based framework (Gao & Ji, 2017), shown in Algorithm 1,
LocalLearn.

In Algorithm 1, subroutine BNStructLearn learns an
optimal DAG over a set of variables in the data, and
can use any exact global BN structure learning algorithm.
Subroutine findPC and findSpouse extract a variable
T ‚Äôs PC set (by Ô¨Ånding parent set P and children set C)
and spouse set given the adjacency matrix of a graph G.
LocalLearn Ô¨Årst sequentially learns the PC set by re-
peatedly using BNStructLearn on a set of nodes Z con-
taining the target node T , its current PC set PCT , and one
new query variable X. Then it uses a similar procedure to
learn the spouse set and update the PC set. PCG
T is guar-
anteed to contain all the true positive PC nodes of T .

Lemma 2. Preservation of True Positive PCs (Niinimaki
& Parviainen, 2012). Let G0 be the faithful DAG of distri-
bution P over V, and G be the DAG learned by exact BN
structure learning algorithms over the subset of variables
ZL ‚äÜ V at the last iteration of Step 1 of Algorithm 1. Let
T be the learned PC set of the target T in G and PC0
PCG
T
be the PC set of T in G0. Under the faithfulness and inÔ¨Ånite
data assumption, PC0

T ‚äÜ PCG
T .

Algorithm 1 LocalLearn

Input: dataset D, target node T
{step 1: Ô¨Ånd the PC set }
PCT ‚Üê ‚àÖ, O ‚Üê V \ {T };
while O is nonempty do

choose X ‚àà O, O ‚Üê O \ {X};
Z ‚Üê {T, X} ‚à™ PCT ;
G ‚Üê BNStructLearn (Z, DZ);
PCT , PT , CT ‚Üê findPC(G, T ) ;

end while
{step 2: remove false PC nodes and Ô¨Ånd spouses}
ST ‚Üê ‚àÖ, O ‚Üê V \ PCT ;
while O is nonempty do

choose X ‚àà O, O ‚Üê O \ {X};
Z ‚Üê {T, X} ‚à™ PCT ‚à™ ST ;
G ‚Üê BNStructLearn (Z, DZ);
PCT , PT , CT ‚Üê findPC(G, T ) ;
ST ‚Üê findSpouse(G, T ) ;

end while
Return: MB ‚Üê PT ‚à™ CT ‚à™ ST ;

However, PCG
T may contain false positive PC nodes (Al-
iferis et al., 2010), as shown in Figure 1 of (Niinimaki &
Parviainen, 2012). The iterative nature of Algorithm 1 can
potentially violate the faithfulness assumption during the
learning, due to absent variables. Previous analysis (Ni-
inimaki & Parviainen, 2012; Gao & Ji, 2017) conjectured
the soundness and completeness of LocalLearn. Here
we provide a new theoretical proofs of these results in the
LocalLearn .

Lemma 3. Preservation of Dependence Relationships be-
tween T and the Learned PC Set. Let G0 be the global
faithful DAG of P for the entire variable set V, and G be
the DAG learned by exact BN structure learning algorithms
over a subset of variables of V, VG, present at the last it-
eration of Step 1 of Algorithm 1. Then in G0 every variable
in the learned PC set PCG
T is dependent of the target T,
conditioned on any subset of the ground truth PC set: i.e.,
X ‚ä•\‚ä• T |Z, ‚àÄX ‚àà PCG
T , ‚àÄZ ‚äÜ PC0

T \ {X}.

Proof. If X ‚àà PC0
T , then the lemma holds automati-
cally. Else if X (cid:54)‚àà PC0
T , assuming ‚àÉX ‚àà S such that
X ‚ä•‚ä• T |S \ X, where S = VG \ {T }. Then, one of the
following two cases must hold in G: T ‚Üí X or X ‚Üí T .
If T ‚Üí X, since each node in ChildrenX ‚à™ SpousesX
is either 1) not saved during the iterative procedure, or 2)
saved as a node of PCG
T , in which case it forms a fully
connected subgraph with X and T and can be changed to a
ParentX . Then, P (X|S \ X) = P (X|PaG
X ) by MB deÔ¨Å-
nition. Since P (X|S\X, T ) = P (X|S\X) by assumption
X ‚ä•‚ä• T |S \ X, then T (cid:54)‚àà PaX since {S \ X} must con-
tain PaG
X . Then by local consistency removing the edge
T ‚Üí X will increase the score, which contradicts the as-

Local-to-Global BN Structure Learning

sumptions. If X ‚Üí T , since P (T |X, S\X) = P (T |S\X)
by assumption, then using a similar argument, X (cid:54)‚àà PaG
T
and removing the edge X ‚Üí T will increase the score,
which contradicts the assumption. Hence, X ‚ä•\‚ä• T |S \ X.
Since Z ‚äÜ PC0

T \ {X} ‚äÜ S \ {X}, the lemma holds.

Proof. Step 1 of Algorithm 1 returns all of the true positive
parents, children, and some descendants, if they exist, by
Lemma 4 and Lemma 5. The tasks left are to add the true
positive spouses and remove false positive PC nodes, i.e.
the non-child descendants, from PCT .

Lemma 3 shows that the false PC nodes consist of only
descendents of T , which is the same as Lemma 3 of (Gao
& Ji, 2017) but without conjectured results:
Lemma 4. PC False Positive Identity. Let PCG
T be the
learned PC set of the target T in the learned graph G from
Step 1 of Algorithm 1, and PC0
T be the ground truth PC
set in G0. The false positives F in PCG
T consist of only
T , i.e., F ‚äÜ Des0
descendants of T in G0, denoted as Des0
T ,
F = PCG

T \ PC0
T .

T consists of the entire PC0

Proof. By Lemma 2, PCG
T and
some false positives F. We show F ‚äÜ Des0
T . According
to Lemma 3, a node X ‚àà F is conditionally dependent of
T given any Z ‚äÜ PC0
T \ {X} in G0. In the last iteration
of Step 1, PCG
T must contain the true positive parents of T
T ‚äÜ PC0
T , as Pa0
Pa0
T by Lemma 2. Therefore,
X ‚ä•\‚ä• T |Pa0
T . However, by the Markov condition, all the
non-descendant nodes X are independent of T given Pa0
T
in G0. Hence non-descendants X cannot be in PCG
T by the
last iteration. Thus, F ‚äÜ Des0
T .

T ‚äÜ PCG

For the sake of complete discussion, we include the follow-
ing property, showing the existence of unblocked paths:

Lemma 5. Coexistence Between Descendants and
Spouses in Score-Based PC Search. In the learned G from
Step 1 of Algorithm 1, the only false positives F in PCG
T be-
long to the descendants of T , Des0
T, due to an unblocked
path between T and its descendants via a V-structure T ‚Üí
Child ‚Üê Spouse in G0.

T must be true. Since PC0

Proof. Lemma 4 shows the Ô¨Årst part of the lemma is true,
and we just need to show the second part holds. Assuming
false positive PC nodes F exist, let X ‚àà F ‚äÜ Des0
T , then
Lemma 4 shows that X ‚ä•\‚ä• T |Z, ‚àÄZ ‚äÜ PC0
T . For F to
exist, X ‚ä•\‚ä• T |PC0
T must be
present in all paths from T to X in G0, in the last iteration
of the score-based PC search the dependence between T
and X occurs only if PC0
T unblocks some paths from T to
X. This can only happen when there is a collider node in
PC0
T is through
an unblocked path that contains a V-structure T ‚Üí child ‚Üê
spouse in G0.

T . Hence, the only way X can exist in PCG

First, we show LocalLearn will Ô¨Ånd all of the true pos-
itive spouses. In Step 2, LocalLearn learns a structure
with the target, the current PC set, the current spouse set,
and one query variable X ‚àà O = V \ PCT . At each
step, PCT is a set that has been found to be dependent
or conditionally dependent of the target. Since PCT in-
cludes and will always include the true positive PC set by
Lemma 2, true positive spouses are conditionally depen-
dent of T given PCT . Following a similar logic as Lemma
2, S0
T must directly connect to the true positive children of
T . Because Step 2 of Algorithm 1 queries every variable
in V and S0
T must be dependent of nonadjacent T given
PC0
T , to capture the correct independence relationships
with the locally consistent score, S0
T must be included in
ST .

Secondly, we show false positive PCs and spouses will be
removed. Since all the true positive PC nodes and spouses
are present in the last iteration, the false positive PC nodes
(the non-MB descendants by Lemma 4), if exist, should
be adjacent to the true positive PC nodes and spouses in
G. However, the DAG obtained from G by removing the
edge from false positives PC to T would score higher by
capturing the same independence relationships (i.e., the de-
scendants are dependent of children and spouse nodes of
T and independent of T given the true positive MB set)
and the dependence relationships between the true positive
MB set and T , but with fewer edges. Therefore, all false
positive PC nodes will be removed from PCT . Similarly,
since all the true positive PC nodes and spouses are present
in the last iteration, if there exist false positive spouses
F ‚äÜ SG
T , F would be parents to some true positive
children nodes C0
T in G and F ‚ä•\‚ä• T |C. If so, F should be
adjacent to S0
T as well in G as every path between C0 and
F must go through S0
T . However, the DAG obtained from
G by removing the edge from C to F would score higher
than G by capturing the same independence relationships
but with less edges. Therefore, there will not be any false
positive spouse nodes.

T \ S0

Thus, PCT and S will contain all and only the true PC and
spouses. Their union contains all and only the MB nodes.
Therefore, LocalLearn is sound and complete.

We show the consistency results of LocalLearn:

Theorem 1. Under the inÔ¨Ånite data and faithfulness as-
sumption, LocalLearn Ô¨Ånds all and only the Markov
Blanket nodes of the target node.

Note that the learned parent set P and children set C from
LocalLearn themselves may not be sound or complete,
due to Markov equivalence, even though their joint set is
sound and complete.

3.2. Local-to-Global Learning

Algorithm 2 Graph Growing Structure Learning

Local-to-Global BN Structure Learning

Using the local structures, one alternative approach to
global structure learning is to learn the local structures of
all nodes and then combine them to construct the global
graph structure from the existing local graphs. Many al-
gorithms (Margaritis & Thrun, 1999; Niinimaki & Parvi-
ainen, 2012) Ô¨Årst use the AND-rule to resolve the potential
conÔ¨Çicts among different local structures between adjacent
variables, and then use the Meek rules (Meek, 1995) to ob-
tain the Ô¨Ånal DAG. The AND-rule seems arbitrary in the
learning results and can introduce errors if one of the neigh-
bors learns a wrong local graph, hence affecting the accu-
racy of the Ô¨Ånal global graph. One naive way to solve such
conÔ¨Çicts is to re-run the subroutine BNStructLearn on
the variable set containing both neighbor sets to redeter-
mine the existence of the edges. However, the procedure is
inefÔ¨Åcient as it repeats learning for local structures of ev-
ery edge after repeating for every node, relearning the same
parts of the graph multiple times.

We propose to anchor the learning at one target variable
T , and then grow the graph by gradually expanding it.
It is made possible as the LocalLearn does not re-
quire neighbors‚Äô local structure to learn T ‚Äôs neighborhood
correctly, unlike previous algorithms. We only use the
necessary variables at each iteration to expand the graph
while keeping other parts of the learned graph Ô¨Åxed. The
proposed graph growing structure learning (GGSL) algo-
rithm is shown in Algorithm 2. Starting from an empty
graph, GGSL iteratively updates the global graph by us-
ing LocalLearn to learn the local structure of one target
variable T at a time. Each local learning uses the set con-
sisting of the current local variables of the target variable
in the learned graph G and variables that are not in the lo-
cal structures of any variable. Then GGSL updates G with
learned results using updateGraph. It runs until all but
one variable is not learned and has four main steps:

Step 1: GGSL chooses one target variable T at each iter-
ation, Ô¨Årst chosen randomly and then based on query set
Q, which contains adjacent nodes of the already queried
variables in the graph G. Q can be maintained as a regular
queue (Ô¨Årst in, Ô¨Årst out). Queried variable set A keeps all
the learned T s and prevents repeated learning.

Step 2: GGSL uses LocalLearn shown in Algorithm 1
to Ô¨Ånd the local structure of the target variable over the
query set Z. This step resolves the potential edge conÔ¨Çicts
through efÔ¨Åcient learning, avoiding the simple AND-rule
used by other local-to-global structure learning algorithms
The main difference between each run of LocalLearn is
that the previous T ‚Äôs will not be considered, unless they are
in the local structure of the current target variable. Hence
the max possible variable set size decreases over iterations,
improving the memory efÔ¨Åciency.

Input: data D, size m, variable set V
Q ‚Üê ‚àÖ; A ‚Üê ‚àÖ;
G ‚Üê zeros(|V |, |V |)
repeat

if Q (cid:54)= ‚àÖ & Q[0] (cid:54)‚àà A then

T ‚Üê Q.pop(0)

else

T ‚Üê the next unqueried variable in V

end if
G ‚Üê GGSL(D, T, G, A, V)
add adjacent nodes of T in G to Q
A ‚Üê A ‚à™ T
until |S| = |V | ‚àí 1
G ‚Üê PDAG-to-DAG(G)
Return: G

Algorithm 3 GGSL Subroutine

Input: data D, target variable T , current DAG G, vari-
able set A, variable set V
PC, P, C ‚Üê findPC(G, T );
Sp ‚Üê findSpouse(G, T )
Z ‚Üê T ‚à™ PC ‚à™ Sp ‚à™ V \ {T, PC, Sp, A}
DZ ‚Üê D of the set Z
MB, P, C, Sp ‚Üê LocalLearn(DZ, T )
G ‚Üê updateGraph(G, T, P, C, Sp)
Return: G

Step 3: Subroutine updateGraph, shown in Algo-
rithm 3, performs the following check to enforce graph
consistency: First, it checks if any directed parent in the
existing G is learned as a child from the local learned chil-
dren set C. If so, it corrects the children into parents. Sec-
ondly, it checks if any directed child in the existing G is
wrongly learned as a parent from P. If so, it corrects the
parents into children. Lastly, the algorithm checks if any P
and C nodes can have a different edge direction without in-
troducing new or destroying existing V-structures; if so, all
these P and C are marked as undirected. Otherwise, they
are marked as directed edges. There checks are needed to
ensure the already-oriented edges remain unchanged, as the
earlier runs of local structure learning uses more variables
and hence are more likely to be correct. Lastly, to orient
the newly learned spouse set Sp is straightforward, due to
the deÔ¨Ånitive nature of V-structures.

Step 4: After repeating the Ô¨Årst three steps for all but one
variable, the last step of GGSL is to obtain a DAG given all
the directed and undirected edges in the graph. Applying
the conventional rules (Meek, 1995) to convert a partial di-
rected acyclic graph (PDAG) to completely directed acyclic
graph is sufÔ¨Åcient, with an option to convert to DAG if de-
sired.

Local-to-Global BN Structure Learning

Algorithm 4 updateGraph

Input: current DAG G, target T , learned parent P, child
set C, spouse set Sp
{Step 1. update the PC set}
for all C in C do

if C conÔ¨Çicts with directed edge C ‚Üí T in G then

add C into P and remove C from C

end if
end for
for all P in P do

end if
end for
if |P| ‚â• 2 then

if P conÔ¨Çicts with directed edge T ‚Üí P in G then

add P into C and remove P from P

orient P and C accordingly as a directed edge in G
else if directed edge T -A, ‚àÄA ‚àà {P ‚à™ C} cannot be
reversed without destroying existing or introducing new
V-structures then

orient T -A accordingly as a directed edge in G

else

orient T -A accordingly as a undirected edge in G

end if
{Step 2. update the spouse set}
Orient Sp and T to their children accordingly as directed
edges in G
Return: G

Lemma 6. Requirement of Post-processing . Subroutine
PDAG-to-DAG is required in a local-to-global learning
system to correctly orient DAG G to its Markov equivalent
class in Algorithm 2.

A simple example, shown in Figure 2 would justify
Lemma 6. If the target T is chosen with the following or-
der: F, E, D, C, and A, then the direction of edges C ‚àí D,
D ‚àí E, and E ‚àí F can be set in both directions and hence
are labeled as undirected edges by Algorithm 2. When
node C becomes the target, then edge C ‚àí D is known.
Other edges D ‚àí E and E ‚àí F have to be checked and cor-
rected. Without subroutine PDAG-to-DAG to propagate
the changes back, the learned DAG is not guaranteed to be
the correct completely partially directed DAG (CPDAG).

We show Algorithm 2 is sound and complete:
Theorem 2. Soundness and Completeness. Under the in-
Ô¨Ånite data and faithfulness assumption, Algorithm 2 GGSL
learns and directs all and only the correct edges in the un-
derlying DAG G0, up to the Markov equivalent class of G0.

Proof. For the Ô¨Årst target variable T1, GGSL Ô¨Ånds the cor-
rect local structure of one target variable by Theorem 1.
The updated DAG G is sound and complete for T1. Start-
ing from the second iteration i ‚â• 2, since the learned graph

Figure 2. An example BN for Lemma 6.
If the query order is
F ‚áí E ‚áí D ‚áí C ‚áí A, the edge E ‚àí F would be not
guaranteed to orient correctly until C is queried and hence the
post processing is needed.

G is shown to be correct, if any variable in the target vari-
able Ti‚Äôs MB set has been queried or saved, they must exist
in the PC and Sp variable, hence in Z of Algorithm 3.
With the rest of variables complementing the PC and Sp
variables, all the true positive PC set and spouse set of Ti
must be included in the set Z of Algorithm 3. By Theo-
rem 1 again, the local structure learned must be sound and
complete. Hence, at each iteration of Algorithm 2, the up-
dated DAG G must be sound and complete for all Tis as
well. Since the PDAG-to-DAG subroutine is also proven
to orient the correct DAG from PDAG (Meek, 1995), the
result DAG G at the end of Algorithm 2 must be in the
Markov equivalent class of G0.

3.3. Case Study

Applying Algorithm 2 to Figure 1a would result in the fol-
lowing procedure, assuming the query order for target T
is A ‚áí B ‚áí C ‚áí D ‚áí E ‚áí F . When T1 = A,
LocalLearn Ô¨Ånds B and C in the local structure of A
with the query set of all variables, with undirected edges
between them , and update G. With T2 = B, the query set
of variables contains its local structure A from G and the
rest of variables. LocalLearn Ô¨Ånds D and C as B‚Äôs lo-
cal structure, and updates G. With T3 = C, LocalLearn
Ô¨Ånds the same A and D and does not make new update of
G. With T4 = D, LocalLearn Ô¨Ånds E and update G
with directed edge D ‚àí E in G. Similarly, LocalLearn
Ô¨Ånds the directed edge E ‚àí F and H ‚àí F due to the deÔ¨Åni-
tive V-structure and update G accordingly, shown in Fig-
ure 1b, when Ti = E and F . PDAG-to-DAG takes G and
produces a possible DAG in Figure 1a.

CDEFABLocal-to-Global BN Structure Learning

3.4. Implementation Optimization

While Algorithm 2 is theoretically sound and complete,
in practice further optimizations in the algorithmic pro-
cedure can be implemented. First,
inside the iterative
LocalLearn procedure, variables in the existing PC and
Sp set of Ti should be queried Ô¨Årst. These true positive
MB set variables could potentially reduce the queried set
size to BNStructLearn at each iteration by removing
non-MB set of variables early in the process. Using this
procedure is similar to the idea behind an improved ver-
sion of LocalLearn (Gao & Ji, 2017), which is shown
to improve accuracy and reduce computational time. Sec-
ondly, using the same concept, one can also keep track of
which variables are removed from Ti‚Äôs local structure after
adding each queried variable X for target Ti‚Äôs local learn-
ing procedure, hence forming a separation set, or sepset, of
X from T . Querying variables from the sepset Ô¨Årst when
X becomes the target could also reduce the potential query
set variable size to LocalLearn, as sepset variables are
more likely to be adjacent variables of X.

3.5. Complexity and Performance Discussion

The exact global learning complexity varies depending
on the algorithm, but is exponential in the worst case.
For example, Dynamic programming approaches (Silan-
der & Myllymaki, 2006) cost O(N 22N ), where N is the
total number of variables present. The most expensive
step of local-to-global approach is each iteration of local
learning using LocalLearn, which costs O(N 32N ) us-
ing the same dynamic programming approach. Repeating
for all the variable present, the local-to-global approach
takes O(N 42N ) in the worst case (Niinimaki & Parvi-
ainen, 2012; Gao & Ji, 2017). As one can see, in the case
where the local structure of one target variable includes
all other variable (such as in the Naive Bayesian model),
the local-to-global approach would match the complexity
of the global approach. However, by the iterative nature
of the learning and the fact that the number of query vari-
ables decreases at later stages of learning, the expected run-
ning time is much lower for the local-to-global learning
approach. If we assume a uniform distribution on the lo-
cal neighbor size of each node in a network of N nodes,
then the expected time complexity of the proposed GGSL
approach is O((cid:80)N
i=1 i32i) = O(N 32N ). If we assume l
to be the maximum size of local neighbors, then the aver-
age complexity would be O(l32l), which can lead to a big
performance gain O(2N ‚àíl).

Our algorithm is a score-based algorithm, as it can use
any one of existing score-based optimal learner as the
BNStructLearn subroutine. Theoretical optimality of
the proposed algorithm holds only under standard assump-
In real datasets, when the faithfulness assumption
tions.

is violated or estimated probabilistic distributions are esti-
mated incorrectly due to insufÔ¨Åcient data, the performances
of all the algorithms (global and local) are not guaran-
teed. During the learning procedure of the GGSL algo-
rithm, even with a smaller query set of variables, the in-
formation about edge existence and orientation with sufÔ¨Å-
cient data does not decrease compared to the global learn-
ing methods, if the local variables around each edge are
all present. Hence, reducing the query set size would not
affect performance with sufÔ¨Åcient data, although the infor-
mation loss does happen in practice. On the other hand,
the estimation performance on the number of data samples
is known to be sensitive to the number of parameters. The
‚àö
standard error of estimation is œÉ/ 2
N , where œÉ is the stan-
dard deviation. Due to the smaller set of variable present,
the computation of Bayesian scores could be more accurate
in practice when the sample size is limited. The trade-off
between the information loss and estimation error varies
among different datasets.

4. Experiments

We compare the proposed algorithms with both global and
local-to-global learning methods. SpeciÔ¨Åcally, we compare
our results with global methods, Dynamic Programming
(DP) structure learning (Silander & Myllymaki, 2006),
Constrained Structure Learning (CSL) (de Campos et al.,
2009), GOBNILP (Cussens et al., 2016), and local methods
Score-based Local Learning (SLL+C) (Niinimaki & Parvi-
ainen, 2012) with three different BNStructLearn as
above (DP, CSL, and GOBNILP), denoted as SLL+C-DP,
SLL+C-CSL, and SLL+C-GOBNILP. We use the existing
implementation of DP (in MATLAB), CSL (in C), and
GOBNILP (in C), and implement our algorithms in MAT-
LAB. We test the algorithms on benchmark BN datasets
from the BN repository1, using the datasets provided from
existing works(Tsamardinos et al., 2006). We run the al-
gorithms with 1000 samples of each dataset 10 times, and
compare BDeu scores of each algorithm, along with the
standard deviation, shown in Table 1. We also compare
the algorithms on a synthetic 7-node network for DP as
BNStructLearn so algorithms can return results within
the time limit. We report the running time (the entire algo-
rithmic time, including data access, score computation and
structure search) of each algorithm2, along with the stan-
dard deviation, shown in Table 2, with the maximum run-
ning time of 24 hours. The experiments are conducted on a
machine with Intel i5-3320M 2.6GHz with 8 GB memory.
Due to memory limitation, the DP method can fail to Ô¨Ån-
ish. CSL and GOBNILP have parameters that can control

1http://www.bnlearn.com/bnrepository/
2 The time results are different from results on the GOBNILP
website, which represent the times of Ô¨Ånding the optimal structure
given already computed scores.

Local-to-Global BN Structure Learning

Table 1. Learning Scores for Different BN Structure Learning Algorithms on Different Datasets.

DATA SET

VARIABLE SIZE

DP

SLL+C-DP

GGSL-DP

DATA SET

VARIABLE SIZE

CSL

SLL+C-CSL

GGSL-CSL

7
37
20
56
60

37
20
56
60

-13854.6¬± 133
OOM
OOM
OOM
OOM

-14224.6¬± 143
-14774.1¬± 210
-13548.0¬±172
-62281.5¬±213
-38713.2¬±255

-13781.2 ¬± 133
-11557.8¬± 379
-12690.0¬± 106
-54551.6 ¬± 404
-37271.2¬± 315

-10989.8 ¬± 196
-12690.0 ¬± 104
-54375.6 ¬±111
-37407.5¬± 228

-11437.0.1¬±268
-12811.4¬±153
-58138.1¬±523
-38634.8¬± 249

-11033.4¬± 382
-12600.0¬± 106
-57794.1¬± 623
-37258.3¬± 340

DATA SET

VARIABLE SIZE

GOBNILP

SLL+C-GOB

GGSL-GOB

37
20
56
60
OOM: OUT OF MEMORY.

DNF
-12690.0 ¬± 104
DNF
DNF

-10337.1¬± 410
-12811.4¬±153
-54192.5¬± 781
-38303.0¬±402

-10575.0¬±269
-12600.0¬± 106
-53411.7¬± 844
-36950.1¬± 382

DNF: DID NOT FINISH

7BN
ALARM
CHILDREN
HAILFINDER
CHILDREN3

ALARM
CHILDREN
HAILFINDER
CHILDREN3

ALARM
CHILDREN
HAILFINDER
CHILDREN3

memory usages.

Table 2. Time Taken,
Learning Algorithms on Different Datasets.

in seconds, for Different BN Structure

Data set
DP
0¬±0
7BN
OOM
Alarm
Children
OOM
Children3 OOM
HailÔ¨Ånder OOM

SLL+C-DP
0¬±0
2975¬±192
721¬±21
39581¬±801
5083¬±279

GGSL-DP
0¬± 0
126¬± 36
29¬± 3
1119¬±369
885¬± 249

Data set

CSL

SLL+C-CSL

GGSL-CSL

Alarm
Children
Children3
HailÔ¨Ånder

22220¬±614
311 ¬±135
52077¬±3622
22640¬±42

512 ¬± 120
131¬±5
1225¬±112
1017¬±220

577¬± 25
140¬± 15
1374¬±395
950¬±141

Data set

GOBNILP

SLL+C-GOB GGSL-GOB

‚â• 24hr
73¬±2

Alarm
Children
Children3 ‚â• 24hr
HailÔ¨Ånder ‚â• 24hr

369¬± 34
63¬±2
443¬± 132
394¬±62 41

499¬±77
71¬± 1
569¬± 213
459¬± 41

OOM: Out of Memory.

As one can see from Table 1, GGSL improves the learn-
ing scores by a signiÔ¨Åcant margin over the SLL+C algo-
rithm, with different BNStructLearn in all four datasets
tested, except one case in ALARM with GOBNILP. It can
even compete with global structure learning approaches
in some cases. GGSL outperforms the global learning
methods in CHILDREN datasets with CSL and GOBNILP.
EfÔ¨Åciency-wise, from Table 2, using DP, GGSL is more ef-

Ô¨Åcient than SLL+C and can achieve one than one order of
speedup. Using CSL and GOBNILP, GGSL has more than
one order of magnitude speed-ups in 3 out of 4 datasets
when compared with global learning method. However,
GGSL‚Äôs running time is generally slower to SLL+C algo-
rithm using CSL and GOBNILP. It is faster than SLL+C
on HAILFINDER with CSL, and is slightly slower in
the other testing cases. We speculate that the difference
in speed gains across different algorithms is mainly the
code base of BNStructLearn, where the extra check-
ing Step 3 in GGSL can take proportionally longer time if
BNStructLearn is implemented in C.

5. Discussion and Conclusion

We have proposed a novel graph expanding learning algo-
rithm to learn BN structure. We strengthen the existing
local structure learning analysis, justifying its soundness
when the traditional faithfulness condition fails with ab-
sent variables, and propose a new local-to-global approach
to combine the local structures efÔ¨Åciently. Experiments
have shown that the proposed GGSL improves the accu-
racy over existing local-to-global algorithms and improves
efÔ¨Åciency over existing global algorithms, both by a signif-
icant margin. In addition, GGSL can work with any exact
score-based BN learning algorithm and achieve consistent
performance gain. The iterative nature of GGSL can have
many applications, such as online BN structure learning
with streaming data. Future work could study how GGSL
would work with constraint-based (van Beek & Hoffmann,
2015; Gao & Ji, 2016b) and approximated BN structure
learning algorithms as the BNStructLearn routines.

Local-to-Global BN Structure Learning

Acknowledgements

We thank Dennis Wei and anonymous reviewers for inspi-
ration and helpful comments.

References

Acid, Silvia, de Campos, Luis M, and Castellano, Javier G.
Learning bayesian network classiÔ¨Åers: Searching in a
space of partially directed acyclic graphs. Machine
Learning, 59(3):213‚Äì235, 2005.

Aliferis, Constantin F., Statnikov, Alexander, Tsamardinos,
Ioannis, Mani, Subramani, and Koutsoukos, Xenofon D.
Local causal and markov blanket induction for causal
discovery and feature selection for classiÔ¨Åcation part i:
Algorithms and empirical evaluation. Journal of Ma-
chine Learning Research, pp. 171‚Äì234, Jan 2010.

Brenner, Eliot and Sontag, David. Sparsityboost: A new
scoring function for learning bayesian network structure.
arXiv preprint arXiv:1309.6820, 2013.

Chen, Eunice Yuh-Jie, Shen, Yujia, Choi, Arthur, and Dar-
wiche, Adnan. Learning bayesian networks with ances-
tral constraints. In Advances in Neural Information Pro-
cessing Systems, pp. 2325‚Äì2333, 2016.

Chickering, David Maxwell. Optimal structure identiÔ¨Åca-
tion with greedy search. Journal of Machine Learning
Research, 2002.

Chickering, David Maxwell, Meek, Christopher, and Heck-
erman, David. Large-sample learning of bayesian net-
works is np-hard. CoRR, abs/1212.2468, 2012.

Cussens, James. Bayesian network learning with cutting
In Proceedings of the Twenty-Seventh Confer-
planes.
ence Annual Conference on Uncertainty in ArtiÔ¨Åcial In-
telligence (UAI-11), pp. 153‚Äì160, Corvallis, Oregon,
2011. AUAI Press.

Cussens, James, Haws, David, and Studen`y, Milan. Poly-
hedral aspects of score equivalence in bayesian network
structure learning. Mathematical Programming, pp. 1‚Äì
40, 2016.

de Campos, Cassio P., Zeng, Zhi, and Ji, Qiang. Struc-
ture learning of Bayesian networks using constraints. In
ICML ‚Äô09: Proceedings of the 26th Annual International
Conference on Machine Learning, pp. 113‚Äì120, New
York, NY, USA, 2009. ACM.

Fu, Shunkai and Desmarais, Michel C. Fast markov blan-
ket discovery algorithm via local learning within single
pass. In Proceedings of the Canadian Society for com-
putational studies of intelligence, 21st conference on Ad-
vances in artiÔ¨Åcial intelligence, Canadian AI‚Äô08, Berlin,
Heidelberg, 2008. Springer-Verlag.

Gao, Tian and Ji, Qiang. Local causal discovery of direct
causes and effects. In Advances in Neural Information
Processing Systems, pp. 2512‚Äì2520, 2015.

Gao, Tian and Ji, Qiang. Constrained local latent vari-
In Proceedings of the Twenty-Fifth In-
able discovery.
ternational Joint Conference on ArtiÔ¨Åcial Intelligence,
IJCAI‚Äô16, pp. 1490‚Äì1496. AAAI Press, 2016a.

Gao, Tian and Ji, Qiang. EfÔ¨Åcient markov blanket discov-
ery and its application. IEEE Transactions on Cybernet-
ics, pp. 1‚Äì11, 2016b.

Gao, Tian and Ji, Qiang. EfÔ¨Åcient score-based markov
blanket discovery. International Journal of Approximate
Reasoning, 80:277‚Äì293, 2017.

Gao, Tian, Wang, Ziheng, and Ji, Qiang. Structured fea-
ture selection. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 4256‚Äì4264, 2015.

Heckerman, David, Geiger, Dan, and Chickering, David M.
Learning bayesian networks: The combination of knowl-
edge and statistical data. Machine learning, 20(3):197‚Äì
243, 1995.

Jaakkola, Tommi, Sontag, David, Globerson, Amir, and
Meila, Marina. Learning bayesian network structure us-
ing lp relaxations. 2010.

Koivisto, Mikko and Sood, Kismat. Exact bayesian struc-
ture discovery in bayesian networks. The Journal of Ma-
chine Learning Research, 5:549‚Äì573, 2004.

Koller, Daphne and Sahami, Mehran. Toward optimal fea-
In ICML 1996, pp. 284‚Äì292. Morgan

ture selection.
Kaufmann, 1996.

Lazic, Nevena, Bishop, Christopher, and Winn, John.
Structural expectation propagation (sep): Bayesian
structure learning for networks with latent variables. In
ArtiÔ¨Åcial Intelligence and Statistics, pp. 379‚Äì387, 2013.

Margaritis, Dimitris and Thrun, Sebastian. Bayesian net-
In Advances
work induction via local neighborhoods.
in Neural Information Processing Systems 12, pp. 505‚Äì
511. MIT Press, 1999.

Meek, Christopher. Strong completeness and faithfulness
In Proceedings of the Eleventh
in bayesian networks.
conference on Uncertainty in artiÔ¨Åcial intelligence, pp.
411‚Äì418. Morgan Kaufmann Publishers Inc., 1995.

Nie, Siqi, Mau¬¥a, Denis D, De Campos, Cassio P, and
Ji, Qiang. Advances in learning bayesian networks of
bounded treewidth. In Advances in Neural Information
Processing Systems, pp. 2285‚Äì2293, 2014.

Local-to-Global BN Structure Learning

Niinimaki, Teppo and Parviainen, Pekka. Local structure
disocvery in bayesian network. In Proceedings of Un-
certainy in ArtiÔ¨Åcal Intelligence, Workshop on Causal
Structure Learning, pp. 634‚Äì643, 2012.

Ott, Sascha, Imoto, Seiya, and Miyano, Satoru. Finding
optimal models for small gene networks. In PaciÔ¨Åc sym-
posium on biocomputing, volume 9, pp. 557‚Äì567, 2004.

Pearl, Judea. Probabilistic reasoning in intelligent systems:
networks of plausible inference. Morgan Kaufmann Pub-
lishers, Inc., 2 edition, 1988.

Pellet, Jean-Philippe and Ellisseeff, Andre. Using markov
blankets for causal structure learning. Journal of Ma-
chine Learning Research, 2008.

Scanagatta, Mauro, de Campos, Cassio P, Corani, Giorgio,
and Zaffalon, Marco. Learning bayesian networks with
thousands of variables. In Advances in Neural Informa-
tion Processing Systems, pp. 1864‚Äì1872, 2015.

Silander, Tomi and Myllymaki, Petri. A simple approach
for Ô¨Ånding the globally optimal bayesian network struc-
ture. In Proceedings of the Twenty-Second Annual Con-
ference on Uncertainty in ArtiÔ¨Åcial Intelligence (UAI),
pp. 445‚Äì452, 2006.

Spirtes, Peter, Glymour, Clark N, and Scheines, Richard.
Computation, Causation, and Discovery. AAAI Press,
1999.

Tsamardinos,

Ioannis, Aliferis, Constantin, Statnikov,
Alexander, and Statnikov, Er. Algorithms for large
In In The 16th Inter-
scale markov blanket discovery.
national FLAIRS Conference, St, pp. 376‚Äì380. AAAI
Press, 2003.

Tsamardinos, Ioannis, Brown, LauraE., and Aliferis, Con-
stantinF. The max-min hill-climbing bayesian network
structure learning algorithm. Machine Learning, 65(1):
31‚Äì78, 2006.

van Beek, Peter and Hoffmann, Hella-Franziska. Ma-
chine learning of bayesian networks using constraint
programming. In International Conference on Principles
and Practice of Constraint Programming, pp. 429‚Äì445.
Springer, 2015.

Yuan, Changhe and Malone, Brandon. Learning optimal
bayesian networks: A shortest path perspective. 48:23‚Äì
65, 2013.

