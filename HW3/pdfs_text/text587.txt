Gradient Projection Iterative Sketch for large-scale constrained Least-squares
(Supplementary materials)

Junqi Tang 1 Mohammad Golbabaee 1 Mike E. Davies 1

1. Supplementary materials

1.1. the proof for Theorem 1

LS solution xt

(cid:63), we have:

(cid:107)rt

i+1(cid:107)2 = sup

(cid:8)vT (xi − xt

(cid:63) − η∇f (xi))(cid:9)

Proof. At ﬁrst we denote the underlying cost function of
GPIS as ft(x):

for t = 0, we have the cost function of the classical sketch
(CS):

ft(x) :=

(cid:107)Sy − SAx(cid:107)2
2,

(1)

1
2

v∈Ct∩Bd

≤ sup

v∈Ct∩Bd

= sup

v∈Ct∩Bd

= sup

v∈Ct∩Bd
≤ sup

u,v∈Ct∩Bd

≤ sup

u,v∈Bd

(cid:8)vT (xi − xt

(cid:63) − η∇f (xi)) + ηvT ∇f (xt

(cid:8)vT (xi − xt

(cid:63)) − ηvT (∇f (xi) − ∇f (xt

(cid:63))(cid:9)

(cid:63)))(cid:9)

(cid:8)vT (I − ηAT ST SA)rt

(cid:9)

i

(cid:8)vT (I − ηAT ST SA)u(cid:9) (cid:107)rt

i(cid:107)2

(cid:8)vT (I − ηAT ST SA)u(cid:9) (cid:107)rt

i(cid:107)2,

(5)

(6)

(7)

(8)

(9)

for t = 1, 2, ..., N we have the the cost function of Iterative
Hessian Sketch (IHS):

We denote:

ft(x) =

(cid:107)St+1A(x − xt)(cid:107)2

2 − mxT AT (y − Axt), (2)

1
2

and then we denote the optimal solution of ft constrained
to set K as xt

i+1(cid:107)2 = (cid:107)xt

(cid:63) and (cid:107)rt

i+1 − xt

(cid:63)(cid:107)2 have:

αt = sup
u,v∈Bd

vT (I − ηAT ST SA)u,

then by recursive subsitution we have:

(cid:107)rt

i+1(cid:107)2 ≤ αi

t(cid:107)rt

0(cid:107)2,

and suppose we run GPIHS inner loop kt time, we have:

(cid:107)rt

i+1 − xt

i+1(cid:107)2 = (cid:107)xt

(cid:63)(cid:107)2 = (cid:107)PK(xt

i − η∇f (xi)) − xt

(cid:63)(cid:107)2
(3)
then we denote cone Ct to be the smallest close cone at xt
(cid:63)
containing the set K − xt
(cid:63), again because of the distance
preservation of translation by Lemma 6.3 of (Oymak et al.,
2015), we have:

(cid:107)rt

kt+1(cid:107)2 ≤ {αt}kt (cid:107)rt

0(cid:107)2,

and we transfer it in terms of A-norm:

(cid:107)rt

kt+1(cid:107)A ≤ {αt}kt

(cid:107)rt

0(cid:107)A.

(cid:115)

L
µ

(cid:107)rt

i+1(cid:107)2 = (cid:107)PK−xt

(cid:63)

= sup

v∈Ct∩Bd

i − η∇f (xi) − xt

(xt
(cid:8)vT (xi − xt

(cid:63))(cid:107)2
(cid:63) − µ∇f (xi))(cid:9) ,

(4)

then because of the optimality condition on the constrained

1Institute of Digital Communications,
Edinbuurgh, EH9 3JL, UK. Correspondence to:
<J.Tang@ed.ac.uk>.

the University of
Junqi Tang

Supplementary materials. Proceedings of the 34 th Interna-
tional Conference on Machine Learning, Sydney, Australia, 2017.
JMLR: W&CP. Copyright 2017 by the author(s).

and,

From the main theorems of the Classical sketch (Pilanci &
Wainwright, 2015) and Iterative Hessian Sketch (Pilanci &
Wainwright, 2016) we have following relationships:

(cid:107)x0

(cid:63) − x(cid:63)(cid:107)A ≤ 2ρ0(cid:107)Ax(cid:63) − y(cid:107)2 = 2ρ0(cid:107)e(cid:107)2,

(10)

and,

(cid:63) − x(cid:63)(cid:107)A ≤ ρt(cid:107)xt
Then by triangle inequality we have:

(cid:107)xt

0 − x(cid:63)(cid:107)A.

(11)

(cid:107)x1

0 − x(cid:63)(cid:107)A ≤ (cid:107)x1

0 − x0

(cid:63)(cid:107)A + 2ρ0(cid:107)e(cid:107)2,

(12)

(cid:107)xt+1

0 − x(cid:63)(cid:107)A ≤ (cid:107)xt+1

0 − xt

(cid:63)(cid:107)A + ρt(cid:107)xt

0 − x(cid:63)(cid:107)A.

(13)

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

Then for t = 0 we can have:

(cid:107)x1

0 − x(cid:63)(cid:107)A ≤ (cid:107)x1

0 − x0
(cid:63)(cid:107)A + 2ρ0(cid:107)e(cid:107)2
(cid:115)

≤ {αt}kt

(cid:107)x0

0 − x0

(cid:63)(cid:107)A + 2ρ0(cid:107)e(cid:107)2,

then chain it. For all the sketched objective function ft(x) ,
t = 0, 1, ..., N , and any pair of vectors x, x(cid:48) ∈ K we have:

ft(x) − ft(x(cid:48))− < (cid:79)ft(x(cid:48)), x − x(cid:48) >= (cid:107)StA(x − x(cid:48))(cid:107)2
2
(23)
(cid:63), by ﬁrst order optimality condition we

(14)

If we set x(cid:48) = xt
immediately have:

for t = 1, 2, ..., N we have:

0 − x(cid:63)(cid:107)A

(cid:107)xt
≤ (cid:107)xt

(cid:63) (cid:107)A + ρt(cid:107)xt−1
0 − xt−1
(cid:115)

0 − x(cid:63)(cid:107)A

≤ {αt}kt

(cid:107)xt−1

0 − xt−1

(cid:63) (cid:107)A

L
µ
0 − x(cid:63)(cid:107)A
(cid:32)

+ ρt(cid:107)xt−1
(cid:40)

≤

{αt}kt

L
µ

L
µ

The last inequality holds because:

(cid:107)xt−1

0 − x(cid:63)

fN −1

(cid:107)A ≤ (cid:107)xt−1

0 − x(cid:63)(cid:107)A + (cid:107)xt−1
0 − x(cid:63)(cid:107)A,

≤ {1 + ρt} (cid:107)xt−1

(cid:63) − x(cid:63)(cid:107)A

Then we denote:

(15)

(16)

t = {αt}kt
ρ(cid:63)

(1 + ρt)

+ ρt

(17)

(cid:32)

(cid:115)

(cid:33)

L
µ

and do recursive substitution we can have:

(cid:115)

(cid:33)

(cid:41)

(1 + ρt)

+ ρt

(cid:107)xt−1

0 − x(cid:63)(cid:107)A,

so we have:

(cid:63)) ≥ (cid:107)StA(x − xt
ft(x) − ft(xt
(cid:107)St A(x − xt
(cid:63))
(cid:107)A(x − xt
(cid:63))(cid:107)2

(cid:107)A(x − xt

(cid:63))(cid:107)2(cid:107)2
2

(cid:63))(cid:107)2
2

inf
v∈range(A)∩Sn−1

(cid:107)Stv(cid:107)2
2

(cid:107)x − xt

(cid:63)(cid:107)2
A,

(cid:27)

=

≥

(cid:26)

(24)

(cid:107)x − xt

(cid:63)(cid:107)A ≤

(cid:112)ft(x) − ft(xt
(cid:63))
inf v∈range(A)∩Sn−1 (cid:107)Stv(cid:107)2

,

(25)

From the convergence theory in (Beck & Teboulle, 2009)
which the authors in their Remark 2.1 have stated to hold
for convex constrained sets, for GPIS inner iterates we
have:

ft(xk) − ft(xt

(cid:63)) ≤

βLR supv∈range(A)∩Sn−1 (cid:107)Stv(cid:107)2
2
2k

and for Acc-GPIS inner loop we have:

ft(xk) − ft(xt

(cid:63)) ≤

2βLR supv∈range(A)∩Sn−1 (cid:107)Stv(cid:107)2
2
(k + 1)2

,

(cid:107)xt

0 − x(cid:63)(cid:107)A ≤

ρ(cid:63)
t

(cid:107)x1

0 − x(cid:63)(cid:107)A.

(18)

(cid:41)

(cid:40) N
(cid:89)

t=1

hence for GPIS:

hence we ﬁnish the proof of Theorem 1.

1.2. The proofs for Theorem 2 and 3

Proof. From the theory of the Classical sketch and Iterative
Hessian Sketch we have following relationships:

for Acc-GPIS,

(cid:107)x0

(cid:63) − x(cid:63)(cid:107)A ≤ 2ρ0(cid:107)Ax(cid:63) − y(cid:107)2 = 2ρ0(cid:107)e(cid:107)2,

(19)

(cid:107)xt+1

0 − xt

(cid:63)(cid:107)A ≤

(cid:114)

βLσtR
2k

,

(cid:107)xt+1

0 − xt

(cid:63)(cid:107)A ≤

(cid:115)

2βLσtR
(k + 1)2 ,

and,

and,

(cid:63) − x(cid:63)(cid:107)A ≤ ρt(cid:107)xt
Then by triangle inequality we have:

(cid:107)xt

0 − x(cid:63)(cid:107)A.

(20)

Then by simply towering the inequalities we shall obtain
the desired results in Theorem 2 and 3.

(cid:107)x1

0 − x(cid:63)(cid:107)A ≤ (cid:107)x1

0 − x0

(cid:63)(cid:107)A + 2ρ0(cid:107)e(cid:107)2,

(21)

for Gaussian sketches

1.3. The proofs for quantitative bounds of αt, ρt and σt

(cid:107)xt+1

0 − x(cid:63)(cid:107)A ≤ (cid:107)xt+1

0 − xt

(cid:63)(cid:107)A + ρt(cid:107)xt

0 − x(cid:63)(cid:107)A.

(22)

To prove the results in Proposition 1, 2 and 3 we need the
following concentration lemmas as pillars:
Lemma 1. For any g ∈ Rd, we have:

The remaining task of this proof is just bound the term
(cid:107)xt+1
(cid:63)(cid:107)A for both GPIS and Acc-GPIS algorithm and

0 − xt

sup
v∈C∩Bd

vT g = max

0,

sup
u∈C∩Sd−1

(cid:26)

(cid:27)

uT g

(30)

,
(26)

(27)

(28)

(29)

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

Proof. By the deﬁnition of cone projection operator we
have:

sup
v∈C∩Bd

vT g = (cid:107)PC(g)(cid:107)2 ≥ 0

(31)

if supv∈C∩Bd vT g > 0:

Proof. This Lemma follows the result of the simpliﬁed
form of the Gordon’s Lemma [Lemma 6.7](Oymak et al.,
2015):

(cid:107)SAv(cid:107)2 ≥ (bm − W(AC ∩ Sn−1) − θ)(cid:107)Av(cid:107)2

µ(bm − W(AC ∩ Sn−1) − θ)(cid:107)v(cid:107)2

sup
v∈C∩Bd

vT g = sup

(cid:107)v(cid:107)2

v∈C∩Bd

≤ sup

u∈C∩Sd−1

uT g, (32)

vT g
(cid:107)v(cid:107)2

(cid:107)SAv(cid:107)2 ≤ (bm + W(AC ∩ Sn−1) + θ)(cid:107)Av(cid:107)2

L(bm + W(AC ∩ Sn−1) + θ)(cid:107)v(cid:107)2

and meanwhile since C ∩ Sd−1 ∈ C ∩ Bd we have:

sup
v∈C∩Bd

vT g ≥ sup

uT g,

u∈C∩Sd−1

(33)

1.3.1. THE PROOF FOR PROPOSITION 1

Proof. Let’s mark out the feasible region of the step-size η:

hence we have:

sup
v∈C∩Bd

vT g = sup

uT g,

u∈C∩Sd−1

(34)

=

vT (I − ηAT ST SA)u

√

√

≥

≤

α(η, StA)
sup
u,v∈Bd

Lemma 2. If supu,v∈C∩Bd vT M u > 0, we have:

sup
u,v∈C∩Bd

vT M u =

sup
u,v∈C∩Sd−1

vT M u

(35)

Proof. Since u, v ∈ C ∩ Bd, (cid:107)u(cid:107)2 and (cid:107)v(cid:107)2 are both less
than or equal to 1, we can have the following upper bound:

sup
u,v∈C∩Bd

vT M u =

sup
u,v∈C∩Bd

(

)(cid:107)v(cid:107)2(cid:107)u(cid:107)2

vT M u
(cid:107)v(cid:107)2(cid:107)u(cid:107)2
vT M u,

≤

sup
u,v∈C∩Sd−1

and meanwhile since C ∩ Sd−1 ∈ C ∩ Bd we have:

sup
u,v∈C∩Bd

vT M u ≥

sup
u,v∈C∩Sd−1

vT M u,

(36)

hence we have:

sup
u,v∈C∩Bd

vT M u =

sup
u,v∈C∩Sd−1

vT M u

(37)

≥ sup
v∈Bd
= sup
v∈Bd

≥ sup
v∈Bd

vT (I − ηAT ST SA)v

((cid:107)v(cid:107)2

2 − η(cid:107)SAv(cid:107)2
2)
√

((1 − ηL(bm +

d + θ − (cid:15))2)(cid:107)v(cid:107)2

2),

so if we choose a step size η ≤
sure that with probability 1 − e− (θ−(cid:15))2
α(η, StA) > 0 and the Lemma 2 become applicable:

d+θ)2 we can en-
((cid:15) > 0) we have

1
√
L(bm+

2

=

=

=

=

≤

α(η, StA)
sup
u,v∈Bd

sup
u,v∈Sd−1

vT (I − ηAT ST SA)u

vT (I − ηAT ST SA)u

1
4

[(u + v)T (I − ηAT ST SA)(u + v)

sup
u,v∈Sd−1
−(u − v)T (I − ηAT ST SA)(u − v)]

2 − η(cid:107)SA(u + v)(cid:107)2
2

[(cid:107)u + v(cid:107)2

1
sup
4
u,v∈Sd−1
−(cid:107)u − v(cid:107)2
2 + η(cid:107)SA(u − v)(cid:107)2
2]
1
4

[(1 − ηµ(bm −

sup
u,v∈Sd−1

√

√

+(ηL(bm +

d + θ)2 − 1)(cid:107)u − v(cid:107)2
2]

d − θ)2)(cid:107)u + v(cid:107)2
2

Lemma 3. If the entries of the sketching matrix S is i.i.d
drawn from Normal distribution and v ∈ C, we have:

(cid:107)SAv(cid:107)2 ≥

µ(bm − W − θ)(cid:107)v(cid:107)2,

(38)

The last line of inquality holds with probability at least 1 −
2e− θ2
2 according to Lemma 3. Then since we have set η ≤
d+θ+(cid:15))2 , and meanwhile notice the fact that (cid:107)u +

1
√

(cid:107)SAv(cid:107)2 ≤

L(bm + W + θ)(cid:107)v(cid:107)2,
√

2 . (bm =

(39)

2 Γ( m+1
2
Γ( m

2 ) ≈

)

with probability at least 1 − e− θ2
√
m, W := W(AC ∩ Sn−1))

(1 − ηµ(bm −

d − θ)2(cid:107)u + v(cid:107)2
2

√

≤ (1 − ηµ(bm −

d − θ)2)

√

L(bm+
v(cid:107)2

2 ≤ 4 we have:

α(η, StA)
1
4

sup
u,v∈Sd−1

≤

√

√

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

If we chose η =

1
√
L(bm+

d+θ)2 we have:

hence we have the following by [Lemma 6.8](Oymak et al.,
2015):

(cid:32)

α(η, StA) ≤

1 −

µ
L

(bm −
(bm +

√
√

d − θ)2
d + θ)2

(cid:33)

,

(40)

(cid:32)

vT

(cid:33)

− I

z

Then let (cid:15) → 0, we shall get the result shown in Proposition
1.

1.3.2. THE PROOF FOR PROPOSITION 2

Proof. Recall that ρt is deﬁned as:

ρ(St, A) =

m StT
supv∈AC∩Sn−1 vT ( 1
1
m (cid:107)Stv(cid:107)2
inf v∈AC∩Sn−1
2

St − I)z

,

(41)

we start by lower-bounding the denominator, by simpliﬁed
Gordon’s lemma [Lemma 6.7](Oymak et al., 2015) we di-
rectly have:

inf
v∈AC∩Sn−1

1
m

(cid:107)Sv(cid:107)2

2 ≥

(bm − W − θ)2
m

,

(42)

with probability at least (1 − e− θ2
upper bound for the numerator:

2 ).Then we move to the

(cid:32)

St

StT
m

vT

(cid:33)

− I

z

St

StT
m
St

StT
m

=

{(v + z)T (

− I)(v + z)

1
4

− (v − z)T (

− I)(v − z)}

=

1
4

{

1
m

(cid:107)St(v + z)(cid:107)2 − (cid:107)v + z(cid:107)2

+ (cid:107)v − z(cid:107)2 −

(cid:107)St(v − z)(cid:107)2},

1
m

and,

W(AC ∩ Sn−1 − z) = Eg(

sup
v∈AC∩Sn−1

gT (v − z))

St

StT
m
(cid:26) 1
m
(cid:26) 1
m
b2
m
m

(cid:26)

(

(cid:26)

(1 −

≤

+

=

+

1
4
1
4
1
4
1
4

vT

≤

≤

√

(cid:27)

(cid:27)

(bm(cid:107)v + z(cid:107)2 + W + θ)2 − (cid:107)v + z(cid:107)2
2

(bm(cid:107)v − z(cid:107)2 + W + θ)2 − (cid:107)v − z(cid:107)2
2

− 1)(cid:107)v + z(cid:107)2

2 +

b2
m
m

)(cid:107)v − z(cid:107)2

2 +

2bm(W + θ)
m
2bm(W + θ)
m

(cid:107)v + z(cid:107)2

(cid:27)

(cid:27)

(cid:107)v − z(cid:107)2

,

(45)
8 ). Note that (cid:107)v + z(cid:107)2 +
2 ≤ 4, we have:

2 + (cid:107)v − z(cid:107)2

with probability at least (1 − 8e− θ2
(cid:107)v − z(cid:107)2 ≤ 2
(cid:32)

2 and (cid:107)v + z(cid:107)2

√

(cid:33)

St

StT
m

− I

z

2bm(W + θ)
m

(cid:107)v + z(cid:107)2 + (cid:107)v − z(cid:107)2
4

+ |

− 1|

(46)

b2
m
m

2bm(W + θ)
m

+ |

− 1|

b2
m
m

thus ﬁnishes the proof.

1.3.3. THE PROOF FOR PROPOSITION 3

Proof. Recall that σt is deﬁned as:

(43)

and the lower bound:

σ(St, A) =

supv∈range(A)∩Sn−1 (cid:107)Stv(cid:107)2
2
inf v∈range(A)∩Sn−1 (cid:107)Stv(cid:107)2
2

,

(47)

by simply apply again the Gordon’s lemma [Lemma
6.7](Oymak et al., 2015), with W(ASd−1) ≤
d, we with
obtain the upper bound on the numerator:
√

√

(cid:107)Stv(cid:107)2

2 ≤ (bm +

d + θ)2,

(48)

sup
v∈range(A)∩Sn−1

inf
v∈range(A)∩Sn−1

(cid:107)Stv(cid:107)2

2 ≥ (bm −

d − θ)2,

(49)

√

both with probability at least 1 − e− θ2
2 .

1.4. Details of the implementation of algorithms and

numerical experiments

For our GPIS and Acc-GPIS algorithms, we have several
key points of implemenations:

= Eg(gT z +

sup
v∈AC∩Sn−1

vT g)

(44)

= W(AC ∩ Sn−1)

• Count sketch

As described in the main text.

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

• Line search

We implement the line-search scheme given by (Nes-
terov, 2007) and is described by Algorithm 3 for GPIS
and Acc-GPIS in our experiments with parameters
γu = 2, and γd = 2. Such choice of line-search pa-
rameters simply means: when even we ﬁnd the con-
dition ft(PK(xi − η(cid:79)ft(xi))) ≤ mL does not hold,
we shrink the step size by a factor of 2; and then at the
beginning of each iteration, we increase the step size
chosen at previous iteration by a factor of 2, then do
backtracking again. Hence our methods are able to en-
sure we use an aggressive step size safely in each iter-
ation. This is an important advantage of the sketched
gradient method since we observe that for stochas-
tic gradient such as SAGA a heuristic backtracking
method similar to Algorithm 3 may work but it will
demand a very small γd (tends to 1) otherwise SAGA
may go unstable, and an aggressive choice like our
γd = 2 is unacceptable for SAGA. (Hence we suspect
that SAGA is unlikely to be able to beneﬁt computa-
tional gains from line-search as our method does.)

• Gradient restart for Acc-GPIS

(O’Donoghue & Candes, 2015) has proposed two
heuristic adaptive restart schemes - gradient restart
and function restart for the accelerated gradient meth-
ods and have shown signiﬁcant improvements without
the need of the knowledge of the functional parame-
ters µ and L. Such restart methods are directly appli-
cable for the Acc-GPIS by nature due to its sketched
deterministic iterations. Here we choose the gradient
restart since it achieves comparable performance in
practice as function restart but cost only O(d) opera-
tions.

1.4.1. PROCEDURE TO GENERATE SYNTHETIC DATA

SETS

The procedure we used to generate a constrained least-
square problem sized n by 100 with approximately s-sparse
solution and a condition number κ strictly follows:

1) Generate a random matrix A sized n by 100 with i.i.d
entries drawn from N (0, 1).

2) Calculate A’s SVD: A = U ΣV T and replace the singu-
lar values diag(Σ)i by a sequence:

diag(Σ)i =

diag(Σ)i−1
κ 1

d

(50)

3) Generate the ”ground truth” vector xgt sized 100 by
1 randomly with only s non-zero entries in a orthongo-
nal transformed domain Φ, and calculate the l1 norm of it

Figure 1. Experimental results on the average choices of GPIS’s
step sizes given by line-search scheme (Nesterov, 2013)

Table 1. Synthetic data set for step size experiment

DATA SET

SIZE

S Φ

SYN4

(20000, 100)

-

I

(r = (cid:107)Φxgt(cid:107)1). Hence the constrained set can be described
as K = {x : (cid:107)Φx(cid:107)1 ≤ r}.

4) Generate a random error vector w with i.i.d entries such
that (cid:107)Axgt(cid:107)2
(cid:107)w(cid:107)2

= 10.

5) Set y = Axgt + w

1.4.2. EXTRA EXPERIMENT FOR STEP SIZE CHOICE

We explore the step size choices the GPIS algorithm pro-
duce through using the line-search scheme with respect to
different sparsity level of the solution. The result we shown
is the average of 50 random trials.

The result of the step-size simulation demonstrates that the
step sizes chosen on average by the line-search scheme for
the GPIS algorithm is actually related with the sparsity of
the ground truth xgt: at a regime when the xgt is sparse
enough, the step size one can achieve goes up rapidly w.r.t
the sparsity. While in our Proposition 2 we revealed that
the outerloop of GPIS/Acc-GPIS can beneﬁt from the con-
strained set, and here surprisingly we also ﬁnd out numer-
ically that the inner loop’s can also beneﬁt from the con-
strained set by aggressively choosing the large step sizes.
Such a result echos the analysis of the PGD algorithm on
constrained Least-squares with a Gaussian map A (Oymak
et al., 2015). Further experiments and theoretical analysis
of such greedy step sizes for sketched gradients and full

02468101214161820sparsity of the solution00.010.020.030.040.050.06average step size given by line-search for GPISm = 200m = 400m = 800Gradient Projection Iterative Sketch for large-scale constrained Least-squares

gradients on general maps is of great interest and will go
beyond the state of the art analysis for convex optimization.

References

Beck, Amir and Teboulle, Marc. A fast iterative shrinkage-
thresholding algorithm for
inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.
doi: 10.1137/080716542. URL http://dx.doi.
org/10.1137/080716542.

linear

Nesterov, Yurii. Gradient methods for minimizing compos-
ite objective function. Technical report, UCL, 2007.

Nesterov, Yurii. Gradient methods for minimizing com-
posite functions. Mathematical Programming, 140(1):
125–161, 2013.

O’Donoghue, Brendan and Candes, Emmanuel. Adaptive
restart for accelerated gradient schemes. Foundations of
computational mathematics, 15(3):715–732, 2015.

Oymak, Samet, Recht, Benjamin, and Soltanolkotabi,
Mahdi. Sharp time–data tradeoffs for linear inverse prob-
lems. arXiv preprint arXiv:1507.04793, 2015.

Pilanci, Mert and Wainwright, Martin J. Randomized
sketches of convex programs with sharp guarantees. In-
formation Theory, IEEE Transactions on, 61(9):5096–
5115, 2015.

Pilanci, Mert and Wainwright, Martin J. Iterative hessian
sketch: Fast and accurate solution approximation for
constrained least-squares. Journal of Machine Learning
Research, 17(53):1–38, 2016.

