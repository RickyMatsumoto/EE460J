Faster Principal Component Regression
and Stable Matrix Chebyshev Approximation

Zeyuan Allen-Zhu * 1 Yuanzhi Li * 2

Abstract
We solve principal component regression (PCR),
up to a multiplicative accuracy 1+γ, by reducing
the problem to (cid:101)O(γ−1) black-box calls of ridge
regression. Therefore, our algorithm does not re-
quire any explicit construction of the top prin-
cipal components, and is suitable for large-scale
PCR instances.
In contrast, previous result re-
quires (cid:101)O(γ−2) such black-box calls. We obtain
this result by developing a general stable recur-
rence formula for matrix Chebyshev polynomi-
als, and a degree-optimal polynomial approxima-
tion to the matrix sign function. Our techniques
may be of independent interests, especially when
designing iterative methods.

1 Introduction

In machine learning and statistics, it is often desirable to
represent a large-scale dataset in a more tractable, lower-
dimensional form, without losing too much information.
One of the most robust ways to achieve this goal is through
principal component projection (PCP):

PCP: project vectors onto the span of the top prin-

cipal components of the a matrix.

It is well-known that PCP decreases noise and increases ef-
ﬁciency in downstream tasks. One of the main applications
is principal component regression (PCR):

PCR: linear regression but restricted to the sub-
space of top principal components.

Classical algorithms for PCP or PCR rely on a principal
component analysis (PCA) solver to recover the top princi-
pal components ﬁrst; with these components available, the

*Equal contribution .

Future version of this paper shall
be found at https://arxiv.org/abs/1608.04773.
1Microsoft Reseaerch 2Princeton University. Correspondence
to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li
<yuanzhil@cs.princeton.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

tasks of PCP and PCR become trivial because the projec-
tion matrix can be constructed explicitly.

Unfortunately, PCA solvers demand a running time that
at least linearly scales with the number of top principal
components chosen for the projection. For instance, to
project a vector onto the top 1000 principal components of
a high-dimensional dataset, even the most efﬁcient Krylov-
based (Musco & Musco, 2015) or Lanczos-based (Allen-
Zhu & Li, 2016a) methods require a running time that is
proportional to 1000 × 40 = 4 × 104 times the input matrix
sparsity, if the Krylov or Lanczos method is executed for
40 iterations. This is usually computationally intractable.

1.1 Approximating PCP Without PCA

In this paper, we propose the following notion of PCP ap-
proximation. Given a data matrix A ∈ Rd(cid:48)×d (with sin-
gular values no greater than 1) and a threshold λ > 0, we
say that an algorithm solves (γ, ε)-approximate PCP if —
informally speaking and up to a multiplicative 1±ε error—
it projects (see Def. 3.1 for a formal deﬁnition)
1. eigenvector ν of A(cid:62)A with value in (cid:2)λ(1 + γ), 1(cid:3) to ν,
2. eigenvector ν of A(cid:62)A with value in (cid:2)0, λ(1 − γ)(cid:3) to (cid:126)0,
3. eigenvector ν of A(cid:62)A with value in (cid:2)λ(1 − γ), λ(1 +

γ)(cid:3) to “anywhere between (cid:126)0 and ν.”

Such a deﬁnition also extends to (γ, ε)-approximate PCR
(see Def. 3.2).

It was ﬁrst noticed by Frostig et al. (Frostig et al., 2016)
that approximate PCP and PCR be solved with a running
time independent of the number of principal components
above threshold λ. More speciﬁcally, they reduced (γ, ε)-
approximate PCP and PCR to

O(cid:0)γ−2 log(1/ε)(cid:1) black-box calls of
any ridge regression subroutine

where each call computes (A(cid:62)A + λI)−1u for some vec-
tor u.1 Our main focus of this paper is to quadratically
improve this performance and reduce PCP and PCR to

1Ridge regression is often considered as an easy-to-solve ma-
chine learning problem: using for instance SVRG (Johnson &
Zhang, 2013), one can usually solve ridge regression to an 10−8
accuracy with at most 40 passes of the data.

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

O(cid:0)γ−1 log(1/γε)(cid:1) black-box calls of
any ridge regression subroutine

where each call again computes (A(cid:62)A + λI)−1u.
Remark 1.1. Frostig et al. only showed their algorithm
satisﬁes the properties 1 and 2 of (γ, ε)-approximation
(but not
the property 3), and thus their proof was
only for matrix A with no singular value in the range
[(cid:112)λ(1 − γ), (cid:112)λ(1 + γ)]. This is known as the eigengap
assumption, which is rarely satisﬁed in practice (Musco &
Musco, 2015). In this paper, we prove our result both with
and without such eigengap assumption. Since our tech-
niques also imply the algorithm of Frostig et al. satisﬁes
property 3, throughout the paper, we say Frostig et al. solve
(γ, ε)-approximate PCP and PCR.

1.2 From PCP to Polynomial Approximation

The main technique of Frostig et al.
struct a polynomial
sgn(x) : [−1, 1] → {±1}:

is to con-
to approximate the sign function

sgn(x) :=

(cid:26) +1, x ≥ 0;
−1, x < 0.

(cid:12)g(x) − sgn(x)(cid:12)
(cid:12)
(cid:12)g(x)(cid:12)
(cid:12)

In particular, given any polynomial g(x) satisfying
(cid:12) ≤ ε ∀x ∈ [−1, −γ] ∪ [γ, 1] ,
(cid:12) ≤ 1 ∀x ∈ [−γ, γ] ,
(1.2)
the problem of (γ, ε)-approximate PCP can be reduced to
computing the matrix polynomial g(S) for S := (A(cid:62)A +
λI)−1(A(cid:62)A − λI) (cf. Fact 7.1). In other words,
• to project any vector χ ∈ Rd to top principal compo-

(1.1)

nents, we can compute g(S)χ instead; and

• to compute g(S)χ, we can reduce it to ridge regression

for each evaluation of Su for some vector u.

Remark 1.2. Since the transformation from A(cid:62)A to S is
not linear, the ﬁnal approximation to the PCP is a ratio-
nal function (as opposed to a polynomial) over A(cid:62)A. We
restrict to polynomial choices of g(·) because in this way,
the ﬁnal rational function has all the denominators being
A(cid:62)A + λI, thus reduces to ridge regressions.
Remark 1.3. The transformation from A(cid:62)A to S ensures
that all the eigenvalues of A(cid:62)A in the range (1 ± γ)λ
roughly map to the eigenvalues of S in the range [−γ, γ].

Main Challenges. There are two main challenges regard-
ing the design of polynomial g(x).

• EFFICIENCY. We wish to minimize the degree n =
deg(g(x)) because the computation of g(S)χ usually
requires n calls of ridge regression.

• STABILITY. We wish g(x) to be stable; that is, g(S)χ
must be given by a recursive formula where if we make
ε(cid:48) error in each recursion (due to error incurred from
ridge regression), the ﬁnal error of g(S)χ must be at
most ε(cid:48) × poly(d).

Remark 1.4. Efﬁcient routines such as SVRG (Johnson &
Zhang, 2013) solve ridge regression and thus compute Su
for any u ∈ Rd, with running times only logarithmically in
1/ε(cid:48). Therefore, by setting ε(cid:48) = ε/poly(d), one can blow
up the running time by a small factor O(log(d)) in order to
obtain an ε-accurate solution for g(S)χ.

The polynomial g(x) constructed by Frostig et al.
comes from truncated Taylor expansion.
It has degree
O(cid:0)γ−2 log(1/ε)(cid:1) and is stable. This γ−2 dependency lim-
its the practical performance of their proposed PCP and
PCR algorithms, especially in a high accuracy regime. At
the same time,

• the optimal degree for a polynomial to satisfy even
only (1.1) is Θ(cid:0)γ−1 log(1/ε)(cid:1) (Eremenko & Yuditskii,
2007; 2011).

Frostig et al. were unable to ﬁnd a stable polynomial
matching this optimal degree and left it as open question.2

1.3 Our Results and Main Ideas

2

We provide an efﬁcient and stable polynomial approxima-
tion to the matrix sign function that has a near-optimal de-
gree O(γ−1 log(1/γε)). At a high level, we construct a
(cid:1)−1/2
polynomial q(x) that approximately equals (cid:0) 1+κ−x
for some κ = Θ(γ2); then we set g(x) := x·q(1+κ−2x2)
which approximates sgn(x).
To construct q(x), we ﬁrst note that (cid:0) 1+κ−x
has
no singular point on [−1, 1] so we can apply Cheby-
shev approximation theory to obtain some q(x) of degree
O(γ−1 log(1/γε)) satisfying
(cid:12)
(cid:17)−1/2(cid:12)
(cid:16) 1 + κ − x
(cid:12)
(cid:12)
(cid:12)q(x) −
(cid:12) ≤ ε for every x ∈ [−1, 1]
2
This can be shown to imply (cid:12)
(cid:12)g(x) − sgn(x)(cid:12)
x ∈ [−1, −γ] ∪ [γ, 1], so (1.1) is satisﬁed.
In order to prove (1.2) , we prove a separate lemma:3
(cid:17)−1/2

(cid:12) ≤ ε for every

(cid:1)−1/2

2

.

for every x ∈ [1, 1 + κ]

.

q(x) ≤

(cid:16) 1 + κ − x
2

Note that this does not follow from standard Chebyshev
theory because Chebyshev approximation guarantees are
only with respect to x ∈ [−1, 1].

This proves the “EFFICIENCY” part of the main challenges
discussed earlier. As for the “STABILITY” part, we prove a
general theorem regarding any weighted sum of Chebyshev
polynomials applied to matrices. We provide a backward
recurrence algorithm and show that it is stable under noisy

2Using degree reduction, Frostig et al. found an explicit poly-
nomial g(x) of degree O(cid:0)γ−1 log(1/γε)(cid:1) satisfying (1.1). How-
ever, that polynomial is unstable because it is constructed mono-
mial by monomial and has exponentially large coefﬁcients in front
of each monomial. Furthermore, it is not clear if their polynomial
satisﬁes the (1.2).

3We proved a general lemma which holds for any function

whose all orders of derivatives are non-negative at x = 0.

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

computations. This may be of independent interest.

For interested readers, we compare our polynomial q(x)
with that of Frostig et al. in Figure 1.

case when the matrix-vector multiplication oracle is only
approximate (like we do in this paper), or the case when S
has eigenvalues in the range [−γ, γ].

1.4 Related Work

There are a few attempts to reduce the cost of PCA when
solving PCR, by for instance approximating the matrix
APλ where Pλ is the PCP projection matrix (Chan &
Hansen, 1990; Boutsidis & Magdon-Ismail, 2014). How-
ever, they cost a running time that linearly scales with the
number of principal components above λ.

A signiﬁcant number of papers have focused on the low-
rank case of PCA (Musco & Musco, 2015; Allen-Zhu &
Li, 2016a; 2017) and its online variant (Allen-Zhu & Li,
2016b). Unfortunately, all of these methods require a run-
ning time that scales at least linearly with respect to the
number of top principal components.

More related to this paper is work on matrix sign func-
tion, which plays an important role in control theory and
quantum chromodynamics. Several results have addressed
Krylov methods for applying the sign function in the so-
called Krylov subspace, without explicitly constructing
any approximate polynomial (van den Eshof et al., 2002;
Schilders et al., 2008). However, Krylov methods are not
(γ, ε)-approximate PCP solvers, and there is no supporting
stability theory behind them.4 Other iterative methods have
also been proposed, see Section 5 of textbook (Higham,
2008). For instance, Schur’s method is a slow one and
also requires the matrix to be explicitly given. The New-
ton’s iteration and its numerous variants (e.g. (Nakatsukasa
& Freund, 2016)) provide rational approximations to the
matrix sign function as opposed to polynomial approxima-
tions. Our result and Frostig et al. (Frostig et al., 2016) dif-
fer from these cited works, because we have only accessed
an approximate ridge regression oracle, so ensuring a poly-
nomial approximation to the sign function and ensuring its
stability are crucial.

Using matrix Chebyshev polynomials to approximate ma-
trix functions is not new. Perhaps the most celebrated ex-
ample is to approximate S−1 using polynomials on S, used
in the analysis of conjugate gradient (Shewchuk, 1994). In-
dependent from this paper,5 Han et al. (Han et al., 2016)
used Chebyshev polynomials to approximate the trace of
the matrix sign function, i.e., Tr(sgn(S)), which is simi-
lar but a different problem.6 Also, they did not study the

4We anyways have included Krylov method in our empirical
evaluation section and shall discuss its performance there, see the
full version of this paper.

5Their paper appeared online two months before us, and we

became aware of their work in March 2017.

6In particular, their degree of the Chebyshev polynomial is
O(cid:0)γ−1(log2(1/γ) + log(1/γ) log(1/ε))(cid:1) in the language of this
paper; in contrast, we have degree O(cid:0)γ−1 log(1/γε)(cid:1).

Roadmap.
In Section 2, we provide notions for this pa-
per and basics for Chebyshev polynomials. In Section 3,
we formally deﬁne approximate PCP and PCR, and reduce
PCR to PCP. In Section 4, we show a general lemma for
In Section 5, we design our
Chebyshev approximations.
polynomial approximation to sgn(x).
In Section 6, we
show how to stably compute Chebyshev polynomials. In
Section 7, we state our main theorems regarding PCP and
PCR. In Section 8, we provide empirical evaluations.

2 Preliminaries
We denote by 1[e] ∈ {0, 1} the indicator function for event
e, by (cid:107)v(cid:107) or (cid:107)v(cid:107)2 the Euclidean norm of a vector v, by M†
the Moore-Penrose pseudo-inverse of a symmetric matrix
M, and by (cid:107)M(cid:107)2 its spectral norm. We sometimes use (cid:126)v
to emphasize that v is a vector.
Given a symmetric d × d matrix M and any f : R →
R, f (M) is the matrix function applied to M, which
to Udiag{f (D1), . . . , f (Dd)}U(cid:62) if M =
is equal
Udiag{D1, . . . , Dd}U(cid:62) is its eigendecomposition.
Throughout the paper, matrix A is of dimension d(cid:48) × d.
We denote by σmax(A) the largest singular value of A.
Following the tradition of (Frostig et al., 2016) and keeping
the notations light, we assume without loss of generality
that σmax(A) ≤ 1. We are interested in PCP and PCR
problems with an eigenvalue threshold λ ∈ (0, 1).
Throughout the paper, we denote by λ1 ≥ · · · ≥ λd ≥
0 the eigenvalues of A(cid:62)A, and by ν1, . . . , νd ∈ Rd
the eigenvectors of A(cid:62)A corresponding to λ1, . . . , λd.
We denote by Pλ the projection matrix Pλ
:=
(ν1, . . . , νj)(ν1, . . . , νj)(cid:62) where j is the largest index sat-
isfying λj ≥ λ. In other words, Pλ is a projection matrix
to the eigenvectors of A(cid:62)A with eigenvalues ≥ λ.

Deﬁnition 2.1. The principal component projection (PCP)
of χ ∈ Rd at threshold λ is ξ∗ = Pλχ.
Deﬁnition 2.2. The principal component regression (PCR)
of regressand b ∈ Rd(cid:48)
at threshold λ is

x∗ = arg miny∈Rd (cid:107)APλy − b(cid:107)2
x∗ = (A(cid:62)A)†Pλ(A(cid:62)b) .

or equivalently

2.1 Ridge Regression

Deﬁnition 2.3. A black-box algorithm ApxRidge(A, λ, u)
is an ε-approximate ridge regression solver, if for every u ∈
Rd, it satisﬁes (cid:107)ApxRidge(A, λ, u)−(A(cid:62)A+λI)−1u(cid:107) ≤
ε(cid:107)u(cid:107).

Ridge regression is equivalent to solving well-conditioned
linear systems, or minimizing strongly convex and smooth
objectives f (y) := 1

2 y(cid:62)(A(cid:62)A + λI)y − u(cid:62)y.

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

(a) degree 21

(b) degree 41

(c) degree 101

Figure 1: Comparing our polynomial g(x) (orange solid curve) with that of Frostig et al. (blue dashed curve).

Remark 2.4. There is huge literature on efﬁcient algorithms
solving ridge regression. Most notably,

(1) Conjugate gradient (Shewchuk, 1994) or accelerated
gradient descent (Nesterov, 2004) gives fastest full-
gradient methods;

(2) SVRG (Johnson & Zhang, 2013) and its acceleration
Katyusha (Allen-Zhu, 2017) give the fastest stochastic-
gradient method; and

(3) NUACDM (Allen-Zhu et al., 2016) gives the fastest

coordinate-descent method.

The running time of (1) is O(nnz(A)λ−1/2 log(1/ε))
where nnz(A) is time to multiply A to any vector. The
running times of (2) and (3) depend on structural proper-
ties of A and are always faster than (1).

Because the best complexity of ridge regression depends
on the structural properties of A, following Frostig et al.,
we only compute our running time in terms of the “number
of black-box calls” to a ridge regression solver.

2.2 Chebyshev Polynomials

Deﬁnition 2.5. Chebyshev polynomials of 1st and 2nd kind
are {Tn(x)}n≥0 and {Un(x)}n≥0 where
T0(x) := 1,
Tn+1(x) := 2x · Tn(x) − Tn−1(x)
T1(x) := x,
U0(x) := 1, U1(x) := 2x, Un+1(x) := 2x · Un(x) − Un−1(x)

Fact 2.6 ((Trefethen, 2013)).
nUn−1(x) for n ≥ 1 and

It satisﬁes

d
dx Tn(x) =

∀n ≥ 0 : Tn(x) =






cos(n arccos(x)),
cosh(n arccosh(x)),
(−1)n cosh(n arccosh(−x)),

if |x| ≤ 1;
if x ≥ 1;
if x ≤ −1.

In particular, when x ≥ 1,
(cid:2)(cid:0)x −

Tn(x) =

(cid:112)

x2 − 1(cid:1)n + (cid:0)x +

(cid:112)

x2 − 1(cid:1)n(cid:3)

Un(x) =

√

1
x2 − 1

(cid:2)(cid:0)x +

(cid:112)

x2 − 1(cid:1)n+1 − (cid:0)x −

(cid:112)

x2 − 1(cid:1)n+1(cid:3)

Deﬁnition 2.7. For function f (x) whose domain con-
tains [−1, 1], its degree-n Chebyshev truncated series and
degree-n Chebyshev interpolation are respectively

pn(x) :=

akTk(x) and qn(x) :=

ckTk(x) ,

1
2

2

n
(cid:88)

k=0

n
(cid:88)

k=0

where ak :=

2 − 1[k = 0]
π
2 − 1[k = 0]
n + 1

(cid:90) 1

−1
n
(cid:88)

j=0

f (x)Tk(x)
√
1 − x2

dx

f (cid:0)xj

(cid:1)Tk

(cid:0)xj

(cid:1) .

ck :=

Above, xj := cos (cid:0) (j+0.5)π
shev point of order n.

n+1

(cid:1) ∈ [−1, 1] is the j-th Cheby-

The following lemma is known as the aliasing formula for
Chebyshev coefﬁcients:

Lemma 2.8 (cf. Theorem 4.2 of (Trefethen, 2013)). Let
f be Lipschitz continuous on [−1, 1] and {ak}, {ck} be de-
ﬁned in Def. 2.7, then

c0 = a0+a2n+a4n+...
for every k ∈ {1, 2, . . . , n − 1},

,

cn = an+a3n+a5n+...

, and

ck = ak +(ak+2n +ak+4n +...)+(a−k+2n +a−k+4n +...)

Deﬁnition 2.9. For every ρ > 0, let Eρ be the ellipse E of
foci ±1 with major radius 1 + ρ. (This is also known as
Bernstein ellipse with parameter 1 + ρ + (cid:112)2ρ + ρ2.)

The following lemma is the main theory regarding Cheby-
shev approximation:
Lemma 2.10 (cf. Theorem 8.1 and 8.2 of (Trefethen,
2013)). Suppose f (z) is analytic on Eρ and |f (z)| ≤ M
on Eρ. Let pn(x) and qn(x) be the degree-n Chebyshev
truncated series and Chebyshev interpolation of f (x) on
[−1, 1]. Then,

• max

|f (x)−pn(x)| ≤

x∈[−1,1]

• max

|f (x)−qn(x)| ≤

x∈[−1,1]

2M
√

ρ+

2ρ+ρ2

4M
√

ρ+

2ρ+ρ2

(cid:0)1+ρ+(cid:112)2ρ + ρ2(cid:1)−n;

(cid:0)1+ρ+(cid:112)2ρ + ρ2(cid:1)−n.

• |a0| ≤ M and |ak| ≤ 2M (cid:0)1 + ρ + (cid:112)2ρ + ρ2(cid:1)−k for k ≥ 1.

3 Approximate PCP and PCR

We formalize our notions of approximation for PCP and
PCR, and provide a reduction from PCR to PCP.

3.1 Our Notions of Approximation

that Frostig et al. (Frostig et al., 2016) work
Recall
only with matrices A that satisfy the eigengap assump-
is, A has no singular value in the range
tion,

that

-1.0-0.50.51.0-1.0-0.50.51.0-1.0-0.50.51.0-1.0-0.50.51.0-1.0-0.50.51.0-1.0-0.50.51.0Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

[(cid:112)λ(1 − γ), (cid:112)λ(1 + γ)]. Their approximation guaran-
tees are very straightforward:

• an output ξ is ε-approximate for PCP on vector χ if

(cid:107)ξ − ξ∗(cid:107) ≤ ε(cid:107)χ(cid:107);

b if (cid:107)x − x∗(cid:107) ≤ ε(cid:107)b(cid:107).

• an output x is ε-approximate for PCR with regressand

Unfortunately, these notions are too strong and impossible
to satisfy for matrices that do not have a large eigengap
around the projection threshold λ.

In this paper we propose the following more general (but
yet very meaningful) approximation notions.

Deﬁnition 3.1. An algorithm B(χ) is (γ, ε)-approximate
PCP for threshold λ, if for every χ ∈ Rd
(cid:0)B(χ) − χ(cid:1)(cid:13)
1. (cid:13)
(cid:13)P(1+γ)λ
(cid:13)(I − P(1−γ)λ)B(χ)(cid:13)
2. (cid:13)
(cid:13) ≤ ε(cid:107)χ(cid:107).
3. ∀i such that λi ∈ (cid:2)(1 − γ)λ, (1 + γ)λ(cid:3), it satisﬁes

(cid:13) ≤ ε(cid:107)χ(cid:107).

|(cid:104)νi, B(χ) − χ(cid:105)| ≤ |(cid:104)νi, χ(cid:105)| + ε(cid:107)χ(cid:107).

Intuitively, the ﬁrst property above states that, if projected
to the eigenspace with eigenvalues above (1 + γ)λ, then
B(χ) and χ are almost identical; the second property states
that, if projected to the eigenspace with eigenvalues below
(1 − γ)λ, then B(χ) is almost zero; and the third property
states that, for each eigenvector νi with eigenvalue in the
range [(1 − γ)λ, (1 + γ)λ], the projection (cid:104)νi, B(χ)(cid:105) must
be between 0 and (cid:104)νi, χ(cid:105) (but up to an error ε(cid:107)χ(cid:107)).
Naturally, Pλ(χ) itself is a (0, 0)-approximate PCP.
We propose the following notion for approximate PCR:

(cid:13)(I − P(1−γ)λ)C(b)(cid:13)

Deﬁnition 3.2. An algorithm C(b) is (γ, ε)-approximate
PCR for threshold λ, if for every b ∈ Rd(cid:48)
1. (cid:13)
2. (cid:107)AC(b) − b(cid:107) ≤ (cid:107)Ax∗ − b(cid:107) + ε(cid:107)b(cid:107).
where x∗ = (A(cid:62)A)†P(1+γ)λA(cid:62)b is the exact PCR solu-
tion for threshold (1 + γ)λ.

(cid:13) ≤ ε(cid:107)b(cid:107).

The ﬁrst notion states that the output x = C(b) has nearly
no correlation with eigenvectors below threshold (1 − γ)λ;
and the second states that the regression error should be
nearly optimal with respect to the exact PCR solution but
at a different threshold (1 + γ)λ.

Relationship to Frostig et al. Under eigengap assump-
tion, our notions are equivalent to Frostig et al.:

no

3.3.

If A has
Fact
[(cid:112)λ(1 − γ), (cid:112)λ(1 + γ)], then
• Def. 3.1 is equivalent to (cid:107)B(χ) − Pλ(χ)(cid:107) ≤ O(ε)(cid:107)χ(cid:107).
• Def. 3.2 implies (cid:107)C(χ) − x∗(cid:107) ≤ O(ε/λ)(cid:107)b(cid:107) and

singular

value

in

(cid:107)C(χ) − x∗(cid:107) ≤ O(ε)(cid:107)b(cid:107) implies Def. 3.2.

3.2 Reductions from PCR to PCP

If the PCP solution ξ = Pλ(A(cid:62)b) is computed exactly,
then by deﬁnition one can compute (A(cid:62)A)†ξ which gives
a solution to PCR by solving a linear system. However, as
pointed by Frostig et al. (Frostig et al., 2016), this compu-
tation is problematic if ξ is only approximate. The follow-
ing approach has been proposed to improve its accuracy by
Frostig et al.
• “compute p((A(cid:62)A + λI)−1)ξ where p(x) is a polyno-

mial that approximates function

x
1−λx .”

This is a good approximation to (A(cid:62)A)†ξ because the
x
1+λx is exactly x−1.
composition of functions
Frostig et al. picked p(x) = pm(x) = (cid:80)m
t=1 λt−1xt which
is a truncated Taylor series, and used the following proce-
dure to compute sm ≈ pm((A(cid:62)A + λI)−1)ξ:

1−λx and

1

s0 = B(A(cid:62)b),

s1 = ApxRidge(A, λ, s0),
∀k ≥ 1 : sk+1 = s1 + λ · ApxRidge(A, λ, sk) .

(3.1)

Above, B is an approximate PCP solver and ApxRidge is
an approximate ridge regression solver. Under eigengap
assumption, Frostig et al. (Frostig et al., 2016) showed

Lemma 3.4 (PCR-to-PCP). For ﬁxed λ, γ, ε ∈ (0, 1),
let A be a matrix whose singular values
lie in
(cid:2)0, (cid:112)(1 − γ)λ(cid:3) ∪ (cid:2)(cid:112)(1 − γ)λ, 1(cid:3). Let ApxRidge be any
O( ε
m2 )-approximate ridge regression solver, and let B be
any (γ, O( ελ
m2 ))-approximate PCP solver7. Then, proce-
dure (3.1) satisﬁes
(cid:107)sm−(A(cid:62)A)†PλA(cid:62)b(cid:107) ≤ ε(cid:107)b(cid:107)

if m = Θ(log(1/εγ)) .

Unfortunately, the above lemma does not hold without
eigengap assumption.
In this paper, we ﬁx this issue by
proving the following analogous lemma:

Lemma 3.5 (gap free PCR-to-PCP). For ﬁxed λ, ε ∈
(0, 1) and γ ∈ (0, 2/3], let A be a matrix whose sin-
gular values are no more than 1. Let ApxRidge be any
O( ε
m2 )-approximate ridge regression solver, and B be
any (γ, O( ελ
m2 ))-approximate PCP solver. Then, proce-
dure (3.1) satisﬁes,
(cid:40)

(cid:107)(I − P(1−γ)λ)sm(cid:107) ≤ ε(cid:107)b(cid:107) , and
(cid:107)Asm − b(cid:107) ≤ (cid:107)A(A(cid:62)A)†P(1+γ)λA(cid:62)b − b(cid:107) + ε(cid:107)b(cid:107)

(cid:41)

if m = Θ(log(1/εγ)

Note that the conclusion of this lemma exactly corre-
sponds to the two properties in our Def. 3.2. The proof
of Lemma 3.5 is not hard, but requires a very careful case
analysis by decomposing vectors b and each sk into three
components, each corresponding to eigenvalues of A(cid:62)A in
the range [0, (1−γ)λ], [(1−γ)λ, (1+γ)λ] and [(1+γ)λ, 1].

7Recall from Fact 3.3 that this requirement is equivalent to

Above, x∗ = (A(cid:62)A)†PλA(cid:62)b is the exact PCR solution.

saying that (cid:107)B(χ) − Pλχ(cid:107) ≤ O( ε

√
m2 )(cid:107)χ(cid:107).

λ

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

We defer the details to the full version.

4 Chebyshev Approximation Outside [−1, 1]

Classical Chebyshev approximation theory (such as
Lemma 2.10) only talks about the behaviors of pn(x) or
gn(x) on interval [−1, 1]. However, for the purpose of this
paper, we must also bound its value for x > 1. We prove
the following general lemma in the full version, and believe
it could be of independent interest: (we denote by f (k)(x)
the k-th derivative of f at x)

Lemma 4.1. Suppose f (z) is analytic on Eρ and for ev-
ery k ≥ 0, f (k)(0) ≥ 0. Then, for every n ∈ N, letting
pn(x) and qn(x) be be the degree-n Chebyshev truncated
series and Chebyshev interpolation of f (x), we have

∀y ∈ [0, ρ] :

0 ≤ pn(1 + y), qn(1 + y) ≤ f (1 + y) .

5 Our Polynomial Approximation of sgn(x)

For ﬁxed κ ∈ (0, 1], we consider the degree-n Cheby-
shev interpolation qn(x) = (cid:80)n
k=0 ckTk(x) of the function
f (x) = (cid:0) 1+κ−x

on [−1, 1]. Def. 2.7 tells us that

(cid:1)−1/2

2

ck :=

2 − 1[k = 0]
n + 1

n
(cid:88)

(cid:16)√

j=0

2 cos

(cid:16) k(j + 0.5)π
n + 1

(cid:17)(cid:17)

(cid:16)

×

1 + κ − cos

(cid:16) (j + 0.5)π
n + 1

(cid:17)(cid:17)−1/2

.

Our ﬁnal polynomial to approximate sgn(x) is therefore
gn(x) = x·qn(1+κ−2x2)

deg(gn(x)) = 2n+1 .

and

We prove the following theorem in this section:

Theorem 5.1. For every α ∈ (0, 1], ε ∈ (0, 1/2), choos-
ing κ = 2α2, our function gn(x) := x · qn(1 + κ − 2x2)
satisﬁes that as long as n ≥ 1√
εα2 , then (see also
Figure 1)

log 3

2α

• |gn(x) − sgn(x)| ≤ ε for every x ∈ [−1, α] ∪ [α, 1].
• gn(x) ∈ [0, 1] for every x ∈ [0, α] and gn(x) ∈

[−1, 0] for every x ∈ [−α, 0].

Note that our degree n = O(cid:0)α−1 log(1/αε)(cid:1) is near-
optimal, because the minimum degree for a polynomial to
satisfy even only the ﬁrst item is Θ(cid:0)α−1 log(1/ε)(cid:1) (Ere-
menko & Yuditskii, 2007; 2011). However, the results of
(Eremenko & Yuditskii, 2007; 2011) are not constructive,
and thus may not lead to stable matrix polynomials.

We prove Theorem 5.1 by ﬁrst establishing two simple
lemmas.
The following lemma is a consequence of
Lemma 2.10:

Lemma 5.2. For every ε ∈ (0, 1/2) and κ ∈ (0, 1], if
n ≥ 1√
κ

(cid:0)log 1

(cid:1) then

κ + log 4
∀x ∈ [−1, 1],

ε

|f (x) − qn(x)| ≤ ε .

Proof of Lemma 5.2. Denoting by f (z) = (cid:0) 1+κ−z
,
we know that f (z) is analytic on ellipse Eρ with ρ =
κ/2, and it satisﬁes |f (z)| ≤ (cid:112)2/κ in Eρ. Applying
(cid:1)
κ + log 4
Lemma 2.10, we know that when n ≥ 1√
κ
(cid:3)

it satisﬁes |f (x) − qn(x)| ≤ ε.

(cid:0)log 1

(cid:1)−0.5

2

ε

lemma an immediate consequence of our
(cid:1)−0.5

The next
Lemma 4.1 with f (z) = (cid:0) 1+κ−z
2
Lemma 5.3. For every ε ∈ (0, 1/2), κ ∈ (0, 1], n ∈ N,
and x ∈ [0, κ], we have

:

0 ≤ qn(1 + x) ≤

(cid:16) κ − x
2

(cid:17)−1/2

.

are now ready to prove

Proof of Theorem 5.1. We
Theorem 5.1.
• When x ∈ [−1, α] ∪ [α, 1], it satisﬁes 1 + κ − 2x2 ∈
[−1, 1]. Therefore, applying Lemma 5.2 we have when-
ever n ≥ 1√
εα2 it satisﬁes |f (1 +
2α
κ−2x2)−qn(1+κ−2x2)|∞ ≤ ε. This further implies

εκ = 1√

κ log 6

log 3

|gn(x)−sgn(x)| = |xqn(1+κ−2x2)−xf (1+κ−2x2)|
≤ |x||f (1 + κ − 2x2) − qn(1 + κ − 2x2)| ≤ ε .

• When |x| ≤ α, it satisﬁes 1 + κ − 2x2 ∈ [1, 1 + κ].

Applying Lemma 5.3 we have for all x ∈ [0, α],
0 ≤ gn(x) = x · qn(1 + κ − 2x2) ≤ x · (x2)−1/2 = 1
and similarly for x ∈ [−α, 0] it satisﬁes 0 ≥ gn(x) ≥
(cid:3)
−1.

A Bound on Chebyshev Coefﬁcients. We also give an
upper bound to the coefﬁcients of polynomial qn(x). Its
proof can be found in the full version, and this upper bound
shall be used in our ﬁnal stability analysis.

Let qn(x) =
k=0 ckTk(x) be the degree-n Chebyshev interpolation
on [−1, 1]. Then, for all i ∈

Lemma 5.4 (coefﬁcients of qn).
(cid:80)n
of f (x) = (cid:0) 1+κ−x
{0, 1, . . . , n},

(cid:1)−1/2

2

|ci| ≤

e(cid:112)32(i + 1)
κ

(cid:16)

1 + κ +

2κ + κ2

(cid:112)

(cid:17)−i

6 Stable Computation of Matrix Chebyshev

Polynomials

that

In this section we show that any polynomial
is
a weighted summation of Chebyshev polynomials with
bounded coefﬁcients, can be stably computed when applied
to matrices with approximate computations. We achieve
so by ﬁrst generalizing Clenshaw’s backward method to
matrix case in Section 6.1 in order to compute a matrix
variant of Chebyshev sum, and then analyze its stability in
Section 6.2 with the help from Elloit’s forward-backward
transformation (Elliott, 1968).

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

Remark 6.1. We wish to point out that although Chebyshev
polynomials are known to be stable under error when com-
puted on scalars (Gil et al., 2007), it is not immediately
clear why it holds also for matrices. Recall that Cheby-
shev polynomials satisfy Tn+1(x) = 2xTn(x) − Tn−1(x).
In the matrix case, we have Tn+1(M)χ = 2MTn(M)χ −
Tn−1(M)χ where χ ∈ Rd is a vector. If we analyzed this
formula coordinate by coordinate, error could blow up by a
factor d per iteration.

In addition, we need to ensure that the stability theorem
holds for matrices M with eigenvalues that can exceed 1.
This is not standard because Chebyshev polynomials are
typically analyzed only on domain [−1, 1].

6.1 Clenshaw’s Method in Matrix Form

Consider any computation of the form

(cid:126)sN :=

Tk(M)(cid:126)ck ∈ Rd

(6.1)

N
(cid:88)

k=0

kχ where c(cid:48)

where M ∈ Rd×d is symmetric and each (cid:126)ck is in Rd. (Note
that for PCP and PCR purposes, we it sufﬁces to consider
k ∈ R is a scalar and χ ∈ Rd is a ﬁxed
(cid:126)ck = c(cid:48)
vector for all k. However, we need to work on this more
general form for our stability analysis.)
Vector sN can be computed using the following procedure:
Lemma 6.2 (backward recurrence). (cid:126)sN = (cid:126)b0−M(cid:126)b1 where

(cid:126)bN +1 := (cid:126)0,

(cid:126)bN := (cid:126)cN ,

and

∀r ∈ {N − 1, . . . , 0} : (cid:126)br := 2M(cid:126)br+1 − (cid:126)br+2 + (cid:126)cr ∈ Rd .

6.2

Inexact Clenshaw’s Method in Matrix Form

We show that, if implemented using the backward recur-
rence formula, the Chebyshev sum of (6.1) can be stably
computed. We deﬁne the following model to capture the
error with respect to matrix-vector multiplications.

Deﬁnition 6.3 (inexact backward recurrence). Let M be
an approximate algorithm that satisﬁes (cid:107)M(u) −Mu(cid:107)2 ≤
ε(cid:107)u(cid:107)2 for every u ∈ Rd. Then, deﬁne inexact backward
recurrence to be

(cid:98)bN +1 := 0, (cid:98)bN := (cid:126)cN ,

and
(cid:1) − (cid:98)br+2 + (cid:126)cr ∈ Rd ,

(cid:98)br+1

∀r ∈ {N − 1, . . . , 0} : (cid:98)br := 2M(cid:0)
and deﬁne the output as (cid:98)sN := (cid:98)b0 − M((cid:98)b1).
The following theorem gives an error analysis to our inex-
act backward recurrence. We prove it in full version, and
the main idea of our proof is to convert each error vector of
a recursion of the backward procedure into an error vector
corresponding to some original (cid:126)ck.
Theorem 6.4 (stable Chebyshev sum). For every N ∈
N∗, suppose the eigenvalues of M are in [a, b] and sup-
pose there are parameters CU ≥ 1, CT ≥ 1, ρ ≥ 1, Cc ≥

0 satisfying ∀k ∈ {0, 1, . . . , N } :
(cid:110)

(cid:94)

ρk(cid:107)(cid:126)ck(cid:107) ≤ Cc

∀x ∈ [a, b] :

|Tk(x)|≤CT ρk
|Uk(x)|≤CU ρk

(cid:111)

.

Then, if the inexact backward recurrence in Def. 6.3 is
applied with ε ≤ 1
, we have
(cid:107)(cid:98)sN − (cid:126)sN (cid:107) ≤ ε · 2(1 + 2N CT )N CU Cc .

4N CU

7 Algorithms and Main Theorems for PCP

and PCR

We are now ready to state our main theorems for PCP and
PCR. We ﬁrst note a simple fact:
(Pλ)χ = I+sgn(S)

Fact 7.1.
λI)−1A(cid:62)A − I = (A(cid:62)A + λI)−1(A(cid:62)A − λI).

where S := 2(A(cid:62)A +

2

In other words, for every vector χ ∈ Rd,
the exact
PCP solution Pλ(χ) is the same as computing (Pλ)χ =
I+sgn(S)
χ. Thus, we can use our polynomial gn(x) intro-
2
duced in Section 5 and compute gn(S)χ ≈ sgn(S)χ. Fi-
nally, in order to compute gn(S), we need to multiply S to
deg(gn) vectors; whenever we do so, we call perform ridge
regression once.

Since the high-level structure of our PCP algorithm is very
clear, due to space limitation, we present the pseudocodes
of our PCP and PCR algorithms in the full version.

7.1 Our Main Theorems

We ﬁrst state our main theorem under the eigengap as-
sumption, in order to provide a direct comparison to that
of Frostig et al. (Frostig et al., 2016).
Theorem 7.2 (eigengap assumption). Given A ∈ Rd(cid:48)×d
and λ, γ ∈ (0, 1), assume that the singular values of A
are in the range [0, (cid:112)(1 − γ)λ] ∪ [(cid:112)(1 + γ)λ, 1]. Given
χ ∈ Rd and b ∈ Rd(cid:48)

, denote by

ξ∗ = Pλχ and x∗ = (A(cid:62)A)−1PλA(cid:62)b
the exact PCP and PCR solutions, and by ApxRidge any
ε(cid:48)-approximate ridge regression solver. Then,
• QuickPCP outputs ξ satisfying (cid:107)ξ∗ − ξ(cid:107) ≤ ε(cid:107)χ(cid:107) with
(cid:1) oracle calls to ApxRidge as long as

O(cid:0)γ−1 log 1
log(1/ε(cid:48)) = Θ(cid:0) log 1

γε

(cid:1).

γε

• QuickPCR outputs x satisfying (cid:107)x − x∗(cid:107) ≤ ε(cid:107)b(cid:107) with
(cid:1) oracle calls to ApxRidge, as long as

O(cid:0)γ−1 log 1
log(1/ε(cid:48)) = Θ(cid:0) log 1

γλε

(cid:1).

γλε

In contrast, the number of ridge-regression oracle calls was
Θ(γ−2 log 1
γλε ) for PCR in
(Frostig et al., 2016). We include the proof of Theorem 7.2
in the full version.

γε ) for PCP and Θ(γ−2 log 1

We state our theorem without the eigengap assumption.

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

Theorem 7.3 (gap-free). Given A ∈ Rd(cid:48)×d, λ ∈ (0, 1),
and γ ∈ (0, 2/3], assume that (cid:107)A(cid:107)2 ≤ 1. Given
χ ∈ Rd and b ∈ Rd(cid:48)
, and suppose ApxRidge is an ε(cid:48)-
approximate ridge regression solver, then

• QuickPCP outputs ξ that is (γ, ε)-approximate PCP
(cid:1) oracle calls to ApxRidge as long

with O(cid:0)γ−1 log 1
as log(1/ε(cid:48)) = Θ(cid:0) log 1

γε

(cid:1).

γε
• QuickPCR outputs x that is (γ, ε)-approximate PCR
(cid:1) oracle calls to ApxRidge as

with O(cid:0)γ−1 log 1
long as elog(1/ε(cid:48)) = Θ(cid:0) log 1

(cid:1).

γλε

γλε

We make a ﬁnal remark here regarding the practical usage
of QuickPCP and QuickPCR.
Remark 7.4. Since our theory is for (γ, ε)-approximations
that have two parameters, the user in principle has to feed
in both γ and n where n is the degree of the polynomial
approximation to the sign function. In practice, however, it
is usually sufﬁcient to obtain (ε, ε)-approximate PCP and
PCR. Therefore, our pseudocodes allow users to set γ = 0
and thus ignore this parameter γ; in such a case, we shall
use γ = log(n)/n which is equivalent to setting γ = Θ(ε)
because n = Θ(γ−1 log(1/γε)).

8 Experiments

We provide empirical evaluations in the full version of this
paper.

9 Conclusion

We summarize our contributions.

• We put forward approximate notions for PCP and PCR
that do not rely on any eigengap assumption. Our no-
tions reduce to standard ones under the eigengap as-
sumption.

• We design near-optimal polynomial approximation

g(x) to sgn(x) satisfying (1.1) and (1.2).

• We develop general stable recurrence formula for ma-
trix Chebyshev polynomials; as a corollary, our g(x)
can be applied to matrices in a stable manner.

• We obtain faster, provable PCA-free algorithms for

PCP and PCR than known results.

References

Allen-Zhu, Zeyuan. Katyusha: The First Direct Accelera-
tion of Stochastic Gradient Methods. In STOC, 2017.

Allen-Zhu, Zeyuan and Li, Yuanzhi. LazySVD: Even
Faster SVD Decomposition Yet Without Agonizing
Pain. In NIPS, 2016a.

Allen-Zhu, Zeyuan and Li, Yuanzhi. Doubly Accelerated
Methods for Faster CCA and Generalized Eigendecom-
position. In Proceedings of the 34th International Con-
ference on Machine Learning, ICML ’17, 2017.

Allen-Zhu, Zeyuan, Richt´arik, Peter, Qu, Zheng, and Yuan,
Yang. Even faster accelerated coordinate descent using
non-uniform sampling. In ICML, 2016.

Boutsidis, Christos and Magdon-Ismail, Malik.

Faster
SVD-truncated regularized least-squares. In 2014 IEEE
International Symposium on Information Theory, pp.
1321–1325. IEEE, 2014.

Chan, Tony F and Hansen, Per Christian. Computing trun-
cated singular value decomposition least squares solu-
tions by rank revealing QR-factorizations. SIAM Journal
on Scientiﬁc and Statistical Computing, 11(3):519–530,
1990.

Elliott, David. Error analysis of an algorithm for summing
certain ﬁnite series. Journal of the Australian Mathemat-
ical Society, 8(02):213–221, 1968.

Eremenko, Alexandre and Yuditskii, Peter. Uniform ap-
proximation of sgn x by polynomials and entire func-
Journal d’Analyse Math´ematique, 101(1):313–
tions.
324, 2007.

Eremenko, Alexandre and Yuditskii, Peter. Polynomials of
the best uniform approximation to sgn (x) on two inter-
vals. Journal d’Analyse Math´ematique, 114(1):285–315,
2011.

Frostig, Roy, Musco, Cameron, Musco, Christopher, and
Sidford, Aaron. Principal Component Projection With-
out Principal Component Analysis. In ICML, 2016.

Gil, Amparo, Segura, Javier, and Temme, Nico M. Nu-
merical Methods for Special Functions. Society for
ISBN
Industrial and Applied Mathematics, jan 2007.
978-0-89871-634-4.
doi: 10.1137/1.9780898717822.
http://epubs.siam.org/doi/
URL
abs/10.1137/1.9780898717822http:
//epubs.siam.org/doi/book/10.1137/
1.9780898717822.

Han, Insu, Malioutov, Dmitry, Avron, Haim, and Shin,
Jinwoo. Approximating the spectral sums of large-
scale matrices using chebyshev approximations. arXiv
preprint arXiv:1606.00942, 2016.

Allen-Zhu, Zeyuan and Li, Yuanzhi. First Efﬁcient Con-
vergence for Streaming k-PCA: a Global, Gap-Free, and
Near-Optimal Rate. ArXiv e-prints, abs/1607.07837,
July 2016b.

Higham, N. Functions of Matrices. Society for Indus-
trial and Applied Mathematics, 2008. doi: 10.1137/1.
9780898717778. URL http://epubs.siam.org/
doi/abs/10.1137/1.9780898717778.

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

Johnson, Rie and Zhang, Tong. Accelerating stochas-
tic gradient descent using predictive variance reduction.
In Advances in Neural Information Processing Systems,
NIPS 2013, pp. 315–323, 2013.

Musco, Cameron and Musco, Christopher. Randomized
block krylov methods for stronger and faster approxi-
mate singular value decomposition. In NIPS, pp. 1396–
1404, 2015.

Nakatsukasa, Yuji and Freund, Roland W. Computing
fundamental matrix decompositions accurately via the
matrix sign function in two iterations: The power of
SIAM Review, 58(3):461–493,
zolotarev’s functions.
2016.

Nesterov, Yurii. Introductory Lectures on Convex Program-
ming Volume: A Basic course, volume I. Kluwer Aca-
demic Publishers, 2004. ISBN 1402075537.

Schilders, Wilhelmus H.A., Van der Vorst, Henk A., and
Rommes, Joost. Model order reduction: theory, research
aspects and applications, volume 13. Springer, 2008.

Shewchuk, Jonathan Richard. An introduction to the conju-
gate gradient method without the agonizing pain, 1994.

Trefethen, Lloyd N. Approximation Theory and Approxi-

mation Practice. SIAM, 2013.

van den Eshof, Jasper, Frommer, Andreas, Lippert, Th,
Schilling, Klaus, and van der Vorst, Henk A. Numerical
methods for the qcdd overlap operator. i. sign-function
and error bounds. Computer Physics Communications,
146(2):203–224, 2002.

