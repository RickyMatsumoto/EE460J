Variational Inference for Sparse and Undirected Models: Appendix

John Ingraham 1 Debora Marks 1

1. Appendix I: PVI algorithm

See Algorithm 1.

2. Appendix II: Experiments

2.1. Spin Models

We generated two synthetic systems. The ﬁrst system was
ferromagnetic (all J ≥ 0) with 64 spins, where neighbor-
ing spins xi, xj have a nonzero interaction of Jij = 0.2
if adjacent on a 4 × 4 × 4 periodic lattice. This coupling
strength equates to being slightly above the critical temper-
ature, meaning the system will be highly correlated despite
the underlying interactions being only nearest-neighbor.

(cid:16)

The second system was a diluted Sherrington-Kirkpatrick
spin glass (Sherrington & Kirkpatrick, 1975; Aurell & Eke-
berg, 2012) with 100 spins. The couplings in this model
were deﬁned by Erd˝os-Renyi random graphs (Erd˝os &
R´enyi, 1960) with non-zero edge weights distributed as
Jij ∼ N
where N p is the average degree. We
generated 5 random systems where the average degree was
N p = 100(0.02) = 2. Across all of the systems, we used
Swendsen-Wang sampling (Swendsen & Wang, 1987) to
sample synthetic data and checked that the sampling was
sufﬁcient to eliminate autocorrelation in the data.

0, 1
N p

(cid:17)

For inference, we tested both L1-regularized deterministic
approaches as well as a variational approach based on Per-
sistent VI. The L1 regularized approaches included Pseu-
dolikelihood, (PL) (Aurell & Ekeberg, 2012), Minimum
Probability Flow (MPF) (Sohl-Dickstein et al., 2011), and
Persistent Contrastive Divergence (PCD) (Tieleman, 2008).
Additionally, we tested the proposed alternative regulariza-
tion method of Pseudolikelihood Decimation (Decelle &
Ricci-Tersenghi, 2014).

For L1 regularized Pseudolikelihood and Minimum Prob-
ability Flow, we selected the hyperparameter λ using 10-
fold cross-validation over 10 logarithmically spaced values
on the interval [0.01, 10]. We performed L1 regulariza-

1Harvard Medical School, Boston, Massachusetts. Correspon-
dence to: John Ingraham <ingraham@fas.harvard.edu>, Debora
Marks <debbie@hms.harvard.edu>.

tion of the deterministic objectives using optimizers from
(Schmidt, 2010), and chose the corresponding L1 hyperpa-
rameter for PCD + L1 based on the optimal cross-validated
value of λ that was selected for L1-regularized Pseudolike-
lihood.

For the hierarchical model inferred with Persisent VI, we
placed a separate noncentered Horseshoe prior over the
ﬁelds and couplings, in accodance with the (centered) hi-
erarchy

sh ∼ C+(0, 1),
σi ∼ C+(0, sh),
hi ∼ N (0, σ2
i ),

sJ ∼ C+(0, 1),
σij ∼ C+(0, sJ ),
Jij ∼ N (0, σ2
ij).

where C+(0, 1) is the standard Half-Cauchy distribution.
We then used PVI-3 with 100 persistent Markov chains
and performed stochastic gradient descent using Adam
(Kingma & Ba, 2014) with default momentum and a learn-
ing rate that linearly decayed from 0.01 to 0 over 5 × 104
iterations.

2.2. Synthetic Protein Data

We constructed a synthetic Potts spin glass with sparse in-
teractions chosen to reﬂect an underlying 3D structure. Af-
ter forming a contact topology from a random packed poly-
mer, we generated synthetic group-Student-t distributed
sitewise bias vectors hi (each 20 × 1) and Gaussian dis-
tributed coupling matrices Jij (each 20 × 20) to mirror
the strong site-bias and weak-coupling regime of proteins.
Since this system is highly frustrated, we thinned 2 × 106
sweeps of Gibbs sampling to 2000 sequences that exhibited
no autocorrelation.

Given 400 of the 2000 synthetic sequences1, we inferred
L2 and group L1-regularized MAP estimates under a pseu-
dolikelihood approximation with 5-fold cross validation
to choose hyperparameters from 6 values in the range
{0.3, 1.0, 3.0, 10.0, 30.0, 100.0}. We also ran PVI-10 with
40 persistent Markov chains and 5000 iterations of stochas-
tic gradient descent with Adam2 (Kingma & Ba, 2014). We
note that the current standards of the ﬁeld are based on L2

1We ﬁnd this effective sample size to mirror natural protein

families (unpublished)

2α = 0.01, β1 = 0.9, β2 = 0.999, no decay

Variational Inference for Sparse and Undirected Models: Appendix

i=1 on x ∈ {1, . . . , q}D

Algorithm 1 Persistent Variational Inference (PVI-n) with Gaussian q(θ|φ)
Require: Model. Undirected p(x|θ) deﬁned by k features {fi(x)}k
Require: Data. Expectations of the features {ED [fi(x)]}k
Require: Prior. Prior gradient ∇ log P (θ)
Require: Number of Gibbs sweeps n, Markov Chains M , variational samples Q
Require: Initial variational parameters µ0, log σ0 (e.g. {0, −3})
// Initialize parameters and Markov chains ˜x
µ ← µ0, log σ ← log σ0
˜x(1:M ) ← RandInt(1, q)
t ← 0
while not converged do

i=1 and sample size N

// Estimate ∇ELBO with Q samples from the variational distribution
∇µL ← 0, ∇log σL ← 0
for s = 1 . . . Q do
(cid:15) ∼ N (0, I|µ|)
θ ← µ + σ (cid:12) (cid:15)
// Estimate model-dependent expectations E, where Ei = Ep(x|θ) [fi(x)]
E ← 0
for m = 1 . . . M do
for j = 1 . . . n do

˜x(m) ← GibbsSweep(p(x|θ), ˜x(m))
E ← E + 1
M n {fi(˜x(m))}k

i=1

end for

end for
// Compute stochastic gradient
G ← N (ED [fi(x)] − E) + ∇ log P (θ)
∇µL ← ∇µL + 1
Q G
∇log σL ← ∇log σL + 1

Q (G (cid:12) (θ − µ) + 1)

end for
// Update parameters with Robbins-Monro sequence {ρt}
µ ← µ + ρt∇µL
log σ ← log σ + ρt∇log σ
t ← t + 1

end while

Variational Inference for Sparse and Undirected Models: Appendix

and Group L1 regularized Pseudolikelihood (Balakrishnan
et al., 2011; Ekeberg et al., 2013).

2.3. Real Protein Data

2.3.1. SAMPLE REWEIGHTING

Natural protein sequences share a common evolutionary
history that introduces signiﬁcant redundancy and correla-
tion between related sequences. Treating them as indepen-
dent data is biased by both the overrepresentation of certain
sequences due to the evolutionary process (phylogeny) or
the human sampling process (biased sequencing of partic-
ular species). Thus, we follow a standard practice of cor-
recting the overrepresentation of sequences by a sample-
reweighting approach (Ekeberg et al., 2013).

Sequence reweighting.
If we were to treat all data as in-
dependent, then every sample would receive unit weight
in the log likelihood. To correct for the over and un-
derrepresentation of certain sequences, we estimate rela-
tive sequence weights using a common inverse neighbor-
hood density based approach from the ﬁeld (Ekeberg et al.,
2013). We set the relative weight of each sequence propor-
tional to the inverse number of neighboring sequences that
differ by a normalized Hamming distance of less than θ.
We use the established value of θ = 0.2.

Effective sample size estimation. We propose a new
deﬁnition for an effective sample size Nef f of correlated
discrete data and derive an algorithm for estimating it from
count data. The estimator is based on the assumption that
in limited data regimes for sparsely coupled systems, the
sample Mutual Information between random variables is
dominated by random, coincidental correlations rather than
actual correlations due to underlying interactions. This is
consistent with classic results on the bias of information
quantities in limited data regimes known as “Miller Mad-
dow bias” (Miller, 1955; Paninski, 2003). If we can deﬁne
a null model for how such coincidental correlations would
arise for a given random sample of size N , then we deﬁne
Nef f as the sample size that matches the expected null MI
to the observed MI.

Ei,j [MInull|Nef f ] = Ei,j [MIdata]

(1)

The expectation on the right is given by the average sam-
ple Mutual Information in the data, while the expectation
on the left will be speciﬁc to a null model for Mutual In-
formation Ei,j [M Inull|N ]. Given a noisy estimator for
Ei,j [M Inull|Nef f ], we solve for Nef f by matching the ex-
pectations with Robbins-Monro stochastic optimization.

To deﬁne
information
the null model of mutual
Ei,j [M Inull|N ] we treat every variable as independent

categorical counts that were drawn from a Dirichlet-
Multinomial hierarchy with a log-uniform hyperprior over
the (symmetric) concentration parameter α.

Given observed frequencies fi and fj for letters xi and xj
together with a candidate sample size N , we (i) use Bayes’
theorem to sample underlying distributions pi, pj that pro-
duced the observed frequencies, (ii) generate N samples
from the null joint distribution pipT
j , and (iii) compute the
sample Mutual Information of this synthetic count data (Al-
gorithm 2).

We also experimented with using both MAP and posterior
mean estimators as plugin approximations ˆpi, ˆpj for the
latent distributions, but found that each of these were bi-
ased estimators of the true sample size in simulation. Pos-
terior mode estimates generally underestimated the null en-
tropy (ˆpi too rough) while the posterior mean overesti-
mated the entropy (ˆpi too smooth).
It seems reasonable
that this would be the behavior of point estimates that do
not account for the uncertainty in the null distributions that
is signaled by the roughness of the frequency data.

We note that this estimator will become invalid as the data
become strong, since the assumption that Mutual Infor-
mation is dominated by sampling noise will break down.
However, for the real protein data that we examined, we
found that this approach for effective sample size correc-
tion was critical for Bayesian methods such as Peristent VI
to be able to set the top level hyperparameters (the sparsity
levels) from the data.

Algorithm 2 Sample the null mutual information as a func-
tion of sample size Ei,j [M Inull|N ]

Require: Sample size N
Require: Observed frequencies fi, fj
Sample positions i ∈ [L], j ∈ [L] \ i
Set count data Ci ← N fi, Cj ← N fj
Sample concentration parameter αi|Ci, αj|Cj with nu-
merical CDF
Sample null distributions pi|Ci, αi, pj|Cj, αj from
Dirichlet
Sample joint count data M(xi, xj) from categorical joint
distribution pipT
j
Compute sample frequencies f = 1
1
N
Compute
(cid:80)

xi
Information M I

M(xi, xj), fj = 1
N

N M(xi, xj), fi =

sample Mutual
f (xi, xj) log f (xi,xj )
fi(xi)fj (xj )

M(xi, xj)

xi,xj

(cid:80)

(cid:80)

=

xj

2.3.2. INFERENCE AND RESULTS

Alignment Our sequence alignment was based on the
Pfam 27.0 family PF00018, which we subsequently pro-
cessed to remove all sequences with more than 25% gaps.

Variational Inference for Sparse and Undirected Models: Appendix

Indels Natural sequences contain insertions and dele-
tions that are coded by ‘gaps’ in alignments. We treated
these as a 21st character (in addition to amino acids) and ﬁt
a q = 21 state Potts model. We acknowledge that, while
this may be standard practice in the ﬁeld, it is a strong in-
dependence approximation because all of the gaps in dele-
tions are perfectly correlated.

Inference We used 10,000 iterations of PVI-10 with 10
variational samples per iteration and 40 persistent Gibbs
chains.

Comparison to 3D structure We collected about 260 3D
structures of SH3 domains referenced on PF00018 (Pfam
27.0) and computed minimum atom distances between all
positions in the Pfam alignment. For each pair i, j, we used
the median of distances across all structures to summarize
the “typical” minimum atom distance between i and j.

References

Aurell, Erik and Ekeberg, Magnus. Inverse ising inference using
all the data. Physical review letters, 108(9):090201, 2012.
Balakrishnan, Sivaraman, Kamisetty, Hetunandan, Carbonell,
Jaime G, Lee, Su-In, and Langmead, Christopher James.
Learning generative models for protein fold families. Pro-
teins: Structure, Function, and Bioinformatics, 79(4):1061–
1078, 2011.

Decelle, Aur´elien and Ricci-Tersenghi, Federico. Pseudolikeli-
hood decimation algorithm improving the inference of the in-
teraction network in a general class of ising models. Physical
review letters, 112(7):070603, 2014.

Ekeberg, Magnus, L¨ovkvist, Cecilia, Lan, Yueheng, Weigt, Mar-
tin, and Aurell, Erik. Improved contact prediction in proteins:
using pseudolikelihoods to infer potts models. Physical Review
E, 87(1):012707, 2013.

Erd˝os, Paul and R´enyi, A. On the evolution of random graphs.

Publ. Math. Inst. Hungar. Acad. Sci, 5:17–61, 1960.

Kingma, Diederik and Ba, Jimmy. Adam: A method for stochas-

tic optimization. arXiv preprint arXiv:1412.6980, 2014.

Miller, George A. Note on the bias of information estimates. In-
formation theory in psychology: Problems and methods, 2(95):
100, 1955.

Paninski, Liam. Estimation of entropy and mutual information.

Neural computation, 15(6):1191–1253, 2003.

Schmidt, Mark. Graphical model structure learning with l1-
PhD thesis, UNIVERSITY OF BRITISH

regularization.
COLUMBIA (Vancouver, 2010.

Sherrington, David and Kirkpatrick, Scott. Solvable model of a

spin-glass. Physical review letters, 35(26):1792, 1975.

Sohl-Dickstein, Jascha, Battaglino, Peter B, and DeWeese,
Michael R. New method for parameter estimation in proba-
bilistic models: minimum probability ﬂow. Physical review
letters, 107(22):220601, 2011.

Swendsen, Robert H and Wang, Jian-Sheng. Nonuniversal critical
dynamics in monte carlo simulations. Physical review letters,
58(2):86, 1987.

Tieleman, Tijmen. Training restricted boltzmann machines us-
ing approximations to the likelihood gradient. In Proceedings
of the 25th international conference on Machine learning, pp.
1064–1071. ACM, 2008.

