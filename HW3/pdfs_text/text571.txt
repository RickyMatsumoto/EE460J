Relative Fisher Information and Natural Gradient for Learning Large
Modular Models

Ke Sun 1 Frank Nielsen 2 3

Abstract
Fisher information and natural gradient provided
deep insights and powerful tools to artiﬁcial neu-
ral networks. However related analysis becomes
more and more difﬁcult as the learner’s structure
turns large and complex. This paper makes a pre-
liminary step towards a new direction. We extract
a local component from a large neural system,
and deﬁne its relative Fisher information metric
that describes accurately this small component,
and is invariant to the other parts of the system.
This concept is important because the geometry
structure is much simpliﬁed and it can be easily
applied to guide the learning of neural networks.
We provide an analysis on a list of commonly
used components, and demonstrate how to use
this concept to further improve optimization.

1. Fisher Information Metric

The Fisher Information Metric (FIM) I(Θ) = (Iij) of a
statistical parametric model p(x | Θ) of order D is deﬁned
by a D × D positive semideﬁnite (psd) matrix (I(Θ) (cid:23) 0)
(cid:104) ∂l
, where l(Θ) denotes
with coefﬁcients Iij = Ep
∂Θi
the log-density function log p(x | Θ). Under light regular-
ity conditions, FIM can be rewritten equivalently as

∂l
∂Θj

(cid:105)

Iij = −Ep

(cid:21)

(cid:20)

∂2l
∂Θi∂Θj

= 4

(cid:90) ∂(cid:112)p(x | Θ)
∂Θi

∂(cid:112)p(x | Θ)
∂Θj

dx.

As its empirical counterpart, the observed FIM (Efron &
Hinkley, 1978) with respect to (wrt) a sample set Xn =
k=1 is ˆI(Θ | Xn) = −∇2l(Θ | Xn), which is of-
{xk}n
ten evaluated at the maximum likelihood estimate Θ =
ˆΘ(Xn). By the law of large numbers, ˆI(Θ) converges to
the (expected) FIM I(Θ) as n → ∞.

1King Abdullah University of Science and Technol-
ogy (KAUST), Saudi Arabia 2 ´Ecole Polytechnique, France
3Sony Computer Science Laboratories Inc.,
Cor-
respondence to: Ke Sun <sunk@ieee.org>, Frank Nielsen
<Frank.Nielsen@acm.org>.

Japan.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

The FIM is not invariant and depends on the parameteri-
zation. We can optionally write I(Θ) as IΘ(Θ) to em-
phasize the coordinate system. By deﬁnition, IΘ(Θ) =
J (cid:124)IΛ(Λ)J where J = (Jij), Jij = ∂Λi
is the Jaco-
∂Θj
bian matrix. For example, the FIM of regular natural ex-
ponential families (NEFs) l(Θ) = Θ(cid:124)t(x) − F (Θ) (log-
linear models with sufﬁcient statistics t(x)) is I(Θ) =
∇2F (Θ) (cid:31) 0, the Hessian of the log-normalizer function
F (Θ). Although exponential families can approximate ar-
bitrarily any smooth density (Cobb et al., 1983), the log-
normalizer function may not be available in closed-form
nor computationally tractable (Montanari, 2015).

The FIM is an important concept for statistical machine
learning. It gives a Riemannian metric (Hotelling, 1929;
Rao, 1945) of the learning parameter space which is
unique ( ˇCencov, 1982; Dowty, 2017). Hence any learn-
ing is in a space that is intrinsically curved based on the
FIM, regardless of the choice of the coordinate system. It
also gives a bound (Fr´echet, 1943; Cram´er, 1946; Nielsen,
2013) of learning efﬁciency saying that the variance of any
unbiased learning of Θ is at least I −1(Θ)/n, where n is
the i.i.d. sample size. The FIM is applied to neural network
optimization (Amari, 1997), metric learning (Lebanon,
2005), reinforcement learning (Thomas, 2014) and mani-
fold learning (Sun & Marchand-Maillet, 2014).

However computing the FIM is expensive. Besides the fact
that learning machines have often singularities (Watanabe,
2009) (|I(Θ)| = 0, not full rank) characterized by plateaux
in gradient learning, computing/estimating the FIM of a
large neuron system (e.g. one with millions of parameters,
Szegedy, Christian et al. 2015) is very challenging due to
the ﬁniteness of data, and the huge number D(D+1)
of ma-
trix coefﬁcients to evaluate. Furthermore, gradient descent
techniques require inverting this large matrix and tuning the
learning rate.

2

To tackle this problem, past works mainly focus on how
to approximate the FIM with a block diagonal form (Ku-
rita, 1994; Le Roux et al., 2008; Martens, 2010; Pascanu &
Bengio, 2014; Martens & Grosse, 2015) or quasi-diagonal
form (Ollivier, 2013; Marceau-Caron & Ollivier, 2016).
This global approach faces increasing approximation error
and increasing computational cost as the system scales up

Relative Fisher Information and Natural Gradient

and as complex and dynamic structures (Looks et al., 2017)
emerge.

This work aims at a different local approach. The idea is
to accurately describe the information geometry (IG) in a
subsystem of the large learning system, which is invariant
to the scaling up and structural change of the global system,
so that the local machinery, including optimization, can be
discussed regardless of the other parts.

For this purpose, a novel concept, the Relative Fisher In-
formation Metric (RFIM), is deﬁned. Unlike the traditional
geometric view of a high-dimensional parameter manifold,
RFIMs deﬁnes multiple projected low-dimensional geome-
tries of subsystems. This geometry is correlated to the pa-
rameters beyond the subsystem and is therefore considered
dynamic. It can be used to characterize the efﬁciency of
a local learning process. Taking this stance has potential
in deep learning because a deep neural network can be de-
composed into many local components such as neurons or
layers. The RFIM is well suited to the compositional block
structures of neural networks. The RFIM can be used for
out-of-core learning.

The paper is organized as follows. Sec. 2 reviews natu-
ral gradient within the context of Multi-Layer Perceptrons
(MLPs). Sec. 3 formally deﬁnes the RFIM, and gives a ta-
ble of RFIMs of several commonly used subsystems. Sec. 4
discusses the advantages of using the RFIM as compared
to the FIM. Sec. 5 gives an algorithmic framework and
proof-of-concept experiments on neural network optimiza-
tion. Sec. 6 presents related works on parameter diago-
nalization. Sec. 7 concludes this work and further hints at
perspectives.

2. Natural Gradient: Review and Insights

Consider a MLP x θ1−→ h1 · · · hL−1
cal model is the following conditional distribution

θL−−→ y, whose statisti-

p(y | x, Θ) =

p(h1 | x, θ1) · · · p(y | hL−1, θL).

(cid:88)

h1,··· ,hL−1

The often intractable sum over h1, · · · , hL−1
can
· · · ,
rid off by deteriorating p(h1 | x, θ1),
be get
p(hL−1 | hL−2, θL−1) to Dirac’s deltas δ, and letting
merely the last layer p(y | hL−1, θL) be stochastic. Other
models such as restricted Boltzmann machines (Nair &
Hinton, 2010; Montavon & M¨uller, 2012), deep belief net-
works (Hinton et al., 2006), dropout (Wager et al., 2013),
and variational autoencoders (Kingma & Welling, 2014) do
consider the hi’s to be stochastic.

The tensor metric of the neuromanifold (Amari, 1995)
M, consisting of all MLPs with the same architec-
ture but different parameter values,
is locally deﬁned
by the FIM. Because a MLP corresponds to a con-

ditional distribution,
its FIM is a function of the in-
put x. By taking an empirical average over the in-
put samples {xk}n
the FIM of a MLP can be ex-
(cid:3) ,
(cid:80)n
pressed as IΘ(Θ) = 1
n
where lk(Θ) = log p(y | xk, Θ) denotes the conditional
log-likelihood function wrt xk.

k=1 Ep(y | xk,Θ)

(cid:2) ∂lk
∂Θ

∂lk
∂Θ(cid:124)

k=1,

To understand the meaning of the Riemannian metric
IΘ(Θ), it measures the intrinsic difference between two
nearby neural networks around Θ ∈ M. A learning step
can be regarded as a tiny displacement δΘ on M. Accord-
ing to the FIM, the inﬁnitesimal square distance

(cid:104)δΘ, δΘ(cid:105)IΘ(Θ) =

Ep(y | xk, Θ)

1
n

n
(cid:88)

k=1

(cid:19)2(cid:35)

(cid:34)(cid:18)

δΘ(cid:124) ∂lk
∂Θ

(1)
measures how much δΘ (with a radius constraint) is sta-
tistically along ∂l
∂Θ , or equivalently how much δΘ affects
intrinsically the conditional distribution p(y | x, Θ).

the

negative

log-likelihood

function
Consider
L(Θ) = − (cid:80)n
k=1 log p(yk | xk, Θ) wrt
the observed
pairs {(xk, yk)}n
k=1, we try to minimize the loss while
maintaining a small learning step size (cid:104)δΘ, δΘ(cid:105)IΘ(Θ) on
M. At Θt ∈ M, the target is to minimize wrt δΘ the
Lagrange function

L(Θt + δΘ) +

1
2γ
≈ L(Θt) + δΘ(cid:124) (cid:53)Θ L(Θt) +

(cid:104)δΘ, δΘ(cid:105)IΘ(Θt)

1
2γ

δΘ(cid:124)IΘ(Θt)δΘ,

where γ > 0 is a learning rate. The optimal solution of the
above quadratic optimization gives a learning step

δΘt = −γI −1

Θ (Θt) (cid:53)Θ L(Θt).

In this update procedure, ˜∇ΘL(Θ) = I −1
Θ (Θ) (cid:53)Θ L(Θ)
replaces the role of the usual gradient ∇ΘL(Θ) and is
called the natural gradient (Amari, 1997).

Although the FIM depends on the chosen parameterization,
the natural gradient is invariant to reparameterization. Let
Λ be another coordinate system and J be the Jacobian ma-
trix of the mapping Θ → Λ. Then we have

Θ (Θ) (cid:53)Θ L(Θ) = (J (cid:124)IΛ(Λ)J )−1 J (cid:124) (cid:53)Λ L(Λ)
I −1
= J −1I −1

Λ (Λ) (cid:53)Λ L(Λ),

showing that ˜∇ΘL(Θ) and ˜∇ΛL(Λ) are the same dynamic
up to coordinate transformation. As the learning rate γ is
not inﬁnitesimal in practice, natural gradient descent actu-
ally depends on the coordinate system (see e.g. Martens
2014). Other intriguing properties of natural gradient opti-
mization lie in being free from getting trapped in plateaux
of the error surface, and attaining Fisher efﬁciency in on-
line learning (see Sec. 4 Amari 1998).

Relative Fisher Information and Natural Gradient

Model:

p(y | Θ, x)

= (cid:80)

(cid:80)

h1

h2

p(h1 | θ1, x)

p(h2 | θ2, h1)

p(y | θ3, h2)

Manifold:

MΘ

Computational graph:
x

Θ

Θ

y

Metric:

Θ I(Θ)

x + ∆x

x

Mθ1

x

h1 + ∆h1

h1

h1

Mθ2

θ1
θ1

θ2

θ2

h1

h2

θ3

θ3

h2

θ1 gh1 (θ1)

θ2

gh2 (θ2)

θ3

gy(θ3)

h1

h2

Mθ3

h2 + ∆h2
h2

y

Figure 1. (left) The traditional global geometry of a MLP; (right) information geometry of subsystems. The gray and blue meshes show
that the subsystem geometry is dynamic when the reference variable makes a tiny move. The square under the (sub-)system means the
(R-)FIM is computed by (i) computing the FIM in the traditional way wrt all free parameters that affect the system output; (ii) choosing
a sub-block that contains only the internal parameters of the (sub-)system and regarding the remaining variables as the reference.

For the sake of simplicity, we do not discuss singular FIMs
with a subset of parameters having zero metric. This set
of parameters forms an analytic variety (Watanabe, 2009),
and technically the MLP as a statistical model is said to
be non-regular (and the parameter Θ is not identiﬁable).
The natural gradient has been extended (Thomas, 2014)
to cope with singular FIMs having positive semi-deﬁnite
matrices by taking the Moore-Penrose pseudo-inverse (that
coincides with the inverse matrix for full rank matrices).

In the family of 2nd-order optimization methods, a fuzzy
line can be drawn from the natural gradient and alternative
methods such as the Hessian-free optimization (Martens,
2010). By deﬁnition, the FIM is a property of the parame-
ter space which is independent or weakly dependent on the
input samples. For example, the FIM of a MLP is indepen-
dent of {yi}. In contrast, the Hessian (or related concepts
such as the Gauss-Newton matrix, Martens 2014) is a prop-
erty of the learning cost function wrt the input samples.

Bonnabel (Bonnabel, 2013) proposed to use the Rieman-
nian exponential map to deﬁne a gradient descent step, thus
ensuring to stay on the manifold for any chosen learning
rate. Convergence is proven for Hadamard manifolds (of
negative curvatures). However, it is not mathematically
tractable to express the exponential map of hierarchical
model manifolds like the neuromanifold.

3. RFIM: Deﬁnition and Expressions

In general, for large parametric systems, it is impossible
to diagonalize or decorrelate all the parameters, so that we
split instead all random variables into three parts θf , θ and
h. We examine their intuitive meanings before giving the
formal deﬁnition. The reference, θf , consists of the major-
ity of the random variables that are considered ﬁxed (there-
fore allowing us to simplify the analysis). This is in anal-
ogy to the notion of a reference frame in physics. θ is the

subsystem parameters, resembling the long-term memory
adapting slowly to the observations (e.g. neural network
weights). The response h is a random variable that reacts
to the variations of θ. Usually, h is the output of the subsys-
tem that is connected to neighbour subsystems (e.g. hidden
layer outputs). Formally, a subsystem which factorizes the
learning machine is characterized by the conditional distri-
bution p(h | θ, θf ), where θ can be estimated based on h
and θf . We make the following deﬁnition.
Deﬁnition 1 (RFIM). Given θf , the RFIM 1 of θ wrt h is

gh (θ | θf )

def= Ep(h | θ, θf )

log p(h | θ, θf )

(cid:20) ∂
∂θ

(cid:21)
∂
∂θ(cid:124) log p(h | θ, θf )

,

or simply gh (θ), corresponding to the estimation of θ
based on observations of h given θf .

For example, consider a MLP. If we choose θf to be the
input features x, choose h to be the ﬁnal output y, and
choose θ to be all the network weights Θ, then the RFIM
becomes the FIM: I(Θ) = gy(Θ | x).

More generally, we can choose the response h to be other
than the observables to compute the Fisher information
of subsystems, especially dynamically during the learn-
ing of the global machine. To see the meaning of the
RFIM, similar to eq. (1), the inﬁnitesimal square distance
∂θ log p(h | θ, θf )(cid:1)2(cid:105)
(cid:104)δθ, δθ(cid:105)gh(θ) = Ep(h | θ, θf )
measures how much δθ impacts intrinsically the stochastic
mapping θ → h which features the subsystem. We have
the following proposition following deﬁnition 1.
Proposition 2 (Relative Geometry Consistency). If θ1 con-
sists of a subset of θ2 so that θ2 = (θ1, ˜θ1), then ∀ ˜θ1,
Mθ1 with the metric gh(θ1 | ˜θ1) has exactly the same Rie-

(cid:104)(cid:0)δθ(cid:124) ∂

1We use the same term “relative FIM” (Zegers, 2015) with a

different deﬁnition.

Relative Fisher Information and Natural Gradient

mannian metric with the sub-manifold {θ2 ∈ Mθ2
˜θ1 is ﬁxed} induced by the ambient metric gh (θ2).

:

When the response h is chosen, then different splits of
(θ, θf ) are consistent with the same ambient geometry.

Figure 1 shows the traditional global geometry of a learn-
ing system, where the curvature is deﬁned by the learner’s
parameter sensitivity to the external environment (x and
y), as compared to the information geometry of subsys-
tems, where the curvature is deﬁned by the parameter sen-
sitivity wrt hidden interface variables h. The two-colored
meshes show that the geometry structure is dynamic and
varies with the reference variable θf .

One should not confuse the RFIM with the diagonal blocks
of the FIM (Kurita, 1994). Both their meanings and ex-
pressions are different. The RFIM is computed by integrat-
ing out the hidden response variables h. The FIM is al-
ways computed by integrating out the observables x and
y. Hence the RFIM is a more general concept and in-
cludes the FIM as a special case. This highlights a main
difference with the backpropagated metric (Ollivier, 2013),
which essentially considers parameter sensitivity wrt the ﬁ-
nal output. Despite the fact that the FIMs of small paramet-
ric structures such as single neurons was studied (Amari,
1997), we are not looking at a small single-component sys-
tem but a component embedded in a large system, targeting
at improving the large system.

In the following we provide a short table of commonly used
RFIMs for future reference (the RFIMs listed are mostly
straightforward from deﬁnition 1, with detailed derivations
given in the supplementary material). This is meaningful
since the RFIM is a new concept. We also want to demon-
strate these simple closed form expressions without any ap-
proximations.

3.1. RFIMs of One Neuron

We start from the RFIM of single neuron models. Consider
a stochastic neuron with input x and weights w. After a
nonlinear activation function f , the output y is randomized
surrounding the mean f (w(cid:124) ˜x) with a variance. Through-
out this paper ˜x = (x(cid:124), 1)(cid:124) denotes the augmented vector
of x (homogeneous coordinates) so that w(cid:124) ˜x contains a
bias term, and a general linear transformation can be writ-
ten simply as A ˜x.

Using x as the reference, the RFIM of w with respect
to y has a common form gy(w | x) = νf (w, x) ˜x ˜x(cid:124),
where νf (w, x) is a positive coefﬁcient with large values
in the linear region, or the effective learning zone of the
neuron. This agrees with early studies on single neuron
FIMs (Amari, 1997; Kurita, 1994).

If f (t) = tanh(t) is the hyperbolic tangent func-

2

tion, then νf (w, x) = sech2(w(cid:124) ˜x), where sech(t) =
exp(t)+exp(−t) is the hyperbolic secant function. Simi-
larly, if f (t) = sigm(t) is the sigmoid function, then
νf (w, x) = sigm (w(cid:124) ˜x) (cid:2)1 − sigm (w(cid:124) ˜x) (cid:3).
If f is deﬁned by Parametric Rectiﬁed Linear Unit
(PReLU) (He et al., 2015), which includes Rectiﬁed Lin-
ear Unit (ReLU) (Nair & Hinton, 2010) as a special case,
so that f (t) = t (t ≥ 0), f (t) = ιt (t < 0), 0 ≤ ι < 1, then
under certain approximations (see supplementary material)

(cid:20)

νf (w, x) =

ι + (1 − ι)sigm

(cid:18) 1 − ι
ω

(cid:19)(cid:21)2

,

w(cid:124) ˜x

where ω > 0 is a hyper-parameter (e.g. ω = 1).

For the exponential linear unit (ELU) (Clevert et al., 2015),
f (t) = t (t ≥ 0), f (t) = α (exp(t) − 1) (t < 0), where
α > 0 is a hyper-parameter. We get

(cid:26) 1

νf (w, x) =

α2 exp (2w(cid:124) ˜x)

if w(cid:124) ˜x ≥ 0
if w(cid:124) ˜x < 0.

3.2. RFIM of One Layer

Let D denote the dimensionality of the corresponding vari-
able. A linear layer with input x, connection weights
W = (cid:2)w1, · · · , wDy
(cid:3), and stochastic output y can be
represented by y ∼ G(W (cid:124) ˜x, σ2I), where I is the iden-
tity matrix, and σ is the scale of the observation noise,
and G(µ, Σ) is a multivariate Gaussian distribution with
mean µ and covariance matrix Σ. We vectorize W by
stacking its columns {wi}. Then gy(W | x) is a tensor
of size (Dx + 1)Dy × (Dx + 1)Dy, given by gy(W | x) =
diag [ ˜x ˜x(cid:124), · · · , ˜x ˜x(cid:124)], where diag(·) means the (block)
diagonal matrix constructed by the given matrix entries.

A nonlinear layer increments a linear layer by adding an
element-wise activation function applied on W (cid:124) ˜x, and
then randomized wrt the choice of the neuron. By deﬁ-
nition 1, its RFIM is given by

gy (W | x)
= diag [ νf (w1, x) ˜x ˜x(cid:124), · · · , νf (wm, x) ˜x ˜x(cid:124) ] ,

(2)

where νf (wi, x) is given in Subsec. 3.1.

A softmax layer, which often appears as the last layer of
a MLP, is given by y ∈ {1, . . . , m}, where p(y) = ηy =

(cid:80)m



exp(wy ˜x)
i=1 exp(wi ˜x) . Its RFIM is a dense matrix given by
−η1ηm ˜x ˜x(cid:124)
1) ˜x ˜x(cid:124)
(η1 − η2
−η2ηm ˜x ˜x(cid:124)
−η2η1 ˜x ˜x(cid:124)
...
...
−ηmη1 ˜x ˜x(cid:124)
(ηm − η2

· · ·
· · ·
. . .
· · ·

gy(W ) =













.

m) ˜x ˜x(cid:124)
i ) ˜x ˜x(cid:124) resembles

Notice that its i’th diagonal block (ηi − η2
the RFIM of a single sigm neuron.

Relative Fisher Information and Natural Gradient

3.3. RFIM of Two Layers

By eq. (2), the one-layer RFIM is a product metric (Jost,
2011) and does not consider the inter-neuron correlations,
which must be obtained by looking at a larger subsys-
tem. Consider a two-layer model with stochastic output
y around the mean vector f (C(cid:124) ˜h), where h = f (W (cid:124) ˜x).
For simplicity, we ignore inter-layer correlations between
the ﬁrst layer and the second layer and focus on the inter-
neuron correlations within the ﬁrst layer. To do this, both x
and C are considered as references to compute the RFIM
of W . By deﬁnition 1, gy(W | x, C) = [Gij]Dh×Dh
and
each block has the form

Gij =

cilcjlνf (cl, h)νf (wi, x)νf (wj, x) ˜x ˜x(cid:124).

Dy
(cid:88)

l=1

Now that we have the one-layer and two-layer RFIMs,
we can either split a given feed-forward neural network
into one-layer subsystems or into two-layer subsystems.
A trade-off is that using a larger subsystem entails greater
analytical and computational difﬁculty, although it could
more accurately model the global system dynamics. In the
extreme case, the FIM is obtained if the whole system is
considered as one single subsystem.

4. RFIM: Key Advantages

This section discusses the theoretical advantages of the
RFIM over the FIM. Consider wlog a MLP with Bernoulli
outputs y ∈ {0, 1}m, whose mean µ is a deterministic
function depending on the input x and the network param-
eters Θ. By Sec. 2, the FIM of the MLP can be computed
as (see supplementary for proof)

n
(cid:88)

m
(cid:88)

i=1

j=1

1
n

I(Θ) =

∂µj(xi)
∂Θ

1
µj(xi)(1 − µj(xi))

∂µj(xi)
∂Θ(cid:124) .
(3)
Therefore rank(I(Θ)) ≤ nm. The rank of a diagonal
block of I(Θ) corresponding to one layer is even smaller.
In a deep neural network (e.g. Szegedy, Christian et al.
2015), if the sample size n < dim(Θ)/m, then I(Θ)
is doomed to be singular. All methods trying to approxi-
mate the FIM suffer from this problem and therefore rely
on proper regularizations. If the network is decomposed
into layers, the RFIM of each subsystem (layer) is given
by eq. (2). Each sample can contribute maximally 1 to
the rank of the neuron-RFIM and can contribute maxi-
mally Dy to the rank of the layer-RFIM. It only requires
maxi{dim(wi)} (the maximum layer width) observations
to have a full rank RFIM, where wi is the weight vector
of the i’th neuron. The RFIM is expected to have a much
higher rank than the FIM. Higher rank means less singu-
larity and more information is captured. Models that can

be distinguished by the RFIM may be identical in the sense
of the FIM. Essentially, the RFIM integrates the internal
randomness (Bengio, 2013) of the neural system by con-
sidering the output of each layer as a random variable. In
theory, the FIM should also consider stochastic neurons.
However it requires marginalizing the joint distribution of
h1, h2, · · · , y. This makes the already infeasible compu-
tation even more challenging.

The RFIM is not an approximation of the FIM but is an ac-
curate metric, deﬁning the geometry of θ wrt to its direct
response h in the system, or adjacent nodes in a graphical
model. By the example in ﬁg. 1, gy(θL) of the last layer is
exactly the corresponding block in I(Θ): they both char-
acterize how θL affects the mapping hL−1 → y. They
start to diverge from the second to last layer. To compute
the geometry of θL−1, the RFIM looks at how θL−1 af-
fects the local mapping hL−2 → hL−1, which can be mea-
sured reliably regardless of the rest of the system (think
of a “debugging” process to separate and measure a single
component). In contrast, the FIM examines how θL−1 af-
fects the non-local mapping hL−2 → y. This is a difﬁcult
task because it must consider the correlation between dif-
ferent layers. As an approximation, the block diagonalized
version of the FIM ignores such correlations and therefore
faces the loss of accuracy.

The RFIM makes it possible to maintain global system sta-
bility so that the intrinsic variations of different subsystems
are balanced during learning. Consider a set of intercon-
nected subsystems with internal parameters {θl} and the
corresponding response variables {hl}. The RFIM ghl (θl)
measures how much the likelihood surface of hl is curved
wrt a small learning step δθl. By constraining the squared
(cid:124)
l ghl (θl)δθl having similar scales,
Riemannian distance δθ
different subsystems will present similar variations during
learning. Within one subsystem, the learning along sen-
sitive parameter directions is penalized. Among different
subsystems, the learning of sensitive subsystems is penal-
ized. Globally, the inter-subsystem stochastic connections
have similar variance, maintaining a stable reference sys-
tem and achieving efﬁcient learning. This is similar to the
idea of batch normalization (BN) (Ioffe & Szegedy, 2015)
but has a deeper theoretical foundation.

Formally, we have the following theorem.

Theorem 3. Consider a learning system represented by a
joint distribution p(x, h) of x (observables) and h (hid-
den variables which connect subsystems). The joint FIM
has a block diag-
J (Θ) = Ep
onal form. Each block is Ep(gh(θ)), where θ is the param-
eters within a subsystem and h is its response variables to
neighour subsystems.

(cid:16) log p(x,h | Θ)
∂Θ

log p(x,h | Θ)
∂Θ(cid:124)

(cid:17)

The global correspondence of the local RFIM is the joint

Relative Fisher Information and Natural Gradient

l dθ

FIM. By theorem 3, the square distance dΘ(cid:124)J (Θ)dΘ =
(cid:124)
Ep((cid:80)
l ghl (θl)dθl) measures the system variance, in-
cluding both the observables x and the hidden variables
h. An intrinsic trade-off between the RFIM and the FIM
is learning system stability versus efﬁciency. Normaliz-
ing the FIM is more efﬁcient because it helps to achieve
Fisher efﬁciency (Amari, 1998). Normalizing the RFIM is
more stable since the hidden variations are bounded, which
only guarantees subsystem Fisher efﬁciency characterized
by the Cram´er-Rao lower bound of local parameters.

5. Relative Natural Gradient Descent

The traditional non-parametric way of applying natural
gradient requires re-calculating the FIM and solving a large
linear system in each learning step. Besides the huge com-
putational cost, it has a large approximation error. For ex-
ample during online learning, a mini-batch of samples can-
not faithfully reﬂect the “true” geometry, which has to in-
tegrate the risk of sample variations. That is, the FIM of a
mini-batch is likely to be singular or poorly conditioned.

A recent series of efforts (Montavon & M¨uller, 2012; Raiko
et al., 2012; Desjardins et al., 2015) are gearing towards
a parametric approach to applying natural gradient, which
memorizes and learns a geometry. For example, natural
neural networks (Desjardins et al., 2015) augment each
layer with a redundant linear layer, and let these linear lay-
ers parametrize the geometry of the neural manifold.

i D2

By dividing the learning system into subsystems, the RFIM
potentially gives a systematical implementation of para-
metric natural gradient descent. The memory complexity
of storing the Riemannian metric has been reduced from
O(D2) to O((cid:80)
i ), where Di = dim(wi) is the size
of the i’th neuron. Consider there are M neurons in total,
then the memory cost is reduced by a factor of M . The
computational complexity has been reduced from O(D(cid:37))
((cid:37) ≈ 2.373, Williams 2012) to O((cid:80)
i ). Optimization
based on RFIM is called Relative Natural Gradient Descent
(RNGD).

i D(cid:37)

The good performance of batch normalization (Ioffe &
Szegedy, 2015) provides an empirical support for the
RFIM. Basically, BN uses an inter-sample normalization
layer to transform the layer input x to z with zero mean and
unit variance and thus reduces “internal covariate shift”. In
a typical case, above this normalization layer is a linear
layer given by y = W (cid:124) ˜z. If each dimension of z is nor-
malized, then the diagonal blocks of the linear layer RFIM
gy(W ) = diag[ ˜z ˜z(cid:124), · · · , ˜z ˜z(cid:124)] become a covariance ma-
trix with identity diagonal entries (after taking an empirical
average). This gives the coordinate system W a well con-
ditioned RFIM for efﬁcient learning.

5.1. RNGD with a relu MLP

This subsection builds a proof-of-concept experiment on
MLP optimization. We partition the MLP into layers (one
layer consists of a linear layer plus an element-wise non-
linear activation function) as the subsystems. By eq. (2),
the RFIM of layer l (l = 1, · · · , L) with input hl−1 (h0 =
x) and weights {wl1, · · · , wlml } is

(cid:104)

diag

νf (wl1, hl−1)˜hl−1 ˜h

(cid:124)

l−1, · · · , νf (wlml , hl−l)˜hl−1 ˜h

(cid:124)
l−1

(cid:105)

.

l=1

one

The

(cid:80)ml

during

subsystem stability

(cid:124)
i=1 νf (wli, hl−1)(δw
li

learn-
ing step δw can be measured geometrically by
(cid:80)L
Using this
term as the geometric cost (the Lagrange term) in the trust
region approach in Sec. 2, we get the following RNGD
method.
In a stochastic gradient descent scenario, each
neuron i in layer l is updated by

˜hl−1)2.

wnew

li ← wold

li − G−1
li

∂E
∂wli

,

where E is the cost function and Gli is a learned metric.
The consideration is that a mini-batch of samples do not
contain enough information to compute the RFIM, which
should be averaged over all training samples. Therefore,
for the i’th neuron in layer l, Gli is initialized to identity,
and is updated based on

Gnew

li ← (1 − λ)Gold

li + λνf (wli, hl−1)˜hl−1

(cid:124)
˜h
l−1 + (cid:15)I,

where (cid:15) > 0 is a hyper-parameter to avoid singularity
caused by small sample size, and the average is taken over
all samples in a mini-batch, and λ is a learning rate.
In
theory, λ should be gradually reduced to zero to guarantee
the convergence of this geometry learning. To avoid solv-
ing a linear system in each iteration, every T iterations we
recompute and store G−1
li based on the most updated Gli.
In the next T iterations, this G−1
li will be used as an ap-
proximation of the inverse RFIM. For the input layer which
scales with the number of input features, and the ﬁnal soft-
max layer, we apply instead the RFIM of the corresponding
linear layer to improve the computational efﬁciency.

We compare different optimizers on classifying MNIST
digits. The network has shape 784-80-80-80-10, with relu
activation units, a ﬁnal soft-max layer, and uses the per-
sample average cross-entropy with L2-regularization as the
learning cost function. We experiment on two different ar-
chitectures: one is a plain MLP (PLAIN); the other has a
batch normalization layer after each hidden layer (BNA),
where a rescaling parameter is applied to ensure enough
ﬂexibility of the parametric structure (Ioffe & Szegedy,
2015). For simplicity, the architecture, mini-batch size
(50), and L2 regularization strength (10−3) are ﬁxed to be
the same for all compared methods. The observations are
consistent when these conﬁgurations vary.

Relative Fisher Information and Natural Gradient

formative samples for each output neuron are centered and
decorrelated.

In the above experiment, RNGD’s computational time per
each epoch is roughly 4 ∼ 10 times more than SGD and
ADAM on a modern graphic card. Therefore in terms of
wall clock time RNGD does not show advantages. This
can be improved by more efﬁcient implementations with
low rank approximation techniques and early stopping. Our
RNGD prototype hints at a promising direction to develop
scalable 2nd-order deep learning optimizers based on the
RFIM.

6. Related Works on FIM Diagonalization

One may ponder whether we can always ﬁnd a suitable pa-
rameterization that yields a diagonal FIM that is straight-
forward to invert. This fundamental problem of parameter
orthogonalization was ﬁrst investigated by Jeffreys (1998)
for decorrelating the parameters of interest from the nui-
sance parameters. Fisher diagonalization yields parameter
orthogonalization (Cox & Reid, 1987), and is proved use-
ful when estimating ˆΘ using a maximum likelihood esti-
mator (MLE) that is asymptotically normally distributed,
ˆΘn ∼ G(Θ, I −1(Θ)/n), and efﬁcient since the variance
of the estimator matches the Cram´er-Rao lower bound. Us-
ing the chain rule, this amounts to ﬁnd a suitable parame-
terization Ω = Ω(Θ) satisfying
(cid:21) ∂Θi
∂Ωk

∂2l
∂Θi∂Θj

∂Θj
∂Ωl

∀k (cid:54)= l.

= 0,

(cid:88)

E

(cid:20)

i,j

2

2

(cid:1) = D(D−1)

Thus in general, we end up with (cid:0)D
(non-
linear) partial differential equations to satisfy (Huzurbazar,
1950). Therefore, in general there is no solution when
(cid:1) > D, that is when D > 3. When D = 2, the sin-
(cid:0)D
2
gle differential equation is usually solvable and tractable,
and the solution may not be unique: For example, Huzur-
bazar (1950) reports two orthogonalization schemes for the
σ p0( x−µ
location-scale families { 1
σ )} that include the Gaus-
sian family and the Cauchy family. Sometimes, the struc-
ture of the differential equation system yields a solution:
For example, Jeffreys (1998) reported a parameter orthog-
onalization for Pearson’s distributions of type I which is of
order D = 4. Cox and Reid (1987) further investigated this
topic with application to conditional inference, and provide
examples (including the Weibull distribution).

From the viewpoint of geometry, the FIM induces a Rie-
mannian manifold with metric tensor g(Θ) = I(Θ).
When the FIM may be degenerate, this yields a pseudo-
In differential
Riemannian manifold (Thomas, 2014).
geometry, orthogonalization amounts to transforming the
square length inﬁnitesimal element gijdΘiΘj of a Rieman-
nian geometry into an orthogonal system ω with match-

(a) A plain MLP with L2 regularization

(b) A MLP with batch normalization and L2
regularization

Figure 2. Learning curves of different optimizers on a MLP with
two different architectures (with and without BN). The best learn-
ing rate for each method is selected based on the validation accu-
racy. Using this learning rate, the learning curves wrt 40 different
random initializations are shown. The mean validation curve is
shown for a clear visualization.

Figure 2 shows the learning curves of different methods.
SGD is stochastic gradient descent. ADAM is the Adam
optimizer (Kingma & Ba, 2014) with β1 = 0.9, β2 = 0.999
and (cid:15) = 10−8. Our RNGD is implemented by modifying
TensorFlow’s (Abadi, Mart´ın et al., 2015) SGD optimizer.
We set empirically T = 100, λ = 0.005 and ω = 1.

RNGD presents a sharper learning curve and better gener-
alization, especially when it is combined with BN. In this
case, the ﬁnal tranining error of RNGD is slightly larger
than ADAM because by validation it favors a larger learn-
ing rate, which is applied on the neural network weights
(based on RNGD) and BN parameters (based on SGD). For
the ReLU activation, νf (wi, x) is approximately binary,
(cid:124)
i ˜x > 0,
emphasizing such informative samples with w
which are the ones contributing to the learning of wi with
non-zero gradient values. Each output neuron has a dif-
ferent subset of informative samples. RNGD normalizes
x differently wrt different output neurons, so that the in-

0.9660.9680.9700.9720.9740.976accuracy020406080100#epochs0.100.150.200.250.300.350.40errorPLAIN+SGD (train)PLAIN+SGD (valid)PLAIN+ADAM (train)PLAIN+ADAM (valid)PLAIN+RNGD (train)PLAIN+RNGD (valid)0.9700.9710.9720.9730.9740.9750.9760.9770.978accuracy020406080100#epochs0.10.20.30.40.5errorBNA+SGD (train)BNA+SGD (valid)BNA+ADAM (train)BNA+ADAM (valid)BNA+RNGD (train)BNA+RNGD (valid)Relative Fisher Information and Natural Gradient

ing square length inﬁnitesimal element ΩiidΩ2
i . However,
such a global orthogonal metric does not exist (Huzurbazar,
1950) when D > 3 for an arbitrary metric tensor, although
interesting Riemannian parameterization structures may be
derived in Riemannian 4D geometry (Grant & Vickers,
2009).

Our work applies to mirror descent as well since natural
gradient is related to mirror descent (Raskutti & Mukher-
jee, 2015) as follows: In mirror descent to minimize a cost
function E(Θ), given a strictly convex distance function
D(·, ·) in the ﬁrst argument (playing the role of the prox-
imity function), we express the gradient descent step as:

the FIM can be made block-diagonal eas-
For NEFs,
ily by using the mixed coordinate system (Amari, 2016)
(Θ1:k, Hk+1:D), where H = Ep[t(x)] = ∇F (Θ) is the
moment parameter, for any k ∈ {1, ..., D − 1}, where vb:e
denotes the subvector (vb, ..., ve)(cid:124) of v. The geometry of
NEFs is a dually ﬂat structure (Amari, 2016) induced by the
convex mgf, the potential function. It deﬁnes a dual afﬁne
and ej = ∂j = ∂
coordinate systems ei = ∂i = ∂
∂Θj
∂Hi
that are orthogonal: (cid:104)ei, ej(cid:105) = δi
j, where δi
j = 1 iff i = j
and δi
j = 0 otherwise. Hence the FIM has two diagonal
blocks. Those dual afﬁne coordinate systems are deﬁned
up to an afﬁne invertible transformation: ˜Θ = AΘ + b,
˜H = A−1H + c.
In particular, for any order-2 NEF
(D = 2), we can always obtain two mixed parameteriza-
tions (Θ1, H2) or (H1, Θ2).

The RFIM contributes another line of thought in parameter
diagonalization. We investigate the Fisher information of
hidden variables, or internal interfaces in the learning ma-
chine. This is novel since the majority of previous works
concentrate on the FIM of the observables, or the external
interface of the machine. From a causality perspective, we
factor out the main cause (parameters within the subsys-
tem) of the response variable with a direct action-reaction
relationship, and regard the remaining parameters as a ref-
erence that can be easily estimated by the empirical dis-
tribution. This simpliﬁcation may lead to broader applica-
tions of Fisher information in machine learning.

The particular case of a mixed coordinate system (that is
not an afﬁne coordinate system) induces in information ge-
ometry (Amari, 2016) a dual pair of orthogonal e- and m-
orthogonal foliations. Our splits in RFIMs consider general
non-orthogonal foliations that provide the factorization de-
compositions of the whole manifold into submanifolds, that
are the leaves of the foliation (see section 3.7 of Amari &
Nagaoka 2000).

7. Conclusion and Discussions

We investigate local structures of large learning systems us-
ing the new concept of Relative Fisher Information Metric.
The key advantage of this approach is that the local learning
dynamics can be analyzed in an accurate way without ap-
proximation. We present a core list of such local structures
in neural networks, and give their corresponding RFIMs.
This list of recipes can be used to provide guiding princi-
ples to design new optimizers for deep learning.

(cid:26)

Θt+1 = arg min
Θ

Θ(cid:62)∇E(Θt) +

D(Θ, Θt)

.

(cid:27)

1
γ

When D(Θ, Θ(cid:48)) is chosen as a Bregman divergence
BF (Θ, Θ(cid:48)) = F (Θ) − F (Θ(cid:48)) − (Θ − Θ(cid:48))(cid:62)∇F (Θ(cid:48)) wrt
to a convex function F , it has been proved that the mirror
descent on the Θ-parameterization is equivalent (Raskutti
& Mukherjee, 2015) to the natural gradient optimization
on the induced Riemannian manifold with metric tensor
(∇2F (Θ)) parameterized by the dual coordinate system
H = ∇F (Θ).

In general, to perform a Riemannian gradient descent for
minimizing a real-valued function f (Θ) on the manifold,
one needs to choose a proper metric tensor given in ma-
trix form G(Θ). Thomas (2014) constructed a toy exam-
ple showing that the natural gradient may diverge while
the ordinary gradient (for G = I) converges. Recently,
Thomas et al. (2016) proposed a new kind of descent
method based on what they called the Energetic Natural
Gradient that generalizes the natural gradient. The en-
ergy distance DE(p(Θ1), p(Θ2))2 = E[2dp(Θ1)(X, Y ) −
dp(Θ1)(X, X (cid:48)) − dp(Θ1)(Y, Y (cid:48))] where X, X (cid:48) ∼ p(Θ1)
and Y, Y (cid:48) ∼ p(Θ2), where dp(Θ1)(·, ·) is a distance met-
ric over the support. Using a Taylor’s expansion on their
energy distance, they get the Energy Information Matrix
(in a way similar to recovering the FIM from a Taylor’s
expansion of any f -divergence like the Kullback-Leibler
divergence). Their idea is to incorporate prior knowledge
on the structure of the support (observation space) to deﬁne
energy distance. Twisting the geometry of the support (say,
Wasserstein’s optimal transport) with the geometry of the
parametric distributions (Fisher-Rao geodesic distances) is
indeed important (Chizat et al., 2015). In information ge-
ometry, invariance on the support is provided by a Markov
morphism that is a probabilistic mapping of the support to
itself ( ˇCencov, 1982). There is no neighbourhood structure
on the support in IG. Markov morphism includes determin-
istic transformation of a random variable by a statistic. It is
well-known that IT (Θ) (cid:22) IX (Θ) with equality iff. T =
T (X) is a sufﬁcient statistic of X. Thus to get the same in-
variance for the energy distance (Thomas et al., 2016), one
shall further require dp(Θ)(T (X), T (Y )) = dp(Θ)(X, Y ).

We believe that RFIMs will provide a sound methodology
to build further efﬁcient systems for deep learning. The
full source codes to reproduce the experimental results are
available at https://www.lix.polytechnique.
fr/˜nielsen/RFIM.

Relative Fisher Information and Natural Gradient

Acknowledgements

The authors would like to thank the anonymous reviewers
and Yann Ollivier for the helpful comments. This work was
mainly conducted when the ﬁrst author was a postdoctoral
researcher at ´Ecole Polytechnique.

References

Abadi, Mart´ın et al. TensorFlow: Large-scale machine
Software

learning on heterogeneous systems, 2015.
available from tensorflow.org.

Amari, Shun’ichi. Information geometry of the EM and em
algorithms for neural networks. Neural Networks, 8(9):
1379–1408, 1995.

Amari, Shun’ichi. Neural learning in structured parameter
In NIPS 9, pp.

spaces – natural Riemannian gradient.
127–133. MIT Press, 1997.

Amari, Shun’ichi. Natural gradient works efﬁciently in

learning. Neural Comput., 10(2):251–276, 1998.

Amari, Shun’ichi.

Information Geometry and its Appli-
cations, volume 194 of Applied Mathematical Sciences.
Springer Japan, 2016.

Amari, Shun’ichi and Nagaoka, Hiroshi. Methods of Infor-
mation Geometry, volume 191 of Translations of Mathe-
matical Monographs. AMS and OUP, 2000. (Published
in Japanese in 1993).

Bengio, Yoshua.

Estimating or propagating gradients
through stochastic neurons. CoRR, abs/1305.2982, 2013.

Bonnabel, Silv`ere. Stochastic gradient descent on Rieman-
IEEE Trans. Automat. Contr., 58(9):

nian manifolds.
2217–2229, 2013.

ˇCencov, Nikolaˇı Nikolaevich. Statistical decision rules and
optimal inference, volume 53 of Translations of Mathe-
matical Monographs. American Mathematical Society,
1982. Translation from the Russian (published in 1972)
edited by Lev J. Leifman.

Chizat, Lenaic, Schmitzer, Bernhard, Peyr´e, Gabriel, and
Vialard, Franc¸ois-Xavier. An Interpolating Distance be-
tween Optimal Transport and Fisher-Rao. arXiv e-prints,
2015. 1506.06430 [math.AP].

Clevert, Djork-Arn´e, Unterthiner, Thomas, and Hochreiter,
Sepp. Fast and accurate deep network learning by ex-
ponential linear units (ELUs). CoRR, abs/1511.07289,
2015.

Cobb, Loren, Koppstein, Peter, and Chen, Neng Hsin. Es-
timation and moment recursion relations for multimodal
distributions of the exponential family. JASA, 78(381):
124–130, 1983.

Cox, D. R. and Reid, N. Parameter orthogonality and ap-
proximate conditional inference. Journal of the Royal
Statistical Society. Series B (Methodological), 49(1):1–
39, 1987.

Cram´er, Harald. Mathematical Methods of Statistics, vol-
ume 9 of Princeton Mathematical Series. Princeton Uni-
versity Press, 1946.

Desjardins, Guillaume, Simonyan, Karen, Pascanu, Raz-
van, and Kavukcuoglu, Koray. Natural neural networks.
In NIPS 28, pp. 2071–2079. Curran Associates, Inc.,
2015.

Dowty, James G. Chentsov’s theorem for exponential fam-
ilies. arXiv preprints, 2017. 1701.08895 [math.ST].

Efron, Bradley and Hinkley, David V. Assessing the ac-
curacy of the maximum likelihood estimator: Observed
versus expected Fisher information. Biometrika, 65(3):
457–487, 1978.

Fr´echet, Maurice. Sur l’extension de certaines evalua-
tions statistiques au cas de petits echantillons. Revue de
l’Institut International de Statistique / Review of the In-
ternational Statistical Institute, 11(3/4):182–205, 1943.

Grant, James DE and Vickers, JA. Block diagonalization of
four-dimensional metrics. Classical and Quantum Grav-
ity, 26(23):235014, 2009.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Delving deep into rectiﬁers: Surpassing human-
level performance on ImageNet classiﬁcation. In ICCV,
2015.

Hinton, Geoffrey E., Osindero, Simon, and Teh, Yee-
Whye. A fast learning algorithm for deep belief nets.
Neural Comput., 18(7):1527–1554, 2006.

Hotelling, Harold. Spaces of statistical parameters. Ameri-
can Mathematical Society Meeting, 1929. (unpublished.
Presented orally by O. Ore during the meeting).

Huzurbazar, Vasant Shankar. Probability distributions and
orthogonal parameters. Mathematical Proceedings of
the Cambridge Philosophical Society, 46(02):281–284,
1950.

Ioffe, Sergey and Szegedy, Christian. Batch normalization:
Accelerating deep network training by reducing internal
In ICML; JMLR: W&CP 37, pp. 448–
covariate shift.
456, 2015.

Jeffreys, Harold. Theory of Probability. Oxford Classic
Texts in the Physical Sciences. OUP, 3rd edition, 1998.
First published in 1939.

Relative Fisher Information and Natural Gradient

Pascanu, Razvan and Bengio, Yoshua. Revisiting nat-
In ICLR, 2014.

ural gradient for deep networks.
arXiv:1301.3584 [cs.LG].

Raiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep
learning made easier by linear transformations in per-
ceptrons. In AISTATS; JMLR W&CP 22, pp. 924–932,
2012.

Rao, Calyampudi Radhakrishna.

Information and accu-
racy attainable in the estimation of statistical parameters.
Bull. Cal. Math. Soc., 37(3):81–91, 1945.

Raskutti, Garvesh and Mukherjee, Sayan. The information
In Geometric Science of
geometry of mirror descent.
Information (GSI), volume 9389 of LNCS, pp. 359–368.
Springer, 2015.

Sun, Ke and Marchand-Maillet, St´ephane. An informa-
tion geometry of statistical manifold learning. In ICML;
JMLR W&CP 32(2), pp. 1–9, 2014.

Szegedy, Christian et al. Going deeper with convolutions.

In CVPR, 2015.

Thomas, Philip. GeNGA: A generalization of natural gra-
dient ascent with positive and negative convergence re-
sults. In ICML; JMLR W&CP 32, pp. 1575–1583, 2014.

Thomas, Philip, da Silva, B. C., Dann, C., and Brunskill,
E. Energetic natural gradient descent. In ICML, 2016.

Wager, Stefan, Wang, Sida, and Liang, Percy S. Dropout
training as adaptive regularization. In NIPS 26, pp. 351–
359. Curran Associates, Inc., 2013.

Watanabe, Sumio. Algebraic Geometry and Statistical
Learning Theory, volume 25 of Cambridge Monographs
on Applied and Computational Mathematics. CUP,
2009.

Williams, Virginia Vassilevska. Multiplying matrices faster
In Annual ACM Sympo-
than Coppersmith-Winograd.
sium on Theory of Computing, STOC’12, pp. 887–898,
2012.

Zegers, Pablo. Fisher information properties. Entropy, 17:

4918–4939, 2015.

Jost, J¨urgen. Riemannian Geometry and Geometric Analy-

sis. Springer, 6th edition, 2011.

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for
stochastic optimization. CoRR, abs/1412.6980, 2014.

Kingma, Diederik P and Welling, Max. Auto-encoding
arXiv:1312.6114

In ICLR, 2014.

variational Bayes.
[stat.ML].

Kurita, Takio. Iterative weighted least squares algorithms
for neural networks classiﬁers. New Generation Com-
puting, 12(4):375–394, 1994.

Le Roux, Nicolas, Manzagol, Pierre-Antoine, and Ben-
gio, Yoshua. Topmoumoute online natural gradient al-
gorithm. In NIPS 20, pp. 849–856. Curran Associates,
Inc., 2008.

Lebanon, Guy. Riemannian geometry and statistical ma-

chine learning. PhD thesis, CMU, 2005.

Looks, Moshe, Herreshoff, Marcello, Hutchins, DeLesley,
and Norvig, Peter. Deep learning with dynamic com-
In ICLR, 2017.
putation graphs.
arXiv:1702.02181
[cs.NE].

Marceau-Caron, Ga´etan and Ollivier, Yann. Practical Rie-
mannian neural networks. CoRR, abs/1602.08007, 2016.

Martens, James. Deep learning via Hessian-free optimiza-

tion. In ICML, pp. 735–742, 2010.

Martens, James. New perspectives on the natural gradient

method. CoRR, abs/1412.1193, 2014.

Martens, James and Grosse, Roger. Optimizing neural net-
works with Kronecker-factored approximate curvature.
In ICML; JMLR: W&CP 37, pp. 2408–2417, 2015.

Montanari, Andrea. Computational implications of reduc-
ing data to sufﬁcient statistics. Electron. J. Statist., 9(2):
2370–2390, 2015.

Montavon, Gr´egoire and M¨uller, Klaus-Robert. Deep
Boltzmann machines and the centering trick. In Neural
Networks: Tricks of the Trade, pp. 621–637. Springer
Berlin Heidelberg, 2nd edition, 2012.

Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units
improve restricted Boltzmann machines. In ICML, pp.
807–814, 2010.

Nielsen, Frank. Cram´er-Rao lower bound and information

geometry. CoRR, abs/1301.3578, 2013.

Ollivier, Yann. Riemannian metrics for neural networks.

CoRR, abs/1303.0818, 2013.

