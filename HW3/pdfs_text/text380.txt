Algorithmic Stability and Hypothesis Complexity

Tongliang Liu 1 G´abor Lugosi 2 3 4 Gergely Neu 5 Dacheng Tao 1

Abstract
We introduce a notion of algorithmic stability of
learning algorithms—that we term argument sta-
bility—that captures stability of the hypothesis
output by the learning algorithm in the normed
space of functions from which hypotheses are se-
lected. The main result of the paper bounds the
generalization error of any learning algorithm in
terms of its argument stability. The bounds are
based on martingale inequalities in the Banach
space to which the hypotheses belong. We apply
the general bounds to bound the performance of
some learning algorithms based on empirical risk
minimization and stochastic gradient descent.

1. Introduction

Many efforts have been made to analyze various notions
of algorithmic stability and prove that a broad spectrum of
learning algorithms are stable in some sense. Intuitively,
a learning algorithm is said to be stable if slight pertur-
bations in the training data result in small changes in the
output of the algorithm, and these changes vanish as the
data set grows bigger and bigger (Bonnans & Shapiro,
2013). For example, Devroye & Wagner (1979), Lugosi
& Pawlak (1994), and Zhang (2003) showed that several
non-parametric learning algorithms are stable; Bousquet &
Elisseeff (2002) proved that (cid:96)2 regularized learning algo-
rithms are uniformly stable; Wibisono et al. (2009) gener-
alized Bousquet and Elisseeff’s results and proved that reg-
ularized learning algorithms with strongly convex penalty
functions on bounded domains, e.g., (cid:96)p regularized learn-
ing algorithms for 1 < p ≤ 2, are also uniformly stable;

1UBTech Sydney AI Institute, School of IT, FEIT, The Uni-
versity of Sydney, Australia 2Department of Economics and
Business, Pompeu Fabra University, Barcelona, Spain 3ICREA,
Pg. Llus Companys 23, 08010 Barcelona, Spain 4Barcelona
Graduate School of Economics 5AI group, DTIC, Universi-
tat Pompeu Fabra, Barcelona, Spain.
Correspondence to:
Tongliang Liu <tliang.liu@gmail.com>, G´abor Lugosi <ga-
bor.lugosi@upf.edu>, Gergely Neu <gergely.neu@gmail.com>,
Dacheng Tao <dacheng.tao@sydney.edu.au>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright
2017 by the author(s).

Hardt et al. (2015) showed that parametric models trained
by stochastic gradient descent algorithms are uniformly
stable; and Liu et al. (2017) proved that tasks in multi-task
learning can act as regularizers and that multi-task learning
in a very general setting will therefore be uniformly stable
under mild assumptions.

The notion of algorithmic stability has been an important
tool in deriving theoretical guarantees of the generalization
abilities of learning algorithms. Various notions of stabil-
ity have been introduced and have been exploited to de-
rive generalization bounds. For some examples, Mukherjee
et al. (2006) proved that a statistical form of leave-one-out
stability is a sufﬁcient and necessary condition for the gen-
eralization and learnability of empirical risk minimization
learning algorithms; Shalev-Shwartz et al. (2010) deﬁned
a weaker notion, the so-called “on-average-replace-one-
example stability”, and showed that this condition is both
sufﬁcient and necessary for the generalization and learn-
ability of a general learning setting.

In this paper we study learning algorithms that select a hy-
pothesis (i.e., a function used for prediction) from a certain
ﬁxed class of functions belonging to a separable Banach
space. We introduce a notion of argument stability which
measures the impact of changing a single training exam-
ple on the hypothesis selected by the learning algorithm.
This notion of stability is stronger than uniform algorith-
mic stability of Bousquet & Elisseeff (2002) that is only
concerned about the change in the loss but not the hypothe-
sis itself. However, as we will show, the new notion is still
quite natural and holds for a variety of learning algorithms.
On the other hand, it allows one to exploit martingale in-
equalities (Boucheron et al., 2013) in the Banach space of
the hypotheses. Indeed, the performance bounds we derive
for stable algorithms depend on characteristics related to
the martingale type of the Banach space.

Generalization bounds typically depend on the complexity
of a class of hypotheses that can be chosen by the learning
algorithm. Exploiting the local estimates of the complex-
ity of the predeﬁned hypothesis class is a promising way
to obtain sharp bounds. Building on martingale inequal-
ities in the Banach space of the hypotheses, we deﬁne a
subset of the predeﬁned hypothesis class, whose elements
will (or will have a high probability to) be output by a

Algorithmic Stability and Hypothesis Complexity

ization error is deﬁned as

learning algorithm, as the algorithmic hypothesis class, and
study the complexity of the algorithmic hypothesis class of
argument-stable learning algorithms. We show that, if the
hypotheses belong to a Hilbert space, the upper bound of
the Rademacher complexity of the algorithmic hypothesis
class will converge at a fast rate of order O(1/n), where n
is the sample size.

The rest of the paper is organized as follows. Section 2
introduces the mathematical framework and the proposed
notion of algorithmic stability. Section 3 presents the main
results of this study, namely the generalization bounds in
terms of argument stability. Section 4 specializes the re-
sults to some learning algorithms, including empirical risk
minimization and stochastic gradient descent. Section 5
concludes the paper.

2. Algorithmic Stability and Hypothesis Class

We consider the classical statistical
learning problem,
where the value of a real random variable Y is to be pre-
dicted based on the observation of an another random vari-
able X. Let S be a training sample of n i.i.d. pairs of ran-
dom variables Z1 = (X1, Y1), . . . , Zn = (Xn, Yn) drawn
from a ﬁxed distribution P on a set Z = X × Y, where
X is the so-called feature space. A learning algorithm
A : S ∈ Z n (cid:55)→ hS ∈ H is a mapping from Z n to a
hypothesis class H that we assume to be a subset of a sepa-
rable Banach space (B, (cid:107)·(cid:107)). We focus on linear prediction
problems, that is, when h(x) is a linear functional of x. We
write h(x) = (cid:104)h, x(cid:105). In other words, we assume that the
feature space X is the algebraic dual of the Banach space
B. We denote the norm in X by (cid:107) · (cid:107)∗. The output hS of
the learning algorithm is a hypothesis used for predicting
the value for Y .

An important special case is when B is a Hilbert space. In
that case we may assume that X = B and that (cid:104)h, x(cid:105) is the
inner product in B.

The quality of the predictions made by any hypothesis will
be measured by a loss function (cid:96) : B × Z → R+ (where
R+ denotes the set of positive reals). Speciﬁcally, (cid:96)(h, Z)
measures the loss of predicting an example Z using a hy-
pothesis h.

The risk of h ∈ H is deﬁned by

R(h) = E(cid:96)(h, Z) ;

while the empirical risk is

RS(h) =

(cid:96)(h, Zi) .

1
n

n
(cid:88)

i=1

For the output hS of a learning algorithm A, the general-

R(hS) − RS(hS) .

(1)

The notion of algorithmic stability was proposed to mea-
sure the changes of outputs of a learning algorithm when
the input is changed. Various ways have been intro-
duced to measure algorithmic stability. Here we re-
the notion of uniform stability deﬁned by Bous-
call
quet & Elisseeff (2002) for comparison purposes. This
notion of stability relies on the altered sample Si =
{Z1, . . . , Zi−1, Z (cid:48)
i, Zi+1, . . . , Zn}, the sample S with the
i-th example being replaced by an independent copy of Zi.
Deﬁnition 1 (Uniform Stability). A learning algorithm A
is β(n)-uniformly stable with respect to the loss function (cid:96)
if for all i ∈ {1, . . . , n},

|(cid:96)(hS, Z) − (cid:96)(hSi, Z)| ≤ β(n) ,

with probability one, where β(n) ∈ R+ .

We propose the following, similar, notion that “acts” on the
hypotheses directly, as opposed to the losses.

Deﬁnition 2 (Uniform Argument Stability). A learning al-
gorithm A is α(n)-uniformly argument stable if for all
i ∈ {1, . . . , n},

(cid:107)hS − hSi(cid:107) ≤ α(n) .

with probability one, where α(n) ∈ R+ .

The two notions of stability are closely related: Intuitively,
if the loss (cid:96)(h, z) is a sufﬁciently smooth function of h,
then uniform argument stability should imply uniform sta-
bility. To make this intuition precise, we deﬁne the notion
of Lipschitz-continuous loss functions below.

Deﬁnition 3 (L-Lipschitz Loss Function). The loss func-
tion (cid:96) : B × Z → R+ is L-Lipschitz for an L > 0 if

|(cid:96)(h, z) − (cid:96)(h(cid:48), z)| ≤ L |(cid:104)h, x(cid:105) − (cid:104)h(cid:48), x(cid:105)|

holds for all z ∈ Z and h, h(cid:48) ∈ H.

Additionally assuming that (cid:107)X(cid:107)∗ is bounded by some
B > 0 with probability one, it is easy to see that an α(n)-
uniformly argument stable learning algorithm is uniformly
stable with β(n) = LBα(n), since

(cid:107)hS − hSi(cid:107) =

sup
x∈X :(cid:107)x(cid:107)∗≤1

((cid:104)hS, x(cid:105) − (cid:104)hSi, x(cid:105)) .

However, the reverse implication need not necessarily hold
and hence uniform argument stability is a stronger notion.

In the rest of the paper, we will focus on L-Lipschitz loss
functions and assume that (cid:107)X(cid:107)∗ ≤ B holds almost surely.

Algorithmic Stability and Hypothesis Complexity

These assumptions are arguably stronger than those made
by Bousquet & Elisseeff (2002) who only require that the
loss function be bounded. In contrast, our results will re-
quire that the loss (cid:96)(h, z) be Lipschitz in the linear form
(cid:104)h, x(cid:105), which is only slightly more general than assum-
ing generalized linear loss functions. Nevertheless, these
stronger assumptions will enable us to prove stronger gen-
eralization bounds.

The relationship between argument stability and general-
ization performance hinges on a property of the Banach
space B that is closely related to the martingale type of
the space—see Pisier (2011) for a comprehensive account.
For concreteness we assume that the Banach space B is
(2, D)-smooth (or of martingale type 2) for some D > 0.
This means that for all h, h(cid:48) ∈ B,

(cid:107)h + h(cid:48)(cid:107)2 + (cid:107)h − h(cid:48)(cid:107)2 ≤ 2(cid:107)h(cid:107)2 + 2D2(cid:107)h(cid:48)(cid:107)2 .

Note that Hilbert spaces are (2, 1)-smooth. The property
we need is described in the following result of (Pinelis,
1994):
Proposition 1. Let D1, . . . , Dn be a martingale difference
sequence taking values in a separable (2, D)-smooth Ba-
nach space B. Then for any (cid:15) > 0,

P

(cid:18)

(cid:33)

(cid:32)

Dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≥ c(cid:15)

n
(cid:88)

sup
n≥1

≤ 2 exp

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
where c is a constant satisfying that (cid:80)∞
∞ ≤ c2
(and (cid:107)Dt(cid:107)∞ is the essential supremum of the random vari-
able (cid:107)Dt(cid:107)).

t=1 (cid:107)Dt(cid:107)2

(cid:15)2
2D2

t=1

−

(cid:19)

,

Our arguments extend, in a straightforward manner, to
more general Banach spaces whenever exponential tail
inequalities for bounded martingale sequences similar to
Proposition 1 are available. We stay with the assumption
of (2, D)-smoothness for convenience and because it ap-
plies to the perhaps most important special case when B is
a Hilbert space. We refer to Rakhlin & Sridharan (2015) for
more information of martingale inequalities of this kind.

A key property of stable algorithms, implied by the martin-
gale inequality, is that the hypothesis hS output by the al-
gorithm is concentrated—in the Banach space B—around
its expectation EhS. This is established in the next simple
lemma.
Lemma 1. Let the Banach space B be (2, D)-smooth. If
a learning algorithm A is α(n)-uniformly argument stable,
then, for any δ > 0,

P

(cid:16)

(cid:17)
(cid:107)hS − EhS(cid:107) ≤ Dα(n)(cid:112)2n log(2/δ)

≥ 1 − δ .

hS − EhS =

Dt .

n
(cid:88)

t=1

so that

We have

∞
(cid:88)

t=1

(cid:107)Dt(cid:107)2
∞

n
(cid:88)

t=1
n
(cid:88)

t=1
n
(cid:88)

=

=

≤

t=1
≤ nα(n)2 .

(cid:107)E(hS|Z1, . . . , Zt) − E(hS|Z1, . . . , Zt−1)(cid:107)2
∞

(cid:107)E(hS − hSt|Z1, . . . , Zt)(cid:107)2
∞

(E((cid:107)(hS − hSt(cid:107)∞|Z1, . . . , Zt))2

Thus, by Proposition 1, we have

(cid:16)

P

(cid:107)hS − EShS(cid:107) ≥ α(n)D(cid:112)2n log(2/δ)

(cid:17)

≤ δ

for δ = 2 exp

(cid:16)

− (cid:15)2
2D2

(cid:17)

.

3. Algorithmic Rademacher Complexity and

Generalization Bound

The concentration result of Lemma 1 justiﬁes the follow-
ing deﬁnition of the “algorithmic hypothesis class”: since
with high probability hS concentrates around its expecta-
tion EhS, what matters in the generalization performance
of the algorithm is the complexity of the ball centered at
EhS and not that of the entire hypothesis class H. This ob-
servation may lead to signiﬁcantly improved performance
guarantees.

Deﬁnition 4 (Algorithmic Hypothesis Class). For a sample
size n and conﬁdence parameter δ > 0, let r = r(n, δ) =
Dα(n)(cid:112)2n log(2/δ) and deﬁne the algorithmic hypothe-
sis class of a stable learning algorithm by

Br = {h ∈ H| (cid:107)h − EhS(cid:107) ≤ r(n, δ)} .

Note that, by Lemma 1, hS ∈ Br with probability at least
1 − δ.

We bound the generalization error (1) in terms of the
Rademacher complexity (Bartlett & Mendelson, 2003) of
the algorithmic hypothesis class. The Rademacher com-
plexity of a hypothesis class H on the feature space X is
deﬁned as

Proof. Introduce the martingale differences

Dt = E(hS|Z1, . . . , Zt) − E(hS|Z1, . . . , Zt−1)

R(H) = E sup
h∈H

1
n

n
(cid:88)

i=1

σi(cid:104)h, Xi(cid:105) ,

Algorithmic Stability and Hypothesis Complexity

where σ1, . . . , σn are i.i.d. Rademacher variables that are
uniformly distributed in {−1, +1}.

The next theorem shows how the Rademacher complexity
of the algorithmic hypothesis class can be bounded. The
bound depends on the type of the feature space X . Recall
that the Banach space (X , (cid:107) · (cid:107)∗) is of type p ≥ 1 if there
exists a constant Cp such that for all x1, . . . , xn ∈ X ,

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∗

σixi

≤ Cp

(cid:33)1/p

(cid:107)xi(cid:107)p
∗

.

(cid:32) n
(cid:88)

i=1

In the important special case when X is a Hilbert space, the
space is of type 2 with constant C2 = 1.
Theorem 1. Assume that B is a (2, D)-smooth Banach
space and that its dual X is of type p.
Suppose that
the marginal distribution of the Xi is such that (cid:107)Xi(cid:107)∗ ≤
B with probability one, for some B > 0.
If a learn-
ing algorithm is α(n)-uniformly argument stable, then the
Rademacher complexity of the algorithmic hypothesis class
Br on the feature space satisﬁes

R(Br) ≤ DCpB(cid:112)2 log(2/δ)α(n)n−1/2+1/p .

In particular, when B is a Hilbert space, the bound simpli-
ﬁes to

R(Br) ≤ B(cid:112)2 log(2/δ)α(n) .

Proof. We have

R(Br)

−σiE(cid:104)hS, Xi(cid:105) + σiE(cid:104)hS, Xi(cid:105))

σi((cid:104)h, Xi(cid:105) − E(cid:104)hS, Xi(cid:105))

σi(cid:104)h, Xi(cid:105)

(σi(cid:104)h, Xi(cid:105)

= E sup
h∈Br

= E sup
h∈Br

= E sup
h∈Br

= E sup
h∈Br

n
(cid:88)

i=1
n
(cid:88)

i=1

n
(cid:88)

i=1
n
(cid:88)

i=1

1
n

1
n

1
n

1
n

1
n

≤ E sup
h∈Br
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

r
n

≤

E

n
(cid:88)

i=1

σiXi

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∗

σi (cid:104)h − EhS, Xi(cid:105)

(cid:107)h − EhS(cid:107)

σiXi

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∗

The theorem above may be easily used to bound the per-
formance of an α(n)-uniformly argument stable learning
algorithm. For simplicity, we state the result for Hilbert
spaces only. The extension to (2, D)-smooth Banach
spaces with a type-p dual is straightforward.

Corollary 1. Assume that B is a separable Hilbert space.
Suppose that the marginal distribution of the Xi is such
that (cid:107)Xi(cid:107)∗ ≤ B with probability one, for some B > 0
and that the loss function is bounded and Lipschitz, that is,
(cid:96)(h, Z) ≤ M with probability one for some M > 0 and
|(cid:96)(h, z) − (cid:96)(h(cid:48), z)| ≤ L |(cid:104)h, x(cid:105) − (cid:104)h(cid:48), x(cid:105)| for all z ∈ Z
and h, h(cid:48) ∈ H. If a learning algorithm is α(n)-uniformly
argument stable, then its generalization error is bounded
as follows. With probability at least 1 − 2δ,

R(hS) − RS(hS)

≤ 2LB(cid:112)2 log(2/δ)α(n) + M

(cid:114)

log(1/δ)
2n

.

Proof. Note ﬁrst that, by Lemma 1, with probability at least
1 − δ,

R(hS) − RS(hS) ≤ sup
h∈Br

(R(h) − RS(h)) .

On the other hand, by the boundedness of the loss function,
and the bounded differences inequality, with probability at
least 1 − δ,

(R(h) − RS(h))

sup
h∈Br

≤ E sup
h∈Br

(R(h) − RS(h)) + M

(cid:114)

log(1/δ)
2n

≤ 2R((cid:96) ◦ Br) + M

(cid:114)

log(1/δ)
2n

,

where (cid:96) ◦ H denotes the set of compositions of functions (cid:96)
and h ∈ H. By the Lipschitz property of the loss function
and a standard contraction argument, i.e., Talagrand Con-
traction Lemma (Ledoux & Talagrand, 2013), we have,

R((cid:96) ◦ Br) ≤ L · R(Br)

≤ LB(cid:112)2 log(2/δ)α(n) .

≤

α(n)D(cid:112)2n log(2/δ)Cp

1
n

(cid:33)1/p

(cid:32) n
(cid:88)

i=1

(cid:107)Xi(cid:107)p
∗

≤ DCpB(cid:112)2 log(2/δ)α(n)n−1/2+1/p ,

concluding the proof.

Note that the order of magnitude of α(n) of many stable
algorithms is of order O(1/n). For the notion of uniform
stability, such bounds appear in Lugosi & Pawlak (1994);
Bousquet & Elisseeff (2002); Wibisono et al. (2009); Hardt
et al. (2015); Liu et al. (2017). As we will show in the
examples below, many of these learning algorithms even
have uniform argument stability of order O(1/n). In such
cases the bound of Corollary 1 is essentially equivalent of

Algorithmic Stability and Hypothesis Complexity

the earlier results cited above. The bound is dominated by

(cid:113) log(1/δ)
2n

the term M
present by using the bounded dif-
ferences inequality. Fluctuations of the order of O(n−1/2)
are often inevitable, especially when R(hS) is not typically
small. When small risk is reasonable to expect, one may
use more advanced concentration inequalities with second-
moment information, at the price of replacing the gener-
alization error by the so-called “deformed” generalization
error R(hS)− a
a−1 RS(hS) where a > 1. The next theorem
derives such a bound, relying on techniques developed by
Bartlett et al. (2005). This result improves essentially on
earlier stability-based bounds.
Theorem 2. Assume that B is a separable Hilbert space.
Suppose that the marginal distribution of the Xi is such
that (cid:107)Xi(cid:107)∗ ≤ B with probability one, for some B > 0
and that the loss function is bounded and Lipschitz, that is,
(cid:96)(h, Z) ≤ M with probability one for some M > 0 and
|(cid:96)(h, z) − (cid:96)(h(cid:48), z)| ≤ L |(cid:104)h, x(cid:105) − (cid:104)h(cid:48), x(cid:105)| for all z ∈ Z
and h, h(cid:48) ∈ H. Let a > 1. If a learning algorithm is α(n)-
uniformly argument stable, then, with probability at least
1 − 2δ,

R(hS) −

RS(hS)

a
a − 1

For any r > 0 and a > 1, if Vr ≤ r/a then every h ∈ Br
satisﬁes

E(cid:96)(h, Z) ≤

(cid:96)(h, Zi) + Vr.

a
a − 1

1
n

n
(cid:88)

i=1

Now, we are ready to prove Theorem 2.

Proof of Theorem 2. First, we introduce an inequality to
build the connection between algorithmic stability and hy-
pothesis complexity. According to Lemma 1, for any a > 1
and δ > 0, with probability at least 1 − δ, we have

R(hS) −

a
a − 1

RS(hS) ≤ sup
h∈Br

(R(h) −

a
a − 1

RS(h)) .

(2)

Second, we are going to upper bound the term
suph∈Br (R(h) − a
It
is easy to check that for any g ∈ Gr, Eg(Z) ≤ r and
g(Z) ∈ [0, M ]. Then

a−1 RS(h)) with high probability.

var(g(Z)) ≤ E(g(Z))2 ≤ M Eg(Z) ≤ M r.

≤ 8LB(cid:112)2 log(2/δ)α(n) +

(6a + 8)M log(1/δ)
3n

.

Applying Proposition 2,

The proof of Theorem 2 relies on techniques developed by
Bartlett et al. (2005).
In particular, we make use of the
following result.
Proposition 2. (Bartlett et al., 2005, Theorem 2.1). Let
F be a class of functions that map X into [0, M ]. As-
sume that there is some ρ > 0 such that for every f ∈ F ,
var(f (X)) ≤ ρ. Then, with probability at least 1 − δ, we
have

Ef (X) −

(cid:33)

f (Xi)

1
n

n
(cid:88)

i=1

(cid:32)

sup
f ∈F
(cid:32)

≤

4R(F ) +

(cid:114)

2ρ log(1/δ)
n

+

4M
3

log(1/δ)
n

(cid:33)

.

To prove the theorem, we also need to introduce the follow-
ing auxiliary lemma.

Deﬁne

Gr(Z) =

(cid:26)

r
max{r, E(cid:96)(h, Z)}

(cid:27)

(cid:96)(h, Z)|h ∈ Br

.

It is evident that Gr ⊆ {α(cid:96) ◦ h|h ∈ Br, α ∈ [0, 1]}. The
following lemma is proven in (Bartlett et al., 2005).
Lemma 2. Deﬁne

Vr = sup
g∈Gr

(cid:32)

Eg(Z) −

(cid:33)

g(Zi)

.

1
n

n
(cid:88)

i=1

Vr ≤ 4R(Gr) +

(cid:114)

2M r log(1/δ)
n

+

4M
3

log(1/δ)
n

.

Let

(cid:114)

4R(Gr) +

We have

2M r log(1/δ)
n

+

4M
3

log(1/δ)
n

=

r
a

.

r ≤

2M a2 log(1/δ)
n

+ 8aR(Gr) +

4
3

2aM log(1/δ)
n

,

which means that there exists an r∗ ≤ 2M a2 log(1/δ)
+
8aR(Gr) + 4
such that Vr∗ ≤ r∗/a holds. Ac-
3
cording to Lemma 2, for any h ∈ Br, with probability at
least 1 − δ, we have

2aM log(1/δ)
n

n

E(cid:96)(h, Z) ≤

(cid:96)(h, Zi) + Vr∗

a
a − 1

1
n

n
(cid:88)

i=1

≤

≤

a
a − 1

1
n

a
a − 1

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:96)(h, Zi) +

r∗
a

(cid:96)(h, Zi) +

2M a log(1/δ)
n

+ 8R(Gr) +

4
3

2M log(1/δ)
n

.

Algorithmic Stability and Hypothesis Complexity

It is easy to verify that Gr ⊆ {α(cid:96) ◦ h|h ∈ Br, α ∈ [0, 1]} ⊆
convBr.

By exploiting their results, we show that stable RERM al-
gorithms have strong generalization properties.

By elementary properties of the Rademacher complexity
(see, e.g., Bartlett & Mendelson (2003)), H (cid:48) ⊆ H implies
R(H (cid:48)) ≤ R(H). Then, with probability at least 1 − δ, we
have

(cid:32)

sup
h∈Br

E(cid:96)(h, X) −

a
a − 1

1
n

n
(cid:88)

i=1

(cid:33)

(cid:96)(h, Xi)

≤

2M a log(1/δ)
n

+ 8R((cid:96) ◦ Br) +

4
3

2M log(1/δ)
n

.

The proof of Theorem 2 is complete by combining the
above inequality with inequality (2), the Talagrand Con-
traction Lemma, and Theorem 1.

In the next section, we specialize the above results to some
learning algorithms by proving their uniform argument sta-
bility.

4. Applications

Various learning algorithms have been proved to possess
some kind of stability. We refer the reader to (Devroye
& Wagner, 1979; Lugosi & Pawlak, 1994; Bousquet &
Elisseeff, 2002; Zhang, 2003; Wibisono et al., 2009; Hardt
et al., 2015; Liu et al., 2017) for such examples, including
stochastic gradient descent methods, empirical risk mini-
mization, and non-parametric learning algorithms such as
k-nearest neighbor rules and kernel regression.

4.1. Empirical Risk Minimization

Regularized empirical risk minimization has been known to
be uniformly stable (Bousquet & Elisseeff, 2002). Here we
consider regularized empirical risk minimization (RERM)
algorithms of the following form. The empirical risk (or
the objective function) of RERM is formulated as

RS,λ(h) =

(cid:96)(h, Xi) + λN (h),

1
n

n
(cid:88)

i=1

where N : h ∈ H (cid:55)→ N (h) ∈ R+ is a convex function. Its
corresponding expected counterpart is deﬁned as

Theorem 3. Assume that B is a separable Hilbert space.
Suppose that the marginal distribution of the Xi is such
that (cid:107)Xi(cid:107)∗ ≤ B with probability one, for some B > 0 and
that the loss function is convex in h, bounded by M and
L-Lipschitz. Suppose that for some constants C and ξ > 1,
the penalty function N (h) satisﬁes

N (hS) + N (hSi) − 2N

≥ C(cid:107)hS − hSi(cid:107)ξ.

(cid:19)

(cid:18) hS + hSi
2

(3)

Then, for any δ > 0, and a > 1, if hS is the output of
RERM, with probability at least 1 − 2δ, we have

R(hS) −

RS(hS)

a
a − 1

ξ−1 (cid:112)2 log(2/δ)

≤ 8LB

(cid:19) 1

(cid:18) LB
Cλn
(6a + 8)M log(1/δ)
3n

.

+

Speciﬁcally, when N (h) = (cid:107)h(cid:107)2, (3) holds with ξ = 2 and
C = 1
2

(cid:0) M
λ

(cid:1) 1
2 .

Proof. The proof of Theorem 3 relies on the following re-
sult implied by Wibisono et al. (2009).

Proposition 3. Assume the conditions of Theorem 3. Then
the RERM learning algorithm is β(n)-uniformly stable
with

and is α(n)-uniformly argument stable with

β(n) =

(cid:19) 1

ξ−1

(cid:18) kξLξ
Cλn

α(n) =

(cid:19) 1

ξ−1

(cid:18) kL
Cλn

,

.

Speciﬁcally, when N (h) = (cid:107)h(cid:107)p
p and 1 < p ≤ 2, the
condition 3 on the penalty function holds with ξ = 2 and
C = 1
r |hr|p and r is
p , where (cid:107)h(cid:107)p
the index for the dimensionality.

4 p(p − 1) (cid:0) M

p = (cid:80)

(cid:1) p−1

λ

Rλ(h) = E(cid:96)(h, X) + λN (h).

Theorem 3 follows by combining Theorem 2 and Proposi-
tion 3.

Bousquet & Elisseeff (2002) proved that (cid:96)2-regularized
learning algorithms are β(n)-uniformly stable. Wibisono
et al. (2009) extended the result and studied a sufﬁcient
condition of the penalty term N (h) to ensure uniform
β(n)-stability. As we now show, both of their proof meth-
ods are applicable to the analysis of uniform argument sta-
bility.

4.2. Stochastic Gradient Descent

Stochastic gradient descent (SGD) is one of the most
widely used optimization methods in machine learning.
Hardt et al. (2015) showed that parametric models trained
by SGD methods are uniformly stable. Their results ap-
ply to both convex and non-convex learning problems and

Algorithmic Stability and Hypothesis Complexity

provide insights for why SGD performs well in practice, in
particular, for deep learning algorithms.

Their results are based on the assumptions that the loss
function employed is both Lipschitz and smooth.
In or-
der to avoid technicalities of deﬁning derivatives in general
Hilbert spaces, in this section we assume that B = X =
Rd, the d-dimensional Euclidean space.
Deﬁnition 5 (Smooth). A differentiable loss function
(cid:96)(h, ·) is s-smooth if for all h, h(cid:48) ∈ H, we have
(cid:107)∇h(cid:96)(h, ·) − ∇h(cid:48)(cid:96)(h(cid:48), ·)(cid:107) ≤ s(cid:107)h − h(cid:48)(cid:107),

where ∇xf (x) denotes the derivative of f (x) with respect
to x and s > 0.
Deﬁnition 6 (Strongly Convex). A differentiable loss func-
tion (cid:96)(h, ·) is γ-strongly convex with respect to (cid:107) · (cid:107) if for
all h, h(cid:48) ∈ H, we have

(∇h(cid:96)(h, ·) − ∇h(cid:48)(cid:96)(h(cid:48), ·))T (h − h(cid:48)) ≥ γ(cid:107)h − h(cid:48)(cid:107)2,

where γ > 0.

Theorem 2 is applicable to the results of SGD when the
general loss function (cid:96)(h, x) is L-Lipschitz, s-smooth, and
h is linear with respect to x. Note that our deﬁnition of L-
Lipschitzness requires the loss function to be Lipschitz in
the linear form (cid:104)h, x(cid:105).
Theorem 4. Let the stochastic gradient update rule be
given by ht+1 = ht − αt∇h(cid:96)(ht, Xit), where αt > 0 is
the learning rate and it is the index for choosing one exam-
ple for the t-th update. Let hT and hi
T denote the outputs
of SGD run on sample S and Si, respectively. Assume that
(cid:107)X(cid:107)∗ ≤ B with probability one. Suppose that the loss
function is L-Lipschitz, s-smooth, and upper bounded by
M . Let SGD is run with a monotonically non-increasing
step size αt ≤ c/t, where c is a universal constant, for T
steps. Then, for any δ > 0 and a > 1, with probability at
least 1 − 2δ, we have

R(hT ) −

RS(hT )

a
a − 1

≤ 8BL

1 + 1/sc
n − 1

+

(6a + 8)M log(1/δ)
3n

.

(2cBL)

1
sc+1 T

sc

sc+1 (cid:112)2 log(2/δ)

When the loss function (cid:96) is convex, L-admissible, s-smooth,
and upper bounded by M , suppose that SGD is run with
step sizes αt ≤ 2/s for T steps. Then, for any δ > 0 and
a > 1, with probability at least 1 − 2δ,

R(hT ) −

RS(hT )

a
a − 1
16B2L2
n

T
(cid:88)

t=1

≤

(cid:112)2 log(2/δ)

αt

+

(6a + 8)M log(1/δ)
3n

.

Moreover, when the loss function (cid:96) is γ-strongly convex, s-
smooth, and upper bounded by M , let the stochastic gradi-
ent update be given by ht+1 = ΠΩ(ht − αt∇h(cid:96)(ht, Xit)),
where Ω is a compact, convex set over which we wish to
optimize and ΠΩ(·) is a projection such that ΠΩ(f ) =
If the loss function is further L-
arg minh∈H (cid:107)h − f (cid:107).
Lipschitz over the set Ω and the projected SGD is run with
a constant step size α ≤ 1/s for T steps. Then, for any
δ > 0 and a > 1, with probability at least 1 − 2δ, the
projected SGD satisﬁes that

R(hT ) −

RS(hT )

a
a − 1
16DB2L2
γn

≤

(cid:112)2 log(2/δ) +

(6a + 8)M log(1/δ)
3n

.

Note that any (cid:96)2 regularized convex loss function is
strongly convex. Bousquet & Elisseeff (2002) studied the
stability of batch methods. When the loss function is
strongly convex, the stability of SGD is consistent with the
result in (Bousquet & Elisseeff, 2002).

While the above result only applies to L-Lipschitz loss
functions as deﬁned in Deﬁnition 3, it does explain some
generalization properties of layer-wise training of neural
In this once-
networks by stochastic gradient descent.
common training scheme (see, e.g., Bengio et al., 2007),
one freezes the parameters of the network before/after a
certain layer and performs SGD for this single layer. It is
easy to see that, as long as the activation function and the
loss function (connected with the network) are Lipschitz-
continuous in their inputs, the overall loss can easily sat-
isfy the continuous conditions of Theorem 4. This implies
that the parameters in each layer may generalize well in a
certain sense if SGD is employed with an early stop.

The proof of Theorem 4 follows immediately from Theo-
rem 2, combined with the following result implied by Hardt
et al. (2015) (which is a collection of the results of Theo-
rems 3.8, 3.9, and 3.12 therein).

Proposition 4. Let the stochastic gradient update be given
by ht+1 = ht −αt∇h(cid:96)(ht, Zit), where αt > 0 is the learn-
ing rate and it is the index for choosing one example for
the t-th update. Let hT and hi
T denote the outputs of SGD
running on sample S and Si respectively. When the loss
function is L-Lipschitz and s-smooth, suppose that SGD is
run with monotonically non-increasing step size αt ≤ c/t,
where c is a universal constant, for T steps. Then,

(cid:107)hT − hi

T (cid:107) ≤

1 + 1/sc
n − 1

(2cBL)

1
sc+1 T

sc
sc+1 .

When the loss function (cid:96) is convex, L-Lipschitz, and s-
smooth, suppose that SGD is run with step sizes αt ≤ 2/s

Algorithmic Stability and Hypothesis Complexity

for T steps. Then,

(cid:107)hT − hi

T (cid:107) ≤

2BL
n

T
(cid:88)

t=1

αt.

Moreover, when the loss function (cid:96) is γ-strongly convex and
s-smooth, let the stochastic gradient update be given by
ht+1 = ΠΩ(ht − αt∇h(cid:96)(ht, Zit)), where Ω is a compact,
convex set over which we wish to optimize and ΠΩ(·) is
a projection such that ΠΩ(f ) = arg minh∈H (cid:107)h − f (cid:107). If
the loss function is L-Lipschitz over the set Ω and the pro-
jected SGD is run with constant step size α ≤ 1/s for T
steps. Then, the projected SGD satisﬁes algorithmic argu-
ment stability with

(cid:107)hT − hi

T (cid:107) ≤

2BL
γn

.

5. Conclusion

We introduced the concepts of uniform argument stability
and algorithmic hypothesis class, deﬁned as the class of
hypotheses that are likely to be output by the learning al-
gorithm. We proposed a general probabilistic framework
to exploit local estimates for the complexity of hypothesis
class to obtain fast convergence rates for stable learning al-
gorithms. Speciﬁcally, we deﬁned the algorithmic hypoth-
esis class by observing that the output of stable learning al-
gorithms concentrates around EhS. The Rademacher com-
plexity deﬁned on the algorithmic hypothesis class then
converges at the same rate as that of the uniform argument
stability in Hilbert space, which are of order O(1/n) for
various learning algorithms, such as empirical risk mini-
mization and stochastic gradient descent. We derived fast
convergence rates of order O(1/n) for their deformed gen-
eralization errors. Unlike previously published guarantees
of similar ﬂavor, our bounds hold with high probability,
rather than only in expectation.

Our study leaves some open problems and allows several
possible extensions. First, the algorithmic hypothesis class
deﬁned in this study depends mainly on the property of
learning algorithms but little on the data distribution.
It
would be interesting to investigate a way to deﬁne an algo-
rithmic hypothesis class by considering both the algorith-
mic property and the data distribution. Second, it would be
interesting to explore if there are some algorithmic proper-
ties other than stability that could result in a small algorith-
mic hypothesis class.

Acknowledgments

Liu and Tao were partially supported by Australian Re-
search Council Projects FT-130101457, DP-140102164,
LP-150100671. Lugosi was partially supported by the

Spanish Ministry of Economy and Competitiveness, Grant
MTM2015-67304-P, and FEDER, EU. Neu was partially
supported by the UPFellows Fellowship (Marie Curie CO-
FUND program 600387).

References

Bartlett, Peter L and Mendelson, Shahar. Rademacher and
Gaussian complexities: Risk bounds and structural re-
sults. Journal of Machine Learning Research, 3:463–
482, 2003.

Bartlett, Peter L, Bousquet, Olivier, and Mendelson, Sha-
har. Local Rademacher complexities. Annals of Statis-
tics, pp. 1497–1537, 2005.

Bengio, Yoshua, Lamblin, Pascal, Popovici, Dan, and
Larochelle, Hugo. Greedy layer-wise training of deep
networks. In NIPS, 2007.

Bonnans, J Fr´ed´eric and Shapiro, Alexander. Perturbation
analysis of optimization problems. Springer Science &
Business Media, 2013.

Boucheron, St´ephane, Lugosi, G´abor, and Massart, Pascal.
Concentration inequalities: A nonasymptotic theory of
independence. OUP Oxford, 2013.

Bousquet, Olivier and Elisseeff, Andr´e. Stability and gen-
eralization. Journal of Machine Learning Research, 2:
499–526, 2002.

Devroye, Luc and Wagner, Terry J. Distribution-free in-
equalities for the deleted and holdout error estimates.
IEEE Transactions on Information Theory, 25(2):202–
207, 1979.

Hardt, Moritz, Recht, Benjamin, and Singer, Yoram. Train
faster, generalize better: Stability of stochastic gradient
descent. arXiv preprint arXiv:1509.01240, 2015.

Ledoux, Michel and Talagrand, Michel. Probability in Ba-
nach spaces: Isoperimetry and processes. Springer Sci-
ence & Business Media, 2013.

Liu, Tongliang, Tao, Dacheng, Song, Mingli, and May-
bank, Stephen J. Algorithm-dependent generalization
IEEE Transactions on
bounds for multi-task learning.
Pattern Analysis and Machine Intelligence, 39(2):227–
241, February 2017.

Lugosi, G´abor and Pawlak, Miroslaw. On the posterior-
probability estimate of the error rate of nonparametric
classiﬁcation rules. IEEE Transactions on Information
Theory, 40(2):475–481, 1994.

Mukherjee, Sayan, Niyogi, Partha, Poggio, Tomaso, and
Rifkin, Ryan. Learning theory: stability is sufﬁcient for

Algorithmic Stability and Hypothesis Complexity

generalization and necessary and sufﬁcient for consis-
tency of empirical risk minimization. Advances in Com-
putational Mathematics, 25:161–193, 2006.

Pinelis, Iosif. Optimum bounds for the distributions of mar-
tingales in Banach spaces. The Annals of Probability, pp.
1679–1706, 1994.

Pisier, Gilles. Martingales in Banach spaces (in connection

with type and cotype). IHP course notes, 2011.

Rakhlin, Alexander and Sridharan, Karthik. On equiva-
lence of martingale tail bounds and deterministic regret
inequalities. arXiv preprint arXiv:1510.03925, 2015.

Shalev-Shwartz, Shai, Shamir, Ohad, Srebro, Nathan, and
Sridharan, Karthik. Learnability, stability and uniform
convergence. Journal of Machine Learning Research,
11:2635–2670, 2010.

Wibisono, Andre, Rosasco, Lorenzo, and Poggio, Tomaso.
Sufﬁcient conditions for uniform stability of regulariza-
tion algorithms. Techincal Report MIT-CSAIL-TR-2009-
060, 2009.

Zhang, Tong. Leave-one-out bounds for kernel methods.

Neural Computation, 15(6):1397–1437, 2003.

