Geometry of Neural Network Loss Surfaces via Random Matrix Theory

Jeffrey Pennington 1 Yasaman Bahri 1

Abstract

2014; Choromanska et al., 2015; Neyshabur et al., 2015),
architecture design, and generalization (Keskar et al.).

Understanding the geometry of neural network
loss surfaces is important for the development of
improved optimization algorithms and for build-
ing a theoretical understanding of why deep
learning works. In this paper, we study the ge-
ometry in terms of the distribution of eigenvalues
of the Hessian matrix at critical points of varying
energy. We introduce an analytical framework
and a set of tools from random matrix theory that
allow us to compute an approximation of this dis-
tribution under a set of simplifying assumptions.
The shape of the spectrum depends strongly on
the energy and another key parameter, φ, which
measures the ratio of parameters to data points.
Our analysis predicts and numerical simulations
support that for critical points of small index, the
number of negative eigenvalues scales like the 3/2
power of the energy. We leave as an open prob-
lem an explanation for our observation that, in
the context of a certain memorization task, the
energy of minimizers is well-approximated by
the function 1

2 (1 − φ)2.

1. Introduction

Neural networks have witnessed a resurgence in recent
years, with a smorgasbord of architectures and conﬁgura-
tions designed to accomplish ever more impressive tasks.
Yet for all the successes won with these methods, we have
managed only a rudimentary understanding of why and in
what contexts they work well. One difﬁculty in extend-
ing our understanding stems from the fact that the neural
network objectives are generically non-convex functions
in high-dimensional parameter spaces, and understanding
their loss surfaces is a challenging task. Nevertheless, an
improved understanding of the loss surface could have a
large impact on optimization (Saxe et al.; Dauphin et al.,

1Google Brain. Correspondence to: Jeffrey Pennington <jpen-

nin@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1.1. Related work

There is no shortage of prior work focused on the loss
surfaces of neural networks. Choromanska et al. (2015)
and Dauphin et al. (2014) highlighted the prevalence of
saddle points as dominant critical points that plague op-
timization, as well as the existence of many local minima
at low loss values. Dauphin et al. (2014) studied the dis-
tribution of critical points as a function of the loss value
empirically and found a trend which is qualitatively sim-
ilar to predictions for random Gaussian landscapes (Bray
& Dean, 2007). Choromanska et al. (2015) argued that the
loss function is well-approximated by a spin-glass model
studied in statistical physics, thereby predicting the exis-
tence of local minima at low loss values and saddle points
at high loss values as the network increases in size. Good-
fellow et al. observed that loss surfaces arising in prac-
tice tend to be smooth and seemingly convex along low-
dimensional slices. Subsequent works (Kawaguchi, 2016;
Safran & Shamir, 2016; Freeman & Bruna, 2016) have fur-
thered these and related ideas empirically or analytically,
but it remains safe to say that we have a long way to go
before a full understanding is achieved.

1.2. Our contributions

One shortcoming of prior theoretical results is that they are
often derived in contexts far removed from practical neu-
ral network settings – for example, some work relies on
results for generic random landscapes unrelated to neural
networks, and other work draws on rather tenuous connec-
tions to spin-glass models. While there is a lot to be gained
from this type of analysis, it leaves open the possibility that
characteristics of loss surfaces speciﬁc to neural networks
may be lost in the more general setting. In this paper, we
focus narrowly on the setting of neural network loss sur-
faces and propose an analytical framework for studying the
spectral density of the Hessian matrix in this context.

Our bottom-up construction assembles an approximation
of the Hessian in terms of blocks derived from the weights,
the data, and the error signals, all of which we assume to

Geometry of Neural Networks

be random variables1. From this viewpoint, the Hessian
may be understood as a structured random matrix and we
study its eigenvalues in the context of random matrix the-
ory, using tools from free probability. We focus on single-
hidden-layer networks, but in principle the framework can
accommodate any network architecture. After establishing
our methodology, we compute approximations to the Hes-
sian at several levels of reﬁnement. One result is a predic-
tion that for critical points of small index, the index scales
like the energy to the 3/2 power.

2. Preliminaries

Let W (1) ∈ Rn1×n0 and W (2) ∈ Rn2×n1 be weight ma-
trices of a single-hidden-layer network without biases. De-
note by x ∈ Rn0×m the input data and by y ∈ Rn2×m the
targets. We will write z(1) = W (1)x for the pre-activations.
We use [·]+ = max(·, 0) to denote the ReLU activation,
which will be our primary focus. The network output is

ˆyiµ =

W (2)

ik [z(1)

kµ ]+ ,

(1)

n1(cid:88)

k=1

and the residuals are eiµ = ˆyiµ − yiµ. We use Latin indices
for features, hidden units, and outputs, and µ to index ex-
amples. We consider mean squared error, so that the loss
(or energy) can be written as,

L = n2(cid:15) =

1
2m

n2,m
(cid:88)

i,µ=1

e2
iµ .

(2)

The Hessian matrix is the matrix of second derivatives of
the loss with respect to the parameters, i.e. Hαβ = ∂2L
,
∂θα∂θβ
where θα ∈ {W (1), W (2)}. It decomposes into two pieces,
H = H0 + H1, where H0 is positive semi-deﬁnite and
where H1 comes from second derivatives and contains all
of the explicit dependence on the residuals. Speciﬁcally,

[H0]αβ ≡

1
m

n2,m
(cid:88)

i,µ=1

∂ ˆyiµ
∂θα

∂ ˆyiµ
∂θβ

1
m

≡

[JJ T ]αβ ,

(3)

where we have introduced the Jacobian matrix, J, and,

[H1]αβ ≡

1
m

n2,m
(cid:88)

i,µ=1

eiµ

(cid:18) ∂2 ˆyiµ
∂θα∂θβ

(cid:19)

.

(4)

We will almost exclusively consider square networks with
n ≡ n0 = n1 = n2. We are interested in the limit of
large networks and datasets, and in practice they are typ-
ically of the same order of magnitude. A useful charac-

1Although we additionally assume the random variables are
independent, our framework does not explicitly require this as-
sumption, and in principle it could be relaxed in exchange for
more technical computations.

terization of the network capacity is the ratio of the num-
ber of parameters to the effective number of examples2,
φ ≡ 2n2/mn = 2n/m. As we will see, φ is a critical
parameter which governs the shape of the distribution. For
instance, eqn. (3) shows that H0 has the form of a covari-
ance matrix computed from the Jacobian, with φ governing
the rank of the result.

We will be making a variety of assumptions throughout this
work. We distinguish primary assumptions, which we use
to establish the foundations of our analytical framework,
from secondary assumptions, which we invoke to simplify
computations. To begin with, we establish our primary as-
sumptions:

1. The matrices H0 and H1 are freely independent, a

property we discuss in sec. 3.

2. The residuals are i.i.d. normal random variables with
tunable variance governed by (cid:15), eiµ ∼ N (0, 2(cid:15)). This
assumption allows the gradient to vanish in the large
m limit, specifying our analysis to critical points.

3. The data features are i.i.d. normal random variables.

4. The weights are i.i.d. normal random variables.

We note that normality may not be strictly necessary. We
will discuss these assumptions and their validity in sec. 5.

3. Primer on Random Matrix Theory

In this section we highlight a selection of results from the
theory of random matrices that will be useful for our anal-
ysis. We only aim to convey the main ideas and do not
attempt a rigorous exposition. For a more thorough intro-
duction, see, e.g., (Tao, 2012).

3.1. Random matrix ensembles

The theory of random matrices is concerned with proper-
ties of matrices M whose entries Mij are random variables.
The degree of independence and the manner in which the
Mij are distributed determine the type of random matrix
ensemble to which M belongs. Here we are interested
primarily in two ensembles: the real Wigner ensemble for
which M is symmetric but otherwise the Mij are i.i.d.; and
the real Wishart ensemble for which M = XX T where
Xij are i.i.d.. Moreover, we will restrict our attention to
studying the limiting spectral density of M. For a random
matrix Mn ∈ Rn×n, the empirical spectral density is de-
ﬁned as,

ρMn(λ) =

δ(λ − λj(Mn)) ,

(5)

1
n

n
(cid:88)

j=1

2In our context of squared error, each of the n2 targets may be

considered an effective example.

Geometry of Neural Networks

where the λj(Mn), j = 1, . . . , n, denote the n eigenvalues
of Mn, including multiplicity, and δ(z) is the Dirac delta
function centered at z. The limiting spectral density is
deﬁned as the limit of eqn. (5) as n → ∞, if it exists.

from which ρ can be recovered using the inversion formula,

ρ(λ) = −

Im G(λ + i(cid:15)) .

(12)

1
π

lim
(cid:15)→0+

For a matrix M of the real Wigner matrix ensemble
whose entries Mij ∼ N (0, σ2), Wigner (1955) computed
its limiting spectral density and found the semi-circle law,

ρSC(λ; σ, φ) =

(cid:40) 1

√

2πσ2
0

4σ2 − λ2

if |λ| ≤ 2σ
otherwise

.

(6)

Similarly, if M = XX T is a real Wishart matrix with
X ∈ Rn×p and Xij ∼ N (0, σ2/p), then the limiting spec-
tral density can be shown to be the Marchenko-Pastur dis-
tribution (Marˇcenko & Pastur, 1967),
(cid:40)

ρMP(λ; σ, φ) =

ρ(λ)
(1 − φ−1)δ(λ) + ρ(λ) otherwise

if φ < 1

,

(7)

(8)

where φ = n/p and,

1
ρ(λ) =
2πλσφ
λ± = σ(1 ± (cid:112)φ)2 .

(cid:112)(λ − λ−)(λ+ − λ)

The Wishart matrix M is low rank if φ > 1, which explains
the delta function density at 0 in that case. Notice that there
is an eigenvalue gap equal to λ−, which depends on φ.

3.2. Free probability theory

Suppose A and B are two random matrices. Under what
conditions can we use information about their individual
spectra to deduce the spectrum of A + B? One way of
analyzing this question is with free probability theory, and
the answer turns out to be exactly when A and B are freely
independent. Two matrices are freely independent if

Ef1(A)g1(B) · · · fk(A)gk(B) = 0

(9)

whenever fi and gi are such that

Efi(A) = 0 , Egi(B) = 0 .

(10)

Notice that when k = 1, this is equivalent to the deﬁni-
tion of classical independence. Intuitively, the eigenspaces
of two freely independent matrices are in “generic posi-
tion” (Speicher, 2009), i.e. they are not aligned in any spe-
cial way. Before we can describe how to compute the spec-
trum of A + B, we must ﬁrst introduce two new concepts,
the Stieltjes transform and the R transform.

3.2.1. THE STIELTJES TRANSFORM

For z ∈ C \ R the Stieltjes transform G of a probability
distribution ρ is deﬁned as,

3.2.2. THE R TRANSFORM

Given the Stieltjes transform G of a probability distribu-
tion ρ, the R transform is deﬁned as the solution to the
functional equation,

R(cid:0)G(z)(cid:1) +

1
G(z)

= z .

(13)

The beneﬁt of the R transform is that it linearizes free con-
volution, in the sense that,

RA+B = RA + RB ,

(14)

if A and B are freely independent. It plays a role in free
probability analogous to that of the log of the Fourier
transform in commutative probability theory.

The prescription for computing the spectrum of A + B
is as follows: 1) Compute the Stieltjes transforms of ρA
and ρB; 2) From the Stieltjes transforms, deduce the R
transforms RA and RB; 3) From RA+B = RA + RB,
compute the Stieltjes transform GA+B; and 4) Invert the
Stieltjes transform to obtain ρA+B.

4. Warm Up: Wishart plus Wigner

Having established some basic tools from random matrix
theory, let us now turn to applying them to computing the
limiting spectral density of the Hessian of a neural network
at critical points. Recall from above that we can decom-
pose the Hessian into two parts, H = H0 + H1 and that
H0 = JJ T /m. Let us make the secondary assumption
that at critical points, the elements of J and H1 are i.i.d.
normal random variables. In this case, we may take H0 to
be a real Wishart matrix and H1 to be a real Wigner matrix.
We acknowledge that these are strong requirements, and
we will discuss the validity of these and other assumptions
in sec. 5.

4.1. Spectral distribution

We now turn our attention to the spectral distribution of H
and how that distribution evolves as the energy changes.
For this purpose, only the relative scaling between H0 and
H1 is relevant and we may for simplicity take σH0 = 1 and
σH1 =

2(cid:15). Then we have,

√

ρH0(λ) = ρMP(λ; 1, φ),

ρH1 (λ) = ρSC(λ;

2(cid:15), φ) ,

(15)
which can be integrated using eqn. (11) to obtain GH0 and
GH1 . Solving eqn. (13) for the R transforms then gives,

√

G(z) =

(cid:90)

ρ(t)
z − t

R

dt ,

(11)

RH0(z) =

, RH1(z) = 2(cid:15)z .

(16)

1
1 − zφ

Geometry of Neural Networks

(a) φ = 1/4

(b) φ = 1/2

(c) φ = 3/4

Figure 1. Spectral distributions of the Wishart + Wigner approximation of the Hessian for three different ratios of parameters to data
points, φ. As the energy (cid:15) of the critical point increases, the spectrum becomes more semicircular and negative eigenvalues emerge.

We proceed by computing the R transform of H,

RH = RH0 + RH1 =

+ 2(cid:15)z ,

(17)

1
1 − zφ

so that, using eqn. (13), we ﬁnd that its Stieltjes transform
GH solves the following cubic equation,

is nontrivial. We discuss a method for doing so in the sup-
plementary material. The full result is too long to present
here, but we ﬁnd that for small α,

α((cid:15), φ) ≈ α0(φ)

(20)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:15) − (cid:15)c
(cid:15)c

(cid:12)
(cid:12)
(cid:12)
(cid:12)

3/2

,

2(cid:15)φ G3

H − (2(cid:15) + zφ)G2

H + (z + φ − 1)GH − 1 = 0 . (18)

where the critical value of (cid:15),

The correct root of this equation is determined by the
asymptotic behavior GH ∼ 1/z as z → ∞ (Tao, 2012).
From this root, the spectral density can be derived through
the Stieltjes inversion formula, eqn. (12). The result is illus-
trated in ﬁg. 1. For small (cid:15), the spectral density resembles
the Marchenko-Pastur distribution, and for small enough
φ there is an eigenvalue gap. As (cid:15) increases past a crit-
ical value (cid:15)c, the eigenvalue gap disappears and negative
eigenvalues begin to appear. As (cid:15) gets large, the spectrum
becomes semicircular.

4.2. Normalized index

Because it measures the number of descent directions3,
one quantity that is of importance in optimization and in
the analysis of critical points is the fraction of negative
eigenvalues of the Hessian, or the index of a critical point,
α. Prior work (Dauphin et al., 2014; Choromanska et al.,
2015) has observed that the index of critical points typi-
cally grows rapidly with energy, so that critical points with
many descent directions have large loss values. The precise
form of this relationship is important for characterizing the
geometry of loss surfaces, and in our framework it can be
readily computed from the spectral density via integration,

(cid:90) 0

−∞

α((cid:15), φ) ≡

ρ(λ; (cid:15), φ) dλ = 1 −

ρ(λ; (cid:15), φ) dλ .

(19)
Given that the spectral density is deﬁned implicitly through
equation eqn. (18), performing this integration analytically

(cid:90) ∞

0

3Here we ignore degenerate critical points for simplicity.

(cid:15)c =

1
16

(1 − 20φ − 8φ2 + (1 + 8φ)3/2) ,

(21)

is the value of the energy below which all critical points are
minimizers. We note that (cid:15)c can be found in a simpler way:
it is the value of (cid:15) below which eqn. (18) has only real roots
at z = 0, i.e. it is the value of (cid:15) for which the discriminant
of the polynomial in eqn. (18) vanishes at z = 0. Also, we
observe that (cid:15)c vanishes cubically as φ approaches 1,

(cid:15)c ≈

(1 − φ)3 + O(1 − φ)4 .

(22)

2
27

The 3/2 scaling in eqn. (20) is the same behavior that was
found in (Bray & Dean, 2007) in the context of the ﬁeld
theory of Gaussian random functions. As we will see later,
the 3/2 scaling persists in a more reﬁned version of this cal-
culation and in numerical simulations.

5. Analysis of Assumptions

5.1. Universality

There is a wealth of literature establishing that many prop-
erties of large random matrices do not depend on the details
of how their entries are distributed, i.e. many results are
universal. For instance, Tao & Vu (2012) show that the
spectrum of Wishart matrices asymptotically approaches
the Marcenko-Pastur law regardless of the distribution of
the individual entries, so long as they are independent, have
mean zero, unit variance, and ﬁnite k-th moment for k > 2.
Analogous results can be found for Wigner matrices (Ag-
garwal). Although we are unaware of any existing analy-

ρ(λ)-2-1012340.20.40.60.81.0λ-2-1012340.20.40.60.81.0λϵ=0.0ϵ=0.1ϵ=0.5ϵ=1.0-2-10123450.20.40.60.81.01.21.4λGeometry of Neural Networks

5.2.2. RESIDUALS ARE I.I.D. RANDOM NORMAL

First, we note that eiµ ∼ N (0, 2(cid:15)) is consistent with the
deﬁnition of the energy (cid:15) in eqn. (2). Furthermore, because
the gradient of the loss is proportional to the residuals, it
vanishes in expectation (i.e. as m → ∞), which specializes
our analysis to critical points. So this assumptions seems
necessary for our analysis. It is also consistent with the pri-
ors leading to the choice of the squared error loss function.
Altogether we believe this assumption is fairly mild.

5.2.3. DATA ARE I.I.D. RANDOM NORMAL

This assumption is almost never strictly satisﬁed, but it is
approximately enforced by common preprocessing meth-
ods, such as whitening and random projections.

5.2.4. WEIGHTS ARE I.I.D. RANDOM NORMAL

Although the i.i.d. assumption is clearly violated for a net-
work that has learned any useful information, the weight
distributions of trained networks often appear random, and
sometimes appear normal (see, e.g., ﬁg. S1 of the supple-
mentary material).

5.3. Secondary assumptions

In sec. 4 we introduced two assumptions we dubbed sec-
ondary, and we discuss their validity here.

5.3.1. J AND H1 ARE I.I.D. RANDOM NORMAL

Given that J and H1 are constructed from the residuals, the
data, and the weights – variables that we assume are i.i.d.
random normal – it is not unreasonable to suppose that their
entries satisfy the universality conditions mentioned above.
In fact, if this were the case, it would go a long way to vali-
date the approximations made in sec. 4. As it turns out, the
situation is more complicated: both J and H1 have sub-
structure which violates the independence requirement for
the universality results to apply. We must understand and
take into account this substructure if we wish to improve
our approximation further. We examine one way of doing
so in the next section.

6. Beyond the Simple Case

In sec. 4 we illustrated our techniques by examining the
simple case of Wishart plus Wigner matrices. This analysis
shed light on several qualitative properties of the spectrum
of the Hessian matrix, but, as discussed in the previous sec-
tion, some of the assumptions were unrealistic. We believe
that it is possible to relax many of these assumptions to pro-
duce results more representative of practical networks. In
this section, we take one step in that direction and focus on
the speciﬁc case of a single-hidden-layer ReLU network.

Figure 2. Testing the validity of the free independence assump-
tion by comparing the eigenvalue distribution of H0 + H1 (in
blue) and H0 + QH1QT (in orange) for randomly generated or-
thogonal Q. The discrepancies are small, providing support for
the assumption. Data is for a trained single-hidden-layer ReLU
autoencoding network with 20 hidden units and no biases on 150
4 × 4 downsampled, grayscaled, whitened CIFAR-10 images.

ses relevant for the speciﬁc matrices studied here, we be-
lieve that our conclusions are likely to exhibit some univer-
sal properties – for example, as in the Wishart and Wigner
cases, normality is probably not necessary.

On the other hand, most universality results and the tools
we are using from random matrix theory are only exact
in the limit of inﬁnite matrix size. Finite-size corrections
are actually largest for small matrices, which (counter-
intuitively) means that the most conservative setting in
which to test our results is for small networks. So we
will investigate our assumptions in this regime. We expect
agreement with theory only to improve for larger systems.

5.2. Primary assumptions

In sec. 2 we introduced a number of assumptions we
dubbed primary and we now discuss their validity.

5.2.1. FREE INDEPENDENCE OF H0 AND H1

Our use of free probability relies on the free independence
of H0 and H1. Generically we may expect some alignment
between the eigenspaces of H0 and H1 so that free inde-
pendence is violated; however, we ﬁnd that this violation
is often quite small in practice. To perform this analysis,
it sufﬁces to examine the discrepancy between the distribu-
tion of H0 + H1 and that of H0 + QH1QT , where Q is an
orthogonal random matrix of Haar measure (Chen et al.,
2016). Fig. 2 show minimal discrepancy for a network
trained on autoencoding task; see the supplementary ma-
terial for further details and additional experiments. More
precise methods for quantifying partial freeness exist (Chen
et al., 2016), but we leave this analysis for future work.

ρ(λ)H=H0+H1H=H0+QH1QT24680.51.01.5λGeometry of Neural Networks

6.1. Product Wishart distribution and H1

In the single-hidden-layer ReLU case, we can write H1 as,

H1 =

(cid:18) 0
H12

T

(cid:19)

,

H12
0

where its off-diagonal blocks are given by the matrix H12,

[H12]ab;cd =

eiµ

∂
∂W (2)
cd

∂ ˆyiµ
∂W (1)
ab

=

ecµδadθ(z(1)

aµ )xbµ .

1
m

1
m

(cid:88)

iµ
m
(cid:88)

µ=1

Here it is understood that the matrix H12 is obtained from
the tensor [H12]ab;cd by independently vectorizing the ﬁrst
two and last two dimensions, δad is the Kronecker delta
function, and θ is the Heaviside theta function. As dis-
cussed above, we take the error to be normally distributed,

ecµ = (cid:0) (cid:88)

W (2)

cj [z(1)

jµ ]+ − ycµ(x)(cid:1) ∼ N (0, 2(cid:15)) .

(24)

j

We are interested in the situation in which the layer width
n and the number of examples m are large, in which case
θ(z(1)
aµ ) can be interpreted as a mask that eliminates half of
the terms in the sum over µ. If we reorder the indices so
that the surviving ones appear ﬁrst, we can write,

H12 ≈

ecˆµxbˆµ =

In ⊗ ˆeˆxT ,

1
m

δad

m/2
(cid:88)

ˆµ=1

1
m

where we have written ˆe and ˆx to denote the n× m
2 matrices
derived from e and x by removing the vanishing half of
their columns.

Owing to the block off-diagonal structure of H1,
the
squares of its eigenvalues are determined by the eigenval-
ues of the product of the blocks,

H12H12

T =

1
m2 In ⊗ (ˆeˆxT )(ˆeˆxT )T .

The Kronecker product gives an n-fold degeneracy to each
eigenvalue, but the spectral density is the same as that of

M ≡

1
m2 (ˆeˆxT )(ˆeˆxT )T .

It follows that the spectral density of H1 is related to that
of M ,

ρH1(λ) = |λ|ρM (λ2) .

(25)

(26)

(27)

(23)

Figure 3. Spectrum of H1 at initialization, using 4 × 4 downsam-
pled, grayscaled, whitened CIFAR-10 in a single layer ReLU au-
toencoder with 16 hidden units. Error signals have been replaced
by noise distributed as N (0, 100), i.e. (cid:15) = 50. Theoretical pre-
diction (red) matches well. Left φ = 1/16, right φ = 1/4.

the cavity method (Dupic & Castillo, 2014). As the speciﬁc
form of the result is not particularly enlightening, we defer
its presentation to the supplementary material. The result
may also be derived using free probability theory (Muller,
2002; Burda et al., 2010). From either perspective it is pos-
sible to derive the the R transform, which reads,

RH1(z) =

(cid:15)φz
2 − (cid:15)φ2z2 .

(28)

See ﬁg. 3 for a comparison of our theoretical prediction for
the spectral density to numerical simulations4.

6.2. H0 and the Wishart assumption

Unlike the situation for H1, to the best of our knowledge
the limiting spectral density of H0 cannot easily be
deduced from known results in the literature. In principle
it can be computed using diagrammatics and the method
of moments (Feinberg & Zee, 1997; Tao, 2012), but this
approach is complicated by serious technical difﬁculties
– for example, the matrix has both block structure and
Kronecker structure, and there are nonlinear functions
applied to the elements. Nevertheless, we may make
some progress by focusing our attention on the smallest
eigenvalues of H0, which we expect to be the most relevant
for computing the smallest eigenvalues of H.

The empirical spectral density of H0 for an autoen-
coding network is shown in ﬁg. 4. At ﬁrst glance, this
distribution does not closely resemble the Marchenko-
Pastur distribution (see, e.g., the (cid:15) = 0 curve in ﬁg. 1)
owing to its heavy tail and small eigenvalue gap. On the
other hand, we are not concerned with the large eigen-
values, and even though the gap is small, its scaling with
φ appears to be well-approximated by λ− from eqn. (8)

Notice that M is a Wishart matrix where each factor is itself
a product of real Gaussian random matrices. The spectral
density for matrices of this type has been computed using

4As the derivation requires that the error signals are random, in
the simulations we manually overwrite the network’s error signals
with random noise.

−15−10−5051015Eigenvalue0.000.050.100.150.200.250.30Probability density H1 over 10 runs, output + noise phi value = 0.0625−15−10−5051015Eigenvalue0.000.050.100.150.20Probability density H1 over 10 runs, output + noise phi value = 0.25Geometry of Neural Networks

Figure 4. The spectrum of H0 at initialization of a single layer
ReLU autoencoder with 16 hidden units and 256 4 × 4 down-
sampled, grayscaled, whitened CIFAR-10 images. There are 16
null directions, and the corresponding zero eigenvalues have been
removed. The inset shows the values of the smallest 35 nonzero
eigenvalues. The positive value of the ﬁrst datapoint reﬂects the
existence of a nonzero gap.

Figure 5. Evolution of the H0 spectral gap as a function of 1/φ,
comparing the Marcenko-Pastur (red) against empirical results for
a 16-hidden unit single-layer ReLU autoencoder at initialization
(black dots, each averaged over 30 independent runs). Dataset
was taken from 4×4 downsampled, grayscaled, whitened CIFAR-
10 images. A single ﬁt parameter, characterizing the variance of
the matrix elements in the Wishart ensemble, is used.

for appropriate σ. See ﬁg. 5. This observation suggests
that as a ﬁrst approximation, it is sensible to continue to
represent the limiting spectral distribution of H0 with the
Marchenko-Pastur distribution.

6.3. Improved approximation to the Hessian

The above analysis motivates us to propose an improved
approximation to RH (z),

RH (z) =

σ
1 − σzφ

+

(cid:15)φz
2 − (cid:15)φ2z2 ,

(29)

where the ﬁrst term is the R transform of the Marchenko-
Pastur distribution with the σ parameter restored. As be-
fore, an algebraic equation deﬁning the Stieltjes transform
of ρH can be deduced from this expression,

2 = 2(cid:0)z − σ(1 − φ)(cid:1)G − φ(cid:0)2σz + (cid:15)(1 − φ)(cid:1)G2

− (cid:15)φ2(cid:0)z + σ(φ − 2)(cid:1)G3

H + z(cid:15)σφ3G4

H .

H

(30)

Using the same techniques as in sec. 4, we can use this
equation to obtain the function α((cid:15), φ). We again ﬁnd the
same 3/2 scaling, i.e.,

α((cid:15), φ) ≈ ˜α0(φ)

(31)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:15) − (cid:15)c
(cid:15)c

(cid:12)
(cid:12)
(cid:12)
(cid:12)

3/2

.

Compared with eqn. (20), the coefﬁcient function ˜α0(φ)
differs from α0(φ) and,

(cid:15)c =

σ2(cid:0)27 − 18χ − χ2 + 8χ3/2(cid:1)
32φ(1 − φ)3

, χ = 1+16φ−8φ2 .

(32)

Despite the apparent pole at φ = 1, (cid:15)c actually vanishes
there,

(cid:15)c ≈

σ2(1 − φ)3 + O(1 − φ)4 .

(33)

8
27

Curiously, for σ = 1/2, this is precisely the same behavior
we found for the behavior of (cid:15)c near 1 in the Wishart
plus Wigner approximation in sec. 4. This observation,
combined with the fact that the 3/2 scaling in eqn. (31) is
also what we found in sec. 4, suggest that it is H0, rather
than H1, that is governing the behavior near (cid:15)c.

6.4. Empirical distribution of critical points

We conduct large-scale experiments to examine the distri-
bution of critical points and compare with our theoretical
predictions. Uniformly sampling critical points of varying
energy is a difﬁcult problem. Instead, we take more of a
brute force approach: for each possible value of the index,
we aim to collect many measurements of the energy and
compute the mean value. Because we cannot control the
index of the obtained critical point, we run a very large
number of experiments (∼50k) in order to obtain sufﬁcient
data for each value of α. This procedure appears to be a
fairly robust method for inferring the α((cid:15)) curve.

the following heuristic for ﬁnding critical
We adopt
points. First we optimize the network with standard gradi-
ent descent until the loss reaches a random value between
0 and the initial loss. From that point on, we switch
to minimizing a new objective, Jg = |∇θL|2, which,
unlike the primary objective, is attracted to saddle points.
Gradient descent on Jg only requires the computation of
Hessian-vector products and can be executed efﬁciently.

0.00.51.01.52.02.53.0Value of eigenvalue0510152025303540Histogram05101520253035kth smallest non-zero eigenvaluek0.0100.0150.0200.0250.0300.0350.0400.045Value0123456780.0000.0020.0040.0060.0080.0100.0120.0140.016Gap in      spectrumGeometry of Neural Networks

(a) Index of critical points versus energy

(b) Energy of minimizers versus parameters/data points

Figure 6. Empirical observations of the distribution of critical points in single-hidden-layer tanh networks with varying ratios of param-
eters to data points, φ. (a) Each point represents the mean energy of critical points with index α, averaged over ∼200 training runs. Solid
lines are best ﬁt curves for small α ≈ α0|(cid:15) − (cid:15)c|3/2. The good agreement (emphasized in the inset, which shows the behavior for small
α) provides support for our theoretical prediction of the 3/2 scaling. (b) The best ﬁt value of (cid:15)c from (a) versus φ. A surprisingly good ﬁt
is obtained with (cid:15)c = 1
2 (1 − φ). The difference between the curves shows the beneﬁt obtained
from using a nonlinear activation function.

2 (1 − φ)2. Linear networks obey (cid:15)c = 1

We discard any run for which the ﬁnal Jg > 10−6;
otherwise we record the ﬁnal energy and index.

7. Conclusions

We consider relatively small networks and datasets in
order to run a large number of experiments. We train
single-hidden-layer tanh networks of size n = 16, which
also equals the input and output dimensionality. For each
training run, the data and targets are randomly sampled
from standard normal distributions, which makes this
a kind of memorization task. The results are summa-
rized in ﬁg. 6. We observe that for small α, the scaling
α ≈ |(cid:15) − (cid:15)c|3/2 is a good approximation, especially for
smaller φ. This agreement with our theoretical predictions
provides support for our analytical framework and for the
validity of our assumptions.

2 (1 − φ).

As a byproduct of our experiments, we observe that the
energy of minimizers is well described by a simple func-
tion, (cid:15)c = 1
2 (1 − φ)2. Curiously, a similar functional form
was derived for linear networks (Advani & Ganguli, 2016),
(cid:15)c = 1
In both cases, the value at φ = 0 and
φ = 1 is understood simply: at φ = 0, the network has
zero effective capacity and the variance of the target distri-
bution determines the loss; at φ = 1, the matrix of hidden
units is no longer rank constrained and can store the entire
input-output map. For intermediate values of φ, the fact
that the exponent of (1 − φ) is larger for tanh networks
than for linear networks is the mathematical manifestation
of the nonlinear network’s better performance for the same
number of parameters. Inspired by these observations and
by the analysis of Zhang et al. (2016), we speculate that
this result may have a simple information-theoretic expla-
nation, but we leave a quantitative analysis to future work.

We introduced a new analytical framework for studying
the Hessian matrix of neural networks based on free
probability and random matrix theory. By decomposing
the Hessian into two pieces H = H0 + H1 one can
systematically study the behavior of the spectrum and
associated quantities as a function of the energy (cid:15) of a
critical point. The approximations invoked are on H0 and
H1 separately, which enables the analysis to move beyond
the simple representation of the Hessian as a member of
the Gaussian Orthogonal Ensemble of random matrices.

We derived explicit predictions for the spectrum and
the index under a set of simplifying assumptions. We
found empirical evidence in support of our prediction
that that small α ≈ |(cid:15) − (cid:15)c|3/2, raising the question of
how universal the 3/2 scaling may be, especially given the
results of (Bray & Dean, 2007). We also showed how
some of our assumptions can be relaxed at the expense of
reduced generality of network architecture and increased
technical calculations. An interesting result of our numeri-
cal simulations of a memorization task is that the energy of
minimizers appears to be well-approximated by a simple
function of φ. We leave the explanation of this observation
as an open problem for future work.

Acknowledgements

We thank Dylan J. Foster, Justin Gilmer, Daniel Holtmann-
Rice, Ben Poole, Maithra Raghu, Samuel S. Schoenholz,
Jascha Sohl-Dickstein, and Felix X. Yu for useful com-
ments and discussions.

α0.00.10.20.30.40.50.000.050.100.150.200.250.30ϕ=2/3ϕ=1/2ϕ=1/3ϕ=1/4ϕ=1/8ϕ=1/16ϵα0.360.380.400.420.440.460.000.020.040.060.08ϵϵc12(1-ϕ)12(1-ϕ)20.00.20.40.60.81.00.00.10.20.30.40.5BenefitofnonlinearityϕGeometry of Neural Networks

References

Advani, Madhu and Ganguli, Surya. Statistical mechanics
of optimal convex inference in high dimensions. Physi-
cal Review X, 6(3):031034, 2016.

Keskar, Nitish Shirish, Mudigere, Dheevatsa, Nocedal,
Jorge, Smelyanskiy, Mikhail, and Tang, Ping Tak Pe-
ter. On large-batch training for deep learning: Gen-
ICLR 2017. URL
eralization gap and sharp minima.
http://arxiv.org/abs/1609.04836.

Aggarwal, Amol.

wigner matrices with few moments.
arXiv:1612.00421.

Bulk universality for generalized
arXiv preprint

Laisant, C-A.

Int´egration des fonctions inverses. Nou-
velles annales de math´ematiques, journal des candidats
aux ´ecoles polytechnique et normale, 5:253–257, 1905.

Bray, Alan J. and Dean, David S. Statistics of critical
points of gaussian ﬁelds on large-dimensional spaces.
Phys. Rev. Lett., 98:150201, Apr 2007. doi: 10.1103/
PhysRevLett.98.150201. URL http://link.aps.
org/doi/10.1103/PhysRevLett.98.150201.

Burda, Z, Jarosz, A, Livan, G, Nowak, MA, and Swiech,
A. Eigenvalues and singular values of products of rect-
angular gaussian random matrices. Physical Review E,
82(6):061114, 2010.

Chen, Jiahao, Van Voorhis, Troy, and Edelman, Alan.
arXiv preprint

Partial freeness of random matrices.
arXiv:1204.2257, 2016.

Choromanska, Anna, Henaff, Mikael, Mathieu, Micha¨el,
Arous, G´erard Ben, and LeCun, Yann. The loss surface
of multilayer networks. JMLR, 38, 2015.

Dauphin, Yann N, Pascanu, Razvan, Gulcehre, Caglar,
Cho, Kyunghyun, Ganguli, Surya, and Bengio, Yoshua.
Identifying and attacking the saddle point problem in
high-dimensional non-convex optimization. Advances
in Neural Information Processing Systems 27, pp. 2933–
2941, 2014.

Dupic, Thomas and Castillo, Isaac P´erez. Spectral density
of products of wishart dilute random matrices. part i: the
dense case. arXiv preprint arXiv:1401.7802, 2014.

Marˇcenko, Vladimir A and Pastur, Leonid Andreevich.
Distribution of eigenvalues for some sets of random ma-
trices. Mathematics of the USSR-Sbornik, 1(4):457,
1967.

Muller, Ralf R. On the asymptotic eigenvalue distribution
IEEE
of concatenated vector-valued fading channels.
Transactions on Information Theory, 48(7):2086–2091,
2002.

Neyshabur, Behnam, Salakhutdinov, Ruslan, and Srebro,
Nathan. Path-sgd: Path-normalized optimization in deep
neural networks. pp. 2413–2421, 2015.

Safran, Itay and Shamir, Ohad. On the quality of the initial
basin in overspeciﬁed neural networks. JMLR, 48, 2016.
URL http://arxiv.org/abs/1511.04210.

Saxe, Andrew M., McClelland, James L., and Ganguli,
Surya. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks. ICLR.

Speicher, Roland. Free probability theory. arXiv preprint

arXiv:0911.0087, 2009.

Tao, T. and Vu, V. Random covariance matrices: universal-
ity of local statistics of eigenvalues. Annals of Probabil-
ity, 40(3):1285–1315, 2012.

Tao, Terence. Topics in random matrix theory, volume 132.
American Mathematical Society Providence, RI, 2012.

Feinberg, Joshua and Zee, Anthony. Renormalizing rectan-
gles and other topics in random matrix theory. Journal
of statistical physics, 87(3-4):473–504, 1997.

Wigner, Eugene P. Characteristic vectors of bordered ma-
trices with inﬁnite dimensions. Annals of Mathematics,
pp. 548–564, 1955.

Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Recht, Ben-
jamin, and Vinyals, Oriol. Understanding deep learn-
ing requires rethinking generalization. arXiv preprint
arXiv:1611.03530, 2016.

Freeman, C. Daniel and Bruna, Joan. Topology and geom-
etry of half-rectiﬁed network optimization. 2016. URL
http://arxiv.org/abs/1611.01540.

Goodfellow, Ian J., Vinyals, Oriol, and Saxe, Andrew M.
Qualitatively characterizing neural network optimization
problems. ICLR 2015. URL http://arxiv.org/
abs/1412.6544.

Kawaguchi, Kenji. Deep learning without poor local min-
ima. Advances in Neural Information Processing Sys-
tems 29, pp. 586–594, 2016.

