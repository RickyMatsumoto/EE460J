Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

Chelsea Finn 1 Pieter Abbeel 1 2 Sergey Levine 1

Abstract

the form of computation required to complete the task.

We propose an algorithm for meta-learning that
is model-agnostic, in the sense that it is com-
patible with any model trained with gradient de-
scent and applicable to a variety of different
learning problems, including classiﬁcation, re-
gression, and reinforcement learning. The goal
of meta-learning is to train a model on a vari-
ety of learning tasks, such that it can solve new
learning tasks using only a small number of train-
ing samples. In our approach, the parameters of
the model are explicitly trained such that a small
number of gradient steps with a small amount
of training data from a new task will produce
good generalization performance on that task. In
effect, our method trains the model to be easy
to ﬁne-tune. We demonstrate that this approach
leads to state-of-the-art performance on two few-
shot image classiﬁcation benchmarks, produces
good results on few-shot regression, and acceler-
ates ﬁne-tuning for policy gradient reinforcement
learning with neural network policies.

1. Introduction

Learning quickly is a hallmark of human intelligence,
whether it involves recognizing objects from a few exam-
ples or quickly learning new skills after just minutes of
experience. Our artiﬁcial agents should be able to do the
same, learning and adapting quickly from only a few exam-
ples, and continuing to adapt as more data becomes avail-
able. This kind of fast and ﬂexible learning is challenging,
since the agent must integrate its prior experience with a
small amount of new information, while avoiding overﬁt-
ting to the new data. Furthermore, the form of prior ex-
perience and new data will depend on the task. As such,
for the greatest applicability, the mechanism for learning to
learn (or meta-learning) should be general to the task and

1University of California, Berkeley 2OpenAI. Correspondence

to: Chelsea Finn <cbﬁnn@eecs.berkeley.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

In this work, we propose a meta-learning algorithm that
is general and model-agnostic, in the sense that it can be
directly applied to any learning problem and model that
is trained with a gradient descent procedure. Our focus
is on deep neural network models, but we illustrate how
our approach can easily handle different architectures and
different problem settings, including classiﬁcation, regres-
sion, and policy gradient reinforcement learning, with min-
imal modiﬁcation. In meta-learning, the goal of the trained
model is to quickly learn a new task from a small amount
of new data, and the model is trained by the meta-learner
to be able to learn on a large number of different tasks.
The key idea underlying our method is to train the model’s
initial parameters such that the model has maximal perfor-
mance on a new task after the parameters have been up-
dated through one or more gradient steps computed with
a small amount of data from that new task. Unlike prior
meta-learning methods that learn an update function or
learning rule (Schmidhuber, 1987; Bengio et al., 1992;
Andrychowicz et al., 2016; Ravi & Larochelle, 2017), our
algorithm does not expand the number of learned param-
eters nor place constraints on the model architecture (e.g.
by requiring a recurrent model (Santoro et al., 2016) or a
Siamese network (Koch, 2015)), and it can be readily com-
bined with fully connected, convolutional, or recurrent neu-
ral networks. It can also be used with a variety of loss func-
tions, including differentiable supervised losses and non-
differentiable reinforcement learning objectives.

The process of training a model’s parameters such that a
few gradient steps, or even a single gradient step, can pro-
duce good results on a new task can be viewed from a fea-
ture learning standpoint as building an internal representa-
tion that is broadly suitable for many tasks. If the internal
representation is suitable to many tasks, simply ﬁne-tuning
the parameters slightly (e.g. by primarily modifying the top
layer weights in a feedforward model) can produce good
results. In effect, our procedure optimizes for models that
are easy and fast to ﬁne-tune, allowing the adaptation to
happen in the right space for fast learning. From a dynami-
cal systems standpoint, our learning process can be viewed
as maximizing the sensitivity of the loss functions of new
tasks with respect to the parameters: when the sensitivity
is high, small local changes to the parameters can lead to

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

large improvements in the task loss.

The primary contribution of this work is a simple model-
and task-agnostic algorithm for meta-learning that trains
a model’s parameters such that a small number of gradi-
ent updates will lead to fast learning on a new task. We
demonstrate the algorithm on different model types, includ-
ing fully connected and convolutional networks, and in sev-
eral distinct domains, including few-shot regression, image
classiﬁcation, and reinforcement learning. Our evaluation
shows that our meta-learning algorithm compares favor-
ably to state-of-the-art one-shot learning methods designed
speciﬁcally for supervised classiﬁcation, while using fewer
parameters, but that it can also be readily applied to regres-
sion and can accelerate reinforcement learning in the pres-
ence of task variability, substantially outperforming direct
pretraining as initialization.

2. Model-Agnostic Meta-Learning

We aim to train models that can achieve rapid adaptation, a
problem setting that is often formalized as few-shot learn-
ing. In this section, we will deﬁne the problem setup and
present the general form of our algorithm.

2.1. Meta-Learning Problem Set-Up

The goal of few-shot meta-learning is to train a model that
can quickly adapt to a new task using only a few datapoints
and training iterations. To accomplish this, the model or
learner is trained during a meta-learning phase on a set
of tasks, such that the trained model can quickly adapt to
new tasks using only a small number of examples or trials.
In effect, the meta-learning problem treats entire tasks as
training examples. In this section, we formalize this meta-
learning problem setting in a general manner, including
brief examples of different learning domains. We will dis-
cuss two different learning domains in detail in Section 3.

We consider a model, denoted f ,
that maps observa-
tions x to outputs a. During meta-learning, the model
is trained to be able to adapt to a large or inﬁnite num-
ber of tasks. Since we would like to apply our frame-
work to a variety of learning problems, from classiﬁca-
tion to reinforcement learning, we introduce a generic
notion of a learning task below.
Formally, each task
T = {L(x1, a1, . . . , xH , aH ), q(x1), q(xt+1|xt, at), H}
consists of a loss function L, a distribution over initial ob-
servations q(x1), a transition distribution q(xt+1|xt, at),
and an episode length H. In i.i.d. supervised learning prob-
lems, the length H = 1. The model may generate samples
of length H by choosing an output at at each time t. The
loss L(x1, a1, . . . , xH , aH ) → R, provides task-speciﬁc
feedback, which might be in the form of a misclassiﬁcation
loss or a cost function in a Markov decision process.

θ

∇L1

θ∗
1

∇L3
∇L2

θ∗
3

θ∗
2

Figure 1. Diagram of our model-agnostic meta-learning algo-
rithm (MAML), which optimizes for a representation θ that can
quickly adapt to new tasks.

In our meta-learning scenario, we consider a distribution
over tasks p(T ) that we want our model to be able to adapt
to. In the K-shot learning setting, the model is trained to
learn a new task Ti drawn from p(T ) from only K samples
drawn from qi and feedback LTi generated by Ti. During
meta-training, a task Ti is sampled from p(T ), the model
is trained with K samples and feedback from the corre-
sponding loss LTi from Ti, and then tested on new samples
from Ti. The model f is then improved by considering how
the test error on new data from qi changes with respect to
the parameters. In effect, the test error on sampled tasks Ti
serves as the training error of the meta-learning process. At
the end of meta-training, new tasks are sampled from p(T ),
and meta-performance is measured by the model’s perfor-
mance after learning from K samples. Generally, tasks
used for meta-testing are held out during meta-training.

2.2. A Model-Agnostic Meta-Learning Algorithm

In contrast to prior work, which has sought to train re-
current neural networks that ingest entire datasets (San-
toro et al., 2016; Duan et al., 2016b) or feature embed-
dings that can be combined with nonparametric methods at
test time (Vinyals et al., 2016; Koch, 2015), we propose a
method that can learn the parameters of any standard model
via meta-learning in such a way as to prepare that model
for fast adaptation. The intuition behind this approach is
that some internal representations are more transferrable
than others. For example, a neural network might learn
internal features that are broadly applicable to all tasks in
p(T ), rather than a single individual task. How can we en-
courage the emergence of such general-purpose representa-
tions? We take an explicit approach to this problem: since
the model will be ﬁne-tuned using a gradient-based learn-
ing rule on a new task, we will aim to learn a model in such
a way that this gradient-based learning rule can make rapid
progress on new tasks drawn from p(T ), without overﬁt-
ting. In effect, we will aim to ﬁnd model parameters that
are sensitive to changes in the task, such that small changes
in the parameters will produce large improvements on the
loss function of any task drawn from p(T ), when altered in
the direction of the gradient of that loss (see Figure 1). We

meta-learninglearning/adaptationModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

Algorithm 1 Model-Agnostic Meta-Learning
Require: p(T ): distribution over tasks
Require: α, β: step size hyperparameters
1: randomly initialize θ
2: while not done do
3:
4:
5:
6:

Sample batch of tasks Ti ∼ p(T )
for all Ti do

Evaluate ∇θLTi (fθ) with respect to K examples
Compute adapted parameters with gradient de-
scent: θ(cid:48)

i = θ − α∇θLTi(fθ)

end for
Update θ ← θ − β∇θ

7:
8:
9: end while

(cid:80)

Ti∼p(T ) LTi(fθ(cid:48)

i

)

make no assumption on the form of the model, other than
to assume that it is parametrized by some parameter vector
θ, and that the loss function is smooth enough in θ that we
can use gradient-based learning techniques.

Formally, we consider a model
represented by a
parametrized function fθ with parameters θ. When adapt-
ing to a new task Ti, the model’s parameters θ become θ(cid:48)
i.
In our method, the updated parameter vector θ(cid:48)
i is computed
using one or more gradient descent updates on task Ti. For
example, when using one gradient update,
θ(cid:48)
i = θ − α∇θLTi(fθ).
The step size α may be ﬁxed as a hyperparameter or meta-
learned. For simplicity of notation, we will consider one
gradient update for the rest of this section, but using multi-
ple gradient updates is a straightforward extension.

The model parameters are trained by optimizing for the per-
formance of fθ(cid:48)
with respect to θ across tasks sampled from
p(T ). More concretely, the meta-objective is as follows:
LTi(fθ−α∇θLTi (fθ))

LTi(fθ(cid:48)

) =

(cid:88)

(cid:88)

i

i

min
θ

Ti∼p(T )

Ti∼p(T )

Note that the meta-optimization is performed over the
model parameters θ, whereas the objective is computed us-
ing the updated model parameters θ(cid:48). In effect, our pro-
posed method aims to optimize the model parameters such
that one or a small number of gradient steps on a new task
will produce maximally effective behavior on that task.

The meta-optimization across tasks is performed via
stochastic gradient descent (SGD), such that the model pa-
rameters θ are updated as follows:
(cid:88)

θ ← θ − β∇θ

LTi(fθ(cid:48)

)

i

(1)

Ti∼p(T )

where β is the meta step size. The full algorithm, in the
general case, is outlined in Algorithm 1.

The MAML meta-gradient update involves a gradient
through a gradient. Computationally, this requires an addi-
tional backward pass through f to compute Hessian-vector

products, which is supported by standard deep learning li-
In our
braries such as TensorFlow (Abadi et al., 2016).
experiments, we also include a comparison to dropping
this backward pass and using a ﬁrst-order approximation,
which we discuss in Section 5.2.

3. Species of MAML
In this section, we discuss speciﬁc instantiations of our
meta-learning algorithm for supervised learning and rein-
forcement learning. The domains differ in the form of loss
function and in how data is generated by the task and pre-
sented to the model, but the same basic adaptation mecha-
nism can be applied in both cases.

3.1. Supervised Regression and Classiﬁcation

Few-shot learning is well-studied in the domain of super-
vised tasks, where the goal is to learn a new function from
only a few input/output pairs for that task, using prior data
from similar tasks for meta-learning. For example, the goal
might be to classify images of a Segway after seeing only
one or a few examples of a Segway, with a model that has
previously seen many other types of objects. Likewise, in
few-shot regression, the goal is to predict the outputs of
a continuous-valued function from only a few datapoints
sampled from that function, after training on many func-
tions with similar statistical properties.

To formalize the supervised regression and classiﬁcation
problems in the context of the meta-learning deﬁnitions in
Section 2.1, we can deﬁne the horizon H = 1 and drop the
timestep subscript on xt, since the model accepts a single
input and produces a single output, rather than a sequence
of inputs and outputs. The task Ti generates K i.i.d. ob-
servations x from qi, and the task loss is represented by the
error between the model’s output for x and the correspond-
ing target values y for that observation and task.

Two common loss functions used for supervised classiﬁca-
tion and regression are cross-entropy and mean-squared er-
ror (MSE), which we will describe below; though, other su-
pervised loss functions may be used as well. For regression
tasks using mean-squared error, the loss takes the form:

LTi(fφ) =

(cid:88)

(cid:107)fφ(x(j)) − y(j)(cid:107)2
2,

(2)

x(j),y(j)∼Ti

where x(j), y(j) are an input/output pair sampled from task
Ti.
In K-shot regression tasks, K input/output pairs are
provided for learning for each task.

Similarly, for discrete classiﬁcation tasks with a cross-
entropy loss, the loss takes the form:
(cid:88)
y(j) log fφ(x(j))

LTi(fφ) =

x(j),y(j)∼Ti

(3)

+ (1 − y(j)) log(1 − fφ(x(j)))

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

Algorithm 2 MAML for Few-Shot Supervised Learning
Require: p(T ): distribution over tasks
Require: α, β: step size hyperparameters

Algorithm 3 MAML for Reinforcement Learning
Require: p(T ): distribution over tasks
Require: α, β: step size hyperparameters

1: randomly initialize θ
2: while not done do
3:
4:
5:
6:

Sample batch of tasks Ti ∼ p(T )
for all Ti do

Sample K datapoints D = {x(j), y(j)} from Ti
Evaluate ∇θLTi (fθ) using D and LTi in Equation (2)
or (3)
Compute adapted parameters with gradient descent:
θ(cid:48)
i = θ − α∇θLTi (fθ)
Sample datapoints D(cid:48)
meta-update

i = {x(j), y(j)} from Ti for the

end for
Update θ ← θ − β∇θ
and LTi in Equation 2 or 3

(cid:80)

11: end while

Ti∼p(T ) LTi (fθ(cid:48)

i

) using each D(cid:48)
i

7:

8:

9:
10:

1: randomly initialize θ
2: while not done do
3:
4:
5:

Sample batch of tasks Ti ∼ p(T )
for all Ti do

6:
7:

8:

9:
10:

Sample K trajectories D = {(x1, a1, ...xH )} using fθ
in Ti
Evaluate ∇θLTi (fθ) using D and LTi in Equation 4
Compute adapted parameters with gradient descent:
θ(cid:48)
i = θ − α∇θLTi (fθ)
Sample trajectories D(cid:48)
in Ti
end for
Update θ ← θ − β∇θ
and LTi in Equation 4

i = {(x1, a1, ...xH )} using fθ(cid:48)

Ti∼p(T ) LTi (fθ(cid:48)

) using each D(cid:48)
i

(cid:80)

i

i

11: end while

According to the conventional terminology, K-shot classi-
ﬁcation tasks use K input/output pairs from each class, for
a total of N K data points for N -way classiﬁcation. Given a
distribution over tasks p(Ti), these loss functions can be di-
rectly inserted into the equations in Section 2.2 to perform
meta-learning, as detailed in Algorithm 2.

3.2. Reinforcement Learning

In reinforcement learning (RL), the goal of few-shot meta-
learning is to enable an agent to quickly acquire a policy for
a new test task using only a small amount of experience in
the test setting. A new task might involve achieving a new
goal or succeeding on a previously trained goal in a new
environment. For example, an agent might learn to quickly
ﬁgure out how to navigate mazes so that, when faced with
a new maze, it can determine how to reliably reach the exit
with only a few samples. In this section, we will discuss
how MAML can be applied to meta-learning for RL.

Each RL task Ti contains an initial state distribution qi(x1)
and a transition distribution qi(xt+1|xt, at), and the loss
LTi corresponds to the (negative) reward function R. The
entire task is therefore a Markov decision process (MDP)
with horizon H, where the learner is allowed to query a
limited number of sample trajectories for few-shot learn-
ing. Any aspect of the MDP may change across tasks in
p(T ). The model being learned, fθ, is a policy that maps
from states xt to a distribution over actions at at each
timestep t ∈ {1, ..., H}. The loss for task Ti and model
fφ takes the form

LTi(fφ) = −Ext,at∼fφ,qTi

Ri(xt, at)

.

(4)

(cid:35)

(cid:34) H
(cid:88)

t=1

In K-shot reinforcement learning, K rollouts from fθ and
task Ti, (x1, a1, ...xH ), and the corresponding rewards
R(xt, at), may be used for adaptation on a new task Ti.

Since the expected reward is generally not differentiable
due to unknown dynamics, we use policy gradient meth-
ods to estimate the gradient both for the model gradient
update(s) and the meta-optimization. Since policy gradi-
ents are an on-policy algorithm, each additional gradient
step during the adaptation of fθ requires new samples from
the current policy fθi(cid:48) . We detail the algorithm in Algo-
rithm 3. This algorithm has the same structure as Algo-
rithm 2, with the principal difference being that steps 5 and
8 require sampling trajectories from the environment cor-
responding to task Ti. Practical implementations of this
method may also use a variety of improvements recently
proposed for policy gradient algorithms, including state
or action-dependent baselines and trust regions (Schulman
et al., 2015).

4. Related Work

The method that we propose in this paper addresses the
general problem of meta-learning (Thrun & Pratt, 1998;
Schmidhuber, 1987; Naik & Mammone, 1992), which in-
cludes few-shot learning. A popular approach for meta-
learning is to train a meta-learner that learns how to up-
date the parameters of the learner’s model (Bengio et al.,
1992; Schmidhuber, 1992; Bengio et al., 1990). This ap-
proach has been applied to learning to optimize deep net-
works (Hochreiter et al., 2001; Andrychowicz et al., 2016;
Li & Malik, 2017), as well as for learning dynamically
changing recurrent networks (Ha et al., 2017). One recent
approach learns both the weight initialization and the opti-
mizer, for few-shot image recognition (Ravi & Larochelle,
2017). Unlike these methods, the MAML learner’s weights
are updated using the gradient, rather than a learned update;
our method does not introduce additional parameters for
meta-learning nor require a particular learner architecture.

Few-shot learning methods have also been developed for

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

speciﬁc tasks such as generative modeling (Edwards &
Storkey, 2017; Rezende et al., 2016) and image recogni-
tion (Vinyals et al., 2016). One successful approach for
few-shot classiﬁcation is to learn to compare new exam-
ples in a learned metric space using e.g. Siamese net-
works (Koch, 2015) or recurrence with attention mech-
anisms (Vinyals et al., 2016; Shyam et al., 2017; Snell
et al., 2017). These approaches have generated some of the
most successful results, but are difﬁcult to directly extend
to other problems, such as reinforcement learning. Our
method, in contrast, is agnostic to the form of the model
and to the particular learning task.

Another approach to meta-learning is to train memory-
augmented models on many tasks, where the recurrent
learner is trained to adapt to new tasks as it is rolled out.
Such networks have been applied to few-shot image recog-
nition (Santoro et al., 2016; Munkhdalai & Yu, 2017) and
learning “fast” reinforcement learning agents (Duan et al.,
2016b; Wang et al., 2016). Our experiments show that
our method outperforms the recurrent approach on few-
shot classiﬁcation. Furthermore, unlike these methods, our
approach simply provides a good weight initialization and
uses the same gradient descent update for both the learner
and meta-update. As a result, it is straightforward to ﬁne-
tune the learner for additional gradient steps.

Our approach is also related to methods for initialization of
deep networks. In computer vision, models pretrained on
large-scale image classiﬁcation have been shown to learn
effective features for a range of problems (Donahue et al.,
In contrast, our method explicitly optimizes the
2014).
model for fast adaptability, allowing it to adapt to new tasks
with only a few examples. Our method can also be viewed
as explicitly maximizing sensitivity of new task losses to
the model parameters. A number of prior works have ex-
plored sensitivity in deep networks, often in the context of
initialization (Saxe et al., 2014; Kirkpatrick et al., 2016).
Most of these works have considered good random initial-
izations, though a number of papers have addressed data-
dependent initializers (Kr¨ahenb¨uhl et al., 2016; Salimans &
Kingma, 2016), including learned initializations (Husken
& Goerick, 2000; Maclaurin et al., 2015). In contrast, our
method explicitly trains the parameters for sensitivity on
a given task distribution, allowing for extremely efﬁcient
adaptation for problems such as K-shot learning and rapid
reinforcement learning in only one or a few gradient steps.

5. Experimental Evaluation

The goal of our experimental evaluation is to answer the
following questions: (1) Can MAML enable fast learning
of new tasks? (2) Can MAML be used for meta-learning
in multiple different domains, including supervised regres-
sion, classiﬁcation, and reinforcement learning? (3) Can a

model learned with MAML continue to improve with addi-
tional gradient updates and/or examples?

All of the meta-learning problems that we consider require
some amount of adaptation to new tasks at test-time. When
possible, we compare our results to an oracle that receives
the identity of the task (which is a problem-dependent rep-
resentation) as an additional input, as an upper bound on
the performance of the model. All of the experiments were
performed using TensorFlow (Abadi et al., 2016), which al-
lows for automatic differentiation through the gradient up-
date(s) during meta-learning. The code is available online1.

5.1. Regression

We start with a simple regression problem that illustrates
the basic principles of MAML. Each task involves regress-
ing from the input to the output of a sine wave, where the
amplitude and phase of the sinusoid are varied between
tasks. Thus, p(T ) is continuous, where the amplitude
varies within [0.1, 5.0] and the phase varies within [0, π],
and the input and output both have a dimensionality of 1.
During training and testing, datapoints x are sampled uni-
formly from [−5.0, 5.0]. The loss is the mean-squared error
between the prediction f (x) and true value. The regres-
sor is a neural network model with 2 hidden layers of size
40 with ReLU nonlinearities. When training with MAML,
we use one gradient update with K = 10 examples with
a ﬁxed step size α = 0.01, and use Adam as the meta-
optimizer (Kingma & Ba, 2015). The baselines are like-
wise trained with Adam. To evaluate performance, we ﬁne-
tune a single meta-learned model on varying numbers of K
examples, and compare performance to two baselines: (a)
pretraining on all of the tasks, which entails training a net-
work to regress to random sinusoid functions and then, at
test-time, ﬁne-tuning with gradient descent on the K pro-
vided points, using an automatically tuned step size, and
(b) an oracle which receives the true amplitude and phase
as input. In Appendix C, we show comparisons to addi-
tional multi-task and adaptation methods.

We evaluate performance by ﬁne-tuning the model learned
by MAML and the pretrained model on K = {5, 10, 20}
datapoints. During ﬁne-tuning, each gradient step is com-
puted using the same K datapoints. The qualitative results,
shown in Figure 2 and further expanded on in Appendix B
show that the learned model is able to quickly adapt with
only 5 datapoints, shown as purple triangles, whereas the
model that is pretrained using standard supervised learning
on all tasks is unable to adequately adapt with so few dat-
apoints without catastrophic overﬁtting. Crucially, when
the K datapoints are all in one half of the input range, the

1Code for the regression and supervised experiments is at
github.com/cbfinn/maml and code for the RL experi-
ments is at github.com/cbfinn/maml_rl

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

Figure 2. Few-shot adaptation for the simple regression task. Left: Note that MAML is able to estimate parts of the curve where there are
no datapoints, indicating that the model has learned about the periodic structure of sine waves. Right: Fine-tuning of a model pretrained
on the same distribution of tasks without MAML, with a tuned step size. Due to the often contradictory outputs on the pre-training tasks,
this model is unable to recover a suitable representation and fails to extrapolate from the small number of test-time samples.

We follow the experimental protocol proposed by Vinyals
et al. (2016), which involves fast learning of N -way clas-
siﬁcation with 1 or 5 shots. The problem of N -way classi-
ﬁcation is set up as follows: select N unseen classes, pro-
vide the model with K different instances of each of the N
classes, and evaluate the model’s ability to classify new in-
stances within the N classes. For Omniglot, we randomly
select 1200 characters for training, irrespective of alphabet,
and use the remaining for testing. The Omniglot dataset is
augmented with rotations by multiples of 90 degrees, as
proposed by Santoro et al. (2016).

Our model follows the same architecture as the embedding
function used by Vinyals et al. (2016), which has 4 mod-
ules with a 3 × 3 convolutions and 64 ﬁlters, followed by
batch normalization (Ioffe & Szegedy, 2015), a ReLU non-
linearity, and 2 × 2 max-pooling. The Omniglot images
are downsampled to 28 × 28, so the dimensionality of the
last hidden layer is 64. As in the baseline classiﬁer used
by Vinyals et al. (2016), the last layer is fed into a soft-
max. For Omniglot, we used strided convolutions instead
of max-pooling. For MiniImagenet, we used 32 ﬁlters per
layer to reduce overﬁtting, as done by (Ravi & Larochelle,
2017). In order to also provide a fair comparison against
memory-augmented neural networks (Santoro et al., 2016)
and to test the ﬂexibility of MAML, we also provide re-
sults for a non-convolutional network. For this, we use a
network with 4 hidden layers with sizes 256, 128, 64, 64,
each including batch normalization and ReLU nonlineari-
ties, followed by a linear layer and softmax. For all models,
the loss function is the cross-entropy error between the pre-
dicted and true class. Additional hyperparameter details are
included in Appendix A.1.

We present the results in Table 1. The convolutional model
learned by MAML compares well to the state-of-the-art re-
sults on this task, narrowly outperforming the prior meth-
ods. Some of these existing methods, such as matching
networks, Siamese networks, and memory models are de-
signed with few-shot classiﬁcation in mind, and are not
readily applicable to domains such as reinforcement learn-
ing. Additionally, the model learned with MAML uses

Figure 3. Quantitative sinusoid regression results showing the
learning curve at meta test-time. Note that MAML continues to
improve with additional gradient steps without overﬁtting to the
extremely small dataset during meta-testing, achieving a loss that
is substantially lower than the baseline ﬁne-tuning approach.

model trained with MAML can still infer the amplitude and
phase in the other half of the range, demonstrating that the
MAML trained model f has learned to model the periodic
nature of the sine wave. Furthermore, we observe both in
the qualitative and quantitative results (Figure 3 and Ap-
pendix B) that the model learned with MAML continues
to improve with additional gradient steps, despite being
trained for maximal performance after one gradient step.
This improvement suggests that MAML optimizes the pa-
rameters such that they lie in a region that is amenable to
fast adaptation and is sensitive to loss functions from p(T ),
as discussed in Section 2.2, rather than overﬁtting to pa-
rameters θ that only improve after one step.

5.2. Classiﬁcation

To evaluate MAML in comparison to prior meta-learning
and few-shot learning algorithms, we applied our method
to few-shot image recognition on the Omniglot (Lake et al.,
2011) and MiniImagenet datasets. The Omniglot dataset
consists of 20 instances of 1623 characters from 50 dif-
ferent alphabets. Each instance was drawn by a different
person. The MiniImagenet dataset was proposed by Ravi
& Larochelle (2017), and involves 64 training classes, 12
validation classes, and 24 test classes. The Omniglot and
MiniImagenet image recognition tasks are the most com-
mon recently used few-shot learning benchmarks (Vinyals
et al., 2016; Santoro et al., 2016; Ravi & Larochelle, 2017).

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

Table 1. Few-shot classiﬁcation on held-out Omniglot characters (top) and the MiniImagenet test set (bottom). MAML achieves results
that are comparable to or outperform state-of-the-art convolutional and recurrent models. Siamese nets, matching nets, and the memory
module approaches are all speciﬁc to classiﬁcation, and are not directly applicable to regression or RL scenarios. The ± shows 95%
conﬁdence intervals over tasks. Note that the Omniglot results may not be strictly comparable since the train/test splits used in the prior
work were not available. The MiniImagenet evaluation of baseline methods and matching networks is from Ravi & Larochelle (2017).

Omniglot (Lake et al., 2011)
MANN, no conv (Santoro et al., 2016)
MAML, no conv (ours)
Siamese nets (Koch, 2015)
matching nets (Vinyals et al., 2016)
neural statistician (Edwards & Storkey, 2017)
memory mod. (Kaiser et al., 2017)
MAML (ours)

1-shot
82.8%

97.3%
98.1%
98.1%
98.4%

89.7 ± 1.1% 97.5 ± 0.6%

5-way Accuracy

20-way Accuracy

5-shot
94.9%

98.4%
98.9%
99.5%
99.6%

1-shot
–
–
88.2%
93.8%
93.2%
95.0%

5-shot
–
–
97.0%
98.5%
98.1%
98.6%

98.7 ± 0.4% 99.9 ± 0.1% 95.8 ± 0.3% 98.9 ± 0.2%

MiniImagenet (Ravi & Larochelle, 2017)
ﬁne-tuning baseline
nearest neighbor baseline
matching nets (Vinyals et al., 2016)
meta-learner LSTM (Ravi & Larochelle, 2017)
MAML, ﬁrst order approx. (ours)
MAML (ours)

5-way Accuracy

5-shot
1-shot
49.79 ± 0.79%
28.86 ± 0.54%
51.04 ± 0.65%
41.08 ± 0.70%
55.31 ± 0.73%
43.56 ± 0.84%
43.44 ± 0.77%
60.60 ± 0.71%
48.07 ± 1.75% 63.15 ± 0.91%
48.70 ± 1.84% 63.11 ± 0.92%

fewer overall parameters compared to matching networks
and the meta-learner LSTM, since the algorithm does not
introduce any additional parameters beyond the weights
of the classiﬁer itself. Compared to these prior methods,
memory-augmented neural networks (Santoro et al., 2016)
speciﬁcally, and recurrent meta-learning models in gen-
eral, represent a more broadly applicable class of meth-
ods that, like MAML, can be used for other tasks such as
reinforcement learning (Duan et al., 2016b; Wang et al.,
2016). However, as shown in the comparison, MAML sig-
niﬁcantly outperforms memory-augmented networks and
the meta-learner LSTM on 5-way Omniglot and MiniIm-
agenet classiﬁcation, both in the 1-shot and 5-shot case.

A signiﬁcant computational expense in MAML comes
from the use of second derivatives when backpropagat-
ing the meta-gradient through the gradient operator in
the meta-objective (see Equation (1)). On MiniImagenet,
we show a comparison to a ﬁrst-order approximation of
MAML, where these second derivatives are omitted. Note
that the resulting method still computes the meta-gradient
at the post-update parameter values θ(cid:48)
i, which provides for
effective meta-learning. Surprisingly however, the perfor-
mance of this method is nearly the same as that obtained
with full second derivatives, suggesting that most of the
improvement in MAML comes from the gradients of the
objective at the post-update parameter values, rather than
the second order updates from differentiating through the
gradient update. Past work has observed that ReLU neu-
ral networks are locally almost linear (Goodfellow et al.,
2015), which suggests that second derivatives may be close
to zero in most cases, partially explaining the good perfor-

Figure 4. Top: quantitative results from 2D navigation task, Bot-
tom: qualitative comparison between model learned with MAML
and with ﬁne-tuning from a pretrained network.

mance of the ﬁrst-order approximation. This approxima-
tion removes the need for computing Hessian-vector prod-
ucts in an additional backward pass, which we found led to
roughly 33% speed-up in network computation.

5.3. Reinforcement Learning

To evaluate MAML on reinforcement learning problems,
we constructed several sets of tasks based off of the sim-
ulated continuous control environments in the rllab bench-
mark suite (Duan et al., 2016a). We discuss the individual
domains below. In all of the domains, the model trained
by MAML is a neural network policy with two hidden lay-
ers of size 100, with ReLU nonlinearities. The gradient
updates are computed using vanilla policy gradient (RE-
INFORCE) (Williams, 1992), and we use trust-region pol-
icy optimization (TRPO) as the meta-optimizer (Schulman
et al., 2015). In order to avoid computing third derivatives,

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

Figure 5. Reinforcement learning results for the half-cheetah and ant locomotion tasks, with the tasks shown on the far right. Each
gradient step requires additional samples from the environment, unlike the supervised learning tasks. The results show that MAML can
adapt to new goal velocities and directions substantially faster than conventional pretraining or random initialization, achieving good
performs in just two or three gradient steps. We exclude the goal velocity, random baseline curves, since the returns are much worse
(< −200 for cheetah and < −25 for ant).

we use ﬁnite differences to compute the Hessian-vector
products for TRPO. For both learning and meta-learning
updates, we use the standard linear feature baseline pro-
posed by Duan et al. (2016a), which is ﬁtted separately at
each iteration for each sampled task in the batch. We com-
pare to three baseline models: (a) pretraining one policy on
all of the tasks and then ﬁne-tuning, (b) training a policy
from randomly initialized weights, and (c) an oracle policy
which receives the parameters of the task as input, which
for the tasks below corresponds to a goal position, goal di-
rection, or goal velocity for the agent. The baseline models
of (a) and (b) are ﬁne-tuned with gradient descent with a
manually tuned step size. Videos of the learned policies
can be viewed at sites.google.com/view/maml
2D Navigation. In our ﬁrst meta-RL experiment, we study
a set of tasks where a point agent must move to different
goal positions in 2D, randomly chosen for each task within
a unit square. The observation is the current 2D position,
and actions correspond to velocity commands clipped to be
in the range [−0.1, 0.1]. The reward is the negative squared
distance to the goal, and episodes terminate when the agent
is within 0.01 of the goal or at the horizon of H = 100. The
policy was trained with MAML to maximize performance
after 1 policy gradient update using 20 trajectories. Ad-
ditional hyperparameter settings for this problem and the
following RL problems are in Appendix A.2. In our evalu-
ation, we compare adaptation to a new task with up to 4 gra-
dient updates, each with 40 samples. The results in Figure 4
show the adaptation performance of models that are initial-
ized with MAML, conventional pretraining on the same set
of tasks, random initialization, and an oracle policy that
receives the goal position as input. The results show that
MAML can learn a model that adapts much more quickly
in a single gradient update, and furthermore continues to
improve with additional updates.

Locomotion. To study how well MAML can scale to more
complex deep RL problems, we also study adaptation on
high-dimensional locomotion tasks with the MuJoCo sim-
ulator (Todorov et al., 2012). The tasks require two sim-
ulated robots – a planar cheetah and a 3D quadruped (the
“ant”) – to run in a particular direction or at a particular
In the goal velocity experiments, the reward is
velocity.

the negative absolute value between the current velocity of
the agent and a goal, which is chosen uniformly at random
between 0.0 and 2.0 for the cheetah and between 0.0 and
3.0 for the ant. In the goal direction experiments, the re-
ward is the magnitude of the velocity in either the forward
or backward direction, chosen at random for each task in
p(T ). The horizon is H = 200, with 20 rollouts per gradi-
ent step for all problems except the ant forward/backward
task, which used 40 rollouts per step. The results in Fig-
ure 5 show that MAML learns a model that can quickly
adapt its velocity and direction with even just a single gra-
dient update, and continues to improve with more gradi-
ent steps. The results also show that, on these challenging
tasks, the MAML initialization substantially outperforms
random initialization and pretraining. In fact, pretraining
is in some cases worse than random initialization, a fact
observed in prior RL work (Parisotto et al., 2016).

6. Discussion and Future Work

We introduced a meta-learning method based on learning
easily adaptable model parameters through gradient de-
scent. Our approach has a number of beneﬁts. It is simple
and does not introduce any learned parameters for meta-
learning. It can be combined with any model representation
that is amenable to gradient-based training, and any differ-
entiable objective, including classiﬁcation, regression, and
reinforcement learning. Lastly, since our method merely
produces a weight initialization, adaptation can be per-
formed with any amount of data and any number of gra-
dient steps, though we demonstrate state-of-the-art results
on classiﬁcation with only one or ﬁve examples per class.
We also show that our method can adapt an RL agent using
policy gradients and a very modest amount of experience.

Reusing knowledge from past tasks may be a crucial in-
gredient in making high-capacity scalable models, such as
deep neural networks, amenable to fast training with small
datasets. We believe that this work is one step toward a sim-
ple and general-purpose meta-learning technique that can
be applied to any problem and any model. Further research
in this area can make multitask initialization a standard in-
gredient in deep learning and reinforcement learning.

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

Acknowledgements

The authors would like to thank Xi Chen and Trevor Darrell
for helpful discussions, Yan Duan and Alex Lee for techni-
cal advice, Nikhil Mishra, Haoran Tang, and Greg Kahn for
feedback on an early draft of the paper, and the anonymous
reviewers for their comments. This work was supported in
part by an ONR PECASE award and an NSF GRFP award.

References

Abadi, Mart´ın, Agarwal, Ashish, Barham, Paul, Brevdo,
Eugene, Chen, Zhifeng, Citro, Craig, Corrado, Greg S,
Davis, Andy, Dean, Jeffrey, Devin, Matthieu, et al. Ten-
sorﬂow: Large-scale machine learning on heterogeneous
distributed systems. arXiv preprint arXiv:1603.04467,
2016.

Andrychowicz, Marcin, Denil, Misha, Gomez, Sergio,
Hoffman, Matthew W, Pfau, David, Schaul, Tom, and
de Freitas, Nando. Learning to learn by gradient descent
by gradient descent. In Neural Information Processing
Systems (NIPS), 2016.

Bengio, Samy, Bengio, Yoshua, Cloutier, Jocelyn, and
Gecsei, Jan. On the optimization of a synaptic learning
In Optimality in Artiﬁcial and Biological Neural
rule.
Networks, pp. 6–8, 1992.

Bengio, Yoshua, Bengio, Samy, and Cloutier, Jocelyn.
Learning a synaptic learning rule.
Universit´e de
Montr´eal, D´epartement d’informatique et de recherche
op´erationnelle, 1990.

Donahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman,
Judy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. De-
caf: A deep convolutional activation feature for generic
visual recognition. In International Conference on Ma-
chine Learning (ICML), 2014.

Duan, Yan, Chen, Xi, Houthooft, Rein, Schulman, John,
and Abbeel, Pieter. Benchmarking deep reinforcement
In International Con-
learning for continuous control.
ference on Machine Learning (ICML), 2016a.

Duan, Yan, Schulman, John, Chen, Xi, Bartlett, Peter L,
Sutskever, Ilya, and Abbeel, Pieter. Rl2: Fast reinforce-
ment learning via slow reinforcement learning. arXiv
preprint arXiv:1611.02779, 2016b.

Ha, David, Dai, Andrew, and Le, Quoc V. Hypernetworks.
International Conference on Learning Representations
(ICLR), 2017.

Hochreiter, Sepp, Younger, A Steven, and Conwell, Pe-
In
ter R. Learning to learn using gradient descent.
International Conference on Artiﬁcial Neural Networks.
Springer, 2001.

Husken, Michael and Goerick, Christian. Fast learning for
problem classes using knowledge based network initial-
In Neural Networks, 2000. IJCNN 2000, Pro-
ization.
ceedings of the IEEE-INNS-ENNS International Joint
Conference on, volume 6, pp. 619–624. IEEE, 2000.

Ioffe, Sergey and Szegedy, Christian. Batch normalization:
Accelerating deep network training by reducing internal
International Conference on Machine
covariate shift.
Learning (ICML), 2015.

Kaiser, Lukasz, Nachum, Oﬁr, Roy, Aurko, and Bengio,
Samy. Learning to remember rare events. International
Conference on Learning Representations (ICLR), 2017.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
International Conference on

stochastic optimization.
Learning Representations (ICLR), 2015.

Kirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil,
Veness, Joel, Desjardins, Guillaume, Rusu, Andrei A,
Milan, Kieran, Quan, John, Ramalho, Tiago, Grabska-
Overcoming catas-
Barwinska, Agnieszka, et al.
trophic forgetting in neural networks. arXiv preprint
arXiv:1612.00796, 2016.

Koch, Gregory. Siamese neural networks for one-shot im-
age recognition. ICML Deep Learning Workshop, 2015.

Kr¨ahenb¨uhl, Philipp, Doersch, Carl, Donahue, Jeff, and
Darrell, Trevor. Data-dependent initializations of con-
volutional neural networks. International Conference on
Learning Representations (ICLR), 2016.

Lake, Brenden M, Salakhutdinov, Ruslan, Gross, Jason,
and Tenenbaum, Joshua B. One shot learning of simple
visual concepts. In Conference of the Cognitive Science
Society (CogSci), 2011.

Edwards, Harrison and Storkey, Amos. Towards a neural
statistician. International Conference on Learning Rep-
resentations (ICLR), 2017.

Li, Ke and Malik, Jitendra. Learning to optimize. Interna-
tional Conference on Learning Representations (ICLR),
2017.

Goodfellow, Ian J, Shlens, Jonathon, and Szegedy, Chris-
tian. Explaining and harnessing adversarial examples.
International Conference on Learning Representations
(ICLR), 2015.

Maclaurin, Dougal, Duvenaud, David, and Adams, Ryan.
Gradient-based hyperparameter optimization through re-
In International Conference on Ma-
versible learning.
chine Learning (ICML), 2015.

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

Munkhdalai, Tsendsuren and Yu, Hong. Meta net-
works. International Conferecence on Machine Learn-
ing (ICML), 2017.

Snell, Jake, Swersky, Kevin, and Zemel, Richard S. Pro-
totypical networks for few-shot learning. arXiv preprint
arXiv:1703.05175, 2017.

Thrun, Sebastian and Pratt, Lorien. Learning to learn.

Springer Science & Business Media, 1998.

Todorov, Emanuel, Erez, Tom, and Tassa, Yuval. Mujoco:
In Inter-
A physics engine for model-based control.
national Conference on Intelligent Robots and Systems
(IROS), 2012.

Vinyals, Oriol, Blundell, Charles, Lillicrap, Tim, Wierstra,
Daan, et al. Matching networks for one shot learning. In
Neural Information Processing Systems (NIPS), 2016.

Wang, Jane X, Kurth-Nelson, Zeb, Tirumala, Dhruva,
Soyer, Hubert, Leibo, Joel Z, Munos, Remi, Blun-
dell, Charles, Kumaran, Dharshan, and Botvinick,
Matt. Learning to reinforcement learn. arXiv preprint
arXiv:1611.05763, 2016.

Williams, Ronald J. Simple statistical gradient-following
learning.

algorithms for connectionist reinforcement
Machine learning, 8(3-4):229–256, 1992.

Naik, Devang K and Mammone, RJ. Meta-neural networks
that learn by learning. In International Joint Conference
on Neural Netowrks (IJCNN), 1992.

Parisotto, Emilio, Ba, Jimmy Lei, and Salakhutdinov, Rus-
lan. Actor-mimic: Deep multitask and transfer reinforce-
International Conference on Learning
ment learning.
Representations (ICLR), 2016.

Ravi, Sachin and Larochelle, Hugo. Optimization as a
In International Confer-

model for few-shot learning.
ence on Learning Representations (ICLR), 2017.

Rei, Marek.

current neural
arXiv:1508.03854, 2015.

Online representation learning in re-
arXiv preprint

language models.

Rezende, Danilo Jimenez, Mohamed, Shakir, Danihelka,
Ivo, Gregor, Karol, and Wierstra, Daan. One-shot gener-
alization in deep generative models. International Con-
ference on Machine Learning (ICML), 2016.

Salimans, Tim and Kingma, Diederik P. Weight normaliza-
tion: A simple reparameterization to accelerate training
of deep neural networks. In Neural Information Process-
ing Systems (NIPS), 2016.

Santoro, Adam, Bartunov, Sergey, Botvinick, Matthew,
Wierstra, Daan, and Lillicrap, Timothy. Meta-learning
In Interna-
with memory-augmented neural networks.
tional Conference on Machine Learning (ICML), 2016.

Saxe, Andrew, McClelland, James, and Ganguli, Surya.
Exact solutions to the nonlinear dynamics of learning in
International Conference
deep linear neural networks.
on Learning Representations (ICLR), 2014.

Schmidhuber, Jurgen. Evolutionary principles in self-
referential learning. On learning how to learn: The
meta-meta-... hook.) Diploma thesis, Institut f. Infor-
matik, Tech. Univ. Munich, 1987.

Schmidhuber, J¨urgen. Learning to control fast-weight
memories: An alternative to dynamic recurrent net-
works. Neural Computation, 1992.

Schulman, John, Levine, Sergey, Abbeel, Pieter, Jordan,
Michael I, and Moritz, Philipp. Trust region policy
optimization. In International Conference on Machine
Learning (ICML), 2015.

Shyam, Pranav, Gupta, Shubham, and Dukkipati, Ambed-
kar. Attentive recurrent comparators. International Con-
ferecence on Machine Learning (ICML), 2017.

