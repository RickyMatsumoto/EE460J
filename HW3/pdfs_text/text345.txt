Supplementary material for the paper: Co-clustering through Optimal
Transport

Charlotte Laclau 1 Ievgen Redko 2 Basarab Matei 1 Youn`es Bennani 1 Vincent Brault 3

In this supplementary material, we present a couple of com-
plementary elements which were omitted in the main paper.
We ﬁrst introduce the Sinkhorn-Knopp algorithm, then we
explain how exactly the synthetic data sets were simulated.
Finally, we analyse the running time results obtained for
both CCOT and CCOT-GW on the generated data sets.

1. Sinkhorn-Knopp algorithm

For
the
the sake of completeness, we ﬁrst present
Sinkhorn’s theorem and explain how it was used to derive
the solution of the regularized optimal transport.

Theorem ((Sinkhorn & Knopp, 1967)). If A is an n × n
matrix with strictly positive elements, then there exist di-
agonal matrices D1 and D2 with strictly positive diagonal
elements such that D1 ∗ A ∗ D2 is doubly stochastic. The
matrices D1 and D2 are unique modulo multiplying the
ﬁrst matrix by a positive number and dividing the second
one by the same number.

We can now cite the following result.

Lemma ((Cuturi, 2013), Lemma 2). For λ > 0, the solu-
tion γ∗

λ is unique and has the form

γ∗
λ = diag(α)ξλdiag(β),

where α and β are two non-negative vectors of Rd uniquely
deﬁned up to a multiplicative factor and ξλ = e−λM is the
element-wise exponential of −λM .

where

According to (Cuturi, 2013), the form of the solution pre-
sented in this Lemma has already been known in the op-
timal transportation theory (Erlander & Stewart, 1990).
Now since ξλ = e−λM is strictly positive, Sinkhorn’s

1CNRS, LIPN, Universit´e Paris 13 - Sorbonne Paris Cit´e,
France 2CNRS UMR 5220 - INSERM U1206, Univ. Lyon 1,
INSA Lyon, F-69621 Villeurbanne, France 3CNRS, LJK, Univ.
Grenoble-Alpes, France. Correspondence to: Charlotte Laclau
<charlotte.laclauc@univ-grenoble-alpes.fr>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

0The ﬁrst author of this paper is now a post-doc in CNRS,

LIG, Univ. Grenoble-Alpes, France

theorem suggests that there exists a unique (up to rescal-
ing) doubly-stochastic matrix that has the desired form
diag(α)ξλdiag(β). Finally, this matrix can be found us-
ing the iterative procedure known as Sinkhorn-Knopp al-
gorithm deﬁned as follows:

α ← 1./ξλβ,

β ← 1./ξ(cid:48)

λα.

2. Additional experimental results

In this Section, we describe the generative process used to
obtain the synthetic data sets. After that, we analyse the im-
pact of the hyper-parameters on our methods and the run-
ning their running time results.

2.1. Simulation process

As mentioned in the paper, we simulate data following
the generative process of the Gaussian Latent Block Mod-
els. These models rely on the assumption that for each
block, the elements of the data matrix A = aij, i =
1, . . . , n; j = 1, . . . , d are distributed according to a Gaus-
k(cid:96) ∈ R+,
sian distribution N (µk(cid:96), σ2
k = 1, . . . , g; (cid:96) = 1, . . . , m, following a probability den-
sity function of this form

k(cid:96)) with µk(cid:96) ∈ R and σ2

f (A; Θ) =

(cid:88)

(cid:89)

πzik
k

(cid:89)

ρ

wj(cid:96)
(cid:96)

(cid:89)

p(aij; (µk(cid:96), σ2

h(cid:96)))

(z,w)∈Z×W

i,k

j,(cid:96)

i,j,k,(cid:96)

p(aij; (µk(cid:96), σ2

k(cid:96))) =

1
(cid:112)2πσ2

k(cid:96)

exp −

(cid:26) (xij − µk(cid:96))2
2σ2
k(cid:96)

(cid:27)zikwj(cid:96)

.

Therefore, this model is parametrised by Θ = (π, ρ, δ)
where π = (π1, . . . , πm), ρ = (ρ1, . . . , ρm) and δ =
((µ11, σ2

11), . . . , (µgm, σ2

gm)).

Then, the simulation process is as follows

- Input: n, d, g, m, Θ.

1. Simulate z according to a multinomial distribu-

tion with parameters (1, π1, . . . , πg).

2. Simulate w according to a multinomial distribu-

tion with parameters (1, ρ1, . . . , ρm)

3. Simulate each co-cluster Ak(cid:96) according to Gaus-

sian density with (µk(cid:96), σ2

k(cid:96))

Co-clustering through Optimal Transport

- Output Data matrix A, partitions z and w.

For the sake of reproducibility, we also report the parame-
ters used in order to generate D1, D2, D3 and D4 in Table 1.

2.2. Visualisation of α and β for CCOT-GW

As the main paper only presents the visualization of α and
β for CCOT, we present the same result for the kernelized
version, CCOT-GW for D4 in Figure 1.

accentuates the differences between the values of α (resp.
β). By doing so, small gaps, that correspond to overlapping
clusters, tend to merge leading to less accurate results. Re-
garding the regularization parameter, the same observation
is valid for CCOT-GW.

2.4. Running time complexity

The running time performance of our algorithms was evalu-
ated on a cluster machine Intel(R) Xeon(R) CPU X2637 @
3.00GHz. We report the average running time (in seconds)
of both approaches for 100 trials obtained on the generated
data sets in Figure 3.

Figure 1. Visualisation of α and β obtained with CCOT-GW on D4
which have a 5 × 4 blocks structure and unbalanced clusters for
instances.

These vectors correctly reveal 4 and 3 signiﬁcant jumps
corresponding to 5 and 4 clusters, respectively.

2.3. Impact of ns and λ

The proposed algorithms require as input the number of
desired subsamples, ns (for CCOT only) and the value of
the regularization parameter, λ. From Figure 2, one can see
that for all four data sets the co-clustering error stabilizes
when ns reaches approximately 700.

Figure 2. On the left, the CCE as a function of the number of ran-
dom samples ns and on the right as a function of the regulariza-
tion parameter λ.

Since D3 has the most complicated block structure, the ob-
tained results are also more sensible to the value of ns com-
pared to other data sets and require a greater number of
samplings. Regarding λ, we observe very slight variations
of the CCE for D1 and D2. However, for D3 higher values
of this parameter impact negatively the performance of our
method. This can be explained by the fact that increasing λ

Figure 3. Mean running times expressed in seconds, over 100 tri-
als, of CCOT and CCOT-GW for each synthetic data sets.

We observe that even though the theoretical complexity of
CCOT is more interesting, it suffers from the data sampling
that is required to cluster all data instances and features.
This limitation becomes less and less pronounced as the
number of data instances approaches the number of fea-
tures. Indeed, on D4, where no sampling is required, CCOT
only takes approximately 0.06 seconds to accurately pro-
duce the resulting partitions, which is signiﬁcantly faster
than CCOT-GW. For all other data conﬁgurations, the ker-
nelized CCOT-GW algorithm is faster than CCOT.

2.5. Recommendation on MOVIELENS

The main task on MOVIELENS is to recommend movies to
users that might ﬁt their interests. For a given co-clustering
structure, this can be done by recommending movies to
users based on the ratings provided by users who belong to
the same cluster. In order to evaluate the efﬁciency of our
approach for this task, we propose to use 90% of the avail-
able ratings for training purpose, and the remaining 10%
for testing. Since our goal is to predict if a user likes a given
movie or not without specifying the degree of the prefer-
ence, we assume that a rating above 3 stands for movies
that were liked. In order to estimate the ratings in the test-
ing phase, we calculate the mean of the block obtained dur-
ing the training phase and attribute it to the missing values
picked for the testing. After 10-folds cross-validation, we
obtain that for 89% of the testing values, our approach is
able to correctly identify the taste of the users. This shows
its potential for recommendation systems application.

01002003000.990.99511.0051.011.015Instancesα01002003000.99511.0051.011.015Variablesβ200400600800100000.050.10.150.2nsCCE  D1D2D3123456700.050.10.150.2λCCE  D1D2D3D4D1D3D3D405101520Time (in sec)  CCOTCCOT−GWCo-clustering through Optimal Transport

Data

Proportions

σ

µ

Data

Proportions

σ

µ

Table 1. Value of the parameters used for the simulations.

D1

π = ρ = (cid:0) 1

3

1
3

(cid:1)

1
3

σ = 0.1,

∀k, (cid:96)

µ =

 D3

π = (cid:0)0.5
0.2

ρ = (cid:0)0.5

0.5(cid:1)
0.1

0.2(cid:1)

σ = 0.2 ∀k, (cid:96)

µ =

D2

π = ρ = (cid:0)0.2

0.3

0.5(cid:1)

σ = 0.15 ∀k, (cid:96) µ =

 D4

π = (cid:0)0.1
ρ = (cid:0)0.25

0.2

0.2

0.3

0.25

0.25

0.2(cid:1)
0.25(cid:1)

σ = 0.15 ∀k, (cid:96) µ =





4.0
1.8
1.5





4.0
1.8
3.5

0.5
4.5
1.5

0.5
4.5
1.5





1.5
1.1
5.5

1.5
5.1
5.5

(cid:18)4.0
0.5







1.5
2.5
2.6
2.4

0.5
3.5

7.5
7.8

1.5
1.5
2.6
2.5

1.5
1.5
1.5
2.6

(cid:19)

0.5
0.5







1.5
1.5
1.5
2.5

References

Cuturi, Marco. Sinkhorn distances: Lightspeed compu-
In Proceedings NIPS, pp.

tation of optimal transport.
2292–2300, 2013.

Erlander, Sven and Stewart, Neil F. The Gravity model in
transportation analysis : theory and extensions. Topics
in transportation. VSP, Utrecht, The Netherlands, 1990.

Sinkhorn, Richard and Knopp, Paul. Concerning nonneg-
ative matrices and doubly stochastic matrices. Paciﬁc
Journal of Mathematics, 21:343–348, 1967.

