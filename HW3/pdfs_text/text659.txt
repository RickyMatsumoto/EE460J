Stochastic Convex Optimization:
Faster Local Growth Implies Faster Global Convergence

Yi Xu 1 Qihang Lin 2 Tianbao Yang 1

2

Abstract
In this paper, a new theory is developed for ﬁrst-
order stochastic convex optimization, showing
that the global convergence rate is sufﬁciently
quantiﬁed by a local growth rate of the objec-
tive function in a neighborhood of the optimal
solutions.
In particular, if the objective func-
tion F (w) in the (cid:15)-sublevel set grows as fast as
(cid:107)w − w∗(cid:107)1/θ
, where w∗ represents the closest
optimal solution to w and θ ∈ (0, 1] quantiﬁes
the local growth rate, the iteration complexity
of ﬁrst-order stochastic optimization for achiev-
ing an (cid:15)-optimal solution can be (cid:101)O(1/(cid:15)2(1−θ)),
which is optimal at most up to a logarithmic fac-
tor. To achieve the faster global convergence,
we develop two different accelerated stochas-
tic subgradient methods by iteratively solving
the original problem approximately in a local re-
gion around a historical solution with the size
of the local region gradually decreasing as the
solution approaches the optimal set. Besides
the theoretical improvements, this work also in-
clude new contributions towards making the pro-
posed algorithms practical: (i) we present prac-
tical variants of accelerated stochastic subgradi-
ent methods that can run without the knowledge
of multiplicative growth constant and even the
growth rate θ; (ii) we consider a broad family
of problems in machine learning to demonstrate
that the proposed algorithms enjoy faster con-
vergence than traditional stochastic subgradient
method. For example, when applied to the (cid:96)1
regularized empirical polyhedral loss minimiza-
tion (e.g., hinge loss, absolute loss), the proposed
stochastic methods have a logarithmic iteration
complexity.

1. Introduction
In this paper, we are interested in solving the following
stochastic optimization problem:

F (w) (cid:44) Eξ[f (w; ξ)],

min
w∈K

(1)

where ξ is a random variable, f (w; ξ) is a convex function
of w, Eξ[·] is the expectation over ξ and K is a convex do-
main. We denote by ∂f (w; ξ) a subgradient of f (w; ξ).
Let K∗ denote the optimal set of (1) and F∗ denote the op-
timal value.

Traditional stochastic subgradient (SSG) method updates
the solution according to

wt+1 = ΠK[wt − ηt∂f (wt; ξt)],

(2)

for t = 1, . . . , T , where ξt is a sampled value of ξ at t-th
iteration, ηt is a step size and ΠK[w] = arg minv∈K (cid:107)w −
v(cid:107)2
2 is a projection operator that projects a point into K.
Previous studies have shown that under the following as-
sumptions i) (cid:107)∂f (w; ξ)(cid:107)2 ≤ G, ii) there exists w∗ ∈ K∗
such that (cid:107)wt − w∗(cid:107)2 ≤ B for t = 1, . . . , T 1, and by set-
ting the step size ηt = B
in (2), with a high probability
√
G
1 − δ we have

T

F ( (cid:98)wT ) − F∗ ≤ O

(cid:16)

GB(1 + (cid:112)log(1/δ))/

√

(cid:17)

,

T

(3)

where (cid:98)wT = (cid:80)T
t=1 wt/T . The above convergence im-
plies that in order to obtain an (cid:15)-optimal solution by
i.e., ﬁnding a w such that F (w) − F∗ ≤ (cid:15)
SSG,
with a high probability 1 − δ, one needs at least T =
O(G2B2(1 + (cid:112)log(1/δ))2/(cid:15)2) in the worst-case.

It is commonly known that the slow convergence of SSG
is due to the variance in the stochastic subgradient and
the non-smoothness nature of the problem as well, which
therefore requires a decreasing step size or a very small step
size. Recently, there emerges a stream of studies on various
variance reduction techniques to accelerate stochastic gra-
dient method (Roux et al., 2012; Zhang et al., 2013; John-
son & Zhang, 2013; Xiao & Zhang, 2014; Defazio et al.,

1Department of Computer Science, The University of Iowa,
Iowa City, IA 52242, USA 2Department of Management Sci-
ences, The University of Iowa, Iowa City, IA 52242, USA. Corre-
spondence to: Tianbao Yang <tianbao-yang@uiowa.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1This holds if we assume the domain K is bounded such that
maxw,v∈K (cid:107)w − v(cid:107)2 ≤ B or if assume dist(w1, K∗) ≤ B/2
and project every solution wt into K ∩ B(w1, B/2).

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

2014). However, they all hinge on the smoothness assump-
tion. The proposed algorithms in this work tackle the issue
of variance of stochastic subgradient without the smooth-
ness assumption from another pespective.

The main motivation for addressing this problem is from
a key observation: a high probability analysis of the SSG
method shows that the variance term of the stochastic sub-
gradient is accompanied by an upper bound of distance of
intermediate solutions to the target solution. This obser-
vation has also been leveraged in previous analysis to de-
sign faster convergence for stochastic convex optimization
that use a strong or uniform convexity condition (Hazan &
Kale, 2011; Juditsky & Nesterov, 2014) or a global growth
condition (Ramdas & Singh, 2013) to control the distance
of intermediate solutions to the optimal solution by their
functional residuals. However, we ﬁnd these global as-
sumptions are completely unnecessary, which may not only
restrict their applications to a broad family of problems but
also worsen the convergence rate due to the larger multi-
plicative growth constant that could be domain-size depen-
dent. In contrast, we develop a new theory only relying on
the local growth condition to control the distance of inter-
mediate solutions to the (cid:15)-optimal solution by their func-
tional residuals but achieving a fast global convergence.

Besides the fundamental difference, the present work also
possesses several unique algorithmic contributions com-
pared with previous similar work on stochastic optimiza-
tion: (i) we have two different ways to control the distance
of intermediate solutions to the (cid:15)-optimal solution, one by
explicitly imposing a bounded ball constraint and another
one by implicitly regularizing the intermediate solutions,
where the later one could be more efﬁcient if the projec-
tion into the intersection of a bounded ball and the prob-
lem domain is complicated; (ii) we develop more practical
variants that can be run without knowing the multiplicative
growth constant though under a slightly stringent condi-
tion; (iii) for problems whose local growth rate is unknown
we still develop an improved convergence result of the pro-
posed algorithms comparing with the SSG method. In ad-
dition, the present work will demonstrate the improved re-
sults and practicability of the proposed algorithms for many
problems in machine learning, which is lacking in similar
previous work.

2. Related Work
The most similar work to the present one is (Ramdas &
Singh, 2013), which studied stochastic convex optimization
under a global growth condition, which they called Tsy-
bakov noise condition. One major difference from their re-
sult is that we achieve the same order of iteration complex-
ity up to a logarithmic factor under only a local growth con-
dition. As observed later on, the multiplicative growth con-

stant in local growth condition is domain-size independent
that is smaller than that in global growth condition, which
could be domain-size dependent. Besides, the stochastic
optimization algorithm in (Ramdas & Singh, 2013) assume
the optimization domain K is bounded, which is removed
In addition, they do not address the issue
in this work.
when the multiplicative constant is unknown and lack study
of applicability for machine learning problems. Juditsky
& Nesterov (2014) presented primal-dual subgradient and
stochastic subgradient methods for solving problems under
the uniform convexity assumption (see the deﬁnition under
Observation 1). As exhibited shortly, the uniform convex-
ity condition covers only a smaller family of problems than
the considered local growth condition. However, when the
problem is uniform convex, the iteration complexity ob-
tained in this work resembles that in (Juditsky & Nesterov,
2014).

Recently, there emerge a wave of studies that attempt to
improve the convergence of existing algorithms under no
strong convexity assumption by considering certain weaker
conditions than strong convexity (Necoara et al., 2015; Liu
et al., 2015; Zhang & Yin, 2013; Liu & Wright, 2015; Gong
& Ye, 2014; Karimi et al., 2016; Zhang, 2016; Qu et al.,
2016; Wang & Lin, 2014). Several recent works (Necoara
et al., 2015; Karimi et al., 2016; Zhang, 2016) have uniﬁed
many of these conditions, implying that they are a kind of
global growth condition with θ = 1/2. Unlike the present
work, most of these developments require certain smooth-
ness assumption except (Qu et al., 2016).

Luo & Tseng (1992a;b; 1993) pioneered the idea of us-
ing local error bound condition to show faster conver-
gence of gradient descent, proximal gradient descent, and
many other methods for a family of structured compos-
ite problems (e.g., the LASSO problem). Many follow-up
works (Hou et al., 2013; Zhou et al., 2015; Zhou & So,
2015) have considered different regularizers (e.g., (cid:96)1,2 reg-
ularizer, nuclear norm regularizer). However, these works
only obtained asymptotically faster (i.e., linear) conver-
gence and they hinge on the smoothness on some parts of
the problem. Yang & Lin (2016); Xu et al. (2016) have
considered the same local growth condition (aka local er-
ror bound condition in their work) for developing faster de-
terministic algorithms for non-smooth optimization. How-
ever, they did not address the problem of stochastic convex
optimization, which restricts their applicability to large-
scale problems in machine learning.

Finally, we note that the improved iteration complexity in
this paper does not contradict to the lower bound in (Ne-
mirovsky A.S. & Yudin, 1983; Nesterov, 2004). The bad
examples constructed to derive the lower bound for gen-
eral non-smooth optimization do not satisfy the assump-
tions made in this work (in particular Assumption 1(b)).

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

3. Preliminaries
Recall the notations K∗ and F∗ that denote the optimal set
of (1) and the optimal value, respectively. For the opti-
mization problem in (1), we make the following assump-
tion throughout the paper.

Assumption 1. For a stochastic optimization problem (1),
we assume

(a) there exist w0 ∈ K and (cid:15)0 ≥ 0 such that F (w0) −

F∗ ≤ (cid:15)0;

(b) K∗ is a non-empty compact set;
(c) There exists a constant G such that (cid:107)∂f (w; ξ)(cid:107)2 ≤ G.

Remark: (a) essentially assumes the availability of a lower
bound of the optimal objective value, which usually holds
for machine learning problems (due to non-negativeness
of the objective function).
(b) simply assumes the opti-
mal set is closed and bounded. This is a relaxed condition
in contrast with most previous work that assume the do-
main K is bounded. Even if K is unbounded, as long as the
function is a proper lower-semicontinuous convex and co-
ercive function deﬁned on a ﬁnite dimensional space, K∗ is
nonempty and compact (Bolte et al., 2015). Note that any
norm-regularized loss function minimization problem on
a ﬁnite dimensional space in machine learning satisfy this
property. (c) is a standard assumption also made in many
previous stochastic gradient-based methods. By Jensen’s
inequality, we also have (cid:107)∂F (w)(cid:107)2 ≤ G.

For any w ∈ K, let w∗ denote the closest optimal solution
in K∗ to w, i.e., w∗ = arg minv∈K∗ (cid:107)v − w(cid:107)2
2, which is
unique. We denote by L(cid:15) the (cid:15)-level set of F (w) and by S(cid:15)
the (cid:15)-sublevel set of F (w), respectively, i.e., L(cid:15) = {w ∈
K : F (w) = F∗ + (cid:15)}, S(cid:15) = {w ∈ K : F (w) ≤ F∗ + (cid:15)}.
Given K∗ is bounded, it follows from (Rockafellar, 1970,
Corollary 8.7.1) that the sublevel set S(cid:15) is bounded for any
(cid:15) ≥ 0 and so as the level set L(cid:15). Let w†
(cid:15) denote the closest
point in the (cid:15)-sublevel set to w, i.e.,

w†

(cid:15) = arg min
v∈S(cid:15)

(cid:107)v − w(cid:107)2
2.

(4)

It is easy to show that w†
(cid:15) ∈ L(cid:15) when w /∈ S(cid:15) (using the
KKT condition). Let B(w, r) = {u ∈ Rd : (cid:107)u−w(cid:107)2 ≤ r}
denote an Euclidean ball centered at w with a radius r.
Denote by dist(w, K∗) = minv∈K∗ (cid:107)w − v(cid:107)2 the dis-
tance between w and the set K∗, by ∂0F (w) the projec-
tion of 0 onto the nonempty closed convex set ∂F (w), i.e.,
(cid:107)∂0F (w)(cid:107)2 = minv∈∂F (w) (cid:107)v(cid:107)2.

3.1. Functional Local Growth Rate

We quantify the functional local growth rate by measuring
how fast the functional value increase when moving a point
away from the optimal solution in the (cid:15)-sublevel set.
In
particular, a function F (w) has a local growth rate θ ∈

(0, 1] in the (cid:15)-sublevel set ((cid:15) (cid:28) 1) if there exists a constant
λ > 0 such that:

λ(cid:107)w − w∗(cid:107)1/θ

2 ≤ F (w) − F∗,

∀w ∈ S(cid:15),

(5)

where w∗ is the closest solution in the optimal set K∗
to w. Note that the local growth rate θ is at most 1.
This is due to that F (w) is G-Lipschitz continuous and
limw→w∗ (cid:107)w − w∗(cid:107)1−α
= 0 if α < 1. The inequality
in (5) can be equivalently written as

2

(cid:107)w − w∗(cid:107)2 ≤ c(F (w) − F∗)θ,

∀w ∈ S(cid:15),

(6)

where c = 1/λθ, which is called as local error bound con-
dition in (Yang & Lin, 2016). In this work, to avoid con-
fusion with earlier work by Luo & Tseng (1992a;b; 1993)
who also explored a related but different local error bound
condition, we refer to the inequality in (5) or (6) as local
growth condition (LGC). If the function F (x) is assumed
to satisfy (5) for all w ∈ K, it is referred to as global
growth condition (GGC). Note that since we do not assume
a bounded K, the GGC might be ill posed. In the following
discussions, when compared with GGC we simply assume
the domain is bounded.

Below, we present several observations mostly from exist-
ing work to clarify the relationship between the LGC (6)
and previous conditions, and also justify our choice of LGC
that covers a much broader family of functions than previ-
ous conditions and induces a smaller multiplicative growth
constant c than that induced by GGC.
Observation 1. Strong convexity or uniform convexity con-
dition implies LGC with θ = 1/2, but not vice versa.
F (w) is said to satisfy a uniform convexity condition on K
with convexity parameters p ≥ 2 and µ if:

F (u) ≥ F (v) + ∂F (v)(cid:62)(u − v) +

, ∀u, v ∈ K.

µ(cid:107)u − v(cid:107)p
2
2

If we let u = w, v = w∗, and ∂F (v) = 0, we have (5)
with θ = 1/p ∈ (0, 1/2]. Clearly LGC covers a broader
family of functions than uniform convexity.
Observation 2. The weak strong convexity (Necoara et al.,
2015), essential strong convexity (Liu et al., 2015), re-
stricted strong convexity (Zhang & Yin, 2013), optimal
strong convexity (Liu & Wright, 2015), semi-strong con-
vexity (Gong & Ye, 2014) and other error bound conditions
considered in several recent work (Karimi et al., 2016;
Zhang, 2016) imply a GGC on the entire optimization do-
main K with θ = 1/2 for a convex function.
Some of these conditions are also equivalent to the GGC
with θ = 1/2. We refer the reader to (Necoara et al.,
2015), (Karimi et al., 2016) and (Zhang, 2016) for more
discussions of these conditions.

The third observation shows that LGC could imply faster
convergence than that induced by GGC.

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

Observation 3. The LGC could induce a smaller constant
c in (6) that is domain-size independent than that induced
by the GGC on the entire optimization domain K.

To illustrate this, we consider a function f (x) = x2 if |x| ≤
1 and f (x) = |x| if 1 < |x| ≤ s, where s speciﬁes the size
of the domain. In the (cid:15)-sublevel set ((cid:15) < 1), the LGC (6)
holds with θ = 1/2 and c = 1. In order to make the in-
equality |x| ≤ cf (x)1/2 hold for all x ∈ [−s, s], we can
s.
see that c = max|x|≤s
As a result, GGC induces a larger c that depends on the
domain size.

f (x)1/2 = max|x|≤s

(cid:112)|x| =

√

|x|

The next observation shows that Luo-Tseng’s local error
bound condition is closely related to the LGC with θ =
1/2. To this end, we ﬁrst give the deﬁnition of Luo-Tseng’s
local error bound condition. Let F (w) = h(w) + P (w),
where h(w) is a proper closed function with an open do-
main containing K and is continuously differentiable with
a locally Lipschitz continuous gradient on any compact set
within dom(h) and P (w) is a proper closed convex func-
tion. Such a function F (w) is said to satisfy Luo-Tseng’s
local error bound if for any ζ > 0, there exists c, ε > 0 so
that (cid:107)w−w∗(cid:107)2 ≤ c(cid:107)proxP (w−∇h(w))−w(cid:107)2, whenever
(cid:107)proxP (w − ∇h(w)) − w(cid:107)2 ≤ ε and F (w) − F∗ ≤ ζ,
where proxP (w) = arg minu∈K

2 + P (w).

2 (cid:107)u − w(cid:107)2

1

Observation 4. If F (w) = h(w) + P (w) is deﬁned above
and satisﬁes the Luo-Tseng’s local error bound condition,
it then implies that there exists a sufﬁciently small (cid:15)(cid:48) > 0
and C > 0 such that (cid:107)w − w∗(cid:107)2 ≤ C(F (w) − F∗)1/2 for
any w ∈ B(w∗, (cid:15)(cid:48)).

This observation was established in (Li & Pong, 2016, The-
orem 4.1). Note that the LGC condition with (cid:15) = G(cid:15)(cid:48) and
θ = 1/2 also implies that (cid:107)w − w∗(cid:107)2 ≤ C(F (w) − F∗)1/2
for any w ∈ B(w∗, (cid:15)(cid:48)). Nonetheless, Luo-Tseng’s local er-
ror bound imposes some smoothness assumption on h(w).

The last observation is that the LGC is equivalent to a
Kurdyka - Łojasiewicz inequality (KL), which was proved
in (Bolte et al., 2015, Theorem 5).

Observation 5. If F (w) satisﬁes a KL inequality, i.e.,
ϕ(cid:48)(F (w) − F∗)(cid:107)∂0F (w)(cid:107)2 ≥ 1 for w ∈ {x ∈ K, F (x) −
F∗ < (cid:15)} with ϕ(s) = csθ, then LGC (6) holds, and vice
versa.

The above KL inequality has been established for con-
tinuous semi-algebraic and subanalytic functions (Attouch
et al., 2013; Bolte et al., 2006; 2015), which cover a broad
family of functions therefore justifying the generality of the
LGC.

Finally, we present a key lemma that can leverage the LGC
to control the distance of intermediate solutions to an (cid:15)-
optimal solution.

Lemma 1. For any w ∈ K and (cid:15) > 0, we have

(cid:107)w − w†

(cid:15)(cid:107)2 ≤

(cid:15), K∗)

dist(w†
(cid:15)

(F (w) − F (w†

(cid:15))),

c

(cid:15) ∈ S(cid:15) is the closest point in the (cid:15)-sublevel set to

where w†
w as deﬁned in (4).
Remark: In view of LGC, we can see that (cid:107)w − w†
(cid:15)(cid:107)2 ≤
(cid:15)1−θ (F (w) − F (w†
(cid:15))) for any w ∈ K. Yang & Lin (2016)
have leveraged this relationship to improve the convergence
of the standard subgradient method. In the sequel, we will
build on this relationship to further develop novel stochas-
tic optimization algorithms with faster convergence in high
probability.

4. Main Results
In this section, we will present the proposed accelerated
stochastic subgradient (ASSG) methods and establish their
improved iteration complexity with a high probability. The
key to our development is to control the distance of inter-
mediate solutions to the (cid:15)-optimal solution by their func-
tional residuals that are decreasing as the solutions ap-
proach the optimal set. It is this decreasing factor that help
mitigate the non-vanishing variance issue in the stochas-
tic subgradient. To formally illustrate this, we consider the
following stochastic subgradient update:

wτ +1 = ΠK∩B(w1,D)[wτ − η∇f (wτ ; ξτ )].

(7)

4GD

3 log( 1
δ )
√

,

Lemma 2. Given w1 ∈ K, apply t-iterations of (7). For
any ﬁxed w ∈ K ∩ B(w1, D) and δ ∈ (0, 1), with a prob-
ability at least 1 − δ, the following inequality holds
(cid:113)

ηG2
2

(cid:107)w1 − w(cid:107)2
2
2ηt

t

+

+

τ =1 wt/t.

F ( (cid:98)wt) − F (w) ≤
where (cid:98)wt = (cid:80)t
Remark: The proof of the above lemma follows similarly
as that of Lemma 10 in (Hazan & Kale, 2011). We note that
the last term is due to the variance of the stochastic subgra-
dients.
In fact, due to the non-smoothness nature of the
problem the variance of the stochastic subgradients cannot
be reduced, we therefore propose to address this issue by
reducing D in light of the inequality in Lemma 1.

The updates in (7) can be also understood as approximately
solving the original problem in the neighborhood of w1. In
light of this, we will also develop a regularized variant of
the proposed method. In the sequel, all omitted proofs can
be found in the supplement.

4.1. Accelerated Stochastic Subgradient Method: the

Constrained variant (ASSG-c)

In this subsection, we present the constrained variant of
ASSG that iteratively solves the original problem approx-

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

imately in an explicitly constructed local neighborhood of
the recent historical solution. The detailed steps are pre-
sented in Algorithm 1. We refer to this variant as ASSG-c.
The algorithm runs in stages and each stage runs t iterations
of updates similar to (7). Thanks to Lemma 1, we gradu-
ally decrease the radius Dk in a stage-wise manner. The
step size keeps the same during each stage and geometri-
cally decreases between stages. We notice that ASSG-c is
similar to the Epoch-GD method by Hazan & Kale (2011)
and the (multi-stage) AC-SA method with domain shrink-
age by Ghadimi & Lan (2013) for stochastic strongly con-
vex optimization. However, the difference between ASSG
and Epoch-GD/AC-SA lies at the initial radius D1 and the
number of iterations per-stage, which is due to difference
between the strong convexity assumption and Lemma 1.
The convergence of ASSG-c is presented in the theorem
below.

(cid:15) )(cid:101), D1 ≥ c(cid:15)0

Theorem 1. Suppose Assumption 1 holds and F (w) obeys
the LGC (6). Given δ ∈ (0, 1), let ˜δ = δ/K, K =
(cid:100)log2( (cid:15)0
(cid:15)1−θ and t be the smallest integer
such that t ≥ max{9, 1728 log(1/˜δ)} G2D2
. Then ASSG-c
(cid:15)2
0
guarantees that, with a probability 1 − δ, F (wK) − F∗ ≤
2(cid:15). As a result, the iteration complexity of ASSG-c for
achieving an 2(cid:15)-optimal solution with a high probabil-
ity 1 − δ is O(c2G2(cid:100)log2( (cid:15)0
(cid:15) )(cid:101) log(1/δ)/(cid:15)2(1−θ)) provided
D1 = O( c(cid:15)0
(cid:15)(1−θ) ).

1

Remark: It is notable that the faster local growth rate θ
implies the faster global convergence, i.e., lower iteration
complexity. In light of the lower bound presented in (Ram-
das & Singh, 2013) under a GGC, our iteration complex-
ity under the LGC is optimal up to at most a logarithmic
factor. It is worth mentioning that unlike traditional high-
probability analysis of SSG that usually requires the do-
main to be bounded, the convergence analysis of ASSG
does not rely on such a condition. Furthermore, the it-
eration complexity of ASSG has a better dependence on
the quality of the initial solution or the size of domain if
it is bounded. In particular, if we let (cid:15)0 = GB assuming
dist(w0, K∗) ≤ B, though this is not necessary in practice,
then the iteration complexity of ASSG has only a logarith-
mic dependence on the distance of the initial solution to the
optimal set, while that of SSG has a quadratic dependence
on this distance. The above theorem requires a target pre-
cision (cid:15) in order to set D1. In subsection 4.3, we alleviate
this requirement to make the algorithm more practical.

4.2. Accelerated Stochastic Subgradient Method: the

Regularized variant (ASSG-r)

One potential issue of ASSG-c is that the projection into the
intersection of the problem domain and an Euclidean ball
might increase the computational cost per-iteration depend-
ing on the problem domain K. To address this issue, we

Let wk
1 = wk−1
for τ = 1, . . . , t − 1 do

Algorithm 1 ASSG-c(w0, K, t, D1, (cid:15)0)
1: Input: w0 ∈ K, K, t, (cid:15)0 and D1 ≥ c(cid:15)0
(cid:15)1−θ
2: Set η1 = (cid:15)0/(3G2)
3: for k = 1, . . . , K do
4:
5:
6:
7:
8:
9:
10: end for
11: Output: wK

wk
end for
Let wk = 1
τ =1 wk
τ
t
Let ηk+1 = ηk/2 and Dk+1 = Dk/2.

τ +1 = ΠK∩B(wk−1,Dk)[wk

(cid:80)t

τ − ηk∂f (wk

τ ; ξk

τ )]

present a regularized variant of ASSG. Before delving into
the details of ASSG-r, we ﬁrst present a common strategy
that solves the non-strongly convex problem (1) by stochas-
tic strongly convex optimization. The basic idea is from the
classical deterministic proximal point algorithm (Rockafel-
lar, 1976) which adds a strongly convex regularizer to the
original problem and solve the resulting proximal problem.
In particular, we construct a new problem

min
w∈K

(cid:98)F (w) = F (w) +

1
2β

(cid:107)w − w1(cid:107)2
2,

(8)

where w1 ∈ K is called the regularization reference point.
Let (cid:98)w∗ denote the optimal solution to the above problem
given w1. It is easy to know (cid:98)F (w) is a 1
β -strongly convex
function on K. We can employ the stochastic subgradient
method suited for strongly convex problems to solve the
above problem. The update is given by

wt+1 = ΠK[w(cid:48)

t+1] = arg min
w∈K

(cid:13)
(cid:13)w − w(cid:48)

t+1

(cid:13)
2
2 ,
(cid:13)

(9)

t+1 = wt − ηt(∂f (wt; ξt) + 1
where w(cid:48)
β (wt − w1)), and
ηt = 2β
2. We present a lemma below to bound (cid:107) (cid:98)w∗ −
t
wt(cid:107)2 and (cid:107)wt − w1(cid:107)2 by the above update, which will be
used in the proof of convergence of ASSG-r for solving (1).
Lemma 3. For any t ≥ 1, we have (cid:107) (cid:98)w∗ − wt(cid:107)2 ≤ 3βG
and (cid:107)wt − w1(cid:107)2 ≤ 2βG.

Remark: The lemma implies that the regularization term
implicitly imposes a constraint on the intermediate solu-
tions to center around the regularization reference point,
which achieves a similar effect as the ball constraint in Al-
gorithm 1.

Recall that the main iteration of the proximal point algo-
rithm (Rockafellar, 1976) is

wk ≈ arg min
w∈K

F (w) +

(cid:107)w − wk−1(cid:107)2
2,

(10)

1
2βk

where wk approximately solves the minimization problem
above with βk changing with k. With the same idea, our

2The factor 2 in the step size is used for proving the high prob-

ability convergence.

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

Algorithm 2 the ASSG-r algorithm for solving (1)
1: Input: w0 ∈ K, K, t, (cid:15)0 and β1 ≥ 2c2(cid:15)0
(cid:15)2(1−θ)
2: for k = 1, . . . , K do
3:
4:
5:
6:
7:
8:
9: end for
10: Output: wK

Let wk
1 = wk−1
for τ = 1, . . . , t − 1 do
τ +1 = (cid:0)1 − 2
τ
τ +1 = ΠK(w(cid:48)

end for
Let wk = 1
t

τ , and βk+1 = βk/2

Let w(cid:48)
Let wk

(cid:1) wk
τ +1)

τ =1 wk

1 − 2β

τ + 2

τ wk

(cid:80)t

τ ∂f (wk

τ ; ξk
τ )

regularized variant of ASSG generates wk from stage k by
solving the minimization problem (10) approximately us-
ing (9). The detailed steps are presented in Algorithm 2,
which starts from a relatively large value of the parameter
β = β1 and gradually decreases β by a constant factor af-
ter running a number of t iterations (9) using the solution
from the previous stage as the new regularization reference
point. Despite of its similarity to the proximal point algo-
rithm, ASSG-r incorporates the LGC into the choices of
βk and the number of iterations per-stage and obtains new
iteration complexity described below.

(cid:15) )(cid:101), β1 ≥ 2c2(cid:15)0

Theorem 2. Suppose Assumption 1 holds and F (w) obeys
the LGC (6). Given δ ∈ (0, 1/e), let ˜δ = δ/K, K =
(cid:100)log2( (cid:15)0
(cid:15)2(1−θ) and t be the smallest inte-
ger such that t ≥ max{3, 136β1G2(1+log(4 log t/˜δ)+log t)
}.
Then ASSG-r guarantees that, with a probability 1 − δ,
F (wK) − F∗ ≤ 2(cid:15). As a result, the iteration complexity
of ASSG-r for achieving an 2(cid:15)-optimal solution with a high
probability 1 − δ is O(c2G2 log((cid:15)0/(cid:15)) log(1/δ)/(cid:15)2(1−θ))
provided β1 = O( 2c2(cid:15)0

(cid:15)0

(cid:15)2(1−θ) ).

4.3. More Practical Variants of ASSG

Readers may have noticed that the presented algorithms re-
quire appropriately setting up the initial values of D1 or β1
that depend on unknown c and potentially unknown θ. This
subsection is devoted to more practical variants of ASSG.
For ease of presentation, we focus on the constrained vari-
ant of ASSG.

When c is known, we present the details of a restarting vari-
ant of ASSG in Algorithm 3, to which we refer as RASSG.
The key idea is to use an increasing sequence of t and an-
other level of restarting for ASSG.

Theorem 3 (RASSG with unknown c). Let (cid:15) ≤ (cid:15)0/4,
ω = 1, and K = (cid:100)log2( (cid:15)0
(cid:15) )(cid:101) in Algorithm 3. Suppose
D(1)
is sufﬁciently large so that there exists ˆ(cid:15)1 ∈ [(cid:15), (cid:15)0/2],
1
with which F (·) satisﬁes a LGC (6) on Sˆ(cid:15)1 with θ ∈ (0, 1)
and the constant c, and D(1)
δ
K(K+1) ,

1 = c(cid:15)0
ˆ(cid:15)1−θ
1
(cid:16)
and t1 = max{9, 1728 log(1/ˆδ)}

. Let ˆδ =

GD(1)

. Then

(cid:17)2

1 /(cid:15)0

1 , t1, (cid:15)0 and ω ∈ (0, 1]

0 = (cid:15)0, η1 = (cid:15)0/(3G2)

Algorithm 3 ASSG with Restarting: RASSG
1: Input: w(0), K, D(1)
2: Set (cid:15)(1)
3: for s = 1, 2, . . . , S do
4:

Let w(s) =ASSG-c(w(s−1), K, ts, D(s)
Let ts+1 = ts22(1−θ), D(s+1)
= D(s)
(cid:15)(s+1)
0
6: end for
7: Output: w(S)

= ω(cid:15)(s)
0

5:

1

1 , (cid:15)(s)
0 )
1 21−θ, and

with at most S = (cid:100)log2(ˆ(cid:15)1/(cid:15))(cid:101) + 1 calls of ASSG-c, Al-
gorithm 3 ﬁnds a solution w(S) such that F (w(S)) − F∗ ≤
2(cid:15). The total number of iterations of RASSG for ob-
taining 2(cid:15)-optimal solution is upper bounded by TS =
O((cid:100)log2( (cid:15)0

(cid:15) )(cid:101) log(1/δ)/(cid:15)2(1−θ)).

Remark: The above theorem requires a slightly stringent
LGC condition on Sˆ(cid:15)1 that is induced by the initial value
of D1. If the problem satisﬁes the LGC with θ = 1, we
can give a slightly smaller value for θ in order to run Al-
gorithm 3. If the target precision (cid:15) is not speciﬁed, we can
give it a sufﬁciently small value (cid:15)(cid:48) (e.g., the machine pre-
cision) that only affects K marginally. The corresponding
iteration complexity for achieving an (cid:15)-optimal solution is
given by O((cid:100)log2( (cid:15)0
(cid:15)(cid:48) )(cid:101) log(1/δ)/(cid:15)2(1−θ)). The parameter
ω ∈ (0, 1] is introduced to increase the practical perfor-
mance of RASSG, which accounts for decrease of the ob-
jective gap of the initial solutions for each call of ASSG-c.

When θ is unknown, we can set θ = 0 and c = Bε with ε ≥
(cid:15) in the LGC (6), where Bε = maxw∈Lε minv∈K∗ (cid:107)w −
v(cid:107)2 is the maximum distance between the points in the ε-
level set Lε and the optimal set K∗. The following theorem
states the convergence result.

Theorem 4 (RASSG with unknown θ). Let θ = 0, (cid:15) ≤
(cid:15)0/4 , ω = 1, and K = (cid:100)log2( (cid:15)0
(cid:15) )(cid:101) in Algorithm 3. Assume
D(1)
is sufﬁciently large so that there exists ˆ(cid:15)1 ∈ [(cid:15), (cid:15)0/2]
1
rendering that D(1)
K(K+1) , and

1 = Bˆ(cid:15)1 (cid:15)0
ˆ(cid:15)1
t1 = max{9, 1728 log(1/ˆδ)}
. Then with
at most S = (cid:100)log2(ˆ(cid:15)1/(cid:15))(cid:101) + 1 calls of ASSG-c, Algo-
rithm 3 ﬁnds a solution w(S) such that F (w(S)) − F∗ ≤
2(cid:15). The total number of iterations of RASSG for ob-
taining 2(cid:15)-optimal solution is upper bounded by TS =
G2B2
O((cid:100)log2( (cid:15)0
ˆ(cid:15)1
).
(cid:15)2

. Let ˆδ =
(cid:16)

(cid:15) )(cid:101) log(1/δ)

GD(1)

1 /(cid:15)0

(cid:17)2

δ

Remark: The Lemma 6 in the supplement shows that B(cid:15)
(cid:15)
is a monotonically decreasing function in terms of (cid:15), which
guarantees the existence of ˆ(cid:15)1 given a sufﬁciently large
D(1)
1 . The iteration complexity of RASSG could be still
better with a smaller factor Bˆ(cid:15)1 than the B in the iteration
complexity of SSG (see (3)), where B is the domain size or
the distance of initial solution to the optimal set.

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

5. Applications in Risk Minimization
In this section, we present some applications of the pro-
posed ASSG to risk minimization in machine learning. Let
(xi, yi), i = 1, . . . , n denote a set of pairs of feature vectors
and labels that follow a distribution P, where xi ∈ X ⊂ Rd
and yi ∈ Y. Many machine learning problems end up solv-
ing the regularized empirical risk minimization problem:

min
w∈Rd

F (w) =

1
n

n
(cid:88)

i=1

(cid:96)(w(cid:62)xi, yi) + λR(w),

(11)

where R(w) is a regularizer, λ is the regularization param-
eter and (cid:96)(z, y) is a loss function. Below we will present
several examples in machine learning that enjoy faster con-
vergence by the proposed ASSG than by SSG.

5.1. Piecewise Linear Minimization

First, we consider some examples of non-smooth and non-
strongly convex problems such that ASSG can achieve
linear convergence.
In particular, we consider the prob-
lem (11) with a piecewise linear loss and (cid:96)1, (cid:96)∞ or (cid:96)1,∞
regularizers.

Piecewise linear loss includes hinge loss, generalized hinge
loss, absolute loss, and (cid:15)-insensitive loss. For particular
forms of these loss functions, please refer to (Yang et al.,
2014). The epigraph of F (w) deﬁned by sum of a piece-
wise linear loss function and an (cid:96)1, (cid:96)∞ or (cid:96)1,∞ norm reg-
ularizer is a polyhedron. According to the polyhedral error
bound condition (Yang & Lin, 2016), for any (cid:15) > 0 there
exists a constant 0 < c < ∞ such that dist(w, K∗) ≤
c(F (w) − F∗) for any w ∈ S(cid:15), meaning that the proposed
ASSG has an O(log((cid:15)0/(cid:15))) iteration complexity for solv-
ing such family of problems. Formally, we state the result
in the following corollary.
Corollary 5. Assume the loss function (cid:96)(z, y) is piecewise
linear, then the problem in (11) with (cid:96)1, (cid:96)∞ or (cid:96)1,∞ norm
regularizer satisfy the LGC in (6) with θ = 1. Hence ASSG
can have an iteration complexity of O(log(1/δ) log((cid:15)0/(cid:15)))
with a high probability 1 − δ.

5.2. Piecewise Convex Quadratic Minimization

(cid:15)

Next, we consider some examples of piecewise quadratic
minimization problems in machine learning and show that
(cid:1). We ﬁrst
ASSG enjoys an iteration complexity of (cid:101)O (cid:0) 1
give an deﬁnition of piecewise convex quadratic functions,
which is from (Li, 2013). A function g(w) is a real
polynomial if there exists k ∈ N+ such that g(w) =
(cid:80)
i ∈ N+ ∪
0≤|αj |≤k λj
{0}, αj = (αj
i . The con-
stant k is called the degree of g. A continuous function
F (w) is said to be a piecewise convex polynomial if there
exist ﬁnitely many polyhedra P1, . . . , Pm with ∪m
j=1Pj =
Rd such that the restriction of F on each Pj is a convex

, where λj ∈ R and αj
i=1 αj

i=1 wαj
(cid:81)d
1, . . . , αj

d), and |αj| = (cid:80)d

i

i

polynomial. Let Fj be the restriction of F on Pj. The de-
gree of a piecewise convex polynomial function F is the
maximum of the degree of each Fj.
If the degree is 2,
the function is referred to as a piecewise convex quadratic
function. Note that a piecewise convex quadratic function
is not necessarily a smooth function nor a convex func-
tion (Li, 2013).

For examples of piecewise convex quadratic problems
in machine learning, one can consider the problem (11)
with a huber loss, squared hinge loss or square loss, and
(cid:96)1, (cid:96)∞, (cid:96)1,∞, or huber norm regularizer (Zadorozhnyi
et al., 2016). The Huber function is deﬁned as (cid:96)γ(z) =
(cid:26) 1

, which is a piecewise con-

2 z2
γ(|z| − 1

if |z| ≤ γ,
2 γ) otherwise,

vex quadratic function. The huber loss function (cid:96)(z, y) =
(cid:96)γ(z − y) has been used for robust regression. A Huber
regularizer is deﬁned as R(w) = (cid:80)d

i=1 (cid:96)γ(wi).

It has been shown that (Li, 2013), if F (w) is convex and
piecewise convex quadratic, then it satisﬁes the LGC (6)
with θ = 1/2. The corollary below summarizes the itera-
tion complexity of ASSG for solving these problems.
Corollary 6. Assume the loss function (cid:96)(z, y) is a convex
and piecewise convex quadratic, then the problem in (11)
with (cid:96)1, (cid:96)∞, (cid:96)1,∞ or huber norm regularizer satisfy the
LGC in (6) with θ = 1/2. Hence ASSG can have an it-
eration complexity of (cid:101)O( log(1/δ)
) with a high probability
1 − δ.

(cid:15)

5.3. Structured composite non-smooth problems

Next, we present a corollary of our main result regarding
the following structured problem:

F (w) (cid:44) h(Xw) + P (w).

(12)

min
w∈Rd

Corollary 7. Assume h(u) is a strongly convex function on
any compact set and P (w) is polyhedral, then the problem
in (12) satisﬁes the LGC in (6) with θ = 1/2. Hence ASSG
can have an iteration complexity of (cid:101)O( log(1/δ)
) with a high
probability 1 − δ.
The proof of the ﬁrst part of Corollary 7 can be found
in (Yang & Lin, 2016). One example of h(u) is p-norm
error (p ∈ (0, 1)), where h(u) = 1
i=1 |ui − yi|p. The
n
local strong convexity of the p-norm error (p ∈ (1, 2)) is
shown in (Goebel & Rockafellar, 2007).

(cid:80)n

(cid:15)

Finally, we give an example that satisﬁes the LGC with
intermediate values θ ∈ (0, 1/2). We can consider an (cid:96)1
constrained (cid:96)p norm regression (Nyquist, 1983):

min
(cid:107)w(cid:107)1≤s

F (w) (cid:44) 1
n

n
(cid:88)

i=1

(x(cid:62)

i w − yi)p,

p ∈ 2N+.

Liu & Yang (2016) have shown that the problem above sat-
isﬁes the LGC in (6) with θ = 1
p .

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

Figure 1. Comparison of different algorithms for solving different problems on different datasets.

6. Experiments
In this section, we perform some experiments to demon-
strate effectiveness of proposed algorithms. We use very
large-scale datasets from libsvm website in experiments,
including covtype.binary (n = 581012), real-sim (n =
72309), url (n = 2396130) for classiﬁcation, million songs
(n = 463715), E2006-tﬁdf (n = 16087), E2006-log1p
(n = 16087) for regression. The detailed statistics of these
datasets are shown in the supplement.

√

We ﬁrst compare ASSG with SSG on three tasks: (cid:96)1 norm
regularized hinge loss minimization for linear classiﬁca-
tion, (cid:96)1 norm regularized Huber loss minimization for lin-
ear regression, and (cid:96)1 norm regularized p-norm robust re-
gression with a loss function (cid:96)(w(cid:62)xi, yi) = |w(cid:62)xi − yi|p.
The regularization parameter λ is set to be 10−4 in all tasks
(We also perform the experiments with λ = 10−2 and in-
clude the results in the supplement). We set γ = 1 in Huber
loss and p = 1.5 in robust regression. In all experiments,
we use the constrained variant of ASSG, i.e., ASSG-c. For
fairness, we use the same initial solution with all zero en-
tries for all algorithms. We use a decreasing step size pro-
portional to 1/
τ (τ is the iteration index) in SSG. The
initial step size of SSG is tuned in a wide range to obtain
the fastest convergence. The step size of ASSG in the ﬁrst
stage is also tuned around the best initial step size of SSG.
The value of D1 in both ASSG and RASSG is set to 100
for all problems. In implementing the RASSG, we restart
every 5 stages with t increased by a factor of 1.15, 2 and
2 respectively for hinge loss, Huber loss and robust regres-
sion. We tune the parameter ω among {0.3, 0.6, 0.9, 1}.
We report the results of ASSG with a ﬁxed number of iter-
ations per-stage t and RASSG with an increasing sequence
of t. The results are plotted in Figure 1 (ﬁrst 6 ﬁgures),
in which we plot the log difference between the objective
value and the smallest obtained objective value (to which
we refer as objective gap) versus number of iterations. The

ﬁgures show that (i) ASSG can quickly converge to a cer-
tain level set determined implicitly by t; (ii) RASSG con-
verges much faster than SSG to more accurate solutions;
(iii) RASSG can gradually decrease the objective value.

Finally, we compare RASSG with state-of-art stochastic
optimization algorithms for solving a ﬁnite-sum problem
with a smooth piecewise quadratic loss (e.g., squared hinge
loss, huber loss) and an (cid:96)1 norm regularization. In partic-
ular, we compare with SAGA (Defazio et al., 2014) and
SVRG++ (Allen-Zhu & Yuan, 2016). We conduct exper-
iments on two high-dimensional datasets url and E2006-
log1p and ﬁx the regularization parameter λ = 10−4 (We
also include the results for λ = 10−2 in the supplement).
We use δ = 1 in Huber loss. For RASSG, we start from
D1 = 100 and t1 = 103, then restart it every 5 stages with
t increased by a factor of 2. We tune the initial step sizes for
all algorithms in a wide range and set the values of param-
eters in SVRG++ followed by (Allen-Zhu & Yuan, 2016).
We plot the objective versus the CPU time (second) in Fig-
ure 1 (last 2 ﬁgures). The results show that RASSG con-
verges faster than other three algorithms for the two tasks.
This is not surprising considering that RASSG, SAGA and
SVRG++ suffer from an iteration complexity of (cid:101)O(1/(cid:15)),
O(n/(cid:15)), and O(n log(1/(cid:15)) + 1/(cid:15)), respectively.

7. Conclusion
In this paper, we have proposed accelerated stochastic sub-
gradient methods for solving general non-strongly convex
stochastic optimization under the functional local growth
condition. The proposed methods enjoy a lower iteration
complexity than vanilla stochastic subgradient method and
also a logarithmic dependence on the impact of the initial
solution. We have also made an extension by developing
a more practical variant. Applications in machine learning
have demonstrated the faster convergence of the proposed
methods.

numberofiterations×1070246810log10(objectivegap)-8-7.5-7-6.5-6-5.5-5-4.5-4-3.5-3hingeloss+ℓ1norm,covtypeSSGASSG(t=106)RASSG(t1=106)numberofiterations×1070246810log10(objectivegap)-5-4.5-4-3.5-3-2.5-2-1.5-1hingeloss+ℓ1norm,real-simSSGASSG(t=106)RASSG(t1=106)numberofiterations×1070246810log10(objectivegap)-7.5-7-6.5-6-5.5-5-4.5-4-3.5-3huberloss+ℓ1norm,millionsongsSSGASSG(t=106)RASSG(t1=106)numberofiterations×1050246810log10(objectivegap)-8-7-6-5-4-3-2-101huberloss+ℓ1norm,E2006-tﬁdfSSGASSG(t=104)RASSG(t1=104)numberofiterations×1070246810log10(objectivegap)-6-5-4-3-2-10robust+ℓ1norm,millionsongsSSGASSG(t=106)RASSG(t1=106)numberofiterations×1050246810log10(objectivegap)-7-6-5-4-3-2-10robust+ℓ1norm,E2006-tﬁdfSSGASSG(t=104)RASSG(t1=104)cputime(s)×10500.511.52objective0.060.080.10.120.140.160.180.20.22squaredhinge+ℓ1norm,urlSSGSAGASVRG++RASSGcputime(s)×10500.511.52objective0.020.040.060.080.10.120.140.16huberloss+ℓ1norm,E2006-log1pSSGSAGASVRG++RASSGStochastic Convex Optimization: Faster Local Growth Implies Global Convergence

Acknowledgement

We thank the anonymous reviewers for their helpful com-
ments. Y. Xu and T. Yang are partially supported by Na-
tional Science Foundation (IIS-1463988, IIS-1545995). T.
Yang would like to thank Lijun Zhang for pointing out
(Kakade & Tewari, 2008) for his attention.

References

Allen-Zhu, Zeyuan and Yuan, Yang.

Improved svrg for
non-strongly-convex or sum-of-non-convex objectives.
In ICML, pp. 1080–1089, 2016.

Attouch, Hedy, Bolte, J´erˆome, and Svaiter, Benar Fux.
Convergence of descent methods for semi-algebraic and
tame problems: proximal algorithms, forward-backward
splitting, and regularized gauss-seidel methods. Math.
Program., 137(1-2):91–129, 2013.

Bolte, J´erˆome, Daniilidis, Aris, and Lewis, Adrian. The
łojasiewicz inequality for nonsmooth subanalytic func-
tions with applications to subgradient dynamical sys-
tems. SIAM J. on Optimization, 17:1205–1223, 2006.

Bolte, J´erˆome, Nguyen, Trong Phong, Peypouquet, Juan,
and Suter, Bruce. From error bounds to the complexity of
ﬁrst-order descent methods for convex functions. CoRR,
abs/1510.08234, 2015.

Defazio, Aaron, Bach, Francis R., and Lacoste-Julien, Si-
mon. SAGA: A fast incremental gradient method with
support for non-strongly convex composite objectives. In
NIPS, pp. 1646–1654, 2014.

Ghadimi, Saeed and Lan, Guanghui. Optimal stochastic
approximation algorithms for strongly convex stochas-
tic composite optimization, ii: Shrinking procedures and
optimal algorithms. SIAM Journal on Optimization, 23
(4):20612089, 2013.

Goebel, R. and Rockafellar, R. T. Local strong convexity
and local lipschitz continuity of the gradient of convex
functions. Journal of Convex Analysis, 2007.

Gong, Pinghua and Ye, Jieping. Linear convergence of
variance-reduced projected stochastic gradient without
strong convexity. CoRR, abs/1406.1102, 2014.

Hazan, Elad and Kale, Satyen. Beyond the regret min-
imization barrier: an optimal algorithm for stochastic
In COLT, pp. 421–436,
strongly-convex optimization.
2011.

Hou, Ke, Zhou, Zirui, So, Anthony Man-Cho, and Luo,
Zhi-Quan. On the linear convergence of the proximal
gradient method for trace norm regularization. In NIPS,
pp. 710–718, 2013.

Johnson, Rie and Zhang, Tong. Accelerating stochastic
gradient descent using predictive variance reduction. In
NIPS, pp. 315–323, 2013.

Juditsky, Anatoli and Nesterov, Yuri. Deterministic and
stochastic primal-dual subgradient algorithms for uni-
Stoch. Syst., 4:44–80,
formly convex minimization.
2014.

Kakade, Sham M. and Tewari, Ambuj. On the general-
ization ability of online strongly convex programming
algorithms. In NIPS, pp. 801–808, 2008.

Karimi, Hamed, Nutini, Julie, and Schmidt, Mark W. Lin-
ear convergence of gradient and proximal-gradient meth-
ods under the polyak-łojasiewicz condition. In ECML-
PKDD, pp. 795–811, 2016.

Li, Guoyin. Global error bounds for piecewise convex
polynomials. Math. Program., 137(1-2):37–64, 2013.

Li, Guoyin and Pong, Ting Kei. Calculus of the expo-
nent of kurdyka- łojasiewicz inequality and its applica-
tions to linear convergence of ﬁrst-order methods. CoRR,
abs/1602.02915, 2016.

Liu, Ji and Wright, Stephen J. Asynchronous stochastic
coordinate descent: Parallelism and convergence proper-
ties. SIAM Journal on Optimization, 25:351–376, 2015.

Liu, Ji, Wright, Stephen J., R´e, Christopher, Bittorf, Vic-
tor, and Sridhar, Srikrishna. An asynchronous parallel
stochastic coordinate descent algorithm. J. Mach. Learn.
Res., 16:285–322, 2015. ISSN 1532-4435.

Liu, Mingrui and Yang, Tianbao. Adaptive accelerated gra-
dient converging methods under holderian error bound
condition. CoRR, abs/1611.07609, 2016.

Luo, Zhi-Quan and Tseng, Paul. On the convergence
of coordinate descent method for convex differentiable
minization. Journal of Optimization Theory and Appli-
cations, 72(1):7–35, 1992a.

Luo, Zhi-Quan and Tseng, Paul. On the linear convergence
of descent methods for convex essenially smooth miniza-
tion. SIAM Journal on Control and Optimization, 30(2):
408–425, 1992b.

Luo, Zhi-Quan and Tseng, Paul. Error bounds and con-
vergence analysis of feasible descent methods: a general
approach. Annals of Operations Research, 46:157–178,
1993.

Necoara, I., Nesterov, Yu., and Glineur, F. Linear conver-
gence of ﬁrst order methods for non-strongly convex op-
timization. CoRR, abs/1504.06298, 2015.

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

Zhang, Hui. New analysis of linear convergence of
gradient-type methods via unifying error bound condi-
tions. CoRR, abs/1606.00269, 2016.

Zhang, Hui and Yin, Wotao. Gradient methods for con-
vex minimization: better rates under weaker conditions.
CoRR, abs/1303.4645, 2013.

Zhang, Lijun, Mahdavi, Mehrdad, and Jin, Rong. Linear
convergence with condition number independent access
of full gradients. In NIPS, pp. 980–988, 2013.

Zhou, Zirui and So, Anthony Man-Cho. A uniﬁed approach
to error bounds for structured convex optimization prob-
lems. CoRR, abs/1512.03518, 2015.

Zhou, Zirui, Zhang, Qi, and So, Anthony Man-Cho. L1p-
norm regularization: Error bounds and convergence rate
In ICML, pp. 1501–
analysis of ﬁrst-order methods.
1510, 2015.

Nemirovsky A.S., Arkadii Semenovich. and Yudin, D. B.
Problem complexity and method efﬁciency in optimiza-
tion. Wiley-Interscience series in discrete mathematics.
Wiley, Chichester, New York, 1983. ISBN 0-471-10345-
4. A Wiley-Interscience publication.

Nesterov, Yurii. Introductory lectures on convex optimiza-
tion : a basic course. Applied optimization. Kluwer
Academic Publ., 2004. ISBN 1-4020-7553-7.

Nyquist, H. The optimal lp norm estimator in linear regres-
sion models. Communications in Statistics - Theory and
Methods, 12(21):2511–2524, 1983.

Qu, Chao, Xu, Huan, and Ong, Chong Jin. Fast rate analy-
sis of some stochastic optimization algorithms. In ICML,
pp. 662–670, 2016.

Ramdas, Aaditya and Singh, Aarti. Optimal rates for
stochastic convex optimization under tsybakov noise
condition. In ICML, pp. 365–373, 2013.

Rockafellar, R. Tyrrell. Monotone operators and the proxi-
mal point algorithm. SIAM Journal on Control and Op-
timization, 14:877–898, 1976.

Rockafellar, R.T. Convex Analysis. Princeton mathematical

series. Princeton University Press, 1970.

Roux, Nicolas Le, Schmidt, Mark W., and Bach, Francis.
A stochastic gradient method with an exponential con-
vergence rate for ﬁnite training sets. In NIPS, pp. 2672–
2680, 2012.

Wang, Po-Wei and Lin, Chih-Jen. Iteration complexity of
feasible descent methods for convex optimization. Jour-
nal of Machine Learning Research, 15(1):1523–1548,
2014.

Xiao, Lin and Zhang, Tong. A proximal stochastic gradi-
ent method with progressive variance reduction. SIAM
Journal on Optimization, 24(4):2057–2075, 2014.

Xu, Yi, Yan, Yan, Lin, Qihang, and Yang, Tianbao. Ho-
motopy smoothing for non-smooth problems with lower
complexity than O(1/(cid:15)). In NIPS, pp. 1208–1216, 2016.

Yang, Tianbao and Lin, Qihang.

Rsg: Beating sgd
without smoothness and/or strong convexity. CoRR,
abs/1512.03107, 2016.

Yang, Tianbao, Mahdavi, Mehrdad, Jin, Rong, and Zhu,
Shenghuo. An efﬁcient primal-dual prox method for
non-smooth optimization. Machine Learning, 2014.

Zadorozhnyi, Oleksandr, Benecke, Gunthard, Mandt,
Stephan, Scheffer, Tobias, and Kloft, Marius. Huber-
norm regularization for linear prediction models.
In
ECML-PKDD, pp. 714–730, 2016.

