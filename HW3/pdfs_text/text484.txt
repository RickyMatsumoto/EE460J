Geometry of Neural Networks

Supplemental Material: Geometry of Neural Network Loss Surfaces via
Random Matrix Theory

1. Computing the normalized index

One way to obtain an expression for the normalized index is to rewrite eqn. (18) as f (G) = z (where f (G) = RH (G) +
1/G), so that G = f −1(z). Integrating the inverse of a function requires only integration of the function itself (Laisant,
1905),

f −1(z)dz = zf −1(z) − F ◦ f −1(z) + C ,

where F is the antiderivative of f . This relation gives,

α((cid:15), φ) = 1 −

(cid:104)
Im

(cid:105)
(cid:15)GH (0)2 + log GH (0) − log (cid:0)1 − φGH (0)(cid:1)/φ

.

(cid:90)

1
π

An explicit representation of GH (0) and thus α((cid:15), φ) is possible by solving the cubic equation in eqn. (18). The full result
is very long and unenlightening, but we ﬁnd that for small α,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
where (cid:15)c is the critical value of (cid:15) below which all critical points are minimizers.

α((cid:15), φ) ≈ α0(φ)

(cid:15) − (cid:15)c
(cid:15)c

1
16

(cid:15)c =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

3/2

,

(1 − 20φ − 8φ2 + (1 + 8φ)3/2) ,

2. On the assumption that the weights are I.I.D. random normal variables

Figure S1. Histogram of weight matrix entries (W (1) left, W (2) right) after training a 50 hidden-unit single-layer ReLU network on a
subset of 500 grayscaled CIFAR-10 images. Left, right histograms have a total of 51,200 and 500 entries.

3. Spectral density of H1 for single-hidden-layer ReLU networks

From (Dupic & Castillo, 2014) and referring to eqn. (27), the density can be written as

where 1/α = φ/2 = n/m,

and,

ρH1 (λ) = (cid:0)1 − min (cid:0)1,

(cid:1)(cid:1)δ(λ) +

α
2

α2|λ|
2(cid:15)

ρc

(cid:16) α2λ2
2(cid:15)

,

α
2

(cid:17)

,

ρc(x, α) =

(cid:0)r+ − r−

(cid:1) 1x∈[θ(1−α)x−,x+] ,

√

3
√
6πx 3

2

9(2 + α)(x − ξ0) ± 6(cid:112)3(x − x−)x(x+ − x) ,
α(8 + α)3/2

√

,

r± = 3(cid:113)

x± =

ξ0 = −

8 + 20α − α2 ±
8

2(−1 + α)3
9(2 + α)

.

(S1)

(S2)

(S3)

(S4)

(S5)

(S6)

(S7)

(S8)

−0.3−0.2−0.10.00.10.20.3Weight Values05001000150020002500Histogram−1.5−1.0−0.50.00.51.01.5Weight Values0246810121416HistogramGeometry of Neural Networks

4. Free independence and the evolution of eigenvalues over training

We plot the eigenvalues of the Hessian H0 + H1 and the transformed Hessian H0 + QH1QT as the parameters evolve over
training. The training set is CIFAR-10 downsampled to 4x4 images, grayscaled and whitened. We train a 16-20-16 ReLU
autoencoding network without biases on the ﬁrst 150 images of the dataset using full-batch gradient descent with learning
rate 0.05. The parameters are initialized as zero-mean Gaussians with variance 2 over the number of incoming units.

Figure S2. Evolution of the eigenvalues of the Hessian over training.

ρ(λ)5100.10.20.30.40.5step=0λ-22468100.10.20.30.40.50.6step=1λH=H0+H1H=H0+QH1QT2468100.10.20.30.40.50.6step=2λρ(λ)2468100.20.40.60.8step=4λρ(λ)2468100.20.40.60.8step=8λρ(λ)24680.20.40.60.81.0step=16λρ(λ)24680.20.40.60.81.01.2step=32λ24680.20.40.60.81.01.21.4step=64λρ(λ)24680.51.01.5step=128λρ(λ)24680.51.01.5step=256λ24680.51.01.5step=512λρ(λ)24680.51.01.5step=1024λρ(λ)24680.51.01.5step=2048λ24680.51.01.5step=4096λρ(λ)24680.51.01.5step=8192λ