Convexiﬁed Convolutional Neural Networks

Yuchen Zhang 1 Percy Liang 1 Martin J. Wainwright 2

Abstract

We describe the class of convexiﬁed convolu-
tional neural networks (CCNNs), which capture
the parameter sharing of convolutional neural
networks in a convex manner. By representing
the nonlinear convolutional ﬁlters as vectors in a
reproducing kernel Hilbert space, the CNN pa-
rameters can be represented in terms of a low-
rank matrix, and the rank constraint can be re-
laxed so as to obtain a convex optimization prob-
lem. For learning two-layer convolutional neu-
ral networks, we prove that the generalization
error obtained by a convexiﬁed CNN converges
to that of the best possible CNN. For learning
deeper networks, we train CCNNs in a layer-
wise manner. Empirically, we ﬁnd that CC-
NNs achieve competitive or better performance
than CNNs trained by backpropagation, SVMs,
fully-connected neural networks, stacked denois-
ing auto-encoders, and other baseline methods.

1. Introduction

Convolutional neural networks (CNNs) (LeCun et al.,
1998) have proven successful across many tasks including
image classiﬁcation (LeCun et al., 1998; Krizhevsky et al.,
2012), face recognition (Lawrence et al., 1997), speech
recognition (Hinton et al., 2012), text classiﬁcation (Wang
et al., 2012), and game playing (Mnih et al., 2015; Silver
et al., 2016). There are two principal advantages of a CNN
over a fully-connected neural network: (i) sparsity—each
nonlinear convolutional ﬁlter acts only on a local patch of
the input, and (ii) parameter sharing—the same ﬁlter is ap-
plied to each patch.

However, as with most neural networks, the standard ap-
proach to training CNNs is based on solving a nonconvex
optimization problem that is known to be NP-hard (Blum

1Stanford University, CA, USA 2University of Califor-
nia, Berkeley, CA, USA. Correspondence to: Yuchen Zhang
<zhangyuc@cs.stanford.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

& Rivest, 1992). In practice, researchers use some ﬂavor
of stochastic gradient method, in which gradients are com-
puted via backpropagation (Bottou, 1998). This approach
has two drawbacks: (i) the rate of convergence, which is at
best only to a local optimum, can be slow due to noncon-
vexity (for instance, see the paper (Fahlman, 1988)), and
(ii) its statistical properties are very difﬁcult to understand,
as the actual performance is determined by some combina-
tion of the CNN architecture along with the optimization
algorithm.

In this paper, with the goal of addressing these two chal-
lenges, we propose a new model class known as convexi-
ﬁed convolutional neural networks (CCNNs). These mod-
els have two desirable features. First, training a CCNN
corresponds to a convex optimization problem, which can
be solved efﬁciently and optimally via a projected gradi-
ent algorithm. Second, the statistical properties of CCNN
models can be studied in a precise and rigorous manner. We
obtain CCNNs by convexifying two-layer CNNs; doing so
requires overcoming two challenges. First, the activation
function of a CNN is nonlinear. In order to address this
issue, we relax the class of CNN ﬁlters to a reproducing
kernel Hilbert space (RKHS). This approach is inspired by
the paper (Zhang et al., 2016a), which put forth a relaxation
for fully-connected neural networks. Second, the parame-
ter sharing induced by CNNs is crucial to its effectiveness
and must be preserved. We show that CNNs with RKHS
ﬁlters can be parametrized by a low-rank matrix. Relaxing
this low-rank constraint to a nuclear norm constraint leads
to our ﬁnal formulation of CCNNs.

On the theoretical front, we prove an oracle inequality on
the generalization error achieved by our class of CCNNs,
showing that it is upper bounded by the best possible per-
formance achievable by a two-layer CNN given inﬁnite
data—a quantity to which we refer as the oracle risk—plus
a model complexity term that decays to zero polynomially
in the sample size. Our results suggest that the sample com-
plexity for CCNNs is signiﬁcantly lower than that of the
convexiﬁed fully-connected neural network (Zhang et al.,
2016a), highlighting the importance of parameter sharing.
For models with more than one hidden layer, our theory
does not apply, but we provide encouraging empirical re-
sults using a greedy layer-wise training heuristic. Finally,
we apply CCNNs to the MNIST handwritten digit dataset

Convexiﬁed Convolutional Neural Networks

as well as four variation datasets (VariationsMNIST), and
ﬁnd that it achieves state-of-the-art accuracy.

Related work. With the empirical success of deep neu-
ral networks, there has been an increasing interest in un-
derstanding its connection to convex optimization. Ben-
gio et al. (2005) showed how to formulate neural network
training as a convex optimization problem involving an in-
ﬁnite number of parameters. Aslan et al. (2013; 2014) pro-
pose a method for learning multi-layer latent-variable mod-
els. They showed that for certain activation functions, the
proposed method is a convex relaxation for learning fully-
connected neural networks.

Past work has studied learning translation-invariant fea-
tures without backpropagation. Mairal et al. (2014) present
convolutional kernel networks. They propose a translation-
invariant kernel whose feature mapping can be approxi-
mated by a composition of the convolution, non-linearity
and pooling operators, obtained through unsupervised
learning. However, this method is not equipped with the
optimality guarantees that we provide for CCNNs in this
paper, even for learning one convolutional layer. The Scat-
Net method (Bruna & Mallat, 2013) uses translation and
deformation-invariant ﬁlters constructed by wavelet analy-
sis; however, these ﬁlters are independent of the data, in
contrast to CCNNs. Daniely et al. (2016) show that a ran-
domly initialized CNN can extract features as powerful as
kernel methods, but it is not clear how to provably improve
the model from random initialization.

Notation. For any positive integer n, we use [n] as a
shorthand for the discrete set {1, 2, . . . , n}. For a rect-
angular matrix A, let (cid:107)A(cid:107)∗ be its nuclear norm, (cid:107)A(cid:107)2 be
its spectral norm (i.e., maximal singular value), and (cid:107)A(cid:107)F
be its Frobenius norm. We use (cid:96)2(N) to denote the set
of countable dimensional vectors v = (v1, v2, . . . ) such
that (cid:80)∞
(cid:96) < ∞. For any vectors u, v ∈ (cid:96)2(N),
the inner product (cid:104)u, v(cid:105) := (cid:80)∞
(cid:96)=1 uivi and the (cid:96)2-norm
(cid:107)u(cid:107)2 := (cid:112)(cid:104)u, u(cid:105) are well deﬁned.

(cid:96)=1 v2

2. Background and problem setup

In this section, we formalize the class of convolutional neu-
ral networks to be learned and describe the associated non-
convex optimization problem.

2.1. Convolutional neural networks

At a high level, a two-layer CNN1 is a function that maps
an input vector x ∈ Rd0 (e.g., an image) to an output vector
in y ∈ Rd2 (e.g., classiﬁcation scores for d2 classes). This
mapping is formed in the following manner:

1Average pooling and multiple channels are also an integral
part of CNNs, but these do not present any new technical chal-
lenges, so that we defer these extensions to Section 4.

• First, we extract a collection of P vectors {zp(x)}P

p=1
of the full input vector x. Each vector zp(x) ∈ Rd1 is
referred to as a patch, and these patches may depend on
overlapping components of x.

• Second, given some choice of activation function
σ : R → R and a collection of weight vectors {wj}r
in Rd1, we deﬁne the functions

j=1

hj(z) := σ(w(cid:62)

j z)

for each patch z ∈ Rd1.

(1)

Each function hj (for j ∈ [r]) is known as a ﬁlter, and
note that the same ﬁlters are applied to each patch—this
corresponds to the parameter sharing of a CNN.

• Third, for each patch index p ∈ [P ], ﬁlter index j ∈ [r],
and output coordinate k ∈ [d2], we introduce a coef-
ﬁcient αk,j,p ∈ R that governs the contribution of the
ﬁlter hj on patch zp(x) to output fk(x). The ﬁnal form
of the CNN is given by f (x) : = (f1(x), . . . , fd2 (x)),
where the kth component is given by

fk(x) :=

αk,j,phj(zp(x)).

(2)

r
(cid:88)

P
(cid:88)

j=1

p=1

Taking the patch functions {zp}P
p=1 and activation function
σ as ﬁxed, the parameters of the CNN are the ﬁlter vectors
w := {wj ∈ Rd1 : j ∈ [r]} along with the collection of
coefﬁcient vectors α := {αk,j ∈ RP : k ∈ [d2], j ∈ [r]}.
We assume that all patch vectors zp(x) ∈ Rd1 are con-
tained in the unit (cid:96)2-ball. This assumption can be satisﬁed
without loss of generality by normalization: By multiply-
ing a constant γ > 0 to every patch zp(x) and multiplying
1/γ to the ﬁlter vectors w, this assumption holds without
changing the the output of the network.

Given some positive radii B1 and B2, we consider the
model class

Fcnn(B1, B2) :=

f of the form (2) : max
j∈[r]

(cid:107)wj(cid:107)2 ≤ B1

(cid:110)

and max

(cid:107)αk,j(cid:107)2 ≤ B2

k∈[d2],j∈[r]

(cid:111)
.

(3)

When the radii (B1, B2) are clear from context, we adopt
Fcnn as a convenient shorthand.

2.2. Empirical risk minimization.

Given an input-output pair (x, y) and a CNN f , we
let L(f (x); y) denote the loss incurred when the out-
put y is predicted via f (x). We assume that the loss
function L is convex and L-Lipschitz in its ﬁrst argu-
ment given any value of its second argument. As a
concrete example, for multiclass classiﬁcation with d2
the output vector y takes values in the dis-
classes,
crete set [d2] = {1, 2, . . . , d2}.
For example, given
a vector f (x) = (f1(x), . . . , fd2(y)) ∈ Rd2 of classiﬁca-
the associated multiclass logistic loss for
tion scores,
:= −fy(x) +
a pair (x, y) is given by L(f (x); y)

Convexiﬁed Convolutional Neural Networks

y(cid:48)=1 exp(fy(cid:48)(x))(cid:1).
log (cid:0) (cid:80)d2
Given n training examples {(xi, yi)}n
compute an empirical risk minimizer:

i=1, we would like to

(cid:98)fcnn ∈ arg min
f ∈Fcnn

L(f (xi); yi).

(4)

n
(cid:88)

i=1

Recalling that functions f ∈ Fcnn depend on the parame-
ters w and α in a highly nonlinear way (2), this optimiza-
tion problem is nonconvex. As mentioned earlier, heuris-
tics based on stochastic gradient methods are used in prac-
tice, which makes it challenging to gain a theoretical un-
derstanding of their behavior. Thus, in the next section, we
describe a relaxation of the class Fcnn for which empirical
risk minimization is convex.

3. Convexifying CNNs

We now turn to the development of the class of convexiﬁed
CNNs. We begin in Section 3.1 by illustrating the proce-
dure for the special case of the linear activation function.
Although the linear case is not of practical interest, it pro-
vides intuition for our more general convexiﬁcation proce-
dure, described in Section 3.2, which applies to nonlinear
activation functions. In particular, we show how embed-
ding the nonlinear problem into an appropriately chosen re-
producing kernel Hilbert space (RKHS) allows us to again
reduce to the linear setting.

3.1. Linear activation functions: low rank relaxations

In order to develop intuition for our approach, let us begin
by considering the simple case of the linear activation func-
tion σ(t) = t. In this case, the ﬁlter hj when applied to the
patch vector zp(x) outputs a Euclidean inner product of the
form hj(zp(x)) = (cid:104)zp(x), wj(cid:105). For each x ∈ Rd0 , we ﬁrst
deﬁne the P × d1-dimensional matrix

z1(x)(cid:62)
...
zP (x)(cid:62)

Z(x) :=


 .

(5)






also

deﬁne

We
vector
αk,j := (αk,j,1, . . . , αk,j,P )(cid:62). With this notation, we
can rewrite equation (2) for the kth output as

P -dimensional

the

fk(x) =

αk,j,p(cid:104)zp(x), wj(cid:105) =

α(cid:62)

k,jZ(x)wj

r
(cid:88)

P
(cid:88)

j=1

p=1

r
(cid:88)

j=1

(cid:16)

= tr

Z(x)

(cid:16) r
(cid:88)

(cid:17)(cid:17)

wjα(cid:62)
k,j

j=1

= tr(Z(x)Ak),

(6)

where in the ﬁnal step, we have deﬁned the d1 × P -
dimensional matrix Ak := (cid:80)r
k,j. Observe that fk
now depends linearly on the matrix parameter Ak. More-
over, the matrix Ak has rank at most r, due to the parameter

j=1 wjα(cid:62)

sharing of CNNs. See Figure 1 for a graphical illustration
of this model structure.

Letting A := (A1, . . . , Ad2) be a concatenation of these
matrices across all d2 output coordinates, we can then de-
ﬁne a function f A : Rd1 → Rd2 of the form

(7)

f A(x) := (tr(Z(x)A1), . . . , tr(Z(x)Ad2 )).
Note that these functions have a linear parameterization in
terms of the underlying matrix A. Our model class cor-
responds to a collection of such functions based on im-
posing certain constraints on the underlying matrix A: in
particular, we deﬁne Fcnn(B1, B2) to be the set of func-
tions f A which satisﬁes:
(C1) maxj∈[r] (cid:107)wj(cid:107)2 ≤ B1,
maxk∈[d2],j∈[r] (cid:107)αk,j(cid:107)2 ≤ B2; and (C2) rank(A) = r.
This is simply an alternative formulation of our original
class of CNNs deﬁned in equation (3). Notice that if the
ﬁlter weights wj are not shared across all patches, then the
constraint (C1) still holds, but constraint (C2) no longer
holds. Thus, the parameter sharing of CNNs is realized by
the low-rank constraint (C2). The matrix A of rank r can
be decomposed as A = U V (cid:62), where both U and V have r
columns. The column space of matrix A contains the con-
volution parameters {wj}, and the row space of A contains
to the output parameters {αk,j}.

The matrices satisfying constraints (C1) and (C2) form a
nonconvex set. A standard convex relaxation is based on
the nuclear norm (cid:107)A(cid:107)∗ corresponding to the sum of the
singular values of A.
It is straightforward to verify that
any matrix A satisfying the constraints (C1) and (C2) must
d2. Con-
have nuclear norm bounded as (cid:107)A(cid:107)∗ ≤ B1B2r
sequently, if we deﬁne the function class
(cid:110)

√

Fccnn :=

f A : (cid:107)A(cid:107)∗ ≤ B1B2r

(cid:112)

(cid:111)
,

d2

(8)

then we are guaranteed that Fccnn ⊇ Fcnn.

We propose to minimize the empirical risk (4) over Fccnn
instead of Fcnn; doing so deﬁnes a convex optimization
problem over this richer class of functions

(cid:98)fccnn := arg min

f A∈Fccnn

L(f A(xi); yi).

(9)

n
(cid:88)

i=1

In Section 3.3, we describe iterative algorithms that can be
used to solve this convex problem in the more general set-
ting of nonlinear activation functions.

3.2. Nonlinear activations: RKHS ﬁlters

For nonlinear activation functions σ, we relax the class of
CNN ﬁlters to a reproducing kernel Hilbert space (RKHS).
As we will show, this relaxation allows us to reduce the
problem to the linear activation case.
Let K : Rd1 × Rd1 → R be a positive semideﬁnite kernel
function. For particular choices of kernels (e.g., the Gaus-

Convexiﬁed Convolutional Neural Networks

Figure 1. The kth output of a CNN fk(x) ∈ R can be expressed as the product between a matrix Z(x) ∈ RP ×d1 whose rows are
features of the input patches and a rank-r matrix Ak ∈ Rd1×P , which is made up of the ﬁlter weights {wj} and coefﬁcients {ak,j,p},
as illustrated. Due to the parameter sharing intrinsic to CNNs, the matrix Ak inherits a low rank structure, which can be encouraged via
convex relaxation using the nuclear norm.

sian RBF kernel) and a sufﬁciently smooth activation func-
tion σ, we are able to show that the ﬁlter h : z (cid:55)→ σ((cid:104)w, z(cid:105))
is contained in the RKHS induced by the kernel function K.
See Section 3.4 for the choice of the kernel function and the
activation function. Let S := {zp(xi) : p ∈ [P ], i ∈ [n]}
be the set of patches in the training dataset. The represen-
ter theorem then implies that for any patch zp(xi) ∈ S, the
function value can be represented by

h(zp(xi)) =

ci(cid:48),p(cid:48)K(zp(xi), zp(cid:48)(xi(cid:48))) (10)

(cid:88)

(i(cid:48),p(cid:48))∈[n]×[P ]

for some coefﬁcients {ci(cid:48),p(cid:48)}(i(cid:48),p(cid:48))∈[n]×[P ]. Filters of the
form (10) are members of the RKHS, because they are lin-
ear combinations of basis functions z (cid:55)→ K(z, zp(cid:48)(xi(cid:48))).
Such ﬁlters are parametrized by a ﬁnite set of coefﬁcients
{ci(cid:48),p(cid:48)}(i(cid:48),p(cid:48))∈[n]×[P ], which can be estimated via empirical
risk minimization.
Let K ∈ RnP ×nP be the symmetric kernel matrix, where
with rows and columns indexed by the example-patch in-
dex pair (i, p) ∈ [n] × [P ]. The entry at row (i, p) and
column (i(cid:48), p(cid:48)) of matrix K is equal to K(zp(xi), zp(cid:48)(xi(cid:48))).
So as to avoid re-deriving everything in the kernelized set-
ting, we perform a reduction to the linear setting of Sec-
tion 3.1. Consider a factorization K = QQ(cid:62) of the kernel
matrix, where Q ∈ RnP ×m; one example is the Cholesky
factorization with m = nP . We can interpret each row
Q(i,p) ∈ Rm as a feature vector in place of the original
zp(xi) ∈ Rd1, and rewrite equation (10) as

h(zp(xi)) = (cid:104)Q(i,p), w(cid:105) where w :=

ci(cid:48),p(cid:48)Q(i(cid:48),p(cid:48)).

(cid:88)

(i(cid:48),p(cid:48))

In order to learn the ﬁlter h, it sufﬁces to learn the m-
dimensional vector w. To do this, deﬁne patch matrices
Z(xi) ∈ RP ×m for each i ∈ [n] so that its p-th row
is Q(i,p). Then the problem reduces to learning a linear
ﬁlter with coefﬁcient vector w. Carrying out all of Sec-

tion 3.1, solving the ERM gives us a parameter matrix
A ∈ Rm×P d2. The only difference is that (cid:96)2-norm con-
straint (C1) needs to be adapted to the norm of the RKHS.
See Appendix B for details.
At test time, given a new input x ∈ Rd0, we can compute a
patch matrix Z(x) ∈ RP ×m as follows:
• The p-th row of this matrix is the feature vector for
patch p, which is equal to Q†v(zp(x)) ∈ Rm, where
for any patch z, the vector v(z) is deﬁned as a nP -
dimensional vector whose (i, p)-th coordinate is equal
to K(z, zp(xi)). We note that if x is an instance xi in
the training set, then the vector Q†v(zp(x)) is exactly
equal to Q(i,p). Thus the mapping Z(x) applies to both
training and testing.

• We can then compute the predictor fk(x) = tr(Z(x)Ak)
via equation (6). Note that we do not explicitly need
to compute the ﬁlter values hj(zp(x)) to compute the
output under the CCNN.

Retrieving ﬁlters. When we learn multi-layer CCNNs
(Section 4), we need to compute the ﬁlters hj explicitly
in order to form the inputs to the next layer. Recall from
Section 3.1 that the column space of matrix A corresponds
to parameters of the convolutional layer, and the row space
of A corresponds to parameters of the output layer. Thus,
once we obtain the parameter matrix A, we compute a rank-
r approximation A ≈ (cid:98)U (cid:98)V (cid:62). Then set the j-th ﬁlter hj to
the mapping

z (cid:55)→ (cid:104) (cid:98)Uj, Q†v(z)(cid:105)

for any patch z ∈ Rd1,

(11)

where (cid:98)Uj ∈ Rm is the j-th column of matrix (cid:98)U , and
Q†v(z) represents the feature vector for patch z.2 The
matrix (cid:98)V (cid:62) encodes parameters of the output layer, thus

2If z is a patch in the training set, namely z = zp(xi), then we

have equation Q†v(z) = Q(i,p)

fk(x)=tr(cid:16)Z(x)z1(x)zP(x)Pd1×w1wrd1r(ﬁlters)×αk,1αk,rrP(patches)(cid:17)AkConvexiﬁed Convolutional Neural Networks

Algorithm 1 Learning two-layer CCNNs
Input: Data {(xi, yi)}n
parameter R > 0, number of ﬁlters r.
1. Construct a kernel matrix K ∈ RnP ×nP such that the entry

i=1, kernel function K, regularization

at column (i, p) and row (i(cid:48), p(cid:48)) is equal to
K(zp(xi), zp(cid:48) (xi(cid:48) )). Compute a factorization K = QQ(cid:62) or
an approximation K ≈ QQ(cid:62), where Q ∈ RnP ×m.

2. For each xi, construct patch matrix Z(xi) ∈ RP ×m whose
p-th row is the (i, p)-th row of Q, where Z(·) is deﬁned in
Section 3.2.

3. Solve the following optimization problem to obtain a matrix

(cid:98)A = ( (cid:98)A1, . . . , (cid:98)Ad2 ):

(cid:98)A ∈ argmin
(cid:107)A(cid:107)∗≤R

(cid:101)L(A) where

(12)

(cid:101)L(A) :=

(cid:16)(cid:0)tr(Z(xi)A1), . . . , tr(Z(xi)Ad2 )(cid:1); yi
L

(cid:17)

.

n
(cid:88)

i=1

4. Compute a rank-r approximation (cid:98)U (cid:98)V (cid:62) ≈ (cid:98)A where

(cid:98)U ∈ Rm×r and (cid:98)V ∈ RP d2×r.

Output: predictor (cid:98)fccnn(x) := (cid:0)tr(Z(x) (cid:98)A1), . . . , tr(Z(x) (cid:98)Ad2 )(cid:1)
and the convolutional layer output H(x) := (cid:98)U (cid:62)Z(x)(cid:62).

doesn’t appear in the ﬁlter expression (11).
It is impor-
tant to note that the ﬁlter retrieval is not unique, because
the rank-r approximation of the matrix A is not unique.
The heuristic we suggest is to form the singular value de-
composition A = U ΛV (cid:62), then deﬁne (cid:98)U to be the ﬁrst r
columns of U .

When we apply all of the r ﬁlters to all patches of an input
x ∈ Rd0 , the resulting output is H(x) := (cid:98)U (cid:62)Z(x)(cid:62) —
this is an r × P matrix whose element at row j and column
p is equal to hj(zp(x)).

3.3. Algorithm

The algorithm for learning a two-layer CCNN is summa-
rized in Algorithm 1; it is a formalization of the steps de-
scribed in Section 3.2. In order to solve the optimization
problem (12), the simplest approach is via projected gra-
dient descent: At iteration t, using a step size ηt > 0, we
form the new matrix At+1 based on the previous iterate At
according to:

At+1 = ΠR

(cid:16)

(cid:17)
At − ηt ∇A (cid:101)L(At)

.

(13)

Here ∇A (cid:101)L denotes the gradient of the objective function
deﬁned in (12), and ΠR denotes the Euclidean projection
onto the nuclear norm ball {A : (cid:107)A(cid:107)∗ ≤ R}. This nuclear
norm projection can be obtained by ﬁrst computing the sin-
gular value decomposition of A, and then projecting the
vector of singular values onto the (cid:96)1-ball. This latter pro-
jection step can be carried out efﬁciently by the algorithm
of Duchi et al. (2008). There are other efﬁcient optimiza-

tion algorithms (Duchi et al., 2011; Xiao & Zhang, 2014)
for solving the problem (12). All these algorithms can be
executed in a stochastic fashion, so that each gradient step
processes a mini-batch of examples.

The computational complexity of each iteration depends on
the width m of the matrix Q. Setting m = nP allows
us to solve the exact kernelized problem, but to improve
the computation efﬁciency, we can use Nystr¨om approxi-
mation (Drineas & Mahoney, 2005) or random feature ap-
proximation (Rahimi & Recht, 2007); both are randomized
methods to obtain a tall-and-thin matrix Q ∈ RnP ×m such
that K ≈ QQ(cid:62). Typically, the parameter m is chosen to be
much smaller than nP . In order to compute the matrix Q,
the Nystr¨om approximation method takes O(m2nP ) time.
The random feature approximation takes O(mnP d1) time,
but can be improved to O(mnP log d1) time using the fast
Hadamard transform (Le et al., 2013). The time complexity
of project gradient descent also scales with m rather than
with nP .

3.4. Theoretical results

In this section, we upper bound the generalization error of
Algorithm 1, proving that it converges to the best possible
generalization error of CNN. We focus on the binary clas-
siﬁcation case where the output dimension is d2 = 1.3

The learning of CCNN requires a kernel function K. We
consider kernel functions whose associated RKHS is large
enough to contain any function of the following form: z (cid:55)→
q((cid:104)w, z(cid:105)), where q is an arbitrary polynomial function and
w ∈ Rd1 is an arbitrary vector. As a concrete example, we
consider the inverse polynomial kernel:
1
2 − (cid:104)z, z(cid:48)(cid:105)

(cid:107)z(cid:107)2 ≤ 1, (cid:107)z(cid:48)(cid:107)2 ≤ 1.

K(z, z(cid:48)) :=

(14)

,

This kernel was studied by Shalev-Shwartz et al. (2011) for
learning halfspaces, and by Zhang et al. (2016a) for learn-
ing fully-connected neural networks. We also consider the
Gaussian RBF kernel:

K(z, z(cid:48)) := e−γ(cid:107)z−z(cid:48)(cid:107)2

2, (cid:107)z(cid:107)2 = (cid:107)z(cid:48)(cid:107)2 = 1.

(15)

As shown by Appendix A, the inverse polynomial kernel
and the Gaussian kernel satisfy the above notion of rich-
ness. We focus on these two kernels for the theoretical
analysis.

Let (cid:98)fccnn be the CCNN that minimizes the empirical
risk (12) using one of the two kernels above. Our main
theoretical result is that for suitably chosen activation func-
tions, the generalization error of (cid:98)fccnn is comparable to that
of the best CNN model. In particular, we consider the fol-
lowing types of activation functions σ:

3We can treat the multiclass case by performing a standard

one-versus-all reduction to the binary case.

(a) arbitrary polynomial functions (e.g., used by Chen &

tains the class of CNNs. This function class is deﬁned as:

Convexiﬁed Convolutional Neural Networks

Manning (2014); Livni et al. (2014)).

(b) sinusoid activation function σ(t) := sin(t) (e.g., used

by Sopena et al. (1999); Isa et al. (2010)).

√

π (cid:82) t

0 e−z2

(c) erf function σerf (t) := 2/

dz, which rep-
resents a close approximation to the sigmoid func-
tion (Zhang et al., 2016a).

(d) a smoothed hinge loss σsh(t) := (cid:82) t

1
2 (σerf (z) +
1)dz, which represents a close approximation to the
ReLU function (Zhang et al., 2016a).

−∞

To understand how these activation functions pair with our
choice of kernels, we consider polynomial expansions of
the above activation functions: σ(t) = (cid:80)∞
j=0 ajtj, and
note that the smoothness of these functions are character-
ized by the rate of their coefﬁcients {aj}∞
j=0 converging to
zero. If σ is a polynomial in category (a), then the richness
of the RKHS guarantees that it contains the class of ﬁlters
activated by function σ. If σ is a non-polynomial function
in categories (b),(c),(d), then as Appendix A shows, the
RKHS contains the ﬁlter only if the coefﬁcients {aj}∞
j=0
converge quickly enough to zero (the criterion depends on
the concrete choice of the kernel). Concretely, the inverse
polynomial kernel is shown to capture all of the four cat-
egories of activations:
thus, (a), (b), (c), and (d) are all
are referred as valid activation functions for the inverse
polynomial kernel. The Gaussian kernel induces a smaller
RKHS, so only (a) and (b) are valid activation functions
for the Gaussian kernel. In contrast, the sigmoid function
and the ReLU function are not valid for either kernel, be-
cause their polynomial expansions fail to converge quickly
enough, or more intuitively speaking, because they are not
smooth enough to be contained in the RKHS.

In the
We are ready to state the main theoretical result.
theorem statement, we use K(X) ∈ RP ×P to denote the
random kernel matrix obtained from an input vector X ∈
Rd0 drawn randomly from the population. More precisely,
the (p, q)-th entry of K(X) is given by K(zp(X), zq(X)).

Theorem 1. Assume that the loss function L(·; y) is L-
Lipchitz continuous for every y ∈ [d2] and that K is the
inverse polynomial kernel or the Gaussian kernel. For any
valid activation function σ, there is a constant Cσ(B1) such
that by choosing hyper-parameter R := Cσ(B1)B2r in Al-
gorithm 1, the expected generalization error is at most
EX,Y [L(f (X); Y )]
EX,Y [L( (cid:98)fccnn(X); Y )] ≤ inf
f ∈Fcnn
c LCσ(B1)B2r(cid:112)log(nP ) EX [(cid:107)K(X)(cid:107)2]
√
n

, (16)

+

where c > 0 is a universal constant.

Proof sketch The proof of Theorem 1 consists of two
parts: First, we consider a larger function class that con-

(cid:110)

Fccnn :=

x (cid:55)→

r∗
(cid:88)

P
(cid:88)

j=1

p=1

αj,phj(zp(x)) : r∗ < ∞ (17)

and

(cid:107)αj(cid:107)2(cid:107)hj(cid:107)H ≤ Cσ(B1)B2d2

(18)

(cid:111)
.

r∗
(cid:88)

j=1

where (cid:107)·(cid:107)H is the norm of the RKHS associated with the
kernel. This new function class relaxes the class of CNNs
in two ways: 1) the ﬁlters are relaxed to belong to the
RKHS, and 2) the (cid:96)2-norm bounds on the weight vectors
are replaced by a single constraint on (cid:107)αj(cid:107)2 and (cid:107)hj(cid:107)H.
We prove the following property for the predictor (cid:98)fccnn: it
must be an empirical risk minimizer of Fccnn. This property
holds even though equation (18) deﬁnes a non-parametric
function class Fccnn, while Algorithm 1 optimizes (cid:98)fccnn in
a parametric function class.

Second, we characterize the Rademacher complexity of
this new function class Fccnn, proving an upper bound for
it based on the matrix concentration theory. Combining
this bound with the classical Rademacher complexity the-
ory (Bartlett & Mendelson, 2003), we conclude that the
generalization loss of (cid:98)fccnn converges to the least possible
generalization error of Fccnn. The latter loss is bounded by
the generalization loss of CNNs (because Fcnn ⊆ Fccnn),
which establishes the theorem. See the full version of this
paper (Zhang et al., 2016b) for a rigorous proof of Theo-
(cid:4)
rem 1.

Remark on activation functions.
It is worth noting that
the quantity Cσ(B1) depends on the activation function σ,
and more precisely, depends on the convergence rate of
the polynomial expansion of σ. Appendix A shows that
if σ is a polynomial function of degree (cid:96), then Cσ(B1) =
O(B(cid:96)
1). If σ is the sinusoid function, the erf function or
the smoothed hinge loss, then the quantity Cσ(B1) will be
exponential in B1. From an algorithmic perspective, we
don’t need to know the activation function for executing
Algorithm 1. From a theoretical perspective, however, the
choice of σ is relevant from the point of Theorem 1 to com-
pare (cid:98)fccnn with the best CNN, whose representation power
is characterized by the choice of σ. Therefore, if a CNN
with a low-degree polynomial σ performs well on a given
task, then CCNN also enjoys correspondingly strong gen-
eralization. Empirically, this is actually borne out: in Sec-
tion 5, we show that the quadratic activation function per-
forms almost as well as the ReLU function for digit classi-
ﬁcation.

Remark on parameter sharing.
In order to demonstrate
the importance of parameter sharing, consider a CNN with-
out parameter sharing, so that we have ﬁlter weights wj,p
for each ﬁlter index j and patch index p. With this change,

Convexiﬁed Convolutional Neural Networks

the new CNN output (2) is

f (x) =

αj,pσ(w(cid:62)

j,pzp(x)),

r
(cid:88)

P
(cid:88)

j=1

p=1

where αj,p ∈ R and wj,p ∈ Rd1. Note that the hid-
den layer of this new network has P times more pa-
rameters than that of the convolutional neural network
with parameter sharing.
These networks without pa-
rameter sharing can be learned by the recursive kernel
method proposed by Zhang et al. (2016a). Their paper
shows that under the norm constraints (cid:107)wj(cid:107)2 ≤ B(cid:48)
1 and
(cid:80)r
2, the excess risk of the recur-
(cid:112)Kmax/n),
sive kernel method is at most O(LCσ(B(cid:48)
1)B(cid:48)
2
where Kmax = maxz:(cid:107)z(cid:107)2≤1 K(z, z) is the maximal value
of the kernel function. Plugging in the norm constraints
of the function class Fcnn, we have B(cid:48)
2 =
P . Thus, the expected risk of the estimated (cid:98)f is
B2r
bounded by:

p=1 |αj,p| ≤ B(cid:48)

1 = B1 and B(cid:48)

(cid:80)P

j=1

√

EX,Y [L( (cid:98)f (X); Y )] ≤ inf
f ∈Fcnn
c LCσ(B1)B2r
n

EX,Y [L(f (X); Y )]
√

P Kmax

√

+

.

(19)

Comparing this bound to Theorem 1, we see that (apart
from the logarithmic terms) they differ in the multiplica-
P Kmax versus (cid:112)E[(cid:107)K(X)(cid:107)2]. Since the
tive factors of
matrix K(X) is P -dimensional, we have

√

(cid:107)K(X)(cid:107)2 ≤ max
p∈[P ]

|K(zp(X), zq(X))| ≤ P Kmax.

(cid:88)

q∈[P ]

√

√

P Kmax is always greater than
This demonstrates that
(cid:112)E[(cid:107)K(X)(cid:107)2].
In general, the ﬁrst term can be up to
P times greater, which implies that the sample
factor of
complexity of the recursive kernel method is up to P times
greater than that of the CCNN. This difference is intuitive
given that the recursive kernel method learns a model with
P times more parameters. Although comparing the upper
bounds doesn’t rigorously show that one method is better
than the other, it gives intuition for understanding the im-
portance of parameter sharing.

4. Learning multi-layer CCNNs

In this section, we describe a heuristic method for learning
CNNs with more layers. The idea is to estimate the param-
eters of the convolutional layers incrementally from bot-
tom to top. Before presenting the multi-layer algorithm, we
present two extensions, average pooling and multi-channel
inputs.

Algorithm 2 Learning multi-layer CCNNs
Input:Data {(xi, yi)}n
m, regularization parameters R1, . . . , Rm, number of ﬁlters
r1, . . . , rm.
Deﬁne H1(x) = x. For each layer s = 2, . . . , m:
• Train a two-layer network by Algorithm 1, taking

i=1, kernel function K, number of layers

{(Hs−1(xi), yi)}n
parameters. Let Hs be the output of the convolutional layer
and (cid:98)fs be the predictor.

i=1 as training examples and Rs, rs as

Output: Predictor (cid:98)fm and the top layer output Hm.

the convolutional layer, then the k-th output of the CCNN
model becomes tr(GZ(x)Ak) where G ∈ RP (cid:48)×P is the
pooling matrix. Thus, performing a pooling operation re-
quires only replacing every matrix Z(xi) in problem (12)
by the pooled matrix GZ(xi). Note that the linearity of
the CCNN allows us to effectively pool before convolu-
tion, even though for the CNN, pooling must be done after
applying the nonlinear ﬁlters. The resulting ERM problem
is still convex, and the number of parameters have been re-
duced by P/P (cid:48)-fold.

If our input has C
Processing multi-channel inputs.
channels (corresponding to RGB colors, for example), then
the input becomes a matrix x ∈ RC×d0 . The c-th row of
matrix x, denoted by x[c] ∈ Rd0, is a vector representing
the c-th channel. We deﬁne the multi-channel patch vector
as a concatenation of patch vectors for each channel:
zp(x) := (zp(x[1]), . . . , zp(x[C])) ∈ RCd1.
Then we construct the feature matrix Z(x) using the con-
catenated patch vectors {zp(x)}P
p=1. From here, everything
else of Algorithm 1 remains the same. We note that this
approach learns a convex relaxation of ﬁlters taking the
form σ((cid:80)C
c=1(cid:104)wc, zp(x[c])(cid:105)), parametrized by the vectors
{wc}C
c=1.

Multi-layer CCNN. Given these extensions, we are
ready to present the algorithm for learning multi-layer CC-
NNs, summarized in Algorithm 2. For each layer s, we call
Algorithm 1 using the output of previous convolutional lay-
ers as input—note that this consists of r channels (one from
each previous ﬁlter); thus we must use the multi-channel
extension. Algorithm 2 outputs a new convolutional layer
along with a prediction function, which is kept only at the
last layer. We optionally use averaging pooling after each
successive layer. to reduce the output dimension of the con-
volutional layers.

5. Experiments

Average pooling. Average pooling is a technique to re-
duce the output dimension of the convolutional layer from
dimensions P × r to dimensions P (cid:48) × r with P (cid:48) < P .
For the CCNN model, if we apply average pooling after

In this section, we compare the CCNN approach with
other methods on the MNIST dataset and more challeng-
ing variations (VariationsMNIST), including adding white
noise (rand), random rotation (rot), random image back-

Convexiﬁed Convolutional Neural Networks

SVMrbf (Vincent et al., 2010)
NN-1 (Vincent et al., 2010)
CNN-1 (ReLU)
CCNN-1
TIRBM (Sohn & Lee, 2012)
SDAE-3 (Vincent et al., 2010)
ScatNet-2 (Bruna & Mallat, 2013)
PCANet-2 (Chan et al., 2015)
CNN-2 (ReLU)
CNN-2 (Quad)
CCNN-2

-

img

rot

rand

9.83%

img+rot
basic
3.03% 14.58% 11.11% 22.61%
55.18%
62.16%
4.69% 20.04% 18.11% 27.41%
3.37%
45.96%
18.84% 14.23%
2.35% 8.13% 13.45% 10.33% 42.90%
35.50%
43.76%
50.48%
35.48%
32.43%
36.90%
30.23%

4.20%
9.53%
2.84% 10.30%
7.48%
1.27% 12.30%
1.06% 6.19%
7.37%
8.27%
5.64%
2.11%
5.30%
1.75%
8.83%
4.64% 6.91%
1.39%

-
16.68%
18.40%
10.95%
10.17%
11.60%
7.44%

-

Table 1. Classiﬁcation error on the basic MNIST and its four variations. The best performance within each block is bolded. “ReLU” and
“Quad” denote using the ReLU and quadratic activation functions, respectively.

ground (img) or combining the last two (img+rot). For
all datasets, we use 10,000 images for training, 2,000 im-
ages for validation and 50,000 images for testing. This
10k/2k/50k partitioning is standard for MNIST varia-
tions (VariationsMNIST).

For the CCNN method and the baseline CNN method, we
train two-layer and three-layer models respectively. The
models with k convolutional layers are denoted by CCNN-
k and CNN-k. Each convolutional layer is constructed on
5 × 5 patches with unit stride, followed by 2 × 2 average
pooling. The ﬁrst and the second convolutional layers con-
tains 16 and 32 ﬁlters, respectively. The loss function is
chosen as the 10-class logistic loss. We use Gaussian ker-
nel for the CCNN. The feature matrix Z(x) is constructed
via random feature approximation (Rahimi & Recht, 2007)
with dimension m = 500 for the ﬁrst convolutional layer
and m = 1000 for the second. Before training each CCNN
layer, we preprocess the input vectors zp(xi) using local
contrast normalization and ZCA whitening (Coates et al.,
2010). The convex optimization problem is solved by pro-
jected SGD with mini-batches of size 50. Code and re-
producible experiments are available on the CodaLab plat-
form4.

As a baseline approach, the CNN models are activated by
the ReLU function σ(t) = max{0, t} or the quadratic
function σ(t) = t2. We train them using mini-batch
SGD. The input images are preprocessed by global con-
trast normalization and ZCA whitening (Srivastava et al.,
2014). We compare our method against several alterna-
tive baselines. The CCNN-1 model is compared against
an SVM with the Gaussian RBF kernel (SVMrbf ) and a
fully connected neural network with one hidden layer (NN-
1). The CCNN-2 model is compared against methods that
report the state-of-the-art results on these datasets, includ-
ing the translation-invariant RBM model (TIRBM) (Sohn
& Lee, 2012), the stacked denoising auto-encoder with

4http://worksheets.codalab.org/

worksheets/0x1468d91a878044fba86a5446f52aacde/

three hidden layers (SDAE-3) (Vincent et al., 2010), the
ScatNet-2 model (Bruna & Mallat, 2013) and the PCANet-
2 model (Chan et al., 2015).

Table 1 shows the classiﬁcation errors on the test set. The
models are grouped with respect to the number of layers
that they contain. For models with one convolutional layer,
the errors of CNN-1 are signiﬁcantly lower than that of
NN-1, highlighting the beneﬁts of local ﬁlters and param-
eter sharing. The CCNN-1 model outperforms CNN-1 on
all datasets. For models with two or more hidden layers,
the CCNN-2 model outperforms CNN-2 on all datasets,
In partic-
and is competitive against the state-of-the-art.
ular, it achieves the best accuracy on the rand, img and
img+rot dataset, and is comparable to the state-of-the-art
on the remaining two datasets. Further adding a third con-
volutional layer doesn’t notibly improve the performance
on these datasets.

In Section 3.4, we showed that if the activation function σ is
a polynomial function, then the CCNN (which does not de-
pend on σ) requires lower sample complexity to match the
performance of the best possible CNN using σ. More pre-
cisely, if σ is a degree-(cid:96) polynomial, then Cσ(B) in the up-
per bound will be controlled by O(B(cid:96)). This motivates us
to study the performance of low-degree polynomial activa-
tions. Table 1 shows that the CNN-2 model with a quadratic
activation function achieves error rates comparable to that
with a ReLU activation: CNN-2 (Quad) outperforms CNN-
2 (ReLU) on the basic and rand datasets, and is only
slightly worse on the rot and img dataset. Since the per-
formance of CCNN matches that of the best possible CNN,
the good performance of the quadratic activation in part ex-
plains why the CCNN is also good.

Acknowledgements. MJW and YZ were partially sup-
ported by the Ofﬁce of Naval Research Grant DOD ONR-
N00014 and the NSF Grant NSF-DMS-1612948. PL and
YZ were partially supported by the Microsoft Faculty Fel-
lowship.

Convexiﬁed Convolutional Neural Networks

References
Aslan, ¨Ozlem, Cheng, Hao, Zhang, Xinhua, and Schuur-
mans, Dale. Convex two-layer modeling. In Advances in
Neural Information Processing Systems, pp. 2985–2993,
2013.

Aslan,

¨Ozlem, Zhang, Xinhua, and Schuurmans, Dale.
In Ad-
Convex deep learning via normalized kernels.
vances in Neural Information Processing Systems, pp.
3275–3283, 2014.

Bartlett, Peter L and Mendelson, Shahar. Rademacher and
Gaussian complexities: Risk bounds and structural re-
sults. The Journal of Machine Learning Research, 3:
463–482, 2003.

Bengio, Yoshua, Roux, Nicolas L, Vincent, Pascal, Delal-
leau, Olivier, and Marcotte, Patrice. Convex neural net-
In Advances in Neural Information Processing
works.
Systems, pp. 123–130, 2005.

Blum, Avrim L and Rivest, Ronald L. Training a 3-node
neural network is NP-complete. Neural Networks, 5(1):
117–127, 1992.

Bottou, L´eon. Online learning and stochastic approxima-
tions. On-line learning in neural networks, 17(9):142,
1998.

Bruna, Joan and Mallat, St´ephane. Invariant scattering con-
volution networks. Pattern Analysis and Machine Intel-
ligence, IEEE Transactions on, 35(8):1872–1886, 2013.

Chan, Tsung-Han, Jia, Kui, Gao, Shenghua, Lu, Jiwen,
Zeng, Zinan, and Ma, Yi. Pcanet: A simple deep learn-
ing baseline for image classiﬁcation? IEEE Transactions
on Image Processing, 24(12):5017–5032, 2015.

Chen, Danqi and Manning, Christopher D. A fast and ac-
curate dependency parser using neural networks. In Pro-
ceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), volume 1,
pp. 740–750, 2014.

Coates, Adam, Lee, Honglak, and Ng, Andrew Y. An
analysis of single-layer networks in unsupervised feature
learning. Ann Arbor, 1001(48109):2, 2010.

Duchi, John, Shalev-Shwartz, Shai, Singer, Yoram, and
Chandra, Tushar. Efﬁcient projections onto the (cid:96)1-ball
for learning in high dimensions. In Proceedings of the
25th International Conference on Machine Learning, pp.
272–279. ACM, 2008.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. The Journal of Machine Learning Re-
search, 12:2121–2159, 2011.

Fahlman, Scott E. An empirical study of learning speed in
back-propagation networks. Journal of Heuristics, 1988.

Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E,
Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, An-
drew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath,
Tara N, et al. Deep neural networks for acoustic mod-
eling in speech recognition: The shared views of four
research groups. Signal Processing Magazine, IEEE, 29
(6):82–97, 2012.

Isa, IS, Saad, Z, Omar, S, Osman, MK, Ahmad, KA, and
Sakim, HA Mat. Suitable mlp network activation func-
tions for breast cancer and thyroid disease detection.
In 2010 Second International Conference on Computa-
tional Intelligence, Modelling and Simulation, pp. 39–
44, 2010.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in Neural Information Processing
Systems, pp. 1097–1105, 2012.

Lawrence, Steve, Giles, C Lee, Tsoi, Ah Chung, and Back,
Andrew D. Face recognition: A convolutional neural-
network approach. Neural Networks, IEEE Transactions
on, 8(1):98–113, 1997.

Le, Quoc, Sarl´os, Tam´as, and Smola, Alex. Fastfood-
approximating kernel expansions in loglinear time.
In
Proceedings of the International Conference on Machine
Learning, 2013.

LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998.

Daniely, Amit, Frostig, Roy, and Singer, Yoram. Toward
deeper understanding of neural networks: The power
of initialization and a dual view on expressivity. arXiv
preprint arXiv:1602.05897, 2016.

Livni, Roi, Shalev-Shwartz, Shai, and Shamir, Ohad. On
the computational efﬁciency of training neural networks.
In Advances in Neural Information Processing Systems,
pp. 855–863, 2014.

Drineas, Petros and Mahoney, Michael W. On the Nystr¨om
method for approximating a Gram matrix for improved
kernel-based learning. The Journal of Machine Learning
Research, 6:2153–2175, 2005.

Mairal, Julien, Koniusz, Piotr, Harchaoui, Zaid, and
Schmid, Cordelia. Convolutional kernel networks.
In
Advances in Neural Information Processing Systems, pp.
2627–2635, 2014.

Convexiﬁed Convolutional Neural Networks

Xiao, Lin and Zhang, Tong. A proximal stochastic gradi-
ent method with progressive variance reduction. SIAM
Journal on Optimization, 24(4):2057–2075, 2014.

Zhang, Yuchen, Lee, Jason D, and Jordan, Michael I. (cid:96)1-
regularized neural networks are improperly learnable in
In Proceedings on the 33rd Interna-
polynomial time.
tional Conference on Machine Learning, 2016a.

Zhang, Yuchen, Liang, Percy, and Wainwright, Martin J.
CoRR,
Convexiﬁed convolutional neural networks.
abs/1609.01000, 2016b. URL http://arxiv.org/
abs/1609.01000.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Rusu, Andrei A, Veness, Joel, Bellemare, Marc G,
Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K,
Ostrovski, Georg, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529–
533, 2015.

Rahimi, Ali and Recht, Benjamin. Random features for
large-scale kernel machines. In Advances in Neural In-
formation Processing Systems, pp. 1177–1184, 2007.

Shalev-Shwartz, Shai, Shamir, Ohad, and Sridharan,
Karthik. Learning kernel-based halfspaces with the 0-
1 loss. SIAM Journal on Computing, 40(6):1623–1646,
2011.

Silver, David, Huang, Aja, Maddison, Chris J, Guez,
Arthur, Sifre, Laurent, Van Den Driessche, George,
Schrittwieser, Julian, Antonoglou, Ioannis, Panneershel-
vam, Veda, Lanctot, Marc, et al. Mastering the game of
Go with deep neural networks and tree search. Nature,
529(7587):484–489, 2016.

Sohn, Kihyuk and Lee, Honglak. Learning invariant repre-
sentations with local transformations. In Proceedings of
the 29th International Conference on Machine Learning
(ICML-12), pp. 1311–1318, 2012.

Sopena, Josep M, Romero, Enrique, and Alquezar, Rene.
Neural networks with periodic and monotonic activation
functions: a comparative study in classiﬁcation prob-
lems. In ICANN 99, pp. 323–328, 1999.

Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:
A simple way to prevent neural networks from overﬁt-
ting. The Journal of Machine Learning Research, 15(1):
1929–1958, 2014.

VariationsMNIST. Variations on the MNIST digits. http:
//www.iro.umontreal.ca/˜lisa/twiki/
bin/view.cgi/Public/MnistVariations,
2007.

Vincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Ben-
gio, Yoshua, and Manzagol, Pierre-Antoine. Stacked
denoising autoencoders: Learning useful representations
in a deep network with a local denoising criterion. The
Journal of Machine Learning Research, 11:3371–3408,
2010.

Wang, Tao, Wu, David J, Coates, Andrew, and Ng, An-
drew Y. End-to-end text recognition with convolutional
neural networks. In Pattern Recognition (ICPR), 2012
21st International Conference on, pp. 3304–3308. IEEE,
2012.

