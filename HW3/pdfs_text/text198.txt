Fake News Mitigation via Point Process Based Intervention

Mehrdad Farajtabar 1 Jiachen Yang 1 Xiaojing Ye 2 Huan Xu 3 Rakshit Trivedi 1 Elias Khalil 1 Shuang Li 3
Le Song 1 Hongyuan Zha 1

Abstract
We propose the ﬁrst multistage intervention
framework that tackles fake news in social net-
works by combining reinforcement learning with
a point process network activity model. The
spread of fake news and mitigation events within
the network is modeled by a multivariate Hawkes
process with additional exogenous control terms.
By choosing a feature representation of states,
deﬁning mitigation actions and constructing re-
ward functions to measure the effectiveness of
mitigation activities, we map the problem of fake
news mitigation into the reinforcement learn-
ing framework. We develop a policy iteration
method unique to the multivariate networked
point process, with the goal of optimizing the
actions for maximal total reward under budget
constraints. Our method shows promising per-
formance in real-time intervention experiments
on a Twitter network to mitigate a surrogate fake
news campaign, and outperforms alternatives on
synthetic datasets.

1. Introduction
The recent proliferation of malicious fake news in social
media has been a source of widespread concern. Given
that more than 62% of U.S. adults turn to social media for
news, with 18% doing so often, fake news can have poten-
tial real-world consequences on a large scale (Gottfried &
Shearer, 2016). For example, within the ﬁnal three months
of the 2016 U.S. presidential election, news stories that fa-
vored either of the two nominees–later proved to be fake–
were shared over 37 million times on Facebook alone, and
over half of those who recalled seeing fake news stories
believed them (Allcott & Gentzkow, 2017). An analysis
by Buzzfeed News shows that the top 20 false election sto-
ries from hoax websites generated nearly 1.5 million more
user engagement activities on Facebook than the top 20 sto-

1School of Computational Science and Engineering, Geor-
2Department of Mathematics and Statistics, Georgia
gia Tech.
3School of Industrial and Systems Engineer-
State University.
ing, Georgia Tech.. Correspondence to: Mehrdad Farajtabar
<mehrdad@gatech.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ries from reputable major news outlets (Silverman, 2016).
Therefore, there is an urgent call to develop effective recti-
fying strategies to mitigate the impact of fake news.

Policies to counter fake news can be categorized by the
level of manual oversight and the aggressiveness of action
required. Aggressively acting on fake news has various
drawbacks. For example, Facebook’s strategy allows users
to report stories as potential fake news, sends these stories
to fact-checking organizations, and ﬂags them as disputed
in users’ newsfeed (Mosseri, 2016). Such direct action on
the offending news requires a high degree of human over-
sight, which can be costly and slow and also may violate
civil rights. The report-and-ﬂag mechanism is also open
to abuse by adversaries who maliciously report real news.
Given these disadvantages, we consider an alternative strat-
egy: optimizing the performance of real news propagation
over the network. Intuitively, we want people’s exposure to
real news to match their exposure to fake news.

We face several key modeling and computational issues.
For example, how to quantify the uncertainty of user ac-
tivities and news propagation within the network? How
to measure the effect of mitigation incentives and activi-
ties? Is it possible to steer the spontaneous user mitiga-
tion activities by an intervention strategy? To address these
questions, we model the temporal randomness of fake news
and mitigation events (“valid news”) as multivariate point
processes with self and mutual excitations, in which the
control incentivizes more spontaneous mitigation events
by contributing to the exogenous activity of campaigner
nodes. The inﬂuence of fake news and mitigation activities
is quantiﬁed using event exposure counts (i.e. the number
of times that a user is exposed to fake or real news posts
from other users whom she follows).

Our key contributions are as follows. We present the ﬁrst
formulation of fake news mitigation as the problem of op-
timal point process intervention in a network. The goal is
to optimize the activity policy of a set of campaigner nodes
to mitigate a fake news process stemming from another set
of nodes. It creates opportunities for designing a variety
of objectives, e.g. minimizing the number of users who
see fake news but were not reached by real news. We give
the ﬁrst derivation of second-order statistics of random ex-
posure counts in the non-stationary case, which is essen-
tial in policy evaluation and improvement. By deﬁning a

Fake News Mitigation via Point Process Based Intervention

Figure 1. The framework of point process based intervention for countering fake news. (1-3) Ofﬂine learning of value function approx-
imation weight vector using LSTD from transition samples generated from model. (4-7) Real-time intervention loop that uses feature
representation of network state to choose optimal exogenous incentive for mitigator nodes.

state space for the network, formulating actions as exoge-
nous intensity, and deﬁning reward functions, we map the
fake news mitigation problem to an optimal policy prob-
lem in a Markov decision process (MDP), which is solved
by model-based least-squares temporal difference learning
(LSTD) speciﬁc to the context of point processes. Further-
more, to the best of our knowledge, we are the ﬁrst to con-
duct a real-time point process intervention experiment.
Related work. The emergence of social media as a promi-
nent news source in the past few years raises concomitant
concerns about the quality, truthfulness, and credibility of
information presented (Mitra et al., 2017). To reduce the
amount of labor-intensive manual fact-checking, there have
been research efforts devoted to building classiﬁers to de-
tect factuality of information, predicting credibility level of
posts, and detecting controversial information from inquiry
phrases (Mitra et al., 2017; Zeng et al., 2016; Zhao et al.,
2015). These works mainly focused on extracting linguis-
tic features from texts to determine the credibility of news
and posts. Our focus in this paper, however, is to design
an incentive strategy so that users can spontaneously take
action to mitigate a real-world fake news epidemic.

Steering user activities by adding external incentives to the
exogenous intensity of Hawkes processes was ﬁrst consid-
ered in (Farajtabar et al., 2014). In (Farajtabar et al., 2016),
a multistage campaigning method to optimally distribute
incentive resources based on dynamic programming was
In these previous works, objective functions
developed.
were designed using expected values of exposure counts
rather than the stochastic exposure process, which may re-
duce the accuracy of solutions. Furthermore, it faced the
demanding problem of computing the cost-to-go using the
Hawkes model, while we address this using linear function
approximation. For stationary Hawkes processes, second
order statistics was derived in (Bacry & Muzy, 2014a;b);

however, it is essential to compute both ﬁrst and second
order statistics for Hawkes processes in the non-stationary
stages due to time sensitivity of the fake news mitigation
task, and we derive it for the ﬁrst time in this paper. Recent
work has also applied methods in stochastic differential
equations to the context of point processes, to ﬁnd the best
intensity for information guiding (Wang et al., 2016) and
achieving highest visibility (Zarezade et al., 2017). While
these works consider networks with only a single process,
our work focuses on optimizing a mitigation process with
respect to a second competing process. Finally, it’s notable
that our approach is related to but much more general than
the inﬂuence maximization problems (Kempe et al., 2003;
Bharathi et al., 2007) as it allows recurrent activity in social
networks (in contrast to binary infection states), a variety of
objectives (not only maximization), and budget constraints.

Reinforcement learning tackles the problem of ﬁnding
good policies for actions to take in MDP where exact so-
lutions are intractable, either due to size or lack of com-
plete knowledge. Large-scale policy evaluation and itera-
tion problems can be tackled by function approximation,
which reduces the solution dimension using feature vector
basis (Sutton & Barto, 1998). By adding control terms to a
multivariate Hawkes process model of random network ac-
tivities, fake news mitigation can be formulated as a policy
optimization problem in an MDP. To address the random-
ness of Hawkes processes, batch reinforcement learning us-
ing samples collected from the trajectory of a ﬁxed behav-
ior policy can be applied (Antos et al., 2007). In particu-
lar, linear Least Squares Temporal Difference (LSTD) uses
a batch of samples to learn a linear approximation of the
value function under a policy with provable convergence
(Bradtke & Barto, 1996). This policy evaluation step al-
ternates with a model-based policy improvement step in a
policy iteration to arrive at successively improved policies.

  F (fake news)Mitigator M1AdamBeckyCharlesDianaMitigator M2Twitter NetworktFake newsM1ttttτ0τ1τ2M2AdamBeckyA B A follows BModel {αij,μ}LSTDSamplesx1,x2,...,xSwπ...1...nL...0100ψ(x)nLuikt〈ψ(x),wπ〉Real-time experiment loopuk=argmaxu1234567(u1k)(u2k)Fake News Mitigation via Point Process Based Intervention

∈ Rn

2. Preliminaries and Problem Statement
Multivariate Hawkes processes. Hawkes process is
a doubly stochastic point process with self-excitations,
meaning that past events increase the chance of arrivals of
new events (Hawkes, 1971), and has been extensively used
to model activities in social networks (Farajtabar et al.,
2015; Linderman & Adams, 2014; He et al., 2015; Rizoiu
et al.; Lee et al., 2016). Let t(cid:96) be the time of the (cid:96)-th event,
then the Hawkes process can be represented by the count-
ing process N (t) = (cid:80)
t(cid:96)≤t h(t − t(cid:96)) that tracks the number
of events up to time t, where h(t) is the standard Heav-
iside function such that h(t) = 1 if t ≥ 0 and = 0 if
t < 0. The conditional intensity function of a point process
is deﬁned as the probability of observing an event in an in-
ﬁnitesimal window given the history. For Hawkes process
it is given by λ(t) = µ + (cid:80)
t(cid:96)<t φ(t − t(cid:96)). Here, µ ≥ 0 is
the exogenous (base) intensity and φ(t) is the Hawkes ker-
nel that describes how fast the excitement of a past event
decays. In this paper, we employ the standard (stationary)
exponential Hawkes kernel, i.e., φ(t) = αe−ωth(t) with
ω > α > 0. In an n-dimensional multivariate Hawkes pro-
cess (MHP), there are n such processes N1(t), . . . , Nn(t)
that can also mutually excite one another, and the con-
ditional intensity λ(t) := (cid:0)λ1(t), . . . , λn(t)(cid:1)(cid:62)
+ is
given by λ(t) = µ + (cid:82) t
0 Φ(t − s) dN (s). Here, N (t) :=
(cid:0)N1(t), . . . , Nn(t)(cid:1)(cid:62)
0 , µ := (µ1, . . . , µn)(cid:62) ∈ Rn
∈ Nn
+,
and [Φ(t)]ij = φij(t) := αije−ωth(t). We let H(t)
denote the ﬁltration of N (t), generated by the σ-algebra
of history (cid:8)(t(cid:96), i(cid:96))|t(cid:96) ≤ t(cid:9) of this point process, where
i(cid:96) ∈ {1, . . . , n} is the identity (node) of the (cid:96)-th event.
Network activities. We model the activities of both fake
news and mitigation events as MHP in the network. Ba-
sically, MHP is a networked point process model with
dependent dimensions (nodes), and can capture the un-
derlying dynamics of networks and activities (Blundell
et al., 2012; Xu et al., 2016; Guo et al., 2015). Deﬁne
F (t) = (cid:0)F1(t), . . . , Fn(t)(cid:1)(cid:62)
0 , where Fi(t) counts
the number of times user i shares a piece of news from
the fake campaign up to time t. Similarly, deﬁne M (t) =
(cid:0)M1(t), . . . , Mn(t)(cid:1)(cid:62)
0 for the mitigation process.
Correspondingly, we have 2 intensity functions: λM (t) =
n (t))(cid:62)
n (t))(cid:62) and λF (t) = (λF
(λM
and two sets of exogenous intensities µM and µF .
Goal. Given that both F (t) and M (t) are modeled by the
Hawkes processes, our goal is to ﬁnd the optimal mitiga-
tion campaign by imposing interventions to users such that
the mitigation effect (rigorously deﬁned in sec. 3.1) can be
maximized or equivalently the fake news be rectiﬁed under
budget constraints. To this end, we measure the inﬂuence
of fake news and mitigation activities using event expo-
sures, describe the mechanism of mitigation interventions,
and quantify the effect of interventions mathematically.

1 (t), . . . , λM

1 (t), . . . , λF

∈ Nn

∈ Nn

Event exposure. Event exposure is a quantitative measure
of campaign inﬂuence, and is represented as a counting
process, E(t) = (cid:0)E1(t), . . . , En(t)(cid:1)(cid:62)
. Here, Ei(t) records
the number of times user i is exposed (she or one of her
neighbors performs an activity) to a campaign N (t) by time
t. Let B be the adjacency matrix of the user network, i.e.,
bij = 1 if user i follows user j. Assume bii = 1 for all
i. Then the exposure process is given by E(t) = BN (t).
We deﬁne F(t) = BF (t) and M(t) = BM (t) as the fake
news and mitigation processes, respectively.

Intervention. Suppose we can perform intervention by
incentivizing a subset of users in the k-th stage during
time [τk, τk+1) for k = 0, 1, . . . . For simplicity we con-
sider uniform time duration τk+1 − τk = ∆T for all k,
since generalization to nonuniform time durations is triv-
ial.
In order to steer the mitigation activities to counter
the fake news (criteria given below) at these stages, we im-
pose an additional constant intervention uk
i ≥ 0 to the ex-
ogenous intensity µi during time [τk, τk+1) for each stage
k = 0, 1, . . . . The mitigation activity intensity at the k-
th stage is λM (t) = µ + uk + (cid:82) t
0 Φ(t − s) dM (s) for
t ∈ [τk, τk+1). Note that the intervention itself exhibits
a stochastic nature: adding uk
to µi is equivalent to in-
i
centivizing user i to increase her activity rate but it is still
uncertain when she will perform an activity, which appro-
priately mimics the randomness in the real world.

Reward function. For each stage k, xk (deﬁned later) is
the state of the whole MDP that encodes all the information
from previous stages and uk is the current control imposed
at this stage. Let Mk
dMj(s)
be the number of times user i is exposed to the mitiga-
tion campaign by time t ∈ [τk, τk+1) within stage k, then
the goal is to steer the expected total number of exposure
i (t; xk, uk) using uk, s.t. the sum of reward functions
Mk
R(xk, uk) (rigorously deﬁned in sec. 3.1) is maximized.

i (t; xk, uk) := (cid:80)

(cid:82) t
τk

j bij

Problem statement. By observing the counting pro-
cess in previous stages (summarized in a sequence of xk)
and taking the future uncertainty into account, the con-
trol problem is to design a policy π such that the con-
trols uk = π(xk) can maximize the total discounted ob-
jective E[(cid:80)∞
k=0 γkRk], where γ ∈ (0, 1] is the discount
rate and Rk is the observed reward at stage k.
In addi-
tion, we may have constraints on the amount of control,
such as a budget constraint on the sum of all interventions
to users at each stage, or a cap over the amount of inten-
sity a user can handle. A feasible set or an action space
over which we ﬁnd the best intervention is represented as
Uk := (cid:8)u ∈ Rn|u(cid:62)ck ≤ Ck, 0 ≤ u ≤ αk(cid:9). Here, ck
i is the
price per unit increase of exogenous intensity of user i and
Ck ∈ R+ is the total budget at stage k. Also, αk
i is the cap
on the amount of activities of the user i.

Fake News Mitigation via Point Process Based Intervention

3. Proposed Method

In this section, we present the formulation of reward func-
tions in terms of event exposures of fake news and miti-
gation activities. Then we derive the key statistics of the
MHP required for reward function evaluation, followed by
the policy iteration scheme to ﬁnd the optimal intervention.

3.1. Fake news mitigation

As we discussed above, the total reward of policy π is de-
ﬁned by the value function

V π(x0) = E

γkRk

x0

(cid:20) ∞
(cid:88)

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(1)

k=0
for the initial state x0 of fake and mitigation processes,
where the observed reward R quantiﬁes the effect of mit-
igation activities M (t) in each stage and γ ∈ (0, 1] is the
discount rate. In this paper, we consider two types of re-
ward functions R(x, u):

1) Correlation Maximization: One possible way is to re-
quire correlation between mitigation exposures and fake
news exposures: people exposed more to fake news should
also be exposed more to the true news, to counter the fake
news campaign. Therefore, we can form the reward func-
tion R in stage k as follows:

R(xk, uk) =

Mk(τk+1; xk, uk)(cid:62)F k(τk+1; xk, uk).

1
n

2) Difference Minimization: Suppose the goal is to mini-
mize the number of unmitigated fake news events, then we
can form a reward function R in stage k as the least squares
of unmitigated numbers:

−1
n

R(xk, uk) =

(cid:13)
(cid:13)
2
(cid:13)Mk(τk+1; xk, uk) − F k(τk+1; xk, uk)
(cid:13)
(cid:13)
(cid:13)
These are two sample realizations of the MHP-MDP based
intervention one can formulate, among many others. To
solve the policy optimization problem argmaxπ V π(x0)
for V π deﬁned in (1), we need to evaluate the value func-
tion V π for any given policy π, which requires the ﬁrst
and second order statistics (moments) of any multivariate
Hawkes processes N (t), as we derive next.

3.2. Second order statistics of non-stationary MHP

For an n-dim MHP N (t) with standard exponential kernel
Φ(t), the following proposition provides closed-form solu-
tion of the mean intensity η(t) := E[λ(t)] for both constant
and time-varying exogenous intensity µ(t):
Proposition 1 (Theorem 3 (Farajtabar et al., 2014; 2016)).
Let N (t) be an n-dimensional MHP deﬁned in sec. 2
with exogenous intensity µ(t) and Hawkes kernel Φ(t) =
Ae−ωth(t), then the mean intensity η(t) is given by
η(t) = (cid:2)e(A−ωI)t + ω(A − ωI)−1 (cid:0)e(A−ωI)t − I(cid:1)(cid:3) µ(t).
(2)

0

0 λ(s) ds] = (cid:82) t

Let Λ(t) = (cid:82) t
0 λ(s) ds be the compensator of N (t), then
by Doob-Meyer’s decomposition theorem N (t) − Λ(t) is
a zero mean martingale. This implies that the ﬁrst order
statistics E[N (t)] can be obtained by E[N (t)] = E[Λ(t)] =
E[λ(s)] ds = (cid:82) t
E[(cid:82) t
0 η(s) ds using eq. (2).
To evaluate the reward function R deﬁned previously,
we need to derive second order statistics of multivariate
Hawkes process N (t) in its non-stationary stage. The fol-
lowing theorem states the key ingredients for the second
order statistics. The proof is provided in the appendix.
Theorem 2. Let N (t) be an n-dim MHP with exogenous
intensity µ and Hawkes kernel Φ deﬁned in sec. 2, then the
second order statistics of N (t) for t, t(cid:48) ≥ 0 is given by

dN (t) dN (t(cid:48))(cid:62)(cid:105)
(cid:104)

E

= G(t(cid:48), t)(cid:62)Σ(t(cid:48)) dt dt(cid:48)+

δ(t − t(cid:48))Σ(t(cid:48)) dt dt(cid:48) + η(t)η(t(cid:48))(cid:62) dt dt(cid:48)

(3)

where η(t) = E[λ(t)] is given in (2), Σ(t) = diag([ηi(t)])
is diagonal, and G is the unique solution of

G(t(cid:48), t) = G(t(cid:48), t) ∗ Φ(t) + Φ(t − t(cid:48)) − δ(t − t(cid:48))I.

(4)

Moreover G(t(cid:48), t)(cid:62)Σ(t(cid:48)) = Σ(t)G(t, t(cid:48)) for all t, t(cid:48) ≥ 0.

Based on Theorem 2, we can compute second order statis-
tics such as E[Ni(t)Nj(t(cid:48))] for all i, j and t, t(cid:48) ≥ 0.

3.3. State Representation
Hawkes process is non-Markovian and one needs com-
plete knowledge of the history to characterize the entire
process. However, when the standard exponential kernel
Φ(t, s) = Ae−ω(t−s)h(t − s) is employed, the effect of
history up to time τk on the future t > τk can be cleverly
summarized by one scalar per dimension (Simma & Jor-
dan, 2012; Farajtabar et al., 2016). For 1 ≤ i ≤ n, deﬁne
i := λk−1
i − µi, (and yi
yk
0 = 0 by convention),
then the intensity due to events of all previous k stages can
be written as (cid:82) τk
In
other words, yk is sufﬁcient to encode the information of
activities in the past k stages that are relevant to future.
Note that we have two separate yk
F to track the dy-
namics of both mitigation and fake processes.

0 Ae−ω(t−s) dN (s) = yke−ω(t−τk).

(τk) − uk−1

M and yk

i

τk−l∆f

Also, in order to tackle objectives over multiple stages, we
add aggregated number of events at L previous ∆f -time
intervals over all dimensions. Deﬁne a vector zk ∈ RnL
(l−1)n+i = (cid:82) τk−(l−1)∆f
where zk
dNi(s) for 1 ≤ i ≤ n
and 1 ≤ l ≤ L. In other words, zk
(l−1)n+i records the num-
ber of events of i-th dimension in the l-th interval of length
∆f prior to time τk. For example, choosing ∆f = ∆T and
setting L = 2 means that events from the two most recent
stages are counted. Similarly, we have two separate zk
M and
zk
F corresponding to the two processes. Now, the state vec-
tor xk ∈ R2nL+2n is the concatenation of the above four
vectors xk = [yk
F ].

M ; yk

M ; zk

F ; zk

Fake News Mitigation via Point Process Based Intervention

Algorithm 1 LSTD policy iteration in point processes
Input: set of samples S, feature ψ(·), discount γ
repeat

Initialize Aπ = 0 and bπ = 0.
for each state x ∈ S do

Aπ ← Aπ + ψ(x)(ψ(x) − γψ(x(cid:48)))(cid:62)
bπ ← bπ + ψ(x)rπ

end for
wπ ← (Aπ)−1bπ
for each state x ∈ S do
π(x) ← argmax

u

end for

until (cid:107)∆wπ(cid:107) < 0.1
return wπ

{E[R(x, u)] + γE[V π(x(cid:48))|u, wπ]}

3.4. Least Squares Temporal Difference
The optimal value function satisﬁes the Bellman equation:

V π(x) = E[R(x, π(x))] + γE[V π(x(cid:48))],

(5)

where x(cid:48) is the next state after taking action based on
policy π at state x. Least squares temporal difference
learning (LSTD) is a sample-efﬁcient procedure for policy
evaluation, which subsequently facilitates policy improve-
ment. The value function is approximated by ˆV π(x) =
(cid:80)D
d ψd(x), where ψd is the d-th feature of state x
and wπ
d is its coefﬁcient for policy π. This can be com-
pactly represented as ˆV π(x) = ψ(x)(cid:62)wπ, where ψ(x) =
(ψ1(x), . . . , ψD(x))(cid:62). The following presents our choice
of features, and explains the policy evaluation and improve-
ment steps inspired by LSTD(0) (Sutton & Barto, 1998).

d=1 wπ

Features. The number of events in a few recent consecu-
tive intervals of point processes have been used as a reliable
feature to parameterize point processes (Parikh et al., 2012;
Qin & Shelton, 2015; Lian et al., 2015). Following their
work we take L prior intervals of length ∆f for each di-
mension of the fake news process and record the number of
events in that period as one feature. ψk
(l−1)n+i
for 1 ≤ i ≤ n and 1 ≤ l ≤ L. This will count for nL
features. Similarly we take nL features from the mitiga-
tion process. Finally, we add a last feature ψk
2nL+1 = 1 as
the bias term. Therefore, ψk = [zk
F ; 1] and the feature
space has dimension D = 2nL + 1.

(l−1)n+i = zk

M ; zk

Policy Evaluation. Substituting the approximation into the
Bellman equation, we have:

ψ(x)(cid:62)wπ = E[R(x, π(x))] + γE[ψ(x(cid:48))(cid:62)]wπ.

(6)

To ﬁnd the best ﬁt of wπ we have to consider all possible
x; however, since the state space is inﬁnite-dimensional,
enumerating all states is impossible and we utilize a set S
of samples S = {x1, . . . , xS}.

Algorithm 2 Real-time fake news mitigation

Input: network A, learned wπ, feature ψ(·), discount γ
repeat

Observe state x of the network activities
u = argmaxa{E[R(x, a)] + γE[V π(x(cid:48))|a, wπ]}
Add u to base exogenous intensity µ and generate mit-
igation event times {ti} using point process model
Create posts at times {ti} using campaigner accounts

until end of campaign

1 , . . . , rπ

s)] = ψ(cid:48)

1 ; . . . ; ψ(cid:48)(cid:62)

Let ψ(xs) = ψs ∈ RD, E[ψ(x(cid:48)
s ∈ RD, and
s = E[R(xs, π(xs))] ∈ R. Then deﬁne matrices of
rπ
S ](cid:62) ∈ RS×D and next
1 ; . . . ; ψ(cid:62)
current features Ψ = [ψ(cid:62)
features Ψ(cid:48) = [ψ(cid:48)(cid:62)
S ](cid:62) ∈ RS×D, the rewards
S](cid:62) ∈ RS, and the sample value func-
rπ = [rπ
tions as vπ = [V π(x1), . . . , V π(xS)](cid:62) ∈ RS. Ap-
pendix C presents how we leverage the ﬁrst and second
order statistics of Hawkes process to ﬁnd E[R(x, u)] and
E[V π(x(cid:48))]. Given the above deﬁnition, the Bellman opti-
mality of eq. (6) can be written in matrix format:
vπ = Ψwπ = rπ + γΨ(cid:48)wπ (cid:44) T πvπ,

(7)

where T π is the Bellman optimality operator. A way to
ﬁnd a good estimate is to force the approximate value func-
tion to be a ﬁxed point of the optimality equation under
the Bellman operator, i.e., T π ˆvπ ≈ ˆvπ. (Lagoudakis &
Parr, 2003). For that, the ﬁxed point has to lie in the
space of approximate value functions, spanned by the ba-
sis functions Ψ. ˆvπ lies in that space by deﬁnition, but
T π ˆvπ may have an orthogonal component and must be pro-
jected. This is achieved by the orthogonal projection oper-
ator (Ψ(Ψ(cid:62)Ψ)−1Ψ(cid:62)). Therefore the approximate value
function ˆvπ must be invariant under one application of the
Bellman operator T π followed by orthogonal projection:

ˆvπ = Ψ(Ψ(cid:62)Ψ)−1Ψ(cid:62)(T π ˆvπ).

(8)

By substituting the linear approximation Ψwπ = vπ into
the above equation and some manipulations, we get a
D × D linear systems of equations Aπωπ = bπ, where
Aπ = Ψ(cid:62)(Ψ − γΨ(cid:48)) and bπ = Ψ(cid:62)rπ, and whose solution
is the ﬁtted coefﬁcients wπ. It has been shown that the esti-
mated wπ converges to the best w∗ as the available number
of samples tends to inﬁnity (Bradtke & Barto, 1996). Ap-
pendix B presents a detailed derivation.

Policy Improvement. The second part of the algorithm
implements policy improvement, i.e., getting an improved
policy π(cid:48) via one-step look-ahead as follows:

π(cid:48)(x) = argmax

E[R(x, u) + γV π(x(cid:48))].

(9)

u

LSTD(0) alternates between the policy improvement and
policy evaluation iteratively until wπ converges (Bradtke
& Barto, 1996). Alg. 1 summarizes this procedure.

Fake News Mitigation via Point Process Based Intervention

Figure 2. Empirical and theoretical second order moments of a Hawkes process, E[dNi(t) dNj(t(cid:48))] for 4 random pairs (i, j) and t(cid:48) = 0
and varying t from 0 to 2.

LSTD in Hawkes context. LSTD is particularly suitable
to the problem we are interested in.
It learns the value
function V π(x), and as such, policy improvement can be
challenging without knowing the model. Because of this,
methods that aim to learn the Q-function Qπ(x, u), such as
LSPI (Lagoudakis & Parr, 2003), are widely applied. The
downside of Q-function based methods is that they typi-
cally require more samples than learning the value func-
tion. Yet, in our setup, learning the value function is sufﬁ-
cient, by writing the action-value function as Qπ(x, u) =
E[R(x, u) + V π(x(cid:48))], and observing that the learned model
of the multivariate Hawkes process enables analytical com-
putation of the expectation (see Appendix C for details):

E[V π(x(cid:48))]
L−1
n
(cid:88)
(cid:88)

=

l=1

i=1
n
(cid:88)

i=1

wπ

ln+izk−1

M,(l−1)n+i + wπ

nL+ln+izk−1

F,(l−1)n+i

+

wiE[zk

M,i] + wnL+iE[zk

F,i] + wπ

2nL+1,

E[R(x, u)] =

1
n
E[R(x, u)] = −

(cid:62)

E[zk
M

1
n
M ](cid:62) B(cid:62)B E[zk

F ].

+

E[zk

2
n

E[zk

M ](cid:62) B(cid:62)B E[zk

F ],

% correlation

B(cid:62)B zk

M ] −

(cid:62)

E[zk
F

B(cid:62)B zk
F ]

1
n

% difference

We require much fewer samples to learn V π(x) compared
to learning an approximate Qπ(x, u), and in particular
compared to LSPI we avoid explicitly discretizing the con-
tinuous action space from which the action u is chosen.

We further remark that
the policy improvement step
ﬁnds the optimal action u at any state x by computing
E[R(x, u) + V π(x(cid:48))], where the action u to be
argmaxu
optimized appears in the calculation of both the expected
current reward and the expected value at the next state. This
optimization problem is convex under our choice of reward
functions and the form of the Hawkes conditional intensity.

After learning the optimal policy (implicit by wπ of the
linearly-approximated value function) we start at the real-
time intervention part. By observing the state we ﬁnd the
optimal intervention intensity by simply solving eq. (9).
Alg. 2 summarizes the real-time mitigation procedure.

4. Experiments
We evaluate our fake news mitigation framework by both
simulated and real-time real-world experiments and show
our approach, Least-squares Temporal Difference (LTD),
signiﬁcantly outperforms several state-of-the-art methods
and alternatives: CEC (an approximate dynamic program-
ming), OPL (an open loop optimization), CLS (a centrality
based measure), EXP (an exposure based centrality mea-
sure), and RND (the random policy). Their details are given
in appendix D. Before explaining the intervention results
we verify the theoretical second order statistics in Fig. 2
whose details can be found in appendix E. Furthermore, we
examine convergence properties and representative power
of linear features in Fig. 5 with the details postponed to
appendix F due to space limitations.

4.1. Synthetic Experiments
Setup. For all except the experiment over network size,
the networks were generated synthetically with n = 300
nodes. Endogenous intensity coefﬁcients were set as aij ∼
U[0, 0.5]. To mimic real world networks, sparsity was set to
0.02, i.e., each edge was kept with probability 0.02. The in-
ﬂuence matrix was scaled appropriately such that the spec-
tral radius is a random number smaller than one to ensure
the stability of the process. The Hawkes kernel parameter
was set to ω = 1, which means loosing roughly 63 % of in-
ﬂuence after 1 unit of time (minutes, hours, etc). Both fake
news and mitigation processes obey these network settings.
Among n nodes, we assume 20 nodes create fake news and
another 20 nodes can be incentivized (via the exogenous
intensity) to spread true news. Each stage has length of
∆T = 1. The discount factor was set to γ = 0.7. For deter-
mining features, we set L = 2 and we choose ∆f = ∆T for
simplicity. The upper bound for the intervention intensity
was chosen by αi ∼ U[0, 0.5]. The price of each person
was ck
i = 1, and the total budget at stage k was randomly
generated as Ck ∼ (n × U[0, 0.5]). 1000 randomly sam-
pled states were used for the LSTD algorithm. To evaluate
a policy (learnt by an algorithm) we simulated the network
under that policy 50 times and take the discounted total re-
ward averaged over these 50 runs as an empirical valuation
of the policy. Furthermore, each single run was simulated
for 10 consecutive stages; from the eleventh stage onward,
the objectives contribute 0.02 of the total reward and can
be safely discarded. For all experiments, the above settings
are assumed unless it is explicitly mentioned otherwise.

00.511.52time-101232nd order momenttheoryempirical00.511.52time-20242nd order momenttheoryempirical00.511.52time0122nd order momenttheoryempirical00.511.52time-0.500.511.52nd order momenttheoryempiricalFake News Mitigation via Point Process Based Intervention

Figure 3. Performance improvement of different methods over the random policy on synthetic networks for correlation maximization

(a)

(b)

(c)

(d)

Figure 4. Performance improvement of different methods over the random policy on synthetic networks for distance minimization

(a)

(b)

(c)

(d)

network size. The difference between alternative methods
and the gap between LTD and others increase with the net-
work size. Furthermore, the performance of all methods
show an increase over random policy when the problem
size gets larger. This illustrates the fact that efﬁcient distri-
bution of budget matters more when confronted with prob-
lems of increasing complexity and size.

Fig. 3-b shows the performance with respect to increas-
ing the mitigation campaign size. Larger campaigns imply
greater ﬂexibility of intervention, which can be exploited
by clever algorithms to achieve higher performance.

Fig. 3-c shows the performance with respect to increasing
sparsity of the network. Interestingly, the performance of
all the algorithms move towards to the random policy as
the network becomes denser. This can be understood by
considering a complete graph, so that no matter how and to
whom we distribute the mitigation budget, all the nodes are
exposed to the mitigation campaign almost equally. How-
ever, since real social networks are usually sparse, the ef-
fectiveness of the proposed method stands out.

Finally, Fig. 3-d shows the performance with respect to the
length of an stage. Longer stage lengths increase the po-
tential for a good policy to attain higher reward than a ran-
dom policy, and this is reﬂected by the sharp increase and
larger performance gap between LTD and others for longer
lengths. We observe the same patterns for the distance min-
imization in Fig. 4 problem and avoid repeating them.

4.2. Real experiments
In this section we explain our real-time intervention results.
To the best of our knowledge, we are the ﬁrst to employ
a real-time experiment to evaluate a point process based
social network intervention strategy.

(a) Correlation

(b) Difference

Figure 5. Convergence of linear approximated value function

Intervention results. Fig. 3 demonstrates the performance
of different methods. Performance of a policy is quanti-
ﬁed as the ratio of the total reward achieved by running the
policy, over the total reward achieved by the random pol-
icy (RND). This allows us to compare the effectiveness of
the algorithms over a variety of settings. All the results
reported are averages over 10 runs with random networks
generated according to the above setup. Overall, it is clear
that LTD is almost consistently the best. It improves over
the random policy by roughly 20 percent. CEC is the sec-
ond best and shows the effectiveness of multi-stage and
closed loop intervention. This validates our intuition that
although CEC computes the reward from both fake news
and mitigation processes, the lack of explicit features cor-
responding to previous events in its value function prevents
it from learning the reason for the reward. Roughly, OPL
is the third best algorithm, due to its negligence of the state
and the actual events that occurred. Next, comes the EXP
algorithm followed by the CLS. The poor performance of
these (compared to others) shows that structural properties
are not sufﬁcient to tackle the fake news mitigation prob-
lem. EXP is roughly better than CEC because it heuristi-
cally takes into account the fake news exposure.

Fig. 3-a shows the performance with respect to increasing

100200300400500network size11.11.2performanceLTDCECOPLEXPCLS1020304050campaign size0.911.11.2performanceLTDCECOPLEXPCLS0.040.080.120.160.2sparsity11.051.11.151.2performanceLTDCECOPLEXPCLS0.40.81.21.62stage length11.21.41.6performanceLTDCECOPLEXPCLS100200300400500network size0.850.90.951performanceLTDCECOPLEXPCLS1020304050campaign size0.70.80.911.1performanceLTDCECOPLEXPCLS0.040.080.120.160.2sparsity0.70.80.911.1performanceLTDCECOPLEXPCLS0.40.81.21.62stage length0.811.2performanceLTDCECOPLEXPCLS0200040006000# samples00.10.20.30.4Squqred Error0200040006000# samples00.10.20.30.4Squqred ErrorFake News Mitigation via Point Process Based Intervention

(a) Correlation

(b) Difference

(a) Correlation

(b) Difference

Figure 6. Results of fake news mitigation on Twitter network

Figure 7. Rank correlation for prediction

Setup. Using ﬁve Twitter accounts, each of which made
ﬁve posts on machine learning topics at random times per
day for a span of two months (Nov.-Dec. 2016), we ac-
cumulated a network of 1894 real users with 23407 di-
rected edges in total. We used this historical data to learn
the network parameters {αij, µi} using maximum likeli-
hood (similar to related work (Zhou et al., 2013; Fara-
jtabar et al., 2014)) with one hour as the time resolution
and the kernel decay parameter ω set to 0.1. As illus-
trated in Fig.1 the optimal policy was learned using LSTD
and policy improvement. Then the real-time experiment
starts: Two of the accounts, interpreted as the source of
fake news, continued to behave using the same random-
ized policy as they did in the data collection stage, while
the posting times of the other three accounts were gener-
ated from (u1, u2, u3)T , produced by our LTD strategy or
a competitor strategy. Each policy was run for 10 stages of
length 12 hours. Therefore, ∆T = ∆f = 12. Since both
fake news and mitigation accounts were tweeting random
posts on machine learning, we assume negligible bias in
the content that can confound the performance. At the end
of each stage, all retweets–by users within the network–
of the posts made during the two most recent stages were
used to construct the feature vector and compute the value
function, which was used to ﬁnd the optimal intervention
for the next stage. The methods CEC and OPL belong to
the same category, and it has been shown that CEC outper-
forms OPL in (Farajtabar et al., 2016). Furthermore, EXP
and CLS also belong to similar families and our synthetic
experiments conﬁrm the superiority of the former. So, to
save time in real interventions, we only test CEC from the
ﬁrst and EXP from the second pair, and compare them with
the random policy (RND) and with our algorithm (LTD).

Real-time intervention results. Fig. 6 shows the perfor-
mance of our results compared to competitors. The results
show that our approach outperforms the other three base-
lines by a reasonable margin. As expected CEC is the sec-
ond best algorithm with a margin of 5 for the correlation
maximization objective. It translates to increase in amount
of correlation equal to 5, which is a noticeable amount.
Furthermore, in the difference minimization task, our ap-
proach reached around 7 in difference. This means that we
decreased the difference in exposure to the two processes to
less 2.6 per user, which is considerable improvement. For
both tasks, LTD made more mitigation posts over all day-

time phases than it did over all nighttime phases, whereas
the competitor strategies did the opposite. This could be
a reason for its better performance. One surprising fact is
that the number of retweets by users outside the network,
which was not used for our features, can exceed the num-
ber of retweets by users within the network. This is because
the “hashtag” feature on Twitter allows posts to be seen by
a much larger set of users, who do not necessarily follow
the source accounts. In addition to retweets, users can also
“like” a post, indicating that they were exposed to fake or
real news; while we measured this, we did not include it in
the reward. Future experiments can use these two observa-
tions to widen the experimental scope and more accurately
measure the effectiveness of a mitigation strategy. Despite
having these limitations, our experiment serves as a proof-
of-concept for the applicability of point process based in-
tervention in networks, and–to the best of our knowledge–
is the ﬁrst to verify the superiority of a method in a real-
time, real-world intervention setting.
Prediction evaluation results. The previous part describes
the more interesting evaluation scheme of real-time inter-
vention in a social media platform. In this part, we used
historical real data to mimic this procedure. We extracted
12 full 10-stage trajectory of events from the 2-month ran-
dom policy historical data. For any of these 10 pairs, the
methods were evaluated according to how well they predict
the relative ordering among these 12 trajectories (with re-
spect to the objective function). To evaluate each method,
we created a sorted list of these 12 trajectories according
to increasing objective, and created a second list sorted
by increasing closeness to the intervention method. This
closeness is the mean squared error between the prescribed
intervention and actual intensity, which we inferred using
maximum likelihood. Then, by computing the rank corre-
lation of the two sorted lists, and repeating for each of the
ﬁve methods, we can ﬁnd out how well they perform on the
prediction task. A better predictor is expected to be a better
mitigation strategy. Fig. 7 shows the performance.

A short discussion is presented in appendix G.

Acknowledgement. This project is supported in part by
NSF IIS-1639792, CNS-1409635, NSF DMS-1620342,
NSF IIS-1218749, NIH BIGDATA 1R01GM108341,
NSF CAREER IIS-1350983, ONR N00014-15-1-2340,
NVIDIA, Intel, and Amazon AWS.

LTDCECEXPRND20304050average correlationLTDCECEXPRND40506070average difference LTDCECOPLEXPCLSRND0.40.50.60.70.8rank correlationLTDCECOPLEXPCLSRND0.40.50.60.70.8rank correlationFake News Mitigation via Point Process Based Intervention

References

Allcott, Hunt and Gentzkow, Matthew. Social media and
fake news in the 2016 election. Technical report, Na-
tional Bureau of Economic Research, 2017.

Antos, Andr´as, Szepesv´ari, Csaba, and Munos, R´emi.
Value-iteration based ﬁtted policy iteration:
learning
with a single trajectory. In Approximate Dynamic Pro-
gramming and Reinforcement Learning, 2007. ADPRL
2007. IEEE International Symposium on, pp. 330–337.
IEEE, 2007.

Bacry, Emmanuel and Muzy, Jean-Franc¸ois. Hawkes
model for price and trades high-frequency dynamics.
Quantitative Finance, 14(7):1147–1166, 2014a.

Bacry, Emmanuel and Muzy, Jean-Francois. Second order
statistics characterization of hawkes processes and non-
parametric estimation. arXiv preprint arXiv:1401.0903,
2014b.

Bertsekas, Dimitri P. Dynamic programming and optimal

control, volume 1. 1995.

Bharathi, Shishir, Kempe, David, and Salek, Mahyar. Com-
petitive inﬂuence maximization in social networks.
In
International Workshop on Web and Internet Economics,
pp. 306–311. Springer, 2007.

Blundell, Charles, Beck, Jeff, and Heller, Katherine A.
Modelling reciprocating relationships with hawkes pro-
cesses. In Advances in Neural Information Processing
Systems, pp. 2600–2608, 2012.

Bradtke, Steven J. and Barto, Andrew G.

Linear
least-squares algorithms for temporal difference learn-
ing. Machine Learning, 22(1):33–57, 1996.
ISSN
1573-0565.
URL
http://dx.doi.org/10.1007/BF00114723.

10.1007/BF00114723.

doi:

Chen, Duanbing, L¨u, Linyuan, Shang, Ming-Sheng,
Zhang, Yi-Cheng, and Zhou, Tao. Identifying inﬂuential
nodes in complex networks. Physica a: Statistical me-
chanics and its applications, 391(4):1777–1787, 2012.

De Arruda, Guilherme Ferraz, Barbieri, Andr´e Luiz,
Rodr´ıguez, Pablo Mart´ın, Rodrigues, Francisco A,
Moreno, Yamir, and da Fontoura Costa, Luciano. Role
of centrality for the identiﬁcation of inﬂuential spreaders
in complex networks. Physical Review E, 90(3):032812,
2014.

Farajtabar, Mehrdad, Du, Nan, Gomez-Rodriguez, Manuel,
Valera, Isabel, Zha, Hongyuan, and Song, Le. Shaping
In Advances in
social activity by incentivizing users.
neural information processing systems, pp. 2474–2482,
2014.

Farajtabar, Mehrdad, Wang, Yichen, Rodriguez,
Manuel Gomez, Li, Shuang, Zha, Hongyuan, and
Song, Le. Coevolve: A joint point process model for
information diffusion and network co-evolution.
In
Advances in Neural Information Processing Systems,
pp. 1954–1962, 2015.

Farajtabar, Mehrdad, Ye, Xiaojing, Harati, Sahar, Song, Le,
and Zha, Hongyuan. Multistage campaigning in social
networks. In Advances in Neural Information Processing
Systems, pp. 4718–4726, 2016.

Gao, Zhenxiang, Shi, Yan, and Chen, Shanzhi.

Identify-
ing inﬂuential nodes for efﬁcient routing in opportunistic
networks. Journal of Communications, 10(1), 2015.

Gottfried, Jeffrey and Shearer, Elisa. News Use Across So-
cial Media Platforms 2016. Pew Research Center, May
2016.

Guo, Fangjian, Blundell, Charles, Wallach, Hanna, and
Heller, Katherine. The bayesian echo chamber: Mod-
eling social inﬂuence via linguistic accommodation. In
Artiﬁcial Intelligence and Statistics, pp. 315–323, 2015.

Hawkes, Alan G. Spectra of some self-exciting and mutu-
ally exciting point processes. Biometrika, 58(1):83–90,
1971.

He, Xinran, Rekatsinas, Theodoros, Foulds, James, Getoor,
Lise, and Liu, Yan. Hawkestopic: A joint model for
network inference and topic modeling from text-based
cascades. In Proceedings of the 32nd International Con-
ference on Machine Learning (ICML-15), pp. 871–880,
2015.

Kempe, David, Kleinberg, Jon, and Tardos, ´Eva. Maxi-
mizing the spread of inﬂuence through a social network.
In Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining,
pp. 137–146. ACM, 2003.

Lagoudakis, Michail G and Parr, Ronald. Least-squares
policy iteration. Journal of machine learning research,
4(Dec):1107–1149, 2003.

Lee, Young, Lim, Kar Wai, and Ong, Cheng Soon. Hawkes
processes with stochastic excitations. In Proceedings of
The 33rd International Conference on Machine Learn-
ing, pp. 79–88, 2016.

Lian, Wenzhao, Henao, Ricardo, Rao, Vinayak, Lucas,
Joseph E, and Carin, Lawrence. A multitask point pro-
cess predictive model. In ICML, pp. 2030–2038, 2015.

Linderman, Scott W and Adams, Ryan P. Discovering la-
tent network structure in point process data. In ICML,
pp. 1413–1421, 2014.

Fake News Mitigation via Point Process Based Intervention

media messages.
ence on Web and Social Media, 2016.

In Tenth International AAAI Confer-

Zhao, Zhe, Resnick, Paul, and Mei, Qiaozhu. Enquiring
minds: Early detection of rumors in social media from
enquiry posts. In Proceedings of the 24th International
Conference on World Wide Web, pp. 1395–1405. ACM,
2015.

Zhou, Ke, Zha, Hongyuan, and Song, Le. Learning so-
cial infectivity in sparse low-rank networks using multi-
In Proceedings of the
dimensional hawkes processes.
Sixteenth International Conference on Artiﬁcial Intelli-
gence and Statistics, pp. 641–649, 2013.

Mitra, Tanushree, Wright, G, and Gilbert, Eric. A parsimo-
nious language model of social media credibility across
disparate events. In Proc. CSCW, 2017.

Mosseri, Adam. Addressing hoaxes and fake news, 2016.
URL http://newsroom.fb.com/news/2016/12/news-
feed-fyi-addressing-hoaxes-and-fake-news/.

Parikh, Ankur P, Gunawardana, Asela, and Meek, Christo-
pher. Conjoint modeling of temporal dependencies in
event streams. BMAW-12 Preface, 2012.

Qin, Zhen and Shelton, Christian R. Auxiliary gibbs sam-
pling for inference in piecewise-constant conditional in-
tensity models. In UAI, pp. 722–731, 2015.

Rizoiu, Marian-Andrei, Xie, Lexing, Sanner, Scott, Ce-
brian, Manuel, Yu, Honglin, and Van Henteryck, Pascal.
Expecting to be hip: Hawkes intensity processes for so-
cial media popularity.

Silverman, Craig.

This

analysis

shows

fake

news

election

viral
formed real news on facebook,
https://www.buzzfeed.com/craigsilverman/viral-
fake-election-news-outperformed-real-news-on-
facebook.

stories
2016.

how
outper-
URL

Simma, Aleksandr and Jordan, Michael I. Modeling events
arXiv preprint

with cascades of poisson processes.
arXiv:1203.3516, 2012.

Sutton, Richard S. and Barto, Andrew G. Introduction to
Reinforcement Learning. MIT Press, Cambridge, MA,
USA, 1st edition, 1998. ISBN 0262193981.

Twitter. Types of tweets and where they appear, 2016. URL

https://support.twitter.com/articles/119138.

Wang, Yichen, Theodorou, Evangelos, Verma, Apurv, and
Song, Le. Steering opinion dynamics in information dif-
fusion networks. CoRR, abs/1603.09021, 2016. URL
http://arxiv.org/abs/1603.09021.

Xu, Qiaofeng, Yang, Deshan, Tan, Jun, Sawatzky, Alex,
and Anastasio, Mark A. Accelerated fast iterative shrink-
age thresholding algorithms for sparsity-regularized
cone-beam ct image reconstruction. Medical Physics,
43(4):1849–1872, 2016.

Zarezade, A., Upadhyay, U., Rabiee, H., and Gomez-
Rodriguez, M. Redqueen: An online algorithm for smart
In WSDM ’17: Pro-
broadcasting in social networks.
ceedings of the 10th ACM International Conference on
Web Search and Data Mining, 2017.

Zeng, Li, Starbird, Kate, and Spiro, Emma S. # uncon-
ﬁrmed: Classifying rumor stance in crisis-related social

Fake News Mitigation via Point Process Based Intervention

A. Proof of Theorem 2

Proof. Fix node index j and t(cid:48) ≥ 0, deﬁne gji(t(cid:48), t) for all node i and t such that

gji(t(cid:48), t) dt = E (cid:2)dNi(t)| dNj(t(cid:48)) = 1(cid:3) − δijδ(t − t(cid:48)) dt − ηi(t) dt

(10)

Since the conditional intensity of Ni(t) is λi(t), we have

gji(t(cid:48), t) dt = E (cid:2)dNi(t)| dNj(t(cid:48)) = 1(cid:3) − δijδ(t − t(cid:48)) dt − ηi(t) dt
= E (cid:2)λi(t)| dNj(t(cid:48)) = 1(cid:3) dt − δijδ(t − t(cid:48)) dt − ηi(t) dt

Furthermore, we have λi(t) = µi(t) + (cid:80)n

(cid:82) t
0 φki(t − s) dNk(s) and hence

k=1

E (cid:2)λi(t)| dNj(t(cid:48)) = 1(cid:3) = µi(t) +

φki(t − s)E[dNk(s)| dNj(t(cid:48)) = 1]

n
(cid:88)

(cid:90) t

(cid:90) t

(cid:90) t

0

0

0

k=1
n
(cid:88)

k=1
n
(cid:88)

k=1

n
(cid:88)

(cid:90) t

k=1

0

= µi(t) +

φki(t − s) (cid:2)gjk(t(cid:48), s) ds + δkjδ(s − t(cid:48)) ds + ηk(s) ds(cid:3)

= µi(t) +

φki(t − s)gjk(t(cid:48), s) ds + φji(t − t(cid:48)) +

φki(t − s)ηk(s) ds

n
(cid:88)

(cid:90) t

k=1

0

where we applied the deﬁnition of gjk in (10) to obtain the second equality. Combining the two equations above and using
the fact that ηi(t) = µi(t) + (cid:80)n

(cid:82) t
0 φki(t − s)ηk(s) ds, we obtain that

k=1

gji(t(cid:48), t) =

φki(t − s)gjk(t(cid:48), s) ds + φji(t − t(cid:48)) − δijδ(t − t(cid:48))

Since j and t(cid:48) are arbitrary, we let G(t(cid:48), t) be the matrix such that the (j, i)-th entry of G(t(cid:48), t) is gji(t(cid:48), t), then we have

G(t(cid:48), t) = G(t(cid:48), t) ∗ Φ(t) + Φ(t − t(cid:48)) − δ(t − t(cid:48))I

Note that the Wiener-Hopf equation (11) determines the unique solution G(t(cid:48), t) for all t ≥ t(cid:48). Moreover, since MHP is
simple and that dNi(t) = 0 or 1 a.s. for all i, we have

E[dNi(t) dNj(t(cid:48))] = Pr (cid:0)dNi(t) = 1, dNj(t(cid:48)) = 1(cid:1)

(11)

(12)

= Pr (cid:0)dNi(t)| dNj(t(cid:48)) = 1(cid:1) Pr (cid:0)dNj(t(cid:48)) = 1(cid:1)
= E[dNi(t)| dNj(t) = 1]E[dNj(t(cid:48))]
= E[dNi(t)| dNj(t) = 1]E[λj(t(cid:48))] dt(cid:48)
= E[dNi(t)| dNj(t) = 1]ηj(t(cid:48)) dt(cid:48)
= gji(t(cid:48), t)ηj(t(cid:48)) dt dt(cid:48) + δijδ(t − t(cid:48))ηj(t(cid:48)) dt dt(cid:48) + ηi(t)ηj(t(cid:48)) dt dt(cid:48)

Similarly, we can switch i and j, and t and t(cid:48) to obtain

E[dNi(t) dNj(t(cid:48))] = gij(t, t(cid:48))ηi(t) dt dt(cid:48) + δijδ(t − t(cid:48))ηi(t) dt dt(cid:48) + ηi(t)ηj(t(cid:48)) dt dt(cid:48)

(13)

Combining (12) and (13) we have that

gji(t(cid:48), t)ηj(t(cid:48)) = gij(t, t(cid:48))ηi(t)

i.e., G(t(cid:48), t)(cid:62)Σ(t(cid:48)) = Σ(t)G(t, t(cid:48)), from which G(t(cid:48), t) for t < t(cid:48) is also uniquely determined. We therefore have

dN (t) dN (t(cid:48))(cid:62)(cid:105)
(cid:104)

E

= G(t(cid:48), t)(cid:62)Σ(t(cid:48)) dt dt(cid:48) + δ(t − t(cid:48))Σ(t(cid:48)) dt dt(cid:48) + η(t)η(t(cid:48))(cid:62) dt dt(cid:48)

(14)

This completes the proof.

Fake News Mitigation via Point Process Based Intervention

B. Details of Policy Evaluation

We seek an approximate value function ˆvπ that is invariant under one application of the Bellman operator T π followed by
orthogonal projection:

ˆvπ = Ψ(Ψ(cid:62)Ψ)−1Ψ(cid:62)(T π ˆvπ)

(15)

By replacing the linear approximation, Ψwπ = vπ , and some manipulations we get:

Ψ(Ψ(cid:62)Ψ)−1Ψ(cid:62)(rπ + γΨ(cid:48)wπ) = Ψwπ

(cid:16)

Ψ

(Ψ(cid:62)Ψ)−1Ψ(cid:62)(rπ + γΨ(cid:48)wπ) − wπ(cid:17)
(Ψ(cid:62)Ψ)−1Ψ(cid:62)(rπ + γΨ(cid:48)wπ) − wπ = 0

= 0

(Ψ(cid:62)Ψ)−1Ψ(cid:62)(rπ + γΨ(cid:48)wπ) = wπ

Ψ(cid:62)(rπ + γΨ(cid:48)wπ) = (Ψ(cid:62)Ψ)wπ
Ψ(cid:62)(Ψ − γΨ(cid:48))
(cid:124)
(cid:125)
(cid:123)(cid:122)
D×D

wπ = Ψ(cid:62)rπ
(cid:124) (cid:123)(cid:122) (cid:125)
D×1

Deﬁning Aπ = Ψ(cid:62)(Ψ − γΨ(cid:48)) and bπ = Ψ(cid:62)rπ the estimated coefﬁcients are the solution of a D × D linear systems of
equation: Aπωπ = bπ.

C. Details of Policy Improvement

Assume we are at the beginning of stage k.The expected feature vector for the next state x(cid:48) is comprised of L intervals per
process, out of which L − 1 are observed. Only the most recent interval is not observed and needs to be re-evaluated in
expectation sense. To compute E[V π(x(cid:48))] have:

E[V π(x(cid:48))] =E[

wπ

d ψd(x(cid:48))]

D
(cid:88)

d=1

(cid:88)

=E[

=

i=1...n,l=1...L
(cid:88)

i=1...n,l=1...L−1
+ wπ

2nL+1

wπ

(l−1)n+izk

M,(l−1)n+i + wπ

nL+(l−1)n+izk

F,(l−1)n+i + wπ

2nL+1]

wπ

ln+izk−1

M,(l−1)n+i + wπ

nL+ln+izk−1

F,(l−1)n+i +

wiE[zk

M,i] + wnL+iE[zk

F,i]

(cid:88)

i=1...n

Then, following (Farajtabar et al., 2016), we obtain

where

E[zk

M ] = Γ (µM + uk) + Υ yk
M
F ] = Γ µF + Υ yk
M

E[zk

Υ = (A − ωI)−1(e(A−ωI)∆ − I)
Γ = Υ + (A − ωI)−1(Υ − I∆)/ω;

To ﬁnd E[R(x, u)] for the two different reward functions we have deﬁned

• Correlation Maximization

E[R(xk, uk)] =

E[Mk(τk+1; xk, uk)(cid:62)F k(τk+1; xk, uk)] =

1
n

(cid:62)

E[zk
M

B(cid:62)B zk
F ]

=

E[zk

M ](cid:62) B(cid:62)B E[zk

F ] =

Γ(µM + uk) + Υyk
M

B(cid:62)B

ΓµF + Υyk
F

(cid:17)(cid:62)

(cid:16)

(cid:17)

(cid:16)

1
n

1
n
1
n

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

Fake News Mitigation via Point Process Based Intervention

Here the second line is due to the fact that mitigation campaign and fake news process are independent of each other
(given the network model). Note the linear dependence of the the objective on our intervention uk which combined
with linear constraints result in a convex optimization problem.

• Difference Minimization

E[R(xk, uk)] = −

(cid:13)
E[
(cid:13)Mk(τk+1; xk, uk) − F k(τk+1; xk, uk)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

]

1
n
1
n
1
n

= −

E[(Bzk

M − B zk

M − B zk

F )]

= −

(cid:62)

E[zk
B(cid:62)B zk
M ]
M
(cid:125)
(cid:123)(cid:122)
(cid:124)
Second order moments

E[zk
(cid:124)

M ](cid:62) B(cid:62)B E[zk
F ]
(cid:123)(cid:122)
(cid:125)
First order moments

−

1
n

(cid:62)

E[zk
B(cid:62)B zk
F ]
F
(cid:124)
(cid:125)
(cid:123)(cid:122)
Second order moments

F )(cid:62)(Bzk
2
n

+

The ﬁrst and second order moments are computed by (Farajtabar et al., 2014) and Theorem 2, respectively.

D. Baselines

We compared our Least-squares Temporal Difference (LTD) intervention procedure with CEC (an approximate dynamic
programming, OPL (an open loop optimization), CLS (a centrality based measure), EXP (an exposure based centrality
measure), and RND (the random policy). The details are as follows:

1) CEC (Farajtabar et al., 2016): This is a recent network intervention algorithm based on point processes. It formulates a
dynamic programming problem,

V k(x) = max
uk

E[R(xk, uk) + γV π(xk+1)],

and uses approximate look-ahead dynamic programming implemented via Certainly Equivalence Control (CEC) (Bert-
sekas, 1995) to ﬁnd the optimum intervention.

2) OPL (Farajtabar et al., 2014): An open loop dynamic programming control based on convex optimization that ﬁnds the
best intervention for all stages in one shot:

(26)

(27)

(28)

(29)

(30)

(cid:88)

E[

γkR(xk, uk)].

max
u1,u2,...,uK

k

Open Loop (OPL) is an important baseline, because comparing it against closed-loop strategies like CEC and LTD indicates
how much feedback information helps improve future decisions. It quantiﬁes the value of information in the context of
dynamic programming and optimal control.

3) CLS: For each node i belonging to the mitigation campaign, we compute its closeness centrality centi =
j dis(i,j) ,
where dis is the shortest distance from i to j. Then, we assign the budget such that ui ∝ centi, meaning that budget
is distributed to mitigation nodes based on their proximity to nodes according to network structure. Closeness Centrality
(CLS) has been widely used in the literature as a baseline for ﬁnding inﬂuential nodes (Chen et al., 2012; De Arruda et al.,
2014; Gao et al., 2015).

(cid:80)

1

4) EXP: The CLS baseline is only structural and does not use the fake news infection data. EXP augments it by computing
l=1 F k−l
an Exposure-based Closeness Centrality, centk
j
, where the numerator is the total number of times node
dis(i,j)
j has been exposed to the fake news campaign in the L intervals before stage k. The more times node j has been exposed
to the fake news, the more important it is for the mitigation campaign to reach it. EXP assigns the budget according to
i ∝ centk
uk
i .
5) RND: This policy assigns a random solution in the convex space of feasible interventions. It serves as a baseline and
improvement over this random policy makes comparison feasible across different settings.

i = (cid:80)

(cid:80)L

j

E. Empirical validation of second order statistics

In this section, we empirically study the theoretical results of section 3.2. The mean and standard deviation of the empirical
second order statistics averaged over 100 simulations was compared to the theoretical mean. This experiment helps to

Fake News Mitigation via Point Process Based Intervention

verify that it can be used in simulations to evaluate the merits of our proposed algorithm and versus the baselines. Fig. 2
demonstrates the second order correlation proﬁle of 4 random pairs of users simulated 100 times. We see that the empirical
average almost matches the theoretical average. Furthermore, it is interesting to see that the standard deviation increases
with time. This is due to the aggregation of more random elements as time passes. Therefore, one should be careful with
using the empirical mean without a sufﬁcient number of random runs when the time interval is large.

F. Linear approximation accuracy

In our LSTD algorithm we used a linear approximation for the value-function. One might ask how accurately can linear
features approximate the value-function. To this end, we take a sample state x as a state with no prior activity and intensity
ψ(x) = (0, . . . , 0, 1). This is the the initial state assumed in our experiment runs. First we empirically found V π(x)
under the learned policy by simulating the process 100 times each with 10 stages. We compare the empirical average
and the standard deviation of the total reward with the one estimated by the linear approximation ψ(cid:62)wπ. Fig. 5 shows the
results for correlation maximization and difference minimization. In both cases, by increasing the number of samples (used
in LSTD), the estimated w leads to better estimation of V π(x). First, the ﬁgure shows that we can achieve a reasonable
accuracy with a fair amount of samples. Secondly, although it appears that the approximation is converging to the empirical
value, we notice that increasing the number of samples beyond 4000 does not improve the error, which maintains a constant
distance from 0. We believe this is because the optimal value function does not lie in the linear span of the feature space.
Employing more complex features, such as polynomial features and deep neural network based representations, remain as
interesting avenues for future work.

G. Discussion

The use of point process based intervention comes with a tradeoff: on one hand, the stochastic nature of multivariate
point processes allows it to model the uncertainty of event occurrences in real-world networks; however, by adopting this
stochastic model, the intervention policy can only set the optimal conditional intensity, rather than the precise best times,
of fake news mitigation events. This can be improved in future experiments by choosing shorter time intervals for stages in
the mitigation campaign. An assumption made in the real-world experiment is that all events by user u are seen by user v
if v follows u, meaning that fake or real news events at u are seen by v. While it is true for Twitter that all tweets by u will
appear on the home timeline of v who follows u (Twitter, 2016), it is not necessarily true that a follower v will see these
tweets (suppose they did not access Twitter that day). Therefore, future experiments can improve the accuracy of reward
and performance measurements by estimating the probability of users being online during certain time intervals and seeing
tweets from accounts they follow.

It is important to note that mitigating fake news, as understood in ordinary language, is a vague and qualitative goal, and
it does not necessarily imply a reduction of fake news events. Matching the exposure of users to real and fake news is one
of many speciﬁc quantitative realizations of the qualitative goal of mitigating fake news. Therefore, exposure matching
is emphatically not an assumption of the paper; rather, it is an objective used to convert fake news mitigation to a precise
mathematical problem in the framework of point processes. One can deﬁne any objective function that deals with the
number of exposures or events from either cascades. Furthermore, the exposure itself can be interpreted in many different
ways depending on the application. Whenever there are multiple dependent event sequences and the rate function of one
or many can be controlled, then the point process framework is helpful to deﬁne objective functions based on events and
to ﬁnd an optimal policy with respect to the desired goal.

For future work we would like to incorporate more complex features, such as quadratic and nonlinear features. Utilizing
deep neural nets is also an interesting future work for modeling complex feature set.

