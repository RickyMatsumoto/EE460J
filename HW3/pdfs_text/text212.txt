Communication-efﬁcient Algorithms for
Distributed Stochastic Principal Component Analysis

Dan Garber 1 Ohad Shamir 2 Nathan Srebro 3

Abstract
We study the fundamental problem of Principal
Component Analysis in a statistical distributed
setting in which each machine out of m stores
a sample of n points sampled i.i.d. from a sin-
gle unknown distribution. We study algorithms
for estimating the leading principal component
of the population covariance matrix that are both
communication-efﬁcient and achieve estimation
error of the order of the centralized ERM so-
lution that uses all mn samples. On the nega-
tive side, we show that in contrast to results ob-
tained for distributed estimation under convex-
ity assumptions, for the PCA objective, simply
averaging the local ERM solutions cannot guar-
antee error that is consistent with the centralized
ERM. We show that this unfortunate phenomena
can be remedied by performing a simple correc-
tion step which correlates between the individual
solutions, and provides an estimator that is con-
sistent with the centralized ERM for sufﬁciently-
large n. We also introduce an iterative distributed
algorithm that is applicable in any regime of n,
which is based on distributed matrix-vector prod-
ucts. The algorithm gives signiﬁcant acceleration
in terms of communication rounds over previous
distributed algorithms, in a wide regime of pa-
rameters.

1. Introduction

Principal Component Analysis (PCA) (Pearson, 1901;
Hotelling, 1933; Jolliffe, 2002) is one of the most cele-
brated and popular techniques in data analysis and ma-

Israel

Institute

Institute of Technology, Haifa,
of Science, Rehovot,

1Technion -
2Weizmann
rael
3Toyota Technological
dence to:
Shamir <ohad.Shamir@weizmann.ac.il>, Nathan
<nati@ttic.edu>.

Is-
Israel
Illinois, USA. Correspon-
Dan Garber <dangar@technion.ac.il>, Ohad
Srebro

Institute,

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

chine learning. For data that consists of N vectors in
Rd, x1, ..., xN , with normalized covariance matrix ˆX =
N
1
i=1 xix>i , The PCA method ﬁnds the k-dimensional
N
subspace (which corresponds to the span of the top k prin-
cipal components) such that the projection of the data onto
the subspace has largest variance, i.e., it is the solution to
the optimization problem:

P

ˆXW

2
F .
k

W

(1)

2Rd

max
k ,WT W=I k
⇥
PCA is often considered in a statistical setting in which the
assumption is that the input vectors are not arbitrary but
sampled i.i.d. from some ﬁxed but unknown distribution
with certain general characteristics
. Then, it is often of
interest to use the observed sample to estimate the top k
principal components of the population covariance matrix,
rather then that of the sample, which leads to the modiﬁed
optimization problem:

D

max
k ,WT W=I k
⇥

Ex

⇠D

xx>

W

2
F .
k

(2)

W

2Rd

⇥

⇤

Of course the empirical estimation problem (1) and the
population estimation problem (2) are well connected, and
it is well-known that under mild assumptions on the dis-
tribution
and given a sufﬁciently large sample, we can
guarantee small estimation error in (2) by solving optimiza-
tion problem (1).

D

D

In this work we consider the problem of estimating the ﬁrst
principal component (i.e., k = 1) in a statistical and dis-
tributed setting. We assume the availability of m machines,
each of which stores a sample of n vectors sampled i.i.d
over Rd, and we are interested
from a ﬁxed distribution
in algorithms that can be applied efﬁciently to solve Prob-
lem (2) for k = 1, with estimation error that approaches
that of a centralized algorithm, which has access to all mn
samples and does not pay for communication between ma-
chines. Indeed, when considering the efﬁciency of algo-
rithms, we will mainly focus on the amount of communi-
cation between machines they require, since this is often
the most expensive resource in distributed computing. We
note that the i.i.d. assumption is standard in many appli-
cations of PCA, and can be leveraged to get more efﬁcient
algorithms than when the data partition is arbitrary. Also,
we will make a standard assumption that the population co-
variance matrix has a non-zero additive gap between the

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

ﬁrst and second eigenvalues, which makes the problem of
estimating the leading principal component meaningful.

A main challenge that often arises in many computational
settings of principal components is that it leads to in-
herently non-convex optimization problems. While many
times these problems turn out to admit efﬁcient algorithms,
the rich toolbox of optimization and statistical estimation
procedures developed for convex problems often cannot be
directly applied to problems such as (1) and (2). Instead,
one often needs to consider a specialized and more involved
analysis, to get analogous convergence results for the PCA
problem. This for instance was the case in a recent wave
of results that applied concepts such as stochastic gradi-
ent updates (Balsubramani et al., 2013; Shamir, 2016a; Jain
et al., 2016b; Allen Zhu & Li, 2016b) and variance reduc-
tion (Shamir, 2015; 2016c; Garber & Hazan, 2015; Garber
et al., 2016; Allen Zhu & Li, 2016a) to the PCA problem.
This is also the case in our distributed setting. For instance,
(Zhang et al., 2013) proposed communication-efﬁcient al-
gorithms for a distributed statistical estimation settings,
similar to ours, but under convexity assumptions. The au-
thors show that under their assumptions, in a wide regime
of parameters (namely when the per-machine sample size
n is large enough), then a simple averaging of the empirical
risk minimizers (ERM), computed locally on each machine,
leads to estimation error of the population parameters of
the order the centralized ERM solution. While averaging
makes perfect sense in a convex setting, it is clear that it
can completely fail in a non-convex setting.
Indeed, we
show that already for the PCA problem with k = 1, simply
averaging the local ERM solutions (and normalizing to ob-
tain a unit vector as required), cannot improve signiﬁcantly
over the estimation error of any single machine. We then
show that a simple ﬁx to the above scheme, namely cor-
relating the directions of individual ERM solutions, reme-
dies this phenomena and results in estimation error similar
to that of the centralized ERM solution. Much like the re-
sults of (Zhang et al., 2013), this result only holds in the
regime when the per-machine sample size n is sufﬁciently
large. As discussed, due to the inherent non-convexity of
the PCA objective, this approach requires a novel analysis
tailored to the PCA problem. In this context, we view this
work as an initiation of a research effort to understand how
to efﬁciently aggregate statistical estimators in a distributed
non-convex setting.

A second line of results for distributed estimation under
convexity assumptions consider iterative algorithms that
perform multiple communication rounds and are based on
distributed gradient computations (some examples include
(Shamir et al., 2014; Zhang & Lin, 2015; Lee et al., 2015;
Shamir, 2016b; Jaggi et al., 2014; Reddi et al., 2016)). The
beneﬁt of these methods is that (a) they provide meaningful
estimation error guarantees in a much wider regime of pa-

rameters than the “one-shot” aggregation methods (namely
in terms of the number of samples per machine), and (b),
due to their iterative nature, they allow to approximate the
centralized ERM solution arbitrary well. Unfortunately,
these methods, all of which rely heavily on convexity as-
sumption, cannot be directly applied to the PCA problem.
Towards designing efﬁcient distributed iterative methods
for our PCA setting, we consider the application of the
recently proposed method of Shift-and-Invert power itera-
tions (S&I) for PCA (Garber & Hazan, 2015; Garber et al.,
2016). The S&I method reduces the problem of computing
the leading eigenvector of a real positive semideﬁnite ma-
trix to that of approximately solving a small number (i.e.
poly-logarithmic in the problem parameters) of systems of
linear equations. These in turn, could be efﬁciently solved
by arbitrary distributed convex solvers. We show that cou-
pling the S&I method with the stochastic pre-conditioning
technique for linear systems proposed in (Zhang & Lin,
2015) and well known fast gradient methods such as the
conjugate gradient method, gives state-of-the-art guaran-
tees in terms of communication costs, and provides a sig-
niﬁcant improvement over distributed variants of classical
fast eigenvector algorithms such as power iterations and the
faster Lanczos algorithm. Much like its convex counter-
parts, which only rely on distributed gradient computations
and simple vector aggregations, our iterative method only
relies on distributed matrix-vector products, i.e., it requires
each machine to only send products of its local empirical
covariance matrix with some input vector.

Beyond the results described so far, (Liang et al., 2014;
Boutsidis et al., 2016) studied distributed algorithms for
PCA in a deterministic setting in which the partition of
the data across machines is arbitrary and communication
is measured in terms of number of transmitted bits. The
approximation guarantees provided in these works are in
terms of the projection of the data onto the leading princi-
pal components (instead of alignment between the estimate
and the optimal solution, studied in this paper). Applying
these results to our setting will give a number of communi-
1), where ✏ is the
cation rounds that scales like poly(✏ 
desired error and   is the population eigengap. In our set-
ting, ✏ will scale with the inverse of the size of the sample,
1, which for these algorithms will result in
i.e., ✏
amount of communication that is polynomial in the size of
the data. In contrast, we will be interested in algorithms
whose communication costs does not scale with n at all. In
this context we note that, by focusing on algorithms that
either perform simple aggregation of local ERM solutions,
or perform only distributed matrix-vector products with the
empirical covariance matrix, we can circumvent the need to
measure communication explicitly in terms of the number
of bits transmitted, which often burdens the analysis of nat-
ural algorithms, such as those proposed here.

(mn) 

1  

⇡

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

2. Preliminaries

2.1. Notation and problem setting
We write vectors in Rd in boldface lower-case letters (e.g.,
v), matrices in boldface upper-case letters (e.g., X), and
scalars are written as lightface letters (e.g., c). We let

k·
denote the standard Euclidean norm for vectors and the

k
spectral norm for matrices.

D

D

⇠D

We consider the following statistical distributed setting.
be a distribution over vectors in Rd with squared
Let
`2 norm at most b, for some b > 0. We consider a setting
in which m machines, numbered 1...m, are each given a
. We let v1 denote
dataset of n samples drawn i.i.d. from
a leading eigenvector of the population covariance matrix
X = Ex
[xx>]. Our goal is to efﬁciently (mainly in
terms of communication) ﬁnd an estimate w for v1, i.e., a
unit vector that maximizes the product (v>1 w)2 with high
probability. Towards this end, we assume that the popula-
tion covariance matrix X has a non-zero eigengap  , i.e.,
  :=  1(X)
) denotes the ith
 2(X) > 0, where  i(
largest eigenvalue of a symmetric real matrix. Note that
 > 0 is necessary for v1 to be uniquely deﬁned (up to
sign).
In addition, we let ˆXi denote the empirical covariance ma-
[m],
trix of the sample stored on machine i for every i
2
1 ...x(i)
j x(i)
i.e., ˆXi = 1
n are the
>j
n
samples stored on machine i. We let ˆX denote the em-
pirical covariance matrix of the union of points across all
ˆXi.
machines i.e., ˆX = 1
m

, where x(i)

j=1 x(i)

m
i=1

P

 

n

·

P

Our model of communication assumes that the m machines
work in rounds during which a central machine (w.l.o.g.
machine 1) can send a single vector in Rd to all other ma-
chines, or every machine can send either the leading eigen-
vector of its local empirical covariance matrix, or the prod-
uct of a single input vector with its local covariance, to ma-
chine 1. We will measure communication complexity in
terms of number of such rounds required to achieve a cer-
tain estimation error.

2.1.1. THE CENTRALIZED SOLUTION

Our primary benchmark for measuring performance will be
the centralized empirical risk minimizer which is the lead-
ing eigenvector of the aggregated empirical covariance ma-
trix ˆX.

The following standard result bounds the error of the cen-
tralized ERM.

(0, 1). Sup-
Lemma 1 (Risk of centralized ERM). Fix p
2
pose that  > 0 and let ˆv1 denote the leading eigenvector
=1 v> ˆXv. Then it holds w.p.
of ˆX, i.e., ˆv1 2
at least 1

arg maxv:

p that

v

k

k

 

(v>1 ˆv1)2

1

 



✏ERM(p) :=

32b2 ln(d/p)
mn 2

.

(3)

Lemma 1 is a direct consequence of the following stan-
dard concentration argument for random matrices, and the
Davis-Kahan sin(✓) theorem (whose proof is given in the
appendix for completeness):
Theorem 1 (Matrix Hoeffding, see (Tropp, 2012)). Let
D
be a distribution over vectors with squared `2 norm at most
n
[xx>]. Let ˆX = 1
b, and let X = Ex
i=1 xix>i ,
n
where x1, ..., xn are sampled i.i.d. from
. Then, it holds
✏2n
ˆX
d
that
16b2
k

k  
⇣
Theorem 2 (Davis-Kahan sin(✓) theorem). Let X, Y be
d matrices with leading eigenvectors vX
symmetric real d
and vY respetively. Also, suppose that  (X) :=  1(X)
 
2
X
Y
 2(X) > 0. Then it holds that 1
.
k
 
 (X)2

✏> 0 : Pr

v>XvY

P
 

D
exp

2 k

⇠D

X

 



⇥

⌘

⇣

⌘

8

✏

2

·

.

 

 



 

2.2. Informal statement of main results and previous

algorithms

We now informally describe our main results, followed by a
detailed description of previous approaches that are directly
applicable to our setting. The algorithmic results (both new
and old) are summarized in Table 1.

2.2.1. MAIN RESULTS

Failure of simple averaging of local ERM solutions We
show that a natural approach of simply averaging the in-
dividual leading eigenvectors of the empirical covariance
matrices ˆXi (and normalizing the obtain a unit vector)
cannot signiﬁcantly improve (beyond logarithmic factors)
over the performance of any of the individual eigenvectors.
More concretely, if we let ˆv(i)
1 denote the leading eigen-
vector of ˆXi for any i
[m], and we denote their average
2
i=1 ˆv(i)
by ¯v1 = 1
m
D
over vectors with magnitude O(1) and covariance eigen-
gap   = 1, such that

1 , then there exists a distribution

P

m

1

8

m, n : E

¯v>1 v1
¯v1k ◆
k
See Theorem 3 in Section 3 for the complete and formal
argument.

D "

=⌦

1
n

 

✓

✓

◆

#

,

2

A successful single communication round algorithm via
correlation of individual ERM solutions We show that
if prior to averaging the local ERM solutions, as suggested
above, we correlate their directions by aligning them ac-
cording to any single machine (say machine number 1),
1 )ˆv(i)
ˆv(1)
i=1 sign(ˆv(i)
i.e., we let ¯v1 = 1
1 , then this
>1
m
p,
(0, 1), w.p. at least 1
guarantees that for any p
2
P
b2 ln

b4 ln2

m

2

1

 

✓

¯v>1 v1
¯v1k ◆

k

= O

0

@

dm
p
⇣
 2mn

+

⌘

 
dm
p
 4n2 1

⇣

⌘

A

.

(4)

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

Method
Centralized ERM
Distributed Power Method
Distributed Lanczos
“Hot-potato” SGD

Average of ERMs with sign-ﬁxing (Theorem 4)

Distributed Shift&Invert + precond. linear systems (Theorem 6)

1

(w>v1)2 w.p. 3/4

 
✏ERM =⇥( b2 ln d
 2mn )
(1 + o(1))
✏ERM ·
(1 + o(1))
✏ERM ·

O(✏ERM)

O(✏ERM) + O
✏ERM ·

b4 ln2 d
 4n2
⇣
(1 + o(1))

# communcation rounds
-
˜O( 1/ )
˜O(
 1/ )
m

p
1

⌘

˜O(min

(b/ )1/2n 

1/4, m1/4

{

)

}

Table 1. Comparison of estimation error and number of communication rounds. For simplicity we ﬁx the failure probability to p = 1/4
and assume mn is in the regime in which Lemma 1 is meaningful, i.e, mn =⌦( b2  
) suppresses logarithmic factors
in b, d, 1/p, 1/✏ERM. For the result of Theorem 4 we assume the regime m = O(d). The sub-constant o(1) factors could be made, in
principle, arbitrary small in all relevant results by trading approximation with communication.

2 ln d). The ˜O(

·

See Theorem 4 in Section 3 for the complete and formal
result.

In particular, in the likely scenario when m = O(d/p)
=
we have that w.p. at least 1
 
✏ERM(p))
, where ✏ERM(p)) is de-
✏ERM(p)
ﬁned in Eq. (3). Another related interpretation of the re-
sults is that the bound in Eq. (4) is comparable with ✏ERM
(up to poly-log factors) when n =⌦
.

¯v1k
¯v>1 v1/
k

2b2m ln(dm/p)

1 + m2

p, 1

 

O

  

 

 

 

 

2

·

·

We also show a matching lower bound that the bound in
 
Eq. (4) is tight (up to poly-log factors) for this aggregation
method.

 

A multi communication round algorithm We present a
distributed algorithm based on the Shift-and-Invert frame-
work for leading eigenvector computation (Garber &
Hazan, 2015; Garber et al., 2016) which is applied to ex-
plicitly solving the centralized ERM problem. We show
(0, 1), when mn =⌦( b2 ln(d/p)/ 2) (i.e.,
that for any p
when Lemma 3 is meaningful), the algorithm produces a
solution w such that w.p. at least 1

p,

2

(v>1 w)2

1

 



✏ERM(p))

·

 
(1 + o(1)) ,

(5)

1/2n 

where ✏ERM(p)) is deﬁned in Eq. (3). The algorithm per-
forms overall ˜O(pb  
1/4) distributed matrix-vector
products with the centralized empirical covariance matrix
ˆX 1. The ˜O(
) notation hides poly-logarithmic factors in
1/p, 1/ , d, 1/✏ERM(p). See Theorem 6 in Section 4 for the
complete and formal result.

·

We note that in particular, under our assumption that mn =
˜⌦(b2/ 2), it holds that the number of distributed matrix-
vector products is upper bounded by ˜O(m1/4). Moreover,
2), we can see that the number
in the regime n =⌦( b2  
of distributed matrix-vector products depends only poly-
logarithmically on the problem parameters.

In general, the sub-constant o(1) factor in (5) could be
made arbitrarily small by trading the approximation error

1i.e., on each round, each machine i sends the product of an
d with its local covariance matrix ˆXi.

input vector in R

with the number of distributed matrix-vector products.

2.2.2. PREVIOUS ALGORITHMS

Distributed versions of classical iterative algorithms:
Classical fast iterative algorithms for computing the lead-
ing eigenvector of a positive semideﬁnite matrix, such as
the well-known Power Method and the Lanczos Algo-
rithm, require iterative multiplications of the input ma-
trix ( ˆX in our case) with the current estimate.
It is thus
straightforward to implement these algorithms in our dis-
tributed setting, by multiplying the same vector with the
covariance matrices at each machine, and averaging the
result. Thus, by well-known convergence guarantees of
these two methods, we will have that for a ﬁxed ✏> 0,
these methods produce a unit vector w such that, for any
p
p, af-
ter O(ˆ 1ˆ  
1 ln(d/p✏)) rounds for the Power Method and
ˆ 1ˆ  
1 ln(d/p✏)) for the Lanczos Algorithm, where
O(
ˆ 1, ˆ  denote the leading eigenvalue and eigengap of ˆX,
respectively. Moreover, in the regime of mn in which
Lemma 1 is meaningful, we can replace ˆ 1, ˆ  with  1, 
in the above bounds, and the result will still hold with high
probability.

✏ w.p. at least 1

(w> ˆv1)2

(0, 1), 1

p

 



 

2

Simple calculations show that in the regime of mn in which
Lemma 1 is meaningful, it holds that our Shift-and-Invert-
based algorithm outperforms distributed Lanczos (in terms
of worst-case guarantees) whenever n = ˜⌦(b2/ 2

1).

“Hot potato” SGD: Another straightforward approach is
to apply a sequential algorithm for direct risk minimiza-
tion that can process the data-points one by one, such as
stochastic gradient descent (SGD), by passing its state from
one machine to the next, after completing a full pass over
the machine’s data. Clearly, this process of making a full
pass over the data of a certain machine before sending the
ﬁnal estimate to the next one, requires overall m commu-
nication rounds in order to make a full pass over all mn
points. SGD for PCA was studied in several results in
recent years (Balsubramani et al., 2013; Shamir, 2016a;c;

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

Jain et al., 2016a; Allen Zhu & Li, 2016b). For instance
applying the result of (Jain et al., 2016a) in this way will
result in a ﬁnal estimate w satisfying

(w>v1)2 = O

1

 

b2 ln d
 2mn

✓

◆

w.p. at least 3/4.

(6)

We note that in the regime in which the bound in (6) is
meaningful it holds that the number of communication
rounds of our Shift-and-Invert-based algorithm is upper-
bounded by ˜O(m1/4) which for sufﬁciently large m domi-
nates the communication complexity of SGD.

3. Single Communication Round Algorithms

via ERM on Each Machine

In this section we consider distributed algorithms that re-
quire only a single round of communication. Naturally for
this regime, all algorithms will be based on aggregating the
ERM solutions of the individual machines, i.e., each ma-
chine i only sends the leading eigenvector of its empiri-
cal covariance matrix ˆXi to a centralized machine (without
loss of generality, machine 1) which it turn combines them
to a single unit vector in some manner.

3.1. Simple averaging of eigenvectors fail

Perhaps the simplest method to aggregate the individual
eigenvectors of each machine is to average them, and then
normalize to obtain a unit vector. For instance, in the
distributed statistical setting considered in (Zhang et al.,
2013), in which the objective is strongly convex, it was
shown that simply averaging the individual ERM solutions
leads, in a meaningful regime of parameters, to estimation
error of the order of the centralized ERM solution. How-
ever, here we show that for PCA, in which the objective is
certainly not convex, this approach fails practically in any
regime, in the sense that the error of the returned aggre-
gated solution can be no better than that returned by any
single machine.

Theorem 3. There exists a distribution over vectors in R2
with `2 norm bounded by a universal constant for which the
eigengap in the covariance matrix is 1 (i.e.,   = 1), such
that if each machine i returns an estimate ˆv(i)
1 which is
an unbiased leading eigenvector of ˆXi (i.e., both outcomes
1 are equally likely), then the aggregated vector
i=1 ˆv(i)
m

1 , +ˆv(i)
ˆv(i)
 
¯v1 = 1
m

satisﬁes

1

P
m, n : E

8

1

"

 

¯v1
¯v1k

⌧

k

2

#

 

, v1

=⌦(1

/n).

The proof is given in the appendix.

3.2. Averaging with Sign Fixing

 

p that
dm
p

.

⌘

1

A

As evident from the statement of Theorem 3, an important
assumption is that each machine produces an unbiased es-
timate, in the sense that the sign of the outcome is uniform
and independent of the other machines. This hints that cor-
relating the signs of the different estimates can circumvent
the lower bound result in Theorem 3. It turns out that this
is indeed the case, as captured by the following theorem:
Theorem 4. Let ˜wi be the leading eigenvector of ˆXi for
any i

2

w =

[m], and consider the unit vector
m
i=1 sign( ˜w>i ˜w1) ˜wi
m
i=1 sign( ˜w>i ˜w1) ˜wik
P
k
(0, 1), it holds w.p. at least 1
P
b4 log2
dm
p
⇣
 2mn

b2 log

+

2

0

⌘

.

(v>1 w)2 = O

⇣
 4n2

1

 

Then, for any p

(7)

@

For ease of presentation, throughout the rest of this section
we denote the correlated vector ˆwi = sign( ˜w>i ˜w1) ˜wi for
any i

[m].

2

The main step towards proving Theorem 4 is to consider
each ˆwi as an approximately unbiased perturbation of the
true leading eigenvector v1 and to upper bound the magni-
tude of this perturbation. This is carried out in the follow-
ing much more general and self-contained lemma, which
might be of independent interest. The proof is given in the
appendix.
Lemma 2. Let A be a positive semideﬁnite matrix with
some ﬁxed leading eigenvector v1, a leading eigenvalue  1
 2(A) > 0. Let ˆA be some
and an eigengap   :=  1(A)
 
 /4.
positive semideﬁnite matrix such that
Then there is a unique leading eigenvector ˆv1 of ˆA such
that

0, and

k 

ˆA

ˆv1, v

A

 

k

i  
v1  
denotes the pseudo-inverse, and c is a positive nu-

A)†( ˆA

ˆA
k

A)v1

( 1I

 
 2

A

 

 



k

c

2

,

h
ˆv1  
 
where
 
 
merical constant.

†

 
 
 

Lemma 2 is central to the proof of the following Lemma,
of which the proof of Theorem 4 is an easy consequence.
We defer the proof of both the Lemma and that of Theorem
4 to the appendix.
Lemma 3. The following two conditions hold with proba-
 2n/cb2), for some numerical
bility at least 1
p
 
constants c, c0 > 0:

d exp(

 

 

•

•

 

 2( ˆXi) > 0.
there
1, . . . , ˆvi

The leading eigenvalue of every ˆXi is simple, i.e.,
 1( ˆXi)
Fixing v1,
vectors ˆvi
maxi k
v1

leading eigen-
such that
m
i=1 ˆvi

ˆX1, . . . , ˆXm,
1
1
4 , and
m

m of
v1k 

ˆvi
1  
c0

exist unique

b2 log(2dm/p)
 2n

 
b2 log(2dm/p)
P
 
 2mn
 

1  

+



.

q

⌘

⇣

 
 
 

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

3.3. Lower Bound for Sign Fixing

We now show that the result of Theorem 4 is tight up to
poly-logarithmic factors and cannot be improved in gen-
eral:
(0, 1) and d > 1, there exist a
Theorem 5. For any  
distribution over vectors in Rd (of norm at most a universal
constant) with eigengap   in the covariance matrix, such
that for any number of machines m and for per-machine
sample size any n sufﬁciently larger than 1/ 2, the aggre-
i=1 ˆv(i)
gated vector ¯v1 = 1
1 (even after sign ﬁxing with
m
the population eigenvector v1) satisﬁes

2

m

2

P
, e1

1

E

¯v1
¯v1k
The proof is given in the appendix.

=⌦

 

⌧

 

"

#

k

✓

1
 2mn

+

1
 4n2

◆

4. A Multi-round Algorithm based on

Shift-and-Invert Iterations

In this section we move on to consider distributed algo-
rithms that perform multiple communication rounds. The
main motivation, beyond improving some poly-logarithmic
factors in the estimation error, is to obtain a result that does
not require the per-machine sample size n to grow with the
number of machines m, as in the result of Theorem 4.

Towards this end we consider the use of the Shift-and-
Invert meta-algorithm, originally described in (Garber &
Hazan, 2015; Garber et al., 2016), to explicitly solve the
centralized ERM objective, i.e., ﬁnd a unit vector that is an
approximate solution to maxv:
Throughout this section we let ˆ 1, ˆ  denote the leading
eigenvalue and eigengap of ˆX, respectively. Also, we as-
sume without loss of generality that b = 1 (i.e., all data
points lie in the unit Euclidean ball).

=1 v> ˆXv.

v

k

k

Since our approach is to approximate the population risk
by approximating the empirical risk, we state the follow-
ing simple lemma for completeness (a proof is given in the
appendix).
Lemma 4 (Risk of approximated-ERM for PCA). Let w
be a unit vector such that (w> ˆv1)2
✏, for some ﬁxed
✏> 0, where ˆv1 is the leading eigenvector of ˆX. Then it
1
holds that 1

(w> ˆv1)2 + p2✏.

(w>v1)2

 

 

1

 



 

4.1. The Shift-and-Invert meta-algorithm

The Shift-and-Invert algorithm (Garber & Hazan, 2015;
Garber et al., 2016) efﬁciently reduces the problem of
computing the leading eigenvector of a positive semidef-
inite matrix ˆX to that of approximately-solving a poly-
logarithmic number of linear systems, i.e., ﬁnding approx-
imate minimizers of convex quadratic optimization prob-
lems of the form

F ,w(z) :=

z>( I

ˆX)z

 

z>w

,

}

 

(8)

1
2

min
2Rd{
z

where  >  1( ˆX) is a shifting parameter. The algorithm is
essentially based on applying power iterations to a shifted
ˆX) 
1, where the shifting param-
and inverted matrix ( I
eter   is carefully chosen. The algorithm that implements
this reduction, originally described in (Garber & Hazan,
2015), is given below (see Algorithm 1).

 

Algorithm 1 SHIFT-AND-INVERT POWER METHOD
1: Input: estimate ˜  for the gap ˆ , accuracy ✏

(0, 1),

2

failure probability p

144d/p2

 

8 ln

 
1
16
1 + ˜ ,
n

2: Set: m1   d
3: Set: ˜✏
min
4: Set:  (0)  
5: repeat
s
6:
for t = 1...m1 do
7:
8:

 

⇣

s + 1 , Ms  

˜ /8
⌘
ˆw0  
( (s

3
2 ln

18d
p2✏

m2+1
⇣

e

⌘

e
m1+1
 

, m2   d
˜ /8
, ✏
4
⌘

⇣

o
random unit vector, s

0

 

1)I

 

 

ˆX)

1), ˆwt

 

 

1 (z)

Find approx. minimizer - ˆwt of F (s
M 
such that

1
s ˆwt

 

k

˜✏

1k 

ˆwt  
end for
9:
ˆwm1 k
ˆwm1 /
10: ws  
k
Find approx. minimizer - vs of F (s
11:
M 
vs  
s wsk 
that
k
1
1
12:  s  
2 ·
w>s vs 
˜ 
13: until  s 
14:  (f )  
15: for t = 1...m2 do
16:

˜✏
˜✏ ,  (s)  

 (s) , Mf  

( (f )I

ˆX)

 (s

 

 

1

Find approx. minimizer - ˆwt of F (f ), ˆwt
that

M 

˜✏

 

 s
2

1)  

ˆwt  
k
17: end for
18: Return: wf  

1
f ˆwt

 

1k 
ˆwm2 k
ˆwm2 /
k

1),ws (z) such

 

1 (z) such

2

 

Lemma 5 (Efﬁcient reduction of top eigenvector to con-
vex optimization; originally Theorem 4.2 in (Garber &
Hazan, 2015)). Suppose that ˆ  :=  1( ˆX)
 2( ˆX) > 0
and suppose that the estimate ˜  in Algorithm 1 satisﬁes
˜ 
[ˆ /2, 3ˆ /4]. Then, with probability at least 1
p,
 
Algorithm 1 ﬁnds a unit vector wf such that (w>f ˆv1)2
 
✏, and the total number of optimization problems of
1
the form (8) solved during the run of the algorithm, is up-
ln(d/p) ln(ˆ  
per bounded by O
. More-
over, throughout the run of the algorithm it holds that
1 + ˆ 

⇣
ˆ 1 =⌦( ˆ ).

1) + ln

⌘⌘

d
p✏

 

⇣

 (s)  

 

the purpose of the repeat-until loop in Algo-
Remark:
rithm 1 is to efﬁciently ﬁnd a shifting parameter  (f ) such
c2ˆ  for some universal constants
that c1ˆ 
ˆ 1 
2 ln(d/p)),
c2 > c1 > 0. When n satisﬁes n =⌦(
we can directly ﬁnd (w.h.p) such a shifting parameter, by

 (f )  

  



Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

simply estimating ˆ 1, ˆ  from the data of a single machine.
Also, we can take ˆw0 to be the leading eigenvector of any
single machine, since this will already have a constant cor-
relation with ˆv1. Thus, for such n, the total number of
1)).
optimization problems can be reduced to O(ln(p 

1✏ 

1/2

1/2 ˜y, then it holds that

if we let ˜z := C 
ˆ 1) 
C1/2M 
˜y
( 
k
p
(0, 1), if we set µ = 4
holds with probability at least 1
p
depends only on the randomness in ˆX1.

 
2

1w

 

 

k

 

.
k

k 
In particular, for any
ln(d/p)/n, then the above
p, where this probability

M 

1w

˜z

Algorithm 1 is a meta-algorithm in the sense that the choice
of solver for the optimization problems min F ,w is un-
speciﬁed, and any solver will do. A simple calculation
shows that a naive application of either the conjugate gra-
dient method or Nesterov’s accelerated gradient method to
solve these optimization problems in a distributed manner,
i.e., the computation of the gradient vector is distributed
across machines, will require overall ˜O
commu-
nication rounds, which does not give any improvement over
the distributed Lanczos approach, described in Subsection
2.2.2. However, this can be substantially improved by tak-
ing advantage of the fact that the data on all machines is
sampled i.i.d.
In particular,
from the same distribution.
we present below an approach based on applying a pre-
conditioner to the optimization Problem (8), in the spirit of
the one described in (Zhang & Lin, 2015).

ˆ 1/ˆ 

 q

 

4.2. Faster Distributed Approximation of Linear

Systems via Local Preconditioning

 

ˆX, for some shift parameter  > ˆ 1, and de-
Let M =  I
ˆX1, where
ﬁne the pre-conditioning matrix C = ( +µ)I
µ is required so C is invertible. Consider now solving the
following modiﬁed quadratic problem:
˜F ,w(y) :=

1/2MC 

1/2w.

y>C 

y>C 

1/2y

(9)

 

1
2

 

Note that if y⇤ is the optimal solution to Problem (9), i.e.,

y⇤ = C1/2M 

1C1/2C 

1/2w = C1/2M 

1w,

then z⇤ := C 

1/2y⇤ is the optimal solution to Problem (8).

The idea behind choosing C this way is very intuitive. Ide-
ally we could have chosen C = M, making the condi-
tion number of ˜F ,w equal to ( ˜F ,w) = 1, which is the
best we can hope for. The problem of course is that this
1/2, which is more
requires us to explicitly compute M 
challenging then just computing the leading eigenvector of
ˆX. The next best thing is thus to choose C based only
on the data available on any single machine, which allows
1/2 without additional communication over-
computing C 
head, and leads to the choice described above. The follow-
ing lemma, rephrased from (Zhang & Lin, 2015), quantiﬁes
exactly how such a choice of C helps in improving the con-
dition number of the new optimization problem, Problem
(9). The proof is given in the appendix.
Lemma 6. Suppose that µ
is 1-smooth and
lar, ( ˜F ,w)

. Then, ˜F ,w(y)
-strongly convex. In particu-
ˆ 1). Moreover, ﬁxing ˜y
Rd,
⌘

 
⇣
1 + 2µ/( 

ˆ 1
 
 
ˆ 1)+2µ

ˆX1k

  k

ˆX

 

( 



 

2

4.2.1. SOLVING THE PRE-CONDITIONED LINEAR

SYSTEMS

We now discuss the application of gradient-based algo-
rithms for ﬁnding an approximate minimizer of the pre-
conditioned problem, Problem (9), in our distributed set-
ting. Towards this end we require a distributed implemen-
tation for the ﬁrst-order oracle of ˜F ,w(y) (i.e., computa-
tion of the value and gradient vector at a queried point).

A straight-forward implementation of the ﬁrst-order oracle
in our distributed setting is given in Algorithm 2.

Algorithm 2 Distributed First-Order Oracle for ˜F ,w(y)
1: Input: shift parameter  > 0, regularization parameter

for C :=

µ > 0, vector w

(  + µ)I

2
2: send ˜y := C 
ˆX1 {
3: for i = 1...m do
send ˜
4:
machine i

Rd, query vector y

1/2y to machines

Rd
2
2, . . . , m
{

}

executed on machine 1
}

 
ri := ˆXi ˜y to machine 1

executed on each
{

}
:= 1
m

5: end for
6: aggregate ˜
r
7: compute ˜F ,w(y) = 1
P
1/2w

y>C 
8: compute

m
˜
ri {
i=1
2 ( y>C 

executed on machine 1
y>C 

1y

)

1/2 ˜
r

}
 

 
executed on machine 1
}
1/2 ˜
C 

{
˜F ,w(y) =  C 

1y

 

r  

C 

1/2w

r

executed on machine 1
}
{
˜F ,w(y))

9: return: ( ˜F ,w(y),

r

We have the following lemma, the proof of which is de-
ferred to the appendix.
Lemma 7. Fix some  >  1( ˆX) and w
1
following two-step algorithm:

Rd, and let
µ > 0 be as in Lemma 6. Fix ✏> 0. Consider the

 

2

1. Apply either the conjugate gradient method or Nes-
terov’s accelerated method with the distributed ﬁrst-
Rd
order oracle described in Algorithm 2 to ﬁnd ˜y
such that ˜F ,w(˜y)
miny

2. Return ˜z = C 

 
1/2 ˜y.
ˆ 1) it holds that
1 + 2µ
ˆ 1
 
 
˜z
✏, and the total number dis-
k
tributed matrix-vector products with the empirical covari-
ance matrix ˆX required to compute ˜z is upper-bounded by

Then, for ✏0 = ✏
2
ˆX1) 
1w
⇣

2Rd ˜F ,w(y)

k 

( I

( 



 

 

 

✏0

2

⌘

 

1

O

1 + 2µ( 

ˆ 1) 

1 ln

1 +

 

✓q

⇣⇣

2µ

ˆ 1

 

 

⌘

w

/[( 

k

k

 

ˆ 1)✏]

.

⌘◆

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

4.3. Putting it all together

We now state our main result for this section, which is a
simple consequence of the previous lemmas. The full proof
is given in the appendix.
(0, 1). Suppose that
Theorem 6. Fix ✏
mn =⌦(   
ln(3d/p)/n. Apply-
ing the Shift-and-Invert algorithm, Algorithm 1, with the
parameters ✏, p/3, and applying the algorithm in Lemma
7 with the parameter µ, to approximately solve the linear
p a unit vector
systems, yields with probability at least 1
wf such that (w>f ˆv1)2

(0, 1) and p
2
2 ln(d/p)). Set µ = 4

 
✏, after executing at most

p

2

1

ln(d/p)
 pn

ln

"

 

 
d
p✏2

ln

✓

◆

  p

ln(d/p)
 2pn !

O

0

s p

@

+ ln2

d
p✏2

ln

1
 

✓

◆

✓

◆ ◆

= ˜O

1
 pn !

 s

distributed matrix-vector products with the empirical co-
variance matrix ˆX.

5. Experiments

To validate some of our theoretical ﬁndings we con-
ducted experiments with single-round algorithms on syn-
thetic data. We generated synthetic datasets using two dis-
tributions. For both distributions we used the covariance
matrix X = U⌃U> with U being a random d
d or-
thonormal matrix and ⌃ is diagonal satisfying: ⌃(1, 1) =
1, ⌃(2, 2) = 0.8,
1),
i.e.,   = 0.2. One dataset was generated according to the
(0, X), and for the second datasets
normal distributions
3/2X1/2y where
we generated samples by taking x =
y

1, 1]. In both cases we set d = 300.

3 : ⌃(j, j) = 0.9

⌃(j

1, j

U [

N

⇥

 

 

 

8

j

·

⇠

 

p

Beyond the single-round algorithms that are based on
aggregating the individual ERM solutions described so
far, we propose an additional natural aggregation ap-
proach, based on aggregating the individual projection ma-
m
i=1 denote the lead-
trices. More concretely, letting
ing eigenvectors of the individual machines, let ¯P1 :=
1
. We then take the ﬁnal estimate w to
m
be the leading eigenvector of the aggregated matrix ¯P1.
Note that as with the sign-ﬁxing based aggregation, this
approach also resolves the sign-ambiguity in the estimates
produced by the different machines, which circumvents the
lower bound result of Theorem 3.

ˆv(i)
1 }
{

1 ˆv(i)
>1

i=1 ˆv(i)

P

m

 

For both datasets we ﬁxed the number of machines to
m = 25. We tested the estimation error (i.e., the value
(w>v1)2 where v1 is the leading eigenvector of X and
1
w is the estimator) of ﬁve benchmarks vs. the per-machine
the centralized solution ˆv1, the average
sample size n:
of the individual (unbiased) ERM solutions (normalized to
unit norm),the average of ERM solutions with sign-ﬁxing,
and the leading eigenvector of the averaged projection ma-

trix. We also plotted the average loss of the individual ERM
solutions. Results are averaged over 400 independent runs.

The results for the normal distribution appear in Figure 1.
The results for the uniform-based distribution are very sim-
ilar and are deferred to the appendix. We can see that, as
our lower bound in Theorem 3 suggests, simply averaging
and normalizing the individual ERM solutions has signif-
icantly worse performance than the centralized ERM so-
lution. Perhaps surprisingly, the performance of this esti-
mator is even worse than the average error of an estimate
computed using only a single machine. We see that both
aggregation methods that are based on correlating the in-
dividual ERM solutions, namely the sign-ﬁxing-based esti-
mator, and the proposed averaging-of-projections heuristic,
are asymptotically consistent with the centralized ERM.
In particular, the averaging-of-projections scheme, at least
empirically, signiﬁcantly outperforms the sign-ﬁxing ap-
proach, which justiﬁes further theoretical investigation of
this heuristic. For the sign ﬁxing approach, we can see that
as suggested by our bounds, the estimator is not consistent
with the centralized ERM solution for small values of n.

centralized ERM
avg. of ERMs
sign-fix avg. of ERMs.
projection avg.
avg. machine loss

r
o
r
r
e
 
.
g
v
a

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

100

200

300
n
Figure 1. Estimation error vs. the per-machine sample size n for
a normal distribution.

400

500

600

6. Discussion

We presented communication-efﬁcient algorithms for dis-
tributed statistical estimation of principal components. Fo-
cusing on our results for methods based on a single commu-
nication round, we initiated a study of how to correctly ag-
gregate distributed ERM solutions in a non-convex setting.
An important take-home message of our work is that in a
non-convex setting, simply averaging the local solutions is
not a good idea. On the positive side, we show that a very
simple correction (i.e., sign-ﬁxing) is possible by leverag-
ing the speciﬁc structure of the problem at hand. It is thus
interesting to develop a richer theory of how to perform
such aggregations in more involved non-convex problems.

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

References

Eigenvalues

and

eigenvectors

of

2x2 matrices.

http://www.math.harvard.edu/archive/
21b_fall_04/exhibits/2dmatrices/.

Allen Zhu, Zeyuan and Li, Yuanzhi. Even faster SVD de-
In Advances
composition yet without agonizing pain.
in Neural Information Processing Systems 29: Annual
Conference on Neural Information Processing Systems
2016, December 5-10, 2016, Barcelona, Spain, pp. 974–
982, 2016a.

Allen Zhu, Zeyuan and Li, Yuanzhi. Fast global conver-
gence of online PCA. CoRR, abs/1607.07837, 2016b.

Balsubramani, Akshay, Dasgupta, Sanjoy, and Freund,
Yoav. The fast convergence of incremental PCA.
In
Advances in Neural Information Processing Systems 26:
27th Annual Conference on Neural Information Process-
ing Systems 2013, pp. 3174–3182, 2013.

Boutsidis, Christos, Woodruff, David P, and Zhong, Peilin.
Optimal principal component analysis in distributed and
In Proceedings of the 48th Annual
streaming models.
ACM SIGACT Symposium on Theory of Computing, pp.
236–249. ACM, 2016.

Garber, Dan and Hazan, Elad. Fast and simple pca via
convex optimization. arXiv preprint arXiv:1509.05647,
2015.

Garber, Dan, Hazan, Elad, Jin, Chi, Kakade, Sham M.,
Musco, Cameron, Netrapalli, Praneeth, and Sidford,
Aaron. Faster eigenvector computation via shift-and-
invert preconditioning. CoRR, abs/1605.08754, 2016.

Golub, Gene H and Pereyra, Victor. The differentiation
of pseudo-inverses and nonlinear least squares problems
whose variables separate. SIAM Journal on numerical
analysis, 10(2):413–432, 1973.

Hotelling, H. Analysis of a complex of statistical variables
into principal components. J. Educ. Psych., 24, 1933.

Jaggi, Martin, Smith, Virginia, Tak´ac, Martin, Terhorst,
Jonathan, Krishnan, Sanjay, Hofmann, Thomas, and Jor-
dan, Michael I. Communication-efﬁcient distributed
dual coordinate ascent. In Advances in Neural Informa-
tion Processing Systems, pp. 3068–3076, 2014.

Jain, Prateek, Jin, Chi, Kakade, Sham M, Netrapalli, Pra-
neeth, and Sidford, Aaron. Matching matrix bern-
stein with little memory: Near-optimal ﬁnite sam-
arXiv preprint
ple guarantees for oja’s algorithm.
arXiv:1602.06929, 2016a.

Jain, Prateek, Jin, Chi, Kakade, Sham M, Netrapalli, Pra-
neeth, and Sidford, Aaron. Matching matrix bern-
stein with little memory: Near-optimal ﬁnite sam-
arXiv preprint
ple guarantees for oja’s algorithm.
arXiv:1602.06929, 2016b.

Jolliffe, IT. Principal component analysis. 2002. Spring-

verlag, New York, 2002.

Lee, Jason D., Ma, Tengyu, and Lin, Qihang. Distributed
stochastic variance reduced gradient methods. CoRR,
abs/1507.07595, 2015.

Liang, Yingyu, Balcan, Maria-Florina F, Kanchanapally,
Improved distributed

Vandana, and Woodruff, David.
principal component analysis. In NIPS, 2014.

Magnus, Jan R. On differentiating eigenvalues and eigen-
vectors. Econometric Theory, 1(02):179–191, 1985.

Pearson, K. On lines and planes of closest ﬁt to systems of
points in space. Philosophical Magazine, 2(6):559–572,
1901.

Reddi, Sashank J., Konecn´y, Jakub, Richt´arik, Peter,
P´oczos, Barnab´as, and Smola, Alexander J. AIDE:
fast and communication efﬁcient distributed optimiza-
tion. CoRR, abs/1608.06879, 2016.

Shamir, Ohad. A stochastic PCA and SVD algorithm with
an exponential convergence rate. In Proceedings of the
32nd International Conference on Machine Learning,
ICML 2015, Lille, France, 6-11 July 2015, pp. 144–152,
2015.

Shamir, Ohad. Convergence of stochastic gradient descent
for PCA:. In Proceedings of the 33nd International Con-
ference on Machine Learning, ICML 2016, New York
City, NY, USA, June 19-24, 2016, pp. 257–265, 2016a.

Shamir, Ohad. Without-replacement sampling for stochas-
tic gradient methods. In Advances in Neural Information
Processing Systems 29: Annual Conference on Neural
Information Processing Systems 2016, December 5-10,
2016, Barcelona, Spain, pp. 46–54, 2016b.

Shamir, Ohad. Fast stochastic algorithms for svd and pca:
Convergence properties and convexity. In Proceedings of
The 33rd International Conference on Machine Learn-
ing, pp. 248–256, 2016c.

Shamir, Ohad, Srebro, Nathan,

and Zhang, Tong.
Communication-efﬁcient distributed optimization using
an approximate newton-type method. In Proceedings of
the 31th International Conference on Machine Learning,
ICML 2014, Beijing, China, 21-26 June 2014, pp. 1000–
1008, 2014.

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

Tropp, Joel A. User-friendly tail bounds for sums of ran-
dom matrices. Foundations of Computational Mathe-
matics, 12(4):389–434, 2012.

Yu, Yi, Wang, Tengyao, and Samworth, Richard J. A use-
ful variant of the davis–kahan theorem for statisticians.
Biometrika, 102(2):315–323, 2015.

Zhang, Yuchen and Lin, Xiao. Disco: Distributed opti-
mization for self-concordant empirical loss. In Proceed-
ings of the 32nd International Conference on Machine
Learning (ICML-15), pp. 362–370, 2015.

Zhang, Yuchen, Duchi, John C, and Wainwright, Martin J.
Communication-efﬁcient algorithms for statistical opti-
mization. Journal of Machine Learning Research, 14:
3321–3363, 2013.

