Recovery Guarantees for One-hidden-layer Neural Networks∗

Kai Zhong 1 Zhao Song 2 Prateek Jain 3 Peter L. Bartlett 4 Inderjit S. Dhillon 5

Abstract

1. Introduction

In this paper, we consider regression problems
with one-hidden-layer neural networks (1NNs).
We distill some properties of activation func-
tions that lead to local strong convexity in the
neighborhood of the ground-truth parameters for
the 1NN squared-loss objective and most popu-
lar nonlinear activation functions satisfy the dis-
tilled properties, including rectiﬁed linear units
(ReLUs), leaky ReLUs, squared ReLUs and sig-
moids. For activation functions that are also
smooth, we show local linear convergence guar-
antees of gradient descent under a resampling
rule. For homogeneous activations, we show ten-
sor methods are able to initialize the parameters
to fall into the local strong convexity region. As
a result, tensor initialization followed by gradient
descent is guaranteed to recover the ground truth
with sample complexity d · log(1/(cid:15)) · poly(k, λ)
and computational complexity n · d · poly(k, λ)
for smooth homogeneous activations with high
probability, where d is the dimension of the in-
put, k (k ≤ d) is the number of hidden nodes,
λ is a conditioning property of the ground-truth
parameter matrix between the input layer and the
hidden layer, (cid:15) is the targeted precision and n is
the number of samples. To the best of our knowl-
edge, this is the ﬁrst work that provides recov-
ery guarantees for 1NNs with both sample com-
plexity and computational complexity linear in
the input dimension and logarithmic in the preci-
sion.

1The University of Texas at Austin, zhongkai@ices.utexas.edu

2The University of Texas at Austin, zhaos@utexas.edu
3Microsoft Research, India, prajain@microsoft.com
4University of California, Berkeley, bartlett@cs.berkeley.edu
5The University of Texas at Austin, inderjit@cs.utexas.edu
∗Full
version
pdf/1706.03175.
<zhongkai@ices.utexas.edu>.

at https://arxiv.org/
Kai Zhong

Correspondence

available

to:

is

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Neural Networks (NNs) have achieved great practical suc-
cess recently. Many theoretical contributions have been
made very recently to understand the extraordinary perfor-
mance of NNs. The remarkable results of NNs on com-
plex tasks in computer vision and natural language process-
ing inspired works on the expressive power of NNs (Co-
hen et al., 2016; Cohen & Shashua, 2016; Raghu et al.,
2016; Daniely et al., 2016; Poole et al., 2016; Montufar
et al., 2014; Telgarsky, 2016). Indeed, several works found
NNs are very powerful and the deeper the more powerful.
However, due to the high non-convexity of NNs, knowing
the expressivity of NNs doesn’t guarantee that the targeted
functions will be learned. Therefore, several other works
focused on the achievability of global optima. Many of
them considered the over-parameterized setting, where the
global optima or local minima close to the global optima
will be achieved when the number of parameters is large
enough, including (Freeman & Bruna, 2016; Haeffele &
Vidal, 2015; Livni et al., 2014; Dauphin et al., 2014; Safran
& Shamir, 2016; Hardt & Ma, 2017). This, however, leads
to overﬁtting easily and can’t provide any generalization
guarantees, which are actually the essential goal in most
tasks.

A few works have considered generalization performance.
For example,
(Xie et al., 2017) provide generaliza-
tion bound under the Rademacher generalization analysis
framework. Recently (Zhang et al., 2017a) describe some
experiments showing that NNs are complex enough that
they actually memorize the training data but still general-
ize well. As they claim, this cannot be explained by ap-
plying generalization analysis techniques, like VC dimen-
sion and Rademacher complexity, to classiﬁcation loss (al-
though it does not rule out a margins analysis—see, for ex-
ample, (Bartlett, 1998); their experiments involve the un-
bounded cross-entropy loss).

In this paper, we don’t develop a new generalization analy-
sis. Instead we focus on parameter recovery setting, where
we assume there are underlying ground-truth parameters
and we provide recovery guarantees for the ground-truth
parameters up to equivalent permutations. Since the param-
eters are exactly recovered, the generalization performance
will also be guaranteed.

Recovery Guarantees for One-hidden-layer Neural Network

Several other techniques are also provided to recover the
parameters or to guarantee generalization performance,
such as tensor methods (Janzamin et al., 2015) and kernel
methods (Arora et al., 2017). These methods require sam-
ple complexity O(d3) or computational complexity (cid:101)O(n2),
which can be intractable in practice.

Recently (Shamir, 2016) show that neither speciﬁc assump-
tions on the niceness of the input distribution or niceness of
the target function alone is sufﬁcient to guarantee learnabil-
ity using gradient-based methods. In this paper, we assume
data points are sampled from Gaussian distribution and the
parameters of hidden neurons are linearly independent.

Our main contributions are as follows,

1. We distill some properties for activation functions,
which are satisﬁed by a wide range of activations, including
ReLU, squared ReLU, sigmoid and tanh. With these prop-
erties we show positive deﬁniteness (PD) of the Hessian
in the neighborhood of the ground-truth parameters given
enough samples (Theorem 4.2). Further, for activations that
are also smooth, we show local linear convergence is guar-
anteed using gradient descent.

2. We propose a tensor method to initialize the parameters
such that the initialized parameters fall into the local posi-
tive deﬁniteness area. Our contribution is that we reduce
the sample/computational complexity from cubic depen-
dency on dimension to linear dependency (Theorem 5.6).

3. Combining the above two results, we provide a globally
converging algorithm (Algorithm 2) for smooth homoge-
neous activations satisfying the distilled properties. The
whole procedure requires sample/computational complex-
ity linear in dimension and logarithmic in precision (Theo-
rem 6.1).

2. Related Work

The recent empirical success of NNs has boosted their the-
oretical analyses (Feng et al., 2016; Balduzzi, 2016; Bal-
duzzi et al., 2016; Sagun et al., 2016; Andoni et al., 2014;
Arora et al., 2017; Goel et al., 2017).
In this paper, we
classify them into three main directions.

2.1. Expressive Power

Expressive power is studied to understand the remarkable
performance of neural networks on complex tasks. Al-
though one-hidden-layer neural networks with sufﬁciently
many hidden nodes can approximate any continuous func-
tion (Hornik, 1991), shallow networks can’t achieve the
same performance in practice as deep networks. Theoreti-
cally, several recent works show the depth of NNs plays an
essential role in the expressive power of neural networks
(Daniely et al., 2016). As shown in (Cohen et al., 2016; Co-

hen & Shashua, 2016; Telgarsky, 2016), functions that can
be implemented by a deep network of polynomial size re-
quire exponential size in order to be implemented by a shal-
low network. (Raghu et al., 2016; Poole et al., 2016; Mont-
ufar et al., 2014; Arora et al., 2017) design some measures
of expressivity that display an exponential dependence on
the depth of the network. However, the increasing of the
expressivity of NNs or its depth also increases the difﬁculty
of the learning process to achieve a good enough model. In
this paper, we focus on 1NNs and provide recovery guar-
antees using a ﬁnite number of samples.

2.2. Achievability of Global Optima

The global convergence is in general not guaranteed for
NNs due to their non-convexity. It is widely believed that
training deep models using gradient-based methods works
so well because the error surface either has no local min-
ima, or if they exist they need to be close in value to the
global minima. (Swirszcz et al., 2016) present examples
showing that for this to be true additional assumptions on
the data, initialization schemes and/or the model classes
have to be made. Indeed the achievability of global optima
has been shown under many different types of assumptions.

In particular, (Choromanska et al., 2015) analyze the loss
surface of a special random neural network through spin-
glass theory and show that it has exponentially many local
optima, whose loss is small and close to that of a global
optimum. Later on, (Kawaguchi, 2016) eliminate some as-
sumptions made by (Choromanska et al., 2015) but still
require the independence of activations as (Choromanska
et al., 2015), which is unrealistic. (Safran & Shamir, 2016)
study the geometric structure of the neural network objec-
tive function. They have shown that with high probabil-
ity random initialization will fall into a basin with a small
objective value when the network is over-parameterized.
(Livni et al., 2014) consider polynomial networks where
the activations are square functions, which are typically not
used in practice. (Haeffele & Vidal, 2015) show that when
a local minimum has zero parameters related to a hidden
node, a global optimum is achieved. (Freeman & Bruna,
2016) study the landscape of 1NN in terms of topology and
geometry, and show that the level set becomes connected
as the network is increasingly over-parameterized. (Hardt
& Ma, 2017) show that products of matrices don’t have
spurious local minima and that deep residual networks can
represent any function on a sample, as long as the num-
ber of parameters is larger than the sample size. (Soudry
& Carmon, 2016) consider over-speciﬁed NNs, where the
number of samples is smaller than the number of weights.
(Dauphin et al., 2014) propose a new approach to second-
order optimization that identiﬁes and attacks the saddle
point problem in high-dimensional non-convex optimiza-
tion. They apply the approach to recurrent neural networks

Recovery Guarantees for One-hidden-layer Neural Network

and show practical performance. (Arora et al., 2017) use
results from tropical geometry to show global optimality of
an algorithm, but it requires (2n)k poly(n) computational
complexity.

Almost all of these results require the number of param-
eters is larger than the number of points, which probably
overﬁts the model and no generalization performance will
be guaranteed. In this paper, we propose an efﬁcient and
provable algorithm for 1NNs that can achieve the underly-
ing ground-truth parameters.

2.3. Generalization Bound / Recovery Guarantees

The achievability of global optima of the objective from
the training data doesn’t guarantee the learned model to be
able to generalize well on unseen testing data. In the lit-
erature, we ﬁnd three main approaches to generalization
guarantees.

1) Use generalization analysis frameworks, including VC
dimension/Rademacher complexity, to bound the general-
ization error. A few works have studied the generalization
(Xie et al., 2017) follow (Soudry
performance for NNs.
& Carmon, 2016) but additionally provide generalization
bounds using Rademacher complexity. They assume the
obtained parameters are in a regularization set so that the
generalization performance is guaranteed, but this assump-
tion can’t be justiﬁed theoretically. (Hardt et al., 2016) ap-
ply stability analysis to the generalization analysis of SGD
for convex and non-convex problems, arguing early stop-
ping is important for generalization performance.

2) Assume an underlying model and try to recover this
model. This direction is popular for many non-convex
problems including matrix completion/sensing (Jain et al.,
2013; Hardt, 2014; Sun & Luo, 2015; Balcan et al., 2017),
mixed linear regression (Zhong et al., 2016), subspace re-
covery (Elhamifar & Vidal, 2009) and other latent models
(Anandkumar et al., 2014).

Without making any assumptions, those non-convex prob-
lems are intractable (Arora et al., 2012a; Gillis & Vavasis,
2015; Song et al., 2017a; Gillis & Glineur, 2011; Razen-
shteyn et al., 2016; Sontag & Roy, 2011; Hardt & Moitra,
2013; Arora et al., 2012b; Yi et al., 2014). Recovery guar-
antees for NNs also need assumptions. Several different ap-
proaches under different assumptions are provided to have
recovery guarantees on different NN settings.

Tensor methods (Anandkumar et al., 2014; Wang et al.,
2015; Wang & Anandkumar, 2016; Song et al., 2016) are
a general tool for recovering models with latent factors
by assuming the data distribution is known. Some exist-
ing recovery guarantees for NNs are provided by tensor
methods (Sedghi & Anandkumar, 2015; Janzamin et al.,
2015). However, (Sedghi & Anandkumar, 2015) only pro-

vide guarantees to recover the subspace spanned by the
weight matrix and no sample complexity is given, while
(Janzamin et al., 2015) require O(d3/(cid:15)2) sample complex-
ity. In this paper, we use tensor methods as an initialization
step so that we don’t need very accurate estimation of the
moments, which enables us to reduce the total sample com-
plexity from 1/(cid:15)2 to log(1/(cid:15)).

(Arora et al., 2014) provide polynomial sample complex-
ity and computational complexity bounds for learning deep
representations in unsupervised setting, and they need to
assume the weights are sparse and randomly distributed in
[−1, 1].

(Tian, 2017) analyze 1NN by assuming Gaussian inputs in
a supervised setting, in particular, regression and classiﬁca-
tion with a teacher. This paper also considers this setting.
However, there are some key differences. a) (Tian, 2017)
require the second-layer parameters are all ones, while we
can learn these parameters. b) In (Tian, 2017), the ground-
truth ﬁrst-layer weight vectors are required to be orthogo-
nal, while we only require linear independence. c) (Tian,
2017) require a good initialization but doesn’t provide ini-
tialization methods, while we show the parameters can be
efﬁciently initialized by tensor methods. d) In (Tian, 2017),
only the population case (inﬁnite sample size) is consid-
ered, so there is no sample complexity analysis, while we
show ﬁnite sample complexity.

Recovery guarantees for convolution neural network with
Gaussian inputs are provided in (Brutzkus & Globerson,
2017), where they show a globally converging guarantee of
gradient descent on a one-hidden-layer no-overlap convo-
lution neural network. However, they consider population
case, so no sample complexity is provided. Also their anal-
ysis depends on ReLU activations and the no-overlap case
is very unlikely to be used in practice. In this paper, we
consider a large range of activation functions, but for one-
hidden-layer fully-connected NNs.

3) Improper Learning. In the improper learning setting for
NNs, the learning algorithm is not restricted to output a
NN, but only should output a prediction function whose er-
ror is not much larger than the error of the best NN among
(Zhang et al., 2016a;b) propose
all the NNs considered.
kernel methods to learn the prediction function which is
guaranteed to have generalization performance close to that
of the NN. However, the sample complexity and compu-
tational complexity are exponential.
(Aslan et al., 2014)
transform NNs to convex semi-deﬁnite programming. The
works by (Bach, 2014) and (Bengio et al., 2005) are also
in this direction. However, these methods are actually
not learning the original NNs. Another work by (Zhang
et al., 2017b) uses random initializations to achieve arbi-
trary small excess risk. However, their algorithm has expo-
nential running time in 1/(cid:15).

Recovery Guarantees for One-hidden-layer Neural Network

Roadmap. The paper is organized as follows.
In Sec-
tion 3, we present our problem setting and show three key
properties of activations required for our guarantees.
In
Section 4, we introduce the formal theorem of local strong
convexity and show local linear convergence for smooth
activations. Section 5 presents a tensor method to initialize
the parameters so that they fall into the basin of the local
strong convexity region.

It is well-known that, training one hidden layer neural net-
work is NP-complete (Blum & Rivest, 1988). Thus, with-
out making any assumptions, learning deep neural network
is intractable. Throughout the paper, we assume x follows
a standard normal distribution; the data is noiseless; the
dimension of input data is at least the number of hidden
nodes; and activation function φ(z) satisﬁes some reason-
able properties.

2.4. Notation

For any positive integer n, we use [n] to denote the set
{1, 2, · · · , n}. For random variable X, let E[X] denote the
expectation of X (if this quantity exists). For any vector
x ∈ Rn, we use (cid:107)x(cid:107) to denote its (cid:96)2 norm. We provide
several deﬁnitions related to matrix A. Let det(A) denote
the determinant of a square matrix A. Let A(cid:62) denote the
transpose of A. Let A† denote the Moore-Penrose pseu-
doinverse of A. Let A−1 denote the inverse of a full rank
square matrix. Let (cid:107)A(cid:107)F denote the Frobenius norm of ma-
trix A. Let (cid:107)A(cid:107) denote the spectral norm of matrix A. Let
σi(A) to denote the i-th largest singular value of A. For any
function f , we deﬁne (cid:101)O(f ) to be f ·logO(1)(f ). In addition
to O(·) notation, for two functions f, g, we use the short-
hand f (cid:46) g (resp. (cid:38)) to indicate that f ≤ Cg (resp. ≥)
for an absolute constant C. We use ⊗ to denote outer prod-
uct and · to denote dot product. Given two column vectors
u, v ∈ Rn, then u ⊗ v ∈ Rn×n and (u ⊗ v)i,j = ui · vj,
and u(cid:62)v = (cid:80)n
i=1 uivi ∈ R. Given three column vec-
tors u, v, w ∈ Rn, then u ⊗ v ⊗ w ∈ Rn×n×n and
(u ⊗ v ⊗ w)i,j,k = ui · vj · wk. We use u⊗r ∈ Rnr
to
denote the vector u’s outer product with itself r − 1 times.

3. Problem Formulation

We consider the following regression problem. Given a set
of n samples

S = {(x1, y1), (x2, y2), · · · (xn, yn)} ⊂ Rd × R,

let D denote a underlying distribution over Rd × R with
parameters

{w∗

1, w∗

2, · · · w∗

k} ⊂ Rd, and {v∗

1, v∗

2, · · · , v∗

k} ⊂ R

such that each sample (x, y) ∈ S is sampled i.i.d. from this
distribution, with

D :

x ∼ N (0, I), y =

i · φ(w∗(cid:62)
v∗

i x),

(1)

k
(cid:88)

i=1

where φ(z) is the activation function, k is the number of
nodes in the hidden layer. The main question we want to
answer is: How many samples are sufﬁcient to recover the
underlying parameters?

Actually our results can be easily extended to multivariate
Gaussian distribution with positive deﬁnite covariance and
zero mean since we can estimate the covariance ﬁrst and
then transform the input to a standard normal distribution
but with some loss of accuracy. Although this paper fo-
cuses on the regression problem, we can transform classi-
ﬁcation problems to regression problems if a good teacher
is provided as described in (Tian, 2017). Our analysis re-
quires k to be no greater than d, since the ﬁrst-layer param-
eters will be linearly dependent otherwise.

1(σ) − α2

2(σ), α0(σ) · α2(σ) − α2

For activation function φ(z), we assume it is continuous
and if it is non-smooth let its ﬁrst derivative be left deriva-
tive. Furthermore, we assume it satisﬁes Property 3.1, 3.2,
and 3.3. These properties are critical for the later analyses.
We also observe that most activation functions actually sat-
isfy these three properties.
Property 3.1. The ﬁrst derivative φ(cid:48)(z) is nonnegative and
homogeneously bounded, i.e., 0 ≤ φ(cid:48)(z) ≤ L1|z|p for
some constants L1 > 0 and p ≥ 0.
Property 3.2. Let αq(σ) = Ez∼N (0,1)[φ(cid:48)(σ · z)zq], ∀q ∈
{0, 1, 2}, and βq(σ) = Ez∼N (0,1)[φ(cid:48)2(σ · z)zq], ∀q ∈
{0, 2}. Let ρ(σ) denote min{β0(σ) − α2
0(σ) −
α2
1(σ), β2(σ) − α2
1(σ)}
The ﬁrst derivative φ(cid:48)(z) satisﬁes that, for all σ > 0, we
have ρ(σ) > 0.
Property 3.3. The second derivative φ(cid:48)(cid:48)(z) is either (a)
globally bounded |φ(cid:48)(cid:48)(z)| ≤ L2 for some constant L2, i.e.,
φ(z) is L2-smooth, or (b) φ(cid:48)(cid:48)(z) = 0 except for e (e is a
ﬁnite constant) points.
Remark 3.4. The ﬁrst two properties are related to the
ﬁrst derivative φ(cid:48)(z) and the last one is about the second
derivative φ(cid:48)(cid:48)(z). At high level, Property 3.1 requires φ
to be non-decreasing with homogeneously bounded deriva-
tive; Property 3.2 requires φ to be highly non-linear; Prop-
erty 3.3 requires φ to be either smooth or piece-wise linear.
Theorem 3.5. ReLU φ(z) = max{z, 0},
leaky
ReLU φ(z) = max{z, 0.01z}, squared ReLU φ(z) =
max{z, 0}2 and any non-linear non-decreasing smooth
functions with bounded symmetric φ(cid:48)(z), like the sigmoid
function φ(z) = 1/(1+e−z), the tanh function and the erf
function φ(z) = (cid:82) z
dt, satisfy Property 3.1,3.2,3.3.
The linear function, φ(z) = z, doesn’t satisfy Property 3.2
and the quadratic function, φ(z) = z2, doesn’t satisfy
Property 3.1 and 3.2.

0 e−t2

Recovery Guarantees for One-hidden-layer Neural Network

The proof can be found in the full version (Zhong et al.,
2017).

If Property 3.3(b) is satisﬁed, φ(cid:48)(cid:48)(z) = 0 almost surely. So
in this case the diagonal blocks of the empirical Hessian
can be written as,

4. Positive Deﬁniteness of Hessian

In this section, we study the Hessian of empirical risk near
the ground truth. We consider the case when v∗ is already
known. Note that for homogeneous activations, we can as-
i ∈ {−1, 1} since vφ(z) = v
sume v∗
|v| φ(|v|1/pz), where
p is the degree of homogeneity. As v∗
i only takes discrete
values for homogeneous activations, in the next section, we
show we can exactly recover v∗ using tensor methods with
ﬁnite samples.

For a set of samples S, we deﬁne the Empirical Risk,

(cid:98)fS(W ) =

1
2|S|

(cid:88)

(cid:32) k

(cid:88)

(x,y)∈S

i=1

(cid:33)2

i φ(w(cid:62)
v∗

i x) − y

.

(2)

For a distribution D, we deﬁne the Expected Risk,

fD(W ) =

1
2

E
(x,y)∼D

(cid:32) k

(cid:88)





i=1

i φ(w(cid:62)
v∗

i x) − y

(3)

(cid:33)2
 .

Let’s calculate the gradient and the Hessian of (cid:98)fS(W ) and
fD(W ). For each j ∈ [k], the partial gradient of fD(W )
with respect to wj can be represented as

∂fD(W )
∂wj

= E

(x,y)∼D

(cid:34)(cid:32) k

(cid:88)

i=1

(cid:33)

(cid:35)

i φ(w(cid:62)
v∗

i x) − y

j φ(cid:48)(w(cid:62)
v∗

j x)x

.

For each j, l ∈ [k] and j (cid:54)= l, the second partial derivative
of fD(W ) for the (j, l)-th off-diagonal block is,

∂2fD(W )
∂wj∂wl

= E

(x,y)∼D

(cid:2)v∗

j v∗

l φ(cid:48)(w(cid:62)

j x)φ(cid:48)(w(cid:62)

l x)xx(cid:62)(cid:3) ,

and for each j ∈ [k], the second partial derivative of
fD(W ) for the j-th diagonal block is

∂2fD(W )
∂w2
j

= E

(x,y)∼D

k
(cid:88)

[(

i=1
j φ(cid:48)(w(cid:62)

+ (v∗

j x))2xx(cid:62)].

i φ(w(cid:62)
v∗

i x) − y)v∗

j φ(cid:48)(cid:48)(w(cid:62)

j x)xx(cid:62)

If φ(z) is non-smooth, we use the Dirac function and its
derivatives to represent φ(cid:48)(cid:48)(z). Replacing the expectation
E(x,y)∼D by the average over the samples |S|−1 (cid:80)
(x,y)∈S,
we obtain the Hessian of the empirical risk.
Considering the case when W = W ∗ ∈ Rd×k, for all j, l ∈
[k], we have,

∂2fD(W ∗)
∂wj∂wl

= E

(x,y)∼D

(cid:2)v∗

j v∗

l φ(cid:48)(w∗(cid:62)

j x)φ(cid:48)(w∗(cid:62)

l x)xx(cid:62)(cid:3) .

∂2 (cid:98)fS(W )
∂w2
j

=

1
|S|

(cid:88)

(x,y)∈S

(v∗

j φ(cid:48)(w(cid:62)

j x))2xx(cid:62).

Now we show the Hessian of the objective near the global
optimum is positive deﬁnite.
Deﬁnition 4.1. Given the ground truth matrix W ∗ ∈
Rd×k,
let σi(W ∗) denote the i-th singular value of
W ∗, often abbreviated as σi. Let κ = σ1/σk, λ =
((cid:81)k
k . Let vmax denote maxi∈[k] |v∗
i=1 σi)/σk
i | and vmin
denote mini∈[k] |v∗
i | . Let ν = vmax/vmin. Let ρ denote
ρ(σk). Let τ = (3σ1/2)4p/ minσ∈[σk/2,3σ1/2]{ρ2(σ)}.
Theorem 4.2. For any W ∈ Rd×k with (cid:107)W − W ∗(cid:107) ≤
poly(1/k, 1/λ, 1/ν, ρ/σ2p
1 ) · (cid:107)W ∗(cid:107), let S denote a set of
i.i.d. samples from distribution D (deﬁned in (1)) and let
the activation function satisfy Property 3.1,3.2,3.3. Then
for any t ≥ 1, if |S| ≥ d · poly(log d, t, k, ν, τ, λ, σ2p
1 /ρ),
we have with probability at least 1 − d−Ω(t),

Ω(v2

minρ(σk)/(κ2λ))I (cid:22) ∇2 (cid:98)fS(W ) (cid:22) O(kv2

maxσ2p

1 )I.

Remark 4.3. As we can see from Theorem 4.2, ρ(σk) from
Property 3.2 plays an important role for positive deﬁnite
(PD) property. Interestingly, many popular activations, like
ReLU, sigmoid and tanh, have ρ(σk) > 0, while some sim-
ple functions like linear (φ(z) = z) and square (φ(z) = z2)
functions have ρ(σk) = 0 and their Hessians are rank-
deﬁcient. Another important numbers are κ and λ, two
different condition numbers of the weight matrix, which di-
If W ∗ is rank
rectly inﬂuences the positive deﬁniteness.
deﬁcient, λ → ∞, κ → ∞ and we don’t have PD property.
In the best case when W ∗ is orthogonal, λ = κ = 1. In the
worse case, λ can be exponential in k. Also W should be
close enough to W ∗. In the next section, we provide tensor
i and v∗
methods to initialize w∗
i such that they satisfy the
conditions in Theorem 4.2.

For the PD property to hold, we need the samples to be in-
dependent of the current parameters. Therefore, we need
to do resampling at each iteration to guarantee the conver-
gence in iterative algorithms like gradient descent. The fol-
lowing theorem provides the linear convergence guarantee
of gradient descent for smooth activations.

Theorem 4.4 (Linear convergence of gradient descent).
Let W be the current iterate satisfying (cid:107)W − W ∗(cid:107) ≤
poly(1/ν, 1/k, 1/λ, ρ/σ2p
1 )(cid:107)W ∗(cid:107). Let S denote a set of
samples from distribution D (deﬁned in (1)) with
i.i.d.
|S| ≥ d · poly(log d, t, k, ν, τ, λ, σ2p
1 /ρ) and let the acti-
vation function satisfy Property 3.1,3.2 and 3.3(a). Deﬁne
maxσ2p
m0 := Θ(v2
1 ).

minρ(σk)/(κ2λ)) and M0 := Θ(kv2

Recovery Guarantees for One-hidden-layer Neural Network

If we perform gradient descent with step size 1/M0 on
(cid:98)fS(W ) and obtain the next iterate,

According to Deﬁnition 5.1, we have the following results,
Claim 5.2. For each j ∈ [4], Mj = (cid:80)k

i mj,iw∗⊗j

i=1 v∗

.

i

(cid:102)W = W −

∇ (cid:98)fS(W ),

1
M0

then with probability at least 1 − d−Ω(t),

(cid:107)(cid:102)W − W ∗(cid:107)2

F ≤ (1 −

)(cid:107)W − W ∗(cid:107)2
F .

m0
M0

Due to the space limitation, we provide the proofs in the
full version.

5. Tensor Methods for Initialization

In this section, we show that Tensor methods can recover
the parameters W ∗ to some precision and exactly recover
v∗ for homogeneous activations.

It is known that most tensor problems are NP-hard (H˚astad,
1990; Hillar & Lim, 2013) or even hard to approximate
(Song et al., 2017b). However, by making some as-
sumptions, tensor decomposition method becomes efﬁcient
(Anandkumar et al., 2014; Wang et al., 2015; Wang &
Anandkumar, 2016; Song et al., 2016). Here we utilize
the noiseless assumption and Gaussian inputs assumption
to show a provable and efﬁcient tensor methods.

5.1. Preliminary

Let’s deﬁne a special outer product (cid:101)⊗ for simpliﬁcation of
the notation. If v ∈ Rd is a vector and I is the identity
matrix, then v (cid:101)⊗I = (cid:80)d
j=1[v ⊗ ej ⊗ ej + ej ⊗ v ⊗ ej +
ej ⊗ ej ⊗ v]. If M is a symmetric rank-r matrix factorized
as M = (cid:80)r

i and I is the identity matrix, then

i=1 siviv(cid:62)

r
(cid:88)

d
(cid:88)

6
(cid:88)

M (cid:101)⊗I =

si

Al,i,j,

i=1

j=1

l=1

where A1,i,j = vi ⊗ vi ⊗ ej ⊗ ej, A2,i,j = vi ⊗ ej ⊗ vi ⊗ ej,
A3,i,j = ej ⊗ vi ⊗ vi ⊗ ej, A4,i,j = vi ⊗ ej ⊗ ej ⊗ vi,
A5,i,j = ej ⊗ vi ⊗ ej ⊗ vi and A6,i,j = ej ⊗ ej ⊗ vi ⊗ vi.

Denote w = w/(cid:107)w(cid:107). Now let’s calculate some moments.

and

deﬁne M1, M2, M3, M4

5.1. We
Deﬁnition
m1,i, m2,i, m3,i, m4,i as follows :
M1 = E(x,y)∼D[y · x].
M2 = E(x,y)∼D[y · (x ⊗ x − I)].
M3 = E(x,y)∼D[y · (x⊗3 − x (cid:101)⊗I)].
M4 = E(x,y)∼D[y · (x⊗4 − (x ⊗ x) (cid:101)⊗I + I (cid:101)⊗I)].
γj(σ) = Ez∼N (0,1)[φ(σ · z)zj], ∀j = 0, 1, 2, 3, 4.
m1,i = γ1((cid:107)w∗
m2,i = γ2((cid:107)w∗
m3,i = γ3((cid:107)w∗
m4,i = γ4((cid:107)w∗

i (cid:107)).
i (cid:107)) − γ0((cid:107)w∗
i (cid:107)) − 3γ1((cid:107)w∗
i (cid:107)) + 3γ0((cid:107)w∗

i (cid:107)).
i (cid:107)).
i (cid:107)) − 6γ2((cid:107)w∗

i (cid:107)).

Note that some mj,i’s will be zero for speciﬁc activations.
For example, for activations with symmetric ﬁrst deriva-
tives, i.e., φ(cid:48)(z) = φ(cid:48)(−z), like sigmoid and erf, we
have φ(z) + φ(−z) being a constant and M2 = 0 since
γ0(σ) = γ2(σ). Another example is ReLU. ReLU func-
tions have vanishing M3, i.e., M3 = 0, as γ3(σ) = 3γ1(σ).
To make tensor methods work, we make the following as-
sumption.
Assumption 5.3. Assume the activation function φ(z)
satisﬁes the following conditions:
1. If Mj (cid:54)= 0, then mj,i (cid:54)= 0 for all i ∈ [k].
2. At least one of M3 and M4 is non-zero.
3. If M1 = M3 = 0, then φ(z) is an even function, i.e.,
φ(z) = φ(−z).
4. If M2 = M4 = 0, then φ(z) is an odd function, i.e.,
φ(z) = −φ(−z).

If φ(z) is an odd function then φ(z) = −φ(−z) and
vφ(w(cid:62)x) = −vφ(−w(cid:62)x). Hence we can always assume
If φ(z) is an even function, then vφ(w(cid:62)x) =
v > 0.
vφ(−w(cid:62)x). So if w recovers w∗ then −w also recovers
w∗. Note that ReLU, leaky ReLU and squared ReLU satisfy
Assumption 5.3. We further deﬁne the following non-zero
moments.
Deﬁnition 5.4. Let α ∈ Rd denote a randomly picked
vector. We deﬁne P2 and P3 as follows: P2 =
Mj2 (I, I, α, · · · , α) , where j2 = min{j ≥ 2|Mj (cid:54)= 0}
and P3 = Mj3(I, I, I, α, · · · , α), where j3 = min{j ≥
3|Mj (cid:54)= 0}.

According to Deﬁnition 5.1 and 5.4, we have,
Claim 5.5. P2 = (cid:80)k
P3 = (cid:80)k
i=1 v∗

i mj2,i(α(cid:62)w∗

i mj3,i(α(cid:62)w∗

i )j3−3w∗⊗3

i=1 v∗

.

i

i )j2−2w∗⊗2

i

and

P3

In other words for the above deﬁnition, P2 is equal
to the ﬁrst non-zero matrix in the ordered sequence
{M2, M3(I, I, α), M4(I, I, α, α)}.
to
the ﬁrst non-zero tensor
in the ordered sequence
{M3, M4(I, I, I, α)}. Since α is randomly picked up,
w∗(cid:62)
i α (cid:54)= 0 and we view this number as a constant through-
out this paper. So by construction and Assumption 5.3,
both P2 and P3 are rank-k. Also, let (cid:98)P2 ∈ Rd×d and
(cid:98)P3 ∈ Rd×d×d denote the corresponding empirical mo-
ments of P2 ∈ Rd×d and P3 ∈ Rd×d×d respectively.

is equal

5.2. Algorithm

Now we brieﬂy introduce how to use a set of samples
with size linear in dimension to recover the ground truth
parameters to some precision. As shown in the previ-
ous section, we have a rank-k 3rd-order moment P3 that

Recovery Guarantees for One-hidden-layer Neural Network

(cid:46) Theorem 5.6

Algorithm 1 Initialization via Tensor Method
1: procedure INITIALIZATION(S)
2:
3:
4:
5:
6:

S2, S3, S4 ← PARTITION(S, 3)
(cid:98)P2 ← ES2 [P2]
V ← POWERMETHOD( (cid:98)P2, k)
(cid:98)R3 ← ES3[P3(V, V, V )]
{(cid:98)ui}i∈[k] ← KCL( (cid:98)R3)
{w(0)
, v(0)
i
Return {w(0)

, v(0)

7:

i }i∈[k]

8:
i
9: end procedure

i }i∈[k] ← RECMAGSIGN(V, {(cid:98)ui}i∈[k], S4)

1, w∗

2, · · · , w∗

has tensor decomposition formed by {w∗
k}.
Therefore, we can use the non-orthogonal decomposition
method (Kuleshov et al., 2015) to decompose the corre-
sponding estimated tensor (cid:98)P3 and obtain an approximation
of the parameters. The precision of the obtained parame-
ters depends on the estimation error of P3, which requires
Ω(d3/(cid:15)2) samples to achieve (cid:15) error. Also, the time com-
plexity for tensor decomposition on a d × d × d tensor is
Ω(d3).

In this paper, we reduce the cubic dependency of sam-
ple/computational complexity in dimension (Janzamin
et al., 2015) to linear dependency. Our idea follows the
techniques used in (Zhong et al., 2016), where they ﬁrst
used a 2nd-order moment P2 to approximate the sub-
space spanned by {w∗
k}, denoted as V , then
use V to reduce a higher-dimensional third-order tensor
P3 ∈ Rd×d×d to a lower-dimensional tensor R3
:=
P3(V, V, V ) ∈ Rk×k×k. Since the tensor decomposi-
tion and the tensor estimation are conducted on a lower-
dimensional Rk×k×k space, the sample complexity and
computational complexity are reduced.

2, · · · , w∗

1, w∗

The detailed algorithm is shown in Algorithm 1. First, we
randomly partition the dataset into three subsets each with
size (cid:101)O(d). Then apply the power method on (cid:98)P2, which is
the estimation of P2 from S2, to estimate V . After that,
the non-orthogonal tensor decomposition (KCL)(Kuleshov
et al., 2015) on (cid:98)R3 outputs (cid:98)ui which estimates siV (cid:62)w∗
i
for i ∈ [k] with unknown sign si ∈ {−1, 1}. Hence w∗
i
can be estimated by siV (cid:98)ui. Finally we estimate the magni-
tude of w∗
i in the RECMAGSIGN func-
tion for homogeneous activations. We discuss the details
of each procedure and provide POWERMETHOD and REC-
MAGSIGN algorithms in the full version.

i and the signs si, v∗

5.3. Theoretical Analysis

We formally present our theorem for Algorithm 1, and pro-
vide the proof in the full version.

Theorem 5.6. Let the activation function be homogeneous
satisfying Assumption 5.3. For any 0 < (cid:15) < 1 and t ≥ 1,

Algorithm 2 Globally Converging Algorithm
1: procedure LEARNING1NN(S, d, k, (cid:15)) (cid:46) Theorem 6.1
2:
3:
4:
5:

T ← log(1/(cid:15)) · poly(k, ν, λ, σ2p
maxσ2p
η ← 1/(kv2
1 ).
S0, S1, · · · , Sq ← PARTITION(S, q + 1).
W (0), v(0) ← INITIALIZATION(S0).
Set v∗
for q = 0, 1, 2, · · · , T − 1 do

in Eq. (2) for all (cid:98)fSq (W ), q ∈ [T ]

i ← v(0)

1 /ρ).

W (q+1) = W (q) − η∇ (cid:98)fSq+1(W (q))

i

6:
7:
8:
9:
10:
11: end procedure

end for
Return {w(T )

i

, v(0)

i }i∈[k]

if |S| ≥ (cid:15)−2 · d · poly(t, k, κ, log d), then there exists an
algorithm (Algorithm 1) that takes |S|k · (cid:101)O(d) time and
outputs a matrix W (0) ∈ Rd×k and a vector v(0) ∈ Rk
such that, with probability at least 1 − d−Ω(t),

(cid:107)W (0) − W ∗(cid:107)F ≤ (cid:15) · poly(k, κ)(cid:107)W ∗(cid:107)F , and v(0)

i = v∗
i .

6. Global Convergence

Combining the positive deﬁniteness of the Hessian near
the global optimal in Section 4 and the tensor initialization
methods in Section 5, we come up with the overall globally
converging algorithm Algorithm 2 and its guarantee Theo-
rem 6.1.

Theorem 6.1 (Global convergence guarantees). Let S de-
note a set of i.i.d. samples from distribution D (deﬁned
in (1)) and let the activation function be homogeneous sat-
isfying Property 3.1, 3.2, 3.3(a) and Assumption 5.3. Then
for any t ≥ 1 and any (cid:15) > 0, if |S| ≥ d log(1/(cid:15)) ·
poly(log d, t, k, λ), T ≥ log(1/(cid:15)) · poly(k, ν, λ, σ2p
1 /ρ)
maxσ2p
and 0 < η ≤ 1/(kv2
1 ), then there is an Algo-
rithm (procedure LEARNING1NN in Algorithm 2) taking
|S| · d · poly(log d, k, λ) time and outputting a matrix
W (T ) ∈ Rd×k and a vector v(0) ∈ Rk satisfying

(cid:107)W (T ) − W ∗(cid:107)F ≤ (cid:15)(cid:107)W ∗(cid:107)F , and v(0)

i = v∗
i .

with probability at least 1 − d−Ω(t).

This follows by combining Theorem 4.4 and Theorem 5.6.

7. Numerical Experiments

In this section we use synthetic data to verify our theoreti-
cal results. We generate data points {xi, yi}i=1,2,··· ,n from
Distribution D(deﬁned in Eq. (1)). We set W ∗ = U ΣV (cid:62),
where U ∈ Rd×k and V ∈ Rk×k are orthogonal matri-
ces generated from QR decomposition of Gaussian ma-
trices, Σ is a diagonal matrix whose diagonal elements

Recovery Guarantees for One-hidden-layer Neural Network

(a) Sample complexity for recovery

(b) Tensor initialization error

(c) Objective v.s. iterations

Figure1. Numerical Experiments

k−1 , · · · , κ.

k−1 , 1 + 2(κ−1)
are 1, 1 + κ−1
In this experiment,
we set κ = 2 and k = 5. We set v∗
to be randomly
i
picked from {−1, 1} with equal chance. We use squared
ReLU φ(z) = max{z, 0}2, which is a smooth homoge-
neous function. For non-orthogonal tensor methods, we di-
rectly use the code provided by (Kuleshov et al., 2015) with
the number of random projections ﬁxed as L = 100. We
pick the stepsize η = 0.02 for gradient descent. In the ex-
periments, we don’t do the resampling since the algorithm
still works well without resampling.

First we show the number of samples required to re-
cover the parameters for different dimensions. We ﬁx
k = 5, change d for d = 10, 20, · · · , 100 and n for
n = 1000, 2000, · · · , 10000. For each pair of d and n,
we run 10 trials. We say a trial successfully recovers the
parameters if there exists a permutation π : [k] → [k], such
that the returned parameters W and v satisfy

max
j∈[k]

{(cid:107)w∗

j − wπ(j)(cid:107)/(cid:107)w∗

j (cid:107)} ≤ 0.01 and vπ(j) = v∗
j .

We record the recovery rates and represent them as grey
scale in Fig. 1(a). As we can see from Fig. 1(a), the least
number of samples required to have 100% recovery rate is
about proportional to the dimension.

Next we test the tensor initialization. We show the error be-
tween the output of the tensor method and the ground truth
parameters against the number of samples under different
dimensions in Fig 1(b). The pure dark blocks indicate, in
at least one of the 10 trials, (cid:80)k
i=1 v(0)
i , which
i
means v(0)
is not correctly initialized. Let Π(k) denote the
set of all possible permutations π : [k] → [k]. The grey
scale represents the averaged error,

(cid:54)= (cid:80)k

i=1 v∗

i

min
π∈Π(k)

max
j∈[k]

{(cid:107)w∗

j − w(0)

π(j)(cid:107)/(cid:107)w∗

j (cid:107)},

over 10 trials. As we can see, with a ﬁxed dimension, the
more samples we have the better initialization we obtain.
We can also see that to achieve the same initialization error,
the sample complexity required is about proportional to the
dimension.

We also compare different initialization methods for gradi-
ent descent in Fig. 1(c). We ﬁx d = 10, k = 5, n = 10000
and compare three different initialization approaches, (I)
Let both v and W be initialized from tensor methods, and
then do gradient descent for W while v is ﬁxed; (II) Let
both v and W be initialized from random Gaussian, and
then do gradient descent for both W and v; (III) Let v = v∗
and W be initialized from random Gaussian, and then do
gradient descent for W while v is ﬁxed. As we can see
from Fig 1(c), Approach (I) is the fastest and Approach (II)
doesn’t converge even if more iterations are allowed. Both
Approach (I) and (III) have linear convergence rate when
the objective value is small enough, which veriﬁes our lo-
cal linear convergence claim.

8. Conclusion

As shown in Theorem 6.1, the tensor initialization followed
by gradient descent will provide a globally converging al-
gorithm with linear time/sample complexity in dimension,
logarithmic in precision and polynomial in other factors for
smooth homogeneous activation functions. Our distilled
properties for activation functions include a wide range of
non-linear functions and hopefully provide an intuition to
understand the role of non-linear activations played in op-
timization. Deeper neural networks and convergence for
SGD will be considered in the future.

Acknowledgments

P. L. Bartlett would like to gratefully acknowledge the sup-
port of Australian Research Council through an Australian
Laureate Fellowship (FL110100281) and through the Aus-
tralian Research Council Centre of Excellence for Mathe-
matical and Statistical Frontiers (ACEMS), and the support
of the NSF through grant IIS-1619362. I. S. Dhillon would
like to gratefully acknowledge the support of NSF grants
CCF-1320746, IIS-1546452 and CCF-1564000. Part of the
work was done while K. Zhong was interning in Microsoft
Research, India. The authors would like to thank Surbhi
Goel, Adam Klivans, Qi Lei, Eric Price, David P. Woodruff,
Peilin Zhong, Hongyang Zhang and Jiong Zhang for useful
discussions.

20406080100d200040006000800010000N00.20.40.60.81Recovery Rate20406080100d200040006000800010000N0.511.5Tensor initialization error02004006008001000iteration-10-505101520log(obj)Initialize v,W with TensorRandomly initialize both v,WFix v=v*, randomly initialize WRecovery Guarantees for One-hidden-layer Neural Network

References

Anandkumar, Animashree, Ge, Rong, Hsu, Daniel, Kakade,
Sham M, and Telgarsky, Matus. Tensor decompositions for
learning latent variable models. JMLR, 15:2773–2832, 2014.
Andoni, Alexandr, Panigrahy, Rina, Valiant, Gregory, and Zhang,
Li. Learning polynomials with neural networks. In Proceed-
ings of the 31st International Conference on Machine Learning
(ICML), pp. 1908–1916, 2014.

Arora, Sanjeev, Ge, Rong, Kannan, Ravindran, and Moitra,
Ankur.
Computing a nonnegative matrix factorization–
provably. In Proceedings of the forty-fourth annual ACM sym-
posium on Theory of computing (STOC), pp. 145–162. ACM,
2012a.

Arora, Sanjeev, Ge, Rong, and Moitra, Ankur. Learning topic
models–going beyond svd. In Foundations of Computer Sci-
ence (FOCS), 2012 IEEE 53rd Annual Symposium on, pp.
1–10. IEEE, 2012b.

Arora, Sanjeev, Bhaskara, Aditya, Ge, Rong, and Ma, Tengyu.
Provable bounds for learning some deep representations.
In
Proceedings of the 31st International Conference on Machine
Learning (ICML), pp. 584–592. https://arxiv.org/
pdf/1310.6343.pdf, 2014.

Arora, Sanjeev, Ge, Rong, Ma, Tengyu, and Risteski, Andrej.
Provable learning of noisy-or networks. In Proceedings of the
49th Annual Symposium on the Theory of Computing (STOC).
https://arxiv.org/pdf/1612.08795.pdf, 2017.
Aslan, ¨Ozlem, Zhang, Xinhua, and Schuurmans, Dale. Convex
deep learning via normalized kernels. In Advances in Neural
Information Processing Systems (NIPS), pp. 3275–3283, 2014.
Bach, Francis. Breaking the curse of dimensionality with convex

neural networks. arXiv preprint arXiv:1412.8690, 2014.

Balcan, Maria-Florina, Liang, Yingyu, Woodruff, David P., and
Zhang, Hongyang. Optimal sample complexity for matrix
completion and related problems via (cid:96)2-regularization. arXiv
preprint arXiv:1704.08683, 2017.

Balduzzi, David. Deep online convex optimization with gated

games. arXiv preprint arXiv:1604.01952, 2016.

Balduzzi, David, McWilliams, Brian, and Butler-Yeoman, Tony.
Neural taylor approximations: Convergence and exploration in
rectiﬁer networks. arXiv preprint arXiv:1611.02345, 2016.
Bartlett, Peter L. The sample complexity of pattern classiﬁcation
with neural networks: the size of the weights is more important
than the size of the network. IEEE Transactions on Information
Theory, 44(2):525–536, 1998.

Bengio, Yoshua, Roux, Nicolas L, Vincent, Pascal, Delalleau,
Olivier, and Marcotte, Patrice. Convex neural networks.
In
Advances in Neural Information Processing Systems (NIPS),
pp. 123–130, 2005.

Blum, Avrim and Rivest, Ronald L. Training a 3-node neural
network is np-complete. In Proceedings of the 1st International
Conference on Neural Information Processing Systems (NIPS),
pp. 494–501. MIT Press, 1988.

Brutzkus, Alon and Globerson, Amir. Globally optimal gradi-
ent descent for a convnet with gaussian inputs. arXiv preprint
arXiv:1702.07966, 2017.

Choromanska, Anna, Henaff, MIkael, Mathieu, Michael,
Ben Arous, Gerard, and LeCun, Yann. The loss surfaces of
multilayer networks. In Proceedings of the Eighteenth Interna-
tional Conference on Artiﬁcial Intelligence and Statistics (AIS-
TATS), pp. 192–204, 2015.

Cohen, Nadav and Shashua, Amnon. Convolutional rectiﬁer net-
works as generalized tensor decompositions. In International
Conference on Machine Learning (ICML), 2016.

Cohen, Nadav, Sharir, Or, and Shashua, Amnon. On the expres-
sive power of deep learning: A tensor analysis. In 29th Annual
Conference on Learning Theory (COLT), pp. 698–728, 2016.
Daniely, Amit, Frostig, Roy, and Singer, Yoram. Toward deeper
understanding of neural networks: The power of initialization
and a dual view on expressivity. In Advances in neural infor-
mation processing systems (NIPS), pp. 2253–2261, 2016.
Dauphin, Yann N, Pascanu, Razvan, Gulcehre, Caglar, Cho,
Kyunghyun, Ganguli, Surya, and Bengio, Yoshua.
Identify-
ing and attacking the saddle point problem in high-dimensional
non-convex optimization. In Advances in neural information
processing systems (NIPS), pp. 2933–2941, 2014.

Elhamifar, Ehsan and Vidal, Ren´e. Sparse subspace clustering. In

CVPR, pp. 2790–2797, 2009.

Feng, Jiashi, Zahavy, Tom, Kang, Bingyi, Xu, Huan, and Mannor,
Shie. Ensemble robustness of deep learning algorithms. arXiv
preprint arXiv:1602.02389, 2016.

Freeman, C Daniel and Bruna, Joan. Topology and geome-
In arXiv preprint.

try of half-rectiﬁed network optimization.
https://arxiv.org/pdf/1611.01540.pdf, 2016.
Gillis, Nicolas and Glineur, Franc¸ois. Low-rank matrix approxi-
mation with weights or missing data is np-hard. SIAM Journal
on Matrix Analysis and Applications, 32(4):1149–1165, 2011.
Gillis, Nicolas and Vavasis, Stephen A. On the complexity of
robust pca and (cid:96)1-norm low-rank matrix approximation. arXiv
preprint arXiv:1509.09236, 2015.

Goel, Surbhi, Kanade, Varun, Klivans, Adam, and Thaler, Justin.
Reliably learning the relu in polynomial time. In 30th Annual
Conference on Learning Theory (COLT). https://arxiv.
org/pdf/1611.10258.pdf, 2017.

Haeffele, Benjamin D and Vidal, Ren´e. Global optimality in ten-
sor factorization, deep learning, and beyond. arXiv preprint
arXiv:1506.07540, 2015.

Hardt, Moritz. Understanding alternating minimization for ma-
trix completion. In Foundations of Computer Science (FOCS),
2014 IEEE 55th Annual Symposium on, pp. 651–660. IEEE,
2014.

Hardt, Moritz and Ma, Tengyu. Identity matters in deep learning.

ICLR, 2017.

Hardt, Moritz and Moitra, Ankur. Algorithms and hardness for
robust subspace recovery. In COLT, volume 30, pp. 354–375,
2013.

Hardt, Moritz, Recht, Ben, and Singer, Yoram. Train faster, gener-
alize better: Stability of stochastic gradient descent. In ICML,
pp. 1225–1234, 2016.

H˚astad, Johan. Tensor rank is np-complete. Journal of Algo-

rithms, 11(4):644–654, 1990.

Hillar, Christopher J and Lim, Lek-Heng. Most tensor problems
In Journal of the ACM (JACM), volume 60(6),
are np-hard.
pp. 45. https://arxiv.org/pdf/0911.1393.pdf,
2013.

Hornik, Kurt. Approximation capabilities of multilayer feedfor-

ward networks. Neural networks, 4(2):251–257, 1991.

Jain, Prateek, Netrapalli, Praneeth, and Sanghavi, Sujay. Low-
rank matrix completion using alternating minimization. In Pro-
ceedings of the forty-ﬁfth annual ACM symposium on Theory
of computing (STOC), 2013.

Recovery Guarantees for One-hidden-layer Neural Network

tions of Computer Science (FOCS), pp. 270–289. IEEE, 2015.
Swirszcz, Grzegorz, Czarnecki, Wojciech Marian, and Pascanu,
Razvan. Local minima in training of deep networks. arXiv
preprint arXiv:1611.06310, 2016.

Telgarsky, Matus. Beneﬁts of depth in neural networks.

In
29th Annual Conference on Learning Theory (COLT), pp.
1517–1539, 2016.

Tian, Yuandong. Symmetry-breaking convergence analysis of
certain two-layered neural networks with ReLU nonlinearity.
In Workshop at International Conference on Learning Repre-
sentation, 2017.

Wang, Yining and Anandkumar, Anima.
differentially-private tensor decomposition.
in Neural
3531–3539, 2016.

Information Processing Systems

Online and
In Advances
(NIPS), pp.

Wang, Yining, Tung, Hsiao-Yu, Smola, Alexander J, and Anand-
kumar, Anima. Fast and guaranteed tensor decomposition via
sketching. In Advances in Neural Information Processing Sys-
tems (NIPS), pp. 991–999, 2015.

Xie, Bo, Liang, Yingyu, and Song, Le. Diversity leads to gener-
alization in neural networks. In International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS), 2017.

Yi, Xinyang, Caramanis, Constantine, and Sanghavi, Sujay. Al-
ternating minimization for mixed linear regression. In ICML,
pp. 613–621, 2014.

Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Recht, Benjamin,
and Vinyals, Oriol. Understanding deep learning requires re-
thinking generalization. In ICLR, 2017a.

Zhang, Yuchen, Lee, Jason D, and Jordan, Michael I. L1-
regularized neural networks are improperly learnable in poly-
nomial time. In Proceedings of The 33rd International Confer-
ence on Machine Learning (ICML), pp. 993–1001, 2016a.
Zhang, Yuchen, Liang, Percy, and Wainwright, Martin J.
arXiv preprint

Convexiﬁed convolutional neural networks.
arXiv:1609.01000, 2016b.

Zhang, Yuchen, Lee, Jason D., Wainwright, Martin J., and Jordan,
Michael I. On the learnability of fully-connected neural net-
works. In International Conference on Artiﬁcial Intelligence
and Statistics, 2017b.

Zhong, Kai, Jain, Prateek, and Dhillon, Inderjit S. Mixed linear
regression with multiple components. In Advances in neural
information processing systems (NIPS), pp. 2190–2198, 2016.
Zhong, Kai, Song, Zhao, Jain, Prateek, Bartlett, Peter L., and
Dhillon, Inderjit S. Recovery guarantees for one-hidden-layer
In ICML. https://arxiv.org/pdf/
neural networks.
1706.03175.pdf, 2017.

Janzamin, Majid, Sedghi, Hanie, and Anandkumar, Anima.
Beating the perils of non-convexity: Guaranteed training
arXiv preprint
of neural networks using tensor methods.
arXiv:1506.08473, 2015.

Kawaguchi, Kenji. Deep learning without poor local minima.

arXiv preprint arXiv:1605.07110, 2016.

Kuleshov, Volodymyr, Chaganty, Arun, and Liang, Percy. Tensor
In Proceedings of the
factorization via matrix factorization.
Eighteenth International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS), pp. 507–516, 2015.

Livni, Roi, Shalev-Shwartz, Shai, and Shamir, Ohad. On the com-
putational efﬁciency of training neural networks. In Advances
in neural information processing systems (NIPS), pp. 855–863,
2014.

Montufar, Guido F, Pascanu, Razvan, Cho, Kyunghyun, and Ben-
gio, Yoshua. On the number of linear regions of deep neural
networks. In Advances in neural information processing sys-
tems (NIPS), pp. 2924–2932, 2014.

Poole, Ben, Lahiri, Subhaneil, Raghu, Maithreyi, Sohl-Dickstein,
Jascha, and Ganguli, Surya. Exponential expressivity in deep
neural networks through transient chaos. In Advances In Neu-
ral Information Processing Systems (NIPS), pp. 3360–3368,
2016.

Raghu, Maithra, Poole, Ben, Kleinberg, Jon, Ganguli, Surya, and
Sohl-Dickstein, Jascha. On the expressive power of deep neural
networks. arXiv preprint arXiv:1606.05336, 2016.

Razenshteyn,

Ilya P, Song, Zhao, and Woodruff, David P.
Weighted low rank approximations with provable guarantees.
In Proceedings of the 48th Annual Symposium on the Theory
of Computing (STOC), pp. 250–263, 2016.

Safran, Itay and Shamir, Ohad. On the quality of the initial basin
in overspeciﬁed neural networks. In International Conference
on Machine Learning (ICML), 2016.

Sagun, Levent, Bottou, L´eon, and LeCun, Yann. Singularity of
the Hessian in deep learning. arXiv preprint arXiv:1611.07476,
2016.

Sedghi, Hanie and Anandkumar, Anima. Provable methods for
training neural networks with sparse connectivity. In Interna-
tional Conference on Learning Representation (ICLR), 2015.
Shamir, Ohad. Distribution-speciﬁc hardness of learning neural

networks. arXiv preprint arXiv:1609.01037, 2016.

Song, Zhao, Woodruff, David P., and Zhang, Huan. Sublinear
time orthogonal tensor decomposition. In Advances in Neural
Information Processing Systems(NIPS), pp. 793–801, 2016.

Low
Song, Zhao, Woodruff, David P., and Zhong, Peilin.
In Pro-
rank approximation with entrywise (cid:96)1-norm error.
ceedings of the 49th Annual Symposium on the Theory of
Computing (STOC). ACM, https://arxiv.org/pdf/
1611.00898.pdf, 2017a.

Song, Zhao, Woodruff, David P., and Zhong, Peilin. Relative error
In arXiv preprint. https:

tensor low rank approximation.
//arxiv.org/pdf/1704.08246.pdf, 2017b.

Sontag, David and Roy, Dan. Complexity of inference in latent
In Advances in neural information pro-

dirichlet allocation.
cessing systems, pp. 1008–1016, 2011.

Soudry, Daniel and Carmon, Yair. No bad local minima: Data in-
dependent training error guarantees for multilayer neural net-
works. arXiv preprint arXiv:1605.08361, 2016.

Sun, Ruoyu and Luo, Zhi-Quan. Guaranteed matrix completion
via non-convex factorization. In IEEE Symposium on Founda-

