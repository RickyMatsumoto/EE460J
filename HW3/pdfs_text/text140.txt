Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix
Estimation on Compressed Data’

This appendix is organized as follows. In Section 1, we state all theoretical results, including our proposed Lemma 1 and
Lemma 2 whose details are not presented in the main text of the paper. In Section 2, we provide detailed proofs for all of
the results. In Section 3, we reformulate and discuss the current theoretical results of the counterparts: Gauss-Inverse and
UniSample-HD. In Section 4, we give a detailed analysis of the computational complexity. Finally, in Section 5, we study
the impact of different α on the estimation accuracy.

Before proceeding, we ﬁrst show the notations used in this appendix.
Notation. Let [k] denote a set of integers {1, 2, . . . , k}. Given a matrix X ∈ Rd×n, for j ∈ [d], i ∈ [n], we let xi ∈ Rd
denote the i-th column of X, and xji denote the (j, i)-th element of X or j-th element of xi. Let {Xt}k
t=1 denote the set of
matrices {X1, X2, . . . , Xk}, and xji,t denote the (j, i)-th element of Xt. Let XT denote the transpose of X, and Tr(X)
denote its trace. Let |x| denote the absolute value of x. Let (cid:107)X(cid:107)2 and (cid:107)X(cid:107)F denote the spectral norm and Frobenius norm
of X, respectively. Let (cid:107)x(cid:107)q = ((cid:80)d
j=1 |xj|q)1/q for q ≥ 1 be the (cid:96)q-norm of x ∈ Rd. Let D(x) or D({xj}) be a square
diagonal matrix with the elements of vector x on the main diagonal, and D(X) also be a square diagonal matrix whose
main diagonal has only the main diagonal elements of X. Finally, X (cid:22) Y means that Y − X is positive semideﬁnite.

1. Provable Results

For convenience, we ﬁrst restate the theorems and their corollaries in the following.
Theorem 1. Assume X ∈ Rd×n and the sampling size 2 ≤ m < d. Sample m entries from each xi ∈ Rd with replacement
k=1 and Si ∈ Rd×m denote the sampling probabilities and sampling matrix, respectively.
by running Algorithm 1. Let {pki}d
Then, the unbiased estimator for the target covariance matrix C = 1
i=1 xixT
n

n XXT can be recovered as

i = 1

(cid:80)n

Ce = (cid:98)C1 − (cid:98)C2,

(cid:80)n

nm−n

i xixT

i SiST

i=1 SiST

where (cid:98)C1 = m
E [Ce] = C.
Theorem 2. Given X ∈ Rd×n and the sampling size 2 ≤ m < d, let C and Ce be deﬁned as in Theorem 1. If the sampling
probabilities satisfy pki = α |xki|
with 0 < α < 1 for all k ∈ [d] and i ∈ [n], then with probability at least
(cid:107)xi(cid:107)1
1 − η − δ,

i )D(bi) with bki =

i , (cid:98)C2 = m

+ (1 − α) x2

1
1+(m−1)pki

D(SiST

i SiST

i xixT

ki
(cid:107)xi(cid:107)2
2

, and

nm−n

i=1

(cid:80)n

(cid:107)Ce − C(cid:107)2 ≤ log(

+

2σ2 log(

(cid:114)

)

2d
2R
δ
3
η ) 14(cid:107)xi(cid:107)2

nmα2

1

(cid:105)

2d
δ

),

(cid:104)

, and σ2 = (cid:80)n

i=1

8(cid:107)xi(cid:107)4
2

n2m2(1−α)2 + 4(cid:107)xi(cid:107)2

1(cid:107)xi(cid:107)2
n2m3α2(1−α)

2

where we deﬁne that R = maxi∈[n]
+ 9(cid:107)xi(cid:107)4

n2m(1−α) + 2(cid:107)xi(cid:107)2

2(cid:107)xi(cid:107)2
1
n2m2α(1−α)

+ (cid:107) (cid:80)n

(cid:105)

2

i=1

2

n + log2( 2nd
1xix2
(cid:107)xi(cid:107)2
n2mα (cid:107)2.

i

(cid:104) 7(cid:107)xi(cid:107)2

Corollary 1. Given X ∈ Rd×n and sampling size 2 ≤ m < d, let C and Ce be constructed by Algorithm 1. Deﬁne
(cid:107)xi(cid:107)1
(cid:107)xi(cid:107)2

d, and (cid:107)xi(cid:107)2 ≤ τ for all i ∈ [n]. Then, with probability at least 1 − η − δ we have

≤ ϕ with 1 ≤ ϕ ≤

√

(cid:107)Ce − C(cid:107)2 ≤ min{ (cid:101)O

f +

(cid:16)

τ 2ϕ
m

(cid:114) 1
n

+ τ 2

(cid:114) 1
nm

(cid:17)

(cid:16)

, (cid:101)O

f +

(cid:114)

τ ϕ
m

d(cid:107)C(cid:107)2
n

+ τ

(cid:114)

d(cid:107)C(cid:107)2
nm

(cid:17)

},

where f = τ 2

n + τ 2ϕ2

nm + τ ϕ

(cid:113) (cid:107)C(cid:107)2

nm , and (cid:101)O(·) hides the logarithmic factors on η, δ, m, n, d, and α.

(1)

(2)

(3)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

Corollary 2. Given X ∈ Rd×n (2 ≤ d) and an unknown population covariance matrix Cp ∈ Rd×d with each column
vector xi ∈ Rd i.i.d. generated from the Gaussian distribution N (0, Cp). Let Ce be constructed by Algorithm 1 with
sampling size 2 ≤ m < d. Then, with probability at least 1 − η − δ − ζ,

(cid:107)Ce − Cp(cid:107)2
(cid:107)Cp(cid:107)2

≤ (cid:101)O

(cid:16) d2
nm

+

d
m

(cid:114)

(cid:17)

;

d
n

Additionally, assuming rank(Cp)≤ r, with probability at least 1 − η − δ − ζ we have

(cid:107)[Ce]r − Cp(cid:107)2
(cid:107)Cp(cid:107)2

≤ (cid:101)O

(cid:16) rd
nm

+

r
m

(cid:114)

(cid:114)

d
n

+

(cid:17)

,

rd
nm

i=1
i=1 being the leading k eigenvectors of Cp and Ce, respectively. Denote by λk the k-th largest eigenvalue of Cp.

where [Ce]r is the solution to minrank(A)≤r (cid:107)A − Ce(cid:107)2, and (cid:101)O(·) hides the logarithmic factors on η, δ, ζ, m, n, d, and α.
Corollary 3. Given X, d, m, Cp and Ce as in Corollary 2. Let (cid:81)
and {ˆui}k
Then, with probability at least 1 − η − δ − ζ,
k − (cid:81)
(cid:107)Cp(cid:107)2

1
λk − λk+1

i with {ui}k

i and (cid:98)(cid:81)

(cid:16) d2
nm

k = (cid:80)k

k = (cid:80)k

i=1 ˆui ˆuT

i=1 uiuT

d
m

(cid:107) (cid:98)(cid:81)

k (cid:107)2

d
n

(6)

(cid:114)

(cid:101)O

+

≤

(cid:17)

,

where the eigengap λk − λk+1 > 0 and (cid:101)O(·) hides the logarithmic factors on η, δ, ζ, m, n, d, and α.

Next, we present two lemmas: Lemma 1 and Lemma 2, which are used to prove the foregoing theorems. The detailed
statements of the two lemmas are omitted in the main text of the paper owing to limited space, and now they are
described below.
Lemma 1. Given any vector x ∈ Rd, and m < d, sample m entries from x with replacement by running Algorithm 1 with
k=1 denote the corresponding sampling probabilities, S ∈ Rd×m denote the corresponding
the inputs x and m. Let {pk}d
rescaled sampling matrix, and {ek}d

k=1 denote the standard basis vectors for Rd. Then, we have

E (cid:2)SST xxT SST (cid:3) =

E (cid:2)D(SST xxT SST )(cid:3) =

d
(cid:88)

k=1

x2
k
mpk

ekeT

k +

m − 1
m

xxT ;

d
(cid:88)

(
k=1

1
mpk

+

m − 1
m

)x2

kekeT
k ;

E (cid:2)(D(SST xxT SST ))2(cid:3) =

d
(cid:88)

k=1

(cid:20) 1
m3p3
k

+

7(m − 1)
m3p2
k

+

6(m2 − 3m + 2)
m3pk

+

m3 − 6m2 + 11m − 6
m3

(cid:21)

kekeT
x4
k ;

E (cid:2)SST xxT SST D(SST xxT SST )(cid:3) = (E (cid:2)D(SST xxT SST )SST xxT SST (cid:3))T

3(m2 − 3m + 2)
m3pk

(cid:21)

kekeT
x4

k +

m − 1
m3 xxT D({

x2
k
p2
k

})

=

+

d
(cid:88)

(cid:20) 1
m3p3
k

+

k=1
3(m2 − 3m + 2)
m3

+

6(m − 1)
m3p2
k
(cid:20)
D({

xxT

E (cid:2)(SST xxT SST )2(cid:3) =

(cid:107)x(cid:107)2

2(m2 − 3m + 2)

(cid:34)

d
(cid:88)

k=1
(cid:34)

+

+

m3

m3

}) +

x2
k
pk
(cid:20) 4(m − 1)
m3p2
k

d
(cid:88)

k=1

m − 3
3

(cid:21)

D({x2

k})

;

+

(cid:21)

1
m3p3
k
(cid:35)

kekeT
x4
k

+

m − 1
m3

d
(cid:88)

k=1

x2
k
pk

x2
k
pk

ekeT
k

(cid:107)x(cid:107)2

2(m3 − 6m2 + 11m − 6)

+

m2 − 3m + 2
m3

(cid:35)

d
(cid:88)

k=1

x2
k
pk

xxT

(4)

(5)

(7)

(8)

(9)

(10)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

+ xxT

(cid:20) 2(m2 − 3m + 2)
m3

D({

}) +

x2
k
pk

m − 1
m3

D({

(cid:21)

})

x2
k
p2
k
(cid:21)

+

(cid:20) 2(m2 − 3m + 2)
m3

D({

}) +

x2
k
pk

m − 1
m3

x2
k
p2
k
k}) denotes a square diagonal matrix with {x2

xxT ,

D({

})

where the expectation is w.r.t. S, and D({x2
extended to other similar notations.
Lemma 2. Given the deﬁnitions in Lemma 1. Then, with probability at least 1 − (cid:80)d

(cid:107)SST xxSST (cid:107)2 ≤

f 2(xk, ηk, m),

(cid:88)

k∈Γ

k}d

k=1 on its diagonal that can be

k=1 ηk, we have

(11)

(12)

where Γ is a set containing at most m different elements of [d] with its cardinality |Γ| ≤ m, and f (xk, ηk, m) = |xk| +
log( 2
ηk

log(2/ηk) ( 1
mpk

(cid:104) |xk|
3mpk

(cid:113) 1

+ |xk|

− 1

m )

(cid:105)
.

+

)

2

9m2p2
k

Remark 1. For the expressions in Lemma 1 and Lemma 2, the sampling probability pk appears in the denominator, which
indicates that the derived bound may be sensitive to a highly small pk (cid:54)= 0. However, in terms of any pk = 0, we can deﬁne
|xk|a
= 0 for a, b > 0, because we follow the rule that pk = 0 only when xk = 0 and xk = 0 can never be sampled. Thus,
pb
k

the aforementioned two lemmas and other derived results are applicable to the case where there exists pk = 0.

2. Analysis

2.1. Technical Theorems

Below, we ﬁrst show the Matrix Bernstein inequality employed for characterizing the sums of independent random vari-
ables/matrices, and then present a matrix perturbation result for eigenvalues.
Theorem 3 (Tropp 2015, p. 76). Let {Ai}L
Deﬁne the variance σ2 = max{(cid:107) (cid:80)L
n) exp( −(cid:15)2/2
σ2+R(cid:15)/3 ) for all (cid:15) ≥ 0.
Theorem 4 (Golub & Van Loan 1996, p. 396). If A ∈ Rd×d and A + E ∈ Rd×d are symmetric matrices, then

i=1 ∈ Rd×n be independent random matrices with E [Ai] = 0 and (cid:107)Ai(cid:107)2 ≤ R.
E (cid:2)AiAT
i=1 Ai(cid:107)2 ≥ (cid:15)) ≤ (d +

(cid:3) (cid:107)2}. Then, P((cid:107) (cid:80)L

(cid:3) (cid:107)2, (cid:107) (cid:80)L

E (cid:2)AT

i Ai

i=1

i=1

i

λk(A) + λd(E) ≤ λk(A + E) ≤ λk(A) + λ1(E)

(13)

for k ∈ [d], where λk(A + E) and λk(A) designate the k-th largest eigenvalues.

2.2. Proof of Lemma 1

Proof. According to Algorithm 1 in the main text of the paper, each column vector in the rescaled sampling matrix S ∈
Rd×m is sampled with replacement from {rk = 1√
are the standard basis vectors for Rd.

k=1 with corresponding probabilities {pk}d

k=1, where {ek}d

ek}d

mpk

k=1

Firstly, we prove Eq. (7). By the deﬁnition, we expand
m
(cid:88)

SST xxT SST =

stj sT
tj

x

xT stj sT
tj

j=1

m
(cid:88)

j=1

(cid:88)

i(cid:54)=j∈[m]

stj sT
tj

xxT stj sT
tj

+

stisT
ti

xxT stj sT
tj

,

=

m
(cid:88)

j=1

where the random variable tj is in [d].

Passing the expectation over S through the sum in Eq. (15), we have
m
(cid:88)

m
(cid:88)

d
(cid:88)

E

stj sT
tj

xxT stj sT
tj

=

P(tj = k)rkrT

k xxT rkrT
k

j=1

j=1

k=1

=

m
(cid:88)

d
(cid:88)

j=1

k=1

pk

1
m2p2
k

ekeT

k xxT ekeT

k =

d
(cid:88)

k=1

x2
k
mpk

ekeT
k ,

(14)

(15)

(16)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

and similarly

E (cid:88)

sti sT
ti

xxT stj sT
tj

=

(cid:88)

d
(cid:88)

d
(cid:88)

i(cid:54)=j∈[m]

k=1

q=1

P(ti = k)P(tj = q)rkrT

k xxT rqrT
q

i(cid:54)=j∈[m]

=

d
(cid:88)

d
(cid:88)

k=1

q=1

xkxq

m − 1
m

ekeT

q =

m − 1
m

xxT .

Now, combing Eq. (16) with Eq. (18) immediately proves Eq. (7).

Then, Eq. (8) can be proved based on Eq. (7) by

E (cid:2)D(SST xxT SST )(cid:3) = D(E (cid:2)SST xxT SST (cid:3)) =

d
(cid:88)

(
k=1

1
mpk

+

m − 1
m

)x2

kekeT
k .

Alternatively, D(SST xxT SST ) can be explicitly expanded by

D(SST xxT SST ) =

stj sT
tj

kekeT
x2
k

stj sT
tj

.

m
(cid:88)

j=1

d
(cid:88)

k=1

m
(cid:88)

j=1

Thus, the whole target expectations in Eq. (9), Eq. (10) and Eq. (11) can be explicitly expanded, and we can use similar
ways of proving Eq. (7) to prove the remainder of the lemma.

To prove Eq. (9), we expand

E (cid:2)(D(SST xxT SST ))2(cid:3) = E

stj sT
tj

kekeT
x2
k

stj sT
tj

)2



= E

stj sT
tj

kekeT
x2
k

stj sT
tj

stj sT
tj

kekeT
x2
k

stj sT
tj



m
(cid:88)

j=1

d
(cid:88)

k=1

m
(cid:88)

j=1

= E

stj sT
tj

kekeT
x2

k stj sT
tj

stj sT
tj

kekeT
x2

k stj sT
tj

m
(cid:88)

j=1





+ E

stj sT
tj

kekeT
x2

k stj sT
tj

stisT
ti

kekeT
x2

k stj sT
tj



(

m
(cid:88)

j=1

m
(cid:88)

j=1

m
(cid:88)

j=1

(cid:88)

i(cid:54)=j∈[m]

m
(cid:88)

j=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1





m
(cid:88)

j=1

m
(cid:88)

j=1

m
(cid:88)

j=1

+ E (cid:88)

i(cid:54)=j∈[m]

+ E (cid:88)

i(cid:54)=j∈[m]

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

stisT
ti

kekeT
x2

k stj sT
tj

stj sT
tj

kekeT
x2

k stj sT
tj

stisT
ti

kekeT
x2

k stj sT
tj

stisT
ti

kekeT
x2

k stj sT
tj

,

(cid:88)

i(cid:54)=j∈[m]

d
(cid:88)

k=1

where the four terms in the last equations are calculated as:

(23) = E

stj sT
tj

kekeT
x2

k stj sT
tj

stj sT
tj

kekeT
x2

k stj sT
tj

m
(cid:88)

j=1

d
(cid:88)

k=1

m
(cid:88)

j=1

d
(cid:88)

k=1

d
(cid:88)

k=1

m
(cid:88)

j=1

d
(cid:88)

k=1

= E

stj sT
tj

kekeT
x2

k stj sT
tj

stj sT
tj

kekeT
x2

k stj sT
tj

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

(26)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

+ E (cid:88)

i(cid:54)=j∈[m]

d
(cid:88)

m
(cid:88)

k1=1

j=1

=

+ E (cid:88)

d
(cid:88)

k=1

pk1

1
m4p4
k1

d
(cid:88)

d
(cid:88)

i(cid:54)=j∈[m]

k1=1

q=1

stisT
ti

kekeT
x2

k stisT
ti

stj sT
tj

kekeT
x2

k stj sT
tj

d
(cid:88)

k=1

ek1 eT
k1

kekeT
x2

k ek1 eT
k1

ek1eT
k1

kekeT
x2

k ek1eT
k1

d
(cid:88)

k=1

d
(cid:88)

k=1

pk1pq

1
m4p2
k1

p2
q

d
(cid:88)

k=1

ek1 eT
k1

kekeT
x2

k ek1eT
k1

eqeT
q

kekeT
x2

k eqeT
q

d
(cid:88)

k=1

=

=

d
(cid:88)

k=1

d
(cid:88)

(
k=1

x4
k
m3p3
k

ekeT

k +

d
(cid:88)

k=1

(m2 − m)x4
k
m4p2
k

ekeT
k

1
m3p3
k

+

m − 1
m3p2
k

)x4

kekeT
k ;

(24) = E

stj sT
tj

kekeT
x2

k stj sT
tj

stisT
ti

kekeT
x2

k stj sT
tj

m
(cid:88)

j=1

d
(cid:88)

k=1

(cid:88)

i(cid:54)=j∈[m]

d
(cid:88)

k=1

stg sT
tg

kekeT
x2

k stg sT
tg

stisT
ti

kekeT
x2

k stj sT
tj

stg sT
tg

kekeT
x2

k stg sT
tg

stisT
ti

kekeT
x2

k stj sT
tj

stg sT
tg

kekeT
x2

k stg sT
tg

stisT
ti

kekeT
x2

k stj sT
tj

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

= E (cid:88)

g(cid:54)=i(cid:54)=j∈[m]

+ E (cid:88)

g=i(cid:54)=j∈[m]

+ E (cid:88)

g=j(cid:54)=i∈[m]

d
(cid:88)

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

m(m − 1)(m − 2)
m4pk1

d
(cid:88)

k=1

ek1eT
k1

kekeT
x2

k ek1eT
k1

ek2 eT
k2

kekeT
x2

k ek3 eT
k3

d
(cid:88)

k=1

k1,k2,k3=1

d
(cid:88)

d
(cid:88)

k1,k3=1

k1,k2=1

m(m − 1)
m4p2
k1

m(m − 1)
m4p2
k1

d
(cid:88)

k=1

d
(cid:88)

k=1

ek1 eT
k1

kekeT
x2

k ek1 eT
k1

ek1eT
k1

kekeT
x2

k ek3 eT
k3

ek1 eT
k1

kekeT
x2

k ek1 eT
k1

ek2eT
k2

kekeT
x2

k ek1 eT
k1

d
(cid:88)

k=1

d
(cid:88)

k=1

m(m − 1)(m − 2)
m4pk1

x4
k1

ek1eT
k1

+

d
(cid:88)

k1=1

m(m − 1)
m4p2
k1

x4
k1

ek1 eT
k1

+

d
(cid:88)

k1=1

m(m − 1)
m4p2
k1

x4
k1

ek1 eT
k1

=

+

+

=

=

d
(cid:88)

k1=1

d
(cid:88)

k=1

(cid:20) m(m − 1)(m − 2)
m4pk

x4
k +

(cid:21)

2m(m − 1)x4
k
m4p2
k

ekeT
k ;

(25) = E (cid:88)

stisT
ti

kekeT
x2

k stj sT
tj

stj sT
tj

kekeT
x2

k stj sT
tj

m
(cid:88)

j=1

d
(cid:88)

k=1

i(cid:54)=j∈[m]

k=1
(cid:20) m(m − 1)(m − 2)
m4pk

=

d
(cid:88)

k=1

x4
k +

(cid:21)

2m(m − 1)x4
k
m4p2
k

ekeT
k ;

(26) = E (cid:88)

i(cid:54)=j∈[m]

stisT
ti

kekeT
x2

k stj sT
tj

stisT
ti

kekeT
x2

k stj sT
tj

(cid:88)

i(cid:54)=j∈[m]

d
(cid:88)

k=1

d
(cid:88)

d
(cid:88)

k=1

(27)

(28)

(29)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

= E (cid:88)

i(cid:54)=j(cid:54)=g(cid:54)=h∈[m]

d
(cid:88)

k=1

stisT
ti

kekeT
x2

k stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

d
(cid:88)

k=1

i(cid:54)=j,i=g,j(cid:54)=h,g(cid:54)=h∈[m]

i(cid:54)=j,i=h,j(cid:54)=g,g(cid:54)=h∈[m]

i(cid:54)=j,i(cid:54)=g,j=h,g(cid:54)=h∈[m]

i(cid:54)=j,i(cid:54)=h,j=g,g(cid:54)=h∈[m]

i(cid:54)=j,i=g,j=h,g(cid:54)=h∈[m]

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

+ E

+ E

+ E

+ E

+ E

+ E

stisT
ti

kekeT
x2

k stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

stisT
ti

kekeT
x2

k stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

stisT
ti

kekeT
x2

k stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

stisT
ti

kekeT
x2

k stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

stisT
ti

kekeT
x2

k stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

stisT
ti

kekeT
x2

k stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

d
(cid:88)

i(cid:54)=j,i=h,j=g,g(cid:54)=h∈[m]

k=1
(cid:20) m(m − 1)(m − 2)(m − 3)
m4

=

k=1

x4
k +

4m(m − 1)(m − 2)
m4pk

x4
k +

2m(m − 1)
m4p2
k

(cid:21)

x4
k

ekeT
k .

Combing the above terms with simpliﬁcation and reformulation completes the proof of Eq. (9).

Now, we continue to prove Eq. (10).





m
(cid:88)

j=1

m
(cid:88)

j=1

m
(cid:88)

j=1

E (cid:2)SST xxT SST D(SST xxT SST )(cid:3)

= E

stj sT
tj

x

xT stj sT
tj

stj sT
tj

kekeT
x2
k

stj sT
tj



m
(cid:88)

j=1

d
(cid:88)

k=1

m
(cid:88)

j=1

m
(cid:88)

j=1



= E

stj sT
tj

xxT stj sT
tj

stj sT
tj

kekeT
x2

k stj sT
tj

m
(cid:88)

j=1

m
(cid:88)

i(cid:54)=j∈[m]

m
(cid:88)

j=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

+ E

stj sT
tj

xxT stj sT
tj

stisT
ti

kekeT
x2

k stj sT
tj

+ E (cid:88)

i(cid:54)=j∈[m]

+ E (cid:88)

i(cid:54)=j∈[m]

stisT
ti

xxT stj sT
tj

stj sT
tj

kekeT
x2

k stj sT
tj

stisT
ti

xxT stj sT
tj

stisT
ti

kekeT
x2

k stj sT
tj

,

(cid:88)

i(cid:54)=j∈[m]

d
(cid:88)

k=1

where we calculate the four terms in the last equation as shown in below:

(32) = E

stj sT
tj

xxT stj sT
tj

stj sT
tj

kekeT
x2

k stj sT
tj

m
(cid:88)

j=1

m
(cid:88)

j=1

d
(cid:88)

k=1

m
(cid:88)

j=1

d
(cid:88)

k=1

= E

stj sT
tj

xxT stj sT
tj

stj sT
tj

kekeT
x2

k stj sT
tj

stisT
ti

xxT stisT
ti

stj sT
tj

kekeT
x2

k stj sT
tj

+ E (cid:88)

i(cid:54)=j∈[m]

d
(cid:88)

k=1

(30)

(31)

(32)

(33)

(34)

(35)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

d
(cid:88)

m
(cid:88)

=

k1=1

j=1

pk1

1
m4p4
k1

+ E (cid:88)

d
(cid:88)

d
(cid:88)

i(cid:54)=j∈[m]

k1=1

q=1

ek1eT
k1

xxT ek1eT
k1

ek1eT
k1

kekeT
x2

k ek1eT
k1

d
(cid:88)

k=1

pk1 pq

1
m4p2
k1

p2
q

ek1 eT
k1

xxT ek1 eT
k1

eqeT
q

kekeT
x2

k eqeT
q

d
(cid:88)

k=1

x4
k
m3p3
k

ekeT

k +

d
(cid:88)

k=1

(m2 − m)x4
k
m4p2
k

ekeT
k

1
m3p3
k

+

m − 1
m3p2
k

)x4

kekeT
k ;

(33) = E

stj sT
tj

xxT stj sT
tj

stisT
ti

kekeT
x2

k stj sT
tj

(cid:88)

i(cid:54)=j∈[m]

d
(cid:88)

k=1

stg sT
tg

xxT stg sT
tg

stisT
ti

kekeT
x2

k stj sT
tj

stg sT
tg

xxT stg sT
tg

stisT
ti

kekeT
x2

k stj sT
tj

stg sT
tg

xxT stg sT
tg

stisT
ti

kekeT
x2

k stj sT
tj

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

m(m − 1)(m − 2)
m4pk1

ek1 eT
k1

xxT ek1eT
k1

ek2 eT
k2

kekeT
x2

k ek3 eT
k3

d
(cid:88)

k=1

ek1eT
k1

xxT ek1 eT
k1

ek1eT
k1

kekeT
x2

k ek3eT
k3

ek1eT
k1

xxT ek1 eT
k1

ek2eT
k2

kekeT
x2

k ek1eT
k1

d
(cid:88)

k=1

d
(cid:88)

k=1

=

=

d
(cid:88)

k=1

d
(cid:88)

(
k=1

m
(cid:88)

j=1

= E (cid:88)

g(cid:54)=i(cid:54)=j∈[m]

+ E (cid:88)

g=i(cid:54)=j∈[m]

+ E (cid:88)

g=j(cid:54)=i∈[m]

d
(cid:88)

k1,k2,k3=1

d
(cid:88)

d
(cid:88)

k1,k3=1

k1,k2=1

m(m − 1)
m4p2
k1

m(m − 1)
m4p2
k1

=

+

+

=

=

d
(cid:88)

k1=1

d
(cid:88)

k=1

(cid:20) m(m − 1)(m − 2)
m4pk

x4
k +

2m(m − 1)
m4p2
k

(cid:21)

x4
k

ekeT
k ;

(34) = E (cid:88)

i(cid:54)=j∈[m]

stisT
ti

xxT stj sT
tj

stj sT
tj

kekeT
x2

k stj sT
tj

m
(cid:88)

j=1

d
(cid:88)

k=1

= E (cid:88)

i(cid:54)=j(cid:54)=g∈[m]

+ E (cid:88)

i(cid:54)=j=g∈[m]

+ E (cid:88)

i=g(cid:54)=j∈[m]

stisT
ti

xxT stj sT
tj

stg sT
tg

kekeT
x2

k stg sT
tg

stisT
ti

xxT stj sT
tj

stg sT
tg

kekeT
x2

k stg sT
tg

stisT
ti

xxT stj sT
tj

stg sT
tg

kekeT
x2

k stg sT
tg

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

m(m − 1)(m − 2)
m4pk1

x4
k1

ek1eT
k1

+

d
(cid:88)

k1=1

m(m − 1)
m4p2
k1

x4
k1

ek1eT
k1

+

d
(cid:88)

k1=1

m(m − 1)
m4p2
k1

x4
k1

ek1eT
k1

(36)

(37)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

d
(cid:88)

k1,k2,k3=1

m(m − 1)(m − 2)
m4pk3

ek1 eT
k1

xxT ek2eT
k2

ek3 eT
k3

kekeT
x2

k ek3 eT
k3

d
(cid:88)

k=1

=

+

+

=

d
(cid:88)

d
(cid:88)

k1,k3=1

k2,k3=1

d
(cid:88)

k1,k3=1

m(m − 1)
m4p2
k3

m(m − 1)
m4p2
k3

ek1eT
k1

xxT ek3 eT
k3

ek3eT
k3

kekeT
x2

k ek3eT
k3

ek3eT
k3

xxT ek2 eT
k2

ek3eT
k3

kekeT
x2

k ek3eT
k3

m(m − 1)(m − 2)
m4pk3

xk1x3
k3

ek1eT
k3

+

d
(cid:88)

k1,k3=1

m(m − 1)
m4p2
k3

xk1x3
k3

ek1 eT
k3

+

d
(cid:88)

k3=1

m(m − 1)
m4p2
k3

x4
k3

ek3 eT
k3

=

m(m − 1)(m − 2)
m4

xxT D({

}) +

xxT D({

}) +

x2
k
pk

m(m − 1)
m4

x2
k
p2
k

m(m − 1)
m4

d
(cid:88)

k=1

x4
k
p2
k

ekeT
k ;

(38)

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

stisT
ti

xxT stj sT
tj

stisT
ti

kekeT
x2

k stj sT
tj

(35) = E (cid:88)

i(cid:54)=j∈[m]

= E (cid:88)

i(cid:54)=j(cid:54)=g(cid:54)=h∈[m]

(cid:88)

i(cid:54)=j∈[m]

d
(cid:88)

k=1

stisT
ti

xxT stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

i(cid:54)=j,i=g,j(cid:54)=h,g(cid:54)=h∈[m]

i(cid:54)=j,i=h,j(cid:54)=g,g(cid:54)=h∈[m]

i(cid:54)=j,i(cid:54)=g,j=h,g(cid:54)=h∈[m]

i(cid:54)=j,i(cid:54)=h,j=g,g(cid:54)=h∈[m]

i(cid:54)=j,i=g,j=h,g(cid:54)=h∈[m]

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

sti sT
ti

xxT stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

sti sT
ti

xxT stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

stisT
ti

xxT stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

stisT
ti

xxT stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

stisT
ti

xxT stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

stisT
ti

xxT stj sT
tj

stg sT
tg

kekeT
x2

k sthsT
th

i(cid:54)=j,i=h,j=g,g(cid:54)=h∈[m]

d
(cid:88)

k1,k2,k3,k4=1

m(m − 1)(m − 2)(m − 3)
m4

xk1xk2 ek1 eT
k2

ek3eT
k3

kekeT
x2

k ek4eT
k4

d
(cid:88)

k=1

k1,k2,k4=1

d
(cid:88)

d
(cid:88)

d
(cid:88)

k1,k2,k3=1

k1,k2,k3=1

m(m − 1)(m − 2)
m4pk1

m(m − 1)(m − 2)
m4pk1

m(m − 1)(m − 2)
m4pk2

xk1 xk2ek1eT
k2

ek1 eT
k1

kekeT
x2

k ek4 eT
k4

xk1 xk2ek1eT
k2

ek3 eT
k3

x2
kekeT

k ek1 eT
k1

xk1 xk2ek1eT
k2

ek3 eT
k3

kekeT
x2

k ek2 eT
k2

d
(cid:88)

k=1

d
(cid:88)

k=1

d
(cid:88)

k=1

+ E

+ E

+ E

+ E

+ E

+ E

=

+

+

+

+

+

+

=

+

+

d
(cid:88)

k1,k2

d
(cid:88)

k1,k2=1

d
(cid:88)

k1,k2=1

d
(cid:88)

k1=1

d
(cid:88)

k1,k2=1

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

d
(cid:88)

k1,k2,k4=1

m(m − 1)(m − 2)
m4pk2

xk1 xk2ek1eT
k2

ek2 eT
k2

kekeT
x2

k ek4 eT
k4

d
(cid:88)

k=1

m(m − 1)
m4pk1 pk2

m(m − 1)
m4pk1 pk2

xk1 xk2ek1eT
k2

ek1eT
k1

kekeT
x2

k ek2eT
k2

xk1xk2 ek1eT
k2

ek2eT
k2

kekeT
x2

k ek1eT
k1

d
(cid:88)

k=1

d
(cid:88)

k=1

m(m − 1)(m − 2)(m − 3)
m4

xk1 x3
k2

ek1eT
k2

+

m(m − 1)(m − 2)
m4pk1

x4
k1

ek1eT
k1

d
(cid:88)

k1=1

m(m − 1)(m − 2)
m4pk1

x4
k1

ek1eT
k1

+

d
(cid:88)

k1,k2=1

m(m − 1)(m − 2)
m4pk2

xk1x3
k2

ek1eT
k2

m(m − 1)(m − 2)
m4pk2

xk1x3
k2

ek1eT
k2

+

d
(cid:88)

k1=1

m(m − 1)
m4p2
k1

x4
k1

ek1 eT
k1

+

d
(cid:88)

k1=1

m(m − 1)
m4p2
k1

x4
k1

ek1 eT
k1

=

m(m − 1)(m − 2)(m − 3)
m4

xxT D({x2

k}) +

m(m − 1)(m − 2)
m4

d
(cid:88)

k=1

x4
k
pk

ekeT
k

+

m(m − 1)(m − 2)
m4

d
(cid:88)

k=1

x4
k
pk

ekeT

k +

m(m − 1)(m − 2)
m4

xxT D({

})

x2
k
pk

+

m(m − 1)(m − 2)
m4

xxT D({

}) +

x2
k
pk

2m(m − 1)
m4

d
(cid:88)

k=1

x4
k
p2
k

ekeT
k .

Combing the above terms with simpliﬁcation and reformulation completes the proof of Eq. (10).

Finally, we have to prove Eq. (11).

E (cid:2)(SST xxT SST )2(cid:3) = E

stj sT
tj

x

xT stj sT
tj

)2





m
(cid:88)

j=1



(

m
(cid:88)

j=1

(cid:88)

i(cid:54)=j∈[m]

= E(

stj sT
tj

xxT stj sT
tj

+

stisT
ti

xxT stj sT
tj

)2

m
(cid:88)

j=1

m
(cid:88)

j=1

m
(cid:88)

j=1

= E(

stj sT
tj

xxT stj sT
tj

)2

+ E(

(cid:88)

i(cid:54)=j∈[m]

sti sT
ti

xxT stj sT
tj

)2

+ E

stj sT
tj

xxT stj sT
tj

stisT
ti

xxT stj sT
tj

+ E (cid:88)

i(cid:54)=j∈[m]

sti sT
ti

xxT stj sT
tj

stj sT
tj

xxT stj sT
tj

,

(cid:88)

i(cid:54)=j∈[m]

m
(cid:88)

j=1

where we calculate the four terms in the last equation as shown in below:

(40) = E

stj sT
tj

xxT stj sT
tj

stj sT
tj

xxT stj sT
tj

stisT
ti

xxT stisT
ti

stj sT
tj

xxT stj sT
tj

+ E (cid:88)

i(cid:54)=j∈[m]

m
(cid:88)

j=1

(39)

(40)

(41)

(42)

(43)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

=

d
(cid:88)

m
(cid:88)

k=1

j=1

pk

1
m4p4
k

ekeT

k xxT ekeT

k ekeT

k xxT ekeT
k

+ E (cid:88)

d
(cid:88)

d
(cid:88)

i(cid:54)=j∈[m]

k=1

q=1

pkpq

1
m4p2
kp2
q

ekeT

k xxT ekeT

k eqeT

q xxT eqeT
q

=

=

d
(cid:88)

k=1

d
(cid:88)

(
k=1

x4
k
m3p3
k

1
m3p3
k


ekeT

k +

d
(cid:88)

k=1

(m2 − m)x4
k
m4p2
k

ekeT
k

+

m − 1
m3p2
k

)x4

kekeT
k ;

(41) = E



(cid:88)

stisT
ti

xxT stj sT
tj

(cid:88)

stisT
ti

xxT stj sT
tj





(44)

stisT
ti

xxT stj sT
tj

xxT sthsT
th

+ E

i(cid:54)=j∈[m]
stg sT
tg

(cid:88)

stisT
ti

xxT stj sT
tj

stg sT
tg

xxT sthsT
th

stisT
ti

xxT stj sT
tj

stg sT
tg

xxT sthsT
th

+ E

stisT
ti

xxT stj sT
tj

stg sT
tg

xxT sthsT
th

stisT
ti

xxT stj sT
tj

stg sT
tg

xxT sthsT
th

+ E

stisT
ti

xxT stj sT
tj

stg sT
tg

xxT sthsT
th

i(cid:54)=j,i=g,j(cid:54)=h,g(cid:54)=h∈[m]

(cid:88)

i(cid:54)=j,i(cid:54)=g,j=h,g(cid:54)=h∈[m]
(cid:88)

i(cid:54)=j,i=g,j=h,g(cid:54)=h∈[m]

stisT
ti

xxT stj sT
tj

stg sT
tg

xxT sthsT
th

m(m − 1)(m − 2)(m − 3)
m4

xk1xk2xk3 xk4ek1eT
k2

ek3 eT
k4

m(m − 1)(m − 2)
m4pk1

m(m − 1)(m − 2)
m4pk2

x2
k1

xk2xk4ek1 eT
k2

ek1eT
k4

+

x2
k1

xk2 xk3ek1eT
k2

ek3 eT
k1

xk1 x2
k2

xk3ek1 eT
k2

ek3eT
k2

+

xk1x2
k2

xk4ek1eT
k2

ek2 eT
k4

d
(cid:88)

d
(cid:88)

k1,k2,k3=1

k1,k2,k4=1

m(m − 1)(m − 2)
m4pk1

m(m − 1)(m − 2)
m4pk2

x2
k1

x2
k2

ek1eT
k2

ek1eT
k2

+

x2
k1

x2
k2

ek1eT
k2

ek2eT
k1

d
(cid:88)

k1,k2=1

m(m − 1)
m4pk1 pk2

m(m − 1)(m − 2)(m − 3)
m4

xk1xk4 ek1 eT
k4

m(m − 1)(m − 2)
m4pk1

x3
k1

xk4 ek1 eT
k4

+

m(m − 1)(m − 2)
m4pk1

x2
k1

ek1 eT
k1

m(m − 1)(m − 2)
m4pk2

xk1x3
k2

ek1 eT
k2

+

m(m − 1)(m − 2)
m4

xk1 xk4ek1eT
k4

d
(cid:88)

d
(cid:88)

k2=1

k1=1

d
(cid:88)

d
(cid:88)

x2
k2

x2
k2
pk2

k2=1

k1,k4=1

m(m − 1)
m4p2
k1

x4
k1

ek1eT
k1

+

k1=1
(cid:107)x(cid:107)2

k2=1
2m(m − 1)(m − 2)(m − 3)

m4

d
(cid:88)

x2
k2
pk2

d
(cid:88)

k1=1

m(m − 1)
m4pk1

x2
k1

ek1 eT
k1

xxT +

m(m − 1)(m − 2)
m4

D({

})xxT

x2
k
pk

i(cid:54)=j∈[m]

= E (cid:88)

i(cid:54)=j(cid:54)=g(cid:54)=h∈[m]

(cid:88)

+ E

+ E

+ E

i(cid:54)=j,i=h,j(cid:54)=g,g(cid:54)=h∈[m]
(cid:88)

i(cid:54)=j,i(cid:54)=h,j=g,g(cid:54)=h∈[m]
(cid:88)

i(cid:54)=j,i=h,j=g,g(cid:54)=h∈[m]

k1,k2,k3,k4=1

d
(cid:88)

d
(cid:88)

d
(cid:88)

k1,k2,k4=1

k1,k2,k3=1

d
(cid:88)

k1,k2

d
(cid:88)

m(m − 1)
m4pk1 pk2

d
(cid:88)

x2
k2

k2=1

k1,k4=1

d
(cid:88)

d
(cid:88)

k1,k4=1

k1,k2=1

d
(cid:88)

=

+

+

+

=

+

+

+

=

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

(cid:107)x(cid:107)2

2m(m − 1)(m − 2)

+

m4

d
(cid:88)

k=1

x2
k
pk

ekeT

k +

m(m − 1)(m − 2)
m4

xxT D({

})

x2
k
pk

+

m(m − 1)(m − 2)
m4

d
(cid:88)

k=1

x2
k
pk

xxT +

m(m − 1)
m4

d
(cid:88)

k=1

x4
k
p2
k

ekeT

k +

m(m − 1)
m4

d
(cid:88)

d
(cid:88)

x2
k
pk

k=1

k=1

x2
k
pk

ekeT
k ;

(45)

(42) = E

stj sT
tj

xxT stj sT
tj

stisT
ti

xxT stj sT
tj

(cid:88)

i(cid:54)=j∈[m]
stisT
ti

stg sT
tg

xxT stg sT
tg

xxT stj sT
tj

+ E (cid:88)

stg sT
tg

xxT stg sT
tg

stisT
ti

xxT stj sT
tj

g=i(cid:54)=j∈[m]

stg sT
tg

xxT stg sT
tg

stisT
ti

xxT stj sT
tj

d
(cid:88)

k1,k2,k3=1

m(m − 1)(m − 2)
m4pk1

ek1eT
k1

xxT ek1eT
k1

ek2 eT
k2

xxT ek3eT
k3

m
(cid:88)

j=1

= E (cid:88)

g(cid:54)=i(cid:54)=j∈[m]

+ E (cid:88)

g=j(cid:54)=i∈[m]

=

+

=

d
(cid:88)

k1,k3=1

d
(cid:88)

k1,k3=1

m(m − 1)
m4p2
k1

ek1eT
k1

xxT ek1 eT
k1

ek1eT
k1

xxT ek3 eT
k3

+

ek1 eT
k1

xxT ek1eT
k1

ek2 eT
k2

xxT ek1 eT
k1

d
(cid:88)

k1,k2=1

m(m − 1)
m4p2
k1

m(m − 1)(m − 2)
m4pk1

x3
k1

xk3 ek1eT
k3

+

d
(cid:88)

k1,k3=1

m(m − 1)
m4p2
k1

x3
k1

xk3ek1eT
k3

+

d
(cid:88)

k1=1

m(m − 1)
m4p2
k1

x4
k1

ek1eT
k1

=

m(m − 1)(m − 2)
m4

x2
k
pk

D({

})xxT +

D({

})xxT +

m(m − 1)
m4

x2
k
p2
k

m(m − 1)
m4

d
(cid:88)

k=1

x4
k
p2
k

ekeT
k ;

(43) =

m(m − 1)(m − 2)
m4

xxT D({

}) +

xxT D({

}) +

x2
k
pk

m(m − 1)
m4

x2
k
p2
k

m(m − 1)
m4

d
(cid:88)

k=1

x4
k
p2
k

ekeT
k .

Combing the above terms with simpliﬁcation and reformulation completes the proof of Eq. (11). To this end, we complete
the whole proof.

2.3. Proof of Lemma 2

Proof. According to the setting, we have that

(cid:107)SST xxT SST (cid:107)2

(a)
= (cid:107)SST x(cid:107)2

2 = (cid:107)

stj sT
tj

x(cid:107)2

2 = (cid:107)

m
(cid:88)

j=1

m
(cid:88)

j=1

1
mptj

xtj etj (cid:107)2
2

= (cid:107)

m
(cid:88)

d
(cid:88)

j=1

k=1

δtj k
mpk

xkek(cid:107)2

2 =

d
(cid:88)

m
(cid:88)
(
k=1

j=1

δtj kxk
mpk

)2 (b)
=

(cid:88)

m
(cid:88)
(
k∈Γ

j=1

δtj kxk
mpk

)2,

t=1 be a set containing at most m different elements of [d] with its cardinality |Γ| ≤ m.

where we let Γ = {γt}|Γ|
In Eq. (48), (a) follows because SST xxT SST is a positive semideﬁnite matrix of rank 1, δtj k returns 1 only when tj = k
and 0 otherwise, and P(δtj k = 1) = P(tj = k) = pk. (b) holds due to that we perform random sampling with replacement
m times on the d entries of x ∈ Rd, and consequently at most m certain different entries from x are sampled.
Let k = γ1 with γ1 ∈ Γ, and we ﬁrst bound | (cid:80)m
j ∈ [m]. We can easily check that {aj}m
continue our following analysis. We see that

|. Deﬁne a random variable aj =
m for all
j=1 are independent with E [aj] = 0, so that we can leverage Theorem 3 to

δtj γ1 xγ1
mpγ1

δtj γ1 xγ1
mpγ1

− xγ1

j=1

max
j∈[m]

|aj| = max{

|xγ1 |
m

(

1
pγ1

− 1),

} ≤

|xγ1 |
m

|xγ1|
mpγ1

,

(46)

(47)

(48)

(49)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

and

m
(cid:88)

j=1

E (cid:2)a2

(cid:3) =

j

x2
γ1
mpγ1

−

x2
γ1
m

.

(50)

Thus, applying Theorem 3 with R = |xγ1 |
mpγ1

and σ2 =

x2
γ1
mpγ1

−

x2
γ1
m obtains that

P(|

m
(cid:88)

j=1

aj| ≥ (cid:15)) ≤ 2 exp(

−(cid:15)2/2

x2
γ1

/(mpγ1) − x2
γ1

/m + |xγ1 |(cid:15)/(3mpγ1)

),

(51)

whose RHS is denoted by ηγ1. Then, with probability at least 1 − ηγ1 we have | (cid:80)m
|xγ1 | + (cid:15). We then replace (cid:15) by other variables to obtain that

j=1 aj| ≤ (cid:15), i.e., | (cid:80)m

j=1

δtj γ1 xγ1
mp1

| ≤

|xγ1| + (cid:15) = |xγ1 | + log(

(cid:34)

2
ηγ1

)

|xγ1|
3mpγ1

(cid:115)

+ |xγ1|

1
9m2p2
γ1

+

2
log(2/ηγ1)

(

1
mpγ1

−

1
m

(cid:35)

)

,

(52)

| for any other k ∈ [d]. The lemma then follows by using the union bound

which is denoted by f (xγ1, ηγ1 , m).
In a similar way, we can bound | (cid:80)m
j=1
over cases for all k ∈ [d].

δtj kxk
mpk

2.4. Proof of Theorem 1

Proof. We have to prove that the unbiased estimator for original covariance matrix C is Eq. (1), i.e., Ce = (cid:98)C1 − (cid:98)C2,
where (cid:98)C1 = m
1
.
1+(m−1)pki
Note that each Si is created by running Algorithm 1, and {Si}n
i=1 are independent matrices. Thus, taking all summands
E (cid:2)SiST

(cid:3) together and leveraging Eq. (7) in Lemma 1 achieves the expectation of (cid:98)C1,

i )D(bi) with bki =

i , and (cid:98)C2 = m

i=1 SiST

D(SiST

i SiST

i SiST

i xixT

i xixT

mn−n

mn−n

(cid:80)n

(cid:80)n

i SiST
i

i xixT

i=1

E[ (cid:98)C1] =

m
nm − n

E

=

1
nm − n

n
(cid:88)

d
(cid:88)

i=1

k=1

n
(cid:88)

i=1

x2
ki
pki

SiST

i xixT

i SiST

i =

ekeT

k +

XXT .

1
n

m
nm − n

n
(cid:88)

(cid:34) d

(cid:88)

i=1

k=1

x2
ki
mpki

ekeT

k +

m − 1
m

xixT
i

(cid:35)

Eq. (53) indicates that (cid:98)C1 is a biased estimator for the original covariance matrix C = 1
need to apply a debiasing procedure to (cid:98)C1 to get an unbiased estimator. By Eq. (8) in Lemma 1, it can be shown that

n XXT = 1

i=1 xixT

n

i . We still

(cid:80)n

E[ (cid:98)C2] =

m
nm − n

n
(cid:88)

i=1

E (cid:2)D(SiST

i xixT

i SiST

i )(cid:3) D(bi) =

1
nm − n

n
(cid:88)

d
(cid:88)

i=1

k=1

x2
ki
pki

ekeT
k .

Combing Eq. (53) with Eq. (54), we immediately see that Ce = (cid:98)C1 − (cid:98)C2 is unbiased for C.

2.5. Proof of Theorem 2

Proof. Here, we have to bound the error (cid:107)Ce−C(cid:107)2. To make the representation compact, we deﬁne Ai = Ai1 −Ai2 −Ai3
with Ai1 = mSiST
i )D(bi)
It
is straightforward to see that {Ai}n
i=1 are independent zero-mean random matrices, which are exactly the setting of the
Matrix Bernstein inequality, as shown in Theorem 3. To bound (cid:107)Ce −C(cid:107)2 via Theorem 3, we need to calculate the relevant
parameters R and σ2 that characterize the range and variance of Ai respectively.

i=1 Ai = Ce − C holds.

, Ai2 = mD(SiST

n . Then, (cid:80)n

, Ai3 = xixT

i xixT
nm−n

i SiST

i SiST

i xixT

nm−n

i

i

We ﬁrst derive R by bounding (cid:107)Ai(cid:107)2 so that (cid:107)Ai(cid:107)2 ≤ R for all i ∈ [n]. Expanding (cid:107)Ai(cid:107)2 gets that

(cid:107)Ai(cid:107)2 = (cid:107)Ai1 − Ai2 − Ai3(cid:107)2 ≤ (cid:107)Ai1 − Ai2(cid:107)2 + (cid:107)Ai3 (cid:107)2

(53)

(54)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

≤(cid:107)Ai1 (cid:107)2 + (cid:107)Ai3(cid:107)2.

The last inequality in Eq. (55) results from

(cid:107)Ai1 − Ai2 (cid:107)2 = max
k∈[d]

|λk(Ai1 − Ai2 )|

(a)
≤ max{|λd(Ai1) − λ1(Ai2)|, |λ1(Ai1 ) − λd(Ai2 )|}
(b)
= max{λ1(Ai2 ), |λ1(Ai1 ) − λd(Ai2)|}
(c)
= max{λ1(Ai2 ), λ1(Ai1) − λd(Ai2)}
(d)
≤ λ1(Ai1 )
(e)
= (cid:107)Ai1(cid:107)2,

where λk(·) is the k-th largest eigenvalue.

(a) follows from that λk(Ai1) − λ1(Ai2) ≤ λk(Ai1 − Ai2) ≤ λk(Ai1) − λd(Ai2) for any k ∈ [d], which can be proved
by combining Theorem 4 with the fact that λd(−Ai2) = −λ1(Ai2) and λ1(−Ai2 ) = −λd(Ai2 ) for Ai2 ∈ Rd×d.
(b) holds because of that λk≥2(Ai1 ) = 0 since Ai1 is a positive semideﬁnite matrix of rank 1, and λk∈[d](Ai2 ) ≥ 0 since
Ai2 is positive semideﬁnite.
(c) follows owing to that λ1(Ai1) = Tr(Ai1) ≥ Tr(Ai2 ) = (cid:80)d
k=1 λk(Ai2 ) ≥ λd(Ai2 ) ≥ 0, where the ﬁrst equality
holds because λk≥2(Ai1 ) = 0, the ﬁrst inequality results from the fact that the diagonal matrix Ai2 is constructed by the
diagonal elements of Ai1 multiplied by positive scalars not bigger than 1, and the second inequality is the consequence of
λk∈[d](Ai2 ) ≥ 0.

(d) results from that λk∈[d](Ai2 ) ≥ 0.

(e) follows owing to that Ai1 is positive semideﬁnite.

Now, we only need to bound (cid:107)Ai1(cid:107)2 and (cid:107)Ai3 (cid:107)2. We have that

Then, Lemma 2 reveals that with probability at least 1 − (cid:80)d

k=1 ηki,

(cid:107)Ai3 (cid:107)2 = (cid:107)

(cid:107)2 =

xixT
i
n

(cid:107)xi(cid:107)2
2
n

.

(cid:107)Ai1(cid:107)2 ≤

m
nm − n

(cid:88)

k∈Γi

f 2(xki, ηki, m),

where Γi = {γti}|Γi|
f (xki, ηki, m) = |xki| + log( 2
ηki

(cid:104) |xki|
3mpki

)

+ |xki|

(cid:113) 1

9m2p2
ki

+

2
log(2/ηki) (

1
mpki

− 1

(cid:105)
m )

.

t=1 is a set occupying at most m different elements of [d] with its cardinality |Γi| ≤ m, and

We derive the similar results for all {xi}n

i=1. Then, by union bound, with probability at least 1 − (cid:80)n

i=1

(cid:80)d

k=1 ηki, we have

R = max
i∈[n]

(cid:34)

m
nm − n

(cid:88)

k∈Γi

f 2(xki, ηki, m) +

(cid:35)

.

(cid:107)xi(cid:107)2
2
n

Applying the well known inequality ((cid:80)n

t=1 at)2 ≤ n (cid:80)n

t=1 a2

t , we have

f 2(xki, ηki, m) ≤ 3x2

ki + 3 log2(

2
ηki
2
ηki

)

)

x2
ki
9m2p2
ki
2x2
ki
3m2p2
ki

+ 3 log2(

+ log(

2
ηki

)

)

2
ηki
6x2
ki
mpki

.

≤ 3x2

ki + log2(

x2
ki
9m2p2
ki

+ 6 log(

)(

2
ηki

x2
ki
mpki

−

x2
ki
m

)

(55)

(56)

(57)

(58)

(59)

(60)

(61)

(62)

(63)

(64)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

Before continuing characterizing R in Eq. (63), we set the sampling probabilities as pki = α |xki|
(cid:107)xi(cid:107)1
easy to check that (cid:80)d
k=1 pki = 1. For 0 < α < 1, we also have pki ≥ α |xki|
(cid:107)xi(cid:107)1
term of Eq. (64) respectively getting that

+ (1 − α) x2

ki
(cid:107)xi(cid:107)2
2

. It is

, then plugging it in the second and third

f 2(xki, ηki, m) ≤ 3x2

ki + log2(

2
ηki

)

2(cid:107)xi(cid:107)2
1
3m2α2 + log(

2
ηki

)

6|xki|(cid:107)xi(cid:107)1
mα

.

Equipped with Eq. (63) and setting ηki = η
(cid:80)n

(cid:80)d

k=1 ηki = 1 − η by

i=1

nd for all i ∈ [n] and k ∈ [d], we bound R with probability at least 1 −

R ≤ max
i∈[n]

(cid:34)

m
nm − n

(cid:88)

(cid:16)

k∈Γi

3x2

ki + log2(

2nd
η

)

2(cid:107)xi(cid:107)2
1
3m2α2 + log(

2nd
η

)

6|xki|(cid:107)xi(cid:107)1
mα

(cid:17)

+

(cid:107)xi(cid:107)2
2
n

(cid:35)

≤ max
i∈[n]

≤ max
i∈[n]

(cid:16)

(cid:20) 2
n
(cid:20) 7(cid:107)xi(cid:107)2
n

2

3(cid:107)xi(cid:107)2

2 + log2(

+ log2(

2nd
η

)

2(cid:107)xi(cid:107)2
1
3mα2 + log(
(cid:21)

2nd
η

)

14(cid:107)xi(cid:107)2
1
nmα2

,

2nd
η

)

6(cid:107)xi(cid:107)2
1
mα

(cid:17)

+

(cid:107)xi(cid:107)2
2
n

(cid:21)

(65)

(66)

where the second inequality follows from that m
α ≤ 1 and log( 2nd
At this stage, we have to derive σ2 by only bounding for (cid:107) (cid:80)n
obtains that

η ) ≥ 1 for n ≥ 1, d ≥ 2, and η ≤ 1.

i=1

m−1 ≤ 2 for m ≥ 2 and |Γi| ≤ m, and the last inequality results from that

E [AiAi] (cid:107)2 since Ai is symmetric. Expanding E [AiAi]

0 (cid:22) E [AiAi] = E [Ai1Ai1 + Ai2 Ai2 + Ai3Ai3 − Ai1 Ai2 − Ai2Ai1
−Ai1 Ai3 − Ai3Ai1 + Ai2 Ai3 + Ai3Ai2] ,

in RHS of which, we bound the expectation of each term. Speciﬁcally, invoking Lemma 1, we have that

n2E [AiAi] =

4
m(m − 1)p2
ki

+

d
(cid:88)

(cid:20)

k=1
(cid:124)

(cid:34)

d
(cid:88)

+

k=1
(cid:124)

(cid:107)xi(cid:107)2

2(m − 2)

m(m − 1)

+

1
m(m − 1)

(cid:123)(cid:122)
2(cid:13)

1
(m − 1)2mp3
ki
(cid:123)(cid:122)
1(cid:13)

d
(cid:88)

k=1

x2
ki
pki

(cid:35)

x2
ki
pki

(cid:21)

kiekeT
x4
k

(cid:125)

(cid:34)

(cid:107)xi(cid:107)2

ekeT
k

+

(cid:125)

(cid:124)

2(m2 − 5m + 6)
m(m − 1)

+

m − 2
m(m − 1)

(cid:35)

d
(cid:88)

k=1

x2
ki
pki

xixT
i

(cid:125)

(cid:123)(cid:122)
3(cid:13)

(cid:125)

})xixT
i

+

+

6(m − 2)
m(m − 1)pki

+

(m − 2)(m − 3)
m(m − 1)

(cid:21)

kiekeT
x4
k

+

2(m − 2)
m(m − 1)
(cid:124)

xixT
i

D({

x2
ki
pki

})

+

(cid:125)

1
m(m − 1)
(cid:124)

xixT
i

D({

(cid:123)(cid:122)
5(cid:13)

x2
ki
p2
ki

2(m − 2)
m(m − 1)
(cid:124)

})

+

(cid:125)

x2
ki
pki

D({

(cid:123)(cid:122)
6(cid:13)

1
m(m − 1)
(cid:124)

x2
ki
p2
ki

D({

(cid:123)(cid:122)
7(cid:13)

})xixT
i

(cid:125)

(cid:123)(cid:122)
4(cid:13)

d
(cid:88)

(cid:20)

k=1

+ D(bi)D(bi)

(cid:124)

1
m(m − 1)2p3
ki

+

7
m(m − 1)p2
ki
(cid:123)(cid:122)
8(cid:13)

2xixT
+ (cid:107)xi(cid:107)2
i
(cid:123)(cid:122)
(cid:125)
(cid:124)
9(cid:13)

+

1
(m − 1)pki

d
(cid:88)

(
k=1
(cid:124)

d
(cid:88)

(cid:20)

− 2

k=1

(cid:124)

1
m(m − 1)2p3
ki

+

6
m(m − 1)p2
ki
(cid:123)(cid:122)
12(cid:13)

(cid:123)(cid:122)
10(cid:13)

(cid:125)

(cid:124)

+

3(m − 2)
m(m − 1)pki

(cid:21)

+ 1)x2

kiekeT
k

D(bi)xixT
i

+ xixT
i

+ 1)x2

kiekeT
k

D(bi)

d
(cid:88)

(
k=1

1
(m − 1)pki
(cid:123)(cid:122)
11(cid:13)

kiekeT
x4
k

D(bi)

−

xixT
i

D({

})D(bi)

3(m − 2)
m(m − 1)
(cid:124)

(cid:125)

(cid:123)(cid:122)
13(cid:13)

(cid:125)

(cid:125)

(cid:125)

x2
ki
pki

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

−

(m − 2)(m − 3)
m(m − 1)

xixT
i

D({x2

−

ki})D(bi)
(cid:125)

3(m − 2)
m(m − 1)
(cid:124)

D(bi)D({

})xixT
i

−

x2
ki
pki

(m − 2)(m − 3)
m(m − 1)

D(bi)D({x2

(cid:123)(cid:122)
15(cid:13)

(cid:125)

(cid:124)

(cid:123)(cid:122)
16(cid:13)

ki})xixT
i
(cid:125)

(cid:123)(cid:122)
14(cid:13)

x2
ki
(m − 1)pki
(cid:123)(cid:122)
17(cid:13)

ekeT

k xixT
i

2xixT
− (cid:107)xi(cid:107)2
i
(cid:123)(cid:122)
(cid:125)
(cid:124)
18(cid:13)

−

(cid:125)

d
(cid:88)

k=1
(cid:124)

x2
ki
(m − 1)pki
(cid:123)(cid:122)
19(cid:13)

xixT

i ekeT
k

2xixT
− (cid:107)xi(cid:107)2
i
(cid:123)(cid:122)
(cid:125)
(cid:124)
20(cid:13)

−

(cid:125)

1
m(m − 1)
(cid:124)

xixT
i

D({

})D(bi)

x2
ki
p2
ki

(cid:125)

(cid:123)(cid:122)
21(cid:13)

(cid:124)

d
(cid:88)

k=1
(cid:124)

−

Because of the limited space, D({ x2
ki
pki
which is also extended to other similar notations.

}) is to denote a square diagonal matrix in Rd×d with { x2
ki
pki

}d
k=1 on its diagonal,

−

1
m(m − 1)
(cid:124)

D(bi)D({

})xixT
i

.

x2
ki
p2
ki

(cid:125)

(cid:123)(cid:122)
22(cid:13)

In Eq. (67), it can be checked that for m ≥ 2, we have

10(cid:13) − 17(cid:13) = 0;

11(cid:13) − 19(cid:13) = 0;

4(cid:13) − 13(cid:13) + 5(cid:13) − 14(cid:13) − 21(cid:13) =

xixT
i
m(m − 1)

D({

((m − 1)/pki)x2
ki
1 + (m − 1)pki

+

(m − 2)(m + 1 − 1/pki)x2
ki
1 + (m − 1)pki

});

6(cid:13) − 15(cid:13) + 7(cid:13) − 16(cid:13) − 22(cid:13) = D({

3(cid:13) + 9(cid:13) − 18(cid:13) − 20(cid:13) =

((m − 1)/pki)x2
ki
1 + (m − 1)pki

+

(cid:34)

(6 − 4m)(cid:107)xi(cid:107)2
2
m(m − 1)

+

m − 2
m(m − 1)

(m − 2)(m + 1 − 1/pki)x2
ki
1 + (m − 1)pki
(cid:35)
d
(cid:88)

xixT
i

x2
ki
pki

k=1

})

xixT
i
m(m − 1)

;

(cid:22)

1
m

d
(cid:88)

k=1

x2
ki
pki

xixT
i ;

8(cid:13) − 12(cid:13) (cid:22) 0;

1(cid:13) (cid:22)

d
(cid:88)

(cid:20) 8x4
ki
m2p2
ki

+

(cid:21)

4x4
ki
m3p3
ki

ekeT
k ;

k=1

d
(cid:88)

(cid:34)

k=1

2(cid:13) (cid:22)

2x2
(cid:107)xi(cid:107)2
ki
mpki

+

2x2
ki
m2pki

(cid:35)

d
(cid:88)

k=1

x2
ki
pki

ekeT
k .

Then, applying Eq. (67) and Eq. (68) obtains that

0 (cid:22) E [AiAi] (cid:22)

+

xixT
i
n2m(m − 1)

D({

(cid:34)

d
(cid:88)

1
n2

8x4
ki
m2p2
ki

+

k=1
((m − 1)/pki)x2
ki
1 + (m − 1)pki

d
(cid:88)

+

+

2x2
ki
m2pki

2x2
(cid:107)xi(cid:107)2
ki
mpki

4x4
ki
m3p3
ki
k=1
(m − 2)(m + 1 − 1/pki)x2
ki
1 + (m − 1)pki

+

x2
ki
pki

})

(cid:35)

ekeT
k

+ D({

((m − 1)/pki)x2
ki
1 + (m − 1)pki

+

(m − 2)(m + 1 − 1/pki)x2
ki
1 + (m − 1)pki

})

xixT
i
n2m(m − 1)

+

1
n2m

d
(cid:88)

k=1

x2
ki
pki

xixT
i .

With Eq. (69) in hand, we can formulate σ2 as

σ2 = (cid:107)

E [AiAi] (cid:107)2 ≤

n
(cid:88)

i=1

(cid:34)

n
(cid:88)

i=1

max
k∈[d]

1
n2

8x4
ki
m2p2
ki

+

4x4
ki
m3p3
ki

+

2x2
(cid:107)xi(cid:107)2
ki
mpki

+

2x2
ki
m2pki

(cid:35)

d
(cid:88)

k=1

x2
ki
pki

(67)

(68)

(69)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

1
n2

(cid:20) 2(cid:107)xi(cid:107)2
m(m − 1)

2

max
k∈[d]

(

((m − 1)/pki)x2
ki
1 + (m − 1)pki

+

(m − 2)(m + 1 + 1/pki)x2
ki
1 + (m − 1)pki
(cid:35)

(cid:21)
)

+

1
n2m

(cid:107)

n
(cid:88)

d
(cid:88)

i=1

k=1

x2
ki
pki

xixT

i (cid:107)2

+

≤

+

n
(cid:88)

i=1

n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:34)

max
k∈[d]

1
n2

8x4
ki
m2p2
ki

+

4x4
ki
m3p3
ki

+

2x2
(cid:107)xi(cid:107)2
ki
mpki

+

2x2
ki
m2pki

d
(cid:88)

k=1

x2
ki
pki

1
n2

(cid:20) 8(cid:107)xi(cid:107)2
mpki

2x2
ki

(cid:21)

+

1
n2m

(cid:107)

max
k∈[d]

n
(cid:88)

d
(cid:88)

i=1

k=1

x2
ki
pki

xixT

i (cid:107)2.

Again, we have to consider the sampling distributions pki = α |xki|
(cid:107)xi(cid:107)1
α |xki|
(cid:107)xi(cid:107)1

and pki ≥ (1 − α) x2
ki
(cid:107)xi(cid:107)2
2

in Eq. (70), we have

+ (1 − α) x2

ki
(cid:107)xi(cid:107)2
2

with 0 < α < 1. Plugging pki ≥

(cid:34)

σ2 ≤

max
k∈[d]

1
n2

8(cid:107)xi(cid:107)4
2
m2(1 − α)2 +

4(cid:107)xi(cid:107)2
1(cid:107)xi(cid:107)2
2
m3α2(1 − α)

+

(cid:107)xi(cid:107)4
2
m(1 − α)

+

2(cid:107)xi(cid:107)2
2
m2(1 − α)

(cid:35)

d
(cid:88)

k=1

|xki|(cid:107)xi(cid:107)1
α

n
(cid:88)

i=1

n
(cid:88)

i=1
n
(cid:88)

(cid:20)

i=1
n
(cid:88)

i=1

+

=

+ (cid:107)

(cid:107)xi(cid:107)2

1xix2
i

n2mα

(cid:107)2.

1
n2

(cid:21)

(cid:20) 8(cid:107)xi(cid:107)4
m(1 − α)

2

+

1
n2m

(cid:107)

max
k∈[d]

n
(cid:88)

d
(cid:88)

|xki|(cid:107)xi(cid:107)1
α

xixT

i (cid:107)2

8(cid:107)xi(cid:107)4
2
n2m2(1 − α)2 +

i=1
1(cid:107)xi(cid:107)2
2

4(cid:107)xi(cid:107)2

k=1

n2m3α2(1 − α)

+

9(cid:107)xi(cid:107)4
2
n2m(1 − α)

+

2(cid:107)xi(cid:107)2
2(cid:107)xi(cid:107)2
1
n2m2α(1 − α)

(cid:21)

2

(cid:80)d

1(cid:107)xi(cid:107)2

|xki|4/3

ki
m3p3
ki

k=1 |xki|4/3 ) for the term 4x4

k=1 |xki|4/3)3 ≤ (cid:107)xi(cid:107)2

in Eq. (70) can produce a result tighter than that in

Note that employing pki = Ω(
Eq. (71), which is because of the fact that ((cid:80)d
equality. However, it is not necessary to apply pki = Ω(
n2m3α2(1−α) = O( (cid:107)xi(cid:107)2
4(cid:107)xi(cid:107)2
to the term 4x4
ki
m3p3
ki
n2m2α(1−α) = O( (cid:107)xi(cid:107)2
2(cid:107)xi(cid:107)2
2(cid:107)xi(cid:107)2
3 , 2
). Similarly, applying other sampling probabilities pki = Ω(
to Eq. (70) will produce a result larger than Eq. (71), which may not be bounded. This is also why we only use
pki = α |xki|
) to tighten R in Eq. (66). This derivation justiﬁes our selection of q = 1, 2 in
(cid:107)xi(cid:107)1

2 always holds owing to the Holder’s in-
in Eq. (70), because the term

in Eq. (70) has already been small enough, which can be smaller than other terms in Eq. (71) like

1(cid:107)xi(cid:107)2
k=1 |xki|4/3 ) to the term 4x4

) in Eq. (71) obtained by applying pki = α |xki|
(cid:107)xi(cid:107)1

|xki|q
k=1 |xki|q ) with q (cid:54)= 1, 4

+ (1 − α) x2

+ (1 − α) x2

= Ω( |xki|
(cid:107)xi(cid:107)1

= Ω( |xki|
(cid:107)xi(cid:107)1

+ x2

ki
m3p3
ki

ki
(cid:107)xi(cid:107)2
2

ki
(cid:107)xi(cid:107)2
2

|xki|4/3

1(cid:107)xi(cid:107)2

1(cid:107)xi(cid:107)2

n2m3

n2m2

(cid:80)d

(cid:80)d

)

2

2

1

ki
(cid:107)xi(cid:107)2
2

pki = Ω(

|xki|q
k=1 |xki|q ) used for constructing the sampling probability pki = α |xki|

(cid:107)xi(cid:107)1

(cid:80)d

+ (1 − α) x2

ki
(cid:107)xi(cid:107)2
2

.

We then invoke Theorem 3 to obtain that for (cid:15) ≥ 0,

P((cid:107)Ce − C(cid:107)2 ≥ (cid:15)) ≤ 2d exp(

−(cid:15)2/2
σ2 + R(cid:15)/3

).

Denote the RHS of Eq. (72) by δ = 2d exp( −(cid:15)2/2
bound we have (cid:107)Ce − C(cid:107)2 ≤ (cid:15) holds with probability at least 1 − η − δ. Furthermore, δ = 2d exp( −(cid:15)2/2
following quadratic equation in (cid:15)

σ2+R(cid:15)/3 ) and consider the failure probability η in Eq. (66), then by union
σ2+R(cid:15)/3 ) yields the

(70)

(71)

(72)

(73)

Solving Eq. (73) gets only one positive root

(cid:15)2
2 log(2d/δ)

−

R(cid:15)
3

− σ2 = 0.

(cid:15) = log(

(cid:34)

R
3

2d
δ

)

(cid:115)
(

+

R
3

)2 +

2σ2
log(2d/δ)

(cid:35)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

≤ log(

+

2σ2 log(

2d
δ

)

2R
3

2d
δ

).

(cid:114)

(cid:113)

Thus, immediately we have (cid:107)Ce − C(cid:107)2 ≤ log( 2d
completes the whole proof.

δ ) 2R

3 +

2σ2 log( 2d

δ ) holds with probability at least 1 − η − δ, which

2.6. Proof of Corollary 1

Proof. According to the setting, substituting that (cid:107)xi(cid:107)2 ≤ τ for all i ∈ [n], (cid:107)xi(cid:107)1
(cid:107)xi(cid:107)2
Theorem 2 establishes that

≤ ϕ with 1 ≤ ϕ ≤

d, and m < d into

√

(cid:107)Ce − C(cid:107)2 ≤ (cid:101)O

(cid:114)

(cid:16) τ 2
n
(cid:16) τ 2
n

+

+

τ 2ϕ2
nm
τ 2ϕ2
nm

+

+

τ 4
nm

τ 4
nm2 +
(cid:114) 1
n

τ 4ϕ2
nm3 +
(cid:114) 1
nm

+ τ 2

τ 2ϕ
m

+

τ 4ϕ2
nm2 +
(cid:114)
(cid:107)C(cid:107)2
nm

(cid:17)

,

+ τ ϕ

≤ (cid:101)O

(cid:107)C(cid:107)2τ 2ϕ2
nm

(cid:17)

where the ﬁrst inequality invokes (cid:80)n
Also, we can adopt (cid:80)n
nTr(C) ≤ nd(cid:107)C(cid:107)2.

i=1 (cid:107)xi(cid:107)4

i=n (cid:107)xi(cid:107)4

2 ≤ nτ 4, and C = (cid:80)n

i=1

2 ≤ ndτ 2(cid:107)C(cid:107)2, which holds because (cid:80)n

xixT
i
n
i=1 (cid:107)xi(cid:107)4

is the original covariance matrix.
2 and (cid:80)n

2 ≤ τ 2 (cid:80)n

i=1 (cid:107)xi(cid:107)2

i=1 (cid:107)xi(cid:107)2

2 =

Hence, we have

(cid:107)Ce − C(cid:107)2 ≤ (cid:101)O

(cid:114)

(cid:16) τ 2
n
(cid:16) τ 2
n

+

+

τ 2ϕ2
nm
τ 2ϕ2
nm

+ τ (cid:112)(cid:107)C(cid:107)2
(cid:114)

+

τ ϕ
m

d(cid:107)C(cid:107)2
n

d
nm2 +
(cid:114)

+ τ

dϕ2
nm3 +
d(cid:107)C(cid:107)2
nm

d
nm

+ τ ϕ

+

(cid:114)

dϕ2
nm2 +
(cid:17)
(cid:107)C(cid:107)2
nm

.

(cid:17)

ϕ2
nm

≤ (cid:101)O

Finally, assigning (cid:107)Ce − C(cid:107)2 by the smaller one of Eq. (75) and Eq. (76) completes the proof.

2.7. Proof of Corollaries 2 and 3

Proof. The proof follows (Azizyan et al., 2015, Corollaries 4-6), where the key component (cid:107)Ce − Cp(cid:107)2 is upper bounded
by (cid:107)Ce − 1
i − Cp(cid:107)2. Then, the derivation results from Theorem 2 in our paper and the
n
Gaussian tail bounds in (Azizyan et al., 2015, Proposition 14).

i (cid:107)2 + (cid:107) 1
n

i=1 xixT

i=1 xixT

(cid:80)n

(cid:80)n

(Azizyan et al., 2015, Proposition 14) shows that with probability at least 1 − ζ for d ≥ 2,

(cid:113)

max
i∈[n]

(cid:107)xi(cid:107)2 ≤

2Tr(Cp) log(nd/ζ);

xixT

i − Cp(cid:107)2 ≤ O(cid:0)(cid:107)Cp(cid:107)2

(cid:112)log(2/ζ)/n(cid:1).

(cid:107)

1
n

n
(cid:88)

i=1

√

Then, applying them and Corollary 1 along with the fact that (cid:107)xi(cid:107)1 ≤

d(cid:107)xi(cid:107)2 and Tr(Cp) ≤ d(cid:107)Cp(cid:107)2 establishes

(cid:107)Ce − Cp(cid:107)2 ≤ (cid:107)Ce −

xixT

i (cid:107)2 + (cid:107)

xixT

i − Cp(cid:107)2

n
(cid:88)

1
n

i=1

(cid:114) 1
n
(cid:114) 1
n
(cid:114)

d
n

≤ (cid:101)O

(cid:16) τ 2
n

+

τ 2ϕ2
nm

+

τ 2ϕ
m

≤ (cid:101)O

≤ (cid:101)O

+

(cid:16) τ 2
n

τ 2ϕ2
nm
(cid:16) d2(cid:107)Cp(cid:107)2
nm

+

+

τ 2ϕ
m
d(cid:107)Cp(cid:107)2
m

1
n

n
(cid:88)

i=1

(cid:115)

(cid:114)

+ τ ϕ

+ τ ϕ
(cid:114) 1
nm

+ τ 2

+ τ 2

(cid:114) 1
nm
(cid:114) 1
nm

(cid:80)n

(cid:107) 1
n

i (cid:107)2

(cid:17)

i=1 xixT
nm

(cid:16)

+ (cid:101)O

(cid:107)Cp(cid:107)2

(cid:17)

(cid:114) 1
n

(cid:17)

(cid:107)Cp(cid:107)2
nm

(cid:107)Cp(cid:107)2

(cid:17)

(cid:114) 1
n

(cid:16)

+ (cid:101)O
(cid:114) 1
nm

(cid:17)

(cid:114) 1
n

+ d(cid:107)Cp(cid:107)2

+ d(cid:107)Cp(cid:107)2

+ (cid:107)Cp(cid:107)2

(74)

(75)

(76)

(77)

(78)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

≤ (cid:101)O

(cid:16) d2(cid:107)Cp(cid:107)2
nm

+

d(cid:107)Cp(cid:107)2
m

(cid:114)

(cid:17)

d
n

with probability at least 1 − η − δ − ζ, where Eq. (78) results from that we invoke Eq. (77) to get (cid:107) 1
n
(cid:107) 1
n

i − Cp(cid:107)2 + (cid:107)Cp(cid:107)2 ≤ (cid:101)O((cid:107)Cp(cid:107)2).

i=1 xixT

(cid:80)n

The proof for the low-rank case where rank(Cp)≤ r additionally adopts

(79)

(cid:80)n

i=1 xixT

i (cid:107)2 ≤

(cid:107)[Ce]r − Cp(cid:107)2 ≤ (cid:107)[Ce]r − Ce(cid:107)2 + (cid:107)Ce − Cp(cid:107)2
≤ (cid:107)[Cp]r − Ce(cid:107)2 + (cid:107)Ce − Cp(cid:107)2
≤ (cid:107)[Cp]r − Cp(cid:107)2 + (cid:107)Cp − Ce(cid:107)2 + (cid:107)Ce − Cp(cid:107)2
= 2(cid:107)Ce − Cp(cid:107)2,

(80)

(81)

where the last equality holds because rank(Cp) ≤ r. Then, armed with Tr(Cp) ≤ rank(Cp)(cid:107)Cp(cid:107)2 ≤ r(cid:107)Cp(cid:107)2, we have

(cid:107)[Ce]r − Cp(cid:107)2 ≤ O((cid:107)Ce − Cp(cid:107)2) ≤ O((cid:107)Ce −

xixT

i (cid:107)2 + (cid:107)

xixT

i − Cp(cid:107)2)

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

≤ (cid:101)O

≤ (cid:101)O

(cid:16) rd(cid:107)Cp(cid:107)2
nm
(cid:16) rd(cid:107)Cp(cid:107)2
nm

+

+

r(cid:107)Cp(cid:107)2
m
r(cid:107)Cp(cid:107)2
m

(cid:114)

(cid:114)

d
n

d
n

(cid:114) 1
nm
(cid:17)

rd
nm

(cid:114)

+ (cid:107)Cp(cid:107)2

+ r(cid:107)Cp(cid:107)2

+ (cid:107)Cp(cid:107)2

+ (cid:107)Cp(cid:107)2

(cid:114)

rd
nm

(cid:17)

(cid:114) 1
n

with probability at least 1 − η − δ − ζ.

The given deﬁnitions also implicitly indicate that Cp and Ce are symmetric. Then, following (Azizyan et al., 2015), the
desired bound in Corollary 3 immediately results from Corollary 2 combined with the Davis-Kahan Theorem (Davis &
Kahan, 1970) that shows (cid:107) (cid:98)(cid:81)

(cid:107)Ce − Cp(cid:107)2.

k − (cid:81)

k (cid:107)2 ≤

1
λk−λk+1

3. Discussion for Counterparts

3.1. Theorems for Gauss-Inverse and UniSample-HD

We ﬁrst use our notations to rephrase current theoretical results provided in (Azizyan et al., 2015, Theorem 3) and (Anaraki
& Becker, 2017, Theorem 6), which correspond to Gauss-Inverse and UniSample-HD, respectively.

Theorem 5 (Azizyan et al. 2015, Theorem 3). Let d ≥ 2 and deﬁne,

S1 = (cid:107)

(cid:107)xi(cid:107)2

2xixT

i (cid:107)2, S2 =

(cid:107)xi(cid:107)4
2.

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

There exists universal constants κ1, κ2 > 0 such that for any 0 < δ < 1, with probability at least 1 − δ,

(cid:107)Ce − C(cid:107)2 ≤ κ1

(cid:16)(cid:114)

(cid:114)

d
m

S1 +

d
m2 S2

(cid:114)

(cid:17)

log(d/δ)
n

+ κ2

d maxi∈[n] (cid:107)xi(cid:107)2
2
nm

log(d/δ).

(82)

Theorem 6 (Anaraki & Becker 2017, Theorem 6). Let each column of Si ∈ Rd×m be chosen uniformly at random from
the set of all canonical basis vectors without replacement. Let ρ > 0 be a bound such that (cid:107)SiST
2 for all
i ∈ [n]. Then, with probability at least 1 − δ

2 ≤ ρ(cid:107)xi(cid:107)2

i xi(cid:107)2

where δ = d exp
(cid:104)

(cid:16) −(cid:15)2/2
σ2+R(cid:15)/3

(cid:17)

d(d−1)
nm(m−1)

(ρ − m(m−1)

d(d−1) ) maxi∈[n] (cid:107)xi(cid:107)2

+ 2(d−m)(cid:107)X(cid:107)2

n(m−1) maxk∈[d],i∈[n] x2

F

(cid:104)(cid:16) d(d−1)

, R = 1
n
2(cid:107)C(cid:107)2 + d−m
(cid:80)n

ki + (d−m)2 maxk∈[d]

n(d−1)(m−1)

m−1 ρ maxi∈[n] (cid:107)xi(cid:107)2
(cid:105)
.

i=1 x4

ki

2(cid:107)D(C)(cid:107)2

m(m−1) ρ + 1

maxi∈[n] (cid:107)xi(cid:107)2

2 + d(d−m)

m(m−1) maxk∈[d],i∈[n] x2

ki

(cid:17)

(cid:107)Ce − C(cid:107)2 ≤ (cid:15),

(83)

(cid:105)
, and σ2 =

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

3.2. Discussion

In this subsection, we will simplify the foregoing two theorems by making Eq. (82) and Eq. (83) explicitly dependent on
n, m and d. Our derivations are natural and straightforward, and we will not deliberately loose Eq. (82) and Eq. (83) in
order to demonstrate the superiority of the theoretical results gained by our weighted sampling method. We deﬁne that
maxi∈n (cid:107)xi(cid:107)2 ≤ τ .
In terms of Eq. (82) in Theorem 5, S1 ≤ maxi∈[n] (cid:107)xi(cid:107)2
maxi∈[n] (cid:107)xi(cid:107)2

2. Then, Eq. (82) can be simpliﬁed and reformulated as

2(cid:107)C(cid:107)2 and S2 ≤ maxi∈[n] (cid:107)xi(cid:107)4

2. Note that 1

F ≤ (cid:107)C(cid:107)2 ≤

nd (cid:107)X(cid:107)2

(cid:115)

(cid:16)

(cid:107)Ce − C(cid:107)2 ≤ (cid:101)O

d(cid:107)C(cid:107)2 maxi∈[n] (cid:107)xi(cid:107)2
2
nm

+

d maxi∈[n] (cid:107)xi(cid:107)4
2
nm2

+

d maxi∈[n] (cid:107)xi(cid:107)2
2
nm

(cid:17)

(cid:115)

(cid:114)

(cid:16)

τ

≤ (cid:101)O

(cid:114)

d(cid:107)C(cid:107)2
nm

+

τ 2
m

d
n

+

(cid:17)

.

τ 2d
nm

If applying S2 ≤ d maxi∈[n] (cid:107)xi(cid:107)2

2(cid:107)C(cid:107)2 in the original paper (Azizyan et al., 2015), we will get that

(cid:107)Ce − C(cid:107)2 ≤ (cid:101)O

(cid:115)

(cid:16)

d(cid:107)C(cid:107)2 maxi∈[n] (cid:107)xi(cid:107)2
2
nm

+

(cid:115)

d2 maxi∈[n] (cid:107)xi(cid:107)2

2(cid:107)C(cid:107)2

nm2

+

d maxi∈[n] (cid:107)xi(cid:107)2
2
nm

(cid:17)

(cid:114)

≤ (cid:101)O

(cid:16) τ d
m

(cid:107)C(cid:107)2
n

+

(cid:17)

.

τ 2d
nm

In summary,

(cid:107)Ce − C(cid:107)2 ≤ min{ (cid:101)O

(cid:114)

(cid:16)

τ

(cid:114)

d(cid:107)C(cid:107)2
nm

+

τ 2
m

d
n

+

(cid:17)

τ 2d
nm

, (cid:101)O

(cid:16) τ d
m

(cid:107)C(cid:107)2
n

+

(cid:17)

}.

τ 2d
nm

(cid:114)

For Eq. (83) in Theorem 6, we ﬁrst simplify its R and σ2. According to (Anaraki & Becker, 2017), to obtain a more accurate
estimation, each xi is required to be multiplied by HD to ﬂatten its large entries before being sampled uniformly without
replacement, where H is a Hadamard matrix with its dimension being 2l (l is a certain positive integer), and D is a diagonal
matrix with its diagonal elements being i.i.d. Rademacher random variables. Note that HDDT HT = DT HT HD is an
identity matrix.

Suppose that we do not have to pad X with zeros until its dimension d = 2l holds. Hence, assuming that d = 2l for
X ∈ Rd×n without loss of generality, we deﬁne Y = HDX ∈ Rd×n below.

Corollary 2 of (Anaraki & Becker, 2017) indicates that with probability at least 1 − β, we have

and

max
k∈[d],i∈[n]

|yki| ≤

(cid:115)

(cid:114) 1
d

2 log(

2nd
β

) max
i∈[n]

(cid:107)xi(cid:107)2

(cid:115)

max
i∈[n]

(cid:107)yi(cid:107)2 ≤

2 log(

) max
i∈[n]

(cid:107)xi(cid:107)2.

2nd
β

(cid:107)SiST

i yi(cid:107)2 ≤

(cid:115)

(cid:114) m
d

2 log(

)(cid:107)xi(cid:107)2.

2nd
β

Corollary 3 of (Anaraki & Becker, 2017) indicates that with probability at least 1 − β, we have

(84)

(85)

(86)

(87)

(88)

(89)

To make a compact representation, we deﬁne θ =

(cid:113)

2 log( 2nd

β ). Obviously, θ > 1.

Then, in Theorem 6, we can replace the input data X by Y. Combing Eq. (89) with the fact that (cid:107)yi(cid:107)2 = (cid:107)HDxi(cid:107)2 =
(cid:107)xi(cid:107)2 getting ρ = (((cid:112) m

for the setting of Theorem 6. Along with θ > 1 and m ≤ d, we have

d θ)2) = mθ2

d

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

R =

(cid:16)

O

(

1
n

d2
m2

mθ2
d

+ 1)θ2 max
i∈[n]

(cid:107)xi(cid:107)2

2 +

d(d − m)
m2

(cid:114) 1
(
d

θ)2 max
i∈[n]

(cid:107)xi(cid:107)2
2

(cid:17)

)θ2 max
i∈[n]

(cid:107)xi(cid:107)2
2

(cid:17)

(cid:17)

max
i∈[n]

(cid:107)xi(cid:107)2
2

(cid:16)

= O

(

dθ2
nm

= (cid:101)O

= (cid:101)O

(cid:16) d
nm
(cid:16) τ 2d
nm

(cid:17)

,

HDXXT DT HT
n

(cid:107)2

−

m(m − 1)
d(d − 1)

θ2 max
i∈[n]

(cid:107)xi(cid:107)2

2(cid:107)D(

max
i∈[n]

(cid:107)xi(cid:107)2

2(cid:107)HDX(cid:107)2

(cid:107)xi(cid:107)2
2(cid:107)

)θ2 max
i∈[n]
HDXXT DT HT
n
(d − m)2
ndm

F +

n

)(cid:107)2

(θ2 −

m − 1
d − 1

)θ2 max
i∈[n]

(cid:107)xi(cid:107)2

2(cid:107)C(cid:107)2 +

(cid:17)

(cid:107)xi(cid:107)4
2

i∈[n]

θ4
d2 max
(d − m)θ4
d
(d − m)2θ4
d3m

max
i∈[n]

max
i∈[n]

(cid:107)xi(cid:107)2
2

(cid:17)

(cid:107)xi(cid:107)4
2

and

σ2 ≤

(

mθ2
d
mθ2
d

(cid:16)

d2
nm2 O
(d − m)
m
d − m
nm

θ2
d

d2
(cid:16) m
nm2 O
d
(d − m)θ2
nmd
(cid:16) d
nm
d(d − m)

max
i∈[n]

+

+

=

+

+

max
i∈[n]

(cid:107)xi(cid:107)2

2nd

θ2
d

2 +

(cid:107)xi(cid:107)2

max
i∈[n]
d − m
nm2 max

i∈[n]

(cid:107)xi(cid:107)4
2

= (cid:101)O

(cid:107)xi(cid:107)2

2(cid:107)C(cid:107)2 +

i∈[n]

nm3 max
(cid:16) d
nm

max
i∈[n]
(cid:16) τ 2d(cid:107)C(cid:107)2
nm

+

= (cid:101)O

= (cid:101)O

(cid:107)xi(cid:107)4

2 +

(d − m)2
nm3d

(cid:17)

max
i∈[n]

(cid:107)xi(cid:107)4
2

(cid:107)xi(cid:107)2

2(cid:107)C(cid:107)2 +

τ 4d(d − m)
nm3

d(d − m)

i∈[n]

nm3 max
τ 4(d − m)2
nm3d

+

(cid:17)

.

(cid:107)xi(cid:107)4

2 +

(d − m)2
nm3d

(cid:17)

max
i∈[n]

(cid:107)xi(cid:107)4
2

n((cid:112)1/dθ)2 maxi∈[n] (cid:107)xi(cid:107)2
n

2

Note that Eq. (92) for simplifying σ2 in Eq. (83) is tighter than the simpliﬁcation result in the original paper (Anaraki &
Becker, 2017) that scales with d2
nm2 . Recalling Eq. (83), and replacing its (cid:15) by R and σ2 to get that with probability at least
1 − δ − β, we have

(cid:107)Ce − C(cid:107)2 ≤ (cid:101)O

(cid:114)

(cid:16)

τ

(cid:114)

d(cid:107)C(cid:107)2
nm

+

τ 2
m

d(d − m)
nm

+

τ 2(d − m)
m

(cid:114) 1

nmd

+

(cid:17)

.

τ 2d
nm

If m = d, then

(cid:107)Ce − C(cid:107)2 ≤ (cid:101)O

(cid:114)

(cid:16)

τ

d(cid:107)C(cid:107)2
nm

+

(cid:17)

.

τ 2d
nm

Although pure sampling without replacement makes no estimation error when m = d, processing the data by a Hadamard
matrix before sampling can result in the error as shown in Eq. (94).

If m < d with m being close to d, then d − m = O(1), and thus we have
(cid:114)

(cid:114)

(cid:107)Ce − C(cid:107)2 ≤ (cid:101)O

(cid:16)

τ

d(cid:107)C(cid:107)2
nm

+

τ 2
m

d
nm

+

(cid:17)

.

τ 2d
nm

If m (cid:28) d or there exists a certain constant κ < 1 with m < κd, then O(d − m) = O(d). In addition to considering that
1
nd (cid:107)X(cid:107)2

F ≤ (cid:107)C(cid:107)2 ≤ maxi∈[n] (cid:107)xi(cid:107)2

2 = τ 2, then we have
(cid:114)

(cid:107)Ce − C(cid:107)2 ≤ (cid:101)O

(cid:16)

τ

d(cid:107)C(cid:107)2
nm

+

τ 2d
m

(cid:114) 1
nm

+

(cid:17)

.

τ 2d
nm

(90)

(91)

(92)

(93)

(94)

(95)

(96)

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

4. Computational Complexity

Recall that we have n data samples in the d-dimensional space, and let m be the target compressed dimension. The
computational comparisons between our proposed method and the other approaches are presented in Table 1, in which
Standard method means computing C directly without data compression. We should explain some terms in the table
before proceeding.

Storage: storing data and random projection matrices (if any) in the remote sites and the fusion center, and storing the
covariance matrix in the fusion center.

Communication: shipping the data and random projection matrices (if any) from remote sites to the fusion center (high
communication cost requires tremendous bandwidth and power consumption).

Time (FLOPS): compressing the data in the remote sites, and calculating the covariance matrix in the fusion center (a low
time complexity means a low power cost and high efﬁciency for the data processing).

Note that, instead of only using the fusion center, data have to be ﬁrst collected from many remote sites like a network of
g (cid:28) n sensors. Then, they are transmitted to the fusion center to estimate the covariance matrix. This procedure shows
why communication cost is required. In the table, except for the communication, the two other compared terms have
contained the total costs in both the remote sites and fusion center.

(cid:80)g

For a covariance matrix deﬁned as C = 1
i=1 xi in the fusion center by
i=1 are distributed in g (cid:28) n remote sites, and uj ∈ Rd is the summation of all data vectors in
¯x = 1
j=1 uj, where {xi}n
n
the j-th remote site before being compressed. Hence, about O(gd) storage, O(gd) communication cost, and O(nd) time
have to be added to the last four methods in Table 1, with g (cid:28) n.

n XXT − ¯x¯xT , we can exactly calculate ¯x = 1

n

(cid:80)n

Method
Standard
Gauss-Inverse
Sparse

Storage
O(nd + d2)
O(nm + d2)
O(nm + d2)
UniSample-HD O(nm + d2)
O(nm + d2)

Table 1. Computational costs in terms of storage, communication, and time.
Communication
O(nd)
O(nm)
O(nm)
O(nm)
O(nm)

Time
O(nd2)
O(nmd + nm2d + nd2) + TG
O(d + nm2) + TS
O(nd log d + nm2)
O(nd + nm log d + nm2)

Our method

From now on, we can focus on the covariance matrix deﬁned as C = 1

n XXT .

nm−n

i SiST

i xi, SiST

i xi}n
i xixT

i xi, and SiST
i

i xi}n
i additionally takes only O(nm2) time, this is due to that each SiST

i=1 can be accurately computed in O(nm) time. Equipped with {SiST
i SiST

First, we derive the computational costs in our propose algorithm. Computing {pki}k∈[d],i∈[n] takes O(nd) time. Then,
sampling nm entries from all data vectors to get Y ∈ Rm×n takes time that is scaled on nm log d up to a certain small
constant. In Eq. (1), each Si, ST
(squared diagonal), has at most m non-zero entries. Hence,
i=1 via the sampled nm entries in Y and the sampling indices in T ∈ Rm×n incurs O(nm) time. With
recovering {Si}n
Y and T in hand, {SiST
i=1, computing
(cid:80)n
i xi ∈ Rd and
(cid:98)C1 = m
i=1 SiST
i ∈ Rd×d has at most m and m2 non-zero entries respectively. Based on the obtained (cid:98)C1, computing
SiST
i xixT
the square diagonal matrix (cid:98)C2 = m
i SiST
i
has at most m non-zero entries in its diagonal. Finally, obtaining C = (cid:98)C1 − (cid:98)C2 incurs O(d) extra time. The total
running time is about O(nd + nm log d + nm + nm + nm + nm2 + nm + d) = O(nd + nm log d + nm2). In the
remote sites, data are compressed into m dimensional space. Computing bki only corresponding to the sampled entries
i )D(bi) in Eq. (1), so that at most nm entries
D(SiST
is enough to exactly calculate the (cid:98)C2 = m
i xixT
. Thus, in the remote sites, Y ∈ Rm×n
from {pki}k∈[d],i∈[n] have to be retained to obtain {bki}, since bki =
and T ∈ Rm×n dominate the storage cost, taking about O(nm) space in total. In the fusion center, O(d2) storage is
additionally used to store the estimated covariance Ce ∈ Rd×d. Similarly, about O(nm) communication cost is required
because of transmitting Y ∈ Rm×n, T ∈ Rm×n, v ∈ Rn, w ∈ Rn and α.

i )D(bi) takes O(nm) time since each SiST

1
1+(m−1)pki

D(SiST

i SiST

i SiST

i xixT

i xixT

nm−n

nm−n

(cid:80)n

(cid:80)n

i=1

i=1

Then, for Standard in Table 1 that means directly calculating covariance matrix through the observed data samples without
compression, it is straightforward to check its computational complexity. X ∈ Rd×n and C ∈ Rd×d takes about O(nd+d2)
storage in total, and X ∈ Rd×n leads to about O(nd) communication burden. Calculating the covariance matrix C =

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

1

i xixT

i Si(ST

i=1 Si(ST

i Si)−1ST

i Si)−1ST

n XXT costs O(nd2) time.
For Gauss-Inverse, (cid:80)n
i , which is the main part of its unbiased estimator, dominates
the computational cost. Generating n different Gaussian matrices {Si ∈ Rd×m}n
i=1 by the pseudorandom number genera-
tor like Mersenne twister (Matsumoto & Nishimura, 1998), which is by far the most widely used, takes considerably large
amount of time in practice. The time cost can be denoted by TG. As Si is dense, computing {ST
i=1 takes O(nmd)
i=1 requires O(nm2d + nm3), which involves matrix multiplications and inversions. Sub-
time. Calculating {(ST
sequently, we repeat the matrix-vector multiplications in {Si(ST
i=1 from the left to right, based on
which we get the target covariance matrix. Finally, it takes at least O(nmd + nm2d + nm3 + nm2 + ndm + nd2) + TG =
i xi ∈ Rm before sending
O(nmd+nm2d+nd2)+TG time for Gauss-Inverse. In the remote sites, we compress data by ST
them to the fusion center. Along with O(d2) storage for the derived covariance matrix, about O(nm + d2) storage space is
required in total. Also, sending {ST

i=1 requires about a O(nm) computational burden.

i xi ∈ Rd}n

i xi ∈ Rm}n

i Si)−1ST

i Si)−1}n

i xi}n

i SiST

i=1 SiST

Note that we have not listed the synchronization cost of Gauss-Inverse in Table 1. In practice, a pseudo-random number
generator is applied to the program in both the remote sites and the fusion center to generate/reconstruct n Gaussian random
matrices {Si ∈ Rd×m}n
i=1, and only n seeds are required to be transmitted from remote sites to the fusion center to recover
the Gaussian random matrices. Therefore, only about O(n) storage and communication cost have to be added in Table 1.
i Si ∈ Rm×m into memory, hence at least O(m2) memory is required.
Also, calculating each (ST
i Si)−1 has to load each ST
For Sparse, calculating (cid:80)n
i xixT
i and subtracting its rescaled diagonal entries dominate the computational
cost (Anaraki, 2016). Generating sparse projection matrices {Si ∈ Rd×q}n
i=1 is also expensive (Anaraki & Becker, 2017),
whose time cost is denoted by TS. The entries of each Si are distributed on {−1, 0, 1} with probabilities { 1
2s }.
Then, each column of Si has d
s non-zero entries in expectation. Empirically, we can ﬁx that q/d = 0.2 or 0.4 according
s )q)
to (Anaraki & Hughes, 2014; Anaraki, 2016). The number of non-zero entries of SiST
in expectation, which ranges from dq
s (1 − q
s )q) = m < d, thus we can solve s with
q/d = 0.2 or 0.4 ﬁxed to obtain that s = O( d2
i=1 takes O( ndq
s ) = O(nm) time
in expectation. Based on it, computing {SiST
s ) = O(nm) time in expectation.
Since each SiST
i and
subtracting its rescaled diagonal entries requires O(nm + nm + nm2 + d) + TS = O(nm2 + d) + TS time in total. Storing
i=1 and the estimated covariance matrix requires O(nm + d2) storage in expectation, where a O(nm) cost
{SiST
results from O(nm) non-zero entries in {SiST
i=1 along with O(nm) corresponding indices. Similarly, sending
{SiST
For UniSample-HD, processing data by a Hadamard matrix by HDX ∈ Rd×n requires O(nd log d) time, where H ∈ Rd×d
can be a Hadamard matrix, D ∈ Rd×d is a diagonal matrix with diagonal elements being i.i.d. Rademacher random
variables, and we suppose that d = 2l holds (l is a certain positive integer). Then, sampling m entries uniformly without
i HDxi ∈ Rd}n
replacement on each data vector by {ST
i=1 takes O(nm) time. Hence, it is straightforward to check that
(cid:80)n
i HD ∈ Rd×d requires O(nd log d+nm+nm2+d2 log d) = O(nd log d+nm2)
i DT HT SiST
i DT HT xixT
time in total. HD ∈ Rd×d can be generated on the ﬂy when we process the data. About O(nm + d2) storage has to be
used for the compressed data and estimated covariance matrix. Obviously, about O(nm) communication cost is required.

i xi ∈ Rd contains only m non-zeros entries in expectation, thus obtaining (cid:80)n

i=1 from remote sites to the fusion center takes at most O(nm) communication cost in expectation.

i xi ∈ Rq}n
i=1 additionally costs O( ndq

i xi ∈ Rd is at least d(1 − (1 − 1

m ). Then computing {ST

s . Deﬁne d(1 − (1 − 1

i xi ∈ Rd}n

i=1 HDSiST

i xi ∈ Rd}n

i xi ∈ Rd}n

i xi ∈ Rd}n

2s ) to dq

2s , 1 − 1

i=1 SiST

i SiST

i xixT

s , 1

5. Impact of the Parameter ααα

5.1. Discussion

To determine if the k-th entry of the data vector xi ∈ Rd should be retained or not, the sampling probability applied in our
method is

pki = α

+ (1 − α)

|xki|
(cid:107)xi(cid:107)1

x2
ki
(cid:107)xi(cid:107)2
2

.

(97)

Achieving our theoretical bound of Theorem 2 requires 0 < α < 1. However, The case α = 1 and α = 0 can also
obtain weaker error bounds, which can be straightforwardly derived from Eqs. (64)(65) and Eqs. (70)(71). The following
illustration reveals the connection between α and error bounds on data owning different properties.

1. Only using α = 0, i.e., (cid:96)2-norm based sampling pki = x2
ki
(cid:107)xi(cid:107)2
2

can yield a very weak bound if there exist some very

Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

small entries |xki| in xi ∈ Rd. E.g., substituting pki = x2
ki
(cid:107)xi(cid:107)2
2

into the term maxk∈[d]

of Eq. (64) or Eq. (70) results

x2
ki
p2
ki

in maxk∈[d]

in the ﬁnal error bound, which becomes inﬁnite if the positive entry |xki| gets close to 0;

(cid:107)xi(cid:107)4
2
x2
ki

2. Only using α = 1, i.e., (cid:96)1-norm based sampling pki = |xki|
(cid:107)xi(cid:107)1

large entries |xki| in xi ∈ Rd. E.g., substituting pki = |xki|
(cid:107)xi(cid:107)1
maxk∈[d] x2
by employing pki = x2
ki
(cid:107)xi(cid:107)2
2

to bound maxk∈[d]

ki(cid:107)xi(cid:107)2

yields a slightly weak bound if there exist some very

into the term maxk∈[d]

x4
ki
p2
ki

of Eq. (70) results in
2 derived
2 = 1 without loss of generality, then

2 = (cid:107)xi(cid:107)4

1 in the ﬁnal error bound, which is always greater than or equal to maxk∈[d] (cid:107)xi(cid:107)4

x4
ki
p2
ki
ki(cid:107)xi(cid:107)2
2=1 maxk∈[d] x2

. Speciﬁcally, assume (cid:107)xi(cid:107)4
√
1 = d+2

d+1

4

(cid:29) 1 if when xji =

(cid:113) √
2

d+1
√
d

d

for all k ∈ [d] with k (cid:54)= j. Also, minxi⊂Rd,(cid:107)xi(cid:107)4

d for all
k ∈ [d] or we have xji = 1 and xki,k(cid:54)=j = 0 for all k ∈ [d] with k (cid:54)= j. Note xi ⊂ Rd in the above optimizations
means that xi is a vector variable in the d-dimensional space, and j is an arbitrary integer in the set [d].

1 = 1 if we have xki =

2=1 maxk∈[d] x2

ki(cid:107)xi(cid:107)2

it is possible that maxxi⊂Rd,(cid:107)xi(cid:107)4
(cid:113) 1
2d+2

√

and xki,k(cid:54)=j =
(cid:113) 1

3. Therefore, α balances the performance by (cid:96)1-norm based sampling and (cid:96)2-norm based sampling. (cid:96)2 sampling penal-
izes small entries more than (cid:96)1 sampling, hence (cid:96)2 sampling is more likely to select larger entries to decrease error
(e.g., case 2). However, different from (cid:96)1 sampling, (cid:96)2 sampling is unstable and sensitive to small entries, and it can
make estimation error incredibly high if extremely small entries are picked (e.g., case 1). Then 0 < α < 1 is applied
to achieve the desired tight bound with pki ≥ (1 − α) x2
to tackle the extreme situation in the case 2 that cannot
ki
(cid:107)xi(cid:107)2
2
be well handled purely by pki ≥ α |xki|
(cid:107)xi(cid:107)1
then increase.

. When α turns from 1 to 0, the estimation error is likely to ﬁrst decrease and

5.2. Experiments

Accordingly, we create four different synthetic datasets: {Ai}4
entries in A1 and A2 are i.i.d. generated from the Gaussian distributions N (

i=1 ∈ R1000×10000 (i.e., d = 1000 and n = 10000). All
, 1
1
100 ),
1000 ) and N (

√

√

,

(cid:113) 1
2d+2

(cid:113) 1
2d+2

respectively. For A3, the entries of its one row are i.i.d. generated from N (

N (

(cid:113) 1
2d+2

√

d

, 1
100 ). For A4, its generation follows the way of X1 in the main text of the paper.

(cid:113) √
2

d+1
√
d

d
, 1
100 ), and the other entries follow

d

Figure 1. Accuracy comparison by decreasing α from 1 to 0 with a step size of 0.1. The error at each α is normalized by that at α = 1
on y-axis, and m/d varies from 0.005 to 0.2 with a step size of 0.005 on x-axis. Roughly, α = 0.9 is a good choice, and the smaller
parameter like α = 0 usually leads to a poorer accuracy and higher variance compared with the other α values.

In Figure 1, the y-axis reports the errors that are normalized by the error incurred at α = 1. For A1, the magnitudes of
the data entries tend to be highly uniformly distributed. Thus, nearly the same results are returned over all α. For A2,
its entries are slightly uniformly distributed with some entries having extremely small magnitudes. Hence, α = 0 has a
poorer performance compared with the others, which is consistent with the case 1 in Section 5.1. A3 contains some entries
larger than the others, and neither α = 0 nor α = 1 achieves the best performance obtained roughly at α = 0.9. Also,
the estimation error ﬁrst decreases and then increases when α turns from 1 to 0. All such simulation results conform to
the case 2 and case 3 in Section 5.1. Considering A4 that is not likely to contain the extreme situation as mentioned in the
case 2 of Section 5.1, we see that best performance is roughly achieved when α gets close to 1.

m/d0.050.10.150.2Error0.60.70.80.911.11.21.3DggGaussian1X8, d=1024 n=10000Alpha-1Alpha-0.9Alpha-0.8Alpha-0.7Alpha-0.6Alpha-0.5Alpha-0.4Alpha-0.3Alpha-0.2Alpha-0.1Alpha-0m/d0.050.10.150.2Rescaled Error0.60.811.21.41.6A1, d=1000 n=10000m/d0.050.10.150.2Rescaled Error0.60.811.21.41.6A2, d=1000 n=10000m/d0.050.10.150.2Rescaled Error0.60.811.21.41.6A3, d=1000 n=10000m/d0.050.10.150.2Rescaled Error024A4, d=1000 n=10000Appendix for ‘Toward Efﬁcient and Accurate Covariance Matrix Estimation on Compressed Data’

References

Anaraki, F. Estimation of the sample covariance matrix from compressive measurements. IET Signal Processing, 2016.

Anaraki, F. and Becker, S. Preconditioned data sparsiﬁcation for big data with applications to pca and k-means. IEEE Transactions on

Information Theory, 2017.

Anaraki, F. and Hughes, S. Memory and computation efﬁcient pca via very sparse random projections. In Proceedings of the 31st

International Conference on Machine Learning (ICML-14), pp. 1341–1349, 2014.

Azizyan, M., Krishnamurthy, A., and Singh, A.

Extreme compressive sampling for covariance estimation.

arXiv preprint

arXiv:1506.00898, 2015.

Davis, C. and Kahan, W. M. The rotation of eigenvectors by a perturbation. iii. SIAM Journal on Numerical Analysis, 7(1):1–46, 1970.

Golub, G. H. and Van Loan, C. F. Matrix computations. 1996.

Matsumoto, M. and Nishimura, T. Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator.

ACM Transactions on Modeling and Computer Simulation, 1998.

Tropp, J. A. An introduction to matrix concentration inequalities. Foundations and Trends in Machine Learning, 8(1-2):1–230, 2015.

