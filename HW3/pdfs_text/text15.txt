Supplement to
“A Semismooth Newton Method for Fast, Generic Convex Programming”

Alnur Ali * 1 Eric Wong * 1 J. Zico Kolter 2

This document contains proofs and supplementary details for the paper “A Semismooth Newton Method for Fast, Generic
Convex Programming”. All section, equation, and ﬁgure numbers in this supplementary document are preceded by the
letter S (all numbering without an S refers to the main paper).

S.1. Proof of Lemma 3.2

exp

The proof relies on the proof of Lemma 3.6, below. Let z, δ ∈ R3, and let δ → 0. Suppose z + δ converges to a point
that falls into one of the ﬁrst three cases given in Section 2. Then, from the statement and proof of Lemma 3.6, an element
JPK∗
(z + δ) of the generalized Jacobian of the projection onto the dual of the exponential cone at z + δ, is just a matrix
with ﬁxed entries, since projections onto convex sets are continuous. If z + δ converges to a point that falls into the fourth
case, then brute force, e.g., using symbolic manipulation software, reveals that an element of the generalized Jacobian
(i.e., the inverse of the speciﬁc 4x4 matrix D given in (S.6), below) is also a constant matrix, even as z(cid:63)
2 , ν(cid:63) → 0; for
completeness, we give D−1 in (S.26), at the end of the supplement. Thus in all the cases, the Jacobian is a constant matrix,
which is enough to establish that the limit in (15) exists.

1 , z(cid:63)

S.2. Proof of Lemma 3.3

First, we give a useful result; its proof is elementary.

Lemma S.2.1. The afﬁne transformation, AF +b, of a (strongly) semismooth map F : Rk → Rk, with A ∈ Rk×k, b ∈ Rk,
is (strongly) semismooth.

Proof. First of all, we have that a map F : Rk → Rk is (strongly) semismooth if and only if its components Fi, for
i = 1, . . . , k, are (strongly) semismooth (Qi & Sun, 1993, Corollary 2.4). Additionally, we have that (strongly) semismooth
maps are closed under linear combinations (Izmailov & Solodov, 2014, Proposition 1.75). Putting the two pieces together
gives the claim.

Now, from Lemma 3.1, we have that the projections onto the nonnegative orthant, second-order cone, positive semideﬁnite
cone, as well as the free cone (an afﬁne map, hence strongly semismooth (Facchinei & Pang, 2007, Proposition 7.4.7)), are
all strongly semismooth. The map F , deﬁned in (16), is just an afﬁne transformation of these projections; thus, by (S.2.1),
it is strongly semismooth.

When K, from (2), is the exponential cone, the analogous claim that the map F is semismooth follows, from Lemma 3.2,
in a similar way.

*Equal contribution 1Machine Learning Department, Carnegie Mellon University 2Computer Science Department, Carnegie Mellon

University. Correspondence to: Alnur Ali <alnurali@cmu.edu>.

Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

S.3. Proof of Lemma 3.4

Proof. We have that (i) the projection onto a convex set (e.g., the nonnegative orthant, second-order cone, positive semidef-
inite cone, exponential cone, and free cone), naturally, yields a convex set; (ii) the afﬁne image of a convex set is a convex
set; and (iii) retaining only some of the coordinates of a convex set is a convex set (Boyd & Vandenberghe, 2004, page 38).
Hence, the components Fi, for i = 1, . . . , 3k, of the map F : R3k → R3k, deﬁned in (16), are convex functions. Thus,
by Clarke (1990, Proposition 1.2), the ith row of any element of the generalized Jacobian is just a subgradient of Fi. Now
observe that the element J of the generalized Jacobian, given in (17), is given by ﬁnding subgradients of the Fi.

S.4. Jacobian of the projection onto the second-order cone

In Section 3.2, we stated that, in one case, the Jacobian of the projection onto the second-order cone at some point z =
(z1, z2) ∈ Rm, with z1 ∈ Rm−1, z2 ∈ R, is a low-rank matrix D ∈ Rm×m; the matrix D is given by










D =

1

2 + z2
2(cid:107)z1(cid:107)2
− z2
2

− z2
2
(z1)1(z1)2
(cid:107)z1(cid:107)3
2

(z1)2
1
(cid:107)z1(cid:107)3
2

− z2
2
2 + z2

1

(z1)1(z1)2
(cid:107)z1(cid:107)3
2
− z2
2

2(cid:107)z1(cid:107)2

(z1)2
2
(cid:107)z1(cid:107)3
2

...

1
2

(z1)1
(cid:107)z1(cid:107)2

...

1
2

(z1)2
(cid:107)z1(cid:107)2

· · ·

· · ·
. . .
· · ·










,

1
2
1
2

(z1)1
(cid:107)z1(cid:107)2
(z1)2
(cid:107)z1(cid:107)2

...

1
2

(S.1)

which can be seen as the sum of diagonal and low-rank matrices. Here, (z1)i denotes the ith component of z1.

S.5. Proof of Lemma 3.5

Rewrite the projection onto the positive semideﬁnite cone as (11) as PKpsd (Z) = Q max(Λ, 0)QT , where Z =
Q max(Λ, 0)QT is the eigenvalue decomposition of some real, symmetric matrix Z, and the max here is interpreted
diagonally. Then, using the chain rule (Magnus & Neudecker, 1995), we get that

JPKpsd

(vec Z)(d vec Z) = d vec PKpsd(Z)

= vec (cid:0)(dQ) max(Λ, 0)QT + Q(d max(Λ, 0))QT + Q max(Λ, 0)(dQ)T (cid:1) ;

so, what remains is computing (each column of) dQ and d max(Λ, 0), i.e., the differential of (each column of) the matrix
of eigenvectors, and the differential of max(Λ, 0), respectively. From Magnus & Neudecker (1995, Chapter 8), we get that

where Z + denotes the pseudo-inverse of the matrix Z, and that

dQi = (ΛiiI − Z)+(dZ)Qi,

[d max(Λ, 0)]ii = I+(Λii)QT

i (dZ)Qi,

by applying the chain rule; here, I+(·) is the indicator function of the nonnegative orthant, i.e., it equals 1 if its argument
is nonnegative and 0 otherwise. Replacing dZ with some real, symmetric matrix ˜Z yields the claim.

S.6. Further details on the per-iteration costs of SCS, Newton-ADMM, and CVXOPT

Here, we elaborate on the costs of a single iteration of SCS, Newton-ADMM, and CVXOPT. For simplicity, we consider
the case where the cone K, in the cone program (1), is just a single cone (handling the case where K is the direct product
of multiple cones is not hard); also, we are mostly interested in the high-dimensional case, where n > m.

During a single iteration of SCS, described in (6) – (8), we must carry out the computations outlined below:

• We must update the ˜u variable, which costs O(max{n2, m2}) (see Section 4.1 of O’Donoghue et al. (2016)).

• We must update the u variable, the cost of which is dominated by the cost of projecting an m-vector onto the dual cone
K∗; for the case of projecting onto the positive semideﬁnite cone, we equivalently consider a matrix with dimensions
√

√

m ×

m. These costs are as follows:

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

– For the nonnegative orthant, Kno, the cost is O(m).
– For the second-order cone, Ksoc, the cost is O(m).
– For the positive semideﬁnite cone, Kpsd, the cost is O(m3/2).
– For the exponential cone, Kexp, the cost is roughly O(m3).

• We must update the v variable, which has negligible cost.

Summing up, the cost of a single iteration of SCS is O(max{n2, m2}) plus the cost of projecting onto the dual cone K∗,
as claimed in the main paper.

For Newton-ADMM, we must compute the ingredients on both sides of (19), F and J, as well as run GMRES and the back-
tracking line search. Computing both F and J can be seen as essentially costing the same as a single iteration of SCS, i.e.,
the cost of projecting onto the dual cone K∗ plus O(max{n2, m2}); the backtracking line search, then, costs the number of
backtracking iterations times the aforementioned cost. Furthermore, running GMRES costs O(max{n2, m2}), assuming
it returns early. Hence the cost of a single iteration of Newton-ADMM is (as claimed in the main paper) the number of
backtracking iterations times the sum of two costs: the cost of projecting onto the dual cone K∗ plus O(max{n2, m2}).

Finally, turning to the interior-point method CVXOPT, it can be seen that the per-iteration cost here is dominated by solving
the Newton system (1.11) in Andersen et al. (2011), essentially costing O(n3).

We mention that the above per-iteration costs can, of course, be improved by taking advantage of sparsity.

S.7. Proof of Lemma 3.6

First, from the Moreau decomposition given in (13), we get that

JPK∗

exp

(z) = I − JPKexp

(−z);

so, what remains is to compute JPKexp
get that

(z), for some z ∈ Rm. Looking back at the ﬁrst three cases given in Section 2, we

JPKexp

(z) =






I,
−I,
diag(1, I+(z2), I+(z3)),

z ∈ Kexp
z ∈ K∗
z1, z2 < 0,

exp

where I+(zi), i = 2, 3, is the indicator function of the nonnegative orthant, i.e., it equals 1 if zi ≥ 0 and 0 otherwise.
For the fourth case, the projection PKexp(z) is the solution to the optimization problem given in (12). Now observe that
(i) the optimization problem (12) is, in fact, convex, since the constraint ˜z2 > 0 is really just implied by the domain of
the function exp(˜z1/˜z2); (ii) the optimization problem (12) is feasible, since z(cid:63)
3 = exp(1) satisﬁes the
constraint; and (iii) we can obtain a solution to the optimization problem (12), by using a Newton method (Parikh & Boyd,
2014, Section 6.3.4).

1 = 1, z(cid:63)

2 = 1, z(cid:63)

The rest of the proof relies on the KKT conditions for the optimization problem (12), as well as differentials (see, e.g.,
Magnus & Neudecker (1995)). The Lagrangian of the optimization problem (12) is given by

(1/2)(cid:107)˜z − z(cid:107)2

2 + ν(˜z2 exp(˜z1/˜z2) − ˜z3),

where ν ∈ R is the dual variable. Thus, we get that the KKT conditions for the optimization problem (12), at a solution
γ(cid:63) = (z(cid:63)

1 , z(cid:63)

2 , z(cid:63)

3 , ν(cid:63)), are

2 − z2 + ν(cid:63)(exp(z(cid:63)
z(cid:63)

1 /z(cid:63)

1 /z(cid:63)

1 − z1 + ν(cid:63) exp(z(cid:63)
z(cid:63)
1 /z(cid:63)
2 ) = 0
2 ) exp(z(cid:63)
2 ) − (z(cid:63)
1 /z(cid:63)
2 )) = 0
3 − z3 − ν(cid:63) = 0
z(cid:63)
2 ) − z(cid:63)
1 /z(cid:63)
2 exp(z(cid:63)
z(cid:63)
3 = 0.

(S.2)

(S.3)

(S.4)

(S.5)

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

Now consider the differentials dz(cid:63)
the condition (S.2) that

1 , dz(cid:63)

2 , dz(cid:63)

3 , dz(cid:63)

4 and dz1, dz2, dz3, dz4 of the KKT conditions (S.2) – (S.5); we get for

⇐⇒ dz(cid:63)

1 − dz1 + (dν(cid:63)) exp(z(cid:63)

1 /z(cid:63)

dz(cid:63)

1 − dz1 + (dν(cid:63)) exp(z(cid:63)

1 /z(cid:63)
2 ) + ν(cid:63) exp(z(cid:63)

⇐⇒ dz(cid:63)

1 − dz1 + (dν(cid:63)) exp(z(cid:63)

1 /z(cid:63)

2 ) + ν(cid:63) exp(z(cid:63)

1 /z(cid:63)
2 )

(cid:104)

⇐⇒

1 + ν(cid:63) exp(z(cid:63)
z(cid:63)
2

1 /z(cid:63)
2 )

− ν(cid:63) exp(z(cid:63)
1 /z(cid:63)
(z(cid:63)
2 )2

2 )z(cid:63)

1

0

exp(z(cid:63)

−

2 ) + ν(cid:63)(d exp(z(cid:63)
1 /z(cid:63)
2 )) = 0
2 )(d(z(cid:63)
1 /z(cid:63)
1 /z(cid:63)
2 )) = 0
(cid:19)
(cid:18) dz(cid:63)
1 (dz(cid:63)
z(cid:63)
2 )
1
(z(cid:63)
z(cid:63)
2 )2
2

dz(cid:63)
1
dz(cid:63)
2
dz(cid:63)
3
dν(cid:63)

1 /z(cid:63)
2 )

= 0











(cid:105)

= dz1.

Repeating the above for the other conditions (S.3) – (S.5), we get that

1 /z(cid:63)
2 )

2 )z(cid:63)

1

1 + ν(cid:63) exp(z(cid:63)
z(cid:63)
2
1 /z(cid:63)
− ν(cid:63) exp(z(cid:63)
(z(cid:63)
2 )2
0
1 /z(cid:63)
exp(z(cid:63)
2 )








(cid:124)

1

1 )2

2 )z(cid:63)

2 )(z(cid:63)

− ν(cid:63) exp(z(cid:63)
1 /z(cid:63)
(z(cid:63)
2 )2
1 /z(cid:63)
1 + ν(cid:63) exp(z(cid:63)
(z(cid:63)
2 )3
0
2 ) exp(z(cid:63)
1 /z(cid:63)
(cid:123)(cid:122)
D

1 /z(cid:63)

(1 − z(cid:63)

0

0
1
2 ) −1

(1 − z(cid:63)

1 /z(cid:63)
2 )

exp(z(cid:63)
1 /z(cid:63)

1 /z(cid:63)
2 )
2 ) exp(z(cid:63)
−1
0














(cid:124)

(cid:125)

dz(cid:63)
1
dz(cid:63)
2
dz(cid:63)
3
dν(cid:63)
(cid:123)(cid:122)
dγ(cid:63)







(cid:125)

=







(cid:124)

dz1
dz2
dz3
dν
(cid:123)(cid:122)
dγ




,



(cid:125)

(S.6)

i.e.,

D(dγ(cid:63)) = dγ ⇐⇒ dγ(cid:63) = D−1(dγ);

here, D is nonsingular, since the optimization problem (12) is feasible. So, by deﬁnition, the upper left 3x3 submatrix of
D−1 is the Jacobian of the projection onto the exponential cone, for the fourth case.

S.8. Intuition behind some of the regularity conditions for Theorem 4.1, Theorem 4.2, and

Here, we elaborate on a couple of the regularity assumptions stated in the main paper.

Theorem 4.3

S.8.1. Regularity condition (A4)

Roughly speaking, the condition (A4) can be seen as requiring that the directional derivative of ˜z (cid:55)→ (cid:107)F (˜z)(cid:107)2
by α1/2(cid:107)F (˜z)(cid:107)2
2.

2 be bounded

We list some (useful) functions satisfying (A4):

• The function F (z) = z2, for z ∈ R. To show that the function F satisﬁes (A4), we proceed by computing the required
ingredients on both sides of (A4). Here, and for the rest of the section, we write D∆F 2(z) to mean the directional
derivative of the function F squared, in the direction ∆, evaluated at z.

We compute, for z > 0 and the Newton direction ∆ = −1, the left-hand side of (A4),

and the right-hand side of (A4),

So, satisfying (A4) means

D∆F 2(z) = −4z3,

−α1/22z3.

−4z3 ≤ −α1/22z3 ⇐⇒ 2 ≥ α1/2,

which is certainly true. Repeating the argument for z < 0 and ∆ = 1 yields a similar result. (When z = 0, it is a
solution.) Hence, F (z) = z2 satisﬁes (A4).

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

• The function F (z) = max(z + 1, cz + 1), with z ∈ R and some c > 0.

We have, for the left-hand side of (A4):

D∆F 2(z = 0) = −2c.

We have, for the right-hand side of (A4):

ˆF (z = 0, ∆ = −1) = J(z = 0)∆ = 1 · (−1) = −1.

So, satisfying (A4) means

−2c ≤ −α1/2 ⇐⇒ c ≥ α1/2/2.

In words, functions that satisfy (A4) cannot have c too small.

• An argument similar the one used above for F (z) = z2 can be used to show that the function F (z) = |z| also satisﬁes

(A4).

We also establish, by using the condition (A4), that the backtracking line search, used in Algorithm 1, terminates. Suppose,
for contradiction, that the backtracking line search never terminates. Then, from the backtracking line search iteration
described in Algorithm 1, we have, for all backtracking iterations k,

((cid:107)F (z) + γ(k)∆(cid:107)2

2 − (cid:107)F (z)(cid:107)2

2)/γ(k) ≥ −α(cid:107)F (z)(cid:107)2
2.

Taking the limit as k → ∞, we get

D∆(cid:107)F (z)(cid:107)2

2 ≥ −α(cid:107)F (z)(cid:107)2
2.

On the other hand, expanding the right-hand side of (A4) gives

F (z)T ( ˆF (z, ∆) + F (z)) − F (z)T F (z)

α1/2F (z)T ˆF (z, ∆) = α1/2 (cid:16)
≤ α1/2 (cid:16)
≤ α1/2 (cid:0)(cid:107)F (z)(cid:107)2ε(cid:107)F (z)(cid:107)2 − (cid:107)F (z)(cid:107)2
2
≤ α1/2 (cid:16)
2 − (cid:107)F (z)(cid:107)2
(1 − α1/2)(cid:107)F (z)(cid:107)2
2
= −α1/4(cid:107)F (z)(cid:107)2
2.

(cid:1)

(cid:17)

(cid:107)F (z)(cid:107)2(cid:107) ˆF (z, ∆) + F (z)(cid:107)2 − (cid:107)F (z)(cid:107)2
2

(cid:17)

(cid:17)

Putting (A4) and (S.12) above together immediately gives

D∆(cid:107)F (z)(cid:107)2

2 ≤ −α1/4(cid:107)F (z)(cid:107)2
2.

But putting (S.7) and (S.13) together gives

−α(cid:107)F (z)(cid:107)2

2 ≤ D∆(cid:107)F (z)(cid:107)2

2 ≤ −α1/4(cid:107)F (z)(cid:107)2
2,

a contradiction, since α ∈ (0, 1).

S.8.2. Regularity condition (A5)

Roughly speaking, the condition (A5) says that the Newton step on each iteration cannot be too large.

(S.7)

(S.8)

(S.9)

(S.10)

(S.11)

(S.12)

(S.13)

(S.14)

(S.15)

(S.16)

(S.17)

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

S.9. Proof of Theorem 4.1

Proof. We begin by recalling the condition under which backtracking line search continues, for a particular iteration of
Newton’s method; this happens as long as (see Algorithm 1)

(cid:107)F (z(i) + t(i)∆(i))(cid:107)2

2 ≥ (1 − αt(i))(cid:107)F (z(i))(cid:107)2
2.

This means that when backtracking line search terminates, we get that

0 ≤ (cid:107)F (z(i+1))(cid:107)2

2 < (1 − αt(i))(cid:107)F (z(i))(cid:107)2

2 < (cid:107)F (z(i))(cid:107)2
2.

(To be clear, in order to get the second inequality here, we used the fact that backtracking line search terminates after (S.14)
in Algorithm 1 no longer holds.) In order to get the third inequality here, we used the simple fact that 0 < 1 − αt(i) ≤ 1,
since 0 < α < 1/2 and 0 < t(i) ≤ 1. So, we have shown that the sequence ((cid:107)F (z(i))(cid:107)2
i=1 is both bounded below
and decreasing. Note that this is just a sequence in R, and thus, by the monotone convergence theorem, it converges.
Furthermore, since every convergent sequence in R is Cauchy, we get that

2)∞

(cid:16)

lim
i→∞

(cid:107)F (z(i))(cid:107)2

2 − (cid:107)F (z(i+1))(cid:107)2
2

= 0.

(cid:17)

On the other hand, by rearranging the second inequality in (S.15), we get that

2 − (cid:107)F (z(i+1))(cid:107)2
So, (S.16) along with taking the lim supi→∞ on both sides of (S.17) yields that limi→∞ αt(i)(cid:107)F (z(i))(cid:107)2
assumption (A1) says that lim supi→∞ t(i) → t > 0, and since α > 0, we get that limi→∞ ˜t(cid:107)F (z(i))(cid:107)2
˜t > 0, and so limi→∞ (cid:107)F (z(i))(cid:107)2

2 = 0, which implies that limi→∞ F (z(i)) = 0, as claimed.

2 > αt(i)(cid:107)F (z(i))(cid:107)2

(cid:107)F (z(i))(cid:107)2

2 ≥ 0.

2 = 0. But
2 = 0, for some

S.10. Proof of Theorem 4.2

Proof. First of all, by the assumption that (z(i))∞

0 ≤ (cid:107)∆(i)(cid:107)2 ≤

i=1 is convergent and assumption (A5), we must have that
1
C2

(cid:107) ˆF (z(i), ∆(i))(cid:107)2 ≤

(cid:107)F (z(i))(cid:107)2,

ε + 1
C2

(S.18)

where the second inequality here follows by rearranging (A5), and the third inequality follows from (20), as well as the
triangle inequality: after computing ∆(i) on Newton iteration i, we are assured that
(cid:107)F (z(i)) + ˆF (z(i), ∆(i))(cid:107)2 ≤ ε(cid:107)F (z(i))(cid:107)2
=⇒ (cid:107) ˆF (z(i), ∆(i))(cid:107)2 − (cid:107)F (z(i))(cid:107)2 ≤ ε(cid:107)F (z(i))(cid:107)2

Hence, since

⇐⇒ (cid:107) ˆF (z(i), ∆(i))(cid:107)2 ≤ (ε + 1)(cid:107)F (z(i))(cid:107)2.

sup
j,(cid:96)

dist(∆(j), ∆((cid:96))) ≤ sup

(cid:107)∆(j)(cid:107)2 + sup
(cid:96)

j

(cid:107)∆((cid:96))(cid:107)2,

and because the right-hand side here is bounded (as per (S.18), as well as the fact that ((cid:107)F (z(i))(cid:107)2
can conclude that the sequence (∆(i))∞

i=1 is bounded. (We used the Euclidean distance here.)

2)∞

i=1 is decreasing), we

let
By the Bolzano-Weierstrass theorem (for Euclidean spaces), this sequence contains a convergent subsequence;
(∆(i))i∈S , for some countable set S, be this subsequence. Deﬁne γ(i) = t(i)/β, i.e., γ(i) is the last t(i) for which (S.14)
was actually true (i.e., when checked at the start of the (i + 1)th Newton iteration). Then we get

2 − (cid:107)F (z(i))(cid:107)2
dividing through by γ(i) and taking limits gives (observe that, from assumption (A2), lim supi→∞ t(i) = 0 =⇒
limi→∞ t(i) = 0)

2 ≥ −αγ(i)(cid:107)F (z(i))(cid:107)2
2;

(cid:107)F (z(i) + γ(i)∆(i))(cid:107)2

−α(cid:107)F (z)(cid:107)2

2 ≤

lim
i,j→∞, j∈S

γ(i)

(cid:107)F (z(i) + γ(i)∆(j))(cid:107)2

2 − (cid:107)F (z(i))(cid:107)2
2

≤

lim
i,j→∞, j∈S

α1/2F (z(i))T ˆF (z(i), ∆(j)),

(S.19)

(S.20)

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

with the second line here following by assumption (A4). Expanding the right-hand side of (S.20), we get

α1/2F (z(i))T ˆF (z(i), ∆(j)) = α1/2F (z(i))T (cid:16) ˆF (z(i), ∆(j)) + F (z(i))

(cid:17)

− α1/2F (z(i))T F (z(i))

≤ α1/2(cid:107)F (z(i))(cid:107)2(cid:107)F (z(i)) + ˆF (z(i), ∆(j))(cid:107)2 − α1/2(cid:107)F (z(i))(cid:107)2
2
≤ α1/2ε(cid:107)F (z(i))(cid:107)2
= −α1/2(cid:107)F (z(i))(cid:107)2

2 − α1/2(cid:107)F (z(i))(cid:107)2
2
2(1 − ε),

with the second line following from the Cauchy-Schwarz inequality, and the third from (20). So, we obtain for the right-
hand side of (S.20) that

lim
i,j→∞, j∈S

α1/2F (z(i))T ˆF (z(i), ∆(j)) ≤ −α1/2(cid:107)F (z)(cid:107)2

2(1 − ε).

(S.21)

Putting together (S.19) and (S.21), we get that

−α(cid:107)F (z)(cid:107)2

2 ≤ −α1/2(cid:107)F (z)(cid:107)2

2(1 − ε) ⇐⇒ 0 ≥ α1/2(cid:107)F (z)(cid:107)2
2

(cid:16)

(1 − ε) − α1/2(cid:17)

.

Now, by assumption (A3), we require that ε < 1 − α1/2 ⇐⇒ (1 − ε) − α1/2 > 0; thus, we must have that (cid:107)F (z)(cid:107)2
0 ⇐⇒ F (z) = 0, as claimed.

2 =

S.11. Proof of Theorem 4.3

Proof. The theorem establishes that the iterates (z(i))∞
i.e., we get, for large enough i and some C > 0, that

i=1 generated by Algorithm 1 are locally quadratically convergent,

Let res(i) = F (z(i)) + J(z(i))∆(i), for convenience. We begin by making two useful observations.

First, using the second part of assumption (A6), we get that

lim
i→∞

|z(i+1) − z|
(z(i) − z)2

= C.

(cid:107)F (z(i)) − res(i)(cid:107)2 = (cid:107)J(z(i))∆(i)(cid:107)2

≤ (cid:107)J(z(i))(cid:107)2(cid:107)∆(i)(cid:107)2
≤ C3(cid:107)∆(i)(cid:107)2.

(cid:107)F (z(i)) − res(i)(cid:107)2 ≥ (cid:107)F (z(i))(cid:107)2 − (cid:107) res(i)(cid:107)2

≥ (cid:107)F (z(i))(cid:107)2 − ε(cid:107)F (z(i))(cid:107)2
≥ (1 − ε)(cid:107)F (z(i))(cid:107)2.

(cid:107)F (z(i))(cid:107)2 ≤ C4(cid:107)∆(i)(cid:107)2
=⇒ (cid:107)F (z(i))(cid:107)2
4 (cid:107)∆(i)(cid:107)2
2 ≤ C 2
2
=⇒ (cid:107) res(i)(cid:107)2 ≤ C5(cid:107)∆(i)(cid:107)2
2

=⇒

(cid:107) res(i)(cid:107)2
(cid:107)∆(i)(cid:107)2
2

≤ C5,

On the other hand, using the triangle inequality as well as (20), we get that

So, putting together (S.22) and (S.23), we get that

(1 − ε)(cid:107)F (z(i))(cid:107)2 ≤ C3(cid:107)∆(i)(cid:107)2

=⇒ (cid:107)F (z(i))(cid:107)2 ≤ C4(cid:107)∆(i)(cid:107)2,

for some constant C4 > 0, since 1 − ε > 0. Squaring both sides, it follows that

(S.22)

(S.23)

(S.24)

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

where C5 > 0 is some constant, and the third line follows because (20) and assumption (A3) tell us that (cid:107) res(i)(cid:107)2 ≤
C(cid:107)F (z(i))(cid:107)2
2 for some constant C > 0. Finally, Facchinei & Kanzow (1997, Theorem 2.5) and the second part of assump-
tion (A6) tell us that the sequence of iterates (z(i))∞

i=1 → z converges quadratically, with F (z) = 0, as claimed.

S.12. Further details on the minimum variance portfolio optimization example

Here, we elaborate on putting the minimum variance portfolio optimization problem (22) into the cone form of (1).

First, we rewrite the minimum variance portfolio optimization problem (22) as

where we used the simple fact (Lobo et al., 1998, Equation 8) that

for some vector α and nonnegative constants θ, γ (for us, α = Σ1/2θ, γ = 1, and δ = w). Then, we rewrite the above
problem as

where we deﬁned

minimize
θ∈Rp, w∈R

subject to

w
(cid:13)
(cid:20) 2Σ1/2θ
(cid:13)
(cid:13)
1 − w
(cid:13)
1 ≤ 1T θ ≤ 1,

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤ 1 + w

αT α ≤ γδ ⇐⇒

(cid:20)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2α
γ − δ

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤ γ + δ,

cT x

minimize
x∈Rp+1
subject to (cid:107)G1x + h(cid:107)2 ≤ qT x + z
G2x ≤ 1, G3x ≤ −1,

x =

c =

(cid:21)

(cid:21)

(cid:20) θ
w
(cid:20) 0
1
(cid:20) 2Σ1/2
0
(cid:21)

G1 =

q =

(cid:20) 0
1
G2 = (cid:2) 1T

(cid:21)

,

0
−1

h =

(cid:21)

(cid:20) 0
1

,

z = 1

0 (cid:3) , G3 = (cid:2) −1T

0 (cid:3) .

Finally, we just use

A =

,

b =

, K = Kp+2

soc × Kno × Kno,







−G1
−qT
G2
G3



















h
z
1
−1

to get the cone form of (1); here, Kp+2

soc denotes the (p + 1)-dimensional second-order cone.

S.13. Further details on the (cid:96)1-penalized logistic regression example

Here, we elaborate on putting the (cid:96)1-penalized logistic regression problem (23) into the cone form of (1). To keep the
notation light, we write

zi = yiXi·θ.

Now, for i = 1, . . . , N , we use the simple fact (Serrano, 2015, Section 9.4.1) that

log

exp(αi)

≤ −θ ⇐⇒

exp(αi + θ) ≤ 1,

(cid:33)

(cid:32)

(cid:88)

i

(cid:88)

i

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

for αi, θ ∈ R, in order to conclude that

log(exp(0) + exp(zi)) ≤ wi ⇐⇒ exp(−wi) + exp(zi − wi) ≤ 1,

(S.25)

where the wi ∈ R are some variables that we will introduce, later on. Next, we “split” the right-hand side of (S.25) into
the following set of constraints:

exp(−wi) ≤ (cid:96)i ⇐⇒

 ∈ Kexp,

i = 1, . . . , N,

exp(zi − wi) ≤ qi ⇐⇒

 ∈ Kexp,

i = 1, . . . , N,

(cid:96)i + qi ≤ 1,

i = 1, . . . , N,











−wi
1
(cid:96)i
zi − wi
1
qi



where (cid:96)i, qi ∈ R are more new variables. Thus, we can write the (cid:96)1-penalized logistic regression problem (23) as

1T w + λ1T t

minimize
θ∈Rp, w∈RN ,
t∈Rp, (cid:96)∈RN ,
q∈RN

subject to

 ∈ Kexp,

i = 1, . . . , N









−wi
1
(cid:96)i
yiXi·θ − wi
1
qi
(cid:96) + q ≤ 1
−t ≤ θ ≤ t.





 ∈ Kexp,

i = 1, . . . , N

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

Finally, to get the cone form of (1), we use




x =

c =

A =

Gi =























































θ
w
t
(cid:96)
q

0
1
λ1
0
0

,















,

0
−I
I

0
0
0

h
...
h
h
...
h
1
0
0



















G1
...
GN
H1
...
HN
0
0
0 −I
0 −I

,





















I
0
0

I
0
0

0
0
0

b =

,

h =



 ,





0
1
0

eT
i
0
0

0
0
0
0
0 −eT
i

 , Hi =





−yiXi·
0
0

eT
i
0
0

0
0
0

0
0
0
0
0 −eT
i



 ,

i = 1, . . . , N,

K = Kexp × · · · × Kexp
(cid:125)

(cid:124)

(cid:123)(cid:122)
N

× Kexp × · · · × Kexp
(cid:123)(cid:122)
(cid:125)
N

(cid:124)

×KN

no × Kp

no × Kp
no;

here, ei, i = 1, . . . , N denotes the ith standard basis vector in RN , and Ki

no denotes the i-dimensional nonnegative orthant.

S.14. Further details on the robust PCA example

Here, we elaborate on putting the robust PCA problem (24) into the cone form of (1).

First, we observe that, using duality arguments (see, e.g., Fazel et al. (2001, Section 3) or Recht et al. (2010, Proposition
2.1)), we can rewrite the robust PCA problem (24) as

minimize
W1∈RN ×N ,W2∈Rp×p,
t∈RN p, L,S∈RN ×p

subject to

(1/2)(tr(W1) + tr(W2))

−t ≤ vec(S) ≤ t
1T t ≤ λ
L + S = X
(cid:20) W1
L
LT W2

(cid:21)

(cid:23) 0.

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

To get the cone form of (1), we use

vec(W1)
vec(W2)
t
vec(L)
vec(S)









,

(1/2) vec(I)
(1/2) vec(I)
0
0
0









,

x =

c =

A =



























(cid:104)

0
0
0
0
0











,

0
0
0
0
0

0 −I
−I
I
0
−I
1T
0
0
0
I
I
0 −I −I
0 GL
0
) vec(G(2,1)
W1

)

GW1 GW2
vec(G(1,1)
W1

where G(i,j)
W1
vec(G(1,1)
W2

where G(i,j)
W2
vec(G(1,1)

L

(cid:104)

(cid:104)

GW1 =

GW2 =

GL =

· · · vec(G(N −1,N )

) vec(G(N,N )

)

W1

W1

(cid:105)

,

is 0 except with the (i, j)th entry of its upper left N × N block set to 1,

) vec(G(2,1)
W2

)

· · · vec(G(p−1,p)

W2

) vec(G(p,p)
W2

)

is 0 except with the (i, j)th entry of its bottom right p × p block set to 1,

(cid:105)

,

(cid:105)

,

) vec(G(2,1)

)

L

· · · vec(G(N −1,p)

) vec(G(N,p)

)

L

L

where G(i,j)
and the (j, i)th entry of its lower left p × N block set to 1,

is 0 except with the (i, j)th entry of its upper right N × p block

L

b =











,











0
0
λ
vec(X)
− vec(X)
0
no × KN p

K = KN p

no × Kno × KN p

no × KN p

no × KN +p

psd

.

Here, Ki
the constraint

psd denotes the (i × i)-dimensional positive semideﬁnite cone. Also, observe that the last row of A, b above encodes

(cid:20) W1

L
LT W2

(cid:21)

∈ KN +p
psd

,

which we can write as a linear matrix inequality (Andersen et al., 2011, Equation 1.7):

(cid:20) W1

L
LT W2

(cid:21)

(cid:23) 0 ⇐⇒

G(i,j)
W1

(W1)ij +

G(i,j)
W2

(W2)ij +

G(i,j)

L Lij (cid:23) 0

(cid:88)

i,j

(cid:88)

i,j

(cid:88)

i,j

⇐⇒ (cid:2) GW1 GW2

0 GL 0 (cid:3) x (cid:23) 0.

Expression for the matrix D−1, used in the proof of Lemma 3.2:

−1 − ez(cid:63)

1 /z(cid:63)
2 ν(cid:63)(z(cid:63)
(z(cid:63)
2 )3

1 )2

− ez(cid:63)

1 /z(cid:63)
(z(cid:63)

1 )2

2 e(z(cid:63)
2 )2

− ez(cid:63)

1 /z(cid:63)
2 ez(cid:63)
z(cid:63)
2

1

− ez(cid:63)

1 /z(cid:63)
(z(cid:63)

2 ν(cid:63)z(cid:63)
1

2 )2 − ez(cid:63)

D−1 = (1/k) ·















2z(cid:63)
1
z(cid:63)

2 − ez(cid:63)

e

−ez(cid:63)

1 /z(cid:63)

2 + ez(cid:63)

1 /z(cid:63)

−ez(cid:63)

1 /z(cid:63)

2 + ez(cid:63)

1 /z(cid:63)

1 )3

2 eν(cid:63) (z(cid:63)
(z(cid:63)
2 )4

1 )3

2 eν(cid:63) (z(cid:63)
(z(cid:63)
2 )4

1 /z(cid:63)
(z(cid:63)
2z(cid:63)
1
z(cid:63)
2 z(cid:63)
1
z(cid:63)
2

+ 2ez(cid:63)

1 )3

2 e(z(cid:63)
2 )3
1 /z(cid:63)
2 )2 − e
(z(cid:63)
− e

2 ν(cid:63)z(cid:63)
1

2z(cid:63)
1
z(cid:63)
2 ν(cid:63)(z(cid:63)
(z(cid:63)
2 )3
2z(cid:63)
1
z(cid:63)
2 ν(cid:63)(z(cid:63)
(z(cid:63)
2 )3

− e

1 )2

1 )2

− ez(cid:63)

1 /z(cid:63)

1 )2

2 eν(cid:63) (z(cid:63)
(z(cid:63)
2 )3

− ez(cid:63)

1 /z(cid:63)

1 )2

2 eν(cid:63) (z(cid:63)
(z(cid:63)
2 )3

ez(cid:63)

1 /z(cid:63)

1 )2

2 eν(cid:63) (z(cid:63)
(z(cid:63)
2 )3

ez(cid:63)

1 /z(cid:63)

1 )2

2 eν(cid:63) (z(cid:63)
(z(cid:63)
2 )3

+ ez(cid:63)

1 /z(cid:63)
2 ez(cid:63)
z(cid:63)
2

1

1 )2

1 /z(cid:63)
2 e(z(cid:63)
(z(cid:63)
2 )2
2 − ez(cid:63)

2z(cid:63)
1
z(cid:63)

1 /z(cid:63)
2 ν(cid:63)
z(cid:63)
2

−1 − e
2z(cid:63)
1
z(cid:63)
2 ν(cid:63)z(cid:63)
1
(z(cid:63)
2z(cid:63)
1
z(cid:63)
2 ν(cid:63)z(cid:63)
1
(z(cid:63)

2 )2 − ez(cid:63)
2 )2 − ez(cid:63)

− e

− e

1 /z(cid:63)
(z(cid:63)

2 eν(cid:63) z(cid:63)
2 )2

1

1 /z(cid:63)
(z(cid:63)

2 eν(cid:63) z(cid:63)
2 )2

1

1
z(cid:63)
2

1 )2
+ e(z(cid:63)
(z(cid:63)
1 )2
+ e(z(cid:63)
(z(cid:63)

2 )2 − ez(cid:63)
2 )2 − ez(cid:63)

1
z(cid:63)
2

2z(cid:63)
1
2 + e
z(cid:63)

−e

2z(cid:63)
1
2 eν(cid:63) (z(cid:63)
z(cid:63)
(z(cid:63)
2 )3

1 )2

− ez(cid:63)

1 /z(cid:63)
(z(cid:63)

1 )2

1 /z(cid:63)
(z(cid:63)

2 e(z(cid:63)
2 )2

− ez(cid:63)

1 /z(cid:63)
2 ez(cid:63)
z(cid:63)
2

1

1 + ez(cid:63)

1 /z(cid:63)
2 ν(cid:63)(z(cid:63)
(z(cid:63)
2 )3

1 )2

+ ez(cid:63)

1 /z(cid:63)
2 ν(cid:63)
z(cid:63)
2

−ez(cid:63)

1 /z(cid:63)

2z(cid:63)
1
z(cid:63)
2 ν(cid:63)z(cid:63)
1
(z(cid:63)
2 )2
+ ez(cid:63)

2 − e
2z(cid:63)
1
z(cid:63)
2 ν(cid:63)
z(cid:63)
2

− e

2 − e
3z(cid:63)
1
z(cid:63)
2 ν(cid:63)z(cid:63)
2 )2 − e
1
(z(cid:63)
+ ez(cid:63)
1 )2

1 /z(cid:63)
2 ν(cid:63)(z(cid:63)
(z(cid:63)
2 )3

−ez(cid:63)

1 /z(cid:63)

1 )3

2 e(z(cid:63)
2 )3
1 + ez(cid:63)

2 z(cid:63)
1

1 /z(cid:63)
z(cid:63)
2
2z(cid:63)
1
2 )2 + 2ez(cid:63)
2 eν(cid:63) z(cid:63)
z(cid:63)
(z(cid:63)

1

1 /z(cid:63)
2 ν(cid:63)
z(cid:63)
2

−ez(cid:63)

1 /z(cid:63)

−ez(cid:63)

1 /z(cid:63)

2 − e

2z(cid:63)
1
z(cid:63)
2 ν(cid:63)z(cid:63)
1
(z(cid:63)
2 )2
+ ez(cid:63)

2 − e
2z(cid:63)
1
z(cid:63)
2 ν(cid:63)
z(cid:63)
2

2 z(cid:63)
1

1 /z(cid:63)
z(cid:63)
2

1 + ez(cid:63)

1 )2

1 /z(cid:63)
2 ν(cid:63)(z(cid:63)
(z(cid:63)
2 )3
(S.26)

+ ez(cid:63)

1 /z(cid:63)
2 ν(cid:63)
z(cid:63)
2















,

where k = −1 − e

2z(cid:63)
1
ˆz2 −

ez(cid:63)

1 /z(cid:63)

1 )2

2 ν(cid:63)(z(cid:63)
(z(cid:63)
2 )3

e

+

1 )2

2z(cid:63)
1
2 eν(cid:63)(z(cid:63)
z(cid:63)
(z(cid:63)
2 )3

−

1 )3

ez(cid:63)

1 /z(cid:63)
(z(cid:63)

2 e(z(cid:63)
2 )3

e

−

3z(cid:63)
1
z(cid:63)
2 ν(cid:63)z(cid:63)
1
2 )2 −
(z(cid:63)

e

2z(cid:63)
1
2 eν(cid:63)z(cid:63)
z(cid:63)
2 )2 +
(z(cid:63)

1

2ez(cid:63)

1 )2

1 /z(cid:63)
(z(cid:63)

2 e(z(cid:63)
2 )2

−

2 ν(cid:63)

ez(cid:63)

1 /z(cid:63)
z(cid:63)
2

−

ez(cid:63)

1

2 ez(cid:63)
1 /z(cid:63)
z(cid:63)
2

.

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

References

Andersen, Martin, Dahl, Joachim, Liu, Zhang, and Vandenberghe, Lieven. Interior point methods for large-scale cone

programming. Optimization for machine learning, pp. 55–83, 2011.

Boyd, Stephen and Vandenberghe, Lieven. Convex Optimization. Cambridge University Press, 2004.

Clarke, Frank. Optimization and Nonsmooth Analysis. SIAM, 1990.

Facchinei, Francisco and Kanzow, Christian. A nonsmooth inexact Newton method for the solution of large-scale nonlinear

complementarity problems. Mathematical Programming, 76(3):493–512, 1997.

Facchinei, Francisco and Pang, Jong-Shi. Finite-Dimensional Variational Inequalities and Complementarity Problems.

Springer, 2007.

Fazel, Maryam, Hindi, Haitham, and Boyd, Stephen. A rank minimization heuristic with application to minimum order
system approximation. In American Control Conference, 2001. Proceedings of the 2001, volume 6, pp. 4734–4739.
IEEE, 2001.

Izmailov, Alexey and Solodov, Mikhail. Newton-Type Methods for Optimization and Variational Problems. Springer, 2014.

Lobo, Miguel Sousa, Vandenberghe, Lieven, Boyd, Stephen, and Lebret, Herv´e. Applications of second-order cone pro-

gramming. Linear algebra and its applications, 284(1):193–228, 1998.

Magnus, Jan and Neudecker, Heinz. Matrix Differential Calculus with Applications in Statistics and Econometrics. John

Wiley & Sons, 1995.

O’Donoghue, Brendan, Chu, Eric, Parikh, Neal, and Boyd, Stephen. Conic optimization via operator splitting and homo-

geneous self-dual embedding. Journal of Optimization Theory and Applications, pp. 1–27, 2016.

Parikh, Neal and Boyd, Stephen. Proximal algorithms. Foundations and Trends in Optimization, 1(3):127–239, 2014.

Qi, Liqun and Sun, Jie. A nonsmooth version of Newton’s method. Mathematical Programming, 58(1-3):353–367, 1993.

Recht, Benjamin, Fazel, Maryam, and Parrilo, Pablo A. Guaranteed minimum-rank solutions of linear matrix equations

via nuclear norm minimization. SIAM review, 52(3):471–501, 2010.

Serrano, Santiago. Algorithms for Unsymmetric Cone Optimization and an Implementation for Problems with the Expo-

nential Cone. PhD thesis, Stanford University, 2015.

