Minimax Regret Bounds for Reinforcement Learning

Mohammad Gheshlaghi Azar 1 Ian Osband 1 Rémi Munos 1

Abstract

√

√

√

We consider the problem of provably optimal
exploration in reinforcement learning for ﬁnite
horizon MDPs. We show that an optimistic
modiﬁcation to value iteration achieves a regret
HSAT +H 2S2A+H
T ) where
bound of (cid:101)O(
H is the time horizon, S the number of states, A
the number of actions and T the number of time-
steps. This result improves over the best previ-
AT ) achieved by the
ous known bound (cid:101)O(HS
UCRL2 algorithm of Jaksch et al. (2010). The
key signiﬁcance of our new results is that when
T ≥ H 3S3A and SA ≥ H, it leads to a regret of
(cid:101)O(
HSAT ) that matches the established lower
HSAT ) up to a logarithmic factor.
bound of Ω(
Our analysis contains two key insights. We use
careful application of concentration inequalities
to the optimal value function as a whole, rather
than to the transitions probabilities (to improve
scaling in S), and we deﬁne Bernstein-based "ex-
ploration bonuses" that use the empirical vari-
ance of the estimated values at the next states (to
improve scaling in H).

√

√

1. Introduction

We consider the reinforcement learning (RL) problem of an
agent interacting with an environment in order to maximize
its cumulative rewards through time (Burnetas & Kate-
hakis, 1997; Sutton & Barto, 1998). We model the envi-
ronment as a Markov decision process (MDP) whose tran-
sition dynamics are unknown from the agent. As the agent
interacts with the environment it observes the states, ac-
tions and rewards generated by the system dynamics. This
leads to a fundamental trade off: should the agent explore
poorly-understood states and actions to gain information
and improve future performance, or exploit its knowledge
to optimize short-run rewards.

1DeepMind, London, UK. Correspondence to: Mohammad

Gheshlaghi Azar <mazar@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

The most common approach to this learning problem is
to separate the process of estimation and optimization.
In this paradigm, point estimates of the unknown quanti-
ties are used in place of the unknown parameters and a
plan is made with respect to these estimates. Naive op-
timization with respect to these point estimates can lead
to premature exploitation and so may never learn the op-
timal policy. Dithering approaches to exploration (e.g.,
(cid:15)-greedy) address this failing through random action se-
lection. However, as this exploration is not directed the
resultant algorithms may take exponentially long to learn
(Kearns & Singh, 2002). In order to learn efﬁciently it is
necessary that the agent prioritizes potentially informative
states and actions. To do this, it is important that the agent
maintains some notion of its own uncertainty.
In some
sense, given any prior belief, the optimal solution to this
exploration/exploitation dilemma is given by the dynamic
programming in the extended Bayesian belief state (Bert-
sekas, 2007). However, the computational demands of this
method become intractable for even small problems (Guez
et al., 2013) while ﬁnite approximations can be arbitrarily
poor (Munos, 2014).

To combat these failings, the majority of provably efﬁcient
learning algorithms employ a heuristic principle known as
optimism in the face of uncertainty (OFU). In these algo-
rithms, each state and action is afforded some “optimism”
such that its imagined value is as high as statistically plau-
sible. The agent then chooses a policy under this opti-
mistic view of the world. This allows for efﬁcient explo-
ration since poorly-understood states and actions are af-
forded higher optimistic bonus. As the agent resolves its
uncertainty, the effects of optimism will reduce and the
agent’s policy will approach optimality. Almost all rein-
forcement learning algorithms with polynomial bounds on
sample complexity employ optimism to guide exploration
(Kearns & Singh, 2002; Brafman & Tennenholtz, 2002;
Strehl et al., 2006; Dann et al., 2017).

An alternative principle motivated by the Thompson sam-
pling (Thompson, 1933) has emerged as a practical com-
petitor to optimism. The algorithm posterior sampling re-
inforcement learning (PSRL) maintains a posterior distri-
bution for MDPs and, at each episode of interaction, fol-
lows a policy which is optimal for a single random sample
(Strens, 2000). Previous works have argue for the potential

Minimax Regret Bounds for Reinforcement Learning

beneﬁts of such PSRL methods over existing optimistic ap-
proaches (Osband et al., 2013; Osband & Van Roy, 2016b)
but they come with guarantees on the Bayesian regret only.
However a very recent work (Agrawal & Jia, 2017) have
shown that an optimistic version of posterior sampling (us-
ing a max over several samples) achieves a frequentist re-
SAT ) (for large T ) in the more general
gret bound (cid:101)O(H
setting of weakly communicating MDPs.

√

At a high level, this work addresses the noted shortcomings
of existing RL algorithms (Bartlett & Tewari, 2009; Jaksch
et al., 2010; Osband & Van Roy, 2016b), in terms of de-
pendency on S and H. We demonstrates that it is possible
to design a simple and computationally efﬁcient optimistic
algorithm that simultaneously address both the loose scal-
ing in S and H to obtain the ﬁrst regret bounds that match
HSAT ) lower bounds as T becomes large.
the Ω(

√

In this paper we present a conceptually simple and com-
putationally efﬁcient approach to optimistic reinforcement
learning in ﬁnite-horizon MDPs and report results for the
frequentist regret. Our algorithm, upper conﬁdence bound
value iteration (UCBVI) is similar to model-based inter-
val estimation (MBIE-EB) (Strehl & Littman, 2005) with a
delicate alteration to the form of the “exploration bonus”.
In particular UCBVI replaces the universal scalar of the
bonus in MBIE-EB with the empirical variance of the next-
state value function of each state-action pair. This alter-
ation is essential to improve the regret bound from (cid:101)O(H)
to (cid:101)O(

H).

√

√

√

HSAT +H 2S2A+H

Our key contribution is to establish a high probability regret
bound (cid:101)O(
T ) where S is the num-
ber of states, A is the number of actions, H is the episode
length and T is the total number of time-steps (and where (cid:101)O
ignores logarithmic factors). Importantly, for T > H 3S3A
and SA ≥ H this bound is (cid:101)O(
HSAT ), which matches
the established lower bound for this problem, up to loga-
rithmic factors (Osband & Van Roy, 2016a).1 This positive
result is the ﬁrst of its kind and helps to address an ongoing
question about where the fundamental lower bounds lie for
reinforcement learning in ﬁnite horizon MDPs (Bartlett &
Tewari, 2009; Dann & Brunskill, 2015; Osband & Van Roy,
2016a). Our reﬁned analysis contains two key ingredients:

√

• We use careful application of Bernstein and Freed-
man inequalities (Bernstein, 1927; Freedman, 1975)
to the concentration of the optimal value function di-
rectly, rather than building conﬁdence sets for the
transitions probabilities and rewards, like in UCRL2
(Jaksch et al., 2010) and UCFH (Dann & Brunskill,
2015).

• We use empirical-variance exploration bonuses based
on Bernstein’s inequality, which together with a recur-
sive Bellman-type Law of Total Variance (LTV) pro-
vide tight bounds on the expected sum of the variances
of the value estimates, in a similar spirit to the analysis
from (Azar et al., 2013; Lattimore & Hutter, 2012).

1In fact the lower bound of (Jaksch et al., 2010) is for the more
general setting of the weakly communicating MDPs and it doesn’t
directly apply to our setting. But a similar approach can be used
to prove a lower bound of same order for the ﬁnite-horizon MDPs,
as it is already used in (Osband & Van Roy, 2016a).

We should be careful to mention the current limitations of
our work, each of which may provide fruitful ground for fu-
ture research. First, we study the setting of episodic, ﬁnite
horizon MDPs and not the more general setting of weakly
communicating systems (Bartlett & Tewari, 2009; Jaksch
et al., 2010). Also we assume that the horizon length H
is known to the learner. Further, our bounds only improve
√
over previous scaling (cid:101)O(HS

AT ) for T > H 3S3A.

We hope that this work will serve to elucidate several of the
existing shortcomings of exploration in the tabular setting
and help further the direction of research towards provably
optimal exploration in reinforcement learning.

2. Problem formulation

In this section, we brieﬂy review some notation, as well as
some standard concepts and deﬁnitions from the theory of
Markov decision processes (MDPs).

Markov Decision Problems We consider the problem of
undiscounted episodic reinforcement learning (RL) (Bert-
sekas & Tsitsiklis, 1996), where an RL agent interacts
with a stochastic environment and this interaction is mod-
eled as a discrete-time MDP. An MDP is a quintuple
(cid:104)S, A, P, R, H(cid:105), where S and A are the set of states and
actions, P is the state transition distribution, The function
R : S ×A → (cid:60) is a real-valued function on the state-action
space and the horizon H is the length of episode. We de-
note by P (·|x, a) and R(x, a) the probability distribution
over the next state and the immediate reward of taking ac-
tion a at state x, respectively. The agent interacts with the
environment in a sequence of episodes. The interaction be-
tween the agent and environment at every episode2 k ∈ [K]
is as follows: starting from xk,1 ∈ S which is chosen by
the environment, the agent interacts with the environment
for H steps by following a sequence of actions chosen in
A and observes a sequence of next-states and rewards un-
til the end of episode. The initial state xk,1 may change
arbitrarily from one episode to the next. We also use the
notation (cid:107) · (cid:107)1 for the (cid:96)1 norm throughout this paper.

Assumption 1 (MDP Regularity). We assume S and A are
ﬁnite sets with cardinalities S, A, respectively. We also
assume that the immediate reward R(x, a) is deterministic

2We write [n] for {i ∈ N | 1 ≤ i ≤ n}.

Minimax Regret Bounds for Reinforcement Learning

and belongs to the interval [0, 1].3

In this paper we focus on the setting where the reward func-
tion R is known, but extending our algorithm to unknown
stochastic rewards poses no real difﬁculty.

The policy during an episode is expressed as a mapping
: S → R de-
π : S × [H] → A. The value V π
h
notes the value function at every step h = 1, 2, . . . , H
and state x ∈ S such that V π
h (x) corresponds to the ex-
pected sum of H − h rewards received under policy π,
starting from xh = x ∈ S. Under Assumption 1 there
exists always a policy π∗ which attains the best possible
h (x) def=
values, and we deﬁne the optimal value function V ∗
supπ V π
h (x) for all x ∈ S and h ≥ 1. The policy π
at every step h deﬁnes the state transition kernel P π
h and
h (y|x) def= P (y|x, π(x, h))
the reward function rπ
h as P π
h (x) def= R(x, π(x, h)) for all x ∈ S. For every
and rπ
V : S → R the right-linear operators P · and P π
h · are
also deﬁned as (P V )(x, a) def= (cid:80)
y∈S P (y|x, a)V (y) for
all (x, a) ∈ S × A and (P π
h (y|s)V (y)
for all x ∈ S, respectively. The Bellman operator for
the policy π, at every step h > 0 and x ∈ S, is deﬁned
as (T π
h V )(x). We also deﬁne
the state-action Bellman operator for all (x, a) ∈ S × A
as (T V )(x, a) def= R(x, a) + (P V )(x, a) and the opti-
mality Bellman operator for all x ∈ S as (T ∗V )(x) def=
maxa∈A(T V )(x, a). For ease of exposition, we remove
the dependence on x and (x, a), e.g., writing P V for
(P V )(x, a) and V for V (x), when there is no possible con-
fusion.

h V )(x) def= rπ

h V )(x) def= (cid:80)

h (x) + (P π

y∈S P π

We measure the performance of the learner over T = KH
steps4 by the regret Regret(K), deﬁned as

Regret(K) def=

1 (xk,1) − V πk
V ∗

1 (xk,1),

K
(cid:88)

k=1

where πk is the control policy followed by the learner at
episode k. Thus the regret measures the expected loss of
following the policy produced by the learner instead of the
optimal policy. So the goal of learner is to follow a se-
quence of policies π1, π2, . . . , πK such that Regret(K) is
as small as possible.

3. Upper conﬁdence bound value iteration

In this section we introduce two variants of the algorithm
that we investigate in this paper. We call the algorithm up-
per conﬁdence bound value iteration (UCBVI). UCBVI is

3For rewards in [Rmin, Rmax] simply rescale these bounds.
4In this paper we will often substitute T =KH to highlight
various dependencies relative to the existing literature. This
equivalence should be kept in mind by the reader.

an extension of value iteration which guarantees that the
resultant value function is a (high-probability) upper con-
ﬁdence bound (UCB) on the optimal value function. This
algorithm is related to the model based interval estimation
(MBIE-EB) algorithm (Strehl & Littman, 2008). Our key
contribution is the precise design of the upper conﬁdence
sets, and the analysis which lead to tight regret bounds.

UCBVI, described in Algorithm 1, calls UCB-Q-values
(Algorithm 2) which returns UCBs on the Q-values com-
puted by value iteration using an empirical Bellman opera-
tor to which is added a conﬁdence bonus bonus. We con-
sider two variants of UCBVI depending on the structure of
bonus, which we present in Algorithms 3 and 4.

Algorithm 1 UCBVI

Initialize data H = ∅
for episode k = 1, 2, . . . , K do

Qk,h = UCB − Q − values(H)
for step h = 1, . . . , H do

Take action ak,h = arg maxa Qk,h(xk,h, a)
Update H = H ∪ (xk,h, ak,h, xk,h+1)

end for

end for

I(x(cid:48) = x,a(cid:48) = a,y(cid:48) = y)

(x(cid:48),a(cid:48),y(cid:48))∈H
y∈S Nk(x,a,y)

Algorithm 2 UCB-Q-values
Require: Bonus algorithm bonus, Data H
Compute, for all (x,a,y) ∈ S × A × S,
Nk(x,a,y) = (cid:80)
Nk(x,a) = (cid:80)
k,h(x,a) = (cid:80)
N (cid:48)
Let K = {(x,a) ∈ S × A, Nk(x,a) > 0}
Estimate (cid:98)Pk(y|x,a) = Nk(x,a,y)
Nk(x,a)
Initialize Vk,H+1(x) = 0 for all (x,a) ∈ S × A
for h = H,H − 1,...,1 do
for (x,a) ∈ S × A do
if (x,a) ∈ K then

(xi,h,ai,h,xi,h+1)∈H

for all (x,a) ∈ K

I(xi,h = x,ai,h = a)

bk,h(x,a) = bonus( (cid:98)Pk,Vk,h+1,Nk,N (cid:48)
Qk,h(x,a) = min(cid:0)Qk−1,h(x,a),H,

k,h)

R(x,a) + ( (cid:98)PkVk,h+1)(x,a) + bk,h(x,a)(cid:1)

else

Qk,h(x,a) = H

end if
Vk,h(x) = maxa∈A Qk,h(x,a)

end for

end for
return Q-values Qk,h

The ﬁrst of these UCBVI-CH is based upon Chernoff-
Hoeffding’s concentration inequality, considers UCBVI
with bonus = bonus_1. bonus_1 is a very simple bound
which only assumes that values are bounded in [0, H]. We

Minimax Regret Bounds for Reinforcement Learning

√

√

will see in Theorem 1 that this very simple algorithm can
already achieve a regret bound of (cid:101)O(H
SAT ), thus im-
proving the best previously known regret bounds from a
S dependence. The intuition for this improved
S to a
S-dependence is that our algorithm (as well as our anal-
ysis) does not consider conﬁdence sets on the transition
dynamics P (y|x, a) like UCRL2 and UCFH do, but in-
stead directly maintains conﬁdence intervals on the opti-
mal value function. This is crucial as, for any given (x, a),
the transition dynamics are S-dimensional whereas the Q-
value function is one-dimensional.

Algorithm 3 bonus_1

Require: (cid:98)Pk(x, a), Nk(x, a)

(cid:113) 1

b(x, a) = 7HL
return b

Nk(x,a) where L = ln(5SAT /δ),

4. Main results

However, the loose form of UCB given by UCBVI-CH
does not look at the value function of the next state, and
just consider it as being bounded in [0, H]. However,
much better bounds can be obtained by looking at the vari-
ance of the next state values. Our main result relies upon
UCBVI with bonus = bonus_2, which we refer to as
UCBVI-BF as it relies on Bernstein-Freedman’s concen-
tration inequalities to build the conﬁdence set. UCBVI-BF
builds upon the intuition for UCBVI-CH but also incorpo-
rates a variance-dependent exploration bonus. This leads to
tighter exploration bonuses and an improved regret bound
of (cid:101)O(

HSAT ).

√

Algorithm 4 bonus_2
Require: (cid:98)Pk(x, a), Vk,h+1, Nk, N (cid:48)

k,h

(cid:115)

b(x,a)=

8LVarY ∼ (cid:98)Pk(·|x,a)(Vk,h+1(Y ))
Nk(x,a)
(cid:104)
min

y (cid:98)Pk(y|x,a)

8(cid:80)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

+

(cid:16) 1002H 3S2AL2

k,h+1(y)

(cid:17)(cid:105)

,H 2

N (cid:48)
Nk(x,a)

+

14HL
3Nk(x,a)

where L=ln(5SAT /δ)
return b

Compared to UCBVI-BF here we use a bonus built from
the empirical variance of the estimated next values. The
idea is that if we had knowledge of the optimal value V ∗,
we could build tight conﬁdence bounds using the variance
of the optimal value function at the next state in place of the
loose bound of H. Since however V ∗ is unknown, here we
use as a surrogate the empirical variance of the estimated
values. As more data is gathered, this variance estimate

will converge to the variance of V ∗. Now we need to make
sure our estimates Vk,h are optimistic (i.e., that they upper
bound V ∗
h ) at all times. This is achieved by adding an ad-
ditional bonus (last term in b(x, a)), which guarantees that
we upper bound the variance of V ∗. Now, using an iter-
ative -Bellman-type- Law of Total Variance, we have (see
proof) that the sum of the next-state variances of V ∗ (over
H time steps) (which is related to the sum of the explo-
ration bonuses over H steps) is bounded by the variance
of the H-steps return. Thus the size of the bonuses built
by UCBVI-BF are constrained over the H steps. And we
prove that the sum of those bonuses do not grow linearly
H only. This is the key for our improved
in H but in
dependence from H to

H.

√

√

In this section we present the main results of the paper,
which are upper bounds on the regret of UCBVI-CH and
UCBVI-BF algorithms. We assume Assumption 1 holds.
Theorem 1 (Regret bound for UCBVI-CH ).
Consider a parameter δ > 0.
UCBVI-CH is bounded w.p. at least 1 − δ, by

Then the regret of

Regret(K) ≤ 20H 3/2L

SAK + 250H 2S2AL2,

√

where L = ln(5HSAT /δ).

For T ≥ HS3A and SA ≥ H this bound translates to a
regret bound of (cid:101)O(H
SAT ), where T = KH is the total
number of time-steps at the end of episode K.

√

√

Theorem 1 is signiﬁcant in that, for large T , it improves
the regret dependence from S to
S, compared to the best
known bound of (Jaksch et al., 2010). The main intuition
for this improved S-dependence is that we bound the esti-
mation error of the next-state value function directly, in-
stead of the transition probabilities. More precisely, in-
stead of bounding the estimation error ( (cid:98)P πk
k − P πk )Vk,h+1
by (cid:107) (cid:98)P πk
k − P πk (cid:107)1(cid:107)Vk,h+1(cid:107)∞ (as is done in (Jaksch et al.,
2010) for example), we bound ( (cid:98)P πk
h+1 instead
(for which a bound with no dependence on S can be
achieved since V ∗ is deterministic) and handle carefully
the correction term ( (cid:98)P πk
Our second result, Theorem 2, demonstrates that we can
improve upon the H-dependence by using a more reﬁned,
Bernstein-Friedman-type, exploration bonus.
Theorem 2 (Regret bound for UCBVI-BF ).
Consider a parameter δ > 0.
UCBVI-BF is bounded w.p. 1 − δ, by

k − P πk )(Vk,h+1 − V ∗

Then the regret of

k − P πk )V ∗

h+1).

Regret(K) ≤ 30HL

√

SAK + 2500H 2S2AL2
√

+4H 3/2

KL,

where L = ln(5HSAT /δ).

Minimax Regret Bounds for Reinforcement Learning

√

We note that for T ≥ H 3S3A and SA ≥ H this bound
translates to a regret bound of (cid:101)O(
HSAT ). This result
is particularly signiﬁcant since, for T large enough (i.e.,
√
T ≥ H 3S3A), our bound is (cid:101)O(
HSAT ) which matches
√
the established lower bound Ω(
HSAT ) of (Jaksch et al.,
2010; Osband & Van Roy, 2016a) up to logarithmic factors.

The key insight is to apply concentration inequalities to
bound the estimation errors and the exploration bonuses in
terms of the variance of V ∗ at the next state. We then use
the fact that the sum of these variances is bounded by the
variance of the return (see e.g., Munos & Moore, 1999;
Azar et al., 2013; Lattimore & Hutter, 2012), which shows
H instead of lin-
that the estimation errors accumulate as
early in H, thus implying the improved H-dependence.

√

Computational efﬁciency

Theorems 1 and 2 guarantee the statistical efﬁciency of
UCBVI. In addition, both UCBVI-CH and UCBVI-BF
are computationally tractable. Each episode both algo-
rithms perform an optimistic value iteration with compu-
tational cost of the same order as solving a known MDP. In
fact, the computational cost of these algorithms can be fur-
ther reduced by only selectively recomputing UCBVI af-
ter sufﬁciently many observations. This technique is com-
mon to the literature (Jaksch et al., 2010; Dann & Brunskill,
2015) and would not affect the ˜O statistical efﬁciency. The
computational cost of this variant of UCBVI then amounts
to (cid:101)O(SA min(SA, T ) min(T, S)) as it only needs to up-
date the model (cid:101)O(SA) times (Jaksch et al., 2010).

Weakly communicating MDPs

In this short paper we focus on the setting of ﬁnite horizon
MDPs. By comparison, previous optimistic approaches to
exploration, such as UCRL2, provide bounds for the more
general setting of weakly communicating MDPs (Jaksch
et al., 2010; Bartlett & Tewari, 2009). However, we believe
that much of the insight from the UCBVI algorithm (and
its analysis) will carry over to this more general setting us-
ing existing techniques such as ‘the doubling trick‘ (Jaksch
et al., 2010).

5. Proof sketch

Here we provide the sketch proof of our results. The full
proof is deferred to the appendix.

5.1. Sketch Proof of Theorem 1

Let Ω = {Vk,h ≥ V ∗
h , ∀k, h} be the event under which
all computed Vk,h values are upper bounds on the optimal
value function. Using backward induction on h (and stan-
dard concentration inequalities) one can prove that Ω holds
with high probability (see Lem. 18 in the appendix). To

simplify notations in this sketch of proof we will not make
the numerical constants explicit, and instead we will de-
note by (cid:3) a numerical constant which can vary from line
to line. The exact values of these constants are provided
in the full proof. We will also make use of simpliﬁed no-
tations, such as using L to represent the logarithmic term
L = ln((cid:3)HSAT /δ).

1 (xk,1) − V πk
1≤k≤K V ∗
1≤k≤K Vk,1(xk,1) − V πk

The cumulative regret at episode K is Regret(K) def=
1 (xk,1). Deﬁne (cid:94)Regret(K) def=
(cid:80)
(cid:80)
1 (xk,1). Under Ω we have
Regret(K) ≤ (cid:94)Regret(K), so we now bound (cid:94)Regret(K).
Deﬁne ∆k,h
h . Thus

def= Vk,h − V πk

and (cid:101)∆k,h

def= V ∗

h − V πk
h
∆k,h ≤ (cid:101)∆k,h = (cid:98)P πk
= ( (cid:98)P πk

k Vk,h+1 + bk,h − P πk V πk
k − P πk )Vk,h+1 + P πk (cid:101)∆k,h+1 + bk,h.

h+1

The difﬁculty in bounding ( (cid:98)P πk
k − P πk )Vk,h+1 is that both
Vk,h+1 and (cid:98)P πk
are random variables and are not inde-
k
pendent (the value function Vk,h+1 computed at h + 1
may depend on the samples collected from state xh,k),
thus a straightforward application of Chernoff-Hoeffding
In (Jaksch et al.,
(CH) inequality does not work here.
2010), this issue is addressed by bounding it by (cid:107) (cid:98)P πk
k −
P πk (cid:107)1(cid:107)Vk,h+1(cid:107)∞ at the price of an additional

S.

√

√

√

The main contribution of our (cid:101)O(H
SAT ) bound (which
S factor compared to the previous bound of
removes a
(Jaksch et al., 2010)) is to handle this term more properly.
Instead of directly bounding ( (cid:98)P πk
k −P πk )Vk,h+1, we bound
( (cid:98)P πk
k −P πk )V ∗
h+1, using straightforward application of CH
S factor since V ∗
h+1 is deterministic),
(which removes the
and deal with the correction term ( (cid:98)P πk
k − P πk )(Vk,h+1 −
V ∗
h+1). We have

√

(cid:101)∆k,h = ( (cid:98)P πk

k − P πk )(Vk,h+1 − V ∗
+P πk (cid:101)∆k,h+1 + bk,h + ek,h,

h+1)

def= ( (cid:98)P πk

k − P πk )V ∗

h+1(xk,h) is the estimation
where ek,h
error of the optimal value function at the next state. Deﬁn-
ing (cid:101)δk,h

def= (cid:101)∆k,h(xk,h), we have

(cid:101)δk,h ≤ ( (cid:98)P πk

k − P πk )∆k,h+1(xk,h) + (cid:101)δk,h+1

+(cid:15)k,h + bk,h + ek,h,

where (cid:15)k,h

def= P πk ∆k,h+1(xk,h) − ∆k,h+1(xk,h+1).

bound on the correction term ( (cid:98)P πk

k −
Step 1:
P πk )∆k,h+1(xk,h). Using Bernstein’s inequality (B),
this term is bounded by

P πk (y|xk,h)

(cid:88)

y

(cid:115)

(cid:3)L
P πk (y|xk,h)nk,h

∆k,h+1(y) +

(cid:3)SHL
nk,h

,

Minimax Regret Bounds for Reinforcement Learning

def= Nk(xk,h, πk(xk,h)). Now considering only
where nk,h
the y such that P πk (y|xk,h)nk,h ≥ (cid:3)H 2L, and since
0 ≤ ∆k,h+1 ≤ (cid:101)∆k,h+1, then ( (cid:98)P πk
k − P πk )∆k,h+1(xk,h) is
bounded by

¯(cid:15)k,h +

(cid:3)L
P πk (xk,h+1|xk,h)nk,h

(cid:101)δk,h+1 +

(cid:3)SHL
nk,h

≤ ¯(cid:15)k,h +

(cid:101)δk,h+1 +

(cid:3)SHL
nk,h

,

(cid:115)

1
H

where ¯(cid:15)k,h

def=

√

(cid:101)δk,h+1

P πk (xk,h+1|xk,h)

(cid:16) (cid:80)

(cid:113) (cid:3)L
nk,h
(cid:17)

.

y P πk (y|xk,h)

√

(cid:101)∆k,h+1(y)
P πk (y|xk,h)

−

The sum over the neglected y such that P πk (y|xk,h)nk,h <
(cid:3)H 2L contributes to an additional term

(cid:115) (cid:3)P πk (y|xk,h)nk,hL
n2

k,h

(cid:88)

y

∆k,h+1(y) ≤

(cid:3)SH 2L
nk,h

.

term (and the smaller order

Neglecting this
term
(cid:3)SHL/nk,h) for now (by the pigeon-hole principle we
can prove that these terms contribute to the ﬁnal regret by
a constant at most (cid:3)S2AH 2L2), we have

(cid:101)δk,h ≤

1 +

(cid:101)δk,h+1 + (cid:15)k,h + ¯(cid:15)k,h + bk,h + ek,h

≤

1 +

(cid:15)k,i + ¯(cid:15)k,i + bk,i + ek,i

(cid:17)
.

(cid:16)

(cid:16)

(cid:124)

(cid:17)

1
H

1
H
(cid:123)(cid:122)
≤e

(cid:17)H

H−1
(cid:88)

(cid:16)

i=h

(cid:125)

The regret is thus bounded by

(cid:94)Regret(K) ≤ (cid:3) (cid:88)

k,h

((cid:15)k,h + ¯(cid:15)k,h + bk,h + ek,h).

(1)

k,h (cid:15)k,h and (cid:80)

We now bound those 4 terms.
(cid:80)

It is easy to check that
k,h ¯(cid:15)k,h are sums of martingale differ-
ences, which are bounded using Azuma’s inequality, and
lead to a regret of (cid:101)O(H
T ) without dependence on the
size of state and action space. The leading terms in the re-
gret bound comes from the sum of the exploration bonuses
(cid:80)

k,h bk,h and the estimation errors (cid:80)

k,h ek,h.

√

Step 2: Bounding the martingales (cid:80)
(cid:80)

k,h ¯(cid:15)k,h. Using Azuma’s inequality we deduce

k,h (cid:15)k,h and

Step 3: Bounding the exploration bonuses (cid:80)
Using the pigeon-hole principle, we have

k,h bk,h:

bk,h = (cid:3)HL

(cid:88)

k,h

(cid:115)

1
nk,h

(cid:88)

k,h

NK (x,a)
(cid:88)

(cid:88)

(cid:114) 1
n

= (cid:3)HL

x,a
√

n=1

≤ (cid:3)HL

SAT .

Step 4: Bounding on the estimation errors (cid:80)

(cid:113) L
nk,h

Using CH, w.h.p. we have ek,h = ( (cid:98)P πk
(cid:3)H
. Thus this bound on the estimation errors are
of the same order as the exploration bonuses (which is the
reason we choose those bonuses...).

k − P πk )V ∗

h+1

(3)

k,h ek,h.
(CH)
≤

Putting everything together: Plugging (2) and (3) into
(1) (and adding the smaller order term) we deduce

Regret(K) ≤ (cid:94)Regret(K) ≤ (cid:3)(cid:0)H

3
2 L

SAK + H 2S2AL2(cid:1).

√

5.2. Sketch Proof of Theorem 2

The proof of Theorem 1 relied on proving by a straightfor-
ward induction over h that Ω = {Vk,h ≥ V ∗
h , ∀k, h} hold
with high probability. In the case of exploration bonuses
deﬁned by:

(cid:115) (cid:3)LV

Y ∼ (cid:98)P

bk,h(x, a) =

(cid:0)Vk,h+1(Y )(cid:1)

πk
k (·|x,a)
Nk(x, a)
(cid:123)(cid:122)
empirical Bernstein

+

(cid:3)HL
Nk(x, a)
(cid:125)

min

(cid:16)
(cid:3)H 3S2AL2 (cid:80)
y
Nk(x, a)
(cid:123)(cid:122)
additional bonus

(cid:98)Pk(y|x,a)
k,h+1(y) , H 2
N (cid:48)

(cid:17)

,

(cid:125)

(4)

(cid:124)

+

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:124)

the backward induction over h is not straightforward. In-
deed, if the Vk,h+1 are upper bounds on V ∗
h+1, it is not nec-
essarily the case that the empirical variance of Vk,h+1 are
upper bound on the empirical variance of V ∗
h+1. However
we can prove by (backward) induction over h that Vk,h+1 is
sufﬁciently close to V ∗
h+1 to guarantee that the variance of
those terms are sufﬁciently close to each other so that the
additional bonus (additional bonus in (4)) will make sure
that Vk,h is still an upper-bound on V ∗
h . More precisely,
deﬁne the set of indices:

(cid:88)

k,h

(Az)
≤ (cid:3)H

√

T L,

(cid:15)k,h

√

(Az)
≤

(cid:3)T L.

¯(cid:15)k,h

(cid:88)

k,h

(2)

[k, h]hist

def= {(i, j), s.t.(1 ≤ i ≤ k ∧ 1 ≤ j ≤ H)

∨(i = k ∧ h < j ≤ H)},

Minimax Regret Bounds for Reinforcement Learning

and the event Ωk,h
Our induction is the following:

def= {Vi,j ≥ V ∗

h , (i, j) ∈ [k, h]hist}.

• Assume that Ωk,h holds.
h+1)(y) ≤ (cid:3)H

(Vk,h+1 − V ∗

Then we prove that
(cid:113) SAL
k,h+1(y) .
N (cid:48)

• We

Y ∼ (cid:98)Pk(·|x,a)

(cid:0)Vk,h+1(Y )(cid:1) +
V
that
deduce
(cid:0)V ∗
h+1(Y )(cid:1),
(cid:3)H 3S2AL2 (cid:80)
(cid:98)Pk(y|x,a)
k,h+1(y) ≥ V
N (cid:48)
so the additional bonus compensates for the possible
variance difference. Thus Vk,h ≥ V ∗
h and Ωk,h−1
holds.

Y ∼ (cid:98)Pk(·|x,a)

y

So in order to prove that all values computed by the
algorithm are upper bounding V ∗, we just need to
prove that under Ωk,h, we have (Vk,h+1 − V ∗
h+1)(y) ≤
(cid:113) A
min((cid:3)H 1.5SL
k,h+1(y) , H), which is obtained by de-
N (cid:48)
riving the following regret bound on

(cid:101)Rk,h(y)

def=

(Vi,h+1 − V πi

h+1)(xi,h+1)I{xi,h+1 = y}

(cid:88)

i≤k

≤ (cid:3)HL

(cid:113)

SAN (cid:48)

k,h+1(y).

(5)

Indeed, since {Vi,h}i is a decreasing sequence in i, we have

(Vk,h+1 − V ∗

h+1)(y) ≤ (cid:101)Rk,h+1(y)/N (cid:48)
≤ (cid:3)HL

SA/N (cid:48)

(cid:113)

k,h+1(y)

k,h+1(y).

Once we have proven that w.h.p., all computed values are
upper bounds on V ∗ (i.e. event Ω), then we prove that under
Ω, the following regret bound holds:

Regret(K) ≤ (cid:94)Regret(K) ≤ (cid:3)(HL

√

SAK + H 2S2AL2).
(6)

√

The proof of (5) relies on the same derivations as those
used for proving (6). The only two differences being that
(i) HK is replaced by N (cid:48)
k,h+1(y), the number of times a
state y was reached at time h + 1, up to episode k, and (ii)
H factor which comes from the fact that
the additional
at any episode, N (cid:48)
k,h+1(y) can only tick once, whereas the
total number of transitions from y during any episode can
be as large as H. The full proof of (5) will be given in
details in the appendix. We now give a proof sketch of (6)
under Ω.

Similar steps used for proving Theorem 1 apply. The main
difference compared to Theorem 1 is the bound on the sum
of the exploration bonuses and the estimation errors (which
we consider in Steps 3’ and 4’ below). This is where we can
H factor. The use of the Bernstein inequal-
remove the
ity makes it possible to bound both of those terms in terms
of the expected sum of variances (under the current policy

√

πk at any episode k) of the next-state values (for that pol-
icy), and then using recursively the Law of Total Variance
to conclude that this quantity is nothing but the variance of
the returns. This step is detailed now. For simplicity of the
exposition of this sketch we neglect second order terms.

Step 3’: Bounding the sum of exploration bonuses bk,h.
We have

(cid:88)

k,h

(cid:118)
(cid:117)
(cid:117)
(cid:116)

+

(cid:124)

√

bk,h = (cid:3)

L

(cid:88)

k,h
(cid:124)

(cid:115) V

Y ∼ (cid:98)P

(cid:0)Vk,h+1(Y )(cid:1)

πk
h (·|xk,h)
nk,h

(cid:123)(cid:122)
main term
(cid:17)

(cid:125)

(cid:88)

+

k,h

(cid:3)L
Nk(x, a)

.

(cid:125)

(cid:16)
(cid:3)H 3S2AL2 (cid:80)

min

(cid:98)Pk(y|x,a)
k,h+1(y) , H 2
N (cid:48)

y

Nk(x, a)

(cid:123)(cid:122)
second order term

(cid:17)1/2

By Cauchy-Schwarz,
(cid:16) (cid:80)
k,h (cid:98)Vk,h+1

the main term is bounded by
def=
V
≤
(cid:3)SA ln(T ) by the pigeon-hole principle, we now focus on
the term (cid:80)

(cid:80)
1
nk,h
(cid:0)Vk,h+1(Y )(cid:1).

(cid:98)Vk,h+1
1
nk,h

Since (cid:80)

πk
h (·|xk,h)

, where

Y ∼ (cid:98)P

k,h (cid:98)Vk,h+1.

k,h

k,h

We now prove that (cid:98)Vk,h+1 is close to Vπk
V

def=
h+1(Y )(cid:1) by bounding the following

(cid:0)V πk

k,h+1

πk
Y ∼P
h (·|xk,h)
quantity:

(cid:98)Vk,h+1 − Vπk

k,h+1

= (cid:98)P πk V 2

k,h+1 − ( (cid:98)P πk Vk,h+1)2
h+1)2 + (P πk V πk
h+1)2

−P πk (V πk

k,h+1 − P πk (V πk

h+1)2 + 2H(P πk − (cid:98)P πk )V ∗

h+1

+ P πk (V 2

k,h+1
(cid:125)

(cid:124)

k,h+1 − (V πk
(cid:123)(cid:122)
k,h)

h+1)2)
(cid:125)

(a(cid:48)

(i)
≤ (cid:98)P πk V 2
(ii)
≤ ( (cid:98)P πk − P πk )V 2

(cid:124)

+(cid:3)H 2

(cid:123)(cid:122)
(ak,h)
(cid:115)

L
nk,h

,

(7)

where (i) holds since under Ωk,h, Vk,h ≥ V ∗
(ii) holds due to Chernoff Hoeffding.

h ≥ V πk

h

and

Step 3’-a: bounding (cid:80)
k,h+1. Using sim-
ilar argument as those used in (Jaksch et al., 2010), we have
that

k,h (cid:98)Vk,h+1 −Vπk

ak,h ≤ H 2(cid:107) (cid:98)P πk − P πk (cid:107)1 ≤ (cid:3)H 2(cid:113)

SL/nk,h,

(where nk,h
pigeon-hole principle, (cid:80)

def= Nk(xk,h, πk(xk,h))). Thus from the

√

k,h ak,h ≤ (cid:3)H 2S

AT L.

Minimax Regret Bounds for Reinforcement Learning

Now a(cid:48)

k,h is bounded as

k,h ≤ 2HP πk (Vk,h − V πk
a(cid:48)
h )
= 2HP πk (cid:98)∆k,h.

Thus using Azuma’s inequality,

a(cid:48)
k,h

(cid:88)

k,h

(Az)
≤ 2H

(cid:88)

(cid:98)δk,h+1 + (cid:3)H 2

T L

√

k,h
≤ 2H 2U + (cid:3)H 2

√

T L,

where U is deﬁned as an upper-bound on the pseudo regret:
U def= (cid:80)
T (an upper bound on the
r.h.s. of (1)).

k,h(bk,h + ek,h) + (cid:3)H

√

k,h

Vπk

Vπk

k,h+1.

(Dominant term)

Step 3’-b: bounding (cid:80)
For any episode k, E[(cid:80)
k,h+1|Hk] is the expected sum
h
of variances of the value function V π
k (y) at the next state
y ∼ P πk (·|xk,h) under the true transition model for the
current policy. A recursive application of the law of to-
tal variance (see e.g., Munos & Moore, 1999; Azar et al.,
2013; Lattimore & Hutter, 2012) shows that this quantity is
nothing else than the variance of the return (sum of H re-
wards) under policy πk: V(cid:0) (cid:80)
h r(xk,h, πk(xk,h))(cid:1), which
is thus bounded by H 2. Finally, using Freedman’s (Fr) in-
equality to bound (cid:80)
k,h+1 by its expectation (see the
k,h
exact derivation in the appendix), we deduce

Vπk

Vπk

k,h+1

(cid:88)

k,h

(F r)
≤

(cid:88)

E

(cid:104) (cid:88)

(cid:105)

√

Vπk

k,h+1|Hk

+ (cid:3)H 2

T L

k

h

√

≤ T H + (cid:3)H 2

T L.

(8)

Thus, using (8), (7) and the bounds on (cid:80) ak,h and (cid:80) a(cid:48)
we deduce that

k,h,

bk,h ≤ (cid:3)L(cid:112)(T H + H 2U )SA.

(cid:88)

k,h

Step 4’: Bounding the sum of estimation errors
(cid:80)
k,h ek,h. We now use Bernstein inequality to bound the

estimation errors

ek,h =

( (cid:98)P πk

k − P πk )V ∗

h+1(xk,h)

(cid:88)

k,h

(cid:88)

k,h

≤

(cid:88)

(cid:3)

k,h

(cid:115) V∗

k,h+1
nk,h

+ (cid:3) HL
nk,h

,

k,h+1

def= VY ∼P πk (·|xk,h)

where V∗
very similar way as in Step 3’ above, we relate V∗
to Vπk
(cid:80)

k,h+1
k,h+1 and use the Law of total variance to bound
Vπk

h+1(Y )(cid:1). Now, in a

k,h+1 by HT and deduce that

(cid:0)V ∗

k,h

ek,h ≤ (cid:3)L(cid:112)(T H + H 2U )SA.

(cid:88)

k,h

√

From (1) we see that U ≤ (cid:3)L(cid:112)(T H + H 2U )SA thus
U ≤ (cid:3)(L

HSAT + H 2SAL2). This implies (6).

√

H factor from
So the reason we are able to remove the
the regret bound comes from the fact that the sum, over H
steps, of the variances of the next state values (which de-
ﬁne the amplitude of the conﬁdence intervals) is at most
Intuitively this
bounded by the variance of the return.
means that the size of the conﬁdence intervals do not add
H only. Although
up linearly over H steps but grows as
the sequence of estimation errors are not independent over
time, we are able to demonstrate a concentration of mea-
sure phenomenon that shows that those estimation errors
concentrate as if they were independent.

√

6. Conclusion

In this paper we reﬁne the familiar concept of optimism
in the face of uncertainty. Our key contribution is the de-
sign and analysis of the algorithm UCBVI-BF , which ad-
dresses two key shortcomings in existing algorithms for op-
timistic exploration in ﬁnite MDPs. First we apply a con-
centration to the value as a whole, rather than the transition
S. Next we
estimates, this leads to a reduction from S to
apply a recursive law of total variance to couple estimates
across an episode, rather than at each time step individu-
ally, this leads to a reduction from H to

H.

√

√

√

Theorem 2 provides the ﬁrst regret bounds which, for suf-
ﬁciently large T , match the lower bounds for the problem
(cid:101)O(
HSAT ) up to logarithmic factors. It remains an open
problem whether we can match the lower bound using this
approach for small T . We believe that the higher order term
can be improved from (cid:101)O(H 2S2A) to (cid:101)O(HS2A) by a more
careful analysis, i.e., a more extensive use of Freedman-
Bernstein inequalities. The same applies to the term of or-
der H

T which can be improved to

HT .

√

√

√

√

These results are particularly signiﬁcant because they help
to estabilish the information-theoretic lower bound of rein-
forcement learning at Ω(
HSAT ) (Osband & Van Roy,
2016a), whereas it was suggested in some previous work
SAT ). Moving from
that lower-bound should be of Ω(H
this big-picture insight to an analytically rigorous bound is
non-trivial. Although we push many of the technical details
to the appendix, our paper also makes several contributions
in terms of analytical tools that may be useful in subsequent
work. In particular we believe that the way we construct the
exploration bonus and conﬁdence intervals in UCBVI-CH
is novel to the literature of RL. Also the constructive ap-
proach in the proof of UCBVI-CH, which bootstraps the
regret bounds to prove that Vk,hs are ucbs, is another ana-
lytical contribution of this paper.

Minimax Regret Bounds for Reinforcement Learning

Acknowledgements

The authors would like to thank Marc Bellemare and all the
other wonderful colleagues at DeepMind for many hours of
discussion and insight leading to this research. We are also
grateful for the anonymous reviewers for their helpful com-
ments and for ﬁxing several mistakes in an earlier version
of this paper.

References

Agrawal, Shipra and Jia, Randy. Posterior sampling for
reinforcement learning: worst-case regret bounds. arXiv
preprint arXiv:1705.07041, 2017.

Azar, Mohammad Gheshlaghi, Munos, Rémi, and Kappen,
Hilbert J. Minimax pac bounds on the sample complex-
ity of reinforcement learning with a generative model.
Machine learning, 91(3):325–349, 2013.

Bartlett, Peter L. and Tewari, Ambuj. REGAL: A regu-
larization based algorithm for reinforcement learning in
In Proceedings of the
weakly communicating MDPs.
25th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI2009), pp. 35–42, June 2009.

Bernstein, S. Theory of probability, 1927.

Bertsekas, D. P. Dynamic Programming and Optimal
Control, volume I. Athena Scientiﬁc, Belmount, Mas-
sachusetts, third edition, 2007.

Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-Dynamic Pro-
gramming. Athena Scientiﬁc, Belmont, Massachusetts,
1996.

Dann, Christoph and Brunskill, Emma. Sample complexity
of episodic ﬁxed-horizon reinforcement learning. In Ad-
vances in Neural Information Processing Systems, 2015.

Dann, Christoph, Lattimore, Tor, and Brunskill, Emma.
Ubev-a more practical algorithm for episodic rl with
near-optimal pac and regret guarantees. arXiv preprint
arXiv:1703.07710, 2017.

Freedman, David A. On tail probabilities for martingales.

the Annals of Probability, pp. 100–118, 1975.

Guez, Arthur, Silver, David, and Dayan, Peter.

Scal-
able and efﬁcient bayes-adaptive reinforcement learning
based on monte-carlo tree search. Journal of Artiﬁcial
Intelligence Research, pp. 841–883, 2013.

Jaksch, T., Ortner, R., and Auer, P. Near-optimal regret
bounds for reinforcement learning. Journal of Machine
Learning Research, 11:1563–1600, 2010.

Kearns, Michael J. and Singh, Satinder P. Near-optimal
reinforcement learning in polynomial time. Machine
Learning, 49(2-3):209–232, 2002.

Lattimore, Tor and Hutter, Marcus. PAC bounds for dis-

counted MDPs. CoRR, abs/1202.3890, 2012.

Maurer, Andreas and Pontil, Massimiliano. Empirical
bernstein bounds and sample variance penalization. stat,
1050:21, 2009.

Munos, R. and Moore, A.

Inﬂuence and variance of a
Markov chain : Application to adaptive discretizations in
optimal control. In Proceedings of the 38th IEEE Con-
ference on Decision and Control, 1999.

Brafman, Ronen I. and Tennenholtz, Moshe. R-max - a
general polynomial time algorithm for near-optimal re-
inforcement learning. Journal of Machine Learning Re-
search, 3:213–231, 2002.

Munos, Rémi. From bandits to Monte-Carlo Tree Search:
The optimistic principle applied to optimization and
planning. Foundations and Trends R(cid:13) in Machine Learn-
ing, 7(1):1–129, 2014.

Bubeck, Sébastien and Cesa-Bianchi, Nicolò. Regret anal-
ysis of stochastic and nonstochastic multi-armed bandit
problems. CoRR, abs/1204.5721, 2012. URL http:
//arxiv.org/abs/1204.5721.

Bubeck, Sébastien, Munos, Rémi, Stoltz, Gilles, and
Szepesvári, Csaba. X-armed bandits. Journal of Ma-
chine Learning Research, 12:1587–1627, 2011.

Burnetas, Apostolos N and Katehakis, Michael N. Optimal
adaptive policies for markov decision processes. Mathe-
matics of Operations Research, 22(1):222–255, 1997.

Cesa-Bianchi, N. and Lugosi, G. Prediction, Learning, and
Games. Cambridge University Press, New York, NY,
USA, 2006.

Osband, Ian and Van Roy, Benjamin. On lower bounds for
regret in reinforcement learning. stat, 1050:9, 2016a.

Osband, Ian and Van Roy, Benjamin. Why is posterior
sampling better than optimism for reinforcement learn-
ing. arXiv preprint arXiv:1607.00215, 2016b.

Osband, Ian, Russo, Dan, and Van Roy, Benjamin. (more)
efﬁcient reinforcement learning via posterior sampling.
In Advances in Neural Information Processing Systems,
pp. 3003–3011, 2013.

Strehl, Alexander L and Littman, Michael L. A theoretical
analysis of model-based interval estimation. In Proceed-
ings of the 22nd international conference on Machine
learning, pp. 856–863. ACM, 2005.

Minimax Regret Bounds for Reinforcement Learning

Strehl, Alexander L and Littman, Michael L. An analysis
of model-based interval estimation for markov decision
processes. Journal of Computer and System Sciences, 74
(8):1309–1331, 2008.

Strehl, Alexander L., Li, Lihong, Wiewiora, Eric, Lang-
ford, John, and Littman, Michael L. PAC model-free
reinforcement learning. In ICML, pp. 881–888, 2006.

Strens, Malcolm J. A. A Bayesian framework for reinforce-

ment learning. In ICML, pp. 943–950, 2000.

Sutton, Richard and Barto, Andrew. Reinforcement Learn-

ing: An Introduction. MIT Press, March 1998.

Thompson, W.R. On the likelihood that one unknown prob-
ability exceeds another in view of the evidence of two
samples. Biometrika, 25(3/4):285–294, 1933.

Weissman, Tsachy, Ordentlich, Erik, Seroussi, Gadiel,
Verdu, Sergio, and Weinberger, Marcelo J.
Inequali-
ties for the l1 deviation of the empirical distribution.
Hewlett-Packard Labs, Tech. Rep, 2003.

