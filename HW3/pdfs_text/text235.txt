Deep Value Networks Learn to
Evaluate and Iteratively Reﬁne Structured Outputs

Michael Gygli 1 * Mohammad Norouzi 2 Anelia Angelova 2

Abstract

complicated high level reasoning to resolve ambiguity.

We approach structured output prediction by op-
timizing a deep value network (DVN) to pre-
cisely estimate the task loss on different out-
put conﬁgurations for a given input. Once the
model is trained, we perform inference by gra-
dient descent on the continuous relaxations of
the output variables to ﬁnd outputs with promis-
ing scores from the value network. When ap-
plied to image segmentation, the value network
takes an image and a segmentation mask as in-
puts and predicts a scalar estimating the inter-
section over union between the input and ground
truth masks. For multi-label classiﬁcation, the
DVN’s objective is to correctly predict the F1
score for any potential label conﬁguration. The
DVN framework achieves the state-of-the-art re-
sults on multi-label prediction and image seg-
mentation benchmarks.

1. Introduction

Structured output prediction is a fundamental problem in
machine learning that entails learning a mapping from in-
put objects to complex multivariate output structures. Be-
cause structured outputs live in a high-dimensional com-
binatorial space, one needs to design factored prediction
models that are not only expressive, but also computation-
ally tractable for both learning and inference. Due to com-
putational considerations, a large body of previous work
(e.g., Lafferty et al. (2001); Tsochantaridis et al. (2004))
has focused on relatively weak graphical models with pair-
wise or small clique potentials. Such models are not ca-
pable of learning complex correlations among the random
variables, making them not suitable for tasks requiring

*Work done during an internship at Google Brain. 1ETH Z¨urich
& gifs.com 2Google Brain, Mountain View, USA. Correspon-
dence to: Michael Gygli <gygli@vision.ee.ethz.ch>, Moham-
mad Norouzi <mnorouzi@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

An expressive family of energy-based models studied by
LeCun et al. (2006) and Belanger & McCallum (2016) ex-
ploits a neural network to score different joint conﬁgura-
tions of inputs and outputs. Once the network is trained,
one simply resorts to gradient-based inference as a mech-
anism to ﬁnd low energy outputs. Despite recent develop-
ments, optimizing parameters of deep energy-based models
remains challenging, limiting their applicability. Moving
beyond large margin training used by previous work (Be-
langer & McCallum, 2016), this paper presents a simpler
and more effective objective inspired by value based rein-
forcement learning for training energy-based models.

Our key intuition is that learning to critique different out-
put conﬁgurations is easier than learning to directly come
up with optimal predictions. Accordingly, we build a deep
value network (DVN) that takes an input x and a corre-
sponding output structure y, both as inputs, and predicts a
scalar score v(x, y) evaluating the quality of the conﬁgura-
tion y and its correspondence with the input x. We exploit
a loss function (cid:96)(y, y∗) that compares an output y against
a ground truth label y∗ to teach a DVN to evaluate different
output conﬁgurations. The goal is to distill the knowledge
of the loss function into the weights of a value network so
that during inference, in the absence of the labeled output
y∗, one can still rely on the value judgments of the neural
net to compare outputs.

To enable effective iterative reﬁnement of structured out-
puts via gradient ascent on the score of a DVN, similar to
Belanger & McCallum (2016), we relax the discrete out-
put variables to live in a continuous space. Moreover, we
extend the domain of loss functions so the loss applies to
continuous variable outputs. For example, for multi-label
classiﬁcation, instead of enforcing each output dimension
yi to be binary, we let yi ∈ [0, 1] and we generalize the
notion of F1 score to apply to continuous predictions. For
image segmentation, we use a similar generalization of in-
tersection over union. Then, we train a DVN on many out-
put examples encouraging the network to predict precise
(negative) loss scores for almost any output conﬁguration.
Figure 1 illustrates the gradient based inference process on
a DVN optimized for image segmentation.

Deep Value Networks

Input x

Gradient based inference
Step 10

Step 30

Step 5

GT label y∗

truth output structures in a high-dimensional space is often
infeasible, one measures the quality of a mapping via a loss
function (cid:96)(y, y(cid:48)) : Y ×Y → R+ that evaluates the distance
between different output structures. Given such a loss func-
tion, the quality of a mapping is measured by empirical loss
over a validation dataset D(cid:48),

(cid:88)

(cid:96)((cid:98)y(x), y∗)

(x,y∗)∈D(cid:48)

(1)

(2)

(3)

This loss can take an arbitrary form and is often non-
differentiable. For multi-label classiﬁcation, a common
loss is negative F1 score and for image segmentation, a typ-
ical loss is negative intersection over union (IOU).

Some structured output prediction methods (Taskar et al.,
2003; Tsochantaridis et al., 2004) learn a mapping from in-
puts to outputs via a score function s(x, y; θ), which evalu-
ates different input-output conﬁgurations based on a linear
function of some joint input-output features ψ(x, y),

s(x, y; θ) = θTψ(x, y) .

The goal of learning is to optimize a score function such
that the model’s predictions denoted (cid:98)y,

(cid:98)y = argmax

y

s(x, y; θ) ,

are closely aligned with ground-truth labels y∗ as measured
by empirical loss in (1) on the training set.

Empirical loss is not amenable to numerical optimization
because the argmax in (3) is discontinuous. Structural
SVM formulations (Taskar et al., 2003; Tsochantaridis
et al., 2004) introduce a margin violation (slack) variable
for each training pair, and deﬁne a continuous upper bound
on the empirical loss. The upper bound on the loss for
an example (x, y∗) and the model’s prediction (cid:98)y takes the
form:

(cid:96)((cid:98)y, y∗)

≤ max

≤ max

y

y

[ (cid:96)(y, y∗)+s(x, y; θ) ] − s(x, (cid:98)y; θ)
[ (cid:96)(y, y∗) + s(x, y; θ) ] − s(x, y∗; θ) .

(4a)

(4b)

Previous work (Taskar et al., 2003; Tsochantaridis et al.,
2004), deﬁnes a surrogate objective on the empirical loss,
by summing over the bound in (4b) for different training
examples, plus a regularizer. This surrogate objective is
convex in θ, which makes optimization convenient.

Figure 1. Segmentation results of DVN on Weizmann horses test
samples. Our gradient based inference method iteratively reﬁnes
segmentation masks to maximize the predicted scores of a deep
value network. Starting from a black mask at step 0, the predic-
tions converge within 30 steps yielding the output segmentation.
See https://goo.gl/8OLufh for more & animated results.

This paper presents a novel training objective for deep
structured output prediction, inspired by value-based re-
inforcement learning algorithms, to precisely evaluate the
quality of any input-output pair. We assess the effective-
ness of the proposed algorithm on multi-label classiﬁca-
tion based on text data and on image segmentation. We
obtain state-of-the-art results in both cases, despite the dif-
ferences of the domains and loss functions. Even given
a small number of input-output pairs, we ﬁnd that we are
able to build powerful structure prediction models. For ex-
ample, on the Weizmann horses dataset (Borenstein & Ull-
man, 2004), without any form of pre-training, we are able
to optimize 2.5 million network parameters on only 200
training images with multiple crops. Our deep value net-
work setup outperforms methods that are pre-trained on
large datasets such as ImageNet (Deng et al., 2009) and
methods that operate on 4× larger inputs. Our source code
based on TensorFlow (Abadi et al., 2015) is available at
https://github.com/gyglim/dvn.

2. Background

Structured output prediction entails learning a mapping
from input objects x ∈ X (e.g., X ≡ RM ) to multivariate
discrete outputs y ∈ Y (e.g., Y ≡ {0, 1}N ). Given a train-
ing dataset of input-output pairs, D ≡ {(x(i), y∗(i))}N
i=1,
we aim to learn a mapping (cid:98)y(x) : X → Y from inputs
to ground truth outputs. Because ﬁnding the exact ground

This paper is inspired by the structural SVM formulation
above, but we give up the convexity of the objective to
obtain more expressive models using a multi-layer neu-
ral networks. Speciﬁcally, we generalize the formulation
above in three ways: 1) use a non-linear score function de-
noted v(x, y; θ) that fuses ψ(·, ·) and θ together and jointly

Deep Value Networks

learns the features. 2) use gradient descend in y for itera-
tive reﬁnement of outputs to approximately ﬁnd the best
(cid:98)y(x). 3) optimize the score function with a regression ob-
jective so that the predicted scores closely approximate the
negative loss values,

∀y ∈ Y, v(x, y; θ) ≈ −(cid:96)(y, y∗) .

(5)

Our deep value network (DVN) is a non-linear function try-
ing to evaluate the value of any output conﬁguration y ∈ Y
accurately. In the structural SVM’s objective, the score sur-
face can vary as long as it does not violate margin con-
straints in (4b). By contrast, we restrict the score surface
much more by penalizing it whenever it over- or underesti-
mates the loss values. This seems to be beneﬁcial as a neu-
ral network v(x, y; θ) has a lot of ﬂexibility, and adding
more suitable constraints can help regularization.

We call our model a deep value network (DVN) to empha-
size the importance of the notion of value in shaping our
ideas, but the DVN architecture can be thought as an exam-
ple of structured prediction energy network (SPEN) (Be-
langer & McCallum, 2016) with similar inference strategy.
Belanger & McCallum rely on the structural SVM surro-
gate objective to train their SPENs, whereas inspired by
value based reinforcement learning, we learn an accurate
estimate of the values as in (5). Empirically, we ﬁnd that
the DVN outperforms large margin SPENs on multi-label
classiﬁcation using a similar neural network architecture.

3. Learning a Deep Value Network

We propose a deep value network architecture, denoted
v(x, y; θ), to evaluate a joint conﬁguration of an input and
a corresponding output via a neural network. More specif-
ically, the deep value network takes as input both x and y
jointly, and after several layers followed by non-linearities,
predicts a scalar v(x, y; θ), which evaluates the quality of
an output y and its compatibility with x. We assume that
during training, one has access to an oracle value function
v∗(y, y∗) = −(cid:96)(y, y∗), which quantiﬁes the quality of any
y. Such an oracle value function assigns optimal values to
any input-output pairs given ground truth labels y∗. During
training, the goal is to optimize the parameters of a value
network, denoted θ, to mimic the behavior of the oracle
value function v∗(y, y∗) as much as possible.

Example oracle value functions for image segmentation
and multi-label classiﬁcation include IOU and F1 metrics,
which are both deﬁned on (y, y∗) ∈ {0, 1}M × {0, 1}M ,

IOU(y, y∗) =
v∗

y ∩ y∗
y ∪ y∗ ,

(6)

(7)

Here y ∩ y∗ denotes the number of dimension i where
i are active and y ∪ y∗ denotes the number
both yi and y∗
of dimensions where at least one of yi and y∗
is active.
i
Assuming that one has learned a suitable value network
that attains v(x, y; θ) ≈ v∗(y, y∗) at every input-output
pairs, in order to infer a prediction for an input x, which
is valued highly by the value network, one needs to ﬁnd
(cid:98)y = argmaxy v(x, y; θ) as described below.

3.1. Gradient based inference

Since v(x, y; θ) represents a complex non-linear func-
tion of (x, y) induced by a neural network, ﬁnding (cid:98)y is
not straightforward, and approximate inference algorithms
based on graph-cut (Boykov et al., 2001) or loopy belief
propagation (Murphy et al., 1999) are not easily applicable.
Instead, we advocate using a simple gradient descent opti-
mizer for inference. To facilitate that, we relax the struc-
tured output variables to live in a real-valued space. For ex-
ample, instead of using y ∈ {0, 1}M , we use y ∈ [0, 1]M .
The key to make this inference algorithm work is that dur-
ing training we make sure that our value estimates are op-
timized along the inference trajectory. Alternatively, one
can make use of input convex neural networks (Amos et al.,
2016) to guarantee convergence to optimal (cid:98)y.
Given a continuous variable y, to ﬁnd a local optimum of
v(x, y; θ) w.r.t. y, we start from an initial prediction y(0)
(i.e., y(0) = [0]M in all of our experiments), followed by
gradient ascent for several steps,

y(t+1) = PY

y(t) + η

(cid:16)

v(x, y(t); θ)

,

(cid:17)

(8)

d
dy

where PY denotes an operator that projects the predicted
outputs back to the feasible set of solutions so that y(t+1)
remains in Y. In the simplest case, where Y = [0, 1]M , the
PY operator projects dimensions smaller than zero back to
zero, and dimensions larger than one to one. After the ﬁnal
gradient step T , we simply round y(T ) to become discrete.
Empirically, we ﬁnd that for a trained DVN, the generated
y(T )’s tend to become nearly binary themselves.

3.2. Optimization

To train a DVN using an oracle value function, ﬁrst, one
needs to extend the domain of v∗(y, y∗) so it applies to
continuous output y’s. For our IOU and F1 scores, we sim-
ply extend the notions of intersection and union by using
element-wise min and max operators,

y ∩ y∗ =

y ∪ y∗ =

(cid:88)M

i=1

(cid:88)M

i=1

min (yi, y∗

i ) ,

max (yi, y∗

i ) .

(9)

(10)

v∗
F1

(y, y∗) =

2 (y ∩ y∗)
(y ∩ y∗) + (y ∪ y∗)

.

Substituting (9) and (10) into (6) and (7) provides a gener-
alization of IOU and F1 score to [0, 1]M × [0, 1]M .

Deep Value Networks

Our training objective aims at minimizing the discrepancy
between v(x(i), y(i)) and v∗(i) on a training set of triplets
(input, output, value∗) denoted D ≡ {(x(i), y(i), v∗(i)}N
i=1.
Very much like Q-learning (Watkins & Dayan, 1992), this
training set evolves over time, and one can make use of an
experience replay buffer. In Section 3.3, we discuss several
strategies to generate training tuples and in our experiments
we evaluate such strategies in terms of their empirical loss,
once a gradient based optimizer is used to ﬁnd (cid:98)y.
Given a dataset of training tuples, one can use an appro-
priate loss to regress v(x, y) to v∗ values. More speciﬁ-
cally, since both IOU and F1 scores lie between 0 and 1,
we used a cross-entropy loss between oracle values vs. our
DVN values. As such, our neural network v(x, y) has a sig-
moid non-linearity at the top to predict a number between
0 and 1, and the loss takes the form,

LCE(θ) =

(cid:88)

− v∗ log v(x, y; θ)

(x,y,v∗)∈D

(11)

− (1 − v∗) log(1 − v(x, y; θ))

The exact form of the loss does not have a signiﬁcant im-
pact on the performance and other loss functions can be
used, e.g., L2. A high level overview for training a DVN
is shown in Algorithm 1. For simplicity, we show the case
when not using a queue and batch size = 1.

3.3. Generating training tuples

Each training tuple comprises an input, an output, and a
corresponding oracle value, i.e., (x, y, v∗). The way train-
ing tuples are generated signiﬁcantly impacts the perfor-
mance of our structured prediction algorithm. In particular,
it is important that the tuples are chosen such that they pro-
vide a good coverage of the space of possible outputs and
result in a large learning signal. There exist several ways to
generate training tuples including:

• running gradient based inference during training.
• generating adversarial tuples that have a large dis-

crepancy between v(x, y; θ) and v∗(y, y∗).

• random samples from Y, maybe biased towards y∗.
We elaborate on these methods below, and present a com-
parison of their performance in Section 5.4. Our ablation
experiments suggest that combining examples from gradi-
ent based inference with adversarial tuples works best.

Ground truth.
In this setup we simply add the ground
truth outputs y∗ into training with a v∗ = 1 to provide
some positive examples.

Inference. In this scenario, we generate samples by run-
ning a gradient based inference algorithm (Section 3.1)
along our training. This procedure is useful because it helps
learning a good value estimate on the output hypotheses
that are generated along the inference trajectory at test time.

Algorithm 1 Deep Value Network training
1: function TRAINEPOCH(training buffer D, initial weights θ,

learning rate λ)

while not converged do

(x, y∗) ∼ D
y ← GENERATEOUPUT(x, θ)
v∗ ← v∗(y, y∗)
(cid:46) Compute loss based on estimation error cf. (11)
L ← −v∗ log v(x, y; θ)

(cid:46) Get a training example
(cid:46) cf. Sec. 3.3
(cid:46) Get oracle value for y

−(1 − v∗) log(1 − v(x, y; θ))

(cid:46) Update DVN weights

2:
3:
4:
5:
6:
7:

θ ← θ − λ d

dθ L

8:
end while
9:
10: end function

To speed up training, we run a parallel inference job using
slightly older neural network weights and accumulate the
inferred examples in a queue.

Random samples.
In this approach, we sample a solu-
tion y proportional to its exponentiated oracle value, i.e., y
is sampled with probability p(y) ∝ exp{v∗(y, y∗)/τ },
where τ > 0 controls the concentration of samples in the
vicinity of the ground truth. At τ = 0 we recover the
ground truth samples above. We follow (Norouzi et al.,
2016) and sample from the exponentiated value distribution
using stratiﬁed sampling, where we group y’s according to
their values. This approach provides a good coverage of
the space of possible solutions.

Adversarial tuples. We maximize the cross-entropy loss
used to train the value network (11) to generate ad-
versarial tuples again using a gradient based optimizer
(e.g., see (Goodfellow et al., 2015; Szegedy et al., 2013).
Such adversarial tuples are the outputs y for which the net-
work over- or underestimates the oracle values the most.
This strategy ﬁnds some difﬁcult tuples that provide a use-
ful learning signal, while ensuring that the value network
has a minimum level of accuracy across all outputs y.

4. Related work

There has been a surge of recent interest in using neural net-
works for structured prediction (Zheng et al., 2015; Chen
et al., 2015; Song et al., 2016). The Structured Predic-
tion Energy Network (SPEN) of (Belanger & McCallum,
2016) inspired in part by (LeCun et al., 2006) is identical
to the DVN architecture. Importantly, the motivation and
the learning objective for SPENs and DVNs are distinct –
SPENs rely on a max-margin surrogate objective whereas
we directly regress the energy of an input-output pair to
its corresponding loss. Unlike SPENs that only consider
multi-label classiﬁcation problems, we also train a deep
convolutional network to successfully address complex im-
age segmentation problems.

Recent work has applied expressive neural networks to

Deep Value Networks

structured prediction to achieve impressive results on ma-
chine translation (Sutskever et al., 2014; Bahdanau et al.,
2015) and image and audio synthesis (van den Oord et al.,
2016b;a; Dahl et al., 2017). Such autoregressive models
impose an order on the output variables and predict out-
puts one variable at a time by formulating a locally normal-
ized probabilistic model. While training is often efﬁcient,
the key limitation of such models is inference complexity,
which grows linearly in the number of output dimensions;
this is not acceptable for high-dimensional output struc-
tures. By contrast, inference under our method is efﬁcient
as all of the output dimensions are updated in parallel.

Our approach is inspired in part by the success of previ-
ous work on value-based reinforcement learning (RL) such
as Q-learning (Watkins, 1989; Watkins & Dayan, 1992)
(see (Sutton & Barto, 1998) for an overview). The main
idea is to learn an estimate of the future reward under the
optimal behavior policy at any point in time. Recent RL
algorithms use a neural network function approximator as
the model to estimate the action values (Van Hasselt et al.,
2016). We adopt similar ideas for structured output pre-
diction, where we use the task loss as the optimal value
estimate. Unlike RL, we use a gradient based inference al-
gorithm to ﬁnd optimal solutions at test time.

Gradient based inference, sometimes called deep dreaming
has led to impressive artwork and has been inﬂuential in de-
signing DVN (Gatys et al., 2015; Mordvintsev et al., 2015;
Nguyen et al., 2016; Dumoulin et al., 2016). Deep dream-
ing and style transfer methods iteratively reﬁne the input
to a neural net to optimize a prespeciﬁed objective. Such
methods often use a pre-trained network to deﬁne a notion
of a perceptual loss (Johnson et al., 2016). By contrast,
we train a task speciﬁc value network to learn the charac-
teristics of a task speciﬁc loss function and we learn the
network’s weights from scratch.

Image segmentation (Arbelaez et al., 2012; Carreira et al.,
2012; Girshick et al., 2014; Hariharan et al., 2015), is a
key problem in computer vision and a canonical example
of structured prediction. Many segmentation approaches
based on Convolutional Neural Networks (CNN) have been
proposed (Girshick et al., 2014; Chen et al., 2014; Eigen &
Fergus, 2015; Long et al., 2015; Ronneberger et al., 2015;
Noh et al., 2015). Most use a deep neural network to make
a per-pixel prediction, thereby modeling pairs of pixels as
being conditionally independent given the input.

To diminish the conditional independence problem, recent
techniques propose to model dependencies among output
labels to reﬁne an initial CNN-based coarse segmenta-
tion. Different ways to incorporate pairwise dependen-
cies within a segmentation mask to obtain more expressive
models are proposed in (Chen et al., 2014; 2016; Ladick`y
et al., 2013; Zheng et al., 2015). Such methods perform

joint inference of the segmentation mask dimensions via
graph-cut (Li et al., 2015), message passing (Kr¨ahenb¨uhl &
Koltun, 2011) or loopy belief propagation (Murphy et al.,
1999), to name a few variants. Some methods incorpo-
rate higher order potentials in CRFs (Kohli et al., 2009) or
model global shape priors with Restricted Boltzmann Ma-
chines (Li et al., 2013; Kae et al., 2013; Yang et al., 2014;
Eslami et al., 2014). Other methods learn to iteratively re-
ﬁne an initial prediction by CNNs, which may just be a
coarse segmentation mask (Safar & Yang, 2015; Pinheiro
et al., 2016; Li et al., 2016).

By contrast, this paper presents a new framework for train-
ing a score function by having a gradient based inference
algorithm in mind during training. Our deep value network
applies to generic structured prediction tasks, as opposed to
some of the methods above, which exploit complex com-
binatorial structures and special constraints such as sub-
modularity to design inference algorithms. Rather, we use
expressive energy models and the simplest conceivable in-
ference algorithm of all – gradient descent.

5. Experimental evaluation

We evaluate the proposed Deep Value Networks on 3 tasks:
multi-label classiﬁcation, binary image segmentation, and a
3-class face segmentation task. Section 5.4 investigates the
sampling mechanisms for DVN training, and Section 5.5
visualizes the learned models.

5.1. Multi-label classiﬁcation

We start by evaluating the method on the task of predict-
ing tags from text inputs. We use standard benchmarks in
multi-label classiﬁcation, namely Bibtex and Bookmarks,
introduced in (Katakis et al., 2008). In this task, multiple
labels are possible per example, and the correct number is
not known. Given the structure in the label space, meth-
ods modeling label correlations often outperform models
with independent label predictions. We compare DVN to
standard baselines including per-label logistic regression
from (Lin et al., 2014), and a two-layer neural network with
cross entropy loss (Belanger & McCallum, 2016), as well
as SPENs (Belanger & McCallum, 2016) and PRLR (Lin
et al., 2014), which is the state-of-the-art on these datasets.
To allow direct comparison with SPENs, we adopt the same
architecture in this paper. Such an architecture combines
local predictions that are non-linear in x, but linear in y,
with a so-called global network, which scores label con-
ﬁguration with a non-linear function of y independent of x
(see Belanger & McCallum (2016), Eqs. (3) - (5)). Both lo-
cal prediction and global networks have one or two hidden
layers with Softplus non-linerarities. We follow the same
experimental protocol and report F1 scores on the same test
split as (Belanger & McCallum, 2016).

Deep Value Networks

Method

Method
Bibtex Bookmarks
37.2
Logistic regression (Lin et al., 2014)
NN baseline (Belanger & McCallum, 2016) 38.9
42.2
SPEN (Belanger & McCallum, 2016)
44.2
PRLR (Lin et al., 2014)
44.7
DVN (Ours)

30.7
33.8
34.4
34.9
37.1

Table 1. Tag prediction from text data. F1 performance of Deep
Value Networks compared to the state-of-the-art on multi-label
classiﬁcation. All prior results are taken from (Lin et al., 2014;
Belanger & McCallum, 2016)

2 CHOPPS (Li et al., 2013)
3
Fully conv (FCN) baseline
×
2
DVN (Ours)
3

8 MMBM2 (Yang et al., 2014)

MMBM2 + GC (Yang et al., 2014)
Shape NN (Safar & Yang, 2015)

e
z
i
s

t
u
p
n
I

2
1
×
8
2
1

Mean Global
IOU % IOU %

69.9
78.56
84.1
-
-
-

-
78.7
84.0
72.1
75.8
83.5

Table 2. Test IOU on Weizmann-32 × 32 dataset. DVN outper-
forms all previous methods, despite using a much lower input res-
olution than (Yang et al., 2014) and (Safar & Yang, 2015).

as determined on the validation set. We empirically found
τ = 0.05 to work best for stratiﬁed sampling. For training
data augmentation purposes we randomly crop the image,
similar to (Krizhevsky et al., 2012). At test time, various
strategies are possible to obtain a full resolution segmen-
tation, which we investigate in Section 5.4. For compari-
son we also implemented a Fully Convolutional Network
(FCN) baseline (Long et al., 2015), by using the same con-
volutional layers as for the value network (cf. Figure 2). If
not explicitly stated, masks are averaged over over 36 crops
for our model and (Long et al., 2015) (see below).

We test and compare our model on the Weizmann horses
segmentation task in Table 2. We tune the hyper-
parameters of the model on a validation set and, once best
hyper-parameters are found, ﬁne-tune on the combination
of training and validation sets. We report the mean image
IOU, as well as the IOU over the whole test set, as com-
monly done in the literature. It is clear that our approach
outperforms previous methods by a signiﬁcant margin on
both metrics. Our model shows strong segmentation re-
sults, without relying on externally trained CNN features
as (e.g., Safar & Yang (2015)). The weights of our value
network are learned from scratch on crops of just 200 train-
ing images. Even though the number of examples is very
small for this dataset, we did not observe overﬁtting dur-
ing training, which we attribute to being able to generate a
large set of segmentation masks for training.

In Figure 3 we show qualitative results for CHOPPS (Li
et al., 2013), our implementation of fully convolutional net-
works (FCN) (Long et al., 2015), and our DVN model.
When comparing our model to FCN, trained on the same
data and resolution, we ﬁnd that the FCN has challenges
correctly segmenting legs and ensuring that the segmenta-
tion masks have a single connected component (e.g., Fig-
ure 3, last two rows). Indeed, the masks produced by the
DVN correspond to much more reasonable horse shapes as
opposed to those of other methods – the DVN seem ca-
pable of learning complex shape models and effectively
grounding them to visual evidence. We also note that in

Figure 2. A deep value network with a feed-forward convolutional
architecture, used for segmentation. The network takes an image
and a segmentation mask as input and predicts a scalar evaluating
the compatibility between the input pairs.

The results are summarized in Table 1. As can be seen from
the table, our method outperforms the logistic regression
baselines by a large margin. It also signiﬁcantly improves
over SPEN, despite not using any pre-training. SPEN, on
the other hand, relies on pre-training of the feature network
with a logistic loss to obtain good results. Our results even
outperform (Lin et al., 2014). This is encouraging, as their
method is speciﬁc to classiﬁcation and encourages sparse
and low-rank predictions, whereas our technique does not
have such dataset speciﬁc regularizers.

5.2. Weizmann horses

The Weizmann horses dataset (Borenstein & Ullman,
2004) is a dataset commonly used for evaluating image seg-
mentation algorithms (Li et al., 2013; Yang et al., 2014;
Safar & Yang, 2015). The dataset consists of 328 im-
ages of left oriented horses and their binary segmentation
masks. We follow (Li et al., 2013; Yang et al., 2014; Sa-
far & Yang, 2015) and evaluate the segmentation results at
32×32 dimensions. Satisfactory segmentation of horses re-
quires learning strong shape priors and complex high level
reasoning, especially at a low resolution of 32×32 pixels,
because small parts such as the legs are often barely visible
in the RGB image. We follow the experimentation protocol
of (Li et al., 2013) and report results on the same test split.

For the DVN we use a simple CNN architecture consisting
of 3 convolutional and 2 fully connected layers (Figure 2).
We use a learning rate of 0.01 and apply dropout on the
ﬁrst fully connected layer with the keeping probability 0.75

Input size 24x24Input

CHOPPS [1] FCN [2]

DVN

GT label

Deep Value Networks

e
z
i
s

t
u
p
n
I

2
2
3

2
0
5
2

Method
Fully conv (FCN) baseline
DVN (Ours)
CRF (as in Kae et al. (2013))
GLOC (Kae et al., 2013)
DNN (Tsogkas et al., 2015)
DNN+CRF+SBM (Tsogkas et al., 2015)

SP Acc. %
95.36
92.44
93.23
94.95
96.54
96.97

Table 3. Superpixel accuracy (SP Acc.) on Labeled Faces in the
Wild test set.

Conﬁguration
Inference + Ground Truth
Inference + Stratiﬁed Sampling
Inference + Adversarial (DVN)
DVN + Mask averaging (9 crops)
DVN + Joint inference (9 crops)
DVN + Mask avg. non-binary (25 crops)
DVN + Joint inf. non-binary (25 crops)
DVN + Mask averaging (25 crops)
DVN + Joint inference (25 crops)

Mean IOU %
76.7
80.8
81.6
81.3
81.6
69.6
80.3
83.1
83.1

Table 4. Test performance of different conﬁgurations on the Weiz-
mann 32x32 dataset.

pre-training and more direct optimization of the per-pixel
prediction methods of (Tsogkas et al., 2015; Long et al.,
2015), (ii) the input resolution and (iii) the properties of
the dataset. In contrast to horses, faces do not have thin
parts and exhibit limited deformations. Thus, a feed for-
ward method as used in (Long et al., 2015), which produces
coarser and smooth predictions is sufﬁcient to obtain good
results. Indeed, this has also been observed in the negligi-
ble improvement of reﬁning CNN predictions with Condi-
tional Random Fields and Restricted Boltzmann machines
(cf. Table 3 last three rows). Despite this, our model is able
to learn a prior on the shape and align it with the image
evidence in most cases. Some failure cases include failing
to recognize subtle and more rare parts such as mustaches,
given their small size, and difﬁculties in correctly labeling
blond hair. Figure 4 shows qualitative results of our seg-
mentation method on this dataset.

5.4. Ablation experiments

In this section we analyze different conﬁgurations of our
method. As already mentioned, generating appropriate
training data for our method is key to learning good value
networks. We compare 3 main approaches: 1) inference +
ground truth, 2) inference + stratiﬁed sampling, and 3) in-
ference + adversarial training. These experiments are con-
ducted on the Weizmann dataset, described above. Table 4,
top portion, reports IOU results for different approaches for
training the dataset. As can be seen, including adversarial
training works best, followed by stratiﬁed sampling. Both
of these methods help explore the space of segmentation

Figure 3. Qualitative results on the Weizmann 32 × 32 dataset.
In comparison to previous works, DVN is able to learn a strong
shape prior and thus correctly detect the horse shapes including
legs. Previous methods are often misled by other objects or low
contrast, thus generating inferior masks. References: [1] Li et al.
(2013) [2] Our implementation of FCN (Long et al., 2015)

our comparison in Table 2, prior methods using larger in-
puts (e.g., 128×128) are also outperformed by DVNs.

5.3. Labeled Faces in the Wild

The Labeled Faces in the Wild (LFW) dataset (Huang et al.,
2007) was proposed for face recognition and contains more
than 13000 images. A subset of 2927 faces was later an-
notated for segmentation by Kae et al. (2013). The labels
are provided on a superpixel basis and consist of 3 classes:
face, hair and background. We use this dataset to test the
application of our approach to multiclass segmentation. We
use the same train, validation, and test splits as (Kae et al.,
2013; Tsogkas et al., 2015). As our method predicts labels
for pixels, we follow (Tsogkas et al., 2015) and map pixel
labels to superpixels by using the most frequent label in a
superpixel as the class. To train the DVN, we use mean
pixel accuracy as our oracle value function, instead of su-
perpixel accuracy.

Table 3 shows quantitative results. DVN performs reason-
ably well, but is outperformed by state of the art methods
on this dataset. We attribute this to three reasons. (i) the

Input

DVN

GT label

Deep Value Networks

(a)

(b)

(c)

(d)

Figure 5. Visualization of the learned horse shapes on the Weiz-
mann dataset. From left to right (a) The mean mask of the train-
ing set (b) mask generated when providing the mean horse image
from the training set (c, d) Outputs generated by our model given
mean horse image plus Gaussian noise (σ = 10) as the input.

this procedure are shown in Figure 5. As one can see, the
segmentation masks found by the value network on (noisy)
mean images resemble a side-view of a horse with some
uncertainty on the leg and head positions. These parts have
the most amount of variation in the dataset. Even though
noisy images do not contain horses, the value network hal-
lucinates proper horse silhouettes, which is what our model
is trained on.

This paper presents a framework for structured output pre-
diction by learning a deep value network that predicts the
quality of different output hypotheses for a given input. As
the DVN learns to predict a value based on both, input and
output, it implicitly learns a prior over output variables and
takes advantage of the joint modelling of the inputs and
outputs. By visualizing the prior for image segmentation,
we indeed ﬁnd that our model learns realistic shape priors.
Furthermore, rather than learning a model by optimizing a
surrogate loss, using DVNs allows to directly train a net-
work to accurately predict the desired performance metric
(e.g., IOU), even if it is non-differentiable. We apply our
method to several standard datasets in multi-label classiﬁ-
cation and image segmentation. Our experiments show that
DVNs apply to different structured prediction problems,
achieving state-of-the-art results with no pre-training.

As future work, we plan to improve the scalability and
computational efﬁciency of our algorithm by inducing in-
put features computed solely on x, which is going to be
computed only once. The gradient based inference can im-
prove by injecting noise to the gradient estimate, similar to
Hamiltonian Monte Carlo sampling. Finally, one can ex-
plore better ways to initialize the inference process.

7. Acknowledgment

We thank Kevin Murphy, Ryan & George Dahl, Vincent
Vanhoucke, Zhifeng Chen, and the Google Brain team for
insightful comments and discussions.

Figure 4. Qualitative results on 3-class segmentation on the LFW
dataset. The last two rows show failure cases, where our model
does not detect some of hair and moustache correctly.

6. Conclusion

masks in the vicinity of ground truth masks better, as op-
posed to just including the ground truth masks. Adding ad-
versarial examples works better than stratiﬁed sampling, as
the adversarial examples are the masks on which the model
is least accurate. Thus, these masks provide useful gradient
information as to help improve the model.

We also investigate ways to do model averaging (Table 4,
bottom portion). Averaging the segmentation masks of
multiple crops leads to improved performance. When the
masks are averaged na¨ıvely, the result becomes blurry,
making it difﬁcult to obtain a ﬁnal segmentation. Instead,
joint inference updates the complete segmentation mask
in each step, using the gradients of the individual crops.
This procedure leads to clean, near-binary segmentation
masks. This is manifested in the performance when using
the raw foreground conﬁdence (Table 4, Mask averaging
non-binary vs. Joint inference non-binary). Joint inference
leads to somewhat improved segmentation results, even af-
ter binarization, in particular when using fewer crops.

5.5. Visualizing the learned correlations

To visualize what the model has learned, we run our infer-
ence algorithm on the mean image of the Weizmann dataset
(training split). Optionally, we perturb the mean image by
adding some Gaussian noise. The masks obtained through

Deep Value Networks

References

Abadi, Mart´ın, Agarwal, Ashish, Barham, Paul, Brevdo,
Eugene, Chen, Zhifeng, Citro, Craig, Corrado, Greg S.,
Davis, Andy, Dean, Jeffrey, Devin, Matthieu, Ghe-
mawat, Sanjay, Goodfellow, Ian, Harp, Andrew, Irv-
ing, Geoffrey, Isard, Michael, Jia, Yangqing, Jozefowicz,
Rafal, Kaiser, Lukasz, Kudlur, Manjunath, Levenberg,
Josh, Man´e, Dan, Monga, Rajat, Moore, Sherry, Murray,
Derek, Olah, Chris, Schuster, Mike, Shlens, Jonathon,
Steiner, Benoit, Sutskever, Ilya, Talwar, Kunal, Tucker,
Paul, Vanhoucke, Vincent, Vasudevan, Vijay, Vi´egas,
Fernanda, Vinyals, Oriol, Warden, Pete, Wattenberg,
Martin, Wicke, Martin, Yu, Yuan, and Zheng, Xiaoqiang.
TensorFlow: Large-scale machine learning on heteroge-
neous systems, 2015. URL http://tensorflow.
org/. Software available from tensorﬂow.org.

Amos, Brandon, Xu, Lei, and Kolter, J Zico. Input convex

neural networks. arXiv:1609.07152, 2016.

Arbelaez, Pablo, Hariharan, Bharath, Gu, Chunhui, Gupta,
Saurabh, Bourdev, Lubomir, and Malik, Jitendra. Se-
mantic segmentation using regions and parts. CVPR,
2012.

Dahl, Ryan, Norouzi, Mohammad, and Shlens, Jonathon.
arXiv:1702.00783,

Pixel recursive super resolution.
2017.

Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai,
and Fei-Fei, Li. ImageNet: A Large-Scale Hierarchical
Image Database. CVPR, 2009.

Dumoulin, Vincent, Shlens, Jonathon, and Kudlur, Manju-
nath. A learned representation for artistic style. 2016.

Eigen, David and Fergus, Rob. Predicting depth, surface
normals and semantic labels with a common multi-scale
convolutional architecture. ICCV, 2015.

Eslami, SM Ali, Heess, Nicolas, Williams, Christopher KI,
and Winn, John. The shape boltzmann machine: a strong
model of object shape. IJCV, 2014.

Gatys, Leon A, Ecker, Alexander S, and Bethge, Matthias.
A neural algorithm of artistic style. arXiv:1508.06576,
2015.

Girshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik,
Jitendra. Rich feature hierarchies for accurate object de-
tection and semantic segmentation. CVPR, 2014.

Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio,
Yoshua. Neural machine translation by jointly learning
to align and translate. ICLR, 2015.

Goodfellow, Ian J, Shlens, Jonathon, and Szegedy, Chris-
tian. Explaining and harnessing adversarial examples.
ICLR, 2015.

Belanger, David and McCallum, Andrew. Structured pre-

diction energy networks. ICML, 2016.

Borenstein, E. and Ullman, S. Learning to segment. ECCV,

2004.

Boykov, Yuri, Veksler, Olga, and Zabih, Ramin. Fast ap-
IEEE

proximate energy minimization via graph cuts.
Trans. PAMI, 2001.

Carreira, Joao, Caseiro, Rui, Batista, Jorge, and Sminchis-
escu, Cristian. Semantic segmentation with second-order
pooling. ECCV, 2012.

Chen, Liang-Chieh, Papandreou, George, Kokkinos, Ia-
sonas, Murphy, Kevin, and Yuille, Alan L. Semantic im-
age segmentation with deep convolutional nets and fully
connected crfs. arXiv:1412.7062, 2014.

Chen, Liang-Chieh, Schwing, Alexander, Yuille, Alan,
and Urtasun, Raquel. Learning deep structured models.
ICML, 2015.

Chen, Liang-Chieh, Papandreou, Iasonas, Murphy, Kevin,
and Yuille, Alan L. Deeplab: Semantic image segmen-
tation with deep convolutional nets, atrous convolution,
and fully connected crfs. arXiv:1606.00915, 2016.

Hariharan, Bharath, Arbelaez, Pablo, and Girshick, Ross.
Hypercolumns for object segmentation and ﬁne-grained
localization. CVPR, 2015.

Huang, Gary B, Ramesh, Manu, Berg, Tamara, and
Learned-Miller, Erik. Labeled faces in the wild: A
database for studying face recognition in unconstrained
environments. Technical report, Technical Report, Uni-
versity of Massachusetts, Amherst, 2007.

Johnson, Justin, Alahi, Alexandre, and Fei-Fei, Li. Per-
ceptual losses for real-time style transfer and super-
resolution. ECCV, 2016.

Kae, Andrew, Sohn, Kihyuk, Lee, Honglak, and Learned-
Miller, Erik. Augmenting crfs with boltzmann machine
shape priors for image labeling. CVPR, 2013.

Katakis, Ioannis, Tsoumakas, Grigorios, and Vlahavas,
Ioannis. Multilabel text classiﬁcation for automated tag
suggestion. ECML PKDD discovery challenge, 2008.

Kohli, Pushmeet, Torr, Philip HS, et al. Robust higher order
potentials for enforcing label consistency. IJCV, 2009.

Kr¨ahenb¨uhl, Philipp and Koltun, Vladlen. Efﬁcient infer-
ence in fully connected crfs with gaussian edge poten-
tials. NIPS, 2011.

Deep Value Networks

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. NIPS, 2012.

Ladick`y, L’ubor, Russell, Chris, Kohli, Pushmeet, and
Inference methods for crfs with co-

Torr, Philip HS.
occurrence statistics. IJCV, 2013.

Lafferty, John, McCallum, Andrew, Pereira, Fernando,
et al. Conditional random ﬁelds: Probabilistic models
for segmenting and labeling sequence data. ICML, 2001.

LeCun, Yann, Chopra, Sumit, Hadsell, Raia, Ranzato, M,
and Huang, F. A tutorial on energy-based learning. Pre-
dicting structured data, 2006.

Li, Jianchao, Wang, Dan, Yan, Canxiang, and Shan,
Shiguang. Object segmentation with deep regression.
ICIP, 2015.

Li, Ke, Hariharan, Bharath, and Malik, Jitendra. Iterative

instance segmentation. CVPR, 2016.

Li, Yujia, Tarlow, Daniel, and Zemel, Richard. Explor-
ing compositional high order pattern potentials for struc-
tured output learning. CVPR, 2013.

Lin, Victoria (Xi), Singh, Sameer, He, Luheng, Taskar,
Ben, and Zettlemoyer, Luke. Multi-label learning with
posterior regularization. NIPS Workshop on Modern Ma-
chine Learning and Natural Language Processing, 2014.

Long, Jonathan, Shelhamer, Evan, and Darrell, Trevor.
Fully convolutional networks for semantic segmentation.
CVPR, 2015.

Mordvintsev, Alexander, Olah, Christopher, and Tyka,
Mike. Inceptionism: Going deeper into neural networks.
Google Research Blog., 2015.

Murphy, Kevin P, Weiss, Yair, and Jordan, Michael I.
Loopy belief propagation for approximate inference: An
empirical study. UAI, 1999.

Nguyen, Anh, Dosovitskiy, Alexey, Yosinski, Jason, Brox,
Thomas, and Clune, Jeff. Synthesizing the preferred in-
puts for neurons in neural networks via deep generator
networks. arXiv:1605.09304, 2016.

Noh, Hyeonwoo, Hong, Seunghoon, and Han, Bohyung.
Learning deconvolution network for semantic segmenta-
tion. ICCV, 2015.

Norouzi, Mohammad, Bengio, Samy, Chen, Zhifeng,
Jaitly, Navdeep, Schuster, Mike, Wu, Yonghui, and
Schuurmans, Dale. Reward augmented maximum likeli-
hood for neural structured prediction. NIPS, 2016.

Pinheiro, P., Lin, T.-Y., Collobert, R., , and Dollar, P. Learn-

ing to reﬁne object segments. ECCV, 2016.

Ronneberger, Olaf, Fischer, Philipp, and Brox, Thomas. U-
net: Convolutional networks for biomedical image seg-
mentation. MICCAI, 2015.

Safar, Simon and Yang, Ming-Hsuan. Learning shape pri-
ors for object segmentation via neural networks. ICIP,
2015.

Song, Yang, Schwing, Alexander, Zemel, Richard, and Ur-
tasun, Raquel. Training deep neural networks via direct
loss minimization. ICML, 2016.

Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence
to sequence learning with neural networks. NIPS, 2014.

Sutton, Richard and Barto, Andrew. Reinforcement learn-

ing: An introduction. The MIT Press, 1998.

Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya,
Bruna, Joan, Erhan, Dumitru, Goodfellow, Ian, and Fer-
Intriguing properties of neural networks.
gus, Rob.
ICLR, 2013.

Taskar, B., Guestrin, C., and Koller, D. Max-margin

Markov networks. NIPS, 2003.

Tsochantaridis, I., Hofmann, T., Joachims, T., and Altun,
Y. Support vector machine learning for interdependent
and structured output spaces. ICML, 2004.

Tsogkas, Stavros, Kokkinos, Iasonas, Papandreou, George,
and Vedaldi, Andrea.
se-
mantic part segmentation with high-level guidance.
arXiv:1505.02438, 2015.

Deep learning for

van den Oord, A¨aron, Dieleman, Sander, Zen, Heiga, Si-
monyan, Karen, Vinyals, Oriol, Graves, Alex, Kalch-
brenner, Nal, Senior, Andrew, and Kavukcuoglu, Ko-
ray. Wavenet: A generative model for raw audio.
arXiv:1609.03499, 2016a.

van den Oord, Aaron, Kalchbrenner, Nal, Espeholt, Lasse,
Kavukcuoglu, Koray, Vinyals, Oriol, and Graves, Alex.
Conditional image generation with pixelcnn decoders.
NIPS, 2016b.

Van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep
reinforcement learning with double q-learning. AAAI,
2016.

Watkins, Christopher J. C. H. and Dayan, Peter. Q-learning.

Machine Learning, 1992.

Watkins, Christopher JCH. Learning from delayed re-
wards. PhD thesis, University of Cambridge England,
1989.

Deep Value Networks

Yang, Jimei, Safar, Simon, and Yang, Ming-Hsuan. Max-
margin boltzmann machines for object segmentation.
CVPR, 2014.

Zheng, Shuai, Jayasumana, Sadeep, Romera-Paredes,
Bernardino, Vineet, Vibhav, Su, Zhizhong, Du, Dalong,
Huang, Chang, and Torr, Philip HS. Conditional random
ﬁelds as recurrent neural networks. CVPR, 2015.

