Supplementary Material to “Dynamic Word Embeddings”

Robert Bamler 1 Stephan Mandt 1

Table 1. Hyperparameters for skip-gram ﬁltering and skip-gram
smoothing.

PARAMETER COMMENT

Ntr = 1000

L = 104
L(cid:48) = 103
d = 100
d = 200
Ntr = 5000
N (cid:48)
tr = 5000

vocabulary size
batch size for smoothing
embedding dimension for SoU and Twitter
embedding dimension for Google books
number of training steps for each t (ﬁltering)
number of pretraining steps with minibatch
sampling (smoothing; see Algorithm 2)
number of training steps without minibatch
sampling (smoothing; see Algorithm 2)
context window size for positive examples
ratio of negative to positive examples
context exponent for negative examples
diffusion const. per year (Google books & SoU)
diffusion const. per year (Twitter)
variance of overall prior
learning rate (ﬁltering)
learning rate during minibatch phase (smoothing)
learning rate after minibatch phase (smoothing)
decay rate of 1st moment estimate
decay rate of 2nd moment estimate (ﬁltering)

cmax = 4
η = 1
γ = 0.75
D = 10−3
D = 1
σ2
0 = 1
α = 10−2
α(cid:48) = 10−2
α = 10−3
β1 = 0.9
β2 = 0.99
β2 = 0.999 decay rate of 2nd moment estimate (smoothing)
δ = 10−8
regularizer of Adam optimizer

1. Dimensionality Reduction in Figure 1

To create the word-clouds in Figure 1 of the main text we
mapped the ﬁtted word embeddings from Rd to the two-
dimensional plane using dynamic t-SNE (Rauber et al.,
2016). Dynamic t-SNE is a non-parametric dimension-
ality reduction algorithm for sequential data. The algo-
rithm ﬁnds a projection to a lower dimension by solving
a non-convex optimization problem that aims at preserving
nearest-neighbor relations at each individual time step. In

1Disney Research,

PA 15213, USA. Correspondence
<Robert.Bamler@disneyresearch.com>,
<Stephan.Mandt@disneyresearch.com>.

4720 Forbes Avenue, Pittsburgh,
Robert Bamler
Stephan Mandt

to:

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

addition, projections at neighboring time steps are aligned
with each other by a quadratic penalty with prefactor λ ≥ 0
for sudden movements.

There is a trade-off between ﬁnding good local projections
for each individual time step (λ → 0), and ﬁnding smooth
projections (large λ). Since we want to analyze the smooth-
ness of word embedding trajectories, we want to avoid
bias towards smooth projections. Unfortunately, setting
λ = 0 is not an option since, in this limit, the optimization
problem is invariant under independent rotations at each
time, rendering trajectories in the two-dimensional projec-
tion plane meaningless. To still avoid bias towards smooth
projections, we anneal λ exponentially towards zero over
the course of the optimization. We start the optimizer with
λ = 0.01, and we reduce λ by 5% with each training step.
We run 100 optimization steps in total, so that λ ≈ 6×10−6
at the end of the training procedure. We used the open-
source implementation,1 set the target perplexities to 200,
and used default values for all other parameters.

2. Hyperparemeters and Construction of n±
1:T

Table 1 lists the hyperparameters used in our experiments.
For the Google books corpus, we used the same context
window size cmax and embedding dimension d as in (Kim
et al., 2014). We reduced d for the SoU and Twitter corpora
to avoid overﬁtting to these much smaller data sets.

In constrast to word2vec, we construct our positive and
negative count matrices n±
ij,t deterministically in a prepro-
cessing step. As detailed below, this is done such that it
resembles as closely as possible the stochastic approach in
word2vec (Mikolov et al., 2013).
In every update step,
word2vec stochastically samples a context window size
uniformly in an interval [1, · · · , cmax], thus the context size
ﬂuctuates and nearby words appear more often in the same
context than words that are far apart from each other in
the sentence. We follow a deterministic scheme that re-
sults in similar statistics. For each pair of words (w1, w2)
in a given sentence, we increase the counts n+
by
max (0, 1 − k/cmax), where 0 ≤ k ≤ cmax is the num-
ber of words that appear between w1 and w2, and iw1 and
jw2 are the words’ unique indices in the vocabulary.

iw1 jw2

1https://github.com/paulorauber/thesne

Supplementary Material to “Dynamic Word Embeddings”

Algorithm 1 Skip-gram ﬁltering; see section 4 of the main
text.

Remark: All updates are analogous for word and con-
text vectors; we drop their indices for simplicity.
Input: number of time steps T , time stamps τ1:T , posi-
tive and negative examples n±
Init. prior means ˜µik,1 ← 0 and variances ˜Σi,1 = Id×d
Init. variational means µik,1 ← 0 and var. Σi,1 = Id×d
for t = 1 to T do
if t (cid:54)= 1 then

1:T , hyperparameters.

Update approximate Gaussian prior with params.
˜µik,t and ˜Σi,t using µik,t−1 and Σi,t−1, see Eq. 13.

end if
Compute entropy Eq[log q(·)] analytically.
Compute expected log Gaussian prior with parameters
˜µik,t and ˜Σk,t analytically.
Maximize Lt in Eq. 11, using black-box VI with the
reparametrization trick.
Obtain µik,t and Σi,t as outcome of the optimization.

end for

We also used a deterministic variant of word2vec to con-
struct the negative count matrices n−
t . In word2vec, η nega-
tive samples (i, j) are drawn for each positive sample (i, j(cid:48))
by drawing η independent values for j from a distribution
t (j) deﬁned below. We deﬁne n−
P (cid:48)
ij,t such that it matches
the expectation value of the number of times that word2vec
would sample the negative word-context pair (i, j). Specif-
ically, we deﬁne

Pt(i) =

P (cid:48)

t (j) =

(cid:80)L

(cid:80)L

ij,t

j=1 n+
i(cid:48),j=1 n+
(cid:0)Pt(j)(cid:1)γ

i(cid:48)j,t

,

(cid:80)L

j(cid:48)=1
(cid:18) L
(cid:88)

(cid:0)Pt(j(cid:48))(cid:1)γ ,
(cid:19)

i(cid:48),j(cid:48)=1

n−
ij,t =

n+

i(cid:48)j(cid:48),t

ηPt(i)P (cid:48)

t (j).

(1)

(2)

(3)

We chose γ = 0.75 as proposed in (Mikolov et al., 2013),
and we set η = 1. In practice, it is not necessary to explic-
itly construct the full matrices n−
t , and it is more efﬁcient
to keep only the distributions Pt(i) and P (cid:48)
t (j) in memory.

3. Skip-gram Filtering Algorithm

The skip-gram ﬁltering algorithm is described in section 4
of the main text. We provide a formulation in pseudocode
in Algorithm 1.

Algorithm 2 Skip-gram smoothing; see section 4. We drop
indices i, j, and k for word, context, end embedding dimen-
sion, respectively, when they are clear from context.

Input: number of time steps T , time stamps τ1:T , word-

context counts n+

1:T , hyperparameters in Table 1

Obtain n−
t ∀t using Eqs. 1–3
Initialize µu,1:T , µv,1:T ← 0
Initialize νu,1:T , νv,1:T , ωu,1:T −1, and ωv,1:T −1 such

that B(cid:62)

u Bu = B(cid:62)

for step = 1 to N (cid:48)

v Bv = Π (see Eqs. 5 and 11)
tr do

Draw I ⊂ {1, . . . , L(cid:48)} with |I| = L(cid:48) uniformly
Draw J ⊂ {1, . . . , L(cid:48)} with |J | = L(cid:48) uniformly
for all i ∈ I do
Draw (cid:15)[s]
Solve Bu,ixui,1:T = (cid:15)ui,1:T for xui,1:T

ui,1:T ∼ N (0, I)

end for
Obtain xvj,1:T by repeating last loop ∀j ∈ J
Calculate gradient estimates of L for minibatch

(I, J ) using Eqs. 10, 14, and 15

Obtain update steps d[·] for all variational parameters
using Adam optimizer with parameters in Table 1
Update µu,1:T ← µu,1:T + d[µu,1:T ], and analogously

for µv,1:T , ωu,1:T −1, and ωv,1:T −1

Update νu,1:T and νv,1:T according to Eq. 18

end for
Repeat above loop for Ntr more steps, this time without

minibatch sampling (i.e., setting L(cid:48) = L)

provided in Algorithm 2.

Variational distribution. For now, we focus on the word
embeddings, and we simplify the notation by dropping the
indices for the vocabulary and embedding dimensions. The
variational distribution for a single embedding dimension
of a single word embedding trajectory is

q(u1:T ) = N (µu,1:T , (B(cid:62)

u Bu)−1).

(4)

Here, µu,1:T is the vector of mean values, and Bu is the
Cholesky decomposition of the precision matrix. We re-
strict the latter to be bidiagonal,










Bu =

νu,1 ωu,1

νu,2 ωu,2
. . .

. . .










(5)

νu,T −1 ωu,T −1

νT

4. Skip-gram Smoothing Algorithm

In this section, we give details for the skip-gram smoothing
algorithm, see section 4 of the main text. A summary is

with νu,t > 0 for all t ∈ {1, . . . , T }. The variational pa-
rameters are µu,1:T , νu,1:T , and ω1:T −1. The variational
distribution of the context embedding trajectories v1:T has
the same structure.

Supplementary Material to “Dynamic Word Embeddings”

With the above form of Bu, the variational distribution is a
Gaussian with an arbitrary tridiagonal symmetric precision
matrix B(cid:62)
u Bu. We chose this variational distribution be-
cause it is the exact posterior of a hidden time-series model
with a Kalman ﬁltering prior and Gaussian noise (Blei &
Lafferty, 2006). Note that our variational distribution is a
generalization of a fully factorized (mean-ﬁeld) distribu-
tion, which is obtained for ωu,t = 0 ∀t.
In the general
case, ωu,t (cid:54)= 0, the variational distribution can capture cor-
relations between all time steps, with a dense covariance
matrix (B(cid:62)

u Bu)−1.

Gradient estimation. The skip-gram smoothing algo-
rithm uses stochastic gradient ascent to ﬁnd the variational
parameters that maximize the ELBO,

L = Eq

(cid:2)log p(U1:T , V1:T , n±

1:T )(cid:3) − Eq

(cid:2)log q(U1:T , V1:T )(cid:3).
(6)

Here, the second term is the entropy, which can be evalu-
ated analytically. We obtain for each component in vocab-
ulary and embedding space,

−Eq[log q(u1:T )] = −

log(νu,t) + const.

(7)

(cid:88)

t

and analogously for −Eq[log q(v1:T )].

The ﬁrst term on the right-hand side of Eq. 6 cannot be eval-
uated analytically. We approximate its gradient by sam-
pling from q using the reparameterization trick (Kingma &
Welling, 2014; Rezende et al., 2014). A naive calculation
would require Ω(T 2) computing time since the derivatives
of L with respect to νu,t and ωu,t for each t depend on
the count matrices n±
t(cid:48) of all t(cid:48). However, as we show next,
there is a more efﬁcient way to obtain all gradient estimates
in Θ(T ) time.

We focus again on a single dimension of a single word em-
bedding trajectory u1:T , and we drop the indices i and k.
We draw S independent samples u[s]
1:T with s ∈ {1, . . . , S}
from q(u1:T ) by parameterizing

using the Adam optimizer (Kingma & Ba, 2014), which ef-
fectively averages over several gradient estimates in its ﬁrst
moment estimate.

The derivatives of L with respect to µu,1:T can be obtained
using Eq. 8 and the chain rule. We ﬁnd

∂L
∂µu,1:T

≈

1
S

S
(cid:88)

(cid:104)

s=1

u,1:T − Πu[s]
Γ[s]

1:T

(cid:105)

.

(10)

Here, Π ∈ RT ×T is the precision matrix of the prior
u1:T ∼ N (0, Π−1).
It is tridiagonal and therefore the
matrix-multiplication Πu[s]
1:T can be carried out efﬁciently.
The non-zero matrix elements of Π are

Π11 = σ−2
ΠT T = σ−2
Πtt = σ−2

0 + σ−2
1
0 + σ−2
T −1
t−1 + σ−2
0 + σ−2

Π1,t+1 = Πt+1,1 = −σ−2

t

t
.

∀t ∈ {2, . . . , T − 1}

The term Γ[s]
u,1:T on the right-hand side of Eq. 10 comes
from the expectation value of the log-likelihood under q. It
is given by

Γ[s]
ui,t =

(cid:104)(cid:0)n+

ij,t + n−

ij,t

(cid:16)

(cid:1) σ

−u[s](cid:62)

i,t v[s]

j,t

(cid:17)

− n−
ij,t

(cid:105)
v[s]
j,t

L
(cid:88)

j=1

where we temporarily restored the indices i and j for words
and contexts, respectively. In deriving Eq. 12, we used the
relations ∂ log σ(x)/∂x = σ(−x) and σ(−x) = 1 − σ(x).

The derivatives of L with respect to νu,t and ωu,t are
more intricate. Using the parameterization in Eqs. 8–9, the
u /∂νt and ∂B−1
derivatives are functions of ∂B−1
u /∂ωt, re-
spectively, where B−1
u is a dense (upper triangular) T × T
matrix. An efﬁcient way to obtain these derivatives is to
use the relation

∂B−1
u
∂νt

= −B−1

u

∂Bu
∂νt

B−1
u

1:T = µu,1:T + x[s]
u[s]

u,1:T

(8)

and similarly for ∂B−1
Eqs. 8–9, we obtain the gradient estimates

u /∂ωt. Using this relation and

with

x[s]
u,1:T = B−1

u (cid:15)[s]

u,1:T where

(cid:15)[s]
u,1:T ∼ N (0, I).

(9)

u,1:T = (cid:15)[s]

We obtain x[s]
u,1:T in Θ(T ) time by solving the bidiagonal
linear system Bux[s]
1:T for the
context embedding trajectories are obtained analogously.
Our implementation uses S = 1, i.e., we draw only a sin-
gle sample per training step. Averaging over several sam-
ples is done implicitly since we calculate the update steps

u,1:T . Samples v[s]

∂L
∂νu,t

∂L
∂ωu,t

≈ −

≈ −

1
S

1
S

S
(cid:88)

s=1

S
(cid:88)

s=1

u,tx[s]
y[s]

u,t −

1
νu,t

,

u,tx[s]
y[s]

u,t+1.

The second term on the right-hand side of Eq. 14 is the
derivative of the entropy, Eq. 7, and

y[s]
u,1:T = (B(cid:62)

u )−1 (cid:104)

u,1:T − Πu[s]
Γ[s]

1:T

(cid:105)

.

(16)

(11)

(12)

(13)

(14)

(15)

Supplementary Material to “Dynamic Word Embeddings”

The values y[s]
bringing B(cid:62)
sponding bidiagonal linear system of equations.

u,1:T can again be obtained in Θ(T ) time by
u to the left-hand side and solving the corre-

Sampling in vocabulary space.
In the above paragraph,
we described an efﬁcient strategy to obtain gradient esti-
mates in only Θ(T ) time. However, the gradient estimation
scales quadratic in the vocabulary size L because all L2 el-
ements of the positive count matrices n+
t contribute to the
gradients. In order speed up the optimization, we pretrain
the model using a minibatch of size L(cid:48) < L in vocabulary
space as explained below. The computational complexity
of a single training step in this setup scales as (L(cid:48))2 rather
than L2. After N (cid:48)
tr = 5000 training steps with minibatch
size L(cid:48), we switch to the full batch size of L and train the
model for another Ntr = 1000 steps.

The subsampling in vocabulary space works as follows. In
each training step, we independently draw a set I of L(cid:48)
random distinct words and a set J of L(cid:48) random distinct
contexts from a uniform probability over the vocabulary.
We then estimate the gradients of L with respect to only
the variational parameters that correspond to words i ∈ I
and contexts j ∈ J . This is possible because both the prior
of our dynamic skip-gram model and the variational distri-
bution factorize in the vocabulary space. The likelihood of
the model, however, does not factorize. This affects only
the deﬁnition of Γ[s]
uik,t in Eq. 12. We replace Γ[s]
uik,t by an
estimate Γ[s](cid:48)
uik,t based on only the contexts j ∈ J in the
current minibatch,

Γ[s]
ui,t =

L
L(cid:48)

(cid:88)

(cid:104) (cid:0)n+

j∈J

ij,t + n−
(cid:105)

ij,t

− n−
ij,t

v[s]
j,t.

(cid:16)

(cid:1) σ

−u[s](cid:62)

i,t v[s]

j,t

(cid:17)

(17)

Here, the prefactor L/L(cid:48) restores the correct ratio between
evidence and prior knowledge (Hoffman et al., 2013).

Enforcing positive deﬁniteness. We update the varia-
tional parameters using stochastic gradient ascent with the
Adam optimizer (Kingma & Ba, 2014). The parame-
ters νu,1:T are the eigenvalues of the matrix Bu, which
is the Cholesky decomposition of the precision matrix
of q. Therefore, νu,t has to be positive for all t ∈
{1, . . . , T }. We use mirror ascent (Ben-Tal et al., 2001;
Beck & Teboulle, 2003) to enforce νu,t > 0. Speciﬁcally,
we update νt to a new value ν(cid:48)
(cid:115)(cid:18) 1
2

νu,td[νu,t] +

t deﬁned by

νu,td[νu,t]

ν(cid:48)
u,t =

+ ν2
u,t

(18)

1
2

(cid:19)2

where d[νu,t] is the step size obtained from the Adam opti-
mizer. Eq. 18 can be derived from the general mirror ascent
u,t) = Φ(cid:48)(νu,t) + d[νu,t] with the mirror
update rule Φ(cid:48)(ν(cid:48)

map Φ : x (cid:55)→ −c1 log(x) + c2x2/2, where we set the pa-
rameters to c1 = νu,t and c2 = 1/νu,t for dimensional
reasons. The update step in Eq. 18 increases (decreases)
νu,t for positive (negative) d[νu,t], while always keeping
its value positive.

Natural basis. As a ﬁnal remark, let us discuss an op-
tional extension to the skip-gram smoothing algorithm that
converges in less training steps. This extension only in-
creases the efﬁciency of the algorithm. It does not change
the underlying model or the choice of variational distri-
bution. Observe that the prior of the dynamic skip-gram
model connects only neighboring time-steps with each
other. Therefore, the gradient of L with respect to µu,t
depends only on the values of µu,t−1 and µu,t+1. A naive
implementation of gradient ascent would thus require T −1
update steps until a change of µu,1 affects updates of µu,T .

This problem can be avoided with a change of basis from
µu,1:T to new parameters ρu,1:T ,

µu,1:T = Aρu,1:T

(19)

with an appropriately chosen invertible matrix A ∈ RT ×T .
Derivatives of L with respect to ρu,1:T are given by the
chain rule, ∂L/∂ρu,1:T = (∂L/∂µu,1:T )A. A natural (but
inefﬁcient) choice for A is to stack the eigenvectors of the
prior precision matrix Π, see Eq. 11, into a matrix. The
eigenvectors of Π are the Fourier modes of the Kalman ﬁl-
tering prior (with a regularization due to σ0). Therefore,
there is a component ρu,t that corresponds to the zero-mode
of Π, and this component learns an average word embed-
ding over all times. Higher modes correspond to changes
of the embedding vector over time. A single update to the
zero immediately affects all elements of µu,1:T , and there-
fore changes the word embeddings at all time steps. Thus,
information propagates quickly along the time dimension.
The downside of this choice for A is that the transforma-
tion in Eq. 19 has complexity Ω(T 2), which makes update
steps slow.

As a compromise between efﬁciency and a natural basis,
we propose to set A in Eq. 19 to the Cholesky decomposi-
tion of the prior covariance matrix Π−1 ≡ AA(cid:62). Thus, A
is still a dense (upper triangular) matrix, and, in our experi-
ments, updates to the last component ρu,T affect all compo-
nents of µu,1:T in an approximately equal amount. Since Π
is tridiagonal, the inverse of A is bidiagonal, and Eq. 19 can
be evaluated in Θ(T ) time by solving Aµu,1:T = ρu,1:T for
µu,1:T . This is the parameterization we used in our imple-
mentation of the skip-gram smoothing algorithm.

References

Beck, Amir and Teboulle, Marc. Mirror Descent and Non-
linear Projected Subgradient Methods for Convex Opti-

Supplementary Material to “Dynamic Word Embeddings”

mization. Operations Research Letters, 31(3):167–175,
2003.

Ben-Tal, Aharon, Margalit, Tamar, and Nemirovski,
Arkadi. The Ordered Subsets Mirror Descent Optimiza-
tion Method with Applications to Tomography. SIAM
Journal on Optimization, 12(1):79–108, 2001.

Blei, David M and Lafferty, John D. Dynamic Topic Mod-
els. In Proceedings of the 23rd International Conference
on Machine Learning, pp. 113–120. ACM, 2006.

Hoffman, Matthew D, Blei, David M, Wang, Chong, and
Paisley, John William. Stochastic Variational Inference.
Journal of Machine Learning Research, 14(1):1303–
1347, 2013.

Kim, Yoon, Chiu, Yi-I, Hanaki, Kentaro, Hegde, Dar-
shan, and Petrov, Slav. Temporal Analysis of Language
In Proceedings of
Through Neural Language Models.
the ACL 2014 Workshop on Language Technologies and
Computational Social Science, pp. 61–65, 2014.

Kingma, Diederik and Ba, Jimmy. Adam: A Method
preprint

arXiv

for Stochastic Optimization.
arXiv:1412.6980, 2014.

Kingma, Diederik P and Welling, Max. Auto-Encoding
In Proceedings of the 2nd Interna-
Variational Bayes.
tional Conference on Learning Representations (ICLR),
2014.

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed Representations of
Words and Phrases and their Compositionality. In Ad-
vances in Neural Information Processing Systems 26, pp.
3111–3119. 2013.

Rauber, Paulo E., Falc˜ao, Alexandre X., and Telea, Alexan-
dru C. Visualizing Time-Dependent Data Using Dy-
namic t-SNE. In EuroVis 2016 - Short Papers, 2016.

Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,
Daan. Stochastic Backpropagation and Approximate In-
ference in Deep Generative Models. In The 31st Interna-
tional Conference on Machine Learning (ICML), 2014.

