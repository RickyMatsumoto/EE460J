Post-Inference Prior Swapping

Appendix for “Post-Inference Prior Swapping”

A. Details on the IS Example (Sec. 2.1)

Here we provide details on the IS example (for a normal πf and Laplace π) given in Sec. 2.1.
We made the following statement: if pf (θ|xn) = N (θ|m, s2), in order for |µh − Epf [ˆµIS

h ]| < δ, we need

T ≥ exp

(cid:26) 1
2s2 (|µh − m| − δ)2

(cid:27)

.

To show this, we ﬁrst give an upper bound on the expected value of the maximum of T zero-mean s2-variance
Gaussian random variables. Let {˜θt}T
t=1. Then, for
some b > 0,

t=1 ∼ g, where g(θ) = N (θ|0, s2), and let Z = maxt{˜θt}T

exp{bEg[Z]} ≤ Eg[exp{bZ}] = Eg

(cid:20)

max
t

(cid:110)
exp{b˜θt}

(cid:111)T

(cid:21)

≤

T
(cid:88)

t=1

t=1

(cid:104)

Eg

exp{b˜θt}

(cid:105)

= T exp{b2s2/2},

where the ﬁrst inequality is due to Jensen’s inequality, and the ﬁnal equality is due to the deﬁnition of a Gaussian
moment generating function. The above implies that

Setting b =

s2 log T , we have that

(cid:113) 2

Eg[Z] ≤

log T
b

+

bs2
2

.

(cid:104)

Eg

max
t

{˜θt}T

t=1

(cid:105)

= Eg[Z] ≤ s(cid:112)2 log T .

However, note that for all {˜θt}T
h(θ) = θ must be less than or equal to maxt{˜θt}T
the maximum of this set). Therefore,

t=1, and weights {w(˜θt)}T

t=1 (such that (cid:80)T

t=1 (since the weighted average of {˜θt}T

t=1 w(˜θt) = 1), the IS estimate ˆµIS

h for
t=1 cannot be larger than

and equivalently

Eg

(cid:2)ˆµIS

h

(cid:3) ≤ Eg

(cid:104)

max
t

{˜θt}T

t=1

(cid:105)

≤ s(cid:112)2 log T ,

T ≥ exp

(cid:26) 1
2s2

Eg

(cid:2)ˆµIS

h

(cid:3)2(cid:27)

.

In our example, we wanted the expected estimate to be within δ of µh, i.e. we wanted |µh − Eg[ˆµIS
δ − µh ≤ Eg[ˆµIS

h ]| < δ ⇐⇒

h ] ≤ µh + δ, and therefore,
(cid:26) 1
2s2

T ≥ exp

(cid:3)2(cid:27)

Eg

(cid:2)ˆµIS

h

≥ exp

(cid:26) 1
2s2 (δ − µh)2

(cid:27)

.

Finally, notice that the original statement involved samples {˜θt}T
t=1 ∼ pf (θ|xn) = N (m, s2) (instead of from
g = N (0, s2)). But this is equivalent to setting pf (θ|xn) = g(θ), and shifting our goal so that we want δ − |µh −
m| ≤ Epf [ˆµIS

h ] ≤ |µh − m| + δ. This gives us the desired bound:

T ≥ exp

(cid:26) 1
2s2

(cid:3)2(cid:27)

Epf

(cid:2)ˆµIS

h

≥ exp

(cid:26) 1
2s2 (δ − |µh − m|)2

(cid:27)

.

Post-Inference Prior Swapping

B. Prior Swapping Pseudocode (for a false posterior PDF inference result ˜pf (θ))

Here we give pseudocode for the prior swapping procedure, given some false posterior PDF inference result ˜pf (θ),
using the prior swap functions ps(θ) ∝ ˜pf (θ)π(θ)
and ∇θ log ps(θ) ∝ ∇θ log ˜pf (θ)+∇θ log π(θ)−∇θ log πf (θ),
as described in Sec. 2.2.

πf (θ)

In Alg. 2, we show prior swapping via the Metropolis-Hastings algorithm, which makes repeated use of ps(θ).
In Alg. 3 we show prior swapping via Hamiltonian Monte Carlo, which makes repeated use of ∇θ log ps(θ). A
special case of Alg. 3, which occurs when we set the number of simulation steps to L = 1 (in line 6), is prior
swapping via Langevin dynamics.

Algorithm 2: Prior swapping via Metropolis-Hastings.
Input: Prior swap function ps(θ), and proposal q.
Output: Samples {θt}T

t=1 ∼ ps(θ) as T → ∞.

1 Initialize θ0.
2 for t = 1, . . . , T do

(cid:46) Initialize Markov chain.

(cid:46) Propose new sample.

Draw θs ∼ q(θs | θt−1).
Draw u ∼ Unif([0, 1]).
(cid:110)

if u < min

1, ps(θs)q(θt|θs)
ps(θt)q(θs|θt)

(cid:111)

then

Set θt ← θs.

(cid:46) Accept proposed sample.

else

Set θt ← θt−1.

(cid:46) Reject proposed sample.

Algorithm 3: Prior swapping via Hamiltonian Monte Carlo.
Input: Prior swap function ps(θ), its gradient-log ∇θ log ps(θ), and step-size (cid:15).
Output: Samples {θt}T

t=1 ∼ ps(θ) as T → ∞.

(cid:46) Initialize Markov chain.

1 Initialize θ0.
2 for t = 1, . . . , T do

(cid:46) Propose new sample (next 4 lines).

Draw rt ∼ N (0, I).
Set ((cid:101)θ0, (cid:101)r0) ← (θt−1, rt−1)
Set (cid:101)r0 ← (cid:101)r0 + (cid:15)
for l = 1, . . . , L do

2 ∇θ log ps((cid:101)θ0) .

Set (cid:101)θl ← (cid:101)θl−1 + (cid:15)(cid:101)rl−1.
Set (cid:101)rl ← (cid:101)rl−1 + (cid:15)∇θ log ps((cid:101)θl).

2 ∇θ log ps((cid:101)θL) .

Set (cid:101)rL ← (cid:101)rL + (cid:15)
Draw u ∼ Unif([0, 1]).
(cid:110)

if u < min

1,

ps((cid:101)θL)(cid:101)r(cid:62)
ps(θt−1)r(cid:62)

L (cid:101)rL
t−1rt−1

(cid:111)

then

Set θt ← (cid:98)θL.

else

Set θt ← θt−1.

(cid:46) Accept proposed sample.

(cid:46) Reject proposed sample.

3

4

5

6

7

8

3

4

5

6

7

8

9

10

11

12

13

14

C. Proofs of Theoretical Guarantees

Post-Inference Prior Swapping

Here, we prove the theorems stated in Sec. 2.3.
Throughout this analysis, we assume that we have T samples {˜θt}Tf
t=1 ⊂ X ⊂ Rd from the false-posterior
pf (θ|xn), and that b ∈ R+ denotes the bandwidth of our semiparametric false-posterior density estimator ˜psp
f (θ).
Let H¨older class Σ(2, L) on X be deﬁned as the set of all (cid:96) = (cid:98)2(cid:99) times differentiable functions f : X → R
whose derivative f (l) satisﬁes

|f ((cid:96))(θ) − f ((cid:96))(θ(cid:48))| ≤ L |θ − θ(cid:48)|2−(cid:96)

for all θ, θ(cid:48) ∈ X .

Let the class of densities P(2, L) be

P(2, L) =

f ∈ Σ(2, L)

f (θ)dθ = 1

.

(cid:26)

(cid:90)

(cid:12)
(cid:12)
(cid:12) f ≥ 0,

(cid:27)

Let data xn = {x1, . . . , xn} ⊂ Y ⊂ Rp, let Z ⊂ Y be any set such that xn ⊂ Z, and let FZ (L) denote the set of
densities p : Y → R that satisfy

| log p(x) − log p(x(cid:48))| ≤ L|x − x(cid:48)|,

for all x, x(cid:48) ∈ Z.

In the following theorems, we assume that the false-posterior density pf (θ|xn) is bounded, i.e. that there exists
some B > 0 such that pf (θ|xn) ≤ B for all θ ∈ Rd; that the prior swap density ps(θ) ∈ P(2, L); and that the
model family p(xn|θ) ∈ FZ (L) for some Z.

Theorem 2.1. For any α = (α1, . . . , αk) ⊂ Rp and k > 0 let ˜pα
M > 0 such that pf (θ|xn)
f (θ) < M , for all θ ∈ Rd.

˜pα

f (θ) be deﬁned as in Eq. (8). Then, there exists

Proof. To prove that there exists M > 0 such that pf (θ|xn)

f (θ) < M , note that the false posterior can be written
˜pα

and the parametric estimate ˜pα

f (θ) is deﬁned to be

pf (θ|xn) =

πf (θ)

L(θ|xi) =

πf (θ)

p(xi|θ),

1
Z1

1
Z1

n
(cid:89)

i=1

n
(cid:89)

i=1

1
Z2

˜pα
f (θ) =

πf (θ)

p(αj|θ)n/k.

k
(cid:89)

j=1

Let d = maxi,j |xi − αj|. For any i ∈ {1, . . . , n}, j ∈ {1, . . . , k},
(cid:12)
(cid:12)
(cid:12)
(cid:12)

| log p(xi|θ) − log p(αj|θ)| ≤ Ld =⇒

log

p(xi|θ)
p(αj|θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ Ld,

and

Therefore

(cid:26)

exp

log

(cid:27)

p(xi|θ)
p(αj|θ)

≤ exp

log

(cid:26)(cid:12)
(cid:12)
(cid:12)
(cid:12)

p(xi|θ)
p(αj|θ)

(cid:27)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ exp{Ld} =⇒

≤ exp{Ld}.

p(xi|θ)
p(αj|θ)

pf (θ|xn)
˜pα
f (θ)

≤

Z2
Z1

(cid:81)n

i=1 p(xi|θ)
j=1 p(αj|θ)n/k

(cid:81)k

Z2
Z1

≤

exp{nLd} = M.

Post-Inference Prior Swapping

t=1 ∼ pα
Corollary 2.1.1. For {θt}T
that satisﬁes Varp [h(θ)] < ∞, the variance of IS estimate ˆµPSis

s (θ) ∝

, w(θt) = pf (θt|xn)
˜pα
f (θt)
h = (cid:80)T

˜pα
f (θ)π(θ)
πf (θ)

(cid:16)(cid:80)T

r=1

pf (θr|xn)
˜pα
f (θr)

(cid:17)−1

t=1 h(θt)w(θt) is ﬁnite.

, and test function

Proof. This follows directly from the sufﬁcient conditions for ﬁnite variance IS estimates given by (Geweke,
1989), which we have proved are satisﬁed for ˆµPSis

in Theorem 2.1.

h

Theorem 2.2. Given false posterior samples {˜θt}Tf
consistent for p(θ|xn), i.e. its mean-squared error satisﬁes

t=1 ∼ pf (θ|xn) and b (cid:16) T −1/(4+d)

f

, the estimator psp
s

is

sup
p(θ|xn)∈P(2,L)

(cid:20)(cid:90)

E

(psp

s (θ) − p(θ|xn))2 dθ

<

(cid:21)

c
T 4/(4+d)
f

for some c > 0 and 0 < b ≤ 1.

Proof. To prove mean-square consistency of our semiparametric prior swap density estimator psp
s , we give a
bound on the mean-squared error (MSE), and show that it tends to zero as we increase the number of samples Tf
drawn from the false-posterior. To prove this, we bound the bias and variance of the estimator, and use this to
bound the MSE. In the following, to avoid cluttering notation, we will drop the subscript pf in Epf [·].
We ﬁrst bound the bias of our semiparametric prior swap estimator. For any p(θ|xn) ∈ P(2, L), we can write the
bias as

|E [psp

s (θ)] − p(θ|xn)| = c1

E

− pf (θ|xn)

(cid:20)
˜psp
f (θ)

π(θ)
πf (θ)

(cid:21)

(cid:105)

(cid:104)

E

π(θ)
πf (θ)
(cid:104)
˜psp
f (θ)

E

(cid:105)

− pf (θ|xn)

(cid:12)
(cid:12)
(cid:12)

˜psp
f (θ)

− pf (θ|xn)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

π(θ)
πf (θ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= c2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤ ch2
(cid:12)
(cid:104)
(cid:12)
(cid:12)

= c3

E

for some c > 0, where we have used the fact that
& Glad, 1995; Wasserman, 2006)).

(cid:105)
˜psp
f (θ)

(cid:12)
(cid:12)
(cid:12) ≤ ˜ch2 for some ˜c > 0 (given in (Hjort
− pf (θ|xn)

We next bound the variance of our semiparametric prior swap estimator. For any p(θ|xn) ∈ P(2, L), we can write
the variance of our estimator as

Var [psp

s (θ)] = c1Var

(cid:20)
˜psp
f (θ)

(cid:21)

π(θ)
πf (θ)
(cid:105)
˜psp
f (θ)

(cid:104)

=

π(θ)2
πf (θ)2 Var
c
Tf hd
(cid:105)
(cid:104)
˜psp
for some c > 0, where we have used the facts that Var
f (θ)
≤ ˜c for
some ˜c > 0 (given in (Hjort & Glad, 1995; Wasserman, 2006)). Next, we will use these two results to bound the
mean-squared error of our semiparametric prior swap estimator, which shows that it is mean-square consistent.

T hd for some c > 0 and E

˜psp
f (θ)

≤ c

(cid:105)2

≤

(cid:104)

We can write the mean-squared error as the sum of the variance and the bias-squared, and therefore,

(cid:20)(cid:90)

E

(psp

s (θ) − p(θ|xn))2 dθ

≤ c1h2 +

(cid:21)

c2
T hd

=

c
T 4/(4+d)
f

for some c > 0, using the fact that h (cid:16) T −1/(4+d)

.

f

D. Further Empirical Results

Post-Inference Prior Swapping

Here we show further empirical results on a logistic regression model with hierarchical target prior given by
π = N (0, α−1I), α ∼ Gamma(γ, 1). We use synthetic data so that we are able to compare the timing and
posterior error of different methods as we tune n and d.

In this experiment, we assume that we are given samples from a false posterior pf (θ|xn), and we want to most-
efﬁciently compute the target posterior under prior π(θ).
In addition to the prior swapping methods, we can
run standard iterative inference algorithms, such as MCMC or variational inference (VI), on the target posterior
(initializing them, for example, at the false posterior mode) as comparisons. The following experiments aim to
show that, once the data size n grows large enough, prior swapping methods become more efﬁcient than standard
inference algorithms. They also aim to show that the held-out test error of prior swapping matches that of these
standard inference algorithms. In these experiments, we also add a prior swap method called prior swapping VI;
this method involves making a VI approximation to pf (θ|xn), and using it for ˜pf (θ). Prior swapping VI allows us
to see whether the test error is similar to standard VI inference algorithms, which compute some approximation
to the posterior. Finally, we show results over a range of target prior hyperparameter values γ to show that prior
swapping maintains accuracy (i.e. has a similar error as standard inference algorithms) over the full range.

We show results in Fig. 6. In (a) and (b) we vary the number of observations (n=10-120,000) and see that prior
swapping has a constant wall time while the wall times of both MCMC and VI increase with n. In (b) we see that
the prior swapping methods achieve the same test error as the standard inference methods. In (c) and (d) we vary
the number of dimensions (d=1-40). In this case, all methods have increasing wall time, and again the test errors
match. In (e), (f), and (g), we vary the prior hyperparameter (γ=1-1.05). For prior swapping, we infer a single
˜pf (θ) (using γ = 1.025) with both MCMC and VI applied to pf (θ|xn), and compute all other hyperparameter
results using this ˜pf (θ). This demonstrates that prior swapping can quickly infer correct results over a range
of hyperparameters. Here, the prior swapping semiparametric method matches the test error of MCMC slightly
better than the parametric method.

Figure 6. Bayesian hierarchical logistic regression: (a-b) Wall time and test error comparisons for varying data size n. As n
is increased, wall time remains constant for prior swapping but grows for standard inference methods. (c-d) Wall time and test
error comparisons for varying model dimensionality d. (e-g) Wall time and test error comparisons for inferences on a set of
prior hyperparameters γ ∈ [1, 1.05]. Here, a single false posterior ˜pf (θ) (computed at γ = 1.025) is used for prior swapping
on all other hyperparameters.

Log wall time (log(seconds))Test ErrorTest ErrorTotal wall time (seconds)Test ErrorNumber of observationsx104Number of observationsx104Number of dimensionsNumber of dimensions1. MCMC2. VI3. PSVI4. PSSemi5. PSParaLog wall time (log(seconds))Log wall time (log(seconds))13,4,5213,4,5212,3,4,52,31,4,52,31,4,52,31,4,5(a)(b)(c)(d)Hyperparameter Hyperparameter (e)(f)(g)11.011.021.031.041.050.090.10.110.120.130.140.150.160.1711.011.021.031.041.05678910110246810x 105051015202530350.050.10.150.20.250.305101520253035345678910110246810x 1040.090.10.110.120.130.140.150.160246810x 104246810121. Target posterior inf. (MCMC)2. Target posterior inf. (VI)3. Prior swap VI4. Prior swap semiparametric5. Prior swap parametric