EfﬁcientDistributedLearningwithSparsityJialeiWang1MladenKolar1NathanSrebro2TongZhang3AbstractWeproposeanovel,efﬁcientapproachfordis-tributedsparselearningwithobservationsran-domlypartitionedacrossmachines.Ineachroundoftheproposedmethod,workermachinescomputethegradientofthelossonlocaldataandthemastermachinesolvesashifted‘1regular-izedlossminimizationproblem.Afteranumberofcommunicationroundsthatscalesonlylog-arithmicallywiththenumberofmachines,andindependentofotherparametersoftheproblem,theproposedapproachprovablymatchesthees-timationerrorboundofcentralizedmethods.1.IntroductionWeconsiderlearningasparselinearregressorβminimiz-ingthepopulationobjective:β(cid:6)(cid:16)argminβEX,Y(cid:18)Dr‘pY,xX,βyqs,(1)wherepX,YqPX(cid:2)Y(cid:132)Rp(cid:2)Yaredrawnfromanun-knowndistributionDand‘p(cid:4),(cid:4)qisaconvexlossfunction,basedonNi.i.d.samplestxi,yiuNi(cid:16)1drawnfromD,andwhenthesupportS:(cid:16)supportpβ(cid:6)q(cid:16)tjPrps|β(cid:6)j(cid:24)0uofβ(cid:6)issmall,|S|⁄s.Inastandardsingle-machineset-ting,acommonempiricalapproachistominimizethe‘1regularizedempiricalloss(see,e.g.,(2)below).Hereweconsiderasettingwheredataaredistributedacrossmma-chines,and,forsimplicity,assume1thatN(cid:16)nm,sothateachmachinejhasaccesstoni.i.d.observations(fromthesamesourceD)txji,yjiuni(cid:16)1(equivalently,thatN(cid:16)nmsamplesarerandomlypartitionedacrossmachines).Themaincontributionofthepaperisanovelalgorithmforestimatingβ(cid:6)inadistributedsetting.Ourestimatoris1UniversityofChicago,USA2ToyotaTechnologicalInsti-tuteatChicago,USA3TencentAILab,China.Correspondenceto:JialeiWang<jialei@uchicago.edu>,MladenKolar<mko-lar@chicagobooth.edu>,NathanSrebro<nati@ttic.edu>,TongZhang<tongzhang@tongzhang-ml.org>.Proceedingsofthe34thInternationalConferenceonMachineLearning,Sydney,Australia,PMLR70,2017.Copyright2017bytheauthor(s).1Resultsinthepapereasilygeneralizetoasettingwhereeachmachinehasadifferentnumberofobservations.abletoachievetheperformanceofacentralizedprocedurethathasaccesstoalldata,whilekeepingcomputationandcommunicationcostslow.Comparedtotheexistingone-shotestimationapproach(Leeetal.,2015b),ourmethodcanachievethesamestatisticalperformancewithoutper-formingtheexpensivedebiasingstep.Asthenumberofcommunicationroundsincreases,theestimationaccuracyimprovesuntilmatchingtheperformanceofacentralizedprocedure,whichhappensafterthelogarithmofthetotalnumberofmachinesrounds.Furthermore,ourresultscanbeachievedunderweakassumptionsonthedatageneratingprocedure.Weassumethatthecommunicationoccursinrounds.Ineachround,machinesexchangemessageswiththemastermachine.Betweentworounds,eachmachineonlycom-putesbasedonitslocalinformation,whichincludeslocaldataandpreviousmessages(Zhangetal.,2013b;Shamir&Srebro,2014;Arjevani&Shamir,2015).Inanon-distributedsetting,efﬁcientestimationproceduresneedtobalancestatisticalefﬁciencywithcomputationefﬁciency(runtime).Inadistributedsetting,thesituationismorecomplicatedandweneedtobalancetworesources,localruntimeandnumberofroundsofcommunication,withthestatisticalerror.Thelocalruntimereferstotheamountofworkeachmachineneedstodo.Thenumberofroundsofcommunicationreferstohowoftendolocalmachinesneedtoexchangemessageswiththemastermachine.Wecompareourproceduretootheralgorithmusingtheafore-mentionedmetrics.Weconsiderthefollowingtwobaselineestimatorsofβ(cid:6):thelocalestimatorusesdataavailableonlyonthemas-ter(ﬁrst)machineandignoresdataavailableonotherma-chines.Inparticular,itcomputesbβlocal(cid:16)argminβ1nn‚i(cid:16)1‘py1i,xx1i,βyq(cid:0)λ||β||1(2)usinglocallyavailabledata.Thelocalprocedureisefﬁcientinbothcommunicationandcomputation,however,there-sultingestimationerrorislargecomparedtoanestimatorthatusesalloftheavailabledata.Theotheridealizedbase-lineisthecentralizedestimatorbβcentralize(cid:16)argminβ1mnm‚j(cid:16)1n‚i(cid:16)1‘pyji,xxji,βyq(cid:0)λ||β||1.EfﬁcientDistributedLearningwithSparsityApproachn`ms2logpms2logp`n`s2logpCommunicationComputationCommunicationComputationCentralizen(cid:4)pTlassopmn,pqn(cid:4)pTlassopmn,pqAvg-Debiaspp(cid:4)Tlassopn,pq(cid:2)(cid:2)Thispaper(EDSL)p2(cid:4)Tlassopn,pqlogm(cid:4)plogm(cid:4)Tlassopn,pqTable1.Comparisonofresourcesrequiredformatchingthecentralizederrorboundofvariousapproachesforhigh-dimensionaldis-tributedsparselinearregressionproblems,whereTlassopn,pqistheruntimeforsolvingageneralizedlassoproblemofsizen(cid:2)p.Unfortunately,duetodatabeinghugeandcommunicationexpensive,wecannotcomputethecentralizedestimator,eventhoughitachievestheoptimalstatisticalerror.Inarelatedsetting,Leeetal.(2015b)studiedaone-shotap-proachtolearningβ(cid:6),calledAvg-Debias,thatisbasedonaveragingthedebiasedlassoestimators(Zhang&Zhang,2013).Understrongassumptionsonthedatagenerat-ingprocedure,theirapproachmatchesthecentralizederrorboundafteroneroundofcommunication.Whileanen-couragingresult,therearelimitationstothisapproach,thatwelistbelow.•ThedebiasingstepinAvg-Debiasiscomputationallyheavyasitrequireseachlocalmachinetoestimateap(cid:2)pmatrix.Forexample,Javanmard(2014)(section5.1)transformstheproblemofestimatingthedebias-ingmatrixΘintopgeneralizedlassoproblems.Thisiscomputationallyprohibitiveforhigh-dimensionalproblems(Zhang&Zhang,2013;Javanmard&Mon-tanari,2014).Incomparison,ourprocedurerequiresonlysolvingone‘1penalizedobjectiveineachitera-tion,whichhasthesametimecomplexityascomput-ingbβlocalin(2).SeeSection2fordetails.•Avg-Debiasprocedureonlymatchesthestatisticaler-rorrateofthecentralizedprocedurewhenthesam-plesizepermachinesatisﬁesn`ms2logp.Ourapproachimprovesthissamplecomplexityton`s2logp.•Avg-Debiasprocedurerequiresstrongconditionsonthedatageneratingprocess.Forexample,thedatamatrixisrequiredtosatisfythegeneralizedcoherenceconditionfordebiasingtowork2.Asweshowhere,suchaconditionisnotneededforconsistenthigh-dimensionalestimationinadistributedsetting.In-stead,weonlyrequirestandardrestrictedeigenvalueconditionthatarecommonlyassumedinthehigh-dimensionalestimationliterature.Ourmethod(EDSL)addressestheaforementionedissues2ThegeneralizedcoherencestatesthatthereexistsamatrixΘ,suchthat||bΣΘ(cid:1)Ip||8(cid:192)blogpn,wherebΣistheempiricalcovariancematrix.ofAvg-Debias.Table1summarizestheresourcesrequiredfortheapproachesdiscussedabovetosolvethedistributedsparselinearregressionproblems.ParallelWorkInparallelwork(publiclyannouncedonarXivsimultaneouslywiththeresultsinthiscontribution),Jordanetal.(2016)presentamethodwhichisequivalenttotheﬁrstiterationofourmethod,andthusachievesthesamecomputationaladvantageoverAvg-DebiasasdepictedintheleftcolumnofTable1anddiscussedintheﬁrstandthirdbulletpointsabove.Jordanetal.extendtheideainwaysdifferentandorthogonaltothissubmission,byconsideringalsolow-dimensionalandBayesianinferenceproblems.Still,forhigh-dimensionalproblems,theyonlyconsideraone-shotprocedure,andsodonotachievesta-tisticaloptimalityinthewayourmethoddoes,anddonotallowusingn(cid:192)ms2logpsamplespermachine(seerighthalfofTable1).Theimprovedone-shotapproachisthusaparallelcontribution,madeconcurrentlybyJordanetal.andbyus,whilethemulti-stepapproachandaccompaniedreductioninrequirednumberofsamples(discusseinthesecondbulletpointabove)andimprovementinstatisticalaccuracyisadistinctcontributionofthisthissubmission.OtherRelatedWorkAlargebodyofliteratureexistsondistributedoptimizationformodernmassivedatasets(Dekeletal.,2012;Duchietal.,2012;2014;Zhangetal.,2013b;Zinkevichetal.,2010;Boydetal.,2011;Balcanetal.,2012;Yang,2013;Jaggietal.,2014;Maetal.,2015;Shamir&Srebro,2014;Zhang&Xiao,2015;Leeetal.,2015a;Arjevani&Shamir,2015).Apopularapproachtodistributedestimationisaveragingestimatorsformedlo-callybydifferentmachines(Mcdonaldetal.,2009;Zinke-vichetal.,2010;Zhangetal.,2012;Huang&Huo,2015).Divide-and-conquerproceduresalsofoundapplicationsinstatisticalinference(Zhaoetal.,2014a;Cheng&Shang,2015;Luetal.,2016).Shamir&Srebro(2014)andRosen-blatt&Nadler(2014)showedthataveraginglocalestima-torsattheendwillhavebaddependenceoneitherconditionnumberordimensionoftheproblem.Yang(2013),Jaggietal.(2014)andSmithetal.(2016)studieddistributedop-timizationusingstochastic(dual)coordinatedescent,theseapproachestrytoﬁndagoodbalancebetweencomputationandcommunication,however,theircommunicationcom-EfﬁcientDistributedLearningwithSparsityplexitydependsbadlyontheconditionnumber.Asare-sult,theyarenotbetterthanﬁrst-orderapproaches,suchas(proximal)acceleratedgradientdescent(Nesterov,1983),intermsofcommunication.Shamiretal.(2014)andZhang&Xiao(2015)proposedtrulycommunication-efﬁcientdis-tributedoptimizationalgorithms.Theyleveragedthelocalsecond-orderinformationand,asaresult,obtainedmilderdependenceontheconditionnumbercomparedtotheﬁrst-orderapproaches(Boydetal.,2011;Shamir&Srebro,2014;Maetal.,2015).LowerboundswerestudiedinZhangetal.(2013a),Bravermanetal.(2015),andArjevani&Shamir(2015).However,itisnotclearhowtoextendtheseexistingapproachestoproblemswithnon-smoothob-jectives,includingthe‘1regularizedproblems.Mostoftheabovementionedworkisfocusedonestima-torsthatare(asymptotically)linear.Averagingattheendreducesthevarianceofthetheselinearestimators,result-inginanestimatorthatmatchestheperformanceofacen-tralizedprocedure.Zhangetal.(2013c)studiedaverag-inglocalestimatorsobtainedbythepenalizedkernelridgeregression,withthe‘2penaltywaschosensmallerthanusualtoavoidthelargebiasproblem.Thesituationinahigh-dimensionalsettingisnotsostraightforward,sincethesparsityinducingpenaltyintroducesthebiasinanon-linearway.Zhaoetal.(2014b)illustratedhowaveragingdebiasedcompositequantileregressionestimatorscanbeusedforefﬁcientinferenceinahigh-dimensionalsetting.Averagingdebiasedhigh-dimensionalestimatorswassub-sequentlyusedinLeeetal.(2015b)fordistributedestima-tion,multi-tasklearning(Wangetal.,2015),andstatisticalinference(Batteyetal.,2015).Notation.Weusernstodenotethesett1,...,nu.ForavectoraPRn,weletsupportpaq(cid:16)tj:aj(cid:24)0ubethesupportset,||a||q,qPr1,8q,the‘q-normdeﬁnedas||a||q(cid:16)p(cid:176)iPrns|ai|qq1{q,and||a||8(cid:16)maxiPrns|ai|.ForamatrixAPRn1(cid:2)n2,weusethefollowingelement-wise‘8matrixnorms||A||8(cid:16)maxiPrn1s,jPrn2s|aij|.DenoteInasn(cid:2)nidentitymatrix.Fortwosequencesofnumberstanu8n(cid:16)1andtbnu8n(cid:16)1,weusean(cid:16)Opbnqtodenotethatan⁄CbnforsomeﬁnitepositiveconstantC,andforallnlargeenough.Ifan(cid:16)Opbnqandbn(cid:16)Opanq,weusethenotationan(cid:22)bn.Wealsousean(cid:192)bnforan(cid:16)Opbnqandan`bnforbn(cid:16)Opanq.PaperOrganization.WedescribeourmethodinSection2,andpresentthemainresultsinthecontextofsparselin-earregressioninSection3,andprovideageneralizedthe-oryinSection4.WedemonstratetheeffectivenessoftheproposalviaexperimentsinSection5,andconcludethepa-perwithdiscussionsinSection6.InAppendix,inSectionAweillustratesomeconcreteexamplesofthegeneralre-sultsinSection4,andallproofsaredeferredinSectionB.MoreexperimentalresultsarepresentedinSectionC.Algorithm1EfﬁcientDistributedSparseLearning(EDSL).Input:Datatxji,yjiujPrms,iPrns,lossfunction‘p(cid:4),(cid:4)q.Initialization:Themasterobtainsbβ0byminimizing(3),andbroadcastbβ0toeveryworker.fort(cid:16)0,1,...doWorkers:forj(cid:16)2,3,...,mdoifReceivebβtfromthemasterthenCalculategradient∇Ljpbβtqandsendittothemaster.endendMaster:ifReceivet∇Ljpbβtqumj(cid:16)2fromallworkersthenObtainbβt(cid:0)1bysolvingtheshifted‘1regularizedproblemin(4).Broadcastbβt(cid:0)1toeveryworker.endend2.MethodologyInthissection,wedetailourprocedureforestimatingβ(cid:6)inadistributedsetting.Algorithm1providesanoutlineofthestepsexecutedbythemasterandworkernodes.LetLjpβq(cid:16)1nn‚i(cid:16)1‘pyji,xxji,βyq,jPrms,betheempiricallossateachmachine.Ourmethodstartsbysolvingalocal‘1regularizedM-estimationprogram.Atiterationt(cid:16)0,themaster(ﬁrst)machineobtainsbβ0asaminimizerofthefollowingprogramminL1pβq(cid:0)λ0||β||1.(3)Thevectorbβ0isbroadcastedtoallothermachines,whichuseittocomputeagradientofthelocallossatbβ0.Inpar-ticular,eachworkercomputes∇Ljpbβ0qandcommunicatesitbacktothemaster.Thisconstitutesoneroundofcom-munication.Attheiterationt(cid:0)1,themastersolvestheshifted‘1regularizedproblembβt(cid:0)1(cid:16)argminβL1pβq(cid:0)C1mm‚j(cid:16)1∇Ljpbβtq(cid:1)∇L1pbβtq,βG(cid:0)λt(cid:0)1||β||1.(4)Aminimizerbβt(cid:0)1iscommunicatedtoothermachines,whichuseittocomputethelocalgradient∇Ljpbβt(cid:0)1qasbefore.Formulation(4)isinspiredbytheproposalinShamiretal.(2014),wheretheauthorsstudieddistributedoptimizationEfﬁcientDistributedLearningwithSparsityforsmoothandstronglyconvexempiricalobjectives.Com-paredtoShamiretal.(2014),wedonotuseanyaveragingscheme,whichwouldrequireadditionalroundsofcommu-nicationand,moreover,weaddan‘1regularizationtermtoensureconsistentestimationinhigh-dimensions.Differentfromthedistributedﬁrst-orderoptimizationapproaches,thereﬁnedobjective(4)leveragesbothglobalﬁrst-orderin-formationandlocalhigher-orderinformation.Toseethis,supposewesetλt(cid:0)1(cid:16)0andthatLjpβqisaquadraticob-jectivewithinvertibleHessian.Thenwehavethefollowingclosedformsolutionfor(4),bβt(cid:0)1(cid:16)bβt(cid:1)(cid:1)∇2L1pbβtq(cid:9)(cid:1)1(cid:4)(cid:5)m(cid:1)1‚jPrms∇Ljpbβtq(cid:12)(cid:13),whichisexactlyasub-sampledNewtonupdatingrule.Un-fortunatelyforhigh-dimensionalproblems,theHessianisnolongerinvertible,anda‘1regularizationisaddedtomakethesolutionwellbehaved.Theregularizationparam-eterλtwillbechoseninaway,sothatitdecreaseswiththeiterationnumbert.Asaresultwewillbeabletoshowthattheﬁnalestimatorperformsaswellatthecentralizedsolution.Wediscussindetailshowtochooseλtinthefol-lowingsection.3.MainResultWeillustrateourmaintheoreticalresultsinthecontextofsparselinearregressionmodelyji(cid:16)xxji,β(cid:6)y(cid:0)(cid:15)ji,iPrns,jPrms,(5)wherexjiisasubgaussianp-dimensionalvectorofin-putvariablesand(cid:15)jiisi.i.d.meanzerosubgaussiannoise.Thelossfunctionconsideredistheusualthesquaredloss‘py,byq(cid:16)12py(cid:1)byq2.Withthisnotation,thecentralizedapproachleadstothelassoestimator(Tibshirani,1996)bβcentralize(cid:16)argminβ1mm‚j(cid:16)1Ljpβq(cid:0)λ||β||1,wherethelossatworkerjisLjpβq(cid:16)12n‚iPrnspyji(cid:1)xβ,xjiyq2.Beforestatingthemainresult,weprovidethedeﬁnitionofthesubgaussiannorm(Vershynin,2012).Deﬁnition1(Subgaussiannorm).Thesubgaussiannorm||X||ψ2ofasubgaussianp-dimensionalrandomvectorX,isdeﬁnedas||X||ψ2(cid:16)supxPSp(cid:1)1supq¡1q(cid:1)1{2pE|xX,xy|qq1{q,whereSp(cid:1)1isthep-dimensionalunitsphere.Wealsoneedanassumptionontherestrictedstrongcon-vexityconstant(Negahbanetal.,2012).Assumption2.Weassumethatthereexistsaκ¡0,suchthatforany∆PCpS,3q,12n||X1∆||22¥κ||∆||22,whereCpS,3q(cid:16)t∆PRp|||∆Sc||1⁄3||∆S||1uisarestrictedconeinRp,andX1(cid:16)rxT11;xT12;...;xT1nsPRn(cid:2)pisthedatamatrixonthemastermachine.Whenxjiarerandomlydrawnfromasubgaussiandistri-bution,Assumption(2)issatisﬁedwithhighprobabilityaslongasn`slogp(Rudelson&Zhou,2013).Wearenowreadytostatetheestimationerrorboundforbβt(cid:0)1obtainedusingAlgorithm1.Theorem3.Assumethatdataaregeneratedfromasparselinearregressionmodelin(5)with||xji||ψ2⁄σXand||(cid:15)ji||ψ2⁄σ.Letλt(cid:0)1(cid:16)2mn(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)‚jPrms‚iPrnsxji(cid:15)ji(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8(cid:0)2L(cid:2)maxj,i||xji||28(cid:10)(cid:4)||bβt(cid:1)β(cid:6)||1(cid:4)clogp2p{δqn(6)Thenfort¥0wehave,withprobabilityatleast1(cid:1)2δ,||bβt(cid:0)1(cid:1)β(cid:6)||1⁄1(cid:1)at(cid:0)1n1(cid:1)an48sσσXκclogpp{δqmn(cid:0)at(cid:0)1nsσσXκclogpnp{δqn,(7)||bβt(cid:0)1(cid:1)β(cid:6)||2⁄1(cid:1)at(cid:0)1n1(cid:1)an12?sσσXκclogpp{δqmn(cid:0)atnbnsσσXκclogpnp{δqn,(8)wherean(cid:16)96sσσXκclogp2p{δqnandbn(cid:16)24?sσσXκclogpnp{δqn.WecansimplifytheboundobtainedinTheorem3bylook-ingatthescalingwithrespectton,m,s,andp,bytreatingκ,σandσXasconstants.Supposen`s2logpandsetλt(cid:22)clogpmn(cid:0)clogpn(cid:3)sclogpn(cid:11)t.EfﬁcientDistributedLearningwithSparsityThefollowingerrorboundsholdforAlgorithm1:||bβt(cid:1)β(cid:6)||1(cid:192)Psclogpmn(cid:0)(cid:3)sclogpn(cid:11)t(cid:0)1,||bβt(cid:1)β(cid:6)||2(cid:192)Pcslogpmn(cid:0)(cid:3)cslogpn(cid:11)(cid:3)sclogpn(cid:11)t.Wecancomparetheaboveboundstotheperformanceofthelocalandcentralizedlasso(Wainwright,2009;Mein-shausen&Yu,2009;Bickeletal.,2009).Forbβlocal,wehave||bβlocal(cid:1)β(cid:6)||1(cid:192)Psclogpnand||bβlocal(cid:1)β(cid:6)||2(cid:192)Pcslogpn.Forbβcentralize,wehave||bβcentralize(cid:1)β(cid:6)||1(cid:192)Psclogpmnand||bβcentralize(cid:1)β(cid:6)||2(cid:192)Pcslogpmn.Weseethatafteroneroundofcommunication,wehave||bβ1(cid:1)β(cid:6)||1(cid:192)Psclogpmn(cid:0)s2logpnand||bβ1(cid:1)β(cid:6)||2(cid:192)Pcslogpmn(cid:0)s3{2logpn.TheseboundsmatchtheresultsinLeeetal.(2015b)with-outexpensivedebiasingstep.Furthermore,whenm(cid:192)ns2logp,theymatchtheperformanceofthecentralizedlasso.Finally,aslongast`logmandn`s2logp,itiseasytocheckthat(cid:2)sblogpn(cid:10)t(cid:0)1(cid:192)sblogpmn.There-fore,||bβt(cid:0)1(cid:1)β(cid:6)||1(cid:192)Psclogpmnand||bβt(cid:0)1(cid:1)β(cid:6)||2(cid:192)Pcslogpmn,whichmatchesthecentralizedlassoperformancewithoutadditionalerrorterms.Thatis,aslongasn`s2logp,theroundsofcommunicationtomatchescentralizedprocedureonlyincreaselogarithmicallywiththenumberofmachinesandindependentofotherparameters.Differently,fordis-tributedlearningmethodsstudiedintheliteratureformini-mizingsmoothobjectives,theroundsofcommunicationtomatchcentralizedprocedureincreasepolynomiallywithm(seetable1in(Zhang&Xiao,2015)).Thisisbecausehereweexploittheunderlyingrestrictedstrongconvexityfromempiricallossfunctions,whilepriorworkondistributedminimizationofsmoothobjectives(Shamiretal.,2014;Zhang&Xiao,2015)onlyconsiderstrongconvexityex-plicitlyfromregularization.4.GeneralizedTheoryandProofSketchInordertoestablishTheorem3,weproveanerrorboundonbβ(cid:1)β(cid:6)forageneralloss‘p(cid:4),(cid:4)qandbβobtainedusingAlgorithm1.Tosimplifythepresentation,weassumethatthedomainXisboundedandthatthelossfunction‘p(cid:4),(cid:4)qissmooth.Assumption4.Theloss‘p(cid:4),(cid:4)qisL-smoothwithrespecttothesecondargument:‘1pa,bq(cid:1)‘1pa,cq⁄L|b(cid:1)c|,@a,b,cPRFurthermore,|‘3pa,bq|⁄Mforalla,bPR.Commonlyusedlossfunctionsinstatisticallearning,in-cludingthesquaredlossforregressionandlogisticlossforclassiﬁcation,satisfythisassumption(Zhangetal.,2013b).Next,westatetherestrictedstrongconvexityconditionforagenerallossfunction(Negahbanetal.,2012).Assumption5.Thereexistsκ¡0suchthatforany∆PCpS,3qL1pβ(cid:6)(cid:0)∆q(cid:1)L1pβ(cid:6)q(cid:1)x∇L1pβ(cid:6)q,∆y¥κ||∆||22,withCpS,3q(cid:16)t∆PRp|||∆Sc||1⁄3||∆S||1u.Therestrictedstrongconvexityholdswithhighprobabil-ityforawiderangeofmodelsanddesignsanditiscom-monlyassumedforshowingconsistentestimationinhigh-dimensions(see,forexample,vandeGeer&B¨uhlmann,2009;Negahbanetal.,2012;Raskuttietal.,2010;Rudel-son&Zhou,2013,fordetails).Ourmaintheoreticalresultestablishesarecursiveesti-mationerrorbound,whichrelatestheestimationerror||bβt(cid:0)1(cid:1)β(cid:6)||tothatofthepreviousiteration||bβt(cid:1)β(cid:6)||1.Theorem6.SupposeAssumption4and5holds.Letλt(cid:0)1(cid:16)2(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1m‚jPrms∇Ljpβ(cid:6)q(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8(cid:0)2L(cid:2)maxj,i||xji||28(cid:10)||β(cid:6)(cid:1)bβt||1clogp2p{δqn(cid:0)2M(cid:2)maxj,i||xji||38(cid:10)(cid:1)||bβt(cid:1)β(cid:6)||21(cid:9).(9)EfﬁcientDistributedLearningwithSparsityThenwithprobabilityatleast1(cid:1)δ,wehave||bβt(cid:0)1(cid:1)β(cid:6)||1⁄48sκ(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1m‚jPrms∇Ljpβ(cid:6)q(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8(cid:0)48sLκ(cid:2)maxj,i||xji||28(cid:10)||β(cid:6)(cid:1)bβt||1clogp2p{δqn(cid:0)48sMκ(cid:2)maxj,i||xji||38(cid:10)(cid:1)||bβt(cid:1)β(cid:6)||21(cid:9),and||bβt(cid:0)1(cid:1)β(cid:6)||2⁄12?sκ(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1m‚jPrms∇Ljpβ(cid:6)q(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8(cid:0)12?sLκ(cid:2)maxj,i||xji||28(cid:10)||β(cid:6)(cid:1)bβt||1clogp2p{δqn(cid:0)4?sMκ(cid:2)maxj,i||xji||38(cid:10)(cid:1)||bβt(cid:1)β(cid:6)||21(cid:9).Theorem6upperboundstheestimationerror||bβt(cid:0)1(cid:1)β(cid:6)||1asafunctionof||bβt(cid:1)β(cid:6)||1.ApplyingTheorem6iteratively,weimmediatelyobtainthefollowingestimationerrorboundwhichdependsonthequalityoflocal‘1regu-larizedestimation||bβ0(cid:1)β(cid:6)||1.Corollary7.SupposetheconditionsofTheorem6aresat-isﬁed.Furthermore,supposethatforallt,wehaveM(cid:2)maxj,i||xji||8(cid:10)||bβt(cid:1)β(cid:6)||1⁄Lclogp2p{δqn.(10)Thenwithprobabilityatleast1(cid:1)δ,wehave||bβt(cid:0)1(cid:1)β(cid:6)||1⁄at(cid:0)1n||bβ0(cid:1)β(cid:6)||1(cid:0)p1(cid:1)anq(cid:1)1p1(cid:1)at(cid:0)1nq(cid:4)48sκ(cid:4)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1m‚jPrms∇Ljpβ(cid:6)q(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8and||bβt(cid:0)1(cid:1)β(cid:6)||2⁄atnbn(cid:4)||bβ0(cid:1)β(cid:6)||1(cid:0)p1(cid:1)anq(cid:1)1p1(cid:1)at(cid:0)1nq(cid:4)12?sκ(cid:4)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1m‚jPrms∇Ljpβ(cid:6)q(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8,wherean(cid:16)96sLκ(cid:2)maxj,i||xji||28(cid:10)clogp2p{δqnandbn(cid:16)24?sLκ(cid:2)maxj,i||xji||28(cid:10)clogp2p{δqn.ForthequadraticlosswehavethatM(cid:16)0andthecondi-tionin(10)holds.Forothertypesoflosses,conditionin(10)willbetruefortlargeenoughwhenm`s2,lead-ingtolocalexponentialrateofconvergenceuntilreachingstatisticaloptimalregion.4.1.ProofSketchofTheorem6Weﬁrstanalyzehowtheestimationerrorbounddecreasesafteroneroundofcommunication.Inparticular,webound||bβt(cid:0)1(cid:1)β(cid:6)||with||bβt(cid:1)β(cid:6)||.DeﬁneeL1pβ,bβtq(cid:16)L1pβq(cid:0)C1m‚jPrms∇Ljpbβtq(cid:1)∇L1pbβtq,βG.(11)Then∇eL1pβ,bβtq(cid:16)∇L1pβq(cid:0)1m‚jPrms∇Ljpbβtq(cid:1)∇L1pbβtq.Thefollowinglemmaboundsthe‘8normof∇eL1pβ,bβtq.Lemma8.Withprobabilityatleast1(cid:1)δ,wehave(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)∇eL1pβ(cid:6),bβtq(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8⁄(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1m‚jPrms∇Ljpβ(cid:6)q(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8(cid:0)2L(cid:2)maxj,i||xji||28(cid:10)||β(cid:6)(cid:1)bβt||1clogp2p{δqn(cid:0)M(cid:2)maxj,i||xji||38(cid:10)(cid:1)||bβt(cid:1)β(cid:6)||21(cid:9).Thelemmaboundsthemagnitudeofthegradientofthelossatoptimumpointβ(cid:6).Thiswillbeusedtoguideourchoiceofthe‘1regularizationparameterλt(cid:0)1in(4).Thefollow-inglemmashowsthataslongasλt(cid:0)1islargeenough,itisguaranteedthatbβt(cid:0)1(cid:1)β(cid:6)isinarestrictedcone.Lemma9.Supposeλt(cid:0)1{2¥(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)∇eL1pβ(cid:6),bβtq(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)8.Thenwithprobabilityatleast1(cid:1)δ,wehavebβt(cid:0)1(cid:1)β(cid:6)PCpS,3q.Basedontheconicconditionandrestrictedstrongconvex-itycondition,wecanobtaintherecursiveerrorboundstatedinTheorem6followingtheproofstrategyasinNegahbanetal.(2012).ApplicationsTheorem6canbeusedtoestablishstatisti-calguaranteesformoregeneralsparselearningproblems,forexampleconsiderthelogisticregressionisapopularclassiﬁcationmodelwherethebinarylabelyjiPt(cid:1)1,1uisdrawnaccordingtoaBernoullidistribution:Ppyji(cid:16)(cid:8)1|xjiq(cid:16)exppyjixxji,β(cid:6)yqexppyjixxji,β(cid:6)yq(cid:0)1,(12)wecanestablishlocalexponentialconvergencewhenap-plyingAlgorithm1toestimateβ(cid:6)inthehigh-dimensionallogisticmodel.SectionAinAppendixprovideformalguaranteesandmoreillustrativeexamples.EfﬁcientDistributedLearningwithSparsitym(cid:16)5m(cid:16)10m(cid:16)200123456789RoundsofCommunications0.150.230.310.390.470.55EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.10.190.280.370.460.55EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.070.170.270.370.470.57EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSLn(cid:16)500,p(cid:16)3000,s(cid:16)10,X(cid:18)Np0,Σq,Σij(cid:16)0.5|i(cid:1)j|{5.0123456789RoundsofCommunications0.60.740.881.021.161.3EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.50.640.780.921.061.2EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.30.580.861.141.421.7EstimationErrorLocalProx-GDCentralizeAvg-DebiasEDSLn(cid:16)1000,p(cid:16)3000,s(cid:16)10,X(cid:18)Np0,Σq,Σij(cid:16)0.5|i(cid:1)j|{5.Figure1.Comparisonofvariousalgorithmsfordistributedsparselearningonsimulateddata,ﬁrstrow:sparselinearregression,secondrow:sparselogisticregression.5.ExperimentsInthissectionwepresentempiricalcomparisonsbetweenvariousapproachesonbothsimulatedandrealworlddatasets3.Werunthealgorithmsforbothdistributedre-gressionandclassiﬁcationproblems,andcomparewiththefollowingalgorithms:i)Local;ii)Centralize;iii)Dis-tributedproximalgradientdescent(ProxGD);iv)Avg-Debias(Leeetal.,2015b)withhardthresholding,andv)theproposedEDSLapproach.5.1.SimulationsWeﬁrstexaminethealgorithmsonsimulateddata.WegeneratetxjiujPrms,iPrnsfromamultivariatenormaldis-tributionwithmeanzeroandcovariancematrixΣ.ThecovarianceΣcontrolstheconditionnumberoftheprob-lemandwewillvaryingittoseehowtheperformancechanges.WesetΣij(cid:16)0.5|i(cid:1)j|forthewell-conditionedsettingandΣij(cid:16)0.5|i(cid:1)j|{5fortheill-conditionedsetting.TheresponsevariabletyjiujPrms,iPrnsaredrawnfrom(5)and(12)forregressionandclassiﬁcationproblems,respec-tively.Forregression,thenoise(cid:15)jiissampledfromastan-dardnormaldistribution.Thetruemodelβ(cid:6)issettobes-sparse,wheretheﬁrsts-entriesaresampledi.i.d.fromauniformdistributioninr0,1s,andtheotherentriesareset3PleaserefertoSectionCinAppendixforfullexperimentalresultsandmoredetailstozero.Werunexperimentswithvariouspn,p,m,sqsettings4.Theestimationerror||bβt(cid:1)β(cid:6)||2isshownversusroundsofcommunicationsforforProxGDandtheproposedEDSLalgorithm.WealsoplottheestimationerrorofLocal,Avg-Debias,andCentralizeashorizontallines,sincethecom-municationcostisﬁxedforforthesealgorithms56.Figure1summarizetheresults,averagedacross10independenttrials.Wehavethefollowingobservations:•TheAvg-Debiasapproachobtainedmuchbetteres-timationerrorcomparedtoLocalafteroneroundofcommunicationandsometimesperformedquiteclosetoCentralize.However,inmostcases,thereisstillagapcomparedwithCentralize,especiallywhentheproblemisnotwell-conditionedormislarge.•ProxGDconvergesveryslowwhentheconditionnumberbecomesbad(Σij(cid:16)0.5|i(cid:1)j|{5case).•Astheorysuggests,EDSLobtainedasolutionthatis4n:samplesizepermachine,p:problemdimension,m:num-berofmachines,s:truesupportsize.5thesealgorithmshavezero,one-shotandfullcommunica-tions,respectively.6Togivesomesensesaboutcomputationalcost,foraproblemwithn(cid:16)200,p(cid:16)1000,ateachroundEDSLtakesabout0.048s,whileAvg-Debiastakesabout40.334s.EfﬁcientDistributedLearningwithSparsity0123456789RoundsofCommunications1.11.281.461.641.822.0ClassiﬁcationError(%)LocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.6770.67950.6820.68450.6870.68950.692NormalizedMSELocalProx-GDCentralizeAvg-DebiasEDSL0123456789RoundsofCommunications0.40.440.480.520.560.6NormalizedMSELocalProx-GDCentralizeAvg-DebiasEDSLmnist1vs2connect4dnaFigure2.Comparisonofvariousapproachesfordistributedsparseregressionandclassiﬁcationonrealworlddatasets.competitivewithAvg-Debiasafteroneroundofcom-munication.TheestimationerrordecreasestomatchperformanceofCentralizewithinfewroundsofcom-munications;typicallylessthan5,eventhoughthetheorysuggestsEDSLwillmatchtheperformanceofcentralizewithinOplogmqroundsofcommunication.Aboveexperimentsillustrateourtheoreticalresultsinﬁnitesamples.Assuggestedbytheory,whensamplesizepermachinenisrelativelysmall,oneroundofcommunica-tionisnotsufﬁcienttomakeAvg-Debiasmatchestheper-formanceofcentralizedprocedure.However,EDSLcouldmatchtheperformanceofAvg-Debiaswithoneroundofcommunicationandfurtherimprovetheestimationqual-itybyexponentiallyreducingthegapbetweencentralizedprocedurewithAvg-Debias,untilmatchingthecentralizedperformance.Thus,theproposedEDSLimprovestheAvg-Debiasapproachbothcomputationallyandstatistically.5.2.Real-worldDataEvaluationInthissection,wecomparethedistributedsparselearningalgorithmsonseveralrealworlddatasets.Foralldatasets,weuse60%ofdatafortraining,20%asheld-outvalida-tionsetfortuningtheparameters,andtheremaining20%fortesting.Werandomlypartitiondata10timesandre-porttheaverageperformanceonthetestset.Forregres-siontasks,theevaluationmetricisthenormalizedMeanSquaredError(normalizedMSE),whileforclassiﬁcationtaskswereportthemiss-classiﬁcationerror.Werandomlypartitionthedataonm(cid:16)10machines.AsubsetoftheresultsareplottedinFigure2whereforsomedatasetstheperformanceofAvg-Debiasissigniﬁcantlyworsethanoth-ers(mostlybecausethedebiasingstepfails),thusweomittheseplots.Sincethereisnowell-speciﬁedmodelonthesedatasets,thecurvesbehavequitedifferentlyondifferentdatasets.How-ever,alargegapbetweenthelocalandcentralizedproce-dureisconsistentasthelateruses10timesmoredata.Avg-Debiasoftenfailsontheserealdatasetsandperformsmuchworsethaninthesimulations,themainreasonmightbethattheassumptions,suchaswell-speciﬁedmodelorgeneral-izedcoherencecondition,fail,thenAvg-Debiascantotallyfailandproducesolutionevenmuchworsethanthelocal.Nevertheless,theproposedEDSLperformsquiterobustonrealworlddatasets,andcanoftenoutputasolutionwhichishighlycompetitivewiththecentralizedmodelwithinafewroundsofcommunications.Wealsoobservedaslight“zig-zag”behaviorforEDSLapproachonsomedatasets.Forexample,onthemushroomsdataset,thepredictiveper-formanceofEDSLisnotstable.Insum,theexperimentalresultsonrealworlddatasetsveriﬁedthattheproposedEDSLmethodiseffectivefordistributedsparselearningproblems.6.ConclusionandDiscussionWeproposedanovelapproachfordistributedlearningwithsparsity,whichisefﬁcientinbothcomputationandcom-munication.Ourtheoreticalanalysisshowedthatthepro-posedmethodworksunderweakerconditionsthanAvg-Debiasestimatorwhilematchesitserrorboundwithone-roundcommunication.Furthermore,theestimationerrorcanbeimprovedwithalogarithmicmoreroundsofcom-municationuntilmatchingthecentralizedprocedure.Ex-perimentsonbothsimulatedandreal-worlddatademon-stratethattheproposedmethodsigniﬁcantlyimprovestheperformanceoveroneshotaveragingapproaches,andmatchesthecentralizedprocedurewithfewiterations.Theremightbeseveralwaystoimprovethiswork.Asweseeinrealdataexperiments,theproposedapproachcanstillperformslightlyworsethanthecentralizedapproachoncertaindatasets.ItisinterestingtoexplorehowtomakeEDSLprovablyworkunderevenweakerassumptions.Forexample,EDSLrequiresOps2logpqsamplespermachinetomatchthecentralizedmethodinOplogmqroundsofcommunications,however,itisnotclearwhetherthesam-plesizerequirementcanbeimproved,whilestillmaintain-inglow-communicationcost.Lastbutnottheleast,itisin-terestingtoexplorepresentedideastoimprovethecompu-tationalcostofcommunication-efﬁcientdistributedmulti-tasklearningwithsharedsupport(Wangetal.,2015).EfﬁcientDistributedLearningwithSparsityReferencesArjevani,YossiandShamir,Ohad.Communicationcom-plexityofdistributedconvexlearningandoptimization.ArXive-prints,arXiv:1506.01900,June2015.Balcan,Maria-Florina,Blum,Avrim,Fine,Shai,andMan-sour,Yishay.Distributedlearning,communicationcom-plexityandprivacy.InMannor,Shie,Srebro,Nathan,andWilliamson,RobertC.(eds.),JMLRW&CP23:COLT2012,volume23,pp.26.1–26.22,2012.Battey,Heather,Fan,Jianqing,Liu,Han,Lu,Junwei,andZhu,Ziwei.Distributedestimationandinferencewithstatisticalguarantees.ArXive-prints,arXiv:1509.05457,September2015.Bickel,PeterJ.,Ritov,Ya’acov,andTsybakov,Alexan-dreB.SimultaneousanalysisoflassoandDantzigse-lector.Ann.Stat.,37(4):1705–1732,2009.doi:10.1214/08-AOS620.Boyd,StephenP.,Parikh,Neal,Chu,Eric,Peleato,Borja,andEckstein,Jonathan.Distributedoptimizationandstatisticallearningviathealternatingdirectionmethodofmultipliers.Found.TrendsMach.Learn.,3(1):1–122,2011.Braverman,Mark,Garg,Ankit,Ma,Tengyu,Nguyen,HuyL.,andWoodruff,DavidP.Communicationlowerboundsforstatisticalestimationproblemsviaadis-tributeddataprocessinginequality.ArXive-prints,arXiv:1506.07216,June2015.Cheng,GuangandShang,Zuofeng.Computationallim-itsofdivide-and-conquermethod.ArXive-prints,arXiv:1512.09226,December2015.Dekel,Ofer,Gilad-Bachrach,Ran,Shamir,Ohad,andXiao,Lin.Optimaldistributedonlinepredictionusingmini-batches.J.Mach.Learn.Res.,13:165–202,2012.ISSN1532-4435.Duchi,JohnC.,Agarwal,Alekh,andWainwright,Mar-tinJ.Dualaveragingfordistributedoptimization:con-vergenceanalysisandnetworkscaling.IEEETrans.Au-tomat.Control,57(3):592–606,2012.ISSN0018-9286.doi:10.1109/TAC.2011.2161027.Duchi,JohnC.,Jordan,MichaelI.,Wainwright,Mar-tinJ.,andZhang,Yuchen.Optimalityguaranteesfordistributedstatisticalestimation.ArXive-prints,arXiv:1405.0782,May2014.Hoeffding,Wassily.Probabilityinequalitiesforsumsofboundedrandomvariables.J.Am.Stat.Assoc.,58:13–30,1963.ISSN0162-1459.Huang,ChengandHuo,Xiaoming.Adistributedone-stepestimator.ArXive-prints,arXiv:1511.01443,November2015.Jaggi,Martin,Smith,Virginia,Tak´ac,Martin,Terhorst,Jonathan,Krishnan,Sanjay,Hofmann,Thomas,andJor-dan,MichaelI.Communication-efﬁcientdistributeddualcoordinateascent.InAdvancesinNeuralInforma-tionProcessingSystems,pp.3068–3076,2014.Javanmard,Adel.Inferenceandestimationinhigh-dimensionaldataanalysis.PhDdissertation,StanfordUniversity,2014.Javanmard,AdelandMontanari,Andrea.Conﬁdenceinter-valsandhypothesistestingforhigh-dimensionalregres-sion.J.Mach.Learn.Res.,15(Oct):2869–2909,2014.Jordan,MichaelI,Lee,JasonD,andYang,Yun.Communication-efﬁcientdistributedstatisticallearning.arXivpreprintarXiv:1605.07689,2016.Lee,JasonD.,Lin,Qihang,Ma,Tengyu,andYang,Tian-bao.Distributedstochasticvariancereducedgradientmethodsandalowerboundforcommunicationcom-plexity.ArXive-prints,arXiv:1507.07595,July2015a.Lee,JasonD.,Sun,Yuekai,Liu,Qiang,andTaylor,JonathanE.Communication-efﬁcientsparseregression:aone-shotapproach.ArXive-prints,arXiv:1503.04337,2015b.Lu,Junwei,Cheng,Guang,andLiu,Han.Nonparametricheterogeneitytestingformassivedata.ArXive-prints,arXiv:1601.06212,January2016.Ma,Chenxin,Smith,Virginia,Jaggi,Martin,Jordan,MichaelI.,Richtrik,Peter,andTak,Martin.Addingvs.averagingindistributedprimal-dualoptimization.ArXive-prints,arXiv:1502.03508,February2015.McCullagh,P.andNelder,J.A.Generalizedlinearmod-els.MonographsonStatisticsandAppliedProbability.Chapman&Hall,London,1989.ISBN0-412-31760-5.doi:10.1007/978-1-4899-3242-6.Secondedition[ofMR0727836].Mcdonald,Ryan,Mohri,Mehryar,Silberman,Nathan,Walker,Dan,andMann,GideonS.Efﬁcientlarge-scaledistributedtrainingofconditionalmaximumen-tropymodels.InBengio,Y.,Schuurmans,D.,Lafferty,J.D.,Williams,C.K.I.,andCulotta,A.(eds.),AdvancesinNeuralInformationProcessingSystems22,pp.1231–1239.CurranAssociates,Inc.,2009.Meinshausen,NicolasandB¨uhlmann,Peter.Highdimen-sionalgraphsandvariableselectionwiththelasso.Ann.Stat.,34(3):1436–1462,2006.Meinshausen,NicolasandYu,B.Lasso-typerecoveryofsparserepresentationsforhigh-dimensionaldata.Ann.Stat.,37(1):246–270,2009.Negahban,SahandN,Ravikumar,Pradeep,Wainwright,MartinJ.,andYu,Bin.Auniﬁedframeworkforhigh-dimensionalanalysisofm-estimatorswithdecompos-ableregularizers.Stat.Sci.,27(4):538–557,2012.Nesterov,Yurii.Amethodofsolvingaconvexprogram-EfﬁcientDistributedLearningwithSparsitymingproblemwithconvergencerateop1{k2q.InSovietMathematicsDoklady,volume27,pp.372–376,1983.Raskutti,Garvesh,Wainwright,MartinJ,andYu,Bin.Re-strictedeigenvaluepropertiesforcorrelatedgaussiande-signs.TheJournalofMachineLearningResearch,11:2241–2259,2010.Ravikumar,Pradeep,Wainwright,MartinJ.,andLafferty,J.D.High-dimensionalisingmodelselectionusing‘1-regularizedlogisticregression.Ann.Stat.,38(3):1287–1319,2010.Rosenblatt,JonathanandNadler,Boaz.Ontheoptimalityofaveragingindistributedstatisticallearning.ArXive-prints,arXiv:1407.2724,July2014.Rudelson,MarkandZhou,Shuheng.Reconstructionfromanisotropicrandommeasurements.InformationTheory,IEEETransactionson,59(6):3434–3447,2013.Shamir,OhadandSrebro,Nathan.Distributedstochas-ticoptimizationandlearning.In52ndAnnualAllertonConferenceonCommunication,Control,andComputing(Allerton),2014,pp.850–857.IEEE,2014.Shamir,Ohad,Srebro,Nathan,andZhang,Tong.Commu-nicationefﬁcientdistributedoptimizationusinganap-proximatenewton-typemethod.InProceedingsofThe31stInternationalConferenceonMachineLearning,pp.1000–1008,2014.Smith,Virginia,Forte,Simone,Ma,Chenxin,Takac,Mar-tin,Jordan,MichaelI,andJaggi,Martin.Cocoa:Agen-eralframeworkforcommunication-efﬁcientdistributedoptimization.arXivpreprintarXiv:1611.02189,2016.Tibshirani,RobertJ.Regressionshrinkageandselectionviathelasso.J.R.Stat.Soc.B,58(1):267–288,1996.ISSN0035-9246.vandeGeer,SaraA.High-dimensionalgeneralizedlinearmodelsandthelasso.Ann.Stat.,36(2):614–645,2008.vandeGeer,SaraA.andB¨uhlmann,Peter.Onthecondi-tionsusedtoproveoracleresultsforthelasso.Electron.J.Stat.,3:1360–1392,2009.Vershynin,Roman.Introductiontothenon-asymptoticanalysisofrandommatrices.InEldar,Y.C.andKu-tyniok,G.(eds.),CompressedSensing:TheoryandAp-plications.CambridgeUniversityPress,2012.Wainwright,MartinJ.Sharpthresholdsforhigh-dimensionalandnoisysparsityrecoveryusing‘1-constrainedquadraticprogramming(lasso).IEEETrans.Inf.Theory,55(5):2183–2202,2009.ISSN0018-9448.doi:10.1109/TIT.2009.2016018.Wang,Jialei,Kolar,Mladen,andSrebro,Nathan.Distributedmultitasklearning.ArXive-prints,arXiv:1510.00633,October2015.Wu,TongTong,Chen,YiFang,Hastie,TrevorJ.,So-bel,Eric,andLange,KennethL.Genome-wideasso-ciationanalysisbylassopenalizedlogisticregression.Bioinformatics,25(6):714–721,2009.doi:10.1093/bioinformatics/btp041.Yang,Tianbao.Tradingcomputationforcommunication:Distributedstochasticdualcoordinateascent.InBurges,C.J.C.,Bottou,L.,Welling,M.,Ghahramani,Z.,andWeinberger,K.Q.(eds.),AdvancesinNeuralInforma-tionProcessingSystems26,pp.629–637.CurranAsso-ciates,Inc.,2013.Yuan,M.andLin,Y.Modelselectionandestimationinthegaussiangraphicalmodel.Biometrika,94(1):19–35,2007.Zhang,Cun-HuiandZhang,StephanieS.Conﬁdencein-tervalsforlowdimensionalparametersinhighdimen-sionallinearmodels.J.R.Stat.Soc.B,76(1):217–242,Jul2013.Zhang,YuchenandXiao,Lin.Communication-efﬁcientdistributedoptimizationofself-concordantempiricalloss.ArXive-prints,arXiv:1501.00263,2015.Zhang,Yuchen,Wainwright,MartinJ.,andDuchi,JohnC.Communication-efﬁcientalgorithmsforstatisticalopti-mization.InAdvancesinNeuralInformationProcessingSystems,pp.1502–1510,2012.Zhang,Yuchen,Duchi,JohnC.,Jordan,MichaelI.,andWainwright,MartinJ.Information-theoreticlowerboundsfordistributedstatisticalestimationwithcom-municationconstraints.InAdvancesinNeuralInforma-tionProcessingSystems,pp.2328–2336,2013a.Zhang,Yuchen,Duchi,JohnC.,andWainwright,MartinJ.Communication-efﬁcientalgorithmsforstatisticalopti-mization.J.Mach.Learn.Res.,14:3321–3363,2013b.ISSN1532-4435.Zhang,Yuchen,Duchi,JohnC,andWainwright,Mar-tinJ.Divideandconquerkernelridgeregression:Adistributedalgorithmwithminimaxoptimalrates.arXivpreprintarXiv:1305.5029,2013c.Zhao,Tianqi,Cheng,Guang,andLiu,Han.Apartiallylinearframeworkformassiveheterogeneousdata.ArXive-prints,arXiv:1410.8570,October2014a.Zhao,Tianqi,Kolar,Mladen,andLiu,Han.Ageneralframeworkforrobusttestingandconﬁdenceregionsinhigh-dimensionalquantileregression.ArXive-prints,arXiv:1412.8724,December2014b.Zhu,JiandHastie,TrevorJ.Classiﬁcationofgenemi-croarraysbypenalizedlogisticregression.Biostatistics,5(3):427–443,2004.doi:10.1093/biostatistics/kxg046.Zinkevich,Martin,Weimer,Markus,Smola,AlexanderJ.,andLi,Lihong.Parallelizedstochasticgradientde-scent.InAdvancesinNeuralInformationProcessing,pp.2595–2603.CurranAssociates,Inc.,2010.