Learning Determinantal Point Processes with Moments and Cycles

John Urschel 1 Victor-Emmanuel Brunel 1 Ankur Moitra 1 Philippe Rigollet 1

Abstract

Determinantal Point Processes (DPPs) are a fam-
ily of probabilistic models that have a repulsive
behavior, and lend themselves naturally to many
tasks in machine learning where returning a di-
verse set of objects is important. While there are
fast algorithms for sampling, marginalization and
conditioning, much less is known about learn-
ing the parameters of a DPP. Our contribution
is twofold:
(i) we establish the optimal sam-
ple complexity achievable in this problem and
show that it is governed by a natural parameter,
which we call the cycle sparsity; (ii) we propose
a provably fast combinatorial algorithm that im-
plements the method of moments efﬁciently and
achieves optimal sample complexity. Finally, we
give experimental results that conﬁrm our theo-
retical ﬁndings.

1. Introduction

Determinantal Point Processes (DPPs) are a family of prob-
abilistic models that arose from the study of quantum me-
chanics (Macchi, 1975) and random matrix theory (Dyson,
1962). Following the seminal work of Kulesza and Taskar
(Kulesza & Taskar, 2012), discrete DPPs have found nu-
merous applications in machine learning, including in doc-
ument and timeline summarization (Lin & Bilmes, 2012;
Yao et al., 2016), image search (Kulesza & Taskar, 2011;
Affandi et al., 2014) and segmentation (Lee et al., 2016),
audio signal processing (Xu & Ou, 2016), bioinformat-
ics (Batmanghelich et al., 2014) and neuroscience (Snoek
et al., 2013). What makes such models appealing is that
they exhibit repulsive behavior and lend themselves nat-
urally to tasks where returning a diverse set of objects is
important.

One way to deﬁne a DPP is through an N × N symmet-
ric positive semideﬁnite matrix K, called a kernel, whose

1Department of Mathematics, MIT, USA. Correspondence to:

John Urschel <urschel@mit.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

eigenvalues are bounded in the range [0, 1]. Then the DPP
associated with K, which we denote by DPP(K), is the
distribution on Y ⊆ [N ] = {1, . . . , N } that satisﬁes, for
any J ⊆ [N ],

P[J ⊆ Y ] = det(KJ ),

where KJ is the principal submatrix of K indexed by the
set J. The graph induced by K is the graph G = ([N ], E)
on the vertex set [N ] that connects i, j ∈ [N ] if and only if
Ki,j (cid:54)= 0.

There are fast algorithms for sampling (or approximately
sampling) from DPP(K) (Deshpande & Rademacher,
2010; Rebeschini & Karbasi, 2015; Li et al., 2016b;a).
Marginalizing the distribution on a subset I ⊆ [N ] and con-
ditioning on the event that J ⊆ Y both result in new DPPs
and closed form expressions for their kernels are known
(Borodin & Rains, 2005).

There has been much less work on the problem of learning
the parameters of a DPP. A variety of heuristics have been
proposed, including Expectation-Maximization (Gillenwa-
ter et al., 2014), MCMC (Affandi et al., 2014), and ﬁxed
point algorithms (Mariet & Sra, 2015). All of these attempt
to solve a nonconvex optimization problem, and no guaran-
tees on their statistical performance are known. Recently,
Brunel et al. (Brunel et al., 2017) studied the rate of esti-
mation achieved by the maximum likelihood estimator, but
the question of efﬁcient computation remains open.

Apart from positive results on sampling, marginalization
and conditioning, most provable results about DPPs are ac-
tually negative. It is conjectured that the maximum like-
lihood estimator is NP-hard to compute (Kulesza, 2012).
Actually, approximating the mode of size k of a DPP to
within a ck factor is known to be NP-hard for some c > 1
(C¸ ivril & Magdon-Ismail, 2009; Summa et al., 2015). The
best known algorithms currently obtain a ek +o(k) approx-
imation factor (Nikolov, 2015; Nikolov & Singh, 2016).

In this work, we bypass the difﬁculties associated with
maximum likelihood estimation by using the method of mo-
ments to achieve optimal sample complexity. We introduce
a parameter (cid:96), which we call the cycle sparsity of the graph
induced by the kernel K, which governs the number of mo-
ments that need to be considered and, thus, the sample com-
plexity. Moreover, we use a reﬁned version of Horton’s al-

Moments, Cycles and Learning DPPs

gorithm (Horton, 1987; Amaldi et al., 2010) to implement
the method of moments in polynomial time.

The cycle sparsity of a graph is the smallest integer (cid:96) so
that the cycles of length at most (cid:96) yield a basis for the cy-
cle space of the graph. Even though there are in general
exponentially many cycles in a graph to consider, Horton’s
algorithm constructs a minimum weight cycle basis and,
in doing so, also reveals the parameter (cid:96) together with a
collection of at most (cid:96) induced cycles spanning the cycle
space.

We use such cycles in order to construct our method of mo-
ments estimator. For any ﬁxed (cid:96) ≥ 2, our overall algorithm
has sample complexity

For any S ⊂ [N ], we write ∆S = det(KS), where KS
denotes the |S| × |S| submatrix of K obtained by keeping
rows and colums with indices in S. Note that for 1 ≤ i (cid:54)=
j ≤ N , we have the following relations:

Ki,i = P[i ∈ Y ],

∆{i,j} = P[{i, j} ⊆ Y ],

(cid:113)

Ki,iKj,j − ∆{i,j}. Therefore, the princi-
and |Ki,j| =
pal minors of size one and two of K determine K up to the
sign of its off-diagonal entries. In fact, for any K, there
exists an (cid:96) depending only on the graph GK induced by K,
such that K can be recovered up to a DN -similarity with
only the knowledge of its principal minors of size at most
(cid:96). We will show that this (cid:96) is exactly the cycle sparsity.

n = O

(cid:16)(cid:0) C
α

(cid:1)2(cid:96)

+

(cid:17)

log N
α2ε2

2.2. DPPs and graphs

for some constant C > 1 and runs in time polynomial in
n and N , and learns the parameters up to an additive ε
with high probability. The (C/α)2(cid:96) term corresponds to
the number of samples needed to recover the signs of the
entries in K. We complement this result with a minimax
lower bound (Theorem 2) to show that this sample com-
plexity is in fact near optimal. In particular, we show that
there is an inﬁnite family of graphs with cycle sparsity (cid:96)
(namely length (cid:96) cycles) on which any algorithm requires
at least (C (cid:48)α)−2(cid:96) samples to recover the signs of the entries
of K for some constant C (cid:48) > 1. Finally, we show experi-
mental results that conﬁrm many quantitative aspects of our
theoretical predictions. Together, our upper bounds, lower
bounds, and experiments present a nuanced understanding
of which DPPs can be learned provably and efﬁciently.

2. Estimation of the Kernel

2.1. Model and deﬁnitions

Let Y1, . . . , Yn be n independent copies of Y ∼ DPP(K),
for some unknown kernel K such that 0 (cid:22) K (cid:22) IN .
It is well known that K is identiﬁed by DPP(K) only
up to ﬂips of the signs of its rows and columns: If K (cid:48)
is another symmetric matrix with 0 (cid:22) K (cid:48) (cid:22) IN , then
DPP(K (cid:48))=DPP(K) if and only if K (cid:48) = DKD for some
D ∈ DN , where DN denotes the class of all N × N di-
agonal matrices with only 1 and −1 on their diagonals
(Kulesza, 2012, Theorem 4.1). We call such a transform
a DN -similarity of K.

In view of this equivalence class, we deﬁne the following
pseudo-distance between kernels K and K (cid:48):

ρ(K, K (cid:48)) = inf

|DKD − K (cid:48)|∞ ,

D∈DN

where for any matrix K, |K|∞ = maxi,j∈[N ] |Ki,j| de-
notes the entrywise sup-norm.

In this section, we review some of the interplay between
graphs and DPPs that plays a key role in the deﬁnition of
our estimator.

We begin by recalling some standard graph theoretic no-
tions. Let G = ([N ], E), |E| = m. A cycle C of G is
any connected subgraph in which each vertex has even de-
gree. Each cycle C is associated with an incidence vec-
tor x ∈ GF (2)m such that xe = 1 if e is an edge in
C and xe = 0 otherwise. The cycle space C of G is
the subspace of GF (2)m spanned by the incidence vec-
tors of the cycles in G. The dimension νG of the cycle
space is called cyclomatic number, and it is well known
that νG := m − N + κ(G), where κ(G) denotes the num-
ber of connected components of G.

Recall that a simple cycle is a graph where every vertex
has either degree two or zero and the set of vertices with
degree two form a connected set. A cycle basis is a basis
of C ⊂ GF (2)m such that every element is a simple cycle.
It is well known that every cycle space has a cycle basis of
induced cycles.

Deﬁnition 1. The cycle sparsity of a graph G is the mini-
mal (cid:96) for which G admits a cycle basis of induced cycles of
length at most (cid:96), with the convention that (cid:96) = 2 whenever
the cycle space is empty. A corresponding cycle basis is
called a shortest maximal cycle basis.

A shortest maximal cycle basis of the cycle space was also
studied for other reasons by (Chickering et al., 1995). We
defer a discussion of computing such a basis to Section 4.

For any subset S ⊆ [N ], denote by GK(S) = (S, E(S))
the subgraph of GK induced by S. A matching of GK(S)
is a subset M ⊆ E(S) such that any two distinct edges in
M are not adjacent in G(S). The set of vertices incident
to some edge in M is denoted by V (M ). We denote by
M(S) the collection of all matchings of GK(S). Then,
if GK(S) is an induced cycle, we can write the principal

Moments, Cycles and Learning DPPs

minor ∆S = det(KS) as follows:
(−1)|M | (cid:89)

(cid:88)

∆S =

K 2
i,j

(cid:89)

Ki,i

M ∈M(S)

{i,j}∈M

i(cid:54)∈V (M )

+ 2 × (−1)|S|+1 (cid:89)

Ki,j.

(1)

{i,j}∈E(S)

Others have considered the relationship between the princi-
pal minors of K and recovery of DPP(K). There has been
work regarding the symmetric principal minor assignment
problem, namely the problem of computing a matrix given
an oracle that gives any principal minor in constant time
(Rising et al., 2015).

In our setting, we can approximate the principal minors of
K by empirical averages. However the accuracy of our
estimator deteriorates with the size of the principal minor,
and we must therefore estimate the smallest possible princi-
pal minors in order to achieve optimal sample complexity.
Here, we prove a new result, namely, that the smallest (cid:96)
such that all the principal minors of K are uniquely deter-
mined by those of size at most (cid:96) is exactly the cycle sparsity
of the graph induced by K.
Proposition 1. Let K ∈ RN ×N be a symmetric matrix,
GK be the graph induced by K, and (cid:96) ≥ 3 be some integer.
The kernel K is completely determined up to DN -similarity
by its principal minors of size at most (cid:96) if and only if the
cycle sparsity of GK is at most (cid:96).

Proof. Note ﬁrst that all the principal minors of K com-
pletely determine K up to a DN -similarity (Rising et al.,
2015, Theorem 3.14). Moreover, recall that principal mi-
nors of degree at most 2 determine the diagonal entries of
K as well as the magnitude of its off-diagonal entries. In
particular, given these principal minors, one only needs to
recover the signs of the off-diagonal entries of K. Let the
sign of cycle C in K be the product of the signs of the
entries of K corresponding to the edges of C.

Suppose GK has cycle sparsity (cid:96) and let (C1, . . . , Cν) be a
cycle basis of GK where each Ci, i ∈ [ν] is an induced cy-
cle of length at most (cid:96). By (1), the sign of any Ci, i ∈ [ν] is
completely determined by the principal minor ∆S, where
S is the set of vertices of Ci and is such that |S| ≤ (cid:96).
Moreover, for i ∈ [ν], let xi ∈ GF (2)m denote the inci-
dence vector of Ci. By deﬁnition, the incidence vector x of
any cycle C is given by (cid:80)
i∈I xi for some subset I ⊂ [ν].
The sign of C is then given by the product of the signs of
Ci, i ∈ I and thus by corresponding principal minors. In
particular, the signs of all cycles are determined by the prin-
cipal minors ∆S with |S| ≤ (cid:96). In turn, by Theorem 3.12
in (Rising et al., 2015), the signs of all cycles completely
determine K, up to a DN -similarity.

Next, suppose the cycle sparsity of GK is at least (cid:96) + 1,

and let C(cid:96) be the subspace of GF (2)m spanned by the in-
duced cycles of length at most (cid:96) in GK. Let x1, . . . , xν be
a basis of C(cid:96) made of the incidence column vectors of in-
duced cycles of length at most (cid:96) in GK and form the matrix
A ∈ GF (2)m×ν by concatenating the xi’s. Since C(cid:96) does
not span the cycle space of GK, ν < νGK ≤ m. Hence,
the rank of A is less than m, so the null space of A(cid:62) is
non trivial. Let ¯x be the incidence column vector of an in-
duced cycle ¯C that is not in C(cid:96), and let h ∈ GL(2)m with
A(cid:62)h = 0, h (cid:54)= 0 and ¯x(cid:62)h = 1. These three conditions are
compatible because ¯C /∈ C(cid:96). We are now in a position to
deﬁne an alternate kernel K (cid:48) as follows: Let K (cid:48)
i,i = Ki,i
and |K (cid:48)
i,j| = |Ki,j| for all i, j ∈ [N ]. We deﬁne the signs
of the off-diagonal entries of K (cid:48) as follows: For all edges
e = {i, j}, i (cid:54)= j, sgn(K (cid:48)
e) = sgn(Ke) if he = 0 and
sgn(K (cid:48)
e) = − sgn(Ke) otherwise. We now check that K
and K (cid:48) have the same principal minors of size at most (cid:96)
but differ on a principal minor of size larger than (cid:96). To that
end, let x be the incidence vector of a cycle C in C(cid:96) so that
x = Aw for some w ∈ GL(2)ν. Thus the sign of C in K
is given by

(cid:89)

Ke = (−1)x(cid:62)h (cid:89)

K (cid:48)
e

e : xe=1

e : xe=1

= (−1)w(cid:62)A(cid:62)h (cid:89)

K (cid:48)

e =

(cid:89)

K (cid:48)
e

e : xe=1

e : xe=1

S

because A(cid:62)h = 0. Therefore, the sign of any C ∈ C(cid:96) is
the same in K and K (cid:48). Now, let S ⊆ [N ] with |S| ≤ (cid:96), and
let G = GKS = GK(cid:48)
be the graph corresponding to KS
(or, equivalently, to K (cid:48)
S). For any induced cycle C in G,
C is also an induced cycle in GK and its length is at most
(cid:96). Hence, C ∈ C(cid:96) and the sign of C is the same in K and
K (cid:48). By (Rising et al., 2015, Theorem 3.12), det(KS) =
S). Next observe that the sign of ¯C in K is given by
det(K (cid:48)
(cid:89)

(cid:89)

Ke = (−1)¯x(cid:62)h (cid:89)

K (cid:48)

e = −

K (cid:48)
e.

e : ¯xe=1

e : ¯xe=1

e : xe=1

Note also that since ¯C is an induced cycle of GK = GK(cid:48),
the above quantity is nonzero. Let ¯S be the set of vertices
in ¯C. By (1) and the above display, we have det(K ¯S) (cid:54)=
det(K (cid:48)
¯S). Together with (Rising et al., 2015, Theorem
3.14), it yields K (cid:54)= DK (cid:48)D for all D ∈ DN .

2.3. Deﬁnition of the Estimator

Our procedure is based on the previous result and can be
summarized as follows. We ﬁrst estimate the diagonal en-
tries (i.e., the principal minors of size one) of K by the
method of moments. By the same method, we estimate the
principal minors of size two of K, and we deduce estimates
of the magnitude of the off-diagonal entries. To use these
estimates to deduce an estimate ˆG of GK, we make the
following assumption on the kernel K.

Moments, Cycles and Learning DPPs

Assumption 1. Fix α ∈ (0, 1). For all 1 ≤ i < j ≤ N ,
either Ki,j = 0, or |Ki,j| ≥ α.

2.4. Geometry

Finally, we ﬁnd a shortest maximal cycle basis of ˆG, and
we set the signs of our non-zero off-diagonal entry esti-
mates by using estimators of the principal minors induced
by the elements of the basis, again obtained by the method
of moments.

For S ⊆ [N ], set ˆ∆S =

1S⊆Yp , and deﬁne

1
n

n
(cid:88)

p=1

ˆKi,i = ˆ∆{i}

and

ˆBi,j = ˆKi,i ˆKj,j − ˆ∆{i,j},

i,j,

where ˆKi,i and ˆBi,j are our estimators of Ki,i and K 2
respectively.
Deﬁne ˆG = ([N ], ˆE), where, for i (cid:54)= j, {i, j} ∈ ˆE if and
2 α2. The graph ˆG is our estimator of GK.
only if ˆBi,j ≥ 1
Let { ˆC1, ..., ˆCν ˆG
} be a shortest maximal cycle basis of the
cycle space of ˆG. Let ˆSi ⊆ [N ] be the subset of vertices of
ˆCi, for 1 ≤ i ≤ ν ˆG. We deﬁne
ˆHi = ˆ∆ ˆSi

(−1)|M | (cid:89)

ˆKi,i,

ˆBi,j

(cid:88)

(cid:89)

−

M ∈M( ˆSi)

{i,j}∈M

i(cid:54)∈V (M )

for 1 ≤ i ≤ ν ˆG. In light of (1), for large enough n, this
quantity should be close to

Hi = 2 × (−1)| ˆSi|+1 (cid:89)

Ki,j .

{i,j}∈E( ˆSi)

We note that this deﬁnition is only symbolic in nature, and
computing ˆHi in this fashion is extremely inefﬁcient. In-
stead, to compute it in practice, we will use the determinant
of an auxiliary matrix, computed via a matrix factorization.
Namely, let us deﬁne the matrix (cid:101)K ∈ RN ×N such that
(cid:101)Ki,i = ˆKi,i for 1 ≤ i ≤ N , and (cid:101)Ki,j = ˆB1/2

i,j . We have

det (cid:101)K ˆSi

=

(cid:88)

(−1)|M | (cid:89)

ˆBi,j

(cid:89)

ˆKi,i

M ∈M

{i,j}∈M

+ 2 × (−1)| ˆSi|+1 (cid:89)

i(cid:54)∈V (M )
ˆB1/2
i,j ,

{i,j}∈ ˆE( ˆSi)

so that we may equivalently write
ˆHi = ˆ∆ ˆSi

− det( (cid:101)K ˆSi

) + 2 × (−1)| ˆSi|+1 (cid:89)

ˆB1/2
i,j .

{i,j}∈ ˆE( ˆSi)

Finally, let ˆm = | ˆE|. Set the matrix A ∈ GF (2)ν ˆG× ˆm
with i-th row representing ˆCi in GF (2)m, 1 ≤ i ≤ ν ˆG,
2 [sgn( ˆHi) + 1],
) ∈ GF (2)ν ˆG with bi = 1
b = (b1, . . . , bν ˆG
1 ≤ i ≤ ν ˆG, and let x ∈ GF (2)m be a solution to the linear
system Ax = b if a solution exists, x = 1m otherwise.
We deﬁne ˆKi,j = 0 if {i, j} /∈ ˆE and ˆKi,j = ˆKj,i =
(2x{i,j} − 1) ˆB1/2
for all {i, j} ∈ ˆE.

i,j

The main result of this subsection is the following lemma
which relates the quality of estimation of K in terms of ρ
to the quality of estimation of the principal minors ∆S.
Lemma 1. Let K satisfy Assumption 1, and let (cid:96) be the
cycle sparsity of GK. Let ε > 0. If | ˆ∆S − ∆S| ≤ ε for all
S ⊆ [N ] with |S| ≤ 2 and if | ˆ∆S − ∆S| ≤ (α/4)|S| for all
S ⊆ [N ] with 3 ≤ |S| ≤ (cid:96), then

ρ( ˆK, K) < 4ε/α .

Proof. We can bound | ˆBi,j − K 2
ˆBi,j ≤ (Ki,i + α2/16)(Kj,j + α2/16) − (∆{i,j} − α2/16)

i,j|, namely,

and
ˆBi,j ≥ (Ki,i − α2/16)(Kj,j − α2/16) − (∆{i,j} + α2/16)

≤ K 2

i,j + α2/4

≥ K 2

i,j − 3α2/16,

giving | ˆBi,j − K 2
i,j| < α2/4. Thus, we can correctly deter-
mine whether Ki,j = 0 or |Ki,j| ≥ α, yielding ˆG = GK.
In particular, the cycle basis ˆC1, . . . , ˆCν ˆG
of ˆG is a cycle
basis of GK. Let 1 ≤ i ≤ ν ˆG. Denote by t = (α/4)|Si|.
We have

(cid:12)
(cid:12)
(cid:12)

ˆHi − Hi

(cid:12)
(cid:12)
(cid:12)
≤ | ˆ∆ ˆSi
− ∆ ˆSi
≤ (α/4)| ˆSi| + |M( ˆSi)|

| + |M( ˆSi)| max
x∈±1
(cid:104)

(cid:105)
(cid:104)
(1 + 4tx)| ˆSi| − 1
(cid:105)
(1 + 4t)| ˆSi| − 1
(cid:36)
(cid:37)(cid:33)

(cid:32)

≤ (α/4)| ˆSi| + T

| ˆSi|,

4t T (| ˆSi|, | ˆSi|)

| ˆSi|
2

| ˆSi|
2 − 1)(2| ˆSi| − 1)

≤ (α/4)| ˆSi| + 4t (2
≤ (α/4)| ˆSi| + t22| ˆSi|
< 2α| ˆSi| ≤ |Hi|,

i=1

(cid:1).

(cid:0)q
i

for positive integers p < q, we denote by
where,
T (q, p) = (cid:80)p
Therefore, we can deter-
mine the sign of the product (cid:81)
{i,j}∈E( ˆSi) Ki,j for ev-
in the cycle basis and recover the signs
ery element
of the non-zero off-diagonal entries of Ki,j. Hence,
ρ( ˆK, K) = max1≤i,j≤N
For i = j,
(cid:12)
(cid:12)
(cid:12)| ˆKi,j| − |Ki,j|
(cid:12) = | ˆKi,i − Ki,i| ≤ ε. For i
(cid:12)
(cid:12)
(cid:54)= j with
ˆE = E, one can easily show that
{i, j} ∈
(cid:12)
(cid:12)
ˆBi,j − K 2
(cid:12)
(cid:12)
(cid:12) ≤ 4ε, yielding
(cid:12)
i,j

(cid:12)
(cid:12)
(cid:12)| ˆKi,j| − |Ki,j|
(cid:12)
(cid:12)
(cid:12).

| ˆB1/2

i,j − |Ki,j|| ≤

4ε
i,j + |Ki,j|(cid:12)
(cid:12)

(cid:12)
(cid:12) ˆB1/2

≤

4ε
α

,

which completes the proof.

Moments, Cycles and Learning DPPs

We are now in a position to establish a sufﬁcient sample
size to estimate K within distance ε.
Theorem 1. Let K satisfy Assumption 1, and let (cid:96) be the
cycle sparsity of GK. Let ε > 0. For any A > 0, there
exists C > 0 such that

(cid:16) 1

n ≥ C

α2ε2 + (cid:96)(cid:0) 4
yields ρ( ˆK, K) ≤ ε with probability at least 1 − N −A.

log N ,

(cid:1)2(cid:96)(cid:17)

α

Proof. Using the previous lemma, and applying a union
bound,
(cid:104)

(cid:104)

P

(cid:105)
ρ( ˆK, K) > ε

≤

(cid:88)

P

(cid:105)
| ˆ∆S − ∆S| > αε/4

|S|≤2

(cid:88)

+

2≤|S|≤(cid:96)

P

| ˆ∆S − ∆S| > (α/4)|S|(cid:105)
(cid:104)

≤ 2N 2e−nα2ε2/8 + 2N (cid:96)+1e−2n(α/4)2(cid:96)
,
(2)

where we used Hoeffding’s inequality.

3. Information theoretic lower bound

We prove an information-theoretic lower bound that holds
already if GK is an (cid:96)-cycle. Let D(K(cid:107)K (cid:48)) and H(K, K (cid:48))
denote respectively the Kullback-Leibler divergence and
the Hellinger distance between DPP(K) and DPP(K (cid:48)).
Lemma 2. For η ∈ {−, +}, let K η be the (cid:96)×(cid:96) matrix with
elements given by



1/2
α
ηα
0

if j = i
if j = i ± 1
if (i, j) ∈ {(1, (cid:96)), ((cid:96), 1)}
otherwise

Ki,j =



.

Then, for any α ≤ 1/8, it holds

D(K(cid:107)K (cid:48)) ≤ 4(6α)(cid:96),

and

H(K, K (cid:48)) ≤ (8α2)(cid:96) .

Proof. It is straightforward to see that

det(K +

J ) − det(K −

J ) =

(cid:40)

2α(cid:96)
0

if J = [(cid:96)]
else

.

where |J| denotes the cardinality of J. The inclusion-
exclusion principle also yields that pη(S) = | det(K η −
I ¯S)| for all S ⊆ [l], where I ¯S stands for the (cid:96) × (cid:96) diago-
nal matrix with ones on its entries (i, i) for i /∈ S, zeros
elsewhere.

Denote by D(K +(cid:107)K −) the Kullback Leibler divergence
between DPP(K +) and DPP(K −):

D(K +(cid:107)K −) =

p+(S) log

(cid:19)

(cid:18) p+(S)
p−(S)

(cid:88)

S⊆[(cid:96)]

(cid:88)

S⊆[(cid:96)]

≤

p+(S)
p−(S)

(p+(S) − p−(S))

≤ 2α(cid:96) (cid:88)

S⊆[(cid:96)]

| det(K + − I ¯S)|
| det(K − − I ¯S)|

,

(4)

by (3). Using the fact that 0 < α ≤ 1/8 and the Gershgorin
circle theorem, we conclude that the absolute value of all
eigenvalues of K η − I ¯S are between 1/4 and 3/4. Thus we
obtain from (4) the bound D(K +(cid:107)K −) ≤ 4(6α)(cid:96).

Using the same arguments as above, the Hellinger distance
H(K +, K −) between DPP(K +) and DPP(K −) satisﬁes

H(K +, K −) =

(cid:32)

(cid:88)

(cid:33)2

p+(J) − p−(J)
(cid:112)p+(J) + (cid:112)p−(J)
4α2(cid:96)
2 · 4−(cid:96) = (8α2)(cid:96)

J⊆[(cid:96)]

(cid:88)

J⊆[(cid:96)]

≤

which completes the proof.

The sample complexity lower bound now follows from
standard arguments.

Theorem 2. Let 0 < ε ≤ α ≤ 1/8 and 3 ≤ (cid:96) ≤ N . There
exists a constant C > 0 such that if

n ≤ C

(cid:16) 8(cid:96)
α2(cid:96) +

log(N/(cid:96))
(6α)(cid:96) +

log N
ε2

(cid:17)

,

then the following holds: for any estimator ˆK based on n
samples, there exists a kernel K that satisﬁes Assumption 1
and such that the cycle sparsity of GK is (cid:96) and for which
ρ( ˆK, K) ≥ ε with probability at least 1/3.

If Y is sampled from DPP(K η), we denote by pη(S) =
P[Y = S], for S ⊆ [(cid:96)].
It follows from the inclusion-
exclusion principle that for all S ⊆ [(cid:96)],

p+(S) − p−(S) =

(−1)|J|(det K +

S∪J − det K −

S∪J )

(cid:88)

J⊆[(cid:96)]\S

Proof. Recall the notation of Lemma 2. First consider the
N × N block diagonal matrix K (resp. K (cid:48)) where its ﬁrst
block is K + (resp. K −) and its second block is IN −(cid:96). By a
standard argument, the Hellinger distance Hn(K, K (cid:48)) be-
tween the product measures DPP(K)⊗n and DPP(K (cid:48))⊗n
satisﬁes

= (−1)(cid:96)−|S|(det K + − det K −) = ±2α(cid:96) ,

1 −

H2

n(K, K (cid:48))
2

= (cid:0)1 −

H2(K, K (cid:48))
2

(cid:1)n

≥ (cid:0)1 −

α2(cid:96)
2 × 8(cid:96)

(cid:1)n

,

(3)

Moments, Cycles and Learning DPPs

which yields the ﬁrst term in the desired lower bound.

Algorithm 1 Compute Estimator ˆK

Next, by padding with zeros, we can assume that L = N/(cid:96)
is an integer. Let K (0) be a block diagonal matrix where
each block is K + (using the notation of Lemma 2). For
j = 1, . . . , L, deﬁne the N × N block diagonal matrix
K (j) as the matrix obtained from K (0) by replacing its jth
block with K − (again using the notation of Lemma 2).

Since DPP(K (j)) is the product measure of L lower di-
mensional DPPs that are each independent of each other,
using Lemma 2 we have D(K (j)(cid:107)K (0)) ≤ 4(6α)(cid:96). Hence,
by Fano’s lemma (see, e.g., Corollary 2.6 in (Tsybakov,
2009)), the sample complexity to learn the kernel of a DPP
within a distance ε ≤ α is

Input: samples Y1, ..., Yn, parameter α > 0.
Compute ˆ∆S for all |S| ≤ 2.
Set ˆKi,i = ˆ∆{i} for 1 ≤ i ≤ N .
Compute ˆBi,j for 1 ≤ i < j ≤ N .
Form (cid:101)K ∈ RN ×N and ˆG = ([N ], ˆE).
Compute a shortest maximal cycle basis {ˆv1, ..., ˆvν ˆG
Compute ˆ∆ ˆSi
for 1 ≤ i ≤ ν ˆG.
Compute ˆC ˆSi
for 1 ≤ i ≤ ν ˆG.
using det (cid:101)K ˆSi
Construct A ∈ GF (2)ν ˆG×m, b ∈ GF (2)ν ˆG .
Solve Ax = b using Gaussian elimination.
Set ˆKi,j = ˆKj,i = (2x{i,j} − 1) ˆB1/2

}.

i,j , for all {i, j} ∈ ˆE.

(cid:19)

(cid:18) log(N/(cid:96))
(6α)(cid:96)

Ω

which yields the second term.

The third term follows from considering K0 = (1/2)IN
and letting Kj be obtained from K0 by adding ε to
the jth entry along the diagonal.
It is easy to see that
D(Kj(cid:107)K0) ≤ 8ε2. Hence, a second application of Fano’s
lemma yields that the sample complexity to learn the kernel
of a DPP within a distance ε is Ω( log N

ε2 ).

The third term in the lower bound is the standard parametric
term and is unavoidable in order to estimate the magnitude
of the coefﬁcients of K. The other terms are more interest-
ing. They reveal that the cycle sparsity of GK, namely, (cid:96),
plays a key role in the task of recovering the sign pattern of
K. Moreover the theorem shows that the sample complex-
ity of our method of moments estimator is near optimal.

4. Algorithms

4.1. Horton’s algorithm
We ﬁrst give an algorithm to compute the estimator ˆK
deﬁned in Section 2. A well-known algorithm of Hor-
ton (Horton, 1987) computes a cycle basis of minimum
total length in time O(m3N ). Subsequently, the running
time was improved to O(m2N/ log N ) time (Amaldi et al.,
2010). Also, it is known that a cycle basis of minimum total
length is a shortest maximal cycle basis (Chickering et al.,
1995). Together, these results imply the following.

Lemma 3. Let G = ([N ], E), |E| = m. There is an
algorithm to compute a shortest maximal cycle basis in
O(m2N/ log N ) time.

In addition, we recall the following standard result re-
garding the complexity of Gaussian elimination (Golub &
Van Loan, 2012).

Lemma 4. Let A ∈ GF (2)ν×m, b ∈ GF (2)ν. Then Gaus-
sian elimination will ﬁnd a vector x ∈ GF (2)m such that
Ax = b or conclude that none exists in O(ν2m) time.

We give our procedure for computing the estimator ˆK in
Algorithm 1. In the following theorem, we bound the run-
ning time of Algorithm 1 and establish an upper bound on
the sample complexity needed to solve the recovery prob-
lem as well as the sample complexity needed to compute
an estimate ˆK that is close to K.
Theorem 3. Let K ∈ RN ×N be a symmetric matrix satis-
fying 0 (cid:22) K (cid:22) I, and satisfying Assumption 1. Let GK be
the graph induced by K and (cid:96) be the cycle sparsity of GK.
Let Y1, ..., Yn be samples from DPP(K) and δ ∈ (0, 1). If

n >

log(N (cid:96)+1/δ)
(α/4)2(cid:96)

,

then with probability at least 1 − δ, Algorithm 1 computes
an estimator ˆK which recovers the signs of K up to a DN -
similarity and satisﬁes

ρ(K, ˆK) <

(cid:18) 8 log(4N (cid:96)+1/δ)
n

1
α

(cid:19)1/2

(5)

in O(m3 + nN 2) time.

Proof. (5) follows directly from (2) in the proof of Theo-
rem 1. That same proof also shows that with probability
at least 1 − δ, the support of GK and the signs of K are
recovered up to a DN -similarity. What remains is to up-
per bound the worst case run time of Algorithm 1. We
will perform this analysis line by line. Initializing ˆK re-
quires O(N 2) operations. Computing ∆S for all subsets
|S| ≤ 2 requires O(nN 2) operations. Setting ˆKi,i requires
O(N ) operations. Computing ˆBi,j for 1 ≤ i < j ≤ N
requires O(N 2) operations. Forming (cid:101)K requires O(N 2)
operations. Forming GK requires O(N 2) operations. By

Moments, Cycles and Learning DPPs

Lemma 3, computing a shortest maximal cycle basis re-
quires O(mN ) operations. Constructing the subsets Si,
1 ≤ i ≤ ν ˆG, requires O(mN ) operations. Computing ˆ∆Si
for 1 ≤ i ≤ ν ˆG requires O(nm) operations. Computing
ˆCSi using det( (cid:101)K[Si]) for 1 ≤ i ≤ ν ˆG requires O(m(cid:96)3)
operations, where a factorization of each (cid:101)K[Si] is used to
compute each determinant in O((cid:96)3) operations. Construct-
ing A and b requires O(m(cid:96)) operations. By Lemma 4,
ﬁnding a solution x using Gaussian elimination requires
O(m3) operations. Setting ˆKi,j for all edges {i, j} ∈ E
requires O(m) operations. Put this all together, Algorithm
1 runs in O(m3 + nN 2) time.

4.2. Chordal Graphs

Here we show that it is possible to obtain faster algorithms
by exploiting the structure of GK. Speciﬁcally, in the case
where GK chordal, we give an O(m) time algorithm to
determine the signs of the off-diagonal entries of the es-
timator ˆK, resulting in an improved overall runtime of
O(m + nN 2). Recall that a graph G = ([N ], E) is said
to be chordal if every induced cycle in G is of length three.
Moreover, a graph G = ([N ], E) has a perfect elimina-
tion ordering (PEO) if there exists an ordering of the ver-
tex set {v1, ..., vN } such that, for all i, the graph induced
by {vi} ∪ {vj|{i, j} ∈ E, j > i} is a clique. It is well
known that a graph is chordal if and only if it has a PEO. A
PEO of a chordal graph with m edges can be computed in
O(m) operations using lexicographic breadth-ﬁrst search
(Rose et al., 1976).

Lemma 5. Let G = ([N ], E), be a chordal graph and
{v1, ..., vn} be a PEO. Given i, let i∗ := min{j|j >
i, {vi, vj} ∈ E}. Then the graph G(cid:48) = ([N ], E(cid:48)), where
E(cid:48) = {{vi, vi∗ }}N −κ(G)

, is a spanning forest of G.

i=1

Proof. We ﬁrst show that there are no cycles in G(cid:48). Sup-
pose to the contrary, that there is an induced cycle C of
length k on the vertices {vj1, ..., vjk }. Let v be the vertex
of smallest index. Then v is connected to two other vertices
in the cycle of larger index. This is a contradiction to the
construction.

What remains is to show that |E(cid:48)| = N − κ(G). It sufﬁces
to prove the case κ(G) = 1. Suppose to the contrary, that
there exists a vertex vi, i < N , with no neighbors of larger
index. Let P be the shortest path in G from vi to vN . By
connectivity, such a path exists. Let vk be the vertex of
smallest index in the path. However, it has two neighbors
in the path of larger index, which must be adjacent to each
other. Therefore, there is a shorter path.

Now, given the chordal graph GK induced by K and the
estimates of principal minors of size at most three, we pro-
vide an algorithm to determine the signs of the edges of

Algorithm 2 Compute Signs of Edges in Chordal Graph
Input: GK = ([N ], E) chordal, ˆ∆S for |S| ≤ 3.

Compute a PEO {v1, ..., vN }.
Compute the spanning forest G(cid:48) = ([N ], E(cid:48)).
Set all edges in E(cid:48) to have positive sign.
Compute ˆC{i,j,i∗} for all {i, j} ∈ E \ E(cid:48), j < i.
Order edges E \ E(cid:48) = {e1, ..., eν} such that i > j if
max ei < max ej.
Visit edges in sorted order and for e = {i, j}, j > i, set

sgn({i, j}) = sgn( ˆC{i,j,i∗}) sgn({i, i∗}) sgn({j, i∗}).

GK, or, equivalently, the off-diagonal entries of K.

Theorem 4. If GK is chordal, Algorithm 2 correctly deter-
mines the signs of the edges of GK in O(m) time.

Proof. We will simultaneously perform a count of the op-
erations and a proof of the correctness of the algorithm.
Computing a PEO requires O(m) operations. Computing
the spanning forest requires O(m) operations. The edges
of the spanning tree can be given arbitrary sign, because it
is a cycle-free graph. This assigns a sign to two edges of
each 3-cycle. Computing each ˆC{i,j,i∗} requires a constant
number of operations because (cid:96) = 3, requiring a total of
O(m − N ) operations. Ordering the edges requires O(m)
operations. Setting the signs of each remaining edge re-
quires O(m) operations.

Therefore, when GK is chordal, the overall complexity
required by our algorithm to compute ˆK is reduced to
O(m + nN 2).

5. Experiments

Here we present experiments to supplement the theoretical
results of the paper. We test our algorithm on two types
of random matrices. First, we consider the matrix K ∈
RN ×N corresponding to the cycle on N vertices,

K =

I +

A,

1
2

1
4

where A is symmetric and has non-zero entries only on the
edges of the cycle, either +1 or −1, each with probability
1/2. By the Gershgorin circle theorem, 0 (cid:22) K (cid:22) I. Next,
we consider the matrix K ∈ RN ×N corresponding to the
clique on N vertices,

K =

I +

1
2

1
√
N
4

A,

Moments, Cycles and Learning DPPs

where A is symmetric and has all entries either +1 or −1,
N (cid:22)
each with probability 1/2. It is well known that −2
A (cid:22) 2

N with high probability, implying 0 (cid:22) K (cid:22) I.

√

√

For both cases and for a range of values of matrix dimen-
sion N and samples n, we run our algorithm on 50 ran-
domly generated instances. We record the proportion of
trials where we recover the graph induced by K, and the
proportion of the trials where we recover both the graph
and correctly determine the signs of the entries.

In Figure 1, the shade of each box represents the propor-
tion of trials where recovery was successful for a given pair
N, n. A completely white box corresponds to zero success
rate, black to a perfect success rate.

The plots corresponding to the cycle and the clique are
telling. We note that for the clique, recovering the spar-
sity pattern and recovering the signs of the off-diagonal en-
tries come hand-in-hand. However, for the cycle, there is
a noticeable gap between the number of samples required
to recover the sparsity pattern and the number of samples
required to recover the signs of the off-diagonal entries.
This empirically conﬁrms the central role that cycle spar-
sity plays in parameter estimation, and further corroborates
our theoretical results.

6. Conclusion and open questions

In this paper, we gave the ﬁrst provable guarantees for
learning the parameters of a DPP. Our upper and lower
bounds reveal the key role played by the parameter (cid:96), which
is the cycle sparsity of graph induced by the kernel of the
DPP. Our estimator does not need to know (cid:96) beforehand,
but can adapt to the instance. Moreover, our procedure
outputs an estimate of (cid:96), which could potentially be used
for further inference questions such as testing and conﬁ-
dence intervals. An interesting open question is whether
on a graph by graph basis, the parameter (cid:96) exactly deter-
mines the optimal sample complexity. Moreover when the
number of samples is too small, can we exactly characterize
which signs can be learned correctly and which cannot (up
to a similarity transformation by D)? Such results would
lend new theoretical insights into the output of algorithms
for learning DPPs, and which individual parameters in the
estimate we can be conﬁdent about and which we cannot.

A.M.

Acknowledgements.
is supported in part by
NSF CAREER Award CCF-1453261, NSF Large CCF-
1565235, a David and Lucile Packard Fellowship and an
Alfred P. Sloan Fellowship. P.R. is supported in part
by NSF CAREER DMS-1541099, NSF DMS-1541100,
DARPA W911NF-16-1-0551, ONR N00014-17-1-2147
and a grant from the MIT NEC Corporation.

(a) graph recovery, cycle

(b) graph and sign recovery, cycle

(c) graph recovery, clique

(d) graph and sign recovery, clique

Figure 1: Plots of the proportion of successive graph recov-
ery, and graph and sign recovery, for random matrices with
cycle and clique graph structure, respectively. The darker
the box, the higher the proportion of trials that were recov-
ered successfully.

Moments, Cycles and Learning DPPs

References

Affandi, Raja Haﬁz, Fox, Emily B., Adams, Ryan P., and
Taskar, Benjamin. Learning the parameters of determi-
nantal point process kernels. In Proceedings of the 31th
International Conference on Machine Learning, ICML
2014, Beijing, China, 21-26 June 2014, pp. 1224–1232,
2014.

Amaldi, Edoardo, Iuliano, Claudio, and Rizzi, Romeo. Ef-
ﬁcient deterministic algorithms for ﬁnding a minimum
cycle basis in undirected graphs. In International Con-
ference on Integer Programming and Combinatorial Op-
timization, pp. 397–410. Springer, 2010.

Batmanghelich, Nematollah Kayhan, Quon, Gerald,
Kulesza, Alex, Kellis, Manolis, Golland, Polina, and
Bornn, Luke. Diversifying sparsity using variational de-
terminantal point processes. ArXiv: 1411.6307, 2014.

Borodin, Alexei and Rains, Eric M. Eynard–mehta theo-
rem, schur process, and their pfafﬁan analogs. Journal
of statistical physics, 121(3):291–317, 2005.

Brunel, Victor-Emmanuel, Moitra, Ankur, Rigollet,
Philippe,
Maximum likeli-
hood estimation of determinantal point processes.
arXiv:1701.06501, 2017.

and Urschel,

John.

Chickering, David M., Geiger, Dan, and Heckerman,
David. On ﬁnding a cycle basis with a shortest maxi-
Information Processing Letters, 54(1):55 –
mal cycle.
58, 1995.

C¸ ivril, Ali and Magdon-Ismail, Malik. On selecting a max-
imum volume sub-matrix of a matrix and related prob-
lems. Theoretical Computer Science, 410(47-49):4801–
4811, 2009.

Deshpande, Amit and Rademacher, Luis. Efﬁcient volume
sampling for row/column subset selection. In Founda-
tions of Computer Science (FOCS), 2010 51st Annual
IEEE Symposium on, pp. 329–338. IEEE, 2010.

Dyson, Freeman J. Statistical theory of the energy levels
of complex systems. III. J. Mathematical Phys., 3:166–
175, 1962. ISSN 0022-2488.

Gillenwater, Jennifer A, Kulesza, Alex, Fox, Emily, and
Taskar, Ben. Expectation-maximization for learning de-
terminantal point processes. In NIPS, 2014.

Golub, Gene H and Van Loan, Charles F. Matrix computa-

tions, volume 3. JHU Press, 2012.

Horton, Joseph Douglas. A polynomial-time algorithm to
ﬁnd the shortest cycle basis of a graph. SIAM Journal on
Computing, 16(2):358–366, 1987.

Kulesza, A. Learning with determinantal point processes.

PhD thesis, University of Pennsylvania, 2012.

Kulesza, Alex and Taskar, Ben. k-DPPs: Fixed-size de-
terminantal point processes. In Proceedings of the 28th
International Conference on Machine Learning, ICML
2011, Bellevue, Washington, USA, June 28 - July 2,
2011, pp. 1193–1200, 2011.

Kulesza, Alex and Taskar, Ben. Determinantal Point
Now Publishers
ISBN 1601986289,

Processes for Machine Learning.
Inc., Hanover, MA, USA, 2012.
9781601986283.

Lee, Donghoon, Cha, Geonho, Yang, Ming-Hsuan, and Oh,
Songhwai. Individualness and determinantal point pro-
In Computer Vision -
cesses for pedestrian detection.
ECCV 2016 - 14th European Conference, Amsterdam,
The Netherlands, October 11-14, 2016, Proceedings,
Part VI, pp. 330–346, 2016.

Li, Chengtao, Jegelka, Stefanie, and Sra, Suvrit. Fast sam-
pling for strongly rayleigh measures with application to
determinantal point processes. 1607.03559, 2016a.

Li, Chengtao, Jegelka, Stefanie, and Sra, Suvrit. Fast dpp
sampling for nystrom with application to kernel meth-
International Conference on Machine Learning
ods.
(ICML), 2016b.

Lin, Hui and Bilmes, Jeff A. Learning mixtures of submod-
ular shells with application to document summarization.
In Proceedings of the Twenty-Eighth Conference on Un-
certainty in Artiﬁcial Intelligence, Catalina Island, CA,
USA, August 14-18, 2012, pp. 479–490, 2012.

Macchi, Odile. The coincidence approach to stochastic
point processes. Advances in Appl. Probability, 7:83–
122, 1975. ISSN 0001-8678.

Mariet, Zelda and Sra, Suvrit. Fixed-point algorithms for
learning determinantal point processes. In Proceedings
of the 32nd International Conference on Machine Learn-
ing (ICML-15), pp. 2389–2397, 2015.

Nikolov, Aleksandar. Randomized rounding for the largest
In Proceedings of the Forty-Seventh
simplex problem.
Annual ACM on Symposium on Theory of Computing,
pp. 861–870. ACM, 2015.

Nikolov, Aleksandar and Singh, Mohit. Maximizing deter-
minants under partition constraints. In STOC, pp. 192–
201, 2016.

Rebeschini, Patrick and Karbasi, Amin. Fast mixing for
discrete point processes. In COLT, pp. 1480–1500, 2015.

Moments, Cycles and Learning DPPs

Rising, Justin, Kulesza, Alex, and Taskar, Ben. An efﬁcient
algorithm for the symmetric principal minor assignment
problem. Linear Algebra and its Applications, 473:126
– 144, 2015.

Rose, Donald J, Tarjan, R Endre, and Lueker, George S. Al-
gorithmic aspects of vertex elimination on graphs. SIAM
Journal on computing, 5(2):266–283, 1976.

Snoek,

Jasper, Zemel, Richard S.,

and Adams,
A determinantal point process la-
Ryan Prescott.
tent variable model for inhibition in neural spiking data.
In Advances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meeting
held December 5-8, 2013, Lake Tahoe, Nevada, United
States., pp. 1932–1940, 2013.

Summa, Marco Di, Eisenbrand, Friedrich, Faenza, Yuri,
and Moldenhauer, Carsten. On largest volume simplices
In Proceedings of the Twenty-
and sub-determinants.
Sixth Annual ACM-SIAM Symposium on Discrete Algo-
rithms, pp. 315–323. Society for Industrial and Applied
Mathematics, 2015.

Tsybakov, Alexandre B. Introduction to nonparametric es-
timation. Springer Series in Statistics. Springer, New
York, 2009.

Xu, Haotian and Ou, Haotian. Scalable discovery of au-
dio ﬁngerprint motifs in broadcast streams with determi-
nantal point process based motif clustering. IEEE/ACM
Trans. Audio, Speech & Language Processing, 24(5):
978–989, 2016.

Yao, Jin-ge, Fan, Feifan, Zhao, Wayne Xin, Wan, Xiaojun,
Chang, Edward Y., and Xiao, Jianguo. Tweet timeline
generation with determinantal point processes. In Pro-
ceedings of the Thirtieth AAAI Conference on Artiﬁcial
Intelligence, February 12-17, 2016, Phoenix, Arizona,
USA., pp. 3080–3086, 2016.

