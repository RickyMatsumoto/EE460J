Supplementary Material for ”Collect at Once, Use Effectively:
Making Non-interactive Locally Private Learning Possible”

1. Omitted Proofs in Section 3

1

(Lemma

in Main Body).

Let
Lemma
3
x1, x2, · · · , xn ∼ i.i.d.D with µ = ED[x] and
supp(D) ⊆ B(0, 1). Let G and {yi}n
i=1 deﬁned in the
above procedure. For each of group Sj ﬁxed, we have the
following with probability 2/3:

(cid:13)
(cid:13)
(cid:13)

1
|Sj|

(cid:88)

yi∈Sj

yi − Gµ

≤ O

(cid:13)
(cid:13)
(cid:13)1

(cid:32)

(cid:33)

p log(nd)
(cid:15)(cid:112)|Sj|

(1)

Proof. Apparently 1
|Sj |

(cid:80)

ri ∼ N (0, 2 log(1.25/δ)

Id).

|Sj |(cid:15)2

i∈Sj

So we have (cid:107) 1
|Sj |

(cid:80)

ri(cid:107)1 ≤ O

i∈Sj

(cid:18)

(cid:19)

p log n
√
|Sj |
(cid:15)

with proba-

9 . We then turn to bound the loss incurred by random

bility 1
sample of data.

(cid:13)
E
(cid:13)
(cid:13)µ −

1
|Sj|

(cid:88)

i∈Sj

(cid:13)
2
(cid:13)
(cid:13)

xi

=

1
|Sj|

d
(cid:88)

l=1

var(x1l)

(2)

≤

1
|Sj|

d
(cid:88)

l=1

E[x2

1l] ≤

1
|Sj|

.

According to Markov Inequality, we have






P

(cid:13)
(cid:13)
(cid:13)µ −

1
|Sj|

(cid:88)

i∈Sj

(cid:13)
2
(cid:13)
(cid:13)

xi

≥

9
|Sj|






≤

1
9

Given x1, x2, · · · , xn ﬁxed under this event, we can easily
derive upper bounds on entries of G(µ − 1
xi):
i∈Sj
|Sj |
for g ∼ N (0, Id) and q = µ − 1
xi, we have
|Sj |
|Sj | with probability 1− 1
|gT q| ≤ 12
9d . By union bound
we have the following with probability 2
9 :

(cid:113) log d

i∈Sj

(cid:80)

(cid:80)

(cid:13)
(cid:13)
(cid:13)G(µ −

1
|Sj|

(cid:88)

i∈Sj

xi)

≤ O

(cid:13)
(cid:13)
(cid:13)1

(cid:32)(cid:115)

(cid:33)

.

p log d
|Sj|

Putting the two inequalities together using union bound, we
get the result.

Lemma 2 (Lemma 6 in Main Body). Under the assump-
tions made in Section 3.2, given projection matrix Φ, with

high probability over the randomness of private mecha-
nism, we have

¯L(wpriv; ¯X, y) − ¯L( ˆw∗; ¯X, y) (cid:54) ˜O

(cid:19)

(cid:18)(cid:114) m
n(cid:15)2

(3)

Proof. Note, once we prove the uniform convergence of
(cid:1) for any w ∈ C,
| ˆL(w; Z, v) − ¯L(w; ¯X, y)| (cid:54) O (cid:0)(cid:112) m
n(cid:15)2
then the conclusion holds directly. Now, we will prove
the uniform convergence. Note Z = ¯X + E, where
E ∈ Rn×m, and each entry eij ∼ N (0, σ2), v = y + r,
where r ∼ N (0, σ2In). Denote ¯w = ΦT w.

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:54) 1
2n
(cid:54) 1
2n
(cid:54) 1
2n
(cid:54) 1
2n
1
n

(cid:12)
ˆL(w; Z, v) − ¯L(w; ¯X, y)
(cid:12)
(cid:12)
1
n

¯wT (Q − ¯X T ¯X) ¯w −

1
2n

(cid:0)vT Z ¯w − yT ¯X ¯w(cid:1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)vT Z ¯w − yT ¯X ¯w(cid:12)
(cid:12)
(cid:12)

|vT Z ¯w − yT ¯X ¯w|

2 +

(cid:13)2 (cid:107) ¯w(cid:107)2

(cid:13)F (cid:107) ¯w(cid:107)2

(cid:13)Q − ¯X T ¯X(cid:13)
(cid:13)

1
n
1
(cid:13)Q − ¯X T ¯X(cid:13)
(cid:13)
2 +
n
(cid:13)Z T Z − nσ2Im − ¯X T ¯X(cid:13)
(cid:13)
(cid:13)F (cid:107) ¯w(cid:107)2
2 + 1
1
(cid:13) ¯X T E(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F (cid:107) ¯w(cid:107)2
(cid:13)ET E − nσ2Im
2 +
n
(cid:13)ET r(cid:13)
(cid:13)2 + (cid:13)
(cid:13)2 + (cid:13)
(cid:13)2

(cid:13) ¯X T r(cid:13)

(cid:1) (cid:107) ¯w(cid:107)2

(cid:13)ET y(cid:13)
(cid:0)(cid:13)

n |vT Z ¯w − yT ¯X ¯w|

(cid:13)F (cid:107) ¯w(cid:107)2

2 +

random projection, we know
From the property of
(cid:54) 1 with high probability. Besides, as each en-
(cid:107) ¯w(cid:107)2
try in E is i.i.d. Gaussian, and E[ET E] = nσ2Im,
(cid:19)
thus we have 1
2n

(cid:13)
(cid:13)ET E − nσ2Im

(cid:113) log m
n

(cid:54) O

(cid:18)

σ

(cid:13)
(cid:13)2

with high probability according to lemma 3, hence
1
2n
ability.

(cid:13)
(cid:13)ET E − nσ2Im

(cid:113) m log m
n

) with high prob-

(cid:54) O(σ

(cid:13)
(cid:13)F

n2

1
n2

i=1(qT

j=1((cid:80)m

(cid:13)
(cid:13) ¯X T E(cid:13)
2
F = 1
(cid:13)

(cid:80)m
j ei)2), where
As
qj, ei are the j-th and i-th column of ¯X and E respec-
1
j ei)2 obeys Chi-
tively. For each j ∈ [m],
n2
square distribution (with some scaling), thus with high
j ei)2 (cid:54) O
probability, 1
. There-
n2
(cid:80)m
j ei)2) (cid:54)
fore, by union bound, we have 1
n2
(cid:16) mσ2
j (cid:107)qj(cid:107)2 =
n

(cid:16) m(cid:107)qj (cid:107)2σ2
n2
j=1((cid:80)m
i=1(qT
(cid:17)
, as (cid:80)

j (cid:107)qj (cid:107)2σ2
n2

i=1(qT

i=1(qT

(cid:18) m (cid:80)

= O

(cid:80)m

(cid:80)m

O

(cid:19)

(cid:17)

Non-interactive Local DP Learning

(cid:18)(cid:113) mσ2

(cid:19)

(cid:54) O

(cid:18)(cid:113) mσ2

(cid:13) ¯ET y(cid:13)
(cid:13)
(cid:13)2

(cid:13) ¯X(cid:13)
(cid:13)
2
(cid:54) n. Hence, there is 1
(cid:13)
n
F
with high probability. Using similar augument, we have
(cid:19)

(cid:13) ¯X T E(cid:13)
(cid:13)
(cid:13)F

1
n
with high probability. For 1
(cid:13), according to matrix
n
concentration inequality (Theorem 4.1.1 in (Tropp et al.,
2015)), we have 1
n

, 1
n
(cid:13) ¯X T r(cid:13)
(cid:13)

(cid:13) ¯X T r(cid:13)
(cid:13)
(cid:13)2

(cid:13) ¯ET r(cid:13)
(cid:13)
(cid:13)2

(cid:18)(cid:113) mσ2

(cid:54) O

(cid:54) O

(cid:16) 1√

(cid:54) O

(cid:19)

(cid:17)

n

n

n

n

.

Combine all these results together, we obtain the desired
conclusion.

Lemma 3 ((Vershynin, 2009)). Suppose x ∈ Rd be a ran-
dom vector satisﬁes E[xxT ] = Id. Denote (cid:107)x(cid:107)φ1
= M ,
represents Orlicz ψ1-norm. Let x1, . . . , xn be
where (cid:107)·(cid:107)ψ1
independent copies of x, then for every (cid:15) ∈ (0, 1), we have
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
Theorem 1 (Theorem 3 in Main Body). Under the assump-
for β > 0,
tion in this section, set m = Θ
then with high probability , there is

(cid:17)
(cid:16)(cid:112)n(cid:15)2 log d

(cid:54) de−n(cid:15)2/4M 2

(cid:32)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

i − Id

xixT

n
(cid:88)

> (cid:15)

1
n

Pr

i=1

(cid:33)

L(wpriv) − L(w∗) = ˜O

(cid:19)1/4(cid:33)

(cid:32)(cid:18) log d
n(cid:15)2

Proof. On one hand,

L(wpriv) − L(w∗)

=L(wpriv) − ¯L(wpriv) + ¯L(wpriv) − ¯L( ˆw∗)
+ ¯L( ˆw∗) − ¯L(w∗) + ¯L(w∗) − L(w∗)
(cid:54) (cid:2)L(wpriv) − ¯L(wpriv) + ¯L(w∗) − L(w∗)(cid:3)

+ ¯L(wpriv) − ¯L( ˆw∗)

(cid:54)G[max

i
+ max

i

{| (cid:10)wpriv, xi
{| (cid:104)w∗, xi(cid:105) − (cid:10)ΦT w∗, ΦT xi

(cid:11) − (cid:10)ΦT wpriv, ΦT xi
(cid:11) |}]

(cid:11) |}

+ [ ¯L(wpriv) − ¯L( ˆw∗)]

(4)

(where G is the Lipschitz constant)

On the other hand, for ∀w ∈ C, ∀x ∈ D, there is

| (cid:104)w, x(cid:105) − (cid:10)ΦT w, ΦT x(cid:11) |
(cid:12)
(cid:107)ΦT (w+x)(cid:107)2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:107)ΦT (w+x)(cid:107)2
4

−(cid:107)w+x(cid:107)2
2

2

2

−(cid:107)ΦT (w−x)(cid:107)2
4

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:54)

2

2−(cid:107)w−x(cid:107)2
4

(cid:12)
(cid:12)
(cid:12)
(cid:12)
−(cid:107)w−x(cid:107)2
2

− (cid:107)w+x(cid:107)2

(cid:107)ΦT (w−x)(cid:107)2
4

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

According to the results of random projection w.r.t. ad-
ditive error (Dirksen, 2016), we know with high probabil-
ity, there is | (cid:104)w, x(cid:105) − (cid:10)ΦT w, ΦT x(cid:11) | (cid:54) O

(cid:18)(cid:113) log d

, for

(cid:19)

m

∀w ∈ C, ∀x ∈ D. Therefore, the ﬁrst term in equation (4)

is less than O

(cid:18)(cid:113) log d

(cid:19)

.

m

From lemma 2, we know ¯L( ¯wpriv) − ¯L( ¯w∗) (cid:54) ˜O (cid:0)(cid:112) m
n(cid:15)2
holds with high probability. Combine these two inequali-
ties, it is easy to determine the optimal m, then obtain the
conclusion.

(cid:1)

Corollary 1 (Corollary 2 in Main Body). Algorithm LDP
kernel mechanism satisﬁes ((cid:15), δ)-LDP, and with high prob-
ability, there is

L ˆH ( ˆwpriv) − LH (f ∗) (cid:54) ˜O

|Φ(x)T f ∗ − ( ˆΦ(x))T ˆwpriv| (cid:54) ˜O

sup
x∈X

(cid:19)1/4(cid:33)

(cid:19)1/8(cid:33)

(cid:32)(cid:18) d
n(cid:15)2
(cid:32)(cid:18) d
n(cid:15)2

Proof. Algorithm satisﬁes local privacy is obvious. For
excess risk, as L ˆH ( ˆwpriv) − LH (f ∗) = L ˆH ( ˆwpriv) −
L ˆH (g∗) + L ˆH (g∗) − LH (f ∗), follow nearly the same
proof of lemma 5 of sparse linear regression, we have
L ˆH ( ˆwpriv) − L ˆH (g∗) (cid:54) ˜O
nearly borrow the proof of Lemma 17 in (Rubinstein et al.,
2012) and property of RRF , we have

. On the other hand,

(cid:18)(cid:113) dp

n(cid:15)2

(cid:19)

L ˆH (g∗) − LH (f ∗) (cid:54) ˜O

(cid:32)(cid:115)

(cid:33)

d
dp

(cid:17)

(cid:16)√

dn(cid:15)2

Combine above two inequalities, and choose optimal dp as
˜O
, we obtain the ﬁrst inequality of the conclu-
sion. Then combine lemma 7 in this paper, it is easy to
obtaint the second inequality.

2. Omitted contents and proofs in Section 4

2.1. Relations between smooth generalized linear losses
(SGLL) and generalized linear models (GLM)

Note that a model is called GLM, if for x, w∗ ∈ Rd, label
y with respect to x is given by a distribution which belongs
to the exponential family:

p(y|x, w∗) = exp

(cid:18) yθ − b(θ)
Φ

(cid:19)

+ c(y, Φ)

(5)

where θ, Φ are parameters, and b(θ), c(y, Φ) are known
functions. Besides, there is an one-to-one continuous dif-
ferentiable transformation g(·) such that g(b(cid:48)(θ)) = xT w∗.

According to the key equality g(b(cid:48)(θ)) = xT w∗, usually
we can obtain smooth function θ = h1(xT w∗), b(θ) =
h2(xT w∗),
function

and what’s more,

univariate

Non-interactive Local DP Learning

approximation error, we know |(g(w)−ˆg(w))T (v−w)| (cid:54)
γ
2 . What’s more, as L(w) is convex and β-smooth, that
2 (cid:107)v − w(cid:107)2.
is 0 (cid:54) L(v) − L(w) − g(w)T (v − w) (cid:54) β
Combined these inequalities, we obtain

− γ
2

(cid:54) L(v) − L(w) − ˆg(w)T (v − w) (cid:54) β

2 (cid:107)v − w(cid:107)2 + γ

2

⇐⇒0 (cid:54) L(v) − (L(w) − γ

2 ) − ˆg(w)T (v − w) (cid:54) β

2 (cid:107)v − w(cid:107)2 + γ

hi(x)(i = 1, 2) satisﬁes the absolutely smooth prop-
erty.

For such GLM, if we consider optimizing the expected neg-
ative logarithmic probability −E(x,y)∼D log p(x, y; w),
once discarding unrelated terms to w, we obtain the new
:= E(x,y)∼D(cid:96)(w; x, y), where
population loss, L(w)
(cid:96)(w; x, y) = −yh1(xT w)+h2(xT w), exactly the form of
smooth generalized linear loss deﬁned in section 4. Hence
our SGLL is a natural loss deﬁned by GLM with additional
smoothness assumptions.

2.2. Omitted proofs

Lemma 4 (Lemma 8 in Main Body). Given any α > 0,
by setting k = c ln 1
α , p = (cid:100)k + eµ2(k; r)(cid:101), where c is a
(cid:13)
(cid:13)
ˆfp(x) − f (x)
(cid:54) α.
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)

constant, we have

Proof. As f, f (cid:48), · · · , f (k−1) are absolutely continuous
(cid:13)f (k)(cid:13)
over [−1, 1], and (cid:13)
(cid:54) µ1(k; r)µ2(k; r)k, according
(cid:13)T
to the results in (Trefethen, 2008), we have
(cid:13)f (k)(cid:13)
2 (cid:13)
(cid:13)T
πk(p − k)k

(cid:13)
ˆfp(x) − f (x)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)∞

(cid:54)

(cid:54) 2µ1(k; r)
πkek

(6)

It is easy to see there exists c > 0, such that the term (6) is
less than α with chosen k, hence the conclusion holds.

Lemma 5 (Lemma 9 in Main Body). For any γ > 0,
setting k = c ln 4r
γ , p = (cid:100)k + 2µ2(k; r)(cid:101), then algo-
rithm 7 outputs a (γ, β, σ) stochastic oracle, where σ =
˜O
.

σ0 + γ + p2p+1(4r)p+1

(cid:16)

(cid:17)

(cid:15)p+2

Proof. According to lemma 4, we know the approximation
error, | ˆm(w; x, y) − m(w; x, y)| (cid:54) γ
2r . For any ﬁxed
(x, y), from the construction of stochastic inexact gradi-
ent oracle, there is E[ ˜G(w; b)|x, y] = ˆG(w; x, y). Denote
ˆg(w) = E(x,y)∼D[ ˆG(w; x, y)], thus we have
2(cid:21)

2(cid:21)

E

(cid:20)(cid:13)
(cid:13)
˜G(w; b) − ˆg(w)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=E

(cid:20)(cid:13)
(cid:13)
˜G(w; b) − ˆG(w; x, y)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ E

(cid:20)(cid:13)
ˆG(w; x, y) − ˆg(w)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

2(cid:21)

For above two terms, combined with results given in lemma
6, we we obtain

(cid:20)(cid:13)
(cid:13)
˜G(w; b) − g(w)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2(cid:21)

(cid:54) ˜O

(cid:32)(cid:18) r(2rp)p+1

(cid:15)p+2

(cid:19)2(cid:33)

+ γ + σ0

E

.

As L(v) − L(w) − ˆg(w)T (v − w) = L(v) − L(w) −
g(w)T (v − w) + (g(w) − ˆg(w))T (v − w), and from the

Note the function value oracles in the stochastic oracle
deﬁnition (either Fγ,β,σ(·) or fγ,β,σ(·)) do not play any
role in the optimization algorithm, hence we can set it as
L(w) − γ

2 , though we do not know how to calculate.

Lemma 6. Based on above statements, we have

E

(cid:20)(cid:13)
(cid:13)
˜G(w; b) − ˆG(w; x, y)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2(cid:21)

(cid:54) ˜O

(cid:18) p4p+2(4r)2p+2
(cid:15)2p+4

(cid:19)

E

(cid:20)(cid:13)
(cid:13)
ˆG(w; x, y) − ˆg(w)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2(cid:21)

(cid:54) (γ + σ0)2

Proof. First, we calculate the variance of each tk,
var(tj) (cid:54) (cid:81)j(j+1)/2
i=j(j−1)/2+1(var(wT zi) + (E[wT zi])2) (cid:54)
˜O

)2j(cid:17)

(cid:16)

.

( p(p+1)
(cid:15)

Next, we upper bound the coefﬁcient ck (as it is the same
for c1k and c2k, hence we use ck for short). Note ck =
(cid:80)p
m=k ambmk, where am is the coefﬁcient of original
function represented by Chebyshev basis, bmk is the co-
efﬁcient of order k monomial in Chebyshev basis Tm(x),
where 0 (cid:54) k (cid:54) m. According to the formula of Tm(x)
given in (Qazi & Rahman, 2007) and well-known Stirling’s
approximation, after some translation, we have

(cid:18)√

m ·

(cid:20) (1 − θ)1−θ
θθ(1 − 2θ)1−2θ

(cid:21)m(cid:19)

O

|bmk| (cid:54) max
θ∈(0, 1
2 )
(cid:54)O (cid:0)√
m2m(cid:1)

Besides, from the absolutely smooth property of h(cid:48)
i(x)(i ∈
{1, 2}) and the convergence results in (Trefethen, 2008),
we have am (cid:54) O (cid:0) 1
m=k ambmk (cid:54)
m2
O (2p). Hence, there is

(cid:1), thus ck = (cid:80)p

var (cid:2)(c2k − c1kzy)tkrk+1(cid:3) (cid:54)r2k+2E

((c2k − c1kzy)tk)2(cid:105)
(cid:104)
(cid:19)

(cid:54)O

(cid:18) p4k+2(4r)2p+2
(cid:15)2k+2

As each (c2k−c1kzy)tkrk+1 is independent with each other
(for different k), which leads to

var

(c2k − c1kzy)tkrk+1

(cid:54) O

(cid:35)

(cid:18) p4p+2(4r)2p+2
(cid:15)2p+2

(cid:19)

(cid:34) p

(cid:88)

k=0

Dirksen, Sjoerd. Dimensionality reduction with subgaus-
sian matrices: a uniﬁed theory. Foundations of Compu-
tational Mathematics, 16(5):1367–1396, 2016.

Qazi, MA and Rahman, QI. Some coefﬁcient estimates for
polynomials on the unit interval. Serdica Mathematical
Journal, 33(4):449p–474p, 2007.

Rubinstein, B., Bartlett, P. L., Huang, L., and Taft, N.
Learning in a large function space: Privacy-preserving
mechanisms for svm learning. Journal of Privacy and
Conﬁdentiality, 4(1):4, 2012.

Trefethen, Lloyd N.

Is gauss quadrature better than

clenshaw–curtis? SIAM review, 50(1):67–87, 2008.

Tropp, Joel A et al. An introduction to matrix concentra-
tion inequalities. Foundations and Trends R(cid:13) in Machine
Learning, 8(1-2):1–230, 2015.

Vershynin, Roman. A note on sums of independent random
matrices after ahlswede-winter. Lecture notes, 2009.

Moreover, var(z0) (cid:54) O (cid:0) 1

(cid:1). Therefore,

(cid:15)2

References

Non-interactive Local DP Learning

E

(cid:20)(cid:13)
(cid:13)
˜G(w; b) − ˆG(w; x, y)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2(cid:21)

(cid:54) ˜O

(cid:18) p4p+2(4r)2p+2
(cid:15)2p+4

(cid:19)

For second inequality in the conclusion, there is

E

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

(cid:20)(cid:13)
ˆG(w; x, y) − ˆg(w)
(cid:13)
(cid:13)
(cid:20)(cid:13)
(cid:13)
ˆG(w; x, y) − G(w; x, y) + G(w; x, y) − g(w) + g(w) − ˆg(w)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:54)γ2 + σ2

0 + 2σ0γ = (γ + σ0)2

(cid:54)E

2(cid:21)

Proposition 1. f (x) = ln(1 + e−x) is absolutely smooth
4kπ3, µ2(k; r) = rk
with µ1(k; r) = r
e

√

Proof. For any r, k > 0, the absolutely continuous of
f (k)(rx) is obvious, now consider (cid:13)

(cid:13)f (k+1)(rx)(cid:13)

(cid:13)T :

(cid:13)
(cid:13)

(cid:13)f (k+1)(cid:13)
(cid:13)
(cid:13)T

dx

(cid:90) 1

=

√

|f (k+2)(rx)|
1 − x2
−1
(cid:13)
(cid:13)
(cid:54)π
(cid:13)f (k+2)(rx)
(cid:13)
(cid:13)
(cid:13)∞
(cid:54)πrk+2 (cid:13)
(cid:80)k+1
(cid:13)
(cid:13)

(cid:54)πrk+2

Ak+1,j−1

k+1
(cid:88)

j=1

j=1 (−1)k+jAk+1,j−1f j(1 − f )k+2−j(cid:13)
(cid:13)
(cid:13)∞

(cid:54)π(k + 1)!rk+2
(cid:54)

√

4π3rk+2(k + 1)k+3/2e−k−1

=r(cid:112)4π3(k + 1)

(cid:19)k+1

(cid:18) r(k + 1)
e

Theorem 2 (Theorem 6 in Main Body). For any α > 0,
set γ = α
γ , p = (cid:100)k + 2µ2(k; r)(cid:101), if n >
(cid:1)2cr ln(8r/α)+2 (cid:0) 1
, using al-

2 , k = c ln 4r
α )4r ln ln(8r/α) (cid:0) 4r
( 8r

O
gorithms 6,7,8, then we have L(wpriv) − L(w∗) (cid:54) α.

(cid:1)(cid:17)

α2(cid:15)2

(cid:16)

(cid:15)

Proof. According to lemma 10 in main body, with
a (γ, β, σ) stochastic oracle, SIGM algorithm con-
n + γ
verges with rate O
to have
(cid:17)

(cid:16) σ√

(cid:17)

(cid:17)

.

(cid:54) α, it sufﬁces if n > O

=

In order
(cid:16) p4p+2(4r)2p+2
α2(cid:15)2p+4
(cid:1)(cid:17)

α2(cid:15)2

, as σ =

(cid:1)2cr ln(8r/α)+2 (cid:0) 1

(cid:16) σ√
(cid:16)

O

O

n + γ
α )4r ln ln(8r/α) (cid:0) 4r
( 8r

(cid:15)

(cid:17)

(cid:16) p2p+1(4r)p+1
(cid:15)p+2

O
ble σ0, γ).

according to lemma 5 (ignoring negligi-

