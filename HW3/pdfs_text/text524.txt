Optimal Algorithms for Smooth and Strongly Convex
Distributed Optimization in Networks

SUPPLEMENTARY MATERIAL

Kevin Scaman 1 Francis Bach 2 S´ebastien Bubeck 3 Yin Tat Lee 3 Laurent Massouli´e 1

Abstract

This supplementary document contains complete
proofs of the theorems presented in the article,
as well as an extension of our algorithm to com-
posite problems particularly relevent for machine
learning applications.

box procedure one has, for all i ∈ {1, ..., n},

¯f A(θi,t)− ¯f A(θ∗) ≥

(cid:18) √
√

α
2

κg − 1
κg + 1

(cid:19)2(1+ t

1+dτ )

(cid:107)θi,0−θ∗(cid:107)2,

(2)

where κg = β/α.

1. Optimal Convergence Rates

1.1. Centralized Algorithms

Proof of Theorem 1. This proof relies on splitting the func-
tion used by Nesterov to prove oracle complexities for
strongly convex and smooth optimization (Nesterov, 2004;
Bubeck, 2015). Let β ≥ α > 0, G = (V, E) a graph and
A ⊂ V a set of nodes of G. For all d > 0, we denote as
Ac
d = {v ∈ V : d(A, v) ≥ d} the set of nodes at distance
: (cid:96)2 → R be
at least d from A, and let, for all i ∈ V, f A
i
the functions deﬁned as:

f A
i (θ) =






α

α

2n (cid:107)θ(cid:107)2
2n (cid:107)θ(cid:107)2
2n (cid:107)θ(cid:107)2

α

2

2 + β−α
8|A| (θ(cid:62)M1θ − 2θ1) if i ∈ A
2 + β−α
if i ∈ Ac
8|Ac
d
otherwise

d| θ(cid:62)M2θ

(1)

where M2

matrix with

: (cid:96)2 → (cid:96)2 is the inﬁnite block diagonal
(cid:16) 1 −1
1

on the diagonal, and M1 =

−1

(cid:17)

(cid:17)

(cid:16) 1

. First, note that, since 0 (cid:22) M1 + M2 (cid:22) 4I,

0
0 M2
¯f A = 1
is α-strongly convex and β-smooth.
n
Then, Theorem 1 is a direct consequence of the following
lemma:

i=1 f A
i

(cid:80)n

Lemma 1. If Ac

d (cid:54)= ∅, then for any t ≥ 0 and any black-

1MSR-INRIA Joint Center, Palaiseau, France 2INRIA, Ecole
Normale Sup´erieure, Paris, France 3Theory group, Microsoft Re-
search, Redmond, United States. Correspondence to: Kevin Sca-
man <kevin.scaman@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Proof. This lemma relies on the fact that most of the co-
ordinates of the vectors in the memory of any node will
remain equal to 0. More precisely, let ki,t = max{k ∈
N : ∃θ ∈ Mi,t s.t. θk (cid:54)= 0} be the last non-zero coordi-
nate of a vector in the memory of node i at time t. Then, un-
der any black-box procedure, we have, for any local com-
putation step,



ki,t + 1{ki,t ≡ 0 mod 2}
ki,t + 1{ki,t ≡ 1 mod 2}
ki,t



if i ∈ A
if i ∈ Ac
d
otherwise

ki,t+1 ≤

. (3)

Indeed, local gradients can only increase even dimensions
for nodes in A and odd dimensions for nodes in Ac
d. The
same holds for gradients of the dual functions, since these
have the same block structure as their convex conjugates.
Thus, in order to reach the third coordinate, algorithms
must ﬁrst perform one local computation in A, then d com-
munication steps in order for a node in Ac
d to have a non-
zero second coordinate, and ﬁnally, one local computation
in Ac
d. Accordingly, one must perform at least k local com-
putation steps and (k −1)d communication steps to achieve
ki,t ≥ k for at least one node i ∈ V, and thus, for any
k ∈ N∗,

∀t < 1 + (k − 1)(1 + dτ ), ki,t ≤ k − 1.

(4)

This implies in particular:

∀i ∈ V, ki,t ≤

+ 1 ≤

+ 1.

(5)

(cid:23)

(cid:22) t − 1
1 + dτ

t
1 + dτ

Furthermore, by deﬁnition of ki,t, one has θi,k = 0 for all
k > ki,t, and thus

(cid:107)θi,t − θ∗(cid:107)2

2 ≥

+∞
(cid:88)

θ∗
k

2.

k=ki,t+1

(6)

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

and, since ¯f A is α-strongly convex,

function F ∗(λ

W ) is

√

¯f A(θi,t) − ¯f A(θ∗) ≥

(cid:107)θi,t − θ∗(cid:107)2
2.

(7)

α
2

Finally, the solution of the global problem minθ
θ∗
k =
and (7) leads to the desired inequality.

(cid:16) √
√

β−
β+

√
√

(cid:17)k

α
α

. Combining this result with Eqs. (5), (6)

¯f A(θ) is

Using the previous lemma with d = ∆ the diameter of G
and A = {v} one of the pair of nodes at distance ∆ returns
the desired result.

1.2. Decentralized Algorithms
Proof of Theorem 2. Let γn = 1−cos( π
n )
n ) be a decreasing
1+cos( π
sequence of positive numbers. Since γ2 = 1 and limn γn =
0, there exists n ≥ 2 such that γn ≥ γ > γn+1. The cases
n = 2 and n ≥ 3 are treated separately. If n ≥ 3, let G
be the linear graph of size n ordered from node v1 to vn,
and weighted with wi,i+1 = 1 − a1{i = 1}. Then, if
A = {v1, ..., v(cid:100)n/32(cid:101)} and d = (1 − 1/16)n − 1, we have
|Ac

d| ≥ |A| and Lemma 1 implies:

¯f A(θi,t)− ¯f A(θ∗) ≥

(cid:18) √
√

nα
2

κg − 1
κg + 1

(cid:19)2(1+ t

1+dτ )

(8)
A simple calculation gives κl = 1 + (κg − 1) n
2|A| , and thus
κg ≥ κl/16. Finally, if we take Wa as the Laplacian of the
weighted graph G, a simple calculation gives that, if a = 0,
γ(Wa) = γn and, if a = 1, the network is disconnected
and γ(Wa) = 0. Thus, by continuity of the eigenvalues of a
matrix, there exists a value a ∈ [0, 1] such that γ(Wa) = γ.
2
Finally, by deﬁnition of n, one has γ > γn+1 ≥
(n+1)2 ,
γ when γ ≤ γ3 = 1
γ − 1) − 1 ≥ 1
and d ≥ 15
16 (
3 .

(cid:113) 2

√

5

For the case n = 2, we consider the totally connected
network of 3 nodes, reweight only the edge (v1, v3) by
a ∈ [0, 1], and let Wa be its Laplacian matrix. If a = 1,
then the network is totally connected and γ(Wa) = 1. If,
on the contrary, a = 0, then the network is a linear graph
and γ(Wa) = γ3. Thus, there exists a value a ∈ [0, 1] such
that γ(Wa) = γ, and applying Lemma 1 with A = {v1}
and d = 1 returns the desired result, since then κg ≥ 2κl/3
and d = 1 ≥ 1√

3γ .

2. Optimal Decentralized Algorithms

2.1. Single-Step Dual Accelerated Method

Proof of Theorem 3. Each step of the algorithm can be de-
composed in ﬁrst computing gradients, and then communi-
cating these gradients across all neighborhoods. Thus, one
step takes a time 1 + τ . Moreover, the Hessian of the dual

√
(

W ⊗ Id)∇2F ∗(λ

W )(

W ⊗ Id),

(9)

√

√

where ⊗ is the Kronecker product and Id is the identity ma-
trix of size d. Also, note that, in Alg.(2), the current values
W ⊗Id (i.e. the set of
xt and yt are always in the image of
matrices x such that x(cid:62)1 = 0). The condition number (in
W ⊗ Id) can thus be upper bounded by κl
γ ,
the image of
γ steps to achieve

and Nesterov’s acceleration requires

(cid:113) κl

√

√

any given precision (Bubeck, 2015).

2.2. Multi-Step Dual Accelerated Method

Proof of Theorem 4. First, since PK(W ) is a gossip ma-
trix, Theorem 3 implies the convergence of Alg.(3). In or-
2
der to simplify the analysis, we multiply W by
(1+γ)λ1(W ) ,
so that the resulting gossip matrix has a spectrum in [1 −
c−1
2 , 1 + c−1
2 ]. Applying Theorem 6.2 in (Auzinger, 2011)
with α = 1 − c−1
and γ = 0 implies that the
minimum

2 , β = 1 + c−1
2

min
p∈PK ,p(0)=0

x∈[1−c−1

max
2 ,1+c−1
2 ]

|p(t) − 1|

(10)

γ(PK(W )) ≥

1 − 2 cK
1
1+c2K
1 + 2 cK
1
1+c2K

1

1

=

(cid:18) 1 − cK
1
1 + cK
1

(cid:19)2

,

(11)

√
where c1 = 1−
√
1+

γ

γ , and taking K = (cid:98) 1√

γ (cid:99) implies

1
(cid:112)γ(PK(W ))

≤

1√

γ +1

γ +1

1 + c

1 − c

1
1√

1

≤ 2.

(12)

The time required to reach an ε > 0 precision us-

ing Alg.(3) is thus O

(cid:16)√

O

κl(1 + 1√

(cid:18)

(1 + Kτ )
(cid:17)
γ τ ) ln(1/ε)

.

(cid:113) κl

γ(PK (W )) ln(1/ε)

=

(cid:19)

3. Composite Problems for Machine Learning

When the local functions are of the form

fi(θ) = gi(Biθ) + c(cid:107)θ(cid:107)2,

(13)

where Bi ∈ Rmi×d and gi is smooth and has proximal
operator which is easy to compute (and hence also g∗
i ),
an additional Lagrange multiplier ν can be used to make
the Fenchel conjugate of gi appear in the dual optimiza-
tion problem. More speciﬁcally, from the primal problem

(cid:107)θi,0−θ∗(cid:107)2.

is attained by PK(x) = 1− TK (c2(1−x))
6.3 of (Auzinger, 2011) leads to

TK (c2)

. Finally, Corollary

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

The bound on the conditional number may be shown
through the two inequalities:

Q(ν, λ) (cid:54) 1
2µ

(cid:107)νi(cid:107)2 +

1
2c

n
(cid:88)

i=1

√

(cid:107)ρλ

W i(cid:107)2
2

Q(ν, λ) (cid:62) 1
2µ

(cid:107)νi(cid:107)2 +

1
1 + η

1
4c

n
(cid:88)

i=1

√

(cid:107)ρλ

W i(cid:107)2
2

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

+

1
2c

(cid:107)B(cid:62)

i νi(cid:107)2
2,

−

1
η

1
4c

n
(cid:88)

i=1

(cid:107)B(cid:62)

i νi(cid:107)2
2,

(cid:107)B(cid:62)

i νi + ρλ

W i(cid:107)2
2.

with η = M 2µ/c.

To maximize the dual problem, we can use (accelerated)
proximal gradient, with the updates:

References

Auzinger, W. Iterative Solution of Large Linear Systems.

Lecture notes, TU Wien, 2011.

Bi(B(cid:62)

i νi,t + ρλt

√

W i)(cid:13)
2
(cid:13)
2

Bubeck, S´ebastien. Convex optimization: Algorithms and
complexity. Foundations and Trends in Machine Learn-
ing, 8(3-4):231–357, 2015.

Nesterov, Yurii. Introductory lectures on convex optimiza-
tion : a basic course. Kluwer Academic Publishers,
2004.

of Eq. (12), one has, with ρ > 0 an arbitrary parameter:

inf
√
W =0

Θ

F (Θ)

=

√

inf

Θ

W =0, ∀i,xi=Biθi

= inf
Θ

sup
λ,ν

n
(cid:88)

(cid:110)

1
n

i=1
√
tr(λ(cid:62)Θ

W )

+

ρ
n

1
n

n
(cid:88)

i=1

gi(xi) + c(cid:107)θi(cid:107)2
2

i Biθi − g∗
ν(cid:62)

i (νi) + c(cid:107)θi(cid:107)2
2

(cid:111)

=

sup
Rmi , λ∈Rd×n

−

ν∈(cid:81)n

i=1

−

1
4cn

n
(cid:88)

i=1

g∗
i (νi)

1
n

n
(cid:88)

i=1
√

νi,t+1 =

g∗
i (ν)

inf
ν∈Rmi
1
(cid:13)
(cid:13)ν − νi,t +
2η

+

η
2c

λt+1 = λt − η

ρ
2cn

n
(cid:88)

i=1

(B(cid:62)

i νi,t + ρλt

W i)

√

√

W

(cid:62)
i .

We can rewrite all updates in terms of zt = λt
Rd×n, as

√

W ∈

νi,t+1 =

g∗
i (ν)

inf
ν∈Rmi
1
(cid:13)
(cid:13)ν − νi,t +
2η

+

η
2c

ρ
2cn

n
(cid:88)

i=1

zt+1 = zt − η

(B(cid:62)

i νi,t + ρzi)W (cid:62)
i .

Bi(B(cid:62)

i νi,t + ρzi,t)(cid:13)
2
(cid:13)
2

In order to compute the convergence rate of such an algo-
rithm, if we assume that:

• each gi is µ-smooth,

• the largest singular value of each Bi is less than M ,

then we simply need to compute the condition number of
the quadratic function

Q(ν, λ) =

(cid:107)νi(cid:107)2

2 +

(cid:107)B(cid:62)

i νi + ρλ

W i(cid:107)2
2.

1
2µ

n
(cid:88)

i=1

1
4c

n
(cid:88)

i=1

√

(cid:0) c
1
µ + M 2), it is lower
With the choice ρ2 =
λmax(W )
(cid:1) 4
bounded by (cid:0)1 + µ M 2
γ , which is a natural upper bound
on κl/γ. Thus this essentially leads to the same conver-
gence rate than the non-composite case with the Nesterov
and Chebyshev accelerations, i.e. (cid:112)κl/γ.

c

