Provable Alternating Gradient Descent for Non-negative Matrix Factorization
with Strong Correlations

Yuanzhi Li 1 Yingyu Liang 1

Abstract

Non-negative matrix factorization is a basic tool
for decomposing data into the feature and weight
matrices under non-negativity constraints, and in
practice is often solved in the alternating min-
imization framework. However,
it is unclear
whether such algorithms can recover the ground-
truth feature matrix when the weights for differ-
ent features are highly correlated, which is com-
mon in applications. This paper proposes a sim-
ple and natural alternating gradient descent based
algorithm, and shows that with a mild initializa-
tion it provably recovers the ground-truth in the
presence of strong correlations. In most interest-
ing cases, the correlation can be in the same order
as the highest possible. Our analysis also reveals
its several favorable features including robust-
ness to noise. We complement our theoretical
results with empirical studies on semi-synthetic
datasets, demonstrating its advantage over sev-
eral popular methods in recovering the ground-
truth.

1. Introduction

Non-negative matrix factorization (NMF) is an important
tool in data analysis and is widely used in image process-
ing, text mining, and hyperspectral imaging (e.g., (Lee &
Seung, 1997; Blei et al., 2003; Yang & Leskovec, 2013)).
Given a set of observations Y = {y(1), y(2), . . . , y(n)},
the goal of NMF is to ﬁnd a feature matrix A =
{a1, a2, . . . , aD} and a non-negative weight matrix X =
{x(1), x(2), . . . , x(n)} such that y(i) ≈ Ax(i) for any i, or
Y ≈ AX for short. The intuition of NMF is to write each
data point as a non-negative combination of the features.

Authors

listed in alphabetic order.
NJ,
Princeton,

1Princeton Uni-
versity,
to:
Yuanzhi Li <yuanzhil@cs.princeton.edu>, Yingyu Liang
<yingyul@cs.princeton.edu>.

Correspondence

USA.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

By doing so, one can avoid cancellation of different fea-
tures and improve interpretability by thinking of each x(i)
as a (unnormalized) probability distribution over the fea-
tures. It is also observed empirically that the non-negativity
constraint on the coefﬁcients can lead to better features and
improved downstream performance of the learned features.

Unlike the counterpart which factorizes Y ≈ AX with-
out assuming non-negativity of X, NMF is usually much
harder to solve, and can even by NP-hard in the worse
case (Arora et al., 2012b). This explains why, despite all
the practical success, NMF largely remains a mystery in
theory. Moreover, many of the theoretical results for NMF
were based on very technical tools such has algebraic ge-
ometry (e.g., (Arora et al., 2012b)) or tensor decomposi-
tion (e.g.
(Anandkumar et al., 2012)), which undermine
their applicability in practice. Arguably, the most widely
used algorithms for NMF use the alternative minimization
scheme: In each iteration, the algorithm alternatively keeps
A or X as ﬁxed and tries to minimize some distance be-
tween Y and AX. Algorithms in this framework, such
as multiplicative update (Lee & Seung, 2001) and alterna-
tive non-negative least square (Kim & Park, 2008), usually
perform well on real world data. However, alternative min-
imization algorithms are usually notoriously difﬁcult to an-
alyze. This problem is poorly understood, with only a few
provable guarantees known (Awasthi & Risteski, 2015; Li
et al., 2016). Most importantly, these results are only for
the case when the coordinates of the weights are from es-
sentially independent distributions, while in practice they
are known to be correlated, for example, in correlated topic
models (Blei & Lafferty, 2006). As far as we know, there
exists no rigorous analysis of practical algorithms for the
case with strong correlations.

In this paper, we provide a theoretical analysis of a nat-
ural algorithm AND (Alternative Non-negative gradient
Descent) that belongs to the practical framework, and show
that it probably recovers the ground-truth given a mild ini-
tialization. It works under general conditions on the feature
matrix and the weights, in particular, allowing strong cor-
It also has multiple favorable features that are
relations.
unique to its success. We further complement our theoreti-
cal analysis by experiments on semi-synthetic data, demon-

Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations

strating that the algorithm converges faster to the ground-
truth than several existing practical algorithms, and provid-
ing positive support for some of the unique features of our
algorithm. Our contributions are detailed below.

1.1. Contributions

In this paper, we assume a generative model of the data
points, given the ground-truth feature matrix A∗. In each
round, we are given y = A∗x,1 where x is sampled i.i.d.
from some unknown distribution µ and the goal is to re-
cover the ground-truth feature matrix A∗. We give an al-
gorithm named AND that starts from a mild initialization
matrix and provably converges to A∗ in polynomial time.
We also justify the convergence through a sequence of ex-
periments. Our algorithm has the following favorable char-
acteristics.

1.1.1. SIMPLE GRADIENT DESCENT ALGORITHM

The algorithm AND runs in stages and keeps a working
matrix A(t) in each stage. At the t-th iteration in a stage,
after getting one sample y, it performs the following:

(Decode)

z = φα

(cid:16)

(A(0))†y

(cid:17)

,
(cid:16)

(Update) A(t+1) = A(t) + η

yz(cid:62) − A(t)zz(cid:62)(cid:17)

,

where α is a threshold parameter,

φα(x) =

(cid:26) x if x ≥ α,
otherwise,

0

(A(0))† is the Moore-Penrose pesudo-inverse of A(0), and
η is the update step size. The decode step aims at recov-
ering the corresponding weight for the data point, and the
update step uses the decoded weight to update the feature
matrix. The ﬁnal working matrix at one stage will be used
as the A(0) in the next stage. See Algorithm 1 for the de-
tails.

At a high level, our update step to the feature matrix can be
thought of as a gradient descent version of alternative non-
negative least square (Kim & Park, 2008), which at each
iteration alternatively minimizes L(A, Z) = (cid:107)Y − AZ(cid:107)2
F
by ﬁxing A or Z. Our algorithm, instead of perform-
ing an complete minimization, performs only a stochas-
tic gradient descent step on the feature matrix. To see
this, consider one data point y and consider minimizing
L(A, z) = (cid:107)y − Az(cid:107)2
F with z ﬁxed. Then the gradient
of A is just −∇L(A) = (y − Az)z(cid:62), which is exactly the
update of our feature matrix in each iteration.

As to the decode step, when α = 0, our decoding can be
regarded as a one-shot approach minimizing (cid:107)Y − AZ(cid:107)2
F

1We also consider the noisy case; see 1.1.5.

restricted to Z ≥ 0. Indeed, if for example projected gra-
dient descent is used to minimize (cid:107)Y − AZ(cid:107)2
F , then the
projection step is exactly applying φα to Z with α = 0. A
key ingredient of our algorithm is choosing α to be larger
than zero and then decreasing it, which allows us to outper-
form the standard algorithms.

Perhaps worth noting, our decoding only uses A(0). Ide-
ally, we would like to use (A(t))† as the decoding matrix
in each iteration. However, such decoding method requires
computing the pseudo-inverse of A(t) at every step, which
is extremely slow.
Instead, we divide the algorithm into
stages and in each stage, we only use the starting matrix
in the decoding, thus the pseudo-inverse only needs to be
computed once per stage and can be used across all itera-
tions inside. We can show that our algorithm converges in
polylogarithmic many stages, thus gives us to a much bet-
ter running time. These are made clear when we formally
present the algorithm in Section 4 and the theorems in Sec-
tion 5 and 6.

1.1.2. HANDLING STRONG CORRELATIONS

The most notable property of AND is that it can provably
deal with highly correlated distribution µ on the weight x,
meaning that the coordinates of x can have very strong
correlations with each other. This is important since such
correlated x naturally shows up in practice. For example,
when a document contains the topic “machine learning”, it
is more likely to contain the topic “computer science” than
“geography” (Blei & Lafferty, 2006).

Most of the previous theoretical approaches for analyz-
ing alternating between decoding and encoding, such
as (Awasthi & Risteski, 2015; Li et al., 2016; Arora
et al., 2015), require the coordinates of x to be pairwise-
independent, or almost pairwise-independent (meaning
Eµ[xixj] ≈ Eµ[xi]Eµ[xj]). In this paper, we show that al-
gorithm AND can recover A∗ even when the coordinates
are highly correlated. As one implication of our result,
when the sparsity of x is O(1) and each entry of x is in
{0, 1}, AND can recover A∗ even if each Eµ[xixj] =
Ω(min{Eµ[xi], Eµ[xj]}), matching (up to constant) the
highest correlation possible. Moreover, we do not assume
any prior knowledge about the distribution µ, and the result
also extends to general sparsities as well.

1.1.3. PSEUDO-INVERSE DECODING

One of the feature of our algorithm is to use Moore-Penrose
pesudo-inverse in decoding.
Inverse decoding was also
used in (Li et al., 2016; Arora et al., 2015; 2016). How-
ever, their algorithms require carefully ﬁnding an inverse
such that certain norm is minimized, which is not as efﬁ-
cient as the vanilla Moore-Penrose pesudo-inverse. It was
also observed in (Arora et al., 2016) that Moore-Penrose

Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations

pesudo-inverse works equally well in practice, but the ex-
periment was done only when A = A∗. In this paper, we
show that Moore-Penrose pesudo-inverse also works well
when A (cid:54)= A∗, both theoretically and empirically.

1.1.4. THRESHOLDING AT DIFFERENT α

Thresholding at a value α > 0 is a common trick used in
many algorithms. However, many of them still only con-
sider a ﬁxed α throughout the entire algorithm. Our contri-
bution is a new method of thresholding that ﬁrst sets α to
be high, and gradually decreases α as the algorithm goes.
Our analysis naturally provides the explicit rate at which
we decrease α, and shows that our algorithm, following this
scheme, can provably converge to the ground-truth A∗ in
polynomial time. Moreover, we also provide experimental
support for these choices.

1.1.5. ROBUSTNESS TO NOISE

We further show that the algorithm is robust to noise. In
particular, we consider the model y = A∗x + ζ, where ζ
is the noise. The algorithm can tolerate a general family of
noise with bounded moments; we present in the main body
the result for a simpliﬁed case with Gaussian noise and pro-
vide the general result in the appendix. The algorithm can
recover the ground-truth matrix up to a small blow-up fac-
tor times the noise level in each example, when the ground-
truth has a good condition number. This robustness is also
supported by our experiments.

2. Related Work

Practical algorithms. Non-negative matrix factorization
has a rich empirical history, starting with the practical al-
gorithms of (Lee & Seung, 1997; 1999; 2001). It has been
widely used in applications and there exist various methods
for NMF, e.g., (Kim & Park, 2008; Lee & Seung, 2001; Ci-
chocki et al., 2007; Ding et al., 2013; 2014). However, they
do not have provable recovery guarantees.

Theoretical analysis. For theoretical analysis, (Arora
et al., 2012b) provided a ﬁxed-parameter tractable algo-
rithm for NMF using algebraic equations. They also pro-
vided matching hardness results: namely they show there is
no algorithm running in time (mW )o(D) unless there is a
sub-exponential running time algorithm for 3-SAT. (Arora
et al., 2012b) also studied NMF under separability assump-
tions about the features, and (Bhattacharyya et al., 2016)
studied NMF under related assumptions. The most re-
lated work is (Li et al., 2016), which analyzed an alter-
nating minimization type algorithm. However, the result
only holds with strong assumptions about the distribution
of the weight x, in particular, with the assumption that the
coordinates of x are independent.

Topic modeling. Topic modeling is a popular generative
model for text data (Blei et al., 2003; Blei, 2012). Usu-
ally, the model results in NMF type optimization problems
with (cid:107)x(cid:107)1 = 1, and a popular heuristic is variational in-
ference, which can be regarded as alternating minimiza-
tion in KL-divergence. Recently, there is a line of theo-
retical work analyzing tensor decomposition (Arora et al.,
2012a; 2013; Anandkumar et al., 2013) or combinatorial
methods (Awasthi & Risteski, 2015). These either need
strong structural assumptions on the word-topic matrix A∗,
or need to know the distribution of the weight x, which is
usually infeasible in applications.

3. Problem and Deﬁnitions

We use (cid:107)M(cid:107)2 to denote the 2-norm of a matrix M. (cid:107)x(cid:107)1
is the 1-norm of a vector x. We use [M]i to denote the i-
th row and [M]i to denote the i-th column of a matrix M.
σmax(M)(σmin(M)) stands for the maximum (minimal)
singular value of M, respectively. We consider a generative
model for non-negative matrix factorization, where the data
y is generated from2

y = A∗x, A∗ ∈ RW ×D

where A∗ is the ground-truth feature matrix, and x is a non-
negative random vector drawn from an unknown distribu-
tion µ. The goal is to recover the ground-truth A∗ from
i.i.d. samples of the observation y.

Since the general non-negative matrix factorization is NP-
hard (Arora et al., 2012b), some assumptions on the distri-
bution of x need to be made. In this paper, we would like to
allow distributions as general as possible, especially those
with strong correlations. Therefore, we introduce the fol-
lowing notion called (r, k, m, λ)-general correlation condi-
tions (GCC) for the distribution of x.

Deﬁnition 1 (General Correlation Conditions, GCC). Let
∆ := E[xx(cid:62)] denote the second moment matrix.

1. (cid:107)x(cid:107)1 ≤ r and xi ∈ [0, 1], ∀i ∈ [D].

2. ∆i,i ≤ 2k

D , ∀i ∈ [D].

3. ∆i,j ≤ m

D2 , ∀i (cid:54)= j ∈ [D].

4. ∆ (cid:23) k

D λI.

The ﬁrst condition regularizes the sparsity of x.3 The sec-
ond condition regularizes each coordinate of xi so that
there is no xi being large too often. The third condition

2Section 6.2 considers the noisy case.
3Throughout this paper, the sparsity of x refers to the (cid:96)1 norm,
which is much weaker than the (cid:96)0 norm (the support sparsity). For
example, in LDA, the (cid:96)1 norm of x is always 1.

Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations

regularizes the maximum pairwise correlation between xi
and xj. The fourth condition always holds for λ = 0 since
E[xx(cid:62)] is a PSD matrix. Later we will assume this condi-
tion holds for some λ > 0 to avoid degenerate cases. Note
that we put the weight k/D before λ such that λ deﬁned
in this way will be a positive constant in many interesting
examples discussed below.

To get a sense of what are the ranges of k, m, and λ given
sparsity r, we consider the following most commonly stud-
ied non-negative random variables.

Proposition 1 (Examples of GCC).

1. If x is chosen uniformly over s-sparse random vectors
with {0, 1} entries, then k = r = s, m = s2 and
λ = 1 − 1
s .

2. If x is uniformly chosen from Dirichlet distribution
D , then r = k = 1 and m = 1

sD

with parameter αi = s
with λ = 1 − 1
s .

For these examples, the result in this paper shows that we
can recover A∗ for aforementioned random variables x as
long as s = O(D1/6). In general, there is a wide range
of parameters (r, k, m, λ) such that learning A∗ is doable
with polynomially many samples of y and in polynomial
time.

However, just the GCC condition is not enough for recov-
ering A∗. We will also need a mild initialization.

Deﬁnition 2 ((cid:96)-initialization). The initial matrix A0 satis-
ﬁes for some (cid:96) ∈ [0, 1),

1. A0 = A∗(Σ + E), for some diagonal matrix Σ and

off-diagonal matrix E.

2. (cid:107)E(cid:107)2 ≤ (cid:96), (cid:107)Σ − I(cid:107)2 ≤ 1
4 .

The condition means that the initialization is not too far
away from the ground-truth A∗. For any i ∈ [D], the i-
th column [A0]i = Σi,i[A∗]i + (cid:80)
j(cid:54)=i Ej,i[A∗]j. So the
condition means that each feature [A0]i has a large frac-
tion of the ground-truth feature [A∗]i and a small fraction
of the other features. Σ can be regarded as the magnitude
of the component from the ground-truth in the initializa-
tion, while E can be regarded as the magnitude of the error
terms. In particular, when Σ = I and E = 0, we have
A0 = A∗. The initialization allows Σ to be a constant
away from I, and the error term E to be (cid:96) (in our theorems
(cid:96) can be as large as a constant).

In practice, such an initialization is typically achieved by
setting the columns of A0 to reasonable “pure” data points
that contain one major feature and a small fraction of some
others (e.g. (lda, 2016; Awasthi & Risteski, 2015)).

for t = 0, 1, . . . , T do

Algorithm 1 Alternating Non-negative gradient Descent
(AND)
Input: Threshold values {α0, α1, . . . , αs}, T , A0
1: A(0) ← A0
2: for j = 0, 1, . . . , s do
3:
4:
5:
6:
7:
8: A(0) ← A(T +1)
9: end for
Output: A ← A(T +1)

On getting sample y(t), do:
(cid:0)(A(0))†y(t)(cid:1)
z(t) ← φαj
A(t+1) ← A(t) + η (cid:0)y(t) − A(t)z(t)(cid:1) (z(t))(cid:62)

end for

4. Algorithm

The algorithm is formally describe in Algorithm 1. It runs
in s stages, and in the j-th stage, uses the same threshold
αj and the same matrix A(0) for decoding, where A(0) is
either the input initialization matrix or the working matrix
obtained at the end of the last stage. Each stage consists
of T iterations, and each iteration decodes one data point
and uses the decoded result to update the working matrix.
It can use a batch of data points instead of one data point,
and our analysis still holds.

By running in stages, we save most of the cost of comput-
ing (A(0))†, as our results show that only polylogarithmic
stages are needed. For the simple case where x ∈ {0, 1}D,
the algorithm can use the same threshold value α = 1/4
for all stages (see Theorem 1), while for the general case,
it needs decreasing threshold values across the stages (see
Theorem 4). Our analysis provides the hint for setting the
threshold; see the discussion after Theorem 4, and Sec-
tion 7 for how to set the threshold in practice.

5. Result for A Simpliﬁed Case

In this section, we consider the following simpliﬁed case:

y = A∗x, x ∈ {0, 1}D.

(5.1)

That is, the weight coordinates xi’s are binary.
Theorem 1 (Main, binary). For the generative model (5.1),
there exists (cid:96) = Ω(1) such that for every (r, k, m, λ)-
GCC x and every (cid:15) > 0, Algorithm AND with T =
poly(D, 1
(cid:15) ) , {αi}s
i=1 for s =
polylog(D, 1
(cid:15) ) and an (cid:96) initialization matrix A0, outputs a
matrix A such that there exists a diagonal matrix Σ (cid:23) 1
2 I
with (cid:107)A − A∗Σ(cid:107)2 ≤ (cid:15) using poly(D, 1
(cid:15) ) samples and it-
erations, as long as

i=1 = { 1

1
poly(D, 1

(cid:15) ), η =

4 }s

m = O

(cid:18) kDλ4
r5

(cid:19)

.

Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations

Therefore, our algorithm recovers the ground-truth A∗ up
to scaling. The scaling in unavoidable since there is no as-
sumption on A∗, so we cannot, for example, distinguish
A∗ from 2A∗. Indeed, if we in addition assume each col-
umn of A∗ has norm 1 as typical in applications, then
we can recover A∗ directly. In particular, by normalizing
each column of A to have norm 1, we can guarantee that
(cid:107)A − A∗(cid:107)2 ≤ O((cid:15)).

In many interesting applications (for example, those in
Proposition 1), k, r, λ are constants. The theorem implies
that the algorithm can recover A∗ even when m = O(D).
In this case, Eµ[xixj] can be as large as O(1/D), the same
order as min{Eµ[xi], Eµ[xj]}, which is the highest possi-
ble correlation.

5.1. Intuition

The intuition comes from assuming that we have the “cor-
rect decoding”, that is, suppose magically for every y(t),
our decoding z(t) = φαj (A†y(t)) = x(t). Here and in this
subsection, A is a shorthand for A(0). The gradient de-
scent is then A(t+1) = A(t) + η(y(t) − A(t)x(t))(x(t))(cid:62).
Subtracting A∗ on both side, we will get

(A(t+1) − A∗) = (A(t) − A∗)(I − ηx(t)(x(t))(cid:62))

Since x(t)(x(t))(cid:62) is positive semideﬁnite, as long as
E[x(t)(x(t))(cid:62)] (cid:31) 0 and η is sufﬁciently small, A(t) will
converge to A∗ eventually.

However, this simple argument does not work when A (cid:54)=
A∗ and thus we do not have the correct decoding. For ex-
ample, if we just let the decoding be ˜z(t) = A†y(t), we will
have y(t) − A˜z(t) = y(t) − A†Ay(t) = (I − A†A)A∗x(t).
Thus, using this decoding, the algorithm can never make
any progress once A and A∗ are in the same subspace.

The most important piece of our proof is to show that af-
ter thresholding, z(t) = φα(A†y(t)) is much closer to x(t)
than ˜z(t). Since A and A∗ are in the same subspace, in-
spired by (Li et al., 2016) we can write A∗ as A(Σ + E)
for a diagonal matrix Σ and an off-diagonal matrix E, and
thus the decoding becomes z(t) = φα(Σx(t) + Ex(t)).
Let us focus on one coordinate of z(t), that is, z(t)
i =
φα(Σi,ix(t)
i + Eix(t)), where Ei is the i-th row of Ei. The
term Σi,ix(t)
is a nice term since it is just a rescaling of
i
x(t)
, while Eix(t) mixes different coordinates of x(t). For
i
simplicity, we just assume for now that x(t)
i ∈ {0, 1} and
Σi,i = 1. In our proof, we will show that the threshold will
remove a large fraction of Eix(t) when x(t)
i = 0, and keep a
i when x(t)
large fraction of Σi,ix(t)
i = 1. Thus, our decod-
ing is much more accurate than without thresholding. To
show this, we maintain a crucial property that for our de-
coding matrix, we always have (cid:107)Ei(cid:107)2 = O(1). Assuming

this, we ﬁrst consider two extreme cases of Ei.

1. Ultra dense: all coordinates of Ei are in the order of
1√
. Since the sparsity of x(t) is r, as long as r =
d
√
d)α, Eix(t) will not pass α and thus z(t)
o(
decoded to zero when x(t)

i will be

i = 0.

2. Ultra sparse: Ei only has few coordinate equal to Ω(1)
and the rest are zero. Unless x(t) has those exact coor-
dinates equal to 1 (which happens not so often), then
i will still be zero when x(t)
z(t)

i = 0.

Of course, the real Ei can be anywhere in between these
two extremes, and thus we need more delicate decoding
lemmas, as shown in the complete proof.

Furthermore, more complication arises when each x(t)
is
i
not just in {0, 1} but can take fractional values. To han-
dle this case, we will set our threshold α to be large at the
beginning and then keep shrinking after each stage. The in-
tuition here is that we ﬁrst decode the coordinates that we
are most conﬁdent in, so we do not decode z(t)
to be non-
zero when x(t)
i = 0. Thus, we will still be able to remove a
large fraction of error caused by Eix(t). However, by set-
ting the threshold α so high, we may introduce more errors
to the nice term Σi,ix(t)
as well, since Σi,ix(t)
i might not
i
be larger than α when x(t)
(cid:54)= 0. Our main contribution is to
i
show that there is a nice trade-off between the errors in Ei
terms and those in Σi,i terms such that as we gradually de-
creases α, the algorithm can converge to the ground-truth.

i

5.2. Proof Sketch

For simplicity, we only focus on one stage and the expected
update. The expected update of A(t) is given by

A(t+1) = A(t) + η(E[yz(cid:62)] − A(t)E[zz(cid:62)]).

Let us write A(0) = A∗(Σ0 + E0) where Σ0 is diagonal
and E0 is off-diagonal. Then the decoding is given by

z = φα((A(0))†y) = φα((Σ0 + E0)−1x).

Let Σ, E be the diagonal part and the off-diagonal part of
(Σ0 + E0)−1.

The key lemma for decoding says that under suitable con-
ditions, z will be close to Σx in the following sense.
Lemma 2 (Decoding, informal). Suppose E is small and
Σ ≈ I. Then with a proper threshold value α, we have

E[Σxx(cid:62)] ≈ E[zx(cid:62)], E[Σxz(cid:62)] ≈ E[zz(cid:62)].

Now, let us write A(t) = A∗(Σt + Et). Then applying the
above decoding lemma, the expected update of Σt + Et is
Σt+1 +Et+1 = (Σt +Et)(I−Σ∆Σ)+Σ−1(Σ∆Σ)+Rt

Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations

where ∆ = E[xx(cid:62)] and Rt is a small error term.

Our second key lemma is about this update.

Lemma 3 (Update, informal). Suppose the update rule is

Σt+1 + Et+1 = (Σt + Et)(1 − ηΛ) + ηQΛ + ηRt

for some PSD matrix Λ and (cid:107)Rt(cid:107)2 ≤ C (cid:48)(cid:48). Then

(cid:107)Σt + Et − Q(cid:107)2 ≤ (cid:107)Σ0 + E0 − Q(cid:107)2(1 − ηλmin(Λ))t

+

C (cid:48)(cid:48)
λmin(Λ)

.

Applying this on our update rule with Q = Σ−1 and
Λ = Σ∆Σ, we know that when the error term is sufﬁ-
ciently small, we can make progress on (cid:107)Σt +Et −Σ−1(cid:107)2.
Furthermore, by using the fact that Σ0 ≈ I and E0 is small,
and the fact that Σ is the diagonal part of (Σ0 + E0)−1, we
can show that after sufﬁciently many iterations, (cid:107)Σt − I(cid:107)2
blows up slightly, while (cid:107)Et(cid:107)2 is reduced signiﬁcantly. Re-
peating this for multiple stages completes the proof.

We note that most technical details are hidden, especially
for the proofs of the decoding lemma, which need to show
that the error term Rt is small. This crucially relies on
the choice of α, and relies on bounding the effect of the
correlation. These then give the setting of α and the bound
on the parameter m in the ﬁnal theorem.

6. More General Results

6.1. Result for General x

This subsection considers the general case where x ∈
[0, 1]D. Then the GCC condition is not enough for recov-
ery, even for k, r, m = O(1) and λ = Ω(1). For example,
GCC does not rule out the case that x is drawn uniformly
over (r − 1)-sparse random vectors with { 1
D , 1} entries,
when one cannot recover even a reasonable approximation
of A∗ since a common vector 1
i[A∗]i shows up in all
D
the samples. This example shows that the difﬁculty arises
if each xi constantly shows up with a small value. To avoid
this, a general and natural way is to assume that each xi,
once being non-zero, has to take a large value with sufﬁ-
cient probability. This is formalized as follows.

(cid:80)

Deﬁnition 3 (Decay condition). A distribution of x satisﬁes
the order-q decay condition for some constant q ≥ 1, if for
all i ∈ [D], xi satisﬁes that for every α > 0,

Pr[xi ≤ α | xi (cid:54)= 0] ≤ αq.

When q = 1, each xi, once being non-zero, is uniformly
distributed in the interval [0, 1]. When q gets larger, each
xi, once being non-zero, will be more likely to take larger

values. We will show that our algorithm has a better guar-
antee for larger q. In the extreme case when q = ∞, xi will
only take {0, 1} values, which reduces to the binary case.

In this paper, we show that this simple decay condition,
combined with the GCC conditions and an initialization
with constant error, is sufﬁcient for recovering A∗.
Theorem 4 (Main). There exists (cid:96) = Ω(1) such that for
every (r, k, m, λ)-GCC x satisfying the order-q condition,
every (cid:15) > 0, there exists T, η and a sequence of {αi} 4
such that Algorithm AND, with (cid:96)-initialization matrix A0,
outputs a matrix A such that there exists a diagonal matrix
Σ (cid:23) 1
(cid:15) ) samples
and iterations, as long as

2 I with (cid:107)A − A∗Σ(cid:107)2 ≤ (cid:15) with poly(D, 1

m = O

(cid:32)

q λ4+ 4

q

kD1− 1
r5+ 6

q+1

(cid:33)

.

As mentioned, in many interesting applications, k = r =
λ = Θ(1), where our algorithm can recover A∗ as long as
m = O(D1− 1
q+1 ). This means Eµ[xixj] = O(D−1− 1
q+1 ),
a factor of D− 1
q+1 away from the highest possible correla-
tion min{Eµ[xi], Eµ[xj]} = O(1/D). Then, the larger q,
the higher correlation it can tolerate. As q goes to inﬁnity,
we recover the result for the case x ∈ {0, 1}D, allowing
the highest order correlation.

The analysis also shows that the decoding threshold should

(cid:16) λ(cid:107)E0(cid:107)2
r

(cid:17) 2

q+1

be α =
where E0 is the error matrix at the
beginning of the stage. Since the error decreases exponen-
tially with stages, this suggests to decrease α exponentially
with stages. This is crucial for AND to recover the ground-
truth; see Section 7 for the experimental results.

6.2. Robustness to Noise

We now consider the case when the data is generated from
y = A∗x + ζ, where ζ is the noise. For the sake of demon-
stration, we will just focus on the case when xi ∈ {0, 1}
and ζ is random Gaussian noise ζ ∼ γN (cid:0)0, 1
W I(cid:1). 5 A
more general theorem can be found in the appendix.

Deﬁnition 4 (((cid:96), ρ)-initialization). The initial matrix A0
satisﬁes for some (cid:96), ρ ∈ [0, 1),

1. A0 = A∗(Σ + E) + N, for some diagonal matrix Σ

and off-diagonal matrix E.

2. (cid:107)E(cid:107)2 ≤ (cid:96), (cid:107)Σ − I(cid:107)2 ≤ 1

4 , (cid:107)N(cid:107)2 ≤ ρ.

Theorem 5 (Noise, binary). Suppose each xi ∈ {0, 1}.
There exists (cid:96) = Ω(1) such that for every (r, k, m, λ)-GCC
x, every (cid:15) > 0, Algorithm AND with T = poly(D, 1
(cid:15) ), η =

4In fact, we will make the choice explicit in the proof.
5we make this scaling so (cid:107)ζ(cid:107)2 ≈ γ.

Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations

4 }4

i=1 = { 1

(cid:15) ) , {αi}s

1
i=1 and an ((cid:96), ρ)-initialization
poly(D, 1
A0 for ρ = O(σmin(A∗)), outputs A such that there exists
a diagonal matrix Σ (cid:23) 1
2 I with
(cid:18)

(cid:19)

(cid:107)A − A∗Σ(cid:107)2 ≤ O

(cid:15) + r

σmax(A∗)
σmin(A∗)λ

γ

using poly(D, 1

(cid:15) ) iterations, as long as m = O

(cid:16) kDλ4
r5

(cid:17)

.

The theorem implies that the algorithm can recover the
ground-truth up to r σmax(A∗)
σmin(A∗)λ times γ, the noise level in
each sample. Although stated here for Gaussian noise for
simplicity, the analysis applies to a much larger class of
noises, including adversarial ones. In particular, we only
need to the noise ζ have sufﬁciently bounded (cid:107)E[ζζ (cid:62)](cid:107)2;
see the appendix for the details. For the special case of
Gaussian noise, by exploiting its properties, it is possible
to improve the error term with a more careful calculation,
though not done here.

7. Experiments

To demonstrate the advantage of AND, we complement the
theoretical analysis with empirical study on semi-synthetic
datasets, where we have ground-truth feature matrices and
can thus verify the convergence. We then provide support
for the beneﬁt of using decreasing thresholds, and test its
robustness to noise.
In the appendix, we further test its
robust to initialization and sparsity of x, and provide qual-
itative results in some real world applications. 6

Setup. Our work focuses on convergence of the solu-
tion to the ground-truth feature matrix. However, real-
world datasets in general do not have ground-truth. So we
construct semi-synthetic datasets in topic modeling: ﬁrst
take the word-topic matrix learned by some topic model-
ing method as the ground-truth A∗, and then draw x from
some speciﬁc distribution µ. For fair comparison, we use
one not learned by any algorithm evaluated here. In partic-
ular, we used the matrix with 100 topics computed by the
algorithm in (Arora et al., 2013) on the NIPS papers dataset
(about 1500 documents, average length about 1000). Based
on this we build two semi-synthetic datasets:

1. DIR. Construct a 100 × 5000 matrix X, whose
columns are from a Dirichlet distribution with pa-
rameters (0.05, 0.05, . . . , 0.05). Then the dataset is
Y = A∗X.

2. CTM. The matrix X is of the same size as above,
while each column is drawn from the logistic normal
prior in the correlated topic model (Blei & Lafferty,
2006). This leads to a dataset with strong correlations.

Note that the word-topic matrix is non-negative. While
some competitor algorithms require a non-negative feature
matrix, AND does not need such a condition. To demon-
strate this, we generate the following synthetic data:

3. NEG. The entries of the matrix A∗ are i.i.d. samples
from the uniform distribution on [−0.5, 0.5). The ma-
trix X is the same as in CTM.

Finally, the following dataset is for testing the robustness
of AND to the noise:

4. NOISE. A∗ and X are the same as in CTM, but Y =
A∗X + N where N is the noise matrix with columns
drawn from γN (cid:0)0, 1

W I(cid:1) with the noise level γ.

Competitors. We compare the algorithm AND to the fol-
lowing popular methods: Alternating Non-negative Least
Square (ANLS (Kim & Park, 2008)), multiplicative update
(MU (Lee & Seung, 2001)), LDA (online version (Hoffman
et al., 2010)),7 and Hierarchical Alternating Least Square
(HALS (Cichocki et al., 2007)).

Evaluation criterion. Given the output matrix A and the
ground truth matrix A∗, the correlation error of the i-th
column is given by

εi(A, A∗) = min

{(cid:107)[A∗]i − σ[A]j(cid:107)2}.

j∈[D],σ∈R

Thus, the error measures how well the i-th column of A∗
is covered by the best column of A up to scaling. We ﬁnd
the best column since in some competitor algorithms, the
columns of the solution A may only correspond to a per-
mutation of the columns of A∗.8

We also deﬁne the total correlation error as

ε(A, A∗) =

εi(A, A∗).

D
(cid:88)

i=1

We report the total correlation error in all the experiments.

Initialization.
In all the experiments, the initialization
matrix A0 is set to A0 = A∗(I + U) where I is the
identity matrix and U is a matrix whose entries are i.i.d.
samples from the uniform distribution on [−0.05, 0.05).
Note that this is a very weak initialization, since [A0]i =
(1 + Ui,i)[A∗]i + (cid:80)
j(cid:54)=i Uj,i[A∗]j and the magnitude of
the noise component (cid:80)
j(cid:54)=i Uj,i[A∗]j can be larger than
the signal part (1 + Ui,i)[A∗]i.

7We use the implementation in the sklearn package (http:

//scikit-learn.org/)

6The

code
public
PrincetonML/AND4NMF.

is

on https://github.com/

8In the Algorithm AND, the columns of A correspond to the

columns of A∗ without permutation.

Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations

(a) on DIR dataset

(b) on CTM dataset

(c) on NEG dataset

Figure 1. The performance of different algorithms on the three datasets. The x-axis is the running time (in seconds), the y-axis is the
logarithm of the total correlation error.

(a) different thresholds on DIR

(b) different thresholds on CTM

(c) robustness to noise

Figure 2. The performance of the algorithm AND with different thresholding schemes, and its robustness to noise. The x-axis is the
running time (in seconds), the y-axis is the logarithm of the total correlation error. (a)(b) Using different thresholding schemes on the
DIR/CTM dataset. “Decreasing thresold” refers to the scheme used in the original AND, “Constant threshold c” refers to using the
threshold value c throughout all iterations. (c) The performance in the presence of noises of various levels.

Hyperparameters and Implementations. For most ex-
periments of AND, we used T = 50 iterations for each
stage, and thresholds αi = 0.1/(1.1)i−1. For experiments
on the robustness to noise, we found T = 100 leads to
better performance. Furthermore, for all the experiments,
instead of using one data point at each step, we used the
whole dataset for update.

7.1. Convergence to the Ground-Truth

Figure 1 shows the convergence rate of the algorithms on
the three datasets. AND converges in linear rate on all three
datasets (note that the y-axis is in log-scale). HALS con-
verges on the DIR and CTM datasets, but the convergence
is in slower rates. Also, on CTM, the error oscillates. Fur-
thermore, it doesn’t converge on NEG where the ground-
truth matrix has negative entries. ANLS converges on DIR
and CTM at a very slow speed due to the non-negative least
square computation in each iteration. 9 All the other algo-

rithms do not converge to the ground-truth, suggesting that
they do not have recovery guarantees.

7.2. The Threshold Schemes

Figure 2(a) shows the results of using different thresholding
schemes on DIR, while Figure 2(b) shows that those on
CTM. When using a constant threshold for all iterations,
the error only decreases for the ﬁrst few steps and then stop
decreasing. This aligns with our analysis and is in strong
contrast to the case with decreasing thresholds.

7.3. Robustness to Noise

Figure 2(c) shows the performance of AND on the NOISE
dataset with various noise levels γ. The error drops at the
ﬁrst few steps, but then stabilizes around a constant related
to the noise level, as predicted by our analysis. This shows
that it can recover the ground-truth to good accuracy, even
when the data have a signiﬁcant amount of noise.

9We also note that even the thresholding of HALS and ALNS
designed for non-negative feature matrices is removed, they still

do not converge on NEG.

05001000150020002500300035004000Time in seconds−40−30−20−10010log(Error)ANDANLSMULDAHALS05001000150020002500300035004000Time in seconds−40−30−20−10010log(Error)ANDANLSMULDAHALS0200400600800100012001400Time in seconds−50−40−30−20−10010log(Error)ANDANLSMULDAHALS05001000150020002500300035004000Time in seconds−40−30−20−10010log(Error)Decreasing thresholdConstant threshold 0.0001Constant threshold 0.105001000150020002500300035004000Time in seconds−40−30−20−10010log(Error)Decreasing thresholdConstant threshold 0.0001Constant threshold 0.1050100150200Time in seconds−10−8−6−4−2024log(Error)noise level 0.1noise level 0.05noise level 0.01noise level 0.005noise level 0.001noise level 0.0005Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations

Blei, David M, Ng, Andrew Y, and Jordan, Michael I. La-

tent dirichlet allocation. JMLR, 3:993–1022, 2003.

Cichocki, Andrzej, Zdunek, Rafal, and Amari, Shun-ichi.
Hierarchical als algorithms for nonnegative matrix and
3d tensor factorization. In International Conference on
Independent Component Analysis and Signal Separa-
tion, pp. 169–176. Springer, 2007.

Ding, W., Rohban, M.H., Ishwar, P., and Saligrama, V.
Topic discovery through data dependent and random
projections. arXiv preprint arXiv:1303.3664, 2013.

Ding, W., Rohban, M.H., Ishwar, P., and Saligrama, V. Ef-
ﬁcient distributed topic modeling with provable guaran-
tees. In AISTAT, pp. 167–175, 2014.

Hoffman, Matthew, Bach, Francis R, and Blei, David M.
In ad-
Online learning for latent dirichlet allocation.
vances in neural information processing systems, pp.
856–864, 2010.

Kim, Hyunsoo and Park, Haesun. Nonnegative matrix fac-
torization based on alternating nonnegativity constrained
least squares and active set method. SIAM journal on
matrix analysis and applications, 30(2):713–730, 2008.

Lee, Daniel D and Seung, H Sebastian. Unsupervised
learning by convex and conic coding. NIPS, pp. 515–
521, 1997.

Lee, Daniel D and Seung, H Sebastian. Learning the parts
of objects by non-negative matrix factorization. Nature,
401(6755):788–791, 1999.

Lee, Daniel D and Seung, H Sebastian. Algorithms for
non-negative matrix factorization. In NIPS, pp. 556–562,
2001.

Li, Yuanzhi, Liang, Yingyu, and Risteski, Andrej. Recov-
ery guarantee of non-negative matrix factorization via al-
ternating updates. Advances in neural information pro-
cessing systems, 2016.

Yang, Jaewon and Leskovec, Jure. Overlapping community
detection at scale: a nonnegative matrix factorization ap-
proach. In Proceedings of the sixth ACM international
conference on Web search and data mining, pp. 587–596.
ACM, 2013.

Acknowledgements

This work was supported in part by NSF grants CCF-
1527371, DMS-1317308, Simons Investigator Award, Si-
mons Collaboration Grant, and ONR-N00014-16-1-2329.
This work was done when Yingyu Liang was visiting the
Simons Institute.

References

Lda-c software. https://github.com/blei-lab/
lda-c/blob/master/readme.txt, 2016. Ac-
cessed: 2016-05-19.

Anandkumar, A., Kakade, S., Foster, D., Liu, Y., and Hsu,
D. Two svds sufﬁce: Spectral decompositions for prob-
abilistic topic modeling and latent dirichlet allocation.
Technical report, 2012.

Anandkumar, A., Hsu, D., Javanmard, A., and Kakade, S.
Learning latent bayesian networks and topic models un-
der expansion constraints. In ICML, 2013.

Arora, S., Ge, R., and Moitra, A. Learning topic models –

going beyond svd. In FOCS, 2012a.

Arora, S., Ge, R., Halpern, Y., Mimno, D., Moitra, A.,
Sontag, D., Wu, Y., and Zhu, M. A practical algorithm
for topic modeling with provable guarantees. In ICML,
2013.

Arora, S., Ge, R., Ma, T., and Moitra, A. Simple, efﬁcient,
and neural algorithms for sparse coding. In COLT, 2015.

Arora, Sanjeev, Ge, Rong, Kannan, Ravindran, and Moitra,
Ankur. Computing a nonnegative matrix factorization–
provably. In STOC, pp. 145–162. ACM, 2012b.

Arora, Sanjeev, Ge, Rong, Koehler, Frederic, Ma, Tengyu,
and Moitra, Ankur. Provable algorithms for inference in
topic models. In Proceedings of The 33rd International
Conference on Machine Learning, pp. 2859–2867, 2016.

Awasthi, Pranjal and Risteski, Andrej. On some provably
correct cases of variational inference for topic models.
In NIPS, pp. 2089–2097, 2015.

Bhattacharyya, Chiranjib, Goyal, Navin, Kannan, Ravin-
dran, and Pani, Jagdeep. Non-negative matrix factor-
ization under heavy noise. In Proceedings of the 33nd
International Conference on Machine Learning, 2016.

Blei, David and Lafferty, John. Correlated topic models.
Advances in neural information processing systems, 18:
147, 2006.

Blei, David M. Probabilistic topic models. Communica-

tions of the ACM, 2012.

