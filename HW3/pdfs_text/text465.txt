Deep Decentralized Multi-task Multi-Agent RL under Partial Observability

Supplemental: Deep Decentralized Multi-task Multi-agent RL under Partial Observability

The following provides additional empirical results. Some plots are reproduced from the main text to ease comparisons.

A. Multi-agent Multi-target (MAMT) Domain Overview

(a) Agents must learn inherent toroidal
transition dynamics in the domain for
fast target capture (e.g., see green target).

(b) In MAMT tasks, no reward is given to
the team above, despite two agents suc-
cessfully capturing their targets.

(c) Example successful simultaneous
capture scenario, where the team is given
+1 reward.

Figure 6. Visualization of MAMT domain. Agents and targets operate on a toroidal m × m gridworld. Each agent (circle) is assigned
a unique target (cross) to capture, but does not observe its assigned target ID. Targets’ states are fully occluded at each timestep with
probability Pf . Despite the simplicity of gridworld transitions, reward sparsity makes this an especially challenging task. During both
learning and execution, the team receives no reward unless all targets are captured simultaneously by their corresponding agents.

B. Empirical Results: Learning on Multi-agent Single-Target (MAST) Domain

Multi-agent Single-target (MAST) domain results for Dec-DRQN and Dec-HDRQN, with 2 agents and Pf = 0.0 (ob-
servation ﬂickering disabled). These results mainly illustrate that Dec-DRQN sometimes has some empirical success in
low-noise domains with small state-space. Note that in the 8 × 8 task, Dec-HDRQN signiﬁcantly outperforms Dec-DRQN,
which converges to a sub-optimal policy despite domain simplicity.

n
r
u
t
e
R

l
a
c
i
r
i
p
m
E

1.0

0.8

0.6

0.4

0.2

0.0

n
r
u
t
e
R

l
a
c
i
r
i
p
m
E

1.0

0.8

0.6

0.4

0.2

0.0

4 × 4 (Dec-DRQN)
5 × 5 (Dec-DRQN)
6 × 6 (Dec-DRQN)
7 × 7 (Dec-DRQN)
8 × 8 (Dec-DRQN)

4 × 4 (Dec-HDRQN)
5 × 5 (Dec-HDRQN)
6 × 6 (Dec-HDRQN)
7 × 7 (Dec-HDRQN)
8 × 8 (Dec-HDRQN)

)
0
a
,
0
s
(
Q

)
0
a
,
0
s
(
Q

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

0.0

5K

10K
Training Epoch

15K

20K

0.0

5K

10K
Training Epoch

15K

20K

(a) Dec-DRQN empirical returns during training.

(b) Dec-DRQN anticipated values during training.

4 × 4 (Dec-DRQN)
5 × 5 (Dec-DRQN)
6 × 6 (Dec-DRQN)
7 × 7 (Dec-DRQN)
8 × 8 (Dec-DRQN)

4 × 4 (Dec-HDRQN)
5 × 5 (Dec-HDRQN)
6 × 6 (Dec-HDRQN)
7 × 7 (Dec-HDRQN)
8 × 8 (Dec-HDRQN)

0.0

5K

10K
Training Epoch

15K

20K

0.0

5K

10K
Training Epoch

15K

20K

(c) Dec-HDRQN empirical returns during training.

(d) Dec-HDRQN anticipated values during training.

Figure 7. Multi-agent Single-target (MAST) domain results for Dec-DRQN and Dec-HDRQN, with 2 agents and Pf = 0.0 (observation
ﬂickering disabled). All plots conducted (at each training epoch) for a batch of 50 randomly-initialized episodes. Anticipated value plots
(on right) were plotted for the exact starting states and actions undertaken for the episodes used in the plots on the left.

Deep Decentralized Multi-task Multi-Agent RL under Partial Observability

C. Empirical Results: Learning on MAMT Domain

Multi-agent Single-target (MAMT) domain results, with 2 agents and Pf = 0.3 (observation ﬂickering disabled). We
also evaluated performance of inter-agent parameter sharing (a centralized approach) in Dec-DRQN (which we called
Dec-DRQN-PS). Additionally, performance of a Double-DQN was deemed to have negligible impacts (Dec-DDRQN).

n
r
u
t
e
R

l
a
c
i
r
i
p
m
E

1.0

0.8

0.6

0.4

0.2

0.0

n
r
u
t
e
R

l
a
c
i
r
i
p
m
E

1.0

0.8

0.6

0.4

0.2

0.0

n
r
u
t
e
R

l
a
c
i
r
i
p
m
E

1.0

0.8

0.6

0.4

0.2

0.0

n
r
u
t
e
R

l
a
c
i
r
i
p
m
E

0.8

0.6

0.4

0.2

0.0

3 × 3 (Dec-DRQN)
4 × 4 (Dec-DRQN)
5 × 5 (Dec-DRQN)
6 × 6 (Dec-DRQN)
7 × 7 (Dec-DRQN)

3 × 3 (Dec-DRQN-PS)
4 × 4 (Dec-DRQN-PS)
5 × 5 (Dec-DRQN-PS)
6 × 6 (Dec-DRQN-PS)
7 × 7 (Dec-DRQN-PS)

3 × 3 (Dec-DDRQN)
4 × 4 (Dec-DDRQN)
5 × 5 (Dec-DDRQN)
6 × 6 (Dec-DDRQN)
7 × 7 (Dec-DDRQN)

3 × 3 (Dec-HDRQN)
4 × 4 (Dec-HDRQN)
5 × 5 (Dec-HDRQN)
6 × 6 (Dec-HDRQN)
7 × 7 (Dec-HDRQN)

)
0
a
,
0
s
(
Q

)
0
a
,
0
s
(
Q

)
0
a
,
0
s
(
Q

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

)
0
a
,
0
s
(
Q

0.8

0.6

0.4

0.2

0.0

0.0

10K 20K 30K 40K 50K 60K 70K

Training Epoch

0.0

10K 20K 30K 40K 50K 60K 70K

Training Epoch

(a) Dec-DRQN empirical returns during training.

(b) Dec-DRQN anticipated values during training.

0.0

10K 20K 30K 40K 50K 60K 70K

Training Epoch

0.0

10K 20K 30K 40K 50K 60K 70K

Training Epoch

(c) Dec-DRQN-PS (with parameter sharing), empirical returns
during training.

(d) Dec-DRQN-PS (with parameter sharing), anticipated values
during training.

0.0

10K 20K 30K 40K 50K 60K 70K

Training Epoch

0.0

10K 20K 30K 40K 50K 60K 70K

Training Epoch

(e) Dec-DDRQN (double DRQN) empirical returns during train-
ing.
1.0

(f) Dec-DDRQN (double DRQN) anticipated values during train-
ing.
1.0

0.0

10K 20K 30K 40K 50K 60K 70K

Training Epoch

0.0

10K 20K 30K 40K 50K 60K 70K

Training Epoch

(g) Dec-HDRQN (our approach) empirical returns during train-
ing.

(h) Dec-HDRQN (our approach) anticipated values during train-
ing.

Figure 8. MAMT domain results for Dec-DRQN and Dec-HDRQN, with 2 agents and Pf = 0.3. All plots conducted (at each training
epoch) for a batch of 50 randomly-initialized episodes. Anticipated value plots (on right) were plotted for the exact starting states and
actions undertaken for the episodes used in the plots on the left.

3 × 3 (Dec-DRQN)
4 × 4 (Dec-DRQN)
5 × 5 (Dec-DRQN)
6 × 6 (Dec-DRQN)
7 × 7 (Dec-DRQN)

3 × 3 (Dec-DRQN-PS)
4 × 4 (Dec-DRQN-PS)
5 × 5 (Dec-DRQN-PS)
6 × 6 (Dec-DRQN-PS)
7 × 7 (Dec-DRQN-PS)

3 × 3 (Dec-DDRQN)
4 × 4 (Dec-DDRQN)
5 × 5 (Dec-DDRQN)
6 × 6 (Dec-DDRQN)
7 × 7 (Dec-DDRQN)

3 × 3 (Dec-HDRQN)
4 × 4 (Dec-HDRQN)
5 × 5 (Dec-HDRQN)
6 × 6 (Dec-HDRQN)
7 × 7 (Dec-HDRQN)

Deep Decentralized Multi-task Multi-Agent RL under Partial Observability

1.0

0.8

)
0
a
,
0
o
(
Q

0.6

0.4

0.2

0.0

0.0

3 × 3 (agent 1)
3 × 3 (agent 2)
4 × 4 (agent 1)
4 × 4 (agent 2)
5 × 5 (agent 1)
5 × 5 (agent 2)
6 × 6 (agent 1)
6 × 6 (agent 2)
7 × 7 (agent 1)
7 × 7 (agent 2)

10K

20K

30K

40K

50K

60K

70K

Training Epoch

Figure 9. Comparison of agents’ anticipated value plots using Dec-HDRQN during training. MAMT domain, with 2 agents and Pf =
0.3. All plots conducted (at each training epoch) for a batch of 50 randomly-initialized episodes. For a given task, agents have similar
anticipated value convergence trends due to shared reward; differences are primarily caused by random initial states and independently
sampled target occlusion events for each agent.

n
r
u
t
e
R

l
a
c
i
r
i
p
m
E

1.0

0.8

0.6

0.4

0.2

0.0

0.0

3 × 3 (Dec-DRQN)
4 × 4 (Dec-DRQN)
3 × 3 (Dec-HDRQN)
4 × 4 (Dec-HDRQN)

)
0
a
,
0
s
(
Q

1.0

0.8

0.6

0.4

0.2

0.0

10K

20K
Training Epoch

30K

40K

(a) Empirical returns during training. For batch of 50 randomly-
initialized games.

0.0

10K

20K
Training Epoch

30K

40K

(b) Anticipated values during training. For speciﬁc starting states
and actions undertaken in the same 50 randomly-initialized games
as Fig. 10a.

Figure 10. MAMT domain results for Dec-DRQN and Dec-HDRQN, with n = 3 agents. Pf = 0.6 for the 3 × 3 task, and Pf = 0.1 for
the 4 × 4 task.

3 × 3 (Dec-DRQN)
4 × 4 (Dec-DRQN)
3 × 3 (Dec-HDRQN)
4 × 4 (Dec-HDRQN)

Deep Decentralized Multi-task Multi-Agent RL under Partial Observability

D. Empirical Results: Learning Sensitivity to
Dec-HDRQN Negative Learning Rate β

E. Empirical Results: Learning Sensitivity to

Dec-HDRQN Recurrent Training
Tracelength Parameter τ

0.0

10K

20K
Training Epoch

30K

40K

(a) Sensitivity of Dec-HDRQN empirical returns to β during train-
ing. For batch of 50 randomly-initialized games.

0.0

10K

20K
Training Epoch

30K

40K

(a) Dec-HDRQN sensitivity to tracelength τ . 6x6 MAMT domain
with Pf = 0.25.

n
r
u
t
e
R

l
a
c
i
r
i
p
m
E

1.0

0.8

0.6

0.4

0.2

0.0

)
0
a
,
0
o
(
Q

¯Q

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

β = 0.0
β = 0.1
β = 0.2
β = 0.3
β = 0.4
β = 0.5

β = 0.0
β = 0.1
β = 0.2
β = 0.3
β = 0.4
β = 0.5

β = 0.6
β = 0.7
β = 0.8
β = 0.9
β = 1.0

β = 0.6
β = 0.7
β = 0.8
β = 0.9
β = 1.0

n
r
u
t
e
R

l
a
c
i
r
i
p
m
E

1.0

0.8

0.6

0.4

0.2

0.0

)
0
a
,
0
s
(
Q

1.0

0.8

0.6

0.4

0.2

0.0

β = 0.0
β = 0.1
β = 0.2
β = 0.3
β = 0.4
β = 0.5

β = 0.6
β = 0.7
β = 0.8
β = 0.9
β = 1.0

0.0

10K

20K
Training Epoch

30K

40K

(b) Sensitivity of Dec-HDRQN predicted action-values to β during
training. For speciﬁc starting states and actions undertaken in the
same 50 randomly-initialized games of Fig. 11a.

0.0

10K

20K
Training Epoch

30K

40K

(c) Sensitivity of Dec-HDRQN average Q values to β during train-
ing. For random minibatch of 32 experienced observation inputs.

Figure 11. Learning sensitivity to β for 6 × 6, 2 agent MAMT
domain with Pf = 0.25. All plots for agent i = 0. β = 1
corresponds to Decentralized Q-learning, β = 0 corresponds to
Distributed Q-learning (not including the distributed policy up-
date step).

Tracelength = 1
Tracelength = 2
Tracelength = 4
Tracelength = 8

Tracelength = 1
Tracelength = 2
Tracelength = 4
Tracelength = 8

0.0

10K

20K
Training Epoch

30K

40K

(b) Sensitivity of Dec-HDRQN predicted action-values to recur-
rent training tracelength parameter. For speciﬁc starting states and
actions undertaken in the same 50 randomly-initialized games of
Fig. 12a.

Figure 12. Learning sensitivity to recurrent training tracelength
parameter for 6 × 6, 2 agent MAMT domain with Pf = 0.25.
All plots for agent i = 0. β = 1 corresponds to Decentralized
Q-learning, β = 0 corresponds to Distributed Q-learning (not in-
cluding the distributed policy update step).

Deep Decentralized Multi-task Multi-Agent RL under Partial Observability

F. Empirical Results: Multi-tasking

Performance Comparison

The below plots show multi-tasking performance of both
the distillation and Multi-HDRQN approaches. Both ap-
proaches were trained on the 3 × 3 through 6 × 6 MAMT
tasks. Multi-DRQN failed to achieve specialized-level per-
formance on all tasks, despite 500K training epochs. By
contrast,
the proposed MT-MARL distillation approach
achieves nominal performance after 100K epochs.

n
r
u
t
e
R

l
a
c
i
r
i
p
m
E

n
r
u
t
e
R

l
a
c
i
r
i
p
m
E

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

3 × 3
4 × 4
5 × 5
6 × 6

3 × 3
4 × 4
5 × 5
6 × 6

0.0

100K

200K

300K

400K

500K

Dec-MHDRQN Epoch

(a) MT-MARL via Multi-HDQRN.

0.0

20K

40K

60K

80K

100K

Distillation Epoch

(b) MT-MARL via specialized and distilled Dec-HDRQN.

Figure 13. Multi-task performance on MAMT domain, n = 2
agents and Pf = 0.3.

