Regularising Non-linear Models Using Feature Side-information

Amina Mollaysa * 1 Pablo Strasser * 1 Alexandros Kalousis 1

Abstract
Very often features come with their own vectorial
descriptions which provide detailed information
about their properties. We refer to these vecto-
rial descriptions as feature side-information. In
the standard learning scenario, input is repre-
sented as a vector of features and the feature side-
information is most often ignored or used only
for feature selection prior to model ﬁtting. We
believe that feature side-information which car-
ries information about features intrinsic property
will help improve model prediction if used in a
proper way during learning process. In this pa-
per, we propose a framework that allows for the
incorporation of the feature side-information dur-
ing the learning of very general model families
to improve the prediction performance. We con-
trol the structures of the learned models so that
they reﬂect features’ similarities as these are de-
ﬁned on the basis of the side-information. We
perform experiments on a number of benchmark
datasets which show signiﬁcant predictive per-
formance gains, over a number of baselines, as a
result of the exploitation of the side-information.

1. Introduction

Side-information in machine learning is a very general term
used in very different learning scenarios with quite dif-
ferent connotations. Nevertheless, generally it is under-
stood as any type of information, other than the learn-
ing instances, which can be used to support the learning
process; typically such information will live in a different
space than the learning instances. Examples include learn-
ing with privileged information (Vapnik & Izmailov, 2015)
in which during training a teacher provides additional in-

*Equal contribution 1University of Applied Sciences, Western
Switzerland; University of Geneva. Correspondence to: Amina
Mollaysa <maolaaisha.aminanmu@hesge.ch>, Pablo Strasser
<pablo.strasser@hesge.ch>, Alexandros Kalousis <Alexan-
dros.Kalousis@hesge.ch>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

formation for the learning instances; this information is not
available in testing.
In metric learning and clustering, it
has been used to denote the availability of additional sim-
ilarity information on instances, i.e. pairs of similar and
dissimilar instances, (Xing et al., 2002). In this paper we
focus on side-information describing the features. We will
consider learning problems in which we have additional in-
formation describing the properties and/or the relations of
the features. The features will have their own vectorial de-
scriptions in some space in which we will describe their
properties.

Real world problems with such properties are very com-
mon. For example in drug efﬁciency prediction problems,
and more general in chemical formulae property predic-
tion problems, drugs/formulae are collections of molecules.
Each molecule comes with its own description, for example
in terms of its physio-chemical properties, and/or its molec-
ular structure. In language modeling, words are features
and the words’ semantic and syntactic properties are their
side-information. In image recognition, pixels are features
and their position is the side-information, and so on. Simi-
lar ideas also appear in tasks such as matrix completion, ro-
bust PCA and collaborative ﬁltering (Rao et al., 2015; Chi-
ang et al., 2016; 2015). There one seeks low rank matrix
decompositions in which the component matrices are con-
strained to follow relationships given by side-information
matrices, typically matrices which contain user and item
descriptors.

Despite the prevalence of such problems, there has been
surprisingly limited work on learning with feature side-
information. Krupka et al., 2008, used the feature side-
information to perform feature selection as a preprocess-
ing step prior to any modelling or learning. More inter-
estingly (Krupka & Tishby, 2007) exploit the feature side-
information directly within the learning process, by forc-
ing features that have similar side-information to have a
similar weight within a SVM model. One can think of
this as a model regularisation technique in which we force
the model structure, i.e. the feature parameters, to reﬂect
the feature manifold as this is given by the feature side-
information.
In the same work the authors also provide
an ad-hoc way to apply the same idea for non-linear mod-
els, more precisely polynomials of low degree. However
the solution that they propose requires an explicit construc-

Regularising Non-linear Models Using Feature Side-information

tion of the different non-linear terms, as well as appropriate
deﬁnitions of the feature side-information that is associated
with them. These deﬁnitions are hand-crafted and depend
on the speciﬁc application problem. Beyond this ad-hoc ap-
proach it is far from clear how one could regularise general
non-linear models so that they follow the feature manifold.

In this paper we present a method for the exploitation of
feature side-information in non-linear models. The main
idea is that the learned model will treat in a similar manner
features that are similar. Intuitively, exchanging the values
of two very similar features should only have a marginal ef-
fect on the model output. This is straightforward for linear
models since we have direct access to how the model treats
the features, i.e. the feature weights. In such a case one can
design regularisers as Krupka & Tishby, 2007, did which
force the feature weights to reﬂect the feature manifold. An
obvious choice would be to apply a Laplacian regulariser
to the linear model, where the Laplacian is based on the
feature similarity. Such regularisers have been previously
used for parameter shrinkage but only in the setting of lin-
ear models where one has direct access to the model param-
eters (Huang et al., 2011). However, in general non-linear
models we no longer have access to the feature weights; the
model parameters are shared between the features and we
cannot disentangle them.

We present a regulariser which forces the learned model
to be invariant/symmetric to relative changes in the val-
ues of similar features. It directly reﬂects the intuition that
small changes in the values of similar features should have
a small effect on the model output. The regulariser relies
on a measure of the model output sensitivity to changes in
all possible pairs of features. The model sensitivity mea-
sure quantiﬁes the norm of the change of the model output
under all possible relative changes of the values of two fea-
tures. We compute this norm by integrating over the rela-
tive changes and the data distribution. Integrating over the
relative changes is problematic we thus give two ways to
approximate the sensitivity measure. In the ﬁrst approach
we rely on a ﬁrst order Taylor expansion of the learned
model under which the sensitivity measure boils down to
the squared norm of the difference of the partial derivatives
of the model with respect to the input features. Under this
approach the regulariser ﬁnally boils down to the applica-
tion of a Laplacian regulariser on the Jacobian of the model.
In the second approach we rely on sampling and data aug-
mentation to generate instances with appropriate relative
changes over different feature pairs. We approximate the
value of the regulariser only on the augmented data.

We implement the above ideas in the context of neural net-
works, nevertheless it is relatively straightforward to use
them in other non-linear models such as SVMs and kernels.

We experiment on a number of text classiﬁcation datasets in

which the side-information is the word2vec representation
of the words. We compare against a number of baselines
and we show signiﬁcant performance improvements.

2. Learning Symmetric Models with Respect

to Feature Similarity

×

×

×

We consider supervised learning settings in which, in ad-
d containing n
dition to the classical data matrix X : n
m,
instances and d features, and the target matrix Y : n
we are also given a matrix Z : d
c, the ith row of which,
denoted by zi, contains a description of the ith feature.
We call Z the feature side-information matrix. Note how
the Z matrix is ﬁxed and independent of the training in-
stances. As in the standard supervised setting, instances,
Rd, are drawn i.i.d from some non-observed
xi ∈X ⊆
Rm,
probability distributions P (
) and targets, yi ∈Y⊆
X
are assigned according to some non-observed conditional
Rm.
distribution P (
In the standard setting
Y|X
we learn a mapping from the input to the output φ : x
∈
Rm using the X, Y matrices by optimizing
Rd
∈
some loss function L.
In this paper we learn the input-
output mapping using in addition to the X, Y, matrices the
feature side-information Z matrix.

Y∈

→

),

y

We bring the feature side-information in the learning pro-
d which
cess through the feature similarity matrix S
we construct from Z as follows. Given two features i, j,
with zi, zj, side-information vectors the Sij element of
S contains their similarity given by some similarity func-
S the Laplacian of the
tion. We will denote by L = D
similarity matrix S; D is the diagonal degree matrix with
Dii =

Rd

−

∈

×

j Sij.

×

!

We use the similarity and the Laplacian matrices to con-
straint the learned model to treat in a similar manner fea-
tures that have similar side-information. This is relatively
straightforward with linear models such as WXT, W
∈
Rm
d, and can be achieved through the introduction of
the Laplacian regulariser Tr (WLWT) =
W.i −
2Sij in the objective function where W.i is the ith
W.j||
column vector of W containing the model parameters asso-
ciated with the ith feature, (Huang et al., 2011). The Lapla-
cian regulariser forces the parameter vectors of the features
to cluster according to the feature similarity.

ij ||

!

However in non-linear models such neat separation of the
model parameters is not possible since these are shared be-
tween the different input features. In order to achieve the
same effect we will now operate directly on the model out-
put. We will do so by requiring that the change in the
model’s output is marginal if we change the relative pro-
portion of two very similar features. Concretely, let i and
j be such features, and ei, ej, be the d-dimensional unit
vectors with the ith and jth dimensions respectively equal

Regularising Non-linear Models Using Feature Side-information

over all instances drawn from P (x) as well as for all λ
vectors that satisfy the equality constraint eq 2. A natural
measure of the degree to which the constraint holds for the
feature pair i, j is given by:

Rij(φ) =

(3)

φ(x + λiei + λjej)

||
φ(x + λ′iei + λ′jej)

"
−
SijI(λ)P (x)dλdx

2

||

where I(λ) = 1 if λi + λj = λ′i + λ′j, and 0 otherwise.
Since we want to deﬁne a regulariser that accounts for all
feature pairs and their similarities we simply have:

R(φ) =

Rij(φ)

(4)

ij
#

Calculating the regularizer is problematic due to the pres-
ence of the I(λ) function that selects the λ subspace over
which the integration is performed. In the next two sec-
tions we will give two ways to approximate it. The ﬁrst
one will be analytical relying on the ﬁrst order Taylor ex-
pansion of φ(x) and its Jacobian. The second one stochas-
tic, essentially performing data augmentation and deﬁning
a regularisation term along the lines of eq. 2.

2.1. An analytical Approximation

We will use the ﬁrst order Taylor expansion of φ(x) to sim-
plify the squared term in eq. 2 by removing the λ variable.
We will start by using the ﬁrst order Taylor expansion to
approximate the value of φ(x + λiei + λjej) at x

φ(x + λiei + λjej)

φ(x) + J(x)(λiei + λjej)

≈

J(x)
plugging the Taylor expansion in eq 2 we get:

d is the Jacobian of φ(x) evaluated at x. Then

Rm

∈

×

(λi −

λ′i)J(x)ei −

(λ′j −

λj)J(x)ej||

||

∝

2

1
Sij

(5)

and since λi + λj = λ′i + λ′j we have (λi −
and eq 5 becomes:

λ′i) = (λ′j −

λj)

J(x)ei −

J(x)ej||

||

2 =

||∇iφ(x)

− ∇jφ(x)

||

∝

2

1
Sij

(6)

∇iφ(x) is the m-dimensional partial derivative of
where
φ(x) with respect to the ith input feature. Using eq 6 we
can approximate Rij as follows:

Rij(φ)

≈

"

||∇iφ(x)

− ∇jφ(x)

||

2SijP (x)dx

Figure1. The blue dot is some given instance, x. The two axes
are the ith and jth features. If the two features are on the limit
identical then the model’s output is constant along the line deﬁned
as: x + λiei + λjej,

λi + λj = c, where c is some constant.

∀

to one. We want that:

φ(x + λiei + λjej)

φ(x + λ′iei + λ′jej)

(1)

≈

λi,λ j,λ ′i,λ ′j ∈

∀

R such that λi + λj = λ′i + λ′j

Equation (1) states that as long as the total contribution of
the i, j, features is kept ﬁxed, the model’s output should be
left almost unchanged. The exact equality will hold when
the i, j, are on the limit identical, i.e. Sij → ∞
. More
general the level of the model’s change should reﬂect the
similarity of the i, j, features, thus a more accurate refor-
mulation of equation 1 is:

φ(x + λiei + λjej)

φ(x + λ′iei + λ′jej)

||

−

2

||

∝

1
Sij

(2)

R such that λi + λj = λ′i + λ′j

λi,λ j,λ ′i,λ ′j ∈
∀
Thus the norm of the change in the model output, that
we get when we alter the relative proportion of two fea-
tures i and j, while keeping their total contribution ﬁxed,
should be inversely proportional to the features similarity,
i.e. large similarity, small output change. The result is that
the model is symmetric to similar features and its output
does not depend on the individual contributions/values of
two similar features but only on their total contribution.
In ﬁgure 1 we visualise the effect of the model constraint
given in eq. 2. Given some instance x and two features
i, j, that are on the limit identical the constraint forces
the model output to be constant on the line deﬁned by
x + λiei + λjej,
R.
We can think of the whole process as the model clustering
together, to some latent factor, features that have very high
similarity. The latent factor captures the original features
total contribution leaving the model’s output unaffected to
relative changes in their values.

λi + λj = c, for some given c

∈

∀

To unclutter notation we will deﬁne the vector λ =
(λi,λ j,λ ′i,λ ′j). We want the constraint of eq. 2 to be valid

Regularising Non-linear Models Using Feature Side-information

from which we get the following approximation of the
R(φ) regulariser:

R(φ)

||∇iφ(x)

− ∇jφ(x)

||

2SijP (x)dx

data augmentation as we will see in the next section. Note
also that the presence of the Jacobian in the objective func-
tion means that if we optimise it using gradient descent we
will need to compute second order partial derivatives which
come with an increasing computational cost.

||∇iφ(x)

− ∇jφ(x)

||

2SijP (x)dx

2.2. A stochastic Approximation

≈

≈

≈

ij "
#

ij

" #

"

Tr[J(x)LJT(x)]P (x)dx

(7)

which is the local linear approximation of the original reg-
ulariser eq 4 on the input instances. Since we only have
access to the training sample and not to P (x) we will get
the sample estimate of eq. 7 given by

ˆR(φ) =

||∇iφ(xk)

− ∇jφ(xk)

||

ij
#

#k

=

=

||∇iφ(xk)

− ∇jφ(xk)

||

ij
#k #
Tr[J(xk)LJT(xk)]

2Sij

2Sij

#k

So the sample based estimate of the regulariser is a sum
of Laplacian regularisers applied on the Jacobian of each
one of the training samples.
It forces the partial deriva-
tives of the model with respect to the input, or equiva-
lently the model’s sensitivity to the input features, to reﬂect
the features similarity in the local neighborhood around
each training point. Or in other words it will constrain the
learned model in a small neighborhood around each train-
ing point to have similar slop in the dimensions that are
associated with similar features. Note that if φ(x) = Wx
then J(xk) = W and Tr[J(xk)LJT(xk)] reduces to the
standard Tr[WLWT] Laplacian regulariser on the columns
of W associated with the input features. Adding the sam-
ple based estimate of the regulariser to the loss function we
get the ﬁnal objective function which we minimize with
φ(x) giving the following minimization problem under the
analytical approximation:

L(yk,φ (xk)) + λ

Tr[J(xk)LJT(xk)]

(9)

min
φ

#k

#k

The approximation of the requlariser is only effective lo-
cally around each training point since it relies on ﬁrst order
Taylor expansion. When the learned function is highly non-
linear, it can force model invariance only to small relative
changes in the values of two similar features. However, as
the size of the relative changes increases and we move away
from the local region the approximation is no longer effec-
tive. The regulariser will not be powerful enough to make
the invariance hold away from the training points. If we
want a less local approximation we can either use higher
order Taylor approximation which is computationally pro-
hibitive or rely on a more global approximation through

Instead of using the ﬁrst order Taylor expansion to sim-
plify the squared term required by the regulariser we can
use sampling to approximate it. Concretely for a given
feature pair, i, j, and a given instance x we randomly
sample p quadruples λ(l)
j ,λ (l)′
,λ (l)
R such that
i
i + λ(l)′
j = λ(l)′
i + λ(l)
λ(l)
, l := 1 . . . p, which we use to
generate p new instance pairs as follows:

,λ (l)′
j

∈

j

i

x

→ $

x + λ(l)
x + λ(l)′

i ei + λ(l)
j ej
i ei + λ(l)′
j ej

(8)

We can now use the training sample and the sampling pro-
cess to get an estimate of Rij(φ) by:

(φ(xk + λ(l)

i ei + λ(l)

j ej)

||

#k #l

−

φ(xk + λ(l)′

i ei + λ(l)′

j ej))

2Sij

||

and of the ﬁnal regulariser R(φ) by:

˜R(φ) =

(φ(xk + λ(l)

i ei + λ(l)

j ej)

||

ij
#k #l
#
φ(xk + λ(l)′

−

i ei + λ(l)′

j ej))

2Sij

(10)

||

So the ﬁnal optimization problem will now become:

L(yk,φ (xk)) + λ ˜R(φ)

(11)

min
φ

#k

Note that the new instances appear only in the regulariser
and not in the loss. The regulariser will penalise models
which do not have the invariance property with respect to
pairs of similar features. In practice when computing ˜R(φ)
we do not want to go through all the pairs of features but
only through the most similar. We do not want to spend
sampling time on data augmentation for dissimilar pairs
since for these there is no effective constraint on the val-
ues of the model’s output. So we simplify the sum run only
over the pairs of similar features. One motivation for the
stochastic approach was the fact that the analytical one re-
lies in an approximation which is only effective locally in
the neighborhood of each learning instance. In the stochas-
tic approach we have control on the size of the neighbor-
hood over which the constraint is enforced through the Eu-
clidean norm of the change vector (λi,λ j); the larger its

Regularising Non-linear Models Using Feature Side-information

value the larger the neighborhood. The smaller the neigh-
borhood the closer we are to the local behavior of the an-
alytical approximation. We should note here that the sam-
pling of stochastic approximation will naturally blend with
the stochastic gradient optimization that we will use to op-
timize our objective functions.

2.3. Optimization

We learn φ with a standard feed forward neural network
with sigmoid activation functions applied on the hidden
layers using stochastic gradient descent. The objective
function of the analytical approach contains the Jacobian
of the model with respect to its input. Calculating the gra-
dient over this results in the introduction of second order
partial derivatives of the model with respect to the inputs
and the model parameters. Bishop, 1992, gave a backprop-
agation algorithm for the exact calculation of the Hessian
of the loss of a multi-layer perceptron. We have adapted
this algorithm so that we can compute the gradient of ob-
jective functions that contain the Jacobian with respect to
the input features, we give the complete gradient calcula-
tion procedure in the appendix.

{

1

−

O

×
×

hk|

(m
(l

h1 ×
m
×

We will give now the computational complexity of each
the two methods. We will denote by l the number of
layers, m the output dimension of the network, hk the
number of hidden units of the kth layer and we will de-
ﬁne hmax = max
k = 1, . . . , l
. The computa-
}
tional complexity of computing the gradient for a single in-
stance of the objective function of the analytical approach
d2) for networks with a single hidden layer
is
O
h3
h2
d) for networks
and
max ×
max ×
×
with more than one hidden layers.
To reduce this com-
putational complexity in our experiments we sparsify S by
keeping only the entries correponding to top 20% biggest
elements and zero out the rest.
The complexity now be-
comes
×
O
h3
h2
m
max) for networks with more than
max ×
one layer. The computational complexity of the stochastic
h2
p)
m
approach is
max ×
while the computational complexity for standard feed for-
h2
max + l
ward network is

d) for one layer networks and

hmax ×
m).

h1 ×
m
×

×
d+l

d2 + l

p + l

(m

m

m

O

O

×

×

×

×

×

×

(l

(l

(l

×
hmax ×

O

×

×

3. Related Work

Krupka & Tishby, 2007, use feature side-information, they
call it meta-features, within a linear SVM model. They
force the SVM’s weights to be similar for features that have
similar side-information. They achieve that through the in-
troduction of a Gaussian prior on the feature weight vector.
The covariance matrix of the Gaussian is a function of the
features similarity. The authors show how to extend their
approach from linear to polynomial models. However, their
approach requires explicit calculation of all the higher or-

der terms limiting its applicability to low order polynomi-
als. Similar work is the Laplacian-based feature regulari-
sation, (Huang et al., 2011), which constraints the feature
weights to reﬂect relations that are given by the Laplacian.
The Laplacian matrix is constructed from available domain
knowledge, what here we call feature side-information. It
can also be constructed from the data; for example as a
function of the feature correlation matrix.

The Laplacian Regulariser (LR) has connections to some
Lasso-based regularisers. With LR feature weights reﬂect
the feature similarity given by the side information. The
fused, graph, and group lasso variants (Tibshirani et al.,
2005; Yuan & Lin, 2006) bring in and out of the model
feature groups; here feature similarity is whether features
belong in the same group or not (1/0 similarity) given by
domain knowledge. If yes their weight values will either
be all zero (out of the model) or different than zero (in the
model) but not necessarily the same. Fused lasso in addi-
tion to that pushes subsequent (connected) features to have
similar weights being even more similar to LR. Such lasso-
based regularisers are also applicable only in linear models
where we have a clear grasp of how each feature is treated
by the model. It is not clear how to use them on non-linear
models. The analytical approach can be adapted easily for
such regularisers making the approach interesting/relevant
for regularisers other than the Laplacian. On a similar note
Rosasco et al., 2013, applied lasso regularisation on the Ja-
cobian of kernel-based models to learn non-linear models
that are sparse with respect to the input features.

The Taylor expansion in the analytical approximation of the
regulariser brings in the Jacobian of the model. Regularis-
ers that use the Jacobian have previously been successfully
used to control the stability/robustness of models to noisy
inputs. Relevant work includes contractive auto encoders,
(Rifai et al., 2011b), and the work of (Zhai & Zhang, 2015).
(Rifai et al., 2011b) use the Frobenius of the Jacobian at the
input instances to force the model to be relatively constant
in small neighbors around the input instances. Such a reg-
ulariser introduces invariance to small input variations.

Optimizing the Jacobian in networks with more than one
layer is cumbersome, thus very often the stochastic ap-
proach is preferred over the analytic e.g. (Zhai & Zhang,
2015). Denoising autoencoders, (Vincent et al., 2010) fol-
low the stochastic paradigm and require that small random
variations in the inputs have only a limited effect on the
model output. Zheng et al., 2016, used Gaussian pertur-
bations to stabilise the network’s output with respect to
variations in the input, essentially augmenting the training
data. Regularisers on higher order derivatives, Hessian, are
also used, (Rifai et al., 2011a), in such cases the stochas-
tic approach is the only choice due to the prohibitive cost
Data augmentation is
of optimizing the Hessian term.

Regularising Non-linear Models Using Feature Side-information

also used when we have additional prior knowledge on the
instance structures to which the models should be invari-
ant. In imaging problems such structures include transla-
tions, rotations, scalings,etc, (Simard et al., 1991; Decoste
& Sch¨olkopf, 2002). (Corduneanu & Jaakkola, 2002) learn
models with built-in invariance by imposing class invari-
ance in dense regions of the feature space; we impose in-
variance to directions determined by the side information.

Closer to our work are (Krupka & Tishby, 2007) and the
works that use Laplacian based regularisers for model reg-
ularisation, e.g. (Huang et al., 2011). However to the best
of our knowledge all previous work was strictly limited to
linear models. We show how to apply such regularisers and
constraints to general classes of non-linear models.

4. Experiments

We will experiment and evaluate our regularisers in two
settings, a synthetic and a real world one. We will compare
the analytical and the stochastic regulariser, which we will
denote by AN and ST respectively, against popular regu-
larisers used with neural networks, namely ℓ2 and Dropout
(Srivastava et al., 2014), over different network architec-
tures. In the real world datasets we also give the results of
the Word Mover’s Distance, WMD, (Kusner et al., 2015)
which makes direct use of the side-information to compute
document distances. Obviously our regularisers and WMD
have an advantage over ℓ2 and dropout since it exploit side-
information which ℓ2 and dropout do not.

We trained both the analytical and the stochastic models,
as well as all baselines against which we compare, using
Adam (Kingma & Ba, 2014). We used α = 0.001,β 1 =
0.9,β 2 = 0.999 for one hidden layer networks, and α =
0.0001 for the networks with more hidden layers. We ini-
tialize all networks parameters using (Glorot & Bengio,
2010). Due to the large computational complexity of the
analytical approach we set the mini-batch size m to ﬁve.
For the stochastic model, as well as for all the baseline
models, we set the mini-batch size to 20. For the analyt-
ical model we set the maximum number of iterations to
5000. For the stochastic model we set the maximum num-
ber of iterations to 10000 for the one layer networks and
to 20000 for networks with more layers. We used early
stopping where we keep 20% of the training data as the
validation set. Every ﬁve parameter updates we compute
the validation error. Training terminates either if we reach
the maximum iteration number or the validation error keeps
increasing more than ten times in a row.

In the stochastic approach we do the sampling for the gen-
eration of the instance pairs within the stochastic gradi-
ent descent process. Concretely for each instance x in a
mini batch we randomly chose a feature pair i, j, from

the set of similar feature pairs. We sample a quadruple
from R respecting the constraint: λi+λj =
λi,λ j,λ ′i,λ ′j}
{
λ′i + λ′j from which we generate the respective instance
pairs. We repeat the process p times each time sampling a
new feature pair i, j, and a new quadruple. We ﬁx the set
of similar feature pairs to be the top 20% of most similar
feature pairs. Thus within each mini-batch of size m we
generate m
p instance pairs and we accumulate the norm
of the respective model output differences in the objective.
In the experiments we set p = 5

×

4.1. Artiﬁcial Datasets

×

∈

Rn

We design a simple data generation process in order to test
the performance of our regularisers when the data gener-
ation mechanism is compatible with the assumptions of
our models. We randomly generate an instance matrix
d by uniformly sampling instances from Rd. We
X
create d/2 feature clusters as follows. To each one of these
clusters we initially assign one of the original input features
without replacement. We assign randomly and uniformly
the remaining d/2 features to the clusters. We use the fea-
ture clusters to deﬁne a latent space where every feature
cluster gives rise to a latent feature. The value of each latent
feature is the sum of the values of the features that belong to
its cluster. On the latent space representation of the training
data we apply a linear transformation that projects the la-
tent space to a new space with lower dimensionality m. On
this lower dimensionality space we apply an element-wise
sigmoid and the ﬁnal class assignment is given by the index
of the maximum sigmoid value. The similarity Sij of the
i, j, features of the original space is 1 if they ended up in the
same cluster and 0 otherwise. We set d = 3000, n = 5000,
m = 5. We will call this dataset A1. The generating pro-
cedure gave a very sparse S matrix with only 0.04% of its
entries being non-zero. Each feature had an average of 1.3
similar features. We used 4000 instances for training and
the rest for testing. During training 20% of the instances
are used for the validation set. We train all algorithms on
the original input space. For all regularisers we used a net-
work with single hidden layer with 100 hidden units. We
tune the hyperparameters based on the performance on the
validation set. We select the λ hyperparameters of of AN,
; we select the λ of
ST, and ℓ2 from
}
−
dropout from [0.1, 0.2, 0.3, 0.4, 0.5]. We set the c in the
augmentation process, that controls the size of the neigbor-
hood within which the output constraints should hold, to
one.

10k
{

3, . . . , 3

k =

|

Both the analytical and the stochastic regulariser bring per-
formance improvements of roughly 10% when compared
to the ℓ2 regulariser and to Dropout, results in table 1. In
ﬁgure 2 we plot the error on the validation set as a function
of epoch number and the real time for the four regularisers.
The analytical and stochastic regulariser require consider-

Regularising Non-linear Models Using Feature Side-information

ably less epochs to converge and that to signigicantly lower
error values than either ℓ2 or dropout. However, since the
computational complexity of each step is larger, the real
time to convergence is larger than that of ℓ2 and dropout.

using exactly the same protocol as in A1. The results are
also given in table 1. As we see the classiﬁcation error of
both ST and AN increases as the dataset sparsity increases
and it approaches that of the standard regularisers.

ST
AN
ℓ2
Dropout

Dataset
A1
A2
A3

}|

= 0
Sij ̸
ST
|{
0.04% 43.70
0.021% 50.08
0.011% 56.20

AN
44.00
51.00
52.50

ℓ2 Dropout
53.60
55.30
55.90

52.70
55.40
54.50

3.4

3.2

3

2.8

2.6

2.4

2.2

2

r
o
r
r
e

 
t

e
s
 
n
o

i
t

a
d

i
l

a
V

1.8

0

ST
AN
ℓ2
Dropout

3.4

3.2

3

2.8

2.6

2.4

2.2

2

r
o
r
r
e
 
t
e
s
 
n
o
i
t
a
d

i
l

a
V

1.8

0

1000

2000
epoch number

3000

4000

2000

4000

6000

8000 10000 12000 14000

real time (seconds)

Figure2. Validation set error for the different regularisers: as a
function of epoch number (left), as a function of realtime (right).

The regularizer we propose constrains the model structure
by forcing the model to reﬂect the feature similarity as
these are given in the similarity matrix. Thus we expect the
structure of the similarity matrix to have an impact on the
performance of the regulariser. To see that let us consider
the trivial case in which S is diagonal.In this case the input
and the latent spaces are equivalent. and the regulariser has
no effect since there are no similarity constraints. If all fea-
tures are identical, i.e. Sij = 1,
i, j, then the latent space
will have a dimensionality of one, and the regulariser has
the strongest effect.

∀

To explore this dependency we generate two additional
synthetic datasets where we use the same generating mech-
anism as in A1 but vary the proportion of features we clus-
ter together to generate latent factors. Concretely in the
synthetic dataset we will call A2 we randomly select a set
A of d/2 features over which we will perform clustering
to deﬁne latent factors. We use the remaining set B of
d/2 features directly as they are in the latent space. We
cluster the features of the A set to d/4 clusters—latent fac-
tors, making sure that as in A1 each cluster has at least one
feature in it. As a result the ﬁnal latent space has a di-
mensionality of d/4 + d/2 = 3d/4. To generate the class
assignments we proceed as in A1. To generate the third
dataset, A3, we select d/4 features to generate A and the
remaining for B. We now cluster the features in A to d/8
clusters, again making sure that there is at least one fea-
ture pre cluster. The dimensionality of the latent space is
now d/8 + 3d/4 = 7d/8; class assignments are generated
as above. We used the same values for n, d, m as in A1.
As we move from A1 to A3 we reduce the number of fea-
tures that are similar to other features, thus we increase the
sparsity of S. For A2 and A3 the percentage of non-zero
elements is 0.021% and 0.011% respectively, compared to
0.04% we had in A1. So A1 is the datasets that has most
constraints while A3 is the one with the least constraints.
We apply the different regularisers in these two datasets

Table1. Classiﬁcation error, %, of the different regularizers, and
% of non zero elements of the similarity matrix S for the three
artiﬁcial datasets.

4.2. Real World Datasets

We evaluated both approaches on the eight document clas-
siﬁcation datasets used in (Kusner et al., 2015). We re-
moved all the words in the SMART stop word list (Salton
& Buckley, 1988). Documents are represented as bag of
words. To speed up training, we removed words that appear
very few times over all the documents of a dataset. Con-
cretely, in 20NEWS we reduce the dictionary size by re-
moving words with a frequency less or equal to three. In the
OHSUMED and CLASSIC datasets we remove words with
frequency one and the in REUTER dataset words with fre-
quency equal or less than two. As feature side-information
we use the word2vec representation of the words which
have a dimensionality of 300 (Mikolov et al., 2013).
In
table 2 we give a description of the ﬁnal datasets on which
we experiment including the number of classes (m) and av-
erage number of unique words per document.

Date set
BBCsport
Twitter
Classic
Amazon
20NEWS
Recipe
Ohsumed
Reuter

n
590
2486
5675
6400
11293
3496
3999
5485

d Unique words(avg) m
5
80.9
3
6
4
34.5
4
28.8
20
51.7
15
44.7
10
50
8
33

9759
4076
7628
4502
6859
4992
7643
5939

Table2. Dataset description

We compute the similarity matrix S from the word2vect
word representations using the heat kernel with bandwidth
parameter σ, i.e. the similarity of i, j, features is given by:
zj)T (zi −
Sij = exp(
zj)). We select σ so that
roughly 20% of the entries of the similarity matrix are in
[0.8, 1] interval.

2σ2 (zi −

−

1

For those datasets without a predeﬁned train/test split
(BBCsport, Twitter, Classic, Amazon, Recipe), we use ﬁve-
fold cross validation and report the average error. We con-
trol the statistical signiﬁcance of the results using the Mac-
Nemar’s test (p
value=0.05). We tune the hyperparame-

−

Regularising Non-linear Models Using Feature Side-information

ters with three-fold inner cross validation. We select the
λs of AN, ST, and ℓ2 from [0.001, 0.01, 0.1, 1, 10]; we se-
lect the λ of dropout from [0.1, 0.2, 0.3, 0.4, 0.5]. We did
a series of experiments in which we varied the number of
hidden layers. Due to the computational complexity of the
backprogation for the AN regulariser we only give results
for the single layer architecture.

In the ﬁrst set of experiments we use a neural network with
one hidden layer and 100 hidden units, we give the results
in table 3. ST is signiﬁcantly better than the AN in six out
of the eight datasets, signiﬁcantly worse once, and equiv-
alent in one dataset. ST is signiﬁcantly better than the ℓ2
in six out of the eight daasets, while it is equivalent in one.
Compared to dropout it is four times signiﬁcantly better
and two times signiﬁcantly worse.

When we increase the number of hidden layers to two with
500 and 100 units on the ﬁrst and second layer ST method
is signiﬁcantly better compared to ℓ2 three times, signiﬁ-
cantly worse three times, while there is no signiﬁcant dif-
ference in two datasets. A similar picture emerges with
respect to Dropout with ST being signiﬁcantly better three
times, signiﬁcantly worse twise, while in three cases there
is no signiﬁcant difference. We give the detailed results in
table 4.

Dataset
BBCsport
Twitter
Classic
Amazon
20NEWS
Recipe
Ohsumed
Reuter

ST
03.39-=-
26.90+++
03.54+++
06.25++-
19.58+++
38.76+++
34.45+==
03.84=+-

AN
02.17==
31.18=-
04.03+=
07.80=-
23.75=-
43.14–
35.75=-
03.38+=

ℓ2
02.72=
31.18-
05.13-
07.57-
23.21-
41.21-
35.26=
06.03-

Dropout WMD
02.17
28.44
03.98
06.44
21.31
39.88
34.39
03.20

04.6
29.00
02.80
07.40
27.00
43.00
44.00
03.50

Table3. Classiﬁcation error, %, with one hidden layer NNs. AN:
analytical approach, ST: stochastic approach. WMD results are
from (Kusner et al., 2015). The +,
and = signs give the signiﬁ-
cance test results of the comparison of the performance of a given
regulariser to those of the regularisers in the subsequent columns;
indicating respectively signiﬁcantly better, worse, no difference.
WMD is not included in the signiﬁcance comparison since at the
time of the experiments we did not have acces to the code.

−

Dataset
BBCsport
Twitter
Classic
Amazon
20NEWS
Recipe
Ohsumed
Reuter

ST
02.04=+
27.93–
03.71+=
05.96++
20.72++
41.53–
35.03==
04.39-=

ℓ2
02.85=
26.64=
04.51-
07.49-
22.49=
39.61=
34.95=
03.88=

Dropout WMD
04.60
02.99
29.00
26.74
02.80
03.69
07.40
06.75
27.00
22.48
43.00
39.31
44.00
35.14
03.50
04.16

Table4. Classiﬁcation error, %, with two hidden layers network.
Table interpretation as in table 3

5. Conclusion and Future Work

Many real world applications come with additional infor-
mation describing the properties of the features. Despite
that, quite limited attention has been given to such setting.
In this paper we develop a regulariser that exploits exactly
such information for general non-linear models. It relies on
the simple intuition that features which have similar prop-
erties should be treated by the learned model in a simi-
lar manner. The regulariser imposes a stability constraint
over the model output. The constraint forces the model to
produce similar outputs for instances the feature values of
which differ only on similar features. We give two ways
to approximate the value of the regulariser. An analytical
one which boils down to the imposition of a Laplacian reg-
ulariser on the Jacobian of the learned model with respect
to the input features and a stochastic one which relies on
sampling.

We experiment with neural networks with the two ap-
proximations of the regulariser and compare their perfor-
mance to well established model regularisers, namely ℓ2
and dropout, on artiﬁcial and real world datasets. In the
artiﬁcial datasets, for which we know that they match the
assumptions of our regulariser we demonstrate signiﬁcant
performance improvements. In the real world datasets the
performance improvements are less striking. One of the
main underlying assumptions of our model is that the fea-
ture side-information is indeed relevant for the learning
problem, when this is indeed the case we will have per-
formance improvements. If it is not the case then the regu-
lariser will not be selected, as a result of the tuning of the
λ parameter.

Along the same lines we want to perform a more detailed
study on how the structure of the similarity matrix, namely
its sparsity and the underlying feature cluster structure, de-
termines the regularisation strength of our regulariser. It is
clear that a sparse similarity matrix will lead to a rather lim-
ited regularisation effect since only few features will be af-
fected. This points to the fact that the regulariser should be
used together with more traditional sparsity inducing reg-
ularisers, especially in the case of a sparse feature simi-
larity matrix. Finally since we use the feature information
through a similarity function it might be the case that the
similarity function that we are using is not appropriate and
better results can be obtained if we also learn the feature
similarity. We leave this for future work.

Acknowledgement This work was supported by SNSF
#146801; H2020, RAWFIE #645220; IS-NET, HSTS.

Regularising Non-linear Models Using Feature Side-information

References

Bishop, Christopher. Exact calculation of the hessian ma-
trix for the multilayer perceptron. Neural Computation,
4:494–501, January 1992.

Bishop, Christopher M. Neural networks for pattern recog-

nition. Oxford university press, 1995.

Chiang, Kai-Yang, Hsieh, Cho-Jui, and Dhillon, Inderjit S.
Matrix completion with noisy side information. In Ad-
vances in Neural Information Processing Systems, pp.
3447–3455, 2015.

Chiang, Kai-Yang, Hsieh, Cho-Jui, and Dhillon, EDU In-
derjit S. Robust principal component analysis with side
information. In Proceedings of The 33rd International
Conference on Machine Learning, pp. 2291–2299, 2016.

Corduneanu, Adrian and Jaakkola, Tommi. On information
regularization. In Proceedings of the Nineteenth confer-
ence on Uncertainty in Artiﬁcial Intelligence, pp. 151–
158. Morgan Kaufmann Publishers Inc., 2002.

Decoste, Dennis and Sch¨olkopf, Bernhard. Training in-
variant support vector machines. Machine Learning, 46
(1):161–190, 2002. ISSN 1573-0565. doi: 10.1023/A:
1012454411458.

Glorot, Xavier and Bengio, Yoshua. Understanding the dif-
ﬁculty of training deep feedforward neural networks. In
Aistats, volume 9, pp. 249–256, 2010.

Huang, Jian, Ma, Shuangge, Li, Hongzhe, and Zhang, Cun-
Hui. The sparse laplacian shrinkage estimator for high-
dimensional regression. The Annals of Statistics, 39(4):
2021–2046, 2011. ISSN 00905364.

Kingma, Diederik and Ba,

Jimmy.
method for stochastic optimization.
arXiv:1412.6980, 2014.

Adam:

A
arXiv preprint

Krupka, Eyal and Tishby, Naftali.

Incorporating prior
In International
knowledge on features into learning.
Conference on Artiﬁcial Intelligence and Statistics, pp.
227–234, 2007.

Krupka, Eyal, Navot, Amir, and Tishby, Naftali. Learn-
ing to select features using their properties. Journal of
Machine Learning Research, 9(Oct):2349–2376, 2008.

Kusner, Matt J, Sun, Yu, Kolkin, Nicholas I, and Wein-
berger, Kilian Q. From word embeddings to document
distances. In Proceedings of the 32nd International Con-
ference on Machine Learning (ICML 2015), pp. 957–
966, 2015.

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed representations of
In Ad-
words and phrases and their compositionality.
vances in neural information processing systems, pp.
3111–3119, 2013.

Rao, Nikhil, Yu, Hsiang-Fu, Ravikumar, Pradeep K, and
Dhillon, Inderjit S. Collaborative ﬁltering with graph
information: Consistency and scalable methods. In Ad-
vances in Neural Information Processing Systems, pp.
2098–2106, 2015.

Rifai, Salah, Mesnil, Gr´egoire, Vincent, Pascal, Muller,
Xavier, Bengio, Yoshua, Dauphin, Yann, and Glorot,
Xavier. Higher order contractive auto-encoder. In Ma-
chine Learning and Knowledge Discovery in Databases
- European Conference, ECML PKDD 2011, Athens,
Greece, September 5-9, 2011, Proceedings, Part II, pp.
645–660, 2011a.

Rifai, Salah, Vincent, Pascal, Muller, Xavier, Glorot,
Xavier, and Bengio, Yoshua. Contractive auto-encoders:
In Pro-
Explicit invariance during feature extraction.
ceedings of the 28th international conference on ma-
chine learning (ICML-11), pp. 833–840, 2011b.

Rosasco, Lorenzo, Villa, Silvia, Mosci, Soﬁa, Santoro,
Matteo, and Verri, Alessandro. Nonparametric sparsity
and regularization. Journal of Machine Learning Re-
search, 14(1):1665–1714, 2013.

Salton, Gerard and Buckley, Christopher. Term-weighting
approaches in automatic text retrieval. Information pro-
cessing & management, 24(5):513–523, 1988.

Simard, Patrice Y., Victorri, Bernard, LeCun, Yann, and
Denker, John S. Tangent prop - A formalism for spec-
ifying selected invariances in an adaptive network.
In
Moody, John E., Hanson, Stephen Jose, and Lippmann,
Richard (eds.), Advances in Neural Information Process-
ing Systems 4, [NIPS Conference, Denver, Colorado,
USA, December 2-5, 1991], pp. 895–903. Morgan Kauf-
mann, 1991.

Srivastava, Nitish, Hinton, Geoffrey E, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:
a simple way to prevent neural networks from overﬁt-
Journal of Machine Learning Research, 15(1):
ting.
1929–1958, 2014.

Tibshirani, Robert, Saunders, Michael, Rosset, Saharon,
Zhu, Ji, and Knight, Keith. Sparsity and smoothness via
the fused lasso. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 67(1):91–108, 2005.

Vapnik, Vladimir and Izmailov, Rauf. Learning using priv-
ileged information: Similarity control and knowledge

Regularising Non-linear Models Using Feature Side-information

transfer.
2023–2049, 2015.

Journal of Machine Learning Research, 16:

Vincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Ben-
gio, Yoshua, and Manzagol, Pierre-Antoine. Stacked de-
noising autoencoders: Learning useful representations in
a deep network with a local denoising criterion. Journal
of Machine Learning Research, 11:3371–3408, 2010.

Xing, Eric P., Ng, Andrew Y., Jordan, Michael I., and Rus-
sell, Stuart J. Distance metric learning with application
to clustering with side-information. In Advances in Neu-
ral Information Processing Systems 15 [Neural Informa-
tion Processing Systems, NIPS 2002, December 9-14,
2002, Vancouver, British Columbia, Canada], pp. 505–
512, 2002.

Yuan, Ming and Lin, Yi. Model selection and estimation in
regression with grouped variables. Journal of the Royal
Statistical Society: Series B (Statistical Methodology),
68(1):49–67, 2006.

Zhai, Shuangfei and Zhang, Zhongfei.

Manifold
CoRR,
regularized discriminative neural networks.
abs/1511.06328, 2015. URL http://arxiv.org/
abs/1511.06328.

Zheng, Stephan, Song, Yang, Leung, Thomas, and Good-
fellow, Ian J.
Improving the robustness of deep neu-
ral networks via stability training. In 2016 IEEE Con-
ference on Computer Vision and Pattern Recognition,
CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp.
4480–4488, 2016.

