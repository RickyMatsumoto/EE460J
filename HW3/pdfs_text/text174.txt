Consistency Analysis for Binary Classiﬁcation Revisited

A. Proofs from Section 2

A.1. Proof of Proposition 1

For the sake of readability, throughout the proof we ab-
breviate Φ = Φ(u, v, p), Φ(cid:48) = Φ(u(cid:48), v(cid:48), p(cid:48)), and denote
∆u = u − u(cid:48), ∆v = v − v(cid:48), ∆p = p − p(cid:48). In this notation,
proving p-Lipschitzness for metric Φ amounts to showing
that:

|Φ − Φ(cid:48)| ≤ Up|∆u| + Vp|∆v| + Pp|∆p|,

for constants Up, Vp, Pp, which may only depend on p.

The following fact is going to be very useful in prov-
ing p-Lipschitzness. If the metric is of the rational form:
Φ(u, v, p) = A(u,v,p)
B(u,v,p) + C, where C is some constant,
B(u, v, p) ≥ Gp for some positive constant Gp (which
may depend on p), and |Φ(u, v, p)| ≤ Φmax for some con-
stant Φmax, it sufﬁces to check p-Lipschitzness of numer-
ator and denominator separately. Indeed, using shorthand
notation A = A(u, v, p), A(cid:48) = A(u(cid:48), v(cid:48), p(cid:48)), and similarly
for B, B(cid:48):

Φ − Φ(cid:48) =

A − A(cid:48)
B(cid:48) B
B
A − A(cid:48)
B

+

=

=

A − A(cid:48) + A(cid:48)
B(cid:48) B(cid:48) − A(cid:48)
B

B(cid:48) B

A(cid:48)
B(cid:48)

B − B(cid:48)
B

,

hence:

|Φ − Φ(cid:48)| ≤

|A − A(cid:48)|
Gp

+

Φmax
Gp

|B(cid:48) − B|.

a) Accuracy Φ(u, v, p) = 1 − v − p + 2u. We have:

Φ − Φ(cid:48) ≤ 2∆u − ∆v − ∆p,

so that by triangle inequality:

so that |B − B(cid:48)| ≤ 2|∆p|.

c) Jaccard similarity Φ(u, v, p) = u

p+v−u . Follows from
the rational form of the metric, since A(u, v, p) = u,
B(u, v, p) = p + v − u, C = 0, Φmax = 1, Gp = p,
and the p-Lipschitzness of A(u, v, p) and B(u, v, p) is
trivial to show by the triangle inequality.

d) G-mean Φ(u, v, p) = u(1−v−p+u)

. Exploiting the
rational form of the metric, we have A(u, v, p) =
u(1 − v − p + u), B(u, v, p) = p(1 − p), C = 0,
Φmax = 1, Gp = p(1 − p). The p-Lipschitzness of B
was shown above for AM measure. As for A:

p(1−p)

A − A(cid:48) = (1 − v − p + u)(u − u(cid:48))
+ u(cid:48)(u − p − v − u(cid:48) − p(cid:48) − v(cid:48))
= (1 − v − p + u)∆u + u(cid:48)(∆u − ∆v − ∆p),

and hence the p-Lipschitzness follows by triangle in-
equality and the fact that |1 − v − p + u| ≤ 2 and
|u(cid:48)| ≤ 1.

e) AUC (v−u)(p−u)

p(1−p)

. Exploiting the rational form of the
metric, we have A(u, v, p) = (v − u)(p − u) and
B(u, v, p) = p(1 − p). The p-Lipschitzness of B was
shown above for AM measure; as for A:

A − A(cid:48) = (v − u)(p − u) − (v(cid:48) − u(cid:48))(p − u)
+ (v(cid:48) − u(cid:48))(p − u) − (v(cid:48) − u(cid:48))(p(cid:48) − u(cid:48))
= (∆v − ∆u)(p − u) + (v(cid:48) − u(cid:48))(∆p − ∆u),

and hence the p-Lipschitzness follows by triangle in-
equality and the fact that |p−u| ≤ 1 and |v(cid:48) −u(cid:48)| ≤ 1.

|Φ − Φ(cid:48)| ≤ 2|∆u| + |∆v| + |∆p|.

f) Linear-fractional metric of the form:

Hence, the statement follows with Up = 2, Vp =
Pp = 1.

b) AM Φ(u, v, p) = 1 − vp−u

2p(1−p) . We can use the re-
sult on the rational metric by noting that A(u, v, p) =
u − vp, B(u, v, p) = B(p) = 2p(1 − p), C = 1,
Φmax = 1, Gp = 2p(1 − p). We can now check the
p-Lipschitzness of A and B separately:
A − A(cid:48) = u − vp − u(cid:48) + v(cid:48)p(cid:48)

= ∆u + (vp(cid:48) − vp) + (v(cid:48)p(cid:48) − vp(cid:48))
= ∆u − v∆p − p(cid:48)∆v,

and since |v| ≤ 1, |p(cid:48)| ≤ 1, p-Lipschitzness follows
from triangle inequality. For the denominator,
B − B(cid:48) = 2p(1 − p) − 2p(cid:48)(1 − p(cid:48))
= 2(p − p(cid:48)) + 2(p(cid:48)2 − p2)
= 2(1 − p(cid:48) − p)(p − p(cid:48)),

Φ(u, v, p) =

a1 + a2u + a3v + a4p
b1 + b2u + b3v + b4p

,

as long as the denominator is bounded from below by
some positive constant Gp. This follows immediately
from the rational form of the metric, as the numera-
tor A(u, v, p) and denominator B(u, v, p) are linear
functions of (u, v, p), so showing p-Lipschitzness of
A(u, v, p) and B(u, v, p) is straightforward.

B. Proofs from Section 3.1

B.1. Proof of Lemma 1

We ﬁx classiﬁer h and use a shorthand notation u, v, (cid:98)u, (cid:98)v
to denote u(h), v(h), (cid:98)u(h), (cid:98)v(h). Due to the Lipschitz as-
sumption:

|Φ(u, v, p)−Φ((cid:98)u, (cid:98)v, (cid:98)p)| ≤ Up|u−(cid:98)u|+Vp|v−(cid:98)v|+Pp|p− (cid:98)p|.

Consistency Analysis for Binary Classiﬁcation Revisited

Fixing x = (x1, . . . , xn) and taking expectation with re-
spect to y = (y1, . . . , yn) conditioned on x, we have:

Ey|x

(cid:2)|Φ(u, v, p) − Φ((cid:98)u, (cid:98)v, (cid:98)p)|(cid:3)
≤ UpEy|x

(cid:2)|u − (cid:98)u|(cid:3) + Vp|v − (cid:98)v| + PpEy|x

(cid:2)|p − (cid:98)p|(cid:3) .

Denote:

We have:

Ey|x

(cid:2)|p − (cid:98)p|(cid:3) = Ey|x

(cid:101)p = Ey|x [(cid:98)p] =

1
n

n
(cid:88)

i=1

η(xi),

(cid:101)u = Ey|x [(cid:98)u] =

h(xi)η(xi)

1
n

n
(cid:88)

i=1

≤ |p − (cid:101)p| + Ey|x
= |p − (cid:101)p| + Ey|x

(cid:2)|p − (cid:101)p + (cid:101)p − (cid:98)p|(cid:3)
(cid:2)|(cid:101)p − (cid:98)p|(cid:3)
(cid:104)(cid:112)((cid:101)p − (cid:98)p)2
(cid:2)((cid:101)p − (cid:98)p)2(cid:3)

Ey|x

(cid:113)

≤ |p − (cid:101)p| +

(cid:105)

(cid:113)

= |p − (cid:101)p| +

Vary|x((cid:98)p) ≤ |p − (cid:101)p| +

(cid:114) 1
4n

,

where the second inequality follows from Jensen’s inequal-
ity applied to a concave function x (cid:55)→
x. In an analogous
way, one can show that:

√

Ey|x

(cid:2)|u − (cid:98)u|(cid:3) ≤ |u − (cid:101)u| +

≤ |u − (cid:101)u| +

(cid:114) u
4n

(cid:114) 1
4n

.

Furthermore, using the convexity of the absolute value
function, Jensen’s inequality implies:

(cid:12)
(cid:12)Φ(u, v, p) − Ey|x
(cid:12)
≤ Ey|x

(cid:2)Φ((cid:98)u, (cid:98)v, (cid:98)p)(cid:3) (cid:12)
(cid:12)
(cid:12)
(cid:2)|Φ(u, v, p) − Φ((cid:98)u, (cid:98)v, (cid:98)p)|(cid:3) ,

so that:
(cid:12)
(cid:12)Φ(u, v, p) − Ey|x
(cid:12)

(cid:2)Φ((cid:98)u, (cid:98)v, (cid:98)p)(cid:3)(cid:12)

(cid:12)
(cid:12) ≤ Up|u − (cid:101)u| + Vp|v − (cid:98)v|
Up + Vp
√
2

+ Pp|p − (cid:101)p| +

n

.

We will now show that under the class of thresholded func-
tions H speciﬁed in the statement of the theorem to which
h belongs, all the terms on the right-hand side are well con-
trolled. The rest of the proof follows in a straightforward
way from Hoeffding’s inequality and Vapnik-Chervonenkis
bounds, except for minor, technical details, which are in-
cluded for completeness.

We ﬁrst apply Hoeffding’s inequality to say that with prob-
ability at least 1 − δ/2,

|p − (cid:101)p| ≤

(cid:115)

log 4
δ
2n

.

Similarly, using standard Rademacher complexity argu-
ments (see, e.g. Mohri et al., 2012), we have, uniformly
over all h ∈ H, with probability 1 − δ/4,
(cid:115)

|v − (cid:98)v| ≤ 2Ex

(cid:2)Rn(H)(cid:3) +

and similarly, with probability 1 − δ/4,

|u − (cid:101)u| ≤ 2Ex

(cid:2)Rn(Hη)(cid:3) +

where Hη = {h · η : h ∈ H}, and:

log 4
δ
2n

,

(cid:115)

log 4
δ
2n

,

Rn(H) = Eσ

(cid:20)

sup
h∈H

(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:21)

(cid:12)
(cid:12)
σih(xi)
(cid:12)

is the Rademacher complexity6 of H. Furthermore, if we
let zi ∈ {−1, 1}, i = 1, . . . , n, with Pr(zi = 1) = 1+η(xi)
,
so that E [zi] = η(xi), we have:

2

n
(cid:88)

i=1

σih(xi)η(xi) = Ez

(cid:104) n
(cid:88)

i=1

σih(xi)zi

(cid:105)
,

so that:

Rn(Hη) = Eσ

(cid:12)
(cid:12)
(cid:12)

1
n

Ez

(cid:104) n
(cid:88)

i=1

σih(xi)zi

(cid:21)

(cid:105)(cid:12)
(cid:12)
(cid:12)

(cid:20)

sup
h∈H

(cid:20)

≤ Eσ,z

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)

1
n

sup
h∈H

σih(xi)zi

(cid:21)

(cid:12)
(cid:12)
(cid:12)

= Eσ

(cid:20)

(cid:12)
(cid:12)
(cid:12)

1
n

sup
h∈H

i=1
n
(cid:88)

i=1

(cid:21)

(cid:12)
(cid:12)
σih(xi)
(cid:12)

= Rn(H),

where the inequality is due to Jensen’s inequality applied to
convex functions | · | and sup{·}, and the second equality is
due to the fact that σizi and σi are distributed in the same
way.

Thus choosing Lp = max{Up, Vp, Pp}, with probability
1 − δ, uniformly over all h ∈ H,

(cid:12)
(cid:12)Φ(u, v, p) − Ey|x
(cid:12)

(cid:2)Φ((cid:98)u, (cid:98)v, (cid:98)p)(cid:3)(cid:12)

(cid:12) ≤ 4LpEx
(cid:12)
(cid:115)

+ 3Lp

(cid:2)Rn(H)(cid:3)

log 4
δ
2n

+

Lp√
n

.

Now, if H is the class of threshold functions on η, its
growth function (Mohri et al., 2012) is equal to m + 1, and
thus we have7:

Rn(H) ≤

(cid:114)

2 log(n + 1)
n

,

6Variables σi, i = 1, . . . , n, are i.i.d. Rademacher variables

distributed according to P(σi = 1) = P(σi = −1) = 1
2 .

7We could alternatively use the fact that VC-dimension of H
is 1, which would give a bound with log(n + 1) replaced by 1 +
log(n).

Consistency Analysis for Binary Classiﬁcation Revisited

so that with probability 1−δ, uniformly over all h ∈ H, we
get the bound in the statement of the theorem. The proof is
complete.

√

(cid:2)Φ((cid:98)u, (cid:98)v, (cid:98)p)(cid:3)(cid:12)

Lower bound. The dependence ˜O(1/
n) on the sam-
ple size stated in Lemma 1 cannot be improved in general.
To see this, take a metric Φ(u, v, p) = u, p-Lipschitzness
of which is trivial to show. Choose h(x) = 1 for all
x. Then, u(h) = p, while (cid:98)u(h) = 1
i=1 yi. Hence,
(cid:12)
(cid:12) = (cid:12)
(cid:12)Φ(u, v, p) − Ey|x
(cid:12)
(cid:12)
(cid:12), where (cid:101)p =
i=1 η(xi) and Ex [(cid:101)p] = p. Assume that η(x) follows
1
n
a binomial distribution with P(η(x) = 1) = P(η(x) =
0) = 1
2 . Denote |p − (cid:101)p| by Z. By Khinchine in-
equality, E [Z] ≥ 2c(cid:112)E [Z 2] = c/
n for some con-
stant c > 0. Furthermore, by Paley-Zygmund inequality
P(Z > E [Z] /2) ≥ (E[Z])2
4E[Z2] ≥ c2. Hence, with constant
probability,

(cid:80)n
(cid:12)p − (cid:101)p(cid:12)

(cid:80)n

√

n

(cid:12)
(cid:12)Φ(u, v, p) − Ey|x
(cid:12)

(cid:2)Φ((cid:98)u, (cid:98)v, (cid:98)p)(cid:3)(cid:12)
(cid:12)
(cid:12) ≥

c
√
2

,

n
√

for some c > 0, which shows that the rate ˜O(1/
be improved.

n) cannot

B.2. Proof of Theorem 1

First, note that for a given P, p-Lipschitzness implies
that Φ(u, v, p) is continuous as a function of (u, v). Let
H = {hη | hη = 1η(x)≥η, η ∈ [0, 1]} be the set of bi-
nary threshold functions on η(x). By Assumption 1, u(hη)
and v(hη) are continuous in the threshold η, and hence the
maximizer of Φ(u, v, p) over H exists due to compactness
of the domain of η. The existence of the maximizer, to-
gether with Assumption 1 and TP monotonicity implies by
(Narasimhan et al., 2014a, Lemma 11) that h∗
PU ∈ H, i.e.
the optimal PU classiﬁer is a threshold function.8.

For any given x = (x1, . . . , xn), let h∗
ETU(x) be the opti-
mal ETU classiﬁer. By TP monotonicity of Ψ, (Natarajan
et al., 2016, Theorem 1) implies that h∗

ETU(x) satisﬁes:

max
i=1,...,n

{η(xi) : h∗

ETU(xi) = 0}

≤ min

i=1,...,n

{η(xi) : h∗

ETU(xi) = 1}.

However, by Assumption 1, η(xi)
(cid:54)= η(xj) for all
i (cid:54)= j with probability one, so that the condition above
is satisﬁed with strict inequality, and hence there exists
τ ∗, which is between max{η(xi) : h∗
ETU(xi) = 0} and
min{η(xi) : h∗
ETU(x)

ETU(xi) = 1}. This means that h∗

8Lemma 11 of Narasimhan et al. (2014a) requires that the PU
maximizer within H is hη for some η ∈ (0, 1). However, we do
not impose this constraint here because the lemma can easily be
extended to the case η ∈ [0, 1] under our assumption that η(x)
has a density over [0, 1].

is a threshold function on η(x) with threshold τ ∗, i.e.
h∗
ETU ∈ H.
To conclude, with probability one, h∗
(cid:113) 2 log(n+1)
n

2n + Lp√
Now, deﬁne (cid:15)/2 = 4Lp
n .
Then, with probability 1 − δ (over the random choice of x),

ETU(x), h∗

PU ∈ H.

(cid:113) log 4

+ 3Lp

δ

Φ(u(h∗

ETU(x)), v(h∗
PU), v(h∗
(cid:2)Φ((cid:98)u(h∗
(cid:2)Φ((cid:98)u(h∗
ETU(x)), v(h∗

ETU(x)), p)
PU), p)
PU), (cid:98)v(h∗
ETU(x)), (cid:98)v(h∗

≤ Φ(u(h∗
≤ Ey|x
≤ Ey|x
≤ Φ(u(h∗

PU), (cid:98)p)(cid:3) + (cid:15)/2

ETU(x)), (cid:98)p)(cid:3) + (cid:15)/2,

ETU(x)), p) + (cid:15),

where we used Lemma 1 twice in the second and fourth
inequality. Hence, with probability 1 − η,

(cid:12)
(cid:12)Φ(u(h∗
(cid:12)

ETU(x)), v(h∗

ETU(x)), p)

− Φ(u(h∗

PU), v(h∗

(cid:12)
(cid:12)
(cid:12) ≤ (cid:15).
PU), p)

Using analogous argument, one can show that with proba-
bility 1 − δ,

(cid:12)
(cid:12)
(cid:12)

Ey|x

(cid:2)Φ((cid:98)u(h∗
− Ey|x

ETU(x)), (cid:98)v(h∗
(cid:2)Φ((cid:98)u(h∗

PU), (cid:98)v(h∗

ETU(x)), (cid:98)p)(cid:3)
PU), (cid:98)p)(cid:3) (cid:12)
(cid:12)
(cid:12) ≤ (cid:15),

which ﬁnishes the proof.

B.3. Finite Sample Regime: Proof of Theorem 2

The PU-optimal classiﬁer is:

ΦPrec(u(h), v(h), p) = argmax

u(h)
v(h) + α

.

h

h∗
PU = argmax

h

Proposition 2.

h∗
PU(x) =

(cid:40)

1, if x ∈ X1,
0, else .

Proof. Note that for the deﬁned h∗
u(h∗

PU) = P(X1), and

PU) = v(h∗

PU classiﬁer, we have

ΦPrec(u(h∗

PU), v(h∗

PU), p) =

P(X1)
P(X1) + α

.

Firstly, observe that for any candidate optimal classiﬁer
h(cid:48), it must hold that h(cid:48)(x) = 0 for all x ∈ X3 (other-
wise the metric strictly decreases). Now, suppose there ex-
ists a classiﬁer h(cid:48)
PU which has strictly higher util-
PU. Then, it must be that h(cid:48)(x) = 1 for all
ity than h∗

(cid:54)= h∗

Consistency Analysis for Binary Classiﬁcation Revisited

x ∈ X2. We have, u(h(cid:48)) = P(X1) + P(X2)(1 −
and v(h(cid:48)) = P(X1) + P(X2). So:

√

α)

ΦPrec(u(h(cid:48)), v(h(cid:48)), p) =

P(X1) + P(X2)(1 −

P(X1) + P(X2) + α

√

α)

.

But for the chosen small value of α, we can show the con-
tradiction that:

ΦPrec(u(h(cid:48)), v(h(cid:48)), p) < ΦPrec(u(h∗

PU), v(h∗

PU), p).

Therefore, h∗

PU as stated is indeed optimal.

We see from the above constructed example that the PU
optimal classiﬁer assigns negative labels to 50% of the data
which are highly likely to belong to the positive class. PU
is sensitive to label noise if the metric is less stable as im-
plied by the high p-Lipschitz constant. Next, we show that
ETU is relatively more robust.

Given a set of instances x = {x1, x2, . . . , xn}, recall that
the ETU-optimal assignments can be computed as:

ETU(x) = s∗ := argmax
h∗
s∈{0,1}n

Ey∼P(.|x)ΦPrec(s, y) .

Proposition 3. On the subset of instances in x that have
deterministic labels, the ETU-optimal predictions satisfy:

ETU(xj) = s∗
h∗

j =

(cid:40)

1, if x ∈ X1,
0, if x ∈ X3 .

Proof. Let Ii = {j : xj ∈ Xi}, for i = 1, 2, 3. Note that
the optimal value at the solution s∗ is given by:

(cid:80)

(cid:80)

j∈I1

Ey∼P(.|x)ΦPrec(s∗, y) =

j + ∆(s∗
s∗
I2
j + (cid:80)
s∗

, yI2 )
s∗
j + αn
(2)
indicates the optimal assignments corresponding
, yI2) is a quantity that depends

where s∗
I2
to indices in I2 and ∆(s∗
I2
only on indices in I2, and is given by:

j∈I1∪I3

j∈I2

,

∆(s∗
I2

, yI2) =

(cid:88)

P(yI2)(cid:104)yI2, s∗
I2

(cid:105)

(3)

yI2 ∈{0,1}|I2|

Fixing the optimal predictions for indices corresponding to
I2, the value (2) is maximized by maximizing the numer-
ator term (cid:80)
s∗
j and minimizing the denominator term
j∈I1
(cid:80)
s∗
j . This is achieved precisely when the opti-
mal solution satisﬁes the statement in the proposition. The
proof is complete.

j∈I1∪I3

We know from Proposition 2 that h∗
PU sets the labels corre-
sponding to indices in the set I2 to 0. Now let us examine
what happens in the case of ETU, when labels have mild
(cid:15), the label of an
noise (i.e. with some small probability
instance from X2 can be 0), at optimality. Consider a can-
didate optimal solution s(cid:48) that behaves exactly like h∗
PU, i.e.
s(cid:48)
j = 0 for all j ∈ I2, for some 1 ≤ k ≤ |I2|.
Then, ∆(s(cid:48)
I2

, yI2) = 0, so:

√

Ey∼P(.|x)ΦPrec(s(cid:48), y) =

|I1|
|I1| + αn

.

Now, consider another candidate solution s(cid:48)(cid:48) that is equal
to s(cid:48), but has a value of 1 corresponding to a subset of in-
dices j1, j2, . . . , jk ∈ I2. The value of this solution can be
shown to be:

Ey∼P(.|x)ΦPrec(s(cid:48)(cid:48), y) =

|I1| + k(1 − (cid:15))
|I1| + k + αn

.

Comparing equations (4) and (5), we have that if:

(4)

(5)

(6)

(cid:15) <

αn
|I1| + αn

,

then s(cid:48)(cid:48) is a strictly better solution than s(cid:48). In particular, as
(5) is mononotic in k, the optimal choice is k = |I2|. This
immediately leads to the following corollary.

Corollary 1.

1. If |I2| = 0, then

ETU(x) := s∗ = h∗
h∗

PU(x) .

ETU(x) := s∗ (cid:54)= h∗
h∗

PU(x) .

In particular, h∗
ETU assigns label 1 to all instances that
are overwhelmingly positive under P, corresponding
to indices I2, whereas h∗

PU assigns label 0.

3. If |I1| = 0, but |I2| > 0 then for any 0 < (cid:15) < 1,

ETU(x) := s∗ (cid:54)= h∗
h∗

PU(x) := 0 .

√

Note that (cid:15) < α/(1 + α) does not hold for our choice of
α. However, case 3 in Corollary 1 is sufﬁcient to es-
(cid:15) =
tablish the bound in Theorem 2, when P(X2) is very large.

C. Proofs for Section 4.1

Fix a binary classiﬁer h : X → {0, 1} and let the input
sample x = (x1, . . . , xn) be generated i.i.d. from P. For
the sake of clarity, abbreviate η(xi) = ηi and h(xi) = hi,
i = 1, . . . , n. In the proofs of Lemma 2 and Lemma 3 we
will use the following:

Note that the predictions coincide with that of h∗
indices.

PU on these

2. Otherwise, if (cid:15) < α

1+α , then

Consistency Analysis for Binary Classiﬁcation Revisited

• Empirical quantities:

(cid:98)u(h) =

hiyi, (cid:98)v(h) =

hi, (cid:98)p =

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

yi,

1
n

n
(cid:88)

i=1

• Semi-empirical quantities:

(cid:101)u(h) =

hiηi,

and (cid:101)p =

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

ηi

(we do not deﬁne (cid:101)v(h), as it would the same as (cid:98)v(h)).

where we used the independence of labels yi, i = 1, . . . , n.
(cid:2)((cid:98)p − (cid:101)p)2(cid:3) is at most 1
Similarly, Ey|x
4n , which in total
gives:

Ey|x

(cid:104)
((cid:98)z − (cid:101)z)(cid:62)∇2Φ((cid:101)z)((cid:98)z − (cid:101)z)

(cid:105)

≤

A
n

.

Using a lower bound −A on the second-order derivatives
and performing a similar chain of reasoning, one also gets:

Ey|x

(cid:104)

(cid:105)
((cid:98)z − (cid:101)z)(cid:62)∇2Φ((cid:101)z)((cid:98)z − (cid:101)z)

≥ −

A
n

.

Note that:

(cid:101)u(h) = Ey|x

(cid:98)u(h)(cid:3) ,
(cid:2)

and (cid:101)p = Ey|x [(cid:98)p] .

We will jointly denote (cid:98)z = ((cid:98)u(h), (cid:98)p), and similarly
(cid:101)z = ((cid:101)u(h), (cid:101)p). We will also abbreviate Φ((cid:98)z) =
Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p) and similarly for Φ((cid:101)z).

From that we have:

(cid:107)Ey|x

(cid:2)Φ((cid:98)z)(cid:3) − Φ((cid:101)z)(cid:107) ≤

A
2n

,

which is exactly what was to be shown.

C.2. Proof of Lemma 3

C.1. Proof of Lemma 2

Assume Φ is two-times differentiable, with all partial
second-order derivatives bounded by A. Taylor expanding
Φ((cid:98)z) around point (cid:101)z up to the second order gives:

Assume Φ is three-times differentiable, with all partial
third-order derivatives bounded by B. Taylor expanding
Φ((cid:98)z) around point (cid:101)z up to the third order gives:
Φ((cid:98)z) = Φ((cid:101)z) + ∇Φ((cid:101)z)(cid:62)((cid:98)z − (cid:101)z)

Φ((cid:98)z) = Φ((cid:101)z) + ∇Φ((cid:101)z)(cid:62)((cid:98)z − (cid:101)z)

+

1
2

((cid:98)z − (cid:101)z)(cid:62)∇2Φ(z)((cid:98)z − (cid:101)z)

for some z between (cid:98)z and (cid:101)z. Note that Ey|x [(cid:98)z] = (cid:101)z, so
that:

Ey|x

(cid:105)
(cid:104)
∇Φ((cid:101)z)(cid:62)((cid:98)z − (cid:101)z)

= 0.

Furthermore, note that:

((cid:98)z − (cid:101)z)(cid:62)∇2Φ(z)((cid:98)z − (cid:101)z)
uu((cid:98)u − (cid:101)u)2 + 2∇2

= ∇2
up((cid:98)u − (cid:101)u)((cid:98)p − (cid:101)p) + ∇2
≤ A(cid:0)((cid:98)u − (cid:101)u)2 + 2|((cid:98)u − (cid:101)u)((cid:98)p − (cid:101)p)| + ((cid:98)p − (cid:101)p)2(cid:1)
≤ 2A(cid:0)((cid:98)u − (cid:101)u)2 + ((cid:98)p − (cid:101)p)2(cid:1),

pp((cid:98)p − (cid:101)p)2

where we used elementary inequality ab ≤ a2 + b2, and
∇2
pp denote the second-order derivatives evalu-
ated at some z = (u, p). Hence:

uu, ∇2

up, ∇2

Ey|x

(cid:104)
((cid:98)z − (cid:101)z)(cid:62)∇2Φ((cid:101)z)((cid:98)z − (cid:101)z)
((cid:98)u − (cid:101)u)2(cid:105)
(cid:104)

Ey|x

≤ 2A

(cid:18)

(cid:105)

+

+

1
2

1
6

((cid:98)z − (cid:101)z)(cid:62)∇2Φ((cid:101)z)((cid:98)z − (cid:101)z)
∂3Φ(z)
∂zα∂zβ∂zγ

2
(cid:88)

α,β,γ=1

((cid:98)zα − (cid:101)zα)((cid:98)zβ − (cid:101)zβ)((cid:98)zγ − (cid:101)zγ),

for some z between (cid:98)z and (cid:101)z. First note that Ey|x [(cid:98)z] = (cid:101)z,
so that:

Ey|x

(cid:105)
(cid:104)
∇Φ((cid:101)z)(cid:62)((cid:98)z − (cid:101)z)

= 0.

Furthermore,

Ey|x

(cid:105)
(cid:104)
∇2((cid:98)z − (cid:101)z)(cid:62)Φ((cid:101)z)((cid:98)z − (cid:101)z)
(cid:16)

(cid:20)

= Ey|x

tr

∇2Φ((cid:101)z)((cid:98)z − (cid:101)z)((cid:98)z − (cid:101)z)(cid:62)(cid:17)(cid:21)

= tr

,

(cid:17)
(cid:16)
∇2Φ((cid:101)z)Σ
(cid:2)((cid:98)z − (cid:101)z)((cid:98)z − (cid:101)z)(cid:62)(cid:3) is the covariance ma-

where Σ = Ey|x
trix of (cid:98)z − (cid:101)z. By independence of examples,

+ Ey|x

(cid:104)

((cid:98)p − (cid:101)p)2(cid:105)(cid:19)

.

Σ =

Eyi|xi

(cid:34)(cid:18)hi(yi − ηi)2 hi(yi − ηi)2
(yi − ηi)2

(cid:19)(cid:35)

1
n2

1
n2

n
(cid:88)

i=1
n
(cid:88)

i=1

=

ηi(1 − ηi)

hi(yi − ηi)2
(cid:18)hi hi
1
hi

(cid:19)

,

Since (cid:98)u is the empirical average over n labels and (cid:101)u is its
(cid:2)((cid:98)u − (cid:101)u)2(cid:3) is the vari-
expectation (over the labels), Ey|x
ance of (cid:98)u, which is at most 1

4n , because (cid:98)u ∈ [0, 1]:

var((cid:98)u) =

1
n2

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

var(hiyi) ≤

hiηi(1 − ηi) ≤

so that:

1
4n

,

(cid:16)

tr

∇2Φ((cid:101)z)Σ

(cid:17)

= (∇2

uu + 2∇2

up)su + ∇2

ppsp,

Consistency Analysis for Binary Classiﬁcation Revisited

where:

sp :=

ηi(1 − ηi),

su :=

hiηi(1 − ηi),

1
n2

1
n2

n
(cid:88)

i=1
n
(cid:88)

i=1

up, ∇2

uu, ∇2

and ∇2
pp denote be the second-order derivative
terms evaluated at ((cid:101)u, (cid:101)p). Thus, to ﬁnish the proof, we
only need to show that the ﬁrst order term is bounded by
B
3 n−3/2. To this end, note that for any numbers ai, bijk,
such that |bijk| ≤ B, i, j, k = 1, . . . , 2:

bijkaiajak ≤ B

|ai||aj||ak| = B(|a1| + |a2|)3.

(cid:88)

ijk

(cid:88)

ijk

By H¨older’s inequality,

2
(cid:88)

i=1

(cid:18) 2

(cid:88)

(cid:19)1/3

|ai|3

22/3,

|ai| ≤

i=1

so that:

B(|a1| + |a2| + |a3|)3 ≤ 4B

(cid:16)

|a1|3 + |a2|3 + |a3|3(cid:17)

.

Hence, if we bound:

∂3Φ(z)
∂zα∂zβ∂zγ

≤ B,

the third-order term 1
6

(cid:80)2

α,β,γ=1 . . . is bounded by:

(cid:16)

2B
3

|(cid:98)u − (cid:101)u|3 + |(cid:98)p − (cid:101)p|3(cid:17)
(cid:2)|(cid:98)u − (cid:101)u|3(cid:3) and Ey|x

We now bound Ey|x
Cauchy-Schwarz inequality,

Ey|x

(cid:104)

|(cid:98)p − (cid:101)p|3(cid:105)

≤

(cid:113)

Ey|x

(cid:2)((cid:98)p − (cid:101)p)4(cid:3)(cid:113)

Ey|x

(cid:2)((cid:98)p − (cid:101)p)2(cid:3).

Before, we already showed that

(cid:104)

Ey|x

((cid:98)p − (cid:101)p)2(cid:105)

1
4n
Denote ai = yi − ηi, and let µk = Ey|x
0, we have:

≤

.

(cid:2)ak

i

(cid:3). Using µ1 =

Ey|x

(cid:104)

((cid:98)p − (cid:101)p)4(cid:105)

=

1
n4

(cid:88)

i,j,k,(cid:96)

aiajaka(cid:96)

nµ4 + 3n(n − 1)µ2
2

(cid:17)

.

(cid:16)

=

1
n4
4 and µ4 ≤ 1

12 , Ey|x

(cid:2)((cid:98)p − (cid:101)p)4(cid:3) ≤ 3

16n2 , and

Ey|x

(cid:104)

|(cid:98)p − (cid:101)p|3(cid:105)

≤

√

3
8

n−3/2 ≤

n−3/2.

1
4

Since µ2 ≤ 1
thus:

(cid:2)|(cid:98)u − (cid:101)u|3(cid:3), we conclude that
Using similar bound for Ey|x
the third-order term is bounded by B
3 n−3/2. Bounding the
third-order derivatives from below by −B, and using simi-
lar reasoning gives a lower bound of the same value. This
ﬁnishes the proof.

C.3. Proof of Theorem 3

Abbreviating Φ(h) = Ey|x
Φa(h) = Φappr(h):
ETU) − Φ(h∗

Φ(h∗

a) = Φ(h∗
(cid:124)

ETU) − Φa(h∗
(cid:123)(cid:122)
≤ B
3n3/2

ETU)
(cid:125)

(cid:2)Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p)(cid:3) and

Φa(h∗
(cid:124)

ETU) − Φa(h∗
a)
(cid:125)

+ Φa(h∗
(cid:124)

a) − Φ(h∗
a)
(cid:125)

≤

(cid:123)(cid:122)
≤0

(cid:123)(cid:122)
≤ B
3n3/2

2B
3n3/2

,

where the bounds shown in the inequalities are from
Lemma 3.

C.4. Derivation of the approximation algorithm for

Fβ-measure

Recall that Fβ(u, v, p) = (1+β2)u
derivatives with respect to u and p are:

β2p+v . The seconder order

=

∂2Fβ
∂u∂p

∂2Fβ
−β2(1 + β2)
2β4(1 + β2)u
(β2p + v)3 .
(β2p + v)2 ,
∂u2 = 0,
To optimize Φappr(h), we ﬁrst sort observations according
to η(xi). Then we precompute:

∂2Fβ
∂p2 =

(cid:101)p =

1
n

n
(cid:88)

i=1

η(xi),

(cid:101)pvar =

η(xi)(1 − η(xi)).

1
n2

n
(cid:88)

i=1

(cid:101)uk =

1
n

k
(cid:88)

i=1

η(xi), (cid:98)vk =

, (cid:101)uk

var =

k
n

1
n2

k
(cid:88)

i=1

η(xi)(1−η(xi)).

We then choose k for which the ETU approximation:

(1 + β2)(cid:101)uk
(cid:101)p + k
β2

n

−

β2(1 + β2)
(cid:101)p + k
(β2

n )2 (cid:101)uk

var +

β4(1 + β2)(cid:101)uk
(cid:101)p + k
(β2

n )3 (cid:101)pvar,

is maximized. The maximization can be done in time lin-
ear in O(n), so the most expensive operation is sorting the
instances.

D. Additional material to Section 4.2

Let x = (x1, . . . , xn) be the input sample (test set) of size
n generated i.i.d. from P. Given x and a function (cid:98)η : X →
[0, 1], let

(cid:98)h = argmax

h∈ (cid:98)H

Ey∼(cid:98)η(x)
(cid:124)

(cid:2)Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p)(cid:3)
(cid:125)

(cid:123)(cid:122)
=:(cid:98)ΦETU(h)

.

(cid:2)|(cid:98)p − (cid:101)p|3(cid:3). By

Next, for each k = 0, 1, . . . , n, we precompute:

Consistency Analysis for Binary Classiﬁcation Revisited

be the classiﬁer returned by the ETU procedure upon re-
ceiving the input sample x. Likewise, let:

for some constant c. Moreover, using p-Lipschitzness of Φ,
we have:

h∗ = argmax

h∈ (cid:98)H

Ey∼η(x)
(cid:124)

(cid:2)Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p)(cid:3)
,
(cid:125)

(cid:123)(cid:122)
=:ΦETU(h)

be the optimal ETU classiﬁer in (cid:98)H. We want to bound the
(cid:105)
difference Ex
. By the deﬁnition
of h∗, ΦETU((cid:98)h) ≤ ΦETU(h∗) for any x, and thus:

|ΦETU((cid:98)h) − ΦETU(h∗)|

(cid:104)

E

(cid:105)
(cid:104)(cid:12)
(cid:12)Φ(u, v, p) − Φ((cid:98)u(cid:48), (cid:98)v, (cid:98)p(cid:48))(cid:12)
(cid:12)
+ VpE (cid:2)|(cid:98)v − v|(cid:3) + PpE (cid:2)|(cid:98)p(cid:48) − p|(cid:3) .

≤ UpE (cid:2)|(cid:98)u(cid:48) − u|(cid:3)

Now,

the term E (cid:2)|(cid:98)v − v|(cid:3) is well-controlled and was
shown in the proof of Lemma 1 to be at most
4n as the
expected deviation of the empirical average of [0, 1]-valued
random variable from its mean. Thus it remains to bound
the terms E (cid:2)|(cid:98)p(cid:48) − p|(cid:3) and E (cid:2)|(cid:98)u(cid:48) − u|(cid:3). Deﬁne:

(cid:113) 1

(cid:104)

Ex

(cid:105)

= Ex

|ΦETU((cid:98)h) − ΦETU(h∗)|
(cid:2)ΦETU(h∗)(cid:3) − Ex
(cid:2)ΦETU(h∗)(cid:3) − Ex
(cid:105)
(cid:98)ΦETU(h∗)

= Ex

(cid:104)

+ Ex
(cid:124)

(cid:105)

(cid:104)
ΦETU((cid:98)h)
(cid:104)
(cid:98)ΦETU(h∗)
(cid:104)

(cid:105)

(cid:105)
(cid:98)ΦETU((cid:98)h)

− Ex
(cid:123)(cid:122)
≤0

(cid:125)

(cid:105)

+ Ex
(cid:12)
(cid:12)
(cid:12)

(cid:104)

− Ex

(cid:105)
(cid:98)ΦETU((cid:98)h)
(cid:104)
ΦETU(h) − (cid:98)ΦETU(h)

(cid:104)
ΦETU((cid:98)h)
(cid:105) (cid:12)
(cid:12)
(cid:12).

Ex

≤ 2 sup
h∈ (cid:98)H

1
n

n
(cid:88)

i=1
n
(cid:88)

(cid:101)p(cid:48) = Ey|x

(cid:98)p(cid:48)(cid:3) =
(cid:2)

(cid:98)η(xi),

h(xi)(cid:98)η(xi),

1
(cid:98)u(cid:48)(cid:3) =
(cid:2)
(cid:101)u(cid:48) = Ey|x
n
(cid:2)
(cid:98)η(x)(cid:3) .
(cid:101)p(cid:48)(cid:3) = E (cid:2)
(cid:101)u(cid:48)(cid:3) = E (cid:2)h(x)(cid:98)η(x)(cid:3) .
(cid:2)

(cid:98)η = Ex
(cid:98)η = Ex
u

i=1

p

(7)

We decompose:

Now, ﬁx some classiﬁer h and input sample x. We
let (cid:98)u(h), (cid:98)v(h), (cid:98)p denote the random variables generated
according to η (for ﬁxed x), while (cid:98)u(cid:48)(h), (cid:98)p(cid:48)(h) denote
random variables generated according to (cid:98)η; for instance,
(cid:98)u(cid:48)(h) = 1
i=1 h(xi)yi, where yi ∼ (cid:98)η(xi). Using this
notation, we have:

(cid:80)n

n

|p − (cid:98)p(cid:48)| ≤ |p − p

(cid:98)η| + |p

(cid:98)η − (cid:101)p(cid:48)| + |(cid:101)p(cid:48) − (cid:98)p(cid:48)|
(cid:2)|p

(cid:98)η − (cid:101)p(cid:48)|(cid:3), as well
As before, we use the fact that Ex
(cid:2)|(cid:101)p(cid:48) − (cid:98)p(cid:48)|(cid:3) are both the expected deviations of the
as Ey|x
empirical averages of [0, 1]-valued random variables from

their means, and therefore are bounded by

(cid:113) 1

4n . Hence:

ΦETU(h) = Ey|x
(cid:98)ΦETU(h) = Ey|x

(cid:2)Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p)(cid:3) ,
(cid:2)Φ((cid:98)u(cid:48)(h), (cid:98)v(h), (cid:98)p(cid:48))(cid:3)

(note that (cid:98)v(h) does not depend on (cid:98)η or η, we (cid:98)v(cid:48)(h) =
(cid:98)v(h)). We now bound the term under sup in (7):

E (cid:2)|(cid:98)p(cid:48) − p|(cid:3) ≤ |p − p

(cid:98)η| +

1
√
n

.

Using analogous way of reasoning, one gets:

E (cid:2)|(cid:98)u(cid:48) − u|(cid:3) ≤ |u − u

(cid:98)η| +

1
√
n

.

(cid:104)

(cid:12)
Ex
(cid:12)
(cid:12)

ΦETU(h) − (cid:98)ΦETU(h)

(cid:105) (cid:12)
(cid:12)
(cid:12)

≤ E

≤ E

(cid:105)

(cid:104)(cid:12)
(cid:12)Φ((cid:98)u, (cid:98)v, (cid:98)p) − Φ((cid:98)u(cid:48), (cid:98)v, (cid:98)p(cid:48))(cid:12)
(cid:12)
(cid:104)(cid:12)
(cid:105)
(cid:12)Φ((cid:98)u, (cid:98)v, (cid:98)p) − Φ(u, v, p)(cid:12)
(cid:12)
(cid:104)(cid:12)
(cid:12)Φ(u, v, p) − Φ((cid:98)u(cid:48), (cid:98)v, (cid:98)p(cid:48))(cid:12)
(cid:12)

+ E

(cid:105)

,

where the ﬁrst inequality is due to Jensen’s inequality ap-
plied to a convex function x (cid:55)→ |x|, the all expectations
except for the ﬁrst line are joint with respect to (x, y),
and for the sake of clarity we drop the dependence on h
in (cid:98)u(h), (cid:98)v(h), (cid:98)u(cid:48)(h). Now, it follow from Lemma 1 that:

Putting it all together, we get:

Ex

(cid:104)
ΦETU(h) − (cid:98)ΦETU(h)

(cid:12)
(cid:12)
(cid:12)

(cid:105) (cid:12)
(cid:12)
(cid:12)

(cid:114)

≤ c(cid:48)

log n
n

+ Up|u(h) − u

(cid:98)η(h)| + Pp|p − p

(cid:98)η|,

for some constant c(cid:48). Using (7), we ﬁnally get:

Ex

(cid:104)(cid:12)
(cid:12)ΦETU((cid:98)h) − ΦETU(h∗)(cid:12)
(cid:12)

(cid:105)

≤ c(cid:48)

(cid:114)

log n
n

+ Pp|p − p

(cid:98)η|
(cid:98)η(h)|,

Up|u(h) − u

+ sup
h∈ (cid:98)H

E

(cid:104)(cid:12)
(cid:12)Φ((cid:98)u, (cid:98)v, (cid:98)p) − Φ(u, v, p)(cid:12)
(cid:12)

(cid:105)

≤ c

(cid:114)

log n
n

,

which was to be shown.

Consistency Analysis for Binary Classiﬁcation Revisited

E. Isotron Algorithm (Kalai & Sastry, 2009)

Here we include the Isotron Algorithm of (Kalai & Sas-
try, 2009) for completeness. The second update step is the
Pool of Adjacent Violators (PAV) routine, which solves the
isotonic regression problem:

1, u∗
u∗

2, . . . , u∗

n = arg

min
u1≤u2≤···≤un

(yi − ui)2,

n
(cid:88)

i=1

where the instances are assumed to be sorted according to
their scores wT x (using w obtained in ﬁrst update step of
the iteration). This is a convex quadratic program and can
be solved efﬁciently. The output link function u of the Al-
gorithm is a piecewise linear estimate.

Algorithm 2 The Isotron algorithm (Kalai & Sastry, 2009).

i=1, iterations T

Input: Training data {(xi, yi)}n
Output: wT , uT
w0 ← 0
u0 ← z (cid:55)→ min(max(0, 2 · z + 1), 1)
for t = 1, 2, . . . , T do
wt ← wt−1 + 1
n
ut ← PAV({(cid:104)wt, xi(cid:105), yi})

(cid:80)n

end for

i=1(yi − ut−1((cid:104)wt−1, xi(cid:105))) · xi

