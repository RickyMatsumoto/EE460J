Nonparanormal Information Estimation

Shashank Singh 1 Barnab´as P´oczos 1

Abstract

We study the problem of using i.i.d.
samples
from an unknown multivariate probability distri-
bution p to estimate the mutual information of p.
This problem has recently received attention in
two settings: (1) where p is assumed to be Gaus-
sian and (2) where p is assumed only to lie in a
large nonparametric smoothness class. Estima-
tors proposed for the Gaussian case converge in
high dimensions when the Gaussian assumption
holds, but are brittle, failing dramatically when
p is not Gaussian, while estimators proposed for
the nonparametric case fail to converge with real-
istic sample sizes except in very low dimension.
Hence, there is a lack of robust mutual informa-
tion estimators for many realistic data. To ad-
dress this, we propose estimators for mutual in-
formation when p is assumed to be a nonparanor-
mal (or Gaussian copula) model, a semiparamet-
ric compromise between Gaussian and nonpara-
metric extremes. Using theoretical bounds and
experiments, we show these estimators strike a
practical balance between robustness and scala-
bility.

1. Introduction

This paper is concerned with the problem of estimating
entropy or mutual information of an unknown probability
density p over RD, given n i.i.d. samples from p. Entropy
and mutual information are fundamental information the-
oretic quantities, and consistent estimators for these quan-
tities have a host of applications within machine learning,
statistics, and signal processing. For example, entropy es-
timators have been used for goodness-of-ﬁt testing (Goria
et al., 2005), parameter estimation in semi-parametric mod-
els (Wolsztynski et al., 2005), texture classiﬁcation and im-
age registration (Hero et al., 2001; 2002), change point de-

1Carnegie Mellon University, Pittsburgh, USA. Correspon-

dence to: Shashank Singh <sss1@andrew.cmu.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

tection (Bercher & Vignat, 2000), and anomaly detection
in networks (Noble & Cook, 2003; Nychis et al., 2008;
Berezi´nski et al., 2015). Mutual information is a popu-
lar nonparametric measure of dependence, whose estima-
tors have been used in feature selection (Peng et al., 2005;
Shishkin et al., 2016), clustering (Aghagolzadeh et al.,
2007), learning graphical models (Chow & Liu, 1968),
fMRI data processing (Chai et al., 2009), prediction of pro-
tein structures (Adami, 2004), boosting and facial expres-
sion recognition (Shan et al., 2005), and ﬁtting deep non-
linear models (Hunter & Hodas, 2016). Estimators for both
entropy and mutual information have been used in indepen-
dent component and subspace analysis (Learned-Miller &
Fisher, 2003; Szab´o et al., 2007a).

Motivated by these and other applications, several very re-
cent lines of work (discussed in Section 3) have studied
information estimation,1 focusing largely on two settings:

If p is known to be Gaussian,
1. Gaussian Setting:
there exist information estimators with mean squared error
(MSE) at most −2 log (cid:0)1 − D
(cid:1) and an (almost matching)
minimax lower bound of 2D/n (Cai et al., 2015).
2. Nonparametric Setting: If p is assumed to lie in a non-
parametric smoothness class, such an s-order2 H¨older or
Sobolev class, then the minimax MSE is of asymptotic or-
der (cid:16) max

(Birg´e & Massart, 1995).

n−1, n− 8s

4s+D

(cid:110)

(cid:111)

n

In the Gaussian setting, consistent estimation is tractable
even in the high-dimensional case where D increases fairly
quickly with n, as long as D/n → 0. However, optimal
estimators for the Gaussian setting rely heavily on the as-
sumption of joint Gaussianity, and their performance can
degrade quickly when the data deviate from Gaussian. Es-
pecially in high dimensions, it is unlikely that data are
jointly Gaussian, making these estimators brittle in prac-
tice. In the nonparametric setting, the theoretical conver-
gence rate decays exponentially with D, and, it has been
found empirically that information estimators for this set-
ting fail to converge at realistic sample sizes in all but very
low dimensions. Also, most nonparametric estimators are
sensitive to tuning of bandwidth parameters, which is chal-

1We will collectively call the closely related problems of en-
tropy and mutual information estimation information estimation.
2Here, s encodes the degree of smoothness, roughly corre-

sponding to the number of continuous derivatives of p.

Nonparanormal Information Estimation

lenging for information estimation, since no empirical error
estimate is available for cross-validation.

Given these factors, though the Gaussian and nonparamet-
ric cases are fairly well understood in theory, there remains
a lack of practical information estimators for the common
case where data are neither exactly Gaussian nor very low
dimensional. The main goal of this paper is to ﬁll the gap
between these two extreme settings by studying informa-
tion estimation in a semiparametric compromise between
the two, known as the “nonparanormal” (a.k.a. “Gaus-
sian copula”) model (see Deﬁnition 2). The nonparanormal
model, analogous to the additive model popular in regres-
sion (Friedman & Stuetzle, 1981), limits complexity of in-
teractions among variables but makes minimal assumptions
on the marginal distribution of each variable. The result
scales better with dimension than nonparametric models,
while being more robust than Gaussian models.

Proofs of our main results, as well as additional experi-
ments and details are given in the extended version of this
paper 3. All code can be found on GitHub4.

2. Problem statement and notation

There are a number of distinct generalizations of mutual
information to more than two variables. The deﬁnition
we consider is simply the difference between the sum of
marginal entropies and the joint entropy:

(Multivariate mutual

information) Let
Deﬁnition 1.
X1, . . . , XD be R-valued random variables with a joint
probability density p : RD → [0, ∞) and marginal
densities p1, ..., pD : R → [0, ∞). The multivariate mutual
information I(X) of X = (X1, . . . , XD) is deﬁned by

I(X) := E
X∼p

log

(cid:34)

(cid:32)

(cid:33)(cid:35)

p(X)
j=1 pj(Xj)

(cid:81)D

=

H(Xj) − H(X),

(1)

D
(cid:88)

j=1

where H(X) = − EX∼p[log p(X)] denotes entropy of X.

This notion of multivariate mutual information, originally
due to Watanabe (1960) (who called it “total correla-
tion”) measures total dependency, or redundancy, within
a set of D random variables.
It has also been called
the “multivariate constraint” (Garner, 1962) and “multi-
information” (Studen`y & Vejnarov´a, 1998). Many re-
lated information theoretic quantities can be expressed in

3accessible

at

http://www.contrib.andrew.

cmu.edu/˜sss1/publications/papers/
nonparanormal-information-estimation.pdf

4https://github.com/sss1/

nonparanormal-information

terms of I(X), and can thus be estimated using estimators
of I(X). Examples include pairwise mutual information
I(X, Y ) = I((X, Y ))−I(X)−I(Y ), which measures de-
pendence between (potentially multivariate) random vari-
ables X and Y , conditional mutual information

I(X|Z) = I((X, Z)) −

I((Xj, Z)),

D
(cid:88)

j=1

which is useful for characterizing how much dependence
within X can be explained by a latent variable Z (Studen`y
& Vejnarov´a, 1998), and transfer entropy (a.k.a. “directed
information”) TX → Y , which measures predictive power
of one time series X on the future of another time series
Y . I(X) is also related to entropy via Eq. (1), but, un-
like the above quantities, this relationship depends on the
marginal distributions of X, and hence involves some ad-
ditional considerations, as discussed in Section 8.

We now deﬁne the class of nonparanormal distributions,
from which we assume our data are drawn.

(Nonparanormal distribution,

a.k.a.
Deﬁnition 2.
Gaussian copula model) A random vector X =
(X1, . . . , XD)T is said to have a nonparanormal distri-
bution (denoted X ∼ N PN (Σ; f )) if there exist func-
j=1 such that each fj : R → R is a diffeomor-
tions {fj}D
phism 5 and f (X) ∼ N (0, Σ), for some (strictly) positive
deﬁnite Σ ∈ RD×D with 1’s on the diagonal (i.e., each
σj = Σj,j = 1). 6 Σ is called the latent covariance of X
and f is called the marginal transformation of X.

The nonparanormal family relaxes many constraints of
the Gaussian family. Nonparanormal distributions can be
multi-modal, skewed, or heavy-tailed, can encode noisy
nonlinear dependencies, and need not be supported on RD.
Minimal assumptions are made on the marginal distribu-
tions; any desired continuously differentiable marginal cu-
mulative distribution function (CDF) Fi of variable Xi cor-
responds to marginal transformation fi(x) = Φ−1(Fi(x))
(where Φ is the standard normal CDF). As examples, for
a Gaussian variable Z, the 2-dimensional case, X1 ∼
N (0, 1), and X2 = T (X1 + Z) is nonparanormal when
T (x) = x3, T = tanh, T = Φ, or any other diffeomor-
phism. On the other hand, the limits of the Gaussian copula
appear, for example, when T (x) = x2, which is not bijec-
tive; then, if E[Z] = 0, the Gaussian copula approximation
of (X1, X2) models X1 and X2 as independent.

5A diffeomorphism is a continuously differentiable bijection

g : R → R ⊆ R such that g−1 is continuously differentiable.

6Setting E [f (X)] = 0 and each σj = 1 ensures model identi-
ﬁability, but does not reduce the model space, since these param-
eters can be absorbed into the marginal transformation f .

We are now ready to formally state our problem:

3.2. Information Estimation

Nonparanormal Information Estimation

Formal Problem Statement: Given n i.i.d.
samples
X1, ..., Xn ∼ N PN (Σ; f ), where Σ and f are both un-
known, we would like to estimate I(X).

Other notation: D denotes the dimension of the data (i.e.,
Σ ∈ RD×D and f : RD → RD). For a positive inte-
ger k, [k] = {1, ..., k} denotes the set of positive integers
less than k (inclusive). For consistency, where possible, we
use i ∈ [n] to index samples and j ∈ [D] to index dimen-
sions (so that, e.g., Xi,j denotes the jth dimension of the
ith sample). Given a data matrix X ∈ Rn×D, our estima-
tors depend on the empirical rank matrix

R ∈ [n]n×D with Ri,j :=

1{Xi,j ≥Xk,j }.

(2)

n
(cid:88)

k=1

For a square matrix A ∈ Rk×k, |A| denotes the determinant
of A, AT denotes the transpose of A, and

(cid:107)A(cid:107)2 := max
x ∈ Rk
(cid:107)x(cid:107)2 = 1

(cid:107)Ax(cid:107)2

and

(cid:107)A(cid:107)F :=

(cid:115) (cid:88)

i,j∈[k]

A2
i,j

denote the spectral and Frobenius norms of A, respectively.
When A is symmetric, λ1(A) ≥ λ2(A) ≥ · · · ≥ λD(A)
are its eigenvalues.

3. Related Work and Our Contributions

3.1. The Nonparanormal

Nonparanormal models have been used for modeling de-
pendencies among high-dimensional data in a number of
ﬁelds, such as graphical modeling of gene expression
data (Liu et al., 2012), of neural data (Berkes et al., 2009),
and of ﬁnancial time series (Malevergne & Sornette, 2003;
Wilson & Ghahramani, 2010; Hern´andez-Lobato et al.,
2013), extreme value analysis in hydrology (Renard &
Lang, 2007; Aghakouchak, 2014), and informative data
compression (Rey & Roth, 2012).

With one recent exception (Ince et al., 2017), previous
information estimators for the nonparanormal case (Cal-
saverini & Vicente, 2009; Ma & Sun, 2011; Elidan, 2013),
rely on fully nonparametric information estimators as sub-
routines, and hence suffer strongly from the curse of di-
mensionality. Very recently, Ince et al. (2017) proposed
what we believe is the ﬁrst mutual information estimator
tailored speciﬁcally to the nonparanormal case; their esti-
mator is equivalent to one of the estimators (IG, described
in Section 4.1) we study. However, they focused on its ap-
plications to neuroimaging data analysis, and did not study
its performance theoretically or empirically.

Our motivation for studying the nonparanormal family
comes from trying to bridge two recent approaches to in-
formation estimation. The ﬁrst has studied fully non-
parametric entropy estimation, assuming only that data are
drawn from a smooth probability density p, where smooth-
ness is typically quantiﬁed by a H¨older or Sobolev expo-
nent s ∈ (0, ∞), roughly corresponding to the continuous
differentiability of s. In this setting, the minimax optimal
MSE rate has been shown by Birg´e & Massart (1995) to
(cid:111)(cid:17)
be O
. This rate slows exponen-
tially with the dimension D, and, while many estimators
have been proposed (P´al et al., 2010; Sricharan et al., 2011;
2013; Singh & P´oczos, 2014a;b; Krishnamurthy et al.,
2014; Moon & Hero, 2014b;a; Singh & P´oczos, 2016a;
Moon et al., 2017) for this setting, their practical use is
limited to a few dimensions7.

n−1, n− 8s

max

4s+D

(cid:110)

(cid:16)

The second area is in the setting where data are assumed to
be drawn from a truly Gaussian distribution. Here the high-
dimensional case is far more optimistic. While this case
had been studied previously (Ahmed & Gokhale, 1989;
Misra et al., 2005; Srivastava & Gupta, 2008), Cai et al.
(2015) recently provided a precise ﬁnite-sample analysis
based on deriving the exact probability law of the log-
determinant log |(cid:98)Σ| of the scatter matrix (cid:98)Σ. From this,
they derived a deterministic bias correction, giving an es-
timator for which they prove an MSE upper bound of
(cid:1) and a high-dimensional central limit the-
−2 log (cid:0)1 − D
orem for the case D → ∞ as n → ∞ (but D < n).

n

Cai et al. (2015) also prove a minimax lower bound of
2D/n on MSE, with several interesting consequences.
First, consistent information estimation is possible only if
D/n → 0. Second, since, for small x, − log(1 − x) ≈
x, this lower bound essentially matches the above upper
bound when D/n is small. Third, they show this lower
bound holds even when restricted to diagonal covariance
matrices. Since the upper bound for the general case and
the lower bound for the diagonal case essentially match, it
follows that Gaussian information estimation is not made
easier by structural assumptions such as Σ being bandable,
sparse, or Toeplitz, as is common in, for example, station-
ary Gaussian process models (Cai & Yuan, 2012).

This 2D/n lower bound extends to our more general non-
paranormal setting. However, we provide a minimax lower
bound suggesting that the nonparanormal setting is strictly
harder, in that optimal rates depend on Σ. Our results imply

7“Few” depends on s and n, but Kandasamy et al. (2015) sug-
gest nonparametric estimators should only be used with D at most
4-6. Rey & Roth (2012) tried using several nonparametric infor-
mation estimators on the Communities and Crime UCI data set
(n = 2195, D = 10), but found all too unstable to be useful.

Nonparanormal Information Estimation

nonparanormal information estimation does become easier
if Σ is assumed to be bandable or Toeplitz.

A closely related point is that known convergence rates for
the fully nonparametric case require the density p to be
bounded away from 0 or have particular tail behavior, due
to singularity of the logarithm near 0 and resulting sensi-
tivity of Shannon information-theoretic functionals to re-
gions of low but non-zero probability. In contrast, Cai et al.
(2015) need no lower-bound-type assumptions in the Gaus-
sian case. In the nonparanormal case, we show some such
condition is needed to prove a uniform rate, but a weaker
condition, a positive lower bound on λD(Σ), sufﬁces.

The main contributions of this paper are the following:

2. We prove upper bounds, of order O(D2/(λ2

1. We propose three estimators, (cid:98)IG, (cid:98)Iρ, and (cid:98)Iτ ,8 for the
mutual information of a nonparanormal distribution.
D(Σ)n))
on the mean squared error of (cid:98)Iρ, providing the ﬁrst
upper bounds for a nonparanormal information esti-
mator. This bound suggests nonparanormal estimators
scale far better with D than nonparametric estimators.
3. We prove a minimax lower bound suggesting that, un-
like the Gaussian case, difﬁculty of nonparanormal in-
formation estimation depends on the true Σ.

4. We give simulations comparing our proposed estima-
tors to Gaussian and nonparametric estimators. Be-
sides conﬁrming and augmenting our theoretical pre-
dictions, these help characterize the settings in which
each nonparanormal estimator works best.

5. We present entropy estimators based on (cid:98)IG, (cid:98)Iρ, and
(cid:98)Iτ . Though nonparanormal entropy estimation re-
quires somewhat different assumptions from mutual
information estimation, we show that entropy can also
be estimated at the rate O(D2/(λ2

D(Σ)n)).

4. Nonparanormal Information Estimators

In this section, we present three different estimators, IG,
Iρ, and Iτ , for the mutual information of a nonparanormal
distribution. We begin with a lemma providing common
motivation for all three estimators.

Since mutual information is invariant to diffeomorphisms
of individual variables, it is easy to see that the mutual in-
formation of a nonparanormal random variable is the same
as that of the latent Gaussian random variable. Speciﬁcally:

Lemma 3. (Nonparanormal mutual information): Sup-
pose X ∼ N PN (Σ; f ). Then,

Lemma 3 shows that mutual information of a nonparanor-
mal random variable depends only the latent covariance Σ;
the marginal transformations are nuisance parameters, al-
lowing us to avoid difﬁcult nonparametric estimation; the
estimators we propose all plug different estimates of Σ into
Eq. (3), after a regularization step described in Section 4.3.

4.1. Estimating Σ by Gaussianization

The ﬁrst estimator (cid:98)ΣG of Σ proceeds in two steps. First, the
data are transformed to have approximately standard nor-
mal marginal distributions, a process Szab´o et al. (2007b)
referred to as “Gaussianization”. By the nonparanormal as-
sumption, the Gaussianized data are approximately jointly
Gaussian. Then, the latent covariance matrix is estimated
by the empirical covariance of the Gaussianized data.

More speciﬁcally, letting Φ−1 denote the quantile function
of the standard normal distribution and recalling the rank
matrix R deﬁned in (2), the Gaussianized data
(cid:18) Ri,j
n + 1

(for i ∈ [n], j ∈ [D])

(cid:101)Xi,j := Φ−1

(cid:19)

are obtained by transforming the empirical CDF of the each
dimension to approximate Φ. Then, we estimate Σ by the
empirical covariance (cid:98)ΣG := 1
n

i=1 (cid:101)Xi (cid:101)X T
i .

(cid:80)n

4.2. Estimating Σ by rank correlation

The second estimator actually has two variants, Iρ and
Iτ , respectively based on relating the latent covariance to
two classic rank-based dependence measures, Spearman’s
ρ and Kendall’s τ . For two random variables X and Y with
CDFs FX , FY : R → [0, 1], ρ and τ are deﬁned by

ρ(X, Y ) := Corr(FX (X), FY (Y ))
τ (X, Y ) := Corr(sign(X − X (cid:48)), sign(Y − Y (cid:48))),

and

respectively, where

Corr(X, Y ) =

E[(X − E[X])(Y − E[Y ])]
(cid:112)V[X]V[Y ]

denotes the standard Pearson correlation operator and
(X (cid:48), Y (cid:48)) is an IID copy of (X, Y ). ρ and τ generalize to
the D-dimensional setting in the form of rank correlation
matrices ρ, τ ∈ [−1, 1]D×D with ρj,k = ρ(Xj, Xk) and
τj,k = τ (Xj, Xk) for each j, k ∈ [D].

Iρ and Iτ are based on a classical result relating the corre-
lation and rank-correlation of a bivariate Gaussian:

I(X) = −

log |Σ|.

1
2

(3)

Theorem 4.
Gaussian joint distribution with covariance Σ. Then,

(Kruskal, 1958): Suppose (X, Y ) has a

8Ince et al. (2017) proposed (cid:98)IG for use in neuroimaging data

analysis. To the best of our knowledge, (cid:98)Iρ and (cid:98)Iτ are novel.

Corr(X, Y ) = 2 sin

ρ(X, Y )

= sin

τ (X, Y )

.

(cid:16) π
6

(cid:17)

(cid:16) π
2

(cid:17)

Nonparanormal Information Estimation

ρ and τ are often preferred over Pearson correlation for
their relative robustness to outliers and applicability to non-
numerical ordinal data. While these are strengths here as
well, the main reason for their relevance is that they are
invariant to marginal transformations (i.e., for diffeomor-
phisms f, g : R → R, ρ(f (X), g(Y )) = ±ρ(X, Y ) and
τ (f (X), g(Y )) = ±τ (X, Y )). As a consequence, the
identity provided in Theorem 4 extends unchanged to the
case (X, Y ) ∼ N PN (Σ; f ). This suggests an estimate for
Σ based on estimating ρ or τ and plugging this element-
2 x(cid:1),
wise into the transform x (cid:55)→ 2 sin (cid:0) π
respectively. Speciﬁcally, Σρ is deﬁned by
(cid:17)
(cid:16) π
6 (cid:98)ρ

6 x(cid:1) or x (cid:55)→ sin (cid:0) π

(cid:98)ρ = (cid:91)Corr(R)

(cid:98)Σρ := 2 sin

, where

is the empirical correlation of the rank matrix R, and sine
2 (cid:98)τ (cid:1), where
is applied element-wise. Similarly, (cid:98)Στ := sin (cid:0) π

(cid:98)τj,k :=

1
(cid:1)
(cid:0)n
2

(cid:88)

i(cid:54)=(cid:96)∈[n]

sign(Xi,j − X(cid:96),j) sign(Xi,k − X(cid:96),k).

4.3. Regularization and estimating I

Unfortunately, unlike usual empirical correlation matrices,
none of (cid:98)ΣG, (cid:98)Σρ, or (cid:98)Στ is almost surely strictly positive
deﬁnite. As a result, directly plugging into the mutual in-
formation functional (3) may give ∞ or be undeﬁned. To
correct for this, we propose a regularization step, in which
we project each estimated latent covariance matrix onto the
(closed) cone S(z) of symmetric matrices with minimum
eigenvalue z > 0. Speciﬁcally, for any z > 0, let

S(z) := (cid:8)A ∈ RD×D : A = AT , λD(A) ≥ z(cid:9) .
For any symmetric matrix A ∈ RD×D with eigendecom-
position (cid:98)Σ = QΛQ−1 (i.e., QQT = QT Q = ID and Λ
is diagonal), the projection Az of A onto S(z) is deﬁned
as Az := QΛzQ−1, where Λz is the diagonal matrix with
jth nonzero entry (Λz)j,j = max{z, Λj,j}. We call this a
“projection” because Az = argminB∈S(z)(cid:107)A − B(cid:107)F (see,
e.g., Henrion & Malick (2012)).

Applying this regularization to (cid:98)ΣG, (cid:98)Σρ, or (cid:98)Στ gives a
strictly positive deﬁnite estimate (cid:98)ΣG,z, (cid:98)Σρ,z, or (cid:98)Στ,z, re-
spectively, of Σ. We can then estimate I by plugging this
into Equation (3), giving our three estimators:

(cid:98)IG,z := −

log

1
2

(cid:12)
(cid:12)
(cid:12)(cid:98)ΣG,z

(cid:12)
(cid:12)
(cid:12) ,

(cid:98)Iρ,z := −

log

1
2
1
2

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)(cid:98)Σρ,z
(cid:12)
(cid:12)
(cid:12) .

(cid:12)
(cid:12)
(cid:12)(cid:98)Στ,z

and

(cid:98)Iτ,z := −

log

5. Upper Bounds on the Error of (cid:98)Iρ,z

Here, we provide ﬁnite-sample upper bounds on the error
of the estimator (cid:98)Iρ based on Spearman’s ρ. Proofs are given

in the Appendix. We ﬁrst bound the bias of (cid:98)Iρ:

i.i.d.∼ N PN (Σ; f ).
Proposition 5. Suppose X1, ..., Xn
Then, there exists a constant C > 0 such that, for any
z > 0, the bias of (cid:98)Iρ,z is at most

(cid:104)

(cid:12)
(cid:12)
(cid:12)

E

(cid:105)

(cid:98)Iρ,z

− I

(cid:12)
(cid:12)
(cid:12) ≤ C

(cid:18) D
√
n
z

+ log

(cid:19)

,

|Σz|
|Σ|

where Σz is the projection of Σ onto S(z).

The ﬁrst term of the bias stems from nonlinearity of the
log-determinant function in Equation 3, which we analyze
via Taylor expansion. The second term,

log

|Σz|
|Σ|

=

(cid:88)

log

λj (Σ)<z

(cid:18) z

λj(Σ)

(cid:19)

,

is due to the regularization step and is actually exact, but is
difﬁcult to simplify or bound without more assumptions on
the spectrum of Σ and choice of z, which we discuss later.
We now turn to bounding the variance of (cid:98)Iρ,z. We ﬁrst
provide an exponential concentration inequality for (cid:98)Iρ,z
around its expectation, based on McDiarmid’s inequality:
i.i.d.∼ N PN (Σ; f ).

Proposition 6. Suppose X1, ..., Xn
Then, for any z, ε > 0,

P

(cid:104)(cid:12)
(cid:12)
(cid:12)(cid:98)Iρ,z − E

(cid:104)
(cid:98)Iρ,z

(cid:105)(cid:12)
(cid:105)
(cid:12)
(cid:12) > ε

≤ 2 exp

−

(cid:18)

nz2ε2
18π2D2

(cid:19)

.

Such exponential concentration bounds are useful when
one wants to simultaneously bound the error of multiple
uses of an estimator, and hence we present it separately as
it may be independently useful. However, for the purpose
of understanding convergence rates, we are more interested
in the variance bound that follows as an easy corollary:

Corollary 7. Suppose X1, ..., Xn
Then, for any z > 0, the variance of (cid:98)Iρ,z is at most

i.i.d.∼ N PN (Σ; f ).

(cid:104)

V

(cid:105)

(cid:98)Iρ,z

≤

36π2D2
z2n

.

Given these bias and variance bounds, a bound on the MSE
of (cid:98)Iρ,z follows via the usual bias-variance decomposition:
Theorem 8. Suppose X ∼ N PN (Σ; f ). Then, there ex-
ists a constant C such that

(cid:20)(cid:16)

E

(cid:17)2(cid:21)

(cid:98)Iρ,z − I

≤ C

(cid:18) D2
z2n

+ log2 |Σz|
|Σ|

(cid:19)

.

(4)

Nonparanormal Information Estimation

A natural question is now how to optimally select the regu-
larization parameter z. While the bound (4) is clearly con-
vex in z, it depends crucially on the unknown spectrum of
Σ, and, in particular, on the smallest eigenvalues of Σ. As
a result, it is difﬁcult to choose z optimally in general, but
we can do so for certain common subclasses of covariance
matrices. For example, if Σ is Toeplitz or bandable (i.e.,
for some c ∈ (0, 1), all |Σi,j| ≤ c|i−j|), then the small-
est eigenvalue of Σ can be bounded below (Cai & Yuan,
2012). When Σ is bandable, as we show in the Appendix,
this bound can be independent of D. In these cases, the
following somewhat simpler MSE bound can be used:
Corollary 9. Suppose X ∼ N PN (Σ; f ), and suppose
z ≤ λD(Σ). Then, there exists a constant C > 0 such that

(cid:20)(cid:16)

E

(cid:98)Iρ,z − I

(cid:17)2(cid:21)

≤

CD2
z2n

.

Clearly, this lower bound tends to ∞ as σ → 1. As written,
this result lower bounds the error of rank-based estimators
in the Gaussian case when σ ≈ 1. However, to the best
of our knowledge, all methods for estimating Σ in the non-
paranormal case are functions of R, and prior work (Hoff,
2007) has shown that the rank matrix R is a generalized
sufﬁcient statistic for Σ (and hence for I) in the nonpara-
normal model. Thus, it is reasonable to think of lower
bounds for rank-based estimators in the Gaussian case as
lower bounds for any estimator in the nonparanormal case.

The proof of this result is based on the simple observa-
tion that the rank matrix can take only ﬁnitely many values.
Hence, as σ → 1, R tends to be perfectly correlated, pro-
viding little information about σ, whereas the dependence
of the estimand Iσ on σ increases sharply. This is intuition
is formalized in the Appendix using Le Cam’s lemma for
lower bounds in two-point parameter estimation problems.

6. Lower Bounds in terms of Σ

7. Empirical Results

i.i.d∼ N (0, Σ) are Gaussian, for the plug-in

We compare 5 mutual information estimators:

If X1, ..., Xn
estimator

(cid:98)I = − 1

2 log

(cid:12)
(cid:12)
(cid:12)(cid:98)Σ

(cid:12)
(cid:12)
(cid:12)

(where

(cid:98)Σ = 1
n

(cid:80)n

i=1 XiX T
i

is the empirical covariance matrix), Cai et al. (2015)
showed that the distribution of (cid:98)I − I is independent of the
true correlation matrix Σ. This follows from the “stability”
of Gaussians (i.e., that nonsingular linear transformations
of Gaussian random variables are Gaussian). In particular,
(cid:98)I − I = log |(cid:98)Σ| − log |Σ| = log |Σ−1/2 (cid:98)ΣΣ−1/2|,
and Σ−1/2 (cid:98)ΣΣ−1/2 has the same distribution as log (cid:98)Σ does
in the special case that Σ = ID is the identity. This prop-
erty is both somewhat surprising, given that I → ∞ as
|Σ| → 0, and useful, leading to a tight analysis of the error
of (cid:98)I and conﬁdence intervals that do not depend on Σ.

It would be convenient if any nonparanormal information
estimators satisﬁed this property. Unfortunately, the main
result of this section is a negative one, showing that this
property is unlikely to hold without additional assumptions:
Proposition 10. Consider the 2-dimensional case
(cid:20)1 σ
σ 1

i.i.d∼ N (0, Σ),

with Σ =

X1, ..., Xn

(5)

(cid:21)

,

and let σ∗ ∈ (0, 1). Suppose an estimator (cid:98)I = (cid:98)I(R) of
Iσ = − 1
2 log(1 − σ2) is a function of the empirical rank
matrix R ∈ Nn×2 of X. Then, there exists a constant C >
0, depending only n, such that the worst-case MSE of (cid:98)I
over σ ∈ (0, σ∗) satisﬁes
(cid:20)(cid:16)

(cid:17)2(cid:21)

≥

1
64

(cid:0)C − log(1 − σ2

∗)(cid:1)2

sup
σ∈(0,σ∗)

E

(cid:98)I(R) − Iσ

• (cid:98)I: Gaussian plug-in estimator with bias-correction

(see Cai et al. (2015)).

• (cid:98)IG: Nonparanormal estimator using Gaussianization.
• (cid:98)Iρ: Nonparanormal estimator using Spearman’s ρ.

• (cid:98)Iτ : Nonparanormal estimator using Kendall’s τ .
• (cid:98)IkNN: Nonparametric estimator using k-nearest neigh-

bor (kNN) statistics.

For Iρ and Iτ , we used a regularization constant z =
10−3. We did not regularize for IG. Although this im-
plies P[IG = ∞] > 0, this is extremely unlikely for
even moderate values of n and never occurred during our
experiments, which all use n ≥ 32. We thus omit denoting
dependence on z. For IkNN, except as noted in Experiment
3, k = 2, based on recent analysis (Singh & P´oczos, 2016b)
suggesting that small values of k are best for estimation.

Sufﬁcient details to reproduce experiments are given in the
Appendix, and MATLAB source code is available at [Omit-
ted for anonymity]. We report MSE based on 1000 i.i.d. tri-
als of each condition. 95% conﬁdence intervals were con-
sistently smaller than plot markers and hence omitted to
avoid cluttering plots. Except as speciﬁed otherwise, each
experiment had the following basic structure: In each trial,
a correlation matrix Σ was drawn by normalizing a random
covariance matrix from a Wishart distribution, and data
i.i.d.∼ N (0, Σ) drawn. All 5 estimators were
X1, ..., Xn
computed from X1, ..., Xn and squared error from true mu-
tual information (computed from Σ) was recorded. Unless
speciﬁed otherwise, n = 100 and D = 25.

Since our nonparanormal information estimators are func-

Nonparanormal Information Estimation

tions of ranks of the data, neither the true mutual infor-
mation nor our non-paranormal estimators depend on the
marginal transformations. Thus, except in Experiment 2,
where we show the effects of transforming marginals, and
Experiment 3, where we add outliers to the data, we per-
form all experiments on truly Gaussian data, with the un-
derstanding that this setting favors the Gaussian estimator.

All experimental results are displayed in Figure 1.

Experiment 1 (Dependence on n): We ﬁrst show non-
paranormal estimators have “parametric” O(n−1) depen-
dence on n, unlike (cid:98)IkNN, which converges far more slowly.
For large n, MSEs of (cid:98)IG, (cid:98)Iρ, and (cid:98)Iτ are close to that of (cid:98)I.

Experiment 2 (Non-Gaussian Marginals): Next, we
show nonparanormal estimators are robust
to non-
Gaussianity of the marginals, unlike (cid:98)I. We applied a non-
linear transformation f to a fraction α ∈ [0, 1] of dimen-
i.i.d.∼
sions of Gaussian data. That is, we drew Z1, ..., Zn
N (0, Σ) and then used data X1, ..., Xn, where

Xi,j =

(cid:26) T (Zi,j)
Zi,j

if j < αD
if j ≥ αD

,

∀i ∈ [n], j ∈ [D],

for a diffeomorphism T . Here, we use T (z) = ez. The
Appendix shows similar results for several other T . (cid:98)I per-
forms poorly even when α is quite small. Poor performance
of (cid:98)IkNN may be due to discontinuity of the density at x = 0.

Experiment 3 (Outliers): We now show that nonparanor-
mal estimators are far more robust to the presence of out-
liers than (cid:98)I or (cid:98)IkNN. To do this, we added outliers to the
data according to the method of Liu et al. (2012). Af-
ter drawing Gaussian data, we independently select (cid:98)βn(cid:99)
samples in each dimension, and replace each i.i.d. uni-
formly at random from {−5, +5}. Performance of (cid:98)I de-
grades rapidly even for small β. (cid:98)IkNN can fail for atomic
distributions, (cid:98)IkNN = ∞ whenever at least k samples are
identical. This mitigate this, we increased k to 20 and ig-
nored trials where (cid:98)IkNN = ∞, but (cid:98)IkNN ceased to give any
ﬁnite estimates when β was sufﬁciently large.

For small values of β, nonparanormal estimators surpris-
ingly improve. We hypothesize this is due to convexity of
the mutual information functional Eq. (3) in Σ. By Jensen’s
inequality, estimators which plug-in an approximately un-
biased estimate (cid:98)Σ of Σ are biased towards overestimating
I. Adding random (uncorrelated) noise reduces estimated
dependence, moving the estimate closer to the true value.
If this nonlinearity is indeed a major source of bias, it
may be possible to derive a von Mises-type bias correction
(see Kandasamy et al. (2015)) accounting for higher-order
terms in the Taylor expansion of the log-determinant.

Experiment 4 (Dependence on Σ): Here, we verify our re-
sults in Section 6 showing that MSE of rank-based estima-

tors approaches ∞ as |Σ| → 0, while MSE of (cid:98)I is indepen-
dent of Σ. Here, we set D = 2 and Σ as in Eq. (5), varying
σ ∈ [0, 1]. Indeed, the MSE of (cid:98)I does not change, while
the MSEs of (cid:98)IG, (cid:98)Iρ, and (cid:98)Iτ all increase as σ → 1. This
increase seems mild in practice, with performance worse
than of (cid:98)I only when σ > 0.99. (cid:98)Iτ appears to perform far
better than (cid:98)IG and (cid:98)Iρ in this regime. Performance of IkNN
degrades far more quickly as σ → 1. This phenomenon is
explored by Gao et al. (2015), who lower bound error of
IkNN in the presence of strong dependencies, and proposed
a correction to improve performance in this case.

It is also interesting that errors of (cid:98)Iρ and (cid:98)Iτ drop as σ → 0.
This is likely because, for small σ, the main source of er-
ror is the variance of (cid:98)ρ and (cid:98)τ (as − log(1 − σ2) ≈ σ2
when σ ≈ 0). When n → ∞ and D is ﬁxed, both
2 sin(π (cid:98)ρ/6) and sin(π(cid:98)τ /2) are asymptotically normal es-
timates of σ, with asymptotic variances proportional to
(1−σ2)2 (Klaassen & Wellner, 1997). By the delta method,
since dI
1−σ2 , (cid:98)Iρ and (cid:98)Iτ are asymptotically normal es-
timates of I, with asymptotic variances proportional to σ2
and hence vanishing as σ → 0.

dσ = σ

8. Estimating Entropy

Thus far, we have discussed estimation of mutual infor-
mation I(X). Mutual information is convenient because
it is invariant under marginal transformation, and hence
I(X) = I(f (X)) depends only on Σ. While the en-
tropy H(X) does depend on the marginal transform f ,
fortunately, by Eq. (1), H(X) differs from I(X) only
by a sum of univariate entropies. Univariate nonpara-
metric estimation of entropy in has been studied exten-
sively, and there exist several estimators (e.g., based on
sample spacings (Beirlant et al., 1997), kernel density es-
timates (Moon et al., 2016) or k-nearest neighbor meth-
ods (Singh & P´oczos, 2016b)) that can estimate H(Xj) at
the rate (cid:16) n−1 in MSE under relatively mild conditions
on the marginal density pj. While the precise assumptions
vary with the choice of estimator, they are mainly (a) that
pj be lower bounded on its support or have particular (e.g.,
exponential) tail behavior, and (b) that pj be smooth, typi-
cally quantiﬁed by a H¨older or Sobolev condition. Details
of these assumptions are in the Appendix.

these conditions,

Under
(cid:98)H1, ..., (cid:98)HD and a constant C > 0 such that

since there exist estimators

E[( (cid:98)Hj − H(Xj))2] ≤ C/n,

∀j ∈ [D].

(6)

Combining these estimators with an estimator, say (cid:98)Iρ,z, of
mutual information gives an estimator of entropy:

(cid:98)Hρ,z := (cid:80)D

j=1 (cid:98)Hj − (cid:98)Iρ,z.

If we assume z = λ−1

D (Σ) is bounded below by a positive

Nonparanormal Information Estimation

(a) Experiment 1

(b) Experiment 2

(c) Experiment 3

(d) Experiment 4

Figure 1. Plots of log10(MSE) plotted over (a) log-sample-size log10(n), (b) fraction α of dimensions with non-Gaussian marginals, (c)
fraction β of outlier samples in each dimension, and (d) covariance Σ1,2 = Cov(X1, X2). Note that the x-axis in (d) is decreasing.

constant, combining inequality (6) with Corollary 9 gives

(cid:20)(cid:16)

E

(cid:98)Hρ,z − H(X)

(cid:17)2(cid:21)

≤

CD2
n

,

where C differs from in (6) but is independent of n and D.

9. Conclusions and Future Work

This paper suggests nonparanormal information estimation
as a practical compromise between the intractable nonpara-
metric case and the limited Gaussian case. We proposed
three estimators for this problem and provided the ﬁrst up-
per bounds for nonparanormal information estimation. We
also gave lower bounds showing how dependence on Σ dif-
fers from the Gaussian case and demonstrated empirically
that nonparanormal estimators are more robust than Gaus-
sian estimators, even in dimensions too high for nonpara-
metric estimators.

Collectively, these results suggest that, by scaling to mod-
erate or high dimensionality without relying on Gaussian-
ity, nonparanormal information estimators may be effec-
tive tools with a number of machine learning applications.
While the best choice of information estimator inevitably
depends on context, as an off-the-shelf guide for practition-
ers, the estimators we suggest, in order of preference, are:

• fully nonparametric if D < 6, n > max{100, 10D}.
• (cid:98)Iρ if D2/n is small and data may have outliers.
• (cid:98)Iτ if D2/n is small and dependencies may be strong.
• (cid:98)IG otherwise.
• (cid:98)I only given strong belief that data are nearly Gaussian.

There are many natural open questions in this line of work.
First, in the nonparanormal model, we focused on esti-
mating mutual information I(X), which does not depend
on marginal transforms f , and entropy, which decomposes
into I(X) and 1-dimensional entropies. In both cases, ad-
ditional structure imposed by the nonparanormal model al-
lows estimation in higher dimensions than fully nonpara-
metric models. Can nonparanormal assumptions lead to

higher dimensional estimators for the many other useful
nonlinear functionals of densities (e.g., Lp norms/distances
and more general (e.g., R´enyi or Tsallis) entropies, mutual
informations, and divergences) that do not decompose?

Second, there is a gap between our upper bound rate of
(cid:107)Σ−1(cid:107)2
2D2/n and the only known lower bound of 2D/n
(from the Gaussian case), though we also showed that
bounds for rank-based estimators depend on Σ. Is quadratic
dependence on D optimal? How much do rates improve
under structural assumptions on Σ? Upper bounds should
be derived for other estimators, such as (cid:98)IG and (cid:98)Iτ . The
2D/n lower bound proof of Cai et al. (2015) for the Gaus-
sian case, based on the Cramer-Rao inequality (Van den
Bos, 2007), is unlikely to tighten in the nonparanormal
case, since Fisher information is invariant to diffeomor-
phisms of the data. Hence, a new approach is needed if
the lower bound in the nonparanormal case is to be raised.

Finally, our work applies to estimating the log-determinant
log |Σ| of the latent correlation in a nonparanormal model.
Besides information estimation,
the work of Cai et al.
(2015) on estimating log |Σ| in the Gaussian model was
motivated by the role of log |Σ| in other multivariate statis-
tical tools, such as quadratic discriminant analysis (QDA)
and MANOVA (Anderson, 1984). Can our estimators lead
to more robust nonparanormal versions of these tools?

ACKNOWLEDGEMENTS

research is

This
supported in part by DOE grant
DESC0011114 and NSF grant IIS1563887 to B.P., and by
an NSF Graduate Research Fellowship to S.S. under Grant
No. DGE-1252522. Any opinions, ﬁndings, and conclu-
sions or recommendations expressed in this material are
those of the author(s) and do not necessarily reﬂect the
views of the National Science Foundation.

References

Adami, Christoph.

Information theory in molecular biology.

Physics of Life Reviews, 1(1):3–22, 2004.

SampleSize(log10(n))1.522.53log10(MSE)-2-10123ˆIˆIGˆIρˆIτˆIKNNnon-GaussianFraction(α)00.51-10123Fractionofoutliers(β)00.20.4-2024Cov(X1,X2)0.9990.99 0.9  0    -3-2.5-2-1.5-1Nonparanormal Information Estimation

Aghagolzadeh, Mehdi, Soltanian-Zadeh, Hamid, Araabi, B, and
Aghagolzadeh, Ali. A hierarchical clustering based on mutual
information maximization. In Image Processing, 2007. ICIP
2007. IEEE International Conference on, volume 1, pp. I–277.
IEEE, 2007.

Aghakouchak, Amir. Entropy–copula in hydrology and climatol-

ogy. J. Hydrometeorology, 15(6):2176–2189, 2014.

Ahmed, Nabil Ali and Gokhale, DV. Entropy expressions and
their estimators for multivariate distributions. IEEE Trans. on
Information Theory, 35(3):688–692, 1989.

Anderson, TW. Multivariate statistical analysis. Wi1ey and Sons,

New York, NY, 1984.

Beirlant, Jan, Dudewicz, Edward J, Gy¨orﬁ, L´aszl´o, and Van der
Meulen, Edward C. Nonparametric entropy estimation: An
overview. International Journal of Mathematical and Statisti-
cal Sciences, 6(1):17–39, 1997.

Bercher, J-F and Vignat, Christophe. Estimating the entropy of a
signal with applications. IEEE Trans. on Signal Processing, 48
(6):1687–1694, 2000.

Berezi´nski, Przemysław, Jasiul, Bartosz, and Szpyrka, Marcin.
An entropy-based network anomaly detection method. En-
tropy, 17(4):2367–2408, 2015.

Gao, Shuyang, Ver Steeg, Greg, and Galstyan, Aram. Efﬁcient
estimation of mutual information for strongly dependent vari-
ables. In AISTATS, 2015.

Garner, Wendell R. Uncertainty and structure as psychological

concepts. 1962.

Gershgorin, Semyon Aranovich. Uber die abgrenzung der eigen-

werte einer matrix. (6):749–754, 1931.

Goria, Mohammed Nawaz, Leonenko, Nikolai N, Mergel, Vic-
tor V, and Novi Inverardi, Pier Luigi. A new class of random
vector entropy estimators and its applications in testing sta-
tistical hypotheses. Nonparametric Statistics, 17(3):277–297,
2005.

Han, Insu, Malioutov, Dmitry, and Shin, Jinwoo. Large-scale log-
determinant computation through stochastic chebyshev expan-
sions. In ICML, pp. 908–917, 2015.

Henrion, Didier and Malick, J´erˆome. Projection methods in conic
optimization. In Handbook on Semideﬁnite, Conic and Poly-
nomial Optimization, pp. 565–600. Springer, 2012.

Hern´andez-Lobato, Jos´e Miguel, Lloyd, James R, and Hern´andez-
Lobato, Daniel. Gaussian process conditional copulas with ap-
plications to ﬁnancial time series. In Advances in Neural In-
formation Processing Systems, pp. 1736–1744, 2013.

Berkes, Pietro, Wood, Frank, and Pillow, Jonathan W. Character-
izing neural dependencies with copula models. In Advances in
Neural Information Processing Systems, pp. 129–136, 2009.

Hero, Alfred O, Ma, Bing, Michel, Olivier, and Gorman, John.
Alpha-divergence for classiﬁcation, indexing and retrieval (re-
vised). 2001.

Bickel, Peter J and Levina, Elizaveta. Regularized estimation of
large covariance matrices. Annals of Stat., pp. 199–227, 2008.

Birg´e, Lucien and Massart, Pascal. Estimation of integral func-

tionals of a density. Annals of Stat., pp. 11–29, 1995.

Cai, T Tony and Yuan, Ming. Adaptive covariance matrix es-
timation through block thresholding. Annals of Stat., 40(4):
2014–2042, 2012.

Cai, T Tony, Liang, Tengyuan, and Zhou, Harrison H. Law of
log determinant of sample covariance matrix and optimal es-
timation of differential entropy for high-dimensional Gaussian
distributions. J. of Multivariate Analysis, 137:161–172, 2015.

Calsaverini, Rafael S and Vicente, Renato. An information-
theoretic approach to statistical dependence: Copula informa-
tion. EPL (Europhysics Letters), 88(6):68003, 2009.

Chai, Barry, Walther, Dirk, Beck, Diane, and Fei-Fei, Li. Explor-
ing functional connectivities of the human brain using multi-
variate information analysis. In Advances in neural informa-
tion processing systems, pp. 270–278, 2009.

Chow, C and Liu, Cong. Approximating discrete probability dis-
tributions with dependence trees. IEEE transactions on Infor-
mation Theory, 14(3):462–467, 1968.

Efron, Bradley and Stein, Charles. The jackknife estimate of vari-

ance. The Annals of Statistics, pp. 586–596, 1981.

Elidan, Gal. Copulas in machine learning. In Copulae in mathe-
matical and quantitative ﬁnance, pp. 39–60. Springer, 2013.

Hero, Alfred O, Ma, Bing, Michel, Olivier JJ, and Gorman, John.
Applications of entropic spanning graphs. IEEE Signal Pro-
cessing Magazine, 19(5):85–95, 2002.

Hoff, Peter D. Extending the rank likelihood for semiparametric
copula estimation. The Annals of Applied Statistics, pp. 265–
283, 2007.

Hunter, Jacob S and Hodas, Nathan O. Mutual information for ﬁt-
ting deep nonlinear models. arXiv preprint arXiv:1612.05708,
2016.

Ince, Robin AA, Giordano, Bruno L, Kayser, Christoph, Rousse-
let, Guillaume A, Gross, Joachim, and Schyns, Philippe G. A
statistical framework for neuroimaging data analysis based on
mutual information estimated via a gaussian copula. Human
brain mapping, 38(3):1541–1573, 2017.

Kandasamy, Kirthevasan, Krishnamurthy, Akshay, Poczos, Barn-
abas, Wasserman, Larry, and Robins, James M. Nonparamet-
ric von mises estimators for entropies, divergences and mutual
informations. In Advances in Neural Information Processing
Systems, pp. 397–405, 2015.

Klaassen, Chris AJ and Wellner, Jon A. Efﬁcient estimation in
the bivariate normal copula model: normal margins are least
favourable. Bernoulli, 3(1):55–77, 1997.

Krishnamurthy, Akshay, Kandasamy, Kirthevasan, Poczos, Barn-
abas, and Wasserman, Larry. Nonparametric estimation of
renyi divergence and friends. In International Conference on
Machine Learning, pp. 919–927, 2014.

Friedman, Jerome H and Stuetzle, Werner. Projection pursuit re-

Kruskal, William H. Ordinal measures of association. JASA, 53

gression. JASA, 76(376):817–823, 1981.

(284):814–861, 1958.

Nonparanormal Information Estimation

Learned-Miller, E. G. and Fisher, J. W. ICA using spacings es-
timates of entropy. Journal of Machine Learning Research, 4:
1271–1295, 2003.

Shan, Caifeng, Gong, Shaogang, and McOwan, Peter W. Condi-
tional mutual infomation based boosting for facial expression
recognition. In BMVC, 2005.

Liu, Han, Han, Fang, Yuan, Ming, Lafferty, John, and Wasser-
man, Larry. High-dimensional semiparametric gaussian copula
graphical models. The Annals of Statistics, 40(4):2293–2326,
2012.

Ma, Jian and Sun, Zengqi. Mutual information is copula entropy.

Tsinghua Science & Tech., 16(1):51–54, 2011.

Malevergne, Yannick and Sornette, Didier. Testing the gaussian
copula hypothesis for ﬁnancial assets dependences. Quantita-
tive Finance, 3(4):231–250, 2003.

Misra, Neeraj, Singh, Harshinder, and Demchuk, Eugene. Esti-
mation of the entropy of a multivariate normal distribution. J.
Multivariate Analysis, 92(2):324–342, 2005.

Mitra, Ritwik and Zhang, Cun-Hui. Multivariate analysis of
nonparametric estimates of large correlation matrices. arXiv
preprint arXiv:1403.6195, 2014.

Moon, Kevin and Hero, Alfred. Multivariate f-divergence esti-
In Advances in Neural Information

mation with conﬁdence.
Processing Systems, pp. 2420–2428, 2014a.

Moon, Kevin R and Hero, Alfred O. Ensemble estimation of mul-
In IEEE International Symposium on

tivariate f-divergence.
Information Theory (ISIT), pp. 356–360. IEEE, 2014b.

Moon, Kevin R, Sricharan, Kumar, Greenewald, Kristjan, and
Hero, Alfred O. Improving convergence of divergence func-
tional ensemble estimators. In IEEE International Symposium
on Information Theory (ISIT), pp. 1133–1137. IEEE, 2016.

Moon, Kevin R, Sricharan, Kumar, and Hero III, Alfred O.
Ensemble estimation of mutual information. arXiv preprint
arXiv:1701.08083, 2017.

Noble, Caleb C and Cook, Diane J. Graph-based anomaly detec-

tion. In KDD, pp. 631–636. ACM, 2003.

Nychis, George, Sekar, Vyas, Andersen, David G, Kim, Hyong,
and Zhang, Hui. An empirical evaluation of entropy-based
In Proceedings of the 8th ACM
trafﬁc anomaly detection.
SIGCOMM conference on Internet measurement, pp. 151–156.
ACM, 2008.

P´al, D´avid, P´oczos, Barnab´as, and Szepesv´ari, Csaba. Estimation
of R´enyi entropy and mutual information based on generalized
nearest-neighbor graphs. In Advances in Neural Information
Processing Systems, pp. 1849–1857, 2010.

Peng, Hanchuan, Long, Fuhui, and Ding, Chris. Feature selec-
tion based on mutual information criteria of max-dependency,
max-relevance, and min-redundancy. IEEE Trans. on Pattern
Analysis and Machine Intelligence, 27(8):1226–1238, 2005.

Renard, Benjamin and Lang, Michel. Use of a Gaussian cop-
ula for multivariate extreme value analysis: some case studies
in hydrology. Advances in Water Resources, 30(4):897–912,
2007.

Shishkin, Alexander, Bezzubtseva, Anastasia, Drutsa, Alexey,
Shishkov,
Ilia, Gladkikh, Ekaterina, Gusev, Gleb, and
Serdyukov, Pavel. Efﬁcient high-order interaction-aware fea-
ture selection based on conditional mutual information. In Ad-
vances in Neural Information Processing Systems, pp. 4637–
4645, 2016.

Singh, Shashank and P´oczos, Barnab´as. Exponential concentra-
tion of a density functional estimator. In Advances in Neural
Information Processing Systems, pp. 3032–3040, 2014a.

Singh, Shashank and P´oczos, Barnab´as. Generalized exponential
concentration inequality for R´enyi divergence estimation.
In
ICML, pp. 333–341, 2014b.

Singh, Shashank and P´oczos, Barnab´as. Finite-sample analysis of
ﬁxed-k nearest neighbor density functional estimators. In Ad-
vances in Neural Information Processing Systems, pp. 1217–
1225, 2016a.

Singh, Shashank and P´oczos, Barnab´as. Analysis of k-nearest
neighbor distances with application to entropy estimation.
arXiv preprint arXiv:1603.08578, 2016b.

Sricharan, Kumar, Raich, Raviv, and Hero, Alfred O. k-nearest
In IEEE
neighbor estimation of entropies with conﬁdence.
International Symposium on Information Theory (ISIT), pp.
1205–1209. IEEE, 2011.

Sricharan, Kumar, Wei, Dennis, and Hero, Alfred O. Ensemble
estimators for multivariate entropy estimation. IEEE Transac-
tions on Information Theory, 59(7):4374–4388, 2013.

Srivastava, Santosh and Gupta, Maya R. Bayesian estimation
In IEEE Inter-
of the entropy of the multivariate Gaussian.
national Symposium on Information Theory (ISIT), pp. 1103–
1107. IEEE, 2008.

Studen`y, Milan and Vejnarov´a, Jirina. The multiinformation func-
tion as a tool for measuring stochastic dependence. In Learning
in graphical models, pp. 261–297. Springer, 1998.

Szab´o, Zolt´an, P´oczos, Barnab´as, and L˝orincz, Andr´as. Under-
complete blind subspace deconvolution. Journal of Machine
Learning Research, 8(May):1063–1095, 2007a.

Szab´o, Zolt´an, P´oczos, Barnab´as, Szirtes, G´abor, and L˝orincz,
Andr´as. Post nonlinear independent subspace analysis. In In-
ternational Conference on Artiﬁcial Neural Networks, pp. 677–
686. Springer, 2007b.

Tsybakov, A.B.

Introduction to Nonparametric Estimation.

Springer Publishing Company, 1st edition, 2008.

Van den Bos, Adriaan. Parameter estimation for scientists and

engineers. John Wiley & Sons, 2007.

Varga, Richard S. Matrix Iterative Analysis, volume 27. Springer

Science & Business Media, 2009.

Rey, M´elanie and Roth, Volker. Meta-Gaussian information bot-
In Advances in Neural Information Processing Sys-

tleneck.
tems, pp. 1916–1924, 2012.

Watanabe, Satosi. Information theoretical analysis of multivariate
correlation. IBM J. of research and development, 4(1):66–82,
1960.

Nonparanormal Information Estimation

Wilson, Andrew and Ghahramani, Zoubin. Copula processes.
In Advances in Neural Information Processing Systems, pp.
2460–2468, 2010.

Wolsztynski, E., Thierry, E., and Pronzato, L. Minimum-entropy
estimation in semi-parametric models. Signal Process., 85(5):
937–949, 2005. ISSN 0165-1684.

B. Proofs of Main Results

Here, we give proofs of our main theoretical results, begin-
ning with upper bounds on the MSE of (cid:98)Iρ and proceeding
to minimax lower bounds in terms of Σ.

C. Upper bounds on the MSE of (cid:98)Iρ

A. Lemmas

Our proofs rely on the following lemmas.

Lemma 11. (Convexity of the inverse operator norm):
The function A (cid:55)→ (cid:107)A−1(cid:107)2 is convex over A (cid:31) 0.

Proof: For A, B (cid:31) 0, let C := τ A + (1 − τ )B. Then,

Proposition 13.

(cid:12)
(cid:12)
(cid:12)

E

(cid:12)
(cid:12)
(cid:12)

(cid:105)

(cid:104)
log |(cid:98)Σz|

(cid:107)Σ(cid:107)2


2

− log |Σ|



+



D
z2n

≤ C

(cid:18) z

(cid:19)

λj(Σ)

2






 .

(cid:88)

log

λj (Σ)<z

Proof: By the triangle inequality,

(cid:12)
(cid:12)
E
(cid:12)

(cid:104)
log |(cid:98)Σz|

(cid:105)

− log |Σ|

(cid:105)

E

(cid:12)
(cid:12)
(cid:12)

(cid:104)
log |(cid:98)Σz|

(cid:12)
(cid:12)
(cid:12) ≤
+ |log |Σz| − log |Σ||

(cid:12)
(cid:12)
− log |Σz|
(cid:12)

For the ﬁrst term, applying the matrix mean value theorem
(Lemma 12) and the inequality (cid:107)A(cid:107)F ≤

D(cid:107)A(cid:107)2

√

(cid:104)

(cid:12)
(cid:12)
(cid:12)

E

log

(cid:105)

(cid:12)
(cid:12)
(cid:12)(cid:98)Σz

(cid:12)
(cid:12)
(cid:12)

− log |Σz|

(cid:105)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ E
1
z
√

≤

E

(cid:12)
(cid:12)
(cid:12)(cid:98)Σz

(cid:12)
(cid:12)
(cid:12) − log |Σz|
(cid:13)
(cid:13)
(cid:13)F

(cid:104)(cid:12)
(cid:12)
(cid:12)log
(cid:104)(cid:13)
(cid:13)
(cid:13)(cid:98)Σz − Σz
(cid:104)(cid:13)
(cid:13)
(cid:13)(cid:98)Σz − Σz

(cid:13)
(cid:13)
(cid:13)2

D
z

E

(cid:105)

(cid:105)

≤

≤

CM Z(cid:107)Σ(cid:107)2D
√

,

z

n

where we used Theorem 1 of Mitra & Zhang (2014), which
gives a constant CM Z such that

E

(cid:105)

(cid:13)
(cid:104)(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:98)Σz − Σz
(cid:13)2
√

≤ CM Z(cid:107)Σ(cid:107)2

(cid:114)

D
n

.

Via the bound (cid:107)Σ(cid:107)2 ≤

D(cid:107)Σ(cid:107)∞, this reduces to

E

(cid:104)(cid:13)
(cid:13)
(cid:13)(cid:98)Σz − Σz

(cid:105)

(cid:13)
(cid:13)
(cid:13)2

≤ CM Z

D
√
n

.

(cid:107) (cid:98)C −1(cid:107)2 =

1
inf x∈RD xT Cx

=

≤

1
inf x∈RD τ xT Ax + (1 − τ )xT Bx

1
τ inf x∈RD xT Ax + (1 − τ ) inf x∈RD xT Bx

≤ τ

= τ (cid:13)

1
inf x∈RD xT Ax
(cid:13)A−1(cid:13)

(cid:13)2 + (1 − τ ) (cid:13)

1
+ (1 − τ )
inf x∈RD xT Bx
(cid:13)B−1(cid:13)
(cid:13)2

via convexity of the function x (cid:55)→ 1/x on (0, ∞).

12.

(Mean-Value Bound on the Log-
Lemma
Determinant): Matrix derivative of
log-determinant.
Suppose A, B (cid:31) 0. Then, for λ := min{λD(A), λD(B)},

|log |A| − log |B|| ≤

(cid:107)A − B(cid:107)F .

Proof: Proof: First recall that the log-determinant is
continuously differentiable over the strict positive deﬁnite
cone, with ∇X log |X| = X −1 for any X (cid:31) 0. Hence, by
the matrix-valued version of the mean value theorem,

log |A| − log |B| = tr(C −1(A − B)),

where C = τ A + (1 − τ )B for some τ ∈ (0, 1). Since for
positive deﬁnite matrices, the inner product can be bounded
by the product of the operator and Frobenius norms, and
clearly C (cid:31) 0, we have

1
λ

1
λ

|log |A| − log |B|| = (cid:107)C −1(cid:107)2(cid:107)A − B(cid:107)F .

Proposition 14.

V

(cid:105)
(cid:104)
(cid:98)I

≤

36π2D2
z2n

.

Finally, it follows by Lemma 11 that

|log |A| − log |B|| ≤

(cid:107)A − B(cid:107)F .

Proof: By the Efron-Stein inequality (Efron & Stein,
1981), since X1, . . . , Xn are independent and identically

distributed,

This translates to a variance bound of

Nonparanormal Information Estimation

V

(cid:104)
(cid:105)
(cid:98)I

≤

log |(cid:98)Σz| − log |(cid:98)Σ(i)
z |

(cid:17)2(cid:21)

=

E

log |(cid:98)Σz| − log |(cid:98)Σ(1)
z |

(cid:17)2(cid:21)

,

(cid:20)(cid:16)

E

n
(cid:88)

i=1

(cid:20)(cid:16)

1
2

n
2

z

is our estimator after

where (cid:98)Σ(1)
independently re-
sampling the ﬁrst sample X1. Applying the multivariate
mean-value theorem (Lemma 12), we have

of Σ

V

(cid:104)
(cid:105)
(cid:98)I

≤

36π2D2
z2n

.

C.1. Lower bound for rank-based estimators in terms

One (perhaps surprising) result of Cai et al. (2015) is that,
as long as D/n → 0, the convergence rate of the estimator
is independent of the true correlation structure Σ. Here,
we show that this desirable property does not hold in the
nonparanormal case.

Proposition 15. Consider the 2-dimensional case

X1, ..., Xn

i.i.d∼ N (0, Σ),

with Σ =

,

(11)

(cid:21)

(cid:20)1 σ
σ 1

and let σ∗ ∈ (0, 1). Suppose an estimator (cid:98)I = (cid:98)I(R) of
Iσ = − 1
2 log(1 − σ2) is a function of the empirical rank
matrix R ∈ Nn×2 of X (as deﬁned in (2)). Then, there
exists a constant C > 0, depending only n, such that the
worst-case MSE of (cid:98)I over σ ∈ (0, σ∗) satisﬁes

sup
σ∈(0,σ∗]

E

(cid:20)(cid:16)

(cid:98)I(R) − Iσ

(cid:17)2(cid:21)

≥

1
64

(cid:0)C − log(1 − σ2

∗)(cid:1)2

→ ∞ as

σ∗ → 1.

Proof: Note that the rank matrix R can take only ﬁnitely
many values. Let R be the set of all (n!)D possible rank
matrices and let R1 ⊆ R be the set of n! rank matrices that
are perfectly correlated. Then, as σ → 1, P[R ∈ R1] → 1,
so, in particular, we can pick σ0 (depending only on n)
such that, for all σ ≥ σ0, P[R ∈ R1] ≥ 1
2 . Since the data
are i.i.d., all rank matrices in R1 have equal probability. It
follows that

DT V (P0||P1) =

(cid:107)P0 − P1(cid:107)1 ≤

1
2

1
2

,

where DT V denotes total variation distance. Finally, by Le
Cam’s Lemma (see, e.g., Section 2.3 of Tsybakov (2008)),

E

sup
σ∈{σ0,σ1}
(Iσ∗ − Iσ0)2
8
(log(1 − σ2

inf
(cid:98)I

≥

≥

(cid:20)(cid:16)

(cid:17)2(cid:21)

(cid:98)I − Iσ

(1 − DT V (Pσ0, Pσ1))

0) − log(1 − σ2

∗))2

64

(cid:12)
(cid:12)
(cid:12)log |(cid:98)Σz| − log |(cid:98)Σ(1)
(cid:12)
(cid:12)
(cid:12) ≤
z |

1
z

(cid:107)(cid:98)Σz − (cid:98)Σ(1)

z (cid:107)F .

τ (cid:107)2 ≤ 1

(cid:107)(cid:98)Σ−1
z . Since S(z) is convex and the Frobe-
nius norm is supported by an inner product, the opera-
tion of projecting onto S(z) is a contraction. In particular,
(cid:13)
(cid:98)Σ − (cid:98)Σ(1)(cid:17)(cid:13)
(cid:13)
(cid:16)
(cid:13)
(cid:13)
(cid:13)
≤
Applying the mean
(cid:13)F
(cid:13)
(cid:13)
value theorem to the function x (cid:55)→ 2 sin (cid:0) π

(cid:98)Σz − (cid:98)Σ(1)

(cid:17)(cid:13)
(cid:13)
(cid:13)F

(cid:16)

z

6 x(cid:1),

(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:98)Σ − (cid:98)Σ(1)(cid:17)(cid:13)
2
(cid:13)
(cid:13)

F

D
(cid:88)

j,k=1

(cid:16)

(cid:98)Σ − (cid:98)Σ(1)(cid:17)2

j,k

=

≤

=

π2
9

π2
9

D
(cid:88)

(cid:16)

(cid:17)2

(cid:98)ρj,k − (cid:98)ρ(1)

j,k

j,k=1
(cid:13)
(cid:13)

(cid:13)(cid:98)ρ − (cid:98)ρ(1)(cid:13)

2
(cid:13)
(cid:13)

F

.

(7)

(8)

(9)

From the formula

(cid:98)ρj,k = 1 −

6 (cid:80)n
i=1 d2
n(n2 − 1)

i,j,k

,

(where di,j,k denotes the difference in ranks of Xi,j and
Xi,k in X1,j, ..., Xn,j and X1,k, ..., Yn,k, respectively), one
can see, since |d1,j,k − d(cid:48)
1,j,k| ≤ n and, for i (cid:54)= 1, |di,j,k −
d(cid:48)
i,j,k| ≤ 1, that

(cid:12)
(cid:12)(cid:98)ρj,k − (cid:98)ρ(1)
(cid:12)

j,k

(cid:12)
(cid:12)
(cid:12) ≤

18
n

,

and hence that

(cid:107)(cid:98)ρ − (cid:98)ρ(1)(cid:107)F ≤

18D
n

.

(10)

It follows from inequality (9) that

(cid:107)(cid:98)Σz − (cid:98)Σ(1)

z (cid:107)F ≤

6πD
n

.

Altogether, this gives

(cid:12)
(cid:12)
(cid:12)log |(cid:98)Σz| − log |(cid:98)Σ(1)
(cid:12)
(cid:12)
z |
(cid:12) ≤
Then, McDiarmid’s Inequality gives, for all ε > 0,

6πD
zn

.

P

(cid:104)(cid:12)
(cid:12)
(cid:12)(cid:98)I − E

(cid:105)(cid:12)
(cid:105)
(cid:104)
(cid:12)
(cid:12) > ε
(cid:98)I

= 2 exp

−

(cid:18)

nz2ε2
18π2D2

(cid:19)

.

D. Details of Experimental Methods

F. Speciﬁc Assumptions for Estimating H(X)

Nonparanormal Information Estimation

Here, we present details needed to reproduce our numerical
simulations. Note that MATLAB source code for these ex-
periments is available at [Omitted for anonymity.], includ-
ing a single runnable script that performs all experiments
and generates all ﬁgures presented in this paper. Speciﬁc
details needed to reproduce experiments are given in the
Appendix,

In short, experiments report empirical mean squared er-
rors based on 100 i.i.d.
trials of each condition. We ini-
tially computed 95% conﬁdence intervals, but these inter-
vals were consistently smaller than marker sizes, so we
omitted them to avoid cluttering plots. Except as spec-
iﬁed otherwise, each experiment followed the same ba-
sic structure, as follows: In each trial, a random correla-
tion matrix Σ ∈ [−1, 1]D×D was drawn by normalizing a
covariance matrix from a Wishart distribution W (ID, D)
with identity scale matrix and D degrees of freedom. Data
X1, ..., Xn were then drawn i.i.d. from N (0, Σ). All es-
timators were applied to the same data. Unless speciﬁed
otherwise, n = 100 and D = 25.

D.1. Computational Considerations

In general, the running time of all the nonparanormal es-
timators considered is O(Dn log n + D2n + D3) (i.e.,
O(Dn log n) to rank or Gaussianize the variables in
each dimension, D2n to compute the covariance matrix,
and O(D3) to compute the log-determinant). All log-
determinants log |Σ| were computed by summing the loga-
rithms of the diagonal of the Cholesky decomposition of Σ,
as this is widely considered to be a fast and numerically sta-
ble approach. Note however that faster (O(D)-time) ran-
domized algorithms (Han et al., 2015) have been proposed
to approximate the log-determinant).

E. Additional Experimental Results

Here, we present variants on the experiments presented in
the main paper, which support but are not necessary for
illustrating our conclusions.

E.1. Effects of Other Marginal Transformations

In Section 7, we showed that the Gaussian estimator (cid:98)I is
highly sensitive to failure of the Gaussian assumption for
even a small fraction of marginals. Figure 1(b), illustrates
this for the transformation x (cid:55)→ exp(x), but we show here
that this is not speciﬁc to the exponential transformation.
As shown in Figures 2 nearly identical results hold when
the marginal transformation f is the hyperbolic tangent
function x (cid:55)→ tanh(x), the cubic function x (cid:55)→ x3, sig-
moid function x (cid:55)→ 1

1+e−x , or standard normal CDF.

As shown in the main paper, to estimate the entropy of a
nonparanormal distribution at the rate O(D2/n), it sufﬁces
to the univariate entropy of each variable Xj at the rate
O(1/n). To do this, additional assumptions are required on
the marginal densities pj. Here, we give detailed sufﬁcient
conditions for this.
Letting Sj ⊆ R denote the support of pj, the two key as-
sumptions can be roughly classiﬁed as follows:

(a) 1

2 -order smoothness9; e.g., a H¨older condition:

sup
x(cid:54)=y∈Sj

|pj(x) − pj(y)|
|x − y|1/2

< L,

or a (slightly weaker) Sobolev condition:

(cid:90)

Sj

p2
j (x) dx < ∞ and

|ξ|1/2|F [pj] (ξ)|

dξ < L,

(cid:17)2

(cid:90)

(cid:16)

Sj

(where F [pj] (ξ) denotes the Fourier transform of pj
evaluated at ξ) for some constant L > 0.

(b) absolute bounds pj(x) ∈ [κ1, κ2] for all x ∈ Sj or

(aj, bj)-exponential tail bounds

f (x)
exp(−ajxbj )

∈ [κ1, κ2]

for all x ∈ Sj

for some κ1, κ2 ∈ (0, ∞).

Under these assumptions, there are a variety of nonpara-
metric univariate entropy estimators that have been shown
to converge at the rate O(1/n) (Beirlant et al., 1997; Kan-
dasamy et al., 2015; Singh & P´oczos, 2016b; Moon et al.,
2016).

G. Lower bounding the eigenvalues of a

bandable matrix

Recall that, for c ∈ (0, 1), a matrix Σ ∈ RD×D is called
c-bandable if there exists a constant c ∈ (0, 1) such that,
for all i, j ∈ D, |Σi,j| ≤ c|i−j|.

Here, we show simple bounds on the eigenvalues of a
bandable correlation matrix Σ. While this result is fairly
straightforward, a brief search the literature turned up no
comparable results. Bickel & Levina (2008), who orig-
inally introduced the class of bandable covariance matri-
ces, separately assumed the existence of lower and upper

9This is stronger than the 1

4 -order smoothness mandated by
the minimax rate for entropy estimation (Birg´e & Massart, 1995),
but appears necessary for most practical entropy estimators. See
Section 4 of Kandasamy et al. (2015) for further details.

Nonparanormal Information Estimation

(a) T (x) = x3

(b) T (x) = tanh(x)

(c) T (x) = 1

1+e−x

(d) T (x) = Φ(x)

Figure 2. Semi-log plot of mean squared error of various estimators over the fraction of non-Gaussian marginals α ∈ [0, 1], for various
marginal transforms T .

bounds on the eigenvalues to prove their results.
In the
context of information estimation, this results of particular
interest because, when c < 1/3 it implies a dimension-
free positive lower bound on the minimum eigenvalue of
Σ, hence complementing our upper bound in Theorem 8.
Proposition 16. Suppose a symmetric matrix Σ ∈ RD×D
is c-bandable and has identical diagonal entries Σj,j =
1. Then, the eigenvalues λ1(Σ), ..., λD(Σ) of Σ can be
bounded as

1 − 3c
1 − c

≤ λ1(Σ), ..., λD(Σ) ≤

1 + c
1 − c

.

In particular, when c < 1/3, we have

0 <

1 − 3c
1 − c

≤ λD(Σ).

Proof: The proof is based on the Gershgorin circle theo-
rem (Gershgorin, 1931; Varga, 2009). In the case of a real
symmetric matrix Σ, this states that the eigenvalues of Σ
lie within a union of intervals

D
(cid:91)

j=1

{λ1(Σ), ..., λD(Σ)} ⊆

[Σj,j − Rj, Σj,j + Rj] , (12)

where Rj := (cid:80)
k(cid:54)=j |Σj,k| is the sum of the absolute values
of the non-diagonal entries of the jth row of Σ. In our case,
since the diagonal entries of Σ are all Σj,j = 1, we simply
have to bound

max
j∈[D]

Rj ≤

c|k−j|.

(cid:88)

k(cid:54)=j

This geometric sum is maximized when j = (cid:100)D/2(cid:101), giving

Rj ≤ 2

cδ = 2c

(cid:98)D/2(cid:99)
(cid:88)

δ=1

1 − c(cid:98)D/2(cid:99)
1 − c

≤

2c
1 − c

.

Finally, the inclusion (12) gives

λD(Σ) ≥ 1 −

2c
1 − c
1−c = 1+c
1−c .

when c < 1/3. 1 + 2c

=

1 − 3c
1 − c

> 0

non-GaussianFraction(α)00.51log10(MSE)-101234non-GaussianFraction(α)00.51log10(MSE)-10123non-GaussianFraction(α)00.51log10(MSE)-101234non-GaussianFraction(α)00.51log10(MSE)-10123