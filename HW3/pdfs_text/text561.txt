Robust Budget Allocation via Continuous Submodular Functions

Matthew Staib 1 Stefanie Jegelka 1

Abstract

The optimal allocation of resources for maximiz-
ing inﬂuence, spread of information or coverage,
has gained attention in the past years, in particu-
lar in machine learning and data mining. But in
applications, the parameters of the problem are
rarely known exactly, and using wrong parame-
ters can lead to undesirable outcomes. We hence
revisit a continuous version of the Budget Allo-
cation or Bipartite Inﬂuence Maximization prob-
lem introduced by Alon et al. (2012) from a ro-
bust optimization perspective, where an adver-
sary may choose the least favorable parameters
within a conﬁdence set. The resulting problem
is a nonconvex-concave saddle point problem
(or game). We show that this nonconvex prob-
lem can be solved exactly by leveraging connec-
tions to continuous submodular functions, and by
solving a constrained submodular minimization
problem. Although constrained submodular min-
imization is hard in general, here, we establish
conditions under which such a problem can be
solved to arbitrary precision (cid:15).

1. Introduction

The optimal allocation of resources for maximizing inﬂu-
ence, spread of information or coverage, has gained at-
tention in the past few years, in particular in machine
learning and data mining (Domingos & Richardson, 2001;
Kempe et al., 2003; Chen et al., 2009; Gomez Rodriguez &
Sch¨olkopf, 2012; Borgs et al., 2014).

In the Budget Allocation Problem, one is given a bipartite
inﬂuence graph between channels S and people T , and the
task is to assign a budget y(s) to each channel s in S with
the goal of maximizing the expected number of inﬂuenced
people I(y). Each edge (s, t) ∈ E between channel s and

1Massachusetts Institute of Technology. Correspondence
to: Matthew Staib <mstaib@mit.edu>, Stefanie Jegelka <ste-
fje@mit.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

person t is weighted with a probability pst that, e.g., an
advertisement on radio station s will inﬂuence person t to
buy some product. The budget y(s) controls how many in-
dependent attempts are made via the channel s to inﬂuence
the people in T . The probability that a customer t is inﬂu-
enced when the advertising budget is y is

It(y) = 1 −

(cid:89)

(s,t)∈E

[1 − pst]y(s),

(1)

and hence the expected number of inﬂuenced people is
I(y) = (cid:80)
t∈T It(y). We write I(y; p) = I(y) to make the
dependence on the probabilities pst explicit. The total bud-
get y must remain within some feasible set Y which may
encode e.g. a total budget limit (cid:80)
s∈S y(s) ≤ C. We allow
the budgets y to be continuous, as in (Bian et al., 2017).

Since its introduction by Alon et al. (2012), several works
have extended the formulation of Budget Allocation and
provided algorithms (Bian et al., 2017; Hatano et al., 2015;
Maehara et al., 2015; Soma et al., 2014; Soma & Yoshida,
2015). Budget Allocation may also be viewed as inﬂu-
ence maximization on a bipartite graph, where information
spreads as in the Independent Cascade model. For integer
y, Budget Allocation and Inﬂuence Maximization are NP-
hard. Yet, constant-factor approximations are possible, and
build on the fact that the inﬂuence function is submodular
in the binary case, and DR-submodular in the integer case
(Soma et al., 2014; Hatano et al., 2015). If y is continuous,
the problem is a concave maximization problem.

The formulation of Budget Allocation assumes that the
transmission probabilities are known exactly. But this is
rarely true in practice. Typically, the probabilities pst,
and possibly the graph itself, must be inferred from obser-
vations (Gomez Rodriguez et al., 2010; Du et al., 2013;
Narasimhan et al., 2015; Du et al., 2014; Netrapalli &
Sanghavi, 2012). In Section 4 we will see that a misspeciﬁ-
cation or point estimate of parameters pst can lead to much
reduced outcomes. A more realistic assumption is to know
conﬁdence intervals for the pst. Realizing this severe de-
ﬁciency, recent work studied robust versions of Inﬂuence
Maximization, where a budget y must be chosen that maxi-
mizes the worst-case approximation ratio over a set of pos-
sible inﬂuence functions (He & Kempe, 2016; Chen et al.,
2016; Lowalekar et al., 2016). The resulting optimization
problem is hard but admits bicriteria approximations.

Robust Budget Allocation via Continuous Submodular Functions

In this work, we revisit Budget Allocation under uncer-
tainty from the perspective of robust optimization (Bert-
simas et al., 2011; Ben-Tal et al., 2009). We maximize the
worst-case inﬂuence – not approximation ratio – for p in a
conﬁdence set centered around the “best guess” (e.g., pos-
terior mean). This avoids pitfalls of the approximation ratio
formulation (which can be misled to return poor worst-case
budgets, as demonstrated in Appendix A), while also allow-
ing us to formulate the problem as a max-min game:

max
y∈Y

min
p∈P

I(y; p),

(2)

where an “adversary” can arbitrarily manipulate p within
the conﬁdence set P. With p ﬁxed, I(y; p) is concave in y.
However, the inﬂuence function I(y; p) is not convex, and
not even quasiconvex, in the adversary’s variables pst.

The new, key insight we exploit in this work is that I(y; p)
has the property of continuous submodularity in p – in con-
trast to previously exploited submodular maximization in y
– and can hence be minimized by generalizing techniques
from discrete submodular optimization (Bach, 2015). The
techniques in (Bach, 2015), however, are restricted to box
constraints, and do not directly apply to our conﬁdence
sets. In fact, general constrained submodular minimization
is hard (Svitkina & Fleischer, 2011; Goel et al., 2009; Iwata
& Nagano, 2009). We make the following contributions:

1. We present an algorithm with optimality bounds for
Robust Budget Allocation in the nonconvex adversar-
ial scenario (2).

2. We provide the ﬁrst results for continuous submodu-
lar minimization with box constraints and one more
“nice” constraint, and conditions under which the al-
gorithm is guaranteed to return a global optimum.

1.1. Background and Related Work

We begin with some background material and, along the
way, discuss related work.

1.1.1. SUBMODULARITY OVER THE INTEGER LATTICE

AND CONTINUOUS DOMAINS

Submodularity is perhaps best known as a property of set
functions. A function F : 2V → R deﬁned on subsets
S ⊆ V of a ground set V is submodular if for all sets
S, T ⊆ V , it holds that F (S) + F (T ) ≥ F (S ∩ T ) +
F (S ∪T ). A similar deﬁnition extends to functions deﬁned
over a distributive lattice L, e.g. the integer lattice. Such a
function f is submodular if for all x, y ∈ L, it holds that

f (x) + f (y) ≥ f (x ∨ y) + f (x ∧ y).

(3)

For the integer lattice and vectors x, y, x ∨ y denotes the
coordinate-wise maximum and x ∧ y the coordinate-wise

minimum. Submodularity has also been considered on
continuous domains X ⊂ Rd, where, if f is also twice-
differentiable, the property of submodularity means that all
off-diagonal entries of the the Hessian are nonpositive, i.e.,
∂f (x)
≤ 0 for all i (cid:54)= j (Topkis, 1978, Theorem 3.2).
∂xi∂xj
These functions may be convex, concave, or neither.

Submodular functions on lattices can be minimized by a
reduction to set functions, more precisely, ring families
(Birkhoff, 1937). Combinatorial algorithms for submod-
ular optimization on lattices are discussed in (Khachaturov
et al., 2012). More recently, Bach (2015) extended results
based on the convex Lov´asz extension, by building on con-
nections to optimal transport. The subclass of L(cid:92)-convex
functions admits strongly polynomial time minimization
(Murota, 2003; Kolmogorov & Shioura, 2009; Murota &
Shioura, 2014), but does not apply in our setting.

Similarly, results for submodular maximization extend to
integer lattices, e.g. (Gottschalk & Peis, 2015). Stronger
results are possible if the submodular function also satis-
ﬁes diminishing returns: for all x ≤ y (coordinate-wise)
and i such that y + ei ∈ X , it holds that f (x + ei) − f (x) ≥
f (y+ei)−f (y). For such DR-submodular functions, many
approximation results for the set function case extend (Bian
et al., 2017; Soma & Yoshida, 2015; Soma et al., 2014). In
particular, Ene & Nguyen (2016) show a generic reduction
to set function optimization that they apply to maximiza-
tion. In fact, it also applies to minimization:

Proposition 1.1. A DR-submodular function f deﬁned
on (cid:81)n
i=1[ki] can be minimized in strongly polynomial
time O(n4 log4 k ·
log2(n log k) · EO + n4 log4 k ·
logO(1)(n log k)), where k = maxi ki and EO is the time
complexity of evaluating f . Here, [ki] = {0, 1, . . . , ki −1}.

In particular, the time complexity is logarithmic in k. For
general lattice submodular functions, this is not possible
without further assumptions.

1.1.2. RELATED PROBLEMS

A sister problem of Budget Allocation is Inﬂuence Max-
imization on general graphs, where a set of seed nodes
is selected to start a propagation process. The inﬂuence
function is still monotone submodular and amenable to
the greedy algorithm (Kempe et al., 2003), but it cannot
be evaluated explicitly and requires approximation (Chen
et al., 2010). Stochastic Coverage (Goemans & Vondr´ak,
2006) is a version of Set Cover where the covering sets
Si ⊂ V are random. A variant of Budget Allocation can be
written as stochastic coverage with multiplicity. Stochastic
Coverage has mainly been studied in the online or adap-
tive setting, where logarithmic approximation factors can
be achieved (Golovin & Krause, 2011; Deshpande et al.,
2016; Adamczyk et al., 2016).

Robust Budget Allocation via Continuous Submodular Functions

i xci

Our objective function (2) is a signomial in p, i.e., a lin-
ear combination of monomials of the form (cid:81)
i . Gen-
eral signomial optimization is NP-hard (Chiang, 2005), but
certain subclasses are tractable: posynomials with all non-
negative coefﬁcients can be minimized via Geometric Pro-
gramming (Boyd et al., 2007), and signomials with a sin-
gle negative coefﬁcient admit sum of squares-like relax-
ations (Chandrasekaran & Shah, 2016). Our problem, a
constrained posynomial maximization, is not in general
a geometric program. Some work addresses this setting
via monomial approximation (Pascual & Ben-Israel, 1970;
Ecker, 1980), but, to our knowledge, our algorithm is the
ﬁrst that solves this problem to arbitrary accuracy.

1.1.3. ROBUST OPTIMIZATION

Two prominent strategies of addressing uncertainty in pa-
rameters of optimization problems are stochastic and ro-
If the distribution of the parameters
bust optimization.
is known (stochastic optimization), formulations such as
value-at-risk (VaR) and conditional value-at-risk (CVaR)
(Rockafellar & Uryasev, 2000; 2002) apply.
In contrast,
robust optimization (Ben-Tal et al., 2009; Bertsimas et al.,
2011) assumes that the parameters (of the cost function and
constraints) can vary arbitrarily within a known conﬁdence
set U , and the aim is to optimize the worst-case setting, i.e.,

min
y

sup
u,A,b∈U

{g(y; u) s.t. Ay ≤ b}.

(4)

Here, we will only have uncertainty in the cost function.

In this paper we are principally concerned with robust max-
imization of the continuous inﬂuence function I(y), but
mention some results for the discrete case. While there
exist results for robust and CVaR optimization of modu-
lar (linear) functions (Nikolova, 2010; Bertsimas & Sim,
2003), submodular objectives do not in general admit such
optimization (Maehara, 2015), but variants admit approxi-
mations (Zhang et al., 2014). The brittleness of submodu-
lar optimization under noise has been studied in (Balkanski
et al., 2016; 2017; Hassidim & Singer, 2016).

directly, and write I(y; x) instead of I(y; p).

2.1. Stochastic Optimization

If a (posterior) distribution of the parameters is known, a
simple strategy is to use expectations. We place a uni-
form prior on xst, and observe nst independent observa-
tions drawn from Ber(xst). If we observe αst failures and
and βst successes, the resulting posterior distribution on
the variable Xst is Beta(1 + αst, 1 + βst). Given such a
posterior, we may optimize

I(y; E[X]), or

E[I(y; X)].

max
y∈Y

max
y∈Y

(5)

(6)

Proposition 2.1. Problems (5) and (6) are concave max-
imization problems over the (convex) set Y and can be
solved exactly.

Concavity of (6) follows since it is an expectation over con-
cave functions, and the problem can be solved by stochastic
gradient ascent or by explicitly computing gradients.

Merely maximizing expectation does not explicitly account
for volatility and hence risk. One option is to include vari-
ance (Ben-Tal & Nemirovski, 2000; Bertsimas et al., 2011;
Atamt¨urk & Narayanan, 2008):

−E[I(y; X)] + ε(cid:112)Var(I(y; X)),

(7)

min
y∈Y

but in our case this CVaR formulation seems difﬁcult:

Fact 2.1. For y in the nonnegative orthant,
the term
(cid:112)Var(I(y; X)) need not be convex or concave, and need
not be submodular or supermodular.

This observation does not rule out a solution, but the appar-
ent difﬁculties further motivate a robust formulation that, as
we will see, is amenable to optimization.

2.2. Robust Optimization

Approximations for robust submodular and inﬂuence opti-
mization have been studied in (Krause et al., 2008; He &
Kempe, 2016; Chen et al., 2016; Lowalekar et al., 2016),
where an adversary can pick among a ﬁnite set of objective
functions or remove selected elements (Orlin et al., 2016).

The focus of this work is the robust version of Budget Al-
location, where we allow an adversary to arbitrarily set the
parameters x within an uncertainty set X . This uncertainty
set may result, for instance, from a known distribution, or
simply assumed bounds. Formally, we solve

2. Robust and Stochastic Budget Allocation

max
y∈Y

min
x∈X

I(y; x),

(8)

The unknown parameters in Budget Allocation are the
transmission probabilities pst or edge weights in a graph.
If these are estimated from data, we may have posterior
distributions or, a weaker assumption, conﬁdence sets for
the parameters. For ease of notation, we will work with
the failure probabilities xst = 1 − pst instead of the pst

where Y ⊂ RS
+ is a convex set with an efﬁcient projec-
tion oracle, and X is an uncertainty set containing an es-
In the sequel, we use uncertainty sets X =
timate ˆx.
{x ∈ Box(l, u) : R(x) ≤ B}, where R is a distance (or di-
vergence) from the estimate ˆx, and Box(l, u) is the box
(cid:81)
(s,t)∈E[lst, ust]. The intervals [lst, ust] can be thought of

Robust Budget Allocation via Continuous Submodular Functions

as either conﬁdence intervals around ˆx, or, if [lst, ust] =
[0, 1], enforce that each xst is a valid probability.

Common examples of uncertainty sets used in Robust Op-
timization are Ellipsoidal and D-norm uncertainty sets
(Bertsimas et al., 2011). Our algorithm in Section 3.1 ap-
plies to both.

Ellipsoidal uncertainty. The ellipsoidal or quadratic un-
certainty set is deﬁned by

X Q(γ) = {x ∈ Box(0, 1) : (x − ˆx)T Σ−1(x − ˆx) ≤ γ},

where Σ is the covariance of the random vector X of proba-
bilities distributed according to our Beta posteriors. In our
case, since the distributions on each xst are independent,
Σ−1 is actually diagonal. Writing Σ = diag(σ2), we have

X Q(γ) =

x ∈ Box(0, 1) :

(cid:110)

Rst(xst) ≤ γ

(cid:111)
,

(cid:88)

(s,t)∈E

where Rst(x) = (xst − ˆxst)2σ−2
st .

D-norm uncertainty. The D-norm uncertainty set is simi-
lar to an (cid:96)1-ball around ˆx, and is deﬁned as

X D(γ) =

x : ∃c ∈ Box(0, 1) s.t.

(cid:110)

xst = ˆxst + (ust − ˆxst)cst,

(cid:88)

cst ≤ γ

(cid:111)
.

(s,t)∈E

Essentially, we allow an adversary to increase ˆxst up to
some upper bound ust, subject to some total budget γ
across all terms xst. The set X D(γ) can be rewritten as
(cid:111)
,

x ∈ Box(ˆx, u) :

X D(γ) =

Rst(xst) ≤ γ

(cid:88)

(cid:110)

(s,t)∈E

where Rst(xst) = (xst − ˆxst)/(ust − ˆxst) is the fraction
of the interval [ˆxst, ust] we have used up in increasing xst.

The min-max formulation maxy∈Y minx∈X I(y; x) has
several merits: the model is not tied to a speciﬁc learning
algorithm for the probabilities x as long as we can choose a
suitable conﬁdence set. Moreover, this formulation allows
to fully hedge against a worst-case scenario.

3. Optimization Algorithm

As noted above, the function I(y; x) is concave as a func-
tion of y for ﬁxed x. As a pointwise minimum of concave
functions, F (y) := minx∈X I(y; x) is concave. Hence, if
we can compute subgradients of F (y), we can solve our
max-min-problem via the subgradient method, as outlined
in Algorithm 1.

A subgradient gy ∈ ∂F (y) at y is given by the gradient of
I(y; x∗) for the minimizing x∗ ∈ arg minx∈X I(y; x), i.e.,

Algorithm 1 Subgradient Ascent

Input: suboptimality tolerance ε > 0, initial feasible
budget y(0) ∈ Y
Output: ε-optimal budget y for Problem (8)
repeat

x(k) ← arg minx∈X I(y(k); x)
g(k) ← ∇yI(y(k); x(k))
L(k) ← I(y(k); x(k))
U (k) ← maxy∈Y I(y; x(k))
γ(k) ← (U (k) − L(k))/(cid:107)g(k)(cid:107)2
2
y(k+1) ← projY (y(k) + γ(k)g(k))
k ← k + 1

until U (k) − L(k) ≤ ε

gy = ∇yI(y; x∗). Hence, we must be able to compute x∗
for any y. We also obtain a duality gap: for any x(cid:48), y(cid:48) we
have

min
x∈X

I(y(cid:48); x) ≤ max
y∈Y

min
x∈X

I(y; x) ≤ max
y∈Y

I(y; x(cid:48)).

(9)

This means we can estimate the optimal value I ∗ and
use it in Polyak’s stepsize rule for the subgradient method
(Polyak, 1987).

√

But I(y; x) is not convex in x, and not even quasicon-
vex. For example, standard methods (Wainwright & Chi-
ang, 2004, Chapter 12) imply that f (x1, x2, x3) = 1 −
x1x2 −
+. Moreover, the
above-mentioned signomial optimization techniques do not
apply for an exact solution either. So, it is not immediately
clear that we can solve the inner optimization problem.

x3 is not quasiconvex on R3

The key insight we will be using is that I(y; x) has a dif-
ferent beneﬁcial property: while not convex, I(y; x) as a
function of x is continuous submodular.
Lemma 3.1. Suppose we have n ≥ 1 differentiable func-
tions fi : R → R+, for i = 1, . . . , n, either all nonincreas-
ing or all nondecreasing. Then, f (x) = (cid:81)n
i=1 fi(xi) is a
continuous supermodular function from Rn to R+.

Proof. For n = 1, the resulting function is modular and
therefore supermodular. In the case n ≥ 2, we simply need
to compute derivatives. The mixed derivatives are

∂f
∂xi∂xj

= f (cid:48)

i (xi)f (cid:48)

j(xj) ·

fk(xk).

(10)

(cid:89)

k(cid:54)=i,j

i and f (cid:48)

By monotonicity, f (cid:48)
j have the same sign, so their
product is nonnegative, and since each fk is nonnegative,
the entire expression is nonnegative. Hence, f (x) is contin-
uous supermodular by Theorem 3.2 of (Topkis, 1978).

Corollary 3.1. The inﬂuence function I(y; x) deﬁned in
Section 2 is continuous submodular in x over the nonnega-
tive orthant, for each y ≥ 0.

Robust Budget Allocation via Continuous Submodular Functions

Proof. Since submodularity is preserved under summation,
it sufﬁces to show that each function It(y) is continuous
submodular. By Lemma 3.1, since fs(z) = zy(s) is non-
negative and monotone nondecreasing for y(s) ≥ 0, the
product (cid:81)
is continuous supermodular in x.
Flipping the sign and adding a constant term yields It(y),
which is hence continuous submodular.

(s,t)∈E xy(s)

st

Conjecture 3.1. Strong duality holds, i.e.

max
y∈Y

min
x∈X

I(y; x) = min
x∈X

max
y∈Y

I(y; x).

(11)

the

then

holds,

strong

duality

If
gap
maxy∈Y I(y; x∗) − minx∈X I(y∗; x) in Equation (9)
If I(y; x) were quasiconvex in x,
is zero at optimality.
strong duality would hold by Sion’s min-max theorem, but
this is not the case. In practice, we observe that the duality
gap always converges to zero.

duality

Bach (2015) demonstrates how to minimize a continu-
ous submodular function H(x) subject to box constraints
x ∈ Box(l, u), up to an arbitrary suboptimality gap ε > 0.
The constraint set X in our Robust Budget Allocation prob-
lem, however, has box constraints with an additional con-
straint R(x) ≤ B. This case is not addressed in any pre-
vious work. Fortunately, for a large class of functions R,
there is still an efﬁcient algorithm for continuous submod-
ular minimization, which we present in the next section.

3.1. Constrained Continuous Submodular Function

Minimization

We next address an algorithm for minimizing a monotone
continuous submodular function H(x) subject to box con-
straints x ∈ Box(l, u) and a constraint R(x) ≤ B:

minimize H(x)
s.t.

R(x) ≤ B
x ∈ Box(l, u).

(12)

(13)

If H and R were convex, the constrained problem would
be equivalent to solving, with the right Lagrange multipler
λ∗ ≥ 0:

minimize H(x) + λ∗R(x)
s.t.

x ∈ Box(l, u).

Although H and R are not necessarily convex here, it turns
out that a similar approach indeed applies. The main idea
of our approach bears similarity with (Nagano et al., 2011)
for the set function case, but our setting with continuous
functions and various uncertainty sets is more general, and
requires more argumentation. We outline our theoretical
results here, and defer further implementation details and
proofs to the appendix.

Following (Bach, 2015), we discretize the problem; for a
sufﬁciently ﬁne discretization, we will achieve arbitrary ac-
curacy. Let A be an interpolation mapping that maps the

discrete set (cid:81)n
i=1[ki] into Box(l, u) = (cid:81)n
i=1[li, ui] via the
componentwise interpolation functions Ai : [ki] → [li, ui].
We say Ai is δ-ﬁne if Ai(xi + 1) − Ai(xi) ≤ δ for all
xi ∈ {0, 1, . . . , ki − 2}, and we say the full interpolation
function A is δ-ﬁne if each Ai is δ-ﬁne.
This mapping yields functions H δ : (cid:81)n
i=1[ki] → R and
Rδ : (cid:81)n
i=1[ki] → R via H δ(x) = H(A(x)) and Rδ(x) =
R(A(x)). H δ is lattice submodular (on the integer lattice).
This construction leads to a reduction of Problem (12) to a
submodular minimization problem over the integer lattice:

minimize H δ(x) + λRδ(x)
x ∈ (cid:81)n
s.t.

i=1[ki].

(14)

Ideally, there should then exist a λ such that the associated
minimizer x(λ) yields a close to optimal solution for the
constrained problem. Theorem 3.1 below states that this is
indeed the case.

Moreover, a second beneﬁt of submodularity is that we can
ﬁnd the entire solution path for Problem (14) by solving a
single optimization problem.

Lemma 3.2. Suppose H is continuous submodular, and
suppose the regularizer R is strictly increasing and sepa-
rable: R(x) = (cid:80)n
i=1 Ri(xi). Then we can recover a min-
imizer x(λ) for the induced discrete Problem (14) for any
λ ∈ R by solving a single convex optimization problem.

The problem in question arises from a relaxation h↓ that
extends H δ in each coordinate i to a function on distribu-
tions over the domain [ki]. These distributions are repre-
sented via their inverse cumulative distribution functions
ρi, which take the coordinate xi as input, and output the
probability of exceeding xi. The function h↓ is an analogue
of the Lov´asz extension of set functions to continuous sub-
modular functions (Bach, 2015), it is convex and coincides
with H δ on lattice points.

Formally, this resulting single optimization problem is:

minimize h↓(ρ) + (cid:80)n
s.t.

ρ ∈ (cid:81)n

i=1

i=1
Rki−1
↓

(cid:80)ki−1

ji=1 aixi (ρi(xi))

(15)

↓ refers to the set of ordered vectors z ∈ Rk that
where Rk
satisfy z1 ≥ z2 ≥ · · · ≥ zk, the notation ρi(xi) denotes the
xi-th coordinate of the vector ρi, and the aixi are strictly
convex functions given by

aixi(t) =

t2 · [Rδ

i (xi) − Rδ

i (xi − 1)].

(16)

1
2

Problem (15) can be solved by Frank-Wolfe methods
(Frank & Wolfe, 1956; Dunn & Harshbarger, 1978;
Lacoste-Julien, 2016; Jaggi, 2013). This is because the
greedy algorithm for computing subgradients of the Lov´asz

Robust Budget Allocation via Continuous Submodular Functions

extension can be generalized, and yields a linear optimiza-
tion oracle for the dual of Problem (15). We detail the re-
lationship between Problems (14) and (15), as well as how
to implement the Frank-Wolfe methods, in Appendix C.

Let ρ∗ be the optimal solution for Problem (15). For any
λ, we obtain a rounded solution x(λ) for Problem (14) by
thresholding: we set x(λ)i = max{j | 1 ≤ j ≤ ki −
1, ρ∗
i (j) < λ for all j. Each x(λ(cid:48)) is
the optimal solution for Problem (14) with λ = λ(cid:48). We use
the largest parameterized solution x(λ) that is still feasible,
i.e. the solution x(λ∗) where λ∗ solves

i (j) ≥ λ}, or zero if ρ∗

min H δ(x(λ))
λ ≥ 0
s.t.
Rδ(x(λ)) ≤ B.

(17)

This λ∗ can be found efﬁciently via binary search or a lin-
ear scan.

Theorem 3.1. Let H be continuous submodular and mono-
tone decreasing, with (cid:96)∞-Lipschitz constant G, and let R
be strictly increasing and separable. Assume all entries
i (j) of the optimal solution ρ∗ of Problem (15) are dis-
ρ∗
tinct. Let x(cid:48) = A(x(λ∗)) be the thresholding correspond-
ing to the optimal solution λ∗ of Problem (17), mapped
back into the original continuous domain X . Then x(cid:48) is
feasible for the continuous Problem (12), and is a 2Gδ-
approximate solution:

H(x(cid:48)) ≤ 2Gδ +

min
x∈Box(l,u), R(x)≤B

H(x).

Theorem 3.1 implies an algorithm for solving Problem (12)
to ε-optimality: (1) set δ = ε/G, (2) compute ρ∗ which
solves Problem (15), (3) ﬁnd the optimal thresholding of
ρ∗ by determining the smallest λ∗ for which Rδ(x(λ∗)) ≤
B, and (4) map x(λ∗) back into continuous space via the
interpolation mapping A.

Optimality Bounds. Theorem 3.1 is proved by compar-
ing x(cid:48) and x∗ to the optimal solution on the discretized
mesh

x∗
d ∈

argmin

H δ(x).

x∈(cid:81)n

i=1[ki]:Rδ(x)≤B

Beyond the theoretical guarantee of Theorem 3.1, for any
problem instance and candidate solution x(cid:48), we can com-
pute a bound on the gap between H(x(cid:48)) and H δ(x∗
d). The
following two bounds are proved in the appendix:

Improvements. The requirement in Theorem 3.1 that the
elements of ρ∗ be distinct may seem somewhat restrictive,
but as long as ρ∗ has distinct elements in the neighborhood
of our particular λ∗, this bound still holds. We see in Sec-
tion 4.1.1 that in practice, ρ∗ almost always has distinct
elements in the regime we care about, and the bounds of
Remark 3.1 are very good.

If H is DR-submodular and R is afﬁne in each coordinate,
then Problem (14) can be represented more compactly via
the reduction of Ene & Nguyen (2016), and hence prob-
lem (12) can be solved more efﬁciently. In particular, the
inﬂuence function I(y; x) is DR-submodular in x when for
each s, y(s) = 0 or y(s) ≥ 1.

3.2. Application to Robust Budget Allocation

The above algorithm directly applies to Robust Alloca-
tion with the uncertainty sets in Section 2.2. The ellip-
soidal uncertainty set X Q corresponds to the constraint that
(cid:80)
(s,t)∈E Rst(xst) ≤ γ with Rst(x) = (xst − ˆxst)2σ−2
st ,
and x ∈ Box(0, 1). By the monotonicity of I(x, y), there
is never incentive to reduce any xst below ˆxst, so we can
replace Box(0, 1) with Box(ˆx, 1). On this interval, each
Rst is strictly increasing, and Theorem 3.1 applies.

For D-norm sets, we have Rst(xst) = (xst − ˆxst)/(ust −
ˆxst). Since each Rst is monotone, Theorem 3.1 applies.

Runtime and Alternatives. Since the core algorithm
is Frank-Wolfe, it is straightforward to show that Prob-
lem (15) can be solved to ε-suboptimality in time
O(ε−1n2δ−3α−1|T |2 log nδ−1), where α is the mini-
If ρ∗ has dis-
mum derivative of the functions Ri.
tinct elements separated by η,
then choosing ε =
η2αδ/8 results in an exact solution to (14) in time
O(η−2n2δ−4α−2|T |2 log nδ−1).

Noting that H δ + λRδ is submodular for all λ, one could
instead perform binary search over λ, each time converting
the objective into a submodular set function via Birkhoff’s
theorem and solving submodular minimization e.g. via one
of the recent fast methods (Chakrabarty et al., 2017; Lee
et al., 2015). However, we are not aware of a practical im-
plementation of the algorithm in (Lee et al., 2015). The
algorithm in (Chakrabarty et al., 2017) yields a solution in
expectation. This approach also requires care in the preci-
sion of the search over λ, whereas our approach searches
directly over the O(nδ−1) elements of ρ∗.

1. We can generate a discrete point x(λ+) satisfying

H(x(cid:48)) ≤ [H(x(cid:48)) − H δ(x(λ+))] + H δ(x∗

d).

4. Experiments

2. The Lagrangian yields the bound

H(x(cid:48)) ≤ λ∗(B − R(x(cid:48))) + H δ(x∗

d).

We evaluate our Robust Budget Allocation algorithm on
both synthetic test data and a real-world bidding dataset
from Yahoo! Webscope (yah) to demonstrate that our
method yields real improvements. For all experiments, we

Robust Budget Allocation via Continuous Submodular Functions

Figure 1. Visualization of the sorted values of ρ∗
i (j) (blue dots)
with comparison to the particular Lagrange multiplier λ∗ (orange
line). In most regimes there are no duplicate values, so that The-
orem 3.1 applies. The theorem only needs distinctness at λ∗.

used Algorithm 1 as the outer loop. For the inner sub-
modular minimization step, we implemented the pairwise
Frank-Wolfe algorithm of (Lacoste-Julien & Jaggi, 2015).
In all cases, the feasible set of budgets Y is {y ∈ RS
+ :
(cid:80)
s∈S y(s) ≤ C} where the speciﬁc budget C depends on
the experiment. Our code is available at git.io/vHXkO.

4.1. Synthetic

On the synthetic data, we probe two questions: (1) how of-
ten does the distinctness condition of Theorem 3.1 hold, so
that we are guaranteed an optimal solution; and (2) what is
the gain of using a robust versus non-robust solution in an
adversarial setting? For both settings, we set |S| = 6 and
|T | = 2 and discretize with δ = 0.001. We generated true
probabilties pst, created Beta posteriors, and built both El-
lipsoidal uncertainty sets X Q(γ) and D-norm sets X D(γ).

4.1.1. OPTIMALITY

Theorem 3.1 and Remark 3.1 demand that the values ρ∗
i (j)
be distinct at our chosen Lagrange multiplier λ∗ and, under
this condition, guarantee optimality. We illustrate this in
four examples: for Ellipsoidal or a D-norm uncertainty set,
and a total inﬂuence budget C ∈ {0.4, 4}. Figure 3 shows
all elements of ρ∗ in sorted order, as well as a horizontal
line indicating our Lagrange multiplier λ∗ which serves as
a threshold. Despite some plateaus, the entries ρ∗
i (j) are
distinct in most regimes, in particular around λ∗, the regime
that is needed for our results. Moreover, in practice (on the
Yahoo data) we observe later in Figure 3 that both solution-
dependent bounds from Remark 3.1 are very good, and all
solutions are optimal within a very small gap.

Figure 2. Comparison of worst-case expected inﬂuences for D-
norm uncertainty sets X D(γ) (left) and ellipsoidal uncertainty
sets X Q(γ) (right), for different total budget bounds C. For any
particular adversary budget γ, we compare minx∈X (γ) I(y; x)
for each candidate allocation y.

4.1.2. ROBUSTNESS AND QUALITY

Next, we probe the effect of a robust versus non-robust so-
lution for different uncertainty sets and budgets γ of the
adversary. We compare our robust solution with using
a point estimate for x, i.e., ynom ∈ argmaxy∈Y I(y; ˆx),
treating estimates as ground truth, and the stochastic solu-
E[I(y; X)] as per Section 2.1.
tion yexpect ∈ argmaxy∈Y
These two optimization problems were solved via standard
ﬁrst-order methods using TFOCS (Becker et al., 2011).

Figure 2 demonstrates that indeed, the alternative budgets
are sensitive to the adversary and the robustly-chosen bud-
get yrobust performs better, even in cases where the other
budgets achieve zero inﬂuence. When the total budget C is
large, yexpect performs nearly as well as yrobust, but when
resources are scarce (C is small) and the actual choice
seems to matter more, yrobust performs far better.

4.2. Yahoo! data

To evaluate our method on real-world data, we formulate
a Budget Allocation instance on advertiser bidding data
from Yahoo! Webscope (yah). This dataset logs bids on
1000 different phrases by advertising accounts. We map
the phrases to channels S and the accounts to customers T ,
with an edge between s and t if a corresponding bid was
made. For each pair (s, t), we draw the associated trans-
mission probability pst uniformly from [0, 0.4]. We bias
these towards zero because we expect people not to be eas-
ily inﬂuenced by advertising in the real world. We then
generate an estimate ˆpst and build up a posterior by gener-

05010000.0140.028D-norm, C = 0.405010000.0050.01Ellipsoidal, C = 0.405010000.0130.026D-norm, C = 405010000.0050.01Ellipsoidal, C = 405101500.10.20.3D-norm, C = 0.400040080000.10.20.3Ellipsoidal, C = 0.40005101500.511.5D-norm, C = 3.740040080000.511.5Ellipsoidal, C = 3.740Robust Budget Allocation via Continuous Submodular Functions

Figure 3. Convergence properties of our algorithm on real data.
In the ﬁrst plot, ‘p’ and ‘d’ refer to primal and dual values, with
dual gap shown on the second plot. The third plot demonstrates
that the problem-dependent suboptimality bounds of Remark 3.1
(x for x(λ+) and L for Lagrangian) are very small (good) for all
inner iterations of this run.

Figure 4. Convergence properties of Frank-Wolfe (FW), versus
the optimal value attained with our scheme (SFM).

ating nst samples from Ber(pst), where nst is the number
of bids between s and t in the dataset.

This transformation yields a bipartite graph with |S| =
1000, |T | = 10475, and more than 50,000 edges that we
use for Budget Allocation. In our experiments, the typical
gap between the naive ynom and robust yrobust was 100-
500 expected inﬂuenced people. We plot convergence of
the outer loop in Figure 3, where we observe fast conver-
gence of both primal inﬂuence value and the dual bound.

4.3. Comparison to ﬁrst-order methods

Given the success of ﬁrst-order methods on nonconvex
problems in practice, it is natural to compare these to our
method for ﬁnding the worst-case vector x. On one of our
Yahoo problem instances with D-norm uncertainty set, we
compared our submodular minimization scheme to Frank-
Wolfe with ﬁxed stepsize as in (Lacoste-Julien, 2016), im-
plementing the linear oracle using MOSEK (MOSEK ApS,
2015).
Interestingly, from various initializations, Frank-
Wolfe ﬁnds an optimal solution, as veriﬁed by comparing
to the guaranteed solution of our algorithm. Note that, due
to non-convexity, there are no formal guarantees for Frank-
Wolfe to be optimal here, motivating the question of global
convergence properties of Frank-Wolfe in the presence of
submodularity.

It is important to note that there are many cases where ﬁrst-
order methods are inefﬁcient or do not apply to our setup.
These methods require either a projection oracle (PO) onto
or linear optimization oracle (LO) over the feasible set X
deﬁned by (cid:96), u and R(x). The D-norm set admits a LO via
linear programming, but we are not aware of any efﬁcient
LO for Ellipsoidal uncertainty, nor PO for either set, that
does not require quadratic programming. Even more, our
algorithm applies for nonconvex functions R(x) which in-
duce nonconvex feasible sets X . Such nonconvex sets may
not even admit a unique projection, while our algorithm
achieves provable solutions.

5. Conclusion

We address the issue of uncertain parameters (or, model
misspeciﬁcation) in Budget Allocation or Bipartite Inﬂu-
ence Maximization (Alon et al., 2012) from a robust op-
timization perspective. The resulting Robust Budget Allo-
cation is a nonconvex-concave saddle point problem. Al-
though the inner optimization problem is nonconvex, we
show how continuous submodularity can be leveraged to
solve the problem to arbitrary accuracy ε, as can be veri-
ﬁed with the proposed bounds on the duality gap. In par-
ticular, our approach extends continuous submodular mini-
mization methods (Bach, 2015) to more general constraint
sets, introducing a mechanism to solve a new class of con-
strained nonconvex optimization problems. We conﬁrm on
synthetic and real data that our method ﬁnds high-quality
solutions that are robust to parameters varying arbitrarily in
an uncertainty set, and scales up to graphs with over 50,000
edges.

There are many compelling directions for further study.
The uncertainty sets we use are standard in the robust opti-
mization literature, but have not been applied to e.g. Robust
Inﬂuence Maximization; it would be interesting to general-
ize our ideas to general graphs. Finally, despite the inher-
ent nonconvexity of our problem, ﬁrst-order methods are
often able to ﬁnd a globally optimal solution. Explaining
this phenomenon requires further study of the geometry of
constrained monotone submodular minimization.

Acknowledgements

We thank the anonymous reviewers for their helpful sug-
gestions. We also thank MIT Supercloud and the Lincoln
Laboratory Supercomputing Center for providing compu-
tational resources. This research was conducted with Gov-
ernment support under and awarded by DoD, Air Force Of-
ﬁce of Scientiﬁc Research, National Defense Science and
Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a,
and also supported by NSF CAREER award 1553284.

0408075008500950010500Robust Budget Allocation via Continuous Submodular Functions

References

Yahoo! Webscope dataset ydata-ysm-advertiser-bids-
URL http://research.yahoo.com/

v1 0.
Academic_Relations.

Adamczyk, Marek, Sviridenko, Maxim, and Ward, Justin.
Submodular Stochastic Probing on Matroids. Mathemat-
ics of Operations Research, 41(3):1022–1038, 2016.
Alon, Noga, Gamzu, Iftah, and Tennenholtz, Moshe. Op-
timizing Budget Allocation Among Channels and Inﬂu-
encers. In WWW. 2012.

Atamt¨urk, Alper and Narayanan, Vishnu. Polymatroids and
mean-risk minimization in discrete optimization. Oper-
ations Research Letters, 36(5):618–622, 2008.

Bach, Francis. Submodular Functions: From Discrete to

Continous Domains. arXiv:1511.00394, 2015.

Balkanski, Eric, Rubinstein, Aviad, and Singer, Yaron. The
power of optimization from samples. In NIPS, 2016.
Balkanski, Eric, Rubinstein, Aviad, and Singer, Yaron. The
In STOC,

limitations of optimization from samples.
2017.

Becker, Stephen R., Cand`es, Emmanuel J., and Grant,
Michael C. Templates for convex cone problems with
applications to sparse signal recovery. Mathematical
programming computation, 3(3):165–218, 2011.

Ben-Tal, Aharon and Nemirovski, Arkadi. Robust so-
lutions of Linear Programming problems contaminated
with uncertain data. Mathematical Programming, 88(3):
411–424, 2000.

Ben-Tal, Aharon, El Ghaoui, Laurent, and Nemirovski,
Princeton University

Arkadi. Robust Optimization.
Press, 2009.

Bertsimas, Dimitris and Sim, Melvyn. Robust discrete op-
timization and network ﬂows. Mathematical program-
ming, 98(1):49–71, 2003.

Bertsimas, Dimitris, Brown, David B., and Caramanis,
Constantine. Theory and Applications of Robust Opti-
mization. SIAM Review, 53(3):464–501, 2011.

Best, Michael J. and Chakravarti, Nilotpal. Active set al-
gorithms for isotonic regression; A unifying framework.
Mathematical Programming, 47(1-3):425–439, 1990.
Bian, Andrew An, Mirzasoleiman, Baharan, Buhmann,
Joachim M., and Krause, Andreas. Guaranteed Non-
convex Optimization: Submodular Maximization over
Continuous Domains. In AISTATS, 2017.

Birkhoff, Garrett. Rings of sets. Duke Mathematical Jour-

nal, 3(3):443–454, 1937.

Borgs, Christian, Brautbar, Michael, Chayes, Jennifer, and
Lucier, Brendan. Maximizing Social Inﬂuence in Nearly
Optimal Time. In SODA, 2014.

Boyd, Stephen, Kim, Seung-Jean, Vandenberghe, Lieven,
and Hassibi, Arash. A tutorial on geometric pro-
gramming. Optimization and engineering, 8(1):67–127,
2007.

Chakrabarty, Deeparnab, Lee, Yin Tat, Sidford, Aaron, and
Wong, Sam Chiu-wai. Subquadratic submodular func-
tion minimization. In STOC, 2017.

Chandrasekaran, Venkat and Shah, Parikshit. Relative En-
tropy Relaxations for Signomial Optimization. SIAM
Journal on Optimization, 26(2):1147–1173, 2016.

Chen, Wei, Wang, Yajun, and Yang, Siyu. Efﬁcient inﬂu-
ence maximization in social networks. In KDD, 2009.
Chen, Wei, Wang, Chi, and Wang, Yajun. Scalable In-
ﬂuence Maximization for Prevalent Viral Marketing in
Large-scale Social Networks. In KDD, 2010.

Chen, Wei, Lin, Tian, Tan, Zihan, Zhao, Mingfei, and
Zhou, Xuren. Robust inﬂuence maximization. In KDD.
2016.

Chiang, Mung. Geometric Programming for Communica-
tion Systems. Commun. Inf. Theory, 2(1/2):1–154, 2005.
Deshpande, Amol, Hellerstein, Lisa, and Kletenik, Devo-
rah. Approximation Algorithms for Stochastic Submod-
ular Set Cover with Applications to Boolean Function
Evaluation and Min-Knapsack. ACM Trans. Algorithms,
12(3):42:1–42:28, 2016.

Domingos, Pedro and Richardson, Matt. Mining the net-

work value of customers. In KDD, 2001.

Du, Nan, Song, Le, Gomez Rodriguez, Manuel, and Zha,
Hongyuan. Scalable inﬂuence estimation in continuous-
time diffusion networks. In NIPS. 2013.

Du, Nan, Liang, Yingyu, Balcan, Maria-Florina, and Song,
Le. Inﬂuence function learning in information diffusion
networks. In ICML, 2014.

Dunn, Joseph C. and Harshbarger, S. Conditional gradi-
ent algorithms with open loop step size rules. Journal
of Mathematical Analysis and Applications, 62(2):432–
444, 1978.

Ecker, Joseph. Geometric Programming: Methods, Com-
putations and Applications. SIAM Review, 22(3):338–
362, 1980.

Ene, Alina and Nguyen, Huy L. A Reduction for Opti-
mizing Lattice Submodular Functions with Diminishing
Returns. arXiv:1606.08362, 2016.
Frank, Marguerite and Wolfe, Philip.

An algorithm
for quadratic programming. Naval Research Logistics
Quarterly, 3(1-2):95–110, 1956.

Goel, Gagan, Karande, Chinmay, Tripathi, Pushkar, and
Wang, Lei. Approximability of combinatorial problems
with multi-agent submodular cost functions. In FOCS,
2009.

Goemans, Michel and Vondr´ak, Jan. Stochastic Covering
and Adaptivity. In LATIN 2006: Theoretical Informatics.
Springer Berlin Heidelberg, 2006.

Golovin, Daniel and Krause, Andreas. Adaptive Submod-
ularity: Theory and Applications in Active Learning and
Journal of Artiﬁcial Intelli-
Stochastic Optimization.
gence, 42:427–486, 2011.

Robust Budget Allocation via Continuous Submodular Functions

Gomez Rodriguez, Manuel and Sch¨olkopf, Bernhard. In-
ﬂuence maximization in continuous time diffusion net-
works. In ICML, 2012.

Gomez Rodriguez, Manuel, Leskovec, Jure, and Krause,
Andreas. Inferring networks of diffusion and inﬂuence.
In KDD, 2010.

Gottschalk, Corinna and Peis, Britta. Submodular func-
tion maximization on the bounded integer lattice. In Ap-
proximation and Online Algorithms: 13th International
Workshop (WAOA), 2015.

Hassidim, Avinatan and Singer, Yaron. Submodular opti-
mization under noise. arXiv preprint arXiv:1601.03095,
2016.

Hatano, Daisuke, Fukunaga, Takuro, Maehara, Takanori,
and Kawarabayashi, Ken-ichi. Lagrangian Decomposi-
tion Algorithm for Allocating Marketing Channels.
In
AAAI, 2015.

He, Xinran and Kempe, David. Robust inﬂuence maxi-

mization. In KDD. 2016.

Iwata, Satoru and Nagano, Kiyohito. Submodular func-
tion minimization under covering constraints. In FOCS,
2009.

Jaggi, Martin. Revisiting Frank-Wolfe: Projection-Free

Sparse Convex Optimization. In ICML, 2013.

Kempe, David, Kleinberg, Jon, and Tardos, ´Eva. Maximiz-
ing the Spread of Inﬂuence Through a Social Network.
In KDD, 2003.

Khachaturov, Vladimir R., Khachaturov, Roman V., and
Khachaturov, Ruben V. Supermodular programming on
ﬁnite lattices. Computational Mathematics and Mathe-
matical Physics, 52(6):855–878, 2012.

Kolmogorov, Vladimir and Shioura, Akiyoshi. New algo-
rithms for convex cost tension problem with application
to computer vision. Discrete Optimization, 6:378–393,
2009.

Krause, Andreas, McMahan, H Brendan, Guestrin, Carlos,
and Gupta, Anupam. Robust submodular observation se-
lection. Journal of Machine Learning Research, 9(Dec):
2761–2801, 2008.

Lacoste-Julien, Simon. Convergence Rate of Frank-Wolfe
for Non-Convex Objectives. arXiv:1607.00345, 2016.
Lacoste-Julien, Simon and Jaggi, Martin. On the global lin-
ear convergence of Frank-Wolfe optimization variants.
In NIPS, 2015.

Lee, Yin Tat, Sidford, Aaron, and Wong, Sam Chiu-wai.
A faster cutting plane method and its implications for
combinatorial and convex optimization. In FOCS, 2015.
Lowalekar, Meghna, Varakantham, Pradeep, and Kumar,
Akshat. Robust Inﬂuence Maximization: (Extended Ab-
stract). In AAMAS, 2016.

Maehara, Takanori. Risk averse submodular utility maxi-
mization. Operations Research Letters, 43(5):526–529,
2015.

Maehara, Takanori, Yabe, Akihiro, and Kawarabayashi,
Ken-ichi. Budget Allocation Problem with Multiple Ad-
vertisers: A Game Theoretic View. In ICML, 2015.
MOSEK ApS. MOSEK MATLAB Toolbox 8.0.0.57,
URL http://docs.mosek.com/8.0/

2015.
toolbox/index.html.

Murota, Kazuo. Discrete convex analysis. SIAM, 2003.
Murota, Kazuo and Shioura, Akiyoshi. Exact bounds for
steepest descent algorithms of l-convex function min-
imization. Operations Research Letters, 42:361–366,
2014.

Nagano, Kiyohito, Kawahara, Yoshinobu, and Aihara,
Kazuyuki. Size-constrained submodular minimization
through minimum norm base. In ICML, 2011.

Narasimhan, Harikrishna, Parkes, David C, and Singer,
Yaron. Learnability of inﬂuence in networks. In NIPS.
2015.

Netrapalli, Praneeth and Sanghavi, Sujay. Learning the
graph of epidemic cascades. In SIGMETRICS. 2012.
Nikolova, Evdokia. Approximation algorithms for reli-
able stochastic combinatorial optimization. In APPROX.
2010.

Orlin, James B., Schulz, Andreas, and Udwani, Rajan. Ro-
bust monotone submodular function maximization.
In
IPCO, 2016.

Pascual, Luis D. and Ben-Israel, Adi. Constrained max-
imization of posynomials by geometric programming.
Journal of Optimization Theory and Applications, 5(2):
73–80, 1970.

Polyak, Boris T. Introduction to Optimization. Number 04;

QA402. 5, P6. 1987.

Rockafellar, R Tyrrell and Uryasev, Stanislav. Optimiza-
tion of conditional value-at-risk. Journal of risk, 2:21–
42, 2000.

Rockafellar, R Tyrrell and Uryasev, Stanislav. Conditional
value-at-risk for general loss distributions. Journal of
banking & ﬁnance, 26(7):1443–1471, 2002.

Soma, Tasuku and Yoshida, Yuichi. A Generalization of
Submodular Cover via the Diminishing Return Property
on the Integer Lattice. In NIPS, 2015.

Soma, Tasuku, Kakimura, Naonori, Inaba, Kazuhiro, and
Kawarabayashi, Ken-ichi. Optimal Budget Alloca-
tion: Theoretical Guarantee and Efﬁcient Algorithm. In
ICML, 2014.

Svitkina, Zoya and Fleischer, Lisa. Submodular approxi-
mation: Sampling-based algorithms and lower bounds.
SIAM Journal on Computing, 40(6):1715–1737, 2011.
Topkis, Donald M. Minimizing a submodular function on
a lattice. Operations research, 26(2):305–321, 1978.

Wainwright, Kevin and Chiang, Alpha.

Fundamental
Methods of Mathematical Economics. McGraw-Hill Ed-
ucation, 2004.

Zhang, Peng, Chen, Wei, Sun, Xiaoming, Wang, Yajun,
and Zhang, Jialin. Minimizing seed set selection with

Robust Budget Allocation via Continuous Submodular Functions

probabilistic coverage guarantee in a social network. In
KDD, 2014.

(18)

(19)

(20)

(21)

Robust Budget Allocation via Continuous Submodular Functions

A. Worst-Case Approximation Ratio versus True Worst-Case

Consider the function f (x; θ) deﬁned on {0, 1} × {0, 1}, with values given by:

f (x; 0) =

f (x; 1) =

(cid:40)

1
x = 0
0.6 x = 1,

(cid:40)

1 x = 0
2 x = 1.

We wish to choose x to maximize f (x; θ) robustly with respect to adversarial choices of θ. If θ were ﬁxed, we could
directly choose x∗
1 = 1. Of course, we want to deal with worst-case θ.
One option is to maximize the worst-case approximation ratio:

θ to maximize f (x; θ). In particular, x∗

0 = 0 and x∗

max
x

min
θ

f (x; θ)
f (x∗
θ; θ)

.

max
x

min
θ

f (x; θ).

One can verify that the best x according to this criterion is x = 1, with worst-case approximation ratio 0.6 and worst-case
function value 0.6. In this paper, we optimize the worst-case of the actual function value:

This criterion will select x = 0, which has a worse worst-case approximation ratio of 0.5, but actually guarantees a function
value of 1, signiﬁcantly better than the 0.6 achieved by the other formulation of robustness.

B. DR-submodularity

B.1. Proof of Proposition 1.1

Proof. The function f can be reduced to a submodular set function g : 2V → R via (Ene & Nguyen, 2016), where
|V | = O(n log k). The function g can be evaluated via mapping from 2V to the domain of f , and then evaluating f , in
time O(n log k · EO). We can directly substitute these complexities into the runtime bound from (Lee et al., 2015).

B.2. DR-submodularity and L(cid:92)-convexity

A function is L(cid:92)-convex if it satisﬁes a discrete version of midpoint convexity, i.e. for all x, y it holds that

f (x) + f (y) ≥ f

(cid:18)(cid:24) x + y

(cid:25)(cid:19)

(cid:18)(cid:22) x + y

(cid:23)(cid:19)

2

+ f

.

2

Remark B.1. An L(cid:92)-convex function need not be DR-submodular, and vice-versa. Hence algorithms for optimizing one
type may not apply for the other.

Proof. Consider f1(x1, x2) = −x2
2, both deﬁned on {0, 1, 2} × {0, 1, 2}. The function
f1 is DR-submodular but violates discrete midpoint convexity for the pair of points (0, 0) and (2, 2), while f2 is L(cid:92)-convex
but does not have diminishing returns in either dimension.

1 − 2x1x2 and f2(x1, x2) = x2

1 + x2

Intuitively-speaking, L(cid:92)-convex functions look like discretizations of convex functions. The continuous objective function
I(x, y) we consider need not be convex, hence its discretization need not be L(cid:92)-convex, and we cannot use those tools.
However, in some regimes (namely if each y(s) ∈ {0} ∪ [1, ∞)), it happens that I(x, y) is DR-submodular in x.

C. Constrained Continuous Submodular Function Minimization

Deﬁne Rn
↓ to be the set of vectors ρ in Rn which are monotone nonincreasing, i.e. ρ(1) ≥ ρ(2) ≥ · · · ≥ ρ(n). As in the
main text, deﬁne [k] = {0, 1, . . . , k − 1}. One of the key results from (Bach, 2015) is that an arbitrary submodular function
H(x) deﬁned on (cid:81)n

i=1[ki] can be extended to a particular convex function h↓(ρ) so that

minimize H(x)
s.t.

x ∈ (cid:81)n

i=1[ki]

⇔

minimize h↓(ρ)
s.t.

ρ ∈ (cid:81)n

Rki−1
↓

.

i=1

(22)

Robust Budget Allocation via Continuous Submodular Functions

Moreover, Theorem 4 from (Bach, 2015) states that, if aiyi are strictly convex functions for all i = 1, . . . , n and each
yi ∈ [ki], then the two problems

and

minimize H(x) + (cid:80)n
s.t.

x ∈ (cid:81)n

i=1
i=1[ki].

(cid:80)xi

yi=1 a(cid:48)

iyi

(λ)

minimize h↓(ρ) + (cid:80)n
s.t.

ρ ∈ (cid:81)n

i=1

i=1
Rki−1
↓

(cid:80)ki−1

xi=1 aixi[ρi(xi)]

are equivalent. In particular, one recovers a solution to Problem (23) for any λ just as alluded to in Lemma 3.2: ﬁnd ρ∗
which solves Problem (24) and, for each component i, choose xi to be the maximal value for which ρ∗

i (xi) ≥ λ.

C.1. Proof of Lemma 3.2
Proof. The discretized form of the regularizer Rδ is also separable and can be written Rδ(x) = (cid:80)n
i=1 Rδ
i = 1, . . . , n and each yi ∈ [ki] with yi ≥ 1, deﬁne aiyi(t) = 1
(t) = t · [Rδ
iyi
Rδ
each aiyi(t) is strictly convex. Then,

i (x). For each
i (yi) −
i (yi − 1)]. Since we assumed R(x) is strictly increasing, the coefﬁcient of t2 in each aiyi(t) is strictly positive, so that

i (yi − 1)], so that a(cid:48)

i (yi) − Rδ

2 t2 · [Rδ

so that the discretized version of the minimization problem can be written as

Since the term Rδ(0) does not depend on the variable x, this minimization is equivalent to

λRδ

i (xi) = λ ·

i (0) +

(cid:0)Rδ

i (yi) − Rδ

i (yi − 1)(cid:1)

(cid:34)
Rδ

(cid:35)

xi(cid:88)

yi=1

xi(cid:88)

yi=1

= λRδ

i (0) +

a(cid:48)
iyi

(λ),

minimize H δ(x) + λRδ(0) + (cid:80)n
s.t.

x ∈ (cid:81)n

i=1[ki].

i=1

(cid:80)xi

yi=1 a(cid:48)

iyi

(λ)

minimize H δ(x) + (cid:80)n
i=1
i=1[ki].
s.t.

x ∈ (cid:81)n

(cid:80)xi

yi=1 a(cid:48)

iyi

(λ)

(23)

(24)

(25)

(26)

(27)

(28)

This problem is in the precise form where we can apply the preceding equivalence result between Problems (23) and (24),
so we are done.

C.2. Proof of Theorem 3.1

Proof. The general idea of this proof is to ﬁrst show that the integer-valued point x∗

d which solves

x∗
d ∈

argmin

H δ(x)

x∈(cid:81)n

i=1[ki]:Rδ(x)≤B

is also nearly a minimizer of the continuous version of the problem, due to the ﬁneness of the discretization. Then, we
show that the solutions traced out by x(λ) get very close to x∗
d. These two results are simply combined via the triangle
inequality.

C.2.1. CONTINUOUS AND DISCRETE PROBLEMS

We begin by proving that

H δ(x∗

d) ≤ Gδ +

min
x∈X :R(x)≤B

H(x).

(29)

Consider x∗ ∈ arg minx∈X :R(x)≤B H(x). If x∗ corresponds to an integral point in the discretized domain, then H(x∗) =
d) and we are done. Else, x∗ has at least one non-integral coordinate. By rounding coordinatewise, we can construct
H δ(x∗
a set X = {x1, . . . , xm} ⊆ (cid:81)n
i=1[ki] so that x∗ ∈ conv({A(x1), . . . , A(xm)}. By monotonicity, there must be some xi ∈

Robust Budget Allocation via Continuous Submodular Functions

X with Rδ(xi) ≤ B, i.e. A(xi) is feasible for the original continuous problem. By construction, since the discretization
given by A is δ-ﬁne, we must have (cid:107)x∗ − A(xi)(cid:107)∞ ≤ δ. Applying the Lipschitz property of H and the optimality of x∗,
we have

Gδ ≥ H(A(xi)) − H(x∗) = H δ(xi) − H(x∗) ≥ H δ(x∗

d) − H(x∗),

from which (29) follows.

Deﬁne λ− and λ+ by

C.2.2. DISCRETE AND PARAMETERIZED DISCRETE PROBLEMS

The next step in proving our suboptimality bound is to prove that

from which it will follow that

λ− ∈

argmin
λ≥0:Rδ(x(λ))≤B

λ+ ∈

argmax
λ≥0:Rδ(x(λ))≥B

H δ(x(λ))

and

H δ(x(λ)).

H δ(x(λ+)) ≤ H δ(x∗

d) ≤ H δ(x(λ−)),

H δ(x(λ−)) ≤ Gδ + H δ(x∗

d).

We begin by stating the min-max inequality, i.e. weak duality:

min

x∈(cid:81)n

i=1[ki]:Rδ(x)≤B

H δ(x) = min

x∈(cid:81)n

i=1[ki]

max
λ≥0

(cid:8)H δ(x) + λ(Rδ(x) − B)(cid:9)

≥ max
λ≥0

= max
λ≥0

min

(cid:8)H δ(x) + λ(Rδ(x) − B)(cid:9)

i=1[ki]

x∈(cid:81)n
(cid:8)H δ(x(λ)) + λ(Rδ(x(λ)) − B)(cid:9)

(cid:8)H δ(x(λ)) + λ(Rδ(x(λ)) − B)(cid:9)

≥

max
λ≥0:Rδ(x(λ))≥B

≥

max
λ≥0:Rδ(x(λ))≥B

H δ(x(λ))

= H δ(x(λ+)).

We can also bound the optimal value of H δ(x∗

d) from the other side:

H δ(x∗

d) =

min

x∈(cid:81)n

i=1[ki]:Rδ(x)≤B

H δ(x) ≤

min
λ≥0:Rδ(x(λ))≤B

H δ(x) = H δ(x(λ−))

because the set of x(λ) parameterized by λ is a subset of the full set {x ∈ (cid:81)n
We have now bounded the optimal value of H δ(x∗
λ ≥ 0 for the parameterization x(λ):

i=1[ki] : Rδ(x) ≤ B}.
d) on either side by optimization problems where we seek an optimal

H δ(x(λ+)) ≤ H δ(x∗

d) ≤ H δ(x(λ−)).

Recall that x(λ) comes from thresholding the values of ρ∗ by λ, and that we assume that the elements of ρ∗ are unique.
Hence, as we increase λ, the components of x decrease by 1 each time. Combining this with the strict monotonicity of R,
we see that (cid:107)x(λ+) − x(λ−)(cid:107)∞ ≤ 1. By the Lipschitz properties of H δ, it follows that (cid:12)
(cid:12) ≤ Gδ.
d) lies in the interval between H δ(x(λ+)) and H δ(x(λ−)), it follows that (cid:12)
Since H δ(x∗

(cid:12)H δ(x(λ+)) − H δ(x(λ−))(cid:12)
d) − H δ(x(λ−))(cid:12)
(cid:12)H δ(x∗

(cid:12) ≤ Gδ.

C.3. Proof of Remark 3.1

Deﬁne λ∗ = λ− as in the previous section, so that x(cid:48) = A(x(λ∗). The x(λ+) bound is a simple consequence from the
above result that

H δ(x(λ+)) ≤ H δ(x∗

d) ≤ H δ(x(λ−)) = H(x(cid:48)).

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

Robust Budget Allocation via Continuous Submodular Functions

As for the Lagrangian bound, since x(λ∗) is a minimizer for the regularized function H δ(x) + λ∗(Rδ(x) − B), it follows
that

H δ(x(λ∗)) + λ∗(Rδ(x(λ∗)) − B) ≤ H δ(x∗

d) + λ∗(Rδ(x∗

d) − B).

Rearranging, and observing that Rδ(x∗

d) ≤ B because x∗

d is feasible, it holds that

H(x(cid:48)) = H δ(x(λ∗)) ≤ H δ(x∗

d) + λ∗(Rδ(x∗

d) − Rδ(x(λ∗))) ≤ H δ(x∗

d) + λ∗(B − R(x(cid:48))).

(39)

(40)

One can also combine either of these bounds with the result from the proof of Theorem 3.1 that H δ(x∗
yielding e.g.

d) ≤ Gδ + H(x∗)

H(x(cid:48)) ≤ Gδ + λ∗(B − R(x(cid:48))) + H δ(x∗

d).

C.4. Solving the Optimization Problem

Now that we have proven equivalence results between the constrained problem we want to solve and the convex prob-
lem (24), we need to actually solve the convex problem. At the beginning of Section 5.2 in (Bach, 2015), it is stated that
this surrogate problem can optimized via the Frank-Wolfe method and its variants, but only the the version of Problem (24)
without the extra functions aixi is elaborated upon. Here we detail how Frank-Wolfe algorithms can be used to solve the
more general parametric regularized problem. Our aim is to spell out very clearly the applicability of Frank-Wolfe to this
problem, for the ease of future practitioners.

Bach (2015) notes that by duality, Problem (24) is equivalent to:

min

h↓(ρ) − H(0) +

aixi[ρi(xi)] =

ρ∈(cid:81)n

i=1

Rki−1
↓

n
(cid:88)

ki−1
(cid:88)

i=1

xi=1

min

ρ∈(cid:81)n

i=1

Rki−1
↓
(cid:40)

max
w∈B(H)

(cid:104)ρ, w(cid:105) +

aixi[ρi(xi)]

n
(cid:88)

ki−1
(cid:88)

i=1

xi=1

n
(cid:88)

ki−1
(cid:88)

i=1

xi=1

(cid:41)

aixi [ρi(xi)]

= max

w∈B(H)

min

(cid:104)ρ, w(cid:105) +

ρ∈(cid:81)n

i=1

Rki−1
↓

:= max

f (w).

w∈B(H)

Here, the base polytope B(H) happens to be the convex hull of all vectors w which could be output by the greedy algorithm
in (Bach, 2015).

It is the dual problem, where we maximize over w, which is amenable to Frank-Wolfe. For Frank-Wolfe methods, we need
two oracles: an oracle which, given w, returns ∇f (w); and an oracle which, given ∇f (w), produces a point s which solves
the linear optimization problem maxs∈B(H)(cid:104)s, ∇f (w)(cid:105).

Per Bach (2015), an optimizer of the linear problem can be computed directly from the greedy algorithm. For the gradient
oracle, recall that we can ﬁnd a subgradient of g(x) = miny h(x, y) at the point x0 by ﬁnding y(x0) which is optimal for
the inner problem, and then computing ∇xh(x, y(x0)). Moreover, if such y(x0) is the unique optimizer, then the resulting
vector is indeed the gradient of g(x) at x0. Hence, in our case, it sufﬁces to ﬁrst ﬁnd ρ(w) which solves the inner problem,
and then ∇f (w) is simply ρ(w) because the inner function is linear in w. Since each function aixi is strictly convex, the
minimizer ρ(w) is unique, conﬁrming that we indeed get a gradient of f , and that f is differentiable.

Of course, we still need to compute the minimizer ρ(w). For a given w, we want to solve

There are no constraints coupling the vectors ρi, and the objective is similarly separable, so we can independently solve n
problems of the form

min

(cid:104)ρ, w(cid:105) +

ρ∈(cid:81)n

i=1

Rki−1
↓

n
(cid:88)

ki−1
(cid:88)

i=1

xi=1

aixi[ρi(xi)]

min
ρ∈Rk−1
↓

(cid:104)ρ, w(cid:105) +

aj(ρj).

k−1
(cid:88)

j=1

Robust Budget Allocation via Continuous Submodular Functions

Recall that each function aiyi(t) takes the form 1
with diagonal entries rj. Our problem can then be written as

2 t2riyi for some riyi > 0. Let D = diag(r), the (k − 1) × (k − 1) matrix

(cid:104)ρ, w(cid:105) +

min
ρ∈Rk−1
↓

1
2

k−1
(cid:88)

j=1

rjρ2

j = min
ρ∈Rk−1
↓

(cid:104)ρ, w(cid:105) +

(cid:104)Dρ, ρ(cid:105)

1
2

= min
ρ∈Rk−1
↓

(cid:104)D1/2ρ, D−1/2w(cid:105) +

(cid:104)D1/2ρ, D1/2ρ(cid:105).

1
2

Completing the square, the above problem is equivalent to

min
ρ∈Rk−1
↓

(cid:107)D1/2ρ + D−1/2w(cid:107)2

2 = min
ρ∈Rk−1
↓

(r1/2

j ρj + r−1/2

j

wj)2

k−1
(cid:88)

j=1

k−1
(cid:88)

j=1

= min
ρ∈Rk−1
↓

rj(ρj + r−1

j wj)2.

This last expression is precisely the problem which is called weighted isotonic regression: we are ﬁtting ρ to diag(r−1)w,
with weights r, subject to a monotonicity constraint. Weighted isotonic regression is solved efﬁciently via the Pool Adja-
cent Violators algorithm of (Best & Chakravarti, 1990).

C.5. Runtime

Frank-Wolfe returns an ε-suboptimal solution in O(ε−1D2L) iterations, where D is the diameter of the feasible region, and
L is the Lipschitz constant for the gradient of the objective (Jaggi, 2013). Our optimization problem is maxw∈B(H) f (w)
as deﬁned in the previous section. Each w ∈ B(H) has O(nδ−1) coordinates of the form H δ(x + ei) − H δ(x). Since H δ
is an expected inﬂuence in the range [0, T ], we can bound the magnitude of each coordinate of w by T and hence D2 by
O(nδ−1T 2). If α is the minimum derivative of the functions Ri, then the smallest coefﬁcient of the functions aixi(t) is
bounded below by αδ. Hence the objective is the conjugate of an αδ-strongly convex function, and therefore has α−1δ−1-
Lipschitz gradient. Combining these, we arrive at the O(ε−1nδ−2α−1T 2) iteration bound. The most expensive step in
each iteration is computing the subgradient, which requires sorting the O(nδ−1) elements of ρ in time O(nδ−1 log nδ−1).
Hence the total runtime of Frank-Wolfe is O(ε−1n2δ−3α−1T 2 log nδ−1).

As speciﬁed in the main text, relating an approximate solution of (15) to a solution of (14) is nontrivial. Assume ρ∗ has
distinct elements separated by η, and chose ε to be less than η2αδ/8. If ρ is ε-suboptimal, then by αδ-strong convexity
we must have (cid:107)ρ − ρ∗(cid:107)2 < η/2, and therefore (cid:107)ρ − ρ∗(cid:107)∞ < η/2. Since the smallest consecutive gap between elements
of ρ∗ is η, this implies that ρ and ρ∗ have the same ordering, and therefore admit the same solution x after thresholding.
Accounting for this choice in ε, we have an exact solution to (14) in total runtime of O(η−2n2δ−4α−2T 2 log nδ−1).

D. Expectation and Variance of the Inﬂuence Function

We wish to study the inﬂuence I(X, y), its expectation and its variance as a function of y. By deﬁnition, the inﬂuence
function is given by

I(X, y) =



1 −

(cid:88)

t∈T



(cid:89)

X y(s)
st

 .

(s,t)∈E

Before we prove the stated results, we will simplify the functions involved.

Maximizing I(X, y) is equivalent to minimizing the function

(cid:88)

(cid:89)

X y(s)
st

t∈T

(s,t)∈E

(41)

(42)

and vice-versa. The particular properties we are interested in, namely convexity and submodularity, are preserved under
sums. Moreover, expectation is linear and variances add, so for our purposes we can focus on only one term of the above

Robust Budget Allocation via Continuous Submodular Functions

sum. After reindexing in terms of i = 1, . . . , n instead of (s, t) ∈ E, we are left studying functions of the form

If f (y) is always convex (or supermodular), then I(X, y) is always concave (submodular) in y, and similarly for their
expectations and variances.

Expectation By independence,

Suppose that each Xi ∼ Beta(αi, βi), so that

f (y) =

n
(cid:89)

i=1

X yi
i

.

E[f (y)] =

E[X yi
i

].

n
(cid:89)

i=1

E[X yi
i

] =

Γ(αi + βi)Γ(αi + yi)
Γ(αi + βi + yi)

=

=

Γ(αi + βi)Γ(αi + yi)Γ(βi)
Γ(αi + βi + yi)Γ(βi)

B(αi + yi, βi)
B(αi, βi)

.

Then,

E[f (y)] =

n
(cid:89)

i=1

B(αi + yi, βi)
B(αi, βi)

n
(cid:89)

i=1

∝

B(αi + yi, βi),

where by ∝ we mean that the product of the denominators is a positive constant, dependent on the problem data but
independent of y.

Variance The variance of f (y) can be written as

(cid:35)

(cid:35)

(cid:35)2

Var

X yi
i

= E

X 2yi
i

− E

X yi
i

(cid:34) n
(cid:89)

i=1

(cid:34) n
(cid:89)

i=1

(cid:34) n
(cid:89)

i=1

E [X yi
i

]2

=

=

n
(cid:89)

i=1
n
(cid:89)

i=1

E

(cid:105)

(cid:104)
X 2yi
i

−

n
(cid:89)

i=1
B(αi + 2yi, βi)
B(αi, βi)

−

n
(cid:89)

i=1

(cid:18) B(αi + yi, βi)
B(αi, βi)

(cid:19)2

.

D.1. Gradient of Expected Inﬂuence

Recall the identity

where ψ is the digamma function. We can then compute each component of the gradient of E[f (y)]:

∂
∂a

B(a, b) = B(a, b)(ψ(a) − ψ(a + b)),

∂
∂yi

(E[f (y)]) =

B(αj + yj, βj) ·

(B(αi + yi, βi))

∂
∂yi

n
(cid:89)

i=1

n
(cid:89)

i=1

1
B(αi, βi)

1
B(αi, βi)

·

·

(cid:89)

j(cid:54)=i

(cid:89)

j(cid:54)=i

=

= E[f (y)] · (ψ(αi + yi) − ψ(αi + yi + βi)).

B(αj + yj, βj) · B(αi + yi, βi) · (ψ(αi + yi) − ψ(αi + yi + βi))

(54)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

(55)

Robust Budget Allocation via Continuous Submodular Functions

D.2. Counterexample for Fact 2.1
We give a speciﬁc choice of parameters n, αi, βi and yi for which the resulting function (cid:112)Var(f (y)) is non-convex, non-
concave, non-submodular and non-supermodular for various points y ∈ Rn
+. For the case T = 1, the function 1 − f (y) is
a valid inﬂuence function, so we have a valid counterexample for (cid:112)Var(I(X, y)).

Consider the case n = 2, with α1 = α2 = 1 and β1 = β2 = 1. This corresponds to the Budget Allocation problem
where we have two sources each with an edge to one customer, and we have only our prior (i.e. no data) on either of the
edge probabilities. Using equation (51), we can directly compute the Hessian of (cid:112)Var(f (y)) at any point y, e.g. using
Mathematica. In particular, for y1 = y2 = 1, the Hessian has a positive and a negative eigenvalue, so (cid:112)Var(f (y)) is
neither convex nor concave at this point. Also for y1 = y2 = 1, the off-diagonal element is negative, so (cid:112)Var(f (y)) is
not supermodular over all of R2
+. However, for y1 = y2 = 3, the off-diagonal element is positive, so our function is also
not submodular.

