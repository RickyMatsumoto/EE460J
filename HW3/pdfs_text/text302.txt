Supplementary Material for Efﬁcient Nonmyopic Active Search

Shali Jiang 1 Gustavo Malkomes 1 Geoff Converse 2 Alyssa Shofner 3 Benjamin Moseley 1 Roman Garnett 1

1. Hardness of Active Search

) = Pr(y = 1

In this section, we present the proof of Theorem 1. We
assume that active search policies have access to the correct
), for
marginal probabilities f (x;
|
any given point x and labeled data
, which may include
“ﬁcticious” observations. Further, the computational cost
will be analyzed as the number of calls to f , i.e., f (x; D)
has unit cost. Note that the optimal policy operates in such
a computational model, with exponentially many calls (in
terms of

) to the marginal probability function f .

x,

D

D

D

|X |

Our proof technique consists of constructing an explicit ac-
tive search instance where a small “secret” set of points S
encodes the location of a larger “hidden” group of positive
points. A particular feasibly exponential-cost policy identi-
ﬁes this small set ﬁrst, and then obtains a large reward by
collecting the revealed targets. We will show that an algo-
rithm with limited computational power (i.e., polynomial
in n =
) will not be able to identify the set S. As a
consequence, its performance will be arbitrarily worse than
an optimal solution as the size of the instance increases.

|X |

The crux of the proof is to construct a class of instances
H
that we present next. Figure 1 shows a schematic represen-
tation of an example instance
H
differ from each other by a permutation of the labels. An
= 22m points, where m is a parameter
instance has n =
of the instance. The search budget is deﬁned to be B = m2.
The points in each instance can be categorized as follows.

. The instances in

I ∈ H

|X |

“Clumps.” These points are partitioned into 2m groups,
which we will call “clumps,” each of size B. All points
in a clump share the same label. Additionally, exactly one
of the clumps comprises all positive points; the remaining
points are all negative. The clump containing the positive
points is chosen uniformly at random; therefore, the prior
marginal probability for any point xc in this category is
m. We denote the clump of all positive
f (xc;

) = pc = 2−
∅

1Washington University in St. Louis, St. Louis, MO, USA
2Simpson College, Indianola, IA, USA 3University of South
Carolina, Columbia, SC, USA. Correspondence to: Shali Jiang
<jiang.s@wustl.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

points C
, where
∗
2m. Figure 1(b) illustrates these points.
1
≤ ∗ ≤

can be regarded as a m-bit integral index

∗

“Isolated points.” The remaining points share the property
that observing any single one of them does not change the
marginal probabilities of any other point. These points
are illustrated in Figures 1(a) (black dots) and 1(c). The
marginal probabilities for any point xb in this category is
deﬁned to be f (xb;
1/2, where we deﬁne
c = √m/2 for convenience. These points can be further
classiﬁed into two categories:

) = pb = 1
∅

(cid:112)

−

c

•

A “secret set,” denoted by S; see Figure 1(a) (black
dots). These points encode which of the clumps C
∗
contains the positive points, using a scheme we de-
scribe below. For ease of exposition, we partition
the set S into m subsets S1, . . . , Sm, each of size dc,
= mdc = m2/2 =
where we deﬁne d = √m. Thus
B/2; the size of this secret set is exactly half the budget.

S

|

|

∗

’s index is

The key of this construction is that each subset Si en-
codes one bit bi of information about which clump C
∗
contains the positive points, using a simple encoding
scheme: the binary representation of the positive clump
C
= b1b2 . . . bm. Each bit is encoded with
∗
a two-step mechanism. First, each Si is partitioned into
d groups of c points, each group encoding a “meta” bit
of information bij, 1
d, by a logical
≤
OR. These meta-bits, not in the problem instance, are
illustrated by the white dots in 1(a). Finally, the meta-
bits associated with Si encode the bit bi via a logical
XOR,1 bi = bi1 ⊕ · · · ⊕
bid.
Independent points. The remaining 22m
−
mdc points are totally independent from the others;
revealing them conveys no information for any other
point. We denote this set of points by R.

2mB

m, 1

≤

−

≤

≤

j

i

•

Observation 1. At least d points from Si need to be ob-
served in order to infer one bit bi of information about the
positive clump.

A virtual bit bi is computed by the XOR operation of the
bid. Notice that
d associated meta-bits bi = bi1 ⊕ · · · ⊕
each bij has same marginal probability of being positive,

1Sum of the bits modulo 2.

Supplementary Material for Efﬁcient Nonmyopic Active Search

(a) Secret set S of mdc isolated points.

(b) 2mB points from clumps Cj.

(c) Isolated and independent points R.

Figure 1: An instance of active search where any efﬁcient algorithm can be arbitrarily worse than an optimal policy.

∀

−

i.e.,

i, Pr(bi = 1) = 1/2.

i, j, Pr(bij = 1) = (1

pb)c = 1/2. We also have
It is necessary to observe all d
∀
meta-bits bij from the same group Si to infer the bit bi,
since observing a fraction of the inputs of an XOR does not
change the marginal belief about its outcome. So observing
1 or fewer points from S conveys no information about
d
) =
the positive clump. Equivalently,
1.
Pr(y = 1

x, Pr(y = 1

x) if

x,

−

D

S

∀

d

|

|

|D ∩

| ≤

−

Observation 2. Observing any number of clump points
does not change the marginal probability of any point in the
secret set S.

We need to make sure that no external information can
help to identify the secret set S. Notice that the knowledge
of bi does not change the marginal probability of any bij;
hence, no point x in S will have a different probability after
observing bi. This means that observing points outside S
does not help distinguish S from the remaining isolated
points R.

Now we formally restate Theorem 1 and provide its proof.
The theorem implies that a polynomial time algorithm can-
not achieve a constant approximation ratio.

Theorem 1. Any (possibly randomized) policy for active
2 log n
search that performs o
),
D
B, has approximation ratio with respect to the
with
(cid:1)
where

expected utility of the optimal policy of

inference calls f (x,

n√ 1

|D| ≤

(cid:0)

1
√log n

O

= n is the number of points, and B is the budget.2

(cid:16)

(cid:17)

|X |

A

Proof. Consider a random instance

and ﬁx a policy
. Let α be the total number of inference calls performed

I ∈ H

2Note we used the little-o notation for the number of inference

calls, and the big-O notation of the approximation ratio.

throughout its execution. At the ith inference call
Di
has a very small

by
A
Pr(y = 1
A
of size at most B. We will show that
A
probability of collecting a large reward on

might use an arbitrary training set

Di),

x,

|

.

I

A

Before analyzing the algorithm
, we present a lower bound
on the performance of an optimal policy. Consider the
following policy with unlimited computational power: In
the ﬁrst iteration, compute the marginal probability of an
arbitrary ﬁxed clump point, conditioning on observing every
possible subset of the isolated points of size d with labels
2 log n) inference
all equal to 1. This set of
calls will reveal the location of the secret set S: exactly
those points will modify the probabilities of the ﬁxed clump
point. Now the policy spends the ﬁrst half of its budget
= B/2). These outcomes
querying the points in S (recall
reveal the hidden clump of positives C
. The policy now
∗
spends the second half of the budget querying (collecting)
these positive points. The expected performance of this
strategy is B/2 + pb
B/2 > B/2. Certainly this is a lower
bound on the optimal performance; hence

(nd) =

(n√ 1

S
|

O

O

|

OPT >

B
2

.

(1)

S

A

A

|Di ∩

at the ith inference. By
Now consider the algorithm
cannot differentiate between the
Observations 1 and 2,
d. Suppose that
points in S and those in R unless
before the ith inference, the algorithm has no information
about S. Then the chance of
Di such that
d is no better than that of a random selection
|Di ∩
2mB is the number of
from n(cid:48) points, where n(cid:48) = n
isolated points. We can upper bound the probability of
A
d, by counting
choosing a dataset
how many subsets would contain at least d points from S,

Di such that

choosing a

|Di ∩

| ≥

| ≥

| ≥

A

−

S

S

(m...…(…(…c……ORdORORXOR…(…(…c……ORdORORXOR…(…(…c……ORdORORXORBB...B(2m(…22m 2mB mdcSupplementary Material for Efﬁcient Nonmyopic Active Search

among all subsets of the n(cid:48) points of size at most B:

Pr

|Di ∩

S

| ≥

d

B/2
d

n(cid:48)
B

d
d

−
−

≤ (cid:0)

n(cid:48)
(cid:1)(cid:0)
B

.

(cid:1)

(2)

(cid:0)

(cid:1)

(cid:0)

(cid:1)
We only consider the isolated points because an algorithm
that only queries the isolated points has a higher prob-
A
ability of hitting the secret set. Also note that technically
n(cid:48)
i + 1 since one
the denomenator in (2) should be
B
Dj, j < i as those be-
would not choose the same subsets
(cid:0)
(cid:1)
α (assuming
fore the ith inference. But asymptotically i
n(cid:48)
α =
,
B

(2n) for now) is of much lower order than
n(cid:48)
B

O
therefore
ability of algorithm
(cid:0)
A
inferences; then ph can be union-bounded:

. Denote ph as the prob-
(cid:1)
ever “hiting” the secret set after α

α + 1 = Θ

(cid:16)(cid:0)

(cid:1)(cid:17)

n(cid:48)
B

−

−

≤

(cid:0)

(cid:1)

ph ≤

<

=

n(cid:48)
B

d
d

−
−

(cid:1)

n(cid:48)
(cid:1)(cid:0)
B
d

α

B/2
d

(cid:0)

α

B
Bd
(cid:1)
(cid:0)
2
(n(cid:48))d
(cid:0)
(cid:1)
α
2n(cid:48)
B2

d

Note B = m2 = 1
√n 1

4 log2 n = Θ(n), so

(cid:1)
4 log2 n and n(cid:48) = n

(cid:0)

2mB = n

−

−

d

2n(cid:48)
B2

(cid:18)

(cid:19)

= Θ

d

2n
B2

(cid:32)(cid:18)

(cid:33)

(cid:19)

√ 1

2 log n






= Θ 

n
32 log4 n (cid:33)

1

(cid:32)



n√ 1

= Θ

2 log n

.

(cid:16)

(cid:17)

ph <

α
Θ(n√ 1
2 log n)

.

We have now derived

So for any α =
constant, we have

O

(cid:0)

n√ 1

2 log n

ε

−

, where ε is a positive

(cid:1)

1
nε

.

(cid:19)

ph <

O

(cid:18)

can be upper bounded by pretending that the algorithm had
a larger budget of size 2B; in which half the budget (i.e.,
B) is spent on querying points in R, and the other half on
querying the clump points. The expected number of targets
found after B queries on R is

Bpb = B(1

c

1/2) = B

1

2√

m

2−

.

(4)

−

(cid:112)

−

(cid:16)

(cid:17)

The expected number of targets found after B queries on
the clump points is

B
2m +

1

(cid:18)
=

1
2m
−
(cid:19) (cid:18)
B(B + 1)
2m+1

.

B
2m

1
1

−
−

+

1
(cid:18)

1

−

2m

1

(cid:19)

−

(

· · ·

)
(cid:19)

(5)

Combining (4) and (5), we get that the expected perfor-
never hits S can be upper
mance in the case when
bounded by

A

B(B + 1)

2m+1 + B

1

−

2√

m

2−

.

The overall expected performance of
bounded by

(cid:0)

(cid:1)
A

can be upper

B(B + 1)

E

A

< Bph +

2m+1 + B

1

−

2√

m

2−

,

(cid:1)
where we have used the trivial upper bound 1 > (1
ph).
Finally, combining with the lower bound of OPT in (1), the
approximation ratio can be upper bounded by

−

(cid:0)

2√

m

2−

(cid:1)

2√

m

2−

√
(cid:1)
2
2√
2−
log n

1

−

(cid:16)

(cid:17)(cid:19)

E

A
OPT

<

Bph + B(B+1)

2m+1 + B
B/2

1

−

(cid:0)

1

−

(cid:0)
+ 2

B + 1
2m + 2
log2 n
4√n

= 2ph +

=

=

O

(cid:18)

O

1
nε +
1
√log n

(cid:18)
n√ 1

.

(cid:19)

= o
√
2√
2
log n
(cid:0)

2−
(cid:1)

for any α =

O

2 log n

ε

−

n√ 1

2 log n

. Note that it

is easy to verify that 2
(cid:0)
L’Hôpital’s rule.

1

−

(cid:0)

(3)

= Θ

1
√log n

with

(cid:1)
(cid:16)

(cid:17)

(cid:1)

A

ever hits the secret set S, for simplicity, we will assume
If
that it achieves maximal performance B. If
never ﬁnds
the secret set, we can further consider the following two
R, no marginal
cases. If the algorithm queries points x
∈
probabilities are changed; if a point x
Cj is queried, for
any clump j, only the marginal probabilities of the clumps
are changed. The expected performance in these two cases

A

∈

2. Results on Individual Activity Classes

Figure 2 shows the learning curves of two-step lookahead
and our method, on the ﬁrst six activity classes (of the total
120) for the ECFP4 ﬁngerprint. We can see clear patterns
indicating a transition from exploration to exploitation, es-
pecially for activity classes 1 and 2. The number of targets
found by ENS ﬁrst grows slowly, but after a certain point,

Supplementary Material for Efﬁcient Nonmyopic Active Search

5. Mean Difference Curves for CiteSeerx and

BMG Data

Figures 5 and 6 show the the mean difference curve between
ENS and two-step, for various budgets, respectively for the
CiteSeerx and BMG data.

Acknowledgments

We would like to thank Brendan Juba for insightful discus-
sion. SJ, GM, and RG were supported by the National Sci-
ence Foundation (NSF) under award number IIA–1355406.
GM was also supported by the Brazilian Federal Agency
for Support and Evaluation of Graduate Education (CAPES).
GC and AS were supported by NSF under award number
CNS–1560191. BM was supported by a Google Research
Award, a Yahoo Research Award, and by NSF under award
number CCF–1617724.

References

ASM Alloy Center Database.

URL http://mio.

asminternational.org/ac/.

Auer, Peter. Using Conﬁdence Bounds for Exploitation–
Exploration Trade-offs. Journal of Machine Learning
Research, 3:397–422, 2002.

(7)

Kawazoe, Yoshiyuki, Yu, Jing-Zhi, Tsai, An-Pang, and
Masumoto, Tsuyoshi (eds.). Nonequilibrium Phase Di-
agrams of Ternary Amorphous Alloys, volume 37A of
Condensed Matter. Springer–Verlag, 1997.

Wang, Xuezhi, Garnett, Roman, and Schneider, Jeff. Ac-
tive Search on Graphs. In Proceedings of the 19th ACM
SIGKDD International Conference on Knowledge Discov-
ery and Data Mining, pp. 731–738, 2013.

Ward, Logan, Agrawal, Ankit, Choudhary, Alok, and
Wolverton, Christopher. A General-Purpose Machine
Learning Framework for Predicting Properties of Inor-
ganic Materials. 2016. arXiv preprint arXiv:1606.09551
[cond-mat.mtrl-sci].

grows increasingly faster through the end of the budget. For
the two-step-lookahead myopic policy, most of the time
it behaves the opposite: it ﬁrst greedly discovers targets
faster than ENS, but in later stages, it usually levels off, not
knowing where else to go. Note the point when ENS transi-
tions from exploration to exploitation can be different for
different problems, but we see a very clear pattern when
these results are averaged over all 120 activity classes, as
shown by the difference of the two average learning curves
in Figure 3 of the main text. We also show the difference
curve for the other ﬁngerprint in Figure 3 here.

3. UCB-Style Score

We mentioned in Section 5 of the main text that we have
investigated the UCB-style (Auer, 2002) score function

α(x,

) = π + γ

π(1

π),

(6)

D

−

(cid:112)

|

D

x,

where π = Pr(y = 1
) and γ is a tradeoff parameter
between exploitation (ﬁrst term) and exploration (second
term). The results of this score varying the hyperparameter γ
are shown in Figure 4, for the CiteSeerx dataset and the drug
discovery dataset with ﬁngerprint ECFP4, using exactly the
same experimental setting as other methods (20 experiments,
same random seed for each, sample model, etc.). Note with
a ﬁxed γ, the score α is maximized at some probability
π = p∗; it is easy to derive that

p∗ =

+

1
2

1
γ2 + 1

2

(cid:112)

∈

by setting the derivative to zero. To better present the results,
we use p∗
[0.5, 1] as the hyper-parameterization of the
score in Figure 4. In summary, on the CiteSeerx dataset, as
shown in Figure 4(a), the performance is maximized at some
p∗ near 0.6, but there does not seem to be a clear pattern.
But when we average the performance on 2 400 experiments
on the ECFP4 data, as shown in Figure 4(c), we see that the
α score is monotonically performing better with larger p∗
(or smaller γ), and converges to the greedy policy (p∗ = 1).

4. Effect of Pruning: Detailed Results

Table 1 shows the detailed results of our pruning study
described in Section 5.4 of the main text.

Table 1: Average number of pruned points in each iteration
for the two chemical datasets.

ﬁngerprint

# pruned

# total

pruned %

ECFP4
GpiDAPH3

94 995
93 173

100 518
100 518

94.5%
92.7%

Supplementary Material for Efﬁcient Nonmyopic Active Search

Figure 2: Number of active compounds found as a function of the number of queries, for protein (activity class) 1 to 6,
ﬁngerprint ECFP4, averaged over 20 experiments for each protein.

01002003004005000100200300numberofqueriesactivesfoundprotein1ENStwo-step01002003004005000100200300numberofqueriesactivesfoundprotein2ENStwo-step01002003004005000100200300numberofqueriesactivesfoundprotein3ENStwo-step01002003004005000200400numberofqueriesactivesfoundprotein4ENStwo-step0100200300400500050100150numberofqueriesactivesfoundprotein5ENStwo-step01002003004005000100200300numberofqueriesactivesfoundprotein6ENStwo-stepSupplementary Material for Efﬁcient Nonmyopic Active Search

Figure 3: The average difference in cumulative targets found
between our method and the two-step policy, averaged over
all 120 activity classes and 20 experiments, on the ﬁnger-
prints GpiDAPH3.

0100200300400500−40−20020numberofqueriesdifferenceinutilitymeandifference95%CISupplementary Material for Efﬁcient Nonmyopic Active Search

(a) CiteSeerx data

(b) BMG data

(c) ECFP4 data

Figure 4: Number of targets found by the UCB-style policy (6), as a function of the hyperparameter p∗ as derived in (7),
averaged over 20 experiments. Note for CiteSeerx and BMG datasets, the grid size of p∗ is 0.01, but for ECFP4, we can only
afford grid size of 0.1. To put these results into perspective, we also show the two-step performances by the red horizontal
line, indicating two-step performs better than the UCB-style policy on all three domains. All these results are with budget
500.

0.50.60.70.80.91130140150p∗averagenumberoftargetsUCB-styletwo-step0.50.60.70.80.91300350400450p∗averagenumberoftargetsUCB-styletwo-step0.50.60.70.80.91150200250300p∗averagenumberofactivesUCB-styletwo-stepSupplementary Material for Efﬁcient Nonmyopic Active Search

(a) Budget 100; p = 0.017

(b) Budget 300; p = 0.020

5
(c) Budget 500; p = 9.9 × 10−

4
(d) Budget 700; p = 1.0 × 10−

(e) Budget 900; p = 0.046

Figure 5: Mean difference curves between our policy and two-step lookahead in terms of number of actives found, along
with the conﬁdence interval, for CiteSeerx data, with budget varying from 100 to 900. Also shown are the p-values of a
two-sided paired t-test testing the null hypothesis that the performance of the policies is equal at termination.

0204060801000510numberofqueriesdifferenceofnumberoftargetsfoundmeandifference95%CI0100200300−100102030numberofqueriesdifferenceofnumberoftargetsfoundmeandifference95%CI01002003004005000204060numberofqueriesdifferenceofnumberoftargetsfoundmeandifference95%CI02004006000204060numberofqueriesdifferenceofnumberoftargetsfoundmeandifference95%CI0200400600800−2002040numberofqueriesdifferenceofnumberoftargetsfoundmeandifference95%CISupplementary Material for Efﬁcient Nonmyopic Active Search

(a) Budget 100; p = 0.068

3
(b) Budget 300; p = 6.1 × 10−

4
(c) Budget 500; p = 2.1 × 10−

3
(d) Budget 700; p = 3.7 × 10−

(e) Budget 900; p = 0.091

Figure 6: Mean difference curves between our policy and two-step lookahead in terms of number of actives found, along
with the conﬁdence interval, for BMG data, with budget varying from 100 to 900. Also shown are the p-values of a two-sided
paired t-test testing the null hypothesis that the performance of the policies is equal at termination.

020406080100−202468numberofqueriesdifferenceofnumberoftargetsfoundmeandifference95%CI01002003000510numberofqueriesdifferenceofnumberoftargetsfoundmeandifference95%CI010020030040050001020numberofqueriesdifferenceofnumberoftargetsfoundmeandifference95%CI020040060001020numberofqueriesdifferenceofnumberoftargetsfoundmeandifference95%CI0200400600800020numberofqueriesdifferenceofnumberoftargetsfoundmeandifference95%CI