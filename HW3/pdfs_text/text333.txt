Understanding Black-box Predictions via Inﬂuence Functions

Pang Wei Koh 1 Percy Liang 1

Abstract
How can we explain the predictions of a black-
box model? In this paper, we use inﬂuence func-
tions — a classic technique from robust statis-
tics — to trace a model’s prediction through the
learning algorithm and back to its training data,
thereby identifying training points most respon-
sible for a given prediction. To scale up inﬂuence
functions to modern machine learning settings,
we develop a simple, efﬁcient implementation
that requires only oracle access to gradients and
Hessian-vector products. We show that even on
non-convex and non-differentiable models where
the theory breaks down, approximations to inﬂu-
ence functions can still provide valuable infor-
mation. On linear models and convolutional neu-
ral networks, we demonstrate that inﬂuence func-
tions are useful for multiple purposes: under-
standing model behavior, debugging models, de-
tecting dataset errors, and even creating visually-
indistinguishable training-set attacks.

1. Introduction

A key question often asked of machine learning systems
is “Why did the system make this prediction?” We want
models that are not just high-performing but also explain-
able. By understanding why a model does what it does, we
can hope to improve the model (Amershi et al., 2015), dis-
cover new science (Shrikumar et al., 2016), and provide
end-users with explanations of actions that impact them
(Goodman & Flaxman, 2016).

However, the best-performing models in many domains —
e.g., deep neural networks for image and speech recogni-
tion (Krizhevsky et al., 2012) — are complicated, black-
box models whose predictions seem hard to explain. Work
on interpreting these black-box models has focused on un-
derstanding how a ﬁxed model leads to particular predic-
tions, e.g., by locally ﬁtting a simpler model around the test

1Stanford University, Stanford, CA. Correspondence to:
Pang Wei Koh <pangwei@cs.stanford.edu>, Percy Liang <pli-
ang@cs.stanford.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

point (Ribeiro et al., 2016) or by perturbing the test point to
see how the prediction changes (Simonyan et al., 2013; Li
et al., 2016b; Datta et al., 2016; Adler et al., 2016). These
works explain the predictions in terms of the model, but
how can we explain where the model came from?

In this paper, we tackle this question by tracing a model’s
predictions through its learning algorithm and back to the
training data, where the model parameters ultimately de-
rive from. To formalize the impact of a training point on a
prediction, we ask the counterfactual: what would happen
if we did not have this training point, or if the values of this
training point were changed slightly?

Answering this question by perturbing the data and retrain-
ing the model can be prohibitively expensive. To overcome
this problem, we use inﬂuence functions, a classic tech-
nique from robust statistics (Cook & Weisberg, 1980) that
tells us how the model parameters change as we upweight
a training point by an inﬁnitesimal amount. This allows us
to “differentiate through the training” to estimate in closed-
form the effect of a variety of training perturbations.

Despite their rich history in statistics, inﬂuence functions
have not seen widespread use in machine learning; to the
best of our knowledge, the work closest to ours is Wo-
jnowicz et al. (2016), which introduced a method for ap-
proximating a quantity related to inﬂuence in generalized
linear models. One obstacle to adoption is that inﬂu-
ence functions require expensive second derivative calcu-
lations and assume model differentiability and convexity,
which limits their applicability in modern contexts where
models are often non-differentiable, non-convex, and high-
dimensional. We address these challenges by showing that
we can efﬁciently approximate inﬂuence functions using
second-order optimization techniques (Pearlmutter, 1994;
Martens, 2010; Agarwal et al., 2016), and that they remain
accurate even as the underlying assumptions of differentia-
bility and convexity degrade.

Inﬂuence functions capture the core idea of studying mod-
els through the lens of their training data. We show that
they are a versatile tool that can be applied to a wide variety
of seemingly disparate tasks: understanding model behav-
ior, debugging models, detecting dataset errors, and cre-
ating visually-indistinguishable adversarial training exam-
ples that can ﬂip neural network test predictions, the train-
ing set analogue of Goodfellow et al. (2015).

Understanding Black-box Predictions via Inﬂuence Functions

2. Approach

2.2. Perturbing a training input

For a point z and parameters θ ∈ Θ,
(cid:80)n

Consider a prediction problem from some input space X
(e.g., images) to an output space Y (e.g., labels). We are
given training points z1, . . . , zn, where zi = (xi, yi) ∈
X × Y.
let
L(z, θ) be the loss, and let 1
i=1 L(zi, θ) be the em-
n
pirical risk. The empirical risk minimizer is given by
ˆθ def= arg minθ∈Θ
i=1 L(zi, θ).1 Assume that the em-
pirical risk is twice-differentiable and strictly convex in θ;
in Section 4 we explore relaxing these assumptions.

(cid:80)n

1
n

2.1. Upweighting a training point

Our goal is to understand the effect of training points on a
model’s predictions. We formalize this goal by asking the
counterfactual: how would the model’s predictions change
if we did not have this training point?

Let us begin by studying the change in model pa-
rameters due to removing a point z from the train-
this change is ˆθ−z − ˆθ, where
ing set.
ˆθ−z
zi(cid:54)=z L(zi, θ). However, retraining
the model for each removed z is prohibitively slow.

Formally,
def= arg minθ∈Θ

(cid:80)

Fortunately, inﬂuence functions give us an efﬁcient approx-
imation. The idea is to compute the parameter change if z
were upweighted by some small (cid:15), giving us new param-
eters ˆθ(cid:15),z
i=1 L(zi, θ) + (cid:15)L(z, θ). A
classic result (Cook & Weisberg, 1982) tells us that the in-
ﬂuence of upweighting z on the parameters ˆθ is given by

def= arg minθ∈Θ

(cid:80)n

1
n

Iup,params(z) def=

dˆθ(cid:15),z
d(cid:15)

(cid:12)
(cid:12)
(cid:12)(cid:15)=0

= −H −1

∇θL(z, ˆθ),

ˆθ

(1)

(cid:80)n

def= 1
n

i=1 ∇2

θL(zi, ˆθ) is the Hessian and is
where Hˆθ
positive deﬁnite (PD) by assumption. In essence, we form
a quadratic approximation to the empirical risk around ˆθ
and take a single Newton step; see appendix A for a deriva-
tion. Since removing a point z is the same as upweighting
it by (cid:15) = − 1
n , we can linearly approximate the parame-
ter change due to removing z by computing ˆθ−z − ˆθ ≈
− 1

n Iup,params(z), without retraining the model.

Next, we apply the chain rule to measure how upweighting
z changes functions of ˆθ.
In particular, the inﬂuence of
upweighting z on the loss at a test point ztest again has a
closed-form expression:

Iup,loss(z, ztest) def=

dL(ztest, ˆθ(cid:15),z)
d(cid:15)

(cid:12)
(cid:12)
(cid:12)(cid:15)=0
= ∇θL(ztest, ˆθ)(cid:62) dˆθ(cid:15),z
= −∇θL(ztest, ˆθ)(cid:62)H −1
ˆθ
1We fold in any regularization terms into L.

d(cid:15)

(2)

(cid:12)
(cid:12)
(cid:12)(cid:15)=0
∇θL(z, ˆθ).

Let us develop a ﬁner-grained notion of inﬂuence by study-
ing a different counterfactual: how would the model’s pre-
dictions change if a training input were modiﬁed?

def= (x + δ, y).
For a training point z = (x, y), deﬁne zδ
Consider the perturbation z (cid:55)→ zδ, and let ˆθzδ,−z be the
empirical risk minimizer on the training points with zδ in
place of z. To approximate its effects, deﬁne the parameters
def=
resulting from moving (cid:15) mass from z onto zδ: ˆθ(cid:15),zδ,−z
i=1 L(zi, θ) + (cid:15)L(zδ, θ) − (cid:15)L(z, θ). An
arg minθ∈Θ
analogous calculation to (1) yields:

(cid:80)n

1
n

dˆθ(cid:15),zδ,−z
d(cid:15)

(cid:12)
(cid:12)
(cid:12)(cid:15)=0

= Iup,params(zδ) − Iup,params(z)

= −H −1

ˆθ

(cid:0)∇θL(zδ, ˆθ) − ∇θL(z, ˆθ)(cid:1).

(3)

As before, we can make the linear approximation ˆθzδ,−z −
ˆθ ≈ − 1
n (Iup,params(zδ) − Iup,params(z)), giving us a closed-
form estimate of the effect of z (cid:55)→ zδ on the model. Anal-
ogous equations also apply for changes in y. While in-
ﬂuence functions might appear to only work for inﬁnitesi-
mal (therefore continuous) perturbations, it is important to
note that this approximation holds for arbitrary δ: the (cid:15)-
upweighting scheme allows us to smoothly interpolate be-
tween z and zδ. This is particularly useful for working with
discrete data (e.g., in NLP) or with discrete label changes.

If x is continuous and δ is small, we can further approxi-
mate (3). Assume that the input domain X ⊆ Rd, the pa-
rameter space Θ ⊆ Rp, and L is differentiable in θ and x.
As (cid:107)δ(cid:107) → 0, ∇θL(zδ, ˆθ) − ∇θL(z, ˆθ) ≈ [∇x∇θL(z, ˆθ)]δ,
where ∇x∇θL(z, ˆθ) ∈ Rp×d. Substituting into (3),

dˆθ(cid:15),zδ,−z
d(cid:15)

(cid:12)
(cid:12)
(cid:12)(cid:15)=0

≈ −H −1

[∇x∇θL(z, ˆθ)]δ.

ˆθ

(4)

n H −1
ˆθ

[∇x∇θL(z, ˆθ)]δ. Dif-

We thus have ˆθzδ,−z − ˆθ ≈ − 1
ferentiating w.r.t. δ and applying the chain rule gives us
Ipert,loss(z, ztest)(cid:62) def= ∇δL(ztest, ˆθzδ,−z)(cid:62)(cid:12)
(cid:12)
(cid:12)δ=0
∇x∇θL(z, ˆθ).
= −∇θL(ztest, ˆθ)(cid:62)H −1
ˆθ

(5)

Ipert,loss(z, ztest)(cid:62)δ tells us the approximate effect that z (cid:55)→
z + δ has on the loss at ztest. By setting δ in the direction of
Ipert,loss(z, ztest), we can construct local perturbations of z
that maximally increase the loss at ztest. In Section 5.2, we
will use this to construct training-set attacks. Finally, we
note that Ipert,loss(z, ztest) can help us identify the features
of z that are most responsible for the prediction on ztest.

2.3. Relation to Euclidean distance

To ﬁnd the training points most relevant to a test point, it
is common to look at its nearest neighbors in Euclidean

Understanding Black-box Predictions via Inﬂuence Functions

ˆθ

Figure 1. Components of inﬂuence. (a) What is the effect of the training loss and H −1
terms in Iup,loss? Here, we plot Iup,loss against
variants that are missing these terms and show that they are necessary for picking up the truly inﬂuential training points. For these
calculations, we use logistic regression to distinguish 1’s from 7’s in MNIST (LeCun et al., 1998), picking an arbitrary test point ztest;
similar trends hold across other test points. Green dots are train images of the same label as the test image (7) while red dots are 1’s.
Left: Without the train loss term, we overestimate the inﬂuence of many training points: the points near the y=0 line should have
Iup,loss close to 0, but instead have high inﬂuence when we remove the train loss term. Mid: Without H −1
, all green training points are
helpful (removing each point increases test loss) and all red points are harmful (removing each point decreases test loss). This is because
∀x, x (cid:23) 0 (all pixel values are positive), so x · xtest ≥ 0, but it is incorrect: many harmful training points actually share the same label as
ztest. See panel (b). Right: Without training loss or H −1
testx,
which fails to accurately capture inﬂuence; the scatter plot deviates quite far from the diagonal. (b) The test image and a harmful training
image with the same label. To the model, they look very different, so the presence of the training image makes the model think that the
test image is less likely to be a 7. The Euclidean inner product does not pick up on these less intuitive, but important, harmful inﬂuences.

, what is left is the scaled Euclidean inner product ytesty · σ(−ytestθ(cid:62)xtest) · x(cid:62)

ˆθ

ˆθ

space (e.g., Ribeiro et al. (2016)); if all points have the
same norm, this is equivalent to choosing x with the largest
x · xtest. For intuition, we compare this to Iup,loss(z, ztest) on
a logistic regression model and show that inﬂuence is much
more accurate at accounting for the effect of training.

1

Let p(y | x) = σ(yθ(cid:62)x), with y ∈ {−1, 1} and σ(t) =
1+exp(−t) . We seek to maximize the probability of the
training set. For a training point z = (x, y), L(z, θ) =
log(1 + exp(−yθ(cid:62)x)), ∇θL(z, θ) = −σ(−yθ(cid:62)x)yx,
and Hθ = 1
i . From (2),
n
Iup,loss(z, ztest) is:

i=1 σ(θ(cid:62)xi)σ(−θ(cid:62)xi)xix(cid:62)

(cid:80)n

−ytesty · σ(−ytestθ(cid:62)xtest) · σ(−yθ(cid:62)x) · x(cid:62)

testH −1
ˆθ

x.

We highlight two key differences from x · xtest. First,
σ(−yθ(cid:62)x) gives points with high training loss more inﬂu-
ence, revealing that outliers can dominate the model pa-
rameters. Second, the weighted covariance matrix H −1
measures the “resistance” of the other training points to the
removal of z; if ∇θL(z, ˆθ) points in a direction of little
variation, its inﬂuence will be higher since moving in that
direction will not signiﬁcantly increase the loss on other
training points. As we show in Fig 1, these differences
mean that inﬂuence functions capture the effect of model
training much more accurately than nearest neighbors.

ˆθ

3. Efﬁciently Calculating Inﬂuence

are

challenges

two computational

to using
There
∇θL(z, ˆθ). First, it
Iup,loss(z, ztest) = −∇θL(ztest, ˆθ)(cid:62)H −1
ˆθ
(cid:80)n
θL(zi, ˆθ),
requires forming and inverting Hˆθ = 1
the Hessian of the empirical risk. With n training points
and θ ∈ Rp, this requires O(np2 + p3) operations, which

i=1 ∇2

n

is too expensive for models like deep neural networks with
millions of parameters. Second, we often want to calculate
Iup,loss(zi, ztest) across all training points zi.

The ﬁrst problem is well-studied in second-order optimiza-
tion. The idea is to avoid explicitly computing H −1
; in-
stead, we use implicit Hessian-vector products (HVPs) to
∇θL(ztest, ˆθ) and then
efﬁciently approximate stest
compute Iup,loss(z, ztest) = −stest · ∇θL(z, ˆθ). This also
solves the second problem: for each test point of inter-
est, we can precompute stest and then efﬁciently compute
−stest · ∇θL(zi, ˆθ) for each training point zi.

def= H −1
ˆθ

ˆθ

We discuss two techniques for approximating stest, both
relying on the fact that the HVP of a single term in Hˆθ,
θL(zi, ˆθ)]v, can be computed for arbitrary v in the same
[∇2
time that ∇θL(zi, ˆθ) would take, which is typically O(p)
(Pearlmutter, 1994).

Conjugate gradients (CG). The ﬁrst technique is a stan-
dard transformation of matrix inversion into an optimiza-
tion problem. Since Hˆθ (cid:31) 0 by assumption, H −1
v ≡
ˆθ
arg mint{ 1
2 t(cid:62)Hˆθt − v(cid:62)t}. We can solve this with CG
approaches that only require the evaluation of Hˆθt, which
takes O(np) time, without explicitly forming Hˆθ. While an
exact solution takes p CG iterations, in practice we can get
a good approximation with fewer iterations; see Martens
(2010) for more details.

Stochastic estimation. With large datasets, standard CG
can be slow; each iteration still goes through all n train-
ing points. We use a method developed by Agarwal et al.
(2016) to get an estimator that only samples a single point
per iteration, which results in signiﬁcant speedups.

Understanding Black-box Predictions via Inﬂuence Functions

j

def= (cid:80)j

Dropping the ˆθ subscript for clarity, let H −1
i=0(I −
H)i, the ﬁrst j terms in the Taylor expansion of H −1.
j = I + (I − H)H −1
Rewrite this recursively as H −1
j−1.
From the validity of the Taylor expansion, H −1
j → H −1 as
j → ∞.2 The key is that at each iteration, we can substi-
tute the full H with a draw from any unbiased (and faster-
to-compute) estimator of H to form ˜Hj. Since E[ ˜H −1
] =
H −1
j

, we still have E[ ˜H −1

] → H −1.

j

j

and recursively compute ˜H −1

from the training data; deﬁne ˜H −1

In particular, we can uniformly sample zi and use
θL(zi, ˆθ) as an unbiased estimator of H. This gives
∇2
us the following procedure: uniformly sample t points
zs1, . . . , zst
0 v =
j v = v + (cid:0)I −
v;
∇2
t v as our ﬁnal unbiased es-
timate of H −1v. We pick t to be large enough such that ˜Ht
stabilizes, and to reduce variance we repeat this procedure
r times and average results. Empirically, we found this sig-
niﬁcantly faster than CG.

θL(zsj , ˆθ)(cid:1) ˜H −1

j−1v, taking ˜H −1

We note that the original method of Agarwal et al. (2016)
for which
dealt only with generalized linear models,
θL(zi, ˆθ)]v can be efﬁciently computed in O(p) time.
[∇2
In our case, we rely on Pearlmutter (1994)’s more general
algorithm for fast HVPs, described above, to achieve the
same time complexity.3

n ∇θL(ztest, ˆθ)(cid:62)H −1
ˆθ

With these techniques, we can compute Iup,loss(zi, ztest)
on all training points zi in O(np + rtp) time; we show in
Section 4.1 that empirically, choosing rt = O(n) gives ac-
curate results. Similarly, we compute Ipert,loss(zi, ztest)(cid:62) =
∇x∇θL(zi, ˆθ)
− 1
two
matrix-vector products: we ﬁrst compute stest,
then
test∇x∇θL(zi, ˆθ), with the same HVP trick.
s(cid:62)
These
computations are easy to implement in auto-grad systems
like TensorFlow (Abadi et al., 2015) and Theano (Theano
D. Team, 2016), as users need only specify L; the rest is
automatically handled.

with

4. Validation and Extensions

Recall that inﬂuence functions are asymptotic approxima-
tions of leave-one-out retraining under the assumptions that
(i) the model parameters ˆθ minimize the empirical risk,
and that (ii) the empirical risk is twice-differentiable and

2We assume w.l.o.g.

θL(zi, ˆθ) (cid:52) I; if this is not
that ∀i, ∇2
true, we can scale the loss down without affecting the parameters.
θL(zi, ˆθ) (e.g., for
In some cases, we can get an upper bound on ∇2
linear models and bounded input), which makes this easy. Other-
wise, we treat the scaling as a separate hyperparameter and tune
it such that the Taylor expansion converges.

3To increase stability, especially with non-convex models (see
Section 4.2), we can also sample a mini-batch of training points
at each iteration, instead of relying on a single training point.

Figure 2. Inﬂuence matches leave-one-out retraining. We arbi-
trarily picked a wrongly-classiﬁed test point ztest, but this trend
held more broadly. These results are from 10-class MNIST. Left:
For each of the 500 training points z with largest (cid:12)
(cid:12)Iup,loss(z, ztest)(cid:12)
(cid:12),
we plotted − 1
n · Iup,loss(z, ztest) against the actual change in test
loss after removing that point and retraining. The inverse HVP
was solved exactly with CG. Mid: Same, but with the stochastic
approximation. Right: The same plot for a CNN, computed on
the 100 most inﬂuential points with CG. For the actual difference
in loss, we removed each point and retrained from ˜θ for 30k steps.

strictly convex. Here, we empirically show that inﬂuence
functions are accurate approximations (Section 4.1) that
provide useful information even when these assumptions
are violated (Sections 4.2, 4.3).

4.1. Inﬂuence functions vs. leave-one-out retraining

Inﬂuence functions assume that the weight on a training
point is changed by an inﬁnitesimally small (cid:15). To investi-
gate the accuracy of using inﬂuence functions to approx-
imate the effect of removing a training point and retrain-
n Iup,loss(z, ztest) with L(ztest, ˆθ−z) −
ing, we compared − 1
L(ztest, ˆθ) (i.e., actually doing leave-one-out retraining).
With a logistic regression model on 10-class MNIST,4 the
predicted and actual changes matched closely (Fig 2-Left).

θL(zi, ˆθ)]v, this runs quickly:

The stochastic approximation from Agarwal et al. (2016)
was also accurate with r = 10 repeats and t = 5, 000 iter-
ations (Fig 2-Mid). Since each iteration only requires one
HVP [∇2
in fact, we accu-
rately estimated H −1v without even looking at every data
point, since n = 55, 000 > rt. Surprisingly, even r = 1
worked; while results were noisier, it was still able to iden-
tify the most inﬂuential points.

4.2. Non-convexity and non-convergence
In Section 2, we took ˆθ as the global minimum. In practice,
if we obtain our parameters ˜θ by running SGD with early
stopping or on non-convex objectives, ˜θ (cid:54)= ˆθ. As a result,
H˜θ could have negative eigenvalues. We show that inﬂu-
ence functions on ˜θ still give meaningful results in practice.

Our approach is to form a convex quadratic approxima-
i.e., ˜L(z, θ) = L(z, ˜θ) +
tion of the loss around ˜θ,

4We trained with L-BFGS (Liu & Nocedal, 1989), with L2
regularization of 0.01, n = 55, 000, and p = 7, 840 parameters.

Understanding Black-box Predictions via Inﬂuence Functions

Figure 3. Smooth approximations to the hinge loss. (a) By varying t, we can approximate the hinge loss with arbitrary accuracy: the
green and blue lines are overlaid on top of each other. (b) Using a random, wrongly-classiﬁed test point, we compared the predicted
vs. actual differences in loss after leave-one-out retraining on the 100 most inﬂuential training points. A similar trend held for other test
points. The SVM objective is to minimize 0.005 (cid:107)w(cid:107)2
i Hinge(yiw(cid:62)xi). Left: Inﬂuence functions were unable to accurately
predict the change, overestimating its magnitude considerably. Mid: Using SmoothHinge(·, 0.001) let us accurately predict the change
in the hinge loss after retraining. Right: Correlation remained high over a wide range of t, though it degrades when t is too large. When
t = 0.001, Pearson’s R = 0.95; when t = 0.1, Pearson’s R = 0.91.

2 + 1

(cid:80)

n

2 (θ − ˜θ)(cid:62)(H˜θ +λI)(θ − ˜θ). Here, λ is
∇L(z, ˜θ)(cid:62)(θ − ˜θ)+ 1
a damping term that we add if H˜θ has negative eigenvalues;
this corresponds to adding L2 regularization on θ. We then
calculate Iup,loss using ˜L. If ˜θ is close to a local minimum,
this is correlated with the result of taking a Newton step
from ˜θ after removing (cid:15) weight from z (see appendix B).

We checked the behavior of Iup,loss in a non-convergent,
non-convex setting by training a convolutional neural net-
work for 500k iterations.5 The model had not converged
and H˜θ was not PD, so we added a damping term with
λ = 0.01. Even in this difﬁcult setting, the predicted and
actual changes in loss were highly correlated (Pearson’s R
= 0.86, Fig 2-Right).

4.3. Non-differentiable losses

θL, do not exist?

What happens when the derivatives of the loss, ∇θL and
∇2
In this section, we show that in-
ﬂuence functions computed on smooth approximations to
non-differentiable losses can predict the behavior of the
original, non-differentiable loss under leave-one-out re-
training. The robustness of this approximation suggests
that we can train non-differentiable models and swap out
non-differentiable components for smoothed versions for
the purposes of calculating inﬂuence.

To see this, we trained a linear SVM on the same 1s
vs. 7s MNIST task in Section 2.3. This involves min-

5The network had 7 sets of convolutional layers with tanh(·)
non-linearities, modeled after the all-convolutional network from
(Springenberg et al., 2014). For speed, we used 10% of the
MNIST training set and only 2,616 parameters, since repeatedly
retraining the network was expensive. Training was done with
mini-batches of 500 examples and the Adam optimizer (Kingma
& Ba, 2014). The model had not converged after 500k iterations;
training it for another 500k iterations, using a full training pass
for each iteration, reduced train loss from 0.14 to 0.12.

imizing Hinge(s) = max(0, 1 − s); this simple piece-
wise linear function is similar to ReLUs, which cause non-
differentiability in neural networks. We set the deriva-
tives at the hinge to 0 and calculated Iup,loss. As one
might expect, this was inaccurate (Fig 3b-Left): the sec-
ond derivative carries no information about how close a
support vector z is to the hinge, so the quadratic approx-
imation of L(z, ˆθ) is linear (up to regularization), which
leads to Iup,loss(z, ztest) overestimating the inﬂuence of z.

For the purposes of calculating inﬂuence, we approximated
Hinge(s) with SmoothHinge(s, t) = t log(1+exp( 1−s
t )),
which approaches the hinge loss as t → 0 (Fig 3a). Using
the same SVM weights as before, we found that calculat-
ing Iup,loss using SmoothHinge(s, 0.001) closely matched
the actual change due to retraining in the original Hinge(s)
(Pearson’s R = 0.95; Fig 3b-Mid) and remained accurate
over a wide range of t (Fig 3b-Right).

5. Use Cases of Inﬂuence Functions

5.1. Understanding model behavior

By telling us the training points “responsible” for a given
prediction, inﬂuence functions reveal insights about how
models rely on and extrapolate from the training data. In
this section, we show that two models can make the same
correct predictions but get there in very different ways.

We compared (a) the state-of-the-art Inception v3 network
(Szegedy et al., 2016) with all but the top layer frozen6 to
(b) an SVM with an RBF kernel on a dog vs. ﬁsh image
classiﬁcation dataset we extracted from ImageNet (Rus-
sakovsky et al., 2015), with 900 training examples for each
class. Freezing neural networks in this way is not uncom-

6We used pre-trained weights from Keras (Chollet, 2015).

Understanding Black-box Predictions via Inﬂuence Functions

mon in computer vision and is equivalent to training a lo-
gistic regression model on the bottleneck features (Don-
ahue et al., 2014). We picked a test image both models
got correct (Fig 4-Top) and used SmoothHinge(·, 0.001)
to compute the inﬂuence for the SVM.

As expected, Iup,loss in the RBF SVM varied inversely with
raw pixel distance, with training images far from the test
image in pixel space having almost no inﬂuence. The In-
ception inﬂuences were much less correlated with distance
in pixel space (Fig 4-Left). Looking at the two most help-
ful images (most positive −Iup,loss) for each model in Fig
4-Right, we see that the Inception network picked up on the
distinctive characteristics of clownﬁsh, whereas the RBF
SVM pattern-matched training images superﬁcially.

Moreover, in the RBF SVM, ﬁsh (green points) close to
the test image were mostly helpful, while dogs (red) were
mostly harmful, with the RBF acting as a soft nearest
neighbor function (Fig 4-Left). In contrast, in the Incep-
tion network, ﬁsh and dogs could be helpful or harmful for
correctly classifying the test image as a ﬁsh; in fact, some
of the most helpful training images were dogs that, to the
model, looked very different from the test ﬁsh (Fig 4-Top).

able from real test images but completely fool a classiﬁer
(Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016).
We demonstrate that inﬂuence functions can be used to
craft adversarial training images that are similarly visually-
indistinguishable and can ﬂip a model’s prediction on a sep-
arate test image. To the best of our knowledge, this is the
ﬁrst proof-of-concept that visually-indistinguishable train-
ing attacks can be executed on otherwise highly-accurate
neural networks.

The key idea is that Ipert,loss(z, ztest) tells us how to mod-
ify training point z to most increase the loss on ztest.
Concretely, for a target test image ztest, we can construct
˜zi, an adversarial version of a training image zi, by ini-
:= Π(˜zi +
:= zi and then iterating ˜zi
tializing ˜zi
α sign(Ipert,loss(˜zi, ztest))), where α is the step size and Π
projects onto the set of valid images that share the same 8-
bit representation with zi. After each iteration, we retrain
the model. This is an iterated, training-set analogue of the
methods used by, e.g., Goodfellow et al. (2015); Moosavi-
Dezfooli et al. (2016) for test-set attacks.

We tested these training attacks on the same Inception net-
work on dogs vs. ﬁsh from Section 5.1, choosing this pair
of animals to provide a stark contrast between the classes.
We set α = 0.02 and ran the attack for 100 iterations
on each test image. As before, we froze all but the top
layer for training; note that computing Ipert,loss still involves
differentiating through the entire network. Originally, the
model correctly classiﬁed 591 / 600 test images. For each
of these 591 test images, considered separately, we tried to
ﬁnd a visually-indistinguishable perturbation (i.e., same 8-
bit representation) to a single training image, out of 1,800
total training images, that would ﬂip the model’s predic-
tion. We were able to do this on 335 (57%) of the 591
test images. By perturbing 2 training images for each test
image, we could ﬂip predictions on 77% of the 591 test im-
ages; and if we perturbed 10 training images, we could ﬂip
all but 1 of the 591. The above results are from attacking
each test image separately, i.e., using a different training set
to attack each test image. We also tried to attack multiple
test images simultaneously by increasing their average loss,
and found that single training image perturbations could si-
multaneously ﬂip multiple test predictions as well (Fig 5).

We make three observations about these attacks. First,
though the change in pixel values is small, the change in
the ﬁnal Inception feature layer is signiﬁcantly larger: us-
ing L2 distance in pixel space, the training values change
by less than 1% of the mean distance of a training point to
its class centroid, whereas in Inception feature space, the
change is on the same order as the mean distance. This
leaves open the possibility that our attacks, while visually-
imperceptible, can be detected by examining the feature
space. Second, the attack tries to perturb the training ex-

vs.
(cid:107)z − ztest(cid:107)2

RBF SVM. Bottom left:
Figure 4. Inception
−Iup,loss(z, ztest) vs.
2. Green dots are ﬁsh and
red dots are dogs. Bottom right: The two most helpful training
images, for each model, on the test. Top right: An image of a
dog in the training set that helped the Inception model correctly
classify the test image as a ﬁsh.

5.2. Adversarial training examples

In this section, we show that models that place a lot of in-
ﬂuence on a small number of points can be vulnerable to
training input perturbations, posing a serious security risk
in real-world ML systems where attackers can inﬂuence the
training data (Huang et al., 2011). Recent work has gener-
ated adversarial test images that are visually indistinguish-

Understanding Black-box Predictions via Inﬂuence Functions

Figure 5. Training-set at-
tacks. We targeted a
set of 30 test images fea-
turing the ﬁrst author’s
dog in a variety of poses
By
and backgrounds.
maximizing the average
loss over
these 30 im-
ages, we created a visually-
imperceptible change to
the particular training im-
age (shown on top) that
ﬂipped predictions on 16
test images.

ample in a direction of low variance, causing the model to
overﬁt in that direction and consequently incorrectly clas-
sify the test images; we expect attacking to be harder as
the number of training examples grows. Third, ambiguous
or mislabeled training images are effective points to attack:
the model has low conﬁdence and thus high loss on them,
making them highly inﬂuential (recall Section 2.3). For ex-
ample, the image in Fig 5 contains both a dog and a ﬁsh and
is highly ambiguous; as a result, it is the training example
that the model is least conﬁdent on (with a conﬁdence of
77%, compared to the next lowest conﬁdence of 90%).

This attack is mathematically equivalent to the gradient-
based training set attacks explored by Biggio et al. (2012);
Mei & Zhu (2015b) and others in the context of different
models. Biggio et al. (2012) constructed a dataset poison-
ing attack against a linear SVM on a two-class MNIST task,
but had to modify the training points in an obviously distin-
guishable way to be effective. Measuring the magnitude of
Ipert,loss gives model developers a way of quantifying how
vulnerable their models are to training-set attacks.

5.3. Debugging domain mismatch

Domain mismatch — where the training distribution does
not match the test distribution — can cause models with
high training accuracy to do poorly on test data (Ben-David
et al., 2010). We show that inﬂuence functions can identify
the training examples most responsible for the errors, help-
ing model developers identify domain mismatch.

As a case study, we predicted whether a patient would be
readmitted to hospital. Domain mismatches are common
in biomedical data, e.g., different hospitals serve different
populations, and models trained on one population can do
poorly on another (Kansagara et al., 2011). We used logis-
tic regression to predict readmission with a balanced train-
ing dataset of 20K diabetic patients from 100+ US hospi-
tals, each represented by 127 features (Strack et al., 2014).7

3 out of the 24 children under age 10 in this dataset were
re-admitted. To induce a domain mismatch, we ﬁltered out
20 children who were not re-admitted, leaving 3 out of 4 re-
admitted. This caused the model to wrongly classify many
children in the test set. Our aim is to identify the 4 children
in the training set as being “responsible” for these errors.

As a baseline, we tried the common practice of looking at
the learned parameters ˆθ to see if the indicator variable for
being a child was obviously different. However, this did
not work: 14/127 features had a larger coefﬁcient.

Picking a random child ztest that the model got wrong, we
calculated −Iup,loss(zi, ztest) for each training point zi. This
clearly highlighted the 4 training children, each of whom
were 30-40 times as inﬂuential as the next most inﬂuential
examples. The 1 child in the training set who was not read-
mitted had a very positive inﬂuence, while the other 3 had
very negative inﬂuences. Moreover, calculating Ipert,loss on
these 4 children showed that the ‘child’ indicator variable
contributed signiﬁcantly to the magnitude of Iup,loss.

5.4. Fixing mislabeled examples

Labels in the real world are often noisy, especially if crowd-
sourced (Fr´enay & Verleysen, 2014), and can even be ad-
versarially corrupted. Even if a human expert could rec-
ognize wrongly labeled examples, it is impossible in many
applications to manually review all of the training data. We
show that inﬂuence functions can help human experts pri-
oritize their attention, allowing them to inspect only the ex-
amples that actually matter.

The key idea is to ﬂag the training points that exert the
most inﬂuence on the model. Because we do not have ac-
cess to the test set, we measure the inﬂuence of zi with
Iup,loss(zi, zi), which approximates the error incurred on zi
if we remove zi from the training set.

Our case study is email spam classiﬁcation, which relies

7Hospital readmission was deﬁned as whether a patient would
be readmitted within the next 30 days. Features were demo-

graphic (e.g., age, race, gender), administrative (e.g., length of
hospital stay), or medical (e.g., test results).

Understanding Black-box Predictions via Inﬂuence Functions

on user-provided labels and is also vulnerable to adversar-
ial attack (Biggio et al., 2011). We ﬂipped the labels of a
random 10% of the training data and then simulated manu-
ally inspecting a fraction of the training points, correcting
them if they had been ﬂipped. Using inﬂuence functions
to prioritize the training points to inspect allowed us to re-
pair the dataset (Fig 6, blue) without checking too many
points, outperforming the baselines of checking points with
the highest train loss (Fig 6, green) or at random (Fig 6,
red). No method had access to the test data.

ods speciﬁc to generalized linear models.

As noted in Section 5.2, our training-set attack is mathe-
matically equivalent to an approach ﬁrst explored by Big-
gio et al. (2012) in the context of SVMs, with follow-up
work extending the framework and applying it to linear
and logistic regression (Mei & Zhu, 2015b), topic mod-
eling (Mei & Zhu, 2015a), and collaborative ﬁltering (Li
et al., 2016a). These papers derived the attack directly from
the KKT conditions without considering inﬂuence, though
for continuous data, the end result is equivalent.
Inﬂu-
ence functions additionally let us consider attacks on dis-
crete data (Section 2.2), but we have not tested this em-
pirically. Our work connects the literature on training-
set attacks with work on “adversarial examples” (Goodfel-
low et al., 2015; Moosavi-Dezfooli et al., 2016), visually-
imperceptible perturbations on test inputs.

In contrast to training-set attacks, Cadamuro et al. (2016)
consider the task of taking an incorrect test prediction and
ﬁnding a small subset of training data such that changing
the labels on this subset makes the prediction correct. They
provide a solution for OLS and Gaussian process models
when the labels are continuous. Our work with inﬂuence
functions allow us to solve this problem in a much larger
range of models and in datasets with discrete labels.

7. Discussion

We have discussed a variety of applications, from creat-
ing training-set attacks to debugging models and ﬁxing
datasets. Underlying each of these applications is a com-
mon tool, the inﬂuence function, which is based on a sim-
ple idea — we can better understand model behavior by
looking at how it was derived from its training data.

At their core, inﬂuence functions measure the effect of lo-
cal changes: what happens when we upweight a point by
an inﬁnitesimally-small (cid:15)? This locality allows us to de-
rive efﬁcient closed-form estimates, and as we show, they
can be surprisingly effective. However, we might want to
ask about more global changes, e.g., how does a subpopu-
lation of patients from this hospital affect the model? Since
inﬂuence functions depend on the model not changing too
much, how to tackle this is an open question.

It seems inevitable that high-performing, complex, black-
box models will become increasingly prevalent and impor-
tant. We hope that the approach presented here — of look-
ing at the model through the lens of the training data —
will become a standard part of the toolkit of developing,
understanding, and diagnosing machine learning.

The code and data for replicating our experiments is avail-
able on GitHub http://bit.ly/gt-influence
and Codalab http://bit.ly/cl-influence.

Figure 6. Fixing mislabeled examples. Plots of how test accu-
racy (left) and the fraction of ﬂipped data detected (right) change
with the fraction of train data checked, using different algorithms
for picking points to check. Error bars show the std. dev. across
40 repeats of this experiment, with a different subset of labels
ﬂipped in each; error bars on the right are too small to be seen.
These results are on the Enron1 spam dataset (Metsis et al., 2006),
with 4,147 training and 1,035 test examples; we trained logistic
regression on a bag-of-words representation of the emails.

6. Related Work

The use of inﬂuence-based diagnostics originated in statis-
tics in the 70s and 80s, driven by seminal papers by Cook
and others (Cook, 1977; Cook & Weisberg, 1980; 1982),
though similar ideas appeared even earlier in other forms,
e.g., the inﬁnitesimal jackknife (Jaeckel, 1972). Earlier
work focused on removing training points from linear mod-
els, with later work extending this to more general models
and a wider variety of perturbations (Cook, 1986; Thomas
& Cook, 1990; Chatterjee & Hadi, 1986; Wei et al., 1998).
Most of this prior work focused on experiments with small
datasets, e.g., n = 24 and p = 10 in Cook & Weisberg
(1980), with special attention therefore paid to exact solu-
tions, or if not possible, characterizations of the error terms.

Inﬂuence functions have not been used much in the ML
literature, with some exceptions. Christmann & Stein-
wart (2004); Debruyne et al. (2008); Liu et al. (2014) use
inﬂuence functions to study model robustness and to do
fast cross-validation in kernel methods. Wojnowicz et al.
(2016) uses matrix sketching to estimate Cook’s distance,
which is closely related to inﬂuence; they focus on priori-
tizing training points for human attention and derive meth-

Understanding Black-box Predictions via Inﬂuence Functions

Acknowledgements

Chollet, F. Keras, 2015.

We thank Jacob Steinhardt, Zhenghao Chen, and Hongseok
Namkoong for helpful discussions and comments. This
work was supported by a Future of Life Research Award
and a Microsoft Research Faculty Fellowship.

Christmann, A. and Steinwart, I. On robustness properties
of convex risk minimization methods for pattern recog-
nition. Journal of Machine Learning Research (JMLR),
5(0):1007–1034, 2004.

References

Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,
Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,
Ghemawat, S., Goodfellow, I. J., Harp, A., Irving, G.,
Isard, M., Jia, Y., J´ozefowicz, R., Kaiser, L., Kudlur, M.,
Levenberg, J., Man´e, D., Monga, R., Moore, S., Mur-
ray, D. G., Olah, C., Schuster, M., Shlens, J., Steiner,
B., Sutskever, I., Talwar, K., Tucker, P. A., Vanhoucke,
V., Vasudevan, V., Vi´egas, F. B., Vinyals, O., Warden, P.,
Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. Ten-
sorﬂow: Large-scale machine learning on heterogeneous
distributed systems. arXiv preprint arXiv:1603.04467,
2015.

Adler, P., Falk, C., Friedler, S. A., Rybeck, G., Scheideg-
ger, C., Smith, B., and Venkatasubramanian, S. Auditing
black-box models for indirect inﬂuence. arXiv preprint
arXiv:1602.07043, 2016.

Agarwal, N., Bullins, B., and Hazan, E. Second order
stochastic optimization in linear time. arXiv preprint
arXiv:1602.03943, 2016.

Cook, R. D. Detection of inﬂuential observation in linear

regression. Technometrics, 19:15–18, 1977.

Cook, R. D. Assessment of local inﬂuence. Journal of the
Royal Statistical Society. Series B (Methodological), pp.
133–169, 1986.

Cook, R. D. and Weisberg, S. Characterizations of an em-
pirical inﬂuence function for detecting inﬂuential cases
in regression. Technometrics, 22:495–508, 1980.

Cook, R. D. and Weisberg, S. Residuals and inﬂuence in

regression. New York: Chapman and Hall, 1982.

Datta, A., Sen, S., and Zick, Y. Algorithmic transparency
via quantitative input inﬂuence: Theory and experiments
In Security and Privacy (SP),
with learning systems.
2016 IEEE Symposium on, pp. 598–617, 2016.

Debruyne, M., Hubert, M., and Suykens, J. A. Model selec-
tion in kernel based regression using the inﬂuence func-
tion. Journal of Machine Learning Research (JMLR), 9
(0):2377–2400, 2008.

Amershi, S., Chickering, M., Drucker, S. M., Lee, B.,
Simard, P., and Suh, J. Modeltracker: Redesigning per-
formance analysis tools for machine learning. In Con-
ference on Human Factors in Computing Systems (CHI),
pp. 337–346, 2015.

Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N.,
Tzeng, E., and Darrell, T. Decaf: A deep convolutional
activation feature for generic visual recognition. In Inter-
national Conference on Machine Learning (ICML), vol-
ume 32, pp. 647–655, 2014.

Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A.,
Pereira, F., and Vaughan, J. W. A theory of learning
from different domains. Machine Learning, 79(1):151–
175, 2010.

Biggio, B., Nelson, B., and Laskov, P. Support vector ma-
chines under adversarial label noise. ACML, 20:97–112,
2011.

Biggio, B., Nelson, B., and Laskov, P. Poisoning attacks
against support vector machines. In International Con-
ference on Machine Learning (ICML), pp. 1467–1474,
2012.

Cadamuro, G., Gilad-Bachrach, R., and Zhu, X. Debug-
ging machine learning models. In ICML Workshop on
Reliable Machine Learning in the Wild, 2016.

Fr´enay, B. and Verleysen, M. Classiﬁcation in the presence
of label noise: a survey. IEEE Transactions on Neural
Networks and Learning Systems, 25:845–869, 2014.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining
In International
and harnessing adversarial examples.
Conference on Learning Representations (ICLR), 2015.

Goodman, B. and Flaxman, S. European union regulations
on algorithmic decision-making and a “right to explana-
tion”. arXiv preprint arXiv:1606.08813, 2016.

Huang, L., Joseph, A. D., Nelson, B., Rubinstein, B. I., and
Tygar, J. Adversarial machine learning. In Proceedings
of the 4th ACM workshop on Security and artiﬁcial in-
telligence, pp. 43–58, 2011.

Chatterjee, S. and Hadi, A. S. Inﬂuential observations, high
leverage points, and outliers in linear regression. Statis-
tical Science, pp. 379–393, 1986.

Jaeckel, L. A.

The inﬁnitesimal jackknife. Unpub-
lished memorandum, Bell Telephone Laboratories, Mur-
ray Hill, NJ, 1972.

Understanding Black-box Predictions via Inﬂuence Functions

Kansagara, D., Englander, H., Salanitro, A., Kagen, D.,
Theobald, C., Freeman, M., and Kripalani, S. Risk pre-
diction models for hospital readmission: a systematic re-
view. JAMA, 306(15):1688–1698, 2011.

Ribeiro, M. T., Singh, S., and Guestrin, C. ”why should
I trust you?”: Explaining the predictions of any classi-
ﬁer. In International Conference on Knowledge Discov-
ery and Data Mining (KDD), 2016.

Kingma, D. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
In Advances in Neural Information Processing Systems
(NIPS), pp. 1097–1105, 2012.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
based learning applied to document recognition. Pro-
ceedings of the IEEE, 86(11):2278–2324, 1998.

Li, B., Wang, Y., Singh, A., and Vorobeychik, Y. Data poi-
soning attacks on factorization-based collaborative ﬁlter-
ing. In Advances in Neural Information Processing Sys-
tems (NIPS), 2016a.

Li, J., Monroe, W., and Jurafsky, D. Understanding neural
networks through representation erasure. arXiv preprint
arXiv:1612.08220, 2016b.

Liu, D. C. and Nocedal, J. On the limited memory BFGS
method for large scale optimization. Mathematical Pro-
gramming, 45(1):503–528, 1989.

Liu, Y., Jiang, S., and Liao, S. Efﬁcient approximation
of cross-validation for kernel methods using Bouligand
inﬂuence function. In International Conference on Ma-
chine Learning (ICML), pp. 324–332, 2014.

Martens, J. Deep learning via hessian-free optimization. In
International Conference on Machine Learning (ICML),
pp. 735–742, 2010.

Mei, S. and Zhu, X. The security of latent Dirichlet alloca-
tion. In Artiﬁcial Intelligence and Statistics (AISTATS),
2015a.

Mei, S. and Zhu, X. Using machine teaching to identify
optimal training-set attacks on machine learners. In As-
sociation for the Advancement of Artiﬁcial Intelligence
(AAAI), 2015b.

Metsis, V., Androutsopoulos, I., and Paliouras, G. Spam ﬁl-
tering with naive Bayes – which naive Bayes? In CEAS,
volume 17, pp. 28–69, 2006.

Moosavi-Dezfooli, S., Fawzi, A., and Frossard, P. Deep-
fool: a simple and accurate method to fool deep neural
networks. In Computer Vision and Pattern Recognition
(CVPR), pp. 2574–2582, 2016.

Pearlmutter, B. A. Fast exact multiplication by the hessian.

Neural Computation, 6(1):147–160, 1994.

Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,
M., et al. ImageNet large scale visual recognition chal-
lenge. International Journal of Computer Vision, 115(3):
211–252, 2015.

Shrikumar, A., Greenside, P., Shcherbina, A., and Kun-
daje, A. Not just a black box: Learning important fea-
tures through propagating activation differences. arXiv
preprint arXiv:1605.01713, 2016.

Simonyan, K., Vedaldi, A., and Zisserman, A. Deep in-
side convolutional networks: Visualising image clas-
arXiv preprint
siﬁcation models and saliency maps.
arXiv:1312.6034, 2013.

Springenberg, J. T., Dosovitskiy, A., Brox, T., and Ried-
miller, M. Striving for simplicity: The all convolutional
net. arXiv preprint arXiv:1412.6806, 2014.

Strack, B., DeShazo, J. P., Gennings, C., Olmo, J. L., Ven-
tura, S., Cios, K. J., and Clore, J. N. Impact of HbA1c
measurement on hospital readmission rates: analysis of
70,000 clinical database patient records. BioMed Re-
search International, 2014, 2014.

Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wo-
jna, Z. Rethinking the Inception architecture for com-
puter vision. In Computer Vision and Pattern Recogni-
tion (CVPR), pp. 2818–2826, 2016.

Theano D. Team.

Theano: A Python framework for
fast computation of mathematical expressions. arXiv
preprint arXiv:1605.02688, 2016.

Thomas, W. and Cook, R. D. Assessing inﬂuence on pre-
dictions from generalized linear models. Technometrics,
32(1):59–65, 1990.

Wei, B., Hu, Y., and Fung, W. Generalized leverage and
its applications. Scandinavian Journal of Statistics, 25:
25–37, 1998.

Wojnowicz, M., Cruz, B., Zhao, X., Wallace, B., Wolff, M.,
Luan, J., and Crable, C. “Inﬂuence sketching”: Find-
ing inﬂuential samples in large-scale regressions. arXiv
preprint arXiv:1611.05923, 2016.

