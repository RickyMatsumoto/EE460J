ADistributionalPerspectiveonReinforcementLearningA.RelatedWorkTothebestofourknowledge,theworkclosesttooursaretwopapers(Morimuraetal.,2010b;a)studyingthedistri-butionalBellmanequationfromtheperspectiveofitscu-mulativedistributionfunctions.Theauthorsproposebothparametricandnonparametricsolutionstolearndistribu-tionsforrisk-sensitivereinforcementlearning.Theyalsoprovidesometheoreticalanalysisforthepolicyevaluationsetting,includingaconsistencyresultinthenonparamet-riccase.Bycontrast,wealsoanalyzethecontrolsetting,andemphasizetheuseofthedistributionalequationstoim-proveapproximatereinforcementlearning.Thevarianceofthereturnhasbeenextensivelystud-iedintherisk-sensitivesetting.Ofnote,Tamaretal.(2016)analyzetheuseoflinearfunctionapproximationtolearnthisvarianceforpolicyevaluation,andPrashanth&Ghavamzadeh(2013)estimatethereturnvarianceinthedesignofarisk-sensitiveactor-criticalgorithm.Mannor&Tsitsiklis(2011)providesnegativeresultsregardingthecomputationofavariance-constrainedsolutiontotheopti-malcontrolproblem.Thedistributionalformulationalsoariseswhenmodellinguncertainty.Deardenetal.(1998)consideredaGaussianapproximationtothevaluedistribution,andmodelledtheuncertaintyovertheparametersofthisapproximationus-ingaNormal-Gammaprior.Engeletal.(2005)leveragedthedistributionalBellmanequationtodeﬁneaGaussianprocessovertheunknownvaluefunction.Morerecently,Geist&Pietquin(2010)proposedanalternativesolutiontothesameproblembasedonunscentedKalmanﬁlters.Webelievemuchoftheanalysisweprovidehere,whichdealswiththeintrinsicrandomnessoftheenvironment,canalsobeappliedtomodellinguncertainty.Ourworkhereisbasedonanumberoffoundationalre-sults,inparticularconcerningalternativeoptimalitycrite-ria.Earlyon,Jaquette(1973)showedthatamomentopti-malitycriterion,whichimposesatotalorderingondistri-butions,isachievableanddeﬁnesastationaryoptimalpol-icy,echoingthesecondpartofTheorem1.Sobel(1982)isusuallycitedastheﬁrstreferencetoBellmanequationsforthehighermoments(butnotthedistribution)ofthere-turn.Chung&Sobel(1987)providesresultsconcerningtheconvergenceofthedistributionalBellmanoperatorintotalvariationdistance.White(1988)studies“nonstandardMDPcriteria”fromtheperspectiveofoptimizingthestate-actionpairoccupancy.Anumberofprobabilisticframeworksforreinforcementlearninghavebeenproposedinrecentyears.Theplan-ningasinferenceapproach(Toussaint&Storkey,2006;Hoffmanetal.,2009)embedsthereturnintoagraphicalmodel,andappliesprobabilisticinferencetodeterminethesequenceofactionsleadingtomaximalexpectedreward.Wangetal.(2008)consideredthedualformulationofre-inforcementlearning,whereoneoptimizesthestationarydistributionsubjecttoconstraintsgivenbythetransitionfunction(Puterman,1994),inparticularitsrelationshiptolinearapproximation.RelatedtothisdualistheCompressandControlalgorithmVenessetal.(2015),whichdescribesavaluefunctionbylearningareturndistributionusingden-sitymodels.OneoftheaimsofthisworkwastoaddressthequestionleftopenbytheirworkofwhetheronecouldbedesignapracticaldistributionalalgorithmbasedontheBellmanequation,ratherthanMonteCarloestimation.B.ProofsLemma1(Partitionlemma).LetA1,A2,...beasetofrandomvariablesdescribingapartitionofΩ,i.e.Ai(ω)∈{0,1}andforanyωthereisexactlyoneAiwithAi(ω)=1.LetU,Vbetworandomvariables.Thendp(cid:0)U,V(cid:1)≤Xidp(AiU,AiV).Proof.Wewillgivetheproofforp<∞,notingthatthesameappliestop=∞.LetYiD:=AiUandZiD:=AiV,respectively.Firstnotethatdpp(AiU,AiV)=infYi,ZiE(cid:2)|Yi−Zi|p(cid:3)=infYi,ZiEhE(cid:2)|Yi−Zi|p|Ai(cid:3)i.Now,|AiU−AiV|p=0wheneverAi=0.ItfollowsthatwecanchooseYi,Zisothatalso|Yi−Zi|p=0wheneverAi=0,withoutincreasingtheexpectednorm.Hencedpp(AiU,AiV)=infYi,ZiPr{Ai=1}E(cid:2)|Yi−Zi|p|Ai=1(cid:3).(8)Next,weclaimthatinfU,VXiPr{Ai=1}Eh(cid:12)(cid:12)AiU−AiV(cid:12)(cid:12)p|Ai=1i(9)≤infY1,Y2,...Z1,Z2,...XiPr{Ai=1}Eh|Yi−Zi(cid:12)(cid:12)p|Ai=1i.Speciﬁcally,theleft-handsideoftheequationisaninﬁ-mumoverallr.v.’swhosecumulativedistributionsareFUandFV,respectively,whiletheright-handsideisanin-ﬁmumoversequencesofr.v’sY1,Y2,...andZ1,Z2,...whosecumulativedistributionsareFAiU,FAiV,respec-tively.Toprovethisupperbound,considerthec.d.f.ofU:FU(y)=Pr{U≤y}=XiPr{Ai=1}Pr{U≤y|Ai=1}=XiPr{Ai=1}Pr{AiU≤y|Ai=1}.ADistributionalPerspectiveonReinforcementLearningHencethedistributionFUisequivalent,inanalmostsuresense,toonethatﬁrstpicksanelementAiofthepartition,thenpicksavalueforUconditionalonthechoiceAi.Ontheotherhand,thec.d.f.ofYiD=AiUisFAiU(y)=Pr{Ai=1}Pr{AiU≤y|Ai=1}+Pr{Ai=0}Pr{AiU≤y|Ai=0}=Pr{Ai=1}Pr{AiU≤y|Ai=1}+Pr{Ai=0}I[y≥0].Thustheright-handsideinﬁmumin(9)hastheadditionalconstraintthatitmustpreservetheconditionalc.d.fs,inparticularwheny≥0.Putanotherway,insteadofhav-ingthefreedomtocompletelyreorderthemappingU:Ω→R,wecanonlyreorderitwithineachelementofthepartition.Wenowwritedpp(U,V)=infU,VkU−Vkp=infU,VE(cid:2)|U−V|p(cid:3)(a)=infU,VXiPr{Ai=1}E(cid:2)|U−V|p|Ai=1(cid:3)=infU,VXiPr{Ai=1}E(cid:2)|AiU−AiV|p|Ai=1(cid:3),where(a)followsbecauseA1,A2,...isapartition.Using(9),thisimpliesdpp(U,V)=infU,VXiPr{Ai=1}Eh(cid:12)(cid:12)AiU−AiV(cid:12)(cid:12)p|Ai=1i≤infY1,Y2,...Z1,Z2,...XiPr{Ai=1}Eh(cid:12)(cid:12)Yi−Zi(cid:12)(cid:12)p|Ai=1i(b)=XiinfYi,ZiPr{Ai=1}Eh(cid:12)(cid:12)Yi−Zi(cid:12)(cid:12)p|Ai=1i(c)=Xidp(AiU,AiV),becausein(b)theindividualcomponentsofthesumareindependentlyminimized;and(c)from(8).Lemma2.¯dpisametricovervaluedistributions.Proof.Theonlynontrivialpropertyisthetriangleinequal-ity.ForanyvaluedistributionY∈Z,write¯dp(Z1,Z2)=supx,adp(Z1(x,a),Z2(x,a))(a)≤supx,a[dp(Z1(x,a),Y(x,a))+dp(Y(x,a),Z2(x,a))]≤supx,adp(Z1(x,a),Y(x,a))+supx,adp(Y(x,a),Z2(x,a))=¯dp(Z1,Y)+¯dp(Y,Z2),wherein(a)weusedthetriangleinequalityfordp.Lemma3.Tπ:Z→Zisaγ-contractionin¯dp.Proof.ConsiderZ1,Z2∈Z.Bydeﬁnition,¯dp(TπZ1,TπZ2)=supx,adp(TπZ1(x,a),TπZ2(x,a)).(10)Bythepropertiesofdp,wehavedp(TπZ1(x,a),TπZ2(x,a))=dp(R(x,a)+γPπZ1(x,a),R(x,a)+γPπZ2(x,a))≤γdp(PπZ1(x,a),PπZ2(x,a))≤γsupx0,a0dp(Z1(x0,a0),Z2(x0,a0)),wherethelastlinefollowsfromthedeﬁnitionofPπ(see(4)).Combiningwith(10)weobtain¯dp(TπZ1,TπZ2)=supx,adp(TπZ1(x,a),TπZ2(x,a))≤γsupx0,a0dp(Z1(x0,a0),Z2(x0,a0))=γ¯dp(Z1,Z2).Proposition1(Sobel,1982).Considertwovaluedistri-butionsZ1,Z2∈Z,andwriteV(Zi)tobethevectorofvariancesofZi.ThenkETπZ1−ETπZ2k∞≤γkEZ1−EZ2k∞,andkV(TπZ1)−V(TπZ2)k∞≤γ2kVZ1−VZ2k∞.Proof.Theﬁrststatementisstandard,anditsprooffollowsfromETπZ=TπEZ,wherethesecondTπdenotestheusualoperatorovervaluefunctions.Now,byindependenceofRandPπZi:V(TπZi(x,a))=V(cid:16)R(x,a)+γPπZi(x,a)(cid:17)=V(R(x,a))+γ2V(PπZi(x,a)).AndnowkV(TπZ1)−V(TπZ2)k∞=supx,a(cid:12)(cid:12)V(TπZ1(x,a))−V(TπZ2(x,a))(cid:12)(cid:12)=supx,aγ2(cid:12)(cid:12)[V(PπZ1(x,a))−V(PπZ2(x,a))](cid:12)(cid:12)=supx,aγ2(cid:12)(cid:12)E[V(Z1(X0,A0))−V(Z2(X0,A0))](cid:12)(cid:12)≤supx0,a0γ2(cid:12)(cid:12)V(Z1(x0,a0))−V(Z2(x0,a0))(cid:12)(cid:12)≤γ2kVZ1−VZ2k∞.Lemma4.LetZ1,Z2∈Z.ThenkETZ1−ETZ2k∞≤γkEZ1−EZ2k∞,andinparticularEZk→Q∗exponentiallyquickly.ADistributionalPerspectiveonReinforcementLearningProof.Theprooffollowsbylinearityofexpectation.WriteTDforthedistributionaloperatorandTEfortheusualop-erator.ThenkETDZ1−ETDZ2k∞=kTEEZ1−TEEZ2k∞≤γkZ1−Z2k∞.Theorem1(Convergenceinthecontrolsetting).LetZk:=TZk−1withZ0∈Z.LetXbemeasurableandsupposethatAisﬁnite.Thenlimk→∞infZ∗∗∈Z∗∗dp(Zk(x,a),Z∗∗(x,a))=0∀x,a.IfXisﬁnite,thenZkconvergestoZ∗∗uniformly.Further-more,ifthereisatotalordering≺onΠ∗,suchthatforanyZ∗∈Z∗,TZ∗=TπZ∗withπ∈GZ∗,π≺π0∀π0∈GZ∗\{π},thenThasauniqueﬁxedpointZ∗∈Z∗.ThegistoftheproofofTheorem1consistsinshowingthatforeverystatex,thereisatimekafterwhichthegreedypolicyw.r.t.Qkismostlyoptimal.Toclearlyexposethestepsinvolved,wewillﬁrstassumeaunique(andthere-foredeterministic)optimalpolicyπ∗,andlaterreturntothegeneralcase;wewilldenotetheoptimalactionatxbyπ∗(x).Fornotationalconvenience,wewillwriteQk:=EZkandGk:=GZk.LetB:=2supZ∈ZkZk∞<∞andlet(cid:15)k:=γkB.WeﬁrstdeﬁnethesetofstatesXk⊆XwhosevaluesmustbesufﬁcientlyclosetoQ∗attimek:Xk:=nx:Q∗(x,π∗(x))−maxa6=π∗(x)Q∗(x,a)>2(cid:15)ko.(11)Indeed,byLemma4,weknowthatafterkiterations|Qk(x,a)−Q∗(x,a)|≤γk|Q0(x,a)−Q∗(x,a)|≤(cid:15)k.Forx∈X,writea∗:=π∗(x).Foranya∈A,wededucethatQk(x,a∗)−Qk(x,a)≥Q∗(x,a∗)−Q∗(x,a)−2(cid:15)k.Itfollowsthatifx∈Xk,thenalsoQk(x,a∗)>Qk(x,a0)foralla06=π∗(x):forthesestates,thegreedypolicyπk(x):=argmaxaQk(x,a)correspondstotheoptimalpolicyπ∗.Lemma5.Foreachx∈Xthereexistsaksuchthat,forallk0≥k,x∈Xk0,andinparticularargmaxaQk(x,a)=π∗(x).Proof.BecauseAisﬁnite,thegap∆(x):=Q∗(x,π∗(x))−maxa6=π∗(x)Q∗(x,a)isattainedforsomestrictlypositive∆(x)>0.Bydeﬁni-tion,thereexistsaksuchthat(cid:15)k=γkB<∆(x)2,andhenceeveryx∈XmusteventuallybeinXk.Thislemmaallowsustoguaranteetheexistenceofaniterationkafterwhichsufﬁcientlymanystatesarewell-behaved,inthesensethatthegreedypolicyatthosestateschoosestheoptimalaction.Wewillcallthesestates“solved”.Weinfactrequirenotonlythesestatestobesolved,butalsomostoftheirsuccessors,andmostofthesuccessorsofthose,andsoon.Weformalizethisnotionasfollows:ﬁxsomeδ>0,letXk,0:=Xk,anddeﬁnefori>0thesetXk,i:=(cid:8)x:x∈Xk,P(Xk−1,i−1|x,π∗(x))≥1−δ(cid:9),Asthefollowinglemmashows,anyxiseventuallycon-tainedintherecursively-deﬁnedsetsXk,i,foranyi.Lemma6.Foranyi∈Nandanyx∈X,thereexistsaksuchthatforallk0≥k,x∈Xk0,i.Proof.FixiandletussupposethatXk,i↑X.ByLemma5,thisistruefori=0.WeinferthatforanyprobabilitymeasurePonX,P(Xk,i)→P(X)=1.Inparticular,foragivenx∈Xk,thisimpliesthatP(Xk,i|x,π∗(x))→P(X|x,π∗(x))=1.Therefore,foranyx,thereexistsatimeafterwhichitisandremainsamemberofXk,i+1,thesetofstatesforwhichP(Xk−1,i|x,π∗(x))≥1−δ.WeconcludethatXk,i+1↑Xalso.Thestatementfollowsbyinduction.ProofofTheorem1.Theproofissimilartopolicyiteration-typeresults,butrequiresmorecareindealingwiththemetricandthepossiblyinﬁnitestatespace.WewillwriteWk(x):=Zk(x,πk(x)),deﬁneW∗similarlyandwithsomeoverloadofnotationwriteTWk(x):=Wk+1(x)=TZk(x,πk+1(x)).Finally,letSki(x):=I[x∈Xk,i]and¯Ski(x)=1−Ski(x).Fixi>0andx∈Xk+1,i+1⊆Xk.WebeginbyusingLemma1toseparatethetransitionfromxintoasolvedtermandanunsolvedterm:PπkWk(x)=SkiWk(X0)+¯SkiWk(X0),whereX0istherandomsuccessorfromtakingactionπk(x):=π∗(x),andwewriteSki=Ski(X0),¯Ski=¯Ski(X0)toeasethenotation.Similarly,PπkW∗(x)=SkiW∗(X0)+¯SkiW∗(X0).ADistributionalPerspectiveonReinforcementLearningNowdp(Wk+1(x),W∗(x))=dp(TWk(x),TW∗(x))(a)≤γdp(PπkWk(x),Pπ∗W∗(x))(b)≤γdp(SkiWk(X0),SkiW∗(X0))+γdp(¯SkiWk(X0),¯SkiW∗(X0)),(12)wherein(a)weusedPropertiesP1andP2oftheWasser-steinmetric,andin(b)weseparatestatesforwhichπk=π∗fromtherestusingLemma1({Ski,¯Ski}formaparti-tionofΩ).Letδi:=Pr{X0/∈Xk,i}=E{¯Ski(X0)}=k¯Ski(X0)kp.FrompropertyP3oftheWassersteinmetric,wehavedp(¯SkiWk(X0),¯SkiW∗(X0))≤supx0dp(¯Ski(X0)Wk(x0),¯Ski(X0)W∗(x0))≤k¯Ski(X0)kpsupx0dp(Wk(x0),W∗(x0))≤δisupx0dp(Wk(x0),W∗(x0))≤δiB.RecallthatB<∞isthelargestattainablekZk∞.Sincealsoδi<δbyourchoiceofx∈Xk+1,i+1,wecanupperboundthesecondtermin(12)byγδB.Thisyieldsdp(Wk+1(x),W∗(x))≤γdp(SkiWk(X0),SkiW∗(X0))+γδB.Byinductiononi>0,weconcludethatforx∈Xk+i,iandsomerandomstateX00istepsforward,dp(Wk+i(x),W∗(x))≤γidp(Sk0Wk(X00),Sk0W∗(X00))+δB1−γ≤γiB+δB1−γ.Henceforanyx∈X,(cid:15)>0,wecantakeδ,i,andﬁnallyklargeenoughtomakedp(Wk(x),W∗(x))<(cid:15).TheproofthenextendstoZk(x,a)byconsideringoneadditionalap-plicationofT.Wenowconsiderthemoregeneralcasewheretherearemultipleoptimalpolicies.WeexpandthedeﬁnitionofXk,iasfollows:Xk,i:=(cid:8)x∈Xk:∀π∗∈Π∗,Ea∗∼π∗(x)P(Xk−1,i−1|x,a∗)≥1−δ(cid:9),Becausethereareﬁnitelymanyactions,Lemma6alsoholdsforthisnewdeﬁnition.Asbefore,takex∈Xk,i,butnowconsiderthesequenceofgreedypoliciesπk,πk−1,...selectedbysuccessiveapplicationsofT,andwriteT¯πk:=TπkTπk−1···Tπk−i+1,suchthatZk+1=T¯πkZk−i+1.NowdenotebyZ∗∗thesetofnonstationaryoptimalpoli-cies.IfwetakeanyZ∗∈Z∗,wededucethatinfZ∗∗∈Z∗∗dp(T¯πkZ∗(x,a),Z∗∗(x,a))≤δB1−γ,sinceZ∗correspondstosomeoptimalpolicyπ∗and¯πkisoptimalalongmostofthetrajectoriesfrom(x,a).Ineffect,T¯πkZ∗isclosetothevaluedistributionofthenonstation-aryoptimalpolicy¯πkπ∗.NowforthisZ∗,infZ∗∗dp(Zk(x,a),Z∗∗(x,a))≤dp(Zk(x,a),T¯πkZ∗(x,a))+infZ∗∗dp(T¯πkZ∗(x,a),Z∗∗(x,a))≤dp(T¯πkZk−i+1(x,a),T¯πkZ∗(x,a))+δB1−γ≤γiB+2δB1−γ,usingthesameargumentasbeforewiththenewly-deﬁnedXk,i.ItfollowsthatinfZ∗∗∈Z∗∗dp(Zk(x,a),Z∗∗(x,a))→0.WhenXisﬁnite,thereexistsaﬁxedkafterwhichXk=X.Theuniformconvergenceresultthenfollows.ToprovetheuniquenessoftheﬁxedpointZ∗whenTse-lectsitsactionsaccordingtotheordering≺,wenotethatforanyoptimalvaluedistributionZ∗,itssetofgreedypoli-ciesisΠ∗.Denotebyπ∗thepolicycomingﬁrstintheor-deringoverΠ∗.ThenT=Tπ∗,whichhasauniqueﬁxedpoint(Section3.3).Proposition4.ThatThasaﬁxedpointZ∗=TZ∗isinsufﬁcienttoguaranteetheconvergenceof{Zk}toZ∗.Weprovidehereasketchoftheresult.Considerasinglestatex1withtwoactions,a1anda2(Figure8).Theﬁrstactionyieldsarewardof1/2,whiletheothereitheryields0or1withequalprobability,andbothactionsareoptimal.Nowtakeγ=1/2andwriteR0,R1,...forthereceivedrewards.Considerastochasticpolicythattakesactiona2withprobabilityp.Forp=0,thereturnisZp=0=11−γ12=1.Forp=1,ontheotherhand,thereturnisrandomandisgivenbythefollowingfractionalnumber(inbinary):Zp=1=R0.R1R2R3···.ADistributionalPerspectiveonReinforcementLearningR = 1/2R = 0 or 1x1a1a2Figure8.Asimpleexampleillustratingtheeffectofanonstation-arypolicyonthevaluedistribution.Asaresult,Zp=1isuniformlydistributedbetween0and2!Infact,notethatZp=0=0.11111···=1.Forsomeintermediaryvalueofp,weobtainadifferentprobabilityofthedifferentdigits,butalwaysputtingsomeprobabilitymassonallreturnsin[0,2].Nowsupposewefollowthenonstationarypolicythattakesa1ontheﬁrststep,thena2fromthereon.Byinspec-tion,thereturnwillbeuniformlydistributedontheinterval[1/2,3/2],whichdoesnotcorrespondtothereturnunderanyvalueofp.ButnowwemayimagineanoperatorTwhichalternatesbetweena1anda2dependingontheex-actvaluedistributionitisappliedto,whichwouldinturnconvergetoanonstationaryoptimalvaluedistribution.Lemma7(SampleWassersteindistance).Let{Pi}beacollectionofrandomvariables,I∈Narandomindexindependentfrom{Pi},andconsiderthemixturerandomvariableP=PI.ForanyrandomvariableQindependentofI,dp(P,Q)≤Ei∼Idp(Pi,Q),andingeneraltheinequalityisstrictand∇Qdp(PI,Q)6=Ei∼I∇Qdp(Pi,Q).Proof.WeprovethisusingLemma1.LetAi:=I[I=i].Wewritedp(P,Q)=dp(PI,Q)=dp(cid:16)XiAiPi,XiAiQ(cid:17)≤Xidp(AiPi,AiQ)≤XiPr{I=i}dp(Pi,Q)=EIdP(Pi,Q).whereinthepenultimatelineweusedtheindependenceofIfromPiandQtoappealtopropertyP3oftheWassersteinmetric.Toshowthattheboundisingeneralstrict,considerthemixturedistributiondepictedinFigure9.Wewillsimplyconsiderthed1metricbetweenthisdistributionPandan-otherdistributionQ.TheﬁrstdistributionisP=(cid:26)0w.p.1/21w.p.1/2.Inthisexample,i∈{1,2},P1=0,andP2=1.Nowconsiderthedistributionwiththesamesupportbutthatputsprobabilitypon0:Q=(cid:26)0w.p.p1w.p.1−p.ThedistancebetweenPandQisd1(P,Q)=|p−12|.Thisisd1(P,Q)=12forp∈{0,1},andstrictlylessthan12foranyothervaluesofp.Ontheotherhand,thecorre-spondingexpecteddistance(aftersamplinganoutcomex1orx2withequalprobability)isEId1(Pi,Q)=12p+12(1−p)=12.Henced1(P,Q)<EId1(Pi,Q)forp∈(0,1).Thisshowsthattheboundisingeneralstrict.Byinspection,itisclearthatthetwogradientsaredifferent.R = 0R = 1xx1x2½½Figure9.ExampleMDPinwhichtheexpectedsampleWasser-steindistanceisgreaterthantheWassersteindistance.Proposition5.Fixsomenext-statedistributionZandpol-icyπ.ConsideraparametricvaluedistributionZθ,andanddeﬁnetheWassersteinlossLW(θ):=dp(Zθ(x,a),R(x,a)+γZ(X0,π(X0))).Letr∼R(x,a)andx0∼P(·|x,a)andconsiderthesamplelossLW(θ,r,x0):=dp(Zθ(x,a),r+γZ(x0,π(x0)).ItsexpectationisanupperboundonthelossLW:LW(θ)≤ER,PLW(θ,r,x0),ingeneralwithstrictinequality.Theresultfollowsdirectlyfromthepreviouslemma.ADistributionalPerspectiveonReinforcementLearning# AtomsWassersteinCategoricalMonte-Carlo TargetStochastic Bellman TargetWassersteinCategoricald1(Z⇡,Z✓)ReturnFZ(a)(b)Figure10.(a)WassersteindistancebetweengroundtruthdistributionZπandapproximatingdistributionsZθ.Varyingnumberofatomsinapproximation,trainingtarget,andlossfunction.(b)ApproximatecumulativedistributionsforﬁverepresentativestatesinCliffWalk.C.AlgorithmicDetailsWhileourtrainingregimecloselyfollowsthatofDQN(Mnihetal.,2015),weuseAdam(Kingma&Ba,2015)insteadofRMSProp(Tieleman&Hinton,2012)forgra-dientrescaling.Wealsoperformedsomehyperparam-etertuningforourﬁnalresults.Speciﬁcally,weeval-uatedtwohyperparametersoverourﬁvetraininggamesandchoosethevaluesthatperformedbest.Thehyperpa-rametervaluesweconsideredwereVMAX∈{3,10,100}and(cid:15)adam∈{1/L,0.1/L,0.01/L,0.001/L,0.0001/L},whereL=32istheminibatchsize.WefoundVMAX=10and(cid:15)adam=0.01/Lperformedbest.Weusedthesamestep-sizevalueasDQN(α=0.00025).Pseudo-codeforthecategoricalalgorithmisgiveninAlgo-rithm1.WeapplytheBellmanupdatetoeachatomsepa-rately,andthenprojectitintothetwonearestatomsintheoriginalsupport.Transitionstoaterminalstatearehandledwithγt=0.D.ComparisonofSampledWassersteinLossandCategoricalProjectionLemma3provesthatforaﬁxedpolicyπthedistributionalBellmanoperatorisaγ-contractionin¯dp,andthereforethatTπwillconvergeindistributiontothetruedistributionofreturnsZπ.Inthissection,weempiricallyvalidatetheseresultsontheCliffWalkdomainshowninFigure11.ThedynamicsoftheproblemmatchthosegivenbySutton&Barto(1998).Wealsostudytheconvergenceofthedistri-butionalBellmanoperatorunderthesampledWassersteinlossandthecategoricalprojection(Equation7)whilefol-The CliﬀSGsafe pathoptimal pathr = -1r = -100Figure11.CliffWalkEnvironment(Sutton&Barto,1998).lowingapolicythattriestotakethesafepathbuthasa10%chanceoftakinganotheractionuniformlyatrandom.Wecomputeaground-truthdistributionofreturnsZπusing10000Monte-Carlo(MC)rolloutsfromeachstate.Wethenperformtwoexperiments,approximatingthevaluedistri-butionateachstatewithourdiscretedistributions.Intheﬁrstexperiment,weperformsupervisedlearningus-ingeithertheWassersteinlossorcategoricalprojection(Equation7)withcross-entropyloss.WeuseZπasthesupervisedtargetandperform5000sweepsoverallstatestoensurebothapproacheshaveconverged.Inthesecondexperiment,weusethesamelossfunctions,butthetrainingtargetcomesfromtheone-stepdistributionalBellmanop-eratorwithsampledtransitions.WeuseVMIN=−100andVMAX=−1.4Forthesampleupdatesweperform10timesasmanysweepsoverthestatespace.Fundamentally,theseexperimentsinvestigatehowwellthetwotrainingregimes4Becausethereisasmallprobabilityoflargernegativereturns,someapproximationerrorisunavoidable.However,thiseffectisrelativelynegligibleinourexperiments.ADistributionalPerspectiveonReinforcementLearning(minimizingtheWassersteinorcategoricalloss)minimizetheWassersteinmetricunderbothideal(supervisedtarget)andpractical(sampledone-stepBellmantarget)conditions.InFigure10aweshowtheﬁnalWassersteindistanced1(Zπ,Zθ)betweenthelearneddistributionsandtheground-truthdistributionaswevarythenumberofatoms.ThegraphshowsthatthecategoricalalgorithmdoesindeedminimizetheWassersteinmetricinboththesupervisedandsampleBellmansetting.ItalsohighlightsthatminimizingtheWassersteinlosswithstochasticgradientdescentisingeneralﬂawed,conﬁrmingtheintuitiongivenbyPropo-sition5.Inrepeatexperimentstheprocessconvergedtodifferentvaluesofd1(Zπ,Zθ),suggestingthepresenceoflocalminima(moreprevalentwithfeweratoms).Figure10providesadditionalinsightintowhythesampledWassersteindistancemayperformpoorly.Here,weseethecumulativedensitiesfortheapproximationslearnedunderthesetwolossesforﬁvedifferentstatesalongthesafepathinCliffWalk.TheWassersteinhasconvergedtoaﬁxed-pointdistribution,butnotonethatcapturesthetrue(MonteCarlo)distributionverywell.Bycomparison,thecategor-icalalgorithmcapturesthevarianceofthetruedistributionmuchmoreaccurately.E.SupplementalVideosandResultsInFigure13weprovidelinkstosupplementalvideosshow-ingtheC51agentduringtrainingonvariousAtari2600games.Figure12showstherelativeperformanceofC51overthecourseoftraining.Figure14providesatableofevaluationresults,comparingC51tootherstate-of-the-artagents.Figures15–18depictparticularlyinterestingframes.# Games SuperiorTraining Frames (millions)C51 vs. DQNC51 vs. HUMANDQN vs. HUMANFigure12.NumberofAtarigameswhereanagent’strainingper-formanceisgreaterthanabaseline(fullytrainedDQN&human).Errorbandsgivestandarddeviations,andaveragesareovernum-berofgames.GAMESVIDEOURLFreewayhttp://youtu.be/97578n9kFIkPonghttp://youtu.be/vIz5P6s80qAQ*Berthttp://youtu.be/v-RbNX4uETwSeaquesthttp://youtu.be/d1yz4PNFUjISpaceInvadershttp://youtu.be/yFBwyPuO2VgFigure13.SupplementalvideosofC51duringtraining.ADistributionalPerspectiveonReinforcementLearningGAMESRANDOMHUMANDQNDDQNDUELPRIOR.DUEL.C51Alien227.87,127.71,620.03,747.74,461.43,941.03,166Amidar5.81,719.5978.01,793.32,354.52,296.81,735Assault222.4742.04,280.45,393.24,621.011,477.07,203Asterix210.08,503.34,359.017,356.528,188.0375,080.0406,211Asteroids719.147,388.71,364.5734.72,837.71,192.71,516Atlantis12,850.029,028.1279,987.0106,056.0382,572.0395,762.03,692,500BankHeist14.2753.1455.01,030.61,611.91,503.1976BattleZone2,360.037,187.529,900.031,700.037,150.035,520.028,742BeamRider363.916,926.58,627.513,772.812,164.030,276.514,074Berzerk123.72,630.4585.61,225.41,472.63,409.01,645Bowling23.1160.750.468.165.546.781.8Boxing0.112.188.091.699.498.997.8Breakout1.730.5385.5418.5345.3366.0748Centipede2,090.912,017.04,657.75,409.47,561.47,687.59,646ChopperCommand811.07,387.86,126.05,809.011,215.013,185.015,600CrazyClimber10,780.535,829.4110,763.0117,282.0143,570.0162,224.0179,877Defender2,874.518,688.923,633.035,338.542,214.041,324.547,092DemonAttack152.11,971.012,149.458,044.260,813.372,878.6130,955DoubleDunk-18.6-16.4-6.6-5.50.1-12.52.5Enduro0.0860.5729.01,211.82,258.22,306.43,454FishingDerby-91.7-38.7-4.915.546.441.38.9Freeway0.029.630.833.30.033.033.9Frostbite65.24,334.7797.41,683.34,672.87,413.03,965Gopher257.62,412.58,777.414,840.815,718.4104,368.233,641Gravitar173.03,351.4473.0412.0588.0238.0440H.E.R.O.1,027.030,826.420,437.820,130.220,818.221,036.538,874IceHockey-11.20.9-1.9-2.70.5-0.4-3.5JamesBond29.0302.8768.51,358.01,312.5812.01,909Kangaroo52.03,035.07,259.012,992.014,854.01,792.012,853Krull1,598.02,665.58,422.37,920.511,451.910,374.49,735Kung-FuMaster258.522,736.326,059.029,710.034,294.048,375.048,192Montezuma’sRevenge0.04,753.30.00.00.00.00.0Ms.Pac-Man307.36,951.63,085.62,711.46,283.53,327.33,415NameThisGame2,292.38,049.08,207.810,616.011,971.115,572.512,542Phoenix761.47,242.68,485.212,252.523,092.270,324.317,490Pitfall!-229.46,463.7-286.1-29.90.00.00.0Pong-20.714.619.520.921.020.920.9PrivateEye24.969,571.3146.7129.7103.0206.015,095Q*Bert163.913,455.013,117.315,088.519,220.318,760.323,784RiverRaid1,338.517,118.07,377.614,884.521,162.620,607.617,322RoadRunner11.57,845.039,544.044,127.069,524.062,151.055,839Robotank2.211.963.965.165.327.552.3Seaquest68.442,054.75,860.616,452.750,254.2931.6266,434Skiing-17,098.1-4,336.9-13,062.3-9,021.8-8,857.4-19,949.9-13,901Solaris1,236.312,326.73,482.83,067.82,250.8133.48,342SpaceInvaders148.01,668.71,692.32,525.56,427.315,311.55,747StarGunner664.010,250.054,282.060,142.089,238.0125,117.049,095Surround-10.06.5-5.6-2.94.41.26.8Tennis-23.8-8.312.2-22.85.10.023.1TimePilot3,568.05,229.24,870.08,339.011,666.07,553.08,329Tutankham11.4167.668.1218.4211.4245.9280UpandDown533.411,693.29,989.922,972.244,939.633,879.115,612Venture0.01,187.5163.098.0497.048.01,520VideoPinball16,256.917,667.9196,760.4309,941.998,209.5479,197.0949,604WizardOfWor563.54,756.52,704.07,492.07,855.012,352.09,300Yars’Revenge3,092.954,576.918,098.911,712.649,622.169,618.135,050Zaxxon32.59,173.35,363.010,163.012,944.013,886.010,513Figure14.Rawscoresacrossallgames,startingwith30no-opactions.ReferencevaluesfromWangetal.(2016).ADistributionalPerspectiveonReinforcementLearningFigure15.FREEWAY:Agentdifferentiatesaction-valuedistributionsunderpressure.Figure16.Q*BERT:Top,leftandright:Predictingwhichactionsareunrecoverablyfatal.Bottom-Left:Valuedistributionshowssteepconsequencesforwrongactions.Bottom-Right:Theagenthasmadeahugemistake.Figure17.SEAQUEST:Left:Bimodaldistribution.Middle:Mighthittheﬁsh.Right:Deﬁnitelygoingtohittheﬁsh.Figure18.SPACEINVADERS:Top-Left:Multi-modaldistributionwithhighuncertainty.Top-Right:Subsequentframe,amorecertaindemise.Bottom-Left:Cleardifferencebetweenactions.Bottom-Middle:Uncertainsurvival.Bottom-Right:Certainsuccess.