Scalable Generative Models for Multi-label Learning with Missing Labels

Vikas Jain 1 * Nirbhay Modhe 1 * Piyush Rai 1

Abstract
We present a scalable, generative framework for
multi-label learning with missing labels. Our
framework consists of a latent factor model for
the binary label matrix, which is coupled with
an exposure model to account for label missing-
ness (i.e., whether a zero in the label matrix is
indeed a zero or denotes a missing observation).
The underlying latent factor model also assumes
that the low-dimensional embeddings of each la-
bel vector are directly conditioned on the respec-
tive feature vector of that example. Our gener-
ative framework admits a simple inference pro-
cedure, such that the parameter estimation re-
duces to a sequence of simple weighted least-
square regression problems, each of which can be
solved easily, efﬁciently, and in parallel. More-
over, inference can also be performed in an on-
line fashion using mini-batches of training ex-
amples, which makes our framework scalable for
large data sets, even when using moderate com-
putational resources. We report both quantita-
tive and qualitative results for our framework on
several benchmark data sets, comparing it with a
number of state-of-the-art methods.

1. Introduction

Multi-label learning (Gibaja & Ventura, 2015; 2014) is the
problem of assigning to an object a subset of labels from a
potentially very large label vocabulary (Prabhu & Varma,
2014; Jain et al., 2016; Babbar & Sch¨olkopf, 2017).
In
contrast to binary or multi-class classiﬁcation, in multi-
label learning, each example is associated with a binary
label vector (potentially very large), denoting the pres-
ence/absence (relevance/irrelevance) of each label. Multi-
label learning has applications in several domains such as
computer vision (Wang et al., 2016), computational adver-

*Equal contribution

1Department of Computer Science and
Enginerring, IIT Kanpur, Kanpur 208016, UP, India. Corre-
spondence to: Vikas Jain <vikasj@iitk.ac.in>, Nirbhay Modhe
<nirbhaym@iitk.ac.in>, Piyush Rai <piyush@cse.iitk.ac.in>

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

tising and recommender systems (Prabhu & Varma, 2014;
Jain et al., 2016), etc.

Several state-of-the-art methods for multi-label learning are
based on certain structural assumptions on the binary la-
bel matrix. Some of the key structural assumptions that
have been used in prior work include low-rank assump-
tion (Yu et al., 2014), locally low-rank assumption (Bha-
tia et al., 2015), and low-rank plus sparse assumption (Xu
et al., 2016), and clusters/topics of labels assumption (Ciss´e
et al., 2016; Rai et al., 2015). Models based on these as-
sumptions are broadly dubbed as embedding based meth-
ods for multi-label learning and offer two key advantages:
(1) The relatedness/correlation among labels can be easily
modeled/captured, and (2) the label vector for each exam-
ple can be represented as a low-dimensional embedding,
which faciliates developing computationally scalable mod-
els for multi-label learning. A more detailed discussion of
prior work is provided in the Related Work section.

Despite the considerable recent interest and progress on
the problem of multi-label learning (Yu et al., 2014; Bhatia
et al., 2015; Wang et al., 2016; Ciss´e et al., 2016), a number
of important issues still remain. One of such issues, espe-
cially for the embdding based methods, is the ambiguity
regarding the zeros vs unobserved (missing) entries in the
binary label vector of each example. Since, in practice, the
true value (0/1) for only a small subset of all the labels can
be obtained, the zeros in the label vector do not necessarily
represent negative labels. A typical heuristic employed by
multi-label learning algorithms is to simply treat all such
the zeros in the label vector as are true negatives (Yu et al.,
2014). Another heuristic is to assign different weights to
the zeros and ones in the binary label matrix (Yu et al.,
2017), which is inspired by matrix factorization based col-
laborative ﬁltering models that learn from implicit (binary)
feedback data (Hu et al., 2008). However, a more princi-
pled strategy to address this issue is highly desirable.

Another important desideratum is scalability, especially in
the case of extreme multi-label learning problems (Prabhu
& Varma, 2014; Jain et al., 2016; Babbar & Sch¨olkopf,
2017), which are characterized by a massive number of la-
bels, features, and examples. Although a number of recent
multi-label learning models have been proposed that can
scale to large-scale problems, these models usually require

Scalable Generative Models for Multi-label Learning with Missing Labels

suming that the prior distribution of un is conditioned on
xn, as p(un|xn) = N (un|Wxn, λ−1
u IK). Here, W =
[w1, . . . , wK](cid:62) ∈ RK×D which denotes the matrix of re-
gression weights that map the feature vector xn to the mean
of the Gaussian prior on un. We further assume a zero-
mean Gaussian prior p(v(cid:96)) = N (v(cid:96)|0, λ−1
v IK) on label
latent factors v(cid:96), (cid:96) = 1, . . . , L. Note that, although we do
not consider it here, our model can also be easily extended
to incorporate label features (if available) by conditioning
Gaussian prior on v(cid:96) on those label features, in the same
manner we condition the prior on un on input features.

large computational resources to truly scale to massive data
sets (Babbar & Sch¨olkopf, 2017; Bhatia et al., 2015; Jain
et al., 2016). Moreover, most of the scalable multi-label
learning algorithms only operate in batch setting and are
usually not designed to work (Prabhu & Varma, 2014; Bha-
tia et al., 2015; Jain et al., 2016) in online settings with
continuous stream of training examples.

In this paper, we present a scalable, generative framework
for multi-label learning, that not only bring to bear the
modeling ﬂexibility of probabilistic, generative models for
the multi-label learning problem (Kapoor et al., 2012; Rai
et al., 2015), but is also designed to handle the above-
mentioned challenges in a principled way. Our framework
is based on a latent factor model for the binary label matrix,
and has the following distinguishing aspects: (1) It natu-
rally handles the issue of missing vs negative labels via a
principled generative model with a exposure model (Liang
et al., 2016) for the label matrix; (2) It is accompanied by
a simple and scalable inference procedure (both via Gibbs
sampling and via fast point estimation); and (3) Inference
can also be easily performed in an online fashion, enabling
us to apply it on large-scale problems, even when using
moderate computational resources.

2. The Model

In the multi-label learning problem, we assume that we are
given N training examples {(x1, y1), . . . , (xN , yN )} with
xn ∈ RD and yn ∈ {0, 1}L, n = 1, . . . , N . We will de-
note X = {x1, . . . , xN } ∈ RN ×D to be the feature matrix
and Y = {y1, . . . , yN } ∈ {0, 1}N ×L to be the label ma-
trix. Given training data {X, Y}, the goal in multi-label
learning is to learn a model that can predict the label vector
y∗ ∈ {0, 1}L for a new test input x∗ ∈ RD.
Note that an entry yn(cid:96) = 0 in the label matrix Y may not
necessarily mean a negative label but could simply mean
that this label is missing (and its true value could be 0 or 1).
As we shall show, our generative model can infer the miss-
ingness of a label yn(cid:96) = 0 by associating another binary
latent variable ξn(cid:96) (called exposure variable). These expo-
sure variables will be incorporated in a latent factor model
(Sec. 2.1) for the label matrix Y and are jointly learned
along with the rest of the model parameters.

2.1. An Exposure-based Latent Factor Model for the

Binary Label Matrix

We model the binary label matrix Y using a latent fac-
tor model. Speciﬁcally, we assume that each training ex-
ample n = 1, . . . , N is associated with a latent factor
un ∈ RK and each label (cid:96) = 1, . . . , L is associated
with a latent factor v(cid:96) ∈ RK. We further condition un
on the feature vector xn ∈ RD of example n by as-

Figure 1. Our generative model in plate notation. Note: Hyperpa-
rameters not shown for brevity.

The complete generative story for each label yn(cid:96) of the bi-
nary label matrix Y is given by

un|xn ∼ N (un|Wxn, λ−1
v(cid:96) ∼ N (v(cid:96)|0, λ−1
v IK)
ξn(cid:96) ∼ Bernoulli(µn(cid:96))

u IK)

yn(cid:96) ∼

(cid:40)

Bernoulli (cid:0)yn(cid:96)|σ(u(cid:62)
if ξn(cid:96) = 0
δ0,

n v(cid:96))(cid:1) ,

if ξn(cid:96) = 1

(1)

(2)

(3)

(4)

where σ(z) = 1/(1 + exp(−z)) denotes the logistic func-
tion. Note that we have associated a binary exposure la-
tent variable ξn(cid:96) with each label yn(cid:96) such that ξn(cid:96) = 0
implies that yn(cid:96) is 0 because it is missing (not exposed),
and ξn(cid:96) = 1 implies that yn(cid:96) is exposed (and could be 0
or 1 depending on the outcome of the Bernoulli draw). In
Eq. 4, δ0 denotes a point-mass at zero, which means that, if
ξn(cid:96) = 0, then yn(cid:96) is zero with probability 1. Otherwise, we
draw the observed label yn(cid:96) from a Bernoulli distribution
as yn(cid:96) ∼ Bernoulli(yn(cid:96)|σ(u(cid:62)
n v(cid:96))). Note that, effectively,
each yn(cid:96) is being modeled using a mixture of two distri-
butions - a Bernoulli with probability given by the sigmoid
σ(u(cid:62)

n v(cid:96))) and a point-mass at 0.

yn(cid:96) ∼ ξn(cid:96)Bern (cid:0)yn(cid:96)|σ(u(cid:62)

n v(cid:96))(cid:1) + (1 − ξn(cid:96))I[yn(cid:96) = 0] (5)

Note that the latent variable ξn(cid:96) decides which of the two
distributions from this mixture generates yn(cid:96). Figure 1
shows our model in the plate notation. Also note that if
yn(cid:96) = 1 then ξn(cid:96) = 1 with probability 1 and therefore ξn(cid:96)
only needs to be inferred for entries for which yn(cid:96) = 0.

Scalable Generative Models for Multi-label Learning with Missing Labels

The generative model speciﬁed in Eq (1)-(4) has two addi-
tional parameters: W = [w1, . . . , wK](cid:62) ∈ RK×D which
denotes the matrix of regression weights that map each
input feature vector xn to the corresponding latent factor
un ∈ RK, and a probability parameter µn(cid:96) ∈ (0, 1) which
denotes the probability of the label yn(cid:96) being exposed (but
note that yn(cid:96) can be 0 or 1, depending on the outcome of
Bernoulli(yn(cid:96)|σ(u(cid:62)
n v(cid:96)))). We refer to µn(cid:96) as the exposure
probability of label (cid:96) for example n.

We assume each regression weight vector wk to have a
Gaussian prior, i.e., wk ∼ N (wk|0, λ−1
w ID). Note that
the spherical covariance of this prior can also be replaced
by a more ﬂexible diagonal covariance, which will give the
model ability to perform feature selection.

For the exposure probability µn(cid:96), we consider two types of
priors. In the ﬁrst case, we simply assume µn(cid:96) = µ(cid:96), ∀n,
which means that the probability that a label (cid:96) is observed
is the same for all the examples (i.e., the label exposure for
the label (cid:96) is global, not example speciﬁc). In this case,
we assume a Beta prior on µ(cid:96), i.e., µ(cid:96) ∼ Beta(α1, α2).
In the second case, we assume access to some contexual
information (often available in applications such as recom-
mender systems) that we may have for each example-label
pair (n, (cid:96)), in form of some given covariates φn(cid:96) ∈ RM .
Given these covariates, we model the label exposure prob-
ability as µn(cid:96) = σ(β(cid:62)φn(cid:96)), where β ∈ RM is a vector of
regression coefﬁcients. We assume a Gaussian prior on β,
i.e., β ∼ N (β|0, λ−1

β IM )

3. Inference

Although the generative model speciﬁed in Eq. 1-4 is not
readily conjugate because the logistic-Bernoulli likelihood
is not conjugate to the Gaussian prior on the latent fac-
tors, we can leverage data-augmentation techniques (Pol-
son et al., 2013) to make the model locally conjugate. This
enables us to develop a simple Gibbs sampling algorithm
for doing inference in our model. The conjugacy also al-
lows us to design an online expectation maximization (EM)
algorithm (Capp´e & Moulines, 2009), which enables us to
apply our model on large-scale problems.

We handle the non-conjugate logistic-Bernoulli likelihood
using the P´olya-gamma augmentation technique (Polson
et al., 2013), which is based on the following identity

(cid:90) ∞

(exp(ψ)a

0

exp (cid:0)−ωψ2/2(cid:1) p(ω)dω

(1 + exp(ψ))b = 2−b exp (κψ)
where κ = a − b/2 and p(ω) = PG(b, 0) denotes
the P´olya-gamma distribution (Polson et al., 2013). This
identity allows us to write any likelihood of the form
(exp(ψ)a
(1+exp(ψ))b (e.g., Bernoulli, binomial, negative-binomial)
as a Gaussian distribution, when conditioned on a PG ran-
dom variable ω|ψ ∼ PG(b, ψ). Speciﬁcally, using PG aug-
mentation, we can write the logistic-Bernoulli likelihood

from Eq. 4 as a Gaussian when conditioned on ωn(cid:96) ∼
PG(1, u(cid:62)
n v(cid:96), conditioned on
ωn(cid:96), becomes a Gaussian
(cid:19)

n v(cid:96)). In particular, ψn(cid:96) = u(cid:62)

(cid:18)

p(ψn(cid:96)|ωn(cid:96)) ∝ exp

κn(cid:96)ψn(cid:96) −

(6)

1
2

ωn(cid:96)ψ2
n(cid:96)

where κn(cid:96) = yn(cid:96) − 0.5. This likelihood with the Gaussian
priors on the latent factors un and v(cid:96) results in Gaussian
posteriors on un and v(cid:96). When doing EM, this also leads to
subproblems that are like least square regression problems.

3.1. Gibbs Sampling

Using the PG augmentation, we can derive the posterior
distributions of all the latent variables in our model, and
perform Gibbs sampling for doing inference in our model.
Due to conjugacy, the inference updates are straightforward
to derive as are summarized below.

Sampling ξn(cid:96): Note that if yn(cid:96) = 1 then ξn(cid:96) = 1 with
probability one, and therefore need not be inferred. For
yn(cid:96) = 0, we sample ξn(cid:96) from the posterior

p(ξn(cid:96) = 1|.) ∝ µn(cid:96)σ(−u(cid:62)
n v(cid:96))
p(ξn(cid:96) = 0|.) ∝ (1 − µn(cid:96)) × 1

(7)

(8)

Sampling µn(cid:96): For the case when µn(cid:96) = µ(cid:96), ∀n, with
Beta(α1, α2) prior on each µ(cid:96), the posterior will be

p(µn(cid:96)|.) = Beta(α1 +

ξn(cid:96), α2 + N −

ξn(cid:96))

(9)

N
(cid:88)

n=1

N
(cid:88)

n=1

Note that, if we parameterize each µn(cid:96) as µn(cid:96) = σ(β(cid:62)φn(cid:96))
where φn(cid:96) is the interaction feature vector for the example-
label pair, and the regresssion weight β is assumed to have
a Gaussian prior, the model is not conjugate. However,
using the PG augmentation allows us to easily derive a
closed-form Gaussian posterior for β.

Sampling un: Given the PG variables Ωn,: = {ωn(cid:96)}L
(cid:96)=1
and the other latent variables, the posterior of un will be
un ∼ N (un|µun , Σun ) where the covariance is given by
Σun = ((cid:80)L
(cid:96) + λuIK)−1 and the mean is
(cid:96)=1 ξn(cid:96)ωn(cid:96)v(cid:96)v(cid:62)
given by µun = Σun((cid:80)L
(cid:96)=1 ξn(cid:96)κn(cid:96)v(cid:96) + λuWxn). Note
that if a label (cid:96) is inferred as not exposed for example n,
i.e., ξn(cid:96) = 0, it does not contribute to the update of un.
Sampling v(cid:96): Given Ω:,(cid:96) = {ωn(cid:96)}N
n=1 and the other latent
, Σv(cid:96))
variables, the posterior v(cid:96) will be v(cid:96) ∼ N (v(cid:96)|µv(cid:96)
where covariance Σv(cid:96) = ((cid:80)N
n + λvIK)−1
n=1 ξn(cid:96)κn(cid:96)un). Note that
and the mean µv(cid:96)
if an example n is inferred as not exposed to label (cid:96), i.e.,
ξn(cid:96) = 0, it does not contribute to the update of v(cid:96).
Sampling W: Each row {wk}K
k=1 of the regression
weights matrix W = [w1, . . . , wK](cid:62) ∈ RK×D will have
a Gaussian posterior given by wk ∼ N (wk|µwk
, Σwk )
where covariance Σwk = (X(cid:62)X + λwID)−1, the mean
= Σwk (X(cid:62)U), and U = [u1, . . . , uN ] ∈ RK×N .
µwk

n=1 ξn(cid:96)ωn(cid:96)unu(cid:62)

= Σv(cid:96) ((cid:80)N

Scalable Generative Models for Multi-label Learning with Missing Labels

3.2. Scalable Inference via EM and Online EM

Although the Gibbs sampler (Sec. 3.1) is easy to derive and
implement in practice, sampling tends to be slow in prac-
tice and convergence may be slow. We therefore present
an online expectation maximization algorithm (Capp´e &
Moulines, 2009) for doing efﬁcient inference in our model.
We ﬁrst show the batch EM updates for our model param-
eters and then describe the online EM algorithm which can
process the training data in small mini-batches of examples,
and results in faster convergence in practice.

3.2.1. THE EM ALGORITHM

The EM algorithm for our model alternates between com-
puting the expectations of the local latent variables, namely
the P´olya-gamma variables {ωn(cid:96)} and the binary exposure
latent variables {ξn(cid:96)} in the E step, and then using these ex-
pectations to estimate the other model parameters un, v(cid:96),
W, and exposure probabilities {µn(cid:96)} in the M step.

The E Step: The E step involves computing the expecta-
tions of the latent variables {ωn(cid:96)} and {ξn(cid:96)}, given the cur-
rent values of the other model parameters un, v(cid:96), W, and
µn(cid:96) estimated in the previous M step. The E step update
equations are given below:

• Expectations of P´olya-gamma variables {ωn(cid:96)}, ∀n, (cid:96)
are known to be available in closed form (Scott & Sun,
2013), and are given by

ηn(cid:96) = E[ωn(cid:96)|ψn(cid:96)] =

1
2ψn(cid:96)

tanh

(cid:19)

(cid:18) ψn(cid:96)
2

(10)

where ψn(cid:96) = u(cid:62)
of un and vm from the previous M step.

n v(cid:96) is computed using the estimates

• Expectations of each of the binary exposure variables

ξn(cid:96), ∀n, (cid:96), are given by

pn(cid:96) = E[ξn(cid:96)|ψn(cid:96)] =

µn(cid:96)σ(−ψn(cid:96))
µn(cid:96)σ(−ψn(cid:96)) + (1 − µn(cid:96))

(11)

The M Step: Given the expectations of the latent vari-
ables computed in the E step, the M step maximizes the
following expected complete data log-likelihood plus log-
prior terms, which we denote as Q(U, V, W, µ), where
U = {un}N
(cid:96)=1, W, and µ = {µn(cid:96)}, ∀n, (cid:96)

n=1, V = {v(cid:96)}L

Q(U, V, W, µ) = −

(κn(cid:96) − ηn(cid:96)u(cid:62)

n v(cid:96))2

1
2

(cid:88)

n,(cid:96)

pn(cid:96)

log Bernoulli(pn(cid:96)|µn(cid:96)) − λu

||un − Wxn||2

− λv

||v(cid:96)||2 − λw||W||2 +

log Beta(µn(cid:96)|α1, α2)

(12)

ηn(cid:96)

N
(cid:88)

n=1

(cid:88)

n,(cid:96)

(cid:88)

+

n,(cid:96)

L
(cid:88)

(cid:96)=1

Note that the ﬁrst term in the objective function given in
Eq. 12 is due to the logistic likelihood transformed into a
Gaussian (using PG augmentation). This term is akin to
a weighted least squares objective where each label being

associated with a weight pn(cid:96) = E[ξn(cid:96)|ψn(cid:96)]. Intuitively, in
the ﬁrst term, the contribution of each label yn(cid:96) to the log-
likelihood gets modulated based on its expected exposure.

Maximizing Q(U, V, W, µ) w.r.t. each of the model pa-
rameters U, V, W, µ, ﬁxing the rest, yields closed-form
updates for each of these. The updates are as follows:
• Estimating each of the latent factors {un}N

n=1 is a

weighted ridge-regression problem with solution

(cid:32) L
(cid:88)

(cid:33)

un = Σun

pn(cid:96)κn(cid:96)v(cid:96) + λuWxn

(13)

(cid:96)=1
(cid:96)=1 pn(cid:96)ηn(cid:96)v(cid:96)v(cid:62)

where Σun = ((cid:80)L
that the updates for {un}N
each other and are easily parallelizable.

(cid:96) + λuIK)−1. Note
n=1 are all independent of

• Estimating each of the label latent factors {v(cid:96)}L
a weighted ridge-regression problem with solution
(cid:32) N
(cid:88)

(cid:33)

(cid:96)=1 is

v(cid:96) = Σv(cid:96)

pn(cid:96)κn(cid:96)un

n=1

(cid:16)(cid:80)N

n=1 pn(cid:96)ηn(cid:96)unu(cid:62)
n + λvIK
where Σv(cid:96) =
.
Again, note that the updates for {vn}L
(cid:96)=1 are all in-
dependent of each other and are easily parallelizable.

(14)

(cid:17)−1

• Estimating the regression weight matrix W is equiva-
lent to solving a vector-valued linear regression prob-
lem un ≈ Wxn, ∀n, with the following updates

(15)

W(cid:62) = (X(cid:62)X + λwID)−1(X(cid:62)U)
Note that solving Eq. (15) exactly requires inverting a
D × D matrix which will be expensive for large D.
However, the EM algorithm does not require solving
for W exactly in each M step. We therefore solve
for W efﬁcient using gradient based methods, such
as conjugate-gradient (CG) method (Bertsekas, 1999),
which allows us to also leverage the sparsity in the
feature matrix X. Typically, a small number of CG
iterations are sufﬁcient in practice.

• Given pn(cid:96) from the E step, the updates for µn(cid:96) for the
case when µn(cid:96) = µ(cid:96), ∀n, is simply the MAP solution

µ(cid:96) =

α1 + (cid:80)N
n=1 pn(cid:96) − 1
α1 + α2 + N − 2

(16)

For the other case when each µn(cid:96) in modeled as µn(cid:96) =
σ(β(cid:62)φn(cid:96)) with a Gaussian prior on β, estimating β
reduces to solving a regression problem with the train-
ing data being {φn(cid:96), pn(cid:96)}, ∀n, (cid:96), where φn(cid:96) is the
given feature vector for the input-label pair n, (cid:96) and
pn(cid:96) is estimated in the E step. Ignoring the prior term
(equivalent to (cid:96)2 regularizer on β), we can estimate β
iteratively using gradient-descent updates

β = β −

(σ(β(cid:62)φn(cid:96)) − pn(cid:96))φn(cid:96)

(17)

τ
N L

(cid:88)

n,(cid:96)

where τ denotes the learning rate.

Scalable Generative Models for Multi-label Learning with Missing Labels

3.2.2. ONLINE EM

4. Related Work

The EM algorithm described in Section 3.2.1 is more efﬁ-
cient than the Gibbs sampler described in Section 3.1. It is
also highly parallelizable since the updates for {u}N
n=1 and
{v(cid:96)}L
(cid:96)=1 can be easily parallelized, and solve for W efﬁ-
ciently using CG updates. However, it is a batch procedure
and requires going over the entire training data in every iter-
ation. For large-scale multi-label learning problems, which
are characterized by large N , D, and L, the batch setting
may not be feasible in practice, especially when having ac-
cess to moderate computational resources and storage.

We therefore present an efﬁcient online version of the EM
algorithm for our model which allows it to scale up to
massive-sized data sets even on machines with moderate
hardware. As we show in our experiments, this enables us
to apply our model to be run efﬁciently on massive data
sets (e.g., one of the data sets we experiment with has more
than 600k examples with about 50k features per example)
even on a standard laptop with very moderate hardware.

The online EM algorithm works by maintaining sufﬁcient
statistics of all the model parameters and updates these
sufﬁcient statistics with every mini-batch of data. For
each mini-batch of training examples, the E step computes
the relevant expectations associated with these observa-
tions and then uses the expectations to update the sufﬁcient
statistics of the parameters to be estimated in the M step.

For example, noting that the sufﬁcient statistics for updat-
ing the label latent factors v(cid:96) = A−1b are given by A =
n +λvIK and b = (cid:80)N
(cid:80)N
n=1 pn(cid:96)κn(cid:96)un, we
can update A and b using a small mini-batch containing Nb
examples as {(xn, yn)}Nb

n=1 pn(cid:96)ηn(cid:96)unu(cid:62)

n=1 as follows

A(t+1) = (1 − γt)A(t) + γtA(new)
b(t+1) = (1 − γt)b(t) + γtb(new)

(18)

(19)

n=1 pn(cid:96)ηn(cid:96)unu(cid:62)

where A(new) = ((cid:80)Nb
n + λvIK), and
b(new) = (cid:80)Nb
n=1 pn(cid:96)κn(cid:96)un are computing using only the
current mini-batch. The sufﬁcient statistics of the other
model parameters can also be updated in the same man-
ner. Here γt is a decaying learning rate (or a forgetting
factor), which also acts as a trade-off between the contri-
bution from the old sufﬁcient statistics computed thus far
and the sufﬁcient statistics contribution from the new mini-
batch of data. We set γt = (a0 + t)−(cid:15) with a0 = 1 and (cid:15) to
be close to 0.5 (Capp´e & Moulines, 2009).
3.2.3. PREDICTION

Given a new test input x∗, we ﬁrst predict its latent factor
u∗ ∈ RK as Wx∗ and then predict each entry of its label
vector y∗ as E[y∗(cid:96)|u∗, v(cid:96)] = σ(u(cid:62)
If we are only
interested in the top few labels, fast search methods such as
maximum inner product search (Fraccaro et al., 2016) can
be used to reduce the computational cost at test time.

∗ v(cid:96)).

A prominent line of work on multi-label learning has been
based on models that learn a low-dimensional embedding
of the label vectors (Chen & Lin, 2012; Yu et al., 2014; Rai
et al., 2015; Bhatia et al., 2015). Note that this amounts to
assuming that the label matrix is low-rank.

Since many real-world data sets have a large number of
rare labels, sometimes the low-rank assumption may not
be appropriate. To address this issue, (Bhatia et al., 2015)
proposed a method which assumes the label matrix to be
locally low-rank. One way to impose this assumption is to
learn embeddings that only try to preserve distances in a
small neighborhood of each example. Another approach to
handle the rare labels is to assume that the label matrix is a
sum of a low-rank and a sparse matrix (Xu et al., 2016).

Note that our latent factor model is equivalent to imposing
a low-rank assumption on the label matrix, and is there-
fore similar in spirit to the label-embedding approaches.
However, unlike the existing label-embedding based ap-
proaches, our generative framework has a principled mech-
anism to handle/infer the unobserved labels. Moreover,
none of the existing label-embedding methods can work
in online fashion, and scaling up these methods to large-
scale problems requires large computational resources. In
addition, our model readily allows incorporating the label
features (if available) by a simple modiﬁcation to the prior
on the label latent factors.

Apart from the label-embedding based multi-label learn-
tree-based methods for multi-label learn-
ing methods,
ing (Agrawal et al., 2013; Prabhu & Varma, 2014; Jain
et al., 2016) are also popular due to being fast at test time,
especially when the number of labels is large. However,
these models usually have high training costs and cannot
be trained easily in an online fashion, unlike our model.
On the other hand, for faster predictions at test time, our
framework model can easily be adapting by replacing the
Gaussian prior on the label latent factors v(cid:96) by a von Mises-
Fisher prior (Fraccaro et al., 2016), which naturally fa-
cilitates using maximum inner-product search techniques,
without the requirement of any post-processing.

Among other models to address the missing labels prob-
lem in multi-label learning, recently, (Kanehira & Harada,
2016) proposed a ranking based framework for learning
from positive and unlabeled data in the context of multi-
label learning. Although this is similar in spirit to our
model in terms of not treating the unobserved labels as ze-
ros, the approach in (Kanehira & Harada, 2016) is funda-
mentally different than ours. Moreover, their setting is not
amenable to online learning, nor does it leverage the low-
rank structure of label matrices with a huge number of la-
bels. Other approaches that try to handle missing labels in-

Scalable Generative Models for Multi-label Learning with Missing Labels

clude (Bucak et al., 2011) which uses group LASSO adap-
tation of a multi-label ranking objective, and (Kong et al.,
2014), which learns a model using a positive and unlabeled
(PU) stochastic gradient descent procedure. However, it
works in batch setting, uses stacking to leverage label cor-
relations, and does not scale to large number of labels.

such as online EM for our model, which enables updating
the model whenever fresh training data is available. This is
in contrast to some of the other state-of-the-art multi-label
learning methods, which although scalable (Bhatia et al.,
2015; Prabhu & Varma, 2014; Jain et al., 2016), are not
suitable to be applied in such online settings.

One-class matrix factorization (OCMF) is also an ap-
proach (Yu et al., 2017) to solve the missing labels prob-
lem by assigning different (but ﬁxed) weights to the ones
and zeros. In contrast to this method, our generative frame-
work can learn the weight for each label by modeling these
weights as latent variables. In another recent work, (Liang
et al., 2016) proposed an exposure model for recommender
system problems posed as matrix factorization of implicit
feedback data. Their approach of modeling the exposure
similar in spirit to our framework.

Some of the early works on generative models for multi-
label learning problems include models speciﬁcally de-
signed for image annotation problems (Barnard et al., 2003;
Feng et al., 2004). Other recent attempts on doing multi-
label learning in more general problem settings include
models such as Bayesian compressive sensing (Kapoor
et al., 2012) and multi-label learning using Bayesian non-
negative matrix factorization (Rai et al., 2015). However,
these models do not have a mechanism to distinguish be-
tween unobserved and negative labels, have complicated
inference, and do not scale to large-scale problems.

Our generative framework is also amenable for various in-
teresting extensions. For example, it can be be extended
to a mixture of latent factor models, which can handle the
situation when the label matrix is not low-rank but a mix-
ture of several low-rank matrices. Note that such an exten-
sion would be a fully generative counter-part of the model
in (Bhatia et al., 2015) which learns a locally low-rank
model but has to rely on an ad-hoc clustering step before-
hand, which is known to be unstable in practice (Bhatia
et al., 2015). Another nice aspect of our framework is that
is naturally allows active learning (Kapoor et al., 2012; Va-
sisht et al., 2014) where we can selectively ask for most
informative labels for an unannotated example. Moreover,
our framework is ﬂexible and inference in our model can
be performed in a fully Bayesian manner (e.g., MCMC or
variational inference) as well as fast point estimation meth-
ods such as (online) EM, that we used in this work.

To summarize, our generative framework offers a ﬂex-
ible way to model the label generation mechanism for
real-world multi-label data sets, which most of the exist-
ing models currently lack. We can model label missing-
ness/observability rigorously under our framework and in-
fer the model parameters easily using a simple inference
procedure. Moreover, the simplicity of the inference proce-
dure makes it easy to design scalable inference algorithms,

5. Experiments

We evaluate our framework on a number of benchmark
data sets and compare it with several state-of-the-art meth-
ods. Our baselines include both label-embedding methods
as well as tree-based methods. The statistics of data sets
we use in our experiments are summarized in Table 1.

Dataset
Bibtex
Mediamill
Eurlex-4K
Movielens
RCV
Wikipedia

N
4880
30993
15539
4000
623847
14146

Ntest
2515
12914
3809
2040
155962
6616

D
1836
120
5000
29
47236
101938

L
159
101
3993
3952
2456
30938

Table 1. Dataset used for the experiments with their properties.
D: number of features, L: number of labels, N : number of train-
ing examples, Ntest: number of test examples

We report both quantitative results (in terms of label predic-
tion accuracies) as well as some qualitative results, namely
looking at the relationship of empirical label frequencies
and label exposure. Note that the label frequency for a
given label denotes how many examples had this label as
1, while label exposure µ(cid:96) ∈ (0, 1) in general refers to
how popular the label (cid:96) is.

In our experiments, we compare with the following state-
of-the-art baselines.

• LEML: This is a low-rank embedding based multi-
label learning model (Yu et al., 2014). LEML as-
sumes the label matrix Y to be modeled as Y ≈ UV
where U = XW. LEML considers various types of
loss functions such as squared loss, logistic loss, hinge
loss, etc. Interestingly, note that LEML with logistic
loss can be seen as a special non-probabilistic case of
our model when also considering λu → ∞, and the
label exposure model turned off.

• BCS: Bayesian Compressive Sensing (BCS) is a gen-
erative model (Kapoor et al., 2012) for the label vec-
tor. It assumes a compressive sensing model for the
label vectors and is essentially a low-rank model.

• FastXML: This is a fast tree-based multi-label learn-
ing model which uses an ensemble of trees (Prabhu &
Varma, 2014).

• PfasterXML: This is an extension of FastXML and
uses propensity-weighted scores to improve perfor-
mance on rare labels (Jain et al., 2016).

Scalable Generative Models for Multi-label Learning with Missing Labels

• PD-Sparse: This model takes a different approach
as compared to label-embedding methods and uses a
margin-maximizing loss for the multi-label learning
problem (Yen et al., 2016).

For the baselines, the reported results are either obtained
using publicly available implementations (with the recom-
mended hyperparameter settings), or the publicly known
best results. We refer to our model as GenEML (for Gen-
erative Exposure-based model for Multi-label Learning)

Hyperparameter Settings: For our model, we set the hy-
perparameters λu and λv to 0.001, which works well on
all the data sets we experimented with. We select the other
two hyperparameters λw and K (number of latent factors)
using cross-validation. On small-/medium-scale data, both
EM and online EM perform comparably and we only report
the results using online EM. On large data sets, we only use
online EM. On the small and medium-scale data, we how-
ever also show a separate experiment comparing EM and
online EM for our model in terms of convergence speed
versus accuracy. For the conjugate gradient (CG) method
used by the M step of our inference algorithm, we run 5 it-
erations, which was found to be sufﬁcient. For online EM,
for each data set, we use mini-batch sizes of 1024 and 4096
and report the one which gives better results.

5.1. Quantitative Results

5.1.1. BENEFIT OF EXPOSURE MODEL

In our ﬁrst experiment, we assess the beneﬁt of using the
exposure model. For this, we apply our model with and
without exposure on a synthetic data set. For this ex-
periment, we generate a synthetic data set with N =500,
D=100, and L=20 and use varying degrees of exposure
probabilities µ(cid:96) ∈ {0.01, 0.05, 0.1, 0.3, 0.5, 0.9} for the
different labels (cid:96) = 1, . . . , 20. We also create a test set
with 500 test examples.

The results are shown in Table 2. As the results show,
our model with exposure turned on outperforms the model
when the exposure is turned off. This clearly demonstrate
the beneﬁt of the exposure model when a signiﬁcant frac-
tion of labels are missing (i.e., not exposed). Our model
also outperforms LEML which does not have a mechanism
to model label exposure.

GenEML

87.8
75.2
65.2

P@1
P@2
P@3

GenEML w/o
Exposure
79.2
68.9
59.0

LEML

78.8
69.1
58.9

Table 2. Precision@k values on synthetic data obtained by our
model with and without using the label exposure

5.1.2. PREDICTION ACCURACIES

In our next set of experiments, in Table 3 we compare
our model (with exposure on) with the other baselines,
in terms of Precision@1, Precision@3, and Precision@5
scores. As Table 3 shows, our model outperforms the other
baselines in most of the cases, except for the RCV and
Wikipedia data, on which our model is outperformed by
LEML and/or PfasterXML. Note, however, that these state-
of-the-art baselines use batch inference methods whereas
we only ran our model in the online setting on a moderate 4
core processor with 8GB RAM. Moreover, our results may
further improve with a more careful hyperparameter tuning
(including selection of minibatch size). The point of the
large-scale data experiment was to mainly show that the our
model can be feasibly run on such large-scale data sets, on
standard machines with moderate computational resources.
Most of the other existing models for multi-label learning
are infeasible to run under such restrictive settings.

5.1.3. BATCH EM VS ONLINE EM

The online version of our EM algorithm is scalable and
faster than its batch counterpart. Fig 2 shows that online
EM converges faster and to a precision score which is very
similar to the batch EM on Bibtex and Mediamill datasets.

Furthermore, online inference is also more effecient,
storage-wise, due the need of maintaining just the sufﬁcient
statistics as in Eq 19 for the updates of each latent factor
un and v(cid:96). For very large datasets, the size of the the suf-
ﬁcient statistics (a D × D covariance matrix) for updating
the regression weight matrix W might not be feasible to
store and update. Therefore, we use cheap, ﬁrst-order gra-
dient based updates for ﬁnding an approximate solution to
the update equation of W in each iteration of the EM algo-
rithm (note that we need not solve for W exactly; the EM
algorithm just requires a few steps of updates for W in the
M step). This further reduces the memory requirement of
our model, while also speeding up inference due to faster
computation of gradients as compared to CG updates.

Figure 2. Convergence time comparison of the batch and online
EM algorithm for inference in our model

5.2. Qualitative Results

Finally, we do some qualitative analyses of our model’s be-
havior. We investigate whether the global frequency of
a label necessarily correlates to its exposure probability.

Scalable Generative Models for Multi-label Learning with Missing Labels

Dataset

Bibtex

Mediamill

Eurlex-4K

Movielens

RCV-2K

Wiki10-31K

P@1
P@3
P@5
P@1
P@3
P@5
P@1
P@3
P@5
P@1
P@3
P@5
P@1
P@3
P@5
P@1
P@3
P@5

BCS WSABIE
60.24
34.87
24.48
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

54.78
32.29
23.98
81.29
64.74
49.83
68.55
55.11
45.12
-
-
-
-
-
-
-
-
-

FastXML
63.42
39.23
28.86
84.22
67.33
53.04
71.36
59.90
50.39
-
-
-
-
-
-
83.03
67.47
57.76

PfasterXML
63.46
39.22
29.14
83.98
67.37
53.02
75.45
62.70
52.51
-
-
-
-
-
-
83.57
68.61
59.10

PD-Sparse
61.29
35.82
25.74
81.86
62.52
45.11
76.43
60.37
49.72
-
-
-
-
-
-
-
-
-

LEML GenEML
62.54
38.41
28.21
84.01
67.20
52.80
63.40
50.35
41.28
54.22
50.44
48.63
89.09
70.96
50.73
73.47
62.43
54.35

64.09
40.14
29.47
87.15
69.98
55.21
77.75
63.98
53.24
55.78
50.62
48.86
87.21
69.22
49.53
76.38
44.49
32.06

Table 3. Performance Comparison using Prec@k of the model with other baselines. The - denotes that either these results were not
available or the method was infeasible to run on that data set. On the large-scale data sets (RCV and Wiki), our model was run using
online EM based inference.

While it may be the case for some data sets where high la-
bel frequency implies a high inferred label exposure prob-
ability (e.g., see Fig. 3 for Bibtex and Mediamill data), it
need not be the case with other data sets. For example,
for Movielens data, each user-movie (example-label) pair
has an some context information (user and movie features)
available for it. As we show in Fig 4, the inferred expo-
sure probability (which depends on the context features) of
the same movie (label) indeed turns out to be different for
different users (examples).

Fig. 4 shows the plot of inferred exposure probabilities µnl
for two users (one female, one male) plotted against the
label frequencies (movie popularities).

Figure 3. Inferred label exposure probabilities for Bibtex and Me-
diamill data sets
As Fig. 4 shows, our model infers that, a popular movie
(shown in red dot in Fig 4) has a high exposure probabil-
ity for the left user (Female, 25, Healthcare/Doctor) while
it has a low exposure probability for the right user (Male,
35, artist). This example illustrates that a high label fre-
quency does not necessarily imply a high exposure proba-
bility, which can be context (user in this case) dependent.

6. Conclusion

We presented a ﬂexible and scalable generative framework
for multi-label learning. Our framework is based on a latent

Figure 4. Inferred user-speciﬁc label exposure probabilties for
two users on Movielens data set: Female, 25, Healthcare/Doctor
(left) and Male, 35, artist (right), with label frequency for each
label. The red circle shows the most frequent movie (Hair-
spray(1988) Comedy, Drama) and its exposure probability for
both the users.

factor model for the label matrix and does not assume that
the zeros in the label matrix are necessarily negative labels.
We use a set of label exposure latent variables to model this,
and infer these exposure probabilities from data. Incorpo-
rating these latent variables leads to improve multi-label
classiﬁcation accuracies, and also enables doing interesting
qualitative analyses. Our model admits a simple inference
procedure which can be implemeted using Gibbs sampling
or EM. We further develop a highly scalable online EM
algorithm for performing inference in our model, which al-
lows our model to be applied on large-scale data sets, even
on standard machines with moderate hardware. The gener-
ative framework makes it easy to extend our model in many
interesting ways. For example, it can be extended to a mix-
ture of latent factor models, which will allow handling the
cases where a single low-rank model does not adequately
capture the structure of the label matrix.

Acknowledgements: PR acknowledges support from Extreme
Classiﬁcation research grant from Microsoft Research India,
DST-SERB Early Career Research Award, Dr. Deep Singh and
Daljeet Kaur Fellowship, and Research-I Foundation, IIT Kanpur.

Scalable Generative Models for Multi-label Learning with Missing Labels

Liang, Dawen, Charlin, Laurent, McInerney, James, and Blei,
In
David M. Modeling user exposure in recommendation.
WWW, 2016.

Polson, Nicholas G, Scott, James G, and Windle, Jesse. Bayesian
inference for logistic models using p´olya–gamma latent vari-
ables. Journal of the American Statistical Association, 108
(504):1339–1349, 2013.

Prabhu, Yashoteja and Varma, Manik. FastXML: a fast, accurate
and stable tree-classiﬁer for extreme multi-label learning. In
KDD, 2014.

Rai, Piyush, Hu, Changwei, Henao, Ricardo, and Carin,
Lawrence. Large-scale bayesian multi-label learning via topic-
based label embeddings. In NIPS, 2015.

Scott, James G and Sun, Liang. Expectation-maximization for
logistic regression. arXiv preprint arXiv:1306.0040, 2013.

Vasisht, Deepak, Damianou, Andreas, Varma, Manik, and
Kapoor, Ashish. Active learning for sparse bayesian multilabel
classiﬁcation. In KDD, 2014.

Wang, Jiang, Yang, Yi, Mao, Junhua, Huang, Zhiheng, Huang,
Chang, and Xu, Wei. CNN-RNN: A uniﬁed framework for
multi-label image classiﬁcation. In CVPR, 2016.

Xu, Chang, Tao, Dacheng, and Xu, Chao. Robust extreme multi-

label learning. In KDD, 2016.

Yen, Ian EH, Huang, Xiangru, Zhong, Kai, Ravikumar, Pradeep,
and Dhillon, Inderjit S. PD-sparse: A primal and dual sparse
approach to extreme multiclass and multilabel classiﬁcation. In
ICML, 2016.

Yu, Hsiang-Fu, Jain, Prateek, Kar, Purushottam, and Dhillon, In-
derjit S. Large-scale multi-label learning with missing labels.
In ICML, 2014.

Yu, Hsiang-Fu, Huang, Hsin-Yuan, Dhillon, Inderjit S, and Lin,
Chih-Jen. A uniﬁed algorithm for one-class structured matrix
factorization with side information. In AAAI, 2017.

References

Agrawal, Rahul, Gupta, Archit, Prabhu, Yashoteja, and Varma,
Manik. Multi-label learning with millions of labels: Recom-
mending advertiser bid phrases for web pages. In WWW, 2013.

Babbar, R. and Sch¨olkopf, B. DiSMEC- distributed sparse ma-
chines for extreme multi-label classiﬁcation. In WSDM, 2017.

Barnard, Kobus, Duygulu, Pinar, Forsyth, David, Freitas,
Nando de, Blei, David M, and Jordan, Michael I. Matching
words and pictures. JMLR, 2003.

Bertsekas, Dimitri P. Nonlinear programming. Athena scientiﬁc

Belmont, 1999.

Bhatia, Kush, Jain, Himanshu, Kar, Purushottam, Varma, Manik,
and Jain, Prateek. Sparse local embeddings for extreme multi-
label classiﬁcation. In NIPS, 2015.

Bucak, Serhat Selcuk, Jin, Rong, and Jain, Anil K. Multi-label
learning with incomplete class assignments. In CVPR, 2011.

Capp´e, Olivier and Moulines, Eric.

On-line expectation–
maximization algorithm for latent data models. Journal of the
Royal Statistical Society: Series B (Statistical Methodology),
2009.

Chen, Yao-Nan and Lin, Hsuan-Tien. Feature-aware label space
In NIPS,

dimension reduction for multi-label classiﬁcation.
2012.

Ciss´e, Moustapha, Al-Shedivat, COM Maruan, and Bengio,
Samy. Adios: Architectures deep in output space. In ICML,
2016.

Feng, SL, Manmatha, Raghavan, and Lavrenko, Victor. Multiple
bernoulli relevance models for image and video annotation. In
CVPR, 2004.

Fraccaro, Marco, Paquet, Ulrich, and Winther, Ole.

Indexable
probabilistic matrix factorization for maximum inner product
search. In AAAI, 2016.

Gibaja, Eva and Ventura, Sebasti´an. Multilabel learning: A re-
view of the state of the art and ongoing research. Wiley Inter-
disciplinary Reviews: Data Mining and Knowledge Discovery,
2014.

Gibaja, Eva and Ventura, Sebasti´an. A tutorial on multilabel

learning. ACM Comput. Surv., 2015.

Hu, Yifan, Koren, Yehuda, and Volinsky, Chris. Collaborative

ﬁltering for implicit feedback datasets. In ICDM, 2008.

Jain, Himanshu, Prabhu, Yashoteja, and Varma, Manik. Extreme
multi-label loss functions for recommendation, tagging, rank-
ing & other missing label applications. In KDD, 2016.

Kanehira, Atsushi and Harada, Tatsuya. Multi-label ranking from

positive and unlabeled data. In CVPR, 2016.

Kapoor, Ashish, Viswanathan, Raajay, and Jain, Prateek. Mul-
tilabel classiﬁcation using bayesian compressed sensing.
In
NIPS, 2012.

Kong, Xiangnan, Wu, Zhaoming, Li, Li-Jia, Zhang, Ruofei, Yu,
Philip S, Wu, Hang, and Fan, Wei. Large-scale multi-label
learning with incomplete label assignments. In SDM, 2014.

