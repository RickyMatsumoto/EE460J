Supplementary material:
Bayesian Optimization with Tree-structured Dependencies

Rodolphe Jenatton 1 Cedric Archambeau 1 Javier Gonzalez 2 Matthias Seeger 1

Abstract

In this supplementary material, we provide addi-
tional experimental results along with some de-
tails about the inference in our model and its non-
linear extension.

1. Experiments

In this section, we provide complementary experimental
results.

1.1. Optimization of synthetic tree-structured functions

In addition to the results presented in the core of the pa-
per, we include simulations with the non-linear extension of
tree, denoted by tree-nonlinear, where the shared
parameters zp = [rv]v∈Vp are modeled in a non-linear fash-
ion (see details in Section 3 of the supplementary material).

Moreover, we consider as well settings where the shared
variables have a quadratic dependency in the leaf objectives,
on both balanced and unbalanced binary trees; see Figure 1
and Figure 2.

The trees on Figure 1 are referred to as small balanced,
while those on Figure 2 are referred to as small
unbalanced. We consider also higher-dimensional ver-
sions of those, with a depth of 4, resulting in respec-
tively 8 and 9 leaves whose constant shifts are respectively
{a × 0.1}8
a=1 (they are referred to as
large balanced and large unbalanced ).

a=1 and {a × 0.1}9

As deﬁned in the core paper, all the non-shared continuous
variables xj’s are deﬁned in [−1, 1], while the shared ones
are in [0, 1]. This implies that the best function value will
always be 0.1.

1Amazon, Berlin, Germany. 2Amazon, Cambridge, United
Kingdom. Correspondence to: Rodolphe Jenatton <jenat-
ton@amazon.de>, Cedric Archambeau <cedrica@amazon.de>,
Javier Gonzalez <gojav@amazon.co.uk>, Matthias Seeger
<matthias@amazon.de>.

We can see that the observations made in the core of the pa-
per are still valid for the unbalanced trees. In particular, with
linearly-dependent shared variables, tree-nonlinear
converges more slowly than the linear version tree (mid-
dle panels of Figures 3 and 4), which may be the price
to pay for having higher-dimensional latent variables c.
Moreover, in presence of quadratically-dependent shared
variables (right panels of Figures 3 and 4), we observe
that tree fails to model adequately the non-linearities,
while tree-nonlinear, as expected, can. In absence of
shared variables (left panels of Figures 3 and 4), tree and
tree-nonlinear are simply equivalent, which explains
why their curves are superimposed. Finally, as dimension
increases (i.e., going from the small to large settings),
the performance of independent worsens, while that of
tree and tree-nonlinear are barely affected.

1.2. Multi-layer perceptron tuning

We report in Figure 5 the results of all methods including the
non-linear extension of tree. The setting is identical to that
described in the core of the paper. We can see that the linear
version tree performs better than tree-nonlinear.
This conclusion, perhaps surprising, is in good agreement
with the recent observations from Zhang et al. (2016) where
simple linear models outperformed more sophisticated, non-
linear competing methods within the context of the opti-
mization of data analytic pipelines.

2. Details about the Tree-structured
semi-parametric Gaussian process
regression model

We start by providing details about the posterior inference.

2.1. Posterior Inference

The joint distribution P (y, g, c) of our model is given by

P (c)(cid:81)

pN (gp; bp, Kp)N (yp; gp + Z(cid:62)

p c, σ2Inp ),

(1)

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

where Kp = [Kp(xi, xj)]i,j∈Ip are kernel matrices, with
the prior P (c) = N (c; 0, Σc). Our goal is to obtain the pos-
terior process P (gp(·)|c, yp) and the posterior distribution

Supplementary material: Bayesian Optimization with Tree-structured Dependencies

x2

0

x2

4 + 0.1

5 + 0.2

x2

6 + 0.3

1

0

x3

1
x2

7 + 0.4

x1

x1

x1

0

1
x2

0

1

0

1

1

0

1

0

0

0

x2, r8

x3, r9

x2

4 + 0.1+r8

x2

5 + 0.2+r8

x2

6 + 0.3+r9

x2

7 + 0.4+r9

x2, r8

x3, r9

x2
4 + 0.1+(r8 − 0.5)2

x2
5 + 0.2+(r8 − 0.5)2

x2
6 + 0.3+(r9 − 0.5)2

x2
7 + 0.4+(r9 − 0.5)2

1

1

Figure 1. Three examples of functions with tree-structured condi-
tional relationships. Each inner node is a binary variable, and a
path in those trees represents successive binary decisions. The
leaves contain univariate quadratic functions that are shifted by
different constant terms. Top: Setting without shared variables.
Middle: r8 and r9 are shared variables that are common to the
functions at the leaves of their respective subtrees. In this example,
the shared variables have a linear dependency in the leaf objectives.
Bottom: Similar to the middle tree, except that r8 and r9 have a
quadratic dependency in the leaf objectives.

P (c|y). This can be done by invoking standard multivari-
ate Gaussian identities (Petersen & Pedersen, 2012) or by
directly reading of the posterior parameters after rewriting
the joint distribution into the following form:
P (y)P (c|y)(cid:81)

pP (gp|c, yp).

The second expression has precisely the form of the poste-
rior for a standard GP regression model: the ﬁrst factor is
the GP prior P (gp), while the second is the Gaussian like-
lihood with observations yp − Z(cid:62)
p c and noise covariance
σ2Inp . Following Rasmussen & Williams (2006), we obtain
the posterior GP distribution as:

gp(·)|c, yp ∼ GP(cid:0)mp(·), Sp(·, ·)(cid:1),

p (yp − Z(cid:62)
where mp(x) = kp(x)(cid:62)M−1
Sp(x, x(cid:48)) = Kp(x, x(cid:48)) − kp(x)(cid:62)M−1
obtain the posterior P (c|y), we note that

p c − bp) + bp and
p kp(x(cid:48)). Next, to

P (c|y) ∝ N (c; 0, Σc)(cid:81)

pN (yp; Z(cid:62)

p c + bp, Mp).

p ZpM−1

2 c(cid:62)Λcc+f (cid:62)
p (yp − bp) and Λc = Σ−1

After some algebra, we can also re-arrange this expres-
sion into a quadratic form P (c|y) ∝ e− 1
c c,
where fc = (cid:80)
c +
(cid:80)
p Z(cid:62)
p ZpM−1
p . Hence, we see that the posterior P (c|y) is
the Gaussian distribution N (Λ−1
c ). Expressions
depending on this distribution are computed using the
Cholesky decomposition of Λc.

fc, Λ−1

c

We next give details about the computation of the marginal
likelihood.

2.2. Marginal Likelihood

Recall the full joint from (1). Using (2) and dividing out
common terms leads to
P (y)P (c|y) = N (c; 0, Σc)(cid:81)

p c + bp, Mp).

pN (yp; Z(cid:62)

Hence, we obtain the following expression for the log-
marginal likelihood log P (y):

(cid:80)

plog N (yp; Z(cid:62)

pc + bp, Mp) + log P (c)− logP (c|y).

3. Details about the non-linear extension of

First, we can decompose the joint P (yp, gp|c):

tree

P (gp|c, yp)N (yp; Z(cid:62)

p c + bp, Mp),

(2)

where Mp = Kp + σ2Inp . In the sequel, we compute ex-
pressions such as M−1
and log |Mp| by using the Cholesky
decomposition Mp = LpL(cid:62)
p . The second factor in (2) is
the distribution P (yp|c), obtained by integrating out gp:

p

(cid:90)

gp

N (yp; gp + Z(cid:62)

p c, Mp)N (gp; bp, Kp)dgp.

It will play a role shortly. The ﬁrst factor in (2) is the
posterior over the latent functions values:

P (gp|c, yp) ∝ N (gp; bp, Kp)N (yp; gp + Z(cid:62)

∝ N (gp; bp, Kp)N (yp − Z(cid:62)

p c, σ2Inp )
p c; gp, σ2Inp ).

For space limitation reasons, we have omitted in the core
paper the details describing how we use in practice the
random Fourier features (Rahimi et al., 2007).

For a given vertex v ∈ V with feature representation rv ∈
RDv , we associate the new Rv-dimensional representation
φv(rv) (cid:44) (cid:112)2/Rv · cos (cid:0)Wvrv + qv

(cid:1)

where cos(·) operates element-wise, Wv ∈ RRv×Dv has en-
tries identically and independently distributed from N (0, 1
γ )
while qv ∈ RRv has its components generated uniformly in
[0, 2π]. Those randomly generated features can be shown
to approximate a RBF kernel with bandwidth γ2 (Rahimi
et al., 2007).

Supplementary material: Bayesian Optimization with Tree-structured Dependencies

x1

x2

0

1

1

0

x3

1

x1

x2, r10

0

1

1

0

x3, r11

1

x2

5 + 0.3

x2

6 + 0.4

x2

7 + 0.5

x2

5 + 0.3+r10

6 + 0.4+r11 x2
x2

7 + 0.5+r11

0

x4

0

1

x2

8 + 0.1

x2

9 + 0.2

0

x4

0

1

x2

8 + 0.1+r10

x2

9 + 0.2+r10

x1

x2, r10

0

x4

0

1

1

0

x3, r11

1

7 + 0.5+(r11 − 0.5)2
x2

0

1
x2
5 + 0.3+(r10 − 0.5)2

6 + 0.4+(r11 − 0.5)2
x2

9 + 0.2+(r10 − 0.5)2
x2

8 + 0.1+(r10 − 0.5)2
x2

Figure 2. Three examples of functions with tree-structured conditional relationships. Each inner node is a binary variable, and a path in
those trees represents successive binary decisions. The leaves contain univariate quadratic functions that are shifted by different constant
terms. Compared with Figure 2, the trees are not balanced anymore. Top left: Setting without shared variables. Top right: r10 and r11 are
shared variables that are common to the functions at the leaves of their respective subtrees. In this example, the shared variables have a
linear dependency in the leaf objectives. Bottom: Similar to the middle tree, except that r10 and r11 have a quadratic dependency in the
leaf objectives.

Rastogi, Rajeev (eds.), KDD, pp. 2065–2074. ACM, 2016.
ISBN 978-1-4503-4232-2.

In practice, to address the problem of the selection of γ,
we consider a strategy consisting of approximating a sum
of RBF kernels for different values of γ. Our experiments
showed that taking γ ∈ {0.1, 1, 10} provided good results.
Moreover, we ﬁx the dimension Rv = 3R for all ver-
v (rv) (cid:44)
tices, where R corresponds to the dimension of φγ
(cid:112)2/Rv · cos(W(γ)
v rv + qv) for each γ ∈ {0.1, 1, 10} with
W(γ)
γ ). The ﬁnal rep-
v
resentation φv(rv) = [φγ
v (rv)]γ∈{0.1,1,10} is obtained via a
concatenation of the φγ

having its entries drawn from N (0, 1

v ’s. In practice, we take R = 25.

References

Petersen, K. B. and Pedersen, M. S. The matrix cookbook.
Technical report, Technical University of Denmark, nov
2012. Version 20121115.

Rahimi, Ali, Recht, Benjamin, et al. Random features for
large-scale kernel machines. In Advances in Neural In-
formation Processing Systems, volume 3, pp. 5, 2007.

Rasmussen, Carl and Williams, Chris. Gaussian Processes

for Machine Learning. MIT Press, 2006.

Zhang, Yuyu, Bahadori, Mohammad Taha, Su, Hang, and
Sun, Jimeng. Flash: Fast bayesian optimization for data
analytic pipelines. In Krishnapuram, Balaji, Shah, Mohak,
Smola, Alexander J., Aggarwal, Charu, Shen, Dou, and

Supplementary material: Bayesian Optimization with Tree-structured Dependencies

Figure 3. Optimization performance over synthetic tree-structured functions, as measured by the log10 distance to the (known) minimum
versus the number of iterations. Top: Results for the small balanced binary trees displayed in Figure 1, without (left), with linearly-
dependent (middle) and quadratically-dependent (right) shared variables. Bottom: Results for larger balanced binary trees with depth 4
and a total of 8 leaves, without (left), with linearly-dependent (middle) and quadratically-dependent (right) shared variables. Best seen in
color.

010203040506070Iterations5432101log10(Distance to optimum)small balanced - no shared variablesarcgp-baselineindependentmarginalrandomsmactreetree-nonlinear010203040506070Iterations432101log10(Distance to optimum)small balanced - linearly shared variablesarcgp-baselineindependentrandomsmactreetree-nonlinear010203040506070Iterations432101log10(Distance to optimum)small balanced - quad. shared variablesarcgp-baselineindependentrandomsmactreetree-nonlinear010203040506070Iterations5432101log10(Distance to optimum)large balanced - no shared variablesarcgp-baselineindependentmarginalrandomsmactreetree-nonlinear010203040506070Iterations432101log10(Distance to optimum)large balanced - linearly shared variablesarcgp-baselineindependentrandomsmactreetree-nonlinear010203040506070Iterations432101log10(Distance to optimum)large balanced - quad. shared variablesarcgp-baselineindependentrandomsmactreetree-nonlinearSupplementary material: Bayesian Optimization with Tree-structured Dependencies

Figure 4. Optimization performance over synthetic tree-structured functions, as measured by the log10 distance to the (known) minimum
versus the number of iterations. Top: Results for the small unbalanced binary trees displayed in Figure 2, without (left), with linearly-
dependent (middle) and quadratically-dependent (right) shared variables. Bottom: Results for the large unbalanced binary trees with depth
4 and a total of 9 leaves, without (left), with linearly-dependent (middle) and quadratically-dependent (right) shared variables. Best seen
in color.

010203040506070Iterations5432101log10(Distance to optimum)small unbalanced - no shared variablesarcgp-baselineindependentmarginalrandomsmactreetree-nonlinear010203040506070Iterations432101log10(Distance to optimum)small unbalanced - linearly shared variablesarcgp-baselineindependentrandomsmactreetree-nonlinear010203040506070Iterations432101log10(Distance to optimum)small unbalanced - quad. shared variablesarcgp-baselineindependentrandomsmactreetree-nonlinear010203040506070Iterations5432101log10(Distance to optimum)large unbalanced - no shared variablesarcgp-baselineindependentmarginalrandomsmactreetree-nonlinear010203040506070Iterations432101log10(Distance to optimum)large unbalanced - linearly shared variablesarcgp-baselineindependentrandomsmactreetree-nonlinear010203040506070Iterations432101log10(Distance to optimum)large unbalanced - quad. shared variablesarcgp-baselineindependentrandomsmactreetree-nonlinearSupplementary material: Bayesian Optimization with Tree-structured Dependencies

Figure 5. Tuning of a multi-layer perceptron for binary classiﬁcation. Average rank aggregated over 45 datasets versus the number of
iterations (lower is better; see details in the main article). Best seen in color.

0102030405060708090Iterations1234567Mean rankRank across all datasets (shared topology)arcgp-baselineindependentrandomsmactreetree-nonlinear