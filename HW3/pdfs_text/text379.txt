Appendix

A. Details of the Proof

Proof of Theorem 2 We assume the optimization starts with an initialized weights w0. t is denoted as the iteration index.
Let wt
s be the model parameter updated by our omniscient teacher and SGD, respectively. We ﬁrst consider the
case where t = 1. For SGD, the ﬁrst gradient update w1

g and wt

s is

Then we compute the difference between w1

(cid:13)
(cid:13)w1

s − w∗(cid:13)
2
2 =
(cid:13)

(cid:13)
(cid:13)
w0 − ηt
(cid:13)
(cid:13)
(cid:13)

∂(cid:96)((cid:10)w0, xs
∂w0

(cid:11) , ys)

.

w1

s = w0 − ηt

s and w∗:
∂(cid:96)((cid:10)w0, x(cid:11) , y)
∂w0

− w∗

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

= (cid:13)

(cid:13)w0 − w∗(cid:13)
2
2 + η2
(cid:13)

t

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∂(cid:96)((cid:10)w0, x(cid:11) , y)
∂w0

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

(cid:42)

− 2ηt

w0 − w∗,

(cid:43)

∂(cid:96)((cid:10)w0, x(cid:11) , y)
∂w0

Because the omniscient teacher is to minimize last two term, so we are guaranteed to have

≤ (cid:13)

g = w0

s − w∗(cid:13)
2
2 .
(cid:13)

(cid:13)
(cid:13)w1
g − w∗(cid:13)
2
(cid:13)
2

g − w∗(cid:13)
2
(cid:13)w1
(cid:13)
2
s − w∗(cid:13)
≤ (cid:13)
2
So with the same initialization w0
2 is always true. Then we consider the case where
(cid:13)
and w∗:
t = k, k ≥ 1. We ﬁrst compute the difference between wk+1
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
∂(cid:96)((cid:10)wk

g − w∗(cid:13)
2
(cid:13)
2

(cid:13)
(cid:13)
wk
(cid:13)
(cid:13)
(cid:13)

g , x(cid:11) , y)

(cid:13)
(cid:13)wk+1

∂(cid:96)((cid:10)wk

∂(cid:96)((cid:10)wk

g − ηt

∂wk+1

s, (cid:13)

− w∗

(cid:43) (cid:27)

(cid:13)w1

(cid:13)w1

(12)

(cid:42)

=

(cid:26)

g

η2
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
∂(cid:96)((cid:10)wk

= (cid:13)

(cid:13)wk

g − w∗(cid:13)
2
(cid:13)
2

+ min
{x,y}

= (cid:13)

(cid:13)wk

g − w∗(cid:13)
2
(cid:13)
2

= (cid:13)

(cid:13)wk

g − w∗(cid:13)
2
(cid:13)
2

+ η2
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
− T V (wk
g )

g , x(cid:11) , y)
∂wk
g

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

− 2ηt

wk

g − w∗,

(cid:11) , yk
∗ )

g , xk
∗
∂wk
g

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

(cid:42)

− 2ηt

wk

g − w∗,

∂(cid:96)((cid:10)wk

g , xk
∗
∂wk
g

g , x(cid:11) , y)
∂wk
g

(cid:11) , yk
∗ )

(cid:43)

where xk
bound the difference between wk+1

∗, yk

∗ is the sample selected by the omniscient teacher in the k-th iteration. Using the given conditions, we can

(10)

(11)

(13)

(cid:13)
(cid:13)wk+1

s − w∗(cid:13)
2
2 =
(cid:13)

s

(cid:13)
(cid:13)
wk
(cid:13)
(cid:13)
(cid:13)

s − ηt

∂(cid:96)((cid:10)wk

and w∗ from below:
s , xs(cid:11) , ys)
∂wk
s
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
s − w∗(cid:13)
2
2 − T V (wk
s )
(cid:13)

s − w∗(cid:13)
2
2 + η2
(cid:13)

∂(cid:96)((cid:10)wk

t

= (cid:13)

(cid:13)wk

≥ (cid:13)

(cid:13)wk

− w∗

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:11) , yk
s )

s , xk
s
∂wk
s

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

(cid:42)

− 2ηt

wk

s − w∗,

∂(cid:96)((cid:10)wk

(cid:43)

(cid:11) , yk
s )

(14)

s , xk
s
∂wk
s

s , yk

s is the sample selected by the random teacher in the k-th iteration. Comparing Eq. 13 and Eq. 14 and using

where xk
the condition in the theorem, the following inequality always holds under the condition (cid:13)

(cid:13)
s − w∗(cid:13)
2
(cid:13)wk+1
2 − T V (wk
(15)
(cid:13)
g − w∗(cid:13)
Further because we already know that (cid:13)
≤ (cid:13)
g − w∗(cid:13)
2
2
(cid:13)w1
(cid:13)
(cid:13)
2
2
s − w∗(cid:107)2
will be always not larger than (cid:107)wt
2 (t can be any iteration). Therefore, in each iteration the omniscient teacher can
always converge not slower than random teacher (SGD).

(cid:13)wk
g ) = (cid:13)
(cid:13)wk
s − w∗(cid:13)
2, using induction we can conclude that (cid:13)
2
(cid:13)

g − w∗(cid:13)
≤ (cid:13)
2
(cid:13)wk
(cid:13)
2
g − w∗(cid:13)
2
(cid:13)wk+1
(cid:13)
2

2 = (cid:13)
s − w∗(cid:13)
2
(cid:13)

s ) ≥ (cid:13)
(cid:13)w1

s − w∗(cid:13)
2
2:
(cid:13)

g − w∗(cid:13)
2
(cid:13)
2

− T V (wk

(cid:13)wk

(cid:13)wt

.

(16)

(17)

(18)

(19)

(20)

Iterative Machine Teaching

Proof of Proposition 3 Consider the square loss (cid:96)((cid:104)w, x(cid:105) , y) = ((cid:104)w, x(cid:105) − y)2, we have ∂(cid:96)((cid:104)w,x(cid:105),y)
Suppose we are given two initializations w1, w2 satisfying (cid:107)w1 − w∗(cid:107)2

= 2((cid:104)w, x(cid:105) − y)x.
2. For square loss, we ﬁrst write out

∂w

2 ≤ (cid:107)w2 − w∗(cid:107)2
{η2

(cid:107)w1 − w∗(cid:107)2 − T V (w1) = (cid:107)w1 − w∗(cid:107)2 + min

t T1(x, y|w1) − 2ηtT2(x, y|w1)}

= (cid:107)w1 − w∗(cid:107)2 + min
{x,y}
(cid:40)

= (cid:107)w1 − w∗(cid:107)2 +

∂(cid:96)((cid:104)w1, x(cid:105) , y)
∂w1

(cid:26)

η2
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)
R
2(
− (cid:107)w1 − w∗(cid:107)2 ,

(cid:107)w1−w∗(cid:107) )2 (cid:107)w1 − w∗(cid:107)2 (w1 − w∗),

if

if

R

(cid:107)w1−w∗(cid:107) ≥ 1
ηt

w1 − w∗,

(cid:29) (cid:27)

∂(cid:96)((cid:104)w1, x∗(cid:105) , y∗)
∂w1
(cid:107)w1−w∗(cid:107) < 1
ηt

R

x∈X ,y∈Y
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

− 2ηt

(cid:28)

Similarly for w2, we have

(cid:107)w2 − w∗(cid:107)2 − T V (w2)

= (cid:107)w2 − w∗(cid:107)2 +

(cid:40)

R
2(
− (cid:107)w2 − w∗(cid:107)2 ,

(cid:107)w2−w∗(cid:107) )2 (cid:107)w2 − w∗(cid:107)2 (w2 − w∗),

if

R

(cid:107)w2−w∗(cid:107) < 1
ηt

if

R

(cid:107)w2−w∗(cid:107) ≥ 1
ηt

There will be three scenarios to consider: (1) Rηt ≤ (cid:107)w1 − w∗(cid:107) ≤ (cid:107)w2 − w∗(cid:107); (2) (cid:107)w1 − w∗(cid:107) ≤ Rηt ≤ (cid:107)w2 − w∗(cid:107); (3)
(cid:107)w1 − w∗(cid:107) ≤ (cid:107)w2 − w∗(cid:107) ≤ Rηt. It is easy to verify that under all three scenarios, we have

(cid:107)w1 − w∗(cid:107)2 − T V (w1) ≤ (cid:107)w2 − w∗(cid:107)2 − T V (w2)

To simplify notations, we denote β((cid:104)w,x(cid:105),y) = ∇(cid:104)w,x(cid:105)(cid:96) ((cid:104)w, x(cid:105) , y) for a loss function (cid:96)(·, ·) in the following proof. For
omniscient teacher, (ˆx, ˆy) denotes a speciﬁc construction of (x, y). Notice that (˜x, ˜y) will not be used in omniscient teacher
case to avoid ambiguity, since the student and the teacher use the same representation space.

Proof of Theorem 4 At t-step, the omniscient teacher selects the samples via optimization

min
x∈X ,y∈Y

η2(cid:107)∇wt(cid:96) (cid:0)(cid:10)wt, x(cid:11) , y(cid:1) (cid:107)2 − 2η (cid:10)wt − w∗, ∇wt(cid:96) (cid:0)(cid:10)wt, x(cid:11) , y(cid:1)(cid:11) .

We denote ˆx = γ (wt − w∗) and ˆy ∈ Y, since γ (w − w∗) ∈ X , we have

η2(cid:107)∇wt(cid:96) (cid:0)(cid:10)wt, x(cid:11) , y(cid:1) (cid:107)2 − 2η (cid:10)wt − w∗, ∇wt (cid:96) (cid:0)(cid:10)wt, x(cid:11) , y(cid:1)(cid:11)

min
x∈X ,y∈Y
(cid:16)

≤

η2β2

((cid:104)wt,ˆx(cid:105),ˆy)γ2 − 2ηβ((cid:104)wt,ˆx(cid:105),ˆy)γ

(cid:107)wt − w∗(cid:107)2
2.

(cid:17)

Plug Eq. (19) into the recursion Eq. (3), we have

(cid:13)wt+1 − w∗(cid:13)
(cid:13)
2
2 = min
(cid:13)

x∈X ,y∈Y

(cid:13)
(cid:13)
wt − η
(cid:13)
(cid:13)

∂(cid:96)((cid:104)w, x(cid:105) , y)
∂w

− w∗

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

= (cid:13)

(cid:13)wt − w∗(cid:13)
2
2 + min
(cid:13)
(cid:16)

x∈X ,y∈Y

η2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∂(cid:96)((cid:104)wt, x(cid:105) , y)
∂wt

(cid:28)

− 2η

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2
(cid:107)wt − w∗(cid:107)2

(cid:17)

wt − w∗,

∂(cid:96)((cid:104)wt, x(cid:105) , y)
∂wt

(cid:29)

≤

1 + η2β2

(cid:107)wt − w∗(cid:107)2
2.
(21)
First we let ν(γ) = minw,y γ∇(cid:104)w,γ(w−w∗)(cid:105)(cid:96) ((cid:104)w, γ (w − w∗)(cid:105) , y). Then we have the condition 0 < ν(γ) ≤
γβ((cid:104)w,γ(w−w∗)(cid:105),ˆy) ≤ 1

((cid:104)wt,ˆx(cid:105),ˆy)γ2 − 2ηβ((cid:104)wt,ˆx(cid:105),ˆy)γ

η < ∞ for any w, y, so we can obtain

2 = (cid:0)1 − ηβ((cid:104)wt,γ(wt−w∗)(cid:105),ˆy)γ(cid:1)2

after simplifying ν(γ) to ν, we therefore have the following inequality from Eq. (21):

Thus we can have the exponential convergence:

0 ≤ 1 − γηβ((cid:104)w,γ(w−w∗)(cid:105),ˆy) ≤ 1 − ην(γ),

2 ≤ (1 − ην)2 (cid:13)
(cid:13)
(cid:13)wt+1 − w∗(cid:13)
2
(cid:13)

(cid:13)wt − w∗(cid:13)
2
2 ,
(cid:13)

(cid:13)wt − w∗(cid:13)
(cid:13)
(cid:17)−1

(cid:16)

log

1
1−ην

log (cid:107)w0−w∗(cid:107)

(cid:15)

(cid:13)2 ≤ (1 − ην)t (cid:13)

(cid:13)w0 − w∗(cid:13)

(cid:13)2 ,

in other words, the student needs

samples to achieve an (cid:15)-approximation of w∗.

Iterative Machine Teaching

Proof of Proposition 5 Because (cid:96) ((cid:104)w, x(cid:105) , y) is ζ1-strongly convex w.r.t. w, we have

(cid:16)

ζ1

(cid:96) ((cid:104)w, x(cid:105) , y) − min

(cid:96) ((cid:104)w, x(cid:105) , y)

w

≤ (cid:107)∇w(cid:96) ((cid:104)w, x(cid:105) , y)(cid:107)2 = β2

((cid:104)w,x(cid:105),y) (cid:107)x(cid:107)2 ,

∀ {x, y} ∈ X × Y,

where X = (cid:8)x ∈ Rd, (cid:107)x(cid:107) ≤ R(cid:9). Using ˆx = γ(w − w∗), γ ≥ 0, we have
(cid:17)

(cid:114)

(cid:16)

(cid:96) ((cid:104)w, γ(w − w∗)(cid:105) , y) − min

(cid:96) ((cid:104)w, γ(w − w∗)(cid:105) , y)

ζ1

≤ β((cid:104)w,γ(w−w∗)(cid:105),y)γ(cid:107)w − w∗(cid:107).

(cid:17)

w

We assume the loss function is always non-negative, i.e., (cid:96) ((cid:104)w, x(cid:105) , y) ≥ 0. Therefore we have

(cid:112)ζ1 ((cid:96) ((cid:104)w, γ(w − w∗)(cid:105) , y)) ≤ β((cid:104)w,γ(w−w∗)(cid:105),y)γ(cid:107)w − w∗(cid:107).
Because (cid:96) ((cid:104)w, x(cid:105) , y) is ζ-strongly convex w.r.t. w, it is also ζ2-strongly convex w.r.t. (cid:104)w, x(cid:105). Then we perform Taylor
expansion to (cid:96) ((cid:104)w, γ(w − w∗)(cid:105) , y) w.r.t. (cid:104)w, x(cid:105) at the point (cid:104)w∗, x(cid:105) and obtain

(cid:96) ((cid:104)w, γ(w − w∗)(cid:105) , y) ≥ (cid:96) ((cid:104)w, γ(w∗ − w∗)(cid:105) , y) + ∇(cid:104)w,x(cid:105)(cid:96) ((cid:104)w, γ(w∗ − w∗)(cid:105) , y) (w − w∗)T x +

(cid:107)(w − w∗)T x(cid:107)2

ζ2
2

which leads to

Combining pieces, we have

(cid:96) ((cid:104)w, γ(w − w∗)(cid:105) , y) ≥

γ2(cid:107)w − w∗(cid:107)4

ζ2
2

γ(cid:107)w − w∗(cid:107) ≤ β((cid:104)w,γ(w−w∗)(cid:105),y)γ.

(cid:114)

ζ1ζ2
2
1
(cid:107)w−w∗(cid:107)η ,

(cid:13)
(cid:13)β((cid:104)w,x(cid:105),y) − β((cid:104)w∗,x(cid:105),y)

(cid:13)
(cid:13) ≤ LR (cid:107)w − w∗(cid:107)

Then if we set γ = min (cid:8)(cid:113) 2
Lipschitz smooth w.r.t. (cid:104)w, x(cid:105) with parameter L, we have

R
(cid:107)w−w∗(cid:107)

ζ1ζ2

(cid:9), we can have 1

η ≤ β((cid:104)w,γ(w−w∗)(cid:105),y)γ. Because (cid:96) ((cid:104)w, x(cid:105) , y) is

Because β((cid:104)w∗,x(cid:105),y) = 0, we have the following inequality:
(cid:13)
(cid:13) ≤ LR (cid:107)w − w∗(cid:107)

(cid:13)
(cid:13)β((cid:104)w,x(cid:105),y)

If we multiply both side with γ, we can have

β((cid:104)w,x(cid:105),y)γ ≤ LR (cid:107)w − w∗(cid:107) γ

By setting γ as

1

LRη(cid:107)w−w∗(cid:107) , we arrive at β((cid:104)w,x(cid:105),y)γ < 1

η . Combining pieces, as long as we set

γ = min

(cid:26)(cid:114) 2
ζ1ζ2

1
η(cid:107)w − w∗(cid:107)

,

R
(cid:107)w − w∗(cid:107)

,

1
LRη (cid:107)w − w∗(cid:107)

(cid:27)

,

then we can have

0 < c ≤ β((cid:104)w,γ ˆx(cid:105),ˆy)γ ≤

1
η

.

where c is a non-zero positive constant. Therefore, we achieve the condition for the exponential synthesis-based teaching.

By the Proposition 5, the absolute loss and sqaure loss are exponentially teachable in synthesis-based case, and we can
obtain γ by plugging into the general form. We will tighten the γ up by analyzing absolute loss and square loss separately.
Besides that, we also show the commonly used loss functions for classiﬁcation, e.g., hinge loss and logistic loss, are also
exponentially teachable in synthesis-based teaching if (cid:107)w∗(cid:107) can be bounded.

Proposition 9 Absolute loss is exponentially teachable in synthesis-based teaching.

Proof To show one loss function is exponentially teachable in synthesis-based case, we just need to ﬁnd the appropriate γ
such that the learning intensity is bounded below and above, according to Theorem 4. For the absolute loss, i.e.,

its sub-gradient is

(cid:96) ((cid:104)w, x(cid:105) , y) = |(cid:104)w, x(cid:105) − y| ,

∇w(cid:96)((cid:104)w, x(cid:105) , y) = sign((cid:104)w, x(cid:105) − y)x,

and thus,

the learning intensity β((cid:104)w,x(cid:105),y) = sign ((cid:104)w, x(cid:105) − y). For w (cid:54)= w∗, plugging ˆx = γ (w − w∗) and

Iterative Machine Teaching

ˆy = (cid:104)w∗, γ (w − w∗)(cid:105) into the learning intensity, we have

βγ(cid:104)w,ˆx(cid:105),ˆyγ = sign (cid:0)γ2 (cid:104)w − w∗, w − w∗(cid:105)(cid:1) γ = γ.

Recall that γ (cid:54)= 0, |γ| ≤

R

(cid:107)wt−w∗(cid:107) , ∀t ∈ N, we have

γ ≤ min
t∈N

R
(cid:107)wt − w∗(cid:107)

:= C.

Set γ = min{C, 1
decreases in every step, we have C =

η }, we have ν = min{C, 1

R

η }. Therefore, we obtain the exponential decay. In fact, since the (cid:107)wt − w∗(cid:107)

(cid:107)w0−w∗(cid:107) . In following proof, we will follow the same argument to use this fact.

Proposition 10 Square loss is exponentially teachable in synthesis-based teaching.

Proof For square loss, i.e.,

its gradient is

(cid:96) ((cid:104)w, x(cid:105) , y) = ((cid:104)w, x(cid:105) − y)2 ,

∇w(cid:96) ((cid:104)w, x(cid:105) , y) = 2 ((cid:104)w, x(cid:105) − y) x,
and thus, the learning intensity β(cid:104)w,x(cid:105),y = 2 ((cid:104)w, x(cid:105) − y). For w (cid:54)= w∗, plugging ˆx = γ (w − w∗) and ˆy =
(cid:104)w∗, γ (w − w∗)(cid:105) into the learning intensity, we have

Set γ = min

√

1
2η(cid:107)wt−w∗(cid:107) ,

R
(cid:107)wt−w∗(cid:107)

(cid:110)

(cid:111)

, we achieve the exponential teachable condition.

β((cid:104)w,ˆx(cid:105),ˆy)γ = 2γ2 (cid:107)w − w∗(cid:107)2 .

Proposition 11 Hinge loss is exponentially teachable in synthesis-based teaching if (cid:107)w∗(cid:107) ≤ 1.

Proof For hinge loss, i.e.,

as long as 1 − y (cid:104)w, x(cid:105) > 0, its subgradient will be

(cid:96) ((cid:104)w, x(cid:105) , y) = max (1 − y (cid:104)w, x(cid:105) , 0) ,

∇w(cid:96) ((cid:104)w, x(cid:105) , y) = −yx.
Denote ˆx = γ (w − w∗), we have β(cid:104)w,ˆx(cid:105),ˆy = −ˆy where ˆy ∈ {−1, 1}. To satisfy the exponential teachable condition, we
need to select ˆy and γ such that






1 − ˆy (cid:104)w, ˆx(cid:105) > 0
0 < −ˆyγ ≤ 1
η
|γ| ≤

R
(cid:107)w−w∗(cid:107)

⇒






ˆyγ (cid:104)w, w − w∗(cid:105) < 1
− 1
|γ| ≤

η ≤ ˆyγ < 0

R
(cid:107)w−w∗(cid:107)

⇒






(cid:104)w, w − w∗(cid:105) > −1
− 1
η ≤ ˆyγ < 0
|γ| ≤

R
(cid:107)w−w∗(cid:107)

.

If (cid:107)w∗(cid:107) ≤ 1, we can show

where the last inequality comes from the fact 1 + a2 − a > 0, and thus, we have (cid:104)w, w − w∗(cid:105) > −1. Therefore, we select
any conﬁguration of ˆy and γ satisfying

(cid:104)w, w∗(cid:105) ≤ (cid:107)w(cid:107) (cid:107)w∗(cid:107) ≤ (cid:107)w(cid:107) < 1 + (cid:107)w(cid:107)2 ,

−

≤ ˆyγ < 0,

and

|γ| ≤

1
η

R
(cid:107)w − w∗(cid:107)

.

Particularly, we set ˆy = −1 and γ = min

(cid:110) 1
η ,

R
(cid:107)w0−w∗(cid:107)

(cid:111)
.

Proposition 12 Logistic loss is exponentially teachable in synthesis-based teaching if (cid:107)w∗(cid:107) ≤ 1.

Proof For the logistic loss, i.e.,

(cid:96) ((cid:104)w, x(cid:105) , y) = log (1 + exp(−y (cid:104)w, x(cid:105))) ,

its gradient is

Denote ˆx = γ (w − w∗), we have β(cid:104)w,ˆx(cid:105),ˆy = −
condition, we need to select ˆy and γ such that
(cid:40)

Iterative Machine Teaching

∇w(cid:96) ((cid:104)w, x(cid:105) , y) = −

yx
1 + exp(y (cid:104)w, x(cid:105))

.

0 < −

ˆyγ

1+exp(ˆy(cid:104)w,ˆx(cid:105)) ≤ 1

η

|γ| ≤

R
(cid:107)w−w∗(cid:107)

.

ˆy

1+exp(ˆy(cid:104)w,ˆx(cid:105)) where ˆy ∈ {−1, 1}. To satisfy the exponential teachable

Particularly, we set ˆy = −1, we can ﬁx the γ by

0 <

γ
1 + exp(γ)

<

γ
1 + exp(ˆy (cid:104)w, ˆx(cid:105))

1
η

,

≤ γ ≤

and

|γ| ≤

R
(cid:107)w − w∗(cid:107)

.

The

γ
1+exp(γ) <
we can choose γ = min

γ

1+exp(ˆy(cid:104)w,ˆx(cid:105)) is obtained by the monotonicity of exp(·) and (cid:104)w, w − w∗(cid:105) > −1 when (cid:107)w∗(cid:107). Therefore,

(cid:110) 1
η ,

R
(cid:107)w0−w∗(cid:107)

(cid:111)

, and thus, the lower bound ν =

γ
1+exp(γ) .

Proof of Corollary 6 In each update, given the training sample x ∈ span (X ), we have wt+1 = wt − ηβ(cid:104)w,x(cid:105),yx,
therefore, the ∆t+1w := wt+1 − w0 ∈ span (X ). If w0 − w∗ ∈ span (X ), wt+1 − w∗ ∈ span (X ), which means by linear
combination, we can construct ˆγ (cid:80)n
ixi = γ (wt − w∗). With the condition that the loss function is exponentially
synthesis-based teachable, we achieve the conclusion that the combination-based omniscient teacher will converge at least
exponentially with the same rate to the synthesis-based teaching.

i=1 αt

Proof of Theorem 8 The proof is similar to the synthesis-based case. However, we introduce the consideration of the
effect of pool-based teaching. Speciﬁcally, we ﬁrst obtain a virtual training sample in full space, and then, we generate the
sample from the candidate pool to mimic the virtual sample.

With the condition w0 − v∗ ∈ span (D), as we discussed in the proof of Corollary 6, in every iteration, wt − v∗ ∈ span (D).
Therefore, we only need to consider in the space of span (D). Meanwhile, since the teacher can rescale the sample, without
loss of generality, we assume if x ∈ X , then −x ∈ X to make the rescaling is always positive.

At t-step, as the loss is exponentially synthesis-based teachable with γ, therefore, we have the virtually constructed sample
{xv, yv} where xv = γ (wt − w∗) with γ satisfying the condition of exponentially teachable in synthesis-based settings,
we ﬁrst rescale the candidate pool X such that

∀x ∈ X , γx (cid:107)x(cid:107) = (cid:107)xv(cid:107) = γ (cid:13)

(cid:13)wt − w∗(cid:13)
(cid:13) .

We denote the rescaled candidate pool as Xt, under the condition of rescalable pool-based teachability, there is a sample
{ˆx, ˆy} ∈ X × Y with scale factor ˆγ such that

min
(x,y)∈Xt×Y

η2(cid:107)∇wt(cid:96) (cid:0)(cid:10)wt, x(cid:11) , y(cid:1) (cid:107)2 − 2η (cid:10)wt − w∗, ∇wt(cid:96) (cid:0)(cid:10)wt, x(cid:11) , y(cid:1)(cid:11)

≤ η2β2

(cid:104)wt,ˆγ ˆx(cid:105),ˆy (cid:107)ˆx(cid:107)2 − 2ηβ(cid:104)wt,ˆγ ˆx(cid:105),ˆy(cid:104)wt − w∗, ˆγ ˆx(cid:105).

We decompose the ˆγ ˆx = axv + xv ⊥ with a = (cid:104)ˆγ ˆx,xv(cid:105)

(cid:107)xv(cid:107)2 . and xv ⊥ = ˆγ ˆx − axv. Then, we have

min
(x,y)∈Xt×Y

η2(cid:107)∇wt(cid:96) (cid:0)(cid:10)wt, x(cid:11) , y(cid:1) (cid:107)2 − 2η (cid:10)wt − w∗, ∇wt(cid:96) (cid:0)(cid:10)wt, x(cid:11) , y(cid:1)(cid:11)

≤ η2β2

= η2β2

= η2β2

(cid:104)wt,ˆγ ˆx(cid:105),ˆy (cid:107)ˆx(cid:107)2 − 2ηβ(cid:104)wt,ˆγ ˆx(cid:105),ˆy(cid:104)wt − w∗, ˆγ ˆx(cid:105)
(cid:104)wt,ˆγ ˆx(cid:105),ˆyγ2 (cid:107)w − w∗(cid:107)2 − 2ηβ(cid:104)wt,ˆγ ˆx(cid:105),ˆy(cid:104)wt − w∗, axv + xv ⊥(cid:105)
(cid:104)wt,ˆγ ˆx(cid:105),ˆyγ2 (cid:107)w − w∗(cid:107)2 − 2ηβ(cid:104)wt,ˆγ ˆx(cid:105),ˆyγa (cid:13)

(cid:13)wt − w∗(cid:13)
2
(cid:13)

.

Under the condition

we denote ν (γ) = minw,ˆx∈X ,ˆy∈Y γβ(cid:104)w,γ w−w∗

ˆx

0 < γβ(cid:104)w,γ w−w∗

2V(X )
η
(cid:105),ˆy > 0 and µ (γ) = maxw,ˆx∈X ,ˆy∈Y γβ(cid:104)w,γ w−w∗

(cid:105),ˆy <

,

ˆx

ˆx

(cid:105),ˆy < 2V(X )

η

.

Iterative Machine Teaching

we have the recursion

with r(η, γ, V(X )) := max
fore, the algorithm converges exponentially

2 ≤ r(η, γ) (cid:13)
(cid:13)wt+1 − w∗(cid:13)
(cid:13)
2
(cid:13)
1 + η2µ (γ)2 − 2ηµ (γ) V(X ), 1 + η2ν (γ)2 − 2ην (γ) V(X )

(cid:13)wt − w∗(cid:13)
2
2 ,
(cid:13)

(cid:111)

(cid:110)

and 0 ≤ r(η, γ) < 1. There-

in other words, the student needs 2

log

clearity, we deﬁne the constant term as C η,γ,V(X )

= 2

log

2

1
r(η,γ,V(X ))

.

(cid:16)

samples to achieve an (cid:15)-approximation of w∗. For
(cid:17)−1

(cid:13)wt − w∗(cid:13)
(cid:13)

1
r(η,γ,V(X ))

(cid:13)2 ≤ r (η, γ)t/2 (cid:13)
(cid:17)−1
log (cid:107)w0−w∗(cid:107)

(cid:15)

(cid:16)

(cid:13)w0 − w∗(cid:13)

(cid:13)2 ,

B. Detailed Experimental Setting

Iterative Machine Teaching

Layer
Conv1.x
Pool1
Conv2.x
Pool2
Conv3.x
Pool3
FC1

CNN-6
[3×3, 16]×2

[3×3, 32]×2

[3×3, 64]×2

32

CNN-9
[3×3, 16]×3
2×2 Max, Stride 2
[3×3, 32]×3
2×2 Max, Stride 2
[3×3, 64]×3
2×2 Max, Stride 2
32

CNN-12
[3×3, 16]×4

[3×3, 32]×4

[3×3, 64]×4

32

Table 1. Our standard CNN architectures for CIFAR-10. Conv1.x, Conv2.x and Conv3.x denote convolution units that may contain
multiple convolution layers. E.g., [3×3, 16]×3 denotes 3 cascaded convolution layers with 16 ﬁlters of size 3×3. The CNNs learning
ends at 20K iterations with multi-step rate decay.

General Settings We have used three linear models in the experiments. In speciﬁc, the formulation of ridge regression
(RR) is

min
w∈Rd,b∈R

1
n

n
(cid:88)

i=1

1
2

(wT xi + b − yi)2 +

(cid:107)w(cid:107)2

λ
2

The formulation of logistic regression (LR) is
n
(cid:88)

min
w∈Rd,b∈R

1
n

The formulation of support vector machine (SVM) is

log(1 + exp{−yi(wT xi + b)}) +

λ
2

(cid:107)w(cid:107)2

i=1

1
n

n
(cid:88)

i=1

min
w∈Rd,b∈R

max(1 − yi(wT xi + b), 0) +

(cid:107)w(cid:107)2

λ
2

Comparison of different teaching strategies We use a linear regression model (ridge regression with λ = 0) for this
experiment. We set R as 1 and uniformly generate 30 data points as our knowledge pool for the teacher. In this ﬁrst case,
we set the feature dimension as 2, while in the second case, feature dimension is 70. The learning rate is set as 0.0001 for
pool-based teaching, same as BGD and SGD.

Experiments on Gaussian data Speciﬁcally, RR is run on training data (xi, y) where each entry in xi is Gaussian
distributed and y = (cid:104)w∗, xi(cid:105) + (cid:15). LR and SVM are run on {X1, +1} and {X2, −1} where xi ∈ X1 is Gaussian distributed
in each entry and +1, −1 are the labels. Speciﬁcally, we use the 10-dimension data that is Gaussian distributed with
(0.5, · · · , 0.5) (label +1) and (−0.5, · · · , −0.5) (label −1) as mean and identity matrix as covariance matrix. We generate
1000 training data points for each class. Learning rate for the same feature space is 0.0001, while learning rate for different
feature spaces are 0.00001. λ is set as 0.00005.

Experiments on uniform spherical data We ﬁrst generate the training data that are uniformly distributed on a unit
sphere (cid:107)xi(cid:107)2 = 1. Then we set the data points on half of the sphere ((0, π]) as label +1 and the other half ((π, 2π]) as label
−1. All the generated data points are 2D. For the scenario of different features, we use a random orthogonal projection
matrix to generate the teacher’s feature space from student’s. Learning rate for the same feature space is 0.001, while
learning rate for different feature spaces are 0.0001. λ is set as 0.00005.

Experiments on MNIST dataset We use 24D random features (projected by a random matrix R784×24) for the MNIST
dataset. The learning rate for all the compared methods are 0.001. Note that, we generate the teacher’s features using a
random projection matrix (R24×24) from the original 24D student’s features. λ is set as 0.00005.

Experiments on CIFAR-10 dataset The learning rate for all the compared methods are 0.001. λ is set as 0.00005. The
goal is to learn the R32×10 fully connected layer, which is also the classiﬁers for 10 classes. The three network we use in
the experiments are shown as follows:

Iterative Machine Teaching

Experiments on infant ego-centric dataset We manually crop and label all the objects that the child is holding for this
experiments. For feature extraction, we use VGG-16 network that is pre-trained on Imagenet dataset. Then we use PCA
to reduce the 4096 dimension to 64 dimension. We train a multi-class logistic regression to classify the objects. Note that,
the omniscient teacher is also applied to train the logistic regression model. The learning rate is set to 0.001 for both SGD
and omniscient teacher.

C. Comparison of different teaching strategies

Iterative Machine Teaching

Figure 8. Comparison of different teaching strategies.

We ﬁrst compare four different teaching strategies for the omniscient teacher. We consider two scenarios. One is that
the dimension of feature space is smaller than the number of samples (the given features are sufﬁcient to represent the
entire feature), and the other is that the feature dimension is greater than the number of samples (the given features are not
sufﬁcient to represent the entire feature). In these two scenarios, we ﬁnd that synthesis-based teaching usually works the
best and always achieves exponential convergence. The combination-based teaching is exactly the same as the synthesis-
based teaching in the ﬁrst scenario, but it is much worse than synthesis in the second scenario. Rescalable pool-based
teaching is also better than pool-based teaching. Empirically, the experiment veriﬁes our theoretical ﬁndings: the more
ﬂexible the teaching strategy is, the more convergence gain we may obtain.

Difference between w  and w*tIteration NumberIteration Number01002003004005000.20.40.60.811.21.4Batch gradient descentStochastic gradient descentSynthesis-based teachingPool-based teachingRescalable pool-based teachingCombination-based teaching0100200300400500012345678910Batch gradient descentStochastic gradient descentSynthesis-based teachingPool-based teachingRescalable pool-based teachingCombination-based teachingDifference between w  and w*t(a) Dimension of the feature space is smaller than the number of samples(b) Dimension of the feature space is greater than the number of samplesD. More experiments on MNIST dataset

Iterative Machine Teaching

We provide more experimental results on MNIST dataset. Fig. 9 shows the selected examples from 7/9 binary digit
classiﬁcation. The results further verify the teacher models tend to select easy examples at ﬁrst and gradually shift their
focuses to difﬁcult examples, very much resembling the human learning. Fig. 10 shows the difference between the
current model parameter and the optimal model parameter over iterations. It also shows that our teachers achieve faster
convergence.

Figure 9. Selected training examples during iteration. (7/9 classiﬁcation)

Figure 10. Teaching logistic regression on MNIST dataset. Left column: 0/1 classiﬁcation. Right column: 3/5 classiﬁcation

Iteration 1-40Iteration 601-640Iteration 1201-1240Iteration 1-40Iteration 601-640Iteration 1201-1240(a) Omniscient Teacher(b) Imitation Teacher0200400600800100012000.120.140.160.180.20.220.240.260.280.30.320200400600800100000.020.040.060.080.10.120.140.160.18Batch gradient descentStochastic gradient descentOmniscient TeachterSurrogate Teacher (same space)Surrogate Teacher (different space)Imitation TeacherDifference between w  and w*Iteration NumberDifference between w  and w*Iteration NumberttDifference between w  and w*tE. Teaching linear models on uniform spherical data

Iterative Machine Teaching

In this experiment, we use a different data distribution to further evaluate the teacher models. We will examine LR and
SVM by classifying uniform spherical data.

Teaching in the same feature space. From Fig. 11, one can observe that the convergence is consistently improved
while using omniscient teacher to provide examples to learners. We ﬁnd that the signiﬁcance of improvement is related
to the training data distribution and loss function, as indicated by our theoretical results. The surrogate teacher produces
less convergence gain in SVM, because the convexity lower bound becomes very loose in this case. Overall, omniscient
teacher still presents strong teaching capability. More interestingly, we use simple SGD run on the sample set selected by
the omniscient teacher and also get faster convergence, showing that the selected example set is better than the entire set in
terms of convergence.

Teaching in different feature spaces. While the teacher and student use different feature spaces, one can observe from
Fig. 11 that the surrogate teacher performs very poorly, even worse than the original SGD and BGD. The imitation teacher
works much better and achieves consistent and signiﬁcant convergence speedup, showing its superiority while the teacher
and the student use different features.

Figure 11. Convergence results on uniform spherical data.

Objective ValueDifference between w  and w*tIteration NumberIteration Number01000200030004000500060000.30.350.40.450.50.550.60.650.70.750.8Batch gradient descentStochastic gradient descentSGD on selected setOmniscient Teacher Surrogate teacher 01000200030004000500060000.811.21.41.61.822.22.42.62.8Objective ValueDifference between w  and w*Iteration NumberIteration Number0500100015002000250000.511.522.5050010001500200025000.10.20.30.40.50.60.70.80.911.1Batch gradient descentStochastic gradient descentSGD on selected setOmniscient TeacherSurrogate teacherObjective ValueDifference between w  and w*tIteration NumberIteration NumberObjective ValueDifference between w  and w*Iteration NumberIteration Number(a) Teaching logistic regression in the same feature space(b) Teaching logistic regression in different feature spaces(c) Teaching support vector machine in the same feature space(d) Teaching support vector machine in different feature spaces020040060080010000.20.30.40.50.60.70.80.911.1Batch gradient descentStochastic gradient descentSurrogate teacher (pool)Imitation teacher (pool)020040060080010000.40.60.811.21.41.61.822.205001000150020002500300000.511.522.530500100015002000250030000.20.250.30.350.40.450.50.550.60.650.7Batch gradient descentStochastic gradient descentSurrogate teacher (pool)Imitation teacher (pool)ttF. Object learning experiment on children’s ego-centric visual data

Iterative Machine Teaching

We experiment with a dataset capturing children and parents interacting with toys in a naturalistic setting (Yurovsky et al.,
2013). These interactions are recorded for around 10.5 minutes with a camera worn low on the child’s forehead. The head-
camera’s visual ﬁeld was 90 degrees wide, providing a broad view of objects visible to the infant. The camera was attached
to a headband that was tightened so that it did not move once set on the child. To calibrate the camera, the experimenter
noted when the child focused on an object and adjusted the camera until the object was in the center of the image in the
control monitor.

For our experiments, we selected interactions of 4 one year old infants. For each parent-child dyad, we annotated the
bounding box location and category of the toy attended to by the infant at each frame. There are 10 objects in total: doll
(34 frames), toy (53 frames), duck (335 frames), frog (2108 frames), helicopter (169 frames), horse (42 frames), mickey
(472 frames), phone (394 frames), sheep (119 frames) and tiger (266 frames). We use a VGG-16 network that is pre-trained
on Imagenet dataset as our feature extraction. We ﬁrst extract the 4096D features from these images and then use PCA to
reduce the dimension to 64D. Finally, we run our omniscient teacher on these ego-centric data.

One can observe from Fig. 12 that our omniscient teacher achieves faster convergence than the random teacher. Moreover,
we give part of the selected training examples of random teacher and omniscient teacher in Fig. 14 and Fig. 15, respectively.
We visualize the selected samples every 50 iterations from the ﬁrst iteration to the 10000th iteration. Interestingly, we ﬁnd
that the training samples that are selected by the omniscient teacher consist of contiguous bouts of experience with the
same object instance, unlike the random teacher. The adjacent samples are similar and the object changes in a smooth way.
These inputs are qualitatively similar in their ordering to the actual visual experiences of infants in our study, as illustrated
in Fig. 13. This can be seen as partial algorithmic conﬁrmation of the desirable structural properties of children’s natural
learning environment, which emphasizes a smooth and continuous evolution of visual experience, in sharp contrast to
random sample selection.

Figure 12. Convergence comparison on infant ego-centric visual data.

Figure 13. Training examples corresponding to the natural sequence of objects experienced by a single infant in our study.

02000400060008000100000.9511.051.11.151.21.251.31.351.41.45SGDOmniscient teacher02000400060008000100001.61.71.81.922.12.22.32.4SGDOmniscient Teacher02000400060008000100001.21.251.31.351.4SGDOmniscient Teacher02000400060008000100002.042.062.082.12.122.142.162.182.2SGDOmniscient TeacherIterationIterationIterationIterationObjective ValueObjective ValueObjective ValueObjective Value(a) Video of infant 1(b) Video of infant 2(d) Video of infant 4(c) Video of infant 3Iterative Machine Teaching

Figure 14. Training examples selected by the random teacher (Stochastic gradient descent).

Figure 15. Training examples selected by the omniscient teacher.

