Emulating the Expert: Inverse Optimization through Online Learning

Andreas Bärmann * 1 Sebastian Pokutta * 2 Oskar Schneider * 1

Abstract
In this paper, we demonstrate how to learn the
objective function of a decision maker while only
observing the problem input data and the deci-
sion maker’s corresponding decisions over mul-
tiple rounds. Our approach is based on online
learning techniques and works for linear objec-
tives over arbitrary sets for which we have a lin-
ear optimization oracle and as such generalizes
previous work based on KKT-system decompo-
sition and dualization approaches. The applica-
bility of our framework for learning linear con-
straints is also discussed brieﬂy. Our algorithm
converges at a rate of O( 1√
), and we demon-
T
strate its effectiveness and applications in prelim-
inary computational results.

1. Introduction

Human decision makers are very good at making decisions
under rather imprecise speciﬁcation of the decision-making
problem both in terms of constraints as well as objective.
One might argue that the human decision maker can pretty
reliably learn from observed previous decisions – a tradi-
tional learning-by-example setup. At the same time, when
we try to turn these decision-making problems into actual
optimization problems, we often run into all types of issues
in terms of specifying the model. In an optimal world, we
would be able to infer or learn the optimization problem
from previously observed decisions taken by an expert.

This problem naturally occurs in many settings where we
do not have direct access to the decision maker’s preference
or objective function but we can observe her behaviour and
the learner as well as the decision maker have access to
the same information. Natural examples are as diverse as

*Equal

contribution

1Friedrich-Alexander-Universität
Erlangen-Nürnberg, Erlangen, Germany 2Georgia Institute of
Technology, Atlanta, USA. Correspondence to: Andreas Bär-
mann <Andreas.Baermann@math.uni-erlangen.de>, Sebastian
Pokutta <Sebastian.Pokutta@isye.gatech.edu>, Oskar Schneider
<Oskar.Schneider@fau.de>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright
2017 by the author(s).

making recommendations based on user history and strate-
gic planning problems, where the agent’s preferences are
unknown but the system is observable. Other examples
include knowledge transfer from a human planner into a
decision-support system: often human operators have ar-
rived at ﬁnely-tuned ‘objective functions’ through many
years of experience, and in many cases it is desirable to
replicate the decision-making process both for scaling up
and also for potentially including it in large-scale scenario
analysis and simulation to explore responses under varying
conditions.

Here we consider the learning of preferences or objectives
from an expert by means of observing her actions. More
precisely, we observe a set of input parameters and corre-
sponding decisions of the form {(p1, x1), . . . , (pT , xT )}.
They are such that pt ∈ P with t = 1, . . . , T is a certain
realization of problem parameters from a given set P ⊆ Rk
and xt is an optimal solution to the optimization problem

max cT
truex
s.t. x ∈ X(pt),

where ctrue ∈ Rn is the expert’s true but unknown objective
and X(pt) ⊆ Rn for some (ﬁxed) n. We assume that we
have full information on the feasible set X(pt) and that we
can compute argmax {cT x | x ∈ X(pt)} for any candidate
objective c ∈ Rn and t = 1, . . . , T . We present an online
learning algorithm based on the multiplicative weights up-
date method that allows us to learn a strategy (c1, . . . , cT )
of subsequent objective function choices with the follow-
ing guarantee: if we optimize according to the surrogate
objective function ct instead of the actual unknown objec-
tive function ctrue in response to parameter realization pt,
we obtain a sequence of optimal decisions (w.r.t. to each ct)
given by

¯xt = argmax {cT

t x | x ∈ X(pt)},

that are essentially as good as the decisions xt taken by the
expert on average. To this end, we interpret the observa-
tions of parameters and expert solutions as revealed over
multiple rounds such that in each round t we are shown the
parameters pt ﬁrst, then take our optimal decision ¯xt ac-
cording to our objective function ct, then we are shown the
solution xt chosen by the expert and ﬁnally we are allowed

Emulating the Expert: Inverse Optimization through Online Learning

to update ct for the next round. For this setup, we will be
able to show that our algorithm attains an error bound of

0 ≤

(ct − ctrue)T (¯xt − xt) ≤ 2K

1
T

T
(cid:88)

t=1

(cid:114)

ln n
T

,

where K ≥ 0 is an upper bound on the (cid:96)∞-diameter of the
feasible regions X(pt) with t = 1, . . . , T . This implies that
both the deviations in true cost cT
true(xt − ¯xt) ≥ 0 as well
as the deviations in surrogate cost cT
t (¯xt − xt) ≥ 0 can be
made arbitrarily small on average. In other words, the av-
erage regret for having decided optimally according to the
surrogate objectives ct vs. having decided optimally for the
true objective ctrue vanishes at a rate of O( 1√
). This re-
T
sult shows that linear objective functions over general fea-
sible sets can be learned from relatively few observations of
historical optimal parameter-solutions pairs. We will also
brieﬂy discuss the case where the objective ctrue is known,
but some linear constraints are unknown in this paper.

Literature Overview

The idea of learning or inferring parts of an optimization
model from data is a reasonably well-studied problem un-
der many different assumptions and applications and has
gained signiﬁcant attention in the optimization commu-
nity over the last few years, as discussed for example in
(den Hertog & Postek, 2016), (Lodi, 2016), (Simchi-Levi,
2014). These papers argue that there would be signiﬁcant
beneﬁts in combining traditional optimization models with
data-derived components. Most approaches in the liter-
ature focus on deriving the objective function of an ex-
pert decision maker based on past observations of input
data and the decisions she took in each instance.
In al-
most all cases, the objective functions are learned by con-
sidering the KKT-conditions or the dual of the (parame-
terized) optimization problem, and as such convexity for
both the feasible region and the objective function is in-
herently assumed. Examples of this approach include (Ke-
shavarz et al., 2011), (Li, 2016) and (Thai & Bayen, 2016),
where the latter also consider the derivation of variational
inequalities from data. Sometimes also distributional as-
sumptions regarding the observations are made. Applica-
tions of such approaches have been heavily studied in the
context of energy systems (Ratliff et al., 2014; Konstanta-
kopoulos et al., 2016), robot motion (Papadopoulos et al.,
2016), (Yang et al., 2014), medicine (Sayre & Ruan, 2014)
and revenue management (Kallus & Udell, 2015; Qiang
& Bayati, 2016; Chen et al., 2015; Kallus & Udell, 2016;
Bertsimas & Kallus, 2016); also in the situation where the
observed decisions were not necessarily optimal (Nielsen
& Jensen, 2004). Very closely related to our learning ap-
proach in terms of the problem formulation is (Troutt et al.,
2005) which was later extended in (Troutt et al., 2006),
where an optimization model is deﬁned that searches for

a linear optimization problem that minimizes the total dif-
ference between the observed solutions and solutions found
by optimizing according to that optimization problem. In
the latter case, the models are solved using LP duality and
cutting planes respectively. In their follow-up work (Troutt
et al., 2008), a genetic algorithm is used to solve the prob-
lem heuristically under rather general assumptions, but in-
herently without any quality guarantees, and (Troutt et al.,
2011) study experimental setups for learning objectives un-
der various stochastic assumptions, focussing on maximal
likelihood estimation, which is generally the case for their
line of work; we make no such assumptions.

Closely related to learning optimization models from ob-
served data is the subject of inverse optimization. Here
the goal is to ﬁnd an objective function that renders the
observed solutions optimal with respect to the concur-
rently observed parameter realizations. Here approaches
mostly from convex optimization are used for inverse op-
timal control (Iyengar & Kang, 2005; Panchea & Ram-
dani, 2015; Molloy et al., 2016), inverse combinatorial op-
timization (D. Burton, 1997; Burton & Toint, 1994; 1992;
Sokkalingam et al., 1999; Ahuja & Orlin, 2000), integer in-
verse optimization (Schaefer, 2009) and inverse optimiza-
tion in the presence of noisy data, such as observed de-
cisions that were suboptimal (Aswani et al., 2016; Chan
et al., 2015).

All these approaches heavily rely on duality and thus re-
quire convexity assumptions both for the feasible region as
well as the objectives. As such, they cannot deal with more
complex, possibly non-convex decision domains. This in
particular includes the important case of integer-valued de-
cisions (such as ‘yes/no’-decisions) and also many other
non-convex setups (several of which admit efﬁcient linear
optimization algorithms). Previously, this was only possi-
ble when the structure of the feasible set could be benef-
ically exploited. In contrast, our approach does not make
any such assumptions and only requires access to a linear
optimization oracle (in short: LP oracle) for the feasible
region.

Also related to our work is inverse reinforcement learning
and apprenticeship learning, where the reward function is
the target to be learned. However, in this case the under-
lying problem is modeled as a Markov decision process
(MDP); see e.g. (Syed & Schapire, 2007) and (Ratia et al.,
2012). Typically, the guarantees are of a different form
though. Similarly, our work is not to be confused with that
of (Taskar et al., 2005) and (III et al., 2005), who develop
online algorithms for learning aggregation vectors for edge
features in graphs that use inverse optimization as a sub-
routine to deﬁne the update rule. In contrast, we do inverse
optimization by means of an online learning algorithm; ba-
sically the reverse setup.

Emulating the Expert: Inverse Optimization through Online Learning

Our approach is based on online learning, and we use the
simple EXP algorithm here to attain the stated regret bound.
The EXP algorithm is commonly also called Multiplicative
Weights Update (MWU) algorithm and was developed in
the works of (Littlestone & Warmuth, 1994), (Vovk, 1990)
and (Freund & Schapire, 1997) (see (Arora et al., 2012;
Hazan, 2016) for a comprehensive introduction; see also
(Audibert et al., 2013)). A similar algorithm was used by
(Plotkin et al., 1995) for solving fractional packing and
covering problems. We also note that our feedback is
stronger than bandit feedback. This requirement is not un-
expected as the costs chosen by the ‘adversary’ depend on
our decision; as such the bandit model (see e.g. (Dani et al.,
2008), (Abbasi-Yadkori et al., 2011)) does not readily ap-
ply.

Contribution

To the best of the authors’ knowledge, this paper makes the
ﬁrst attempt to learn the objective function of an optimiza-
tion model from data using an online learning approach.

Online learning of optimization problems. Based on sam-
ples for the input-output relationship of an optimization
problem solved by a decision maker, our aim is to learn
an objective function which is consistent with the observed
input-output relationship; this is the best one can hope for.
In our setup, the expert solves the decision-making prob-
lem repeatedly for different input parameter realizations.
From these observations, we are able to learn a strategy of
objective functions that emulate the expert’s unknown ob-
jective function such that the difference in solution quality
between the solutions converges to zero on average.

While previous methods based on dualization or KKT-
system-based approaches can lead to similar or even
stronger results in the continuous/convex case, online
learning allows us to relax this convexity requirement and
to work with arbitrary decision domains as long as we are
able to optimize a linear function over them. Thus, we do
not explicitly analyze the KKT-system or the dual program
(in the case of LPs; see Remark 3.1).
In particular, one
might consider our algorithm as an algorithmic analogue
of the KKT-system (or dual program) in the convex case.

To summarize, we stress that
(a) we do not make any
assumptions regarding distribution of the observations,
(b) the observations can be chosen by a fully-adaptive ad-
versary, and (c) we do not require any convexity assump-
tions regarding the feasible regions and only rely on ac-
cess to an LP oracle. We would also like to mention that
our approach can be extended to work with slowly chang-
ing objectives using appropriate online learning algorithms
such as, for example, (Jadbabaie et al., 2015) or (Zinkevich,
2003); the regret bounds will depend on the rate of change.

Preliminary Computational Tests. While a full computa-
tional study is beyond the scope of this paper and left for
future work, we implemented a ﬁrst preliminary version of
our algorithm, and we report computational results for a
few select problems.

2. Problem Setting

We consider the following family of optimization prob-
lems (OPT(p))p, which depend on a parameter p ∈ P ⊆
Rk for some k ∈ N:

max cT
truex
s.t. x ∈ X(p),

where ctrue ∈ Rn is the objective function and X(p) ⊆
Rn is the feasible region, where the latter depends on the
parameter p. Of particular interest to us will be feasible
regions that arise as polyhedra deﬁned by linear constraints
and their intersections with integer lattices:

X(p) = {x ∈ Zn−l × Rl | A(p)x ≤ b(p)}

with A(p) ∈ Rm×n and b(p) ∈ Rm. However, our ap-
proach can also readily by applied in the case of more com-
plex feasible regions, such as mixed-integer sets bounded
by convex functions:

X(p) = {x ∈ Zn−l × Rl | G(p, x) ≤ 0}

with G : P × Zn−l × Rl → R convex – or even more
general settings. In fact, for any possible choice of model
for the set of feasible decisions, we only require the avail-
ability of a linear optimization oracle capable of optimizing
linear functions over X(p) for any p ∈ P . We call a de-
cision x ∈ Rn optimal for p if it is an optimal solution to
OPT(p).

We assume that Problem OPT(p) models a parameterized
optimization problem which has to be solved repeatedly for
various input parameters p. Our task is to learn the ﬁxed ob-
jective function ctrue from given observations of the param-
eter p and a corresponding optimal solution x to OPT(p).
To this end, we further assume that we are given a series of
observations ((pt, xt))t of parameter realizations pt ∈ P
together with an optimal solution xt to OPT(pt) computed
by the expert for t = 1, . . . , T ; these observations are re-
vealed over time in an online fashion: in round t, we ob-
tain a parameter setting pt and compute an optimal solu-
tion ¯xt ∈ X(pt) with respect to an objective function ct
based on what we have learned about ctrue so far. Then
we are shown the solution xt the expert with knowledge of
ctrue would have taken and can use this information to up-
date our inferred objective function for the next round. In
the end, we would like to be able to use our inferred objec-
tive function to take decisions that are essentially as good

Emulating the Expert: Inverse Optimization through Online Learning

as those chosen by the expert in an appropriate aggregation
measure such as, for example, ‘on average’ or ‘with high
probability’. The quality of the inferred objective is mea-
sured in terms of cost deviation between our solutions ¯xt
and the solutions xt obtained by the expert. Details will
be given in the next section, where we will derive an algo-
rithm based on multiplicative weights updates (MWU) to
solve the above task.

To ﬁx some useful notations, let v(i) denote the i-th com-
ponent of a vector v throughout and let [n] := {1, . . . , n}
for any natural number n. Further, let 1n := (1, . . . , 1)
denote the all-ones vector in Rn.

As a technical assumption, we further demand (cid:107)ctrue(cid:107)1 =
1 and ctrue ≥ 0. Both requirements are without loss of
generality and are motivated by our choice of MWU as the
online learning algorithm to simplify the exposition. We
can drop these assumptions by employing, for example, the
Online Gradient Descent algorithm of (Zinkevich, 2003),
which requires an explicit projection step.

3. Learning Objectives

Ideally, we would like to ﬁnd the true objective func-
tion ctrue as a solution to the following optimization prob-
lem:

(cid:18)(cid:18)

(cid:88)

min
c∈Rn
+ :
(cid:107)c(cid:107)1=1

t∈[T ]

max
x∈X(pt)

(cid:19)

(cid:19)

cT x

− cT xt

,

(1)

where xt ∈ X(p) is the optimal decision taken by the
expert in round t. The (normalized) true objective func-
tion ctrue ≥ 0 is an optimal solution to Problem (1) with
objective value 0. This is because any solution ˆc with
(cid:107)ˆc(cid:107)1 = 1 is feasible and produces non-negative summands

(cid:18)

max
x∈X(pt)

(cid:19)

ˆcT x

− ˆcT xt,

t ∈ [T ],

as we assume xt ∈ X(pt) to be optimal for pt with respect
to ctrue. Thus, the optimal value of (1) is bounded from
below by 0.

When solving Problem (1) we are interested in an objective
function vector c ∈ Rn that delivers a consistent explana-
tion for why the expert chose xt as his response to the pa-
rameters pt in round t = 1, . . . , T . To be more precise, we
want to minimize the deviation between the optimal value
obtained when optimizing over X(pt) with our guess for
the objective function c and the value of the expert’s deci-
sion evaluated according to our guess c, averaged over all
observations. Our algorithm presented here will provide
even stronger guarantees in some cases, such as the one de-
scribed in Section 3.1, showing that we can replicate the
decision-making behavior of the expert.

Problem (1) contains T instances of the following maxi-
mization subproblem:

max cT x
s.t. x ∈ X(pt)

(2a)

(2b)

For each t = 1, . . . , T , the corresponding Subproblem (2)
asks for an optimal solution ¯xt when optimizing over the
feasible set X(pt) with our guess c as the objective func-
tion.

Remark 3.1. Note that in the case of polyhedral feasible
regions, i.e. pt = (At, bt) ∈ Rm×n × Rm and X(pt) =
{x ∈ Rn
+ | Atx ≤ bt} for t = 1, . . . , T , Problem (1)
can be reformulated as a linear program by dualizing the
T instances of Subproblem (2). This yields

T
(cid:88)

min

(bT

t yt − cT xt)

t=1
s.t. AT

t yt ≥ c
yt ≥ 0

(∀t = 1, . . . , T )

(∀t = 1, . . . , T )

n
(cid:88)

i=1

ci = 1

c ≥ 0,

where the yt are the corresponding dual variables and the
xt are the observed decisions from the expert (i.e. the latter
are part of the input data). This problem asks for a primal
objective function vector c that minimizes the total dual-
ity gap summed over all primal-dual pairs (xt, yt) while
all yt’s shall be dual feasible, which makes the xt’s the re-
spective primal optimal solutions. Thus, Problem (1) can
be seen as a direct generalization of the linear primal-dual
optimization problem.
In fact, our approach also covers
non-convex cases, e.g. mixed-integer linear programs.

Problem (1) can be interpreted as a game over T rounds
between a player who chooses an objective function ct in
round t ∈ [T ] and a player who knows the true objective
function ctrue and chooses the observations (pt, xt) in a po-
tentially adversarial way. The payoff of the latter player in
each round t is equal to cT
t (¯xt − xt) ≥ 0, i.e. the differ-
ence in cost between our solution and the expert’s solution
as given by our guessed objective function ct.

As Problem (1) is hard to solve in general, we will
design an algorithm that,
rather than ﬁnding an op-
timal objective c, ﬁnds a strategy of objective func-
tions (c1, c2, . . . , cT ) to play in each round whose error in
solution quality as compared to the true objective function
is as small as possible. Our aim will then be to give a qual-
ity guarantee for this strategy in terms of the number of
observations.

t x | X(pt)} {solve Subprob-

Substituting back for the yt’s and using

Emulating the Expert: Inverse Optimization through Online Learning

To allow for approximation guarantees, it is necessary that
the observed feasible sets have a common upper bound on
their (cid:96)∞-diameter.
Deﬁnition 3.2. The (cid:96)∞-diameter of a set S ⊆ Rn, de-
noted by diam∞(S) is the largest distance between any
two points x1, x2 ∈ S, measured in the inﬁnity-norm, i.e.
diam∞(S) := maxx1,x2∈S(cid:107)x1 − x2(cid:107)∞.

In the following, we assume that there exists a K ≥ 0 with
diam∞(X(pt)) ≤ K for all t = 1, . . . , T . With these
prerequisites, our application of multiplicative weights up-
dates (MWU) to learn the objective function of an opti-
mization problem proceeds as outlined in Algorithm 1.

1
T

T
(cid:88)

t=1

and further

comparison between the objective function ct chosen in
each round t with the unknown true objective function ctrue:

T
(cid:88)

t=1

T
(cid:88)

t=1

cT
t yt ≤

cT
true(yt + η|yt|) +

ln n
η

,

where the |yt| is to be understood component-wise. Using
that each each entry of |yt| is at most 1 and dividing by T ,
we can conclude

cT
t yt −

cT
trueyt ≤ η

ctrue(i) +

n
(cid:88)

i=1

ln n
ηT

1
T

T
(cid:88)

t=1

1
T

T
(cid:88)

t=1

1
T

T
(cid:88)

t=1

cT
t yt −

cT
trueyt = η +

ln n
ηT

.

The right-hand side attains its minimum for η =
which yields the bound

(cid:113) ln n
T ,

1
T

T
(cid:88)

t=1

cT
t yt −

cT
trueyt ≤ 2

1
T

T
(cid:88)

t=1

(cid:114)

ln n
T

.

max
t=1,...,T

(cid:107)¯xt − xt(cid:107)∞ ≤ max
t=1,...,T

diam∞(X(pt)) ≤ K,

we obtain

T
(cid:88)

1
T

T
(cid:88)

1
T

cT
t (¯xt − xt) +

cT
true(xt − ¯xt) ≤ 2K

t=1

t=1
Observe that for each summand t ∈ [T ] we have cT
t (¯xt −
xt) ≥ 0 as ¯xt, xt ∈ X(pt) and ¯xt is the maximum over this
set with respect to ct. With a similar argument, we see that
cT
true(xt − ¯xt) ≥ 0 for all t ∈ [T ]. Thus, we have

(cid:114)

ln n
T

.

0 ≤

(ct − ctrue)T (¯xt − xt) ≤ 2K

(3)

(cid:114)

ln n
T

,

1
T

T
(cid:88)

t=1

and similarly for the separate terms with analogue argu-
mentation. This establishes the claim.

Note that by using exponential updates of the form

wt+1(i) ← wt(i)e−ηyt(i)

in line 11 of the algorithm, we could attain the same bound,
cf. (Arora et al., 2012, Theorem 2.3). Secondly, we remark
that our choice of the learning rate η requires the number
of rounds T to be known beforehand.

From the above theorem, we can conclude that the average
error over all observations (pt, xt) for t = 1, . . . , T when
choosing objective function ct in iteration t of Algorithm 1
instead of ctrue converges to 0 with an increasing number of
observations T at a rate of roughly O( 1√
T

):

Algorithm 1 Online Objective Function Learning

(cid:113) ln n

input observations (pt, xt) for t = 1, . . . , T
output sequence of objectives c1, c2, . . . , cT
1: η ←
T {set learning rate}
2: w1 ← 1n {initialize weights}
3: for t = 1, . . . , T do
ct ← wt
4:
(cid:107)wt(cid:107)1
¯xt ← argmax {cT
lem (2)}
if ¯xt = xt then

{normalize weights}

5:

else

yt ← 0

yt ← ¯xt−xt

6:
7:
8:
9:
10:
11:
12: end for
13: return (c1, c2, . . . , cT ).

(cid:107)¯xt−xt(cid:107)∞

end if
wt+1(i) ← wt(i)(1 − ηyt(i)) {update weights}

For the series of objectives functions (ct)t that our algo-
rithm produces over rounds t = 1, . . . , T , we can establish
the following guarantee:

Theorem 3.3. Let K ≥ 0 with diam∞ X(pt) ≤ K for all
t = 1, . . . , T . Then we have

0 ≤

(ct − ctrue)T (¯xt − xt) ≤ 2K

1
T

T
(cid:88)

t=1

(cid:114)

ln n
T

,

and in particular it also holds:

1. 0 ≤ 1
T

(cid:80)T

t=1 cT

t (¯xt − xt) ≤ 2K

2. 0 ≤ 1
T

(cid:80)T

t=1 cT

true(xt − ¯xt) ≤ 2K

(cid:113) ln n
T ,

(cid:113) ln n
T .

Proof. According to the standard performance guarantee
of MWU (see, e.g., (Arora et al., 2012), Corollary 2.2),
Algorithm 1 attains the following solution quality for the

Emulating the Expert: Inverse Optimization through Online Learning

Corollary 3.4. Let K ≥ 0 with diam∞ X(pt) ≤ K for all
t = 1, . . . , T . Then we have

argmin {cT x | x ∈ X(pt)} so that for xt (cid:54)= ¯x we have

1. limT →∞

(cid:80)T

t=1 cT

t (¯xt − xt) = 0

and

2. limT →∞

(cid:80)T

t=1 cT

true(xt − ¯xt) = 0.

1
T

1
T

In other words, both the average error incurred from replac-
ing the actual objective function ctrue by the estimation ct
as well as the average error in solution quality with respect
to ctrue tend to 0 as T grows.

Moreover, using Markov’s inequality we also obtain the
following quantitative bound on the deviation by more than
ε > 0 from the average cost:

Corollary 3.5. Let ε > 0. Then the fraction of observa-
tions xt with cT
T + ε is at most
true(xt − ¯xt) ≥ 2K
√
. In particular, for any 0 < p < 1 after
1 −

(cid:113) ln n

2K

T ≥ ln n

ε
ln n
T +ε
(cid:16) (1−p)2K
pε

(cid:17)2

vations xt with cost cT
is at most p.

observations the fraction of obser-

true(xt − ¯xt) ≥ ε

1−p ≥ 2K

(cid:113) ln n

T + ε

Proof. The ﬁrst part is an obvious application of Markov’s
inequality. The second part follows from solving 1 −
≤ p for T and plugging in values.

√

ε
ln n
T +ε

2K

3.1. The Stable Case

Note that limT →∞(ct − ctrue)T (¯xt − xt) = 0, as de-
rived from Equation (3) does not necessarily imply that we
can approximate ctrue itself. A counterexample is the case
where X(pt) ⊆ {x ∈ Rn | x(1) = 0}, which means that
any two objective functions c1, c2 (cid:54)= 0 with c2(i) = κc1(i)
for i = 2, . . . , n and 0 < κ ≤ 1 are equivalent if
c1(1), c2(1) are chosen such that (cid:107)c1(cid:107)1 = (cid:107)c2(cid:107)1 = 1.
Using this construction, we can easily ﬁnd examples for
which (cid:107)c1 − c2(cid:107)1 = 2(1 − κ), but where the two objective
functions are equivalent in terms of optimal solutions.

While in most applications it is sufﬁcient to be able to pro-
duce solutions via the surrogate objectives that are essen-
tially equivalent to those for the true objective, we will
show now that under slightly strengthened assumptions we
can obtain signiﬁcantly stronger guarantees for the conver-
gence of the solutions: we will show that in the long run
we learn to emulate the true optimal solutions provided that
the problems have unique solutions as we will make precise
now.

cT
true(xt − ¯xt) ≥ ∆,

i.e. either the two optimal solutions coincide or they differ
by at least ∆ with respect to ctrue. In particular, optimizing
ctrue over X(pt) leads to a unique optimal solution for all
pt with t ∈ [T ]. While this condition sounds unnatural at
ﬁrst, for example it is trivially satisﬁed for the important
case where X(pt) with t ∈ [T ] is a polytope with ver-
tices in {0, 1} and ctrue is a rational vector. In this case,
write ctrue = d
+ and observe that the min-
(cid:107)d(cid:107)1
imum change in objective value between any two vertices
x, y of the 0/1-polytope with cT
truey is bounded by
, so that ∆-stability with ∆ := 1
|cT
(cid:107)d(cid:107)1
holds in this case. The same argument works for more gen-
eral polytopes via bounding the minimum non-zero change
in objective function value via the encoding length.

true(x − y)| ≥ 1
(cid:107)d(cid:107)1

with d ∈ Zn

truex (cid:54)= cT

We obtain the following simple corollary of Theorem 3.3.

Corollary 3.6. Let K ≥ 0 with diam∞ X(pt) ≤ K for all
t = 1, . . . , T , let (X(pt))t be ∆-stable for some ∆ > 0,
and let NT := {t ∈ [T ] | ¯xt (cid:54)= xt}. Then

|NT | ≤ 2K

(cid:114)

T ln n
∆

.

(cid:80)T

(cid:113) ln n

t=1 cT

Proof. We start with the guarantee from the proof of Theo-
rem 3.3: 0 ≤ 1
true(xt − ¯xt) ≤ 2K
T . Now let
T
(cid:80)
NT be as above so that 0 ≤ 1
cT
true(xt − ¯xt) ≤
T
2K
true(xt − ¯xt) as xt was
optimal for ctrue together with ∆-stability. We thus ob-
tain 1
T , which is equivalent to |NT | ≤

T . Observe that ∆ ≤ cT

(cid:113) ln n

(cid:113) ln n

t∈NT

T |NT |∆ ≤ 2K
(cid:113) T ln n
∆ .

2K

(cid:113) ln n

T |NT | ≤ 2K

From the above corollary, we obtain in particular that in
the ∆-stable case we have 1
T ∆ , i.e. the
average number of times that ¯xt deviates from xt tends to
0 in the long run. We hasten to stress, however, that the
convergence implied by this bound can potentially be slow
as it is exponential in the actual encoding length of ctrue;
this is to be expected given the convergence rates of our
algorithm and online learning algorithms in general.

3.2. Learning Constraints

We say that the sequence of feasible regions (X(pt))t
is ∆-stable for ctrue for some ∆ > 0 if for any t ∈
[T ], c ∈ Rn with (cid:107)c(cid:107)1 = 1, c (cid:54)= ctrue and ¯xt
:=

We will only very brieﬂy address the case of learning con-
straints due to space limitations. We consider the family of
optimization problems (OPT2(p))p, p ∈ P ⊆ Rk, given

Emulating the Expert: Inverse Optimization through Online Learning

by

at time t. The customer solves the following optimization
problem OPT(pt) on day t:

max c(p)T x
s.t. Ax ≤ btrue

x ≥ 0,

where c(p) ∈ Rn is the objective function, A ∈ Rm×n is
the constraint matrix and btrue ∈ Rm is the right-hand side.

We assume that the ct’s are known to both the learner and
the expert. The same can be assumed for A without loss of
generality by standard arguments. The right-hand side btrue
is only known to the expert and to be learned from observ-
ing pt as well as an optimal solution xt for OPT2(pt) in
round t.

The most natural approach for solving this learning prob-
lem is to apply Algorithm 1 to the dual of OPT2(pt):

truey

min bT
s.t. AT y ≥ c(pt)

y ≥ 0,

where y are the dual variables for the linear constraints. In
the dual problem, btrue is the unknown objective function
(btrue ≥ 0 without loss of generality), while the constraints
to be optimized over in each round are known – the same
setting as before.
It is important to note though that the
learner has to observe the dual optimal solutions yt and
the guarantee will be that the dual regret is tending to 0.
It remains open whether this can be also achieved when
receiving only the primal optimal solutions xt as feedback;
we suspect the answer to be in the negative in general.

4. Applications

We will now sketch two select applications of our frame-
work for learning objective functions. These are the learn-
ing of customer preferences from observed purchases and
the learning of travel times in a road network.

Our preliminary computational experiments have been ob-
tained on a Mac Book Pro (2016) with an Intel Core i5 CPU
with two 2.00 GHz cores. We have implemented our frame-
work using python and Gurobi 7.0.1 (Gurobi Optimization,
Inc., 2016).

4.1. Learning Customer Preferences

We consider a market, where different goods G can be
bought by its customers. The prices for the goods can vary
over different days t ∈ [T ]. We assume that the goods are
chosen by the customer to maximize utility given their bud-
get constraints. Each sample (pt, xt) corresponds to a day
t ∈ [T ] where pt = (pt0, ptG) with pt0 is the budget of
the buyer and pG
t contains the prices ptg for each good g

max

ugxg

(cid:88)

g∈G
(cid:88)

g∈G

s.t.

ptgxg ≤ pt0

x ∈ {0, 1}|G|,

where the utility ug of good g of the customer is unknown
(and kept constant over time).

We consider two different setups: in the ﬁrst setup, we as-
sume that the goods are divisible, which means that the
condition x ∈ {0, 1}|G| is relaxed to x ∈ [0, 1]|G|; this
is the Linear Knapsack Problem. In the second setup, the
goods are not divisible, so that we solve the problem with
the original constraint x ∈ {0, 1}|G| as an integer program;
this is the Integer Knapsack Problem.

We generated random instances for our computational re-
sults, considering T = 1000 observations for a varying
number of goods n ∈ {100, 500, 1000}. The customer’s
unknown utility vector is chosen at random as (arbitrary)
integer numbers from the interval [1, 1000] from a uniform
distribution and then normalized to have (cid:96)1-norm 1. The
prices on day t are chosen to be ptg = ug + 100 + rtg,
where rtg is an integer uniformly chosen at random from
the interval [−10, 10]. Choosing utilities and weights sim-
ilar to each other typically leads to harder (integer) knap-
sack problems, cf. (Pisinger, 2005). The right-hand side
pt0 is then again an integer drawn uniformly from the in-
terval [1, (cid:80)

g∈G ptg − 1].

Table 1 Errors for Integer Knapsack with n = 1000 items

Error / T
Average objective error
Average solution error
Average cumulative error

10
0.1511
0.0492
0.2003

100
0.0185
0.0087
0.0272

1000
0.0036
0.0013
0.0049

In Table 1, we show the computational results for the Inte-
ger Knapsack Problem with n = 1000 items. We report
errors for 10, 100 and 1000 iterations of our algorithm.
The objective error for each round t ∈ [T ] is deﬁned by
cT
t ( ¯xt − xt) and describes the deviation between the solu-
tion ¯xt found by the oracle in that round and the solution
xt observed from the expert as evaluated with our guess for
the objective function ct. Accordingly, the solution error
in each round t is deﬁned as cT
true( ¯xt − xt) and evaluates
the deviation between the two solutions in the true objec-
tive function. Together they yield the total error, given by
(ct − ctrue)T (¯xt − xt), which is the total deviation between
choosing ct and ctrue in Problem (1) in round t. Each of
these error types is shown in our plots over time, depict-
ing both the error in a given round t and the average error

Emulating the Expert: Inverse Optimization through Online Learning

Figure 1. Linear Knapsack problem with n = 100 items over
t=1(ct − ctrue)T (¯xt − xt) over
T = 1000 iterations. We plot 1
T
T on the x-axis in blue. In red we plot the cost (ct − ctrue)T (¯xt −
xT ) of round t. As can be seen, after few iterations most solutions
reside on the x-axis and only few deviate beyond the average.

(cid:80)T

Figure 2. Resource-constrained shortest path problem on a grid
graph with m = 15 rows and n = 30 columns as described above
for T = 1000 iterations. Total error as in Figure 1. Convergence
is slower here (although with an error that is several orders of
magnitude smaller) as the problem is much more complex. Still,
in most rounds we have an error close to 0.

over rounds t(cid:48) ∈ [t]. We depict a representative knapsack
instance in Figure 1.

4.2. Learning Travel Times

While the ﬁrst example explored learning over a temporal
horizon, in this example the various observations t ∈ [T ]
arise from different drivers in a road network. More pre-
cisely, we consider a resource-constrained shortest path
problem, where we are given a graph G = (V, E), where
drivers have to ﬁnd cost-minimal s-t-paths subject to a re-
source or budget constraint. For each arc e ∈ E, we denote
the arc length with ae. The observations t ∈ [T ] repre-
sent drivers in the network and each observation (pt, xt)
consists of pt = (p1
t is the starting point
and p2
t is the ending point of the journey of driver t.
It
is assumed that driver t takes the path with the shortest
travel time with respect to the unknown travel times ce with
e ∈ E while at the same time being subject to a limit p3
t of
total distance that can be traveled. The values of xt indi-
cate the traversed edges of the graph that driver t takes. The
optimization problem OPT(pt) solved by driver t is then:

t ), where p1

t , p3

t , p2

min

cexe

(cid:88)

e∈E

s.t.

(cid:88)

(cid:88)

xe −

xe =

e∈δ−(v)

e∈δ+(v)






if v = p1
−1,
t
if v = p2
1,
t
0, otherwise

(∀v ∈ V )

aexe ≤ p3
t

(cid:88)

e∈E

x ∈ {0, 1}|E|,

and we want to learn the values ce for e ∈ E corresponding
to the travel time to traverse arc e.

We created instances of the problem based on grid graphs
with m rows and n columns. The unknown driving times
vector of the network and the resource value for each arc
(in our case the length of the arc) were chosen at random
in the same fashion as for the knapsack problems. For each
sample, we chose a random pair of an origin and a destina-
tion node. The resource limit for the sample is calculated as
the length of the shortest path between the selected nodes
multiplied by 1.25. In other words: ﬁnd the fastest path
while driving at most 25% more than length of the shortest
path. See Figure 2 for our result over T = 1000 samples.

Additional computations are included in the Appendix.

5. Final Remarks

In its current form, we explicitly use the optimality of the
observed actions to learn the objective function. While be-
yond the scope of this paper, it would be interesting to anal-
yse to what extent this optimality requirement can be re-
laxed to approximate solutions. Clearly, one can simply re-
deﬁne the underlying feasible regions X(pt) to correspond
to the approximate feasible regions, however this can lead
to unwanted effects of the summands in our regret bound
not being non-negative anymore if the solutions obtained
for the surrogate objective is better than the approximately
optimal observed solution. Another important question is
to what extent our framework can be extended to the learn-
ing of constraints (and objective functions simultaneously).

11002003004005006007008009001000Round0.00.10.20.30.4Total erroraverageper round11002003004005006007008009001000Round0.0000.0020.0040.0060.0080.010Total erroraverageper roundEmulating the Expert: Inverse Optimization through Online Learning

Acknowledgements

We would like to thank the reviewers for the helpful com-
ments. Research reported in this paper was partially sup-
ported by NSF CAREER award CMMI-1452463.

References

Abbasi-Yadkori, Yasin, Pál, Dávid, and Szepevári, Csaba.
In
Improved algorithms for linear stochastic bandits.
Conference on Neural Information Processing System
(NIPS), 2011.

Ahuja, Ravindra K. and Orlin, James B. A faster algorithm
for the inverse spanning tree problem. Journal of Algo-
rithms, pp. 177–193, 2000.

Arora, Sanjeev, Hazan, Elad, and Kale, Satyen. The multi-
plicative weights update method: A meta-algorithm and
applications. Theory of Computing, 8:121–164, 2012.

Aswani, Anil, Shen, Zuo-Jun Max, and Siddiq, Auyon. In-
verse optimization with noisy data. Technical report,
University of California, Berkeley, 2016. available at:
https://arxiv.org/abs/1507.03266.

Audibert, Jean-Yves, Bubeck, Sébastien, and Lugosi, Gá-
bor. Regret in online combinatorial optimization. Math-
ematics of Operations Research, 39(1):31–45, 2013.

Bertsimas, Dimitris and Kallus, Nathan. Pricing from ob-
servational data. Technical report, Massachusetts In-
available at:https://
stitute of Technology, 2016.
arxiv.org/pdf/1605.02347.

Burton, D. and Toint, Ph. L. On an instance of the inverse
shortest paths problem. Mathematical Programming, 53
(1):45–61, 1992.

Burton, D. and Toint, Ph. L. On the use of an inverse
shortest paths algorithm for recovering linearly corre-
lated costs. Mathematical Programming, 63(1):1–22,
1994.

Chan, Timothy C. Y., Lee, Taewoo, and Terekhov, Daria.
Goodness of ﬁt in inverse optimization. Technical re-
port, University of Toronto, Canada, 2015. available at:
https://arxiv.org/abs/1511.04650.

Dani, Varsha, Hayes, Thomas P., and Kakade, Sham. M.
Stochastic linear optimization under bandit feedback. In
Conference on Learning Theory (COLT), 2008.

den Hertog, Dick and Postek, Krzysztof. Bridging the
gap between predictive and prescriptive analytics –
new optimization methodology needed. Technical re-
port, Tilburg University, Netherlands, 2016. Available
at: http://www.optimization-online.org/
DB_HTML/2016/12/5779.html.

Freund, Yoav and Schapire, Robert E. Adaptive game play-
ing using multiplicative weights. Games and Economic
Behavior, 29:79–103, 1997.

Gurobi Optimization, Inc. Gurobi optimizer reference
manual. http://www.gurobi.com, 2016. URL
http://www.gurobi.com.

Hazan, Elad. Introduction to online convex optimization.
Foundations and Trends in Optimization, 2(3–4):157–
325, 2016. doi: 10.1561/2400000013. URL http:
//ocobook.cs.princeton.edu/.

III, Hal Daumé, Khuller, Samir, Purohit, Manish, and
Sanders, Gregory. On correcting inputs: Inverse opti-
In Proceed-
mization for online structured prediction.
ings of the IARCS Annual Conference on Foundations of
Software Technology and Theoretical Computer Science
(FSTTCS), 2005.

Iyengar, Garud and Kang, Wanmo. Inverse conic program-
ming with applications. Operations Research Letters, 33
(3):319–330, 2005.

Jadbabaie, Ali, Rakhlin, Alexander, Shahrampour, Shahin,
and Sridharan, Karthik. Online optimization: Competing
with dynamic comparators. In AISTATS, 2015.

Kallus, Nathan and Udell, Madeleine. Learning pref-
erences from assortment choices in a heterogeneous
Technical report, Massechusetts Insti-
population.
available at: https:
tute of Technology, 2015.
//pdfs.semanticscholar.org/b29d/
026b6776e94a00d1ea15f83518ebbbd14d85.
pdf.

Chen, Xi, Owen, Zachary, Pixton, Clark, and Simchi-Levi,
David. A statistical learning approach to personalization
in revenue management. Technical report, New York
University, 2015.

Kallus, Nathan and Udell, Madeleine.
high

personalization
assortment
Technical
report,
https://arxiv.org/pdf/1610.05604.

in

Cornell University,

Dynamic
dimensions.
2016.

D. Burton, W. R. Pulleyblank, Ph. L. Toint. Network Op-
timization, chapter The Inverse Shortest Paths Problem
with Upper Bounds on Shortest Paths Costs, pp. 156–
171. Springer, 1997.

Keshavarz, Arezou, Wang, Yang, and Boyd, Stephen. Im-
In Proceedings of
puting a convex objective function.
the 2011 IEEE International Symposium on Intelligent
Control (ISIC), pp. 613–619, 2011.

Emulating the Expert: Inverse Optimization through Online Learning

Konstantakopoulos, Ionnias C., Ratliff, Lillian J., Jin,
Ming, Spanos, Costas, and Sastry, S. Shankar. Smart
building energy efﬁciency via social game: A robust
utility learning framework for closing-the-loop.
In
2016 1st International Workshop on Science of Smart
City Operations and Platforms Engineering (SCOPE) in
partnership with Global City Teams Challenge (GCTC)
(SCOPE - GCTC), pp. 1–6, 2016.

Li, Jonathan Yu-Meng.

Inverse optimization of convex
risk function. Technical report, University of Ottawa,
Canada, 2016. available at: https://arxiv.org/
abs/1607.07099.

Littlestone, Nick and Warmuth, Manfred K. The weighted
majority algorithm. Information and Computation, 108:
212–261, 1994.

Lodi, Andrea.

Big data & mixed-integer

Presentation,

ear) programming.
https://atienergyworkshop.files.
wordpress.com/2015/11/andrealodi.pdf,
2016.

available

Molloy, Timothy L., Tsai, Dorian, Ford, Jason J., and
Perez, Tristan. Discrete-time inverse optimal control
with partial-state information: A soft-optimality ap-
proach with constrained state estimation. In 2016 IEEE
55th Conference on Decision and Control (CDC), pp.
1926–1932, 2016.

Nielsen, Thomas D. and Jensen, Finn V. Learning a deci-
sion makers’s utility function from (possibly) inconsis-
tent behavior. Artiﬁcial Intelligence, 160:53–78, 2004.

Panchea, Adina M. and Ramdani, Nacim. Towards solving
inverse optimal control in a bounded-error framework.
In 2015 American Control Conference (ACC), pp. 4910–
4915, 2015.

Papadopoulos, Alessandro Vittorio, Bascetta, Luca, and
Ferretti, Gianni. Generation of human walking paths.
Autonomous Robots, 40(1):55–75, 2016.

Pisinger, David. Where are the hard knapsack problems?
Computers & Operations Research, 32(9):2271–2284,
2005.

Ratia, Héctor, Montesano, Luis, and Martinez-Cantin,
On the performance of maximum likeli-
arXiv preprint

Ruben.
hood inverse reinforcement learning.
arXiv:1202.1558, 2012.

Ratliff, Lillian J., Dong, Roy, Ohlsson, Henrik, and Sastry,
S. Shankar. Incentive design and utility learning via en-
ergy disaggregation. In Proceedings of the 19th World
Congress of the International Federation of Automatic
Control, pp. 3158–3163, 2014.

Sayre, G. A. and Ruan, D. Automatic treatment planning
with convex imputing. Journal of Physics: Conference
Series, 489(1), 2014.

Schaefer, Andrew. Inverse integer programming. Optimiza-

tion Letters, 3(4):483–489, 2009.

(nonlin-
at:

Simchi-Levi, David. OM research: From problem-driven
to data-driven research. Manufacturing & Service Oper-
ations Management, 16(1):2–10, 2014.

Sokkalingam, P. T., Ahuja, Ravindra K., and Orlin,
Solving inverse spanning tree problems
James B.
through network ﬂow techniques. Operations Research,
47(2):291–298, 1999.

Syed, Umar and Schapire, Robert E. A game-theoretic
approach to apprenticeship learning. In Conference on
Neural Information Processing System (NIPS), 2007.

Taskar, Ben, Chatalbashev, Vassil, Koller, Daphne, and
Guestrin, Carlos. Learning structured prediction models:
A large margin approach. In Proceedings of the Interna-
tional Conference on Machine Learning (ICML), 2005.

Thai, Jérôme and Bayen, Alexandre M.

Imputing a
variational inequality function or a convex objective
Journal of Mathe-
function: A robust approach.
matical Analysis and Applications, 2016.
to appear,
available at: http://www.sciencedirect.com/
science/article/pii/S0022247X16305340.

Troutt, Marvin D., Tadisina, Suresh K., Sohn, Changsoo,
and Brandyberry, Alan A. Linear programming system
identiﬁcation. European Journal on Operational Re-
search, 161:663–672, 2005.

Plotkin, Serge A., Shmoys, David B., and Éva Tardos.
Fast approximation algorithms for fractional packing
and covering problems. Mathematics of Operations Re-
search, 20(2):257–301, 1995.

Troutt, Marvin D., Pang, Wan-Kai, and Hung-Huo, Shui.
Behavioral estimation of mathematical programming ob-
jective function coefﬁcients. Management Science, 52
(3):422–434, 2006.

Qiang,

Sheng and Bayati, Mohsen.

Dynamic
re-
Technical
pricing with demand covariates.
port, Stanford University,
available at:
2016.
https://papers.ssrn.com/sol3/papers2.
cfm?abstract_id=2765257.

Troutt, Marvin D., Brandybarry, Alan A., Sohn, Chang-
soo, and Tadisina, Suresh K. Linear programming sys-
tem identiﬁcation: The general nonnegative parameters
case. European Journal on Operational Research, 185:
63–75, 2008.

Emulating the Expert: Inverse Optimization through Online Learning

Troutt, Marvin D, Gwebu, Kholekile L, Wang, Jing, and
Brandyberry, Alan A. Some experiments on subjective
optimisation. International Journal of Operational Re-
search, 12(1):79–103, 2011.

Vovk, Volodimir G. Aggregating strategies. In Conference

on Learning Theory (COLT), 1990.

Yang, Insoon, Zeilinger, Melanie N., and Tomlin, Claire J.
Utility learning model predictive control for personal
In 53rd IEEE Conference on Decision
electric loads.
and Control, pp. 4868–4874, 2014.

Zinkevich, Martin. Online convex programming and
Technical
generalized inﬁnitesimal gradient ascent.
report, School of Computer Science, Carnegie Mellon
University, 2003. available at: http://www-cgi.
cs.cmu.edu/afs/cs.cmu.edu/Web/People/
maz/publications/techconvex.pdf.

Emulating the Expert: Inverse Optimization through Online Learning

6. Supplementary Material

In this Appendix, we present a somewhat broader range
of visualizations of our results on the Linear and Integer
Knapsack as well as the RCSP instances.

First, we show the different error measures for the Linear
Knapsack over 100 and 1000 rounds each – the total error
in Figures 3, the objective error in Figure 4 and the solu-
tion error in Figure 5. Conﬁrming the theoretical results in
Section 3, we see that the average errors decrease roughly
at a rate of O( 1√
T

) in the number of observations T .

(a) Over T = 100 rounds

(a) Over T = 100 rounds

(b) Over T = 1000 rounds

Figure 4. Objective error for the Linear Knapsack instance with
n = 1000 items

(b) Over T = 1000 rounds

Figure 3. Total error for the Linear Knapsack instance with n =
1000 items

Figure 6 shows the corresponding development of the dis-
tance between the objective function ct played in round t
and the true objective funtion ctrue, as well as the difference
of two successively played objective functions ct and ct−1,
as measured in the 1-norm. We see that the played objec-
tive functions get closer to the true objective functions very

1102030405060708090100Round0.000.050.100.150.200.250.300.35Total erroraverageper round11002003004005006007008009001000Round0.000.050.100.150.200.250.300.35Total erroraverageper round1102030405060708090100Round0.000.050.100.150.200.250.30Objective erroraverageper round11002003004005006007008009001000Round0.000.050.100.150.200.250.30Objective erroraverageper roundEmulating the Expert: Inverse Optimization through Online Learning

quickly, but that eventually our algorithm converges to an
alternative objective function. This conﬁrms that conver-
gence towards the true objective function cannot not be ex-
pected in general, see our discussion in Section 3. Never-
theless, even without ﬁnding the true objective function, the
solutions obtained from the learned sequence of objective
functions are undistinguishable from the solutions chosen
be the expert on average.

Figures 7 and 8 show similar ﬁndings for the Integer Knap-
sack with 1000 items, where we would like to point out that
this is the much harder optimization problem, theoretically.

We also did some experiments where the learner is only us-
ing the observations for a small number of rounds to update
the objective function and from there on only uses this ap-
proximation. We depict these results in Figure 9. Finally,
we show the same graphics for the case of the RCSP on a
grid graph with m = 15 rows and n = 30 columns and
1000 observations in Figures 10 and 11, again conﬁrming
our theoretical results.

(a) Over T = 100 rounds

(b) Over T = 1000 rounds

Figure 5. Solution error for the Linear Knapsack instance with
n = 1000 items

Figure 6. Distance of ct to ctrue and ct to ct−1 for the Linear Knap-
sack instance with n = 1000 items over T = 1000 rounds

1102030405060708090100Round0.000.010.020.030.040.050.060.07Solution erroraverageper round11002003004005006007008009001000Round0.000.010.020.030.040.050.060.07Solution erroraverageper round11002003004005006007008009001000Round0.00.10.20.30.4Diff errorconsecutive c'sc and ctrueEmulating the Expert: Inverse Optimization through Online Learning

Figure 8. Distance of ct to ctrue and ct to ct−1 for the Integer
Knapsack instance with n = 1000 items over T = 1000 rounds

(a) Total error

(b) Objective error

Figure 9. Total error for Integer Knapsack Instance with n =
1000 items. Here we only performed updates of the objective
for the ﬁrst 100 rounds and kept ct constant after that. As can
be seen, the learned function c100 performs very well. Even after
100 rounds enough information about ctrue has been learned to be
competitive for the next 900 rounds. Note though, that if the pa-
rameters are chosen adversarial and not at random as done here,
this would not be possible in general.

(c) Solution error

Figure 7. The different error measures for the Integer Knapsack
instance with n = 1000 items over T = 1000 rounds

11002003004005006007008009001000Round0.000.050.100.150.200.250.300.350.40Total erroraverageper round11002003004005006007008009001000Round0.000.050.100.150.200.250.30Objective erroraverageper round11002003004005006007008009001000Round0.000.010.020.030.040.050.060.07Solution erroraverageper round11002003004005006007008009001000Round0.00.10.20.30.4Diff errorconsecutive c'sc and ctrue11002003004005006007008009001000Round0.000.050.100.150.200.250.300.350.40Total erroraverageper roundEmulating the Expert: Inverse Optimization through Online Learning

(a) Total error

(b) Objective error

Figure 11. Distance of ct to ctrue and ct to ct−1 for the RCSP in-
stance on a grid graph with m = 15 rows and n = 30 columns
over T = 1000 iterations

(c) Solution error

Figure 10. The different error measures for the RCSP instance on
a grid graph with m = 15 rows and n = 30 columns over T =
1000 iterations

11002003004005006007008009001000Round0.0000.0020.0040.0060.0080.010Total erroraverageper round11002003004005006007008009001000Round0.0000.0010.0020.0030.0040.0050.0060.0070.008Objective erroraverageper round11002003004005006007008009001000Round0.00000.00050.00100.00150.00200.00250.00300.00350.0040Solution erroraverageper round11002003004005006007008009001000Round0.00.10.20.30.40.5Diff errorconsecutive c'sc and ctrue