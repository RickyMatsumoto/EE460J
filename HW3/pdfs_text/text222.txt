On Context-Dependent Clustering of Bandits

Claudio Gentile 1 Shuai Li 2 Purushottam Kar 3 Alexandros Karatzoglou 4 Giovanni Zappella 5 Evans Etrue 1

Abstract

We investigate a novel cluster-of-bandit algo-
rithm CAB for collaborative recommendation
tasks that implements the underlying feedback
sharing mechanism by estimating user neigh-
borhoods in a context-dependent manner. CAB
makes sharp departures from the state of the art
by incorporating collaborative effects into infer-
ence, as well as learning processes in a manner
that seamlessly interleaves explore-exploit trade-
offs and collaborative steps. We prove regret
bounds for CAB under various data-dependent
assumptions which exhibit a crisp dependence on
the expected number of clusters over the users,
a natural measure of the statistical difﬁculty of
the learning task. Experiments on production and
real-world datasets show that CAB offers signif-
icantly increased prediction performance against
a representative pool of state-of-the-art methods.

1. Introduction

In many prominent applications of bandit algorithms, such
as computational advertising, web-page content optimiza-
tion and recommendation systems, one of the main sources
of information is embedded in the preference relationships
between users and the items served. Preference patterns,
emerging from clicks, views or purchase of items, are typ-
ically exploited through collaborative ﬁltering techniques.

In fact, it is common knowledge in recommendation sys-
tems practice, that collaborative effects carry more infor-
mation about user preferences than say, demographic meta-
data (Pilaszy & Tikk, 2009). Yet, as content recommenda-
tion functionalities are incorporated in diverse online ser-
vices, the requirements differ vastly too. For instance, in a

1DiSTA, University of Insubria, Italy 2University of Cam-
bridge, United Kingdom 3IIT Kanpur, India 4Telefonica Re-
search, Spain 5Amazon Dev Center, Germany (work done while
at the University of Milan, Italy). Correspondence to: Claudio
Gentile <claudio.gentile@uninsubria.it>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

movie recommendation system, where the catalog is rela-
tively static and ratings for items accumulate, one can eas-
ily deploy collaborative ﬁltering methods such as matrix
factorization or restricted Boltzmann machines. However,
the same methods become practically impossible to use in
more dynamic environments such as in news or YouTube
video recommendation, where one has to deal with a near-
continuous stream of new items to be recommended, along
with new users to be served. These dynamic environments
pose a dual challenge to recommendation methods: 1) How
to present the new items to the users (or, vice versa, which
items to present to new users), in order to optimally gather
preference information on the new content (exploration),
and 2) How to use all the available user-item preference in-
formation gathered so far (exploitation). Ideally, one would
like to exploit both the content information but also, and
more importantly, the collaborative effects that can be ob-
served across users and items.

When the users to serve are many and the content universe
(or content popularity) changes rapidly over time, recom-
mendation services have to show both strong adaptation
in matching user preferences and high degree of algorith-
mic scalability/responsiveness so as to allow effective on-
line deployment. In typical scenarios like social networks,
where users are engaged in technology-mediated interac-
tions inﬂuencing each other’s behavior, it is often possible
to single out a few groups or communities made up of users
sharing similar interests and/or behavior. Such communi-
ties are not static over time and, more often than not, are
clustered around speciﬁc content types, so that a given set
of users can in fact host a multiplex of interdependent com-
munities depending on speciﬁc content items, which can
change dramatically on the ﬂy. We call this multiplex of
interdependent clusterings over users induced by the con-
tent universe, a context-dependent clustering. In addition
to the above, the set of users itself can change over time,
for new users get targeted by the service, others may sign
out or unregister. Thus, a recommendation method has to
readily adapt to a changing set of both users and items.

In this paper, we introduce and analyze the CAB (Context-
Aware clustering of Bandits) algorithm, a simple and ﬂexi-
ble algorithm rooted in the linear contextual bandit frame-
work that does the above by incorporating collaborative ef-
fects which traditional approaches to contextual bandits ig-

On Context-Dependent Clustering of Bandits

nore (e.g., (Auer, 2002; Li et al., 2010; Chu et al., 2011;
Abbasi-Yadkori et al., 2011)). CAB adapts to match user
preferences in the face of a constantly evolving content uni-
verse and set of targeted users and implements the context-
dependent clustering intuition by computing clusterings of
bandits which allows each content item to cluster users into
groups (which are few relative to the total number of users),
where within each group, users tend to react similarly
when that item gets recommended. CAB distinguishes it-
self in allowing distinct items to induce distinct clusterings
which is frequently observed in practice (Sutskever et al.,
2009). These clusterings are in turn suggestive of a natu-
ral context-dependent feedback sharing mechanism across
users. CAB is thus able to exploit collaborative effects in
contextual bandit settings in a manner similar to neighbor-
hood techniques in batch collaborative ﬁltering.

We analyze CAB from both, theoretical and experimental
standpoints. We show that CAB enjoys a regret bound
wherein the number of users engaged essentially enters
in the regret bound only through the expected number of
context-dependent clusters over the users, a natural mea-
sure of the predictive hardness of learning these users. We
extend this result to provide a sharper bound under spar-
sity assumptions on the user model vectors. Experimen-
tally, we present comparative evidence on production and
real-world datasets that CAB signiﬁcantly outperforms, in
terms of predictive performance, state-of-the-art contextual
bandit algorithms that either do not leverage any clustering
at all or do so in a context-independent fashion.

1.1. Related Work

The literature on contextual bandit algorithms is too vast
to be surveyed here.
In the sequel, we point out works
that most closely relate to ours. The technique of sequen-
tially clustering users in the bandit setting was introduced
by Gentile et al. (2014) and Maillard & Mannor (2014), but
has also been inspired by earlier works such as (Azar et al.,
2013) on transfer learning for stochastic bandits, and (Djo-
longa et al., 2013) on low-rank (Gaussian Process) bandits.
This led to further developments such as (Nguyen & Lauw,
2014) which relies on k-means clustering, (Korda et al.,
2016) which proposes distributed clustering of conﬁdence
ball algorithms for solving linear bandit problems in peer
to peer networks, and (Zhou & Brunskill, 2016) that learns
the latent classes of users (the clusters) so as to better serve
new users. Related papers that implement feedback sharing
mechanisms by leveraging (additional) social information
among users include (Cesa-Bianchi et al., 2013; Wu et al.,
2016). In all these cases, the way users are grouped is not
context-dependent. Even more related to our work is the
recent work of Li et al. (2016) which proposes to simulta-
neously cluster users as well as items, with item clusters
dictating user clusters. However, a signiﬁcant limitation of

this approach is that the content universe has to be ﬁnite
and known in advance, and in addition to the resulting al-
gorithm being somewhat involved.

It is worth stressing that having context-dependent clusters
not only changes the model considerably, as compared to
previous works, but the algorithms as well. All previous
works have a clustering process which is context-oblivious,
or else assume a static item universe. A speciﬁc draw-
back of works such as (Gentile et al., 2014; Li et al., 2016)
is that the clustering is unidirectional, in that users/items
once (erroneously) separated into different clusters cannot
be joined again, even if future evidence suggests so. Com-
pared to previous works, our approach distinguishes itself
for being simple and ﬂexible (e.g., we can seamlessly ac-
commodate the inclusion/exclusion of items and users), as
well as for performing feedback propagation among users
in a context-dependent manner. As will be demonstrated
in Section 5, this offers signiﬁcant performance boosts in
real-world recommendation settings.

2. Notation and Preliminaries

We will consider the bandit clustering model standard in
the literature, but with the crucial difference that we will
allow user behavior similarity to be represented by a fam-
ily of clusterings that depend on the speciﬁc item context
under consideration. In particular, we let U = {1, . . . , n}
represent the set of n users. An item, represented by its
feature vector x ∈ Rd will be seen as inducing a (po-
tentially unique) partition of the user set U into a small
number m(x) of clusters {U1(x), U2(x), . . . , Um(x)(x)},
where m(x) (cid:28) n. Users belonging to the same cluster
Uj(x) share similar behavior w.r.t. x (i.e. they both like or
both dislike the item represented by x), while users lying
in different clusters have signiﬁcantly different behavior.

This is a very ﬂexible model that allows users to agree on
their opinion of certain items and disagree on others, some-
thing that often holds in practice. It is important to note that
the mapping x → {U1(x), U2(x), . . . , Um(x)(x)} speci-
fying the partitioning of U into the clusters determined by
x (including the number of clusters m(x)), and the com-
mon user behavior within each cluster are unknown to the
learner, and have to be inferred based on user feedback.

For the sake of simplicity, we assume that the context-
dependent clustering is determined by linear functions x →
u(cid:62)
i x, each one parameterized by an unknown vector ui ∈
Rd hosted at user i ∈ U, with (cid:107)ui(cid:107) = 1 for all i, in such
a way that if users i, i(cid:48) ∈ U are in the same cluster w.r.t. x
then u(cid:62)
i x = u(cid:62)
i(cid:48) x, and if i, i(cid:48) ∈ U are in different clusters
i x − u(cid:62)
w.r.t. x then |u(cid:62)
i(cid:48) x| ≥ γ, for some gap parame-
ter γ > 0.1 We will henceforth call this assumption the

1 As usual, this hypothesis may be relaxed by assuming the

On Context-Dependent Clustering of Bandits

γ-gap assumption. For user vectors u1, . . . , un ∈ Rd cor-
responding to the n users (note that these vectors are un-
known to the algorithm), context x ∈ Rd, and user index
i ∈ U, we denote by Ni(x) the true neighborhood of i w.r.t.
x, i.e., Ni(x) = {j ∈ U : u(cid:62)
i x}. Hence, Ni(x)
is simply the cluster (over U) that i belongs to w.r.t. x. No-
tice that i ∈ Ni(x) for any i and any x. We will henceforth
assume that all items vectors x satisfy (cid:107)x(cid:107) ≤ 1.

j x = u(cid:62)

As is standard in linear bandit settings (Auer, 2002; Chu
et al., 2011; Abbasi-Yadkori et al., 2011; Krause & Ong,
2011; Crammer & Gentile, 2011; Yue et al., 2012; Djo-
longa et al., 2013; Cesa-Bianchi et al., 2013; Agrawal &
Goyal, 2013; Gentile et al., 2014; Li et al., 2016; Korda
et al., 2016), the (unknown) user vector ui determines the
average behavior of user i. More precisely, upon encoun-
tering an item context vector x, user i “reacts” by deliver-
ing a payoff value yi(x) = u(cid:62)
i x + (cid:15)i(x) , where (cid:15)i(x) is a
conditionally zero-mean sub-Gaussian error variable with
(conditional) variance parameter σ2(x) ≤ σ2 for all x.2
Hence, conditioned on the past, the quantity u(cid:62)
i x is indeed
the expected payoff observed at user i for context vector x.
For the sake of concreteness, we will assume that for all
i ∈ U and x ∈ Rd we have yi(x) ∈ [−1, +1] a.s..

Learning takes place over a discrete sequence of time steps
(or rounds). At each time t = 1, 2, . . . , the learner receives
a user index it ∈ U, representing the user to serve con-
tent. Notice that the user to serve may change from round
to round, and the same user may recur several times. To-
gether with it, the learner receives a set of context vectors
Ct = {xt,1, xt,2, . . . , xt,ct} ⊆ Rd, such that (cid:107)xt,k(cid:107) ≤ 1
for all t and k = 1, . . . , ct, encoding the content which
is currently available for recommendation to user it. The
learner is compelled to pick some ¯xt = xt,kt ∈ Ct to
recommend to it, and then observes it’s feedback in the
form of a payoff yt ∈ [−1, +1] whose (conditional) expec-
tation is u(cid:62)
t=1 =
it
{(i1, C1), (i2, C2), . . . , (iT , CT )} are generated by an ex-
ogenous process and, in a sense, represents the ”data at
hand”. As we shall see in Section 4, the performance of
our algorithm will depend on the properties of these data.

¯xt. The sequence of pairings {it, Ct}T

The practical goal of the learner is to maximize its to-
tal payoff (cid:80)T
t=1 yt over T time steps. From a theoretical
standpoint, we are instead interested in bounding the cumu-
lative regret incurred by our algorithm. More precisely, let
the regret rt of the learner at time t be the extent to which
the average payoff of the best choice in hindsight at user it
exceeds the average payoff of the algorithm’s choice, i.e.,

i x and u(cid:62)

existence of two thresholds, one for the within-cluster distance of
u(cid:62)

i(cid:48) x, the other for the between-cluster distance.
2 Recall that a zero-mean random variable X is sub-Gaussian
with variance parameter σ2 if E[exp(sX)] ≤ exp(s2 σ2/2) for
all s ∈ R. Any variable X with E[X] = 0 and |X| ≤ b is
sub-Gaussian with variance parameter upper bounded by b2.

rt =

(cid:16)

max
x∈Ct

(cid:17)

u(cid:62)
it

x

−u(cid:62)
it

¯xt .

We are aimed at bounding with high probability (over the
noise variables (cid:15)it ( ¯xt), and any other possible source of
randomness) the cumulative regret (cid:80)T
t=1 rt . As a special
case of the above model, when the set of items do not pos-
sess informative features, we can always resort to the non-
contextual bandit setting (e.g., (Auer et al., 2002; Audibert
et al., 2009)). To implement this approach, we simply take
the set of all items (which must be ﬁnite for this technique
to work), and apply a one-hot encoding by assigning to the
i-th item, the i-th canonical basis vector ei, with one at the
i-th position and zero everywhere else as the context vec-
tor. It is easy to see that the expected payoff given by user
i on item j will simply be the j-th component of vector ui.

Our aim would be to obtain a regret bound that gracefully
improves as the context-dependent clustering structure over
the users becomes stronger. More speciﬁcally, values taken
by the number of clusters m(x) would be of particular in-
terest since we expect to reap the strongest collaborative ef-
fects when m(x) is small whereas not much can be done by
way of collaborative analysis if m(x) ≈ n. Consequently,
a desirable regret bound would be one that scales with
m(x). Yet, recall that m(x) is a function of the context
vector x, which means that we expect our regret bound to
also depend on the properties of the actual data {it, Ct}T
t=1.
We will see in Section 4 that, under suitable stochastic as-
sumptions on the way {it, Ct}T
t=1 is generated, our regret
analysis essentially replaces the dependence on the total
number of users n by the (possibly much) smaller quan-
tity E[m(x)], the expected number of clusters over users,
the expectation being over the draw of context vectors x.

3. The Context-Aware Bandit Algorithm

We present Context-Aware (clustering of) Bandits (dubbed
as CAB, see Algorithm 1), an upper-conﬁdence bound-
based algorithm for performing recommendations in the
context-sensitive bandit clustering model. Similar to previ-
ous works (Cesa-Bianchi et al., 2013; Gentile et al., 2014;
Nguyen & Lauw, 2014; Li et al., 2016; Wu et al., 2016),
CAB maintains a vector estimate wi,t to serve as a proxy
to the unknown user vector ui at time t. CAB also main-
tains standard correlation matrices Mi,t. The standard con-
ﬁdence bound function for user i for item x at time t is de-
i,t x, for a suitable func-
rived as CBi,t(x) = α(t)
tion α(t) = O(
d log t).

x(cid:62)M −1

(cid:113)

√

However, CAB makes sharp departures from previous
works both in the way items are recommended, as well as
in they way the proxy estimates wi,t are updated.

Item Recommendation: At time t, we are required to
serve user it ∈ U by presenting an item out of a set of items

On Context-Dependent Clustering of Bandits

Algorithm 1 Context-Aware clustering of Bandits (CAB).
1: Input: Separation parameter γ, exploration parameter α(t).
2: Init: bi,0 = 0 ∈ Rd and Mi,0 = I ∈ Rd×d, i ∈ U .
3: for t = 1, 2, . . . , T do
Set wi,t−1 = M −1
4:

for all i ∈ U;

i,t−1bi,t−1,
(cid:113)

x(cid:62)M −1
Use CBi,t(x) = α(t)
Receive user it ∈ U to be served, and context vectors Ct =
{xt,1, . . . , xt,ct } for items to be recommended from;
for k = 1, . . . , ct do

i,t−1x, for all x, i ∈ U;

Compute neighborhood (cid:98)Nk := (cid:98)Nit,t(xt,k) for this item
it,t−1xt,k − w(cid:62)
(cid:98)Nk =

j ∈ U : |w(cid:62)

j,t−1xt,k|

(cid:110)

≤ CBit,t−1(xt,k) + CBj,t−1(xt,k)

(cid:111)

.

Set w

Set CB

(cid:80)

(cid:98)Nk,t−1 = 1
| (cid:98)Nk|
(cid:98)Nk,t−1(xt,k) = 1
| (cid:98)Nk|

j∈ (cid:98)Nk

wj,t−1;
(cid:80)

j∈ (cid:98)Nk

end for
Recommend item ¯xt = xt,kt ∈ Ct such that

CBj,t−1(xt,k);

kt = argmax
k=1,...,ct

(cid:16)

w

(cid:62)xt,k + CB

(cid:98)Nk,t−1

(cid:17)
(cid:98)Nk,t−1(xt,k)

;

Observe payoff yt ∈ [−1, 1];
if CBit,t−1( ¯xt) ≥ γ/4 then

Set Mit,t = Mit,t−1 + ¯xt ¯x(cid:62)
t ,
Set bit,t = bit,t−1 + yt ¯xt,
Set Mj,t = Mj,t−1, bj,t = bj,t−1 for all j (cid:54)= it;

else

for all j ∈ (cid:98)Nkt such that CBj,t−1( ¯xt) < γ/4 do

Mj,t = Mj,t−1 + ¯xt ¯x(cid:62)
t ,
bj,t = bj,t−1 + yt ¯xt;

end for
Set Mj,t = Mj,t−1, bj,t = bj,t−1 for all j /∈ (cid:98)Nkt and
for j ∈ (cid:98)Nkt such that CBj,t−1( ¯xt) ≥ γ/4 .

5:
6:

7:
8:

9:

10:
11:
12:

13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

end if
24:
25: end for

Ct = {xt,1, . . . , xt,ct} available at time t. To do so, CAB
ﬁrst computes for each item xt,k in Ct, the set of users that
are likely to give the item a similar payoff as it. This set
(cid:98)Nit,t(xt,k) is the estimated neighborhood of user it with
respect to item xt,k. A user j is included in (cid:98)Nit,t(xt,k) if
the estimated payoff it gives to the item xt,k is sufﬁciently
close to that given to the item by user it (see step 8).

CAB incorporates collaborative effects by lifting the no-
tions of the user proxy and conﬁdence bounds to a set of
users N ⊆ U. CAB uses an inexpensive, ﬂat averag-
(cid:80)
ing lift: CBN,t(x) = 1
j∈N CBj,t(x) and wN,t =
|N |
1
j∈N wj,t. Next, CAB uses (see step 12) aggre-
|N |
(cid:98)Nit,t(xt,k)(xt,k) and aggre-
gated conﬁdence bounds CB
(cid:98)Nit,t(xt,k),t−1 to select an item ¯xt =
gated proxy vectors w
xt,kt ∈ Ct based on an upper conﬁdence estimation step.

(cid:80)

Proxy Updates: Classical approaches update the user
proxies wi,t by solving a regularized least squares problem
involving (feature representations of) items served previ-
ously to user i and payoffs received. However, CAB re-

mains fully committed to the collaborative approach (see
steps 14-24) by allowing a user i to inherit updates due to
an item x served to another user j if the two users are in-
deed deemed to agree on their opinion on item x with a
sufﬁciently high degree of conﬁdence. The proxies wj,t
are updated after receiving the feedback yt from user it.

If CAB is not too conﬁdent regarding the opinion it has
along the direction ¯xt, formally CBit,t−1( ¯xt) ≥ γ/4, then
only the proxy at user it is updated (see step 15-17). How-
ever, if CAB is conﬁdent, i.e., if CBit,t−1( ¯xt) < γ/4 then
the proxy updates are performed (see steps 19-23) for all
users j in it’s estimated neighborhood with respect to ¯xt
about whose opinions CAB is conﬁdent too. Notice that
all such users j undergo the same update, which is moti-
vated by the algorithm’s belief that (cid:98)Nit,t( ¯xt) = Nit( ¯xt),
i.e., that the conditional expectation u(cid:62)
¯xt of yt given ¯xt
it
is actually also equal to u(cid:62)
j ¯xt for all users j ∈ (cid:98)Nit,t( ¯xt)
such that CBj,t−1( ¯xt) < γ/4.

It is worth noting that CAB is extremely ﬂexible in han-
dling a ﬂuid set of users U. Due to its context-sensitive user
aggregation step, which is repeated at every round, CAB
allows users to be added or dropped on the ﬂy, in a seam-
less manner. This is in strike contrast to past approaches
to bandit aggregation, such as GobLin (Cesa-Bianchi et al.,
2013), CLUB (Gentile et al., 2014), and COFIBA (Li et al.,
2016), where more involved feedback sharing mechanisms
across the users are implemented which are based either on
static network Laplacians or on time-evolving connected
components of graphs over a given set of users.

4. Regret Analysis

Our regret analysis depends on a speciﬁc measure of hard-
for an observed sequence of
ness of the data at hand:
users {it}T
t=1 = {i1, . . . , iT } and corresponding sequence
of item sets {Ct}T
t=1 = {C1, . . . , CT }, where Ct =
{xt,1, . . . , xt,ct}, the hardness HD({it, Ct}T
t=1, η) of the
pairing {it, Ct}T
t=1 at level η > 0 is deﬁned as

HD({it, Ct}T

t=1, η)

(cid:110)

t
(cid:88)

I +

s≤t : is=j

= max

t = 1, . . . , T : ∃j ∈ U, ∃k1, k2, . . . , kt, :

xs,ks x(cid:62)

s,ks has smallest eigenvalue ≤ η

(cid:111)

.

Given a data sequence {it, Ct}, HD({it, Ct}T
t=1, η) mea-
sures the number of rounds we need to wait until correla-
tion matrices Mj,t, corresponding to all users j ∈ U have
eigenvalues lower bounded by η. For sake of convenience
the above quantity is calculated using the worst possible
way of updating the matrices Mj,t through rank-one ad-
justments based on the data. Based on the above hardness
deﬁnition, the following result summarizes our main efforts
in this section.

On Context-Dependent Clustering of Bandits

Theorem 1 Suppose CAB is executed on {it, Ct}T
t=1, such
that ct ≤ c for all t, and the condition |u(cid:62)
j,tx| ≤
CBj,t(x) holding for all j ∈ U, x ∈ Rd, along with the
γ-gap assumption. Then the cumulative regret (cid:80)T
t=1 rt of
CAB can be deterministically upper bounded as follows:

j x − w(cid:62)

(cid:32)

(cid:16)

rt ≤ 9α(T )

c HD

{it, Ct}T

t=1,

16 α2(T )
γ2

(cid:17)

T
(cid:88)

t=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)d log T

+

T
(cid:88)

t=1

n
|Nit ( ¯xt)|

(cid:33)
,

√

log T ). Some comments are in
where we set α(T ) = O(
order. Theorem 1 delivers a deterministic regret bound on
the cumulative regret, and is composed of two terms. The
ﬁrst term is a measure of hardness of the data sequence
{it, Ct}T
t=1 at hand whereas the second term is the usual
√
T -style term in linear bandit regret analyses (Auer, 2002;
Chu et al., 2011; Abbasi-Yadkori et al., 2011). However,
note that the dependence of the second term on the total
number n of users to be served gets replaced by a much
|Nit ( ¯xt)| that depends on the actual size of
smaller quantity
context-dependent clusters of the served users. The state-
ment of the theorem requires |u(cid:62)
j,tx| ≤ CBj,t(x) ,
which (see Lemma 3 below) holds from standard results.

j x − w(cid:62)

n

We will shortly see that if the pairings {it, Ct}T
t=1 are gen-
erated in a favorable manner, such as sampling the item
vectors xt,k i.i.d. from a (possibly unknown) distribution
over the instance space (see Lemma 1 below), the hardness
measure can be upper bounded with high probability by a
term of the form log T
γ2 . Similarly, for the second term, in
the simple case when Nit( ¯xt) = B for all t, the second
term has the form (cid:112) n
B T , up to log factors. Notice that
√
T is roughly the regret effort for learning a single bandit,
and (cid:112) n
B -many (unrelated)
clusters of bandits when the clustering is known. Thus, in
this example, it is the ratio n
B that quantiﬁes the hardness of
the problem, insofar clustering is concerned. Again, under
favorable circumstances (see Lemma 2 below), we can re-
late the quantity (cid:80)T
|Nit ( ¯xt)| to the expected number of
context-dependent clusters of users, the expectation being
w.r.t. the random draw of item context vectors.

B T is the effort for learning n

t=1

n

On the other hand, making no assumptions whatsoever
on the way {it, Ct}T
t=1 is generated makes it hard to ex-
ploit the cluster structure. For instance, if {it, Ct}T
t=1
is generated by an adaptive adversary, this might cause
t=1, η(cid:1) to become linear in T for any constant
HD (cid:0){it, Ct}T
η > 1, thereby making the bound in Theorem 1 vacu-
ous. However, a naive algorithm that disregards the cluster
structure, making no attempts to incorporate collaborative
effects, and running n-many independent LINUCB-like al-
gorithms (Auer, 2002; Abbasi-Yadkori et al., 2011; Chu

et al., 2011), will still easily yield a

n T regret bound3.

√

A sufﬁcient condition for controlling the hardness term in
Theorem 1 is provided by the following lemma.

Lemma 1 For each round t, let the context vectors Ct =
{xt,1, . . . , xt,ct} be generated i.i.d. (conditioned on it, ct,
past data {is, Cs}t−1
s=1 and rewards y1, . . . , yt−1) from a
sub-Gaussian random vector X ∈ Rd with (conditional)
variance parameter ν2, such that (cid:107)X(cid:107) ≤ 1 a.s., and
E[XX (cid:62)] is full rank with smallest eigenvalue λ > 0. Let
ct ≤ c for all t, and ν2 ≤ λ2
8 ln(4c) . Finally, let the sequence
{it}T
t=1 be generated uniformly at random, 4 independent
of all other variables. Then with probability at least 1 − δ,

(cid:16)

HD

{it, Ct}T

t=1, η

= O

(cid:17)

(cid:18) n η

λ2 log

(cid:18) T nd
δ

(cid:19)(cid:19)

.

The following lemma handles the second term in the bound
of Theorem 1.

Lemma 2 For each round t, let the context vectors Ct =
{xt,1, . . . , xt,ct} be generated i.i.d. (conditioned on it, ct,
past data {is, Cs}t−1
s=1 and rewards y1, . . . , yt−1) from a
random vector X ∈ Rd with (cid:107)X(cid:107) ≤ 1. Let also ct ≤ c
for all t. Then, with probability at least 1 − δ,

T
(cid:88)

t=1

1
|Nit ( ¯xt)|

≤

2T c E[m(X)]
n

+ 12 log

(cid:18) log T
δ

(cid:19)

.

Remark 1 The linear dependence on c can be turned to
logarithmic, e.g., at the cost of an extra sub-Gaussian as-
|Ni(x)| , i ∈ U.
sumption on variables

1

Finally, we recall the following upper conﬁdence bound,
from (Abbasi-Yadkori et al., 2011).

(cid:113)

x(cid:62)M −1

j,t x, with α(t) =

Lemma 3 Let CBj,t(x) = α(t)
(cid:17)

(cid:16)(cid:113)

O

d log T n
δ

.5 Then, under the payoff noise model de-
j,tx| ≤ CBj,t(x) holds uni-

j x − w(cid:62)

|u(cid:62)

ﬁned in Section 2,
formly for all j ∈ U, x ∈ Rd, and t = 1, 2, . . ..

A straightforward combination of Theorem 1 with Lem-
mata 1, 2, and 3 yields the following result.

Corollary 1 Let CBj,t(x) be deﬁned with α(t) as in
Lemma 3, and let the γ-gap assumption hold. Assume con-
text vectors are generated as in Lemma 1 in such a way that

3 To see this, simply observe that each of the n LINUCB-like
Ti, where Ti is the
Ti ≤

√
t=1 rt ≤ (cid:80)n

algorithms has a regret bound of the form
number of rounds where it = i. Then (cid:80)T
√
n T , with equality if Ti = T /n for all i.
4 Any (non-uniform) process {it}T

t=1 that nevertheless en-
sures that each user i gets served a number of times that is likely
to grow unbounded with T would sufﬁce here.

i=1

√

5 The big-oh notation here hides the dependence on the vari-

ance σ2 of the payoff values.

On Context-Dependent Clustering of Bandits

the sub-Gaussian assumption therein holds with ct ≤ c. Fi-
nally, let the sequence {it}T
t=1 be generated as described in
Lemma 1. Then, with probability at least 1 − δ, the regret
of CAB (Algorithm 1) satisﬁes

rt = R + (cid:101)O

(cid:16)

(cid:17)
d(cid:112) T c (E[m(X)])

,

T
(cid:88)

t=1

where the (cid:101)O-notation hides logarithmic factors in T N d
and R is of the form6

δ

,

√

d

c n d
λ2 γ2

log2.5

(cid:18) T nd
δ

(cid:19)

.

R =

Sparse user models. We conclude with a pointer to an
extension of the CAB framework for sparse linear models
that extends past analyses on sparse linear bandits for a sin-
gle user (Abbasi-Yadkori et al., 2012; Carpentier & Munos,
2012; Carpentier, 2015), to include collaborative effects. If
all user vectors are sparse, i.e. for all i ∈ U, (cid:107)ui(cid:107)0 ≤ s, for
s (cid:28) d, then by replacing the least-squares solution in Step
4 of Algorithm 1 with the solution computed by a fully cor-
rective sparse recovery method (Dai & Milenkovic, 2009;
Jain et al., 2014; Needell & Tropp, 2008), we can obtain
an improved regret bound. Speciﬁcally, we can replace the
s, and the factor d
factor d
√
multiplying the

d in the term R above by s2√

T -term by a term of the form

s d.

√

√

5. Experiments

We tested CAB on production and real-world datasets, and
compared them to standard baselines as well as to state-
of-the-art bandit and clustering of bandit algorithms. For
datasets with no item features, a one-hot encoding was
used. We attempted to follow, as closely as possible, pre-
vious experimental settings, such as those as described
in (Cesa-Bianchi et al., 2013; Gentile et al., 2014; Korda
et al., 2016; Li et al., 2016).

5.1. Dataset Description

Tuenti. Tuenti (owned by Telefonica) is a Spanish social
network website that serves ads on its site, the data contains
ad impressions viewed by users along with a variable that
registers a click on an ad. The dataset contains d = 105
ads, n = 14, 612 users, and 15M records/timesteps. We
adopted a one hot encoding scheme for the items, hence
items are described by the unit-norm vectors e1, . . . , ed.
Since the available payoffs are those associated with the
items served by the system, we performed ofﬂine policy
evaluation through a standard importance sampling tech-
nique: we discarded on the ﬂy all records where the sys-
tem’s recommendation (the logged policy) did not coincide

6 We note that no special efforts have been devoted here to

obtain sharper upper bounds on R.

with the algorithms’ recommendations. The resulting num-
ber of retained records was around T = 1M , loosely de-
pending on the different algorithms and runs. Yet, because
this technique delivers reliable estimates when the logged
policy makes random choices (e.g., (Li et al., 2010)), we
actually simulated a random logged policy as follows. At
each round t, we retained the ad served to the current user it
with payoff value at (1 = “clicked”, 0 = “not clicked”), but
also included 14 extra items (hence ct = 15 for all t) drawn
uniformly at random in such a way that, for any item ej, if
ej occurs in some set Ct, this item will be the one served
by system only 1/15 of the times. Notice that this random
selection was independent of the available payoff at.

KDD Cup. This dataset was released for the KDD Cup
2012 Online Advertising Competition7 where the instances
were derived from the session logs of the search engine
soso.com. A search session included user, query and ad
information, and was divided into multiple instances, each
being described using the ad impressed at that time at a cer-
tain depth and position. Instances were aggregated with the
same user ID, ad ID, and query. We took the chronologi-
cal order among all the instances, and seeded the algorithm
with the ﬁrst ct = 20 instances (the length of recommen-
dation lists). Payoffs at are again binary. The resulting
dataset had n = 10, 333 distinct users, and d = 6, 780
distinct ads. Similar to the Tuenti dataset, we generated
random recommendation lists, and a random logged pol-
icy. We employed one-hot encoding as well in this dataset.
The number of retained records was around T = 0.1M .

Avazu. This dataset was released for the Avazu Click-
Through Rate Prediction Challenge on Kaggle8. Here
click-through data were ordered chronologically, and non-
clicks and clicks were subsampled according to different
strategies. As before, we simulated a random logged pol-
icy over recommendation lists of size ct = 20 ∀t. Payoffs
are once again binary. The ﬁnal dataset had n = 48, 723
users, ct = 20 for all t, d = 5, 099 items, while the number
of retained records was around T = 1.1M . Again, we took
the one-hot encoding for the items.

LastFM and Delicious. These two datasets9 are extracted
from the music streaming service Last.fm and the social
bookmarking web service Delicious. The LastFM dataset
includes n = 1,892 users, and 17,632 items (the artists). De-
licious refers to n = 1,861 users, and 69,226 items (URLs).
Preprocessing of data followed previous experimental set-
tings where these datasets have been used, e.g., (Cesa-
Bianchi et al., 2013; Gentile et al., 2014). Speciﬁcally, us-
ing a tf-idf representation of the available items, the con-
text vectors xt,i were generated by retaining only the ﬁrst

7http://www.kddcup2012.org/c/kddcup2012-track2
8https://www.kaggle.com/c/avazu-ctr-prediction
9www.grouplens.org/node/462

On Context-Dependent Clustering of Bandits

Table 1. Dataset statistics. Here, n is the number of users, d is the
dimension of the item vectors (which corresponds to the number
of items for Tuenti, KDD Cup and Avazu), ct is the size of the
recommendation lists, and T is the number of records (or just
retained records, in the case of Tuenti, KDD Cup and Avazu).

DATASET
TUENTI
KDD CUP
AVAZU
LASTFM
DELICIOUS

n
14,612
10,333
48,723
1,892
1,861

d
105
6,780
5,099
25
25

ct
T
15 (cid:39)1,000,000
(cid:39)100,000
20
20 (cid:39)1,100,000
50,000
25
50,000
25

d = 25 principal components. Binary payoffs were created
as follows. LastFM: If a user listened to an artist at least
once the payoff is 1, otherwise it is 0. Delicious: the payoff
is 1 if the user bookmarked the URL, and 0 otherwise. We
processed the datasets to make them suitable for use with
multi-armed bandit algorithms. Recommendation lists Ct
of size ct = 25 ∀t were generated at random by ﬁrst select-
ing index it at random over the n users, and then padding
with 24 vectors chosen at random from the available items
up to that time step, in such a way that at least one of these
25 items had payoff 1 for the current user it. This was re-
peated for T = 50, 000 times for the two datasets.

5.2. Algorithms

We used the ﬁrst 20% of each dataset to tune the algo-
rithms’ parameters through a grid search, and report results
on the remaining 80%. All results are averaged over 5 runs.
We compared to a number of state-of-the art bandit and
clustering-of-bandit methods:

• CLUB (Gentile et al., 2014):

sequentially reﬁnes
user clusters based on their conﬁdence ellipsoid balls;
We seeded the graph over users by an initial ran-
dom Erdos-Renyi graphs with sparsity parameter p =
(3 log n)/n. Since this is a randomized algorithm,
each run was repeated ﬁve times, and the results av-
eraged (the observed variance turned out to be small).

• DynUCB (Nguyen & Lauw, 2014): uses a traditional

k-Means algorithm to cluster bandits.

• LinUCB-SINGLE: uses a single instance of Lin-
UCB (Chu et al., 2011) to serve all users, i.e., all users
belong to the same cluster, independent of the items.

• LinUCB-MULTIPLE: uses an independent instance
of LinUCB per user with no user interactions. Each
user forms his own cluster, independent of the items.

• The following variant of CAB (see Algorithm 1):
each user j is considered for addition to the estimated
neighborhoods (cid:98)Nk of the currently served user it only
if wj,t−1 has been updated at least once in the past.

• RAN: recommends a uniformly random item from Ct.

Figure 1. Clickthrough Rate vs. retained records (”time”) on three
online advertising datasets. The higher the curves the better.

All
tested algorithms
(except RAN) are based on
the form CBi,t(x) =
upper-conﬁdence bounds of
α (cid:112)x(cid:62)Ni,tx log(1 + t). We tuned α for all algorithms
across the grid {0, 0.01, 0.02, . . . , 0.2}. The α2 parameter
in CLUB was tuned within {0.1, 0, 2, . . . , 0.5}. The num-
ber of clusters in DynUCB was increased in an exponential
progression, starting from 1, and ending to n. Finally, the γ
parameter in CAB was simply set to 0.2. In fact, the value
of γ did not happen to have a signiﬁcant inﬂuence on the
performance of the version of CAB we tested.

5.3. Results

Figures 1, 2, 3 summarize our experimental results. For the
online advertising datasets Tuenti, KDD Cup, and Avazu
(Figure 1), we report performance using the Click-Through
Rate (CTR), hence the higher the curves the better. For the
LastFM and Delicious datasets (Figure 2), we report the
ratio of the cumulative regret of the tested algorithm to the
cumulative regret of RAN, hence the lower the better.

Our experimental setting, as well some results reproduced
here, are in line with past work in the area (Li et al., 2010;
Cesa-Bianchi et al., 2013; Gentile et al., 2014; Li et al.,
2016). Given the way data have been prepared, our ﬁnd-
ings give reliable estimates of the actual CTR (Figure 1) or
regret (Figure 2) performance of the tested algorithms.

12345678x 10500.010.020.030.04TimeCTRTuenti  CABCLUBDynUCBLinUCB−SINGLELinUCB−MULTIPLE12345678x 1040.040.0450.050.0550.060.0650.070.0750.08TimeCTRKDD Cup  CABCLUBDynUCBLinUCB−SINGLELinUCB−MULTIPLE123456789x 1050.060.080.10.120.140.160.180.20.22TimeCTRAvazu  CABCLUBDynUCBLinUCB−SINGLELinUCB−MULTIPLEOn Context-Dependent Clustering of Bandits

ever, to be relatively poor; this can be attributed to the way
the LastFM dataset was generated. Here users typically
have little interaction with the music serving system and a
lot of the songs played were generated by a recommender.
Hence while there are collaborative effects, they are rela-
tively weak compared to datasets such as Tuenti.

On the other hand, on the Delicious dataset
the best
performing strategy seems to be LinUCB-MULTIPLE,
which deliberately avoids any feedback sharing mechanism
among the users. This dataset reﬂects user web-browsing
patterns, as evinced by their bookmarks. As noted previ-
ously (Gentile et al., 2014), this dataset does not seem to
contain any collaborative information, hence we can hardly
expect to take advantage of clustering efforts. To shed fur-
ther light, in Figure 3 we plot the average distance between
a linear model for user it and the corresponding linear mod-
els for all other users j (cid:54)= it, as a function of t. For each
user of these two datasets, these linear models were com-
puted by taking the whole test set and treating each pairing
(xt,k, yt,k) with it = i, and yt,k = 1 as a training sample
for a (regularized) least-squares estimator for user i. The
conclusion we can draw after visually comparing the left
and the right plots in Figure 3 is that on Delicious these es-
timated user models tend to be signiﬁcantly more separated
than on LastFM, which readily explains the effectiveness
of LinUCB-MULTIPLE. Moreover, on Delicious, studies
have shown that tags which are used as item features are
generally chosen by users to reﬂect their interests and for
personal use, hence we can expect these features to diverge
even for similar websites. On the other hand, LastFM tags
are typically indicative of the genre of the song.

6. Conclusions and Ongoing Research

In this paper we proposed CAB, a novel contextual ban-
dit algorithm for personalized recommendation systems.
CAB effectively incorporates collaborative effects by im-
plementing a simple context-dependent feedback sharing
mechanism which greatly relaxes restrictions imposed by
earlier works. We established crisp regret bounds for CAB
which scale gracefully with the expected number of clus-
ters over the users. We further found CAB to outperform
existing approaches in practice, using extensive experimen-
tation on a number of production and real-world datasets.

We have begun preliminary experiments with Thompson
sampling versions of CAB and its competitors but have thus
far not observed statistically signiﬁcant deviations from re-
sults in Section 5. From a theoretical standpoint, it would
be nice to complement our upper bound in Corollary 1 with
a lower bound helping characterize the regret complexity
of our problem. We are also planning to experimentally
benchmark with performance of the sparse bandit version
of CAB that infers sparse user models.

Figure 2. Ratio of the cumulative regret of the algorithms to
the cumulative regret of RAN against time on the two datasets
LastFM and Delicious. The lower the curves the better.

Figure 3. Average (estimated) Euclidean distance between the
served user it and all other users, as a function of t for the two
datasets LastFM (left) and Delicious (right). The distance is
computed by associating with each user a model vector obtained
through a regularized least-squares solution based on all available
data for that user (instance vectors and corresponding payoffs).

In four out of ﬁve datasets, CAB was found to offer per-
formance superior to all other baselines. CAB performed
particularly well on the Tuenti dataset where it delivered
almost double the CTR compared to some of the baselines.
CAB’s performance advantage was more moderate on the
KDD Cup and Avazu datasets. This is expected since ex-
ploiting collaborative effects is more important on a dataset
like Tuenti, where users are exposed to a few (≈ 100) ads,
as compared to the KDD Cup dataset (where ads are modu-
lated by a user query) and the Avazu dataset, both of which
have a much broader ad base (≈ 7000). This provides a
strong indication that CAB effectively exploits collabora-
tive effects. In general, on Tuenti, KDD Cup, and Avazu
(Figure 1), CAB was found to offer beneﬁts in the cold-
start region (i.e., the initial relatively small fraction of time
horizon), but also continued to maintain a lead throughout.

On the LastFM and Delicious datasets (Figure 2), the re-
sults we report are consistent with (Gentile et al., 2014).
On LastFM all methods are again outperformed by CAB.
The overall performance of all bandit methods seems how-

0.511.522.533.54x 1040.80.850.90.951TimeCum. Regr. of Alg. / Cum. Regr. of RANLastFM  CAB CLUB DynUCB LinUCB−SINGLE LinUCB−MULTIPLE0.511.522.533.54x 1040.80.850.90.951TimeCum. Regr. of Alg. / Cum. Regr. of RANDelicious  CAB CLUB DynUCB LinUCB−SINGLE LinUCB−MULTIPLE00.511.522.533.54x 10400.0050.010.0150.020.0250.030.035TimeAverage Euclidean distanceLastFM: Average Euclidean distance of served user to remaining users00.511.522.533.54x 10400.0050.010.0150.020.0250.030.035TimeAverage Euclidean distanceDelicious: Average Euclidean distance of served user to remaining usersOn Context-Dependent Clustering of Bandits

Acknowledgments

The authors thank the ICML reviewers for useful com-
ments. CG acknowledges partial support from Criteo
through a Faculty Research Award. PK is supported by the
Deep Singh and Daljeet Kaur Faculty Fellowship and the
Research-I foundation at IIT Kanpur, and thanks Microsoft
Research India for a research grant. GZ would like to thank
N. Cesa-Bianchi for inspiring discussions regarding bandit
algorithms and related topics.

References

Abbasi-Yadkori, Yasin, Pal, David, and Szepesvari, Csaba.
Improved Algorithms for Linear Stochastic Bandits. In
Proceedings of the 25th Annual Conference on Neural
Information Processing Systems (NIPS), 2011.

Abbasi-Yadkori, Yasin, P´al, D´avid, and Szepesv´ari, Csaba.
Online-to-Conﬁdence-Set Conversions and Application
to Sparse Stochastic Bandits. In Proceedings of the 15th
International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), 2012.

Agrawal, Shipra and Goyal, Navin. Thompson Sampling
for Contextual Bandits with Linear Payoffs. In Proceed-
ings of the 30th International Conference on Machine
Learning (ICML), 2013.

Audibert, Jean Yves, Munos, Remi, and Szepesvari, Csaba.
Exploration-exploitation Tradeoff using Variance Esti-
mates in Multi-armed Bandits. Theoretical Computer
Science, 410(19):1876–1902, 2009.

Auer, Peter. Using Conﬁdence Bounds for Exploration-
Exploitation Trade-Offs. Journal of Machine Learning
Research, 3:397–422, 2002.

Auer, Peter, Cesa-Bianchi, Nicolo, and Fischer, Paul.
Finite-time Analysis of the Multiarmed Bandit Problem.
Machine Learning, 47:235–256, 2002.

Azar, Mohammad Gheshlaghi, Lazaric, Alessandro, and
Brunskill, Emma. Sequential Transfer in Multi-armed
Bandit with Finite Set of Models. In Proceedings of the
27th Annual Conference on Neural Information Process-
ing Systems (NIPS), 2013.

Carpentier, Alexandra. Implementable Conﬁdence Sets in
In Proceedings of the
High Dimensional Regression.
18th International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS), 2015.

Carpentier, Alexandra and Munos, Remi. Bandit The-
ory meets Compressed Sensing for High-dimensional
In Proceedings of the 15th
Stochastic Linear Bandit.
International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), 2012.

Cesa-Bianchi, Nicolo, Gentile, Claudio, and Zappella, Gio-
vanni. A Gang of Bandits. In Proceedings of the 27th An-
nual Conference on Neural Information Processing Sys-
tems (NIPS), 2013.

Chu, Wei, Li, Lihong, Reyzin, Lev, and Schapire, Robert.
Contextual Bandits with Linear Payoff Functions.
In
Proceedings of the 14th International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS), 2011.

Crammer, Koby and Gentile, Claudio. Multiclass Classiﬁ-
cation with Bandit Feedback using Adaptive Regulariza-
In Proceedings of the 28th International Confer-
tion.
ence on Machine Learning (ICML), 2011.

Dai, Wei and Milenkovic, Olgica.

Subspace Pursuit
for Compressive Sensing Signal Reconstruction. IEEE
Transactions on Information Theory, 55(5):2230–2249,
2009.

Dekel, Ofer, Gentile, Claudio, and Sridharan, Karthik. Se-
lective Sampling and Active Learning from Single and
Multiple Teachers. Journal of Machine Learning Re-
search, 13:2655–2697, 2012.

Djolonga, Josip, Krause, Andreas, and Cevher, Volkan.
In Pro-
High-Dimensional Gaussian Process Bandits.
ceedings of the 27th Annual Conference on Neural In-
formation Processing Systems (NIPS), 2013.

Gentile, Claudio, Li, Shuai, and Zappella, Giovanni. On-
In Proceedings of the 31st
line Clustering of Bandits.
International Conference on Machine Learning (ICML),
2014.

Jain, Prateek, Tewari, Ambuj, and Kar, Purushottam.
On Iterative Hard Thresholding Methods for High-
dimensional M-Estimation. In Proceedings of the 28th
Annual Conference on Neural Information Processing
Systems (NIPS), 2014.

Kakade, S. and Tewari, A. On the Generalization Ability
of Online Strongly Convex Programming Algorithms. In
Proceedings of the 22nd Annual Conference on Neural
Information Processing Systems (NIPS), 2008.

Korda, Nathan, Szorenyi, Balazs, and Li, Shuai. Dis-
tributed Clustering of Linear Bandits in Peer to Peer Net-
works. In Proceedings of the 33rd International Confer-
ence on Machine Learning (ICML), 2016.

Krause, Andreas and Ong, Cheng Soon. Contextual Gaus-
sian Process Bandit Optimization. In Proceedings of the
25th Annual Conference on Neural Information Process-
ing Systems (NIPS), 2011.

Li, Lihong, Chu, Wei, Langford, John, and Schapire,
Robert. A Contextual-Bandit Approach to Personalized

On Context-Dependent Clustering of Bandits

News Article Recommendation. In Proceedings of the
19th International World Wide Web Conference (WWW),
2010.

Li, Shuai, Karatzoglou, Alexandros, and Gentile, Claudio.
Collaborative Filtering Bandits. In 39th Annual Interna-
tional ACM-SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR), 2016.

Maillard, Odalric-Ambrym and Mannor, Shie. Latent Ban-
dits. In Proceedings of the 31st International Conference
on Machine Learning (ICML), 2014.

Needell, Deanna and Tropp, Joel A. CoSaMP: Iterative
Signal Recovery from Incomplete and Inaccurate Sam-
ples. Applied Computational Harmonic Analysis, 26:
301–321, 2008.

Nguyen, Trong and Lauw, Hady. Dynamic Clustering of
Contextual Multi-Armed Bandits. In Proceedings of the
23rd ACM International Conference on Information and
Knowledge Management (CIKM), 2014.

Pilaszy, Istvan and Tikk, Domonkos. Recommending New
Movies: Even a Few Ratings Are More Valuable Than
Metadata. In Proceedings of the 3rd ACM Conference
on Recommender Systems (RecSys), 2009.

Sutskever, Ilya, Salakhutdinov, Ruslan, and Tenenbaum,
Joshua. Modelling Relational Data using Bayesian Clus-
tered Tensor Factorization. In Proceedings of the 23rd
Annual Conference on Neural Information Processing
Systems (NIPS), 2009.

Tropp, Joel A. Freedman’s Inequality for Matrix Martin-
gales. Electronic Communications in Probability, 16:
262–270, 2011.

Wu, Qingyun, Wang, Huazheng, Gu, Quanquan, and
Wang, Hongning. Contextual Bandits in A Collaborative
Environment. In 39th Annual International ACM-SIGIR
Conference on Research and Development in Informa-
tion Retrieval (SIGIR), 2016.

Yue, Yisong, Hong, Sue Ann, and Guestrin, Carlos. Hierar-
chical Exploration for Accelerating Contextual Bandits.
In Proceedings of the 29th International Conference on
Machine Learning (ICML), 2012.

Zhou, Li and Brunskill, Emma. Latent Contextual Ban-
dits and their Application to Personalized Recommenda-
tions for New Users. In Proceedings of the 25th Inter-
national Joint Conference on Artiﬁcial Intelligence (IJ-
CAI), 2016.

