Fast Bayesian Permanental Processes

A. Supplementary Material

Accompanying the submission Fast Bayesian Intensity Estimation for the Permanental Process.

A.1. Exact Expected Log Loss
We evaluate our estimated ˆλ using the expectation under the true PP(λ) of the log likelihood under PP(ˆλ), where PP is
the Poisson process. Adams et al. (2009) approximate this quantity using Monte Carlo, employing numerical integration
for (1). It turns out that for the computational cost of one such numerical integration, we may compute the expected loss
using standard results for L´evy processes (Cont & Tankov, 2004). An elementary self contained argument runs as follows:
(cid:104)

(cid:105)(cid:105)

(cid:104)

(cid:104)

EX∼PP(λ)

(cid:105)
log pX∼PP(ˆλ)(X)

= Ecard(X)

EX∼PP(λ)| card(X)
(cid:16)

(cid:104)

card(X)

log ˆΛ(Ω) + H(λ, ˆλ)

− ˆΛ(Ω)

(cid:17)

(cid:105)

log pX∼PP(ˆλ)(X)

= Ecard(X)
(cid:16)

= Γ(Ω)
(cid:90)

(cid:16)

=

x∈Ω

log ˆΛ(Ω) + H(λ, ˆλ)

− ˆΛ(Ω)

λ(x) log ˆλ(x) − ˆλ(x)

dx,

(cid:17)

(cid:17)

where Ω is the sampling domain, H(λ, ˆλ) := (cid:82)
λ(x)
Λ(Ω) log
functions proportional to λ and ˆλ and we recall Λ(S) := (cid:82)
x∈S λ(x) dx. The ﬁrst line is the tower law of expectation. To
see the second line, note that we may sample X ∼ PP(λ) by ﬁrst sampling card(X) ∼ Poisson(Λ(Ω)), and then drawing
each element of X according to the probability density proportional to λ. The third line uses the Poisson expectation
Ecard(X) [card(X)] = Γ(Ω) and the fourth some simple algebra.

dx is the cross-entropy between the probability density

ˆλ(x)
ˆΛ(Ω)

x∈Ω

As an aside, we may therefore write the Kullback-Leibler divergence in a form resembling that for probability distributions:

DKL

(cid:0)PP(f )(cid:13)

(cid:13) PP(g)(cid:1) = EX∼PP(λ)
(cid:90)

(cid:18)

(cid:105)
(cid:104)
log pX∼PP(λ)(X) − log pX∼PP(ˆλ)(X)

=

f (x) log

+ g(x) − f (x)

dx.

x∈Ω

f (x)
g(x)

(cid:19)

A.2. Bayesian Decision Theory for the Expected Log Loss

To determine the intensity function which maximises the expected log likelihood we deﬁne the loss
(cid:96)(λ, λ(cid:48)) := Eni∼N (Bi),i=1,2,...,m|λ log p (N (Bi) = ni, i = 1, 2, . . . , m|λ(cid:48))
where N (Bi) is the random variable representing the number of points in the set Bi ⊆ Ω, Ω is the domain of the process
and we recall Λ(S) := (cid:82)

x∈S λ(x) dx. It is well known that (Baddeley, 2007)

p(N (Bi) = ni, i = 1, 2, . . . , m|λ) =

exp(−Λ(Ω)).

Λ(Bi)ni
ni!

(cid:89)

i

Bayesian decision theory considers the expected loss

L(λ(cid:48)) := Eλ|D [(cid:96)(λ, λ(cid:48))] ,

where the expectation is with respect to the posterior predictive distribution given the data D. Combining these expressions
and assuming without loss of generality that Ω = (cid:83)

i Bi yields

L(λ(cid:48)) = Eλ|D

Eni∼N (Bi),i=1,2,...,m|λ

(ni log Λ(cid:48)(Bi) − log(ni!) − Λ(cid:48)(Bi))

(cid:34)

(cid:104) (cid:88)

i

(cid:35)

(cid:105)

.

The optimal choice is Λ∗ := argmaxλ(cid:48) L(λ(cid:48)), so by stationarity

and so λ∗ = Eλ|D[λ], the expectation of the posterior predictive distribution.

λ∗(Bi) = Eλ|D

(cid:2) Eni∼N (Bi)|λ [ni](cid:3)

= Eλ|D [Λ(Bi)] ,

Fast Bayesian Permanental Processes

(a) λ0

(b) λ1

(c) λ2

(d) λ3

(e) λ4

Figure 6. Predictive distributions for the test problems of subsection 6.2.

0.00.51.01.52.02.53.0inputdomainΩ=[0,π]02040Poissonintensityλdatalocationsxitrueintensityλ(x)predictivemedian[0.1,0.9]pred.intervalpredictivesamples0.00.51.01.52.02.53.00500.00.51.01.52.02.53.0025500.00.51.01.52.02.53.0020400.00.51.01.52.02.53.0050A.3. Standard Laplace Approximations for the GP

Fast Bayesian Permanental Processes

Following e.g. (Rasmussen & Williams, 2006), assume that we are given an independent and identically distributed sample
{(xi, yi)}1≤i≤m, and the goal is to estimate p(y|x). Let the true joint in f = (f (xi))i, y = (y(xi))i be

where K = (k(xi), xj)ij and X = (x1, x2, . . . , xm). The Laplace approximation ﬁts a normal to the posterior,

log p(y, f |X, k) = log p(y|f ) + log p(f |X, k)

= log p(y|f ) −

f (cid:62)K −1f −

log |K| −

log 2π,

1
2

log p(f |y, X) ≈ log N (f | ˆf , Q)

= −

(f − ˆf )(cid:62)Q−1(f − ˆf ) −

log |Q| −

log 2π

1
2

:= log q(f |y, X).

1
2

1
2

m
2

m
2

ˆf and Q come from a second order approximation of the log posterior at its mode, i.e.

ˆf = argmax

p(y|f , X)

f

= argmax
f

p(y, f |X)

Q−1 = −

∂2

(cid:12)
(cid:12)
∂f ∂f (cid:62) log p(y, f |X)
(cid:12)
(cid:12)f = ˆf

= K −1 + W
∂2
∂f 2
i

Wii = −

log p(yi|fi)

(cid:12)
(cid:12)
(cid:12)
(cid:12)fi= ˆfi

Taylor expanding log p(y, f |X) at f = ˆf ,

log p(y, f |X) ≈ log p(y, ˆf |X) −

(f − ˆf )(cid:62)Q−1(f − ˆf )

(17)

ˆf (cid:62)K −1 ˆf −

log |K| −

log 2π −

(f − ˆf )(cid:62)Q−1(f − ˆf )

1
2

m
2

1
2

Now

log

exp(−

x(cid:62)H −1x)dx =

log 2π +

log |H|

So we get the approximate marginal likelihood

1
2
= log p(y|f = ˆf ) −

:= log q(y, f |X)

1
2

1
2

(cid:90)

(cid:90)

log Z := log p(y|X)

≈ log

q(y, f |X)df

= log p(y|f = ˆf ) −

ˆf (cid:62)K −1 ˆf −

log |K| −

log |K −1 + W |

= log p(y|f = ˆf ) −

ˆf (cid:62)K −1 ˆf −

log |I + KW |

1
2
1
2

This is a standard textbook approach (Rasmussen & Williams, 2006), but we can get the same approximation via

log p(y|X) ≈ log q(y, ˆf |X) − log q( ˆf |y, X),

since the right hand side is true for all f , not just ˆf . Hence we need only subtract the approximate log likelihoods as above.
By evaluating at ˆf , the second r.h.s. term in (17), vanishes immediately.

(18)

(19)

1
2

1
2

m
2

1
2
1
2

