Analytical Guarantees on Numerical Precision of Deep Neural Networks

Charbel Sakr Yongjune Kim Naresh Shanbhag

Abstract
The acclaimed successes of neural networks of-
ten overshadow their tremendous complexity.
We focus on numerical precision - a key param-
eter deﬁning the complexity of neural networks.
First, we present theoretical bounds on the ac-
curacy in presence of limited precision.
Inter-
estingly, these bounds can be computed via the
back-propagation algorithm. Hence, by com-
bining our theoretical analysis and the back-
propagation algorithm, we are able to readily de-
termine the minimum precision needed to pre-
serve accuracy without having to resort to time-
consuming ﬁxed-point simulations. We provide
numerical evidence showing how our approach
allows us to maintain high accuracy but with
lower complexity than state-of-the-art binary net-
works.

1. Introduction

Neural networks have achieved state-of-the-art accuracy on
many machine learning tasks. AlexNet (Krizhevsky et al.,
2012) had a deep impact a few years ago in the ImageNet
Large Scale Visual Recognition Challenge (ILSVRC) and
triggered intensive research efforts on deep neural net-
works. Recently, ResNet (He et al., 2016) has outper-
formed humans in recognition tasks.

For

These networks have very high computational complex-
ity.
instance, AlexNet has 60 million param-
eters and 650,000 neurons (Krizhevsky et al., 2012).
Its convolutional
layers alone require 666 million
multiply-accumulates (MACs) per 227 × 227 image (13k
MACs/pixel) and 2.3 million weights (Chen et al., 2016).
Deepface’s network involves more than 120 million pa-
rameters (Taigman et al., 2014). ResNet is a 152-layer

The authors are with the University of Illinois at Urbana-
IL 61801 USA.
Champaign, 1308 W Main St., Urabna,
Correspondence
Charbel Sakr <sakr2@illinois.edu>,
Yongjune Kim <yongjune@illinois.edu>, Naresh Shanbhag
<shanbhag@illinois.edu>.

to:

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

deep residual network. This high complexity of deep
neural networks prevents its deployment on energy and
resource-constrained platforms such as mobile devices and
autonomous platforms.

1.1. Related Work

One of the most effective approaches for reducing resource
utilization is to implement ﬁxed-point neural networks. As
mentioned in (Lin et al., 2016a), there are two approaches
for designing ﬁxed-point neural networks: (1) directly train
a ﬁxed-point neural network, and (2) quantize a pre-trained
ﬂoating-point neural network to obtain a ﬁxed-point net-
work.

As an example of ﬁxed-point training, Gupta et al. (2015)
showed that 16-bit ﬁxed-point representation incurs little
accuracy degradation by using stochastic rounding. A more
aggressive approach is to design binary networks such as
Kim & Smaragdis (2016) which used bitwise operations to
replace the arithmetic operations and Rastegari et al. (2016)
which explored optimal binarization schemes. BinaryCon-
nect (Courbariaux et al., 2015) trained networks using
binary weights while BinaryNet (Hubara et al., 2016b)
trained networks with binary weights and activations.

Although these ﬁxed-point training approaches make it
possible to design ﬁxed-point neural networks achieving
excellent accuracy, training based on ﬁxed-point arithmetic
is generally harder than ﬂoating-point training since the op-
timization is done in a discrete space.

Hence, in this paper, we focus on the second approach
that quantizes a pre-trained ﬂoating-pointing network to a
ﬁxed-point network. This approach leverages the exten-
sive work in training state-of-the-art ﬂoating-point neural
networks such as dropout (Srivastava et al., 2014), maxout
(Goodfellow et al., 2013), network-in-network (Lin et al.,
2013), and residual learning (He et al., 2016) to name a
few. In this approach, proper precision needs to be deter-
mined after training to reduce complexity while minimiz-
ing the accuracy loss. In (Hwang & Sung, 2014), exhaus-
tive search is performed to determine a suitable precision
allocation. Recently, Lin et al. (2016a) offered an analytical
solution for non-uniform bit precision based on the signal-
to-quantization-noise ratio (SQNR). However, the use of
non-uniform quantization step sizes at each layer is difﬁ-

Analytical Guarantees on Numerical Precision of Deep Neural Networks

cult to implement as it requires multiple variable precision
arithmetic units.

In addition to ﬁxed-point
implementation, many ap-
proaches have been proposed to lower the complexity of
deep neural networks in terms of the number of arithmetic
operations. Han et al. (2015) employs a three-step training
method to identify important connections, prune the unim-
portant ones, and retrain on the pruned network. Zhang
et al. (2015) replaces the original convolutional layers by
smaller sequential layers to reduce computations. These
approaches are complementary to our technique of quantiz-
ing a pre-trained ﬂoating-point neural network into a ﬁxed-
point one.

In this paper, we obtain analytical bounds on the accu-
racy of ﬁxed-point networks that are obtained by quantizing
a conventionally trained ﬂoating-point network. Further-
more, by deﬁning meaningful measures of a ﬁxed-point
network’s hardware complexity viz.
computational and
representational costs, we develop a principled approach to
precision assignment using these bounds in order to mini-
mize these complexity measures.

1.2. Contributions

Our contributions are both theoretical and practical. We
summarize our main contributions:

• We derive theoretical bounds on the misclassiﬁcation
rate in presence of limited precision and thus deter-
mine analytically how accuracy and precision trade-
off with each other.

• Employing the theoretical bounds and the back-
propagation algorithm, we show that proper precision
assignments can be readily determined while main-
taining accuracy close to ﬂoating-point networks.
• We analytically determine which of weights or acti-
vations need more precision, and we show that typi-
cally the precision requirements of weights are greater
than those of activations for fully-connected networks
and are similar to each other for networks with shared
weights such as convolutional neural networks.

• We introduce computational and representational
costs as meaningful metrics to evaluate the complexity
of neural networks under ﬁxed precision assignment.
• We validate our ﬁndings on the MNIST and CIFAR10
datasets demonstrating the ease with which ﬁxed-
point networks with complexity smaller than state-
of-the-art binary networks can be derived from pre-
trained ﬂoating-point networks with minimal loss in
accuracy.

It is worth mentioning that our proposed method is general
and can be applied to every class of neural networks such as
multilayer perceptrons and convolutional neural networks.

2. Fixed-Point Neural Networks

2.1. Accuracy of Fixed-Point and Floating-Point

Networks

For a given ﬂoating-point neural network and its ﬁxed-
point counterpart we deﬁne: 1) the ﬂoating-point error
probability pe,f l = Pr{ ˆYf l (cid:54)= Y } where ˆYf l is the output
of the ﬂoating-point network and Y is the true label; 2) the
ﬁxed-point error probability pe,f x = Pr{ ˆYf x (cid:54)= Y } where
ˆYf x is the output of the ﬁxed-point network; 3) the mis-
match probability between ﬁxed-point and ﬂoating-point
pm = Pr{ ˆYf x (cid:54)= ˆYf l}. Observe that:

pe,f x ≤ pe,f l + pm

(1)

The right-hand-side represents the worst case of having no
overlap between misclassiﬁed samples and samples whose
predicted labels are in error due to quantization. We pro-
vide a formal proof of (1) in the supplementary section.
Note that pe,f x is a quantity of interest as it characterizes
the accuracy of the ﬁxed-point system. We employ pm as
a proxy to pe,f x because it brings in the effects of quan-
tization into the picture as opposed to pe,f l which solely
depends on the algorithm. This observation was made in
(Sakr et al., 2017) and allowed for an analytical characteri-
zation of linear classiﬁers as a function of precision.

2.2. Fixed-Point Quantization

The study of ﬁxed-point systems and algorithms is well es-
tablished in the context of signal processing and communi-
cation systems (Shanbhag, 2016). A popular example is the
least mean square (LMS) algorithm for which bounds on
precision requirements for input, weights, and updates have
been derived (Goel & Shanbhag, 1998). In such analyses,
it is standard practice (Caraiscos & Liu, 1984) to assume
all signed quantities lie in [−1, 1] and all unsigned quanti-
ties lie in [0, 2]. Of course, activations and weights can be
designed to satisfy this assumption during training. A B-
bit ﬁxed-point number af x would be related to its ﬂoating-
point value a as follows:

af x = a + qa

(2)

where qa is the quantization noise which is modeled as
an independent uniform random variable distributed over
(cid:3), where ∆ = 2−(B−1) is the quantization step
(cid:2)− ∆
(Caraiscos & Liu, 1984).

2 , ∆

2

2.3. Complexity in Fixed-Point

We argue that the complexity of a ﬁxed-point system has
two aspects: computational and representational costs. In
what follows, we consider activations and weights to be
quantized to BA and BW bits, respectively.

Analytical Guarantees on Numerical Precision of Deep Neural Networks

The computational cost is a measure of the computational
resources utilized for generating a single decision, and is
deﬁned as the number of 1 bit full adders (F As). A full
adder is a canonical building block of arithmetic units. We
assume arithmetic operations are executed using the com-
monly used ripple carry adder (Knauer, 1989) and Baugh-
Wooley multiplier (Baugh & Wooley, 1973) architectures
designed using F As. Consequently, the number of F As
used to compute a D-dimensional dot product of activa-
tions and weights is (Lin et al., 2016b):

DBABW + (D − 1)(BA + BW + (cid:100)log2(D)(cid:101) − 1) (3)

Hence, an important aspect of the computational cost of a
dot product is that it is an increasing function of the product
of activation precision (BA), weight precision (BW ), and
dimension (D).

We deﬁne the representational cost as the total number of
bits needed to represent all network parameters, i.e., both
activations and weights. This cost is a measure of the stor-
age complexity and communications costs associated with
data movement. The total representational cost of a ﬁxed-
point network is:

|A| BA + |W| BW

(4)

bits, where A and W are the index sets of all activations
and weights in the network, respectively. Observe that the
representational cost is linear in activation and weight pre-
cisions as compared to the computational cost.

Equations (3) - (4) illustrate that, though computational and
representational costs are not independent, they are differ-
ent. Together, they describe the implementation costs asso-
ciated with a network. We shall employ both when evalu-
ating the complexity of ﬁxed-point networks.

2.4. Setup

Here, we establish notation. Let us consider neural net-
works deployed on a M -class classiﬁcation task. For
a given input, the network would typically have M nu-
merical outputs {zi}M
i=1 and the decision would be ˆy =
zi. Each numerical output is a function of
arg max

i=1,...,M

weights and activations in the network:

where qah and qwh are the quantization noise terms of the
activation ah and weight wh, respectively. Here, {qah }h∈A
are independent uniformly distributed random variables
(cid:3) and {qwh }h∈W are independent uniformly
on (cid:2)− ∆A
distributed random variables on (cid:2)− ∆W
(cid:3), with ∆A =
2−(BA−1) and ∆W = 2−(BW −1).

2 , ∆W

2 , ∆A

2

2

In quantization noise analysis, it is standard to ignore cross-
products of quantization noise terms as their contribution is
negligible. Therefore, using Taylor’s theorem, we express
the total quantization noise at the output of the ﬁxed-point
network as:

qzi =

(cid:88)

h∈A

qah

∂zi
∂ah

(cid:88)

+

qwh

h∈W

∂zi
∂wh

.

(7)

Note that the derivatives in (7) are obtained as part of the
back-propagation algorithm. Thus, using our results, it
is possible to estimate the precision requirements of deep
neural networks during training itself. As will be shown
later, this requires one additional back-propagation itera-
tion to be executed after the weights have converged.

3. Bounds on Mismatch Probability

3.1. Second Order Bound

We present our ﬁrst result. It is an analytical upper bound
on the mismatch probability pm between a ﬁxed-point neu-
ral network and its ﬂoating-point counterpart.

Theorem 1. Given BA and BW , the mismatch probabil-
ity pm between a ﬁxed-point network and its ﬂoating-point
counterpart is upper bounded as follows:

pm ≤

∆2
A
24

E








M
(cid:88)

i=1
i(cid:54)= ˆYf l

(cid:80)
h∈A

∂(Zi−Z ˆYf l
∂Ah

(cid:12)
(cid:12)
(cid:12)
(cid:12)
|Zi − Z ˆYf l

|2

2



)

(cid:12)
(cid:12)
(cid:12)
(cid:12)






+

∆2
W
24

E








M
(cid:88)

i=1
i(cid:54)= ˆYf l

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)
h∈W

∂(Zi−Z ˆYf l
∂wh

)

|Zi − Z ˆYf l

|2

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)








(8)

zi = f ({ah}h∈A, {wh}h∈W )

(5)

where expectations are taken over a random input and
{Ah}h∈A, {Zi}M

i=1, and ˆYf l are thus random variables.

for i = 1, . . . , M , where ah denotes the activation indexed
by h and wh denotes the weight indexed by h. When acti-
vations and weights are quantized to BA and BW bits, re-
spectively, the output zi is corrupted by quantization noise
qzi so that:

zi + qzi = f ({ah + qah}h∈A, {wh + qwh }h∈W )

(6)

Proof. The detailed proof can be found in the supplemen-
tary section. Here, we provide the main idea and the intu-
ition behind the proof.

of

heart

evaluating
The
the
(cid:1) for any pair of outputs zi
Pr (cid:0)zi + qzi > zj + qzj
and zj where zj > zi. Equivalently, we need to evaluate

proof

lies

in

Analytical Guarantees on Numerical Precision of Deep Neural Networks

clearly highlights the gains in practicality of our analytical
approach over a trial-and-error based search.

Finally, (8) reveals a very interesting aspect of the trade-off
between activation precision BA and weight precision BW .
We rewrite (8) as:

(9)

pm ≤ ∆2

AEA + ∆2

W EW

(11)

Pr (cid:0)qzi − qzj > zj − zi

(cid:1). But from (7), we have:

qzi − qzj =

(cid:88)

h∈A

qah

∂(zi − zj)
∂ah

(cid:88)

+

qwh

h∈W

∂(zi − zj)
∂wh

.

linear
qzi − qzj
a

combination of quan-
In (9), we have
a
is a zero mean
tization noise terms,
distribu-
random variable
This means that Pr (cid:0)qzi − qzj > zj − zi
(cid:1) =
tion.
2 Pr (cid:0)|qzi − qzj | > |zj − zi|(cid:1), which allows us to use
1
Chebyshev’s inequality. Indeed, from (9), the variance of
qzi − qzj is given by:

symmetric

having

∆2
A
12

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

h∈A

∂(zi − zj)
∂ah

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

∆2
W
12

(cid:88)

h∈W

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂(zi − zj)
∂wh

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

where

and

so that

Pr (cid:0)zi + qzi > zj + qzj

(cid:1)

(cid:80)

∆2
A

h∈A

∂(zi−zj )
∂ah

(cid:12)
(cid:12)
(cid:12)

≤

(cid:80)

2

(cid:12)
(cid:12)
(cid:12)

+ ∆2
W
24 |zi − zj|2

h∈W

(cid:12)
(cid:12)
(cid:12)

∂(zi−zj )
∂wh

2

(cid:12)
(cid:12)
(cid:12)

.

(10)

As explained in the supplementary section, it is possible to
obtain to (8) from (10) using standard probabilistic argu-
ments.

Before proceeding, we point out that the two expecta-
tions in (8) are taken over a random input but the weights
{wh}h∈W are frozen after training and are hence determin-
istic.

Several observations are to be made. First notice that the
mismatch probability pm increases with ∆2
W . This
is to be expected as smaller precision leads to more mis-
match. Theorem 1 says a little bit more:
the mismatch
probability decreases exponentially with precision, because
∆A = 2−(BA−1) and ∆W = 2−(BW −1).

A and ∆2

Note that the quantities in the expectations in (8) can be
obtained as part of a standard back-propagation procedure.
Indeed, once the weights are frozen, it is enough to per-
form one forward pass on an estimation set (which should
have statistically signiﬁcant cardinality), record the numer-
ical outputs, perform one backward pass and probe all rel-
evant derivatives. Thus, (8) can be readily computed.

Another practical aspect of Theorem 1 is that this opera-
tion needs to be done only once as these quantities do not
depend on precision. Once they are determined, for any
given precision assignment, we simply evaluate (8) and
combine it with (1) to obtain an estimate (upper bound) on
the accuracy of the ﬁxed-point instance. This way the pre-
cision necessary to achieve a speciﬁc mismatch probabil-
ity is obtained from a trained ﬂoating-point network. This

EA = E








M
(cid:88)

i=1
i(cid:54)= ˆYf l

∂(Zi−Z ˆYf l
∂Ah

(cid:80)
h∈A
24|Zi − Z ˆYf l

)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
|2

2








EW = E








(cid:80)
h∈W

M
(cid:88)

∂(Zi−Z ˆYf l
∂wh

)

24|Zi − Z ˆYf l

|2

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)








.

i=1
i(cid:54)= ˆYf l

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

The ﬁrst term in (11) characterizes the impact of quantizing
activations on the overall accuracy while the second char-
acterizes that of weight quantization. It might be the case
that one of the two terms dominates the sum depending on
the values of EA and EW . This means that either the ac-
tivations or the weights are assigned more precision than
necessary. An intuitive ﬁrst step to efﬁciently get a smaller
upper bound is to make the two terms of comparable order.
That can be made by setting ∆2
W EW which is
equivalent to

AEA = ∆2

BA − BW = round

log2

(12)

(cid:32)

(cid:33)

(cid:114) EA
EW

where round() denotes the rounding operation. This is an
effective way of taking care of one of the two degrees of
freedom introduced by (8).

A natural question to ask would be which of EA and EW
is typically larger. That is to say, to whom, activations or
weights, should one assign more precision. In deep neu-
ral networks, there are more weights than activations, a
trend particularly observed in deep networks with most lay-
ers fully connected. This trend, though not as pronounced,
is also observed in networks with shared weights, such as
convolutional neural networks. However, there exist a few
counterexamples such as the networks in (Hubara et al.,
2016b) and (Hubara et al., 2016a). It is thus reasonable to
expect EW ≥ EA, and consequently the precision require-
ments of weights will, in general, be more than those of
activations.

One way to interpret (11) is to consider minimizing the up-
per bound in (8) subject to BA+BW = c for some constant

Analytical Guarantees on Numerical Precision of Deep Neural Networks

c. Indeed, it can be shown that (12) would be a necessary
condition of the corresponding solution. This is an appli-
cation of the arithmetic-geometric mean inequality. Effec-
tively, (11) is of particular interest when considering com-
putational cost which increases as a function of the product
of both precisions (see Section 2.3).

3.2. Tighter Bound

We present a tighter upper bound on pm based on the Cher-
noff bound.
Theorem 2. Given BA and BW , the mismatch probabil-
ity pm between a ﬁxed-point network and its ﬂoating-point
counterpart is upper bounded as follows:

pm ≤ E

e−S(i, ˆYf l )

P (i, ˆYf l)

P (i, ˆYf l)

1

2

(13)













M
(cid:88)

i=1
i(cid:54)= ˆYf l

where, for i (cid:54)= j,

S(i,j) =

(cid:80)

h∈A

(cid:16)
D(i,j)
Ah

3(Zi − Zj)2
+ (cid:80)

(cid:17)2

h∈W

(cid:17)2 ,

(cid:16)

D(i,j)
wh

D(i,j)
Ah

=

∆A
2

∂(Zi − Zj)
∂Ah

, D(i,j)
wh

∂(Zi − Zj)
∂wh

,

(cid:16)

=

∆W
2
T (i,j)D(i,j)
Ah
T (i,j)D(i,j)
Ah
(cid:16)
T (i,j)D(i,j)
wh
T (i,j)D(i,j)
wh

(cid:17)

(cid:17)

,

,

P (i,j)
1

=

P (i,j)
2

=

sinh

sinh

(cid:89)

h∈A

(cid:89)

h∈W

and

T (i,j) =

S(i,j)
Zj − Zi

.

Proof. Again, we leave the technical details for the supple-
mentary section. Here we also provide the main idea and
intuition.

in Theorem 1, we shall

(cid:1) = Pr (cid:0)qzi − qzj > zj − zi

focus on evaluating
As
(cid:1) for
Pr (cid:0)zi + qzi > zj + qzj
any pair of outputs zi and zj where zj > zi. The key
difference here is that we will use the Chernoff bound in
order to account for the complete quantization noise statis-
tics. Indeed, letting v = zj − zi, we have:

Pr (cid:0)qzi − qzj > v(cid:1) ≤ e−tvE

(cid:104)

et(qzi −qzj )(cid:105)

for any t > 0. We show that:

(cid:104)

et(qzi −qzj )(cid:105)

E

=

(cid:89)

sinh (tda,h)
tda,h

(cid:89)

h∈W

sinh (tdw,h)
tdw,h

h∈A

where da,h = ∆A
2
yields:

∂(zi−zj )
∂ah

and dw,h = ∆W
2

∂(zi−zj )
∂wh

. This

Pr (cid:0)qzi − qzj > v(cid:1)
≤ e−tv (cid:89)

sinh (tda,h)
tda,h

(cid:89)

h∈W

sinh (tdw,h)
tdw,h

.

h∈A

(14)

We show that the right-hand-side is minimized over posi-
tive values of t when:

t =

(cid:80)

h∈A (da,h)2 + (cid:80)

h∈W (dw,h)2 .

3v

Substituting this value of t into (14) and using standard
probabilistic arguments, we obtain (13).

2

1

P (i, ˆYf l)

The ﬁrst observation to be made is that Theorem 2 indicates
that, on average, pm is upper bounded by an exponentially
decaying function of the quantity S(i, ˆYf l) for all i (cid:54)= ˆYf l up
to a correction factor P (i, ˆYf l)
. This correction fac-
tor is a product of terms typically centered around 1 (each
term is of the form sinh(x)
x ≈ 1 for small x). On the other
hand, S(i, ˆYf l), by deﬁnition, is the ratio of the excess con-
ﬁdence the ﬂoating-point network has in the label ˆYf l over
the total quantization noise variance reﬂected at the output,
i.e., S(i, ˆYf l) is the SQNR. Hence, Theorem 2 states that the
tolerance of a neural network to quantization is, on aver-
age, exponentially decaying with the SQNR at its output.
In terms of precision, Theorem 2 states that pm is bounded
by a double exponentially decaying function of precision
(that is an exponential function of an exponential function).
Note how this bound is tighter than that of Theorem 1.

This double exponential relationship between accuracy and
precision is not too surprising when one considers the prob-
lem of binary hypothesis testing under additive Gaussian
noise (Blahut, 2010) scenario. In this scenario, it is well-
known that the probability of error is an exponentially de-
caying function of the signal-to-noise ratio (SNR) in the
high-SNR regime. Theorem 2 points out a similar relation-
ship between accuracy and precision but it does so using
rudimentary probability principles without relying on high-
SNR approximations.

While Theorem 2 is much tighter than Theorem 1 theoreti-
cally, it is not as convenient to use. In order to use Theorem
2, one has to perform a forward-backward pass and select
relevant quantities and apply (13) for each choice of BA
and BW . However, a lot of information, e.g.
the deriva-
tives, can be reused at each run, and so the runs may be
lumped into one forward-backward pass. In a sense, the
complexity of computing the bound in Theorem 2 lies be-
tween the evaluation of (11) and the complicated conven-
tional trial-and-error based search.

We now illustrate the applications of these bounds.

Analytical Guarantees on Numerical Precision of Deep Neural Networks

(a)

(b)

Figure 1: Validity of bounds for MNIST when: (a) BW = BA and (b) BW = BA + 3 as dictated by (12) (EA = 41 and
EW = 3803 so that log2

≈ 3.2). FX Sim denotes ﬁxed point simulations.

(cid:113) EW
EA

4. Simulation Results

We conduct numerical simulations to illustrate both the va-
lidity and usefulness of the analysis developed in the previ-
ous section. We show how it is possible to reduce precision
in an aggressive yet principled manner. We present results
on two popular datasets: MNIST and CIFAR-10. The met-
rics we address are threefold:

• Accuracy measured in terms of test error.
• Computational cost measured in #F As (see Section

2.3, (3) was used to compute #F As per MAC).

• Representational cost measured in bits (see Section

2.3, (4) was used).

We compare our results to similar works conducting sim-
ilar experiments: 1) the work on ﬁxed-point training with
stochastic quantization (SQ) (Gupta et al., 2015) and 2) Bi-
naryNet (BN) (Hubara et al., 2016b).

4.1. DNN on MNIST

First, we conduct simulations on the MNIST dataset for
handwritten character recognition (LeCun et al., 1998).
The dataset consists of 60K training samples and 10K test
samples. Each sample consists of an image and a label. Im-
ages are of size 28 × 28 pixels representing a handwritten
digit between 0 and 9. Labels take the value of the corre-
sponding digit.

In this ﬁrst experiment, we chose an architecture of 784 −
512 − 512 − 512 − 10, i.e., 3 hidden layers, each of 512
units. We ﬁrst trained the network in ﬂoating-point using
the back-propagation algorithm. We used a batch size of
200 and a learning rate of 0.1 with a decay rate of 0.978
per epoch. We restore the learning rate every 100 epochs,
the decay rate makes the learning rate vary between 0.1
and 0.01. We train the ﬁrst 300 epochs using 15% dropout,
the second 300 epochs using 20% dropout, and the third

300 epochs using 25% dropout (900 epochs overall).
It
appears from the original dropout work (Srivastava et al.,
2014) that the typical 50% dropout fraction works best for
very wide multi-layer perceptrons (MLPs) (4096 to 8912
hidden units). For this reason, we chose to experiment with
smaller dropout fractions.

The only pre-processing done is to scale the inputs between
−1 and 1. We used ReLU activations with the subtle ad-
dition of a right rectiﬁer for values larger than 2 (as dis-
cussed in Section 2). The resulting activation is also called
a hard sigmoid. We also clipped the weights to lie in [−1, 1]
at each iteration. The resulting test error we obtained in
ﬂoating-point is 1.36%.

Figure 1 illustrates the validity of our analysis.
Indeed,
both bounds (based on Theorems 1 & 2) successfully up-
per bound the test error obtained through ﬁxed-point sim-
ulations. Figure 1 (b) demonstrates the utility of (12). In-
deed, setting BW = BA allows us to reduce the precision
to about 6 or 7 bits before the accuracy start degrading. In
addition, under these conditions we found EA = 41 and
(cid:113) EW
EW = 3803 so that log2
≈ 3.2. Thus, setting
EA
BW = BA + 3 as dictated by (12) allows for more ag-
gressive precision reduction. Activation precision BA can
now be reduced to about 3 or 4 bits before the accuracy de-
grades. To compute the bounds, we used an estimation set
of 1000 random samples from the dataset.

We compare our results with SQ which used a 784−1000−
1000−10 architecture on 16-bit ﬁxed-point activations and
weights. A stochastic rounding scheme was used to com-
pensate for quantization. We also compare our results with
BN with a 784 − 2048 − 2048 − 2048 − 10 architecture on
binary quantities. A stochastic rounding scheme was also
used during training.

Table 1 shows some comparisons with related works in
terms of accuracy, computational cost, and representational

  (bits)      Test errorAB  (bits)        CDTest errorAnalytical Guarantees on Numerical Precision of Deep Neural Networks

Precision Assignment

Test error (%)

Floating-point
(8, 8)
(6, 6)
(6, 9)
(4, 7)
SQ (16, 16) (Gupta et al., 2015)
BN (1, 1) (Hubara et al., 2016b)

1.36
1.41
1.54
1.35
1.43
1.4
1.4

Computational
Cost (106 F As)
N/A
82.9
53.1
72.7
44.7
533
117

Representational
Cost (106 bits)
N/A
7.5
5.63
8.43
6.54
28
10

Table 1: Results for MNIST: Comparison of accuracy, computational cost, and representational cost with state-of-the-art
related works. Chosen precision assignments are obtained from Figure 1.

cost. For comparison, we selected four notable design op-
tions from Figures 1 (a,b):

A. Smallest (BA, BW ) such that BW = BA and
In this case

pm ≤ 1% as bounded by Theorem 1.
(BA, BW ) = (8, 8).

B. Smallest (BA, BW ) such that BW = BA and
In this case

pm ≤ 1% as bounded by Theorem 2.
(BA, BW ) = (6, 6).

C. Smallest (BA, BW ) such that BW = BA + 3 as dic-
tated by (12) and pm ≤ 1% as bounded by Theorem
1. In this case (BA, BW ) = (6, 9).

D. Smallest (BA, BW ) such that BW = BA + 3 as dic-
tated by (12) and pm ≤ 1% as bounded by Theorem
2. In this case (BA, BW ) = (4, 7).

As can be seen in Table 1, the accuracy is similar across
all design options including the results reported by SQ
and BN. Interestingly, for all four design options, our net-
work has a smaller computational cost than BN. In addi-
tion, SQ’s computational cost is about 4.6× that of BN
(533M/117M). The greatest reduction in computational
cost is obtained for a precision assignment of (4, 7) cor-
responding to a 2.6× and 11.9× reduction compared to
BN (117M/44.7M) and SQ (533M/44.7M), respectively.
The corresponding test error rate is of 1.43%. Similar
trends are observed for representational costs. Again, our
four designs have smaller representational cost than even
BN. BN itself has 2.8× smaller representational cost than
SQ (28M/10M). Note that a precision assignment of (6, 6)
yields 1.8× and 5.0× smaller representational costs than
BN (10M/5.63M) and SQ (28M/5.63M), respectively. The
corresponding test error rate is 1.54%.

The fact that we are able to achieve lesser computational
and representational costs than BN while maintaining sim-
ilar accuracy highlights two important points. First, the
width of a network severely impacts its complexity. We
made our network four times as narrow as BN’s and still
managed to use eight times as many bits per parameter
without exceeding BN’s complexity. Second, our results il-
lustrate the strength of numbering systems, speciﬁcally, the

Figure 2: Validity of bounds for CIFAR when BW = BA
which is also dictated by (12) (EA = 21033 and EW =
≈ 0.29). FX Sim denotes ﬁxed
31641 so that log2
point simulations.

(cid:113) EW
EA

strength of ﬁxed-point representations. Our results indicate
that a correct and meaningful multi-bit representation of
parameters is better in both complexity and accuracy than
a 1-bit unstructured allocation.

4.2. CNN on CIFAR 10

We conduct a similar experiment on the CIFAR10 dataset
(Krizhevsky & Hinton, 2009). The dataset consists of
60K color images each representing airplanes, automo-
biles, birds, cats, deers, dogs, frogs, horses, ships, and
trucks. 50K of these images constitute the training set, and
the 10K remaining are for testing. SQ’s architecture on
this dataset is a simple one: three convolutional layers, in-
terleaved by max pooling layers. The output of the ﬁnal
pooling layer is fed to a 10-way softmax output layer. The
reported accuracy using 16-bit ﬁxed-point arithmetic is a
25.4% test error. BN’s architecture is a much wider and
deeper architecture based on VGG (Simonyan & Zisser-
man, 2014). The reported accuracy of the binary network
is an impressive 10.15% which is of benchmarking quality
even for full precision networks.

We adopt a similar architecture as SQ, but leverage re-

  (bits)      Test errorABAnalytical Guarantees on Numerical Precision of Deep Neural Networks

Precision Assignment

Test error (%)

Floating-point
(12, 12)
(10, 10)
SQ (16, 16) (Gupta et al., 2015)
BN (1, 1) (Hubara et al., 2016b)

17.02
17.08
17.23
25.4
10.15

Computational
Cost (106 F As)
N/A
3626
2749
4203
3608

Representational
Cost (106 bits)
N/A
5.09
4.24
4.54
6.48

Table 2: Results for CIFAR10: Comparison of accuracy, computational cost, and representational cost with state-of-the-art
related works. Chosen precision assignments are obtained from Figure 2.

cent advances in convolutional neural networks (CNNs)
research. It has been shown that adding networks within
convolutional layers (in the ‘Network in Network’ sense)
as described in (Lin et al., 2013) signiﬁcantly enhances
accuracy, while not incurring much complexity overhead.
Hence, we replace SQ’s architecture by a deep one which
we describe as 64C5 − 64C1 − 64C1 − M P 2 − 64C5 −
64C1−64C1−M P 2−64C5−64F C −64F C −64F C −
10, where C5 denotes 5 × 5 kernels, C1 denotes 1 × 1
kernels (they emulate the networks in networks), M P 2
denotes 2 × 2 max pooling, and F C denotes fully con-
nected components. As is customary for this dataset, we
apply zero-phase component analysis (ZCA) whitening to
the data before training. Because this dataset is a challeng-
ing one, we ﬁrst ﬁne-tune the hyperparameters (learning
rate, weight decay rate, and momentum), then train for 300
epochs. The best accuracy we reach in ﬂoating point using
this 12-layer deep network is 17.02%.

Figure 2 shows the results of our ﬁxed-point simulation and
analysis. Note that, while both bounds from Theorems 1
and 2 still successfully upper bound the test error, these are
not as tight as in our MNIST experiment. Furthermore, in
this case, (12) dictates keeping BW = BA as EA = 21033
≈ 0.29. The fact that
and EW = 31641 so that log2
EW ≥ EA is expected as there are typically more weights
than activations in a neural network. However, note that in
this case the contrast between EW and EA is not as sharp
as in our MNIST experiment. This is mainly due to the
higher weight to activation ratio in fully connected DNNs
than in CNNs.

(cid:113) EW
EA

We again select two design options:

A. Smallest (BA, BW ) such that BW = BA and
In this case

pm ≤ 1% as bounded by Theorem 1.
(BA, BW ) = (12, 12).

B. Smallest (BA, BW ) such that BW = BA and
In this case

pm ≤ 1% as bounded by Theorem 2.
(BA, BW ) = (10, 10).

Table 2 indicates that BN is the most accurate with 10.15%
test error. Interestingly, it has lesser computational cost but
more representational cost than SQ. This is due to the de-
pendence of the computational cost on the product of BA

and BW . The least complex network is ours when setting
(BA, BW ) = (10, 10) and its test error is 17.23% which
is already a large improvement on SQ in spite of having
smaller computational and representational costs. This net-
work is also less complex than that of BN.

The main take away here is that CNNs are quite different
from fully connected DNNs when it comes to precision re-
quirements. Furthermore, from Table 2 we observe that
BN achieves the least test error. It seems that this better
accuracy is due to its greater representational power rather
than its computational power (BN’s representational cost
is much higher than the others as opposed to its computa-
tional cost).

5. Conclusion

In this paper we analyzed the quantization tolerance of neu-
ral networks. We used our analysis to efﬁciently reduce
weight and activation precisions while maintaining similar
ﬁdelity as the ﬂoating-point initiation. Speciﬁcally, we ob-
tained bounds on the mismatch probability between a ﬁxed-
point network and its ﬂoating-point counterpart in terms
of precision. We showed that a neural network’s accuracy
degradation due to quantization decreases double exponen-
tially as a function of precision. Our analysis provides a
straightforward method to obtain an upper bound on the
network’s error probability as a function of precision. We
used these results on real datasets to minimize the compu-
tational and representational costs of a ﬁxed-point network
while maintaining accuracy.

Our work addresses the general problem of resource con-
strained machine learning. One take away is that it is im-
perative to understand the trade-offs between accuracy and
complexity. In our work, we used precision as a parame-
ter to analytically characterize this trade-off. Nevertheless,
additional aspects of complexity in neural networks such as
their structure and their sparsity can also be accounted for.
In fact, more work can be done in that regard. Our work
may be viewed as a ﬁrst step in developing a uniﬁed and
principled framework to understand complexity vs. accu-
racy trade-offs in deep neural networks and other machine
learning algorithms.

Analytical Guarantees on Numerical Precision of Deep Neural Networks

Blahut, Richard E. Fast algorithms for signal processing.

Cambridge University Press, 2010.

4,839,849.

Acknowledgment

This work was supported in part by Systems on Nanoscale
Information fabriCs (SONIC), one of the six SRC STARnet
Centers, sponsored by MARCO and DARPA.

References

Baugh, Charles R and Wooley, Bruce A. A two’s com-
IEEE
plement parallel array multiplication algorithm.
Transactions on Computers, 100(12):1045–1047, 1973.

Caraiscos, Christos and Liu, Bede. A roundoff error analy-
sis of the lms adaptive algorithm. IEEE Transactions on
Acoustics, Speech, and Signal Processing, 32(1):34–41,
1984.

Chen, Y. et al. Eyeriss: An energy-efﬁcient reconﬁgurable
accelerator for deep convolutional neural networks. In
2016 IEEE International Solid-State Circuits Confer-
ence (ISSCC), pp. 262–263. IEEE, 2016.

Courbariaux, Matthieu et al. Binaryconnect: Training deep
neural networks with binary weights during propaga-
In Advances in Neural Information Processing
tions.
Systems, pp. 3123–3131, 2015.

Goel, M. and Shanbhag, N. Finite-precision analysis of the
pipelined strength-reduced adaptive ﬁlter. Signal Pro-
cessing, IEEE Transactions on, 46(6):1763–1769, 1998.

Goodfellow, Ian J et al. Maxout networks. ICML (3), 28:

1319–1327, 2013.

Gupta, S. et al. Deep Learning with Limited Numerical
In Proceedings of The 32nd International
Precision.
Conference on Machine Learning, pp. 1737–1746, 2015.

Hubara, Itay et al. Binarized neural networks.

In Ad-
vances in Neural Information Processing Systems, pp.
4107–4115, 2016b.

Hwang, Kyuyeon and Sung, Wonyong. Fixed-point feed-
forward deep neural network design using weights+ 1, 0,
and- 1. In Signal Processing Systems (SiPS), 2014 IEEE
Workshop on, pp. 1–6. IEEE, 2014.

Kim, M. and Smaragdis, P. Bitwise Neural Networks.

arXiv preprint arXiv:1601.06071, 2016.

Knauer, Karl. Ripple-carry adder, June 13 1989. US Patent

Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple

layers of features from tiny images. 2009.

Krizhevsky, Alex et al. Imagenet classiﬁcation with deep
In Advances in neural

convolutional neural networks.
information processing systems, pp. 1097–1105, 2012.

LeCun, Yann, Cortes, Corinna, and Burges, Christo-
pher JC. The MNIST database of handwritten digits,
1998.

Lin, Darryl et al. Fixed point quantization of deep convo-
lutional networks. In Proceedings of The 33rd Interna-
tional Conference on Machine Learning, pp. 2849–2858,
2016a.

Lin, Min et al. Network in network.

arXiv preprint

arXiv:1312.4400, 2013.

Lin, Yingyan et al. Variation-tolerant architectures for con-
volutional neural networks in the near threshold voltage
regime. In Signal Processing Systems (SiPS), 2016 IEEE
International Workshop on, pp. 17–22. IEEE, 2016b.

Rastegari, Mohammad et al. Xnor-net: Imagenet classi-
ﬁcation using binary convolutional neural networks. In
European Conference on Computer Vision, pp. 525–542.
Springer, 2016.

Han, Song et al. Learning both weights and connections
for efﬁcient neural network. In Advances in Neural In-
formation Processing Systems (NIPS), pp. 1135–1143,
2015.

Sakr, Charbel et al. Minimum precision requirements for
the SVM-SGD learning algorithm. In Acoustics, Speech
and Signal Processing (ICASSP), 2017 IEEE Interna-
tional Conference on. IEEE, 2017.

He, Kaiming et al. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 770–778,
2016.

Hubara, Itay, Courbariaux, Matthieu, Soudry, Daniel, El-
Yaniv, Ran, and Bengio, Yoshua. Quantized neu-
ral networks: Training neural networks with low
arXiv preprint
precision weights and activations.
arXiv:1609.07061, 2016a.

Shanbhag, Naresh R. Energy-efﬁcient machine learning
in silicon: A communications-inspired approach. arXiv
preprint arXiv:1611.03109, 2016.

Simonyan, Karen and Zisserman, Andrew. Very deep con-
volutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.

Srivastava, Nitish et al. Dropout: a simple way to prevent
neural networks from overﬁtting. Journal of Machine
Learning Research, 15(1):1929–1958, 2014.

Analytical Guarantees on Numerical Precision of Deep Neural Networks

Taigman, Yaniv et al. Deepface: Closing the gap to human-
level performance in face veriﬁcation. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1701–1708, 2014.

Zhang, Xiangyu et al. Accelerating very deep convolu-
IEEE
tional networks for classiﬁcation and detection.
Transactions on Pattern Analysis and Machine Intelli-
gence, 2015.

