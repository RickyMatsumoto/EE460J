A Uniﬁed View of Multi-Label Performance Measures

Xi-Zhu Wu 1 Zhi-Hua Zhou 1

Abstract
Multi-label classiﬁcation deals with the problem
where each instance is associated with multiple
class labels. Because evaluation in multi-label
classiﬁcation is more complicated than single-
label setting, a number of performance measures
have been proposed. It is noticed that an algo-
rithm usually performs differently on different
measures. Therefore, it is important to under-
stand which algorithms perform well on which
measure(s) and why.
In this paper, we pro-
pose a uniﬁed margin view to revisit eleven per-
formance measures in multi-label classiﬁcation.
In particular, we deﬁne label-wise margin and
instance-wise margin, and prove that through
maximizing these margins, different correspond-
ing performance measures are to be optimized.
Based on the deﬁned margins, a max-margin ap-
proach called LIMO is designed and empirical
results validate our theoretical ﬁndings.

1. Introduction

Multi-label classiﬁcation aims to build classiﬁcation mod-
els for objects assigned with multiple labels simultane-
ously, which is a common learning paradigm in real-world
applications.
In text categorization, a document may be
associated with a range of topics, such as science, enter-
tainment, and news (Schapire & Singer, 2000); in image
classiﬁcation, an image can have both ﬁeld and mountain
tags (Boutell et al., 2004); in music information retrieval, a
piece of music can convey various messages such as clas-
sic, piano and passionate (Turnbull et al., 2008).

In traditional supervised classiﬁcation, generalization per-
formance of the learning system is usually evaluated by ac-
curacy, or F-measure if misclassiﬁcation costs are unequal.
In contrast to single-label classiﬁcation, performance eval-
uation in multi-label classiﬁcation is more complicated, as

1National Key Laboratory for Novel Software Technology,
Nanjing University, Nanjing 210023, China. Correspondence to:
Zhi-Hua Zhou <zhouzh@lamda.nju.edu.cn>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

each instance can be associated with multiple labels simul-
taneously. For example, it is difﬁcult to tell which mistake
of the following two cases is more serious: one instance
with three incorrect labels vs.
three instances each with
one incorrect label. Therefore, a number of performance
measures focusing on different aspects have been proposed,
such as Hamming loss, ranking loss, one-error, average
precision, coverage (Schapire & Singer, 2000), micro-F1
and macro-F1 (Tsoumakas et al., 2011).

Multi-label learning algorithms usually perform differently
on different measures; however, there are only a few stud-
ies about multi-label performance measures. Dembczyn-
ski et al. (2010) showed that Hamming loss and subset
0/1 loss could not be optimized at the same time. Gao
& Zhou (2013) proposed to study the Bayes consistency
of surrogate losses for multi-label learning; they proved
that none of convex surrogate loss is consistent with rank-
ing loss, and gave a consistent surrogate loss function for
Hamming loss in deterministic case. There are a number of
studies about F-measure, mostly focusing on single-label
tasks, including multi-label learning as application. For
example, Ye et al. (2012) gave justiﬁcations and connec-
tions about F-measure optimization using decision theo-
retic approaches (DTA) and empirical utility maximization
approaches (EUM). Later, Waegeman et al. (2014) studied
the F-measure optimality of inference algorithms from the
DTA perspective. Koyejo et al. (2015) devoted to study
of EUM optimal multi-label classiﬁers. These theoretical
studies offer much insight, though lacking a uniﬁed under-
standing of relation among a variety of multi-label perfor-
mance measures. Moreover, some performance measures
which have been popularly used in evaluation (Zhang &
Wu, 2015) have not been theoretically studied.

In this paper, we try to disclose some shared proper-
ties among different measures and establish a uniﬁed un-
derstanding for multi-label performance evaluation. We
propose a margin view to revisit eleven commonly used
including Hamming
multi-label performance measures,
loss, ranking loss, one-error, coverage, average precision,
macro-, micro- and instance-averaging F-measures and
AUCs. Speciﬁcally, we propose the concepts of label-
wise margin and instance-wise margin, based on which the
corresponding effectiveness of multi-label classiﬁers is de-
ﬁned and then used as bridge to connect different perfor-

A Uniﬁed View of Multi-Label Performance Measures

mance measures. Our theoretical results show that by max-
imizing instance-wise margin, macro-AUC, macro-F1 and
Hamming loss are to be optimized, whereas by maximiz-
ing label-wise margin, the other eight performance mea-
sures except micro-AUC are to be optimized.
Inspired
by the theoretical ﬁndings, we design the LIMO (Label-
wise and Instance-wise Margins Optimization) approach to
maximize both the two margins. Experiments validate our
theoretical ﬁndings and demonstrate a ﬂexible way to opti-
mize different measures through one approach by different
parameter settings.

The rest of the paper is organized as follows. Section 2 in-
troduces the notation and deﬁnitions of eleven multi-label
performance measures. Section 3 proposes the label-wise
and instance-wise margins, and presents our theoretical re-
sults. Section 4 presents the LIMO approach. Section 5
reports the results of experiments. Finally, Section 6 con-
cludes and indicates several future issues.

2. Preliminaries

2.1. Notation

Assume that xi ∈ Rd×1 is a real value instance vector,
yi ∈ {0, 1}l×1 is a label vector for xi. m denotes the num-
ber of training samples. Therefore yij (i ∈ {1, . . . , m}, j ∈
{1, . . . , l}) means the jth label of the ith instance, and
yij = 1 or 0 means the jth label is relevant or irrelevant.
The instance matrix is X ∈ Rm×d and the label matrix
is Y ∈ {0, 1}m×l. H : Rd → {0, 1}l is the multi-label
classiﬁer, which consists of l models, one for a label, so
H = {h1, . . . , hl} and hj(xi) denotes the prediction of
yij. Moreover, F : Rd → Rl is the multi-label predictor
and the predicted value can be regarded as the conﬁdence of
relevance. Similarly, F can be decomposed as {f1, . . . , fl}
where fj(xi) denotes the predicted value of yij.

H can be induced from F via thresholding functions. For
example, hj(xi) = [[fj(xi) > t(xi)]] uses a thresholding
function based on the instance xi and outputs 1 if predicted
value is higher than the threshold. [[π]] returns 1 if predicate
π holds, and 0 otherwise.

For simpliﬁcation, we use Y i· to denote the ith row vector
and Y ·j to denote the jth column vector of the label matrix.
Furthermore, Y +
i· (or Y −
i· ) denotes the index set of relevant
(or irrelevant) labels of Y i·. Formally, Y +
i· = {j |yij = 1}
and Y −
i· = {j |yij = 0}. In terms of jth column of label
matrix, Y +
·j = {i |yij = 1} denotes the index set of positive
instances of the jth label and Y −
·j = {i |yij = 0} denotes
the set of negative instances similarly. We use | · | to denote
the cardinality of a set, thus, the number of relevant labels
of xi is |Y +
i· |.

2.2. Multi-label Performance Measures

Table 1 summarizes the eleven multi-label performance
measures commonly used in previous studies. The ﬁrst ﬁve
measures (Hamming loss, ranking loss, one-error, cover-
age, average precision) are considered in Schapire & Singer
(2000) and a multitude of works, e.g., Huang et al. (2012)
and Zhang & Wu (2015). The next six measures are ex-
tensions of F-measure and AUC (the Area Under the ROC
Curve) in multi-label classiﬁcation via different averaging
strategies. These F-measures are popluar both in algorithm
evaluation (Liu & Tsang, 2015) and theoretical analysis
(Koyejo et al., 2015). AUCs are used for algorithm eval-
uation such as in Lampert (2011), Pham et al. (2015) and
Zhang & Wu (2015).

Some of these measures are deﬁned on classiﬁer H,
and they care about the binary classiﬁcation performance.
While some of these measures are deﬁned on predictor F ,
and they usually measure the ranking performance of the
predictor. We have noticed that some performance mea-
sures on ranking are ill-deﬁned when F is a constant func-
tion. For example, if F outputs 1 for all labels, then one-
error(F ) = 0, coverage(F ) = 0 and various AUCs will be
1, which are the optimal values respectively. In multi-label
learning community, there is often an underlying assump-
tion that a total ranking can be induced from continuous
real-value predictions, which is common in practical cases.
In this paper, we still stick to the convention in previous
works and assume that no tie happens in continuous pre-
diction to solve this deﬁnition ﬂaw.

3. Theoretical Results

Here we deﬁne two new concepts: label-wise margin and
instance-wise margin.
Deﬁnition 1. Given a multi-label predictor F : Rd → Rl
and F = {f1, . . . , fl}, a training set (X, Y ), the label-
wise margin on instance xi is deﬁned as:

γlabel
i = min
u,v

{fu(xi) − fv(xi) | (u, v) ∈ Y +

i· × Y −

i· }.

i·

is the set of all the (relevant, irrelevant) label

Y +
i· × Y −
index pairs of instance i.
Deﬁnition 2. Given a multi-label predictor F : Rd → Rl
and F = {f1, . . . , fl}, a training set (X, Y ), the instance-
wise margin on label Y ·j is deﬁned as:

γinst
j = min
a,b

{fj(xa) − fj(xb) | (a, b) ∈ Y +

·j × Y −

·j }.

Y +
·j × Y −
index pairs of label j.

·j is the set of all the (positive, negative) instance

Label-wise margin and instance-wise margin describe the
discriminative ability of F . The larger the label-wise mar-

A Uniﬁed View of Multi-Label Performance Measures

Table 1. Deﬁnitions of eleven multi-label performance measures

Note

hloss(H) =

[[hij (cid:54)= yij ]]

The fraction of misclassiﬁed labels

The average fraction of reversely or-
dered label pairs of each instance.

The fraction of instances whose most
conﬁdent label is irrelevant.

The number of more labels on aver-
age should include to cover all rele-
vant labels

The average fraction of relevant la-
bels ranked higher than one other rel-
evant label.

Measure

Hamming loss

ranking loss

one-error

coverage

average precision

Formulation

1
ml

m
(cid:88)

l
(cid:88)

i=1

j=1

1
m

m
(cid:88)

i=1

1
m

m
(cid:88)

i=1

[[ max
+
j∈Y
i·

rloss(F ) =

1
m

m
(cid:88)

i=1

|S i
rank|
i· ||Y −
|Y +
i· |

rank = {(u, v)|fu(xi) ≤ fv(xi), (u, v) ∈ Y +
S i

i· × Y −
i· }

one-error(F ) =

[[arg max F (xi) /∈ Y +
i· ]]

coverage(F ) =

rankF (xi, j) − 1]]

avgprec(F ) =

precision = {k ∈ Y +
S ij

m
(cid:88)

1
m

1
|Y +
i· |

(cid:88)

|S ij

precision|
rankF (xi, j)

i=1

+
i·
i· |rankF (xi, k) ≤ rankF (xi, j)}

j∈Y

1
l

l
(cid:88)

j=1

2 (cid:80)m
i=1 yij hij
i=1 yij + (cid:80)m

i=1 hij

(cid:80)m

1
m

m
(cid:88)

i=1

j=1 yij hij

2 (cid:80)l
j=1 yij + (cid:80)l

j=1 hij

(cid:80)l

macro-F1

macro-F1(H) =

F-measure averaging on each label.

instance-F1

instance-F1(H) =

F-measure averaging on each instance.

micro-F1

micro-F1(H) =

(cid:80)m

i=1 yij hij

j=1

2 (cid:80)l
(cid:80)m

(cid:80)l

j=1

i=1 yij + (cid:80)l

j=1

(cid:80)m

i=1 hij

F-measure averaging on the predic-
tion matrix.

macro-AUC

instance-AUC

micro-AUC

macro-AUC(F ) =

macro = {(a, b) ∈ Y +
S j

l
(cid:88)

1
l
·j × Y −

|S j
macro|
·j ||Y −
|Y +
·j |
j=1
·j |fj (xa) ≥ fj (xb)}

instance-AUC(F ) =

instance = {(u, v) ∈ Y +
S i

m
(cid:88)

1
m
i· × Y −

|S i
instance|
i· ||Y −
|Y +
i· |
i=1
i· |fu(xi) ≥ fv(xi)}

AUC averaging on each label. Smacro
is the set of correctly ordered instance
pairs on each label.

AUC averaging on each instance.
Sinstance is the set of correctly ordered
label pairs on each instance.

micro-AUC(F ) =

Smicro = {(a, b, i, j)|(a, b) ∈ Y +

·i × Y −

((cid:80)m

i=1 |Y +

|Smicro|
i· |) · ((cid:80)m

i=1 |Y −
·j , fi(xa) ≥ fj (xb)}

i· |)

AUC averaging on prediction matrix.
Smicro is the set of correct quadruples.

gin, the easier to distinguish relevant and irrelevant labels
of an instance. Meanwhile, the larger the instance-wise
margin, the easier for F to distinguish positive and neg-
ative instances of a particular label. Therefore, we want
to maximize label-wise/instance-wise margin to get better
performance.

Although we prefer maximizing these two margins, with
respect to performance measures, the objective can be re-
laxed. We deﬁne three properties a predictor F can have:
label-wise effective, instance-wise effective and double ef-
fective.

Deﬁnition 3. If all the label-wise margins of F on a dataset
D = (X, Y ) are positive, this predictor F is label-wise
effective on D.

Deﬁnition 4. If all the instance-wise margins of F on a
dataset D = (X, Y ) are positive, this predictor F is
instance-wise effective on D.

Deﬁnition 5. If all the label-wise margins and instance-
wise margins of F on a dataset D = (X, Y ) are positive,
this predictor F is double effective on D.

Roughly speaking, label-wise effective means F can ex-
actly distinguish relevant and irrelevant labels of each in-
stance and instance-wise effective means F can exactly dis-
tinguish positive and negative instances of every label. Not
surprisingly, double effective F has the strongest ability in
distinguishing.

In the next two subsections, we use the effectiveness to an-

A Uniﬁed View of Multi-Label Performance Measures

alyze different performance measures, and summarize the
analysis results in Section 3.3.

Therefore, the size of the correct ordered prediction value
pair on instance i is:

3.1. Performance Measures on Ranking

Several multi-label performance measures can be empiri-
cally optimized according to the following theorems:

Theorem 1. If a multi-label predictor F is label-wise effec-
tive on D, then ranking loss, one-error, coverage, average
precision and instance-AUC are optimized on the dataset.

Proof.
(a) Ranking loss: From the deﬁnition of label-
wise effective, for every pair (u, v) ∈ Y +
i· × Y −
i· , we have
fu(xi) > fv(xi). Therefore, the reversed set S i
rank (in Ta-
ble 1 ranking loss) is empty and the cardinality of the set is
zero, which implies the cardinality sum of all reversed sets
rloss(F ) = 0. Ranking loss is optimized.

(b) One-error: For a label-wise effective F , because label-
wise margin is positive on an instance xi, we have:

max
u

fu(xi) > max

fv(xi), ∀u ∈ Y +

i· , ∀v ∈ Y −
i· .

v

Then

∀xi, arg max F (xi) ∈ Y +
i· .

Thus, [[arg max F (xi) /∈ Y +
and one-error(F ) = 0. One-error is optimized.

i· ]] = 0 for every instance xi,

|{(u, v) ∈ Y +

i· × Y −

i· |fu(xi) ≥ fv(xi)}| = |Y +

i· ||Y −
i· |.

So instance-AUC(F ) = 1 and instance-AUC is optimized.

Similar to the proof of instance-AUC, we can prove the re-
sult of macro-AUC:

Theorem 2. If a multi-label predictor F is instance-wise
effective on D, then macro-AUC is optimized.

Proof. Because of instance-wise effective, for a label vec-
tor Y ·j, we have:

fj(xa) > fj(xb), ∀(a, b) ∈ Y +

·j × Y −
·j .

Therefore, the size of the correct ordered prediction value
pair on label j is:

{(a, b) ∈ Y +

·j × Y −

·j |fj(xa) ≥ fj(xb)} = |Y +

·j ||Y −
·j |.

So macro-AUC(F ) = 1 and macro-AUC is optimized.

Micro-AUC sees the label matrix as a whole and cannot be
optimized by instance-wise effective F or label-wise effec-
tive F . However, the double effective F is much more pow-
erful. We now prove the following result of micro-AUC.

(c) Coverage: When F is label-wise effective, the maxi-
mum rank of a relevant label is less than the minimum rank
of an irrelevant label, which means:

Theorem 3. If a multi-label predictor F is double effective
on D, then as the number of instances grows, micro-AUC
is optimized.

max
u∈Y +
i·
max
u∈Y +
i·

rankF (xi, u) < min
v∈Y −
i·
rankF (xi, u) = |Y +
i· |.

rankF (xi, v),

(1)

Proof. We ﬁrst prove a result of
random variables
Ai, B, C. If n random variables A1, A2, · · · , An are drawn
from uniform distribution U (0, 1), for a random constant a,
the event that at least one Ai is smaller than a is:

Therefore, coverage can be calculated as:

Pr[∃Ai, Ai ≤ a] = 1 − (1 − a)n.

coverage(F ) =

1
m

(cid:88)m

i=1

[|Y +

i· | − 1].

Which is the optimal value of coverage.

(d) Average precision: Assume that j is a relevant label of
instance i, it follows from Equation (1) that:

rankF (xi, j) = |{k ∈ Y +

i· |rankF (xi, k) ≤ rankF (xi, j)}|

Since rankF (xi, j) is exactly the deﬁnition of S ij
avgprec(F ) = 1, i.e, average precision is optimized.

precision,

(e) Instance-AUC: Because of label-wise effective, for an
instance xi, we have:

fu(xi) > fv(xi), ∀(u, v) ∈ Y +

i· × Y −
i· .

Another random variable B is uniformly distributed in
(0, min{Ai}), and the probability that a random variable
C ∼ U (0, 1) is bigger than B is:

Pr[C > B] ≥ Pr[(C ≥ a) ∧ (∃Ai, Ai ≤ a)]

=(1 −

)[1 − (1 − a)n].

(2)

a
2

For any small a, we can choose a large enough n to make
Equation (2) close to 1.

Given a label matrix Y ∈ {0, 1}m×l and the correspond-
ing prediction matrix F ∈ (0, 1)m×l, because predictor F
is double effective, the prediction matrix satisﬁes the fol-
lowing conditions:

Fij > Fiu if Yij = 1 ∧ Yiu = 0,
Fij > Fvj if Yij = 1 ∧ Yvj = 0.

A Uniﬁed View of Multi-Label Performance Measures

To force the value in F is in (0, 1), we further assume a
uniform distribution Fij ∼ U (0, 1) when Yij = 1.

for instance-wise effective F . It is reasonable to use either
t(xi) or tj for double effective F .

If Yij = 0, then Fij should be less than Fiu if Yiu = 1
and Fvj if Yvj = 1. Suppose that the minimum value b is
deﬁned as:

b = min

{Fvj|Yvj = 1}, min

{Fiu|Yiu = 1}

u

(cid:110)

min
v

(cid:111)
.

Then Fij is drawn from U (0, b). And we can choose a
small constant value a > b.

According to Equation (2), the probability that a random
pair (i, j, u, v) to be a correct micro pair is:

Pmicro = Pr[Fij > Fuv|Yij = 1, Yuv = 0]

≥ (1 −

)[1 − (1 − a)n],

a
2

k
ml

where n =

(m + l − 2)

In the practical case, the number of labels is proportional
to the number of instances: k ∝ m. We assume k = pm
where p is a constant smaller than l.

lim
m→∞

n = lim
m→∞

(m + l − 2) = ∞,

p
l
|Smicro|
i· |) · ((cid:80)m

lim
m→∞

|((cid:80)m

i=1 |Y +

i=1 |Y −

i· |)|

= lim
m→∞

Pmicro = 1.

Therefore, micro-AUC is to be optimized as the number of
instances grows.

With the above analysis, we can conclude that a label-wise
effective F can optimize ranking loss, one-error, cover-
age, average precision, instance-AUC, micro-AUC and an
instance-wise effective F can optimize macro-AUC. For
micro-AUC, a double effective F can optimize it as the
number of instances increases.

3.2. Performance Measures on Classiﬁcation

As mentioned in Section 2.2, there are some measures eval-
uating classiﬁer H instead of predictor F . There are many
thresholding or binarization strategies (Fan & Lin, 2007;
F¨urnkranz et al., 2008; Read et al., 2011). For simplicity,
we focus on two main strategies: thresholding on each in-
stance and thresholding on each label.

A label-wise effective F can be equipped with a thresh-
olding function based on each instance such as t(xi) and
construct the H by hj(xi) = [[fj(xi) > t(xi)]]. However,
using t(xi) on an instance-wise effective F is unreason-
able since the predicted values on different labels may not
be comparable. In a word, we should use suitable thresh-
old function on different effective F s, i.e., t(xi) on each
instance for label-wise effective F , and tj on each label

To formally analyze the performance measures on classiﬁ-
cation, we deﬁne the threshold error:

Deﬁnition 6. Given a descending ordered real-value se-
quence x1, x2, . . . , xk with an optimal cut number c∗,
where c∗ ∈ N and 1 ≤ c∗ ≤ k. For a real value
threshold t ∈ (xk − 1, x1 + 1), the threshold error (cid:15) =
| arg mini(xi) − c∗| where xi > t.

Intuitively, the threshold error (cid:15) counts how many items
are incorrectly classiﬁed on a descending ordered sequence
where the correct answer is c∗. Based on the threshold er-
ror, we propose the following theorems about performance
measures on classiﬁcation.

Theorem 4. For a label-wise effective F , if the threshold-
ing function makes at most (cid:15)i error on each instance i, the
micro-F1, instance-F1 and Hamming loss are bounded as
follows:

micro-F1(H) = instance-F1(H)

≥

m
(cid:88)

min

i=1
(cid:88)m

1
m

1
ml

(cid:15)i.

i=1

hloss(H) ≤

(cid:110) 2(|Y +
2|Y +

i· | − (cid:15)i)
i· | − (cid:15)i

,

2|Y +
i· |
i· | + (cid:15)i

2|Y +

(cid:111)
,

The main idea of the above theorem is that, given the
threshold error and the number of relevant labels, we can
compute the gap between the worst possible and the perfect
contingency table. Hence the F-measure is based on the
contingency table, the lower bound can be deduced. The
detailed proof of Theorem 4 is in Appendix A.1.

Similar to Theorem 4, we can prove the results for label-
wise effective F :

Theorem 5. For an instance-wise effective F , if the thresh-
olding function makes at most (cid:15)j error on each label j, then
the macro-F1 and Hamming loss are bounded as follows:

macro-F1(H) ≥

min

(cid:110) 2(|Y +
2|Y +

·j | − (cid:15)j)
·j | − (cid:15)j

,

2|Y +
·j |
·j | + (cid:15)j

2|Y +

(cid:111)
,

l
(cid:88)

1
l

j=1
1
ml

(cid:88)l

hloss(H) ≤

(cid:15)j.

j=1

The detailed proof of Theorem 5 is in Appendix A.2.

With the above analysis, we can conclude that a label-
wise effective F can optimize instance-F1 and micro-F1,
an instance-wise effective F can optimize macro-F1. Both
the two effective F s can optimize Hamming loss. For a
double effective F , because it enjoys both the properties,
it can optimize all the above mentioned performance mea-
sures if proper thresholds are used.

A Uniﬁed View of Multi-Label Performance Measures

Table 2. Summary of performance measures optimized by x-
effective multi-label predictor (F ). ‘(cid:88)’ means F in this cell is
proved to optimize this measure; ‘(cid:55)’ means F in this cell does
not necessarily optimize the measure; ‘•’/‘◦’ means the calcula-
tion is with/without thresholding.

x-effective F
label-wise inst-wise double

Threshold

Measure

ranking loss
avg. precision
one-error
coverage
instance-AUC
macro-AUC
micro-AUC
macro-F1
instance-F1
micro-F1
Hamming loss

3.3. Summary

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:55)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

◦
◦
◦
◦
◦
◦
◦
•
•
•
•

Table 2 summarizes our theoretical results in Section 3.1
and 3.2. Each row shows the results of one multi-label per-
formance measure. Note that double effective is a special
case of label-wise effective and instance-wise effective and
thus, if one performance measure is optimized by either
label-wise or instance-wise effective predictor, it will also
be optimized by double effective predictor.

In the light of the analysis, the performance on different
performance measures through optimizing margins can be
expected. For example, if one maximizes instance-wise
margin on each label, s/he will get good performance on
macro-AUC but may suffer higher loss on ranking loss,
coverage and some other measures where ‘(cid:55)’ marked in the
inst-wise column. If one tries to maximize the label-wise
margin but pay no attention to instance-wise margin, s/he
may perform well on average precision but poor on macro-
F1 (e.g., Elisseeff & Weston (2002)). Maximzing both the
label-wise margin and instance-wise margins to get a dou-
ble effective F is expected to be the best choice.

4. The LIMO Approach

The above analysis reveals that maximizing different mar-
gins will optimize different measures, and if possible, dou-
ble effective F is prefered since it enjoys the beneﬁts of
maximizing both the label-wise margin and the instance-
wise margin. Therefore, we propose the LIMO approach.
LIMO is a single approach which can optimize both the two
margins, and it can also be degenerated to optimize either
margin seperately via parameter setting.

4.1. Formulation

Suppose that F is a linear predictor, which means F (X) =
W T X where W = [w1, w2, · · · , wl]. We propose the
following formulation:

arg min
W ,ξ

l
(cid:88)

||wi||2 + λ1

m
(cid:88)

(cid:88)

ξuv
i + λ2

i=1
i=1
(u,v)
v xi > 1 − ξuv
u xi − w(cid:62)
s.t. w(cid:62)
i ≥ 0,
i
for i = 1, · · · , m and (u, v) ∈ Y +
j xb > 1 − ξj
j xa − w(cid:62)
ab ≥ 0,
for j = 1, · · · , l and (a, b) ∈ Y +

ab, ξj

, ξuv

w(cid:62)

l
(cid:88)

(cid:88)

j=1

(a,b)

ξj
ab

i· × Y −

i·

,

·j × Y −
·j .

(3)

i

and ξj

Here ξuv
ab are the slack variables, and λ1, λ2 are the
trade-off parameters. When both λ1 and λ2 are positive,
both label-wise and instance-wise margins are considered.
If we set λ1 = 0 (or λ2 = 0), then only the instance-wise
(or label-wise) margin is considered. In this paper, if the
approach only considers instance-wise (or label-wise) mar-
gin , we call the approach as LIMO-inst (or LIMO-label).
And LIMO considers both the two margins.

Algorithm 1 LIMO

Input:

Data matrix X ∈ Rm×d, label matrix Y ∈ {0, 1}m×l,
step size η, trade-off parameters λ1, λ2, and the max-
imium iteration number T .

√

Procedure:
1: Initialize W 0 with N (0, 1/
2: Compute the weight vector cinst of each instance,
i=1 |Y +
3: Compute the weight vector clabel of each label,
j=1 |Y +

d) random values.

i = |Y +
cinst

i· |/ (cid:80)m

·j ||Y −
·j |.

i· ||Y −
i· |.

i· ||Y −

clabel
j

·j ||Y −

·j |/ (cid:80)l
= |Y +
4: for t = 1, 2, · · · , T do
5:
6:

Random sample an instance xt
i using weight cinst,
Random sample a positive label yiu and a negative
label yiv of instance xt
i.
i + w(cid:62)
u xt
v xt
if 1 − w(cid:62)
i > 0 then
u − η(−λ1xt
u = wt−1
v − η(λ1xt
v = wt−1

wt
wt
end if
Random sample index j of label using weight clabel.
Random sample a positive instance xt
a and a nega-
tive instance xt
b on label j.
j xt
if 1 − w(cid:62)

i + wt−1
u ).
).
v

i + wt−1

j xt
j = wt−1

a + w(cid:62)
j − η(λ2(xt

b > 0 then
b − xt

a) + wt−1

).

j

7:
8:
9:
10:
11:
12:

13:

wt
14:
end if
15:
16: end for
17: W = 1
T
Output:

(cid:80)T

t=1 W t.

Multi-label linear model W .

A Uniﬁed View of Multi-Label Performance Measures

4.2. Algorithm

The objective Equation (3) is difﬁcult to solve directly be-
cause of the large number of constraints and slack vari-
ables. For a training set with m instances and l labels, the
number of constraints will be O(m2l + ml2), which may
exceed memory limit in real-world applicaitons.

In order to deal with the computational problem, we solve
Equation (3) by stochastic gradient descent (SGD) with
ﬁxed step size and the default averaging technique in
Shalev-Shwartz & Ben-David (2014, Chapter 14.3). The
key point of SGD is to ﬁnd out a random vector, whose
expected value at each iteration equals the gradient direc-
tion. We randomly sample two kinds of triplets and use
them to compute the correct direction. At each iteration t,
we sample a triplet (xt
i, yiu, yiv) where yiu is relevant and
yiv is irrelevant, and a triplet (j, xt
a is a pos-
itive instance and xt
b is a negative instance both on label j.
Then we use the two triplets to compute the random gra-
dient vector for SGD. The detailed algorithm is presented
in Algorithm 1 and the proof that the random vector is an
unbiased estimation of the gradient direction is available in
Appendix A.3.

b) where xt

a, xt

After the training procedure, we can use the linear model to
predict continuous conﬁdence values on the training data,
then choose the best threshold value by optimizing a spe-
ciﬁc classiﬁcation measure.

5. Experiments

We conduct experiments with LIMO on both synthetic and
benchmark data. Note that the main purpose of our work
is to study multi-label performance measures from the as-
pect of margin optimization, and thus, the goal of our ex-
periments is to validate our theoretical ﬁndings rather than
claim that LIMO is superior, although its performance is
really highly competitive.

5.1. Synthetic Data

We conduct experiments on synthetic data with 4 labels.
2000 data points are randomly generated from a (−1, +1)2
square, and the labels are assigned as in Figure 1. 50%
data are held out for testing. The synthetic data is designed
to simulate a typical real-world circumstance. The number
of co-occurrent labels varies, the regions of each label are
different and the data cannot be perfectly seperated by a
linear learner.

To demonstrate the relationship between margins and per-
formance measures, we degenerate LIMO to only consider
either margin by setting the trade-off parameter λ1 or λ2
to zero. LIMO-inst sets λ1 = 0 and LIMO-label sets
λ2 = 0. The other parameter is set to 100 and LIMO sets

space consists of
Figure 1. Input
four regions with different assign-
ments of the label set {A, B, C, D}.
The center point is with coordinate
(0, 0).

λ1 = λ2 = 100. Ten replications of the experiment are
conducted and the average results are reported. Because
the range of performance measure coverage is not [0, 1],
while some performance measures are better when higher,
and some are better when lower, we rescale all the perfor-
mance values into relative values for clearer visualization.
The best one is rescaled to 1 and the worst one is rescaled to
0. Figure 2 shows the relative results, where the originally
worst performance value is given on the right.

Figure 2. Summary of the relative performance on ranking mea-
sures. The more to the left, the better the performance.

The results shown in Figure 2 support our theoretical ﬁnd-
ings in Table 2. For example, micro-AUC is considered to
be optimized by double effective F but not the other two,
therefore LIMO (the red circle) gets the best relative value.
For some measures proved to be optimized by label-wise
margin such as ranking loss, average precision, coverage
and instance-AUC, LIMO-label beats LIMO-inst. While
for macro-AUC, LIMO-inst wins. For one-error, all three
versions of LIMO do extremely well and get less than 0.001
absolute value, which may be the reason why the relative
values are unexpected.

Figure 3. Summary of the relative performance on classiﬁcation
measures. The more to the left, the better the performance.

Figure 3 shows the relative performance on classiﬁcation.
We use two types of thresholding discussed in Section 3.2:

ABCDABDBCA-101-101 instance F1macro F1micro F1Hamming loss1.00.80.60.40.20.0 LIMO-inst-t LIMO-inst-t(x) LIMO-label-t LIMO-label-t(x) LIMO-t LIMO-t(x)0.8040.8520.8370.188relative valueabsolute worst valuemicro-AUCinstance-AUCmacro-AUCcoverageone-erroravg. precisionranking loss1.00.80.60.40.20.0 LIMO-inst LIMO-label LIMO0.8540.9730.8281.5750.0010.9920.027relative value absolute worst value *****instance F1macro F1micro F1Hamming loss1.00.80.60.40.20.0 LIMO-inst-t LIMO-inst-t(x) LIMO-label-t LIMO-label-t(x) LIMO-t LIMO-t(x)0.8040.8520.8370.188relative valueabsolute worst valuemicro-AUCinstance-AUCmacro-AUCcoverageone-erroravg. precisionranking loss1.00.80.60.40.20.0 LIMO-inst LIMO-label LIMO0.8540.9730.8281.5750.0010.9920.027relative value absolute worst value *****A Uniﬁed View of Multi-Label Performance Measures

threshold function based on each instance or each label (de-
noted by -t(x) or -t in the legend). The thresholds are esti-
mated on training data. This ﬁgure exactly shows our the-
oretical results: LIMO-label equipped with t(x) can opti-
mize instance-F1 and micro-F1; LIMO-inst equipped with
t can optimize macro-F1. By considering both label-wise
margin and instance-wise margin, LIMO works well on all
four classiﬁcaiton measures.

5.2. Benchmark Data

We conduct experiments on eleven multi-label perfor-
mance measures to further show that optimizing the label-
wise or the instance-wise margin can lead to different re-
sults, as revealed in our theoretical analysis.

Five benchmark multi-label datasets1 are used in our exper-
iments. We choose them because they denote different do-
mains: (i) A music dataset CAL500, (ii) an email dataset
enron, (iii) a clinical text dataset medical, (iv) an image
dataset corel5k, (v) a tagging dataset bibtex. We randomly
split each dataset into two parts, i.e., 70% for training and
30% for testing. The experiments are repeated ten times,
and the averaged results are reported.

Because our algorithm optimizes a linear model, three lin-
ear methods called Binary Relevance (BR) (Zhang & Zhou,
2014), ML-kNN (Zhang & Zhou, 2007) and GFM (Waege-
man et al., 2014) are provided for fair comparison. As
in experiments on synthetic data, we degenerate LIMO
(λ1 = λ2 = 1) to LIMO-inst (λ1 = 0, λ2 = 1) and LIMO-
label (λ1 = 1, λ2 = 0). The step size of SGD is set to 0.01.
For BR, L2-regularized SVM (Chang & Lin, 2011) with
C=1 is used as base learner. For ML-kNN and GFM, the
number of nearest neighbors is 10. Suitable thresholds dis-
cussed in Section 3.2 are used for classiﬁcation measures.
We take the default parameter settings recommended by au-
thors of the compared methods respectively. Because on
one hand, we believe the parameter settings recommended
by their authors are meaningful, on the other hand, it is hard
to say which parameter setting is better in terms of eleven
performance measures.

Because some measures are better when higher, and some
measures are better when lower, to demonstrate the re-
sults more clearly, we compute the average rank of each
approach over all datasets on a speciﬁc measure. For ex-
ample, when we want to examine how LIMO performs on
ranking loss, we ﬁrst compute the ranks on each dataset:
LIMO ranks 1st on CAL500, enron, bibtex and ranks 2nd
on medical, corel5k. Then the average rank of LIMO on
ranking loss is (1+1+1+2+2)/5=1.4. Figure 4 shows the av-
erage ranks. Due to the space limit, the detailed results
used to compute the ranks are provided in Appendix B.2.

1http://mulan.sourceforge.net/datasets-mlc.html

Figure 4. Average rank on benchmark data. The smaller the rank
value, the better the performance.

The results in Figure 4 are consistent with our theoretical
ﬁndings. LIMO-inst (the square) performs well on marco-
F1 and macro-AUC, while LIMO-label (the triangle) per-
forms well on other performance measures. LIMO (the cir-
cle) almost ranks top on every performance measure.

The experiments on synthetic and benchmark data support
our theoretical analysis. Although different performance
measures focus on different aspects, they share the com-
mon property which is formalized in our work as label-
wise margin and instance-wise margin. In practice, it is rec-
ommended to use higher weight (λ1/λ2) on speciﬁc mar-
gin to optimize the required performance measure. LIMO
with nonlinear predictors may perform better, which needs
a novel optimization algorithm.

6. Conclusion

In this paper, we establish a uniﬁed view for a variety of
multi-label performance measures. Based on the proposed
concepts of label-wise/instance-wise margins, we prove
that some performance measures are to be optimized by
label-wise effective classiﬁers, whereas some by instance-
wise effective classiﬁers. Inspired by the theoretical ﬁnd-
ings, we design the LIMO approach which can be adjusted
to label-wise/instance-wise effective via different parame-
ter settings.

Our work discloses that there are some shared properties
among different subsets of multi-label performance mea-
sures. This explains why some measures seem to be redun-
dant in experiments, and suggests that in future empirical
studies, rather than randomly grasp a set of measures for
evaluation, it is more informative to evaluate using mea-
sures with different properties, such as some measures op-
timized by label-wise effective predictors and some opti-
mized by instance-wise effective predictors. In the future,
it is encouraging to study the asymptotic properties of these
performance measures when the two margins are subopti-
mal. The margin view also sheds a light for the design of
novel multi-label algorithms.

micro-AUCmicro-F1macro-AUCmacro-F1instance-AUCinstance-F1Hamming losscoverageone-erroravg. precisionranking loss123456 average rank BR ML-kNN GFM LIMO-inst LIMO-label LIMO************A Uniﬁed View of Multi-Label Performance Measures

Read, Jesse, Pfahringer, Bernhard, Holmes, Geoff, and
Frank, Eibe. Classiﬁer chains for multi-label classiﬁ-
cation. Machine Learning, 85(3):333–359, 2011.

Schapire, Robert E and Singer, Yoram. Boostexter: A
boosting-based system for text categorization. Machine
Learning, 39(2):135–168, 2000.

Shalev-Shwartz, Shai and Ben-David, Shai. Understanding
Machine Learning: From Theory to Algorithms. Cam-
bridge University Press, 2014.

Tsoumakas, Grigorios, Katakis, Ioannis, and Vlahavas,
Ioannis P. Random k-labelsets for multilabel classiﬁca-
tion. IEEE Trans. Knowledge and Data Engineering, 23
(7):1079–1089, 2011.

Turnbull, Douglas, Barrington, Luke, Torres, David A., and
Lanckriet, Gert R. G. Semantic annotation and retrieval
of music and sound effects. IEEE Trans. Audio, Speech
& Language Processing, 16(2):467–476, 2008.

Waegeman, Willem, Dembczynski, Krzysztof, Jachnik,
Arkadiusz, Cheng, Weiwei, and H¨ullermeier, Eyke. On
the bayes-optimality of F-measure maximizers. Journal
of Machine Learning Research, 15(1):3333–3388, 2014.

Ye, Nan, Chai, Kian Ming Adam, Lee, Wee Sun, and
Chieu, Hai Leong. Optimizing F-measure: A tale of two
approaches. In ICML, 2012.

Zhang, Min-Ling and Wu, Lei. LIFT: Multi-label learning
with label-speciﬁc features. IEEE Trans. Pattern Analy-
sis and Machine Intelligence, 37(1):107–120, 2015.

Zhang, Min-Ling and Zhou, Zhi-Hua. ML-KNN: A
lazy learning approach to multi-label learning. Pattern
Recognition, 40(7):2038–2048, 2007.

Zhang, Min-Ling and Zhou, Zhi-Hua. A review on multi-
label learning algorithms. IEEE Trans. Knowledge and
Data Engineering, 26(8):1819–1837, 2014.

Acknowledgements

This research was supported by the NSFC (61333014), 973
Program (2014CB340501), and the Collaborative Innova-
tion Center of Novel Software Technology and Industrial-
ization. Authors want to thank reviewers for helpful com-
ments, and thank Sheng-Jun Huang, Xiu-Shen Wei, Miao
Xu for reading a draft.

References

Boutell, Matthew R., Luo, Jiebo, Shen, Xipeng, and
Brown, Christopher M. Learning multi-label scene clas-
siﬁcation. Pattern Recognition, 37(9):1757–1771, 2004.

Chang, Chih-Chung and Lin, Chih-Jen. LIBSVM: A li-
brary for support vector machines. ACM Trans. Intelli-
gent Systems and Technology, 2(3):27:1–27:27, 2011.

Dembczynski, Krzysztof, Waegeman, Willem, Cheng,
Weiwei, and H¨ullermeier, Eyke. Regret analysis for per-
formance metrics in multi-label classiﬁcation: the case
of hamming and subset zero-one loss. In ECML/PKDD,
pp. 280–295. 2010.

Elisseeff, Andr´e and Weston, Jason. A kernel method
for multi-labelled classiﬁcation. In NIPS, pp. 681–687,
2002.

Fan, Rong-En and Lin, Chih-Jen. A study on threshold
selection for multi-label classiﬁcation. National Taiwan
University, Technical Report, pp. 1–23, 2007.

F¨urnkranz,

Johannes, H¨ullermeier, Eyke, Menc´ıa,
Eneldo Loza, and Brinker, Klaus. Multilabel classiﬁca-
tion via calibrated label ranking. Machine Learning, 73
(2):133–153, 2008.

Gao, Wei and Zhou, Zhi-Hua. On the consistency of multi-
label learning. Artiﬁcial Intelligence, 199:22–44, 2013.

Huang, Sheng-Jun, Yu, Yang, and Zhou, Zhi-Hua. Multi-
label hypothesis reuse. In ACM SIGKDD, pp. 525–533,
2012.

Koyejo, Oluwasanmi, Natarajan, Nagarajan, Ravikumar,
Pradeep, and Dhillon, Inderjit S. Consistent multilabel
classiﬁcation. In NIPS, pp. 3321–3329, 2015.

Lampert, Christoph H. Maximum margin multi-label struc-

tured prediction. In NIPS, pp. 289–297, 2011.

Liu, Weiwei and Tsang, Ivor W. On the optimality of clas-
In NIPS, pp.

siﬁer chain for multi-label classiﬁcation.
712–720, 2015.

Pham, Anh T., Raich, Raviv, Fern, Xiaoli Z., and Arriaga,
Jes´us P´erez. Multi-instance multi-label learning in the
presence of novel class instances. In ICML, pp. 2427–
2435, 2015.

