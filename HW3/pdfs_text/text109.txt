Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs

Alon Brutzkus 1 Amir Globerson 1

Abstract

Deep learning models are often successfully
trained using gradient descent, despite the worst
case hardness of the underlying non-convex op-
timization problem. The key question is then un-
der what conditions can one prove that optimiza-
tion will succeed. Here we provide a strong re-
sult of this kind. We consider a neural net with
one hidden layer and a convolutional structure
with no overlap, and a ReLU activation func-
tion. For this architecture we show that learn-
ing is NP-complete in the general case, but that
when the input distribution is Gaussian, gradient
descent converges to the global optimum in poly-
nomial time. To the best of our knowledge, this
is the ﬁrst global optimality guarantee of gradient
descent on a convolutional neural network with
ReLU activations.

1. Introduction

Deep neural networks have achieved state-of-the-art perfor-
mance on many machine learning tasks in areas such as nat-
ural language processing (Wu et al., 2016), computer vision
(Krizhevsky et al., 2012) and speech recognition (Hinton
et al., 2012). Training of such networks is often success-
fully performed by minimizing a high-dimensional non-
convex objective function, using simple ﬁrst-order methods
such as stochastic gradient descent.

Nonetheless, the success of deep learning from an op-
timization perspective is poorly understood theoretically.
Current results are mostly pessimistic, suggesting that even
training a 3-node neural network is NP-hard (Blum &
Rivest, 1993), and that the objective function of a single
neuron can admit exponentially many local minima (Auer
et al., 1996; Safran & Shamir, 2016). There have been re-

1Tel Aviv University, Blavatnik School

puter Science.
<alonbrutzkus@mail.tau.ac.il>,
<gamir@cs.tau.ac.il>.

Correspondence

of Com-
Alon Brutzkus
Globerson

to:
Amir

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

cent attempts to bridge this gap between theory and prac-
tice. Several works focus on the geometric properties of
loss functions that neural networks attempt to minimize.
For some simpliﬁed architectures, such as linear activa-
tions, it can be shown that there are no bad local minima
(Kawaguchi, 2016). Extension of these results to the non-
linear case currently requires very strong independence as-
sumptions between the activations of the neurons and the
inputs (Kawaguchi, 2016).

Since gradient descent is the main “work-horse” of deep
learning it is of key interest to understand its convergence
properties. However, there are no results showing that
gradient descent is globally optimal for non-linear mod-
els, except for the case of many hidden neurons (Andoni
et al., 2014) and non-linear activation functions that are not
widely used in practice (Zhang et al., 2017).1 Here we pro-
vide the ﬁrst such result for a neural architecture that has
two very common components: namely a ReLU activation
function and a convolution layer.

The architecture considered in the current paper is shown in
Figure 1. We refer to these models as no-overlap networks.
A no-overlap network can be viewed as a simple convolu-
tion layer with non overlapping ﬁlters, followed by a ReLU
activation function, and then average pooling. Formally, let
w ∈ Rm denote the ﬁlter coefﬁcients, and assume the input
x is in Rd. Deﬁne k = d/m and assume that k is integral.
Partition x into k non-overlapping parts and denote x[i] the
ith part. Finally, deﬁne σ to be the ReLU activation func-
tion, namely σ (z) = max{0, z}. Then the output of the
network in Figure 1 is given by:

f (x; w) =

σ (w · x[i])

(1)

1
k

(cid:88)

i

We note that such architectures have been used in several
works (Lin et al., 2013; Milletari et al., 2016), but we view
them as important ﬁrstly because they capture key proper-
ties of general convolutional networks.

We address the realizable case, where training data is gen-
erated from a function as in Eq. 1 with weight vector
w∗. Training data is then generated by sampling n train-
ing points x1, . . . , xn from a distribution D, and assigning
them labels using y = f (x; w∗). The learning problem is

1See more related work in Section 2.

Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs

then to ﬁnd a w that minimizes the squared loss. In other
words, solve the optimization problem:

min
w

1
n

(cid:88)

i

(f (xi; w) − yi)2

(2)

In the limit n → ∞, this is equivalent to minimizing the
population risk:

(cid:96)(w) = Ex∼D

(cid:104)

(f (x; w) − f (x; w∗))2(cid:105)

(3)

Like several recent works (Hardt et al., 2016; Hardt & Ma,
2016) we focus on minimizing the population risk, leaving
the ﬁnite sample case to future work. We believe the pop-
ulation risk captures the key characteristics of the problem,
since the large data regime is the one of interest.

on the data. We provide an empirical demonstration of this
in Section 6 where gradient descent is shown to succeed in
the Gaussian case and fail for a different distribution.

To further understand the role of overlap in the network,
we consider networks that do have overlap between the ﬁl-
ters. In Section 7.1 we show that in this case, even under
Gaussian distributed inputs, there will be non-optimal local
minima. Thus, gradient descent will no longer be optimal
In Section 7.2 we show empirically
in the overlap case.
that these local optima may be overcome in practice by us-
ing gradient descent with multiple restarts.

Taken together, our results are the ﬁrst to demonstrate
distribution dependent optimality of gradient descent for
learning a neural network with a convolutional like archi-
tecture and a ReLU activation function.

2. Related Work

Hardness of learning neural networks has been demon-
strated for many different settings. For example, Blum
& Rivest (1993) show that learning a neural network with
one hidden layer with a sign activation function is NP-hard
in the realizable case. Livni et al. (2014) extend this to
other activation functions and bounded norm optimization.
Hardness can also be shown for improper learning under
certain cryptographic assumptions (e.g., see Daniely et al.,
2014; Klivans, 2008; Livni et al., 2014). Note that these
hardness results do not hold for the regression and tied pa-
rameter setting that we consider.

Due to the above hardness results, it is clear that the suc-
cess of deep-learning can only be explained by making ad-
ditional assumptions about the data generating distribution.
The classic algorithm by Baum (1990) shows that intersec-
tion of halfspaces (i.e., a speciﬁc instance of a one hidden
layer network) is PAC learnable under any symmetric dis-
tribution. This was later extended in Klivans et al. (2009)
to log-concave distributions.

The above works do not consider gradient descent as the
optimization method, leaving open the question of which
assumptions can lead to global optimality of gradient de-
scent. Such results have been hard to obtain, and we sur-
vey some recent ones below. One instance when gradi-
ent descent can succeed is when there are enough hidden
units such that random initialization of the ﬁrst layer can
lead to zero error even if only the second layer is trained.
Such over-speciﬁed networks have been considered (An-
doni et al., 2014; Livni et al., 2014) and it was shown that
gradient descent can globally learn them in some cases
(Andoni et al., 2014). However, the assumption of over-
speciﬁcation is very restrictive and limits generalization.
In contrast, we show convergence of gradient descent to a
global optimum for any network size and consider convo-

Figure 1. Convolutional neural network with non-overlapping ﬁl-
In the ﬁrst layer, a ﬁlter w is applied to non-overlapping
ters.
parts of the input vector x, and the output passes through a ReLU
activation function. The outputs of the neurons are then averaged
to give the output y.

Our key results are as follows:

• Worst Case Hardness: Despite the simplicity of No-
Overlap Networks, we show that learning them is in
fact hard if D is unconstrained. Speciﬁcally, in Sec-
tion 4, we show that learning No-Overlap Networks is
NP complete via a reduction from a variant of the set
splitting problem.

• Distribution Dependent Tractability: When D
corresponds to independent Gaussian variables with
µ = 0, σ2 = 1, we show in Section 5 that No-Overlap
Networks can be learned in polynomial time using gra-
dient descent.

The above two results nicely demonstrate the gap between
worst-case intractability and tractability under assumptions

wReLU+yxwwGlobally Optimal Gradient Descent for a ConvNet with Gaussian Inputs

lutional neural networks with shared parameters. Another
interesting case is linear dynamical systems, where Hardt
et al. (2016) show that under independence assumptions
maximum likelihood is quasi-concave and hence solvable
with gradient ascent.

Recent work by Mei et al. (2016) shows that regres-
sion with a single neuron and certain non-linear activation
functions, can be learned with gradient descent for sub-
Gaussian inputs. We note that their architecture is signiﬁ-
cantly simpler than ours, in that it uses a single neuron. In
fact, their regression problem can also be solved via meth-
ods for generalized linear models (Kakade et al., 2011).

Shamir (2016) recently showed that there is a limit to what
distribution dependent results can achieve. Namely, it was
shown that for large enough one-hidden layer networks,
no distributional assumption such as Gaussian inputs can
make gradient descent tractable. Importantly, the construc-
tion in Shamir (2016) does not use parameter tying and thus
is not applicable to the architecture we study here.

Several works have focused on understanding the loss sur-
face of neural network objectives, but without direct al-
gorithmic implications. Kawaguchi (2016) show that lin-
ear neural networks do not suffer from bad local minima.
Hardt & Ma (2016) consider objectives of linear residual
networks and prove that there are no critical points other
than the global optimum. Soudry & Carmon (2016) show
that in the objective of over-parameterized neural networks
with dropout-like noise, all differentiable local minima are
global. Other works (Safran & Shamir, 2016; Haeffele
& Vidal, 2015) give similar results for over-speciﬁed net-
works. All of these results are purely geometric and do not
have direct implications on convergence of optimization al-
gorithms. Janzamin et al. (2015) and Goel et al. (2016),
suggest alternatives to gradient-based methods for learning
neural networks. However, these algorithms are not widely
used in practice. Finally, Choromanska et al. (2015) use
spin glass models to argue that, under certain generative
modelling and architectural constraints, local minima are
likely to have low loss values.

The theory of non-convex optimization is closely related
there has
to the theory of neural networks. Recently,
been substantial progress in proving convergence guaran-
tees of simple ﬁrst-order methods in various machine learn-
ing problems, that don’t correspond to typical neural nets.
These include for example matrix completion (Ge et al.,
2016) and tensor decompositions (Ge et al., 2015).

Finally, recent work by Zhang et al. (2016) shows that neu-
ral nets can perfectly ﬁt random labelings of the data. Un-
derstanding this from an optimization perspective is largely
an open problem.

3. Preliminaries

We use bold-faced letters for vectors and capital letters for
matrices. The ith row of a matrix A is denoted by ai.

In our analysis in Section 5 and Section 7.1 we assume that
the input feature x ∈ Rd is a vector of IID Gaussian ran-
dom variables with zero mean and variance one.2 Denote
this distribution by G. We consider networks with one hid-
den layer, and k hidden units. Our main focus will be on
No-Overlap Networks, but we begin with a more general
one-hidden-layer neural network with a fully-connected
layer parameterized by W ∈ Rk,d followed by average
pooling. The network output is then:

f (x; W ) =

σ (wi · x)

(4)

1
k

(cid:88)

i

where σ () is the pointwise ReLU function.

We consider the realizable setting where there exists a true
W ∗ using which the training data is generated. The popu-
lation risk (see Eq. 3) is then:

(cid:96)(W ) = EG

(cid:2)(f (x; W ) − f (x; W ∗))2(cid:3) ,

(5)

As we show next, (cid:96)(W ) can be considerably simpliﬁed.
First, deﬁne:

g(u, v) = EG [σ (u · x) σ (v · x)]

(6)

Simple algebra then shows that:

(cid:96)(W ) =

1
k2

(cid:88)

i,j

(cid:2)g(wi, wj) − 2g(wi, w∗

j ) + g(w∗

i , w∗

j )(cid:3)

The next Lemma from Cho & Saul (2009) shows that
g(u, v) has a simple form.

Lemma 3.1. (Cho & Saul, 2009, Section 2) Given u, v ∈
Rd denote by θu,v the angle between u and v. Then:

g(u, v) =

(cid:107)u(cid:107) (cid:107)v(cid:107)

sin θu,v +

π − θu,v

cos θu,v

(cid:32)

(cid:16)

(cid:17)

1
2π

(7)

(cid:33)

The gradient of g with respect to u also turns out to have
a simple form, as stated in the lemma below. The proof is
deferred to the supplementary material.

Lemma 3.2. Let g be as deﬁned in Eq. 6. Then g is differ-
entiable at all points u (cid:54)= 0 and

∂g(u, v)
∂u

1
2π

u
(cid:107)u(cid:107)

=

(cid:107)v(cid:107)

sin θu,v +

π − θu,v

v

(cid:16)

1
2π

(cid:17)

2The variance per variable can be arbitrary. We choose one for

simplicity.

Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs

We conclude by special-casing the results above to No-
Overlap Networks. In this case, the entire model is spec-
iﬁed by a single ﬁlter vector w ∈ Rm. The rows wi
are mostly zeros, except for the indices ((i − 1)m +
1, . . . , im) which take the values of w. Namely, wi =
(cid:1) where 0l ∈ Rl is a zero vector.
(cid:0)0(i−1)m, w, 0d−im
The same holds for the vectors w∗
i with a weight vector
w∗. This simpliﬁes the loss considerably, since for all i:
2 (cid:107)w(cid:107)2, and for all i (cid:54)= j: g(wi, wj) =
g(wi, wi) = 1
2π (cid:107)w(cid:107)2 and g(wi, w∗
1
2π (cid:107)w(cid:107) (cid:107)w∗(cid:107). Thus the loss
(cid:96)(w) for No-Overlap Networks yields (up to additive fac-
tors in w∗):

j ) = 1

(cid:96)(w) =

γ(cid:107)w(cid:107)2 − 2kg(w, w∗) − 2β (cid:107)w(cid:107) (cid:107)w∗(cid:107)

(cid:105)

(8)

(cid:104)

1
k2

where β = k2−k

2π and γ = β + k
2 .

4. Learning No-Overlap Networks is

NP-Complete

The No-Overlap Networks architecture is a simpliﬁed con-
volutional layer with average pooling. However, as we
show here, learning it is still a hard problem. This will
motivate our exploration of distribution dependent results
in Section 5.

Recall that our focus is on minimizing the squared error in
Eq. 3. For this section, we do not make any assumptions
about D. Thus D can be a distribution with uniform mass
on training points x1, . . . , xn, recovering the empirical risk
in Eq. 2. We know that (cid:96)(w) in Eq. 3 can be minimized by
setting w = w∗ and the corresponding squared loss (cid:96)(w)
will be zero. However, we of course do not know w∗, and
the question is how difﬁcult is it to minimize (cid:96)(w). In what
follows we show that this is hard. Namely, it is an NP-
complete problem to ﬁnd a w that comes (cid:15)0 close to the
minimum of (cid:96)(w), for some constant (cid:15)0.

We begin by deﬁning the Set-Splitting-by-k-Sets problem,
which is a variant of the classic Set-Splitting problem
(Garey & Johnson, 1990). After establishing the hardness
of Set-Splitting-by-k-Sets, we will provide a reduction from
it to learning No-Overlap Networks.
Deﬁnition 1. The Set-Splitting-by-k-Sets decision problem
is deﬁned as follows: Given a ﬁnite set S of d elements and
a collection C of at most (k − 1)d subsets Cj of S, do there
exist disjoint sets S1, S2, ..., Sk such that (cid:83)
i Si = S and
for all j and i, Cj (cid:54)⊆ Si?

For k = 2 and without the upper bound on |C| this is
known as the Set-Splitting decision problem which is NP-
complete (Garey & Johnson, 1990). Next, we show that
Set-Splitting-by-k-Sets is NP-complete. The proof is via a
reduction from 3SAT and induction, and is provided in the
supplementary material.

Proposition 4.1. Set-Splitting-by-k-Sets is NP-complete
for all k ≥ 2.

We next formulate the No-Overlap Networks optimization
problem.

Deﬁnition 2. The k-Non-Overlap-Opt problem is deﬁned
as follows. The input is a distribution DX,Y over input-
output pairs x, y where x ∈ Rd. If the input is realizable
by a no-overlap network with k hidden neurons, then the
output is a vector w such that:

EDX,Y

(cid:104)

(f (x; w) − y))2(cid:105)

<

1
4k5d

(9)

Otherwise an arbitrary weight vector is returned.3

The above problem returns a w that minimizes the
1
population-risk up to
4k5d accuracy. It is thus easier than
minimizing the risk to an arbitrary precision (cid:15) (see Section
5, Theorem 5.2).

We prove the following theorem, which uses some ideas
from Blum & Rivest (1993), but introduces additional con-
structions needed for the no overlap case.

Theorem 4.2. For all k ≥ 2, the k-Non-Overlap-Opt prob-
lem is NP-complete.

Proof. We will show a reduction from Set-Splitting-by-k-
sets to k-Non-Overlap-Opt. Assume a given instance of the
Set-Splitting-by-k-sets problem with a set S and collection
of subsets C. Denote S = {1, 2, ..., d} and |C| ≤ (k − 1)d.
Let 0d ∈ Rd be the all zeros vector. For a vector v ∈ Rd,
deﬁne the vector di(v) ∈ Rkd to be the concatenation of
i − 1 vectors 0d, followed by v and k − i vectors 0d, and
let d(v) = (d1(v), d2(v), ..., dk(v)) ∈ Rk2d.

We next deﬁne a training set for k-Non-Overlap-Opt. For
each element i ∈ S deﬁne an input vector xi = d(ei),
where ei is the standard basis of Rd. Assign the label
yi = 1
k to this input. In addition, for each subset Cj ∈ C
deﬁne the vector xd+j = d((cid:80)
ei) and label yd+j = 0.
i∈Cj
Thus we have |S| + |C| data points in Rk2d. Let DX,Y be a
uniform distribution over the training set points (i.e., each
point with probability at least 1

kd since |C| ≤ (k − 1)d).

We will now show that the given instance of Set-Splitting-
by-k-sets has a solution (i.e., there exist splitting sets) if and
only if k-Non-Overlap-Opt returns a weight vector with low
risk. First, assume there exist splitting sets S1, ..., Sk.4 For
each 1 ≤ l ≤ k deﬁne the vector aSl ∈ Rd such that
for all i ∈ Sl, aSl
i = −d otherwise. Deﬁne
a No-Overlap Network with k2d inputs and weight vector

i = 1 and aSl

3We assume that the population risk is efﬁciently computable.
4The sets are disjoint, their union is S and for all j and i,

Cj (cid:54)⊆ Si.

Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs

w = (aS1, aS2, ..., aSk ) ∈ Rkd. Then for all 1 ≤ i ≤ d
we have:

5. No-Overlap Networks can be Learned for

Gaussian Inputs

f (xi; w) =

(cid:80)k

l=1 σ((aSl )T ei)
k

1
k

=

= yi

(10)

and for all j:

(cid:80)k

l=1 σ((aSl)T ((cid:80)

ei))

i∈Cj

k

f (xd+j; w) =

= 0 = yd+j
(11)
where the last equality follows since for all l and j, Cj (cid:54)⊆
Sl. There thus exists a w for which the error in Eq. 9 is zero
and k-Non-Overlap-Opt will return a weight vector with
low risk.

Conversely, assume that k-Non-Overlap-Opt returned a
w ∈ Rkd with risk less than
1
4k5d on DX,Y above. De-
note by w = (w1, w2, ..., wk), where wl ∈ Rd. We will
show that this implies that there exist k splitting sets. For
all x(cid:48), y(cid:48) in the training set it holds that:5

(f (x(cid:48); w) − y(cid:48))2
kd

≤ EDX,Y [(f (x; w) − y)2] <

1
4k5d

This implies that for all i and j,

|f (d(ei); w) −

| <

|f (d(

ei); w)| <

1
k

1
2k2 ,

(cid:88)

i∈Cj

1
2k2

(12)
l ei > 1
Deﬁne sets Sl = {i | wT
2k } for 1 ≤ l ≤ k and
WLOG assume they are disjoint by arbitrarily assigning
points that belong to more than one set, to one of the sets
they belong to. We will next show that these Sl are split-
ting. Namely, it holds that (cid:83)
l Sl = S and no subset Cj is a
subset of some Sl.

(cid:80)k

l ei)

> 1

k − 1

l=1 σ(wT
2k2 > 1
Since f (d(ei); w) =
2k
k
for all i, it follows that for each i ∈ S there exists 1 ≤
l ei > 1
l ≤ k such that wT
2k . Therefore, by the deﬁni-
tion of Sl we deduce that (cid:83)
l Sl = S. To show the sec-
ond property, assume by contradiction that for some j and
ei) > |Cj |
m, Cj ⊆ Sm. Then wT
2k , which im-
l ((cid:80)
(cid:80)k
l=1 σ(wT
plies that f (d((cid:80)
k
|Cj |
2k2 ≥ 1

2k2 , a contradiction. This concludes our proof.

ei); w) =

m((cid:80)

i∈Cj

i∈Cj

ei))

i∈Cj

>

To conclude, we have shown that No-Overlap Networks are
hard to learn if one does not make any assumptions about
the training data. In fact we have shown that ﬁnding a w
1
4k5d is hard. In the next section, we show
with loss at most
that certain distributional assumptions make the problem
tractable.

In this section we assume that the input features x are gen-
erated via a Gaussian distribution G, as in Section 3. We
will show that in this case, gradient descent will converge
with high probability to the global optimum of (cid:96)(w) (Eq. 8)
in polynomial time.

In order to analyze convergence of gradient descent on
(cid:96), we need a characterization of all
the critical and
non-differentiable points. We show that (cid:96) has a non-
differentiable point and a degenerate saddle point.6 There-
fore, recent methods for showing global convergence of
gradient-based optimizers on non-convex objectives (Lee
et al., 2016; Ge et al., 2015) cannot be used in our case, be-
cause they assume all saddles are strict 7 and the objective
function is continuously differentiable everywhere.

The characterization is given in the following proposition.
The proof relies on the fact that (cid:96)(w) depends only on
(cid:107)w(cid:107),(cid:107)w∗(cid:107) and θw,w∗ , and therefore w.l.o.g. it can be as-
sumed that w∗ lies on one of the axes. Then by a symmetry
argument, in order to prove properties of the gradient and
the Hessian, it sufﬁces to calculate partial derivatives with
respect to at most three variables.

Proposition 5.1. Let (cid:96)(w) be deﬁned as in Eq. 8. Then the
following holds:

1. (cid:96)(w) is differentiable if and only if w (cid:54)= 0.

2. For k > 1, (cid:96)(w) has three critical points:

(a) A local maximum at w = 0.
(b) A unique global minimum at w = w∗.
(c) A degenerate

saddle

point

at w

=

−(

k2−k

k2+(π−1)k )w∗.

For k = 1, w = 0 is not a local maximum and the
unique global minimum w∗ is the only differentiable
critical point 8.

We next consider a simple gradient descent update rule for
minimizing (cid:96)(w) and analyze its convergence. Let λ >
0 denote the step size. Then the update at iteration t is
simply:

wt+1 = wt − λ∇(cid:96)(wt)

(13)

Our main result, stated formally below, is that the above
update is guaranteed to converge to an (cid:15) accurate solution
after O( 1
(cid:15)2 ) iterations. We note that the dependence of the

6A saddle point is degenerate if the Hessian at the point has

only non-negative eigenvalues and at least one zero eigenvalue.

7A saddle point is strict if the Hessian at the point has at least

5The LHS is true because for a non-negative random variable

one negative eigenvalue.

X, E[X] ≥ p(x)x for all x, and in our case p(x) ≥ 1
kd .

8See Figure 2.

Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs

follows from the fact that w = 0 is a local maximum.13

The fact that wt stays away from the problematic points
allows us to show that (cid:96)(w) has a Lipschitz continuous
gradient on the line between wt and wt+1, with constant
L = ˜O(1).12 By standard optimization analysis (Nesterov,
2004) it follows that after T = O( 1
(cid:15)2 ) iterations we will
have (cid:107)∇l(wt)(cid:107) ≤ O((cid:15)) for some 0 ≤ t ≤ T . This in
(cid:15))-close to w∗.
turn can be used to show that wt is O(
Finally, since (cid:96)(w) ≤ d(cid:107)w − w∗(cid:107)2, it follows that wt ap-
proximates the global minimum to within O((cid:15)) accuracy.

√

Theorem 5.2 implies that gradient descent converges to a
point w such that (cid:96)(w) ≤ 1
d2 in time O(poly(d)) where
d is the input dimension.14 The following corollary thus
follows.

Corollary 5.3. Gradient descent
the k-Non-
Overlap-Opt problem under the Gaussian assumption on
D with high probability and in polynomial time.

solves

6. Empirical Illustration of Tractability Gap

The results in the previous sections showed that No-
Overlap Networks optimization is hard in the general case,
but tractable for Gaussian inputs. Here we empirically
demonstrate both the easy and hard cases. The training data
for the two cases will be generated by using the same w∗
but different distributions over x.

To generate the “hard” case, we begin with a set splitting
problem. In particular, we consider a set S with 40 ele-
ments and a collection C of 760 subsets of S, each of size
20. We choose Cj such that there exists subsets S1,S2 that
split the subsets Cj. We use the reduction in Section 4
to convert this into a No-Overlap Networks optimization
problem. This results in a training set of size 800.

Since we know the w∗ that solves the set splitting prob-
lem, we can use it to label data from a different distribution.
Motivated by Section 5 we use a Gaussian distribution G as
deﬁned earlier and generate a training set of the same size
(namely 800) and labels given by the no-overlap network
with weight w∗.

For these two learning problems we used AdaGrad (Duchi
et al., 2011) to optimize the empirical risk (plain gradient
descent also converges, but AdaGrad requires less tuning
of step size). For both datasets we used a random normal
initializer and for each we chose the best performing learn-
ing rate schedule. The training error for each setting as a
function of the number of epochs is shown in Figure 3. It is
clear that in the non-Gaussian case, AdaGrad gets trapped

13The proof holds even for k = 1 where w = 0 is not a local

14Note that the complexity of a gradient descent iteration is

Figure 2. Colormap of (cid:96)(w) (Eq. 8) in 2 dimensions for w∗ =
(1, 1) and k = 10.

convergence rate on (cid:15) is similar to standard results on con-
vergence of gradient descent to stationary points (e.g., see
discussion in Allen-Zhu & Hazan, 2016).
Theorem 5.2. Assume (cid:107)w∗(cid:107) = 1.9 For any δ > 0 and
0 < (cid:15) < δ sin πδ
, there exists 0 < λ < 1 10 such that
with probability at least 1 − δ, gradient descent initialized
randomly from the unit sphere with learning rate λ will get
to a point w such that (cid:96)(w) ≤ O((cid:15)) in O( 1
(cid:15)2 ) iterations.11

k

The complete proof is provided in the supplementary ma-
terial. Here we provide a high level overview. In particular,
we ﬁrst explain why gradient descent will stay away from
the two bad points mentioned in Lemma 5.1.

First we note that the gradient of (cid:96)(w) at wt is given by:

∇(cid:96)(wt) = −c1(wt, w∗)wt − c2(wt, w∗)w∗ ,

(14)

where c1 and c2 are two functions such that c1 ≥ −1, c2 ≥
0 and c2 = 0 if and only if θwt,w∗ = π. Thus the gradient
is a sum of a vector in the direction of wt and a vector in
the direction of w∗. At iteration t + 1 we have:

wt+1 = (1 + λc1(wt, w∗))wt + λc2(wt, w∗)w∗

(15)

(cid:54)= π, we have
It follows that for λ < 1 and θwt,w∗
θwt+1,w∗ < θwt,w∗ . Therefore, if θw0,w∗ (cid:54)= π , we will
never converge to the saddle point in Lemma 5.1.

Next, assuming (cid:107)w0(cid:107) > 0 and that θw0,w∗ ≤ (1 − δ)π
(which occurs with probability 1 − δ), it can be shown that
the norm of wt is always bounded away from zero by a
constant M = ˜Ω(1).12 The proof is quite technical and

9Assumed for simplicity, otherwise (cid:107)w∗(cid:107) is a constant factor.
10λ can be found explicitly.
11O(·) hides a linear factor in d.
12 ˜Ω and ˜O hide factors of (cid:107)w∗(cid:107), θw0,w∗ , k and δ.

maximum.

polynomial in d.

Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs

lapping ﬁlter of size 2 with stride 1, i.e., for all 1 ≤ i ≤
i = (0i−1, w∗, 0d−i−1)
k wi = (0i−1, w, 0d−i−1), w∗
where 0l = (0, 0, ..., 0) ∈ Rl, w = (w1, w2) is a vector of
2 parameters and w∗ = (−w∗, w∗) ∈ R2, w∗ > 0. Deﬁne
the following vectors wr = (w1, w2, 0), wl = (0, w1, w2),
l = (0, −w∗, w∗) and denote by
w∗
θw,v the angle between two vectors w and v.

r = (−w∗, w∗, 0), w∗

One might wonder why the analysis of the overlapping
case should be any different than the non-overlapping case.
However, even for a ﬁlter of size two, as above, the loss
function and consequently the gradient, are more complex
in the overlapping case. Indeed, the loss function in this
case is given by:

(cid:96)(w) = α((cid:107)w(cid:107)2 + (cid:107)w∗(cid:107)2) − βg(w, w∗)
+ (β − 2)(g(wr, wl) − g(wl, w∗
r)
l )) − γ (cid:107)w(cid:107) (cid:107)w∗(cid:107)
l ) + g(w∗
− g(wr, w∗

r, w∗

(16)

where α = 1
k2

(cid:0) k
2 + k2−3k+2

2π

(cid:1), β = 2k and γ = k2−3k+2

.

π

Figure 4. The population risk for a network with overlapping ﬁl-
ters, with a two dimensional ﬁlter w∗ = [−1, 1], k = 4, d = 5,
and Gaussian inputs.

Compared to the objective in Eq. 8 which depends only on
(cid:107)w(cid:107), (cid:107)w(cid:107) and θw,w∗ , we see that the objective in Eq. 16
has new terms such as g(wr, w∗
l ) which has a more com-
plicated dependence on the weight vectors w∗ and w. This
does not only have implications on the analysis, but also
on the geometric properties of the loss function and the dy-
namics of gradient descent. In particular, in Figure 4 we
see that the objective has a large sub-optimal region which
is not the case when the ﬁlters are non-overlapping.

As in the previous section we consider gradient descent up-
dates as in Eq. 13. The following Proposition shows that
if w is initialized in the interior of the fourth quadrant of
R2, then it will stay there for all remaining iterations. The
proof is a straightforward inspection of the components of
the gradient, and is provided in the supplementary.

Figure 3. Training loss of Adagrad on the Gaussian and Non-
Gaussian datasets. See Section 6 for details.

at a sub-optimal point, whereas the Gaussian case is solved
optimally.15 In the Gaussian case AdaGrad converged to
w∗. Therefore, given the Gaussian dataset we were able
to recover the true weight vector w∗, whereas given the
data constructed via the reduction we were not, even though
both datasets were of the same size. We conclude that these
empirical ﬁndings are in line with our theoretical results.

7. Networks with Overlapping Filters

Thus far we showed that the non-overlapping case becomes
tractable under Gaussian inputs. A natural question is
then what happens when overlaps are allowed (namely, the
stride is smaller than the ﬁlter size). Will gradient descent
still ﬁnd a global optimum? Here we show that this is in
fact not the case, and that with probability greater than 1
4
gradient descent will get stuck in a sub-optimal region. In
Section 7.1 we analyze this setting for a two dimensional
example and provide bounds on the level of suboptimality.
In Section 7.2 we report on an empirical study of optimiza-
tion for networks with overlapping ﬁlters. Our results sug-
gest that by restarting gradient descent a constant number
of times, it will converge to the global minimum with high
probability. Complete proofs of the results are provided in
the supplementary material.

7.1. Suboptimality of Gradient Descent for R2

We consider an instance where there are k = d − 1 neu-
rons and matrices W, W ∗ ∈ Rk×d correspond to an over-

15We note that the value of 0.06 attained by the non-Gaussian
case is quite high, since the zero weight vector in this case has
loss of order 0.1.

020406080Iteration00.020.040.060.080.1LossNon-GaussianGaussianGlobally Optimal Gradient Descent for a ConvNet with Gaussian Inputs

Proposition 7.1. For any λ ∈ (0, 1
of the fourth quadrant of R2 then so is wt+1.

3 ), if wt is in the interior

follows easily from equating the population risk (Eq. 3) to 0
and the full proof is deferred to the supplementary material.

Note that in our example the global optimum w∗ is in the
second quadrant (it’s easy to show that it is also unique).
Hence, if initialized at the fourth quadrant, gradient descent
will remain in a sub-optimal region. The sub-optimality
can be clearly seen in Figure 4. In the proposition below
we formalize this observation by giving a tight lower bound
on the values of (cid:96)(w) for w in the fourth quadrant. Specif-
ically, we show that the sub-optimality scales with O( 1
k2 ).
The proof idea is to express all angles between all the vec-
tors that appear in Eq. 16 via a single angle parameter θ
between w in the fourth quadrant and the positive x-axis.
Then it is possible to prove the relatively simpler one di-
mensional inequality that depends on θ.
Proposition 7.2. Let h(k) = k2−3k+2
2(k−1)
3
2h(k)+1

+
, then for all w in the fourth quadrant l(w) ≥
k2(2h(k)+2) (cid:107)w∗(cid:107)2 and this lower bound is attained by
˜w = − h(k)

3(k−1)
π

+

√

π

h(k)+1 w∗.

The above two propositions result in the following char-
acterization of the sub-optimality of gradient descent for
w ∈ R2 and overlapping ﬁlters.
Theorem 7.3. Deﬁne h(k) as in Proposition 7.2. Then with
probability ≥ 1
4 , a randomly initialized gradient descent
with learning rate λ ∈ (0, 1
3 ) will get stuck in a sub-optimal
region, where each point in this region has loss at least
k2(2h(k)+2) (cid:107)w∗(cid:107)2 and this bound is tight.

2h(k)+1

7.2. Empirical study of Gradient Descent for m > 2

In Section 7.1 we showed that already for m = 2, net-
works with w ∈ Rm and ﬁlter overlaps exhibit more com-
plex behavior than those without overlap. This leaves open
the question of what happens in the general case under the
Gaussian assumption, for various values of d, m and over-
laps. We leave the theoretical analysis of this question to
future work, but here report on empirical ﬁndings that hint
at what the solution should look like.

We experimented with a range of d, m and overlap values
(see supplementary material for details of the experimen-
tal setup). For each value of d, m and overlap we sam-
pled 90 values of w∗ from various uniform input distribu-
tions with different supports and several pre-deﬁned deter-
ministic values. This resulted in more than 1200 different
sampled w∗. For each such w∗ we ran gradient descent
multiple times, each initialized randomly from a different
w0. Using the results from these runs, we could estimate
the probability of sampling a w0 that would converge to
the unique global minimum. Viewed differently, this is the
probability mass of the basin of attraction of the global op-
timum. We note that the uniqueness of the global minimum

Our results are that across all values of d, m, overlap and
w∗, the probability mass of the basin of attraction is at least
1
17 . The practical implication is that multiple restarts of gra-
dient descent (in this case a few dozen) will ﬁnd the global
optimum with high probability. We leave formal analysis
of this intriguing fact for future work.

8. Discussion

The key theoretical question in deep learning is why it suc-
ceeds in ﬁnding good models despite the non-convexity of
the training loss. It is clear that an answer must character-
ize speciﬁc settings where deep learning provably works.
Despite considerable recent effort, such a case has not been
shown. Here we provide the ﬁrst analysis of a non-linear ar-
chitecture where gradient descent is globally optimal, for a
certain input distribution, namely Gaussian. Thus our spe-
ciﬁc characterization is both in terms of architecture (no-
overlap networks, single hidden layer, and average pool-
ing) and input distribution. We show that learning in no-
overlap architectures is hard, so that some input distribu-
tion restriction is necessary for tractability. Note however,
that it is certainly possible that other, non-Gaussian, distri-
butions also result in tractability. Some candidates would
be sub-Gaussian and log-concave distributions.

Our derivation addressed the population risk, which for the
Gaussian case can be calculated in closed form. In prac-
tice, one minimizes an empirical risk. Our experiments in
Section 6 suggest that optimizing the empirical risk in the
Gaussian case is tractable. It would be interesting to prove
this formally. It is likely that measure concentration results
can be used to get similar results to those we had for the
population risk (e.g., see Mei et al., 2016; Xu et al., 2016,
for use of such tools).

Convolution layers are among the basic building block of
neural networks. Our work is among the ﬁrst to analyze
optimization for these. The architecture we study is similar
in structure to convolutional networks, in the sense of us-
ing parameter tying and pooling. However, most standard
convolutional layers have overlap and use max pooling. In
Section 7 we provide initial results for the case of over-
lap, showing there is hope for proving optimality for gradi-
ent descent with random restarts. Analyzing max pooling
would be very interesting and is left for future work.

Finally, we note that distribution dependent tractability has
been shown for intersection of halfspaces (Klivans et al.,
2009), which is a non-convolutional architecture. However,
these results do not use gradient descent. It would be very
interesting to use our techniques to try and understand gra-
dient descent for the population risk in these settings.

Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs

Acknowledgements

This work was supported by the Blavatnik Computer Sci-
ence Research Fund, the Intel Collaborative Research Insti-
tute for Computational Intelligence (ICRI-CI), and an ISF
Centers of Excellence grant.

References

Ge, Rong, Lee, Jason D, and Ma, Tengyu. Matrix com-
pletion has no spurious local minimum. In Advances in
Neural Information Processing Systems, pp. 2973–2981,
2016.

Goel, Surbhi, Kanade, Varun, Klivans, Adam, and Thaler,
Justin. Reliably learning the ReLU in polynomial time.
arXiv preprint arXiv:1611.10258, 2016.

Allen-Zhu, Zeyuan and Hazan, Elad. Variance reduc-
tion for faster non-convex optimization. arXiv preprint
arXiv:1603.05643, 2016.

Haeffele, Benjamin D and Vidal, Ren´e. Global optimality
in tensor factorization, deep learning, and beyond. arXiv
preprint arXiv:1506.07540, 2015.

Andoni, Alexandr, Panigrahy, Rina, Valiant, Gregory, and
Zhang, Li. Learning polynomials with neural networks.
In Proceedings of the 31th International Conference on
Machine Learning, pp. 1908–1916, 2014.

Auer, Peter, Herbster, Mark, Warmuth, Manfred K, et al.
Exponentially many local minima for single neurons.
Advances in neural information processing systems, pp.
316–322, 1996.

Baum, Eric B. A polynomial time algorithm that learns
two hidden unit nets. Neural Computation, 2(4):510–
522, 1990.

Blum, Avrim L and Rivest, Ronald L. Training a 3-node
In Machine learning:

neural network is np-complete.
From theory to applications, pp. 9–28. Springer, 1993.

Cho, Youngmin and Saul, Lawrence K. Kernel methods
In Advances in neural information

for deep learning.
processing systems, pp. 342–350, 2009.

Choromanska, Anna, Henaff, Mikael, Mathieu, Michael,
Arous, G´erard Ben, and LeCun, Yann. The loss surfaces
of multilayer networks. In AISTATS, 2015.

Daniely, Amit, Linial, Nati, and Shalev-Shwartz, Shai.
From average case complexity to improper learning
In Proceedings of the 46th Annual ACM
complexity.
Symposium on Theory of Computing, pp. 441–448.
ACM, 2014.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research,
12(Jul):2121–2159, 2011.

Garey, Michael R. and Johnson, David S.

Comput-
ers and Intractability; A Guide to the Theory of NP-
Completeness. W. H. Freeman & Co., New York, NY,
USA, 1990. ISBN 0716710455.

Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang. Es-
caping from saddle points-online stochastic gradient for
tensor decomposition. In COLT, pp. 797–842, 2015.

Hardt, Moritz and Ma, Tengyu.

Identity matters in deep

learning. arXiv preprint arXiv:1611.04231, 2016.

Hardt, Moritz, Ma, Tengyu, and Recht, Benjamin. Gradient
descent learns linear dynamical systems. arXiv preprint
arXiv:1609.05191, 2016.

Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E,
Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, An-
drew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath,
Tara N, et al. Deep neural networks for acoustic mod-
eling in speech recognition: The shared views of four
research groups. IEEE Signal Processing Magazine, 29
(6):82–97, 2012.

Janzamin, Majid, Sedghi, Hanie, and Anandkumar, Anima.
Beating the perils of non-convexity: Guaranteed training
of neural networks using tensor methods. arXiv preprint
arXiv:1506.08473, 2015.

Kakade, Sham M, Kanade, Varun, Shamir, Ohad, and
Kalai, Adam. Efﬁcient learning of generalized linear
and single index models with isotonic regression. In Ad-
vances in Neural Information Processing Systems 24, pp.
927–935. 2011.

Kawaguchi, Kenji. Deep learning without poor local min-
ima. In Advances In Neural Information Processing Sys-
tems, pp. 586–594, 2016.

Klivans, Adam. Cryptographic hardness of learning. In En-
cyclopedia of Algorithms, pp. 210–212. Springer, 2008.

Klivans, Adam R, Long, Philip M, and Tang, Alex K.
Baums algorithm learns intersections of halfspaces with
respect to log-concave distributions. In Approximation,
Randomization, and Combinatorial Optimization. Algo-
rithms and Techniques, pp. 588–600. Springer, 2009.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in neural information processing
systems, pp. 1097–1105, 2012.

Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs

Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Recht,
Benjamin, and Vinyals, Oriol. Understanding deep
CoRR,
learning requires rethinking generalization.
abs/1611.03530, 2016. URL http://arxiv.org/
abs/1611.03530.

Zhang, Qiuyi, Panigrahy, Rina, Sachdeva, Sushant, and
Rahimi, Ali. Electron-proton dynamics in deep learning.
arXiv preprint arXiv:1702.00458, 2017.

Lee, Jason D., Simchowitz, Max, Jordan, Michael I., and
Recht, Benjamin. Gradient descent only converges to
minimizers. In Proceedings of the 29th Conference on
Learning Theory, pp. 1246–1257, 2016.

Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in

network. arXiv preprint arXiv:1312.4400, 2013.

Livni, Roi, Shalev-Shwartz, Shai, and Shamir, Ohad. On
the computational efﬁciency of training neural networks.
In Advances in Neural Information Processing Systems,
pp. 855–863, 2014.

Mei, Song, Bai, Yu, and Montanari, Andrea. The landscape
of empirical risk for non-convex losses. arXiv preprint
arXiv:1607.06534, 2016.

Milletari, Fausto, Navab, Nassir, and Ahmadi, Seyed-
Ahmad. V-net: Fully convolutional neural networks for
In 3D Vision
volumetric medical image segmentation.
(3DV), 2016 Fourth International Conference on, pp.
565–571. IEEE, 2016.

Nesterov, Yurii. Introductory lectures on convex optimiza-

tion. pp. 22–29, 2004.

Safran, Itay and Shamir, Ohad. On the quality of the initial
basin in overspeciﬁed neural networks. In Proceedings
of the 33nd International Conference on Machine Learn-
ing, pp. 774–782, 2016.

Shamir, Ohad. Distribution-speciﬁc hardness of learn-
ing neural networks. arXiv preprint arXiv:1609.01037,
2016.

Soudry, Daniel and Carmon, Yair. No bad local minima:
Data independent training error guarantees for multi-
layer neural networks. arXiv preprint arXiv:1605.08361,
2016.

Wu, Yonghui, Schuster, Mike, Chen, Zhifeng, Le, Quoc V.,
Norouzi, Mohammad, Macherey, Wolfgang, Krikun,
Maxim, Cao, Yuan, Gao, Qin, Macherey, Klaus,
Klingner, Jeff, Shah, Apurva, Johnson, Melvin, Liu,
Xiaobing, Kaiser, Lukasz, Gouws, Stephan, Kato,
Yoshikiyo, Kudo, Taku, Kazawa, Hideto, Stevens, Keith,
Kurian, George, Patil, Nishant, Wang, Wei, Young, Cliff,
Smith, Jason, Riesa, Jason, Rudnick, Alex, Vinyals,
Oriol, Corrado, Greg, Hughes, Macduff, and Dean, Jef-
frey. Google’s neural machine translation system: Bridg-
ing the gap between human and machine translation.
CoRR, abs/1609.08144, 2016.

Xu, Ji, Hsu, Daniel J, and Maleki, Arian. Global analysis of
expectation maximization for mixtures of two gaussians.
In Advances in Neural Information Processing Systems,
pp. 2676–2684, 2016.

