Analytical Guarantees on Numerical Precision of Deep Neural Networks

Supplementary Material

The main purpose of this supplementary section is to pro-
vide proofs for Theorems 1 and 2.

where both inequalities are due to the union bound.

Preliminaries

Here we shall give a proof of (1) as well as preliminary
results that will be needed to complete the proofs of Theo-
rems 1 and 2.
Proposition 1. The ﬁxed point error probability pe,f x is
upper bounded as shown in (1).

The next result is also straightforward, but quite useful in
obtaining upper bounds that are fully determined by aver-
ages.

Proposition 3. Given a random variable X and an event
E, we have:

E [X · 1E ] = E [X|E] Pr(E)

(16)

Proof. From the deﬁnitions of pe,f x, pe,f l, and pm,

where 1E denotes the indicator function of the event E.

pe,f x
= Pr{ ˆYf x (cid:54)= Y }
= Pr{ ˆYf x (cid:54)= Y, ˆYf x = ˆYf l} + Pr{ ˆYf x (cid:54)= Y, ˆYf x (cid:54)= ˆYf l}
= Pr{ ˆYf l (cid:54)= Y, ˆYf x = ˆYf l} + Pr{ ˆYf x (cid:54)= Y, ˆYf x (cid:54)= ˆYf l}
≤ pe,f l + pm.

Proof. By the law of total expectation,

E [X · 1E ]
= E [X · 1E | E] Pr(E) + E [X · 1E | E c] Pr(E c)
= E [X · 1 | E] Pr(E) + E [X · 0 | E c] Pr(E c)
= E [X|E] Pr(E).

Next is a simple result that allows us to replace the prob-
lem of upper bounding pm by several smaller and easier
problems by virtue of the union bound.
Proposition 2. In a M -class classiﬁcation problem, the to-
tal mismatch probability can be upper bounded as follows:

pm ≤

M
(cid:88)

M
(cid:88)

j=1

i=1,i(cid:54)=j

Pr( ˆYf x = i| ˆYf l = j) Pr( ˆYf l = j)

Proof.

pm = Pr( ˆYf x (cid:54)= ˆYf l) = Pr

( ˆYf x (cid:54)= j, ˆYf l = j)





M
(cid:91)

j=1

Pr( ˆYf x (cid:54)= j, ˆYf l = j)

M
(cid:88)

j=1

M
(cid:88)

j=1

M
(cid:88)

j=1

M
(cid:88)

≤

=

=

≤

Pr( ˆYf x (cid:54)= j| ˆYf l = j) Pr( ˆYf l = j)



Pr



M
(cid:91)

i=1,i(cid:54)=j

(cid:12)
(cid:12)
ˆYf x = i
(cid:12)
(cid:12)

ˆYf l = j


 Pr( ˆYf l = j)

M
(cid:88)

j=1

i=1,i(cid:54)=j

Pr( ˆYf x = i| ˆYf l = j) Pr( ˆYf l = j)

Proof of Theorem 1

Let us deﬁne pm,j→i for i (cid:54)= j as follows.

pm,j→i = Pr{ ˆYf x = i | ˆYf l = j}

(17)

(15)





We ﬁrst prove the following Lemma.

Lemma 1. Given BX and BF , if the output of the ﬂoating-
point network is ˆYf l = j, then that of the ﬁxed-point net-
work would be ˆYf x = i with a probability upper bounded
as follows:

pm,j→i ≤

∆2
A
24

E

(cid:80)

(cid:12)
(cid:12)
(cid:12)

h∈A

∂(Zi−Zj )
∂Ah
|Zi − Zj|2

2

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆYf l = j








+

∆2
W
24

E

(cid:80)

(cid:12)
(cid:12)
(cid:12)

h∈W

∂(Zi−Zj )
∂wh
|Zi − Zj|2

2

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆYf l = j


 . (18)











Proof. We can claim that, if i (cid:54)= j:

pm,j→i ≤ Pr{Zi + qZi > Zj + qZj | ˆYf l = j}

(19)

where the equality holds for M = 2.

Analytical Guarantees on Numerical Precision of Deep Neural Networks

From the law of total probability,

pm,j→i
(cid:90)

≤

fX(x) Pr

zi + qzi > zj + qzj | ˆYf l = j, x

dx,

(cid:16)

(cid:17)

(20)

where x denotes the input of the network, or equivalently
an element from the dataset and fX() is the distribution of
the input data. But for one speciﬁc x given ˆYf l = j, we
have:

Pr (cid:0)zi + qzi > zj + qzj

(cid:1) =

Pr (cid:0)(cid:12)

(cid:12)qzi − qzj

(cid:12)
(cid:12) > |zj − zi|(cid:1)

1
2

where the 1
2 term is due to the symmetry of the distribution
of the quantization noise around zero per output. By (7),
we can claim that

qzi − qzj =

(cid:88)

h∈A

qah

∂(zi − zj)
∂ah

(cid:88)

+

qwh

h∈W

∂(zi − zj)
∂wh

.

Lemma 2. Given BA and BW , pm,j→i is upper bounded
as follows:

(cid:34)

pm,j→i ≤ E

e−T ·V (cid:89)

sinh (T · DA,h)
T · DA,h

·

h∈A

sinh (T · DW,h)
T · DW,h

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

ˆYf l = j

(cid:89)

h∈W

(24)

where T =

DA,h = ∆A

(cid:80)

h∈A ∆2
2 · ∂(Zi−Zj )

∂Ah

3V
A,h+(cid:80)
h∈W ∆2
, and DW,h = ∆W
2

, V = Zj − Zi,
· ∂(Zi−Zj )
∂Wh

W,h

.

Proof. The setup is similar to that of Lemma 1. Denote
v = zj − zi. By the Chernoff bound,

Pr (cid:0)qzi − qzj > v(cid:1) ≤ e−tvE

(cid:104)

et(qzi −qzj )(cid:105)

Note that qzi − qzj is a zero mean random variable with the
following variance

(cid:104)

et(qzi −qzj )(cid:105)

E

=

(cid:89)

E

(cid:104)
etqah d(cid:48)

ah

(cid:105) (cid:89)

E

(cid:104)
etqwh d(cid:48)

wh

(cid:105)

h∈A

h∈W

for any t > 0. Because quantizations noise terms are inde-
pendent, by (21),

(21)

∆2
A
12

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

h∈A

∂(zi − zj)
∂ah

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

∆2
W
12

(cid:88)

h∈W

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂(zi − zj)
∂wh

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

By Chebyshev’s inequality, we obtain

Pr (cid:0)zi + qzi > zj + qzj

(cid:1)

(cid:80)

∆2
A

h∈A

∂(zi−zj )
∂ah

(cid:12)
(cid:12)
(cid:12)

≤

(cid:80)

2

(cid:12)
(cid:12)
(cid:12)

+ ∆2
W
24 |zi − zj|2

h∈W

(cid:12)
(cid:12)
(cid:12)

∂(zi−zj )
∂wh

2

(cid:12)
(cid:12)
(cid:12)

.

(22)

From (20) and (22), we can derive (18).

Plugging (18) of Lemma 1 into (15) and using (16),

pm ≤

M
(cid:88)

M
(cid:88)

j=1

i=1,i(cid:54)=j






∆2
A
24

E






(cid:80)

(cid:12)
(cid:12)
(cid:12)

h∈A

∂(Zi−Zj )
∂Ah
|Zi − Zj|2

2

(cid:12)
(cid:12)
(cid:12)






1 ˆYf l=j

+

∆2
W
24

E






(cid:80)

(cid:12)
(cid:12)
(cid:12)

h∈W

∂(Zi−Zj )
∂wh
|Zi − Zj|2

2

(cid:12)
(cid:12)
(cid:12)





1 ˆYf l=j







(23)

which can be simpliﬁed into (8) in Theorem 1.

Proof of Theorem 2

We start with the following lemma.

where d(cid:48)
ah
(cid:104)
etqah d(cid:48)
E

ah

= ∂(zi−zj )
(cid:105)

∂ah

is given by

and d(cid:48)

wh

= ∂(zi−zj )

∂wh

. Also,

(cid:104)

E

etqah d(cid:48)

ah

(cid:105)

=

etqah d(cid:48)

ah dqah

(cid:19)

(cid:18) td(cid:48)
∆A
ah
2

1
∆A

(cid:90) ∆A

2

− ∆A
2

=

=

sinh

ah

2
td(cid:48)
∆A
sinh (tdah )
tdah

d(cid:48)
∆A
ah
2

.

Similarly, E
d(cid:48)
∆W
wh
2

.

where dwh =

where dah =
sinh (tdwh)
tdwh
Hence,

(cid:104)

etqwh d(cid:48)

wh

(cid:105)

=

Pr (cid:0)qzi − qzj > v(cid:1)
≤ e−tv (cid:89)

sinh (tda,h)
tda,h

(cid:89)

h∈W

sinh (tdw,h)
tdw,h

.

h∈A

(25)

By taking logarithms, the right-hand-side is given by

−tv +

(cid:0) ln sinh (tda,h) − ln (tda,h) (cid:1)

+

(cid:0) ln sinh (tdw,h) − ln (tdw,h) (cid:1).

(cid:88)

h∈A
(cid:88)

h∈W

Analytical Guarantees on Numerical Precision of Deep Neural Networks

This term corresponds to a linear function of t added to a
sum of log-moment generating functions. It is hence con-
vex in t. By taking derivative with respective to t and set-
ting to zero,

v +

|A| + |W|
t

(cid:88)

=

h∈A

da,h
tanh(tda,h)

(cid:88)

+

h∈W

dw,h
tanh(tdw,h)

.

But tanh(x) = x − 1
terms yields:

3 x3 + o (cid:0)x5(cid:1), so dropping ﬁfth order

v+

|A| + |W|
t
(cid:88)

=

1
t(1 − (tda,h)2

)

3

(cid:88)

+

h∈W

1
t(1 − (tdw,h)2

.

)

3

h∈A

Note, for the terms inside the summations, we divided nu-
merator and denominator by da,h and dw,h, respectively,
then factored the denominator by t. Now, me multiply both
sides by t to get:

tv+ |A| + |W| =
1
1 − (tda,h)2

(cid:88)

h∈A

3

(cid:88)

+

h∈W

1
1 − (tdw,h)2

3

.

1

1−x2 = 1 + x2 + o(x4), so we drop fourth order

Also
terms:

tv + |A| + |W|
(cid:18)

(cid:88)

1 +

=

h∈A

(tda,h)2
3

(cid:19)

+

(cid:18)

(cid:88)

1 +

(cid:19)

(tdw,h)2
3

h∈W

which yields:

t =

(cid:80)

h∈A (da,h)2 + (cid:80)

h∈W (dw,h)2

(26)

3v

By plugging (25) into (26) and using the similar method of
Lemma 1, we can derive (24) of Lemma 2.

Theorem 2 is obtained by plugging (24) of Lemma 2 into
(15) and using (16). Of course, D(i,j)
is the random vari-
Ah
able of da,h when ˆyf x = i and ˆyf l = j, and the same ap-
plies to D(i,j)
wh and dw,h. We dropped the superscript (i, j)
in the Lemma as it was not needed for the consistency of
the deﬁnitions.

