Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear
Regression under RIP

Satyen Kale 1 Zohar Karnin 2 Tengyuan Liang 3 D´avid P´al 4

Abstract

Online sparse linear regression is an online prob-
lem where an algorithm repeatedly chooses a
subset of coordinates to observe in an adversar-
ially chosen feature vector, makes a real-valued
prediction, receives the true label, and incurs the
squared loss. The goal is to design an online
learning algorithm with sublinear regret to the
best sparse linear predictor in hindsight. With-
out any assumptions, this problem is known to
be computationally intractable. In this paper, we
make the assumption that data matrix satisﬁes re-
stricted isometry property, and show that this as-
sumption leads to computationally efﬁcient algo-
rithms with sublinear regret for two variants of
the problem. In the ﬁrst variant, the true label is
generated according to a sparse linear model with
additive Gaussian noise. In the second, the true
label is chosen adversarially.

1. Introduction

In modern real-world sequential prediction problems, sam-
ples are typically high dimensional, and construction of
the features may itself be a computationally intensive task.
Therefore in sequential prediction, due to the computation
and resource constraints, it is preferable to design algo-
rithms that compute only a limited number of features for
each new data example. One example of this situation,
from (Cesa-Bianchi et al., 2011), is medical diagnosis of a
disease, in which each feature is the result of a medical test
on the patient. Since it is undesirable to subject a patient to

1Google Research, New York.

2Amazon, New York.
3University of Chicago, Booth School of Business, Chicago.
4Yahoo Research, New York. Work done while the au-
Correspon-
thors were at Yahoo Research, New York.
Satyen Kale <satyenkale@google.com>, Zo-
dence to:
har Karnin <zkarnin@gmail.com>,
Liang
<Tengyuan.Liang@chicagobooth.edu>,
P´al
<dpal@yahoo-inc.com>.

Tengyuan
D´avid

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

a battery of medical tests, we would like to adaptively de-
sign diagnostic procedures that rely on only a few, highly
informative tests.

Online sparse linear regression (OSLR) is a sequential pre-
diction problem in which an algorithm is allowed to see
only a small subset of coordinates of each feature vector.
The problem is parameterized by 3 positive integers: d,
the dimension of the feature vectors, k, the sparsity of the
linear regressors we compare the algorithm’s performance
to, and k0, a budget on the number of features that can be
queried in each round by the algorithm. Generally we have
k (cid:28) d and k0 ≥ k but not signiﬁcantly larger (our algo-
rithms need1 k0 = ˜O(k)).

In the OSLR problem, the algorithm makes predictions
In each round t, nature
over a sequence of T rounds.
chooses a feature vector xt ∈ Rd, the algorithm chooses
a subset of {1, 2, . . . , d} of size at most k(cid:48) and observes
the corresponding coordinates of the feature vector. It then
makes a prediction (cid:98)yt ∈ R based on the observed features,
observes the true label yt, and suffers loss (yt − (cid:98)yt)2. The
goal of the learner is to make the cumulative loss compara-
ble to that of the best k-sparse linear predictor w in hind-
sight. The performance of the online learner is measured
by the regret, which is deﬁned as the difference between
the two losses:

RegretT =

(yt − (cid:98)yt)2− min

w: (cid:107)w(cid:107)0≤k

(yt − (cid:104)xt, w(cid:105))2 .

T
(cid:88)

t=1

T
(cid:88)

t=1

The goal is to construct algorithms that enjoy regret that is
sub-linear in T , the total number of rounds. A sub-linear
regret implies that in the asymptotic sense, the average per-
round loss of the algorithm approaches the average per-
round loss of the best k-sparse linear predictor.

Sparse regression is in general a computationally hard
In particular, given k, x1, x2, . . . , xT and
problem.
y1, y2, . . . , yT as inputs, the ofﬂine problem of ﬁnding a
k-sparse w that minimizes the error (cid:80)T
t=1(yt − (cid:104)xt, w(cid:105))2
does not admit a polynomial time algorithm under standard
complexity assumptions (Foster et al., 2015). This hard-

1In this paper, we use the ˜O(·) notation to suppress factors that

are polylogarithmic in the natural parameters of the problem.

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

ness persists even under the assumption that there exists a
k-sparse w∗ such that yt = (cid:104)xt, w∗(cid:105) for all t. Furthermore,
the computational hardness is present even when the solu-
tion is required to be only (cid:101)O(k)-sparse solution and has to
minimize the error only approximately; see (Foster et al.,
2015) for details. The hardness result was extended to on-
line sparse regression by (Foster et al., 2016). They showed
that for all δ > 0 there exists no polynomial-time algorithm
with regret O(T 1−δ) unless N P ⊆ BP P .

Foster et al. (2016) posed the open question of what ad-
ditional assumptions can be made on the data to make the
problem tractable. In this paper, we answer this open ques-
tion by providing efﬁcient algorithms with sublinear regret
under the assumption that the matrix of feature vectors sat-
isﬁes the restricted isometry property (RIP) (Candes & Tao,
2005). It has been shown that if RIP holds and there exists
a sparse linear predictor w∗ such that yt = (cid:104)xt, w∗(cid:105) + ηt
where ηt is independent noise, the ofﬂine sparse linear
regression problem admits computationally efﬁcient algo-
rithms, e.g., (Candes & Tao, 2007). RIP and related Re-
stricted Eigenvalue Condition (Bickel et al., 2009) have
been widely used as a standard assumption for theoretical
analysis in the compressive sensing and sparse regression
literature, in the ofﬂine case.
In the online setting, it is
natural to ask whether sparse regression avoids the com-
putational difﬁculty under an appropriate form of the RIP
condition. In this paper, we answer this question in a pos-
itive way, both in the realizable setting and in the agnostic
setting. As a by-product, we resolve the adaptive feature
selection problem as the efﬁcient algorithms we propose in
this paper adaptively choose a different “sparse” subset of
features to query at each round. This is closely related to
attribute-efﬁcient learning (see discussion in Section 1.2)
and online model selection.

1.1. Summary of Results

for

two models

regression for

We design polynomial-time algorithms for online sparse
the sequence
linear
(x1, y1), (x2, y2), . . . , (xT , yT ). The ﬁrst model is called
the realizable and the second is called agnostic. In both
models, we assume that, after proper normalization, for all
large enough t, the matrix Xt formed from the ﬁrst t feature
vectors x1, x2, . . . , xt satisﬁes the restricted isometry prop-
erty. The two models differ in the assumptions on yt. The
realizable model assumes that yt = (cid:104)xt, w∗(cid:105) + ηt where
w∗ is k-sparse and ηt is an independent noise. In the ag-
nostic model, yt can be arbitrary, and therefore, the regret
bounds we obtain are worse than in the realizable setting.
The models and corresponding algorithms are presented in
Sections 2 and 3 respectively. Interestingly enough, the al-
gorithms and their corresponding analyses are completely
different in the realizable and agnostic case.

Our algorithms allow for somewhat more ﬂexibility than
the problem deﬁnition: they are designed to work with a
budget k0 on the number of features that can be queried that
may be larger than the sparsity parameter k of the compara-
tor. The regret bounds we derive improve with increasing
values of k0. In the case when k0 ≈ k, the dependence on
d in the regret bounds is polynomial, as can be expected
in limited feedback settings (this is analogous to polyno-
mial dependence on d in bandit settings). In the extreme
case when k0 = d, i.e. we have access to all the features,
the dependence on the dimension d in the regret bounds we
prove is only logarithmic. The interpretation is that if we
have full access to the features, but the goal is to compete
with just k sparse linear regressors, then the number of data
points that need to be seen to achieve good predictive accu-
racy has only logarithmic dependence on d. This is analo-
gous to the (ofﬂine) compressed sensing setting where the
sample complexity bounds, under RIP, only depend loga-
rithmically on d.

A major building block in the solution for the realizable set-
ting (Section 2) consists of identifying the best k-sparse lin-
ear predictor for the past data at any round in the prediction
problem. This is done by solving a sparse regression prob-
lem on the observed data. The solution of this problem can-
not be obtained by a simple application of say, the Dantzig
selector (Candes & Tao, 2007) since we do not observe the
data matrix X, but rather a subsample of its entries. Our
algorithm is a variant of the Dantzig selector that incorpo-
rates random sampling into the optimization, and computes
a near-optimal solution by solving a linear program. The
resulting algorithm has a regret bound of (cid:101)O(log T ). This
bound has optimal dependence on T , since even in the full
information setting where all features are observed there is
a lower bound of Ω(log T ) (Hazan & Kale, 2014).

The algorithm for the agnostic setting relies on the theory
of submodular optimization. The analysis in (Boutsidis
et al., 2015) shows that the RIP assumption implies that
the set function deﬁned as the minimum loss achievable
by a linear regressor restricted to the set in question satis-
ﬁes a property called weak supermodularity. Weak super-
modularity is a relaxation of standard supermodularity that
is still strong enough to show performance bounds for the
standard greedy feature selection algorithm for solving the
sparse regression problem. We then employ a technique
developed by Streeter & Golovin (2008) to construct an
online learning algorithm that mimics the greedy feature
selection algorithm. The resulting algorithm has a regret
bound of (cid:101)O(T 2/3). It is unclear if this bound has the op-
timal dependence on T : it is easy to prove a lower bound
T ) on the regret using standard arguments for the
of Ω(
multiarmed bandit problem.

√

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

1.2. Related work

without RIP (Foster et al., 2015; 2016).

A related setting is attribute-efﬁcient
learning (Cesa-
Bianchi et al., 2011; Hazan & Koren, 2012; Kukliansky &
Shamir, 2015). This is a batch learning problem in which
the examples are generated i.i.d., and the goal is to simply
output a linear regressor using only a limited number of
features per example with bounded excess risk compared
to the optimal linear regressor, when given full access to
the features at test time. Since the goal is not prediction
but simply computing the optimal linear regressor, efﬁcient
algorithms exist and have been developed by the aforemen-
tioned papers.

Without any assumptions, only inefﬁcient algorithms for
the online sparse linear regression problem are known Zol-
ghadr et al. (2013); Foster et al. (2016). Kale (2014) posed
the open question of whether it is possible to design an ef-
ﬁcient algorithm for the problem with a sublinear regret
bound. This question was answered in the negative by Fos-
ter et al. (2016), who showed that efﬁciency can only be
obtained under additional assumptions on the data. This
paper shows that the RIP assumption yields tractability in
the online setting just as it does in the batch setting.

In the realizable setting, the linear program at the heart of
the algorithm is motivated from Dantzig selection (Candes
& Tao, 2007) and error-in-variable regression (Rosenbaum
& Tsybakov, 2010; Belloni et al., 2016). The problem of
ﬁnding the best sparse linear predictor when only a sample
of the entries in the data matrix is available is also discussed
by Belloni et al. (2016) (see also the references therein). In
fact, these papers solve a more general problem where we
observe a matrix Z rather than X that is an unbiased esti-
mator of X. While we can use their results in a black-box
manner, they are tailored for the setting where the variance
of each Zij is constant and it is difﬁcult to obtain the exact
dependence on this variance in their bounds. In our setting,
this variance can be linear in the dimension of the feature
vectors, and hence we wish to control the dependence on
the variance in the bounds. Thus, we use an algorithm that
is similar to the one in (Belloni et al., 2016), and provide
an analysis for it (in the supplementary material). As an
added bonus, our algorithm results in solving a linear pro-
gram rather than a conic or general convex program, hence
admits a solution that is more computationally efﬁcient.

In the agnostic setting, the computationally efﬁcient algo-
rithm we propose is motivated from (online) supermodu-
lar optimization (Natarajan, 1995; Boutsidis et al., 2015;
Streeter & Golovin, 2008). The algorithm is computation-
ally efﬁcient and enjoys sublinear regret under an RIP-like
condition, as we will show in Section 3. This result can
be contrasted with the known computationally prohibitive
algorithms for online sparse linear regression (Zolghadr
et al., 2013; Foster et al., 2016), and the hardness result

1.3. Notation and Preliminaries

For d ∈ N, we denote by [d] the set {1, 2, . . . , d}. For a
vector in x ∈ Rd, denote by x(i) its i-th coordinate. For
a subset S ⊆ [d], we use the notation RS to indicate the
vector space spanned by the coordinate axes indexed by S
(i.e. the set of all vectors w supported on the set S). For a
vector x ∈ Rd, denote by x(S) ∈ Rd the projection of x
on RS. That is, the coordinates of x(S) are

x(S)(i) =

(cid:40)

x(i)
0

if i ∈ S,
if i (cid:54)∈ S,

for i = 1, 2, . . . , d.

i u(i) · v(i) be the inner product of vectors

Let (cid:104)u, v(cid:105) = (cid:80)
u and v.
For p ∈ [0, ∞], the (cid:96)p-norm of a vector x ∈ Rd is denoted
by (cid:107)x(cid:107)p. For p ∈ (0, ∞), (cid:107)x(cid:107)p = ((cid:80)
i |xi|p)1/p, (cid:107)x(cid:107)∞ =
maxi |xi|, and (cid:107)x(cid:107)0 is the number of non-zero coordinates
of x.

The following deﬁnition will play a key role:

Deﬁnition 1 (Restricted Isometry Property (Candes & Tao,
2007)). Let (cid:15) ∈ (0, 1) and k ≥ 0. We say that a matrix
X ∈ Rn×d satisﬁes restricted isometry property (RIP) with
parameters ((cid:15), k) if for any w ∈ Rd with (cid:107)w(cid:107)0 ≤ k we
have

(1 − (cid:15)) (cid:107)w(cid:107)2 ≤

(cid:107)Xw(cid:107)2 ≤ (1 + (cid:15)) (cid:107)w(cid:107)2 .

1
√
n

One can show that RIP holds with overwhelming probabil-
ity if n = Ω((cid:15)−2k log(ed/k)) and each row of the matrix
is sampled independently from an isotropic sub-Gaussian
distribution. In the realizable setting, the sub-Gaussian as-
sumption can be relaxed to incorporate heavy tail distri-
bution via the “small ball” analysis introduced in Mendel-
son (2014), since we only require one-sided lower isometry
property.

1.4. Proper Online Sparse Linear Regression

We introduce a variant of online sparse regression (OSLR),
which we call proper online sparse linear regression
(POSLR). The adjective “proper” is to indicate that the al-
gorithm is required to output a weight vector in each round
and its prediction is computed by taking an inner product
with the feature vector.

We assume that
there is an underlying sequence
(x1, y1), (x2, y2), . . . , (xT , yT ) of labeled examples in
Rd × R. In each round t = 1, 2, . . . , T , the algorithm be-
haves according to the following protocol:

1. Choose a vector wt ∈ Rd such that (cid:107)wt(cid:107)0 ≤ k.

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

2. Choose St ⊆ [d] of size at most k0.

2.1. Algorithm

3. Observe xt(St) and yt, and incur loss (yt −(cid:104)xt, wt(cid:105))2.

the algorithm makes the prediction (cid:98)yt

:=
Essentially,
(cid:104)xt, wt(cid:105) in round t. The regret after T rounds of an al-
gorithm with respect to w ∈ Rd is

RegretT (w) =

(yt − (cid:104)xt, wt(cid:105))2−

(yt − (cid:104)xt, w(cid:105))2 .

T
(cid:88)

t=1

T
(cid:88)

t=1

The regret after T rounds of an algorithm with respect to
the best k-sparse linear regressor is deﬁned as

RegretT = max

RegretT (w) .

w: (cid:107)w(cid:107)0≤k

Note that any algorithm for POSLR gives rise to an al-
gorithm for OSLR. Namely, if an algorithm for POSLR
chooses wt and St, the corresponding algorithm for OSLR
queries the coordinates St ∪ {i
: wt(i) (cid:54)= 0}. The algo-
rithm for OSLR queries at most k0 + k coordinates and has
the same regret as the algorithm for POSLR.

Additionally, POSLR allows parameters settings which do
not have corresponding counterparts in OSLR. Namely, we
can consider the sparse “full information” setting where
k0 = d and k (cid:28) d.

rows of Xt are xT

We denote by Xt the t × d matrix of ﬁrst t unlabeled sam-
1 , xT
t . Similarly, we
ples i.e.
denote by Yt ∈ Rt the vector of ﬁrst t labels y1, y2, . . . , yt.
We use the shorthand notation X, Y for XT and YT respec-
tively.

2 , . . . , xT

In order to get computationally efﬁcient algorithms, we as-
sume that that for all t ≥ t0, the matrix Xt satisﬁes the
restricted isometry condition. The parameter t0 and RIP
parameters k, (cid:15) will be speciﬁed later.

2. Realizable Model

In this section we design an algorithm for POSLR for the
realizable model. In this setting we assume that there is a
vector w∗ ∈ Rd such that (cid:107)w∗(cid:107)0 ≤ k and the sequence
of labels y1, y2, . . . , yT is generated according to the linear
model

The algorithm maintains an unbiased estimate (cid:98)Xt of the
matrix Xt. The rows of (cid:98)Xt are vectors (cid:98)xT
2 , . . . , (cid:98)xT
t
which are unbiased estimates of xT
t . To con-
struct the estimates, in each round t, the set St ⊆ [d] is
chosen uniformly at random from the collection of all sub-
sets of [d] of size k0. The estimate is

1 , (cid:98)xT
2 , . . . , xT

1 , xT

(cid:98)xt =

d
k0

· xt(St).

(2)

(3)

To compute the predictions of the algorithm, we consider
the linear program

minimize (cid:107)w(cid:107)1 s.t.

Yt − (cid:98)Xtw

+

(cid:98)Dtw

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

(cid:98)X T
t

(cid:115)

≤ C

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

1
t

(cid:17)

(cid:18)

d log(td/δ)
tk0

σ +

(cid:19)

.

d
k0

Here, C > 0 is a universal constant, and δ ∈ (0, 1) is the
allowed failure probability. (cid:98)Dt, deﬁned in equation (5), is
a diagonal matrix that offsets the bias on the diag( (cid:98)X T
t (cid:98)Xt).

The linear program (3) is called the Dantzig selector. We
denote its optimal solution by (cid:98)wt+1. (We deﬁne (cid:98)w1 = 0.)
Based on (cid:98)wt, we construct (cid:101)wt ∈ Rd. Let | (cid:98)wt(i1)| ≥
| (cid:98)wt(i2)| ≥ · · · ≥ | (cid:98)wt(id)| be the coordinates sorted ac-
cording to the their absolute value, breaking ties according
to their index. Let (cid:101)St = {i1, i2, . . . , ik} be the top k coor-
dinates. We deﬁne (cid:101)wt as

(cid:101)wt = (cid:98)wt( (cid:101)St).
The actual prediction wt is either zero if t ≤ t0 or (cid:101)ws for
some s ≤ t and it gets updated whenever t is a power of 2.

(4)

The algorithm queries at most k + k0 features each round,
and the linear program can be solved in polynomial time
using simplex method or interior point method. The al-
gorithm solves the linear program only (cid:100)log2 T (cid:101) times by
using the same vector in the rounds 2s, . . . , 2s+1 − 1. This
lazy update improves both the computational aspects of the
algorithm and the regret bound.

2.2. Main Result

yt = (cid:104)xt, w∗(cid:105) + ηt ,

(1)

The main result in this section provides a logarithmic regret
bound under the following assumptions 2

where η1, η2, . . . , ηT are independent random variables
from N (0, σ2). We assume that the standard deviation σ,
or an upper bound of it, is given to the algorithm as input.
We assume that (cid:107)w∗(cid:107)1 ≤ 1 and (cid:107)xt(cid:107)∞ ≤ 1 for all t.
For convenience, we use η to denote the vector
(η1, η2, . . . , ηT ) of noise variables.

• The feature vectors have the property that for any
t ≥ t0, the matrix Xt satisﬁes the RIP condition with
( 1
5 , 3k), with t0 = O(k log(d) log(T )).

2A more precise statement with the exact dependence on the
problem parameters can be found in the supplementary material.

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

Predict wt = 0

else if t is a power of 2 then

Let (cid:98)wt be the solution of linear program (3)
Compute (cid:101)wt according to (4)
Predict wt = (cid:101)wt

Algorithm 1 Dantzig Selector for POSLR
Require: T , σ, t0, k, k0
1: for t = 1, 2, . . . , T do
if t ≤ t0 then
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: end for

end if
Let St ⊆ [d] be a random subset of size k0
Observe xt(St) and yt
Construct estimate (cid:98)xt according to (2)
Append (cid:98)xT

t to (cid:98)Xt−1 to form (cid:98)Xt ∈ Rt×d

Predict wt = wt−1

else

• The underlying POSLR online prediction problem has
a sparsity budget of k and observation budget k0.

• The model is realizable as deﬁned in equation (1) with
i.i.d unbiased Gaussian noise with standard deviation
σ = O(1).

Theorem 2. For any δ > 0, with probability at least 1 − δ,
Algorithm 1 satisﬁes

RegretT = O (cid:0)k2 log(d/δ)(d/k0)3 log(T )(cid:1) .

The theorem asserts that an O(log T ) regret bound is ef-
ﬁciently achievable in the realizable setting. Furthermore
when k0 = Ω(d) the regret scales as log(d) meaning that
we do not necessarily require T ≥ d to obtain a meaningful
result. We note that the complete expression for arbitrary
t0, σ is given in (13) in the supplementary material.

The algorithm can be easily understood via the error-in-
variable equation

yt = (cid:104)xt, w∗(cid:105) + ηt ,
(cid:98)xt = xt + ξt.

with E[ξt] = E[(cid:98)xt − xt] = 0, where the expectation is
taken over random sampling introduced by the algorithm
when performing feature exploration. The learner observes
yt as well as the “noisy” feature vector (cid:98)xt, and aims to
recover w∗.

t Xt. By taking (cid:98)X T

As mentioned above, we (implicitly) need an unbiased es-
timator of X T
t (cid:98)Xt it is easy to verify
that the off-diagonal entries are indeed unbiased however
this is not the case for the diagonal. To this end we deﬁne
Dt ∈ Rd×d as the diagonal matrix compensating for the

sampling bias on the diagonal elements of (cid:98)X T

t (cid:98)Xt

Dt =

(cid:19)

− 1

(cid:18) d
k0

· diag (cid:0)X T

t Xt

(cid:1)

and the estimated bias from the observed data is

(cid:18)

(cid:98)Dt =

1 −

(cid:19)

k0
d

· diag

(cid:16)

(cid:98)X T

t (cid:98)Xt

(cid:17)

.

(5)

Therefore, program (1) can be viewed as Dantzig selector
with plug-in unbiased estimates for X T
t Xt using
limited observed features.

t Yt and X T

2.3. Sketch of Proof

The main building block in proving Theorem 2 is stated in
Lemma 3. It proves that the sequence of solutions (cid:98)wt con-
verges to the optimal response w∗ based on which the sig-
nal yt is created. More accurately, ignoring all second order
terms, it shows that (cid:107) (cid:98)wt − w∗(cid:107)1 ≤ O(1/
t). In Lemma 4
we show that the same applies for the sparse approxima-
tion wt of (cid:98)wt. Now, since (cid:107)xt(cid:107)∞ ≤ 1 we get that the
difference between our response (cid:104)xt, wt(cid:105) and the (almost)
optimal response (cid:104)xt, w∗(cid:105) is bounded by 1/
t. Given this,
a careful calculation of the difference of losses leads to a
regret bound w.r.t. w∗. Speciﬁcally, an elementary analysis
of the loss expression leads to the equality

√

√

RegretT (w∗) =

2ηt (cid:104)xt, w∗ − wt(cid:105) + ((cid:104)xt, w∗ − wt(cid:105))2

T
(cid:88)

t=1

A bound on both summands can clearly be expressed in
terms of | (cid:104)xt, w∗ − wt(cid:105) | = O(1/
t). The right summand
requires a martingale concentration bound and the left is
trivial. For both we obtain a bound of O(log(T )).

√

We are now left with two technicalities. The ﬁrst is that
w∗ is not necessarily the empirically optimal response. To
this end we provide, in Lemma 16 in the supplementary
material, a constant (independent of T ) bound on the regret
of w∗ compared to the empirical optimum. The second
technicality is the fact that we do not solve for (cid:98)wt in every
round, but in exponential gaps. This translates to an added
factor of 2 to the bound (cid:107)wt − w∗(cid:107)1 that affects only the
constants in the O(·) terms.

Lemma 3 (Estimation Rates). Assume that the matrix
Xt ∈ Rt×d satisﬁes the RIP condition with ((cid:15), 3k) for some
(cid:15) < 1/5. Let (cid:98)wn+1 ∈ Rd be the optimal solution of pro-
gram (3). With probability at least 1 − δ,

(cid:107) (cid:98)wt+1 − w∗(cid:107)2 ≤ C ·

(cid:107) (cid:98)wt+1 − w∗(cid:107)1 ≤ C ·

(cid:115)

d
k0
(cid:115)

·

k log(d/δ)
t

(cid:18)

σ +

(cid:19)

d
k0

d
k0

k2 log(d/δ)
t

(cid:18)

σ +

(cid:19)

d
k0

,

.

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

Here C > 0 is some universal constant and σ is the stan-
dard deviation of the noise.

Note the (cid:98)wt may not be sparse; it can have many non-zero
coordinates that are small in absolute value. However, we
take the top k coordinates of (cid:98)wt in absolute value. Thanks
3.
to the Lemma 4 below, we lose only a constant factor
Lemma 4. Let (cid:98)w ∈ Rd be an arbitrary vector and let
w∗ ∈ Rd be a k-sparse vector. Let (cid:101)S ⊆ [d] be the top
k coordinates of (cid:98)w in absolute value. Then,

√

(cid:13)
(cid:13)

(cid:13) (cid:98)w( (cid:101)S) − w∗(cid:13)
(cid:13)
(cid:13)2

≤

√

3 (cid:107) (cid:98)w − w∗(cid:107)2 .

3. Agnostic Setting

In this section we focus on the agnostic setting, where
we don’t impose any distributional assumption on the se-
quence. In this setting, there is no “true” sparse model, but
the learner — with limited access to features — is compet-
ing with the best k-sparse model deﬁned using full infor-
mation {(xt, yt)}T

t=1.

As before, we do assume that xt and yt are bounded. With-
out loss of generality, (cid:107)xt(cid:107)∞ ≤ 1, and |yt| ≤ 1 for all
t. Once again, without any regularity condition on the de-
sign matrix, Foster et al. (2016) have shown that achieving
a sub-linear regret O(T 1−δ) is in general computationally
hard, for any constant δ > 0 unless NP ⊆ BPP.

We give an efﬁcient algorithm that achieves sub-linear re-
gret under the assumption that the design matrix of any
(sufﬁciently long) block of consecutive data points has
bounded restricted condition number, which we deﬁne be-
low:
Deﬁnition 5 (Restricted Condition Number). Let k ∈ N be
a sparsity parameter. The restricted condition number for
sparsity k of a matrix X ∈ Rn×d is deﬁned as

sup
v,w: (cid:107)v(cid:107)=(cid:107)w(cid:107)=1,
(cid:107)v(cid:107)0,(cid:107)w(cid:107)0≤k

(cid:107)Xv(cid:107)
(cid:107)Xw(cid:107)

.

Note that in the random design setting where xt, for t ∈
[T ], are isotropic sub-Gaussian vectors, t0 = O(log T +
k log d) sufﬁces to satisfy BBRCNP with high probability,
where the O(·) notation hides a constant depending on κ.

We assume in this section that the sequence of feature vec-
tors satisﬁes BBRCNP with parameters (κ, K) for some
K = O(k log(T )) to be deﬁned in the course of the analy-
sis.

3.1. Algorithm

The algorithm in the agnostic setting is of distinct nature
from that in the stochastic setting. Our algorithm is mo-
tivated from literature on maximization of sub-modular
set function (Natarajan, 1995; Streeter & Golovin, 2008;
Boutsidis et al., 2015). Though the problem being NP-
hard, greedy algorithm on sub-modular maximization pro-
vides provable good approximation ratio. Speciﬁcally,
(Streeter & Golovin, 2008) considered online optimization
of super/sub-modular set functions using expert algorithm
as sub-routine.
(Natarajan, 1995; Boutsidis et al., 2015)
cast the sparse linear regression as maximization of weakly
supermodular function. We will introduce an algorithm that
blends various ideas from referred literature, to attack the
online sparse regression with limited features.

First, let’s introduce the notion of a weakly supermodular
function.
Deﬁnition 7. For parameters k ∈ N and α ≥ 1, a set
function g : [d] → R is (k, α)-weakly supermodular if for
any two sets S ⊆ T ⊆ [d] with |T | ≤ k, the following two
inequalities hold:

1. (monotonicity) g(T ) ≤ g(S), and

2. (approximately decreasing marginal gain)

g(S) − g(T ) ≤ α

[g(S) − g(S ∪ {i})].

(cid:88)

i∈T \S

It is easy to see that if a matrix X satisﬁes RIP with param-
eters ((cid:15), k), then its restricted condition number for sparsity
k is at most 1+(cid:15)
1−(cid:15) . Thus, having bounded restricted condition
number is a weaker requirement than RIP.

We now deﬁne the Block Bounded Restricted Condition
Number Property (BBRCNP):
Deﬁnition 6 (Block Bounded Restricted Condition Num-
ber Property). Let κ > 0 and k ∈ N. A sequence of feature
vectors x1, x2, . . . , xT satisﬁes BBRCNP with parameters
(κ, K) if there is a constant t0 such that for any sequence
of consecutive time steps T with |T | ≥ t0, the restricted
condition number for sparsity k of X, the design matrix of
the feature vectors xt for t ∈ T , is at most κ.

The deﬁnition is slightly stronger than that in (Boutsidis
et al., 2015). We will show that sparse linear regression can
be viewed as weakly supermodular minimization in Deﬁni-
tion 7 once the design matrix has bounded restricted condi-
tion number.

Now we outline the algorithm (see Algorithm 2). We divide
the rounds 1, 2, . . . , T into mini-batches of size B each (so
there are T /B such batches). The b-th batch thus consists
of the examples (xt, yt) for t ∈ Tb := {(b − 1)B + 1, (b −
1)B + 1, . . . , bB}. Within the b-th batch, our algorithm
queries the same subset of features of size at most k0.

The algorithm consists of few key steps. First, one can
show that under BBRCNP, as long as B is large enough,

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

the loss within batch b deﬁnes a weakly supermodular set
function

Algorithm 2 Online Greedy Algorithm for POSLR
Require: Mini-batch size B, sparsity parameters k0 and

gt(S) =

1
B

inf
w∈RS

(cid:88)

t∈Tb

(yt − (cid:104)xt, w(cid:105))2.

Therefore, we can formulate the original online sparse re-
gression problem into online weakly supermodular min-
imization problem. For the latter problem, we develop
an online greedy algorithm along the lines of (Streeter &
Golovin, 2008). We employ k1 = O∗(k) budgeted experts
algorithms (Amin et al., 2015), denoted BEXP, with bud-
get parameter3 k0
. The precise characteristics of BEXP
k1
are given in Theorem 8 (adapted from Theorem 2 in (Amin
et al., 2015)).
Theorem 8. For the problem of prediction from expert ad-
vice, let there be d experts, and let k ∈ [d] be a budget
parameter. In each prediction round t, the BEXP algorithm
chooses an expert jt and a set of experts Ut containing jt
of size at most k, obtains as feedback the losses of all the
experts in Ut, suffers the loss of expert jt, and guarantees
T over T prediction
an expected regret bound of 2
rounds.

(cid:113) d log(d)
k

At the beginning of each mini-batch b, the BEXP algo-
rithms are run. Each BEXP algorithm outputs a set of
coordinates of size k0
as well as a special coordinate in
k1
that set. The union of all of these sets is then used as the
set of features to query throughout the subsequent mini-
batch. Within the mini-batch, the algorithm runs the stan-
dard Vovk-Azoury-Warmuth algorithm for linear predic-
tion with square loss restricted to set of special coordinates
output by all the BEXP algorithms.

At the end of the mini-batch, every BEXP algorithm is pro-
vided carefully constructed losses for each coordinate that
was output as feedback. These losses ensure that the set of
special coordinates chosen by the BEXP algorithms mimic
the greedy algorithm for weakly supermodular minimiza-
tion.

3.2. Main Result

In this section, we will show that Algorithm 2 achieves sub-
linear regret under BBRCNP.
Theorem 9. Suppose the sequence of feature vectors sat-
isﬁes BBRCNP with parameters (κ, k1 + k) for k1 =
1
3 κ2k log(T ), and assume that T is large enough so that
t0 ≤ ( k0T
κ2dk )1/3. Then if Algorithm 2 is run with parame-
ters B = ( k0T
κ2dk )1/3 and k1 as speciﬁed above, its expected
regret is at most ˜O(( κ8dk4
k0

)1/3T 2/3).

Proof. The proof relies on a number of lemmas whose

3We assume, for convenience, that k0 is divisible by k1.

k1

1: Set up k1 budgeted prediction algorithms BEXP(i) for
i ∈ [k1], each using the coordinates in [d] as “experts”
with a per-round budget of k0
k1

.

2: for b = 1, 2, . . . , T /B do
3:

b

b and subset
b ∈

from BEXP(i) such that j(i)

For each i ∈ [k1], obtain a coordinate j(i)
of coordinates U (i)
U (i)
.
b
Deﬁne V (0)
{j(i(cid:48))
Set up the Vovk-Azoury-Warmuth (VAW) algorithm
for predicting using the features in V (k1)
for t ∈ Tb do

b = ∅ and for each i ∈ [k1] deﬁne V (i)

| i(cid:48) ≤ i}.

b =

.

b

b

b

, obtain xt(St), and pass

i∈[k1] U (i)

Set St = (cid:83)
xt(V (k1)
b
Set wt to be the weight vector output by VAW.
Obtain the true label yt and pass it to VAW.

) to VAW.

end for
Deﬁne the function

gb(S) =

1
B

inf
w∈RS

(cid:88)

t∈Tb

(yt − (cid:104)xt, w(cid:105))2.

(6)

For each j ∈ U (i)
, compute gb(V (i−1)
pass it BEXP(i) as the loss for expert j.

b

b

∪ {j}) and

4:

5:

6:
7:

8:
9:
10:
11:

12:

13: end for

proofs can be found in the supplementary material. We be-
gin with the connection between sparse linear regression,
weakly supermodular function and RIP, formally stated in
Lemma 10. This lemma is a direct consequence of Lemma
5 in (Boutsidis et al., 2015).

Lemma 10. Consider a sequence of examples (xt, yt) ∈
Rd × R for t = 1, 2, . . . , B, and let X be the design matrix
for the sequence. Consider the set function associated with
least squares optimization:

g(S) = inf
w∈RS

1
B

B
(cid:88)

t=1

(yt − (cid:104)xt, w(cid:105))2.

Suppose the restricted condition number of X for sparsity
k is bounded by κ. Then g(S) is (k, κ2)-weakly supermod-
ular.

Even though minimization of weakly supermodular func-
tions is NP-hard, the greedy algorithm provides a good ap-
proximation, as shown in the next lemma.

Lemma 11. Consider a (k, α)-weakly supermodular set
function g(·). Let j∗ := arg minj g({j}). Then, for any

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

subset V of size at most k, we have

4. Conclusions and Future Work

g({j∗}) − g(V ) ≤

(cid:16)

1 − 1

α|V |

(cid:17)

[g(∅) − g(V )].

The BEXP algorithms essentially implement the greedy al-
gorithm in an online fashion. Using the properties of the
BEXP algorithm, we have the following regret guarantee:

Lemma 12. Suppose the sequence of feature vectors satis-
ﬁes BBRCNP with parameters ((cid:15), k1 + k). Then for any set
V of coordinates of size at most k, we have

gb(V (k1)
b

) − gb(V )







T /B
(cid:88)

E



b=1

T /B
(cid:88)

(cid:16)

b=1

≤

(cid:17)k1

1 − 1

κ2|V |

[gb(∅) − gb(V )] + 2κ2k

(cid:113) dk1 log(d)T
k0B

.

Finally, within every mini-batch, the VAW algorithm guar-
antees the following regret bound, an immediate conse-
quence of Theorem 11.8 in Cesa-Bianchi & Lugosi (2006):

Lemma 13. Within every batch b, the VAW algorithm gen-
erates weight vectors wt for t ∈ Tb such that

(yt − (cid:104)xt, wt(cid:105))2 − Bgb(V (k1)

b

) ≤ O(k1 log(B)).

(cid:88)

t∈Tb

In this paper, we gave computationally efﬁcient algorithms
for the online sparse linear regression problem under the
assumption that the design matrices of the feature vectors
satisfy RIP-type properties. Since the problem is hard with-
out any assumptions, our work is the ﬁrst one to show that
assumptions that are similar to the ones used to sparse re-
covery in the batch setting yield tractability in the online
setting as well.

Several open questions remain in this line of work and will
be the basis for future work. Is it possible to improve the
regret bound in the agnostic setting? Can we give match-
ing lower bounds on the regret in various settings? Is it
possible to relax the RIP assumption on the design ma-
trices and still have efﬁcient algorithms? Some obvious
weakenings of the RIP assumption we have made don’t
yield tractability. For example, simply assuming that the
ﬁnal matrix XT satisﬁes RIP rather than every intermedi-
ate matrix Xt for large enough t is not sufﬁcient; a sim-
ple tweak to the lower bound construction of Foster et al.
(2016) shows this. This tweak consists of simply padding
the construction with enough dummy examples which are
well-conditioned enough to overcome the ill-conditioning
of the original construction so that RIP is satisﬁed by XT .
We note however that in the realizable setting, our analy-
sis can be easily adapted to work under weaker conditions
such as irrepresentability (Zhao & Yu, 2006; Javanmard &
Montanari, 2013).

We can now prove Theorem 9. Combining the bounds of
lemma 12 and 13, we conclude that for any subset of coor-
dinates V of size at most k, we have

References

(cid:34) T

(cid:88)

E

(yt − (cid:104)xt, wt(cid:105))2

(cid:35)

(7)

Amin, Kareem, Kale, Satyen, Tesauro, Gerald, and Turaga,
Deepak S. Budgeted prediction with expert advice. In
AAAI, pp. 2490–2496, 2015.

≤

Bgb(V ) + B(1 − 1

κ2|V | )k1[gb(∅) − gb(V )]

(8)

+ O

κ2k

(cid:113) dk1 log(d)BT
k0

(cid:19)

+

k1 log(B)

.

(9)

T
B

T
(cid:88)

t=1

Bgb(V ) ≤ inf
w∈RV

(yt − (cid:104)xt, w(cid:105))2,

t=1

T /B
(cid:88)

b=1
(cid:18)

T /B
(cid:88)

b=1

Finally, note that

and

T /B
(cid:88)

b=1

B(1 − 1

κ2|V | )k1[gb(∅) − gb(V )] ≤ T · exp(− k1

κ2k ),

because gb(∅) ≤ 1. Using these bounds in (9), and plug-
ging in the speciﬁed values of B and k1, we get the stated
regret bound.

Belloni, Alexandre, Rosenbaum, Mathieu, and Tsybakov,
Alexandre B. Linear and conic programming estimators
in high dimensional errors-in-variables models. Jour-
nal of the Royal Statistical Society: Series B (Statistical
Methodology), 2016. ISSN 1467-9868.

Bickel, Peter J, Ritov, Ya’acov, and Tsybakov, Alexan-
dre B. Simultaneous analysis of Lasso and Dantzig se-
lector. The Annals of Statistics, pp. 1705–1732, 2009.

Boutsidis, Christos, Liberty, Edo, and Sviridenko, Maxim.
Greedy minimization of weakly supermodular set func-
tions. arXiv preprint arXiv:1502.06528, 2015.

Candes, Emmanuel and Tao, Terence. The Dantzig selec-
tor: statistical estimation when p is much larger than n.
The Annals of Statistics, pp. 2313–2351, 2007.

Candes, Emmanuel J and Tao, Terence. Decoding by linear

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

programming. IEEE transactions on information theory,
51(12):4203–4215, 2005.

Cesa-Bianchi, Nicol`o and Lugosi, G´abor.

Prediction,
learning, and games. Cambridge University Press, 2006.

Cesa-Bianchi, Nicol`o, Shalev-Shwartz, Shai, and Shamir,
Ohad. Efﬁcient learning with partially observed at-
Journal of Machine Learning Research, 12
tributes.
(Oct):2857–2878, 2011.

Foster, Dean, Karloff, Howard, and Thaler, Justin. Variable

selection is hard. In COLT, pp. 696–709, 2015.

Foster, Dean, Kale, Satyen, and Karloff, Howard. Online

sparse linear regression. In COLT, 2016.

Hazan, Elad and Kale, Satyen.

Beyond the regret
minimization barrier: optimal algorithms for stochas-
tic strongly-convex optimization. Journal of Machine
Learning Research, 15(1):2489–2512, 2014.

Hazan, Elad and Koren, Tomer. Linear regression with lim-

ited observation. In ICML, 2012.

Javanmard, Adel and Montanari, Andrea. Model selection
for high-dimensional regression under the generalized
In NIPS, pp. 3012–3020,
irrepresentability condition.
2013.

Kale, Satyen. Open problem: Efﬁcient online sparse re-

gression. In COLT, pp. 1299–1301, 2014.

Kukliansky, Doron and Shamir, Ohad. Attribute efﬁcient
linear regression with distribution-dependent sampling.
In ICML, pp. 153–161, 2015.

Mendelson, Shahar. Learning without concentration.

In

COLT, pp. 25–39, 2014.

Natarajan, Balas Kausik. Sparse approximate solutions to
linear systems. SIAM journal on computing, 24(2):227–
234, 1995.

Rosenbaum, Mathieu and Tsybakov, Alexandre B. Sparse
recovery under matrix uncertainty. The Annals of Statis-
tics, 38(5):2620–2651, 2010.

Streeter, Matthew J. and Golovin, Daniel. An online algo-
In NIPS,

rithm for maximizing submodular functions.
pp. 1577–1584, 2008.

Zhao, Peng and Yu, Bin. On model selection consistency
of lasso. Journal of Machine learning research, 7(Nov):
2541–2563, 2006.

Zolghadr, Navid, Bart´ok, G´abor, Greiner, Russell, Gy¨orgy,
Andr´as, and Szepesv´ari, Csaba. Online learning with
In NIPS, pp. 1241–1249,
costly features and labels.
2013.

