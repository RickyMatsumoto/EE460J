Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Guangyong Chen 1 Shengyu Zhang 1 Di Lin 2 Hui Huang 2 Pheng Ann Heng 1 3

Abstract

While crowdsourcing has been a cost and time
efﬁcient method to label massive samples, one
critical issue is quality control, for which the key
challenge is to infer the ground truth from noisy
or even adversarial data by various users. A large
class of crowdsourcing problems, such as those
involving age, grade, level, or stage, have an or-
dinal structure in their labels. Based on a tech-
nique of sampling estimated label from the pos-
terior distribution, we deﬁne a novel separating
width among the labeled observations to charac-
terize the quality of sampled labels, and develop
an efﬁcient algorithm to optimize it through solv-
ing multiple linear decision boundaries and ad-
justing prior distributions. Our algorithm is em-
pirically evaluated on several real world datasets,
and demonstrates its supremacy over state-of-
the-art methods.

1. Introduction

Crowdsourcing has drawn increasing popularity in the ﬁeld
of machine learning by annotating millions of items in a
short time with relatively low cost (Howe, 2006; Welin-
der & Perona; Deng et al., 2013; Jiang et al., 2015). This
provides a great opportunity to build up large-scale train-
ing sets for complex models, such as deep neural networks
(Krizhevsky et al., 2012), and to reach consensus among
non-experts, such as peer grading in today’s popular mas-
sive open online course (MOOC) systems. However, the
quality of the collected results is often unreliable and di-
verse, and there are spammers who give random labels to
make easy money, or even adversaries who deliberately
give wrong answers. To address this issue, most crowd-

1The Chinese University of Hong Kong, Hong Kong, China.
2Shenzhen University, China. 3Guangdong Provincial Key Labo-
ratory of Computer Vision and Virtual Reality Technology, Shen-
zhen Institutes of Advanced Technology, Chinese Academy of
Sciences, Shenzhen, China. Correspondence to: Shengyu Zhang
<syzhang@cse.cuhk.edu.hk>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

sourcing systems resort to distributing each item to a num-
ber of redundant workers. This raises a challenging ques-
tion of how to aggregate such noisy and redundant labels.

An intuitive and baseline approach for crowdsourcing is
to identify each item following the majority voting (MV)
result of workers. Unfortunately, this approach is error-
prone since it treats each worker equally, and the accu-
racy severely deteriorates with the fraction of less qualiﬁed
workers, spammers or adversaries. Weighted majority vot-
ing (WMV) (Karger et al., 2011) method tries to address
this issue by associating each worker with a weight to char-
acterize his expertise. Specially, max-margin majority vot-
ing (M3V) method (Tian & Zhu, 2015) optimizes the asso-
ciated variables in WMV by maximizing the minimal dif-
ference between the aggregated score of the potential true
label and the aggregated scores of others.

In a different approach, Dawid-Skene (DS) model (Dawid
& Skene, 1979) represents each worker’s expertise by a
confusion matrix and uses a latent variable model to gen-
erate collected labels, which implicitly assumes a worker
to perform equally well across all items in a common
class. This model can be iteratively inferred by the famous
Expectation-Maximization (EM) method (Dempster et al.,
1977), and works well in practice.
In particular, (Zhang
et al., 2016) employs the spectral method (Anandkumar
et al., 2012) to initialize the DS model, and obtains an op-
timal convergence rate up to a logarithmic factor by EM
method. Recently, (Zhou et al., 2012; Tian & Zhu, 2015)
proposed to improve the aggregating performance by inte-
grating the merits of MV method with DS model. The per-
formance of DS model and its variants often relies on the
specially conceived priors with some manually conﬁgured
hyperparameters.

All the above mentioned approaches are for aggregating
general multiclass labels. In many practical applications,
however, the labels have a natural ordinal structure. For
instance, in MOOCs, students are often required to grade
their own assignments on an ordinal scale of 5 (excellent),
4 (good), 3 (fair), 2 (pass) and 1 (failure). In medical imag-
ing, doctors are often required to mark images on an ordinal
scale of stage 1, stage 2, stage 3, and stage 4. Ordinal la-
bel faces an issue of diverse standards. For example, given
four assignments whose true grades are 5, 4, 3 and 2, a

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

strict marker may rate them as 4, 3, 2 and 1. If we ignore
the ordinal structure, we may identify this marker as an ad-
versary because all his answer labels are considered wrong.
But actually this marker grades all assignments in a correct
order, which should be incorporated to improve the crowd-
sourcing performance.

This inspires us to transform the K-class ordinal labeling
to K − 1 binary classiﬁcations. That is, instead of directly
using a label answer k ∈ [K] = {1, 2, . . . , K}, we use it
to answer K − 1 questions “Is the label greater than i” for
all i ∈ [K − 1]. In this way, the harsh marker’s answer
4 for the ﬁrst assignment would give three correct answers
(on thresholds i = 1, 2, 3) and only one wrong answer (on
threshold i = 4), making the marker’s answers highly use-
ful in the aggregation.

For each binary problem, we can employ a Gibbs sampler
to generate label estimations from the generative model of
crowdsourcing. However, these label estimations could be
error-prone especially for the difﬁcult items, whose labels
may be sampled according to the uniform distribution, and
it is well known that the performance of a generative model
heavily relies on the specially conceived priors. To ad-
dress these issues in binary crowdsourcing tasks, we de-
ﬁne a separating width to characterize the quality of label
estimations, and solve it by optimizing a linear decision
boundary. The similar idea has been previously explored in
(Cortes & Vapnik, 1995) and found a lot of success for su-
pervising learning problems. By optimizing the separating
width among two classes, we can improve the sampling
accuracy and update the prior distributions automatically
during the learning process. To characterize the quality of
aggregating ordinal labels from K classes, we introduce
K − 1 decision boundaries to help optimize the separating
width. As demonstrated empirically, our method achieves
the best performance on the real-world datasets compared
to other state-of-the-art methods.

The rest of this paper is organized as follows. Sec. 2 in-
troduces some preliminary works for crowdsourcing tasks.
Sec. 3 presents the generative model employed in this pa-
per. Sec. 4.1 derives the objective function for binary ag-
gregating problem, which is extended for the ordinal case
in Sec. 4.2. The derivations of inference method are dis-
cussed in Sec. 5. Sec. 6 evaluates the performance of our
method on some real-world datasets, and Sec. 7 concludes
this paper.

2. Problem Setting and Preliminary Work

In this section, we formalize the problem and survey some
preliminary methods. Suppose that there are M workers
and N items taken from a total of K classes. For item i,
deﬁne an M × K matrix Ri by putting Ri
jk = 1 if worker

j labels the item as k, and Ri
jk = 0 otherwise. Note that
Ri is a highly sparse matrix since each item is usually as-
signed to a small number of workers. The objective of a
crowdsourcing problem is to identify the true label zi of
item i based on the sparse matrices {R1, . . . , RN }.

2.1. Majority Voting Method and its Variants

Majority Voting (MV) has been widely used to ﬁnd the
most likely label for item i by solving the following prob-
lem,

zi = argk max 1T

M Riek,

(1)

where 1M is a all-one column vector of dimension M and
ek is the k-th standard basis. Weighted majority voting
(WMV) (Karger et al., 2011) generalizes MV by assign-
ing weight vector η ∈ RM ×1 to the workers and solving
the following problem

zi = argk max ηT Riek.

(2)

Specially, max-margin majority voting (M3V) (Tian &
Zhu, 2015) deﬁnes the crowdsourcing margin as the mini-
mal difference between the aggregated score of the poten-
tial true label and aggregated scores of other labels, and
solves η by maximizing the sum of the crowdsourcing mar-
gins of all items.

2.2. Dawid-Skene Model and its Variants

Dawid-Skene (DS) model has been another popular way to
aggregate collected labels by capturing the uncertainties of
labeling behaviors in a generative model. Compared with
WMV and M3V, both of which characterize the expertise
of worker j by a scaler variable, DS model characterizes
the expertise of worker j with an individual confusion ma-
trix Aj ∈ RK×K, where the (k, d)-th entry denotes the
probability that worker j labels a class k sample as class d.
Denote A = {Aj}M
j=1. DS model aims to maximize the
likelihood of observed samples R = {Ri}N
i=1 as follows,

max
A

L =

(cid:90)

ln

N
(cid:88)

i=1

p(Ri|zi, A)p(zi)dzi,

(3)

(cid:81)K
where p(Ri|zi, A) = (cid:81)M
d=1(Aj
jd and zi is a
latent variable with p(zi) = 1
K , ∀i ∈ [N ]. This likeli-
hood function can be optimized iteratively by EM method
(Dempster et al., 1977) as,

zid)Ri

j=1

E-step: q(zi = k) ∝ exp

M
(cid:88)

K
(cid:88)

j=1

d=1

Ri

jd ln Aj

kd,

(4)

M-step: Aj

kd ∝

q(zi = k)Ri

jd.

N
(cid:88)

i=1

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

j=1

(cid:80)K

d=1 Ri

jd ln Aj

Thus, the collected labels are aggregated following the
rule zi = argk max exp (cid:80)M
kd, where
the unknown parameters A can be updated in M-step
through maximum likelihood estimation (MLE) principle.
Recently, spectral methods have been applied to obtain a
better initialization of the DS model (Zhang et al., 2016),
which achieves an optimal convergence rate up to a loga-
rithmic factor. By assuming some special structures of the
confusion matrices A, (Raykar et al., 2010) studies homo-
geneous DS model, and (Moreno et al., 2015) studies the
existence of clusters of workers.

2.3. Recent Achievements

Some recent improvements have been achieved by combin-
ing MV related methods and DS related methods. (Zhou
et al., 2012) assumes labels are generated according to a
distribution over workers, items and labels, which can be
inferred by minimizing its entropy with constraints devel-
oped from MV method and DS model. (Tian & Zhu, 2015)
incorporates M3V method with DS model in a regularized
Bayesian framework (Zhu et al., 2014), and approximates
the posterior distribution over the true labels with a Gibbs
sampler. Nowadays, binary and general multi-class crowd-
sourcing problems have been widely studied in the litera-
ture, but the ordinal sibling has not received nearly as much
attention yet. The work (Zhou et al., 2014) tries to use
the ordinal structure and makes an assumption that work-
ers have difﬁculty distinguishing between two adjacent or-
dinal classes whereas it is much easier to distinguish be-
tween two far-away classes. In this paper, we will develop a
novel objective function to aggregate the ordinal labels, and
achieve the best performance on the real-world datasets.

3. Generative Model of Crowdsourcing

In this section, we present a fully Bayesian model to gen-
erate observed matrices R. First we note that some items
may be intrinsically hard to label even for experts (which
is not uncommon in, for example, medical imaging). To
model such difﬁculty, we introduce a K-dimensional vec-
tor ωi to denote the prior distribution of true label of the
item i even for experts. (For items clearly from category
k, the vector ωi would be just the standard basis ek.) De-
note ω = {ωi}N
i=1. We can obtain a joint distribution as
follows.

Figure 1. Illustrating how the Dirichlet density changes with re-
spect to the scalar value α.

Figure 2. Graphical model of our generative model.

Bayesian framework over A and ω with prior from Dirich-
let distributions (Minka, 2000), a family that has found nu-
merous successful applications (such as topic models) to
generate prior distributions. We assume that both workers’
expertise A and items’ difﬁculty ω are random variables
from the family of Dirichlet distributions

D(x|α) =

Γ(Kα)
ΓK(α)

K
(cid:89)

t=1

xα−1
t

,

(6)

where Γ(·) is the gamma function. As illustrated in Fig-
ure 1, the concentration parameter α controls the sparsity
preference of random vector. A precisely described item i
should have ωi be associated with a small concentration pa-
rameter, resulting in a sparse prior vector, while a vaguely
described item should be associated with a large concen-
tration parameter. To model the expertise Aj of the worker
j, we also prefer that it has a small concentration parame-
ter. We formulate the prior distributions over A and ω as
follows.

p(A|α) =

D(Aj

k:|αj), p(ω|β) =

D(ωi|βi),

(7)

(cid:89)

i

(cid:89)

j,k

where α = {αj}M
and (7) gives the following joint distribution,

j=1 and β = {βi}N

i=1. Combining Eq. (5)

p(R, A, z, ω|α, β) = p(R, z|A, ω)p(ω|β)p(A|α).

The graphical model can be found in Fig. 2.

p(R, z|A, ω) =

(Aj

kd)Ri

jd

I(zi=k)ωi
k

I(zi=k)

,

(5)

(cid:89)

i,j,d,k

Given the matrices R, we can get the posterior distribution
over A, z and ω, which can be formulated as

where A contains the confusion matrices of all workers like
DS model, z is the label vector to be solved and I(·) is an
indicator function. Since usually most workers just anno-
tate a few items, we may not have sufﬁcient samples to
infer z, A and ω. To overcome this, we formulate a fully

p(A, z, ω|R, α, β) =

p(R, A, z, ω|α, β)
(cid:82) p(R, A, z, ω|α, β)dAzω

.

(8)
We can obtain a classiﬁer as argz max p(z|R, α, β) =
argz max (cid:82) p(A, z, ω|R, α, β)dAω to label
the item,

α=0.1 α=1 α=2 𝑦1𝑦2𝑦2𝑦2𝑦1𝑦1𝛽𝛼𝑅𝑧𝜔𝑀𝑁𝐴Learning to Aggregate Ordinal Labels by Maximizing Separating Width

parametrized by the hyperparameters α and β. Conven-
tionally, researchers mainly focus on how to approximate
the posterior distribution with better accuracy and running-
time performance with the ﬁxed prior distributions, or up-
dating the prior distributions by introducing new priors
over α and β (Kim & Ghahramani, 2012; Moreno et al.,
2015). However, the performance of the above generative
model heavily relies on the specially conceived priors to
incorporate domain knowledge, which transmits affects on
the posterior estimations through Bayes’ rules. Given a
family of prior choices, we prefer the classiﬁer with the
more powerful discriminative capability to achieve better
generalization performance.

4. Maximizing the Separating Distance

4.1. Binary Crowdsourcing Problem

Before we present the objective function to aggregate or-
dinal labels, we ﬁrstly consider a simple case, the binary
crowdsourcing problem with K = 2. As shown in Eq. (8),
by varying the hyperparameters α and β, we can obtain
a series of posterior approximations to identify unlabeled
items via Bayes rule. Moreover, by ﬁxing the hyperparam-
eters α and β, we can get multiple estimations of the true
label of one item, which are randomly sampled from the
posterior distribution over its true label. Thus, we are moti-
vated to ﬁnd the most favored set of label estimations over
all items.

For a better generalization performance, we try to maxi-
mize the separation width between two classes. As shown
in Fig. 3, the label set 2 is preferred to the set 1, because
the set 2 has a larger separation width between two classes.
To evaluate the separating width of samples with the label
set z = {zi}N
i=1, with zi ∈ {−1, 1}, ∀i ∈ [N ], we in-
troduce a linear decision boundary f (Ri) = aT Rib with
a ∈ RM ×1 and b ∈ RK×1. Our decision boundary is for-
mulated refer to the formulas in Eq. (1) and (2), where a
denote the worker expertise and b transforms worker’s la-
bel into a scale variable. Thus, we deﬁne an optimization
problem as,

2 = xT x and the minimal value L(a∗, b∗) char-
where (cid:107)x(cid:107)2
acterizes the separating width of the label set z. This op-
timization problem can be understood from the objective
function used in support vector machine (SVM) (Cortes &
Vapnik, 1995), where the objective function is to maximize
2)−1 and the
the margin width ((cid:107)baT (cid:107)F )−1 = ((cid:107)a(cid:107)2
constrains state that all samples lie on the correct side of
the margin. (The constraint in the above optimization prob-
lem can be viewed as the inner product of Ri and a rank-

2(cid:107)b(cid:107)2

1 matrix baT . One may wonder why conﬁning to rank-1
measurements. Note that MV (Eq.(1)) and WMV (Eq.(2))
are also of the rank-1 form, and our experiments also show
that using higher rank measurements actually makes the
generalization performance worse; see experiments in Ap-
pendix.) Since z is a random variable generated from the
posterior distribution (8), we need to reformulate the objec-
tive function (9) as follows,

min
a,b,α,β

L(a, b, α, β) = (cid:107)a(cid:107)2

2(cid:107)b(cid:107)2
2,

(10)

s.t. Ep(zi|Ri,α,β)ziaT Rib ≥ 1,

∀i ∈ [N ],

p(A, z, ω|R, α, β) ∝ p(R, A, z, ω|α, β).

Practically, the labeled samples are often linearly insepara-
ble by a single hyperplane; see Set 3 in Fig. 3. To cope
with this issue, we relax the hard constrains by introducing
non-negative slack variables ξi, one for each sample, and
obtain a “soft” model as follows.

min
a,b,α,β,ξ

L(a, b, α, β) = (cid:107)a(cid:107)2

λ2
λ1
s.t. Ep(zi|Ri,α,β)ziaT Rib ≥ 1 − ξi,

2(cid:107)b(cid:107)2

2 +

N
(cid:88)

i=1

ξi,

(11)

∀i ∈ [N ],

p(A, z, ω|R, α, β) ∝ p(R, A, z, ω|α, β),

where λ2
is used as a positive regularization parameter for
λ1
later convenience, and 1 − ξi is the soft-margin for item
If Ri lies on the correct side of the margin, ξi = 0.
i.
For sample on the wrong side, ξi is proportional to the
distance to the margin. Thus, the value of ξi reﬂects the
difﬁculty of identifying item i, or the error allowed to mis-
classify the item i. The calculation of p(A, z, ω|R, α, β)
is intractable because it involves that of the marginal dis-
tribution p(R|α, β). To address this issue, we introduce a
redundant distribution q(A, z, ω) and rewrite the optimiza-
tion problem as follows.

min
a,b,α,β,q,ξ

L(a, b, α, β, ξ) = (cid:107)a(cid:107)2

2(cid:107)b(cid:107)2

2 +

λ2
λ1

N
(cid:88)

i=1

ξi,

(12)

s.t. Eq(zi)ziaT Rib ≥ 1 − ξi,

∀i ∈ [N ],

where q and p are shorthand for q(A, z, ω) and
p(A, z, ω|R, α, β), respectively, to simplify the presenta-
tions in the rest of the paper. Let ζi = 1 − ziaT Rib, we can
turn the optimization problem into the following one with
two regularization terms: mina,b,α,β,q,ξ L(a, b, α, β, q),
where L is deﬁned by

L(a, b, α, β, q) = KL(q(cid:107)p) + λ1(cid:107)a(cid:107)2

2(cid:107)b(cid:107)2
2

N
(cid:88)

(cid:88)

+ 2λ2

i=1

zi∈{−1,1}

q(zi)(ζi)+,

(13)

L(a, b) = (cid:107)a(cid:107)2

2(cid:107)b(cid:107)2
2,

(9)

KL(q(cid:107)p) = 0.

min
a,b

s.t.

ziaT Rib ≥ 1,

∀i ∈ [N ],

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Figure 3. Four label sets. Set 1 contains one possible label set. Set 2 contains another label set, which is preferred to the set 1. Set
3 contains a label set, whose separating width can be estimated with the slack variables. Set 4 contains a set of ordinal labels, whose
separating width can be estimated by transforming the ordinal problem into multiple binary ones.

where (ζi)+ = max{0, ζi} is the hinge loss function
widely used in training classiﬁers . The factor 2λ2 is intro-
duced here to simplify the derivations of inference methods
later. By optimizing the unknown parameters in the objec-
tive function in Eq.(13), we can obtain the estimated labels
with the largest separating width.

4.2. Ordinal Crowdsourcing Problem

As introduced in Section 1, transforming a K-class ordi-
nal labeling problem (“what is the label of this item?”) to
(K − 1) binary classiﬁcation problems (“Is the label of this
item greater than k?” for k ∈ [K − 1]) allows us to exploit
more useful information from workers. The transform is
illustrated in Set 4 of Fig. 3, where we have items coming
from K ordered classes, C1, . . . , CK. We look for K −1 de-
cision boundaries, with boundary t discriminating classes
C1 ∪ · · · ∪ Ct and classes Ct+1 ∪ · · · ∪ CK. For the t-th
binary question, we introduce a linear decision boundary
t Ribt. It is easily veriﬁed that all bound-
as ft(Ri) = aT
aries intersecting at the zero point. With a = {at}K−1
t=1 and
b = {bt}K−1

t=1 , the loss function in Eq. (13) becomes

L(a, b, α, β, q) =KL(q(cid:107)p) + λ1

(cid:107)at(cid:107)2

2(cid:107)bt(cid:107)2
2

K−1
(cid:88)

t=1

+ 2λ2

q(zi)

(ζit)+, (14)

N
(cid:88)

K
(cid:88)

i=1

zi=1

K−1
(cid:88)

t=1

t Ribt with sgnt(zi) = −1 if
where ζit = 1 − sgnt(zi)aT
zi ≤ t and sgnt(zi) = 1 if zi > t. It is obvious that our or-
dinal model will degenerate into binary one when K = 2.
When K ≥ 3, the ordinal label should be estimated by con-
sidering the predicted results from K − 1 binary problems.

5. Inference Details

In this section, we present the implementation details to
infer the true labels and all other unknown parameters in-
volved in ordinal crowdsourcing problems. Our inference
method consists of two parts. In the ﬁrst part, we employ

a Gibbs sampler to approximately sample from the poste-
rior distribution p = p(A, z, ω|R, α, β). In the second
part, we update the hyperparameters (α, β) and linear de-
cision boundaries based on the gradient method to achieve
the largest separating width.

To approximate the intractable posterior distribution p,
there are two standard approaches, which are Variational
Bayesian (VB) and Gibbs sampling. Compared with the
Gibbs sampling method, VB is usually difﬁcult in its func-
tional optimization, especially hard in our case due to the
hinge loss function. Moreover, VB often suffers from in-
accuracy because of the potentially impractical assumption
of independence of variables. Gibbs sampling is applicable
here, because it provides numerical approximations to the
integration problems in large dimensional spaces by gener-
ating an instance from the conditional distribution of each
variable in turn. It can be shown that the sequence of sam-
ples constitutes a Markov chain, which ﬁnally converges to
the targeted posterior distribution as the stationary distribu-
tion.

Since the sampling process of the confusion matrices A
and the items’ difﬁculties ω can be developed in the stan-
dard way, we leave their derivations in Appendix and
mainly discuss the sampling process of true labels z here.
The difﬁculty of sampling z is mainly due to the hinge
loss function (ζit)+. We employ data augmented tech-
nique (Polson & Scott, 2011) to approximate the hinge loss
function. According to the equality (Andrews & Mallows,
1974),

exp(−2λ2(ζit)+) =

φ(zi, γit|Ri)dγit,

(15)

(cid:90)

with φ(zi, γit|Ri) = (2πγit)− 1
(γit + λ2ζit)2)
and γit as a non-negative augmented variable, we can re-
formulate the objective function in Eq.(14) as follows.

2 exp( −1
2γit

L(a, b, α, β, q) ≤ KL(q(cid:107)p) + λ1

t atbT
aT

t bt

(16)

(cid:88)

t

(cid:90)

(cid:88)

+

i,zi,t

q(zi)q(γit) ln

q(γit)
φ(zi, γit|Ri)

dγit,

𝑥2Set 1𝑥2Set 2𝑥2Set 3𝑥2Set 4𝑥1𝑥1𝑥1𝑥1𝑙1𝑙2𝜁1𝜁2𝒞2𝒞1𝒞2𝒞1𝒞2𝒞1𝒞1𝒞2𝒞3𝑙1𝑙1Learning to Aggregate Ordinal Labels by Maximizing Separating Width

where the inequality comes from Jensen’s inequality with
a new distribution q(γit) to help to approximate the hinge
loss function exp(−2λ2(ζit)+). Note that the right hand
side of the inequality is tractable, minimizing which would
give an upper bound of the original optimization problem.
Before we sample the true labels of items, we need to ﬁrst
generate augmented variables γ = {γit}N,K−1
i=1,t=1. When
ﬁxing other random variables, we can generate the (i, t)-th
augmented parameter according to the following general-
ized inverse Gaussian (GIG) distribution,

γit ∼

exp[−

(γit +

(17)

1
Z

γ− 1

2

it

1
2

λ2
2ζ 2
it
γit

)],

where Z is the normalization term. It has been shown that
1
can be drawn efﬁciently with O(1) time complexity
γit
(Michael et al., 1976).

Here, we can sample the true labels of all items. Let
φ(z, γ|R) = (cid:81)N
t=1 φ(zi, γit|Ri). Rewrite the ob-
i=1
jective function shown in Eq. (16) with respect to q(zi) as
follows,

(cid:81)K−1

L(q(zi))

(cid:90)

(cid:88)

i,zi,t

≤ KL(q(cid:107)p) +

q(zi)q(γit) ln

q(γit)
φ(zi, γit|Ri)

dγit

= KL(q · q(γ)(cid:107)p · φ(z, γ)).

(18)

Thus, with all other parameters ﬁxed, we can sample zi ∈
[K] according to the following distribution,

q(zi) ∝ p(Ri, A, zi, ωi|α, β)

φ(zi, γit|Ri

t).

(19)

K−1
(cid:89)

t=1

Let us examine the two terms on the right hand side. The
ﬁrst term comes from the generative model of crowdsourc-
ing, while the second term maximizes the separating width
of the estimated ordinal labels. For the binary crowdsourc-
ing problem, we have only one decision boundary to mea-
sure the separating width, while for the ordinal crowdsourc-
ing problem with K classes, we get K − 1 intersected de-
cision boundaries to measure the separating width. After
obtaining a set of random samples to approximate the joint
posterior distribution q over all model parameters and aug-
mented variables, the objective function shown in Eq. (14)
becomes a parametric function with respect to a, b, α and
β. Thus, we can intuitively update these parameters based
on the gradient method. The derivations of the updating
formulas over a, b, α and β can be found in Appendix.
Let (cid:104)f (x)(cid:105) = (cid:82) q(x)f (x)dx denote the expectation of
f (x) with respect to the distribution of q(x). Our method is
outlined in Algorithm 1, in which each while iteration con-
sists of two for loops, and the source code with demo can
be found on the website1. In the ﬁrst for loop, we employ

1http://appsrv.cse.cuhk.edu.hk/˜gychen/

i=1, λ1, λ2 and the learning rates η.

i=1 by MV, a, b, α and β

Algorithm 1 Our Ordinal Crowdsourcing Method
1: Input: R = {Ri}N
2: Initializing z = {zi}N
3: while not convergence do
for i = 1 : N do
4:
kd|αj + (cid:80)N
kd ∼ D(Aj
Aj
i=1 Ri
5:
jd
k|βi + I(zi = k))
k ∼ D(ωi
ωi
6:
Z γ− 1
2 (γit + λ2
2ζ2
exp[− 1
γit ∼ 1
)]
γit
zi ∼ p(Ri, A, zi, ωi|α, β) (cid:81)K−1
t=1 φ(zi, γit|Ri
t)

I(zi = k))

7:

it

it

2

8:
9:
10:
11:

12:

13:

14:
15:
16:

end for
for t = 1 : K − 1 do
Σat = 2λ1(cid:107)bt(cid:107)2
((cid:80)N
at = Σ−1
at
Σbt = 2λ1(cid:107)at(cid:107)2
((cid:80)N
bt = Σ−1
bt

end for
αj ← αj − η ∂L(αj )
∂αj
βi ← βi − η ∂L(βi)
∂βi

17:
18: end while

2

i=1

2I + (cid:80)N
i=1(λ2 + λ2
2I + (cid:80)N
i=1(λ2 + λ2

λ2
t RiT
(cid:104)γit(cid:105) RibtbT
2
(cid:104)γit(cid:105) )(cid:104)sgnt(zi)(cid:105)Ribt)
t RiRiT at
t Ri)

λ2
(cid:104)γit(cid:105) aT
2
(cid:104)γit(cid:105) )(cid:104)sgnt(zi)(cid:105)aT

i=1

2

, ∀j ∈ [M ]

, ∀i ∈ [N ]

a Gibbs sampler to generate the random variables to ap-
proximate the posterior distribution. In the second part, we
solve the separating width by optimizing K − 1 decision
boundaries, and update the prior distributions by gradient
method. Compared with the traditional generative model
of crowdsourcing, including DS model and its variants, our
method introduces an augment variable γit to approximate
the hinge loss function, which is further involved in the
generation of true labels. This algorithm is iteratively im-
plemented to reach a local optimum.

6. Experiments and Discussions

To fully evaluate the ideas employed in this paper, we
present empirical studies of our aggregating method in
comparison with competitive ones not only on ordinal
crowdsourcing tasks, but also binary crowdsourcing tasks.
For our method, we conﬁgure λ1 = λ2 = 1, α = 1M , β =
1N , η = 1 × 10−5 and initialize zi by the majority voting
result. In each run of our method, we generate 80 samples
to approximate the posterior distribution and discard the
ﬁrst 10 samples as burn-in steps. The reported error rate of
our method is averaged over 10 runs, and all experiments
are conducted in a PC with Intel Core i7 1.8GHz CPU and
8.00GB RAM.

6.1. Binary Crowdsourcing Tasks

We ﬁrst evaluate our method on three binary benchmark
datasets shown in Table 1, include labeling bird species

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Table 1. The summary of real-world datasets used in the compar-
ison experiments.

Name
Bird
RTE
TREC
Web
Age

Classes
2
2
2
5
7

Items Workers Labels/item
108
800
19,033
2,665
1,002

39.0
10.0
4.64
5.84
10

39
164
762
177
165

6.2. Ordinal Crowdsourcing Tasks

In this part, we report empirical results of our method on
ordinal benchmark datasets in comparison with competi-
tive ones. We consider MV, MV-DS, and G-CrowdSVM as
baselines, and compare our method with Entropy(O) (Zhou
et al., 2014), which consider the ordinal structures in la-
bels. As shown in Table 1, we have two ordinal datasets.
One is to judge the relevance of query-URL pairs with
a 5-level rating score (Web dataset), and the other is to
identify the age of each subject with a 7-level rating score
(Age dataset). To evaluate the performance of aggregat-
ing ordinal labels, we deﬁne three following error measure-
|z| (cid:107)z − ˆz(cid:107)0, l1 = 1
ments as: l0 = 1 − 1
|z| (cid:107)z − ˆz(cid:107)1 and
l2 = 1
|z| (cid:107)z − ˆz(cid:107)2. Compared with the error rate l0, the
measures l1 and l2 take precision into consideration, and
may be preferred for aggregating ordinal labels when one
cares about the severity of error.

Table 3 summarizes the performance of all methods on
the ordinal datasets, and shows that our method consis-
tently outperforms the others in predicting the ordinal la-
bels of items. Similar with our method, G-CrowdSVM
attempts to maximize the margin between the aggregated
score of potential true label and the aggregated score of
others, and achieves the better performance in comparison
with the state-of-the-art method to aggregate ordinal labels,
Entropy(O). Compared with G-CrowdSVM, we treat the
problem of aggregating collected labels as the classiﬁcation
problem, and introduce K − 1 decision boundaries to con-
sider the ordinal relationship among categories. As shown
in Table 3, on the Web dataset, our method signiﬁcantly re-
duces the average l0 error rate from 7.99% to 3.22%. In ad-
dition, the average l1 error of our method is 3.69%, which
is only slightly larger than the l0 error rate of 3.22%. It
means that, even for the incorrect labels ˆzi outputted by
our algorithm, our ˆzi is not far away from its true answer
zi, resulting in a relatively small damage.

The Web dataset has been widely used in the evaluation
of ordinal crowdsourcing problem. On this dataset, our
method introduces 4 decision boundaries to measure the
separating width of generated true labels. To help to under-
stand the linear decision boundaries learned by our method,
we illustrate the average value over 80 samples of {bt}4
t=1
as Fig. 4. It can be seen that the absolute value of all en-
tries in bt is approximated to 1. To be more concrete, let us
consider the ﬁrst decision boundary with b1, which calcu-
late Rib1 for the item i. Thus, Rib1 successfully reduces
the ordinal problem into a binary one, the j-th entry in Ribi
would be −1 if worker j ranks item i as 1 and 1 if worker
j rank item i as 2, 3, 4 and 5. Note that at characterizes the
expertise of all workers for the t-th binary problem. Fig. 5
contains three confusion matrices, including the averaged
confusion matrix of all workers, the confusion matrices of

Figure 4. The average value of 80 samples of {bt}4
run.

t=1 in a random

(Welinder et al., 2010) (Bird dataset), recognizing textual
entailment (Snow et al., 2008) (RTE dataset) and access-
ing the relevance of topic-document pairs with a binary
judgment in TREC 2011 crowdsourcing track (Gabriella &
Matthew, 2011) (TREC dataset). The competitive meth-
ods include the pure majority voting estimator (refereed to
as MV), the EM method for DS model initialized by ma-
jority voting (refereed to as MV-DS), the EM method for
DS model initialized by spectral method (refereed to as
Opt-DS) (Zhang et al., 2016), the Gibbs sampler for the
Bayesian extension of M3V (Tian & Zhu, 2015) (referred
to as G-CrowdSVM), the SVD-based algorithm proposed
in (Ghosh et al., 2011) (referred to as Gh-SVD), and the
Eigenvalues of Ratio algorithm proposed in (Dalvi et al.,
2013) (referred to as Eig-Ratio). The performance of all
methods are evaluated by error as l0 = 1 − 1
|z| (cid:107)z − ˆz(cid:107)0,
where z contains true labels of items (available in all these
datasets) and ˆz contains estimations given by our algorithm
(not using any information of z). Noted that the reported
error rates of G-CrowdSVM are the average over 10 ran-
dom runs as our method.

As shown in Table 2, our method achieves the best per-
formance among all methods on three benchmark datasets.
Without regards to the prior knowledges over workers’ ex-
pertise and items’ difﬁculties, we can degenerate our model
into MV-DS model by setting λ2 = λ1 = 0. Com-
paring with the performance of MV-DS model, especially
Opt-DS method, we present a more complicated genera-
tive model, leading to better predictive results. Compared
with G-CrowdSVM method, our method updates prior dis-
tributions and improves the sampling accuracy by optimiz-
ing the separating width during the learning process, which
leads to the improvements of predictive performance on all
datasets.

𝑏1𝑏2𝑏3𝑏4Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Table 2. l0 error rate (%) in predicting the latent labels on three binary benchmark datasets.

Binary Dataset
Bird
RTE
TREC

Ours
9.25±0.17
7.00±0.29
29.30±0.11

G-CrowdSVM Opt-DS MV-DS MV
24.07
10.09
10.31
7.12
34.86
29.80

10.37±0.41
7.72±0.22
31.32±0.34

11.11
7.12
30.02

Gh-SVD Eig-Ratio

27.78
49.13
42.99

27.78
9.00
43.96

Table 3. Errors in predicting the latent labels on two ordinal benchmark datasets.

Ordinal Dataset

Web

Age

Ours
0.0322±0.0013
0.0369±0.0032
0.2153±0.0019
0.3210±0.0025
0.3493±0.0036
0.6192±0.0047

G-CrowdSVM Entropy(O) MV-DS MV
0.0799±0.0026
0.1040
0.0940±0.0057
0.1173
0.3629±0.0044
0.3816
0.3298±0.0036
0.3732
0.3737±0.0033
0.4541
0.6592±0.0028
0.7936

0.1574
0.2149
0.5358
0.3962
0.5000
0.8518

0.2693
0.4251
0.9247
0.3488
0.4083
0.7297

l0
l1
l2
l0
l1
l2

Figure 5. (a) the averaged confusion matrix over all worker, (b) the confusion matrix of an expert, (c) the confusion matrix of a spammer.

7. Conclusions

In this paper, we develop a novel method to aggregate
ordinal labels by optimizing the separating width among
classes. To measure the separating width among ordinal
labels, we ﬁrst investigate a binary case, and then extend
our achievements to the ordinal one. With K − 1 decision
boundaries, we deﬁne an optimization problem for measur-
ing the separating width among ordinal classes. The newly
introduced boundaries not only help to optimize the hyper-
parameters, but also calibrate the estimated labels sampled
from the generative model. A Gibbs sampler is adopted to
approximate the posterior distribution, while the gradient
method is used to calculate the separating width and opti-
mize the hyperparameters.

As demonstrated on the ordinal datasets, which is the main
focus of this paper, our method consistently achieves the
best performance compared with competitive ones, and
the improvements on Web dataset are signiﬁcant. As
demonstrated by the experimental results on the binary
datasets, our algorithm works slightly better than any pre-
vious method. Thus, our algorithm provides a uniform
method in both binary and ordinal cases, and can be practi-
cally useful for real-world applications.

Figure 6. Error rates per iteration of various estimators on the Web
dataset.

an expert and a spammer. It can be found that the spam-
mer ranks all items randomly to make easy money, while
the expert has a confusion matrix similar to the identical
matrix. Our method can accurately estimate the confusion
matrices of all workers even given the averaged confusion
matrix acts like a spammer. Fig. 6 summarizes the training
time and error rates after each iteration for all estimators on
the Web dataset. It can be found that the proposed method
coverages to a lower error rate and all other three methods
have error convergence curves all above ours.

(a)(b)(c)𝒞1𝒞2𝒞3𝒞4𝒞5𝒞1𝒞2𝒞3𝒞4𝒞5𝒞1𝒞2𝒞3𝒞4𝒞5𝒞1𝒞2𝒞3𝒞4𝒞501002003004005006000.020.040.060.080.10.120.140.160.18Time (s)Error rateMV-DSEntropy(O)G-CrowdSVMOursLearning to Aggregate Ordinal Labels by Maximizing Separating Width

Acknowledgements

We would like to thank anonymous reviewers for their
valuable comments to improve the presentation of this pa-
per. This work was supported by the China 973 Program
(Project No. 2015CB351706) and a grant from the Na-
tional Natural Science Foundation of China (Project No.
61233012). Shengyu Zhang was supported by Research
Grants Council of the Hong Kong S.A.R. (Project no.
CUHK14239416).

References

Anandkumar, A., Liu, Y. K., Hsu, D. J., Foster, D. P., and
Kakade, S. M. A spectral algorithm for latent dirichlet
allocation. In Advances in Neural Information Process-
ing Systems, pp. 917–925, 2012.

Andrews, D. F. and Mallows, C. L. Scale mixtures of nor-
mal distributions. Journal of the Royal Statistical Soci-
ety. Series B (Methodological), pp. 99–102, 1974.

Cortes, C. and Vapnik, V. Support-vector networks. Ma-

chine learning, 20(3):273–297, 1995.

Dalvi, N., Dasgupta, A., and Kumar, R. .and Rastogi, V.
Aggregating crowdsourced binary ratings. In Proceed-
ings of the 22nd international conference on World Wide
Web, pp. 285–294. ACM, 2013.

Dawid, A. P. and Skene, A. M. Maximum likelihood es-
timation of observer error-rates using the em algorithm.
Applied statistics, pp. 20–28, 1979.

Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum
likelihood from incomplete data via the em algorithm.
Journal of the royal statistical society. Series B (method-
ological), pp. 1–38, 1977.

Deng, J., Krause, J., and Li, F. F. Fine-grained crowd-
sourcing for ﬁne-grained recognition. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 580–587, 2013.

Gabriella, K. and Matthew, L. Overview of the trec 2011
In Proceedings of TREC 2011,

crowdsourcing track.
2011.

Ghosh, A., Kale, S., and McAfee, P. Who moderates
the moderators?: crowdsourcing abuse detection in user-
generated content. In Proceedings of the 12th ACM con-
ference on Electronic commerce, pp. 167–176. ACM,
2011.

Howe, J. The rise of crowdsourcing. Wired magazine, 14

(6):1–4, 2006.

Jiang, M., Huang, S., Duan, J., and Zhao, Q. Salicon:
Saliency in context. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, pp.
1072–1080. IEEE, 2015.

Karger, D. R., Oh, S., and Shah, D. Iterative learning for
reliable crowdsourcing systems. In Advances in Neural
Information Processing Systems, pp. 1953–1961, 2011.

Kim, H. C. and Ghahramani, Z. Bayesian classiﬁer combi-

nation. In AISTATS, pp. 619–627, 2012.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
In Advances in Neural Information Processing Systems,
pp. 1097–1105, 2012.

Michael, J. R., Schucany, W. R., and Haas, R. W. Generat-
ing random variates using transformations with multiple
roots. The American Statistician, 30(2):88–90, 1976.

Minka, T. Estimating a dirichlet distribution, 2000.

Moreno, P. G., Art´es-Rodr´ıguez, A., Teh, Y. W., and
Perez-Cruz, F. Bayesian nonparametric crowdsourcing.
Journal of Machine Learning Research, 16:1607–1627,
2015.

Polson, N. G. and Scott, S. L. Data augmentation for
support vector machines. Bayesian Analysis, 6(1):1–23,
2011.

Raykar, V. C., Yu, S., Zhao, L. H., Valadez, G. H., Florin,
C., Bogoni, L., and Moy, L. Learning from crowds. Jour-
nal of Machine Learning Research, 11(Apr):1297–1322,
2010.

Snow, R., O’Connor, B., Jurafsky, D., and Ng, A. Y. Cheap
and fast—but is it good?: evaluating non-expert annota-
tions for natural language tasks. In Proceedings of the
conference on empirical methods in natural language
processing, pp. 254–263. Association for Computational
Linguistics, 2008.

Tian, T. and Zhu, J. Max-margin majority voting for learn-
In Advances in Neural Information

ing from crowds.
Processing Systems, pp. 1621–1629, 2015.

Welinder, P. and Perona, P. Online crowdsourcing: Rating
annotators and obtaining cost-effective labels. In 2010
IEEE Computer Society Conference on Computer Vision
and Pattern Recognition-Workshops.

Welinder, P., Branson, S., Perona, P., and Belongie, S. J.
The multidimensional wisdom of crowds. In Advances in
Neural Information Processing Systems, pp. 2424–2432,
2010.

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Zhang, Y., Chen, X., Zhou, D., and Jordan, M. I. Spec-
tral methods meet em: a provably optimal algorithm for
crowdsourcing. Journal of Machine Learning Research,
17(102):1–44, 2016.

Zhou, D., Basu, S., Mao, Y., and Platt, J. C. Learning
from the wisdom of crowds by minimax entropy. In Ad-
vances in Neural Information Processing Systems, pp.
2195–2203, 2012.

Zhou, D., Liu, Q., Platt, J. C., and Meek, C. Aggregat-
ing ordinal labels from crowds by minimax conditional
entropy. In ICML, pp. 262–270, 2014.

Zhu, J., Chen, N., and Xing, E. P. Bayesian inference with
posterior regularization and applications to inﬁnite latent
svms. Journal of Machine Learning Research, 15(1):
1799–1847, 2014.

