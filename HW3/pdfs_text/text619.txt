Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Yu-Xiang Wang 1 Alekh Agarwal 2 Miroslav Dudík 2

Abstract
We study the off-policy evaluation problem—
estimating the value of a target policy using data
collected by another policy—under the contextual
bandit model. We consider the general (agnostic)
setting without access to a consistent model of re-
wards and establish a minimax lower bound on the
mean squared error (MSE). The bound is matched
up to constants by the inverse propensity scoring
(IPS) and doubly robust (DR) estimators. This
highlights the difﬁculty of the agnostic contextual
setting, in contrast with multi-armed bandits and
contextual bandits with access to a consistent re-
ward model, where IPS is suboptimal. We then
propose the SWITCH estimator, which can use an
existing reward model (not necessarily consistent)
to achieve a better bias-variance tradeoff than IPS
and DR. We prove an upper bound on its MSE
and demonstrate its beneﬁts empirically on a di-
verse collection of data sets, often outperforming
prior work by orders of magnitude.

1. Introduction

Contextual bandits refer to a learning setting where the
learner repeatedly observes a context, takes an action and
observes a reward for the chosen action in the observed
context, but no feedback on any other action. An example is
movie recommendation, where the context describes a user,
actions are candidate movies and the reward measures if the
user enjoys the recommended movie. The learner produces
a policy, meaning a mapping from contexts to actions. A
common question in such settings is, given a target policy,
what is its expected reward? By letting the policy choose
actions (e.g., recommend movies to users), we can compute
its reward. Such online evaluation is typically costly since it
exposes users to an untested experimental policy, and does

1Carnegie Mellon University, Pittsburgh, PA 2Microsoft Re-
search, New York, NY. Correspondence to: Yu-Xiang Wang <yux-
iangw@cs.cmu.edu>, Alekh Agarwal <alekha@microsoft.com>,
Miroslav Dudík <mdudik@microsoft.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

not scale to evaluating many different target policies.

Off-policy evaluation is an alternative paradigm for the same
question. Given logs from the existing system, which might
be choosing actions according to a very different logging
policy than the one we seek to evaluate, can we estimate
the expected reward of the target policy? There are three
classes of approaches to address this question: the direct
method (DM), also known as regression adjustment, inverse
propensity scoring (IPS) (Horvitz & Thompson, 1952) and
doubly robust (DR) estimators (Robins & Rotnitzky, 1995;
Bang & Robins, 2005; Dudík et al., 2011; 2014).

Our ﬁrst goal in this paper is to study the optimality of
these three classes of approaches (or lack thereof), and more
fundamentally, to quantify the statistical hardness of off-
policy evaluation. This problem was previously studied
for multi-armed bandits (Li et al., 2015) and is related to
a large body of work on asymptotically optimal estimators
of average treatment effects (ATE) (Hahn, 1998; Hirano
et al., 2003; Imbens et al., 2007; Rothe, 2016), which can be
viewed as a special case of off-policy evaluation. In both set-
tings, a major underlying assumption is that rewards can be
consistently estimated from the features (i.e., covariates) de-
scribing contexts and actions, either via a parametric model
or non-parametrically. Under such consistency assumptions,
it has been shown that DM and/or DR are optimal (Imbens
et al., 2007; Li et al., 2015; Rothe, 2016),1 whereas standard
IPS is not (Hahn, 1998; Li et al., 2015), but it becomes
(asymptotically) optimal when the true propensity scores
are replaced by suitable estimates (Hirano et al., 2003).

Unfortunately, consistency of a reward model can be difﬁ-
cult to achieve in practice. Parametric models tend to suffer
from a large bias (see, e.g., the empirical evaluation of Dudík
et al., 2011) and non-parametric models are limited to small
dimensions, otherwise non-asymptotic terms become too
large (see, e.g., the analysis of non-parametric regression by
Bertin et al., 2004). Therefore, here we ask: What can be
said about hardness of policy evaluation in the absence of
reward-model consistency?

In this pursuit, we provide the ﬁrst rate-optimal lower bound
on the mean-squared error (MSE) for off-policy evaluation

1The precise assumptions vary for each estimator, and are

somewhat weaker for DR than for DM.

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

in contextual bandits without consistency assumptions. Our
lower bound matches the upper bounds of IPS and DR up
to constants, when given a non-degenerate context distribu-
tions. This result is in contrast with the suboptimality of IPS
under previously studied consistency assumptions, which
implies that the two settings are qualitatively different.

Whereas IPS and DR are both minimax optimal, our ex-
periments (similar to prior work) show that IPS is readily
outperformed by DR, even when using a simple parametric
regression model that is not asymptotically consistent. We
attribute this to a lower variance of the DR estimator. We
also empirically observe that while DR is generally highly
competitive, it is sometimes substantially outperformed by
DM. We therefore ask whether it is possible to achieve an
even better bias-variance tradeoff than DR. We answer af-
ﬁrmatively and propose a new class of estimators, called
the SWITCH estimators, that adaptively interpolate between
DM and DR (or IPS). We show that SWITCH has MSE no
worse than DR (or IPS) in the worst case, but is robust to
large importance weights and can achieve a substantially
smaller variance than DR or IPS.

We empirically evaluate the SWITCH estimators against a
number of strong baselines from prior work, using a previ-
ously used experimental setup to simulate contextual bandit
problems on real-world multiclass classiﬁcation data. The
results afﬁrm the superior bias-variance tradeoff of SWITCH
estimators, with substantial improvements across a number
of problems.

In summary, the ﬁrst part of our paper initiates the study
of optimal estimators in a ﬁnite-sample setting and without
making strong modeling assumptions, while the second
part shows how to practically exploit domain knowledge by
building better estimators.

2. Setup

In contextual bandit problems, the learning agent observes
a context x, takes an action a and observes a scalar reward
r for the action chosen in the context. Here the context
x is a feature vector from some domain X ⊆ Rd, drawn
according to a distribution λ. Actions a are drawn from a
ﬁnite set A. Rewards r have a distribution conditioned on x
and a denoted by D(r | x, a). The decision rule of the agent
is called a policy, which maps contexts to distributions over
actions, allowing for randomization in the action choice.
We write µ(a | x) and π(a | x) to denote the logging and
target policies respectively. Given a policy π, we extend it
to a joint distribution over (x, a, r), where x ∼ λ, action
a ∼ π(a | x), and r ∼ D(r | x, a). With this notation, given
n i.i.d. samples (xi, ai, ri) ∼ µ, we wish to compute the
value of π:

vπ = Eπ[r] = Ex∼λEa∼π(·|x)Er∼D(·|a,x)[r].

(1)

In order to correct for the mismatch in the action distribu-
tions under µ and π, it is typical to use importance weights,
deﬁned as ρ(x, a) := π(a|x)/µ(a|x). For consistent estima-
tion, it is standard to assume that ρ(x, a) (cid:54)= ∞, correspond-
ing to absolute continuity of π with respect to µ, meaning
that whenever π(a|x) > 0, then also µ(a|x) > 0. We make
this assumption throughout the paper. In the remainder of
the setup we present three common estimators of vπ.

The ﬁrst is the inverse propensity scoring (IPS) estimator
(Horvitz & Thompson, 1952), deﬁned as

ˆvπ
IPS =

ρ(xi, ai)ri.

(2)

n
(cid:88)

i=1

IPS is unbiased and makes no assumptions about how re-
wards might depend on contexts and actions. When such
information is available, it is natural to posit a parametric or
non-parametric model of E[r | x, a] and ﬁt it on the logged
data to obtain a reward estimator ˆr(x, a). Policy evaluation
can now simply be performed by scoring π according to ˆr
as

ˆvπ
DM =

1
n

n
(cid:88)

(cid:88)

i=1

a∈A

π(a | xi)ˆr(xi, a),

(3)

where the DM stands for direct method (Dudík et al., 2011),
also known as regression adjustment or imputation (Rothe,
2016). IPS can have a large variance when the target and
logging policies differ substantially, and parametric variants
of DM can be inconsistent, leading to a large bias. Therefore,
both in theory and practice, it is beneﬁcial to combine the
approaches into a doubly robust estimator (Cassel et al.,
1976; Robins & Rotnitzky, 1995; Dudík et al., 2011), such
as the following variant,

n
(cid:88)

(cid:20)
ρ(xi, ai)(cid:0)ri − ˆr(xi, ai)(cid:1)

ˆvπ
DR =

1
n

i=1

π(a | xi)ˆr(xi, a)

(4)

(cid:21)
.

(cid:88)

+

a∈A

Note that IPS is a special case of DR with ˆr ≡ 0. In the
sequel, we mostly focus on IPS and DR, and then suggest
how to improve them by further interpolating with DM.

3. Limits of Off-policy Evaluation

In this section, we study the off-policy evaluation problem
in a minimax setup. After setting up the framework, we
present our lower bound and the matching upper bounds for
IPS and DR under appropriate conditions.

While minimax optimality is standard in statistical estima-
tions, it is not the only notion of optimality. An alternative
framework is that of asymptotic optimality, which estab-
lishes Cramer-Rao style bounds on the asymptotic variance
of estimators. We use the minimax framework, because it

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

is the most amenable to ﬁnite-sample lower bounds, and is
complementary to previous asymptotic results, as we discuss
after presenting our main results.

required for consistency of IPS (see, e.g., Dudík et al., 2014).
Our assumption holds for instance when the context space
is ﬁnite, because then both ρ and Rmax are bounded.

3.1. Minimax Framework

3.2. Minimax Lower Bound for Off-policy Evaluation

Off-policy evaluation is a statistical estimation problem,
where the goal is to estimate vπ given n i.i.d. samples gen-
erated according to a policy µ. We study this problem in a
standard minimax framework and seek to answer the follow-
ing question. What is the smallest MSE that any estimator
can achieve in the worst case over a large class of contextual
bandit problems? As is usual in the minimax setting, we
want the class of problems to be rich enough so that the esti-
mation problem is not trivial, and to be small enough so that
the lower bounds are not driven by complete pathologies. In
our problem, we ﬁx λ, µ and π, and only take worst case
over a class of reward distributions. This allows the upper
and lower bounds to depend on λ, µ and π, highlighting
how these ground-truth parameters inﬂuence the problem
difﬁculty. The family of reward distributions D(r | x, a) that
we study is a natural generalization of the class studied by
Li et al. (2015) for multi-armed bandits. We assume we are
given maps Rmax : X × A → R+ and σ : X × A → R+,
and deﬁne the class of reward distributions R(σ, Rmax) as2

R(σ, Rmax) :=

(cid:110)

D(r|x, a) : 0 ≤ ED[r|x, a] ≤ Rmax(x, a)
(cid:111)
.

and VarD[r|x, a] ≤ σ2(x, a) for all x, a

Note that σ and Rmax are allowed to change over contexts
and actions. Formally, an estimator is any function ˆv :
(X × A × R)n → R that takes n data points collected by
µ and outputs an estimate of vπ. The minimax risk of off-
policy evaluation over the class R(σ, Rmax), denoted by
Rn(π; λ, µ, σ, Rmax), is deﬁned as

inf
ˆv

sup
D(r|x,a)∈R(σ,Rmax)

E (cid:2)(ˆv − vπ)2(cid:3) .

(5)

Recall that the expectation is taken over the n samples
drawn from µ, along with any randomness in the estima-
tor. The main goal of this section is to obtain a lower
bound on the minimax risk. To state our bound, recall
that ρ(x, a) = π(a | x)/µ(a | x) < ∞ is an importance
weight at (x, a). We make the following technical assump-
tion on our problem instances, described by tuples of the
form (π, λ, µ, σ, Rmax):
Assumption 1. There exists (cid:15) > 0 such that Eµ
and Eµ

(cid:2)(ρRmax)2+(cid:15)(cid:3) are ﬁnite.

(cid:2)(ρσ)2+(cid:15)(cid:3)

This assumption is only a slight strengthening of the assump-
tion that Eµ[(ρσ)2] and Eµ[(ρRmax)2] be ﬁnite, which is

2Technically, the inequalities in the deﬁnition of R(σ, Rmax)

need to hold almost surely with x ∼ λ and a ∼ µ(· | x).

With the minimax setup in place, we now give our main
lower bound on the minimax risk for off-policy evaluation
and discuss its consequences. Our bound depends on a
parameter γ ∈ [0, 1] and a derived indicator random variable
ξγ(x, a) := 1(µ(x, a) ≤ γ), which separates out the pairs
(x, a) that appear “frequently” under µ.3 As we will see,
the “frequent” pairs (x, a) (where ξγ = 0) correspond to the
intrinsically realizable part of the problem, where consistent
reward models can be constructed. The “infrequent” pairs
(where ξγ = 1) constitute the part that is non-realizable in
the worst-case. When X ⊆ Rd and λ is continuous with
respect to the Lebesgue measure, then ξγ(x, a) = 1 for
all γ ∈ [0, 1], so the problem is non-realizable everywhere
in the worst-case. Our result uses the following problem-
dependent constant (deﬁned with the convention 0/0 = 0):

Cγ := 22+(cid:15) max

(cid:26) Eµ[(ρσ)2+(cid:15)]2
Eµ[(ρσ)2]2+(cid:15) ,

Eµ[ξγ(ρRmax)2+(cid:15)]2
Eµ[ξγ(ρRmax)2]2+(cid:15)

(cid:27)

.

Theorem 1. Assume that a problem instance satisﬁes As-
sumption 1 with some (cid:15) > 0. Then for any γ ∈ [0, 1] and
max](cid:9), the minimax
any n ≥ max(cid:8)16C 1/(cid:15)
, 2C 2/(cid:15)
risk Rn(π; λ, µ, σ, Rmax) satisﬁes the lower bound

γ Eµ[σ2/R2

γ

Eµ

(cid:2)ρ2σ2(cid:3) + Eµ

(cid:2)ξγρ2R2

1 − 350nγ log(5/γ)

(cid:17)

.

(cid:3)(cid:16)

max

700n

The bound holds for every γ ∈ [0, 1], and we can take the
maximum over γ. In particular, we get the following simple
corollary under continuous context distributions.

Corollary 1. Under conditions of Theorem 1, assume fur-
ther that λ has a density relative to Lebesgue measure. Then

Rn(π; λ, µ, σ, Rmax) ≥

Eµ

(cid:2)ρ2σ2(cid:3) + Eµ
700n

(cid:2)ρ2R2

max

(cid:3)

.

If λ is a mixture of a density and point masses, then γ = 0
will exclude the point masses from the second term of the
lower bound. In general, choosing γ = O(cid:0)1/(n log n)(cid:1)
excludes the contexts likely to appear multiple times, and
ensures that the second term in Theorem 1 remains non-
trivial (when µ(x, a) ≤ γ with positive probability).

Before sketching the proof of Theorem 1, we discuss its
preconditions and implications.

3Formally, µ(x, a) corresponds to µ( {(x, a)} ), i.e., the mea-
sure under µ of the set {(x, a)}. For example, when λ is a contin-
uous distribution then µ(x, a) = 0 everywhere.

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Preconditions of the theorem: The theorem assumes the
existence of a (problem-dependent) constant Cγ which
depends on the constant γ and various moments of the
importance-weighted rewards. When Rmax and σ are
bounded (a common situation), Cγ measures how heavy-
tailed the importance weights are. Note that Cγ < ∞ for
all γ ∈ [0, 1] whenever Assumption 1 holds, and so the
condition on n in Theorem 1 is eventually satisﬁed as long
as the random variable σ/Rmax has a bounded second mo-
ment. This is quite reasonable since in typical applications
the a priori bound on expected rewards is on the same or-
der or larger than the a priori bound on the reward noise.
For the remainder of the discussion, we assume that n is
appropriately large so the preconditions of the theorem hold.

Comparison with upper bounds: The setting of Corol-
lary 1 is typical of many contextual bandit applications. In
this setting both IPS and DR achieve the minimax risk up to
a multiplicative constant. Let r∗(x, a) := E[r | x, a]. Recall
that DR is using an estimator ˆr(x, a) of r∗(x, a), and IPS
can be viewed as a special case of DR with ˆr ≡ 0. By
Lemma 3.3(i) of Dudík et al. (2014), the MSE of DR is

E[(ˆvπ

DR − vπ)2]
(cid:16)

=

1
n

Eµ[ρ2σ2] + Varx∼DEa∼µ(·|x)[ρr∗]
(cid:17)
+ Ex∼DVara∼µ(·|x)[ρ(ˆr − r∗)]

.

(6)

n (Eµ[ρ2σ2] + Eµ[ρ2R2

Note that 0 ≤ r∗ ≤ Rmax, so if the estimator ˆr also satisﬁes
0 ≤ ˆr ≤ Rmax, we obtain that the risk of DR (with IPS as
max])(cid:1).
a special case) is at most O(cid:0) 1
This means that IPS and DR are unimprovable, in the worst
case, beyond constant factors. Another implication is that
the lower bound of Corollary 1 is sharp, and the minimax
max])(cid:1). While
risk is precisely Θ(cid:0) 1
IPS and DR exhibit the same minimax rates, Eq. (6) also
immediately shows that DR will be better than IPS whenever
ˆr is even moderately good (better than ˆr ≡ 0).

n (Eµ[ρ2σ2] + Eµ[ρ2R2

Comparison with asymptotic optimality results: As dis-
cussed in Section 1, previous work on optimal off-policy
evaluation, speciﬁcally the average treatment estimation, as-
sumes that it is possible to consistently estimate r∗(x, a) =
E[r | x, a]. Under such an assumption it is possible to
(asymptotically) match the risk of DR with the perfect
reward estimator ˆr ≡ r(cid:63), and this is the best possi-
ble asymptotic risk (Hahn, 1998). This optimal risk is
(cid:0)Eµ[ρ2σ2] + Varx∼DEπ[r∗ | x](cid:1), corresponding to the
1
n
ﬁrst two terms of Eq. (6), with no dependence on Rmax.
Several estimators achieve this risk, including the multi-
plicative constant, under various consistency assumptions
(Hahn, 1998; Hirano et al., 2003; Imbens et al., 2007; Rothe,
2016). Note that this is strictly below our lower bound for
continuous λ. That is, consistency assumptions yield a bet-
ter asymptotic risk than possible in the agnostic setting. The

gap in constants between our upper and lower bounds is due
to the ﬁnite-sample setting, where lower-order terms cannot
be ignored, but have to be explicitly bounded. Indeed, apart
from the result of Li et al. (2015), discussed below, ours is
the ﬁrst ﬁnite-sample lower bound for off-policy evaluation.

Comparison with multi-armed bandits: For multi-armed
bandits, equivalent to contextual bandits with a single con-
text, Li et al. (2015) show that the minimax risk equals
Θ(Eµ[ρ2σ2]/n) and is achieved, e.g., by DM, whereas IPS
is suboptimal. They also obtain a similar result for con-
textual bandits, assuming that each context appears with a
large-enough probability to estimate its associated rewards
by empirical averages (amounting to realizability). While
we obtain a larger lower bound, this is not a contradiction,
because we allow arbitrarily small probabilities of individ-
ual contexts and even continuous distributions, where the
probability of any single context is zero.

On a closer inspection, the ﬁrst term of our bound in The-
orem 1 coincides with the lower bound of Li et al. (2015)
(up to constants). The second term (optimized over γ) is
non-zero only if there are contexts with small probabilities
relative to the number of samples. In multi-armed bandits,
we recover the bound of Li et al. (2015). When the context
distribution is continuous, or the probability of seeing re-
peated contexts in a data set of size n is small, we get the
minimax optimality of IPS.

One of our key contributions is to highlight this agnostic con-
textual regime where IPS is optimal. In the non-contextual
regime, where each context appears frequently, the rewards
for each context-action pair can be consistently estimated
by empirical averages. Similarly, the asymptotic results
discussed earlier focus on a setting where rewards can be
consistently estimated thanks to parametric assumptions or
smoothness (for non-parametric estimation), with the goal
of asymptotic efﬁciency. Our work complements that line
of research. In many practical situations, we wish to evalu-
ate policies on high-dimensional context spaces, where the
consistent estimation of rewards is not a feasible option. In
other words, the agnostic contextual regime dominates.

The distinction between the contextual and non-contextual
regime is also present in our proof, which combines a non-
contextual lower bound due to the reward noise, similar to
the analysis of Li et al. (2015), and an additional bound
arising for non-degenerate context distributions. This latter
result is a key technical novelty of our paper.

Proof sketch: We only sketch some of the main ideas
here and defer the full proof to Appendix A. For simplicity,
we discuss the case where λ is a continuous distribution.
We consider two separate problem instances corresponding
to the two terms in Theorem 1. The ﬁrst part is relatively
straightforward and reduces the problem to Gaussian mean

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

estimation. We focus on the second part which depends on
Rmax. Our construction deﬁnes a prior over the reward dis-
tributions, D(r | x, a). Given any (x, a), a problem instance
is given by

E[r | x, a] = η(x, a) =

(cid:40)

Rmax(x, a) w.p. θ(x, a),
0

w.p. 1 − θ(x, a),

for θ(x, a) to be appropriately chosen. Once η is drawn, we
consider a problem instance deﬁned by η where the rewards
are deterministic and the only randomness is in the contexts.
In order to lower bound the MSE across all problems, it
sufﬁces to lower bound Eθ[MSEη(ˆv)]. That is, we can
compute the MSE of an estimator for each individual η, and
take expectation of the MSEs under the prior prescribed
by θ. If the expectation is large, we know that there is a
problem instance where the estimator incurs a large MSE.

A key insight in our proof is that this expectation can be
lower bounded by MSEEθ[η(x,a)](ˆv), corresponding to the
MSE of a single problem instance with the actual rewards,
rather than η(x, a), drawn according to θ and with the mean
reward function Eθ[η(x, a)]. This is powerful, since this
new problem instance has stochastic rewards, just like Gaus-
sian mean estimation, and is amenable to standard tech-
niques. The lower bound by MSEEθ[η(x,a)](ˆv) is only valid
when the context distribution λ is rich enough (e.g., contin-
uous). In that case, our reasoning shows that with enough
randomness in the context distribution, a problem with even
a deterministic reward function is extremely challenging.

4. Incorporating Reward Models

As discussed in the previous section, it is generally possible
to beat our minimax bound when consistent reward models
exist. We also argued that even in the absence of a consistent
model, when DR and IPS both achieve optimal risk rates,
the performance of DR on ﬁnite samples will be better than
IPS as long as the reward model is even moderately good
(see Eq. 6). However, under a large reward noise σ, DR may
still suffer from high variance when the importance weights
are large, even when given a perfect reward model. In this
section, we derive a class of estimators that leverage reward
models to directly address this source of high variance, in a
manner very different from the standard DR approach.

further and propose to estimate the rewards for actions by
two distinct strategies, based on whether they have a large
or a small importance weight in a given context. When
importance weights are small, we continue to use our fa-
vorite unbiased estimators, but switch to directly applying
the (potentially biased) reward model on actions with large
importance weights. Here, “small” and “large” are deﬁned
via a threshold parameter τ . Varying this parameter be-
tween 0 and ∞ leads to a family of estimators which we call
the SWITCH estimators as they switch between an agnostic
approach (such as DR or IPS) and the direct method.

We now formalize this intuition, and begin by decomposing
vπ according to importance weights:
Eπ[r] = Eπ[r1(ρ ≤ τ )] + Eπ[r1(ρ > τ )]

= Eµ[ρr1(ρ ≤ τ )]

+ Ex∼λ

(cid:104)(cid:88)

(cid:105)
ED[r | x, a] π(a | x) 1(ρ(x, a)>τ )

.

a∈A

Conceptually, we split our problem into two. The ﬁrst prob-
lem always has small importance weights, so we can use
unbiased estimators such as IPS or DR. The second problem,
where importance weights are large, is addressed by DM.
Writing this out leads to the following estimator:

ˆvSWITCH =

[riρi1(ρi ≤ τ )]

1
n

n
(cid:88)

i=1

n
(cid:88)

(cid:88)

i=1

a∈A

+

1
n

ˆr(xi, a)π(a | xi)1(ρ(xi, a) > τ ).

(7)

Note that the above estimator speciﬁcally uses IPS on the
ﬁrst part of the problem. When DR is used instead of IPS,
we refer to the resulting estimator as SWITCH-DR. The
reward model used within the DR part of the SWITCH-DR
estimator can be the same or different from the reward model
used to impute rewards in the second part. We next present
a bound on the MSE of the SWITCH estimator using IPS. A
similar bound holds for SWITCH-DR.
Theorem 2. Let (cid:15)(a, x) := ˆr(a, x) − E[r|a, x] be the bias
of ˆr and assume ˆr(x, a) ∈ [0, Rmax(x, a)] almost surely.
Then for ˆvSWITCH, with τ > 0, the MSE is at most
2
n

max1(ρ > τ )(cid:3) (cid:111)

(cid:1) ρ21(ρ ≤ τ )(cid:3) + Eπ

(cid:2)(cid:0)σ2+R2

(cid:2)R2

Eµ

max

(cid:110)

+ Eπ

(cid:2)(cid:15)1(ρ > τ )(cid:3)2

.

4.1. The SWITCH Estimators

Our starting point is the observation that insistence on main-
taining unbiasedness puts the DR estimator at one extreme
end of the bias-variance tradeoff. Prior works have con-
sidered ideas such as truncating the rewards or importance
weights when the importance weights are large (see, e.g.,
Bottou et al. 2013), which can dramatically reduce the vari-
ance at the cost of a little bias. We take the intuition a step

The proposed estimator interpolates between DM and IPS.
For τ = 0, SWITCH coincides with DM, while τ → ∞
yields IPS. Consequently, SWITCH estimator is minimax
optimal when τ is appropriately chosen. However, unlike
IPS and DR, the SWITCH and SWITCH-DR estimators are
by design more robust to large (or heavy-tailed) importance
weights. Several estimators related to SWITCH have been
previously studied:

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

1. Bottou et al. (2013) consider a special case of SWITCH
with ˆr ≡ 0, meaning that all the actions with large
importance weights are eliminated from IPS. We refer
to this method as Trimmed IPS.

2. Thomas & Brunskill (2016) study an estimator similar
to SWITCH in the more general setting of reinforcement
learning. Their MAGIC estimator can be seen as using
several candidate thresholds τ and then evaluating the
policy by a weighted sum of the estimators correspond-
ing to each τ . Similar to our approach of automatically
determining τ , they determine the weighting of estima-
tors via optimization (as we discuss below).

4.2. Automatic Parameter Tuning

So far we have discussed the properties of the SWITCH esti-
mators assuming that the parameter τ is chosen well. Our
goal is to obtain the best of IPS and DM, but a poor choice
of τ might easily give us the worst of the two estimators.
Therefore, a method for selecting τ plays an essential role.
A natural criterion would be to pick τ that minimizes the
MSE of the resulting estimator. Since we do not know the
precise MSE (as vπ is unknown), an alternative is to mini-
mize its data-dependent estimate. Recalling that the MSE
can be written as the sum of variance and squared bias, we
estimate and bound the terms individually.

Recall that we are working with a data set (xi, ai, ri) and
ρi := π(ai | xi)/µ(ai | xi). Using this data, it is straight-
forward to estimate the variance of the SWITCH estimator.
Let Yi(τ ) denote the estimated value that π obtains on the
data point xi according to the SWITCH estimator with the
threshold τ , that is

bound the squared bias as

Eπ

(cid:2)(cid:15)1(ρ > τ )(cid:3)2

≤ Eπ

(cid:2)Rmax1(ρ > τ )(cid:3)2

.

Replacing the expectation with an average, we obtain

2
τ :=
(cid:100)Bias

(cid:20) 1
n

n
(cid:88)

i=1

Eπ

(cid:2)Rmax1(ρ > τ ) (cid:12)

(cid:12) xi

(cid:21)2
(cid:3)

.

With these estimates, we pick the threshold (cid:98)τ by optimizing
the sum of estimated variance and the upper bound on bias,

(cid:98)τ := argmin

τ

(cid:100)Varτ + (cid:100)Bias

2
τ .

(9)

Our upper bound on the bias is rather conservative, as it
upper bounds the error of DM at the largest possible value
for every data point. This has the effect of favoring the
use of the unbiased part in SWITCH whenever possible,
unless the variance would overwhelm even an arbitrarily
biased DM. This conservative choice, however, immediately
implies the minimax optimality of the SWITCH estimator
using (cid:98)τ , because the incurred bias is no more than our upper
bound, and it is incurred only when the minimax optimal
IPS estimator would be suffering an even larger variance.

Our automatic tuning is related to the MAGIC estimator
of Thomas & Brunskill (2016). The key differences are
that we pick only one threshold τ , while they combine
the estimates with many different τ s using a weighting
function. They pick this weighting function by optimizing a
bias-variance tradeoff, but with signiﬁcantly different bias
and variance estimators. In our experiments, the automatic
tuning using Eq. (9) generally works better than MAGIC.

Yi(τ ) := riρi1(ρi≤τ )+

ˆr(xi, a)π(a|xi)1(ρ(xi, a)>τ ),

5. Experiments

(cid:88)

a∈A

and ¯Y (τ ) := 1
n
xi are i.i.d., the variance can be estimated as

i=1 Yi(τ ). Since ˆvSWITCH = ¯Y (τ ) and the

(cid:80)n

Var( ¯Y (τ )) ≈

(Yi(τ ) − ¯Y (τ ))2 =: (cid:100)Varτ ,

(8)

1
n2

n
(cid:88)

i=1

where the approximation above is clearly consistent since
the random variables Yi are appropriately bounded as long
as the rewards are bounded, because the importance weights
are capped at the threshold τ .

Next we turn to the bias term. For understanding bias,
we look at the MSE bound in Theorem 2, and observe
that the last term in that theorem is precisely the squared
bias. Rather than using a direct bias estimate, which would
require knowledge of the error in ˆr, we will upper bound
this term. We assume that the function Rmax(x, a) is known.
This is not limiting since in most practical applications an a
priori bound on the rewards is known. Then we can upper

We next empirically evaluate the proposed SWITCH estima-
tors on the 10 UCI data sets previously used for off-policy
evaluation (Dudík et al., 2011). We convert the multi-class
classiﬁcation problem to contextual bandits by treating the
labels as actions for a policy µ, and recording the reward of
1 if the correct label is chosen, and 0 otherwise.

In addition to this deterministic reward model, we also con-
sider a noisy reward model for each data set, which reveals
the correct reward with probability 0.5 and outputs a ran-
dom coin toss otherwise. Theoretically, this should lead
to bigger σ2 and larger variance in all estimators. In both
reward models, Rmax ≡ 1 is a valid bound.

The target policy π is the deterministic decision of a logistic
regression classiﬁer learned on the multi-class data, while
the logging policy µ samples according to the probability
estimates of a logistic model learned on a covariate-shifted
version of the data. The covariate shift is obtained as in
prior work (Dudík et al., 2011; Gretton et al., 2009).

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

(a) Deterministic reward

(b) Noisy reward

Figure 1. The number of UCI data sets where each method achieves at least a given Rel. MSE. On the left, the UCI labels are used as is;
on the right, label noise is added. Curves towards top-left achieve smaller MSE in more cases. Methods in dashed lines are “cheating”
by choosing the threshold τ to optimize test MSE. SWITCH-DR outperforms baselines and our tuning of τ is not too far from the best
possible. Each data set uses an n which is the size of the data set, drawn via bootstrap sampling and results are averaged over 500 trials.

(a) yeast / deterministic reward

(b) yeast / noisy reward

(c) optdigits / deterministic reward

(d) optdigits / noisy reward

Figure 2. MSE of different methods as a function of input data size. Top: optdigits data set. Bottom: yeast data set.

Relative error w.r.t. IPS10-1100101Number of data sets012345678910Relative error w.r.t. IPS10-1100101Number of data sets012345678910IPSDMDRSWITCH-DRoracle-SWITCH-DRoracle-Trim/TrunIPSSWITCH-DR-magicIPSDMDRSWITCH-DRoracle-SWITCH-DRoracle-Trim/TrunIPSSWITCH-DR-magicn102103MSE0.00410-20.020.0510-1yeast: n = 1484, d = 8, k = 10n102103MSE0.00410-20.020.0510-1yeast: n = 1484, d = 8, k = 10n102103MSE0.150.20.30.40.50.65optdigits: n = 5620, d = 64, k = 10n102103MSE0.020.0510-10.2optdigits: n = 5620, d = 64, k = 10Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

In each data set with n examples, we treat the uniform distri-
bution over the data set itself as a surrogate of the population
distribution so that we know the ground truth of the re-
wards. Then, in the simulator, we randomly draw i.i.d. data
sets of size 100, 200, 500, 1000, 2000, 5000, 10000, . . . un-
til reaching n, with 500 different repetitions of each size.
We estimate MSE of each estimator by taking the empir-
ical average of the squared error over the 500 replicates;
note that we can calculate the squared error exactly, because
we know vπ. For some of the methods, e.g., IPS and DR,
the MSE can have a very large variance due to the poten-
tially large importance weights. This leads to very large
error bars if we estimate their MSE even with 500 repli-
cates. To circumvent this issue, we report a clipped version
of the MSE that truncates the squared error to 1, namely
MSE = E[(ˆv − vπ)2 ∧ 1]. This allows us to get valid conﬁ-
dence intervals for our empirical estimates of this quantity.
Note that this does not change the MSE estimate of our
approach at all, but is signiﬁcantly more favorable towards
IPS and DR. In this section, whenever we refer to “MSE”,
we are referring to this truncated version.

We compare SWITCH and SWITCH-DR against the follow-
ing baselines: 1. IPS; 2. DM trained via logistic regression;
3. DR; 4. Truncated and Reweighted IPS (TrunIPS); and
5. Trimmed IPS (TrimIPS).

In DM, we train ˆr and then evaluate the policy on the same
contextual bandit data set. Following Dudík et al. (2011),
DR is constructed by randomly splitting the contextual ban-
dit data into two folds, estimating ˆr on one fold, and then
evaluating π on the other fold and vice versa, obtaining
two estimates. The ﬁnal estimate is the average of the two.
TrunIPS is a variant of IPS, where importance weights are
capped at a threshold τ and then renormalized to sum to
one (see, e.g., Bembom & van der Laan, 2008). TrimIPS is a
special case of SWITCH due to Bottou et al. (2013) described
earlier, where ˆr ≡ 0.

For SWITCH and SWITCH-DR as well as TrunIPS and Trim-
IPS we select the parameter τ by our automatic tuning from
Section 4.2. To evaluate our tuning approach, we also in-
clude the results for the τ tuned optimally in hindsight,
which we refer to as the oracle setting, and also show re-
sults obtained by the multi-threshold MAGIC approach. In
all these approaches we optimize among 21 possible thresh-
olds, from an exponential grid between the smallest and the
largest importance weight observed in the data, considering
all actions in each observed context.

In order to stay comparable across data sets and data sizes,
our performance measure is the relative MSE with re-
spect to the IPS. Thus, for each estimator ˆv, we calculate
Rel. MSE(ˆv) = MSE(ˆv)

MSE(ˆvIPS) .

The results are summarized in Figure 1, plotting the number

of data sets where each method achieves at least a given
relative MSE.4 Thus, methods that achieve smaller MSE
across more data sets are towards the top-left corner of
the plot, and a larger area under the curve indicates better
performance. Some of the differences in MSE are several
orders of magnitude large since the relative MSE is shown
on the logaritmic scale. As we see, SWITCH-DR dominates
all baselines and our empirical tuning of τ is not too far from
the best possible. The automatic tuning by MAGIC tends
to revert to DM, because its bias estimate is too optimistic
and so DM is preferred whenever IPS or DR have some
signiﬁcant variance. The gains of SWITCH-DR are even
greater in the noisy-reward setting, where we add label
noise to UCI data.

In Figure 2, we illustrate the convergence of MSE as n in-
creases. We select two data sets and show how SWITCH-DR
performs against baselines in two typical cases: (i) when the
direct method works well initially but is outperformed by
IPS and DR as n gets large, and (ii) when the direct method
works poorly. In the ﬁrst case, SWITCH-DR outperforms
both DM and IPS, while DR improves over IPS only mod-
erately. In the second case, SWITCH-DR performs about
as well as IPS and DR despite a poor performance of DM.
In all cases, SWITCH-DR is robust to additional noise in
the reward, while IPS and DR suffer from higher variance.
Results for the remaining data sets are in Appendix D.

6. Conclusion

In this paper we have carried out minimax analysis of off-
policy evaluation in contextual bandits and showed that
IPS and DR are minimax optimal in the worst-case, when
no consistent reward model is available. This result com-
plements existing asymptotic theory with assumptions on
reward models, and highlights the differences between ag-
nostic and consistent settings. Practically, the result further
motivates the importance of using side information, possibly
by modeling rewards directly, especially when importance
weights are too large. Given this observation, we propose
a new class of estimators called SWITCH that can be used
to combine any importance weighting estimators, including
IPS and DR, with DM. The estimators adaptively switch
between DM when the importance weights are large and
either IPS or DR when the importance weights are small.
We show that the new estimators have favorable theoretical
properties and also work well on real-world data. Many in-
teresting directions remain open for future work, including
high-probability upper bounds on the ﬁnite-sample MSE of
SWITCH estimators, as well as sharper ﬁnite-sample lower
bounds under realistic assumptions on the reward model.

4For clarity, we have excluded SWITCH, which signiﬁcantly
outperforms IPS, but is dominated by SWITCH-DR. Similarly, we
only report the better of oracle-TrimIPS and oracle-TrunIPS.

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Hoeffding, Wassily. Probability inequalities for sums of
bounded random variables. Journal of the American
Statistical Association, 58(301):13–30, 1963.

Horvitz, Daniel G and Thompson, Donovan J. A gener-
alization of sampling without replacement from a ﬁnite
universe. Journal of the American Statistical Association,
47(260):663–685, 1952.

Imbens, Guido, Newey, Whitney, and Ridder, Geert. Mean-
squared-error calculations for average treatment effects.
Technical report, 2007.

Lafferty, John, Liu, Han, and Wasserman, Larry. Minimax
theory, 2008. URL http://www.stat.cmu.edu/
~larry/=sml/Minimax.pdf.

Li, Lihong, Munos, Rémi, and Szepesvári, Csaba. Toward
minimax off-policy value estimation. In AISTATS, 2015.

Robins, James M and Rotnitzky, Andrea. Semiparametric
efﬁciency in multivariate regression models with missing
data. Journal of the American Statistical Association, 90
(429):122–129, 1995.

Rothe, Christoph. The value of knowing the propensity
score for estimating average treatment effects. IZA Dis-
cussion Paper Series, 2016.

Sierpi´nski, Wacław. Sur les fonctions d’ensemble additives
et continues. Fundamenta Mathematicae, 3:240–246,
1922.

Thomas, Philip S and Brunskill, Emma. Data-efﬁcient off-
policy policy evaluation for reinforcement learning. In
ICML, 2016.

Acknowledgments

The work was partially completed during YW’s internship at
Microsoft Research NYC from May 2016 to Aug 2016. The
authors would like to thank Lihong Li and John Langford
for helpful discussions, Edward Kennedy for bringing our
attention to related problems and recent developments in
causal inference, and an anonymous reviewer for pointing
out relevant econometric references and providing valuable
feedback that helped connect our work with research on
average treatment effects.

References

Bang, Heejung and Robins, James M. Doubly robust es-
timation in missing data and causal inference models.
Biometrics, 61(4):962–973, 2005.

Bembom, Oliver and van der Laan, Mark J. Data-adaptive
selection of the truncation level for inverse-probability-
of-treatment-weighted estimators. 2008.

Bertin, Karine et al. Asymptotically exact minimax estima-
tion in sup-norm for anisotropic Hölder classes. Bernoulli,
10(5):873–888, 2004.

Bottou, Léon, Peters, Jonas, Candela, Joaquin Quinonero,
Charles, Denis Xavier, Chickering, Max, Portugaly, Elon,
Ray, Dipankar, Simard, Patrice Y, and Snelson, Ed. Coun-
terfactual reasoning and learning systems: the example of
computational advertising. Journal of Machine Learning
Research, 14(1):3207–3260, 2013.

Cassel, Claes M, Särndal, Carl E, and Wretman, Jan H.
Some results on generalized difference estimation and
generalized regression estimation for ﬁnite populations.
Biometrika, 63(3):615–620, 1976.

Dudík, Miroslav, Langford, John, and Li, Lihong. Doubly
robust policy evaluation and learning. In ICML, 2011.

Dudík, Miroslav, Erhan, Dumitru, Langford, John, and Li,
Lihong. Doubly robust policy evaluation and optimiza-
tion. Statistical Science, 29(4):485–511, 2014.

Gretton, Arthur, Smola, Alex, Huang, Jiayuan, Schmittfull,
Marcel, Borgwardt, Karsten, and Schölkopf, Bernhard.
Covariate shift by kernel mean matching. Dataset Shift
in Machine Learning, 3(4):5, 2009.

Hahn, Jinyong. On the role of the propensity score in ef-
ﬁcient semiparametric estimation of average treatment
effects. Econometrica, pp. 315–331, 1998.

Hirano, Keisuke, Imbens, Guido W, and Ridder, Geert. Ef-
ﬁcient estimation of average treatment effects using the
estimated propensity score. Econometrica, 71(4):1161–
1189, 2003.

