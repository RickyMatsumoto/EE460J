Dissipativity Theory for Nesterov’s Accelerated Method

Bin Hu 1 Laurent Lessard 1

Abstract

In this paper, we adapt the control theoretic con-
cept of dissipativity theory to provide a natural
understanding of Nesterov’s accelerated method.
Our theory ties rigorous convergence rate anal-
ysis to the physically intuitive notion of energy
dissipation. Moreover, dissipativity allows one to
efﬁciently construct Lyapunov functions (either
numerically or analytically) by solving a small
semideﬁnite program. Using novel supply rate
functions, we show how to recover known rate
bounds for Nesterov’s method and we generalize
the approach to certify both linear and sublinear
rates in a variety of settings. Finally, we link the
continuous-time version of dissipativity to recent
works on algorithm analysis that use discretiza-
tions of ordinary differential equations.

1. Introduction

Nesterov’s accelerated method (Nesterov, 2003) has gar-
nered attention and interest in the machine learning com-
munity because of its fast global convergence rate guaran-
tees. The original convergence rate proofs of Nesterov’s
accelerated method are derived using the method of es-
timate sequences, which has proven difﬁcult to interpret.
This observation motivated a sequence of recent works
on new analysis and interpretations of Nesterov’s acceler-
ated method (Bubeck et al., 2015; Lessard et al., 2016; Su
et al., 2016; Drusvyatskiy et al., 2016; Flammarion & Bach,
2015; Wibisono et al., 2016; Wilson et al., 2016).

Many of these recent papers rely on Lyapunov-based sta-
bility arguments. Lyapunov theory is an analogue to the
principle of minimum energy and brings a physical intu-
ition to convergence behaviors. When applying such proof
techniques, one must construct a Lyapunov function, which
is a nonnegative function of the algorithm’s state (an “in-
ternal energy”) that decreases along all admissible trajec-

1University of Wisconsin–Madison, Madison, WI 53706,

USA. Correspondence to: Bin Hu <bhu38@wisc.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

tories. Once a Lyapunov function is found, one can relate
the rate of decrease of this internal energy to the rate of
convergence of the algorithm. The main challenge in ap-
plying Lyapunov’s method is ﬁnding a suitable Lyapunov
function.

There are two main approaches for Lyapunov function con-
structions. The ﬁrst approach adopts the integral quadratic
constraint (IQC) framework (Megretski & Rantzer, 1997)
from control theory and formulates a linear matrix equal-
ity (LMI) whose feasibility implies the linear convergence
of the algorithm (Lessard et al., 2016). Despite the gen-
erality of the IQC approach and the small size of the as-
sociated LMI, one must typically resort to numerical sim-
ulations to solve the LMI. The second approach seeks an
ordinary differential equation (ODE) that can be appropri-
ately discretized to yield the algorithm of interest. One can
then gain intuition about the trajectories of the algorithm
by examining trajectories of the continuous-time ODE (Su
et al., 2016; Wibisono et al., 2016; Wilson et al., 2016).
The work of Wilson et al. (2016) also establishes a gen-
eral equivalence between Lyapunov functions and estimate
sequence proofs.

In this paper, we bridge the IQC approach (Lessard et al.,
2016) and the discretization approach (Wilson et al., 2016)
by using dissipativity theory (Willems, 1972a;b). The term
“dissipativity” is borrowed from the notion of energy dis-
sipation in physics and the theory provides a general ap-
proach for the intuitive understanding and construction
of Lyapunov functions. Dissipativity for quadratic Lya-
punov functions in particular (Willems, 1972b) has seen
widespread use in controls. In the sequel, we tailor dissi-
pativity theory to the automated construction of Lyapunov
functions, which are not necessarily quadratic, for the anal-
ysis of optimization algorithms. Our dissipation inequality
leads to an LMI condition that is simpler than the one in
Lessard et al. (2016) and hence more amenable to being
solved analytically. When specialized to Nesterov’s accel-
erated method, our LMI recovers the Lyapunov function
proposed in Wilson et al. (2016). Finally, we extend our
LMI-based approach to the sublinear convergence analy-
sis of Nesterov’s accelerated method in both discrete and
continuous time domains. This complements the original
LMI-based approach in Lessard et al. (2016), which mainly
handles linear convergence rate analyses.

Dissipativity Theory for Nesterov’s Accelerated Method

An LMI-based approach for sublinear rate analysis simi-
lar to ours was independently and simultaneously proposed
by Fazlyab et al. (2017). While this work and the present
work both draw connections to the continuous-time results
mentioned above, different algorithms and function classes
are emphasized. For example, Fazlyab et al. (2017) de-
velops LMIs for gradient descent and proximal/projection-
based variants with convex/quasi-convex objective func-
tions.
In contrast, the present work develops LMIs tai-
lored to the analysis of discrete-time accelerated methods
and Nesterov’s method in particular.

2. Preliminaries

2.1. Notation

Let R and R+ denote the real and nonnegative real num-
bers, respectively. Let Ip and 0p denote the p × p identity
and zero matrices, respectively. The Kronecker product of
two matrices is denoted A ⊗ B and satisﬁes the properties
(A⊗B)T = AT⊗BT and (A⊗B)(C⊗D) = (AC)⊗(BD)
when the matrices have compatible dimensions. Matrix
inequalities hold in the semideﬁnite sense unless other-
wise indicated. A differentiable function f : Rp → R
is m-strongly convex if f (x) ≥ f (y) + ∇f (y)T(x −
2 (cid:107)x − y(cid:107)2 for all x, y ∈ Rp and is L-smooth if
y) + m
(cid:107)∇f (x)−∇f (y)(cid:107) ≤ L(cid:107)x−y(cid:107) for all x, y ∈ Rp. Note that
f is convex if f is 0-strongly convex. We use x(cid:63) to denote
a point satisfying ∇f (x(cid:63)) = 0. When f is L-smooth and
m-strongly convex, x(cid:63) is unique.

2.2. Classical Dissipativity Theory

Consider a linear dynamical system governed by the state-
space model

ξk+1 = Aξk + Bwk.

(1)

Here, ξk ∈ Rnξ is the state, wk ∈ Rnw is the input, A ∈
Rnξ×nξ is the state transition matrix, and B ∈ Rnξ×nw
is the input matrix. The input wk can be physically in-
terpreted as a driving force. Classical dissipativity theory
describes how the internal energy stored in the state ξk
evolves with time k as one applies the input wk to drive
the system. A key concept in dissipativity theory is the
supply rate, which characterizes the energy change in ξk
due to the driving force wk. The supply rate is a function
S : Rnξ × Rnw → R that maps any state/input pair (ξ, w)
to a scalar measuring the amount of energy delivered from
w to state ξ. Now we introduce the notion of dissipativity.

Deﬁnition 1 The dynamical system (1) is dissipative with
respect to the supply rate S if there exists a function V :
Rnξ → R+ such that V (ξ) ≥ 0 for all ξ ∈ Rnξ and

V (ξk+1) − V (ξk) ≤ S(ξk, wk)

(2)

for all k. The function V is called a storage function, which
quantiﬁes the energy stored in the state ξ. In addition, (2)
is called the dissipation inequality.

The dissipation inequality (2) states that the change of the
internal energy stored in ξk is equal to the difference be-
tween the supplied energy and the dissipated energy. Since
there will always be some energy dissipating from the sys-
tem, the change in the stored energy (which is exactly
V (ξk+1) − V (ξk)) is always bounded above by the en-
ergy supplied to the system (which is exactly S(ξk, wk)).
A variant of (2) known as the exponential dissipation in-
equality states that for some 0 ≤ ρ < 1, we have

V (ξk+1) − ρ2V (ξk) ≤ S(ξk, wk),

(3)

which states that at least a fraction (1 − ρ2) of the internal
energy will dissipate at every step.

The dissipation inequality (3) provides a direct way to con-
struct a Lyapunov function based on the storage function.
It is often the case that we have prior knowledge about how
the driving force wk is related to the state ξk. Thus, we may
know additional information about the supply rate function
S(ξk, wk). For example, if S(ξk, wk) ≤ 0 for all k then (3)
directly implies that V (ξk+1) ≤ ρ2V (ξk), and the storage
function V can serve as a Lyapunov function. The condi-
tion S(ξk, wk) ≤ 0 means that the driving force wk does
not inject any energy into the system and may even extract
energy out of the system. Then, the internal energy will
decrease no slower than the linear rate ρ2 and approach a
minimum value at equilibrium.

An advantage of dissipativity theory is that for any
quadratic supply rate, one can automatically construct the
dissipation inequality using semideﬁnite programming. We
now state a standard result from the controls literature.

Theorem 2 Consider the following quadratic supply rate
with X ∈ R(nξ+nw)×(nξ+nw) and X = X T.

S(ξ, w) :=

(cid:21)T

(cid:20) ξ
w

X

(cid:21)

(cid:20) ξ
w

.

(4)

If there exists a matrix P ∈ Rnξ×nξ with P ≥ 0 such that

(cid:20)ATP A − ρ2P ATP B
BTP B

BTP A

(cid:21)

− X ≤ 0,

(5)

then the dissipation inequality (3) holds for all trajectories
of (1) with V (ξ) := ξTP ξ.

Proof. Based on the state-space model (1), we have

V (ξk+1) = ξT

k+1P ξk+1

= (Aξk + Bwk)TP (Aξk + Bwk)
(cid:21)T (cid:20)ATP A ATP B
(cid:21) (cid:20) ξk
BTP A BTP B
wk

(cid:20) ξk
wk

=

(cid:21)

.

Dissipativity Theory for Nesterov’s Accelerated Method

Hence we can left and right multiply (5) by (cid:2)ξT
(cid:2)ξT

, and directly obtain the desired conclusion.

k wT
k

(cid:3)T

k wT
k

(cid:3) and

Hence for any 0 ≤ ρ < 1, we have p(cid:107)xk+1 − x(cid:63)(cid:107)2 ≤
ρ2p(cid:107)xk − x(cid:63)(cid:107)2 if there exists p ≥ 0 such that

The left-hand side of (5) is linear in P , so (5) is a linear
matrix inequality (LMI) for any ﬁxed A, B, X, ρ. The set
of P such that (5) holds is therefore a convex set and can be
efﬁciently searched using interior point methods, for exam-
ple. To apply the dissipativity theory for linear convergence
rate analysis, one typically follows two steps.

1. Choose a proper quadratic supply rate function S sat-
isfying certain desired properties, e.g. S(ξk, wk) ≤ 0.

2. Solve the LMI (5) to obtain a storage function V ,
which is then used to construct a Lyapunov function.

In step 2, the LMI obtained is typically very small, e.g. 2×2
or 3 × 3, so we can often solve the LMI analytically. For
illustrative purposes, we rephrase the existing LMI analysis
of the gradient descent method (Lessard et al., 2016, §4.4)
using the notion of dissipativity.

2.3. Example: Dissipativity for Gradient Descent

There is an intrinsic connection between dissipativity the-
ory and the IQC approach (Megretski et al., 2010; Seiler,
2015). The IQC analysis of the gradient descent method
in Lessard et al. (2016) may be reframed using dissipativ-
ity theory. Then, the pointwise IQC (Lessard et al., 2016,
Lemma 6) amounts to using a quadratic supply rate S with
S ≤ 0. Speciﬁcally, assume f is L-smooth and m-strongly
convex, and consider the gradient descent method

xk+1 = xk − α∇f (xk).

(6)

We have xk+1 − x(cid:63) = xk − x(cid:63) − α∇f (xk), where x(cid:63) is the
unique point satisfying ∇f (x(cid:63)) = 0. Deﬁne ξk := xk − x(cid:63)
and wk := ∇f (xk). Then the gradient descent method
is modeled by (1) with A := Ip and B := −αIp. Since
wk = ∇f (ξk + x(cid:63)), we can deﬁne the following quadratic
supply rate

S(ξk, wk) =

(cid:21)T (cid:20)

(cid:20) ξk
wk

2mLIp
−(m + L)Ip

−(m + L)Ip
2Ip

(cid:21)

(cid:21) (cid:20) ξk
wk

(cid:20)(1 − ρ2)p −αp
(cid:21)
α2p

−αp

+

(cid:20)−2mL m + L
(cid:21)
m + L

−2

≤ 0

(8)

The LMI (8) is simple and can be analytically solved to
recover the existing rate results for the gradient descent
For example, we can choose (α, ρ, p) to be
method.
L+m , L−m
L , 1 − m
( 1
2 (L + m)2) to immedi-
L , L2) or (
ately recover the standard rate results in Polyak (1987).

L+m , 1

2

Based on the example above, it is evident that choosing a
proper supply rate is critical for the construction of a Lya-
punov function. The supply rate (7) turns out to be inad-
equate for the analysis of Nesterov’s accelerated method.
For Nesterov’s accelerated method, the dependence be-
tween the internal energy and the driving force is more
complicated due to the presence of momentum terms. We
will next develop a new supply rate that captures this com-
plicated dependence. We will also make use of this new
supply rate to recover the standard linear rate results for
Nesterov’s accelerated method.

3. Dissipativity for Accelerated Linear Rates

3.1. Dissipativity for Nesterov’s Method

Suppose f is L-smooth and m-strongly convex with m >
0. Let x(cid:63) be the unique point satisfying ∇f (x(cid:63)) = 0. Now
we consider Nesterov’s accelerated method, which uses the
following iteration rule to ﬁnd x(cid:63):

xk+1 = yk − α∇f (yk),

yk = (1 + β)xk − βxk−1.

(9a)

(9b)

We can rewrite (9) as

(cid:21)

(cid:20)xk+1 − x(cid:63)
xk − x(cid:63)

= A

(cid:21)

(cid:20) xk − x(cid:63)
xk−1 − x(cid:63)

+ Bwk

(10)

where wk := ∇f (yk) = ∇f ((1 + β)xk − βxk−1). Also,
A := ˜A ⊗ Ip, B := ˜B ⊗ Ip, and ˜A, ˜B are deﬁned by

(7)

˜A :=

(cid:20)1 + β −β
0

1

(cid:21)

,

˜B :=

(cid:21)

(cid:20)−α
0

.

(11)

By co-coercivity, we have S(ξk, wk) ≤ 0 for all k. This
just restates Lessard et al. (2016, Lemma 6). Then, we can
directly apply Theorem 2 to construct the dissipation in-
equality. We can parameterize P = p ⊗ Ip and deﬁne the
storage function as V (ξk) = p(cid:107)ξk(cid:107)2 = p(cid:107)xk − x(cid:63)(cid:107)2. The
LMI (5) becomes
(cid:21)
(cid:18)(cid:20)(1 − ρ2)p −αp
α2p

(cid:20)−2mL m + L
m + L

⊗ Ip ≤ 0.

−αp

−2

(cid:21)(cid:19)

+

.

(xk−1 − x(cid:63))T(cid:3)T

Hence, Nesterov’s accelerated method (9) is in the form of
(1) with ξk = (cid:2)(xk − x(cid:63))T
Nesterov’s accelerated method can improve the conver-
gence rate since the input wk depends on both xk and
xk−1, and drives the state in a speciﬁc direction, i.e. along
(1+β)xk −βxk−1. This leads to a supply rate that extracts
energy out of the system signiﬁcantly faster than with gra-
dient descent. This is formally stated in the next lemma.

Dissipativity Theory for Nesterov’s Accelerated Method

Lemma 3 Let f be L-smooth and m-strongly convex with
m > 0. Let x(cid:63) be the unique point satisfying ∇f (x(cid:63)) = 0.
Consider Nesterov’s method (9) or equivalently (10). The
following inequalities hold for all trajectories.

then set P := ˜P ⊗ Ip and deﬁne the Lyapunov function

Vk :=

(cid:21)T

(cid:20) xk − x(cid:63)
xk−1 − x(cid:63)

P

(cid:21)

(cid:20) xk − x(cid:63)
xk−1 − x(cid:63)

+ f (xk) − f (x(cid:63)),

(16)

 ≤ f (xk) − f (xk+1)

which satisﬁes Vk+1 ≤ ρ2Vk for all k. Moreover, we have
f (xk) − f (x(cid:63)) ≤ ρ2kV0 for Nesterov’s method.









xk − x(cid:63)
xk−1 − x(cid:63)
∇f (yk)

xk − x(cid:63)
xk−1 − x(cid:63)
∇f (yk)



T





X1





T





X2







xk − x(cid:63)
xk−1 − x(cid:63)
∇f (yk)

xk − x(cid:63)
xk−1 − x(cid:63)
∇f (yk)

 ≤ f (x(cid:63)) − f (xk+1)

where Xi = ˜Xi ⊗ Ip for i = 1, 2, and ˜Xi are deﬁned by

˜X1 :=

˜X2 :=









1
2

1
2

β2m −β2m
−β2m β2m

−β
β
α(2 − Lα)





β

−β
(1 + β)2m −β(1 + β)m −(1 + β)
β2m
−β(1 + β)m
β
−(1 + β)

β
α(2 − Lα)



 (13)

(12)

Given any 0 ≤ ρ ≤ 1, one can deﬁne the supply rate as
(4) with a particular choice of X := ρ2X1 + (1 − ρ2)X2.
Then this supply rate satisﬁes the condition

S(ξk, wk) ≤ ρ2(f (xk) − f (x(cid:63)))

− (f (xk+1) − f (x(cid:63))).

(14)

Proof. The proof is similar to the proof of (3.23)–(3.24)
in Bubeck (2015), but Bubeck (2015, Lemma 3.6) must be
modiﬁed to account for the strong convexity of f . See the
supplementary material for a detailed proof.

The supply rate (14) captures how the driving force wk is
impacting the future state xk+1. The physical interpreta-
tion is that there is some amount of hidden energy in the
system that takes the form of f (xk) − f (x(cid:63)). The supply
rate condition (14) describes how the driving force wk is
coupled with the hidden energy in the future. It says the
delivered energy is bounded by a weighted decrease of the
hidden energy. Based on this supply rate, one can search
Lyapunov function using the following theorem.

Theorem 4 Let f be L-smooth and m-strongly convex
with m > 0. Let x(cid:63) be the unique point satisfying
∇f (x(cid:63)) = 0. Consider Nesterov’s accelerated method (9).
For any rate 0 ≤ ρ < 1, set ˜X := ρ2 ˜X1+(1−ρ2) ˜X2 where
˜X1 and ˜X2 are deﬁned in (12)–(13). In addition, let ˜A, ˜B
be deﬁned by (11). If there exists a matrix ˜P ∈ R2×2 with
˜P ≥ 0 such that

(cid:20) ˜AT ˜P ˜A − ρ2 ˜P
˜BT ˜P ˜A

(cid:21)

˜AT ˜P ˜B
˜BT ˜P ˜B

− ˜X ≤ 0

(15)

Proof. Take the Kronecker product of (15) and Ip, and
hence (5) holds with A := ˜A ⊗ Ip, B := ˜B ⊗ Ip, and
X := ˜X⊗Ip. Let the supply rate S be deﬁned by (4). Then,
deﬁne the quadratic storage function V (ξk) := ξT
k P ξk
and apply Theorem 2 to show V (ξk+1) − ρ2V (ξk) ≤
S(ξk, wk). Based on the supply rate condition (14), we can
deﬁne the Lyapunov function Vk := V (ξk)+f (xk)−f (x(cid:63))
and show Vk+1 ≤ ρ2Vk. Finally, since P ≥ 0, we have
f (xk) − f (x(cid:63)) ≤ ρ2kV0.

We can immediately recover the proposed Lyapunov func-
tion in Wilson et al. (2016, Theorem 6) by setting ˜P to

˜P =






(cid:113) L
2
(cid:113) L
2

2 −






(cid:113) m

(cid:104)(cid:113) L
2

(cid:113) m

2 −

(cid:105)

(cid:113) L
2

.

(17)

Clearly ˜P ≥ 0. Now deﬁne κ := L
√
κ+1 , and ρ2 = 1 − (cid:112) m
κ−1√
β =
verify that the left side of the LMI (5) is equal to

m . Given α = 1
L ,
L , it is straightforward to

√

m(
2(κ +

κ − 1)3
√
κ)





−1
1
1 −1
0
0


0
0
 ,
0

which is clearly negative semideﬁnite. Hence we can
immediately construct a Lyapunov function using (16) to
prove the linear rate ρ2 = 1 − (cid:112) m
L .

Searching for analytic certiﬁcates such as (17) can either
be carried out by directly analyzing the LMI, or by using
numerical solutions to guide the search. For example, nu-
merically solving (15) for any ﬁxed L and m directly yields
(17), which makes ﬁnding the analytical expression easy.

3.2. Dissipativity Theory for More General Methods

We demonstrate the generality of the dissipativity theory
on a more general variant of Nesterov’s method. Consider
a modiﬁed accelerated method

xk+1 = (1 + β)xk − βxk−1 − α∇f (yk),

yk = (1 + η)xk − ηxk−1.

(18a)

(18b)

When β = η, we recover Nesterov’s accelerated method.
When η = 0, we recover the Heavy-ball method of Polyak

Dissipativity Theory for Nesterov’s Accelerated Method

(1987). We can rewrite (18) in state-space form (10) where
wk := ∇f (yk) = ∇f ((1 + η)xk − ηxk−1), A := ˜A ⊗ Ip,
B := ˜B ⊗ Ip, and ˜A, ˜B are deﬁned by

improved rate bounds. The same is true of supply rates.
For example, we could include λ1, λ2 ≥ 0 as decision vari-
ables and search for a dissipation inequality with supply
rate λ1S1 + λ2S2 where e.g. S1 is (7) and S2 is (21).

˜A :=

(cid:20)1 + β −β
0

1

(cid:21)

,

˜B :=

(cid:21)

(cid:20)−α
0

.

Lemma 5 Let f be L-smooth and m-strongly convex with
m > 0. Let x(cid:63) be the unique point satisfying ∇f (x(cid:63)) = 0.
Consider the general accelerated method (18). Deﬁne the
state ξk := (cid:2)(xk − x(cid:63))T
and the input
wk := ∇f (yk) = ∇f ((1 + η)xk − ηxk−1). Then the
following inequalities hold for all trajectories.

(xk−1 − x(cid:63))T(cid:3)T

4. Dissipativity for Sublinear Rates

The LMI approach in (Lessard et al., 2016) is tailored for
the analysis of linear convergence rates for algorithms that
are time-invariant (the A and B matrices in (1) do not
change with k). We now show that dissipativity theory can
be used to analyze the sublinear rates O(1/k) and O(1/k2)
via slight modiﬁcations of the dissipation inequality.

(cid:21)T

(cid:21)T

(cid:20) ξk
wk
(cid:20) ξk
wk

(cid:21)

(cid:21)

(cid:20) ξk
wk
(cid:20) ξk
wk

(X1 + X2)

≤ f (xk) − f (xk+1)

(19)

4.1. Dissipativity for O(1/k) rates

(X1 + X3)

≤ f (x(cid:63)) − f (xk+1)

(20)

The O(1/k) modiﬁcation, which we present ﬁrst, is very
similar to the linear rate result.

with Xi := ˜Xi ⊗ Ip for i = 1, 2, 3, and ˜Xi are deﬁned by

Theorem 7 Suppose f has a ﬁnite minimum f(cid:63). Consider
the LTI system (1) with a supply rate satisfying

˜X1 :=

˜X2 :=

˜X3 :=

















1
2

1
2

1
2



−(1 − Lα)δ
(1 − Lα)δ
α(2 − Lα)

Lδ2
−Lδ2
(1 − Lα)δ

−Lδ2
Lδ2
−(1 − Lα)δ
η2m −η2m −η
−η2m η2m
η
0
−η
(1 + η)2m −η(1 + η)m −(1 + η)
η2m
−η(1 + η)m
η
−(1 + η)

η
0



η





with δ := β − η. In addition, one can deﬁne the supply
rate as (4) with X := X1 + ρ2X2 + (1 − ρ2)X3. Then for
all trajectories (ξk, wk) of the general accelerated method
(18), this supply rate satisﬁes the inequality

S(ξk, wk) ≤ ρ2(f (xk) − f (x(cid:63)))

− (f (xk+1) − f (x(cid:63))).

(21)

Proof. A detailed proof is presented in the supplementary
material. One mainly needs to modify the proof by taking
the difference between β and η into accounts.

Based the supply rate (21), we can immediately modify
Theorem 4 to handle the more general algorithm (18).
Although we do not have general analytical formulas for
the convergence rate of (18), preliminary numerical results
suggest that there are a family of (α, β, η) leading to the
rate ρ2 = 1 − (cid:112) m
L , and the required value of ˜P is quite
different from (17). This indicates that our proposed LMI
approach could go beyond the Lyapunov function (17).

Remark 6 It is noted in Lessard et al. (2016, §3.2) that
searching over combinations of multiple IQCs may yield

S(ξk, wk) ≤ −(f (zk) − f(cid:63))

(22)

for some sequence {zk}.
If there exists a nonnegative
storage function V such that the dissipation inequality (2)
holds over all trajectories of (ξk, wk), then the following
inequality holds over all trajectories as well.

T
(cid:88)

k=0

(f (zk) − f(cid:63)) ≤ V (ξ0).

(23)

In addition, we have the sublinear convergence rate

min
k:k≤T

(f (zk) − f(cid:63)) ≤

V (ξ0)
T + 1

.

(24)

If f (zk+1) ≤ f (zk) for all k, then (24) implies that
f (zk) − f(cid:63) ≤ V (ξ0)

k+1 for all k.

Proof. By the supply rate condition (22) and the dissipation
inequality (2), we immediately get

V (ξk+1) − V (ξk) + f (zk) − f(cid:63) ≤ 0.

Summing the above inequality from k = 0 to T and using
V ≥ 0 yields the desired result.

To address the sublinear rate analysis, the critical step is
If f is L-smooth
to choose an appropriate supply rate.
and convex, this is easily done. Consider the gradient
method (6) and deﬁne the quantities ξk := xk − x(cid:63),
A := Ip, and B := −αIp as in Section 2.3. Since f is
L-smooth and convex, deﬁne the quadratic supply rate

S(ξk, wk) :=

(cid:20) ξk
wk

(cid:21)T (cid:20) 0p
− 1
2 Ip

− 1
2 Ip
1
2L Ip

(cid:21)

(cid:21) (cid:20) ξk
wk

,

Dissipativity Theory for Nesterov’s Accelerated Method

which satisﬁes S(ξk, wk) ≤ f(cid:63) − f (xk) for all k (co-
coercivity). Then we can directly apply the LMI (5) with
ρ = 1 to construct the dissipation inequality. Setting
P = p ⊗ Ip and deﬁning the storage function as V (ξk) :=
p(cid:107)ξk(cid:107)2 = p(cid:107)xk − x(cid:63)(cid:107)2, the LMI (5) becomes

be proved using the same proof technique in Theorem 2.
Note that we need (28) to simultaneously hold for all k.
This leads to an inﬁnite number of LMIs in general.

Now we consider Nesterov’s accelerated method for a con-
vex L-smooth objective function f (Nesterov, 2003).

Due to the (1, 1) entry being zero, (25) holds if and only if

ζ−1 = 0,

ζk+1 =

(cid:18)(cid:20) 0

−αp
−αp α2p

(cid:21)

+

(cid:21)(cid:19)

(cid:20)0
2 − 1
1

1
2

2L

⊗ Ip ≤ 0,

which is equivalent to

(cid:20)
0
−αp + 1

−αp + 1
2
2 α2p − 1

2L

(cid:21)

≤ 0.

(25)

(cid:40) −αp + 1
α2p − 1

2 = 0
2L ≤ 0

=⇒

(cid:40) p = 1
2α
α ≤ 1
L

We can choose α = 1

L and the bound (24) becomes

min
k≤T

(f (xk) − f(cid:63)) ≤

L(cid:107)x0 − x(cid:63)(cid:107)2
2(T + 1)

.

Since gradient descent has monotonically nonincreasing it-
erates, that is f (xk+1) ≤ f (xk) for all k, we immediately
recover the standard O(1/k) rate result.

4.2. Dissipativity for O(1/k2) rates

Certifying a O(1/k) rate for the gradient method required
solving a single LMI (25). However, this is not the case
for the O(1/k2) rate analysis of Nesterov’s accelerated
method. Nesterov’s algorithm has parameters that depend
on k so the analysis is more involved. We will begin with
the general case and then specialize to Nesterov’s algo-
rithm. Consider the dynamical system

ξk+1 = Akξk + Bkwk

(26)

The state matrix Ak and input matrix Bk change with the
time step k, and hence (26) is referred to as a “linear time-
varying” (LTV) system. The analysis of LTV systems typ-
ically requires a time-dependent supply rate such as

Sk(ξk, wk) :=

(cid:21)T

(cid:20) ξk
wk

Xk

(cid:21)

(cid:20) ξk
wk

(27)

If there exists a sequence {Pk} with Pk ≥ 0 such that

(cid:34)

AT

k Pk+1Ak − Pk AT
BT
BT

k Pk+1Ak

k Pk+1Bk
k Pk+1Bk

(cid:35)

− Xk ≤ 0

(28)

˜Mk :=

for all k, then we have Vk+1(ξk+1) − Vk(ξk) ≤ Sk(ξk, wk)
with the time-dependent storage function deﬁned as
Vk(ξk) := ξT
k Pkξk. This is a standard approach for dis-
sipation inequality constructions of LTV systems and can

xk+1 = yk − αk∇f (yk),

yk = (1 + βk)xk − βkxk−1.

(29a)

(29b)

It is known that (29) achieves a rate of O(1/k2) when
αk := 1/L and βk is deﬁned recursively as follows.

1 + (cid:112)1 + 4ζ 2
2

k

,

βk =

ζk−1 − 1
ζk

.

The sequence {ζk} satisﬁes ζ 2
k−1. We now
present a dissipativity theory for the sublinear rate analysis
of Nesterov’s accelerated method. Rewrite (29) as

k − ζk = ζ 2

(cid:21)

(cid:20)xk+1 − x(cid:63)
xk − x(cid:63)

= Ak

(cid:21)

(cid:20) xk − x(cid:63)
xk−1 − x(cid:63)

+ Bkwk

(30)

where wk := ∇f (yk) = ∇f ((1 + βk)xk − βkxk−1),
Ak := ˜Ak ⊗ Ip, Bk := ˜Bk ⊗ Ip, and ˜Ak, ˜Bk are given by
(cid:21)

(cid:21)

˜Ak :=

(cid:20)1 + βk −βk
0

1

,

˜Bk :=

(cid:20)−αk
0

.

Hence, Nesterov’s accelerated method (29) is in the form
of (26) with ξk := (cid:2)(xk − x(cid:63))T
. The
O(1/k2) rate analysis of Nesterov’s method (29) requires
the following time-dependent supply rate.

(xk−1 − x(cid:63))T(cid:3)T

Lemma 8 Let f be L-smooth and convex. Let x(cid:63) be a
point satisfying ∇f (x(cid:63)) = 0. In addition, set f(cid:63) := f (x(cid:63)).
Consider Nesterov’s method (29) or equivalently (30). The
following inequalities hold for all trajectories and for all k.









xk − x(cid:63)
xk−1 − x(cid:63)
∇f (yk)

xk − x(cid:63)
xk−1 − x(cid:63)
∇f (yk)



T



Mk



T



Nk













xk − x(cid:63)
xk−1 − x(cid:63)
∇f (yk)

xk − x(cid:63)
xk−1 − x(cid:63)
∇f (yk)

 ≤ f (xk) − f (xk+1)

 ≤ f (x(cid:63)) − f (xk+1)

where Mk := ˜Mk ⊗ Ip, Nk := ˜Nk ⊗ Ip, and ˜Mk, ˜Nk are
deﬁned by











0
0
1
2 βk

0
0
− 1
2 βk
0
0
2 (1 + βk)

− 1




 ,

− 1

− 1
2 βk
1
2 βk
1
2L
0
0
1
2 βk




 .

2 (1 + βk)
1
2 βk
1
2L

(31)

(32)

˜Nk :=

Dissipativity Theory for Nesterov’s Accelerated Method

Given any nondecreasing sequence {µk}, one can deﬁne
the supply rate as (27) with the particular choice Xk :=
µkMk + (µk+1 − µk)Nk for all k. Then this supply rate
satisﬁes the condition

m is incorporated into the formulas of Mk, Nk. By setting
µk := ρ−2k and Pk := ρ−2kP , then the LMI (34) is the
same for all k and we recover (5). This illustrates how the
inﬁnite number of LMIs (34) can collapse to a single LMI
under special circumstances.

S(ξk, wk) ≤ µk(f (xk) − f(cid:63))

− µk+1(f (xk+1) − f(cid:63)).

(33)

5. Continuous-time Dissipation Inequality

Proof. The proof is very similar to the proof of Lemma 3
with an extra condition m = 0. A detailed proof is pre-
sented in the supplementary material.

Theorem 9 Consider the LTV dynamical system (26). If
there exist matrices {Pk} with Pk ≥ 0 and a nondecreasing
sequence of nonnegative scalars {µk} such that

(cid:34)

AT

k Pk+1Ak − Pk AT
BT
BT

k Pk+1Ak

k Pk+1Bk
k Pk+1Bk

(cid:35)

− µkMk − (µk+1 − µk)Nk ≤ 0

(34)

then we have Vk+1(ξk+1) − Vk(ξk) ≤ Sk(ξk, wk) with the
storage function Vk(ξk) := ξT
k Pkξk and the supply rate
(27) using Xk := µkMk + (µk+1 − µk)Nk for all k. In
addition, if this supply rate satisﬁes (33), we have

f (xk) − f(cid:63) ≤

µ0(f (x0) − f(cid:63)) + V0(ξ0)
µk

(35)

Proof. Based on the state-space model (26), we can left
and right multiply (34) by (cid:2)ξT
, and
directly obtain the dissipation inequality. Combining this
dissipation inequality with (33), we can show

(cid:3) and (cid:2)ξT

k wT
k

k wT
k

(cid:3)T

Vk+1(ξk+1) + µk+1(fk+1 − f(cid:63)) ≤ Vk(ξk) + µk(fk − f(cid:63)).

Summing the above inequality as in the proof of Theorem 7
and using the fact that Pk ≥ 0 for all k yields the result.

(cid:21)

(cid:2)ζk−1

(cid:20) ζk−1
1 − ζk−1

We are now ready to show the O(1/k2) rate result for
Nesterov’s accelerated method. Set µk := (ζk−1)2 and
(cid:3). Note that Pk ≥ 0
Pk := L
2
and µk+1 − µk = ζk. It is straightforward to verify that this
choice of {Pk, µk} makes the left side of (34) the zero ma-
trix and hence (35) holds. Using the fact that ζk−1 ≥ k/2
(easily proved by induction), we have µk ≥ k2/4 and the
O(1/k2) rate for Nesterov’s method follows.

1 − ζk−1

Remark 10 Theorem 9 is quite general. The inﬁnite fam-
ily of LMIs (34) can also be applied for linear rate analy-
sis and collapses down to the single LMI (5) in that case.
To apply (34) to linear rate analysis, one needs to slightly
modify (31)–(32) such that the strong convexity parameter

Finally, we brieﬂy discuss dissipativity theory for the
continuous-time ODEs used in optimization research. Note
that dissipativity theory was ﬁrst introduced in Willems
(1972a;b) in the context of continuous-time systems. We
denote continuous-time variables in upper case. Consider a
continuous-time state-space model

˙Λ(t) = A(t)Λ(t) + B(t)W (t)

(36)

where Λ(t) is the state, W (t) is the input, and ˙Λ(t) denotes
the time derivative of Λ(t). In continuous-time, the supply
rate is a function S : RnΛ × RnW × R+ → R that assigns
a scalar to each possible state and input pair. Here, we
allow S to also depend on time t ∈ R+. To simplify our
exposition, we will omit the explicit time dependence (t)
from our notation.

Deﬁnition 11 The dynamical system (36) is dissipative
with respect to the supply rate S if there exists a func-
tion V : RnΛ × R+ → R+ such that V (Λ, t) ≥ 0 for
all Λ ∈ RnΛ and t ≥ 0 and

˙V (Λ, t) ≤ S(Λ, W, t)

(37)

˙V denotes the Lie
for every trajectory of (36). Here,
derivative (or total derivative); it accounts for Λ’s depen-
dence on t. The function V is called a storage function,
and (37) is a (continuous-time) dissipation inequality.

For any given quadratic supply rate, one can automatically
construct the continuous-time dissipation inequality using
semideﬁnite programs. The following result is standard in
the controls literature.

Theorem 12 Suppose X(t) ∈ R(nΛ+nW )×(nΛ+nW ) and
X(t)T = X(t) for all t. Consider the quadratic supply rate

S(Λ, W, t) :=

for all t.

(38)

(cid:21)T

(cid:20) Λ
W

(cid:21)

(cid:20) Λ
W

X

If there exists a family of matrices P (t) ∈ RnΛ×nΛ with
P (t) ≥ 0 such that

(cid:20)ATP + P A + ˙P P B
0

BTP

(cid:21)

− X ≤ 0

for all t.

(39)

Then we have ˙V (Λ, t) ≤ S(Λ, W, t) with the storage func-
tion deﬁned as V (Λ, t) := ΛTP Λ.

Dissipativity Theory for Nesterov’s Accelerated Method

Proof. Based on the state-space model (36), we can apply
the product rule for total derivatives and obtain

˙V (Λ, t) = ˙ΛTP Λ + ΛTP ˙Λ + ΛT ˙P Λ

(cid:20) Λ
W

(cid:21)T (cid:20)ATP + P A + ˙P P B
BTP
0

(cid:21) (cid:20) Λ
W

(cid:21)

.

=

Hence we can left and right multiply (39) by (cid:2)ΛT W T(cid:3)
and (cid:2)ΛT W T(cid:3)T

and obtain the desired conclusion.

The algebraic structure of the LMI (39) is simpler than that
of its discrete-time counterpart (5) because for given P , the
continuous-time LMI is linear in A, B rather than being
quadratic. This may explain why continuous-time ODEs
are sometimes more amenable to analytic approaches than
their discretized counterparts.

We demonstrate the utility of (39) on the continuous-time
limit of Nesterov’s accelerated method in Su et al. (2016):

¨Y +

˙Y + ∇f (Y ) = 0,

(40)

3
t

which we rewrite as (36) with Λ := (cid:2) ˙Y T Y T − xT
,
W := ∇f (Y ), x(cid:63) is a point satisfying ∇f (x(cid:63)) = 0, and
A, B are deﬁned by

(cid:3)T

(cid:63)

A(t) :=

, B(t) :=

(cid:20)− 3
t Ip
Ip

(cid:21)

0p
0p

(cid:21)

(cid:20)−Ip
0p

.

Suppose f is convex and set f(cid:63) := f (x(cid:63)). Su et al. (2016,
Theorem 3) constructs the Lyapunov function V(Y, t) :=
˙Y − x(cid:63)(cid:107)2 to show that ˙V ≤ 0 and
t2(f (Y )−f(cid:63))+2(cid:107)Y + t
2
then directly demonstrate a O(1/t2) rate for the ODE (40).
To illustrate the power of the dissipation inequality, we use
the LMI (39) to recover this Lyapunov function. Denote
G(Y, t) := t2(f (Y ) − f(cid:63)). Note that convexity implies
f (Y ) − f(cid:63) ≤ ∇f (Y )T(Y − x(cid:63)), which we rewrite as

2t(f (Y ) − f(cid:63)) ≤





˙Y
Y − x(cid:63)
W



T





0p
0p
0p

0p
0p
tIp









.

˙Y
Y − x(cid:63)
W

0p
tIp
0p

Ip

Ip

2 Ip

2 Ip

(cid:3)T (cid:2) t

Clearly S(Λ, W, t) ≤ − ˙G(Y, t). Now we can choose
P (t) := 2 (cid:2) t
(cid:3). Substituting P and X
into (39), the left side of (39) becomes identically zero.
˙V (Λ, t) ≤ S(Λ, W, t) ≤ − ˙G(Y, t) with the
Therefore,
storage function V (Λ, t) := ΛTP Λ. By deﬁning the Lya-
punov function V(Y, t) := V (Y, t) + G(Y, t), we imme-
diately obtain ˙V ≤ 0 and also recover the same Lyapunov
function used in Su et al. (2016).

Remark 13 As in the discrete-time case, the inﬁnite fam-
ily of LMIs (39) can also be reduced to a single LMI for
the linear rate analysis of continuous-time ODEs. For fur-
ther discussion on the topic of continuous-time exponential
dissipation inequalities, see Hu & Seiler (2016).

6. Conclusion and Future Work

In this paper, we developed new notions of dissipativity
theory for understanding of Nesterov’s accelerated method.
Our approach enjoys advantages of both the IQC frame-
work (Lessard et al., 2016) and the discretization approach
(Wilson et al., 2016) in the sense that our proposed LMI
condition is simple enough for analytical rate analysis of
Nesterov’s method and can also be easily generalized to
more complicated algorithms. Our approach also gives an
intuitive interpretation of the convergence behavior of Nes-
terov’s method using an energy dissipation perspective.

One potential application of our dissipativity theory is for
the design of accelerated methods that are robust to gradi-
ent noise. This is similar to the algorithm design work in
Lessard et al. (2016, §6). However, compared with the IQC
approach in Lessard et al. (2016), our dissipativity theory
leads to smaller LMIs. This can be beneﬁcial since smaller
LMIs are generally easier to solve analytically. In addition,
the IQC approach in Lessard et al. (2016) is only applicable
to strongly-convex objective functions while our dissipa-
tivity theory may facilitate the design of robust algorithm
for weakly-convex objective functions. The dissipativity
framework may also lead to the design of adaptive or time-
varying algorithms.

Since ˙G(Y, t) = 2t(f (Y ) − f(cid:63)) + t2∇f (Y )T ˙Y , we have

Acknowledgements

˙G ≤





˙Y
Y − x(cid:63)
W



T 





0p
0p
t2
2 Ip

0p
0p
tIp

t2
2 Ip
tIp
0p











 .

˙Y
Y − x(cid:63)
W

Now choose the supply rate S as (38) with X(t) given by

X(t) := −





0p
0p
t2
2 Ip

0p
0p
tIp



 .

t2
2 Ip
tIp
0p

Both authors would like to thank the anonymous reviewers
for helpful suggestions that improved the clarity and qual-
ity of the ﬁnal manuscript.

This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 1656951. Both
authors also acknowledge support from the Wisconsin In-
stitute for Discovery, the College of Engineering, and the
Department of Electrical and Computer Engineering at the
University of Wisconsin–Madison.

Dissipativity Theory for Nesterov’s Accelerated Method

Wibisono, A., Wilson, A., and Jordan, M. A varia-
tional perspective on accelerated methods in optimiza-
tion. Proceedings of the National Academy of Sciences,
pp. 201614734, 2016.

Willems, J.C. Dissipative dynamical systems Part I: Gen-
eral theory. Archive for Rational Mech. and Analysis, 45
(5):321–351, 1972a.

Willems, J.C. Dissipative dynamical systems Part II: Linear
systems with quadratic supply rates. Archive for Ratio-
nal Mech. and Analysis, 45(5):352–393, 1972b.

Wilson, A., Recht, B., and Jordan, M. A Lyapunov analysis
of momentum methods in optimization. arXiv preprint
arXiv:1611.02635, 2016.

References

Bubeck, S. Convex optimization: Algorithms and com-
plexity. Foundations and Trends R(cid:13) in Machine Learning,
8(3-4):231–357, 2015.

Bubeck, S., Lee, Y., and Singh, M. A geometric alterna-
tive to Nesterov’s accelerated gradient descent. arXiv
preprint arXiv:1506.08187, 2015.

Drusvyatskiy, D., Fazel, M., and Roy, S. An optimal
ﬁrst order method based on optimal quadratic averaging.
arXiv preprint arXiv:1604.06543, 2016.

Fazlyab, M., Ribeiro, A., Morari, M., and Preciado,
V. M. Analysis of optimization algorithms via inte-
gral quadratic constraints: Nonstrongly convex prob-
lems. arXiv preprint arXiv:1705.03615, 2017.

Flammarion, N. and Bach, F. From averaging to acceler-
ation, there is only a step-size. In COLT, pp. 658–695,
2015.

Hu, B. and Seiler, P. Exponential decay rate conditions
for uncertain linear systems using integral quadratic con-
IEEE Transactions on Automatic Control, 61
straints.
(11):3561–3567, 2016.

Lessard, L., Recht, B., and Packard, A. Analysis and design
of optimization algorithms via integral quadratic con-
straints. SIAM Journal on Optimization, 26(1):57–95,
2016.

Megretski, A. and Rantzer, A. System analysis via integral
quadratic constraints. IEEE Transactions on Automatic
Control, 42:819–830, 1997.

Megretski, A., J¨onsson, U., Kao, C. Y., and Rantzer,
Integral

A. Control Systems Handbook, chapter 41:
Quadratic Constraints. CRC Press, 2010.

Nesterov, Y.

Introductory Lectures on Convex Optimiza-
tion: A Basic Course. Kluwer Academic Publishers,
2003.

Polyak, B. T. Introduction to optimization. Optimization

Software, 1987.

Seiler, P. Stability analysis with dissipation inequalities and
integral quadratic constraints. IEEE Transactions on Au-
tomatic Control, 60(6):1704–1709, 2015.

Su, W., Boyd, S., and Cand`es, E. A differential equation for
modeling Nesterov’s accelerated gradient method: The-
ory and insights. Journal of Machine Learning Research,
17(153):1–43, 2016.

