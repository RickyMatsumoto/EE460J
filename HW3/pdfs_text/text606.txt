Supplementary material: Multilabel Classiﬁcation with Group Testing and Codes

A. Constructions

We present some additional group testing constructions in
this section.

A.1. Random Constructions-Proofs

Proposition. (Random Construction Prop. 1.) An m × d
random binary {0, 1} matrix A where each entry is 1 with
probability ρ = 1
k+1 , is (k, 3k log d)-disjunct with very
high probability, if m = O(k2 log d).

1

k+1

k+1 )k]m.

Proof. For the case of e = 1, the proof follows from The-
orem 8.1.3, Corollary 8.1.4 and 8.1.5 in (Du & Hwang,
2000). The bound on the number of classiﬁers is m ≤
3(k + 1)2 log d and the probability is (k + 1)(cid:0) d
(cid:1)[1 −
k+1 (1 − 1
To show that the matrix is also (k, e)-disjunct with high
|supp(A(i)) \
probability, we have to just show that
supp(A(j))| > e for any two distinct columns A(i) and
A(j) (Corollary 8.3.2 in (Du & Hwang, 2000)). For any
|supp(A(i)) \ supp(A(j))| is a bi-
two ﬁxed columns,
nomial random variable Bin(m,
If we choose,
m = (3 + (cid:15))(k + 1)2 log d, (cid:15) > 0, we ﬁnd the expectation
of this variable to be 3k log d. Therefore we can choose
e = 3k log d, and the matrix is going to be (k, e)-disjunct
with high probability.

k
(k+1)2 ).

Theorem. (Restating Theorem 1.) Suppose we wish to re-
cover a k sparse binary vector y ∈ Rd. A random bi-
nary {0, 1} matrix A where each entry is 1 with probability
ρ = 1/k recovers 1 − ε proportion of the support of y cor-
rectly with high probability, for ε > 0, for m = O(k log d).
This matrix will also detect e = Ω(m) errors.

we include an item with probability ρ. We will show that
the probability of error Pe in this case is o(1) which will
implies existence of a matrix A that achieves Pe of o(1).

The probability that any one test will be successful to dis-
tinguish between T and T (cid:48) is therefore (here we are taking
the sizes of T and T (cid:48) exactly equal to k and not less than
equal to, which is permissible without much loss of gener-
ality),

2(1 − ρ)k(1 − (1 − ρ)k−r) = 2

1 −

(cid:17)k(cid:16)

(cid:16)

1 −

1 −

(cid:17)k−r(cid:17)

(cid:18)

≥ 2 · 3−1

1 − exp

−

≥

1 − exp(−ε)

,

(cid:16)

(cid:17)(cid:19)

1
k
2
3

(cid:16)

(cid:16)

k − r
k

1
k
(cid:17)

where in the second line we have used inequalities 1 − x ≤
exp(−x) for all x and 1 − x ≥ 3−x for any x ≤ 0.17
(which is true for any k ≥ 6). We have also used the fact
that r < (1 − ε)|T | ≤ (1 − ε)k.

Hence the probability that A successfully distinguish be-
tween T and T (cid:48) in less than e tests is,

(cid:19)(cid:16)

(cid:18)m
i

(cid:88)

i≤e

(cid:16)

2
3

1−

1−exp(−ε)

(cid:17)(cid:17)m−i(cid:16) 2
3

(cid:16)

1−exp(−ε)

(cid:17)(cid:17)i

.

This probability is going to be upper bounded by
exp(−δm) whenever e < 2
3 (1 − exp(−ε))m for some
δ > 0. Therefore, for this ensemble,

Pe ≤

(cid:16) k
(cid:88)

(cid:19)(cid:17)2

(cid:18)d
i

i=0

exp(−δm) → 0,

for an m such that m = O(k log d). Therefore e =
O(k log d).

Proof. This is a modiﬁcation of Theorem 1 in (Mazumdar
& Mohajer, 2014). Suppose, T ⊂ [d] is the set of defec-
tives. The recovery will be successful as long as we return
a set T (cid:48) such that r ≡ |T ∩ T (cid:48)| ≥ (1 − ε)|T |.

Our object of interest is the probability of error Pe, the
probability of existence of a pair T and T (cid:48), |T |, |T (cid:48)| ≤ k
such that less than e tests fails to distinguish this pair, where
r < (1 − ε)|T |.

We assume the testing matrix A is chosen randomly from
the ensemble of all m × d matrix in the following way.
Each entry of A is 1 with probability ρ ≡ 1
k , and it is zero
with the remaining probability. In other words, in each test

A.2. Concatenated code based constructions

Many code based constructions have been proposed with
the optimal length of m = Θ(k2 logk d) (Mazumdar,
2016). One such code based construction of interest is the
Algebraic-Geometric codes.

Considering q = r2, where r is an integer, using the re-
sults in (Tsfasman et al., 2007), we can generate a family
of Algebraic-Geometric (AG) codes of length mq, satisfy-
ing mq ≥ ra+1 − ra + 1, where a is an even integer. Using
the Kautz-Singleton mechanism, we can convert this AG
code to a binary code that has constant weight w = mq.
The length of the binary code will be m = qmq.

Multilabel classiﬁcation via group testing

Proposition 7. We can construct an Algebraic-Geometric
code matrix that recovers 1 − ε proportion of nonze-
ros in y with high probability, for ε > 0, with m ≥
16k log2k d log(d/ε). This matrix will also detect e =
(cid:17)
(cid:16)
8 log(d/ε) − 8 log(d/ε)
2k−1

log2k d errors.

− 1

√

Proof. The proof follows from the results developed
in (Mazumdar, 2016). For a q-ary Algebraic-Geometric
Code with q ≥ 2k, that is converted to a binary code us-
ing Kautz-Singleton mechanism, we have the 1 − ε recov-
ery guarantees for m ≥ 16k log d
log 2k log(d/ε). We know if the
code has a distance h, then e = h/2. The q-ary AG code
satisﬁes

h ≥ 2m/q − 2 logq d −

√

2m
q − 1)

.

q(

We get the value for e upon substitution.

For MLGT, we have the following results for different con-
structions:

• If A is constructed via randomized construction of
Prop. 1 with m = O(k2 log d) rows, then the average
error rate is t/k − 3

2 log d for t > 3/2 log d.

• If A is constructed via randomized construction of
Thm. 1 with m = O(k log d) rows, then the average
error rate is (t/k − O(log d) + εk/d) for t > k log d.
• If A is constructed deterministically via Kautz-
construction of
Singleton Reed-Solomon codes
Prop. 2 with m = O(k2 logk d) rows, then the average
k logk d − O(1) for t > k logk d.
error rate is

• If A is constructed via expander graph-based con-
struction of Prop. 6 with m = O(k2 log(d/k)) rows,
then the average error rate is t/k − log(d/k) for t >
k/2 log(d/k).

t

rate is zero for smaller number of mis-

The error
classiﬁcations t.

B. Experiments

Datasets: We use some popular publicly available multi-
label datasets in our experiments. All datasets were ob-
tained from The Extreme Classiﬁcation Repository2 (Bha-
tia et al., 2015). Details about the datasets and the refer-
ences for their original sources can be found in the repos-
itory. Table 4 gives the statistics of these datasets.
In
the table, d = #labels, ¯k =average sparsity per instance,
n = #instances and p = #features.

Details of the experiments:

2https://manikvarma.github.io/downloads/

XC/XMLRepository.html

d
Dataset
101
Mediamill
159
Bibtex
983
Delicious
2456
RCV1-2K
3993
EurLex-4K
AmazonCat-13K 13330
30938
Wiki10-31K

Table 4. Dataset statistics
n
30993
4880
12920
623847
15539
1186239
14146

¯k
4.38
2.40
19.03
4.79
5.31
5.04
18.64

p
120
1839
500
47236
5000
203882
101938

• We use simple least squares binary classiﬁers for
training and prediction in MLGT. This is because, this
classiﬁer is extremely simple and fast. Also, we use
least squares regressors for other compared methods
(hence, it is a fair comparison). We note that MLGT
performs well with this simple classiﬁer. We can im-
prove the performance of MLGT further by using a
more advanced classiﬁer.

• In the prediction algorithm of MLGT, we have a pa-
rameter e, the number of errors the algorithm should
try to correct. The ideal value for e will depend on the
GT matrix used, the values of m, k and d. However,
note that we can test for different values of e at no ad-
ditional cost. That is, once we compute the Boolean
AND between the predicted reduced vector and the
GT matrix (the dominant operation), we can get dif-
ferent prediction vectors for a range of e and choose
an e that gives the highest training P@k.

• The Orthogonal Matching Pursuit (OMP) algorithm
used for MLCS is as implemented by the SPAMS
http://spams-devel.gforge.
library
inria.fr/.

• Many of the datasets have very sparse feature matri-
ces. In such cases, we reduced the feature dimension
by choosing only the prominent features. That is the
features that have nonzero values for at least half of
the considered training points.

• The results we obtained for the dataset Delicious were
consistently poor. This is because, the average spar-
sity for this dataset is ¯k = 19.03. We selected data
instances with sparsity at most kmax = 12. Still, the
GT matrices used were not k-disjunct for this case.
Also, the feature dimension is small (p = 500). Re-
sults with CS for this data were poor as well in terms
of Hamming loss. However, the precision was better.

• For AmazonCat-13K, the training precision for CS
method is perfect. However, the Hamming error is
poor because they returned many false classes.

• The runtimes reported in the main paper (using
cputime in Matlab) includes generation of compres-
sion matrices, multiplying the matrix to the label vec-
tors (boolean OR/SVD computation), training the m

Multilabel classiﬁcation via group testing

classiﬁers, and prediction of n training and nt test
points. All runtime experiments were conducted on
an Intel Core i7-5557U CPU @ 3.10GHz machine.

• The runtimes were averaged over 3 trials for smaller
datasets (ﬁrst 4). So were the errors. But for larger
datasets (last 2), results for just one trial is reported.
For larger datasets, our MLGT runtime is almost half
of the next best timing and yields the lowest error.

• For SLEEC, there are seven parameters to be tuned.
We set these parameters to the values provided by the
authors online. The ideal parameters to be set in this
algorithm for each of the datasets we used (expect
In
RCV1-2k) were provided by the authors online.
general, we found SLEEC to be very expensive for
large datsets. Also, there is no procedure to select
these seven parameters and are seem to be selected
in a trial and error fashion.

