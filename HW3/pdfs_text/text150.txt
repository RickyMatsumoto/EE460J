Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution

A. Fisher information matrix for the Normal Distribution

Under regularity conditions (Wasserman, 2013), the Fisher information matrix can also be obtained from the second-order
partial derivatives of the log-likelihood function

where l(θ) = log πθ(a

s). This gives us the Fisher information for the Normal distribution

I(θ) =

∂2l(θ)
∂θ2 ] ,

E[

−

(D1)

(D2)

(µ, σ) =

I

Ea

πθ

∼

−

∂2l
∂µ2
∂2l
∂σ∂µ

∂2l
∂µ∂σ
∂2l
∂σ2 �

�

�

=

Ea

πθ

∼

−

1
σ2
−
2 (a

−
σ3

µ)

−

µ)

2 (a
−
σ3
µ)2

−
3(a
−
σ4

−

+ 1

σ2 �

1
σ2
0

=

�

0
2
σ2 �

.

B. Fisher information matrix for the Beta Distribution

To see how variance changes as the policy converges and becomes more deterministic, let us ﬁrst compute the partial
derivative of log πθ(a

s) with respect to shape parameter α

where ψ(

) = d

dz log Γ(

·

) is the digamma function. Similar results can also be derived for β. From (D1), we have

∂ log πθ(a

s)

|

=

log

∂α

Γ(α + β)
Γ(α)Γ(β)

aα

−

1(1

a)β

−

1

−

∂
∂α
∂
∂α

�

=

(log Γ(α + β)

log Γ(α) + (α

1) log a)

= log a + ψ(α + β)

ψ(α) ,

−

−

�

−

·

I

(α, β) =

Ea

πθ

∼

−

=

Ea

πθ

∼

−

∂2l
∂α∂β
∂2l
∂β2 �

∂2l
∂α2
∂2l
∂β∂α
∂
∂α (ψ(α + β)
∂
∂α (ψ(α + β))

−

�

�

ψ(α))

∂
∂β (ψ(α + β))

∂
∂β (ψ(α + β)

ψ(β)) �

−

=

ψ�(α)

ψ�(α + β)

−
ψ�(α + β)

�

−

ψ�(α + β)

ψ�(α + β)

−
ψ�(β)

−

,

�

where ψ�(z) = ψ(1)(z) and ψ(m)(z) = dm+1
goes to 0 as z

.

→ ∞

dzm+1 log Γ(z) is the polygamma function of order m. Figure 7 shows how ψ�(z)

Figure 7. Graphs of the digamma function, the polygamma function, and the harmonic series. The harmonic series and the digamma
function are related by

is the Euler-Mascheroni constant.

1
k = ψ(z + 1) + γ, where γ = 0.57721

z
k=1

· · ·

|

|

�

