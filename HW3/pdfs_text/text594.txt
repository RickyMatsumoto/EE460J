Supplementary Material: Evaluating the Variance of Likelihood-Ratio
Gradient Estimators

Seiya Tokui 1 2 Issei Sato 3 2

A. Derivations of Estimators for Binary

as follows using Eq. (1).

Variables

In this section, we give the derivations of estimators for
binary variables given in Sec. 6. Let

qφi(zi|pai) = µzi

i (1 − µi)1−zi

be a Bernoulli distribution of a mean parameter µi =
µi(pai, φi). Here we only focus on the derivative w.r.t.
µi instead of φi. The derivative w.r.t. φi can be derived
by simply multiplying ∇φiµi to the derivative given in this
section.

The log probability of zi is given by

log qφi(zi|pai) = zi log µi + (1 − zi) log(1 − µi),

and its derivative is

∂
∂µi

log qφi(zi|pai) =

zi
µi

−

1 − zi
1 − µi

=

(cid:40) 1
µi
− 1

1−µi

(f (x, z) − bi(x, (cid:15)\i))

log qφi(zi|pai)

∂
∂µi
1 − zi
1 − µi

(cid:19)

(cid:18) zi
µi

−

= (fzi − bi)

(cid:40)

=

(f1 − bi)/µi

−(f0 − bi)/(1 − µi)

if zi = 1,
if zi = 0.

Since zi = 1 holds with probability µi, the estimator is
written as follows.

(cid:40)

∆LR

i =

(f1 − bi)/µi

w.p. µi,

−(f0 − bi)/(1 − µi) w.p. 1 − µi.

(4)

Optimal Estimator: The optimal estimator is given by
the following formula.

if zi = 1
if zi = 0.
(1)

∂
∂µi

E(cid:15)if (x, gφ(x, (cid:15)))

(cid:88)

=

f (x, z)

zi

∂
∂µi

(cid:12)
(cid:12)
qφi(zi|pai)
(cid:12)z\i=hφ\i

.

(5)

(x,zi,(cid:15)\i)

The derivative of the probability is simply given as follows.

By substituting Eq. (2), we obtain the following estimator.

∂
∂µi

qφi(zi|pai) =

(cid:40)

1
−1

if zi = 1
if zi = 0.

(2)

∆(cid:63)

i =

f (x, z)

(cid:88)

zi

∂
∂µi

(cid:12)
(cid:12)
qφi(zi|pai)
(cid:12)z\i=hφ\i

(x,zi,(cid:15)\i)

Let fk = f (x, zi = k, z\i = hφ\i(x, zi, (cid:15)\i)) be the sim-
ulated objective function f evaluated at ﬁxed zi = k ∈
{0, 1} and the noise (cid:15)\i.

Likelihood-Ratio Estimator: Recall that the likelihood-
ratio estimator for a general class of distribution is formu-
lated by the following equation.

= f1 × 1 + f0 × (−1)
= f1 − f0.

It can also be derived by simply calculating the mean of Eq.
(4).

Local Expectation Gradient: The local expectation gra-
dient estimator is given by the following expectation.

∂
∂µi

E(cid:15)if (x, gφ(x, (cid:15)))

∂
∂µi

F (φ; x)

= Eqφ(z\i|x)

= E(cid:15)i(f (x, z) − bi(x, (cid:15)\i))

log qφi(zi|pai).

(3)

∂
∂µi

(cid:88)

zi

qφ(zi|mbi)
qφi(zi|pai)

∂
∂µi

f (x, z)

qφi(zi|pai). (6)

Here we consider an independent baseline,
i.e., bi =
bi(x, (cid:15)\i) is constant against (cid:15)i. When qφi is a Bernoulli
distribution, the Monte Carlo estimate of Eq. (3) is written

Let πi = qφ(zi = 1|mbi) and f (cid:48)
k = f (x, zi = k, z\i =
hφ\i(x, zi = 1 − k, (cid:15)\i)). Here f (cid:48)
k is the value of f evalu-
ated at zi = k ∈ {0, 1} and other variables z\i computed

Supplementary Material: Evaluating the Variance of Likelihood-Ratio Gradient Estimators

from (cid:15)\i and zi = 1 − k. Note that the evaluation of the
local expectation gradient estimator proceeds as follows:

It can be seen as a likelihood-ratio estimator with baseline
given by

1. Sample (cid:15) and compute z = gφ(x, (cid:15)).

2. Discard zi so that we obtain z\i ∼ qφ(z\i|x).

3. Compute the summation in Eq. (6).

Therefore, if zi = 1 is sampled at the ﬁrst step, the value
of f used in the estimation is f1 and f (cid:48)
0 instead of f0, i.e.,
z\i = hφ\i(x, zi, (cid:15)\i) is not re-evaluated at zi = 0. Simi-
larly, if zi = 0 is sampled at the ﬁrst step, the value of f
used in the estimation is f (cid:48)
1 and f0. Based on these obser-
vations, we derive the estimator using Eq. (2). If zi = 1,
then

f (x, zi = k, z\i)

qφi(zi = k|pai)

∂
∂µi

(cid:88)

k

=

qφ(zi = k|mbi)
qφi (zi = k|pai)
πi
µi

1 − πi
1 − µi

f1 −

f (cid:48)
0.

It is further transformed as follows.

f1 −

1 − πi
1 − µi

f (cid:48)
0

πif1 −

(cid:19)

1 − πi
1 − µi

µif (cid:48)
0

f1 − (1 − πi)f1 −

πi
µi

=

=

=

(cid:18)

(cid:18)

(cid:18)

1
µi
1
µi
1
µi

(cid:19)

1 − πi
1 − µi

µif (cid:48)
0

(cid:19)

f1 −

1 − πi
1 − µi

((1 − µi)f1 + µif (cid:48)
0)

.

(8)

If zi = 0, then

(cid:88)

k

=

qφ(zi = k|mbi)
qφi (zi = k|pai)
πi
µi

1 − πi
1 − µi

f (cid:48)
1 −

f0.

f (x, zi = k, z\i)

qφi(zi = k|pai)

∂
∂µi

(7)

(9)

bLEG
i

=

(cid:40) 1−π
1−µi
π
µi

((1 − µi)f1 + µif (cid:48)
0)
1 + µif0)

((1 − µi)f (cid:48)

if zi = 1,
if zi = 0.

Note that this baseline might depend on zi. Since the local
expectation gradient is an unbiased estimator of the true
gradient, the residual term of the likelihood-ratio gradient
estimation Ci is kept 0.

B. Additional Results from the Experiments

The performance of each method on the training datasets is
shown in Fig. 1 and Fig. 2. The trends are almost same as
those of the validation scores.

The results on the test dataset are shown in Table 1. The log
likelihood is approximated by the Monte Calo lower bound
of Burda et al. (2015) with a sample size of 50,000.

C. Relationship between the RAM estimator
and the Use of Monte Carlo Objectives

It has been shown that the use of Monte Carlo objectives
(Burda et al., 2015; Mnih & Rezende, 2016) can improve
the learning of generative models including variational au-
toencoders and sigmoid belief networks. The importance-
weighted autoencoders (IWAE) (Burda et al., 2015) and
VIMCO estimator (Mnih & Rezende, 2016) are gradient
estimators for Monte Carlo objectives. Both of these eval-
uate the function f at multiple points, while the RAM es-
timator also evaluates multiple points. The use of multi-
point evaluation is orthogonal between these methods and
the RAM estimator. In the methods for Monte Carlo objec-
tives, the multi-point evaluation comes from the improved
lower bound of the log likelihood, i.e., they alter the ob-
jective function. On the other hand, in the RAM estimator,
the multi-point evaluation is introduced purely for variance
reduction. In particular, the RAM estimator is applicable
to any kind of stochastic computational graphs. The exper-
iments in this study is conducted to compare the variance
of each gradient estimator for the same objective function,
and therefore VIMCO estimator is omitted from the com-
parison.

It is further transformed similarly to Eq. (8) as follows.

f0

πi
µi

f (cid:48)
1 −

= −

= −

1 − πi
1 − µi
(cid:18)
1
1 − µi
1
1 − µi

(cid:18)

πi
µi

(1 − πi)f0 −

(1 − µi)f (cid:48)
1

πi
µi

(cid:19)

(cid:19)

f0 −

(µif0 + (1 − µi)f (cid:48)
1)

.

(10)

References

By combining Eq. (8) and Eq. (10), we obtain the follow-
ing estimator.

∆LEG
i

=




f1− 1−πi
1−µi

((1−µi)f1+µif (cid:48)
0)

f0− πi
µi



−

µi

((1−µi)f (cid:48)
1−µi

1+µif0)

w.p. µi,

w.p. 1 − µi.

Burda, Yuri, Grosse, Roger, and Salakhutdinov, Ruslan.
In Proceedings of
Importance weighted autoencoders.
the 3rd International Conference on Learning Represen-
tations (ICLR), 2015.

Mnih, Andriy and Rezende, Danilo Jimenez. Variational
inference for monte carlo objectives. In Proceedings of

Supplementary Material: Evaluating the Variance of Likelihood-Ratio Gradient Estimators

Figure 1. Training curves of two-layer SBN. Left: results using MNIST dataset. Right: results using Omniglot dataset.

Figure 2. Training curves of four-layer SBN. Left: results using MNIST dataset. Right: results using Omniglot dataset.

the 33rd International Conference on Machine Learning,
2016.

025050075010001250150017502000Iteration (x1000)−135−130−125−120−115−110−105−100Variational lower boundLRLR+CLR+C+IDBMuProp+C+IDBLEGoptimalMNIST (200-200-784)025050075010001250150017502000Iteration (x1000)−145−140−135−130−125−120−115−110Variational lower boundLRLR+CLR+C+IDBMuProp+C+IDBLEGoptimalOmniglot (200-200-784)025050075010001250150017502000Iteration (x1000)−140−130−120−110−100Variational lower boundLRLR+CLR+C+IDBMuProp+C+IDBLEGoptimalMNIST (200-200-200-200-784)025050075010001250150017502000Iteration (x1000)−150−140−130−120−110Variational lower boundLRLR+CLR+C+IDBMuProp+C+IDBLEGoptimalOmniglot (200-200-200-200-784)Supplementary Material: Evaluating the Variance of Likelihood-Ratio Gradient Estimators

Table 1. Test results of gradient estimators. The learning rate with the best validation performance is used in each method. VB stands
for variational lower bound of the log likelihood, and LL stands for the log likelihood estimation.

MNIST (shallow)

VB

LL

MNIST (deep)
LL
VB

Omniglot (shallow)

VB

LL

Omniglot (deep)
VB

LL

LR
LR+C
LR+C+IDB
MuProp+C+IDB
LEG
optimal

-127.33
-107.21
-98.04
-99.96
-106.75
-97.64

-108.53
-97.90
-92.68
-94.23
-98.22
-92.55

-119.93
-105.38
-94.10
-95.03
-103.26
-93.31

-103.53
-95.30
-89.02
-89.83
-93.26
-88.97

-139.17
-122.10
-111.10
-112.97
-121.68
-110.60

-124.08
-113.87
-107.14
-108.28
-113.56
-106.90

-137.54
-123.27
-108.72
-109.55
-121.27
-108.17

-122.85
-114.27
-105.00
-105.52
-112.80
-104.85

