Uniform Deviation Bounds for k-Means Clustering

Olivier Bachem 1 Mario Lucic 1 S. Hamed Hassani 1 Andreas Krause 1

Abstract

Uniform deviation bounds limit the difference be-
tween a model’s expected loss and its loss on a
random sample uniformly for all models in a learn-
ing problem. In this paper, we provide a novel
framework to obtain uniform deviation bounds
for unbounded loss functions. As a result, we
obtain competitive uniform deviation bounds for
k-Means clustering under weak assumptions on
the underlying distribution. If the fourth moment
is bounded, we prove a rate of O
com-

(cid:16)

(cid:17)

2

4

pared to the previously known O
rate.
We further show that this rate also depends on the
kurtosis — the normalized fourth moment which
measures the “tailedness” of the distribution. We
also provide improved rates under progressively
stronger assumptions, namely, bounded higher
moments, subgaussianity and bounded support of
the underlying distribution.

m− 1
(cid:16)

m− 1

(cid:17)

1. Introduction

Empirical risk minimization — i.e. the training of models on
a ﬁnite sample drawn i.i.d from an underlying distribution
— is a central paradigm in machine learning. The hope is
that models trained on the ﬁnite sample perform provably
well even on previously unseen samples from the underly-
ing distribution. But how many samples m are required
to guarantee a low approximation error (cid:15)? Uniform devi-
ation bounds provide the answer. Informally, they are the
worst-case difference across all possible models between
the empirical loss of a model and its expected loss. As such,
they determine how many samples are required to achieve a
ﬁxed error in terms of the loss function. In this paper, we
consider the popular k-Means clustering problem and pro-
vide uniform deviation bounds based on weak assumptions
on the underlying data generating distribution.

1Department of Computer Science, ETH Zurich. Correspon-

dence to: Olivier Bachem <olivier.bachem@inf.ethz.ch>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

Related work. Traditional Vapnik-Chervonenkis theory
provides tools to obtain uniform deviation bounds for bi-
nary concept classes such as classiﬁcation using halfspaces
(Vapnik & Chervonenkis, 1971). While these results have
been extended to provide uniform deviation bounds for sets
of continuous functions bounded in [0, 1] (Haussler, 1992;
Li et al., 2001), these results are not readily applied to k-
Means clustering as the underlying loss function in k-Means
clustering is continuous and unbounded.

In their seminal work, Pollard (1981) shows that k-Means
clustering is strongly consistent, i.e., that the optimal cluster
centers on a random sample converge almost surely to the
optimal centers of the distribution under a weak assump-
tion. This has sparked a long line of research on cluster
stability (Ben-David et al., 2006; Rakhlin & Caponnetto,
2007; Shamir & Tishby, 2007; 2008) which investigates the
convergence of optimal parameters both asymptotically and
for ﬁnite samples. The vector quantization literature offers
insights into the convergence of empirically optimal quan-
tizers for k-Means in terms of the objective: A minimax
rate of O
is known if the underlying distribution
has bounded support (Linder et al., 1994; Bartlett et al.,
1998). A better rate of O(cid:0)m−1(cid:1) may be achieved for ﬁ-
nite support (Antos et al., 2005) or under both bounded
support and regularity assumptions (Levrard et al., 2013).
Ben-David (2007) provides a uniform convergence result for
center based clustering under a bounded support assumption
and Telgarsky & Dasgupta (2013) prove uniform deviation
bounds for k-Means clustering if the underlying distribution
satisﬁes moment assumptions (see Section 4).

m− 1

(cid:16)

(cid:17)

2

Empirical risk minimization with fat-tailed losses has been
studied in Mendelson et al. (2014), Mendelson (2014),
Gr¨unwald & Mehta (2016) and Dinh et al. (2016). Dinh et al.
(2016) provide fast learning rates for k-Means clustering
but under stronger assumptions than the ones considered in
this paper. Guarantees similar to uniform deviation bounds
can be obtained using importance sampling in the context
of coreset construction (Bachem et al., 2015; Lucic et al.,
2016; 2017).

Our contributions. We provide a new framework to obtain
uniform deviation bounds for unbounded loss functions. We
apply it to k-Means clustering and provide uniform devi-
ation bounds with a rate of O
for ﬁnite samples

m− 1

(cid:16)

(cid:17)

2

Uniform Deviation Bounds for k-Means Clustering

under weak assumptions. In contrast to prior work, our
bounds are all scale-invariant and hold for any set of k clus-
ter centers (not only for a restricted solution set). We show
that convergence depends on the kurtosis of the underly-
ing distribution, which is the normalized fourth moment
and measures the “tailedness” of a distribution. If bounded
higher moments are available, we provide improved bounds
that depend upon the normalized higher moments and we
sharpen them even further under the stronger assumptions
of subgaussianity and bounded support.

2. Problem statement for k-Means

We ﬁrst focus on uniform deviation bounds for k-Means
clustering, and defer the (more technical) framework for
unbounded loss functions to Section 5. We consider the
d-dimensional Euclidean space and deﬁne

d(x, Q)2 = min
q∈Q

(cid:107)x − q(cid:107)2
2

for any x ∈ Rd and any ﬁnite set Q ⊂ Rd. Furthermore,
slightly abusing the notation, for any x, y ∈ Rd, we set
d(x, y)2 = d(x, {y})2 = (cid:107)x − y(cid:107)2
2.
Statistical k-Means. Let P be any distribution on Rd with
(cid:2)d(x, µ)2(cid:3) ∈ (0, ∞). For any set
µ = EP [x] and σ2 = EP
Q ⊂ Rd of k ∈ N cluster centers, the expected quantization
(cid:2)d(x, Q)2(cid:3). The goal of the statistical
error is given by EP
k-Means problem is to ﬁnd a set of k cluster centers such
that the expected quantization error is minimized.

Empirical k-Means. Let X denote a ﬁnite set of points in
Rd. The goal of the empirical k-Means problem is to ﬁnd
a set Q of k cluster centers in Rd such that the empirical
quantization error φX (Q) is minimized, where

φX (Q) =

d(x, Q)2.

1
|X |

(cid:88)

x∈X

Empirical risk minimization. In practice, the underlying
data distribution P in statistical learning problems is often
unknown. Instead, one is only able to observe independent
samples from P . The empirical risk minimization principle
advocates that ﬁnding a good solution on a random sam-
ple Xm also provides provably good solutions to certain
statistical learning problems if enough samples m are used.

Uniform deviation bounds. For k-Means, such a result
may be shown by bounding the deviation between the ex-
pected loss and the empirical error, i.e.,

(cid:12)
(cid:12)φXm (Q) − EP

(cid:2)d(x, Q)2(cid:3)(cid:12)
(cid:12) ,

uniformly for all possible clusterings Q ∈ Rd×k. If this
difference is sufﬁciently small for a given m, one may then
solve the empirical k-Means problem on Xm and obtain
provable guarantees on the expected quantization error.

3. Uniform deviation bounds for k-Means

A simple approach would be to bound the deviation by an
absolute error (cid:15), i.e., to require that

(cid:12)
(cid:12)φXm(Q) − EP

(cid:2)d(x, Q)2(cid:3)(cid:12)

(cid:12) ≤ (cid:15)

(1)

(cid:12) ≤

(cid:2)d(x, Q)2(cid:3)(cid:12)

(cid:12)
(cid:12)φXm (Q) − EP

uniformly for a set of possible solutions (Telgarsky & Das-
gupta, 2013). However, in this paper, we provide uniform
deviation bounds of a more general form: For any distri-
bution P and a sample of m = f ((cid:15), δ, k, d, P ) points, we
require that with probability at least 1 − δ
(cid:15)
2

(cid:2)d(x, Q)2(cid:3)
(2)
uniformly for all Q ∈ Rd×k. The terms on the right-hand
side may be interpreted as follows: The ﬁrst term based on
the variance σ2 corresponds to a scale-invariant, additive
approximation error. The second term is a multiplicative ap-
proximation error that allows the guarantee to hold even for
solutions Q with a large expected quantization error. Similar
additive error terms were recently explored by Bachem et al.
(2016; 2017) in the context of seeding for k-Means.

σ2 +

EP

(cid:15)
2

There are three key reasons why we choose (2) over (1):
First, (1) is not scale-invariant and may thus not hold for
classes of distributions that are equal up to scaling. Sec-
ond, (1) may not hold for an unbounded solution space, e.g.
Rd×k. Third, we can always rescale P to unit variance and
(cid:2)d(x, Q)2(cid:3) ≤ σ2.
restrict ourselves to solutions Q with EP
Then, (2) implies (1) for a suitable transformation of P .

Importance of scale-invariance. If we scale all the points
in a data set X and all possible sets of solutions Q by some
λ > 0, then the empirical quantization error is scaled by
λ2. Similarly, if we consider the random variable λx where
x ∼ P , then the expected quantization error is scaled by
λ2. At the same time, the k-Means problem remains the
same: an optimal solution of the scaled problem is simply a
scaled optimal solution of the original problem. Crucially,
however, it is impossible to achieve the guarantee in (1) for
distributions that are equal up to scaling: Suppose that (1)
holds for some error tolerance (cid:15), and sample size m with
probability at least 1 − δ. Consider a distribution P and a
solution Q ∈ Rd×k such that with probability at least δ

a < (cid:12)

(cid:12)φXm (Q) − EP

(cid:2)d(x, Q)2(cid:3)(cid:12)
(cid:12) .
a(cid:15) , let ˜P be the distribution of
for some a > 0.1 For λ > 1√
the random variable λx where x ∼ P and let ˜Xm consist of
m samples from ˜P . Deﬁning ˜Q = {λq | q ∈ Q}, we have
with probability at least δ

(cid:12)
(cid:12)
(cid:12)φ ˜Xm

(cid:16) ˜Q
(cid:17)

− E ˜P

(cid:16)

(cid:104)
d

x, ˜Q

(cid:17)2(cid:105)(cid:12)
(cid:12) > aλ2 > (cid:15)
(cid:12)

1For example, let P be a nondegenerate multivariate normal

distribution and Q consist of k copies of the origin.

Uniform Deviation Bounds for k-Means Clustering

which contradicts (1) for the distribution ˜P and the solu-
tion ˜Q. Hence, (1) cannot hold for both P and its scaled
transformation ˜P .

Unrestricted solution space One way to guarantee scale-
invariance would be require that

(cid:12)
(cid:12)φXm(Q) − EP

(cid:2)d(x, Q)2(cid:3)(cid:12)

(cid:12) ≤ (cid:15)σ2

(3)

for all Q ∈ Rd×k. However, while (3) is scale-invariant, it is
also impossible to achieve for all solutions Q as the follow-
ing example shows. For simplicity, consider the 1-Means
problem in 1-dimensional space and let P be a distribution
with zero mean. Let Xm denote m independent samples
from P and denote by ˆµ the mean of Xm. For any ﬁnite m,
suppose that ˆµ (cid:54)= 0 with high probability2 and consider a
solution Q consisting of a single point q ∈ R. We then have

(cid:12)
(cid:12)φXm({q}) − EP

(cid:2)d(x, {q})2(cid:3)(cid:12)
(cid:12)
(cid:12)φXm({ˆµ}) + d(ˆµ, q)2 − σ2 − d(0, q)2(cid:12)
(cid:12)
(cid:12)φXm({ˆµ}) − σ2 + q2 − 2q ˆµ + ˆµ2 − q2(cid:12)
(cid:12)
(cid:12)φXm({ˆµ}) − σ2 + ˆµ2 − 2q ˆµ(cid:12)
(cid:12)

= (cid:12)
= (cid:12)
= (cid:12)

(4)

Since ˆµ (cid:54)= 0 with high probability, clearly this expression
diverges as q → ∞ and thus (3) cannot hold for arbitrary
solutions Q ∈ Rd×k. Intuitively, the key issue is that both
the empirical and the statistical error become unbounded as
q → ∞. Previous approaches such as Telgarsky & Dasgupta
(2013) solve this issue by restricting the solution space from
Rd×k to solutions that are no worse than some threshold. In
contrast, we allow the deviation between the empirical and
(cid:2)d(x, Q)2(cid:3).
the expected quantization error to scale with EP

Arbitrary distributions. Finally, we show that we either
need to impose assumptions on P or equivalently make the
relationship between m, (cid:15) and δ in (2) depend on the under-
lying distribution P . Suppose that there exists a sample size
m ∈ N, an error tolerance (cid:15) ∈ (0, 1) and a maximal failure
probability δ ∈ (0, 1) such that (2) holds for any distribution
P . Let P be the Bernoulli distribution on {0, 1} ⊂ R with
P [x = 1] = p for p ∈ (δ 1
m , 1). By design, we have µ = p,
(cid:2)d(x, 1)2(cid:3) = (1 − p). Furthermore,
σ2 = p(1 − p) and EP
with probability at least δ, the set Xm of m independent
samples from P consists of m copies of a point at one.
Hence, (2) implies that with probability at least 1 − δ

(cid:12)
(cid:12)φXm(1) − EP

(cid:2)d(x, 1)2(cid:3)(cid:12)

(cid:12) ≤ (cid:15)EP

(cid:2)d(x, 1)2(cid:3)

(cid:2)d(x, 1)2(cid:3). However, with probability at
since σ2 ≤ EP
least δ, we have φXm(1) = 0 which would imply 1 ≤ (cid:15) and
thus lead to a contradiction with (cid:15) ∈ (0, 1).

2For example, if P is the standard normal distribution.

4. Key results for k-Means

In this section, we present our main results for k-Means and
defer the analysis and proofs to Sections 6.

4.1. Kurtosis bound

Similar to Telgarsky & Dasgupta (2013), the weakest as-
sumption that we require is that the fourth moment of
d(x, µ) for x ∈ P is bounded.3 Our results are based on the
kurtosis of P which we deﬁne as

ˆM4 =

EP

(cid:2)d(x, µ)4(cid:3)

.

σ4

The kurtosis is the normalized fourth moment and is a scale-
invariant measure of the “tailedness” of a distribution. For
example, the normal distribution has a kurtosis of 3, while
more heavy tailed distributions such as the t-Student dis-
tribution or the Pareto distribution have a potentially un-
bounded kurtosis. A natural interpretation of the kurtosis is
provided by Moors (1986). For simplicity, consider a data
set with unit variance. Then, the kurtosis may be restated as
the shifted variance of d(x, µ)2, i.e.,

ˆM4 = Var (cid:0)d(x, µ)2(cid:1) + 1.

This provides a valuable insight into why the kurtosis is rel-
evant for our setting: For simplicity, suppose we would like
(cid:2)d(x, µ)2(cid:3)
to estimate the expected quantization error EP
by the empirical quantization error φXm({µ}) on a ﬁnite
sample Xm.4 Then, the kurtosis measures the dispersion
(cid:2)d(x, µ)2(cid:3) and provides a
of d(x, µ)2 around its mean EP
bound on how many samples are required to achieve an error
of (cid:15). While this simple example provides the key insight
for the trivial solution Q = {µ}, it requires a non-trivial
effort to extend the guarantee in (2) to hold uniformly for
all solutions Q ∈ Rd×k.

With the use of a novel framework to learn unbounded loss
functions (presented in Section 5), we are able to provide
the following guarantee for k-Means.
Theorem 1 (Kurtosis). Let (cid:15), δ ∈ (0, 1) and k ∈ N. Let P
be any distribution on Rd with kurtosis ˆM4 < ∞. For

12800

m ≥

(cid:17)

(cid:18)

(cid:16)

8 + ˆM4
(cid:15)2δ

3 + 30k(d + 4) log 6k + log

(cid:19)

1
δ

let X = {x1, x2, . . . , xm} be m independent samples from
P . Then, with probability at least 1 − δ, for all Q ∈ Rd×k

(cid:15)
2

(cid:15)
2

EP

(cid:12) ≤

σ2 +

(cid:2)d(x, Q)2(cid:3)(cid:12)

(cid:12)
(cid:12)φX (Q) − EP
3While our random variables x ∈ P are potentially multivari-
ate, it sufﬁces to consider the behavior of the univariate random
variable d(x, µ) for the assumptions in this section.

(cid:2)d(x, Q)2(cid:3) .

4This is a hypothetical exercise as EP

(cid:2)d(x, µ)2(cid:3) = 1 by de-
sign. However, it provides an insight to the importance of the
kurtosis.

Uniform Deviation Bounds for k-Means Clustering

The proof is provided in Section 6.1. The number of sufﬁ-
cient samples

m ∈ O

dk log k + log

(cid:18)

(cid:32) ˆM4
(cid:15)2δ

(cid:19)(cid:33)

1
δ

(cid:17)

is linear in the kurtosis ˆM4 and the dimensionality d, near-
linear in the number of clusters k and 1
δ , and quadratic in
1
(cid:15) . Intuitively, the bound may be interpreted as follows:
(cid:16) ˆM4
Ω
samples are required such that the guarantee holds
(cid:15)2δ
for a single solution Q ∈ Rd×k. Informally, a generaliza-
tion of the Vapnik Chervonenkis dimension for k-Means
clustering may be bounded by O(dk log k) and measures
the “complexity” of the learning problem. The multiplica-
tive dk log k + log 1
δ term intuitively extends the guarantee
uniformly to all possible Q ∈ Rd×k.

Comparison to Telgarsky & Dasgupta (2013). While we
require a bound on the normalized fourth moment, i.e., the
kurtosis, Telgarsky & Dasgupta (2013) consider the case
where all unnormalized moments up to the fourth are uni-
formly bounded by some M , i.e.,
(cid:2)d(x, µ)l(cid:3) ≤ M,

1 ≤ l ≤ 4.

EP

They provide uniform deviation bounds for all solutions Q
(cid:2)d(x, Q)2(cid:3) ≤ c for some
such that either φX (Q) ≤ c or EP
c > 0. To compare our bounds, we consider a data set with
unit variance and restrict ourselves to solutions Q ∈ Rd×k
with an expected quantization error of at most the variance,
(cid:2)d(x, Q)2(cid:3) ≤ σ2 = 1. Consider the deviation
i.e., EP
(cid:12)
(cid:12)φX (Q) − EP

(cid:2)d(x, Q)2(cid:3)(cid:12)
(cid:12) .

∆ =

sup
Q∈Rd×k:EP [d(x,Q)2]≤1

Telgarsky & Dasgupta (2013) bound this deviation by

∆ ∈ O

dk log(M dm) + log

+

(cid:32)(cid:115)

(cid:18)

M 2
√
m

(cid:19)

1
δ

(cid:114) 1
mδ2

(cid:33)
.

In contrast, our bound in Theorem 1 implies

Theorem 2 provides an improved uniform deviation bound
if P has bounded higher moments.
Theorem 2 (Moment bound). Let (cid:15) ∈ (0, 1), δ ∈ (0, 1)
and k ∈ N. Let P be any distribution on Rd with ﬁnite p-th
order moment bound ˆMp < ∞ for p ∈ {4, 8, . . . , ∞}. For
with
m ≥ max

p (cid:17)
(cid:1) 8

, (cid:0) 8

(cid:16) 3200m1
(cid:15)2

m1 = p

(cid:18)

4 + ˆMp

4
p

δ
(cid:19) (cid:18)

3 + 30k(d + 4) log 6k + log

(cid:19)

1
δ

let X = {x1, x2, . . . , xm} be m independent samples from
P . Then, with probability at least 1 − δ, for all Q ∈ Rd×k
(cid:2)d(x, Q)2(cid:3) .

(cid:2)d(x, Q)2(cid:3)(cid:12)

(cid:12)
(cid:12)φX (Q) − EP

σ2 +

(cid:12) ≤

EP

(cid:15)
2

(cid:15)
2

The proof is provided in Section 6.2. Compared to the
previous bound based on the kurtosis, Theorem 2 requires

m ∈ Ω

dk log k + log

+

4
p

(cid:18)





p ˆMp
(cid:15)2

(cid:19)

1
δ





(cid:19) 8

p

(cid:18) 1
δ

samples. With higher order moment bounds, it is easier to
achieve high probability results since the dependence on 1
δ
compared to near linear for a kurtosis

is only of O

p (cid:17)
(cid:1) 8

(cid:16)(cid:0) 1
δ

4

bound. The quantity ˆMp
p may be interpreted as a bound on
the kurtosis ˆM4 based on the higher order moment ˆMp since
Hoelder’s inequality implies that ˆM4 ≤ ˆMp
p . While the
result only holds for p ∈ {8, 12, 16, . . . , ∞}, it is trivially
(cid:107)
extended to p(cid:48) ≥ 8: Consider Theorem 2 with p = 4

4

(cid:106) p(cid:48)
4

and note that by Hoelder’s inequality ˆMp

p ≤ ˆMp(cid:48)

4

4

p(cid:48) .

Comparison to Telgarsky & Dasgupta (2013). Again,
we consider distributions P that have unit variance and
we restrict ourselves to solutions Q ∈ Rd×k with an
expected quantization error of at most the variance, i.e.,
(cid:2)d(x, Q)2(cid:3) ≤ σ2 = 1. Telgarsky & Dasgupta (2013)
EP
require that there exists a bound M

EP

(cid:2)d(x, µ)l(cid:3) ≤ M,

1 ≤ l ≤ p.



(cid:115)

(cid:18)

ˆM4
mδ



(cid:19)

.

1
δ

∆ ∈ O



dk log k + log

Then, for m sufﬁciently large, ∆ is of

The key difference lies in how ∆ scales with the sample
size m. While Telgarsky & Dasgupta (2013) show a rate of
∆ ∈ O

, we improve it to ∆ ∈ O

m− 1

m− 1

(cid:17)

(cid:16)

(cid:16)

(cid:17)

.

4

2

4.2. Bounded higher moments

The tail behavior of d(x, µ) may be characterized by the
moments of P . For p ∈ N, consider the standardized p-th
moment of P , i.e.,

ˆMp =

EP [d(x, µ)p]
σp

.



(cid:115)

O



(cid:18)

8
p

M
m1− 4

p

dk ln(M

4
p dm) + ln

(cid:19)

1
δ

+

p
4

2
4 − 2
3

p

(cid:18) 1
δ

m

(cid:19) 4

p



.

In contrast, we obtain that, for m sufﬁciently large,




(cid:118)
(cid:117)
(cid:116) p ˆMp
(cid:117)
m




4
p

(cid:18)

∆ ∈ O

dk log k + log

(cid:19)

1
δ


.

(cid:17)

(cid:16)

While Telgarsky & Dasgupta (2013) only show a rate of
O
rate for all
higher moment bounds.

as p → ∞, we obtain a ∈ O

m− 1

m− 1

(cid:17)

(cid:16)

2

2

Uniform Deviation Bounds for k-Means Clustering

4.3. Subgaussianity

If the distribution P is subgaussian, then all its moments
ˆMp are bounded. By optimizing p in Theorem 2, we are
able to show the following bound.

Theorem 3 (Subgaussian bound). Let (cid:15) ∈ (0, 1), δ ∈ (0, 1)
and k ∈ N. Let P be any distribution on Rd with µ =
EP [x] and

∀t > 0 :

P [d(x, µ) > tσ] ≤ a exp

−

(cid:18)

(cid:19)

t2
√
b

for some a > 1, b > 0. Let m ≥ 3200m1

with

(cid:15)2

(cid:18)

m1 = p

4 +

(cid:19) (cid:18)

abp2
4

3 + 30k(d + 4) log 6k + log

(cid:19)

.

1
δ

and p = 9 + 3 log 1
δ . Let X = {x1, x2, . . . , xm} be m
independent samples from P . Then, with probability at least
1 − δ, for all Q ∈ Rd×k

(cid:12)
(cid:12)φX (Q) − EP

(cid:2)d(x, Q)2(cid:3)(cid:12)

(cid:12) ≤

σ2 +

EP

(cid:2)d(x, Q)2(cid:3) .

(cid:15)
2

(cid:15)
2

The proof is provided in Section 6.3. In O(·) notation,

m ∈ O

dk log k + log

(cid:32)

(cid:18)

ab log3 1
δ
σ(cid:15)2

(cid:19)(cid:33)

1
δ

samples are hence sufﬁcient. This result features a poly-
logarithmic dependence on 1
δ compared to the polynomial
dependence for the bounds based on bounded higher mo-
ments. The sufﬁcient sample size further scales linearly
with the (scale-invariant) subgaussianity parameters a and b.
For example, if P is a one-dimensional normal distribution
of any scale, we would have a = 2 and b = 1.

4.4. Bounded support

σ4 . This allows us to obtain Theorem 4.

The strongest assumption that we consider is if the support
of P is bounded by a hypersphere in Rd with diameter
R > 0. This ensures that almost surely d(x, µ) ≤ R and
hence ˆM4 ≤ R4
Theorem 4 (Bounded support). Let (cid:15) ∈ (0, 1), δ ∈ (0, 1)
and k ∈ N. Let P be any distribution on Rd, with µ =
(cid:2)d(x, µ)2(cid:3) ∈ (0, ∞), whose support
EP [x] and σ2 = EP
is contained in a d-dimensional hypersphere of diameter
R > 0. For

12800

(cid:16)

8 + R4
σ4

(cid:17)

(cid:18)

m ≥

(cid:15)2

3 + 30k(d + 4) log 6k + log

(cid:19)

1
δ

let X = {x1, x2, . . . , xm} be m independent samples from
P . Then, with probability at least 1 − δ, for all Q ∈ Rd×k

(cid:12)
(cid:12)φX (Q) − EP

(cid:2)d(x, Q)2(cid:3)(cid:12)

(cid:12) ≤

σ2 +

EP

(cid:2)d(x, Q)2(cid:3) .

(cid:15)
2

(cid:15)
2

The proof is provided in Section 6.4. Again, the sufﬁcient
sample size scales linearly with the kurtosis bound R4
σ4 . How-
ever, the bound is only logarithmic in 1
δ .

5. Framework for unbounded loss functions

To obtain the results presented in Section 4, we propose a
novel framework to uniformly approximate the expected
values of a set of unbounded functions based on a random
sample. We consider a function family F mapping from
an arbitrary input space X to R≥0 and a distribution P
on X . We further require a generalization of the Vapnik-
Chervonenkis dimension to continuous, unbounded func-
tions5 — the pseudo-dimension.
Deﬁnition 1 (Haussler (1992); Li et al. (2001)). The pseudo-
dimension of a set F of functions from X to R≥0, de-
noted by Pdim(F), is the largest d(cid:48) such there is a se-
quence x1, . . . , xd(cid:48) of domain elements from X and a se-
quence r1, . . . , rd(cid:48) of reals such that for each b1, . . . , bd(cid:48) ∈
{above, below}, there is an f ∈ F such that for all
i = 1, . . . , d(cid:48), we have f (xi) ≥ ri ⇐⇒ bi = above.

Similar to the VC dimension, the pseudo-dimension mea-
sures the cardinality of the largest subset of X that can be
shattered by the function family F. Informally, the pseudo-
dimension measures the richness of F and plays a critical
role in providing a uniform approximation guarantee across
all f ∈ F. With this notion, we are able to state the main
result in our framework.
Theorem 5. Let (cid:15) ∈ (0, 1), δ ∈ (0, 1) and t > 0. Let F
be a family of functions from X to R≥0 with Pdim(F ) =
d < ∞. Let s : X → R≥0 be a function such that s(x) ≥
supf ∈F f (x) for all x ∈ X . Let P be any distribution on
X and for

m ≥

3 + 5d + log

(cid:18)

200t
(cid:15)2

(cid:19)

,

1
δ

let x1, x2, . . . , x2m be 2m independent samples from P .

Then, if

EP

(cid:2)s(x)2(cid:3) ≤ t and P

(cid:34)

1
2m

2m
(cid:88)

i=1

(cid:35)
s(xi)2 > t

≤

δ
4

,

(5)

it holds with probability at least 1 − δ that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

(cid:12)
(cid:12)
f (xi) − EP [f (x)]
(cid:12)
(cid:12)
(cid:12)

≤ (cid:15),

∀f ∈ F.

(6)

Applying Theorem 5 to a function family F requires three
steps: First, one needs to bound the pseudo-dimension of

5The pseudo-dimension was originally deﬁned for sets of func-
tions mapping to [0, 1] (Haussler, 1992; Li et al., 2001). However,
it is trivially extended to unbounded functions mapping to R≥0.

Uniform Deviation Bounds for k-Means Clustering

F. Second, it is necessary to ﬁnd a function s : X → R≥0
such that

bounded using Hoeffding’s inequality by

f (x) ≤ s(x),

∀x ∈ X and ∀f ∈ F.

Ideally, such a bound should be as tight as possible. Third,
one needs to ﬁnd some t > 0 and a sample size

m ≥

3 + 5d + log

(cid:18)

200t
(cid:15)2

(cid:19)

1
δ

such that

(cid:104)

s(x)2(cid:105)

≤ t

EP

and P

(cid:34)

1
2m

2m
(cid:88)

i=1

(cid:35)
s(xi)2 > t

≤

δ
4

.

Finding such a bound usually entails examining the tail
behavior of s(x)2 under P . Furthermore, it is evident that
a bound t may only be found if EP
is bounded
and that assumptions on the distribution P are required. In
Section 6, we will see that for k-Means a function s(x)
with EP
< ∞ may be found if the kurtosis of P is
bounded.

s(x)2(cid:105)

s(x)2(cid:105)

(cid:104)

(cid:104)

We defer the proof of Theorem 5 to Section B of the Supple-
mentary Materials and provide a short proof sketch that cap-
tures the main insight. The proof is based on symmetrization,
the bounding of covering numbers and chaining — com-
mon techniques in the empirical process literature (Pollard,
1984; Li et al., 2001; Boucheron et al., 2013; Koltchinskii,
2011; van der Vaart & Wellner, 1996). The novelty lies in
considering loss functions f (·) and cover functions s(·) in
Theorem 5 that are potentially unbounded.

Proof sketch. Our proof is based on a double sampling ap-
proach. Let xm+1, xm+2, . . . , x2m be an additional m in-
dependent samples from P and let σ1, σ2, . . . , σm be inde-
pendent random variables uniformly sampled from {−1, 1}.
s(x)2(cid:105)
(cid:104)
Then, we show that, if EP
≤ t, the probability
of (6) not holding may be bounded by the probability that
there exists a f ∈ F such that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

σi (f (xi) − f (xi+m))

> (cid:15).

(7)

We ﬁrst provide the intuition for a single function f ∈ F and
then show how we extend it to all f ∈ F. While the function
f (x) is not bounded, for a given sample x1, x2, . . . , x2m,
each f (xi) is contained within [0, s(xi)]. Given the sample
x1, x2, . . . , x2m, the random variable σi (f (xi) − f (xi+m)
is bounded in 0 ± max (s(xi), s(xi+m)) and has zero mean.
Hence, given independent samples x1, x2, . . . , x2m, the
probability of (7) occurring for a single f ∈ F can be

(cid:32)

2 exp

−

1
m
(cid:32)

≤ 2 exp

−

2m(cid:15)2

(cid:33)

(cid:80)m

i=1 max (s(xi), s(xi+m))2

m(cid:15)2
(cid:80)2m
i=1 s(xi)2

1
2m

(cid:33)

.

(cid:80)2m

least 1 − δ

By (5), with probability at
i=1 s(xi)2 ≤ t and we hence require m ∈ Ω
1
2m
samples to guarantee that (7) does not hold for a single
f ∈ F with probability at least 1 − δ
2 .

4 , we have
(cid:17)
(cid:16) t log 1
(cid:15)2

δ

To bound the probability that there exists any f ∈ F such
that (7) holds, we show in Lemma 5 (see Section B of the
Supplementary Materials) that, given independent samples
x1, x2, . . . , x2m,

(cid:34)

P

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

∃f ∈ F :

σi(f (xi) − f (xi+m))

> (cid:15)

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 4 (cid:0)16e2(cid:1)Pdim(F )

e

−

200 1
2m

(cid:15)2m
(cid:80)2m
i=1

s(xi)2 .

The key difﬁculty in proving Lemma 5 is that the functions
f ∈ F are not bounded uniformly in [0, 1]. To this end,
we provide in Lemma 4 a novel result that bounds the size
of (cid:15)-packings of F if the functions f ∈ F are bounded in
expectation. Based on Lemma 5, we then prove the main
claim of Theorem 5.

6. Analysis for k-Means

s(x)2(cid:105)
(cid:104)

In order to apply Theorem 5 to k-Means clustering, we re-
quire a suitable family F, an upper bound s(x) and a bound
on EP
. We provide this in Lemma 1 and defer
i=1 s(xi)2 to the proofs of Theorems 2-4.
bounding 1
2m
Lemma 1 (k-Means). Let k ∈ N. Let P be any distribution
(cid:2)d(x, µ)2(cid:3) ∈ (0, ∞) and
on Rd with µ = EP [x], σ2 = EP
bounded kurtosis ˆM4. For any x ∈ Rd and any Q ∈ Rd×k,
deﬁne

(cid:80)2m

fQ(x) =

d(x, Q)2

1

2 σ2 + 1

2

EP [d(x, Q)2]

(8)

as well as the function family F = (cid:8)fQ(·) | Q ∈ Rd×k(cid:9).
Let

s(x) =

4 d(x, µ)2
σ2

+ 8.

We then have

Pdim(F) ≤ 6k(d + 4) log 6k,

fQ(x) ≤ s(x)

(9)

(10)

for any x ∈ Rd and Q ∈ Rd×k and

Proof. Hoelder’s inequality implies

Uniform Deviation Bounds for k-Means Clustering

(cid:104)

s(x)2(cid:105)

EP

= 128 + 16 ˆM4.

(11)

The proof of Lemma 1 is provided in Section C of the
Supplementary Materials. The deﬁnition of fQ(x) in (8) is
motivated as follows: If we use Theorem 5 to guarantee

(cid:12)
m
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:12)
(cid:12)
f (xi) − EP [f (x)]
(cid:12)
(cid:12)
(cid:12)

≤ (cid:15) ∀f ∈ F.

(12)

then this implies

(cid:12)
(cid:12)φX (Q) − EP

(cid:2)d(x, Q)2(cid:3)(cid:12)

(cid:12) ≤

σ2 +

EP

(cid:2)d(x, Q)2(cid:3)

(cid:15)
2

(cid:15)
2

(13)
as is required by Theorems 2-4. Lemma 1 further shows
that E [s(x)]2 is bounded if and only if the kurtosis of P is
bounded. This is the reason why a bounded kurtosis is the
weakest assumption on P that we require in Section 4.

We now proceed to prove Theorems 2-4 by applying Theo-
i=1 s(xi)2.
rem 5 and examining the tail behavior of 1
2m

(cid:80)2m

6.1. Proof of Theorem 1 (kurtosis bound)

The bound based on the kurtosis follows easily from
Markov’s inequality.

Proof. We consider the choice t = 4
/δ.
By Markov’s inequality and linearity of expectation, we
then have by Lemma 1 that

128 + 16 ˆM4

(cid:16)

(cid:17)

(cid:34)

P

1
2m

2m
(cid:88)

i=1

(cid:35)

s(xi)2 > t

≤

E (cid:2) s(x)2 (cid:3)
t

=

δ
4

.

≤ t. Hence, we may apply Theo-

s(x)2(cid:105)
(cid:104)

Furthermore, EP
rem 5 to obtain that for
(cid:17)

(cid:16)

12800

8 + ˆM4
(cid:15)2δ

(cid:18)

m ≥

it holds with probability at least 1 − δ that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

(cid:12)
(cid:12)
f (xi) − E [f (x)]
(cid:12)
(cid:12)
(cid:12)

≤ (cid:15) ∀f ∈ F.

This implies the main claim and thus concludes the proof.

6.2. Proof of Theorem 2 (higher order moment bound)

We prove the result by bounding the higher moments
i=1 s(xi)2 using the Marcinkiewicz-Zygmund in-
of 1
2m
equality and subsequently applying Markov’s inequality.

(cid:80)2m

EP

(cid:2)d(x, µ)4(cid:3)

ˆM4 =

EP [d(x, µ)p]
σ4
σ4
Hence, by Lemma 1 we have that EP
16 ˆMp

p Since s(x)2 ≥ 0 for all x ∈ Rd, we have

≤

4
p

4

4
p

≤ ˆMp

(cid:2)s(x)2(cid:3) ≤ 128 +

(cid:12)
(cid:12)s(x)2 − EP

(cid:2)s(x)2(cid:3)(cid:12)

(cid:12) ≤ max (cid:0)s(x)2, EP
(cid:18)

(cid:2)s(x)2(cid:3)(cid:1)

≤ max

s(x)2, 128 + 16 ˆMp

(cid:19)

4
p

≤ 128

+ 16 max

(cid:18)

ˆMp

4

p , 2

d(x, µ)4
σ4

(cid:19)

.

(14)

This implies that

EP

(cid:104)(cid:12)
(cid:12)s(x)2 − EP

p

4 (cid:105)

(cid:2)s(x)2(cid:3)(cid:12)
(cid:12)
(cid:18)

≤ 256

p
4 + 32

p
4 max

≤ 256

p
4 + 32

≤ 256

p
4 + 64

p
4 max
4 ˆMp.

p

p
4

EP [d(x, µ)p]
σp

(cid:19)

(15)

ˆMp, 2
(cid:16) ˆMp, 2

(cid:17)

p

4 ˆMp

We apply a variant of the Marcinkiewicz-Zygmund inequal-
ity (Ren & Liang, 2001) to the zero-mean random variable
s(x)2 − EP

(cid:2)s(x)2(cid:3) to obtain

EP

2m
(cid:88)

1
2m


(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
i=1
(cid:18) p − 4
√
2m
4
(cid:18) p − 4
√
2m
4

≤

≤

(cid:0)s(xi)2 − EP

(cid:2)s(x)2(cid:3)(cid:1)

p

4 



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19) p

4

EP

(cid:104)(cid:12)
(cid:12)s(x)2 − EP

(cid:2)s(x)2(cid:3)(cid:12)
(cid:12)

p

4 (cid:105)

(16)

(cid:19) p

4 (cid:16)

256

p
4 + 64

p

4 ˆMp

(cid:17)

P

(cid:34) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2m
(cid:88)

1
2m
i=1
(cid:18) p − 4
√
2m
4u
(cid:18) p − 4
√
4u
2m
(cid:18) p − 4
√
2m
4u
(cid:18) p − 4
√
2m
u

≤

≤

≤ 2

≤ 2

(cid:0)s(xi)2 − EP

(cid:12)
(cid:12)
(cid:2)s(x)2(cid:3)(cid:1)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

> u

256

p
4 + 64

p

4 ˆMp

(cid:17)

(cid:19) p

4 (cid:16)

(cid:19) p

4

(cid:16)

2 max

256

p
4 , 64

p

4 ˆMp

(cid:17)

(17)

(cid:18)

max

256, 64 ˆMp

(cid:19)(cid:19) p

4

4
p

(cid:18)

64 + 16 ˆMp

(cid:19)(cid:19) p

4

4
p

.

3 + 30k(d + 4) log 6k + log

For u > 0, the Markov inequality implies

(cid:19)

,

1
δ

For u = (p − 4)

64 + 16 ˆMp

, we thus have

(cid:18)

(cid:19)

4
p

Let u(t) = b

2 which implies du/dt = b

p

p
4 t

p
4

p
2 t

p

2 −1. Hence,

Uniform Deviation Bounds for k-Means Clustering

P

(cid:34) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2m

2m
(cid:88)

i=1

(cid:0)s(xi)2 − EP

(cid:12)
(cid:12)
(cid:2)s(x)2(cid:3)(cid:1)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

> u

≤ 2m− p

8

ˆMp ≤

p
ab
4 p
2

(cid:90) ∞

0

p

e−tt

2 −1dt.

(18)

By the deﬁnition of the gamma function and since p is even,
we have
(cid:90) ∞

(cid:17) p

2 −1

p

e−tt

2 −1dt = Γ

(cid:17)

(cid:16) p
2

=

(cid:16) p
2

(cid:17)

− 1

! ≤

(cid:16) p
2

0

(cid:0)s(xi)2 − EP

(cid:2)s(x)2(cid:3)(cid:1)

> u

≤

(19)

Hence, for p ∈ {4, 8, . . . , ∞}, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

δ
4

Since m ≥ (cid:0) 8

(cid:1) 8

δ

p , this implies

P

(cid:34) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2m

2m
(cid:88)

i=1

It holds that

(cid:19)

4
p

(cid:18)

64 + 16 ˆMp
(cid:19)

4
p

(cid:18)

≤ p

64 + 16 ˆMp

u + EP

(cid:2)s(x)2(cid:3) = (p − 4)

+ 128 + 16 ˆMp

4
p

Let p∗ = 4 (cid:6) 5

4 + 3

4 log 1

δ

(cid:7) which implies

ˆMp

4

p ≤

1
4

4

a

p bp2 ≤

abp2.

1
4

p∗ ≥ 5 + 3 log

≥

1
δ

8
log 48

log

8
δ

(20)

p∗ ≤ 48. We instantiate Theorem 2 with

(cid:1) 8

δ

and thus (cid:0) 8
the p∗th-order bound ˆMp∗ of P . Since (cid:0) 8
minimum sample size is thus

δ

(cid:1) 8

p∗ ≤ 48, the

3200p∗
(cid:15)2

(cid:18)

4 +

abp∗
4

2

(cid:19) (cid:18)

(21)

3 + 30k(d + 4) log 6k + log

(cid:19)

.

1
δ

The main claim ﬁnally holds since p∗ ≤ p = 9 + 3 log 1
δ .

This implies the main claim and thus concludes the proof.

7. Conclusion

We set t = p

64 + 16 ˆMp

and thus have

(cid:18)

(cid:19)

4
p

(cid:34)

P

1
2m

2m
(cid:88)

i=1

s(xi)2 > t

≤

(cid:35)

δ
4

In combination with EP
thus apply Theorem 5. Since m ≥ 3200m1

(cid:104)

s(x)2(cid:105)

(cid:15)2 with

≤ t by Lemma 1, we may

(cid:18)

m1 = p

4 + ˆMp

(cid:19) (cid:18)

4
p

3 + 30k(d + 4) log 6k + log

(cid:19)

1
δ

it holds with probability at least 1 − δ that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

(cid:12)
(cid:12)
f (xi) − E [f (x)]
(cid:12)
(cid:12)
(cid:12)

≤ (cid:15) ∀f ∈ F.

6.3. Proof of Theorem 3 (subgaussianity)

Under subgaussianity, all moments of d(x, µ) are bounded.
We show the result by optimizing over p in Theorem 2.

Proof. For p ∈ {4, 8, . . . , ∞}, we have

p(cid:21)

(cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)

d(x, µ)
σ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

(cid:20) d(x, µ)
σ
(cid:32)

a exp

−

ˆMp = EP
(cid:90) ∞

=

≤

0
(cid:90) ∞

0

(cid:33)

2
p

u
√

b

du.

(cid:21)

1
p

> u

du

(22)

6.4. Proof of Theorem 4 (bounded support)

Proof. Let t = 128 + 64R4/σ4. Since the support of P is
bounded, we have s(x) ≤ t for all x ∈ Rd. This implies
i=1 s(xi)2 ≤ t almost
that EP
≤ t and that 1
2m
surely. The result then follows from Theorem 5.

s(x)2(cid:105)

(cid:80)2m

(cid:104)

We have presented a framework to uniformly approximate
the expected value of unbounded functions on a sample.
With this framework we are able to provide theoretical guar-
antees for empirical risk minimization in k-Means clustering
if the kurtosis of the underlying distribution is bounded. In
particular, we obtain state-of-the art bounds on the sufﬁcient
number of samples to achieve a given uniform approxima-
tion error. If the underlying distribution fulﬁlls stronger
assumptions, such as bounded higher moments, subgaus-
sianity or bounded support, our analysis yields progressively
better bounds. We conjecture that Theorem 5 can be applied
to other related problems such as hard and soft Bregman
clustering, likelihood estimation of Gaussian mixture mod-
els, as well as nonparametric clustering problems. However,
such results do not follow immediately and require addi-
tional arguments beyond the scope of this paper.

Uniform Deviation Bounds for k-Means Clustering

Acknowledgements

This research was partially supported by SNSF NRP 75,
ERC StG 307036, a Google Ph.D. Fellowship and an IBM
Ph.D. Fellowship. This work was done in part while An-
dreas Krause was visiting the Simons Institute for the The-
ory of Computing.

References

Antos, Andr´as, Gyorﬁ, L, and Gyorgy, Andras. Individual conver-
gence rates in empirical vector quantizer design. IEEE Transac-
tions on Information Theory, 51(11):4013–4022, 2005.

Bachem, Olivier, Lucic, Mario, and Krause, Andreas. Coresets for
nonparametric estimation - the case of DP-means. In Interna-
tional Conference on Machine Learning (ICML), 2015.

Bachem, Olivier, Lucic, Mario, Hassani, S. Hamed, and Krause,
Andreas. Fast and provably good seedings for k-means. In
Advances in Neural Information Processing Systems (NIPS), pp.
55–63, 2016.

Bachem, Olivier, Lucic, Mario, and Krause, Andreas. Distributed
and provably good seedings for k-means in constant rounds. In
To appear in International Conference on Machine Learning
(ICML), pp. 209–217, 2017.

Bartlett, Peter L, Linder, Tam´as, and Lugosi, G´abor. The minimax
IEEE
distortion redundancy in empirical quantizer design.
Transactions on Information Theory, 44(5):1802–1813, 1998.

Ben-David, Shai. A framework for statistical clustering with
constant time approximation algorithms for k-median and k-
means clustering. Machine Learning, 66(2-3):243–257, 2007.

Ben-David, Shai, Von Luxburg, Ulrike, and P´al, D´avid. A sober
In International Conference on
look at clustering stability.
Computational Learning Theory (COLT), pp. 5–19. Springer,
2006.

Boucheron, St´ephane, Lugosi, G´abor, and Massart, Pascal. Concen-
tration inequalities: A nonasymptotic theory of independence.
Oxford University Press, 2013.

Dinh, Vu C, Ho, Lam S, Nguyen, Binh, and Nguyen, Duy. Fast
learning rates with heavy-tailed losses. In Advances in Neural
Information Processing Systems (NIPS), pp. 505–513, 2016.

Gr¨unwald, Peter D and Mehta, Nishant A. Fast rates with un-

bounded losses. arXiv preprint arXiv:1605.00252, 2016.

Har-Peled, Sariel. Geometric approximation algorithms, volume

173. American Mathematical Society Boston, 2011.

Haussler, David. Decision theoretic generalizations of the pac
model for neural net and other learning applications. Informa-
tion and Computation, 100(1):78–150, 1992.

Hoeffding, Wassily. Probability inequalities for sums of bounded
random variables. Journal of the American Statistical Associa-
tion, 58(301):13–30, 1963.

Koltchinskii, Vladimir. Oracle Inequalities in Empirical Risk
Minimization and Sparse Recovery Problems. Lecture Notes in
Mathematics. Springer, 2011.

Levrard, Cl´ement et al. Fast rates for empirical vector quantization.

Electronic Journal of Statistics, 7:1716–1746, 2013.

Li, Yi, Long, Philip M, and Srinivasan, Aravind. Improved bounds
on the sample complexity of learning. Journal of Computer and
System Sciences, 62(3):516–527, 2001.

Linder, Tam´as, Lugosi, G´abor, and Zeger, Kenneth. Rates of con-
vergence in the source coding theorem, in empirical quantizer
design, and in universal lossy source coding. IEEE Transactions
on Information Theory, 40(6):1728–1740, 1994.

Lucic, Mario, Bachem, Olivier, and Krause, Andreas. Strong
coresets for hard and soft bregman clustering with applications
to exponential family mixtures. In International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS), May 2016.

Lucic, Mario, Faulkner, Matthew, Krause, Andreas, and Feldman,
Dan. Training mixture models at scale via coresets. To appear
in Journal of Machine Learning Research (JMLR), 2017.

Mendelson, Shahar. Learning without concentration for general

loss functions. arXiv preprint arXiv:1410.3192, 2014.

Mendelson, Shahar et al. Learning without concentration.

In
International Conference on Computational Learning Theory
(COLT), pp. 25–39, 2014.

Moors, Johannes J A. The meaning of kurtosis: Darlington reex-
amined. The American Statistician, 40(4):283–284, 1986.

Pollard, David. Strong consistency of k-means clustering. The

Annals of Statistics, 9(1):135–140, 1981.

Pollard, David. Convergence of stochastic processes. Springer

Series in Statistics. Springer, 1984.

Rakhlin, Alexander and Caponnetto, Andrea. Stability of k-means
clustering. Advances in Neural Information Processing Systems
(NIPS), 19:1121, 2007.

Ren, Yao-Feng and Liang, Han-Ying. On the best constant in
Marcinkiewicz-Zygmund inequality. Statistics & Probability
Letters, 53(3):227–233, 2001.

Sauer, Norbert. On the density of families of sets. Journal of

Combinatorial Theory, Series A, 13(1):145–147, 1972.

Shamir, Ohad and Tishby, Naftali. Cluster stability for ﬁnite
samples. In Advances in Neural Information Processing Systems
(NIPS), pp. 1297–1304, 2007.

Shamir, Ohad and Tishby, Naftali. Model selection and stability
in k-means clustering. In International Conference on Com-
putational Learning Theory (COLT), pp. 367–378. Citeseer,
2008.

Telgarsky, Matus J and Dasgupta, Sanjoy. Moment-based uniform
In Advances in
deviation bounds for k-means and friends.
Neural Information Processing Systems (NIPS), pp. 2940–2948,
2013.

van der Vaart, Aad W and Wellner, Jon A. Weak convergence and
empirical processes: with applications to statistics. Springer
Series in Statistics. Springer New York, 1996.

Vapnik, Vladimir N and Chervonenkis, Alexey Ya. On the uniform
convergence of relative frequencies of events to their probabili-
ties. Theory of Probability & Its Applications, 16(2):264–280,
1971.

A. Auxiliary lemmas

For j ≥ 2, we have

j −

j − 1 ≤

2 − 1 and hence

√

√

√

Uniform Deviation Bounds for k-Means Clustering

For the following proofs we require two auxiliary lemmas.

Lemma 2. Let x > 0 and a > 0. If

x ≤ a log x

(23)

then it holds that

x ≤ 2a log 2a.

√

x > 0 and thus log

x ≤

(24)

√

Proof. Since x > 0, we have
√

x. Together with (23), this implies
√

x ≤ a log x = 2a log

x ≤ 2a

x,

√

and thus

x ≤ 4a2.

(cid:32)

1 −

(cid:33)

(cid:114) 1
2

Sn ≤

(cid:16)√

+

2 − 1

(cid:17) n
(cid:88)

(cid:114) 1
2
(cid:114) 1
2
(cid:114) 1
2

=

=

√

√

2 − 1
2

2 − 1
2

+

+

j

(cid:114) 1
2

j−2

j=2

n
(cid:88)

j=2

n
(cid:88)

j=0
(cid:124)

(cid:114) 1
2
(cid:114) 1
2

(cid:123)(cid:122)
(∗)

.

j

(cid:125)

The

term (∗)
(cid:80)n

j=0

is
(cid:113) 1
j
2

limn→∞

a geometric

series

and hence

(cid:16)

=

1 −

(cid:113) 1
2

(cid:17)−1

. This implies

We show the result by contradiction. Suppose that

lim
n →∞

Sn ≤

x > 2a log 2a.

Together with (23), this implies

2a log 2a < a log x

which in turn leads to the contradiction

x > 4a2.

This concludes the proof since (24) must hold.

Lemma 3. For n ∈ N, deﬁne

Sn =

(cid:114) j
2j

n
(cid:88)

j=1

lim
n→∞

Sn ≤ 5.

Then,

Proof. Subtracting

from Sn yields

(cid:114) 1
2

Sn =

n
(cid:88)

(cid:114) j

2j+1 =

j=1

(cid:114) j − 1
2j

n
(cid:88)

j=2

(cid:32)

1 −

(cid:33)

(cid:114) 1
2

Sn =

√

√

j −

j − 1

n
(cid:88)

j=1

(cid:114) 1
2

=

+

2j/2
√

n
(cid:88)

j=2

√

j −

j − 1

.

2j/2

(cid:33)−1

(cid:32)

1 −

(cid:114) 1
2


(cid:114) 1


2

√

(cid:16)

2

1 −

(cid:17)

2 − 1
(cid:113) 1
2
√






(cid:16)

+

1

=

√

(cid:16)

2

1 −

(cid:113) 1
2

(cid:17) +

2

(cid:16)

1 −

(cid:17)

(cid:113) 1
2
(cid:17)2

(cid:113) 1
2

2

1 −

=

√

=

√

2
2 − 1
2
2 − 1
√
2

= 2 + 2
≤ 5

√
√

2 + 1
2 + 1

as desired.

B. Proof of Theorem 5

We ﬁrst show two results, Lemma 4 and 5 and then use them
to prove Theorem 5.
Deﬁnition 2. Let F be a family of functions from X to R≥0
and Q an arbitrary measure on X . For any f, g ∈ F, we
deﬁne the distance function

dL1(Q)(f, g) =

|f (x) − g(x)| dQ(x).

(cid:90)

X

For any f ∈ F and A ⊆ F, we further deﬁne

dL1(Q)(f, A) = min
g∈A

dL1(Q)(f, g).

Deﬁnition 3. For (cid:15) > 0, a set A ⊆ B is an (cid:15)-packing of B
with respect to some metric d if for any two distinct x, y ∈ A,
d(x, y) > (cid:15). The cardinality of the largest (cid:15)-packing of B
with respect to d is denoted by M ((cid:15), B, d).

Uniform Deviation Bounds for k-Means Clustering

Lemma 4 ((cid:15)-packing). Let F be a family of functions from
X to R≥0 with Pdim(F ) = d. For all x ∈ X , let s(x) =
supf ∈F f (x). Let Q be an arbitrary measure on X with
0 < EQ [s(x)] < ∞. Then, for all 0 < (cid:15) ≤ EQ [s(x)],

M (cid:0)(cid:15), F, dL1(Q)

(cid:1) ≤ 8

(cid:18) 2eEQ [s(x)]
(cid:15)

(cid:19)2d

.

Consider the set X0 = {x ∈ X : s(x) = 0} and deﬁne
X>0 = X \ X0. By deﬁnition of ˜Q, X0 is zero set of
˜Q and, since f (x), g(x) ∈ [0, s(x)] for all x ∈ X , we have
(cid:82)

|f (x) − g(x)| dQ(x) = 0.

X0
For any two distinct f, g ∈ G, we thus have for all i =
1, . . . , m

Proof. Our proof is similar to the proof of Theorem 6
in Haussler (1992). The difference is that we consider a
function family F that is not uniformly bounded but only
bounded in expectation. The key idea is to construct a ran-
dom sample and to use the expected number of dichotomies
on that set to bound the size of an (cid:15)-packing by the pseudo-
dimension.
Noting that by deﬁnition s(x) ≥ 0 and EQ [s(x)] < ∞, we
deﬁne the probability measure ˜Q on X using the Radon-
Nikodym derivative

d ˜Q(x)
dQ(x)

=

s(x)
EQ [s(x)]

,

∀x ∈ X .

Let (cid:126)x = (x1, x2, . . . , xm) be a random vector in X m, where
each xi is drawn independently at random from ˜Q. Given
(cid:126)x, let (cid:126)r = (r1, r2, . . . , rm) be a random vector, where each
ri is drawn independently at random from a uniform distri-
bution on [0, s(xi)].

∈

any f

restriction
For
(f (x1), . . . , f (xm)) by f(cid:126)x and set F(cid:126)x = {f(cid:126)x | f ∈ F}.
For any vector (cid:126)z ∈ Rm, we deﬁne

F, we denote

the

sign((cid:126)z) = (sign(z1), . . . , sign(zm)) .

The set of dichotomies induced by (cid:126)r on F(cid:126)x is given by

sign(F(cid:126)x − (cid:126)r) = {sign(f(cid:126)x − (cid:126)r) | f ∈ F} .

P [sign(f (xi) − ri) (cid:54)= sign(g(xi) − ri)]

(cid:90)

(cid:90) |f (xi)−g(xi)|

X

0
(cid:90) |f (xi)−g(xi)|

1
s(x)

drd ˜Q(x)

1
s(x)

drd ˜Q(x)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

=

=

=

=

=

>

X>0

0

|f (xi) − g(xi)|
s(x)

d ˜Q(x)

|f (xi) − g(xi)|
EQ [s(x)]

dQ(x)

X>0

X>0

|f (xi) − g(xi)|
EQ [s(x)]

X

dQ(x)

(cid:15)
EQ [s(x)]

This allows us to bound the probability that two distinct
f, g ∈ G produce the same dichotomy on all m samples, i.e.

P [sign(f(cid:126)x − (cid:126)r) = sign(g(cid:126)x − (cid:126)r)]

m
(cid:89)

i=1
(cid:18)

=

(1 − P [ sign(f (xi) − ri) (cid:54)= sign(g(xi) − ri) ])

≤

1 −

(cid:15)
EQ [s(x)]

(cid:19)m

(cid:18)

≤ exp

−

(cid:15)m
EQ [s(x)]

(cid:19)

.

Given (cid:126)x ∈ X m and (cid:126)r ∈ Rm, let H be the subset of G with
unique dichotomies, i.e., H ⊆ G such that for any f ∈ H,

For m ≥ d, Sauer’s Lemma (Sauer, 1972; Vapnik & Cher-
vonenkis, 1971) bounds the size of this set by

sign(f(cid:126)x − (cid:126)r) (cid:54)= sign(g(cid:126)x − (cid:126)r),

|sign(F(cid:126)x − (cid:126)r)| ≤ (em/d)d,

for all g ∈ G \ {f }. We then have

for all (cid:126)x ∈ X m and (cid:126)r ∈ Rm. Hence, the expected number
of dichotomies is also bounded, i.e.

P [f /∈ H]

E [|sign(F(cid:126)x − (cid:126)r)|] ≤ (em/d)d.

(25)

= P [ ∃g ∈ G \ {f } : sign(f(cid:126)x − (cid:126)r) = sign(g(cid:126)x − (cid:126)r) ]
≤ |G| max

P [ sign(f(cid:126)x − (cid:126)r) = sign(g(cid:126)x − (cid:126)r) ]

Let G be a (cid:15)-separated subset of F with respect to dL1(Q)
with |G|= M (cid:0)(cid:15), F, dL1(Q)
(cid:1). By deﬁnition, for any two
distinct f, g ∈ G, we have

g∈G\{f }
(cid:18)

≤ |G|· exp

−

(cid:15)m
EQ [s(x)]

(cid:19)

.

(cid:90)

X

|f (x) − g(x)| dQ(x) > (cid:15).

This allows us to bound the expected number of dichotomies

from below, i.e.,

Together with (27) and |G|= M (cid:0)(cid:15), F, dL1(Q)

(cid:1), we have

Uniform Deviation Bounds for k-Means Clustering

E [|sign(F(cid:126)x − (cid:126)r)|] ≥ E [|sign(G(cid:126)x − (cid:126)r)|]
≥ E [|sign(H(cid:126)x − (cid:126)r)|]
≥ E [|H|]
(cid:88)

=

(1 − P [ f /∈ H ])

f ∈G
(cid:20)

≥ |G|

1 − |G|· exp

−

(cid:18)

(cid:15)m
EQ [s(x)]

(cid:19)(cid:21)

.

Together with (25), we thus have for m ≥ d

(cid:20)

(cid:17)d

(cid:16) em
d

≥ |G|

1 − |G|· exp

−

.

(26)

(cid:18)

(cid:19)(cid:21)

(cid:15)m
EQ [s(x)]

Consider the case

EQ [s(x)]
(cid:15)
Since (cid:15) ≤ EQ [s(x)] and |G|= M (cid:0)(cid:15), F, dL1(Q)
have

ln(2|G|) < d.

(cid:1), we then

M (cid:0)(cid:15), F, dL1(Q)

(cid:1) = |G|≤

ed(cid:15)/EQ[s(x)] ≤

1
2

1
2

ed

as required to show the result. We hence assume
EQ [s(x)] ln(2|G|)/(cid:15) ≥ d for the remainder of the proof.
Let m ≥ EQ [s(x)] ln(2|G|)/(cid:15) which implies

1 − |G|· exp

−

(cid:18)

(cid:15)m
EQ [s(x)]

(cid:19)

≥

1
2

.

Together with (26) and m ≥ d, it follows that

|G|≤ 2

(cid:18) eEQ [s(x)]
(cid:15)d

(cid:19)d

ln(2|G|)

and hence

(cid:112)|G|

2ddd(cid:112)2|G|
(ln 2|G|)d ≤ 2

√

2

(cid:18) 2eEQ [s(x)]
(cid:15)

(cid:19)d

.

(27)

Since ln x ≤ x, we have for x = (2|G|)1/2d that

ln(2|G|)1/2d ≤ (2|G|)1/2d

which implies

and hence

ln(2|G|) ≤ 2d(2|G|)1/2d

1 ≤

2ddd(cid:112)2|G|
(ln 2|G|)d .

M (cid:0)(cid:15), F, dL1(Q)

(cid:1) = |G|≤ 8

(cid:18) 2eEQ [s(x)]
(cid:15)

(cid:19)2d

as required which concludes the proof.

Lemma 5 (Chaining). Let F be a family of functions from
X to R≥0 with Pdim(F ) = d < ∞. For all x ∈ X ,
let s(x) = supf ∈F f (x). For m ≥ 200K(2d + 1)/(cid:15)2, let
x1, . . . , x2m be a subset of X with K = 1
i=1 s(xi)2 <
2m
∞. For i = 1, 2, . . . , m, let σi be drawn from {−1, 1}
uniformly at random. Then, for all 0 < (cid:15) ≤ 1,

(cid:80)2m

P

(cid:34)

∃f ∈ F :

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤ 4 (cid:0)16e2(cid:1)d

m
(cid:88)

1
m
e− (cid:15)2m

i=1

200K

σi(f (xi) − f (xi+m))

> (cid:15)

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)2m

Proof. Consider the case 1
i=1 s(xi) ≤ 0. By deﬁ-
2m
nition, we have s(xi) ≥ f (xi) ≥ 0 for all f ∈ F and
i = 1, . . . , 2m. Thus, f (xi) = 0 for all i = 1, . . . , 2m.
The claim then follows directly since

m
(cid:88)

i=1

σi(f (xi) − f (xi+m) = 0

for all f ∈ F. For the remainder of the proof, we hence
only need to consider the case 1
2m

i=1 s(xi) > 0.

(cid:80)2m

We deﬁne the discrete measure Q by placing an atom at
each xi with weight proportional to s(xi). More formally,

PX∼Q [X = x] =

1{xi=x},

∀x ∈ X .

2m
(cid:88)

i=1

s(xi)
k=1 s(xk)

(cid:80)2m

Since (cid:80)2m

i=1 s(xi)2 < ∞ and (cid:80)2m

i=1 s(xi) > 0, we have

EQ [s(x)] =

< ∞.

(28)

(cid:80)2m

i=1 s(xi)2
k=1 s(xk)

(cid:80)2m

For j ∈ N, let γj = EQ [s(x)] /2j. We deﬁne a sequence
G1, G2, . . . , G∞ of γj-packings of F as follows: Let the set
G0 consist of an arbitrary element f ∈ F. For any j ∈ N,
we initialize Gj to Gj−1. Then, we select a single element
f ∈ F with dL1(Q)(f, Gj) > γj and add it to Gj. We repeat
this until no such element f ∈ F with dL1(Q)(f, Gj) > γj
is left. By deﬁnition, Gj is an γj-packing of F with respect
to dL1(Q). Hence, for any f ∈ F, we have

dL1(Q)(f, Gj) ≤ γj = EQ [s(x)] /2j.

(29)

Uniform Deviation Bounds for k-Means Clustering

By Lemma 4, the size of Gj is bounded by

|Gj|≤ 2(2e2j)2d = 22d(j+1)+1e2d.

(30)

for some κ > 0. By (29) and j sufﬁciently large, there
exists a g ∈ G such that

For each f ∈ F and j ∈ N, we deﬁne the closest element
in Gj by

φj(f ) = arg min
g∈Gj
By (30), Gj is ﬁnite for each j ∈ N and the minimum is
well-deﬁned.

dL1(Q)(f, g).

We construct the following sequence H1, H2, . . . , H∞: Let
H1 be equal to G1. For each j ∈ {2, 3, . . . , ∞}, we deﬁne

Hj = {g − φj−1(g) : g ∈ Gj} .

For all j ∈ N and h ∈ Hj, there is hence a gh ∈ Gj such
that h = gh(x) − φj−1(gh). By (29), we thus have for all
j ∈ N and h ∈ Hj

EQ [|h(x)|] = dL1(Q)(gh, Gj−1) ≤ γj−1.

(31)

Furthermore, by (30), we have for all j ∈ N

|Hj|≤ |Gj|≤ 22d(j+1)+1e2d.

(32)

dL1(Q)(f, g) <

4
(cid:80)2m
k=1 s(xk)

κ2.

1
2m

(35)

Using the triangle inequality, we have

(cid:15) + κ =

σi(f (xi) − f (xi+m))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

1
m

m
(cid:88)

i=1

m
(cid:88)

i=1

≤

+

1
m

2m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
σi(g(xi) − g(xi+m))
(cid:12)
(cid:12)

|f (xi) − g(xi)| .

Using the Cauchy-Schwarz inequality,
that
f (x), g(x) ∈ [0, s(x)] for all x ∈ X , as well as the def-
inition of dL1(Q) and (35), we may bound

the fact

The key idea is that intuitively any f ∈ F can be ad-
ditively decomposed into functions from the sequence
H1, H2, . . . , H∞. By deﬁnition, for any j ∈ N, any func-
tion g ∈ Gj can be rewritten as

1
m

2m
(cid:88)

i =1

|f (xi) − g(xi)|

|f (xi) − g(xi)|2

g =

hg,k

j
(cid:88)

k=1

G =

Gj.

∞
(cid:91)

j=1

where (hg,1, hg,2, . . . , hg,j) are functions in H1 × H2 ×
. . . × Hj. Let j → ∞ and deﬁne

|f (xi) − g(xi)| s(xi)

(cid:80)2m

k=1 s(xk)
m2

2m
(cid:88)

i=1

|f (xi) − g(xi)|

s(xi)
k=1 s(xk)

(cid:80)2m

(cid:80)2m

k=1 s(xk)
2m

dL1(Q)(f, g)

Clearly, G is dense in F with respect to dL1(Q). We claim
that, as a consequence,

∃f ∈ F :

σi(f (xi) − f (xi+m))

> (cid:15)

(33)

if and only if

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∃g ∈ G :

σi(g(xi) − g(xi+m))

> (cid:15).

(34)

Since G ⊆ F, we have (34) =⇒ (33). To show the
converse, assume ∃f ∈ F such that

Together with (B), we hence have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

σi(g(xi) − g(xi+m))

> (cid:15).

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

σi(f (xi) − f (xi+m))

= (cid:15) + κ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

which implies (33) =⇒ (34) as claimed.

As a consequence, it is sufﬁcient to only consider G instead

1
m2

1
m2

2m
(cid:88)

i=1

2m
(cid:88)

i=1

≤

≤

=

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:115)

=

4

< κ.

Uniform Deviation Bounds for k-Means Clustering

Since all Xi are independent, we may apply Hoeffding’s
inequality. By Theorem 2 of Hoeffding (1963), we have

P

(cid:34) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

(cid:12)
(cid:12)
Xi − E [Xi]
(cid:12)
(cid:12)
(cid:12)

(cid:35)

> (cid:15)j

≤ e

−

2(cid:15)2

j m
(ai−bi)2

1
m

(cid:80)m

i=1

≤ P

∃g ∈ G :

σi(hg,j(xi) − hg,j(xi+m))

> (cid:15)

Using (37), we have

≤ P

∃g ∈ G :

σi(hg,j(xi) − hg,j(xi+m))

> (cid:15)

which implies that

(ai −bi)2 = 4(h(xi)−h(xi+m))2 ≤ 4h(xi)2 +4h(xi+m)2

of F. More formally,

(cid:34)

P

∃f ∈ F :

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
σi(f (xi) − f (xi+m))
(cid:12)
(cid:12)

(cid:35)

> (cid:15)

= P

∃g ∈ G :

(cid:12)
(cid:12)
(cid:12)
σi(g(xi) − g(xi+m))
(cid:12)
(cid:12)

(cid:35)

> (cid:15)

(cid:34)

(cid:34)

(cid:34)

1
m

1
m
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∞
(cid:88)

1
m

m
(cid:88)

i=1
m
(cid:88)

i=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

∞
(cid:88)

j=1

m
(cid:88)

j=1

i=1

(cid:35)

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

For i ∈ N, let (cid:15)j = (cid:15)
5
(cid:80)∞

j=1 (cid:15)j ≤ (cid:15). Suppose it holds that

(cid:113) j

2j . In Lemma 3, we show that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
σi(hg,j(xi) − hg,j(xi+m))
(cid:12)
(cid:12)

≤ (cid:15)j

for all g ∈ G and j ∈ N. Then, we have that
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

σi(hg,j(xi) − hg,j(xi+m))

∞
(cid:88)

m
(cid:88)

1
m

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

j=1

i=1

≤

∞
(cid:88)

j=1

(cid:15)j ≤ (cid:15)

for all g ∈ G. Hence, using the union bound, we have

∃f ∈ F :

σi(f (xi) − f (xi+m))

> (cid:15)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

∞
(cid:88)

P

≤

∃g ∈ G :

(cid:34)

(cid:34)

m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
σi(hg,j(xi) − hg,j(xi+m))
(cid:12)
(cid:12)

(cid:35)

> (cid:15)j

(cid:35)

P

∃h ∈ Hj :

σi(h(xi) − h(xi+m))

> (cid:15)j

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|Hj| max
h∈Hj

P

(cid:34) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
σi(h(xi) − h(xi+m))
(cid:12)
(cid:12)

(cid:35)

> (cid:15)j

(cid:34)

P

j=1

∞
(cid:88)

j=1

∞
(cid:88)

j=1

=

≤

(36)

We now use Hoeffding’s inequality to bound the probability
that

m
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

i=1
for a single j ∈ N and h ∈ Hj.

σi(h(xi) − h(xi+m))

> (cid:15)j,

For i ∈ {1, 2, . . . , m}, consider the random variables

Xi = σi(h(xi) − h(xi+m)).

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

By (31) and (28), we thus have

1
m

m
(cid:88)

i=1

(ai − bi)2 ≤ 8

h(xi)2.

1
2m

2m
(cid:88)

i=1

Using h(xi) ∈ [0, s(xi)] and the deﬁnition of EQ [·], we
have

1
m

m
(cid:88)

i =1

(ai − bi)2 ≤ 8

|h(xi)|s(xi)

2m
(cid:88)

i=1

1
2m

(cid:32)

1
2m

2m
(cid:88)

i=1

= 8

s(xi)

EQ [|h(x)|]

(cid:33)

1
m

m
(cid:88)

i =1

(ai − bi)2 ≤ 8

(cid:32)

2m
(cid:88)

i=1

1
2m

(cid:32)

(cid:33)

s(xi)

γj−1

(cid:33)

= 24−j

s(xi)

EQ [s(x)]

1
2m

1
2m

1
2m

2m
(cid:88)

i=1

2m
(cid:88)

i=1

2m
(cid:88)

i=1

(cid:32)

(cid:32)

= 24−j

= 24−j

= 24−jK

s(xi)

(cid:33) (cid:80)2m
(cid:80)2m

i=1 s(xi)2
k=1 s(xk)

(cid:33)

s(xi)2

Since (cid:15)i = (cid:15)
5

2j this implies

(cid:113) j

2(cid:15)2
j m
i=1(ai − bi)2

(cid:80)m

−

1
m

≤ −

(cid:15)2mj
200K

Since σi are uniformly drawn at random from {−1, 1}, we
have E [Xi] = 0 for all i ∈ {1, 2, . . . , m}. Furthermore,
each Xi is bounded in

[ai, bi] = [0 ± (h(xi) − h(xi+m))].

(37)

Hence, for any j ∈ N and any h ∈ Hj

P

(cid:34) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
σi(h(xi) − h(xi+m))
(cid:12)
(cid:12)

(cid:35)

> (cid:15)j

≤ e− (cid:15)2mj
200K .

Uniform Deviation Bounds for k-Means Clustering

Together with (32), this allows us to bound (36), i.e.,

and assume that the event A holds, i.e., there exists a f (cid:48) ∈ F
such that

∃f ∈ F :

σi(f (xi) − f (xi+m))

> (cid:15)

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m
(cid:88)

i=1

1
m

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
|Hj|e− (cid:15)2mj

200K

(cid:34)

P

≤

≤

∞
(cid:88)

j=1
∞
(cid:88)

j=1

22d(j−1)+4d+1e2de− (cid:15)2mj

200K

= 24d+1e2de− (cid:15)2m

200K

∞
(cid:88)

(cid:16)

4de− (cid:15)2m

200K

(cid:17)j−1

= 24d+1e2de− (cid:15)2m

200K

(cid:16)

4de− (cid:15)2m

200K

(cid:17)j

.

j=1
∞
(cid:88)

j=0

By assumption in the main claim, we have m ≥ 200K(2d +
1)/(cid:15)2 and hence

0 ≤ 4de− (cid:15)2m

200K ≤

1
2

.

This implies

4de− (cid:15)2m

100K

(cid:17)j

≤

∞
(cid:88)

(cid:16)

j=0

∞
(cid:88)

j=0

1
2j = 2

Together with

σi(f (xi) − f (xi+m))

> (cid:15)

this implies that

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

and hence

P

(cid:34)

∃f ∈ F :

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤ 4 (cid:0)16e2(cid:1)d

m
(cid:88)

1
m
e− (cid:15)2m

i=1

200K

which concludes the proof.

With these results we are able to prove Theorem 5.

Proof of Theorem 5. Our goal is to upper bound the proba-
bility of the event

A =

∃f ∈ F :

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

(cid:12)
(cid:12)
f (xi) − E [f (x)]
(cid:12)
(cid:12)
(cid:12)

(cid:41)

> (cid:15)

Since

by δ, i.e., to prove P [A] ≤ δ. Consider the event

we thus have

(cid:40)

(cid:40)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

(cid:12)
(cid:12)
f (cid:48)(xi) − E [f (cid:48)(x)]
(cid:12)
(cid:12)
(cid:12)

> (cid:15).

For any f ∈ F, Markov’s inequality in combination with
Jensen’s inequality implies

P

(cid:34) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

2m
(cid:88)

(cid:12)
(cid:12)
f (xi) − E [f (x)]
(cid:12)
(cid:12)
(cid:12)

>

(cid:15)
2

(cid:35)

i=m+1
(cid:20) (cid:12)
(cid:12)
(cid:12)

1
m

4 · E

(cid:80)2m

2 (cid:21)
(cid:12)
i=m+1 f (xi) − E [f (x)]
(cid:12)
(cid:12)

≤

≤

≤

=

(cid:15)2

4 · E

(cid:20) (cid:12)
(cid:12)
(cid:12)

1
m

(cid:80)2m

(cid:12)
(cid:12)
i=m+1 f (xi)
(cid:12)

2 (cid:21)

4 · E

(cid:104) 1
m

(cid:15)2
i=m+1 |f (xi)|2 (cid:105)
(cid:80)2m
m(cid:15)2

4 · E (cid:2) s(x)2 (cid:3)
m(cid:15)2

.

m ≥

3 + 5d + log

≥

(cid:18)

200t
(cid:15)2

(cid:19)

1
δ

8t
(cid:15)2

P

(cid:34) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

2m
(cid:88)

i=m+1

(cid:12)
(cid:12)
f (cid:48)(xi) − E [f (cid:48)(x)]
(cid:12)
(cid:12)
(cid:12)

(cid:35)

≤ (cid:15)/2

≥

1
2

(cid:2)s(x)2(cid:3) ≤ t by (5). Thus, given A, the event B

since EP
holds with probability at least 1/2, i.e.,

P [B | A] ≤ 1/2.

P [B] = P [A ∩ B] = P [B | A] P [A] ,

B =

∃f ∈ F :

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

2m
(cid:88)

i=m+1

(cid:12)
(cid:12)
f (xi) − E [f (x)]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
f (xi) − E [f (x)]
(cid:12)
(cid:12)
(cid:12)

∩

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

> (cid:15)

We consider the event

(cid:41)

(cid:40)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

≤ (cid:15)/2

.

C =

∃f ∈ F :

(f (xi) − f (xi+m))

> (cid:15)/2

(38)

(cid:41)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P [A] ≤ 2 · P [B] .

Uniform Deviation Bounds for k-Means Clustering

and note that, if B holds, then there exists a f (cid:48) ∈ F such
that

m
(cid:88)

i=1
m
(cid:88)

i=1

(cid:12)
(cid:12)
f (cid:48)(xi) − E [f (cid:48)(x)]
(cid:12)
(cid:12)
(cid:12)

f (cid:48)(xi) −

f (cid:48)(xi)

2m
(cid:88)

1
m

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

1
m

2m
(cid:88)

i=m+1

i=m+1

(cid:12)
(cid:12)
f (cid:48)(xi) − E [f (cid:48)(x)]
(cid:12)
(cid:12)
(cid:12)
(cid:125)

(cid:123)(cid:122)
≤ (cid:15)
2

(cid:15) <

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

1
m
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:124)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

which implies that there exists a f (cid:48) ∈ F with

(f (cid:48)(xi) − f (cid:48)(xi+m))

> (cid:15)/2 .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Hence, B ⊆ C which in combination with (38) implies that

P [A] ≤ 2 · P [B] ≤ 2 · P [C] .

(39)

Let (cid:126)σ = (σ1, σ2, . . . , σm) be a random vector where each
σi is sampled independently at random from a uniform
distribution on {−1, 1}. We deﬁne the event

Consider any ﬁxed vector (cid:126)x = (x1, x2, . . . , x2m):
If
E does not hold, then P(cid:126)σ [D|E] 1E = 0. Otherwise,
1
i=1 s(xi)2 ≤ t and consequently Lemma 5 with
2m
K = t implies that for m ≥ 200t(2d + 1)/(cid:15)2

(cid:80)2m

P(cid:126)σ [D|E] ≤ 4 (cid:0)16e2(cid:1)d

e− (cid:15)2m
200t .

As a result, we have for m ≥ 200t(2d + 1)/(cid:15)2

P [D] ≤

(cid:104)

+ E(cid:126)x

4 (cid:0)16e2(cid:1)d

e− (cid:15)2m

200t 1E

(cid:105)

≤

+ 4 (cid:0)16e2(cid:1)d

e− (cid:15)2m
200t .

δ
4
δ
4

In combination with (39) and (40), this implies that for
m ≥ 200t(2d + 1)/(cid:15)2

P [A] ≤

+ 8 (cid:0)16e2(cid:1)d

e− (cid:15)2m
200t .

δ
2

By the main claim, we always have m ≥ 200t(2d + 1)/(cid:15)2
and hence we only need to show that

P [A] ≤

+ 8 (cid:0)16e2(cid:1)d

e− (cid:15)2m

200t ≤ δ.

δ
2

D =

f ∈ F :

σi (f (xi) − f (xi+m))

> (cid:15)/2

.

This is equivalent to

(cid:40)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
m

m
(cid:88)

i=1

(cid:41)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

In essence, σi randomly permutes xi and xm+i for any
i ∈ {1, . . . , m}. Hence, since all xi are identically and
independently distributed and hence exchangeable, we have

and

8 (cid:0)16e2(cid:1)d

e− (cid:15)2m

200t ≤ δ/2

P [C] = P [D] .

(40)

log 16 + d(log 16 + 2) −

≤ ln δ.

(cid:15)2m
200t

Consider the event

This is the case if we have

and let E denote its complement. By (5), we have that

E =

(cid:40)

1
2m

2m
(cid:88)

i=1

(cid:41)

s(xi)2 ≤ t

P (cid:2)E(cid:3) ≤

δ
4

.

log 16 + d(log 16 + 2) + log

≤

1
δ

(cid:15)2m
200t

or equivalently

(cid:18)

200t
(cid:15)2

m ≥

log 16 + d(log 16 + 2) + log

(cid:19)

.

1
δ

Let E(cid:126)x [·] denote the expectation with regards to the random
vector (cid:126)x = (x1, x2, . . . , x2m) and P(cid:126)σ [·] the probability with
regards to the random vector (cid:126)σ. By construction, (cid:126)σ and (cid:126)x
are independent and the event E only depends on (cid:126)x but not
on (cid:126)σ. We thus have

The main claim thus holds since

m ≥

3 + 5d + log

(cid:18)

200t
(cid:15)2

(cid:19)

1
δ

which concludes the proof.

P [D] = P (cid:2)D ∩ E(cid:3) + P [D ∩ E]
≤ P (cid:2)E(cid:3) + E(cid:126)x [P(cid:126)σ [D ∩ E]]
+ E(cid:126)x [P(cid:126)σ [D] 1E]

≤

=

+ E(cid:126)x [P(cid:126)σ [D|E] 1E] .

δ
4
δ
4

Uniform Deviation Bounds for k-Means Clustering

C. Proof of Lemma 1

Proof. We ﬁrst show (9) with the same notation as in the
proof of Lemma 4. For any f ∈ F, (cid:126)x ∈ X m and (cid:126)r ∈ Rm,
we denote the restriction (f (x1), . . . , f (xm)) by f(cid:126)x and set
F(cid:126)x = {f(cid:126)x | f ∈ F}. For any vector (cid:126)z ∈ Rm, we deﬁne

sign((cid:126)z) = (sign(z1), . . . , sign(zm)) .

The set of dichotomies induced by (cid:126)r on F(cid:126)x is given by

sign(F(cid:126)x − (cid:126)r) = {sign(f(cid:126)x − (cid:126)r) | f ∈ F} .

Let ¯m be equal to the pseudo-dimension of F. This implies
that there exist two vectors ¯x ∈ X ¯m and ¯r ∈ R ¯m that are
shattered by F, i.e.,

|sign(F¯x − ¯r)| = 2 ¯m.

(41)

By (42), both F and G induce the same dichotomies on
(¯x, ¯r) and (˜x,(cid:126)0) respectively and thus

|sign(F¯x − ¯r)| = |sign(G˜x)| .

(43)

We deﬁne the function family

(cid:26)

H =

min
˜q∈Q

(cid:104)·, ˜q(cid:105) | Q ∈ R(d+3)×k

(cid:27)

and note that by construction G ⊆ H. This implies

|sign(G˜x)| ≤ |sign(H˜x)| .

(44)

Consider the function family

I = (cid:8)(cid:104)·, ˜q(cid:105) | ˜q ∈ Rd+3(cid:9) .

For any Q ∈ R(d+3)×k, it holds that

Consider any x ∈ (cid:126)x, its corresponding r ∈ (cid:126)r and any
fQ(·) ∈ F. Deﬁning σ2

(cid:2)d(x, Q)2(cid:3), we have that

Q = EP

(cid:26)

x ∈ ˜x | min
˜q∈Q

(cid:27)

(cid:91)

˜q∈Q

(cid:104)x, ˜q(cid:105) ≤ 0

=

{x ∈ ˜x | (cid:104)x, ˜q(cid:105) ≤ 0} .

sign(fQ(x) − rx)

d(x, Q)2

(cid:19)

= sign

= sign

1

2 σ2 + 1
d(x, Q)2 −

EP [d(x, Q)2]
(cid:0)σ2 + σ2

2

rx
2

− r

(cid:1)(cid:17)

Q

= sign

d(x, q)2 −

r
2

(cid:0)σ2 + σ2

(cid:1)

Q

= sign

(cid:2)xT x − 2xT q + qT q(cid:3) −

(cid:18)

(cid:16)

(cid:18)

(cid:18)

(cid:32)

min
q∈Q

min
q∈Q

(cid:19)

r
2

= sign

min
˜q∈ ˜Q(Q)

(cid:104)˜x(x, r), ˜q(cid:105)

(cid:33)
,

Since |Q|= k, this implies that there exists an injective
mapping from H to the k-fold Cartesian product of I that
generates the same dichotomies. In turn, this implies

|sign(H˜x)| ≤ |sign(I˜x)|k .

(45)

(cid:19)

(cid:0)σ2 + σ2

(cid:1)

Q

The dichotomies induced by I are generated by halfspaces
in Rd+3. The Vapnik-Chervonenkis dimension of halfspaces
in Rd+3 is bounded by d + 4 (Har-Peled, 2011) and thus
Pdim(I) ≤ d + 4. Together with Sauer’s Lemma (Sauer,
1972; Vapnik & Chervonenkis, 1971), this implies

(42)

|sign(I˜x)| ≤

(cid:18) e ¯m
d + 4

(cid:19)d+4

.

where we have used the mappings

Combining (41), (43), (44), (45) and (46) yields

˜x(x, r) =













−2x
−r/2
1
xT x

and

˜Q(Q) =

| q ∈ Q

.












q
σ2 + σ2
Q
qT q
1











2 ¯m ≤

(cid:18) e ¯m
d + 4

(cid:19)(d+4)k

.

For Pdim(F) = ¯m < d + 4, the main claim holds trivially.
On the other hand, for ¯m ≥ d + 4, (47) implies that

(cid:18)

¯m
d + 4

≤

k
log 2

1 + log

(cid:19)

¯m
d + 4

≤

2k
log 2

log

¯m
d + 4

Since ¯m

d+4 > 0 and 2k

log 2 > 0, Lemma 2 implies that

Consider the vector ˜x = (˜x(¯x1, ¯r1), . . . , ˜x(¯x ¯m, ¯r ¯m)) and
the function family

¯m
d + 4

≤

4k
log 2

log

4k
log 2

(cid:40)

G =

min
˜q∈ ˜Q(Q)

(cid:41)

(cid:104)·, ˜q(cid:105) | Q ∈ Rd×k

.

Since

4
log 2 ≈ 5.77 < 6, this proves the claim in (9), i.e.,

Pdim(F) = ¯m ≤ 6k(d + 4) log 6k.

(46)

(47)

Uniform Deviation Bounds for k-Means Clustering

Next, we prove (10). For any x ∈ Rd and Q ∈ Rd×k, we
have by the triangle inequality

d(x, Q)2 ≤ (d(x, µ) + d(µ, Q))2

= d(x, µ)2 + d(µ, Q)2 + 2 d(x, µ) d(µ, Q)

For any 0 ≤ a ≤ b, it holds that

2ab = ab + a(b − a) + a2 ≤ ab + b(b − a) + a2 = b2 + a2.

Since either 0 ≤ d(x, µ) ≤ d(µ, Q) or 0 ≤ d(µ, Q) <
d(x, µ), we thus have for any x ∈ Rd and Q ∈ Rd×k

d(x, Q)2 ≤ 2 d(x, µ)2 + 2 d(µ, Q)2.

(48)

By the same argument it also holds that for any x ∈ Rd and
Q ∈ Rd×k

d(µ, Q)2 ≤ 2 d(x, µ)2 + 2 d(x, Q)2.

By taking the expectation with regards to P and noting that
σ2 = EP

(cid:2)d(x, µ)2(cid:3) < ∞, we obtain for any Q ∈ Rd×k

d(µ, Q)2 ≤ 2σ2 + 2EP

(cid:2)d(x, Q)2(cid:3) .

(49)

Combining (48) and (49) implies that for any x ∈ Rd and
Q ∈ Rd×k

d(x, Q)2 ≤ 2 d(x, µ)2 + 4σ2 + 4EP

(cid:2)d(x, Q)2(cid:3)

(cid:18)

(cid:18)

(cid:18)

≤

4 +

≤

4 +

≤

8 +

(cid:19)

(cid:19)

2 d(x, µ)2
σ2
2 d(x, µ)2
σ2
4 d(x, µ)2
σ2

(cid:19) 1
2

σ2 + 4EP

(cid:2)d(x, Q)2(cid:3)

(cid:0)σ2 + EP

(cid:2)d(x, Q)2(cid:3)(cid:1)

(cid:0)σ2 + EP

(cid:2)d(x, Q)2(cid:3)(cid:1) .

By the deﬁnition of fQ(x), this proves (10). Finally we
have

s(x)2(cid:105)
(cid:104)

EP

= EP

(cid:19)2(cid:35)

+ 8

(cid:34)(cid:18) 4 d(x, µ)2
σ2
(cid:20)(cid:18) 16 d(x, µ)4

σ4
EP

(cid:2)d(x, µ)4(cid:3)

= EP

= 128 + 16

= 128 + 16 ˆM4.

σ4

which shows (11) and concludes the proof.

+

64 d(x, µ)2
σ2

(cid:19)(cid:21)

+ 64

