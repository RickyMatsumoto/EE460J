High-Dimensional Variance-Reduced Stochastic Gradient
Expectation-Maximization Algorithm

Rongda Zhu 1 Lingxiao Wang 2 Chengxiang Zhai 3 Quanquan Gu 2

Abstract

We propose a generic stochastic expectation-
maximization (EM) algorithm for the estimation
of high-dimensional latent variable models. At the
core of our algorithm is a novel semi-stochastic
variance-reduced gradient designed for the Q-
function in the EM algorithm. Under a mild con-
dition on the initialization, our algorithm is guar-
anteed to attain a linear convergence rate to the un-
known parameter of the latent variable model, and
achieve an optimal statistical rate up to a logarith-
mic factor for parameter estimation. Compared
with existing high-dimensional EM algorithms,
our algorithm enjoys a better computational com-
plexity and is therefore more efﬁcient. We apply
our generic algorithm to two illustrative latent
variable models: Gaussian mixture model and
mixture of linear regression, and demonstrate the
advantages of our algorithm by both theoretical
analysis and numerical experiments. We believe
that the proposed semi-stochastic gradient is of
independent interest for general nonconvex opti-
mization problems with bivariate structures.

1. Introduction

As a popular algorithm for the estimation of latent vari-
able models, the expectation-maximization (EM) algorithm
(Dempster et al., 1977; Wu, 1983) has been widely used
in machine learning and statistics (Jain et al., 1999; Tseng,
2004; Han et al., 2011; Little & Rubin, 2014). Although EM
is well-known to often converge to an empirically good local
estimator (Wu, 1983), ﬁnite sample theoretical guarantees
for its performance have not been established until recent

1Facebook, Inc., Menlo Park, CA 94025 2Department of Com-
puter Science, University of Virginia, Charlottesville, VA 22904,
USA 3Department of Computer Science, University of Illinois
at Urbana-Champaign, Urbana, IL 61801. Correspondence to:
Quanquan Gu <qg5w@virginia.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

studies (Balakrishnan et al., 2014; Wang et al., 2014; Yi &
Caramanis, 2015). Speciﬁcally, the ﬁrst local convergence
theory and ﬁnite sample statistical rates of convergence for
the conventional EM algorithm and its gradient ascent vari-
ant were established in Balakrishnan et al. (2014). Later
on, Wang et al. (2014) extended the conventional EM al-
gorithm as well as gradient ascent EM algorithm to the
high-dimensional setting, where the number of parameters
is comparable to or even larger than the sample size. A
key idea used in their algorithms is an additional truncation
step after the maximization step (M-step), which is able to
exploit the intrinsic sparse structure of the high-dimensional
latent variable models. Yi & Caramanis (2015) also pro-
posed a high-dimensional EM algorithm, which, instead
of using truncation, uses a regularized M-estimator in the
M-step. In the high-dimensional setting, the gradient EM
algorithm is especially appealing, because exact maximiza-
tion based M-step can be very time consuming, or even
ill-posed. Nonetheless, gradient EM algorithms can still
be computationally prohibitive when the number of obser-
vations is also large, since they need to calculate the full
gradient at each iteration, whose time complexity is linear
in the sample size.

In this paper, we address the aforementioned computational
challenge in the large-scale high-dimensional setting, by
proposing a novel variance-reduced stochastic gradient EM
algorithm with theoretical guarantees. Our algorithm is
along the line of gradient EM algorithms (Balakrishnan
et al., 2014; Wang et al., 2014), where the M-step is achieved
by one-step gradient ascent rather than (regularized) exact
maximization (Yi & Caramanis, 2015). Instead of using
a full gradient at each iteration as in existing gradient EM
algorithms, we signiﬁcantly reduce the computational cost
by utilizing stochastic variance-reduced gradient, which
is inspired by recent advances in stochastic optimization
(Roux et al., 2012; Johnson & Zhang, 2013; Shalev Shwartz
& Zhang, 2013; Defazio et al., 2014; Zhang & Gu, 2016).
To accommodate the special bivariate structure of the Q-
function (i.e., the expected value of the log likelihood func-
tion, with respect to the conditional distribution of the latent
variable given the observed variable under the current esti-
mate of the model parameter) in EM algorithm, we design a
novel semi-stochastic variance-reduced gradient which sets

High-Dimensional Variance-Reduced Stochastic Gradient Expectation-Maximization Algorithm

our work apart from all existing methods and greatly helps
reduce the intrinsic variance of the stochastic gradient of the
Q-function in the EM algorithm. We apply our algorithm to
two popular latent variable models and thorough numerical
experiments are provided to backup our theory. In particular,
we summarize our major contributions as follows:

• We propose a novel high-dimensional EM algorithm by
incorporating variance reduction into the stochastic gra-
dient method for EM. Speciﬁcally, we design a novel
semi-stochastic gradient tailored to the bivariate structure
of the Q-function in the EM algorithm. To the best of our
knowledge, this is the ﬁrst work ever that brings variance
reduction into the stochastic gradient EM algorithm in
the high-dimensional scenario.

• We prove that our proposed algorithm converges at a
linear rate to the unknown model parameter and achieves
the best-known statistical rate of convergence with a mild
condition on the initialization.

• We show that the proposed algorithm has an improved
overall computational complexity over the state-of-the-
art algorithm. Speciﬁcally, to achieve an optimization
error of (cid:15), our algorithm needs O(cid:0)(N + bκ2) · log(1/(cid:15))(cid:1)
gradient evaluations1, where N is the sample size, b is
the mini batch size that will be discussed later, and κ is
the restricted condition number. In contrast, the gradient
complexity of the state-of-the-art high-dimensional EM
algorithm (Wang et al., 2014) is O(cid:0)κN log(1/(cid:15))(cid:1). As
long as κ ≤ N/b, which holds in most real cases, the
overall gradient complexity of our algorithm is less than
Wang et al. (2014).

• Different from the proof technique used in existing work
(Balakrishnan et al., 2014; Wang et al., 2014; Yi & Cara-
manis, 2015), which analyzes both the population and
sample versions of the Q-function, we directly analyze
the sample version of the Q-function. Our proof is much
simpler and provides a good interface to analyze the semi-
stochastic gradient.

The rest of the paper is organized as follows. We introduce
the related work in Section 2, and then present our algorithm
and its applications to two representative latent variable
models in Section 3. We demonstrate the main theoretical
result as well as its implication to speciﬁc latent variable
models in Section 4, followed by experimental results in
Section 5. Finally, we conclude our paper and point out
some future work in Section 6.
Notation: Let A = [Aij] ∈ Rd×d be a matrix and
v = (v1, . . . , vd)(cid:62) ∈ Rd be a vector. We deﬁne the (cid:96)q-

1Throughout this paper, we consider the calculation of the
gradient of the Q-function over a data point as a unit gradient
evaluation cost. And we use the gradient complexity, i.e., the
number of gradient evaluation units, to fairly compare different
algorithms.

j=1 v2

(cid:113)(cid:80)d

j=1 |vj|q(cid:1)1/q

norm (q ≥ 1) of v as (cid:107)v(cid:107)q = (cid:0)(cid:80)d
. Specif-
ically, (cid:107)v(cid:107)0 denotes the number of nonzero entries of v,
(cid:107)v(cid:107)2 =
j and (cid:107)v(cid:107)∞ = maxj |vj|. For q ≥ 1,
we deﬁne (cid:107)A(cid:107)q as the operator norm of A. Speciﬁcally,
(cid:107)A(cid:107)2 is the spectral norm. We let (cid:107)A(cid:107)∞,∞ = maxi,j |Aij|.
For an integer d > 1, we deﬁne [d] = {1, . . . , d}. For an
index set I ∈ [d] and vector v ∈ Rd, we use vI ∈ Rd to
denote the vector where [vI]j = vj if j ∈ I, and [vI]j = 0
otherwise. We use supp(v) to denote the index set of its
nonzero entries, and supp(v, s) to denote the index set of
top s largest |vj|’s. C is used to denote some absolute con-
stants. The values of these constants may be different from
case to case. λmax(A) and λmin(A) are used to denote
the largest and smallest eigenvalues of matrix A. We use
B(r; β) to denote the ball centered at β with radius r.

2. Related Work

In this section, we discuss some related work in detail. Even
with its long history in theory and practice of the EM algo-
rithm (Dempster et al., 1977; Wu, 1983; Tseng, 2004), the
ﬁnite sample statistical guarantees on EM algorithm have
not been pursued until recent research (Balakrishnan et al.,
2014; Wang et al., 2014; Yi & Caramanis, 2015). In a pio-
neering work by Balakrishnan et al. (2014), both statistical
and computational analysis of EM algorithm was conducted
in the classical regime. Speciﬁcally, the authors treated
EM algorithms as a special perturbed form of standard gra-
dient methods, and they showed that with an appropriate
initialization, their algorithm achieves a locally linear con-
vergence rate to the unknown model parameter. However,
their work is limited to the classical regime. While in the
high-dimensional regime, when data dimension is much
larger than the number of samples, the M-step is often in-
tractable or even not well deﬁned. In order to extend this
work to the high-dimensional scenario, Wang et al. (2014)
addressed this challenge by inserting a truncation step to
enforce the sparsity of the parameter. They proved that
their algorithm also enjoys locally linear convergence to the
model parameter up to certain statistical error. Yi & Cara-
manis (2015) proposed a high-dimensional extension of EM
algorithms via a regularized M-estimator, and provided sim-
ilar theoretical guarantees. In addition, both Balakrishnan
et al. (2014) and Wang et al. (2014) proposed gradient vari-
ants of the EM algorithm, which can be computationally
faster than exact maximization based EM.

Although the gradient based EM algorithms (Balakrishnan
et al., 2014; Wang et al., 2014) have been proved to achieve
guaranteed performance, these deterministic approaches
can incur huge computational cost in big data and high-
dimensional scenario since they need costly calculation of
full gradient at each iteration. Stochastic gradient methods
are a common workaround to large scale optimization (Bot-

High-Dimensional Variance-Reduced Stochastic Gradient Expectation-Maximization Algorithm

tou, 2010; Gemulla et al., 2011), because one only needs to
calculate a mini-batch of the stochastic gradients each time.
However, due to the intrinsic variance introduced by the
stochastic gradient, these methods often have a slower con-
vergence rate compared with full gradient methods. There-
fore, a lot of variance reduction techniques have been pro-
posed to reduce the variance of the stochastic gradient and
pursue a faster convergence rate. One of the most pop-
ular methods is the stochastic variance-reduced gradient
(SVRG) (Johnson & Zhang, 2013). Inspired by this method,
various machine learning tasks (Li et al., 2016; Chen & Gu,
2016; Garber & Hazan, 2015) have utilized the stochastic
variance reduction technique to provide improved perfor-
mance of nonconvex optimization with univariate structures.
Recently, Reddi et al. (2016); Allen-Zhu & Hazan (2016)
also analyzed SVRG for the general univariate nonconvex
ﬁnite-sum optimization. Motivated by all of these SVRG
methods, one natural question is that, can we accelerate
gradient based EM algorithms using SVRG? We show in
this work that the answer is in the afﬁrmative. Since all
the aforementioned SVRG methods can not be applied to
the special bivariate structure of the Q-function, in order to
incorporate the variance reduction technique into stochastic
gradient based EM algorithms, we need to construct a new
semi-stochastic gradient.

where ∇1 ¯QN (·; ·) denotes the gradient on the ﬁrst variable
and η is the learning rate.
In the high-dimensional regime, we assume β∗ ∈ Rd is
sparse with (cid:107)β(cid:107)0 ≤ s∗. In order to ensure the sparsity of the
estimator, Wang et al. (2014) proposed to use a truncation
step (i.e., T-step) following the M-step.

3.2. Illustrative Examples

We now introduce two representative latent variable models
as running examples for high-dimensional EM algorithms.

Example 3.1 (Sparse Gaussian Mixture Model). The ran-
dom variable Y ∈ Rd is given by

Y = Z · β∗ + V ,

where Z is a random variable with P(Z = 1) = P(Z =
−1) = 1/2, and V ∼ N (0, Σ) is a Gaussian random vector,
with Σ being the covariance matrix, V and Z are inde-
pendent, and (cid:107)β∗(cid:107)0 ≤ s∗. We assume Σ is known for
simplicity.

Example 3.2 (Mixture of Sparse Linear Regression). Let
X ∈ Rd ∼ N (0, Σ) be a Gaussian random vector, and
V ∼ N (0, σ2) be a univariate normal random variable. The
random variable Y ∈ R is given by

3. Methodology

Y = Z · X (cid:62)β∗ + V,

In this section, we present our proposed algorithm. We
ﬁrst introduce the general framework of the EM method,
and then give two representative high-dimensional latent
variable models as examples before going into the details of
our algorithm.

3.1. Background

We now brieﬂy review the latent variable model and the
conventional EM algorithm. Let Y ∈ Y be the observed
random variable and Z ∈ Z be the latent random variable
with joint distribution fβ(y, z) and conditional distribution
pβ(z|y), with the model parameter β ∈ Rd. Given N
observations {yi}N
i=1 of Y , the EM algorithm aims at max-
imizing the Q-function

¯QN (β; β(cid:48)) =

pβ(cid:48)(z|yi) · log fβ(yi, z) dz.

1
N

N
(cid:88)

(cid:90)

i=1

Z

Particularly, in the l-th iteration of EM algorithm, we evalu-
ate ¯QN (β; β(l)) in the E-step, and perform the maximiza-
tion of ¯QN (β; β(l)) on β in the M-step. For example, in the
standard gradient ascent implementation of EM algorithm,
the M-step is given by

β(l+1) = β(l) + η∇1 ¯QN (β(l); β(l)),

where Z is a random variable with P(Z = 1) = P(Z =
−1) = 1/2. Here X, V and Z are independent, and
(cid:107)β(cid:107)0 ≤ s∗. In addition, we assume that σ is known.

3.3. Proposed Algorithm

Now we present our high-dimensional EM algorithm based
on stochastic variance-reduced gradient ascent. The outline
of the proposed algorithm is described in Algorithm 1.

Since our algorithm is based on stochastic gradient, we di-
vide the N samples into n mini-batches {Di}n
i=1, and deﬁne
i=1 on these mini-batches, i.e., qi(β; β(cid:48)) =
function {qi}n
(cid:82)
1/b (cid:80)
Z pβ(cid:48)(z|yj) · log fβ(yj, z) dz, where we let b
be the mini-batch size, and N = nb. Let Qn(β; β(cid:48)) =
1/n (cid:80)n
i=1 qi(β; β(cid:48)). It is easy to show that Qn(β; β(cid:48)) =
¯QN (β; β(cid:48)).

j∈Di

Note that in Algorithm 1, to ensure the sparsity of the out-
put estimator , we use the hard thresholding operator (Blu-
mensath & Davies, 2009), Hs(v) = vsupp(v,s), which only
keeps the largest s entries in magnitude of a vector v ∈ Rd.
The sparsity parameter s controls the sparsity level of the
estimated parameter, and is critical to the estimation error
as we will show later.

We can see that there are two layers of iterations in our
algorithm. For each outer iteration, we ﬁrst conduct E-step,

High-Dimensional Variance-Reduced Stochastic Gradient Expectation-Maximization Algorithm

Algorithm 1 Variance Reduced Stochastic Gradient EM
Algorithm (VRSGEM)
1: Parameter: Sparsity Parameter s, Maximum Number of
Outer Iterations m, Number of Inner Iterations T , learning
rate η

2: Initialization:

(cid:101)β(0) = Hs(βinit),

5:

(cid:0)β; (cid:101)β(l)(cid:1) with the dataset
(cid:101)µ = ∇1Qn( (cid:101)β; (cid:101)β)

3: For l = 0 to m − 1
E-step:
4:
Evaluate Qn
(cid:101)β = (cid:101)β(l),
M-step:
β(0) = (cid:101)β
Randomly select jl uniformly from {0, . . . , T − 1}
For t = 0 to jl
Randomly select i from [n] uniformly
v(t) = ∇1qi
β(t+0.5) = β(t) + ηv(t),
T-step: β(t+1) = Hs(β(t+0.5))

(cid:0)β(t); (cid:101)β(cid:1) − ∇1qi

(cid:101)β; (cid:101)β(cid:1) + (cid:101)µ,
(cid:0)

6:

7:
8:
9:
10:
11:
12: End For
13: Output: (cid:98)β = (cid:101)β(m)

End For
(cid:101)β(l+1) = β(jl+1)

where we compute the averaged gradient (cid:101)µ based on the
whole dataset and the model parameter from last outer it-
eration. This averaged gradient will be used repetitively
in the M-step for variance reduction. In M-step, we have
the inner iterations. We ﬁrst determine the number of inner
iterations, which is randomly selected from [T ] uniformly.
At each inner iteration, we make use of the variance reduc-
tion technique. Note that we extend the variance reduction
idea originally proposed by Johnson & Zhang (2013) to
the bivariate structure of the Q-function. Speciﬁcally, we
design a novel semi-stochastic gradient on mini-batches as
v(t) = ∇1qi(β(t); (cid:101)β) − ∇1qi( (cid:101)β; (cid:101)β) + (cid:101)µ, which ﬁxes the
second variable within each outer iteration for the sake of
convergence guarantee. While the standard gradient im-
plementation of EM algorithm (Wang et al., 2014) uses
∇1 ¯QN (β(t); β(t)) to update the parameter at each iteration,
our newly designed semi-stochastic gradient is proved to
better reduce the variance and attain a lower gradient com-
plexity. After ﬁnishing all the inner iterations, we use the
output from the last inner iteration as the output of this outer
iteration. We use the output from the last outer iteration as
the ﬁnal estimator.

We believe our newly proposed semi-stochastic gradient is
of independent interest for the stochastic optimization of
functions with bivariate structures, to prove a faster rate of
convergence.

4. Main Theoretical Results

described in Section 3.2.

To facilitate the technical analysis of our algorithm, we fo-
cus on the resampling version of Algorithm 1 following the
convention of previous work (Wang et al., 2014; Yi & Cara-
manis, 2015). The key difference between the resampling
version and Algorithm 1 is that we split the whole dateset
into m subsets and use one subset for each outer iteration.
The details of the resampling version of our algorithm is
provided in the longer version of this paper. It is worth
noting that the resampling version of our algorithm is able
to decouple the dependence between consecutive outer it-
erations, and it is only used to simplify the technical proof.
In practice including our experiment, we use Algorithm 1
rather than the resampling version.

Before we present the main results, we introduce three con-
ditions that are essential for our analysis.

Condition 4.1 (Smoothness). For any β, β1, β2 ∈
B(p(cid:107)β∗(cid:107)2; β∗), where p ∈ (0, 1) is a model-dependent
constant, for any i ∈ [n], qi(·; ·) in Algorithm 1 satisﬁes the
smoothness condition with respect to the ﬁrst variable with
parameter L:

(cid:13)∇1qi(β1; β) − ∇1qi(β2; β)(cid:13)
(cid:13)

(cid:13)2 ≤ L(cid:13)

(cid:13)β1 − β2

(cid:13)
(cid:13)2.

Condition 4.1 says that the gradient of qi(·; ·) we use in
each inner iteration is Lipschitz continuous with respect to
the ﬁrst variable when the ﬁrst and second variables are
within the ball B(p(cid:107)β∗(cid:107)2; β∗). There exists a wide range
of models with this condition holding.

∈
Condition 4.2 (Concavity). For
B(p(cid:107)β∗(cid:107)2; β∗), where p ∈ (0, 1) is a model-dependent
constant, the function Qn(·; ·) satisﬁes the strong concavity
condition with parameter µ:

all β, β1, β2

(cid:2)∇1Qn(β1; β) − ∇1Qn(β2; β)(cid:3)(cid:62)

(β1 − β2)
≤ −µ(cid:107)β2 − β1(cid:107)2
2.

Condition 4.2 requires Qn(·; ·) to be strongly concave with
respect to the ﬁrst variable when the ﬁrst and second vari-
ables are within the ball B(p(cid:107)β∗(cid:107)2; β∗). This is a reason-
able requirement when N is large enough.

Condition 4.3 (First-order stability). For the true model
parameter β∗ and any β ∈ B(p(cid:107)β∗(cid:107)2; β∗), where p ∈
(cid:0)·; ·(cid:1) satisﬁes the
(0, 1) is a model-dependent constant, Qn
ﬁrst-order stability with parameter γ:
(cid:13)
(cid:13)∇1Qn(β∗; β) − ∇1Qn(β∗; β∗)(cid:13)

(cid:13)β − β∗(cid:13)
(cid:13)2.

(cid:13)2 ≤ γ(cid:13)

In this section, we show the main theory on the theoretical
guarantees of our proposed Algorithm 1. We also present
the implications of our algorithm applied to two models

Condition 4.3 requires that the gradient ∇1Qn(β∗; ·) is
stable with regard to the second variable, with the second
variable within the ball B(p(cid:107)β∗(cid:107)2; β∗). There are actually

High-Dimensional Variance-Reduced Stochastic Gradient Expectation-Maximization Algorithm

various versions of this condition in previous work (Yi &
Caramanis, 2015; Balakrishnan et al., 2014) on population
version Q(·; ·) = E[Qn(·, ·)]. Here we impose the condition
on the sample Q-function, i.e., Qn(·, ·), because our proof
technique directly analyzes the sample Q-function. Intu-
itively, when the sample size N is sufﬁciently large, Qn(·; ·)
and Q(·; ·) should be close. Therefore, this condition should
hold for Qn(·; ·) as well.

Due to the space limit, we verify the above conditions for
the two examples in the longer version of this paper. We use
κ = L/µ to denote the condition number .

4.1. Theory for the Generic Model

With the above conditions on qi(·; ·) and Qn(·; ·), we have
the following theorem to characterize the estimation error
of our estimator (cid:101)β(r) returned by the resampling version of
Algorithm 1.

Theorem 4.4. Suppose qi(·; ·) satisﬁes Condition 4.1 and
Qn(·; ·) satisﬁes Conditions 4.2, 4.3. We also assume
(cid:13)β∗(cid:13)
(cid:13)2 ≤ p(cid:13)
that (cid:13)
If
η ≤ µ/(8L2), and T and s are chosen such that

(cid:13)2, where p ∈ (0, 1).

(cid:13)βinit − β∗(cid:13)

ρ =

1
T (1 − τ )

+

2αη(cid:2)ηL2 + (2η + L/µ2)γ2(cid:3)
1 − τ

< 1,

√

√

where τ = α(1 − ηµ + 2η2L2) and α = 1 +
s − s∗,
then the estimator (cid:101)β(r) from the resampling version of Al-
gorithm 1 satisﬁes
E(cid:13)
(cid:13)2 ≤ ρr/2(cid:13)
(cid:13) (cid:101)β(r)−β∗(cid:13)
(cid:115)

(cid:13)βinit − β∗(cid:13)
(cid:13)2

s∗/

+

2(cid:101)sαη(2η + L/µ2)
(1 − τ )(1 − ρ)

(cid:13)
(cid:13)∇1Qn

(cid:0)β∗; β∗(cid:1)(cid:13)

(cid:13)∞,

where (cid:101)s = 2s + s∗.
Remark 4.5. As suggested in Theorem 4.4 that by choosing
an appropriate learning rate η, a sufﬁciently large number of
inner iterations T , and sparsity parameter s such that ρ < 1,
we can achieve a linear convergence rate. Here we give an
example to show that such ρ is achievable. If we choose
step size η = µ/(8L2), and truncation parameter s satisﬁes

s >

(cid:20) 4(1 − K)2
K 2

(cid:21)
s∗,

+ 1

where

Then, we can get

K =

5µ2
96L2 −

µ2γ2
12L4 −

γ2
3Lµ

> 0.

and the contraction parameter ρ in Theorem 4.4 can be
simpliﬁed as

ρ ≤

1
T (1 − τ )

+

3
4

.

Therefore, if we choose T ≥ 256κ2/(cid:0)3(α − 1)(cid:1), we can
obtain ρ ≤ 7/8, ensuring the linear convergence rate.

Remark 4.6. The right hand side of (4.1) in Theorem 4.4
consists of two terms. The ﬁrst term stands for the op-
timization error. The second term is the statistical error.
According to Remark 4.5, we can ensure the linear con-
vergence rate of our algorithm. Thus for any error bound
(cid:15) > 0, we need r ≥ 2 logρ−1[(cid:107)βinit − β∗(cid:107)2/(cid:15)] iterations
to let the optimization error ρr/2(cid:107)βinit − β∗(cid:107)2 ≤ (cid:15), which
basically requires O(cid:0) log(1/(cid:15))(cid:1) outer iterations. For each
outer iteration, we need to compute T gradients of qi(·, ·),
and one full gradient. Since we have T = O(κ2), which
is suggested in Remark 4.5, the gradient complexity of our
algorithm would be O(cid:0)(N + bκ2) · log(1/(cid:15))(cid:1). Neverthe-
less, for the state-of-the-art gradient based high-dimensional
EM algorithm (Wang et al., 2014), its gradient complexity
is O(cid:0)κN log(1/(cid:15))(cid:1). As long as κ ≤ N/b, the gradient
complexity of our algorithm is less than that of Wang et al.
(2014). Since in big data scenarios, N is always very large
and b as the batch size is relatively small, this condition is
naturally satisﬁed in most real applications.

The second term on the right-hand side of (4.1) stands for
the upper bound of the statistical error, which depends on
speciﬁc models as we will introduce later.

4.2. Implications for Speciﬁc Models

(4.1)

Now we apply our algorithm to the two examples introduced
in Section 3.2.

4.2.1. SPARSE GAUSSIAN MIXTURE MODEL

The next corollary gives the implication of our main theory
for sparse Gaussian mixture models.

4.7. Under
and

Corollary
of
the
suppose (cid:13)
≤
Theorem 4.4
((cid:112)λmin(Σ)/λmax(Σ)/4)(cid:13)
(cid:13)β∗(cid:13)
Then with proba-
(cid:13)2.
bility at least 1 − 2e/d, the estimator (cid:98)β = (cid:101)β(m) from the
resampling version of Algorithm 1 satisﬁes

conditions
same
(cid:13)βinit − β∗(cid:13)
(cid:13)2

(cid:13) (cid:98)β − β∗(cid:13)
E(cid:13)

(cid:13)2 ≤ ρm/2(cid:13)
(cid:114)

(cid:13)βinit − β∗(cid:13)
(cid:13)2

+ CΦκ3/2

s∗ log d · log N
N

,

(4.2)

α <

1
1 − 5µ2/96L2 + µ2γ2/12L4 + γ2/3Lµ

,

where Φ = λmin(Σ)(cid:0)(cid:107)Σ−1β∗(cid:107)∞ + σλ−1/2
L/µ.

min (Σ)(cid:1) and κ =

High-Dimensional Variance-Reduced Stochastic Gradient Expectation-Maximization Algorithm

Proof Sketch. For sparse Gaussian mixture model, we
have Conditions 4.1 to 4.3 hold with parameters L =
1/λmin(Σ), µ = 1/λmax(Σ), and γ = 20(ξ2 + ξ + 1 +
ξ−2)e−ξ2/64/λmin(Σ), where ξ = (cid:107)Σ−1/2β∗(cid:107)2 denotes
the signal-to-noise ratio (SNR). Next, (cid:101)s = 2s + s∗ is of the
same order as s∗. For the term (cid:107)∇1Qn(β∗; β∗)(cid:107)∞ in (4.1),
we have the following inequality holds with probability at
least 1 − 2e/d

(cid:107)∇1Qn(β∗; β∗)(cid:107)∞

(cid:18)

≤ C

(cid:107)Σ−1β∗(cid:107)∞ +

(cid:19)(cid:114)

σ
(cid:112)λmin(Σ)

log d · log N
N

.

This completes the proof.

Remark 4.8. We can see that the parameters in Condi-
tions 4.1 and 4.2 are determined by the covariance matrix Σ,
which is reasonable because Σ actually denotes the variance
of the data. For Condition 4.3, we need to introduce the
signal-to-noise ratio (SNR). The concept of SNR in parame-
ter estimation is also proposed in Balakrishnan et al. (2014);
Dasgupta & Schulman (2007). Since we have extended the
covariance matrix of noise from identity matrix in previous
work to any positive deﬁnite matrix, our SNR is also a little
bit different from their deﬁnition. Generally speaking, for
GMM with lower SNR, the variance of the noise makes
it harder or even impossible for the algorithm to converge.
Therefore, it is always reasonable to have a requirement for
the SNR of GMM to be large enough for reliable parameter
estimation. Spectral method (Anandkumar et al., 2014) can
be used to match the requirement on initialization for GMM,
however, we ﬁnd that random initialization also performs
reasonably well in practice as we will show later.

According to Remark 4.5, by choosing appropriate learn-
ing rate η,
inner iterations T , and sparsity parameter
s, we can ensure linear convergence rate of our algo-
rithm. Therefore, from Corollary 4.7, we know that af-
ter O(cid:0) log (cid:0)N/(s∗ log d log N )(cid:1)(cid:1) number of iterations, the
output of our algorithm attains O((cid:112)s∗ log d · log N/N ) sta-
tistical error, which matches the best-known error bound
(Wang et al., 2014; Yi & Caramanis, 2015) for Gaussian
mixture model up to a logarithmic factor log N . Note that
the extra logarithmic factor is due to the resampling strategy.

4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION

The implication of our main theory for mixture of linear
regression is presented in the following corollary.

4.9. Under
and

Corollary
of
the
suppose (cid:13)
≤
Theorem 4.4
((cid:112)λmin(Σ)/λmax(Σ)/32)(cid:13)
(cid:13)β∗(cid:13)
Then with proba-
(cid:13)2.
bility at least 1 − 2e/d, the estimator (cid:98)β = (cid:101)β(m) from the

conditions
same
(cid:13)βinit − β∗(cid:13)
(cid:13)2

resampling version of Algorithm 1 satisﬁes
(cid:13) (cid:98)β − β∗(cid:13)
E(cid:13)

(cid:13)2 ≤ ρm/2(cid:13)
(cid:18)

(cid:13)βinit − β∗(cid:13)
(cid:13)2
σ
(cid:112)λmax(Σ)

+ Cκ3/2

(cid:107)β∗(cid:107)2 +

(cid:19)(cid:114)

s∗ log d · log N
N

,

where κ = L/µ.

Proof Sketch. For mixture of linear regression, we have
Conditions 4.1 to 4.3 hold with parameters L = 2λmax(Σ),
µ = λmin(Σ)/2, and γ = γ1λmax(Σ), where γ1 ∈ (0, 1/3)
is a constant. We also show that (cid:101)s is of the same order as s∗.
Next, for the term (cid:107)∇1Qn(β∗; β∗)(cid:107)∞ in (4.1), we have the
following inequality holds with probability at least 1 − 2e/d

(cid:107)∇1Qn(β∗; β∗)(cid:107)∞

≤ C(cid:0)λmax(Σ)(cid:107)β∗(cid:107)2 + λ1/2

max(Σ)σ(cid:1)

(cid:114)

log d · log N
N

.

This completes the proof.

inner

iterations T ,

Remark 4.10. According to Remark 4.5, our algo-
rithm can achieve a linear convergence rate with ap-
propriate learning rate η,
and
sparsity parameter s.
Thus Corollary 4.9 tells
us that after O(cid:0) log (cid:0)N/(s∗ log d log N )(cid:1)(cid:1) number of
outer iterations,
the output of our algorithm achieves
O((cid:112)s∗ log d · log N/N ) statistical error, which matches
the best-known statistical error (Yi & Caramanis, 2015)
for mixture of linear regression up to a logarithmic factor
from the resampling strategy. Speciﬁcally, the dependence
on (cid:107)β∗(cid:107)2 is due to the fundamental limits of EM, which
also appears in Balakrishnan et al. (2014); Yi & Caramanis
(2015). There is also a spectral method (Chaganty & Liang,
2013) helping the initialization of MLR, but we use random
initialization which also performs well in our experiments.

5. Experiments

In this section, we present experiment results to validate
our theory. For parameter estimation, we use Gaussian
mixture model and mixture of linear regression, and com-
pare our proposed variance-reduced stochastic gradient
EM algorithm (VRSGEM) with two state-of-the-art high-
dimensional EM algorithms as baselines:

• (HDGEM) High-Dimensional Gradient EM algorithm
proposed in Wang et al. (2014): the gradient variant of
high-dimensional EM method enforcing sparsity struc-
ture.

• (HDREM) High-Dimensional Regularized EM algorithm
proposed in Yi & Caramanis (2015): the method based
on decaying regularization.

Since high-dimensional scenario is much more challenging,
we only compare our algorithm with high-dimensional EM
algorithms.

High-Dimensional Variance-Reduced Stochastic Gradient Expectation-Maximization Algorithm

(a)

(b)

(c)

(d)

Figure 1. Comparison of optimization error (cid:107) (cid:101)β(l) − (cid:98)β(cid:107)2 and overall estimation error (cid:107) (cid:101)β(l) − β∗(cid:107)2 for GMM. s∗ = 5, d = 256, b = 100,
N = 5000. (a) (b) errors against iterations, (c) (d) errors against training time.

(a)

(b)

(c)

(d)

Figure 2. Comparison of optimization error (cid:107) (cid:101)β(l) − (cid:98)β(cid:107)2 and overall estimation error (cid:107) (cid:101)β(l) − β∗(cid:107)2 for GMM. s∗ = 10, d = 512,
b = 200, N = 10000. (a) (b) errors against iteration, (c) (d) errors against training time.

5.1. Experimental Setup

For each latent variable model, we compare both the op-
timization error (cid:107) (cid:101)β(l) − (cid:98)β(cid:107)2 featuring the convergence of
the estimator to the local optima, and the overall estimation
error (cid:107) (cid:101)β(l) − β∗(cid:107)2 featuring the overall estimation accuracy
with regard to the true model parameter β∗. We also show
the convergence comparison in terms of training time. All
the comparisons are under two different parameter settings:
s∗ = 5, d = 256, b = 100, N = 5000 and s∗ = 10,
d = 512, b = 200, N = 10000. For VRSGEM, we choose
m = 30, n = 50 and T = 50 across all settings and models.
Besides the comparison of different algorithms, we also
verify our statistical rate of convergence by plotting the sta-
tistical error (cid:107) (cid:98)β − β∗(cid:107) against (cid:112)s∗ log d/N . Speciﬁcally,
we ﬁx d = 512 and show the plots of three cases s∗ = 5,
s∗ = 10 and s∗ = 15 with varying N .

In each experiment setting, we run 100 trials and show
the averaged results. The learning rate η is tuned by grid
search and s is chosen by cross validation. We use random
initialization.

5.2. Gaussian Mixture Model

variance matrix Σ of V is chosen to be a diagonal ma-
trix with all elements being 1. We randomly set two el-
ements to λmax(Σ) = 10, and another two elements to
λmin(Σ) = 0.1. The results are shown in Figures 1 and 2.

From Figures 1(a) and 2(a), we can see that all three algo-
rithms have linear convergence as Corollary 4.7 suggests.
VRSGEM clearly enjoys a faster convergence rate than the
baselines. Moreover, as shown in Figures 1(b) and 2(b), the
performance on overall estimation error of our algorithm
is as good as HDGEM, which is far better than HDREM.
In terms of time consumption, our algorithm also enjoys a
remarkable advantage over the baselines as shown in Fig-
ures 1(c), 1(d), 2(c) and 2(d).

The statistical error results are shown in Figure 5. From
Figure 5(a), we can see that statistical error of VRSGEM
shows a linear dependency on (cid:112)s∗ log d/N across different
settings of s∗, verifying results in Corollary 4.7.

5.3. Mixture of Linear Regression

Similar to the setting for GMM, we use the same covariance
matrix Σ in Section 5.2 for X here. For V , we let σ = 1.
We show the results in Figures 3 and 4.

We test VRSGEM on Gaussian mixture models introduced
in Section 3.2. For the sake of simplicity and better match-
ing the problem setting of the baseline methods, the co-

From Figures 3(a) and 4(a), we can see that VRSGEM
achieves linear convergence which is consistent with Corol-
lary 4.9, and our algorithm signiﬁcantly outperforms the

Iteration Index0102030Optimization Error10-1010-5100HDGEMHDREMVRSGEMIteration Index0102030Overall Estimation Error10-1100HDGEMHDREMVRSGEMTraining Time (in Seconds)0246Optimization Error10-1010-5100HDGEMHDREMVRSGEMTraining Time (in Seconds)0246Overall Estimation Error10-1100HDGEMHDREMVRSGEMIteration Index0102030Optimization Error10-1510-1010-5100HDGEMHDREMVRSGEMIteration Index0102030Overall Estimation Error10-210-1100HDGEMHDREMVRSGEMTraining Time (in Seconds)051015Optimization Error10-1510-1010-5100HDGEMHDREMVRSGEMTraining Time (in Seconds)051015Overall Estimation Error10-210-1100HDGEMHDREMVRSGEMHigh-Dimensional Variance-Reduced Stochastic Gradient Expectation-Maximization Algorithm

(a)

(b)

(c)

(d)

Figure 3. Comparison of optimization error (cid:107) (cid:101)β(l) − (cid:98)β(cid:107)2 and overall estimation error (cid:107) (cid:101)β(l) − β∗(cid:107)2 for MLR. s∗ = 5, d = 256, b = 100,
N = 5000. (a) (b) errors against iterations, (c) (d) errors against training time.

(a)

(b)

(c)

(d)

Figure 4. Comparison of optimization error (cid:107) (cid:101)β(l) − (cid:98)β(cid:107)2 and overall estimation error (cid:107) (cid:101)β(l) − β∗(cid:107)2 for MLR. s∗ = 10, d = 512,
b = 200, N = 10000. (a) (b) errors against iteration, (c) (d) errors against training time.

variance reduced gradient. We show that with an appropriate
initialization, the proposed algorithm converges at a linear
rate and attains the optimal statistical rate. We apply our
proposed algorithm to two popular latent variable models
in the high-dimensional regime and numerical experiments
are provided to support our theory.

It is worth noting that, the proposed algorithm is directly
applicable to the classical regime, by dropping the T-step.
It will give rise to an accelerated stochastic extension of
conventional EM algorithm, and the corresponding theory
in this paper can be extended to the classical regime anal-
ogously (Balakrishnan et al., 2014). We will investigate
this by-product in our future work. We also plan to extend
our algorithm to the estimation of high-dimensional latent
variable models with low-rank parameters (Yi & Caramanis,
2015).

Acknowledgments

We would like to thank the anonymous reviewers for their
helpful comments. This research was sponsored in part
by the National Science Foundation under Grant Numbers
CNS-1513939, CNS-1027965, IIS-1629161, IIS-1618948,
IIS-1652539. The views and conclusions contained in this
paper are those of the authors and should not be interpreted
as representing any funding agencies.

(a) GMM

(b) MLR

Figure 5. Statistical error (cid:107) (cid:98)β − β∗(cid:107)2 of VRSGEM against
(cid:112)s∗ log d/N with ﬁxed d = 512 and varying s∗ and N .

baselines in terms of optimization error. In terms of overall
estimation error shown in Figures 3(b) and 4(b), VRSGEM
is as good as HDGEM and beats HDREM by a remark-
able margin. Our algorithm also beats the baselines in
time consumption for convergence as we can see in Fig-
ures 3(c), 3(d), 4(c) and 4(d). Overall, VRSGEM achieves
the best performance among all the methods.

In addition, from Figure 5(b), we can see that for MLR,
the statistical error of VRSGEM is of order (cid:112)s∗ log d/N ,
which supports Corollary 4.9.

6. Conclusions and Future Work

We propose a novel accelerated stochastic gradient EM
algorithm based on a uniquely constructed semi-stochastic

Iteration Index0102030Optimization Error10-1010-5100HDGEMHDREMVRSGEMIteration Index0102030Overall Estimation Error10-1100HDGEMHDREMVRSGEMTraining Time (in Seconds)01234Optimization Error10-1010-5100HDGEMHDREMVRSGEMTraining Time (in Seconds)01234Overall Estimation Error10-1100HDGEMHDREMVRSGEMIteration Index0102030Optimization Error10-810-610-410-2100HDGEMHDREMVRSGEMIteration Index0102030Overall Estimation Error10-1100HDGEMHDREMVRSGEMTraining Time (in Seconds)051015Optimization Error10-810-610-410-2100HDGEMHDREMVRSGEMTraining Time (in Seconds)051015Overall Estimation Error10-1100HDGEMHDREMVRSGEMps$logd=n00.511.52kb-!-$k200.511.52s$=5s$=10s$=15ps$logd=n00.20.40.60.8kb-!-$k20.10.20.30.40.50.60.70.8s$=5s$=10s$=15High-Dimensional Variance-Reduced Stochastic Gradient Expectation-Maximization Algorithm

References

Allen-Zhu, Zeyuan and Hazan, Elad. Variance reduc-
tion for faster non-convex optimization. arXiv preprint
arXiv:1603.05643, 2016.

Anandkumar, Animashree, Ge, Rong, Hsu, Daniel, Kakade,
Sham M., and Telgarsky, Matus. Tensor decompositions
for learning latent variable models. Journal of Machine
Learning Research, 15:2773–2832, 2014.

Balakrishnan, Sivaraman, Wainwright, Martin J, and Yu,
Bin. Statistical guarantees for the EM algorithm: From
population to sample-based analysis. arXiv preprint
arXiv:1408.2156, 2014.

Blumensath, Thomas and Davies, Mike E. Iterative hard
thresholding for compressed sensing. Applied and Com-
putational Harmonic Analysis, 27(3):265–274, 2009.

Bottou, L´eon. Large-scale machine learning with stochastic
gradient descent. In Proceedings of COMPSTAT’2010,
pp. 177–186. Springer, 2010.

Chaganty, Arun Tejasvi and Liang, Percy. Spectral ex-
perts for estimating mixtures of linear regressions. arXiv
preprint arXiv:1306.3729, 2013.

Chen, Jinghui and Gu, Quanquan. Accelerated stochastic
block coordinate gradient descent for sparsity constrained
nonconvex optimization. In Proceedings of the Thirty-
Second Conference on Uncertainty in Artiﬁcial Intelli-
gence, pp. 132–141. AUAI Press, 2016.

Dasgupta, Sanjoy and Schulman, Leonard. A probabilis-
tic analysis of EM for mixtures of separated, spherical
Gaussians. Journal of Machine Learning Research, 8:
203–226, 2007.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.
Saga: A fast incremental gradient method with support
In Ad-
for non-strongly convex composite objectives.
vances in Neural Information Processing Systems, pp.
1646–1654, 2014.

Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum
likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society. Series B (Statisti-
cal Methodology), 39(1):1–38, 1977. ISSN 00359246.

Garber, Dan and Hazan, Elad. Fast and simple pca via
convex optimization. arXiv preprint arXiv:1509.05647,
2015.

Gemulla, Rainer, Nijkamp, Erik, Haas, Peter J, and Sis-
manis, Yannis. Large-scale matrix factorization with
distributed stochastic gradient descent. In Proceedings
of the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining, pp. 69–77. ACM,
2011.

Han, Jiawei, Pei, Jian, and Kamber, Micheline. Data mining:

concepts and techniques. Elsevier, 2011.

Jain, Anil K, Murty, M Narasimha, and Flynn, Patrick J.
Data clustering: a review. ACM computing surveys
(CSUR), 31(3):264–323, 1999.

Johnson, Rie and Zhang, Tong. Accelerating stochastic
gradient descent using predictive variance reduction. In
Advances in Neural Information Processing Systems, pp.
315–323, 2013.

Li, Xingguo, Zhao, Tuo, Arora, Raman, Liu, Han, and
Haupt, Jarvis. Stochastic variance reduced optimiza-
arXiv preprint
tion for nonconvex sparse learning.
arXiv:1605.02711, 2016.

Little, Roderick JA and Rubin, Donald B. Statistical analy-

sis with missing data. John Wiley & Sons, 2014.

Reddi, Sashank J, Hefny, Ahmed, Sra, Suvrit, P´ocz´os,
Barnab´as, and Smola, Alex. Stochastic variance re-
arXiv preprint
duction for nonconvex optimization.
arXiv:1603.06160, 2016.

Roux, Nicolas L, Schmidt, Mark, and Bach, Francis R. A
stochastic gradient method with an exponential conver-
gence rate for ﬁnite training sets. In Advances in Neural
Information Processing Systems, pp. 2663–2671, 2012.

Shalev Shwartz, Shai and Zhang, Tong. Stochastic dual
coordinate ascent methods for regularized loss minimiza-
tion. Journal of Machine Learning Research, 14(Feb):
567–599, 2013.

Tseng, Paul. An analysis of the EM algorithm and entropy-
like proximal point methods. Mathematics of Operations
Research, 29(1):27–44, 2004. ISSN 0364765X.

Wang, Zhaoran, Gu, Quanquan, Ning, Yang, and Liu, Han.
High dimensional expectation-maximization algorithm:
Statistical optimization and asymptotic normality. arXiv
preprint arXiv:1412.8729, 2014.

Wu, C. F. Jeff. On the convergence properties of the EM
algorithm. The Annals of Statistics, 11(1):95–103, 03
1983. doi: 10.1214/aos/1176346060.

Yi, Xinyang and Caramanis, Constantine. Regularized em
algorithms: A uniﬁed framework and statistical guar-
antees. In Advances in Neural Information Processing
Systems, pp. 1567–1575, 2015.

Zhang, Aston and Gu, Quanquan. Accelerated stochas-
tic block coordinate descent with optimal sampling. In
Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
pp. 2035–2044. ACM, 2016.

