Supplementary Material for
Hierarchy Through Composition with Multitask LMDPs

Andrew M. Saxe 1 Adam C. Earle 2 Benjamin Rosman 2 3

1. Generality of the LMDP

1.1. Tower of Hanoi

We demonstrate here how more general non-navigation tasks
can be modeled as LMDPs. The LMDP is deﬁned by a three-
tuple L = (cid:104)S, P, R(cid:105), where S is a set of states, P is a passive
transition probability distribution P : S × S → [0, 1], and
R is an expected instantaneous reward function R : S → R.

The speciﬁc form of the LMDP can seem to limit the ap-
plicability of the framework. Yet as shown in a variety of
recent work (Jonsson & Gómez, 2016), and in the exam-
ples given here, many standard domains can be translated to
the MDP framework; and there exists a general procedure
for embedding traditional MDPs in the LMDP framework
(Todorov, 2009).

A key difference between the LMDP and standard MDPs is
the fact that the cost function must include the KL term pe-
nalizing deviation from the ‘passive dynamics.’ While this
may seem limiting, most domains of interest have some no-
tion of ‘efﬁcient’ actions, making a control cost a reasonably
natural and universal phenomenon. Indeed, we suggest that
for many real-world domains the standard MDP formulation
is overly general, discarding useful structure in most real
world domains–namely, a preference for efﬁcient actions.
Standard MDP formulations commonly place small negative
rewards on each action to instantiate this efﬁciency goal, but
they retain the ﬂexibility to, for instance, prefer energetically
inefﬁcient trajectories by placing positive rewards on each
action. The drawback of this ﬂexibility is the unstructured
maximization over actions in the standard Bellman equation,
which prevents compositionality. In this sense, by accepting
additional structure which is broadly present in real-world
domains, the LMDP yields more explicit solutions. Here
we show how several common domains may be modeled as
LMDPs.

1Center for Brain Science, Harvard University 2School of
Computer Science and Applied Mathematics, University of
the Witwatersrand 3Council for Scientiﬁc and Industrial Re-
search, South Africa. Correspondence to: Andrew M. Saxe
<asaxe@fas.harvard.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

The passive dynamics approach is most natural when a do-
main has literal physical dynamics that occur in the absence
of control inputs, such as the dynamics of a robot arm acted
on by gravity or nearby positions in a spatial navigation
task. However, the approach can also be used to model non-
physical situations. The Tower of Hanoi puzzle (Simsek,
2008) demonstrates how an LMDP can model ‘conceptual’
tasks that do not possess physical dynamics. The puzzle
consists of blocks of different sizes stacked on three pegs
(Fig. 1A). The rules of the game are that a larger block may
not be stacked on a smaller block. Fig. 1B shows how this
setting may be instantiated and solved as an LMDP, by iden-
tifying a state with each unique conﬁguration of blocks, and
deﬁning the passive dynamics such that the only nonzero
transitions correspond to legal moves. The resulting LMDP
can be solved to yield the optimal trajectories (Fig. 1C).
With the MLMDP, new tasks like “place the medium size
block on the middle peg” can be represented as a new pat-
tern of boundary rewards over states (Fig. 1D), with the
optimal solution ﬂowing immediately from the composition-
ality property (Fig. 1E).

1.2. Taxi

The TAXI domain (Dietterich, 2000) is a common bench-
mark problem in hierarchical reinforcement learning, in
which a taxi must navigate a m × n grid-world domain in
which a subset of the states are demarcated as pick-up or
drop-off locations (l) for one or more passengers (p) (see
Fig. 2). The taxi may typically move in the four cardinal
directions, remain at it’s current state, execute a pick-up
action when at one of the demarcated locations with the
passenger, or execute the drop-off action when at one of the
demarcated locations with the passenger on board. Here the
agent must operate in the product space of the base domain
m × n, and the possible location-passenger conﬁgurations,
l choose p, for a complete state-space of |S|= (m × n)× (l
choose p).

The passive transition dynamics P required by the LMDP
framework are typically taken to be the resulting Markov
chain under a uniformly random policy over actions. Ex-
plicitly we might begin by deﬁning the transition dynamics

Hierarchy Through Composition with Multitask LMDPs

Figure 1. Tower of Hanoi. (A) The Tower of Hanoi puzzle contains blocks of different sizes and three pegs. A larger block may not be
placed on a smaller block. The goal is to reform the stack of blocks on the rightmost peg. (B) State graph representing this task as an
LMDP. Circles denote block conﬁgurations, edges denote transitions between states effected by valid moves, and the color of the circle
indicates the boundary reward for reaching each state (red is higher reward). (C) Cost-to-go solution found via LMDP. Arrows depict
trajectories, which all converge to the target conﬁguration. (D) A different task in the same domain corresponds to a different reward
structure across each state. Here the task is “place the medium size block on the middle peg”. (E) Cost-to-go solution for this new task,
found through composition using the MLMDP. Arrows denote trajectories.

for the base m × n grid-world domain which, under a uni-
formly random policy, results in the expected normalized
state-frequency visits under a random walk. This is simply
the banded matrix obtained by executing the up, down, left,
right and stay actions.

We then note that transitions between the states factored
by the l choose p possible passenger conﬁgurations can
only be realized by executing either the pick-up or drop-off
action. By way of example executing the pick-up action in
state (mli,Pj , nli,Pj ) transitions the state to (mli,Pt, nli,Pt )
indicating that the agent is still in the same base domain
state, but that the passenger is now in the taxi. The pick-up
and drop-off actions, must be evaluated at all l choose p
realizations of the base domain in order to map between all
the factored states.

Boundary rewards are simply placed at the appropriate base
domain state in the factor corresponding to the desired ﬁnal
location-passenger conﬁguration.

Moreover, standard hierarchical task decompositions may
be realized in our hierarchical framework. In the TAXI do-
main it is common to deﬁne a set of subtasks corresponding
to the macro-actions of navigating the taxi to one of the spec-
iﬁed locations (or the slightly more abstract GetPassenger
action), see Fig. 2for a simple task-graph description of
this. The full policy of getting a passenger at location A
to location B may then we described by ﬁrst executing the
appropriate macro-action to navigate the taxi to A, then exe-
cuting the pick-up action, then executing the macro-action
to navigate the taxi to B, and ﬁnally executing the drop-
off action. This hierarchy can be directly codiﬁed in our
scheme.

Firstly we must deﬁne the subtask policy corresponding to
navigating the taxi to location A (regardless of the passen-
gers position). This task is realized by placing boundary

rewards at the l choose p states of the full state-space corre-
sponding to all of the states in which the taxi is at location A
(one for each possible passenger location conﬁguration). A
similar procedure is followed to deﬁned the subtask polices
corresponding to navigating the taxi to the other locations.
The pick-up and drop-off actions are already suitably de-
ﬁned. Solving the full MDP by solving this structured task
decomposition mirrors the MAXQ approaches taken by
other authors (Jonsson & Gómez, 2016). Critical to realiz-
ing these task decompositions in our framework is the fact
that multiple states may be linked to a single subtask.

1.3. Robot visual servoing and warehouse scheduling

Finally, we note that several interesting domains have
been formulated as LMDPs, including the Mountain Car
(Todorov, 2006), warehouse scheduling (Jonsson & Gómez,
2016), and visual servoing on a robot (Kinjo et al., 2013).

2. Relationship to other hierarchical

approaches

Compared to standard HRL schemes, our approach differs
by relying on a parallel, distributed combination of tasks
enabled by the composition property of the LMDP: A mid-
level option in standard HRL chooses one of a discrete set
of subtasks to execute, whereas a mid-level layer in our
HRL scheme chooses a weighted task blend which drives
behavior.

The work of Jonsson & Gómez (2016) develops a MAXQ
hierarchy within the LMDP formalism, enabling powerful
hierarchical decomposition. This approach differs from
ours in the role compositionality plays. The MAXQ LMDP
makes use of compositionality to let a subtask have more
than one goal state by statically combining several single-
goal subtasks. Our scheme uses compositionality as an

ABCDEHierarchy Through Composition with Multitask LMDPs

Figure 2. Deﬁning MAXQ subtasks by their boundary reward structures. [A] The task-graph for the MAXQ decomposition. [B]
Representation of the boundary reward structure for subtasks that implement a MAXQ-like task structure. Each small panel depicts the
5 × 5 grid of spatial locations. Each row corresponds to a different location of the passenger (there are ﬁve possible passenger locations:
the location ∗ corresponds to the passenger being in the taxi, and locations A-D are the pick-up/drop-off points depicted in the taxi domain
at left). Each column depicts the boundary reward for a speciﬁc subtask {t1, · · · , t5}. White indicates high reward. By way of example,
the subtask ‘get to location A’ in-paints boundary rewards at each of the states corresponding to the taxi being at location A in the base
domain, regardless of where the passenger is located. Subtask t5 corresponds to the pick-up action by in-painting boundary rewards into
all states corresponding to the passenger being in the taxi. [C] An alternative view: the boundary reward graph for the subtasks. In this
representation it is clear that subtasks t1, · · · , t4 in-paint boundary rewards to exactly ﬁve states and collapse across passenger location,
while t5 in-paints rewards to all states in which the passenger is on board, collapsing across spatial location.

integral part of the execution model to enable a dynamic,
changing blend of tasks, such that every subtask is always
executing with different weightings. Future work might
explore the potential of combining these schemes.

3. Computational complexity advantages of

hierarchy

When solving multiple tasks, hierarchy can qualitatively
improve the efﬁciency of the solution. Fig. 3A shows a
simple domain consisting of N states arranged on a ring.
The agent wishes to perform N tasks corresponding to navi-
gating to each particular state. The transition dynamics are
local, meaning that there is nonzero probability of tran-
sitioning only to adjacent states or remaining still. As
noted in the main text, the hierarchical solution requires
only O(N log N ) iterations of z-learning, and requires only
O(N log N ) nonzero values to represent the value function
for all tasks, compared to O(N 2) for the ﬂat case. In par-
ticular, following the formulation given in the main text,
z-iteration requires

D
(cid:88)

l=1

M

(cid:18) N

M l +

N
M l−1

(cid:19)

= N (1 + M )

1 − (1/M )D
1 − 1/M

≤ N (1 + M )

≈ O(N log N )

iterations to propagate a value function signal to every state.
Moreover, when the desirability functions are initialized
to zero, K iterations of z-learning can only yield O(K)

nonzero elements in the desirability function, due to the
local transition dynamics. Hence by the same analysis, the
memory requirements are also O(N log N ).

To verify these theoretical results numerically, we imple-
ment this domain as follows: transitions to adjacent states
occur with probability p = .2, and the probability of re-
maining still is 1 − 2p. The interior rewards for each state
are −.1, the reward for reaching the goal state is 1, and the
KL penalty parameter λ = 1. Finally, for the hierarchical
formulation, we choose the probability of subtask transi-
tions α = .1, and place next-level states M = log N apart.
Fig. 3B shows the propagation of the value function over
successive iterations of z-learning. As can be seen, the front
progresses roughly linearly due to the local transition struc-
ture. Fig. 3C-E show the resulting number of iterations, wall
clock time, and memory usage for the ﬂat and hierarchical
cases. The ﬂat solver exhibits roughly quadratic scaling
while the hierarchical version is linearithmic.

4. Pseudocode for Hierarchical MLMDP

The execution structure of the hierarchical MLMDP algo-
rithm is listed in Algorithm 1. Brieﬂy, sampling a transition
to a subtask state at level l causes the next higher level
l + 1 to activate and begin its own sampling process. Higher
layers then communicate a task weighting w to lower layers.

ABCD*t1t2t3t4t5Boundary	Rewards	for	MAXQABCD*t1t2t3t4t5Boundary	Rewards	Graph	for	MAXQ(B)(C)TAXI DomainMAXQ task structureNavigateGet	to	AGet	to	BGet	to	CGet	to	DPick-up(A)Hierarchy Through Composition with Multitask LMDPs

Figure 3. Computational complexity advantages of hierarchy with the Multitask LMDP. (A) A ring of N states in which transitions may
only be made to adjacent states. The agent desires to perform N tasks corresponding to navigation to each position in the ring. (B)
Cost-to-go function over successive z-iterations. Because of the local transition structure, the cost-to-go function progresses about one
state on each iteration. Learning to reach a position K states away thus requires O(K) iterations. (C) Total iterations required to learn all
N tasks. (D) Wall clock time. (E) Total nonzero entries in cost-to-go representation (proportional to memory usage).

Simsek, O. Behavioral building blocks for autonomous agents:
Description, identiﬁcation, and learning. PhD thesis, University
of Massachussetts, Amherst, 2008.

Todorov, E. Linearly-solvable Markov decision problems. In NIPS,

Todorov, E. Efﬁcient computation of optimal actions. Proceedings
of the National Academy of Sciences, 106(28):11478–11483, 7
2009.

Compute controlled transition probability P 1(z1)
Sample next state → s1(t + 1)
if s1(t + 1) is a base state then

2006.

Algorithm 1 Hierarchical MLMDP
1: Initialize variables (z1, λ, · · ·)
2: while Not at goal do
3:
4:
5:
6:
7:
8:
9:
10:

Transition to next state
Update z1 via Z-iteration

for k = 2 → L do

else if s1(t + 1) is a higher layer state then

)

Compute controlled transition probability
P k(zk∗
Sample next state → sk(t + 1)
if sk(t + 1) is a subtask state then

Compute rk−1
via Eqn.(10)
Compute wk−1 via Eqn.(7)
Return

b

s1(t + 1) is a Terminal boundary state
Terminate episode

Repeat

else

11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24: end while

end if

else

end if
end for

References

Dietterich, T.G. Hierarchical Reinforcement Learning with the
MAXQ Value Function Decomposition. Journal of Artiﬁcial
Intelligence Research, 13:227–303, 2000.

Jonsson, A. and Gómez, V. Hierarchical Linearly-Solvable Markov

Decision Problems. In ICAPS, 2016.

Kinjo, K., Uchibe, E., and Doya, K. Evaluation of linearly solvable
Markov decision process with dynamic model learning in a
mobile robot navigation task. Frontiers in neurorobotics, 7
(April):7, 2013.

50100150200250300Iteration100200300400500Position-400-300-200-100A0500100015002000# States012345Total iterations#105FlatHier0500100015002000# States010203040Wall clock time (s)FlatHier0500100015002000# States0246810Total nonzero elements#105FlatHierBCDNstatesNtasksE