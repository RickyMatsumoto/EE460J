Dual Supervised Learning (Supplementary Document)

Yingce Xia 1 Tao Qin 2 Wei Chen 2 Jiang Bian 2 Nenghai Yu 1 Tie-Yan Liu 2

A. Theoretical Analysis

As we know, the ﬁnal goal of the dual learning is to give
correct predictions for the unseen test data. That is to say,
we want to minimize the (expected) risk of the dual models,
which is deﬁned as follows1:

R(f, g) = E

(cid:20) (cid:96)1(f (x), y) + (cid:96)2(g(y), x)
2

(cid:21)

, ∀f ∈ F, g ∈ G,

where F = {f (x; θxy); θxy ∈ Θxy}, G = {g(x; θyx); θyx ∈
Θyx}, Θxy and Θyx are parameter spaces, and the E is tak-
en over the underlying distribution P . Besides, let D de-
note the product space of the two models satisfying prob-
abilistic duality, i.e., the constraint in Eqn.(4). For ease of
reference, deﬁne Hdual as (F × G) ∩ D.

Deﬁne the empirical risk on the n sample as follows: for
any f ∈ F, g ∈ G,

Rn(f, g) =

1
n

(cid:88)n

(cid:96)1(f (xi), yi) + (cid:96)2(g(yi), xi)
2

.

i=1

Following (Bartlett & Mendelson, 2002), we introduce
Rademacher complexity for dual supervised learning, a
measure for the complexity of the hypothesis.
Deﬁnition 1. Deﬁne the Rademacher complexity of DSL,
RDSL

n , as follows:

(cid:104)

RDSL

n = E
z,σ

sup
(f,g)∈Hdual

(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:105)
(cid:0)(cid:96)1(f (xi), yi)+(cid:96)2(g(yi), xi)(cid:1)(cid:12)
(cid:12)

,

σi

where z = {z1, z2, · · · , zn} ∼ P n, zi = (xi, yi) in which
xi ∈ X and yi ∈ Y, σ = {σ1, · · · , σm} are i.i.d sampled
with P (σi = 1) = P (σi = −1) = 0.5.

Based on RDSL
supervised learning:

n , we have the following theorem for dual

1School of Information Science and Technology, Universi-
ty of Science and Technology of China, Hefei, Anhui, China
2Microsoft Research, Beijing, China. Correspondence to: Tao
Qin <taoqin@microsoft.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1The parameters θxy and θyx in the dual models will be omit-

ted when the context is clear.

Theorem 1 ((Mohri et al., 2012)). Let 1
2 (cid:96)1(f (x), y) +
1
2 (cid:96)2(g(y), x) be a mapping from X × Y to [0, 1]. Then, for
any δ ∈ (0, 1), with probability at least 1 − δ, the following
inequality holds for any (f, g) ∈ Hdual,

R(f, g) ≤ Rn(f, g) + 2RDSL

n +

(cid:114) 1
2n

1
δ

ln(

).

(1)

Similarly, we deﬁne the Rademacher complexity for the s-
tandard supervised learning RSL
n under our framework by
replacing the Hdual in Deﬁnition 1 by F × G. With proba-
bility at least 1−δ, the generation error bound of supervised
learning is smaller than 2RSL
n +

(cid:113) 1

2n ln( 1

δ ).

Since Hdual ∈ F × G, by the deﬁnition of Rademacher
n ≤ RSL
complexity, we have RDSL
n . Therefore, DSL enjoys
a smaller generation error bound than supervised learning.

The approximation of dual supervised learning is deﬁned
as

R(f ∗

F , g∗

F ) − R∗

(2)

in which

F ) = inf R(f, g), s.t. (f, g) ∈ Hdual;

F , g∗

R(f ∗
R∗ = inf R(f, g).

The approximation error for supervised learning is similar-
ly deﬁned.

Deﬁne Py|x = {P (y|x; θxy)|θxy ∈ Θxy},
Px|y = {P (x|y; θyx)|θyx ∈ Θyx}. Let P ∗
x|y de-
note the two conditional probabilities derived from P . We
have the following theorem:
Theorem 2. If P ∗
x|y ∈ Px|y, then super-
vised learning and DSL has the same approximation error.

y|x ∈ Py|x and P ∗

y|x and P ∗

Proof. By deﬁnition, we can verify both of the two approx-
imation errors are zero.

B. Details about the Language Models for

Marginal Distributions

We use the LSTM language models (Sundermeyer et al.,
2012; Mikolov et al., 2010) to characterize the marginal
distribution of a sentence x, deﬁned as (cid:81)Tx
i=1 P (xi|x<i),

Dual Supervised Learning

where xi is the i-th word in x, Tx denotes the number of
words in x, and the index < i indicates {1, 2, · · · , i − 1}.
The embedding dimension and hidden node are both 1024.
We apply 0.5 dropout to the input embedding and the last
hidden layer before softmax. The validation perplexities
of the language models are shown in Table 1, where the
validation sets are the same as those for machine translation
tasks.

Table 1. Validation Perplexities of Language Models
En↔De
En↔Fr

En↔Zh

En
88.72

Fr
58.90

En
101.44

De
90.54

En
70.11

Zh
113.43

As shown in Table 1, the perplexities of different language
models vary a lot, but out DSL can make improvements
on all the translation tasks (Please refer to Table 1 of the
main text). This shows that DSL is not very sensitive to the
qualities of the two marginal distributions.

For the marginal distributions for sentences of sentiment
classiﬁcation, we choose the LSTM language model again
like those for machine translation applications. The two
differences are: (i) the vocabulary size is 10000; (ii) the
word embedding dimension is 500. The perplexity of this
language model is 58.74.

References

Bartlett, Peter L and Mendelson, Shahar. Rademacher and
gaussian complexities: Risk bounds and structural re-
sults. Journal of Machine Learning Research, 3(Nov):
463–482, 2002.

Mikolov, Tomas, Karaﬁát, Martin, Burget, Lukas, Cer-
nock`y, Jan, and Khudanpur, Sanjeev. Recurrent neu-
ral network based language model. In Interspeech, vol-
ume 2, pp. 3, 2010.

Mohri, Mehryar, Rostamizadeh, Afshin, and Talwalkar,
Ameet. Foundations of machine learning. MIT press,
2012.

Sundermeyer, Martin, Schlüter, Ralf, and Ney, Hermann.
Lstm neural networks for language modeling. In Inter-
speech, pp. 194–197, 2012.

