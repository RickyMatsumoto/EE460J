Joint Dimensionality Reduction and Metric Learning: A Geometric Take

Mehrtash Harandi 1 2 Mathieu Salzmann 3 Richard Hartley 2 1

Abstract
To be tractable and robust to data noise, exist-
ing metric learning algorithms commonly rely on
PCA as a pre-processing step. How can we know,
however, that PCA, or any other speciﬁc dimen-
sionality reduction technique, is the method of
choice for the problem at hand? The answer is
simple: We cannot! To address this issue, in this
paper, we develop a Riemannian framework to
jointly learn a mapping performing dimensional-
ity reduction and a metric in the induced space.
Our experiments evidence that, while we directly
work on high-dimensional features, our approach
yields competitive runtimes with and higher ac-
curacy than state-of-the-art metric learning algo-
rithms.

1. Introduction

“To make it tractable for the distance metric learning al-
gorithms we perform dimensionality reduction by PCA to a
100 dimensional subspace” (Koestinger et al., 2012). “Like
most of the metric learning methods we ﬁrst center the
dataset and reduce the dimensionality to a n-dimensional
space by PCA” (Bohn´e et al., 2014). “ITML and LDML are
intractable when using 600 PCA dimensions” (Guillaumin
et al., 2009).

These quotations, extracted from the metric learning litera-
ture, give rise to a simple question: Is PCA, or, more gen-
erally, dimensionality reduction, a must to make metric
learning work on high-dimensional data, such as that in
computer vision problems?

To quantify this, in the top portion of Table 1, we pro-
vide the area under the ROC curve of state-of-the-art metric
learning techniques applied to the ASLAN dataset (Kliper-
Gross et al., 2012) using various PCA dimensions (denoted

1Data61, CSIRO, Canberra, Australia 2Australian Na-
3CVLab, EPFL,
Mehrtash Harandi

tional University, Canberra, Australia
Switzerland.
Correspondence
<mehrtash.harandi@anu.edu.au>.

to:

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

by p). These results suggest that state-of-the-art methods
either scale poorly with the dimensionality of the input and
thus require PCA to remain tractable (e.g., LDML), or re-
quire PCA to achieve an accuracy comparable to the other
baselines (e.g., KISSME).

In essence, this observation indicates that dimensionality
reduction is beneﬁcial to (i) reduce the computational bur-
den of the algorithms; and (ii) extract the relevant informa-
tion from the original noisy data. However, it also raises an
additional question: Is PCA, or any other speciﬁc dimen-
sionality reduction technique, really the best method for
the problem at hand? In other words, Shouldn’t we rather
learn the low-dimensional representation and the metric
jointly?

Motivated by these questions, in this paper, we introduce a
uniﬁed formulation for dimensionality reduction and met-
ric learning. As suggested by our results on the ASLAN
dataset in the bottom row of Table 1, our method out-
performs the state-of-the-art metric learning techniques.
Furthermore, despite the fact that we directly use high-
dimensional features as input, our method has comparable
runtimes to that of the fastest algorithms working on PCA-
based low-dimensional representations.

In the context of Mahalanobis metric learning, several
methods have proposed to allow the metric M to have low
rank, thus inherently performing dimensionality reduction.
Most methods, however, achieve this implicitly, by letting
M be positive semi-deﬁnite (Weinberger & Saul, 2009;
Davis et al., 2007), which, as opposed to explicit dimen-
sionality reduction, does not reduce the computational cost
of these algorithms. As a consequence, they still need to
rely on PCA as a pre-processing step in practice. While (Lu
et al., 2014) explicitly decomposes M = LLT , it enforces
orthogonality constraints on L to disambiguate the solu-
tions, thus effectively only performing dimensionality re-
duction, and not metric learning. By contrast, our approach
lets us learn a complete Mahalanobis metric jointly with a
low-dimensional projection.

At the heart of our joint dimensionality reduction and met-
ric learning formulation lie notions of Riemannian geom-
etry and quotient spaces. More speciﬁcally, we model the
projection to a low-dimensional space as a point on a Stiefel
manifold, and the metric in this space as a Symmetric Pos-

Joint Dimensionality Reduction and Metric Learning: A Geometric Take

Method
NCA (Goldberger et al., 2004)
ITML (Davis et al., 2007)
LDML (Guillaumin et al., 2009)
LMNN (Weinberger & Saul, 2009)
KISSME (Koestinger et al., 2012)
GMML (Zadeh et al., 2016)
DRML (Ours)

p = 25
0.594 (610s)
0.575 (13s)
0.602 (220s)
0.587 (37s)
0.574 (8s)
0.570 (9s)
0.630 (7s)

p = 150
0.586 (635s)
0.579 (100s)
0.598 (900s)
0.591 (42s)
0.522 (15s)
0.554 (34s)
0.627 (12s)

p = 500
0.584 (720s)
0.569 (1360s)
0.611 (3000s)
0.585 (1130s)
0.504 (35s)
0.543 (130s)
0.621 (105s)

p = 1000
0.584 (850s)
0.571 (10050s)
0.609 (6900s)
0.583 (2325s)
0.501 (100s)
0.539 (370s)
0.617 (360s)

Table 1: AUCs and training times for the state-of-the-art metric learning techniques and for our approach (DRML) on
the ASLAN dataset (Kliper-Gross et al., 2012). The baseline algorithms were applied after projecting the features to a
p-dimensional space by PCA, whereas our method learns the p-dimensional representation and the metric jointly.

itive Deﬁnite (SPD) matrix. We then show that our search
space reduces to a quotient of the product space of the
Stiefel and SPD manifolds with the orthogonal group. By
building upon recent advances in optimization on Rieman-
nian matrix manifolds (Absil et al., 2009), we therefore de-
velop a mathematical framework that effectively and efﬁ-
ciently lets us ﬁnd a solution in this space. Furthermore,
we show that our formulation can be kernelized. This not
only lets us handle non-linearity in the data, but also makes
our approach applicable to non-vectorial input data, such as
linear subspaces (Harandi et al., 2014), which have proven
beneﬁcial for many recognition tasks.

We demonstrate the beneﬁts of our joint dimensionality re-
duction and metric learning approach over existing metric
learning schemes on several tasks, including action similar-
ity matching, face veriﬁcation and person re-identiﬁcation.

2. Mathematical Background

In this work, as most metric learning algorithms, we are in-
terested in learning a Mahalanobis distance deﬁned below.
Deﬁnition 1 (The Mahalanobis distance). The Maha-
lanobis distance between x and ˜x in Rn is deﬁned as

M (x, ˜x) = (cid:107)x − ˜x(cid:107)2
d2

M = (x − ˜x)T M (x − ˜x) .

(1)

To have a valid metric, the Mahalanobis matrix M must
be positive deﬁnite.

As will be shown in Section 3, our approach to learning
a Mahalanobis metric can be formulated as a non-convex
optimization problem on a Riemannian manifold. This type
of problems can be expressed with the general form

have shown the beneﬁts of truly exploiting the geometry of
the manifold over standard constrained optimization. As
a consequence, these techniques have become increasingly
popular in diverse application domains (Mishra et al., 2014;
Harandi et al., 2017; Cunningham & Ghahramani, 2015).
A detailed discussion of Riemannian optimization goes be-
yond the scope of this paper, and we refer the interested
reader to (Absil et al., 2009).

As will be discussed in details in Section 3, we formulate
metric learning in the quotient space of the product space
of two Riemannian manifolds with the orthogonal group.
The two Riemannian manifolds at the heart of this formula-
tion are the Stiefel manifold and the manifold of Symmetric
Positive Deﬁnite (SPD) matrices deﬁned below.

Deﬁnition 2 (The Stiefel Manifold). The set of (n × p)-
dimensional matrices, p ≤ n, with orthonormal columns
endowed with the Frobenius inner product1 forms a com-
pact Riemannian manifold called the Stiefel manifold
St(p, n) (Boothby, 2003).

St(p, n) (cid:44) {W ∈ Rn×p : W T W = Ip} .

(3)

Deﬁnition 3 (The SPD Manifold). The set of (p × p) di-
mensional real, SPD matrices endowed with the Afﬁne In-
variant Riemannian Metric (AIRM) (Pennec et al., 2006)
forms the SPD manifold S p
++.

S p
++

(cid:44) {M ∈ Rp×p : vT M v > 0, ∀v ∈ Rp − {0p}} .
(4)

The dimensionality of St(p, n) and S p
and p(p + 1)/2, respectively.

++ are np− 1

2 p(p+1)

minimize f (z)

s.t. z ∈ M ,

3. Our Approach

(2)

where M is a Riemannian manifold, i.e., informally, a
smooth surface that locally resembles a Euclidean space.

Our goal, as that of many other metric learning algorithms,
is to learn a Mahalanobis distance between the input mea-
Ideally, this distance should reﬂect the class
surements.

While Riemannian manifolds can often be explicitly en-
coded in terms of constraints on z, the recent advances
in Riemannian optimization techniques (Absil et al., 2009)

1Note that the literature is divided between this choice and
another form of Riemannian metric. See (Edelman et al., 1998)
for details.

Joint Dimensionality Reduction and Metric Learning: A Geometric Take

labels of the samples. Furthermore, motivated by our anal-
ysis of existing methods, which all beneﬁt from a PCA pre-
processing step, we also seek to reduce the dimensionality
of the data. However, in contrast to existing methods, we
propose to learn the lower-dimensional representation and
the Mahalanobis distance in that space jointly.
More speciﬁcally, we want to learn a projection W : Rn →
Rp and a Mahalanobis matrix M ∈ S p
++, such that the
induced distance in Rp is more discriminative. To this
end, let X = {(xi, ˜xi, yi)}m
i=1 be a set of triplets, where
xi, ˜xi ∈ Rn are the feature vectors of two training sam-
ples, and the label yi ∈ {0, 1} determines whether xi and
˜xi are similar (yi = 1) or not (yi = 0). The Mahalanobis
distance between xi and ˜xi in the low-dimensional space
can thus be written as

M ,W (xi, ˜xi) = (W T xi − W T ˜xi)T M (W T xi − W T ˜xi)
d2

= (x − ˜xi)T W M W T (xi − ˜xi) .

(5)

To learn a latent space whose Mahalanobis distance reﬂects
class similarity, we make use of the logistic loss. More pre-
cisely, for each pair of samples (xi, ˜xi) sharing the same
label, i.e., yi = 1, we deﬁne the loss

(cid:96)(xi, ˜xi|yi = 1) = log(1 + pi) ,

(6)

with

pi = exp

(cid:16)

(cid:17)
β(x − ˜x)T W M W T (x − ˜x)

, β > 0.

(7)

Conversely, for a pair of samples (xj, ˜xj) whose labels dif-
fer, i.e., yj = 0, we deﬁne the loss

(cid:96)(xj, ˜xj|yj = 0) = log(1 + p−1

j ) .

(8)

Intuitively,
the loss of Eq. 6 is minimized when
d2
M ,W (xi, ˜xi) → 0, whereas the loss of Eq. 8 is mini-
mized when d2

M ,W (xj, ˜xj) → ∞.

The losses for all training triplets can be grouped into a cost
function of the form
L(W , M |X) (cid:44) (cid:88)

log(1 + pi)

+

i|yi=1
(cid:88)

i|yi=0

log(1 + p−1

i

) + λr(M , M 0), (9)

++ × S p

which further encodes a regularizer on M . This regular-
izer, r : S p
++ → R+, allows us to exploit prior
knowledge on the Mahalanobis matrix, encoded by a refer-
ence matrix M 0. Following common practice (Davis et al.,
2007; Hoffman et al., 2014), we make use of the asymmet-
ric Burg divergence, which yields

r(M , M 0) = Tr(MM −1

0 ) − log det(MM −1

0 ) − p .

In our experiments, since typically no strong prior is avail-
able, we simply use M 0 = Ip, i.e., the identity matrix.
Joint dimensionality reduction and metric learning can then
be achieved by minimizing the cost function of Eq. 9 w.r.t.
W and M . To avoid degeneracies, and following common
practice in dimensionality reduction, we constrain W to be
a matrix with orthonormal columns. That is,

W T W = Ip.

(11)

With this constraint, W is in fact a point on the Stiefel
manifold St(p, n). Since both M and W lie on Rieman-
nian manifolds, albeit different ones, we propose to make
use of Riemannian optimization to solve our problem, as
described below.

3.1. Manifold-based Optimization

To determine W and M , we need to solve the optimization
problem

(12)

L(W , M |X)
min
W ,M
s.t. W T W = Ip, M (cid:31) 0 .
Jointly minimizing with respect to W and M can be
achieved by making use of the product space of the Stiefel
and SPD manifolds, Mp = St(p, n) × S p
++. Both St(p, n)
and S p
++ are smooth homogeneous spaces and their prod-
uct preserves smoothness and differentiability (Absil et al.,
2009). Thus, Mp can be given a Riemannian structure.
However, in our case, a closer look at L(W , M |X) reveals
that
L(W , M |X) = L(W R, RT M R|X) , ∀R ∈ Op , (13)

where Op is the orthogonal group. This implies that
π : Mp×Op → Mp : (cid:0)(W , M (cid:1), R(cid:1) → (cid:0)W R, RT M R(cid:1)
(14)
is a right group action on Mp. The theorem below estab-
lishes an important property about the action of Op on Mp,
which will prove crucial to our develop our approach.
Theorem 1. The set M (cid:44) (cid:0)St(p, n) × S p
the equivalence relation

(cid:1)\O(p) with

++

[(cid:0)W , M (cid:1)] ∼

(cid:111)
(cid:110)(cid:0)W R, RT M R(cid:1); ∀R ∈ O(p)

(15)

and Riemannian metric
g(W ,M )((ξW , ξM ) , (ςW , ςM )) = 2 Tr(ξT

W ςW )
(16)
+ Tr(M −1ξM M −1ςM )

forms a Riemannian quotient manifold.

Proof. See the supplementary material.

The search space of our problem therefore truly is this Rie-
mannian manifold M. To be able to perform Riemannian
optimization on M, below, we derive the required entities.

(10)

Joint Dimensionality Reduction and Metric Learning: A Geometric Take

The Geometry of M

The general theory of quotient manifolds (Lee, 2003; Ab-
sil et al., 2009) tells us that the equivalence relation splits
the tangent space of Mp at Ω = (W , M ) into two comple-
mentary parts: the horizontal space HΩMp and the vertical
space VΩMp. These two spaces are such that

gp(hΩ, vΩ) = 0, ∀hΩ ∈ HΩMp and ∀vΩ ∈ VΩMp ,

(17)
where gp is the Riemannian metric of the product manifold
Mp. The vertical space VΩMp has the property that pro-
jecting any of its vectors to Mp via the exponential map
yields a point in the equivalence class of Ω. Therefore, the
tangent space of M can be identiﬁed with the horizontal
space, i.e., T[Ω]M (cid:44) HΩMp.

A tangent vector ξ↑
Ω ∈ T[Ω]M can be obtained from a
tangent vector ξΩ ∈ TΩMp by projection.
It can be
shown that the horizontal space at Mp (cid:51) (W , M ) =
(U [Ip, 0p,n−p]T , M ) with U ∈ On is the set (details in
the supplementary material)

(cid:40) (cid:18)

(cid:20)V M −1 − M −1V
B

(cid:21)

U

(cid:19) (cid:41)

, V

,

with V ∈ Sym(p), B ∈ R(n−p)×p. Furthermore, we have
the following theorem to obtain the tangent vectors in M.

Theorem 2 (Projecting on the Horizontal Space). For
(ξW , ξM ) ∈ T(W ,M )Mp, the horizontal vector (i.e., the
associated tangent vector in T[(W ,M )]M) is identiﬁed as

(cid:0)ξW − W Θ, ξM − M Θ + ΘM (cid:1) ,

(18)

with Θ the solution of the following Sylvester equation:

ΘM 2 + M 2Θ = M (cid:0)ξT

W W − W T ξW +
M −1ξM − ξM M −1(cid:1)M .

(19)

Proof. See the supplementary material.

To perform Newton-type optimization on M, we also need
the form of the retraction R[(W ,M )] : T[(W ,M )]M → M,
which follows from the retraction on Mp. In particular, we
suggest the following retraction:

R[(W ,M )](ξW , ξM ) (cid:44) (cid:0)uf(W + ξW ),
M 1/2 expm(M −1/2ξM M −1/2)M 1/2(cid:1) .

(20)

Here uf(A) = A(AT A)−1/2, which yields an orthogonal
matrix and expm(·) denotes the matrix exponential. Alto-
gether, this provides us with the tools required to perform
Riemannian optimization to solve our problem. The only
missing mathematical entity is the Euclidean gradient of

Figure 1: Convergence behavior of our algorithm.

our loss function w.r.t. W and M , which we provide in
the supplementary material.

In our experiments, we employed Conjugate Gradient de-
scent on M to solve (12). In particular, we implemented
the operations required for our manifold within the manopt
Riemannian optimization toolbox (Boumal et al., 2014).
The code is available at https://sites.google.
com/site/mehrtashharandi/.

In Fig. 1, we illustrate the typical convergence behavior
of our algorithm using the ASLAN dataset (Kliper-Gross
et al., 2012). In our experiments, we have observed that the
algorithm converges quite fast (typically in less than 25 it-
erations), thus making it scalable to learning large metrics.

3.2. Computational Complexity

The complexity of each iteration of our algorithm to
solve (12) depends on the computational cost of the fol-
lowing major steps:

• Objective

function

evaluation.

Computing

L(W , M |X) takes O(mnp + mp2 + p3 + np2).

• Euclidean gradient evaluation: Computing ∇W takes
O(mn2 + pn2 + np2), and computing ∇M takes
O(mn2 + pn2 + np2 + p3). Note that some com-
putations are common to both ∇W and ∇M . Hence
the total ﬂops for this step is less than the addition of
the Stiefel and SPD parts.

• Projecting (∇W , ∇M ) to the tangent space of Mp

takes O(2p2(n + p)).

• Projecting a tangent vector in Mp costs O(2np2) to
form the Sylvester equation and O(p3) to solve it us-
ing the Bartels - Stewart algorithm (Bartels & Stewart,
1972).

• Retraction: For the Stiefel part, the retraction τSt
takes O(4np2 + 11p3). For the SPD part, τSPD takes
O(3p3).

3550360036503700375038000.20.40.60.81.01.21.41.61.82.03.63.84.04.45.4Cost Running time (sec) Joint Dimensionality Reduction and Metric Learning: A Geometric Take

These steps are either linear or quadratic in n. Therefore,
and as evidenced by our experiments, our approach can ef-
fectively and efﬁciently handle high-dimensional input fea-
tures without any PCA pre-processing.
Remark 1. The number of unknowns determined by our
algorithm corresponds to the dimensionality of Mp, that
2 p(p + 1) − 1
is, np − 1
2 p(2n −
p + 1). By contrast, the metric learning techniques that
utilize PCA as a pre-processing step only determine 1
2 p(p+
1) unknowns, which is typically much smaller. As such,
our method can potentially better leverage large amounts
of training triplets. In our Big Data era, we believe this to
be an important strength of our approach.

2 p(p − 1) = 1

2 p(p + 1) + 1

3.3. Discussion
An SPD matrix M ∈ S p
++ can be decomposed as U DU T
with U ∈ Op and D a diagonal matrix with positive ele-
ments. As such, the term W M W T appearing in our loss
L(W , M |X) can be written as

W M W T = W U DU T W T = V DV T ,

with St(p, n) (cid:51) V = W U . Thus, our optimization prob-
lem can be expressed by a loss L(V , D|X) with a search
space deﬁned as St(p, n) × Rp
+. Theoretically, this rep-
resentation has the same expressive power as our formula-
tion if we ignore the invariance of V DV T to permutations.
However, in the context of ﬁxed-rank matrix factorization
(see (Mishra et al., 2014), Section 3.2), it has been shown
that, for a parametrization of the form W M V T , where
W , V ∈ St(p, n), modeling M as an SPD matrix is typi-
cally more effective than as a diagonal matrix with positive
elements. The argument there is that it “gives more ﬂexi-
bility to optimization algorithms” (Mishra et al., 2014).
One can also factorize W M W T as LLT with L ∈ Rn×p.
This factorization, though being widely used, is not in-
variant to the action of Op, meaning that replacing L →
LR, R ∈ Op will not change the loss. Such an invariance
hinders gradient descent algorithms, as shown for exam-
ple in (Journ´ee et al., 2010; Mishra et al., 2014). In Sec-
tion 6, we empirically show that this is indeed the case for
the problem of interest here, i.e., metric learning.

In (Journ´ee et al., 2010), the invariance induced by the ac-
tion of Op in a factorization of the form LLT is taken into
account. In particular, the authors make use of a quotient
geometry to overcome the undesirable effects of the invari-
ance in gradient descent optimization. There is a subtle,
yet important difference between our formulation and that
of (Journ´ee et al., 2010): Our approach can beneﬁt from a
factorization with redundancy, which is effective in prac-
tice. Furthermore, note that the geometry developed in our
paper can also handle the case where a Mahalanobis metric
is searched for (i.e., without recasting the problem as a fac-

Method
Euc-LLT
Rim-LLT
Rim-V DV T
W MW T (Ours)

p = 25
0.597
0.625
0.622
0.630

p = 150
0.600
0.601
0.624
0.627

p = 500
0.599
0.602
0.615
0.621

p = 1000
0.601
0.608
0.610
0.617

Table 2: AUC for various geometries on ASLAN.
torization problem), which is the case in techniques such
as (Globerson & Roweis, 2005; Koestinger et al., 2012;
Zadeh et al., 2016).

Before concluding this part, we contrast the aforemen-
tioned factorization for the experiment reported in Table 1.
To this end, we replace the term W M W T in our loss with
1. LLT , L ∈ Rn×p, and optimize using Euclidean geom-
etry. We call this solution Euc-LLT .
2. LLT , L ∈ Rn×p, and optimize using the geometry
developed in (Journ´ee et al., 2010). We call this solution
Rim-LLT .
3. V DV T , V ∈ St(p, n) and D a diagonal and posi-
tive matrix. We optimize using the geometry of the prod-
uct manifold St(p, n) × Rp
+. We call this solution Rim-
V DV T .

Following the experiment shown in Table 1, we evaluate
the AUC for various dimensionalities using the aforemen-
tioned geometries. The results are provided in Table 2.
First, we note that the general practice, i.e., using Euclidean
geometry, is signiﬁcantly outperformed by its Riemannian
counterparts. The quotient geometry developed in (Journ´ee
et al., 2010) performs on par with our approach for low di-
mensionalities (e.g., p = 25). However, for larger dimen-
sionalities, our technique yields more accurate solutions,
suggesting that the redundancy in the formulation plays an
important role. The importance of the redundancy can also
be noticed by comparing Rim-V DV T against our solu-
tion. In terms of computation time, the diagonal form, i.e.,
Rim-V DV T yields only slightly faster runtimes. In the
particular case of ASLAN, the training time for p = 1000
was reduced to 150s.

4. Kernelizing the Solution

We now show how our approach can handle nonlinearity
in the data, as well as generalize to non-vectorial input
data, such as linear subspaces, which have proven effec-
tive for video recognition (Turaga et al., 2011; Harandi
et al., 2014; Jayasumana et al., 2015). Following com-
mon practice when converting a linear algorithm to a non-
linear one (e.g., from PCA to kernel PCA), we make use
of a mapping of the input data to a Reproducing Kernel
Hilbert Space (RKHS). As shown below, the resulting al-
gorithm then only depends on kernel values (i.e., it does not
explicitly depend on the mapping to RKHS). Since much
progress has recently been made in developing positive def-

Joint Dimensionality Reduction and Metric Learning: A Geometric Take

inite kernels for non-vectorial data (Harandi et al., 2014;
Jayasumana et al., 2015; Vishwanathan et al., 2010), this
makes our approach applicable to a much broader variety
of input types.

Speciﬁcally, let φ : X → H be a mapping from the input
space X to an RKHS H with corresponding kernel function
k(xi, xj) = (cid:104)φ(xi), φ(xj)(cid:105). Following the same formal-
ism as before, we can deﬁne a cost function of the form

LH(W , M |X) (cid:44) (cid:88)

log(1 + ˜pi)

(21)

+

i,yi=1
(cid:88)

i,yi=0

log(1 + ˜p−1

i

) + λr(M , M 0),

H(x, ˜x) = (cid:0)φ(xi) −
H(xi, ˜xi)(cid:1) and d2
with ˜pi = exp (cid:0)βd2
φ(xj)(cid:1)T
W M W T (cid:0)φ(xi) − φ(xj)(cid:1), with M ∈ S p
++ and
W ∈ St(p, dim(H)). Note that for universal kernel func-
tions, such as the Gaussian kernel, dim(H) → ∞. We
therefore need a formulation where only the kernel func-
tion appears, and not φ explicitly. To this end, we exploit
the representer theorem (Sch¨olkopf et al., 2001), which
states that the mapping W lies in the span of the train-
ing data, and can thus be expressed as W = Φ(D)A.
Here, Φ(D) = (φ(d1), · · · , φ(dl)) ∈ Rdim(H)×l is a ma-
trix that stacks the representation of the l training samples
in the feature space. In this formalism, the orthogonality
constraint on W can be written as

W T W = AT Φ(D)T Φ(D)A = AT K(D, D)A = Ip ,

where K(D, D) ∈ S l
++ is the kernel matrix with elements
[K(D, D)]i,j = k(di, dj). Let us deﬁne St(p, l) (cid:51) B =
K(D, D)1/2A, such that the orthogonality constraint be-
comes BT B = Ip. This lets us write
H(x, ˜x) = (cid:0)φ(x) − φ(˜x)(cid:1)T
d2

= (cid:0)k(x, D) − k(˜x, D)(cid:1)T
× K(D, D)− 1

W M W T (cid:0)φ(x) − φ(˜x)(cid:1)
K(D, D)− 1
2 (cid:0)k(x, D) − k(˜x, D)(cid:1),

(22)

2 BM BT

where Rl (cid:51) k(x, D) = (cid:0)k(x, d1), · · · , k(x, dl)(cid:1)T
. Thus,
the cost deﬁned in Eq. 21 can be rewritten as a function
of B, i.e., LH(B, M |X), which, by taking d2
H(x, ˜x) from
Eq. 22, only depends on kernel values. Since this cost func-
tion has essentially the same form as the one derived in Sec-
tion 3, and the variables M and B lie on the same types of
manifold as those of Section 3, we can use the same opti-
mization strategy as before.

5. Related Work

Metric learning is a well-studied problem whose origins
can be traced back to the early eighties (e.g., (Short &
Fukunaga, 1981)). Here, we focus on the prime represen-
tatives that will be used as baselines in our experiments.

For a more thorough study, we refer the reader to the recent
book by (Bellet et al., 2015).

idea

(Goldberger et al., 2004)

of Neighborhood Component Analysis
The
(NCA)
is to optimize the
error of a stochastic nearest neighbor classiﬁer in the space
induced by the Mahalanobis metric. The Information-
Theoretic Metric Learning (ITML) algorithm, proposed
by (Davis et al., 2007), learns a Mahalanobis metric by
exploiting a notion of margin between pairs of samples.
More precisely, the algorithm searches for a Mahalanobis
matrix satisfying two types of constraints:
(i) an upper
bound u on the distance between pairs of samples from
the same class, i.e., in our formalism, d2
M (xi, ˜xi) ≤ u,
∀i | yi = 1; (ii) a lower bound l on the distance be-
tween pairs of dissimilar samples, i.e., d2
M (xi, ˜xi) ≥ l,
∀i | yi = 0.

The Large Margin Nearest Neighbors (LMNN) of (Wein-
berger & Saul, 2009) introduces the notion of local margins
for metric learning. In LMNN, learning the Mahalanobis
metric is expressed as a convex optimization problem that
encourages the k nearest neighbors of any training instance
xi to belong to the same class as xi, while keeping away
instances of other classes.

Discriminant

based Metric

Logistic
Learning
(LDML) (Guillaumin et al., 2009) relies on a Maha-
lanobis distance-based sigmoid function to encode the
likelihood that two samples belong to the same class. The
metric is then learned by maximizing the likelihood of the
sample pairs (xi, ˜xi) that truly belong to the same class,
i.e., yi = 1, while minimizing that of the sample pairs that
do not, i.e., yi = 0.

While effective, all the above-mentioned techniques rely
on PCA as a pre-processing step to remain tractable. By
contrast, the efﬁcient “Keep It Simple and Straightforward
Metric” (KISSME) algorithm of (Koestinger et al., 2012)
focuses on addressing large-scale problems. KISSME as-
sumes that the similar and dissimilar pairs are generated
from two independent Gaussian distributions. Comput-
ing the Mahalanobis metric then translates to maximiz-
ing a log-likelihood, which can be achieved in closed-
form. As illustrated in Table 1, however, this algorithm
requires PCA pre-processing to achieve accuracies com-
parable to the ones produced by the other algorithms. In
the spirit of KISSME, Geometric Mean Metric Learn-
ing (GMML) (Zadeh et al., 2016) relies on the geodesic
connecting two covariance matrices to identify the Maha-
lanobis metric.

While effective and quite efﬁcient, the above-mentioned
techniques usually rely on PCA as a pre-processing step
to reduce the dimensionality of the data. As evidenced by
our experiments, this pre-processing step is sub-optimal.

Joint Dimensionality Reduction and Metric Learning: A Geometric Take

Method

baseline (Kliper-Gross et al., 2012)
NCA (Goldberger et al., 2004)
ITML (Davis et al., 2007)
LDML (Guillaumin et al., 2009)
LMNN (Weinberger & Saul, 2009)
KISSME (Koestinger et al., 2012)
GMML (Zadeh et al., 2016)
DRML
kDRML

HoG

AUC
CRR
0.56
54.2%
0.59
56.6%
0.58
55.6%
0.61
57.3%
0.59
55.9%
0.58
55.2%
0.57
55.6%
58.3%
0.63
59.7% 0.64

HoF

AUC
CRR
0.57
54.0%
0.60
57.1%
0.55
53.9%
0.60
56.5%
0.56
53.5%
0.54
52.8%
0.53
52.8%
0.64
59.1%
60.7% 0.64

HnF

AUC
CRR
0.58
54.5%
0.60
56.7%
0.58
55.9%
0.61
58.0%
0.59
56.0%
0.60
55.7%
0.58
55.8%
0.63
58.6%
59.2% 0.63

Figure 2: Examples from the ASLAN dataset.

Table 3: Average accuracy and AUC for the ASLAN dataset.

6. Experimental Evaluation

We now evaluate our algorithms (DRML and kDRML)
and compare them with the representative baseline met-
ric learning methods discussed above, i.e., NCA (Gold-
berger et al., 2004) LMNN (Weinberger & Saul,
2009), ITML (Davis et al., 2007), LDML (Guillaumin
et al., 2009), KISSME (Koestinger et al., 2012) and
GMML (Zadeh et al., 2016), as well as with dataset-
speciﬁc baselines mentioned below. Our experiments con-
sist of two parts. First, we make use of benchmark datasets
where the data can be represented in vector (Euclidean)
form, and thus both DRML and kDRML are applica-
ble. Second, we consider manifold-valued data where only
kDRML applies.

In all our experiments, we followed the so-called restricted
protocol. That is, the only information accessible to the
algorithms is the similarity/dissimilarity labels of pairs of
samples; the class labels of the samples are unknown. For
all the methods, we report the results obtained with the best
subspace dimension. Note that this means that not all meth-
ods use the same subspace dimension. However, it makes
the comparison more fair, since it truly shows the full po-
tential of the algorithms.

6.1. Experiments with Euclidean Data

ACTION SIMILARITY MATCHING.

As a ﬁrst experiment, we considered the task of action sim-
ilarity recognition using the ASLAN dataset (Kliper-Gross
et al., 2012). The ASLAN dataset contains 3,697 human
action clips collected from YouTube, spanning over 432
unique action categories (see Fig. 2). The sample distribu-
tion across the categories is highly uneven, with 116 classes
possessing only one video clip. The benchmark proto-
col focuses on action similarity (same/not-same), rather
than action classiﬁcation, and testing is performed on
previously-unseen actions.

The dataset comes with 10 predeﬁned splits of the data,
where each split consists of 5,400 training and 600 testing
pairs of action videos. The ASLAN dataset also provides
three different types of descriptors: Histogram of Oriented

Gradients (HoG), Histogram of Optical Flow (HoF), and
a composition of both (referred to as HnF). The videos
are represented by spatiotemporal bags of features (Laptev
et al., 2008) with a codebook of size 5,000. For kDRML,
we used an RBF Gaussian kernel whose bandwidth was set
using Jaakkola’s heuristic (Jaakkola et al., 1999).

In Table 3, we report the classiﬁcation accuracy and the
Area Under the ROC Curve (AUC) of our algorithms and
of the baselines. Here, we also include the results of the
benchmark (Kliper-Gross et al., 2012), which provides us
with a direct comparison of previously published results.
Note that DRML and kDRML outperform all the other al-
gorithms. In general, kDRML performs better than DRML.

To further evidence the beneﬁts of jointly learning the
low-dimensional projection and the metric, we performed
the following experiment, using the HoG features. We
ﬁxed the matrix W to the subspace obtained by PCA, and
learned the metric using our loss function. This resulted in
a drop in accuracy of roughly 1%, i.e., a CRR of 57.4%.
This conﬁrms our intuition that we can achieve better than
PCA by jointly learning the subspace and the metric.
Remark 2. In (Kliper-Gross et al., 2012), it was shown
that other metrics (e.g., the cosine similarity) could out-
perform the Euclidean distance (used here as a baseline).
In principle, our framework can also be used to learn co-
sine similarities by generalizing the inner product (cid:104)a, b(cid:105) as
aT W M W T b. Doing so, however, goes beyond the scope
of this paper.

PERSON RE-IDENTIFICATION.

For the task of person re-identiﬁcation, we used the iLIDS
dataset (Zheng et al., 2009). The dataset consists of 476
images of 119 pedestrians and was captured in an airport.
The number of images for each person varies from 2 to 8.
The dataset contains severe occlusions caused by people
and baggage.

In our experiments, we adopted the single-shot protocol.
That is, the dataset was randomly divided into two sub-
sets, training and test, with 59 and 60 exclusive individ-
uals, respectively. The random splitting was repeated 10
times. In each partition, one image from each individual

Joint Dimensionality Reduction and Metric Learning: A Geometric Take

Figure 3: Examples from the iLIDS dataset (Zheng et al., 2009).

Method
NCA (Goldberger et al., 2004)
kLFDA-lin (Xiong et al., 2014)
kLFDA-Chi2 (Xiong et al., 2014)
ITML (Davis et al., 2007)
LDML (Guillaumin et al., 2009)
LMNN (Weinberger & Saul, 2009)
KISSME (Koestinger et al., 2012)
GMML (Zadeh et al., 2016)
DRML
kDRML

r = 1
27.9%
32.3%
36.5%
29.5%
27.8%
32.6%
28.0%
31.1%
32.0%
39.2%

r = 5
r = 10
r = 20
52.0%
65.5%
80.7%
57.2%
70.0%
83.9%
64.1%
76.5%
88.5%
50.3%
62.6%
76.4%
53.2%
67.0%
82.5%
56.2%
68.9%
83.0%
54.2%
67.9%
81.6%
55.6%
68.5%
82.9%
85.7%
71.6%
57.6%
65.5% 77.6% 89.5%

Table 4: CMC at rank 1, 5, 10 and 20 on the iLIDS dataset.

in the test set was randomly selected as the reference im-
age and the rest of the images were used as query images.
This process was repeated 20 times. We used the features
provided by the authors of (Xiong et al., 2014). These fea-
tures describe each image using 16-bin histograms from the
RGB, YUV and HSV color channels, as well as texture his-
tograms based on Local Binary Patterns (Ojala et al., 2002)
extracted from 6 non-overlapping horizontal bands. For the
kernel-based solutions, i.e., kDRML and kLFDA, we used
the Chi-square kernel.

We report performance in terms of the Cumulative Match
Characteristic (CMC) curves for different rank values r
indicating that we search for a correct match among
the r nearest neighbors. Table 4 compares our results
with those of the baseline metric learning algorithms, as
well as with kernel Local Fisher Discriminant Analysis
(kLFDA) (Xiong et al., 2014), which represents the state-
of-the-art on this dataset. Our kDRML method achieves the
highest scores for all ranks. Note that kLFDA requires the
subject identities during training, while the other methods,
including ours, don’t. Despite this, kDRML outperforms
the state-of-the-art results of kLFDA-Chi2.

6.2. Experiments with Manifold-Valued Data

To illustrate the fact that our algorithm generalizes to non-
vectorial input data, we utilized the Youtube Faces (YTF)
dataset (Wolf et al., 2011) and represented each video as a
point on a Grassmann manifold. The YTF dataset contains
3,425 videos of 1,595 subjects collected from the YouTube
website. These videos depict large variations in pose, il-
lumination and expression. To evaluate the performance
of the algorithms, we followed the protocol suggested
in (Wolf et al., 2011). Speciﬁcally, we used the 5,000 video
pairs ofﬁcially provided with the dataset, which are equally
divided into 10 folds. Each fold contains 250 ‘same’ and
250 ‘not-same’ pairs. We used the provided LBP features

Method
baseline (Wolf et al., 2011)
NCA (Goldberger et al., 2004)
ITML (Davis et al., 2007)
LDML (Guillaumin et al., 2009)
LMNN (Weinberger & Saul, 2009)
KISSME (Koestinger et al., 2012)
GMML (Zadeh et al., 2016)
kDRML

Acc.
65.4%
63.3%
62.8%
59.7%
57.8%
67.5%
67.7%
71.9%

AUC
0.698
0.794
0.715
0.631
0.763
0.778
0.738
0.798

EER
0.360
0.284
0.346
0.411
0.306
0.301
0.328
0.277

Table 5: Average accuracy, AUC and EER on the YTF
dataset (Wolf et al., 2011).

and modeled each video by a subspace of dimensionality
10 as described in (Wolf et al., 2011). As a result, each
video was modeled as a point on the Grassmann manifold
G(10, 1770), where 1,770 is the dimensionality of the LBP
features. We used the projection kernel deﬁned as

kproj(Si, Sj) = (cid:107)ST

i Sj(cid:107)2

F .

While kDRML directly uses a kernel function, some base-
lines (e.g., KISSME), do not. To still be able to report re-
sults for these baselines, we utilized kernel PCA, instead of
PCA, to create their inputs.

Table 5 summarizes the performance of the metric learn-
ing techniques and the baseline (Wolf et al., 2011) using
the same input, i.e., subspaces of dimensionality 10. Here
again, kDRML comfortably outperforms the other methods
for all the error metrics. For example, the gap in accuracy
between kDRML and its closest competitor, i.e., KISSME,
is more than 4%.
Remark 3. Note that, as shown in (Feragen et al., 2015),
an RBF kernel of the form exp(−σd2
g(·, ·)) with dg being
the geodesic distance on the Grassmann manifold is not a
positive deﬁnite kernel. The projection kernel, however, has
been shown to be positive deﬁnite (Hamm & Lee, 2008),
which, ultimately, is all we require to make our algorithm
applicable to manifold-valued data. Furthermore, this ker-
nel has proven effective in a variety of applications (Hamm
& Lee, 2008; Harandi et al., 2017).

7. Conclusions and Future Work

In this paper, we have argued against treating dimensional-
ity reduction as a pre-processing step to metric learning.
We have therefore introduced a framework that learns a
low-dimensional representation and a Mahalanobis metric
in this space in a uniﬁed manner. We have shown that the
resulting framework could be cast an optimization prob-
lem on the quotient space of the product space of two Rie-
mannian manifolds with the orthogonal group. Our experi-
ments have evidenced the beneﬁts of our uniﬁed approach
over state-of-the-art metric learning algorithms that rely on
PCA as a pre-processing step.
In the future, we plan to
study the use of other cost functions within our uniﬁed
framework, especially formulations based on the concept
of large margin.

Joint Dimensionality Reduction and Metric Learning: A Geometric Take

References

Absil, P-A, Mahony, Robert, and Sepulchre, Rodolphe.
Optimization algorithms on matrix manifolds. Princeton
University Press, 2009.

Bartels, Richard H. and Stewart, GW. Solution of the ma-
trix equation ax+ xb= c. Communications of the ACM,
15(9):820–826, 1972.

Bellet, Aur´elien, Habrard, Amaury, and Sebban, Marc.
Metric Learning. Morgan & Claypool Publishers, 2015.

Bohn´e, Julien, Ying, Yiming, Gentric, St´ephane, and Pon-
til, Massimiliano. Large margin local metric learn-
ing. In Proc. European Conference on Computer Vision
(ECCV), pp. 679–694. Springer, 2014.

Boothby, William Munger. An introduction to differen-
tiable manifolds and Riemannian geometry, volume 120.
Gulf Professional Publishing, 2003.

Boumal, N., Mishra, B., Absil, P.-A., and Sepulchre, R.
Manopt, a Matlab toolbox for optimization on mani-
folds. Journal of Machine Learning Research, 15:1455–
1459, 2014. URL http://www.manopt.org.

Cunningham, John P and Ghahramani, Zoubin. Linear di-
mensionality reduction: Survey, insights, and general-
izations. JMLR, 2015.

Davis, Jason V, Kulis, Brian, Jain, Prateek, Sra, Suvrit,
Information-theoretic metric
In Proc. Int. Conference on Machine Learn-

and Dhillon, Inderjit S.
learning.
ing (ICML), pp. 209–216, 2007.

Edelman, Alan, Arias, Tom´as A, and Smith, Steven T. The
geometry of algorithms with orthogonality constraints.
SIAM journal on Matrix Analysis and Applications, 20
(2):303–353, 1998.

Hamm, Jihun and Lee, Daniel D. Grassmann discriminant
analysis: a unifying view on subspace-based learning. In
Proc. Int. Conference on Machine Learning (ICML), pp.
376–383. ACM, 2008.

Harandi, Mehrtash, Salzmann, Mathieu,

Jayasumana,
Sadeep, Hartley, Richard, and Li, Hongdong. Expand-
ing the family of Grassmannian kernels: An embedding
perspective. In Proc. European Conference on Computer
Vision (ECCV), pp. 408–423. Springer, 2014.

Harandi, Mehrtash, Salzmann, Mathieu, and Hartley,
Richard. Dimensionality reduction on SPD manifolds:
IEEE
The emergence of geometry-aware methods.
Trans. Pattern Analysis and Machine Intelligence, 2017.

Hoffman, Judy, Rodner, Erik, Donahue, Jeff, Kulis, Brian,
and Saenko, Kate. Asymmetric and category invariant
feature transformations for domain adaptation. Int. Jour-
nal of Computer Vision, 109(1-2):28–41, 2014.

Huang, Gary B, Ramesh, Manu, Berg, Tamara, and
Learned-Miller, Erik. Labeled faces in the wild: A
database for studying face recognition in unconstrained
environments. Technical report, Technical Report 07-49,
University of Massachusetts, Amherst, 2007.

Jaakkola, Tommi, Diekhans, Mark, and Haussler, David.
Using the Fisher kernel method to detect remote protein
homologies. In Proceedings of the Seventh International
Conference on Intelligent Systems for Molecular Biol-
ogy, pp. 149–158. AAAI Press, 1999.

Jayasumana, S., Hartley, R., Salzmann, M., Li, H., and
Harandi, M. Kernel methods on Riemannian manifolds
with Gaussian RBF kernels. Pattern Analysis and Ma-
chine Intelligence, IEEE Transactions on, 37(12):2464–
2477, 2015.

Feragen, Aasa, Lauze, Francois, and Hauberg, Soren.
Geodesic exponential kernels: When curvature and lin-
earity conﬂict. In Proc. IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2015.

Journ´ee, Michel, Bach, Francis, Absil, P-A, and Sepulchre,
Rodolphe. Low-rank optimization on the cone of posi-
tive semideﬁnite matrices. SIAM Journal on Optimiza-
tion, 20(5):2327–2351, 2010.

Globerson, Amir and Roweis, Sam. Metric learning by
collapsing classes. In Proc. Advances in Neural Infor-
mation Processing Systems (NIPS), volume 18, pp. 451–
458, 2005.

Goldberger, Jacob, Roweis, Sam, Hinton, Geoff, and
Salakhutdinov, Ruslan. Neighbourhood components
analysis. In Proc. Advances in Neural Information Pro-
cessing Systems (NIPS), 2004.

Kliper-Gross, Orit, Hassner, Tal, and Wolf, Lior. The action
similarity labeling challenge. IEEE Trans. Pattern Anal-
ysis and Machine Intelligence, 34(3):615–621, 2012.

Koestinger, Martin, Hirzer, Martin, Wohlhart, Paul, Roth,
Peter M, and Bischof, Horst. Large scale metric learning
from equivalence constraints. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition (CVPR),
pp. 2288–2295. IEEE, 2012.

Guillaumin, Matthieu, Verbeek,

Jakob, and Schmid,
Is that you? metric learning approaches for
In Proc. Int. Conference on Com-

Cordelia.
face identiﬁcation.
puter Vision (ICCV), pp. 498–505, 2009.

Laptev, I., Marszalek, M., Schmid, C., and Rozenfeld,
B. Learning realistic human actions from movies.
In
Proc. IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 1–8, 2008.

Joint Dimensionality Reduction and Metric Learning: A Geometric Take

Zadeh, Pourya, Hosseini, Reshad, and Sra, Suvrit. Geo-
In Proc. Int. Conference

metric mean metric learning.
on Machine Learning (ICML), pp. 2464–2471, 2016.

Zheng, Wei-Shi, Gong, Shaogang, and Xiang, Tao. Asso-
ciating groups of people. In BMVC, volume 2, pp. 6,
2009.

Lee, John M. Smooth manifolds. Springer, 2003.

Lu, Jiwen, Zhou, Xiuzhuang, Tan, Yap-Pen, Shang,
Yuanyuan, and Zhou, Jie. Neighborhood repulsed met-
IEEE Trans. Pat-
ric learning for kinship veriﬁcation.
tern Analysis and Machine Intelligence, 36(2):331–345,
2014.

Mishra, B, Meyer, G, Bonnabel, S, and Sepulchre, R.
Fixed-rank matrix factorizations and Riemannian low-
rank optimization. Computational Statistics, 29(3-4):
591–621, 2014.

Ojala, Timo, Pietik¨ainen, Matti, and M¨aenp¨a¨a, Topi. Mul-
texture
tiresolution gray-scale and rotation invariant
IEEE Trans.
classiﬁcation with local binary patterns.
Pattern Analysis and Machine Intelligence, 24(7):971–
987, 2002.

Pennec, Xavier, Fillard, Pierre, and Ayache, Nicholas. A
Riemannian framework for tensor computing. Int. Jour-
nal of Computer Vision, 66(1):41–66, 2006.

Sch¨olkopf, Bernhard, Herbrich, Ralf, and Smola, Alex J.
In Computational

A generalized representer theorem.
learning theory, pp. 416–426. Springer, 2001.

Short, Robert D and Fukunaga, Keinosuke. The opti-
mal distance measure for nearest neighbor classiﬁcation.
IEEE Transactions on Information Theory, 27(5):622–
627, 1981.

Turaga, P., Veeraraghavan, A., Srivastava, A., and Chel-
lappa, R. Statistical computations on Grassmann and
Stiefel manifolds for image and video-based recognition.
IEEE Trans. Pattern Analysis and Machine Intelligence,
33(11):2273–2286, 2011.

Vishwanathan, S Vichy N, Schraudolph, Nicol N, Kondor,
Risi, and Borgwardt, Karsten M. Graph kernels. Journal
of Machine Learning Research, 11:1201–1242, 2010.

Weinberger, Kilian Q and Saul, Lawrence K. Distance met-
ric learning for large margin nearest neighbor classiﬁca-
tion. Journal of Machine Learning Research, 10:207–
244, 2009.

Wolf, Lior, Hassner, Tal, and Maoz, Itay. Face recognition
in unconstrained videos with matched background sim-
ilarity. In Proc. IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 529–534, 2011.

Xiong, Fei, Gou, Mengran, Camps, Octavia, and Sznaier,
Mario. Person re-identiﬁcation using kernel-based met-
ric learning methods. In Proc. European Conference on
Computer Vision (ECCV), pp. 1–16. Springer, 2014.

