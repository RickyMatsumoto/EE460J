Supplementary Material for:
Recursive Partitioning for Personalization using Observational Data

Nathan Kallus 1

Omitted Proofs

Proof of Theorem 1. By Asn. 1, we have

E [Y | X = x, T = t] = E [Y (T ) | X = x, T = t]
= E [Y (t) | X = x, T = t]
= E [Y (t) | X = x]

(deﬁnition of Y = Y (T ))
(conditioned on T = t)
(Asn. 1).

Consider a realization of the data and X = x where convergence occurs for all t ∈ [m]. Let

(cid:15)(x) = inf{ζ : s ∈ [m], ζ = (cid:0)E [Y | X = x, T = s] − mint∈[m] E [Y | X = x, T = t](cid:1) > 0},

where inf(∅) = ∞. By assumption of convergence at this realization of the data and X = x, we have that even-
tually for all t ∈ [m], |ˆµt,nt(x) − E [Y | X = x, T = t]| < (cid:15)(x)/2, at which point we must necessarily also have
ˆτn(x) ∈ arg mint∈[m] E [Y | X = x, T = t] = arg mint∈[m] E [Y (t) | X = x]. By assumption of pointwise consistency
and because the intersection of ﬁnitely many a.s. events is a.s., the set of such realization of the data and X = x have
probability 1.

Proof of Theorem 2. First note that, given any x with P (T = t | X = x) > 0, we have

E [Y | X = x, T = t] =

E[Y I[T =t]|X=x]
P(T =t|X=x) = E

(cid:104) Y I[T =t]
φ(t,x)

(cid:105)

| X = x

= E

(cid:104) Y I[T =t]

φ(T,X) | X = x

(cid:105)

= E

(cid:104) Y I[T =t]
Q

(cid:105)

| X = x

.

Therefore, since P (T = t | X) > 0 almost surely,

R(τ ) = E [Y (τ (X))] = E [E [Y (τ (X)) | X]]
= E [E [Y (τ (X)) | X, T = τ (X)]]
= E [E [Y | X, T = τ (X)]]
= E [E [Y I [T = τ (X)]/Q | X]]
= E [Y I [T = τ (X)]/Q]

(iterated expectations)
(Asn. 1)
(deﬁnition of Y )
(above observation)
(iterated expectations) .

Proof of Theorem 4. We start with 1vA. Restrict to x such that φ(s, x) > 0 ∀s (almost everywhere). Let µ(t, x) =
E [Y (t) | X = x]. Under Asn. 1,

δtvA(x) = E [Y | X = x, T = t] − E [Y | X = x, T (cid:54)= t]

= E [Y | X = x, T = t] − (cid:80)
= µ(t, x) − (cid:80)

s(cid:54)=t φ(s, x)µ(s, x)/ (cid:80)

s(cid:54)=t

s(cid:54)=t φ(s, x).

E [Y | X = x, T = s] P (T = s | X = x, T (cid:54)= t)

1School of Operations Research and Information Engineering and Cornell Tech, Cornell University. Correspondence to: Nathan

Kallus <kallus@cornell.edu>.

Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

Supplementary Material for: Recursive Partitioning for Personalization using Observational Data

(cid:12)
(cid:12)
(cid:12)

ˆδtvA
n (x) − δtvA(x)

Since φ(s, x) > 0, it’s clear that δtvA(x) ≤ δsvA(x) ∀s if and only if µ(t, x) ≤ µ(s, x) ∀s. The rest of the proof for 1vA
follows the same way as Thm. 1, showing that, under the assumption of pointwise consistent estimation, the estimation
(cid:12)
(cid:12)
(cid:12) is eventually smaller than half the decision gap, (cid:15)1vA(x) = inf{ζ : s ∈ [m], ζ =
gap supt∈[m]
(cid:0)δsvA(x) − mint∈[m] δtvA(x)(cid:1) > 0}, a.s. and for almost everywhere x.
Next, we deal with 1v1-A. Fix x. Fix any tm ∈ arg maxt∈[m] µ(t, x). Let δtvmin(x) = mins(cid:54)=t δtvs(x). If t, s (cid:54)= tm, then
δtvmin(x)−δsvmin(x) = µ(t, x)−µ(s, x). On the other hand, for any t ∈ [m], we always have both µ(t, x)−µ(tm, x) ≤ 0
and δtvmin(x) − δtmvmin(x) ≤ 0. Therefore, we have

t ∈ arg mint∈[m] µ(t, x) ⇐⇒ µ(t, x) − µ(s, x) ≤ 0 ∀s (cid:54)= t ⇐⇒ µ(t, x) − µ(s, x) ≤ 0 ∀s (cid:54)= t, tm

⇐⇒ δtvmin(x) − δsvmin(x) ≤ 0 ∀s (cid:54)= t, tm ⇐⇒ δtvmin(x) − δsvmin(x) ≤ 0 ∀s (cid:54)= t
⇐⇒ t ∈ arg mint∈[m] δtvmin(x).

(cid:12)
(cid:12)
(cid:12)

ˆδtvmin
n

(cid:12)
(cid:12)
(x) − δtvmin(x)
(cid:12)

ˆδtvmin
n

ˆδtvs
nt+ns

=

Let

(x)

and

that

note

ˆδtvs
nt+ns

supt∈[m]

(x)
(cid:12)
(cid:12)
(cid:12)

mins(cid:54)=t
(cid:12)
(cid:12)
(x) − δtvs(x)
supt∈[m],s∈[m]
(cid:12), which converges to zero under pointwise consistency. The rest of the proof
for 1v1-A follows as above, showing that this estimation gap is eventually smaller than half the decision gap,
(cid:15)1v1-A(x) = inf{ζ : s ∈ [m], ζ = (cid:0)δsvmin(x) − mint∈[m] δtvmin(x)(cid:1) > 0}, a.s. and for almost everywhere x.
Next, we deal with 1v1-B. Fix x and a realization of the data where convergence holds for all t (cid:54)= s. Then, eventually
(cid:12)
(cid:12)
ˆδtvs
(cid:12) ≤ |δtvs(x)| /2 for all t (cid:54)= s such that δtvs(x) (cid:54)= 0. That is, eventually I
(cid:12)
(cid:12)
(x) − δtvs(x)
(x) < 0
=
(cid:12)
nt+ns
I [δtvs(x) < 0] for all t (cid:54)= s such that δtvs(x) (cid:54)= 0. Restrict to such large enough n. Let kt(x) = (cid:80)
I [δtvs(x) < 0],
ˆkt(x) = (cid:80)
m − kmin(x) ⇐⇒ ˆkt(x) ≥ m − kmin(x) ⇐= t ∈ arg maxt∈[m]

(cid:12)arg mint∈[m] µ(t, x)(cid:12)
(cid:12). Then, t ∈ arg mint∈[m] µ(t, x) ⇐⇒ kt(x) =
(cid:104)ˆδtvs
I

, and kmin(x) = (cid:12)

(cid:105)
(x) < 0
.

(cid:105)
(x) < 0

(cid:104)ˆδtvs

(cid:104)ˆδtvs

nt+ns

nt+ns

(cid:80)

t(cid:54)=s

t(cid:54)=s

≤

(cid:105)

I

s(cid:54)=t

nt+ns

Proof of Theorem 5. By random sampling, (Xij , Tij , Yij (1), . . . , Yij (m)) are distributed iid as (X, T, Y (1), . . . , Y (m))
is in population. For j ∈ [ntest], let ijt be ij’s match for treatment t, or ij if Tij = t. Under exact matching,
Yijt(1), . . . , Yijt(m) | Xji is distributed the same as Yij (1), . . . , Yij (m) | Xji, Tji = t. By writing ˆYij t = Yijt =
(cid:80)m

I [t = s] Yijs(s), we see that

s=1

E[ ˆYij τ (Xij )] = E (cid:2)E (cid:2)(cid:80)m

(cid:3)(cid:3)

s=1

I (cid:2)s = τ (Xij )(cid:3) Yijs(s) | Xij
E (cid:2)I (cid:2)s = τ (Xij )(cid:3) E (cid:2)Yijs(s) | Xji
E (cid:2)I [s = τ (Xi)] E (cid:2)Yij (s) | Xi, Ti = s(cid:3)(cid:3)
E (cid:2)I [s = τ (Xi)] E (cid:2)Yij (s) | Xi
I [s = τ (Xi)] Yij (s) | Xi

(cid:3)(cid:3)
(cid:3)(cid:3)

(cid:3)(cid:3)

s=1

s=1

= (cid:80)m
= (cid:80)m
= (cid:80)m
= E (cid:2)E (cid:2)(cid:80)m
= E (cid:2)Yij (τ (Xij ))(cid:3)

s=1

s=1

(iterated expectation)

(linearity)
(exact matching)
(Asn. 1)
(linearity)
(iterated expectation)

