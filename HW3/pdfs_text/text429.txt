Differentially Private Submodular Maximization:
Data Summarization in Disguise

Marko Mitrovic 1 Mark Bun 1 2 Andreas Krause 3 Amin Karbasi 1

Abstract

Many data summarization applications are cap-
tured by the general framework of submodular
maximization. As a consequence, a wide range
of efﬁcient approximation algorithms have been
developed. However, when such applications in-
volve sensitive data about individuals, their pri-
vacy concerns are not automatically addressed.
To remedy this problem, we propose a gen-
eral and systematic study of differentially private
submodular maximization. We present privacy-
preserving algorithms for both monotone and
non-monotone submodular maximization under
cardinality, matroid, and p-extendible system
constraints, with guarantees that are competitive
with optimal solutions. Along the way, we ana-
lyze a new algorithm for non-monotone submod-
ular maximization under a cardinality constraint,
which is the ﬁrst (even non-privately) to achieve
a constant approximation ratio with a linear num-
ber of function evaluations. We additionally pro-
vide two concrete experiments to validate the ef-
ﬁcacy of these algorithms.

1. Introduction
A set function f : 2V → R is said to be submodular if
for all sets S ⊆ T ⊆ V and every element v ∈ V we
have f (S ∪ {v}) − f (S) ≥ f (T ∪ {v}) − f (T ). That is,
the marginal contribution of any element v to the value of
the function f (S) diminishes as the input set S increases.
The theory of submodular maximization uniﬁes and gener-
alizes diverse problems in combinatorial optimization, in-
cluding the Max-Cover, Max-Cut, and Facility Location
problems. In turn, this theory has recently found numerous
applications to problems in machine learning, data science,
and artiﬁcial intelligence. A few such applications include
exemplar-based clustering (Krause & Gomes, 2010), fea-
ture selection for classiﬁcation (Krause & Guestrin, 2005),
document and corpus summarization (Lin & Bilmes, 2011;

1Yale University, New Haven, CT, USA 2Princeton University,
Princeton, NJ, USA 3ETH Zurich, Zurich, Switzerland. Corre-
spondence to: Marko Mitrovic <marko.mitrovic@yale.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Kirchhoff & Bilmes, 2014; Sipos et al., 2012), crowd teach-
ing (Singla et al., 2014), and inﬂuence maximization in so-
cial networks (Kempe et al., 2003).

Some of the most compelling use cases for these appli-
cations concern sensitive data about individuals (Mirza-
soleiman et al., 2016a;b). As a running example, let us
consider the speciﬁc problem of determining which of a
collection of features (e.g. age, height, weight, etc.) are
most relevant to a binary classiﬁcation task (e.g. predict-
In
ing whether an individual is likely to have diabetes).
this problem, a sensitive training set takes the form D =
{(xi, yi)}n
i=1 where each individual i’s data consists of fea-
tures xi,1, . . . , xi,m together with a class label yi. The goal
is to identify a small subset S ⊆ [m] of features which can
then be used to build a good classiﬁer for y. Many tech-
niques exist for feature selection, including one based on
maximizing a submodular function which captures the mu-
tual information between a subset of features and the class
label of interest (Krause & Guestrin, 2005). However, for
both legal (e.g. compliance with HIPAA regulations) and
ethical reasons, it is important that the selection of relevant
features does not compromise the privacy of any individual
who has contributed to the training data set. Unfortunately,
the theory of submodular maximization does not in itself
accommodate such privacy concerns.

To this end, we propose a systematic study of differentially
private submodular maximization to enable these applica-
tions based on submodular maximization, while provably
guaranteeing individual-level privacy. The notion of differ-
ential privacy (Dwork et al., 2006) offers a strong protec-
tion of individual-level privacy. Nevertheless, differential
privacy has been shown to permit useful data analysis and
machine learning tasks. In a nutshell, the deﬁnition formal-
izes a guarantee that no individual’s data should have too
signiﬁcant an effect on the outcome of a computation. We
provide the formal deﬁnition in Section 2. Such a privacy
guarantee is obtained through the introduction of random
noise, so private submodular maximization is conceptually
related to the problem of submodular maximization in the
presence of noise (Cheraghchi, 2012; Hassidim & Singer,
2016).

In this work, we study the following problem under various
conditions on the submodular objective function f (mono-
tone vs. non-monotone), and various choices of the con-

Differentially Private Submodular Maximization

straint C (cardinality, matroid, or p-extendible system).

Problem 1. Given a sensitive dataset D associated to
a submodular function fD : 2V → R: Find a subset
S ∈ C ⊂ 2V that approximately maximizes fD(S) in a
manner that guarantees differential privacy with respect to
the input dataset D.

(cid:80)n

An important special case of this problem was stud-
ied in prior work of Gupta et al. (2010). They con-
sidered the “combinatorial public projects” problem (Pa-
padimitriou et al., 2008), where given a dataset D =
(x1, . . . , xn), the function fD takes the particular form
fD(S) = 1
i=1 fxi(S) for monotone submodular func-
n
: 2V → [0, 1], and is to be maximized subject
tions fxi
to a cardinality constraint |S| ≤ k. We call functions of
this form decomposable. They presented a simple greedy
algorithm, which will be central to our work, together with
a tailored analysis which achieves strong accuracy guaran-
tees in this special case.

However, there are many cases of Problem 1 which do not
fall into the combinatorial public projects framework. For
some problems, including feature selection via mutual in-
formation, the submodular function fD of interest depends
on the dataset D in ways much more complicated than aver-
aging functions associated to each individual. The focus of
our work is on understanding Problem 1 in circumstances
which capture a broader class of useful applications and
constraints in machine learning. We summarize our spe-
ciﬁc contributions in Section 1.2.

1.1. The greedy paradigm

Even without concern for privacy, the problem of submod-
ular maximization poses computational challenges. In par-
ticular, exact submodular maximization subject to a car-
dinality constraint is NP-hard. One of the principal ap-
proaches to designing efﬁcient approximation algorithms
is to use a greedy strategy (Nemhauser et al., 1978). Con-
sider the problem of maximizing a set function f (S) sub-
ject to the cardinality constraint |S| ≤ k. In each of rounds
i = 1, . . . , k, the basic greedy algorithm constructs Si
from Si−1 by adding the element vi ∈ (V \ Si−1) which
maximizes the marginal gain f (Si−1 ∪ {vi}) − f (Si−1).
Nemhauser et al. (1978) famously showed that this algo-
rithm yields a (1−1/e)-approximation to the optimal value
of f (S) whenever f is a monotone submodular function.

In the combinatorial public projects setting, Gupta et al.
(2010) showed how to make the greedy algorithm compat-
ible with differential privacy by randomizing the procedure
for selecting each vi. This selection procedure is speciﬁed
by the differentially private exponential mechanism of Mc-
Sherry & Talwar (2007), which (probabilistically) guaran-
tees that the vi selected in each round is almost as good as
the true marginal gain maximizer. Remarkably, Gupta et al.
(2010) show that the cumulative privacy guarantee of the

resulting randomized greedy algorithm is not much worse
than that of a single run of the exponential mechanism.
This analysis is highly tailored to the structure of the com-
binatorial public projects problem. However, replacing this
tailored analysis with the more generic “advanced com-
position theorem” for differential privacy (Dwork et al.,
2010), one still obtains useful results for the more general
class of “low-sensitivity” submodular functions.

1.2. Our contributions

Table 1 summarizes the approximation guarantees we ob-
tain for Problem 1 under increasingly more general classes
of submodular functions fD (read top to bottom), and in-
creasingly more general types of constraints (read left to
right). In each entry, OPT denotes the value of the optimal
non-private solution. Below we draw attention to a few par-
ticular contributions, including some that are not expressed
in Table 1.
Non-monotone objective functions. Submodular maxi-
mization for non-monotone functions is signiﬁcantly more
challenging than it is for monotone objectives. In particu-
lar, the basic greedy algorithm of Nemahauser et al. fails,
and cannot guarantee any constant-factor approximation.
Several works (Feldman et al., 2017; Mirzasoleiman et al.,
2016a; Buchbinder et al., 2014; Feldman et al., 2011) have
identiﬁed variations of the greedy algorithm that do yield
constant-factor approximations for non-monotone objec-
tives. However, it is not clear how to modify any of these
algorithms to accommodate differential privacy.

Our starting point is instead the “stochastic greedy” al-
gorithm of Mirzasoleiman et al. (2015), which was orig-
inally designed to perform monotone submodular maxi-
mization in linear time. Drawing ideas from Buchbinder
et al. (2014), we give a new analysis of the stochastic
greedy algorithm to show that it also gives a 1
e (1 − 1/e)-
approximation for non-monotone submodular functions.
To our knowledge, this is the ﬁrst algorithm making ex-
actly |V | function evaluations which achieves a constant-
factor approximation for either monotone or non-monotone
objectives. Moreover, it is immediately clear how to use
the exponential mechanism to make this algorithm differ-
entially private.

This phenomenon is analogous to how stochastic variants
of gradient descent are more naturally suited to provid-
ing differential privacy than their deterministic counter-
parts (Song et al., 2013; Bassily et al., 2014). That is, our
results illustrate how techniques for making algorithms fast
are also helpful in making them privacy-preserving.

General constraints. While a cardinality constraint is
perhaps the most natural to place on a submodular maxi-
mization problem, some machine learning problems, e.g.
personalized data summarization (Mirzasoleiman et al.,
2016a), require the use of more general types of con-

Differentially Private Submodular Maximization

Comb. Pub. Proj.

(cid:0)1 − 1

e

Monotone

Non-monotone

Cardinality
(cid:17)
(cid:16) k log |V |
n

(cid:1) OPT −O
(cid:0)1 − 1
(cid:0)1 − 1

e

1
e

e

(cid:1) OPT −O
(cid:1) OPT −O

(cid:16) k3/2 log |V |
n
(cid:16) k3/2 log |V |
n

(cid:17)

(Gupta et al., 2010)
(cid:17)

Matroid

1
2 OPT −O
1
2 OPT −O

(cid:17)

(cid:16) k log |V |
n
(cid:16) k3/2 log |V |
n

(cid:17)

–

p-Extendible

(cid:17)

(cid:16) k log |V |
n
(cid:16) k3/2 log |V |
n

(cid:17)

1
p+1 OPT −O
1
p+1 OPT −O
–

Table 1. Guarantees of expected solution quality for privately maximizing a sensitivity-(1/n) submodular function fD. The parameter k
represents either a cardinality constraint, or the size of the set returned (for matroid or p-extendible system constraints). Full expressions
with explicit dependencies on differential privacy parameters ε, δ appear in the body of the paper.

straints. For instance, one may wish to maximize a sub-
modular function f (S) subject to S ∈ I for an arbitrary
matroid I, or subject to S being contained in an inter-
section of p matroids (more generally, a p-extendible sys-
tem). For these types of constraints, the greedy algorithm
still yields a constant factor approximation for monotone
objective functions (Fisher et al., 1978; Jenkyns, 1976;
C˘alinescu et al., 2011). We show in this work that the anal-
ysis provided by C˘alinescu et al. (2011) for matroids and
p-extendible families can be adapted to handle additional
error introduced for differential privacy.

General selection procedures. For worst-case datasets,
the exponential mechanism is optimal within each round
of private maximization. However, it may be sub-optimal
for datasets enjoying additional structural properties. For-
tunately, the greedy framework we use is ﬂexible with re-
gard to the choice of the selection procedure. For instance,
one can replace the exponential mechanism in a black-box
manner with the “large margin mechanism” of Chaudhuri
et al. (2014) to obtain error bounds that replace the ex-
plicit dependence on log |V | in Table 1 with a term that
may be signiﬁcantly smaller for real datasets. We give a
slightly simpliﬁed analysis of the large margin mechanism,
and present it in a manner suitable for greedy algorithms
which access the same data set multiple times. (These guar-
antees are more complicated, but spelled out in Section 5.)
For submodular functions exhibiting additional structure,
one may also be able to perform each maximization step
with the “choosing mechanism” of Beimel et al. (2016)
and Bun et al. (2015).

2. Preliminaries

Let V be ﬁnite set which we will refer to as the ground set
and let X be a ﬁnite set which we will refer to as the data
universe. A dataset is an n-tuple D = (x1, . . . , xn) ∈ X n.
Suppose each dataset D is associated to a set function fD :
2V → R. The manner in which fD depends on D will be
application-speciﬁc, but it is assumed that the association
between D and fD is public information.
Deﬁnition 2. A set function fD : 2V → R is submodular
if for all sets S ⊆ T ⊆ V and every element v ∈ V , we
have fD(S ∪ {v}) − fD(S) ≥ fD(T ∪ {v}) − fD(T ).
Moreover, If fD(S) ≤ fD(T ) whenever S ⊆ T , we say
fD is monotone. If for every dataset D = (x1, . . . , xn),
the function fD = 1
i=1 fxi for monotone submodular
n

(cid:80)n

functions fxi : 2V → [0, λ], we say fD is λ-decomposable.
The problem of maximizing a decomposable submodu-
lar function was considered as the “combinatorial public
projects problem” by Papadimitriou et al. (2008).

We are interested in the problem of approximately max-
imizing a submodular function subject to differential pri-
vacy. The deﬁnition of differential privacy relies on the
notion of neighboring datasets, which are simply tuples
D, D(cid:48) ∈ X n that differ in at most one entry. If D, D(cid:48) are
neighboring, we write D ∼ D(cid:48).
Deﬁnition 3. A randomized algorithm M : X n → R sat-
isﬁes (ε, δ)-differential privacy if for all measurable sets
T ⊆ R and all neighboring datasets D ∼ D(cid:48),

Pr[M (D) ∈ T ] ≤ eε Pr[M (D(cid:48)) ∈ T ] + δ.

Differentially private algorithms must be calibrated to the
sensitivity of the function of interest with respect to small
changes in the input dataset, deﬁned formally as follows.
Deﬁnition 4. The sensitivity of a set function fD : 2V →
R (depending on a dataset D) with respect to a constraint
C ⊆ 2V is deﬁned as
max
D∼D(cid:48)

|fD(S) − fD(cid:48)(S)|.

max
S∈C

Composition of Differential Privacy. The analyses of
our algorithms rely crucially on composition theorems for
differential privacy. For a sequence of privacy parame-
ters {(εi, δi)}k
i=1, we informally refer to the k-fold adap-
tive composition of (εi, δi)-differentially private algorithms
as the output of a mechanism M ∗ that behaves as fol-
lows on an input D:
In each of rounds i = 1, . . . , k,
the algorithm M ∗ selects an (εi, δi)-differentially private
algorithm Mi possibly depending on the previous out-
comes M1(D), . . . , Mi(D) (but not directly on the sensi-
tive dataset D itself), and releases Mi(D). For a formal
treatment of adaptive composition, see (Dwork et al., 2010;
Dwork & Roth, 2014).

Theorem 5. (Dwork & Lei, 2009; Dwork et al., 2010;
Bun & Steinke, 2016) The k-fold adaptive composition
of (ε0, δ0)-differentially private algorithms satisﬁes (ε, δ)-
differential privacy where

1. ε = kε0 and δ = kδ0.

(Basic Composition).

2. ε = 1
δ(cid:48) > 0.

2 kε2

0 + (cid:112)2 log(1/δ(cid:48))ε0 and δ = δ(cid:48) + kδ, for any

(Advanced Composition)

Differentially Private Submodular Maximization

Exponential Mechanism. The
exponential mecha-
nism (McSherry & Talwar, 2007) is a general prim-
itive for solving discrete optimization problems.
Let
q : V × X n → R be a “quality” function measuring
how good a solution v ∈ V is with respect to a dataset
D ∈ X n. We say a quality function q has sensitivity λ if
for all v ∈ V and all neighboring datasets D ∼ D(cid:48), we
have |q(v, D) − q(v, D(cid:48))| ≤ λ.
Proposition 6. Let ε > 0 and let q : V × X n be a quality
function with sensitivity λ. Deﬁne the exponential mech-
anism as the algorithm which selects every v ∈ V with
probability proportional to exp(εq(v, D)/2λ).

• The

exponential mechanism provides

(ε, 0)-

differential privacy.
• For every D ∈ X n,

E[q(ˆv, D)] ≥ max
v∈V

q(v, D) −

2λ · ln |V |
ε

,

where ˆv is the output of the exponential mechanism on
dataset D.

The privacy guarantee and a “with high probability” utility
guarantee of the exponential mechanism are due to McSh-
erry & Talwar (2007). A simple proof of the utility guaran-
tee in expectation appears in (Bassily et al., 2016).
3. Monotone Submodular Maximization
In this section, we present a variant of the basic greedy al-
gorithm which will enable maximization of monotone sub-
modular functions. This algorithm simply replaces each
greedy selection step with a privacy-preserving selection
algorithm denoted O. The selection function O takes as
input a quality function q : U × X n → R and a dataset
D, as well as privacy parameters ε0, δ0, and outputs an el-
ement u ∈ U . We begin in the simplest case of monotone
submodular maximization with a cardinality constraint (Al-
gorithm 1). The algorithm for more general constraints ap-
pears in Section 3.1.

Algorithm 1 was already studied by Gupta et al. (2010) in
the special case where fD is decomposable, and O is the
exponential mechanism. We generalize their result to the
much broader class of low-sensitivity monotone submodu-
lar functions.

Algorithm 1 Diff. Private Greedy (Cardinality) GO
Input: Submodular function fD : 2V → R, dataset D,
cardinality constraint k, privacy parameters ε0, δ0
Output: Size k subset of V

1. Initialize S0 = ∅
2. For i = 1, . . . , k:

• Deﬁne qi : (V \Si−1)×X n → R via qi(v, ˜D) =

f ˜D(Si−1 ∪ {v}) − f ˜D(Si−1)
• Compute vi ←R O(qi, D; ε0, δ0)
• Update Si ← (Si−1 ∪ {vi})

Theorem 7. (Gupta et al., 2010) Suppose fD : 2V → R
is λ-decomposable (cf. Deﬁnition 2). Let δ > 0 and let
ε0 ≥ 0 be such that ε = 2 · ε0 · (e − 1) ln(3e/δ) ≤ 1. Then
instantiating Algorithm 1 with O = EM and parameter
ε0 > 0 provides (ε, δ)-differential privacy.

Moreover, for every D ∈ X n,
1
e

E [fD(Sk)] ≥

1 −

(cid:18)

(cid:19)

OPT −

2λk ln |V |
ε0

where Sk ←R GEM(D).
Unfortunately, the privacy analysis of Theorem 7 makes es-
sential use of the decomposability of fD, and does not di-
rectly generalize to arbitrary submodular functions of low-
sensitivity. Replacing the privacy analysis of (Gupta et al.,
2010) with the Composition Theorem 5 instead gives
Theorem 8. Suppose fD : 2V → R is monotone and has
sensitivity λ. Then instantiating Algorithm 1 with O = EM
and parameter ε0 > 0 provides (ε = kε0, δ = 0)-
differential privacy. It also provides (ε, δ)-differential pri-
0/2 + ε0 · (cid:112)2k ln(1/δ).
vacy for every δ > 0 with ε = kε2

Moreover, for every D ∈ X n,
1
e

E [fD(Sk)] ≥

1 −

(cid:18)

(cid:19)

where Sk ←R GEM(D).

OPT −

2λk ln |V |
ε0

3.1. Matroid and p-Extendible System Constraints
We now show how to extend Algorithm 1 to privately maxi-
mize monotone submodular functions subject to more gen-
eral constraints. To start, we review the deﬁnition of a p-
extendible system. Consider a ground set V and a non-
empty downward-closed family of subsets I ⊆ 2V (i.e. if
T ∈ I, then S ∈ I for every S ⊆ T ). Such an I is called a
family of independent sets. The pair (V, I) is said to be a p-
extendible system (Mestre, 2006) if for all S ⊂ T ∈ I, and
v ∈ V such that S ∪{v} ∈ I, there exists a set Z ⊆ (T \S)
such that |Z| ≤ p and (T \ Z) ∪ {v} ∈ I. Let r(I) denote
the size of the largest independent set in I.

The deﬁnition of a matroid coincides with that of a 1-
extendible system (with rank r(I)). For p ≥ 2, the no-
tion of a p-extendible system strictly generalizes that of an
intersection of p matroids. A slight modiﬁcation of Algo-
rithm 1 gives a uniﬁed algorithm for privately maximizing
a monotone submodular function subject to matroid and p-
extendible system constraints, presented as Algorithm 2.
Theorem 9. Suppose fD : 2V → R is λ-decomposable
(cf. Deﬁnition 2). Let δ > 0 and let ε0 ≥ 0 be such that
ε = 2 · ε0 · (e − 1) ln(3e/δ) ≤ 1. Then instantiating Al-
gorithm 2 with O = EM and parameter ε0 > 0 provides
(ε, δ)-differential privacy. Moreover, for every D ∈ X n,
(cid:19)

E [fD(S)] ≥

· OPT −

1
p + 1

p
p + 1

(cid:18) 2λr(I) ln |V |
ε0

3. Return Sk

where S ←R GEM(D).

Differentially Private Submodular Maximization

Algorithm 2 Differentially Private Greedy (p-system) GO
Input: Submodular function fD : 2V → R, dataset D, p-
extendible family (V, I), privacy parameters ε0, δ0
Output: Maximal independent subset of V

1. Initialize S = ∅
2. While S ∈ I is not maximal:

• Deﬁne q : (V \ S) × X n → R via q(v, ˜D) =

f ˜D(S ∪ {v}) − f ˜D(S)

• Compute vi ←R O(q, D; ε0, δ0)
• Update S ← (S ∪ {vi})

3. Return S

Theorem 10. Suppose fD : 2V → R has sensitivity λ.
Then instantiating Algorithm 2 with O = EM and param-
eter ε0 > 0 provides (ε = r(I)ε0, δ = 0)-differential pri-
vacy. It also provides (ε, δ)-differential privacy for every
δ > 0 with ε = r(I)ε2/2 + ε · (cid:112)2r(I) ln(1/δ).
Moreover, for every D ∈ X n,

E [fD(S)] ≥

· OPT −

1
p + 1

p
p + 1

(cid:18) 2λr(I) ln |V |
ε0

(cid:19)

where S ←R GEM(D).

4. Non-Monotone Submodular Maximization

We now consider the problem of privately maximizing an
arbitrary, possibly non-monotone, submodular function un-
der a cardinality constraint. In general, the greedy algo-
rithm presented in Section 3 fails to give any constant-
factor approximation. Instead, our algorithm in this sec-
tion will be based on the “stochastic greedy” algorithm ﬁrst
studied by Mirzasoleiman et al. (2015).
In each round,
the stochastic greedy algorithm ﬁrst subsamples a random
1
k · ln(1/α) fraction of the ground set for some α > 0,
and then greedily selects the item from this subsample that
maximizes marginal gain. Mirzasoleiman et al. (2015)
showed that for a monotone objective function f , this algo-
rithm provides a (1 − 1/e − α)-approximation to the opti-
mal solution. Their original motivation was to improve the
running time of the greedy algorithm: from O(|V |·k) eval-
uations of the objective function to linear O(|V | · ln(1/α)).

Unfortunately, the stochastic greedy algorithm does not
provide any approximation guarantee for non-monotone
submodular functions. Buchbinder et al. (2014) instead
proposed a “random greedy” algorithm that, in each itera-
tion, randomly selects one of the k elements with the high-
est marginal gain. Buchbinder et al. (2014) showed that the
random greedy algorithm achieves a 1/e approximation to
the optimal solution (in expectation), using k|V | function
evaluations. However, it is not clear how to adapt this algo-
rithm to accommodate differential privacy, since its analy-
sis has a brittle dependence on the sampling procedure.

We make two main contributions to the analysis of the

stochastic greedy and random greedy algorithms. First,
we show that running the stochastic greedy algorithm on
an exact 1
k fraction of the ground set per iteration still
gives a (0.468)-approximation for monotone objectives,
and moreover, gives a 1
e (1 − 1/e)-approximation even for
non-monotone objectives. Note that this algorithm evalu-
ates the objective function on only |V | elements, and still
provides a constant factor approximation guarantee. This
makes our “subsample-greedy” algorithm the fastest algo-
rithm for maximizing a general submodular function sub-
ject to a cardinality constraint (albeit with slightly worse
approximation guarantees). Second, we show that the guar-
antees of this algorithm are robust to using a randomized
the exponential or large
greedy selection procedure (e.g.
margin mechanism), and hence it can be adapted to ensure
differential privacy.

We present the subsample-greedy algorithm as Algorithm 3
below. Assume that V is augmented by enough “dummy
elements” to ensure that |V |/k is an integer; each dummy
element u is deﬁned so that fD(S ∪ {u}) = fD(S) for
every set S. We also explicitly account for an additional
set U of k dummy elements, and ensure that at least one
appears in every subsample.

Algorithm 3 Diff. Private “Subsample-Greedy” SGO
Input: Submodular function fD : 2V → R, dataset D,
cardinality constraint k, privacy parameters ε0, δ0
Output: Size k subset of V

1. Initialize S0 = ∅, dummy elements U =

{u1, . . . , uk}
2. For i = 1, . . . , k:

• Sample Vi ⊂ V a uniformly random subset of
size |V |/k and ui a random dummy element
• Deﬁne qi : (Vi∪{ui})×X n → R via qi(v, ˜D) =

f ˜D(Si−1 ∪ {v}) − f ˜D(Si−1)
• Compute vi ←R O(qi, D; ε0, δ0)
• Update Si ← (Si−1 ∪ {vi})

3. Return Sk with all dummy elements removed

Theorem 11. Suppose fD : 2V → R has sensitivity λ.
Then instantiating Algorithm 3 with O = EM provides
(ε, δ)-differential privacy, and for every D ∈ X n,
(cid:18)

1
e
where S ←R SGEM(D). Moreover, if fD is monotone, then

2λk ln |V |
ε

E [fD(S)] ≥

OPT −

1 −

1
e

(cid:19)

E [fD(S)] ≥

1 − e−(1−1/e)(cid:17)
(cid:16)

OPT −

2λk ln |V |
ε

≈ 0.468 OPT −

2λk ln |V |
ε

.

The guarantees of Theorem 11 are of interest even with-
out privacy. Letting MAX denote the selection procedure
which simply outputs the true maximizer (equivalently,

Differentially Private Submodular Maximization

which runs the exponential mechanism with ε0 = +∞),
we obtain the following non-private algorithm for maxi-
mizing a submodular function fD:
Corollary 12. Let fD : 2V → R be any submodular func-
tion. Instantiating Algorithm 3 with O = MAX gives

(cid:18)

(cid:19)

E [fD(S)] ≥

1
e
where S ←R SGMAX(D). Moreover, if fD is monotone,
then

OPT

1 −

1
e

E [fD(S)] ≥

(cid:16)

1 − e−(1−1/e)(cid:17)

OPT ≈ 0.468 OPT .

5. The Large Margin Mechanism

The accuracy guarantee of the exponential mechanism can
be pessimistic on datasets where q(·, D) exhibits addi-
tional structure. For example, suppose that when the el-
ements of V are sorted so that q(v1, D) ≥ q(v2, D) ≥
· · · ≥ q(v|V |, D), there exists an (cid:96) such that q(v1, D) (cid:29)
q(v(cid:96)+1, D). Then only the top (cid:96) ground set items are rel-
evant to the optimization problem, so running the expo-
nential mechanism on these should maintain differential
privacy, but with error proportional to ln (cid:96) rather than to
ln |V |. The large margin mechanism of Chaudhuri et al.
(2014), like the exponential mechanism, generically solves
discrete optimization problems. However, it automatically
leverages this additional margin structure whenever it ex-
ists. Asymptotically, the error guarantee of the large margin
mechanism is always at most that of the exponential mech-
anism, but can be much smaller when the data exhibits a
margin for small (cid:96).
Formally, given a quality function q : V × X n → R and
parameters (cid:96) ∈ N, γ > 0, a dataset D satisﬁes the ((cid:96), γ)-
margin condition if q(v(cid:96)+1, D) < q(v1, D) − γ.

For each (cid:96) = 1, . . . , |V |, deﬁne

(cid:18)

g(cid:96) = λ ·

3 +

G(cid:96) =

8λ ln(2/δ)
ε

(cid:19)

4 ln(2(cid:96)/δ)
ε
16λ ln(7(cid:96)2/δ)
ε

+

+ g(cid:96).

Recall that the Laplace distribution Lap(b) is speciﬁed by
the density function 1
2b exp(−|x|/b).
Replacing the exponential mechanism with the large mar-
gin mechanism gives analogues of our results for monotone
submodular maximization with a cardinality constraint,
monotone submodular maximization over a p-extendible
system, and non-monotone submodular maximization with
a cardinality constraint:
Theorem 13. Suppose fD : 2V → R is monotone
and has sensitivity λ. Then instantiating Algorithm 1
with O = LMM and parameters ε0, δ0 = 0 provides
It also provides (ε, δ(cid:48) +
(kε0, kδ0)-differential privacy.

(cid:19)

1 −

E [fD(Sk)|E] ≥

kδ0)-differential privacy for every δ(cid:48) > 0 with ε = kε2/2+
ε · (cid:112)2k ln(1/δ(cid:48)).
Moreover, for every D ∈ X n, there exists an event E with
Pr[E] ≥ 1 − β such that
(cid:18)

1
e
where Sk ←R GLMM(D), and D satisﬁes the ((cid:96)i, γi)-
margin condition with respect to every function of the form
qi(v, D) = fD( ˆSi−1 ∪ {v}) − fD( ˆSi−1), with γi =
24λ ln(k/β)/ε + G(cid:96)i.
Theorem 14. Instantiating Algorithm 2 with O = LMM
under all of the conditions of Theorem 13 gives the same
privacy guarantee (replacing k with r(I)) and gives

4λ ln (cid:96)i
ε0

OPT −

k
(cid:88)

i=1

E [fD(S)|E] ≥

· OPT −

1
p + 1

r(I)
(cid:88)

i=1

4λ ln (cid:96)i
ε0

.

Theorem 15. Instantiating Algorithm 3 with O = LMM
under all of the conditions of Theorem 13 gives the same
privacy guarantee and gives

E [fD(Sk)|E] ≥

1 −

OPT −

(cid:18)

1
e

(cid:19)

1
e

k
(cid:88)

i=1

4λ ln (cid:96)i
ε0

.

Moreover, if fD is monotone, then

E [fD(Sk)|E] ≥ 0.468 OPT −

k
(cid:88)

i=1

4λ ln (cid:96)i
ε0

.

6. Experimental Results

In this section we describe two concrete applications of our
mechanisms.

6.1. Location Privacy
We analyze a dataset of 10,000 Uber pickups in Manhattan
in April 2014 (UberDataset). Each individual entry in the
dataset consists of the longitude and latitude coordinates of
the pickup location. We want to use this dataset to select
k public locations as waiting spots for idle Uber drivers,
while also guaranteeing differential privacy for the passen-
gers whose locations appear in this dataset.1 We consider
two different public sets of locations L:

• LP opular is a set of 33 popular locations in Manhattan.
• LGrid is a set of 33 locations spread evenly across

Manhattan in a grid-like manner.

We deﬁne a utility function M (i, j) to be the normalized
Manhattan distance between a pickup location i and the
waiting location j. That is, if pickup location i is located at
coordinates (i1, i2) and the waiting location j is located at
|i1 − j1| + |i2 − j2|
m

coordinates (j1, j2), then M (i, j) =
,
where m = 0.266 is simply the Manhattan distance be-
tween the two furthest spread apart points in Manhattan.

1Under the assumption that each pickup corresponds to a

unique individual.

Differentially Private Submodular Maximization

(a) LP opular

(b) LGrid

(c) LP opular

(d) LGrid

(e) LP opular:
greedy

Non-private

(f) LP opular: EM-based greedy

(g) LGrid: Non-
private greedy

(h) LGrid: EM-
based greedy

Figure 1. (a) and (b) show utility for various cardinalities (k). (c) and (d) ﬁx k = 3 and show utility for various privacy parameters ((cid:15)).
Utility is normalized to be between 0 and 1. (e) - (h) shows a representative top 3 set under various settings.

This normalization ensures that 0 ≤ M (i, j) ≤ 1, for all
i, j. In order to make sure we have a maximization prob-
lem, we deﬁne the following objective function: fD(S) =
n −
M (i, j), where n = |D| = 10000.

(cid:88)

min
j∈S

i∈D

Observation 16. The function fD is λ-decomposable for
λ = 1 (and hence has sensitivity 1).

This form of objective function is known to be monotone
submodular and so we can use the greedy algorithms stud-
ied in this paper. We use (cid:15) = 0.1 and δ = 2−20. For our set-
tings of parameters, “basic composition” outperforms “ad-
vanced composition,” so the privacy budget of (cid:15) = 0.1 is
split equally across the k iterations, meaning the mecha-
nism at each iteration uses (cid:15)0 = (cid:15)
k . Our ﬁgures plot the
average utility across 100 simulations.

From Figures 1(a) and (b) we see that the results for both
LP opular and LGrid are relatively similar and unsurpris-
ing. The non-private greedy algorithm achieves the high-
est utility, but both the exponential mechanism (EM)-based
greedy and large margin mechanism (LMM)-based greedy
algorithms exhibit comparable utility while preserving a
Interestingly, we also see that the
high level of privacy.
utilities of the EM-based and LMM-based algorithms are
almost identical for both LP opular and LGrid. This indi-
cates that our mechanisms are actually selecting good loca-
tions, rather than just getting lucky because there are a lot
of good locations to choose from.

Figures 1(c) and (d) show how the utility of the EM-based

and LMM-based algorithms vary with the privacy parame-
ter (cid:15). We can also think of this as varying the dataset size
for a ﬁxed (cid:15). We ﬁx k = 3 and take the average of 100
simulations for each value of (cid:15). We see that even for very
small (cid:15), our algorithms outperform fully random selection.
As (cid:15) increases, so does the utility. It is not shown in this
ﬁgure, but varying δ has very little effect.

From Figures 1(e) - (h), we see that the both the non-private
and private algorithms select public locations that are rel-
atively close to each other. For example, for the LP opular
set of locations, the Empire State Building is close to the
New York Public Library, the Soho Grand Hotel is close to
NYU, and the Grand Army Plaza is close to the UN Head-
quarters. As a result, the private mechanisms manage to
achieve comparable utility, while also masking the users’
exact locations.

The theory described in Section 5 suggests that, at least
asymptotically, the large margin mechanism-based algo-
rithm should outperform the exponential mechanism-based
algorithm. However, in our experiments, we ﬁnd that the
large margin mechanism is generally only able to ﬁnd a
margin in the ﬁrst iteration of the greedy algorithm. This is
because the threshold for ﬁnding a margin depends only on
(cid:15), δ, and n and thus it stays the same across all k iterations.
On the other hand, the marginal gain at each iteration drops
very quickly, so the mechanism fails to ﬁnd a margin and
thus samples from all remaining locations. However, since
the large margin mechanism spends half of its privacy bud-

Differentially Private Submodular Maximization

(a) Graphical model of Naive Bayes

(b) Expected mutual information

(c) Representative top 3 features

Figure 2. Privately selecting health features, from national health examination surveys, that correlate most with diabetes.

get to try to ﬁnd a margin, the sampling step gives slightly
worse guarantees than does the plain exponential mecha-
nism, thus giving us the slightly weaker results we see in
the ﬁgures.

6.2. Feature Selection Privacy

We analyze a dataset created from a combination of Na-
tional Health Examination Surveys ranging from 2007 to
2014 (NHANESDataset). There are n = 23, 876 individ-
uals in the dataset with information on whether or not they
have diabetes, along with m = 23 other potentially related
binary health features. Our goal is to privately select k of
these features that provide as much information about the
diabetes class variable as possible.

More speciﬁcally, our goal is to maximize the mutual in-
formation between Y and XS, where Y is a binary random
variable indicating whether or not an individual has dia-
betes and XS is a random variable that represents a set S
of k binary health features. Mutual information takes the
form:

I(Y ; X) =

p(x, y) log2

(cid:88)

(cid:88)

y∈Y

x∈X

(cid:18) p(x, y)
p(x)p(y)

(cid:19)

.

Under the Naive Bayes assumption, we suppose the
joint distribution on (Y, X1, . . . , Xk) takes the form

k
(cid:89)

i=1

p(y, x1, . . . , xk) = p(y)

p(xi | y). Therefore, we can

easily specify the entire probability distribution by ﬁnding
each p(xi | y). We estimate each p(xi | y) by counting
frequencies in the dataset.

Our goal is to choose a size k subset S of the features in
order to maximize fD(S) = I(Y ; XS). Mutual informa-
tion (under the Naive Bayes assumption) for feature selec-
tion is known to be monotone submodular in S (Krause &
Guestrin, 2005), and thus we can apply the greedy algo-
rithms described in this paper.
Claim 17. In iteration i of the greedy algorithm, the sensi-
tivity of fD(S) is (2i+1) log2(n)
We run 1,000 simulations with (cid:15) = 1.0 and δ = 2−20.
As we can see from Figure 2(b), our private mechanisms

n

.

maintain a comparable utility relative to the non-private
algorithm. We also observe an interesting phenomenon
where the expected utility obtained by our mechanism is
not necessarily monotonically increasing with the number
of features selected. This is an artifact of the fact that if
we are selecting k features, then composition requires us to
divide (cid:15) so that each iteration uses privacy budget (cid:15)
k . This
is problematic for this particular application because there
happens to be one feature (insulin administration) that has
much higher value than the rest. Therefore, the reduced
probability of picking this single best feature (as a result of
the lower privacy parameter (cid:15)
k ) is not compensated for by
selecting more features.

From Figure 2(c), we see that both the private and non-
private mechanisms generally select insulin administration
as the top feature. However, while all three of the top fea-
tures selected by the non-private algorithm are clearly re-
lated to diabetes, the non-private mechanisms tend to select
one feature (in our case, gender or having received a blood
transfusion) that may not be quite as relevant.

7. Conclusion
We have presented a general framework for maximizing
submodular functions while guaranteeing differential pri-
vacy. Our results demonstrate that simple and ﬂexible
greedy algorithms can preserve privacy while achieving
competitive guarantees for a variety of submodular maxi-
mization problems: for all functions under cardinality con-
straints, as well as for monotone functions under matroid
and p-extendible system constraints. Via our motivation to
identify algorithms that could be made differentially pri-
vate, we discovered a non-monotone submodular maxi-
mization algorithm that achieves guarantees that are novel
even without concern for privacy. Finally, our experiments
show that our algorithms are indeed competitive with their
non-private counterparts.

Acknowledgments. This work was supported by DARPA
Young Faculty Award (D16AP00046), Simons-Berkeley
fellowship, and ERC StG 307036. This work was done in
part while Amin Karbasi and Andreas Krause were visiting
the Simons Institute for the Theory of Computing.

DiabetesAsthmaVigorous ExerciseTaking InsulinHigh Blood PressureInsulinInsulinInsulinOverweightOverweightGenderHigh Blood PressureBlood TransfusionVigorous ExerciseNon-Private GreedyExponential MechanismLarge Margin MechanismDifferentially Private Submodular Maximization

References

Bassily, Raef, Smith, Adam D., and Thakurta, Abhradeep.
Private empirical risk minimization: Efﬁcient algorithms
and tight error bounds. In FOCS, pp. 464–473, 2014.

Bassily, Raef, Nissim, Kobbi, Smith, Adam D., Steinke,
Thomas, Stemmer, Uri, and Ullman, Jonathan. Algo-
rithmic stability for adaptive data analysis. In STOC, pp.
1046–1059, 2016.

Beimel, Amos, Nissim, Kobbi, and Stemmer, Uri. Private
learning and sanitization: Pure vs. approximate differen-
tial privacy. Theory of Computing, 12(1):1–61, 2016.

Buchbinder, Niv, Feldman, Moran, Naor, Joseph, and
Schwartz, Roy. Submodular maximization with cardi-
nality constraints. In SODA, pp. 1433–1452, 2014.

Bun, Mark and Steinke, Thomas. Concentrated differential
privacy: Simpliﬁcations, extensions, and lower bounds.
In TCC, pp. 635–658, 2016.

Bun, Mark, Nissim, Kobbi, Stemmer, Uri, and Vadhan,
Salil P. Differentially private release and learning of
threshold functions. In FOCS, pp. 634–649, 2015.

C˘alinescu, Gruia, Chekuri, Chandra, P´al, Martin, and
Vondr´ak, Jan. Maximizing a monotone submodular
function subject to a matroid constraint. SIAM Journal
on Computing, 2011.

Chaudhuri, Kamalika, Hsu, Daniel J., and Song, Shuang.
The large margin mechanism for differentially private
maximization. In NIPS, pp. 1287–1295, 2014.

Cheraghchi, Mahdi, et al. Submodular functions are noise

stable. In SODA, 2012.

Dwork, Cynthia and Roth, Aaron. The algorithmic founda-
tions of differential privacy. Foundations and Trends in
Theoretical Computer Science, 9(3-4):211–407, 2014.

Dwork, Cynthia, McSherry, Frank, Nissim, Kobbi, and
Smith, Adam D. Calibrating noise to sensitivity in pri-
vate data analysis. In TCC, pp. 265–284, 2006.

Dwork, Cynthia, Rothblum, Guy N., and Vadhan, Salil P.
Boosting and differential privacy. In FOCS, pp. 51–60,
2010.

Feldman, Moran, Naor, Joseph, and Schwartz, Roy. A uni-
ﬁed continuous greedy algorithm for submodular maxi-
mization. In FOCS, 2011.

Feldman, Moran, Harshaw, Christopher, and Karbasi,
Amin. Greed is good: Near-optimal submodular max-
imization via greedy optimization. In COLT, 2017.

Fisher, Marshall L., Nemhauser, George L., and Wolsey,
Laurence A. An analysis of approximations for maxi-
mizing submodular set functions - II. Mathematical Pro-
gramming Study, (8), 1978.

Gupta, Anupam, Ligett, Katrina, McSherry, Frank, Roth,
Aaron, and Talwar, Kunal. Differentially private combi-
natorial optimization. In SODA, pp. 1106–1125, 2010.

Hassidim, Avinatan and Singer, Yaron. Submodular op-
timization under noise. CoRR, abs/1601.03095, 2016.
URL http://arxiv.org/abs/1601.03095.

Jenkyns, T. A. The efﬁcacy of the “greedy” algorithm.
In South Eastern Conference on Combinatorics, Graph
Theory and Computing, 1976.

Kempe, David, Kleinberg, Jon, and Tardos, ´Eva. Maximiz-
ing the spread of inﬂuence through a social network. In
KDD, 2003.

Kirchhoff, Katrin and Bilmes, Jeff. Submodularity for data
selection in statistical machine translation. In EMNLP,
2014.

Krause, A. and Guestrin, C. Near-optimal nonmyopic value

of information in graphical models. In UAI, 2005.

Krause, Andreas and Gomes, Ryan G. Budgeted nonpara-

metric learning from data streams. In ICML, 2010.

Lin, Hui and Bilmes, Jeff. A class of submodular functions

for document summarization. In ACL, 2011.

McSherry, Frank and Talwar, Kunal. Mechanism design
via differential privacy. In FOCS, pp. 94–103, 2007.

Mestre, Juli´an. Greedy in approximation algorithms.

In

Mirzasoleiman, Baharan, Badanidiyuru, Ashwinkumar,
Karbasi, Amin, Vondrak, Jan, and Krause, Andreas.
Lazier than lazy greedy. In AAAI, 2015.

Mirzasoleiman, Baharan, Badanidiyuru, Ashwinkumar,
and Karbasi, Amin. Fast constrained submodular max-
imization: Personalized data summarization. In ICML,
2016a.

Mirzasoleiman, Baharan, Zadimoghaddam, Morteza, and
Fast distributed submodular cover:

Karbasi, Amin.
Public-private data summarization. In NIPS, 2016b.

Nemhauser, George L., Wolsey, Laurence A., and Fisher,
Marshall L. An analysis of approximations for maxi-
mizing submodular set functions - I. Mathematical Pro-
gramming, 1978.

Dwork, Cynthia and Lei, Jing. Differential privacy and ro-

ESA, pp. 528–539, 2006.

bust statistics. In STOC, pp. 371–380, 2009.

Differentially Private Submodular Maximization

NHANESDataset. National health and nutrition exami-
nation survey (2007 - 2014). URL https://wwwn.
cdc.gov/nchs/nhanes/default.aspx.

Papadimitriou, Christos H., Schapira, Michael, and Singer,
Yaron. On the hardness of being truthful. In FOCS, pp.
250–259, 2008.

Singla, Adish, Bogunovic, Ilija, Bart´ok, G´abor, Karbasi,
Amin, and Krause, Andreas. Near-optimally teaching
the crowd to classify. In ICML, 2014.

Sipos, Ruben, Swaminathan, Adith, Shivaswamy, Pannaga,
and Joachims, Thorsten. Temporal corpus summariza-
tion using submodular word coverage. In CIKM, 2012.

Song, Shuang, Chaudhuri, Kamalika,

and Sarwate,
Anand D. Stochastic gradient descent with differentially
private updates. In GlobalSIP, pp. 245–248, 2013.

UberDataset.

Uber

city.

york
kaggle.com/fivethirtyeight/
uber-pickups-in-new-york-city.

URL

pickups

in
new
https://www.

