Coordinated Multi-Agent Imitation Learning

A. Variational Inference Derivation for

Hidden Markov Models

In this section, we provide the mathematical derivation for
the structured variational inference procedure. We focus
on the training for Bayesian Hidden Markov Model, in par-
ticular the Forward-Backward procedure to complete the
description of Algorithm 3. The mathematical details for
other types of graphical models depend on the family of
such models and should follow similar derivations. Further
relevant details on stochastic variational inference can be
found in (Hoffman et al., 2013; Johnson & Willsky, 2014;
Beal, 2003). d

Settings. Given an arbitrarily ordered set of trajecto-
ries U “ tU1, . . . , UK, Cu, let the coordination mecha-
nism underlying each such U be governed by a true un-
known model p, with global parameters θ. We suppress
the agent/policy subscript and consider a generic featurized
trajectory xt “ rut, cts @t. Let the latent role sequence for
the same agent be z “ z1:T .

At any time t, each agent is acting according to a latent role
zt „ Categoricalt¯1, ¯2, . . . , ¯Ku, which are the local
parameters to the structured model.

Ideally, role and index asignment can be obtained by cal-
culating the posterior ppz|x, θq, which is often intractable.
One way to infer the role assignment is via approximat-
ing the intractable posterior ppz|x, θq using Bayesian infer-
ence, typically via MCMC or mean-ﬁeld variational meth-
ods. Since sampling-based MCMC methods are often slow,
we instead aim to learn to approximate ppz|x, θq by a sim-
pler distribution q via Bayesian inference.
In particular,
we employ techniques from stochastic variational inference
(Hoffman et al., 2013), which allows for efﬁcient stochastic
training on mini-batches that can naturally integrate with
our imitation learning subroutine.

Structured Variational Inference for Unsupervised Role
Learning. Consider a full probabilistic model:

For unsupervised structured prediction problem over a fam-
ily of graphical model, we focus on the structured mean-
ﬁeld variational family, which factorizes q as qpz, θq “
qpzqqpθq (Hoffman & Blei, 2014) and decomposes the
ELBO objective:

L “ Eqrlog ppθs ´ Eqrlog qpθs

` Eqrlogpppz, x|θqs ´ Eqrlogpqpzqqs.

(3)

This factorization breaks the dependency between θ and z,
but not between single latent states zt, unlike variational
inference for i.i.d data (Kingma & Welling, 2013).

Variational inference optimizes the objective L typically
using natural gradient ascent over global factors qpθq and
local factors qpzq.
(Under mean-ﬁeld assumption, opti-
mization typically proceeds via alternating updates of θ
and z.) Stochastic variational inference performs such up-
dates efﬁciently in mini-batches. For graphical models,
structured stochastic variational inference optimizes L us-
ing natural gradient ascent over global factors qpθq and
message-passing scheme over local factors qpzq. We as-
sume the prior ppθq and complete conditionals ppzt, xt|θq
are conjugate pairs of exponential family, which gives nat-
ural gradient of L with respect to qpθq convenient forms
(Johnson & Willsky, 2014). Denote the exponential family
forms of ppθq and ppzt, yt|θq by:

ln ppθq “ xηθ, tθpθqy ´ Aθpηθq

ln ppzt, xt|θq “ xηzxpθq, tzxpzt, xtqy ´ Azxpηzxpθqq

where ηθ and ηzx are functions indicating natural param-
eters, tθ and tzx are sufﬁcient statistics and Ap¨q are log-
normalizers ((Blei et al., 2017)). Note that in general, dif-
ferent subscripts corresponding to η, t, A indicate different
function parameterization (not simply a change in variable
value assignment). Conjugacy in the exponential family
yields that (Blei et al., 2017):

tθpθq “ rηzxpθq, ´Azxpηzxpθqqs

ppθ, z, xq “ ppθq

ppzt|θqppxt|zt, θq

and that

Tź

t“1

with global latent variables θ, local latent variables z “
tztuT
t“1. Posterior approximation is often cast as optimiz-
ing over a simpler model class Q, via searching for global
parameters θ and local latent variables z that maximize the
evidence lower bound (ELBO) L:

ppθ|zt, xtq9 exptxηθ ` rtzxpzt, xtq, 1s , tθpθqyu

(4)

Conjugacy in the exponential family also implies that the
optimal qpθq is in the same family (Blei et al., 2017), i.e.

qpθq “ exptxrηθ, tθpθqy ´ Aθprηθqu

log ppxq ě Eq rlog ppz, θ, xqs ´ Eq rlog qpz, θqs

for some natural parameters rηθ of qpθq.

ﬁ L pqpz, θqq .

Maximizing L is equivalent to ﬁnding q P Q to minimize
the KL divergence KL pqpz, θ|xq||ppz, θ|xqq.

To optimize over global parameters qpθq, conjugacy in
the exponential family allows obtaining convenient expres-
sion for the gradient of L with respect to natural param-
eters rηθ. The derivation is shown similarly to (Johnson

Coordinated Multi-Agent Imitation Learning

& Willsky, 2014) and (Blei et al., 2017) - we use simpli-
ﬁed notations rη ﬁ rηθ, η ﬁ ηθ, A ﬁ Aθ, and tpz, xq ﬁ
ř
T
t“1 rtzxpzt, xtq, 1s. Taking advantage of the exponential
family identity Eqpθqrtθpθqs “ ∇Aprηq, the objective L can
be re-written as:

L “ Eqpθqqpzq rln ppθ|z, xq ´ ln qpθqs

“ xη ` Eqpzqrtpz, xqs, ∇Aprηqy ´ pxrη, ∇Aprηqy ´ Aprηqq

Differentiating with respect to rη, we have that

`

∇rηL “

∇2Aprηq

˘ `

˘
η ` Eqpzqrtpz, xqs ´ rη

The natural gradient of L, denoted r∇rη, is deﬁned as r∇rη ﬁ
˘
`
∇2Aprηq
∇rη. And so the natural gradient of L can be
compactly described as:

´1

r∇rηL “ η `

Eqpztqtrtzxpzt, xtq, 1su ´ rη

(5)

Tÿ

t“1

Thus a stochastic natural descent update on the global pa-
rameters rηθ proceeds at step n by sampling a mini-batch xt
and taking the global update with step size ρn:

rηθ Ð p1 ´ ρnqrηθ ` ρnpηθ ` bJEq˚pztqrtpzt, xtqsq

(6)

where b is a vector of scaling factors adjusting for the rel-
ative size of the mini-batches. Here the global update as-
sumes optimal local update q˚pzq has been computed. In
each step however, the local factors q˚pztq are computed
with mean ﬁeld updates and the current value of qpθq (anal-
ogous to coordinate ascent). In what follows, we provide
the derivation for the update rules for Hidden Markov Mod-
els, which are the particular instantiation of the graphical
model we use to represent the role transition for our multi-
agent settings.

Variational factor updates via message passing for Hid-
den Markov Models. For HMMs, we can view global pa-
rameters θ as the parameters of the underlying HMMs such
as transition matrix and emission probabilities, while local
parameters z govern hidden state assignment at each time
step.

Fixing the global parameters, the local updates are based on
message passing over the graphical model. The exact math-
ematical derivation depends on the speciﬁc graph structure.
The simplest scenario is to assume independence among
zt’s, which resembles naive Bayes. We instead focus on
Hidden Markov Models to capture ﬁrst-order dependen-
cies in role transitions over play sequences. In this case,
global parameters θ “ pp0, P, φq where P “ rPijsK
i,j“1
is the transition matrix with Pij “ ppzt “ j|zt´1 “ iq,
φ “ tφiuK
i“1 are the emission parameters, and p0 is the
initial distribution.

Consider a Bayesian HMM on K latent states. Priors on
the model parameters include the initial state distribution
p0, transition matrix P with rows denoted p1, . . . , pK, and
the emission parameters φ “ tφiuK
i“1. In this case we have
the global parameters θ “ pp0, P, φq. For Hidden Markov
Model with observation x1:T and latent sequence z1:T , the
generative model over the parameters is given by φi „
ppφq (i.i.d from prior), pi „ Dirpαiq, z1 „ p0, zt`1 „ pzt,
and xt „ ppxt|φztq (conditional distribution given param-
eters φ). We can also write the transition matrix:

ﬁ

ﬃ
ﬂ

»

—
–

p1
...
pK

P “

The Bayesian hierarchical model over the parameters, hid-
den state sequence z1:T , and observation sequence y1:T is

iid„ ppφq, pi „ Dirpαiq

φi
z1 „ p0, zt`1 „ pzt, xt „ ppxt|φztq

For HMMs, we have a full probabilistic model: ppz, x|θq “
ś
T
p0pz1q
t“1 ppzt|zt´1, P qppxt|zt, φq. Deﬁne the likeli-
hood potential Lt,i “ ppxt|φiq, the likelihood of the latent
sequence, given observation and model parameters, is as
follows:

ppz1:T |x1:T , P, φq “

˜

exp

log p0pz1q `

log Pzt´1,zt `

log Lt,zt ´ Z

Tÿ

t“2

Tÿ

t“1

¸

(7)

where Z is the normalizing constant. Following the no-
tation and derivation from (Johnson & Willsky, 2014),
we denote ppz1:T |x1:T ,P,φq “ HMMpp0, P, Lq. Under
mean ﬁeld assumption, we approximate the true poste-
rior ppP, φ, z1:T |x1:T q with a mean ﬁeld variational family
qpP qqpφqqpz1:T q and update each variational factor in turn
while ﬁxing the others.

Fixing the global parameters θ, taking expectation of log
of (7), we derive the update rule for qpzq as qpz1:T q “
HMMp

rP , rp0, rLq where:

rPj,k “ exptEqpP q lnpPj,kqu
rp0,k “ exptln Eqpp0qp0,ku
rLt,k “ exptEqpφkq lnpppxt|zt “ kqqu

to qpz1:T q,
To calculate the expectation with respect
which is necessary for updating other
the
factors,
Forward-Backward recursion of HMMs is deﬁned by

Coordinated Multi-Agent Imitation Learning

(8)

(9)

(10)

forward messages F and backward messages B:
Kÿ

Ft,i “

Ft´1,j

rPj,i

rLt,i

Bt,i “

rPi,j

rLt`1,jBt`1,j

j“1
Kÿ

j“1

F1,i “ p0piq
BT,i “ 1

As a summary, calculating the gradient w.r.t z yields the
following optimal variational distribution over the latent se-
quence:

q˚pzq9 exp

´
EqpP qrln p0pz1qs `

EqpP qrlog Pzt´1,zts

Tÿ

t“2

¯
Eqpφq lnrppxt|ztqs

,

`

Tÿ

t“1

‰

“

“

which gives the local updates for q˚pzq, given current esti-
mates of P and φ:

(11)

rPj,k “ exp
rppxt|zt “ kq “ exp

EqpP q lnpPj,kq
Eqpφq ln ppxt|xt “ kq

‰

,

(12)
for k “ 1, . . . , K, t “ 1, . . . , T , and then use p0, rP , rp
to run the forward-backward algorithm to compute the up-
date q˚pzt “ kq and q˚pzt´1 “ j, zt “ kq. The forward-
backward algorithm in the local update step takes OpK 2T q
time for a chain of length T and K hidden states.

Training to learn model parameters for HMMs. Com-
bining natural gradient step with message-passing scheme
for HMMs yield speciﬁc update rules for learning the
model parameters. Again for HMMs, the global parameters
are θ “ pp0, P, φq and local variables z “ z1:T . Assuming
the priors on observation parameter ppφiq and likelihoods
ppxt|φiq are conjugate pairs of exponential family distribu-
tion for all i, the conditionals ppφi|xq have the form as seen
from equation 4:

ppφi|xq9 exptxηφi ` rtx,ipxq, 1s, tφipφiqyu
For structured mean ﬁeld inference, the approximation qpθq
factorizes as qpP qqpp0qqpφq. At each iteration, stochastic
variational inference sample a sequence x1:T from the data
set (e.g. trajectory from any randomly sampled player) and
perform stochastic gradient step on qpP qqpp0qqpφq. In or-
der to compute the gradient, we need to calculate expected
sufﬁcient statistics w.r.t the optimal factor for qpz1:T q,
which in turns depends on current value of qpP qqpp0qqpφq.

Following the notation from (Johnson & Willsky, 2014),
we write the prior and mean ﬁeld factors as

pppiq “ Dirpαiq, ppφiq9 exptxηφi, tφipφiqyu
qppiq “ Dirprαiq, qpφiq9 exptxrηφi, tφipφiqyu

Tÿ

t“1

T ´1ÿ

t“1

Tÿ

t“1

“

T ´1ÿ

t“1

Algorithm 5 Coordinated Structure Learning
LearnStructure tU1, . . . , UK, C, θ, ρu ÞÑ qpθ, zq
Input: Set of trajectories U “ tUkuK

k“1. Context C
Previous parameters θ “ pp0, θP , θφq, stepsize ρ

t“1 “ trut,k, ctsu @t, k.X “ tXkuK

1: Xk “ txt,kuT
2: Local update: Compute rP and rp per equation 11 and 12
and compute qpzq “ Forward-BackwardpX, rP , rpq

k“1

3: Global update of θ, per equations 16, 17, and 18.
output Updated model qpθ, zq “ qpθqqpzq

Using message passing scheme as per equations (8) and (9),
we deﬁne the intermediate quantities:

ptx,i ﬁ Eqpz1:T q

Irzt “ istx,ipxtq

“

Ft,iBt,irtx,ipxtq, 1s{Z

(13)

ppttrans,iqj ﬁ Eqpz1:T q

Irzt “ i, zt`1 “ js

Ft,i

rPi,j

rLt`1,jBt`1,j{Z

(14)

(15)

pptinitqi ﬁ Eqpz1:T q

Irz1 “ is “ rp0B1,i{Z

ř

where Z ﬁ
the indicator function.

K

i“1 FT,i is the normalizing constant, and I is

Given these expected sufﬁcient statistics, the speciﬁc up-
date rules corresponding to the natural gradient step in the
natural parameters of qpP q, qpp0q, and qpφq become:

rηφ,i Ð p1 ´ ρqrηφ,i ` ρpηφ,i ` bJptx,iq
rαi Ð p1 ´ ρqrαi ` ρpαi ` bJpttrans,iq
rα0 Ð p1 ´ ρqrα0 ` ρpα0 ` bJptinit,iq

(16)

(17)

(18)

B. Experimental Evaluation

B.1. Batch-Version of Algorithm 2 for Predator-Prey

B.2. Visualizing Role Assignment for Soccer

The Gaussian components of latent structure in ﬁgure 7
give interesting insight about the latent structure of the
demonstration data, which correspond to a popular for-
mation arrangement in professional soccer. Unlike the
predator-prey domain, however, the players are sometimes
expected to switch and swap roles. Figure 8 displays the
tendency that each learning policy k would takes on other
roles outside of its dominant mode. Policies indexed 0 ´ 3
tend to stay most consistent with the prescribed latent roles.
We observe that these also correspond to players with the
least variance in their action trajectories. Imitation loss is

Coordinated Multi-Agent Imitation Learning

Algorithm 6 Multi-Agent Data Aggregation Imitation
Learning
LearnpA1, A2, . . . , AK, C|Dq
Input: Ordered actions Ak “ tat,kuT

t“1 @k, context

Input: Aggregating data set D1, .., DK for each policy
Input: base routine TrainpS, Aq mapping state to ac-

tctuT

t“1

tions

1: for t “ 0, 1, 2, . . . , T do
2:
3:

Roll-out ˆat`1,k “ πkpˆst,kq @ agent k
Cross-update for each policy k P t1, . . . , Ku
ˆst`1,k “ ϕk prˆat`1,1, . . . , ˆat`1,k, . . . , ˆat`1,K, ct`1sq
Collect expert action a˚
Aggregate data set Dk “ Dk Y tˆst`1,k, a˚

t`1,k given state ˆst`1,k @k

t`1,kuT ´1
t“0

4:

5:
6: end for
7: πk Ð TrainpDkq
output K new policies π1, π2, . . . , πK

Figure 8. Role frequency assigned to policy, according to the max-
imum likelihood estimate of the latent structured model

generally higher for less consistent roles (e.g. policies in-
dexed 8 ´ 9). Intuitively, entropy regularization encourages
a decomposition of roles that result in learning policies as
decoupled as possible, in order to minimize the imitation
loss.

