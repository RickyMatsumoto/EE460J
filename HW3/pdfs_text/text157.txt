AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

A. Related work

There have been several major lines of research on the the-
oretical understanding of neural networks. The ﬁrst one
deals with understanding the properties of the objective
function used when training neural networks (Choroman-
ska et al., 2014; Sagun et al., 2014; Zhang et al., 2015; Livni
et al., 2014; Kawaguchi, 2016). The second involves study-
ing the black-box optimization algorithms that are often
used for training these networks (Hardt et al., 2015; Lian
et al., 2015). The third analyzes the statistical and gen-
eralization properties of neural networks (Bartlett, 1998;
Zhang et al., 2016; Neyshabur et al., 2015; Sun et al., 2016).
The fourth adopts a generative point of view assuming that
the data actually comes from a particular network, which it
shows how to recover (Arora et al., 2014; 2015). The ﬁfth
investigates the expressive ability of neural networks, ana-
lyzing what types of mappings they can learn (Cohen et al.,
2015; Eldan & Shamir, 2015; Telgarsky, 2016; Daniely
et al., 2016). This paper is most closely related to the
work on statistical and generalization properties of neural
networks. However, instead of analyzing the problem of
learning with a ﬁxed architecture, we study a more general
task of learning both architecture and model parameters si-
multaneously. On the other hand, the insights that we gain
by studying this more general setting can also be directly
applied to the setting with a ﬁxed architecture.

There has also been extensive work involving structure
learning for neural networks (Kwok & Yeung, 1997; Le-
ung et al., 2003; Islam et al., 2003; Lehtokangas, 1999; Is-
lam et al., 2009; Ma & Khorasani, 2003; Narasimha et al.,
2008; Han & Qiao, 2013; Kotani et al., 1997; Alvarez &
Salzmann, 2016). All these publications seek to grow and
prune the neural network architecture using some heuris-
tic. More recently, search-based approaches have been an
area of active research (Ha et al., 2016; Chen et al., 2015;
Zoph & Le, 2016; Baker et al., 2016). In this line of work,
a learning meta-algorithm is used to search for an efﬁcient
architecture. Once a better architecture is found, previously
trained networks are discarded. This search requires a sig-
niﬁcant amount of computational resources. Additionally,
(Saxena & Verbeek, 2016) presented an approach to over-
come the tedious process of exploring individual network
architectures via a so-called fabric that embeds an expo-
nentially large number of architectures.

To the best of our knowledge, none of these methods comes
with a theoretical guarantee on their performance. Further-
more, optimization problems associated with these meth-
ods are intractable. In contrast, the structure learning algo-
rithms introduced in this paper are directly based on data-
dependent generalization bounds and aim to solve a convex
optimization problem by adaptively growing the network
and preserving previously trained components.

Finally, (Janzamin et al., 2015) is another paper that an-
alyzes the generalization and training of two-layer neural
networks through tensor methods. Our work uses different
methods, applies to arbitrary networks, and also learns a
network structure from a single input layer.

B. Proofs

We will use the following structural learning guarantee for
ensembles of hypotheses.

Theorem 2 (DeepBoost Generalization Bound, Theorem 1,
(Cortes et al., 2014)). Let H be a hypothesis set admit-
ting a decomposition H = ∪l
i=1Hi for some l > 1. Fix
ρ > 0. Then, for any δ > 0, with probability at least 1 − δ
over the draw of a sample S from Dm, the following in-
equality holds for any f = (cid:80)T
t=1 αtht with αt ∈ R+ and
(cid:80)T

t=1 αt = 1:

R(f ) ≤ (cid:98)RS,ρ +

αtRm(Hkt) +

(cid:114)

2
ρ

log l
m

T
(cid:88)

t=1

4
ρ
(cid:115)(cid:24) 4

+

(cid:18) ρ2m
log l

(cid:19) (cid:25) log l
m

+

log( 2
δ )
2m

,

ρ2 log

where, for each ht ∈ H, kt denotes the smallest k ∈ [l]
such that ht ∈ Hkt.
Theorem 1. Fix ρ > 0. Then, for any δ > 0, with
probability at least 1 − δ over the draw of a sample S
of size m from Dm, the following inequality holds for all
f = (cid:80)l

k=1 wk · hk ∈ F:

R(f ) ≤ (cid:98)RS,ρ(f ) +

4
ρ

l
(cid:88)

k=1

(cid:13)
(cid:13)wk

(cid:13)
(cid:13)1Rm( (cid:101)Hk) +

(cid:114)

2
ρ

log l
m

+ C(ρ, l, m, δ),

where C(ρ, l, m, δ) =
(cid:17)

(cid:16) 1
ρ

(cid:113) log l
m

(cid:101)O

.

(cid:113)(cid:6) 4

ρ2 log( ρ2m

log l )(cid:7) log l

m + log( 2
δ )
2m =

Proof. This result follows directly from Theorem 2.

Theorem 1 can be straightforwardly generalized to the
multi-class classiﬁcation setting by using the ensemble
margin bounds of Kuznetsov et al. (2014).

Lemma 1. For any k > 1, the empirical Rademacher
complexity of Hk for a sample S of size m can be upper-
bounded as follows in terms of those of Hss with s < k:

(cid:98)RS(Hk) ≤ 2

1
q

Λk,sn

s (cid:98)RS(Hs).

k−1
(cid:88)

s=1

AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

Proof. By deﬁnition, (cid:98)RS(Hk) can be expressed as follows:




where the second inequality holds by Talagrand’s contrac-
tion lemma.

(cid:98)RS(Hk) =

1
m

E
σ




sup
hs∈Hns
s
(cid:107)us(cid:107)p≤Λk,s

m
(cid:88)

k−1
(cid:88)

σi

i=1

s=1

us · (ϕs ◦ hs)(xi)


.

By the sub-additivity of the supremum, it can be upper-
bounded as follows:

(cid:98)RS(Hk) ≤

σius · (ϕs ◦ hs)(xi)



E
σ




k−1
(cid:88)

s=1

1
m

m
(cid:88)

i=1

sup
hs∈Hns
s
(cid:107)us(cid:107)p≤Λk,s




.

We now bound each term of this sum, starting with the fol-
lowing chain of equalities:

s=1 2Λs,s−1 and Nk = (cid:81)k

Lemma 2. Let Λk = (cid:81)k
s=1 ns−1.
Then, for any k ≥ 1, the empirical Rademacher complexity
of H∗
k for a sample S of size m can be upper bounded as
follows:

(cid:98)RS(H∗

1
q
k) ≤ r∞ΛkN
k

(cid:114)

log(2n0)
2m

.

Proof. The empirical Rademacher complexity of H1 can
be bounded as follows:

(cid:98)RS(H1) =

σius · (ϕs ◦ hs)(xi)



E
σ




1
m

m
(cid:88)

i=1

sup
hs∈Hns
s
(cid:107)us(cid:107)p≤Λk,s
(cid:34)

=

Λk,s
m

E
σ

sup
hs∈Hns
s

=

Λk,sn
m

E
σ

sup
h∈Hs

(cid:34)



1
q
s

1
q
s

m
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

i=1

m
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

Λk,sn
m

E
σ


 sup
h∈Hs
σ∈{−1,+1}

σ

m
(cid:88)

i=1

σi(ϕs ◦ hs)(xi)

σi(ϕs ◦ h)(xi)

σi(ϕs ◦ h)(xi)




 ,






(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)q
(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the second equality holds by deﬁnition of the dual
norm and the third equality by the following equality:
|zi|]q(cid:105) 1

|zi|q(cid:105) 1

(cid:104) n
(cid:88)

(cid:104) n
(cid:88)

=

q

q

sup
zi∈Z

(cid:107)z(cid:107)q = sup
zi∈Z

[ sup
zi∈Z

i=1

i=1

= n

1
q sup
zi∈Z

|zi|.

The following chain of inequalities concludes the proof:



1
q
s

Λk,sn
m

E
σ


 sup
h∈Hs
σ∈{−1,+1}

σ

m
(cid:88)

i=1

σi(ϕs ◦ h)(xi)






1
q
s

Λk,sn
m

≤

+

Λk,s
m

E
σ

=

2Λk,sn
m

≤

2Λk,sn
m

1
q
s

1
q
s

1
q

(cid:34)

E
σ

(cid:34)

sup
h∈Hs

m
(cid:88)

i=1
m
(cid:88)

i=1

sup
h∈Hs
(cid:34)

E
σ

E
σ

(cid:34)

sup
h∈Hs

sup
h∈Hs

m
(cid:88)

i=1

m
(cid:88)

i=1

≤ 2Λk,sn

s (cid:98)RS(Hs),

σi(ϕs ◦ h)(xi)

−σi(ϕj ◦ h)(xi)

(cid:35)

(cid:35)

(cid:35)

σi(ϕs ◦ h)(xi)

(cid:35)

σih(xi)

(cid:35)

(cid:35)

sup
(cid:107)u(cid:107)p≤Λ1,0

m
(cid:88)

i=1

σiu · Ψ(xi)

u ·

σiΨ(xi)

m
(cid:88)

i=1

σi[Ψ(xi)]

(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)q

σi[Ψ(xi)]

(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

sup
(cid:107)u(cid:107)p≤Λ1,0
(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)

m
(cid:88)

i=1
(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:34)

m
(cid:88)

i=1

E
σ

E
σ

max
j∈[1,n1]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

m
(cid:88)

i=1



E
σ


 max
j∈[1,n1]
s∈{−1,+1}

m
(cid:88)

i=1

σi[Ψ(xi)]j

(cid:34)

(cid:34)

1
m

1
m

E
σ

E
σ

Λ1,0
m

E
σ

=

=

≤

Λ1,0n
m

=

Λ1,0n
m

=

Λ1,0n
m

1
q
0

1
q
0

1
q
0

1
q

≤ Λ1,0n

0 r∞

= r∞Λ1,0n

√

(cid:114)

m

(cid:112)2 log(2n0)
m
2 log(2n0)
m

.

1
q
0

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)






σis[Ψ(xi)]j

The result then follows by application of Lemma 1.

Corollary 1. Fix ρ > 0. Let Λk = (cid:81)k
s=1 4Λs,s−1 and
Nk = (cid:81)k
s=1 ns−1. Then, for any δ > 0, with probability at
least 1 − δ over the draw of a sample S of size m from Dm,
the following inequality holds for all f = (cid:80)l
k=1 wk · hk ∈
F∗:

R(f ) ≤ (cid:98)RS,ρ(f ) +

2
ρ

l
(cid:88)

k=1

(cid:20)

(cid:13)
(cid:13)wk

(cid:13)
(cid:13)1

r∞ΛkN

(cid:114)

1
q
k

(cid:21)

2 log(2n0)
m

(cid:114)

+

2
ρ

log l
m

+ C(ρ, l, m, δ),

where C(ρ, l, m, δ) =
(cid:17)

log l )(cid:7) log l
, and where r∞ = ES∼Dm[r∞].

ρ2 log( ρ2m

(cid:113) log l
m

(cid:16) 1
ρ

(cid:101)O

(cid:113)(cid:6) 4

m + log( 2
δ )
2m =

AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

optimization problem:

argmin
lt−1+1
s=1

H(cid:48)
s

h∈∪

min
w∈R

Ft(w, h).

Remarkably, the subnetwork that solves this inﬁnite di-
mensional optimization problem can be obtained directly
in closed-form:
Theorem 3 (ADANET.CVX Optimization). Let (w∗, h∗)
be the solution to the following optimization problem:

argmin
lt−1
H(cid:48)
s
s=1

h∈∪

min
w∈R

Ft(w, h).

Let Dt be a distribution over the sample (xi, yi)m
i=1 such
that Dt(i) ∝ Φ(cid:48)(1 − yift−1(xi)), and denote (cid:15)t,h =
Ei∼Dt[yih(xi)].
Then,

w∗h∗ = w(s∗)h(s∗),

where (w(s∗), h(s∗)) are deﬁned by:

s∗ = argmax
s∈[lt−1]

Λs,s−1(cid:107)(cid:15)t,hs−1,t−1(cid:107)q.

Λs,s−1|(cid:15)t,hs−1,t−1,i|q−1 sgn((cid:15)t,hs−1,t−1,i)

u(s)
i =

(cid:107)(cid:15)t,hs−1,t−1(cid:107)

q
p
q

h(s∗) = u(s∗) · (ϕs ◦ hs−1,t−1)

w(s∗) = argmin

w∈R

m
(cid:88)

i=1

1
m
(cid:17)

− yiwh(s∗)(xi)

+ Γs∗ |w|.

(cid:16)

Φ

1 − yift−1(xi)

Proof. By deﬁnition,

Ft(w, h)
m
(cid:88)

=

1
m

i=1

(cid:16)

Φ

1 − yi

(cid:0)ft−1(xi) − wh(xi)(cid:1)(cid:17)

+ Γh|w|.

Notice that the minimizer over ∪lt−1+1
s can be deter-
mined by comparing the minimizers over each H(cid:48)
s.

H(cid:48)

s=1

Moreover, since the penalty term Γh|w| has the same con-
tribution for every h ∈ H(cid:48)
s, it has no impact on the optimal
choice of h over H(cid:48)
s. Thus, to ﬁnd the minimizer over each
H(cid:48)
s, we can compute the derivative of Ft − Γh|w| with re-
spect to w:

(w, h)

d(Ft − Γh|η|)
dw
m
(cid:88)

=

−1
m

i=1

yih(xi)Φ(cid:48)(cid:16)

1 − yift−1(xi)

(cid:17)

.

Figure 4. Illustration
by
ADANET.CVX. Units at each layer (other than the output
layer) are only connected to units in the layer below.

designed

network

neural

of

a

Proof. Since F∗ is the convex hull of H∗, we can apply
k) instead of Rm( (cid:101)Hk). Observe
Theorem 1 with Rm( (cid:101)H∗
that, since for any k ∈ [l], (cid:101)H∗
k is the union of H∗
k and
its reﬂection, to derive a bound on Rm( (cid:101)H∗
k) from a bound
on Rm( (cid:101)Hk) it sufﬁces to double each Λs,s−1. Combining
this observation with the bound of Lemma 2 completes the
proof.

C. Alternative algorithm

In this section, we present an alternative algorithm,
ADANET.CVX, that generates candidate subnetworks in
closed-form using Banach space duality.

As in Section 5, let ft−1 denote the ADANET model after
t − 1 rounds, and let lt−1 be the depth of the architecture.
ADANET.CVX will consider lt−1 + 1 candidate subnet-
works, one for each layer in the model plus an additional
one for extending the model.

Let h(s) denote the candidate subnetwork associated to
layer s ∈ [lt−1 + 1]. We deﬁne h(s) to be a single unit
in layer s that is connected to units of ft−1 in layer s − 1:

h(s) ∈ {x (cid:55)→ u · (ϕs−1 ◦ hs−1,t−1)(x) :
u ∈ Rns−1,t−1, (cid:107)u(cid:107)p ≤ Λs,s−1}.

See Figure 4 for an illustration of the type of neural network
designed using these candidate subnetworks.

For convenience, we denote this space of subnetworks by
H(cid:48)
s:

H(cid:48)

s = {x (cid:55)→ u · (ϕs−1 ◦ hs−1,t−1)(x) :
u ∈ Rns−1,t−1 , (cid:107)u(cid:107)p ≤ Λs,s−1}.

Now, recall the notation

Ft(w, h)
m
(cid:88)

=

1
m

i=1

(cid:16)

Φ

1 − yi(ft−1(xi) − wh(xi))

+ Γh|w|

(cid:17)

used in Section 5. As in ADANET, the candidate subnet-
work chosen by ADANET.CVX is given by the following

AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

Now, if we let

Dt(i)St = Φ(cid:48)(cid:16)

1 − yift−1(xi)

(cid:17)

,

then this expression is equal to
(cid:35)

−

(cid:34) m
(cid:88)

i=1

yih(xi)Dt(i)

= (2(cid:15)t,h − 1)

St
m

St
m

,

where (cid:15)t,h = Ei∼Dt[yih(xi)]. Thus, it follows that for any
s ∈ [lt−1 + 1],

argmax
h∈H(cid:48)
s

d(Ft − Γh|w|)
dw

(w, h) = argmax

(cid:15)t,h.

h∈H(cid:48)
s

Note that we still need to search for the optimal descent
coordinate over an inﬁnite dimensional space. However,
we can write

(cid:15)t,h

E
i∼Dt

max
h∈H(cid:48)
s
= max
h∈H(cid:48)
s
max
u∈Rns−1,t−1
max
u∈Rns−1,t−1

=

=

[yih(xi)]

E
i∼Dt
u · E
i∼Dt

[yiu · (ϕs−1 ◦ hs−1,t−1)(xi)]

[yi(ϕs−1 ◦ hs−1,t−1)(xi)].

Now, if we denote by u(s) the connection weights associ-
ated to h(s), then we claim that

u(s)
i =

Λs,s−1|(cid:15)t,hs−1,t−1,i |q−1 sgn((cid:15)t,hs−1,t−1,i )

,

q
p
(cid:107)(cid:15)t,hs−1,t−1 (cid:107)
q

which is a consequence of Banach space duality. To see
this, note ﬁrst that by H¨older’s inequality, every u ∈
Rns−1,t−1 with (cid:107)u(cid:107)p ≤ Λs,s−1 satisﬁes:

[yi(ϕs−1 ◦ hs−1,t−1)(xi)]

u · E
i∼Dt
≤ (cid:107)u(cid:107)p(cid:107) E
i∼Dt
≤ Λs,s−1(cid:107) E
i∼Dt

[yi(ϕs−1 ◦ hs−1,t−1)(xi)](cid:107)q.

At the same time, our choice of u(s) also attains this upper
bound:

u(s) · (cid:15)t,hs−1,t−1
ns−1,t−1
(cid:88)

u(s)
i (cid:15)t,hs−1,t−1,i

i=1
ns−1,t−1
(cid:88)

Λs,s−1

i=1

q
p
(cid:107)(cid:15)t,hs−1,t−1(cid:107)
q

=

=

=

Λs,s−1

(cid:107)(cid:15)t,hs−1,t−1(cid:107)

q
p
q

(cid:107)(cid:15)t,hs−1,t−1(cid:107)q
q

= Λs,s−1(cid:107)(cid:15)t,hs−1,t−1(cid:107)q.

|(cid:15)t,hs−1,t−1,i |q

ADANET.CVX(S = ((xi, yi)m

i=1)

f0 ← 0
for t ← 1 to T do

s∗ ← argmaxs∈[lt−1+1] Λs,s−1(cid:107)(cid:15)t,hs−1,t−1(cid:107)q.
u(s∗)
i ←

Λs∗ ,s∗−1|(cid:15)t,hs∗−1,t−1,i

|q−1 sgn((cid:15)t,hs−1,t−1,i )

q
p
q

(cid:107)

(cid:107)(cid:15)t,hs∗−1,t−1
h(cid:48) ← u(s∗) · (φs∗−1 ◦ hs∗−1,t−1)
η(cid:48) ← MINIMIZE( ˜Ft(η, h(cid:48)))
ft ← ft−1 + η(cid:48) · h(cid:48)

return fT

1
2
3

4

5
6
7
8

Figure 5. Pseudocode of the ADANET.CVX algorithm.

Thus, u(s) and the associated network h(s) is the coor-
dinate that maximizes the derivative of Ft with respect
to w among all subnetworks in H(cid:48)
s. Moreover, h(s) also
achieves the value: Λs,s−1(cid:107)(cid:15)t,hs−1,t−1(cid:107)q.

This implies that by computing Λs,s−1(cid:107)(cid:15)t,hs−1,t−1 (cid:107)q for
every s ∈ [lt−1 + 1], we can ﬁnd the descent coordinate
across all s ∈ [lt−1 + 1] that improves the objective by
the largest amount. Moreover, we can then solve for the
optimal step size in this direction to compute the weight
update.

The theorem above deﬁnes the choice of descent coordi-
nate at each round and motivates the following algorithm,
ADANET.CVX. At each round, ADANET.CVX can de-
sign the optimal candidate subnetwork within its searched
space in closed form, leading to an extremely efﬁcient up-
date. However, this comes at the cost of a more restrictive
search space than the one used in ADANET. The pseu-
docode of ADANET.CVX is provided in Figure 5.

In this section, we report the results of some additional ex-
periments.

In our ﬁrst set of experiments, we compared how many tri-
als were needed for a bandit algorithm (Snoek et al., 2012)
to ﬁnd a close-to-optimal set of hyperparameters. Our ex-
periment (see Table 5) shows that both ADANET and tra-
ditional NNs often ﬁnd close-to-optimal parameter values
within the ﬁrst 200 trials. The number of trials were av-
eraged over 10 folds. Note that optimal parameters for
ADANET result in better accuracy than those of traditional
NNs. This serves as further evidence that ADANET is more
efﬁcient in both ﬁnding close-to-optimal accuracy values
and in ﬁnding the best optimal network architecture when
compared to traditional NNs.

It should be noted that, in general, the number of trials that

[yi(ϕs−1 ◦ hs−1,t−1)(xi)](cid:107)q

D. More experiments

AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks

Table 5. Performance of hyperparameter search on the CIFAR
cat-dog task.

Algorithm Average number of trials

ADANET
NN

165.8
136

are needed to ﬁnd close-to-optimal parameters depends on
the algorithm used to perform hyperparameter optimiza-
tion.

Our ﬁnal experiment consisted of ﬁrst
running the
ADANET algorithm to learn an architecture and weights
and then running back-propagation algorithm on the re-
sulting architecture with weights learned by ADANET as
initialization. We used the cat-dog label pair task and
3,000 back-propagation steps with the same learning rate
as the one used to train each subnetwork. Using the same
cross-validation setup as in Section 6 for our CIFAR-10 ex-
periments, this led to a test accuracy of 0.6908 (with a stan-
dard deviation of 0.01224), which is slightly worse than the
accuracy of 0.6924 obtained by running the ADANET al-
gorithm alone. This further demonstrates that ADANET is
able to learn both the network architecture and its weights
simultaneously.

E. Implementation details

In this section, we provide some additional implementa-
tion details regarding the experiments presented in Sec-
tion 6. Our system involves two components implemented
on a CPU: 1) a subnetwork model that handles each sub-
network; 2) an ADANET model to compose multiple sub-
networks and a classiﬁcation layer to combine all output
weights. For both these components, we used the Tensor-
ﬂow package. We implemented a custom layer using ma-
trix multiplication, including embeddings from the other
subnetworks. In addition, our loss function was based on
Eq. (5) and optimized via stochastic optimization (Kingma
& Ba, 2014).

