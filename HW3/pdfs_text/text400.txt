Frame-based Data Factorizations

Sebastian Mair 1 Ahc`ene Boubekki 1 2 Ulf Brefeld 1

Abstract

factors,

Archetypal Analysis is the method of choice
to compute interpretable matrix factorizations.
is represented as a convex
Every data point
i.e., points on the
combination of
boundary of the convex hull of the data. This
renders computation inefﬁcient. In this paper, we
show that the set of vertices of a convex hull, the
so-called frame, can be efﬁciently computed by
a quadratic program. We provide theoretical and
empirical results for our proposed approach and
make use of the frame to accelerate Archetypal
The novel method yields similar
Analysis.
reconstruction errors as baseline competitors but
is much faster to compute.

1. Introduction

Matrix factorizations are used in many different learning
tasks, including clustering, classiﬁcation, recommendation,
image denoising and
text and social network analysis,
hyperspectral
imaging; consequently, a great deal of
approaches to factor a matrix into two or more matrices
have been proposed.

A common limitation of matrix factorization techniques,
is a lack of interpretability of the resulting
however,
factors.
This observation also holds for non-negative
matrix factorization (Paatero & Tapper, 1994; Lee &
Seung, 1999) (NMF). Although NMF constrains data,
matrices, and factors to be non-negative, the remaining
factors may lie far from other data points and do not
necessarily contribute to interpretability. Recently,
the
focus on interpretable factors led to stochastic NMFs
for clusterings (Arora et al., 2011; 2013) and convex
constraints (Ding et al., 2010).

However, computing interpretable factors is actually an
“old” problem that has been introduced by Cutler and

1Leuphana University, L¨uneburg, Germany 2German Institute
for Educational Research, Frankfurt am Main, Germany.
Correspondence to: Sebastian Mair <mair@leuphana.de>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Breiman (1994). Their idea is to represent every data
point as a convex combination of factors, the so-called
archetypes. Since the archetypes are themselves restricted
to be convex combinations of data points, they need to lie
on the boundary of the convex hull of the data; hence, they
are either close to actual data points or mixtures thereof,
see Figure 1. An archetype-based factorization yields two
stochastic matrices and naturally interpretable factors for
arbitrary data matrices. Variants of Archtypal Analysis
(AA) are for instance introduced in Mørup and Hansen
(2010), Chen et al. (2014) and Bauckhage et al. (2015).

The computation of convex hulls is generally demanding.
For AA, a straight forward idea is to make use of (e.g.,
precomputed) vertices of the convex hull of the data. We
will refer to the set of vertices as the frame. Given
the frame, computing the archetypes reduces to a search
within the frame. We propose to go one step further and
approximate AA by restricting the whole optimization to
the frame: ﬁrstly, we compute the frame and secondly,
we compute the archetypes on the frame (and just on the
frame). Naturally, the result is a factorization of the frame
itself. The factorization can then be extended to the all
data points by recomputing the weights in hindsight. The
idea is similar to Thurau et al.
(2011) who propose to
approximate the frame using two-dimensional projections;
unfortunately, their approach adds an unnecessary layer
Ideally, given an
of approximation to the problem:
appropriate approximation of the frame,
the bulk of
the remaining data points should be reconstructed either
lossless or at only a small cost.

Standard approaches to compute the convex hull
like
Quickhull (Barber et al., 1996) become infeasible for
dimensionalities larger than three because of dispensable
triangulations. Discarding the triangulations leads to linear
programming (LP)-based solutions (Dul´a & Helgason,
1996; Ottmann et al., 2001; Dul´a & L´opez, 2012) which
test whether a point at-hand is part of the frame. These
approaches require adequate preprocessing as duplicates
may cause false negatives.

In this paper, we (i) show that the exact frame can be
computed by a quadratic program (QP), (ii) reduce the
optimization to an existing algorithm and (iii) provide
theoretical and empirical justiﬁcations for the developed
method.

Frame-based Data Factorizations

Algorithm 1 Archetypal Analysis (AA)

Input: data matrix X, number of archetypes p
Output: factor matrices A, Z = BX (ABX ≈ X)
Z = initial guess of archetypes on X
while not converged do
for i = 1, 2, . . . , n do
ai = argmin

(cid:107)Z(cid:62)ai − xi(cid:107)2
2

(cid:107)ai(cid:107)1=1,aij ≥0

end for
Z = (A(cid:62)A)−1A(cid:62)X
for k = 1, 2, . . . , p do
bk = argmin

(cid:107)bk(cid:107)1=1,bkj ≥0

end for
Z = BX

end while

(cid:107)X(cid:62)bk − zk(cid:107)2
2

where (cid:107) · (cid:107)F denotes the Frobenius norm. The optimization
problem is non-convex in A and B but convex in A for a
ﬁxed B and vice versa. Usually, it is solved by iteratively
solving for A and B as outlined in Algorithm 1. Cutler and
Breiman (1994) prove that the archetypes z1, . . . , zp lie on
the boundary ∂ conv(X ) of the convex hull of the data X
for 1 < p < n. From a geometrical point of view, AA
yields an approximation of the convex hull of size p. Every
point inside this polygon can be reconstructed in a lossless
way and every point outside will be projected onto it. The
optimization of the RSS will minimize the reconstruction
error.

3. Frame-based Factorizations

3.1. Preliminaries

We consider a discrete data set X = {x1, . . . , xn}n
i=1
consisting of n data points in d dimensions, i.e., xi ∈ Rd
for i = 1, 2, . . . , n. The convex hull conv(X ) is the
intersection of all convex sets containing X . Furthermore,
conv(X ) is the set of all convex combinations of points in
X . The intersection of X with the boundary of X , i.e.,
X ∩ ∂ conv(X ), is called the frame F. Hence, the frame
consists of the extreme points of X . Those points cannot be
represented as convex combinations of other points rather
than themselves. Note that the frame F and the data set X
yield the same convex hull, i.e., conv(F) = conv(X ). By
q = |F| we refer to the size of the frame and we call the
portion of points in X belonging to the frame F, the frame
density. In the remainder we assume n > d.

There are some straightforward consequences that will
become handy in the remainder. When the inner product
between two points is maximized, one of the points is an
extreme point and thus belongs to the frame of the data.

Figure 1. Comparison of factorizations computed with NMF
(green squares) and AA (red stars). While the former may place
the factors far from data, Archetypal Analysis uses data points on
the boundary of the convex hull and yields an intuitive solution.

The remainder is structured as follows. Section 2 reviews
Archetypal Analysis and Section 3 contains the main
contribution, a new computational method for ﬁnding the
frame and a frame-based matrix factorization. Section 4
reports on empirical results and Section 5 introduces an
application as an autoencoder. Section 6 concludes.

2. Archetypal Analysis

Let X = {x1, . . . , xn}n
i=1 be a data set consisting of n data
points in d dimensions and X ∈ Rn×d be the data matrix.
The goal of Archetypal Analysis (AA) (Cutler & Breiman,
1994) is to ﬁnd a factorization of the data

X = ABX = AZ,

(1)

where A ∈ Rn×p, Z = BX ∈ Rp×d are the factors
and p ∈ N is the latent dimensionality. The matrix A
contains the weights ai ∈ Rp (i = 1, . . . , n) of the
convex combinations of the p archetypes z1, . . . , zp which
are stacked in the matrix Z, i.e.,

∀i :

xi =

(ai)k · zk,

(ai)k = 1,

(ai)k ≥ 0.

The archetypes zk (k = 1, . . . , p) are themselves convex
combinations of data points. The matrix B holds the
weights bk ∈ Rn (k = 1, . . . , p) of those convex
combinations and selects the archetypes from the given
data set X. The representation is as follows:

∀k :

zk =

(bk)i · xi,

(bk)i = 1,

(bk)i ≥ 0.

The matrices A and B are row-stochastic due to their
constraints. The factorization is obtained by minimizing
the residual sum of squares (RSS)

min RSS(p) = (cid:107)X − AZ(cid:107)2

F = (cid:107)X − ABX(cid:107)2
F ,

p
(cid:88)

k=1

n
(cid:88)

i=1

p
(cid:88)

k=1

n
(cid:88)

i=1

Frame-based Data Factorizations

Algorithm 2 Frame-AA

Input: data X , number of archetypes p ∈ N
Output: factor matrices A, Z = BH (ABH ≈ X)
F = Frame(X )
H = frame matrix
A, Z = ArchetypalAnalysis(H, p)
A = 0 ∈ Rn×p
for i = 1, 2, . . . , n do
ai = argmin

(cid:107)Z(cid:62)ai − xi(cid:107)2
2

(cid:107)ai(cid:107)1=1,aij ≥0

end for

Lemma 1. Let X be a ﬁnite set of discrete points, then

∀x ∈ X :

argmax
x(cid:48)∈X

(cid:104)x, x(cid:48)(cid:105) ∈ F.

Proof. Linearity and convexity of the inner product imply
that its maximum is realized by an extreme point of the
domain. Since the domain is X , the maximum belongs to
its frame F.

In addition, every point of the domain lies in the span of
some points on the frame.

Proposition 1. Every point x of X can be written as a
convex combination of at most d + 1 points from the frame
F of X .

Proof. See Brondsted (2012).

3.2. Motivation

The idea of the proposal is as follows. Due to the fact that
the archetypes lie on the boundary of the convex hull, it is
possible to restrict the search of the archetypes to the frame.
However, we intend to go one step further. Since the frame
F and the data X yield the same convex hull, we restrict
the factorization in Equation (1) to the matrix H ∈ Rq×d
of stacked points of the frame, i.e.,

H = ABH = AZ.

The idea is depicted in Figure 2. Although Archetypal
Analysis is only computed on the frame it often yields
almost identical archetypes as AA computed on the whole
data set as illustrated in the right subplot of Figure 2. The
reduction of points yields an accelerated computation.

Assuming a low frame density, i.e., q (cid:28) n, a sufﬁcient
approximation of the frame, and therefore the convex hull,
will also cover most of the points in the interior of its
induced polygon. On the other hand, if the frame density is
high, the problem will reduce to standard AA in the limit
q → n and the speed up will vanish. Based on the nature of

Figure 2. Approximation of Archetypal Analysis.

the problem, we claim that AA makes no sense in scenarios
of high frame densities. This is because almost all points
will be projected unless p is chosen very high which is
usually not the case.

After computing the factorization by AA, we have to
recompute the weight matrix A ∈ Rq×p for all data points
using the optimized archetypes within Z. This procedure
is called Frame-AA and presented in Algorithm 2. Note
that the idea does not only apply for standard Archetypal
Analysis as presented in Cutler and Breiman (1994) but
also all variants thereof (Mørup & Hansen, 2010; Chen
et al., 2014; Bauckhage et al., 2015).

Until now, we assumed that the frame F or equivalently
the frame matrix H ∈ Rq×d, as required in the ﬁrst line
of the algorithm, is already given. In the following section,
we present a novel algorithm for efﬁciently computing the
frame of a discrete data set.

3.3. Representing a Single Point

Let F be the frame of X and X ∈ Rd×n be the design
matrix of X . The idea is as follows. For every x ∈ X ,
we aim to solve the linear system Xs = x subject to the
constraints that the solution s is non-negative, sums up to
one, and uses only points from the frame, i.e.,

Xs = x
si ≥ 0 ∧ 1(cid:62)s = 1 ∧ si (cid:54)= 0 ⇒ xi ∈ F.

s. t.

(2)

Theorem 1 reduces this to a least-squares approach.
Theorem 1. Let F be the frame of X , X ∈ Rd×n be
the design matrix of X , x ∈ X and ¯X = (cid:2)X(cid:62), 1(cid:3)(cid:62)
∈
R(d+1)×n as well as ¯x = (x(cid:62), 1)(cid:62) ∈ Rd+1 be the
augmented versions of X and x. Then, the following
problems have the same solutions.

(i) Xs = x s. t. si ≥ 0 ∧ 1(cid:62)s = 1 ∧ si (cid:54)= 0 ⇒ xi ∈ F.
(ii) ¯Xs = ¯x s. t. si ≥ 0 ∧ si (cid:54)= 0 ⇒ xi ∈ F.
2 (cid:107) ¯Xs − ¯x(cid:107)2

2 s. t. si (cid:54)= 0 ⇒ xi ∈ F.

(iii) argmins≥0

1

Proof. (i) is equivalent to (ii) as it integrates the constraint
1(cid:62)s = 1 into the system of linear equations. Proposition 1

Algorithm 3 Lawson and Hanson’s active set algorithm

Frame-based Data Factorizations

Input: design matrix ¯X and right hand side ¯x
Output: solution s with si ≥ 0 ∀i
P = ∅
Z = {1, 2, . . . , n}
s = 0
w = −∇f (s) = ¯X(cid:62)(¯x − ¯Xs)
while Z (cid:54)= ∅ and ∃ w[Z] ≥ ε do
k = argmaxj{ wj | j ∈ Z }
Z = Z \ {k}
P = P ∪ {k}
¯XP = ¯X[:, P] ∈ R(d+1)×|P|
z = argminz (cid:107) ¯XP z − ¯x(cid:107)2
z[Z] = 0
J = { j ∈ P | zj ≤ ε }
while |J | > 0 do

2

| j ∈ J }

sj
sj −zj

α = min{
s = s + α · (z − s)
for i = 1, 2, . . . , n do

if i ∈ P and |si| ≤ ε then

P = P \ {i}
Z = Z ∪ {i}
¯XP [:, i] = 0

end if
end for
z = argminz (cid:107) ¯XP z − ¯x(cid:107)2
z[Z] = 0

2

end while
x = z
w = −∇f (s) = ¯X(cid:62)(¯x − ¯Xs)

end while

assures that (i) has a solution. Hence, the minimum of
2 (cid:107) ¯Xs − ¯x(cid:107)2
1
2 is always zero and a solution of (iii) is also
a solution to (ii) and (i).

If the condition that the positive positions of s refer to
points on the frame is dropped, then problem

s = argmins≥0 f (s) =

(cid:107) ¯Xs − ¯x(cid:107)2
2

(3)

1
2

is equivalent to the non-negative least squares (NNLS)
problem which is a special case of a QP. The active-set
method from Lawson and Hanson (1995) is proven to yield
a least-squares estimate of Equation (3). As the minimum
is zero, it also gives a solution to the linear problem up
to the condition that the points contributing to the solution
belong to the frame. The method of Lawson and Hanson
(1995) called NNLS is outlined in Algorithm 3.

Theorem 2. Algorithm 3 from Lawson and Hanson (1995)
solves the problem in Equation (2).

Proof. Let Algorithm 3 solve the problem in Equation (3).
Denote by w = −∇f (s) = ¯X(cid:62)(¯x − ¯Xs) the negative
gradient with respect to s. The points { ¯xi | i ∈ P }
involved in the solution are selected via

k = argmaxj{ wj | j ∈ Z }

= argmaxj{ (cid:2) ¯X(cid:62)(¯x − ¯Xs)(cid:3)

j | j ∈ Z }

= argmaxj{ (cid:104)¯xj, ¯x(cid:105) −

si · (cid:104)¯xi, ¯xj(cid:105) | j ∈ Z }.

n
(cid:88)

i=1

Lemma 1 assures that those points belong to the frame.
Since there is no other way of selecting points, all of them
belong to the frame. Hence, the condition si (cid:54)= 0 ⇒ xi ∈
F is satisﬁed. The claim follows with Theorem 1.

The following proposition from Lawson and Hanson
(1995) characterizes the solution of this method.
Proposition 2 (Kuhn-Tucker conditions). A vector s ∈ Rn
is a solution for f (s) = 1
2 if and only if there
exists a vector w ∈ Rn and a partitioning of the integers
from 1 to d + 1 into subsets Z and P such that with w =
−∇f (s) = ¯X(cid:62)(x − ¯Xs) it holds that

2 (cid:107) ¯Xs − ¯x(cid:107)2

si = 0 for i ∈ Z,
wi ≤ 0 for i ∈ Z,

si > 0 for i ∈ P,
wi = 0 for i ∈ P.

Proposition 1 states that no more than d + 1 extreme points
are needed to deﬁne a point.
In addition, Algorithm 3
chooses iteratively one extreme point after another. As it is
guaranteed to lower the error in each iteration (Lawson &
Hanson, 1995) it terminates with at most d+1 non-negative
positions and yields a sparse convex combination.

3.4. Computing the Frame

Until now, we represented a single point as a sparse convex
combination of points on the frame. By doing this for every
point xi (i = 1, 2, . . . , n) in the data set, we obtain the
frame F of X . Algorithm 4 summarizes this procedure,
called NNLS-Frame, and Corollary 1 proves this claim.
Corollary 1. Algorithm 4 yields the frame F of X .

Proof. Theorem 2 ensures that the positive positions of
every solution sk (k = 1, 2, . . . , n) refers to points on the
frame. Taking the union of those positions recovers the
frame indices E since every extreme point deﬁnes itself and
therefore its index is part of the union.

However, until now it remains unclear whether the positive
elements of the solution refer to points on the frame. The
following theorem shows that this is actually the case.

that Algorithm 4 already realizes

Note
a matrix
factorization for the special case p = q. This is also
stated in the following corollary.

Frame-based Data Factorizations

Algorithm 4 NNLS-Frame
Input: design matrix ¯X
Output: indices of ext. points E and weight matrix W
E = ∅
W = 0 ∈ Rn×n
for k = 1, 2, . . . , n do
sk = NNLS( ¯X, ¯xk)
Pk = { i ∈ {1, 2, . . . , n} | (sk)i > 0 }
E = E ∪ Pk
W[k, :] = sk

end for

Algorithm 5 Divide and Conquer strategy of NNLS-Frame

Input: data X , number of splits K ∈ N
Output: indices of extreme points E
X = (cid:83)K
E = ∅
for k = 1, 2, . . . , K do

k=1 X (k) with X (i) ∩ X (j) = ∅ for i (cid:54)= j

Ek = NNLS-Frame(X (k))
E = E ∪ Ek

end for
E = NNLS-Frame(XE )

Corollary 2. Let E and W and be the result of Algorithm 4.
Then it holds that

(i) X = XW = X[:, E]W[E, :] is a factorization of the
design matrix X with W[E, :] ∈ Rp×n being a non-
negative stochastic matrix and X[:, E] ∈ Rd×p being
the submatrix of X containing only the frame of X .

(ii) the factorization above is lossless, i.e.,

(cid:107)X − X[:, E]W[E, :](cid:107)2

F = 0.

3.5. Complexity Analysis

First,

There are two main computations within the NNLS method
in Algorithm 3.
the negative gradient is being
computed, which has a complexity of O(n(d + 1)).
Second, an unconstrained least-squares problem z =
argminz (cid:107) ¯XP z − ¯x(cid:107)2
2 is being solved. The latter can
¯XP )−1 ¯X(cid:62)
be rewritten as z[P] = ( ¯X(cid:62)
P ¯x and has a
P
complexity of O((d + 1) · |P|2), where the average size
of P is 1
2 (d + 1). Since no more than (d + 1) points are
sufﬁcient for the solution (compare Theorem 2) the while
loop is being executed at most (d + 1) times. Therefore,
the complexity of the NNLS method is O(NNLS) =
O((d + 1)[ 1
4 (d + 1)3 + n(d + 1)]) on average. The
complexity of NNLS-Frame presented in Algorithm 4 is
O(n · NNLS) = O( n
4 (d + 1)4 + n2(d + 1)2). Actually,
the multiplier is less than n since points which are already
known to be extreme can be omitted. In contrast, the fastest
LP-based approach (Dul´a & L´opez, 2012) so far scales in
O(n · LPd+1,|F | +dn|F|), where LPr,t is the runtime of a
LP with r equality constraints and t non-negative variables.

3.6. Computing the Frame Efﬁciently

One way to speed up the frame computation is a divide and
conquer approach. The underlying principle is stated in the
following lemma.

Lemma 2. Let A and B be non-empty discrete sets, then

Figure 3. Percentage of discovered extreme points versus
percentage of iterations on synthetic data with n = 2500 points
in d = 5 dimensions.

The idea is as follows. Let X (1) ∪ . . . ∪ X (K) be a random
partition of X . The size nk of every subset X (k) should
be signiﬁcantly smaller than the size of X , i.e., nk (cid:28) n.
The assumption of having a pairwise disjunction is not
necessary but reasonable.
Instead of the whole data set
X , Algorithm 4 is now executed on every subset X (k) for
k = 1, 2, . . . , K. Finally, the frame of X is obtained by
merging the frames of every subset and run the proposed
approach again on it. The procedure is summarized in
Algorithm 5. The for-loops of Algorithms 2, 4 and 5 can
be trivially parallelized.

4. Experiments

4.1. Computing the Frame

For the experiments in this section we want to control the
frame density. Hence, use the same synthetic data1 as in
Dul´a and L´opez (2012) which was generated according
to a procedure described in L´opez (2005). The data
consists of n = 2500, 5000, 7500, 10000 data points in
d = 5, 10, 15, 20 dimensions with a frame density of
1, 15, 25, 50, 75 percent respectively.

conv(A ∪ B) = conv (cid:0) conv(A) ∪ conv(B)(cid:1).

1http://www.people.vcu.edu/˜jdula/

FramesAlgorithms/

Frame-based Data Factorizations

Figure 4. Timing results on synthetic data with n = 10000 points.

The third experiment is about the divide and conquer
approach introduced in Section 3.6. As shown in Figure 5,
the runtime is halved for every conﬁguration using only
three partitions. The take-home message is, that even a
small number of partitions can lead to a substantial speed
up. Note that those partitions can be processed in parallel.

Table 1. Data sets and their frame size and density.

data set
(Dul´a & L´opez, 2012)
Banking2
(Dul´a & L´opez, 2012)
Banking1
(Epifanio et al., 2013)
USAFSurvey
(Horton & Nakai, 1996)
yeast
(Dul´a & L´opez, 2012)
Banking3
(Vinu´e, 2017)
SpanishSurvey
swiss-heads (Cutler & Breiman, 1994)
skel2
(Heinz et al., 2003)
(Cutler & Breiman, 1994)
ozone

n
12456
4971
2420
1484
19939
600
200
507
330

d
8
7
6
8
11
5
6
10
10

q
715
345
368
242
4960
150
115
431
308

frame density
5.74%
6.94%
15.21%
16.31%
24.88%
25.00%
57.50%
85.01%
93.33%

4.2. Matrix Factorization

We now compare the frame-based Archetypal Analysis,
Frame-AA (F-AA), to several baselines including standard
Archetypal Analysis (Cutler & Breiman, 1994) (AA),
(CH-NMF),
ConvexHull-NMF (Thurau et al., 2011)
Convex-NMF (Ding et al., 2010) (C-NMF) and standard
NMF (Lee & Seung, 1999). The ﬁrst two methods are
implemented in Python. For CH-NMF and C-NMF we
use pymf 2, and for NMF we use scikit-learn 3. Table 1
depicts the data sets used for this experiment. The frame
sizes and densities are computed with our proposed NNLS-
Frame method.

Table 2 reports on the results for the choice of p = 6 in
terms of reconstruction error measured with the Frobenius
norm. We obtain similar results for p = 8, 10, 12.
The number of iterations executed per algorithm is ﬁxed
to 100 in order to obtain fair results. We use random
initializations for all algorithms and report on averages
over 36 repetitions. As seen in Table 2, F-AA yields
similar errors as AA. The lowest errors are achieved with
NMF. The baseline CH-NMF, which approximates the

2http://pypi.python.org/pypi/PyMF
3http://scikit-learn.org

Figure 5. Timing results for the divide and conquer approach on
synthetic data with n = 10000 points in d = 5 dimensions.

In contrast

The ﬁrst experiment is about the discovery of extreme
to LP-based approaches (Dul´a &
points.
Helgason, 1996; Ottmann et al., 2001), which discover up
to one extreme point per iteration, NNLS-Frame ﬁnds up
to d + 1 extreme points. This is a result of Theorem 2 and
illustrated in Figure 3. The graph shows the percentage
of discovered extreme points against the percentage of
iterations conducted on a synthetic data set with n =
2500 points in 5 dimensions with various frame densities.
The lower the frame density the faster the frame is being
discovered. Even for a fairly high frame density of 75%,
the discovery is faster than approximately linear as one
could expect from an LP-based approach. This is depicted
as a dashed line. Note that if an approximation of the frame
is sufﬁcient, the computation could be stopped.

In our second experiment we show that ﬁnding the frame is
faster than using LP-based approaches. The two baselines
(2001) and Dul´a and
are taken from Ottmann et al.
Helgason (1996). We make use of the implementation by
Dul´a and L´opez (2012) and implement our approach in
the same programming language for compatibility. We test
on data sets of sizes n = 2500, 5000, 7500, 10000 and of
dimensionality d = 5, 10, 15, 20. Figure 4 shows timing
results for n = 10000. The ﬁgure shows that our approach
is always faster than the baselines, irrespectively of the
conﬁguration. We obtain the same result on the remaining
data set sizes n = 2500, 5000, 7500 which are not shown.

Frame-based Data Factorizations

Table 2. Average Frobenius norm reported on 36 repetitions with
random initializations for p = 6. ”-” indicates that the method
could not be executed due to negative data.

data set
Banking2
Banking1
USAFSurvey
yeast
Banking3
SpanishSurvey
swiss-heads
skel2
ozone

F-AA
90.08
67.20
904.22
5.43
134.30
94.84
75.05
64.84
1532.12

AA CH-NMF C-NMF
-
-
2192.30
7.04
-
254.92
116.07
101.73
-

-
-
1239.67
9.18
-
117.91
87.06
77.78
-

72.35
58.97
902.07
5.02
131.81
93.51
74.67
64.87
1669.70

NMF
-
-
688.35
3.52
-
32.14
47.16
51.75
-

frame instead of computing it exactly, yields higher error
of approximately 20% while C-NMF is even worse.
In
summary, the error of F-AA is similar to standard AA and
much better than CH-NMF as well as C-NMF.

Usually, it is a-priori not known how to choose the latent
dimensionality p. A standard approach is to choose p
with respect to the so-called elbow criterion which requires
several runs for different values of p to be executed.
In
such a scenario our approximative approach is particularly
beneﬁcial. Frame-AA requires the computation of the
frame F before archetypes can be located. However, once
the frame is complete,
it can be used for ﬁnding any
number of archetypes. Hence, it is interesting to see the
cumulative time taken by the methods when evaluating
several conﬁgurations, say p = 4, 6, 8, . . . , 16.

This is depicted in Figure 6 for the USAFSurvey data set.
We report again on averages as well as standard errors on
36 repetitions executed on random initializations. Frame-
AA turns out fastest at the ﬁrst evaluation of p = 4 despite
the computation of the frame. Since the frame is static for
a data set, it can be reused for all remaining computations
and Frame-AA clearly outperforms its peers. Note that
the divide and conquer strategy proposed in Algorithm 5
leads to an additional speed-up that is not shown here.
The reconstruction error is shown in the bottom part of
Figure 6. Once again, Frame-AA yields a very similar
error as standard AA, which is computed on the whole data
set, while C-NMF and CH-NMF baselines perform much
worse.

5. Autoencoders

We consider a prototypical application of Frame-AA as an
autoencoder similar to the approach of Bauckhage et al.
(2015). However, instead of focusing on the reconstruction,
we are interested in the quality of the learned embedding
given by the weights of the convex combinations.

For this exemplary task we make use of the mnist data

Figure 6. Cumulative time for several evaluations of p as well as
reconstruction error for USAFSurvey data.

set. It consists of handwritten images of the digits zero to
nine which are of size 28 × 28 rendering the problem 784-
dimensional. We split every image into 4 × 4 sub-images
yielding 49 patches per image. Those patches are stacked-
up to form a new design matrix. For lack of space we focus
on a subtask and aim to separate digit 1 from digit 7. The
task is thus phrased as a standard binary classiﬁcation task.
We sample 500 images per class. The ﬁnal design matrix is
of size 49000 × 16 and its frame density is approximately
14%. The design matrix is then factorized as described
in Section 3.2. After the factorization, every patch is
described by p weights contained within the matrix A. The
weights of the convex combinations are then vectorized
yielding an embedding of size m = 49p per image.

As baseline we employ a deep learning autoencoder that
uses a convolutional layer (LeCun et al., 1989) to learn
intermediate representations of images. To perform a fair
comparison, the convolutional layer learns p ﬁlters of the
same size as Frame-AA, that is 4 × 4 and stride 4 × 4.
This layer has a sigmoid activation function and is followed
by an upsampling layer that recovers the original image
size. The last layer is another convolutional layer with one
ﬁlter of size 4 × 4, stride one, and sigmoid activation. We
optimize the network using squared loss as in the matrix
factorization. We compute an embedding analogously to
Frame-AA by representing an image by its output of the
ﬁrst convolutional layer, yielding the same embedding size
of m = 49p.

For every obtained embedding, an SVM is trained on p =
2, 4, . . . , 16 archetypes/ﬁlters meaning that every image is
now represented by a vector of size m = 98, 196, . . . , 784.
Note that the last one is identical to the original image
size. We deploy an RBF kernel and the parameters C and
γ are optimized on a 2−7, . . . , 27 grid respectively. For

Frame-based Data Factorizations

Figure 8. Archetypes (left) and ﬁlters (right) for p = 8.

we computed the frame by leveraging the well known
active-set method for non-negative least squares problems
called NNLS. We provided a theoretical underpinning for
our approach and conducted a series of experiments to
compare the computation of the frame with two LP-based
approaches to show our competitiveness.

Moreover, we proposed an approximation of Archetypal
Analysis called Frame-AA by restricting the optimization
of the archetypes to the frame. We showed that if a small
number of data points approximated the frame sufﬁciently,
the resulting approximation of AA on the whole data set
turned out appropriate. This approximation does not only
work for Archetype Analysis but also for all variants of
it like for instance Mørup and Hansen (2010), Chen et al.
(2014), and Bauckhage et al. (2015).

Empirically, we showed that the error of Frame-AA is only
slightly higher than standard Archetypal Analysis but was
often much faster once the frame has been computed. We
leveraged this observation and proposed an efﬁcient model
selection strategy to ﬁnd the optimal number of archetypes.
We proposed to compute the frame only once and then re-
used it for all subsequent computations for other numbers
of archetypes. We observed an enormous acceleration in
scenarios with low frame densities.

the data were extreme.

Low frame densities were attained when only small
portions of
On the other
hand, high frame densities diminish this speed-up and
the computational time converges to that of a standard
Archetypal Analysis. However, we argued that these
scenarios are not suited for Archetypal Analyses in the ﬁrst
case.

Furthermore, we compared Frame-AA to several state-
of-the-art baselines. Frame-AA either outperformed its
competitors or performed on-par. On the example of
an autoencoder, Frame-AA led to nicely interpretable
classiﬁcations in contrast to convolutional neural networks.

Figure 7. Predictive performance in terms of accuracy and
reconstruction error in terms of Frobenius norm for various
embedding sizes.

the hyperparameter search we use a hold out set consisting
of another 500 images per class that are disjoint from the
training set.

To evaluate the predictive performance we sample another
500 test images per class and obtain their embeddings using
the trained approaches described above. The results are
depicted in Figure 7. The top part of the ﬁgure depicts the
reconstruction errors in terms of the Frobenius norm. The
curve of Frame-AA has a typical elbow-structure as one
would expect, while the error of the net is almost static.
Although the net achieves smaller reconstruction errors,
Figure 7 (bottom) shows that the SVM effectively leverages
the representation learned by F-AA. The archetype-based
embedding leads to more accurate classiﬁcation models.

The learnt archetypes/ﬁlters are shown in Figure 8 for the
case of p = 8 yielding an embedding of size m = 392.
The archetypes in the left part of the ﬁgure resembles
edge and border detectors while the ﬁlters on the right
appear random. Moreover, every patch is represented
as a convex combination of the shown archetypes. That
is, the classiﬁcation of a patch can be explained by the
corresponding weights to render the results interpretable.

6. Conclusion

We proposed a novel method for computing the frame of a
data set. The frame is a subset of the data set containing
only the extreme points, i.e., the vertices of the convex hull
of the data. While standard approaches like Quickhull were
infeasible for scenarios with more than three dimensions,

Acknowledgements

This work has been funded in parts by the German Federal
Ministry of Education and Science BMBF under grant
QQM/01LSA1503C.

Frame-based Data Factorizations

Horton, Paul and Nakai, Kenta.

system for

classiﬁcation
localization sites of proteins.
pp. 109–115, 1996.

predicting

A probabilistic
cellular
the
In Ismb, volume 4,

Lawson, Charles L and Hanson, Richard J. Solving least

squares problems, volume 15. SIAM, 1995.

LeCun, Yann, Boser, Bernhard, Denker,

John S,
Henderson, Donnie, Howard, Richard E, Hubbard,
Wayne, and Jackel, Lawrence D. Backpropagation
applied to handwritten zip code recognition. Neural
computation, 1(4):541–551, 1989.

Lee, Daniel D and Seung, H Sebastian. Learning the parts
of objects by non-negative matrix factorization. Nature,
401(6755):788–791, 1999.

Lopez, Francisco-Javier. Generating random points (or
vectors) controlling the percentage of them that are
extreme in their convex (or positive) hull. Journal of
Mathematical Modelling and Algorithms, 4(2):219–234,
2005.

Mørup, Morten and Hansen, Lars Kai. Archetypal analysis
for machine learning. In Machine Learning for Signal
Processing (MLSP), 2010 IEEE International Workshop
on, pp. 172–177. IEEE, 2010.

Ottmann, Thomas, Schuierer, Sven, and Soundaralakshmi,
Subbiah.
Enumerating extreme points in higher
dimensions. Nordic Journal of Computing, 8(2):179–
192, 2001.

Paatero, Pentti and Tapper, Unto.

Positive matrix
factorization: A non-negative factor model with
optimal utilization of error estimates of data values.
Environmetrics, 5(2):111–126, 1994.

Thurau, Christian, Kersting, Kristian, Wahabzada,
Mirwaes, and Bauckhage, Christian.
Convex non-
negative matrix factorization for massive datasets.
Knowledge and information systems, 29(2):457–478,
2011.

Vinu´e, Guillermo. Anthropometry: An R package for
analysis of anthropometric data. Journal of Statistical
Software, 77(6):1–40, 2017.

References

Arora, Raman, Gupta, Maya, Kapila, Amol,

and
Fazel, Maryam. Clustering by left-stochastic matrix
factorization. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pp. 761–
768, 2011.

Arora, Raman, Gupta, Maya R, Kapila, Amol, and Fazel,
Maryam. Similarity-based clustering by left-stochastic
Journal of Machine Learning
matrix factorization.
Research, 14(1):1715–1746, 2013.

Barber, C Bradford, Dobkin, David P, and Huhdanpaa,
Hannu. The quickhull algorithm for convex hulls. ACM
Transactions on Mathematical Software (TOMS), 22(4):
469–483, 1996.

Bauckhage, C, Kersting, HF, Thurau, C, et al. Archetypal
In Workshop New
8.

analysis as an autoencoder.
Challenges in Neural Computation 2015, pp.
Citeseer, 2015.

Brondsted, Arne. An introduction to convex polytopes,
volume 90. Springer Science & Business Media, 2012.

Chen, Yuansi, Mairal,

Julien, and Harchaoui, Zaid.
Fast and robust archetypal analysis for representation
In Proceedings of the IEEE Conference on
learning.
Computer Vision and Pattern Recognition, pp. 1478–
1485, 2014.

Cutler, Adele and Breiman, Leo. Archetypal analysis.

Technometrics, 36(4):338–347, 1994.

Ding, Chris HQ, Li, Tao, and Jordan, Michael

I.
Convex and semi-nonnegative matrix factorizations.
IEEE transactions on pattern analysis and machine
intelligence, 32(1):45–55, 2010.

Dul´a, Jos´e H and Helgason, Richard V. A new procedure
for identifying the frame of the convex hull of a
ﬁnite collection of points in multidimensional space.
European Journal of Operational Research, 92(2):352–
367, 1996.

Dul´a, Jos´e H and L´opez, Francisco J. Competing output-
sensitive frame algorithms. Computational Geometry,
45(4):186–197, 2012.

Epifanio,

Irene, Vinu´e, G, and Alemany, Sandra.
Archetypal analysis:
for estimating
contributions
boundary cases in multivariate accommodation problem.
Computers & Industrial Engineering, 64(3):757–765,
2013.

Heinz, Grete, Peterson, Louis J, Johnson, Roger W,
and Kerk, Carter J. Exploring relationships in body
Journal of Statistics Education, 11(2),
dimensions.
2003.

