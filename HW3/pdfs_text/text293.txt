Simultaneous Learning of Trees and Representations for Extreme Classiﬁcation and Density Estimation

Simultaneous Learning of Trees and Representations for Extreme Classiﬁcation
with Application to Language Modeling
(Supplementary material)

9. Geometric interpretation of probabilities p(n)

and p(n)
j|i

j

Discrete:
p(n)
1 = 6
p(n)
1|1 = 3

12 = 0.5
3 = 1,

p(n)
1|2 = 3

3 = 1,

p(n)
1|3 = 0

3 = 0,

p(n)
1|4 = 0

3 = 0

Continuous:
p(n)
1 = 1
p(n)
1|1 = 1
p(n)
1|2 = 1
p(n)
1|3 = 1
p(n)
1|4 = 1

12 (σ(100) + σ(70) + . . . + σ(−70) + σ(−100)) ≈ 0.5
3 (σ(100) + σ(70) + σ(100)) ≈ 1
3 (σ(100) + σ(70) + σ(100)) ≈ 1
3 (σ(−100) + σ(−70) + σ(−100)) ≈ 0
3 (σ(−100) + σ(−70) + σ(−100)) ≈ 0

Figure 3. The comparison of discrete and continuous deﬁnitions of probabilities p(n)
and binary tree (M = 2). n is an exemplary node, e.g. root. σ denotes sigmoid function. Color circles denote data points.

j|i on a simple example with K = 4 classes

and p(n)

j

Remark 3. One could deﬁne p(n)
as the ratio of the number of examples that reach node n and are sent to its jth child
j
to the total the number of examples that reach node n and p(n)
j|i as the ratio of the number of examples that reach node n,
correspond to label i, and are sent to the jth child of node n to the total the number of examples that reach node n and
correspond to label i. We instead look at the continuous counter-parts of these discrete deﬁnitions as given by Equations 8
and 9 and illustrated in Figure 3 (note that continuous deﬁnitions have elegant geometric interpretation based on margins),
which simpliﬁes the optimization problem.

10. Theoretical proofs

Proof of Lemma 1. Recall the form of the objective deﬁned in 6:

Where:

Jn =

K
(cid:88)

i=1

q(n)
i

(cid:16) M
(cid:88)

j=1

|p(n)

(cid:17)
j − p(n)
j|i |

=

(cid:104)

E

i∼q(n)

n (i, p(n)
f J

·|· , q(n))

(cid:105)

2
M

2
M

M
(cid:88)

j=1

M
(cid:88)

n (i, p(n)
f J

·|· , q(n)) =

(cid:12)
j − p(n)
(cid:12)p(n)
(cid:12)

j|i

(cid:12)
(cid:12)
(cid:12) =

M
(cid:88)

j=1

(cid:12)
(cid:12)p(n)
(cid:12)
j|i −

K
(cid:88)

i(cid:48)=1

i(cid:48) p(n)
q(n)

j|i(cid:48)

(cid:12)
(cid:12)
(cid:12)

=

K
(cid:88)

(cid:12)
(cid:12)
(cid:12)

j=1

i(cid:48)=1

(1i=i(cid:48) − q(n)

i(cid:48) )p(n)

j|i(cid:48)

(cid:12)
(cid:12)
(cid:12)

K1K2K3K410010070100100707070100100100100hSimultaneous Learning of Trees and Representations for Extreme Classiﬁcation and Density Estimation

Hence:

And:

Thus:

And:

Hence:

By assigning each label j to a speciﬁc child i under the constraint that no child has more than L labels, we take a step in
the direction ∂E ∈ {0, 1}M ×K, where:

·|· , q(n))

∂f J

n (i, p(n)
∂p(n)
j|i

= (1 − q(n)

i

) sign(p(n)

j|i − p(n)

j

)

∂f J

·|· , q(n))

n (i, p(n)
∂ log p(n)
j|i

= (1 − q(n)

) sign(p(n)

j|i − p(n)

j

)

= (1 − q(n)

) sign(p(n)

j|i − p(n)

j

)p(n)
j|i

i

i

∂p(n)
j|i
∂ log p(n)
j|i

∀i ∈ [1, K], (cid:80)M

∀j ∈ [1, M ], (cid:80)K

j=1 ∂Ej,i = 1
and
i=1 ∂Ej,i ≤ L

∂Jn
∂p(n)
·|·

∂E =

(cid:105)
·|· , q(n))

(cid:104)
n (i, p(n)
f J
∂p(n)
·|·

∂E

E

i∼q(n)

2
M

2
M

K
(cid:88)

i=1

=

q(n)
i

(1 − q(n)

)

i

sign(p(n)

j|i − p(n)

j

)∂Ej,i

∂Jn
∂ log p(n)
·|·

∂E =

2
M

K
(cid:88)

i=1

q(n)
i

(1 − q(n)

)

i

sign(p(n)

j|i − p(n)

j

)p(n)

j|i ∂Ej,i

M
(cid:88)

(cid:16)

j=1

M
(cid:88)

(cid:16)

j=1

(cid:17)

(cid:17)

(13)

(14)

(15)

If there exists such an assignment for which 13 is positive, then the greedy method proposed in 2 ﬁnds it. Indeed, suppose
that Algorithm 2 assigns label i to child j and i(cid:48) to j(cid:48). Suppose now that another assignment ∂E(cid:48) sends i to j(cid:48) and i to j(cid:48).
Then:

(cid:16)

∂E − ∂E(cid:48)(cid:17)

=

∂Jn
∂p(n)
·|·

(cid:16) ∂Jn
∂p(n)
j|i

+

∂Jn
∂p(n)
j(cid:48)|i(cid:48)

(cid:17)

−

(cid:16) ∂Jn
∂p(n)
j|i(cid:48)

+

(cid:17)

∂Jn
∂p(n)
j(cid:48)|i

Since the algorithm assigns children by descending order of ∂Jn
∂p(n)
j|i

until a child j is full, we have:

∂Jn
∂p(n)
j|i

≥

∂Jn
∂p(n)
j|i(cid:48)

and

∂Jn
∂p(n)
(cid:48)j|i(cid:48)

≥

∂Jn
∂p(n)
j(cid:48)|i

(cid:16)

∂E − ∂E(cid:48)(cid:17)

≥ 0

∂Jn
∂p(n)
·|·

Thus, the greedy algorithm ﬁnds the assignment that most increases Jn most under the children size constraints.
Moreover, ∂Jn
∂p(n)
·|·

is always positive for L ≤ M or L ≥ 2M (M − 2).

Proof of Lemma 2. Both Jn and JT are deﬁned as the sum of non-negative values which gives the lower-bound. We next
derive the upper-bound on Jn. Recall:

Jn =

2
M

M
(cid:88)

K
(cid:88)

j=1

i=1

q(n)
i

|p(n)

j − p(n)

j|i | =

2
M

M
(cid:88)

K
(cid:88)

j=1

i=1

q(n)
i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

K
(cid:88)

l=1

q(n)
l p(n)

j|l − p(n)

j|i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Simultaneous Learning of Trees and Representations for Extreme Classiﬁcation and Density Estimation

j = (cid:80)K
l p(n)
since p(n)
following two sets of indices:

l=1 q(n)

j|l . The objective Jn is maximized on the extremes of the [0, 1] interval. Thus, deﬁne the

Oj = {i : i ∈ {1, 2, . . . , K}, p(n)

j|i = 1}

and

Zj = {i : i ∈ {1, 2, . . . , K}, p(n)

j|i = 0}.

We omit indexing these sets with n for the ease of notation. We continue as follows



q(n)
i

1 −

(cid:88)

l∈Oj





q(n)
l

 +

(cid:88)

q(n)
i

(cid:88)

q(n)
l



i∈Zj

l∈Oj










M
(cid:88)

(cid:88)

j=1

i∈Oj

M
(cid:88)

(cid:88)

j=1

i∈Oj

Jn ≤

2
M

4
M

4
M

=

=

q(n)
i −

2



q(n)
i










(cid:88)

i∈Oj




1 −

M
(cid:88)





(cid:88)

j=1

i∈Oj

2



q(n)
i




 ,









M
(cid:88)

j=1

1
M

(cid:88)

i∈Oj




2

q(n)
i





Jn ≤

− 4

4
M

4
M

=

(cid:18)

1 −

(cid:19)

1
M

where the last inequality is the consequence of the following: (cid:80)M
thus (cid:80)M
j=1

q(n)
i = 1. Apllying Jensen’s ineqality to the last inequality obtained gives

j = 1 and p(n)

j=1 p(n)

j = (cid:80)K

(cid:80)

i∈Oj

l=1 q(n)

l p(n)

j|l = (cid:80)

i∈Oj

q(n)
i

,

That ends the proof.

Proof of Lemma 3. We start from proving that if the split in node n is perfectly balanced, i.e. ∀j={1,2,...,M }p(n)
perfectly pure, i.e. ∀j={1,2,...,M }
i={1,2,...,K}

j|i ) = 0, then Jn admits the highest value Jn = 4

j|i , 1 − p(n)

min(p(n)

(cid:0)1 − 1

M

M

j = 1

M , and
(cid:1). Since the

split is maximally balanced we write:

Jn =

2
M

M
(cid:88)

K
(cid:88)

j=1

i=1

q(n)
i

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
M

− p(n)
j|i

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

Since the split is maximally pure, each p(n)
of indices:

j|i can only take value 0 or 1. As in the proof of previous lemma, deﬁne two sets

Oj = {i : i ∈ {1, 2, . . . , K}, p(n)

j|i = 1}

and

Zj = {i : i ∈ {1, 2, . . . , K}, p(n)

j|i = 0}.

We omit indexing these sets with n for the ease of notation. Thus

Jn =

(cid:88)

q(n)
i

1 −

(cid:88)

+

q(n)
i

(cid:18)

(cid:18)

(cid:19)

(cid:19)

1
M

1
M

j=1

i∈Oj

(cid:88)

q(n)
i

1 −

j=1

i∈Oj





1
M

i∈Zj



+

1
M

1 −

(cid:88)

i∈Oj





q(n)
i





M
(cid:88)

M
(cid:88)









(cid:18)

(cid:18)

1 −

1 −

2
M

2
M

2
M

4
M

=

=

=

(cid:19) M
(cid:88)

(cid:88)

j=1

i∈Oj

q(n)
i +

2
M

2
M

1
M

(cid:19)

,

Simultaneous Learning of Trees and Representations for Extreme Classiﬁcation and Density Estimation

where the last equality comes from the fact that (cid:80)M
(cid:80)M

(cid:80)

j=1

i∈Oj

q(n)
i = 1.

j=1 p(n)

j = 1 and p(n)

j = (cid:80)K

l=1 q(n)

l p(n)

j|l = (cid:80)

i∈Oj

q(n)
i

, thus

Thus we are done with proving one induction direction. Next we prove that if Jn admits the highest value Jn =
(cid:0)1 − 1
4
M , and perfectly pure, i.e.
M
∀j={1,2,...,M }
i={1,2,...,K}

(cid:1), then the split in node n is perfectly balanced, i.e. ∀j={1,2,...,M }p(n)

j|i , 1 − p(n)

j = 1

min(p(n)

j|i ) = 0.

M

Without loss of generality assume each q(n)
interval [0, 1], where each p(n)
p(n)
j|i ’s are 1. The function J(h) is differentiable in these extremes. Next, deﬁne three sets of indices:

∈ (0, 1). The objective Jn is certainly maximized in the extremes of the
j|i ’s are 0 or all

j|i is either 0 or 1. Also, at maximum it cannot be that for any given j, all p(n)

i

Aj = {i :

i p(n)
q(n)

j|l ≥ p(n)
j|i }

and

Bj = {i :

i p(n)
q(n)

j|l < p(n)
j|i }

and

Cj = {i :

i p(n)
q(n)

j|l > p(n)

j|i }.

K
(cid:88)

l=1

K
(cid:88)

l=1

K
(cid:88)

l=1

We omit indexing these sets with n for the ease of notation. Objective Jn can then be re-written as

Jn =

M
(cid:88)





2
M

(cid:32) K
(cid:88)

(cid:88)

q(n)
i

j=1

i∈Aj

l=1

i p(n)
q(n)

j|l − p(n)

j|i

+ 2

q(n)
i

p(n)
j|i −

i p(n)
q(n)

j|l

(cid:33)

(cid:32)

(cid:88)

i∈Bj

K
(cid:88)

l=1

(cid:33)
 ,

We next compute the derivatives of Jn with respect to p(n)
differentiable and obtain

j|z , where z = {1, 2, . . . , K}, everywhere where the function is

(cid:40)

=

∂Jn
∂p(n)
j|z

2q(n)
z
2q(n)
z

((cid:80)
i∈Cj
(1 − (cid:80)

q(n)
i − 1)
q(n)
)
i

i∈Bj

if z ∈ Cj
if z ∈ Bj

,

Note that in the extremes of the interval [0, 1] where Jn is maximized, it cannot be that (cid:80)
q(n)
i = 1
thus the gradient is non-zero. This fact and the fact that Jn is convex imply that Jn can only be maximized at the extremes
of the [0, 1] interval. Thus if Jn admits the highest value, then the node split is perfectly pure. We still need to show that
if Jn admits the highest value, then the node split is also perfectly balanced. We give a proof by contradiction, thus we
assume that at least for one value of j, p(n)
M + xj, then at
least for one value of j, xj (cid:54)= 0. Lets once again deﬁne two sets of indices (we omit indexing xj and these sets with n for
the ease of notation):

M , or in other words if we decompose each p(n)

i = 1 or (cid:80)
q(n)

j = 1

as p(n)

(cid:54)= 1

i∈Bj

i∈Cj

j

j

Oj = {i : i ∈ {1, 2, . . . , K}, p(n)

j|i = 1}

and

Zj = {i : i ∈ {1, 2, . . . , K}, p(n)

j|i = 0},

Simultaneous Learning of Trees and Representations for Extreme Classiﬁcation and Density Estimation

and recall that p(n)

j = (cid:80)K

l=1 q(n)

l p(n)

j|l = (cid:80)

i∈Oj

q(n)
i

. We proceed as follows

4
M

(cid:18)

1 −

(cid:19)

1
M

= Jn =

M
(cid:88)

(cid:88)





j=1

i∈Oj

q(n)
i

(1 − p(n)

) +

j

i p(n)
q(n)

j



(cid:88)

i∈Zj



(cid:104)
p(n)
j

(1 − p(n)

) + p(n)

(1 − p(n)

j

j

j

(cid:105)
)

2
M

2
M

4
M

4
M

4
M

4
M

4
M

M
(cid:88)

j=1

M
(cid:88)

j=1


1 −



1 −



1 −

(cid:18)

1 −

M
(cid:88)

j=1

M
(cid:88)

j=1

1
M

1
M

(cid:19)

=

=

=

=

=

<

(cid:104)
j − (p(n)
p(n)

j

)2(cid:105)



(p(n)
j

)2



(cid:19)2


+ xj

(cid:18) 1
M

−

2
M

M
(cid:88)

j=1

xj −



x2
j



M
(cid:88)

j=1

Thus we obtain the contradiction which ends the proof.

Proof of Lemma 4. Since we node that the split is perfectly pure, then each p(n)

j|i is either 0 or 1. Thus we deﬁne two sets

Oj = {i : i ∈ {1, 2, . . . , K}, p(n)

j|i = 1}

and

Zj = {i : i ∈ {1, 2, . . . , K}, p(n)

j|i = 0}.

Jn =

2
M

M
(cid:88)

(cid:88)





j=1

i∈Oj



q(n)
i

(1 − pj) +

q(n)
i pj



(cid:88)

i∈Zj

Note that pj = (cid:80)

q(n)
i

. Then

i∈Oj

Jn =

[pj (1 − pj) + (1 − pj)pj] =

pj (1 − pj) =

1 −

4
M

M
(cid:88)

j=1



4
M



p2
j



M
(cid:88)

j=1

2
M

M
(cid:88)

j=1

and thus

and thus

Lets express pj as pj = 1

M + (cid:15)j, where (cid:15)j ∈ [− 1

M , 1 − 1

M ]. Then

M
(cid:88)

j=1

p2
j =

M
(cid:88)

j=1

(cid:18) 1
M

(cid:19)2

+ (cid:15)j

=

1
M

+

2
M

M
(cid:88)

j=1

(cid:15)j +

(cid:15)2
j =

M
(cid:88)

j=1

1
M

+

M
(cid:88)

j=1

(cid:15)2
j ,

since 2
M

(cid:80)M

j=1 (cid:15)j = 0. Thus combining Equation 16 and 17

M
(cid:88)

j=1

p2
j = 1 −

M Jn
4

.

1
M

+

M
(cid:88)

j=1

(cid:15)2
j = 1 −

M Jn
4

(16)

(17)

Simultaneous Learning of Trees and Representations for Extreme Classiﬁcation and Density Estimation

and thus

The last statement implies that

which is equivalent to

M
(cid:88)

j=1

(cid:15)2
j = 1 −

1
M

−

M Jn
4

.

max
j=1,2,...,M

(cid:15)j ≤

(cid:114)

1 −

−

1
M

M Jn
4

,

min
j=1,2,...,M

pj =

− max
j

(cid:15)j ≥

1
M

(cid:114)

1
M

−

1 −

−

1
M

M Jn
4

=

−

1
M

(cid:112)M (J ∗ − Jn)
2

.

Proof of Lemma 5. Since the split is perfectly balanced we have the following:

Jn =

2
M

M
(cid:88)

K
(cid:88)

j=1

i=1

q(n)
i

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
M

− p(n)
j|i

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2
M

K
(cid:88)

M
(cid:88)

i=1

j=1

q(n)
i

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
M

− p(n)
j|i

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Deﬁne two sets

Then

Ai = {j : j ∈ {1, 2, . . . , K}, p(n)

j|i <

and

Bi = {j : j ∈ {1, 2, . . . , K}, p(n)

j|i ≥

1
M

}

1
M

}.

Jn =

2
M

2
M

2
M

K
(cid:88)

i=1

K
(cid:88)

i=1

K
(cid:88)

i=1





(cid:88)

q(n)
i

j∈Ai


q(n)
i



q(n)
i

(cid:88)

j∈Ai

(cid:88)

j∈Ai





(cid:18) 1
M

(cid:18) 1
M

(cid:18) 1
M

=

=

(cid:19)





1
M

(cid:19)

− p(n)
j|i

(cid:88)

+

j∈Bi

(cid:18)

q(n)
i

p(n)
j|i −

− p(n)
j|i

(cid:19)

+

(cid:18)

(cid:88)

p(n)
j|i −

j∈Bi

(cid:19)





1
M

(cid:19)

+

(cid:18)

(cid:88)

(1 −

− p(n)
j|i

1
M

j∈Bi

) − (1 − p(n)
j|i )

(cid:19)





Recall that the optimal value of Jn is:

J ∗ =

4
M

(cid:18)

1 −

(cid:19)

=

1
M

2
M

N
(cid:88)

i=1

(cid:20)

q(n)
i

(M − 1)

+

1 −

(cid:18)

1
M

(cid:19)(cid:21)

=

1
M

2
M

N
(cid:88)

i=1





q(n)
i





(cid:88)

j∈Ai∪Bi



 −

1
M

1
M

(cid:18)

+

1 −

(cid:19)





1
M

Note Ai can have at most M − 1 elements. Furthermore, ∀j ∈ Ai, p(n)


j|i < 1 − p(n)

j|i . Then, we have:

J ∗ − J n =

2
M

K
(cid:88)

i=1

(cid:88)

q(n)
i



p(n)
j|i +

j∈Ai

j∈Bi

(cid:18)

(cid:88)

(1 − p(n)

j|i ) +

− (1 −

1
M

(cid:19)

)

−

1
M

1
M

(cid:18)

+

1 −

(cid:19)





1
M

Hence, since Bi has at least one element:

J ∗ − J n ≥

q(n)
i

2
M

2
M

K
(cid:88)

i=1

K
(cid:88)

i=1

≥

≥ 2α









(cid:88)

j∈Ai

M
(cid:88)

j=1

p(n)
j|i +

(cid:88)

(cid:16)

j∈Bi

1 − p(n)
j|i

(cid:17)







q(n)
i

min(p(n)

j|i , 1 − p(n)
j|i )



Simultaneous Learning of Trees and Representations for Extreme Classiﬁcation and Density Estimation

Proof of Theorem 1. Let the weight of the tree leaf be deﬁned as the probability that a randomly chosen data point x drawn
from some ﬁxed target distribution P reaches this leaf. Suppose at time step t, n is the heaviest leaf and has weight w.
Consider splitting this leaf to M children n1, n2, . . . , nM . Let the weight of the jth child be denoted as wj. Also for the
ease of notation let pj refer to p(n)
j=1 pj = 1) and pj|i refer to p(n)
j|i , and furthermore let qi be the shorthand
. Recall that pj = (cid:80)K
for q(n)
i=1 qi = 1. Notice that for any j = {1, 2, . . . , M }, wj = wpj. Let q
i
be the k-element vector with ith entry equal to qi. Deﬁne the following function: ˜Ge(q) = (cid:80)K
. Recall the

(recall that (cid:80)m
i=1 qipj|i and (cid:80)K

(cid:17)

j

i=1 qi ln

(cid:16) 1
qi

(cid:80)K

expression for the entropy of tree leaves: Ge = (cid:80)

l∈L wl
= qipj|i
the split the contribution of node n to Ge was equal to w ˜Ge(q). Note that for any j = {1, 2, . . . , M }, q(nj )
pj
the probability that a randomly chosen x drawn from P has label i given that x reaches node nj. For brevity, let qnj
i be
denoted as qj,i. Let qj be the k-element vector with ith entry equal to qj,i. Notice that q = (cid:80)M
j=1 pjqj. After the split
the contribution of the same, now internal, node n changes to w (cid:80)M
j=1 pj ˜Ge(qj). We denote the difference between the
contribution of node n to the value of the entropy-based objectives in times t and t + 1 as

, where L is a set of all tree leaves. Before

i=1 q(l)

1
q(l)
i

ln

is

i

i

(cid:18)

(cid:19)

∆e

t := Ge

t − Ge

t+1 = w


 ˜Ge(q) −


pj ˜Ge(qj)

 .

M
(cid:88)

j=1

(18)

The entropy function ˜Ge is strongly concave with respect to l1-norm with modulus 1, thus we extend the inequality given
by Equation 7 in (Choromanska et al., 2016) by applying Theorem 5.2. from (Azocar et al., 2011) and obtain the following
bound

∆e

t = w


 ˜Ge(q) −


pj ˜Ge(qj)


M
(cid:88)

j=1

≥ w

pj(cid:107)qj −

M
(cid:88)

plql(cid:107)2
1

1
2

1
2

1
2

1
2

1
2

M
(cid:88)

j=1

M
(cid:88)

j=1

M
(cid:88)

j=1

M
(cid:88)

j=1

M
(cid:88)

j=1

= w

= w

= w

= w

l=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

qipj|i
pj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

pj|i
pj

pj|i
pj

qi

qi

(cid:32) K
(cid:88)

i=1
(cid:32) K
(cid:88)

i=1
(cid:32) K
(cid:88)

i=1
(cid:32) K
(cid:88)

i=1

pj

pj

pj

1
pj

(cid:33)2

(cid:12)
(cid:12)pj|i − pj

(cid:12)
(cid:12)

qi

.

M
(cid:88)

−

pl

qipl|i
pl

(cid:33)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

l=1

M
(cid:88)

l=1

−

pl|i

(cid:12)
(cid:33)2
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

− 1

γ ∈

(cid:20) M
2

min
j=1,2,...,M

pj, 1 −

min
j=1,2,...,M

pj

M
2

(cid:21)

,

Before proceeding, we will bound each pj. Note that by the Weak Hypothesis Assumption we have

thus

thus all pjs are such that pj ≥ 2γ

M . Thus

min
j=1,2,...,M

pj ≥

2γ
M

,

max
j=1,2,...,M

pj ≤ 1 −

(M − 1) =

2γ
M

M (1 − 2γ) + 2γ
M

.

Simultaneous Learning of Trees and Representations for Extreme Classiﬁcation and Density Estimation

Thus all pjs are such that pj ≤ M (1−2γ)+2γ

.

M

∆e

t ≥ w

M 2
2[(M (1 − 2γ) + 2γ]

≥ w

M 2
2[(M (1 − 2γ) + 2γ]

= w

M 2
8[(M (1 − 2γ) + 2γ]

=

M 2
[(M (1 − 2γ) + 2γ]

wJ 2
n
8

,

(cid:32) K
(cid:88)

1
M

M
(cid:88)

j=1


i=1

K
(cid:88)

M
(cid:88)

1
M

j=1

i=1

2
M

M
(cid:88)

K
(cid:88)

j=1

i=1







(cid:33)2



2



2

(cid:12)
(cid:12)pj|i − pj

(cid:12)
(cid:12)

qi

(cid:12)
(cid:12)pj|i − pj

(cid:12)
(cid:12)

qi



(cid:12)
(cid:12)pj|i − pj

(cid:12)
(cid:12)

qi



where the last inequality is a consequence of Jensen’s inequality. w can further be lower-bounded by noticing the following

Ge

t =

(cid:88)

wl

K
(cid:88)

q(l)
i

ln

l∈L

i=1

(cid:32)

(cid:33)

1
q(l)
i

≤

(cid:88)

l∈L

(cid:88)

l∈L

wl ln K ≤ w ln K

1 = [t(M − 1) + 1]w ln K ≤ (t + 1)(M − 1)w ln K,

where the ﬁrst inequality results from the fact that uniform distribution maximizes the entropy.

This gives the lower-bound on ∆e

t of the following form:

and by using Weak Hypothesis Assumption we get

∆e

t ≥

M 2Ge
8(t + 1)[M (1 − 2γ) + 2γ](M − 1) ln K

t J 2
n

,

∆e

t ≥≥

M 2Ge
8(t + 1)[M (1 − 2γ) + 2γ](M − 1) ln K

t γ2

Following the recursion of the proof in Section 3.2 in (Choromanska et al., 2016) (note that in our case Ge
1) ln K), we obtain that under the Weak Hypothesis Assumption, for any κ ∈ [0, 2(M − 1) ln K], to obtain Ge
sufﬁces to make

1 ≤ 2(M −
t ≤ κ it

t ≥

(cid:18) 2(M − 1) ln K
κ

(cid:19) 16[M (1−2γ)+2γ](M −1) ln K
M 2 log2 eγ2

splits. We next proceed to directly proving the error bound. Denote w(l) to be the probability that a data point x reached
leaf l. Recall that q(l)
i =
i
P (y(x) = i|x reached l). Let the label assigned to the leaf be the majority label and thus lets assume that the leaf is
assigned to label i if and only if the following is true ∀z={1,2,...,k}

is the probability that the data point x corresponds to label i given that x reached l, i.e. q(l)

z . Therefore we can write that

i ≥ q(l)
q(l)

z(cid:54)=i

(cid:15)(T ) =

P (t(x) = i, y(x) (cid:54)= i)

w(l)

P (t(x) = i, y(x) (cid:54)= i|x reached l)

K
(cid:88)

i=1

K
(cid:88)

i=1

K
(cid:88)

i=1

(cid:88)

l∈L

(cid:88)

l∈L

(cid:88)

l∈L
(cid:88)

l∈L

=

=

=

=

w(l)

P (y(x) (cid:54)= i|t(x) = i, x reached l)P (t(x) = i|x reached l)

w(l)(1 − max(q(l)

1 , q(l)

2 , . . . , q(l)

K ))

P (t(x) = i|x reached l)

K
(cid:88)

i=1

w(l)(1 − max(q(l)

1 , q(l)

2 , . . . , q(l)

K ))

(19)

(20)

Simultaneous Learning of Trees and Representations for Extreme Classiﬁcation and Density Estimation

Consider again the Shannon entropy G(T ) of the leaves of tree T that is deﬁned as

Let il = arg maxi={1,2,...,K} q(l)

i

. Note that

Ge(T ) =

w(l)

q(l)
i

log2

(cid:88)

l∈L

K
(cid:88)

i=1

1
q(l)
i

.

Ge(T ) =

w(l)

q(l)
i

log2

1
q(l)
i

1
q(l)
i

K
(cid:88)

i=1

K
(cid:88)

i=1
i(cid:54)=il

K
(cid:88)

i=1
i(cid:54)=il

w(l)

q(l)
i

log2

w(l)

q(l)
i

(cid:88)

l∈L

(cid:88)

l∈L

(cid:88)

l∈L

(cid:88)

≥

≥

=

l∈L
= (cid:15)(T ),

w(l)(1 − max(q(l)

1 , q(l)

2 , . . . , q(l)

K ))

(21)

(22)

where the last inequality comes from the fact that ∀i={1,2,...,K}

q(l)
i ≤ 0.5 and thus ∀i={1,2,...,K}

1
q(l)
i

i(cid:54)=il

i(cid:54)=il

∈ [2; +∞] and

consequently ∀i={1,2,...,K}

log2

∈ [1; +∞].

1
q(l)
i

i(cid:54)=il

We next use the proof of Theorem 6 in (Choromanska et al., 2016). The proof modiﬁes only slightly for our purposes and
thus we only list these modiﬁcations below.

• Since we deﬁne the Shannon entropy through logarithm with base 2 instead of the natural logarithm, the right hand
ln 2 and

side of inequality (2.6) in (Shalev-Shwartz, 2012) should have an additional multiplicative factor equal to 1
thus the right-hand side of the inequality stated in Lemma 14 has to have the same multiplicative factor.

• For the same reason as above, the right-hand side of the inequality in Lemma 9 should take logarithm with base 2 of

k instead of the natural logarithm of k.

Propagating these changes in the proof of Theorem 6 results in the statement of Theorem 1.

Proof of Corollary 1. Note that the lower-bound on ∆e

t from the previous prove could be made tighter as follows:

∆e

t ≥ w

(cid:32) K
(cid:88)

1
2

M
(cid:88)

j=1

1
pj

(cid:33)2

(cid:12)
(cid:12)pj|i − pj

(cid:12)
(cid:12)

qi

= w

≥ w

= w

M 2
2

M 2
2

M 2
8







i=1
(cid:32) K
(cid:88)

1
M

M
(cid:88)

j=1


i=1

K
(cid:88)

M
(cid:88)

1
M

j=1

i=1

2
M

M
(cid:88)

K
(cid:88)

j=1

i=1

=

M 2wJ 2
n
8

,

(cid:33)2



2



2

(cid:12)
(cid:12)pj|i − pj

(cid:12)
(cid:12)

qi

(cid:12)
(cid:12)pj|i − pj

(cid:12)
(cid:12)

qi



(cid:12)
(cid:12)pj|i − pj

(cid:12)
(cid:12)

qi



Simultaneous Learning of Trees and Representations for Extreme Classiﬁcation and Density Estimation

d

Model

Arity Prec Rec Train Test

TagSpace

FastText

50

Huffman Tree

Learned Tree

TagSpace

FastText

200

Huffman Tree

Learned Tree

-

2

5
20

5
20

-

2

5
20

5
20

30.1

-

3h8

6h

27.2

4.17

8m 1m

28.3
29.9

31.6
32.1

35.6

35.2

35.8
36.4

36.1
36.6

4.33
4.58

4.85
4.92

8m 1m
10m 3m

18m 1m
30m 3m

-

5h32

15h

5.4

12m 1m

5.5
5.59

5.53
5.61

13m 2m
18m 3m

35m 3m
45m 8m

Table 3. Classiﬁcation performance on the YFCC100M dataset.

Model

perp.

train ms/batch

test ms/batch

Random Tree

Flat soft-max

Learned Tree

172

151

159

5.1

11.5

6.3

2.7

5.1

2.6

Table 4. Comparison of a ﬂat soft-max to a 25-ary hierarchical soft-max (learned, random and heuristic-based tree).

where the ﬁrst inequality was taken from the proof of Theorem 1 and the following equality follows from the fact that each
node is balanced. By next following exactly the same steps as shown in the proof of Theorem 1 we obtain the corollary.

11. Experimental Setting

11.1. Classiﬁcation

For the YFCC100M experiments, we learned our models with SGD with a linearly decreasing rate for ﬁve epochs. We run
a hyper-parameter search on the learning rate (in {0.01, 0.02, 0.05, 0.1, 0.25, 0.5}). In the learned tree settings, the learning
rate stays constant for the ﬁrst half of training, during which the AssignLabels() routine is called 50 times. We run the
experiments in a Hogwild data-parallel setting using 12 threads on an Intel Xeon E5-2690v4 2.6GHz CPU. At prediction
time, we perform a truncated depth ﬁrst search to ﬁnd the most likely label (using the same idea as in a branch-and-bound
algorithm: if a node score is less than that of the best current label, then all of its descendants are out).

11.2. Density Estimation

In our experiments, we use a context window size of 4. We optimize the objectives with Adagrad, run a hyper-parameter
search on the batch size (in {32, 64, 128}) and learning rate (in {0.01, 0.02, 0.05, 0.1, 0.25, 0.5}). The hidden represen-
tation dimension is 200. In the learned tree settings, the AssignLabels() routine is called 50 times per epoch. We used a
12GB NVIDIA GeForce GTX TITAN GPU and all tree-based models are 65-ary for the Gutenberg data and 25-ary for
Pen TreeBank. Table 4 provides the perplexity and speed results on the PTB text.

For the Cluster Tree, we learn dimension 50 word embeddings with FastTree for 5 epochs using a hierarchical softmax loss,
then obtain 45 = 652 centroids using the ScikitLearn implementation of MiniBatchKmeans, and greedily assign words to
clusters until full (when a cluster has 65 words).

Simultaneous Learning of Trees and Representations for Extreme Classiﬁcation and Density Estimation

Algorithm 3 Label Assignment Algorithm under Depth Constraint

Input Node statistics, max depth D

Paths from root to labels: P = (ci)K
node ID n and depth d
List of labels currently reaching the node

i=1

Ouput Updated paths

Lists of labels now assigned to each of n’s
children under depth constraints

j|i . (cid:12) is the element-wise

procedure AssignLabels (labels, n, d)

j

and p(n)

// ﬁrst, compute p(n)
// multiplication
pavg
0 ← 0
count ← 0
for i in labels do
pavg
0 ← pavg
count ← count + Countsn,i
pavg
pavg
0 ← pavg

0 /count

0 + SumProbasn,i

i ← SumProbasn,i/Countsn,i

// then, assign each label to a child of n under depth
// constraints
unassigned ← labels
full ← ∅
for j = 1 to M do
assignedj ← ∅

while unassigned (cid:54)= ∅ do

(cid:46)(cid:46) ∂Jn
∂p(n)
j|i

is given in Equation 10

(cid:18)

(cid:19)

∂Jn
∂p(n)
j|i

(i∗, j∗) ← argmax

i∈unassigned,j(cid:54)∈full

ci∗
d ← (n, j∗)
assignedj∗ ← assignedj∗ ∪ {i∗}
unassigned ← unassigned \ {i∗}
if |assignedj∗ | = M D−d then

full ← full ∪ {j∗}

for j = 1 to M do

return assigned

AssignLabels (assignedj, childn,j, d + 1)

Leaf 229
suggested
watched
created
violated
introduced
discovered
carried
described
accepted
listed
. . .

Leaf 230
vegas
&
calif.
park
n.j.
conn.
pa.
pa.
ii
d.
. . .

Leaf 300
payments
buy-outs
swings
gains
taxes
operations
proﬁts
penalties
relations
liabilities
. . .

Leaf 231
operates
includes
intends
makes
means
helps
seeks
reduces
continues
fails
. . .

Table 5. Example of labels reaching leaf nodes in the ﬁnal tree. We can identify a leaf for 3rd person verbs, one for past participates, one
for plural nouns, and one (loosely) for places.

