Schema Networks: Zero-shot Transfer with a Generative Causal Model of
Intuitive Physics

Ken Kansky Tom Silver David A. M´ely Mohamed Eldawy Miguel L´azaro-Gredilla Xinghua Lou
Nimrod Dorfman Szymon Sidor Scott Phoenix Dileep George

Abstract

tasks.

The recent adaptation of deep neural network-
based methods to reinforcement learning and
planning domains has yielded remarkable
progress on individual
Nonetheless,
progress on task-to-task transfer remains limited.
In pursuit of efﬁcient and robust generalization,
we introduce the Schema Network, an object-
oriented generative physics simulator capable
of disentangling multiple causes of events and
reasoning backward through causes to achieve
goals. The richly structured architecture of the
Schema Network can learn the dynamics of an
environment directly from data. We compare
Schema Networks with Asynchronous Advan-
tage Actor-Critic and Progressive Networks on a
suite of Breakout variations, reporting results on
training efﬁciency and zero-shot generalization,
consistently demonstrating faster, more robust
learning and better transfer. We argue that
generalizing from limited data and learning
causal relationships are essential abilities on the
path toward generally intelligent systems.

Figure 1. Variations of Breakout. From top left: standard version,
middle wall, half negative bricks, offset paddle, random target,
and juggling. After training on the standard version, Schema Net-
works are able to generalize to the other variations without any
additional training.

1. Introduction

A longstanding ambition of research in artiﬁcial intelli-
gence is to efﬁciently generalize experience in one scenario
to other similar scenarios. Such generalization is essential
for an embodied agent working to accomplish a variety of
goals in a changing world. Despite remarkable progress on
individual tasks like Atari 2600 games (Mnih et al., 2015;
Van Hasselt et al., 2016; Mnih et al., 2016) and Go (Silver
et al., 2016a), the ability of state-of-the-art models to trans-
fer learning from one environment to the next remains lim-

All authors afﬁliated with Vicarious AI, California, USA. Cor-
respondence to: Ken Kansky <ken@vicarious.com>, Tom Silver
<tom@vicarious.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ited. For instance, consider the variations of Breakout illus-
trated in Fig. 1. In these environments the positions of ob-
jects are perturbed, but the object movements and sources
of reward remain the same. While humans have no trouble
generalizing experience from the basic Breakout to its vari-
ations, deep neural network-based models are easily fooled
(Taylor & Stone, 2009; Rusu et al., 2016).

The model-free approach of deep reinforcement learning
(Deep RL) such as the Deep-Q Network and its descen-
dants is inherently hindered by the same feature that makes
it desirable for single-scenario tasks: it makes no assump-
tions about the structure of the domain. Recent work has
suggested how to overcome this deﬁciency by utilizing
object-based representations (Diuk et al., 2008; Usunier
et al., 2016). Such a representation is motivated by the

Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics

well-acknowledged Gestalt principle, which states that the
ability to perceive objects as a bounded ﬁgure in front of
an unbounded background is fundamental to all perception
(Weiten, 2012). Battaglia et al. (2016) and Chang et al.
(2016) go further, deﬁning hardcoded relations between
objects as part of the input.

While object-based and relational representations have
shown great promise alone, they stop short of modeling
causality – the ability to reason about previous observations
and explain away alternative causes. A causal model is es-
sential for regression planning, in which an agent works
backward from a desired future state to produce a plan
(Anderson, 1990). Reasoning backward and allowing for
multiple causation requires a framework like Probabilistic
Graphical Models (PGMs), which can natively support ex-
plaining away (Koller & Friedman, 2009).

Here we introduce Schema Networks – a generative model
for object-oriented reinforcement learning and planning1.
Schema Networks incorporate key desiderata for the ﬂex-
ible and compositional transfer of learned prior knowl-
edge to new settings. 1) Knowledge is represented with
“schemas” – local cause-effect relationships involving one
or more object entities; 2) In a new setting, these cause-
effect relationships are traversed to guide action selection;
and 3) The representation deals with uncertainty, multiple-
causation, and explaining away in a principled way. We
ﬁrst describe the representational framework and learning
algorithms and then demonstrate how action policies can
be generated by treating planning as inference in a fac-
tor graph. We evaluate the end-to-end system on Break-
out variations and compare against Asynchronous Advan-
tage Actor-Critic (A3C) (Mnih et al., 2016) and Progres-
sive Networks (PNs) (Rusu et al., 2016), the latter of which
extends A3C explicitly to handle transfer. We show that
the structure of the Schema Network enables efﬁcient and
robust generalization beyond these Deep RL models.

2. Related Work

The ﬁeld of reinforcement learning has witnessed signiﬁ-
cant progress with the recent adaptation of deep learning
methods to traditional frameworks like Q-learning. Since
the introduction of the Deep Q-network (DQN) (Mnih
et al., 2015), which uses experience replay to achieve
human-level performance on a set of Atari 2600 games,
several innovations have enabled faster convergence and
better performance with less memory. The asynchronous
methods introduced by Mnih et al. (2016) exploit multi-
ple agents acting in copies of the same environment, com-
bining their experiences into one model. As the Asyn-

1We borrow the term “schema” from Drescher (1991), whose
schema mechanism inspired the early development of our model.

chronous Advantage Actor-Critic (A3C) is the best among
these methods, we use it as our primary comparison.

Model-free Deep RL models like A3C are unable to
substantially generalize beyond their training experience
(Jaderberg et al., 2016; Rusu et al., 2016). To address this
limitation, recent work has attempted to introduce more
structure into neural network-based models. The Interac-
tion Network (Battaglia et al., 2016) (IN) and the Neural
Physics Engine (NPE) (Chang et al., 2016) use object-level
and pairwise relational representations to learn models of
intuitive physics. The primary advantage of these models
is their amenability to gradient-based methods, though such
techniques might be applied to Schema Networks as well.
Schema Networks offer two key advantages: latent phys-
ical properties and relations need not be hardcoded, and
planning can make use of backward search, since the model
can distinguish different causes. Furthermore, neither INs
nor NPEs have been applied in RL domains. Progress in
model-based Deep RL has thus far been limited, though
methods like Embed to Control (Watter et al., 2015), Value
Iteration Networks (Tamar et al., 2016), and the Predictron
(Silver et al., 2016b) demonstrate the promise of this direc-
tion. However, these approaches do not exploit the object-
relational representation of INs or NPEs, nor do they incor-
porate a backward model for regression planning.

Schema Networks build upon the ideas of the Object-
Oriented Markov Decision Process (OO-MDP) introduced
by Diuk et al. (2008) (see also Scholz et al. (2014)). Re-
lated frameworks include relational and ﬁrst-order logical
MDPs (Guestrin et al., 2003a). These formalisms, which
harken back to classical AI’s roots in symbolic reasoning,
are designed to enable robust generalization. Recent work
by Garnelo et al. (2016) on “deep symbolic reinforcement
learning” makes this connection explicit, marrying ﬁrst-
order logic with deep RL. This effort is similar in spirit to
our work with Schema Networks, but like INs and NPEs, it
lacks a mechanism to learn disentangled causes of the same
effect and cannot perform regression planning.

Schema Networks transfer experience from one scenario
to other similar scenarios that exhibit repeatable structure
and sub-structure (Taylor & Stone, 2009). Rusu et al.
(2016) show how A3C can be augmented to similarly ex-
ploit common structure between tasks via Progressive Net-
works (PNs). A PN is constructed by successively training
copies of A3C on each task of interest. With each new
task, the existing network is frozen, another copy of A3C
is added, and lateral connections between the frozen net-
work and the new copy are established to facilitate trans-
fer of features learned during previous tasks. One obvious
limitation of PNs is that the number of network parameters
must grow quadratically with the number of tasks. How-
ever, even if this growth rate was improved, the PN would

Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics

still be unable to generalize from biased training data with-
out continuing to learn on the test environment. In contrast,
Schema Networks exhibit zero-shot transfer.

Schema Networks are implemented as probabilistic graph-
ical models (PGMs), which provide practical inference and
structure learning techniques. Additionally, inference with
uncertainty and explaining away are naturally supported by
PGMs. We direct the readers to (Koller & Friedman, 2009)
and (Jordan, 1998) for a thorough overview of PGMs. In
particular, early work on factored MDPs has demonstrated
how PGMs can be applied in RL and planning settings
(Guestrin et al., 2003b).

3. Schema Networks

3.1. MDPs and Notation

The traditional formalism for the Reinforcement Learning
problem is the Markov Decision Process (MDP). An MDP
M is a ﬁve-tuple (S, A, T, R, γ), where S is a set of states,
A is a set of actions, T (s(t+1)|s(t), a(t)) is the probabil-
ity of transitioning from state s(t) ∈ S to s(t+1) ∈ S af-
ter action a(t) ∈ A, R(r(t+1)|s(t), a(t)) is the probability
of receiving reward r(t+1) ∈ R after executing action a(t)
while in state s(t), and γ ∈ [0, 1] is the rate at which future
rewards are exponentially discounted.

3.2. Model Deﬁnition

A Schema Network is a structured generative model of an
MDP. We ﬁrst describe the architecture of the model infor-
mally. An image input is parsed into a list of entities, which
may be thought of as instances of objects in the sense of
OO-MDPs (Diuk et al., 2008). All entities share the same
collection of attributes. We refer to a speciﬁc attribute of
a speciﬁc entity as an entity-attribute, which is represented
as a binary variable to indicate the presence of that attribute
for an entity. An entity state is an assignment of states to
all attributes of the entity, and the complete model state is
the set of all entity states.

A grounded schema is a binary variable associated with
a particular entity-attribute in the next timestep, whose
value depends on the present values of a set of binary
entity-attributes. The event that one of these present entity-
attributes assumes the value 1 is called a precondition of the
grounded schema. When all preconditions of a grounded
schema are satisﬁed, we say that the schema is active, and
it predicts the activation of its associated entity-attribute.
Grounded schemas may also predict rewards and may be
conditioned on actions, both of which are represented as
binary variables. For instance, a grounded schema might
deﬁne a distribution over Entity 1’s “position” attribute at
time 5, conditioned on Entity 2’s “position” attribute at
time 4 and the action “UP” at time 4. Grounded schemas

Figure 2. Architecture of a Schema Network. An ungrounded
schema is a template for a factor that predicts either the value
of an entity-attribute (A) or a future reward (B) based on entity
states and actions taken in the present. Self-transitions (C) predict
that entity-attributes remain in the same state when no schema is
active to predict a change. Self-transitions allow continuous or
categorical variables to be represented by a set of binary variables
(depicted as smaller nodes). The grounded schema factors, instan-
tiated from ungrounded schemas at all positions, times, and entity
bindings, are combined with self-transitions to create a Schema
Network (D).

are instantiated from ungrounded schemas, which behave
like templates for grounded schemas to be instantiated at
different times and in different combinations of entities.
For example, an ungrounded schema could predict the “po-
sition” attribute of Entity x at time t + 1 conditioned on
the “position” of Entity y at time t and the action “UP”
at time t; this ungrounded schema could be instantiated at
time t = 4 with x = 1 and y = 2 to create the grounded
schema described above. In the case of attributes like “po-
sition” that are inherently continuous or categorical, several
binary variables may be used to discretely approximate the
distribution (see the smaller nodes in Figure 2). A Schema
Network is a factor graph that contains all grounded instan-
tiations of a set of ungrounded schemas over some window
of time, illustrated in Figure 2.

We now formalize the Schema Network factor graph. For
simplicity, suppose the number of entities and the num-
ber of attributes are ﬁxed at N and M respectively. Let
Ei refer to the ith entity and let α(t)
i,j refer to the jth at-
tribute value of the ith entity at time t. We use the notation
i = (α(t)
E(t)
i,M ) to refer to the state of the ith en-

i,1, ..., α(t)

Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics

tity at time t. The complete state of the MDP modeled by
the network at time t is then s(t) = (E(t)
N ). Ac-
tions and rewards are also represented with sets of binary
variables, denoted a(t) and r(t+1) respectively. A Schema
Network for time t will contain the variables in s(t), a(t),
s(t+1), and r(t+1).

1 , ..., E(t)

let AND(v1, ..., vn) = (cid:81)n

Let φk denote the variable for grounded schema k. φk
is bound to a speciﬁc entity-attribute αi,j and activates it
when the schema is active. Multiple grounded schemas
can predict the same attribute, and these predictions are
For binary variables
combined through an OR gate.
v1, ..., vn,
i=1 P (vi = 1),
and OR(v1, ..., vn) = 1 − (cid:81)n
i=1(1 − P (vi = 1)).
A grounded schema is connected to its precondition
entity-attributes with an AND factor, written as φk =
AND(αi1,j1, ..., αiH ,jH , a) for H entity-attribute precondi-
tions and an optional action a. There is no restriction on
how many entities or attributes from a single entity can be
preconditions of a grounded schema.

template)

An ungrounded schema (or
is represented
as Φl(Ex1, ..., ExH ) = AND(αx1,y1, αx1,y2..., αxH ,yH ),
where xh determines the relative entity index of the h-th
precondition and yh determines which attribute variable is
the precondition. The ungrounded schema is a template
that can be bound to multiple speciﬁc entities and locations
to generate grounded schemas.

A subset of attributes corresponds to discrete positions.
These attributes are treated differently from all others,
whose semantic meanings are unknown to the model.
When a schema predicts a movement to a new position,
we must inform the previously active position attribute to
be inactive unless there is another schema that predicts it
to remain active. We introduce a self-transition variable to
represent the probability that a position attribute will re-
main active in the next time step when no schema predicts
a change from that position. We compute the self-transition
variable as Λi,j = AND(¬φ1, ..., ¬φk, si,j) for entity i
and position attribute j, where the set φ1...φk includes all
schemas that predict the future position of the same entity
i and include si,j as a precondition.

(cid:81)M

With these terms deﬁned, we may now compute
the transition function, which can be factorized as
T (s(t+1)|s(t), a(t)) = (cid:81)N
|s(t), a(t)).
i=1
An entity-attribute is active at the next time step if either
a schema predicts it to be active or if its self-transition vari-
able is active: Ti,j(s(t+1)
|s(t)) = OR(φk1 , ..., φkQ, Λi,j),
where k1...kQ are the indices of all grounded schemas that
predict si,j.

j=1 Ti,j(s(t+1)

i,j

i,j

3.3. Construction of Entities and Attributes

In practice we assume that a vision system is responsi-
ble for detecting and tracking entities in an image.
It is
therefore largely up to the vision system to determine what
constitutes an entity. Essentially any trackable image fea-
ture could be an entity, which most typically includes ob-
jects, their boundaries, and their surfaces. Recent work has
demonstrated one possible method for unsupervised entity
construction using autoencoders (Garnelo et al., 2016). De-
pending on the task, Schema Networks could learn to rea-
son ﬂexibly at different levels of representation. For ex-
ample, using entities from surfaces might be most relevant
for predicting collisions, while using one entity per object
might be most relevant for predicting whether it can be con-
trolled by an action. The experiments in this paper utilize
surface entities, described further in Section 5.

entity attributes can be provided by the
Similarly,
vision system, and these attributes typically include:
color/appearance, surface/edge orientation, object cate-
gory, or part-of an object category (e.g.
front-left tire).
For simplicity we here restrict the entities to have fully ob-
servable attributes, but in general they could have latent at-
tributes such as “bounciness” or “magnetism.”

3.4. Connections to Existing Models

Schema Networks are closely related to Object-Oriented
MDPs (OO-MDPs) (Diuk et al., 2008) and Relational
MDPs (R-MDPs) (Guestrin et al., 2003a). However, nei-
ther OO-MDPs nor R-MDPs deﬁne a transition function
with an explicit OR of possible causes, and traditionally
transition functions have not been learned in these models.
In contrast, Schema Networks provide an explicit OR to
reason about multiple causation, which enables regression
planning. Additionally, the structure of Schema Networks
is amenable to efﬁcient learning.

Schema Networks are also related to the recently proposed
Interaction Network (IN) (Battaglia et al., 2016) and Neural
Physics Engine (NPE) (Chang et al., 2016). At a high level,
INs, NPEs, and Schema Networks are much alike – objects
are to entities as relations are to schemas. However, nei-
ther INs nor NPEs are generative and hence do not support
regression planning from a goal through causal chains. Be-
cause Schema Networks are generative models, they sup-
port more ﬂexible inference and search strategies for plan-
ning. Additionally, the learned structures in Schema Net-
works are amenable to human interpretation, explicitly fac-
torizing different causes, making prediction errors easier to
relate to the learned model parameters.

Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics

4. Learning and Planning in Schema

applying LP relaxations to structure learning.

Networks

In this section we describe how to train Schema Networks
(i.e., learn its structure) from interactions with an environ-
ment, as well as how they can be used to perform planning.
Planning is not only necessary at test time to maximize re-
ward, but also can be used to improve exploration during
the training procedure.

4.1. Training Procedure

Given a series of actions, rewards and images, we represent
each possible action and reward with a binary variable, and
we convert each image into a set of entity states. The num-
ber of entities is allowed to vary between adjacent frames,
accounting for objects appearing or moving out of view.

Given a dataset of entity states over time, we preprocess the
entity states into a representation that is more convenient
for learning. For N entities observed over T timesteps, we
wish to predict α(t)
i,j on the basis of the attribute values of
the ith entity and its spatial neighbors at time t − 1 (for
1 ≤ i ≤ N and 2 ≤ t ≤ T ). The attribute values of
E(t−1)
and its neighbors can be represented as a row vector
i
of length M R, where M is the number of attributes and
R − 1 is the number of neighbor positions of each entity,
determined by a ﬁxed radius. Let X ∈ {0, 1}D×D(cid:48)
be
the arrangement of all such vectors into a binary matrix,
with D = N T and D(cid:48) = M R. Let y ∈ {0, 1}D be a
binary vector such that if row r in X refers to E(t−1)
, then
yr = α(t)
i,j . Schemas are then learned to predict y from X
using the method described in Section 4.2.

i

While gathering data, actions are chosen by planning using
the schemas that have been learned so far. This planning
algorithm is described in Section 4.3. We use an ε-greedy
approach to encourage exploration, taking a random ac-
tion at each timestep with small probability. We found no
need to perform any additional policy learning, and after
convergence predictions were accurate enough to allow for
successful planning. As shown in Section 5, since learn-
ing only involves understanding the dynamics of the game,
transfer learning is simpliﬁed and there is no need for pol-
icy adaptation.

4.2. Schema Learning

Structure learning in graphical models is a well studied
topic in machine learning (Koller & Friedman, 2009; Jor-
dan, 1998). To learn the structure of the Schema Network,
we cast the problem as a supervised learning problem over
a discrete space of parameterizations (the schemas), and
then apply a greedy algorithm that solves a sequence of LP
relaxations. See Jaakkola et al. (2010) for further work on

With X and y deﬁned above, the learning problem is to ﬁnd
a mapping

y = fW (X) = XW(cid:126)1

where all the involved variables are binary and operations
follow Boolean logic: addition corresponds to ORing, and
overlining to negation. W ∈ {0, 1}D(cid:48)×L is a binary
matrix, with each column representing one (ungrounded)
schema for at most L schemas. The elements set to 1 in
each schema represent an existing connection between that
schema and an input condition (see Fig. 2). The outputs
of each individual schema are ORed to produce the ﬁnal
prediction.

We would like to minimize the prediction error of Schema
Networks while keeping them as simple as possible. A suit-
able objective function is

min
W ∈{0,1}D(cid:48)×L

1
D

|y − fW (X)|1 + C|W |1,

(1)

where the ﬁrst term computes the prediction error, the sec-
ond term estimates the complexity and parameter C con-
trols the trade-off between both. This is an NP-hard prob-
lem for which we cannot hope to ﬁnd an exact solution,
except for very small environments.

We consider a greedy solution in which linear program-
ming (LP) relaxations are used to ﬁnd each new schema.
Starting from the empty set, we greedily add schemas
(columns to W ) that have perfect precision and increase
recall for the prediction of y (See Algorithm 1 in the
In each successive iteration, only the
Supplementary).
input-output pairs for which the current schema network
is predicting an output of zero are passed. This procedure
monotonically decreases the prediction error of the overall
schema network, while increasing its complexity. The pro-
cess stops when we hit some predeﬁned complexity limit.
In our implementation, the greedy schema selection pro-
duces very sparse schemas, and we simply set a limit to
the number of schemas to add. For this algorithm to work,
no contradictions can exist in the input data (such as the
same input appearing twice with different labels). Such
contradictions might appear in stochastic environments and
would not be artifacts in real environments, so we prepro-
cess the input data to remove them.

4.3. Planning as Probabilistic Inference

The full Schema Network graph (Fig. 2) provides a proba-
bilistic model for the set of rewards that will be achieved by
a sequence of actions. Finding the sequence of actions that
will result in a given set of rewards becomes then a MAP

Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics

inference problem. This problem can be addressed approx-
imately using max-product belief propagation (MPBP) (At-
tias, 2003). Another option is variational inference. Cheng
et al. (2013) use variational inference for planning but re-
sort to MPBP to optimize the variational free energy func-
tional. We will follow the ﬁrst approach.

Choosing a positive reward goal state We will choose
the potentially feasible positive reward that happens soon-
est within our explored window, clamp its state to 1, and
backtrack (see below) to ﬁnd the set of actions that lead to
it. If backtracking fails, we will repeat for the remaining
potentially feasible positive rewards.

Without loss of generality, we will consider the present
time step to be t = 0. The state, action and reward variables
for t ≤ 0 are observed, and we will consider inference over
the unobserved variables in a look-ahead window of size2
T , {s(t), a(t), r(t)}T −1
t=0 . Since the Schema Network is built
exclusively of compatibility factors that can take values 0
or 1, any variable assignment is either impossible or equally
probable under the joint distribution of the graph. Thus, if
we want to know if there exists any global assignment that
activates a binary variable (say, variable r(t)
(+) signaling pos-
itive reward at some future time t > 0), we should look at
the max-marginal ˜p(r(t)
(+) = 1). It will be 0 if no global
assignment compatible with both the SN and existing ob-
servations can lead to activate the reward, or 1 if it is fea-
sible. Similarly, we will be interested in the max-marginal
˜p(r(t)
(−) = 0), i.e., whether it is feasible to ﬁnd a conﬁgura-
tion that avoids a negative reward.

At a high-level, planning proceeds as follows: Identify fea-
sible desirable states (activating positive rewards and de-
activating negative rewards), clamp their value to our de-
sires by adding a unary potential to the factor graph, and
then ﬁnd the MAP conﬁguration of the resulting graph.
The MAP conﬁguration contains the values of the action
variables that are required to reach our goal of activat-
ing/deactivating a variable. We can also look at S to see
how the model “imagines” the evolution of the entities un-
til they reach their goal state. Then we perform the actions
found by the planner and repeat. We now explain each of
these stages in more detail.

Potential feasibility analysis First we run a feasibility
analysis. To this end, a forward pass MPBP from time 0
to time T is performed. This provides a (coarse) approx-
imation to the desired max-marginals for every variable.
Because the SN graph is loopy, MPBP is not exact and the
forward pass can be too optimistic, announcing the feasi-
bility of states that are unfeasible3. Actual feasibility will
be veriﬁed later, at the backtracking stage.

2In contrast with MDPs,

the reward is discounted with a
rolling square window instead of an exponentially weighted one.
3To illustrate the problem, consider the case in which it is fea-
sible for an entity to move at time t to position A or position B
(but obviously not both) and then some reward is conditioned on
that type of entity being in both positions: A single forward pass
will not handle the entanglement properly and will incorrectly re-
port that such reward is also feasible.

Avoiding negative rewards Keeping the selected posi-
tive reward variable clamped to 1 (if it was found in the
previous step), we now repeat the same procedure on the
negative rewards. Among the negative rewards that have
been found as potentially feasible to turn off, we clamp to
zero as many negative rewards as we can ﬁnd a jointly sat-
isfying backtrack. If no positive reward was feasible, we
backtrack from the earliest predicted negative reward.

Backtracking This step is akin to Viterbi backtracking,
a message passing backward pass that ﬁnds a satisfying
conﬁguration. Unlike the HMM for which the Viterbi al-
gorithm was designed, our model is loopy, so a standard
backward pass is not enough to ﬁnd a satisfying conﬁgura-
tion (although can help to ﬁnd good candidates). We com-
bine the standard backward pass with a depth-ﬁrst search
algorithm to ﬁnd a satisfying conﬁguration.

5. Experiments

We compared the performance of Schema Networks, A3C,
and PNs (Progressive Networks) on several variations of
the game Breakout. The chosen variations all share sim-
ilar dynamics, but the layouts change, requiring different
policies to achieve high scores. A diverse set of concepts
must be learned to correctly predict object movements and
rewards. For example, when predicting why rewards occur,
the model must disentangle possible causes to discover that
reward depends on the color of a brick but is independent
of the ball’s velocity and position where it was hit. While
these causal relationships are straightforward for humans to
recover, we have yet to see any existing approach for learn-
ing a generative model that can recover all of these dynam-
ics without supervision and transfer them effectively.

Schema Networks rely on an input of entity states instead
of raw images, and we provided the same information to
A3C and PNs by augmenting the three color channels of the
image with 34 additional channels. Four of these channels
indicated the shape to which each pixel belongs, including
shapes for bricks, balls, and walls. Another 30 channels
indicated the positions of parts of the paddle, where each
part consisted of a single pixel. To reduce training time,
we did not provide A3C and PN with part channels for ob-
jects other than the paddle, since these are not required to
learn the dynamics or predict scores. Removing irrelevant
inputs could only give A3C and PN an advantage, since

Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics

(a) Mini Breakout Learning Rate

(b) Middle Wall Learning Rate

Figure 3. Comparison of learning rates. (a) Schema Networks and A3C were trained for 100k frames in Mini Breakout. Plot shows the
average of 5 training attempts for Schema Networks and the best of 5 training attempts for A3C, which did not converge as reliably. (b)
PNs and Schema Networks were pretrained on 100K frames of Standard Breakout, and then training continued on 45K additional frames
of the Middle Wall variation. We show performance as a function of training frames for both models. Note that Schema Networks are
ignoring all the additional training data, since all the required schemas were learned during pretraining. For Schema Networks, zero-shot
transfer learning is happening.

the input to Schema Networks did not treat any object dif-
ferently. Schema Networks were provided separate entities
for each part (pixel) of each object, and each entity con-
tained 53 attributes corresponding to the available part la-
bels (21 for bricks, 30 for the paddle, 1 for walls, and 1 for
the ball). Only one of these part attributes was active per
entity. Schema Networks had to learn that some attributes,
like parts of bricks, were irrelevant for prediction.

5.1. Transfer Learning

This experiment examines how effectively Schema Net-
works and PNs are able to learn a new Breakout variation
after pretraining, which examines how well the two mod-
els can transfer existing knowledge to a new task. Fig. 3a
shows the learning rates during 100k frames of training on
Mini Breakout. In a second experiment, we pretrained on
Large Breakout for 100k frames and continued training on
the Middle Wall variation, shown in Fig. 1b. Fig. 3b shows
that PNs require signiﬁcant time to learn in this new en-
vironment, while Schema Networks do not learn anything
new because the dynamics are the same.

Rather than comparing transfer with additional training us-
ing PNs, in these variations we can compare zero-shot gen-
eralization by training A3C only on Standard Breakout.
Fig. 1b-e shows some of these variations with the following
modiﬁcations from the training environment:

• Offset Paddle (Fig. 1d): The paddle is shifted upward

by a few pixels.

• Middle Wall (Fig. 1b): A wall is placed in the middle
of the screen, requiring the agent to aim around it to
hit the bricks.

• Random Target (Fig. 1e): A group of bricks is
destoyed when the ball hits any of them and then reap-
pears in a new random position, requiring the agent to
delibarately aim at the group.

• Juggling (Fig. 1f, enlarged from actual environment
to see the balls): Without any bricks, three balls are
launched in such a way that a perfect policy could jug-
gle them without dropping any.

5.2. Zero-Shot Generalization

Many Breakout variations can be constructed that all in-
volve the same dynamics. If a model correctly learns the
dynamics from one variation, in theory the others could
be played perfectly by planning using the learned model.

Table 1 shows the average scores per episode in each
Breakout variation. These results show that A3C has failed
to recognize the common dynamics and adapt its policy ac-
cordingly. This comes as no surprise, as the policy it has
learned for Standard Breakout is no longer applicable in
these variations. Simply adding an offset to the paddle is

Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics

Table 1. Zero-Shot Average Score per Episode Average of the 2 best out of 5 training attempts for A3C, and average of 5 training
attempts for Schema Networks. A3C was trained on 200k frames of Standard Breakout (hence its zero-shot scores for Standard Breakout
are unknown) while Schema Networks were trained on 100k frames of Mini Breakout. Episodes were limited to 2500 frames for all
variations. In every case the average Schema Network scores are better than the best A3C scores by more than one standard deviation.

Standard Breakout Offset Paddle Middle Wall
A3C Image Only
N/A
A3C Image + Entities N/A
Schema Networks

0.60 ± 20.05
11.10 ± 17.44
41.42 ± 6.29

9.55 ± 17.44
8.00 ± 14.61
35.22 ± 12.23

36.33 ± 6.17

Random Target

Juggling

6.83 ± 5.02 −39.35 ± 14.57
6.88 ± 6.19 −17.52 ± 17.39
21.38 ± 5.02 −0.11 ± 0.34

Table 2. Average Score per Episode on Half Negative Bricks
A3C and Schema Networks were trained on 200k frames of Ran-
dom Negative Bricks, both to convergence. Testing episodes were
limited to 1000 frames. Negative rewards are sometimes unavoid-
able, resulting in higher variance for all methods.

A3C Image Only
A3C Image + Entities
Schema Networks

Half Negative Bricks
−3.80 ± 7.55
1.55 ± 6.41
5.77 ± 4.30

sufﬁcient to confuse A3C, which has not learned the causal
nature of controlling the paddle with actions and control-
ling the ball with the paddle. The Middle Wall and Random
Target variations illustrate that Schema Networks are aim-
ing to deliberately cause positive rewards from ball-brick
collisions, while A3C struggles to adapt its policy accord-
ingly. The Juggling variation is particularly challenging,
since it is not clear which ball to respond to unless the
model understands that the lowest downward-moving ball
is the most imminent cause of a negative reward. By learn-
ing and transferring the correct causal dynamics, Schema
Networks outperform A3C in all variations.

5.3. Testing for Learned Causes

To better evaluate whether these models are truly learning
the causes of rewards, we designed one more zero-shot gen-
eralization experiment. We trained both Schema Networks
and A3C on a Mini Breakout variation in which the color
of a brick determines whether a positive or negative reward
is received when it is destroyed. Six colors of bricks pro-
vide +1 reward, and two colors provide -1 reward. Negative
bricks occurred in random positions 33% of the time dur-
ing training. Then during testing, the bricks were arranged
into two halves, with all positive colored bricks on one half
and negative colored bricks on the other (see Fig. 1c). If
the causes of rewards have been correctly learned, the agent
should prefer to aim for the positive half whenever possible.
As Table 1 shows, Schema Networks have correctly learned

from random arrangements which brick colors cause which
rewards, preferring to aim for the positive half during test-
ing, while A3C demonstrates no preference for one half or
the other, achieving an average score near zero.

6. Discussion and Conclusion

In this work we have demonstrated the promise of Schema
Networks with strong performance on a suite of Breakout
variations.
Instead of learning policies to maximize re-
wards, the learning objective for Schema Networks is de-
signed to understand causality within these environments.
The fact that Schema Networks are able to achieve rewards
more efﬁciently than state-of-the-art model-free methods
like A3C is all the more notable, since high scores are a
byproduct of learning an accurate model of the game.

The success of Schema Networks is derived in part from
the entity-based representation of state. Our results suggest
that providing Deep RL models like A3C with such a repre-
sentation as input can improve both training efﬁciency and
generalization. This ﬁnding corroborates recent attempts
(Usunier et al., 2016; Garnelo et al., 2016; Chang et al.,
2016; Battaglia et al., 2016) to incorporate object and rela-
tional structure into neural network-based models.

The environments considered in this work are conceptually
diverse but also simpliﬁed in a number of ways with re-
spect to the real world: states, actions, and rewards are all
discretized as binary random variables; the dynamics of the
environments are deterministic; and there is no uncertainty
in the observed entity states. In future work we plan to ad-
dress each of these limitations, adapting Schema Networks
to continuous, stochastic domains.

Schema Networks have shown promise toward multi-task
transfer where Deep RL struggles. This transfer is enabled
by explicit causal structures, which in turn allow for plan-
ning in novel tasks. As progress in RL and planning con-
tinues, robust generalization from limited experience will
be vital for future intelligent systems.

Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics

Acknowledgements

Special thanks to Eric Purdy and Ramki Gummadi for use-
ful insights and discussions during the preparation of this
work.

References

Anderson, John R. Cognitive psychology and its impli-
cations. WH Freeman/Times Books/Henry Holt & Co,
1990.

Attias, Hagai. Planning by probabilistic inference. In AIS-

TATS, 2003.

Battaglia, Peter, Pascanu, Razvan, Lai, Matthew, Rezende,
Danilo Jimenez, et al.
Interaction networks for learn-
ing about objects, relations and physics. In Advances in
Neural Information Processing Systems, pp. 4502–4510,
2016.

Chang, Michael B, Ullman, Tomer, Torralba, Antonio, and
Tenenbaum, Joshua B. A compositional object-based ap-
proach to learning physical dynamics. arXiv preprint
arXiv:1612.00341, 2016.

Cheng, Qiang, Liu, Qiang, Chen, Feng, and Ihler, Alexan-
der T. Variational planning for graph-based mdps.
In
Advances in Neural Information Processing Systems, pp.
2976–2984, 2013.

Diuk, Carlos, Cohen, Andre, and Littman, Michael L.
An object-oriented representation for efﬁcient reinforce-
ment learning. In Proceedings of the 25th international
conference on Machine learning, pp. 240–247. ACM,
2008.

Drescher, Gary L. Made-up minds: a constructivist ap-

proach to artiﬁcial intelligence. MIT press, 1991.

Garnelo, Marta, Arulkumaran, Kai, and Shanahan, Murray.
Towards deep symbolic reinforcement learning. arXiv
preprint arXiv:1609.05518, 2016.

Guestrin, Carlos, Koller, Daphne, Gearhart, Chris, and
Kanodia, Neal. Generalizing plans to new environments
In Proceedings of the 18th inter-
in relational mdps.
national joint conference on Artiﬁcial intelligence, pp.
1003–1010. Morgan Kaufmann Publishers Inc., 2003a.

Guestrin, Carlos, Koller, Daphne, Parr, Ronald, and
Venkataraman, Shobha. Efﬁcient solution algorithms
for factored mdps. Journal of Artiﬁcial Intelligence Re-
search, 19:399–468, 2003b.

Jaakkola, Tommi S, Sontag, David, Globerson, Amir,
Meila, Marina, et al. Learning bayesian network struc-
In AISTATS, pp. 358–365,
ture using lp relaxations.
2010.

Jaderberg, Max, Mnih, Volodymyr, Czarnecki, Woj-
ciech Marian, Schaul, Tom, Leibo, Joel Z, Silver,
David, and Kavukcuoglu, Koray. Reinforcement learn-
ing with unsupervised auxiliary tasks. arXiv preprint
arXiv:1611.05397, 2016.

Jordan, Michael Irwin. Learning in graphical models, vol-
ume 89. Springer Science & Business Media, 1998.

Koller, Daphne and Friedman, Nir. Probabilistic graphical
models: principles and techniques. MIT press, 2009.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Rusu, Andrei A, Veness, Joel, Bellemare, Marc G,
Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K,
Ostrovski, Georg, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529–
533, 2015.

Mnih, Volodymyr, Badia, Adria Puigdomenech, Mirza,
Mehdi, Graves, Alex, Lillicrap, Timothy, Harley, Tim,
Silver, David, and Kavukcuoglu, Koray. Asynchronous
In Proceed-
methods for deep reinforcement learning.
ings of The 33rd International Conference on Machine
Learning, pp. 1928–1937, 2016.

Rusu, Andrei A, Rabinowitz, Neil C, Desjardins, Guil-
laume, Soyer, Hubert, Kirkpatrick, James, Kavukcuoglu,
Koray, Pascanu, Razvan, and Hadsell, Raia. Progres-
sive neural networks. arXiv preprint arXiv:1606.04671,
2016.

Scholz, Jonathan, Levihn, Martin, Isbell, Charles, and
Wingate, David. A physics-based model prior for object-
oriented mdps. In Proceedings of the 31st International
Conference on Machine Learning (ICML-14), pp. 1089–
1097, 2014.

Silver, David, Huang, Aja, Maddison, Chris J, Guez,
Arthur, Sifre, Laurent, Van Den Driessche, George,
Schrittwieser, Julian, Antonoglou, Ioannis, Panneershel-
vam, Veda, Lanctot, Marc, et al. Mastering the game of
go with deep neural networks and tree search. Nature,
529(7587):484–489, 2016a.

Silver, David, van Hasselt, Hado, Hessel, Matteo, Schaul,
Tom, Guez, Arthur, Harley, Tim, Dulac-Arnold, Gabriel,
Reichert, David, Rabinowitz, Neil, Barreto, Andre, et al.
The predictron: End-to-end learning and planning. arXiv
preprint arXiv:1612.08810, 2016b.

Tamar, Aviv, Levine, Sergey, Abbeel, Pieter, WU, YI, and
Thomas, Garrett. Value iteration networks. In Advances
in Neural Information Processing Systems, pp. 2146–
2154, 2016.

Taylor, Matthew E and Stone, Peter. Transfer learning for
reinforcement learning domains: A survey. Journal of
Machine Learning Research, 10(Jul):1633–1685, 2009.

Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics

Usunier, Nicolas, Synnaeve, Gabriel, Lin, Zeming, and
Chintala, Soumith. Episodic exploration for deep deter-
ministic policies: An application to starcraft microman-
agement tasks. arXiv preprint arXiv:1609.02993, 2016.

Van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep
reinforcement learning with double q-learning. In AAAI,
pp. 2094–2100, 2016.

Watter, Manuel, Springenberg, Jost, Boedecker, Joschka,
and Riedmiller, Martin. Embed to control: A locally lin-
ear latent dynamics model for control from raw images.
In Advances in Neural Information Processing Systems,
pp. 2746–2754, 2015.

Weiten, W. Psychology: Themes and Variations. PSY 113
General Psychology Series. Cengage Learning, 2012.
URL https://books.
ISBN 9781111354749.
google.com/books?id=a4tznfeTxV8C.

