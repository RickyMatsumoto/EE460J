Appendix A. Some Important Lemmas

In this section, we give several important lemmas which will be used in the proof of the
theorems of this paper.

Lemma 9 If A and B are d
×
where 0 < �0 < 1, then we have

d symmetric positive matrices, and (1

�0)B

A

�

�

−

(1+�0)B

where I is the identity matrix.

A1/2B−

1A1/2

I

�0,

−

� ≤

�

Proof Because A
This implies zT Az

�
zT Bz ≤

(1 + �0)B, we have zT [A
1 + �0 for any z

−

= 0. Subsequently,

(1 + �0)B]z

≤

0 for any nonzero z

Rd.

∈

λmax(B−

1A) =λmax(B−

uT B−

1/2AB−

1/2)
1/2AB−
uT u

1/2u

= max
=0
u

= max
=0
z

zT Az
zT Bz

1 + �0,

≤

where the last equality is obtained by setting z = B−
1A and A1/2B−
1
between 1

�0 and 1 + �0. Therefore, we have

�0. Since B−

−

1A1/2 are similar, the eigenvalues of A1/2B−

≥
1A1/2 are all

1/2u. Similarly, we have λmin(B−

1A)

−

A1/2B−

1A1/2

I

�0.

−

� ≤

�

Lemma 10 ([3]) Let X1, X2, . . . , Xk be independent, random, symmetric, real matrices of
k
i=1 Xi,
LI, where I is the d
Xi �
size d
×
µmin = λmin(E[Y ]) and µmax = λmax(E[Y ]). Then,

d identity matrix. Let Y =

d with 0

×

�

�

P (λmin(Y )

(1

�)µmin)

�2µmin/L

d

e−

≤

·

≤
Lemma 11 ([3]) Given a matrix A
that

∈

−
Rm

n, construct an m

×

n random matrix R such

×

Compute the per-sample second moment:

E[R] = A and

R

�

� ≤

L.

M = max

E[RRT ]

, E[RT R]

{�

�

.

�}

Form the matrix sampling estimator

Ri, where each Ri is an independent copy of R.

¯R =

1
s

s

�i=1

1

�
�
�
Then, for all t

0

≥

¯R

A

�

−

� ≥

≤

(m + n) exp

t

�

P

�

st2/2
M + 2Lt/3

−

.

�

�

Lemma 12 Assume (9) and (10) hold. Let 0 < δ < 1, 0 < � < 1 and 0 < c be given. If we
2fj(x(t)),
sample fi’s uniformly with the sample size
then we have the following results:
16K2 log(2d/δ)
(a) If
c2�2

and construct H (t) = 1
|S|

, it holds that

∈S ∇

|S|

�

j

|S| ≥

(b) If

|S| ≥

K log(2d/δ)
σ�2

, it holds that

H (t)

�

− ∇

2F (x(t))

�c.

� ≤

λmin(H (t))

(1

�)σ.

≥

−

|S|

i.i.d random matrces H (t)
j

Proof Consider
1/n for all i = 1, . . . , n. Then, we have E(H (t)
and the positive semi-deﬁnite property of H (t)
j
K log(d/δ)
By Lemma 10, we have that if
σ�2
at least 1

|S| ≥

δ.

−

We deﬁne random maxtrices Xj = H (t)
2K and

2

j − ∇

E[Xj] = 0,

Xj�

�

≤

Xj� ≤

�

, j = 1, . . . ,

j =
such that P
2F (x(t)) for all j = 1, . . . ,

|S|

�

H (t)

∇

∇

j ) =
, we have λmax(H (t)
j )
, λmin(H (t))

(1

�
. By (9)
K and λmin(H (t)
j )
�)σ holds with probability

|S|

≥

0.

≤

2fi(x(t))

=

≥

−

2F (x(t)) for all j = 1, . . . ,

. We have

|S|

4K2. By Lemma 11, we have

H (t)

P(

�

− ∇

2F (x(t))

�c)

2d exp−

� ≥

≤

c2�2
|S|
16K2 .

When

|S| ≥

16K2 log(2d/δ)
c2�2

,

H (t)

�

− ∇

2F (x(t))

� ≤

�c holds with probability at least 1

δ.

−

Appendix B. Proofs of theorems of Section 3

Proof of Theorem 3 By Assumption 1 and 2, we have that F (x) is µ-strongly convex
and

F (x) is L-Lipschitz continuous. Hence, we have

∇

µ

≤

λmin(

2F (x))

λmax(

2F (x))

∇

≤

∇

L.

≤

Hence, for any x in domain, it holds that

κ =

L
µ ≥

κ(

2F (x)).

∇

2

By Taylor’s theorem, we obtain

F (x(t+1))

=

=

∇

∇

∇

+

=

F (x(t))
1

[

∇

0

�
2F (x(t))

1
2

F (x(t)) +

2F (x(t))(

p(t)) +

2F (x(t) + sp(t))

2F (x(t))](

p(t))ds

∇

−

2F (x(t))[H (t)]−

1

∇
0
�
F (x(t)) +

− ∇
2F (x(t))[H (t)]−

1

−
F (x(t))

2F (x(t))p(t)

∇

− ∇

1

[

− ∇
2F (x(t) + sp(t))

∇
2F (x(t))](

∇
p(t))ds

− ∇

−

I

[

2F (x(t))]

2 [H (t)]−

1[

1

2F (x(t))]

1
2

−

∇

�
2F (x(t))([H (t)]−

�

1

∇

�
+

∇

F (x(t))

p(t)) +

∇

−

∇
1

0

�

[

∇

1
2

−

2F (x(t))

∇

F (x(t))

∇

� �
2F (x(t) + sp(t))

�
2F (x(t))](

− ∇

p(t))ds.

−

Hence, we have the following identity

2F (x(t))

F (x(t+1)) =

I

[

2F (x(t))]

2 [H (t)]−

1[

1

2F (x(t))]

1
2

∇

�

1
2

−

�

∇

∇
2F (x(t))]

�
+ [

+ [

2F (x(t))]−

1
2

−

∇

∇

1

1

2 ([H (t)]−
1

[

∇

0

�

∇
F (x(t))

� �

p(t))

−

∇
2F (x(t) + sp(t))

2F (x(t))

∇

F (x(t))

∇

1
2

−

�

2F (x(t))](

p(t))ds.

− ∇

−

For notational simplicity, we denote M =
can obtain

∇

�

2F (x(t))

1

−

and M ∗ =

2F (x∗)

1. Then we

−

�

∇

[

2F (x(t))]

2 [H (t)]−

1[

1

2F (x(t))]

1
2

I

�
+
�
�

+

�

∇

[

�

∇

−
[

∇
2F (x(t))]

1
2

2F (x(t))]−

�∇

�
p(t)
�
�

�

1

[H (t)]−
1

F (x(t))

−

∇
2F (x(t) + sp(t))

��
1
2

�

0 �∇

�

∇

�

�

F (x(t))

�M

2F (x(t))

p(t)

ds.

��

�

− ∇

F (x(t+1))

�∇

�M ≤

We bound the three terms on the right-hand side of the above equation respectively.

For the ﬁrst term, using Lemma 9, we have

I

[

2F (x(t))]

2 [H (t)]−

1[

1

2F (x(t))]

1
2

∇

F (x(t))

�M ≤

�0�∇

F (x(t))

�M .

�∇

−

∇

�
�
�

For the second term, by the fact that

AB

�

� ≥ �

A

σmin(B) and condition
�

F (x(t))

H (t)p(t)

�∇

−

�1
κ �∇

� ≤

F (x(t))

� ≤

κ(

2F (x(t))) �∇

F (x(t))

,

�

�1

∇

�
�
�

3

[

�

∇

2F (x(t))]

1
2

��
1
2F (x(t))]
2

[

[H (t)]−

1

F (x(t))

p(t)

∇

−
2F (x(t))]−

�
1
2 )

= �

∇
λmin([

�
2F (x(t))]−

1
2 )

λmin([

∇

[H (t)]−

1

F (x(t))

H (t)p(t)

��∇

−

�

�

[

�
∇
λmin([

2F (x(t))]

1
2

�
2F (x(t))]−

∇
2F (x(t))

1

2 ) �
1

[H (t)]−

F (x(t))

�M

��∇

��

[H (t)]−

1

(λmin([

2F (x(t))]−

1
2 )

�

∇

F (x(t))

)

�

�∇

we obtain

≤

κ(

2F (x(t)))

∇

≤

κ(

∇
�1

2F (x(t))) �∇
F (x(t))

≤

1

�0 �∇

−

�M .

∇
�1

�1

∇

For the third term, we bound it for the case that

∇
2F (x) is Lipschitz continuous respectively.

and the case

2F (x) is not Lipschitz continuous

First, we consider the case that
∇
close to the optimal point x∗. Because
small value γ such that it holds that

∇

2F (x) is not Lipschitz continuous but is continuous
2F (x) is continuous near x∗, there exists a suﬃcient

and

[

�

∇

2F (x∗)]−

1

[

2F (x(t))]−

1

<

−

∇

ν(t)
L

,

�

2F (x∗)

2F (x(t))

<

�∇

− ∇

�

η(t)µ
√κ

,

(14)

(15)

when

x(t)
�

x∗� ≤

−

γ. Therefore, ν(t) and η(t) will go to 0 as x(t) goes to x∗.
1

By µ-strong convexity, we have

1
µ for all x(t) suﬃciently close to x∗.

2F (xt)]−

[
�

∇

� ≤

Because of Eqn. (2), we have

[H (t)]−

1

(1 + �0)

[

2F (xt)]−

1

�

� ≤

�

∇

� ≤

(1

1
�0)µ

.

−

We deﬁne r(t) =

F (x(t))

H (t)p(t). Then we have that the direction vector satisﬁes

∇

−

p(t)

=

[H (t)]−

1

�

�

�

(

�

�

r(t)

+

F (x(t))

�

�∇

)

�

≤

(1

2
�0)µ �∇

F (x(t))

,

�

−

(16)

where the second inequality is because

r(t)

=

�

�

�∇

F (x(t))

H (t)p(t)

−

�1
κ �∇

F (x(t))

� ≤ �∇

F (x(t))

.

�

� ≤

4

Hence, with

x

�

−

x∗� ≤

γ, combining condition (15), we have

2F (x(t) + sp(t))

2F (x(t))

p(t)

ds

��

�

− ∇

2F (x(t))]−

1
2

[

�

∇

�

0 �∇
1

�
1
2F (x(t))]−
2

1

�

0

�

2F (x(t))]−

1
2

p(t)

ds

�
µη(t)
√κ �∇

µη(t)
√κ �
2
�0)µ
1
2

(1
�
−
2F (x(t))]−

[

F (x(t))

�

∇

�
√κλmin([

�
2F (x(t))]−

1
2 )

λmin([

∇

2F (x(t))]−

1
2 )

F (x(t))

�∇

�

≤ �

∇

[

[

≤ �

∇

2η(t)
�0
1
−
2η(t)
1

−

≤

≤

∇
F (x(t))
�M .

�0 �∇

Therefore, we have

F (x(t+1))

�M ≤

�0�∇

F (x(t))

�M +

�∇

F (x(t))

�M +

�0 �∇

2η(t)
1

−

�0 �∇

F (x(t))

�M

�1

1
−
2η(t)
1

−

=

�0 +

�

�1

−

1

�0

+

F (x(t))

�M .

�0 �

�∇

Now, we show the relationship between

� · �M and

� · �M ∗. By Eqn. (14), we have

−

λmax(

2F (x∗))

≤

∇

−

∇

uT u

uT ([

2F (x∗)]−

1

[

2F (x(t))]−

1)u

≤

λmax(

2F (x∗))

uT u,

ν(t)

∇

for any nonzero u

Rd, which implies that

(1

−

ν(t))uT [

2F (x(t))]−

1u

uT [

2F (x∗)]−

1u

(1 + ν(t))uT [

2F (x(t))]−

1u.

≤

∇

≤

∇

ν(t)

∇

∈

∇

That is,

By this relationship between

(1

ν(t))

−

u

u
�M ≤ �

�
� · �M and

�M ∗ ≤
� · �M ∗, we get

(1 + ν(t))

u

�M .

�

F (x(t+1))

�∇

�0 +

�M ∗ ≤

�

�1

−

1

�0

+

2η(t)
1

−

�0 �

1 + ν(t)
1

ν(t) �∇

−

F (x(t))

�M ∗

Second, we consider the case that
We have that the direction vector satisﬁes

∇

2F (x) is Lipschitz continuous with parameter ˆL.

p(t)

�

� ≤

(1

2
�0)λmin(

−

2F (x(t))) �∇

F (x(t))

.

�

∇

5

Because

2F (x) is Lipschitz continuous with parameter ˆL, we have

2F (x(t))]−

2
min([
λ−

2F (x(t))]−

1

2 )λ2

min([

2F (x(t))]−

1
2 )

p(t)

2
�

�

∇

2F (x(t) + sp(t))

2F (x(t))

p(t)

ds

��

�

− ∇

∇

2F (x(t))]−

1
2

[

�

∇

�

0 �∇
1

�
1
2F (x(t))]−
2

1

�

0

�
1
2

�

[

∇

=

≤ �
ˆL
2 �
ˆL
2 �

≤

[

[

∇

∇

=

(1

s ˆL

p(t)

2ds

�

�

∇

∇

2F (x(t))]−

1
2

2
min([
λ−

2F (x(t))]−

1
2 )

�
2 ˆLλmax(

∇
2F (x(t)))

2F (x(t)))
λmin(

�0)2λ2
−
2
�0)2 ·

min(
∇
ˆLκ
µ√µ �∇

�
2
F (x(t))
M .
�

(1

�

−

2
�0)λmin(

F (x(t))

∇
2
M

�

2F (x(t))) �∇

∇

≤

(1

−

Thus, we have

2F (x(t)))

�∇

F (x(t))

2
M

�

2

�

F (x(t+1))

�∇

�M ≤

�0 +

1

−
By the Lipschitz continuity of

�

F (x(t))

�M +

�∇

�0 �
−
2F (x) and the condition

(1

2
�0)2 ·

ˆLκ
µ√µ �∇

F (x(t))

2
M .

�

�1

∇

x∗

x(t)

�

−

� ≤

µ
ˆLκ ≤

λmin(
ˆLκ(

2F (x∗))
∇
2F (x(t)))

,

∇

we obtain

[

�

∇

−

∇

2F (x∗)]−

1

[

2F (x(t))]−

1

2F (x∗)]−

1

2F (x(t))]−

1

[
� ≤�
ˆL

∇
[
�

∇
ν(t)λmin([

≤

≤

∇

[
��
1

∇
[
��
2F (x(t))]−

∇

1).

2F (x∗)]−

2F (x(t))]−

2F (x∗)
x(t)

x∗

−

�

��∇
1

��

− ∇

2F (x(t))

�

Hence, we can obtain that for any u

2F (x(t))]−

1)uT y

yT ([

≤

2F (x(t))]−

1)y

ν(t)λmin([

2F (x(t))]−

1)yT y,

≤

∇

Rd,
2F (x∗)]−

1

∈

∇

[

−

∇

(1

−

ν(t))uT [

2F (x(t))]−

1u

uT [

2F (x∗)]−

1u

(1 + ν(t))uT [

2F (x(t))]−

1u.

∇

≤

∇

≤

∇

Accordingly, we have

(1

ν(t))

−

u

u
�M ≤ �

�

�M ∗ ≤

(1 + ν(t))

u

�M .

�

F (x(t+1))

�∇

�0 +

�M ∗ ≤

�

�1

1 + ν(t)
1

ν(t) �∇

F (x(t))

�M ∗ +

(1

1

�0 �

−

−

2
�0)2 ·

ˆLκ
µ√µ

−

(1 + ν(t))2

1

ν(t) �∇

−

F (x(t))

2
M ∗

�

ν(t)λmin([

−
which yields

∇

That is,

6

Appendix C. Proofs of theorems of Section 4

Proof of Theorem 4 If S is an �0-subspace embedding matrix w.r.t. B(x(t)), then we
have

(1

�0)

∇

−

�

2F (x(t))

[B(x(t))]T ST SB(x(t))

(1 + �0)

2F (x(t)).

(17)

�

∇

By simple transformation and omitting �2

0, (17) can be transformed into

(1

−

�0)[B(x(t))]T ST S

2B(x(t))

2F (x(t))

(1 + �0)[B(x(t))]T ST SB(x(t)).

∇

� ∇

�

The convergence rate can be derived directly from Theorem 3.

Appendix D. Proofs of theorems of Section 5

Proof of Theorem 5 By Lemma 12, when
property:

|S| ≥

16K2 log(2d/δ)
σ2�2
0

, H (t) has the following

The above property implies the following:

H (t)

�

− ∇

2F (x(t))

� ≤

�0σ.

yT (H (t)

|

�0σyT y

− ∇

2F (x(t)))y
yT (H (t)

| ≤

�0σyT y,
2F (x(t)))y

⇒ −

H (t)

(1

−

⇒

⇒

≤
�0σI
−
�0)H (t)

� ∇

� ∇

− ∇
2F (x(t))
2F (x(t))

�

�

≤
H (t) + �0σI
(1 + �0)H (t).

�0σyT y

The convergence rate can be derived directly from Theorem 3.

Proof of Theorem 6
By Lemma 12, when

16K2 log(2d/δ)
β2

, we have

|S| ≥

2F (x(t))

H (t)

β,

−

|S|� ≤

�∇

with probability at least 1

δ. Hence, we can derive

−

yT (
∇
|
yT H (t)
|S|
yT H (t)y

y

yT H (t)y

⇒

⇒

⇒

−

−

−

2F (x(t))

−
βyT y

H (t)
|S|
yT

≤

)y

βyT y

| ≤
2F (x(t))y

∇
βyT y
(1)

αyT y

−
(α + β)yT y

≤
yT

yT H (t)
|S|

≤

y + βyT y

yT

2F (x(t))y

yT H (t)y

αyT y + βyT y

∇
2F (x(t))y

≤
(2)
yT H (t)y + (β

−

≤

∇

≤

α)yT y.

−

7

(2)

≤

For
we have

case, we consider two cases respectively. The ﬁrst case is β

σ/2

−

α

≤

≤

β, and

Now we ﬁrst consider

case, we have

(1)

≤
yT H (t)y
yT H (t)y

⇒

⇒

⇒

−

≤

≤

∇

∇

(α + β)yT y
yT

2F (x(t))y
yT
2F (x(t))y + (α + β)yT y

∇

≤

yT H (t)y

yT

2F (x(t))y +

2F (x(t))y

yT

α + β
σ
∇
2F (x(t))y

yT

2F (x(t))y

≤

∇

yT H (t)y

1 +

≤
�
α + β
σ + α + β
α + β
σ + α + β

�

�

1

1

−

−

⇒

�

⇒

�

α + β
σ

yT

∇

�
yT H (t)y

H (t)

F (x(t)).

�

2F (x(t))y
2F (x(t))y

2F (x(t))y

yT
yT

yT

∇

∇

∇

β

α

1

−

−
σ

�
yT

�
2F (x(t))y

⇒

⇒

⇒

⇒

∇

2F (x(t))

⇒∇

�

�

yT H (t)y + (β

α)yT y
−
yT H (t)y

≤
2F (x(t))y

≤
yT H (t)y

α)yT y
α

−
−
σ
∇
2F (x(t))y

yT

≤

−

−
yT

(β
β

∇

≤

�
1 +

1 +

σ

β
−
σ + α

−
α

≤
α

β

−
(β

β

�

−

−

α)

�
H (t).

yT H (t)y

yT H (t)y

For the case β < α, we can derive

yT

2F (x(t))y

yT H (t)y + (β

α)yT y

yT H (t)y

∇
2F (x(t))

≤
(1 + 0)H (t).

−

≤

Hence, for β

σ

α, we have

⇒∇

−

≤

1

�

�

�

α + β
σ + α + β

−

H (t)

F (x(t))

1 +

�

�

�

β
−
σ + α

α

−

β

�

H (t).

Therefore, �0 in Theorem 3 can be set as follows:

�0 = max

β
−
σ + α

α

−

�

α + β
σ + α + β

,

β

.

�

The convergence properties can derived from Theorem 3 directly.

8

Proof of Theorem 7
We denote the SVD of H (t)
S

as follows

H (t)
S

= U ˆΛU T = Ur ˆΛrU T

r + U

ˆΛ

rU T
r.
\

\

r

\

By Lemma 12, when

16K2 log(2d/δ)
β2

, we have

|S| ≥

2F (x(t))

H (t)

β,

−

|S|� ≤

�∇

with probability at least 1

δ. Hence, we can derive

−

yT (
|
∇
yT H (t)
|S|

y

2F (x(t))

−
βyT y

⇒

⇒

−

yT H (t)y + yT U

≤
r(ˆΛ
yT H (t)y + yT U

\

)y

H (t)
|S|
yT

∇

r −
\
r(ˆΛ

\

\
βIr

yT H (t)y

yT U

−

�

⇒

βyT y

| ≤
yT H (t)
2F (x(t))y
≤
|S|
ˆλ(t)
r+1I)U T
βyT y
ry
−
≤
\
ˆλ(t)
ry + βyT y
r+1I)U T
\

r −

(β + ˆλ(t)

r+1)I

ˆΛ

r �

\

r −

\

y + βyT y

yT

2F (x(t))y

∇

U T y

yT

2F (x(t))y

(1)

≤

∇

yT H (t)y + yT U

βIr

�

(β

−

ˆλ(t)
r+1)I

r + ˆΛ

\

r �

\

U T y

≤

(2)

≤

Now we ﬁrst consider

case, we have

(1)

≤

U T y

yT

2F (x(t))y

(1)

≤

∇

r �

\

yT H (t)y

yT U

βIr

−

≤

≤

ˆΛ

(β + ˆλ(t)

r+1)I
�
r −
\
2F (x(t))y + (β + ˆλ(t)
r+1)yty
β + ˆλ(t)
r+1
σ
2β + ˆλr+1
σ

2F (x(t))y +

2F (x(t))y +

yT

yT

∇

∇

∇

yT

yT

yT

2F (x(t))y

2F (x(t))y

∇

∇

≤
2β + λ(t)
r+1
σ + 2β + λ(t)

r+1 �

yT H (t)y

yT

2F (x(t))y.

≤

∇

yT H (t)y

yT H (t)y

yT H (t)y

⇒

⇒

⇒

1

⇒ �

−

Hence we have

β

1 +

�

λ(t)
r+1 −

β �

H (t)

2F (x).

� ∇

9

Now we ﬁrst consider

case, we have

yT

2F (x(t))y

yT H (t)y + yT U

∇

βIr

�

(β

−

ˆλ(t)
r+1)I

r + ˆΛ

\

r �

\

U T y

(2)

≤

≤

≤

yT H (t)y +

yT H (t)y

β
ˆλ(t)
r+1

β

1 +

≤ �

λ(t)
r+1 −

β �

yT H (t)y,

where the last inequality is because λ(t)

r+1 −

β

≤

ˆλ(t)
r+1. Hence, we have

2F (x)

∇

1 +

� �

β

λ(t)
r+1 −

β �

H (t).

Hence, we have

�0 = max

β

�

λ(t)
r+1 −

β

,

2β + λ(t)
r+1
σ + 2β + λ(t)

r+1 �

< 1,

because β

λ(t)
r+1

2 .

≤

The convergence properties can be derived directly by Theorem 3.

Appendix E. Subsampled Hessian and Gradient

In fact, we can also subsample gradient to accelerate the subsampled Newton method. The
detailed procedure is presented in Algorithm 5 [1, 2].

Theorem 13 Let F (x) satisfy the properties described in Theorem 3. We also assume
Eqn. (9) and Eqn. (10) hold and let 0 < δ < 1 and 0 < �0 < 1/2 be given. Let
|Sg|
be set such that Eqn. (2) holds and it holds that

|SH |

and

g(x(t))

�

− ∇

F (x(t))

� ≤

�2
κ �∇

F (x(t))

.

�

The direction vector p(t) is computed as in Algorithm 5. Then for t = 1, . . . , T , we have the
following convergence properties:

(a) There exists a suﬃcient small value γ, 0 < ν(t) < 1, and 0 < η(t) < 1 such that when

x(t)

�

x∗� ≤

−

γ, then for each iteration, it holds that

F (x(t+1))

�∇

�M ∗ ≤

(�0 + 2�2 + 4η(t))

1 + ν(t)
1

ν(t) �∇

−

F (x(t))

�M ∗

with probability at least 1

δ.

−

10

Algorithm 5 Subsampled Hessian and Subsampled Gradient.
1: Input: x(0), 0 < δ < 1, 0 < �0 < 1;
2: Set the sample size
3: for t = 0, 1, . . . until termination do
4:

Select a sample set

|SH |

and

.

|Sg|
SH , of size
Sg of size

1g(x(t));
p(t);

−

Select a sample set
Calculate p(t) = [H (t)]−
Update x(t+1) = x(t)

5:

6:
7:
8: end for

and construct H (t) = 1
|S|
and calculate g(x(t)) = 1
|Sg|

�

j

2fj(x(t));
fi(x(t)).

∈S ∇
i

∈Sg ∇

|S|
|Sg|

�

(b) If

2F (x(t)) is also Lipschitz continuous and

x(t)

satisﬁes Eqn. (6), then for each

∇

iteration, it holds that

{

}

F (x(t+1))

�∇

(�0 + 2�2)

�M ∗ ≤

with probability at least 1

δ.

−

1 + ν(t)
1

ν(t) �∇

−

F (x(t))

�M ∗ +

8 ˆLκ
µ√µ

(1 + ν(t))2

1

ν(t) �∇

−

F (x(t))

2
M ∗

.

�

In common cases, subsampled gradient g(x(t)) needs to subsample over 80% of samples
to guarantee convergence of the algorithm. Roosta-Khorasani and Mahoney [2] showed
fi(x(t))
2), where G(x(t)) = maxi �∇
F (x(t))
G(x(t))2κ2/(ν2(t)
for
that it needs
�
i = 1, . . . , n. When x(t) is close to x∗,
F (x(t))
will go to n as
is close to 0. Hence
�
iteration goes. This is the reason why the Newton method and variants of the subsampled
Newton method are very sensitive to the accuracy of subsampled gradient.

|Sg| ≥

�∇
�∇

|Sg|

�

The proof of Theorem 13 is almost the same with Theorem 3. For completeness, we

give the detailed proof as follows.
Proof By Taylor’s theorem, we obtain

F (x(t+1))

=

=

∇

∇

∇

+

=

F (x(t))
1

[

∇

0

�
2F (x(t))

1
2

F (x(t)) +

2F (x(t))(

p(t)) +

2F (x(t) + sp(t))

2F (x(t))](

p(t))ds

∇

−

2F (x(t))[H (t)]−

1

∇
0
�
F (x(t)) +

− ∇
2F (x(t))[H (t)]−

1

−
F (x(t))

2F (x(t))p(t)

∇

− ∇

1

[

− ∇
2F (x(t) + sp(t))

∇
2F (x(t))](

∇
p(t))ds

− ∇

−

I

[

2F (x(t))]

2 [H (t)]−

1[

1

2F (x(t))]

1
2

−

∇

�
2F (x(t))([H (t)]−

�

1

∇

�
+

∇

F (x(t))

p(t)) +

∇

−

∇
1

0

�

[

∇

1
2

−

2F (x(t))

∇

F (x(t))

∇

� �
2F (x(t) + sp(t))

�
2F (x(t))](

− ∇

p(t))ds.

−

Hence, we have the following identity

2F (x(t))

F (x(t+1)) =

I

[

2F (x(t))]

2 [H (t)]−

1[

1

2F (x(t))]

1
2

∇

�

1
2

−

�

∇

∇
2F (x(t))]

�
+ [

+ [

2F (x(t))]−

1
2

−

∇

∇

1

1

2 ([H (t)]−
1

[

∇

0

�

11

∇
F (x(t))

� �

p(t))

−

∇
2F (x(t) + sp(t))

2F (x(t))

∇

F (x(t))

∇

1
2

−

�

2F (x(t))](

p(t))ds.

− ∇

−

Further more, we deﬁne M =

2F (x(t))

1

−

, we can obtain

F (x(t+1))

�∇

�M ≤

[

∇
2F (x(t))]
�
∇
2F (x(t))]

1
2

−
[

�

∇

I

�
+
�
�

+

1

2 [H (t)]−

�

1[

2F (x(t))]

1
2

∇
1(

∇

F (x(t))

�∇
�
g(x(t)))
�
�
−

[H (t)]−
1

��
1
2

F (x(t))

�M

�
2F (x(t))

��
We will bound the three terms on the right hand of above equation seperately.

0 �∇

− ∇

∇

�

�

�

[

2F (x(t))]−

2F (x(t) + sp(t))

p(t)

ds.

�

�∇

F (x(t))

F (x(t))

�M ≤

�M .
�0�∇
σmin(B) and condition

�
AB
�
�
� ≥ �

�

A

�

g(x(t))

�

−

F (x(t))

g(x(t))

−

�

��∇

For the ﬁrst term, using Lemma 9, we have

[

2F (x(t))]

2 [H (t)]−

1[

1

2F (x(t))]

1
2

−

∇

∇

For the second term, by the fact that

F (x(t))

∇

F (x(t))

�2�∇
2F (x(t))]

1
2

, we obtain

�
[H (t)]−

1(

I

�
�
�
� ≤
[

F (x(t))

g(x(t)))

∇

−

λmin([

∇

1
2 )

2F (x(t))]−

1
2 )

��

�
[H (t)]−

1

[H (t)]−

1

F (x(t))

��∇

�M

�

∇

[

= �

∇
λmin([

��
1
2F (x(t))]
2

�
2F (x(t))]−
2F (x(t))]

1
2

∇
[

�2

≤

∇
�
λmin([
�2

�
2F (x(t))]−
∇
F (x(t))

1

2 ) �

�0 �∇

≤

≤

1
−
2�2�∇

F (x(t))

�M

�M

For the third term, we bound it for the case that

∇
2F (x) is Lipschitz continuous respectively.

and the case

∇

(a) Now we consider the case that
close to the optimal point x∗. Because
small value δ such that Eqn. (14) and Eqn. (15) hold when

∇
∇

2F (x) is not Lipschitz continuous but is continuous
2F (x) is continuous near x∗, there exists a suﬃcient

δ.

By µ-strong convexity, we have

2F (xt)]−

1

[

�

∇

� ≤

1
µ for all x(t) suﬃciently close to x∗.

x(t)
�

x∗� ≤

−

Then we have

2F (x) is not Lipschitz continuous

p(t)

=

[H (t)]−

1

g(x(t))

�

�

�

��

� ≤

1 + �2/κ
(1

�0)µ �∇

−

F (x(t))

� ≤

(1

−

2
�0)µ �∇

F (x(t))

.

�

δ, combining condition (15), we have

Hence, with

x

�

−

2F (x(t) + sp(t))

2F (x(t))

p(t)

ds

��

�

− ∇

x∗� ≤
2F (x(t))]−

2F (x(t))]−

2F (x(t))]−

[

[

[

�

∇

≤�

∇

≤�

∇

1

�

0 �∇
1

�

�

0

�

1
2

1
2

1
2

(1
�
−
2F (x(t))]−

p(t)

ds

�
µη(t)
√κ �∇

µη(t)
√κ �
2
�0)µ
1
2

2η(t)
�0
1

−
4η(t)

[

∇

�
√κλmin([
F (x(t))

�∇

≤

≤

∇
�M ,

12

F (x(t))

�

�
2F (x(t))]−

1
2 )

λmin([

∇

2F (x(t))]−

1
2 )

F (x(t))

�∇

�

Therefore, we have

By Eqn. (14), we have

F (x(t+1))

�∇

�M ≤

F (x(t))

�0�∇
=(�0 + 2�2 + 4η(t))

�M + 2�2�∇
�∇

F (x(t))
�M .

F (x(t))

�M + 4η(t)

�∇

F (x(t))

�M

ν(t)

yT y

yT ([

2F (x∗)]−

1

[

2F (x(t))]−

1)y

ν(t)

yT y,

−

λmax(

∇
ν(t))yT [
y
ν(t))

2F (x∗))

≤
2F (x(t))]−
y

∇
�M ≤ �

�M ∗ ≤

1y

∇
yT [
≤
(1 + ν(t))

∇

⇒

−

(1

(1

−

⇒

�
By this relationship between

� · �M and

∇
1y

−
2F (x∗)]−
�M .
y
�
� · �M ∗, we get

≤

≤
(1 + ν(t))yT [

λmax(
∇
2F (x(t))]−

2F (x∗))
1y

∇

F (x(t+1))

�∇

�M ∗ ≤

(�0 + 2�2 + 4η(t))

1 + ν(t)
1

ν(t) �∇

−

F (x(t))

�M ∗

(b) Now we consider the case that
∇
The same to the previous proof, we have

2F (x) is Lipschitz continuous with parameter ˆL.

p(t)

=

[H (t)]−

1

g(x(t))

�

�

�

��

� ≤

1 + �2/κ
(1

�0)µ �∇

−

F (x(t))

� ≤

(1

−

2
�0)µ �∇

F (x(t))

.

�

Because

2F (x) is Lipschitz continuous with parameter ˆL, we have

2F (x(t) + sp(t))

2F (x(t))

p(t)

ds

��

�

− ∇

2F (x(t))]−

2F (x(t))]−

1

�

0 �∇
1

�

1
2

1
2

sL

p(t)

2ds

�

�

�

0

�
1
2

�

∇

�

∇

[

[

∇

=

≤�
ˆL
2 �
ˆL
2 �

≤

[

[

∇

∇

2F (x(t))]−

2
min([
λ−

2F (x(t))]−

1

2 )λ2

min([

2F (x(t))]−

1
2 )

2F (x(t))]−

1
2

2
min([
λ−

2F (x(t))]−

1
2 )

�
2 ˆLλmax(

∇
2F (x(t)))

2F (x(t)))
λmin(

∇

(1

�

−

2
�0)λmin(

F (x(t))

∇
2
M

�

2F (x(t))) �∇

∇

p(t)

2

�

�

2

�

∇

∇

�

=

≤

�0)2λ2

min(

∇
F (x(t))

(1
−
8 ˆLκ
µ√µ �∇

2
M ,

�

2F (x(t)))

�∇

F (x(t))

2
M

�

where the last inequality is because �0 ≤

1/2. Hence, we have

F (x(t+1))

(�0 + 2�2)

�M ≤

F (x(t))

�M +

�∇

�∇

8 ˆLκ
µ√µ �∇

F (x(t))

2
M .

�

By the Lipschitz continuity of

2F (x) and the condition

∇

x∗

x(t)

�

−

� ≤

µ
ˆLκ ≤

λmin(
ˆLκ(

2F (x∗))
∇
2F (x(t)))

,

∇

13

we obtain

2F (x∗)]−

1

[

2F (x(t))]−

1

2F (x∗)]−

1

2F (x(t))]−

1

[

�

∇

−

∇

[
� ≤�
ˆL

∇
[
�

∇
ν(t)λmin([

≤

≤

∇

[
��
1

∇
[
��
2F (x(t))]−

∇

1).

2F (x∗)]−

2F (x(t))]−

2F (x∗)
x(t)

x∗

−

�

��∇
1

��

− ∇

2F (x(t))

�

Hence, we can derive

ν(t)λmin([
ν(t))yT [
y
ν(t))

−

−
(1

(1

−

�

yT ([

1)yT y
1y

∇

2F (x(t))]−
2F (x(t))]−
y

∇
�M ≤ �

�M ∗ ≤

≤
yT [
≤
(1 + ν(t))

∇

∇
2F (x∗)]−
�M .
y

�

1

2F (x∗)]−
1y

≤

⇒

⇒

Hence, we have

[

2F (x(t))]−

∇

−
(1 + ν(t))yT [

1)y
≤
2F (x(t))]−

ν(t)λmin([
1y

∇

∇

2F (x(t))]−

1)yT y,

F (x(t+1))

�∇

(�0 + 2�2)

�M ∗ ≤

1 + ν(t)
1

ν(t) �∇

−

F (x(t))

�M ∗ +

8 ˆLκ
µ√µ

(1 + ν(t))2

1

ν(t) �∇

−

F (x(t))

2
M ∗

�

Table 2: Datasets Description
n
Dataset
8124
mushrooms
32561
a9a
581012
Covertype

d
112
123
54

source
UCI
UCI
UCI

Appendix F. Unnecessity of Lipschitz continuity of Hessian

In this section, we validate our theoretical results about unnecessity of the Lipschitz con-
2F (x). We conduct experiment on the primal problem for the linear
tinuity condition of
SVM which can be written as

∇

min
x

F (x) =

1
2 �

x

2 +

�

C
2n

�(bi,

x, ai�

)

�

n

�i=1

where (ai, bi) denotes the training data, x deﬁnes the separating hyperplane, C > 0, and
In our experiment, we choose Hinge-2 loss as our loss function
�(
whose deﬁnition is

) is the loss function.

·

Let SV (t) denote the set of indices of all the support vectors at iteration t, i.e.,

�(b,

x, a

) = max(0, 1

�

�

b

x, a
�

�

−

)2.

SV (t) =

i : bi�
{

x(t), ai�

.

< 1
}

14

Then the Hessian matrix of F (x(t)) can be written as

2F (x(t)) = I +

∇

1
n

aiaT
i .

SV (t)

�i
∈

From the above equation, we can see that

2F (x) is not Lipschitz continuous.

Without loss of generality, we use the Subsampled Newton method (Algorithm 2) in our
experiment. We sample 5% support vectors in each iteration. Our experiments on three
datasets whose detailed description is in Table 2 and report our results in Figure 3.

From Figure 3, we can see that Subsampled Newton converges linearly and the Newton
method converges superlinearly. This matches our theory that the Lipschitz continuity of

2F (x) is not necessary to achieve a linear or superlinear convergence rate.

∇

mushrooms

a9a

Subsampled Newton
Newton

Subsampled Newton
Newton

covtype

Subsampled Newton
Newton

)
r
r
e
(
g
o

l

5

0

-5

-10

-15

-20

-25

-30

-35

)
r
r
e
(
g
o

l

160

140

120

100

80

60

40

20

0

-20

-40

∇

)
r
r
e
(
g
o

l

5

0

-5

-10

-15

-20

-25

-30

-35

5

10

15

20

25

2

4

6

10

12

14

5

10

20

25

30

iteration

(a) mushrooms.

8
iteration

(b) a9a.

15
iteration

(c) covtype.

Figure 3: Convergence properties on diﬀerent datasets.

References

[1] Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of
stochastic hessian information in optimization methods for machine learning. SIAM
Journal on Optimization, 21(3):977–995, 2011.

[2] Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled newton methods ii:

Local convergence rates. arXiv preprint arXiv:1601.04738, 2016.

[3] Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations

and Trends R
�

in Machine Learning, 8(1-2):1–230, 2015.

15

