Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

A. Proofs for Realizable Setting

Proof of Lemma 3. Let ∆ := (cid:98)w − w∗ be the difference between the true answer and solution to the optimization problem.
Let S to be the support of w∗ and let Sc = [d] \ S be the complements of S. Consider the permutation i1, . . . , id−k of
Sc for which |∆(ij)| ≥ |∆(ij+1)| for all j. That is, the permutation dictated by the magnitude of the entries of ∆ outside
of S. We split Sc into subsets of size k according to this permutation: Deﬁne Sj, for j ≥ 1 as {i(j−1)k+1, . . . , ijk}. For
convenience we also denote by S01 the set S ∪ S1.
Now, consider the matrix XS01 ∈ Rt×|S01| whose columns are those of X with indices S01. The Restricted Isometry
Property of X dictates that for any vector c ∈ RS01 ,

(1 − (cid:15)) (cid:107)c(cid:107)2 ≤

(cid:107)XS01 c(cid:107)2 ≤ (1 + (cid:15)) (cid:107)c(cid:107)2 .

Let V ⊆ Rt be the subspace of dimension |S01| that is the image of the linear operator XS01, and let PV ∈ Rt×t be the
projection matrix onto that subspace. We have, for any vector z ∈ Rt that

1
√
n

1
√
n

(1 − (cid:15)) (cid:107)PV z(cid:107) ≤

(cid:13)
(cid:13)X T
S01

z(cid:13)
(cid:13) ≤ (1 + (cid:15)) (cid:107)PV z(cid:107)

(cid:107)PV X∆(cid:107) ≤

√

1
t(1 − (cid:15))

(cid:13)
(cid:13)X T
S01

X∆(cid:13)
(cid:13)

PV X∆ = PV X∆(S01) +

PV X∆(Sj)

(cid:88)

j≥2

PV X∆(Sj) = XS01 cj

∀c, c(cid:48) 1
n

(cid:104)XS(cid:48)c, XS(cid:48)(cid:48) c(cid:48)(cid:105) ≤ (2(cid:15) − (cid:15)2) (cid:107)c(cid:107)2 (cid:107)c(cid:48)(cid:107)2

We apply this to z = X∆ and conclude that

We continue to lower bound the quantity of (cid:107)PV X∆(cid:107). We decompose PV X∆ as

(10)

(11)

Now, according to the deﬁnition of V we that there exist vectors {cj}j≥2 in R|S01| for which

We now invoke Lemma 1.1 from (Candes & Tao, 2005) stating that for any S(cid:48), S(cid:48)(cid:48) with |S(cid:48)| + |S(cid:48)(cid:48)| ≤ 3k it holds that

We apply this for S01, Sj, j ≥ 2 and conclude that

Dividing through by (cid:107)PV X∆(Sj)(cid:107)2, we get

(cid:107)PV X∆(Sj)(cid:107)2

2 = (cid:104)PV X∆(Sj), X∆(Sj)(cid:105) ≤ 2(cid:15)t (cid:107)cj(cid:107)2 · (cid:107)∆(Sj)(cid:107) ≤

(cid:107)PV X∆(Sj)(cid:107)2 · (cid:107)∆(Sj)(cid:107)2 .

√

2(cid:15)
t
1 − (cid:15)

√

2(cid:15)
t
1 − (cid:15)

(cid:107)PV X∆(Sj)(cid:107) ≤

(cid:107)∆(Sj)(cid:107) .

(12)

Let us now bound the sum (cid:107)∆(Sj)(cid:107). By the deﬁnition of Sj we know that any element i ∈ Sj has the property ∆(i) ≤
(1/k) (cid:107)∆(Sj−1)(cid:107)1. Hence

(cid:88)

j≥2

(cid:107)∆(Sj)(cid:107) ≤ (1/

k)

(cid:107)∆(Sj)(cid:107)1 = (1/

k) (cid:107)∆(Sc)(cid:107)1

√

√

(cid:88)

j≥1

We now combine this inequality with Equations (10), (11) and (12)

1
t

(cid:13)
(cid:13)X T
S01

X∆(cid:13)

(cid:13) ≥

(cid:107)PV X∆(cid:107)

1 − (cid:15)
√
t
1 − (cid:15)
√
t

1 − (cid:15)
√
t

1 − (cid:15)
√
t

≥

≥

≥

(cid:107)PV X∆(S01)(cid:107) −

(cid:107)PV X∆(Sj)(cid:107)

1 − (cid:15)
√
n

(cid:88)

j≥2

(cid:107)X∆(S01)(cid:107) − 2(cid:15)

(cid:107)∆(Sj)(cid:107)

(cid:88)

j≥2

2(cid:15)
√
k

(cid:107)X∆(S01)(cid:107) −

(cid:107)∆(Sc)(cid:107)1

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

The third inequality holds since X∆(S01) ∈ V hence PV X∆(S01) = X∆(S01). We continue to bound the expression by
claiming that (cid:107)∆(S)(cid:107)1 ≥ (cid:107)∆(Sc)(cid:107)1. This holds since in Sc, (cid:98)wSc = ∆(Sc) hence

(cid:107)w∗(cid:107)1 = (cid:107) (cid:98)w − ∆(Sc) − ∆(S)(cid:107)1 ≤ (cid:107) (cid:98)w(cid:107)1 + ((cid:107)∆(S)(cid:107)1 − (cid:107)∆(Sc)(cid:107)1)

Now, the optimality of (cid:98)w implies (cid:107) (cid:98)w(cid:107)1 ≤ (cid:107)w∗(cid:107)1, hence indeed (cid:107)∆(S)(cid:107)1 ≥ (cid:107)∆(Sc)(cid:107)1.

(cid:107)∆(Sc)(cid:107)1 ≤ (cid:107)∆(S)(cid:107)1 ≤

k (cid:107)∆(S)(cid:107)2 ≤ (cid:107)∆(S01)(cid:107)2 ≤

(cid:107)X∆(S01)(cid:107)

√

√

k
(1 − (cid:15))

√

t

We continue the chain of inequalities

1
t

(cid:13)
(cid:13)X T
S01

X∆(cid:13)

(cid:13) ≥

1 − (cid:15)
√
n

(cid:107)X∆(S01)(cid:107) −

2(cid:15)
√
k

(cid:107)∆(Sc)(cid:107)1
√

(cid:32)

1 − (cid:15)
√
n

−

2(cid:15)
√
k

·

k
(1 − (cid:15))

√

n

(cid:33)

≥ (cid:107)X∆(S01)(cid:107)

=

(1 − (cid:15))2 − 2(cid:15)

(1 − (cid:15))

t

√

(cid:107)X∆(S01)(cid:107)

Rearranging we conclude that

(cid:107)∆(S01)(cid:107) ≤

√

(cid:107)X∆(S01)(cid:107)

1

(1 − (cid:15))

t

1
((1 − (cid:15))2 − 2(cid:15))t

(cid:13)
(cid:13)X T
S01

X∆(cid:13)
(cid:13)

(cid:13)X T X∆(cid:13)
(cid:13)
(cid:13)∞

≤

≤

√

2k
(1 − 4(cid:15))t

(cid:115)

≤ C

dk log(d/δ)
tk0

(cid:18)

σ +

d
k0

(cid:19)

(cid:107)w∗(cid:107)1

(RIP of X)

(since for any z ∈ R2k, (cid:107)z(cid:107)2 ≤

2k (cid:107)z(cid:107)∞)

√

(Lemma 14 and (cid:15) < 1/5)

for some constant C. We continue our bound on (cid:107)∆(cid:107) by showing that (cid:107)∆(Sc

01)(cid:107) ≤ (cid:107)∆(S01)(cid:107)

(cid:107)∆(Sc

01)(cid:107)2

2

(i)
≤ (cid:107)∆(Sc)(cid:107)2
1 ·

(cid:88)

1
j2 ≤

1
k

j≥k+1

(cid:107)∆(Sc)(cid:107)2

1 ≤

(cid:107)∆(S)(cid:107)2

1 ≤ (cid:107)∆(S)(cid:107)2
2 .

1
k

Inequality (i) holds due to the following: Let αi be the absolute value of the i’th largest (in absolute value) element of
∆(Sc). It obviously holds that αi ≤ (cid:107)∆(Sc)(cid:107)1 /i. Now, according to the deﬁnition of S01 we have that (cid:107)∆(Sc
2 =
(cid:80)

01)(cid:107)2

j≥k+1 α2

i and the inequality follows. Hence,

(cid:107)∆(Sc

01)(cid:107)2 ≤ (cid:107)∆(S)(cid:107)2 ≤ (cid:107)∆(S01)(cid:107)2 .

We conclude that

√

(cid:107)∆(cid:107)2 ≤

2 (cid:107)∆(S01)(cid:107)2 ≤ C

dk log(d/δ)
tk0

(cid:18)

σ +

d
k0

(cid:19)

(cid:107)w∗(cid:107)1

for some universal constant C > 0. Since (cid:107)∆(S)(cid:107)1 ≥ (cid:107)∆(Sc)(cid:107)1 and |S| ≤ k we get that

(cid:107)∆(cid:107)1 ≤ 2 (cid:107)∆(S)(cid:107)1 ≤ 2

k (cid:107)∆(S)(cid:107)2 ≤ 2

k (cid:107)∆(cid:107)2

√

and the claim follows.

(cid:115)

√

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

Proof of Lemma 4. Let S be the support of w∗. We can decompose the square of the left hand side as

(cid:13)
(cid:13)

(cid:13) (cid:98)w( (cid:101)S) − w∗(cid:13)

2
(cid:13)
(cid:13)
2

=

(cid:88)

i∈S∩ (cid:101)S

( (cid:98)w(i) − w∗(i))2 +

( (cid:98)w(i))2 +

(cid:88)

i∈ (cid:101)S\S

(cid:88)

(w∗(i))2.

i∈S\ (cid:101)S

We upper bound the last sum on the right hand side as

(cid:88)

(w∗(i))2 =

(cid:88)

[( (cid:98)w(i) − w∗(i)) + ( (cid:98)w(i))]2

i∈S\ (cid:101)S

( (cid:98)w(i) − w∗(i))2 + ( (cid:98)w(i))2

i∈S\ (cid:101)S
(cid:88)

≤ 2

i∈S\ (cid:101)S
(cid:88)

i∈S\ (cid:101)S

≤ 2

( (cid:98)w(i) − w∗(i))2 + 2

( (cid:98)w(i))2 ,

(cid:88)

i∈ (cid:101)S\S

where ﬁrst inequality follows from the elementary inequality (a + b)2 ≤ 2a2 + 2b2 and the second inequality is due to the
fact that (cid:101)S contains top k entries of (cid:98)w in absolute value and |S \ (cid:101)S| = | (cid:101)S \ S|. Hence,

(cid:13)
(cid:13)

(cid:13) (cid:98)w( (cid:101)S) − w∗(cid:13)

2
(cid:13)
(cid:13)
2

( (cid:98)w(i) − w∗(i))2 +

(cid:88)

( (cid:98)w(i))2 +

(cid:88)

(w∗(i))2

( (cid:98)w(i) − w∗(i))2 + 2

i∈ (cid:101)S\S
(cid:88)

i∈S\ (cid:101)S
( (cid:98)w(i) − w∗(i))2 + 3

(cid:88)

( (cid:98)w(i))2

( (cid:98)w(i) − w∗(i))2 + 2

( (cid:98)w(i) − w∗(i))2 + 3

( (cid:98)w(i))2

i∈ (cid:101)S\S
(cid:88)

i∈ (cid:101)S\S

i∈S\ (cid:101)S
(cid:88)

i∈S\ (cid:101)S

(cid:88)

( (cid:98)w(i))2

i∈ (cid:101)S\S

(cid:88)

i∈S∩ (cid:101)S
(cid:88)

=

≤

i∈S∩ (cid:101)S
(cid:88)

≤ 2

i∈S∩ (cid:101)S
(cid:88)

i∈S

d
(cid:88)

i=1

= 2

( (cid:98)w(i) − w∗(i))2 + 3

≤ 3

( (cid:98)w(i) − w∗(i))2

= 3 (cid:107) (cid:98)w − w∗(cid:107)2
2 .

Taking square root ﬁnishes the proof.

Lemma 14. There exists a universal constant C > 0 such that, with probability at least 1 − δ, the convex program (3) is
feasible and its optimal solution (cid:98)w satisﬁes

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

X T

(cid:13)
(cid:13)
t Xt( (cid:98)w − w∗)
(cid:13)
(cid:13)∞

(cid:115)

≤ C

d log(d/δ)
tk0

(cid:18)

σ +

d
k0

(cid:19)

.

(cid:107)w∗(cid:107)1

We note that the above lemma is beyond simple triangle inequality on the feasibility constraints, as the left hand side
depends on actual design matrix Xt which we do not observe, instead of (cid:98)Xt.

Proof. To simplify notation, we drop subscript t. Namely, let X = Xt, (cid:98)X = Xt and (cid:98)D = (cid:98)Dt, and also let η =
(η1, η2, . . . , ηt) be the vector of noise variables.

First, we show that w∗ satisﬁes the constraint of (3) with probability at least 1 − δ. We upper bound

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

(cid:98)X T (Y − (cid:98)Xw∗) +

(cid:98)Dw∗

(cid:98)X T (X − (cid:98)X) +

(cid:98)D

w∗ +

(cid:98)X T η

1
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

=

≤

(cid:13)
(cid:20) 1
(cid:13)
(cid:13)
t
(cid:13)
(cid:13)
(cid:20) 1
(cid:13)
(cid:13)
t
(cid:13)

(cid:21)

(cid:21)

1
t
1
t

1
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

+

1
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13) (cid:98)X T η
(cid:13)

(cid:13)
(cid:13)
(cid:13)∞

(cid:98)X T (X − (cid:98)X) +

(cid:98)D

w∗

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

We ﬁrst bound the left summand. By Lemma 15, we have

(cid:13)
(cid:20) 1
(cid:13)
(cid:13)
t
(cid:13)

(cid:98)X T (X − (cid:98)X) +

(cid:98)D

w∗

≤ (cid:107)w∗(cid:107)1 ·

(cid:21)

1
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ (cid:107)w∗(cid:107)1

+

( (cid:98)X − X)T ( (cid:98)X − X) −

(cid:19)

1
t

(cid:98)D

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:98)X T (X − (cid:98)X) +
(cid:13)
(cid:13)
X T ( (cid:98)X − X)
(cid:13)
(cid:13)∞

1
t

(cid:98)D

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
1
(cid:13)
(cid:13)
t
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t
1
t
(cid:115)

≤ (cid:107)w∗(cid:107)1 C ·

d3 log(d/δ)
tk0

3

.

For the right summand, since η is vector of i.i.d Gaussians with variance σ2, with probability at least 1 − δ,

1
t

(cid:13)
(cid:13) (cid:98)X T η
(cid:13)

(cid:13)
(cid:13)
(cid:13)∞

≤ C

(cid:112)log(d/δ) · max

(cid:13)
(cid:13)
(cid:13) (cid:98)X(i)

(cid:13)
(cid:13)
(cid:13)2

i∈[d]

σ
t

where (cid:98)X(1), (cid:98)X(2), . . . , (cid:98)X(d) are the columns of (cid:98)X. Since the absolute value of the entries of (cid:98)X is at most d/k0, we have
(cid:13)
(cid:13)
(cid:13) (cid:98)X(i)

≤ (cid:112)td/k0 and thus

(cid:13)
(cid:13)
(cid:13)2

Combining the inequalities so far provides

1
t

(cid:13)
(cid:13) (cid:98)X T η
(cid:13)

(cid:13)
(cid:13)
(cid:13)∞

≤ Cσ

d log(d/δ)
tk0

.

(cid:115)

(cid:115)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

(cid:98)X T (Y − (cid:98)Xw∗) +

(cid:98)Dw∗

≤ C

1
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

d log(d/δ)
tk0

(cid:18)

σ +

d
k0

(cid:19)

(cid:107)w∗(cid:107)1

and hence conclude the constraint of the optimization problem (3) is satisﬁed (at least) by w∗ and thus the optimization
problem is feasible.
Now consider the vector ∆ := (cid:98)w − w∗, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

X T X∆

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t
1
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t
1
t

( (cid:98)X T (cid:98)X − (cid:98)D)∆

+

( (cid:98)X T (cid:98)X − (cid:98)D − X T X)∆

+

( (cid:98)X T (cid:98)X − (cid:98)D)∆
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

X T ( (cid:98)X − X)∆

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

( (cid:98)X − X)T X∆
(cid:13)
(cid:18) 1
(cid:13)
(cid:13)
t
(cid:13)

+

( (cid:98)X − X)T ( (cid:98)X − X) −

(cid:98)D

∆

(cid:19)

1
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

According to Lemma 15 we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

(cid:13)
(cid:13)
X T ( (cid:98)X − X)∆
(cid:13)
(cid:13)∞

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

(cid:13)
(cid:13)
X T ( (cid:98)X − X)
(cid:13)
(cid:13)∞

(cid:107)∆(cid:107)1 ≤ C

(cid:115)

d log(d/δ)
tk0

((cid:107)w∗(cid:107)1 + (cid:107) (cid:98)w(cid:107)1) ≤ 2C

(cid:115)

d log(d/δ)
tk0

· (cid:107)w∗(cid:107)1

where the last
(cid:13)
(cid:13)
(cid:13)

t ( (cid:98)X − X)T X∆

1

inequality is by the optimality of (cid:98)w.
(cid:13)
(cid:13)
(cid:13)∞

. The last summand can also be bounded by using Lemma 15 and the optimality of (cid:98)w.

The same argument provides an identical bound for

(cid:13)
(cid:18) 1
(cid:13)
(cid:13)
t
(cid:13)

( (cid:98)X − X)T ( (cid:98)X − X) −

(cid:98)D

∆

≤ 2C

(cid:19)

1
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:115)

d3 log(d/δ)
tk0

3

· (cid:107)w∗(cid:107)1

Finally, according to the feasibility of (cid:98)w and w∗ we may bound the ﬁrst summand

(cid:13)
(cid:18) 1
(cid:13)
(cid:13)
t
(cid:13)

(cid:98)X T (cid:98)X −

(cid:98)D

∆

≤ 2C

(cid:19)

1
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

d log(d/δ)
tk0

(cid:18)

σ +

d
k0

(cid:19)

,

(cid:107)w∗(cid:107)1

(cid:115)

and reach the ﬁnal bound.

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

Lemma 15. For any t ≥ t0, with probability at least 1 − δ, the following two inequalities hold

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

( (cid:98)Xt − Xt)T ( (cid:98)Xt − Xt) −

(cid:98)Dt

≤ C

1
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:115)

(cid:115)

d3 log(d/δ)
tk0

3

d log(d/δ)
tk0

,

,

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
t

X T

t ( (cid:98)Xt − Xt)

≤ C

where (cid:107)·(cid:107)∞ denotes the maximum of the absolute values of the entries of a matrix.

Proof. Throughout we use that |xs(i)| ≤ 1 for all i ∈ [d] and all s ∈ [t], and (2) ((cid:98)xs(i) − xs(i))2 − 1
absolute value of at most (d/k0)2 and variance of at most (d/k0)3. For the ﬁrst term, let’s bound

t Dii is unbiased with

For i = j, we have

(cid:20) 1
t

( (cid:98)X − X)T ( (cid:98)X − X) −

(cid:21)

1
t

(cid:98)D

ij

=

1
t

t
(cid:88)

s=1

((cid:98)xs(i) − xs(i))((cid:98)xs(j) − xs(j)) −

(cid:98)Dij

1
t

(cid:34)(cid:18)

E

((cid:98)xs(i) − xs(i))2 −

(cid:19)2(cid:35)

1
t

Dii

≤ E (cid:2)((cid:98)xs(i) − xs(i))4(cid:3) ≤ (d/k0)3

((cid:98)xs(i) − xs(i))2 −

Dii ≤ (d/k0)2, E

1
t

(cid:20)
((cid:98)xs(i) − xs(i))2 −

(cid:21)

Dii

= 0

1
t

Hence, by Bernstein’s inequality, for any v > 0,

Pr

(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
t

t
(cid:88)

s=1

((cid:98)xs(i) − xs(i))2 −

Dii

> v

≤ 2 exp

−

(cid:35)

(cid:18)

v2t
(d/k0)3 + (d/k0)2v/3

(cid:19)

.

It follows that for any δ > 0, with probability at least 1 − δ it holds for all i ∈ [d] that,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
t

t
(cid:88)

s=1

((cid:98)xs(i) − xs(i))2 −

Dii

≤ O

(cid:32)

(cid:115)

log(d/δ)d2
tk0

2

+

log(d/δ)d3
tk0

3

(cid:33)

.

Similarly we have 1

t ( (cid:98)Dii − Dii) ≤ O

(cid:18)

log(d/δ)d2
tk0

2 +

(cid:113) log(d/δ)d3
tk0

3

(cid:19)

.

1
t

1
t

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

For i (cid:54)= j we use an analogous argument, only now the variance term in Bernstein’s inequality is (d/k0)2 rather than
(d/k0)3, hence only reach a tighter bound.

For the second term, we again bound via Bernstein’s inequality as

(cid:21)
X T ( (cid:98)X − X)

(cid:20) 1
t

1
t

t
(cid:88)

s=1

=

ij

xs(i)((cid:98)xs(j) − xs(j)) ≤ O





(cid:115)

d log(d/δ)
tk0

+

d log(d/δ)
tk0





√

The claim now follows by noticing that for large enough t, the dominating terms are those that scale as 1/

t.

Proof of Theorem 2. By Lemma 3,

(cid:107)wt+1 − w∗(cid:107)2 ≤ O





(cid:115)

d
k0

k log(d/δ)
t

(σ +

(cid:107)w∗(cid:107)1)

 .

d
k0



Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

We have

RegretT (w∗) − Regrett0(w∗) =

(yt − (cid:104)xt, wt(cid:105))2 − (yt − (cid:104)xt, w∗(cid:105))2

T
(cid:88)

t=t0+1

T
(cid:88)

t=t0+1

T
(cid:88)

t=t0+1

T
(cid:88)

t=t0+1

=

=

=

((cid:104)xt, w∗ − wt(cid:105) + ηt)2 − η2
t

((cid:104)xt, w∗ − wt(cid:105) + 2ηt) (cid:104)xt, w∗ − wt(cid:105)

2ηt (cid:104)xt, w∗ − wt(cid:105) + ((cid:104)xt, w∗ − wt(cid:105))2 ,

where we used that yt = (cid:104)xt, wt(cid:105) + ηt. To bound the regret we require the upper bound, that occurs with probability of at
least 1 − δ,

∀t ≥ t0

|(cid:104)xt, w∗ − wt(cid:105)|

(i)
≤ (cid:107)xt(cid:107)∞

(cid:113)

(cid:107)wt − w∗(cid:107)0 · (cid:107)wt − w∗(cid:107)2



(cid:115)

(ii)
≤ O

k ·

d
k0

log(log(T )d/δ)
t

(cid:18)

σ +



(cid:19)

 .

d
k0

(cid:112)|S|. Inequality
Inequality (i) holds since (cid:104)a, b(cid:105) ≤ (cid:107)a(S)(cid:107)2 · (cid:107)b(cid:107)2 with S being the support of b and (cid:107)a(S)(cid:107)2 ≤ (cid:107)a(cid:107)∞
(ii) follows from Lemma 3 and Lemma 4, and a union bound over the (cid:100)log(T )(cid:101) many times the vector wt is updated. Now,
for the left summand of the regret bound we have by Martingale concentration inequality that w.p. 1 − δ

T
(cid:88)

t=t0+1

2ηt (cid:104)xt, wt − w∗(cid:105) ≤ O



σ

(cid:118)
(cid:117)
(cid:117)
(cid:116)log(1/δ)

T
(cid:88)

t=t0+1

(cid:104)xt, wt − w∗(cid:105)2







(cid:115)

= O

σ

log(1/δ) log(T )k2 ·

d log(d log(T )/δ)
k0

(cid:18)

σ +

(cid:19)2
 .

d
k0

The right summand is bounded as

T
(cid:88)

t=t0+1

(cid:104)xt, w∗ − wt(cid:105)2 = O

k2 ·

(cid:32)

d log(d log(T )/δ)
k0

(cid:18)

σ +

(cid:19)2

d
k0

(cid:33)

· log(T )

.

Clearly, the right summand dominates the left one.

It remains to bound the regret in ﬁrst t0 rounds. Since wt = 0 for t ≤ t0, we have

Regrett0(w∗) =

2ηt (cid:104)xt, w∗(cid:105) + ((cid:104)xt, w∗(cid:105))2 ≤ O

(cid:16)

σ(cid:112)t0 log(1/δ) + t0

(cid:17)

.

t0(cid:88)

t=1

Here, we used that | (cid:104)xt, w∗(cid:105) | ≤ 1 since (cid:107)xt(cid:107)∞ ≤ 1 and (cid:107)w∗(cid:107)1 ≤ 1. We also used that ηt (cid:104)xt, w∗(cid:105) ∼ N (0, σ2 (cid:104)xt, w∗(cid:105)2)
and η1 (cid:104)x1, w∗(cid:105) , η2 (cid:104)x2, w∗(cid:105) , . . . , ηt0 (cid:104)xt0 , w∗(cid:105) are independent. Thus their sum is a Gaussian with variance at most σ2t0.
Collecting all the terms along with Lemma 16, bounding the difference RegretT − RegretT (w∗), gives
(cid:33)

RegretT ≤

(cid:32)
t0 + (cid:112)t0 log(1/δ) + k2 ·

d log(d log(T )/δ)
k0

(cid:18)

σ +

(cid:19)2

d
k0

· log(T )

(13)

Lemma 16. In the realizable case, w.p. at least 1 − δ we have for any sequence of wt that RegretT − RegretT (w∗) =
O(σ2k log(d/δ)).

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

Proof. It is an easy exercise to show that RegretT − RegretT (w∗) is equal to the regret on an algorithm that always plays
w∗. We thus continue to bound the regret of w∗.
Let ∆ ∈ Rd be the difference between w∗ and ˜w, the empirical optimal solution for the sparse regression problem. The
loss associated with w∗ is clearly (cid:107)η(cid:107)2, where η is the noise term y = Xw∗ + η. The loss associated with ˜w is

(cid:107)X(w∗ + ∆) − Xw∗ − η(cid:107)2 = (cid:107)η − X∆(cid:107)2 = (cid:107)η − X ˜S∆(cid:107)2

where ˜S is the support of ∆, having a cardinality of at most 2k. The closed form solution for the least-squares problem
dictates that

(cid:107)η − X ˜S∆(cid:107)2 ≥ (cid:107)η − X ˜SX †
˜S
Here, A† is the pseudo inverse of a matrix A and XS is the matrix obtained from the columns of X whose indices are in
S. It follows that the regret of w∗ is bounded by

η(cid:107)2 = (cid:107)η(cid:107)2 − (cid:107)X ˜SX †
˜S

η(cid:107)2 .

(cid:107)X ˜SX †
˜S

η(cid:107)2

for some subset ˜S of size at most 2k. To bound this quantity we use a high probability bound for (cid:107)XSX †
set S, and take a union bound over all possible sets of cardinality 2k. For a ﬁxed set S we have that (cid:107)XSX †
distributed according to the χ2

2k distribution. The tail bounds of this distribution suggest that

Sη(cid:107)2 for a ﬁxed
Sη(cid:107)2/σ2 is

(cid:104)
(cid:107)XSX †

Pr

Sη(cid:107)2 > 2kσ2 + 2σ2

2kx + 2σ2x

≤ exp(−x)

√

(cid:105)

meaning that with probability at least 1 − δ/d2k we have

(cid:107)XSX †

Sη(cid:107)2 < 2kσ2 + 2σ2(cid:112)2k · 2k · log(d/δ) + 2σ2 · 2k · log(d/δ) = O(σ2k log(d/δ))

Taking a union bound over all possible subsets of size ≤ 2k we get that w.p. at least 1 − δ the regret of w∗ is at most
O(σ2k log(d/δ)).

B. Proofs for Agnostic Setting
We begin with an auxiliary lemma for Lemma 10, informally proving that for any matrix ¯X with BBRCNP (Deﬁnition 6)
and vector y, the set function

is weakly supermodular. Its proof can be found in (Boutsidis et al., 2015), yet for completeness we provide it here as well.
Lemma 17. [Lemma 5 in (Boutsidis et al., 2015)] Let ¯X be a matrix whose columns have 2-norm at most 1 and y be a
vector with (cid:107)y(cid:107)∞ ≤ 1 of dimension matching the number of rows in X. the set function

is α-weakly supermodular for sparsity k for α = maxS:|S|≤k 1/σmin(XS)2, where XS is the submatrix of X obtained by
choosing the columns indexed by S, and σmin(A) is the smallest singular value of A.

Proof. Firstly, the well known closed form solution for the least-squares problem informs us that

g(S) = inf
w∈RS

(cid:107)y − ¯Xw(cid:107)2

g(S) = inf
w∈RS

(cid:107)y − Xw(cid:107)2

g(S) = inf
w∈RS

(cid:107)y − Xw(cid:107)2,

= yT [I − (X T

S )†X T

S ]y.

We use the notation A† for the pseudoinverse of a matrix A. That is, if the singular value decomposition of A is A =
(cid:80)

i σiuivT

i with σi > 0 then A† = (cid:80)

i σ−1

i viuT
i .

Let us ﬁrst estimate g(S) − g(T ), for sets S ⊂ T . For brevity, deﬁne HS as the projection matrix XSX †
S projecting onto
the column space of XS. Denote by ZT \S the matrix whose columns are those of XT \S projected away from the span of
XS, and normalized. Namely, writing xi as the i’th column of X, ζi = (cid:107)(I − HS)xi(cid:107), zi = (I − HS)xi/ζi, and ZT \S’s

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

columns are {zi}i∈T \S. Notice that the columns of ZT \S and XS are orthogonal, hence according to the Pythagorean
theorem it holds that

g(S) = (cid:107)y(cid:107)2 − (cid:107)HSy(cid:107)2, g(T ) = (cid:107)y(cid:107)2 − (cid:107)HSy(cid:107)2 − (cid:107)ZT \SZ †

T \Sy(cid:107)2

meaning that g(S)−g(T ) = (cid:107)ZT \SZ †
(zT

j y)2, since zj is a unit vector. Let us now decompose g(S) − g(T ).

T \Sy(cid:107)2. In particular, this implies that for any j /∈ S it holds that g(S)−g(S ∪{j}) =

g(S) − g(T ) = (cid:107)ZT \SZ †

T \Sy(cid:107)2 = (cid:107)(Z T

T \S)†Z T

T \Sy(cid:107)2 ≤ (cid:107)(Z T

T \S)†(cid:107)2 · (cid:107)Z T

T \Sy(cid:107)2

The norm used in the last inequality is the matrix operator norm. We now bound both factors of the product on the RHS
T \S(cid:107) ≤ (cid:107)X †
T (cid:107). To see this, consider a vector w ∈ R|T \S|,
separately. For the ﬁrst factor, we claim that (cid:107)(Z T
for convenience denote its entries by {w(i)}i∈T \S, and write zi = (xi − (cid:80)
j∈S αijxj)/ζi. We have

T \S)†(cid:107) = (cid:107)Z †

ZT \Sw =

ziw(i) =

xiw(i)/ζi −

(cid:88)

(cid:88)

w(i)αij/ζi = XT w(cid:48)

(cid:88)

xj

(cid:88)

j∈S

i∈T \S

i∈T \S

i∈T \S

for the vector w(cid:48) ∈ R|T | deﬁned as w(cid:48)(i) = w(i)/ζi for i ∈ T \ S and w(cid:48)(j) = − (cid:80)
i∈T \S w(i)αij/ζi for j ∈ S. Since
ζi ≤ (cid:107)xi(cid:107) ≤ 1 we must have (cid:107)w(cid:48)(cid:107) ≥ (cid:107)w(cid:107). Consider now the unit vector w for which (cid:107)ZT \Sw(cid:107) = (cid:107)Z †
T \S(cid:107)−1, that is, the
unit norm singular vector corresponding to the smallest non-zero singular value of ZT \S. For this w, and its corresponding
vector w(cid:48), we have

(cid:107)Z †

T \S(cid:107)−1 = (cid:107)ZT \Sw(cid:107) = (cid:107)XT w(cid:48)(cid:107) ≥ σmin(XT )(cid:107)w(cid:48)(cid:107) ≥ σmin(XT )(cid:107)w(cid:107) = σmin(XT ).

It follows that

(cid:107)(Z T

T \S)†(cid:107)2 = (cid:107)Z †

T \S(cid:107)2 ≤ 1/σmin(XT )2

We continue to bound the right factor of product.

By combining the inequalities we obtained the required result:

(cid:107)Z T

T \Sy(cid:107)2 =

(cid:88)

(zT

i y)2 =

(cid:88)

g(S) − g(S ∪ {i}).

i∈T \S

i∈T \S

g(S) − g(T ) ≤ (cid:0)1/σmin(XT )2(cid:1) (cid:88)

g(S) − g(S ∪ {i}).

i∈T \S

Proof of Lemma 10. We would like to apply Lemma 17 on the design matrix X. The only catch is that the columns of X
may not be bounded by 1 in norm. To remedy this, let j be the index of the column with the maximum norm and consider
the matrix ¯X = 1
(cid:107)Xj (cid:107) X instead (here, Xj is the j-th column of X; note that Xj = Xej for the j-th standard basis vector
ej). Now, for any subset S of coordinates,

inf
w∈RS

(cid:107)y − ¯Xw(cid:107)2 = inf
w∈RS

(cid:107)y − Xw(cid:107)2.

Thus, we conclude that the set function of interest, g(S) = inf w∈RS (cid:107)y − Xw(cid:107)2, is α-weakly supermodular for sparsity k
for α = maxS:|S|≤k (cid:107) ¯X †
2. For any subset of coordinates S of size at most k, let w be a unit norm right singular vector of
¯XS corresponding to the smallest singular value, so that (cid:107) ¯X †
(cid:107)Xw(cid:48)(cid:107) , where w(cid:48) is the vector
(cid:107) ¯XS w(cid:107) . But
w extended to all coordinates by padding with zeros.

(cid:107) ¯XS w(cid:107) = (cid:107)Xej (cid:107)

S(cid:107)2 = 1

S(cid:107)2

1

Since the restricted condition number of X for sparsity k is bounded by κ we conclude that (cid:107)Xej (cid:107)
holds for any subset S of size at most k, we conclude that α ≤ κ2.

(cid:107)Xw(cid:48)(cid:107) ≤ κ. Since this bound

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

Proof of Lemma 11. By the α-weak supermodularity of g, we have

g(∅) − g(V ) ≤ α ·

[g(∅) − g({j})]

(cid:88)

j∈V

≤ α|V | · [(g(∅) − g(V )) − (g({j∗}) − g(V ))].

Rearranging, we get the claimed bounds.

The following lemma gives a useful property of weakly supermodular functions.
Lemma 18. Let g(·) be a (k, α)-weakly supermodular set function and U be a subset with |U | < k. Then g(cid:48)(S) :=
g(U ∪ S) is (k − |U |, α)-weakly supermodular.

Proof. For any two subsets S ⊆ T with |T | ≤ k − |U |, we have

g(cid:48)(S) − g(cid:48)(T ) = g(U ∪ S) − g(U ∪ T ) ≤ α

[g(U ∪ S) − g(U ∪ S ∪ {j})]

≤ α

[g(U ∪ S) − g(U ∪ S ∪ {j})] = α

[g(cid:48)(S) − g(cid:48)(S ∪ {j})].

(cid:88)

j∈T \S

(cid:88)

j∈(T ∪U )\(S∪U )

(cid:88)

j∈T \S

Proof of Lemma 12. For i ∈ {0, 1, . . . , k1}, deﬁne the set function g(i)
b
First, we analyze the performance of the BEXP algorithms. Fix any i ∈ [k1] and consider BEXP(i). Conceptually, for any
j ∈ [d], the loss of expert j at the end of mini-batch b is gb(V (i−1)
∪ j) (note that this loss is only evaluated for j ∈ U (i)
b
in the algorithm). To bound the regret, we need to bound the magnitude of the losses. Note that for any subset S, we have
0 ≤ gb(S) ≤ 1
y2
t ≤ 1. Thus, the regret guarantee of BEXP (Theorem 8) implies that for any i ∈ [k1] and any
B
j ∈ [d], we have

b (S) = gb(S ∪ V (i)

as g(i)

t∈Tb

(cid:80)

).

b

b



T /B
(cid:88)

E



b=1

gb(V (i−1)
b

∪ {j(i)

b })

 ≤

gb(V (i−1)
b

∪ {j}) + 2

(cid:113) dk1 log(d)T
k0B

.



T /B
(cid:88)

b=1

The expectation above is conditioned on the randomness in V (i−1)
b
∪ {j(i)
the g(i−1) and g(i) functions, and using the fact that V (i−1)

b } = V (i)

b

, we get

, for b ∈ [T /B]. Rewriting the above inequality using



T /B
(cid:88)

E



b=1



b

T /B
(cid:88)

b=1

g(i)
b (∅)

 ≤

g(i−1)
b

({j}) + 2

(cid:113) dk1 log(d)T
k0B

.

(14)

Next, since we assumed that the sequence of feature vectors satisﬁes BBRCNP with parameters ((cid:15), k1 + k), Lemma 10
implies that the set function gb deﬁned in (6) is (k1 + k, κ2)-weakly supermodular for κ = 1+(cid:15)
1−(cid:15) . By Lemma 18, the set
function g(i)
b

is (k, κ2)-weakly supermodular (since |V (i)

| ≤ k1).

b

It is easy to check that the sum of weakly supermodular functions is also weakly supermodular (with the same parameters),
and hence (cid:80)T /B
({j}),
we have, for any subset V of size at most k,

is also (k, κ2)-weakly supermodular. Hence, by Lemma 11, if j∗ = arg minj

b=1 g(i−1)

b=1 g(i−1)

(cid:80)T /B

b

b

g(i−1)
b

({j∗}) − g(i−1)

b

(V ) ≤ (1 − 1

κ2|V | )[

g(i−1)
b

(∅) − g(i−1)

(V )].

b

Since gb(V ) ≥ gb(V ∪ V (i−1)

b

) = g(i−1)
b

(V ), the above inequality implies that

g(i−1)
b

({j∗}) − gb(V ) ≤ (1 − 1

κ2|V | )[

g(i−1)
b

(∅) − gb(V )].

T /B
(cid:88)

b=1

T /B
(cid:88)

b=1

T /B
(cid:88)

b=1

T /B
(cid:88)

b=1

Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear Regression under RIP

Combining this bound with (14) for j = j∗, we get



T /B
(cid:88)

E



b=1

g(i)
b (∅) − gb(V )


 ≤ (1 − 1

κ2|V | )[

g(i−1)
b

(∅) − gb(V )] + 2

(cid:113) dk1 log(d)T
k0B

.

Applying this bound recursively for i ∈ [k1] and simplifying, we get



T /B
(cid:88)

E



b=1

g(k1)
b

(∅) − gb(V )


 ≤ (1 − 1

κ2|V | )k1[

g(0)
b (∅) − gb(V )] + 2κ2|V |

(cid:113) dk1 log(d)T
k0B

.

Using the deﬁnitions of g(k1)

and g(0)

b

b

, and the fact that |V | ≤ k, we get the claimed bound.

T /B
(cid:88)

b=1

T /B
(cid:88)

b=1

