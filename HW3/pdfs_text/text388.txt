How Close Are the Eigenvectors of the Sample and Actual Covariance
Matrices?

Andreas Loukas 1

Abstract
How many samples are sufﬁcient to guarantee
that the eigenvectors of the sample covariance
matrix are close to those of the actual covari-
ance matrix? For a wide family of distributions,
including distributions with ﬁnite second mo-
ment and sub-gaussian distributions supported in
a centered Euclidean ball, we prove that the in-
ner product between eigenvectors of the sam-
ple and actual covariance matrices decreases pro-
portionally to the respective eigenvalue distance
and the number of samples. Our ﬁndings imply
non-asymptotic concentration bounds for eigen-
vectors and eigenvalues and carry strong conse-
quences for the non-asymptotic analysis of PCA
and its applications. For instance, they provide
conditions for separating components estimated
from O(1) samples and show that even few sam-
ples can be sufﬁcient to perform dimensionality
reduction, especially for low-rank covariances.

1 Introduction

The covariance matrix C of an n-dimensional distribution
is an integral part of data analysis, with numerous occur-
rences in machine learning and signal processing.
It is
therefore crucial to understand how close is the sample co-
variance, i.e., the matrix (cid:101)C estimated from a ﬁnite num-
ber of samples m, to the actual covariance matrix. Fol-
lowing developments in the tools for the concentration of
measure, (Vershynin, 2012) showed that a sample size of
m = O(n) is up to iterated logarithmic factors sufﬁcient
for all distributions with ﬁnite fourth moment supported in
n). Similar results
a centered Euclidean ball of radius O(
hold for sub-exponential (Adamczak et al., 2010) and ﬁnite
second moment distributions (Rudelson, 1999).

√

We take an alternative standpoint and ask if we can do

1 ´Ecole Polytechnique F´ed´erale de Lausanne, Switzerland.
Correspondence to: Andreas Loukas <andreas.loukas@epﬂ.ch>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

better when only a subset of the spectrum is of interest.
Concretely, our objective is to characterize how many sam-
ples are sufﬁcient to guarantee that an eigenvector and/or
eigenvalue of the sample and actual covariance matrices
are, respectively, sufﬁciently close. Our approach is moti-
vated by the observation that methods that utilize the co-
variance commonly prioritize the estimation of principal
eigenspaces. For instance, in (local) principal component
analysis we are usually interested in the direction of the
ﬁrst few eigenvectors (Berkmann & Caelli, 1994; Kamb-
hatla & Leen, 1997), where in linear dimensionality reduc-
tion one projects the data to the span of the ﬁrst few eigen-
vectors (Jolliffe, 2002; Frostig et al., 2016).
In the non-
asymptotic regime, an analysis of these methods hinges on
characterizing how close are the principal eigenvectors and
eigenspaces of the sample and actual covariance matrices.

Our ﬁnding is that the “spectral leaking” occurring in the
eigenvector estimation is strongly concentrated along the
eigenvalue axis. In other words, the eigenvector (cid:101)ui of the
sample covariance is far less likely to lie in the span of
an eigenvector uj of the actual covariance when the eigen-
value distance |λi − λj| is large, and the concentration of
the distribution in the direction of uj is small. This agrees
with the intuition that principal components are easier to
estimate, exactly because they are more likely to appear in
the samples of the distribution.

We provide a mathematical argument conﬁrming this phe-
nomenon. Under fairly general conditions, we prove that
(cid:33)

(cid:33)

(cid:32)

(cid:32)

m = O

and m = O

(1)

k2
j
(λi − λj)2

k2
i
λ2
i

samples are asymptotically almost surely (a.a.s.) sufﬁcient
to guarantee that |(cid:104)(cid:101)ui, uj(cid:105)| and |(cid:101)λi −λi|/λi, respectively, is
small for all distributions with ﬁnite second moment. Here,
k2
j is a measure of the concentration of the distribution in
the direction of uj. We also attain a high probability bound
for sub-gaussian distributions supported in a centered Eu-
clidean ball. Interestingly, our results lead to sample esti-
mates for linear dimensionality reduction, and suggest that
linear reduction is feasible even from few samples.

To the best of our knowledge, these are the ﬁrst non-
asymptotic results concerning the eigenvectors of the sam-

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

(a) m = 10

(b) m = 100

(c) m = 500

(d) m = 1000

Figure 1: Inner products (cid:104)(cid:101)ui, uj(cid:105) are localized w.r.t.
n = 784 samples are needed to approximate u1 and u4.

the eigenvalue axis. The phenomenon is shown for MNIST. Much fewer than

ple covariance of non-Normal distributions. Previous stud-
ies have intensively investigated the limiting distribution
of the eigenvalues of a sample covariance matrix (Sil-
verstein & Bai, 1995; Bai, 1999), such as the small-
est and largest eigenvalues (Bai & Yin, 1993) and the
eigenvalue support (Bai & Silverstein, 1998). Eigenvec-
tors and eigenprojections have attracted less attention; the
main research thrust entails using tools from the theory of
large-dimensional matrices to characterize limiting distri-
butions (Anderson, 1963; Girko, 1996; Schott, 1997; Bai
et al., 2007) and it has limited applicability in the non-
asymptotic setting where the sample size m is small and
n cannot be arbitrary large.

Differently, we use techniques from perturbation analysis
and non-asymptotic concentration of measure. However,
in contrast to arguments commonly used to reason about
eigenspaces (Davis & Kahan, 1970; Yu et al., 2015; Huang
et al., 2009; Hunter & Strohmer, 2010), our bounds can
characterize weighted linear combinations of (cid:104)(cid:101)ui, uj(cid:105)2 over
i and j, and do not depend on the minimal eigenvalue
gap separating two eigenspaces but rather on all eigenvalue
differences. The latter renders them useful to many real
datasets, where the eigenvalue gap is not signiﬁcant but the
eigenvalue magnitudes decrease sufﬁciently fast.

We also note two recent works targeting the non-
assymptotic regime of Normal distributions. Shaghaghi
and Vorobyov recently characterized the ﬁrst two moments
of the subspace projection error, a result which implies
sample estimates (Shaghaghi & Vorobyov, 2015), but is
restricted to speciﬁc projectors. A reﬁned concentration
analysis for spectral projectors of Normal distributions was
also presented in (Koltchinskii & Lounici, 2015). Finally,
we remark that there exist alternative estimators for the
spectrum of the covariance with better asymptotic proper-
ties (Ahmed, 1998; Mestre, 2008). Instead, we here focus
on the standard estimates, i.e., the eigenvalues and eigen-
vectors of the sample covariance.

2 Problem Statement and Main Results

Let x ∈ Cn be a sample of a multivariate distribution and
denote by x1, x2, . . . , xm the m independent samples used

to form the sample covariance, deﬁned as

(cid:101)C =

m
(cid:88)

p=1

(xp − ¯x)(xp − ¯x)∗
m

,

(2)

where ¯x is the sample mean. Denote by ui the eigenvector
of C associated with eigenvalue λi, and correspondingly
for the eigenvectors (cid:101)ui and eigenvalues (cid:101)λi of (cid:101)C, such that
λ1 ≥ λ2 ≥ . . . ≥ λn. We ask:

Problem 1. How many samples are sufﬁcient to guarantee
that the inner product |(cid:104)(cid:101)ui, uj(cid:105)| = |(cid:101)u∗
i uj| and the eigen-
value gap |δλi| = |(cid:101)λi − λi| is smaller than some constant
t with probability larger than (cid:15)?

Clearly, when asking that all eigenvectors and eigenvalues
of the sample and actual covariance matrices are close, we
will require at least as many samples as needed to ensure
that (cid:107) (cid:101)C − C(cid:107)2 ≤ t. However, we might do better when
only a subset of the spectrum is of interest. The reason
is that inner products |(cid:104)(cid:101)ui, uj(cid:105)| are strongly concentrated
along the eigenvalue axis. To illustrate this phenomenon,
let us consider the distribution constructed by the n = 784
pixel values of digit ‘1’ in the MNIST database. Figure 1,
compares the eigenvectors uj of the covariance computed
from all 6742 images, to the eigenvectors (cid:101)ui of the sam-
ple covariance matrices (cid:101)C computed from a random sub-
set of m = 10, 100, 500, and 1000 samples. For each
i = 1, 4, 20, 100, we depict at λj the average of |(cid:104)(cid:101)ui, uj(cid:105)|
over 100 sampling draws. We observe that: (i) The magni-
tude of (cid:104)(cid:101)ui, uj(cid:105) is inversely proportional to their eigenvalue
gap |λi − λj|. (ii) Eigenvector (cid:101)uj mostly lies in the span of
eigenvectors uj over which the distribution is concentrated.

We formalize these statements in two steps.

2.1 Perturbation arguments

First, we work in the setting of Hermitian matrices and no-
tice the following inequality:

Theorem 3.2. For Hermitian matrices C and (cid:101)C = δC+C,
with eigenvectors uj and (cid:101)ui respectively, the inequality

|(cid:104)(cid:101)ui, uj(cid:105)| ≤

2 (cid:107)δCuj(cid:107)2
|λi − λj|

,

10-510-310-1101λj00.20.40.60.81|h˜ui,uji|˜u1˜u4˜u20˜u100h˜u1,u1i10-510-310-1101λj00.20.40.60.81|h˜ui,uji|˜u1˜u4˜u20˜u100h˜u4,u4i10-510-310-1101λj00.20.40.60.81|h˜ui,uji|˜u1˜u4˜u20˜u100h˜u20,u20i10-510-310-1101λj00.20.40.60.81|h˜ui,uji|˜u1˜u4˜u20˜u100h˜u100,u100iHow Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

holds for sgn(λi − λj) 2(cid:101)λi > sgn(λi − λj)(λi + λj) and
λi (cid:54)= λj.

The above stands out from standard eigenspace perturba-
tion results, such as the sin(Θ) Theorem (Davis & Ka-
han, 1970) and its variants (Huang et al., 2009; Hunter &
Strohmer, 2010; Yu et al., 2015) for three main reasons:

First, Theorem 3.2 characterizes the angle between any
pair of eigenvectors allowing us to jointly bound any linear
combination of inner-products. Though this often proves
handy (c.f. Section 5), it is infeasible using sin(Θ)-type ar-
guments. Second, classical bounds are not appropriate for
a probabilistic analysis as they feature ratios of dependent
random variables (corresponding to perturbation terms). In
the analysis of spectral clustering, this complication was
dealt with by assuming that |λi − λj| ≤ |(cid:101)λi − λj| (Hunter
& Strohmer, 2010). We weaken this condition at a cost of
In contrast to previous work, we
a multiplicative factor.
also prove that the condition is met a.a.s. Third, previous
bounds are expressed in terms of the minimal eigenvalue
gap between eigenvectors lying in the interior and exte-
rior of the subspace of interest. This is a limiting factor
in practice as it renders the results only amenable to situa-
tions where there is a very large eigenvalue gap separating
the subspaces. The proposed result improves upon this by
considering every eigenvalue difference.

2.2 Concentration of measure

The second part of our analysis focuses on the covariance
and has a statistical ﬂavor. In particular, we give an answer
to Problem 1 for various families of distributions.

In the context of distributions with ﬁnite second moment,
we prove in Section 4.1 that:

Theorem 4.1. For any two eigenvectors (cid:101)ui and uj of the
sample and actual covariance respectively, and for any real
number t > 0:

P(|(cid:104)(cid:101)ui, uj(cid:105)| ≥ t) ≤

1
m

(cid:16)

2kj
t |λi − λj|

(cid:17)2

,

(3)

subject to the same conditions as Theorem 3.2.

For eigenvalues, we have the following corollary:

Corollary 2.1. For any eigenvalues λi and (cid:101)λi of C and (cid:101)C,
respectively, and for any t > 0, we have

(cid:32)

P

|(cid:101)λi − λi|
λi

(cid:33)

≥ t

≤

1
m

(cid:18) ki
λi t

(cid:19)2

.

(cid:3) − λ2
Term kj = (E(cid:2)(cid:107)xx∗uj(cid:107)2
j )1/2 captures the tendency
of the distribution to fall in the span of uj: the smaller the
tail in the direction of uj the less likely we are going to
confuse (cid:101)ui with uj.

2

j = λ2

For normal distributions, we have that k2
j + λjtr(C)
and the number of samples needed for |(cid:104)(cid:101)ui, uj(cid:105)| to be small
i ) when λj = O(1) and m = O(λ−2
is m = O(tr(C)/λ2
)
λj = O(tr(C)−1). Thus for normal distributions,
when
principal components ui and uj with min{λi/λj, λi} =
Ω(tr(C)1/2) can be distinguished given a constant number
of samples. On the other hand, estimating λi with small
relative error requires m = O(tr(C)/λi) samples and can
thus be achieved from very few samples when λi is large1.

i

In Section 4.2, we also give a sharp bound for the family of
distributions supported within a ball (i.e., (cid:107)x(cid:107) ≤ r a.s.).

Theorem 4.2. For sub-gaussian distributions supported
within a centered Euclidean ball of radius r, there exists
an absolute constant c s.t. for any real number t > 0,

P(|(cid:104)(cid:101)ui, uj(cid:105)| ≥ t) ≤ exp

1 −

(cid:32)

(cid:33)

c m Φij(t)2
λj (cid:107)x(cid:107)2
Ψ2

,

(4)

where Φij(t) = |λi−λj | t−2λj
same conditions as Theorem 3.2.

2 (r2/λj −1)1/2 −2 (cid:107)x(cid:107)Ψ2

and subject to the

Above, (cid:107)x(cid:107)Ψ2
is the sub-gaussian norm, for which we usu-
= O(1) (Vershynin, 2010). As such, the
ally have (cid:107)x(cid:107)Ψ2
theorem implies that, whenever λi (cid:29) λj = O(1), the sam-
ple requirement is with high probability m = O(r2/λ2

i ).

These theorems solidify our experimental ﬁndings shown
in Figure 1 and provide a concrete characterization of the
relation between the spectrum of the sample and actual co-
variance matrix as a function of the number of samples,
the eigenvalue gap, and the distribution properties. As ex-
empliﬁed in Section 5 for linear dimensionality reduction,
we believe that our results carry strong implications for the
non-asymptotic analysis of PCA-based methods.

3 Perturbation Arguments

Before focusing on the sample covariance matrix, it helps
to study (cid:104)(cid:101)ui, uj(cid:105) in the setting of Hermitian matrices. The
presentation of the results is split in three parts. Section 3.1
starts by studying some basic properties of inner products
of the form (cid:104)(cid:101)ui, uj(cid:105), for any i and j. The results are used in
Section 3.2 to provide a ﬁrst bound on the angle between
two eigenvectors, and reﬁned in Section 3.3.

3.1 Basic observations

We start by noticing an exact relation between the angle of
a perturbed eigenvector and the actual eigenvectors of C.

Lemma 3.1. For every i and j in 1, 2, . . . , n, the relation
((cid:101)λi − λj) ((cid:101)u∗

i uj) = (cid:80)n

j δCu(cid:96)) holds .

i u(cid:96)) (u∗

(cid:96)=1((cid:101)u∗

1Though the same cannot be stated about the absolute error

|δλi|, that is smaller for small λi.

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

Proof. The proof follows from a modiﬁcation of a standard
argument in perturbation theory. We start from the deﬁni-
tion (cid:101)C (cid:101)ui = (cid:101)λi (cid:101)ui and write

(C + δC) (ui + δui) = (λi + δλi) (ui + δui).

(5)

Expanded, the above expression becomes

Cδui + δCui + δCδui

= λiδui + δλiui + δλiδui,

(6)

where we used the fact that Cui = λiui to eliminate
two terms. To proceed, we substitute δui = (cid:80)n
j=1 βijuj,
where βij = δu∗
i uj, into (6) and multiply from the left by
u∗
j , resulting to:

n
(cid:88)

(cid:96)=1

βiju∗

j Cu(cid:96) + u∗

j δCui +

βiju∗

j δCu(cid:96)

n
(cid:88)

(cid:96)=1

= λi

βiju∗

j u(cid:96) + δλiu∗

j ui + δλi

βiju∗

j u(cid:96) (7)

n
(cid:88)

(cid:96)=1

n
(cid:88)

(cid:96)=1

Proof. We rewrite Lemma 3.1 as

((cid:101)λi − λj)2((cid:101)u∗

i uj)2 =

((cid:101)u∗

i u(cid:96)) (u∗

j δCu(cid:96))

.

(10)

(cid:33)2

We now use the Cauchy-Schwartz inequality

((cid:101)λi − λj)2((cid:101)u∗

i uj)2 ≤

((cid:101)u∗

i u(cid:96))2

(u∗

j δCu(cid:96))2

n
(cid:88)

(cid:96)=1

=

(u∗

j δCu(cid:96))2 = (cid:107)δC uj(cid:107)2
2,

(11)

(cid:32) n
(cid:88)

(cid:96)=1

n
(cid:88)

(cid:96)=1
n
(cid:88)

(cid:96)=1

where in the last step we exploited Lemma 3.2. The proof
concludes by taking a square root at both sides of the in-
equality.

(u∗

Lemma 3.2.

j δCu(cid:96))2 = (cid:107)δC uj(cid:107)2
2.

n
(cid:80)
(cid:96)=1
Proof. We ﬁrst notice that u∗
j δCu(cid:96) is a scalar and equal to
its transpose. Moreover, δC is Hermitian as the difference
of two Hermitian matrices. We therefore have that

Cancelling the unnecessary terms and rearranging, we have

(u∗

j δCu(cid:96))2 =

j δCu(cid:96)u∗
u∗

(cid:96) δCuj

δλiu∗

j ui + (λi + δλi − λj)βij

= u∗

j δCui +

βiju∗

j δCu(cid:96).

(8)

n
(cid:88)

(cid:96)=1

At this point, we note that (λi + δλi − λj) = (cid:101)λi − λj and
i uj − u∗
furthermore that βij = (cid:101)u∗
i uj. With this in place,
equation (8) becomes

δλiu∗

j ui + ((cid:101)λi − λj) ((cid:101)u∗

i uj − u∗

i uj)

= u∗

j δCui +

((cid:101)u∗

i u(cid:96)) u∗

j δCu(cid:96) − u∗

j δCui.

(9)

n
(cid:88)

(cid:96)=1

The proof completes by noticing that, in the left hand side,
all terms other than ((cid:101)λi − λj) (cid:101)u∗
i uj fall-off, either due to
u∗
i uj = 0, when i (cid:54)= j, or because δλi = (cid:101)λi − λj, o.w.

As the expression reveals, (cid:104)(cid:101)ui, uj(cid:105) depends on the orienta-
tion of (cid:101)ui with respect to all other u(cid:96). Moreover, the angles
between eigenvectors depend not only on the minimal gap
between the subspace of interest and its complement space
(as in the sin(Θ) theorem), but on every difference (cid:101)λi − λj.
This is a crucial ingredient to a tight bound, that will be
retained throughout our analysis.

3.2 Bounding arbitrary angles

We proceed to decouple the inner products.

Theorem 3.1. For any Hermitian matrices C and (cid:101)C =
δC + C, with eigenvectors uj and (cid:101)ui respectively, we have
that |(cid:101)λi − λj| |(cid:104)(cid:101)ui, uj(cid:105)| ≤ (cid:107)δC uj(cid:107)2.

= u∗

j δC

(u(cid:96)u∗

(cid:96) )δCuj = u∗

j δCδCuj = (cid:107)δCuj(cid:107)2
2,

n
(cid:88)

(cid:96)=1

n
(cid:88)

(cid:96)=1

n
(cid:88)

(cid:96)=1

matching our claim.

3.3 Reﬁnement

As a last step, we move all perturbation terms to the numer-
ator, at the expense of a multiplicative constant factor.

Theorem 3.2. For Hermitian matrices C and (cid:101)C = δC+C,
with eigenvectors uj and (cid:101)ui respectively, the inequality

|(cid:104)(cid:101)ui, uj(cid:105)| ≤

2 (cid:107)δCuj(cid:107)2
|λi − λj|

,

holds for sgn(λi − λj) 2(cid:101)λi > sgn(λi − λj)(λi + λj) and
λi (cid:54)= λj.

Proof. Adding and subtracting λi from the left side of the
expression in Lemma 3.1 and from deﬁnition we have

n
(cid:88)

(cid:96)=1

(δλi + λi − λj) ((cid:101)u∗

i uj) =

((cid:101)u∗

i u(cid:96)) (u∗

j δCu(cid:96)).

(12)

For λi (cid:54)= λj, the above expression can be re-written as

(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:80)
(cid:96)=1

|(cid:101)u∗

i uj| =

((cid:101)u∗

i u(cid:96)) (u∗

j δCu(cid:96)) − δλi ((cid:101)u∗
|λi − λj|

i uj)

(cid:12)
(cid:12)
(cid:12)
(cid:12)




(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:80)
(cid:96)=1

((cid:101)u∗

i u(cid:96)) (u∗

(cid:12)
(cid:12)
j δCu(cid:96))
(cid:12)
(cid:12)

|λi − λj|

,

|δλi| |(cid:101)u∗
i uj|
|λi − λj|

. (13)






≤ 2 max



How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

Let us examine the right-hand side inequality carefully.
Obviously, when the condition |λi − λj| ≤ 2 |δλi| is not
met, the right clause of (13) is irrelevant. Therefore, for
|δλi| < |λi − λj| /2 the bound simpliﬁes to

2

(cid:12)
n
(cid:12)
(cid:80)
(cid:12)
(cid:12)
(cid:96)=1

j δCu(cid:96))

i u(cid:96)) (u∗

((cid:101)u∗
|λi − λj|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

|(cid:101)u∗

i uj| ≤

(14)

Similar to the proof of Theorem 3.1, applying the Cauchy-
Schwartz inequality we have that
(cid:115) n
(cid:80)
(cid:96)=1

j δCu(cid:96))2

i u(cid:96))2

((cid:101)u∗

(u∗

2

n
(cid:80)
(cid:96)=1
|λi − λj|

|(cid:101)u∗

i uj| ≤

=

2 (cid:107)δCuj(cid:107)2
|λi − λj|

,

(15)

where in the last step we used Lemma 3.2. To ﬁnish the
proof we notice that, due to Theorem 3.2, whenever |λi −
λj| ≤ |(cid:101)λi − λj|, one has

|(cid:101)u∗

i uj| ≤

(cid:107)δC uj(cid:107)2
|(cid:101)λi − λj|

≤

(cid:107)δC uj(cid:107)2
|λi − λj|

<

2 (cid:107)δCuj(cid:107)2
|λi − λj|

.

(16)

Our bound therefore holds for the union of intervals |δλi| <
|λi − λj| /2 and |λi − λj| ≤ |(cid:101)λi − λj|, i.e., for (cid:101)λi > (λi +
λj)/2 when λi > λj and for (cid:101)λi < (λi + λj)/2 when
λi < λj.

4 Concentration of Measure

This section builds on the perturbation results of Section 3
to characterize how far any inner product (cid:104)(cid:101)ui, uj(cid:105) and
eigenvalue (cid:101)λi are from the ideal estimates.

Before proceeding, we remark on some simpliﬁcations em-
ployed in the following. W.l.o.g., we will assume that the
mean E[x] is zero. In addition, we will assume the per-
spective of Theorem 3.2, for which the inequality sgn(λi −
λj) 2(cid:101)λi > sgn(λi−λj)(λi+λj) holds. This event is shown
to occur a.a.s. when the gap and the sample size are sufﬁ-
ciently large, but it is convenient to assume that it happens
almost surely. In fact, removing this assumption is possible
(see Section 4.1.2), but it is largely not pursued here as it
leads to less elegant and sharp estimates.

4.1 Distributions with ﬁnite second moment

Our ﬁrst ﬂavor of results is based on a variant of the
Tchebichef inequality and holds for any distribution with
ﬁnite second moment, though only with moderate proba-
bility estimates.

4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES

Theorem 4.1. For any two eigenvectors (cid:101)ui and uj of the
sample and actual covariance respectively, with λi (cid:54)= λj,
and for any real number t > 0, we have

P(|(cid:104)(cid:101)ui, uj(cid:105)| ≥ t) ≤

1
m

(cid:16)

2 kj
t |λi − λj|

(cid:17)2

for sgn(λi − λj) 2(cid:101)λi > sgn(λi − λj)(λi + λj) and kj =
(cid:0)E(cid:2)(cid:107)xx∗uj(cid:107)2

(cid:3) − λ2

(cid:1)1/2

.

2

j

Proof. According to a variant of Tchebichef’s inequal-
ity (Sarwate, 2013), for any random variable X and for any
real numbers t > 0 and α:

P(|X − α| ≥ t) ≤

Var[X] + (E[X] − α)2
t2

.

(17)

Setting X = (cid:104)(cid:101)ui, uj(cid:105) and α = 0, we have

P(|(cid:104)(cid:101)ui, uj(cid:105)| ≥ t) ≤

Var[(cid:104)(cid:101)ui, uj(cid:105)] + E[(cid:104)(cid:101)ui, uj(cid:105)]2
t2
4 E(cid:2)(cid:107)δCuj(cid:107)2
(cid:3)
t2(λi − λj)2 ,

E(cid:2)(cid:104)(cid:101)ui, uj(cid:105)2(cid:3)
t2

≤

2

=

(18)

where the last inequality follows from Theorem 3.2. We
continue by expanding δC using the deﬁnition of the eigen-
value decomposition and substituting the expectation.

E(cid:2)(cid:107)δCuj(cid:107)2

2

(cid:3) = E

(cid:105)

(cid:104)
(cid:107) (cid:101)Cuj − λjuj(cid:107)2
2
(cid:104)
u∗
j ( (cid:101)C − λj)( (cid:101)C − λj)uj
(cid:104)
u∗
j (cid:101)C 2uj
(cid:104)
j (cid:101)C 2uj
u∗

j − 2λju∗

− λ2
j .

+ λ2

(cid:105)

(cid:105)

(cid:105)

= E

= E

= E

(cid:104)

(cid:105)

j E

(cid:101)C

uj

(19)

In addition,

(cid:104)

E

j (cid:101)C 2uj
u∗

(cid:105)

=

m
(cid:88)

p,q=1

E(cid:2)(xpx∗
p)(xqx∗
m2

q)(cid:3)

uj

u∗
j

(cid:88)

=

u∗
j

E(cid:2)xpx∗

(cid:3)

(cid:3) E(cid:2)xqx∗
p
m2

q

uj +

(cid:3)

E(cid:2)xpx∗
pxpx∗
p
m2

uj

u∗
j

m
(cid:88)

p=1

=

p(cid:54)=q
m(m − 1)
m2
1
m

) λ2

λ2
j +
1
m

j +

= (1 −

and therefore

j E[xx∗xx∗] uj
u∗

1
m
j E[xx∗xx∗] uj
u∗

(20)

1
m

2

(cid:3) = (1 −

E(cid:2)(cid:107)δCuj(cid:107)2
u∗
j E[xx∗xx∗] uj − λ2
j
m

=

) λ2

=

1
m
E(cid:2)(cid:107)xx∗uj(cid:107)2

2

m

j +

j E[xx∗xx∗] uj − λ2
u∗
j

(cid:3) − λ2

j

.

We start with the concentration of inner-products |(cid:104)(cid:101)ui, uj(cid:105)|.

Putting everything together, the claim follows.

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

The following corollary will be very useful when applying
our results.

Corollary 4.1. For any weights wij and real t > 0:



P



(cid:88)

i(cid:54)=j



wij(cid:104)(cid:101)ui, uj(cid:105)2 > t

 ≤

4 wij k2
j
m t (λi − λj)2 ,

(cid:88)

i(cid:54)=j

where kj = (cid:0)E(cid:2)(cid:107)xx∗uj(cid:107)2
and wij (cid:54)= 0 when
2
λi (cid:54)= λj and sgn(λi − λj) 2(cid:101)λi > sgn(λi − λj)(λi + λj).

(cid:3) − λ2

(cid:1)1/2

j

Proof. We proceed as in the proof of Theorem 4.1:



P



(cid:16) (cid:88)

wij(cid:104)(cid:101)ui, uj(cid:105)2(cid:17) 1

2

> t

 ≤

i(cid:54)=j



(cid:104)(cid:80)

i(cid:54)=j wij(cid:104)(cid:101)ui, uj(cid:105)2(cid:105)

E

t2
E(cid:2)(cid:107)δCuj(cid:107)2
2
(λi − λj)2

(cid:3)

≤

4
t2

(cid:88)

i(cid:54)=j

wij

(21)

The claim follows by computing E(cid:2)(cid:107)δCuj(cid:107)2
2
and squaring both terms within the probability.

(cid:3) (as before)

4.1.2 EIGENVALUE CONCENTRATION

Though perhaps less sharp than what is currently known
(e.g., see (Silverstein & Bai, 1995; Bai & Silverstein, 1998)
for the asymptotic setting), it might be interesting to ob-
serve that a slight modiﬁcation of the same argument can
be used to characterize the eigenvalue relative difference,
and as a consequence the main condition of Theorem 4.1.

Corollary 4.2. For any eigenvalues λi and (cid:101)λi of C and (cid:101)C,
respectively, and for any t > 0, we have

(cid:32)

P

|(cid:101)λi − λi|
λi

(cid:33)

≥ t

≤

1
m

(cid:18) ki
λi t

(cid:19)2

,

where ki = (E(cid:2)(cid:107)xx∗ui(cid:107)2
Proof. Directly from the Bauer-Fike theorem (Bauer &
Fike, 1960) one sees that

(cid:3) − λi)1/2.

2

|δλi| ≤ (cid:107) (cid:101)Cui − λiui(cid:107)2 = (cid:107)δCui(cid:107)2.

(22)

The proof is then identical to that of Theorem 4.1.

Using this, we ﬁnd that the event E = {sgn(λi −λj) 2(cid:101)λi >
sgn(λi − λj)(λi + λj)} occurs with probability at least

(cid:18)

P(E) ≥ P

|(cid:101)λi − λi| <

(cid:19)

|λi − λj|
2

> 1 −

2k2
i
m|λi − λj|

.

Therefore, one eliminates the condition from Theo-
rem 4.1’s statement by relaxing the bound to

P(|(cid:104)(cid:101)ui, uj(cid:105)| ≥ t) ≤ P(|(cid:104)(cid:101)ui, uj(cid:105)| ≥ t | E) + (1 − P(E))

<

2
m|λi − λj|

(cid:16)

2k2
j
t2|λi − λj|

(cid:17)
.

+ k2
i

(23)

4.1.3 THE INFLUENCE OF THE DISTRIBUTION
As seen by the straightforward inequality E(cid:2)(cid:107)xx∗uj(cid:107)2
(cid:3) ≤
E(cid:2)(cid:107)x(cid:107)4
(cid:3), kj connects to the kurtosis of the distribution.
However, it also captures the tendency of the distribution
to fall in the span of uj.

2

2

To see this, we will work with the whitened random vec-
tors ε = C +1/2x, where C + denotes the Moore–Penrose
pseudoinverse of C. In particular,
(cid:104)

(cid:105)

k2
j = E

u∗
j C 1/2εε∗Cεε∗C 1/2uj
(cid:104)

(cid:107)Λ1/2U ∗εε∗uj(cid:107)2
2

(cid:105)

− λj)

− λ2
j

λ(cid:96)E(cid:2)ˆε((cid:96))2 ˆε(j)2(cid:3) − λj

(cid:17)

,

(24)

= λj(E
(cid:16) n
(cid:88)

= λj

(cid:96)=1

where ˆε = U ∗ε. It is therefore easier to untangle the spaces
spanned by (cid:101)ui and uj when the variance of the distribution
along the latter space is small (the expression is trivially
minimized when λj → 0) or when the variance is entirely
contained along that space (the expression is also small
when λi = 0 for all i (cid:54)= j). In addition, it can be seen
that distributions with fast decaying tails allow for better
principal component identiﬁcation (E(cid:2)ˆε(j)4(cid:3) is a measure
of kurtosis over the direction of uj).

For the particular case of a Normal distribution, we provide
a closed-form expression.

Corollary 4.3. For a Normal distribution, we have k2
λj (λj + tr(C)).

j =

Proof. For a centered and normal distribution with iden-
tity covariance, the choice of basis is arbitrary and the vec-
tor ˆε = U ∗ε is also zero mean with identity covariance.
Moreover, for every (cid:96) (cid:54)= j we can write E(cid:2)ˆε((cid:96))2 ˆε(j)2(cid:3) =
E(cid:2)ˆε((cid:96))2(cid:3) E(cid:2)ˆε(j)2(cid:3) = 1. This implies that

E(cid:2)(cid:107)xx∗uj(cid:107)2

2

(cid:3) = λ2

j E(cid:2)ˆε(j)4(cid:3) + λj

n
(cid:88)

λ(cid:96)

= λ2

(cid:96)(cid:54)=j
j (3 − 1) + λjtr(C) = 2λ2
j + λjtr(C)
j = λj (λj + tr(C)).

and, accordingly, k2

(25)

4.2 Distributions supported in a Euclidean ball

Our last result provides a sharper probability estimate for
the family of sub-gaussian distributions supported in a cen-
tered Euclidean ball of radius r, with their Ψ2-norm

(cid:107)x(cid:107)Ψ2

= sup

(cid:107)(cid:104)x, y(cid:105)(cid:107)ψ2

,

y∈S n−1

(26)

where S n−1 is the unit sphere and with the ψ2-norm of a
random variable X deﬁned as

(cid:107)X(cid:107)ψ2

= sup
p≥1

p−1/2E[|X|p]1/p .

(27)

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

Our setting is therefore similar to the one used to study co-
variance estimation (Vershynin, 2012). Due to space con-
straints, we refer the reader to the excellent review arti-
cle (Vershynin, 2010) for an introduction to sub-gaussian
distributions as a tool for non-asymptotic analysis of ran-
dom matrices.

Theorem 4.2. For sub-gaussian distributions supported
within a centered Euclidean ball of radius r, there exists an
absolute constant c, independent of the sample size, such
that for any real number t > 0,

(cid:33)

(cid:33)
.

(cid:32) m
(cid:88)

p=1
(cid:32) m
(cid:88)

p=1

= P

(|ˆεp(j)| a + λj) ≥ 0.5 mt |λi − λj|

= P

|ˆεp(j)| ≥

m (0.5 t |λi − λj| − λj)
a

(31)

By Lemma 4.1 however, the left hand side is a sum of in-
dependent sub-gaussian variables. Since the summands are
not centered, we expand each |ˆεp(j)| = zp + E[|ˆεp(j)|]
in terms of a centered sub-gaussian zp with the same ψ2-
norm. Furthermore, by Jensen’s inequality and Lemma 4.1

P(|(cid:104)(cid:101)ui, uj(cid:105)| ≥ t) ≤ exp

1 −

(cid:32)

(cid:33)

,

c m Φij(t)2
λj (cid:107)x(cid:107)2
Ψ2

(28)

E[|ˆεp(j)|] ≤ E(cid:2)ˆεp(j)2(cid:3)1/2

≤

(cid:107)x(cid:107)Ψ2

.

(32)

2
λj

, λi (cid:54)= λj and

Therefore, if we set Φij(t) = (0.5 |λi−λj | t−λj )

(r2/λj −1)1/2 − 2 (cid:107)x(cid:107)Ψ2

=

≤

=

=

where Φij(t) = |λi−λj | t−2λj
2 (r2/λj −1)1/2 − 2 (cid:107)x(cid:107)Ψ2
sgn(λi − λj) 2(cid:101)λi > sgn(λi − λj)(λi + λj).

Proof. We start
from the simple observation that,
for every upper bound B of
|(cid:104)(cid:101)ui, uj(cid:105)|
the relation
P(|(cid:104)(cid:101)ui, uj(cid:105)| > t) ≤ P(B > t) holds. To proceed there-
fore we will construct a bound with a known tail. As we
saw in Sections 3.3 and 4.1,

|(cid:104)(cid:101)ui, uj(cid:105)| ≤

2 (cid:107)δCuj(cid:107)2
|λi − λj|

(cid:13)
(cid:13)(1/m) (cid:80)m
(cid:13)

2

2 (cid:80)m

p=1

(cid:13)
(cid:13)xpx∗
m |λi − λj|
(cid:113)

(cid:13)
(cid:13)
puj − λjuj)
(cid:13)2

p=1(xpx∗
|λi − λj|
puj − λjuj

(cid:13)
(cid:13)2

2 (cid:80)m

p=1

(u∗

j xp)2(x∗

pxp) − 2λj(u∗

j xp)2 + λ2
j

(cid:113)

2 (cid:80)m

p=1

(u∗

j xp)2((cid:107)xp(cid:107)2
m |λi − λj|

2 − λj) + λ2
j

(29)

Assuming further that (cid:107)x(cid:107)2 ≤ r, and since the numerator
is minimized when (cid:107)xp(cid:107)2
2 approaches λj, we can write for
every sample x = C 1/2ε:

(cid:113)

(u∗

j x)2((cid:107)x(cid:107)2

(cid:113)

(u∗

j x)2(r2 − λj) + λ2
j

2 − λj) + λ2
(cid:113)

j ≤

=

λj(u∗
(cid:113)

j ε)2(r2 − λj) + λ2
j

≤ |u∗

j ε|

λjr2 − λ2

j + λj,

(30)

which is a shifted and scaled version of the random variable
|ˆε(j)| = |u∗

j ε|. Setting a = (λjr2 − λ2

j )1/2, we have

P(|(cid:104)(cid:101)ui, uj(cid:105)| ≥ t) ≤ P

(cid:32) 2 (cid:80)m

p=1(|ˆεp(j)| a + λj)

(cid:33)

≥ t

m |λi − λj|

P(|(cid:104)(cid:101)ui, uj(cid:105)| ≥ t) ≤ P

zp ≥

(cid:32) m
(cid:88)

p=1

(cid:33)
.

mΦij(t)
λj

(33)

Moreover, by the rotation invariance principle, the left hand
side of the last inequality is a sub-gaussian with ψ2-norm
(cid:80)m
p=1 (cid:107)zp(cid:107)2
≤
smaller than (c1
ψ2
(c1m/λj)1/2 (cid:107)x(cid:107)Ψ2
, for some absolute constant c1. As a
consequence, there exists an absolute constant c2, such that
for each θ > 0:

)1/2 = (c1m)1/2 (cid:107)z(cid:107)ψ2

P

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m
(cid:88)

p=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:32)

zp

≥ θ

≤ exp

1 −

(cid:33)

.

c2 θ2λj
m (cid:107)x(cid:107)2
Ψ2

(34)

Substituting θ = m Φij(t)/λj, we have

(cid:32)

(cid:32)

(cid:33)

mλ2

c2 m2 Φij(t)2λj
j (cid:107)x(cid:107)2
Ψ2
(cid:33)
c2 m Φij(t)2
λj (cid:107)x(cid:107)2
Ψ2

,

= exp

1 −

(35)

which is the desired bound.

Lemma 4.1. If x is a sub-gaussian random vector and ε =
C +1/2x, then for every i, the random variable ˆε(i) = u∗
i ε
is also sub-gaussian, with (cid:107)ˆε(i)(cid:107)ψ2
Proof. Notice that

≤ (cid:107)x(cid:107)Ψ2

√
/

λi.

(cid:107)x(cid:107)Ψ2

= sup

(cid:107)(cid:104)x, y(cid:105)(cid:107)ψ2

= sup

y∈S n−1

y∈S n−1

λ1/2
j

(u∗

j y)(u∗

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

j=1

(cid:13)
(cid:13)
(cid:13)
j ε)
(cid:13)
(cid:13)
(cid:13)ψ2

≥

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

j=1

λ1/2
j

(u∗

(cid:13)
(cid:13)
(cid:13)
j ui)ˆε(j)
(cid:13)
(cid:13)
(cid:13)ψ2

= λ1/2
i

(cid:107)ˆε(i)(cid:107)ψ2

,

(36)

where, for the last inequality, we set y = ui.

m |λi − λj|

P(|(cid:104)(cid:101)ui, uj(cid:105)| ≥ t) ≤ exp

1 −

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

5 Application to Dimensionality Reduction

To emphasize the utility of our results, in the following we
consider the practical example of linear dimensionality re-
duction. We show that a direct application of our bounds
leads to upper estimates on the sample requirement.

In terms of mean squared error, the optimal way to reduce
the dimension of a sample x of a distribution is by project-
ing it over the subspace of the covariance with maximum
variance. Denote by Ik the diagonal matrix with the ﬁrst
k diagonal entries equal to one and the rest zero. When
the actual covariance is known, the expected energy loss
induced by the Pkx = IkU ∗x projection is
E(cid:2)(cid:107)x(cid:107)2
(cid:80)

(cid:3)

2 − (cid:107)Pkx(cid:107)2
2
E[(cid:107)x(cid:107)2
2]

=

i>k λi
tr(C)

.

loss(Pk) =

(37)

However, when the projector (cid:101)Pk = Ik (cid:101)U ∗ is constructed
from the sample covariance, we have

loss( (cid:101)Pk) =

(cid:104)
(cid:107)x(cid:107)2

E

(cid:105)

2 − (cid:107) (cid:101)Pkx(cid:107)2
2
E[(cid:107)x(cid:107)2
2]

=

=

(cid:80)n

(cid:80)n

i=1 λi − tr(Ik (cid:101)U ∗U ΛU ∗ (cid:101)U )
tr(C)
i≤k,j((cid:101)u∗
tr(C)

i=1 λi −(cid:80)

i uj)2λj

(38)

with the expectation taken over the to-be-projected vectors
x, but not the samples used to estimate the covariance. Af-
ter slight manipulation, one ﬁnds that

loss( (cid:101)Pk) = loss(Pk) +

(cid:80)
i≤k,j(cid:54)=i

((cid:101)u∗

i uj)2(λi − λj)

tr(C)

.

(39)

The loss difference has an intuitive interpretation: when re-
ducing the dimension with (cid:101)Pk one looses either by discard-
ing useful energy (terms j > k), or by displacing kept com-
ponents within the permissible eigenspace (terms j ≤ k).
Note also that all terms with j < i are negative and can
be excluded from the sum if we are satisﬁed we an upper
estimate2.

It is an implication of (39) and Corollary 4.1 that, when its
conditions hold, for any distribution and t > 0

(cid:18)

P

loss( (cid:101)Pk) > loss(Pk) +

(cid:19)

≤

t
tr(C)

(cid:88)

i≤k
j>i

4 k2
j
mt |λi − λj|

.

Observe that the loss difference becomes particularly small
whenever k is small: (i) the terms in the sum are fewer and
(ii) the magnitude of each term decreases (due to |λi −λj|).

2A similar approach could also be utilized to derive a lower

bound of the quantity loss( (cid:101)Pk) − loss(Pk).

Figure 2: The ﬁgure depicts for each k, the sample size needed
such that the loss difference loss( (cid:101)Pk)−loss(Pk) becomes smaller
than some tolerance. We can observe that, in MNIST, linear di-
mensionality reduction works with fewer than n = 725 samples
when the size k of the reduced dimension is small.

This phenomenon is also numerically veriﬁed in Figure 2
for the distribution of the images featuring digit ‘3’ in
MNIST (total 6131 images with n = 784 pixels each).
The ﬁgure depicts for different k how many samples are
required such that the loss difference is smaller than a tol-
erance threshold, here 0.02, 0.05, and 0.1. Each point in the
ﬁgure corresponds to an average over 10 sampling draws.
The trends featured in these numerical results agree with
our theoretical intuition. Moreover they illustrate that for
modest k the sample requirement is far smaller than n.

It is also interesting to observe that for covariance matri-
ces that are (approximately) low-rank, we obtain estimates
reminiscent of compressed sensing (Cand`es et al., 2011),
in the sense that the sample requirement becomes a func-
tion of the non-zero eigenvalues. Though intuitive, with the
exception of (Koltchinskii et al., 2016), this dependency of
the estimation accuracy on the rank was not transparent in
known results for covariance estimation (Rudelson, 1999;
Adamczak et al., 2010; Vershynin, 2012).

6 Conclusions

The main contribution of this paper was the derivation
of non-asymptotic bounds for the concentration of inner-
products |(cid:104)(cid:101)ui, uj(cid:105)| involving eigenvectors of the sample
and actual covariance matrices. We also showed how these
results can be extended to reason about eigenvalues and we
applied them to the non-asymptotic analysis of linear di-
mensionality reduction.

We have identiﬁed two interesting directions for further re-
search. The ﬁrst has to do with obtaining tighter estimates.
Especially with regards to our perturbation arguments, we
believe that our current bounds on inner products could be
sharpened by at least a constant multiplicative factor. The
second direction involves using our results for the analy-
sis of methods that utilize the eigenvectors of the covari-
ance, such that principal component projection and regres-
sion (Jolliffe, 1982; Frostig et al., 2016).

248163264k20406080100samplerequirement(%ofn)tolerance=0.02tolerance=0.05tolerance=0.1How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

References

Adamczak, Radosław, Litvak, Alexander, Pajor, Alain, and
Tomczak-Jaegermann, Nicole. Quantitative estimates of
the convergence of the empirical covariance matrix in
log-concave ensembles. Journal of the American Math-
ematical Society, 23(2):535–561, 2010.

Ahmed, SE. Large-sample estimation strategies for eigen-
values of a wishart matrix. Metrika, 47(1):35–45, 1998.

Anderson, Theodore Wilbur. Asymptotic theory for prin-
cipal component analysis. The Annals of Mathematical
Statistics, 34(1):122–148, 1963.

Bai, ZD. Methodologies in spectral analysis of large di-
mensional random matrices, a review. Statistica Sinica,
pp. 611–662, 1999.

Bai, ZD and Yin, YQ. Limit of the smallest eigenvalue of a
large dimensional sample covariance matrix. The annals
of Probability, pp. 1275–1294, 1993.

Bai, ZD, Miao, BQ, Pan, GM, et al. On asymptotics of
eigenvectors of large sample covariance matrix. The An-
nals of Probability, 35(4):1532–1572, 2007.

Bai, Zhi-Dong and Silverstein, Jack W. No eigenvalues
outside the support of the limiting spectral distribution of
large-dimensional sample covariance matrices. Annals
of probability, pp. 316–345, 1998.

Bauer, Friedrich L and Fike, Charles T. Norms and exclu-
sion theorems. Numerische Mathematik, 2(1):137–141,
1960.

Berkmann, Jens and Caelli, Terry. Computation of surface
geometry and segmentation using covariance techniques.
IEEE Transactions on Pattern Analysis and Machine In-
telligence, 16(11):1114–1116, 1994.

Cand`es, Emmanuel J., Li, Xiaodong, Ma, Yi, and Wright,
John. Robust principal component analysis? Journal of
the ACM, 58(3):11:1–11:37, June 2011.

Huang, Ling, Yan, Donghui, Taft, Nina, and Jordan,
Michael I. Spectral clustering with perturbed data. In
Advances in Neural Information Processing Systems, pp.
705–712, 2009.

Hunter, Blake and Strohmer, Thomas.

Performance
analysis of spectral clustering on compressed, incom-
arXiv preprint
plete and inaccurate measurements.
arXiv:1011.0997, 2010.

Jolliffe, Ian. Principal component analysis. Wiley Online

Library, 2002.

Jolliffe, Ian T. A note on the use of principal components
in regression. Applied Statistics, pp. 300–303, 1982.

Kambhatla, Nandakishore and Leen, Todd K. Dimension
reduction by local principal component analysis. Neural
computation, 9(7):1493–1516, 1997.

Koltchinskii, Vladimir and Lounici, Karim. Normal ap-
proximation and concentration of spectral projectors of
sample covariance. arXiv preprint arXiv:1504.07333,
2015.

Koltchinskii, Vladimir, Lounici, Karim, et al. Asymptotics
and concentration bounds for bilinear forms of spectral
projectors of sample covariance. In Annales de l’Institut
Henri Poincar´e, Probabilit´es et Statistiques, volume 52,
pp. 1976–2013. Institut Henri Poincar´e, 2016.

Mestre, Xavier.

Improved estimation of eigenvalues and
eigenvectors of covariance matrices using their sample
IEEE Transactions on Information Theory,
estimates.
54(11), 2008.

Rudelson, Mark. Random vectors in the isotropic position.
Journal of Functional Analysis, 164(1):60–72, 1999.

Sarwate,

event

Two-sided

Dilip.
for

in-
the
symmetric
equality
mean?
Mathematics Stack Exchange, 2013.
URL:http://math.stackexchange.com/q/144675 (version:
2012-05-13).

chebyshev
around

not

Davis, Chandler and Kahan, William Morton. The rotation
of eigenvectors by a perturbation. III. SIAM Journal on
Numerical Analysis, 7(1):1–46, 1970.

Schott, James R. Asymptotics of eigenprojections of corre-
lation matrices with some applications in principal com-
ponents analysis. Biometrika, pp. 327–337, 1997.

Frostig, Roy, Musco, Cameron, Musco, Christopher, and
Sidford, Aaron. Principal component projection with-
In Proceedings of
out principal component analysis.
The 33rd International Conference on Machine Learn-
ing, pp. 2349–2357, 2016.

Girko, V. Strong law for the eigenvalues and eigenvectors

of empirical covariance matrices. 1996.

Shaghaghi, Mahdi and Vorobyov, Sergiy A. Subspace
leakage analysis of sample data covariance matrix.
In
ICASSP, pp. 3447–3451. IEEE, 2015.

Silverstein, Jack W and Bai, ZD. On the empirical dis-
tribution of eigenvalues of a class of large dimensional
random matrices. Journal of Multivariate analysis, 54
(2):175–192, 1995.

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

Vershynin, Roman.

Introduction to the non-asymptotic

analysis of random matrices. arXiv:1011.3027, 2010.

Vershynin, Roman. How close is the sample covariance
matrix to the actual covariance matrix? Journal of The-
oretical Probability, 25(3):655–686, 2012.

Yu, Yi, Wang, Tengyao, Samworth, Richard J, et al. A use-
ful variant of the davis–kahan theorem for statisticians.
Biometrika, 102(2):315–323, 2015.

