SPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling

Jun-ichiro Hirayama 1 2 Aapo Hyv¨arinen 3 4 Motoaki Kawanabe 2 1

Abstract
We present a novel probabilistic framework for
a hierarchical extension of independent compo-
nent analysis (ICA), with a particular motiva-
tion in neuroscientiﬁc data analysis and model-
ing. The framework incorporates a general sub-
space pooling with linear ICA-like layers stacked
recursively. Unlike related previous models, our
generative model is fully tractable: both the like-
lihood and the posterior estimates of latent vari-
ables can readily be computed with analytically
simple formulae. The model is particularly sim-
ple in the case of complex-valued data since
the pooling can be reduced to taking the modu-
lus of complex numbers. Experiments on elec-
troencephalography (EEG) and natural images
demonstrate the validity of the method.

1. Introduction

Linear component analysis and pooling are two funda-
mental concepts of unsupervised representation or feature
learning on continuous-valued data. The basic method for
linear decomposition is independent component analysis
(ICA) (Hyv¨arinen et al., 2001b) or sparse coding. Pooling
originates from computational models of “complex cells”
in the visual cortex (Hubel & Wiesel, 1962; Adelson &
Bergen, 1985), which typically takes the sum of squares
of components or neuronal outputs (L2-pooling) to achieve
invariances in higher features. The combination of the two
concepts have so far found many applications, including
advanced image recognition by deep neural networks.

In the present study, we focus on applications of great
current interest related to neuroscience/engineering, such

1RIKEN Center for Advanced Intelligence Project (AIP),
Tokyo, Japan 2Advanced Telecommunications Research Insti-
tute International (ATR), Kyoto, Japan 3Department of Com-
puter Science and HIIT, University of Helsinki, Finland 4Gatsby
Computational Neuroscience Unit, University College Lon-
Jun-ichiro Hirayama <jun-
don, UK. Correspondence to:
ichiro.hirayama@riken.jp>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

as electro- or magneto-encephalography (EEG/MEG) sig-
nal analysis and natural image statistics (Hyv¨arinen et al.,
2009). Related previous studies have longly attempted to
combine together linear component analysis and pooling
and further built them up to hierarchical probabilistic mod-
els (Hyv¨arinen & Hoyer, 2000; Hyv¨arinen et al., 2001a;
Valpola et al., 2004; Karklin & Lewicki, 2005; Shan et al.,
2006; Onton & Makeig, 2009; Cadieu & Olshausen, 2012;
Hirayama et al., 2015; Hosoya & Hyv¨arinen, 2015), among
which one of the earliest combination of ICA with pool-
ing was independent subspace analysis (ISA) (Hyv¨arinen
& Hoyer, 2000). A more general energy-based modeling
(EBM) framework (e.g., Osindero et al., 2006; Salakhut-
dinov & Hinton, 2009; K¨oster & Hyv¨arinen, 2010; Ngiam
et al., 2011) has also been popularly used. These devel-
opments were somewhat parallel with the rise of general
unsupervised deep learning techniques (see, e.g., Bengio
et al., 2013, for review), while those neuroscientiﬁc appli-
cations speciﬁcally seek simple explanations and interpre-
tations of data, and even two- or three-layer architectures
have been of practical relevance.

However, related previous models were highly intractable,
and they necessarily resorted to approximative or non-
conventional methods, e.g., for learning and inference.
Such a lack of theoretical transparency, as well as the com-
putational difﬁculties associated, has hindered their exten-
sive applications and further developments. Speciﬁcally,
hierarchical generative models usually need approxima-
tions or numerical methods to evaluate the posterior esti-
mates on latent variables or the likelihood (Bengio et al.,
2013); EBM may avoid approximate posterior computa-
tion, while being still hampered by an intractable parti-
tion function (normalizing constant) to compute the likeli-
hood. Conventional maximum likelihood (ML) estimation
is thus not easily applicable in both types of models. In
practice, simply stacking together ICA/ISA models trained
layerwise, or with ﬁxed lower layers, has often been used
as an alternative (e.g., Shan et al., 2006; Onton & Makeig,
2009; Le et al., 2011; Cadieu & Olshausen, 2012; Hosoya
& Hyv¨arinen, 2015) although its theoretical underpinning
is rather unclear.

Here, we present a simple, fully tractable statistical frame-
work for a hierarchical extension of ICA with an intrin-
sic pooling mechanism. We will refer to the framework as

SPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling

(a) General model

(b) Special case

SPLICE, abbreviating stacked pooling and linear compo-
nents estimation. By fully tractable, we mean that both the
posterior estimates on latent variables and the likelihood
function associated are given by simple (computable) ana-
lytical formula without resorting to any approximations. In
this sense, both general hierarchical generative models and
EBMs have only a limited tractability.

Our SPLICE extends ISA so that the subspaces may be de-
pendent of each other via higher layers’ latent variables;
the layers can in principle be stacked recursively without
violating the full tractability of the model. In the present
study, we speciﬁcally introduce the basic framework and
a practical learning scheme which combines a layerwise
ICA pretraining with an unsupervised ﬁnetuning of the en-
tire layers by non-approximate ML. As a proof of concept,
we also demonstrate the method with EEG and natural im-
ages, as commonly targeted in related previous studies. The
method turns out to have interesting connections to neural
networks, which we also brieﬂy discuss below.

2. Proposed Method

2.1. First-Layer Model

We begin with formulating the generative model for our
SPLICE. Denote by xt observed data vectors (t =
1, 2, . . . , n), either real- or complex-valued, consisting of
d entries xit. Each of the d entries is given by a linear
combination of the same number of unknown (ﬁrst-layer)
components or sources, collectively denoted as source vec-
tor st. Here, we consider the fundamental case where xt
and st are independently and identically distributed (i.i.d.).
Omitting sample index t for notational simplicity, we write

x = As,

(1)

where the coefﬁcient matrix A, called mixing matrix, is
square and assumed to be invertible; the inverse W :=
A−1 is called demixing matrix. For convenience, we as-
sume without loss of generality that x and s are zero-mean,
by subtracting the sample mean from original data vectors.

Like ISA, we divide the d ﬁrst-layer sources into m groups
without overlapping, and denote by s[j] the vector consist-
ing of the dj sources in the j-th group (d = (cid:80)m
j=1 dj).
Hence s[j] represents a dj-dimensional subspace in the
original data space, spanned by the corresponding columns
in A. Unlike ISA, however, the m source vectors s[j] may
be dependent of each other in our generative model.

2.2. Second-Layer Model

To extend the model to multiple layers by modeling the
dependencies between the subspaces, we introduce an ad-
ditional (second) layer on the top of the above ISA-like ﬁrst
layer model. Note that we don’t count the pooling as a sep-

Figure 1: Generative model of SPLICE: (a) A higher layer
directly gives the squared L2-norms of lower sources s
within each subspace. (b) An important special case having
one complex source s per subspace.

arate layer, so what we call the second layer is called the
third layer in some previous work.

Speciﬁcally, we assume that source vectors s[j] may be de-
pendent of each other in their “powers” or “energies,” as
typically quantiﬁed by their (squared) L2-norms (cid:107)s[j](cid:107)2.
We model the dependency using a linear mixing of addi-
tional (second-layer) sources with pointwise nonlinearities:

(cid:107)s[j](cid:107)2 = F −1

([A(cid:48)s(cid:48)]j),

j

j = 1, 2, . . . , m,

(2)

where a monotonic link function Fj : [0, ∞) → R maps
(nonnegative) squared norms into real values, with its in-
; A(cid:48) and s(cid:48) are invertible mixing ma-
verse denoted by F −1
trix and source vector (and W(cid:48) := A(cid:48)−1 is demixing ma-
trix) of second layer, and [·]j denotes j-th entry of a vector.
For later convenience, we denote by x(cid:48) := A(cid:48)s(cid:48) (with en-
tries x(cid:48)

j) the “observed” data vector for the second layer.

j

To fully specify the generative model, in the present study,
we simply put a sphericality assumption on every s[j]; i.e.,
we assume that the corresponding normalized vector

u[j] := s[j]/(cid:107)s[j](cid:107),

(3)

for every j, is uniformly distributed on the unit hyper-
sphere, independently of any other random variables.

2.3. Third Layer and Beyond, and a Special Case

An intriguing fact with our model is that it can in princi-
ple be extended with any number of layers (Fig. 1(a)), up
to the limit of subspace partitioning. This can be done by
recursively stacking a higher layer (2) to generate the lower
layer’s subspace norms, with the lower layer appropriately
partitioned into subspaces. Note that in the second and fur-
ther layers, complex-valued variables may not be useful at
least in our current i.i.d. setting. At the top layer, we sim-
ply assume that the sources are mutually independent and

SPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling

non-Gaussian; throughout our experiments in Section 4, we
used a typical prior, p(s) = (1/2)sech((π/2)s), which cor-
responds to the conventional tanh nonlinearity in ICA.

scale parameter λj can be ﬁxed in the same manner as
above. Figure 2 illustrates the forms of this type of F and
F −1 for complex sources when dj = 1.

On the other hand, if one’s goal is primarily to give a sim-
ple explanation of data, adding extra layers might over-
complicate the model.
In fact, a simpliﬁed special case
of our model (Fig. 1(b)), having only two layers with
one complex-valued source per subspace (i.e., |sj|2 =
([A(cid:48)s(cid:48)]j)), may already have a high practical relevance
F −1
j
in the context of neuroscientiﬁc data analysis and model-
ing (e.g., Onton & Makeig, 2009; Cadieu & Olshausen,
2012; Hirayama et al., 2015). Then the squares |sj|2 and
arguments arg sj speciﬁcally represent the power (squared
amplitude) and phase of an oscillatory source signal, where
the sphericality assumption reduces to the circularity of sj,
i.e., the phase is uniform, and is independent of the power.

2.4. Choice of Intermediate Nonlinearity Fj

The true forms of Fj in (2) are usually unknown and ideally
they would be learned from the data by either parametric or
nonparametric methods. However, in practice, it is presum-
ably sufﬁcient that they are ﬁxed, as a ﬁrst approximation,
so that the computational costs can be reduced.

Logarithm Conventionally, one typical option is the log-
arithm (e.g., Valpola et al., 2004; Karklin & Lewicki, 2005;
Cadieu & Olshausen, 2012), i.e.,

Fj(q) = ln(λjq),

(4)

where λj is a nonnegative scale parameter. The scale pa-
rameter λj can in fact be arbitrarily chosen, because one
cannot determine the true scales of the sources due to the
inherent scaling ambiguity as in ICA. To avoid this ambi-
guity, we speciﬁcally set λj so that

Fj(1) = 0.

Gaussianization Another popular choice in the literature
is (radial) Gaussianization (Chen & Gopinath, 2001; Shan
et al., 2006; Lyu & Simoncelli, 2009), generally given by

Fj(q) = Φ−1(Ψj(λjq)),

where Φ and Ψj denote the cumulative distribution func-
tion (cdf) of standard Gaussian distribution and that of a
certain distribution over [0, ∞), respectively. Gaussianiza-
tion originally had no generative interpretation but we may
use the principle as an intuitive “adversarial” deﬁnition of
Fj, i.e., any non-Gaussianity in the data comes from the
non-Gaussianity of the second-layer sources s(cid:48)
k; one may
simply set the cdf Ψj(λj(·)) of qj = (cid:107)s[j](cid:107)2 as chi-squared
with an appropriate degrees of freedom, so that s[j] is Gaus-
sian when the second layer x(cid:48)
j is (standard) Gaussian. The

(5)

(6)

Figure 2: Forms of Gaussianization-based nonlinearity F
(left panel) and its inverse F −1 (right) for s[j] ∈ C, dj = 1.

2.5. Properties of the Model

Now we will show that the hierarchical probabilistic model
formulated above is in fact fully tractable. To ease exposi-
tion, we will give the result only for the two-layer case, but
the generalization with more layers is straightforward.

2.5.1. THE PDF IS ANALYTICALLY NORMALIZED

First, we show that the probability density function (pdf) of
observed data vector x, associated with our model, is ana-
lytically normalized, i.e., the density has a unit sum without
any intractable normalizing constant. Thus, the likelihood
of our model can easily be evaluated without approximative
or numerical techniques. The lack of this desirable prop-
erty has long been an obstacle in hierarchical generative or
energy-based modeling combined with pooling.

To derive the pdf, ﬁrst observe that the linear map s (cid:55)→ x
implies that p(x) = ps(Wx)| det W|c, where ps(·) de-
notes the pdf of ﬁrst-layer source vector s and c = 1 and
2 for real- and complex cases, respectively (c.f. Adali &
Haykin, 2010, sec. 1). Then, the following theorem explic-
itly relates ps(·) to the second layer’s pdf of (cid:107)s[j](cid:107)2:
Theorem 1. Denote by q the vector having qj := (cid:107)s[j](cid:107)2
in the j-th entry. Assume that 1) q has the pdf given by
pq(q1, . . . , qm) and 2) unit vectors uj := s[j]/(cid:107)s[j](cid:107) are in-
dependent of q and uniformly distributed in (the Cartesian
products of) the corresponding unit hyperspheres. Then,

ps(s) = pq((cid:107)s[1](cid:107)2, . . . , (cid:107)s[m](cid:107)2)

κj((cid:107)s[j](cid:107)2),

(7)

(cid:89)

j

where κj(qj) = q1−dj /2
q1−dj
j

Γ(dj)π−dj (if complex).

j

Γ(dj/2)π−dj /2 (if s is real) or

The proof is given in Supplementary Material A. Theo-
rem 1 is a generalization of the result on single spherically-
symmetric random vector (e.g., Ollila et al., 2012).

The formula (7) in fact holds for any probabilistic model
pq(·) of the second layer. Our SPLICE speciﬁcally in-
troduces the model (2) which resembles “post-nonlinear”

q02468x′-8-6-4-202Flogx′-202q02468F−1expSPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling

ICA (Taleb & Jutten, 1999), implying that

pq(q) =

exp(cid:0)−Hk(w(cid:48)

k · F (q))(cid:1)

|fj(qj)|| det W(cid:48)|,

d
(cid:89)

k=1

d
(cid:89)

j=1

k) = − ln p(s(cid:48)

where Hk(s(cid:48)
k) are ﬁxed functions that corre-
spond to any typical choice of non-Gaussian prior such that
p(s(cid:48)) = (cid:81)
k). We denote the entrywise mapping Fj
collectively by F : Rd
+ → Rd and the ﬁrst derivatives of
Fj by fj. We also denote by w(cid:48)
k the k-th transposed row of
W(cid:48) and by a dot operator the standard inner product.

k p(s(cid:48)

Taken together, we eventually obtain the pdf of x, which is
analytically normalized:

p(x) =

exp(cid:0)−Hk(

(cid:88)

kjFj((cid:107)W[j]x(cid:107)2))(cid:1)| det W(cid:48)|
w(cid:48)

j

violating the full analytical tractability, which will be an
interesting open topic for future study. For example, non-
linear activation functions (e.g., leaky rectiﬁed linear unit)
and other types of pooling (e.g., Lp-pooling, by introducing
2 sign(s)) can readily be incorporated at least if the
s (cid:55)→ |s|
extra nonlinearity is bijective by itself and the associated
Jacobian determinant is analytically tractable.

p

2.6. Learning by Maximum Likelihood (SPLICE-ML)

Next we develop the method for parameter estimation
(learning) in our generative model. The analytically sim-
ple form of the pdf (8) makes conventional maximum like-
lihood (ML) estimation readily applicable, which theoret-
ically has a number of desirable properties. To obtain an
ML estimate, we simply minimize the sample average of
the corresponding loss function, L := − ln p(x) + const.,
given for the two-layer case by

exp(−Gj((cid:107)W[j]x(cid:107)2))| det W|c,

(8)

L =

(cid:0)(cid:88)

Hk

kjFj((cid:107)W[j]x(cid:107)2)(cid:1) − ln | det W(cid:48)|
w(cid:48)

d
(cid:89)

k=1

×

d
(cid:89)

j=1

where W[j] consists of only the dj rows in W so that s[j] =
W[j]x, and Gj(q) := − ln |fj(q)| − ln κj(q).

+

Gj

(cid:0)(cid:107)W[j]x(cid:107)2(cid:1) − c ln | det W|.

(10)

m
(cid:88)

k=1
m
(cid:88)

j=1

j

2.5.2. EXACT POSTERIOR ESTIMATE VIA POOLING

Second, we see that our model also allows a simple analyt-
ical estimate on latent variables, which was in fact already
implied by the above development. The inverse process can
readily be obtained for the two linear layers by s = Wx
and s(cid:48) = W(cid:48)x(cid:48), since we assumed that both A and A(cid:48) are
invertible. The remaining part that links the ﬁrst layer s and
the second layer x(cid:48) is also readily given from Eq. (2) as

j = Fj((cid:107)s[j](cid:107)2) = Fj(
x(cid:48)

|si|2),

(9)

(cid:88)

i∈Sj

for every j, where Sj denotes the index set for the sources
belonging to the j-the subspace. Hence the overall transfor-
mation from observed x to the top-level representation s(cid:48) is
given by an analytically very simple form. Although this
consequence is almost obvious from the deﬁnition of our
model, this is still remarkable since previous hierarchical
generative models usually did not possess such a tractabil-
ity which in fact partly led to the invention of EBM.

Note that the relation (9) essentially implements an L2-
pooling operation.
Interestingly, the two demixing lay-
ers, interleaved by the pooling layer, constitute a simpli-
ﬁed multilayer neural network with linear neurons. Thus,
one may also view SPLICE as a principled framework for
unsupervised learning of a multilayer neural network with
pooling layers.

From the neural network or EBM viewpoint, Eq. (10) is in-
teresting since the loss is associated with not only the out-
put (the top) but also the intermediate (lower) layers. Such
a layer-speciﬁc loss has seldom been used in the literature.

In practice, the log-determinant terms in (10) may lead to
a computational difﬁculty due to the costly matrix inver-
sion when evaluating the gradient. Fortunately, the popular
(stochastic) natural gradient method (Amari et al., 1996) is
readily applicable for our model just like ICA, which can
eliminate the need for matrix inversion. In our experiment
(Section 4), however, we actually used the limited-memory
BFGS quasi-Newton method (Schmidt, 2005) with an ex-
plicit matrix inversion, as it converged empirically faster in
our setting (results not shown).

Since the objective function is not convex, the optimiza-
tion needs to start with a good initial estimate not to stack
with poor local optima. We use the following two-step ap-
proach that resembles a typical pretraining-ﬁnetuning strat-
egy in the deep learning literature. That is, we ﬁrst perform
a layerwise learning developed below (SPLICE-LW) and
then optimize the likelihood of the entire layers (SPLICE-
ML) by starting from the layerwise solution. Note that both
steps are unsupervised, in contrast to typical ﬁnetuning
strategies in deep neural networks which are supervised.

2.7. Practical Layerwise Learning (SPLICE-LW)

We remark that the framework of SPLICE can even be ex-
tended with other ingredients of neural networks without

Our unsupervised layerwise learning scheme combines
ICA with an adaptive subspace partitioning for pool-

SPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling

ing. A similar approach has previously been studied for
ISA (Szab´o et al., 2012, see also Hosoya & Hyv¨arinen,
2016). Speciﬁcally, we ﬁrst perform an ICA to estimate the
sources sit up to their permutation, and then solve a simple
optimization problem (see below) to assign the sources into
a preset number of subspaces (except for the special case of
dj ≡ 1), which adaptively partition the data space into sub-
spaces. The input to the upper layer can then be computed
by (9), for which ICA is applied again. For a general num-
ber of layers, the procedure is recursively applied.

The idea of the adaptive subspace partitioning scheme is
that our model implies that the sources’ correlation-in-
squares ωij := corr(|si|2, |sj|2) (i (cid:54)= j) are constant γkl
if si ∈ Sk and sj ∈ Sl because of the L2-sphericality
within each subspace; matrix Ω = (ωij) thus has a block
structure after an appropriate permutation of rows and
columns. This observation leads to a simple objective func-
tion (cid:107)Ω − ZTΓZ(cid:107)2 (with Frobenius norm) to be mini-
mized with respect to Z = (zki) and Γ = (γkl), where
Z ∈ {0, 1}m×d is a subspace assignment matrix such that
zki = 1 if and only if source i belongs to subspace k.

The problem further reduces to an equivalent maximization
T
(cid:107)2 with respect to (cid:101)Z := (ZZT)−1/2Z (Supple-
of (cid:107)(cid:101)ZΩ(cid:101)Z
mentary Material B), where (cid:101)Z is necessarily nonnegative
T
= I. We thus borrow the idea of orthogonal re-
and (cid:101)Z(cid:101)Z
laxation from spectral clustering (Yu & Shi, 2003, see also
Ding et al., 2006), and alternatively solve

max
V

(cid:107)VΩVT(cid:107)2

s.t. VVT = I, V ∈ Rm×d
+ ,

(11)

which previously appeared in a rather different context (Hi-
rayama et al., 2016). Due to the joint orthogonality and
nonnegativity, solution V has at most one nonzero entry in
each column, which readily gives the subspace assignment.
In our experiment below, we used an alternating projected
gradient algorithm to solve (11), which empirically worked
very well for natural images (Section 4.3).

3. Related Work

3.1. Natural Image Statistics and EEG/MEG Analysis

Our primary motivation for the development of a simple
and tractable hierarchical probabilistic model is in data
analysis and modeling related to neuroscience/engineering.
In fact, many preceding studies on natural image statistics
and EEG/MEG data analysis developed hierarchical exten-
sions of ICA, while their intractability has hindered exten-
sive applications or further developments.

A conventional approach was to make the second layer
explain the dependency in variances of ﬁrst-layer sources
(Hyv¨arinen & Hoyer, 2000; Hyv¨arinen et al., 2001a;
Valpola et al., 2004; Karklin & Lewicki, 2005; Zhang

j

k a(cid:48)

((cid:80)

jks(cid:48)

& Hyv¨arinen, 2010; Hirayama et al., 2015), e.g., sj ∼
N(0, F −1
k)) under a conditional Gaussianity as-
sumption. However, this approach usually leads to in-
tractability in learning and inference except for restricted
special cases (Hyv¨arinen & Hoyer, 2000; Hirayama et al.,
2015). Alternatively, two recent studies modeled |sj|2
rather than the variance (Cadieu & Olshausen, 2012; Hi-
rayama & Hyv¨arinen, 2012), but neither of them imple-
mented subspace pooling with a full tractability.

In practice, many previous studies have rather preferred the
layerwise learning strategy, which stacks together ICA/ISA
models trained layerwise, or apply ICA upon ﬁxed lower
layers (Shan et al., 2006; Onton & Makeig, 2009; Cadieu &
Olshausen, 2012; Hosoya & Hyv¨arinen, 2015); the stacked
ISA strategy has also been developed in other application
ﬁeld (Le et al., 2011). Our new development may give a
theoretical basis for the previous layerwise approach and
also provides a principled unsupervised ﬁnetuning method.

3.2. Energy-Based Modeling (EBM)

Another line of research on natural image statistics have
used the energy-based modeling (EBM) strategy (e.g.,
Osindero et al., 2006; Salakhutdinov & Hinton, 2009;
K¨oster & Hyv¨arinen, 2010; Ngiam et al., 2011) instead of
the hierarchical generative approach. However, EBM suf-
fers from computational difﬁculties related to an intractable
partition function, as well as limited interpretability since
there are no independent latent variables.

To compare our model with EBM in Section 4, we intro-
duce an EBM with deterministic hidden units that corre-
sponds to our SPLICE in the two-layer setting. The associ-
ated loss function, i.e., the negative log-pdf up to irrelevant
additive terms, is given by

LEBM =

(cid:88)

Hk

(cid:18)(cid:88)

k

j

w(cid:48)

kjFj

(cid:0)(cid:107)W[j]x(cid:107)2(cid:1)

+ ln Z, (12)

(cid:19)

where Z(W, W(cid:48)) is the partition function to ensure
(cid:82) p(x)dx = 1. We emphasize that partition function Z
in the EBM is intractable while it is simple and tractable
in SPLICE. Moreover, the Hk now lacks the connection to
k) of independent sources; hence, s(cid:48)
the prior pdf p(s(cid:48)
k :=
(cid:0)(cid:107)W[j]x(cid:107)2(cid:1) are generally not independent in
(cid:80)
kjFj

j w(cid:48)

EBM, which is a clear distinction from SPLICE.

3.3. Nonlinear ICA and Deep Generative Models

From a more general perspective, another important type
of hierarchical extension of ICA is nonlinear ICA using
multilayer neural networks (Almeida, 2003; Dinh et al.,
2014; Hyv¨arinen & Morioka, 2017b). The difference from
SPLICE (or other related models) is that the theory of
nonlinear ICA basically assumes a bijectivity between ob-

SPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling

servations and (nonlinear) independent components. Fur-
thermore, the general nonlinear ICA model is not identiﬁ-
able (Hyv¨arinen & Pajunen, 1999) (for an alternative ap-
proach, see Hyv¨arinen & Morioka, 2017b;a). In contrast,
simulations below indicate that our model is identiﬁable,
although we don’t have a formal proof.

In a related context, some authors (Deco & Brauer, 1995;
Dinh et al., 2014) have pointed out and addressed the com-
putational difﬁculty associated with the Jacobian determi-
nant of the multilayer neural network. Fortunately, SPLICE
explicitly decomposes the Jacobian determinant into ana-
lytically tractable terms (Eq. (8)), and the popular natural
gradient technique for ICA can further simplify the com-
putation (Section 2.6).

Recently, several new techniques have been made avail-
able for learning and inference in general-purpose hierar-
chical (deep) generative models on continuous data, such as
variational/autoencoder methods and non-classical learn-
ing principles (e.g., Kingma & Welling, 2014; Kingma
et al., 2014; Goodfellow et al., 2014; Rezende & Mohamed,
2015; Hyv¨arinen & Morioka, 2017b;a). These develop-
ments mainly seek a computational tractability of learning
and inference, maintaining the complexity (representation
capability) of the model. In contrast, our SPLICE rather
reduces the complexity (while keeping the essence) of the
model to achieve the analytical tractability as well as the
interpretability, with a particular emphasis on the tractable
pooling. The motivations, as well as the target applications,
are therefore quite distinct between the two approaches.

4. Experimental Results

In this section, we demonstrate our SPLICE in a simulation
study and with two motivating real datasets.

4.1. Synthetic Data

First, we examined the important special case of SPLICE
(Fig. 1(b)) with a synthetic dataset, and further with a real
EEG dataset (Section 4.2). The goal was to demonstrate the
validity of the basic concept of SPLICE (i.e., combining
pooling and linear layers in a fully tractable manner) as
well as its practical relevance in exploratory signal analysis.

The two-layer model assumes that both observed x and
ﬁrst-layer source vectors s are complex-valued, where the
pooling operation reduces to taking the squared modulus
of each scalar source variable; the adaptive subspace parti-
tioning (Section 2.7) was not necessary in this basic case.
SPLICE-LW consecutively performed real and complex-
valued versions of FastICA (Hyv¨arinen, 1999; Bingham
& Hyv¨arinen, 2000). SPLICE-ML used the SPLICE-LW
solution as the initial estimate. For comparison, we also
trained the EBM (12) by noise-contrastive estimation (Gut-

Figure 3: Synthetic Data: performance index (Amari et al.,
1996) (top; smaller is better) and mean absolute correla-
tions of true and estimated sources (bottom; larger is bet-
ter) in each of the two layers (left: Layer 1; right: Layer 2).
The plot shows the mean and standard deviation of 24 runs.

mann & Hyv¨arinen, 2012) (EBM-NCE) using SPLICE-LW
as the initial estimate. We generated the reference (noise)
dataset for NCE from multivariate Gaussian of the same
mean and covariance as the original, having ten times larger
sample size. All methods used Gaussianization-based Fj
(Fig. 2), speciﬁcally with Ψ(q) = 1 − exp(−λq) (i.e., the
cdf of Exponential distribution) and λ = − ln(1 − Φ(0)).

A simple simulation was performed to compare the basic
performance of the three methods for blind source sep-
aration in each layer (Fig. 3). The dataset consisted of
30-dimensional complex-valued vectors xt which we syn-
thesized by our generative model. We generated the top-
layer sources s(cid:48)
k from t-distribution of the three degrees of
freedom and every entry in A(cid:48) and (the real and complex
parts of) A uniformly in [−1, 1]. For simplicity, we used
the same Gaussianization-based F −1 to generate the data.
As seen in the ﬁgure, both SPLICE-LW and SPLICE-ML
clearly outperformed EBM in Layer 2, which well corre-
sponds to the lack of prior independence in EBM (Sec-
tion 3.2). In Layer 1, all the methods seem to have correctly
recovered the ﬁrst-layer sources, as indicated by the high
mean absolute correlations, while the two SPLICE meth-
ods exhibited improvements in accuracy particularly with
smaller sample sizes. The consistent improvements (or
non-degradation) by SPLICE-ML over SPLICE-LW also

Sample Size103104105Performance Index-100102030405060Layer 1EBM-NCESPLICE-LWSPLICE-MLSample Size103104105Performance Index0100200300400500Layer 2Sample Size103104105Mean Absolute Correlation0.950.960.970.980.9911.01Layer 1Sample Size103104105Mean Absolute Correlation0.30.40.50.60.70.8Layer 2SPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling

demonstrated the effect of unsupervised ﬁnetuning.

4.2. EEG Data

To further demonstrate the applicability to exploratory
analysis of neuroimaging signals, we applied the same
methods to a publicly available EEG datasets: Datasets
1 (Blankertz et al., 2007) from the BCI competition IV
(http://www.bbci.de/competition/iv/). The data were mea-
sured in four human subjects during a number of tri-
als of a two-class cued motor imagery task; see Supple-
mentary Material C for the details of data preprocessing.
We eventually obtained the complex-valued data vectors
xt ∈ C1845 by concatenating 41 sensor channels’ com-
plex time-frequency spectra (45 points within 8-30Hz; typ-
ical α and β bands) at every time points indexed by t =
1, 2, . . . , 4000. As a preprocessing, PCA reduced the di-
mensionality with the 99% of total variance kept.

The idea for the analysis was that the amplitude |sj| of os-
cillatory EEG sources might couple together to represent
higher-order information, in particular, that associated with
the ongoing task states (i.e., imagery of two different mo-
tor modalities like left and right hands). To verify this, we
evaluated individual second-layer sources s(cid:48)
k, obtained in
the unsupervised manner by each of the three methods, in
terms of their relevance to discriminating the task states.

Speciﬁcally, we calculated AUC (area under the ROC
curve) as the relevance measure, by regarding the within-
trial average of every s(cid:48)
k as a single discriminant score
(Fig. 4). We also evaluated the similar score on another
dataset (provided originally for the evaluation purpose in
the competition) by transferring the same model with-
out modiﬁcation (Supplementary Material C). For both
datasets, the increase of the fraction of high-AUC com-
ponents s(cid:48)
k by the two SPLICE methods is evident by the
heavier upper tails as well as the Q-Q plots above the
straight lines; the effect of ﬁnetuning was unclear in this
result. The result implies that SPLICE may effectively
discover task-related functional couplings of source ampli-
tudes, which will be practically useful to enhance further
explorations of data or help consolidating new hypotheses.

4.3. Natural Images

Finally, we demonstrate the validity of our general SPLICE
model (Fig. 1(a)), using (real-valued) natural images ob-
tained from ImageNet10K (Deng et al., 2010). We fol-
lowed (Hosoya & Hyv¨arinen, 2015) for basic preprocess-
ing. Image patches were of 32 × 32 pixels, with the pixel
values in each patch normalized to have zero mean and unit
variance. The dimensionality was then reduced to d = 200
by PCA. The logarithmic nonlinearity F was commonly
used in both SPLICE and EBM.

Figure 4: EEG Data: relevance of individual second-layer
components s(cid:48)
k to the task state. Left: distributions of AUC
scores by the three methods (‘×’: median, ‘+’: 1st & 3rd
quartiles, ‘(cid:5)’: 90-percentile) for original and transfer data.
Right: quantile-quantile (Q-Q) plots between SPLICE-LW
and EBM-NCE (yellow line connects 1st and 3rd quartiles,
extended by red dashed line; ‘(cid:5)’: 90-percentile).

We ﬁrst quantitatively compared two-layer SPLICE and
EBM (as well as a single-layer ICA) by their test log-
likelihood evaluated with 10-fold cross-validation (CV),
for two different sample sizes n (Table 1). The number
of second-layer sources was commonly set as m = 50.
SPLICE-LW initialized the model parameters and subspace
partitioning in both SPLICE-ML and EBM-NCE. The like-
lihood of EBM was numerically evaluated with hybrid an-
nealed importance sampling (AIS) (Ngiam et al., 2011;
Sohl-Dickstein & Culpepper, 2012) 1. For a reference, we
also give another result of SPLICE when its partition func-
tion (= 1) was evaluated numerically by AIS. The table
indicates a superior performance of SPLICE-ML, followed
by SPLICE-LW, as compared to ICA or EBM-NCE. The
AIS was very accurate in this result. The low performance
of EBM could be attributed to either the model difference
or the relative inefﬁciency (in sample size) of NCE as com-
pared to the (quasi-)ML estimators of SPLICE-ML/LW.

The two-layer SPLICE model can be considered a tractable
and generative counterpart to existing models in natural im-
age statistics (e.g., Gutmann & Hyv¨arinen, 2013; Hosoya &
Hyv¨arinen, 2015). Note that our second layer actually cor-
responds to their third layer as we do not count the inter-
mediate pooling layer. In fact, our model qualitatively re-
produced local spatial pooling of ﬁrst-layer linear features
(Fig. 5(a)) as well as various types of excitatory/inhibitory

1We obtained the Matlab code from https://github.com/Sohl-
Dickstein/Hamiltonian-Annealed-Importance-Sampling. We set
the number of intermediate distributions as 107.

EBM-NCESPLICE-LWSPLICE-ML0.50.60.7AUCOriginalEBM-NCESPLICE-LWSPLICE-ML0.60.7Transfer0.60.70.60.7SPLICE-LWQ-Q PlotOriginal0.6EBM-NCE0.6SPLICE-LWTransferSPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling

(a) First Layer

(b) Second Layer

(c) Effect of Increasing Layers

Figure 5: Natural Images: visualizations and the effect of increasing layers. (a) Examples of ﬁrst layer’s feature pooling
learned by two-layer SPLICE-ML (n = 200, 000; one pool per row, randomly selected out of 50). In each row, the left-most
panel superimposes oriented bars that “iconify” (Gutmann & Hyv¨arinen, 2012) the pooled Gabor-like feature detectors w
illustrated on its right. (b) All the 50 second-layer feature detectors w(cid:48)
k. Each panel visualizes a weighted superposition
(using weights w(cid:48)
kj, separately within positive and negative signs) of the 50 binarized iconiﬁed images of pooled ﬁrst-layer
features (i.e., the left-most panels in (a)). Red and blue corresponds to positive and negative signs and their thickness to
absolute values (normalized in each sign to span the color ranges). The global sign of each w(cid:48)
k (originally indeﬁnite) was
chosen so that its maximum absolute entry was positive. (c) Test average log-likelihood computed for both SPLICE-LW
and ML with the numbers of layers 2, 3 and 4. The mean and standard deviations by 10-fold cross validation are shown.

Table 1: Natural Images: comparison with ICA and EBM.
Test average log-likelihood evaluated by 10-fold cross val-
idation (mean±SD) for different sample sizes n.

of subspaces) or limited statistical regularities that can be
present in small image patches.

Test Avg. Log-Likelihood (10-fold CV)

5. Conclusion

Method
ICA

n = 100, 000
- 270.09 ± 0.96
SPLICE-LW - 222.21 ± 1.04
- 200.77 ± 6.53
SPLICE-ML
SPLICE-ML∗
-200.76 ± 6.54
EBM-NCE∗
-342.12 ± 17.36
(∗: with a numerically estimated partition function)

n = 200, 000
- 271.63 ± 1.40
-221.18 ± 0.86
-199.67 ± 6.46
-199.68 ± 6.47
-330.04 ± 20.02

patterns on pooled ﬁrst-layer features (Fig. 5(b)) which the-
oretically models the properties of cortical neurons in the
visual area V2 (Hosoya & Hyv¨arinen, 2015).

Finally, we examined the effect of increasing the number of
layers (Fig. 5(c), n = 200, 000). The three-layer SPLICE
model included an additional pooling layer reducing from
50 to 10 inputs to the third-layer ICA; the four-layer model
further added the pooling and ICA layers which reduces
the dimensionality from 10 to 2. A remarkable increase
of test likelihood by adding the layers is seen in SPLICE-
LW; a weak increase was seen but not completely evident
in SPLICE-ML. We therefore conclude that adding layers
was effective to compensate errors in the pretrained lower
layers, while only two layers (but not one layer; see ICA
in Table 1) may be almost sufﬁcient to represent the data
when combined with ﬁnetuning. This result could be ac-
counted for by either misspeciﬁcation of third- and fourth-
layer models (e.g., the type of nonlinearity F or the number

We introduced SPLICE, a novel hierarchical extension of
ICA with an intrinsic pooling mechanism. The striking fea-
ture of SPLICE is that the model is fully tractable, i.e., both
the posterior estimates on latent variables and the associ-
ated pdf or likelihood can be computed with simple ana-
lytical formulae. The conceptual simplicity of SPLICE, as
well as its approximation-free nature, will be particularly
useful in exploratory data analysis and modeling, for ex-
ample, in neuroscientiﬁc contexts. As a proof-of-concept,
we demonstrated the applicability of the method with EEG
and natural image patches.

So far, the development of hierarchical or deep generative
models has been hampered by intractability and the ensu-
ing computational difﬁculties. Intriguingly, SPLICE relies
only on conventional principles for statistical modeling and
estimation, and it can easily be extended with an arbitrary
number of layers as well as other typical ingredients of
multilayer neural networks. We hope our developments
will open up new avenues for applications of hierarchical
probabilistic models in unsupervised representation learn-
ing of continuous-valued data.

Acknowledgments

This work was partially supported by a contract with
the National Institute of Information and Communica-

234Number of Layers-220-210-200Test Avg. Log-LikelihoodSPLICE-MLSPLICE-LWSPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling

tions Technology entitled, Development of network dy-
namics modeling methods for human brain data simulation
systems, Strategic International Collaborative Research
Program (SICORP) from Japan Science and Technology
Agency (JST), and KAKENHI 15H02759 from Japan Soci-
ety for the Promotion of Science (JSPS). A.H. was funded
by the Academy of Finland (Centre-of-Excellence in In-
verse Problems Research).

References

Adali, T. and Haykin, S. Adaptive Signal Processing: Next

Generation Solutions. Wiley-IEEE Press, 2010.

Adelson, E. H. and Bergen, J. R. Spatiotemporal energy
models for the perception of motion. Journal of the Op-
tical Society of America A, 2(2):284–299, 1985.

Almeida, L. B. MISEP–linear and nonlinear ICA based
on mutual information. Journal of Machine Learning
Research, 4:1297–1318, 2003.

Amari, S., Cichocki, A., and Yang, H. H. A new learning
In Advances in
algorithm for blind signal separation.
Neural Information Processing Systems 8 (NIPS1995),
pp. 757–763, 1996.

Bengio, Y., Courville, A., and Vincent, P. Representation
learning: A review and new perspectives. IEEE Trans-
actions on Pattern Analysis & Machine Intelligence, 35
(8):1798–1828, 2013.

Bingham, E. and Hyv¨arinen, A. A fast ﬁxed-point algo-
rithm for independent component analysis of complex
valued signals. International Journal of Neural Systems,
10(1):1–8, 2000.

Blankertz, B., Dornhege, G., Krauledat, M., M¨uller, K.-R.,
and Curio, G. The non-invasive Berlin brain-computer
interface: Fast acquisition of effective performance in
untrained subjects. NeuroImage, 37(2):539–550, 2007.

Cadieu, C. F. and Olshausen, B. A. Learning intermediate-
level representations of form and motion from natural
movies. Neural Computation, 24(4):827–866, 2012.

Ding, C., Li, T., Peng, W., and Park, H. Orthogonal
In
nonnegative matrix tri-factorizations for clustering.
Proceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data mining
(KDD2006), pp. 126–135, 2006.

Dinh, L., Krueger, D., and Bengio, Y. NICE: Non-linear
independent components estimation. arXiv:1410.8516,
2014.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y. Generative adversarial nets. In Advances in Neural
Information Processing Systems, volume 27, pp. 2672–
2680, 2014.

Gutmann, M. U. and Hyv¨arinen, A. Noise-contrastive es-
timation of unnormalized statistical models, with appli-
cations to natural image statistics. Journal of Machine
Learning Research, 13:307–361, 2012.

Gutmann, M. U. and Hyv¨arinen, A. A three-layer model
of natural image statistics. Journal of Physiology-Paris,
2013.

Hirayama, J. and Hyv¨arinen, A. Structural equations and
divisive normalization for energy-dependent component
analysis. In Advances in Neural Information Processing
Systems 24 (NIPS2011), pp. 1872–1880, 2012.

Hirayama, J., Ogawa, T., and Hyv¨arinen, A. Unifying blind
separation and clustering for resting-state EEG/MEG
functional connectivity analysis. Neural Compution, 27
(7):1373–1404, 2015.

Hirayama, J., Hyv¨arinen, A., Kiviniemi, V., Kawanabe, M.,
and Yamashita, O. Characterizing variability of modular
brain connectivity with constrained principal component
analysis. PLoS ONE, 11(12):e0168180, 2016.

Hosoya, H. and Hyv¨arinen, A. A hierarchical statistical
model of natural images explains tuning properties in
V2. The Journal of Neuroscience, 35(29):10412–10428,
2015.

Chen, S. S. and Gopinath, R. A. Gaussianization.

In
Advances in Neural Information Processing Systems 13
(NIPS2000), pp. 423–429, 2001.

Hosoya, H. and Hyv¨arinen, A. Learning visual spatial pool-
ing by strong PCA dimension reduction. Neural Compu-
tation, 82:1–16, 2016.

Deco, G. and Brauer, W. Nonlinear higher-order statisti-
cal decorrelation by volume-conserving neural architec-
tures. Neural Networks, 8:525–535, 1995.

Hubel, D. H. and Wiesel, T.N. Receptive ﬁelds, binocular
interaction and functional architecture in the cat’s visual
cortex. Journal of Physiology, 160(1):106–154, 1962.

Deng, J., Berg, A. C., Li, K., and Fei-Fei, Li. What does
classifying more than 10,000 image categories tell us?
In Computer Vision – ECCV2010, pp. 71–84, 2010.

Hyv¨arinen, A. Fast and robust ﬁxed-point algorithms for
independent component analysis. IEEE Transactions on
Neural Networks, 10(3):626–634, 1999.

SPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling

Hyv¨arinen, A. and Hoyer, P. O. Emergence of phase and
shift invariant features by decomposition of natural im-
ages into independent feature subspaces. Neural Com-
putation, 12(7):1705–1720, 2000.

Ngiam, J., Chen, Z., Koh, P., and Ng., A. Y. Learning deep
energy models. In Proceedings of the Twenty-Eighth In-
ternational Conference on Machine Learning (ICML),
pp. 1105–1112, 2011.

Hyv¨arinen, A. and Morioka, H. Nonlinear ICA of tem-
porally dependent stationary sources. In Proc. Artiﬁcial
Intelligence and Statistics (AISTATS2017), 2017a.

Hyv¨arinen, A. and Morioka, H. Unsupervised feature ex-
traction by time-contrastive learning and nonlinear ICA.
In Advances in Neural Information Processing Systems
(NIPS2016), 2017b.

Hyv¨arinen, A. and Pajunen, P. Nonlinear independent com-
ponent analysis: Existence and uniqueness results. Neu-
ral Networks, 12(3):429–439, 1999.

Hyv¨arinen, A., Hoyer, P. O., and Inki, M. Topographic
independent component analysis. Neural Computation,
13(7):1527–1558, 2001a.

Hyv¨arinen, A., Karhunen, J., and Oja, E.

Independent

Component Analysis. John Wiley & Sons, 2001b.

Hyv¨arinen, A., Hurri, J., and Hoyer, P. O. Natural Image
Statistics – A probabilistic approach to early computa-
tional vision. Springer-Verlag, 2009.

Karklin, Y. and Lewicki, M. S. A hierarchical Bayesian
model for learning nonlinear statistical regularities in
nonstationary natural signals. Neural Computation, 17:
397–423, 2005.

Kingma, D. P. and Welling, M. Auto-encoding variational
Bayes. In Proceedings of the International Conference
on Learning Representations (ICLR), 2014.

Kingma, D. P., Mohamed, S., Rezende, D. J., and Welling,
M. Semi-supervised learning with deep generative mod-
els. In Advances in Neural Information Processing Sys-
tems (NIPS), volume 27, pp. 3581–3589, 2014.

K¨oster, U. and Hyv¨arinen, A. A two-layer model of natural
stimuli estimated with score matching. Neural Compu-
tation, 22:2308–2333, 2010.

Le, Q. V., Zou, W. Y., Yeung, S. Y., and Ng, A. Y. Learn-
ing hierarchical invariant spatio-temporal features for ac-
tion recognition with independent subspace analysis. In
Proceedings of the 2011 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR ’11, pp. 3361–
3368, 2011. ISBN 978-1-4577-0394-2.

Ollila, E., Tyler, D. E., Koivunen, V., and Poor, H. V. Com-
plex elliptically symmetric distributions: Survey, new
IEEE Transactions on Signal
results and applications.
Processing, 60(11):5597–5625, 2012.

Onton, J. and Makeig, S. High-frequency broadband mod-
ulations of electroencephalographic spectra. Frontiers in
Neuroscience, 159:99–120, 2009.

Osindero, S., Welling, M., and Hinton, G. E. Topographic
product models applied to natural scene statistics. Neu-
ral Computation, 18:381–414, 2006.

Rezende, D. J. and Mohamed, S. Variational Inference with
Normalizing Flows. In Proceedings of the 32nd Inter-
national Conference on Machine Learning (ICML2015),
volume 37, pp. 1530–1538, 2015.

Salakhutdinov, R. and Hinton, G. Deep Boltzmann ma-
chines. In Proceedings of the 12th International Confer-
ence on Artiﬁcial Intelligence and Statistics (AISTATS),
volume 5 of JMLR: W&CP, pp. 448–455, 2009.

Schmidt, M. minFunc: unconstrained differentiable mul-
tivariate optimization in Matlab. http://www.cs.ubc.ca/
˜schmidtm/Software/minFunc.html, 2005.

Shan, H., Zhang, L., and Cottrell, G. W. Recursive ICA.
In Advances in Neural Information Processing Systems,
volume 19, pp. 1273–1280, 2006.

Sohl-Dickstein, J. and Culpepper, B. J. Hamiltonian an-
nealed importance sampling for partition function esti-
mation. arXiv:1205.1925, 2012.

Szab´o, Z., P´oczos, B., and L¨orincz, A. Separation theorem
for independent subspace analysis and its consequences.
Pattern Recognition, 45(4):1782–1791, 2012.

Taleb, A. and Jutten, C. Source separation in post-nonlinear
IEEE Transactions on signal processing, 47

mixtures.
(10):2807–2820, 1999.

Valpola, H., Harva, M., and Karhunen, J. Hierarchical
models of variance sources. Signal Processing, 84(2):
267–282, 2004.

Yu, S. X. and Shi, J. Multiclass spectral clustering.

In
Proceedings of the Ninth IEEE International Conference
on Computer Vision (ICCV2003), pp. 313–319, 2003.

Lyu, S. and Simoncelli, E. P. Nonlinear extraction of
‘Independent Components’ of natural images using ra-
dial Gaussianization. Neural Computation, 21(6):1485–
1519, 2009.

Zhang, K. and Hyv¨arinen, A. Source separation and higher-
order causal analysis of MEG and EEG. In Proceedings
of the 26th Conference on Uncertainty in Artiﬁcial Intel-
ligence (UAI 2010), pp. 709–716, 2010.

