Contextual Decision Processes with low Bellman rank are PAC-Learnable

Nan Jiang 1 Akshay Krishnamurthy 2 Alekh Agarwal 3 John Langford 3 Robert E. Schapire 3

Abstract

This paper studies systematic exploration for re-
inforcement learning (RL) with rich observations
and function approximation. We introduce con-
textual decision processes (CDPs),
that unify
most prior RL settings. Our ﬁrst contribution is
a complexity measure, the Bellman rank, that we
show enables tractable learning of near-optimal
behavior in CDPs and is naturally small for many
well-studied RL models. Our second contribu-
tion is a new RL algorithm that does system-
atic exploration to learn near-optimal behavior in
CDPs with low Bellman rank. The algorithm re-
quires a number of samples that is polynomial
in all relevant parameters but independent of the
number of unique contexts. Our approach uses
Bellman error minimization with optimistic ex-
ploration and provides new insights into efﬁcient
exploration for RL with function approximation.

1. Introduction

In this paper, we study reinforcement learning (RL) prob-
lems where the agent receives rich sensory observations
from the environment, forms complex contexts from sen-
sorimotor streams, uses function approximation to gener-
alize to unseen contexts, and must perform systematic ex-
ploration to learn efﬁciently. Such problems are at the core
of empirical RL research (e.g., Mnih et al., 2015; Belle-
mare et al., 2016), yet no existing theory provides rigorous
and satisfactory guarantees in a general setting. This situ-
ation motivates an important question: how can we solve
RL problems where exploration is critical and the agent re-
ceives rich observations, in a sample-efﬁcient manner?

To answer the question, we propose a new formulation,
Contextual Decision Processes (CDPs), to capture a large
class of sequential decision-making problems: CDPs gen-

1University of Michigan, Ann Arbor 2University of Mas-
sachusetts, Amherst 3Microsoft Research, New York. Correspon-
dence to: Nan Jiang <nanjiang@umich.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

eralize MDPs where the state forms the context (Ex. 1) and
POMDPs where the history forms the context (Ex. 2), and
can be much more concise than alternative formulations
based on sufﬁcient statistics (e.g., Hutter, 2005). We deﬁne
CDPs in Section 2, and the learning goal is to ﬁnd a near-
optimal policy for a CDP with the help of a value-function
approximator in a sample-efﬁcient manner.1

A structural assumption: When the context space is very
large or inﬁnite, as is common in practice, lower bounds
that are exponential in the problem horizon preclude efﬁ-
cient learning in CDPs, even when simple function approx-
imators are used. However, RL problems arising in applica-
tions are often far more benign than the pathological lower
bound instances, and we identify a structural assumption
capturing this intuition. As our ﬁrst major contribution, we
deﬁne a notion of Bellman factorization (Deﬁnition 5) in
Section 3, and focus on problems with low Bellman rank.

At a high level, Bellman rank is an algebraic dimension
capturing the interplay between the CDP and the value-
function approximator that we show is small for many
previously-studied settings. For example, every MDP with
a tabular value-function has Bellman rank bounded by the
rank of its transition matrix, which is at most the number
of states but can be considerably smaller. For a POMDP
with reactive value-functions, the Bellman rank is at most
the number of hidden states and has no dependence on the
observation space. We provide other instances of low Bell-
man rank including Linear Quadratic Regulators and Pre-
dictive State Representations. Overall, CDPs with a small
Bellman rank yield a uniﬁed framework for a large class of
sequential decision making problems.

A new algorithm: Our second contribution is a new algo-
rithm for episodic RL called OLIVE (Optimism Led Itera-
tive Value-function Elimination), detailed in Section 4.1.
OLIVE iteratively reﬁnes a space of candidate Q-value
functions F. At each iteration, it chooses a value func-
tion f using an optimistic criterion and collects trajectories
from the corresponding greedy policy πf . If πf attains a
high-value, the algorithm terminates and outputs f . Other-

1Throughout the paper, by sample-efﬁcient we mean a number
of trajectories that is polynomial in the problem horizon, number
of actions, Bellman rank (to be introduced), and polylogarithmic
in the number of candidate value-functions.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Model
Bellman rank
PAC Learning

Tabular MDP Low-rank MDP Reactive POMDP Reactive PSR

# states
known

rank
new

# hidden states
extended

PSR rank
new

LQR
# state variables
known3

Table 1. Summary of settings having low Bellman rank, with formal statements in Section 3 (Proposition 1 to 5, from left to right in the
table). The 2nd row gives the parameters that bound the Bellman rank. In the 3rd row, “known” means that sample-efﬁcient algorithms
already exist (e.g., tabular MDPs), “extended” means our results substantially extend previous work (e.g., (Krishnamurthy et al., 2016)
for reactive POMDPs), and “new” means our result gives the ﬁrst sample-efﬁcient algorithm (e.g., MDPs with low-rank transitions).

wise, it eliminates all g ∈ F which violate certain Bellman
equations under trajectories generated by πf and performs
the next iteration with this reﬁned class of functions.

A PAC guarantee: We prove that OLIVE performs
learning in CDPs with a small Bell-
sample-efﬁcient
Concretely, when the Q(cid:63)-
man rank (Section 4.2).
function for the CDP is contained in F, OLIVE re-
quires ˜O(M 2H 3K log(N/δ)/(cid:15)2) trajectories to ﬁnd an (cid:15)-
suboptimal policy,2 where M is the Bellman rank, H is the
length of a trajectory, K is the number of actions, N is the
cardinality of F, and δ is the failure probability.

Importantly, the sample complexity bound has a logarith-
mic dependence on N , enabling powerful function approx-
imation, and no direct dependence on the size of the context
space, which can be very large or inﬁnite. As many exist-
ing models, including the ones highlighted in Table 1, have
low Bellman rank, the result immediately implies sample-
efﬁcient learning in all of these settings.3

The main PAC-guarantee can be extended in several ways,
discussed in Appendix A. Speciﬁcally, OLIVE is robust to
the failure of our assumptions, can adapt to unknown Bell-
man rank, and can handle inﬁnite function classes with
bounded statistical complexity. These extensions demon-
strate that the Bellman rank robustly captures the difﬁculty
of exploration in sequential-decision making problems.

To summarize, this work advances our understanding of RL
with complex observations where long-term planning and
exploration are critical. While OLIVE represents an expo-
nential advance in statistical efﬁciency, its computational
complexity, which is polynomial in N , is intractable for
the powerful function classes of interest. This computa-
tional issue must be addressed before we can empirically
evaluate the effectiveness of the proposed algorithm. We
discuss this and other future directions in Section 6.

Related work. There is rich theoretical literature on RL
in tabular settings, including MDPs (Kearns & Singh, 2002;
Brafman & Tennenholtz, 2003; Strehl et al., 2006) and
POMDPs (Azizzadenesheli et al., 2016) with small state

2A logarithmic dependence on a norm parameter ζ is omitted

here, as ζ is polynomial in most cases.

3Our algorithm requires discrete action spaces and does not

immediately apply to LQRs; see more discussion in Section 3.

and observation spaces, with an emphasis on sophisti-
cated exploration to ﬁnd near-optimal policies in a sample-
efﬁcient manner. While there have been extensions to large
state spaces (Kakade et al., 2003; Jong & Stone, 2007;
Pazis & Parr, 2016), these approaches fail to be a good
ﬁt for practical scenarios where the environment is typi-
cally perceived through complex observations such as im-
age, text, or audio signals. Alternatively, Monte Carlo Tree
Search (MCTS) methods can handle large state spaces, but
only at the cost of exponential dependence on the planning
horizon (Kearns et al., 2002; Kocsis & Szepesv´ari, 2006).

Closest to our work are the results of Wen & Van Roy
(2013) and Krishnamurthy et al. (2016), which also obtain
sample complexity independent of the number of unique
contexts, but only under deterministic dynamics and other
special structures. In contrast, we study a much broader
class of problems with relatively mild conditions.

On the empirical side, recent successes on both the Atari
platform (Mnih et al., 2015; Wang et al., 2015) and Go (Sil-
ver et al., 2016) have sparked a ﬂurry of research interest.
These approaches leverage advances in deep learning for
powerful function approximation, but typically use simple
strategies, such as (cid:15)-greedy, for exploration. Better explo-
ration strategies, such as pseudo-counts in Bellemare et al.
(2016), and combining MCTS with function approximation
(e.g., Silver et al. (2016)), typically require strong domain
knowledge and large amounts of data to be successful.

Hallak et al. (2015) have proposed Contextual MDPs,
where each context parametrizes an MDP. In contrast, our
use of contexts is in analogy with contextual bandits (Lang-
ford & Zhang, 2008), and is similar to state features in RL.

2. Contextual Decision Processes (CDPs)

In this section, we introduce a new model, called a Con-
textual Decision Process, as a uniﬁed framework for rein-
forcement learning with rich observations.

2.1. Model and Examples

CDPs make minimal assumptions to capture a general class
of RL problems and are deﬁned as follows.

Deﬁnition 1 (Contextual Decision Process (CDP)). A

Contextual Decision Processes with low Bellman rank are PAC-Learnable

(ﬁnite-horizon) CDP is deﬁned as a tuple (X , A, H, P ),
where X is the context space, A is the action space, and
H is the horizon of the problem. P = (P∅, P+) is the sys-
tem descriptor, where P∅ ∈ ∆(X ) is a distribution over
initial contexts, that is x1 ∼ P∅, and P+ : (X × A ×
R)∗ × X × A → ∆(R × X ) elicits the next reward and
context from the interactions so far x1, a1, r1, . . . , xh, ah:

(rh, xh+1) ∼ P+(x1, a1, r1, . . . , xh, ah).

In a CDP, the agent interacts with the environment in
episodes.
In an episode, the agent observes a context
x1, takes action a1, receives reward r1 and observes x2,
repeating H times. A policy π : X → A spec-
ah =
iﬁes the agent’s decision-making strategy,
π(xh), ∀h ∈ [H], and induces a distribution over trajecto-
ries (x1, a1, r1, . . . , xH , aH , rH , xH+1) via the system de-
scriptor P . The value of a policy, V π, is deﬁned as
(cid:104)(cid:80)H

i.e.

(cid:105)

h=1 rH

(cid:12)
(cid:12)
(cid:12) a1:H ∼ π

,

V π = EP

(1)

where a1:H ∼ π abbreviates for a1 = π(x1), . . . , aH =
π(xH ). Throughout, the expectation is always taken over
contexts and rewards drawn according to the system de-
scriptor P , so we suppress the subscript P . The goal of the
agent is to ﬁnd a policy π that attains the largest value.

like MDPs and

CDPs capture classical RL models,
POMDPs, with appropriately chosen contexts:
Example 1 (MDPs with states as contexts). Consider a
ﬁnite-horizon MDP (S, A, H, Γ1, Γ, R), where S is the
state space, A is the action space, H is the horizon, Γ1 ∈
∆(S) is the initial state distribution, Γ : S × A → ∆(S)
is the state transition function, R : S × A → ∆([0, 1])
is the reward function, and an episode takes the form of
(s1, a1, r1, . . . , sH , aH , rH ). We can convert the MDP
to a CDP (X , A, H, P ) by letting X = S × [H] and
xh = (sh, h), which allows the set of policies {X →
A} to contain the optimal policy (Puterman, 1994). The
system descriptor is P = (P∅, P+), where P∅(x1) =
Γ1(s1),
and P+(rh, xh+1 | x1, a1, r1, . . . , xh, ah) =
R(rh|sh, ah) Γ(sh+1|sh, ah).

As above, the system descriptor for a model is usually obvi-
ous and we omit its speciﬁcation in the following examples.
Turning to POMDPs, it might seem that a CDP limits the
agent’s decision-making strategies to memoryless (or reac-
tive) policies, as we only consider policies in {X → A}.
This is not true. We clarify this issue by showing that we
can use the history as context, and the induced CDP suffers
no loss in the ability to represent optimal policies.
Example 2 (POMDPs with histories as contexts). Con-
sider a ﬁnite-horizon POMDP with a hidden state space S,
an observation space O, and an emission process Ds speci-
fying a distribution over O. We can convert the POMDP to

a CDP (X , A, H, P ) by letting X = (O ×A×R)∗ ×O and
xh = (o1, a1, r1, . . . , oh) is the observed history at level h.

Our next example considers a POMDP where the context
can be substantially more concise than the full history. As
will be formalized in Section 2.2, all we need is that the
context can express a good value function, which is sig-
niﬁcantly weaker than requiring it be a sufﬁcient statistic
(unlike e.g., Hutter 2005). Therefore, it is important to sep-
arate the context in a CDP from any precise notion of state
in the process, and instead keep it as a modeling choice.

Example 3 (POMDPs with sliding windows of observa-
tions as contexts). Sometimes partial observability can be
resolved by using a small history: for example, in Atari
games, it is common to keep track of the last 4 images
(Mnih et al., 2015). In this case, we can represent the prob-
lem as a CDP by letting xh = (oh−3, oh−2, oh−1, oh).

We hope the above examples demonstrate the generality
and ﬂexibility of the CDP framework. Finally, we intro-
duce a regularity assumption on the rewards.

Assumption 1 (Boundedness of rewards). We assume
that regardless of how actions are chosen, for any h =
1, . . . , H, rh ≥ 0 and (cid:80)H

h=1 rh ≤ 1 almost surely.4

2.2. Value-based RL and Function Approximation

A CDP makes no assumptions on the cardinality of the con-
text space, which makes it critical to generalize across con-
texts, since an agent might not observe the same context
twice. Hence, we consider value-based RL with function
approximation. That is, the agent is given a set of func-
tions F ⊆ X × A → [0, 1] and uses it to approximate
an action-value function (or Q-function). To avoid impos-
ing boundary-conditions, we set f (xH+1) ≡ 0 w.l.o.g. For
ease of presentation, we assume that F is ﬁnite with |F| =
N < ∞ throughout the paper. In Appendix A.3 we allow
inﬁnite function classes with bounded complexity.

As in typical value-based RL, the goal is to identify f ∈ F
which respects a particular set of Bellman equations and
achieves a high value with its greedy policy πf (x) =
argmaxa∈A f (x, a). We next set up the appropriate exten-
sions of Bellman equations to CDPs and the optimal value
V (cid:63)
F through a series of deﬁnitions. Unlike MDPs, these in-
volve both the CDP and function approximator F.

Deﬁnition 2 (Average Bellman error). Given a policy π :
X → A and a function f : X × A → [0, 1], the average
Bellman error of f under π at level h is deﬁned as

E(f, π, h) = E (cid:2)f (xh,ah) − rh − f (xh+1, ah+1)
(cid:12)
(cid:12) a1:h−1 ∼ π, ah:h+1 ∼ πf

(cid:3).

(2)

4The bound of 1 is w.l.o.g. More generally, we may simply
replace (cid:15) with (cid:15)/R in all the sample complexity results when the
bound is R. See more discussion in Kakade (2003, Section 2.2.3).

Contextual Decision Processes with low Bellman rank are PAC-Learnable

The average Bellman error measures the self-consistency
of f between its predictions at levels h and h + 1, when all
the previous actions are taken according to some policy π.
We now deﬁne a set of Bellman equations.

Deﬁnition 3 (Bellman equations and validity of f ). Given
an (f, π, h) triple, a Bellman equation posits E(f, π, h) =
0. We say f ∈ F is valid if the Bellman equation on
(f, πf (cid:48), h) holds for every f (cid:48) ∈ F, h ∈ [H].

Note that the validity assumption only considers roll-ins
according to the greedy policies πf , which is the natural
policy class given F. In MDPs, each Bellman equation can
be viewed as the linear combination of the standard Bell-
man optimality equations for Q(cid:63),5 where the coefﬁcients
are the probabilities with which the roll-in policy π visits
each state. This leads to the following consequence.
Fact 1 (Q(cid:63) is always valid). Given an MDP and a space of
functions F : S × [H] × A → [0, 1], if Q(cid:63) ∈ F, then in the
corresponding CDP with X = S × [H], Q(cid:63) is valid.

While Q(cid:63) satisﬁes the Bellman equations and yields the
optimal policy π(cid:63) = πQ(cid:63) , there can be other functions
which also satisfy the equations while yielding suboptimal
policies. For instance, if f (x, πf (x)) correctly predicts the
long-term reward of πf , then f is always valid. Since va-
lidity alone does not imply that we get a good policy, it
is natural to search for a valid value function which also
induces a high-value policy. We formalize this goal next.

Deﬁnition 4 (Optimal value). Deﬁne

f (cid:63) = argmax

V πf , and V (cid:63)

F = V πf (cid:63) .

f ∈F : f is valid

Fact 2. In the setting of Fact 1, we have f (cid:63) = Q(cid:63) ∈ F,
F = V (cid:63), which is the optimal long-term value.
and V (cid:63)

Deﬁnition 4 implicitly assumes that there is at least one
valid f ∈ F. This is weaker than the realizability assump-
tion made in the value-based RL literature, that F contains
Q(cid:63) of an MDP (e.g., Krishnamurthy et al., 2016) (see
Facts 1 and 2). While some works only require Q(cid:63) to be
approximately captured (e.g., Antos et al., 2008), our al-
gorithm can also be adapted to work with an approximate
notion of validity as discussed in Appendix A.4.

3. Bellman Factorization and Bellman Rank

CDPs are general models for sequential decision making,
but are there efﬁcient RL algorithms for them?

Unfortunately, without further assumptions,
learning in
CDPs is generally hard, since they subsume MDPs and

5We refer the readers who are not familiar with the deﬁnition

of Q(cid:63) to standard texts, such as (Sutton & Barto, 1998).

POMDPs with arbitrarily large state/observation spaces.
Formally, the sample complexity of learning CDPs in the
worst-case is Ω(K H ) when K = |A|, even when the com-
plexity of the function class, measured by log |F|, is small.
The result is due to Krishnamurthy et al. (2016) and is in-
cluded in Appendix F.1 for completeness.

Of course the lower bound instances are quite pathologi-
cal and devoid of any structure that is often present in real
problems. To capture these realistic scenarios, we propose
a new complexity measure and restrict our attention to set-
tings where this measure is low. As we will see, this mea-
sure is naturally small for many existing models, and, when
it is small, efﬁcient reinforcement learning is possible.

The complexity measure we propose is a structural charac-
terization of the set of Bellman equations induced by the
CDP and the class F (recall Deﬁnitions 2 and 3), that we
need to check to ﬁnd valid functions. Checking validity by
enumeration is statistically intractable for large F, since it
requires Ω(|F|) samples to perform all roll-ins. However,
observe that the Bellman equations are structured in tabular
MDPs: the average Bellman error under any roll-in policy
is a stochastic combination of the single-state errors, and
checking the single-state errors (which is tractable) is suf-
ﬁcient to guarantee validity. This observation hints toward
a more general phenomenon: whenever the collection of
Bellman errors across all roll-in policies can be concisely
represented, we may be able to check the validity of all
functions in a tractable way.

This intuition motivates a new complexity measure that we
call the Bellman rank. Deﬁne the Bellman error matrices,
one for each h, to be |F| × |F| matrices where the (f, f (cid:48))th
entry is the Bellman error E(f, πf (cid:48), h).
Informally, the
Bellman rank for a CDP and a given value-function class
F is a uniform upper bound on the rank of these H Bell-
man error matrices.

Deﬁnition 5 (Bellman factorization and Bellman rank). We
say that a CDP (X , A, H, P ) and F ⊂ X ×A → [0, 1] ad-
mit Bellman factorization with Bellman rank M and norm
parameter ζ, if there exists νh : F → RM , ξh : F → RM
for each h ∈ [H], such that for any f, f (cid:48) ∈ F, h ∈ [H],

E(f, πf (cid:48), h) = (cid:104)νh(f (cid:48)), ξh(f )(cid:105),

(3)

and (cid:107)νh(f (cid:48))(cid:107)2 · (cid:107)ξh(f )(cid:107)2 ≤ ζ < ∞.

The exact factorization in Eq. (3) can be relaxed to an ap-
proximate version as is discussed in Appendix A.4. Unlike
rank-based notions in PSRs (Littman et al., 2001) and mul-
tiplicity automata (Sch¨utzenberger, 1961), Bellman rank
depends both on the process and the class F.
In the re-
mainder of this section we showcase the generality of Def-
inition 5 by describing a number of common RL settings
that have a small Bellman rank. Throughout, we see how

Contextual Decision Processes with low Bellman rank are PAC-Learnable

the Bellman rank captures the process-speciﬁc structures
that allow for efﬁcient exploration. Proofs of all claims in
this section are deferred to Appendix B.

We start with the tabular MDP setting, and show that the
Bellman rank is at most the number of states.

Proposition 1 (Bellman rank bounded by number of states
in MDPs). Consider the setting of Example 1 with the cor-
responding CDP. With any class F, this model admits a
Bellman factorization with M = |S| and ζ = 2

M .

√

The MDP example is particularly simple as each coordinate
of the M -dimensional space corresponds to a state, which
is observable. Our next few examples show that this is not
necessary, and that the Bellman factorization can be based
on latent properties of the process. We next consider large
MDPs whose transition dynamics have a low-rank struc-
ture. A closely related setting has been considered by Bar-
reto et al. (2011; 2014) where the low-rank structure is ex-
ploited to speed up MDP planning, but no sample-efﬁcient
RL algorithms were previously known for this setting.

Proposition 2 (Bellman rank in low-rank MDPs, infor-
mally). Consider the setting of Example 1 with a transi-
tion matrix Γ having rank at most M . The induced CDP
along with any F ⊂ X × A → [0, 1] admits a Bellman
factorization with Bellman rank M .

The next example considers POMDPs with large observa-
tions spaces and reactive value functions, where the Bell-
man rank is at most the number of hidden states.

Proposition 3 (Bellman rank bounded by hidden states in
reactive POMDPs). Consider the setting of Example 3 with
|S| < ∞ and a sliding window of size 1. Given any F ⊂
X × A → [0, 1], this model admits a Bellman factorization
M .
with M = |S| and ζ = 2

√

Propositions 2 and 3 can be proved under a uniﬁed model
that generalizes POMDPs by allowing the transition and
reward functions to depend on the observation (Figure 1).
This model captures the experimental settings considered
in state-of-the-art empirical RL work, where agents act in a
grid-world (|S| is small) and receives complex and rich ob-
servations such as raw pixel images (|O| is large); see e.g.,
Johnson et al. (2016). The model also subsumes and gen-
eralizes the setting of Krishnamurthy et al. (2016) which
requires deterministic transitions in the underlying MDP.

Next, we consider Predictive State Representations (PSRs),
which are models of partially observable systems with pa-
rameters grounded in observable quantities (Littman et al.,
2001). Similar to the case of POMDPs, we can bound the
Bellman rank in terms of the rank of the PSR6 when the
candidate value functions are reactive.

. . .

s

o

r

a

. . .

s(cid:48)

o(cid:48)

Figure 1. A uniﬁed model that subsumes MDPs and reactive
POMDPs and has a low Bellman rank. Gray nodes represent un-
observable quantities while diamonds are actions controlled by
the agent. The dashed arrow indicates that the action is a function
only of the current observation, so value functions are reactive.

Proposition 4 (Bellman rank in PSRs, informally). Con-
sider a partially observable system with observation space
O and the induced CDP (X , A, H, P ) with xh = (oh, h).
If the linear dimension of the system (i.e., rank of its PSR
model) is at most L, then given any F : X × A → [0, 1],
the Bellman rank is bounded by LK.

The last example considers a class of linear control prob-
lems called Linear Quadratic Regulators (LQRs). We show
that the Bellman rank in LQRs is bounded by the dimension
of the state space. Unlike previous examples, here we cru-
cially use structure of the quadratic value functions, which
is the form Q(cid:63) takes. Exploration in this class of prob-
lems has been previously considered by Osband & Van Roy
(2014). Note that the algorithm to be introduced in the next
section does not directly apply to LQRs due to the continu-
ous action space, and adaptations that exploit the structure
of the action space may be needed.

Proposition 5 (Bellman rank in LQRs, informally). An
LQR can be viewed as an MDP with continuous state space
Rd and action space RK, where the dynamics are de-
scribed by some linear equations. Given the function class
F which consists of non-stationary quadratic functions of
the state, the Bellman rank is bounded by d2 + 1.

4. Algorithm and Main Results

In this section we present our algorithm for learning CDPs
that have a Bellman factorization with small Bellman rank,
along with the main sample complexity guarantee. To aid
presentation and help convey the main ideas, we make three
simplifying assumptions. We assume that (1) the agent
knows the Bellman rank M and the corresponding norm
bound, (2) the function class F is ﬁnite with cardinality N ,
and (3) the validity and Bellman factorization conditions
(Deﬁnitions 3 and 5) hold exactly. We relax these assump-
tions in Section 5 and Appendix A.

6Every POMDP has an equivalent PSR whose rank is bounded

by the number of hidden states (Singh et al., 2004).

We are interested in designing an algorithm for PAC
Learning CDPs. We say that an algorithm PAC learns a

Contextual Decision Processes with low Bellman rank are PAC-Learnable

CDP if given F, two parameters (cid:15), δ ∈ (0, 1), and ac-
cess to the CDP, the algorithm outputs a policy ˆπ with
V ˆπ ≥ V (cid:63)
F − (cid:15) with probability at least 1 − δ. The
sample complexity is the number of episodes needed to
achieve such a guarantee, and is typically expressed in
terms of (cid:15), δ, and other relevant parameters. The goal
is to design an algorithm with sample complexity that
is Poly(M, K, H, 1/(cid:15), log(N ), log(1/δ)) where M is the
Bellman rank, K is the number of actions, and H is the
time horizon. Importantly, the bound allows no dependence
on the number of unique contexts |X |.

4.1. Algorithm

Pseudocode for our algorithm, OLIVE (Optimism Led It-
erative Value-function Elimination), is displayed in Algo-
rithm 1. Theorem 1 describes how to set the parameters
nest, neval, n, and φ. For brevity, we introduce a shorthand
for empirical Bellman errors given a tuple (x, a, r, x(cid:48)):

σ(f, x, a, r, x(cid:48)) := f (x, a) − r − f (x(cid:48), πf (x(cid:48))).

(4)

At a high level, the algorithm aims to eliminate functions
f ∈ F that fail to satisfy the validity condition in Deﬁni-
tion 3. This is done by Lines 13 and 14 inside the loop of
the algorithm. Line 13 uses importance weighting to get
an unbiased estimate of E(f, πt, ht), the average Bellman
error for function f on roll-in policy πt at time ht. Thus,
Line 14 eliminates functions that have high average Bell-
man error under πt and hence are not valid.

The other major component of the algorithm involves
choosing the roll-in policy πt and level ht on which to
do the learning step. At iteration t, we choose the roll-in
policy πt optimistically, by choosing ft that predicts the
highest value at the starting context distribution and setting
πt = πft. To pick ht, we compute ft’s average Bellman
error on its own roll-in distribution (Line 7), and set ht to
be any level for which this average Bellman error is high
(See Line 11). As we will show, these choices ensure that
substantial learning happens on each iteration, guarantee-
ing that the algorithm uses polynomially many episodes.

The last component is the termination criterion. The algo-
rithm terminates if ft has small average Bellman error on
its own roll-in distribution at all levels. This criteria guar-
antees that πt is near optimal.

Computationally, the algorithm requires enumeration of the
value-function class, which we expect to be extremely large
or inﬁnite in practice. A computationally efﬁcient imple-
mentation is essential for a practical algorithm, which re-
mains an open question. We focus on the sample efﬁciency
of the algorithm in this paper.

Intuition for OLIVE. To convey intuition, it is helpful to
ignore any sampling effects by replacing all empirical esti-

mates with population values and set (cid:15) to 0. The ﬁrst im-
portant fact is that the algorithm never eliminates a valid
function, since the learning step in Line 14 only elimi-
nates a function f if we can ﬁnd a distribution on which
If f is valid, then
it has a large average Bellman error.
E(f, π, h) = 0 for all π, h, so f is never eliminated.

The second fact is that if a function f is valid, then its pre-
dicted value is exactly the value achieved by the greedy
policy πf , that is Vf = E[f (x1, πf (x1))] = V πf . This is
based on the following lemma.

Lemma 1 (Value-function error decomposition). Deﬁne
Vf = E[f (x1, πf (x1))]. Then ∀f : X × A → [0, 1],

Vf − V πf =

E(f, πf , h).

(5)

H
(cid:88)

h=1

Therefore, since ft is chosen optimistically as the maxi-
mizer of the value prediction among the surviving func-
if
tions, and since we never eliminate valid functions,
OLIVE terminates, it must output a policy with value V (cid:63)
F .
In the analysis, we incorporate sampling effects to derive
robust versions of these facts so the algorithm always out-
puts a policy that is at most (cid:15)-suboptimal.

The more challenging component is bounding the number
of iterations of the algorithm, which is critical for obtain-
ing a polynomial sample complexity bound. This argu-
ment crucially relies on the Bellman factorization (Deﬁ-
nition 5), which enables us to embed the distributions over
contexts for any roll-in policy into M dimensions and mea-
sure progress in this low-dimensional space.

For now, ﬁx some h and focus on the iterations where
ht = h. If we ignore sampling effects we can set φ = 0. By
using the Bellman factorization to write E(f, πft, h) as an
inner product, we can think of the learning step in Line 14
as introducing a homogeneous linear constraint on the set
of ξh(f ) vectors: (cid:104)νh(ft), ξh(f )(cid:105) = 0. Now, if we exe-
cute the learning step at h again in a later iteration t(cid:48), we
have (cid:104)νh(ft(cid:48)), ξh(ft(cid:48))(cid:105) (cid:54)= 0 from Line 11. Importantly, this
means that νh(ft(cid:48)) must be linearly independent from pre-
vious νh(ft) since (cid:104)νh(ft), ξh(ft(cid:48))(cid:105) = 0. Since every time
ht = h, the number of linearly independent constraints in-
creases by 1, the number of iterations where ht = h is at
most M , the dimension of the space. Thus the Bellman
rank (times H) upper-bounds the number of iterations.

The above heuristic reasoning, despite relying on the brittle
notion of linear independence, can be made robust. With
sampling effects, rather than homogeneous linear equali-
ties, the learning step for level h introduces linear inequal-
ity constraints to the ξh(f ) vectors. But if f (cid:48) is a sur-
viving function that forces us to train at h, it means that
(cid:104)νh(f (cid:48)), ξh(f (cid:48))(cid:105) is very large, while (cid:104)νh(·), ξh(f (cid:48))(cid:105) is very
small for all previous νh(·) vectors used in the learning

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Algorithm 1 OLIVE (F, M, ζ, (cid:15), δ) – Optimism Led Iterative Value-function Elimination
1: Collect nest trajectories with actions taken in an arbitrary manner; save initial contexts {x(i)
2: Estimate the predicted value for each f ∈ F: ˆVf = 1
nest
3: F0 ← F.
4: for t = 1, 2, . . . do
5:

1 , πf (x(i)

i=1 f (x(i)

1 )).

(cid:80)nest

1 }nest
i=1.

h = πt(x(i)
h , a(i)

i=1 σ(f, x(i)

h ) for all h and i = 1, . . . , neval).
h , r(i)

h , x(i)

h+1).

h ) for all h (cid:54)= ht and a(i)
ht
=πf (x(i)
σ(f, x(i)
ht
ht
1/K

1[a(i)
ht

)]

is drawn uniformly at random.
, a(i)
ht

ht+1).

, r(i)
ht

, x(i)

(see Eq. (4))

6:

7:

8:
9:
10:
11:

12:

13:

ˆVf , πt = πft.
Choose policy ft = argmaxf ∈Ft−1
Collect neval trajectories by following πt (i.e. a(i)
Estimate ∀h ∈ [H], ˜E(ft, πt, h) := 1
neval
if (cid:80)H

˜E(ft, πt, h) ≤ 5(cid:15)/8 then

(cid:80)neval

h=1
Terminate and output πt.

end if
Pick ht ∈ [H] such that ˜E(ft, πt, ht) ≥ 5(cid:15)/(8H).
Collect n trajectories where a(i)

Estimate ∀f ∈ F, (cid:98)E(f, πt, ht) := 1
n
Learn Ft =

f ∈ Ft−1 :

(cid:110)

h = πt(x(i)
(cid:80)n
(cid:12)
(cid:12)
(cid:12) ≤ φ

(cid:12)
(cid:12)
(cid:12) (cid:98)E(f, πt, ht)

i=1

(cid:111)

.

14:
15: end for

step. Intuitively this means that the new νh(f (cid:48)) vector is
quite different from all of the previous ones. Our proof uses
a volumetric argument to show that this sufﬁces to guar-
antee substantial learning takes place. In more detail, we
track the volume of an enclosing ellipsoid of the surviving
ξh(f ) functions and show that each time we learn at level
h this volume shrinks multiplicatively, which results in an
iteration complexity that is linear in M H.

The optimistic choice for ft is critical for driving the
agent’s exploration. With this choice, if ft is valid, then the
algorithm terminates correctly, and if ft is not valid, then
substantial progress is made. Thus the agent does not get
stuck exploring with many valid but suboptimal functions,
which could result in exponential sample complexity.

4.2. Sample Complexity

We now turn to the main result, which guarantees that
OLIVE PAC-learns Contextual Decision Processes with
polynomial sample complexity.

Theorem 1. For any (cid:15), δ ∈ (0, 1), any CDP and function
class F that admit a Bellman factorization with parameters
M and ζ, run OLIVE with the following parameters:

φ =

neval =

(cid:15)
√
12H
288H 2
(cid:15)2

,

M

log

nest =

32
(cid:15)2 log(6N/δ),
√
M ζ/(cid:15))

(cid:32)

12H 2M log(6H

n =

4608H 2M K
(cid:15)2

log

(cid:32)

12N HM log(6H

M ζ/(cid:15))

(cid:33)

.

δ

δ

(cid:33)

,

√

F − (cid:15) (recall Deﬁnition 3 for V (cid:63)
F ),

ˆπ that satisﬁes V ˆπ ≥ V (cid:63)
and the number of episodes required is at most7
(cid:18) M 2H 3K
(cid:15)2

log(N ζ/δ)

˜O

(cid:19)

.

(6)

Thus, if a CDP and function class F admit a Bellman fac-
torization with small Bellman rank and F contains valid
functions, OLIVE is guaranteed to ﬁnd a near optimal valid
function using only polynomially many episodes. To our
knowledge, this is the most general polynomial sample
complexity bound for RL with rich observations and func-
tion approximation, as many popular models are shown to
admit small Bellman rank (see Section 3, Table 1). The re-
sult also certiﬁes that the notion of Bellman factorization,
which is quite general, is sufﬁcient for efﬁcient exploration
and learning in sequential decision making problems.

It is worth brieﬂy comparing this result with prior work.

1. The most closely related result is the recent work
of Krishnamurthy et al. (2016), who also consider
episodic RL with inﬁnite observation spaces and func-
tion approximation. The model studied there is a CDP
with Bellman rank M , so our result applies as is to that
setting. Importantly, we eliminate the need for deter-
ministic transitions in that work, while improving the
dependence on H and (cid:15), although with worse scaling
in M . We emphasize that our result applies to a much
more general class of models.

2. Several works provide sample complexity bounds
for ﬁtted value/policy iteration methods (e.g., Munos

7We use ˜O(·) notation to suppress poly-logarithmic depen-

Then, with probability at least 1−δ, OLIVE returns a policy

dence on everything except N and δ.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

(2003); Antos et al. (2008); Munos & Szepesv´ari
(2008)). While these results are relevant, they do not
address the exploration issue, which is our main fo-
cus, and circumvent it by impliciting assuming an ex-
ploratory policy for data collection.

3. Ng & Jordan (2000) proposed a policy search method
for POMDPs called PEGASUS, with a sample com-
plexity that scales polynomially with the statistical
complexity of the policy class and the horizon. De-
spite the powerful result, the algorithm requires care-
ful control over the random numbers that determine
the state transitions. While the assumption can hold
for certain simulated environments, the scope of ap-
plications is relatively limited.

4. Since CDPs include small-state MDPs (Kearns &
Singh, 2002; Brafman & Tennenholtz, 2003; Strehl
et al., 2006), the algorithm can be applied as is to
these problems. Unfortunately, our sample com-
plexity is polynomially worse than the state of the
art ˜O( M poly(H)K
log(1/δ)) bounds for PAC-learning
MDPs (Dann & Brunskill, 2015). On the other hand,
the algorithm also applies to MDPs with inﬁnite state
spaces with Bellman factorizations, which cannot be
handled by tabular approaches.

(cid:15)2

5. Finally, Contextual Decision Processes also encom-
pass contextual bandits, where the sample complexity
is Θ(K log(N )/(cid:15)2) (Agarwal et al., 2014). As con-
textual bandits have M = H = 1, OLIVE achieves
the optimal sample complexity in this case.

Turning brieﬂy to lower bounds, since the CDP setting with
Bellman factorization is new, general lower bounds for the
broad class do not exist. However, we can use MDP lower
bounds for guidance on the question of optimality, since the
small-state MDPs in Example 1 are a special case. While
no existing MDP lower bounds apply as is (because formu-
lations vary), in Appendix F.2 we adapt ideas from Auer
et al. (2002) to obtain a Ω(M KH/(cid:15)2) sample complexity
lower bound for learning the MDPs in Example 1.

In comparison, the sample complexity in Theorem 1 is
worse in M, H, and log(N ) factors, but of course the
small-state MDP is a signiﬁcantly simpler special case. We
leave as future work the question of optimal sample com-
plexity for learning CDPs with low Bellman rank.

context-value function class G ⊂ X → [0, 1] and a
policy class Π ⊂ X → A instead of a context-action
value class as in OLIVE, with sample complexity de-
pending on the pseudo-dimension of G and the Natara-
jan dimension of Π. These are standard measures for
regression and multi-class classiﬁcation, and several
natural classes have known bounds.

2. Competing with approximately valid value-functions
with inexact Bellman factorization. For this result, we
extend the deﬁnition of validity and V (cid:63)
F (Defs. 3 and 4)
to allow small but non-zero Bellman errors, and also
only require that the Bellman error matrices have a
low rank approximation with small (cid:96)∞ error.

3. Adapting to unknown Bellman rank. Here we run
OLIVE with choices of M growing at a doubling
schedule and show that the PAC-guarantee is pre-
served without loss in sample complexity.

6. Discussion

In this paper, we presented a new model for RL with rich
observations, called Contextual Decision Processes, and
a structural property, the Bellman factorization, of these
models that enables sample-efﬁcient learning. The uniﬁed
approach allows us to address several settings of practical
interest that have largely eluded RL theory to date. Our
work also elicits several further questions:

1. Can we obtain a computationally efﬁcient algorithm
for some form of this setting? Prior related work
(for instance in contextual bandits (Dudik et al., 2011;
Agarwal et al., 2014)) used supervised learning ora-
cles for computationally efﬁcient approaches. Is there
a suitable oracle for this setting?

2. The sample complexity depends polynomially on the
cardinality of the action space. Can we extend the re-
sults to handle large or continuous action spaces (e.g.,
by incorporating concepts such as Eluder dimension
(Russo & Van Roy, 2013))?

3. Can we address sample-efﬁcient RL given only a pol-
icy class rather than a value function class? Empirical
approaches often rely on policy gradients, which are
subject to local optima. Are there parallel results to
this work, without access to value functions?

5. Extensions

Resolutions to these questions are important for further
connecting RL theory with practice.

The basic result presented here is quite robust and admits
many extensions, some of which we brieﬂy describe here;
the details are deferred to Appendix A.

Acknowledgements

1. Handling inﬁnite function classes with dependence
on VC-dimension like quantities. This result uses a

Part of this work was completed while NJ and AK were at
Microsoft Research. NJ was partially supported by Rack-
ham Predoctoral Fellowship in University of Michigan.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

References

Agarwal, Alekh, Hsu, Daniel, Kale, Satyen, Langford, John, Li,
Lihong, and Schapire, Robert E. Taming the monster: A fast
and simple algorithm for contextual bandits. In International
Conference on Machine Learning, 2014.

Haussler, David. Sphere packing numbers for subsets of the
Boolean n-cube with bounded Vapnik-Chervonenkis dimen-
sion. Journal of Combinatorial Theory, Series A, 1995.

Haussler, David and Long, Philip M. A generalization of Sauer’s
lemma. Journal of Combinatorial Theory, Series A, 1995.

Anderson, Brian D.O. and Moore, John B. Optimal Control: Lin-

ear Quadratic Methods. Courier Corporation, 2007.

Hutter, Marcus. Universal Artiﬁcial Intelligence: Sequential De-
cisions based on Algorithmic Probability. Springer, 2005.

Antos, Andr´as, Szepesv´ari, Csaba, and Munos, R´emi. Learn-
ing near-optimal policies with bellman-residual minimization
based ﬁtted policy iteration and a single sample path. Machine
Learning, 2008.

Johnson, Matthew, Hofmann, Katja, Hutton, Tim, and Bignell,
David. The Malmo Platform for artiﬁcial intelligence experi-
mentation. In International Joint Conference on Artiﬁcial In-
telligence, 2016.

Auer, Peter, Cesa-Bianchi, Nicolo, Freund, Yoav, and Schapire,
Robert E. The nonstochastic multiarmed bandit problem. SIAM
Journal on Computing, 2002.

Jong, Nicholas K. and Stone, Peter. Model-based exploration in
In Abstraction, Reformulation, and

continuous state spaces.
Approximation, 2007.

Azizzadenesheli, Kamyar, Lazaric, Alessandro, and Anandku-
mar, Animashree. Reinforcement learning of POMDPs using
spectral methods. Conference on Learning Theory, 2016.

Barreto, Andr´e da Motta Salles, Pineau, Joelle, and Precup,
Doina. Policy iteration based on stochastic factorization. Jour-
nal of Artiﬁcial Intelligence Research, 2014.

Barreto, Andre S, Precup, Doina, and Pineau, Joelle. Reinforce-
ment learning using kernel-based stochastic factorization. In
Advances in Neural Information Processing Systems, 2011.

Bellemare, Marc G., Srinivasan, Sriram, Ostrovski, Georg,
Schaul, Tom, Saxton, David, and Munos, Remi. Unifying
count-based exploration and intrinsic motivation. In Advances
in Neural Information Processing Systems, 2016.

Ben-David, Shai, Cesa-Bianchi, Nicolo, and Long, Philip M.
Characterizations of learnability for classes of {0,. . . , n}-
valued functions. In Conference on Learning Theory, 1992.

Bland, Robert G, Goldfarb, Donald, and Todd, Michael J. The

ellipsoid method: A survey. Operations research, 1981.

Boots, Byron, Siddiqi, Sajid M., and Gordon, Geoffrey J. Clos-
ing the learning-planning loop with predictive state representa-
tions. International Journal of Robotics Research, 2011.

Brafman, Ronen I. and Tennenholtz, Moshe. R-max – a gen-
eral polynomial time algorithm for near-optimal reinforcement
learning. Journal of Machine Learning Research, 2003.

Dann, Christoph and Brunskill, Emma. Sample complexity of
episodic ﬁxed-horizon reinforcement learning. In Advances in
Neural Information Processing Systems, 2015.

Devroye, Luc, Gy¨orﬁ, L´aszl´o, and Lugosi, G´abor. A Probabilistic

Theory of Pattern Recognition. Springer-Verlag, 1996.

Dudik, Miroslav, Hsu, Daniel, Kale, Satyen, Karampatziakis,
Nikos, Langford, John, Reyzin, Lev, and Zhang, Tong. Efﬁ-
cient optimal learning for contextual bandits. In Uncertainty in
Artiﬁcial Intelligence, 2011.

Hallak, Assaf, Di Castro, Dotan, and Mannor, Shie. Contextual

Markov decision processes. arXiv:1502.02259, 2015.

Haussler, David. Decision theoretic generalizations of the PAC
model for neural net and other learning applications. Informa-
tion and computation, 1992.

Kakade, Sham. On the sample complexity of reinforcement learn-

ing. PhD thesis, University College London, 2003.

Kakade, Sham, Kearns, Michael, and Langford, John. Explo-
ration in metric state spaces. In International Conference on
Machine Learning, 2003.

Kearns, Michael and Singh, Satinder. Near-optimal reinforcement

learning in polynomial time. Machine Learning, 2002.

Kearns, Michael, Mansour, Yishay, and Ng, Andrew Y. A sparse
sampling algorithm for near-optimal planning in large Markov
decision processes. Machine Learning, 2002.

Kocsis, Levente and Szepesv´ari, Csaba. Bandit based Monte-
Carlo planning. In European Conference on Machine Learn-
ing, 2006.

Krishnamurthy, Akshay, Agarwal, Alekh, and Langford, John.
In Ad-

PAC reinforcement learning with rich observations.
vances in Neural Information Processing Systems, 2016.

Langford, John and Zhang, Tong. The epoch-greedy algorithm
for multi-armed bandits with side information. In Advances in
Neural Information Processing Systems, 2008.

Li, Lihong. A unifying framework for computational reinforce-
ment learning theory. PhD thesis, Rutgers, The State Univer-
sity of New Jersey, 2009.

Littman, Michael L., Sutton, Richard S., and Singh, Satinder. Pre-
dictive representations of state. In Advances in Neural Infor-
mation Processing Systems, 2001.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, An-
drei A, Veness, Joel, Bellemare, Marc G, Graves, Alex, Ried-
miller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, Pe-
tersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioan-
nis, King, Helen, Kumaran, Dharshan, Wierstra, Daan, Legg,
Shane, and Hassabis, Demis. Human-level control through
deep reinforcement learning. Nature, 2015.

Munos, R´emi. Error bounds for approximate policy iteration. In

International Conference on Machine Learning, 2003.

Munos, R´emi and Szepesv´ari, Csaba. Finite-time bounds for ﬁtted
value iteration. Journal of Machine Learning Research, 2008.

Natarajan, Balas K. On learning sets and functions. Machine

Learning, 1989.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Ng, Andrew Y and Jordan, Michael. Pegasus: A policy search
method for large MDPs and POMDPs. In Uncertainty in Arti-
ﬁcial Intelligence, 2000.

Osband, Ian and Van Roy, Benjamin. Model-based reinforcement
learning and the eluder dimension. In Advances in Neural In-
formation Processing Systems, 2014.

Panchenko, Dmitriy. Some extensions of an inequality of Vapnik
and Chervonenkis. Electronic Communications in Probability,
2002.

Pazis, Jason and Parr, Ronald. Efﬁcient PAC-optimal exploration
in concurrent, continuous state MDPs with delayed updates. In
Conference on Artiﬁcial Intelligence, 2016.

Pollard, David. Convergence of Stochastic Processes. Springer

Science & Business Media, 2012.

Puterman, Martin. Markov Decision Processes: Discrete Stochas-

tic Dynamic Programming. Wiley-Interscience, 1994.

Russo, Dan and Van Roy, Benjamin. Eluder dimension and the
sample complexity of optimistic exploration. In Advances in
Neural Information Processing Systems, 2013.

Sch¨utzenberger, M.P. On the deﬁnition of a family of automata.

Information and Control, 1961.

Silver, David, Huang, Aja, Maddison, Chris J., Guez, Arther,
Sifre, Laurent, van den Driessche, George, Schrittwieser,
Julian, Antonoglou, Ioannis, Penneershelvam, Veda, Lanc-
tot, Marc, Dieleman, Sander, Grewe, Dominik, Nham, John,
Kalchbrenner, Nal, Sutskever, Ilya, Lillicrap, Timothy, Leach,
Madeleine, Kavukcuoglu, Koray, Graepel, Thore, and Hass-
abis, Demis. Mastering the game of Go with deep neural net-
works and tree search. Nature, 2016.

Singh, Satinder and Yee, Richard C. An upper bound on the loss
from approximate optimal-value functions. Machine Learning,
1994.

Singh, Satinder, James, Michael R., and Rudary, Matthew R.
Predictive state representations: A new theory for modeling
In Uncertainty in Artiﬁcial Intelligence,
dynamical systems.
2004.

Strehl, Alexander L., Li, Lihong, Wiewiora, Eric, Langford, John,
and Littman, Michael L. PAC model-free reinforcement learn-
ing. In International Conference on Machine Learning, 2006.

Sutton, Richard S and Barto, Andrew G. Reinforcement Learning:

An Introduction. MIT Press, 1998.

Todd, Michael J. On minimum volume ellipsoids containing part
of a given ellipsoid. Mathematics of Operations Research,
1982.

Todd, Michael J and Yıldırım, E Alper. On Khachiyan’s algorithm
for the computation of minimum-volume enclosing ellipsoids.
Discrete Applied Mathematics, 2007.

Wang, Ziyu, de Freitas, Nando, and Lanctot, Marc. Dueling net-
work architectures for deep reinforcement learning. In Inter-
national Conference on Machine Learning, 2015.

Wen, Zheng and Van Roy, Benjamin. Efﬁcient exploration and
value function generalization in deterministic systems. In Ad-
vances in Neural Information Processing Systems, 2013.

