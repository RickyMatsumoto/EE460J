Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo

Matthew D. Hoffman 1

ﬁt

using
an

variational
approximation

Abstract
Deep latent Gaussian models are powerful
and popular probabilistic models of high-
These models are almost
dimensional data.
expectation-
always
true
maximization,
to
maximum-marginal-likelihood estimation.
In
this paper, we propose a different approach:
than use a variational approximation
rather
(which produces biased gradient signals), we
use Markov chain Monte Carlo (MCMC, which
allows us to trade bias for computation). We
ﬁnd that our MCMC-based approach has several
advantages:
likeli-
hoods, produces sharper images, and does not
suffer from the variational overpruning effect.
MCMC’s additional computational overhead
proves to be signiﬁcant, but not prohibitive.

it yields higher held-out

1. Introduction

Deep latent Gaussian models (DLGMs;
Kingma &
Welling, 2014; Rezende et al., 2014) are powerful gener-
ative models that can be efﬁciently ﬁt to very large datasets
of complicated, high-dimensional data. These models as-
sume that the observed data were generated by sampling
some latent variables, feeding them into a deep neural net-
work, and adding some noise to that network’s output.

The central computational challenge in ﬁtting these models
is approximate posterior inference; to make the model bet-
ter ﬁt an observation x, we need to estimate the posterior
distribution p(z | x) over the latent variables z that gener-
ated x. Exact posterior inference is intractable, so these
models are almost always ﬁt with variational inference
(Jordan et al., 1999), which approximates the intractable
posterior with a distribution q from some tractable family.

The choice of family for q matters—more expressive fam-
ilies will better approximate the posterior, leading to better

1Google, San Francisco, California, USA. Correspondence to:

Matthew D. Hoffman <mhoffman@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

learning signals. There has been a ﬂurry of papers recently
proposing more ﬂexible variational families, all of which
show that the quality of the variational approximation mat-
ters (Salimans et al., 2015; Rezende & Mohamed, 2015;
Burda et al., 2016; Tran et al., 2016; Sønderby et al., 2016;
Kingma & Salimans, 2016).

The other workhorse of approximate posterior inference is
Markov chain Monte Carlo (MCMC; Neal, 1993), which
sets up and simulates a random process whose stationary
distribution is the posterior of interest. Compared to varia-
tional methods, MCMC methods have a reputation for be-
ing slow, but accurate. Perhaps surprisingly, there is little
work exploring the use of MCMC to ﬁt DLGMs (Salimans
et al., 2015; Wolf et al., 2016; Han et al., 2017).

In this paper, we explore the advantages of the MCMC ap-
proach, and ﬁnd that, as promised, it allows us to mean-
ingfully trade off bias and computation. By initializing our
chains with a sample from a variational approximation, we
are able to both speed up convergence and guarantee that
our posterior approximation is strictly better than that of
the underlying variational approximation. Some minor ad-
ditional reﬁnements dramatically speed up the algorithm.

In section 5, we show that the MCMC approach elimi-
nates overpruning and blurriness (two known issues with
variational autoencoders), and achieves good held-out
log-likelihood on the dynamically binarized permutation-
invariant MNIST dataset.

2. Background

2.1. Variational Autoencoders and DLGMs

A deep latent Gaussian model (Kingma & Welling, 2014;
Rezende et al., 2014, DLGM; ) assumes the following gen-
erative process for each observation x:

1. Sample a vector z ∈ RK variables independently from

the standard normal distribution.

2. Compute a vector-valued nonlinear function gθ(z). g
is typically a deep neural network with some parame-
ters θ.

3. Sample x from some distribution f (g(z)) that takes

the output of g as a parameter.

Learning DLGMs with MCMC

if x is a vector of

For example,
real-valued num-
bers, a relatively simple DLGM would assume g(z) =
W max{0, V z + c} + b and xd ∼ N (gd(z), σ); g(z) is the
output of a one-hidden-layer ReLU network, and the model
assumes that g is further corrupted by Gaussian noise with
known variance σ2. In general, g could be more complex
(e.g., it might be a deep convolutional network), and dif-
ferent elements of z could be injected at different layers of
computation.

This procedure deﬁnes a joint probability distribution

p(z, x) = N (z; 0, I)f (x; gθ(z)).

(1)

We can easily sample from and compute this joint distribu-
tion for any given parameters θ, latent vector z, and obser-
vation x.

We want to ﬁt the model to data by maximizing the
marginal likelihood of a dataset x1,...,N under the model.
This is an optimization problem; the goal is to choose

θ(cid:63) = arg max

log pθ(xn)

1
N

1
N

(cid:88)

n
(cid:88)

n

θ

θ

(cid:90)

z

= arg max

log

pθ(z, xn)dz.

(2)

Unfortunately, the integral over z is usually intractable.

The typical solution is to apply variational expectation-
maximization (VEM; Bishop, 2006), which approximates
the intractable marginal log p(x) with the evidence lower
bound (ELBO) L(φ, θ):

L(φ, θ, x) (cid:44) Eq[log pθ(x | z)] − KL(qz|x || pz)

(3)

= log pθ(x) − KL(qz|x || pz|x) ≤ log pθ(x).

qφ(z | x) is any family of distributions that we choose;
it is indexed by some parameters φ and an observation x.
Eq denotes expectations with respect to q. KL(q || p) de-
notes the Kullback-Leibler divergence (KLD) between two
distributions q and p. The inequality follows from the non-
negativity of the KLD, and since KL(pz|x || pz|x) = 0, the
bound is tight when qφ(z | x) = pθ(z | x).
The standard choice in DLGMs is to choose qφ(z | x) (cid:44)
h(z; rφ(x)), where rφ(x) is the vector-valued output of an
additional neural network and h(z; r) is a tractable distri-
bution with parameters r. A common form for q is a mul-
tivariate Gaussian; when q(z | x) = (cid:81)
k q(zk | x), q is
called a “mean-ﬁeld” approximation (Jordan et al., 1999).
In this framework, sometimes r is called an “encoder” net-
work and g is called a “decoder” network by analogy to
classical neural autoencoders. This analogy suggested the
name “variational autoencoder” (VAE; Kingma & Welling,
2014); in this paper we will use “DLGM” to refer to the

generative model and “VAE” to refer to the paired en-
coder/decoder architecture for inference/generation.
In VAEs the ﬁrst expectation Eq[log pθ(x | z)] is usually in-
tractable, since it is the integral of a complicated nonlinear
function. But we can estimate it by Monte Carlo sampling
from q. Usually a single sample sufﬁces. Depending on
h, the KLD term may or may not be tractable to compute
exactly, but again it can be approximated by Monte Carlo
as long as h can be computed.

This architecture has a lot to offer: if the variational dis-
tribution q is inexpensive to compute, then we can quickly
and easily estimate and optimize the ELBO using stochas-
tic optimization.

But this speed and ease comes at the price of accuracy.
To the extent that q(z | x) diverges from p(z | x), the
ELBO may be a poor approximation of the true marginal
likelihood, and the VEM estimate of θ may be systemati-
cally different than the true maximum-likelihood estimate
(MLE). In particular, there are two ways to adjust pθ to
increase the ELBO: increase the true marginal likelihood
log pθ(x), or make the typical posterior pθ(z | x) close in
KLD to qφ(z | x).

2.2. Variational pruning

In VAEs this issue tends to manifest itself most dramat-
ically as “pruning”—a phenomenon where the optimizer
severs the connections between most of the latent variables
z and the data (MacKay, 2001). Pruning occurs because of
the KL(qz|x || pz|x) term in the ELBO. Imagine that we
sever the connections between z1 and x, so that z1 and x
are independent. (This can be easily done by zeroing out
some weights in the neural network gθ(z).) Then

KL(qz|x || pz|x) = KL(qz1|x || pz1) + KL(qz2:K |x || pz2:K |x).

(4)

The ﬁrst KLD term can be set to zero by setting q(z1 |
x) = N (z1; 0, 1). To the extent that it is easier for the
optimizer to improve the ELBO by such a strategy, it will
ﬁnd a (possibly bad) local optimum that uses only a few
latent dimensions. Hoffman & Blei (2015); Theis & Hoff-
man (2015) found that for classic mixture models and latent
factor models such as latent Dirichlet allocation (Blei et al.,
2003) this sort of pruning was mostly a local optimum
problem that could be remedied by improved variational
approximations or optimization schemes. Bowman et al.
(2016) and Sønderby et al. (2016) found improved local
optima in VAEs by initially ignoring and then gradually in-
troducing the KL(qz|x || pz) term, and Burda et al. (2016)
found that initializing a mean-ﬁeld VAE with a model that
had been trained with a more ﬂexible q distribution yielded
less pruning and better local optima. But unlike the results
of Hoffman & Blei (2015) for LDA, Burda et al. (2016)

Learning DLGMs with MCMC

still found signiﬁcant pruning even with the more ﬂexible
posterior approximation, and that pruning increased when
moving from the ﬂexible approximation to the mean-ﬁeld
approximation.

The results of Burda et al. (2016) suggest that this prun-
ing phenomenon is not solely due to local optima. Adding
another latent variable to the model should make it pos-
sible to improve the average true marginal log-likelihood
log pθ(x), but it will also make the typical posterior p(z |
x) more difﬁcult to approximate, increasing the average
KL(qz|x || pz|x) term. At some point the marginal cost in
KLD will be greater than the marginal beneﬁt in log pθ(x),
even at the global optimum.

What makes the VAE ELBO so susceptible to pruning com-
pared to shallow mixture and factor models? One expla-
nation is that the expressive power of shallow models is
tightly bound to latent dimensionality—e.g., fewer mixture
components makes for a less-powerful mixture model. So
if a shallow model is ﬁt to a large, high-dimensional dataset
drawn from a difﬁcult-to-model population, it may be pos-
sible to justify using many latent dimensions, since there is
no other way to explain the observed variation in the data.

By contrast, the expressive power of DLGMs is a function
of both the latent dimensionality and the complexity of the
nonlinear function gθ(z). Even if z is one-dimensional,
a DLGM can in principle approximate arbitrary smooth
densities in RD if g approximates a space-ﬁlling curve in
RD and f adds some low-variance noise to g. An exam-
ple of a one-latent-dimension DLGM approximating a two-
dimensional uniform distribution is given in ﬁgure 1. This
example is contrived, but it could be easily implemented
by a neural network (though it might be hard to ﬁt), and it
demonstrates the decoupling of latent dimensionality from
expressive power.

2.3. Markov chain Monte Carlo and Hamiltonian

Monte Carlo

Markov chain Monte Carlo (MCMC; Neal, 1993) is the
classic alternative to variational methods for approximate
posterior inference. MCMC methods proceed by designing
a Markov chain whose stationary distribution is the distri-
bution of interest, then simulating that chain to draw sam-
ples from that distribution. For our purposes here, MCMC
methods have many weaknesses compared to variational
they may require many “burn in” iterations to
methods:
forget their initial state, successive samples from the chain
may be highly correlated, it is sometimes challenging to
diagnose when a chain has burned in (Gelman & Shirley,
2011), and getting an estimate of marginal probabilities us-
ing MCMC is relatively expensive and complicated (Neal,
2001).

Figure 1. Approximating a 2-d distribution with a 1-d DLGM.
Here z ∼ N (0, 1), g1(z) = (Φ(z) − 1)n + (cid:80)n−1
i=0 σ(τ (Φ(z)n −
i − 1)), g2(z) = 1
i=0 σ(τ (Φ(z)n − i)), and xd ∼
Uniform(gd(z) − 1
2n ). Φ(·) denotes the standard
normal CDF, and σ(·) denotes the sigmoid function. Here n = 20
and τ = 1000. As n and τ get large, the model converges to a
2-d uniform distribution.

2 + (cid:80)n−1
n ( 1
2n , gd(z) + 1

Algorithm 1 Hamiltonian Monte Carlo

Input: previous latent variable z, data x,
step size vector (cid:15), number of steps L.

Sample r ∼ N (0, I).
Initialize z(cid:48) ← z, r(cid:48) ← r.
for l = 1 to L do

Update r(cid:48) ← r(cid:48) + (cid:15)
Update z(cid:48) ← z(cid:48) + (cid:15) ◦ r(cid:48)
Update r(cid:48) ← r(cid:48) + (cid:15)

end for
Sample u ∼ Uniform(0, 1).
if u < exp{− 1
Return z(cid:48)

2 ||r(cid:48)||2 + 1

else

end if

Return z

2 ◦ ∇z log p(z(cid:48), x).

2 ◦ ∇z log p(z(cid:48), x).

2 ||r||2} p(z(cid:48),x)

p(z,x) then

MCMC’s great advantage is that it allows us to trade com-
putation for accuracy without limit. We can estimate any
expectation with respect to the posterior arbitrarily pre-
cisely if we run the chain long enough. In contrast, even
very powerful variational methods may be limited by the
form of the variational approximation.

A particularly powerful MCMC algorithm is Hamiltonian
Monte Carlo (HMC, sometimes called hybrid Monte Carlo;
Duane et al., 1987; Neal, 1993; 2011). Suppose the goal
is to sample from p(z | x) ∝ p(z, x); HMC augments
this model to p(r, z | x) (cid:44) N (r; 0, I)p(z | x), where the
“momentum” variable r has the same dimensionality as z.

HMC transforms the problem of posterior sampling into
the problem of simulating the time-evolution of a ﬁctitious
physical system. The algorithm interprets the latent vari-

0.00.20.40.60.81.0g1(z)0.00.20.40.60.81.0g2(z)Samples from p(g(z))0.00.20.40.60.81.0x10.00.20.40.60.81.0x2Samples from p(x)Learning DLGMs with MCMC

2 ||r||2 − D

ables z as a position vector, log p(z, x) as a negative po-
tential energy function, and the Gaussian log-likelihood
− 1
2 log 2π as a negative kinetic energy func-
tion. The sum of the potential and kinetic energy func-
tions deﬁnes a Hamiltonian, which in turn deﬁnes a set of
dynamics that are time-reversible, conserve volume, and
conserve energy. Those dynamics can be simulated with
the “leapfrog” integrator, which is also time-reversible and
volume-preserving, and for small enough integration time
steps is approximately energy-conserving over long trajec-
tories (Leimkuhler & Reich, 2004). Since the leapfrog in-
tegrator is reversible and volume-preserving, we can use
it to deﬁne a deterministic Metropolis proposal. The ac-
ceptance probability of this proposal is min{1, p(r(cid:48),z(cid:48),x)
p(r,z,x) },
which depends only on the difference in energy between
the time-evolved state r(cid:48), z(cid:48) and the original state r, z; if
the simulation is accurate, that difference will be small and
the probability of acceptance will be high.

Algorithm 1 outlines the procedure for a single iteration of
HMC. Each iteration, we resample the momentum r, ap-
ply L iterations of the symplectic “leapfrog” integrator, and
then apply a Metropolis correction to accept or reject the
proposal z(cid:48). Each call requires L calls to the gradient func-
tion ∇z log p(z(cid:48), x)—each leapfrog update begins with the
gradient from the previous update, and if the algorithm is
applied repeatedly then we can cache the initial gradient
∇z log p(z, x).

The algorithm can be used with separate step sizes for each
dimension of z and r (Neal, 2011), which is analogous to
using a diagonal preconditioner in gradient descent. The
optimal step size vector will depend on the problem, but
we can get intuitions from the idealized case where p(z |
x) = (cid:81)
k N (zk; µk, σk). In this case, the optimal setting is
(cid:15)k ∝ σk; this allows each dimension to be explored equally
quickly. We will use this intuition below.

3. Practical MCMC for DLGMs

We are interested in optimizing the average marginal log-
likelihood

(cid:80)

1
N

n log pθ(xn) = 1
N

(cid:80)

n log (cid:82)

z pθ(z, xn)dz.

(5)

Its gradient with respect to θ is

(cid:80)

1
N

n ∇θ log (cid:82)
(cid:80)
= 1
n
N

z pθ(z, xn)dz

(cid:82)
z pθ(z | xn)∇θ log pθ(z, xn)dz,

(6)

since this is the gradient of the ELBO when q(z | x) =
p(z | x) and the ELBO is tight (Dempster et al., 1977).

The standard VAE training recipe typically approximates
this expectation by replacing p(z | x) with a more tractable

distribution q(z | x) and computing a Monte Carlo estimate
of 1
N

Eq[∇θ log p(z, xn)]:

(cid:80)

n

γVAE = 1
|S|

(cid:80)

s∈S ∇θ log pθ(zs, xs),

(7)

where S is a randomly (or cyclically) chosen set of integers
and zs is drawn from q(z | xs). That is, the expectation
with respect to q is approximated with a single sample, and
the expectation with respect to the training set is approxi-
mated with a minibatch. These stochastic approximations
introduce noise, but no bias.

However, replacing the true posterior p(z | x) with q(z |
x) does introduce bias that cannot be averaged away. We
would like to reduce or eliminate this bias.

A naive approach would be to simply run HMC from some
generic initialization to estimate ∇θ log p(x). However, if
we do not run the chain for long enough, this approach may
actually produce a more biased estimate, since it may not
have had time to forget its initialization. Running it for
“long enough” to compete with a variational approximation
may be quite expensive. Instead, we propose using HMC
to improve on an initial variational approximation.

The core idea is this:
initialize an HMC sampler with a
sample from a variational approximation, run it for a small
number of iterations, and use the last sample to get a (hope-
fully nearly unbiased) estimate of ∇θ log p(x). More for-
mally, we deﬁne the reﬁned distribution q(cid:48) as

(cid:15),L,M (z | x) (cid:44)
q(cid:48)

q(˜z | x)HMC(cid:15),L,M (z | ˜z, x)d˜z,

(8)

(cid:90)

˜z

where HMC(cid:15),L,M (z | ˜z, x) is the distribution of the last
sample from an M -step HMC chain with step-size vector (cid:15)
and L leapfrog steps per iteration. In this approach, we sub-
stitute q(cid:48) for q when estimating the gradient of log pθ(x).
Regardless of how many HMC reﬁnement steps we run, q(cid:48)
is guaranteed to have lower KL divergence to the posterior
p(z | x) than q does (Cover & Thomas, 1991, section 2.9 ).
In fact, in some cases the KLD from q(cid:48)(z | x) to p(z | x)
may go down exponentially fast in M (Ottobre & Pavliotis,
2011). This is an appealing property. The bias in the gradi-
ent of the ELBO comes precisely from the KL(qz|x || pz|x)
term, and as that term approaches 0 so too must its gradient
(since 0 is the minimum possible KLD).

This observation suggests that we should still care about
making the initial variational distribution q(z | x) as close
as possible to the posterior p(z | x), since that should re-
duce the number of HMC steps needed to bring q(cid:48) tolerably
close to the posterior. To that end, we use a standard mean-
ﬁeld inference network approach as proposed by Kingma &
Welling (2014) and Rezende et al. (2014). This inference
network’s parameters φ are trained as usual to maximize

qφ(z|x) ]. But, unlike in VAEs, we will not optimize

the number of leapfrog steps L, and the step size vector (cid:15).

Learning DLGMs with MCMC

Eq[log pθ(z,x)
θ using this objective.

3.1. Reﬁnements

In practice, two reﬁnements to the approach outlined above
make it possible to achieve good results with far less com-
putation.

Learning a shared shearing matrix to rotate q(z | x).
DLGMs are unidentiﬁable up to a rotation. For example,
suppose we have two simple DLGMs deﬁned by

p(z) = N (z; 0, I); g(z) = V max{0, W z + b} + c;

p(x | z) = f (x; g(z));
p(cid:48)(z(cid:48)) = N (z(cid:48); 0, I); g(cid:48)(z(cid:48)) = V max{0, W U z(cid:48) + b} + c;
p(cid:48)(x | z(cid:48)) = f (x; g(z(cid:48))),
(9)

where U is an arbitrary rotation matrix. These models de-
ﬁne the same marginal distribution, i.e., p(x) = p(cid:48)(x). We
can see this by substituting z (cid:44) U z(cid:48); the rotation does
not change the marginal distribution of z, and it makes
g(z) = g(cid:48)(z(cid:48)), and therefore p(x | z) = p(cid:48)(x | z(cid:48)).

Such a rotation does, however, rotate the posteriors; that is,
p(z | x) = p(cid:48)(U (cid:62)z | x). This is bad, because mean-ﬁeld
variational inference is not rotationally invariant—it works
best when the posterior exhibits no correlations. In normal
VAE training, the generative network is optimized to be as
friendly as possible to mean-ﬁeld variational inference, so
it learns to rotate its weight matrices to break this rotational
symmetry. In our setup, this does not naturally happen; our
whole goal is in a sense to avoid letting information about
the variational approximation leak through to the genera-
tive network.

To address this issue, we introduce an extra lower-
triangular matrix A in the generative network that can par-
tially correct for such rotations, so that our model is

z ∼ N (0, I);

z(cid:48) (cid:44) Az;

p(x | z) (cid:44) f (g(z(cid:48))).

(10)

A is optimized with φ to maximize the variational ELBO
Eq[log pθ(z,x)
qφ(z|x) ]. A is constrained to have all of its diagonal
elements equal to 1 so that it cannot prune out any latent
dimensions. If q(z | x) = N (z; µ(x), diag(σ(x)2)), then
q(z(cid:48) | x) = N (z(cid:48); Aµ(x), Adiag(σ(x)2)A(cid:62)), so adding A
allows q(z(cid:48) | x) to have correlation structure. We found
that adding this correlation structure signiﬁcantly improves
the quality of the initial mean-ﬁeld approximation. In con-
junction with the per-variable step size technique outlined
below, we found that using this shearing matrix speeds up
the algorithm by 2–3x.

Setting per-variable step sizes and the number of
leapfrog steps. HMC has two critical tuning parameters:

As in gradient descent,
large step sizes allow for fast
progress, but step sizes that are too large lead to unstable re-
sults (and therefore Metropolis rejections). It makes sense
to use smaller step sizes in the dimensions with larger gra-
dients and more oscillatory dynamics.

If L is too small, then the algorithm will make small up-
dates each iteration, leading to random-walk behavior and
If L is too large, eventually
slow mixing (Neal, 2011).
the trajectory will turn back and retrace its steps, wasting
computation. Hoffman & Gelman (2014) proposed the no-
U-turn sampler (NUTS), a variant that automatically de-
termines an appropriate number of steps, but NUTS is a
complicated recursive algorithm that is quite difﬁcult to im-
plement in computational graph languages such as Tensor-
Flow (Abadi et al., 2016).

We adopt a simple heuristic for setting the step sizes and
leapfrog steps based on intuitions from the case where the
posterior is Gaussian with diagonal covariance.
In this
case, the continuous-time Hamiltonian dynamics in each
dimension k are sinusoidal with period 2πσk, where σk is
the scale of dimension k.
If we use a step size vector (cid:15)
such that (cid:15)k = (cid:15)0σk, then after L steps of leapfrog integra-
tion the dynamics in each dimension will have advanced
by (cid:15)0L
(cid:101), so that each
HMC iteration simulates approximately (2π)−1 ≈ 0.16 pe-
riods. This number is somewhat arbitrary; it is chosen to be
slightly less than 0.25 periods. We found that using larger
integration times sometimes caused undesirable resonances
and aliasing (Neal, 2011).

2π periods. We therefore set L = (cid:100) 1
(cid:15)0

We tune (cid:15)0 throughout training to give a reasonable worst-
case acceptance rate. In each minibatch, we compute the
average acceptance rate over the M HMC iterations for
each example in the minibatch. If the smallest average ac-
ceptance rate is less than 0.25, we decrease (cid:15)0 by 0.5%;
otherwise we increase (cid:15)0 by 0.5%. This worst-case pro-
cedure ensures that (cid:15)0 is small enough to allow the chains
for all training examples to mix; simply tuning the average
acceptance rate would not guarantee this. Although this re-
sembles an adaptive MCMC algorithm (Andrieu & Thoms,
2008), note that adaptation is done between MCMC runs,
so the usual caveats and requirements of vanishing adapta-
tion do not apply.

We summarize our HMC-based approach to DLGM train-
ing in algorithm 2. Figure 2 sketches a speciﬁc training
architecture.

3.2. Evaluating held-out likelihood

A beneﬁt of variational methods over MCMC is that they
immediately provide easy-to-estimate lower bounds on the
marginal likelihood. A relatively reliable (but expensive)

Learning DLGMs with MCMC

Algorithm 2 Hamiltonian Monte Carlo for DLGMs

Input: dataset D, SGD step size schedule γ(t), mini-
batch size S, initial HMC step size (cid:15)0, number of HMC
steps M , number of iterations T .
Randomly initialize the inference network parameters φ,
the generative network parameters θ, and the shearing
matrix A.
for t = 1 to T do

Compute L = (cid:100)(cid:15)0(t)−1(cid:101).
Select a minibatch S from the dataset D.
for xs ∈ S do

Sample zs,0 ∼ N (µφ(xs), σφ(xs)).
Compute γs
φ = ∇φ log
Compute γs
A = ∇A log
Compute (cid:15)s = (cid:15)0(t)σφ(xs).
for m = 1 to M do

pθ(zs,0,xs)
N (zs,0;µφ(xs),σφ(xs)) .
pθ(zs,0,xs)
N (zs,0;µφ(xs),σφ(xs)) .

sample zs,m from HMC(zs,m−1, xs, (cid:15)s, L).

end for
Compute γs

θ = ∇θ log pθ(zs,M , xs).

end for
(cid:80)
Apply gradient update to φ using γφ = 1
s γs
φ.
S
(cid:80)
Apply gradient update to θ using γθ = 1
s γs
θ .
S
(cid:80)
Apply gradient update to A using γA = 1
s γs
A, set
S
Ak,k = 1, Ak,l = 0 for all k ∈ {1, . . . , K} and l > k.

end for

way to use MCMC to estimate marginal likelihoods is an-
nealed importance sampling (AIS; Neal, 2001), and we
follow Wu et al. (2016) in using AIS to evaluate our trained
models on held-out data. We use 20 samples per test exam-
ple, 1000 annealing steps, and the same HMC step size and
number of leapfrog steps as during training.

4. Comparison to Related Work

The idea of using MCMC to improve variational approxi-
mations has come up a number of times (e.g., De Freitas
et al., 2001; Mimno et al., 2012).

Our approach is most closely related to the Hamiltonian
variational inference (HVI) approach proposed by Sali-
mans et al. (2015). HVI also initializes an HMC algorithm
with a sample from an inference network. But HVI learns
to explicitly lower-bound the entropy of the marginal dis-
tribution of the ﬁnal sample from the MCMC chain, mak-
ing it possible to directly optimize and estimate a bound
on the ELBO obtained by using this marginal distribution
as a variational distribution. By contrast, we make no at-
tempt to estimate that ELBO, and instead optimize our vari-
ational parameters φ to maximize the much looser ELBO
deﬁned by the initial variational distribution. This is a rel-
ative strength of HVI.

Our approach also has several relative strengths over HVI.
First, it is simpler—HVI requires training an auxiliary in-
verse inference network to reverse the HMC Markov chain
that approximates the posterior. Second, to the extent that
this inverse network cannot exactly reverse the Markov
chain, the HVI lower bound on the ELBO will not be tight,
reintroducing some bias in the gradient estimates. Finally,
using HVI with Markov chains that involve multiple ac-
cept/reject steps is difﬁcult and/or expensive, since the in-
verse inference network must then learn to infer the bi-
nary accept/reject random variables. This last issue is high-
lighted by our results in section 5, where (like Wolf et al.,
2016) we ﬁnd that using multiple HMC steps is important.

(2017) proposed using
More recently, Han et al.
Metropolis-adjusted Langevin (i.e., HMC with one
leapfrog step) to ﬁt DLGMs in a stochastic EM framework.
Their approach is effective for small datasets, but it is inher-
ently a batch method that involves maintaining a Markov
chain for each example throughout training. This limits its
ability to scale to very large datasets.

There has been a ﬂurry of work in the last few years devel-
oping more and more accurate variational approximations
for VAEs (Salimans et al., 2015; Rezende & Mohamed,
2015; Burda et al., 2016; Tran et al., 2016; Sønderby et al.,
2016; Kingma & Salimans, 2016). This work has demon-
strated the beneﬁts of using accurate posterior approxima-
tions, which originally motivated us to explore using highly
accurate MCMC approximations.

5. Experiments

Below, we compare the results of our proposed method to a
baseline mean-ﬁeld VAE and to other published results on
the binarized MNIST dataset (LeCun et al., 1998). In all
experiments, we used a simple fully connected architecture
with one stochastic layer of 64 latent variables and two de-
terministic hidden layers with 1024 hidden units each and
softplus nonlinearities. The architecture is sketched in ﬁg-
ure 2. For MNIST, we used 60,000 images for training
and held out 10,000 for testing. The training images were
re-binarized each epoch to prevent overﬁtting (as done by,
e.g., Burda et al., 2016). All models were optimized us-
ing Adam (Kingma & Ba, 2015) with default parameters
and a minibatch size of 250. We trained all models for 500
epochs with a learning rate of 0.001, then for another 100
epochs with a learning rate of 0.0001.

5.1. Held-out likelihoods

We estimate held-out likelihood using annealed importance
sampling (AIS; Neal, 2001). Performance as a function of
number of steps of HMC is summarized in ﬁgure 3.

Using more HMC steps during training clearly leads to

Learning DLGMs with MCMC

Figure 3. Held-out likelihood as a function of number of HMC
steps M used during training. More computation leads to better
results, but even a few HMC steps provide a major advantage over
standard VAE training (M = 0).

Table 1. Reported held-out log-likelihoods on dynamically bina-
rized permutation-invariant MNIST, with number of stochastic
layers. HMC-DLGM-2 and HMC-DLGM-20 denote our ap-
proach with 2 and 20 HMC steps, respectively. Other approaches
are due to [1] Salimans et al. (2015), [2] Burda et al. (2016), [3]
Tran et al. (2016), [4] Sønderby et al. (2016).

Figure 2. Architecture used in the experiments. Dotted lines de-
note weight sharing. “fc (N )” denotes a dense matrix multiply
with output dimension N . “shear (N )” denotes a unit-diagonal
lower-triangular matrix multiply with output dimension N . Pa-
rameters to cyan nodes are optimized with respect to the inference
log-loss, parameters to purple nodes are optimized with respect to
the generation log-loss.

MODEL

# LAYERS ≤ log p(x)

VAE BASELINE
HVI [1]
HMC-DLGM-2
IWAE [2]
IWAE [2]
HMC-DLGM-20
VGP [3]
LVAE [4]

1
1
1
1
2
1
2
5

−90.14
−85.51
−85.15
−84.78
−82.90
−82.53
−81.90
−81.74

better models. Our best log-likelihood of −82.53 com-
pares favorably with other reported values in the litera-
ture for permutation-invariant (i.e., non-convolutional) bi-
narized MNIST models (see table 1). These number could
doubtless be improved—we did very little experimentation
with architectural choices. For example, all models in ta-
ble 1 that outperform ours (and some that don’t) use 2–5
stochastic layers, whereas we use only one.

The held-out likelihood with two HMC steps is very close
to that obtained by Salimans et al. (2015), who used only
one HMC update due to the computational difﬁculty of us-
ing many Metropolis accept/reject steps in their framework.
This suggests that the improved performance of our ap-
proach is indeed due to the use of many HMC steps (rather
than due to architecture, hyperparameters, etc.).

The per-minibatch cost of our approach goes up linearly
with M , the number of HMC steps. Training an MNIST
model with M = 20 took about 8 hours on an NVIDIA

K20 GPU, 21 times longer than with M = 11. It may be
instructive to compare this number with the cost of train-
ing an IWAE (Burda et al., 2016), which scales linearly in
the number of samples used during training—Burda et al.
(2016) used 50 samples in their experiments.

5.2. Pruning and blurriness

In this section, we evaluate some qualitative features of the
MNIST model trained with our HMC-based procedure.

Figure 4 shows some samples from the VAE and from the
DLGM trained with M = 20. The VAE samples are blurry,
while the HMC-trained model generates sharp samples.
Some of the HMC-trained model’s samples look strange,

1This is less than might be expected, given that each HMC
iteration applies several leapfrog steps (typically 2–3). The cost
of a leapfrog step is less than the cost of computing gradients in a
VAE, since we need not backprop through the inference network.

z0fc (1024)softplusfc (1024)softplusfc (D)μ(x), σ(x)softplusfc (1024)softplusxfc (1024)Normal(0, I)HMCshear (K)zMfc (1024)softplusfc (1024)softplusfc (D)shear (K)inferencelog-loss [φ, A]generationlog-loss [θ]Performance versus ComputationNumber of HMC steps MHeld-out log-likelihood-82-83-84-85-86-87-88-89-90-9105101520Learning DLGMs with MCMC

dimensions to estimate

E[x|z1:C] = (cid:82)

z(cid:48) N (z(cid:48); 0, I)E[x|z = (z1:C, z(cid:48))]dz(cid:48),

(12)

where z(cid:48) is a vector of dimension K − C, and (·, ·) denotes
concatenation. If we ﬁrst rotate z by V , then z1:C will in
a sense denote the C most-important directions in z-space,
and information that is encoded by the less-important di-
rections zC+1:K will be blurred out in E[x | z1:C].

Figure 4. Left: Samples from a VAE. Right: Samples from a
DLGM trained with HMC. More examples are in the supplement.

but many look good, which is consistent with the observa-
tion of Theis et al. (2016) that maximum-likelihood train-
ing places a relatively small penalty on generating many
bad samples (e.g., generating 95% garbage costs only 3
nats of log-likelihood).

Figure 6. Effects of keeping only the ﬁrst C most important di-
mensions of z, marginalizing out the remaining dimensions. More
examples are in the supplement.

Figure 6 displays E[x | z1:C] for various values of C for
ﬁve z vectors2 for the model trained with M = 20. Iden-
tity and broad structure is indeed encoded by the lower-
order principal components, but the images remain some-
what blurry until many higher-order components are added.
This suggests that variational pruning is at least partially re-
sponsible for some of the blurriness associated with VAEs.

6. Conclusion

We have proposed a practical approach to using HMC to
train DLGMs. This approach yields less-biased gradients
of the true marginal likelihood of the data than mean-ﬁeld
VAEs, and thereby learns better generative models.
It is
more expensive than standard VAE training, but not pro-
hibitively so.

References

Abadi, Mart´ın, Agarwal, Ashish, Barham, Paul, Brevdo,
Eugene, Chen, Zhifeng, Citro, Craig, Corrado, Gre-
gory S., Davis, Andy, Dean, Jeffrey, Devin, Matthieu,

Figure 5. Inﬂuence of latent directions on output space.

Following Krishnan & Hoffman (2016),
to determine
which directions in z-space are most and least impor-
tant, we sample 10,000 random vectors z1:10000 from their
N (0, I) marginal distribution and compute the singular
value decomposition of the average Jacobian matrix J of
the expected observation vector with respect to z:

J (cid:44) 1

10000

(cid:80)10000
i=1

∂gθ(z)
∂z

= U SV (cid:62).

(11)

(cid:12)
(cid:12)
(cid:12)zi

Figure 5 shows the singular value spectra given by the di-
agonal of S for the VAE and the HMC-trained DLGM. The
VAE’s spectrum falls off sharply at about 10, showing that
the remaining 118 dimensions are not used to explain the
data. The spectrum for the DLGM trained with HMC falls
off much more gradually.

To get a qualitative idea of what is encoded in which di-
mensions, we can marginalize out all but the ﬁrst C latent

2The z vectors are randomly sampled, but curated due to space

constraints. Uncurated examples are in the supplement.

Log-Singular-Value Spectrum of JSingular value indexSingular value10310210110010-110-2VAEHMC1002030405060C = 10C = 20C = 30C = 40C = 50C = 64Learning DLGMs with MCMC

Ghemawat, Sanjay, Goodfellow,
Ian J., Harp, An-
drew, Irving, Geoffrey, Isard, Michael, Jia, Yangqing,
J´ozefowicz, Rafal, Kaiser, Lukasz, Kudlur, Manjunath,
Levenberg, Josh, Man´e, Dan, Monga, Rajat, Moore,
Sherry, Murray, Derek Gordon, Olah, Chris, Schuster,
Mike, Shlens, Jonathon, Steiner, Benoit, Sutskever, Ilya,
Talwar, Kunal, Tucker, Paul A., Vanhoucke, Vincent,
Vasudevan, Vijay, Vi´egas, Fernanda B., Vinyals, Oriol,
Warden, Pete, Wattenberg, Martin, Wicke, Martin, Yu,
Yuan, and Zheng, Xiaoqiang. TensorFlow: Large-scale
machine learning on heterogeneous distributed systems.
arXiv preprint arXiv:1603.04467, 2016.

Andrieu, Christophe and Thoms, Johannes. A tutorial on
adaptive MCMC. Statistics and Computing, 18(4):343–
373, 2008.

Bishop, C. Pattern Recognition and Machine Learning.

Springer New York., 2006.

Blei, D., Ng, A., and Jordan, M. Latent Dirichlet allo-
cation. Journal of Machine Learning Research, 3:993–
1022, January 2003.

Bowman, Samuel R, Vilnis, Luke, Vinyals, Oriol, Dai, An-
drew M, Jozefowicz, Rafal, and Bengio, Samy. Generat-
ing sentences from a continuous space. CoNLL, pp. 10,
2016.

Burda, Yuri, Grosse, Roger, and Salakhutdinov, Ruslan.
In International

Importance weighted autoencoders.
Conference on Learning Representations, 2016.

Cover, Thomas M and Thomas, Joy A. Elements of infor-

mation theory. John Wiley & Sons, 1991.

De Freitas, Nando, Højen-Sørensen, Pedro,

Jordan,
Michael I, and Russell, Stuart. Variational MCMC.
In Proceedings of the Seventeenth conference on Un-
certainty in artiﬁcial intelligence, pp. 120–127. Morgan
Kaufmann Publishers Inc., 2001.

Dempster, A., Laird, N., and Rubin, D. Maximum likeli-
hood from incomplete data via the EM algorithm. Jour-
nal of the Royal Statistical Society, Series B, 39:1–38,
1977.

Duane, Simon, Kennedy, Anthony D, Pendleton, Brian J,
and Roweth, Duncan. Hybrid monte carlo. Physics let-
ters B, 195(2):216–222, 1987.

Gelman, Andrew and Shirley, Kenneth.

Inference from
simulations and monitoring convergence. In Handbook
of Markov chain Monte Carlo, pp. 163–174. Chapman
and Hall/CRC, 2011.

Hoffman, Matthew D. and Blei, David M. Structured
In International Con-
stochastic variational inference.
ference on Artiﬁcial Intelligence and Statistics, pp. 361–
369, 2015.

Hoffman, Matthew D. and Gelman, Andrew. The no-u-turn
sampler: adaptively setting path lengths in Hamiltonian
Monte Carlo. Journal of Machine Learning Research,
15(1):1593–1623, 2014.

Jordan, M., Ghahramani, Z., Jaakkola, T., and Saul, L. In-
troduction to variational methods for graphical models.
Machine Learning, 37:183–233, 1999.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. In International conference on
learning representations, 2015.

Kingma, Diederik P and Salimans, Tim. Improving varia-
tional inference with inverse autoregressive ﬂow. In Ad-
vances in Neural Information Processing Systems, 2016.

Kingma, D.P. and Welling, Max. Auto-encoding varia-
tional Bayes. In International Conference on Learning
Representations, 2014.

Krishnan, Rahul G and Hoffman, Matthew D.

Inference
& introspection in deep generative models of sparse
In NIPS Workshop on Advances in Approximate
data.
Bayesian Inference, 2016.

LeCun, Yann, Cortes, Corinna, and Burges, Christo-
pher JC. The mnist database of handwritten digits, 1998.

Leimkuhler, Benedict and Reich, Sebastian. Simulating
hamiltonian dynamics, volume 14. Cambridge Univer-
sity Press, 2004.

MacKay, David J. C. Local minima, symmetry-breaking,
and model pruning in variational free energy minimiza-
tion. Technical report, 2001.

Mimno, D., Hoffman, M., and Blei, D. Sparse stochastic
inference for latent Dirichlet allocation. In International
Conference on Machine Learning, 2012.

Neal, R. Probabilistic inference using Markov chain Monte
Carlo methods. Technical Report CRG-TR-93-1, De-
partment of Computer Science, University of Toronto,
1993.

Neal, Radford M. Annealed importance sampling. Statis-

tics and Computing, 11(2):125–139, 2001.

Han, Tian, Lu, Yang, Zhu, Song-Chun, and Wu, Ying Nian.
Alternating back-propagation for generator network. In
AAAI Conference on Artiﬁcial Intelligence, 2017.

Neal, Radford M. MCMC using Hamiltonian dynamics. In
Handbook of Markov Chain Monte Carlo, pp. 113–162.
Chapman and Hall/CRC, 2011.

Learning DLGMs with MCMC

Ottobre, M and Pavliotis, GA. Asymptotic analysis for
the generalized Langevin equation. Nonlinearity, 24(5):
1629, 2011.

Rezende, Danilo and Mohamed, Shakir. Variational infer-
In International Confer-

ence with normalizing ﬂows.
ence on Machine Learning, pp. 1530–1538, 2015.

Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,
Daan. Stochastic backpropagation and approximate in-
ference in deep generative models. In International Con-
ference on Machine Learning, 2014.

Salimans, Tim, Kingma, Diederik P, Welling, Max, et al.
Markov chain Monte Carlo and variational inference:
Bridging the gap. In International Conference on Ma-
chine Learning, volume 37, pp. 1218–1226, 2015.

Sønderby, Casper Kaae, Raiko, Tapani, Maaløe, Lars,
Sønderby, Søren Kaae, and Winther, Ole. Ladder varia-
tional autoencoders. In Advances in Neural Information
Processing Systems, pp. 3738–3746, 2016.

Theis, L., van den Oord, A., and Bethge, M. A note on the
evaluation of generative models. In International Con-
ference on Learning Representations, Apr 2016. URL
http://arxiv.org/abs/1511.01844.

Theis, Lucas and Hoffman, Matthew D. A trust-region
method for stochastic variational inference with appli-
cations to streaming data. In International Conference
on Machine Learning, pp. 2503–2511, 2015.

Tran, D., Ranganath, R., and Blei, D. The variational Gaus-
sian process. In International Conference on Learning
Representations, 2016.

Wolf, Christopher, Karl, Maximilian, and van der Smagt,
Patrick. Variational inference with Hamiltonian Monte
Carlo. arXiv preprint arXiv:1609.08203, 2016.

Wu, Yuhuai, Burda, Yuri, Salakhutdinov, Ruslan, and
On the quantitative analysis of
arXiv preprint

Grosse, Roger.
decoder-based generative models.
arXiv:1611.04273, 2016.

