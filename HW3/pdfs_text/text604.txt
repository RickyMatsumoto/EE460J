Breaking Locality Accelerates Block Gauss-Seidel

Stephen Tu 1 Shivaram Venkataraman 1 Ashia C. Wilson 1 Alex Gittens 2 Michael I. Jordan 1 Benjamin Recht 1

Abstract

Recent work by Nesterov and Stich (2016)
showed that momentum can be used to accel-
erate the rate of convergence for block Gauss-
Seidel in the setting where a ﬁxed partition-
ing of the coordinates is chosen ahead of time.
We show that this setting is too restrictive, con-
structing instances where breaking locality by
running non-accelerated Gauss-Seidel with ran-
domly sampled coordinates substantially outper-
forms accelerated Gauss-Seidel with any ﬁxed
partitioning. Motivated by this ﬁnding, we an-
alyze the accelerated block Gauss-Seidel algo-
rithm in the random coordinate sampling set-
ting. Our analysis captures the beneﬁt of ac-
celeration with a new data-dependent parame-
ter which is well behaved when the matrix sub-
blocks are well-conditioned. Empirically, we
show that accelerated Gauss-Seidel with random
coordinate sampling provides speedups for large
scale machine learning tasks when compared to
non-accelerated Gauss-Seidel and the classical
conjugate-gradient algorithm.

1. Introduction

The randomized Gauss-Seidel method is a commonly used
iterative algorithm to compute the solution of an n
n lin-
ear system Ax = b by updating a single coordinate at a
time in a randomized order. While this approach is known
to converge linearly to the true solution when A is positive
deﬁnite (see e.g. (Leventhal & Lewis, 2010)), in practice
it is often more efﬁcient to update a small block of coordi-
nates at a time due to the effects of cache locality.

×

In extending randomized Gauss-Seidel to the block setting,
a natural question that arises is how one should sample the
next block. At one extreme a ﬁxed partition of the coordi-

1UC Berkeley, Berkeley, California, USA 2Rensselaer Poly-
technic Institute, Troy, New York, USA. Correspondence to:
Stephen Tu <stephent@berkeley.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

nates is chosen ahead of time. The algorithm is restricted
to randomly selecting blocks from this ﬁxed partitioning,
thus favoring data locality. At the other extreme we break
locality by sampling a new set of random coordinates to
form a block at every iteration.

Theoretically, the ﬁxed partition case is well understood
both for Gauss-Seidel (Qu et al., 2015; Gower & Richt´arik,
2015) and its Nesterov accelerated variant (Nesterov &
Stich, 2016). More speciﬁcally, at most O(µ−
part log(1/ε))
iterations of Gauss-Seidel are sufﬁcient to reach a solution
with at most ε error, where µpart is a quantity which mea-
sures how well the A matrix is preconditioned by the block
diagonal matrix containing the sub-blocks corresponding
to the ﬁxed partitioning. When acceleration is used, Nes-
terov and Stich (2016) show that the rate improves to

1

O

1

n
p µ−

part log(1/ε)

, where p is the partition size.

(cid:16)q

(cid:17)

For the random coordinate selection model, the existing
literature is less complete. While it is known (Qu et al.,
2015; Gower & Richt´arik, 2015) that the iteration complex-
ity with random coordinate section is O(µ−
rand log(1/ε))
for an ε error solution, µrand is another instance dependent
quantity which is not directly comparable to µpart. Hence
it is not obvious how much better, if at all, one expects
random coordinate selection to perform compared to ﬁxed
partitioning.

1

Our ﬁrst contribution in this paper is to show that, when
compared to the random coordinate selection model, the
ﬁxed partition model can perform very poorly in terms of
iteration complexity to reach a pre-speciﬁed error. Speciﬁ-
cally, we present a family of instances (similar to the matri-
ces recently studied by Lee and Wright (2016)) where non-
accelerated Gauss-Seidel with random coordinate selection
performs arbitrarily faster than both non-accelerated and
even accelerated Gauss-Seidel, using any ﬁxed partition.
Our result thus shows the importance of the sampling strat-
egy and that acceleration cannot make up for a poor choice
of sampling distribution.

This ﬁnding motivates us to further study the beneﬁts of
acceleration under the random coordinate selection model.
the beneﬁts are more nuanced under this
Interestingly,
model. We show that acceleration improves the rate from

O(µ−

1
rand log(1/ε)) to O

1

νµ−

rand log(1/ε)

, where ν is

(cid:18)q

(cid:19)

Breaking Locality Accelerates Block Gauss-Seidel

1
a new instance dependent quantity that satisﬁes ν
rand.
µ−
We derive a bound on ν which suggests that if the sub-
blocks of A are all well conditioned, then acceleration can
provide substantial speedups. We note that this is merely a
sufﬁcient condition, and our experiments suggest that our
bound is conservative.

≤

In the process of deriving our results, we also develop a
general proof framework for randomized accelerated meth-
ods based on Wilson et al. (2016) which avoids the use of
estimate sequences in favor of an explicit Lyapunov func-
tion. Using our proof framework we are able to recover
recent results (Nesterov & Stich, 2016; Allen-Zhu et al.,
2016) on accelerated coordinate descent. Furthermore, our
proof framework allows us to immediately transfer our re-
sults on Gauss-Seidel over to the randomized accelerated
Kaczmarz algorithm, extending a recent result by Liu and
Wright (2016) on updating a single constraint at a time to
the block case.

Finally, we empirically demonstrate that despite its theoret-
ical nuances, accelerated Gauss-Seidel using random coor-
dinate selection can provide signiﬁcant speedups in prac-
tical applications over Gauss-Seidel with ﬁxed partition
sampling, as well as the classical conjugate-gradient (CG)
algorithm. As an example, for a kernel ridge regression
(KRR) task in machine learning on the augmented CIFAR-
10 dataset (n = 250, 000), acceleration with random coor-
dinate sampling performs up to 1.5
faster than accelera-
tion with a ﬁxed partitioning to reach an error tolerance of
2, with the gap substantially widening for smaller error
10−
faster than
tolerances. Furthermore, it performs over 3.5
conjugate-gradient on the same task.

×

×

2. Background

×

We assume that we are given an n
n matrix A which is
positive deﬁnite, and an n dimensional response vector b.
We also ﬁx an integer p which denotes a block size. Under
the assumption of A being positive deﬁnite, the function
xTb is strongly convex and smooth. Re-
f (x) = 1
cent analysis of Gauss-Seidel (Gower & Richt´arik, 2015)
proceeds by noting the connection between Gauss-Seidel
and (block) coordinate descent on f . This is the point of
view we will take in this paper.

2 xTAx

−

2.1. Existing rates for randomized block Gauss-Seidel

We ﬁrst describe the sketching framework of (Qu et al.,
2015; Gower & Richt´arik, 2015) and show how it yields
rates on Gauss-Seidel when blocks are chosen via a ﬁxed
partition or randomly at every iteration. While we will only
focus on the special case when the sketch matrix represents
column sampling, the sketching framework allows us to
provide a uniﬁed analysis of both cases.

D

be drawn iid from

be a distribution over Rn

p, and
To be more precise, let
If we perform block
let Sk ∼ D
coordinate descent by minimizing f along the range of Sk,
then the randomized block Gauss-Seidel update is given by
T
k ASk)†S

Sk(S

b) .

(1)

D

×

T

.

xk+1 = xk −

k (Axk −

|

|

J

⊆

2[n] with

Column sampling. Every index set J
=
p induces a sketching matrix S(J) = (eJ(1), ..., eJ(p))
where ei denotes the i-th standard basis vector in Rn,
and J(1), ..., J(p) is any ordering of the elements of J.
By equipping different probability measures on 2[n], one
can easily describe ﬁxed partition sampling as well as
random coordinate sampling (and many other sampling
schemes). The former puts uniform mass on the index sets
n
J1, ..., Jn/p, whereas the latter puts uniform mass on all
p
index sets of size p. Furthermore, in the sketching frame-
(cid:1)
work there is no limitation to use a uniform distribution,
nor is there any limitation to use a ﬁxed p for every itera-
tion. For this paper, however, we will restrict our attention
to these cases.

(cid:0)

≥

Existing rates. Under the assumptions stated above, (Qu
et al., 2015; Gower & Richt´arik, 2015) show that for every
k

0, the sequence (1) satisﬁes

E[

µ)k/2

k

x

x

≤

(1

∗kA]

xk −

∗kA ,

x0 −
k

(2)
−
where µ = λmin(E[PA1/2S]). The expectation in (2) is
taken with respect to the randomness of S0, S1, ..., and the
expectation in the deﬁnition of µ is taken with respect to
. Under both ﬁxed partitioning and random coor-
S
dinate selection, µ > 0 is guaranteed (see e.g. (Gower &
Richt´arik, 2015), Lemma 4.3). Thus, (1) achieves a linear
rate of convergence to the true solution, with the rate gov-
erned by the µ quantity shown above.

∼ D

We now specialize (2) to ﬁxed partitioning and random co-
ordinate sampling, and provide some intuition for why we
expect the latter to outperform the former in terms of it-
eration complexity. We ﬁrst consider the case when the
sampling distribution corresponds to ﬁxed partitioning. As-
sume for notational convenience that the ﬁxed partitioning
corresponds to placing the ﬁrst p coordinates in the ﬁrst
partition J1, the next p coordinates in the second partition
J2, and so on. Here, µ = µpart corresponds to a measure
of how close the product of A with the inverse of the block
diagonal is to the identity matrix, deﬁned as

A

λmin

blkdiag

µpart =

p
n
Above, AJi denotes the p
p matrix corresponding to the
sub-matrix of A indexed by the i-th partition. A loose lower
bound on µpart is

, ..., A−

1
A−
J1

1
Jn/p

(cid:17)(cid:17)

(3)

×

(cid:16)

(cid:16)

.

·

µpart ≥

p
n

λmin(A)

max1

i

≤

≤

n/p λmax(AJi )

.

(4)

Breaking Locality Accelerates Block Gauss-Seidel

On the other hand, in the random coordinate case, Qu et
al. (2015) derive a lower bound on µ = µrand as

µrand ≥

β + (1

β)

−

p
n

(cid:18)

max1

n Aii

i
≤
λmin(A)

≤

1

−

(cid:19)

,

(5)

(cid:16)

n
p

−

−

1)/(n

log(1/ε)

and random coordi-

where β = (p
1). Using the lower bounds
(4) and (5), we can upper bound the iteration com-
plexity of ﬁxed partition Gauss-Seidel Npart
by
O

max1≤i≤n/p λmax(AJi )
λmin(A)
.
nate Gauss-Seidel Nrand as O
Comparing the bound on Npart to the bound on Nrand,
(cid:17)
it is not unreasonable to expect that random coordinate
sampling has better iteration complexity than ﬁxed par-
tition sampling in certain cases.
In Section 3, we verify
this by constructing instances A such that ﬁxed partition
Gauss-Seidel
takes arbitrarily more iterations to reach
a pre-speciﬁed error tolerance compared with random
coordinate Gauss-Seidel.

max1≤i≤n Aii
(cid:17)
λmin(A)

log(1/ε)

n
p

(cid:16)

2.2. Accelerated rates for ﬁxed partition Gauss-Seidel

Based on the interpretation of Gauss-Seidel as block coor-
dinate descent on the function f , we can use Theorem 1 of
Nesterov and Stich (2016) to recover a procedure and a rate
for accelerating (1) in the ﬁxed partition case; the speciﬁc
details are discussed in the full version of the paper (Tu
et al., 2017). We will refer to this procedure as ACDM.

The convergence guarantee of the ACDM procedure is that
for all k

0,

≥

k/2

p
n

µpart

D0

.

(6)

E[

xk −
k

x

∗kA]

≤

O

1

−

x

!

r

x0 −
k

 (cid:18)
(cid:19)
Above, D0 =
∗kA and µpart is the same quan-
tity deﬁned in (3). Comparing (6) to the non-accelerated
Gauss-Seidel rate given in (2), we see that acceleration im-
proves the iteration complexity to reach a solution with ε
error from O(µ−
,
as discussed in Section 1.

part log(1/ε)) to O

part log(1/ε)

n
p µ−

1

1

(cid:16)q

(cid:17)

3. Results

We now present the main results of the paper. Proofs can
be found in the full version (Tu et al., 2017) of this paper.

3.1. Fixed partition vs random coordinate sampling

Our ﬁrst result is to construct instances where Gauss-Seidel
with ﬁxed partition sampling runs arbitrarily slower than
random coordinate sampling, even if acceleration is used.
n positive deﬁnite matrices A
with Aα,β

Consider the family of n
given by A =

Aα,β : α > 0, α + β > 0
}
{

×

1n1T

n. The family A exhibits

deﬁned as Aα,β = αI + β
n
a crucial property that ΠTAα,βΠ = Aα,β for every n
×
n permutation matrix Π. Lee and Wright (2016) recently
exploited this invariance to illustrate the behavior of cyclic
versus randomized permutations in coordinate descent.

We explore the behavior of Gauss-Seidel as the matrices
Aα,β become ill-conditioned. To do this, we consider a par-
ticular parameterization which holds the minimum eigen-
value equal to one and sends the maximum eigenvalue to
inﬁnity via the sub-family
A1,β}β>0. Our ﬁrst proposi-
{
tion characterizes the behavior of Gauss-Seidel with ﬁxed
partitions on this sub-family.

|

Ji|

Ji}
{

k
i=1 be any partition of

Proposition 3.1. Fix β > 0 and positive integers n, p, k
such that n = pk. Let
1, ..., n
}
{
p as the column selec-
= p, and denote Si ∈
with
×
Rn
p takes on value Si
tor for partition Ji. Suppose S
A we have that
with probability 1/k. For every A1,β ∈
.

µpart =

Rn

(7)

∈

×

p
n + βp

Next, we perform a similar calculation under the random
column sampling model.

Proposition 3.2. Fix β > 0 and integers n, p such that
p is cho-
1 < p < n. Suppose each column of S
∈
sen uniformly at random from
without replace-
e1, ..., en}
{
A we have that
ment. For every A1,β ∈
p
n + βp

1)βp
−
1)(n + βp)

µrand =

Rn

(8)

(n

(p

+

×

.

−

The differences between (7) and (8) are striking. Let us
assume that β is much larger than n. Then, we have that
1
1 for (8). That
µpart ≈
is, µpart can be made arbitrarily smaller than µrand as β
grows.

1/β for (7), whereas µrand ≈

−
−

p
n

Our next proposition states that the rate of Gauss-Seidel
from (2) is tight order-wise in that for any instance there
always exists a starting point which saturates the bound.

Proposition 3.3. Let A be an n
n positive deﬁnite
matrix, and let S be a random matrix such that µ =
λmin(E[PA1/2S]) > 0. Let x
denote the solution to
Rn such that
Ax = b. There exists a starting point x0 ∈
the sequence (1) satisﬁes for all k

0,

×

∗

E[

xk −
k

x

∗kA]

≥

(1

−

x0 −

k

x

∗kA .

(9)

≥
µ)k

From (2) we see that Gauss-Seidel using random co-
ordinates computes a solution xk satisfying E[
xk −
p log(1/ε)) iterations. On
∗kA1,β ]
x
the other hand, Proposition 3.3 states that for any ﬁxed par-
tition, there exists an input x0 such that k = Ω(β log(1/ε))
iterations are required to reach the same ε error tolerance.

ε in at most k = O( n

≤

k

Breaking Locality Accelerates Block Gauss-Seidel

Furthermore, the situation does not improve even if use
ACDM from (Nesterov & Stich, 2016). Proposition 3.6,
which we describe later, implies that for any ﬁxed partition
n
there exists an input x0 such that k = Ω
p β log(1/ε)
iterations are required for ACDM to reach ε error. Hence
(cid:17)
as β
, the gap between random coordinate and ﬁxed
partitioning can be made arbitrarily large. These ﬁndings
are numerically veriﬁed in Section 5.1.

−→ ∞

(cid:16)q

3.2. A Lyapunov analysis of accelerated Gauss-Seidel

and Kaczmarz

Motivated by our ﬁndings, our goal is to understand the
behavior of accelerated Gauss-Seidel under random coor-
dinate sampling. In order to do this, we establish a general
framework from which the behavior of accelerated Gauss-
Seidel with random coordinate sampling follows immedi-
ately, along with rates for accelerated randomized Kacz-
marz (Liu & Wright, 2016) and the accelerated coordinate
descent methods of (Nesterov & Stich, 2016) and (Allen-
Zhu et al., 2016).

For conciseness, we describe a simpler version of our
framework which is still able to capture both the Gauss-
Seidel and Kaczmarz results, deferring the general version
to the full version of the paper. Our general result requires
a bit more notation, but follows the same line of reasoning.

×

Let H be a random n
n positive semi-deﬁnite matrix.
Put G = E[H], and suppose that G exists and is positive
deﬁnite. Furthermore, let f : Rn
R be strongly convex
and smooth, and let µ denote the strong convexity constant
of f w.r.t. the

−→

k·kG−1 norm.

Consider the following sequence
by the recurrence

(xk, yk, zk)
{

}k

≥

0 deﬁned

zk ,

τ
1 + τ

yk +

xk+1 =

1
1 + τ
Hk∇
yk+1 = xk+1 −
zk+1 = zk + τ (xk+1 −

f (xk+1) ,
τ
µ

zk)

−

(10a)

(10b)

Hk∇

f (xk+1) ,

(10c)

where H0, H1, ... are independent realizations of H and
τ is a parameter to be chosen. Following (Wilson et al.,
2016), we construct a candidate Lyapunov function Vk for
the sequence (10) deﬁned as

Vk = f (yk)

+

f

∗

−

µ
2 k

zk −

x

2
G−1 .
∗k

(11)

The following theorem demonstrates that Vk is indeed a
Lyapunov function for (xk, yk, zk).
Theorem 3.4. Let f, G, H be as deﬁned above. Suppose
further that f has 1-Lipschitz gradients w.r.t.
k·kG−1
norm, and for every ﬁxed x

Rn,

the

f (Φ(x; H))

∈
f (x)

≤

1
2 k∇

f (x)

2
H ,
k

−

holds for a.e. H, where Φ(x; H) = x
µ/ν, with
(10) as τ =

−

∇

H

f (x). Set τ in

p

ν = λmax

E

(G−

1/2HG−

1/2)2

.

Then for every k

(cid:16)
h
0, we have

≥
E[Vk]

(1

≤

−

τ )kV0 .

i(cid:17)

We now proceed to specialize Theorem 3.4 to both the
Gauss-Seidel and Kaczmarz settings.

3.2.1. ACCELERATED GAUSS-SEIDEL

Rn

×

∈

−

2 xTAx

p denote a random sketching matrix. As sug-
Let S
xTb and put
gested in Section 2, we set f (x) = 1
H = S(STAS)†ST. Note that G = E[S(STAS)†ST] is
positive deﬁnite iff λmin(E[PA1/2S]) > 0, and is hence sat-
isﬁed for both ﬁxed partition and random coordinate sam-
pling (c.f. Section 2). Next, the fact that f is 1-Lipschitz
k·kG−1 norm and the condition (12) are standard
w.r.t. the
calculations. All the hypotheses of Theorem 3.4 are thus
satisﬁed, and the conclusion is Theorem 3.5, which charac-
terizes the rate of convergence for accelerated Gauss-Seidel
(Algorithm 1).

∈

1.

≥

p

µ/ν.

(0, 1), ν

Rn
×
Rn

n, A
≻
p, x0 ∈
×

Algorithm 1 Accelerated randomized block Gauss-Seidel.
Rn, sketching matrices
Require: A
0, b
∈
∈
Rn, µ
T
1
Sk}
−
k=0 ⊆
{
1: Set τ =
2: Set y0 = z0 = x0.
3: for k = 0, ..., T
xk+1 = 1
4:
5: Hk = Sk(ST
6:
7:
8: end for
9: Return yT .

1+τ zk.
k ASk)†ST
k .
Hk(Axk+1 −
−

yk+1 = xk+1 −
zk+1 = zk + τ (xk+1 −

b).
τ
µ Hk(Axk+1 −

1 do
−
1+τ yk + τ

zk)

b).

∈

×

Theorem 3.5. Let A be an n
n positive deﬁnite matrix
Rn denote the unique vector sat-
Rn. Let x
and b
= b. Suppose each Sk, k = 0, 1, 2, ... is an
isfying Ax
p. Put
independent copy of a random matrix S
µ = λmin(E[PA1/2S]), and suppose the distribution of S
satisﬁes µ > 0. Invoke Algorithm 1 with µ and ν, where

∗ ∈

Rn

∈

×

∗

ν = λmax

E

(G−

1/2HG−

1/2)2

,

(13)

with H = S(STAS)†ST and G = E[H]. Then with τ =

(cid:16)

h

i(cid:17)

µ/ν, for all k

0,

p

E[

yk −
k

x

≥
∗kA]

≤

√2(1

τ )k/2

−

x0 −

k

x

∗kA .

(14)

(12)

Note that in the setting of Theorem 3.5, by the deﬁnition
1/µ. Therefore,
of ν and µ, it is always the case that ν

≤

Breaking Locality Accelerates Block Gauss-Seidel

the iteration complexity of acceleration is at least as good
as the iteration complexity without acceleration.

m and µ = 1

m λmin(ATA). Hence, the above the-
ν
orem states that the iteration complexity to reach ε er-

≤

We conclude our discussion of Gauss-Seidel by describ-
ing the analogue of Proposition 3.3 for Algorithm 1, which
shows that our analysis in Theorem 3.5 is tight order-wise.
The following proposition applies to ACDM as well; we
show in the full version of the paper how ACDM can be
viewed as a special case of Algorithm 1.

Proposition 3.6. Under the setting of Theorem 3.5, there
Rn such that the iterates
exists starting positions y0, z0 ∈
(yk, zk)
{

0 produced by Algorithm 1 satisfy

≥

∗kA +

zk −
k

x

∗kA]

≥

(1

−

τ )k

y0 −

k

x

∗kA .

}k
yk −

x

E[

k

3.2.2. ACCELERATED KACZMARZ

The argument for Theorem 3.5 can be slightly modiﬁed to
yield a result for randomized accelerated Kaczmarz in the
sketching framework, for the case of a consistent overde-
termined linear system.

∗

×

satisfying Ax

n matrix A which
Speciﬁcally, suppose we are given an m
(A). Our goal is to recover
has full column rank, and b
∈ R
the unique x
= b. To do this, we apply
∗
a similar line of reasoning as (Lee & Sidford, 2013). We
set f (x) = 1
2
2 and H = PATS, where S again
∗k
is our random sketching matrix. At ﬁrst, it appears our
choice of f is problematic since we do not have access to
f , but a quick calculation shows that H
f and
f (x) =
∇
∇
(STA)†ST(Ax
b, the se-
b). Hence, with rk = Axk −
quence (10) simpliﬁes to

2 k

−

−

x

x

zk ,

yk +

xk+1 =

1
1 + τ
yk+1 = xk+1 −
zk+1 = zk + τ (xk+1 −

τ
1 + τ
T
k A)†S

(S

T
k rk+1 ,

zk)

−

τ
µ

(15a)

(15b)

(S

T
k A)†S

T
k rk+1 . (15c)

The remainder of the argument proceeds nearly identically,
and leads to the following theorem.

Rm

Theorem 3.7. Let A be an m
n matrix with full col-
×
Suppose each Sk, k =
.
umn rank, and b = Ax
∗
0, 1, 2, ... is an independent copy of a random sketch-
p. Put H = PATS and G =
ing matrix S
E[H]. The sequence (15) with µ = λmin(E[PATS]), ν =
E
µ/ν satisﬁes
λmax
for all k
(cid:0)
E[

∈
1/2HG−

, and τ =

(G−
0,

τ )k/2

√2(1

1/2)2

(16)

(cid:3)(cid:1)

x

x

×

p
∗k2 .

x0 −

k

(cid:2)
≥
yk −
k

∗k2]

≤

−

Specialized to the setting of (Liu & Wright, 2016) where
each row of A has unit norm and is sampled uniformly
at every iteration, it can be shown (Section A.5.1) that

ror is O

m
√λmin(ATA)

(cid:18)

rem 5.1 of (Liu & Wright, 2016) order-wise. However,
Theorem 3.7 applies in general for any sketching matrix.

(cid:19)

log(1/ε)

, which matches Theo-

3.3. Specializing accelerated Gauss-Seidel to random

coordinate sampling

We now instantiate Theorem 3.5 to random coordinate
sampling. The µ quantity which appears in Theorem 3.5
is identical to the quantity appearing in the rate (2) of non-
accelerated Gauss-Seidel. That is, the iteration complex-

ity to reach tolerance ε is O

1

νµ−

rand log(1/ε)

, and the

(cid:19)

(cid:18)q
only new term here is ν. In order to provide a more intu-
itive interpretation of the ν quantity, we present an upper
bound on ν in terms of an effective block condition num-
2[n], de-
ber deﬁned as follows. Given an index set J
ﬁne the effective block condition number of a matrix A as
κeﬀ,J (A) = maxi∈J Aii
κ(AJ ) al-
λmin(AJ ) . Note that κeﬀ,J (A)
ways. The following lemma gives upper and lower bounds
on the ν quantity.
Lemma 3.8. Let A be an n
let p satisfy 1 < p < n. We have that

n positive deﬁnite matrix and

⊆

×

≤

n
p ≤

ν

≤

n
p

p
n

1
1

−
−

(cid:18)

+

1

−

(cid:18)

p
n

1
1

−
−

(cid:19)

κeﬀ,p(A)

,

(cid:19)

where κeﬀ,p(A) = maxJ
=p κeﬀ,J (A), ν is deﬁned
2[n]:
in (13), and the distribution of S corresponds to uniformly
selecting p coordinates without replacement.

⊆

J

|

|

×

Lemma 3.8 states that if the p
p sub-blocks of A are
well-conditioned as deﬁned by the effective block condi-
tion number κeﬀ,J (A), then the speed-up of accelerated
Gauss-Seidel with random coordinate selection over its
non-accelerate counterpart parallels the case of ﬁxed par-
titioning sampling (i.e. the rate described in (6) versus the
rate in (2)). This is a reasonable condition, since very ill-
conditioned sub-blocks will lead to numerical instabilities
in solving the sub-problems when implementing Gauss-
Seidel. On the other hand, we note that Lemma 3.8 pro-
vides merely a sufﬁcient condition for speed-ups from ac-
celeration, and is conservative. Our numerically exper-
iments in Section A.7.2 suggest that in many cases the
ν parameter behaves closer to the lower bound n/p than
Lemma 3.8 suggests.

We can now combine Theorem 3.5 with (5) to derive the
following upper bound on the iteration complexity of ac-
celerated Gauss-Seidel with random coordinates as

Nrand,acc ≤

O

n
p s

max1

n Aii

i
≤
λmin(A)

≤

 

κeﬀ,p(A) log(1/ε)

.

!

Breaking Locality Accelerates Block Gauss-Seidel

Illustrative example. We conclude our results by illus-
trating our bounds on a simple example. Consider the sub-
family

A , with

Aδ}δ>0 ⊆

{

Aδ = An+δ,

n , δ > 0 .

−

(17)

(cid:16)

n
p

≤

n(n

−
−

A simple calculation yields that κeﬀ,p(Aδ) = n
1+δ
p+δ ,
−
n
−
1
1 + p
and hence Lemma 3.8 states that ν(Aδ)
.
1
n
Furthermore, by a similar calculation to Proposition 3.2,
(cid:17)
pδ
p+δ) . Assuming for simplicity that p =
µrand =
(0, 1), Theorem 3.5 states that at most
o(n) and δ
O( n3/2
log(1/ε)) iterations are sufﬁcient for an ε-accurate
p√δ
solution. On the other hand, without acceleration (2) states
that O( n2
pδ log(1/ε)) iterations are sufﬁcient and Proposi-
tion 3.3 shows there exists a starting position for which it is
necessary. Hence, as either n grows large or δ tends to zero,
the beneﬁts of acceleration become more pronounced.

−
∈

4. Related Work

We split the related work into two broad categories of in-
terest: (a) work related to coordinate descent (CD) methods
on convex functions and (b) randomized solvers designed
for solving consistent linear systems.

When A is positive deﬁnite, Gauss-Seidel can be inter-
preted as an instance of coordinate descent on a strongly
convex quadratic function. We therefore review related
work on both non-accelerated and accelerated coordinate
descent, focusing on the randomized setting instead of the
more classical cyclic order or Gauss-Southwell rule for se-
lecting the next coordinate. See (Tseng & Yun, 2009) for
a discussion on non-random selection rules, (Nutini et al.,
2015) for a comparison of random selection versus Gauss-
Southwell, and (Nutini et al., 2016) for efﬁcient implemen-
tations of Gauss-Southwell.

Nesterov’s original paper in (2012) ﬁrst considered ran-
domized CD on convex functions, assuming a partitioning
of coordinates ﬁxed ahead of time. The analysis included
both non-accelerated and accelerated variants for convex
functions. This work sparked a resurgence of interest in CD
methods for large problems. Most relevant to our paper are
extensions to the block setting (Richt´arik & Tak´a˘c, 2014),
handling arbitrary sampling distributions (Qu & Richt´arik,
2014a;b; Fountoulakis & Tappenden, 2016), and second or-
der updates for quadratic functions (Qu et al., 2016).

For accelerated CD, Lee and Sidford (2013) generalize the
analysis of Nesterov (2012). While the analysis of (Lee &
Sidford, 2013) was limited to selecting a single coordinate
at a time, several follow on works (Qu & Richt´arik, 2014a;
Lin et al., 2014; Lu & Xiao, 2015; Fercoq & Richt´arik,
2015) generalize to block and non-smooth settings. More
recently, both Allen-Zhu et al. (2016) and Nesterov and

Stich (2016) independently improve the results of (Lee &
Sidford, 2013) by using a different non-uniform sampling
distribution. One of the most notable aspects of the analysis
in (Allen-Zhu et al., 2016) is a departure from the (proba-
bilistic) estimate sequence framework of Nesterov. Instead,
the authors construct a valid Lyapunov function for coordi-
nate descent, although they do not explicitly mention this.
In our work, we make this Lyapunov point of view ex-
plicit. The constants in our acceleration updates arise from
a particular discretization and Lyapunov function outlined
from Wilson et al. (2016). Using this framework makes our
proof particularly transparent, and allows us to recover re-
sults for strongly convex functions from (Allen-Zhu et al.,
2016) and (Nesterov & Stich, 2016) as a special case.

From the numerical analysis side both the Gauss-Seidel and
Kaczmarz algorithm are classical methods. Strohmer and
Vershynin (2009) were the ﬁrst to prove a linear rate of
convergence for randomized Kaczmarz, and Leventhal and
Lewis (2010) provide a similar kind of analysis for ran-
domized Gauss-Seidel. Both of these were in the single
constraint/coordinate setting. The block setting was later
analyzed by Needell and Tropp (2014). More recently,
Gower and Richt´arik (2015) provide a uniﬁed analysis for
both randomized block Gauss-Seidel and Kaczmarz in the
sketching framework. We adopt this framework in this pa-
per. Finally, Liu and Wright (2016) provide an accelerated
analysis of randomized Kaczmarz once again in the single
constraint setting and we extend this to the block setting.

5. Experiments

In this section we experimentally validate our theoretical
results on how our accelerated algorithms can improve con-
vergence rates. Our experiments use a combination of
synthetic matrices and matrices from large scale machine
learning tasks.

Setup. We run all our experiments on a 4 socket Intel Xeon
CPU E7-8870 machine with 18 cores per socket and 1TB of
DRAM. We implement all our algorithms in Python using
numpy, and use the Intel MKL library with 72 OpenMP
threads for numerical operations. We report errors as rel-
2
2
ative errors, i.e.
A. Finally, we use the
A/
∗k
∗k
best values of µ and ν found by tuning each experiment.

xk −
k

x
k

x

We implement ﬁxed partitioning by creating random blocks
of coordinates at the beginning of the experiment and cache
the corresponding matrix blocks to improve performance.
For random coordinate sampling, we select a new block of
coordinates at each iteration.

For our ﬁxed partition experiments, we restrict our
attention to uniform sampling. While Gower and
Richt´arik (2015) propose a non-uniform scheme based on
Tr(STAS), for translation-invariant kernels this reduces to

Breaking Locality Accelerates Block Gauss-Seidel

Figure 1. Experiments comparing ﬁxed partitions versus random
coordinate sampling for the example from Section 3.1 with n =
5000 coordinates, block size p = 500.

Figure 2. Experiments comparing ﬁxed partitions versus uniform
random sampling for CIFAR-10 augmented matrix while running
kernel ridge regression. The matrix has n = 250000 coordinates
and we set block size to p = 10000.

uniform sampling. Furthermore, as the kernel block Lip-
schitz constants were also roughly the same, other non-
uniform schemes (Allen-Zhu et al., 2016) also reduce to
nearly uniform sampling.

5.1. Fixed partitioning vs random coordinate sampling

Our ﬁrst set of experiments numerically verify the sepa-
ration between ﬁxed partitioning sampling versus random
coordinate sampling.

∼

Figure 1 shows the progress per iteration on solving
A1,βx = b, with the A1,β deﬁned in Section 3.1. Here
N (0, I).
we set n = 5000, p = 500, β = 1000, and b
Figure 1 veriﬁes our analytical ﬁndings in Section 3.1, that
the ﬁxed partition scheme is substantially worse than uni-
form sampling on this instance. It also shows that in this
case, acceleration provides little beneﬁt in the case of ran-
dom coordinate sampling. This is because both µ and 1/ν
are order-wise p/n, and hence the rate for accelerated and
non-accelerated coordinate descent coincide. However we
note that this only applies for matrices where µ is as large
as it can be (i.e. p/n), that is instances for which Gauss-
Seidel is already converging at the optimal rate (see (Gower
& Richt´arik, 2015), Lemma 4.2).

5.2. Kernel ridge regression

We next evaluate how ﬁxed partitioning and random co-
ordinate sampling affects the performance of Gauss-Seidel
on large scale machine learning tasks. We use the popu-
lar image classiﬁcation dataset CIFAR-10 and evaluate a
kernel ridge regression (KRR) task with a Gaussian ker-
n
nel. Speciﬁcally, given a labeled dataset
i=1, we
solve the linear system (K + λI)α = Y with Kij =
2
2), where λ, γ > 0 are tunable param-
exp(
eters. The key property of KRR is that the kernel matrix K
is positive semi-deﬁnite, and hence Algorithm 1 applies.

xi −

(xi, yi)

xjk

−

γ

{

k

}

Figure 3. Comparing conjugate gradient with accelerated and un-
accelerated Gauss-Seidel methods for CIFAR-10 augmented ma-
trix while running kernel ridge regression. The matrix has n =
250000 coordinates and we set block size to p = 10000.

For the CIFAR-10 dataset, we augment the dataset1 to in-
clude ﬁve reﬂections, translations per-image and then ap-
ply standard pre-processing steps used in image classiﬁca-
tion (Coates & Ng, 2012; Sparks et al., 2017). We ﬁnally
apply a Gaussian kernel on our pre-processed images and
the resulting kernel matrix has n = 250000 coordinates.
We also include experiments on a smaller MNIST kernel
matrix (n = 60000) in Section A.7.

Results from running 500 iterations of random coordinate
sampling and ﬁxed partitioning algorithms are shown in
Figure 2. Comparing convergence across iterations, sim-
ilar to previous section, we see that un-accelerated Gauss-
Seidel with random coordinate sampling is better than ac-
celerated Gauss-Seidel with ﬁxed partitioning. However
we also see that using acceleration with random sampling
can further improve the convergence rates, especially to
achieve errors of 10−

3 or lower.

1Similar to https://github.com/akrizhevsky/cuda-convnet2.

0100200300400500Iteration10−3010−2710−2410−2110−1810−1510−1210−910−610−3100kxk−x∗k2A/kx∗k2AId+RankOne,n=5000,p=500GSFixedPartitionGS-AccFixedPartitionGSRandomCoordinatesGS-AccRandomCoordinates0100200300400500Iteration10−410−310−210−1100kxk−x∗k2A/kx∗k2ACIFAR-10KRR,n=250k,p=10kGSFixedPartitionGS-AccFixedPartitionGSRandomCoordinatesGS-AccRandomCoordinates01000200030004000500060007000Time(s)10−610−510−410−310−210−1100kxk−x∗k2A/kx∗k2ACIFAR-10KRR,n=250k,p=10kGSFixedPartitionGS-AccFixedPartitionGSRandomCoordinatesGS-AccRandomCoordinatesConjugateGradientBreaking Locality Accelerates Block Gauss-Seidel

CG. In terms of iteration complexity, standard results state
that CG takes at most O(√κ log(1/ε)) iterations to reach
an ε error solution, where κ denotes the condition num-
ber of A. On the other hand, Gauss-Seidel takes at most
O( n
In the
case of any (normalized) kernel matrix associated with a
translation-invariant kernel such as the Gaussian kernel, we
have max1
n Aii = 1, and hence generally speaking
i
≤
≤
κeﬀ is much lower than κ.

p κeﬀ log(1/ε)), where κeﬀ = max1≤i≤n Aii

λmin(A)

.

5.4. Effect of block size

We next analyze the importance of the block size p for the
accelerated Gauss-Seidel method. As the values of µ and ν
change for each setting of p, we use a smaller MNIST ma-
trix for this experiment. We apply a random feature trans-
formation (Rahimi & Recht, 2007) to generate an n
d
matrix F with d = 5000 features. We then use A = F TF
and b = F TY as inputs to the algorithm. Figure 4 shows
5 error as we vary
the wall clock time to converge to 10−
the block size from p = 50 to p = 1000.

×

Increasing the block-size improves the amount of progress
that is made per iteration but the time taken per iteration
increases as O(p3) (Line 5, Algorithm 1). However, using
efﬁcient BLAS-3 primitives usually affords a speedup from
systems techniques like cache blocking. We see the effects
of this in Figure 4 where using p = 500 performs better
than using p = 50. We also see that these beneﬁts reduce
for much larger block sizes and thus p = 1000 is slower.

6. Conclusion

In this paper, we extended the accelerated block Gauss-
Seidel algorithm beyond ﬁxed partition sampling. Our
analysis introduced a new data-dependent parameter ν
which governs the speed-up of acceleration. Specializ-
ing our theory to random coordinate sampling, we derived
an upper bound on ν which shows that well conditioned
blocks are a sufﬁcient condition to ensure speedup. Exper-
imentally, we showed that random coordinate sampling is
readily accelerated beyond what our bound suggests.

The most obvious question remains to derive a sharper
bound on the ν constant from Theorem 3.5. Another inter-
esting question is whether or not the iteration complexity
of random coordinate sampling is always bounded above
by the iteration complexity with ﬁxed coordinate sampling.

We also plan to study an implementation of accelerated
Gauss-Seidel in a distributed setting (Tu et al., 2016). The
main challenges here are in determining how to sample
coordinates without signiﬁcant communication overheads,
and to efﬁciently estimate µ and ν. To do this, we wish to
explore other sampling schemes such as shufﬂing the coor-
dinates at the end of every epoch (Recht & R´e, 2013).

Figure 4. The effect of block size on the accelerated Gauss-Seidel
method. For the MNIST dataset (pre-processed using random fea-
tures) we see that block size of p = 500 works best.

We also compare the convergence with respect to running
time in Figure 3. Fixed partitioning has better performance
in practice random access is expensive in multi-core sys-
tems. However, we see that this speedup in implementation
comes at a substantial cost in terms of convergence rate.
For example in the case of CIFAR-10, using ﬁxed parti-
2 after around 7000 sec-
tions leads to an error of 1.2
×
onds. In comparison we see that random coordinate sam-
pling achieves a similar error in around 4500 seconds and
is thus 1.5
faster. We also note that this speedup increases
for lower error tolerances.

10−

×

5.3. Comparing Gauss-Seidel to Conjugate-Gradient

We also compared Gauss-Seidel with random coordinate
sampling to the classical conjugate-gradient (CG) algo-
rithm. CG is an important baseline to compare with, as it
is the de-facto standard iterative algorithm for solving lin-
ear systems in the numerical analysis community. While
we report the results of CG without preconditioning, we
remark that the performance using a standard banded pre-
conditioner was not any better. However, for KRR specif-
ically, there have been recent efforts (Avron et al., 2017;
Rudi et al., 2017) to develop better preconditioners, and we
leave a more thorough comparison for future work. The
results of our experiment are shown in Figure 3. We note
that Gauss-Seidel both with and without acceleration out-
perform CG. As an example, we note that to reach er-
1 on CIFAR-10, CG takes roughly 7000 seconds,
ror 10−
compared to less than 2000 seconds for accelerated Gauss-
Seidel, which is a 3.5

improvement.

×

To understand this performance difference, we recall that
our matrices A are fully dense, and hence each iteration
of CG takes O(n2). On the other hand, each iteration of
both non-accelerated and accelerated Gauss-Seidel takes
O(np2 + p3). Hence, as long as p = O(n2/3), the time
per iteration of Gauss-Seidel is order-wise no worse than

0.00.51.01.52.02.53.03.54.04.5Time(s)to10−5errorp=50p=100p=200p=500p=800p=1000MNISTRandomFeatures,n=5000Breaking Locality Accelerates Block Gauss-Seidel

Acknowledgements

We thank Ross Boczar for assisting us with Mathemat-
ica support for non-commutative algebras, Orianna De-
Masi for providing useful feedback on earlier drafts of
this manuscript, and the anonymous reviewers for their
helpful feedback. ACW is supported by an NSF Gradu-
ate Research Fellowship. BR is generously supported by
ONR awards N00014-11-1-0723 and N00014-13-1-0129,
NSF award CCF-1359814, the DARPA Fundamental Lim-
its of Learning (Fun LoL) Program, a Sloan Research Fel-
lowship, and a Google Research Award. This research is
supported in part by DHS Award HSHQDC-16-3-00083,
NSF CISE Expeditions Award CCF-1139158, DOE Award
SN10040 DE-SC0012463, and DARPA XData Award
FA8750-12-2-0331, and gifts from Amazon Web Services,
Google, IBM, SAP, The Thomas and Stacey Siebel Foun-
dation, Apple Inc., Arimo, Blue Goji, Bosch, Cisco, Cray,
Cloudera, Ericsson, Facebook, Fujitsu, HP, Huawei, In-
tel, Microsoft, Mitre, Pivotal, Samsung, Schlumberger,
Splunk, State Farm and VMware.

References

Allen-Zhu, Zeyuan, Richt´arik, Peter, Qu, Zheng, and Yuan,
Yang. Even Faster Accelerated Coordinate Descent Us-
ing Non-Uniform Sampling. In ICML, 2016.

Avron, Haim, Clarkson, Kenneth L., and Woodruff,
David P. Faster Kernel Ridge Regression Using Sketch-
ing and Preconditioning. arXiv, 1611.03220, 2017.

Coates, Adam and Ng, Andrew Y. Learning Feature Rep-
resentations with K-Means. In Neural Networks: Tricks
of the Trade. Springer, 2012.

Fercoq, Olivier and Richt´arik, Peter. Accelerated, Parallel,
and Proximal Coordinate Descent. SIAM J. Optim., 25
(4), 2015.

Fountoulakis, Kimon and Tappenden, Rachael. A Flexible
Coordinate Descent Method. arXiv, 1507.03713, 2016.

Gower, Robert M. and Richt´arik, Peter. Randomized It-
erative Methods for Linear Systems. SIAM Journal on
Matrix Analysis and Applications, 36, 2015.

Conditioning. Mathematics of Operations Research, 35
(3), 2010.

Lin, Qihang, Lu, Zhaosong, and Xiao, Lin. An Accelerated
Proximal Coordinate Gradient Method. In NIPS, 2014.

Liu, Ji and Wright, Stephen J. An Accelerated Randomized
Kaczmarz Algorithm. Mathematics of Computation, 85
(297), 2016.

Lu, Zhaosong and Xiao, Lin. On the Complexity Analy-
sis of Randomized Block-Coordinate Descent Methods.
Mathematical Programming, 152(1–2), 2015.

Needell, Deanna and Tropp, Joel A. Paved with Good In-
tentions: Analysis of a Randomized Block Kaczmarz
Method. Linear Algebra and its Applications, 441, 2014.

Nesterov, Yurii. Efﬁciency of Coordinate Descent Methods
on Huge-Scale Optimization Problems. SIAM J. Optim.,
22(2), 2012.

Nesterov, Yurii and Stich, Sebastian. Efﬁciency of Ac-
celerated Coordinate Descent Method on Structured
Optimization Problems. Technical report, Universit´e
catholique de Louvain, CORE Discussion Papers, 2016.

Nutini, Julie, Schmidt, Mark, Laradji, Issam H., Friedlan-
der, Michael, and Koepke, Hoyt. Coordinate Descent
Converges Faster with the Gauss-Southwell Rule Than
Random Selection. In ICML, 2015.

Nutini, Julie, Sepehry, Behrooz, Laradji, Issam, Schmidt,
Mark, Koepke, Hoyt, and Virani, Alim. Convergence
Rates for Greedy Kaczmarz Algorithms, and Faster
Randomized Kaczmarz Rules Using the Orthogonality
Graph. In UAI, 2016.

Qu, Zheng and Richt´arik, Peter. Coordinate Descent
with Arbitrary Sampling I: Algorithms and Complexity.
arXiv, 1412.8060, 2014a.

Qu, Zheng and Richt´arik, Peter. Coordinate Descent with
Arbitrary Sampling II: Expected Separable Overapprox-
imation. arXiv, 1412.8063, 2014b.

Lee, Ching-Pei and Wright, Stephen J. Random Permuta-
tions Fix a Worst Case for Cyclic Coordinate Descent.
arXiv, 1607.08320, 2016.

Qu, Zheng, Richt´arik, Peter, and Zhang, Tong. Random-
ized Dual Coordinate Ascent with Arbitrary Sampling.
In NIPS, 2015.

Lee, Yin Tat and Sidford, Aaron. Efﬁcient Accelerated
Coordinate Descent Methods and Faster Algorithms for
Solving Linear Systems. In FOCS, 2013.

Qu, Zheng, Richt´arik, Peter, Tak´a˘c, Martin, and Fercoq,
Olivier. SDNA: Stochastic Dual Newton Ascent for Em-
pirical Risk Minimization. In ICML, 2016.

Leventhal, Dennis and Lewis, Adrian S. Randomized
Methods for Linear Constraints: Convergence Rates and

Rahimi, Ali and Recht, Benjamin. Random Features for

Large-Scale Kernel Machines. In NIPS, 2007.

Breaking Locality Accelerates Block Gauss-Seidel

Recht, Benjamin and R´e, Christopher. Parallel Stochas-
tic Gradient Algorithms for Large-Scale Matrix Com-
pletion. Mathematical Programming Computation, 5(2):
201–226, 2013.

Richt´arik, Peter and Tak´a˘c, Martin. Iteration Complexity
of Randomized Block-Coordinate Descent Methods for
Minimizing a Composite Function. Mathematical Pro-
gramming, 114, 2014.

Rudi, Alessandro, Carratino, Luigi, and Rosasco, Lorenzo.
FALKON: An Optimal Large Scale Kernel Method.
arXiv, 1705.10958, 2017.

Sparks, Evan R., Venkataraman, Shivaram, Kaftan, Tomer,
Franklin, Michael, and Recht, Benjamin. KeystoneML:
Optimizing Pipelines for Large-Scale Advanced Analyt-
ics. In ICDE, 2017.

Strohmer, Thomas and Vershynin, Roman. A Random-
ized Kaczmarz Algorithm with Exponential Conver-
gence. Journal of Fourier Analysis and Applications, 15
(1), 2009.

Tseng, Paul and Yun, Sangwoon. A Coordinate Gradient
Descent Method for Nonsmooth Separable Minimiza-
tion. Mathematical Programming, 117(1), 2009.

Tu, Stephen, Roelofs, Rebecca, Venkataraman, Shivaram,
and Recht, Benjamin. Large Scale Kernel Learning
using Block Coordinate Descent. arXiv, 1602.05310,
2016.

Tu, Stephen, Venkataraman, Shivaram, Wilson, Ashia C.,
Gittens, Alex, Jordan, Michael I., and Recht, Ben-
jamin. Breaking Locality Accelerates Block Gauss-
Seidel. arXiv, 1701.03863, 2017.

Wilson, Ashia C., Recht, Benjamin, and Jordan, Michael I.
A Lyapunov Analysis of Momentum Methods in Opti-
mization. arXiv, 1611.02635, 2016.

