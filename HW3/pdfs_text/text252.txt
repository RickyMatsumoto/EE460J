Kernelized Support Tensor Machines

Lifang He 1 Chun-Ta Lu 1 Guixiang Ma 1 Shen Wang 1 Linlin Shen 2 Philip S. Yu 1 3 Ann B. Ragin 4

Abstract

In the context of supervised tensor learning, pre-
serving the structural information and exploit-
ing the discriminative nonlinear relationships of
tensor data are crucial for improving the perfor-
mance of learning tasks. Based on tensor fac-
torization theory and kernel methods, we pro-
pose a novel Kernelized Support Tensor Ma-
chine (KSTM) which integrates kernelized ten-
sor factorization with maximum-margin crite-
rion. Speciﬁcally, the kernelized factorization
technique is introduced to approximate the ten-
sor data in kernel space such that the complex
nonlinear relationships within tensor data can be
explored. Further, dual structural preserving ker-
nels are devised to learn the nonlinear bound-
ary between tensor data. As a result of joint
optimization, the kernels obtained in KSTM ex-
hibit better generalization power to discrimina-
tive analysis. The experimental results on real-
world neuroimaging datasets show the superior-
ity of KSTM over the state-of-the-art techniques.

1. Introduction

In many real-world applications, data samples intrinsically
come in the form of two-dimensional (matrices) or multi-
dimensional arrays (tensors). In medical neuroimaging, for
instance, a functional magnetic resonance imaging (fMRI)
sample is naturally a third-order tensor consisting of 3D
voxels. There has been extensive work in supervised tensor
learning (STL) recently. For example, (Tao et al., 2007)
proposed a STL framework that extends the standard lin-
ear support vector machine (SVM) learning framework to
tensor patterns by constructing multilinear models. Under

1Department of Computer Science, University of Illinois at
Chicago, Chicago, IL, USA 2Institute for Computer Vision, Shen-
zhen University, Shenzhen, China 3Institute for Data Science,
Tsinghua University, Beijing, China 4Department of Radiology,
Northwestern University, Chicago, IL, USA. Correspondence to:
Linlin Shen <llshen@szu.edu.cn>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

this learning framework, several tensor-based linear mod-
els (Zhou et al., 2013; Hao et al., 2013) have been devel-
oped. These methods assume, explicitly or implicitly, that
data are linearly separable in the input space. However,
in practice this assumption is often violated and the linear
decision boundaries do not adequately separate the classes.

In order to apply kernel methods for tensor data, several
works (Signoretto et al., 2011; 2012; Zhao et al., 2013a)
have been presented to convert the input tensors into vec-
tors (or matrices), which are then used to construct kernels.
This kind of conversion, though, will destroy the structure
information of the tensor data. Moreover, the dimension-
ality of the resulting vector typically becomes very high,
which leads to the curse of dimensionality and small sam-
ple size problems (Lu et al., 2008; Yan et al., 2007).

Recently,
(Hao et al., 2013; He et al., 2014; Ma et al.,
2016) employed CANDECOMP/PARAFAC (CP) factor-
ization (Kolda & Bader, 2009) on the input tensor to foster
the use of kernel methods for STL problems. However, as
indicated in (Rubinov et al., 2009; Luo et al., 2011), the
underlying structure of real data is often nonlinear. Al-
though the CP factorization provides a good approximation
to the original tensor data, it only concerned with multilin-
ear formulas. Thus, it is difﬁcult to model complex non-
linear relationships within the tensor data. Most recently,
(He et al., 2017) extended CP factorization to the nonlinear
case through the exploitation of the representer theorem,
and then used kernelized CP (KCP) factorization to facil-
itate kernel learning. To the best of our knowledge, there
is no existing work that tackles factorization and prediction
as a joint optimization problem over the kernel methods.

This paper focuses on developing kernelized tensor factor-
ization with kernel maximum-margin constraint, referred
as Kernelized Support Tensor Machine (KSTM). KSTM
includes two principal ingredients. First, inspired by (Sig-
noretto et al., 2013), we introduce a general formulation
of kernelized tensor factorization in the tensor product re-
producing kernel Hilbert space, namely kernelized Tucker
model, which provides a new perspective on understand-
ing the KCP process. Second, we apply kernel trick to
embed the compact representations extracted by KCP into
the dual structure-preserving kernels (He et al., 2014) in
conjunction with a maximum-margin method to solve the

Kernelized Support Tensor Machines

STL problems. By integrating KCP and classiﬁcation as
a joint optimization problem, KSTM can beneﬁt from la-
bel information during the factorization process such that
the extracted representations from KCP are more discrim-
inative. Alternately, the kernels obtained in KSTM have
greater discriminating power and have potential to enhance
the classiﬁcation performance.

To demonstrate the effectiveness of the proposed KSTM,
we conduct experiments on real-life fMRI neuroimaging
data for classiﬁcation problems. The experimental results
show that KSTM has signiﬁcant improvements over other
related state-of-the-art classiﬁcation methods,
including
vector based, matrix unfolding based, other tensor based
kernel methods, and 3D convolutional neural networks.

Table 1. List of basic symbols.

Symbol Deﬁnition and description

x
x
X
X
[1 : M ]
vec(·)
(cid:104)·, ·(cid:105)
⊗
∗
(cid:12)
δ(·)
κ(·, ·)

each lowercase letter represents a scale
each boldface lowercase letter represents a vector
each boldface uppercase letter represents a matrix
each calligraphic letter represents a tensor, set or space
a set of integers in the range of 1 to M inclusively.
denotes column stacking operator
denotes inner product
denotes tensor product (outer product)
denotes Hadamard (element-wise) product
denotes Khatri-Rao product
denotes delta function
represents a kernel function

The remainder of the paper is organized as follows. In Sec-
tion 2, we give the notation and preliminaries. In Section 3,
we review our concerned problem. In Section 4 we present
our model and the learning algorithm. In Section 5, we con-
duct experimental analysis to justify the proposed method.
Finally, we conclude the paper in Section 6.

(cid:74)

·
(cid:75)

and
is used for shorthand. When all the factor matrices
have the same number of components, and the core tensor
is super-diagonal, Tucker model simpliﬁes to CP model. In
general, CP model is considered to be a multilinear low-
rank approximation while Tucker model is regarded as a
multilinear subspace approximation (Zhao et al., 2013a).

2. Notation and Preliminaries

In this section we introduce some preliminary knowledge
on tensor algebra (Kolda & Bader, 2009) and tensor prod-
uct reproducing kernel Hilbert space (Signoretto et al.,
2013), together with notation. Table 1 lists basic symbols
that will be used throughout the paper.

2.1. Tensor Algebra

A tensor is a multi-dimensional array that generalizes
matrix representation, whose dimension is called mode
or way. An M -th order tensor is represented as X ∈
RI1×I2×···×IM , and its element is denoted as xi1,··· ,iM .
The m-mode matricization of tensor X is denoted by
X(m) ∈ RIm×J , where J = Πm
k(cid:54)=mIk. The inner product
of two tensors X , Y ∈ RI1×···×IM is deﬁned by (cid:104)X , Y(cid:105) =
(cid:80)I1
i1=1 xi1,··· ,iM yi1,··· ,iM . The outer product of
M vectors x(m) ∈ RIm for m ∈ [1 : M ] is an M -th
order tensor and deﬁned elementwise by (cid:0)x(1) ⊗ · · · ⊗
x(M )(cid:1)
for all values of the indices.

i1=1 · · · (cid:80)IM

= x(1)
i1

· · · x(M )
iM

i1,··· ,iM

The two most commonly used factorizations are the Tucker
For a generic tensor X ∈
model and CP model.
RI1×···×IM , its Tucker factorization is deﬁned as

R1(cid:88)

RM(cid:88)

X ≈

· · ·

gr1,...,rM u(1)
r1

◦ · · · ◦ u(M )
rM

r1=1
rM =1
G; U(1), · · · , U(M )

=

,
(cid:75)
(cid:74)
where U(m) = [u(m)
, · · · , u(m)
R ] are factor matrices of
size Im × Rm, G ∈ RR1×···×RN is called the core tensor,

(1)

1

2.2. Tensor Product Reproducing Kernel Hilbert Space

For any m ∈ [1 : M ], let (Hm, (cid:104)·, ·(cid:105)m, κ(m)) be a repro-
ducing kernel Hilbert space (RKHS) of functions on a set
Xm with a reproducing kernel κ(m) : Xm × Xm → R
and the inner product operator (cid:104)·, ·(cid:105)m. The space H =
H1⊗· · ·⊗HM is called a tensor product RKHS of functions
on domain X , where X := X1×· · ·×XM . In particular, as-
sume x is the generic tuple x = (x(1), · · · , x(M )) ∈ X , let
deﬁne the tensor product space formed by the linear com-
binations of the functions f (m) for m ∈ [1 : M ] as

f (1) ⊗ · · · ⊗ f (M ) : x (cid:55)→

f (m)(x(m)), f (m) ∈ Hm.

M
(cid:89)

m=1

It holds that

(cid:88)

j

=

αj(f (1)
j1

⊗ · · · ⊗ f (M )
jM

)(x) =

f (m)
jm

(x(m))

M
(cid:89)

(cid:88)

αj

j

m=1

M
(cid:89)

(cid:88)

αj

j

m=1

(cid:104)f (m)
jm

, k(m)
x

(cid:105)m.

(2)

where j = (j1, · · · , jM ) is a multi-index, αj is the combi-
nation coefﬁcient, and k(m)
is the function k(m)(·, x(m)) :
t (cid:55)→ k(m)(t, x(m)).

x

3. Problem Formulation and Related Work

Assume we are given a collection of training samples D =
i=1, where Xi ∈ RI1×I2×···×IM is the i-th input
{Xi, yi}N
sample and yi ∈ {−1, 1} is its corresponding class label.

Kernelized Support Tensor Machines

As we have seen, Xi is represented in tensor form. To ﬁt
a classiﬁer, a commonly used approach is to stack Xi into
a vector. Let xi (cid:44) vec(Xi) ∈ RI1I2···IM . The soft margin
SVM is deﬁned as

min
w,b

1
2

N
(cid:88)

i=1

wTw + C

[1 − yi(wTxi + b)]+,

(3)

where [1 − t]+ = max(0, 1 − t)p and p is 1 or 2. When
p = 1, Problem (3) is called L1-SVM, and when p = 2,
L2-SVM. w ∈ RI1I2···IM is a vector of regression coef-
ﬁcients, b ∈ R is an offset term, and C is a pre-speciﬁed
regularization parameter.

When reshaped into vector vec(Xi), however, the correla-
tion among the tensor is ignored. It would be more reason-
able to exploit the correlation information in developing a
classiﬁer, because the correlation is useful and beneﬁcial in
improving the classiﬁcation performance. Intuitively, one
can consider the following formulation:

arg min
W,b

L(cid:0)y, (cid:104)W, X (cid:105) + b(cid:1) + P (W),

(4)

where W ∈ RI1×I2×···×IM is the tensor of regression co-
efﬁcients, L(·) is a loss function and P (W) is a penalty
function deﬁned on W. For example,

min
W,b

1
2

(cid:104)W, W(cid:105) + C

(cid:2)1 − yi

(cid:0)(cid:104)W, Xi(cid:105) + b(cid:1)(cid:3)

+.

(5)

N
(cid:88)

i=1

is

this

formulation

equiv-
However,
to Problem (3) when w = vec(W), be-
alent
cause (cid:104)W, W(cid:105) = vec(W)Tvec(W) = wTw and
(cid:104)W, Xi(cid:105) = vec(W)Tvec(Xi) = wTxi. This implies that
the formulation (5) cannot directly address our concern.

essentially

To capture the multi-way correlation, an alternative ap-
proach is to consider the dependency of the regression ten-
sor W. In particular, one can impose a low-rank constraint
on W to leverage the structure information within Xi. For
example, (Tao et al., 2007) and (Cao et al., 2014) assumed
that W ≈ w(1) ⊗ · · · ⊗ w(M ), where for m ∈ [1 : M ],
w(m) ∈ RIm. (Kotsia et al., 2012) and (Lu et al., 2017)
assumed that W ≈
. Several re-
searchers (Hao et al., 2013; Liu et al., 2015), however, have
pointed out that this type of models might end up with a
suboptimal or even worse solution since it is only an ap-
proximation of Problems (3) and (5).

W(1), · · · , W(M )

(cid:74)

(cid:75)

On the other hand, some tensor based kernel methods (Sig-
noretto et al., 2011; Zhao et al., 2013b; He et al., 2014; Guo
et al., 2014) have been proposed to solve Problem (5) in the
dual space, which takes the form:

max
β∈Q

1
2

−

βTKβ + qTβ,

(6)

where β is the vector of dual variables, Q is the domain
of β, q is a vector with qi = 1/yi, and K ∈ RN ×N is
a kernel matrix deﬁned by a kernel function κ(Xi, Xj) =
(cid:104)Φ(Xi), Φ(Xj)(cid:105). Notice that kernel function becomes the
only domain speciﬁc module of Problem (6). In this line,
it is essential to optimize kernel design and learn directly
from data. In general terms, it can be viewed as the problem
of ﬁnding a mapping on the input data:

arg min
Φ(·)

J(X , Φ(X )) + Ω(X ),

(7)

where J(·) is a certain criterion between X and Φ(X ), and
Ω(X ) is a speciﬁc constraint imposed on the priors of X .
It is worthwhile to note that Φ(·) can be implicit or explicit
depending on the learning criterion.
In the conventional
tensor based kernel methods (Signoretto et al., 2011; Zhao
et al., 2013b; He et al., 2014), Problem (7) is learned sep-
arately without considering any label information. While
it is usually beneﬁcial to utilize label information during
kernel learning.

It is desirable to consider both label information and multi-
way correlations within tensor data. Pursuing this idea, it
is natural to study the Problem (4) and the Problem (7) to-
gether. Besides, considering W can be explicitly expressed
by W = (cid:80)
i=1 βiΦ(Xi), we can make a transformation be-
tween W and Φ(X ), thus expressing J(X , Φ(X )) through
W. Based on this idea, we present the following formula-
tion:

L(cid:0)y, (cid:104)W, X (cid:105) + b(cid:1) + P (W) + Ω(X ).

(8)

arg min
W,b

Notice that as L(·) is inherently associated with W and X ,
thus all three terms in Problem (8) will be dependent on
each other.

4. Methodology

Tensor provides a natural representation for multi-way
data, but there is no guarantee that it will be effective for
learning. Learning will only be successful if the regularities
that underlie data can be captured by the learning model.
Hence, how to deﬁne Ω(X ) is critical to solve Problem (8).
In the following, we ﬁrst introduce a kernelized tensor fac-
torization to deﬁne Ω(X ). Then we propose the kernelized
support tensor machine (KSTM) to solve the whole Prob-
lem (8), followed by the learning algorithm.

4.1. Kernelized Tensor Factorization

It is well-known that tensor factorization provides a means
to capture the multi-way correlation of tensor data. How-
ever, it only gives a good approximation – rather than the
discriminative capacity. Besides, the standard factorization
is only concerned with multilinear formulas without con-
sidering the nonlinear relationships within the tensor. To

Kernelized Support Tensor Machines

overcome these issues, here we present a general formu-
lation of kernelized tensor factorization, on making use of
the notion of tensor product RKHS. In particular, given an
M -th order tensor X ∈ RI1×···×IM , we treat X as an el-
ement of the tensor product RKHS H, and assume that it
has a low-rank structure in space H, such that the following
ﬁtting criterion holds:

(cid:88)

(cid:0)xi −

(cid:88)

αj

M
(cid:89)

(cid:104)k(m)
x

, f (m)
jm

(cid:105)m

(cid:1)2

j

m=1

arg min
αj , f (m)
jm

i

= min

(cid:107)X −

G, F(m)

G; K(1)F(1), · · · , K(M )F(M )
(cid:74)

(cid:75)

(cid:107)2
F

(9)

where G ∈ RJ1×···×JM consists of the elements αj, i.e.,
αj1,··· ,jM , K(m) ∈ RIm×Im and F(m) ∈ RIm×Jm consist
of the elements k(m)

respectively.

and f (m)
jm

x

Eq. (9) can be viewed as a type of kernelized Tucker factor-
ization model, where kernel matrices K(m) deﬁned on each
mode allow to capture the nonlinear relationships within
each mode and the major discriminative features between
the modes. Speciﬁcally, when G is super-diagonal and the
size of each mode of G is the same, i.e., J1 = · · · = JM , it
reduces to the kernelized CP (KCP) factorization model in
(He et al., 2017), which can be formulated as

min
U(1),··· ,U(M )

(cid:74)

(cid:107)X −

K(1)U(1), · · · , K(M )U(M )

(cid:107)2
F . (10)
(cid:75)

Note that G is absorbed into the matrices. Moreover, it
should be noted that although Eq. (10) is a special case of
Eq. (9), they have different application scenarios that are
similar to CP and Tucker models, see e.g., (Cichocki et al.,
2015; Wang et al., 2015; Shao et al., 2015; Cao et al., 2017).

4.2. Kernelized Support Tensor Machine

To solve Problem (8), we pursue a discriminative and non-
linear factorization machine by coupling kernelized tensor
factorization with a maximum-margin classiﬁer. For sim-
plicity, we focus on the KCP model. We formulate the pri-
mal model of KSTM as follows:

capture the multi-way and nonlinear correlations within the
tensor data. On the other hand, by sharing the kernel ma-
trices K(m) for different data samples Xi, it makes KSTM
possible to characterize tensor data taking into account both
common and discriminative features.

The traditional solution for SVM classiﬁer is generally ob-
tained in the dual domain (Vapnik, 2013). However, since
the weight tensor W and the latent factor matrices U(m)
are inherently coupled in (11), it is complicated to obtain
the dual form of Problem (11). Inspired by the idea of pri-
mal optimizations of non-linear SVMs (Chapelle, 2007),
the well-known kernel trick is introduced here to implicitly
capture the non-linear structures. Therefore, we replace W
with a functional form f ( (cid:98)X ) as follows:

i

f ( (cid:98)X ) =

βiκ( (cid:98)Xi, (cid:98)X ),

(12)

N
(cid:88)

i=1

where κ(·) is a kernel function. After replacing W by
f ( (cid:98)X ), Problem (11) is transformed as follows:

min
,K(m),β,b

γ

U(m)
i

N
(cid:88)

i=1

(cid:107)Xi −

K(1)U(1)

, · · · , K(M )U(M )

i

i

(cid:107)2
F

(cid:75)

+ λ

βiβjκ( (cid:98)Xi, (cid:98)Xj)

N
(cid:88)

i,j=1

+

N
(cid:88)

i=1

(cid:2)1 − yi

(cid:0)

βjκ( (cid:98)Xi, (cid:98)Xj) + b(cid:1)(cid:3)

+,

(13)

N
(cid:88)

j=1

where λ = 1/C is the weight between the loss function
and the margin, and γ is the relative weight between the
generative and the discriminative components.

Writing the kernel matrix (cid:98)K, such that (cid:98)ki,j = κ( (cid:98)Xi, (cid:98)Xj),
and (cid:98)ki is the i-th column of (cid:98)K, we can rewrite Problem (13)
as follows:

min
,K(m),β,b

γ

U(m)
i

(cid:107)Xi −

K(1)U(1)

, · · · , K(M )U(M )

i

i

(cid:74)

(cid:74)

(cid:107)2
F

(cid:75)

(14)

N
(cid:88)

i=1

N
(cid:88)

i=1

min
,K(m),W,b

U(m)
i

N
(cid:88)

i=1

γ

(cid:124)

+ (cid:104)W, W(cid:105)
(cid:124) (cid:123)(cid:122) (cid:125)
P (W)

+ C

(cid:124)

N
(cid:88)

i=1

(cid:107)Xi −

K(1)U(1)

, · · · , K(M )U(M )

i

i

+ λβT (cid:98)Kβ +

(cid:2)1 − yi

(cid:0)

i β + b(cid:1)(cid:3)
(cid:98)kT

+.

(cid:74)

(cid:123)(cid:122)
Ω(X )

(cid:107)2
F
(cid:75)

(cid:125)

This can be regarded as the dual form of Problem (11).

(cid:2)1 − yi

(cid:0)(cid:104)W, (cid:98)Xi(cid:105) + b(cid:1)(cid:3)

(11)

,

+

(cid:125)

(cid:123)(cid:122)
L(cid:0)y, (cid:104)W, (cid:98)X (cid:105) + b(cid:1)

where γ and C are parameters to control the approxi-
mate error and prediction loss respectively, and (cid:98)Xi (cid:44)
U(1)
, which have the same size as Xi. Recall
i
(cid:74)
(cid:75)
that the principle of KCP factorization, our KSTM is able to

, · · · , U(M )

i

4.3. Learning Algorithm

Now we discuss the solution of Problem (14). As an ex-
ample, we consider the case of L2 loss, i.e., [1 − t]+ =
max(0, 1 − t)2. The objective function is non-convex,
and solving for the global minimum is difﬁcult in general.
Therefore we derive an efﬁcient iterative algorithm to reach
the local optimum, by alternatively minimizing Problem

Kernelized Support Tensor Machines

i β + b in the following.

(14) for each variable while ﬁxing the other. For the sake
of simplicity, we let (cid:98)yi (cid:44) (cid:98)kT
Update K(m) : Since there is no supervised information
involving K(m), we can utilize the original CP factoriza-
tion optimization technique to ﬁnd a solution, by solving
the following linear system of equation:

K(m)

N
(cid:88)

(cid:16)

i=1

where V(−m)
i
(cid:0)V(−m)
(cid:1)T
V(−m)
i
of the data Xi.

i

U(m)

i W(−m)

i

(cid:17)

=

(cid:0)Xi(m)V(−m)

(cid:1),

i

(15)

N
(cid:88)

i=1

= (cid:12)M

j(cid:54)=m(K(j)U(j)

i ), W(−m)

i

=

, and Xi(m) is the m-mode matricization

Update β : Similar to (Chapelle, 2007), for a given value
of the vector β, we say that a sample (cid:98)Xi is a support tensor
if yi(cid:98)yi < 1, that is, if the loss on this sample is nonzero.
After reordering the samples such that the ﬁrst Nv samples
are support tensors, the ﬁrst order gradient with respect to
β is as follows:

∇β = 2(λ (cid:98)Kβ + (cid:98)KI0( (cid:98)Kβ − y + b1)),

(16)

where y is the class label vector, 1 is a vector of all ones
of length N , and I0 is an N × N diagonal matrix with the
ﬁrst Ns diagonal entries (number of support tensors) being
1 and 0 others, i.e.,

I0 =

(cid:21)
(cid:20)INs 0
0
0

.

(17)

i

Setting Eq. (16) equal to zero, we can ﬁnd an optimal solu-
tion β.
Update U(m)
: As the kernel function is inherently cou-
pled to U(m)
, selecting an appropriate kernel function is a
crucial point for optimization design and performance en-
hancement. Following (He et al., 2014), we consider map-
ping the tensor data into tensor Hilbert space and then per-
forming the CP factorization in the Hilbert space. The map-
ping function is deﬁned as:

i

R
(cid:88)

M
(cid:89)

φ :

⊗u(m)

r →

R
(cid:88)

M
(cid:89)

(cid:16)

(cid:17)

.

u(m)
r

⊗φ

(18)

r=1

m=1

r=1

m=1

In this respect, it corresponds to mapping the latent rep-
resentations of tensor data into high-dimensional tensor
Hilbert space that retains the multi-way structure. Then the
kernel is just the standard inner product of tensors on that
space. Formally, we can derive a dual structural preserving
kernel function as follows:

(cid:16)

(cid:17)

κ

(cid:98)Xi, (cid:98)Xj

= κ

(cid:32) R
(cid:88)

M
(cid:89)

⊗u(m)
ip

,

R
(cid:88)

M
(cid:89)

(cid:33)

⊗u(m)
jq

p=1

m=1

q=1

m=1

R
(cid:88)

R
(cid:88)

M
(cid:89)

=

p=1

q=1

m=1

(cid:16)

κ

ip , u(m)
u(m)

jq

(cid:17)

.

By virtue of its derivation, we see that such a kernel func-
tion can take the multi-way structure within tensor ﬂexi-
bility into consideration. Different kernel functions spec-
ify different hypothesis spaces or even different knowledge
embeddings of the data and thus can be viewed as captur-
ing different notions of correlations. Here we consider both
linear and nonlinear cases as examples. By using the innear
product, the linear kernel can be directly derived as:

(cid:16)

κ

(cid:98)Xi, (cid:98)Xj

=

(cid:17)

R
(cid:88)

R
(cid:88)

M
(cid:89)

ip u(m)
u(m)T

jq

p=1

q=1

m=1

= 1T(cid:16) M

(cid:89)

∗(cid:0)U(m)T

i U(m)

j

(cid:1)(cid:17)

1.

(19)

m=1

For the case of Gaussian RBF kernel, it can be formulated
in the similar form

(cid:16)

κ

(cid:98)Xi, (cid:98)Xj

=

(cid:17)

R
(cid:88)

R
(cid:88)

(cid:32)

exp

−σ

p=1
q=1
= 1THij1.

M
(cid:88)

m=1

(cid:107)u(m)

ip − u(m)

jq (cid:107)2

(cid:33)

(20)

where σ is used to choose an appropriate kernel width,
and Hij ∈ RR×R is a matrix whose element hpq
ij =
exp(−σ (cid:80)M

jq (cid:107)2).
In general cases, the ﬁrst order gradient with respect to
U(m)
i

ip − u(m)

can be written as

m=1 (cid:107)u(m)

∇U(m)

i

=

∂Ω(·)
∂U(m)
i

+

∂P (·)
∂U(m)
i

+

∂L(·)
∂U(m)
i

(21)

= 2γ(K(m)TK(m)U(m)

i W(−m)

i

− K(m)Xi(m)V(−m)

)

i

with

∂Ω(·)
∂U(m)
i

∂P (·)
∂U(m)
i

∂L(·)
∂U(m)
i






N
(cid:88)

j=1

Ns(cid:88)

j=1

= 2λβi

βj

(cid:17)

(cid:16)

∂κ

(cid:98)Xi, (cid:98)Xj
∂U(m)
i

(cid:17)

(cid:16)

∂κ

(cid:98)Xi, (cid:98)Xj
∂U(m)
i
(cid:17)

= 2δ(i)

((cid:98)yj − yj)βj

+ 2βi

((cid:98)yj − yj)

Ns(cid:88)

j=1

(cid:16)

∂κ

(cid:98)Xi, (cid:98)Xj
∂U(m)
i

(22)
where δ(·) is a delta function indicating that sample is a
support tensor.

The partial gradient of U(m)
the linear kernel is given by

i with respect to κ( (cid:98)Xi, (cid:98)Xj) for

(cid:17)

(cid:16)

∂κ

(cid:98)Xi, (cid:98)Xj
∂U(m)
i

= U(m)
j

(cid:16) M
(cid:89)

k(cid:54)=m

∗(cid:0)U(k)T

i U(k)

j

(cid:1)(cid:17)

.

(23)

Kernelized Support Tensor Machines

Fixing {K(m)}, β and b, update U(m)

by Eq. (21)

i

Figure 1. (a) Visualization of an fMRI image from six angles, (b)
An illustration of third-order tensor of an fMRI image.

(cid:16)

= 2σ

U(m)

j HT

ij − U(m)

i

diag(Hij1)

.

5. Experiments and Results

Algorithm 1 Learning KSTMs
Input: Training data D, rank of factorization R, regularization

}, {K(m)}, β, b

parameters γ and λ

i

i

i

for m := 1 to M do
Fixing {U(m)

Output: Model parameters {U(m)
1: Initialize {U(m)
}, {K(m)}, β, b
2: repeat
3:
4:
5:
6:
7:
8:
9:
10:
11:
end for
12:
Fixing {U(m)
13:
14: until convergence

for m := 1 to M do

end for

i

}, update K(m) by Eq. (15)

end for
Compute kernel matrix (cid:98)K by Eq. (19) or Eq. (20)
Fixing {U(m)
for i := 1 to N do

}, {K(m)} and b, update β by Eq. (16)

i

}, {K(m)} and β, update b by Eq. (25)

For Gaussian RBF kernel, it is given by

(cid:17)

(cid:16)

∂κ

(cid:98)Xi, (cid:98)Xj
∂U(m)
i

Update b : The ﬁrst order gradient with respect to b is:

∇b = 2

((cid:98)yi − yi).

Ns(cid:88)

i=1

(cid:17)

(24)

(25)

The overall optimization process is given in Algorithm 1.

Convergence Analysis. Although we have to solve Prob-
lem (14) in an iterative process due to non-convexity, each
of subproblem is convex with respect to one variable. The
objective monotonically decreases in each iteration and it
has a lower bound (proof can be derived immediately based
on the result in (Tao et al., 2007)). Therefore, it guarantees
that we can ﬁnd the optimal solution of each iteration and
ﬁnally, Algorithm 1 can converge to a local minimum of
the objective function in (14).

m=1 Im, S2 = (cid:80)M

Computational Analysis. For brevity, we denote S1 =
m=1(Im)2, S3 = (cid:80)M
(cid:80)M
m=1(Im)3, and
π = ΠM
m=1Im. Assuming Im > R. In Algorithm 1, solv-
ing K(m) in lines 3-5 requires O(N (M RS2 + M Rπ +
R2S1 + πS1) + RS3). In line 6, computing (cid:98)K requires
O(N 2R2S1). Line 7 solves β with O(N 3). Lines 8-12
solve U(m)
taking O(N 2NsM + N 2R2M S1 + N M Rπ +
N S3 + N R3S1). Line 13 solves b with O(N Ns).

i

Classiﬁcation Rules. By solving Problem (14) on the
training data, we can obtain the shared kernel matri-
ces {K(1), · · · , K(M )}. Given a test sample X , ac-
cording to the relation between CP and KCP, we have
X =
.
(cid:75)
(cid:74)
Therefore, we ﬁrst compute CP factorization on the test

K(1)V(1), · · · , K(M )V(M )

U(1), · · · , U(M )

≈

(cid:74)

(cid:75)

sample X , and then use (cid:98)X =
as its KCP
(cid:75)
factorization, where V(m) = K(m)−1
U(m) for m ∈ [1 :
M ]. Upon solution, we can classify the test sample X by
using the test kernel matrix.

V(1), · · · , V(M )
(cid:74)

In order to empirically evaluate the effectiveness of the pro-
posed KSTM, we conduct extensive experiments on real-
life neuroimaging (fMRI) data for disease prediction and
compare with several state-of-the-art methods. In the fol-
lowing, we introduce the datasets and baselines used and
describe how we performed the experiments. Then we
present the experimental results as well as the analysis.

5.1. Data Collection and Preprocessing

In the experiments, we consider three resting-state fMRI
datasets as follows:

Alzheimer’s Disease (ADNI): This dataset is collected from
the Alzheimer’s Disease Neuroimaging Initiative1. It con-
tains the resting-state fMRI images of 33 subjects, includ-
ing patients with Mild Cognitive Impairment (MCI) or
Alzheimer’s Disease (AD), and normal controls. We ap-
plied SPM8 2 and REST3 to preprocess the data. After
this, we averaged each individual image over time domain,
resulting in 33 samples of size 61 × 73 × 61. We treat
AD+MCI as the negative class, and the normal controls as
positive class. Finally, we scaled each individual to [0, 1],
as the normalization is very important for group analyses
among different subjects. A detailed description of prepro-
cessing is available in (He et al., 2014).

Human Immunodeﬁciency Virus Infection (HIV): This
dataset is collected from Chicago Early HIV Infection
Study in Northwestern University (Wang et al., 2011),

1http://adni.loni.usc.edu/
2http://www.l.ion.ucl.ac.uk/spm/software/spm8/
3http://resting-fmri.sourceforge.net

1310754211310754211310754211310754211-mode2-mode(a)(b)Kernelized Support Tensor Machines

Table 2. Summary of compared methods. C is trade-off parameter, σ is kernel width parameter, R is the rank of tensor factorization.
KSTM
Methods
Type of Input Data
Tensor
Correlation Exploited
Kernel Explored
Nonlinear Factorization
Parameters

sKL
Matrices
One-way
Unsupervised Unsupervised Unsupervised
Unexplored
Unexplored
C, σ
C, σ

SVM / SVM+PCA
Vectors
One-way
Supervised
——
C, σ

Unsupervised Unsupervised
Unexplored
C, σ, R

Multi-way Multi-way
Supervised
Supervised
Explored
——
γ, C, σ, R
Many*

STuM
Tensor
Multi-way
——
Unexplored
C, σ, R

DuSK
Tensor
Multi-way

MMK
Tensor
Multi-way

K3rd
Vectors
One-way

FK
Matrices
One-way

Explored
C, σ, R

3D CNN
Tensor

——
C, σ

Table 3. Classiﬁcation accuracy comparison (mean ± standard
deviation)
Methods
SVM
SVM+PCA
K3rd
sKL
FK
STuM
DuSK
MMKbest
MMKcov
3D CNN
KSTM

ADNI
0.49 ± 0.02
0.50 ± 0.02
0.55 ± 0.01
0.51 ± 0.03
0.51 ± 0.02
0.52 ± 0.01
0.75 ± 0.02
0.81 ± 0.01
0.69 ± 0.01
0.52 ± 0.03
0.84 ± 0.03

HIV
0.70 ± 0.01
0.73 ± 0.03
0.75 ± 0.02
0.65 ± 0.02
0.70 ± 0.01
0.66 ± 0.01
0.74 ± 0.00
0.79 ± 0.01
0.72 ± 0.02
0.75 ± 0.02
0.82 ± 0.02

ADHD
0.58 ± 0.00
0.63 ± 0.01
0.55 ± 0.00
0.50 ± 0.04
0.50 ± 0.00
0.54 ± 0.03
0.65 ± 0.01
0.70 ± 0.01
0.66 ± 0.02
0.68 ± 0.02
0.74 ± 0.02

which contains 83 fMRI brain images of patients with early
HIV infection (negative) and normal controls (positive).
We used the same preprocessing steps as in ADNI dataset,
resulting in 83 samples of size 61 × 73 × 61.

Attention Deﬁcit Hyperactivity Disorder (ADHD): This
dataset is collected from ADHD-200 global competition
dataset4, which originally contains 776 subjects, either
ADHD patients (negative) or normal controls (positive).
Since the dataset is unbalanced, we randomly sampled 100
ADHD patients and 100 normal controls for this study. Fi-
nally, we averaged each individual over time domain, re-
sulting in 200 samples of size 58 × 49 × 47.

5.2. Baselines and Metrics

To establish a comparative study, we use the following nine
state-of-the-art methods as baselines, each representing a
different strategy.

• SVM: It is SVM with RBF kernel, which is the most
widely used vector method for classiﬁcation. In the
following methods, we use it as the classiﬁer, if not
stated explicitly.

• SVM-PCA: It is a vector-based subspace learning
algorithm, which ﬁrst uses PCA to reduce the in-
put dimension and then feeds into SVM model.
This method is commonly used to deal with high-
dimensional classiﬁcation, in particular fMRI classi-
ﬁcation (Song et al., 2011; Xie et al., 2009).

• K3rd: It is a vector based tensor kernel method, which

4http://neurobureau.projects.nitrc.org/ADHD200/

exploits the input tensor along each mode to capture
structural information and has been used to analyze
fMRI data together with RBF kernel (Park, 2011).

• sKL: It is a matrix unfolding based tensor kernel
method that deﬁned based on the symmetric Kullback-
Leibler divergence, and has been used to reconstruct
3D movement (Zhao et al., 2013b).

• FK: It is also a matrix unfolding based tensor kernel
method, but deﬁned based on multilinear SVD. The
constituent kernels are from the class of RBF kernels
(Signoretto et al., 2011).

• STuM: It is a support tucker machine approach, where
the weight tensor parameter is decomposed using the
Tucker factorization (Kotsia & Patras, 2011).

• DuSK: It is a tensor kernel method based upon CP
factorization, which has been used to analyze fMRI
data together with RBF kernel (He et al., 2014).

• MMK: It is the most recent tensor kernel method
based upon KCP factorization (He et al., 2017), which
incorporates the KCP into the DuSK. Since the shared
kernel matrices involved in KCP are hard to estimate
without a prior knowledge, we consider two schemes:
ﬁrst, we randomly generate them and select the best
result of 50 repeated times (denoted as MMKbest).
Second, we perform CP factorization for each data
sample and use the covariance matrix of each mode
as input (denoted as MMKcov).

• 3D-CNN: It is a 3D convolutional neural network ex-
tended from 2D version (Gupta et al., 2013), which
uses the convolution kernel. The convolution kernel is
the cubic ﬁlters learned from data, which has a small
receptive ﬁeld, but extends through the full depth of
the input volume.

Table 2 summarizes the compared methods. We consider
eleven methods in total and evaluate their classiﬁcation per-
formance. For the evaluation where SVM is needed, we
apply LibSVM (Chang & Lin, 2011), a widely used im-
plementation of SVM, with RBF kernel as the classiﬁer.
We perform 5-fold cross-validation and use classiﬁcation
accuracy as the evaluation measure. This process was re-
peated 50 times for all methods and the average classiﬁ-
cation accuracy of each method is reported as the result.
The optimal parameters for all methods are determined by
grid search. The optimal trade-off parameter is selected

Kernelized Support Tensor Machines

(a) ADNI

(b) HIV

(c) ADHD

Figure 2. Test accuracy vs. R on (a) ADNI, (b) HIV, and (c) ADHD, where the red triangles indicate the peak positions.

from C ∈ {2−8, 2−7, · · · , 28}, the kernel width parame-
ter is selected from σ ∈ {2−8, 2−7, · · · , 28}, the optimal
rank R is selected from {1, 2, · · · , 10}, and the regularized
factorization parameter is selected from γ ∈ {2−8, 2−7,
· · · , 28}. Note that in the proposed method λ = 1/C. The
optimal parameters for 3D-CNN, i.e., receptive ﬁeld (R),
zero-padding (P ), the input volume dimensions (Width ×
Height × Depth, or W × H × D ) and stride length (S) are
tuned based on (Gupta et al., 2013).

5.3. Classiﬁcation Performance

Experimental results in Table 3 shows classiﬁcation perfor-
mance of compared methods, where the best result is high-
lighted in bold type. We can see that the proposed KSTM
outperforms all the other methods on three datasets. This
is mainly because KSTM can learn the nonlinear relation-
ships embedded within the tensor together with consider-
ing prior knowledge across different data samples, while
the other methods fail to fully explore the nonlinear rela-
tionships or prior knowledge in the tensor object.

Speciﬁcally, the kernel methods which unfold the input
data into vectors or matrices tend to have a lower perfor-
mance compared to those who preserve the tensor struc-
ture. This indicates that unfolding tensor into vectors or
matrices would lose the multi-way structural information
within tensor, leading to the degraded performance. Be-
sides, KSTM always performs better than DuSK, which
empirically shows the effectiveness of feature extraction
in tensor data rather than approximation. Compared to
MMK method, we can see that a further improvement can
be achieved by beneﬁting from the use of label information
during the factorization procedure. These results demon-
strate the effectiveness and considerable advantages of the
proposed KSTM method for fMRI classiﬁcation.

5.4. Parameter Sensitivity

Although the optimal values of the parameters in our pro-
posed KSTM are found by grid search, it is still important
to see the sensitivity of KSTM to the rank of factorization

R. To this end, we vary R ∈ {1, 2, · · · , 10}, while the
other parameters are still selected by grid search. Figure 2
shows the variation of test accuracy over different R on
three datasets. We can observe that the rank parameter R
has a signiﬁcant effect on the test accuracy and the optimal
value of R depends on the data, while in general the opti-
mal value of R lies in the range 2 ≤ R ≤ 6, which may
provide a good guidance for selection of the R in advance.

In summary, the classiﬁcation performance of KSTM re-
lies on parameter R, which is difﬁcult to specify the op-
timal value in advance. However, in most cases the opti-
mal value of R lies in a small range of values and it is not
time-consuming to ﬁnd it using the grid search strategy in
practical applications.

6. Conclusion

In this paper, we have introduced a Kernelized Support
Tensor Machine (KSTM), with an application to neu-
roimaging classiﬁcation. Different from conventional ker-
nel methods, KSTM is based on the integration of ker-
nelized tensor factorization with kernel maximum-margin
classiﬁer. Typically this is done by deﬁning a joint opti-
mization problem, so that the kernels obtained in KSTM
have a greater discriminating power. The kernelized tensor
factorization is introduced to capture the complex nonlin-
ear relationships within tensor data, by means of the no-
tion of tensor product RKHS, which supplies a new per-
spective on tensor factorization methods. Empirical stud-
ies on three different neurological disorder prediction tasks
demonstrated the superiority of KSTM over existing state-
of-the-art tensor classiﬁcation methods.

Acknowledgements

This work is supported in part by NSFC through grants
61503253, 61672357 and 61672313, NSF through grants
IIS-1526499 and CNS-1626432, NIH through grant R01-
MH080636, and the Science Foundation of Shenzhen
through grant JCYJ20160422144110140.

246810RANK R00.20.40.60.8Accuracy246810RANK R00.20.40.60.8Accuracy246810Rank R00.20.40.60.8AccuracyKernelized Support Tensor Machines

References

Cao, Bokai, He, Lifang, Kong, Xiangnan, Yu, Philip S.,
Hao, Zhifeng, and Ragin, Ann B. Tensor-based multi-
view feature selection with applications to brain dis-
eases. In ICDM, pp. 40–49. IEEE, 2014.

Cao, Bokai, He, Lifang, Wei, Xiaokai, Xing, Mengqi, Yu,
Philip S., Klumpp, Heide, and Leow, Alex D.
t-bne:
Tensor-based brain network embedding. In SDM, 2017.

Chang, Chih-Chung and Lin, Chih-Jen. Libsvm: a library
for support vector machines. ACM Transactions on In-
telligent Systems and Technology, 2(3):27, 2011.

Chapelle, Olivier. Training a support vector machine in the
primal. Neural computation, 19(5):1155–1178, 2007.

Cichocki, Andrzej, Mandic, Danilo, De Lathauwer, Lieven,
Zhou, Guoxu, Zhao, Qibin, Caiafa, Cesar, and Phan,
Huy Anh. Tensor decompositions for signal process-
ing applications: From two-way to multiway component
analysis. IEEE Signal Processing Magazine, 32(2):145–
163, 2015.

Liu, Xiaolan, Guo, Tengjiao, He, Lifang, and Yang, Xi-
aowei. A low-rank approximation-based transductive
support tensor machine for semisupervised classiﬁca-
IEEE Transactions on Image Processing, 24(6):
tion.
1825–1838, 2015.

Lu, Chun-Ta, He, Lifang, Shao, Weixiang, Cao, Bokai,
and Yu, Philip S. Multilinear factorization machines for
multi-task multi-view learning. In WSDM, pp. 701–709.
ACM, 2017.

Lu, Haiping, Plataniotis, Konstantinos N, and Venet-
sanopoulos, Anastasios N. Mpca: Multilinear principal
IEEE Transac-
component analysis of tensor objects.
tions on Neural Networks, 19(1):18–39, 2008.

Luo, Dijun, Nie, Feiping, Huang, Heng, and Ding, Chris H.
Cauchy graph embedding. In ICML, pp. 553–560, 2011.

Ma, Guixiang, He, Lifang, Lu, Chun-Ta, Yu, Philip S.,
Shen, Linlin, and Ragin, Ann B. Spatio-temporal ten-
sor analysis for whole-brain fmri classiﬁcation. In SDM,
pp. 819–827. SIAM, 2016.

Guo, Tengjiao, Han, Le, He, Lifang, and Yang, Xiaowei.
A ga-based feature selection and parameter optimization
for linear support higher-order tensor machine. Neuro-
computing, 144:408–416, 2014.

Park, Sung Won. Multifactor analysis for fmri brain image
classiﬁcation by subject and motor task. Electrical and
computer engineering technical report, Carnegie Mellon
University, 2011.

Gupta, Ashish, Ayhan, Murat, and Maida, Anthony. Nat-
ural image bases to represent neuroimaging data.
In
ICML, pp. 987–994, 2013.

Hao, Zhifeng, He, Lifang, Chen, Bingqian, and Yang, Xi-
aowei. A linear support higher-order tensor machine for
classiﬁcation. IEEE Transactions on Image Processing,
22(7):2911–2920, 2013.

He, Lifang, Kong, Xiangnan, Yu, Philip S, Yang, Xi-
aowei, Ragin, Ann B, and Hao, Zhifeng. Dusk: A dual
structure-preserving kernel for supervised tensor learn-
ing with applications to neuroimages. In SDM, pp. 127–
135, 2014.

He, Lifang, Lu, Chun-Ta, Ding, Hao, Wang, Shen, Shen,
Linlin, Yu, Philip S., and Ragin, Ann B. Multi-way
multi-level kernel modeling for neuroimaging classiﬁca-
tion. In CVPR, 2017.

Kolda, Tamara G and Bader, Brett W. Tensor decompo-
sitions and applications. SIAM review, 51(3):455–500,
2009.

Kotsia, Irene and Patras, Ioannis. Support tucker machines.

In CVPR, pp. 633–640. IEEE, 2011.

Kotsia, Irene, Guo, Weiwei, and Patras, Ioannis. Higher
rank support tensor machines for visual recognition. Pat-
tern Recognition, 45(12):4192–4203, 2012.

Rubinov, Mikail, Knock, Stuart A, Stam, Cornelis J,
Micheloyannis, Siﬁs, Harris, Anthony WF, Williams,
Leanne M, and Breakspear, Michael. Small-world prop-
erties of nonlinear brain activity in schizophrenia. Hu-
man brain mapping, 30(2):403–416, 2009.

Shao, Weixiang, He, Lifang, and Yu, Philip S. Clustering
on multi-source incomplete data via tensor modeling and
factorization. In PAKDD, pp. 485–497. Springer, 2015.

Signoretto, Marco, De Lathauwer, Lieven, and Suykens,
Johan AK. A kernel-based framework to tensorial data
analysis. Neural networks, 24(8):861–874, 2011.

Signoretto, Marco, Olivetti, Emanuele, De Lathauwer,
Lieven, and Suykens, Johan AK. Classiﬁcation of mul-
IEEE
tichannel signals with cumulant-based kernels.
Transactions on Signal Processing, 60(5):2304–2314,
2012.

Signoretto, Marco, De Lathauwer, Lieven, and Suykens,
Learning tensors in reproducing kernel
Johan AK.
hilbert spaces with multilinear spectral penalties. arXiv
preprint arXiv:1310.4977, 2013.

Song, Sutao, Zhan, Zhichao, Long, Zhiying, Zhang, Jia-
cai, and Yao, Li. Comparative study of svm methods
combined with voxel selection for object category clas-
siﬁcation on fmri data. PloS one, 6(2):e17191, 2011.

Kernelized Support Tensor Machines

Tao, Dacheng, Li, Xuelong, Wu, Xindong, Hu, Weiming,
and Maybank, Stephen J. Supervised tensor learning.
Knowledge and Information Systems, 13(1):1–42, 2007.

Vapnik, Vladimir. The nature of statistical learning theory.

Springer science & business media, 2013.

Wang, Senzhang, He, Lifang, Stenneth, Leon, Yu, Philip S.,
and Li, Zhoujun. Citywide trafﬁc congestion estimation
with social media. In GIS, pp. 34. ACM, 2015.

Yan, Shuicheng, Xu, Dong, Yang, Qiang, Zhang, Lei, Tang,
Xiaoou, and Zhang, Hong-Jiang. Multilinear discrimi-
IEEE Transactions
nant analysis for face recognition.
on Image Processing, 16(1):212–220, 2007.

Zhao, Qibin, Zhou, Guoxu, Adalı, T¨ulay, Zhang, Liqing,
and Cichocki, Andrzej. Kernel-based tensor partial
least squares for reconstruction of limb movements. In
ICASSP, pp. 3577–3581. IEEE, 2013a.

Wang, Xue, Foryt, Paul, Ochs, Renee, Chung, Jae-Hoon,
Wu, Ying, Parrish, Todd, and Ragin, Ann B. Abnor-
malities in resting-state functional connectivity in early
human immunodeﬁciency virus infection. Brain connec-
tivity, 1(3):207–217, 2011.

Zhao, Qibin, Zhou, Guoxu, Adali, Tulay, Zhang, Liqing,
and Cichocki, Andrzej. Kernelization of tensor-based
models for multiway data analysis: Processing of mul-
tidimensional structured data. IEEE Signal Processing
Magazine, 30(4):137–148, 2013b.

Xie, Song-yun, Guo, Rong, Li, Ning-fei, Wang, Ge, and
Zhao, Hai-tao. Brain fmri processing and classiﬁcation
In IJCNN, pp.
based on combination of pca and svm.
3384–3389. IEEE, 2009.

Zhou, Hua, Li, Lexin, and Zhu, Hongtu. Tensor regression
with applications in neuroimaging data analysis. Journal
of the American Statistical Association, 108(502):540–
552, 2013.

