Bidirectional Learning for Time-series Models with Hidden Units

A. Supplementary Materials for Bidirectional Learning for Time-series Models with Hidden

Units

Here, we derive speciﬁc learning rules suggested by (27)-(28) as well as those with approximation with (29). These learning
rule can be derived in a way similar to the learning rules (18)-(22) are derived from (17). We also provide some of the
details, which are omitted in the derivation of (18)-(22).

The learning rules for U and Z are derived from (27)-(28) as follows:

U[d] ← U[d] + η log pθ(x[t]|x[<t], h[<t])

α[s−1] (h[s] − (cid:104)H[s](cid:105)φ)(cid:62)

Z[d] ← Z[d] + η log pθ(x[t]|x[<t], h[<t])

β[s−1] (h[s] − (cid:104)H[s](cid:105)φ)(cid:62)

U[δ] ← U[δ] + η log pθ(x[t]|x[<t], h[<t])

x[s−δ] (h[s] − (cid:104)H[s](cid:105)φ)(cid:62)

Z[δ] ← Z[δ] + η log pθ(x[t]|x[<t], h[<t])

h[s−δ] (h[s] − (cid:104)H[s](cid:105)φ)(cid:62)

t−1
(cid:88)

s=(cid:96)

t−1
(cid:88)

s=(cid:96)

t−1
(cid:88)

s=(cid:96)

t−1
(cid:88)

s=(cid:96)

for 1 ≤ δ < d, where (cid:104)H[s](cid:105)φ denotes the expected values of h[s] with respect to the conditional distribution given by the
following pφ:

pφ(h[s]|x[<s], h[<s]) =

1
Z (cid:48) exp(−Eφ(h[s]|x[<s], h[<s]))

for any binary vectors h[s], where Z (cid:48) is a normalization factor for the probabilities to sum up to one, and

Eφ(h[s]|x[<s], h[<s]) = −

(x[s−δ])(cid:62)U[δ] h[s] −

(h[s−δ])(cid:62)Z[δ] h[s] − (α[s−1])(cid:62) U[d]h[s] − (β[s−1])(cid:62) Z[d]h[s].

The energy in (37) can be decomposed into the energy associated with each hidden unit j as follows:

Eφ(h[s]|x[<s], h[<s]) =

Eφ,j(h[s]

j |x[<s], h[<s])

where H denotes the set of hidden units, and

Eφ,j(h[s]

j |x[<s], h[<s]) = −

(x[s−δ])(cid:62)U[δ]

:,j h[s]

j −

(h[s−δ])(cid:62)Z[δ]

:,j h[s]

j − (α[s−1])(cid:62)U[d]

:,j h[s]

j − (β[s−1])(cid:62)Z[d]

:,j h[s]
j ,

d−1
(cid:88)

δ=1

d−1
(cid:88)

δ=1

where U:,j denotes a column vector corresponding to the j-th column of U, and Z:,j is deﬁned analogously.

Then (36) can be expressed as

where

pφ(h[s]|x[<s], h[<s]) =

pφ,j(h[s]

j |x[<s], h[<s]),

pφ,j(h[s]

j |x[<s], h[<s]) =

exp(−Eφ,j(h[s]

j |x[<s], h[<s]))

exp(−Eφ,j(h[s]

j = 0|x[<s], h[<s])) + exp(−Eφ,j(h[s]

j = 1|x[<s], h[<s]))

=

exp(−Eφ,j(h[s]
1 + exp(−Eφ,j(h[s]

j |x[<s], h[<s]))
j = 1|x[<s], h[<s]))

.

d−1
(cid:88)

δ=1

d−1
(cid:88)

δ=1

(cid:88)

j∈H

(cid:89)

j∈H

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

Bidirectional Learning for Time-series Models with Hidden Units

The j-th element of (cid:104)H[s](cid:105)φ is then given by

(cid:104)H [s]

j (cid:105)φ = pφ,j(h[s]

j = 1|x[<s], h[<s])

In (32)-(35), the value of (cid:104)H[s](cid:105)φ is computed with the latest values of φ. Let φ[t−1] be the value of φ immediately before
step t. With the recursive computation of (29), the learning rules of (32)-(35) are approximated with the following learning
rules:

U[d] ← U[d] + η (1 − γ) log pθ(x[t]|x[<t], h[<t])

γt−1−s α[s−1] (h[s] − (cid:104)H[s](cid:105)φ[s−1])(cid:62)

Z[d] ← Z[d] + η (1 − γ) log pθ(x[t]|x[<t], h[<t])

γt−1−s β[s−1] (h[s] − (cid:104)H[s](cid:105)φ[s−1])(cid:62)

U[δ] ← U[δ] + η (1 − γ) log pθ(x[t]|x[<t], h[<t])

γt−1−s x[s−δ] (h[s] − (cid:104)H[s](cid:105)φ[s−1])(cid:62)

Z[δ] ← Z[δ] + η (1 − γ) log pθ(x[t]|x[<t], h[<t])

γt−1−s h[s−δ] (h[s] − (cid:104)H[s](cid:105)φ[s−1])(cid:62)

t−1
(cid:88)

s=(cid:96)

t−1
(cid:88)

s=(cid:96)

t−1
(cid:88)

s=(cid:96)

t−1
(cid:88)

s=(cid:96)

for 1 ≤ δ < d, where the quantity such as

can be computed recursively as

G(cid:48)

t−1 ≡

γt−1−s α[s−1] (h[s] − (cid:104)H[s](cid:105)φ[s−1])(cid:62)

t−1
(cid:88)

s=(cid:96)

G(cid:48)

t ← γ G(cid:48)

t−1 + (1 − γ) α[t−1] (h[t] − (cid:104)H[t](cid:105)φ[t−1])(cid:62).

One may consider real-valued units as well (Dasgupta & Osogami, 2017; Osogami, 2016). For example, each of x[t]
h[t]
j may have a Gaussian distribution with the following density:

i and

pθ,i(x[t]

i |x[<t], h[<t]) =

exp

−

pφ,i(h[t]

j |x[<t], h[<t]) =

exp

−

1
(cid:112)2 π σ2

i

1

(cid:113)

2 π σ2
j

(cid:18)

(cid:18)

(cid:0)x[t]

i − Eθ,i(x[t]

(cid:0)h[t]

j − Eφ,j(h[t]

i = 1|x[<t], h[<t])(cid:1)2
2 σ2
i
j = 1|x[<t], h[<t])(cid:1)2
2 σ2
j

(cid:19)

(cid:19)
,

where σ2

i and σ2

j are variance parameters, Eφ,j is given by (39), and Eθ,i(x[t]

i = 1|x[<t], h[<t]) is given by

Eθ,i(x[s]

i = 1|x[<s], h[<s]) = −bi −

(x[s−δ])(cid:62)W[δ]

:,i −

(h[s−δ])(cid:62)V[δ]

:,i − (α[s−1])(cid:62)W[d]

:,i − (β[s−1])(cid:62)V[d]
:,i .

(52)

d−1
(cid:88)

δ=1

d−1
(cid:88)

δ=1

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

