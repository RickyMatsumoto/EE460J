Uncorrelation and Evenness: a New Diversity-Promoting Regularizer

Pengtao Xie 1 2 Aarti Singh 1 Eric P. Xing 2

Abstract
Latent space models (LSMs) provide a princi-
pled and effective way to extract hidden patterns
from observed data. To cope with two challenges
(1) how to capture infrequent pat-
in LSMs:
terns when pattern frequency is imbalanced and
(2) how to reduce model size without sacriﬁcing
their expressiveness, several studies have been
proposed to “diversify” LSMs, which design reg-
ularizers to encourage the components therein to
be “diverse”.
In light of the limitations of ex-
isting approaches, we design a new diversity-
promoting regularizer by considering two fac-
tors: uncorrelation and evenness, which encour-
age the components to be uncorrelated and to
play equally important roles in modeling data.
Formally, this amounts to encouraging the co-
variance matrix of the components to have more
uniform eigenvalues. We apply the regularizer
to two LSMs and develop an efﬁcient optimiza-
tion algorithm. Experiments on healthcare, im-
age and text data demonstrate the effectiveness
of the regularizer.

1. Introduction

A fundamental task in machine learning (ML) is to discover
latent patterns underlying data, for instance, extracting top-
ics from documents and communities from social networks.
Latent space models (Bishop, 1998; Knott & Bartholomew,
1999; Blei, 2014) are effective tools to accomplish this
task. An LSM contains a collection of learnable compo-
nents such as hidden units in neural networks and factors in
factor analysis (Harman, 1960). Each component is aimed
at capturing a hidden pattern. In most LSMs, components
are parameterized by vectors.

Among the many challenges encountered in latent space
modeling, two of them are of particular interest to us.

1Machine Learning Department, Carnegie Mellon Univer-
sity 2Petuum Inc. Correspondence to: Pengtao Xie <peng-
taox@cs.cmu.edu>, Eric P. Xing <eric.xing@petuum.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

First, under many circumstances, the frequency of patterns
is highly imbalanced. Some patterns have very high fre-
quency while others occur less frequently. As a typical ex-
ample, in a news corpus, politics and economics are fre-
quent topics (patterns) while furniture and gardening are
infrequent. Classic LSMs are sensitive to the skewness
of pattern frequency and less capable of capturing the in-
frequent patterns (Wang et al., 2014). Second, when us-
ing LSMs, one needs to carefully balance the tradeoff be-
tween model size (precisely, the number of components)
and modeling power (Xie, 2015). Larger-sized LSMs are
more expressive, but incur higher computational complex-
ity. It is desirable but challenging to achieve sufﬁcient mod-
eling power with a small number of components.

To address these two challenges, recent studies (Zou &
Adams, 2012; Cogswell et al., 2015; Xie et al., 2015; 2016)
investigate a “diversiﬁcation” strategy which encourages
the components in LSMs to be mutually different, either
through frequentist-style regularization (Zou & Adams,
2012; Cogswell et al., 2015; Xie et al., 2015) or Bayesian
(1)
learning (Xie et al., 2016). They conjecture that:
through “diversiﬁcation”, some components that are orig-
inally aggregated around frequent patterns can be pushed
apart to cover infrequent patterns; (2) “diversiﬁed” compo-
nents bear less redundancy and are mutually complemen-
tary; a small number of such components are sufﬁcient to
model data well.

Along this line of research, several diversity-promoting
regularizers have been proposed, based upon determinan-
tal point process (Kulesza & Taskar, 2012; Zou & Adams,
2012), cosine similarity (Yu et al., 2011; Bao et al., 2013;
Xie et al., 2015) and covariance (Malkin & Bilmes, 2008;
Cogswell et al., 2015). While these regularizers demon-
strate notable efﬁcacy, they have certain limitations, such
as sensitivity to vector scaling (Zou & Adams, 2012;
Malkin & Bilmes, 2008), inability to measure diversity
in a global manner (Yu et al., 2011; Bao et al., 2013;
Xie et al., 2015) and computational inefﬁciency (Cogswell
et al., 2015). To address these limitations, we propose
a new diversity-promoting regularizer gaining inspiration
from principal component analysis (Jolliffe, 2002), bio-
logical diversity (Magurran, 2013) and information theory
(Cover & Thomas, 2012).

Uncorrelation and Evenness: a New Diversity-Promoting Regularizer

We characterize “diversity” by considering two factors:
uncorrelation and evenness. Uncorrelation (Cogswell
et al., 2015) encourages the components to be uncorre-
lated, such that each component can independently capture
a unique pattern. Evenness is inspired from biological di-
versity (Magurran, 2013) where an ecosystem is deemed
to be more diverse if different species contribute equally to
the maintenance of biological balance. Analogously, when
measuring component diversity, we assign an “importance”
score to each component and encourage these scores to be
even.
In the context of latent space modeling, evenness
ensures each component plays a signiﬁcant role in pattern
discovery rather than being dominated by others.

We study uncorrelation and evenness from a statistical per-
spective. The components are considered as random vari-
ables and the eigenvalues of their covariance matrix can
be leveraged to characterize these two factors. First, ac-
cording to Principle Component Analysis (Jolliffe, 2002),
the disparity of eigenvalues reﬂects the correlation among
components:
the more uniform the eigenvalues, the less
correlated the components. Second, eigenvalues represent
the variance along principal directions and can be used to
measure the “importance” of components. Promoting uni-
form importance amounts to encouraging evenness among
eigenvalues.

To promote uniformity among the eigenvalues, we encour-
age the discrete distribution parametrized by the normal-
ized eigenvalues to have small Kullback-Leibler divergence
with the uniform distribution, based on which, we deﬁne
a uniform eigenvalue regularizer (UER) and make a con-
nection with the von Neumann entropy (Bengtsson & Zy-
czkowski, 2007) and with the von Neumann divergence
(Kulis et al., 2009). We apply UER to two LSMs – distance
metric learning (DML) (Xing et al., 2002) and long short-
term memory (LSTM) network (Hochreiter & Schmidhu-
ber, 1997) – to encourage their components to be diverse
and develop an efﬁcient optimization algorithm. Experi-
ments on healthcare, image and text data demonstrate that
UER (1) greatly improves the performance of LSMs; (2)
better captures infrequent patterns; (3) reduces model size
without sacriﬁcing modeling power; (4) outperforms other
diversity-promoting regularizers.

The major contributions of this paper are:

• We propose a new diversity-promoting regularizer
from the perspectives of uncorrelation and evenness.
• We propose to simultaneously promote uncorrelation
and evenness by encouraging uniformity among the
eigenvalues of the covariance matrix of components.
• We develop an efﬁcient projected gradient descent al-

gorithm to solve UE regularized LSM problems.

• In experiments, we demonstrate the effectiveness of

this regularizer on two LSMs: DML and LSTM.

The rest of the paper is organized as follows. Section 2
reviews related works. Section 3 introduces the uniform
eigenvalue regularizer. Section 4 presents experimental re-
sults and Section 5 concludes the paper.

2. Related Works

Diversity promoting regularization has been widely used
in classiﬁcation (Malkin & Bilmes, 2008), ensemble learn-
ing (Yu et al., 2011) and latent space modeling (Zou &
In the sequel,
Adams, 2012; Xie et al., 2015; 2017).
we present a brief review of existing diversity-promoting
regularizers. Several regularizers (Yu et al., 2011; Bao
et al., 2013; Xie et al., 2015; 2017) are based on pair-
if every two compo-
wise dissimilarity of components:
nents are dissimilar, then overall the set of components
are “diverse”. Given the weight vectors {aj}m
j=1 of m
components, Yu et al. (2011) deﬁne the regularizer as
(cid:80)
1≤j<k≤m(1−cjk), where cjk is the cosine similarity be-
tween component j and k. In (Bao et al., 2013), the score
1
is deﬁned as − log(
β where
β > 0. In (Xie et al., 2015), the score is deﬁned as mean
of {arccos(|cjk|)} minus the variance of {arccos(|cjk|)},
where the variance term is utilized to encourage the dissim-
ilarity scores {arccos(|cjk|)} to be even. Xie et al. (2017)
deﬁne the regularizer as (cid:80)
1≤i<j≤m k(ai, aj) where k(·, ·)
is a kernel function. These regularizers are applied to clas-
siﬁers ensemble, neural network and restricted Boltzmann
machine. While these regularizers can capture pairwise
dissimilarities between components, they are unable to cap-
ture higher-order “diversity”.

1≤j<k≤m β|cjk|)

1
m(m−1)

(cid:80)

Determinantal Point Process (DPP) (Kulesza & Taskar,
2012) was used by (Zou & Adams, 2012; Mariet & Sra,
2015) to encourage the topic vectors in Latent Dirichlet
Allocation (Blei et al., 2003), Gaussian mean vectors in
Gaussian Mixture Model and hidden units in neural net-
work to be “diverse”. The DPP regularizer is deﬁned as
− log det(L), where L is a m × m kernel matrix and
det(·) denotes the determinant of the matrix. Lij equals
to k(ai, aj) and k(·, ·) is a kernel function. In geometry,
det(L) is the volume of the parallelepiped formed by vec-
tors in the feature space associated with kernel k. Vectors
that result in a larger volume are considered to be more
“diverse”. Since volume depends on all vectors simulta-
neously, DPP is able to measure diversity in a global way.
The drawback of DPP lies in its sensitivity to the scaling of
vectors. The volume increases with the (cid:96)2 norm of vectors,
but “diversity” does not. Malkin & Bilmes (2008) propose
to promote diversity by maximizing the determinant of vec-
tors’ covariance matrix. Similar to DPP, this regularizer is
sensitive to vector scaling.

Unlike the aforementioned regularizers which are deﬁned
directly on weight vectors, Cogswell et al. (2015) design a

Uncorrelation and Evenness: a New Diversity-Promoting Regularizer

Figure 1. Two views of the component matrix

regularizer on hidden activations in the neural network and
inﬂuence the parameters indirectly. The number of hidden
activations could be much larger than that of weight param-
eters (like in a convolutional neural network), which may
render this regularizer to be computationally inefﬁcient.

3. Method

In this section, we develop a uniform eigenvalue regularizer
and apply it to promote “diversity” in two LSMs.

3.1. Uniform Eigenvalue Regularizer

A latent space model (LSM) is equipped with a set of m
components and each component is represented with a vec-
tor a ∈ Rd. To achieve broader coverage of infrequent pat-
terns and reduce model size without sacriﬁcing modeling
power, previous works (Zou & Adams, 2012; Xie et al.,
2015) propose to “diversify” the components by imposing
a regularizer over them.

As a subjective concept, “diversity” has been deﬁned in
various ways as reviewed in Section 2. In this paper, we
deﬁne a new measure of “diversity” by taking two factors
into consideration: uncorrelation and evenness. Uncorrela-
tion is a measure of how uncorrelated the components are.
Literally, less correlation is equivalent to more diversity.
Evenness is borrowed from biological diversity (Magurran,
2013), which measures how equally important different
species are in maintaining the ecological balance within an
ecosystem. If no species dominates another, the ecosystem
is deemed as more diverse. Likewise, in latent space mod-
eling, we desire the components to play equally important
roles and no one dominates another, such that each compo-
nent contributes signiﬁcantly to the modeling of data.

We characterize the uncorrelation among components from
a statistical perspective: treating the components as random
variables and measuring their covariance which is propor-
tional to their correlation. Let A ∈ Rd×m denote the com-
ponent matrix where in the k-th column is the parameter
vector ak of component k. Alternatively, we can take a
row view (Figure 1(b)) of A: each component is treated as
a random variable and each row vector ˜a(cid:62)
i can be seen as
a sample drawn from the random vector formed by the m
components. Let µ = 1
d A(cid:62)1 be the sample
d
mean, where the elements of 1 ∈ Rd are all 1. We compute
the empirical covariance matrix of the components as

i=1 ˜ai = 1

(cid:80)d

G = 1
d
= 1

(cid:80)d
d A(cid:62)A − ( 1

i=1(˜ai − µ)(˜ai − µ)(cid:62)
d A(cid:62)1)( 1

d A(cid:62)1)(cid:62)

(1)

Figure 2. When the principal directions (u1 and u2) are not
aligned with the coordinate axis, the level of disparity between the
eigenvalues (λ1 and λ2) indicates the correlation between random
variables (components).

Figure 3. When the principal directions (u1 and u2) are aligned
with the coordinate axis, the magnitude of eigenvalues represents
the importance of components.

Imposing the constraint A(cid:62)1 = 0, we have G = 1
d A(cid:62)A.
Suppose A is a full rank matrix and m < d, then G is a
full-rank matrix with rank m.

k=1 λkuku(cid:62)

For the next step, we show that the eigenvalues of G
play important roles in characterizing the uncorrelation and
evenness of components. We start with uncorrelation. Let
G = (cid:80)m
k be the eigendecomposition where λk
is an eigenvalue and uk is the associated eigenvector. As
is well known in Principle Component Analysis (Jolliffe,
2002), an eigenvector uk of the covariance matrix G rep-
resents a principal direction of the data points and the as-
sociated eigenvalue λk tells the variability of points along
that direction. As shown in Figure 2(a), the larger λk is, the
more spread out the points along the direction uk. When
the eigenvectors (principal directions) are not aligned with
coordinate axis (as shown in Figure 2), the level of disparity
among eigenvalues indicates the level of correlation among
the m components (random variables). The more different
the eigenvalues are, the higher the correlation is. As shown
in Figure 2(a), λ1 is about three times larger than λ2 and
there is a high correlation along the direction u1. On the
other hand, in Figure 2(b), the two eigenvalues are close to
each other and the points evenly spread out in both direc-
tions with negligible correlation. In light of this, we would
utilize the uniformity among eigenvalues of G to measure
how uncorrelated the components are.

Secondly, we relate the eigenvalues with the other factor
of diversity: evenness. When the eigenvectors are aligned
with the coordinate axis (as shown in Figure 3(a)), the com-
ponents are uncorrelated. In this case, we bring in evenness
to measure diversity. As stated earlier, we ﬁrst need to as-
sign each component an importance score. Since the eigen-
vectors are in parallel to the coordinate axis, the eigenval-
ues reﬂect the variance of components. Analogous to PCA
which posits that random variables with larger variance are

(a) (b) DimensionsComponent VectorsRandom VariablesSamples(a)  λ1 > λ2(b)  λ1   λ2λ1u1u2λ1u1u2λ2λ2(a)  λ1 > λ2(b)  λ1   λ2u2u1λ2λ1λ1u1u2λ2Uncorrelation and Evenness: a New Diversity-Promoting Regularizer

more important, we use variance to measure importance.
As shown in Figure 3(a), component 1 has a larger eigen-
value λ1 and accordingly larger variability, hence is more
important than component 2. According to the evenness
criteria, the components are more diverse if their impor-
tance match, which motivates us to encourage the eigen-
values to be uniform. As shown in Figure 3(b), the two
eigenvalues are close and the two components have roughly
the same variability, hence are similarly important.

To sum up, we desire to encourage the eigenvalues to be
even in both cases:
(1) when the eigenvectors are not
aligned with the coordinate axis, they are preferred to be
even to reduce the correlation of components; (2) when the
eigenvectors are aligned with the coordinate axis, they are
encouraged to be even such that different components con-
tribute equally in modeling data. Previously, encouraging
evenness among variances (eigenvalues) is investigated in
other problems, such as learning compact representations
for efﬁcient hashing (Kong & Li, 2012; Ge et al., 2013).

(cid:80)m

λk
j=1 λj

Next, we discuss how to promote uniformity among eigen-
values. The basic idea is: we normalize the eigenvalues
into a probability simplex and encourage the discrete
distribution parameterized by the normalized eigenval-
ues to have small Kullback-Leibler
(KL) divergence
with the uniform distribution. Given the eigenvalues
{λk}m
k=1, we ﬁrst normalize them into a probability
simplex (cid:98)λk =
based on which we deﬁne a
distribution on a discrete random variable X = 1, · · · , m
where p(X = k) = (cid:98)λk.
In addition, to guarantee the
eigenvalues are strictly positive, we require A(cid:62)A to be
positive deﬁnite. To encourage {(cid:98)λk}m
k=1 to be uniform,
we encourage the distribution p(X) to be “close” to
a uniform distribution q(X = k) = 1
m , where the
“closeness” is measured using KL divergence KL(p||q):
(cid:80)m
k=1 λk log λk
j=1 λj +log m.
(cid:80)m
j=1 λj
In this equation, (cid:80)m
k=1 λk log λk
to
tr(( 1
d A(cid:62)A)), where log(·) denotes ma-
trix logarithm. To show this, note that log( 1
d A(cid:62)A) =
(cid:80)m
k , according to the property of matrix
logarithm. Then we have tr(( 1
d A(cid:62)A)) equals
k )((cid:80)m
tr(((cid:80)m
k )) which
to
equals to (cid:80)m
k=1 λk log λk. According to the property
of trace, we have tr( 1
k=1 λk. Then the
KL divergence can be turned into a diversity-promoting
uniform eigenvalue regularizer (UER):
d A(cid:62)A) log( 1
tr( 1
d A(cid:62)A)

d A(cid:62)A) log( 1
k=1 log(λk)uku(cid:62)

d A(cid:62)A) = (cid:80)m

k=1 log(λk)uku(cid:62)

k=1 (cid:98)λk log (cid:98)λk

d A(cid:62)A)log( 1

k=1 λkuku(cid:62)

is equivalent

−log (cid:80)m

d A(cid:62)A))

− log tr(

1/m =

A(cid:62)A)

tr(( 1

1
d

(2)

(cid:80)m

subject to A(cid:62)A (cid:31) 0 and A(cid:62)1 = 0. Compared with pre-
vious diversity-promoting regularizers, UER has the fol-
lowing beneﬁts: (1) It measures the diversity of all com-
ponents in a holistic way, rather than reducing to pairwise

dissimilarities as other regularizers (Yu et al., 2011; Bao
et al., 2013; Xie et al., 2015) do. This enables UER to
capture global relations among components.
(2) Unlike
determinant-based regularizers (Malkin & Bilmes, 2008;
Zou & Adams, 2012) that are sensitive to vector scal-
ing, UER is derived from normalized eigenvalues where
the normalization effectively removes scaling. (3) UER is
amenable for computation. First, unlike DoCev (Cogswell
et al., 2015) that is deﬁned over data-dependent interme-
diate variables incurring computational inefﬁciency, UER
is directly deﬁned on model parameters independent of
data. Second, unlike the regularizers proposed in (Bao
et al., 2013; Xie et al., 2015) that are non-smooth, UER
is a smooth function. The dominating computation in UER
is the matrix logarithm. It does not substantially increase
computational overhead as long as the number of compo-
nents is not too large (e.g., less than 1000).

We apply UER to promote diversity in LSMs. Let L(A)
denote the objective function of an LSM, then an UE-
regularized LSM problem can be deﬁned as

minA L(A) + λ( tr(( 1
s.t.

A(cid:62)1 = 0, A(cid:62)A (cid:31) 0

d A(cid:62)A) log( 1
tr( 1
d A(cid:62)A)

d A(cid:62)A))

− log tr( 1

d A(cid:62)A))

where λ is the regularization parameter. Similar to other
diversity-promoting regularizers, UER is non-convex.
Since L(A) in most LSMs is non-convex, adding UER
does not substantially increase difﬁculty for optimization.

Connection with von Neumann Entropy In this sec-
tion, we make a connection between UER and von Neu-
mann entropy. A matrix M is referred to as a density
matrix (Bengtsson & Zyczkowski, 2007) if its eigenvalues
are strictly positive and sum to one, equivalently, M (cid:31) 0
and tr(M) = 1. The von Neumann entropy (Bengts-
son & Zyczkowski, 2007) of M is deﬁned as S(M) =
−tr(M log M), which is essentially the Shannon entropy
of its eigenvalues. If the covariance matrix G of compo-
nents is a density matrix, then we can use its von Neumann
entropy to deﬁne a UER. To encourage the eigenvalues
{λk}m
k=1 of G to be even, we directly encourage the KL
divergence between the distribution parameterized by the
eigenvalues (without normalization) and the uniform distri-
bution to be small: (cid:80)m
k=1 λk log λk
k=1 λk log λk +
log m, which is equivalent to encouraging the Shannon en-
tropy of the eigenvalues − (cid:80)m
k=1 λk log λk, i.e., the von
Neumann entropy of G to be large. Then a new UER can
be deﬁned as the negative von Neumann entropy of G:
tr(( 1
d A(cid:62)A) log( 1
d A(cid:62)A)), subject to the constraints: (1)
A(cid:62)A (cid:31) 0; (2) tr( 1
d A(cid:62)A) = 1; (3) A(cid:62)1 = 0. This new
UER is a special case of the previous one (Eq.(2)).

1/m = (cid:80)m

Connection with von Neumann Divergence Next we
make a connection between the UER and von Neumann
divergence (Kulis et al., 2009). Given two positive deﬁ-

Uncorrelation and Evenness: a New Diversity-Promoting Regularizer

nite matrices X and Y, their von Neumann divergence is
deﬁned as tr(X log X − X log Y − X + Y), which mea-
sures the closeness between the two matrices. Given two
vectors x, y ∈ Rm, their generalized KL divergence can be
deﬁned as (cid:80)m
) − (xk − yk), which measures
the closeness between two vectors. To encourage unifor-
mity among the eigenvalues of the covariance matrix G,
we can decrease the generalized KL divergence between
these eigenvalues and an all-1 vector:

k=1 xk log( xk
yk

k=1 λk log( λk

1 ) − (λk − 1)

(cid:80)m
= tr(( 1

d A(cid:62)A)) + m

d A(cid:62)A) log( 1

d A(cid:62)A)) − tr( 1
which is the von Neumann divergence between G and an
identity matrix. Hence, encouraging uniformity among
eigenvalues can be achieved by making G to be close to
an identity matrix based on the von Neumann divergence.

(3)

3.2. Case Studies

In this section, we apply the uniform eigenvalue regular-
izer to promote diversity in two latent space models: DML
and LSTM. We also applied it to latent Dirichlet alloca-
tion (Blei et al., 2003) and classiﬁer ensemble (Yu et al.,
2011). Due to space limit, the results of the latter two are
deferred to the supplements.

Distance Metric Learning (DML) Given data pairs ei-
ther labeled as “similar” or “dissimilar”, DML (Xing et al.,
2002; Davis et al., 2007; Guillaumin et al., 2009) aims to
learn a distance metric under which similar pairs would be
placed close to each other and dissimilar pairs are sepa-
rated apart. The learned distance can beneﬁt a wide range
of tasks, including retrieval, clustering and classiﬁcation.
Following (Weinberger & Saul, 2009), we deﬁne the dis-
tance metric between x, y ∈ Rd as (cid:107)A(cid:62)x − A(cid:62)y(cid:107)2
2 where
A ∈ Rd×m is a parameter matrix whose column vec-
tors are components. Built upon the DML formulation
in (Xie, 2015), an uniform-eigenvalue regularized DML
(DML-UE) problem can be formulated as

minA

(cid:80)

(cid:107)A(cid:62)x − A(cid:62)y(cid:107)2
2

(x,y)∈S
+ (cid:80)

(x,y)∈D

max(0, 1 − (cid:107)A(cid:62)x − A(cid:62)y(cid:107)2
2)

d A(cid:62)A) log( 1
+λ( tr(( 1
tr( 1
d A(cid:62)A)
A(cid:62)1 = 0, A(cid:62)A (cid:31) 0

d A(cid:62)A))

s.t.

− log tr( 1

d A(cid:62)A))

(4)
where S and D are the set of similar and dissimilar pairs
respectively. The ﬁrst and second term in the objective
function encourage similar pairs to have small distance and
dissimilar pairs to have large distance respectively. The
learned metrics are applied for information retrieval.

Long Short-Term Memory (LSTM) Network LSTM
(Hochreiter & Schmidhuber, 1997) is a type of recurrent
neural network, that is better at capturing long-term depen-
dency in sequential modeling. At each time step t where

the input is xt, there is an input gate it, a forget gate ft, an
output gate ot, a memory cell ct and a hidden state ht. The
transition equations among them are

it = σ(W(i)xt + U(i)ht−1 + b(i))
ft = σ(W(f )xt + U(f )ht−1 + b(f ))
ot = σ(W(o)xt + U(o)ht−1 + b(o))
ct = it (cid:12) tanh(W(c)xt + U(c)ht−1 + b(c)) + ft (cid:12) ct−1
ht = ot (cid:12) tanh(ct)

where W = {W(s)|s ∈ S = {i, f, o, c}} and U =
{U(s)|s ∈ S} are gate-speciﬁc weight matrices and B =
{b(s)|s ∈ S} are bias vectors. The row vectors in W and
U are treated as components. Let L(W, U, B) denote the
loss function of an LSTM network and R(·) denote the
UER (including constraints), then a UE-regularized LSTM
problem can be deﬁned as
minW,U ,B L(W, U, B) + λ (cid:80)

s∈S(R(W(s)) + R(U(s)))

(5)
The LSTM network is applied for cloze-style reading com-
prehension (CSRC). The network architecture follows that
in (Seo et al., 2017), which achieves the state of the art per-
formance on CSRC.

3.3. Algorithm

We develop a projected gradient descent (PGD) algorithm
to solve the UE-regularized LSM problem in Eq.(5). The
constraint A(cid:62)A (cid:31) 0 ensures the eigenvalues of A(cid:62)A are
positive, such that log(A(cid:62)A) is well-deﬁned. However,
it makes optimization very nasty. To address this issue,
we add a small perturbation (cid:15)I over A(cid:62)A where (cid:15) is a
close-to-zero positive scalar and I is an identity matrix, to
ensure log(A(cid:62)A + (cid:15)I) is always well-deﬁned. Accord-
ingly, the constraint A(cid:62)A (cid:31) 0 can be eliminated. The
PGD algorithm iteratively performs three steps: (1) com-
pute (sub)gradient (cid:52)A of the objective function; (2) up-
date A using gradient descent: (cid:101)A ← A − η (cid:52) A; (3)
project (cid:101)A to the constraint set {A|A(cid:62)1 = 0}.
In step
d A(cid:62)A + (cid:15)I) log( 1
(1), the derivative of tr(( 1
d A(cid:62)A + (cid:15)I))
d A(log( 1
is 2
d A(cid:62)A + (cid:15)I) + I). To compute the logarithm
of 1
d A(cid:62)A + (cid:15)I, we perform an eigen-decomposition of
this matrix into UΛU(cid:62), transform Λ into another diag-
onal matrix (cid:101)Λ where (cid:101)Λjj = log(Λjj) and then compute
log( 1
d A(cid:62)A + (cid:15)I) as U (cid:101)ΛU(cid:62). The complexity of eigen-
decomposing this m-by-m matrix is O(m3).
In our ap-
plications, m is no more than 500, so O(m3) is not a big
bottleneck. In addition, this matrix is symmetric and the
symmetry can be leveraged for fast eigen-decomposition.
In implementation, we use the MAGMA library that sup-
ports efﬁcient eigen-decomposition of symmetric matri-
In step (3), the projec-
ces on both CPUs and GPUs.
tion operation amounts to solving the following problem:
F subject to A(cid:62)1 = 0. According to
minA
KKT conditions (Boyd & Vandenberghe, 2004), we have

2 (cid:107)A − (cid:101)A(cid:107)2

1

Uncorrelation and Evenness: a New Diversity-Promoting Regularizer

MIMIC
Cars
Birds
CNN
DailyMail

#Train
40K
8144
9000
380K
879K

#Test Dim.
18K 7207
4096
8041
4096
2788
–
3198
–
53K

#Class
2833
196
200
–
–

Table 1. Dataset Statistics

A − (cid:101)A + 1λ(cid:62) = 0 and A(cid:62)1 = 0. Solving this system of
equations, we get A = (I − 1
d 11(cid:62)) (cid:101)A, which centers the
row vectors in (cid:101)A to have zero mean.

4. Experiments

In this section, we present experimental results.

Dataset We used ﬁve datasets in the experiments: an
electronic health record dataset MIMIC-III (Johnson et al.,
2016); two image datasets Stanford-Cars (Krause et al.,
2013) and Caltech-UCSD-Birds (Welinder et al., 2010);
two question answering (QA) datasets CNN and Daily-
Mail (Hermann et al., 2015). The ﬁrst three were used
for DML and the last two for LSTM. Their statistics are
summarized in Table 1. MIMIC-III contains hospital ad-
missions of patients. The class label of each admission is
the primarily diagnosed disease. For Stanford-Cars, CNN
and DailyMail, we use a single train/test split speciﬁed by
the data providers; for the other two, ﬁve random splits are
performed and the results are averaged over the ﬁve runs.
For the MIMIC-III dataset, we extract 7207-dimensional
features: (1) 2 dimensions from demographics, including
age and gender; (2) 5300 dimensions from clinical notes,
including 5000-dimensional bag-of-words (weighted us-
ing tf-idf) and 300-dimensional Word2Vec (Mikolov et al.,
2013); (3) 1905-dimensions from lab tests where the zero-
order, ﬁrst-order and second-order temporal features are
extracted for each of the 635 lab items. For bag-of-words,
we remove stop words, then select the 5000 words with
largest document frequency. For Word2Vec, we train 300-
dimensional embeddings for each word; to represent a doc-
ument, we average the embeddings of all words in this doc-
ument. For the two image datasets, we use the VGG16 (Si-
monyan & Zisserman, 2014) convolutional neural network
trained on the ImageNet (Deng et al., 2009) dataset to ex-
tract features, which are the 4096-dimensional outputs of
the second fully-connected layer. In the two QA datasets,
each instance consists of a passage, a question and an an-
swer. The question is a cloze-style task where an entity is
replaced by a placeholder and the goal is to infer this miss-
ing entity (answer) from all the possible entities appearing
in the passage.

Experimental Setup In DML experiments, two samples
are labeled as similar if belonging to the same class and
dissimilar otherwise. The learned distance metrics are ap-

DML
EUC
ITML
LDML
GMML
DML-L2
DML-L1
DML-LowRank
DML-Dropout
DML-DC
DML-CS
DML-DPP
DML-IC
DML-MA
DML-DeCov
DML-UE

MIMIC
72.5 ± 0.3
58.3 ± 0.1
69.3 ± 0.4
70.9 ± 0.9
71.2 ± 0.3
72.9 ± 0.1
72.6 ± 0.6
72.5 ± 0.7
73.1 ± 0.3
73.7 ± 0.4
73.5 ± 0.5
74.2 ± 0.3
74.3 ± 0.2
73.6 ± 0.4
72.6 ± 0.1
75.4 ± 0.3

Cars
53.1 ± 0.0
37.8 ± 0.0
50.1 ± 0.0
51.3 ± 0.0
54.2 ± 0.0
53.4 ± 0.0
53.7 ± 0.0
53.3 ± 0.0
53.5 ± 0.0
57.1 ± 0.0
55.7 ± 0.0
55.9 ± 0.0
56.3 ± 0.0
55.8 ± 0.0
56.2 ± 0.0
58.2 ± 0.0

Birds
55.9 ± 0.5
43.2 ± 0.0
52.9 ± 0.3
52.1 ± 0.2
53.7 ± 0.6
57.1 ± 0.4
56.4 ± 0.2
56.1 ± 0.6
56.6 ± 0.3
56.5 ± 0.4
57.4 ± 0.2
56.9 ± 0.7
57.8 ± 0.2
58.2 ± 0.1
56.2 ± 0.8
59.4 ± 0.2

Table 2. Precision@10 (%) on three datasets. The Cars dataset
has a single train/test split, hence the standard error is 0.

plied for retrieval whose performance is evaluated using
precision@K. We compare with two sets of regularizers:
(1) diversity-promoting regularizers based on determinant
of covariance (DC) (Malkin & Bilmes, 2008), cosine sim-
ilarity (CS) (Yu et al., 2011), determinantal point process
(DPP) (Kulesza & Taskar, 2012; Zou & Adams, 2012),
InCoherence (IC) (Bao et al., 2013), mutual angles (MA)
(Xie et al., 2015), and decorrelation (DeCov) (Cogswell
et al., 2015); (2) regularizers that are designed for other
purposes, including L2 norm for small norm, L1 norm for
sparsity, low-rankness (Recht et al., 2010) and Dropout
(Srivastava et al., 2014). All these regularizers are ap-
plied to the same DML formulation (Eq.(4) without the
In addition, we compare with vanilla Eu-
regularizer).
clidean distance (EUC) and other distance learning meth-
ods including information theoretic metric learning (ITML)
(Davis et al., 2007), logistic discriminant metric learning
(LDML) (Guillaumin et al., 2009), and geometric mean
metric learning (GMML) (Zadeh et al., 2016). We use 5-
fold cross validation to tune the regularization parameter
in {10−5, 10−4, · · · , 105} and the number of components
in {50, 100, 200, · · · , 500}. The best tuned regularization
parameters of UER are: 0.001 for MIMIC, 0.01 for Cars
and Birds. The best tuned component numbers are: 200 for
MIMIC, 100 for Cars and 200 for Birds. The learning rate
of the PGD algorithm is set to 0.001.

In LSTM experiments, the model architecture and exper-
imental settings follow the Bidirectional Attention Flow
(BIDAF) (Seo et al., 2017) model, which consists of the
following layers: character embedding, word embedding,
contextual embedding, attention ﬂow, modeling and out-
put. The contextual and modeling layers use long short-
term memory (LSTM) networks (Seo et al., 2017). In char-

Uncorrelation and Evenness: a New Diversity-Promoting Regularizer

MIMIC Cars Birds Average

DML
DML-L2
DML-L1
DML-LowRank
DML-Dropout
DML-DC
DML-CS
DML-DPP
DML-IC
DML-MA
DML-DeCov
DML-UE

300
300
300
400
300
200
300
200
400
300
300
200

300
300
300
300
300
400
100
300
300
200
400
100

500
500
500
400
400
400
300
300
200
300
300
200

367
367
367
367
333
333
233
267
300
267
333
167

Table 3. Optimal number of components.

acter embedding based on convolutional neural network,
100 1D ﬁlters are used, each with a width of 5. The hidden
state size is set to 100. AdaDelta (Zeiler, 2012) is used for
optimization with a minibatch size of 48. Dropout (Srivas-
tava et al., 2014) with probability 0.2 is used for all LSTM
layers. The model is trained for 8 epochs with early stop
when the validation accuracy starts to drop. We compare
UER with other diversity-promoting regularizers including
DC, CS, DPP, IC, MA and DeCov.

Results Table 2 shows the retrieval precision (K = 10)
on three datasets, where we observe: (1) DML-UE achieves
much better precision than DML, proving that UER is
an effective regularizer in improving generalization per-
formance; (2) UER outperforms other diversity-promoting
regularizers possibly due to its capability to capture global
relations among all components and insensitivity to vector
scaling; (3) diversity-promoting regularizers perform bet-
ter than other types of regularizers such as L2, L1, low
rank and Dropout, corroborating the efﬁcacy of inducing
diversity; (4) DML-UE outperforms other popular distance
learning methods such as ITML, LDML and GMML.

Table 3 shows the number of components that achieves
the precision in Table 2. Compared with DML, DML-
UE uses much fewer components to achieve better preci-
sion. For example, on the Cars dataset, DML-UE achieves
58.2% precision with 100 components. In contrast, with
more components (300), DML achieves a much lower pre-
cision (53.1%). This demonstrates that by encouraging the
components to be diverse, UER is able to reduce model
size without sacriﬁcing modeling power. UER encour-
ages equal “importance” among components such that each
component plays a signiﬁcant role in modeling data. As
a result, it sufﬁces to use a small number of components
to achieve larger modeling power. Compared with other
diversity-promoting regularizers, UER achieves better pre-
cision with fewer components, demonstrating its ability to
better promote diversity.

DML
EUC
ITML
LDML
GMML
DML-L2
DML-L1
DML-LowRank
DML-Dropout
DML-DC
DML-CS
DML-DPP
DML-IC
DML-MA
DML-DeCov
DML-UE

Frequent
77.6 ± 0.2
58.7 ± 0.1
74.2 ± 0.6
76.1 ± 0.8
75.9 ± 0.1
77.5 ± 0.3
77.4 ± 0.5
77.7 ± 0.5
78.1 ± 0.2
77.9 ± 0.4
78.0 ± 0.5
77.3 ± 0.2
78.5 ± 0.3
76.8 ± 0.2
77.1 ± 0.1
78.3 ± 0.3

Infrequent
64.2 ± 0.3
57.6 ± 0.2
61.3 ± 0.3
62.3 ± 0.9
63.5 ± 0.4
65.4 ± 0.1
64.8 ± 0.8
64.0 ± 0.8
64.9 ± 0.4
66.8 ± 0.2
66.2 ± 0.7
69.1 ± 0.5
67.4 ± 0.2
68.4 ± 0.4
65.3 ± 0.1
70.7 ± 0.4

Table 4. Precision@10 (%) on frequent and infrequent diseases
of the MIMIC-III dataset.

Next, we verify whether “diversifying” the components in
DML can better capture infrequent patterns. In the MIMIC-
III dataset, we consider diseases as patterns and consider a
disease as “frequent” if more than 1000 hospital admissions
are diagnosed with this disease and “infrequent” if other-
wise. Table 4 shows the retrieval precision on frequent dis-
eases and infrequent diseases. As can be seen, compared
with the baselines, DML-UE achieves more improvement
on infrequent diseases than on frequent diseases. This in-
dicates that by encouraging the components to diversely
spread out, UER is able to better capture infrequent patterns
(diseases in this case) without compromising the perfor-
mance on frequent patterns. On infrequent diseases, DML-
UE outperforms other diversity-promoting methods, show-
ing the advantage of UER over other diversity-promoting
regularizers. To further verify this, we select 3 most fre-
quent diseases (hypertension, AFib, CAD) and randomly
select 5 infrequent ones (helicobacter pylori, acute chole-
cystitis, joint pain-shlder, dysarthria, pressure ulcer), and
show the precision@10 on each individual disease in Ta-
ble 5. As can be seen, on the ﬁve infrequent diseases,
DML-UE achieves higher precision than baselines while
on the three frequent diseases, DML-UE achieves compa-
rable precision.

We empirically verify whether UER can promote uncorre-
lation and evenness. Given m component vectors, we com-
pute the empirical correlation (cosine similarity) of every
two vectors, then average these pairwise correlation scores
to measure the overall correlation of m vectors. We per-
form the study by learning distance metrics that have 200
components, on the MIMIC-III dataset. The average corre-
lation under unregularized DML and DML-UE is 0.73 and
0.57 respectively. This shows that UER can reduce corre-

Uncorrelation and Evenness: a New Diversity-Promoting Regularizer

DML
EUC
ITML
LDML
GMML
DML-L2
DML-L1
DML-LowRank
DML-Dropout
DML-DC
DML-CS
DML-DPP
DML-IC
DML-MA
DML-DeCov
DML-UE

1 (3566)
80.2 ± 0.5
66.4 ± 1.2
75.4 ± 1.0
77.0 ± 0.8
76.3 ± 0.4
81.0 ± 0.5
78.2 ± 1.0
79.6 ± 0.4
81.6 ± 0.5
77.9 ± 1.0
80.0 ± 0.5
79.8 ± 0.8
78.8 ± 1.3
77.3 ± 1.1
80.7 ± 0.5
81.4 ± 0.9

2 (3498)
79.4 ± 0.8
69.6 ± 1.2
76.3 ± 0.9
75.7 ± 1.0
78.7 ± 0.8
79.3 ± 0.8
79.9 ± 1.1
79.7 ± 0.5
78.7 ± 0.7
77.3 ± 0.9
80.3 ± 0.7
77.6 ± 0.2
79.2 ± 1.1
80.1 ± 1.0
78.8 ± 0.7
82.4 ± 0.8

3 (2757)
80.9 ± 0.7
61.8 ± 0.4
79.3 ± 0.9
78.1 ± 0.6
79.8 ± 0.8
77.6 ± 0.4
80.8 ± 1.2
75.4 ± 0.5
80.7 ± 0.5
80.3 ± 1.2
80.8 ± 0.6
77.4 ± 0.7
77.0 ± 0.8
81.0 ± 0.7
80.5 ± 1.1
80.5 ± 0.4

4 (204)
5.6 ± 1.6
7.2 ± 1.0
5.3 ± 1.2
3.2 ± 1.3
3.9 ± 1.8
4.4 ± 1.1
6.3 ± 1.9
3.1 ± 1.0
3.2 ± 1.5
7.1 ± 0.8
9.4 ± 1.3
10.1 ± 1.1
11.8 ± 0.6
11.5 ± 1.1
10.5 ± 0.8
14.3 ± 0.9

5 (176)
6.1 ± 0.8
6.9 ± 0.6
3.7 ± 1.3
5.6 ± 1.8
5.9 ± 1.5
5.6 ± 0.9
4.8 ± 1.1
9.2 ± 1.2
4.2 ± 1.9
8.9 ± 0.9
4.8 ± 1.7
10.3 ± 0.8
9.2 ± 1.4
9.9 ± 1.1
11.4 ± 1.2
11.2 ± 1.3

6 (148)
4.9 ± 0.8
3.1 ± 1.4
6.0 ± 0.9
3.8 ± 1.1
11.9 ± 1.4
3.7 ± 0.9
9.5 ± 1.0
5.5 ± 1.4
6.1 ± 0.9
9.7 ± 1.4
8.9 ± 1.9
8.8 ± 1.7
5.7 ± 1.6
4.9 ± 1.1
9.2 ± 0.7
10.7 ± 1.8

7 (131)
4.3 ± 1.7
6.8 ± 1.2
3.3 ± 0.9
6.7 ± 1.4
6.1 ± 0.6
4.9 ± 1.2
7.7 ± 1.0
4.8 ± 0.6
4.2 ± 0.8
11.9 ± 0.7
9.7 ± 0.7
11.7 ± 1.2
8.7 ± 1.4
7.6 ± 1.2
9.8 ± 1.2
15.8 ± 1.4

8 (121)
5.2 ± 0.6
2.4 ± 0.7
5.0 ± 1.0
5.0 ± 1.3
6.3 ± 1.5
6.2 ± 1.1
5.6 ± 1.7
4.5 ± 1.5
6.2 ± 1.9
9.0 ± 1.6
9.0 ± 1.0
8.4 ± 1.3
9.6 ± 0.7
10.4 ± 1.4
10.4 ± 1.2
13.2 ± 0.7

Table 5. Precision@10 (%) on three frequent and ﬁve infrequent diseases. The number next to a disease ID is its frequency.

DML
DML-DC
DML-CS
DML-DPP
DML-IC
DML-MA
DML-DeCov
DML-UE

MIMIC Cars Birds
10.1
9.1
11.7
10.9
10.5
9.7
11.2
10.6
10.5
9.7
10.6
9.4
10.8
10.1
11.5
10.5

20.5
22.3
20.9
22.6
21.1
21.3
21.7
22.8

Table 6. Average runtime (hours) of DML methods

i a}N

i a}N

lation. To measure evenness, we ﬁrst measure the “impor-
tance” of components. For each component with parameter
vector a, we project the training examples {xi}N
i=1 onto a:
{x(cid:62)
i=1, then use the variance of {x(cid:62)
i=1 to measure
the importance of this component. After that, we map these
importance scores into a probabilistic simplex using soft-
max. Finally, the evenness is measured by the KL diver-
gence between the discrete distribution parameterized by
these probabilities and a uniform distribution. A smaller
KL divergence indicates larger evenness. On MIMIC-III
with 200 components, the KL divergence under unregular-
ized DML and DML-UE is 3.54 and 2.92 respectively. This
suggests that our regularizer is able to encourage evenness.

Table 6 shows the runtime taken by DML methods to reach
convergence. Compared with unregularized DML, DML-
UE does not increase the training time substantially. The
relative increase is 11.2% on MIMIC, 15.4% on Cars and
13.9% on Birds. The runtime of DML-UE is close to DML
regularized by other diversity-promoting regularizers.

In the LSTM experiments, Table 7 shows state of the art ac-
curacy on the two QA datasets. Compared with the original
BIDAF (Seo et al., 2017), our method BIDAF-UE achieves
better accuracy, further demonstrating UER’s ability to im-
prove generalization performance. Besides, UER outper-
forms other regularizers.

Kadlec et al. (2016)
Kobayashi et al. (2016)
Sordoni et al. (2016)
Trischler et al. (2016)
Chen et al. (2016)
Dhingra et al. (2016)
Cui et al. (2016)
Shen et al. (2016)
BIDAF
BIDAF-DC
BIDAF-CS
BIDAF-DPP
BIDAF-IC
BIDAF-MA
BIDAF-DeCov
BIDAF-UE
Dhingra et al. (2016)
Dhingra et al. (2017)

CNN

Dev
68.6
71.3
72.6
73.4
73.8
73.0
73.1
72.9
76.31
76.36
76.43
76.32
76.41
76.49
76.35
76.58
77.9
79.2

Test
69.5
72.9
73.3
74.0
73.6
73.8
74.4
74.7
76.94
76.98
77.10
77.04
77.21
77.09
77.15
77.27
77.9
78.6

DailyMail
Test
Dev
73.9
75.0
–
–
–
–
–
–
76.6
77.6
75.7
76.7
–
–
76.6
77.6
79.63
80.33
79.68
80.51
79.71
80.37
79.77
80.45
79.83
80.49
79.74
80.42
79.67
80.38
79.86
80.63
80.9
81.5
–
–

Table 7. Accuracy (%) on the two QA datasets

5. Conclusions

We propose a new diversity-promoting regularizer from
the perspectives of uncorrelation which prefers the compo-
nents in LSMs to be uncorrelated and evenness which en-
courages the components to contribute equally to the mod-
eling of data. Gaining insight from PCA, promoting un-
correlation and evenness both amount to encouraging the
covariance matrix of components to have uniform eigen-
values, which leads to a uniform eigenvalue regularizer
(UER). The UER is applied to DML and LSTM. Experi-
mental studies reveal that UER greatly boosts the perfor-
mance of LSMs, better captures infrequent patterns, re-
duces model size without compromising modeling power
and outperforms other diversity-promoting regularizers.

Uncorrelation and Evenness: a New Diversity-Promoting Regularizer

Acknowledgements

We would like to thank the anonymous reviewers for the
helpful suggestions and comments. P.X and E.X are sup-
ported by National Institutes of Health P30DA035778,
R01GM114311, National Science Foundation IIS1617583,
DARPA FA872105C0003 and Pennsylvania Department of
Health BD4BH4100070287.

References

Bao, Yebo, Jiang, Hui, Dai, Lirong, and Liu, Cong.

In-
coherent training of deep neural networks to decorrelate
bottleneck features for speech recognition. 2013.

Bengtsson, Ingemar and Zyczkowski, Karol. Geometry of
quantum states: an introduction to quantum entangle-
ment. Cambridge University Press, 2007.

Bishop, Christopher M. Latent variable models. In Learn-
ing in graphical models, pp. 371–403. Springer, 1998.

Blei, David M. Build, compute, critique, repeat: Data
analysis with latent variable models. Annual Review of
Statistics and Its Application, 2014.

Blei, David M, Ng, Andrew Y, and Jordan, Michael I. La-
tent dirichlet allocation. the Journal of machine Learn-
ing research, 2003.

Boyd, Stephen and Vandenberghe, Lieven. Convex opti-

mization. Cambridge university press, 2004.

Chen, Danqi, Bolton, Jason, and Manning, Christopher D.
A thorough examination of the cnn/daily mail reading
comprehension task. arXiv preprint arXiv:1606.02858,
2016.

Cogswell, Michael, Ahmed, Faruk, Girshick, Ross, Zit-
nick, Larry, and Batra, Dhruv. Reducing overﬁtting in
deep networks by decorrelating representations. arXiv
preprint arXiv:1511.06068, 2015.

image database. In Computer Vision and Pattern Recog-
nition, 2009. CVPR 2009. IEEE Conference on, pp. 248–
255. IEEE, 2009.

Dhingra, Bhuwan, Liu, Hanxiao, Cohen, William W, and
Salakhutdinov, Ruslan. Gated-attention readers for text
comprehension. arXiv preprint arXiv:1606.01549, 2016.

Dhingra, Bhuwan, Yang, Zhilin, Cohen, William W, and
Salakhutdinov, Ruslan. Linguistic knowledge as mem-
arXiv preprint
ory for recurrent neural networks.
arXiv:1703.02620, 2017.

Ge, Tiezheng, He, Kaiming, Ke, Qifa, and Sun, Jian. Op-
timized product quantization for approximate nearest
In Proceedings of the IEEE Confer-
neighbor search.
ence on Computer Vision and Pattern Recognition, pp.
2946–2953, 2013.

Guillaumin, Matthieu, Verbeek,

Jakob, and Schmid,
Cordelia.
Is that you? metric learning approaches for
face identiﬁcation. In IEEE International Conference on
Computer Vision. IEEE, 2009.

Harman, Harry H. Modern factor analysis. 1960.

Hermann, Karl Moritz, Kocisky, Tomas, Grefenstette, Ed-
ward, Espeholt, Lasse, Kay, Will, Suleyman, Mustafa,
and Blunsom, Phil. Teaching machines to read and com-
prehend. In Advances in Neural Information Processing
Systems, pp. 1693–1701, 2015.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-
term memory. Neural computation, 9(8):1735–1780,
1997.

Johnson, Alistair EW, Pollard, Tom J, Shen, Lu, Lehman,
Li-wei H, Feng, Mengling, Ghassemi, Mohammad,
Moody, Benjamin, Szolovits, Peter, Celi, Leo Anthony,
and Mark, Roger G. Mimic-iii, a freely accessible criti-
cal care database. Scientiﬁc data, 3, 2016.

Cover, Thomas M and Thomas, Joy A. Elements of infor-

mation theory. John Wiley & Sons, 2012.

Library, 2002.

Jolliffe, Ian. Principal component analysis. Wiley Online

Cui, Yiming, Chen, Zhipeng, Wei, Si, Wang, Shijin, Liu,
Ting, and Hu, Guoping. Attention-over-attention neu-
ral networks for reading comprehension. arXiv preprint
arXiv:1607.04423, 2016.

Davis, Jason V, Kulis, Brian, Jain, Prateek, Sra, Suvrit, and
Dhillon, Inderjit S. Information-theoretic metric learn-
ing. In Proceedings of the 24th international conference
on Machine learning. ACM, 2007.

Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai,
Imagenet: A large-scale hierarchical

and Fei-Fei, Li.

Kadlec, Rudolf, Schmid, Martin, Bajgar, Ondrej, and
Kleindienst, Jan. Text understanding with the attention
sum reader network. ACL, 2016.

Knott, Martin and Bartholomew, David J. Latent variable
models and factor analysis. Number 7. Edward Arnold,
1999.

Kobayashi, Sosuke, Tian, Ran, Okazaki, Naoaki, and
Inui, Kentaro. Dynamic entity representation with max-
In Proceedings of
pooling improves machine reading.
NAACL-HLT, pp. 850–855, 2016.

Uncorrelation and Evenness: a New Diversity-Promoting Regularizer

Kong, Weihao and Li, Wu-Jun. Isotropic hashing. In Ad-
vances in Neural Information Processing Systems, pp.
1646–1654, 2012.

Krause, Jonathan, Stark, Michael, Deng, Jia, and Fei-Fei,
Li. 3d object representations for ﬁne-grained categoriza-
In Proceedings of the IEEE International Con-
tion.
ference on Computer Vision Workshops, pp. 554–561,
2013.

Kulesza, Alex and Taskar, Ben. Determinantal point
arXiv preprint

processes for machine learning.
arXiv:1207.6083, 2012.

Kulis, Brian, Sustik, M´aty´as A, and Dhillon, Inderjit S.
Low-rank kernel learning with bregman matrix diver-
Journal of Machine Learning Research, 10
gences.
(Feb):341–376, 2009.

Magurran, Anne E. Measuring biological diversity. John

Wiley & Sons, 2013.

Malkin, Jonathan and Bilmes, Jeff. Ratio semi-deﬁnite
In 2008 IEEE International Conference on
classiﬁers.
Acoustics, Speech and Signal Processing, pp. 4113–
4116. IEEE, 2008.

Mariet, Zelda and Sra, Suvrit. Diversity networks. arXiv

preprint arXiv:1511.05077, 2015.

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed representations of
In Ad-
words and phrases and their compositionality.
vances in neural information processing systems, pp.
3111–3119, 2013.

Srivastava, Nitish, Hinton, Geoffrey E, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:
a simple way to prevent neural networks from overﬁt-
Journal of Machine Learning Research, 15(1):
ting.
1929–1958, 2014.

Trischler, Adam, Ye, Zheng, Yuan, Xingdi, and Sule-
man, Kaheer. Natural language comprehension with the
epireader. arXiv preprint arXiv:1606.02270, 2016.

Wang, Yi, Zhao, Xuemin, Sun, Zhenlong, Yan, Hao, Wang,
Lifeng, Jin, Zhihui, Wang, Liubin, Gao, Yang, Law,
Ching, and Zeng, Jia. Peacock: Learning long-tail topic
features for industrial applications. ACM Transactions
on Intelligent Systems and Technology, 2014.

Weinberger, Kilian Q and Saul, Lawrence K. Distance met-
ric learning for large margin nearest neighbor classiﬁca-
tion. Journal of Machine Learning Research, 10(Feb):
207–244, 2009.

Welinder, Peter, Branson, Steve, Mita, Takeshi, Wah,
Catherine, Schroff, Florian, Belongie, Serge, and Per-
ona, Pietro. Caltech-ucsd birds 200. 2010.

Xie, Bo, Liang, Yingyu, and Song, Le. Diversity leads to

generalization in neural networks. AISTATS, 2017.

Xie, Pengtao. Learning compact and effective distance
metrics with diversity regularization. In European Con-
ference on Machine Learning, 2015.

Xie, Pengtao, Deng, Yuntian, and Xing, Eric P. Diversify-
ing restricted boltzmann machine for document model-
ing. In ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining, 2015.

Recht, Benjamin, Fazel, Maryam, and Parrilo, Pablo A.
Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM review,
52(3):471–501, 2010.

Xie, Pengtao, Zhu, Jun, and Xing, Eric. Diversity-
promoting bayesian learning of latent variable models.
In Proceedings of The 33rd International Conference on
Machine Learning, pp. 59–68, 2016.

Seo, Minjoon, Kembhavi, Aniruddha, Farhadi, Ali, and
Hajishirzi, Hannaneh. Bidirectional attention ﬂow for
machine comprehension. ICLR, 2017.

Shen, Yelong, Huang, Po-Sen, Gao, Jianfeng, and Chen,
Weizhu. Reasonet: Learning to stop reading in machine
comprehension. arXiv preprint arXiv:1609.05284, 2016.

Simonyan, Karen and Zisserman, Andrew. Very deep con-
volutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.

Sordoni, Alessandro, Bachman, Philip, Trischler, Adam,
Iterative alternating neu-
arXiv preprint

and Bengio, Yoshua.
ral attention for machine reading.
arXiv:1606.02245, 2016.

Xing, Eric P, Jordan, Michael I, Russell, Stuart, and Ng,
Andrew Y. Distance metric learning with application to
clustering with side-information. In Advances in neural
information processing systems, 2002.

Yu, Yang, Li, Yu-Feng, and Zhou, Zhi-Hua. Diversity reg-

ularized machine. 2011.

Zadeh, Pourya Habib, Hosseini, Reshad, and Sra, Suvrit.

Geometric mean metric learning. 2016.

Zeiler, Matthew D. Adadelta: an adaptive learning rate

method. arXiv preprint arXiv:1212.5701, 2012.

Zou, James Y and Adams, Ryan P. Priors for diversity in
generative latent variable models. In Advances in Neural
Information Processing Systems, pp. 2996–3004, 2012.

