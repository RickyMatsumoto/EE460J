Capacity Releasing Diffusion for Speed and Locality

000
001
002
003
004
005
006
007
008
009
010
011
012
013
014
015
016
017
018
019
020
021
022
023
024
025
026
027
028
029
030
031
032
033
034
035
036
037
038
039
040
041
042
043
044
045
046
047
048
049
050
051
052
053
054

A. CRD inner procedure

We ﬁrst ﬁll in the missing details in the CRD-inner subrou-
tine (Algorithm 1).

Note an ineligible arc (v, u) must remain ineligible until
the next relabel of v, so we only need to check each arc out
of v once between consecutive relabels. We use current(v)
to keep track of the arcs out of v that we have checked since
the last relabel of v. We always pick an active vertex v with
the lowest label. Then for any eligible arc (v, u), we know
m(u) ≤ d(u), so we can push at least 1 along (v, u) (with-
out violating m(u) ≤ 2d(u)), which is crucial to bound
the total work. We keep the list Q in non-decreasing order
of the vertices’ labels, for efﬁcient look-up of the lowest
labeled active vertex, and Add, Remove, Shift are the op-
erations to maintain this order. Note these operations can
be implemented to take O(1) work. In particular, when we
add a node u to Q, it will always be the active node with
lowest label, so will be put at the beginning. We only re-
move the ﬁrst element v from Q, and when we shift a node
v in Q, we know l(v) increases by exactly 1. To maintain
Q, we simply need to pick two linked lists, one containing
all the active nodes with non-decreasing labels, and another
linked list containing one pointer for each label value, as
long as there is some active node with that label, and the
pointer contains the position of ﬁrst such active node in Q.
Maintaining this two lists together can give O(1) time Add,
Remove, Shift.

Theorem 1. Given G, m(·), and φ ∈ (0, 1], such that
|m(·)| ≤ vol(G), and ∀v : m(v) ≤ 2d(v) at the start,
CRD-inner terminates with one of the following cases

(1) CRD-inner ﬁnishes the full CRD step: ∀v : m(v) ≤

d(v).

(2) There are nodes with excess, and we can ﬁnd a cut A
of conductance O(φ). Moreover, ∀v ∈ A : 2d(v) ≥
m(v) ≥ d(v), and ∀v ∈ ¯A : m(v) ≤ d(v).

Proof. Let l(·) be the labels of vertices at termination, and
let Bi = {v|l(v) = i}. We make the following observa-
tions: l(v) = h ⇒ 2d(v) ≥ m(v) ≥ d(v); h > l(v) ≥
1 ⇒ m(v) = d(v); l(v) = 0 ⇒ m(v) ≤ d(v).

Now we proceed to prove the main theorem of CRD-inner.

edge of v’s list of edges.

The running time is O(|m(·)| log |m(·)|/φ).

.

l(v) ← l(v) + 1.

Algorithm 1 CRD-inner(G,m(·),φ)

. Initialization:
.
.
.

. ∀{v, u} ∈ E, m(u, v) = m(v, u) = 0.
. Q = {v|m(v) > d(v)}, h = 3 log |m(·)|
. ∀v, l(v) = 0, and current(v) is the ﬁrst edge in

φ

v’s list of incident edges.

. While Q is not empty
.
.
.
.
.
.
.
.

. Let v be the lowest labeled vertex in Q.
. Push/Relabel(v).
. If Push/Relabel(v) pushes mass along (v, u)
. If v becomes in-active, Remove(v, Q)
.
. If u becomes active, Add(u, Q)
.
. Else If Push/Relabel(v) increases l(v) by 1
.
.

. If l(v) < h, Shift(v, Q)
. Else Remove(v, Q)

Push/Relabel(v)
. Let {v, u} be current(v).
. If arc (v, u) is eligible, then Push(v, u).
. Else
.
.
.
.

. If {v, u} is not the last edge in v’s list of edges.
.
. Else (i.e., {v, u} is the last edge of v)
.

. Relabel(v), and set current(v) be the ﬁrst

. Set current(v) be the next edge of v.

Push(v, u)
. Assertion: rm(v, u) > 0, l(v) ≥ l(u) + 1.

ex(v) > 0, m(u) < 2d(u).

. ψ = min (ex(v), rm(v, u), 2d(u) − m(u))
. Send ψ units of mass from v to u:

m(v, u) ← m(v, u) + ψ, m(u, v) ← m(u, v) − ψ.
m(v) ← m(v) − ψ, m(u) ← m(u) + ψ.

Relabel(v)
. Assertion: v is active, and ∀u ∈ V ,

rm(v, u) > 0 =⇒ l(v) ≤ l(u).

Capacity Releasing Diffusion for Speed and Locality

Since |m(·)| ≤ vol(G), if B0 = ∅, it must be |m(·)| =
vol(G), and every v has m(v) = d(v), so we get case (1).
If Bh = ∅, we also get case (1).
If Bh, B0 (cid:54)= ∅, let Si = ∪h
j=iBj be the set of nodes with
label at least i. We have h level cuts Sh, . . . , S1, where
vol(Sh) ≥ 1, and Sj ⊆ Si if j > i. We claim one of
these level cuts must have conductance O(φ). For any Si,
we divide the edges from Si to Si into two groups: 1) edge
across one level (i.e., from node in Bi to node in Bi−1),
and 2) edges across more than one level. Let z1(i), z2(i)
be the number of edges in the two groups respectively, and
deﬁne φg(i) def= zg(i)/vol(Si) for g = 1, 2.
First we show that, there must be a i∗ between h and h/2
such that φ1(i∗) ≤ φ. By contradiction, if φ1(i) > φ for all
i = h, . . . , h/2, since vol(Si−1) ≥ vol(Si)(1 + φ1(Si)),
we get vol(Sh/2) ≥ (1 + φ)h/2vol(Sh). With h =
3 log |m(·)|/φ, we have vol(Sh/2) ≥ Ω(|m(·)|3/2), and
since nodes in Sh/2 are all saturated, we get a contradic-
tion since we must have vol(Sh/2) ≤ |m(·)|.
Now we consider any edge {v, u} counted in z2(i∗) (i.e.,
v ∈ Si∗ , u ∈ Si∗ , l(v) − l(u) ≥ 2). Since i∗ ≥ h/2 > 1/φ,
ˆc(v, u) = 1/φ. l(v)−l(u) > 2 suggests rm(v, u) = 0, thus
m(v, u) = 1/φ (i.e., 1/φ mass pushed out of Si∗ along
each edge counted in z2(i∗)). Each edge counted in z1(i∗)
can have at most 1/φ mass pushed into Si∗ , and at most
2vol(Si∗ ) mass can start in Si∗ , then we know
z2(i∗)/φ ≤ z1(i∗)/φ + 2vol(Si∗ )

We will let A be Si∗ , and we have
z1(i∗) + z2(i∗)
vol(Si∗ )

φ(A) =

≤ 4φ = O(φ)

Here we assume Si∗ is the smaller side of the cut to com-
pute the conductance. If this is not the case, i.e. vol(Si∗ ) >
vol(G)/2, we just carry out the same argument as above,
but run the region growing argument from level h/4 up to
level h/2, and get a low conductance cut, and still let A
to be the side containing Sh. The additional properties of
elements in A follows from Sh ⊆ A ⊆ Sh/4.

Now we proceed to the running time. The initialization
takes O(|m(·)|). Subsequently, each iteration takes O(1)
work. We will ﬁrst attribute the work in each iteration to
either a push or a relabel. Then we will charge the work on
pushes and relabels to the absorbed mass, such that each
unit of absorbed mass gets charged O(h) work. Recall the
absorbed mass at v are the ﬁrst up to d(v) mass starting
at or pushed into v, and these mass never leave v, as the
algorithm only pushes excess mass. This will prove the
result, as there are at most |m(·)| units of (absorbed) mass
in total.

of ψ mass, we charge O(ψ) to that push operation. Since
ψ ≥ 1, we charged the push enough to cover the work in
that iteration. If the call to Push/Relabel(v) doesn’t push,
we charge the O(1) work of the iteration to the next relabel
of v (or the last relabel if there is no next relabel). The
latter can happen at most d(v) times between consecutive
relabels of v, so each relabel of v is charged O(d(v)) work.

We now charge the work on pushes and relabels to the ab-
sorbed mass. Note each time we relabel v, there are d(v)
units of absorbed mass at v, so we charge the O(d(v)) work
on the relabel to the absorbed mass, and each unit gets
charged O(1). There is at most h relabels of v, so each
unit of absorbed mass is charged O(h) in total by all the
relabels.

For the work on pushes, we consider the potential function
Λ = (cid:80)
v ex(v)l(v). Λ is always non-negative, and as we
only push excess mass downhill, each push of ψ units of
mass decrease Λ by at least ψ, so we can charge the work
on pushes to the increment of Λ. It only increases at relabel.
When we relabel v, Λ is increased by ex(v). Since ex(v) ≤
d(v), we can charge O(1) to each unit of absorbed mass at
v to cover Λ’s increment. In total we can charge all pushes
(via Λ) to absorbed mass, and each unit is charged with
O(h).

If we need to compute the cut A in case (2), the running
time is O(vol(S1)), which is O(|m(·)|).

B. Local Clustering

Recall we assume B to satisfy the following conditions.

def= φS (B)
Assumption 1. σ1
Assumption 2. There exists σ2 ≥ Ω(1), such that
any T ⊂ B with volB(T ) ≤ volB(B)/2 satisﬁes

φ(B) ≥ Ω(1)

|E(T, B \ T )|

|E(T, V \ B)| log vol(B) log

≥ σ2.

1
φS (B)

1

Now we proceed to prove the main lemma.
Lemma 1. In the j-th CRD step, let Mj be the total
amount of mass in B at the start, and Lj be the amount
then
of mass that ever leaves B during the diffusion,
σ2 log vol(B) ) · Mj when Mj ≤ volB(B)/2, and
Lj ≤ O(
Lj ≤ O( 1
σ1
Proof. For simplicity, we assume once a unit of mass
leaves B, it is never routed back. Intuitively, mass com-
ing back into B should only help the algorithm, and indeed
the results don’t change without this assumption. We de-
note |Mj(S)| as the amount of mass on nodes in a set S at
the start of the CRD-inner call.

) · Mj when Mj ≥ volB(B)/2.

In each iteration of Unit-Flow, the algorithm picks a lowest
labeled active node v. If Push/Relabel(v) ends with a push

We have two cases, corresponding to whether the diffusion
already spread a lot of mass over B. If Mj ≥ volB(B)/2,

055
056
057
058
059
060
061
062
063
064
065
066
067
068
069
070
071
072
073
074
075
076
077
078
079
080
081
082
083
084
085
086
087
088
089
090
091
092
093
094
095
096
097
098
099
100
101
102
103
104
105
106
107
108
109

Capacity Releasing Diffusion for Speed and Locality

110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164

we use the upperbound 1/φ that is enforced on the net mass
over any edge to limit the amount of mass that can leak out.
In particular Lj ≤ O(vol(B)φ(B)/φS(B)), since there are
vol(B)φ(B) edges from B to ¯B, and φ = Θ(φS(B)) in
CRD-inner. As Mj ≥ Ω(vol(B)), we have Lj ≤ O( 1
) ·
σ1
Mj.

The second case is when Mj ≤ volB(B)/2. In this case, a
combination of Assumption 2 and capacity releasing con-
trols the leakage of mass. Intuitively, there are still many
nodes in B that the diffusion can spread mass to. For the
nodes in B with excess on them, when they push their ex-
cess, most of the downhill directions go to nodes inside B.
As a consequence of capacity releasing, only a small frac-
tion of mass will leak out.

In particular, let l(·) be the labels on nodes when CRD-
inner ﬁnishes, we consider Bi = {v ∈ B|l(v) = i} and
the level cuts Si = {v ∈ B|l(v) ≥ i} for i = h, . . . , 1.
As Mj ≤ volB(B)/2, we know vol(Sh) ≤ vol(Sh−1) ≤
. . . ≤ vol(S1) ≤ volB(B)/2.
In this case, we can use
Assumption 2 on all level cuts Sh, . . . , S1. Moreover, for a
node v ∈ Bi, the ”‘effective”’ capacity of an arc from v to
¯B is min(i, 1/φ). Formally, we can bound Lj by the total
(effective) outgoing capacity, which is

h
(cid:88)

i=1

|E(Bi, ¯B)| · min(i,

) =

|E(Si, ¯B)|

(1)

1
φ

1
φ
(cid:88)

i=1

where h is the bound on labels used in unit ﬂow.

We design a charging scheme to charge the above quantity
(the right hand side) to the mass in ∆j(B), such that each
unit of mass is charged O(1/(σ2 log vol(B))). It follows
that Lj ≤ O(

σ2 log vol(B) ) · |∆j(B)|.

1

|E(Si,B\Si)|

Recall that, |E(Si, ¯B)| ≤
σ2 log vol(B) log(1/φ) from As-
sumption 2. We divide edges in E(Si, B \ Si) into two
groups: : 1) edges across one level, and 2) edges across
more than one level. Let z1(i), z2(i) be the number of
edges in the two groups respectively.

≥

z1(i)

|E(Si, B \ Si)|/3, we

If
charge
3/(σ2 log vol(B) log(1/φ)) to each edge in group 1.
These edges in turn transfer the charge to the absorbed
mass at their endpoints in Bi. Since each node v in level
i ≥ 1 has d(v) absorbed mass, each unit of absorbed mass
is charged O(1/(σ2 log vol(B) log(1/φ))). Note that the
group 1 edges of different level i’s are disjoint, so each
unit of absorbed mass will only be charged once this way.

If z1(i) ≤ |E(Si, B \ Si)|/3, we know z2(i) − z1(i) ≥
|E(Si, B \ Si)|/3. Group 2 edges in total send at least
(i − 1)z2(i) mass from Si to B \ Si, and at most (i −
1)z1(i) of these mass are pushed into Si by group 1 edges.
Thus, there are at least (i − 1)|E(Si, B \ Si)|/3 mass
that start in Si, and are absorbed by nodes at level be-

low i (possibly outside B).
In particular, this suggests
|Mj(Si)| ≥ (i − 1)|E(Si, B \ Si)|/3, and we split the
total charge |E(Si, ¯B)| evenly on these mass, so each
unit of mass is charged O(1/(iσ2 log vol(B) log(1/φ))).
Since we sum from i = 1/φ to 1 in (RHS of)
Eqn (1), we charge some mass multiple times (as
Si’s not disjoint), but we can bound the total charge
by (cid:80)1/φ
· O(1/(σ2 log vol(B) log(1/φ))), which is
i=1
O(1/(σ2 log vol(B))). This completes the proof.

1
i

C. Empirical Set-up and Results

C.1. Datasets

We chose the graphs of John Hopkins, Rice, Simmons and
Colgate universities/colleges. The actual IDs of the graphs
in Facebook100 dataset are Johns Hopkins55, Rice31,
Simmons81 and Colgate88. These graphs are anonymized
Facebook graphs on a particular day in September 2005 for
student social networks. The graphs are unweighted and
they represent “friendship ties”. The data form a subset
of the Facebook100 dataset from (Traud et al., 2012). We
chose these 4 graphs out of 100 due to their large assorta-
tivity value in the ﬁrst column of Table A.2 in (Traud et al.,
2012), where the data were ﬁrst introduced and analyzed.
Details about the graphs are shown is Table 1.

Graph

volume

nodes

edges

John Hopkins
Rice
Simmons
Colgate

373144
369652
65968
310086

5157
4083
1510
3482

186572
184826
32984
155043

Table 1. Graphs used for experiments.

Each graph in the Facebook dataset comes along with 6 fea-
tures, i.e., second major, high school, gender, dorm, major
index and year. We construct “ground truth” clusters by us-
ing the features for each node. In particular, we consider
nodes with the same value of a feature to be a cluster, e.g.,
students of year 2009. We loop over all possible clusters
and consider as ground truth the ones that have volume
larger than 1000, conductance smaller than 0.5 and gap
larger than 0.5. Filtering results in moderate scale clusters
for which the internal volume is at least twice as much as
the volume of the edges that leave the cluster. Additionally,
gap at least 0.5 means that the smallest nonzero eigenvalue
of the normalized Laplacian of the subgraph deﬁned by the
cluster is at least twice larger than the conductance of the
cluster in the whole graph. The clusters per graph that sat-
isfy the latter constraints are shown in Table 2. Notice that
the clusters which remain after ﬁltering correspond to fea-
tures year or dorm. This agrees with (Traud et al., 2012)

Capacity Releasing Diffusion for Speed and Locality

Table 2. Clusters of chosen graphs in Table 1, see Subsection C.1
for details.

C.3. Real-world experiments

165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219

in which it is stated that the features with clusters with the
best assortativity value correspond to the feature of year or
dorm.

year/dorm volume

size

gap

cond.

.

p
o
H

217
2009
203
e
c
i
2009
R
. 2007
m
2009
i
S
2006
2007
2008
2009

e
t
a
g
l
o
C

10696
32454
43321
30858
14424
11845
62064
68381
62429
35369

200
886
403
607
281
277
556
588
640
638

1.48
0.67
0.58
0.73
0.57
5.35
0.57
0.69
1.19
3.49

0.26
0.19
0.46
0.33
0.47
0.1
0.48
0.41
0.29
0.11

C.2. Performance criteria and parameter tuning

For real-world Facebook graphs since we calculate the
ground truth clusters in Table 2 then we measure perfor-
mance by calculating precision and recall for the output
clusters of the algorithms.

We set the parameters of CRD to φ = 1/3 for all exper-
iments. At each iteration we use sweep cut on the labels
returned by the CRD-inner subroutine to ﬁnd a cut of small
conductance, and over all iterations of CRD we return the
cluster with the lowest conductance.

ACL has two parameters, the teleportation parameter α and
a tolerance parameter (cid:15). Ideally the former should be set ac-
cording to the reciprocal of the mixing time of a a random
walk within the target cluster, which is equal to the small-
est nonzero eigenvalue of the normalized Laplacian for the
subgraph that corresponds to the target cluster. Let us de-
note the eigenvalue with λ. In our case the target cluster is a
ground truth cluster from Table 2. We use this information
to set parameter α. In particular, for each node in the clus-
ters in Table 2 we run ACL 4 times where α is set based on
a range of values in [λ/2, 2λ] with a step of (2λ − λ/2)/4.
The tolerance parameter (cid:15) is set to 10−7 for all experiments
in order to guarantee accurate solutions for the PageRank
linear system. For each parameter setting we use sweep cut
to ﬁnd a cluster of low conductance, and over all parameter
settings we return the cluster with the lowest conductance
value as an output of ACL.

For real-world experiments we show results for ACLopt.
In this version of ACL, for each parameter setting of α we
use sweep cut algorithm to obtain a low conductance clus-
ter and then we compute its precision and recall. Over all
parameter settings we keep the cluster with the best F1-

score; a combination of precision and recall. This is an
extra level of supervision for the selection of the telepor-
tation parameter α, which is not possible in practice since
it requires ground truth information. However, the perfor-
mance of ACLopt demonstrates the performance of ACL in
case that we could make optimal selection of parameter α
among the given range of parameters (which also includes
ground truth information) for the precision and recall crite-
ria.

Finally, we set the reference set of FlowI to be the output
set of best conductance of ACL out of its 4 runs for each
node. By this we aim to obtain an improved cluster to ACL
in terms of conductance. Note that FlowI is a global algo-
rithm, which means that it accesses the information from
the whole graph compared to CRD and ACL which are lo-
cal algorithms.

For clusters in Table 2 we sample uniformly at random
half of their nodes. For each node we run CRD, ACL
and ACL+FlowI. We report the results using box plots,
which graphically summarizes groups of numerical data
using quartiles. In these plot the orange line is the median,
the blue box below the median is the ﬁrst quartile, the blue
box above the median is the third quartile, the extended
long lines below and above the box are the maximum and
minimum values and the circles are outliers.

The results for John Hopkins university are shown in Figure
1. Notice in this ﬁgure that CRD performs better than ACL
and ACLopt, which both use ground truth information, see
parameter tuning in Subsection C.2. CRD performs simi-
larly to ACL+FlowI, where FlowI is a global algorithm, but
CRD is a local algorithm. Overall all methods have large
medians for this graph because the clusters with dorm 217
and year 2009 are clusters with low conductance compared
to the ones in other universities/colleges which we will dis-
cuss in the remaining experiments of this subsection.

The results for Rice university are shown in Figure 2. No-
tice that both clusters of dorm 203 and year 2009 for Rice
university are worse in terms of conductance compared to
the clusters of John Hopkins university. Therefore the per-
formance of the methods is decreased. For the cluster of
dorm 203 with conductance 0.46 CRD has larger median
than ACL, ACLopt and ACL+Flow in terms of precision.
The latter methods obtain larger median for recall, but this
is because ACL leaks lots of probability mass outside of
the ground truth cluster since as indicated by its large con-
ductance value many nodes in this cluster are connected ex-
ternally. For cluster of year 2009 CRD outperforms ACL,
which fails to recover the cluster because it leaks mass out-
side the cluster, FlowI corrects the problem and locates the
correct cluster at the expense of touching the whole graph.

Capacity Releasing Diffusion for Speed and Locality

Notice that all methods have a signiﬁcant amount of vari-
ance and outliers, which is also explained by the large con-
ductance values of the clusters.

The results for Simmons college are shown in Figure 3. No-
tice that Simmons college in Table 2 has two clusters, one
with poor conductance 0.47 for students of year 2007 and
one low conductance 0.1 for students of year 2009. The
former with conductance 0.47 means that the internal vol-
ume is nearly half the volume of the outgoing edges. This
has a strong implication in the performance of CRD, ACL
and ACLopt which get median precision about 0.5. This
happens because the methods push half of the ﬂow (CRD)
and half of the probability mass (ACL) outside the ground
truth cluster, which results in median precision 0.5. ACL
achieves about 20% more (median) recall than CRD but
this is because ACL touched more nodes than CRD during
execution of the algorithm. Notice that ACL+FlowI fails
for the cluster of year 2007, this is because FlowI is a global
algorithm, hence it ﬁnds a cluster that has low conductance
but it is not the ground truth cluster. The second cluster
of year 2009 has low conductance hence all methods have
large median performance with CRD being slightly better
than ACL, ACLopt and ACL+FlowI.

The results for Colgate university are shown in Figure 4.
The interesting property of the clusters in Table 2 for Col-
gate university is that their conductance varies from low 0.1
to large 0.48. Therefore in Figure 4 we see a smooth tran-
sition of performance for all methods from poor to good
In particular, for the cluster of year 2006
performance.
the conductance is 0.48 and CRD, ACL and ACLopt per-
form poorly by having median precision about 50%, recall
is slightly better for ACL but this is because we allow it
touch a bigger part of the graph. ACL+FlowI fails to locate
the cluster. For the cluster of year 2007 the conductance
is 0.41 and the performance of CRD, ACL and ACLopt is
increased with CRD having larger (median) precision and
ACL having larger (median) recall as in the previous clus-
ter. Conductance is smaller for the cluster of year 2008,
for which we observe substantially improved performance
for CRD with large median precision and recall. On the
contrary, ACL, ACLopt and ACL+FlowI have nearly 30%
less median precision in the best case and similar median
recall, but only because a large amount of probability mass
is leaked and a big part of the graph is touched which in-
cludes the ground truth cluster. Finally, the cluster of year
2009 has low conductance 0.11 and all methods have good
performance for precision and recall.

References

Traud, A. L., Mucha, P. J., and Porter, M. A. Social struc-
ture of facebook networks. Physica A: Statistical Me-
chanics and its Applications, 391(16):4165–4180, 2012.

220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274

Figure 1. Precision and recall results for John Hopkins university

Figure 2. Precision and recall results for Rice university

Figure 3. Precision and recall results for Simmons college

Capacity Releasing Diffusion for Speed and Locality

275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329

Figure 4. Precision and recall results for Colgate university

