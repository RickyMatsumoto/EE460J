Identiﬁcation and Model Testing in Linear Structural Equation Models using
Auxiliary Variables

Bryant Chen 1 Daniel Kumor 2 Elias Bareinboim 2

Abstract

We developed a novel approach to identiﬁca-
tion and model testing in linear structural equa-
tion models (SEMs) based on auxiliary variables
(AVs), which generalizes a widely-used family
of methods known as instrumental variables. The
identiﬁcation problem is concerned with the con-
ditions under which causal parameters can be
uniquely estimated from an observational, non-
causal covariance matrix. In this paper, we pro-
vide an algorithm for the identiﬁcation of causal
parameters in linear structural models that sub-
sumes previous state-of-the-art methods. In other
words, our algorithm identiﬁes strictly more co-
efﬁcients and models than methods previously
known in the literature. Our algorithm builds
on a graph-theoretic characterization of condi-
tional independence relations between auxiliary
and model variables, which is developed in this
paper. Further, we leverage this new character-
ization for allowing identiﬁcation when limited
experimental data or new substantive knowledge
about the domain is available. Lastly, we develop
a new procedure for model testing using AVs.

1. Introduction

The problem of estimating causal effects is one of the fun-
damental problems in the data-driven sciences. In order to
estimate a causal effect, the desired effect must be identiﬁed
or uniquely expressible in terms of the probability distribu-
tion over the available data. Causal effects are identiﬁed
by design in randomized control trials, but in many appli-
cations, such experiments are not possible. When only ob-
servational data is available, determining whether a causal

1IBM Research, San Jose, California, USA 2Purdue Uni-
versity, West Lafeyette,
Indiana, USA. Correspondence to:
Bryant Chen <bryant.chen@ibm.com>, Daniel Kumor <dku-
mor@purdue.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

effect is identiﬁed requires modeling the underlying causal
structure, which is generally done using structural equa-
tion models (SEMs) (also called structural causal models)
(Pearl, 2009; Bareinboim and Pearl, 2016).

A structural equation model consists of a set of equations
that describe the underlying data-generating process for a
set of variables. While SEMs, in their most general, non-
parametric form do not require any assumptions about the
form of these functions, in many ﬁelds, including machine
learning, psychology, and the social sciences, linear SEMs
are used. A linear SEM consists of a set of equations of
the form, X = ΛX + U , where X = [x1, ..., xn]t is a
vector containing the model variables, Λ is a matrix con-
taining the coefﬁcients of the model, and Λij represents the
direct effect of xi on xj, and U = [u1, ..., un]t is a vector
of normally distributed error terms, which represents omit-
ted or latent variables.1 The matrix Λ contains zeroes on
the diagonal, and Λij = 0 whenever xi is not a cause of
xj. The covariance matrix of X will be denoted by Σ and
the covariance matrix over the error terms, U, by Ω.
In
this paper, we will restrict our attention to semi-Markovian
models (Pearl, 2009), models where the rows of Λ can be
arranged so that it is lower triangular, and the correspond-
ing graph is acyclic.

When modeling using SEMs, researchers typically specify
the model by setting certain entries of Λ and Ω to zero (i.e.
exclusion and independence restrictions), while leaving the
rest of the entries as free parameters to be estimated from
data2. Restricting a particular entry Λij to zero reﬂects the
assumption that Yi has no direct effect on Yj. Similarly,
restricting Ωij to zero reﬂects the assumption that there are
no unobserved common causes of both Yi and Yj. Once the

1Instrumental and auxiliary variables can also be used when
normality is not assumed, but to simplify the proofs in the paper,
we will, as is commonly done by empirical researchers, assume
normality.

2There are a number of algorithms for discovering the model
structure from data(Spirtes et al., 2000; Shimizu et al., 2006;
Pearl, 2009; Zhang and Hyv¨arinen, 2009; Mooij et al., 2016).
However, it is only in very rare instances that these methods are
able to uniquely determine the model structure. As a result, model
speciﬁcation generally utilizes knowledge about the domain under
study.

Identiﬁcation and Model Testing using AVs

parameters are estimated, causal effects (as well as coun-
terfactual quantities) can be computed from the structural
coefﬁcients directly (Pearl, 2009; Chen and Pearl, 2014).
However, in order to be estimable from data, a parameter
must ﬁrst be identiﬁed. In some cases, the modeling as-
sumptions are not strong enough, and there are multiple,
often inﬁnite, values for the parameter that are consistent
with the observed data. As a result, two fundamental prob-
lems in SEMs are to identify and estimate the model pa-
rameters and to test the underlying assumptions that enable
identiﬁcation.

The problem of identiﬁcation has been studied extensively
by econometricians and social scientists (Fisher, 1966;
Bowden and Turkington, 1984; Bekker et al., 1994; Rig-
don, 1995) and more recently by the AI and statistics com-
munities using graphical methods (Spirtes et al., 1998;
Tian, 2007; 2009; Brito and Pearl, 2002a;c; 2006; Barein-
boim and Pearl, 2016). To our knowledge, the most gen-
eral, efﬁcient algorithm for model identiﬁcation is the g-HT
algorithm given by Chen (2016) combined with ancestor
decomposition (Drton and Weihs, 2016). This method gen-
eralizes the half-trek algorithm of Foygel et al. (2012) and
utilizes ancestor decomposition, which expands on an idea
by Tian (2005) where the model is decomposed into sim-
pler sub-models. Graphical methods have also been applied
to the problem of testing the causal assumptions embedded
in an SEM. For example, d-separation (Pearl, 2009) and
overidentiﬁcation (Pearl, 2004; Chen et al., 2014) provide
the means to discover testable implications of the model,
which can be used to test it against data.

Despite decades of attention and work from diverse ﬁelds,
the identiﬁcation problem3 has still not been efﬁciently
solved4. There are identiﬁable parameters and models that
none of the above methods are able to identify. Simi-
larly, there are testable implications of SEMs that the above
methods are unable to detect. One promising avenue to aid
in both tasks are auxiliary variables (Chen et al., 2016).
Each of the aforementioned methods for identiﬁcation and
model testing only utilizes restrictions on the entries of Λ
and Ω to zero. Auxiliary variables can be used to incorpo-
rate knowledge of non-zero coefﬁcient values into existing
methods for identiﬁcation and model testing. These coefﬁ-
cient values could be obtained, for example, from a previ-
ously conducted randomized experiment, from substantive
understanding of the domain, or even from another iden-
tiﬁcation technique. The intuition behind auxiliary vari-

3To be precise, we are referring to the problem of identiﬁ-
cation almost everywhere (Brito and Pearl, 2002b), also called
generic identiﬁcation (Foygel et al., 2012).

4An exhaustive procedure can be obtained using Gr¨obner
bases methods (Foygel et al., 2012). However, these methods
are computationally intractable for anything but the smallest of
graphs.

ables is simple: if the coefﬁcient from variable w to z, β,
is known, then we would like to remove the direct effect of
w on z by subtracting it from z. This removal eliminates
confounding paths through w and is performed by creating
a variable z∗ = z − βw, which is used as a proxy for z. In
many cases, z∗ allows the identiﬁcation of parameters or
testable implications using existing methods when z could
not.

Chen et al. (2016) demonstrated how auxiliary variables
could be utilized in simple instrumental sets (instrumen-
tal sets that do not utilize conditioning to block spurious
paths) (Brito and Pearl, 2002a; van der Zander et al., 2015)
and proved that any model identiﬁable using the g-HT al-
gorithm is also identiﬁable using auxiliary simple instru-
mental sets.

Since auxiliary variables allow knowledge of non-zero co-
efﬁcient values to be incorporated into existing methods
for identiﬁcation, they are also directly applicable to the
problem of z-identiﬁcation (Bareinboim and Pearl, 2012),
in which partial experimental data is available. Addition-
ally, the cancellation of paths that results from adding an
AV may result in conditional independence constraints be-
tween the AV and other variables that can be used to test
the model.

In this paper, we generalize the results of Chen et al. (2016)
and demonstrate how auxiliary variables can be utilized in
generalized instrumental sets, which allow for condition-
ing to block spurious paths. We prove that, unlike aux-
iliary simple instrumental sets, this generalization strictly
subsumes the g-HT algorithm. Additionally, we introduce
quasi-instrumental sets, which utilize auxiliary variables
to identify coefﬁcients when partial experimental data is
available. Quasi-instrumental sets are incorporated into
our identiﬁcation algorithm, allowing it to better address
the problem of z-identiﬁcation. To our knowledge, this
algorithm is the ﬁrst systematic method for tackling z-
identiﬁcation in linear systems. We also demonstrate how
auxiliary instrumental sets and quasi-instrumental sets can
be used to derive over-identifying constraints, which can
be used to test the model speciﬁcation against data. More-
over, we prove that these overidentifying constraints sub-
sume conditional independence constraints among auxil-
iary variables. Lastly, we discuss related work, showing
how auxiliary IVs are able to unite a variety of disparate
methods under a single framework.

2. Preliminaries

The causal graph or path diagram of an SEM is a graph,
G = (V, D, B), where V are nodes or vertices, D directed
edges, and B bidirected edges. The nodes represent model
variables. Directed eges encode the direction of causal-

Identiﬁcation and Model Testing using AVs

ity, and for each coefﬁcient Λij (cid:54)= 0, an edge is drawn
from xi to xj. Each directed edge, therefore, is associ-
ated with a coefﬁcient in the SEM, which we will often
refer to as its structural coefﬁcient. Additionally, when it is
clear from context, we may abuse notation slightly and use
coefﬁcients and directed edges interchangeably. The error
terms, ui, are not shown explicitly in the graph. However,
a bidirected edge between two nodes indicates that their
corresponding error terms may be statistically dependent
while the lack of a bidirected edge indicates that the error
terms are independent.

We will use standard graph terminology with P a(y) denot-
ing the parents of y, Anc(y) denoting the ancestors of Y ,
De(y) denoting the descendants of y, and Sib(y) denoting
the siblings of y, the variables that are connected to y via
a bidirected edge. He(E) denotes the heads of a set of di-
rected edges, E, while T a(E) denotes the tails. Addition-
ally, for a node v, the set of edges for which He(E) = v is
denoted Inc(v). Lastly, we will utilize d-separation (Pearl,
2009).

We will use σ(x, y|W ) to denote the partial covariance be-
tween two random variables, x and y, given a set of vari-
ables, W , and σ(x, y|W )G as the partial covariance be-
tween random variables x and y given W implied by the
graph G. We will assume without loss of generality that
the model variables have been standardized to mean 0 and
variance 1.

Deﬁnition 1. For a given unblocked (given the empty set)
path, π, from x to y, Left(π) is the set of nodes, if any,
that has a directed edge leaving it in the direction of x in
addition to x. Right(π) is the set of nodes, if any, that has a
directed edge leaving it in the direction of y in addition to
y.

For example, consider the path π = x ← vL
1 ← ... ←
k ← vT → vR
vL
j → ... → vR
1 → y. In this case, Left(π)
= ∪k
i=1vL
i ∪ {y, vT }.
i=1vR
vT is a member of both Right(π) and Left(π).

i ∪ {x, vT } and Right(π) = ∪j

Deﬁnition 2. A set of paths, π1, ..., πn, has no sided inter-
section if for all πi, πj ∈ {π1, ..., πn} such that πi (cid:54)= πj,
Left(πi)∩Left(πj)=Right(πi)∩Right(πj) = ∅.

Wright’s rules (Wright, 1921) allow us to equate the model-
implied covariance, σ(x, y)M , between any pair of vari-
ables, x and y,
to the sum of products of parameters
along unblocked paths between x and y.5 Let Π =

5Wright’s rules characterize the relationship between the co-
variance matrix and model parameters. Therefore, any question
about identiﬁcation using the covariance matrix can be decided
by studying the solutions for this system of equations. However,
since these equations are polynomials and not linear, it can be
very difﬁcult to analyze identiﬁcation of models using Wright’s
rules.

{π1, π2, ..., πk} denote the unblocked paths between x and
y, and let pi be the product of structural coefﬁcients along
path πi. Then the covariance between variables x and y is
(cid:80)

i pi.

Lastly, we deﬁne auxiliary variables and the augmented
graph.
Deﬁnition 3 (Auxiliary Variable). Given a linear SEM with
graph G and a set of edges E whose coefﬁcient values
are known, an auxiliary variable is a variable, z∗ = z −
(cid:80)
i eiti, where {e1, ..., ek} ⊆ E ∩ Inc(z) and ti = T a(ei)

for all i ∈ {1, ..., k}.

If not otherwise speciﬁed, z∗ refers to the auxiliary vari-
able, z − c1t1 − ... − cltl, where {c1, ..., cl} are the co-
efﬁcients of E ∩ Inc(z) and E is the set of directed
edges whose coefﬁcient values are known. In other words,
z∗ is the auxiliary variable for z where as many known
coefﬁcients are subtracted out as possible. Chen et al.
(2016) demonstrated that the covariance between any aux-
iliary variables and model variables can be computed using
Wright’s rules on the augmented graph, deﬁned below.
Deﬁnition 4. (Chen et al., 2016) Let M be a linear SEM
with graph G and a set of directed edges E such that their
coefﬁcient values are known. The E-augmented model,
M E+, includes all variables and structural equations of
M in addition to new auxiliary variables, y∗
k, one
for each variable in He(E) = {y1, ..., yk} such that the
structural equation for y∗
is y∗
i , where
i
Xi = T a(E) ∩ P a(yi), for all i ∈ {1, ..., k}. The corre-
sponding augmented graph is denoted GE+.

i = yi − ΛXiyiT t

1, ...y∗

If the value of β is
For example, consider Figure 1a.
known, we can generate an auxiliary variable x∗ = x − βt.
The β-augmented graph Gβ+ is depicted in Figure 1b. In
some cases, x∗ allows the identiﬁcation of coefﬁcients and
testable implications using existing methods when x could
not, due to the fact that the back-door paths from x to y that
go through β cancel with the back-door paths from x∗ to y
that go through −β. This can be seen by expressing the
covariance of x∗ and y in terms of the model parameters
using Wright’s rules.

3. Auxiliary and Quasi-Instrumental Sets

Two, perhaps the most common, methods for estimat-
ing causal effects are OLS regression and two-stage least-
squares (2SLS) regression. Both of these methods as-
sume that the underlying causal relationships between vari-
ables are linear, in addition to other causal assumptions that
guarantee identiﬁcation. The single-door criterion (Pearl,
2009) graphically characterizes when the assumptions suf-
ﬁcient to estimate a causal effect using regression are sat-
isﬁed in a linear SEM. Similarly, Brito and Pearl (2002a)
gave a graphical characterization for when a variable z

Identiﬁcation and Model Testing using AVs

(a)

(b)

(c)

Figure 1. (a) α is not identiﬁed using IVs (b) α is identiﬁed using x∗ as an auxiliary IV given w1 (c) conditioning on descendants of x
induces correlation between x∗ and y

qualiﬁes as an IV so that 2SLS regression provides a con-
sistent estimate of the causal effect. In this section, we give
a graphical criterion for when AVs can be utilized in gen-
eralized instrumental sets, which extends both the single-
door criterion and IVs. Additionally, we introduce quasi-
instrumental sets, which utilize AVs to better address the
problem of z-identiﬁcation.

First, we give a simple graphical criterion for when an AV
would be conditionally independent of another variable,
which will allow us to incorporate AVs into instrumental
sets, as well as other identiﬁcation and model testing meth-
ods that require the ability to detect conditional indepen-
dence in the graph.

Theorem 1. Given a linear SEM with graph G, where E ⊆
Inc(z) is a set of edges whose coefﬁcient values are known,
if W ∪ {y} does not contain descendants of z and GE−
represents the graph G with the edges for E removed, then
(z∗

|=y|W )GE+ if and only if (z|=y|W )GE− .6

Proof. Proofs for all theorems and lemmas can be found in
the Appendix (Chen et al., 2017).

Next, we demonstrate how AVs can be incorporated into
generalized instrumental sets, deﬁned below.

6The theorem disallows descendants of the generating vari-
able in the conditioning set. At ﬁrst glance, this may appear to
limit the ability to block biasing paths among AVs. However, we
conjecture that if z cannot be separated from y in G, then z∗ will
almost surely not be independent of y given W , if W contains
descendants of z. To illustrate, consider the example shown in
Figure 1c. x∗ = x − βt is independent of y, as can be veri-
ﬁed using Wright’s rules, but x∗ is not independent of y given
d! An intuitive explanation for this surprising result is that con-
ditioning on d, a descendant of x, in Figure 1c induces correla-
tion between the error term of x and t, since x acts as a “virtual
collider”. As a result, we have a “virtual path” from x∗ to y,
x∗ ← x ← ux ↔ t → y. See Pearl (2009, p. 339) for a detailed
discussion of virtual colliders.

Theorem 2. (Brito and Pearl, 2002a) Given a linear model
with graph G, the coefﬁcients for a set of edges E =
{(x1, y), ..., (xk, y)} are identiﬁed if there exists triplets
(z1, W1, p1), ..., (zk, Wk, pk) such that for i = 1, ..., k,

(i) (zi|=y|Wi)GE− , where W does not contain any de-
scendants of y and GE− is the graph obtained by
deleting the edges, E from G,

(ii) pi is a path between zi and xi that is not blocked by

(iii) the set of paths, {p1, ..., pk} has no sided intersec-

Wi, and

tion.7

If the above conditions are satisﬁed, we say that Z is a gen-
eralized instrumental set for E or simply an instrumental
set for E.8

In some cases, a variable z may not satisfy condition (i)
above but an auxiliary variable z∗ does. For example, in
Figure 1a, we cannot identify α using Theorem 2. Block-
ing the path x ← t ↔ y by conditioning on t opens the
path, x ↔ t ↔ y. Moreover, we cannot use t or s in
an instrumental set due to the edges t ↔ y and s ↔ y.
However, s is an IV for β, allowing us to generate an AV,
x∗ = x − β · t1, as in Figure 1b. Now, α can be identiﬁed
using x∗ as an auxiliary instrument given w1.

Theorem 1 tells us when (i) of Theorem 2 can be satisﬁed
using an AV, z∗
i . We simply check whether zi can be sep-
arated from y in GE∪Ez−, where Ez ⊆ Inc(zi) is the set
of zi’s edges whose coefﬁcient values are known. When an
instrumental set includes AVs, we call the set an auxiliary
instrumental set or auxiliary IV set for short.

7Brito and Pearl (2002a) provided an alternative statement of
condition (iii). A proof that the two statement are, in fact, equiv-
alent is given in the Appendix (Chen et al., 2017).

8Note that when k = 1, z1 is an IV for (x1, y). Further, if

z1 = x1, then x1 satisﬁes the single-door criterion for (x1, y).

Identiﬁcation and Model Testing using AVs

(a)

(b)

(ii) for i = 1, ..., k, pi is a path between zi and xi that is

not blocked by Wi, where xi = He(αi), and

(iii) the set of paths {p1, ..., pk} has no sided intersection

Theorem 3. If Z ∗ is a quasi-instrumental set for E, then
E is identiﬁable.

Lastly, the following corollary provides a simple graphical
condition for when a single variable or AV qualiﬁes as a
quasi-IV.
Corollary 1. Given a linear SEM with graph G, z∗ is a
quasi-IV for α given W if W does not contain any descen-
dants of z, and z is an IV for α given W in GEz∪Ey−,
where Ez ⊆ Inc(z) and Ey ⊆ Inc(y) are sets of edges
whose coefﬁcient values are known.

Auxiliary and quasi-IV sets enable a bootstrapping proce-
dure whereby complex models can be identiﬁed by itera-
tively identifying coefﬁcients and using them to generate
new auxiliary variables. For example, consider Figure 3a.
First, we are able to identify b and c using IVs, but no other
coefﬁcients. Once b is identiﬁed, Corollary 1 tells us that e
is identiﬁed using v∗
3 since v3 is an IV for e when the edge
for b is removed (see Figure 3b). Now, the identiﬁcation of
e allows us to identify a and d using v∗
5, since v5 is an IV
for a and d when the edge for e is removed (see Figure 3c).
This general strategy is the basis for our identiﬁcation, z-
identiﬁcation, and model testing algorithm, described next.

4. Identiﬁcation and z-Identiﬁcation

Algorithm

In this section, we construct an identiﬁcation algorithm
that operationalizes the bootstrapping approach described
in Section 3. First, we describe how to algorithmically ﬁnd
a quasi-instrumental set for a set of coefﬁcients E, given a
set of known coefﬁcients, IDEdges.

The problem of ﬁnding generalized instrumental sets was
addressed by van der Zander and Liskiewicz (2016). They
provided an algorithm, TestGeneralIVs, that determines
whether a given set Z is a generalized instrumental set for
a set of edges, E, that runs in polynomial time if we bound
the size of the coefﬁcient set to be identiﬁed. More specif-
ically, their algorithm has a running time of O((k!)2nk),
where n is the number of variables in the graph and k =
|E|.9

Our method, TestQIS, given in the Appendix (Chen et al.,
2017), generalizes TestGeneralIVs, for quasi-IV sets.

9van der Zander and Liskiewicz (2016) also give an algorithm
that tests whether Z is a simple conditional instrumental sets in
O(nm) time. A simple conditional instrumental set is a general-
ized instrumental set where W1 = W2 = ... = Wk

Figure 2. (a) α is not identiﬁed using IVs (b) α is identiﬁed using
Z as a quasi-IV after adding auxiliary variable Y ∗

Figure 1a also demonstrates the importance of extending
the simple auxiliary instrumental sets introduced by Chen
et al. (2016) to allow for conditioning. α can only be iden-
tiﬁed if we block the paths x ↔ w1 → y and x ↔ w1 →
w2 → y by conditioning on w1.

When knowledge of coefﬁcient values are known a priori,
it may be helpful to generate an AV from the outcome vari-
able y. For example, in Figure 2a, α cannot be identiﬁed.
However, suppose that it is possible to run a surrogate ex-
periment and randomize z. This experiment would allow
us to estimate γ and generate the AV, Y ∗ = Y − γZ. Now,
z is not technically an instrument for α, but it can be shown
that α = rY ∗Z.W
. Chen et al. (2016) called such variables
rXZ
quasi-instrumental variables or quasi-IVs for short.

Interestingly, while quasi-IVs are valuable for the problem
of z-identiﬁcation, they do no better than instrumental sets
when applied to the standard identiﬁcation problem, where
no external knowledge of coefﬁcient values is available.
For example, consider again Figure 2a. In order to use z
as a quasi-IV for α, we would ﬁrst have to identify γ using
an IV. If such a variable existed, say z(cid:48), then we could have
simply identiﬁed {α, γ} using the IV set {z, z(cid:48)}.

Next, we formally deﬁne quasi-instrumental sets or quasi-
IV sets for short. Note that auxiliary IV sets are also quasi-
IV sets.

Deﬁnition 5. Given a linear SEM with graph G, a set of
edges EK whose coefﬁcient values are known, and a set of
structural coefﬁcients α = {α1, α2, ..., αk}, the set Z =
{z1, ..., zk} is a quasi-instrumental set if there exist triples
(z1, W1, p1), ..., (zk, Wj, pk) such that:

(i) For i = 1, ..., k, either:

(a) the elements of Wi are non-descendants of y,
where Ey = EK ∩

and (zi|=y|Wi)GE∪Ey
Inc(y).

(b) the elements of Wi are non-descendants of zi
where Ezy =

and y, and (zi|=y|Wi)GE∪Ezy
EK ∩ (Inc(z) ∪ Inc(y)).

Identiﬁcation and Model Testing using AVs

(a)

(b)

(c)

Figure 3. (a) b is identiﬁed using either v2 or v1 as an instrument and c is identiﬁed using w as an instrument (b) e is identiﬁed using v∗
3
as an auxiliary instrument given (c) a and d are identiﬁed using v∗

5 as an auxiliary instrument

FindQIS, also given in the Appendix (Chen et al., 2017),
searches for a quasi-IV set by checking all subsets of Z ⊆
(Anc(zi) ∪ Anc(y)) using TestQIS. It returns a quasi-IV
set, as well as its conditioning sets, if one exists.

{w1}. Now, {d} is identiﬁed using y∗ = y − bx, and e
is identiﬁed using t∗
2. In the second iteration, we return to
{b, c, f } and ﬁnd that it is now identiﬁed using the auxiliary
IV set, {x, w1, t∗

3}.

(cid:48)

(cid:48)

(cid:48)

(cid:48)

In some cases an instrumental set may not exist for C,
, where C ⊂ C
but one exists for C
. Conversely, there
may not be an instrumental set for C
, but there is one for
C ⊂ C
. As a result, we may have to check all possi-
ble subsets of a variable’s coefﬁcients in order to determine
whether a given subset is identiﬁable using auxiliary in-
strumental sets. This search can be simpliﬁed somewhat
by noting that if E is a connected edge set (deﬁned below)
with no instrumental set, then there is no superset E
with
an instrumental set.
Deﬁnition 6. (Chen et al., 2014) For an arbitrary variable,
V , let P a1, P a2, ..., P ak be the unique partition of Pa(V)
such that any two parents are placed in the same subset,
P ai, whenever they are connected by an unblocked path. A
connected edge set with head V is a set of directed edges
from P ai to V for some i ∈ {1, 2, ..., k}.

(cid:48)

The ID algorithm, called qID utilizes FindQIS to iden-
tify as many coefﬁcients as possible in a given model with
graph G. It iterates through each connected edge set and
attempts to identify it using FindQIS.
If it is unable to
identify the connected edge set, it then attempts to iden-
tify subsets of the connected edge set. After the algo-
rithm has attempted to identify each connected edge set, it
again attempts to identify each unidentiﬁed connected edge
set, since each newly identiﬁed coefﬁcient may enable the
identiﬁcation of previously unidentiﬁable coefﬁcients. This
process is repeated until all coefﬁcients have been identi-
ﬁed or no new coefﬁcients have been identiﬁed in the last
iteration. The algorithm is polynomial if the degree of each
node in the graph is bounded.

Our algorithm identiﬁes the model depicted in Figure 4b in
the following way. First, let us assume that the connected
edge sets are arbitrarily ordered, ({a}, {b, c, f }, {d}, {e}).
Now, the ﬁrst edge to be identiﬁed would be a using w1
as an IV. There is no auxiliary IV set for {b, c, f }, and we
would attempt to ﬁnd one for its subsets. We ﬁnd that {b}
is identiﬁed using {x} as an IV set with conditioning set

Algorithm 1 qID(G, Σ, IDEdges)

Initialize: EdgeSets ← all connected edge sets in G
repeat

for all ES in EdgeSets such that
ES (cid:54)⊆ IDEdges do
y ← He(ES)
for all E ⊆ ES such that E (cid:54)⊆ IDEdges do
(Z, W ) ← FindQIS(G, ES, IDEdges)
if (Z, W ) (cid:54)=⊥ then

Identify ES using Z ∗ as an auxiliary
instrumental set in G(IDEdges∩Inc(Z))+
IDEdges ← IDEdges ∪ ES

end if

end for

end for

until All coefﬁcients have been identiﬁed or no coefﬁ-
cients have been identiﬁed in the last iteration

In contrast, Figure 4b is not identiﬁed using simple instru-
mental sets and auxiliary variables. We cannot identify b
without conditioning on w1, which means that the only co-
efﬁcients identiﬁed using auxiliary simple instrumental sets
is a. Since Chen et al. (2016) showed that any coefﬁcient
identiﬁed using the generalized half-trek criterion (g-HTC)
can be identiﬁed using auxiliary variables and simple in-
strumental sets, we know that qID is able to identify coef-
ﬁcients and models that the g-HT algorithm is not. More-
over, qID will identify any coefﬁcients that are identiﬁable
using auxiliary variables and simple instrumental sets, giv-
ing us the following theorem.

Theorem 4. Given an arbitrary linear causal model, if a
set of coefﬁcients is identiﬁable using the g-HT algorithm,
then it is identiﬁable using qID. Additionally, there are
models that are not identiﬁed using the g-HT algorithm,
but identiﬁed using qID.

Identiﬁcation and Model Testing using AVs

(b)

(a)

necessary to generate the AV. The key point is that this
identiﬁcation cannot rely on the same lack of edge whose
existence we are trying to test!

In the above example, we identiﬁed α using z1 as an IV.
σ(z2, y∗) = 0 follows from the lack of edge between z2
and y. However, even if this edge did exist, z∗ still equals
z − σ(y,z1)
σ(x,z1) x. In contrast, σ(z1, y∗) = 0 follows from the
lack of edge between z1 and y. The existence of this edge
would disallow z1 as an instrument and z∗ = z − αx (cid:54)=
z − σ(y,z1)

σ(x,z1) x.

σ(x,z1) = σ(y,z2)

Another way to derive the constraint σ(z2, y∗) = 0 is via
overidentiﬁcation. α can be identiﬁed using either z1 or z2
and equating the corresponding expressions yields the con-
straint σ(y,z1)
σ(x,z2) , which is clearly equivalent to the
previous constraint σ(z2, y∗) = 0. In fact, we show (Theo-
rem 6) that whenever a variable z cannot be separated from
another variable y, but z∗ can be, the resulting AV condi-
tional independence, if it is non-vacuous, is equivalent to an
overidentifying constraint that can be derived using quasi-
IVs. As a result, all non-vacuous AV conditional indepen-
dences are captured by overidentifying constraints derived
using quasi-IVs!

First, we give a sufﬁcient condition for when a set of edges
α is overidentiﬁed.

Theorem 5. Let Z be a quasi-IV set for structural coefﬁ-
cients α = {α1, ..., αk} and E be a set of known edges. If
there exists a node s satisfying the conditions listed below,
then α is overidentiﬁed and we obtain the constraint .

Figure 4. (a) σ(z2, y∗) = 0, where y∗ = y − σ(y,z1)
σ(x,z1) , and, equiv-
alently, α is overidentiﬁed using z1 and z2 as IVs (b) the model
is identiﬁed using auxiliary instrumental sets, but not the g-HT
algorithm

5. Deriving Testable Implications using AVs

Theorem 1 also enables us to derive new vanishing partial
correlation constraints that can be used to test the model.
For example, in Figure 4a, α can be identiﬁed using z1 as
an instrument. Once α is identiﬁed, we can generate the
AV y∗ = y − αx = y − σ(y,z1)
σ(x,z1) x, and Theorem 1 tells us
that the correlation of z2 and y∗ should vanish. As a result,
we can test the model speciﬁcation by verifying that this
constraint holds in the data.

Theorem 1 also tells us that the correlation between z1 and
y∗ should also vanish. However, upon closer inspection,
we ﬁnd that this implication does not actually constrain the
covariance matrix:

σ(z1, y∗) =σ(z1, y − αx)

=σ(z1, y) −

σ(z1, x) = 0.

(i) s /∈ Z

σ(y, z1)
σ(x, z1)

In other words, our “testable implication” that σ(z1, y∗) =
0 is equivalent to stating σ(z1, y) − σ(z1, y) = 0–a tautol-
ogy! In contrast,

σ(z2, y∗) = σ(z2, y) −

σ(z2, x) = 0

σ(z1, y)
σ(x, z1)

does provide a true testable implication.

Shpitser et al. (2009) noticed a similar phenomenon when
deriving dormant independences in non-parametric mod-
els, and their explanation applies to conditional indepen-
dence constraints among AVs as well. The idea is the fol-
lowing: When the model implies that two variables are con-
ditionally independent, it relies on the modeled assumption
that there is no edge between those variables. As a result,
verifying that the constraint holds in data represents a test
that this assumption is valid. However, unlike conditional
independence constraints between model variables, condi-
tional independence constraints among AVs rely upon the
absence of certain edges in order to identify the coefﬁcients

(ii) There exists an unblocked path between s and y in-

cluding an edge in α

(iii) There exists a conditioning set W that does not block

the path p, such that either:

(a) the elements of W are non-descendants of y, and
(s|=y|W )Gα∪Ey −, where Ey = E ∩ Inc(y))
(b) the elements of W are non-descendants of s and
y, and (s|=y|W )Gα∪Es∪Ey − where Es = E ∩
Inc(s).

The above theorem can be used to derive an overidentifying
constraint for every variable that satisﬁes (i)-(iii) above. It
can also be applied when α is known a priori, yielding a
z-overidentifying constraint. In this case, Z = ∅ would be
a quasi-IV set that trivially identiﬁes α.

The following theorem states that non-vacuous AV condi-
tional independence constraints are subsumed by quasi-IV
overidentifying and z-overidentifying constraints.

Identiﬁcation and Model Testing using AVs

Theorem 6. Let z∗ = z − e1t1 − ... − ektk and suppose
there does not exist W such that (z|=y|W )G. There ex-
ists W such that W ∩ De(z) = ∅ and (z∗
|=y|W ) is non-
vacuous if and only if y satisﬁes the conditions of Theorem
5 for E = {e1, ..., ek}.

The above theorem also applies when y is an AV, called
y∗.
In this case, we simply replace (z|=y|W )G with
(z|=y∗|W )GEy +, where Ey ⊆ Inc(y) is a set of edges
whose coefﬁcient values are known.

Algorithm 2 uses quasi-IV sets to output overidentiﬁying
constraints in a graph given an optional set of identiﬁed
edges. It uses isEIV, which is a slightly modiﬁed version
of FindQIS that tests whether w ﬁts the conditions of The-
orem 6. Details of isEIV can be found in the Appendix
(Chen et al., 2017).

Algorithm 2 Finds overidentifying constraints for G

function CONSTRAINTFINDER(G,Σ,IDEdges)

for all ES ∈ Edge Sets of G do

(Z, W ) ← FINDQIS(ES,G,IDEdges)
if (Z, W ) (cid:54)= ⊥ then

for all w ∈ V \ Z ∪ {He(ES)} do
if ISEIV(w,ES,G,IDEdges) then
Add constraint awA−1b = bw

end if

end for

end if

end for
end function

6. Discussion and Related Work

In this section, we discuss how (single-variable) auxiliary
IVs encompass a number of previous identiﬁcation meth-
ods developed in economics (Hausman and Taylor, 1983),
computer science (Chan and Kuroki, 2010), and epidemi-
ology (Shardell, 2012).

Hausman and Taylor (1983) showed that if the equation for
a given variable, z = β1p1 + ... + βkpk + uz, is identiﬁed,
then the error term uz can be estimated and used as an in-
strument for other coefﬁcients. In this case, the auxiliary
variable z∗ = z − β1p1 − ... − βkpk is equal to the error
term uz. As a result, whenever the error term is estimable
and can be used as an IV, we can also generate an auxil-
iary instrument. However, there are times when only some
of the coefﬁcients in an equation are identiﬁable, and as a
result, the error term cannot be used as an instrument, but
we can nevertheless generate an auxiliary instrument. As a
result, auxiliary IVs strictly subsume error term IVs.

Chan and Kuroki (2010) gave sufﬁcient conditions for
when a descendant of x and a descendant of y could be

used in analogous manner to IVs to identify the effect of
x on y. In the context of AVs, this method is equivalent
to generating an auxiliary instrument from the descendant
by subtracting the total effect of x on the descendant or the
total effect of y on the descendant (depending on whether
the variable is a descendant of x or y). In this paper, we
generated AVs by subtracting out direct effects, but clearly
the work can be extended to subtracting out total effects.
The beneﬁt of AVs over these descendant IVs is that they
can be generated from a variety of variables, not just de-
scendants of x and y. Additionally, descendants of x or y
can generate AVs from other total or direct effects, not just
the effect of x or y on the descendant.

The notion of “subtracting out a direct effect” in order to
turn a variable into an instrument was also noted by Shard-
ell (2012) when attemping to identify the total effect of x
on y. It was noticed that in certain cases, the violation of the
independence restriction of a potential instrument z (i.e. z
is not independent of the error term of y) could be remedied
by identifying, using ordinary least squares regression, and
then subtracting out the necessary direct effects on y. AVs
generalize and operationalize this notion so that it can be
used on arbitrary sets of known coefﬁcient values and be
utilized in conjunction with existing graphical methods for
identiﬁcation and enumeration of testable implications.

Additionally, as we have alluded to earlier, the highly al-
gebraic, state-of-the-art g-HTC can also be understood in
terms of auxiliary instruments. Identiﬁcation using the g-
HTC is equivalent to identiﬁcation using auxiliary simple
instrumental sets.

In summary, auxiliary instruments are not only the basis
for the most general identiﬁcation algorithm yet devised,
but they also unify disparate identiﬁcation methods under
a single framework. Moreover, AVs are directly applicable
to the tasks of z-identiﬁcation and model testing. Finally,
they can, in principle, enhance any method for identiﬁca-
tion, model testing, or other tasks that relies on graphical
separation.

7. Conclusion

In this paper, we graphically characterized conditional in-
dependence among AVs, allowing us to demonstrate how
they can help generalized instrumental sets in the problem
of identiﬁcation. We provided an algorithm that identiﬁes
more models than the g-HT algorithm, subsuming the state-
of-the-art for identiﬁcation in linear models. Additionally,
we introduced quasi-IV sets, and constructed an algorithm
that utilizes them to attack the problem of z-identiﬁcation.
Finally, we proved that AV conditional independences are
subsumed by overidentifying constraints and gave an algo-
rithm for deriving overidentifying constraints.

Identiﬁcation and Model Testing using AVs

Acknowledgements

We would like to thank Judea Pearl, Mathias Drton,
Thomas Richardson, and Luca Weihs for helpful discus-
sions. This research was supported in parts by grants from
NSF #IIS-1302448 and #IIS-1527490 and ONR #N00014-
13-1-0153 and #N00014-13-1-0153.

References

BAREINBOIM, E. and PEARL, J. (2012). Causal inference
by surrogate experiments: z-identiﬁability. In Proceed-
ings of the Twenty-Eighth Conference on Uncertainty
in Artiﬁcial Intelligence (N. de Freitas and K. Murphy,
eds.). AUAI Press, Corvallis, OR.

BAREINBOIM, E. and PEARL, J. (2016). Causal infer-
ence and the data-fusion problem. Proceedings of the
National Academy of Sciences 113 7345–7352.

BEKKER, P., MERCKENS, A. and WANSBEEK, T. (1994).
Identiﬁcation, Equivalent Models, and Computer Alge-
bra. Statistical Modeling and Decision Science, Aca-
demic Press.

BOWDEN, R. and TURKINGTON, D. (1984). Instrumen-
tal Variables. Cambridge University Press, Cambridge,
England.

BRITO, C. and PEARL, J. (2002a). Generalized instrumen-
In Uncertainty in Artiﬁcial Intelligence,
tal variables.
Proceedings of the Eighteenth Conference (A. Darwiche
and N. Friedman, eds.). Morgan Kaufmann, San Fran-
cisco, 85–93.

BRITO, C. and PEARL, J. (2002b). A graphical criterion
for the identiﬁcation of causal effects in linear models.
In Proceedings of the Eighteenth National Conference
on Artiﬁcial Intelligence. AAAI Press/The MIT Press,
Menlo Park, CA, 533–538.

BRITO, C. and PEARL, J. (2002c). A new identiﬁcation
condition for recursive models with correlated errors.
Journal Structural Equation Modeling 9 459–474.

BRITO, C. and PEARL, J. (2006). Graphical condition for
identiﬁcation in recursive SEM. In Proceedings of the
Twenty-Third Conference on Uncertainty in Artiﬁcial In-
telligence. AUAI Press, Corvallis, OR, 47–54.

CHAN, H. and KUROKI, M. (2010). Using descendants
as instrumental variables for the identiﬁcation of direct
In Proceedings of the
causal effects in linear sems.
Thirteenth International Conference on Artiﬁcial Intel-
ligence and Statistics (AISTATS).

CHEN, B. (2016). Identiﬁcation and overidentiﬁcation of
linear structural equation models. In Advances In Neural
Information Processing Systems. 1579–1587.

CHEN, B., KUMOR, D. and BAREINBOIM, E. (2017).
Identiﬁcation and model testing in linear structural equa-
tion models using auxiliary variables. arXiv preprint
arXiv:1612.03451; Technical report R-27, Purdue AI
Lab, Dept. of Computer Science, Purdue University. .

CHEN, B. and PEARL, J. (2014). Graphical tools for
linear structural equation modeling.
Tech. Rep. R-
432, <http://ftp.cs.ucla.edu/pub/stat ser/r432.pdf>, De-
partment of Computer Science, University of California,
Los Angeles, CA. Forthcoming, Psychometrika.

CHEN, B., PEARL, J. and BAREINBOIM, E. (2016). Incor-
porating knowledge into linear structural equation mod-
In Proceedings of the
els using auxiliary variables.
Twenty-ﬁfth International Joint Conference on Artiﬁcial
Intelligence (S. Kambhampati, ed.).

of

linear

CHEN, B., TIAN, J. and PEARL, J. (2014). Testable
equation mod-
implications
the Twenty-eighth AAAI
els.
Conference on Artiﬁcial
Intelligence (C. E. Brod-
ley and P. Stone, eds.). AAAI Press, Palo, CA.
<http://ftp.cs.ucla.edu/pub/stat ser/r428-reprint.pdf>.

In Proceedings of

structual

DRTON, M. and WEIHS, L. (2016). Generic identiﬁa-
bility of linear structural equation models by ancestor
Scandinavian Journal of Statistics
decomposition.
n/a–n/a10.1111/sjos.12227.
URL
12227

http://dx.doi.org/10.1111/sjos.

FISHER, F. (1966). The Identiﬁcation Problem in Econo-

metrics. McGraw-Hill, New York.

FOYGEL, R., DRAISMA, J. and DRTON, M. (2012). Half-
trek criterion for generic identiﬁability of linear struc-
tural equation models. The Annals of Statistics 40 1682–
1713.

HAUSMAN, J. A. and TAYLOR, W. E. (1983).

Identiﬁ-
cation in linear simultaneous equations models with co-
variance restrictions: an instrumental variables interpre-
tation. Econometrica: Journal of the Econometric Soci-
ety 1527–1549.

MOOIJ, J. M., PETERS, J., JANZING, D., ZSCHEIS-
CHLER, J. and SCH ¨OLKOPF, B. (2016). Distinguishing
cause from effect using observational data: methods and
benchmarks. Journal of Machine Learning Research 17
1–102.

PEARL, J. (2004). Robustness of causal claims. In Pro-
ceedings of the Twentieth Conference Uncertainty in Ar-
tiﬁcial Intelligence (M. Chickering and J. Halpern, eds.).
AUAI Press, Arlington, VA, 446–453.

Identiﬁcation and Model Testing using AVs

ZHANG, K. and HYV ¨ARINEN, A. (2009). On the identiﬁa-
bility of the post-nonlinear causal model. In Proceedings
of the Twenty-Fifth Conference on Uncertainty in Artiﬁ-
cial Intelligence. AUAI Press.

PEARL, J. (2009). Causality: Models, Reasoning, and In-
ference. 2nd ed. Cambridge University Press, New York.

RIGDON, E. E. (1995). A necessary and sufﬁcient identi-
ﬁcation rule for structural models estimated in practice.
Multivariate Behavioral Research 30 359–383.

SHARDELL, M. (2012). Methods to overcome violations
of an instrumental variable assumption: Converting a
confounder into an instrument. Computational statistics
& data analysis 56 2317–2333.

SHIMIZU, S., HOYER, P. O., HYV ¨ARINEN, A. and KER-
MINEN, A. (2006). A linear non-gaussian acyclic model
for causal discovery. Journal of Machine Learning Re-
search 7 2003–2030.

SHPITSER, I., RICHARDSON, T. S. and ROBINS, J. M.

(2009). Testing edges by truncations. In IJCAI.

SPIRTES, P., GLYMOUR, C. N. and SCHEINES, R. (2000).
Causation, prediction, and search, vol. 81. MIT press.

SPIRTES, P., RICHARDSON, T., MEEK, C., SCHEINES,
R. and GLYMOUR, C. (1998). Using path diagrams as
a structural equation modelling tool. Sociological Meth-
ods and Research 27 182–225.

TIAN, J. (2005). Identifying direct causal effects in linear
models. In Proceedings of the National Conference on
Artiﬁcial Intelligence, vol. 20. Menlo Park, CA; Cam-
bridge, MA; London; AAAI Press; MIT Press; 1999.

TIAN, J. (2007). A criterion for parameter identiﬁcation
In Proceedings of the
in structural equation models.
Twenty-Third Conference Annual Conference on Uncer-
tainty in Artiﬁcial Intelligence (UAI-07). AUAI Press,
Corvallis, Oregon.

TIAN, J. (2009). Parameter identiﬁcation in a class of lin-
In Proceedings of the
ear structural equation models.
Twenty-First International Joint Conference on Artiﬁcial
Intelligence (IJCAI-09).

VAN DER ZANDER, B. and LISKIEWICZ, M. (2016).
Searching for generalized instrumental variables. In Pro-
ceedings of the 19th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS-16)).

VAN DER ZANDER, B., TEXTOR, J. and LISKIEWICZ, M.
(2015). Efﬁciently ﬁnding conditional instruments for
In Proceedings of the Twenty-Fourth
causal inference.
International Joint Conference on Artiﬁcial Intelligence
(IJCAI 2015).

WRIGHT, S. (1921). Correlation and causation. Journal of

Agricultural Research 20 557–585.

