Nystr¨om Method with Kernel K-means++ Samples as Landmarks

Dino Oglic 1 2 Thomas G¨artner 2

Abstract
We investigate, theoretically and empirically, the
effectiveness of kernel K-means++ samples as
landmarks in the Nystr¨om method for low-rank
approximation of kernel matrices. Previous em-
pirical studies (Zhang et al., 2008; Kumar et al.,
2012) observe that the landmarks obtained using
(kernel) K-means clustering deﬁne a good low-
rank approximation of kernel matrices. However,
the existing work does not provide a theoretical
guarantee on the approximation error for this ap-
proach to landmark selection. We close this gap
and provide the ﬁrst bound on the approxima-
tion error of the Nystr¨om method with kernel K-
means++ samples as landmarks. Moreover, for
the frequently used Gaussian kernel we provide
a theoretically sound motivation for performing
Lloyd reﬁnements of kernel K-means++ land-
marks in the instance space. We substantiate our
theoretical results empirically by comparing the
approach to several state-of-the-art algorithms.

1. Introduction

We consider the problem of ﬁnding a good low-rank approx-
imation for a given symmetric and positive deﬁnite matrix.
Such matrices arise in kernel methods (Sch¨olkopf & Smola,
2001) where the data is often ﬁrst transformed to a sym-
metric and positive deﬁnite matrix and then an off-the-shelf
matrix-based algorithm is used for solving classiﬁcation
and regression problems, clustering, anomaly detection, and
dimensionality reduction (Bach & Jordan, 2005). These
learning problems can often be posed as convex optimiza-
tion problems for which the representer theorem (Wahba,
1990) guarantees that the optimal solution can be found in
the subspace of the kernel feature space spanned by the
instances. To ﬁnd the optimal solution in a problem with n
instances, it is often required to perform a matrix inversion

1Institut f¨ur Informatik III, Universit¨at Bonn, Germany 2School
of Computer Science, The University of Nottingham, United King-
dom. Correspondence to: Dino Oglic <dino.oglic@uni-bonn.de>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

or eigendecomposition which scale as O (cid:0)n3(cid:1). To overcome
this computational shortcoming and scale kernel methods
to large scale datasets, Williams & Seeger (2001) have pro-
posed to use a variant of the Nystr¨om method (Nystr¨om,
1930) for low-rank approximation of kernel matrices. The
approach is motivated by the fact that frequently used ker-
nels have a fast decaying spectrum and that small eigen-
values can be removed without a signiﬁcant effect on the
precision (Sch¨olkopf & Smola, 2001). For a given sub-set
of l landmarks, the Nystr¨om method ﬁnds a low-rank ap-
proximation in time O (cid:0)l2n + l3(cid:1) and kernel methods with
the low-rank approximation in place of the kernel matrix
scale as O (cid:0)l3(cid:1). In practice, l (cid:28) n and the approach can
scale kernel methods to millions of instances.

The crucial step in the Nystr¨om approximation of a symmet-
ric and positive deﬁnite matrix is the choice of landmarks
and an optimal choice is a difﬁcult discrete/combinatorial
problem directly inﬂuencing the goodness of the approxi-
mation (Section 2). A large part of the existing work has,
therefore, focused on providing approximation guarantees
for different landmark selection strategies. Following this
line of research, we propose to select landmarks using the
kernel K-means++ sampling scheme (Arthur & Vassil-
vitskii, 2007) and provide the ﬁrst bound on the relative
approximation error in the Frobenius norm for this strategy
(Section 3). An important part of our theoretical contribu-
tion is the ﬁrst complete proof of a claim by Ding & He
(2004) on the relation between the subspace spanned by
optimal K-means centroids and left singular vectors of the
feature space (Proposition 1). While our proof covers the
general case, that of Ding & He (2004) is restricted to data
matrices with piecewise constant right singular vectors.

Having given a bound on the approximation error for the
proposed landmark selection strategy, we provide a brief
overview of the existing landmark selection algorithms and
discuss our work in relation to approaches directly compa-
rable to ours (Section 4). For the frequently used Gaussian
kernel, we also theoretically motivate the instance space
Lloyd reﬁnements (Lloyd, 1982) of kernel K-means++
landmarks. The results of our empirical study are presented
in Section 5 and indicate a superior performance of the
proposed approach over competing methods. This is in
agreement with the previous studies on K-means centroids
as landmarks by Zhang et al. (2008) and Kumar et al. (2012).

Nystr¨om Method with Kernel K-means++ Samples as Landmarks

2. Nystr¨om Method

In this section, we review the Nystr¨om method for low-
rank approximation of kernel matrices. The method was
originally proposed for the approximation of integral eigen-
functions (Nystr¨om, 1930) and later adopted to low-rank
approximation of kernel matrices by Williams & Seeger
(2001). We present it here in a slightly different light by fol-
lowing the approach to subspace approximations by Smola
& Sch¨olkopf (2000, 2001).

Let X be an instance space and X = {x1, x2, · · · , xn}
an independent sample from a Borel probability measure
deﬁned on X . Let H be a reproducing kernel Hilbert space
with a positive deﬁnite kernel h : X × X → R. Given a
set of landmark points Z = {z1, · · · zm} (not necessarily
a subset of the sample) the goal is to approximate kernel
functions h (xi, ·) for all i = 1, n using linear combinations
of the landmarks. This goal can be formally stated as

min
α∈Rm×n

n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

m
(cid:88)

j=1

h (xi, ·) −

αj,ih (zj, ·)

.

(1)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

H

Let H denote the kernel matrix over all samples and land-
marks and let HZ denote the block in this matrix corre-
sponding to the kernel values between the landmarks. Addi-
tionally, let hx denote a vector with entries corresponding to
the kernel values between an instance x and the landmarks.
After expanding the norm, the problem is transformed into

min
α∈Rm×n

n
(cid:88)

i=1

Hii − 2h(cid:62)
xi

αi + α(cid:62)

i HZαi ,

(2)

where αi denotes the ith column of α. Each summand in
the optimization objective is a convex function depending
only on one column of α. Hence, the optimal solution is

α = H −1

Z HZ×X .

From here it then follows that the optimal approximation
˜HX|Z of the matrix HX using landmarks Z is given by

˜HX|Z = HX×ZH −1

Z HZ×X .

While the problem of computing the optimal projections of
instances to a subspace spanned by the landmarks is con-
vex and solvable in closed form (see above), the problem
of choosing the best set of landmarks is a combinatorial
problem that is difﬁcult to solve. To evaluate the effective-
ness of the subspace spanned by a given set of landmarks
it is standard to use the Schatten matrix norms (Weidmann,
1980). The Schatten p-norm of a symmetric and positive def-
inite matrix H is deﬁned as (cid:107)H(cid:107)p = ((cid:80)n
1/p , where
λi ≥ 0 are eigenvalues of H and p ≥ 1. For p = ∞ the
Schatten p-norm is equal to the operator norm and for p = 2

i=1 λp
i )

it is equal to the Frobenius norm. The three most frequently
used Schatten norms are p = 1, 2, ∞ and for these norms
the following inequalities hold:

(cid:107)H(cid:107)∞ = max

λi ≤

λ2
i =

tr (H (cid:62)H) = (cid:107)H(cid:107)2

(cid:113)

(cid:115)(cid:88)

i

λi = tr (H) = (cid:107)H(cid:107)1 .

(cid:88)

≤

i

i

From Eq. (1) and (2) it follows that for a subspace spanned
by a given set of landmarks Z, the 1-norm approximation
error of the optimal projections onto this space is given by

L (α∗) = tr(HX ) − tr( ˜HX|Z) =

(cid:13)
(cid:13)HX − ˜HX|Z
(cid:13)

(cid:13)
(cid:13)
(cid:13)1

.

The latter equation follows from the properties of trace
and the fact that Ξ = HX − ˜HX|Z is a symmetric and
positive deﬁnite matrix with Ξij = (cid:104)ξ (xi, ·), ξ (xj, ·)(cid:105)H
and ξ (xi, ·) = h (xi, ·) − (cid:80)m

k,ih (zk, ·).

k=1 α∗

For a good Nystr¨om approximation of a kernel matrix it is
crucial to select the landmarks to reduce the error in one of
the frequently used Schatten p-norms, i.e.,

Z ∗ =

arg min
Z⊂span(X), |Z|=K

(cid:13)
(cid:13)HX − ˜HX|Z
(cid:13)

(cid:13)
(cid:13)
(cid:13)p

.

Let us denote with VK and ΛK the top K eigenvectors and
eigenvalues of the kernel matrix HX . Then, at the low-rank
approximation ˜H ∗
K , the Schatten p-norm
error attains its minimal value (Golub & van Loan, 1996).

X|Z = VKΛKV (cid:62)

3. Kernel K-means++ Samples as Landmarks

We start with a review of K-means clustering (Lloyd, 1982)
and then give the ﬁrst complete proof of a claim stated
in Ding & He (2004) and Xu et al. (2015) on the relation
between the subspace spanned by the top (K − 1) left sin-
gular vectors of the data matrix and that spanned by optimal
K-means centroids. Building on a result by Arthur & Vas-
silvitskii (2007) we then bound the relative approximation
error in the Frobenius norm of the Nystr¨om method with
kernel K-means++ samples as landmarks.
Let the instance space X ⊂ Rd and let K denote the number
of clusters. In K-means clustering the goal is to choose a
set of centers C = {c1, · · · , cK} minimizing the potential

φ(C) =

(cid:107)x − c(cid:107)2 =

(cid:107)x − ck(cid:107)2 ,

(cid:88)

x∈X

min
c∈C

K
(cid:88)

(cid:88)

k=1

x∈Pk

where Pk = {x ∈ X | P (x) = ck} is a clustering cell
and P : X → C denotes the centroid assignment func-
tion. For a clustering cell Pk the centroid is computed as
x. In the remainder of the section, we denote

(cid:80)

1
|Pk|

x∈Pk

Nystr¨om Method with Kernel K-means++ Samples as Landmarks

with P ∈ Rn×K the cluster indicator matrix of the cluster-
ing C such that pij = 1/√
nj when instance xi is assigned
to centroid cj, and pij = 0 otherwise. Here nj denotes the
number of instances assigned to centroid cj. Without loss
of generality, we assume that the columns of the data matrix
X ∈ Rd×n are centered instances (i.e., (cid:80)n

i=1 xi/n = 0).

Now, using the introduced notation we can write the cluster-
ing potential as (Ding & He, 2004; Boutsidis et al., 2009)

φ (C) = (cid:13)

(cid:13)X − XP P (cid:62)(cid:13)
2
2 .
(cid:13)

Denoting with pi the ith column in P we have that it holds
p(cid:62)
i pj = δij, where δij = 1 if i = j and otherwise δij = 0.
Hence, it holds that P (cid:62)P = IK and P is an orthogonal
projection matrix with rank K. Let C denote the family of
all possible clustering indicator matrices of rank K. Then,
the K-means optimization problem is equivalent to the con-
strained low-rank approximation problem

P ∗ = arg min

P ∈C

(cid:13)X − XP P (cid:62)(cid:13)
(cid:13)
2
2 .
(cid:13)

From here, using the relation between the squared Schatten
2-norm and the matrix trace we obtain

P ∗ = arg min

tr (cid:0)X (cid:62)X(cid:1) − tr (cid:0)P (cid:62)X (cid:62)XP (cid:1) .

(3)

P ∈C

√

i=1

In the remainder of the section, we refer to the constrained
optimization objective from Eq. (3) as the discrete prob-
lem. For this problem, Ding & He (2004) observe that the
set of vectors {p1, · · · , pK, e/√
n} is linearly dependent (e
is a vector of ones) and that the rank of the optimization
problem can be reduced. As (cid:80)K
nipi = e, there exists
a linear orthonormal transformation of the subspace basis
given by the columns of P such that one of the vectors in
the new basis of the subspace spanned by P is e/√
n. Such
transformations are equivalent to a rotation of the subspace.
Let R ∈ RK×K denote an orthonormal transformation
matrix such that the vectors {pi}K
i=1 with
qK = 1√
n e. This is equivalent to requiring that the Kth col-
umn in R is rK = (cid:0)(cid:112) n1
n , · · · , (cid:112) nK
i e = 0 for
i = 1, K − 1. Moreover, from Q = P R and R(cid:62)R = IK it
follows that

i=1 map to {qi}K

and q(cid:62)

(cid:1)(cid:62)

n

Q(cid:62)Q = R(cid:62)P (cid:62)P R = R(cid:62)R = IK.

Hence, if we denote with QK−1 the matrix-block with the
ﬁrst (K − 1) columns of Q then the problem from Eq. (3)
can be written as (Ding & He, 2004; Xu et al., 2015)

Q∗

K−1 =

arg max
QK−1∈Rn×(K−1)

tr (cid:0)Q(cid:62)

K−1X (cid:62)XQK−1

(cid:1)

s.t.

Q(cid:62)

K−1QK−1 = IK−1

Q = P R ∧ qK =

1
√
n

e.

While P is an orthonormal indicator/sparse matrix of rank
K, Q is a piecewise constant and in general non-sparse or-
thonormal matrix of the same rank. The latter optimization
problem can be relaxed by not adding the structural con-
straints Q = P R and qK = e/√
n. The resulting optimiza-
tion problem is known as the Rayleigh–Ritz quotient (e.g.,
see L¨utkepohl, 1997) and in the remainder of the section we
refer to it as the continuous problem. The optimal solution
to the continuous problem is (up to a rotation of the basis)
deﬁned by the top (K − 1) eigenvectors from the eigende-
composition of the positive deﬁnite matrix X (cid:62)X and the
optimal value of the relaxed optimization objective is the
sum of the eigenvalues corresponding to this solution. As
the continuous solution is (in general) not sparse, the dis-
crete problem is better described with non-sparse piecewise
constant matrix Q than with sparse indicator matrix P .

Ding & He (2004) and Xu et al. (2015) have formulated
a theorem which claims that the subspace spanned by op-
timal K-centroids is in fact the subspace spanned by the
top (K − 1) left singular vectors of X. The proofs pro-
vided in these works are, however, restricted to the case
when the discrete and continuous/relaxed version of the
optimization problem match. We address here this claim
without that restriction and amend their formulation ac-
cordingly. For this purpose, let C ∗ = {c1, · · · , cK}
be K centroids specifying an optimal K-means cluster-
ing (i.e., minimizing the potential). The between clus-
ter scatter matrix S = (cid:80)K
i projects any vector
x ∈ X to a subspace spanned by the centroid vectors, i.e.,
i x(cid:1) ci ∈ span {c1, · · · , cK}. Let also
Sx = (cid:80)K
λK denote the Kth eigenvalue of H = X (cid:62)X and assume
the eigenvalues are listed in descending order. A proof of
the following proposition is provided in Appendix A.
Proposition 1. Suppose that the subspace spanned by op-
timal K-means centroids has a basis that consists of left
singular vectors of X. If the gap between the eigenvalues
λK−1 and λK is sufﬁciently large (see the proof for explicit
deﬁnition), then the optimal K-means centroids and the top
(K − 1) left singular vectors of X span the same subspace.

i=1 nicic(cid:62)

i=1 ni

(cid:0)c(cid:62)

Proposition 2. In contrast to the claim by Ding & He (2004)
and Xu et al. (2015), it is possible that no basis of the
subspace spanned by optimal K-means centroids consists
of left singular vectors of X. In that case, the subspace
spanned by the top (K − 1) left singular vectors is different
from that spanned by optimal K-means centroids.

Let X = U ΣV (cid:62) be an SVD decomposition of X and denote
with UK the top K left singular vectors from this decom-
position. Let also U ⊥
K denote the dual matrix of UK and
φ (C ∗ | UK) the clustering potential given by the projec-
tions of X and C ∗ onto the subspace UK.
Proposition 3. Let HK denote the optimal rank K approx-
imation of the Gram matrix H = X (cid:62)X and let C ∗ be an

Nystr¨om Method with Kernel K-means++ Samples as Landmarks

optimal K-means clustering of X. Then, it holds

φ (C ∗)≤ (cid:107)H − HK−1(cid:107)1 + φ (C ∗ | UK−1) .

Let us now relate Proposition 1 to the result from Section 2
where we were interested in ﬁnding a set of landmarks span-
ning the subspace that preserves most of the variance of
the data in the kernel feature space. Assuming that the
conditions from Proposition 1 are satisﬁed, the Nystr¨om
approximation using optimal kernel K-means centroids as
landmarks projects the data to a subspace with the highest
possible variance. Hence, under these conditions optimal
kernel K-means landmarks provide the optimal rank (K −1)
reconstruction of the kernel matrix. However, for a kernel
K-means centroid there does not necessarily exist a point
in the instance space that maps to it. To account for this
and the hardness of the kernel K-means clustering prob-
lem (Aloise et al., 2009), we propose to approximate the
centroids with kernel K-means++ samples. This sampling
strategy iteratively builds up a set of landmarks such that
in each iteration an instance is selected with probability
proportional to its contribution to the clustering potential
in which previously selected instances act as cluster cen-
ters. For a problem with n instances and dimension d, the
strategy selects K landmarks in time O (Knd).

Before we give a bound on the Nystr¨om approximation with
kernel K-means++ samples as landmarks, we provide a
result by Arthur & Vassilvitskii (2007) on the approximation
error of the optimal clustering using this sampling scheme.
Theorem 4. [Arthur & Vassilvitskii (2007)] If a clustering
C is constructed using the K-means++ sampling scheme
then the corresponding clustering potential φ (C) satisﬁes

E [φ (C)] ≤ 8 (ln K + 2) φ (C ∗) ,

where C ∗ is an optimal clustering and the expectation is
taken with respect to the sampling distribution.

Having presented all the relevant results, we now give a
bound on the approximation error of the Nystr¨om method
with kernel K-means++ samples as landmarks. A proof of
the following theorem is provided in Appendix A.
Theorem 5. Let H be a kernel matrix with a ﬁnite rank
factorization H = Φ (X)(cid:62) Φ (X). Denote with HK the
optimal rank K approximation of H and let ˜HK be the
Nystr¨om approximation of the same rank obtained using
kernel K-means++ samples as landmarks. Then, it holds

(cid:34)

E

(cid:35)

(cid:107)H − ˜HK(cid:107)2
(cid:107)H − HK(cid:107)2

≤ 8(ln(K + 1) + 2)(

n − K + ΘK),

√

with ΘK = φ(C∗|UK )/(cid:107)H−HK (cid:107)2, where UK denotes the top
K left singular vectors of Φ (X) and C ∗ an optimal kernel
K-means clustering with (K + 1) clusters.

Corollary 6. When φ (C ∗ | UK) ≤
then the additive term ΘK ≤

√

n − K and

n − K (cid:107)H − HK(cid:107)2,

√

(cid:34)

E

(cid:35)

(cid:107)H − ˜HK(cid:107)2
(cid:107)H − HK(cid:107)2

(cid:16)

√

(cid:17)

∈ O

ln K

n − K

.

(4)

The given bound for low-rank approximation of symmetric
and positive deﬁnite matrices holds for the Nystr¨om method
with kernel K-means++ samples as landmarks without
any Lloyd iterations (Lloyd, 1982). To obtain even better
landmarks, it is possible to ﬁrst sample candidates using
the kernel K-means++ sampling scheme and then attempt
a Lloyd reﬁnement in the instance space (motivation for
this is provided in Section 4.3). If the clustering potential
is decreased as a result of this, the iteration is considered
successful and the landmarks are updated. Otherwise, the
reﬁnement is rejected and current candidates are selected as
landmarks. This is one of the landmark selection strategies
we analyze in our experiments (e.g., see Appendix C).

Let us now discuss the properties of our bound with re-
spect to the rank of the approximation. From Corollary 6
it follows that the bound on the relative approximation er-
ror increases initially (for small K) with ln K and then
decreases as K approaches n. This is to be expected as a
larger K means we are trying to ﬁnd a higher dimensional
subspace and initially this results in having to solve a more
difﬁcult problem. The bound on the low-rank approxima-
tion error is, on the other hand, obtained by multiplying
with (cid:107)H − HK(cid:107)2 which depends on the spectrum of the
kernel matrix and decreases with K. In order to be able to
generalize at all, one has to assume that the spectrum falls
rather sharply and typical assumptions are λi ∈ O (i−a)
with a > 1 or λi ∈ O (cid:0)e−bi(cid:1) with b > 0 (e.g., see Sec-
tion 4.3, Bach, 2013). It is simple to show that for a ≥ 2,
K > 1, and λi ∈ O (i−a) such falls are sharper than ln K
(Corollary 7, Appendix A). Thus, our bound on the low-rank
approximation error decreases with K for sensible choices
of the kernel function. Note that a similar state-of-the-art
bound on the relative approximation error by Li et al. (2016)
exhibits worse behavior and grows linearly with K.

4. Discussion

We start with a brief overview of alternative approaches
to landmark selection in the Nystr¨om method for low-rank
approximation of kernel matrices. Following this, we fo-
cus on a bound that is the most similar to ours, that of
K-DPP-Nystr¨om (Li et al., 2016). Then, for the frequently
used Gaussian kernel, we provide a theoretically sound
motivation for performing Lloyd reﬁnements of kernel K-
means++ landmarks in the instance space instead of the
kernel feature space. These reﬁnements are computationally
cheaper than those performed in the kernel feature space
and can only improve the positioning of the landmarks.

Nystr¨om Method with Kernel K-means++ Samples as Landmarks

4.1. Related Approaches

As pointed in Sections 1 and 2, the choice of landmarks
is instrumental for the goodness of the Nystr¨om low-rank
approximations. For this reason, the existing work on the
Nystr¨om method has focused mainly on landmark selection
techniques with theoretical guarantees. These approaches
can be divided into four groups: i) random sampling, ii)
greedy methods, iii) methods based on the Cholesky decom-
position, iv) vector quantization (e.g., K-means clustering).

The simplest strategy for choosing the landmarks is by uni-
formly sampling them from a given set of instances. This
was the strategy that was proposed by Williams & Seeger
(2001) in the ﬁrst paper on the Nystr¨om method for low-rank
approximation of kernel matrices. Following this, more so-
phisticated non-uniform sampling schemes were proposed.
The schemes that received a lot of attention over the past
years are the selection of landmarks by sampling propor-
tional to column norms of the kernel matrix (Drineas et al.,
2006), diagonal entries of the kernel matrix Drineas & Ma-
honey (2005), approximate leverage scores (Alaoui & Ma-
honey, 2015; Gittens & Mahoney, 2016), and submatrix
determinants (Belabbas & Wolfe, 2009; Li et al., 2016).
From this group of methods, the approximate leverage score
sampling and the K-DPP Nystr¨om method (see Section 4.2)
are considered state-of-the-art methods in low-rank approxi-
mation of kernel matrices.

The second group of landmark selection techniques are
greedy methods. A well-performing representative from
this group is a method for sparse approximations proposed
by Smola & Sch¨olkopf (2000) for which it was later inde-
pendently established (Kumar et al., 2012) that it performs
very well in practice—second only to K-means clustering.

The third group of methods relies on the incomplete
Cholesky decomposition to construct a low-rank approx-
imation of a kernel matrix (Fine & Scheinberg, 2002; Bach
& Jordan, 2005; Kulis et al., 2006). An interesting aspect of
the work by Bach & Jordan (2005) and that of Kulis et al.
(2006) is the incorporation of side information/labels into
the process of ﬁnding a good low-rank approximations of a
given kernel matrix.

Beside these approaches, an inﬂuential ensemble method
for low-rank approximation of kernel matrices was pro-
posed by Kumar et al. (2012). This work also contains an
empirical study with a number of approaches to landmark
selection. Kumar et al. (2012) also note that the landmarks
obtained using instance space K-means clustering perform
the best among non-ensemble methods.

4.2. K-DPP Nystr¨om Method

The ﬁrst bound on the Nystr¨om approximation with land-
marks sampled proportional to submatrix determinants was

given by Belabbas & Wolfe (2009). Li et al. (2016) recog-
nize this sampling scheme as a determinantal point process
and extend the bound to account for the case when l land-
marks are selected to make an approximation of rank K ≤ l.
That bound can be formally speciﬁed as (Li et al., 2016)

(cid:34)

E

(cid:35)

(cid:107)H − ˜HK(cid:107)2
(cid:107)H − HK(cid:107)2

≤

l + 1
l + 1 − K

√

n − K.

(5)

For l = K, the bound can be derived from that of Belabbas
& Wolfe (Theorem 1, 2009) by applying the inequalities
between the corresponding Schatten p-norms.

The bounds obtained by Belabbas & Wolfe (2009) and Li
et al. (2016) can be directly compared to the bound from
Corollary 6. From Eq. (5), for l = K + 1, we get that
the expected relative approximation error of the K-DPP
Nystr¨om method scales like O (cid:0)K
n − K(cid:1). For a good
worst case guarantee on the generalization error of learning
with Nystr¨om approximations (see, e.g., Yang et al., 2012),
√
the parameter K scales as
n. Plugging this parameter
estimate into Eq. (4), we see that the upper bound on the ex-
pected error with kernel K-means++ landmarks scales like
O (

n ln n) and that with K-DPP landmarks like O (n).

√

√

Having compared our bound to that of K-DPP landmark
selection, we now discuss some speciﬁcs of the empirical
study performed by Li et al. (2016). The crucial step of
that landmark selection strategy is the ability to efﬁciently
sample from a K-DPP. To achieve this, the authors have pro-
posed to use a Markov chain with a worst case mixing time
linear in the number of instances. The mixing bound holds
provided that a data-dependent parameter satisﬁes a condi-
tion which is computationally difﬁcult to verify (Section 5,
Li et al., 2016). Moreover, there are cases when this condi-
tion is not satisﬁed and for which the mixing bound does not
hold. In their empirical evaluation of the K-DPP Nystr¨om
method, Li et al. (2016) have chosen the initial state of the
Markov chain by sampling it using the K-means++ scheme
and then run the chain for 100-300 iterations. While the
choice of the initial state is not discussed by the authors, one
reason that this could be a good choice is because it starts the
chain from a high density region. To verify this hypothesis,
we simulate the K-DPP Nystr¨om method by choosing the
initial state uniformly at random and run the chain for 1 000
and 10 000 steps (Section 5). Our empirical results indicate
that starting the K-DPP chain with K-means++ samples is
instrumental for performing well with this method in terms
of runtime and accuracy (Figure 6, Li et al., 2016). More-
over, for the case when the initial state is sampled uniformly
at random, our study indicates that the chain might need at
least one pass through the data to reach a region with good
landmarks. The latter is computationally inefﬁcient already
on datasets with more than 10 000 instances.

Nystr¨om Method with Kernel K-means++ Samples as Landmarks

4.3. Instance Space K-means Centroids as Landmarks

We ﬁrst address the approach to landmark selection based
on K-means clustering in the instance space (Zhang et al.,
2008) and then give a theoretically sound motivation for why
these landmarks work well with the frequently used Gaus-
sian kernel. The outlined reasoning motivates the instance
space Lloyd reﬁnements of kernel K-means++ samples
and it can be extended to other kernel feature spaces by
following the derivations from Burges (1999).

The only existing bound for instance space K-means land-
marks was provided by Zhang et al. (2008). However, this
bound only works for kernel functions that satisfy

(h (a, b) − h (c, d))2 ≤ η (h, X )

(cid:16)

(cid:107)a − c(cid:107)2 − (cid:107)b − d(cid:107)2(cid:17)

,

for all a, b, c, d ∈ X and a data- and kernel-dependent con-
stant η (h, X ). In contrast to this, our bound holds for all
kernels over Euclidean spaces. The bound given by Zhang
et al. (2008) is also a worst case bound, while ours is a
bound in the expectation. The type of the error itself is also
different, as we bound the relative error and Zhang et al.
(2008) bound the error in the Frobenius norm. The disad-
vantage of the latter is in the sensitivity to scaling and such
bounds become loose even if a single entry of the matrix
is large (Li et al., 2016). Having established the difference
in the type of the bounds, it cannot be claimed that one is
sharper than the other. However, it is important to note that
the bound by Zhang et al. (Proposition 3, 2008) contains the
full clustering potential φ (C ∗) multiplied by n
n/K as a
term and this is signiﬁcantly larger than the rank dependent
term from our bound (e.g., see Theorem 5).

√

Burges (1999) has investigated the geometry of kernel fea-
ture spaces and a part of that work refers to the Gaussian
kernel. We review the results related to this kernel feature
space and give an intuition for why K-means clustering in
the instance space provides a good set of landmarks for the
Nystr¨om approximation of the Gaussian kernel matrix. The
reasoning can be extended to other kernel feature spaces
as long as the manifold onto which the data is projected
in the kernel feature space is a ﬂat Riemannian manifold
with the geodesic distance between the points expressed in
terms of the Euclidean distance between instances (e.g., see
Riemmannian metric tensors in Burges, 1999).

The frequently used Gaussian kernel is given by

h (x, y) = (cid:104)Φ (x), Φ (y)(cid:105) = exp (cid:0)(cid:107)x−y(cid:107)2/2σ2(cid:1) ,

where the feature map Φ (x) is inﬁnite dimensional and for
a subset X of the instance space X ∈ Rd also inﬁnitely
continuously differentiable on X. As in Burges (1999) we
denote with S the image of X in the reproducing kernel
Hilbert space of h. The image S is a r ≤ d dimensional
surface in this Hilbert space. As noted by Burges (1999)

the image S is a Hausdorff space (Hilbert space is a metric
space and, thus, a Hausdorff space) and has a countable
basis of open sets (the reproducing kernel Hilbert space
of the Gaussian kernel is separable). So, for S to be a
differentiable manifold (Boothby, 1986) the image S needs
to be locally Euclidean of dimension r ≤ d.

We assume that our set of instances X is mapped to a differ-
entiable manifold in the reproducing kernel Hilbert space
H. On this manifold a Riemannian metric can be deﬁned
and, thus, the set X is mapped to a Riemannian manifold S.
Burges (1999) has showed that the Riemannian metric ten-
sor induced by this kernel feature map is gij = δij
σ2 , where
δij = 1 if i = j and zero otherwise (1 ≤ i, j ≤ d). This
form of the tensor implies that the manifold is ﬂat.

From the obtained metric tensor, it follows that the squared
geodesic distance between two points Φ (x) and Φ (y) on S
is equal to the σ-scaled Euclidean distance between x and y
in the instance space, i.e., dS (Φ (x) , Φ (y))2 = (cid:107)x−y(cid:107)2/σ2.
For a cluster Pk, the geodesic centroid is a point on S that
minimizes the distance to other cluster points (centroid in
the K-means sense). For our instance space, we have that

c∗
k = arg min

c∈Rd

(cid:88)

x∈Pk

(cid:107)x − c(cid:107)2 ⇒ c∗

k =

1
|Pk|

(cid:88)

x.

x∈Pk

Thus, by doing K-means clustering in the instance space
we are performing approximate geodesic clustering on the
manifold onto which the data is embedded in the Gaussian
kernel feature space. It is important to note here that a cen-
troid from the instance space is only an approximation to
the geodesic centroid from the kernel feature space – the
preimage of the kernel feature space centroid does not nec-
essarily exist. As the manifold is ﬂat, geodesic centroids are
‘good’ approximations to kernel K-means centroids. Hence,
by selecting centroids obtained using K-means clustering
in the instance space we are making a good estimate of
the kernel K-means centroids. For the latter centroids, we
know that under the conditions of Proposition 1 they span
the same subspace as the top (K − 1) left singular vectors
of a ﬁnite rank factorization of the kernel matrix and, thus,
deﬁne a good low-rank approximation of the kernel matrix.

5. Experiments

Having reviewed the state-of-the-art methods in select-
ing landmarks for the Nystr¨om low-rank approximation
of kernel matrices, we perform a series of experiments to
demonstrate the effectiveness of the proposed approach
and substantiate our claims from Sections 3 and 4. We
achieve this by comparing our approach to the state-of-the-
art in landmark selection – approximate leverage score sam-
pling (Gittens & Mahoney, 2016) and the K-DPP Nystr¨om
method (Belabbas & Wolfe, 2009; Li et al., 2016).

Nystr¨om Method with Kernel K-means++ Samples as Landmarks

Figure 1. The ﬁgure shows the lift of the approximation error in the Frobenius norm as the bandwidth parameter of the Gaussian kernel
varies and the approximation rank is ﬁxed to K = 100. The lift of a landmark selection strategy indicates how much better it is to
approximate the kernel matrix with landmarks obtained using that strategy compared to the uniformly sampled ones.

Figure 2. The ﬁgure shows the time it takes to select landmarks via different schemes together with the corresponding error in the
Frobenius norm while the bandwidth of the Gaussian kernel varies and the approximation rank is ﬁxed to K = 100.

Before we present and discuss our empirical results, we
provide a brief summary of the experimental setup. The
experiments were performed on 13 real-world datasets avail-
able at the UCI and LIACC repositories. Each of the selected
datasets consists of more than 5 000 instances. Prior to
running the experiments, the datasets were standardized to
have zero mean and unit variance. We measure the good-
ness of a landmark selection strategy with the lift of the
approximation error in the Frobenius norm and the time
needed to select the landmarks. The lift of the approxima-
tion error of a given strategy is computed by dividing the
error obtained by sampling landmarks uniformly without
replacement (Williams & Seeger, 2001) with the error of
the given strategy. In contrast to the empirical study by Li
et al. (2016), we do not perform any sub-sampling of the
datasets with less than 25 000 instances and compute the
Frobenius norm error using full kernel matrices. On one
larger dataset with more than 25 000 instances the memory
requirements were hindering our parallel implementation
and we, therefore, subsampled it to 25 000 instances (ct-
slice dataset, Appendix C). By performing our empirical
study on full datasets, we are avoiding a potentially negative
inﬂuence of the sub-sampling on the effectiveness of the
compared landmark selection strategies, time consumed,

and the accuracy of the approximation error. Following pre-
vious empirical studies (Drineas & Mahoney, 2005; Kumar
et al., 2012; Li et al., 2016), we evaluate the goodness of
landmark selection strategies using the Gaussian kernel and
repeat all experiments 10 times to account for their non-
deterministic nature. We refer to γ = 1/σ2 as the bandwidth
of the Gaussian kernel and in order to determine the band-
width interval we sample 5 000 instances and compute their
squared pairwise distances. From these distances we take
the inverse of 1 and 99 percentile values as the right and
left endpoints. To force the kernel matrix to have a large
number of signiﬁcant spectral components (i.e., the Gaus-
sian kernel matrix approaches to the identity matrix), we
require the right bandwidth endpoint to be at least 1. From
the logspace of the determined interval we choose 10 evenly
spaced values as bandwidth parameters. In the remainder of
the section, we summarize our ﬁndings with 5 datasets and
provide the complete empirical study in Appendix C.

In the ﬁrst set of experiments, we ﬁx the approximation
rank and evaluate the performance of the landmark selection
strategies while varying the bandwidth of the Gaussian ker-
nel. Similar to Kumar et al. (2012), we observe that for most
datasets at a standard choice of bandwidth – inverse median
squared pairwise distance between instances – the princi-

-16-12-8-40-1.2.35.68.912.2logγloglift(a)aileronskernelK-means++leveragescores(uniformsketch)K-DPP(1000MCsteps)kernelK-means++(withrestarts)leveragescores(K-diagonalsketch)K-DPP(10000MCsteps)-16-12-8-40-0.50.61.72.83.9logγ(b)parkinsons-13.2-9.9-6.6-3.30.-0.52.96.39.713.1logγ(c)elevators-23.2-17.4-11.6-5.80.-0.20.0.20.40.6logγ(d)ujil-13.2-9.9-6.6-3.30.-0.51.22.94.66.3logγ(e)cal-housing00.010.4011300-20.-13.2-6.40.47.2timeinsecondslogFrobeniuserror(a)aileronskernelK-means++leveragescores(uniformsketch)K-DPP(1000MCsteps)uniformkernelK-means++(withrestarts)leveragescores(K-diagonalsketch)K-DPP(10000MCsteps)00.010.4011300-20.-13.2-6.40.47.2timeinseconds(b)parkinsons00.010.4011300-15.2-9.6-4.1.67.2timeinseconds(c)elevators00.010.4011300-10.-5.7-1.42.97.2timeinseconds(d)ujil00.010.4011300-22.-14.7-7.4-0.17.2timeinseconds(e)cal-housingNystr¨om Method with Kernel K-means++ Samples as Landmarks

Figure 3. The ﬁgure shows the improvement in the lift of the approximation error measured in the Frobenius norm that comes as a result
of the increase in the rank of the approximation. The bandwidth parameter of the Gaussian kernel is set to the inverse of the squared
median pairwise distance between the samples.

pal part of the spectral mass is concentrated at the top 100
eigenvalues and we set the approximation rank K = 100.
Figure 1 demonstrates the effectiveness of evaluated selec-
tion strategies as the bandwidth varies. More precisely, as
the log value of the bandwidth parameter approaches to
zero the kernel matrix is close to being the identity matrix,
thus, hindering low-rank approximations. In contrast to this,
as the bandwidth value gets smaller the spectrum mass be-
comes concentrated in a small number of eigenvalues and
low-rank approximations are more accurate. Overall, the
kernel K-means++ sampling scheme performs the best
across all 13 datasets. It is the best performing method on
10 of the considered datasets and a competitive alternative
on the remaining ones. The improvement over alternative
approaches is especially evident on datasets ailerons and
elevators. The approximate leverage score sampling is on
most datasets competitive and achieves a signiﬁcantly better
approximation than alternatives on the dataset cal-housing.
The approximations for the K-DPP Nystr¨om method with
10 000 MC steps are more accurate than that with 1 000
steps. The low lift values for that method seem to indicate
that the approach moves rather slowly away from the ini-
tial state sampled uniformly at random. This choice of the
initial state is the main difference in the experimental setup
compared to the study by Li et al. (2016) where the K-DPP
chain was initialized with K-means++ sampling scheme.

Figure 2 depicts the runtime costs incurred by each of the
sampling schemes.
It is evident that compared to other
methods the cost of running the K-DPP chain with uni-
formly chosen initial state for more than 1 000 steps results
in a huge runtime cost without an appropriate reward in the
accuracy. From this ﬁgure it is also evident that the approxi-
mate leverage score and kernel K-means++ sampling are
efﬁcient and run in approximately the same time apart from
the dataset ujil (see also ct-slice, Appendix C). This dataset
has more than 500 attributes and it is time consuming for
the kernel K-means++ sampling scheme (our implementa-
tion does not cache/pre-compute the kernel matrix). While

on such large dimensional datasets the kernel K-means++
sampling scheme is not as fast as the approximate lever-
age score sampling, it is still the best performing landmark
selection technique in terms of the accuracy.

In Figure 3 we summarize the results of the second exper-
iment where we compare the improvement in the approxi-
mation achieved by each of the methods as the rank of the
approximation is increased from 5 to 100. The results indi-
cate that the kernel K-means++ sampling achieves the best
increase in the lift of the approximation error. On most of
the datasets the approximate leverage score sampling is com-
petitive. That method also performs much better than the
K-DPP Nystr¨om approach initialized via uniform sampling.

As the landmark subspace captured by our approach depends
on the gap between the eigenvalues and that of the approx-
imate leverage score sampling on the size of the sketch
matrix, we also evaluate the strategies in a setting where
l landmarks are selected in order to make a rank K < l
approximation of the kernel matrix. Similar to the ﬁrst ex-
periment, we ﬁx the rank to K = 100 and in addition to the
already discussed case with l = K we consider cases with
l = K ln n and l = K ln K. Due to space restrictions, the
details of this experiment are provided in Appendix C. The
results indicate that there is barely any difference between
the lift curves for the kernel K-means++ sampling with
l = K ln K and l = K ln n landmarks. In their empirical
study, Gittens & Mahoney (2016) have observed that for uni-
formly selected landmarks, (cid:15) ∈ [0, 1], and l ∈ O (K ln n),
the average rank K approximation errors are within (1 + (cid:15))
of the optimal rank K approximation errors. Thus, based
on that and our empirical results it seems sufﬁcient to take
K ln K landmarks for an accurate rank K approximation
of the kernel matrix. Moreover, the gain in the accuracy for
our approach with l = K ln K landmarks comes with only
a slight increase in the time taken to select the landmarks.
Across all datasets, the proposed sampling scheme is the
best performing landmark selection technique in this setting.

5102550100-0.21.63.45.27.rankloglift(a)aileronskernelK-means++leveragescores(uniformsketch)K-DPP(1000MCsteps)kernelK-means++(withrestarts)leveragescores(K-diagonalsketch)K-DPP(10000MCsteps)5102550100-0.20.81.82.83.8rank(b)parkinsons5102550100-0.21.53.24.96.6rank(c)elevators5102550100-0.8-0.5-0.20.10.4rank(d)ujil5102550100-0.40.92.23.54.8rank(e)cal-housingNystr¨om Method with Kernel K-means++ Samples as Landmarks

Acknowledgment: We are grateful for access to the Univer-
sity of Nottingham High Performance Computing Facility.

References

Alaoui, Ahmed E. and Mahoney, Michael W. Fast randomized
kernel ridge regression with statistical guarantees. In Advances
in Neural Information Processing Systems 28, 2015.

Aloise, Daniel, Deshpande, Amit, Hansen, Pierre, and Popat,
Preyas. NP-hardness of Euclidean sum-of-squares clustering.
Machine Learning, 2009.

Arthur, David and Vassilvitskii, Sergei. K-means++: The ad-
vantages of careful seeding. In Proceedings of the Eighteenth
Annual ACM-SIAM Symposium on Discrete Algorithms, 2007.

Kulis, Brian, Sustik, M´aty´as, and Dhillon, Inderjit. Learning low-
rank kernel matrices. In Proceedings of the 23rd International
Conference on Machine Learning, 2006.

Kumar, Sanjiv, Mohri, Mehryar, and Talwalkar, Ameet. Sampling
methods for the Nystr¨om method. Journal of Machine Learning
Research, 2012.

Li, Chengtao, Jegelka, Stefanie, and Sra, Suvrit. Fast DPP sam-
pling for Nystr¨om with application to kernel methods.
In
Proceedings of the 33rd International Conference on Machine
Learning, 2016.

Lloyd, Stuart. Least squares quantization in PCM. IEEE Transac-

tions on Information Theory, 1982.

L¨utkepohl, Helmut. Handbook of Matrices. Wiley, 1997.

Bach, Francis R. Sharp analysis of low-rank kernel matrix approx-
imations. In Proceedings of the 26th Annual Conference on
Learning Theory, 2013.

Nystr¨om, Evert J.

¨Uber die praktische Auﬂ¨osung von Integral-
gleichungen mit Anwendungen auf Randwertaufgaben. Acta
Mathematica, 1930.

Sch¨olkopf, Bernhard and Smola, Alexander J. Learning with
kernels: Support vector machines, regularization, optimization,
and beyond. MIT Press, 2001.

Smola, Alexander J. and Sch¨olkopf, Bernhard. Sparse greedy
matrix approximation for machine learning. In Proceedings of
the 17th International Conference on Machine Learning, 2000.

Wahba, Grace. Spline models for observational data. SIAM, 1990.

Weidmann, Joachim. Linear operators in Hilbert spaces. Springer-

Verlag, 1980.

Williams, Christopher K. I. and Seeger, Matthias. Using the
Nystr¨om method to speed up kernel machines. In Advances in
Neural Information Processing Systems 13. 2001.

Xu, Qin, Ding, Chris, Liu, Jinpei, and Luo, Bin. PCA-guided

search for K-means. Pattern Recognition Letters, 2015.

Yang, Tianbao, Li, Yu-feng, Mahdavi, Mehrdad, Jin, Rong, and
Zhou, Zhi-Hua. Nystr¨om method vs random Fourier features:
A theoretical and empirical comparison. In Advances in Neural
Information Processing Systems 25. 2012.

Zhang, Kai, Tsang, Ivor W., and Kwok, James T.

Improved
Nystr¨om low-rank approximation and error analysis. In Proceed-
ings of the 25th International Conference on Machine Learning,
2008.

Bach, Francis R. and Jordan, Michael I. Predictive low-rank
decomposition for kernel methods. In Proceedings of the 22nd
International Conference on Machine Learning, 2005.

Belabbas, Mohamed A. and Wolfe, Patrick J. Spectral methods
in machine learning: New strategies for very large datasets.
Proceedings of the National Academy of Sciences of the USA,
2009.

Boothby, William M. An introduction to differentiable manifolds

and Riemannian geometry. Academic Press, 1986.

Boutsidis, Christos, Drineas, Petros, and Mahoney, Michael W.
Unsupervised feature selection for the K-means clustering prob-
lem. In Advances in Neural Information Processing Systems 22,
2009.

Burges, Christopher J. C. Geometry and invariance in kernel based
methods. In Advances in Kernel Methods. MIT Press, 1999.

Ding, Chris and He, Xiaofeng. K-means clustering via principal
component analysis. In Proceedings of the 21st International
Conference on Machine Learning, 2004.

Drineas, Petros and Mahoney, Michael W. On the Nystr¨om method
for approximating a Gram matrix for improved kernel-based
learning. Journal of Machine Learning Research, 2005.

Drineas, Petros, Kannan, Ravi, and Mahoney, Michael W. Fast
Monte Carlo algorithms for matrices II: Computing a low-rank
approximation to a matrix. SIAM Journal on Computing, 2006.

Fine, Shai and Scheinberg, Katya. Efﬁcient SVM training using
low-rank kernel representations. Journal of Machine Learning
Research, 2002.

Gittens, Alex and Mahoney, Michael W. Revisiting the Nystr¨om
method for improved large-scale machine learning. Journal
Machine Learning Research, 2016.

Golub, Gene H. and van Loan, Charles F. Matrix Computations.

Johns Hopkins University Press, 1996.

Kanungo, Tapas, Mount, David M., Netanyahu, Nathan S., Piatko,
Christine D., Silverman, Ruth, and Wu, Angela Y. A local
search approximation algorithm for K-means clustering.
In
Proceedings of the Eighteenth Annual Symposium on Computa-
tional Geometry, 2002.

