Stochastic Modiﬁed Equations
and Adaptive Stochastic Gradient Algorithms

Qianxiao Li 1 Cheng Tai 2 3 Weinan E 2 3 4

Abstract

We develop the method of stochastic modiﬁed
equations (SME), in which stochastic gradient
algorithms are approximated in the weak sense
by continuous-time stochastic differential equa-
tions. We exploit the continuous formulation
together with optimal control theory to derive
novel adaptive hyper-parameter adjustment poli-
cies. Our algorithms have competitive perfor-
mance with the added beneﬁt of being robust to
varying models and datasets. This provides a
general methodology for the analysis and design
of stochastic gradient algorithms.

1. Introduction

Stochastic gradient algorithms are often used to solve opti-
mization problems of the form

min
x2Rd

f (x) :=

fi(x);

(1)

1
n

n
X

i=1

: Rd ! R for i = 1; : : : ; n.
where f; fi
In machine
learning applications, f is typically the total loss function
whereas each fi represents the loss due to the ith training
sample. x is a vector of trainable parameters and n is the
training sample size, which is typically very large.

Solving (1) using the standard gradient descent (GD) re-
quires n gradient evaluations per step and is prohibitively
expensive when n (cid:29) 1. An alternative, the stochastic gra-
dient descent (SGD), is to replace the full gradient rf by
a sampled version, serving as an unbiased estimator. In its
simplest form, the SGD iteration is written as

xk+1 = xk (cid:0) (cid:17)rf(cid:13)k (xk);

(2)

1Institute of High Performance Computing, Singapore 2Peking
University, Beijing, China 3Beijing Institute of Big Data Re-
search, Beijing, China 4Princeton University, Princeton, NJ, USA.
Correspondence to: Qianxiao Li <liqix@ihpc.a-star.edu.sg>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

where k (cid:21) 0 and f(cid:13)kg are i.i.d uniform variates taking val-
ues in f1; 2; (cid:1) (cid:1) (cid:1) ; ng. The step-size (cid:17) is the learning rate.
Unlike GD, SGD samples the full gradient and its com-
putational complexity per iterate is independent of n. For
this reason, stochastic gradient algorithms have become in-
creasingly popular in large scale problems.

Many convergence results are available for SGD and its
variants. However, most are upper-bound type results for
(strongly) convex objectives, often lacking the precision
and generality to characterize the behavior of algorithms
in practical settings. This makes it harder to translate theo-
retical understanding into algorithm analysis and design.

In this work, we address this by pursuing a different ana-
lytical direction. We derive continuous-time stochastic dif-
ferential equations (SDE) that can be understood as weak
approximations (i.e.
approximations in distribution) of
stochastic gradient algorithms. These SDEs contain higher
order terms that vanish as (cid:17) ! 0, but at ﬁnite and small
(cid:17) they offer much needed insight of the algorithms un-
der consideration.
In this sense, our framework can be
viewed as a stochastic parallel of the method of modiﬁed
equations in the analysis of classical ﬁnite difference meth-
ods (Noh & Protter, 1960; Daly, 1963; Hirt, 1968; Warm-
ing & Hyett, 1974). For this reason, we refer to these SDEs
as stochastic modiﬁed equations (SME). Using the SMEs,
we can quantify, in a precise and general way, the leading-
order dynamics of the SGD and its variants. Moreover, the
continuous-time treatment allows the application of opti-
mal control theory to study the problems of adaptive hyper-
parameter adjustments. This gives rise to novel adaptive al-
gorithms and perhaps more importantly, a general method-
ology for understanding and improving stochastic gradient
algorithms.

Notation. We distinguish sequential and dimensional in-
dices by writing a bracket around the latter, e.g. xk;(i) is
the ith coordinate of the vector xk, the kth SGD iterate.

2. Stochastic Modiﬁed Equations

We now introduce the SME approximation. Background
materials on SDEs are found in Supplementary Materials

SME and Adaptive Stochastic Gradient Algorithms

(SM) B and references therein. First, rewrite the SGD iter-
ation rule (2) as

xk+1 (cid:0) xk = (cid:0)(cid:17)rf (xk) +
p

p

(cid:17)Vk;

(3)

where Vk =
(cid:17)(rf (xk) (cid:0) rf(cid:13)k (xk)) is a d-dimensional
random vector. Conditioned on xk, Vk has mean 0 and
covariance matrix (cid:17)(cid:6)(xk) with

(cid:6)(x) =

(rf (x) (cid:0) rfi(x))(rf (x) (cid:0) rfi(x))T :

(4)

1
n

n
X

i=1

Now, consider the Stochastic differential equation

dXt = b(Xt)dt + (cid:27)(Xt)dWt; X0 = x0;

(5)

(cid:1)t(cid:27)(Xk)Zk, Zk (cid:24) N (0; I) resembles

whose Euler discretization Xk+1 = Xk + (cid:1)tb(Xk) +
p
(3) if we set
(cid:1)t = (cid:17), b (cid:24) (cid:0)rf and (cid:27) (cid:24) ((cid:17)(cid:6))1=2. Then, we would
expect (5) to be an approximation of (2) with the identiﬁ-
cation t = k(cid:17). It is now important to discuss the precise
meaning of “an approximation”. The noises that drive the
paths of SGD and SDE are independent processes, hence
we must understand approximations in the weak sense.

Deﬁnition 1. Let 0 < (cid:17) < 1, T > 0 and set N = bT =(cid:17)c.
Let G denote the set of functions of polynomial growth,
i.e. g 2 G if there exists constants K; (cid:20) > 0 such that
jg(x)j < K(1 + jxj(cid:20)). We say that the SDE (5) is an
order (cid:11) weak approximation to the SGD (2) if for every
g 2 G, there exists C > 0, independent of (cid:17), such that for
all k = 0; 1; : : : ; N ,

jEg(Xk(cid:17)) (cid:0) Eg(xk)j < C(cid:17)(cid:11):

The deﬁnition above is standard in numerical analysis of
SDEs (Milstein, 1995; Kloeden & Platen, 2011).
Intu-
itively, weak approximations are close to the original pro-
cess not in terms of individual sample paths, but their dis-
tributions. We now state informally the approximation the-
orem.

Informal Statement of Theorem 1. Let T > 0 and deﬁne
(cid:6) : Rd ! Rd(cid:2)d by (4). Assume f; fi are Lipschitz con-
tinuous, have at most linear asymptotic growth and have
sufﬁciently high derivatives belonging to G. Then,

(i) The stochastic process Xt, t 2 [0; T ] satisfying

dXt = (cid:0)rf (Xt)dt + ((cid:17)(cid:6)(Xt))

1
2 dWt;

(6)

is an order 1 weak approximation of the SGD.

The full statement, proof and numerical veriﬁcation of
Thm. 1 is given in SM. C. We hereafter call equations (6)
and (7) stochastic modiﬁed equations (SME) for the SGD
iterations (2). We refer to the second order approxima-
tion (7) for exact calculations in Sec. 3 whereas for sim-
plicity, we use the ﬁrst order approximation (6) when dis-
cussing acceleration schemes in Sec. 4, where the order of
accuracy is less important.

Thm. 1 allows us to use the SME to deduce distributional
properties of the SGD. This result differs from usual con-
vergence studies in that it describes dynamical behavior
and is derived without convexity assumptions on f or fi.
In the next section, we use the SME to deduce some dy-
namical properties of the SGD.

3. The Dynamics of SGD

3.1. A Solvable SME

We start with a case where the SME is exactly solvable. Let
n = 2, d = 1 and set f (x) = x2 with f1(x) = (x (cid:0) 1)2 (cid:0) 1
and f2(x) = (x + 1)2 (cid:0) 1. Then, the SME (7) for the SGD
iterations on this objective is (see SM. D.1)

dXt = (cid:0)2(1 + (cid:17))Xtdt + 2

(cid:17)dWt;

p

with X0 = x0. This is the well-known Ornstein-Uhlenbeck
process (Uhlenbeck & Ornstein, 1930), which is exactly
solvable (see SM. B.3), yielding the Gaussian distribution

Xt (cid:24) N (x0e(cid:0)2(1+(cid:17))t; (cid:17)

1+(cid:17) (1 (cid:0) e(cid:0)4(1+(cid:17))t)):

We observe that EXt = x0e(cid:0)2(1+(cid:17))t converges exponen-
tially to the optimum x = 0 with rate (cid:0)2(1 + (cid:17)) but
VarXt = (cid:17) (cid:0)1 (cid:0) e(cid:0)4(1+(cid:17))t(cid:1) =(1 + (cid:17)) increases from 0 to
an asymptotic value of (cid:17)=(1 + (cid:17)). The separation t(cid:3) be-
tween the descent phase and the ﬂuctuations phase is given
by EXt(cid:3) =

VarXt(cid:3) , whose solution is

p

t(cid:3) = 1

4(1+(cid:17)) log(1 + (cid:17)+1

(cid:17) x2
0)

For t < t(cid:3), descent dominates and when t > t(cid:3), ﬂuctua-
tion dominates. This two-phase behavior is known for con-
vex cases via error bounds (Moulines, 2011; Needell et al.,
2014). Using the SME, we obtained a precise characteri-
zation of this behavior, including an exact expression for
t(cid:3). In Fig. 1, we verify the SME predictions regarding the
mean, variance and the two-phase behavior.

(ii) The stochastic process Xt, t 2 [0; T ] satisfying

3.2. Stochastic Asymptotic Expansion

dXt = (cid:0)r(f (Xt) + (cid:17)

4 jrf (Xt)j2)dt + ((cid:17)(cid:6)(Xt))

is an order 2 weak approximation of the SGD.

1
2 dWt
(7)

In general, we cannot expect to solve the SME exactly, es-
pecially for d > 1. However, observe that the noise terms
in the SMEs (6) and (7) are O((cid:17)1=2). Hence, we can write
(cid:17)X1;t + : : :
Xt as an asymptotic series Xt = X0;t +

p

SME and Adaptive Stochastic Gradient Algorithms

(a)

(b)

(a)

(b)

Figure 1. Comparison of the SME predictions vs SGD for the sim-
ple quadratic objective. We set x0 = 1, (cid:17) =5e-3. (a) The pre-
dicted mean and standard deviations agree well with the empir-
ical moments of the SGD, obtained by averaging 5e3 runs. (b)
50 sample SGD paths the predicted transition time k(cid:3) = t(cid:3)=(cid:17).
We observe that k(cid:3) corresponds to the separation of descent and
ﬂuctuating regimes for typical sample paths.

Figure 2. Comparison of the moments of SGD iterates with the
SME and its asymptotic approximation (Asymp, Eq. 8) for the
non-convex objective with (cid:14) = 0:2 and (cid:15) = 0:1. The landscape
is shown in (a). In (b), we plot the magnitude of the mean and the
covariance matrix for the SGD, SME and Asymp. We take (cid:17) =1e-
4 and x0 = (1; 1:5). All moments are obtained by sampling over
1e3 runs (the SME and Asymp are integrated numerically). We
observe a good agreement.

where each Xj;t is a stochastic process with initial condi-
tion X0;0 = x0 and Xj;0 = 0 for j (cid:21) 1. We substitute
this into the SME and expand in orders of (cid:17)1=2 and equate
the terms of the same order to get equations for Xj;t for
j (cid:21) 0. This procedure is justiﬁed rigorously in Freidlin
et al. (2012). We obtain to leading order (see SM. B.5),

learning rate and momentum parameter adjustment poli-
cies. These are particular illustrations of a general method-
ology to analyze and improve upon SGD variants. We will
focus on the one dimensional case d = 1, and subsequently
apply the results to high dimensional problems by local di-
agonal approximations.

Xt (cid:24) N (X0;t; (cid:17)St);

(8)

4.1. Learning Rate

_X0;t = (cid:0)rf (X0;t); X0;0 = x0 and
where X0;t solves
_St = (cid:0)StHt (cid:0) HtSt + (cid:6)t, where Ht = Hf (X0;t), with
Hf denoting the Hessian of f , and (cid:6)t = (cid:6)(X0;t), S0 = 0.
It is then possible to deduce the dynamics of the SGD. For
example, there is generally a transition between descent
and ﬂuctuating regimes. St has a steady state (assuming
it is asymptotically stable) with jS1j (cid:24) j(cid:6)1j=jH1j. This
means that one should expect a ﬂuctuating regime where
the covariance of the SGD is of order O((cid:17)j(cid:6)1j=jH1j).
Preceding this ﬂuctuating regime is a descent regime gov-
erned by the gradient ﬂow.

We validate our approximations on a non-convex objective.
Set d = 2, n = 3 with the sample objectives f1(x) = x2
(1),
f2(x) = x2
(2) and f3(x) = (cid:14) cos(x(1)=(cid:15)) cos(x(2)=(cid:15)). In
Fig. 2(a), we plot f for (cid:15) = 0:1; (cid:14) = 0:2, showing the
complex landscape. In Fig. 2(b), we compare the SGD mo-
ments jE(xk)j and jCov(xk)j with predictions of the SME
and its asymptotic approximation (8). We observe that our
approximations indeed hold for this objective.

4. Adaptive Hyper-parameter Adjustment

We showed in the previous section that the SME formula-
tion help us better understand the precise dynamics of the
SGD. The natural question is how this can translate to de-
signing practical algorithms. In this section, we exploit the
continuous-time nature of our framework to derive adaptive

4.1.1. OPTIMAL CONTROL FORMULATION

1D SGD iterations with learning rate adjustment can be
written as

xk+1 = xk (cid:0) (cid:17)ukf 0(xk);

(9)

where uk 2 [0; 1] is the adjustment factor and (cid:17) is the
maximum allowed learning rate. The corresponding SME
for (9) is given by (SM. D.1)

dXt = (cid:0)utf 0(Xt)dt + ut

p(cid:17)(cid:6)(Xt)dWt;

(10)

where ut 2 [0; 1] is now the continuous time analogue of
the adjustment factor uk with the usual identiﬁcation t =
k(cid:17). The effect of learning rate adjustment on the dynamics
of SGD is clear. Larger uk results in a larger drift term
in the SME and hence faster initial descent. However, the
same factor is also multiplied to the noise term, causing
greater asymptotic ﬂuctuations. The optimal learning rate
schedule must balance of these two effects. The problem
can therefore be posed as follows: given f; fi, how can we
best choose a schedule or policy for adjusting the learning
rate in order to minimize Ef at the end of the run? More
precisely, this can be cast as an optimal control problem1

Ef (XT ) subject to (10);

min
u

1See SM. E for a brief overview of optimal control theory.

0246k (×100)0.20.61.0MomentsMean  StdSGDSMESGDSME0246k (×100)0.20.61.0xkk*x(1)x(2)f01234k (×105)1234Moments|Mean|  |Cov/η|12SGDSMEAsympSGDSMEAsympSME and Adaptive Stochastic Gradient Algorithms

where the time-dependent function u is minimized over an
admissible control set to be speciﬁed. To make headway
analytically, we now turn to a simple quadratic objective.

4.1.2. OPTIMAL CONTROL OF THE LEARNING RATE

2 a(x (cid:0) b)2 with a; b 2 R.
Consider the objective f (x) = 1
Moreover, we assume the fi’s are such that (cid:6)(x) = (cid:6) > 0
is a positive constant. The SME is then

dXt = (cid:0)aut(Xt (cid:0) b)dt + ut

p(cid:17)(cid:6)dWt:

(11)

Now, assume u take values in the non-random control set
containing all Borel-measurable functions from [0; T ] to
[0; 1]. Deﬁning mt = Ef (Xt), and applying Itˆo formula
to (11), we have

_mt = (cid:0)2autmt + 1

2 a(cid:17)(cid:6)u2
t :

(12)

Hence, we may now recast the control problem as

min
u:[0;T ]![0;1]

mT subject to (12):

This problem can solved by dynamic programming, using
the Hamilton-Jacobi-Bellman equation (Bellman, 1956).
We obtain the optimal control policy (SM. E.3)

u(cid:3)
t =

(
1
min(1; 2mt
(cid:17)(cid:6) )

a (cid:20) 0;
a > 0:

(13)

This policy is of feed-back form since it depends on the
current value of the controlled variable mt. Let us interpret
the solution. First, if a < 0 we always set the maximum
learning rate ut = 1. This makes sense because we have
a concave objective where symmetrical ﬂuctuations about
any point x results in a lower average value of f (x). Hence,
not only do high learning rates improve descent, the high
ﬂuctuations that accompany it also lowers Ef . Next, For
the convex case a > 0, the solution tells us that when the
objective value is large compared to variations in the gradi-
ent, we should use the maximum learning rate. When the
objective decreases sufﬁciently, ﬂuctuations will dominate
and hence we should lower the learning rate according to
the feed-back policy ut = 2mt=(cid:17)(cid:6).

With the policy (13), we can solve (12) and plug the solu-
tion for mt back into (13) to obtain the annealing schedule

(

1

u(cid:3)
t =

1
1+a(t(cid:0)t(cid:3))

a (cid:20) 0 or t (cid:20) t(cid:3);
a > 0 and t > t(cid:3);

where t(cid:3) = (1=2a) log(4m0=(cid:17)(cid:6) (cid:0) 1). Note that by
putting a = 2; b = 0; (cid:6) = 4, for small (cid:17), this expres-
sion agrees with the transition time (8) between descent
and ﬂuctuating phases for the SGD dynamics considered in
Sec. 3.1. Thus, this annealing schedule says that maximum

learning rate should be used for descent phases, whereas
(cid:24) 1=t decay on learning rate should be applied after onset
of ﬂuctuations. Our annealing result agree asymptotically
with the commonly studied annealing schedules (Moulines,
2011; Shamir & Zhang, 2013), but the difference is that we
suggest maximum learning rate before the onset of ﬂuctua-
tions. Of course, the key limitation is that our result is only
valid for this particular objective. This naturally brings us
to the next question: how does one apply the optimal con-
trol results to general objectives?

4.1.3. APPLICATION TO GENERAL OBJECTIVES

Pd

Now, we turn to the setting where d > 1 and f; fi are
not necessarily quadratic. The most important result in
Sec. 4.1.2 is the feed-back control law (13). To apply it, we
make a local diagonal-quadratic assumption: we assume
that for each x 2 Rd, there exists a(i); b(i) 2 R so that
f (x) (cid:25) 1
i=1 a(i)(x(i) (cid:0) b(i))2 holds locally in x. We
2
also assume (cid:6)(x) (cid:25) diagf(cid:6)(1); : : : ; (cid:6)(d)g where each (cid:6)(i)
is locally constant. By considering a separate learning rate
scale u(i) for each trainable dimension, the control problem
decouples to d separate problems of the form considered in
Sec. 4.1.2. And hence, we may set u(cid:3)
(i) element-wise ac-
cording to the policy (13).

k;(i); xk;(i); x2

the diagonal-quadratic as-
Since we only assume that
the terms a(i), b(i), (cid:6)(i) and
sumption holds locally,
m(i) (cid:25) 1
2 a(i)(x(i) (cid:0) b(i))2 must be updated on the
ﬂy. There are potentially many methods for doing so.
The approach we take exploits the linear relationship
rf(i) (cid:25) a(i)(x(i) (cid:0) b(i)). Consequently, we may es-
timate a(i); b(i) via linear regression on the ﬂy:
for
each dimension, we maintain exponential moving aver-
ages (EMA) fgk;(i); g2
k;(i); xgk;(i)g where
For example, gk+1;(i) =
gk;(i) = rf(cid:13)k (xk)(i).
(cid:12)k;(i)gk;(i) + (1 (cid:0) (cid:12)k;(i))gk;(i). The EMA decay parame-
ter (cid:12)k;(i) controls the effective averaging window size. We
adaptively adjust it so that it is small when gradient vari-
ations are large, and vice versa. We employ the heuris-
tic (cid:12)k+1;(i) = (g2
k;(i). This is similar to
the approach in Schaul et al. (2013). We also clip each
(cid:12)k+1;(i) to [(cid:12)min; (cid:12)max] to improve stability. Here, we use
[0:9; 0:999] for all experiments, but we checked that per-
formance is insensitive to these values. We can now com-
pute ak;(i); bk;(i) by the ordinary-least-squares formula and
(cid:6)k;(i) as the variance of the gradients:

k;(i) (cid:0) g2

k;(i))=g2

ak;(i) =

;

x2

k;(i)

gxk;(i) (cid:0) gk;(i)xk;(i)
k;(i) (cid:0) x2
gk;(i)
ak;(i)
k;(i):

k;(i) (cid:0) g2

;

bk;(i) = xk;(i) (cid:0)

(cid:6)k;(i) = g2

(14)

SME and Adaptive Stochastic Gradient Algorithms

Algorithm 1 controlled SGD (cSGD)

Hyper-parameters: (cid:17), u0
Initialize x0; (cid:12)0;(i) = 0:9 8i
for k = 0 to (#iterations (cid:0) 1) do

Compute sample gradient rf(cid:13)k (xk)
for i = 1 to d do

k;(i); xgk;(i)g

k;(i); xk;(i); x2

Update EMA fgk;(i); g2
with decay parameter (cid:12)k;(i)
Compute ak;(i), bk;(i), (cid:6)k;(i) using (14)
Compute u(cid:3)
k;(i) using (15)
k;(i) (cid:0) g2
(cid:12)k+1;(i) = (g2
uk+1;(i) = (cid:12)k;(i)uk;(i) + (1 (cid:0) (cid:12)k;(i))u(cid:3)
xk+1;(i) = xk;(i) (cid:0) (cid:17)uk;(i)rf(cid:13)k (xk)(i)

k;(i))=g2

k;(i) and clip

k;(i)

end for

end for

This allows us to estimate the policy (13) as

(1

)

(cid:17)(cid:6)k;(i)

u(cid:3)
k;(i) =

min(1; ak;(i)(xk;(i)(cid:0)bk;(i))2

ak;(i) (cid:20) 0;
ak;(i) > 0:
(15)
for i = 1; 2; : : : ; d. Since quantities are computed from
exponentially averaged sources, we should also update our
learning rate policy in the same way. The algorithm is sum-
marized in Alg. 1. Due to its optimal control origin, we
hereafter call this algorithm the controlled SGD (cSGD)

Remark 1. Alg. 1 can similarly be applied to mini-batch
SGD. Let the batch-size be M , which reduces the covari-
ance by M times and so (cid:17) in the SME is replaced by (cid:17)=M .
However, at the same time estimating (cid:6)k from mini-batch
gradient sample variances will underestimate (cid:6)(xk) by a
factor of M . Thus the product (cid:17)(cid:6)k remains unchanged and
Alg. 1 can be applied with no changes.

Remark 2. The additional overheads in cSGD are
from maintaining exponential averages and estimating
ak; bk; (cid:6)k on the ﬂy with the relevant formulas. These are
O(d) operations and hence scalable. Our current rough
implementation runs (cid:24) 40 (cid:0) 60% slower per epoch than
the plain SGD. This is expected to be improved by optimiza-
tion, parallelization or updating quantities less frequently.

4.1.4. PERFORMANCE ON BENCHMARKS

Let us test cSGD on common deep learning benchmarks.
We consider three different models. M0: a fully con-
nected neural network with one hidden layer and ReLU
activations, trained on the MNIST dataset (LeCun et al.,
1998); C0: a fully connected neural network with two hid-
den layers and Tanh activations, trained on the CIFAR-10
dataset (Krizhevsky & Hinton, 2009); C1: a convolution
network with four convolution layers and two fully con-
nected layers also trained on CIFAR-10. Model details

are found in SM. F.1. In Fig. 3, we compare the perfor-
mance of cSGD with Adagrad (Duchi et al., 2011) and
Adam (Kingma & Ba, 2015) optimizers. We illustrate in
particular their sensitivity to different learning rate choices
by performing a log-uniform random search over three or-
ders of magnitude. We observe that cSGD is robust to
different initial and maximum learning rates (provided the
latter is big enough, e.g. we can take (cid:17) = 1 for all ex-
periments) and changing network structures, while obtain-
ing similar performance to well-tuned versions of the other
methods (see also Tab. 1). In particular, notice that the best
learning rates found for Adagrad and Adam generally differ
for different neural networks. On the other hand, many val-
ues can be used for cSGD with little performance loss. For
brevity we only show the test accuracies, but the training
accuracies have similar behavior (see SM. F.5).

4.2. Momentum Parameter

Another practical way of speeding up the plain SGD is to
employ momentum updates - an idea dating back to deter-
ministic optimization (Polyak, 1964; Nesterov, 1983; Qian,
1999). However, the stochastic version has important dif-
ferences, especially in regimes where sampling noise dom-
inates. Nevertheless, provided that the momentum param-
eter is well-tuned, the momentum SGD (MSGD) is very
effective in speeding up convergence, particularly in early
stages of training (Sutskever et al., 2013).

Selecting an appropriate momentum parameter is important
in practice. Typically, generic values (e.g. 0.9, 0.99) are
suggested without fully elucidating their effect on the SGD
dynamics. In this section, we use the SME framework to
analyze the precise dynamics of MSGD and derive effec-
tive adaptive momentum parameter adjustment policies.

4.2.1. SME FOR MSGD

The SGD with momentum can be written as the following
coupled updates

vk+1 = (cid:22)vk (cid:0) (cid:17)f 0
(cid:13)k
xk+1 = xk + vk+1:

(xk);

(16)

The parameter (cid:22) is the momentum parameter taking values
in the range 0 (cid:20) (cid:22) (cid:20) 1. Intuitively, the momentum term
vk remembers past update directions and pushes along xk,
which may otherwise slow down at e.g. narrow parts of the
landscape. The corresponding SME is now a coupled SDE

dVt = ((cid:0)(cid:17)(cid:0)1(1 (cid:0) (cid:22))Vt (cid:0) f 0(Xt))dt + ((cid:17)(cid:6)(Xt))
dXt = (cid:17)(cid:0)1Vtdt:

1

2 dWt;

(17)

This can be derived by comparing (16) with the Euler dis-
cretization scheme of (17) and matching moments. Details
can be found in SM. D.3.

SME and Adaptive Stochastic Gradient Algorithms

(a) M0 (fully connected NN, MNIST)

(b) C0 (fully connected NN, CIFAR-10)

(c) C1 (CNN, CIFAR-10)

Figure 3. cSGD vs Adagrad and Adam for different models and
datasets, with different hyper-parameters.
For M0, we per-
form log-uniform random search with 50 samples over intervals:
cSGD: u0 2[1e-2,1], (cid:17) 2[1e-1,1]; Adagrad: (cid:17) 2[1e-3,1]; Adam:
(cid:17) 2[1e-4,1e-1]. For C0, we perform same search over intervals:
cSGD: u0 2[1e-2,1], (cid:17) 2[1e-1,1]; Adagrad: (cid:17) 2[1e-3,1]; Adam:
(cid:17) 2[1e-6,1e-3]. We average the resulting learning curves for
each choice over 10 runs. For C1, due to long training times we
choose 5 representative learning rates for each method. cSGD:
(cid:17) 2f1e-2,5e-2,1e-1,5e-1,1g, u0 = 1; Adagrad: (cid:17) 2f1e-3,5e-
3,1e-2,5e-2,1e-1g; Adam: (cid:17) 2f5e-4,1e-3,1e-2,2e-2,5e-2g. One
sample learning curve is generated for each choice. In all cases,
we use mini-batches of size 128. We evaluate the resulting learn-
ing curves by the area-under-curve. The worst, median and best
learning curves are shown as dotted, solid, and dot-dashed lines
respectively. The shaded areas represent the distribution of learn-
ing curves for all searched values. We observe that cSGD is
relatively robust with respect to initial/maximum learning rates
and the network structures, and requires little tuning while having
comparable performance to well-tuned versions of the other meth-
ods (see Tab. 1). This holds across different models and datasets.

4.2.2. THE EFFECT OF MOMENTUM

As in Sec. 4.1, we take the prototypical example
f (x) = 1
2 a(x (cid:0) b)2 with (cid:6) constant and study the ef-
fect of incorporating momentum updates. Deﬁne Mt =
(Ef (Xt); EV 2
t ; EVtf 0(Xt)) 2 R3. By applying Itˆo for-
mula to (17), we obtain the ODE system

(cid:18) 0

_Mt = A((cid:22))Mt+B;
a=(cid:17)
0
(cid:0)2
(cid:0)(1(cid:0)(cid:22))=(cid:17)

(cid:19)

0 (cid:0)2(1(cid:0)(cid:22))=(cid:17)
(cid:0)2

1=(cid:17)

; B =

:

(18)

(cid:17)

(cid:16) 0
(cid:17)(cid:6)
0

A((cid:22)) =

(a)

(b)

Figure 4. (a) Comparison of the SME prediction (18) with SGD
for the same quadratic example in Sec. 3.1, which has a = 2,
b = 0 and (cid:6) = 4. We set (cid:17) =5e-3 so that (cid:22)opt = 0:8. We
plot the mean of f averaged over 1e5 SGD runs against the SME
predictions for (cid:22) = 0:65; 0:8; 0:95. We observe that in all cases
the approximation is accurate. In particular, the SME correctly
predicts the effect of momentum: (cid:22) = (cid:22)opt gives the best aver-
age initial descent rate, (cid:22) > (cid:22)opt causes oscillatory behavior, and
increasing (cid:22) generally increases asymptotic ﬂuctuations. (b) The
dynamics of averaged equation (20), which serves as an approxi-
mation of the solution of the full SME moment equation (18).

If a < 0, A((cid:22)) has a positive eigenvalue and hence Mt
diverges exponentially. Since f is negative, its value must
then decrease exponentially for all (cid:22), and the descent rate
is maximized at (cid:22) = 1. The more interesting case is
when a > 0.
Instead of solving (18), we observe that
all eigenvalues of A((cid:22)) have negative real parts as long as
(cid:22) < 1. Therefore, Mt has an exponential decay domi-
nated by jR(cid:21)((cid:22))j, where R denotes real part and (cid:21)((cid:22)) =
(cid:17) [(1 (cid:0) (cid:22)) (cid:0) p(1 (cid:0) (cid:22))2 (cid:0) 4a(cid:17)] is the eigenvalue with
(cid:0) 1
the least negative real part. Observe that the descent rate
jR(cid:21)((cid:22))j is maximized at

(cid:22)opt = max(1 (cid:0) 2

a(cid:17); 0)

(19)

p

(cid:17)2 (cid:6)

(cid:17)(cid:6)
4(1(cid:0)(cid:22))

and when (cid:22) > (cid:22)opt, (cid:21) becomes complex. Also, from (18)
we have Mt ! M1 = (cid:0)A((cid:22))(cid:0)1B = (cid:0)
2(1(cid:0)(cid:22)) 0 (cid:1),
provided the steady state is stable. The role of momen-
tum in this problem is now clear. To leading order in (cid:17)
we have (cid:21)((cid:22)) (cid:24) (cid:0)2a=(1 (cid:0) (cid:22)) for (cid:22) (cid:20) (cid:22)opt. Hence, any
non-zero momentum will improve the initial convergence
rate. In fact, the choice (cid:22)opt is optimal and above it, oscil-
lations set in because of a complex (cid:21). At the same time,
increasing momentum also causes increment in eventual
ﬂuctuations, since jM1j = O((1 (cid:0) (cid:22))(cid:0)1). In Fig. 4(a),
we demonstrate the accuracy of the SME prediction (18)
by comparing MSGD iterations. Armed with an intuitive
understanding of the effect of momentum, we can now use
optimal control to design policies to adapt the momentum
parameter.

4.2.3. OPTIMAL CONTROL OF THE MOMENTUM

PARAMETER

For a < 0, we have discussed previously that (cid:22) = 1 max-
imizes the descent rate and ﬂuctuations generally help de-

01234Epoch0.00.20.40.60.81.0Test acccSGDηu04e-26e-39e-101234EpochAdagradη2e-34e-23e-101234EpochAdamη9e-22e-26e-301234Epoch0.00.10.20.30.40.5Test acccSGDηu03e-31e-23e-101234EpochAdagradη1e-31e-28e-201234EpochAdamη1e-68e-42e-4050100150Epoch0.00.20.40.60.81.0Test acccSGDηu01e-21e-15e-1050100150EpochAdagradη5e-15e-21e-2050100150EpochAdamη5e-21e-25e-400.511.52k (×100)1e-21e-11e01e1fMSGD  SMEμ<μoptμ=μoptμ>μoptμ<μoptμ=μoptμ>μopt00.511.52k (×100)1e-21e-11e01e1mkηAvg eqnμ<μoptμ=μoptμ>μoptSME and Adaptive Stochastic Gradient Algorithms

crease concave functions. Thus, the optimal control is al-
ways (cid:22) = 1. The non-trivial case is when a > 0. Due
to its bi-linearity, directly controlling (18) leads to bang-
bang type solutions2 that are rarely feed-back laws (Parda-
los & Yatsenko, 2010) and thus difﬁcult to apply in prac-
tice. Instead, we notice that the descent rate is dominated
by R(cid:21)((cid:22)), and the leading order asymptotic ﬂuctuations is
(cid:17)(cid:6)=(4(1 (cid:0) (cid:22))), hence we may consider

_mt = R(cid:21)((cid:22))(mt (cid:0) m1((cid:22)))

(20)

where mt 2 R and m1((cid:22)) = (cid:17)(cid:6)=(4(1 (cid:0) (cid:22))) is the lead-
ing order estimate of jM1j. Equation (20) can be under-
stood as the approximate evolution, in an averaged sense, of
the magnitude of Mt. Fig. 4(b) shows that (20) is a reason-
able approximation of the dynamics of MSGD. This allows
us to pose the optimal control problem on the momentum
parameter as

min
(cid:22):[0;T ]![0;1]

mT subject to (20);

with (cid:22) = (cid:22)t. Solving this control problem yields the (ap-
proximate) feed-back policy (SM. E.4)

(

(cid:22)(cid:3)

t =

1
min((cid:22)opt; max(0; 1 (cid:0) (cid:17)(cid:6)
4mt

))

a (cid:20) 0;
a > 0;

(21)

with (cid:22)opt given in (19). This says that when far from opti-
mum (mt large), we set (cid:22) = (cid:22)opt which maximizes average
descent rate. When mt=(cid:17)(cid:6) (cid:24)
a(cid:17), ﬂuctuations set in and
we lower (cid:22).

p

As in Sec. 4.1.3, we turn the control policy above into
a generally applicable algorithm by performing local
diagonal-quadratic approximations and estimating the rele-
vant quantities on the ﬂy. The resulting algorithm is mostly
identical to Alg. 1 except we now use (21) to update (cid:22)k;(i)
and SGD updates are replaced with MSGD updates (see
S.M. F.4 for the full algorithm). We refer to this algorithm
as the controlled momentum SGD (cMSGD).

4.2.4. PERFORMANCE ON BENCHMARKS

We apply cMSGD to the same three set-ups in Sec. 4.1.4,
and compare its performance to the plain Momentum SGD
with ﬁxed momentum parameters (MSGD) and the anneal-
ing schedule suggested in (Sutskever et al., 2013), with
(cid:22)k = min(1 (cid:0) 2(cid:0)1(cid:0)log2(bk=250c+1); (cid:22)max) (MSGD-A). In
Fig. 5, we perform a log-uniform search over the hyper-
parameters (cid:22)0, (cid:22) and (cid:22)max. We see that cMSGD achieves
superior performance to MSGD and MSGD-A (see Tab. 1),

2Bang-bang solutions are control solutions lying on the bound-
ary of the control set and abruptly jumps among the boundary val-
ues. For example, in this case it jumps between (cid:22) = 0 and (cid:22) = 1
repeatedly.

(a) M0 (fully connected NN, MNIST)

(b) C0 (fully connected NN, CIFAR-10)

(c) C1 (CNN, CIFAR-10)

Figure 5. cMSGD vs MSGD and MSGD-A on the same three
models. We set (cid:17) =1e-2 for M0 and (cid:17) =1e-3 for C0, C1.
For M0 and C0, we perform a log-uniform random search
for 1 (cid:0) (cid:22)0 and 1 (cid:0) (cid:22) in [5e-3,5e-1]. For C1, we sample
(cid:22)0; (cid:22); (cid:22)max 2f0.9,0.95,0.99,0.995,0.999g. The remaining set-up
is identical to that in Fig. 3. Again, we observe that cMSGD is an
adaptive scheme that is robust to varying hyper-parameters and
network structures, and out-performs MSGD and MSGD-A.

Table 1. Best average test accuracy found by random/grid search.

CSGD ADAGRAD ADAM

CMSGD MSGD MSGD-A

M0
C0
C1

0.925
0.461
0.875

0.923
0.457
0.878

0.924
0.460
0.881

0.924
0.461
0.876

0.914
0.453
0.868

0.908
0.446
0.869

especially when the latter has badly tuned (cid:22); (cid:22)max. More-
over, it is insensitive to the choice of initial (cid:22)0. Just like
cSGD, this holds across changing network structures. Fur-
ther, cMSGD also adapts to other hyper-parameter varia-
tions. In Fig. 6, we take tuned (cid:22); (cid:22)max (and any (cid:22)0) and
vary the learning rate (cid:17). We observe that cMSGD adapts to
the new learning rates whereas the performance of MSGD
and MSGD-A deteriorates and (cid:22); (cid:22)max must be re-tuned
to obtain reasonable accuracy. In fact, it is often the case
that MSGD and MSGD-A diverge when (cid:17) is large, whereas
cMSGD remains stable.

5. Related Work

Classical bound-type convergence results for SGD and
variants include Moulines (2011); Shamir & Zhang (2013);

01234Epoch0.00.20.40.60.81.0Test acccMSGDμ00.990.900.9401234EpochMSGDμ0.550.940.9701234EpochMSGD-Aμmax0.550.990.9201234Epoch0.00.10.20.30.40.5Test acccMSGDμ00.520.960.9801234EpochMSGDμ0.530.970.9901234EpochMSGD-Aμmax0.580.960.99050100150Epoch0.00.20.40.60.81.0Test acccMSGDμ00.950.9950.999050100150EpochMSGDμ0.9990.990.9050100150EpochMSGD-Aμmax0.9990.990.9SME and Adaptive Stochastic Gradient Algorithms

(a) M0 (fully connected NN, MNIST)

(b) C0 (fully connected NN, CIFAR-10)

(c) C1 (CNN, CIFAR-10)

Figure 6. Comparing the sensitivity of cMSGD, MSGD and
MSGD-A to different learning rates. The set-up is same as that in
Fig. 5 except that for MSGD and MSGD-A, we now ﬁx (cid:22); (cid:22)max
to be the best values found in Fig. 5 for each experiment, but we
vary the learning rate in the ranges: M0 and C0: (cid:17) 2[1e-3,1], C1:
(cid:17) 2f1e-3,5e-3,1e-2,2e-2,1e-1g. For cMSGD, we saw from Fig. 5
that the value of (cid:22)0 is mostly inconsequential, so we simply set
(cid:22)0 = 0 and vary (cid:17) in the same ranges. We observe that the unlike
MSGD and MSGD-A, cMSGD is generally robust to changing
learning rates and this further conﬁrms its adaptive properties.

Bach & Moulines (2013); Needell et al. (2014); Xiao &
Zhang (2014); Shalev-Shwartz & Zhang (2014). Our ap-
proach differs in that we obtain precise, albeit only distri-
butional, descriptions of the SGD dynamics that hold in
non-convex situations.

In the vein of continuous approximation to stochastic al-
gorithms, a related body of work is stochastic approxima-
tion theory (Kushner & Yin, 2003; Ljung et al., 2012),
which establish ODEs as almost sure limits of trajecto-
ries of stochastic algorithms. In contrast, we obtain SDEs
that are weak limits that approximate not individual sample
paths, but their distributions. Other deterministic continu-
ous time approximation methods include Su et al. (2014);
Krichene et al. (2015); Wibisono et al. (2016).

Related work in SDE approximations of
the SGD
are Mandt et al. (2015; 2016), where the authors derived
In contrast, we estab-
the ﬁrst order SME heuristically.
lish a rigorous statement for this type of approximations
(Thm. 1). Moreover, we use asymptotic analysis and con-
trol theory to translate understanding into practical algo-

rithms. Outside of the machine learning literature, similar
modiﬁed equation methods also appear in numerical anal-
ysis of SDEs (Zygalakis, 2011) and quantifying uncertain-
ties in ODEs (Conrad et al., 2015).

The second half of our work deals with practical prob-
lems of adaptive selection of the learning rate and mo-
mentum parameter. There is abundant literature on learn-
ing rate adjustments, including annealing schedules (Rob-
bins & Monro, 1951; Moulines, 2011; Xu, 2011; Shamir
& Zhang, 2013), adaptive per-element adjustments (Duchi
et al., 2011; Zeiler, 2012; Tieleman & Hinton, 2012;
Kingma & Ba, 2015) and meta-learning (Andrychowicz
et al., 2016). Our approach differs in that optimal control
theory provides a natural, non-black-box framework for de-
veloping dynamic feed-back adjustments, allowing us to
obtain adaptive algorithms that are truly robust to chang-
ing model settings. Our learning rate adjustment policy is
similar to Schaul et al. (2013); Schaul & LeCun (2013)
based on one-step optimization, although we arrive at it
from control theory. Our method may also be easier to
implement because it does not require estimating diago-
nal Hessians via back-propagation. There is less litera-
ture on momentum parameter selection. A heuristic an-
nealing schedule (referred to as MSGD-A earlier) is sug-
gested in Sutskever et al. (2013), based on the original work
of Nesterov (1983). The choice of momentum parameter in
deterministic problems is discussed in Qian (1999); Nes-
terov (2013). To the best of our knowledge, a systematic
stochastic treatment of adaptive momentum parameter se-
lection for MSGD has not be considered before.

6. Conclusion and Outlook

Our main contribution is twofold. First, we propose the
SME as a uniﬁed framework for quantifying the dynam-
ics of SGD and its variants, beyond the classical convex
regime. Tools from stochastic calculus and asymptotic
analysis provide precise dynamical description of these al-
gorithms, which help us understand important phenomena,
such as descent-ﬂuctuation transitions and the nature of ac-
celeration schemes. Second, we use control theory as a
natural framework to derive adaptive adjustment policies
for the learning rate and momentum parameter. This trans-
lates to robust algorithms that requires little tuning across
multiple datasets and model choices.

An interesting direction of future work is extending the
SME framework to develop adaptive adjustment schemes
for other hyper-parameters in SGD variants, such as
Polyak-Ruppert Averaging (Polyak & Juditsky, 1992),
SVRG (Johnson & Zhang, 2013) and elastic averaging
SGD (Zhang et al., 2015). More generally, the SME frame-
work may be a promising methodology for the analysis and
design of stochastic gradient algorithms and beyond.

01234Epoch0.00.20.40.60.81.0Test acccMSGDη1e-39e-12e-101234EpochMSGDη9e-12e-33e-201234EpochMSGD-Aη1e-32e-22e-101234Epoch0.00.10.20.30.40.5Test acccMSGDη1e-32e-28e-201234EpochMSGDη12e-21e-301234EpochMSGD-Aη1e-36e-34e-2050100150Epoch0.00.20.40.60.81.0Test acccMSGDη2e-11e-11e-2050100150EpochMSGDη1e-11e-21e-3050100150EpochMSGD-Aη1e-11e-21e-3SME and Adaptive Stochastic Gradient Algorithms

Acknowledgements

We would like to thank the anonymous reviewers for their
constructive comments. We are also grateful for the many
discussions with Dr Sixin Zhang. This work is supported in
part by Major Program of NNSFC under grant 91130005,
DOE DE-SC0009248, and ONR N00014-13-1-0338.

References

Andrychowicz, Marcin, Denil, Misha, Gomez, Sergio,
Hoffman, Matthew W, Pfau, David, Schaul, Tom, and
de Freitas, Nando. Learning to learn by gradient descent
by gradient descent. In Advances in Neural Information
Processing Systems, pp. 3981–3989, 2016.

Bach, Francis and Moulines, Eric. Non-strongly-convex
smooth stochastic approximation with convergence rate
O(1/n). In Advances in Neural Information Processing
Systems, pp. 773–781, 2013.

Bellman, Richard. Dynamic programming and Lagrange
multipliers. Proceedings of the National Academy of Sci-
ences, 42(10):767–769, 1956.

Conrad, Patrick R, Girolami, Mark, S¨arkk¨a, Simo, Stuart,
Andrew, and Zygalakis, Konstantinos. Probability mea-
sures for numerical solutions of differential equations.
arXiv preprint arXiv:1506.04592, 2015.

Daly, Bart J. The stability properties of a coupled pair of
non-linear partial difference equations. Mathematics of
Computation, 17(84):346–360, 1963.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research,
12(Jul):2121–2159, 2011.

Freidlin, Mark I, Sz¨ucs, Joseph, and Wentzell, Alexan-
der D. Random perturbations of dynamical systems, vol-
ume 260. Springer Science & Business Media, 2012.

Hirt, CW. Heuristic stability theory for ﬁnite-difference
equations. Journal of Computational Physics, 2(4):339–
355, 1968.

Johnson, Rie and Zhang, Tong. Accelerating stochastic
gradient descent using predictive variance reduction. In
Advances in Neural Information Processing Systems, pp.
315–323, 2013.

Kingma, Diederik and Ba, Jimmy. Adam: A method for

stochastic optimization. ICLR, 2015.

Kloeden, P. E. and Platen, E. Numerical Solution of
Stochastic Differential Equations. Springer, New York,
corrected edition, June 2011.

Krichene, Walid, Bayen, Alexandre, and Bartlett, Peter L.
Accelerated mirror descent in continuous and discrete
time. In Advances in neural information processing sys-
tems, pp. 2845–2853, 2015.

Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple

layers of features from tiny images. 2009.

Kushner, Harold and Yin, G George. Stochastic approxi-
mation and recursive algorithms and applications, vol-
ume 35. Springer Science & Business Media, 2003.

LeCun, Yann, Cortes, Corinna, and Burges, Christo-
pher JC. The mnist dataset of handwritten digits. URL
http://yann. lecun. com/exdb/mnist, 1998.

Ljung, Lennart, Pﬂug, Georg Ch, and Walk, Harro.
Stochastic approximation and optimization of random
systems, volume 17. Birkh¨auser, 2012.

Mandt, Stephan, Hoffman, Matthew D, and Blei, David M.
Continuous-time limit of stochastic gradient descent re-
visited. In NIPS-2015, 2015.

Mandt, Stephan, Hoffman, Matthew D, and Blei, David M.
A variational analysis of stochastic gradient algorithms.
arXiv preprint arXiv:1602.02666, 2016.

Milstein, GN. Numerical integration of stochastic differen-
tial equations, volume 313. Springer Science & Business
Media, 1995.

Moulines, Eric and, Francis R. Non-asymptotic analysis of
stochastic approximation algorithms for machine learn-
ing. In Advances in Neural Information Processing Sys-
tems, pp. 451–459, 2011.

Needell, Deanna, Ward, Rachel, and Srebro, Nati. Stochas-
tic gradient descent, weighted sampling, and the ran-
domized algorithm. In Advances in Neural Information
Processing Systems, pp. 1017–1025, 2014.

Nesterov, Yurii. A method of solving a convex program-
ming problem with convergence rate O(1/k2). In Soviet
Mathematics Doklady, volume 27, pp. 372–376, 1983.

Nesterov, Yurii. Introductory lectures on convex optimiza-
tion: A basic course, volume 87. Springer Science &
Business Media, 2013.

Noh, WF and Protter, MH. Difference methods and the
equations of hydrodynamics. Technical report, Califor-
nia. Univ., Livermore. Lawrence Radiation Lab., 1960.

Pardalos, Panos M and Yatsenko, Vitaliy A. Optimization
and Control of Bilinear Systems: Theory, Algorithms,
and Applications, volume 11. Springer Science & Busi-
ness Media, 2010.

SME and Adaptive Stochastic Gradient Algorithms

Wibisono, Andre, Wilson, Ashia C., and Jordan, Michael I.
A variational perspective on accelerated methods in op-
timization. Proceedings of the National Academy of Sci-
ences, 113(47):E7351–E7358, 2016. doi: 10.1073/pnas.
1614734113.

Xiao, Lin and Zhang, Tong. A proximal stochastic gradi-
ent method with progressive variance reduction. SIAM
Journal on Optimization, 24(4):2057–2075, 2014.

Xu, Wei. Towards optimal one pass large scale learning
with averaged stochastic gradient descent. arXiv preprint
arXiv:1107.2490, 2011.

Zeiler, Matthew D. ADADELTA: an adaptive learning rate

method. arXiv preprint arXiv:1212.5701, 2012.

Zhang, Sixin, Choromanska, Anna E, and LeCun, Yann.
Deep learning with elastic averaging SGD. In Advances
in Neural Information Processing Systems, pp. 685–693,
2015.

Zygalakis, KC. On the existence and the applications of
modiﬁed equations for stochastic differential equations.
SIAM Journal on Scientiﬁc Computing, 33(1):102–130,
2011.

Polyak, Boris T. Some methods of speeding up the conver-
gence of iteration methods. USSR Computational Math-
ematics and Mathematical Physics, 4(5):1–17, 1964.

Polyak, Boris T and Juditsky, Anatoli B. Acceleration of
stochastic approximation by averaging. SIAM Journal
on Control and Optimization, 30(4):838–855, 1992.

Qian, Ning. On the momentum term in gradient descent
learning algorithms. Neural networks, 12(1):145–151,
1999.

Robbins, Herbert and Monro, Sutton. A stochastic approx-
imation method. The annals of mathematical statistics,
pp. 400–407, 1951.

Schaul, Tom and LeCun, Yann. Adaptive learning rates and
parallelization for stochastic, sparse, non-smooth gradi-
ents. arXiv preprint arXiv:1301.3764, 2013.

Schaul, Tom, Zhang, Sixin, and LeCun, Yann. No more
pesky learning rates. In ICML (3), volume 28, pp. 343–
351, 2013.

Shalev-Shwartz, Shai and Zhang, Tong. Accelerated proxi-
mal stochastic dual coordinate ascent for regularized loss
minimization. Mathematical Programming, pp. 1–41,
2014.

Shamir, Ohad and Zhang, Tong. Stochastic gradient de-
scent for non-smooth optimization: Convergence results
and optimal averaging schemes. In ICML (1), pp. 71–79,
2013.

Su, Weijie, Boyd, Stephen, and Candes, Emmanuel. A
differential equation for modeling Nesterovs accelerated
In Advances in
gradient method: theory and insights.
Neural Information Processing Systems, pp. 2510–2518,
2014.

Sutskever, Ilya, Martens, James, Dahl, George, and Hinton,
Geoffrey. On the importance of initialization and mo-
mentum in deep learning. In Proceedings of the 30th in-
ternational conference on machine learning (ICML-13),
pp. 1139–1147, 2013.

Tieleman, T. and Hinton, G. Lecture 6.5 - RMSProp. Tech-

nical report, 2012.

Uhlenbeck, George E and Ornstein, Leonard S. On the
theory of the Brownian motion. Physical review, 36(5):
823, 1930.

Warming, RF and Hyett, BJ. The modiﬁed equation ap-
proach to the stability and accuracy analysis of ﬁnite-
difference methods. Journal of computational physics,
14(2):159–179, 1974.

