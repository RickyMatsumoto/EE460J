Robust Probabilistic Modeling with Bayesian Data Reweighting

Yixin Wang 1 Alp Kucukelbir 1 David M. Blei 1

Abstract
Probabilistic models analyze data by relying on
a set of assumptions. Data that exhibit devia-
tions from these assumptions can undermine infer-
ence and prediction quality. Robust models offer
protection against mismatch between a model’s
assumptions and reality. We propose a way to
systematically detect and mitigate mismatch of a
large class of probabilistic models. The idea is
to raise the likelihood of each observation to a
weight and then to infer both the latent variables
and the weights from data. Inferring the weights
allows a model to identify observations that match
its assumptions and down-weight others. This en-
ables robust inference and improves predictive
accuracy. We study four different forms of mis-
match with reality, ranging from missing latent
groups to structure misspeciﬁcation. A Poisson
factorization analysis of the Movielens 1M dataset
shows the beneﬁts of this approach in a practical
scenario.

1. Introduction

Probabilistic modeling is a powerful approach to discov-
ering hidden patterns in data. We begin by expressing as-
sumptions about the class of patterns we expect to discover;
this is how we design a probability model. We follow by
inferring the posterior of the model; this is how we discover
the speciﬁc patterns manifest in an observed data set. Ad-
vances in automated inference (Hoffman & Gelman, 2014;
Mansinghka et al., 2014; Kucukelbir et al., 2017) enable
easy development of new models for machine learning and
artiﬁcial intelligence (Ghahramani, 2015).

In this paper, we present a recipe to robustify probabilistic
models. What do we mean by “robustify”? Departure from
a model’s assumptions can undermine its inference and
prediction performance. This can arise due to corrupted

y
t
i
s
n
e
D

observations, or in general, measurements that do not belong
to the process we are modeling. Robust models should
perform well in spite of such mismatch with reality.

Consider a movie recommendation system. We gather data
of people watching movies via the account they use to log in.
Imagine a situation where a few observations are corrupted
For example, a child logs in to her account and regularly
watches popular animated ﬁlms. One day, her parents use
the same account to watch a horror movie. Recommenda-
tion models, like Poisson factorization (PF), struggle with
this kind of corrupted data (see Section 4): it begins to
recommend horror movies.

What can be done to detect and mitigate this effect? One
strategy is to design new models that are less sensitive to
corrupted data, such as by replacing a Gaussian likelihood
with a heavier-tailed t distribution (Huber, 2011; Insua &
Ruggeri, 2012). Most probabilistic models we use have
more sophisticated structures; these template solutions for
speciﬁc distributions are not readily applicable. Other clas-
sical robust techniques act mostly on distances between
observations (Huber, 1973); these approaches struggle with
high-dimensional data. How can we still make use of our fa-
vorite probabilistic models while making them less sensitive
to the messy nature of reality?

Main idea. We propose reweighted probabilistic models
(RPM). The idea is simple. First, posit a probabilistic model.
Then adjust the contribution of each observation by raising
each likelihood term to its own (latent) weight. Finally, infer
these weights along with the latent variables of the original
probability model. The posterior of this adjusted model
identiﬁes observations that match its assumptions; it down-
weights observations that disagree with its assumptions.

Uncorrupted
Corrupted
Original Model
Reweighted Model

1Columbia University, New York City, USA. Correspondence

to: Yixin Wang <yixin.wang@columbia.edu>.

Proceedings of the 34th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

0

1

2

Figure 1. Fitting a unimodal distribution to a dataset with corrupted
measurements. The RPM downweights the corrupted observations.

Figure 1 depicts this tradeoff. The dataset includes cor-

Robust Probabilistic Modeling with Bayesian Data Reweighting

rupted measurements that undermine the original model;
Bayesian data reweighting automatically trades off the low
likelihood of the corrupted data near 1.5 to focus on the
uncorrupted data near zero. The RPM (green curve) detects
this mismatch and mitigates its effect compared to the poor
ﬁt of the original model (red curve).

Formally, consider a dataset of N independent observations
.y1; : : : ; yN /. The likelihood factorizes as a product
y
ˇ/, where ˇ is a set of latent variables. Posit a
1 `.yn

D
N
n

D

j
prior distribution pˇ .ˇ/.
Q
Bayesian data reweighting follows three steps:

1. Deﬁne a probabilistic model pˇ .ˇ/

N
n

D

1 `.yn

ˇ/.

j

2. Raise each likelihood to a positive latent weight wn.
Then choose a prior on the weights pw .w/, where w
D
.w1; : : : ; wN /. This gives a reweighted probabilistic
model (RPM)

Q

p.y; ˇ; w/

pˇ .ˇ/pw .w/

1
Z

D

N

1
n
Y
D

`.yn

ˇ/wn;

j

where Z is the normalizing factor.

3. Infer the posterior of both the latent variables ˇ and
y/.

the weights w, p.ˇ; w

j

The latent weights w allow an RPM to automatically explore
which observations match its assumptions and which do not.
Writing out the logarithm of the RPM gives some intuition;
it is equal (up to an additive constant) to

log pˇ .ˇ/

log pw .w/

wn log `.yn

ˇ/:

(1)

C

C

n
X

j

Posterior inference, loosely speaking, seeks to maximize
the above with respect to ˇ and w. The prior on the weights
pw .w/ plays a critical role:
it trades off extremely low
likelihood terms, caused by corrupted measurements, while
encouraging the weights to be close to one. We study three
options for this prior in Section 2.

How does Bayesian data reweighting induce robustness?
First, consider how the weights w affect Equation (1). The
logarithm of our priors are dominated by the log wn term:
this is the price of moving wn from one towards zero. By
shrinking wn, we gain an increase in wn log `.yn
ˇ/ while
paying a price in a log wn. The gain outweighs the price we
ˇ/ is very negative. Our priors are set to
pay if log `.yn
prefer wn to stay close to one; an RPM only shrinks wn for
very unlikely (e.g., corrupted) measurements.

j

j

Now consider how the latent variables ˇ affect Equation (1).
As the weights of unlikely measurements shrink, the like-
lihood term can afford to assign low mass to those cor-
rupted measurements and focus on the rest of the dataset.

Jointly, the weights and latent variables work together to
automatically identify unlikely measurements and focus on
observations that match the original model’s assumptions.

Section 2 presents these intuitions in full detail, along with
theoretical corroboration. In Section 3, we study four mod-
els under various forms of mismatch with reality, including
missing modeling assumptions, misspeciﬁed nonlinearities,
and skewed data. RPMs provide better parameter inference
and improved predictive accuracy across these models. Sec-
tion 4 presents a recommendation system example, where
we improve on predictive performance and identify atypical
ﬁlm enthusiasts in the Movielens 1M dataset.

Related work. Jerzy Neyman elegantly motivates the main
idea behind robust probabilistic modeling, a ﬁeld that has
attracted much research attention in the past century.

Every attempt to use mathematics to study
some real phenomena must begin with building a
mathematical model of these phenomena. Of ne-
cessity, the model simpliﬁes matters to a greater
or lesser extent and a number of details are ig-
nored.
[...] The solution of the mathematical
problem may be correct and yet it may be in vi-
olent conﬂict with realities simply because the
original assumptions of the mathematical model
diverge essentially from the conditions of the prac-
tical problem considered. (Neyman, 1949, p.22).

Our work draws on three themes around robust modeling.

The ﬁrst is a body of work on robust statistics and machine
learning (Provost & Fawcett, 2001; Song et al., 2002; Yu
et al., 2012; McWilliams et al., 2014; Feng et al., 2014;
Shaﬁeezadeh-Abadeh et al., 2015). These developments
focus on making speciﬁc models more robust to imprecise
measurements.

One strategy is popular: localization. To localize a proba-
bilistic model, allow each likelihood to depend on its own
“copy” of the latent variable ˇn. This transforms the model
into

N

p.y; ˇ; ˛/

p˛.˛/

`.yn

ˇn/pˇ .ˇn

˛/;

(2)

D

j

j

1
n
Y
D
where a top-level latent variable ˛ ties together all the ˇn
variables (de Finetti, 1961; Wang & Blei, 2015).1 Localiza-
tion decreases the effect of imprecise measurements. RPMs
present a broader approach to mitigating mismatch, with
improved performance over localization (Sections 3 and 4).

The second theme is robust Bayesian analysis, which stud-
ies sensitivity with respect to the prior (Berger et al., 1994).

1 Localization also relates to James-Stein shrinkage; Efron

(2010) connects these dots.

Robust Probabilistic Modeling with Bayesian Data Reweighting

Recent advances directly focus on sensitivity of the poste-
rior (Minsker et al., 2014; Miller & Dunson, 2015) or the
posterior predictive distribution (Kucukelbir & Blei, 2015).
We draw connections to these ideas throughout this paper.

The third theme is data reweighting. This involves design-
ing individual reweighting schemes for speciﬁc tasks and
models. Consider robust methods that toss away “outliers.”
This strategy involves manually assigning binary weights
to datapoints (Huber, 2011). Another example is covari-
ate shift adaptation/importance sampling where reweighting
transforms data to match another target distribution (Veach
& Guibas, 1995; Sugiyama et al., 2007; Shimodaira, 2000;
Wen et al., 2014). In contrast, RPMs treat weights as latent
variables. The weights are automatically inferred; no cus-
tom design is required. RPMs also connect to ideas around
ensemble learning and boosting (Schapire & Freund, 2012).
Boosting procedures reweight datapoints to build an ensem-
ble of predictors for supervised learning, whereas RPMs
apply to Bayesian models in general.

2. Reweighted Probabilistic Models

pˇ

ˇ

yn

N

(a) Original probabilistic model

pˇ

ˇ

yn

pw

wn

N

(b) Reweighted probabilistic model (RPM)

p˛

˛

ˇn

yn

N

(c) Localized probabilistic model

Reweighted probabilistic models (RPM) offer a new ap-
proach to robust modeling. The idea is to automatically
identify observations that match the assumptions of the
model and to base posterior inference on these observations.

Figure 2. RPMs begin with a probabilistic model (a) and introduce
a set of weights w as latent variables. This gives a model (b) that
explores which data observations match its assumptions. Localiza-
tion (c), instead, builds a hierarchical model. (Appendix A shows
when a localized model is also an RPM.)

2.1. Deﬁnitions

N
n

over

An RPM scaffolds
probabilistic model,
pˇ .ˇ/
Raise each likelihood to a
latent weight and posit a prior on the weights. This gives
the reweighted joint density

1 `.yn

ˇ/.

Q

a

D

j

p.y; ˇ; w/

pˇ .ˇ/pw .w/

`.yn

ˇ/wn;

(3)

1
Z

D

N

1
n
Y
D

j

N
n

D

1 `.yn

j

ˇ/wn dy dˇ dw is

where Z
D
the normalizing factor.
R

pˇ .ˇ/pw .w/

Q
The reweighted density integrates to one when the normaliz-
ing factor Z is ﬁnite. This is always true when the likelihood
ˇ/ is an exponential family distribution with Lesbegue
`.
base measure (Bernardo & Smith, 2009); this is the class of
models we study in this paper.2

 j

RPMs apply to likelihoods that factorize over the observa-
tions. (We discuss non-exchangeable models in Section 5.)
Figure 2 depicts an RPM as a graphical model. Speciﬁc
models may have additional structure, such as a separation
of local and global latent variables (Hoffman et al., 2013),
or ﬁxed parameters; we omit these in this ﬁgure.

2Heavy-tailed likelihoods and Bayesian nonparametric priors

may violate this condition; we leave these for future analysis.

2

The reweighted model introduces a set of weights; these
are latent variables, each with support wn
R>0. To gain
intuition, consider how these weights affect the posterior,
which is proportional to the product of the likelihood of ev-
ery measurement. A weight wn that is close to zero ﬂattens
ˇ/wn; a weight that
out its corresponding likelihood `.yn
is larger than one makes its likelihood more peaked. This, in
turn, enables the posterior to focus on some measurements
more than others. The prior pw .w/ ensures that not too
many likelihood terms get ﬂattened; in this sense, it plays
an important regularization role.

j

We study three options for this prior on weights: a bank
of Beta distributions, a scaled Dirichlet distribution, and a
bank of Gamma distributions.

Bank of Beta priors. This option constrains each weight as
.0; 1/. We posit an independent prior for each weight
wn

2

pw .w/

Beta.wn

a; b/

(4)

I

N

D

1
n
Y
D

and use the same parameters a and b for all weights. This
is the most conservative option for the RPM; it ensures that
none of the likelihoods ever becomes more peaked than it
was in the original model.

Robust Probabilistic Modeling with Bayesian Data Reweighting

The parameters a, b offer an expressive language to describe
different attitudes towards the weights. For example, setting
both parameters less than one makes the Beta act like a “two
spikes and a slab” prior, encouraging weights to be close to
zero or one, but not in between. As another example, setting
a greater than b encourages weights to lean towards one.

Scaled Dirichlet prior. This option ensures the sum of the
weights equals N . We posit a symmetric Dirichlet prior on
all the weights

w
pv.v/

D

D

N v
Dirichlet.a1/

(5)

b

where a is a scalar parameter and 1 is a .N
1/ vector of
ones. In the original model, where all the weights are one,
then the sum of the weights is N . The Dirichlet option main-
tains this balance; while certain likelihoods may become
more peaked, others will ﬂatten to compensate.

⇥

The concentration parameter a gives an intuitive way to
conﬁgure the Dirichlet. Small values for a allow the model
to easily up- or down-weight many data observations; larger
values for a prefer a smoother distribution of weights. The
Dirichlet option connects to the bootstrap approaches in
Rubin et al. (1981); Kucukelbir & Blei (2015), which also
preserves the sum of weights as N .

Bank of Gamma priors. Here we posit an independent
Gamma prior for each weight

pw .w/

Gamma.wn

a; b/

(6)

I

N

D

1
n
Y
D

and use the same parameters a and b for all weights. We
do not recommend this option, because observations can
be arbitrarily up- or down-weighted. In this paper, we only
consider Equation (6) for our theoretical analysis in Sec-
tion 2.2.

The bank of Beta and Dirichlet options perform similarly.
We prefer the Beta option as it is more conservative, yet
ﬁnd the Dirichlet to be less sensitive to its parameters. We
explore these options in the empirical study (Section 3).

2.2. Theory and intuition

How can theory justify Bayesian data reweighting? Here we
investigate its robustness properties. These analyses intend
to conﬁrm our intuition from Section 1. Appendices B and C
present proofs in full technical detail.

Intuition. Recall the logarithm of the RPM joint density
from Equation (1).Now compute the maximum-a-posterior
(MAP) estimate of the weights w. The partial derivative is

@ log p.y; ˇ; w/
@wn

d log pw .wn/
dwn

C

D

log `.yn

ˇ/

(7)

j

for all n
1; : : : ; N . Plug the Gamma prior from Equa-
tion (6) into the partial derivative in Equation (7) and set it
equal to zero. This gives the MAP estimate of wn,

D

wn

D

b

 

a

1
 
log `.yn

:

ˇ/

j

(8)

wn is an increasing function of the log
The MAP estimate
b
likelihood of yn when a > 1. This reveals that
wn shrinks
the contribution of observations that are unlikely under the
log likelihood; in turn, this encourages the MAP estimate
ˇ to describe the majority of the observations. This is
for
how an RPM makes a probabilistic model more robust.

b

b

A similar argument holds for other exponential family priors
on w with log wn as a sufﬁcient statistic. We formalize this
intuition and generalize it in the following theorem, which
establishes sufﬁcient conditions where a RPM improves the
inference of its latent variables ˇ.

Theorem 1 Denote the true value of ˇ as ˇ⇤. Let the pos-
terior mean of ˇ under the weighted and unweighted model
be Nˇw and Nˇu respectively. Assume mild conditions on
pw , ` and the corruption level, and that
 
`.yn
n with high probability. Then,
8
there exists an N ⇤ such that for N > N ⇤, we have
ˇ⇤j
j Nˇu
2 denotes second order
, where
stochastic dominance. (Details in Appendix B.)

ˇ⇤/
j
ˇ⇤j⌫

<✏ holds

j Nˇw /

j Nˇw

`.yn

⌫

 

 

2

j

j

The likelihood bounding assumption is common in robust
statistics theory; it is satisﬁed for both likely and unlikely
(corrupted) measurements. How much of an improvement
does it give? We can quantify this through the inﬂuence
function (IF) of Nˇw .
Consider a distribution G and a statistic T .G/ to be a func-
tion of data that comes iid from G. Take a ﬁxed distribution,
T; F / mea-
e.g., the population distribution, F . Then, IF.z
sures how much an additional observation at z affects the
statistic T .F /. Deﬁne

I

IF.z

T; F /

I

D

t

lim
0C

!

T .tız

.1

t/F /

T .F /

C

 
t

 

for z where this limit exists. Roughly, the IF measures the
asymptotic bias on T .F / caused by a speciﬁc observation
z that does not come from F . We consider a statistic T to
be robust if its IF is a bounded function of z, i.e., if outliers
can only exert a limited inﬂuence (Huber, 2011).

Here, we study the IF of the posterior mean T
D Nˇw under
ˇ⇤/. Say a
`.
the true data generating distribution F
value z has likelihood `.z
ˇ⇤/ that is nearly zero; we think
of this z as corrupted. Now consider the weight function
induced by the prior pw .w/. Rewrite it as a function of the
log likelihood, like w.log `.

ˇ⇤// as in Equation (8).

D

 j

j

Theorem 2 If lima
w.a/ <
0:

, then IF.z

1

 j
w.a/
I Nˇw ; `.
 j

! 1

0 and lima
0 as `.z

D
ˇ⇤//

!

a

 
!

! 1
ˇ⇤/
j

Robust Probabilistic Modeling with Bayesian Data Reweighting

This result shows that an RPM is robust in that its IF goes
to zero for unlikely measurements. This is true for all three
priors. (Details in Appendix C.)

2.3. Inference and computation

We now turn to inferring the posterior of an RPM,
p.ˇ; w
y/. The posterior lacks an analytic closed-form
expression for all but the simplest of models; even if the
original model admits such a posterior for ˇ, the reweighted
posterior may take a different form.

j

To approximate the posterior, we appeal to probabilistic pro-
gramming. A probabilistic programming system enables a
user to write a probability model as a computer program and
then compile that program into an inference executable. Au-
tomated inference is the backbone of such systems: it takes
in a probability model, expressed as a program, and outputs
an efﬁcient algorithm for inference. We use automated infer-
ence in Stan, a probabilistic programming system (Carpenter
et al., 2015).

In the empirical study that follows, we highlight how RPMs
detect and mitigate various forms of model mismatch. As
a common metric, we compare the predictive accuracy on
held out data for the original, localized, and reweighted
model.

j

D

y/

`.yé

The posterior predictive likelihood of a new datapoint
yé is poriginal.yé
y/ dˇ: Lo-
calization couples each observation with its own copy
R
of the latent variable;

this gives plocalized.yé

D
y/ d˛ dˇé where ˇé is the lo-
calized latent variable for the new datapoint. The prior
’
˛/ has the same form as pˇ in Equation (2).
p.ˇé

ˇé/p.ˇé

ˇ/p.ˇ

˛/p.˛

`.yé

y/

j

j

j

j

j

j

j

Bayesian data reweighting gives the following posterior
predictive likelihood

pRPM.yé

y/

j

D

p.yé

ˇ; wé/pRPM.ˇ

y/p.wé/ dwé dˇ;

j

j

“
where pRPM.ˇ
y/ is the marginal posterior, integrating out
the inferred weights of the training dataset, and the prior
p.wé/ has the same form as pw in Equation (3).

j

3. Empirical Study

We study RPMs under four types of mismatch with reality.
This section involves simulations of realistic scenarios; the
next section presents a recommendation system example
using real data. We default to No-U-Turn sampler (NUTS)
(Hoffman & Gelman, 2014) for inference in all experiments,
except for Sections 3.5 and 4 where we leverage variational
inference (Kucukelbir et al., 2017). The additional computa-
tional cost of inferring the weights is unnoticeable relative
to inference in the original model.

3.1. Outliers: a network wait-time example

ˇ/

A router receives packets over a network and measures
the time it waits for each packet. Suppose we typically
observe wait-times that follow a Poisson distribution with
5. We model each measurement using a Poisson
rate ˇ
D
Poisson.ˇ/ and posit a Gamma prior
likelihood `.yn
j
0:5/.
on the rate pˇ .ˇ/

D
Imagine that F % percent of the time, the network fails.
During these failures, the wait-times come from a Poisson
with much higher rate ˇ
50. Thus, the data actually
contains a mixture of two Poisson distributions; yet, our
model only assumes one. (Details in Appendix D.1.)

D
Gam.a

2; b

D

D

D

0

5

10

15

20

ˇ

(a) Posteriors for F

25% failure rate.

D

y
t
i
s
n
e
D

25

15

5

ˇ

truth
prior
original
RPM Beta
RPM Dirichlet
localized

original
RPM Beta
RPM Dirichlet
localized

0

0:1

0:2

0:3

0:4

F

(b) Posterior 95% credible intervals.

Figure 3. Outliers simulation study. We compare Beta.0:1; 0:01/
and Dir.1/ as priors for the reweighted probabilistic model. (a)
Posterior distributions on ˇ show a marked difference in detecting
the correct wait-time rate of ˇ
5. (b) Posterior 95% conﬁdence
D
intervals across failure rates F show consistent behavior for both
Beta and Dirichlet priors. (N

100 with 50 replications.)

D

How do we expect an RPM to behave in this situation?
Suppose the network failed 25% of the time. Figure 3a
shows the posterior distribution on the rate ˇ. The original
posterior is centered at 18; this is troubling, not only because
the rate is wrong but also because of how conﬁdent the
posterior ﬁt is. Localization introduces greater uncertainty,
yet still estimates a rate around 15. The RPM correctly
identiﬁes that the majority of the observations come from
ˇ
5. Observations from when the network failed are
down-weighted. It gives a conﬁdent posterior centered at
ﬁve.

D

Figure 3b shows posterior 95% credible intervals of ˇ under
failure rates up to F
45%. The RPM is robust to cor-
rupted measurements; instead it focuses on data that it can

D

Robust Probabilistic Modeling with Bayesian Data Reweighting

explain within its assumptions. When there is no corruption,
the RPM performs just as well as the original model.

Visualizing the weights elucidates this point. Figure 4 shows
the posterior mean estimates of w for F
25%. The
weights are sorted into two groups, for ease of viewing.
The weights of the corrupted observations are essentially
zero; this downweighting is what allows the RPM to shift
its posterior on ˇ towards ﬁve.

D

ˇ

1

0:5

 

⇠

Unif.

xn
10; 10/. Consider a Bayesian logistic regres-
sion model without intercept. Posit a prior on the slope as
pˇ .ˇ/
N .0; 10/ and assume a Beta.0:1; 0:01/ prior on
the weights. (Details in Appendix D.2.)

D

ˇmen
original
RPM
localized

F

s
t
h
g
i
e

W

2

1

0

0

75

100

Data Index

Figure 4. Posterior means of the weights w under the Dirichlet
prior. For visualization purposes, we sorted the data into two
groups: the ﬁrst 75 contain observations from the normal network;
the remaining 25 are the observations when the network fails.

Despite this downweighting, the RPM posteriors on ˇ are
not overdispersed, as in the localized case. This is due to the
interplay we described in the introduction. Downweighting
observations should lead to a smaller effective sample size,
which would increase posterior uncertainty. But the down-
weighted datapoints are corrupted observations; including
them also increases posterior uncertainty.

The RPM is insensitive to the prior on the weights; both
Beta and Dirichlet options perform similarly. From here on,
we focus on the Beta option. We let the shape parameter
103; this
a scale with the data size N such that N=a
encodes a mild attitude towards unit weights. We now move
on to other forms of mismatch with reality.

⇡

3.2. Missing latent groups: predicting color blindness

Color blindness is unevenly hereditary: it is much higher for
men than for women (Boron & Boulpaep, 2012). Suppose
we are not aware of this fact. We have a dataset of both gen-
ders with each individual’s color blindness status and his/her
relevant family history. No gender information is available.
Consider analyzing this data using logistic regression. It can
only capture one hereditary group. Thus, logistic regression
misrepresents both groups, even though men exhibit strong
heredity. In contrast, an RPM can detect and mitigate the
missing group effect by focusing on the dominant hereditary
trait. Here we consider men as the dominant group.

We simulate this scenario by drawing binary indicators of
pn// where
Bernoulli.1=1
color blindness yn
the pn’s come from two latent groups: men exhibit a
stronger dependency on family history (pn
0:5xn) than
0:01xn). We simulate family history as
women (pn

exp.

D

C

⇠

 

D

0

0.1

0.2

0.3

0.4

Figure 5. Missing latent groups study. Posterior 95% credible
0:5,
intervals for the RPM always include the dominant ˇmen D
as we vary the percentage of females in the data. Dataset size
N

100 with 50 replications.

D

Figure 5 shows the posterior 95% credible intervals of ˇ
as we vary the percentage of females from F
0% to
40%. A horizontal line indicates the correct slope for the
0:5. As the size of the missing
dominant group, ˇmen D
latent group (women) increases, the original model quickly
shifts its credible interval away from 0:5. The reweighted
0:5 for all
and localized posteriors both contain ˇmen D
percentages, but the localized model exhibits much higher
variance in its estimates.

D

This analysis shows how RPMs can mitigate the effect of
missing latent groups. While the original logistic regression
model would perform equally poorly on both groups, an
RPM is able to automatically focus on the dominant group.

Corrupted (F
Clean (F

D
0%)

25%)

D

y
t
i
s
n
e
D

0

1

Ep.w

y/Œwç
j

Figure 6. Kernel density estimate of the distribution of weights
across all measurements in the missing latent groups study. The
percentage of females is denoted by F . A hypothetical clean
dataset receives weights that concentrate around one; the actual
corrupted dataset exhibits a two-hump distribution of weights.

An RPM also functions as a diagnostic tool to detect mis-
match with reality. The distribution of the inferred weights
indicates the presence of datapoints that defy the assump-
tions of the original model. Figure 6 shows a kernel density
estimate of the inferred posterior weights. A hypothetical
dataset with no corrupted measurements receives weights
close to one. In contrast, the actual dataset with measure-
ments from a missing latent group exhibit a bimodal dis-
tribution of weights. Testing for bimodality of the inferred
weights is one way in which an RPM can be used to diagnose
mismatch with reality.

Robust Probabilistic Modeling with Bayesian Data Reweighting

True structure

Model structure

ˇ0
ˇ0
ˇ0

C
C
C

ˇ1x1
ˇ1x1
ˇ1x1

C
C
C

ˇ2x2
ˇ2x2
ˇ2x2

C
C

ˇ3x1x2
ˇ3x2
2

ˇ0
ˇ0
ˇ0

C
C
C

ˇ1x1
ˇ1x1
ˇ1x1

C
C

ˇ2x2
ˇ2x2

Original
mean(std)

RPM
mean(std)

Localization
mean(std)

3.16(1.37)
30.79(2.60)
0.58(0.38)

2.20(1.25)
16.32(1.96)
0.60(0.40)

2.63(1.85)
21.08(5.20)
0.98(0.54)

Table 1. RPMs improve absolute deviations of posterior mean ˇ1 estimates. (50 replications.)

Murphy, 2012). While real data may indeed come from
a ﬁnite mixture of clusters, there is no reason to assume
each cluster is distributed as a Gaussian. Inspired by the
experiments in Miller & Dunson (2015), we show how a
reweighted DPMM reliably recovers the correct number of
components in a mixture of skewnormals dataset.

3.3. Covariate dependence misspeciﬁcation: a lung

cancer risk study

Consider a study of lung cancer risk. While tobacco usage
exhibits a clear connection, other factors may also contribute.
For instance, obesity and tobacco usage appear to interact,
with evidence towards a quadratic dependence on obesity
(Odegaard et al., 2010).

Denote tobacco usage as x1 and obesity as x2. We study
three models of lung cancer risk dependency on these co-
variates. We are primarily interested in understanding the
effect of tobacco usage; thus we focus on ˇ1, the regression
coefﬁcient for tobacco. In each model, some form of covari-
ance misspeciﬁcation discriminates the true structure from
the assumed structure.

100 with
For each model, we simulate a dataset of size N
D
N .0; 102/
random covariates x1
and regression coefﬁcients ˇ0;1;2;3
10; 10/.
Consider a Bayesian linear regression model with prior
N .0; 10/. (Details in Appendix D.3.)
pˇ .ˇ/

N .10; 52/ and x2

⇠
Unif.

⇠

⇠

 

D

Table 1 summarizes the misspeciﬁcation and shows absolute
differences on the estimated ˇ1 regression coefﬁcient. The
RPM yields better estimates of ˇ1 in the ﬁrst two models.
These highlight how the RPM leverages datapoints useful for
estimating ˇ1. The third model is particularly challenging
because obesity is ignored in the misspeciﬁed model. Here,
the RPM gives similar results to the original model; this
highlights that RPMs can only use available information.
Since the original model lacks dependence on x2, the RPM
cannot compensate for this.

3.4. Predictive likelihood results

Table 2 shows how RPMs also improve predictive accuracy.
In all the above examples, we simulate test data with and
without their respective types of corruption. RPMs improve
prediction for both clean and corrupted data, as they focus
on data that match the assumptions of the original model.

3.5. Skewed data: cluster selection in a mixture model

Finally, we show how RPMs handle skewed data. The
Dirichlet process mixture model (DPMM) is a versatile
model for density estimation and clustering (Bishop, 2006;

(a) Original model

(b) RPM

D

Figure 7. A ﬁnite approximation DPMM to skewnormal distributed
data that come from three groups. The shade of each cluster
indicates the inferred mixture proportions (N

2000).

A standard Gaussian mixture model (GMM) with large K
and a sparse Dirichlet prior on the mixture proportions is
an approximation to a DPMM (Ishwaran & James, 2012).
We simulate three clusters from two-dimensional skewnor-
mal distributions and ﬁt a GMM with maximum K
30.
Here we use automatic differentiation variational inference
(ADVI), as NUTS struggles with inference of mixture mod-
els (Kucukelbir et al., 2017). (Details in Appendix D.4.)

D

Figure 7 shows posterior mean estimates from the original
GMM; it incorrectly ﬁnds six clusters. In contrast, the RPM
identiﬁes the correct three clusters. Datapoints in the tails
of each cluster get down-weighted; these are datapoints that
do not match the Gaussianity assumption of the model.

4. Case Study: Poisson factorization for

recommendation

We now turn to a study of real data: a recommendation
system. Consider a video streaming service; data comes as a
binary matrix of users and the movies they choose to watch.
How can we identify patterns from such data? Poisson
factorization (PF) offers a ﬂexible solution (Cemgil, 2009;
Gopalan et al., 2015). The idea is to infer a K-dimensional

Robust Probabilistic Modeling with Bayesian Data Reweighting

Outliers

Clean

Corrupted

Missing latent groups Misspeciﬁed structure
Corrupted

Corrupted

Clean

Clean

Original model
Localized model
RPM

−744.2
−730.8
−328.5

−1244.5
−1258.4
−1146.9

−108.6
−53.6
−43.9

−103.9
−112.7
−90.5

−136.3
−192.5
−124.1

−161.7
−193.1
−144.1

Table 2. Posterior predictive likelihoods of clean and corrupted test data. Outliers and missing latent groups have F
D
misspeciﬁed structure is missing the interaction term. Results are similar for other levels and types of mismatch with reality.

25%. The

Average log likelihood

Corrupted users
1%

2%

0%

Original model
RPM

1:68
1:53

 
 

1:73
1:53

 
 

1:74
1:52

 
 

Table 3. Held-out predictive accuracy under varying amounts of
corruption. Held-out users chosen randomly (20% of total users).

latent space of user preferences ✓ and movie attributes ˇ.
The inner product ✓ >ˇ determines the rate of a Poisson
likelihood for each binary measurement; Gamma priors
on ✓ and ˇ promote sparse patterns. As a result, PF ﬁnds
interpretable groupings of movies, often clustered according
to popularity or genre. (Full model in Appendix E.)

How does classical PF compare to its reweighted counter-
part? As input, we use the MovieLens 1M dataset, which
contains one million movie ratings from 6 000 users on
4 000 movies. We place iid Gamma.1; 0:001/ priors on the
preferences and attributes. Here, we have the option of
reweighting users or items. We focus on users and place a
Beta.100; 1/ prior on their weights. For this model, we use
MAP estimation. (Localization is computationally challeng-
ing for PF; it requires a separate “copy” of ✓ for each movie,
along with a separate ˇ for each user. This dramatically
increases computational cost.)

1

0:5

s
t
h
g
i
e

W

1

0:5

clean

0.1 0.5

1

Ratio of corruption (R)

(a) Original dataset

(b) Corrupted users

Figure 8. Inferred weights for clean and corrupted data. (a) Most
users receive weights very close to one. (b) Corrupted users receive
weights much smaller than one. Larger ratios of corruption R
imply lower weights.

We begin by analyzing the original
(clean) dataset.
Reweighting improves the average held-out log likelihood
1:53 of the corre-
from

1:68 of the original model to

 

 

sponding RPM. The boxplot in Figure 8a shows the inferred
weights. The majority of users receive weight one, but a few
users are down-weighted. These are ﬁlm enthusiasts who
appear to indiscriminately watch many movies from many
genres. (Appendix F shows an example.) These users do
not contribute towards identifying movies that go together;
this explains why the RPM down-weights them.

Recall the example from our introduction. A child typically
watches popular animated ﬁlms, but her parents occasionally
use her account to watch horror ﬁlms. We simulate this by
corrupting a small percentage of users. We replace a ratio
.0:1; 0:5; 1/ of these users’ movies with randomly
R
selected movies.

D

The boxplot in Figure 8b shows the weights we infer for
these corrupted users, based on how many of their movies
we randomly replace. The weights decrease as we corrupt
more movies. Table 3 shows how this leads to higher held-
out predictive accuracy; down-weighting these corrupted
users leads to better prediction.

5. Discussion

Reweighted probabilistic models (RPM) offer a systematic
approach to mitigating various forms of mismatch with
reality. The idea is to raise each data likelihood to a weight
and to infer the weights along with the hidden patterns.
We demonstrate how this strategy introduces robustness and
improves prediction accuracy across four types of mismatch.

RPMs also offer a way to detect mismatch with reality. The
distribution of the inferred weights sheds light onto data-
points that fail to match the original model’s assumptions.
RPMs can thus lead to new model development and deeper
insights about our data.

RPMs can also work with non-exchangeable data, such
as time series. Some time series models admit exchange-
able likelihood approximations (Guinness & Stein, 2013).
For other models, a non-overlapping windowing approach
would also work. The idea of reweighting could also extend
to structured likelihoods, such as Hawkes process models.

Robust Probabilistic Modeling with Bayesian Data Reweighting

Acknowledgements

We thank Adji Dieng, Yuanjun Gao, Inchi Hu, Christian
Naesseth, Rajesh Ranganath, Francisco Ruiz, and Dustin
Tran for their insightful comments. This work is supported
by NSF IIS-1247664, ONR N00014-11-1-0651, DARPA
PPAML FA8750-14-2-0009, DARPA SIMPLEX N66001-
15-C-4032, and the Alfred P. Sloan Foundation.

References

Berger, James O, Moreno, Elías, Pericchi, Luis Raul, Ba-
yarri, M Jesús, Bernardo, José M, Cano, Juan A, De la
Horra, Julián, Martín, Jacinto, Ríos-Insúa, David, Betrò,
Bruno, et al. An overview of robust Bayesian analysis.
Test, 3(1):5–124, 1994.

Bernardo, José M and Smith, Adrian FM. Bayesian Theory.

John Wiley & Sons, 2009.

Bishop, Christopher M. Pattern Recognition and Machine

Learning. Springer New York, 2006.

Boron, Walter F and Boulpaep, Emile L. Medical Physiol-

ogy. Elsevier, 2012.

Carpenter, Bob, Gelman, Andrew, Hoffman, Matt, Lee,
Daniel, Goodrich, Ben, Betancourt, Michael, Brubaker,
Marcus A, Guo, Jiqiang, Li, Peter, and Riddell, Allen.
Stan: a probabilistic programming language. Journal of
Statistical Software, 2015.

Cemgil, Ali Taylan. Bayesian inference for nonnegative
matrix factorisation models. Computational Intelligence
and Neuroscience, 2009.

de Finetti, Bruno. The Bayesian approach to the rejection
of outliers. In Proceedings of the Fourth Berkeley Sympo-
sium on Probability and Statistics, 1961.

Efron, Bradley. Large-Scale Inference. Cambridge Univer-

sity Press, 2010.

Feng, Jiashi, Xu, Huan, Mannor, Shie, and Yan, Shuicheng.
Robust logistic regression and classiﬁcation. In NIPS.
2014.

Ghahramani, Zoubin. Probabilistic machine learning and
artiﬁcial intelligence. Nature, 521(7553):452–459, 2015.

Gopalan, Prem, Hofman, Jake M, and Blei, David M. Scal-
able recommendation with hierarchical Poisson factoriza-
tion. UAI, 2015.

Guinness, Joseph and Stein, Michael L. Transformation to
approximate independence for locally stationary Gaus-
sian processes. Journal of Time Series Analysis, 34(5):
574–590, 2013.

Hoffman, Matthew D and Gelman, Andrew. The No-U-Turn
sampler. Journal of Machine Learning Research, 15(1):
1593–1623, 2014.

Hoffman, Matthew D, Blei, David M, Wang, Chong, and
Paisley, John. Stochastic variational inference. The Jour-
nal of Machine Learning Research, 14(1):1303–1347,
2013.

Huber, Peter J. Robust regression: asymptotics, conjectures
and Monte Carlo. The Annals of Statistics, pp. 799–821,
1973.

Huber, Peter J. Robust Statistics. Springer, 2011.

Insua, David Ríos and Ruggeri, Fabrizio. Robust Bayesian
Analysis. Springer Science & Business Media, 2012.

Ishwaran, Hemant and James, Lancelot F. Approximate
Dirichlet process computing in ﬁnite normal mixtures.
Journal of Computational and Graphical Statistics, 2012.

Kucukelbir, Alp and Blei, David M. Population empirical

Bayes. In UAI, 2015.

Kucukelbir, Alp, Tran, Dustin, Ranganath, Rajesh, Gelman,
Andrew, and Blei, David M. Automatic differentiation
variational inference. Journal of Machine Learning Re-
search, 18(14):1–45, 2017.

Mansinghka, Vikash, Selsam, Daniel, and Perov, Yura. Ven-
ture: a higher-order probabilistic programming platform
with programmable inference. arXiv:1404.0099, 2014.

McWilliams, Brian, Krummenacher, Gabriel, Lucic, Mario,
and Buhmann, Joachim M. Fast and robust least squares
estimation in corrupted linear models. In NIPS. 2014.

Miller, Jeffrey W and Dunson, David B. Robust Bayesian in-
ference via coarsening. arXiv preprint arXiv:1506.06101,
2015.

Minsker, Stanislav, Srivastava, Sanvesh, Lin, Lizhen, and
Dunson, David B. Robust and scalable Bayes via a
median of subset posterior measures. arXiv preprint
arXiv:1403.2660, 2014.

Murphy, Kevin P. Machine Learning: a Probabilistic Per-

spective. MIT Press, 2012.

Neyman, Jerzy. On the problem of estimating the number of
schools of ﬁsh. In J. Neyman, M. Loeve and Yerushalmy,
J. (eds.), University of California Publications in Statis-
tics, volume 1, chapter 3, pp. 21–36. University of Cali-
fornia Press, 1949.

Odegaard, Andrew O, Pereira, Mark A, Koh, Woon-Puay,
Gross, Myron D, Duval, Sue, Mimi, C Yu, and Yuan,
Jian-Min. BMI, all-cause and cause-speciﬁc mortality in

Robust Probabilistic Modeling with Bayesian Data Reweighting

Chinese Singaporean men and women. PLoS One, 5(11),
2010.

Provost, Foster and Fawcett, Tom. Robust classiﬁcation
for imprecise environments. Machine Learning, 42(3):
203–231, 2001.

Rubin, Donald B et al. The Bayesian bootstrap. The annals

of statistics, 9(1):130–134, 1981.

Schapire, R.E. and Freund, Y. Boosting: Foundations and

Algorithms. MIT Press, 2012.

Shaﬁeezadeh-Abadeh, Soroosh, Esfahani, Peyman Moha-
jerin, and Kuhn, Daniel. Distributionally robust logistic
regression. In NIPS. 2015.

Shimodaira, Hidetoshi. Improving predictive inference un-
der covariate shift by weighting the log-likelihood func-
tion. Journal of Statistical Planning and Inference, 90(2):
227–244, 2000.

Song, Qing, Hu, Wenjie, and Xie, Wenfang. Robust sup-
port vector machine with bullet hole image classiﬁcation.
Systems, Man, and Cybernetics, Part C: Applications and
Reviews, IEEE Transactions on, 32(4):440–448, 2002.

Sugiyama, Masashi, Krauledat, Matthias, and Müller,
Klaus-Robert. Covariate shift adaptation by importance
weighted cross validation. Journal of Machine Learning
Research, 8:985–1005, 2007.

Veach, Eric and Guibas, Leonidas J. Optimally combin-
ing sampling techniques for Monte Carlo rendering. In
Proceedings of the 22nd annual conference on Computer
graphics and interactive techniques, pp. 419–428. ACM,
1995.

Wang, Chong and Blei, David M. A general method
arXiv preprint

robust Bayesian modeling.

for
arXiv:1510.05078, 2015.

Wen, Junfeng, Yu, Chun-nam, and Greiner, Russell. Ro-
bust learning under uncertain test distributions: Relating
covariate shift to model misspeciﬁcation. In ICML, 2014.

Yu, Yaoliang, Aslan, Özlem, and Schuurmans, Dale. A
In NIPS.

polynomial-time form of robust regression.
2012.

