An Efﬁcient, Sparsity-Preserving, Online Algorithm for Low-Rank
Approximation

David Anderson * 1 Ming Gu * 1

Abstract

Low-rank matrix approximation is a fundamental
tool in data analysis for processing large datasets,
reducing noise, and ﬁnding important signals.
In this work, we present a novel truncated LU
factorization called Spectrum-Revealing LU
(SRLU) for effective low-rank matrix approxi-
mation, and develop a fast algorithm to compute
an SRLU factorization. We provide both matrix
and singular value approximation error bounds
for the SRLU approximation computed by our
algorithm. Our analysis suggests that SRLU is
competitive with the best low-rank matrix ap-
proximation methods, deterministic or random-
ized, in both computational complexity and ap-
proximation quality. Numeric experiments illus-
trate that SRLU preserves sparsity, highlights im-
portant data features and variables, can be efﬁ-
ciently updated, and calculates data approxima-
tions nearly as accurately as possible. To the best
of our knowledge this is the ﬁrst practical variant
of the LU factorization for effective and efﬁcient
low-rank matrix approximation.

1. Introduction

Low-rank approximation is an essential data processing
technique for understanding large or noisy data in diverse
areas including data compression, image and pattern recog-
nition, signal processing, compressed sensing, latent se-
mantic indexing, anomaly detection, and recommendation
systems. Recent machine learning applications include
training neural networks (Jaderberg et al., 2014; Kirk-
patrick et al., 2017), second order online learning (Luo
et al., 2016), representation learning (Wang et al., 2016),
and reinforcement learning (Ghavamzadeh et al., 2010).

*Equal contribution

1University of California, Berke-
Correspondence to: David Anderson <davidander-

ley.
son@berkeley.edu>, Ming Gu <mgu@berkeley.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Additionally, a recent trend in machine learning is to in-
clude an approximation of second order information for
better accuracy and faster convergence (Krummenacher
et al., 2016).

In this work, we introduce a novel low-rank approximation
algorithm called Spectrum-Revealing LU (SRLU) that can
be efﬁciently computed and updated. Furthermore, SRLU
preserves sparsity and can identify important data variables
and observations. Our algorithm works on any data matrix,
and achieves an approximation accuracy that only differs
from the accuracy of the best approximation possible for
any given rank by a constant factor.1

The major innovation in SRLU is the efﬁcient calculation
of a truncated LU factorization of the form

Π1AΠT

2 =

m − k

In−k

(cid:19) (cid:18)

k

n − k
U11 U12
S

(cid:19)

(cid:18)

k
L11
k
m − k L21
(cid:19)
(cid:18)L11
L21
def= (cid:98)L (cid:98)U,

≈

(cid:0)U11 U12

(cid:1)

where Π1 and Π2 are judiciously chosen permutation ma-
trices. The LU factorization is unstable, and in practice
is implemented by pivoting (interchanging) rows during
factorization, i.e. choosing permutation matrix Π1. For
the truncated LU factorization to have any signiﬁcance,
nevertheless, complete pivoting (interchanging rows and
columns) is necessary to guarantee that the factors (cid:98)L and
(cid:98)U are well-deﬁned and that their product accurately repre-
sents the original data. Previously, complete pivoting was
impractical as a matrix factorization technique because it
requires accessing the entire data matrix at every iteration,
but SRLU efﬁciently achieves complete pivoting through
randomization and includes a deterministic follow-up pro-
cedure to ensure a hight quality low-rank matrix approxi-
mation, as supported by rigorous theory and numeric ex-
periments.

1The truncated SVD is known to provide the best low-rank
matrix approximation, but it is rarely used for large scale practical
data analysis. See a brief discussion of the SVD in supplemental
material.

An Efﬁcient Algorithm for Low-Rank Approximation

1.1. Background on the LU factorization

Algorithm 1 presents a basic implementation of the LU fac-
torization, where the result is stored in place such that the
upper triangular part of A becomes U and the strictly lower
triangular part becomes the strictly lower part of L, with
the diagonal of L implicitly known to contain all ones. LU
with partial pivoting ﬁnds the largest entry in the ith column
from row i to m and pivots the row with that entry to the ith
row. LU with complete pivoting ﬁnds the largest entry in
the submatrix Ai+1:m,i+1:n and pivots that entry to Ai,i. It
is generally known and accepted that partial pivoting is suf-
ﬁcient for general, real-world data matrices in the context
of linear equation solving.

Algorithm 1 The LU factorization
1: Inputs: Data matrix A ∈ Rm×n
2: for i = 1, 2, · · · , min(m, n) do
3:
4:
5:
6:
7: Ai+1:m,i+1:n −= Ai+1:m,1:i · A1:i,i+1:n
8: end for

Perform row and/or column pivots
for k = i + 1, · · · , m do
Ak,i = Ak,i/Ai,i

end for

Perform column pivots

Algorithm 2 Crout LU
1: Inputs: Data matrix A ∈ Rm×n, block size b
2: for j = 0, b, 2b, · · · , min(m, n)/b − 1 do
3:
4: Aj+1:m,j+1:j+b− =
5:
6:
7:
8: Aj+1:j+b,j+b+1:n −=
9:
10: end for

Apply Algorithm 1 on Aj+1:m,j+1:j+b
Apply the row pivots to other columns of A

Aj+1:j+b,1:j · A1:j,j+b+1:n

Aj+1:m,1:j · A1:j,j+1:j+b.

Line 7 of Algorithm 1 is known as the Schur update. Given
a sparse input, this is the only step of the LU factorization
that causes ﬁll. As the algorithm progresses, ﬁll will com-
pound and may become dense, but the LU factorization,
and truncated LU in particular, generally preserves some,
if not most, of the sparsity of a sparse input. A numeric
illustration is presented below.

There are many variations of the LU factorization. In Al-
gorithm 2 the Crout version of LU is presented in block
form. The column pivoting entails selecting the next b
columns so that the in-place LU step is performed on a
non-singular matrix (provided the remaining entries are not
all zero). Note that the matrix multiplication steps are the
bottleneck of this algorithm, requiring O(mnb) operations
each in general.

The LU factorization has been studied extensively since
long before the invention of computers, with notable re-
sults from many mathematicians, including Gauss, Turing,
and Wilkinson. Current research on LU factorizations in-
cludes communication-avoiding implementations, such as
tournament pivoting (Khabou et al., 2013), sparse imple-
mentations (Grigori et al., 2007), and new computation
of preconditioners (Chow & Patel, 2015). A randomized
approach to efﬁciently compute the LU factorization with
complete pivoting recently appeared in (Melgaard & Gu,
2015). These results are all in the context of linear equation
solving, either directly or indirectly through an incomplete
factorization used to precondition an iterative method. This
work repurposes the LU factorization to create a novel efﬁ-
cient and effective low-rank approximation algorithm using
modern randomization technology.

2. Previous Work

2.1. Low-Rank Matrix Approximation (LRMA)

Previous work on low-rank data approximation includes
the Interpolative Decomposition (ID) (Cheng et al., 2005),
the truncated QR with column pivoting factorization (Gu &
Eisenstat, 1996), and other deterministic column selection
algorithms, such as in (Batson et al., 2012).

Randomized algorithms have grown in popularity in recent
years because of their ability to efﬁciently process large
data matrices and because they can be supported with rig-
orous theory. Randomized low-rank approximation algo-
rithms generally fall into one of two categories: sampling
algorithms and black box algorithms. Sampling algorithms
form data approximations from a random selection of rows
and/or columns of the data. Examples include (Desh-
pande et al., 2006; Deshpande & Vempala, 2006; Frieze
et al., 2004; Mahoney & Drineas, 2009). (Drineas et al.,
2008) showed that for a given approximate rank k, a ran-
domly drawn subset C of c = O (cid:0)k log(k)(cid:15)−2 log (1/δ)(cid:1)
columns of the data, a randomly drawn subset R of
r = O (cid:0)c log(c)(cid:15)−2 log (1/δ)(cid:1) rows of the data, and set-
ting U = C†AR†, then the matrix approximation error
(cid:107)A − CUR(cid:107)F is at most a factor of 1 + (cid:15) from the optimal
rank k approximation with probability at least 1 − δ. Black
box algorithms typically approximate a data matrix in the
form

A ≈ QT QA,

where Q is an orthonormal basis of the random projection
(usually using SVD, QR, or ID). The result of (Johnson &
Lindenstrauss, 1984) provided the theoretical groundwork
for these algorithms, which have been extensively studied
(Clarkson & Woodruff, 2012; Halko et al., 2011; Martins-
son et al., 2006; Papadimitriou et al., 2000; Sarlos, 2006;
Woolfe et al., 2008; Liberty et al., 2007; Gu, 2015). Note

An Efﬁcient Algorithm for Low-Rank Approximation

that the projection of an m-by-n data matrix is of size m-
by-(cid:96), for some oversampling parameter (cid:96) ≥ k, and k is
the target rank. Thus the computational challenge is the
orthogonalization of the projection (the random projection
can be applied quickly, as described in these works). A pre-
vious result on randomized LU factorizations for low-rank
approximation was presented in (Aizenbud et al., 2016),
but is uncompetitive in terms of theoretical results and com-
putational performance with the work presented here.

For both sampling and black box algorithms the tuning pa-
rameter (cid:15) cannot be arbitrarily small, as the methods be-
come meaningless if the number of rows and columns sam-
pled (in the case of sampling algorithms) or the size of the
random projection (in the case of black box algorithms)
surpasses the size of the data. A common practice is (cid:15) ≈ 1
2 .

2.2. Guaranteeing Quality

Rank-revealing algorithms (Chan, 1987) are LRMA algo-
rithms that guarantee the approximation is of high quality
by also capturing the rank of the data within a tolerance
(see supplementary materials for deﬁnitions). These meth-
ods, nevertheless, attempt to build an important submatrix
of the data, and do not directly compute a low-rank ap-
proximation. Furthermore, they do not attempt to capture
all positive singular values of the data. (Miranian & Gu,
2003) introduced a new type of high-quality LRMA algo-
rithms that can capture all singular values of a data matrix
within a tolerance, but requires extra computation to bound
approximations of the left and right null spaces of the data
matrix. Rank-revealing algorithms in general are designed
around a deﬁnition that is not speciﬁcally appropriate for
LRMA.

A key advancement of this work is a new deﬁnition of high
quality low-rank approximation:

Deﬁnition 1 A rank-k truncated LU factorization is
spectrum-revealing if
(cid:13)
(cid:13)
(cid:13)2

≤ q1(k, m, n)σk+1 (A)

(cid:13)
(cid:13)
(cid:13)A − (cid:98)L (cid:98)U

and

(cid:16)

(cid:17)

σj

(cid:98)L (cid:98)U

≥

σj (A)
q2(k, m, n)

for 1 ≤ j ≤ k and q1(k, m, n) and q2(k, m, n) are
bounded by a low degree polynomial in k, m, and n.

Deﬁnition 1 has precisely what we desire in an LRMA,
and no additional requirements. The constants, q1(k, m, n)
and q2(k, m, n) are at least 1 for any rank-k approximation
by (Eckart & Young, 1936). This work shows theoretically
and numerically that our algorithm, SRLU, is spectrum-
revealing in that it always ﬁnds such q1 and q2, often with
q1, q2 = O(1) in practice.

Algorithm 3 TRLUCP
1: Inputs: Data matrix A ∈ Rm×n, target rank k, block
size b, oversampling parameter p ≥ b, random Gaus-
sian matrix Ω ∈ Rp×m, (cid:98)L and (cid:98)U are initially 0 matri-
ces

2: Calculate random projection R = ΩA
3: for j = 0, b, 2b, · · · , k − b do
4:

Perform column selection algorithm on R and swap
columns of A
Update block column of (cid:98)L
Perform block LU with partial row pivoting and
swap rows of A
Update block row of (cid:98)U
Update R

5:
6:

7:
8:
9: end for

2.3. Low-Rank and Other Approximations in Machine

Learning

Low-rank and other approximation algorithms have ap-
peared recently in a variety of machine learning applica-
tions. In (Krummenacher et al., 2016), randomized low-
rank approximation is applied directly to the adaptive opti-
mization algorithm ADAGRAD to incorporate variable de-
pendence during optimization to approximate the full ma-
trix version of ADAGRAD with a signiﬁcantly reduced
computational complexity. In (Kirkpatrick et al., 2017), a
diagonal approximation of the posterior distribution of pre-
vious data is utilized to alleviate catastrophic forgetting.

3. Main Contribution: Spectrum-Revealing

LU (SRLU)

Our algorithm for computing SRLU is composed of two
subroutines: partially factoring the data matrix with ran-
domized complete pivoting (TRLUCP) and performing
swaps to improve the quality of the approximation (SRP).
The ﬁrst provides an efﬁcient algorithm for computing a
truncated LU factorization, whereas the second ensures the
resulting approximation is provably reliable.

3.1. Truncated Randomized LU with Complete

Pivoting (TRLUCP)

Intuitively, TRLUCP performs deterministic LU with par-
tial row pivoting for some initial data with permuted
columns. TRLUCP uses a random projection of the Schur
complement to cheaply ﬁnd and move forward columns
that are more likely to be representative of the data. To ac-
complish this, Algorithm 3 performs an iteration of block
LU factorization in a careful order that resembles Crout LU
reduction. The ordering is reasoned as follows: LU with
partial row pivoting cannot be performed until the needed

An Efﬁcient Algorithm for Low-Rank Approximation

columns are selected, and so column selection must ﬁrst
occur at each iteration. Once a block column is selected, a
partial Schur update must be performed on that block col-
umn before proceeding. At this point, an iteration of block
LU with partial row pivoting can be performed on the cur-
rent block. Once the row pivoting is performed, a partial
Schur update of the block of pivoted rows of U can be per-
formed, which completes the factorization up to rank j + b.
Finally, the projection matrix R can be cheaply updated to
prepare for the next iteration. Note that any column se-
lection method may be used when picking column pivots
from R, such as QR with column pivoting, LU with row
pivoting, or even this algorithm can again be run on the
subproblem of column selection of R. The ﬂop count of
TRLUCP is dominated by the three matrix multiplication
steps (lines 2, 5, and 7). The total number of ﬂops is

F TRLUCP = 2pmn + (m + n)k2 + O (k(m + n)) .

Note the transparent constants, and, because matrix mul-
tiplication is the bottleneck, this algorithm can be imple-
mented efﬁciently in terms of both computation as well
as memory usage. Because the output of TRLUCP is
only written once, the total number of memory writes is
(m + n − k)k. Minimizing the number of data writes by
only writing data once signiﬁcantly improves efﬁciency be-
cause writing data is typically one of the slowest compu-
tational operations. Also worth consideration is the sim-
plicity of the LU decomposition, which only involves three
types of operations: matrix multiply, scaling, and pivoting.
By contrast, state-of-the-art calculation of both the full and
truncated SVD requires a more complex process of bidiag-
onalization. The projection R can be updated efﬁciently
to become a random projection of the Schur complement
for the next iteration. This calculation involves the current
progress of the LU factorization and the random matrix Ω,
and is described in detail in the appendix.

3.2. Spectrum-Revealing Pivoting (SRP)

TRLUCP produces high-quality data approximations for
almost all data matrices, despite the lack of theoretical
guarantees, but can miss important rows or columns of the
data. Next, we develop an efﬁcient variant of the existing
rank-revealing LU algorithms (Gu & Eisenstat, 1996; Mira-
nian & Gu, 2003) to rapidly detect and, if necessary, correct
any possible matrix approximation failures of TRLUCP.

Intuitively, the quality of the factorization can be tested by
searching for the next choice of pivot in the Schur com-
plement if the factorization continued and determining if
the addition of that element would signiﬁcantly improve
the approximation quality. If so, then the row and column
with this element should be included in the approximation
and another row and column should be excluded to main-
tain rank. Because TRLUCP does not provide an updated

Schur complement, the largest element in the Schur com-
plement can be approximated by ﬁnding the column of R
with largest norm, performing a Schur update of that col-
umn, and then picking the largest element in that column.
Let α be this element, and, without loss of generality, as-
sume it is the ﬁrst entry of the Schur complement. Denote:

Π1AΠT

2 =





L11
(cid:96)T
L31

U11









1

I



 .

u U13
sT
α
12
s21 S22

(1)

Next, we must ﬁnd the row and column that should be re-
placed if the row and column containing α are important.
Note that the smallest entry of L11U11 may still lie in an
important row and column, and so the largest element of
the inverse should be examined instead. Thus we propose
deﬁning

A11

def=

(cid:18)L11
(cid:96)T

(cid:19) (cid:18)U11 u
α
1

(cid:19)

and testing

(cid:107)A

−1
11 (cid:107)max ≤

f
|α|

(2)

for a tolerance parameter f > 1 that provides a control of
accuracy versus the number of swaps needed. Should the
test fail, the row and column containing α are swapped with
−1
the row and column containing the largest element in A
11 .
Note that this element may occur in the last row or last col-
−1
umn of A
11 , indicating only a column swap or row swap
respectively is needed. When the swaps are performed,
the factorization must be updated to maintain truncated LU
form. We have developed a variant of the LU updating al-
gorithm of (Gondzio, 2007) to efﬁciently update the SRLU
factorization.

SRP can be implemented efﬁciently: each swap requires
−1
at most O (k(m + n)) operations, and (cid:107)A
11 (cid:107)max can be
quickly and reliably estimated using (Higham & Relton,
2015). An argument similar to that used in (Miranian &
Gu, 2003) shows that each swap will increase (cid:12)
(cid:1)(cid:12)
(cid:12)
by a factor at least f , hence will never repeat. At termina-
tion, SRP will ensure a partial LU factorization of the form
(1) that satisﬁes condition (2). We will discuss spectrum-
revealing properties of this factorization in Section 4.2.

(cid:12)det (cid:0)A11

It is possible to derive theoretical upper bounds on the
worst number of swaps necessary in SRP, but in practice,
this number is zero for most matrices, and does not exceed
3 − 5 in the most pathological data matrix of dimension at
most 1000 we can contrive.

SRLU can be used effectively to approximate second or-
der information in machine learning. SRLU can be used
as a modiﬁcation to ADAGRAD in a manner similar to the

An Efﬁcient Algorithm for Low-Rank Approximation

Algorithm 4 Spectrum-Revealing Pivoting (SRP)

1: Input: Truncated LU factorization A ≈ (cid:98)L (cid:98)U, toler-

ance f > 1
−1
11 (cid:107)max > f

|α| do

2: while (cid:107)A
3:

4:

Set α to be the largest element in S (or ﬁnd an ap-
proximate α using R)
Swap row and column containing α with row and
−1
column of largest element in A
11
Update truncated LU factorization

5:
6: end while

low-rank approximation method in (Krummenacher et al.,
2016). Applying the initialization technique in this work,
SRLU would likely provide an efﬁcient and accurate adap-
tive stochastic optimization algorithm. SRLU can also be-
come a full-rank approximation (low-rank plus diagonal)
by adding a diagonal approximation of the Schur comple-
ment. Such an approximation could be appropriate for im-
proving memory in artiﬁcial intelligence, such as in (Kirk-
patrick et al., 2017). SRLU is also a freestanding compres-
sion algorithm.

3.3. The CUR Decomposition with LU

swap is expected (although examples can be constructed
that require more than one swap), which requires at most
O (k (m + n)) ﬂops. By contrast, the URV decomposi-
tion of (Stewart, 1992) is O (cid:0)n2(cid:1), while SVD updating
requires O (cid:0)(m + n) min2 (m, n)(cid:1) operations in general,
2 (cid:15)(cid:1) for a numerical approx-
or O (cid:0)(m + n) min (m, n) log2
imation with the fast multipole method.

4. Theoretical Results for SRLU

Factorizations

4.1. Analysis of General Truncated LU Decompositions

Theorem 1 Let (·)s denote the rank-s truncated SVD for
s ≤ k (cid:28) m, n. Then for any truncated LU factorization
with Schur complement S:

(cid:107)Π1AΠT

2 − (cid:98)L (cid:98)U(cid:107) = (cid:107)S(cid:107)

for any norm, and

(cid:107)Π1AΠT

2 −

(cid:98)L (cid:98)U

(cid:16)

(cid:17)

s

(cid:107)2 ≤ 2(cid:107)S(cid:107)2 + σs+1 (A) .

Theorem 2 For a general rank-k truncated LU decompo-
sition, we have for all 1 ≤ j ≤ k,

A natural extension of truncated LU factorizations is
a CUR-type decomposition for increased accuracy (Ma-
honey & Drineas, 2009):





(cid:16)

(cid:17)

σj (A) ≤ σj

(cid:98)L (cid:98)U

1 +

1 +





(cid:17)



 .

(cid:107)S(cid:107)2
σj (A)

(cid:107)S(cid:107)2
(cid:16)

σk

(cid:98)L (cid:98)U

Π1AΠT

2 ≈ (cid:98)L

(cid:16)

(cid:98)L†A (cid:98)U†(cid:17)

(cid:98)U def= (cid:98)LM (cid:98)U.

Theorem 3 CUR Error Bounds.

As with standard CUR, the factors (cid:98)L and (cid:98)U retain (much
of) the sparsity of the original data, while M is a small,
k-by-k matrix. The CUR decomposition can improve the
accuracy of an SRLU with minimal extra needed memory.
Extra computational time, nevertheless, is needed to calcu-
late M. A more efﬁcient, approximate CUR decomposition
can be obtained by replacing A with a high quality approx-
imation (such as an SRLU factorization of high rank) in the
calculation of M.

and

3.4. The Online SRLU Factorization

Given a factored data matrix A ∈ Rm×n and new observa-

2 = (cid:0)
tions BΠT
decomposition takes the form

k
m − k
B1 B2

(cid:1) ∈ Rs×m, an augmented LU

(cid:19)

(cid:18)Π1AΠT
BΠT
2

2

=





L11
L21
L31

I









I

U11 U12
S
Snew



 ,

where L31 = B1U−1
11 U12. An
SRLU factorization can then be obtained by simply per-
forming correcting swaps. For a rank-1 update, at most 1

11 and Snew = B2 − B1U−1

(cid:107)Π1AΠT

2 − (cid:98)LM (cid:98)U(cid:107)2 ≤ 2(cid:107)S(cid:107)2

(cid:107)Π1AΠT

2 − (cid:98)LM (cid:98)U(cid:107)F ≤ (cid:107)S(cid:107)F .

Theorem 1 simply concludes that the approximation is ac-
curate if the Schur complement is small, but the singular
value bounds of Theorem 2 are needed to guarantee that
the approximation retains structural properties of the orig-
inal data, such as an accurate approximation of the rank
and the spectrum. Furthermore, singular values bounds can
be signiﬁcantly stronger than the more familiar norm error
bounds that appear in Theorem 1. Theorem 2 provides a
general framework for singular value bounds, and bound-
ing the terms in this theorem provided guidance in the de-
sign and development of SRLU. Just as in the case of de-
terministic LU with complete pivoting, the sizes of

(cid:107)S(cid:107)2
σk((cid:98)L (cid:98)U)
range from moderate to small for almost all

and

(cid:107)S(cid:107)2
σj((cid:98)L (cid:98)U)

data matrices of practical interest. They, nevertheless, can-
not be effectively bounded for a general TRLUCP factor-
ization, implying the need for Algorithm 4 to ensure that

An Efﬁcient Algorithm for Low-Rank Approximation

these terms are controlled. While the error bounds in The-
orem 3 for the CUR decomposition do not improve upon
the result in Theorem 1, CUR bounds for SRLU speciﬁ-
cally will be considerably stronger. Next, we present our
main theoretical contributions.

and

Theorem 6

(cid:107)Π1AΠT

2 − (cid:98)LM (cid:98)U(cid:107)2 ≤ 2γσk+1 (A)

4.2. Analysis of the Spectrum-Revealing LU

Decomposition

√

Theorem 4 (SRLU Error Bounds.) For j ≤ k and γ =
mn), SRP produces a rank-k SRLU factorization
O (f k
with

(cid:107)Π1AΠT
2 − (cid:98)L (cid:98)U(cid:107)2 ≤ γσk+1 (A) ,
(cid:16)
(cid:17)

(cid:107)Π1AΠT

2 −

(cid:98)L (cid:98)U

(cid:107)2 ≤ σj+1 (A)

(cid:16)
1 + 2γ σk+1(A)
σj+1(A)

(cid:17)

j

Theorem 4 is a special case of Theorem 1 for SRLU fac-
torizations. For a data matrix with a rapidly decaying spec-
trum, the right-hand side of the second inequality is close
to σj+1 (A), a substantial improvement over the sharpness
of the bounds in (Drineas et al., 2008).

Theorem 5 (SRLU Spectral Bound). For 1 ≤ j ≤ k, SRP
produces a rank-k SRLU factorization with

(cid:107)Π1AΠT

2 − (cid:98)LM (cid:98)U(cid:107)F ≤ ωσk+1 (A) ,
√

mn) is the same as in Theorem 4, and

where γ = O (f k
ω = O (f kmn).

Theorem 7 If σ2

j (A) > 2(cid:107)S(cid:107)2

2 then

σj (A) ≥ σj

(cid:98)LM (cid:98)U

≥ σj (A)

1 − 2γ

(cid:16)

(cid:17)

(cid:115)

(cid:19)2

(cid:18) σk+1 (A)
σj (A)

for γ = O (cid:0)mnk2f 2(cid:1) and f is an input parameter control-
ling a tradeoff of quality vs. speed as before.

As before, the constants are small in practice. Observe that
for most real data matrices, their singular values decay with
increasing j. For such matrices this result is signiﬁcantly
stronger than Theorem 5.

(cid:17)

(cid:18)

(cid:98)L (cid:98)U

≤ σj (A)

1 + τ

(cid:19)

σk+1 (A)
σj (A)

5. Experiments

5.1. Speed and Accuracy Tests

(cid:16)

≤ σj

σj (A)
1 + τ σk+1(A)
σj (A)
for τ ≤ O (cid:0)mnk2f 3(cid:1).

While the worst case upper bound on τ is large, it is
dimension-dependent, and j and k may be chosen so that
σk+1(A)
is arbitrarily small compared to τ . In particular, if
σj (A)
k is the numeric rank of A, then the singular values of the
approximation are numerically equal to those of the data.

These bounds are problem-speciﬁc bounds because their
quality depends on the spectrum of the original data, rather
than universal constants that appear in previous results. The
beneﬁt of these problem-speciﬁc bounds is that an approx-
imation of data with a rapidly decaying spectrum is guar-
anteed to be high-quality. Furthermore, if σk+1 (A) is not
small compared to σj (A), then no high-quality low-rank
approximation is possible in the 2 and Frobenius norms.
Thus, in this sense, the bounds presented in Theorems 4
and 5 are optimal.

Given a high-quality rank-k truncated LU factorization,
Theorem 5 ensures that a low-rank approximation of rank
(cid:96) with (cid:96) < k of the compressed data is an accurate rank-(cid:96)
approximation of the full data. The proof of this theorem
centers on bounding the terms in Theorems 1 and 2. Exper-
iments will show that τ is small in almost all cases.

Stronger results are achieved with the CUR version of
SRLU:

In Figure 1, the accuracy of our method is compared to
the accuracy of the truncated SVD. Note that SRLU did
not perform any swaps in these experiments. “CUR” is
the CUR version of the output of SRLU. Note that both
methods exhibits a convergence rate similar to that of the
truncated SVD (TSVD), and so only a constant amount of
extra work is needed to achieve the same accuracy. When
the singular values decay slowly, the CUR decomposition
provides a greater accuracy boost.
In Figure 2, the run-
time of SRLU is compared to that of the truncated SVD, as
well as Subspace Iteration (Gu, 2015). Note that for Sub-
space Iteration, we choose iteration parameter q = 0 and
do not measure the time of applying the random projec-
tion, in acknowledgement that fast methods exist to apply a
random projection to a data matrix. Also, the block size im-
plemented in SRLU is signiﬁcantly smaller than the block
size used by the standard software LAPACK, as the size of
the block size affects the size of the projection. See supple-
ment for additional details. All numeric experiments were
run on NERSC’s Edison. For timing experiments, the trun-
cated SVD is calculated with PROPACK.

Even more impressive, the factorization stage of SRLU be-
comes arbitrarily faster than the standard implementation
of the LU decomposition. Although the standard LU de-
composition is not a low-rank approximation algorithm, it
is known to be roughly 10 times faster than the SVD (Dem-
mel, 1997). See appendix for details.

An Efﬁcient Algorithm for Low-Rank Approximation

(a) Spectral Decay = 0.8

(b) Spectral Decay = 0.95

Figure 1. Accuracy experiment on random 1000x1000 matrices with different rates of spectral decay.

(a) Rank-100 Factorizations

(b) Time vs. Truncation Rank

Figure 2. Time experiment on various random matrices, and a time experiment on a 1000x1000 matrix with varying truncation ranks.

Table 1. Errors of low-rank approximations of the given target
rank. SRLU is measured using the CUR version of the factor-
ization.

DATA

k

Gaus.

SRFT

Dual BCH

SRLU

S80PIn1
deter3
lc3d
lc3d

63
127
63
78

3.85
9.27
18.39

3.80
9.30
16.36

3.81
9.26
15.49

2.84
8.30
16.94
15.11

Next, we compare SRLU against competing algorithms. In
(Ubaru et al., 2015), error-correcting codes are introduced
to yield improved accuracy over existing random projec-
tion low-rank approximation algorithms. Their algorithm,
denoted Dual BCH, is compared against SRLU as well as
two other random projection methods: Gaus., which uses
a Gaussian random projection, and SRFT, which uses a
Fourier transform to apply a random projection. We test the
spectral norm error of these algorithms on matrices from
the sparse matrix collection in (Davis & Hu, 2011).

In Table 1, results for SRLU are averaged over 5 exper-
iments. Using tuning parameter f = 5, no swaps were
needed in all cases. The matrices being tested are sparse
matrices from various engineering problems. S80PIn1
is 4,028 by 4,028, deter3 is 7,647 by 21,777, and
lp ceria3d (abbreviated lc3d) is 3,576 by 4,400. Note

that SRLU, a more efﬁcient algorithm, provides a better ap-
proximation in two of the three experiments. With a little
extra oversampling, a practical assumption due to the speed
advantage, SRLU achieves a competitive quality approx-
imation. The oversampling highlights an additional and
unique advantage of SRLU over competing algorithms: if
more accuracy is desired, then the factorization can simply
continue as needed.

5.2. Sparsity Preservation Tests

The SRLU factorization is tested on sparse, unsymmetric
matrices from (Davis & Hu, 2011). Figure 3 shows the
sparsity patterns of the factors of an SRLU factorization
of a sparse data matrix representing a circuit simulation
(oscil dcop), as well as a full LU decomposition of the
data. Note that the LU decomposition preserves the spar-
sity of the data initially, but the full LU decomposition be-
comes dense. Several more experiments are shown in the
supplement.

5.3. Towards Feature Selection

An image processing example is now presented to illustrate
the beneﬁt of highlighting important rows and columns se-
lection.
In Figure 4 an image is compressed to a rank-
50 approximation using SRLU. Note that the rows and
columns chosen overlap with the astronaut and the planet,
implying that minimal storage is needed to capture the

An Efﬁcient Algorithm for Low-Rank Approximation

words/terms, and an initial SRLU factorization of rank 20
was performed on the ﬁrst 30K documents. The initial fac-
torization contained none of the top ﬁve words, but, af-
ter adding the remaining documents and updating, the top
three were included in the approximation. The fourth and
ﬁfth words ‘market’ and ‘california’ have high covariance
with at least two of the three top words, and so their inclu-
sion may be redundant in a low-rank approximation.

6. Conclusion

We have presented SRLU, a low-rank approximation
method with many desirable properties: efﬁciency, accu-
racy, sparsity-preservation, the ability to be updated, and
the ability to highlight important data features and vari-
ables. Extensive theory and numeric experiments have il-
lustrated the efﬁciency and effectiveness of this method.

Acknowledgements

This research was supported in part by NSF Award CCF-
1319312.

References

Aizenbud, Y., Shabat, G., and Averbuch, A. Random-
ized lu decomposition using sparse projections. CoRR,
abs/1601.04280, 2016.

Batson, J., Spielman, D. A., and Srivastava, N. Twice-
ramanujan sparsiﬁers. SIAM Journal on Computing, 41
(6):1704–1721, 2012.

Chan, T. F. Rank revealing qr factorizations. Linear alge-

bra and its applications, (88/89):67–82, 1987.

Cheng, H., Gimbutas, Z., Martinsson, P.-G., and Rokhlin,
V. On the compression of low rank matrices. SIAM J.
Scientiﬁc Computing, 26(4):1389–1404, 2005.

Chow, E. and Patel, A. Fine-grained parallel incomplete
lu factorization. SIAM J. Scientiﬁc Computing, 37(2),
2015.

Clarkson, K. L. and Woodruff, D. P. Low rank approx-
imation and regression in input sparsity time. CoRR,
abs/1207.6365, 2012.

Davis, T. A. and Hu, Y. The university of ﬂorida sparse ma-
trix collection. ACM Transactions on Mathematical Soft-
ware, 38:1:1–1:25, 2011. URL http://www.cise.
ufl.edu/research/sparse/matrices.

Demmel, J. Applied Numerical Linear Algebra. SIAM,

1997.

(a) L and U patterns of a low-rank factorization

(b) L and U patterns of the full factorization

Figure 3. The sparsity patterns of the L and U matrices of a rank
43 SRLU factorization, followed by the sparsity pattern of the L
and U matrices of a full LU decomposition of the same data. For
the SRLU factorization, the green entries compose the low-rank
approximation of the data. The red entries are the additional data
needed for an exact factorization.

black background, which composes approximately two
thirds of the image. While this result cannot be called fea-
ture selection per se, the rows and columns selected high-
light where to look for features: rows and/or columns are
selected in a higher density around the astronaut, the cur-
vature of the planet, and the storm front on the planet.

(a) Original

(b) SRLU

(c) Rows and Cols.

Figure 4. Image processing example.
image
(NASA), a rank-50 approximation with SRLU, and a highlight
of the rows and columns selected by SRLU.

The original

5.4. Online Data Processing

Online SRLU is tested here on the Enron email corpus
(Lichman, 2013). The documents were initially reverse-
sorted by the usage of the most common word, and then
reverse-sorted by the second most, and this process was
repeated for the ﬁve most common words (the top ﬁve
words were used signiﬁcantly more than any other), so that
the most common words occurred most at the end of the
corpus. The data contains 39,861 documents and 28,102

An Efﬁcient Algorithm for Low-Rank Approximation

Deshpande, A. and Vempala, S. Adaptive sampling and fast
low-rank matrix approximation. In APPROX-RANDOM,
volume 4110 of Lecture Notes in Computer Science, pp.
292–303. Springer, 2006.

Deshpande, A., Rademacher, L., Vempala, S., and Wang,
G. Matrix approximation and projective clustering via
volume sampling. Theory of Computing, 2(12):225–247,
2006.

Drineas, P., Mahoney, M. W., and Muthukrishnan, S.
Relative-error cur matrix decompositions. SIAM J. Ma-
trix Analysis Applications, 30(2):844–881, 2008.

Eckart, C. and Young, G. The approximation of one matrix
by another of lower rank. Psychometrika, 1(3):211–218,
1936.

Frieze, A. M., Kannan, R., and Vempala, S. Fast monte-
carlo algorithms for ﬁnding low-rank approximations. J.
ACM, 51(6):1025–1041, 2004.

Ghavamzadeh, M., Lazaric, A., Maillard, O.-A., and
Munos, R. Lstd with random projections. In NIPS, pp.
721–729, 2010.

Gondzio, J. Stable algorithm for updating dense lu factor-
ization after row or column exchange and row and col-
umn addition or deletion. Optimization: A journal of
Mathematical Programming and Operations Research,
pp. 7–26, 2007.

Grigori, L., Demmel, J., and Li, X. S. Parallel symbolic
factorization for sparse lu with static pivoting. SIAM J.
Scientiﬁc Computing, 29(3):1289–1314, 2007.

Gu, M. Subspace iteration randomization and singular
value problems. SIAM J. Scientiﬁc Computing, 37(3),
2015.

Gu, M. and Eisenstat, S. C. Efﬁcient algorithms for com-
puting a strong rank-revealing qr factorization. SIAM J.
Sci. Comput., 17(4):848–869, 1996.

Halko, N., Martinsson, P.-G., and Tropp, J. A. Finding
structure with randomness: Probabilistic algorithms for
constructing approximate matrix decompositions. SIAM
Review, 53(2):217–288, 2011.

Higham, N. J. and Relton, S. D. Estimating the largest
entries of a matrix. 2015. URL http://eprints.
ma.man.ac.uk/.

Jaderberg, M., Vedaldi, A., and Zisserman, A. Speeding up
convolutional neural networks with low rank expansions.
CoRR, abs/1405.3866, 2014.

Johnson, W. B. and Lindenstrauss, J. Extensions of lip-
schitz mappings into a hilbert space. Contemporary
Mathematics, 26:189–206, 1984.

Khabou, A., Demmel, J., Grigori, L., and Gu, M. Lu factor-
ization with panel rank revealing pivoting and its com-
munication avoiding version. SIAM J. Matrix Analysis
Applications, 34(3):1401–1429, 2013.

Kirkpatrick, J., Pascanu, R., Rabinowitz, N. C., Veness,
J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J.,
Ramalho, T., Grabska-Barwinska, A., Hassabis, D.,
Clopath, C., Kumaran, D., and Hadsell, R. Overcom-
ing catastrophic forgetting in neural networks. Pro-
ceedings of the National Academy of Sciences, 114(13):
3521–3526, 2017.

Krummenacher, G., McWilliams, B., Kilcher, Y., Buh-
mann, J. M., and Meinshausen, N. Scalable adaptive
stochastic optimization using random projections.
In
NIPS, pp. 1750–1758, 2016.

Liberty, E., Woolfe, F., Martinsson, P.G., Rokhlin, V., and
Tygert, M. Randomized algorithms for the low-rank ap-
proximation of matrices. Proceedings of the National
Academy of Sciences, 104(51):20167, 2007.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Luo, H., Agarwal, A., Cesa-Bianchi, N., and Langford, J.
Efﬁcient second order online learning by sketching. In
NIPS, pp. 902–910, 2016.

Mahoney, M. W. and Drineas, P. Cur matrix decompo-
sitions for improved data analysis. Proceedings of the
National Academy of Sciences, 106(3):697–702, 2009.

Martinsson, P.-G., Rokhlin, V., and Tygert, M. A random-
ized algorithm for the approximation of matrices. Tech.
Rep., Yale University, Department of Computer Science,
(1361), 2006.

Melgaard, C. and Gu, M. Gaussian elimination with ran-
domized complete pivoting. CoRR, abs/1511.08528,
2015.

Miranian, L. and Gu, M. Stong rank revealing lu factor-
izations. Linear Algebra and its Applications, 367:1–16,
2003.

Nasa celebrates 50 years of

spacewalk-
URL https://www.nasa.gov/image-

NASA.
ing.
feature/nasa-celebrates-50-years-of-
spacewalking. Accessed on August 22, 2015.
Published on June 3, 2015. Original photograph from
February 7, 1984.

An Efﬁcient Algorithm for Low-Rank Approximation

Papadimitriou, C. H., Raghavan, P., Tamaki, H., and Vem-
pala, S. Latent semantic indexing: A probabilistic anal-
ysis. J. Comput. Syst. Sci., 61(2):217–235, 2000.

Sarlos, T.

Improved approximation algorithms for large
matrices via random projections. In FOCS, pp. 143–152.
IEEE Computer Society, 2006.

Stewart, G. W. An updating algorithm for subspace track-
ing. IEEE Trans. Signal Processing, 40(6):1535–1541,
1992.

Ubaru, S., Mazumdar, A., and Saad, Y. Low rank approxi-
mation using error correcting coding matrices. In ICML,
volume 37, pp. 702–710, 2015.

Wang, W. Y., Mehdad, Y., Radev, D. R., and Stent, A. A
low-rank approximation approach to learning joint em-
beddings of news stories and images for timeline sum-
marization. pp. 58–68, 2016.

Woolfe, F., Liberty, E., Rokhlin, V., and Tygert, M. A fast
randomized algorithm for the approximation of matrices.
Applied and Computational Harmonic Analysis, 25(3):
335–366, 2008.

