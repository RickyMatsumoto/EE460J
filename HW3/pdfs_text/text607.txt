Learning Stable Stochastic Nonlinear Dynamical Systems

Jonas Umlauft 1 Sandra Hirche 1

Abstract
A data-driven identiﬁcation of dynamical sys-
tems requiring only minimal prior knowledge
is promising whenever no analytically derived
model structure is available, e.g., from ﬁrst prin-
ciples in physics. However, meta-knowledge on
the system’s behavior is often given and should
be exploited: Stability as fundamental property
is essential when the model is used for controller
design or movement generation. Therefore, this
paper proposes a framework for learning sta-
ble stochastic systems from data. We focus on
identifying a state-dependent coefﬁcient form of
the nonlinear stochastic model which is globally
asymptotically stable according to probabilistic
Lyapunov methods. We compare our approach
to other state of the art methods on real-world
datasets in terms of ﬂexibility and stability.

1. Introduction

An accurate identiﬁcation of the system dynamics is the
ﬁrst and very crucial step to many modern control methods.
Although reinforcement learning also allows model-free
search for optimal policies, it is known to be less efﬁcient
and difﬁcult to analyze. Therefore, classical control en-
gineers employ system identiﬁcation techniques to obtain
parametric model descriptions of dynamical systems from
observation data, e.g., in the linear case ARX and ARMAX
models. The identiﬁcation focuses on model selection, i.e.,
ﬁnding the model structure and the corresponding set of pa-
rameters. But often this set of model candidates is difﬁcult
to ﬁnd, especially for complex, possibly non-deterministic,
systems (Ljung, 1998). Therefore, the need for data-driven
models has emerged recently as control engineering is in-
creasingly applied in areas without analytic description of
the dynamical system. We consider the following two ap-
plication scenarios: First, assume a set of trajectories for a

1Chair of Information-oriented Control, Technical University
of Munich, Munich, Germany. Correspondence to: Jonas Um-
lauft <jonas.umlauft@tum.de>.

Proceedings of the 34th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

robotic task is given through human demonstrations, e.g.,
object grasping. The goal is to represent the motion with a
dynamical system. To ensure the reproduction terminates
at the desired ﬁnal point (object to grasp), we introduce the
stability constraint. Second, consider a dynamical system
which is known to be stable, e.g., a pendulum which rests
in hanging position. The goal is to identify the dynamics
precisely without further physical insights.

Bayesian non-parametric methods, more particularly Gaus-
sian Processes (GPs) where successfully employed by Ko-
cijan et al. (2005) and Wang et al. (2005) for system iden-
tiﬁcation. Other approaches focus on learning switching
linear systems (Fox et al., 2009) or employ an EM algo-
rithm (Ghahramani & Roweis, 1999) for nonlinear systems.
However, these approaches neglect the prior assumption
that the dynamical system is stable, which becomes cru-
cial when the learned model is used as a generative pro-
cess such as in movement generation for robotics (Ijspeert
et al., 2002). If stability is not considered during learning,
the identiﬁed model suffers from spurious attractors which
are not part of the true dynamics or instability.

Only little work has merged the extensive knowledge on
stability theory from control engineering with the powerful
data-driven approaches for system identiﬁcation: For ex-
ample Boots et al. (2008) and Chiuso & Pillonetto (2010)
take stability constraints for learning dynamical systems
into account but are limited to linear systems. The work
by Khansari-Zadeh & Billard (2011) ensures stability of
the system by constraining the optimization of a Gaus-
sian Mixture Model (GMM) to stability conditions derived
from Lyapunov methods. The work by Paraschos et al.
(2013) relies on a phase variable to ensure stability, which
makes the approach time-dependent and therefore less ro-
bust. Control Lyapunov functions are used by Khansari-
Zadeh & Billard (2014) to ensure global stability for the
learned system. These approaches partially employ prob-
abilistic models (GP, GMM), but limit the analysis to the
deterministic part by only considering the mean regression.
By discarding the true underlying probability distribution,
information regarding reliability of the model provided by
the data is lost. This leads to overconﬁdent conclusions
regarding performance or safety on the real system.

Therefore, this work proposes a framework for learning

Stable Stochastic Nonlinear Dynamical Systems

probabilistic nonlinear dynamical systems from observa-
tion, which takes the prior assumption of stability into ac-
count. The required stochastic stability conditions of the
discrete-time Markov processes are derived from Lyapunov
theory. We provide simulation results to validate the pro-
posed approach and compare it to previously mentioned
methods for identifying dynamical systems.

2. Problem Formulation

This also implies the following type of convergence, which
might be more intuitive to the reader.
Deﬁnition 2 (Convergence in probability). The chain
converges to the origin in probability if
(cid:15))
for each (cid:15) > 0.

xk}
{
0,
→

xk(cid:107) ≥
(
(cid:107)

P

We do not consider any control input here, thus the identiﬁ-
cation takes place for the closed-loop system for a existing
controller or an uncontrolled system.

We consider an autonomous, dynamical, discrete-time sys-
= Rd. The state
tem with continuous-valued state xk ∈ X
evolves according to an unknown stochastic process1

3.1. The Model

3. Stability Conditions for the Model

xk+1 = ˆf (xk, ˆωk),

(1)

Consider the state-dependent coefﬁcient form of f ψ

P

,
and the probability measure

and ˆωk is a random variable from
with initial value x0 ∈ X
) with sample space Ω, the
the probability space (Ω,
F
corresponding σ-algebra
.
P
F
Since xk ∈ X
is ﬁxed at each step, (1) describes a state de-
pendent distribution over xk+1. A realization of ˆωk ∈
Ω, is
drawn at every time step, yielding a realization of the next
step. As the distribution for xk+1 only depends on the state
at time step k, ˆf is a Markov process, denoted by

.

xk}
{

{

¯xn, ¯xn+1}

We assume that consecutive measurements of the state are
available, thus N data pairs are given in the trainings set
N
D =
n=1. Based on these measurements, we
model the unknown dynamics ˆf including the distribu-
tion ˆωk using the prior knowledge, that the stochastic pro-
cess (1) converges to the origin xk = 0. The model con-
sists of the mapping f ψ and a encoding of the random vari-
able ω deﬁned by a ﬁnite parameter vector ψ
Ψ. As the
model f ψ must best possibly explain the data D, the prob-
lem is formulated as constrained likelihood maximization

∈

ψ∗ = arg max

log

N
(cid:88)

n=1

ψ

(cid:0)¯xn+1|

P

¯xn, f ψ

(cid:1) ,

s.t.

xk}

{

converges to the origin for k

.
→ ∞

(2a)

(2b)

As different stochastic stability concepts exist, the conver-
gence in (2b) is deﬁned as convergence with probability one
(w.p.1) (Kushner, 1971):

{
xk(cid:107) ≥

Deﬁnition 1 (Convergence w.p.1).
origin w.p.1 if, for each (cid:15) > 0,

converges to the
xk}
(cid:15) only ﬁnitely often.

(cid:107)
1Notation: Bold symbols denote vectors or multivariate func-
tions, capital letters matrices and Ip the p × p identity ma-
trix. A (cid:31) 0 denotes positive deﬁniteness of the matrix A,
E [·] the expected value, V [·] the variance of a random variable
and C [·, ·] the covariance between two random variables, where
C [a] = C [a, a]. ¯X denotes a realization of the random variable
X.
Imitating Matlab indexing, A(:,i) denotes the i-th column,
A(j,:) the j-th row and A(1:2,i) the ﬁrst and second element in the
i-the column of A. The i-th entry of the vector xk is denoted xk,i.

xk+1 = A(xk)xk,

(3)

∈

X →

where, for a ﬁxed xk, A is a random variable from the prob-
ability space (ΩA,
PA) with the sample space ΩA ⊆
FA,
d. The probability density function of A is speciﬁed
Rd
×
Θ, which is state dependent through
by the vector θ
θψ :
Θ. This mapping is itself parametrized by a
vector ψ. At each step, a realization of A, denoted by ¯A,
is drawn and multiplied by the state xk to proceed by one
step. This is visualized in Figure 1 along with the two-layer
model structure: The ﬁrst layer maps current state xk ∈ X
Θ. The
onto the parameter θ
mapping is parametrized by ψ. The second layer is the
probability distribution on A which assigns to each element
in the sample space ΩA a probability based on θ.

Θ, denoted by θψ :

X →

∈

Figure 1. Illustration of the two layer design of the proposed
model and the process of simulation.

To illustrate this multilayer design, we give a brief example
in the scalar case d = 1: Assume A(xk) follows, for a
(µ, σ). Therefore,
given xk, a Gaussian distribution A
∼ N
R+,
the parameter vector is θ
R, σ
R+. The dependency of these parameters
thus Θ
R
on the current state xk is expressed in θψ
N
(cid:20)wxk
(cid:21)
zx2
k

= [µ σ](cid:124) with µ

(cid:20)µ(xk)
σ(xk)

, e.g.,
(cid:21)

= θψ
N

(xk) =

N ⊂

(4)

×

∈

∈

N

,

θψ:X→ΘxkΘ→(ΩA,FA,PA)Axk=xk+1∼samplexk+1t+1→tψθA∼ModelSimulationStable Stochastic Nonlinear Dynamical Systems

where linear dependency of the mean on the state and a
quadratic relation between variance and the state is as-
here are ψ = [w z](cid:124).
sumed. The parameters deﬁning θψ
N
Generally, the ﬁrst layer θψ :
Θ can be any state of
X →
the art parametric regression method which is parametrized
by ψ. For layer two, any probability distribution with a
ﬁxed set of parameters is applicable for A.

Leaving the stochastic aspect aside, model (3) is the state-
dependent coefﬁcient (SDC) form which is reached by fac-
torizing a nonlinear system into a linear-like structure. It
was shown, that for a any continuous differentiable func-
tion f with f (0) = 0, their exists a matrix-valued function
A(x) such that f (x) = A(x)x, see Cimen (2008). Thus,
the SDC form is not limiting the expressive power of our
model. It also reﬂects the setup of many real-world system,
e.g., consider an actuator whose output is generally noisy
and the magnitude of the noise is dependent on the temper-
ature. By modeling the temperature as a state, the model (3)
allows to capture this varying precision of the actuator.

The structure of the model (3) combines two important cri-
teria. First, it provides more ﬂexibility than a linear sys-
tem with random parameters, so it encodes also nonlinear
dynamics.Second, it is simple enough to allow a quadratic
Lyapunov function analysis and therefore the derivation of
analytic constraints for convergence as needed for the opti-
mization in (2).

3.2. Stability Analysis

For approaching the problem as formulated in Section 2,
an analytic condition for the constraint in the optimization
problem (2b), given that f ψ is of the form (3), is needed.
The literature on stability criteria for dynamical systems is
very rich and for nonlinear systems Lyapunov type meth-
ods are often used. They are based on the following idea:
If there is a function representing the ”energy” in the sys-
tem (called Lyapunov function) which constantly decreases
over time, the state will converge to a ”zero energy” state,
the origin. More precise, the Lyapunov function must be
positive deﬁnite and it must be strictly decreasing over
time, except in the origin. Using the stochastic discrete-
time version of Lyapunov methods and the Borel-Cantelli
Lemma leads to the following conditions for exponential
stability (which implies convergence w.p.1 as deﬁned in
Deﬁnition 1)

Theorem 1 (Exponential Stability,
Given a positive deﬁnite function V (xk)

(Kushner, 1971)).
0 for which

≥

xk]
E [V (xk+1)
|

−

V (xk)

≤ −

αV (xk),

xk ∈ X \
∀

0,
(5)

for some α > 0 then

E [V (xk+m)
V (xk+m)

xk]
|

(1
≤
0 for m

−

α)mV (xk) and
(w.p.1).

→

→ ∞

(6)

(7)

For the class of systems in (3) a quadratic function V (xk)
is a proper Lyapunov function to derive sufﬁcient stability
constraints for arbitrary distributions on A as shown in the
following proposition:
Proposition 1 (Stability of the model (3)). Consider a
stochastic process generated from (3) where in each step a
d.
realization of A is drawn from sample space ΩA ⊂
The process is globally exponentially stable at xk = 0 if
0 such that
there exists a P

Rd
×

(cid:124)

(cid:31)
(xk)] P E [A(xk)] +Q

E [A

(1

α)P

0,

−

−

(cid:22)

,
xk ∈ X
∀
(8)

for some α > 0, where Q is deﬁned as
(cid:88)

Q(i,j)(xk) =

P(l,:) C

(cid:2)A(:,i)(xk), A(l,j)(xk)(cid:3),

(9)

l

.

for any x0 ∈ X
Proof. Considering
a
(cid:124)
V (xk) = x
kP xk with P
Theorem 1 in (5) is given as
(cid:3)

(cid:124)

k+1P xk+1|

xk+1

(cid:2)x

E

quadratic Lyapunov
0,

function
the inequality from

(cid:23)

−
which yields for the stochastic2 process xk+1 = A(xk)xk

(cid:124)
kP xk ≤ −
x

αx

(cid:124)
kP xk,

x

(cid:124)
k E [A

(cid:124)

] P E [A] xk + Tr (P C [Axk])
(1

(cid:124)
α)x

−
kP xk ≤

0,

−

−

(10)
.
xk ∈ X
∀

Now, an expression for the trace is derived as follows
(cid:32)

(cid:35)(cid:33)

(cid:34)

Tr (P C [Axk]) = Tr

P C

A(:,i)xk,i

(cid:88)

i

= Tr

P

xk,ixk,j C

(cid:2)A(:,i), A(:,j)


(cid:3)




(cid:88)

i,j

P(l,:) C

(cid:2)A(:,i), A(l,j)

(cid:3) xk,ixk,j, = x

(cid:124)
kQxk

where deﬁnition of Q in (9) was substituted. Using this
simpliﬁcation, (10) is rewritten as

(cid:19)

E [A] P E [A] +Q

−
xk ∈ X
which must hold for
∀
(cid:124)
trix E [A]
(1
P E [A] +Q
−
−
deﬁnite, which concludes the proof.

0,

−

(1

α)P

xk ≤
. To ensure this, the ma-
α)P must be negative semi-

2The xk dependency of the random process A as been dropped

for notational convenience.

=

(cid:88)

i,j,l

(cid:18) (cid:124)

x

(cid:124)
k

Stable Stochastic Nonlinear Dynamical Systems

E [A(xk)]2 + V [A(xk)]

1

α,

≤

−

xk ∈ X
∀

.

(11)

V [A(xk)] = κ2

V

(cid:104)

(cid:105)
˜A(xk)

= κ2

4. Stable Learning with Various Distributions

Condition (11) is rewritten to

P

≺

−

xk ∈ X
∀

The interpretation of Proposition 1 is analogue to the lin-
ear deterministic case xk+1 = Axk which is stable if there
exists a matrix P for which A(cid:124)P A
0: In the non-
linear case in (3) the negative deﬁniteness must be fulﬁlled
for A(xk),
. The probabilistic nature of the sys-
tem (3) in addition requires ”a buffer”, which here is Q.
The deterministic case is reconstructed if A has zero vari-
ance. The scalar case, considered in the following remark,
also allows an intuitive insight to the Proposition 1: There
is a trade-off between the magnitude of the expected value
and the variance of A as follows:
Remark 1. In the scalar case3, i.e. d = 1 in (3), with
Q = P V [A(xk)] condition (8) simpliﬁes for any P > 0 to

Our learning framework consists of three major steps:

1. Chose any probability distribution for the random
variable A in (3) which is given by a ﬁxed set of pa-
Θ and whose ﬁrst two moments are
rameters θ
∈
Θ for
It is assumed that subset Θ∗
available.
which (8) is fulﬁlled is non-empty, thus Θ∗
.

⊆
=

∅

2. Chose any parametric regression method to represent
Θ. The parameters of this
Ψ. The set of all ψ for

the mapping θψ :
mapping are denoted by ψ
which all xk ∈ X

map to Θ∗ is denoted by Ψ∗.

X →

∈

3. The likelihood maximization under constraints

N
(cid:88)

(cid:16)

ψ∗ = arg max
Ψ∗

ψ

log

P

xn+1|

∈

n=1

xn, θψ(cid:17)

,

(12)

is solved, where ψ
with P

0 and α > 0.

∈

(cid:31)

Ψ∗ is equivalent to constraint (8)

The optimization (12) is a general constrained nonlinear
program in a rather high dimensional space (depending on
number of parameters of the regression method in step 2).
However, independent of the optimality, the model f ψ∗ of
the form (3) is exponentially stable, thus any sample path
of the system converges. For computational simplicity, we
focus on two types of distribution which naturally fulﬁll the
constraints as explained in the next sections.

4.1. Stability with Beta Distribution

For certain choices of distributions, constraint (8) is ful-
ﬁlled for all possible parameter θ, thus Θ∗ = Θ, which

makes the optimization unconstrained. One example of
such a distribution is the Beta distribution as given in the
following corollary.
Corollary 1. The scalar system xk+1 = A(xk)xk
where A(xk) = κ( ˜A(xk)
η) with Beta distributed
˜A(xk)
(a(xk), b(xk)) and κ = 2, η = 0.5 with state
dependent parameters [a(xk) b(xk)](cid:124) = θψ
(xk), with any
B
θψ
+ is exponentially stable.
B

= R2

X →

∼ B

−

Θ

B

:

Proof. Applying the afﬁne transformation to mean and
variance leads to4
(cid:16)

(cid:18) a

(cid:19)

(cid:17)

(cid:104)

E [A(xk)] = κ

E

(cid:105)
˜A(xk)

η

−

,

= κ

η
a + b −
ab
(a + b)2(a + b + 1)

.

η))2 + κ2

1

≤

−

α,

V [A(xk)]

(κ(E [A(xk)]

the best possible

xk ∈ X
∀
for η minimizes
where
(E [A(xk)]
leaves the largest possi-
ble range for κ. As E [A] is in the interval ]0, 1[ the
minimization is achieved with the choice η = 1
2 . Then,
condition (11), divided by κ2 on both sides, evaluates to

η)2, because it

choice

−

−

,

(a + b)2 −

a2

+

1
4

a
a + b
ab
(a + b)(a + b + 1)
(cid:123)(cid:122)
(cid:125)
(cid:124)
0... 1
4

−

+

+

ab
(a + b)2(a + b + 1)
1
α
4 ≤

−
κ2

1

.

=

As α > 0 can be chosen arbitrarily small this condition
2. Hence, according to Theorem 1
holds for every
the system xk+1 = A(xk)xk is exponentially stable.

| ≤

κ
|

To ensure maximal ﬂexibility of the model, κ = 2 is set
for further considerations. This leads to the conclusion that
Θ∗
+. Therefore, in the optimization, no con-
B
straints on ψ must be considered, thus Ψ = Ψ∗.

= R2

= Θ

B

4.2. Stability with Dirichlet Distribution

Constructing A from a Dirichlet distribution also allows for
unconstrained optimization as it also leads to stable behav-
ior as shown in the following corollary.
Corollary 2. The d-dimensional system xk+1 = A(xk)xk
where each row of A(xk) consists of the ﬁrst d elements of
a d + 1 dimensional Dirichlet distributed vector, thus,

A(i,:) = a(i)

(1:d), with a(i)

with any θψi
D

:

X →

Θ

,

D

∀

(cid:16)

θψi
D

(cid:17)

(xk)

,

∼ D

i = 1 . . . d,

∀

i, is asymptotically stable w.p.1.

3Even though A, Q, P are scalars here, we keep them capital-

4The state dependency of a, b is dropped for notational conve-

ized for notational consistency.

nience.

(cid:54)
Stable Stochastic Nonlinear Dynamical Systems

∀

∀

¯A
∀

∈
d
(cid:88)

j=1

Proof. By construction the sample space of A, ΩA contains
only elements for which

A(i,j) > 0, A(i,j) < 1,

i, j = 1 . . . d

and

A(i,j) < 1,

i = 1 . . . d.

(13)

d
(cid:88)

j=1

Consider now a realization of A(xk) denoted by ¯A and
since the following statements hold for any realization in
the sample space, we omit writing

ΩA. It follows

d
(cid:88)

j=1

¯A(i,j) < 1

i
∀

⇒

max
i=1:d

¯A(i,j) < 1

¯A

⇒ (cid:107)

(cid:107)∞

= max
i=1:d

¯A(i,j)|
|

< 1,

d
(cid:88)

j=1

where the last inequality holds because all elements of ¯A
are strictly positive and
denotes the Maximum Ab-
(cid:107)∞
solute Row Sum Norm. Consider now M consecutive re-
alizations ¯A(i) with i = 1, . . . , M . For the maximum norm
of state in the M -th step holds

¯A

(cid:107)

xk+M (cid:107)∞
(cid:107)

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

M
(cid:89)

m=1
(cid:16)

¯A(m)xk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
∞
¯A(m)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

max
m

≤

M
(cid:89)

¯A(m)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

xk(cid:107)∞
(cid:107)

∞

xk(cid:107)∞
(cid:107)

M
→∞
−−−−→

0,

≤

m=1

(cid:17)M

∞

where the submultiplicativity property of induced matrices
(Horn & Johnson, 2013) is used. As convergence towards
the origin holds for each element in the sample space, the
system is stable with probability one. Therefore, the pa-
rameter space is unrestricted Θ∗
D

d+1
+ .

= R

= Θ

D

Remark 2. Note that this approach only allows to rep-
resent the special class of positive systems. Nevertheless,
positive systems play an important role in control engineer-
ing for modeling the evolution of strictly positive quantities
as shown in (Farina & Rinaldi, 2011).

Remark 3. An afﬁne transformation, as shown for
Beta distribution is not possible here because ab-
solute values are taken in the row sum.
There-
¯A(i,j)|
conclude
cannot
fore,
j=1 |
(cid:80)d
0.5)
|

< 1 one
< 1 for any κ > 1.

from (cid:80)d
κ( ¯A(i,j) −
5. Simulations

j=1 |

5.1. Setup

Figure 2. Comparison of true and inferred xk dependent variance
(top) and mean (bottom) function.

:

B

B

Θ

X →

is used for the mapping from the state to the parameters
θ
. Thus, the parameter vector ψ, is the con-
catenation of the prior πl, the means µl and the covariances
Σl for l = 1, . . . L . The code (based on Calinon (2009))
includes k-means clustering initialization and a transforma-
tion of Σl and πl to make it an unconstrained optimization.
To evaluate the likelihood function for each training point
¯xn+1, ¯xn}
, the Beta distribution parameters are computed
{
[an bn](cid:124) = θψ
(¯xn) using GMR. Then, the log likelihood
of ¯An = ¯xn+1/¯xn given the parameters [an bn] is evalu-
B
ated using the density function of the Beta distribution. As
= [a b](cid:124)
all possible parameter θ
+ lead to stability,
ﬁnding ψ∗ is an unconstrained optimization problem.

R2

∈

B

:

D

Θ

X →

For the Dirichlet distribution, the mapping from the state
to the parameters θψ
uses a nearest neighbor
D
approach for computational simplicity. The 2d = 4 closest
data points are considered for ﬁtting the training parame-
ters of the Dirichlet distribution locally. Then a training
point is placed at the center of these four points. At re-
production, the closest such training point and its Dirichlet
parameters are taken for regression. This does not necessar-
ily maximize the likelihood, but shows accurate results for
reproduction. We compare the following models from lit-
erature regarding reproduction precision and convergence
properties:

We validate our approach, labeled LeSSS (for Learning
Stable Stochastic Systems), using synthetic and human mo-
tion data and the simulation of a chemical reactor. For
the Beta distribution, Gaussian Mixture Regression (GMR)

•

The approach introduced by Boots et al. (2008)
learns stable linear dynamical system (stable LDS)
from data.
It constraints the search of the deter-
ministic dynamic matrix A to ensure the stability

024·10−3V[A]V[A(xk)]inferredV[A(xk)]true−8−6−4−20246800.51xk¯A,E[A]TrainingDataE[A(xk)]inferredE[A(xk)]trueStable Stochastic Nonlinear Dynamical Systems

of xk+1 = Axk.

•

•

∼ GP

Gaussian Process Dynamical Models (GPDM) (Wang
et al., 2005) represent dynamical system in the gen-
eral form xk+1 = f (xk), with Gaussian Process
(0, k(xk, x(cid:48)k)). We employ a zero prior mean
f
function and a squared exponential kernel. The hyper-
parameters of the kernel are optimized using the like-
lihood as described by Rasmussen & Williams (2006).
In reproduction, this method can either be used in de-
terministic setting by only taking the posterior mean
prediction µGP(xk) thus xk+1 = µGP(xk) or the
(µGP(xk), ΣGP(xk)),
stochastic setting xk+1 ∼ N
where ΣGP(xk) is the posterior variance. GPDMs are
bounded (Beckers & Hirche, 2016a;b) but not stable.

The Stable Estimator of Dynamical Systems (SEDS)
as introduced by Khansari-Zadeh & Billard (2011)
constraints the likelihood optimization of GMR pa-
rameters to a class of mean stable dynamical time-
The GMR maps from cur-
continuous systems.
rent state x to the time derivative ˙x.
fo-
cuses on deterministic systems by only consider-
ing stability criteria for the mean prediction of the
GMR, µGMM(x), while ignoring the stochastic na-
ture of GMMs, (its variance prediction ΣGMM). We
also run this method in a stochastic setting, where
˙x
(µGMM(x), ΣGMM(x)). For our simulations,
ﬁve mixtures are employed.

∼ N

It

Before starting the comparison to existing approaches,
LeSSS is demonstrated on a synthetic dataset.

5.2. Simulation 1: Synthetic Data

For the ﬁrst simulation, the task is to identify the stable
nonlinear stochastic system given by

xk+1 = A(xk)xk,

(14)

where A(xk)

(cid:0)(xk −

∼ B

5)2, (xk + 5)2(cid:1) .

The learning algorithm is given 100 training points
100
¯xn, ¯xn+1}
n=1 equally spaced in the state space interval
{
8, 8] which are drawn from the state dependent Beta dis-
[
−
tribution (14). Here L = 3 was chosen for the number of
mixtures in the GMR for the mapping θψ
B

X →

Θ

B

:

.

Figure 2 compares the mean and variance of the original
It clearly
system (14) to the one inferred by our model.
shows that the model offers sufﬁcient ﬂexibility to recon-
struct the original system. Note: It is also possible to verify
the parameter functions a(xk), b(xk) as given in (14), but
we directly look at the mean and variance functions as there
exists a unique mapping and it is more intuitive for inter-
pretation. It must be omitted, that the data was generated
from the same model which the algorithm is learning. This

Figure 3. The inferred mean and variance function for the training
set of the projected Z-shape movement are shown along with the
training data (the realizations of the random variable ¯A).

Figure 4. Comparison of simulations for x0 = 193 for stable
LDS and the mean predictions from the models GPDM, SEDS
and LeSSS.

explains the good ﬁtting, but is of course not often the case
in practical application. Therefore, we continue with a real
world dataset in the following.

5.3. Simulation 2: Human Motion Data

For the next simulation, we use the data set for letter-
shaped motions provided by Khansari-Zadeh & Billard
(2011). The 225 trainings points of 3 trajectories of the two
dimensional Z-shaped motion are projected on the y-axis.
The GMR for θψ
is trained with two mixtures.
B

B
Figure 3 shows the training data along with the ﬁt of the
mean and variance functions. The mean function shows a
smoothed estimate of the training data. The model iden-
tiﬁes properly that the training data has higher variability
(around xk = 0) and captures this in its variance function.

X →

Θ

:

Figure 4 compares the reproduction of the models stable
LDS, GPDM, SEDS and LeSSS if taking the deterministic
(mean) output of each model (all starting from the same

0501001502000.60.81xk¯A,E[A]TrainingDataE[A(xk)]inferredV[A(xk)]inferred00.020.040.060.08V[A]0204060801001200100200kxkTrainingDatastableLDSGPDMSEDSLeSSSStable Stochastic Nonlinear Dynamical Systems

Figure 5. Comparison of simulations for x0 = 193 with the
stochastic models of GPDM, SEDS and LeSSS.

initial point). The stable LDS approach leads to a con-
verging trajectory, but fails to capture the complexity of the
dynamic (as the true dynamic is nonlinear). The GPDM
converges to a spurious attractor at x
9.3 which is
undesired but not surprising. SEDS and LeSSS both lead
to asymptotic stable reproductions of the movement. Since
the data does not contain the full state (due to the projection
on the y-axis), it is not possible to reproduce the movement
precisely with a dynamical system model.

≈ −

Figure 5 compares the reproduction of the three stochastic
dynamical models GPDM, SEDS and LeSSS based on three
sample paths drawn from each model. The GPDM again
converges to the spurious attractor. SEDS clearly shows
that convergence of the mean is not sufﬁcient for converg-
ing trajectories of a stochastic system, as the drawn sam-
ple paths are strongly oscillating around the origin without
In the stochastic case only LeSSS
tendency to converge.
generates converging trajectories.

Figure 6 shows an example for the human motion imitation
in the 2D case on a different training data set.
It shows
the deterministic trajectory and 5 sample path realizations,
where all of them show high reproduction precision and
convergence to the orign.

5.4. Simulation 3: Chemical Reactor Simulation

For the last validation, we utilize simulated data from a
simpliﬁed chemical reactor (Einarsson, 1998). The closed-
loop reactor is modeled by a piecewise afﬁne system with
two states: the ﬂuid level x1 and the temperature x2. Both
states are physically positive quantities, therefore the ap-
proach in Section 4.2 is suitable. The switching between
different dynamic matrices is state dependent and occurs
at x1 = 3 and x2 = 50, which corresponds to a discrete
change of the control inputs. The training data consists of 8
trajectories of 15 steps each, which are pairwise initialized
at the 4 different regions of the dynamics and perturbed

Figure 6. 2D simulation for human motion data set with LeSSS.

Error/stable?
deterministic
stochastic

stable LDS GPDM
173/no
177/no

322/yes
n/a

SEDS
332/yes
364/no

LeSSS
162/yes
165/yes

Table 1. Comparison of the reproduction error in terms of area
between each demonstration and its corresponding reproduction
for the stochastic and deterministic case for the chemical reactor
simulation. The area is computed for each of the 8 initial points
separately and cumulated for each of the approaches. It also indi-
cates which models are stable.

with white noise with σ = 0.01 for both states.

Figure 7 shows the training data along with the reproduc-
tion using stable LDS, GPDM, SEDS and LeSSS in the de-
terministic setting. The initial points in the test case were
set close to the one in the training data. The stable LDS is
not capable to capture the varying behavior in the different
regions of the piecewise afﬁne system and therefore fails in
accuracy of the reproduction. GPDM leads again to conver-
gence outside the origin, which is undesirable. SEDS and
LeSSS are both converging as it is enforced by design. Fig-
ure 8 shows that similar to the 1D case GPDM and SEDS
fail to converge in the stochastic case while LeSSS is sta-
ble in all sample paths. Table 1 compares the methods
with regard to the reproduction precision quantitatively. It
shows that LeSSS outperforms other methods in this mea-
sure while providing the necessary guarantees regarding
convergence.

5.5. Discussion

The simulations show that LeSSS is powerful enough to
represent various nonlinear dynamics, while capturing the
probabilistic nature of the process. The incorporation of
the prior knowledge on goal convergence ensures that the
learned model is stable in probability.

The computational complexity for learning the parameters
of the model using interior-point methods, is mainly deter-
mined by the employed mapping in the ﬁrst layer θψ. The

0204060801001200100200NoconvergencetooriginOscillationbutnoconvergencekxkTrainingDataGPDMSEDSLeSSS050100150200250300050100x1x2TrainingDataDeterministicStochasticStable Stochastic Nonlinear Dynamical Systems

Figure 7. Training data (black) of chemical reactor and the deterministic simulation for stable LDS, GPDM, SEDS and LeSSS approach.

Figure 8. Training data (black) of chemical reactor and the stochastic simulation for GPDM, SEDS and LeSSS approach.

Simulation 2
Time
Simulation 3
Time

LDS
0.016s
LDS
4.54s

GPDM SEDS
8.53s
2.47s
GPDM SEDS
22.3s
11.2s

LeSSS
9.88s
LeSSS
18.2s

6. Conclusion

Table 2. Computation times for model learning and simulation for
all compared approaches.

computation times on a i5 CPU 2.30GHz, 2 Cores and 8GB
RAM are given for Simulation 2 and 3 in Table 2. Since the
GPDM, SEDS and LeSSS all solve non-convex optimiza-
tion problems, their commutation times are in the same or-
der of magnitude. The linear model has advantage here.
Regarding the scalability with more training points, the pa-
rameter ﬁtting performs similarly to other approaches re-
quiring likelihood computation since this is the major fac-
tor. However, the scalability strongly depends on the em-
ployed distribution and the mapping in the ﬁrst layer θψ.

This work only deals with system with a single equilibrium
point, but could be extended to system with more com-
plex attractor dynamics. However, further knowledge is
required because - in addition to the position of all equi-
librium points - their regions of attraction must be known.

This work proposes a framework for learning nonlinear sta-
ble stochastic dynamical systems from data. We introduce
a ﬂexible model, which builds on the state-dependent co-
efﬁcient form and derive exponential stability conditions
based on stochastic Lyapunov methods. The criteria is ap-
plicable to various probability distributions, while we focus
to investigate the application to Beta and Dirichlet distribu-
tions. Simulation results verify sufﬁcient ﬂexibility of the
model and the correct identiﬁcation of the system’s uncer-
tainty. In comparison to existing approaches it showed ad-
vantages in reproduction precision and convergence prop-
erties on human motion data and simulated data from a real
system.

Acknowledgment

The research leading to these results has received funding
from the European Research Council under the European
Union Seventh Framework Program (FP7/2007-2013) ERC
Starting Grant ”Control based on Human Models (con-
humo)” agreement number 337654. We also would like
to thank the reviewers for very constructive feedback.

0246020406080Reproductionveryimprecisex1ﬂuidlevelx2temperaturestableLDS0246Noconvergencetooriginx1ﬂuidlevelGPDM0246Reproductionveryimprecisex1ﬂuidlevelSEDS0246Precisereproductionandconvergencex1ﬂuidlevelLeSSS0246020406080Noconvergencetooriginx1ﬂuidlevelx2temperatureGPDM0246Oscillationhindersconvergencex1ﬂuidlevelSEDS0246Precisereproductionandconvergencex1ﬂuidlevelLeSSSStable Stochastic Nonlinear Dynamical Systems

References

Beckers, Thomas and Hirche, Sandra. Equilibrium distri-
butions and stability analysis of Gaussian process state
space models. In Conference on Decision and Control
(CDC), pp. 6355–6361, 2016a.

Beckers, Thomas and Hirche, Sandra. Stability of Gaus-
In European Control

sian process state space models.
Conference, pp. 2275–2281, 2016b.

Boots, Byron, Gordon, Geoffrey J, and Siddiqi, Sajid M. A
constraint generation approach to learning stable linear
dynamical systems. In Advances in Neural Information
Processing Systems (NIPS), pp. 1329–1336. Curran As-
sociates, Inc., 2008.

Calinon, Sylvain. Robot Programming by Demonstration:
A Probabilistic Approach. EPFL/CRC Press, 2009.

Chiuso, Alessandro and Pillonetto, Gianluigi. Learning
sparse dynamic linear systems using stable spline ker-
nels and exponential hyperpriors. In Advances in Neural
Information Processing Systems (NIPS), pp. 397–405.
Curran Associates, Inc., 2010.

Cimen, Tayfun. State-dependent Riccati equation (SDRE)
IFAC Proceedings Volumes, 41(2):

control: A survey.
3761–3775, 2008.

Einarsson, Valur. On Veriﬁcation of Switched Systems us-
ing Abstractions. PhD thesis, Linkoping University, Au-
tomatic Control, The Institute of Technology, 1998.

Farina, Lorenzo and Rinaldi, Sergio. Positive linear sys-
tems: Theory and applications. John Wiley&Sons, 2011.

Fox, Emily, Sudderth, Erik B., Jordan, Michael I., and
Willsky, Alan S. Nonparametric Bayesian learning
In Advances
of switching linear dynamical systems.
in Neural Information Processing Systems (NIPS), pp.
457–464. Curran Associates, Inc., 2009.

Ghahramani, Zoubin and Roweis, Sam T. Learning non-
linear dynamical systems using an EM algorithm.
In
Advances in Neural Information Processing Systems
(NIPS), pp. 431–437. MIT Press, 1999.

Horn, Roger A. and Johnson, Charles R. Matrix analysis.

Cambridge Univ. Press, Cambridge, 2013.

Ijspeert, Auke Jan, Nakanishi, Jun, and Schaal, Stefan.
Movement imitation with nonlinear dynamical systems
In International Conference on
in humanoid robots.
Robotics and Automation (ICRA). IEEE, 2002.

Khansari-Zadeh, Seyed Mohammad and Billard, Aude.
Learning stable nonlinear dynamical systems with Gaus-
sian mixture models. IEEE Transactions on Robotics, 27
(5):943–957, 2011.

Khansari-Zadeh, Seyed Mohammad and Billard, Aude.
Learning control Lyapunov function to ensure stabil-
ity of dynamical system-based robot reaching mo-
tions. Robotics and Autonomous Systems, 62(6):752–
765, 2014.

Kocijan, J., Girard, A., Banko, B., and Murray-Smith,
R. Dynamic systems identiﬁcation with Gaussian pro-
cesses. Mathematical and Computer Modelling of Dy-
namical Systems, 11(4), 411-424, 2005.

Kushner, Harold Joseph. Introduction to stochastic control.

Holt, Rinehart and Winston New York, 1971.

Ljung, Lennart. System Identiﬁcation. Prentice Hall PTR,

NJ, USA, 1998.

Paraschos, Alexandros, Daniel, Christian, Peters, Jan, and
Neumann, Gerhard. Probabilistic movement primitives.
In Advances in Neural Information Processing Systems
(NIPS), pp. 2616–2624, 2013.

Rasmussen, Carl Edward and Williams, Christopher KI.
Gaussian Processes for Machine Learning. MIT Press,
Cambridge, MA, USA, January 2006.

Wang, Jack M., Fleet, David J., and Hertzmann, Aaron.
In Advances
Gaussian process dynamical models.
in Neural Information Processing Systems (NIPS), pp.
1441–1448, 2005.

