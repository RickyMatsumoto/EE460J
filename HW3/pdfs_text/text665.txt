High-dimensional Non-Gaussian Single Index Models via Thresholded Score
Function Estimation

Zhuoran Yang 1 Krishnakumar Balasubramanian 1 Han Liu 1

Abstract

called as the indices.

We consider estimating the parametric compo-
nent of single index models in high dimensions.
Compared with existing work, we do not require
the covariate to be normally distributed. Utilizing
Stein’s Lemma, we propose estimators based on
the score function of the covariate. Moreover, to
handle score function and response variables that
are heavy-tailed, our estimators are constructed
via carefully thresholding their empirical coun-
terparts. Under a bounded fourth moment condi-
tion, we establish optimal statistical rates of con-
vergence for the proposed estimators. Extensive
numerical experiments are provided to back up
our theory.

1. Introduction

Estimators for high-dimensional parametric (linear) mod-
els have been developed and analyzed extensively in
two decades (see for example (B¨uhlmann &
the last
van de Geer, 2011; Vershynin, 2015) for comprehensive
overviews). While being a useful testbed for illustrating
conceptual phenomenon, they often suffer from a lack of
ﬂexibility in modeling real-world situations. On the other
hand, completely nonparametric models, although ﬂexi-
ble, suffer from the curse of dimensionality unless restric-
tive additive sparsity or smoothness assumptions are im-
posed (Ravikumar et al., 2009; Yuan et al., 2016). An in-
teresting compromise between the parametric and nonpara-
metric models is provided by the so-called semiparametric
index models (Horowitz, 2009). Here, the response and the
covariate are linked through a low-dimensional nonpara-
metric function that takes in as input a linear transforma-
tion of the covariate. The nonparametric component is also
called as the link function and the linear components are

1 Department of Operations Research and Financial Engineer-
ing, Princeton University, Princeton, NJ 08544, USA. Correspon-
dence to: Han Liu <hanliu@princeton.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

In this work, we focus on the simplest family of such mod-
els, the single index models (SIMs), which assume that the
response Y and the covariate X satisfy Y = f ((cid:104)X, β∗(cid:105)) +
(cid:15), where β∗ is the true signal, (cid:15) is the mean-zero random
noise, and f is a univariate link function. (see §2 for the
precise deﬁnition). They form the basis of more compli-
cated models such as Multiple Index Models (MIMs) (Di-
aconis & Shahshahani, 1984) and Deep Neural Networks
(DNNs) (LeCun et al., 2015), which are cascades of MIMs.
Moreover, we focus on the task of estimating the para-
metric (linear) component β∗ without the knowledge of
the nonparametric part f in the high-dimensional setting,
where the number of samples is much smaller than the di-
mensionality of β∗.

Estimating the parametric component without depending
on the speciﬁc form of the nonparametric part appears nat-
urally in several situations. For example, in one-bit com-
pressed sensing (Boufounos & Baraniuk, 2008) and sparse
generalized linear models (Loh & Wainwright, 2015), we
are interested in recovering the true signal vector based on
nonlinear measurements. Furthermore, in a DNN, the acti-
vation function is pre-speciﬁed and the task is to estimate
the linear components, which are used for prediction in the
test stage. Performing nonlinear least-squares in this set-
ting, leads to nonconvex optimization problems that are in-
variably sub-optimal without further assumptions. Hence,
developing estimators for the linear component that are
both statistically accurate and computationally efﬁcient for
a class of activation functions provide a compelling alter-
native. Understanding such estimators for SIMs is hence
crucial for understanding the more complicated DNNs.

Although SIMs appear to be a simple extension of the
standard linear models, most existing work in the high-
dimensional setting assume X follows a Gaussian distri-
bution for estimating β∗ without the knowledge of the non-
parametric part.
It is not clear whether those estimation
methods are still valid and optimal when X is drawn from
a more general class of distributions. To relax the Gaussian
assumption, we study the setting where the distribution of
X is non-Gaussian but known a priori.

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

1.1. Challenges of the Single Index Models

There are signiﬁcant challenges that appear when we are
dealing with estimators for SIMs. They can be summa-
rized as assumptions on either the link function or the data
distribution (for example, non-Gaussian assumption).

1. Knowledge of link function: Suppose the link func-
tion is known, for example, f (u) = u2 which corre-
sponds to the phase retrieval model (see (Jaganathan
et al., 2015) for a survey and history of this model).
Then using an M-estimator to estimate β∗ is a natural
procedure (Jaganathan et al., 2015). But computation-
ally the problem becomes nonconvex and one need to
resort to either SDP based convex relaxations that are
computationally expensive or do non-convex alternat-
ing minimization that require Gaussian assumptions
on the data for successful initialization in the high-
dimensional setting (Cai et al., 2015). Furthermore,
if the link function is changed, it might become chal-
lenging or impossible to obtain provably computable
estimators.

2. Knowledge of data distribution: Now suppose we
want to be agnostic about the link function, i.e., we
want to estimate the linear component for a general
class of link functions. Then it becomes necessary to
make assumptions about the distribution from which
the covariates are sampled from.
In particular, as-
suming the covariate has Gaussian and symmetric el-
liptical distributions respectively, (Plan & Vershynin,
2016) and (Goldstein et al., 2016) propose estimators
in the high-dimensional setting for a large class of un-
known link functions.

As mentioned previously, our estimators are based on
Stein’s Lemma for non-Gaussian distributions, which uti-
lizes the score function. Estimating with the score function
is challenging due to their heavy tails. In order to illustrate
that, consider the univariate histograms provided in Figure-
1. The dark shaded, more concentrated one corresponds to
the histogram of 10000 i.i.d. samples from Gamma dis-
tribution with scale and shape parameters set to 5 and 0.2
respectively. The transparent histogram corresponds to the
distribution of the score function of the same Gamma dis-
tribution. Note that even when the actual Gamma distribu-
tion is well concentrated, the distribution of the correspond-
ing score function is well-spread and heavy-tailed. In the
high dimensional setting, in order to estimate with the score
functions, we require certain vectors or matrices based on
the score functions to be well-concentrated in appropriate
norms. In order to achieve that, we construct robust esti-
mators via careful truncation arguments to balance the bias
(due to thresholding)-variance (of the estimator) tradeoff
and achieve the required concentration.

Figure 1: Histogram of Score Function based on 10000
independent samples from the Gamma distribution with
shape 5 and scale 0.2. The dark histogram (we recommend
the reader to zoom in to notice it) concentrated around zero
corresponds to the Gamma distribution and the transparent
histogram corresponds to the distribution of the score of the
same Gamma distribution.

1.2. Related Work

There is a signiﬁcant body of work on SIMs in the low-
dimensional setting. They are based on assumptions on
either the distribution of the covariate or the link func-
tions. Assuming a monotonic link function, (Han, 1987;
Sherman, 1993) propose the maximum rank correlation
estimator exploiting the relationship between monotonic
functions and rank-correlations. Furthermore, (Li & Duan,
1989) propose an estimator for a wide class of unknown
link functions under the assumption that the covariate fol-
lows a symmetric elliptical distribution. This assumption is
restrictive as often times the covariates are not from a sym-
metric distribution. For example, in several economic ap-
plications where the covariates are usually highly skewed
and heavy-tailed (Horowitz, 2009). A line of work for
estimation in SIMs is proposed by Ker-Chau Li which is
based on sliced inverse regression (Li, 1991) and princi-
pal Hessian directions (Li, 1992) . These estimators are
based on similar symmetry assumptions and involve com-
puting second-order (conditional and unconditional) mo-
ments which are difﬁcult to estimate in high-dimensions
without restrictive assumptions.

The success of Lasso and related linear estimators in high-
dimensions (B¨uhlmann & van de Geer, 2011), also en-
abled the exploration of high-dimensional SIMs. Although,
this is very much work in progress. As mentioned previ-
ously, (Plan & Vershynin, 2016) show that the Lasso es-
timator works for the SIMs in high dimensions when the
data is Gaussian. A more tighter albeit an asymptotic re-
sults under the same setting was proved in (Thrampoulidis
et al., 2015). Very recently (Goldstein et al., 2016) extend

-10010203040500200400600800100012001400Non-Gaussian Single Index Models via Thresholded Score Function Estimation

the results of (Li & Duan, 1989) to the high dimensional
setting but it suffers from similar problems as mentioned
in the low-dimensional setting. For the case of monotone
nonparametric component, (Yang et al., 2015) analyze a
non-convex least squares approach under the assumption
that the data is sub-Gaussian. However, the success of
their method hinges on the knowledge of the link func-
tion. Furthermore, (Jiang & Liu, 2014; Lin et al., 2015;
Zhu et al., 2006) analyze the sliced inverse regression esti-
mator in the high-dimensional setting concentrating mainly
on support recovery and consistency properties. Similar
to the low-dimensional case, the assumptions made on the
covariate distribution restrict them from several real-world
applications involving non-Gaussian or non-symmetric co-
variate, for example high-dimensional problems in eco-
nomics (Fan et al., 2011). Furthermore, several results
are established on a case-by-case basis for ﬁxed link func-
tion. Speciﬁcally (Boufounos & Baraniuk, 2008; Ai et al.,
2014) and (Davenport et al., 2014) consider 1-bit com-
pressed sensing and matrix completion respectively, where
the link is assumed to be the sign function. Also, (Wald-
spurger et al., 2015) and (Cai et al., 2015) propose and an-
alyze convex and non-convex estimators for phase retrieval
respectively, in which the link is the square function. All
the above works, except (Ai et al., 2014) make Gaussian
assumptions on the data and are specialized for the speciﬁc
link functions. The non-asymptotic result obtained in (Ai
et al., 2014) is under sub-Gaussian assumptions, but the es-
timator is not consistent. Finally, there is a line of work
focussing on estimating both the parametric and the non-
parametric component (Kalai & Sastry, 2009; Kakade et al.,
2011; Alquier & Biau, 2013; Radchenko, 2015). We do not
focus on this situation in this paper as mentioned before.

To summarize, all the above works require restrictive as-
sumption on either the data distribution or on the link func-
tion. We propose and analyze an estimator for a class of
(unknown) link functions for the case when the covariates
are drawn from a non-Gaussian distribution – under the as-
sumption that we know the distribution a priori. Note that
in several situations, one could ﬁt specialized distributions,
to real-world data that is often times skewed and heavy-
tailed, so that it provides a good generative model of the
data. Also, mixture of Gaussian distribution, with the num-
ber of components selected appropriately, approximates the
set of all square integrable distributions to arbitrary accu-
racy (see for example (McLachlan & Peel, 2004)). Fur-
thermore, since this is a density estimation problem it is
unlabeled and there is no issue of label scarcity. Hence it is
possible to get accurate estimate of the distribution in most
situations of interest. Thus our work is complementary to
the existing literature and provides an estimator for a class
of models that is not addressed in the previous works. We
conclude this section with a summary of our main contri-

butions in this paper:

• We propose estimators for the parametric component
of a sparse SIM and low-rank SIM for a class of un-
known link function under the assumption that the co-
variate distribution is non-Gaussian but known a pri-
ori.

• We show that it is possible to recover a s-sparse d-
dimensional vector and a rank-r, d1 × d2 dimensional
matrix with number of samples of the order of s log d
and r(d1 + d2) log(d1 + d2) respectively under signif-
icantly mild moment assumptions in the SIM setting.

• We provide numerical simulation results that conﬁrm

our theoretical predictions.

2. Single Index Models

In this section, we introduce the notation and deﬁne the
single index models. Throughout this work, we use [n] to
denote the set {1, . . . , n}. In addition, for a vector v ∈ Rd,
we denote by (cid:107)v(cid:107)p the (cid:96)p-norm of v for any p ≥ 1. We
use S d−1 to denote the unit sphere in Rd, which is deﬁned
as S d−1 = {v ∈ Rd : (cid:107)v(cid:107)2 = 1}. In addition, we deﬁne
the support of v ∈ Rd as supp(v) = {j ∈ [d], vj (cid:54)= 0}.
Moreover, we denote the nuclear norm, operator norm, and
Frobenius norm of a matrix A ∈ Rd1×d2 by (cid:107)·(cid:107)(cid:63), (cid:107)·(cid:107)op, and
(cid:107) · (cid:107)fro, respectively. We denote by vec(A) the vectoriza-
tion of matrix A, which is a vector in Rd1·d2. For two ma-
trices A, B ∈ Rd1×d2 we deﬁne the trace inner product as
(cid:104)A, B(cid:105) = Trace(A(cid:62)B). Note that it can be viewed as the
standard inner product between vec(A) and vec(B). In ad-
dition, for an univariate function g : R → R, we denote by
g ◦ (v) and g ◦ (A) the output of applying g to each element
of a vector v and a matrix A, respectively. Finally, for a ran-
dom variable X ∈ R with density p, we use p⊗d : Rd → R
to denote the joint density of {X1, · · · , Xd}, which are d
identical copies of X.

Now we are ready to deﬁne the statistical model. Let
f : R → R be an univariate function and β∗ be the pa-
rameter of interest, which is a structured vector or a matrix.
The single index model in general is formulated as

Y = f ((cid:104)X, β∗(cid:105)) + (cid:15),

(2.1)

where X is the covariate, Y ∈ R is the response, and
(cid:15) is the exogenous noise that is independent of X. We
assume that (cid:15) is centered and has bounded fourth moment,
i.e., Ep0((cid:15)) = 0 and E((cid:15)4) ≤ C for an absolute constant
C > 0. Note in particular that this allows for heavy-tailed
In addition, we assume that the entries
noise as well.
random variables with density p0. This
of X are i.i.d.
assumption could be further relaxed using more sophisti-
cated concentration arguments; here we focus on the i.i.d.
setting to clearly present the main message of this paper.

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

Let {(Yi, Xi)}n
i=1 be n i.i.d. observations of the SIM. Our
goal is to consistently estimate β∗ without the knowledge
of f . In particular, we focus on the case when β∗ is either
sparse or low-rank, which are deﬁned as follows.

1 , · · · , β∗

Sparse single index model: In this setting, we assume
d)(cid:62) is a sparse vector in Rd with s∗
that β∗ = (β∗
nonzero entries, such that s∗ (cid:28) n (cid:28) d. Moreover, for the
model in (2.1) to be identiﬁable, we further assume β∗ lies
on the unit sphere S d−1 as the norm of β∗ can always be
absorbed in the unknown link function f .

Low-rank single index model: In this setting, we assume
that β∗ ∈ Rd1×d2 has rank r∗ (cid:28) min{d1, d2}.
In this
scenario, X ∈ Rd1×d2 and the inner product in (2.1) is
(cid:104)X, β∗(cid:105) = Trace(X (cid:62)β∗). For model identiﬁability, we
further assume that (cid:107)β∗(cid:107)F = 1, similar to the sparse case.

3. Estimation via Score Functions

Our estimator is primarily motivated by an interesting phe-
nomenon illustrated in (Plan & Vershynin, 2016) for the
Gaussian setting. Below, we ﬁrst brieﬂy summarize the
result from (Plan & Vershynin, 2016) and then provide
our alternative justiﬁcation for the same result via Stein’s
Lemma. We mainly leverage this alternative justiﬁcation
and propose our estimators for the more general setting we
consider. Assuming for simplicity, we work in the one-
dimensional setting and are given n i.i.d. samples from the
SIM. Consider the least-squares estimator

(cid:98)βLS = argmin

β∈R

1
n

n
(cid:88)

i=1

(Yi − Xiβ)2 .

Note that the above estimator is the standard least-squares
estimator assuming a linear model (i.e., identity link func-
tion). The surprising observation from (Plan & Vershynin,
2016) is that, under the crucial assumption that X is stan-
dard Gaussian, (cid:98)βLS is a good estimator of β∗ (up to a scal-
ing) even when the data is generated from the nonlinear
SIM. The same holds true for the high-dimensional setting
when the minimization is performed in an appropriately
constrained norm-ball (for example, the (cid:96)1-ball). Hence the
theory developed for the linear setting could be leveraged to
understand the performance in the SIM setting. Below, we
give an alternative justiﬁcation for the above estimator as an
implication of Stein’s Lemma in the Gaussian case, which
is summarized as follows.
Proposition 3.1 (Gaussian Stein’s Lemma (Stein, 1972)).
Let X ∼ N (0, 1) and g : R → R be a continuos function
such that E|g(cid:48)(X)| ≤ ∞. Then we have E[g(X)X] =
E[g(cid:48)(X)].

Note that in our context for SIMs, we have E[f (cid:48)(X)] ∝ β∗
and E[f (X)X] = E[Y · X]. Now consider the following

estimator, which is based on performing least-squares on
the sample version of the above proposition:

(cid:98)βSL = argmin

β∈R

1
n

n
(cid:88)

i=1

(YiXi − β)2

Note that (cid:98)βLS and (cid:98)βSL are the same estimators assuming
X ∼ N (0, 1), as n → ∞. This observation leads to an al-
ternative interpretation of the estimator proposed by (Plan
& Vershynin, 2016) via Stein’s Lemma for Gaussian ran-
dom variables. Thus it provides an alternative justiﬁcation
for why the linear least-squares estimator should work in
the SIM setting. This observation naturally leads to lever-
aging non-Gaussian versions of Stein’s Lemma for dealing
with non-Gaussian covariates.

We now describe our estimator for the non-Gaussian setting
based on the above observation. We ﬁrst deﬁne the score
function associate to a density. Let p : Rd → R be a prob-
ability density function deﬁned on Rd. The score function
Sp : Rd → R associated to p is deﬁned as

Sp(x) = −∇x[log p(x)] = −∇xp(x)/p(x).

Note that in the above deﬁnition, the derivative is taken
with respect to x. This is different from the more traditional
deﬁnition of the score function where the density belongs
to a parametrized family and the derivative is taken with re-
spect to the parameters. In the rest of the paper to simplify
the notation, we omit the subscript x from ∇x. We also
omit the subscript p from Sp when the underlying density
p is clear from the context.

We now describe a version of Stein’s Lemma that is ap-
plicable for non-Gaussian random variables. Note from
the motivating example for the Gaussian case that while
utilizing the Stein’s Lemma for SIM estimation, assump-
tions on the function in Stein’s Lemma translate directly
to those on the link function in SIM. We now introduce a
version of Stein’s Lemma that applies to non-Gaussian ran-
dom variables and for continuously differentiable functions
from (Stein et al., 2004). A more general version of the
Stein’s Lemma that applies to a class of regular functions
is available in (Stein et al., 2004). We assume continuously
differentiable functions in the Stein’s Lemma below as they
cover a wide range of practical SIM such as generalized lin-
ear models and single-layer neural networks.

Lemma 3.2 (Non-Gaussian Stein’s Lemma (Stein et al.,
2004)). Let g : Rd → R be continuously differentiable
function and X ∈ Rd be a random vector with density
p : Rd → R, which is also continuously differentiable. Un-
der the assumption that the expectations E[g(X) · S(X)]
and E[∇g(X)] are both well-deﬁned, we have the follow-

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

ing generalized Stein’s identity

E[g(X) · S(X)] = −

g(x) · ∇p(x)dx

(cid:90)

Rd

(cid:90)

=

Rd

∇g(x) · p(x)dx = E[∇g(X)].

(3.1)

Recall that in the two single index models introduced in
§2, X in (2.1) has i.i.d. entries with density p0. To unify
both the vector and matrix settings, in the low-rank SIM,
we identify X with vec(X) ∈ Rd where d = d1 · d2. In
this case, X has density p = p⊗d
and the corresponding
score function S : Rd → Rd is given by

0

S(x) = −∇ log p(x) = −∇p(x)/p(x) = s0 ◦ (x), (3.2)

where the univariate function s0 = p(cid:48)
0/p0 is applied to each
entry of x. Thus S(X) has i.i.d. entries. In addition, by
Lemma 3.2, we have E[S(X)] = 0 by setting g to be a
constant function in (3.1). Moreover, in the context of SIMs
speciﬁed in (2.1), we have

E[Y · S(X)] = E(cid:2)f ((cid:104)X, β∗(cid:105)) · S(X)(cid:3)

= E[f (cid:48)((cid:104)X, β∗(cid:105))] · β∗,

as long as the density and the link function satisfy the con-
ditions stated in Lemma 3.2. This implies that optimization
problem

minimize
β∈Rd

(cid:8)(cid:104)β, β(cid:105) − 2E[Y · (cid:104)S(X), β(cid:105)](cid:9)

(3.3)

has solution β = µ · β∗, where µ = E[f (cid:48)((cid:104)X, β∗(cid:105))]. Hence
the above program could be used to obtain the unknown β∗
as long as µ (cid:54)= 0. Before we proceed to describe the sam-
ple version of the above program, we make the following
brief remark. The requirement µ (cid:54)= 0 rules out in partic-
ular the use of our approach for non-Gaussian phase re-
trieval (where f (u) = u2) as in that case we have µ = 0
when X is centered. But we emphasize that the same holds
true in the Gaussian and elliptical setting as well, as noted
in (Plan & Vershynin, 2016) and (Goldstein et al., 2016).
Their methods also fail to recover the true β∗ when the SIM
model corresponds to phase retrieval. We refer the reader to
§6 for a discussion on overcoming this limitation.

Finally, we use a sample version of the above program as
an estimator for the unknown β∗. In order to deal with the
high-dimensional setting, we consider a regularized version
of the above formulation. More speciﬁcally, we use the
(cid:96)1-norm and nuclear norm regularization in the vector and
matrix settings respectively. However, a major difﬁculty in
the sample setting for this procedure is that E[Y · S(X)]
and its empirical counterpart may not be close enough due
to a lack of concentration. Recall our discussion from §1.1
that even if the random variable X is light-tailed, its score-
function S(x) might be arbitrarily heavy-tailed. Further-
more, bounded-fourth moment assumption on the noise, Y

too can be heavy-tailed. Thus the naive method of using the
sample version of (3.3) to estimate β∗ leads to sub-optimal
statistical rates of convergence.

To improve concentration and obtain optimal rates of con-
vergence, we replace Y · S(X) with a transformed random
variable T (Y, X), which will be deﬁned precisely in §4 for
the sparse and low-rank cases. In particular, T (Y, X) is a
carefully truncated version of Y · S(X), introduced and an-
alyzed in (Catoni et al., 2012; Fan et al., 2016) for related
problems, that enables us to obtain well-concentrated esti-
mators. Thus our ﬁnal estimator (cid:98)β is deﬁned as the solution
to the following regularized optimization problem

minimize
β∈Rd

L(β) + λ · R(β),

(3.4)

L(β) = (cid:104)β, β(cid:105) −

(cid:104)T (Yi, Xi), β(cid:11),

2
n

n
(cid:88)

i=1

where λ > 0 is the regularization parameter which will be
speciﬁed later and R(·) is the (cid:96)1-norm in the vector case
and the nuclear norm in the matrix case.

4. Theoretical Results

In this section, we state our main results in Theorem 4.2
and Theorem 4.3,which establish the statistical rates of
convergence of the estimator deﬁned in §3. The proof
for both Theorems is presented in the supplementary ma-
terial. Before doing so, we introduce our main moment
assumption for the single index model. This assumption is
made apart from the assumptions made on the noise and
the link function in §2 and §3 respectively. Recall that
each entry of the score function deﬁned in (3.2) is equal
to s0(u) = −p(cid:48)
0(u)/p0(u). We ﬁrst state the assumption
and make a few remarks about it.

Assumption 4.1. There exists an absolute constant M >
0 such that E(Y 4) ≤ M and Ep0[s4
0(U )] ≤ M , where
random variable U ∈ R has density p0.

Consider the assumption E(Y 4) ≤ M . By Cauchy-
Schwarz inequality we have E(Y 4) ≤ 4E((cid:15)4) +
4E[f 4((cid:104)X, β∗)]. Note that we assum (cid:15) to be centered, inde-
pendent of X and has bounded fourth moment (see §2). If
the covariate X has bounded fourth moment along the di-
rection of true parameter, since f (·) is continuously differ-
entiable, f ((cid:104)X, β∗(cid:105)) has bounded fourth moment as well
if f (·) is deﬁned on a compact subset of R. Hence the
condition E(Y 4) ≤ M is relatively easy to satisfy and sig-
niﬁcantly milder than assuming that Y is bounded or has
lighter tails. Furthermore, Ep0[s4
0(U )] ≤ M is relatively
mild and satisﬁed by a wide class of random variables.
Speciﬁcally random variables that are non-symmetric and
non-Gaussian satisfy this property thereby allowing our ap-
proach to work with covariates not previously possible.

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

We believe it is highly non-trivial to weaken this condition
without losing signiﬁcantly in the rates of convergence that
we discuss below.

4.1. Sparse Single Index Model

Under the above assumptions, we ﬁrst state our theorem
on the sparse SIM. As discussed in §3, Y · S(X) can by
heavy-tailed and hence we apply truncation to achieve con-
centration. Denote the j-th entry of the score function S
in (3.2) as Sj : Rd → R, j ∈ [d]. We deﬁne the truncated
response and score function as

(cid:101)Y = sign(Y ) · (|Y | ∧ τ ),
(cid:101)Sj(x) = sign[Sj(x)] · (cid:2)|Sj(x)| ∧ τ (cid:3),

(4.1)

where τ > 0 is a predetermined threshold value. We deﬁne
(cid:101)Yi similarly for all Yi, i ∈ [n]. Then we deﬁne the estima-
tor (cid:98)β as the solution to the optimization problem in (3.4)
with T (Yi, Xi) = (cid:101)Yi · (cid:101)S(Xi) and R(β) = (cid:107)β(cid:107)1. Here we
apply elementwise truncation in T to ensure the sample av-
erage of T converges to E[Y · S(X)] in the (cid:96)∞-norm for an
appropriately chosen τ . Note that the (cid:96)∞-norm is the dual
norm of the (cid:96)1-norm. Such a convergence requirement in
the dual norm is standard in the analysis of regularized M -
estimators (Negahban et al., 2012) to achieve optimal rates.
The following theorem characterizes the convergence rates
of (cid:98)β.
Theorem 4.2 (Signal recovery for the sparse single index
model). For the sparse SIM deﬁned in §2, we assume that
β∗ ∈ Rd has s∗ nonzero entries. Under Assumption 4.1,
we let τ = 2(M · log d/n)1/4 in (4.1) and set the regu-
larization parameter λ in (3.4) as C(cid:112)M · log d/n, where
C > 0 is an absolute constant. Then with probability at
least 1−d−2, the (cid:96)1-regularized estimator (cid:98)β deﬁned in (3.4)
satisﬁes

(cid:107) (cid:98)β − µβ∗(cid:107)2 ≤

s∗ · λ, (cid:107) (cid:98)β − µβ∗(cid:107)1 ≤ 4s∗ · λ.

√

From this theorem, the (cid:96)1- and (cid:96)2-convergence rates of (cid:98)β
are (cid:107) (cid:98)β − µβ∗(cid:107)1 = O(s∗(cid:112)log d/n) and (cid:107) (cid:98)β − µβ∗(cid:107)2 =
O((cid:112)s∗ log d/n), respectively. These rates match the con-
vergence rates of sparse generalized linear models (Loh
& Wainwright, 2015) and sparse single index models with
Gaussian and symmetric elliptical covariates (Plan & Ver-
shynin, 2016; Goldstein et al., 2016) which are known to
be minimax-optimal for this problem via matching lower
bounds.

4.2. Low-rank Single Index Model

We next state our theorem for the low-rank SIM. In this
case, we apply the nuclear norm regularization to promote
low-rankness. Note that by deﬁnition, T is matrix-valued.

Since the dual norm of the nuclear norm is the operator
norm, we need the sample average of T to converge to
E[Y ·S(X)] in the operator norm rapidly to achieve optimal
rates of convergence. To achieve such a goal, we leverage
the truncation argument from (Catoni et al., 2012; Minsker,
2016) to construct T (Y, X).
Let φ : R → R be a non-decreasing function such that

− log(1−x+x2/2) ≤ φ(x) ≤ log(1+x+x2/2), ∀x ∈ R.

Based on φ, we deﬁne a linear mapping ψ : Rd1×d2 →
Rd1×d2 as follows. For any A ∈ Rd1×d2, let

and let ΥΛΥ(cid:62) be the eigenvalue decomposition of (cid:101)A. In
addition, let B = Υ[φ ◦ (Λ)]Υ(cid:62), where ψ is applied ele-
mentwisely on Λ. Then we write B in block from as

(cid:101)A =

(cid:21)

(cid:20) 0 A
A(cid:62) 0

B =

(cid:21)

(cid:20)B11 B12
B21 B22

and deﬁne ψ(A) = B12. Finally, we deﬁne T (Y, X) =
1/κ · ψ(cid:2)κ · Y · S(X)(cid:3), where κ > 0 will be speciﬁed later.
Therefore, our ﬁnal estimator (cid:98)β ∈ Rd1×d2 is deﬁned as the
solution to the optimization problem in (3.4) with R(β) =
(cid:107)β(cid:107)(cid:63). We note here the minimization in (3.4) is taken over
Rd1×d2 . The following theorem quantiﬁes the convergence
rates of the proposed estimator.

Theorem 4.3 (Signal recovery for the low-rank single in-
dex model). For the low-rank single index model deﬁned
in §2, we assume that rank(β∗) = r∗. Under Assumption
4.1, we let

κ =

2(cid:112)n · log(d1 + d2)
(cid:112)(d1 + d2)M

in T (Y, X). Moreover, the regularization parameter λ in
(3.4) is set to C(cid:112)M · (d1 + d2) · log(d1 + d2)/n, where
C > 0 is an absolute constant. Then with probability at
least 1 − (d1 + d2)−2, the nuclear norm regularized estima-
tor (cid:98)β satisﬁes

(cid:107) (cid:98)β − µβ∗(cid:107)fro ≤ 3

r∗ · λ, (cid:107) (cid:98)β − µβ∗(cid:107)(cid:63) ≤ 12r∗ · λ.

√

this

(cid:107) (cid:98)β − µβ∗(cid:107)fro

theorem, we

=
By
have
O((cid:112)r∗(d1 + d2) · log(d1 + d2)/n) and (cid:107) (cid:98)β − µβ∗(cid:107)(cid:63) =
O(r∗ · (cid:112)(d1 + d2) · log(d1 + d2)/n). Note that the rate
obtained is minimax-optimal up to a logarithmic factor.
it matches the rates for low-rank single
Furthermore,
index models with Gaussian and symmetric elliptical
distributions up to a logarithmic factor (Plan & Vershynin,
2016; Goldstein et al., 2016).

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

with non-Gaussian data. Our current approach requires
that µ (cid:54)= 0 which is not applicable. The main reason this
happens is we use a ﬁrst-order version of Stein’s Lemma.
Such a problem could overcome by second-order Stein’s
Lemma (Janzamin et al., 2014). Obtaining rate-optimal
estimators based on second-order score functions require
addressing several challenges. Concentrating on phase re-
trieval (and sparse phase retrieval) we plan to report our
results for the above problem in the near future.

5. Numerical Experiments

√

We assess the ﬁnite sample performance of the proposed
estimators on simulated data. Throughout this section, we
let (cid:15) ∼ N (0, 1) and set the link function in (2.1) as one of
2u + 4 exp(−2u2),
f1(u) = 3u + 10 sin(u) and f2(u) =
which are plotted in Figure 2. We set p0 to be one of
(i) Gamma distribution with shape parameter 5 and scale
parameter 1, (ii) Student’s t-distribution with 5 degrees of
freedom, and (iii) Rayleigh distribution with scale parame-
ter 2. To measure the estimation accuracy, we use the co-
sine distance

cos θ( (cid:98)β, β∗) = 1 − (cid:107) (cid:98)β(cid:107)−1

• |(cid:104) (cid:98)β, β∗(cid:105)|,

where • stands for the Euclidean norm in the vector case
and the Frobenius norm when β∗ is a matrix. Here we re-
port the cosine distance rather than (cid:107) (cid:98)β − µβ∗(cid:107)• to com-
pare the performances for X having different distributions,
where µ may have different values.

√

For the vector case, we ﬁx d = 2000, s∗ = 5 and vary
n. The support of β∗ is chosen uniformly random among
all subsets of {1, . . . , d}. For each j ∈ supp(β∗), we set
β∗
s∗ · γj, where each γj is an i.i.d. Rademacher
j = 1/
random variable. In addition, the regularization parameter
λ is set to 4(cid:112)log d/n. We plot the cosine distance against
the signal strength (cid:112)s∗ log d/n in Figure 4-(a) and (b) for
f1 and f2 respectively, based on 200 independent trials for
each n. As shown in this ﬁgure, the estimation error grows
sublinearly as a function of the signal strength.

√

As for the matrix case, we ﬁx d1 = d2 = 20, r∗ = 3 and let
n vary. The signal parameter β∗ is equal to U SV (cid:62), where
U, V ∈ Rd×d are random orthogonal matrices and S is a
diagonal matrix with r∗ nonzero entries. Moreover, we set
r∗, which implies
the nonzero diagonal entries of S as 1/
(cid:107)β∗(cid:107)fro = 1. We set the regularization parameter as λ =
2(cid:112)(d1 + d2) log(d1 + d2)/n. Furthermore, we use the
proximal gradient descent algorithm (with the learning rate
ﬁxed to 0.05) to solve the nuclear norm regularization prob-
lem in (3.4). To present the result, we plot the cosine distant
against the signal strength (cid:112)r∗(d1 + d2) log(d1 + d2)/n
in Figure 4-(b) based on 200 independent trials. As shown
in this ﬁgure, the error is bounded by a linear function of
the signal strength, which corroborates Theorem 4.3.

6. Conclusion

In this paper, we consider SIMs in the high-dimensional
non-Gaussian setting and proposed estimators based on
Stein’s Lemma for a wider class of unknown link func-
tions and covariate distributions. We consider both sparse
and low-rank models and propose minimax rate-optimal
estimators under fairly mild assumptions. An interesting
avenue of future work is the problem of phase retrieval

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

Figure 2: Plot of the link functions f1(u) = 3u + 10 · sin(u) (left) and f2(u) =
are nonlinear and not monotone.

√

2u + 4 exp(−2u2) (right). Both functions

Figure 3: Cosine distances between the true parameter and the estimated parameter in the sparse SIM with the link function
in 2.1 set to f1 (left) and f2 (right). Here we set d=2000. s∗ = 5 and vary n.

Figure 4: Cosine distances between the true parameter and the estimated parameter in the low-rank SIM with link function
in 2.1 set to f1 (left) and f2 (right). Here we set d1 = d2 = 20. r∗ = 3 and vary n.

u-10-50510f(u)=3"u+10sin(u)-40-30-20-10010203040u-10-50510f(u)=p2a+4exp(!2a2)-15-10-5051015ps$logd=n0.10.20.30.40.5cos3(b-;-$)00.050.10.150.20.250.3Gamma(5;2)t(5)Rayleigh(2)ps$logd=n0.10.20.30.40.5cos3(b-;-$)00.010.020.030.040.050.060.070.080.090.1Gamma(5;2)t(5)Rayleigh(2)pr$(d1+d2)log(d1+d2)=n0.10.20.30.40.50.60.7cos3(b-;-$)00.10.20.30.40.50.6Gamma(5;2)t(5)Rayleigh(2)pr$(d1+d2)log(d1+d2)=n0.10.20.30.40.50.60.7cos3(b-;-$)00.10.20.30.40.50.6Gamma(5;2)t(5)Rayleigh(2)Non-Gaussian Single Index Models via Thresholded Score Function Estimation

References

Ai, Albert, Lapanowski, Alex, Plan, Yaniv, and Vershynin,
Roman. One-bit compressed sensing with non-gaussian
measurements. Linear Algebra and its Applications,
441:222–239, 2014.

Alquier, Pierre and Biau, G´erard. Sparse single-index
model. The Journal of Machine Learning Research, 14
(1):243–280, 2013.

Boucheron, St´ephane, Lugosi, G´abor, and Massart, Pascal.
Concentration inequalities: A nonasymptotic theory of
independence. Oxford university press, 2013.

Boufounos, Petros T and Baraniuk, Richard G. 1-bit com-
pressive sensing. In Information Sciences and Systems,
2008. CISS 2008. 42nd Annual Conference on, pp. 16–
21. IEEE, 2008.

B¨uhlmann, Peter and van de Geer, Sara. Statistics for high-
dimensional data: methods, theory and applications.
Springer Science & Business Media, 2011.

Cai, T Tony, Li, Xiaodong, and Ma, Zongming. Op-
timal rates of convergence for noisy sparse phase re-
trieval via thresholded wirtinger ﬂow. arXiv preprint
arXiv:1506.03382, 2015.

Catoni, Olivier et al. Challenging the empirical mean
and empirical variance: a deviation study. Annales de
l’Institut Henri Poincar´e, Probabilit´es et Statistiques, 48
(4):1148–1185, 2012.

Davenport, Mark A, Plan, Yaniv, van den Berg, Ewout, and
Wootters, Mary. 1-bit matrix completion. Information
and Inference, 3(3):189–223, 2014.

Diaconis, P. and Shahshahani, M. On nonlinear functions
of linear combinations. SIAM Journal on Scientiﬁc and
Statistical Computing, 5(1):175–191, 1984.

Fan, J., Lv, J., and Qi, L. Sparse high-dimensional models
in economics. Annual review of economics, 3(1):291–
317, 2011.

Fan, Jianqing, Wang, Weichen, and Zhu, Ziwei. Ro-
arXiv preprint

low-rank matrix recovery.

bust
arXiv:1603.08315, 2016.

Goldstein, Larry, Minsker, Stanislav, and Wei, Xiaohan.
Structured signal recovery from non-linear and heavy-
tailed measurements. arXiv preprint arXiv:1609.01025,
2016.

Horowitz, Joel L.

Semiparametric and nonparametric

methods in econometrics, volume 12. Springer, 2009.

Jaganathan, Kishore, Eldar, Yonina C, and Hassibi, Babak.
Phase retrieval: An overview of recent developments.
arXiv preprint arXiv:1510.07713, 2015.

Janzamin, Majid, Sedghi, Hanie, and Anandkumar, An-
ima. Score function features for discriminative learn-
arXiv preprint
ing: Matrix and tensor framework.
arXiv:1412.2863, 2014.

Jiang, B. and Liu, J. S. Variable selection for general in-
dex models via sliced inverse regression. The Annals of
Statistics, 42(5):1751–1786, 2014.

Kakade, Sham M, Kanade, Varun, Shamir, Ohad, and
Kalai, Adam. Efﬁcient learning of generalized linear
and single index models with isotonic regression. In Ad-
vances in Neural Information Processing Systems, pp.
927–935, 2011.

Kalai, Adam Tauman and Sastry, Ravi. The isotron algo-
rithm: High-dimensional isotonic regression. In Confer-
ence on Learning Theory, 2009.

LeCun, Yann, Bengio, Yoshua, and Hinton, Geoffrey. Deep

learning. Nature, 521(7553):436–444, 2015.

Li, Ker-Chau. Sliced inverse regression for dimension re-
duction. Journal of the American Statistical Association,
86(414):316–327, 1991.

Li, Ker-Chau. On principal Hessian directions for data vi-
sualization and dimension reduction: Another applica-
tion of Stein’s lemma. Journal of the American Statisti-
cal Association, 87(420):1025–1039, 1992.

Li, Ker-Chau and Duan, Naihua. Regression analysis un-
der link violation. The Annals of Statistics, 17(3):1009–
1052, 1989.

Lin, Q., Zhao, Z., and Liu, J. S. On consistency and sparsity
for sliced inverse regression in high dimensions. arXiv
preprint arXiv:1507.03895, 2015.

Loh, Po-Ling and Wainwright, Martin J. Regularized m-
estimators with nonconvexity: Statistical and algorith-
mic theory for local optima. Journal of Machine Learn-
ing Research, 16:559–616, 2015.

McLachlan, Geoffrey and Peel, David. Finite mixture mod-

els. John Wiley & Sons, 2004.

Han, Aaron K. Non-parametric analysis of a generalized
regression model: the maximum rank correlation estima-
tor. Journal of Econometrics, 35(2-3):303–316, 1987.

Minsker, Stanislav. Sub-gaussian estimators of the mean
arXiv

of a random matrix with heavy-tailed entries.
preprint arXiv:1605.07129, 2016.

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

Zhu, Lixing, Miao, Baiqi, and Peng, Heng. On sliced in-
verse regression with high-dimensional covariates. Jour-
nal of the American Statistical Association, 101(474):
630–643, 2006.

Negahban, Sahand N., Ravikumar, Pradeep, Wainwright,
Martin J., and Yu, Bin. A uniﬁed framework for high-
dimensional analysis of M -estimators with decompos-
able regularizers. Statistical Science, 27(4):538–557, 11
2012.

Plan, Yaniv and Vershynin, Roman. The generalized lasso
with non-linear observations. IEEE Transactions on in-
formation theory, 62(3):1528–1537, 2016.

Radchenko, Peter. High dimensional single index models.
Journal of Multivariate Analysis, 139:266–282, 2015.

Ravikumar, Pradeep, Lafferty, John, Liu, Han, and Wasser-
man, Larry. Sparse additive models. Journal of the Royal
Statistical Society: Series B (Statistical Methodology),
71(5):1009–1030, 2009.

Sherman, Robert P. The limiting distribution of the maxi-
mum rank correlation estimator. Econometrica: Journal
of the Econometric Society, 61(1):123–137, 1993.

Stein, C. A bound for the error in the normal approxima-
tion to the distribution of a sum of dependent random
variables. In Proceedings of the Sixth Berkeley Sympo-
sium on Mathematical Statistics and Probability, Volume
2: Probability Theory. The Regents of the University of
California, 1972.

Stein, Charles, Diaconis, Persi, Holmes, Susan, Reinert,
Gesine, et al. Use of exchangeable pairs in the analysis of
simulations. In Stein’s Method. Institute of Mathematical
Statistics, 2004.

Thrampoulidis, Christos, Abbasi, Ehsan, and Hassibi,
Babak. Lasso with non-linear measurements is equiva-
lent to one with linear measurements. Advances in Neu-
ral Information Processing Systems, 2015.

Vershynin, Roman. Estimation in high dimensions: a ge-
ometric perspective. In Sampling theory, a renaissance,
pp. 3–66. Springer, 2015.

Waldspurger, Ir`ene, dAspremont, Alexandre, and Mallat,
St´ephane. Phase recovery, maxcut and complex semidef-
inite programming. Mathematical Programming, 149(1-
2):47–81, 2015.

Yang, Zhuoran, Wang, Zhaoran, Liu, Han, Eldar, Yonina C,
and Zhang, Tong. Sparse nonlinear regression: Param-
eter estimation and asymptotic inference. International
Conference on Machine Learning, 2015.

Yuan, Ming, Zhou, Ding-Xuan, et al. Minimax optimal
rates of estimation in high dimensional additive models.
The Annals of Statistics, 44(6):2564–2593, 2016.

