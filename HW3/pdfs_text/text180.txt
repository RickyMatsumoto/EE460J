Being Robust (in High Dimensions) Can Be Practical

Ilias Diakonikolas * 1 Gautam Kamath * 2 Daniel M. Kane * 3 Jerry Li * 2 Ankur Moitra * 2 Alistair Stewart * 1

Abstract
Robust estimation is much more challenging in
high dimensions than it is in one dimension:
Most techniques either lead to intractable opti-
mization problems or estimators that can toler-
ate only a tiny fraction of errors. Recent work
in theoretical computer science has shown that,
in appropriate distributional models, it is possi-
ble to robustly estimate the mean and covariance
with polynomial time algorithms that can tolerate
a constant fraction of corruptions, independent of
the dimension. However, the sample and time
complexity of these algorithms is prohibitively
large for high-dimensional applications. In this
work, we address both of these issues by estab-
lishing sample complexity bounds that are opti-
mal, up to logarithmic factors, as well as giving
various reﬁnements that allow the algorithms to
tolerate a much larger fraction of corruptions. Fi-
nally, we show on both synthetic and real data
that our algorithms have state-of-the-art perfor-
mance and suddenly make high-dimensional ro-
bust estimation a realistic possibility.

1. Introduction

Robust statistics was founded in the seminal works of
(Tukey, 1960) and (Huber, 1964). The overarching motto
is that any model (especially a parametric one) is only ap-
proximately valid, and that any estimator designed for a
particular distribution that is to be used in practice must
also be stable in the presence of model misspeciﬁcation.
The standard setup is to assume that the samples we are

*Equal contribution

1University of Southern California,
Los Angeles, California, USA 2Massachusetts Institute of
Technology, Cambridge, Massachusetts, USA 3University
of California, San Diego, La Jolla, California, USA. Cor-
Ilias Diakonikolas <diakonik@usc.edu>,
respondence to:
Gautam Kamath <g@csail.mit.edu>, Daniel M. Kane
<dakane@cs.ucsd.edu>,
Li <jerryzli@mit.edu>,
Jerry
Ankur Moitra <moitra@mit.edu>, Alistair Stewart <alis-
tais@usc.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

given come from a nice distribution, but that an adversary
has the power to arbitrarily corrupt a constant fraction of
the observed data. After several decades of work, the ro-
bust statistics community has discovered a myriad of esti-
mators that are provably robust. An important feature of
this line of work is that it can tolerate a constant fraction of
corruptions independent of the dimension and that there are
estimators for both the location (e.g., the mean) and scale
(e.g., the covariance). See (Huber & Ronchetti, 2009) and
(Hampel et al., 1986) for further background.

It turns out that there are vast gaps in our understanding of
robustness, when computational considerations are taken
In one dimension, robustness and compu-
into account.
tational efﬁciency are in perfect harmony. The empirical
mean and empirical variance are not robust, because a sin-
gle corruption can arbitrarily bias these estimates, but alter-
natives such as the median and the interquartile range are
straightforward to compute and are provably robust.

But in high dimensions, there is a striking tension between
robustness and computational efﬁciency. Let us consider
estimators for location. The Tukey median (Tukey, 1960)
is a natural generalization of the one-dimensional median
to high-dimensions. It is known that it behaves well (i.e.,
it needs few samples) when estimating the mean for vari-
ous symmetric distributions (Donoho & Gasko, 1992; Chen
et al., 2016). However, it is hard to compute in gen-
eral (Johnson & Preparata, 1978; Amaldi & Kann, 1995)
and the many heuristics for computing it degrade badly
in the quality of their approximation as the dimension
scales (Clarkson et al., 1993; Chan, 2004; Miller & Sheehy,
2010). The same issues plague estimators for scale. The
minimum volume ellipsoid (Rousseeuw, 1985) is a natural
generalization of the one-dimensional interquartile range
and is provably robust in high-dimensions, but is also hard
to compute. And once again, heuristics for computing
it (Van Aelst & Rousseeuw, 2009; Rousseeuw & Struyf,
1998) work poorly in high dimensions.

The fact that robustness in high dimensions seems to come

Collectively,

the authors were supported by NSF CCF-
1652862, CCF-1551875, CCF-1617730, CCF-1650733, CCF-
1553288, CCF-1453261, ONR N00014-12-1-0999, three Sloan
Research Fellowships, two Google Faculty Research Awards, an
NSF fellowship, the MIT NEC Corporation, and a USC startup
grant.

Being Robust (in High Dimensions) Can Be Practical

at such a steep price has long been a point of consterna-
tion within robust statistics. In a 1997 retrospective on the
development of robust statistics, Huber laments: “It is one
thing to design a theoretical algorithm whose purpose is to
prove [large fractions of corruptions can be tolerated] and
quite another thing to design a practical version that can
be used not merely on small, but also on medium sized re-
gression problems, with a 2000 by 50 matrix or so. This
last requirement would seem to exclude all of the recently
proposed [techniques].”

The goal of this paper is to answer Huber’s call to ac-
tion and design estimators for both the mean and covari-
ance that are highly practical, provably robust, and work
in high-dimensions. Such estimators make the promise of
robust statistics – estimators that work in high-dimensions
and limit the error induced by outliers – much closer to a
reality.

First, we make some remarks to dispel some common mis-
conceptions. There has been a considerable amount of re-
cent work on robust principal component analysis, much
of it making use of semideﬁnite programming. Some of
these works can tolerate a constant fraction of corruptions
(Cand`es et al., 2011), however require that the locations of
the corruptions are evenly spread throughout the dataset so
that no individual sample is entirely corrupted. In contrast,
the usual models in robust statistics are quite rigid in what
they require and they do this for good reason. A common
scenario that is used to motivate robust statistical methods
is if two studies are mixed together, and one subpopulation
does not ﬁt the model. Then one wants estimators that work
without assuming anything at all about these outliers.

There have also been semideﬁnite programming methods
proposed for robust principal component analysis with out-
liers (Xu et al., 2010). These methods assume that the un-
corrupted matrix is rank r and that the fraction of outliers is
at most 1/r, which again degrades badly as the rank of the
matrix increases. Moreover, any method that uses semidef-
inite programming will have difﬁculty scaling to the sizes
of the problems we consider here. For sake of compari-
son – even with state-of-the-art interior point methods – it
is not currently feasible to solve the types of semideﬁnite
programs that have been proposed when the matrices have
dimension larger than a hundred.

1.1. Robustness in a Generative Model

Recent works in theoretical computer science have sought
to circumvent the usual difﬁculties of designing efﬁcient
and robust algorithms by instead working in a generative
model. The starting point for our paper is the work of Di-
akonikolas et al. (2016a) who gave an efﬁcient algorithm
for the problem of agnostically learning a Gaussian: Given
a polynomial number of samples from a high-dimensional

Gaussian N (µ, Σ), where an adversary has arbitrarily cor-
rupted an ε-fraction, ﬁnd a set of parameters N (cid:48)((cid:98)µ, (cid:98)Σ) that
satisfy dT V (N , N (cid:48)) ≤ (cid:101)O(ε)1.

Total variation distance is the natural metric to use to mea-
sure closeness of the parameters, since a (1 − ε)-fraction of
the observed samples came from a Gaussian. (Diakoniko-
las et al., 2016a) gave an algorithm for the above prob-
lem (note that the guarantees are dimension independent),
whose running time and sample complexity are polynomial
in the dimension d and 1/ε. (Lai et al., 2016) independently
gave an algorithm for the unknown mean case that achieves
dT V (N , N (cid:48)) ≤ (cid:101)O(ε
log d), and in the unknown covari-
ance case achieves guarantees in a weaker metric that is not
afﬁne invariant. A crucial feature is that both algorithms
work even when the moments of the underlying distribu-
tion satisfy certain conditions, and thus are not necessar-
ily brittle to the modeling assumption that the inliers come
from a Gaussian distribution.

√

A more conceptual way to view such work is as a proof-
of-concept that the Tukey median and minimum volume
ellipsoid can be computed efﬁciently in a natural family of
distributional models. This follows because not only would
these be good estimates for the mean and covariance in the
above model, but in fact any estimates that are good must
also be close to them. Thus, these works ﬁt into the emerg-
ing research direction of circumventing worst-case lower
bounds by going beyond worst-case analysis.

Since the dissemination of the aforementioned works (Di-
akonikolas et al., 2016a; Lai et al., 2016), there has been a
ﬂurry of research activity on computationally efﬁcient ro-
bust estimation in a variety of high-dimensional settings,
including studying graphical models (Diakonikolas et al.,
2016b), understanding the computation-robustness tradeoff
for statistical query algorithms (Diakonikolas et al., 2016c),
tolerating much more noise by allowing the algorithm to
output a list of candidate hypotheses (Charikar et al., 2017),
and developing robust algorithms under sparsity assump-
tions (Li, 2017; Du et al., 2017), and more (Diakonikolas
et al., 2017; Steinhardt et al., 2017).

1.2. Our Results

Our goal in this work is to show that high-dimensional ro-
bust estimation can be highly practical. However, there
are two major obstacles to achieving this. First, the sam-
ple complexity and running time of the algorithms in (Di-
akonikolas et al., 2016a) is prohibitively large for high-
dimensional applications. We just would not be able to
store as many samples as we would need, in order to com-

1We use the notation ˜O(·) to hide factors which are polylog-
arithmic in the argument – in particular, we note that this bound
does not depend on the dimension.

pute accurate estimates, in high-dimensional applications.

are able to scale to hundreds of dimensions.

Being Robust (in High Dimensions) Can Be Practical

Our ﬁrst main contribution is to show nearly-tight bounds
on the sample complexity of the ﬁltering-based algorithm
of (Diakonikolas et al., 2016a). Roughly speaking, we ac-
complish this with a new deﬁnition of the good set which
straightforwardly plugs into the existing analysis, show-
ing that one can estimate the mean with (cid:101)O(d/ε2) samples
(when the covariance is known) and the covariance with
(cid:101)O(d2/ε2) samples. Both of these bounds are information-
theoretically optimal, up to logarithmic factors.

Our second main contribution is to vastly improve the frac-
tion of adversarial corruptions that can be tolerated in appli-
cations. The fraction of errors that the algorithms of (Di-
akonikolas et al., 2016a) can tolerate is indeed a constant
that is independent of the dimension, but it is very small
both in theory and in practice – a naive implementation of
the algorithm did not remove any outliers in many realistic
scenarios. We avoid this by giving new ways to empiri-
cally tune the threshold for where to remove points from
the sample set.

Finally, we show that the same bounds on the error guar-
antee continue to work even when the underlying distribu-
tion is sub-Gaussian. This theoretically conﬁrms that the
robustness guarantees of such algorithms are in fact not
overly brittle to the distributional assumptions. In fact, the
ﬁltering algorithm of (Diakonikolas et al., 2016a) is easily
shown to be robust under much weaker distributional as-
sumptions, while retaining near-optimal sample and error
guarantees. As an example, we show that it yields a near
sample-optimal efﬁcient estimator for robustly estimating
the mean of a distribution, under the assumption that its
covariance is bounded. Even in this regime, the ﬁltering
algorithm guarantees optimal error, up to a constant fac-
tor. Furthermore we empirically corroborate this ﬁnding by
showing that the algorithm works well on real world data,
as we describe below.

Now we come to the task of testing out our algorithms. To
the best of our knowledge, there have been no experimental
evaluations of the performance of the myriad of approaches
to robust estimation. It remains mostly a mystery which
ones perform well in high-dimensions, and which do not.
To test out our algorithms, we design a synthetic experi-
ment where a (1 − ε)-fraction of the samples come from
a Gaussian and the rest are noise and sampled from an-
other distribution (in many cases, Bernoulli). This gives us
a baseline to compare how well various algorithms recover
µ and Σ, and how their performance degrades based on
the dimension. Our plots show a predictable and yet strik-
ing phenomenon: All earlier approaches have error rates
that scale polynomially with the dimension and ours is a
constant that is almost indistinguishable from the error that
comes from sample noise alone. Moreover, our algorithms

But are algorithms for agnostically learning a Gaussian un-
duly sensitive to the distributional assumptions they make?
We are able to give an intriguing visual demonstration of
our techniques on real data. The famous study of (Novem-
bre et al., 2008) showed that performing principal compo-
nent analysis on a matrix of genetic data recovers a map of
Europe. More precisely, the top two singular vectors deﬁne
a projection into the plane and when the groups of individ-
uals are color-coded with where they are from, we recover
familiar country boundaries that corresponds to the map of
Europe. The conclusion from their study was that genes
mirror geography. Given that one of the most important
applications of robust estimation ought to be in exploratory
data analysis, we ask: To what extent can we recover the
map of Europe in the presence of noise? We show that
when a small number of corrupted samples are added to
the dataset, the picture becomes entirely distorted (and this
continues to hold even for many other methods that have
been proposed). In contrast, when we run our algorithm,
we are able to once again recover the map of Europe. Thus,
even when some fraction of the data has been corrupted
(e.g., medical studies were pooled together even though the
subpopulations studied were different), it is still possible to
perform principal component analysis and recover qualita-
tively similar conclusions as if there were no noise at all!

2. Formal Framework

Notation. For a vector v, we will let (cid:107)v(cid:107)2 denote its Eu-
clidean norm. If M is a matrix, we will let (cid:107)M (cid:107)2 denote its
spectral norm and (cid:107)M (cid:107)F denote its Frobenius norm. We
will write X ∈u S to denote that X is drawn from the
empirical distribution deﬁned by S.

Robust Estimation. We consider the following powerful
model of robust estimation that generalizes many other ex-
isting models, including Huber’s contamination model:
Deﬁnition 2.1. Given ε > 0 and a distribution family D,
the adversary operates as follows: The algorithm speciﬁes
some number of samples m. The adversary generates m
samples X1, X2, . . . , Xm from some (unknown) D ∈ D.
It then draws m(cid:48) from an appropriate distribution. This
distribution is allowed to depend on X1, X2, . . . , Xm, but
when marginalized over the m samples satisﬁes m(cid:48) ∼
Bin(ε, m). The adversary is allowed to inspect the sam-
ples, removes m(cid:48) of them, and replaces them with arbitrary
points. The set of m points is then given to the algorithm.

In summary, the adversary is allowed to inspect the samples
before corrupting them, both by adding corrupted points
and deleting uncorrupted points.
In contrast, in Huber’s
model the adversary is oblivious to the samples and is only
allowed to add corrupted points.

Being Robust (in High Dimensions) Can Be Practical

We remark that there are no computational restrictions on
the adversary. The goal is to return the parameters of a dis-
tribution (cid:98)D in D that are close to the true parameters in an
appropriate metric. For the case of the mean, our metric
will be the Euclidean distance. For the covariance, we will
use the Mahalanobis distance, i.e., (cid:107)Σ−1/2 (cid:98)ΣΣ−1/2 − I(cid:107)F .
This is a strong afﬁne invariant distance that implies corre-
sponding bounds in total variation distance.

We will use the following terminology:

Deﬁnition 2.2. We say that a set of samples is ε-corrupted
if it is generated by the process described in Deﬁnition 2.1.

3. Nearly Sample-Optimal Efﬁcient Robust

Learning

In this section, we present near sample-optimal efﬁcient ro-
bust estimators for the mean and the covariance of high-
dimensional distributions under various structural assump-
tions of varying strength. Our estimators rely on the ﬁlter-
ing technique introduced in (Diakonikolas et al., 2016a).

This paper gave two algorithmic techniques: the ﬁrst one
was a spectral technique to iteratively remove outliers from
the dataset (ﬁltering), and the second one was a soft-outlier
removal method relying on convex programming. The ﬁl-
tering technique seemed amenable to practical implemen-
tation (as it only uses simple eigenvalue computations),
but the corresponding sample complexity bounds given in
(Diakonikolas et al., 2016a) are polynomially worse than
the information-theoretic minimum. On the other hand,
the convex programming technique of Diakonikolas et al.
(2016a) achieved better sample complexity bounds (e.g.,
near sample-optimal for robust mean estimation), but re-
lied on the ellipsoid method, which seemed to preclude a
practically efﬁcient implementation.

In this work, we achieve the best of both worlds: we give a
better analysis of the ﬁlter, giving sample-optimal bounds
(up to logarithmic factors) for both the mean and the co-
variance. Moreover, we show that the ﬁltering technique
easily extends to much weaker distributional assumptions
(e.g., under bounded second moments). Roughly speaking,
the ﬁltering technique follows a general iterative recipe: (1)
via spectral methods, ﬁnd some univariate test which is vi-
olated by the corrupted points, (2) ﬁnd some concrete tail
bound violated by the corrupted points, and (3) discard all
points which violate this tail bound.

We start with sub-gaussian distributions. Recall that if P
is sub-gaussian on Rd with mean vector µ and parame-
ter ν > 0, then for any unit vector v ∈ Rd we have that
PrX∼P [|v · (X − µ)| ≥ t] ≤ exp(−t2/2ν).

Theorem 3.1. Let G be a sub-gaussian distribution on Rd
with parameter ν = Θ(1), mean µG, covariance matrix

I, and ε > 0. Let S be an ε-corrupted set of samples
from G of size Ω((d/ε2) poly log(d/ε)). There exists an
efﬁcient algorithm that, on input S and ε > 0, returns a
mean vector (cid:98)µ so that with probability at least 9/10 we
have (cid:107)(cid:98)µ − µG(cid:107)2 = O(ε(cid:112)log(1/ε)).
Diakonikolas et al. (2016a) gave algorithms for robustly es-
timating the mean of a Gaussian distribution with known
covariance and for robustly estimating the mean of a bi-
nary product distribution. The main motivation for consid-
ering these speciﬁc distribution families is that robustly es-
timating the mean within Euclidean distance immediately
implies total variation distance bounds for these families.
The above theorem establishes that these guarantees hold
in a more general setting with near sample-optimal bounds.
Under a bounded second moment assumption, we show:
Theorem 3.2. Let P be a distribution on Rd with unknown
mean vector µP and unknown covariance matrix ΣP (cid:22)
σ2I. Let S be an ε-corrupted set of samples from P of size
Θ((d/ε) log d). There exists an efﬁcient algorithm that, on
input S and ε > 0, with probability 9/10 outputs (cid:98)µ with
(cid:107)(cid:98)µ − µP (cid:107)2 ≤ O(
The sample size above is optimal, up to a logarithmic fac-
tor, and the error guarantee is easily seen to be the best pos-
sible up to a constant factor. The main difference between
the ﬁltering algorithm establishing the above theorem and
the ﬁltering algorithm for the sub-gaussian case is how we
choose the threshold for the ﬁlter. Instead of looking for a
violation of a concentration inequality, here we will choose
a threshold at random.
In this case, randomly choosing
a threshold weighted towards higher thresholds sufﬁces to
throw out more corrupted samples than uncorrupted sam-
ples in expectation. Although it is possible to reject many
good samples this way, we show that the algorithm still
only rejects a total of O(ε) samples with high probability.

εσ).

√

Finally, estimating the covariance of a Gaussian:
Theorem 3.3. Let G ∼ N (0, Σ) be a Gaussian in d dimen-
sions, and let ε > 0. Let S be an ε-corrupted set of samples
from G of size Ω((d2/ε2) poly log(d/ε)). There exists an
efﬁcient algorithm that, given S and ε, returns the param-
eters of a Gaussian distribution G(cid:48) ∼ N (0, (cid:98)Σ) so that with
probability at least 9/10, it holds (cid:107)I −Σ−1/2 (cid:98)ΣΣ−1/2(cid:107)F =
O(ε log(1/ε)).

We now provide a high-level description of the main in-
gredient which yields these improved sample complexity
bounds. The initial analysis of Diakonikolas et al. (2016a)
established sample complexity bounds which were sub-
optimal by polynomial factors because it insisted that the
set of good samples (i.e., before the corruption) satisﬁed
very tight tail bounds. To some degree such bounds are
necessary, as when we perform our ﬁltering procedure, we
need to ensure that not too many good samples are thrown

Being Robust (in High Dimensions) Can Be Practical

away. However, the old analysis required that fairly strong
tail bounds hold uniformly. The idea for the improvement
is as follows: If the errors are sufﬁcient to cause the vari-
ance of some polynomial p (linear in the unknown mean
case or quadratic in the unknown covariance case) to in-
crease by more than ε, it must be the case that for some T ,
roughly an ε/T 2 fraction of samples are error points with
|p(x)| > T . As long as we can ensure that less than an
ε/T 2 fraction of our good sample points have |p(x)| > T ,
this will sufﬁce for our ﬁltering procedure to work. For
small values of T , these are much weaker tail bounds than
were needed previously and can be achieved with a smaller
number of samples. For large values of T , these tail bounds
are comparable to those used in previous work (Diakoniko-
las et al., 2016a) , but in such cases we can take advantage
of the fact that |p(G)| > T only with very small probabil-
ity, again allowing us to reduce the sample complexity. The
details are deferred to the supplementary material.

4. Filtering

We now describe the ﬁltering technique more rigorously, as
well as some additional practical heuristics.

4.1. Robust Mean Estimation

We ﬁrst consider mean estimation. The algorithms which
achieve Theorems 3.1 and 3.2 both follow the general
recipe in Procedure 1. We must specify three parameter
functions:
• Thres(ε) is a threshold function—we terminate if the
covariance has spectral norm bounded by Thres(ε).

• Tail(T, d, ε, δ, τ ) is an univariate tail bound, which
would only be violated by a τ fraction of points if they
were uncorrupted, but is violated by many more of the cur-
rent set of points.

• δ(ε, s) is a slack function, which we require for techni-
cal reasons.

Given these objects, our ﬁlter is fairly easy to state: ﬁrst,
we compute the empirical covariance. Then, we check
if the spectral norm of the empirical covariance exceeds
Thres(ε). If it does not, we output the empirical mean with
the current set of data points. Otherwise, we project onto
the top eigenvector of the empirical covariance, and throw
away all points which violate Tail(T, d, ε, δ, τ ), for some
choice of slack function δ.

Sub-gaussian case To instantiate this algorithm for the
subgaussian case, we take Thres(ε) = O(ε log 1/ε),
δ(ε, s) = 3(cid:112)ε(s − 1),
and Tail(T, d, ε, δ, τ ) =
8 exp(−T 2/2ν) + 8
T 2 log(d log(d/ετ )) , where ν is the sub-
gaussian parameter. See the supplementary material for de-
tails.

ε

Procedure 1 Filter-based algorithm template for robust
mean estimation
1: Input:

samples S,

ε-corrupted

set
Thres(ε), Tail(T, d, ε, δ, τ ), δ(ε, s)

An

of

2: Compute the sample mean µS(cid:48)

= EX∈uS(cid:48)[X], covari-
ance Σ, approximations for the largest absolute eigen-
value and eigenvector of Σ, λ∗ := (cid:107)Σ(cid:107)2, and v∗.

.

return µS(cid:48)

3: if (cid:107)Σ(cid:107)2 ≤ Thres(ε) then
4:
5: end if
6: Let δ = δ(ε, (cid:107)Σ(cid:107)2).
7: Find T > 0 such that

(cid:104)
|v∗ · (X − µS(cid:48)

Pr
X∈uS(cid:48)

)| > T + δ

> Tail(T, d, ε, δ, τ ).

(cid:105)

8: return {x ∈ S(cid:48) : |v∗ · (x − µS(cid:48)

)| ≤ T + δ}.

Second moment case To instantiate this algorithm for
the second moment case, we take Thres(ε) = 9, δ = 0,
and we take Tail to be a random rescaling of the largest
deviation in the data set, in the direction v∗. See the sup-
plementary material for details.

4.2. Robust Covariance Estimation

Our algorithm for robust covariance follows the exact
recipe outlined above, with one key difference—we check
for deviations in the empirical fourth moment tensor. Intu-
itively, just as in the robust mean setting, we used degree-2
information to detect outliers for the mean (the degree-1
moment), here we use degree-4 information to detect out-
liers for the covariance (the degree-2 moment).

This corresponds to ﬁnding a normalized degree-2 poly-
nomial whose empirical variance is too large.
Filter-
ing along this polynomial with an appropriate choice of
Thres(ε), δ(ε, s), and Tail gives the desired bounds. See
the supplementary material for more details.

4.3. Better Univariate Tests

In the algorithms described above for robust mean estima-
tion, after projecting onto one dimension, we center the
points at the empirical mean along this direction. This is
theoretically sufﬁcient, however, introduces additional con-
stant factors since the empirical mean along this direction
may be corrupted. Instead, one can use a robust estimate
for the mean in one direction. Namely, it is well known that
the median is a provably robust estimator for the mean for
symmetric distributions (Huber & Ronchetti, 2009; Ham-
pel et al., 1986), and under certain models it is in fact op-
timal in terms of its resilience to noise (Dvoretzky et al.,
1956; Massart, 1990; Chen, 1998; Daskalakis & Kamath,
2014; Diakonikolas et al., 2017). By centering the points

Being Robust (in High Dimensions) Can Be Practical

Isotropic

Skewed

1.5

1

0.5

r
o
r
r
e

2
(cid:96)

s
s
e
c
x
e

0

100

0.15

r
o
r
r
e

2
(cid:96)

s
s
e
c
x
e

0.1

0.05

200

300

400

100

200

300

400

dimension

Filtering
Sample mean w/ noise
RANSAC

dimension

LRVMean
Pruning
Geometric Median

Figure 1. Experiments with synthetic data for robust mean esti-
mation: excess (cid:96)2 error is reported against dimension.

at the median instead of the mean, we are able to achieve
better error in practice.

4.4. Adaptive Tail Bounding

In our empirical evaluation, we found that it was im-
portant to ﬁnd an appropriate choice of Tail, to achieve
good error rates, especially for robust covariance estima-
tion. Concretely, in this setting, our tail bound is given by
Tail(T, d, ε, δ, τ ) = C1 exp(−C2T ) + Tail2(T, d, ε, δ, τ ),
for some function Tail2, and constants C1, C2. We found
that for reasonable settings, the term that dominated was
always the ﬁrst term on the RHS, and that Tail2 is less sig-
niﬁcant. Thus, we focused on optimizing the ﬁrst term.

We found that depending on the setting, it was useful to
change the constant C2. In particular, in low dimensions,
we could be more stringent, and enforce a stronger tail
bound (which corresponds to a higher C2), but in higher
dimensions, we must be more lax with the tail bound. To
do this in a principled manner, we introduced a heuristic we
call adaptive tail bounding. Our goal is to ﬁnd a choice of
C2 which throws away roughly an ε-fraction of points. The
heuristic is fairly simple: we start with some initial guess
for C2. We then run our ﬁlter with this C2. If we throw
away too many data points, we increase our C2, and retry.
If we throw away too few, then we decrease our C2 and
retry. Since increasing C2 strictly decreases the number of
points thrown away, and vice versa, we binary search over
our choice of C2 until we reach something close to our tar-
get accuracy. In our current implementation, we stop when
the fraction of points we throw away is between ε/2 and
3ε/2, or if we’ve binary searched for too long. We found
that this heuristic drastically improves our accuracy, and
allows our algorithm to scale fairly smoothly from low to
high dimension.

5. Experiments

We performed an empirical evaluation of the above algo-
rithms on synthetic and real data sets with and without

r
o
r
r
e

s
i
b
o
n
a
l
a
h
a
M

s
s
e
c
x
e

r
o
r
r
e

s
i
b
o
n
a
l
a
h
a
M

s
s
e
c
x
e

1.5

0.5

1

0

0.4

0.2

0

r
o
r
r
e

s
i
b
o
n
a
l
a
h
a
M

s
s
e
c
x
e

r
o
r
r
e

s
i
b
o
n
a
l
a
h
a
M

s
s
e
c
x
e

200

100

0

1

0

0.5

20

40

60

80

100

20

40

60

80

100

dimension

dimension

20

40

60

80

100

20

40

60

80

100

dimension

Filtering
Sample covariance w/ noise
RANSAC

dimension

LRVCov
Pruning

Figure 2. Experiments with synthetic data for robust covariance
estimation: excess Mahalanobis error is reported against dimen-
sion.

synthetic noise. All experiments were done on a laptop
computer with a 2.7 GHz Intel Core i5 CPU and 8 GB of
RAM. The focus of this evaluation was on statistical ac-
curacy, not time efﬁciency. In all synthetic trials, our al-
gorithm consistently had the smallest error, sometimes or-
ders of magnitude better than any other algorithms. In the
semi-synthetic benchmark, our algorithm also (arguably)
performs the best, though this is subjective. While we did
not optimize our code for runtime, it is always comparable
to (and often better than) the effective alternatives.

5.1. Synthetic Data

Experiments with synthetic data allow us to verify the error
guarantees and the sample complexity rates proven in Sec-
tion 3. In both cases, the experiments validate the accuracy
and usefulness of our algorithm, almost exactly matching
the best rate without noise.

Unknown mean The results of our synthetic mean ex-
periment are shown in Figure 1.
In the synthetic mean
experiment, we set ε = 0.1, and for dimension d =
[100, 150, . . . , 400], we generate n = 10d
ε2 samples, where
a (1 − ε)-fraction come from N (µ, I), and an ε fraction
come from a noise distribution. Our goal is to produce an
estimator which minimizes the (cid:96)2 error the estimator has
to the truth. As a baseline, we compute the error that is
achieved by only the uncorrupted sample points. This error
will be used as the gold standard for comparison, since in
the presence of error, this is roughly the best one could do

Being Robust (in High Dimensions) Can Be Practical

even if all the noise points were identiﬁed exactly.2

On this data, we compared the performance of our Filter al-
gorithm to that of (1) the empirical mean of all the points,
(2) a trivial pruning procedure, (3) the geometric median of
the data, (4) a RANSAC-based mean estimation algorithm,
and (5) a recently proposed robust estimator for the mean
due to (Lai et al., 2016), which we will call LRVMean. For
(5), we use the implementation available in their Github.3
In Figure 1, the x-axis indicates the dimension of the exper-
iment, and the y-axis measures the (cid:96)2 error of our estimated
mean minus the (cid:96)2 error of the empirical mean of the true
samples from the Gaussian, i.e., the excess error induced
over the sampling error.

We tried various noise distributions, and found that the
same qualitative pattern arose for all of them. In the re-
ported experiment, our noise distribution was a mixture
of two binary product distributions, where one had a cou-
ple of large coordinates (see the supplementary material
for a detailed description). For all (nontrivial) error dis-
tributions we tried, we observed that indeed the empirical
mean, pruning, geometric median, and RANSAC all have
error which diverges as d grows, as the theory predicts.
On the other hand, both our algorithm and LRVMean have
markedly smaller error as a function of dimension. Indeed,
our algorithm’s error is almost identical to that of the em-
pirical mean of the uncorrupted sample points.

Unknown covariance See Figure 2 for the results of our
synthetic covariance experiment. Our setup is similar to
that for the synthetic mean. Since both our algorithm and
LRVCov require access to fourth moments, we ran into is-
sues with limited memory on machines. This limitation
prevented us from performing experiments at the same di-
mensionality as the unknown mean setting, and we could
not use as many samples. We ﬁx ε = 0.05. For di-
mension d = [10, 20, . . . , 100], we generate 0.5d
sam-
ε2
ples, where a (1 − ε)-fraction come from N (0, Σ), and
the rest come from a noise distribution. We measure dis-
tance in the natural afﬁne invariant way, namely, the Ma-
halanobis distance induced by Σ to the identity: err((cid:98)Σ) =
(cid:107)Σ−1/2 (cid:98)ΣΣ−1/2 − I(cid:107)F . As before, we use the empirical
error of only the uncorrupted data points as a benchmark.

On this corrupted data, we compared the performance of
our Filter algorithm to that of (1) the empirical covari-
ance of all the points, (2) a trivial pruning procedure, (3)
a RANSAC-based minimal volume ellipsoid (MVE) algo-
rithm, and (5) a recently proposed robust estimator for the
covariance due to (Lai et al., 2016), which we will call

2We note that it is possible that an estimator may achieve

slightly better error than this baseline.

LRVCov. For (5), we again obtained the implementation
from their Github repository.

We tried various choices of Σ and noise distribution. Fig-
ure 2 shows two choices of Σ and noise. Again, the x-axis
indicates the dimension of the experiment and the y-axis
indicates the estimator’s excess Mahalanobis error over the
sampling error. In the left ﬁgure, we set Σ = I, and our
noise points are simply all located at the all-zeros vector. In
the right ﬁgure, we set Σ = I +10e1eT
1 , where e1 is the ﬁrst
basis vector, and our noise distribution is a somewhat more
complicated distribution, which is similarly spiked, but in a
different, random, direction. We formally deﬁne this distri-
bution in the supplementary material. For all choices of Σ
and noise we tried, the qualitative behavior of our algorithm
and LRVCov was unchanged. Namely, we seem to match
the empirical error without noise up to a very small slack,
for all dimensions. On the other hand, the performance of
empirical mean, pruning, and RANSAC varies widely with
the noise distribution. The performance of all these algo-
rithms degrades substantially with dimension, and their er-
ror gets worse as we increase the skew of the underlying
data. The performance of LRVCov is the most similar to
ours, but again is worse by a large constant factor. In par-
ticular, our excess risk was on the order of 10−4 for large
d, for both experiments, whereas the excess risk achieved
by LRVCov was in all cases a constant between 0.1 and 2.

These experiments demonstrate that our statistical guaran-
tees are in fact quite strong. As our excess error is al-
most zero (and orders of magnitude smaller than other ap-
proaches), this suggests that our sample complexity is in-
deed near-optimal, since we match the rate without noise,
and that the constants and logarithmic factors in the theo-
retical recovery guarantee are often small or non-existent.

5.2. Semi-synthetic Data

To demonstrate the efﬁcacy of our method on real data,
we revisit the famous study of Novembre et al. (2008). In
this study, the authors investigated data collected as part of
the POPRES project. This dataset consists of the genotyp-
ing of thousands of individuals using the Affymetrix 500K
single nucleotide polymorphism (SNP) chip. The authors
pruned the dataset to obtain the genetic data of over 1387
European individuals, annotated by their country of origin.
Using principal components analysis, they produce a two-
dimensional summary of the genetic variation, which bears
a striking resemblance to the map of Europe.

Our experimental setup is as follows. While the original
dataset is very high dimensional, we use a 20 dimensional
version of the dataset as found in the authors’ GitHub4. We

4https://github.com/NovembreLab/Novembre_

3https://github.com/kal2000/AgnosticMean\

etal_2008_misc

AndCovarianceCode

Being Robust (in High Dimensions) Can Be Practical

The data projected onto the top two
directions of the original data set
without noise

The data projected onto the top two direc-
tions
of the noisy data set after pruning

The ﬁltered set of points projected onto the
top two directions returned by the ﬁlter

The data projected onto the top two
directions returned by the ﬁlter

Figure 3. Given genetic data from (Novembre et al., 2008), projected down to 20-dimensions, with added noise. Colors indicate the
individual’s country of origin, and match the colors of the countries in the map of Europe. Black points are added noise. The top left
plot is the original plot from (Novembre et al., 2008). We recover Europe in the presence of noise whereas naive methods do not.

ﬁrst randomly rotate the data, as then 20 dimensional data
was diagonalized, and the high dimensional data does not
ε
follow such structure. We then add an additional
1−ε frac-
tion of points (so that they make up an ε-fraction of the ﬁnal
points). These added points were discrete points, following
a simple product distribution (described in the supplemen-
tary materials). We used a number of methods to obtain
a covariance matrix for this dataset, and we projected the
data onto the top two singular vectors of this matrix.
In
Figure 3, we compare our techniques to pruning. In par-
ticular, our output was able to more or less reproduce the
map of Europe, whereas pruning fails to. In the supplemen-
tary material, we also compare our result with a number of
other techniques, including those we tested against in the
unknown covariance experiments, and other robust PCA
techniques. The only alternative algorithm which was able
to produce meaningful output was LRVCov, which pro-
duced output that was similar to ours, but which produced
a map which was somewhat more skewed. We believe that
our algorithm produces the best picture.

In Figure 3, we also display the actual points which were
output by our algorithm’s Filter. While it manages to re-
move most of the noise points, it also seems to remove
some of the true data points, particularly those from Eastern
Europe and Turkey. We attribute this to a lack of samples
from these regions, and thus one could consider them as
outliers to a dataset consisting of Western European indi-
viduals. For instance, Turkey had 4 data points, so it seems
quite reasonable that any robust algorithm would naturally
consider these points outliers.

We view our experiments as a proof of concept demon-
stration that our techniques can be useful in real world
exploratory data analysis tasks, particularly those in high-
dimensions. Our experiments reveal that a minimal amount
of noise can completely disrupt a data analyst’s ability to
notice an interesting phenomenon, thus limiting us to only
very well-curated data sets. But with robust methods, this
noise does not interfere with scientiﬁc discovery, and we
can still recover interesting patterns which otherwise would
have been obscured by noise.

-0.2-0.100.10.20.3-0.2-0.15-0.1-0.0500.050.10.15Filter Output-0.2-0.100.10.20.3-0.2-0.15-0.1-0.0500.050.10.15Filter Projection-0.2-0.100.10.20.3-0.15-0.1-0.0500.050.10.150.2Original Data-0.2-0.100.10.20.3-0.15-0.1-0.0500.050.10.150.2Pruning ProjectionBeing Robust (in High Dimensions) Can Be Practical

References

Amaldi, E. and Kann, V. The complexity and approxima-
bility of ﬁnding maximum feasible subsystems of linear
relations. Theoretical Computer Science, 147:181–210,
1995.

Cand`es, E. J., Li, X., Ma, Y., and Wright, J. Robust princi-

pal component analysis? J. ACM, 58(3):11, 2011.

Chan, T. M. An optimal randomized algorithm for max-
In Proceedings of the Fifteenth
imum tukey depth.
Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA), pp. 430–436, 2004.

Charikar, M., Steinhardt, J., and Valiant, G. Learning from

untrusted data. In Proceedings of STOC’17, 2017.

Chen, M., Gao, C., and Ren, Z. A general decision theory
for huber’s ε-contamination model. Electronic Journal
of Statistics, 10(2):3752–3774, 2016.

Chen, Z. A note on bias robustness of the median. Statistics

& probability letters, 38(4):363–368, 1998.

Clarkson, K. L., Eppstein, D., Miller, G. L., Sturtivant, C.,
and Teng, S.-H. Approximating center points with iter-
ated radon points. In Proceedings of the Ninth Annual
Symposium on Computational Geometry, SCG ’93, pp.
91–98, New York, NY, USA, 1993. ACM.

Daskalakis, C. and Kamath, G. Faster and sample near-
optimal algorithms for proper learning mixtures of gaus-
sians. In Proceedings of The 27th Conference on Learn-
ing Theory, COLT 2014, pp. 1183–1213, 2014.

Diakonikolas, I., Kamath, G., Kane, D. M., Li, J., Moitra,
A., and Stewart, A. Robust estimators in high dimen-
In Pro-
sions without the computational intractability.
ceedings of FOCS’16, 2016a. Full version available at
https://arxiv.org/pdf/1604.06443.pdf.

Diakonikolas, I., Kane, D. M., and Stewart, A. Robust
learning of ﬁxed-structure bayesian networks. CoRR,
URL https://arxiv.
abs/1606.07384, 2016b.
org/abs/1606.07384.

Diakonikolas, I., Kane, D. M., and Stewart, A. Statisti-
cal query lower bounds for robust estimation of high-
dimensional gaussians and gaussian mixtures. CoRR,
abs/1611.03473, 2016c. URL http://arxiv.org/
abs/1611.03473.

Diakonikolas, I., Kamath, G., Kane, D. M., Li, J., Moitra,
A., and Stewart, A. Robustly learning a gaussian: Get-
ting optimal error, efﬁciently. CoRR, abs/1704.03866,
2017.

Donoho, D. L. and Gasko, M. Breakdown properties of lo-
cation estimates based on halfspace depth and projected
outlyingness. Ann. Statist., 20(4):1803–1827, 12 1992.

Du, S. S., Balakrishnan, S., and Singh, A. Computation-
ally efﬁcient robust estimation of sparse functionals. In
Proceedings of COLT’17, 2017.

Dvoretzky, A., Kiefer, J., and Wolfowitz, J. Asymptotic
minimax character of the sample distribution function
and of the classical multinomial estimator. Ann. Mathe-
matical Statistics, 27(3):642–669, 1956.

Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., and Sta-
hel, W. A. Robust statistics. The approach based on in-
ﬂuence functions. Wiley New York, 1986.

Huber, P. J. Robust estimation of a location parameter. The
Annals of Mathematical Statistics, 35(1):73–101, 1964.

Huber, P. J. and Ronchetti, E. M. Robust statistics. Wiley

New York, 2009.

Johnson, D. S. and Preparata, F. P. The densest hemi-
sphere problem. Theoretical Computer Science, 6:93–
107, 1978.

Lai, K. A., Rao, A. B., and Vempala, S. Agnostic es-
In Proceedings of

timation of mean and covariance.
FOCS’16, 2016.

Li, J. Robust sparse estimation tasks in high dimensions.

In Proceedings of COLT’17, 2017.

Massart, P. The tight constant in the Dvoretzky-Kiefer-
Annals of Probability, 18(3):

Wolfowitz inequality.
1269–1283, 1990.

Miller, G.L. and Sheehy, D. Approximate centerpoints with

proofs. Comput. Geom., 43(8):647–654, 2010.

Novembre, J., Johnson, T., Bryc, K., Kutalik, Z., Boyko,
A. R., Auton, A., Indap, A., King, K. S., Bergmann, S.,
Nelson, M. R., et al. Genes mirror geography within
europe. Nature, 456(7218):98–101, 2008.

Rousseeuw, P. Multivariate estimation with high break-
down point. Mathematical Statistics and Applications,
pp. 283–297, 1985.

Rousseeuw, P. J. and Struyf, A. Computing location depth
and regression depth in higher dimensions. Statistics and
Computing, 8(3):193–203, 1998.

Steinhardt, J., Charikar, M., and Valiant, G. Resilience: A
criterion for learning in the presence of arbitrary outliers.
CoRR, abs/1703.04940, 2017.

Being Robust (in High Dimensions) Can Be Practical

Tukey, J.W. A survey of sampling from contaminated dis-
tributions. Contributions to probability and statistics, 2:
448–485, 1960.

Van Aelst, S. and Rousseeuw, P. Minimum volume ellip-
soid. Wiley Interdisciplinary Reviews: Computational
Statistics, 1(1):71–82, 2009.

Xu, H., Caramanis, C., and Sanghavi, S. Robust pca via
outlier pursuit. In Advances in Neural Information Pro-
cessing Systems, pp. 2496–2504, 2010.

