Globally Induced Forest: A Prepruning Compression Scheme
Supplementary material

Jean-Michel Begon 1 Arnaud Joly 1 Pierre Geurts 1

1. GIF algorithm

and the solution is given by

Figure 1 illustrates visually the inner loop of the GIF build-
ing algorithm: a subset of the candidates nodes is chosen
uniformely at random. The contribution of each node is
evaluated and the one which reduces the error the most is
added to the model. Its children are then built and added to
the candidate list.

2. Optimization problem

We are building an additive model by inserting progres-
sively nodes in the forest. At time t, we are trying to ﬁnd
the best node j(t) from the candidate list Ct and its associ-
ated optimal weight w(t)
j

:

w(t)

j =

1
|Zj|

(cid:88)

i∈Zj

r(t−1)
i

(3)

i

where r(t−1)
= yi − ˆy(t−1)(xi) is the residual at time
t − 1 for the ith training instance and Zj = {1 ≤ i ≤
N |zj(xi) = 1} is the subset of instances reaching node j.

Classiﬁcation For classiﬁcation we used the multi-
exponential loss (Zhu et al., 2009). First, we need to encode
the labels so that

y(k)
i =

(cid:40)

1,
− 1

K−1 , otherwise

if the class of yi is k

j(t), w(t)

j = arg min
j∈Ct,w∈RK

N
(cid:88)

i=1

(cid:16)

(cid:17)
yi, ˆy(t−1)(xi) + wzj(xi)

L

where K is the number of classes.
(cid:80)K

i = 0. The optimization then becomes

k=1 y(k)

Notice that

(1)

w(t)

j = arg min
w∈RK

N
(cid:88)

i=1

exp

(cid:18) −1
K

yT
i

(cid:16)

ˆy(t−1)(xi) + wzj(xi)

Regression For regression, we used the L2-norm:

for 1 ≤ k ≤ K, where

where (xi, yi)N
i=1 is the learning sample, ˆy(t−1)() is the
model at time t − 1, zj() is the node indicator functions,
meaning that it is 1 if its argument reaches node j and 0
otherwise.

This problem is solved in two steps. First a node j is
selected from Ct and the corresponding optimal weight,
alongside the error reduction, are computed. This is re-
peated for all nodes and the one achieving the best improve-
ment is selected.

w(t)

j = arg min

w∈R

(cid:16)

N
(cid:88)

i=1

L

yi, ˆy(t−1)(xi) + wzj(xi)

(2)

(cid:17)2

1Department of Electrical Engineering and Computer Sci-
ence University of Li`ege, Li`ege, Belgium. Correspondence
to: Jean-Michel Begon <jm.begon@ulg.ac.be>, Pierre Geurts
<p.geurts@ulg.ac.be>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

= arg min
w∈RK

F (t−1)
j

(w)

Solving for ∇F (t−1)

j

(w) = 0 yields

α(t−1,k)

j

φ(k)(w) =

α(t−1,l)

φ(l)(w)

j

(7)

1
K

K
(cid:88)

l=1

α(t−1,k)

j

(cid:44) (cid:88)

(cid:16)

exp

−µ(t−1)
i

(cid:17)

i∈Z(k)
j

K
(cid:88)

k=1
(cid:18)

µ(t−1)
i

(cid:44) 1
K

yi ˆy(t−1,k)(xi)

φ(k)(w) (cid:44) exp

−

ψ(k)(w)

1
K

(cid:19)

K
(cid:88)

1
K − 1

l=1,l(cid:54)=k

ψ(k)(w) (cid:44) −w(k) +

w(l)

(11)

(4)

(cid:17)(cid:19)

(5)

(6)

(8)

(9)

(10)

Equation 7 is equivalent to

The prediction of node j is

Globally Induced Forest

T rj = 1
|Zj |
diction is:

(cid:80)

i∈Zj

yi. We need to show that the GIF pre-

ˆyj =

1
|Zj|

(cid:88)

i∈Zj

yi

(cid:0)yi − ˆyπj

(cid:1)

ˆyj = ˆyπj + wj
1
|Zj|

= ˆyπj +

(cid:88)

i∈Zj
(cid:88)

i∈Zj

1
|Zj|

=

1
|Zj|

(cid:88)

i∈Zj

yi

= ˆyπj +

(yi) − ˆyπj

The ﬁrst step is how the additive model is built. The second
is the optimal weight value of node j derived in Equation
3, the third step is due to the fact that the prediction at πj is
constant since there is only one tree.

(cid:19)

ˆy(t−1,k)(xi)

3.2. Classiﬁcation

In order to have the same prediction as the underlying tree,
we must demonstrate that the probability of being in class

j = {1 ≤ i ≤ N |zi,j = 1 ∧ y(k)

where Z (k)
i = 1} is the
subset of learning instances of class k reaching node j. In
words, µ(t−1)
is the hyper-margin of instance i at time t−1
and α(t−1,k)
is the class error of label k for node j at time
j
t − 1.

i

α(t−1,k)

φ(k)(w) = α(t−1,l)

φ(l)(w) 1 ≤ k, l ≤ K (12)

j

j

In keeping with the output representation (Equation 4), we
can impose a zero-sum constraint on the prediction to get
a unique solution for the kth component of w(t)
. If it is
j
imposed at each stage, it means that

K
(cid:88)

k=1

K
(cid:88)

k=1

ˆy(t−1,k) =

ˆy(t,k) = 0 =

w(k)

(13)

K
(cid:88)

k=1

and this is not impacted by the learning rate.

The corresponding solution is

φ(k)(w) = exp

−

α(t−1,k)

j

=

(cid:88)

i∈Z(k)
j

(cid:18)

1
K − 1
(cid:18)

exp

−

(cid:19)

w(k)

1
K − 1

w(t,k)
j

=

K − 1
K

K
(cid:88)

l=1

log

α(t−1,k)
j
α(t−1,l)

j

(14)

(15)

(16)

In the case of a single tree (T = 1) and a unit learning rate
(λ = 1), both the square loss in regression and the multi-
exponential loss in classiﬁcation produce the same predic-
tions as the underlying tree. This is due to the fact that,
when examining the weight to give to node j at time t, the
prediction of time t − 1 relates to the parent node πj of
j. It is thus independent of t and is also the same for all
instances reaching that node.

Consequently, we will adopt the following slight change in
notation:

ˆyj = ˆy(πj ) + wj

(17)

meaning that the prediction associated to any object reach-
ing node j is the weight of j plus the prediction associated
to its parent πj. With ˆy(π1) = 0, the prediction of the root’s
pseudo-parent.

3. Equivalence of GIF and the underlying tree

exp

l associated to node j will be

|Z(l)
j |
|Zj | .

Under the zero-sum constraint, we have

(cid:18) 1

K − 1

(cid:19)

w(l)
j

α(l)
πj

=

=

1
cj
1
cj

1
cj

(cid:88)

(cid:18)

exp

−

(cid:19)

1
K − 1

ˆy(l)
πj

i∈Z(l)
j

=

|Z (l)
j

| exp

−

(cid:18)

(cid:19)

1
K − 1

ˆy(l)
πj

(cid:18) 1

exp

K − 1

(cid:19)

ˆy(l)
j

= exp

(cid:18) 1

K − 1

(cid:19)

ˆy(l)
πj

exp

(cid:18) 1

K − 1

=

1
cj

|Z (l)
j

|

Pj(l) =

exp

(cid:17)

(cid:16) 1
K−1 ˆy(l)
(cid:16) 1
K−1 ˆy(k)

j

j

(cid:80)K

k=1 exp

(cid:17) =

|

|Z (l)
j
|Zj|

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(cid:19)

(25)

w(l)
j

(26)

(27)

(28)

3.1. Regression

In regression, the tree prediction T rj of any leaf j is the
average of the learning set’s outputs reaching that node:

(cid:16)(cid:81)K

k=1 α(k)
where cj =
ity is a consequence of the value of w(l)
j

K

j

(cid:17) 1

is a constant. The ﬁrst equal-

(Equation 16). The

Globally Induced Forest

second is a due to the deﬁnition of α(l)
(Equation 15). The
j
third is a consequence of having a single tree: the predic-
tion of the parent is the same for all instances.

Notice that, in both regression and classiﬁcation, the equiv-
alence also holds for an internal node: the prediction is the
one the tree would have yielded if that node had been a leaf.

4. Datasets

Table 1 sums up the main characteristics of the datasets
we used. Abalone, CT slice, California data housig (Ca-
data), Musk2, Vowel and Letter come from the UCI Ma-
chine Learning Repository (Blake & Merz, 1998). Ring-
norm, Twonorm and Waveform are described in (Breiman
et al., 1998). Hwang F5 comes from the DELVE repository
1. The noise parameter of the Friedman1 dataset (Fried-
man, 1991) has been set to 1. Hastie is described in (Fried-
man et al., 2001). Out of the 500 features of Madelon
(Guyon et al., 2004), 20 are informative and 50 are redun-
dant; the others are noise. Mnist8vs9 is the Mnist dataset
(LeCun et al., 1998) of which only the 8 and 9 digits have
been kept. Binary versions of the Mnist, Letter and Vowel
datasets have been created as well by grouping the ﬁrst half
and second half classes together.

Table 1. Characteristics of the datasets. N is the learning sample
size, TS stands for testing set, and p is the number of features.

DATASET
FRIEDMAN1
ABALONE
CT SLICE
HWANG F5
CADATA
RINGNORM
TWONORM
HASTIE
MUSK2
MADELON
MNIST8VS9
WAVEFORM
VOWEL
MNIST
LETTER

N
300
2506
2000
2000
12384
300
300
2000
2000
2200
11800
3500
495
50000
16000

|T S|
2000
1671
51500
11600
8256
7100
7100
10000
4598
2200
1983
1500
495
10000
4000

p
10
10
385
2
8
20
10
10
166
500
784
40
10
784
8

# CLASSES
-
-
-
-
-
2
2
2
2
2
2
3
11
10
26

Breadth ﬁrst deepening This variant consist in adding
the nodes level after level, from left to right, producing a
heaped forest. As a consequence, all trees have the same
(order of) height, implying that the forest can be quite wide
but usually shallow.

Random deepening This variant consist in ﬁrst choosing
a tree and then choosing one of its leaves to transform to a
decision nodes. Both choices are made uniformly at ran-
dom so that the trees are expected to have approximately
the same number of nodes. The depth, however, might vary
signiﬁcantly.

Best ﬁrst deepening This variant consist in choosing,
among all leaves which could be turned into a internal
node, the one which reduces its local impurity the most.
Let Nc, Nl and Nr be the number of instances reaching
the candidate node, candidate left child and candidate right
child respectively. Let also Ic, Il and Ir be the impurity
(gini index in classiﬁcation, variance in regression) of the
instances reaching the candidate node, candidate left child
and candidate right child respectively. Then, for N learning
instances, the local impurity reduction is deﬁned as:

∆Ic (cid:44) Nc
N

(cid:20)
Ic −

(cid:18) Nl
Nc

Il +

(cid:19)(cid:21)

Nr
Nc

Ir

(29)

Since the fraction of learning instances reaching the can-
didate is accounted for in the reduction of impurity, this
approach will naturally favor higher nodes in the trees.

Experiment We conducted the same experiment as for
GIF: the three algorithms were tested on ten folds with dif-
ferent learning sample/testing sample splits and were sub-
jected to the 1% and 10% node constraints. We started with
a pool of T = 1000 roots and no restriction was imposed
regarding the depth. All of the m = p the features were
examined in regression and m =
p in classiﬁcation, as
suggested in (Geurts et al., 2006). Table 2 holds the av-
erage mean square error for the ﬁve regression problems
and Table 3 holds the average misclassiﬁcation rate for the
classiﬁcation problems.

√

5. Comparison with local baseline algorithms

We have tested three deepening algorithm for decision for-
est relying on non-global metrics, meaning that the choice
of the best candidate is not made according to how well the
forest, as a whole, performs. These algorithms share that
the ﬁnal model is exactly a sub-forest of the un-pruned for-
est: contrary to GIF, no internal weights are ﬁtted and the
predictions of at the leaves are the usual tree predictions.

1http://www.cs.utoronto.ca/delve

Regression The trend is quite clear: both at 1% and 10%,
the breadth ﬁrst algorithm is the best and the best ﬁrst is
(largely) the worst. There are two instances where the local
baselines are able to beat GIF: on Abalone and Hwang F5
at 10%. Interestingly, these are the same cases on which
GIF was beaten by a small forest of Extremely random-
ized trees. The 10% Hwang F5 case aside, the local base-
lines always underperform the smaller fully-developed for-
est. Overall, such variants do not seem adequate for regres-
sion.

Globally Induced Forest

Table 2. Average mean square error for local baselines at 1% and 10% budgets (T = 1000, m = p).

DATASET
FRIEDMAN1
ABALONE
CT SLICE
HWANG F5 ×10−2
CADATA ×10−2

BREADTH FIRST10%
6.02 ± 0.28
4.72 ± 0.23
30.39 ± 1.90
6.73 ± 0.07
29.24 ± 0.73

RANDOM10%
6.80 ± 0.34
4.77 ± 0.23
36.19 ± 1.84
6.83 ± 0.06
31.08 ± 0.74

BEST FIRST10%
15.00 ± 0.39
6.82 ± 0.33
310.87 ± 4.79
56.57 ± 6.03
75.23 ± 0.95

BREADTH FIRST1%
11.73 ± 0.46
5.42 ± 0.27
82.19 ± 2.41
8.52 ± 0.24
43.40 ± 1.18

RANDOM1%
12.52 ± 0.47
5.55 ± 0.27
97.24 ± 1.90
13.17 ± 0.44
47.47 ± 1.02

BEST FIRST1%
15.29 ± 0.42
6.82 ± 0.33
313.84 ± 4.64
56.60 ± 6.07
75.48 ± 0.95

Table 3. Error rate (%) for local baselines at 1% and 10% budgets (T = 1000, m =
The last three are multiclass. The three in the middle are their binary versions.

√

p). The six ﬁrst datasets are binary classiﬁcation.

DATASET
RINGNORM
TWONORM
HASTIE
MUSK2
MADELON
MNIST8VS9
BIN. VOWEL
BIN. MNIST
BIN. LETTER
WAVEFORM
VOWEL
MNIST
LETTER

BREADTH FIRST10%
4.25 ± 1.24
3.51 ± 0.26
11.30 ± 1.20
7.01 ± 0.40
11.68 ± 0.67
2.20 ± 0.38
8.99 ± 1.96
4.46 ± 0.25
5.91 ± 0.43
14.74 ± 0.63
14.26 ± 2.41
4.63 ± 0.27
7.06 ± 0.29

RANDOM10%
4.08 ± 1.12
3.53 ± 0.30
11.18 ± 1.16
7.63 ± 0.43
11.92 ± 0.65
2.37 ± 0.39
8.85 ± 2.03
4.91 ± 0.27
5.71 ± 0.40
14.83 ± 0.76
13.21 ± 2.33
4.96 ± 0.26
6.39 ± 0.20

BEST FIRST10%
8.38 ± 6.94
5.59 ± 1.85
21.24 ± 7.11
15.42 ± 0.23
19.12 ± 1.94
6.17 ± 0.73
16.57 ± 3.02
21.71 ± 0.30
26.16 ± 0.86
20.25 ± 2.22
41.49 ± 5.45
28.54 ± 0.59
36.92 ± 1.80

BREADTH FIRST1%
8.94 ± 7.45
5.91 ± 3.03
13.92 ± 2.93
15.42 ± 0.23
16.26 ± 0.97
4.53 ± 0.48
18.73 ± 3.08
10.09 ± 0.25
17.91 ± 0.77
16.75 ± 1.26
42.40 ± 4.33
8.60 ± 0.35
22.11 ± 0.59

RANDOM1%
8.53 ± 7.04
6.52 ± 4.28
14.29 ± 3.20
15.42 ± 0.23
16.70 ± 1.07
4.84 ± 0.51
19.90 ± 3.71
11.78 ± 0.32
18.05 ± 0.78
17.13 ± 1.25
40.28 ± 4.62
9.76 ± 0.31
20.90 ± 0.55

BEST FIRST1%
8.94 ± 7.41
7.28 ± 4.34
21.24 ± 7.12
15.42 ± 0.23
20.14 ± 2.41
6.67 ± 0.69
21.80 ± 4.38
22.50 ± 0.35
26.19 ± 0.88
20.45 ± 2.21
50.44 ± 5.81
29.72 ± 0.61
37.27 ± 1.78

Classiﬁcation In classiﬁcation, the breadth ﬁrst and ran-
dom baselines tend to perform similarly, one beating the
other on some problems. Once again, the best ﬁrst ap-
proach seems to be lagging behind on some datasets. At
10%, the local baselines cannot rival with the other meth-
ods. Only on Waveform are they able to reach the other
performances—a setting where all methods seems to pro-
duce close results. At 1%, the breadth ﬁrst and/or the
random methods surpass the ET10% on Twonorm, Hastie,
Madelon and Waveform. Those datasets correspond to
cases where ET was under-performing signiﬁcantly com-
pared to GIF. All in all, the local baselines are never able
to beat GIF, even in the multiclass setting, which is partic-
ularly defavorable for GIF. Once again, the conclusion is
against the purely local baselines.

We believed the poor performances of the baselines are due
to the building mechanism of traditional ensemble meth-
ods. Although the trees are built independently and with
randomization, there remains an important redundancy be-
tween them, which is especially defavorable to pruning. A
global approach is better able to avoid redundancy and can
thus better exploit the node budget. This would also explain
why the best ﬁrst variant performs worst in both regression
and classiﬁcation: it is prone at picking redundant nodes,
which will usually offer the same kind of impurity reduc-
tion.

References

Blake, Catherine and Merz, Christopher J. {UCI} reposi-

tory of machine learning databases. 1998.

Breiman, Leo et al. Arcing classiﬁer (with discussion and a
rejoinder by the author). The annals of statistics, 26(3):
801–849, 1998.

Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
The elements of statistical learning, volume 1. Springer
series in statistics Springer, Berlin, 2001.

Friedman, Jerome H. Multivariate adaptive regression

splines. The annals of statistics, pp. 1–67, 1991.

Geurts, Pierre, Ernst, Damien, and Wehenkel, Louis. Ex-
tremely randomized trees. Machine learning, 63(1):3–
42, 2006.

Guyon, Isabelle, Gunn, Steve R, Ben-Hur, Asa, and Dror,
Gideon. Result analysis of the nips 2003 feature selec-
tion challenge. In NIPS, volume 4, pp. 545–552, 2004.

LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998.

Zhu, Ji, Zou, Hui, Rosset, Saharon, and Hastie, Trevor.
Multi-class adaboost. Statistics and its Interface, 2(3):
349–360, 2009.

Globally Induced Forest

(a) Current forest at time t

(b) A subset of candidates Ct is drawn uniformely at random from
the set of candidates C (step 8)

(c) The error reduction is computed for all candidates of Ct (step 9)

(d) The best node (highest error reduction) is selected (step 9)

(e) The chosen node is introduced in the model (step 10)

(f) The children of the chosen node are computed (step 11) and
added to the candidate list (step 12)

Figure 1. Illustration of the GIF regression building algorithm (T = 3, CW = 3)

ˆyx()=y+λw9z9(x)4	  1	  5	  10	  11	  12	  13	  6	  2	  7	  14	  15	  16	  17	  8	  3	  18	  19	  20	  21	  Node	  belonging	  to	  the	  model	  Candidate	  node	  Hypothe=cal	  unpruned	  trees	  9	  ˆyx()=y+λw9z9(x)4	  1	  5	  10	  11	  12	  13	  6	  2	  7	  14	  15	  16	  17	  8	  3	  18	  19	  20	  21	  Node	  belonging	  to	  the	  model	  Candidate	  node	  Hypothe=cal	  unpruned	  trees	  9	  Randomly	  preselected	  candidate	  node	  ˆyx()=y+λw9z9(x)4	  1	  5	  10	  11	  12	  13	  6	  2	  7	  14	  15	  16	  17	  8	  3	  18	  19	  20	  21	  Node	  belonging	  to	  the	  model	  Candidate	  node	  Hypothe=cal	  unpruned	  trees	  9	  Δerr	  2.6	  1.5	  0.9	  Randomly	  preselected	  candidate	  node	  ˆyx()=y+λw9z9(x)4	  1	  5	  10	  11	  12	  13	  6	  2	  7	  14	  15	  16	  17	  8	  3	  18	  19	  20	  21	  Node	  belonging	  to	  the	  model	  Candidate	  node	  Hypothe=cal	  unpruned	  trees	  9	  Δerr	  2.6	  1.5	  0.9	  Randomly	  preselected	  candidate	  node	  Chosen	  node	  ˆyx()=y+λw9z9(x)+λw6z6(x)4	  1	  5	  10	  11	  12	  13	  2	  7	  14	  15	  16	  17	  8	  3	  18	  19	  20	  21	  Node	  belonging	  to	  the	  model	  Candidate	  node	  Hypothe=cal	  unpruned	  trees	  9	  6	  ˆyx()=y+λw9z9(x)+λw6z6(x)4	  1	  5	  10	  11	  12	  13	  2	  7	  14	  15	  16	  17	  8	  3	  18	  19	  20	  21	  Node	  belonging	  to	  the	  model	  Candidate	  node	  Hypothe=cal	  unpruned	  trees	  9	  6	  