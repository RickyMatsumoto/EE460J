Tensor Decomposition via Simultaneous Power Iteration
(Supplementary Material)

Po-An Wang 1 Chi-Jen Lu 1

A. Technical Lemmas

δ ∈ (0, 1), we have

For a matrix A, let σmax(A) and σmin(A) denote its largest
and smallest singular values, respectively. Then we will
need the following lemma relating such singular values of
a matrix and its sub-matrix.

Lemma A.1. (Corollary 3.1.3 in (Hom & Johnson, 1991))
Let A and B be matrices such that B is derived from A by
deleting some of its rows and/or columns. Then σmax(A) ≥
σmax(B) and σmin(A) ≤ σmin(B).

For a matrix Z, let Z (cid:12)2 = Z (cid:12) Z denote the Hadamard
(entry-wise) product of Z with itself. Then we will need the
following lemma relating the singular values of matrices Z
and Z (cid:12)2.
Lemma A.2. For any matrix Z, σmin(Z (cid:12)2) ≥ (σmin(Z))2
and σmax(Z (cid:12)2) ≤ (σmax(Z))2.

Proof. One can relate the singular values of the Hadamard
product Z (cid:12)2 to those of the Kronecker product Z ⊗ Z.
In particular, as Z (cid:12) Z can be obtain from Z ⊗ Z by
deleting some rows and columns, Lemma A.1 tells us that
σmin(Z (cid:12) Z) ≥ σmin(Z ⊗ Z) and σmax(Z (cid:12) Z) ≤
σmax(Z ⊗ Z). Then the lemma follows as the Kronecker
product Z ⊗Z is known to have the property that σmin(Z ⊗
Z) = (σmin(Z))2 and σmax(Z ⊗ Z) = (σmax(Z))2.1

We will need the following two tail bounds. The ﬁrst is for
the sum of the squares of independent standard normal ran-
dom variables, known as the χ-square distribution, which
follows from the bound in (Laurent & Massart, 2000).

Lemma A.3. Let z1, . . . , zL be a sequence of i.i.d. random
variables, each from the distribution N (0, 1). Then for any

1Academia Sinica, Taiwan. Correspondence to: Po-An Wang

<poanwang@iis.sinica.edu.tw>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1See e.g. Theorem 4.2.12 in (Hom & Johnson, 1991) for the
case of square matrices; the extension to rectangular matrices is
straightforward.

Pr

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

1
L

(cid:12)
(cid:12)
(cid:12)
z2
i − 1
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈[L]


 ≤ 2−Ω(cid:0)δ2L(cid:1)

.

≥ δ

The second is the following matrix version of the Bernstein
inequality (see e.g. Theorem 1.6 in (Tropp, 2012)).

Lemma A.4. Consider a ﬁnite sequence Z1, . . . , Zn of in-
dependent, random, matrices in Rd×k. Assume that each
random matrix satisﬁes E[Zi] = 0 and (cid:107)Zi(cid:107) ≤ R almost
surely. Deﬁne the variance parameter

σ2 = max

(cid:40)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:13)
(cid:13)
E[ZiZ (cid:62)
(cid:13)
i ]
(cid:13)
(cid:13)

,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

E[Z (cid:62)

(cid:41)

.

(cid:13)
(cid:13)
(cid:13)
i Zi]
(cid:13)
(cid:13)

Then, for all t ≥ 0,

Pr

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:35)

Zi

≥ t

≤ (d + k) · 2

−t2
σ2+Rt/3 .

We will also need the following two matrix perturbation
bounds.

Lemma A.5. (Theorem 2.5 in (Stewart & Sun, 1990)) Let
A, E ∈ Rk×k be given. If A is invertible, and (cid:13)
(cid:13) <
1 then ¯A := A + E is invertible, and

(cid:13)A−1E(cid:13)

(cid:13) ¯A−1 − A−1(cid:13)
(cid:13)

(cid:13) ≤

(cid:107)E(cid:107) (cid:13)
(cid:13)A−1(cid:13)
2
(cid:13)
1 − (cid:107)A−1E(cid:107)

.

Lemma A.6. (Lemma 2.2 in (Schmitt, 1992)) Given any
A, ¯A ∈ Rk×k with smallest singular values σ > 0 and
¯σ > 0, respectively, we have

(cid:13)
¯A
(cid:13)
(cid:13)

1
2 − A

1
2

(cid:13)
(cid:13)
(cid:13) ≤

(cid:13) ¯A − A(cid:13)
(cid:13)
(cid:13)
2 + σ 1
¯σ 1

2

.

B. Proofs in Section 3

B.1. Proof of Lemma 1

Recall that Q(t) is derived from Y (t) by the QR decom-
position Y (t) = Q(t) · R(t) via the Gram-Schmidt process,

Tensor Decomposition via Simultaneous Power Iteration

which has the same effect as performing k copies of the QR
decomposition on the k sub-matrices Y (t)
[m], for m ∈ [k], to
obtain the k sub-matrices Q(t)

[m], for m ∈ [k].

[m]

, Q(cid:48) for Q(t)

Let us ﬁx any m ∈ [k] and t ≥ 0. To simply our no-
tation, we will drop the indices of m and t in the fol-
lowing. We will write Q for Q(t−1)
[m], Y for
Y (t)
[m], and ˆΦ for ˆΦ(t)
[m]. We will write U for U[m], with
the vector u1, . . . , um as its columns, while we will use
V to denote the d × (d − m) matrix having the vectors
um+1, . . . , ud as its columns. We will write tan, cos, sin
for tanm, cosm, sinm, respectively. Furthermore, for a ma-
trix A, let σmin(A) and σmax(A) denote its smallest and
largest singular values, respectively.

Recall that our goal is to bound tan(Q(cid:48)) in terms of
tan(Q). As discussed before, Q(cid:48) is derived from Y by a
QR decomposition, with Y = Q(cid:48)R for some matrix R. To
achieve our goal, we will ﬁrst show that R is invertible so
that Q(cid:48) = Y R−1, and then relate tan(Q(cid:48)) to the singular
values σmax(V (cid:62)Y ) and σmin(U (cid:62)Y ), followed by bound-
ing these two singular values.

First, from the condition (3), we have cos Q > 0 which
implies that Q has full rank and consists of orthonormal
columns. Our key lemma is the following, which we will
prove later in Subsection B.1.1.
Lemma B.1. The following two bounds hold:

• σmin

• σmax

(cid:0)U (cid:62)Y (cid:1) ≥ λm cos2(Q) − (cid:107) ˆΦ(cid:107), and
(cid:0)V (cid:62)Y (cid:1) ≤ λm+1 sin2(Q) + (cid:107) ˆΦ(cid:107).

Using this lemma and the assumption (cid:107) ˆΦ(cid:107) ≤ (cid:52) cos2(Q)
in (3), we have

σmin

(cid:0)U (cid:62)Y (cid:1) ≥ λm cos2(Q) − (cid:52) cos2(Q) > 0,

as ∆ ≤ λm
2 . This implies that Y has full rank, R−1 exists,
Q(cid:48) = Y R−1 has orthonormal columns, and (U (cid:62)Q(cid:48))−1 ex-
ists. Then from standard properties of principal angles (see
e.g. (Zhu & Knyazev, 2012)), we know that

tan(Q(cid:48)) =

σmax(V (cid:62)Q(cid:48))
σmin(U (cid:62)Q(cid:48))

= (cid:13)

(cid:13)(V (cid:62)Q(cid:48))(U (cid:62)Q(cid:48))−1(cid:13)
(cid:13) ,

which equals
(cid:13)(V (cid:62)Y R−1)(U (cid:62)Y R−1)−1(cid:13)
(cid:13)

(cid:13) = (cid:13)

(cid:13)(V (cid:62)Y )(U (cid:62)Y )−1(cid:13)
(cid:13)
σmax(V (cid:62)Y )
σmin(U (cid:62)Y )

.

≤

Then by Lemma B.1, together with the assumption from
(3) that (cid:107) ˆΦ(cid:107) ≤ (cid:52)β = (cid:52)β sin2(Q) + (cid:52)β cos2(Q), we can
bound the denominator by

σmax(V (cid:62)Y ) ≤ (λm+1 + (cid:52)β) sin2(Q) + (cid:52)β cos2(Q),

while using the assumption (cid:107) ˆΦ(cid:107) ≤ (cid:52) cos2(Q), we can
bound the enumerator by

σmin(U (cid:62)Y ) ≥ λm cos2(Q) − (cid:52) cos2(Q).

Combining these bounds together, we obtain

tan(Q(cid:48)) ≤

λm+1 + (cid:52)β
λm − (cid:52)

tan2(Q) +

(cid:52)β
λm − (cid:52)

.

Then the rest of the analysis is identical to that of Hardt &
Price (2014) (for the proof of their Lemma 2.2). Specif-
ically, we can rewrite the righthand side above as the
weighted average of two terms

(1 − α) ·

λm+1 + (cid:52)β
λm+1 + 2(cid:52)

tan2(Q) + α · β,

with α =

λm+1+3(cid:52) , which can be upper-bounded by

(cid:52)

max

(cid:26) λm+1 + (cid:52)β
λm+1 + 2(cid:52)

(cid:27)

tan2(Q), β

,

and similarly, we can also have

λm+1 + (cid:52)β
λm+1 + 2(cid:52)

≤ max

(cid:26) λm+1

λm+1 + (cid:52)

(cid:27)

, β

.

λm+1
λm+1+(cid:52) ≤ (
Since
thus have the desired bound

λm+1

λm+1+4(cid:52) )1/4 = ( λm+1

λm

)1/4 = ρ, we

tan(Q(cid:48)) ≤ max (cid:8)max {ρ, β} tan2(Q), β(cid:9) .

To ﬁnish the proof, it remains to prove Lemma B.1, which
we do next.

B.1.1. PROOF OF LEMMA B.1

Recall from (2) that for any column Yj of Y and for any
target vector ui,

u(cid:62)
i Yj = λi

(cid:0)u(cid:62)

i Qj

(cid:1)2

+ u(cid:62)
i

ˆΦj.

These equations can be summarized as

U (cid:62)Y = Λ (cid:0)U (cid:62)Q(cid:1)(cid:12)2
V (cid:62)Y = ¯Λ (cid:0)V (cid:62)Q(cid:1)(cid:12)2

+ U (cid:62) ˆΦ, and
+ V (cid:62) ˆΦ,

using the notation Λ for the m × m diagonal matrix with
λ1, . . . , λm at its diagonal, ¯Λ for the (d − m) × (d − m)
diagonal matrix with λm+1, . . . , λd at its diagonal, and
A(cid:12)2 = A (cid:12) A for the Hadamard (entry-wise) product of
matrix A with itself. From this, we have

σmin

(cid:0)U (cid:62)Y (cid:1) = σmin

(cid:16)

(cid:16)

Λ (cid:0)U (cid:62)Q(cid:1)(cid:12)2
Λ (cid:0)U (cid:62)Q(cid:1)(cid:12)2(cid:17)

(cid:17)
+ U (cid:62) ˆΦ
(cid:13)
(cid:13)
(cid:13)U (cid:62) ˆΦ
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
ˆΦ
(cid:13)
(cid:13)
(cid:13) ,
−
(cid:13)

−
(cid:16)(cid:0)U (cid:62)Q(cid:1)(cid:12)2(cid:17)

≥ σmin

≥ σmin (Λ) σmin

Tensor Decomposition via Simultaneous Power Iteration

as well as

σmax

(cid:0)V (cid:62)Y (cid:1) = σmax

(cid:16)¯Λ (cid:0)V (cid:62)Q(cid:1)(cid:12)2
(cid:16)¯Λ (cid:0)V (cid:62)Q(cid:1)(cid:12)2(cid:17)
(cid:0)¯Λ(cid:1) σmax

(cid:17)
+ V (cid:62) ˆΦ
(cid:13)
(cid:13)V (cid:62) ˆΦ
(cid:13)
+
(cid:16)(cid:0)V (cid:62)Q(cid:1)(cid:12)2(cid:17)
+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
ˆΦ
(cid:13)
(cid:13)
(cid:13) .
(cid:13)

≤ σmax

≤ σmax

From Lemma A.2, we have
(cid:16)(cid:0)U (cid:62)Q(cid:1)(cid:12)2(cid:17)

σmin

≥ (cid:0)σmin

(cid:0)U (cid:62)Q(cid:1)(cid:1)2

= cos2(Q),

since Q has orthonormal columns, and moreover

(cid:16)(cid:0)V (cid:62)Q(cid:1)(cid:12)2(cid:17)

σmax

≤ (cid:0)σmax

(cid:0)V (cid:62)Q(cid:1)(cid:1)2

= sin2(Q).

(cid:0)¯Λ(cid:1) = λm and σmax (Λ) = λm+1, Lemma B.1

As σmin
follows.

B.2. Proof of Theorem 2

Suppose we have Q(0) such that for every m ∈ [k],
tanm(Q(0)) ≤ 1 and hence cosm(Q(0)) ≥ 1√
. We would
2
like to apply Lemma 1 repeatedly with β = ε
2 to achieve
tanm(Q(t)) ≤ ε
2 for every m. To be able to do this, we
need to verify that for every t, the condition (3) in Lemma 1
is satisﬁed. For this, we ﬁrst claim that (cid:107) ˆΦ(t)
2 . This
holds since for any vector x = (x1, . . . , xm) of unit length,
(cid:13)
ˆΦ(t)
(cid:13)
(cid:13)

(cid:13)
(cid:13)xjΦ(Id, Q(t)
(cid:13)

(cid:13)
j , Q(t)
(cid:13)
(cid:13) ≤
j )

[m](cid:107) ≤ ∆ε

|xj| · (cid:107)Φ(cid:107)

(cid:13)
(cid:13)
(cid:13) ≤

[m]x

(cid:88)

(cid:88)

j∈[m]

j∈[m]

which by the Cauchy-Schwarz inequality is at most

√

√

m(cid:107)x(cid:107) · (cid:107)Φ(cid:107) =

m · (cid:107)Φ(cid:107) ≤

∆ε
2

.

Moreover, we can assume without loss of generality that
max{ ε
2 , ρ} = ρ because otherwise, we immediately have
tanm(Q(1)) ≤ ε
2 for every m, as the condition (3) is sat-
isﬁed for t = 1. Then a simple induction shows that for
every t ≥ 1, the condition (3) in Lemma 1 holds and

tanm(Q(t)) ≤ max

(cid:111)
m(Q(t−1))

(cid:110) ε
2
(cid:110) ε
2

, ρ tan2
, ρ2t−1(cid:111)

≤ max

for every m. Thus, we have ρ2t−1 ≤ ε
ε
2 whenever t ≥ N − 1, for some

2 and tanm(Q(t)) ≤

N = O

log

= O

log

log

(cid:32)

(cid:33)

log 1
ε
log 1
ρ

(cid:18)

(cid:18) 1
γ

(cid:19)(cid:19)

,

1
ε

by noting that log 1

ρ ≥ Ω(log 1
Next, we show that for any t ≥ N , each Q(t)
i
enough to ui. For this, we rely on the following.

1−γ ) ≥ Ω(γ).

is close

Proposition B.1. For any t ≥ N , we have u(cid:62)
(cid:113)

i Q(t)

i ≥

1 − ε2

2 for every i ∈ [k].

Proof. Let us ﬁx any i ∈ [k]. In the following, we ﬁrst
i )2 ≥ 1 − ε2
i Q(t)
show that (u(cid:62)
2 for any t ≥ N − 1, and then
i Q(t)
we show that u(cid:62)
i ≥ 0 for any t ≥ N , which together
prove the proposition.

First, consider any t ≥ N − 1, which from the discus-
sion above has tani(Q(t)) ≤ ε
i )2 ≥
cos2

2 . Note that (u(cid:62)

i−1(Q(t)), because

i (Q(t)) − sin2

i Q(t)

cos2

i (Q(t)) ≤

i Q(t)

[i]

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)u(cid:62)
(cid:13)
(cid:13)
(cid:13)u(cid:62)
(cid:13)
≤ sin2

=

i Q(t)

[i−1]

(cid:16)

+

(cid:13)
2
(cid:13)
(cid:13)

i Q(t)
u(cid:62)

i

(cid:17)2

i−1(Q(t)) +

(cid:16)

i Q(t)
u(cid:62)

i

(cid:17)2

.

From this and the fact that cos2
sin2

i−1(Q(t)) ≤ tan2

i−1(Q(t)), we get

i (Q(t)) =

1
1+tan2

i (Q(t)) and

(cid:16)

i Q(t)
u(cid:62)

i

(cid:17)2

≥

1
1 + ε2
4

−

≥ 1 −

ε2
4

ε2
2

.

Next, consider any t ≥ N and our goal is to show that
i Q(t)
u(cid:62)

i > 0. Recall that Q(t)

is derived from

i

Y (t)
i =

(cid:88)

(cid:16)

λj

(cid:17)2

j Q(t−1)
u(cid:62)

i

· uj + ˆΦ(t−1)

i

j

by subtracting from it its projection to some unit vector z
in the column space of Q(t)
[i−1] and then scaling it to unit
i Q(t)
length. Thus, the sign of u(cid:62)
is the same as that of
(cid:17)

(cid:16)

(cid:16)

(cid:17)

i

u(cid:62)
i

Y (t)
i −

z(cid:62)Y (t)
i
(cid:16)

z

= u(cid:62)

i −

i Y (t)
(cid:16)

≥ λi

z(cid:62)Y (t)
i
(cid:17)2

−

i z(cid:1)

(cid:17) (cid:0)u(cid:62)
(cid:13)
ˆΦ(t−1)
(cid:13)
(cid:13)
i

(cid:18)

i Q(t−1)
u(cid:62)
i
(cid:19)
ε2
2

1 −

−

5ε
8

,

≥ λi

(cid:13)
(cid:13) − sini−1(Q(t))
(cid:13)

i

)2 ≥ 1− ε2

2 for t−1 ≥ N −1, (cid:107) ˆΦ(t−1)

i Q(t−1)
since (u(cid:62)
(cid:107) ≤
i
2 ≤ ε
8 , and sini−1(Q(t)) ≤ tani−1(Q(t)) ≤ ε
∆ε
2 for t ≥
N . Finally, as λi ≥ λk ≥ 2ε, the last line above is positive,
which implies that u(cid:62)
i > 0.

i Q(t)

As (cid:107)Q(t)
plies that for any t ≥ N and i ∈ [k],

i (cid:107) = (cid:107)ui(cid:107) = 1, this proposition immediately im-

(cid:13)
(cid:13)Q(t)
(cid:13)

i − ui

(cid:13)
(cid:13)
(cid:13) =

(cid:113)

2 − 2u(cid:62)

i Q(t)

i ≤ ε,

Tensor Decomposition via Simultaneous Power Iteration

as u(cid:62)

i Q(t)

i ≥ (u(cid:62)

i Q(t)

i )2 ≥ 1 − ε2
2 .

Finally, let us show that for any t ≥ N , each λi can be
approximated well by ˆλi = ¯T (Q(t)
, Q(t)
i ). Fix any
i
t ≥ N and i ∈ [k]. Note that |λi − ˆλi| is at most

, Q(t)
i

(cid:12)
(cid:12)
(cid:12)λi − T

(cid:16)

Q(t)
i

, Q(t)
i

, Q(t)
i

(cid:17)(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)Φ

(cid:16)

Q(t)
i

, Q(t)
i

, Q(t)
i

(cid:17)(cid:12)
(cid:12)
(cid:12) .

The second term above is at most (cid:107)Φ(cid:107)(cid:107)Q(t)
the ﬁrst term above is at most
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j(cid:54)=i
(cid:17)3(cid:19)

i Q(t)
u(cid:62)

j Q(t)
u(cid:62)

(cid:17)3(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)3(cid:12)
(cid:12)
(cid:12)
(cid:12)

λi − λi

(cid:88)

λj

+

(cid:18)

(cid:18)

(cid:16)

(cid:16)

(cid:16)

i

i

(cid:88)

≤ λi

1 −

i Q(t)
u(cid:62)

i

+

λj

1 −

j(cid:54)=i

i (cid:107)3 ≤ ε

4 , and

(cid:17)2(cid:19)

(cid:16)

i Q(t)
u(cid:62)

i

≤ λi

3ε2
4

+

(cid:88)

j(cid:54)=i

λj

ε2
2

,

where the ﬁrst inequality uses the fact that for j (cid:54)= i,
i )2 ≤ (cid:107)Q(t)
j Q(t)
(u(cid:62)
i )2 =
i )2, while the second inequality uses the bound
1 − (u(cid:62)
i Q(t)
(u(cid:62)
4 . As a result, we have

i )3 ≤ (u(cid:62)
i Q(t)
i )3 ≥ (1 − ε2

i (cid:107)2 − (u(cid:62)

i Q(t)

j Q(t)

2 )3/2 ≥ 1 − 3ε2
3ε2
(cid:12)
(cid:12)
(cid:12) ≤
4

(cid:88)

λj

j

(cid:12)
(cid:12)λi − ˆλi
(cid:12)

+

≤ ε,

ε
4

using the assumption (cid:80)
the proof of the theorem.

B.3. Proof of Lemma 2

From the deﬁnition of ¯w, we can express u(cid:62)

i ¯w as

1
L

(cid:88)

j∈[L]

1
L

(cid:88)

j∈[L]

T (ui, wj, wj) +

Φ(ui, wj, wj).

(1)

The ﬁrst term above equals

1
L

(cid:88)

j∈[L]

λi

(cid:0)u(cid:62)

i wj

(cid:1)2

= λi ·

(cid:88)

(cid:0)u(cid:62)

i wj

(cid:1)2

,

1
L

j∈[L]

and note that the sum has a χ-square distribution be-
cause each u(cid:62)
i wj is an independent random variable with
the standard normal distribution N (0, 1).2 Then from
Lemma A.3, we know that for δ = γ
4 , there exists some
L ≤ O( 1
γ2 log d) such that the ﬁrst term in (1) differs from
λi by at most λiγ
4 with probability at least 1 − 1

200d2 .

The second term in (1) can be bounded in a similar way
as follows. Since (cid:107)ui(cid:107) = 1 and Φ is symmetric, we know

2This is because each component of wj has the distribution
i wj has mean (cid:80)
r ui,r · 0 = 0
i,r · 1 = 1, where ui,r denotes the r’th com-

N (0, 1), and the distribution of u(cid:62)
and variance (cid:80)
r u2
ponent of ui.

r∈[d]

3d and can be decomposed as (cid:80)

that the matrix Φ(ui, Id, Id) has norm (cid:107)Φ(ui, Id, Id)(cid:107) ≤
˜λr · ˜ur ⊗ ˜ur,
(cid:107)Φ(cid:107) ≤ ∆
for some orthonormal vectors ˜ur’s as well as some values
˜λr’s, each with |˜λr| ≤ ∆
3d . Then by a similar analysis as
above, together with a union bound, we can have with prob-
200d2 = 1 − 1
ability at least 1 − d ·
(cid:12)
(cid:12)
(cid:12)
Φ(ui, wj, wj)
(cid:12)
(cid:12)
(cid:12)

200d that

(cid:0)˜u(cid:62)

r wj

(cid:12)
(cid:12)
(cid:12) ·

1
L

1
L

(cid:88)

(cid:88)

(cid:88)

j∈[L]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜λr

r∈[d]

(cid:1)2

≤

(cid:12)
(cid:12)
(cid:12)

1

(cid:16)

·

1 +

∆
3d

j∈[L]
(cid:17)
γ
4

≤

≤

(cid:88)

r∈[d]
∆
2

.

By combining the two bounds above, we can conclude that
for any i ∈ [d], the sum in (1) differs from λi by at most
4 (λiγ + 2∆) with probability at least 1 − 1
1
100d . Then the
lemma immediately follows by a union bound.

B.4. Proof of Lemma 3

Consider any ¯w satisfying the condition (5) in Lemma 2.
By deﬁnition, ¯M = ¯T (Id, Id, ¯w) can be decomposed as

T (Id, Id, ¯w) + Φ(Id, Id, ¯w).

T (Id, Id, ¯w) =

λi

(cid:0)u(cid:62)

i ¯w(cid:1) · ui ⊗ ui,

(cid:88)

i∈[d]

with ¯λi = λi(u(cid:62)
vector, respectively. Note that as ∆ ≤ λi−λi+1

i ¯w) and ui as its i’th eigenvalue and eigen-

, we have

4

¯λi ≥ λ2

i −

≥ λ2

i −

≥ λ2

i −

i γ + 2λi∆(cid:1)
λ2
i − λiλi+1
8

i+1

−

(cid:0)λ2

1
4
i − λ2
λ2
4
3 (cid:0)λ2
i − λ2
8

i+1

(cid:1)

,

as well as

¯λi+1 ≤ λ2

i+1 +

(cid:0)λ2

1
4

3 (cid:0)λ2

i+1

i+1γ + 2λi+1∆(cid:1)
i − λ2
λ2
4λ2
i
i − λ2
8

i+1

+

(cid:1)

,

≤ λ2

i+1 + λ2

i+1

≤ λ2

i+1 +

λi+1λi − λ2
8

i+1

which together imply that

¯λi − ¯λi+1 ≥

i − λ2
λ2
4

i+1

≥ ∆2.

j λj ≤ 1 from (1). This completes

The ﬁrst matrix can be expressed as

Tensor Decomposition via Simultaneous Power Iteration

It remains to bound the norm of Φ(Id, Id, ¯w), which is

(cid:107)Φ(Id, Id, ¯w)(cid:107) ≤ (cid:107)Φ(cid:107) · (cid:107) ¯w(cid:107),

where (cid:107) ¯w(cid:107)2 = (cid:80)

(cid:0)u(cid:62)

i ¯w(cid:1)2

i∈[d]

is at most

(cid:18)

(cid:88)

i∈[d]

1
4

(cid:19)2

(cid:88)

i∈[d]

λi +

(λiγ + 2∆)

≤

(2λi)2 ≤ 4.

This implies that (cid:107)Φ(Id, Id, ¯w)(cid:107) ≤ 2(cid:107)Φ(cid:107), which completes
the proof of the lemma.

B.5. Proof of Lemma 4
Suppose we have a matrix ¯M = M + ¯Φ, where

M =

¯λi · ui ⊗ ui

(cid:88)

i∈[d]

√

with ¯λi − ¯λi+1 ≥ ∆2 for every i ∈ [k] and (cid:107) ¯Φ(cid:107) ≤ α1∆2
dk
for a small enough constant α1. The key observation is that
although we run one copy of the matrix power method of
Hardt & Price (2014) to update the whole d×k matrix Z (s),
we can actually see our algorithm as running k copies of
the matrix power method on k sub-matrices Z (s)
[1] , . . . , Z (s)
[k]
simultaneously. This allows us to apply their analysis im-
mediately.

More precisely, although our QR decomposition at each
step s is applied to the whole d × k matrix Y (s) to ob-
tain our d × k matrix Z (s), the Gram-Schmidt process we
use has the effect that each d × m sub-matrix Z (s)
[m] can also
be seen as obtained from the d × m matrix Y (s)
[m] by a QR
decomposition. Thus, our algorithm can be seen as running
k copies of the algorithm of (Hardt & Price, 2014) simulta-
neously, and we can apply the following lemma of theirs3
simultaneously for every m ∈ [k] with X (s) being our d×k
matrix Z (s)
[m].

Lemma B.2. Fix any m ∈ [k]. Suppose that the initial
X (0) and the noise G(s)
m = ¯Φ · X (s) at each step s is such
that

5

(cid:13)
[m]G(s)
(cid:13)U (cid:62)
(cid:13)
m
(cid:13)
(cid:13)G(s)
(cid:13)
5

m

(cid:13)
(cid:13) ≤ (cid:0)¯λm − ¯λm+1
(cid:13)
(cid:13)
(cid:13) ≤ (cid:0)¯λm − ¯λm+1
(cid:13)

(cid:1) cosm(X (0))
(cid:1) (cid:15)

for some (cid:15) < 1

2 . Then for γm = 1 −

¯λm+1
¯λm

, there exists

some S = O( 1
γm
we have tanm(X (t)) ≤ (cid:15).

log tanm(X (0))

(cid:15)

) such that for any t ≥ S

3It corresponds to Theorem 2.3 in (Hardt & Price, 2014). Al-
though it is stated there for m = k, it in fact works for any value
of k and hence m.

It remains to show that we can have an initial Z (0), such
that for each m ∈ [k], Z (0)
[m] satisﬁes the condition required
by Lemma B.2. For this, we need the following bound from
(Mitliagkas et al., 2013).
Proposition B.2. For any δ, we have

(cid:20)
cosk(Z (0)) ≤

Pr

(cid:21)

δ
√
dk

≤ O(δ) + 2−Ω(d).

By applying this proposition, with δ = 10α0 for a small
enough constant α0, we can have cosk(Z (0)) ≥ 10α0√
dk
with high probability. From Lemma A.1, we know that
for any m ∈ [k], cosm(Z (0)) = σmin(U (cid:62)
[m]) ≥
σmin(U (cid:62)Z (0)) = cosk(Z (0)). Thus, with high probabil-
ity we in fact have cosm(Z (0)) ≥ 10α0√
for every m ∈ [k].
dk
Given such an initial Z (0), we can have for every s and m
that

[m]Z (0)

5(cid:107)G(s)

m (cid:107) ≤ 5(cid:107) ¯Φ(cid:107) ≤

10α0∆2
√
dk

which satisﬁes the two conditions needed by Lemma B.2,
with (cid:15) = 1
3 . Then we can repeatedly apply Lemma B.2,
simultaneously for every m ∈ [k], and a simple induc-
tion shows that for some S = O( 1
γ log d), we have
tanm(Z (s)) ≤ (cid:15) < 1 for any m ∈ [k] and s ≥ S. This
completes the proof of our Lemma 4

C. Proofs in Section 5

C.1. Proof of Lemma 5

First, we claim that tank(Q) < 1 with high probability.
To show this, note that by Proposition B.2, we have with
high probability that cosk(Z) > 4α0√
for a small enough
dk
constant α0. In the following, let us assume that we indeed
dk
have such a matrix Z, and note that it has tank(Z) <
.
4α0
Then we need the following.
Lemma C.1. (Lemma 2.2 in (Hardt & Price, 2014)) Let
Z, G ∈ Rd×k satisfy
(cid:13)U (cid:62)G(cid:13)
4 (cid:13)

(cid:13) ≤ (λk − λk+1) cosk(Z)

√

4 (cid:107)G(cid:107) ≤ (λk − λk+1) β

for some β < 1. Then for ρ =
tank(M Z + G) ≤ max{β, max{β, ρ} tank(Z)}.

(cid:17)1/4

(cid:16) λk+1
λk

, we have

Recall that we assume λk+1 = 0, and to apply the lemma,
let β = 4α0√
dk

and G = ¯ΦZ. Note that

(cid:107)G(cid:107) ≤ (cid:13)

(cid:13) ¯Φ(cid:13)

(cid:13) ≤

λkβ
4

,

which satisﬁes both requirements of the lemma, and thus
with ¯Y = M Z + G, we have

tank(Q) = tank( ¯Y ) ≤ β tank(Z) < 1.

Tensor Decomposition via Simultaneous Power Iteration

Next, let us bound σmin(P ) and σmax(P ). Recall that

P = Q(cid:62)M Q = Q(cid:62)U ΛU (cid:62)Q,

• σmin(P ) ≥ λk

2 , and

• (cid:107) ¯P − 1

2 − P − 1

2 (cid:107) ≤ λkε
64 .

which implies that

σmin(P ) ≥ σmin

σmax(P ) ≤ σmax

(cid:0)Q(cid:62)U (cid:1) σmin (Λ) σmin
(cid:0)Q(cid:62)U (cid:1) σmax (Λ) σmax

(cid:0)U (cid:62)Q(cid:1) and
(cid:0)U (cid:62)Q(cid:1) .

Since the matrix Q has orthonormal columns, we have

(σmin

(cid:0)U (cid:62)Q(cid:1))2 = (cosk(Q))2 =

1

1 + tan2

k(Q)

≥

1
2

,

as well as

Assume from now on that the above three conditions hold.
Next, observe that

(cid:13) ¯T (cid:0) ¯W , ¯W , ¯W (cid:1) − T (W, W, W )(cid:13)
(cid:13)
(cid:13)

≤ (cid:13)
(cid:13) ¯T (cid:0) ¯W , ¯W , ¯W (cid:1) − T (cid:0) ¯W , ¯W , ¯W (cid:1)(cid:13)
(cid:13) +
(cid:13)T (cid:0) ¯W , ¯W , ¯W (cid:1) − T (cid:0)W, ¯W , ¯W (cid:1)(cid:13)
(cid:13)
(cid:13) +
(cid:13)T (cid:0)W, ¯W , ¯W (cid:1) − T (cid:0)W, W, ¯W (cid:1)(cid:13)
(cid:13)
(cid:13) +
(cid:13)T (cid:0)W, W, ¯W (cid:1) − T (W, W, W )(cid:13)
(cid:13)
(cid:13) .

(2)

(3)

(4)

(5)

σmax

(cid:0)U (cid:62)Q(cid:1) ≤ (cid:107)U (cid:107) (cid:107)Q(cid:107) = 1.

The term in (2) is at most

Finally, as σmax (Λ) = λ1 and σmin (Λ) = λk, we have

σmin(P ) ≥

and σmax(P ) ≤ λ1.

λk
2

C.2. Proof of Lemma 6

First, from the deﬁnition, we have

(cid:13) ¯P − P (cid:13)
(cid:13)

(cid:13) = (cid:13)

(cid:13)Q(cid:62) ¯M Q − Q(cid:62)M Q(cid:13)

(cid:13) ≤ (cid:107)Q(cid:107)2 (cid:13)

(cid:13) ¯Φ(cid:13)

(cid:13) ≤ (cid:15)

as Q has orthonormal columns so that (cid:107)Q(cid:107)2 ≤ 1. There-
fore, given the assumption that 0 < (cid:15) ≤ σmin(P )
, we have
σmin( ¯P ) ≥ σmin(P ) − (cid:13)
which implies that ¯P is invertible.

(cid:13) ¯P − P (cid:13)

(cid:13) > 0,

2

Then according to Lemma A.5, we have

(cid:13) ¯P −1 − P −1(cid:13)
(cid:13)

(cid:13) ≤

(cid:13) ¯P − P (cid:13)
(cid:13)
(cid:13)
(cid:13)P −1(cid:13)
2
(cid:13)
(cid:13)
(cid:13) ¯P − P (cid:13)
1 − (cid:13)
(cid:13) (cid:107)P −1(cid:107)

≤ 2(cid:15)(σmin(P ))−2,

(cid:13)P −1(cid:13)

as (cid:13)
Combining this with Lemma A.6, we obtain

(cid:13) = (σmin(P ))−1 and (cid:107) ¯P − P (cid:107) ≤ (cid:15) ≤ σmin(P )

2

.

(cid:13)
¯P − 1
(cid:13)
(cid:13)

2 − P − 1

2

(cid:13)
(cid:13)
(cid:13) ≤

(cid:13) ¯P −1 − P −1(cid:13)
(cid:13)
(cid:13)
2 + (σmin(P −1)) 1

2

(σmin( ¯P −1)) 1

≤ 2(cid:15)(σmin(P ))−2(σmax(P ))

1
2 ,

since σmin( ¯P −1) ≥ 0 and σmin(P −1) = (σmax(P ))−1.

C.3. Proof of Theorem 3

First, given ε ∈ (0, 1
},
for a small enough constant α0, we know from Lemma 5
and Lemma 6 that with high probability,

2 ) and (cid:107) ¯Φ(cid:107) ≤ α0ε min{ λk√

dk

, λ3
k√
λ1

• σmax(P ) ≤ λ1,

(cid:13)Φ (cid:0) ¯W , ¯W , ¯W (cid:1)(cid:13)
(cid:13)

(cid:13) ≤ (cid:107)Φ(cid:107) (cid:13)

(cid:13) ¯W (cid:13)
3
(cid:13)

≤

ε
4

as (cid:107)Φ(cid:107) ≤ α0λ

3
2

k ε for a small enough constant α0 and
(cid:13)
(cid:13)
(cid:13) ≤

(cid:13)
¯P − 1
(cid:13)
(cid:13)

(cid:13)
¯P − 1
(cid:13)
(cid:13)

(cid:13) ≤ (cid:107)Q(cid:107)

(cid:13)
(cid:13)
(cid:13) ,

(cid:13) ¯W (cid:13)
(cid:13)

2

2

which can be upper-bounded by

(cid:13)
(cid:13)P − 1
(cid:13)

2

(cid:13)
(cid:13)
(cid:13) +

(cid:13)
¯P − 1
(cid:13)
(cid:13)

2 − P − 1

2

(cid:13)
(cid:13) ≤ 4λ− 1
(cid:13)
k .

2

The term in (3) is at most
(cid:13)T (cid:0) ¯W − W, ¯W , ¯W (cid:1)(cid:13)
(cid:13)

(cid:13) ¯W − W (cid:13)
(cid:13) ¯W (cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2 − P − 1
(cid:13) 16λ−1
(cid:13)

k

2

(cid:13) ≤ (cid:107)T (cid:107) (cid:13)
(cid:13)
¯P − 1
(cid:13)
(cid:13)
ε
4

≤

≤

.

Similarly, the term in (4) can be upper-bounded by

(cid:107)T (cid:107) (cid:13)

(cid:13) ¯W − W (cid:13)

(cid:13) (cid:107)W (cid:107) (cid:13)

(cid:13) ¯W (cid:13)

(cid:13) ≤

and the term in (5) can be upper-bounded by

(cid:107)T (cid:107) (cid:13)

(cid:13) ¯W − W (cid:13)

(cid:13) (cid:107)W (cid:107)2 ≤

As a result, we can conclude that

(cid:13) ¯T (cid:0) ¯W , ¯W , ¯W (cid:1) − T (W, W, W )(cid:13)
(cid:13)

(cid:13) ≤ ε

with high probability, which proves the theorem.

ε
4

ε
4

.

D. Proofs in Section 6

Our streaming algorithm for orthogonal tensors with g of
the form g(x) = x ⊗ x ⊗ x is summarized in Algorithm 2.
We will use the parameters

L =

c0 log k
∆2

, S =

c0 log k
γ

, N = c0 log

log

(cid:18) 1
γ

(cid:19)

1
ε

Tensor Decomposition via Simultaneous Power Iteration

Algorithm 2 Streaming robust tensor power method

a stream of data {x1, x2, . . . , }, parameters

s=1 , {Jt}N

t=1.

T = Ex[x ⊗ x ⊗ x], we also have
(cid:104)
Ex

Ew[T (Id, w, w)] = Ew

Input:
L, S, N , index sets {Bs}S
Initialization Phase
Let ¯w = 0 ∈ Rd.
for τ = 1 to L do

Update ¯w = ¯w + 1

L xτ .

1

, . . . , Y (0)

end for
Sample Y (0)
Factorize Y (0) as Z (0)R(0) by QR decomposition.
for s = 1 to S do
for τ ∈ Bs do

k ∼ N d (0, 1).

Update Y (s) = Y (s) + 1
|Bs|

(cid:0)x(cid:62)

τ ¯w(cid:1) xτ x(cid:62)

τ Z (s−1).

end for
Factorize Y (s) as Z (s)R(s) by QR decomposition.

end for
Tensor power phase
Let Q(0) = Z (S).
for t = 1 to N do
for τ ∈ Jt do
Update Y (t)

i = Y (t)
i = λ(t)

i + 1

|Jt| xτ
(cid:16)

i + 1
|Jt|

(cid:16)

τ Q(t)
x(cid:62)
i
(cid:17)3

τ Q(t)
x(cid:62)

i

(cid:17)2

, ∀i ∈ [k].

, ∀i ∈ [k].

Update λ(t)

end for
Factorize Y (t) as Q(t)R(t) by QR decomposition.

end for
Output: ˆui = Q(N )

i

and ˆλi = λ(N )

i

, ∀i ∈ [k]

for a large enough constant c0. Moreover, we partition the
time steps into consecutive blocks: with the ﬁrst block [L]
for ﬁnding the vector ¯w, the next S blocks B1, . . . , BS for
the matrix power method in the initialization phase, fol-
lowed by N blocks J1, . . . , JN for the tensor power phase,
with their sizes |Bs| and |Jt| given in (6) and (6) respec-
tively. The proofs of related lemmas in Section 6 are given
next.

D.1. Proof of Lemma 7

First, from the assumption that T = Ex[x ⊗ x ⊗ x], where
Ex[·] denotes the expectation over the distribution of x, we
have the following.
Proposition D.1. Ex[(cid:107)x(cid:107)2x] = (cid:80)

i∈[d] λiui.

Proof. Recall that if we sample w according to the dis-
then for any u ∈ Rd, we have
tribution N d(0, 1),
Ew[(u(cid:62)w)2] = (cid:107)u(cid:107)2, where Ew[·] denotes the expectation
over w. Then we have

Ew[T (Id, w, w)] =

(cid:88)

λiEw

(cid:104)(cid:0)u(cid:62)

i w(cid:1)2(cid:105)

ui =

i∈[d]

(cid:88)

i∈[d]

λiui,

as (cid:107)ui(cid:107)2 = 1. On the other hand, from the assumption that

(cid:104)(cid:0)x(cid:62)w(cid:1)2
(cid:104)(cid:0)x(cid:62)w(cid:1)2

(cid:105)(cid:105)

(cid:105)(cid:105)

x

x

(cid:104)

= Ex
= Ex

Ew
(cid:2)(cid:107)x(cid:107)2x(cid:3) .

The proposition follows by combining these two equalities.

This suggests us to take ¯w = 1
τ =1((cid:107)xτ (cid:107)2xτ ), for some
L
L to be determined next. This is because for any i ∈ [k],
the random variable zτ = u(cid:62)
i ((cid:107)xτ (cid:107)2xτ ) falls in [−1, 1] and
has expected value

(cid:80)L

E[zτ ] = u(cid:62)
i

λiui = λi

(cid:88)

i∈[d]

for each τ , so that for δ = 1

Pr (cid:2)(cid:12)

(cid:12)u(cid:62)

i ¯w − λi

(cid:12)
(cid:12) > δ(cid:3) = Pr

4 (λiγ + 2∆),
(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

L
(cid:88)

1
L

τ =1

zτ − λi

> δ

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

which by Hoeffding inequality is at most

2−Ω(cid:0)δ2L(cid:1)

≤

1
100k

δ2 ) = O( 1
for some L = O( 1
∆2 log k). Then by a union
bound, with probability 0.99 we have ¯w satisfying |u(cid:62)
i ¯w −
λi| ≤ δ for every i ∈ [k]. As ¯w can clearly be computed in
O(d) space, the lemma follows.

D.2. Proof of Lemma 8

The streaming algorithm for this lemma can be found in the
initialization phase of our Algorithm 2, which is based on
that of Li et al. (2016).

Recall that Li et al. (2016) considered the matrix case,
in which each vector xτ in the stream has the expecta-
tion E[xτ ⊗ xτ ] = M for some d × d matrix M to
be decomposed. To apply their result, let us make the
connection by seeing T (Id, Id, ¯w) as their matrix M and
Mτ = g(xτ )(Id, Id, ¯w) = (x(cid:62)
τ ¯w) · xτ ⊗ xτ as their esti-
mator xτ ⊗ xτ , by noting that

E[Mτ ] = E[g(xτ )(Id, Id, ¯w)] = E[g(xτ )](Id, Id, ¯w) = M.

Since (cid:107) ¯w(cid:107) ≤ 1, (cid:107)Mτ (cid:107) ≤ (cid:107) ¯w(cid:107)(cid:107)xτ (cid:107)3 ≤ 1, and (cid:107)M (cid:107) ≤
(cid:107)T (cid:107)(cid:107) ¯w(cid:107) ≤ 1, we have

(cid:107)Mτ − M (cid:107) ≤ (cid:107)Mτ (cid:107) + (cid:107)M (cid:107) ≤ 2.

Thus, we have from the matrix Bernstein inequality
(Lemma A.4) that
(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ 2d2−Ω(δ2|B|),

Mτ − M

1
|B|

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≥ δ

(cid:88)

Pr

(cid:35)

τ ∈B

Tensor Decomposition via Simultaneous Power Iteration

for any block B of time steps, and this allows us to apply
the analysis of (Li et al., 2016).

as its j’th column, so that ˆΦ(t) = 1
|Jt|
we have E[Aτ ] = 0 for each τ , because

(cid:80)

Aτ . Note that

τ ∈Jt

Following (Li et al., 2016), we use the parameters

εs = ε0ρs and βs = min

ρ/

1 + ε2

s−1, ρεs−1

,

(cid:26)

(cid:113)

(cid:27)

√

dk
α0

with ε0 =
the time steps into S = O( 1
block Bs having size

for a small enough constant α0, and divide
γ log d) blocks, with the s’th

|Bs| ≤

c0 log(ds)
∆4β2
s

,

(6)

for a large enough constant c0. Then according to the anal-
ysis in (Li et al., 2016) together with that in our proof of
Lemma 4, one can show that tanm(Z (s)) ≤ εs for every
s ≤ S, so that we can have tanm(Z (S)) ≤ 1, for ev-
ery m ∈ [k]. Moreover, from the analysis in (Li et al.,
2016), we know that the number of samples needed can be
bounded by

S
(cid:88)

s=1

|Bs| ≤ O

(cid:18) ε2

0 log(dS)
∆4γ

(cid:19)

= O

(cid:18) dk log(dS)
∆4γ

(cid:19)

.

the matrix product
Finally, note that for each update,
Mτ Z (s−1) equals (x(cid:62)
τ Z (s−1), which can be com-
puted in O(kd) space. Thus, the algorithm works in O(kd)
space, and the lemma follows.

τ ¯w)xτ x(cid:62)

D.3. Proof of Theorem 4

According to Lemma 7 and Lemma 8, let us assume that
we have obtained some Z ∈ Rd×k such that tanm(Z) < 1
for every m ∈ [k]. Now let us focus on the tensor power
phase.

Consider a ﬁxed iteration t. We would like to show that
tanm(Q(t)) ≤ βt with high probability, using Lemma 1.
For this, we need to show that the condition (3) there is
satisﬁed with high probability. For j ∈ [k], let qj denote
Q(t−1)
j = Φ(Id, qj, qj), which now
j
equals

, and recall that ˆΦ(t)

1
|Jt|

(cid:88)

τ ∈Jt

(cid:0)x(cid:62)

τ qj

(cid:1)2

xτ − T (Id, qj, qj) .

Let ˆΦ(t) be the d × k matrix with ˆΦ(t)
j
Then we have the following.
Lemma D.1. (cid:107) ˆΦ(t)(cid:107) > ∆βt

2 with probability at most

1
200t2 .

as its j’th column.

Proof. Let us see ˆΦ(t) as the average of |Jt| i.i.d. random
matrices, so that we can apply the matrix Bernstein inequal-
ity (Lemma A.4). More precisely, for τ ∈ Jt, let Aτ denote
the d × k matrix with

(cid:0)x(cid:62)

τ qj

(cid:1)2

xτ − T (Id, qj, qj)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:104)(cid:0)x(cid:62)
τ qj

E

(cid:1)2

xτ

(cid:105)

= E [(xτ ⊗ xτ ⊗ xτ ) (Id, qj, qj)]

= (E [xτ ⊗ xτ ⊗ xτ ]) (Id, qj, qj)
= T (Id, qj, qj) .

Moreover, we claim that (cid:107)Aτ (cid:107) ≤ 2. This is because for any
v = (v1, . . . , vk) ∈ Rk with (cid:107)v(cid:107) = 1, (cid:107)Aτ v(cid:107) is at most
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

vjT (Id, qj, qj)

(cid:0)x(cid:62)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

τ qj

(cid:88)

(cid:88)

j∈[k]

j∈[k]

xτ

vj

(cid:1)2

+

where the ﬁrst term above is at most

(cid:88)

|vj| (cid:0)x(cid:62)

τ qj

(cid:1)2

(cid:107)xτ (cid:107) ≤

(cid:88)

(cid:0)x(cid:62)

τ qj

(cid:1)2

≤ (cid:107)xτ (cid:107) ≤ 1

j∈[k]

j∈[k]

as the qj’s are orthonormal, while the second term above is

(cid:88)

(cid:88)

vj

λi

(cid:0)u(cid:62)

i qj

(cid:1)2

ui

j∈[k]

i∈[d]

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

which can be upper-bounded by

(cid:88)

λi

(cid:88)

vj

(cid:0)u(cid:62)

i qj

(cid:1)2

ui

(cid:88)

≤

λi ≤ 1

i∈[d]

j∈[k]

i∈[d]

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

using a similar argument and the assumption that
(cid:80)
i∈[d] λi ≤ 1. Then we can apply Lemma A.4, and con-

clude that

Pr

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
|Jt|

(cid:88)

τ ∈Jt

Aτ

>

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:35)

∆βt
2

≤

1
200t2

for our choice of |Jt|.

1

tan2

m(Q(0)) =

m(Q(0))+1 ≥ 1

Note that (cid:107) ˆΦ(t)
[m](cid:107) ≤ (cid:107) ˆΦ(t)(cid:107) for any m ∈ [k], and recall that
we start with cos2
2 . There-
fore, given this lemma, we can then apply Lemma 1 re-
peatedly and a simple induction shows that the probabil-
ity that tanm(Q(t)) > βt for some m and t is at most
(cid:80)
200t2 ≤ 0.01. Thus, with high probability we have
t
tanm(Q(t)) ≤ βt for every m and t. Let N be the number
2 for t ≤ N − 2 and β = ε
such that βt > ε
2 for t ≥ N − 1.
Note that

1

N = O

log

= O

log

log

(cid:32)

(cid:33)

log 1
ε
log 1
ρ

(cid:18)

(cid:18) 1
γ

(cid:19)(cid:19)

,

1
ε

and recall from the proof of Theorem 2 that from Q(N ) we
can obtain the required ˆui’s and ˆλi’s.

Tensor Decomposition via Simultaneous Power Iteration

Tropp, Joel A. User-friendly tail bounds for sums of ran-
dom matrices. Foundations of Computational Mathe-
matics, 12(4):389–434, 2012.

Zhu, Peizhen. and Knyazev, Andrew V. Angles be-
tween subspaces and their tangents. Arxiv preprint at
arXiv:1209.0523, 2012.

It remains to bound the number of samples needed in this
phase, which is

N
(cid:88)

t=1

|Jt| ≤ O

(cid:18) log(dN )
∆2

(cid:19) N
(cid:88)

t=1

1
β2
t

.

As βt = max{ρ2t−1, ε
and βt = βN −2ρ2t−2N −2
Therefore,

2 }, we have βt = ε
2 ρ2t−2N −2

≥ ε

2 for t ≥ N − 1
for t ≤ N − 2.

N
(cid:88)

t=1

1
β2
t

≤

8
ε2 +

4
ε2

N −2
(cid:88)

t=1

ρ2(2N −2−2t) ≤ O

(cid:18)

1
ε2(1 − ρ4)

(cid:19)

,

1

1−ρ4 = 1+ρ4

1−ρ8 ≤
= mini∈[k]

λ2
i+1
λ2
i

2

1−ρ8 and 1 − ρ8 = 1 −
i −λ2
λ2
= γ. As a result, we
λ2
i

i+1

where

maxi∈[k]
have

N
(cid:88)

t=1

|Jt| ≤ O

(cid:18) log(dN )
∆2γε2

(cid:19)

.

Combining this with the number of samples for the initial-
ization phase, including that for ﬁnding ¯w, we have the
stated sample complexity bound of the theorem.

References

Hardt, Moritz and Price, Eric. The noisy power method: A
meta algorithm with applications. In Advances in Neural
Information Processing Systems, pp. 2861–2869, 2014.

Hom, Roger A and Johnson, Charles R. Topics in matrix
analysis. Cambridge University Press, New York, 1991.

Laurent, B. and Massart, P. Adaptive estimation of a
Annals of

quadratic functional by model selection.
Statistics, 28(5):1302–1338, 2000.

Li, Chun-Liang, Lin, Hsuan-Tien, and Lu, Chi-Jen. Rivalry
of two families of algorithms for memory-restricted
streaming PCA. In Proceedings of the 19th International
Conference on Artiﬁcial Intelligence and Statistics (AIS-
TATS), pp. 473–481, 2016.

Mitliagkas, Ioannis, Caramanis, Constantine, and Jain, Pra-
teek. Memory limited, streaming PCA. In Advances in
Neural Information Processing Systems, pp. 2886–2894,
2013.

Schmitt, Bernhard A.

Perturbation bounds for matrix
square roots and pythagorean sums. Linear algebra and
its applications, 174:215–227, 1992.

Stewart, Gilbert W. and Sun, Ji-Guang. Matrix Perturba-

tion Theory. Academic Press, 1990.

