Gradient Boosted Decision Trees for High Dimensional Sparse Output

heap Q. The algorithm scans through all the elements in
the prediction vectors (each contains k (idx, value) pairs)
for each tree h1(xi), · · · , hm(xi). For each (idx, value)
pair, we ﬁrst use hash to add the value to the corresponding
index pidx.
If the index is already in the heap, then up-
date the corresponding value. If Q.size() is smaller than
B, then add the (idx, pidx) pair to Q. Otherwise compare
pidx with Q.min(), and replace the minimum number in Q
by pidx if pidx is larger than Q.min(). Since the size of Q
is always ≤ B, the complexity is O(T k log B).

A. Additional Experiments on DisMEC

DisMEC (Babbar & Sch¨olkopf, 2017) is a distributed ex-
treme multi-label learning framework based on one-versus-
rest linear classiﬁers with explicit model size controlled
by pruning small weights. Unlike other methods we have
compared in Section 5, DisMEC mainly focuses on paral-
lelizing an extremely large number of one-versus-all clas-
siﬁers in large-scale distributed settings with double layers
of parallelization (multi-core and multi-machine).

DisMEC’s primarily advantage is that it does not make any
low-rank or sparsity assumption for the data and thus pre-
diction performance is better, and its model size is reason-
ably small due to weight pruning. However, it has the same
time complexity as the naive one-versus-all method and
requires much more computation than all other methods
we have compared in Section 5. For example, on dataset
Wiki10-31K, our algorithm needs less than 20 minutes on 1
core (refer to Table 1), while DisMEC requires 10 minutes
on 300 cores as reported in (Babbar & Sch¨olkopf, 2017)
and we record about 450 minutes training time using 4
cores.

Table 5: Experiments on DISMEC. Time refers to predic-
tion times in seconds. Size is the modelsize in megabytes.

Time
0.59 0.087 87.77 70.25
3.4 66.80 61.79
0.24
880 84.12 74.71
Wiki10-31K 771.8

GBDT-SPARSE (proposed)
DISMEC
Size P@1 P@3
Size P@1 P@3 Time
3.54 84.23 67.85
0.60
0.13
4.76 69.29 63.62
1.30 85.81 84.34 70.82

Mediamill
Delicious

We use the DisMEC implementation from its authors 4. We
found that in their experiment implementation, a TF-IDF
(term frequency inverse document frequency) feature con-
version is used. Using TF-IDF features can improve pre-
diction accuracy for text based datasets, and we found that
it is necessary to use TF-IDF to get a good accuracy for
Wiki10-31K. Therefore, we pre-process Wiki10-31K us-
ing TF-IDF for DisMEC in this section (we do not use TF-
IDF pre-processing in all other experiments). Due to our
limited computing resources, we only include Mediamill,
Wiki10-31K and Delicious in this experiment. The re-
sult is shown in Table 5. DisMEC achieves similar per-
formance with our method, but note that DisMEC requires
much more computation resources than our method. Larger
Datasets like Delicious-200K is practically unfeasible on a
single machine (with only a few cores) using DisMEC.

B. Prediction time for GBDT-Sparse

In many real world applications, only top-B labels are
needed with very small B (typically 1,3,5). In those cases,
we can further reduce the prediction time to O(T k log B).
To do that, we need a hash (with O(T k) size) and a min-

4https://sites.google.com/site/rohitbabbar/code/dismec

