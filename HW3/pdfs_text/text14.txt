A Semismooth Newton Method for Fast, Generic Convex Programming

Alnur Ali * 1 Eric Wong * 1 J. Zico Kolter 2

Abstract

We introduce Newton-ADMM, a method for fast
conic optimization. The basic idea is to view
the residuals of consecutive iterates generated by
the alternating direction method of multipliers
(ADMM) as a set of ﬁxed point equations, and
then use a nonsmooth Newton method to ﬁnd a
solution; we apply the basic idea to the Split-
ting Cone Solver (SCS), a state-of-the-art method
for solving generic conic optimization problems.
We demonstrate theoretically, by extending the
theory of semismooth operators, that Newton-
ADMM converges rapidly (i.e., quadratically) to
a solution; empirically, Newton-ADMM is sig-
niﬁcantly faster than SCS on a number of prob-
lems. The method also has essentially no tun-
ing parameters, generates certiﬁcates of primal
or dual infeasibility, when appropriate, and can
be specialized to solve speciﬁc convex problems.

1. Introduction and related work

Conic optimization problems (or cone programs) are con-
vex optimization problems of the form

minimize
x∈Rn

cT x

subject to b − Ax ∈ K,

(1)

where c ∈ Rn, A ∈ Rm×n, b ∈ Rm, K are problem data,
speciﬁed by the user, and K is a proper cone (Nesterov &
Nemirovskii, 1994; Ben-Tal & Nemirovski, 2001; Boyd &
Vandenberghe, 2004); we give a formal treatment of proper
cones in Section 2, but a simple example of a proper cone,
for now, is the nonnegative orthant, i.e., the set of all points
in Rm with nonnegative components. These problems are
quite general, encapsulating a number of standard problem
classes: e.g., taking K as the nonnegative orthant yields a
linear program; taking K as the positive semideﬁnite cone,

*Equal contribution 1Machine Learning Department, Carnegie
Mellon University 2Computer Science Department, Carnegie
Correspondence to: Alnur Ali <alnu-
Mellon University.
rali@cmu.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

i.e., the space of m × m positive semideﬁnite matrices Sm
+ ,
yields a semideﬁnite program; and taking K as the second-
order (or Lorentz) cone {(x, y) ∈ Rm−1 × R : (cid:107)x(cid:107)2 ≤ y}
yields a second-order cone program (a quadratic program
is a special case).

Due, in part, to their generality, cone programs have been
the focus of much recent work, and additionally form the
basis of many convex optimization modeling frameworks,
e.g., sdpsol (Wu & Boyd, 2000), YALMIP (Lofberg, 2005),
and the CVX family of frameworks (Grant, 2004; Diamond
& Boyd, 2016; Udell et al., 2014). These frameworks gen-
erally make it easy to quickly solve small and medium-
sized convex optimization problems to high accuracy; they
work by allowing the user to specify a generic convex opti-
mization problem in a way that resembles its mathematical
representation, then convert the problem into a form sim-
ilar to (1), and ﬁnally solve the problem. Primal-dual in-
terior point methods, e.g., SeDuMi (Sturm, 2002), SDPT3
(Toh et al., 2012), and CVXOPT (Andersen et al., 2011),
are common for solving these cone programs. These meth-
ods are useful, as they generally converge to high accuracy
in just tens of iterations, but they solve a Newton system
on each iteration, and so have difﬁculty scaling to high-
dimensional (i.e., large-n) problems.

In recent work, O’Donoghue et al. (2016) use the alter-
nating direction method of multipliers (ADMM) (Boyd
et al., 2011) to solve generic cone programs; operator split-
ting methods (e.g., ADMM, Peaceman-Rachford splitting
(Peaceman & Rachford, 1955), Douglas-Rachford split-
ting (Douglas & Rachford, 1956), and dual decomposition)
generally converge to modest accuracy in just a few itera-
tions, so the approach (called the splitting conic solver, or
SCS) is scalable, and also has a number of other beneﬁts,
e.g., provding certiﬁcates of primal or dual infeasibility.

In this paper, we introduce a new method (called “Newton-
ADMM”) for solving large-scale, generic cone programs
rapidly to high accuracy. The basic idea is to view the usual
ADMM recurrence relation as a ﬁxed point iteration, and
then use a truncated, nonsmooth Newton method to ﬁnd a
ﬁxed point; to justify the approach, we extend the theory
of semismooth operators, coming out of the applied math-
ematics literature over the last two decades (Mifﬂin, 1977;
Qi & Sun, 1993; Mart´ınez & Qi, 1995; Facchinei et al.,

A Semismooth Newton Method for Fast, Generic Convex Programming

1996), although it has received little attention from the ma-
chine learning community (Ferris & Munson, 2004). We
apply the approach to the ﬁxed point iteration associated
with SCS, to obtain a general purpose conic optimizer. We
show, under regularity conditions, that Newton-ADMM is
quadratically convergent; empirically, Newton-ADMM is
signiﬁcantly faster than SCS, on a number of problems.
Also, Newton-ADMM has essentially no tuning parame-
ters, and generates certiﬁcates of infeasibility, helpful in
diagnosing problem misspeciﬁcation.

The rest of the paper is organized as follows. In Section
2, we give the background on cone programs, SCS, and
semismooth operators, required to derive our method for
solving generic cone programs, Newton-ADMM. . In Sec-
tion 3, we present Newton-ADMM, and establish some of
its basic properties. In Section 4, we give various conver-
gence guarantees.
In Section 5, we empirically evaluate
Newton-ADMM, and describe an extension as a special-
ized solver. We conclude with a discussion in Section 6.

2. Background

We ﬁrst give some background on cones. Using this back-
ground, we go on to describe SCS, the cone program solver
of O’Donoghue et al. (2016), in more detail. Finally, we
give an overview of semismoothness (Mifﬂin, 1977), a gen-
eralization of smoothness, central to our Newton method.

2.1. Cone programming

We say that a set C is a cone if, for all x ∈ C, and θ ≥ 0,
we get that θx ∈ C. The dual cone C∗, associated with the
cone C, is deﬁned as the set {y : yT x ≥ 0, ∀x ∈ C}.
Additionally, a cone C is a convex cone if, for all x, y ∈ C,
and θ1, θ2 ≥ 0, we get that θ1x + θ2y ∈ C. A cone C
is a proper cone if it is (i) convex; (ii) closed; (iii) solid,
i.e., its interior is nonempty; and (iv) pointed, i.e., if both
x, −x ∈ C, then we get that x = 0.

The nonnegative orthant, second-order cone, and positive
semideﬁnite cone are all proper cones (Boyd & Vanden-
berghe, 2004, Section 2.4.1); these cones, along with the
exponential cone (deﬁned below), can be used to represent
most convex optimization problems encountered in prac-
tice. The exponential cone (see, e.g., Serrano (2015)), Kexp,
is a three-dimensional proper cone, deﬁned as the closure
of the epigraph of the perspective of exp(x), with x ∈ R:

Kexp = {(x, y, z) : x ∈ R, y > 0, z ≥ y exp(x/y)}
∪ {(x, 0, z) : x ≤ 0, z ≥ 0} .

Cone programs resembling (1) were ﬁrst described by Nes-
terov & Nemirovskii (1994, page 67), although special
cases were, of course, considered earlier. Standard refer-

ences include Ben-Tal & Nemirovski (2001) and Boyd &
Vandenberghe (2004, Section 4.6.1).

2.2. SCS

Roughly speaking, SCS is an application of ADMM to
a particular feasibility problem arising from the Karush-
Kuhn-Tucker (KKT) optimality conditions associated with
a cone program. To see this, consider a reformulation of
the cone program (1), with slack variable s ∈ Rm:

minimize
x∈Rn, s

cT x subject to Ax + s = b, s ∈ K.

(2)

The KKT conditions can be seen, after introducing dual
variables r ∈ Rn, y ∈ K∗, for the implicit constraint x ∈
Rn and the explicit constraints, respectively, to be

AT y + c = r

(stationarity)

(dual feasibility)

Ax + s = b, s ∈ K (primal feasibility)
r ∈ {0}n, y ∈ K∗
−cT x − bT y = 0 (complementary slackness),
where K∗ is the dual cone of K; thus, we can obtain a so-
lution to (2), by solving the KKT system














(cid:21)

AT
0
−A
0
−cT −bT

r
s
0
x ∈ Rn, y ∈ K∗, r ∈ {0}n, s ∈ K.

(cid:20) x
y

 =

c
b
0

+







 ,

(3)

Self-dual homogeneous embedding. When the cone
program (2) is primal/dual infeasible, there is no solution
to the KKT system (3); so, consider embedding the system
(3) in a larger system, with new variables τ, κ, and solving











AT
0
−A
0
−cT −bT



c
b
0

x
y
τ

r
s
κ





 =



 ,

(4)

x ∈ Rn, y ∈ K∗, τ ∈ R+, r ∈ {0}n, s ∈ K, κ ∈ R+,

which is always solvable. The embedding (4), due to Ye
et al. (1994), has a number of other nice properties. Ob-
serve that when τ (cid:63) = 1, κ(cid:63) = 0 are solutions to the em-
bedding (4), we recover the KKT system (3); it turns out
that the solutions τ (cid:63), κ(cid:63) characterize the primal or dual
In particular, if
(in)feasibility of the cone program (2).
τ (cid:63) > 0, κ(cid:63) = 0, then the cone program (2) is feasible,
with a primal-dual solution (1/τ (cid:63))(x(cid:63), y(cid:63), r(cid:63), s(cid:63)); on the
other hand, if τ (cid:63) = 0, κ(cid:63) ≥ 0, then (2) is primal or dual in-
feasible (or both), depending on the exact values of τ (cid:63), κ(cid:63)
(O’Donoghue et al., 2016, Section 2.3). The embedding (4)
can also be seen as ﬁrst-order homogeneous, in the sense
that (x(cid:63), y(cid:63), τ (cid:63), r(cid:63), s(cid:63), κ(cid:63)) being a solution to (4) implies
that k(x(cid:63), y(cid:63), τ (cid:63), r(cid:63), s(cid:63), κ(cid:63)), for k ≥ 0, is also a solution.
Finally, viewing the embedding (4) as a feasibility problem,
the dual of the feasibility problem turns out to be the origi-
nal feasibility problem, i.e., the embedding is self-dual.

A Semismooth Newton Method for Fast, Generic Convex Programming

ADMM-based algorithm. As mentioned,
ding (4) can be viewed as the feasibility problem

the embed-

• Positive semideﬁnite cone, Kpsd. The projection is

ﬁnd u, v

subject to Qu = v, (u, v) ∈ C × C∗,

where we write C = Rn ×K∗ ×R+, C∗ = {0}n ×K ×R+,

Q =





AT
0
−A
0
−cT −bT



 ,

c
b
0

u =

v =



 ,





x
y
τ



 .





r
s
κ

(5)

Introducing new variables ˜u, ˜v ∈ Rk, where k = n+m+1,
and rewriting so that we may apply ADMM, we get:

minimize
u, v, ˜u, ˜v

subject to

IC×C∗ (u, v) + IQu(cid:63)=v(cid:63) (˜u, ˜v)
(cid:21)
(cid:20) u
v

(cid:20) ˜u
˜v

=

(cid:21)

,

where IC×C∗ and IQu(cid:63)=v(cid:63) are the indicator functions of the
product space C × C∗, and the afﬁne space of solutions to
Qu = v, respectively; after simplifying (see O’Donoghue
et al. (2016, Section 3)), the ADMM recurrences are just

˜u ← (I + Q)−1(u + v).
u ← PC(˜u − v)
v ← v − ˜u + u,

(6)

(7)

(8)

where PC denotes the projection onto C. For the update (6),
Q is a skew-symmetric matrix, hence I + Q is nonsingular,
so the update can be done efﬁciently via the Schur comple-
ment, matrix inversion lemma, and LDLT factorization.

Projections onto dual cones. For the update (7), the pro-
jection onto C boils down to separate projections onto the
“free” cone Rn, the dual cone of K, and the nonnegative or-
thant R+. These projections, for many K, are well-known:

• Free cone. Here, PRn (z) = z, for z ∈ Rn.

• Nonnegative orthant, Kno. The projection onto Kno is
simply given by applying the positive part operator:

PKno(z) = max{z, 0}.

(9)

• Second-order cone, Ksoc. Write z = (z1, z2) ∈
Rm, z1 ∈ Rm−1, z2 ∈ R. Then the projection is

PKsoc(z) =






0,
z,
2 (1 + z2
1
(cid:107)z1(cid:107)2

(cid:107)z1(cid:107)2 ≤ −z2
(cid:107)z1(cid:107)2 ≤ z2
)(z1, (cid:107)z1(cid:107)2), otherwise.

PKpsd (Z) =

max{λi, 0}qiqT
i ,

(11)

(cid:88)

i

where Z = (cid:80)
tion of Z.

i λiqiqT
i

is the eigenvalue decomposi-

• Exponential cone, Kexp. If z ∈ Kexp, then PKexp (z) =
z. If −z ∈ K∗
exp, then PKexp (z) = 0. If z1, z2 < 0,
i.e., the ﬁrst two components of z are negative, then
PKexp = (z1, max{z2, 0}, max{z3, 0}). Otherwise,
the projection is given by

argmin
˜z∈R3:˜z2>0
subject to

(1/2)(cid:107)˜z − z(cid:107)2
2

˜z2 exp(˜z1/˜z2) = ˜z3,

(12)

which can be computed using a Newton method
(Parikh & Boyd, 2014, Section 6.3.4).

The nonnegative orthant, second-order cone, and positive
semideﬁnite cone are all self-dual, so projecting onto these
cones is equivalent to projecting onto their dual cones; to
project onto the dual of the exponential cone, we use the
Moreau decomposition to get

PK∗

exp

(z) = z + PKexp (−z).

(13)

2.3. Semismooth operators

Here, we give an overview of semismoothness; good ref-
erences include Ulbrich (2011) and Izmailov & Solodov
(2014). We consider maps F : Rk → Rk that are lo-
cally Lipschitz, i.e., for all z1 ∈ Rk, and z2 ∈ N (z1, δ),
where N (z1, δ) is a ball centered at z1 with radius δ > 0,
there exists some Lz1 > 0, such that (cid:107)F (z1) − F (z2)(cid:107)2 ≤
Lz1(cid:107)z1 − z2(cid:107)2. By a result known as Rademacher’s the-
orem (Evans & Gariepy, 2015, Section 3.1.2, Theorem 2),
we get that F is differentiable almost everywhere; we let
D denote the points at which F is differentiable, so that
Rk \ D is a set of measure zero.

The generalized Jacobian. Clarke (1990) suggested the
generalized Jacobian as a way to deﬁne the derivative of
a locally Lipschitz map F : Rk → Rk, at all points. The
generalized Jacobian is related to the subgradient, as well
as the directional derivative, as we discuss later on; the
generalized Jacobian, though, turns out to be quite useful
for deﬁning effective nonsmooth Newton methods. The
generalized Jacobian J (z) at a point z ∈ Rk of a map
F : Rk → Rk, is deﬁned as (co denotes convex hull)

J (z) = co

J(zi) : (zi) ∈ D, (zi) → z

(14)

(cid:111)

,

(cid:110)

lim
i→∞

where J(zi) ∈ Rk×k is the usual Jacobian of F at zi. Two
useful properties of the generalized Jacobian (Clarke, 1990,

(10)

A Semismooth Newton Method for Fast, Generic Convex Programming

Proposition 1.2): (i) J (z), at any z, is always nonempty;
and (ii) if each component Fi is convex, then the ith row of
any element of J (z) is just a subgradient of Fi at z.

(Strong) semismoothness and consequences. We say
that a map F : Rk → Rk is semismooth if it is locally
Lipschitz, and if, for all z, δ ∈ Rk, the limit

lim
δ→0, J∈J (z+δ)

Jδ

(15)

exists (see, e.g., Mifﬂin (1977, Deﬁnition 1) and Qi &
Sun (1993, Section 2)). The above deﬁnition is somewhat
opaque, so various works have provided an alternative char-
acterization of semismoothness: F is semismooth if and
only if it is (i) locally Lipschitz; (ii) directionally differen-
tiable, in every direction; and (iii) we get

lim
δ→0, J∈J (z+δ)

(cid:107)F (z + δ) − F (z) − Jδ(cid:107)2
(cid:107)δ(cid:107)2

= 0,

i.e., (cid:107)F (z + δ) − F (z) − Jδ(cid:107)2 = o((cid:107)δ(cid:107)2), δ → 0 (see,
e.g., Qi & Sun (1993, Theorem 2.3), Hinterm¨uller (2010,
Theorem 2.9), Qi & Sun (1999, page 2), and Mart´ınez &
Qi (1995, Proposition 2)). Examples of semismooth func-
tions include log(1 + |x|), all convex functions, and all
smooth functions (Mifﬂin, 1977; ´Smieta´nski, 2007); on the
other hand, (cid:112)|x| is not semismooth. A linear combina-
tion of semismooth functions is semismooth (Izmailov &
Solodov, 2014, Proposition 1.75). Finally, we say that a
map is strongly semismooth if, under the same conditions
as above, we can replace (15) with

lim sup
δ→0, J∈J (z+δ)

(cid:107)F (z + δ) − F (z) − Jδ(cid:107)2
(cid:107)δ(cid:107)2
2

< ∞,

i.e., (cid:107)F (z + δ) − F (z) − Jδ(cid:107)2 = O((cid:107)δ(cid:107)2
2), δ → 0 (see
Facchinei et al. (1996, Proposition 2.3) and Facchinei &
Kanzow (1997, Deﬁnition 1)).

3. Newton-ADMM and its basic properties

Next, we describe Newton-ADMM, our nonsmooth New-
ton method for generic convex programming; again, the ba-
sic idea is to view the ADMM recurrences (6) – (8), used by
SCS, as a ﬁxed point iteration, and then use a nonsmooth
Newton method to ﬁnd a ﬁxed point. Accordingly, we let

F (z) =





˜u − (I + Q)−1(u + v)
u − PC(˜u − v)
˜u − u



 ,

which are just the residuals of the consecutive ADMM it-
erates given by (6) – (8), and z = (˜u, u, v) ∈ R3k; multi-
plying by diag(I + Q, I, I) to change coordinates gives

F (z) =





(I + Q)˜u − (u + v)
u − PC(˜u − v)
˜u − u



 .

(16)

Now, we would like to apply a Newton method to F ,
but projections onto proper cones are not differentiable,
in general. However, for many cones of interest, they are
(strongly) semismooth; the following lemma summarizes.
Lemma 3.1. Projections onto the nonnegative orthant,
second-order cone, and positive semideﬁnite cone are all
strongly semismooth; see, e.g., Kong et al. (2009, Section
1), Kanzow & Fukushima (2006, Lemma 2.3), and Sun &
Sun (2002, Corollary 4.15), respectively.

Additionally, we give the following new result, for the ex-
ponential cone, which may be of independent interest.
Lemma 3.2. The projection onto the exponential cone is
semismooth.

We defer all proofs to the supplement.

Putting the pieces together, the following lemma estab-
lishes that F , deﬁned in (16), is (strongly) semismooth.
Lemma 3.3. When K,
is
from the cone program (1),
the nonnegative orthant, second-order cone, or positive
semideﬁnite cone,
is
strongly semismooth; when K is the exponential cone, then
the map F is semismooth.

then the map F , deﬁned in (16),

The preceding results lay the groundwork for us to use a
semismooth Newton method (Qi & Sun, 1993), applied to
F , where we replace the usual Jacobian with any element
of the generalized Jacobian (14); however, as many have
observed (Khan & Barton, 2017), it is not always straight-
forward to compute an element of the generalized Jacobian.
Fortunately, for us, we can just compute a subgradient of
each row of F , as the following lemma establishes.
Lemma 3.4. The ith row of each element of the generalized
Jacobian J (z) at z of the map F is just a subgradient of
Fi, i = 1, . . . , 3k, at z.

Using the lemma, an element J ∈ R3k×3k of the general-
ized Jacobian of the map F ∈ R3k is then just





J =

I + Q −I −I

Ju
−I

0

I



 ,

where



0

0

0





0
0
1

I
0
0

0
I
0

Ju =

0
0
−(cid:96)

I
0 JPK∗
0

−I
0 −JPK∗
0

0
0
(cid:96)
(18)
is a (k×3k)-dimensional matrix forming the second row of
J; (cid:96) equals 1 if ˜uτ − vκ ≥ 0 and 0 otherwise; and JPK∗ ∈
Rm×m is the Jacobian of the projection onto the dual cone
K∗. Here and below, we use subscripts to select compo-
nents, e.g., ˜uτ selects the τ -component of ˜u from (5), and
we write J to mean J(z), where z = (˜u, u, v) ∈ R3k.

0

(17)



A Semismooth Newton Method for Fast, Generic Convex Programming

3.1. Final algorithm

Algorithm 1 Newton-ADMM for convex optimization

(cid:107)F + J ˆ∆(cid:107)2 ≤ ε(cid:107)F (cid:107)2,

(20)

end for
return the ux- divided by the uτ -components of z(T )

Later, we discuss computing JPK∗ , the Jacobian of the pro-
jection onto the dual cone K∗, for various cones K; these
pieces let us compute an element J, given in (17) – (18),
of the generalized Jacobian of the map F , deﬁned in (16),
which we use instead of the usual Jacobian, in a semis-
mooth Newton method; below, we describe a way to scale
the method to larger problems (i.e., values of n).

Truncated, semismooth Newton method. The conju-
gate gradient method is, seemingly, an appropriate choice
here, as it only approximately solves the Newton system

J∆ = −F,

(19)

with variable ∆ ∈ R3k; unfortunately, in our case, J is non-
symmetric, so we appeal instead to the generalized mini-
mum residual method (GMRES) (Saad & Schultz, 1986).
We run GMRES until

where ˆ∆ is the approximate solution from a particular it-
eration of GMRES, and ε is a user-deﬁned tolerance; i.e.,
we run GMRES until the approximation error is acceptable.
After GMRES computes an approximate Newton step, we
use backtracking line search to compute a step size.

Now recall, from Section 2, that ∆(cid:63) = 0 is always a triv-
ial solution to the Newton system (19), due to homogene-
ity; so, we initialize the ˜uτ , uτ , vκ-components of z to 1,
which avoids converging to the trivial solution. Finally, we
mention that when K, in the cone program (1), is the direct
product of several proper cones, then Ju, in (18), simply
consists of multiple such matrices, just stacked vertically.

We describe the entire method in Algorithm 1. The method
has essentially no tuning parameters, since, for all the ex-
periments, we just ﬁx the maximum number of Newton
iterations T = 100; the backtracking line search param-
eters α = 0.001, β = 0.5; and the GMRES tolerances
ε(i) = 1/(i + 1), for each Newton iteration i. The cost of
each Newton iteration is the number of backtracking line
search iterations times the sum of two costs: the cost of
projecting onto a dual cone and the cost of GMRES, i.e.,
O(max{n2, m2}), assuming GMRES returns early. Simi-
larly, the cost of each ADMM iteration of SCS is the cost
of projecting onto a dual cone plus O(max{n2, m2}).

3.2. Jacobians of projections onto dual cones

Here, we derive the Jacobians of projections onto the dual
cones of the nonnegative orthant, second-order cone, posi-
tive semideﬁnite cone, and the exponential cone; here, we
write JPK∗ to mean JPK∗ (z), where z = ˜uy − vs ∈ Rm.

Input: problem data c ∈ Rn, A ∈ Rm×n, b ∈ Rm;
cones K; maximum number of Newton iterations T ;
backtracking line search parameters α ∈ (0, 1/2), β ∈
(0, 1); GMRES approximation tolerances (ε(i))T
Output: a solution to (2)
initialize ˜u(1) = u(1) = v(1) = 0 and ˜u(1)
v(1)
κ = 1
// avoids trivial solution
initialize z(1) = (˜u(1), u(1), v(1))
for i = 1, . . . , T do

τ = u(1)

τ =

i=1

compute J(z(i)), F (z(i))
// see (16), (17), Sec. 3.2
compute the Newton step ∆(i), i.e., by approximately
solving J(z(i))∆(i) = −F (z(i)) using GMRES with
approximation tolerance ε(i)
initialize t(i) = 1
while (cid:107)F (z(i) + t(i)∆(i))(cid:107)2
do

2 ≥ (1 − αt(i))(cid:107)F (z(i))(cid:107)2
2

// initialize step size t(i)

// see (20)

t(i) = βt(i)

// for backtracking line search

end while
update z(i+1) = z(i) + t(i)∆(i)

Nonnegative orthant. Since the nonnegative orthant is
self-dual, we can simply ﬁnd a subgradient of each compo-
nent in (9), to get that JPK∗ is diagonal with, say, (JPK∗ )ii
set to 1 if (˜uy −vs)i ≥ 0 and 0 otherwise, for i = 1, . . . , m.

z1 ∈
Second-order cone. Write z = (z1, z2),
Rm−1, z2 ∈ R. The second-order cone is self-dual, as
well, so we can ﬁnd subgradients of (10), to get that

JPK∗ =






0,
(cid:107)z1(cid:107)2 ≤ −z2
(cid:107)z1(cid:107)2 ≤ z2
I,
D, otherwise,

(21)

where D is a low-rank matrix (details in the supplement).

Positive semideﬁnite cone. The projection map onto the
(self-dual) positive semideﬁnite cone is matrix-valued, so
computing the Jacobian is more involved. We leverage the
fact that most implementations of GMRES need only the
product JPK∗ (vec Z), provided by the below lemma using
matrix differentials (Magnus & Neudecker, 1995); here,
vec is the vectorization of a real, symmetric matrix Z.
Lemma 3.5. Let Z = QΛQT be the eigenvalue decompo-
sition of Z, and let ˜Z be a real, symmetric matrix. Then
(vec Z)(vec ˜Z) = vec (cid:0)(dQ) max(Λ, 0)QT
+Q(d max(Λ, 0))QT + Q max(Λ, 0)(dQ)T (cid:1) ,

JPKpsd

where, here, the max is interpreted diagonally;
dQi = (ΛiiI − Z)+ ˜ZQi; [d max(Λ, 0)]ii = I+(Λii)QT

i

˜ZQi;

A Semismooth Newton Method for Fast, Generic Convex Programming

Z + denotes the pseudo-inverse of Z; and I+(·) is the indi-
cator function of the nonnegative orthant.

A5. For Theorem 4.2, we assume, for all z ∈ R3k and

∆ ∈ R3k, and for some C2 > 0, that

Exponential cone. Recall, from (12), that the projection
onto the exponential cone is not analytic, so computing the
Jacobian is much more involved, as well. The following
lemma provides a Newton method for computing the Jaco-
bian, using the KKT conditions for (12) and differentials.
Lemma 3.6. Let z ∈ R3.
JPKexp

(z) = I −

Then JPK∗

(−z), where

exp

JPKexp

(z) =






I,
−I,
diag(1, I+(z2), I+(z3)),

z ∈ Kexp
z ∈ K∗
exp
z1, z2 < 0;

otherwise, JPKexp
supplement, due to space constraints.

(z) is a particular 3x3 matrix given in the

4. Convergence guarantees

Here, we give some convergence results for Newton-
ADMM, the method presented in Algorithm 1.

First, we show that, under standard regularity assumptions,
the iterates (z(i))∞
i=1 generated by Algorithm 1 are glob-
ally convergent, i.e., given some initial point, the iterates
converge to a solution of F (z) = 0, where i is a Newton it-
eration counter. We break the statement (and proof) of the
result up into two cases. Theorem 4.1 establishes the re-
sult, when the sequence of step sizes (t(i))∞
i=1 converges to
some number bounded away from zero and one. Theorem
4.2 establishes the result when the step sizes converge zero.

Below, we state our regularity conditions, which are similar
to those given in Han et al. (1992); Mart´ınez & Qi (1995);
Facchinei et al. (1996); we elaborate in the supplement.

A1. For Theorem 4.1, we assume lim supi→∞ t(i) < 1.
A2. For Theorem 4.2, we assume lim supi→∞ t(i) = 0.

A3. For Theorem 4.2, we assume (i) that the GMRES ap-
proximation tolerances ε(i) are uniformly bounded by
ε as in ε(i) ≤ ε < 1 − α1/2, (ii) that (ε(i))∞
i=1 → 0,
and (iii) that ε(i) = O((cid:107)F (z(i))(cid:107)2).

A4. For Theorem 4.2, we assume, for every convergent se-
i=1 → z, (γ(i))∞
i=1 satisfying assump-

quence (z(i))∞
tion (A2) above, and (∆(j))∞

j=1 → ∆, that

(cid:107)F (z(i) + γ(i)∆(j))(cid:107)2

2 − (cid:107)F (z(i))(cid:107)2
2

lim
i,j→∞

γ(i)

≤ lim

i,j→∞

α1/2F (z(i))T ˆF (z(i), ∆(j)),

where, for notational convenience, we write

ˆF (z(i), ∆(j)) = J(z(i))∆(j).

C2(cid:107)∆(cid:107)2 ≤ (cid:107) ˆF (z, ∆)(cid:107)2.

A6. For Theorem 4.3, we assume, for all z ∈ R3k, J(z) ∈
J (z), (i) that (cid:107)J(z)(cid:107)2 ≤ C3, for some constant C3 >
0; and (ii) that every element of J (z) is invertible.

The two global convergence results are given below; the
proofs are based on arguments in Mart´ınez & Qi (1995,
Theorem 5a), but we use fewer user-deﬁned parameters,
and a different line search method.

4.1

(Global

convergence,

Theorem
with
lim supi→∞ t(i) = t, for some 0 < t < 1). Assume
condition (A1) stated above. Then limi→∞ F (z(i)) = 0.
Theorem
with
lim supi→∞ t(i) = 0). Assume conditions (A2), (A3), (A4),
and (A5) stated above. Suppose the sequence (z(i))∞
i=1
converges to some z ∈ R3k. Then F (z) = 0.

convergence,

(Global

4.2

Next, we show, in Theorem 4.3, that when F is strongly
semismooth, i.e., K is the nonnegative orthant, second-
order cone, or positive semideﬁnite cone,
the iterates
(z(i))∞
i=1 generated by Algorithm 1 are locally quadrati-
cally convergent; the proof is similar to that of Facchinei
et al. (1996, Theorem 3.2b), for semismooth maps.

Theorem 4.3 (Local quadratic convergence). Assume con-
Then the sequence of iter-
dition (A6) stated above.
ates (z(i))∞
i=1 → z generated by Algorithm 1 converges
quadratically, with F (z) = 0, for large enough i.

When K is the exponential cone, i.e., F is semismooth, the
iterates generated by Algorithm 1 are locally superlinearly
convergent (Facchinei et al., 1996, Theorem 3.2b).

5. Numerical examples

Next, we present an empirical evaluation of Newton-
ADMM, on several problems; in these, we directly com-
pare to SCS, which Newton-ADMM builds on, as it is the
most relevant benchmark for us (O’Donoghue et al. (2016)
observe that, with an optimized implementation, SCS out-
performs SeDuMi, as well as SDPT3). We evaluate, for
both methods, the time taken to reach the solution as well
as the optimal objective value; we obtained these by run-
ning an interior point method (Andersen et al., 2011) to
high accuracy. Table 1 describes the problem sizes, for both
the cone form of (1), as well as the familiar form that the
problem is usually written in. Later, we also describe ex-
tending Newton-ADMM to accelerate any ADMM-based
algorithm, applied to any convex problem; here, we com-
pare to state-of-the-art baselines for speciﬁc problems.

A Semismooth Newton Method for Fast, Generic Convex Programming

Table 1: Problem sizes, for the cone form (n, m) of (1), and the
familiar form (p, N ) that the problem is usually written in.

5.3. (cid:96)1-penalized logistic regression

PROBLEM

n

m

p

N

CONES

LINEAR PROG.
PORTFOLIO OPT.
LOGISTIC REG.
ROBUST PCA

600
2,501
3,200
4,376

1,200
2,504
7,200
8,103

600
2,500
100
25

300
–

Kno
Ksoc, Kno
1,000 Kexp, Kno
Kpsd, Kno

25

5.1. Random linear programs (LPs)

We compare Newton-ADMM and SCS on a linear program

minimize
x∈Rp

cT x

subject to Gx = h, x ≥ 0,

where c ∈ Rp, G ∈ RN ×p, h ∈ RN are problem
data, and the inequality is interpreted elementwise. To en-
sure primal feasibility, we generated a solution x(cid:63) by sam-
pling its entries from a normal distribution, then project-
ing onto the nonnegative orthant; we generated G (with
p = 600, N = 300, so G is wide) by sampling entries from
a normal distribution, then taking h = Gx(cid:63). To ensure dual
feasibility, we generated dual solutions ν(cid:63), λ(cid:63), associated
with the equality and inequality constraints, by sampling
their entries from a normal and Uniform(0, 1) distribution,
respectively; to ensure complementary slackness, we set
c = −GT ν(cid:63) + λ(cid:63). Finally, to put the linear program into
the cone form of (1), and hence (2), we just take

A =





G
−G
I



 ,







h
−h
0

b =

 , K = Kno.

The ﬁrst column of Figure 1 presents the time taken, by
both Newton-ADMM and SCS, to reach the optimal ob-
jective value, as well as to reach the solution; we see that
Newton-ADMM outperforms SCS in both metrics.

5.2. Minimum variance portfolio optimization

We consider a minimum variance portfolio optimization
problem (see, e.g., Khare et al. (2015); Ali et al. (2016)),

minimize
θ∈Rp

θT Σθ

subject to 1T θ = 1,

(22)

where, here, the problem data Σ ∈ Sp
++ is the covariance
matrix associated with the prices of p = 2, 500 assets; we
generated Σ by sampling a positive deﬁnite matrix. The
goal of the problem is to allocate wealth across p assets
such that the overall risk is minimized; shorting is allowed.
Putting the above problem into the cone form of (1) yields,
for K, the direct product of the second-order cone and the
nonnegative orthant (details in the supplement). The sec-
ond column of Figure 1 shows the results; we again see
that Newton-ADMM outperforms SCS.

We consider (cid:96)1-penalized logistic regression, i.e.,

minimize
θ∈Rp

(cid:80)N

i=1 log(1 + exp(yiXi·θ)) + λ(cid:107)θ(cid:107)1,

(23)

where, here, y ∈ RN here is a response vector; X ∈ RN ×p
is a data matrix, with Xi· denoting the ith row of X; and
λ ≥ 0 is a tuning parameter. We generated p = 100 sparse
underlying coefﬁcients θ(cid:63), by sampling entries from a nor-
mal distribution, then setting ≈ 90% of the entries to zero;
we generated X (with N = 1, 000) by sampling its entries
from a normal distribution, then set y = Xθ(cid:63) + δ, where
δ is (additive) Gaussian noise. For simplicity, we set the
tuning parameter λ = 1. Putting the above problem into
the cone form of (1) yields, for K, the direct product of the
exponential cone and the nonnegative orthant (details in the
supplement); the problem size in cone form ends up being
large (see Table 1). In the third column of Figure 1, we see
that Newton-ADMM outperforms SCS.

5.4. Robust principal components analysis (PCA)

Finally, we consider robust PCA,

minimize
L,S∈RN ×p

(cid:107)L(cid:107)∗ subject to (cid:107)S(cid:107)1 ≤ λ, L + S = X, (24)

where (cid:107) · (cid:107)∗ and (cid:107) · (cid:107)1 are the nuclear and elementwise (cid:96)1-
norms, respectively, and X ∈ RN ×p, λ ≥ 0 (Cand`es et al.,
2011, Equation 1.1). We generated a low-rank matrix L(cid:63),
with rank ≈ 1
2 N ; a sparse matrix S(cid:63), by sampling entries
from Uniform(0, 1), then setting ≈ 90% of the entries to
zero; and ﬁnally set X = L(cid:63) + S(cid:63). We set λ = 1. The
goal is to decompose the obsevations X into low-rank L
and sparse S components. Putting the above problem into
the cone form of (1) yields, for K, the direct product of the
positive semideﬁnite cone and nonnegative orthant (details
in the supplement). We see that Newton-ADMM and SCS
are comparable, in the fourth column of Figure 1.

5.5. Extension as a specialized solver

Finally, we observe that the basic idea of treating the resid-
uals of consecutive ADMM iterates as a ﬁxed point itera-
tion, and then ﬁnding a ﬁxed point using a Newton method,
is completely general, i.e., the same idea can be used to ac-
celerate (virtually) any ADMM-based algorithm, for a con-
vex problem. To illustrate, consider the lasso problem,

(1/2)(cid:107)y − Xθ(cid:107)2

minimize
θ∈Rp

(25)
where y ∈ RN , X ∈ RN ×p, λ ≥ 0; the ADMM recur-
rences (Parikh & Boyd, 2014, Section 6.4) are

2 + λ(cid:107)θ(cid:107)1,

θ ← (X T X + ρI)−1(X T y + ρ(κ − µ))
κ ← Sλ/ρ(θ + µ)
µ ← µ + θ − κ,

(26)

(27)

(28)

A Semismooth Newton Method for Fast, Generic Convex Programming

Figure 1: Comparison of Newton-ADMM and SCS (O’Donoghue et al., 2016), on several convex problems. Columns, from left to right:
linear programming, portfolio optimization, (cid:96)1-penalized logistic regression, robust PCA. Top row: wallclock time vs. log-distance to
the optimal objective value, obtained by running an interior point method. Bottom row: wallclock time vs. log-distance, in a Euclidean
norm sense, to the solution. Each plot is one representative run out of 20 (the variance was negligible). Best viewed in color.

where ρ > 0, κ, µ ∈ Rp are the tuning parameter and
auxiliary variables, introduced by ADMM, respectively,
and Sλ/ρ(·) is the soft-thresholding operator. The map
F : R3p → R3p, from (16), with components set to the
residuals of the ADMM iterates given in (26) – (28), is then





F (z) =

(X T X + ρI)θ − (X T y + ρ(κ − µ))
κ − Sλ/ρ(θ + µ)
κ − θ



 ,

where z = (θ, κ, µ) ∈ R3p, and we also changed coordi-
nates, similar to before. An element J ∈ R3p×3p of the
generalized Jacobian of F is then




X T X + ρI −ρI

J =



−D
−I

ρI
D
0

 ,

I
I

where D ∈ Rp×p is diagonal with Dii set to 1 if |θi +µi| >
λ/ρ and 0 otherwise, for i = 1, . . . , m.

In the left panel of Figure 2, we compare a specialized
Newton-ADMM applied directly to the lasso problem (25),
with the ADMM algorithm for (26) – (28), a proximal
gradient method (Beck & Teboulle, 2009), and a heavily-
optimized implementation of coordinate descent (Friedman
et al., 2007); we set p = 400, N = 200, λ = 10, ρ = 1.
Here, the specialized Newton-ADMM is quite competi-
tive with these strong baselines; the specialized Newton-
ADMM outperforms Newton-ADMM applied to the cone
program (2), so we omit the latter from the comparison.
Stella et al. (2016) recently described a related approach.

In the right panel of Figure 2, we present a similar com-
parison, for sparse inverse covariance estimation, with the
QUIC method of Hsieh et al. (2014); Newton-ADMM
clearly performs best (p = N = 1, 000, λ = ρ = 1,
details in the supplement).

Figure 2: Left: wallclock time vs. log-distance to the optimal
objective value, on the lasso problem, for the specialized Newton-
ADMM method, standard ADMM, a proximal gradient method,
and a heavily-optimized coordinate descent implementation (as
a reference benchmark). Right: for a sparse inverse covariance
estimation problem, with specialized Newton-ADMM, standard
ADMM, and QUIC (Hsieh et al., 2014). Best viewed in color.

6. Discussion

We introduced Newton-ADMM, a new method for generic
convex programming. The basic idea is use a nonsmooth
Newton method to ﬁnd a ﬁxed point of the residuals of the
consecutive ADMM iterates generated by SCS, a state-of-
the-art solver for cone programs; we showed that the ba-
sic idea is fairly general, and can be applied to accelerate
(virtually) any ADMM-based algorithm. We presented the-
oretical and empirical support that Newton-ADMM con-
verges rapidly (i.e., quadratically) to a solution, outper-
forming SCS across several problems.

Acknowledgements. AA was supported by the DoE
Computational Science Graduate Fellowship DE-FG02-
97ER25308. EW was supported by DARPA, under award
number FA8750-17-2-0027. We thank Po-Wei Wang and
the referees for a careful proof-reading.

010203040Seconds10-1210-1110-1010-910-810-710-610-510-410-310-210-1100101102SuboptimalitySCSNewton-ADMM0510152025303540Seconds10-710-610-510-410-310-2SuboptimalitySCSNewton-ADMM02004006008001000Seconds10-1210-1010-810-610-410-2100102104106108101010121014SuboptimalitySCSNewton-ADMM05101520Seconds10-810-610-410-2100102104106108101010121014SuboptimalityNewton-ADMMSCS010203040Seconds10-1210-1110-1010-910-810-710-610-510-410-310-210-1100101102Estimation errorSCSNewton-ADMM020406080100Seconds10-910-810-710-610-510-410-310-210-1100101102Estimation errorSCSNewton-ADMM02004006008001000Seconds10-1210-1010-810-610-410-2100102104106108101010121014Estimation errorSCSNewton-ADMM05101520Seconds10-810-610-410-2100102104106108101010121014Estimation errorNewton-ADMMSCS0.00.51.01.52.02.53.0Seconds10-710-610-510-410-310-210-1100101102103SuboptimalityProximal gradient methodCoordinate descentADMMNewton-ADMM0100200300400500600700800Seconds10-910-810-710-610-510-410-310-210-1100101102103104105SuboptimalityQUICADMMNewton-ADMMA Semismooth Newton Method for Fast, Generic Convex Programming

References

Ali, Alnur, Khare, Kshitij, Oh, Sang-Yun, and Rajaratnam,
Bala. Generalized pseudolikelihood methods for inverse
covariance estimation. Technical report, 2016. Available
at http://arxiv.org/pdf/1606.00033.pdf.

Andersen, Martin, Dahl, Joachim, Liu, Zhang, and Vanden-
berghe, Lieven.
Interior point methods for large-scale
cone programming. Optimization for machine learning,
pp. 55–83, 2011.

Beck, Amir and Teboulle, Marc. A fast iterative shrinkage-
thresholding algorithm for
inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.

linear

Ben-Tal, Aharon and Nemirovski, Arkadi. Lectures on
Modern Convex Optimization: Analysis, Algorithms,
and Engineering Applications. SIAM, 2001.

Boyd, Stephen and Vandenberghe, Lieven. Convex Opti-

mization. Cambridge University Press, 2004.

Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
Eckstein, Jonathan. Distributed optimization and statisti-
cal learning via the alternating direction method of mul-
tipliers. Foundations and Trends in Machine Learning,
3(1):1–122, 2011.

Cand`es, Emmanuel, Li, Xiaodong, Ma, Yi, and Wright,
Journal

John. Robust principal component analysis?
of the ACM, 58(3):11, 2011.

Clarke, Frank. Optimization and Nonsmooth Analysis.

SIAM, 1990.

Diamond, Steven and Boyd, Stephen. CVXPY: A Python-
embedded modeling language for convex optimization.
Journal of Machine Learning Research, 17(83):1–5,
2016.

Douglas, Jim and Rachford, Henry. On the numerical solu-
tion of heat conduction problems in two and three space
variables. Transactions of the American Mathematical
Society, 82(2):421–439, 1956.

Evans, Lawrence and Gariepy, Ronald. Measure Theory
and Fine Properties of Functions. CRC Press, 2015.

Facchinei, Francisco and Kanzow, Christian. A nons-
mooth inexact Newton method for the solution of large-
scale nonlinear complementarity problems. Mathemati-
cal Programming, 76(3):493–512, 1997.

Facchinei, Francisco, Fischer, Andreas, and Kanzow,
Inexact Newton methods for semismooth
inequality

Christian.
equations with applications to variational
problems, 1996.

Ferris, Michael and Munson, Todd. Semismooth support
vector machines. Mathematical Programming, 101(1):
185–204, 2004.

Friedman, Jerome, Hastie, Trevor, H¨oﬂing, Holger, and
Tibshirani, Robert. Pathwise coordinate optimization.
The Annals of Applied Statistics, 1(2):302–332, 2007.

Grant, Michael. Disciplined Convex Programming. PhD

thesis, Stanford University, 2004.

Han, Shih-Ping, Pang, Jong-Shi, and Rangaraj, Narayan.
Globally convergent Newton methods for nonsmooth
equations. Mathematics of Operations Research, 17(3):
586–607, 1992.

Hinterm¨uller, Michael.

Semismooth Newton methods
and applications. Technical report, 2010. Available at
http://www.math.uni-hamburg.de/home/
hinze/Psfiles/Hintermueller_OWNotes.
pdf.

Hsieh, Cho-Jui, Sustik, M´aty´as, Dhillon, Inderjit, and
Ravikumar, Pradeep. QUIC: Quadratic approximation
for sparse inverse covariance estimation. Journal of Ma-
chine Learning Research, 15(1):2911–2947, 2014.

Izmailov, Alexey and Solodov, Mikhail. Newton-Type
Methods for Optimization and Variational Problems.
Springer, 2014.

Kanzow, Christian and Fukushima, Masao. Semismooth
methods for linear and nonlinear second-order cone pro-
grams. Technical report, 2006.

Khan, Kamil A and Barton, Paul. Generalized derivatives
IEEE Transactions on Automatic

for hybrid systems.
Control, 2017.

Khare, Kshitij, Oh, Sang-Yun, and Rajaratnam, Bala. A
convex pseudolikelihood framework for high dimen-
sional partial correlation estimation with convergence
guarantees. Journal of the Royal Statistical Society: Se-
ries B, 77(4):803–825, 2015.

Kong, Lingchen, Tunc¸el, Levent, and Xiu, Naihua. Clarke
generalized Jacobian of the projection onto symmetric
cones. Set-Valued and Variational Analysis, 17(2):135–
151, 2009.

Lofberg, Johan. YALMIP: A toolbox for modeling and
optimization in MATLAB. In 2004 IEEE International
Symposium on Computer Aided Control Systems Design,
pp. 284–289. IEEE, 2005.

Magnus, Jan and Neudecker, Heinz. Matrix Differential
Calculus with Applications in Statistics and Economet-
rics. John Wiley & Sons, 1995.

A Semismooth Newton Method for Fast, Generic Convex Programming

Mart´ınez, Jos´e and Qi, Liqun. Inexact Newton methods for
solving nonsmooth equations. Journal of Computational
and Applied Mathematics, 60(1):127–145, 1995.

Sun, Defeng and Sun, Jie. Semismooth matrix-valued func-
tions. Mathematics of Operations Research, 27(1):150–
169, 2002.

Mifﬂin, Robert. Semismooth and semiconvex functions in
constrained optimization. SIAM Journal on Control and
Optimization, 15(6):959–972, 1977.

Nesterov, Yurii and Nemirovskii, Arkadii.

Interior Point
Polynomial Algorithms in Convex Programming. SIAM,
1994.

O’Donoghue, Brendan, Chu, Eric, Parikh, Neal, and Boyd,
Stephen. Conic optimization via operator splitting and
Journal of Opti-
homogeneous self-dual embedding.
mization Theory and Applications, pp. 1–27, 2016.

Toh, Kim-Chuan, Todd, Michael, and T¨ut¨unc¨u, Reha. On
the implementation and usage of SDPT3 — a MAT-
LAB software package for semideﬁnite/quadratic/linear
In Handbook on Semideﬁ-
programming, version 4.0.
nite, Conic, and Polynomial Optimization, pp. 715–754.
Springer, 2012.

Udell, Madeleine, Mohan, Karanveer, Zeng, David, Hong,
Jenny, Diamond, Steven, and Boyd, Stephen. Convex
In Proceedings of the 1st First
optimization in Julia.
Workshop for High Performance Technical Computing
in Dynamic Languages, pp. 18–28. IEEE, 2014.

Parikh, Neal and Boyd, Stephen. Proximal algorithms.
Foundations and Trends in Optimization, 1(3):127–239,
2014.

Ulbrich, Michael. Semismooth Newton Methods for Varia-
tional Inequalities and Constrained Optimization Prob-
lems in Function Spaces. SIAM, 2011.

Peaceman, Donald and Rachford, Henry. The numerical
solution of parabolic and elliptic differential equations.
Journal of the Society for Industrial and Applied Mathe-
matics, 3(1):28–41, 1955.

Wu, Shao-Po and Boyd, Stephen. sdpsol: A parser/solver
for semideﬁnite programs with matrix structure. Ad-
vances in Linear Matrix Inequality Methods in Control,
pp. 79–91, 2000.

√

Ye, Yinyu, Todd, Michael, and Mizuno, Shinji. An
nL)-iteration homogeneous and self-dual
O(
linear
programming algorithm. Mathematics of Operations Re-
search, 19(1):53–67, 1994.

Qi, Liqun and Sun, Defeng. A survey of some nonsmooth

equations and smoothing Newton methods, 1999.

Qi, Liqun and Sun, Jie. A nonsmooth version of Newton’s
method. Mathematical Programming, 58(1-3):353–367,
1993.

Saad, Youcef and Schultz, Martin. GMRES: A general-
ized minimal residual algorithm for solving nonsymmet-
ric linear systems. SIAM Journal on Scientiﬁc and Sta-
tistical Computing, 7(3):856–869, 1986.

Serrano, Santiago. Algorithms for Unsymmetric Cone Op-
timization and an Implementation for Problems with the
Exponential Cone.
PhD thesis, Stanford University,
2015.

´Smieta´nski, Marek. A generalized Jacobian based New-
ton method for semismooth block triangular system of
equations. Journal of Computational and Applied Math-
ematics, 205(1):305–313, 2007.

Stella, Lorenzo, Themelis, Andreas, and Patrinos, Pana-
giotis. Forward-backward quasi-Newton methods for
nonsmooth optimization problems. Technical report,
2016. Available at https://arxiv.org/pdf/
1604.08096.pdf.

Sturm, Jos. Implementation of interior point methods for
mixed semideﬁnite and second order cone optimization
problems. Optimization Methods and Software, 17(6):
1105–1154, 2002.

