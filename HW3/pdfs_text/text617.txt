Fast Bayesian Intensity Estimation for the Permanental Process

Christian J. Walder 1 2 Adrian N. Bishop 1 2 3

Abstract

in (Lloyd et al., 2015; Kom Samo & Roberts, 2015).

The Cox process is a stochastic process which
generalises the Poisson process by letting the un-
derlying intensity function itself be a stochastic
process. In this paper we present a fast Bayesian
inference scheme for the permanental process,
a Cox process under which the square root of
the intensity is a Gaussian process. In particu-
lar we exploit connections with reproducing ker-
nel Hilbert spaces, to derive efﬁcient approxi-
mate Bayesian inference algorithms based on the
Laplace approximation to the predictive distribu-
tion and marginal likelihood. We obtain a simple
algorithm which we apply to toy and real-world
problems, obtaining orders of magnitude speed
improvements over previous work.

1. Introduction

The Poisson process is an important model for point data
in which samples of the process are locally ﬁnite subsets
of some domain such as time or space. The process is
parametrised by an intensity function, the integral of which
gives the expected number of points in the domain of inte-
gration — for a gentle introduction we recommend (Bad-
deley, 2007). In the typical case of unknown intensity func-
tion we may place a non-parametric prior over it via e.g. the
Gaussian Process (GP) and perform Bayesian inference.

Inference under such models is challenging due to both the
GP prior and the non factorial nature of the Poisson process
likelihood (1), which includes an integral of the intensity
function. One may resort to discretising the domain (Rath-
bun & Cressie, 1994; Møller et al., 1998; Rue et al., 2009)
or performing Monte Carlo approximations (Adams et al.,
2009; Diggle et al., 2013). Fast Laplace approximates were
studied in (Cunningham et al., 2008; Illian et al., 2012;
Flaxman et al., 2015) and variational methods were applied

1Data61, CSIRO, Australia 2The Australian National Uni-
versity 3University of Technology Sydney. Correspondence to:
Christian <christian.walder@anu.edu.au>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

To satisfy non-negativity of the intensity function one
transforms the GP prior. The log-Gaussian Cox Process,
with GP distributed log intensity, has been the subject of
(Rathbun & Cressie, 1994; Møller
much study; see e.g.
et al., 1998; Illian et al., 2012; Diggle et al., 2013), Alter-
native formulations for introducing a GP prior exist, e.g.
(Adams et al., 2009). More recent research has highlighted
the analytical and computational advantages (Lloyd et al.,
2015; 2016; Flaxman et al., 2017; Møller et al., 1998) of the
permanental process, which has GP distributed square root
intensity (Shirai & Takahashi, 2003; McCullagh & Møller,
2006) — we discuss the relationship between these meth-
ods and the present work in more detail in subsection 2.2.

In section 2 we introduce the Poisson and permanental pro-
cesses, and place our work in the context of existing lit-
erature. Section 3 reviews Flaxman et al. (2017), slightly
recasting it as regularised maximum likelihood for the per-
manental process. Our Bayesian scheme is then derived in
section 4. In section 5 we discuss the choice of covariance
function for the GP prior, before presenting some numeri-
cal experiments in section 6 and concluding in section 7.

2. The Model

2.1. The Poisson Process

We view the inhomogeneous Poisson process on Ω as a
distribution over locally ﬁnite subsets of Ω. The num-
ber N (X ) of elements in some X ⊆ Ω is assumed to
be distributed as Poisson(Λ(X , µ)), where Λ(S, µ) :=
(cid:82)
x∈S λ(x)dµ(x) gives the mean of the Poisson distribu-
tion. It turns out that this implies the likelihood function

p ({xi}m

i=1 |λ, Ω) =

λ(xi) exp (−Λ(Ω)) .

(1)

m
(cid:89)

i=1

2.2. Latent Gaussian Process Intensities

To model unknown λ(x), we employ a non-parametric
prior over functions, namely the Gaussian process (GP).
To ensure that λ is non-negative valued we include a de-
terministic “link” function g : R → R+ so that we have
the prior over λ deﬁned by λ = g ◦ f and f ∼ GP(k),
where k is the covariance function for f . The most com-

Fast Bayesian Permanental Processes

mon choice for g is the exponential function exp(·), lead-
ing to the log-Gaussian Cox process (LGCP) (Møller et al.,
1998). Recently Adams et al. (2009) employed the trans-
formation g(z) = λ∗(1 + exp(−z))−1 , which permits ef-
ﬁcient sampling via thinning (Lewis & Shedler, 1979) due
to the bound 0 ≤ λ(x) ≤ λ∗.

2.2.1. PERMANENTAL PROCESSES: SQUARED LINK

FUNCTION

In this paper we focus on the choice g(z) = 1
2 z2, known
as the permanental process (Shirai & Takahashi, 2003; Mc-
Cullagh & Møller, 2006). Two recent papers have demon-
strated the analytical and computational advantages of this
link function.

1. Flaxman et al. (2017) derived a non-probabilistic reg-
ularisation based algorithm which we review in sec-
tion 3, and which exploited properties of reproduc-
ing kernel Hilbert spaces. The present work gener-
alises their result, providing probabilistic predictions
and Bayesian model selection. Our derivation is by
necessity entirely different to Flaxman et al. (2017), as
their representer theorem (Sch¨olkopf et al., 2001) ar-
gument is insufﬁcient for our probabilistic setting (see
e.g. subsubsection 4.1.6).

2. (Lloyd et al., 2015) derived a variational approxima-
tion to a Bayesian model with the squared link func-
tion, based on an inducing variable scheme similar to
(Titsias, 2009), and exploiting the tractability of cer-
tain required integrals. The present work has the ad-
vantage of 1) not requiring the inducing point approx-
imation, 2) being free of non-closed form expressions
such as their ˜G and 3) being simpler to implement
and orders of magnitude faster in practice while, as
we demonstrate, exhibiting comparable predictive ac-
curacy.

3. Regularised Maximum Likelihood

Flaxman et al. (2017) combined (1) with the regularisation
term (cid:107)f (cid:107)2
H(k), leading to the regularised maximum likeli-
hood estimator for f , namely ˆf :=

argmax
f

m
(cid:88)

i=1

1
2

log

f 2(xi) −

(cid:107)f (cid:107)2

1
2

(cid:16)

(cid:124)

L2(Ω,µ) + (cid:107)f (cid:107)2
(cid:123)(cid:122)
H(k,Ω,µ)

:=(cid:107)f (cid:107)2

H(k)

(cid:17)

,

(cid:125)

(2)

where we have implicitly deﬁned the new RKHS
H(k, Ω, µ) := H(˜k). Now, provided we can compute
the associated new reproducing kernel ˜k, then we may
appeal to the representer theorem (Kimeldorf & Wahba,
1971) in order to compute the ˆf , which takes the form

(cid:80)m

i=1 αi

˜k(xi, ·) for some αi. The function ˜k may be ex-

pressed in terms of the Mercer expansion (Mercer, 1909)

k(x, y) =

λiφi(x)φi(y),

(3)

N
(cid:88)

i=1

where φi are orthonormal in L2(Ω, µ). To satisfy for arbi-
trary f = (cid:80)
i wiφi the reproducing property (Aronszajn,
1950)

k(x, ·),

wiφ(·)i

:= f (x) =

wi, φi(x) (4)

(cid:68)

(cid:88)

i

(cid:69)

H(k)

(cid:88)

i

we let φi be orthogonal in H(k), obtaining (cid:104)φi, φj(cid:105) =
δijλ−1
i /λi, and from
i
(2) we have (cid:107)(cid:80)

H(k) = (cid:80)
H(k,Ω,µ) = (cid:80)

i w2
i (1 + λ−1

. Hence, (cid:107)(cid:80)

i wiφi(cid:107)2

i wiφi(cid:107)2

i w2

), so

i

˜k(x, y) =

N
(cid:88)

i=1

1
1 + λ−1

i

φi(x)φi(y).

(5)

For approximate Bayesian inference however, we cannot
simply appeal to the representer theorem.

4. Approximate Bayesian Inference

In subsection A.3 of the supplementary material, we re-
view the standard Laplace approximation to the GP with
non-Gaussian likelihood. This a useful set-up for what fol-
lows, but is not directly generalisable to our case due to the
integral in (1). Instead, in subsection 4.1 we now take a
different approach based on the Mercer expansion.

4.1. Laplace Approximation
It is tempting to na¨ıvely substitute ˜k into subsection A.3 of
the supplementary material, and to neglect the integral part
of the likelihood. Indeed, this gives the correct approximate
predictive distribution. The marginal likelihood does not
work in this way however (due to the log determinant in
(18)). We now perform a more direct analysis.

4.1.1. MERCER EXPANSION SETUP

Mercer’s theorem allows us to write (3), where for non-
degenerate kernels, N = ∞. Assume a linear model in
Φ(x) = (φi(x))i so that1

f (x) = w(cid:62)Φ(x),

(6)

and let w ∼ N (0, Λ) where Λ = (λi)ii is a diagonal co-
variance matrix. This is equivalent to f ∼ GP(k) because

cov(f (x), f (z)) = Φ(x)(cid:62)Λ Φ(z) = k(x, z).

1We use a sloppy notation where (x)i is the i-th element of x

while (xi)i is a vector with i-th element xi, etc.

Fast Bayesian Permanental Processes

Figure 1. Test function λ0 of subsection 6.2. The vertical grey lines are the input data points, sampled from the ground truth intensity
depicted by the heavy grey line. We plot two samples from the approximate predictive distribution (black lines) along with the median
(solid red) and [0.1, 0.9] interval (ﬁlled red). The GP prior is that of subsection 5.1 with hyper-parameters m = 2, number of cosine
frequencies N = 256, and the remaining a, b chosen to maximise the marginal likelihood.

Recall that the Poisson process on Ω with intensity λ(x) =
2 f 2(x) has likelihood for X := {xi}m
1

i=1

Crucially, ˆw must satisfy the stationarity condition

ˆw = (I + Λ−1)−1 ∇w log h(X|w)|w= ˆw ,

(9)

log p(X|w, Ω, k) =

f 2(xi)

−

m
(cid:88)

i=1
(cid:124)

log

1
2
(cid:123)(cid:122)
:=log h(X|w)

(cid:125)

1
2

(cid:90)

(cid:124)

x∈Ω

f 2(x)dµ(x)

(cid:123)(cid:122)
w(cid:62)w

(cid:125)

where

The joint in w, X is

log p(w, X|Ω, k)

= log h(X|w)−

w(cid:62)(I+Λ−1)w−

log |Λ|−

log 2π.

ˆf (x∗) := E [f (x∗)|X, Ω, k]

1
2

1
2

N
2

∇w log h(X|w)|w= ˆw = 2

m
(cid:88)

i=1

Φ(xi)
Φ(xi)(cid:62) ˆw

.

The approximate predictive mean is therefore

= Φ(x∗)(cid:62) ˆw

m
(cid:88)

=

2
Φ(xi)(cid:62) ˆw

i=1
m
(cid:88)

i=1

:=

αi

˜k(xi, x∗).

· Φ(xi)(cid:62)(I + Λ−1)−1Φ(x∗)

4.1.2. LAPLACE APPROXIMATION

We make a Laplace approximation to the posterior, which
is the normal distribution

log p(w|X, Ω, k) ≈ log N (w| ˆw, Q)
1
2

(w − ˆw)(cid:62)Q−1(w − ˆw) −

= −

1
2

(7)

N
2

:= log q(w|X, Ω, k),

log |Q| −

log 2π

This reveals the same ˜k as (5). From (10) we have

ˆαi = 2/ ˆf (xi).

(10)

(11)

where ˆw is chosen as the mode of the true posterior, and Q
is the inverse Hessian of the true posterior, evaluated at ˆw.

Putting (9), (10) and (11) into (8), we obtain

4.1.3. PREDICTIVE MEAN

The mode ˆw is

ˆα = argmin

α

m
(cid:88)

i=1

log α2

i +

α(cid:62) ˜Kα,

1
2

ˆw = argmax

log p(w|X, Ω, k)

w

w

= argmax

log h(X|w) −

w(cid:62)(I + Λ−1)w.

(8)

1
2

where ˜K = (˜k(xi), xj)ij. This is equivalent to Flaxman
et al. (2017), though slightly simpliﬁed by (11). Interest-
ingly, unlike Flaxman et al. (2017) (or the analogous sec-
tion 3), we did not appeal to the representer theorem.

0.00.51.01.52.02.53.0inputdomainΩ=[0,π]02040Poissonintensityλdatalocationsxitrueintensityλ(x)predictivemedian[0.1,0.9]pred.intervalpredictivesamples4.1.4. PREDICTIVE VARIANCE

Similarly to (19) we get approximate marginal likelihood

Fast Bayesian Permanental Processes

We now compute the Q in (7). The Hessian term giving the
inverse covariance becomes

Q−1 = −

∂2

(cid:12)
(cid:12)
∂w∂w(cid:62) log p(w, X, Ω, k)
(cid:12)
(cid:12)w= ˆw

= I + Λ−1 + W

W = −

∂2

∂w∂w(cid:62) log h(X|w)
m
Φ(xi)Φ(xi)(cid:62)
(cid:88)
(Φ(xi)(cid:62) ˆw)2

= 2

i=1
:= V DV (cid:62),

(cid:12)
(cid:12)
(cid:12)
(cid:12)w= ˆw

2 I ∈ Rm×m. The
where V:i = αi × Φ(xi) and D = 1
approximate predictive variance can now be rewritten as an
m-dimensional matrix expression using the identity (Z +
V DV (cid:62)) = Z −1 − Z −1V (V (cid:62)Z −1V + D−1)V (cid:62)Z −1 with
and Z = I + Λ−1 along with a little algebra, to derive2

log p(X|Ω, k) ≈ log q( ˆw, X|Ω, k) − log q( ˆw|X, Ω, k)

= log h(X| ˆw)
(cid:125)
i=1 log 2α2
i

(cid:124)
− (cid:80)m

(cid:123)(cid:122)

−

(cid:16)

1
2

ˆw(cid:62)(I + Λ−1) ˆw
(cid:124)
(cid:123)(cid:122)
(cid:125)
α(cid:62) ˜k(X,X)α

− log |Λ|+log |Q|

(cid:17)

(12)

We now use the determinant identity |Z + V DV (cid:62)| =
|Z||D||V (cid:62)Z −1V + D−1| with the same Z, V and D as
subsubsection 4.1.4 to derive

− log |Λ| + log |Q| = − log |Λ| − log |Z + V DV (cid:62)|

= − log (cid:12)

=

log

N
(cid:88)

i=1
(cid:124)

(cid:12)Λ(I + Λ−1)(cid:12)
1
1 + λi
(cid:125)

− log

(cid:12) − log (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:123)(cid:122)
:=V(k,Ω,µ)

(cid:12)D−1 + V (cid:62)Z −1V (cid:12)

(cid:12) + c

˜k(X, X) (cid:12) (αα(cid:62)) + 2I

(cid:12)
(cid:12)
(cid:12) + c,

(13)

where c = m log(2). V(k, Ω, µ) is the crucial ingredient,
not accounted for by na¨ıvely putting ˜k into subsection A.3.

σ2(x∗) := Var [f (x∗)|X, Ω, k]

= Φ(x∗)(cid:62)Q Φ(x∗)
= ˜k(x∗, x∗) − (cid:0)˜k(x∗, X) (cid:12) α(cid:1)S−1(cid:0)α(cid:62) (cid:12) ˜k(X, x∗)(cid:1),

5. Covariance Functions

To apply our inference scheme we need to compute:

where (cid:12) is the Hadamard product, or element-wise multi-
plication, and S := (cid:0)˜k(X, X) (cid:12) (αα(cid:62)) + 2I(cid:1).

4.1.5. PREDICTIVE DISTRIBUTION

the

predictive

approximate

Given
distribution
f (x∗)|X, Ω, k ∼ N ( ˆf (x∗), σ2(x∗)) := N (µ, σ2) and the
relation λ(·) = 1
2 f 2(·) it is straightforward to derive the
corresponding3 λ(x∗)|X, Ω, k ∼ Gamma(α, β) where
the shape α = (µ2+σ2)2

2σ2(2µ2+σ2) and the scale β = 2µ2σ2+σ4

µ2+σ2

.

4.1.6. MARGINAL LIKELIHOOD

Letting q( ˆw, X|Ω, k) be
expansion of
log p(w, X|Ω, k) about the mode w = ˆw and evalu-
ating at ˆw gives, as linear and quadratic terms vanish,

the Taylor

log q( ˆw, X|Ω, k) = log p( ˆw, X|Ω, k)

= log h(X| ˆw)−

ˆw(cid:62)(I+Λ−1) ˆw−

log |Λ|−

log 2π.

1
2

N
2

1
2

2Where e.g. ˜k(X, x∗) is an m × 1 matrix of evaluations of ˜k.
3Gamma(x|α, β) has p.d.f.

Γ(k)βk xα−1 exp(−x/β).

1

1. The function ˜k from equation (10), studied recently
by Flaxman et al. (2017) and earlier by Sollich &
Williams (2005) as the equivalent kernel.

2. The associated term V(k, Ω, µ) from equation (13),

required for the marginal likelihood.

This is often challenging for compact domains such as the
unit hyper-cube. Such domains are crucial however, if we
are to avoid the well-known edge-effects which arise from
neglecting the fact that our data are sampled from, say, a
two dimensional rectangle. In subsection 5.1 we provide a
simple constructive approach to the case Ω = [0, 1]d. The
following subsection 5.2 presents the general approxima-
tion scheme due to Flaxman et al. (2017), for the case when
we have k but not its Mercer expansion.

5.1. Thin-Plate Semi-norms on the Hyper-Cube

Consider the input domain Ω = [0, π]d with Lebesgue mea-
sure µ. A classical function regularisation term is the so
called m-th order thin-plate spline semi-norm,

(cid:104)f, g(cid:105)T P(m) :=

(cid:88)

|α|=m

(cid:90)

m!
j αj!

(cid:81)

x∈Ω

∂mf
∂xα

∂mg
∂xα dµ(x)

= (cid:104)f, ∆mg(cid:105)L2(Ω) + B.

(14)

Fast Bayesian Permanental Processes

(a) Decomposition of the marginal likelihood

(b) Predictive mean intensity.

Figure 2. Modeling the Redwood dataset: (a) the log marginal likelihood log p(X|Ω, k) along with its component terms from (12)
and (13), as a function of the hyper-parameter a from subsection 5.1; (b) the mean intensity corresponding to the maximum marginal
likelihood parameters, with isolines at 50, 100, 150, 200 and 250. See subsection 6.5 for the details.

Here α is a multi-index running over all indices of total
order |α| := (cid:80)
j αj = m, and the boundary conditions
B come from formal integration (see e.g. Wahba (1990,
section 2.4). We neglect B (for reasons explained shortly)
and include the zero-th derivative to deﬁne

(cid:104)f, g(cid:105)H(k) := (cid:104)f, (a∆m + b)g(cid:105)L2(Ω).

We may select the free parameters a > 0, b > 0 and
m ∈ Z+ using the maximum marginal likelihood criterion.
In general, it is challenging to obtain the expressions we re-
quire in closed form for arbitrary d, Ω and m. The analyti-
cal limit in the literature appears to be the case m = 2 with
dimension d = 1 along with so-called Neumann bound-
ary conditions (which impose a vanishing gradient on the
boundary (Sommerfeld & Straus, 1949)). That ˜k has been
derived in closed form as the reproducing kernel of an as-
sociated Sobolev space by Thomas-Agnan (1996).

We now present a simple but powerful scheme which side-
steps these challenges via a well chosen series expansion.
Consider the basis function

φβ(x) := (2/π)d/2

(cid:112)1/2

[βj =0]

cos(βjxj),

d
(cid:89)

j=1

where β is a multi-index with non-negative (integral) val-
ues, and [·] denotes the indicator function (which is one if
the condition is satisﬁed and zero otherwise). The φβ form
a convenient basis for our purposes. They are orthonormal:

(cid:104)φβ, φγ(cid:105)L2(Ω) = [β = γ],

and also eigenfunctions of our regularisation operator with

(a∆m + b)φβ =

(cid:1)m

β2
j

(cid:17)

+ b

φβ.

(15)

(cid:16)

a(cid:0)

d
(cid:88)

j=1

Now if we restrict the function space to

(cid:110)

H(k) :=

f =

(cid:88)

β≥0

cβφβ : (cid:107)f (cid:107)2

H(k) =

c2
β/λβ < ∞

(cid:111)
,

(cid:88)

β≥0

then it is easily veriﬁed that the boundary conditions B in
(14) vanish. This is a common approach to solving partial
differential equations with Neumann boundary conditions
(see e.g. Sommerfeld & Straus (1949)). By restricting in
this way, we merely impose zero partial derivatives at the
boundary, while otherwise enjoying the usual Fourier se-
ries approximation properties. Hence we can combine the
reproducing property (4) with (14) and (15) to derive

k(x, y) =

λβφβ(x)φβ(y),

(16)

(cid:88)

β≥0

(cid:1)m

+ b).

j=1 β2
j

where λβ := 1/(a(cid:0) (cid:80)d
The above covariance function is not required for our in-
ference algorithm. Rather, the point is that since the basis
is also orthonormal, we may substitute λβ and φβ into (10)
and (13) to obtain ˜k and V(k), as required.

Series truncation. We have discovered closed form ex-
pressions for ˜k only for m ≤ 2 and d = 1. In practice
we may truncate the series at any order and still obtain a
valid model due to the equivalence with the linear model
(6). Hence, a large approximation error (in terms of ˜k)
due to truncation may be irrelevant from a machine learn-
ing perspective, merely implying a different GP prior over
functions. Indeed, the maximum marginal likelihood cri-
terion based on subsubsection 4.1.6 may guide the selec-
tion of an appropriate truncation order, although some care
needs to be taken in this case.

10−610−510−410−310−210−1100parametersaandb−750−500−2500250500logp(X|Ω,k)andcomponentterms−12α(cid:62)˜k(X,X)α−12log(cid:12)(cid:12)(cid:12)˜k(X,X)(cid:12)(αα(cid:62))+2I(cid:12)(cid:12)(cid:12)12V(k,Ω,µ)12mlog(2)logh(X|ˆw)logp(X|Ω,k)maximum0.00.20.40.60.81.00.00.20.40.60.81.050100150200250300Fast Bayesian Permanental Processes

5.2. Arbitrary Covariances and Domains

Flaxman et al. (2017) suggested the following approxima-
tion for ˜k, for the case when k is known but the associated
Mercer expansion is not. The approximation is remark-
ably general and elegant, and may even be applied to non-
vectorial data by employing, say, a kernel function deﬁned
on strings (Lodhi et al., 2002). The idea is to note that the
φi, λi pairs are eigenfunctions of the integral operator (see
Rasmussen & Williams (2006) section 4.3)

Tk : H(k) → H(k)

f (cid:55)→ Tkf :=

k(x, ·)f (x)p(x) dx,

(cid:90)

x∈Ω

where p is related to µ of the previous subsection by
µ(x) = p(x) dx. The Nystr¨om approximation (Nystr¨om,
1928) to Tk draws m samples X from p and deﬁnes
T (X)
x∈X k(x, ·)g(x). Then the eigenfunctions
k
and eigenvectors of Tk may be approximated via the eigen-
vectors e(mat)

and eigenvalues λ(mat)

of 2 k(X, X), as

g := 1
m

(cid:80)

i

i

φ(X)
i
λ(X)
i

√

:=
:= λ(mat)
i

/m.

m/λ(mat)
i

k(·, X)e(mat)

i

These approximations may be used for ˜k, as in (Flaxman
et al., 2017), as well as our V(k, Ω, µ).

6. Experiments

6.1. Setup

Evaluation We use two metrics. The (cid:96)2 Error is the
squared difference to the ground truth w.r.t. the Lebesgue
measure: (cid:82)
x∈Ω(λ(x) − λtrue(x))2 dx. The test log like-
lihood is the logarithm of (1) at an independent
test
sample (one sample being a set of points, i.e.
a sam-
ple from the process), which we summarise by averag-
ing over a ﬁnite number of test sets (for real data where
the ground truth intensity is unknown) and otherwise (if
we have the ground truth) by the analytical expression
EX∼PP(λ)

(cid:105)
(cid:104)
log pX∼PP(ˆλ)(X)

=

(cid:90)

(cid:16)

x∈Ω

λ(x) log ˆλ(x) − ˆλ(x)

dx,

(cid:17)

where P P (λ) is the process with intensity λ (see the sup-
plementary subsection A.1). This evaluation metric is
novel in this context, yet more accurate and computation-
ally cheaper than the sampling of e.g. (Adams et al., 2009).

Decision Theory The above metrics are functions of a
single estimated intensity. In all cases we use the predictive
mean intensity for evaluation. We demonstrate in subsec-
tion A.2 of the supplementary material that this is optimal

(a) (cid:96)2 error vs. marginal likelihood.

(b) Expected log-loss vs. marginal likelihood.

Figure 3. The relationship between the log marginal likelihood
and the (cid:96)2 error (both scaled and shifted to [0, 1]), on the bench-
mark problems of subsection 6.2 — see ﬁgure 3 for the details.

for the expected test log likelihood evaluation (the (cid:96)2 error
cases is similar as is trivial to show).

Algorithms We compare our new Laplace Bayesian
Point Process (LBPP) with two covariances:
the cosine
kernel of subsection 5.1 with ﬁxed m = 2 and hyper-
parameters a and b (LBPP-Cos), and the Gaussian kernel
k(x, z) = γ2 exp(|x − y|2 /(2β2)) with the method of
subsection 5.2 (LBPP-G). We compared with the Varia-
tional Bayesian Point Process (VBPP) (Lloyd et al., 2015)
using the same Gaussian kernel. LBPP-G and VBPP use
a regular grid for X (of subsection 5.2) and the inducing
points, respectively. To compare timing we vary the num-
ber of basis functions, i.e. the number of grid points for
LBPP-G and VBPP, and cosine terms for LBPP-Cos. We
include the baseline kernel smoothing with edge correction
(KS+EC) method (Diggle, 1985; Lloyd et al., 2015). All

0.000.250.500.751.00logp(X|Ω,k)(normalised)0.00.20.40.60.81.0(cid:96)2Error(normalised)λ0λ1λ2λ3λ40.00.20.40.60.81.0logp(X|Ω,k)(normalised)0.00.20.40.60.81.0ExpectedTestLogLikelihood(normalised)λ0λ1λ2λ3λ4Fast Bayesian Permanental Processes

Figure 4. Mean ± one standard error performance on the toy problems of subsection 6.2 as a function of the number of basis functions
(with KS+EC results replicated along the horizontal axis for comparison). The vertical axes of each ﬁgure are normalised scores: see
subsubsection 6.2.2 for the details.

inference is performed with maximum marginal likelihood,
except for KS+EC where we maximise the leave one out
metric described in (Lloyd et al., 2015).

6.2. 1D Toy Examples

We drew ﬁve toy intensities, λ0, λ2, . . . , λ4 as 1
2 f 2 where f
was sampled from the GP of Gaussian covariance (deﬁned
above) with γ = 5 and β = 0.5. Figure 1 depicts λ0 — see
the caption for a description. The remaining test functions
are shown in ﬁgure 6 of the supplementary material.

6.2.1. MODEL SELECTION

As the marginal likelihood log p(X|Ω, k) is a key advan-
tage of our method over the non-probabilistic approach
of Flaxman et al. (2017), we investigated its efﬁcacy for
model selection. Figure 3 plots log p(X|Ω, k) against our
two error metrics, both rescaled to [0, 1] for effective vi-
sualisation, based on a single training sample per test func-
tion. We observe a strong relationship, with larger values of
log p(X|Ω, k) generally corresponding to lower error. This
demonstrates the practical utility of both the marginal like-
lihood itself, and our Laplace approximation to it.

6.2.2. EVALUATION

We sampled 100 training sets from each of our ﬁve toy
functions. Figure 4 shows our evaluation metrics along
with the ﬁtting time as a function of the number of ba-
sis functions. For visualisation all metrics (including ﬁt
time) are scaled to [0, 1] by dividing by the maximum for
the given test function, over data replicates and algorithms.
LBBP-G and and VBPP achieve the best performance, but
our LBPP-G is two orders of magnitude faster. Our KS+EC
implementation follows the methodology of Lloyd et al.
(2015): we ﬁt the kernel density bandwidth using aver-
age leave one out log likelihood. This involves a quadratic
number of log p.d.f. of the truncated normal calculations,

and log-sum-exp calculations, both of which involve large
time constants, but are asymptotically superior to the other
methods we considered. LBBP-Cos is slightly inferior in
terms of expected test log likelihood, which is expected due
to the toy functions having been sampled according to the
same Gaussian kernel of LBPP-G and VBPP (as well as the
density estimator of KS+EC).

6.3. Real Data

We compared the methods on three real world datasets,

• coal: 190 points in one temporal dimension, indi-
cating the time of fatal coal mining accidents in the
United Kingdom, from 1851 to 1962 (Collins, 2013);

• redwood: 195 California redwood tree locations from

a square sampling region (Ripley, 1977);

• cav: 138 caveolae locations from a square sampling
region of muscle ﬁber (Davison & Hinkley, 2013).

6.4. Computational Speed

Similarly to subsubsection 6.2.2 we evaluate the ﬁtting
speed and statistical performance vs. number of basis func-
tions — see ﬁgure 5. We omit the (cid:96)2 error as the ground
truth is unknown. Instead we generate 100 test problems
by each time randomly assigning each original datum to ei-
ther the training or the testing set with equal probability.
Again we observe similar predictive performance of LBPP
and VBPP, but with much faster ﬁt times for our LBPP. In-
terestingly LBPP-Cos slightly outperform LBPP-G.

6.5. 2D California Redwood Dataset

We conclude by further investigating the redwood dataset.
Once again we employed the ML-II procedure to determine
a and b, ﬁxing m = 2, for the covariance function of sub-
section 5.1, using the lowest 32 cosine frequencies in each

510152025303540455055606570750.920.940.96NormalisedMetricExpectedTestLogLikelihood51015202530354045505560657075Numberofbasisfunctions/inducingpoints0.120.140.160.180.20(cid:96)2Error5101520253035404550556065707510−310−210−1100FitTimeKS+ECLBPP-CosLBPP-GVBPPFast Bayesian Permanental Processes

(a) cas dataset

(b) coal dataset

(c) redwood dataset

Figure 5. Mean ± one standard error performance of the different methods on real data, as a function of the number of basis functions
(with KS+EC results replicated along the horizontal axis for comparison). See subsection 6.4 for the details.

dimension for a total of N = 322 basis functions in the ex-
pansion (16). For ease of visualisation we also ﬁxed a = b.

Figure 2 plots the results, including a decomposition of the
log marginal likelihood log p(X|Ω, k) and a visualisation
of the predictive mean. The mean function strongly re-
sembles the result presented by Adams et al. (2009), where
computationally expensive MCMC was employed.

The decomposition of the marginal likelihood on the left
of ﬁgure 2 provides insight into the role of the individual
terms in (12) and (13) which make up log p(X|Ω, k). In
particular, the term V(k, Ω, µ) from (13) acts as a regu-
lariser, guarding against over-ﬁtting, and balancing against
the data term h of (12).

7. Conclusion

We have discussed the permanental process, which places
a Gaussian Process prior over the square root of the inten-

sity function of the Poisson process, and derived the equa-
tions required for empirical Bayes under a Laplace poste-
rior approximation. Our analysis provides 1) an alterna-
tive derivation and probabilistic generalization of (Flaxman
et al., 2017), and 2) an efﬁcient and easier to implement
alternative which does not rely on inducing inputs (but
rather reproducing kernel Hilbert space theory), to the re-
lated Bayesian approach of Lloyd et al. (2015). This further
demonstrates, in a new way, the mathematical convenience
and practical utility of the permanental process formulation
(in comparison with say log Gaussian Cox processes).

Acknowledgements

Thanks to Young Lee, Kar Wai Lim and Cheng Soon Ong
for useful discussions. Adrian is supported by the Aus-
tralian Research Council (ARC) via a Discovery Early Ca-
reer Researcher Award (DE-120102873).

510152025303540455055606570750.40.50.60.70.80.9NormalisedMetric(ratio)TestLogLikelihood5101520253035404550556065707510−310−210−1100FitTimeKS+ECLBPP-CosLBPP-GVBPP510152025303540455055606570750.8NormalisedMetric(ratio)5101520253035404550556065707510−310−210−110051015202530354045505560657075Numberofbasisfunctions/inducingpoints0.50.60.70.8NormalisedMetric(ratio)51015202530354045505560657075Numberofbasisfunctions/inducingpoints10−310−210−1Fast Bayesian Permanental Processes

References

Adams, Ryan P., Murray, Iain, and MacKay, David J. C.
Tractable nonparametric Bayesian inference in Poisson
processes with gaussian process intensities.
In Bot-
tou, L´eon and Littman, Michael (eds.), ICML, pp. 9–16,
Montreal, June 2009.

Aronszajn, N. Theory of reproducing kernels. Transactions

of the American Mathematical Society, 68, 1950.

Baddeley, Adrian. Spatial point processes and their appli-
cations. Lecture Notes in Mathematics: Stochastic Ge-
ometry, 1892:1–75, 2007.

Collins, Michael.

The Inside-Outside Algorithm.

Columbia University Lecture Notes, 2013.

Cont, Rama and Tankov, Peter. Financial modelling with
jump processes. Financial mathematics series. Chapman
and Hall/CRC, London, 2004. ISBN 1-5848-8413-4.

Cunningham, J.P., Shenoy, K.V., and Sahani, M. Fast
Gaussian Process Methods for Point Process Intensity
In Proceedings of the 25th International
Estimation.
Conference on Machine Learning (ICML), pp. 192–199,
Helsinki, Finland, 2008.

Davison, A. C. and Hinkley, D. V.

Bootstrap Meth-
ods and Their Application.
Cambridge University
Press, New York, NY, USA, 2013. ISBN 0511802846,
9780511802843.

Diggle, Peter. A kernel method for smoothing point process

data. Applied Statistics, 34:138–147, 1985.

Diggle, P.J., Moraga, P., Rowlingson, B., and Taylor,
B.M. Spatial and spatio-temporal log-Gaussian Cox pro-
cesses: Extending the geostatistical paradigm. Statistical
Science, 28(4):542–563, 2013.

Flaxman, S., Wilson, A.G., Neill, D.B., Nickisch, H., and
Smola, A.J. Fast Kronecker Inference in Gaussian Pro-
cesses with non-Gaussian Likelihoods. In Proceedings
of the 32nd International Conference on Machine Learn-
ing (ICML), pp. 607–616, Lille, France, 2015.

Flaxman, S., Teh, Y.W., and Sejdinovic, D. Poisson Inten-
sity Estimation with Reproducing Kernels. In Interna-
tional Conference on Artiﬁcial Intelligence and Statistics
(AISTATS), 2017.

Illian, J.B., Sørbye, S.H., and Rue, H. A toolbox for ﬁtting
complex spatial point process models using integrated
nested Laplace approximation (INLA). The Annals of
Applied Statistics, 6(4):1499–1530, 2012.

Kimeldorf, G. and Wahba, G. Some results on Tchebychef-
ﬁan spline functions. Journal of Mathematical Analysis
and Appl., 33:82–95, 1971.

Kom Samo, Y.-L. and Roberts, S. Scalable nonparametric
bayesian inference on point processes with gaussian pro-
cesses. In Proceedings of the 32nd International Con-
ference on Machine Learning (ICML), pp. 2227–2236,
Lille, France, 2015.

Lewis, P. A. W. and Shedler, G. S. Simulation of nonho-
mogeneous Poisson processes by thinning. Naval Res.
Logistics Quart, 26:403–413, 1979.

Lloyd, Chris M., Gunter, Tom, Osborne, Michael A.,
Roberts, Stephen J., and Nickson, Tom. Latent point pro-
cess allocation. In Proceedings of the 19th International
Conference on Artiﬁcial Intelligence and Statistics, AIS-
TATS 2016, Cadiz, Spain, May 9-11, 2016, pp. 389–397,
2016.

Lloyd, C.M., Gunter, T., Osborne, M.A., and Roberts,
S.J. Variational inference for gaussian process modu-
lated poisson processes. In Proceedings of the 32nd In-
ternational Conference on Machine Learning (ICML),
pp. 1814–1822, Lille, France, 2015.

Lodhi, Huma, Saunders, Craig, Shawe-Taylor, John, Cris-
tianini, Nello, and Watkins, Chris. Text Classiﬁcation
using String Kernels. Journal of Machine Learning Re-
search, 2:419–444, February 2002.

McCullagh, Peter and Møller, Jesper. The permanental pro-
cess. Advances in Applied Probability, 38(4):873–888,
2006.

Mercer, J. Functions of positive and negative type, and their
connection with the theory of integral equations. Philo-
sophical Transactions of the Royal Society of London A:
Mathematical, Physical and Engineering Sciences, 209
(441-458):415–446, 1909. ISSN 0264-3952.

Møller, J, Syversveen, A, and Waagepetersen, R. Log gaus-
sian cox processes. Scandanavian Journal of Statistics,
25:451–482, 1998.

Nystr¨om, Evert Johannes.

¨Uber die praktische auﬂ¨osung
von linearen integralgleichungen mit anwendungen auf
randwertaufgaben der potentialtheorie. Commentationes
physico-mathematicae, 4:1–52, 1928.

Rasmussen, C. E. and Williams, C. K.I. Gaussian Pro-
cesses for Machine Learning. Adaptive Computation
and Machine Learning. The MIT Press, Cambridge,
Massachusetts, 2006.

Rathbun, Stephen L. and Cressie, Noel. Asymptotic prop-
erties of estimators for the parameters of spatial inhomo-
geneous poisson point processes. Advances in Applied
Probability, 26:122–154, 1994.

Fast Bayesian Permanental Processes

Ripley, B. D. Modeling Spatial Patterns. Journal of the
Royal Statistical Society Series B Statistical Methodolol-
ogy, 39(2), 1977.

Rue, H˚avard, Martino, Sara, and Chopin, Nicolas. Approx-
imate bayesian inference for latent gaussian models by
using integrated nested laplace approximations. Jour-
nal of the Royal Statistical Society: Series B (Statistical
Methodology), 71(2):319–392, 2009.

Sch¨olkopf, Bernhard, Herbrich, Ralf, and Smola, Alex J.
A generalized representer theorem. In Proc. of the 14th
Annual Conf. on Computational Learning Theory, pp.
416–426, London, UK, 2001. Springer-Verlag. ISBN 3-
540-42343-5.

Shirai, Tomoyuki and Takahashi, Yoichiro. Random point
ﬁelds associated with certain fredholm determinants ii:
Fermion shifts and their ergodic and gibbs properties.
Ann. Probab., 31(3):1533–1564, 07 2003. doi: 10.1214/
aop/1055425789.

Sollich, Peter and Williams, Christopher K. I. Under-
standing Gaussian Process Regression Using the Equiv-
alent Kernel, pp. 211–228. Springer Berlin Heidelberg,
Berlin, Heidelberg, 2005.

Sommerfeld, Arnold and Straus, Ernst Gabor. Partial dif-
ferential equations in physics. Pure and applied mathe-
matics. Academic Press, New York, 1949.

Thomas-Agnan, Christine. Computing a family of repro-
ducing kernels for statistical applications. Numerical Al-
gorithms, 13(1):21–32, 1996.

Titsias, Michalis K. Variational learning of inducing vari-
ables in sparse gaussian processes. In In Artiﬁcial Intel-
ligence and Statistics 12, pp. 567–574, 2009.

Wahba, G. Spline Models for Observational Data. Series
in Applied Math., Vol. 59, SIAM, Philadelphia, 1990.

