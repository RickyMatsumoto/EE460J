Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to
Non-smooth Concave Maximization

Bo Liu 1 Xiao-Tong Yuan 2 Lezi Wang 1 Qingshan Liu 2 Dimitris N. Metaxas 1

Abstract
Iterative Hard Thresholding (IHT) is a class of
projected gradient descent methods for optimiz-
ing sparsity-constrained minimization models,
with the best known efﬁciency and scalability
in practice. As far as we know, the existing
IHT-style methods are designed for sparse min-
It remains open to
imization in primal form.
explore duality theory and algorithms in such a
non-convex and NP-hard problem setting. In this
paper, we bridge this gap by establishing a dual-
ity theory for sparsity-constrained minimization
with (cid:96)2-regularized loss function and proposing
an IHT-style algorithm for dual maximization.
Our sparse duality theory provides a set of suf-
ﬁcient and necessary conditions under which the
original NP-hard/non-convex problem can be e-
quivalently solved in a dual formulation. The
proposed dual IHT algorithm is a super-gradient
method for maximizing the non-smooth dual ob-
jective. An interesting ﬁnding is that the sparse
recovery performance of dual IHT is invariant to
the Restricted Isometry Property (RIP), which is
required by virtually all the existing primal IHT
algorithms without sparsity relaxation. More-
over, a stochastic variant of dual IHT is proposed
for large-scale stochastic optimization. Numer-
ical results demonstrate the superiority of du-
al IHT algorithms to the state-of-the-art primal
IHT-style algorithms in model estimation accu-
racy and computational efﬁciency.

1. Introduction

Sparse learning has emerged as an effective approach
to alleviate model overﬁtting when feature dimension

1Department of CS, Rutgers University, Piscataway, NJ,
08854, USA. 2B-DAT Lab, Nanjing University of Information
Science & Technology, Nanjing, Jiangsu, 210044, China. Cor-
respondence to: Bo Liu <lb507@cs.rutgers.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

outnumbers training sample. Given a set of training
i=1 in which xi ∈ Rd is the feature rep-
samples{(xi, yi)}N
resentation and yi ∈ R the corresponding label, the fol-
lowing sparsity-constrained (cid:96)2-norm regularized loss min-
imization problem is often considered in high-dimensional
analysis:

min
(cid:107)w(cid:107)0≤k

P (w) :=

1
N

N
(cid:88)

i=1

l(w(cid:62)xi, yi) +

(cid:107)w(cid:107)2.

(1)

λ
2

Here l(·; ·) is a convex loss function, w ∈ Rd is the
model parameter vector and λ controls the regularization
strength. For example, the squared loss l(a, b) = (b − a)2
is used in linear regression and the hinge loss l(a, b) =
max{0, 1 − ab} in support vector machines. Due to the p-
resence of cardinality constraint (cid:107)w(cid:107)0 ≤ k, the problem (1)
is simultaneously non-convex and NP-hard in general, and
thus is challenging for optimization. A popular way to ad-
dress this challenge is to use proper convex relaxation, e.g.,
(cid:96)1 norm (Tibshirani, 1996) and k-support norm (Argyriou
et al., 2012), as an alternative of the cardinality constrain-
t. However, the convex relaxation based techniques tend to
introduce bias for parameter estimation.

In this paper, we are interested in algorithms that direct-
ly minimize the non-convex formulation in (1). Early ef-
forts mainly lie in compressed sensing for signal recovery,
which is a special case of (1) with squared loss. Among
others, a family of the so called Iterative Hard Threshold-
ing (IHT) methods (Blumensath & Davies, 2009; Foucart,
2011) have gained signiﬁcant interests and they have been
witnessed to offer the fastest and most scalable solutions in
many cases. More recently, IHT-style methods have been
generalized to handle generic convex loss functions (Beck
& Eldar, 2013; Yuan et al., 2014; Jain et al., 2014) as well
as structured sparsity constraints (Jain et al., 2016). The
common theme of these methods is to iterate between gra-
dient descent and hard thresholding to maintain sparsity of
solution while minimizing the objective value.

Although IHT-style methods have been extensively studied,
the state-of-the-art is only designed for the primal formu-
lation (1). It remains an open problem to investigate the
feasibility of solving the original NP-hard/non-convex for-
mulation in a dual space that might potentially further im-

Dual Iterative Hard Thresholding

prove computational efﬁciency. To ﬁll this gap, inspired by
the recent success of dual methods in regularized learning
problems, we systematically build a sparse duality theory
and propose an IHT-style algorithm along with its stochas-
tic variant for dual optimization.

Overview of our contribution. The core contribution of
this work is two-fold in theory and algorithm. As the theo-
retical contribution, we have established a novel sparse La-
grangian duality theory for the NP-hard/non-convex prob-
lem (1) which to the best of our knowledge has not been
reported elsewhere in literature. We provide in this part a
set of sufﬁcient and necessary conditions under which one
can safely solve the original non-convex problem through
maximizing its concave dual objective function. As the al-
gorithmic contribution, we propose the dual IHT (DIHT)
algorithm as a super-gradient method to maximize the non-
smooth dual objective. In high level description, DIHT iter-
ates between dual gradient ascent and primal hard thresh-
olding pursuit until convergence. A stochastic variant of
DIHT is proposed to handle large-scale learning problem-
s. For both algorithms, we provide non-asymptotic con-
vergence analysis on parameter estimation error, sparsity
recovery, and primal-dual gap as well.
In sharp contrast
to the existing analysis for primal IHT-style algorithms,
our analysis is not relying on Restricted Isometry Property
(RIP) conditions and thus is less restrictive in real-life high-
dimensional statistical settings. Numerical results on syn-
thetic datasets and machine learning benchmark dataset-
s demonstrate that dual IHT signiﬁcantly outperforms the
state-of-the-art primal IHT algorithms in accuracy and ef-
ﬁciency. The theoretical and algorithmic contributions of
this paper are highlighted in below:

• Sparse Lagrangian duality theory: we established a
sparse saddle point theorem (Theorem 1), a sparse
mini-max theorem (Theorem 2) and a sparse strong
duality theorem (Theorem 3).

• Dual optimization: we proposed an IHT-style algo-
rithm along with its stochastic extension for non-
smooth dual maximization. These algorithms have
been shown to converge at the rate of 1
(cid:15) in dual
parameter estimation error and 1
(cid:15)2 in primal-dual
gap (see Theorem 4 and Theorem 5). These guaran-
tees are invariant to RIP conditions which are required
by virtually all the primal IHT-style methods without
using relaxed sparsity levels.

(cid:15)2 ln 1

(cid:15) ln 1

Notation. Before continuing, we deﬁne some notations to
be used. Let x ∈ Rd be a vector and F be an index set. We
use HF (x) to denote the truncation operator that restricts x
to the set F . Hk(x) is a truncation operator which preserves
the top k (in magnitude) entries of x and sets the remaining
to be zero. The notation supp(x) represents the index set

of nonzero entries of x. We conventionally deﬁne (cid:107)x(cid:107)∞ =
maxi |[x]i| and deﬁne xmin = mini∈supp(x) |[x]i|. For a
matrix A, σmax(A) (σmin(A)) denotes its largest (smallest)
singular value.

In §2 we brieﬂy review some relevant work.

Organization. The rest of this paper is organized as fol-
lows:
In
§3 we develop a Lagrangian duality theory for sparsity-
constrained minimization problems. The dual IHT-style al-
gorithms along with convergence analysis are presented in
§4. The numerical evaluation results are reported in §5.1.
Finally, the concluding remarks are made in §6. All the
technical proofs are deferred to the appendix sections.

2. Related Work

For generic convex objective beyond quadratic loss, the rate
of convergence and parameter estimation error of IHT-style
methods were established under proper RIP (or restricted
strong condition number) bound conditions (Blumensath,
2013; Yuan et al., 2014; 2016). In (Jain et al., 2014), several
relaxed variants of IHT-style algorithms were presented for
which the estimation consistency can be established with-
out requiring the RIP conditions. In (Bahmani et al., 2013),
a gradient support pursuit algorithm is proposed and ana-
lyzed. In large-scale settings where a full gradient evalua-
tion on all data becomes a bottleneck, stochastic and vari-
ance reduction techniques have been adopted to improve
the computational efﬁciency of IHT (Nguyen et al., 2014;
Li et al., 2016; Chen & Gu, 2016).

Dual optimization algorithms have been widely used in var-
ious learning tasks including SVMs (Hsieh et al., 2008) and
multi-task learning (Lapin et al., 2014).
In recent years,
stochastic dual optimization methods have gained signif-
icant attention in large-scale machine learning (Shalev-
Shwartz & Zhang, 2013a;b). To further improve computa-
tional efﬁciency, some primal-dual methods are developed
to alternately minimize the primal objective and maximize
the dual objective. The successful examples of primal-dual
methods include learning total variation regularized mod-
el (Chambolle & Pock, 2011) and generalized Dantzig s-
elector (Lee et al., 2016). More recently, a number of s-
tochastic variants (Zhang & Xiao, 2015; Yu et al., 2015)
and parallel variants (Zhu & Storkey, 2016) were devel-
oped to make the primal-dual algorithms more scalable and
efﬁcient.

3. A Sparse Lagrangian Duality Theory

In this section, we establish weak and strong duality the-
ory that guarantees the original non-convex and NP-hard
problem in (1) can be equivalently solved in a dual space.
The results in this part build the theoretical foundation of
developing dual IHT methods.

Dual Iterative Hard Thresholding

L( ¯w, α) ≤ L( ¯w, ¯α) ≤ L(w, ¯α).

(3)

Corollary 1. The mini-max relationship

i (αi) = max

From now on we abbreviate li(w(cid:62)xi) = l(w(cid:62)xi, yi). The
convexity of l(w(cid:62)xi, yi) implies that li(u) is also con-
vex. Let l∗
{αiu − li(u)} be the convex
conjugate of li(u) and F ⊆ R be the feasible set of
αi. According to the well-known expression of li(u) =
maxαi∈F {αiu − l∗
i (αi)}, the problem (1) can be reformu-
lated into the following mini-max formulation:

u

min
(cid:107)w(cid:107)0≤k

1
N

N
(cid:88)

i=1

max
αi∈F

{αiw(cid:62)xi − l∗

i (αi)} +

(cid:107)w(cid:107)2.

(2)

The following Lagrangian form will be useful in analysis:

L(w, α) =

(cid:0)αiw(cid:62)xi − l∗

i (αi)(cid:1) +

(cid:107)w(cid:107)2,

1
N

N
(cid:88)

i=1

where α = [α1, ..., αN ] ∈ F N is the vector of dual vari-
ables. We now introduce the following concept of sparse
saddle point which is a restriction of the conventional sad-
dle point to the setting of sparse optimization.
Deﬁnition 1 (Sparse Saddle Point). A pair ( ¯w, ¯α) ∈ Rd ×
F N is said to be a k-sparse saddle point for L if (cid:107) ¯w(cid:107)0 ≤ k
and the following holds for all (cid:107)w(cid:107)0 ≤ k, α ∈ F N :

λ
2

λ
2

Different from the conventional deﬁnition of saddle point,
the k-sparse saddle point only requires the inequality (3)
holds for any arbitrary k-sparse vector w. The follow-
ing result is a basic sparse saddle point theorem for L.
Throughout the paper, we will use f (cid:48)(·) to denote a sub-
gradient (or super-gradient) of a convex (or concave) func-
tion f (·), and use ∂f (·) to denote its sub-differential (or
super-differential).
Theorem 1 (Sparse Saddle Point Theorem). Let ¯w ∈ Rd
be a k-sparse primal vector and ¯α ∈ F N be a dual vector.
Then the pair ( ¯w, ¯α) is a sparse saddle point for L if and
only if the following conditions hold:

(a) ¯w solves the primal problem in (1);

(b) ¯α ∈ [∂l1( ¯w(cid:62)x1), ..., ∂lN ( ¯w(cid:62)xN )];

(c) ¯w = Hk

(cid:16)

− 1
λN

(cid:80)N

i=1 ¯αixi

(cid:17)

.

Proof. A proof of this result is given in Appendix A.1.

Remark 1. Theorem 1 shows that the conditions (a)∼(c)
are sufﬁcient and necessary to guarantee the existence of a
sparse saddle point for the Lagrangian form L. This result
is different from from the traditional saddle point theorem
which requires the use of the Slater Constraint Qualiﬁca-
tion to guarantee the existence of saddle point.

Remark 2. Let us consider P (cid:48)( ¯w) = 1
i=1 ¯αixi +
N
λ ¯w ∈ ∂P ( ¯w). Denote ¯F = supp( ¯w). It is easy to veri-
fy that the condition (c) in Theorem 1 is equivalent to

(cid:80)N

H ¯F (P (cid:48)( ¯w)) = 0,

¯wmin ≥

(cid:107)P (cid:48)( ¯w)(cid:107)∞.

1
λ

The following sparse mini-max theorem guarantees that the
min and max in (2) can be safely switched if and only if
there exists a sparse saddle point for L(w, α).

Theorem 2 (Sparse Mini-Max Theorem). The mini-max
relationship

max
α∈F N

min
(cid:107)w(cid:107)0≤k

L(w, α) = min

(cid:107)w(cid:107)0≤k

max
α∈F N

L(w, α)

(4)

holds if and only if there exists a sparse saddle point ( ¯w, ¯α)
for L.

Proof. A proof of this result is given in Appendix A.2.

The sparse mini-max result in Theorem 2 provides sufﬁ-
cient and necessary conditions under which one can safely
exchange a min-max for a max-min, in the presence of s-
parsity constraint. The following corollary is a direct con-
sequence of applying Theorem 1 to Theorem 2.

max
α∈F N

min
(cid:107)w(cid:107)0≤k

L(w, α) = min

(cid:107)w(cid:107)0≤k

max
α∈F N

L(w, α)

holds if and only if there exist a k-sparse primal vector
¯w ∈ Rd and a dual vector ¯α ∈ F N such that the con-
ditions (a)∼(c) in Theorem 1 are satisﬁed.

The mini-max result in Theorem 2 can be used as a basis for
establishing sparse duality theory. Indeed, we have already
shown the following:

min
(cid:107)w(cid:107)0≤k

max
α∈F N

L(w, α) = min

P (w).

(cid:107)w(cid:107)0≤k

This is called the primal minimization problem and it is
the min-max side of the sparse mini-max theorem. The
other side, the max-min problem, will be called as the
dual maximization problem with dual objective function
D(α) := min(cid:107)w(cid:107)0≤k L(w, α), i.e.,

max
α∈F N

D(α) = max
α∈F N

min
(cid:107)w(cid:107)0≤k

L(w, α).

(5)

The following Lemma 1 shows that the dual objective func-
tion D(α) is concave and explicitly gives the expression of
its super-differential.

Lemma 1. The dual objective function D(α) is given by

D(α) =

−l∗

i (αi) −

(cid:107)w(α)(cid:107)2,

λ
2

1
N

N
(cid:88)

i=1

Dual Iterative Hard Thresholding

− 1
where w(α) = Hk
N λ
is concave and its super-differential is given by

i=1 αixi

(cid:80)N

. Moreover, D(α)

(cid:16)

(cid:17)

∂D(α) =

[w(α)(cid:62)x1−∂l∗

1(α1), ..., w(α)(cid:62)xN −∂l∗

N (αN )].

1
N

Particularly, if w(α) is unique at α and {l∗
i }i=1,...,N are
differentiable, then ∂D(α) is unique and it is the super-
gradient of D(α).

Algorithm 1 Dual Iterative Hard Thresholding (DIHT)
Input

: Training set {xi, yi}N
i=1. Regularization strength
parameter λ. Cardinality constraint k. Step-size η.
1 = ... = α(0)

Initialization w(0) = 0, α(0)
for t = 1, 2, ..., T do

N = 0.

(S1) Dual projected super-gradient ascent: ∀ i ∈
{1, 2, ..., N },

α(t)

i = PF

α(t−1)
i

+ η(t−1)g(t−1)

i

(cid:16)

(cid:17)

,

(6)

Proof. A proof of this result is given in Appendix A.3.

Based on Theorem 1 and Theorem 2, we are able to further
establish a sparse strong duality theorem which gives the
sufﬁcient and necessary conditions under which the opti-
mal values of the primal and dual problems coincide.
Theorem 3 (Sparse Strong Duality Theorem). Let ¯w ∈ Rd
is a k-sparse primal vector and ¯α ∈ F N be a dual vec-
tor. Then ¯α solves the dual problem in (5), i.e., D(¯α) ≥
D(α), ∀α ∈ F N , and P ( ¯w) = D(¯α) if and only if the
pair ( ¯w, ¯α) satisﬁes the conditions (a)∼(c) in Theorem 1.

Proof. A proof of this result is given in Appendix A.4.

:=
We deﬁne the sparse primal-dual gap (cid:15)P D(w, α)
P (w) − D(α). The main message conveyed by Theo-
rem 3 is that the sparse primal-dual gap reaches zero at the
primal-dual pair ( ¯w, ¯α) if and only if the conditions (a)∼(c)
in Theorem 1 hold.

The sparse duality theory developed in this section sug-
gests a natural way for ﬁnding the global minimum of the
sparsity-constrained minimization problem in (1) via max-
imizing its dual problem in (5). Once the dual maximizer
¯α is estimated, the primal sparse minimizer ¯w can then be
recovered from it according to the prima-dual connection
(cid:17)
¯w = Hk
as given in the condition (c).
Since the dual objective function D(α) is shown to be con-
cave, its global maximum can be estimated using any con-
vex/concave optimization method. In the next section, we
present a simple projected super-gradient method to solve
the dual maximization problem.

i=1 ¯αixi

− 1
λN

(cid:80)N

(cid:16)

4. Dual Iterative Hard Thresholding

Generally, D(α) is a non-smooth function since: 1) the
conjugate function l∗
i of an arbitrary convex loss li is gen-
erally non-smooth and 2) the term (cid:107)w(α)(cid:107)2 is non-smooth
with respect to α due to the truncation operation involved
in computing w(α). Therefore, smooth optimization meth-
ods are not directly applicable here and we resort to sub-
gradient-type methods to solve the non-smooth dual maxi-
mization problem in (5).

i

= 1

N (x(cid:62)

i w(t−1) − l∗(cid:48)

where g(t−1)
)) is the
super-gradient and PF (·) is the Euclidian projection
operator with respect to feasible set F.
(S2) Primal hard thresholding:

i (α(t−1)

i

w(t) = Hk

(cid:32)

−

1
λN

N
(cid:88)

i=1

(cid:33)

α(t)

i xi

.

(7)

end
Output: w(T ).

4.1. Algorithm

The Dual Iterative Hard Thresholding (DIHT) algorith-
m, as outlined in Algorithm 1, is essentially a project-
ed super-gradient method for maximizing D(α). The
procedure generates a sequence of prima-dual pairs
(w(0), α(0)), (w(1), α(1)), . . . from an initial pair w(0) = 0
and α(0) = 0. At the t-th iteration, the dual update step S1
conducts the projected super-gradient ascent in (6) to up-
date α(t) from α(t−1) and w(t−1). Then in the primal up-
date step S2, the primal variable w(t) is constructed from
α(t) using a k-sparse truncation operation in (7).

When a batch estimation of super-gradient D(cid:48)(α) becomes
expensive in large-scale applications, it is natural to consid-
er the stochastic implementation of DIHT, namely SDIHT,
as outlined in Algorithm 2. Different from the batch com-
putation in Algorithm 1, the dual update step S1 in Algo-
rithm 2 randomly selects a block of samples (from a given
block partition of samples) and update their corresponding
dual variables according to (8). Then in the primal update
step S2.1, we incrementally update an intermediate accu-
mulation vector ˜w(t) which records − 1
i xi as a
λN
weighted sum of samples. In S2.2, the primal vector w(t) is
updated by applying k-sparse truncation on ˜w(t). The SDI-
HT is essentially a block-coordinate super-gradient method
for the dual problem. Particularly, in the extreme case
m = 1, SDIHT reduces to the batch DIHT. At the oppo-
site extreme end with m = N , i.e., each block contains
one sample, SDIHT becomes a stochastic coordinate-wise
super-gradient method.

i=1 α(t)

(cid:80)N

Dual Iterative Hard Thresholding

Algorithm 2 Stochastic Dual Iterative Hard Thresholding
(SDIHT)
Input

: Training set {xi, yi}N
i=1. Regularization strength
parameter λ. Cardinality constraint k. Step-size
η. A block disjoint partition {B1, ..., Bm} of the
sample index set [1, ..., N ].
Initialization w(0) = ˜w(0) = 0, α(0)
for t = 1, 2, ..., T do

1 = ... = α(0)

N = 0.

(S1) Dual projected super-gradient ascent: Uniformly
randomly select an index block B(t)
i ∈ {B1, ..., Bm}.
For all j ∈ B(t)

as

i update α(t)
(cid:16)

j

α(t)

j = PF

α(t−1)
j

+ η(t−1)g(t−1)

j

(cid:17)

.

(8)

Particularly, σmax(X, d) = σmax(X) and σmin(X, d) =
σmin(X). We say a univariate differentiable function f (x)
is γ-smooth if ∀x, y, f (y) ≤ f (x) + (cid:104)f (cid:48)(x), y − x(cid:105) + γ
2 |x −
y|2. The following is our main theorem on the dual param-
eter estimation error, support recovery and primal-dual gap
of DIHT.

Theorem 4. Assume that
η(t) =
(λN µ+σmin(X,k))2 and c2 = (r + λρ)2 (cid:16)

Set
(λN µ+σmin(X,k))(t+1) . Deﬁne constants c1 =
(cid:17)2
.

1 + σmax(X,k)

is 1/µ-smooth.

N 3(r+λρ)2

λN 2

µλN

li

(a) Parameter

estimation error:

sequence
{α(t)}t≥1 generated by Algorithm 1 satisﬁes the
following estimation error inequality:

The

j = α(t−1)

Set α(t)
, ∀j /∈ B(t)
.
(S2) Primal hard thresholding:
– (S2.1) Intermediate update:

j

i

(cid:107)α(t) − ¯α(cid:107)2 ≤ c1

(cid:18) 1
t

+

(cid:19)

,

ln t
t

˜w(t) = ˜w(t−1) −

(α(t)

j − α(t−1)

j

)xj.

(9)

1
λN

(cid:88)

j∈B(t)

i

(b) Support recovery and primal-dual gap: Assume ad-
λ (cid:107)P (cid:48)( ¯w)(cid:107)∞ > 0. Then,

ditionally that ¯(cid:15) := ¯wmin − 1
supp(w(t)) = supp( ¯w) when

– (S2.2) Hard thresholding: w(t) = Hk( ˜w(t)).

end
Output: w(T ).

The dual update (8) in SDIHT is much more efﬁcient than
DIHT as the former only needs to access a small subset of
samples at a time. If the hard thresholding operation in pri-
mal update becomes a bottleneck, e.g., in high-dimensional
settings, we suggest to use SDIHT with relatively smaller
number of blocks so that the hard thresholding operation in
S2.2 can be less frequently called.

4.2. Convergence analysis

We now analyze the non-asymptotic convergence behav-
ior of DIHT and SDIHT. In the following analysis, we
will denote ¯α = arg maxα∈F N D(α) and use the abbre-
viation (cid:15)(t)
P D := (cid:15)P D(w(t), α(t)). Let r = maxa∈F |a|
be the bound of the dual feasible set F and ρ =
maxi,a∈F |l∗(cid:48)
For example, such quantities exist
when li and l∗
i are Lipschitz continuous (Shalev-Shwartz
& Zhang, 2013b). We also assume without loss of gener-
ality that (cid:107)xi(cid:107) ≤ 1. Let X = [x1, ..., xN ] ∈ Rd×N be the
data matrix. Given an index set F , we denote XF as the
restriction of X with rows restricted to F . The following
quantities will be used in our analysis:

i (a)|.

σ2
max(X, s) = sup
u∈Rn,F

σ2
min(X, s) = inf

u∈Rn,F

(cid:8)u(cid:62)X (cid:62)

(cid:8)u(cid:62)X (cid:62)

F XF u | |F | ≤ s, (cid:107)u(cid:107) = 1(cid:9) ,
F XF u | |F | ≤ s, (cid:107)u(cid:107) = 1(cid:9) .

t ≥ t0 =

(cid:24) 12c1σ2

max(X)

λ2N 2¯(cid:15)2

ln

12c1σ2

max(X)

λ2N 2¯(cid:15)2

(cid:25)

.

Moreover, for any (cid:15) > 0, the primal-dual gap sat-
isﬁes (cid:15)(t)
P D ≤ (cid:15) when t ≥ max{t0, t1} where t1 =
(cid:6) 3c1c2
λ2N (cid:15)2 ln 3c1c2
λ2N (cid:15)2

(cid:7).

Proof. A proof of this result is given in Appendix A.5.

Remark 3. The theorem allows µ = 0 when σmin(X, k) >
0. If µ > 0, then σmin(X, k) is allowed to be zero and thus
the step-size can be set as η(t) = N

µ(t+1) .

P ≤ (cid:15)(t)

Consider primal sub-optimality (cid:15)(t)
P := P (w(t)) − P ( ¯w).
Since (cid:15)(t)
P D always holds, the convergence rates
in Theorem 4 are applicable to the primal sub-optimality
as well. An interesting observation is that these conver-
gence results on (cid:15)(t)
P are not relying on the Restricted Isom-
etry Property (RIP) (or restricted strong condition number)
which is required in most existing analysis of IHT-style al-
gorithms (Blumensath & Davies, 2009; Yuan et al., 2014).
In (Jain et al., 2014), several relaxed variants of IHT-style
algorithms are presented for which the estimation consis-
tency can be established without requiring the RIP condi-
tions. In contrast to the RIP-free sparse recovery analysis
in (Jain et al., 2014), our Theorem 4 does not require the
sparsity level k to be relaxed.

For SDIHT, we can establish similar non-asymptotic con-
vergence results as summarized in the following theorem.
Theorem 5. Assume that li is 1/µ-smooth. Set η(t) =
λmN 2
(λN µ+σmin(X,k))(t+1) .

Dual Iterative Hard Thresholding

(a) Parameter

estimation error:

sequence
{α(t)}t≥1 generated by Algorithm 2 satisﬁes the
following expected estimation error inequality:

The

E[(cid:107)α(t) − ¯α(cid:107)2] ≤ mc1

(cid:18) 1
t

+

(cid:19)

,

ln t
t

(b) Support recovery and primal-dual gap: Assume ad-
ditionally that ¯(cid:15) := ¯wmin − 1
λ (cid:107)P (cid:48)( ¯w)(cid:107)∞ > 0. Then,
for any δ ∈ (0, 1), with probability at least 1 − δ, it
holds that supp(w(t)) = supp( ¯w) when

t ≥ t2 =

(cid:24) 12mc1σ2

max(X)

λ2δ2N 2¯(cid:15)2

ln

12mc1σ2

max(X)

λ2δ2N 2¯(cid:15)2

(cid:25)

.

Moreover, with probability at least 1 − δ, the primal-
dual gap satisﬁes (cid:15)(t)
P D ≤ (cid:15) when t ≥ max{4t2, t3}
where t3 = (cid:6) 12mc1c2

λ2δ2N (cid:15)2 ln 12mc1c2
λ2δ2N (cid:15)2

(cid:7).

Proof. A proof of this result is given in Appendix A.6.

Remark 4. Theorem 5 shows that, up to scaling factors,
the expected or high probability iteration complexity of S-
DIHT is almost identical to that of DIHT. The scaling fac-
tor m appeared in t2 and t3 reﬂects a trade-off between
the decreased per-iteration cost and the increased iteration
complexity.

5. Experiments

This section dedicates in demonstrating the accuracy and
efﬁciency of the proposed algorithms. We ﬁrst show the
model estimation performance of DIHT when applied to
sparse ridge regression models on synthetic datasets. Then
we evaluate the efﬁciency of DIHT/SDIHT on sparse (cid:96)2-
regularized Huber loss and Hinge loss minimization tasks
using real-world datasets.

5.1. Model parameter estimation accuracy evaluation

A synthetic model is generated with sparse model param-
]. Each xi ∈ Rd of the
eter ¯w = [1, 1, · · · , 1
, 0, 0, · · · , 0
(cid:124)
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:125)
d−¯k

(cid:123)(cid:122)
¯k
N training data examples {xi}N
i=1 is designed to have t-
wo components. The ﬁrst component is the top-¯k feature
dimensions drawn from multivariate Gaussian distribution
N (µ1, Σ). Each entry in µ1 ∈ R¯k independently follows
standard normal distribution. The entries of covariance
Σij =
. The second component consist-
s the left d − ¯k feature dimensions. It follows N (µ2, I)
where each entry in µ2 ∈ Rd−¯k is drawn from standard
normal distribution. We simulate two data parameter set-
tings: (1) d = 500, ¯k = 100; (2) d = 300, ¯k = 100.
In each data parameter setting 150 random data copies are

i = j
i (cid:54)= j

(cid:26) 1

0.25

(a) Model estimation error

(b) Percentage of support re-
covery success

Figure 1. Model parameter estimation performance comparison
between DIHT and baseline algorithms on the two synthetic
dataset settings. The varying number of training sample is de-
noted by N .

produced independently. The task is to solve the following
(cid:96)2-regularized sparse linear regression problem:

min
(cid:107)w(cid:107)≤k

1
N

N
(cid:88)

i=1

lsq(yi, w(cid:62)xi) +

(cid:107)w(cid:107)2,

λ
2

i

sq(αi) = α2

where lsq(yi, w(cid:62)xi) = (yi − w(cid:62)xi)2. The responses
{yi}N
i=1 are produced by yi = ¯w(cid:62)xi + εi, where εi ∼
N (0, 1). The convex conjugate of lsq(yi, w(cid:62)xi) is known
as l∗
4 + yiαi (Shalev-Shwartz & Zhang, 2013b).
We consider solving the problem under the sparsity level
k = ¯k. Two measurements are calculated for evaluation.
The ﬁrst is parameter estimation error (cid:107)w − ¯w(cid:107)/(cid:107) ¯w(cid:107). A-
part from it we calculate the percentage of successful sup-
port recovery (P SSR) as the second performance metric.
A successful support recovery is obtained if supp( ¯w) =
supp(w). The evaluation is conducted on the generated
batch data copies to calculate the percentage of successful
support recovery. We use 50 data copies as validation set
to select the parameter λ from {10−6, ..., 102} and the per-
centage of successful support recovery is evaluated on the
other 100 data copies.

Iterative hard thresholding (IHT) (Blumensath & Davies,
2009) and hard thresholding pursuit (HTP) (Foucart, 2011)
are used as the baseline primal algorithms. The parameter
estimation error and percentage of successful support re-
covery curves under varying training size are illustrated in
Figure 1. We can observe from this group of curves that DI-
HT consistently achieves lower parameter estimation error
and higher rate of successful support recovery than IHT and
HTP. It is noteworthy that most signiﬁcant performance gap
between DIHT and the baselines occurs when the training
size N is comparable to or slightly smaller than the sparsity
level ¯k.

N5075100125150kw!7wk=k7wk0.10.20.30.40.50.6IHT(d=500)HTP(d=500)DIHT(d=500)IHT(d=300)HTP(d=300)DIHT(d=300)N5075100125150PSSR(%)020406080100IHT(d=500)HTP(d=500)DIHT(d=500)IHT(d=300)HTP(d=300)DIHT(d=300)Dual Iterative Hard Thresholding

(a) RCV1, λ = 0.002

(b) RCV1, λ = 0.0002

(c) News20, λ = 0.002

(d) News20, λ = 0.0002

Figure 2. Huber loss: Running time (in second) comparison between the considered algorithms.

(a) DIHT on RCV1

(b) SDIHT on RCV1

(c) DIHT on News20

(d) SDIHT on News20

Figure 3. Huber loss: The primal-dual gap evolving curves of DIHT and SDIHT. k = 600 for RCV1 and k = 60000 for News20.

5.2. Model training efﬁciency evaluation

5.2.1. HUBER LOSS MODEL LEARNING

We now evaluate the considered algorithms on the follow-
ing (cid:96)2-regularized sparse Huber loss minimization prob-
lem:

min
(cid:107)w(cid:107)0≤k

1
N

N
(cid:88)

i=1

lHuber(yix(cid:62)

i w) +

(cid:107)w(cid:107)2,

(10)

λ
2

where

lHuber(yix(cid:62)

i w) =






0
1 − yix(cid:62)
1
2γ (1 − yix(cid:62)

i w − γ
2
i w)2

i w ≥ 1
i w < 1 − γ

yix(cid:62)
yix(cid:62)
otherwise

.

It is known that (Shalev-Shwartz & Zhang, 2013b)

l∗
Huber(αi) =

(cid:26) yiαi + γ
+∞

2 α2

i

if yiαi ∈ [−1, 0]
otherwise

.

Two binary benchmark datasets from LibSVM data reposi-
tory1, RCV1 (d = 47, 236) and News20 (d = 1, 355, 191),
are used for algorithm efﬁciency evaluation and compari-
son. We select 0.5 million samples from RCV1 dataset for

1https://www.csie.ntu.edu.tw/˜cjlin/

libsvmtools/datasets/binary.html

model training (N (cid:29) d). For news20 dataset, all of the
19, 996 samples are used as training data (d (cid:29) N ).

We evaluate the algorithm efﬁciency of DIHT and SDIHT
by comparing their running time against three primal base-
line algorithms: IHT, HTP and gradient hard thresholding
with stochastic variance reduction (SVR-GHT) (Li et al.,
2016). We ﬁrst run IHT by setting its convergence criterion
to be |P (w(t))−P (w(t−1))|
≤ 10−4 or maximum number of
iteration is reached. After that we test the time cost spend
by other algorithms to make the primal loss reach P (w(t)).
The parameter update step-size of all the considered algo-
rithms is tuned by grid search. The parameter γ is set to be
0.25. For the two stochastic algorithms SDIHT and SVR-
GHT we randomly partition the training data into |B| = 10
mini-batches.

P (w(t))

Figure 2 shows the running time curves on both datasets
under varying sparsity level k and regularization strength
λ = 0.002, 0.0002.
It is obvious that under all tested
(k, λ) conﬁgurations on both datasets, DIHT and SDIHT
need much less time than the primal baseline algorithms,
IHT, HTP and SVR-GHT to reach the same primal sub-
optimality. Figure 3 shows the primal-dual gap conver-
gence curves with respect to the number of epochs. This
group of results support the theoretical prediction in Theo-
rem 4 and 5 that (cid:15)P D converges non-asymptotically.

200400600800100010-1100101102Running timeIHTHTPSVR-GHTDIHTSDIHT200400600800100010-1100101102103Running timeIHTHTPSVR-GHTDIHTSDIHT2000040000600008000010000010-1100101102Running timeIHTHTPSVR-GHTDIHTSDIHT2000040000600008000010000010-1100101102103Running timeIHTHTPSVR-GHTDIHTSDIHT20406080Epoch10-310-210-1100101102Primal-dual gap=0.002=0.0002246810Epoch10-210-1100Primal-dual gap=0.002=0.000220406080Epoch10-310-210-1100101102103Primal-dual gap=0.002=0.0002246810Epoch10-310-210-1100101Primal-dual gap=0.002=0.0002Dual Iterative Hard Thresholding

(a) RCV1, λ = 0.002

(b) RCV1, λ = 0.0002

(c) News20, λ = 0.002

(d) News20, λ = 0.0002

Figure 4. Hinge loss: Running time (in second) comparison between the considered algorithms.

(a) DIHT on RCV1

(b) SDIHT on RCV1

(c) DIHT on News20

(d) SDIHT on News20

Figure 5. Hinge loss: The primal-dual gap evolving curves of DIHT and SDIHT. k = 600 for RCV1 and k = 60000 for News20.

5.2.2. HINGE LOSS MODEL LEARNING

We further test the performance of our algorithms when
applied to the following (cid:96)2-regularized sparse hinge loss
minimization problem:

min
(cid:107)w(cid:107)0≤k

1
N

N
(cid:88)

i=1

lHinge(yix(cid:62)

i w) +

(cid:107)w(cid:107)2,

λ
2

where lHinge(yix(cid:62)
dard to know (Hsieh et al., 2008)

i w) = max(0, 1 − yiw(cid:62)xi). It is stan-

l∗
Hinge(αi) =

(cid:26) yiαi

if yiαi ∈ [−1, 0]

+∞ otherwise

.

We follow the same experiment protocol as in § 5.2.1
to compare the considered algorithms on the benchmark
datasets. The time cost comparison is illustrated in Fig-
ure 4 and the prima-dual gap sub-optimality is illustrated
in Figure 5. This group of results indicate that DIHT and
SDIHT still exhibit remarkable efﬁciency advantage over
the considered primal IHT algorithms even when the loss
function is non-smooth.

6. Conclusion

In this paper, we systematically investigate duality theory
and algorithms for solving the sparsity-constrained mini-
mization problem which is NP-hard and non-convex in its

primal form. As a theoretical contribution, we develop a
sparse Lagrangian duality theory which guarantees strong
duality in sparse settings, under mild sufﬁcient and neces-
sary conditions. This theory opens the gate to solve the
original NP-hard/non-convex problem equivalently in a d-
ual space. We then propose DIHT as a ﬁrst-order method to
maximize the non-smooth dual concave formulation. The
algorithm is characterized by dual super-gradient ascen-
t and primal hard thresholding. To further improve itera-
tion efﬁciency in large-scale settings, we propose SDIHT
as a block stochastic variant of DIHT. For both algorithm-
s we have proved sub-linear primal-dual gap convergence
rate when the primal loss is smooth, without assuming RIP-
style conditions. Based on our theoretical ﬁndings and nu-
merical results, we conclude that DIHT and SDIHT are the-
oretically sound and computationally attractive alternatives
to the conventional primal IHT algorithms, especially when
the sample size is smaller than feature dimensionality.

Acknowledgements

Xiao-Tong Yuan is supported in part by Natural Sci-
ence Foundation of China (NSFC) under Grant 61402232,
Grant 61522308, and in part by Natural Science Founda-
tion of Jiangsu Province of China (NSFJPC) under Grant
BK20141003. Qingshan Liu is supported in part by NSFC
under Grant 61532009.

200400600800100010-1100101102Running timeIHTHTPSVR-GHTDIHTSDIHT200400600800100010-1100101102103Running timeIHTHTPSVR-GHTDIHTSDIHT20000400006000080000100000100101102Running timeIHTHTPSVR-GHTDIHTSDIHT2000040000600008000010000010-1100101102Running timeIHTHTPSVR-GHTDIHTSDIHT20406080Epoch10-310-210-1100101102Primal-dual gap=0.002=0.0002246810Epoch10-210-1100Primal-dual gap=0.002=0.000220406080Epoch10-310-210-1100101102103Primal-dual gap=0.002=0.0002246810Epoch10-210-1100101102Primal-dual gap=0.002=0.0002Dual Iterative Hard Thresholding

References

Argyriou, Andreas, Foygel, Rina, and Srebro, Nathan. S-
parse prediction with the k-support norm. In Advances
in Neural Information Processing Systems, 2012.

Bahmani, Sohail, Raj, Bhiksha, and Boufounos, Petros T.
Journal of

Greedy sparsity-constrained optimization.
Machine Learning Research, 14(Mar):807–841, 2013.

Beck, Amir and Eldar, Yonina C. Sparsity constrained
nonlinear optimization: Optimality conditions and al-
gorithms. SIAM Journal on Optimization, 23(3):1480–
1509, 2013.

Blumensath, Thomas. Compressed sensing with nonlinear
observations and related nonlinear optimization prob-
lems. IEEE Transactions on Information Theory, 59(6):
3466–3474, 2013.

Blumensath, Thomas and Davies, Mike E. Iterative hard
thresholding for compressed sensing. Applied and Com-
putational Harmonic Analysis, 27(3):265–274, 2009.

Chambolle, Antonin and Pock, Thomas. A ﬁrst-order
primal-dual algorithm for convex problems with appli-
cations to imaging. Journal of Mathematical Imaging
and Vision, 40(1):120–145, 2011.

Chen, Jinghui and Gu, Quanquan. Accelerated stochas-
tic block coordinate gradient descent for sparsity con-
strained nonconvex optimization. In Conference on Un-
certainty in Artiﬁcial Intelligence, 2016.

Foucart, Simon. Hard thresholding pursuit: an algorithm
for compressive sensing. SIAM Journal on Numerical
Analysis, 49(6):2543–2563, 2011.

Hsieh, Cho-Jui, Chang, Kai-Wei, Lin, Chih-Jen, Keerthi,
S Sathiya, and Sundararajan, Sellamanickam. A dual
coordinate descent method for large-scale linear svm. In
International conference on Machine learning, 2008.

Jain, Prateek, Tewari, Ambuj, and Kar, Purushottam. On it-
erative hard thresholding methods for high-dimensional
m-estimation. In Advances in Neural Information Pro-
cessing Systems, 2014.

Jain, Prateek, Rao, Nikhil, and Dhillon, Inderjit. Structured
sparse regression via greedy hard-thresholding. arXiv
preprint arXiv:1602.06042, 2016.

Lapin, Maksim, Schiele, Bernt, and Hein, Matthias. Scal-
able multitask representation learning for scene classi-
ﬁcation. In IEEE Conference on Computer Vision and
Pattern Recognition, 2014.

Lee, Sangkyun, Brzyski, Damian, and Bogdan, Malgorza-
ta. Fast saddle-point algorithm for generalized dantzig
selector and fdr control with the ordered (cid:96)1-norm.
In
International Conference on Artiﬁcial Intelligence and
Statistics, 2016.

Li, Xingguo, Zhao, Tuo, Arora, Raman, Liu, Han, and
Haupt, Jarvis. Stochastic variance reduced optimization
for nonconvex sparse learning. In International Confer-
ence on Machine Learning, 2016.

Nguyen, Nam, Needell, Deanna, and Woolf, Tina. Lin-
ear convergence of stochastic iterative greedy algorithms
with sparse constraints. arXiv preprint arXiv:1407.0088,
2014.

Shalev-Shwartz, Shai and Zhang, Tong. Accelerated mini-
batch stochastic dual coordinate ascent. In Advances in
Neural Information Processing Systems, 2013a.

Shalev-Shwartz, Shai and Zhang, Tong. Stochastic du-
al coordinate ascent methods for regularized loss. The
Journal of Machine Learning Research, 14(1):567–599,
2013b.

Tibshirani, Robert. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society. Series
B (Methodological), pp. 267–288, 1996.

Yu, Adams Wei, Lin, Qihang, and Yang, Tianbao. Dou-
bly stochastic primal-dual coordinate method for empir-
ical risk minimization and bilinear saddle-point problem.
arXiv preprint arXiv:1508.03390, 2015.

Yuan, Xiao-Tong., Li, Ping, and Zhang, Tong. Gradien-
t hard thresholding pursuit for sparsity-constrained op-
In International Conference on Machine
timization.
Learning, 2014.

Yuan, Xiao-Tong, Li, Ping, and Zhang, Tong. Exact recov-
ery of hard thresholding pursuit. In Advances in Neural
Information Processing Systems, 2016.

Zhang, Yuchen and Xiao, Lin. Stochastic primal-dual co-
ordinate method for regularized empirical risk minimiza-
tion. In International Conference on Machine Learning,
2015.

Zhu, Zhanxing and Storkey, Amos J. Stochastic paral-
lel block coordinate descent for large-scale saddle point
problems. In AAAI Conference on Artiﬁcial Intelligence,
2016.

