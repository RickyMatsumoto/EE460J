Supplementary Materials

A Inverse polynomial kernel and Gaussian kernel

In this appendix, we describe the properties of the two types of kernels — the inverse polynomial
kernel (14) and the Gaussian RBF kernel (15). We prove that the associated reproducing kernel
Hilbert Spaces (RKHS) of these kernels contain ﬁlters taking the form h : z (cid:55)→ σ((cid:104)w, z(cid:105)) for
particular activation functions σ.

A.1 Inverse polynomial kernel

We ﬁrst verify that the function (14) is a kernel function. This holds since that we can ﬁnd a
mapping ϕ : Rd1 → (cid:96)2(N) such that K(z, z(cid:48)) = (cid:104)ϕ(z), ϕ(z(cid:48))(cid:105). We use zi to represent the i-th
coordinate of an inﬁnite-dimensional vector z. The (k1, . . . , kj)-th coordinate of ϕ(z), where j ∈ N
and k1, . . . , kj ∈ [d1], is deﬁned as 2− j+1

2 xk1 . . . xkj . By this deﬁnition, we have

(cid:104)ϕ(x), ϕ(y)(cid:105) =

2−(j+1) (cid:88)

zk1 . . . zkj z(cid:48)

k1 . . . z(cid:48)
kj

.

(k1,...,kj )∈[d1]j

Since (cid:107)z(cid:107)2 ≤ 1 and (cid:107)z(cid:48)(cid:107)2 ≤ 1, the series on the right-hand side is absolutely convergent. The inner
term on the right-hand side of equation (21) can be simpliﬁed to

(21)

(22)

(cid:88)

(k1,...,kj )∈[d1]j

zk1 . . . zkj z(cid:48)

k1 . . . z(cid:48)
kj

= ((cid:104)z, z(cid:48)(cid:105))j.

Combining equations (21) and (22) and using the fact that |(cid:104)z, z(cid:48)(cid:105)| ≤ 1, we have

(cid:104)ϕ(z), ϕ(z(cid:48))(cid:105) =

2−(j+1)((cid:104)z, z(cid:48)(cid:105))j (i)
=

1
2 − (cid:104)z, z(cid:48)(cid:105)

= K(z, z(cid:48)),

∞
(cid:88)

j=0

∞
(cid:88)

j=0

which veriﬁes that K is a kernel function and ϕ is the associated feature map. Next, we prove that
the associated RKHS contains the class of nonlinear ﬁlters. The lemma was proved by Zhang et al.
[2]. We include the proof to make the paper self-contained.

Lemma 1. Assume that the function σ(x) has a polynomial expansion σ(t) = (cid:80)∞

j=0 ajtj. Let
j λ2j. If Cσ((cid:107)w(cid:107)2) < ∞, then the RKHS induced by the inverse polynomial

Cσ(λ) :=
kernel contains function h : z (cid:55)→ σ((cid:104)w, z(cid:105)) with Hilbert norm (cid:107)h(cid:107)H = Cσ((cid:107)w(cid:107)2).

j=0 2j+1a2

(cid:113)(cid:80)∞

1

Proof. Let ϕ be the feature map that we have deﬁned for the polynomial inverse kernel. We deﬁne
vector w ∈ (cid:96)2(N) as follow: the (k1, . . . , kj)-th coordinate of w, where j ∈ N and k1, . . . , kj ∈ [d1],
is equal to 2

2 ajwk1 . . . wkj . By this deﬁnition, we have

j+1

σ((cid:104)w, z(cid:105)) =

aj((cid:104)w, z(cid:105))j =

wk1 . . . wkj zk1 . . . zkj = (cid:104)w, ϕ(z)(cid:105),

(23)

∞
(cid:88)

t=0

∞
(cid:88)

j=0

aj

(cid:88)

(k1,...,kj )∈[d1]j

where the ﬁrst equation holds since σ(x) has a polynomial expansion σ(x) = (cid:80)∞
j=0 ajxj, the second
by expanding the inner product, and the third by deﬁnition of w and ϕ(z). The (cid:96)2-norm of w is
equal to:

(cid:107)w(cid:107)2

2 =

2j+1a2
j

(cid:88)

∞
(cid:88)

j=0

(k1,...,kj )∈[d1]j

∞
(cid:88)

j=0

w2

k1w2

k2 · · · w2
kj

=

2j+1a2

j (cid:107)w(cid:107)2j

2 = C2

σ((cid:107)w(cid:107)2) < ∞.

(24)

By the basic property of the RKHS, the Hilbert norm of h is equal to the (cid:96)2-norm of w. Combining
equations (23) and (24), we conclude that h ∈ H and (cid:107)h(cid:107)H = (cid:107)w(cid:107)2 = Cσ((cid:107)w(cid:107)2).

According to Lemma 1, it suﬃces to upper bound Cσ(λ) for a particular activation function
σ. To make Cσ(λ) < ∞, the coeﬃcients {aj}∞
j=0 must quickly converge to zero, meaning that the
activation function must be suﬃciently smooth. For polynomial functions of degree (cid:96), the deﬁnition
of Cσ implies that Cσ(λ) = O(λ(cid:96)). For the sinusoid activation σ(t) := sin(t), we have

Cσ(λ) =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

∞
(cid:88)

j=0

22j+2

((2j + 1)!)2 · (λ2)2j+1 ≤ 2eλ2

.

For the erf function and the smoothed hinge loss function deﬁned in Section 3.4, Zhang et al. [2,
Proposition 1] proved that Cσ(λ) = O(ecλ2) for universal numerical constant c > 0.

A.2 Gaussian kernel

The Gaussian kernel also induces an RKHS that contains a particular class of nonlinear ﬁlters. The
proof is similar to that of Lemma 1.
Lemma 2. Assume that the function σ(x) has a polynomial expansion σ(t) = (cid:80)∞

j=0 ajtj. Let
j λ2j. If Cσ((cid:107)w(cid:107)2) < ∞, then the RKHS induced by the Gaussian kernel

Cσ(λ) :=
contains the function h : z (cid:55)→ σ((cid:104)w, z(cid:105)) with Hilbert norm (cid:107)h(cid:107)H = Cσ((cid:107)w(cid:107)2).

(cid:113)(cid:80)∞
j=0

j!e2γ
(2γ)j a2

Proof. When (cid:107)z(cid:107)2 = (cid:107)z(cid:48)(cid:107)2 = 1, It is well-known [see, e.g. 1] the following mapping ϕ : Rd1 → (cid:96)2(N)
is a feature map for the Gaussian RBF kernel: the (k1, . . . , kj)-th coordinate of ϕ(z), where j ∈ N
and k1, . . . , kj ∈ [d1], is deﬁned as e−γ((2γ)j/j!)1/2xk1 . . . xkj . Similar to equation (23), we deﬁne a
vector w ∈ (cid:96)2(N) as follow: the (k1, . . . , kj)-th coordinate of w, where j ∈ N and k1, . . . , kj ∈ [d1],
is equal to eγ((2γ)j/j!)−1/2ajwk1 . . . wkj . By this deﬁnition, we have

σ((cid:104)w, z(cid:105)) =

aj((cid:104)w, z(cid:105))j =

wk1 . . . wkj zk1 . . . zkj = (cid:104)w, ϕ(z)(cid:105).

(25)

∞
(cid:88)

t=0

∞
(cid:88)

j=0

aj

(k1,...,kj )∈[d1]j

(cid:88)

2

The (cid:96)2-norm of w is equal to:

(cid:107)w(cid:107)2

2 =

∞
(cid:88)

j=0

j!e2γ
(2γ)j a2

j

(cid:88)

w2

k1w2

k2 · · · w2
kj

=

(k1,...,kj )∈[d1]j

∞
(cid:88)

j=0

j!e2γ
(2γ)j a2

j (cid:107)w(cid:107)2j

2 = C2

σ((cid:107)w(cid:107)2) < ∞.

(26)

Combining equations (23) and (24), we conclude that h ∈ H and (cid:107)h(cid:107)H = (cid:107)w(cid:107)2 = Cσ((cid:107)w(cid:107)2).

Comparing Lemma 1 and Lemma 2, we ﬁnd that the Gaussian kernel imposes a stronger con-
dition on the smoothness of the activation function. For polynomial functions of degree (cid:96), we still
have Cσ(λ) = O(λ(cid:96)). For the sinusoid activation σ(t) := sin(t), it can be veriﬁed that

Cσ(λ) =

(cid:118)
(cid:117)
(cid:117)
(cid:116)e2γ

∞
(cid:88)

j=0

1
(2j + 1)!

·

(cid:16) λ2
2γ

(cid:17)2j+1

≤ eλ2/(4γ)+γ.

However, the value of Cσ(λ) is inﬁnite when σ is the erf function or the smoothed hinge loss,
meaning that the Gaussian kernel’s RKHS doesn’t contain ﬁlters activated by these two functions.

B Convex relaxation for nonlinear activation

In this appendix, we provide a detailed derivation of the relaxation for nonlinear activation functions
that we previously sketched in Section 3.2. Recall that the ﬁlter output is σ((cid:104)wj, z(cid:105)). Appendix A
shows that given a suﬃciently smooth activation function σ, we can ﬁnd some kernel function
K : Rd1 × Rd1 → R and a feature map ϕ : Rd1 → (cid:96)2(N) satisfying K(z, z(cid:48)) ≡ (cid:104)ϕ(z), ϕ(z(cid:48))(cid:105), such that

σ((cid:104)wj, z(cid:105)) ≡ (cid:104)wj, ϕ(z)(cid:105).

(27)

Here wj ∈ (cid:96)2(N) is a countable-dimensional vector and ϕ := (ϕ1, ϕ2, . . . ) is a countable sequence
of functions. Moreover, the (cid:96)2-norm of wj is bounded as (cid:107)wj(cid:107)2 ≤ Cσ((cid:107)wj(cid:107)2) for a monotonically
increasing function Cσ that depends on the kernel (see Lemma 1 and Lemma 2). As a consequence,
we may use ϕ(z) as the vectorized representation of the patch z, and use wj as the linear trans-
formation weights, then the problem is reduced to training a CNN with the identity activation
function.

The ﬁlter is parametrized by an inﬁnite-dimensional vector wj. Our next step is to reduce the
original ERM problem to a ﬁnite-dimensional one. In order to minimize the empirical risk, one
only needs to concern the output on the training data, that is, the output of (cid:104)wj, ϕ(zp(xi))(cid:105) for
all (i, p) ∈ [n] × [P ]. Let T be the orthogonal projector onto the linear subspace spanned by the
vectors {ϕ(zp(xi)) : (i, p) ∈ [n] × [P ]}. Then we have

∀ (i, p) ∈ [n] × [P ] :

(cid:104)wj, ϕ(zp(xi))(cid:105) = (cid:104)wj, T ϕ(zp(xi))(cid:105) = (cid:104)T wj, ϕ(zp(xi))(cid:105).

The last equation follows since the orthogonal projector T is self-adjoint. Thus, for empirical risk
minimization, we can without loss of generality assume that wj belongs to the linear subspace
spanned by {ϕ(zp(xi)) : (i, p) ∈ [n] × [P ]} and reparametrize it by:

wj =

(cid:88)

βj,(i,p)ϕ(zp(xi)).

(i,p)∈[n]×[P ]

(28)

3

Let βj ∈ RnP be a vector whose whose (i, p)-th coordinate is βj,(i,p). In order to estimate wj, it
suﬃces to estimate the vector βj. By deﬁnition, the vector satisﬁes the relation β(cid:62)
j Kβj = (cid:107)wj(cid:107)2
2,
where K is the nP × nP kernel matrix deﬁned in Section 3.2. As a consequence, if we can ﬁnd a
matrix Q such that QQ(cid:62) = K, then we have the norm constraint

(cid:107)Q(cid:62)βj(cid:107)2 =

β(cid:62)
j Kβj = (cid:107)wj(cid:107)2 ≤ Cσ((cid:107)wj(cid:107)2) ≤ Cσ(B).

(cid:113)

Let v(z) ∈ RnP be a vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). Then by equa-
tions (27) and (28), the ﬁlter output can be written as

σ(cid:0)(cid:104)wj, z(cid:105)(cid:1) ≡ (cid:104)wj, ϕ(z)(cid:105) ≡ (cid:104)βj, v(z)(cid:105).

(29)

(30)

For any patch zp(xi) in the training data, the vector v(zp(xi)) belongs to the column space of the
kernel matrix K. Therefore, letting Q† represent the pseudo-inverse of matrix Q, we have

∀ (i, p) ∈ [n] × [P ] :

(cid:104)βj, v(zp(xi))(cid:105) = β(cid:62)

j QQ†v(zp(xi)) = (cid:104)(Q(cid:62))†Q(cid:62)βj, v(zp(xi))(cid:105).

It means that if we replace the vector βj on the right-hand side of equation (30) by the vector
(Q(cid:62))†Q(cid:62)βj, then it won’t change the empirical risk. Thus, for ERM we can parametrize the ﬁlters
by

hj(z) := (cid:104)(Q(cid:62))†Q(cid:62)βj, v(z)(cid:105) = (cid:104)Q†v(z), Q(cid:62)βj(cid:105).

(31)

Let Z(x) be an P × nP matrix whose p-th row is equal to Q†v(zp(x)). Similar to the steps in

equation (6), we have

fk(x) =

(cid:16)
k,jZ(x)Q(cid:62)βj = tr
α(cid:62)

Z(x)

(cid:17)(cid:17)

Q(cid:62)βjα(cid:62)
k,j

= tr(Z(x)Ak),

(cid:16) r
(cid:88)

j=1

r
(cid:88)

j=1

where Ak := (cid:80)r
k,j.
matrices, then this larger matrix satisﬁes the constraints:

j=1 Q(cid:62)βjα(cid:62)

If we let A := (A1, . . . , Ad2) denote the concatenation of these

Constraint (C1): max
j∈[r]

(cid:107)Q(cid:62)βj(cid:107)2 ≤ Cσ(B1) and

max
(k,j)∈[d2]×[r]

(cid:107)αk,j(cid:107)2 ≤ B2.

Constraint (C2): The matrix A has rank at most r.

We relax these two constraints to the nuclear norm constraint:

(cid:107)A(cid:107)∗ ≤ Cσ(B1)B2r

d2.

(cid:112)

(32)

By comparing constraints (8) and (32), we see that the only diﬀerence is that the term B1 in the
norm bound has been replaced by Cσ(B1). This change is needed because we have used the kernel
trick to handle nonlinear activation functions.

References

[1] I. Steinwart and A. Christmann. Support vector machines. Springer, New York, 2008.

[2] Y. Zhang, J. D. Lee, and M. I. Jordan. (cid:96)1-regularized neural networks are improperly learnable
in polynomial time. In Proceedings on the 33rd International Conference on Machine Learning,
2016.

4

