Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

A. Additional Experiment Details

C.1. Multi-task baselines

In this section, we provide additional details of the experi-
mental set-up and hyperparameters.

A.1. Classiﬁcation

For N-way, K-shot classiﬁcation, each gradient is com-
puted using a batch size of N K examples. For Omniglot,
the 5-way convolutional and non-convolutional MAML
models were each trained with 1 gradient step with step size
α = 0.4 and a meta batch-size of 32 tasks. The network
was evaluated using 3 gradient steps with the same step
size α = 0.4. The 20-way convolutional MAML model
was trained and evaluated with 5 gradient steps with step
size α = 0.1. During training, the meta batch-size was set
to 16 tasks. For MiniImagenet, both models were trained
using 5 gradient steps of size α = 0.01, and evaluated using
10 gradient steps at test time. Following Ravi & Larochelle
(2017), 15 examples per class were used for evaluating the
post-update meta-gradient. We used a meta batch-size of
4 and 2 tasks for 1-shot and 5-shot training respectively.
All models were trained for 60000 iterations on a single
NVIDIA Pascal Titan X GPU.

A.2. Reinforcement Learning

In all reinforcement learning experiments, the MAML pol-
icy was trained using a single gradient step with α = 0.1.
During evaluation, we found that halving the learning rate
after the ﬁrst gradient step produced superior performance.
Thus, the step size during adaptation was set to α = 0.1
for the ﬁrst step, and α = 0.05 for all future steps. The
step sizes for the baseline methods were manually tuned for
each domain. In the 2D navigation, we used a meta batch
size of 20; in the locomotion problems, we used a meta
batch size of 40 tasks. The MAML models were trained
for up to 500 meta-iterations, and the model with the best
average return during training was used for evaluation. For
the ant goal velocity task, we added a positive reward bonus
at each timestep to prevent the ant from ending the episode.

B. Additional Sinusoid Results

In Figure 6, we show the full quantitative results of the
MAML model trained on 10-shot learning and evaluated
on 5-shot, 10-shot, and 20-shot. In Figure 7, we show the
qualitative performance of MAML and the pretrained base-
line on randomly sampled sinusoids.

C. Additional Comparisons

In this section, we include more thorough evaluations of
our approach, including additional multi-task baselines and
a comparison representative of the approach of Rei (2015).

The pretraining baseline in the main text trained a single
network on all tasks, which we referred to as “pretraining
on all tasks”. To evaluate the model, as with MAML, we
ﬁne-tuned this model on each test task using K examples.
In the domains that we study, different tasks involve dif-
ferent output values for the same input. As a result, by
pre-training on all tasks, the model would learn to output
the average output for a particular input value. In some in-
stances, this model may learn very little about the actual
domain, and instead learn about the range of the output
space.

We experimented with a multi-task method to provide a
point of comparison, where instead of averaging in the out-
put space, we averaged in the parameter space. To achieve
averaging in parameter space, we sequentially trained 500
separate models on 500 tasks drawn from p(T ). Each
model was initialized randomly and trained on a large
amount of data from its assigned task. We then took the
average parameter vector across models and ﬁne-tuned on
5 datapoints with a tuned step size. All of our experiments
for this method were on the sinusoid task because of com-
putational requirements. The error of the individual regres-
sors was low: less than 0.02 on their respective sine waves.

We tried three variants of this set-up. During training of
the individual regressors, we tried using one of the fol-
lowing: no regularization, standard (cid:96)2 weight decay, and
(cid:96)2 weight regularization to the mean parameter vector thus
far of the trained regressors. The latter two variants en-
courage the individual models to ﬁnd parsimonious solu-
tions. When using regularization, we set the magnitude of
the regularization to be as high as possible without signif-
icantly deterring performance. In our results, we refer to
this approach as “multi-task”. As seen in the results in Ta-
ble 2, we ﬁnd averaging in the parameter space (multi-task)
performed worse than averaging in the output space (pre-
training on all tasks). This suggests that it is difﬁcult to
ﬁnd parsimonious solutions to multiple tasks when training
on tasks separately, and that MAML is learning a solution
that is more sophisticated than the mean optimal parameter
vector.

C.2. Context vector adaptation

Rei (2015) developed a method which learns a context vec-
tor that can be adapted online, with an application to re-
current language models. The parameters in this context
vector are learned and adapted in the same way as the pa-
rameters in the MAML model. To provide a comparison
to using such a context vector for meta-learning problems,
we concatenated a set of free parameters z to the input x,
and only allowed the gradient steps to modify z, rather than
modifying the model parameters θ, as in MAML. For im-

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

Figure 6. Quantitative sinusoid regression results showing test-time learning curves with varying numbers of K test-time samples. Each
gradient step is computed using the same K examples. Note that MAML continues to improve with additional gradient steps without
overﬁtting to the extremely small dataset during meta-testing, and achieves a loss that is substantially lower than the baseline ﬁne-tuning
approach.

Table 2. Additional multi-task baselines on the sinusoid regres-
sion domain, showing 5-shot mean squared error. The results sug-
gest that MAML is learning a solution more sophisticated than the
mean optimal parameter vector.

num. grad steps
multi-task, no reg
multi-task, l2 reg
multi-task, reg to mean θ
pretrain on all tasks
MAML (ours)

1
4.19
7.18
2.91
2.41
0.67

5
3.85
5.69
2.72
2.23
0.38

10
3.69
5.60
2.71
2.19
0.35

Table 3. 5-way Omniglot Classiﬁcation
1-shot

5-shot

context vector
MAML

94.9 ± 0.9% 97.7 ± 0.3%
98.7 ± 0.4% 99.9 ± 0.1%

age inputs, z was concatenated channel-wise with the input

image. We ran this method on Omniglot and two RL do-
mains following the same experimental protocol. We report
the results in Tables 3, 4, and 5. Learning an adaptable con-
text vector performed well on the toy pointmass problem,
but sub-par on more difﬁcult problems, likely due to a less
ﬂexible meta-optimization.

Table 4. 2D Pointmass, average return

num. grad steps
context vector −42.42 −13.90 −5.17 −3.18
MAML (ours) −40.41 −11.68 −3.33 −3.23

3

1

2

0

Table 5. Half-cheetah forward/backward, average return

num. grad steps
context vector −40.49 −44.08 −38.27 −42.50
315.65
MAML (ours) −50.69

293.19

313.48

3

2

1

0

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks

Figure 7. A random sample of qualitative results from the sinusoid regression task.

