Constrained Policy Optimization

10. Appendix

10.1. Proof of Policy Performance Bound

10.1.1. PRELIMINARIES

Our analysis will make extensive use of the discounted future state distribution, dπ, which is deﬁned as

It allows us to express the expected discounted total reward compactly as

dπ(s) = (1

γ)

−

∞

t=0
�

γtP (st = s

π).

|

J(π) =

[R(s, a, s�)] ,

(17)

1

−

1

γ

E
dπ
s
∼
π
a
∼
s�
P
∼

where by a
of reducing clutter, but it should be clear from context that a and s� depend on s.

P , we mean s�

π, we mean a

s), and by s�

P (

π(

∼

∼

∼

∼

·|

·|

s, a). We drop the explicit notation for the sake

First, we examine some useful properties of dπ that become apparent in vector form for ﬁnite state spaces. Let pt
denote the vector with components pt
components Pπ(s�

π(s) = P (st = s
s); then pt

|
| denote the transition matrix with

π), and let Pπ ∈
π = P t
πµ and
−

|
π = Pπpt

s, a)π(a

daP (s�

s) =

π ∈

R|

R|

|×|

S

S

S

1

|

�

|

|

dπ = (1

γ)

∞

(γPπ)tµ

−

−

t=0
�
−

= (1

γ)(I

γPπ)−

1µ.

This formulation helps us easily obtain the following lemma.
Lemma 1. For any function f : S

R and any policy π,

→
(1

γ) E
µ
s
∼

−

[f (s)] + E
dπ
s
∼
π
a
∼
s�
P
∼

[γf (s�)]

[f (s)] = 0.

−

s

E
dπ
∼

Proof. Multiply both sides of (18) by (I

γPπ) and take the inner product with the vector f

−

S

|.

R|

∈

Combining this with (17), we obtain the following, for any function f and any policy π:

J(π) = E
µ
∼

s

[f (s)] +

[R(s, a, s�) + γf (s�)

f (s)] .

−

1

−

1

γ

E
dπ
s
∼
π
a
∼
s�
P
∼

This identity is nice for two reasons. First: if we pick f to be an approximator of the value function V π, then (20) relates
µ[f (s)]) and to the on-policy average
the true discounted return of the policy (J(π)) to the estimate of the policy return (Es
f (s) has
TD-error of the approximator; this is aesthetically satisfying. Second: it shows that reward-shaping by γf (s�)
µ[f (s)], a ﬁxed constant independent of policy; this illustrates
the effect of translating the total discounted return by Es
the ﬁnding of Ng. et al. (1999) that reward shaping by γf (s�) + f (s) does not change the optimal policy.

−

∼

∼

It is also helpful to introduce an identity for the vector difference of the discounted future state visitation distributions on
two different policies, π� and π. Deﬁne the matrices G

Pπ. Then:

γPπ)−

.
= (I

.
= (I

1, ¯G

γPπ� )−

1, and Δ = Pπ� −

−

1

G−

¯G−

1 = (I

−

−
= γΔ;

γPπ)

(I

−

−

−
γPπ� )

left-multiplying by G and right-multiplying by ¯G, we obtain

¯G

G = γ ¯GΔG.

−

(18)

(19)

(20)

Thus

Constrained Policy Optimization

dπ�

−

¯G
dπ = (1
G
γ)
−
−
γ) ¯GΔGµ
= γ(1
�
�
= γ ¯GΔdπ.

−

µ

(21)

(22)

(23)

(24)

For simplicity in what follows, we will only consider MDPs with ﬁnite state and action spaces, although our attention is
on MDPs that are too large for tabular methods.

10.1.2. MAIN RESULTS

In this section, we will derive and present the new policy improvement bound. We will begin with a lemma:
Lemma 2. For any function f : S

R and any policies π� and π, deﬁne

and �π�
f

.
= maxs |

Ea

π�,s�∼

∼

Lπ,f (π�)

1

(R(s, a, s�) + γf (s�)

f (s))

,

−

�

→
.
= E
dπ
s
∼
π
a
∼
s�
P
∼

π�(a
π(a

s)
|
s) −
|

��

�

P [R(s, a, s�) + γf (s�)

f (s)]

. Then the following bounds hold:

J(π�)

J(π)

Lπ,f (π�)

2�π�

f DT V (dπ�

dπ)

−

J(π�)

J(π)

Lπ,f (π�) + 2�π�

f DT V (dπ�

dπ)

−

−

||

||

,

,

�

�

|

−
1

−
1

−

γ

γ

�

�

≥

1

≤

1

where DT V is the total variational divergence. Furthermore, the bounds are tight (when π� = π, the LHS and RHS are
identically zero).

Proof. First, for notational convenience, let δf (s, a, s�)
quantity is intentionally suggestive—this bears a strong resemblance to a TD-error.) By (20), we obtain the identity

f (s). (The choice of δ to denote this

−

.
= R(s, a, s�) + γf (s�)

J(π�)

J(π) =

−

[δf (s, a, s�)]

1

−

1

γ

E
dπ�
π�
P

s
∼
a
∼
s�
∼








−

E
dπ
s
∼
π
a
∼
s�
P
∼

[δf (s, a, s�)] .





Now, we restrict our attention to the ﬁrst term in this equation. Let ¯δπ�
Ea

P [δf (s, a, s�)

f ∈

s]. Observe that
|

π�,s�∼

∼

S

| denote the vector of components ¯δπ�

f (s) =

R|

[δf (s, a, s�)] =

dπ� , ¯δπ�
f

�

E
dπ�
π�
P

s
∼
a
∼
s�
∼

�

�

=

dπ, ¯δπ�
f

+

dπ�

dπ, ¯δπ�
f

−

�

�

�

This term is then straightforwardly bounded by applying H¨older’s inequality; for any p, q
we have

[1,

] such that 1/p+1/q = 1,

dπ, ¯δπ�
f

+

dπ�

dπ

¯δπ�
f

−

�

�

�
�
�

p

�
�
�

�
�
�

q ≥

�
�
�

E
dπ�
π�
P

s
∼
a
∼
s�
∼

[δf (s, a, s�)]

dπ, ¯δπ�
f

≥

�

−

�

�
�
�

The lower bound leads to (23), and the upper bound leads to (24).

∈
dπ�

∞
dπ

−

¯δπ�
f

.

q

p

�
�
�

�
�
�

�
�
�

We choose p = 1 and q =
the inner product

dπ�

∞
dπ, ¯δπ�
f

−

�

�

; however, we believe that this step is very interesting, and different choices for dealing with

may lead to novel and useful bounds.

dπ�

With
1
by the importance sampling identity,

= 2DT V (dπ�

dπ

−

||

dπ) and

�
�
�

�
�
�

�
�
�

∞

�
�
�

Constrained Policy Optimization

¯δπ�
f

= �π�

f , the bounds are almost obtained. The last step is to observe that,

dπ, ¯δπ�
f

�

�

[δf (s, a, s�)]

=

=

E
dπ
s
∼
π�
a
∼
P
s�
∼

E
dπ
s
∼
π
a
∼
s�
P
∼

π�(a
π(a

s)
|
s)
|

�

��

δf (s, a, s�)

.

�

After grouping terms, the bounds are obtained.

This lemma makes use of many ideas that have been explored before; for the special case of f = V π, this strategy (after
bounding DT V (dπ�
dπ)) leads directly to some of the policy improvement bounds previously obtained by Pirotta et al.
and Schulman et al. The form given here is slightly more general, however, because it allows for freedom in choosing f .

||

P [δV π� (s, a, s�)

Remark. It is reasonable to ask if there is a choice of f which maximizes the lower bound here. This turns out to trivially
π� [Aπ� (s, a)] = 0 (by the deﬁnition
be f = V π� . Observe that Es�∼
of Aπ� ), thus ¯δπ�
; from (20) with f = V π� , we can
= 0 and �π�
V π�
V π�
J(π). Thus, for f = V π� , we recover an exact equality. While this is not practically
see that this exactly equals J(π�)
useful to us (because, when we want to optimize a lower bound with respect to π�, it is too expensive to evaluate V π� for
each candidate to be practical), it provides insight: the penalty coefﬁcient on the divergence captures information about the
mismatch between f and V π� .

s, a] = Aπ� (s, a). For all states, Ea
|
∼
Aπ� (s, a)
= 0. Also, Lπ,V π� (π�) =

dπ,a

Es

−

−

�

�

∼

∼

π

Next, we are interested in bounding the divergence term,
knowledge, this is a new result.

dπ�
�

−

dπ

�1. We give the following lemma; to the best of our

Lemma 3. The divergence between discounted future state visitation distributions,
divergence of the policies π� and π:

�

dπ�

dπ

−

�1, is bounded by an average

dπ�

�

−

dπ

�1 ≤

1

2γ

−

γ

s

E
dπ
∼

[DT V (π�

π)[s]] ,

||

(25)

where DT V (π�

π)[s] = (1/2)

||

π�(a

s)

|

π(a

s)

.

|

|

−

a |

�

Proof. First, using (21), we obtain

¯G

�

�1 is bounded by:

dπ�

�

−

dπ

�1 = γ
γ
≤

¯GΔdπ
¯G

�1�

�

�

�1
Δdπ

�1.

¯G

�

�1 =

�

(I

−

γPπ� )−

1

�1 ≤

γt

Pπ� �

�

t
1 = (1

−

1

γ)−

∞

t=0
�

To conclude the lemma, we bound

Δdπ

�1.

�

Δdπ

�

�1 =

Constrained Policy Optimization

Δ(s�

s)dπ(s)

s)

dπ(s)

|

|

�
�
�
�
�

|

|

�
s
�
�
�
Δ(s�
�
�
|

�s�

�s,s�

a
�
P (s�

�s,s�

�
�
�
�
�
�s,a,s�

|

|

≤

=

≤

=

π�(a

s)

|

π(a

s)

|

|

−

dπ(s)

s,a
�
= 2 E
∼

s

dπ

[DT V (π�

π)[s]] .

||

P (s�

s, a) (π�(a

s)

π(a

s))

dπ(s)

|

−

s, a)

π�(a

s)

|

|

π(a

s)

|

|

−

|

�
�
�
dπ(s)
�
�

The new policy improvement bound follows immediately.

Theorem 1. For any function f : S

R and any policies π� and π, deﬁne δf (s, a, s�)

→

.
= R(s, a, s�) + γf (s�)

f (s),

−

�π�
f

.
= max

s

Ea

|

π�,s�∼

∼

P [δf (s, a, s�)]

,

|

Lπ,f (π�)

1

δf (s, a, s�)

, and

π�(a
π(a

s)
|
s) −
|

��

�

.
= E
dπ
s
∼
π
a
∼
s�
P
∼

D±π,f (π�)

.
=

Lπ,f (π�)

γ ±

(1

2γ�π�
f

γ)2 E

s

dπ

∼

−

1

−
π(a

[DT V (π�

π)[s]] ,

�

D+

π,f (π�)

J(π�)

J(π)

−

≥

≥

D−π,f (π�).

�

||

||

where DT V (π�
The following bounds hold:

π)[s] = (1/2)

||

π�(a

s)

|

−

a |

|

|

s)

is the total variational divergence between action distributions at s.

Furthermore, the bounds are tight (when π� = π, all three expressions are identically zero).

Proof. Begin with the bounds from lemma 2 and bound the divergence DT V (dπ�

dπ) by lemma 3.

10.2. Proof of Analytical Solution to LQCLP

Theorem 2 (Optimizing Linear Objective with Linear and Quadratic Constraints). Consider the problem

(4)

(26)

where g, b, x
point x∗ satisﬁes

∈

Rn, c, δ

R, δ > 0, H

Sn, and H

∈

∈

0. When there is at least one strictly feasible point, the optimal

p∗ = min

gT x

x
s.t. bT x + c
xT Hx

0

≤
δ,

≤

�

1
λ∗

−

x∗ =

H −

1 (g + ν∗b) ,

where λ∗ and ν∗ are deﬁned by

Constrained Policy Optimization

ν∗ =

r

λ∗c
−
s

,

�+

�

λ∗ = arg max

fa(λ)
fb(λ)

.
= 1
2λ
.
1
=
2

λ

0 �

≥

−
1b, and s = bT H −

1b.

�

�

r2
q
s −
q
λ + λδ
�

+ λ
2

c2
s −

δ

�

rc
s

−

�

r > 0

if λc
−
otherwise,

with q = gT H −

1g, r = gT H −

Furthermore, let Λa

.
=

�

.
=

λ

λc

{

|

−

r > 0, λ

0
}

≥

, and Λb

λc

λ
|

{

−

r

≤

≥

}

0, λ

0

. The value of λ∗ satisﬁes

.
= Proj

λ∗a

λ∗

∈ �

r2/s
c2/s

q
δ

−
−

��

, Λa

, λ∗b

�

.
= Proj

q
δ

, Λb

,

��

��

with λ∗ = λ∗a if fa(λ∗a) > fb(λ∗b ) and λ∗ = λ∗b otherwise, and Proj(a, S) is the projection of a point x on to a set S. Note:
the projection of a point x

R onto a convex segment of R, [a, b], has value Proj(x, [a, b]) = max(a, min(b, x)).

∈

Proof. This is a convex optimization problem. When there is at least one strictly feasible point, strong duality holds by
Slater’s theorem. We exploit strong duality to solve the problem analytically.

p∗ = min

x

gT x +

xT Hx

δ

+ ν

bT x + c

λ
2

�

−

�

xT Hx + (g + νb)T x +

νc

Strong duality

1 (g + νb)

H −

1
λ
−
(g + νb)T H −

1 (g + νb) +

νc

Plug in x∗

(x, λ, ν) = 0

∇xL

�

�

�

λδ

1
2

−

�

1
2

λδ

−

�

�
1
2

λδ

−

�

max
0
λ
≥
0
ν
≥

min
x

λ
2

= max
0
λ
0
ν

≥
≥

=

⇒

x∗ =

1
2λ

1
2λ

−

−

∂
L
∂ν

ν =

= max
0
λ
0
ν

≥
≥

= max
0
λ
0
ν

≥
≥

=

⇒

=

⇒

= max
λ

0 �

≥

q + 2νr + ν2s

+

νc

�

�

(2r + 2νs) + c

�

=

−
λc

1
2λ

−
s

r

�
1
2λ
1
�
2

−

�+
r2
q
s −
q
λ + λδ
�

+ λ
2

c2
s −

δ

�

rc
s

−

�

if λ
if λ

Λa
Λb

∈

∈

Notation:

.
=
.
=

Λa
Λb

λc
λ
|
{
λc
λ
|
{

−
−

r > 0, λ
0, λ
r

≤

,

0
0

}
}

≥
≥

�
Observe that when c < 0, Λa = [0, r/c) and Λb = [r/c,

∞
Notes on interpreting the coefﬁcients in the dual problem:

�

); when c > 0, Λa = [r/c,

) and Λb = [0, r/c).

∞

Notation: q

.
= gT H −

1g, r

.
= gT H −

1b, s

.
= bT H −

1b.

Optimizing single-variable convex quadratic function over R+

•

We are guaranteed to have r2/s
s = bT H −

1b. The Cauchy-Scwarz inequality gives:

−

≤

q

0 by the Cauchy-Schwarz inequality. Recall that q = gT H −

1g, r = gT H −

1b,

H −

1/2b

H −

1/2g

�

2
2 ≥
�
1g

≥

gT H −

2
2�

�
1b

� �

=

⇒
∴ qs
�

bT H −
r2.

≥

�

�

�

T

H −

1/2b

H −

1/2g

��
bT H −

1g

�
2

�

2

��

Constrained Policy Optimization

•

The coefﬁcient c2/s
An intersection occurs if there exists an x such that c + bT x = 0 with xT Hx
we solve

δ relates to whether or not the plane of the linear constraint intersects the quadratic trust region.
δ. To check whether this is the case,

−

x∗ = arg min

xT Hx :

x

≤
c + bT x = 0

(27)

and see if x∗
c2/s

δ

T Hx∗
0, then the plane intersects the trust region; otherwise, it does not.

δ. The solution to this optimization problem is x∗ = cH −

≤

1b/s, thus x∗

T Hx∗ = c2/s. If

−

≤

−

If c2/s
δ > 0 and c < 0, then the quadratic trust region lies entirely within the linear constraint-satisfying halfspace, and
we can remove the linear constraint without changing the optimization problem. If c2/s
δ > 0 and c > 0, the problem
is infeasible (the intersection of the quadratic trust region and linear constraint-satisfying halfspace is empty). Otherwise,
we follow the procedure below.

−

Solving the dual for λ: for any A > 0, B > 0, the problem

f (λ)

max
0
λ
≥

.
=

1
2

−

A
λ

�
√AB.

+ Bλ

�

has optimal point λ∗ =

A/B and optimal value f (λ∗) =

−
We can use this solution form to obtain the optimal point on each segment of the piecewise continuous dual function for λ:

�

optimal point (before projection)

optimal point (after projection)

objective

fa(λ)

fb(λ)

.
=

1
2λ

.
=

1
2

−

r2
s −

q

+

�

c2
s −

δ

λ
2

�

−

�

rc
s

�

�

q
λ

+ λδ

�

r2/s
c2/s

−
−

.
=

λa

.
=

λb

q
δ

q
δ

�

�

The optimization is completed by comparing fa(λ∗a) and fb(λ∗b ):

λ∗ =

λ∗a
λ∗b

�

fa(λ∗a)
≥
otherwise.

fb(λ∗b )

10.3. Experimental Parameters

10.3.1. ENVIRONMENTS

In the Circle environments, the reward and cost functions are

λ∗a = Proj(λa, Λa)

λ∗b = Proj(λb, Λb)

−
[x, y]
|�
> xlim] ,
|
where x, y are the coordinates in the plane, v is the velocity, and d, xlim are environmental parameters. We set these
parameters to be

C(s) = 111 [

1 +
x

�2 −

R(s) =

d

|

|

,

vT [

y, x]

Point-mass Ant Humanoid
10
3

15
2.5

10
2.5

d
xlim

In Point-Gather, the agent receives a reward of +10 for collecting an apple, and a cost of 1 for collecting a bomb. Two
apples and eight bombs spawn on the map at the start of each episode. In Ant-Gather, the reward and cost structure was
10 for falling over (which results in the episode ending). Eight
the same, except that the agent also receives a reward of
apples and eight bombs spawn on the map at the start of each episode.

−

Constrained Policy Optimization

Figure 5. In the Circle task, reward is maximized by moving along the green circle. The agent is not allowed to enter the blue regions,
so its optimal constrained path follows the line segments AD and BC.

10.3.2. ALGORITHM PARAMETERS

In all experiments, we use Gaussian policies with mean vectors given as the outputs of neural networks, and with variances
that are separate learnable parameters. The policy networks for all experiments have two hidden layers of sizes (64, 32)
with tanh activation functions.

We use GAE-λ (Schulman et al., 2016) to estimate the advantages and constraint advantages, with neural network value
functions. The value functions have the same architecture and activation functions as the policy networks. We found that
having different λGAE values for the regular advantages and the constraint advantages worked best. We denote the λGAE
used for the constraint advantages as λGAE

.

C

For the failure prediction networks Pφ(s
U ), we use neural networks with a single hidden layer of size (32), with output
of one sigmoid unit. At each iteration, the failure prediction network is updated by some number of gradient descent steps
using the Adam update rule to minimize the prediction error. To reiterate, the failure prediction network is a model for the
probability that the agent will, at some point in the next T time steps, enter an unsafe state. The cost bonus was weighted
by a coefﬁcient α, which was 1 in all experiments except for Ant-Gather, where it was 0.01. Because of the short time
horizon, no cost bonus was used for Point-Gather.

→

For all experiments, we used a discount factor of γ = 0.995, a GAE-λ for estimating the regular advantages of λGAE =
0.95, and a KL-divergence step size of δKL = 0.01.

Experiment-speciﬁc parameters are as follows:

Parameter
Batch size
Rollout length
Maximum constraint value d
Failure prediction horizon T
Failure predictor SGD steps per itr
Predictor coeff α
λGAE
C

Point-Circle Ant-Circle Humanoid-Circle

Point-Gather Ant-Gather

50,000
50-65
5
5
25
1
1

100,000
500
10
20
25
1
0.5

50,000
1000
10
20
25
1
0.5

50,000
15
0.1
(N/A)
(N/A)
(N/A)
1

100,000
500
0.2
20
10
0.01
0.5

Note that these same parameters were used for all algorithms.

We found that the Point environment was agnostic to λGAE
essary to set λGAE
constraint gradient magnitude, which led the algorithm to take unsafe steps. The choice λGAE
hyperparameter search in

, but for the higher-dimensional environments, it was nec-
to a value < 1. Failing to discount the constraint advantages led to substantial overestimates of the
= 0.5 was obtained by a

, but 0.92 worked nearly as well.

0.5, 0.92, 1

C

C

C

{

}

10.3.3. PRIMAL-DUAL OPTIMIZATION IMPLEMENTATION

Our primal-dual implementation is intended to be as close as possible to our CPO implementation. The key difference
is that the dual variables for the constraints are stateful, learnable parameters, unlike in CPO where they are solved from
scratch at each update.

The update equations for our PDO implementation are

Constrained Policy Optimization

θk+1 = θk + sj

(g

�

−

1(g

νkb)

−

H −

1 (g

νkb)

−

νk+1 = (νk + α (JC(πk)

2δ
νkb)T H −

d))+ ,

−

where sj is from the backtracking line search (s
, where J is the backtrack budget; this is
}
the same line search as is used in CPO and TRPO), and α is a learning rate for the dual parameters. α is an important
hyperparameter of the algorithm: if it is set to be too small, the dual variable won’t update quickly enough to meaningfully
enforce the constraint; if it is too high, the algorithm will overcorrect in response to constraint violations and behave too
conservatively. We experimented with a relaxed learning rate, α = 0.001, and an aggressive learning rate, α = 0.01. The
aggressive learning rate performed better in our experiments, so all of our reported results are for α = 0.01.

(0, 1) and j

0, 1, ..., J

∈ {

∈

Selecting the correct learning rate can be challenging; the need to do this is obviated by CPO.

