Dropout Inference in Bayesian Neural Networks with Alpha-divergences

A. Code Example

The following is a code snippet showing how our inference can be implemented with a few lines of Keras code (Chollet,
2015). We deﬁne a new loss function bbalpha softmax cross entropy with mc logits, that takes MC sam-
pled logits as an input. This is demonstrated for the case of classiﬁcation. Regression can be implemented in a similar
way.

def bbalpha_softmax_cross_entropy_with_mc_logits(alpha):

def loss(y_true, mc_logits):

# mc_logits: output of GenerateMCSamples, of shape M x K x D
mc_log_softmax = mc_logits - K.max(mc_logits, axis=2, keepdims=True)
mc_log_softmax = mc_log_softmax - logsumexp(mc_log_softmax, 2)
mc_ll = K.sum(y_true * mc_log_softmax, -1)
return - 1. / alpha * (logsumexp(alpha * mc_ll, 1) + K.log(1.0 / K_mc))

# M x K

return loss

MC samples for this loss can be generated using GenerateMCSamples, with layers being a list of Keras initialised
layers:

def GenerateMCSamples(inp, layers, K_mc=20):

output_list = []
for _ in xrange(K_mc):

output_list += [apply_layers(inp, layers)]

def pack_out(output_list):

output = K.pack(output_list) # K_mc x nb_batch x nb_classes
return K.permute_dimensions(output, (1, 0, 2)) # nb_batch x K_mc x nb_classes

def pack_shape(s):

s = s[0]
return (s[0], K_mc, s[1])

out = Lambda(pack_out, output_shape=pack_shape)(output_list)
return out

The above two functions rely on the following auxiliary functions:

def logsumexp(x, axis=None):

x_max = K.max(x, axis=axis, keepdims=True)
return K.log(K.sum(K.exp(x - x_max), axis=axis, keepdims=True)) + x_max

def apply_layers(inp, layers):

output = inp
for layer in layers:

output = layer(output)

return output

B. Alpha-divergence minimisation

There are various available deﬁnitions of α-divergences, and in this work we mainly used two of them: Amari’s deﬁnition
(Amari, 1985) adapted to EP context (Minka, 2005), and R´enyi divergence (R´enyi, 1961) which is more used in information
theory research.

• Amari’s α-divergence (Amari, 1985):

• R´enyi’s α-divergence (R´enyi, 1961):

Dα[p||q] =

1
α(1 − α)

(cid:18)

(cid:90)

1 −

p(ω)αq(ω)1−αdω

.

(cid:19)

Rα[p||q] =

log

p(ω)αq(ω)1−αdω.

(cid:90)

1
α − 1

Dropout Inference in Bayesian Neural Networks with Alpha-divergences

α(1−α) (1 − exp [(α − 1)Rα[p||q]]). In power
These two divergence can be converted to each other, e.g. Dα[p||q] =
EP (Minka, 2004), this α-divergence is minimised using projection-based updates. When the approximate posterior q
has an exponential family form, minimising Dα[p||q] requires moment matching to the “tilted distribution” ˜pα(ω) ∝
p(ω)αq(ω)1−α. This projection update might be intractable for non-exponential family q distributions, and instead BB-α
deploys a gradient-based update to search a local minimum. We will present the original derivation of the BB-α energy
below and discuss how it relates to power EP.

1

C. Original Derivation of BB-α Energy

Here we include the original formulation of the BB-α energy for completeness. Consider approximating a distribution of
the following form

p(ω) =

p0(ω)

fn(ω),

1
Z

N
(cid:89)

n

in which the prior distribution p0(ω) has an exponential family form p0(ω) ∝ exp (cid:2)λT
0 φ(ω)(cid:3). Here λ0 is called natural
parameter or canonical parameter of the exponential family distribution, and φ(ω) is the sufﬁcient statistic. As the factors
fn might not be conjugate to the prior, the exact posterior no longer belongs to the same exponential family as the prior,
and hence need approximations. EP construct such approximation by ﬁrst approximating each complicated factor fn with
n φ(ω)(cid:3), then constructing the approximate distribution as
a simpler one ˜fn(ω) ∝ exp (cid:2)λT




q(ω) =

exp



λn

φ(ω)

 ,

1
Z(λq)

(cid:33)T

(cid:32) N
(cid:88)

n=0

with λq = λ0 + (cid:80)N
using the following procedure (for α (cid:54)= 0):

n=1 λn and Z(λq) the normalising constant/partition function. These local parameters are updated

1 compute cavity distribution q\n(ω) ∝ q(ω)/ ˜fn(ω), equivalently. λ\n ← λq − λn;

2 compute the tilted distribution by inserting the likelihood term ˜pn(ω) ∝ q\n(ω)fn(ω);

3 compute a projection update: λq ← arg minλ Dα[˜pn||qλ] with qλ an exponential family with natural parameter λ;
4 recover the site approximation by λn ← λq − λ\n and form the ﬁnal update λq ← (cid:80)

n λn + λ0.

When converged, the solutions of λn return a ﬁxed point of the so called power EP energy:

LPEP(λ0, {λn}) = log Z(λ0) + (

− 1) log Z(λq) −

fn(ω)α exp (cid:2)(λq − αλn)T φ(ω)(cid:3) dω.

(8)

N
α

(cid:90)

log

1
α

N
(cid:88)

n=1

But more importantly, before convergence all these local parameters λn are maintained in memory. This indicates that
power EP does not scale with big data: consider Gaussian approximations which has O(d2) parameters with d the dimen-
sionality of ω. Then the space complexity of power EP is O(N d2), which is clearly prohibitive for big models like neural
networks that are typically applied to large datasets. BB-α provides a simple solution of this memory overhead by sharing
the local parameters, i.e. deﬁning λn = λ for all n = 1, ..., N . Furthermore, under the mild condition that the exponential
family is regular, there exist a one-to-one mapping between λq and λ (given a ﬁxed λ0). Hence we arrive at a “global”
optimisation problem in the sense that only one parameter λq is optimised, where the objective function is the BB-α energy

Lα(λ0, λq) = log Z(λ0) − log Z(λq) −

(9)

1
α

N
(cid:88)

n=1

log Eq

(cid:20)(cid:18)

fn(ω)
exp [λT φ(ω)]

(cid:19)α(cid:21)

.

One could verify that this is equivalent to the BB-α energy function presented in the main text by considering exponential
family q distributions.

Although empirical evaluations have demonstrated the superior performance of BB-α, the original formulation is difﬁcult
to interpret for practitioners. First the local alpha-divergence minimisation interpretation is inherited from power EP, and

Dropout Inference in Bayesian Neural Networks with Alpha-divergences

the intuition of power EP itself might already pose challenges for practitioners. Second, the derivation of BB-α from
power EP is ad hoc and lacks theoretical justiﬁcation. It has been shown that power EP energy can be viewed as the dual
objective to a continuous version of Bethe free-energy, in which λn represents the Lagrange multiplier of the constraints
in the primal problem. Hence tying the Lagrange multipliers would effectively changes the primal problem, thus losing
a number of nice guarantees. Nevertheless this approximation has been shown to work well in real-world settings, which
motivated our work to extend BB-α to dropout approximation.

D. Full Regression Results

Dataset
boston
concrete
energy
kin8nm
power
protein
red wine
yacht
naval
year

1030
768
8192
9568
45730

α = 0.0
2.42±0.05
2.98±0.02
1.75±0.01

α = 0.5
2.38±0.06
2.88±0.02
0.74±0.02

α = 1.0
2.50±0.10
2.96±0.03
0.81±0.02

Table 1. Regression experiment: Average negative test log likelihood/nats
GP
2.22±0.07
2.85±0.02
1.29±0.01

VI-G
N D
2.52±0.03
506 13
3.11±0.02
8
0.77±0.02
8
8 -0.83±0.00 -1.03±0.00 -1.10±0.00 -1.35±0.00 -1.31±0.01 -1.12±0.01
2.82±0.01
2.76±0.00
4
2.91±0.00
2.86±0.00
9
0.96±0.01
0.95±0.02
1588 11
1.77±0.01
1.15±0.06
6
11934 16 -2.80±0.00 -2.80±0.00 -2.80±0.00 -7.31±0.00 -4.86±0.04 -6.49±0.29
NA±NA 0.65±NA 3.60±NA
515345 90

HMC
2.27±0.03
2.72±0.02
0.93±0.01

2.66±0.01
2.95±0.05
0.67±0.01
1.15±0.03

2.70±0.00
2.77±0.00
0.91±0.02
1.62±0.01

2.78±0.01
2.87±0.00
0.92±0.01
1.08±0.04

2.79±0.01
2.87±0.00
0.92±0.01
1.38±0.01

3.59±NA 3.54±NA -3.59±NA

308

Table 2. Regression experiment: Average test RMSE

Dataset
boston
concrete
energy
kin8nm
power
protein
red wine
yacht
naval
year

GP

VI-G

N D

HMC

α = 0.5

α = 0.0

α = 1.0
506 13 2.85±0.19 2.97±0.19 3.04±0.17 2.76±0.20 2.43±0.07 2.89±0.17
8 4.92±0.13 4.62±0.12 4.76±0.15 4.12±0.14 5.55±0.02 5.42±0.11
1030
8 1.02±0.03 1.11±0.02 1.10±0.02 0.48±0.01 1.02±0.02 0.51±0.01
768
8 0.09±0.00 0.09±0.00 0.08±0.00 0.06±0.00 0.07±0.00 0.08±0.00
8192
4 4.04±0.04 4.01±0.04 3.98±0.04 3.73±0.04 3.75±0.03 4.07±0.04
9568
9 4.28±0.02 4.28±0.04 4.23±0.01 3.91±0.02 4.83±0.21 4.45±0.02
45730
1588 11 0.61±0.01 0.62±0.01 0.63±0.01 0.63±0.01 0.57±0.01 0.63±0.01
6 0.76±0.05 0.85±0.06 0.88±0.06 0.56±0.05 1.15±0.09 0.81±0.05
308
11934 16 0.01±0.00 0.01±0.00 0.01±0.00 0.00±0.00 0.00±0.00 0.00±0.00
515345 90 8.66±NA 8.80±NA 8.97±NA NA±NA 0.79±NA 8.88±NA

E. Run time trade-off

We provide an assessment of the running time trade-offs of using an increasing number of samples at training time. Unlike
VI, in our inference we rely on a large number of samples to reduce estimator bias. When a small number of samples is
used (K = 1) our method collapses to standard VI. In Figure 8 we see both test accuracy as well as test log likelihood
for a fully connected NN with four layers of 1024 units trained on the MNIST dataset, with α = 1. The two metrics are
shown as a function of wall-clock run time for different values of K ∈ {1, 10, 100}. As can be seen, K = 1 converges to
test accuracy of 98.8% faster than the other values of K, which converge to the same accuracy. On the other hand, when
assessing test log likelihood, both K = 1 and K = 10 attain value −600 within 1000 seconds, but K = 10 continues
improving its test log likelihood and converges to value −500 after 3000 seconds. K = 100 converges to the same value
but requires much longer running time, possibly because of noise from other processes.

Dropout Inference in Bayesian Neural Networks with Alpha-divergences

(a) Test accuracy

(b) Test log likelihood

Figure 8. Run time experiment on the MNIST dataset for different number of samples K.

102103104105106Seconds (log scale)0.950.960.970.980.99Test accuracy (MC dropout)K=1K=10K=100102103104105106Seconds (log scale)25002000150010005000Test llK=1K=10K=100