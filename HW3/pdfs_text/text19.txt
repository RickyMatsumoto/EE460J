Doubly Accelerated Methods for Faster CCA
and Generalized Eigendecomposition

Zeyuan Allen-Zhu * 1 Yuanzhi Li * 2

Abstract
We study k-GenEV, the problem of ﬁnding the
top k generalized eigenvectors, and k-CCA, the
problem of ﬁnding the top k vectors in canonical-
correlation analysis. We propose algorithms
LazyEV and LazyCCA to solve the two problems
with running times linearly dependent on the in-
put size and on k. Furthermore, our algorithms
are doubly-accelerated: our running times de-
pend only on the square root of the matrix condi-
tion number, and on the square root of the eigen-
gap. This is the ﬁrst such result for both k-
GenEV or k-CCA. We also provide the ﬁrst gap-
free results, which provide running times that de-
pend on 1/

ε rather than the eigengap.

√

1 Introduction

The Generalized Eigenvector (GenEV) problem and the
Canonical Correlation Analysis (CCA) are two fundamen-
tal problems in scientiﬁc computing, machine learning, op-
erations research, and statistics. Algorithms solving these
problems are often used to extract features to compare
large-scale datasets, as well as used for problems in regres-
sion (Kakade & Foster, 2007), clustering (Chaudhuri et al.,
2009), classiﬁcation (Karampatziakis & Mineiro, 2014),
word embeddings (Dhillon et al., 2011), and many others.
GenEV. Given two symmetric matrices A, B ∈ Rd×d
where B is positive deﬁnite. The GenEV problem is to ﬁnd
generalized eigenvectors v1, . . . , vd where each vi satisﬁes

vi ∈ arg max

v∈Rd

(cid:12)
(cid:12)v(cid:62)Av(cid:12)

(cid:12) s.t.

(cid:26) v(cid:62)Bv = 1

v(cid:62)Bvj = 0 ∀j ∈ [i − 1]

The values λi
i Avi are known as the generalized
eigenvalues, and it satisﬁes |λ1| ≥ · · · |λd|. Following the

def= v(cid:62)

*Equal contribution .

Future version of

this paper
shall be found at http://arxiv.org/abs/1607.06017.
1Microsoft Research 2Princeton University. Correspondence
to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li
<yuanzhil@cs.princeton.edu>.

tradition of (Wang et al., 2016; Garber & Hazan, 2015), we

assume without loss of generality that λi ∈ [−1, 1].

n X (cid:62)X, Sxy = 1

CCA. Given matrices X ∈ Rn×dx , Y ∈ Rn×dy and de-
noting by Sxx = 1
n Y (cid:62)Y ,
the CCA problem is to ﬁnd canonical-correlation vectors
{(φi, ψi)}r
(φi, ψi) ∈ arg max

i=1 where r = min{dx, dy} and each pair
(cid:8)φ(cid:62)Sxyψ(cid:9)

n X (cid:62)Y , Syy = 1

such that

φ∈Rdx ,ψ∈Rdy
(cid:26) φ(cid:62)Sxxφ = 1 ∧ φ(cid:62)Sxxφj = 0 ∀j ∈ [i − 1]
ψ(cid:62)Syyψ = 1 ∧ ψ(cid:62)Syyψj = 0 ∀j ∈ [i − 1]

The values σi
canonical-correlation coefﬁcients, and

def= φ(cid:62)

i Sxyψi ≥ 0 are known as the

1 ≥ σ1 ≥ · · · ≥ σr ≥ 0 is always satisﬁed.

It is a fact that solving CCA exactly can be reduced to solv-
ing GenEV exactly, if one deﬁnes B = diag{Sxx, Syy} ∈
Rd×d and A = [[0, Sxy]; [S(cid:62)
xy, 0]] ∈ Rd×d for d def= dx +dy;
(This reduction does not always hold if
see Lemma 2.3.
the generalized eigenvectors are computed only approxi-
mately.)

Despite the fundamental importance and the frequent ne-
cessity in applications, there are few results on obtaining
provably efﬁcient algorithms for GenEV and CCA until
very recently.
In the breakthrough result of Ma, Lu and
Foster (Ma et al., 2015), they proposed to study algorithms
to ﬁnd top k generalized eigenvectors (k-GenEV) or top k
canonical-correlation vectors (k-CCA). They designed an
alternating minimization algorithm whose running time is
only linear in the input matrix sparsity and nearly-linear in
k. Such algorithms are very appealing because in real-life
applications, it is often only relevant to obtain top correla-
tion vectors, as opposed to the less meaningful vectors in
the directions where the datasets do not correlate. Unfortu-
nately, the method of Ma, Lu and Foster has a running time
that linearly scales with κ and 1/gap, where

• κ ≥ 1 is the condition number of matrix B in GenEV,

or of matrices X (cid:62)X, Y (cid:62)Y in CCA; and
• gap ∈ [0, 1) is the eigengap λk−λk+1

in GenEV, or

λk

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

σk−σk+1
σk

in CCA.

These parameters are usually not constants and scale with

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

the problem size.

no slower than non-accelerated ones in the worst-case.

Challenge 1: Acceleration

• Gap-free results are better because they imply gap-

For many easier scientiﬁc computing problems, we are able
to design algorithms that have accelerated dependencies on
κ and 1/gap. As two concrete examples, k-PCA can be
gap as opposed
solved with a running time linearly in 1/
to 1/gap (Golub & Van Loan, 2012); computing B−1w
κ as
for a vector w can be solved in time linearly in
opposed to κ, where κ is the condition number of matrix
B (Shewchuk, 1994; Axelsson, 1985; Nesterov, 1983).

√

√

Therefore, can we obtain doubly-accelerated methods for
k-GenEV and k-CCA, meaning that the running times lin-
gap? Before this paper,
κ and 1/
early scale with both
for the general case k > 1, the method of Ge et al. (Ge
et al., 2016) made acceleration possible for parameter κ,
but not for parameter 1/gap (see Table 1).

√

√

Challenge 2: Gap-Freeness

Since gap can be even zero in the extreme case, can we
design algorithms that do not scale with 1/gap? Recall
that this is possible for the easier task of k-PCA. The block
Krylov method (Musco & Musco, 2015) runs in time linear
gap, where ε is the approxima-
in 1/
tion ratio. There is no gap-free result previously known for
k-GenEV or k-CCA even for k = 1.

ε as opposed to 1/

√

√

Challenge 3: Stochasticity

√

κ for κ being the condition number of B.

For matrix-related problems, one can usually obtain
stochastic running times which requires some notations to
describe.
Consider a simple task of computing B−1w for some vec-
tor w, where accelerated methods solve it in time linearly
If B =
in
1
n X (cid:62)X is given in the form of a covariance matrix where
X ∈ Rn×d, then (accelerated) stochastic methods compute
B−1w in a time linearly in (1 + (cid:112)κ(cid:48)/n) instead of
κ,
where κ(cid:48) = maxi∈[n]{(cid:107)Xi(cid:107)2}
∈ (cid:2)κ, nκ(cid:3) and Xi is the i-th
row of X. (See Lemma 2.6.) Since 1 + (cid:112)κ(cid:48)/n ≤ O(
κ),
stochastic methods are no slower than non-stochastic ones.

λmin(B)

√

√

So, can we obtain a similar but doubly-accelerated
stochastic method for k-CCA?1 Note that, if the doubly-
accelerated requirement is dropped, this task is easier and
indeed possible, see Ge et al. (Ge et al., 2016). However,
since their stochastic method is not doubly-accelerated, in
certain parameter regimes, it runs even slower than non-
stochastic ones (even for k = 1, see Table 2).

Remark.
running time:

In general, if designed properly, for worst case

• Accelerated results are usually better because they are

dependent ones.2

• Stochastic results are usually better because they are no

slower than non-stochastic ones in the worst-case.

1.1 Our Main Results

We provide algorithms LazyEV and LazyCCA that are
doubly-accelerated, gap-free, and stochastic.3
For the general k-GenEV problem, our LazyEV can be im-
plemented to run in time4
κ

√

(cid:17)

(cid:101)O

(cid:16) knnz(B)
√
gap

(cid:101)O

(cid:16) knnz(B)
ε

√

√

κ

+

+

√

knnz(A) + k2d
gap
knnz(A) + k2d
√
ε

(cid:17)

or

in the gap-dependent and gap-free cases respectively. Since
gap
our running time only linearly depends on
(resp.

ε), our algorithm LazyEV is doubly-accelerated.

κ and

√

√

√

For the general k-CCA problem, our LazyCCA can be im-
plemented to run in time

(cid:16) knnz(X, Y ) · (cid:0)1 + (cid:112)κ(cid:48)/n(cid:1) + k2d
√

(cid:17)

or

gap

(cid:16) knnz(X, Y ) · (cid:0)1 + (cid:112)κ(cid:48)/n(cid:1) + k2d
√

(cid:17)

(cid:101)O

(cid:101)O

ε
in the gap-dependent and gap-free cases respectively.
Here, nnz(X, Y ) = nnz(X) + nnz(Y ) and κ(cid:48) =
2 maxi{(cid:107)Xi(cid:107)2,(cid:107)Yi(cid:107)2}
λmin(diag{Sxx,Syy}) where Xi or Yi is the i-th row vector
of X or Y . Therefore, our algorithm LazyCCA is doubly-
accelerated and stochastic.

We fully compare our results with prior work in Table 2
(for k = 1) and Table 1 (for k ≥ 1), and summarize our
main contributions:

• For k > 1, we outperform all relevant prior works (see
Table 1). Moreover, no known method was doubly-
accelerated even in the non-stochastic setting.

• For k ≥ 1, we obtain the ﬁrst gap-free running time.

• Even for k = 1, we outperform most of the state-of-

the-arts (see Table 2).

Note that for CCA with k > 1, previous result CCALin
only outputs the subspace spanned by the top k correlation
vectors but does not identify which vector gives the highest
correlation and so on. Our LazyCCA provides per-vector

2If a method depends on 1/ε then one can choose ε = gap

and this translates to a gap-dependent running time.

3Recalling Footnote 1, for notational simplicity, we only state

our k-GenEV result in non-stochastic running time.

1 Note that a similar problem can be also asked for k-GenEV
when A and B are both given in their covariance matrix forms.
We refrain from doing it in this paper for notational simplicity.

4Throughout the paper, we use the (cid:101)O notation to hide poly-
logarithmic factors with respect to κ, 1/gap, 1/ε, d, n. We use
nnz(M ) to denote the time needed to multiply M to a vector.

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

Problem

Paper

k-GenEV

GenELin (Ge et al., 2016)

LazyEV Theorem 4.3

LazyEV Theorem 4.4

κB

√

Running time
(cid:101)O(cid:0) knnz(B)
(cid:101)O(cid:0) knnz(B)
(cid:101)O(cid:0) knnz(B)

gap
√

gap

√

√

√

κB

κB

ε

gap

+ knnz(A)+k2d
+ knnz(A)+k2d
+ knnz(A)+k2d

gap

√

√

ε

(cid:1)

(cid:1)

(× for outperformed)
(cid:1)

×

gap-free?

negative EV?

Problem

Paper

(× for outperformed)

gap-free?

stochastic?

AppGrad (Ma et al., 2015)

CCALin (Ge et al., 2016)

CCALin (Ge et al., 2016)

LazyCCA (arXiv version)

LazyCCA (arXiv version)

LazyCCA (arXiv version)

LazyCCA (arXiv version)

(cid:1)

√

(cid:1)

gap

Running time
(cid:101)O(cid:0) knnz(X,Y )·κ+k2d
(cid:101)O(cid:0) knnz(X,Y )·
κ+k2d
gap
√
(cid:0)
(cid:101)O(cid:0) knnz(X,Y )·
1+
(cid:0)
(cid:101)O(cid:0) knnz(X,Y )·
1+
√
(cid:0)
(cid:101)O(cid:0) knnz(X,Y )·
1+
√
(cid:101)O(cid:0)knnz(X, Y ) · (cid:0)1 +
(cid:101)O(cid:0)knnz(X, Y ) · (cid:0)1 +

gap
√

√

gap

ε

(cid:1)
+k2d
κ(cid:48)/n

(cid:1)
+k2d
κ(cid:48)/n

(cid:1)
+k2d
κ(cid:48)/n

(cid:1)

(cid:1)

(cid:1)

(local conv.)

×

×

×

√

√

√

κ(cid:48)
gap·σk·(nnz(X,Y )/kd)1/4
(cid:1)(cid:1)
κ(cid:48)
ε·σk·(nnz(X,Y )/kd)1/4

√

(cid:1)(cid:1)

k-CCA

no

no

yes

no

no

no

no

yes

no

yes

no

yes

yes

no

no

yes

yes

yes

doubly

doubly

Table 1: Performance comparison on k-GenEV and k-CCA.

In GenEV, gap = λk−λk+1
λk
In CCA, gap = σk−σk+1

∈ [0, 1] and κB = λmax(B)
λmin(B) > 1.
∈ [0, 1], κ = λmax(diag{Sxx,Syy })

σk

λmin(diag{Sxx,Syy }) > 1, κ(cid:48) = 2 maxi{(cid:107)Xi(cid:107)2,(cid:107)Yi(cid:107)2}

λmin(diag{Sxx,Syy }) ∈ [κ, 2nκ], and σk ∈ [0, 1].

Remark 1. Stochastic methods depend on a modiﬁed condition number κ(cid:48). The reason κ(cid:48) ∈ [κ, 2nκ] is in Fact 2.5.
Remark 2. All non-stochastic CCA methods in this table have been outperformed because 1 + (cid:112)κ(cid:48)/n ≤ O(κ).
Remark 3. Doubly-stochastic methods are not necessarily interesting. We discuss them in Section 1.2.

guarantees on all the top k correlation vectors.

the experiments of (Shalev-Shwartz & Zhang, 2014);

1.2 Our Side Results on Doubly-Stochastic Methods

Recall that when considering acceleration, there are two
parameters κ and 1/gap. One can also design stochas-
tic methods with respect to both parameters κ and 1/gap,
meaning that

√
κ(cid:48)/nc
√
gap

with a running time proportional to 1 +

√
√

1+

κ(cid:48)/n
gap

√
κ
instead of
gap (non-stochastic).
(stochastic) or
√
The constant c is usually 1/2. We call such methods
doubly-stochastic.

√
√

Unfortunately, doubly-stochastic methods are usually
slower than stochastic ones. Take 1-CCA as an example.
The best stochastic running time (obtained exclusively by
(cid:1). In contrast,
us) for 1-CCA is nnz(X, Y ) · (cid:101)O(cid:0) 1+
if one uses a doubly-stochastic method —either (Wang
et al., 2016) or our LazyCCA— the running time becomes
nnz(X, Y ) · (cid:101)O(cid:0)1 +
doubly-stochastic methods are faster than stochastic ones
κ(cid:48)
σ1

(cid:1). Therefore, for 1-CCA,

√
κ(cid:48)/n1/4
√
gap·σ1

≤ o(n1/2) .

only when

κ(cid:48)/n
gap

• κ(cid:48) is between n1/2 and 100n in all the CCA experi-

ments of (Wang et al., 2016); and

• by Fact 2.5 it satisﬁes κ(cid:48) ≥ d so κ(cid:48) cannot be smaller
than o(n1/2) unless d (cid:28) n1/2.5 Even worse, parameter
σ1 ∈ [0, 1] is usually much smaller than 1. Note that σ1
is scaling invariant: even if one scales X and Y up by
the same factor, σ1 remains unchanged.

Nevertheless, to compare our LazyCCA with all relevant
prior works, we obtain doubly-stochastic running times for
k-CCA as well. Our running time matches that of (Wang
et al., 2016) when k = 1, and no doubly-stochastic running
time for k > 1 was known before our work.

1.3 Other Related Works

For the easier task of PCA and SVD, the ﬁrst gap-free
result was obtained by Musco and Musco (Musco &
Musco, 2015), the ﬁrst stochastic result was obtained by
Shamir (Shamir, 2015), and the ﬁrst accelerated stochas-
tic result was obtained by Garber et al. (Garber & Hazan,
2015; Garber et al., 2016). The shift-and-invert precondi-
tioning technique of Garber et al. is also used in this paper.

For another

related problem PCR (principle compo-

The above condition is usually not satisﬁed. For instance,
• κ(cid:48) is usually around n for most interesting data-sets, cf.

5Note that item (3) κ(cid:48) ≥ d may not hold in the more general

setting of CCA, see Remark A.1.

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

Problem

Paper

(× for outperformed)

gap-free?

negative EV?

Problem

Paper

(× for outperformed)

gap-free?

stochastic?

GenELin (Ge et al., 2016)

1-GenEV

LazyEV Theorem 4.3

LazyEV Theorem 4.4

1-CCA

ALS (Wang et al., 2016)

AppGrad (Ma et al., 2015)

CCALin (Ge et al., 2016)

ALS (Wang et al., 2016)

SI (Wang et al., 2016)

CCALin (Ge et al., 2016)

LazyCCA (arXiv version)

LazyCCA (arXiv version)

SI (Wang et al., 2016)

LazyCCA (arXiv version)

LazyCCA (arXiv version)

√

gap

κB

Running time
(cid:101)O(cid:0) nnz(B)
(cid:101)O(cid:0) nnz(B)
(cid:101)O(cid:0) nnz(B)

+ nnz(A)
gap
gap + nnz(A)
gap
√
κB√
+ nnz(A)
√
ε

κB

√

√

√

ε

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

gap

κ
gap

κ
gap2

Running time
nnz(X, Y ) · (cid:101)O(cid:0) κ
nnz(X, Y ) · (cid:101)O(cid:0) √
nnz(X, Y ) · (cid:101)O(cid:0) √
nnz(X, Y ) · (cid:101)O(cid:0) √
κ
gap·σ1
√
nnz(X, Y ) · (cid:101)O(cid:0) 1+
nnz(X, Y ) · (cid:101)O(cid:0) 1+
nnz(X, Y ) · (cid:101)O(cid:0) 1+
nnz(X, Y ) · (cid:101)O(cid:0) 1+

gap2
√
√
√
√

gap
√

√

(cid:1)

κ(cid:48)/n

κ(cid:48)/n

κ(cid:48)/n
gap
κ(cid:48)/n
ε
√

nnz(X, Y ) · (cid:101)O

1 +

(cid:16)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:17)

κ(cid:48)/n1/4
√
gap·σ1

(see Remark 3)

nnz(X, Y ) · (cid:101)O(cid:0)1 +
nnz(X, Y ) · (cid:101)O(cid:0)1 +

√

√

κ(cid:48)/n1/4
√
gap·σ1
κ(cid:48)/n1/4
√
ε·σ1

(cid:1)

(cid:1)

×

×

×

×

×

×

×

no

no

yes

no

no

no

no

no

no

no

yes

no

no

yes

no

yes

yes

no

no

no

no

yes

yes

yes

yes

doubly

doubly

doubly

Table 2: Performance comparison on 1-GenEV and 1-CCA.

In GenEV, gap = λ1−λ2
λ1
In CCA, gap = σ1−σ2

∈ [0, 1] and κB = λmax(B)
λmin(B) > 1.
∈ [0, 1], κ = λmax(diag{Sxx,Syy })

σ1

λmin(diag{Sxx,Syy }) > 1, κ(cid:48) = 2 maxi{(cid:107)Xi(cid:107)2,(cid:107)Yi(cid:107)2}

λmin(diag{Sxx,Syy }) ∈ [κ, 2nκ], and σ1 ∈ [0, 1].

Remark 1. Stochastic methods depend on modiﬁed condition number κ(cid:48); the reason κ(cid:48) ∈ [κ, 2nκ] is in Def. 2.4.
Remark 2. All non-stochastic CCA methods in this table have been outperformed because 1 + (cid:112)κ(cid:48)/n ≤ O(κ).

Remark 3. Doubly-stochastic methods are not necessarily interesting. We discuss them in Section 1.2.

Remark 4. Some CCA methods have a running time dependency on σ1 ∈ [0, 1], and this is intrinsic and cannot be removed.
In particular, if we scale the data matrix X and Y , the value σ1 stays the same.

Remark 5. The only (non-doubly-stochastic) doubly-accelerated method before our work is SI (Wang et al., 2016) (for 1-CCA
only). Our LazyEV is faster than theirs by a factor Ω((cid:112)nκ/κ(cid:48) × (cid:112)1/σ1). Here, nκ/κ(cid:48) ≥ 1/2 and 1/σ1 ≥ 1 are two
scaling-invariant quantities usually much greater than 1.

nent regression), we recently obtained an accelerated
method (Allen-Zhu & Li, 2017) as opposed the previously
non-accelerated one (Frostig et al., 2016); however, the ac-
celeration techniques there are not relevant to this paper.

For GenEV and CCA, many scalable algorithms have been
designed recently (Ma et al., 2015; Wang & Livescu, 2015;
Michaeli et al., 2015; Witten et al., 2009; Lu & Foster,
2014). However, as summarized by the authors of CCALin,
these cited methods are more or less heuristics and do not
have provable guarantees. Furthermore, for k > 1, the
AppGrad method (Ma et al., 2015) only provides local con-
vergence guarantees and thus requires a warm-start whose

computational complexity is not discussed in their paper.

Finally, our algorithms on GenEV and CCA are based on
ﬁnding vectors one-by-one, which is advantageous in prac-
tice because one does not need k to be known and can stop
the algorithm whenever the eigenvalues (or correlation val-
ues) are too small. Known approaches for k > 1 cases
(such as GenELin, CCALin, AppGrad) ﬁnd all k vectors at
once, therefore requiring k to be known beforehand. As a
separate note, these known approaches do not need the user
to know the desired accuracy a priori but our LazyEV and
LazyCCA algorithms do.

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

2 Preliminaries

We denote by (cid:107)x(cid:107) or (cid:107)x(cid:107)2 the Euclidean norm of vector
x. We denote by (cid:107)A(cid:107)2, (cid:107)A(cid:107)F , and (cid:107)A(cid:107)Sq respectively the
spectral, Frobenius, and Schatten q-norm of matrix A (for
q ≥ 1). We write A (cid:23) B if A, B are symmetric and A − B
is positive semi-deﬁnite (PSD), and write A (cid:31) B if A, B
are symmetric but A − B is positive deﬁnite (PD). We de-
note by λmax(M ) and λmin(M ) the largest and smallest
eigenvalue of a symmetric matrix M , and by κM the con-
dition number λmax(M )/λmin(M ) of a PSD matrix M .
Throughout this paper, we use nnz(M ) to denote the time
to multiply matrix M to any arbitrary vector. For two ma-
trices X, Y , we denote by nnz(X, Y ) = nnz(X)+nnz(Y ),
and by Xi or Yi the i-th row vector of X or Y . We
also use poly(x1, x2, . . . , xt) to represent a quantity that
is asymptotically at most polynomial in terms of vari-
ables x1, . . . , xt. Given a column orthonormal matrix
U ∈ Rn×k, we denote by U ⊥ ∈ Rn×(n−k) the column
orthonormal matrix consisting of an arbitrary basis in the
space orthogonal to the span of U ’s columns.
Given a PSD matrix B and a vector v, v(cid:62)Bv is the B-semi-
norm of v. Two vectors v, w are B-orthogonal if v(cid:62)Bw =
0. We denote by B−1 the Moore-Penrose pseudoinverse
of B if B is not invertible, and by B1/2 the matrix square
root of B (satisfying B1/2 (cid:23) 0). All occurrences of B−1,
B1/2 and B−1/2 are for analysis purpose only. Our ﬁnal
algorithms only require multiplications of B to vectors.

Deﬁnition 2.1 (GenEV). Given symmetric matrices
A, B ∈ Rd×d where B is positive deﬁnite. The general-
ized eigenvectors of A with respect to B are v1, . . . , vd,
where each vi is
(cid:40)
(cid:12)v(cid:62)Av(cid:12)
(cid:12)

v(cid:62)Bv = 1
v(cid:62)Bvj = 0 ∀j ∈ [i − 1]

vi ∈ arg max

(cid:12) s.t.

(cid:41)

v∈Rd

The generalized eigenvalues λ1, . . . , λd satisfy λi =
v(cid:62)
i Avi which can be negative.

Following (Wang et al., 2016; Garber & Hazan, 2015), we
assume without loss of generality that λi ∈ [−1, 1].
Deﬁnition 2.2 (CCA). Given X ∈ Rn×dx , Y ∈ Rn×dy ,
letting Sxx = 1
n Y (cid:62)Y ,
the canonical-correlation vectors are {(φi, ψi)}r
where r = min{dx, dy} and for all i ∈ [r]:

n X (cid:62)X, Sxy = 1

n X (cid:62)Y , Syy = 1

i=1

(cid:40)

(φi, ψi) ∈ arg max

φ(cid:62)Sxyψ such that

φ∈Rdx ,ψ∈Rdy

(cid:110) φ(cid:62)Sxxφ = 1 ∧ φ(cid:62)Sxxφj = 0 ∀j ∈ [i − 1]
ψ(cid:62)Syyψ = 1 ∧ ψ(cid:62)Syyψj = 0 ∀j ∈ [i − 1]

(cid:41)

(cid:111)

The corresponding canonical-correlation coefﬁcients
σ1, . . . , σr satisfy σi = φ(cid:62)

i Sxyψi ∈ [0, 1].

We emphasize that σi always lies in [0, 1] and is scaling-
invariant. When dealing with a CCA problem, we also de-
note by d = dx + dy.

Lemma 2.3 (CCA to GenEV). Given a CCA problem with
matrices X ∈ Rn×dx , Y ∈ Rn×dy , let the canonical-
correlation vectors and coefﬁcients be {(φi, ψi, σi)}r
where r = min{dx, dy}. Deﬁne A =

i=1
and

(cid:17)

(cid:16) 0 Sxy
S(cid:62)
0
xy

(cid:16) Sxx 0
0 Syy

(cid:17)

B =
spect to B has 2r eigenvalues {±σi}r
ing generalized eigenvectors
,
maining dx + dy − 2r eigenvalues are zeros.

. Then, the GenEV problem of A with re-
i=1 and correspond-
(cid:16) −φi
. The re-
ψi

(cid:110)(cid:16) φi
ψi

(cid:17)(cid:111)n

i=1

(cid:17)

Deﬁnition 2.4.
Lemma 2.3. We deﬁne condition numbers

In CCA, let A and B be as deﬁned in

κ def= κB = λmax(B)

λmin(B) and κ(cid:48) def= 2 maxi{(cid:107)Xi(cid:107)2,(cid:107)Yi(cid:107)2}

λmin(B)

.

Fact 2.5. κ(cid:48) ∈ [κ, 2nκ] and κ(cid:48) ≥ d. (See full version.)

Lemma 2.6. Given matrices X ∈ Rn×dx , Y ∈ Rn×dy ,
let A and B be as deﬁned in Lemma 2.3. For every w ∈
Rd, the Katyusha method (Allen-Zhu, 2017) ﬁnds a vector
w(cid:48) ∈ Rd satisfying (cid:107)w(cid:48) − B−1Aw(cid:107) ≤ ε in time
κ(cid:107)w(cid:107)2
nnz(X, Y ) · (cid:0)1 + (cid:112)κ(cid:48)/n(cid:1) · log
ε

O

(cid:16)

(cid:17)

.

3 Leading Eigenvector via Two-Sided

Shift-and-Invert

We introduce AppxPCA±, the multiplicative approximation
algorithm for computing the two-sided leading eigenvector
of a symmetric matrix. AppxPCA± uses the shift-and-invert
framework (Garber & Hazan, 2015; Garber et al., 2016),
and shall become our building block for the LazyEV and
LazyCCA algorithms in the subsequent sections.

Our pseudo-code Algorithm 1 is a modiﬁcation of Algo-
rithm 5 in (Garber & Hazan, 2015), and reduces the eigen-
vector problem to oracle calls to an arbitrary matrix inver-
sion oracle A. The main differences between AppxPCA±
and (Garber & Hazan, 2015) are two-fold.
First, given a symmetric matrix M , AppxPCA± simultane-
ously considers an upper-bounding shift together with a
lower-bounding shift, and try to perform power methods
with respect to (λI − M )−1 and (λI + M )−1. This al-
lows us to determine approximately how close λ is to the
largest and the smallest eigenvalues of M , and decrease λ
accordingly. In the end, AppxPCA± outputs an approximate
eigenvector of M that corresponds to a negative eigenvalue
if needed. Second, we provide a multiplicative-error guar-
antee rather than additive as appeared in (Garber & Hazan,
2015). Without such guarantee, our ﬁnal running time will
depend on

gap·λmax(M ) rather than 1

gap .6

1

6This is why the SI method of (Wang et al., 2016) also uses

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

Algorithm 1 AppxPCA±(A, M, δ×, ε, p)
Input: A, an approximate matrix inversion method; M ∈ Rd×d, a symmetric matrix satisfying −I (cid:22) M (cid:22) I; δ× ∈
(0, 0.5], a multiplicative error; ε ∈ (0, 1), a numerical accuracy parameter; and p ∈ (0, 1), the conﬁdence parameter.
(cid:5) (cid:98)w0 is a random unit vector, see Def. 3.2
(cid:5) θ is the parameter of RanInit, see Def. 3.2

p2

64m1

(cid:0) δ×
48

(cid:1)(cid:7), m2 ← (cid:6) log (cid:0) 36dθ
p2ε
(cid:1)m2

1: (cid:98)w0 ← RanInit(d); s ← 0; λ(0) ← 1 + δ×;
2: m1 ← (cid:6)4 log (cid:0) 288dθ
(cid:1)(cid:7);
3: (cid:101)ε1 ← 1
4: repeat
5:
6:
7:

s ← s + 1;
for t = 1 to m1 do

Apply A to ﬁnd (cid:98)wt satisfying (cid:13)

(cid:1)m1 and (cid:101)ε2 ← ε

(cid:0) δ×
48

8m2

(cid:13) (cid:98)wt − (λ(s−1)I − M )−1

(cid:98)wt−1

(cid:13)
(cid:13) ≤ (cid:101)ε1;

8:
9:
10:
11:

wa ← (cid:98)wm1/(cid:107) (cid:98)wm1(cid:107);
Apply A to ﬁnd va satisfying (cid:13)
for t = 1 to m1 do

(cid:13)va − (λ(s−1)I − M )−1wa

Apply A to ﬁnd (cid:98)wt satisfying (cid:13)

(cid:13) (cid:98)wt − (λ(s−1)I + M )−1

(cid:13)
(cid:13) ≤ (cid:101)ε1;
(cid:13)
(cid:13) ≤ (cid:101)ε1;

(cid:98)wt−1

(cid:5) m1 = T PM(8, 1/32, p) and m2 = T PM(2, ε/4, p), see Lemma B.1

(cid:5) wa is roughly (λ(s−1)I − M )−m1

(cid:98)w0 then normalized

(cid:13)
(cid:13)vb − (λ(s−1)I + M )−1wb
(cid:13) ≤ (cid:101)ε1;
and λ(s) ← λ(s−1) − ∆(s)
2 ;

(cid:5) wb is roughly (λ(s−1)I + M )−m1

(cid:98)w0 then normalized

max{w(cid:62)

12:
13:

1
a va,w(cid:62)

wb ← (cid:98)wm1/(cid:107) (cid:98)wm1(cid:107);
Apply A to ﬁnd vb satisfying (cid:13)
∆(s) ← 1
2 ·
14:
15: until ∆(s) ≤ δ×λ(s)
16: f ← s;
17: if the last w(cid:62)
18:
19:

a va ≥ w(cid:62)
for t = 1 to m2 do

b vb then

12

b vb}−(cid:101)ε1

Apply A to ﬁnd (cid:98)wt satisfying (cid:13)
return (+, w) where w def= (cid:98)wm2 /(cid:107) (cid:98)wm2(cid:107).

(cid:13) (cid:98)wt − (λ(f )I − M )−1

(cid:98)wt−1

(cid:13)
(cid:13) ≤ (cid:101)ε2;

for t = 1 to m2 do

Apply A to ﬁnd (cid:98)wt satisfying (cid:13)
return (−, w) where w def= (cid:98)wm2/(cid:107) (cid:98)wm2(cid:107).

(cid:13) (cid:98)wt − (λ(f )I + M )−1

(cid:98)wt−1

(cid:13)
(cid:13) ≤ (cid:101)ε2;

20:
21: else
22:
23:

24:
25: end if

We prove in full version the following theorem:
Theorem 3.1 (AppxPCA±, informal). Let M ∈ Rd×d be a
symmetric matrix with eigenvalues 1 ≥ λ1 ≥ · · · ≥ λd ≥
−1 and eigenvectors u1, . . . , ud. Let λ∗ = max{λ1, −λd}.
With probability at least 1 − p, AppxPCA± produces a pair
(sgn, w) satisfying

• if sgn = +, then w is an approx. positive eigenvector:

w(cid:62)M w ≥

1−

(cid:16)

λ∗ (cid:94) (cid:88)

(cid:17)

δ×
2

(w(cid:62)ui)2 ≤ ε

i∈[d]
λi≤(1−δ×/2)λ∗

• if sgn = −, then w is an approx. negative eigenvector:

w(cid:62)M w ≤ −

1−

(cid:16)

λ∗ (cid:94) (cid:88)

(cid:17)

δ×
2

(w(cid:62)ui)2 ≤ ε

i∈[d]
λi≥−(1−δ×/2)λ∗

The number of oracle calls to A is (cid:101)O(log(1/δ×)), and each
time we call A it satisﬁes

shift-and-invert but depends on

1
gap·σ1

in Table 2.

1

•

δ×

] and

• λmax(λ(s)I−M )

λmin(λ(s)I−M ) , λmax(λ(s)I+M )
1
λmin(λ(s)I−M ) ,

λmin(λ(s)I+M ) ∈ [1, 96
λmin(λ(s)I+M ) ≤ 48
We remark here that, unlike the original shift-and-invert
method which chooses a random (Gaussian) unit vector in
Line 1 of AppxPCA±, we have allowed this initial vector to
be generated from an arbitrary θ-conditioned random vec-
tor generator (for later use), deﬁned as follows:

δ×λ∗ .

An algorithm RanInit(d) is a θ-
Deﬁnition 3.2.
conditioned random vector generator if w = RanInit(d)
is a d-dimensional unit vector and, for every p ∈ (0, 1),
every unit vector u ∈ Rd, with probability at least 1 − p, it
satisﬁes (u(cid:62)w)2 ≤ p2θ
9d .

This modiﬁcation is needed in order to obtain our efﬁcient
implementations of GenEV and CCA. One can construct a
θ-conditioned random vector generator as follows:
Proposition 3.3. Given a PSD matrix B ∈ Rd×d, if we set
RanInit(d) def= B1/2v
(v(cid:62)Bv)0.5 where v is a random Gaussian
vector, then RanInit(d) is a θ-conditioned random vector

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

generator for θ = κB.

4 LazyEV: Generalized Eigendecomposition

In this section, we construct an algorithm LazyEV that,
given symmetric matrix M ∈ Rd×d, computes approxi-
mately the k leading eigenvectors of M that have the largest
absolute eigenvalues. Then, for the original k-GenEV
problem, we set M = B−1/2AB−1/2 and run LazyEV.
This is our plan to ﬁnd the top k leading generalized eigen-
vectors of A with respect to B.

Our algorithm LazyEV is formally stated in Algorithm 2.
The algorithm applies k times AppxPCA±, each time com-
puting an approximate leading eigenvector of M with a
multiplicative error δ×/2, and projects the matrix M into
the orthogonal space with respect to the obtained leading
eigenvector. We state our main approximation theorem be-
low.
Theorem 4.1 (informal). Let M ∈ Rd×d be a symmetric
matrix with eigenvalues λ1, . . . , λd ∈ [−1, 1] and corre-
sponding eigenvectors u1, . . . , ud, and |λ1| ≥ · · · ≥ |λd|.
If εpca is sufﬁciently small,7 LazyEV outputs a (column) or-
thonormal matrix Vk = (v1, . . . , vk) ∈ Rd×k which, with
probability at least 1 − p, satisﬁes:
(a) (cid:107)V (cid:62)

k U (cid:107)2 ≤ ε where U = (uj, . . . , ud) and j is the

smallest index satisfying |λj| ≤ (1 − δ×)λk.

(b) For every i ∈ [k], (1−δ×)|λi| ≤ |v(cid:62)

i M vi| ≤ 1

1−δ×

|λi|.

Above, property (a) ensures the k columns of Vk have neg-
ligible correlation with the eigenvectors of M whose ab-
solute eigenvalues are ≤ (1 − δ×)λk; property (b) ensures
the Rayleigh quotients v(cid:62)
i M vi are all correct up to a 1±δ×
error. We in fact have shown two more useful properties in
the full version that may be of independent interest.
The next theorem states that, if M = B−1/2AB−1/2, our
LazyEV can be implemented without the necessity to com-
pute B1/2 or B−1/2.
Theorem 4.2 (running time). Let A, B ∈ Rd×d be two
symmetric matrices satisfying B (cid:31) 0 and −B (cid:22) A (cid:22) B.
Suppose M = B−1/2AB−1/2 and RanInit(d) is deﬁned
in Proposition 3.3 with respect to B. Then, the computa-
tion of V ← B−1/2LazyEV(A, M, k, δ×, εpca, p) can be
implemented to run in time
(cid:16) knnz(B)+k2d+kΥ
√

where Υ is the time to multiply

(cid:17)

• (cid:101)O

δ×

B−1A to a vector, or

√

(cid:16) k

• (cid:101)O

κB nnz(B)+knnz(A)+k2d
δ×
dient to multiply B−1A to a vector.

√

(cid:17)

if we use Conjugate gra-

7Meaning εpca ≤ O(cid:0)poly(ε, δ×,

d )(cid:1). The complete
speciﬁcations of εpca is included in the full version. Since our ﬁnal
running time only depends on log(1/εpca), we have not attempted
to improve the constants in this polynomial dependency.

|λk+1| , 1

|λ1|

Choosing parameter δ× as either gap or ε, our two main
theorems above immediately imply the following results
for the k-GenEV problem: (proved in full version)

Let
Theorem 4.3 (gap-dependent GenEV, informal).
A, B ∈ Rd×d be two symmetric matrices satisfying B (cid:31)
0 and −B (cid:22) A (cid:22) B. Suppose the generalized eigen-
value and eigenvector pairs of A with respect to B are
{(λi, ui)}d
i=1, and it satisﬁes 1 ≥ |λ1| ≥ · · · ≥ |λd|.
Then, LazyEV outputs V k ∈ Rd×k satisfying

V

(cid:62)
k BV k = I

and (cid:107)V
√

(cid:62)
k BW (cid:107)2 ≤ ε

in time

(cid:16) k

(cid:101)O

κBnnz(B) + knnz(A) + k2d
gap

√

(cid:17)

Here, W = (uk+1, . . . , ud) and gap = |λk|−|λk+1|

.

|λk|

In the same
Theorem 4.4 (gap-free GenEV, informal).
setting as Theorem 4.3, our LazyEV outputs V k =
(v1, . . . , vk) ∈ Rd×k satisfying V

(cid:62)
k BV k = I and

∀s ∈ [k] : (cid:12)

(cid:12)v(cid:62)

(cid:104)

(cid:12)
(cid:12) ∈

(1 − ε)|λs|,

(cid:105)

|λs|
1 − ε

in time

(cid:101)O

κBnnz(B) + knnz(A) + k2d
ε

√

(cid:17)

.

s Avs
√

(cid:16) k

5

Ideas Behind Theorems 4.1 and 4.2

In Section 5.1 we discuss how to ensure accuracy: that is,
why does LazyEV guarantee to approximately ﬁnd the top
eigenvectors of M . In the full version of this paper, we also
discuss how to implement LazyEV without compute B1/2
explicitly, thus proving Theorem 4.2.

5.1

Ideas Behind Theorem 4.1

Our approximation guarantee in Theorem 4.1 is a nat-
ural generalization of the recent work on fast iterative
methods to ﬁnd the top k eigenvectors of a PSD ma-
trix M (Allen-Zhu & Li, 2016). That method is called
LazySVD and we summarize it as follows.

At a high level, LazySVD ﬁnds the top k eigenvectors one-
by-one and approximately. Starting with M0 = M , in the
s-th iteration where s ∈ [k], LazySVD computes approxi-
mately the leading eigenvector of matrix Ms−1 and call it
vs. Then, LazySVD projects Ms ← (I − vsv(cid:62)
s )Ms−1(I −
vsv(cid:62)
s ) and proceeds to the next iteration.
While the algorithmic idea of LazySVD is simple, the anal-
ysis requires some careful linear algebraic lemmas. Most
notably, if vs is an approximate leading eigenvector of
Ms−1, then one needs to prove that the small eigenvectors
of Ms−1 somehow still “embed” into that of Ms after pro-
jection. This is achieved by a gap-free variant of the Wedin
theorem plus a few other technical lemmas, and we rec-
ommend interested readers to see the high-level overview
section of (Allen-Zhu & Li, 2016).

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

Algorithm 2 LazyEV(A, M, k, δ×, εpca, p)
Input: A, an approximate matrix inversion method; M ∈ Rd×d, a matrix satisfying −I (cid:22) M (cid:22) I; k ∈ [d], the desired
rank; δ× ∈ (0, 1), a multiplicative error; εpca ∈ (0, 1), a numerical accuracy; and p ∈ (0, 1), a conﬁdence parameter.

1: M0 ← M ; V0 = [];
2: for s = 1 to k do
(∼, v(cid:48)
3:
vs ← (cid:0)(I − Vs−1V (cid:62)
4:
Vs ← [Vs−1, vs];
5:
Ms ← (I − vsv(cid:62)
6:
7: end for
8: return Vk.

s) ← AppxPCA±(A, Ms−1, δ×/2, εpca, p/k);
(cid:13)
(cid:1)/(cid:13)
(cid:13);

(cid:13)(I − Vs−1V (cid:62)

s−1)v(cid:48)
s

s−1)v(cid:48)
s

(cid:5) v(cid:48)

s is an approximate two-sided leading eigenvector of Ms−1
s to V ⊥
s−1

(cid:5) project v(cid:48)

s )Ms−1(I − vsv(cid:62)
s )

(cid:5) we also have Ms = (I − VsV (cid:62)

s )M (I − VsV (cid:62)
s )

In this paper, to relax the assumption that M is PSD, and to
ﬁnd leading eigenvectors whose absolute eigenvalues are
large, we have to make several non-trivial changes. On
the algorithm side, LazyEV uses our two-sided shift-and-
invert method in Section 3 to ﬁnd the leading eigenvector
of Ms−1 with largest absolute eigenvalue. On the analysis
side, we have to make sure all lemmas properly deal with
negative eigenvalues. For instance:
• If we perform a projection M (cid:48) ← (I − vv(cid:62))M (I −
vv(cid:62)) where v correlates by at most ε with all eigenvec-
tors of M whose absolute eigenvalues are smaller than a
threshold µ, then, after the projection, we need to prove
that these eigenvectors can be approximately “embed-
ded” into the eigenspace spanned by all eigenvectors of
M (cid:48) whose absolute eigenvalues are smaller than µ + τ .
The approximation of this embedding should depend on
ε, µ and τ .

The full proof of Theorem 4.1 is in the arXiv version. It re-
lies on a few matrix algebraic lemmas (including the afore-
mentioned “embedding lemma”).

6 Conclusion

In this paper we propose new iterative methods to solve
the generalized eigenvector and the canonical correlation
analysis problems. Our methods ﬁnd the most signiﬁcant k
eigenvectors or correlation vectors, and have running times
that linearly scales with k.

Most importantly, our methods are doubly-accelerated: the
running times have square-root dependencies both with re-
spect to the condition number of the matrix (i.e., κ) and
with respect to the eigengap (i.e., gap). They are the ﬁrst
doubly-accelerated iterative methods at least for k > 1.
They can also be made gap-free, and are the ﬁrst gap-free
iterative methods even for 1-GenEV or 1-CCA.

Although this is a theory paper, we believe that if imple-
mented carefully, our methods can outperform not only
previous iterative methods (such as GenELin, AppGrad,
CCALin), but also the commercial mathematics libraries
for sparse matrices of dimension more than 10, 000. We

leave it a future work for such careful comparisons.

References

Allen-Zhu, Zeyuan. Katyusha: The First Direct Accelera-
tion of Stochastic Gradient Methods. In STOC, 2017.

Allen-Zhu, Zeyuan and Li, Yuanzhi. LazySVD: Even
Faster SVD Decomposition Yet Without Agonizing
Pain. In NIPS, 2016.

Allen-Zhu, Zeyuan and Li, Yuanzhi. Faster Principal Com-
ponent Regression and Stable Matrix Chebyshev Ap-
In Proceedings of the 34th International
proximation.
Conference on Machine Learning, ICML ’17, 2017.

Allen-Zhu, Zeyuan and Orecchia, Lorenzo. Linear Cou-
pling: An Ultimate Uniﬁcation of Gradient and Mirror
Descent. In Proceedings of the 8th Innovations in Theo-
retical Computer Science, ITCS ’17, 2017.

Allen-Zhu, Zeyuan and Yuan, Yang.

Improved SVRG
for Non-Strongly-Convex or Sum-of-Non-Convex Ob-
jectives. In ICML, 2016.

Allen-Zhu, Zeyuan, Richt´arik, Peter, Qu, Zheng, and Yuan,
Yang. Even faster accelerated coordinate descent using
non-uniform sampling. In ICML, 2016.

Arora, Sanjeev, Rao, Satish, and Vazirani, Umesh V. Ex-
pander ﬂows, geometric embeddings and graph parti-
tioning. Journal of the ACM, 56(2), 2009.

Aujol, J-F and Dossal, Ch. Stability of over-relaxations
for the forward-backward algorithm, application to ﬁsta.
SIAM Journal on Optimization, 25(4):2408–2433, 2015.

Axelsson, Owe. A survey of preconditioned iterative meth-
ods for linear systems of algebraic equations. BIT Nu-
merical Mathematics, 25(1):165–187, 1985.

Chaudhuri, Kamalika, Kakade, Sham M, Livescu, Karen,
and Sridharan, Karthik. Multi-view clustering via canon-
ical correlation analysis. In ICML, pp. 129–136, 2009.

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

Dhillon, Paramveer, Foster, Dean P, and Ungar, Lyle H.
In
Multi-view learning of word embeddings via cca.
NIPS, pp. 199–207, 2011.

Frostig, Roy, Musco, Cameron, Musco, Christopher, and
Sidford, Aaron. Principal Component Projection With-
out Principal Component Analysis. In ICML, 2016.

Garber, Dan and Hazan, Elad. Fast and simple PCA via
convex optimization. ArXiv e-prints, September 2015.

Garber, Dan, Hazan, Elad, Jin, Chi, Kakade, Sham M.,
Musco, Cameron, Netrapalli, Praneeth, and Sidford,
Aaron. Robust shift-and-invert preconditioning: Faster
and more sample efﬁcient algorithms for eigenvector
computation. In ICML, 2016.

Shalev-Shwartz, Shai and Zhang, Tong. Accelerated Prox-
imal Stochastic Dual Coordinate Ascent for Regularized
In Proceedings of the 31st Inter-
Loss Minimization.
national Conference on Machine Learning, ICML 2014,
pp. 64–72, 2014.

Shamir, Ohad. A Stochastic PCA and SVD Algorithm with
an Exponential Convergence Rate. In ICML, pp. 144—-
153, 2015.

Shewchuk, Jonathan Richard. An introduction to the conju-
gate gradient method without the agonizing pain, 1994.

Wang, Weiran and Livescu, Karen.

Large-scale ap-
proximate kernel canonical correlation analysis. arXiv
preprint, abs/1511.04773, 2015.

Ge, Rong, Jin, Chi, Kakade, Sham M., Netrapalli, Pra-
neeth, and Sidford, Aaron. Efﬁcient Algorithms for
Large-scale Generalized Eigenvector Computation and
Canonical Correlation Analysis. In ICML, 2016.

Wang, Weiran, Wang, Jialei, Garber, Dan, and Srebro,
Nathan. Efﬁcient Globally Convergent Stochastic Op-
timization for Canonical Correlation Analysis. In NIPS,
2016.

Witten, Daniela M, Tibshirani, Robert, and Hastie, Trevor.
A penalized matrix decomposition, with applications to
sparse principal components and canonical correlation
analysis. Biostatistics, pp. kxp008, 2009.

Golub, Gene H. and Van Loan, Charles F. Matrix Com-
ISBN

putations. The JHU Press, 4th edition, 2012.
1421407949.

Kakade, Sham M and Foster, Dean P. Multi-view regres-
sion via canonical correlation analysis. In Learning the-
ory, pp. 82–96. Springer, 2007.

Karampatziakis, Nikos and Mineiro, Paul. Discriminative
features via generalized eigenvectors. In ICML, pp. 494–
502, 2014.

Lu, Yichao and Foster, Dean P. Large scale canonical cor-
In NIPS,

relation analysis with iterative least squares.
pp. 91–99, 2014.

Ma, Zhuang, Lu, Yichao, and Foster, Dean. Finding linear
structure in large datasets with scalable canonical corre-
lation analysis. In ICML, pp. 169–178, 2015.

Michaeli, Tomer, Wang, Weiran, and Livescu, Karen.
arXiv

Nonparametric canonical correlation analysis.
preprint, abs/1511.04839, 2015.

Musco, Cameron and Musco, Christopher. Randomized
block krylov methods for stronger and faster approxi-
mate singular value decomposition. In NIPS, pp. 1396–
1404, 2015.

Nesterov, Yurii. A method of solving a convex program-
ming problem with convergence rate O(1/k2). In Dok-
lady AN SSSR (translated as Soviet Mathematics Dok-
lady), volume 269, pp. 543–547, 1983.

Shalev-Shwartz, Shai. SDCA without Duality, Regulariza-

tion, and Individual Convexity. In ICML, 2016.

