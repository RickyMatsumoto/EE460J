Stochastic Adaptive Quasi-Newton Methods for Minimizing Expected Values

Chaoxu Zhou ∗ 1 Wenbo Gao ∗ 1 Donald Goldfarb 1

Abstract
We propose a novel class of stochastic, adaptive
methods for minimizing self-concordant func-
tions which can be expressed as an expected
value. These methods generate an estimate of
the true objective function by taking the empir-
ical mean over a sample drawn at each step,
making the problem tractable. The use of adap-
tive step sizes eliminates the need for the user
to supply a step size. Methods in this class in-
clude extensions of gradient descent (GD) and
BFGS. We show that, given a suitable amount of
sampling, the stochastic adaptive GD attains lin-
ear convergence in expectation, and with further
sampling, the stochastic adaptive BFGS attains
R-superlinear convergence. We present experi-
ments showing that these methods compare fa-
vorably to SGD.

1. Introduction

We are concerned with minimizing functions of the form

min
x∈Rn

F (x) = Eξf (x, ξ)

(1)

Many common problems in statistics and machine learning
can be put into this form. For instance, in the empirical
risk minimization framework, a model is learned from a set
{y1, . . . , ym} of training data by minimizing an empirical
loss function of the form

min
x

L(x) =

f (x, yi)

(2)

1
m

m
(cid:88)

i=1

It is easy to see that this formulation is equivalent to taking
ξ to be the uniform distribution on the points {y1, . . . , ym}.

An objective function of the form (1) is often impractical,
as the distribution of ξ is generally unavailable, making it

*Equal contribution 1Dept. of Industrial Engineering and Op-
erations Research, Columbia University. Correspondence to:
Chaoxu Zhou <cz2364@columbia.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

infeasible to analytically compute Ef (x, ξ). This can be
resolved by replacing the expectation Ef (x, ξ) by the es-
timate (2). The strong law of large numbers implies that
the sample mean L(x) converges almost surely to F (x) as
the number of samples m increases, provided that the sam-
ples yi are drawn independently from the distribution of ξ.
However, even the concrete problem (2) is not a good target
for classical optimization algorithms, as the amount of data
m is frequently extremely large. A better strategy when op-
timizing (2) is to consider subsamples of the data to reduce
the computational cost. This leads to stochastic algorithms
where the objective function changes at each iteration by
randomly selecting subsamples.

Many stochastic algorithms have been proposed which use
this approach for solving (2), notably stochastic gradi-
ent descent (SGD) (Bottou, 2010), and variance-reduced
extensions of SGD, such as SVRG (Johnson & Zhang,
2013), SAG (Schmidt et al., 2013), and SAGA (Defazio
et al., 2014). These methods are ﬁrst-order methods, ex-
tending gradient descent to the stochastic setting, and the
latter three (variance-reduced) methods can be shown to
converge linearly for strongly convex objectives. Lin-
early convergent stochastic Limited Memory BFGS algo-
rithms (Byrd et al., 2016)(Moritz et al., 2016)(Gower et al.,
2016) have also been proposed. It is then natural to con-
sider stochastic extensions of quasi-Newton and second-
order methods. One such method, the Newton Incremen-
tal Method (NIM) (Rodomanov & Kropotov, 2016), com-
bines cyclic updating of a ﬁxed collection of functions
f (x, y1), . . . , f (x, ym) with Newton’s method, and attains
local superlinear convergence.

One of the key obstacles to developing stochastic exten-
sions of quasi-Newton methods is the necessity of select-
ing appropriate step sizes. The analysis of the global con-
vergence of the BFGS method (Powell, 1976) and other
members of Broyden’s convex class (Byrd et al., 1987)
assumes that Armijo-Wolfe inexact line search is used.
This is rather undesirable for a stochastic algorithm, as
line search is both computationally expensive and difﬁ-
cult to analyze in a probabilistic setting. However, there
is a special class of functions, the self-concordant func-
tions, whose properties allow us to compute an adaptive
step size and thereby avoid performing line searches.
In
(Gao & Goldfarb, 2016), it is shown that the BFGS method

Stochastic Adaptive Quasi-Newton Methods

(Broyden, 1967) (Fletcher, 1970)(Goldfarb, 1970)(Shanno,
1970) with adaptive step sizes converges superlinearly
when applied to self-concordant functions.

In this paper, our goal is to develop a stochastic quasi-
Newton algorithm for self-concordant functions. We pro-
pose an iterative method of the following form. At the k-th
iteration, we draw mk i.i.d samples ξ1, . . . , ξmk . Deﬁne the
empirical objective function at the k-th iteration to be

Fk(x) =

f (x, ξi)

(3)

1
mk

mk(cid:88)

i=1

Let Hk be a positive deﬁnite matrix. The next step direction
is given by

dk = −Hk∇Fk(xk)

and the step size by

tk =

αk
1 + αkδk

where

αk =

∇Fk(xk)T Hk∇Fk(xk)
δ2
k

(cid:113)

δk =

∇Fk(xk)T Hk∇2Fk(xk)Hk∇Fk(xk)

(4)

The motivation for this step size is described in Section 4.

A key feature of these methods is that the step size tk can
be computed analytically, using only local information, and
adapts itself to the local curvature. A ﬁxed step size η is
typically used in variance-reduced ﬁrst-order methods, and
this step size must be determined experimentally. The theo-
retical analysis that has been provided for these methods is
of little help in choosing η, as often η is constrained to be
impractically small, and moreover, is related to unknown
constants such as the Lipschitz parameter of the gradient.
Furthermore, a ﬁxed η which was effective in one regime
may become ineffective as the algorithm progresses, and
enters regions of varying curvature.

Our new methods are also capable of solving general prob-
lems of the form (1). This is in contrast to popular
incremental-type methods such as SAG, SAGA, and NIM,
which, because of their stored updating scheme, can only
be applied to problems of the form (2) with a ﬁxed data
set {y1, . . . , ym}. This opens up new avenues for the so-
lutions of problems where new data can be sampled as the
algorithm progresses, as opposed to having a ﬁxed training
set throughout. In this respect, our new method is akin to
Streaming SVRG (Frostig et al., 2015), which is also able
to solve (1) with similar conditions on the growth of the
samples mk.

1. Taking Hk = I yields the stochastic adaptive gradient

descent method (SA-GD).

2. Fixing H0, and then taking Hk+1 to be the BFGS
update of Hk, yields the stochastic adaptive BFGS
method (SA-BFGS).

Our methods can be proven to converge under a suitable
sampling regime. When the number of samples mk is ﬁxed,
and large enough, both SA-GD and SA-BFGS converge R-
linearly in expectation to an (cid:15)-optimal solution. To obtain
R-superlinear convergence, the number of samples must
be allowed to grow rapidly. This principle, of increasing
the number of samples to match the desired rate of conver-
gence, was previously applied to R-linear convergence in
(Byrd et al., 2012). The SA-BFGS algorithm can be shown
to converge R-superlinearly almost surely to the true opti-
mal solution if the number of samples is increased so that
m−1
k

converges R-superlinearly to 0.

This paper is organized as follows. In Section 2, we in-
troduce our notation, and the technical assumptions needed
for the analysis of our methods. In Section 3, we describe
the relevant results from stochastic analysis which moti-
vate our algorithms. In Section 4, we brieﬂy describe the
required theory of self-concordant functions. In Section 5,
we formally deﬁne stochastic adaptive methods, and state
the convergence results. In Section 6, we present prelim-
inary numerical experiments, and conclude with a discus-
sion in Section 7. Full proofs of the convergence theorems
can be found in the supplementary materials.

2. Assumptions and Notation

The number of variables is n. We write g(x) = ∇F (x)
and G(x) = ∇2F (x), and gk(x) = ∇Fk(x) and Gk(x) =
∇2Fk(x) for the gradient and Hessian of the empirical ob-
jective function.
In the context of a sequence of iterates
{xk}∞
k=0 generated by an algorithm, we also write gk with
no argument to denote gk(xk), and Gk for Gk(xk). In the
context of BFGS, Hk denotes the approximation of the in-
verse Hessian, and Bk = H −1
k . The step direction (gener-
ally −Hkgk) is denoted dk, and the step size by tk, so the
actual step taken is sk = tkdk.

The optimal solution of min
x∈Rn

F (x) is denoted x∗, and the

optimal solution of the empirical problem min
x∈Rn
denoted x∗

k is a random variable.

k. Note that x∗

Fk(x) is

Unless otherwise speciﬁed, the norm (cid:107) · (cid:107) is the 2-norm, or
the operator 2-norm. The Frobenius norm is indicated as
(cid:107) · (cid:107)F .

By choosing the matrices Hk appropriately, we obtain
stochastic extensions of classical methods.
In particular,
two choices of Hk will be of interest:

We make the following technical assumptions on F (x) and
f (x, ξ). We will explain the motivation for these assump-
tions at the relevant points in the discussion.

Stochastic Adaptive Quasi-Newton Methods

Assumptions:

1. There exist constants L ≥ (cid:96) > 0 such that for every
x ∈ Rn and every realization of ξ, the Hessian of f
with respect to x satisﬁes

(cid:96)I (cid:22) ∇2

xf (x, ξ) (cid:22) LI

That is, f (x, ξ) is strongly convex for all ξ, with the
eigenvalues of ∇2
xf (x, ξ) bounded by (cid:96) and L. The
condition number, denoted κ, is given by L/(cid:96).

2. Fk(x) is standard self-concordant for every possible

sampling ξ1, . . . , ξmk .

3. There exist compact sets D0 and D with x∗ ∈ D and
D0 ⊆ D, such that if x0 is chosen in D0, then for
all possible realizations of the samples ξ1, . . . , ξmk for
every k, the sequence of iterates {xk}∞
k=0 produced
by the algorithm is contained within D. We use D =
sup{(cid:107)x − y(cid:107) : x, y ∈ D} for the diameter of D.

Furthermore, we assume that the objective values and
gradients are bounded:

u = sup

f (x, ξ) < ∞

l = inf
ξ

inf
x∈D

f (x, ξ) > −∞

γ = sup

(cid:107)∇f (x, ξ)(cid:107) < ∞

sup
x∈D

sup
x∈D

ξ

ξ

4. (For BFGS only) The Hessian G(x) is Lipschitz con-

tinuous with constant LH .

Note that Assumption 1 is standard when analyzing
stochastic algorithms. The function f (x, ξ) is commonly
a loss function, and either f (x, ξ) is itself strongly convex,
or each f (x, ξ) is weakly convex and the strong convex-
ity is ensured by adding a quadratic regularization term to
F (x).

3. Stochastic Framework

Our analysis is based on a uniform convergence law, a
standard technique in learning theory and empirical pro-
cesses. The central idea is that Fk(x) is an empirical
mean estimating F (x), and we can closely control the er-
ror |Fk(x) − F (x)| over all x ∈ D by varying the sample
size mk. The relevant stochastic analysis can be found in
(W. van der Vaart & Wellner, 1996) and (Goldfarb et al.,
2017). These powerful techniques are required to analyze
second-order stochastic methods, since inverting the prod-
uct of a sampled Hessian and sampled gradient generates
both dependence and non-linearity.
In comparison, ﬁrst-
order stochastic methods can be analyzed by evaluating the
It
expected value and variance of the sampled gradient.

is also useful to compare this approach with that used in
(Byrd et al., 2012) and (Friedlander & Goh, 2013), where
the variance of the gradient is controlled by pointwise esti-
mates or pointwise tail bounds.

Theorem 3.1 (Corollary 1, (Goldfarb et al., 2017)). Let
δ > 0 and 0 < (cid:15) < min{D, δ

2L }. Then

P(sup
x∈D

|Fk(x)−F (x)| > δ) ≤ c((cid:15)) exp

−

(cid:18)

mk(δ − 2L(cid:15))2
2(u − l)2

(cid:19)

where c((cid:15)) = 2nn/2Dn(cid:15)−n. If mk ≥ 3, then

E sup
x∈D

|Fk(x) − F (x)| ≤ C

E|Fk(x∗

k) − F (x∗)| ≤ C

(cid:114) log mk
mk
(cid:114) log mk
mk

where C is a constant (depending only on F ) given by

4(|u|+|l|)nn/2Dn exp

−n

log

(cid:20)

(cid:18)

(cid:19)(cid:21)

u − l
√
2L
2

√

+(u−l)

n + 1

Assumption 3 is required for the uniform law of Theo-
rem 3.1 to hold. The set D0 is assumed to be a region
where, if x0 is chosen within D0, the path of the stochastic
algorithm will remain within the larger set D. For practical
purposes, we may take D to be an arbitrarily large bounded
set in order to ensure convergence, though this worsens the
complexity bounds provided by Theorem 3.1. We note that
the constant C is very much a ‘worst-case’ bound, and for
almost every problem arising in practice, the expected dif-
ference will be much smaller than Theorem 3.1 suggests.
Rather, the crucial implication is that the expected differ-

(cid:113) log mk
mk

, allowing a sharp level

ence diminishes at the rate
of control by adjusting mk.

4. Self-Concordant Functions and Adaptive

Methods

A convex function f : Rn → R is self-concordant if there
exists a constant κ such that for every x ∈ Rn and every
h ∈ Rn, we have

|∇3f (x)[h, h, h]| ≤ κ(∇2f (x)[h, h])3/2

If κ = 2, f is standard self-concordant. Self-concordant
functions were introduced by Nesterov and Nemirovski
in the context of interior point methods (Nesterov & Ne-
mirovski, 1994).

Many common problems have self-concordant formula-
tions. At least one method, the DiSCO algorithm of Zhang
and Xiao (2015), is tailored for distributed self-concordant
optimization and has been applied to many regression prob-
lems. Convex quadratic objective functions have third

Stochastic Adaptive Quasi-Newton Methods

derivatives equal to zero, and are therefore trivially stan-
In particular, least squares regres-
dard self-concordant.
sion is self-concordant. In (Zhang & Xiao, 2015), it is also
shown that regularized regression, with either logistic loss
or hinge loss, is self-concordant.

For self-concordant functions, the notion of a local norm
is especially useful. Given a convex function f , the local
norm with respect to f at the point x is given by

(cid:113)

(cid:107)h(cid:107)x =

hT ∇2f (x)h

Consider an iterative method for minimizing self-
concordant functions with steps given as follows. On the k-
th step, the step direction dk is given by dk = −Hk∇f (xk)
for some positive deﬁnite matrix Hk, and the step size is
given by tk = αk
, where δk = (cid:107)dk(cid:107)xk and αk =
gT
k Hkgk
δ2
k

1+αkδk

.

Methods of this type have been analyzed in (Tran-Dinh
et al., 2015) and (Gao & Goldfarb, 2016). In (Gao & Gold-
farb, 2016), the above choice of αk is shown to guarantees
a decrease in the function value.
Theorem 4.1 (Lemma 4.1, (Gao & Goldfarb, 2016)). Let
ρk = ∇f (xk)T Hk∇f (xk). If αk is chosen to be αk = ρk
,
δ2
k
then

f (xk + tkdk) ≤ f (xk) − ω(ηk),

where ηk = ρk
δk

and the function ω(z) = z − log(1 + z).

We make Assumption 2 in order to apply Theorem 4.1 to
the empirical objective functions Fk(x). A natural question
is whether Assumption 2 can be relaxed to the assumption
that f (x, ξ) is self-concordant for all ξ, and F (x) is stan-
dard self-concordant. In the case where ξ is ﬁnitely sup-
ported, it is possible to scale F (x) so that we may use this
weaker assumption.
Lemma 4.2. Suppose that ξ is ﬁnitely supported on
{a1, . . . , am}, with pi = P(ξ = ai). Suppose that f (x, ai)
is self-concordant with constant κi. Let θ = 1
. Then
4
the scaled function F (x) = E[θf (x, ξ)] is standard self-
concordant, and every empirical objective function F k(x)
is standard self-concordant.

max κ2
i
min pi

i

Proof. Observe that θf (x, ai)pi is self-concordant with
constant θ−1/2p−1/2
κi ≤ 2. We deduce that EF (x) =
(cid:80)m
i=1 θf (x, ai)pi is standard self-concordant. Further-
i=1 pi = 1, we have min pi ≤
more,
1
m . Thus, 1
m θf (x, ai) is self-concordant with constant
θ−1/2m−1/2κi ≤ 2, which implies that F k(x) is standard
self-concordant for every possible F k(x).

since (cid:80)m

However, if we do not assume that each f (x, ξ) is self-
concordant, or if ξ is not compactly supported, it is un-

Algorithm 1 SA-GD

Input: x0, {m0, m1, . . .}
for k = 0, 1, 2, . . . do

Sample ξ1, . . . , ξmk i.i.d from ξ
Compute:

gk = ∇Fk(xk), dk = −gk,

(cid:113)

δk =

gT
k Gk(xk)gk,

αk =

gT
k gk
δ2
k

,

tk =

αk
1 + αkδk

,

where Fk(x) = 1
mk
Set xk+1 = xk + tkgk.

(cid:80)mk

i=1 f (x, ξi).

end for

clear whether every possible Fk(x) will be standard self-
concordant, even when F (x) is standard self-concordant.
Thus, we impose Assumption 2 in our analysis, while ob-
serving that it is unnecessary in the practical case where ξ
is derived from a ﬁnite data set.

5. Stochastic Adaptive Methods

Our basic approach is to sample an empirical objective
function Fk(x) at each step, and then compute the step di-
rection and adaptive step size (4) using Fk. For particular
choices of the matrices Hk, we recover analogues of classi-
cal methods such as gradient descent, L-BFGS, and BFGS.

5.1. Stochastic Adaptive GD

When Hk = I for every k, the resulting method is stochas-
tic adaptive gradient descent (SA-GD), which is given as
Algorithm 1. The number of samples drawn at each itera-
tion is left as an input to the algorithm, and (weak) bounds
on the required number mk can be inferred from the con-
vergence analysis.

Theorem 5.1. Let (cid:15) > 0. Suppose that at each iteration,
we draw m i.i.d samples. We may choose k = (cid:101)O(log (cid:15)−1)
and m = (cid:101)O((cid:15)−2 log (cid:15)−1) so that the SA-GD method con-
verges in expectation to an (cid:15)-optimal solution after k steps.
Here, (cid:101)O(·) absorbs constants depending only on F , namely
C and κ.

A more precise quantitative form of Theorem 5.1, and its
proof, appear as Theorem B.1 in the supplementary ma-
terials. The argument can be sketched as follows. The-
orem 4.1 implies that the adaptive step size tk produces
a linear decrease towards the optimal value of Fk; that is,
Fk(xk+1)−Fk(x∗
k) ≤ r(Fk(xk)−Fk(x∗
k)). Here r is a de-
terministic constant depending only on (cid:96), L, and γ. By iter-
ating, we can bound the true gap F (xk+1)−F (x∗) in terms

Stochastic Adaptive Quasi-Newton Methods

Algorithm 2 SA-BFGS

Input: x0, H0, {m0, m1, . . .}, β < 1
for k = 0, 1, 2, . . . do

Sample ξ1, . . . , ξmk i.i.d from ξ
Compute

gk = ∇Fk(xk), dk = −Hkgk

(cid:113)

δk =

dT
k Gk(xk)dk,

αk =

gT
k Hkgk
δ2
k

,

tk =

αk
1 + αkδk

,

i=1 f (x, ξi).

(cid:80)mk

where Fk(x) = 1
mk
Set gk+1 = ∇Fk(xk + tkdk)
Set yk = gk+1 − gk
k+1dk < βgT
if gT
Set dk = gk
Recompute δk, αk, tk
Set Hk+1 = Hk

k dk then

else

Set Hk+1 = (I − skyT
k
sT
k yk

)Hk(I − yksT
k
sT
k yk

) + sksT
k
sT
k yk

end if
Set xk+1 = xk + tkdk.

end for

of rk(F0(x1) − F0(x∗

0)) and rj sup
x∈D
for 1 ≤ j ≤ k. The ﬁrst term can be made smaller than (cid:15) by
taking k to be O(log (cid:15)−1), and the second can be bounded
with Theorem 3.1.

|Fk+1−j(x) − F (x)|

5.2. Stochastic Adaptive BFGS

By updating Hk using the BFGS formula, we obtain the
stochastic adaptive BFGS (SA-BFGS) method, which is
given in Algorithm 2.

In Algorithm 2, we use the standard BFGS update with
yk = gk(xk+1) − gk(xk). Another option is to replace yk
with the action of the Hessian on sk, so yk = Gk(xk)sk.
In general, we must compute Gk(xk)dk when ﬁnding the
adaptive step size, so we can re-use the result of that com-
putation instead of computing an extra gradient gk(xk+1).

For technical reasons, our SA-BFGS procedure tests
whether the Wolfe condition is satisﬁed for the adaptive
step size tk. If not, we revert to taking a SA-GD step. This
is an artifact of our analysis, and under suitable conditions
on the growth of the samples mk, there will be some point
after which the Wolfe condition is necessarily satisﬁed on
every step. In practice, this test can be omitted.

There are also two possible ways to implement SA-BFGS.
For problems with n at most medium-sized, it is possible to
explicitly store the matrix Hk, and compute dk by a matrix

product −Hkgk. For n very large, it is infeasible to store
Hk, and we can instead store the pairs (sk, yk) and compute
−Hkgk using a two-loop recursion (Nocedal, 1980). This
corresponds to stochastic adaptive L-BFGS (SA-LBFGS)
if we limit the number of past pairs (sk, yk) to only the h
most recent, and to SA-BFGS if we store everything. The
amount of storage used by SA-LBFGS surpasses that of
SA-BFGS as h approaches n.

Theorem 5.1 holds for SA-LBFGS, since Theorem B.1
holds for any method where {Hk} has uniformly bounded
eigenvalues. Thus, SA-LBFGS also converges in expecta-
tion to an (cid:15)-optimal solution after k = (cid:101)O(log (cid:15)−1) steps
given samples of size m = (cid:101)O((cid:15)−2 log (cid:15)−1), though now
the constants within the big-O are also dependent on h.

As with SA-GD, one can obtain an (cid:15)-optimal solution in
expectation from SA-BFGS with a ﬁxed amount of sam-
pling:

Theorem 5.2. Let (cid:15) > 0. Suppose that at each iteration,
we draw m i.i.d samples. We may choose k = (cid:101)O(log (cid:15)−1)
and m = (cid:101)O((cid:15)−2(log (cid:15)−1)3) so that the SA-BFGS method
converges in expectation to an (cid:15)-optimal solution after k
steps. Here, (cid:101)O(·) absorbs constants depending only on F ,
namely C and κ.

This theorem follows from Lemma C.9 in the supplemen-
tary materials. Note that the complexity bound is in fact
weaker than that of SA-GD. This occurs because the ini-
tial matrices H0, . . . , Hk may be poor approximations of
(∇2F (x))−1, in which case the BFGS directions may per-
form poorly. We have little theoretical control over Hk ex-
cept in the limit, and this is reﬂected in the weaker iteration
bounds for (cid:15)-optimality.

For convergence to the true optimal solution x∗, it is nec-
essary for the number of samples mk to increase over time.
In fact, by increasing the number of samples appropriately,
the SA-BFGS algorithm can be shown to converge to x∗
R-superlinearly with probability 1.
Theorem 5.3. Suppose that we draw mk samples on the k-
th step, where m−1
converges R-superlinearly to 0. Then
the SA-BFGS algorithm converges R-superlinearly to the
optimal solution x∗ almost surely.

k

The complete proof is left to Lemma C.1 in the supplemen-
tary materials. We brieﬂy sketch it here.

Taking mk to be an increasing sequence, we can guarantee
that the sequence {ηk = ρk
} converges to 0. From basic
δk
results on the adaptive step size, this implies that tk even-
tually satisﬁes the Armijo-Wolfe conditions, and therefore
the algorithm eventually uses the BFGS direction, and per-
forms a BFGS update on Hk, at every step. Our test for
the Wolfe condition ensures that in every iteration, we ei-
ther can control the progress by analyzing the BFGS up-

Stochastic Adaptive Quasi-Newton Methods

date Hk+1, or by using our earlier results on SA-GD steps.
Together, these facts imply that SA-BFGS converges R-
linearly almost surely, and consequently, the sum of dis-
tances (cid:80)∞
k=0 (cid:107)xk − x∗(cid:107) is ﬁnite almost surely.
Following the classical argument, we analyze the evolution
of Hk+1 to show that the steps taken by SA-BFGS con-
verge to the Newton step. This is precisely the well-known
Dennis-Mor´e condition (Dennis Jr. & Mor´e, 1974) for Q-
superlinear convergence; however, in the stochastic setting,
we have an additional error term resulting from the vari-
ance in the sampling of the gradients and Hessians. We
apply Theorem 3.1 to show that gk(x) and G(x) rapidly
converge to the true gradient and Hessian, and thus SA-
BFGS attains R-superlinear convergence.

Increasing mk at this rate is clearly infeasible in reality,
and it is non-trivial to determine a level of sampling which
produces (iteration-wise) superlinear convergence in a rea-
sonable amount of real time. We note, for comparison, that
the Streaming SVRG method (Frostig et al., 2015) requires
sample sizes for the gradient that grow at a geometric rate,
and that the dynamic batch method (Byrd et al., 2012) ob-
tains R-linear convergence in expectation by increasing the
sample sizes at a geometric rate. A superlinear increase in
the number of samples may be unavoidable as a condition
for superlinear convergence, given the statistical error of
the sample.

In practice, we are often uninterested in ﬁnding the true
minimizer, and instead aim to quickly ﬁnd an approximate
solution. The theoretical R-superlinear rate of SA-BFGS
suggests that SA-BFGS or SA-LBFGS may offer practical
speedups over ﬁrst-order stochastic methods, even without
using substantial sampling. One intuitive heuristic would
be to draw fewer samples in the early phase of the algo-
rithm, when computing the gradient with high accuracy is
less valuable, and then increasing the number of samples as
the solutions approach optimality to reduce ﬂuctuation.

6. Numerical Experiments

We compared several implementations of SA-GD and SA-
BFGS against the original SGD method and the robust
SGD method (Nemirovski et al., 2009) on a penalized least
squares problem with random design. That is, the objective
function has the form

L(w) = E(Y − X T w)2 +

min
w∈Rp

1
2

(cid:107)w(cid:107)2
2

where X ∈ Rp and Y ∈ R are random variables with ﬁnite
second moments. We base our model on a standard linear
regression problem with Gaussian errors, so Y = X T β + (cid:15)
for a deterministic vector β ∈ Rp and (cid:15) ∼ N (0, 1) is a
noise component. X was drawn according to a multivariate
N (0, Σ(ρ)), where Σ(ρ) = (1 − ρ2)Ip + ρ2J (here J is the

all-ones matrix). By varying ρ, we control the condition
number of the expected Hessian. We tested problems of
size p = 100 and p = 500.

The following eight algorithms were tested:

SGD: SGD with ﬁxed mk = p and diminishing step sizes
k+1000 for problems with p = 100, and tk =

1

tk =
1
k+5000 for p = 500.

SA-GD: SA-GD with ﬁxed mk = p.

SA-GD-I: SA-GD with increasing samples mk = 1

2 p +

1.01k.

SA-BFGS: SA-BFGS with ﬁxed mk = p. We do not
test for the Wolfe condition at each step, and instead
always take the adaptive BFGS step and perform a
BFGS update.

1

SA-BFGS-I: SA-BFGS with increasing samples mk =
2 p + 1.01k. We do not test for the Wolfe condition at
each step, and instead always take the adaptive BFGS
step and perform a BFGS update.

1

SA-BFGS-GD: SA-BFGS with increasing samples mk =
2 p+1.01k. We test for the Wolfe condition and switch
to taking a SA-GD step when the adaptive step tk for
SA-BFGS fails the test.

1√
N

R-S-GD-C: Robust SGD with constant step size tk =
, where N is the pre-determined number of steps.
At the k-th iteration, output the average of the previ-
ous k/2 solutions.

R-S-GD-V: Robust SGD with diminishing step size tk =
. At the k-th iteration, output the average of the

1√
k

previous k/2 solutions.

We also tested the Streaming SVRG algorithm with the
hyperparameters speciﬁed in Corollary 4 (Frostig et al.,
2015). However, this algorithm was difﬁcult to tune and
performed poorly, so we have omitted it from the ﬁgures.

The algorithms preﬁxed by “SA-” used the adaptive step
size. For SGD, we followed the standard practice of us-
ing a diminishing and non-summable step size tk = a
k+b .
The values a = 1, b = {1000, 5000} in our test were cho-
sen experimentally. The SA-BFGS algorithms were imple-
mented using a two-loop recursion to compute Hkgk when
p = 500. This signiﬁcantly improved their performance
compared to storing the matrix Hk explicitly.

Figure 1 shows the performance of each algorithm on a se-
ries of problems with varying problem size p and parame-
ter ρ. The y-axis measures the gap log(f (x(t)) − f (x∗))
of the solution x(t) obtained by the algorithm after using t

Stochastic Adaptive Quasi-Newton Methods

seconds of CPU time. The algorithms were implemented in
Matlab 2015a, and the system was an Intel i5-5200U run-
ning Ubuntu.

The increasing sample sizes used in SA-GD-I do not appear
to reduce its variance, compared to SA-GD with mk ﬁxed,
which goes against our initial expectations.
In plots (a),
(b), (e), and (f), SA-GD-I appears to exhibit the same ﬂuc-
tuations as SA-GD when the points approach optimality.
In plot (c), SA-GD-I brieﬂy surpasses SA-GD before the
objective value jumps again. It is only in plot (d) that SA-
GD-I appears to descend more consistently than SA-GD.
This suggests that, for these problem instances, the rate of
sampling mk = O(1.01k) could even be increased further,
and that the tradeoff between taking more steps, versus tak-
ing fewer steps with more accurate estimates, favors more
accurate steps.

Likewise, SA-BFGS-I failed to consistently perform bet-
ter than SA-BFGS with mk ﬁxed. SA-BFGS-I seemed to
perform relatively better when the dimension was larger,
whereas SA-BFGS was faster when p = 100. One notable
example was plot (f), where SA-BFGS initially descends
rapidly before abruptly stalling, and is quickly surpassed
by SA-BFGS-I. The data going beyond 60 seconds of CPU
time veriﬁed that SA-BFGS continued to stall, and never
managed to obtain a log-error of less than 1. Our inter-
pretation is that for this problem, the ﬁxed sample size is
too small to obtain an accurate solution.
It also appears
that SA-BFGS requires larger samples to obtain a stable
solution when the problem is particularly ill-conditioned.
In the plots (e), and particularly (f), where ρ = 0.9, SA-
BFGS rapidly reaches the point where variance in the gra-
dient estimates introduces too much noise to make further
progress.

What is also interesting is that SA-GD often outperforms
SA-BFGS if both algorithms use the same ﬁxed sample
size. While somewhat disappointing, there is a natural rea-
son for this. BFGS optimizes a local quadratic model of the
objective, and is performant when its approximation Hk
resembles the true Hessian. The Hessian exhibits greater
variance than the gradient, simply by virtue of having n2
components compared to n, and we generally expect that
more sampling is needed to accurately estimate the Hes-
sian. With the same amount of sampling, SA-BFGS is
therefore ‘noisier’ than SA-GD relative to the true function.

We also see this reﬂected in the performance of SA-BFGS-
GD, which was able to outperform SA-BFGS and SA-
BFGS-I when p = 100 despite the additional cost of check-
ing the Wolfe condition. For p = 500, it is less clear
whether a better strategy is simply to perform SA-BFGS-
I. One strategy that might be effective is a ﬁxed pattern of
‘switching’ between GD and BFGS steps, with a greater
number of GD steps in the early phase.

Predictably, robust SGD was the most stable method (given
the lack of variance-reduction in the other methods). Both
R-S-GD-C and R-S-GD-V exhibited almost no ﬂuctua-
tions, while improving at a reasonable rate. The R-S-GD-C
method with a constant step size outperformed R-S-GD-V,
which we ﬁnd counterintuitive given that R-S-GD-C used a
smaller step size than R-S-GD-V. This method of variance-
reduction by averaging the solutions seems to be effective
and can be applied to any iterative method.

Numerical results for ERM (empirical risk minimization)
problems can be found in section D in the supplementary
material.

7. Discussion

For self-concordant objective functions, adaptive methods
eliminate the need to choose a step size, which is a big
advantage. This is independent of a new difﬁculty which
arises for stochastic methods, which is the choice of the
sample size mk. From our experiments, we see that choos-
ing mk appropriately is crucial. Unlike SGD and SVRG,
where it is often most effective to use mini-batches of size
1, SA-BFGS and SA-GD work best with comparatively
larger samples.

Note that SA-GD and SA-BFGS are not purely ﬁrst-order
methods, as computing the adaptive step size requires cal-
culating the Hessian-vector product Gk(xk)dk. However,
it is often possible to calculate Hessian-vector products efﬁ-
ciently, and at far less cost than computing the full Hessian
∇2Fk(x). Consider, for example, a a logistic regression
problem where the empirical loss function is

L(x) =

log(1 + e−yiaT

i x)

1
m

m
(cid:88)

i=1

i x/(1 + e−yiaT

for sampled data {(ai, yi) : ai ∈ Rn, yi ∈ {−1, 1}}.
Let A denote the n × m matrix with columns ai. The
gradient is given by AT β, where β is the vector βi =
−yie−yiaT
i x), and the Hessian is given
by ABAT , where B is the diagonal matrix with entries
Bii = e−yiaT
i x/(1 + e−yiaT
i x)2. To compute the product
dT ∇2L(x)d, it sufﬁces to compute (cid:80)m
i d)2, and
i=1 Bii(aT
this requires approximately the same number of arithmetic
operations as computing ∇L(x). Thus, computing δk for
the adaptive step size requires roughly the same amount of
work as one additional gradient calculation. Also, as men-
tioned in Section 5, we may replace the BFGS update in
SA-BFGS with a modiﬁed update using the Hessian action
yk = Gk(xk)dk to save effort.

This work represents only a preliminary step in the devel-
opment of stochastic quasi-Newton methods. There are
several key questions that remain open:

Stochastic Adaptive Quasi-Newton Methods

(a) ρ = 0, p = 100

(b) ρ = 0, p = 500

(c) ρ = 0.5, p = 100

(d) ρ = 0.5, p = 500

(e) ρ = 0.9, p = 100

(f) ρ = 0.9, p = 500

Figure 1. Experimental results for p = 100, 500 and varying ρ. The x-axis is the elapsed CPU time and the y-axis is log(f (x) − f (x∗)).

1. Theorem 5.3 partially resolves a question posed by
Moritz et al.
(Moritz et al., 2016), by proving su-
perlinear convergence of a stochastic algorithm under
rather restrictive conditions. It would be strengthened
greatly if we could prove superlinear convergence un-
der weaker conditions on mk.

2. The theory developed for adaptive step sizes only
applies to self-concordant functions, but the adap-
tive step size itself can be interpreted for non-self-
concordant functions, as an adjustment based on the
local curvature. It would be of great interest to extend
adaptive methods to general convex functions, thereby
replacing both inexact line search and ﬁxed step sizes
on a large class of problems.

3. How can variance-reduction be applied to SA-GD and
SA-BFGS? The control variates used in SVRG are ef-
fective, though costly, and perhaps difﬁcult to com-
pute for functions of the form Ef (x, ξ). Another pos-
sible technique is to output an average of the solutions,
as is done by robust SGD, though this introduces an
additional hyperparameter (the number of past solu-
tions to average). Better variance-reduction strategies
might compensate for noisy estimates, allowing us to
reduce the number of samples needed in practice.

4. What heuristics can be developed for the sample sizes
mk to improve the stability and speed up performance
of SA-GD and SA-BFGS?

CPU time(s)00.10.20.30.40.50.60.70.80.91log(f(xk) - f(x*))-2-10123456ρ = 0, p = 100CPU time(s)0102030405060log(f(xk) - f(x*))-2-1012345678ρ = 0, p = 500CPU time(s)00.10.20.30.40.50.60.70.80.91log(f(xk) - f(x*))-3-2-10123456ρ = 0.5, p = 100CPU time(s)0102030405060log(f(xk) - f(x*))-2-1012345678ρ = 0.5, p = 500CPU time(s)00.10.20.30.40.50.60.70.80.91log(f(xk) - f(x*))-3-2-10123456ρ = 0.9, p = 100CPU time(s)0102030405060log(f(xk) - f(x*))-3-2-101234567ρ = 0.9, p = 500S-GDSA-GDSA-GD-ISA-BFGSSA-BFGS-ISA-BFGS-GDR-S-GD-CR-S-GD-VStochastic Adaptive Quasi-Newton Methods

References

Bottou, L´eon. Large-scale machine learning with stochas-
In Proceedings of COMP-

tic gradient descent.
STAT’2010, pp. 177–186. Physica-Verlag HD, 2010.

Broyden, Charles G. Quasi-Newton methods and their ap-
plication to function minimisation. Mathematics of Com-
putation, 21(99):368–381, 1967.

Byrd, Richard H., Nocedal, Jorge, and Yuan, Ya-Xiang.
Global convergence of a class of quasi-Newton methods
on convex problems. Siam. J. Numer. Anal., (5):1171–
1190, 1987.

Byrd, Richard H, Chin, Gillian M, Nocedal, Jorge, and Wu,
Yuchen. Sample size selection in optimization methods
for machine learning. Mathematical Programming, 134
(1):127–155, 2012.

Byrd, Richard H., Hansen, S. L., Nocedal, Jorge, and
Singer, Yoram. A stochastic quasi-newton method for
large-scale optimization. SIAM Journal on Optimization,
26(2):1008–1031, 2016.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.
Saga: A fast incremental gradient method with support
In Ad-
for non-strongly convex composite objectives.
vances in Neural Information Processing Systems, pp.
1646–1654, 2014.

Dennis Jr., John E. and Mor´e, Jorge J. Characteriza-
tion of superlinear convergence and its application to
quasi-Newton methods. Math. Comp., 28(106):549–560,
1974.

Fletcher, Roger. A new approach to variable metric algo-
rithms. The Computer Journal, 13(3):317–322, 1970.

Friedlander, Michael P and Goh, Gabriel.

bounds for stochastic approximation.
arXiv:1304.5586, 2013.

Tail
arXiv preprint

Frostig, Roy, Ge, Rong, Kakade, Sham M, and Sidford,
Aaron. Competing with the empirical risk minimizer in
a single pass. In COLT, pp. 728–763, 2015.

Gao, Wenbo and Goldfarb, Donald. Quasi-Newton meth-
ods: Superlinear convergence without line search for
self-concordant functions. in review. arXiv:1612.06965,
2016.

Goldfarb, Donald. A family of variable-metric methods
derived by variational means. Mathematics of Computa-
tion, 24(109):23–26, 1970.

Goldfarb, Donald, Iyengar, Garud, and Zhou, Chaoxu. Lin-
ear convergence of stochastic Frank-Wolfe variants. In
Proceedings of the 20th International Conference on Ar-
tiﬁcial Intelligence and Statistics, pp. 1066–1074, 2017.

Gower, Robert, Goldfarb, Donald, and Richt´arik, Peter.
Stochastic block BFGS : Squeezing more curvature out
of data. In Proceedings of the 33rd International Con-
ference on Machine Learning, pp. 1869–1878, 2016.

Johnson, Rie and Zhang, Tong. Accelerating stochastic
gradient descent using predictive variance reduction. In
Advances in Neural Information Processing Systems, pp.
315–323, 2013.

Moritz, Philipp, Nishihara, Robert, and Jordan, Michael I.
A linearly-convergent stochastic L-BFGS algorithm. In
Proceedings of the Nineteenth International Conference
on Artiﬁcial Intelligence and Statistics, pp. 249–258,
2016.

Nemirovski, Arkadi, Juditsky, Anatoli, Lan, Guanghui, and
Shapiro, Alexander. Robust stochastic approximation
approach to stochastic programming. SIAM Journal on
Optimization, 19(4):1574–1609, 2009.

Nesterov, Yurii and Nemirovski, Arkadi.

Interior-point
polynomial algorithms in convex programming. Society
for Industrial and Applied Mathematics, Philadelphia,
1994.

Nocedal, Jorge. Updating quasi-Newton matrices with lim-
ited storage. Math. Comp., 35(151):773–782, 1980.

Powell, Michael J. D. Some global convergence proper-
ties of a variable metric algorithm for minimization with-
out exact line searches. In Cottle, Richard and Lemke,
C.E. (eds.), Nonlinear Programming, volume IX. SIAM-
AMS Proceedings, 1976.

Rodomanov, Anton and Kropotov, Dmitry.

A
superlinearly-convergent proximal newton-type method
for the optimization of ﬁnite sums. In Proceedings of the
33rd International Conference on Machine Learning,
pp. 2597–2605, 2016.

Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Min-
imizing ﬁnite sums with the stochastic average gradient.
Mathematical Programming, pp. 1–30, 2013.

Shanno, David F. Conditioning of quasi-Newton methods
for function minimization. Mathematics of Computa-
tion, 24(111):647–656, 1970.

Tran-Dinh, Quoc, Kyrillidis, Anastasios, and Cevher,
Volkan. Composite self-concordant minimization. Jour-
nal of Machine Learning Research, 16(Mar):371–416,
2015.

W. van der Vaart, Aad. and Wellner, Jon A. Weak Con-
vergence and Empirical Processes. Springer New York,
1996.

Stochastic Adaptive Quasi-Newton Methods

Zhang, Yuchen and Xiao, Lin. Disco: Distributed opti-
In JMLR:
mization for self-concordant empirical loss.
Workshop and Conference Proceedings, volume 32, pp.
362–370, 2015.

