Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models
with KL-control

Natasha Jaques 1 2 Shixiang Gu 1 3 4 Dzmitry Bahdanau 1 5 Jos´e Miguel Hern´andez-Lobato 3
Richard E. Turner 3 Douglas Eck 1

Abstract
This paper proposes a general method for im-
proving the structure and quality of sequences
generated by a recurrent neural network (RNN),
while maintaining information originally learned
from data, as well as sample diversity. An RNN
is ﬁrst pre-trained on data using maximum likeli-
hood estimation (MLE), and the probability dis-
tribution over the next token in the sequence
learned by this model is treated as a prior pol-
icy. Another RNN is then trained using reinforce-
ment learning (RL) to generate higher-quality
outputs that account for domain-speciﬁc incen-
tives while retaining proximity to the prior pol-
icy of the MLE RNN. To formalize this objec-
tive, we derive novel off-policy RL methods for
RNNs from KL-control. The effectiveness of the
approach is demonstrated on two applications; 1)
generating novel musical melodies, and 2) com-
putational molecular generation. For both prob-
lems, we show that the proposed method im-
proves the desired properties and structure of the
generated sequences, while maintaining informa-
tion learned from data.

1. Introduction

The approach of training sequence generation models using
likelihood maximization suffers from known failure modes,
and it is notoriously difﬁcult to ensure multi-step generated
sequences have coherent global structure. For example,
long short-term memory (LSTM) (Hochreiter & Schmid-
huber, 1997) networks trained to predict the next charac-
ter in sequences of text may produce text that has correct

1Google Brain, Mountain View, USA 2Massachusetts Insti-
tute of Technology, Cambridge, USA 3University of Cambridge,
Cambridge, UK 4Max Planck Institute for Intelligent Systems,
Stuttgart, Germany 5Universit´e de Montr´eal, Montr´eal, Canada.
Correspondence to: Natasha Jaques <jaquesn@mit.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

spelling, punctuation, and even a semblance of grammar,
but the generated text shifts so rapidly from topic to topic,
that it is almost completely nonsensical (see (Graves, 2013)
for an example). Similar networks trained to predict the
next note in a melody suffer from the same problem; the
generated music has no consistent theme or structure, and
appears wandering and random. In addition, these models
are prone to excessively repeating the same output token, a
problem that has also been noted in the context of recurrent
dialog generation models (Li et al., 2016).

To ameliorate these problems we propose Sequence Tutor,
a novel approach which uses RL to impose structure on a
sequence generation RNN via task-speciﬁc rewards, while
simultaneously ensuring that information learned from data
is retained. This is accomplished by maintaining a ﬁxed
copy of a sequence generation RNN pre-trained on data,
which is termed the Reward RNN. Rather than simply us-
ing the Reward RNN to supply part of the rewards to
our model, we derive novel off-policy RL methods for se-
quence generation from KL-control that allow us to directly
penalize Kullback Leibler (KL) divergence from the policy
deﬁned by the Reward RNN. As a byproduct of minimizing
KL our objective includes an entropy regularization term
that encourages high entropy in the distribution of the RL
model. This is ideal for sequence generation tasks such as
text, music, or molecule generation, in which maintaining
diversity in the samples generated by the model is critical.

Sequence Tutor effectively combines both data and task-
related goals, without relying on either as a perfect met-
ric of task success. This is an important novel direction
of research. Much previous work on combining RL and
MLE has used MLE training simply as a way to bootstrap
the training of an RL model (Ranzato et al., 2015; Bah-
danau et al., 2016; Li et al., 2016), since training with RL
from scratch is difﬁcult. However, this approach does not
encourage diversity of the generated samples, and can be
problematic when task-speciﬁc rewards are incomplete or
imperfect. Designing an appropriate reward deﬁnition is
highly non-trivial, and often the hand-crafted rewards can-
not be fully trusted (Vedantam et al., 2015; Liu et al., 2016).
And yet, relying on data alone can be insufﬁcient when the
data itself contains biases, as has been shown for text data

Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control

(Caliskan-Islam et al., 2016), or when domain-speciﬁc con-
straints cannot be encoded directly into MLE training. By
learning a policy that trades off staying close to the data dis-
tribution while improving performance on speciﬁc metrics,
Sequence Tutor reduces both of these problems.

This paper contributes to the sequence training and RL lit-
erature by a) proposing a novel method for combining MLE
and RL training; b) showing the connection between KL
control and sequence generation; c) deriving the explicit
relationships among a generalized variant of Ψ-learning
(Rawlik et al., 2012), G-learning (Fox et al., 2015), and
Q-learning with log prior augmentation, and being the ﬁrst
to empirically compare these methods and use them with
deep neural networks.

We explore the usefulness of our approach for two se-
quence generation applications. The ﬁrst, music genera-
tion, is a difﬁcult problem in which the aesthetic beauty of
generated sequences cannot be fully captured in a known
reward function, but in which models trained purely on
data cannot produce well-structured sequences. Through
an empirical study, we show that by imposing rules of mu-
sic theory on a melody generation model, Sequence Tutor
is able to produce melodies which are varied, yet more har-
monious, interesting, and rated as signiﬁcantly more sub-
jectively pleasing than those of the MLE model. Further,
Sequence Tutor is able to signiﬁcantly reduce unwanted be-
haviors and failure modes of the original RNN. The effec-
tiveness of Sequence Tutor is also demonstrated for com-
putational molecular generation, a task in which the goal is
to generate novel drug-like molecules with desirable prop-
erties by outputting a string representation of the molecule
encoding. However, generating valid molecules can prove
difﬁcult, as it is hard for probabilistic models to learn all
the constraints that deﬁne physically realizable molecules
directly from data (G´omez-Bombarelli et al., 2016). We
show that Sequence Tutor is able to yield a higher percent-
age of valid molecules than the baseline MLE RNN, and
the generated molecules score higher on metrics of drug-
likeness and ease of synthesis.

2. Related Work

Recent work has attempted to use both MLE and RL in
the context of structured prediction. While the attempts
were successful, the problems of maintaining information
about the data distribution and diversity in the generated
samples were not addressed. MIXER (Mixed Incremen-
tal Cross-Entropy Reinforce)
(Ranzato et al., 2015) uses
BLEU score as a reward signal to gradually introduce a RL
loss to a text translation model. Bahdanau et al. (2016) ap-
plies an actor-critic method and uses BLEU score directly
to train a critic network to output the value of each word,
where the actor is again initialized with the policy of an

RNN trained with next-step prediction. Li et al. (2016) use
RL to improve a pre-trained dialog model with heuristic
rewards. These approaches assume that the complete task
reward speciﬁcation is available. They pre-train a good
policy with supervised learning so that RL can be used
to learn the true task objective, since it can be difﬁcult to
reach convergence when training with pure RL. However,
the original MLE policy of these models is overwritten by
the RL training process. In contrast, Sequence Tutor uses
rewards to correct certain properties of the generated data,
while learning most information from data and maintaining
this information; an important ability when the true reward
function is not available or imperfect.

(RAML)
augmented maximum likelihood
Reward
(Norouzi et al., 2016) is an approach designed to improve
MLE training of a translation model by augmenting the
ground truth targets with additional outputs that are within
a small edit distance, and performing MLE training against
those as well. The authors show that their approach is
equivalent to minimizing KL-divergence between an RL
exponentiated payoff distribution based on edit distance,
and the MLE distribution. In contrast, our goal is genera-
tion rather than prediction, and we train an RL rather than
MLE model. The RAML approach, while an important
contribution, is only viable if it is possible to generate
additional MLE training samples that are similar in terms
of the reward function to the ground truth (i.e. samples
within a small edit distance). However in some domains,
including the two explored in this paper, generating similar
samples with high reward is not only not possible, but in
fact constitutes the entire problem under investigation.

Finally, our approach is related to KL control (Todorov,
2007; Kappen et al., 2012; Rawlik et al., 2012), a branch
of stochastic optimal control (SOC) (Stengel, 1986). There
is also a connection between this work and Maximum En-
tropy Inverse RL (Ziebart et al., 2008), which can be seen
as KL control with a ﬂat, improper prior. From KL control,
we take inspiration from two off-policy, model-free meth-
ods, Ψ-learning (Rawlik et al., 2012) and G-learning (Fox
et al., 2015). Both approaches are derived from a KL-
regularized RL objective, where an agent maximizes the
reward while incurring additional penalty for divergence
from some prior policy. While our methods rely on simi-
lar derivations presented in these papers, our methods have
different motivations and forms from the original papers.
The original Ψ-learning (Rawlik et al., 2012) restricts the
prior policy to be the policy at the previous iteration and
solves the original RL objective with conservative, KL-
regularized policy updates, similar to conservative policy
gradient methods (Kakade, 2002; Peters et al., 2010; Schul-
man et al., 2015). The original G-learning (Fox et al., 2015)
penalizes divergence from a simple uniform prior policy
in order to cope with over-estimation of target Q values.

Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control

These techniques have not been applied to deep learning
techniques or with RNNs, or as a way to improve a pre-
trained MLE model. Our work is the ﬁrst to explore these
methods in such a context, and includes a Q-learning model
with additional cross-entropy reward as a comparable alter-
native. To the best of our knowledge, our work is the ﬁrst
to provide comparisons among these three approaches.

There has also been prior work in the domain of genera-
tive modeling of music. Using RNNs for this purpose has
been explored in a variety of contexts, including generat-
ing Celtic folk music (Sturm et al., 2016), or improvising
the blues (Eck & Schmidhuber, 2002). Often, this involves
training the RNN to predict the next note in a monophonic
melody; however, as mentioned above, the melodies gener-
ated by this model tend to wander and lack musical struc-
ture. Some authors have experimented with encoding musi-
cal structure into a hierarchical RNN with layers dedicated
to generated the melody, drums, and chords (Chu et al.,
2016). Other approaches have examined RNNs with richer
expressivity, latent-variables for notes, or raw audio synthe-
sis (Boulanger-Lewandowski et al., 2012; Gu et al., 2015;
Chung et al., 2015). Recently, Wavenet produced impres-
sive performance in generating music from raw audio using
convolutional neural networks with receptive ﬁelds at var-
ious time scales (van den Oord et al., 2016). However, the
authors themselves note that “even with a receptive ﬁeld
of several seconds, the models did not enforce long-range
consistency which resulted in second-to-second variations
in genre, instrumentation, and sound quality” (p. 8).

Finally, prior work has successfully performed computa-
tional molecular generation with deep neural networks.
Segler et al. (2017) demonstrated that an LSTM trained
on sets of biologically active molecules can be used to
generate novel molecules with similar properties. G´omez-
Bombarelli et al. (2016) trained a variational autoencoder
to learn a compact embedding of molecules encoded using
the SMILES notation. By interpolating in the embedding
space and optimizing for desirable metrics of drug quality,
the authors were able to decode molecules with high scores
on these metrics. However, producing embeddings that led
to valid molecules was difﬁcult; in some cases, as little as
1% of generated sequences proved to be a valid molecule
encoding.

3. Background

In RL, an agent interacts with an environment. Given the
state of the environment at time t, st, the agent takes an ac-
tion at according to its policy π(at|st), receives a reward
r(st, at), and the environment transitions to state, st+1.The
agent’s goal is to maximize reward over a sequence of ac-
tions, with a discount factor of γ applied to future rewards.
The optimal deterministic policy π∗ is known to satisfy the

following Bellman optimality equation,

Q(st, at; π∗) = r(st, at)

+ γEp(st+1|st,at)[max
at+1

Q(st+1, at+1; π∗)]

where Qπ(st, at) = Eπ[(cid:80)∞
t(cid:48)=t γt(cid:48)−tr(st(cid:48), at(cid:48))] is the Q
function of a policy π. In Deep Q-learning (Mnih et al.,
2013), a neural network called the deep Q-network (DQN)
is trained to approximate Q(s, a; θ), using the following
objective,

L(θ) = Eβ[(r(s, a) + γ max
a(cid:48)

Q(s(cid:48), a(cid:48); θ−) − Q(s, a; θ))2]

(1)

(2)

where β is the exploration policy, and θ− is the parameters
of the target Q-network (Mnih et al., 2013) that is held ﬁxed
during the gradient computation. The target Q-network is
updated more slowly than the Q-network; for example the
moving average of θ can be used as θ−, as proposed by
Lillicrap et al. (2015). Exploration can be performed with
either the (cid:15)-greedy method or Boltzmann sampling. Ad-
ditional techniques such as a replay memory (Mnih et al.,
2013) are used to stabilize and improve learning.

4. Sequence Tutor

Given a trained sequence generation RNN, we would like
to impose domain-speciﬁc rewards based on the structure
and quality of generated sequences, while still maintain-
ing information about typical sequences learned from data.
Therefore, we treat the trained model as a black-box prior
policy, and focus on developing a method that can tune
some properties of the model without interfering with the
original probability distribution learned from data. The
separation between the trained sequence model and the tun-
ing method is important, as it prevents RL training from
overwriting the original policy. To accomplish this task,
we propose Sequence Tutor. An LSTM trained on data sup-
plies the initial weights for three networks in the model: a
recurrent Q-network and target Q-network, and a Reward
RNN. The Reward RNN is held ﬁxed during training, and
treated as a prior policy which can supply the probability
of a given token in a sequence as originally learned from
data.

To apply RL to sequence generation, generating the next
token in the sequence is treated as an action a. The state of
the environment consists of all of the tokens generated so
far, i.e. st = {a1, a2, ...at−1}. Given action at, we would
like the reward rt to combine information about the prior
policy p(at|st) as output by the Reward RNN, as well as
some domain- or task-speciﬁc rewards rT . Figure 1 illus-
trates these ideas.

Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control

r(s, a) = log p(a|s) + rT (a, s)/c

(3)

L(θ) = Eπ[

r(st, at)/c + log p(at|st) − log πθ(at|st)]

(cid:88)

t

variants of Q-learning with minimal modiﬁcations, which
give rise to different properties. Let τ = {a1, a2, ..., at−1}
represent the sequence, r(τ ) the reward of the sequence,
p(τ ) be the prior distribution over τ given by the trained
sequence model, and q(τ ) be the policy of the Sequence
Tutor model. The objective is then to maximize the follow-
ing expression with respect to q(τ ), where DKL represents
the KL divergence of distributions:

L(q) = Eq(τ )[r(τ )]/c − DKL[q(τ )||p(τ )].

(6)

We express q(τ ) in terms of a parametrized recurrent
i.e. q(τ ) = (cid:81)T
policy πθ(at|st),
t=1 πθ(at|st) where
st = {a1, a2, ..., at−1}, indicates that the system is non-
Markovian. The prior policy is expressed similarly p(τ ) =
(cid:81)T
t=1 p(at|st). The reinforcement learning objective is the
following, where Eπ[·] below indicates expectation with re-
spect to sequences sampled from π,

The difference between this equation and Eq. 4 is that an
entropy regularizer is now included, and thus the optimal
policy is no longer deterministic. Below, we derive gen-
eral temporal-difference based methods for the KL-control
problem for sequence generation.

4.3. Recurrent Generalized Ψ-learning

Let V π(st) deﬁne the recurrent value function of the policy
πθ, given by,

V π(st) = Eπ[

r(st(cid:48), at(cid:48))/c + log p(at(cid:48)|st(cid:48))

(7)

∞
(cid:88)

t(cid:48)=t

− log π(at(cid:48)|st(cid:48))]

We deﬁne the generalized Ψ function, analogous to Q func-
tion for KL control, as below. We call this generalized Ψ
function, as it was introduced in deriving Ψ-learning (Raw-
lik et al., 2012), and the following derivation is a general-
ization to the Ψ-learning algorithm.

Ψπ(st, at) = r(st, at)/c + log p(at|st) + V π(st+1) (8)

Note that the state st+1 is given deterministically by st =
{a1, a2, ..., at−1} and at for sequence modeling, and thus
the expressions do not contain the usual stochastic dynam-
ics p(st+1|st, at). The value function V π(st) can be recur-
sively expressed in terms of Ψπ,

V π(st) = Eπ[Ψπ(st, at)] + H[π(.|st)]

= Eπ[Ψπ(st, at) − log π(at|st)]

(9)

(10)

Fixing Ψ(st, at) = Ψπ(st, at) and constraining π to be a
probability distribution, the optimal greedy policy update

Figure 1: An RNN pre-trained on data using MLE supplies
the initial weights for the Q-network and target Q-network,
and a ﬁxed copy is used as the Reward RNN.

4.1. Q-learning with log prior augmentation

The simplest and most na¨ıve way to incorporate informa-
tion about the prior policy is to directly augment the task-
speciﬁc rewards with the output of the Reward RNN. In this
case, the total reward given at time t becomes:

where c is a constant controlling the emphasis placed on
the task-speciﬁc rewards. Given the DQN objective in Eq.
2 and modiﬁed reward function in Eq. 3, the objective and
learned policy are:

L(θ) = Eβ[(log p(a|s) + rM T (a, s)/c

+ γ max

Q(s(cid:48), a(cid:48); θ−) − Q(s, a; θ))2]

a(cid:48)

πθ(a|s) = δ(a = arg max

Q(s, a; θ)).

a

(4)

(5)

This modiﬁed objective forces the model to learn that the
most valuable actions are those that conform to the music
theory rules, but still have high probability in the original
data. However, the DQN learns a deterministic policy (as
shown in Eq. 5), which is not ideal for sequence gener-
ation. Therefore, after the model is trained, we generate
sequences by sampling from the softmax function applied
to the predicted Q-values.

4.2. KL Control for Sequence Generation

If we cast sequence generation as a sequential decision-
making problem and the desired sequence properties in
terms of target rewards, the problem can be expressed
as a KL control problem for a non-Markovian system.
KL control (Todorov, 2007; Kappen et al., 2012; Raw-
lik et al., 2012) is a branch of stochastic optimal control
(SOC) (Stengel, 1986), which studies an RL, or control,
problem in which the agent tries maximizing its task re-
ward while minimizing deviation from a prior policy. For
our purposes, we treat a trained MLE sequence model as
the prior policy, and thus the objective is to train a new pol-
icy, or sequence model, to maximize some rewards while
keeping close to the original MLE model. We show that
such KL control formulation allows us to derive additional

Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control

π∗ can be derived, along with the corresponding optimal
value function,

4.5. Sequence Tutor implementation

π∗(at|st) ∝ eΨ(st,at)
(cid:88)
V ∗(st) = log

eΨ(st,at)

at

(11)

(12)

Given Eq. 8 and 12, the following Bellman optimality
equation for generalized Ψ function is derived.

Ψ∗(st, at) = r(st, at)/c + log p(at|st)
(cid:88)

+ log

exp(Ψ∗(st+1, at+1))

(13)

at+1

The Ψ-learning loss directly follows:

LΨ(θ) = Eβ[(Ψθ(st, at) − yt)2] where

yt = log p(at|st) + r(st, at)/c + γ log

(14)
eΨ−(st+1,a(cid:48))

(cid:88)

a(cid:48)

β corresponds to sampling sequence trajectories from
an arbitrary distribution;
in practice, the experience re-
play (Mnih et al., 2013). Ψ− indicates that it uses the target
network. Ψθ, i.e. πθ, is parametrized with recurrent neu-
ral networks, and for discrete actions, πθ is effectively a
softmax layer on top of Ψθ.

4.4. Recurrent G-learning

We can derive another algorithm by parametrizing Ψθ in-
directly by Ψθ(st, at) = log p(at|st) + Gθ(st, at). Sub-
stituting into above equations, we get a different temporal-
difference method:

LG(θ) = Eβ[(Gθ(st, at) − yt)2] where

yt = r(st, at)/c + γ log

(15)
p(a(cid:48)|st+1)eG−(st+1,a(cid:48)) and

(cid:88)

a(cid:48)

πθ(at|st) ∝ p(at|st) exp(Gθ(st, at))

This formulation corresponds to G-learning (Fox et al.,
2015), which can thus be seen as a special case of gen-
eralized Ψ-learning. Unlike Ψ learning, which directly
builds knowledge about the prior policy into the Ψ func-
tion, the G-function does not give the policy directly but in-
stead needs to be dynamically mixed with the prior policy
probabilities. While this computation is straight-forward
for discrete action domains as here, extensions to continu-
ous action domains require additional considerations such
as normalizability of Ψ-function parametrization (Gu et al.,
2016).

The KL control-based derivation also has another beneﬁt
in that the stochastic policies can be directly used as an
exploration strategy, instead of heuristics such as (cid:15)-greedy
or additive noise (Mnih et al., 2013; Lillicrap et al., 2015).

Following from the above derivations, we compare three
methods for implementing Sequence Tutor: Q-learning
with log prior augmentation (based on Eq. 4), generalized
Ψ-learning (based on Eq. 14), and G-learning (based on
Eq. 15). A pre-trained sequence generation LSTM is used
as the Reward RNN, to supply the cross entropy reward in
Q-learning and the prior policy in G- and generalized Ψ-
learning. These approaches are compared to both the orig-
inal performance of the MLE RNN, and a model trained
using only RL and no prior policy. Model evaluation is
performed every 100,000 training epochs, by generating
100 sequences and assessing the average rT and log p(a|s).
The code for Sequence Tutor, including a checkpointed
version of the trained melody RNN is available at redacted
for anonymous submission.

5. Experiment I: Melody Generation

Music compositions adhere to relatively well-deﬁned struc-
tural rules, making music an interesting sequence gen-
eration challenge. For example, music theory tells that
groups of notes belong to keys, chords follow progressions,
and songs have consistent structures made up of musical
phrases. Our research question is therefore whether such
constraints can be learned by an RNN, while still allowing
it to maintain note probabilities learned from data.

To test this hypothesis, we developed several rules that we
believe describe pleasant-sounding melodies, taking inspi-
ration from a text on melodic composition (Gauldin, 1995).
We do not claim these characteristics are exhaustive or
strictly necessary for good composition; rather, they are an
incomplete measure of task success that can simply guide
the model towards traditional composition structure. It is
therefore crucial that the Sequence Tutor approach allows
the model to retain knowledge learned from real songs in
the training data. The rules comprising the music-speciﬁc
reward function rT (a, s) encourage melodies to: stay in
key, start with the tonic note, resolve melodic leaps, have
a unique maximum and minimum note, prefer harmonious
intervals, play motifs and repeat them, have a low autocor-
relation at a lag of 1, 2, and 3 beats, and avoid excessively
repeating notes. Interestingly, while excessively repeating
tokens is a common problem in RNN sequence generation
models, avoiding this behavior is also Gauldin’s ﬁrst rule
of melodic composition (p. 42).

To train the model, we begin by extracting monophonic
melodies from a corpus of 30,000 MIDI songs and encod-
ing them as one-hot sequences of notes1. These melodies
are then used to train an LSTM with one layer of 100 cells.

1More information about both the note encoding and the re-

ward metrics is available in the supplementary material.

Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control

Optimization was performed with Adam (Kingma & Ba,
2014), a batch size of 128, initial learning rate of .5, and a
stepwise learning rate decay of 0.85 every 1000 steps. Gra-
dients were clipped to ensure the L2 norm was less than 5,
and weight regularization was applied with β = 2.5×10−5.
Finally, the losses for the ﬁrst 8 notes of each sequence
were not used to train the model, since it cannot reasonably
be expected to accurately predict them with no context. The
trained RNN eventually obtained a validation accuracy of
92% and a log perplexity score of .2536. This model was
used as described above to initialize the three sub-networks
in the Sequence Tutor model.

The Sequence Tutor model was trained using a similar con-
ﬁguration to the one above, except with a batch size of
32, and a reward discount factor of γ=.5. The Target-
Q-network’s weights θ− were gradually updated towards
those of the Q-network (θ) according to the formula (1 −
η)θ− + ηθ, where η = .01 is the Target-Q-network up-
date rate. A strength of our model is that the inﬂuence of
data and task-speciﬁc rewards can be explicitly controlled
by adjusting the temperature parameter c. We replicated
our results for a number of settings for c; we present re-
sults for c=.5 below because we believe them to be most
musically pleasing, however additional results are available
at https://goo.gl/cTZy8r. Similarly, we replicated
the results using both (cid:15)-greedy and Boltzmann exploration,
and present the results using (cid:15)-greedy exploration below.

5.1. Results

Table 1 provides quantitative results in the form of per-
formance on the music theory rules to which we trained
the model to adhere; for example, we can assess the frac-
tion of notes played by the model which belonged to the
correct key, or the fraction of melodic leaps that were re-
solved. The statistics were computed by randomly generat-
ing 100,000 melodies from each model.

Metric
Repeated notes
Mean autocorr. lag 1
Mean autocorr. lag 2
Mean autocorr. lag 3
Notes not in key
Starts with tonic
Leaps resolved
Unique max note
Unique min note
Notes in motif
Notes in repeat motif

Ψ

G

.55
.31
17

-.10
-.01
.01

Q
MLE
0.0% 0.02% 0.03%
63.3%
-.11
-.16
.03
.14
.03
-.13
1.00% 0.60% 28.7%
0.1%
28.8% 28.7% 0.0%
0.9%
91.1% 90.0% 52.2%
77.2%
56.4% 59.4% 37.1%
64.7%
51.9% 58.3% 56.5%
49.4%
75.7% 73.8% 69.3%
5.9%
0.007% 0.11% 0.09% 0.01%

Table 1: Statistics of music theory rule adherence based on
100,000 randomly initialized melodies generated by each
model. The top half of the table contains metrics that
should be near zero, while the bottom half contains metrics
that should increase. Bolded entries represent signiﬁcant
improvements over the MLE baseline.

The results above demonstrate that the application of RL is
able to correct almost all of the targeted “bad behaviors”
of the MLE RNN, while improving performance on the
desired metrics. For example, the original LSTM model
was extremely prone to repeating the same note; after ap-
plying RL, we see that the number of notes belonging to
some excessively repeated segment has dropped from 63%
to nearly 0% in all of the Sequence Tutor models. While
the metrics for the G model did not improve as consistently,
the Q and Ψ models successfully learned to adhere to most
of the imposed rules. The degree of improvement on these
metrics is related to the magnitude of the reward given for
the behavior. For example, a strong penalty of -100 was
applied each time a note was excessively repeated, while
a reward of only 3 was applied at the end of a melody
for unique extrema notes (which most likely explains the
lack of improvement on this metric). The reward values
could be adjusted to improve the metrics further, however
we found that these values produced pleasant melodies.

While the metrics indicate that the targeted behaviors of
the RNN have improved, it is not clear whether the models
have retained information about the training data. Figure
2a plots the average log p(a|s) as produced by the Reward
RNN for melodies generated by the models every 100,000
training epochs; Figure 2b plots the average rT . Included
in the plot is an RL only model trained using only the mu-
sic theory rewards, with no information about log p(a|s).
Since each model is initialized with the weights of the
trained MLE RNN, we see that as the models quickly learn
to adhere to the music theory constraints, log p(a|s) falls
from its initial point. For the RL only model, log p(a|s)
reaches an average of -3.65, which is equivalent to an aver-
age p(a|s) of approximately 0.026, or essentially a random
policy over the 38 actions with respect to the distribution
deﬁned by the Reward RNN. Figure 2a shows that each
of our models (Q, Ψ, and G) attain higher log p(a|s) val-
ues than this baseline, indicating they have maintained in-
formation about the data distribution, even over 3,000,000
training steps. The G-learning implementation scores high-
est on this metric, at the cost of slightly lower average
rT . This compromise between data probability and adher-
ence to music theory could explain the difference in the G
model’s performance on the music theory metrics in Table
1. Finally, we have veriﬁed that by increasing the c pa-
rameter it is possible to train all the models to have even
higher average log p(a|s), but found that c = 0.5 produced
melodies that sounded better subjectively.

The question remains whether the RL-tutored models actu-
ally produce more pleasing melodies. The sample melodies
used for the study are available here: goo.gl/XIYt9m;
we encourage readers to judge their quality for themselves.
To more formally answer this question, we conducted a
user study via Amazon Mechanical Turk in which partic-

Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control

(a) Reward RNN reward: log p(a|s)

(b) Music theory reward

Figure 2: Average reward obtained by sampling 100
melodies every 100,000 training epochs. The three mod-
els are compared to a model trained using only the music
theory rewards rT .

ipants were asked to rate which of two randomly selected
melodies they preferred on a Likert scale. A total of 192
ratings were collected; each model was involved in 92 of
these comparisons. Figure 3 plots the number of compar-
isons in which a melody from each model was selected as
the most musically pleasing. A Kruskal-Wallis H test of the
ratings showed that there was a statistically signiﬁcant dif-
ference between the models, χ2(3) = 109.480, p < 0.001.
Mann-Whitney U post-hoc tests revealed that the melodies
from all three Sequence Tuner models (Q, Ψ, and G) had
signiﬁcantly higher ratings than the melodies of the MLE
RNN, p < .001. The Q and Ψ melodies were also rated as
signiﬁcantly more pleasing than those of the G model, but
did not differ signiﬁcantly from each other.

Figure 3: The number of times a melody from each model
was selected as most musically pleasing. Error bars reﬂect
the std. dev. of a binomial distribution ﬁt to the binary
win/loss data from each model.

produce energetic and chaotic melodies, which include se-
quences of repeated notes. This repetition is likely because
the G policy as deﬁned in Eq. 15 directly mixes p(a|s) with
the output of the G network, and the MLE RNN strongly
favours repeating notes. The most pleasant melodies are
generated by the Q and Ψ models. These melodies stay
ﬁrmly in key and frequently choose more harmonious inter-
val steps, leading to melodic and pleasant melodies. How-
ever, it is clear they have retained information about the
training data; for example, the sample q2.wav in the sam-
ple directory ends with a seemingly familiar riff.

While we acknowledge that the monophonic melodies gen-
erated by these models — which are based on highly sim-
plistic rules of melodic composition — do not approach the
level of artistic merit of human composers, we believe this
study provides a proof-of-concept that encoding even in-
complete and partially speciﬁed domain knowledge using
our method can help the outputs of an LSTM adhere to a
more consistent structure. The musical complexity of the
songs is limited not just by the heuristic rules, but also by
the simple monophonic encoding, which cannot represent
the dynamics and expressivity of a musical performance.
Although these melodies cannot surpass those of human
musicians, attempting to train a model to generate aesthet-
ically pleasing outputs in the absence of a better metric of
human taste than log-likelihood is a problem of broader in-
terest to the artiﬁcial intelligence community.

5.2. Discussion

Listening to the samples produced by the MLE RNN re-
veals that they are sometimes dischordant and usually dull;
the model tends to place rests frequently, repeat the same
note, and produce melodies with little variation. In con-
trast, the melodies produced by the Sequence Tutor mod-
els are more varied and interesting. The G model tends to

6. Experiment II: Computational Molecular

Generation

As a follow-on experiment, we tested the effectiveness of
Sequence Tutor for generating a higher yield of synthet-
ically accessible drug-like molecules. Organic molecules
can be encoded using the commonly used SMILES repre-
sentation (Weininger, 1970). For example, amphetamine

Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control

can be encoded as ‘CC(N)Cc1ccccc1’, while creatine is
‘CN(CC(=O)O)C(=N)N’. Using this character encoding, it
is straightforward to train an MLE RNN to generate se-
quences of SMILES characters; we trained such a model
using the same settings as described above for the melody
MLE RNN. However, only about a third of the molecules
generated using this simple approach are actually valid
SMILES encodings. Further, this approach does not di-
rectly optimize for metrics of molecule or drug quality.
These metrics include: a) the water-octanol partition co-
efﬁcient (logP), which is important in assessing the drug-
likeness of a molecule; b) synthetic accessibility (SA) (Ertl
& Schuffenhauer, 2009), a score from 1-10 that is lower if
the molecule is easier to synthesize; and c) Quantitative Es-
timation of Drug-likeness (QED) (Bickerton et al., 2012), a
more subjective measure of drug-likeness based on abstract
ideas of medicinal aesthetics.

To optimize for these metrics, while simultaneously im-
proving the percent yield of valid molecules from the RNN,
we constructed a reward function that incentivizes valid-
ity, logP, SA, and QED using an open-source library called
RDkit (http://www.rdkit.org/).
Included in the
reward function was a penalty for molecules with unreal-
istically large carbon rings (size larger than 6), as per pre-
vious work (G´omez-Bombarelli et al., 2016). Finally, af-
ter observing that the model could exploit the reward func-
tion by generating the simple molecule ‘N’ repeatedly, or
‘CCCCC...’ (which produces an unrealistically high logP
value), we added penalties for sequences shorter than, or
with more consecutive carbon atoms than, any sequence
in the training data. Sequence Tutor was then trained us-
ing these rewards, the pre-trained MLE RNN, and similar
settings to the ﬁrst experiment, except with (cid:15)-greedy explo-
ration with (cid:15) = .01, a batch size of 512, and discount factor
γ = .95. For this experiment, we also made use of prior-
itized experience replay (Schaul et al., 2015) to allow the
model to more frequently learn from relatively rare valid
samples. A value of c = 2.85 led to a higher yield of valid
molecules with high metrics, but still encouraged the diver-
sity of generated samples.

6.1. Results and discussion

As the Ψ algorithm produced the best results for the mu-
sic generation task, we focused on using this technique
for generating molecules. Table 2 shows the performance
of this model against the original MLE model according
to metrics of validity, drug-likeness, and synthetic acces-
sibility. Once again, Sequence Tutor is able to signiﬁ-
cantly improve almost all of the targeted metrics. How-
ever, it should be noted that the Sequence Tutor model
tends to produce simplistic molecules involving more car-
bon atoms than the MLE baseline; e.g. Sequence Tutor
may produce ‘SNCc1ccccc1’, while the MLE produces

‘C(=O)c1ccc(S(=O)(=O)N(C)C)c(Cl)c1’, which is the rea-
son for the Sequence Tutor model’s lower QED scores.
This effect is due to the fact that simple sequences are more
likely to be valid, have high logP and SA scores, and car-
bon is highly likely under the distribution learned by the
MLE model. A higher reward for QED and further im-
provement of the task-speciﬁc rewards based on domain
knowledge could help to alleviate these problems. Overall,
the fact that Sequence Tutor can improve the percentage of
valid molecules produced as well as the logP and synthetic
accessibility scores serves as a proof-of-concept that Se-
quence Tutor may be valuable in a number of domains for
imparting domain knowledge onto a sequence predictor.

Metric
Percent valid
Mean logP
Mean QED
Mean SA penalty
Mean ring penalty

Q

MLE
30.3% 35.8%
2.07
.678
-2.77
-.096

4.21
.417
-1.79
-.001

Table 2: Statistics of molecule validity and quality based
on 100,000 randomly initialized samples. Bolded entries
represent signiﬁcant improvements over the MLE baseline.

7. Conclusion and Future Work

We have derived a novel sequence learning framework
which uses RL to correct properties of sequences gener-
ated by an RNN, while maintaining information learned
from MLE training on data, and ensuring the diversity of
generated samples. By demonstrating a connection be-
tween our sequence generation approach and KL-control,
we have derived three novel RL-based methods for opti-
mizing sequence generation models. These methods were
empirically compared in the context of a music generation
task, and further demonstrated on a computational molecu-
lar generation task. Sequence Tutor showed promising re-
sults in terms of both adherence to task-speciﬁc rules, and
subjective quality of the generated sequences.

We believe the Sequence Tutor approach of using RL to
reﬁne RNN models could be promising for a number of ap-
plications, including the reduction of bias in deep learning
models. While manually writing a domain-speciﬁc reward
function may seem unappealing, that approach is limited
by the quality of the data that can be collected, and be-
sides, even state-of-the-art sequence models often fail to
learn all the aspects of high-level structure (van den Oord
et al., 2016; Graves, 2013). Further, the data may contain
hidden biases, as has been demonstrated for popular lan-
guage models (Caliskan-Islam et al., 2016). In contrast to
relying solely on possibly biased data, our approach allows
for encoding high-level domain knowledge into the RNN,
providing a general, alternative tool for training sequence
models.

Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control

ACKNOWLEDGMENTS

This work was supported by Google Brain, the MIT Me-
dia Lab Consortium, and Canada’s Natural Sciences and
Engineering Research Council (NSERC). We thank Greg
Wayne, Sergey Levine, and Timothy Lillicrap for helpful
discussions on RL and stochastic optimal control and Kyle
Kastner and Tim Cooijmans for valuable insight into train-
ing RNNs.

References

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh
Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and
Yoshua Bengio. An actor-critic algorithm for sequence
prediction. arXiv preprint arXiv:1607.07086, 2016.

G Richard Bickerton, Gaia V Paolini, J´er´emy Besnard,
Sorel Muresan, and Andrew L Hopkins. Quantifying the
chemical beauty of drugs. Nature chemistry, 4(2):90–98,
2012.

Boulanger-Lewandowski, Bengio, and Vincent. Modeling
temporal dependencies in high-dimensional sequences:
Application to polyphonic music generation and tran-
scription. arXiv preprint:1206.6392, 2012.

Caliskan-Islam, Bryson, and Narayanan. Semantics de-
rived automatically from language corpora necessarily
contain human biases. arXiv preprint:1608.07187, 2016.

Hang Chu, Raquel Urtasun, and Sanja Fidler. Song from pi:
A musically plausible network for pop music generation.
arXiv preprint arXiv:1611.03477, 2016.

Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth
Goel, Aaron C Courville, and Yoshua Bengio. A re-
current latent variable model for sequential data. In Ad-
vances in neural information processing systems (NIPS),
pp. 2980–2988, 2015.

Eck and Schmidhuber. Finding temporal structure in mu-
sic: Blues improvisation with LSTM recurrent networks.
In Neural Networks for Signal Processing, pp. 747–756.
IEEE, 2002.

Peter Ertl and Ansgar Schuffenhauer. Estimation of syn-
thetic accessibility score of drug-like molecules based on
molecular complexity and fragment contributions. Jour-
nal of cheminformatics, 1(1):8, 2009.

Roy Fox, Ari Pakman, and Naftali Tishby. Taming the
noise in reinforcement learning via soft updates. arXiv
preprint arXiv:1512.08562, 2015.

Gauldin. A practical approach to eighteenth-century coun-

terpoint. Waveland Pr Inc, 1995.

Rafael G´omez-Bombarelli, David Duvenaud, Jos´e Miguel
Hern´andez-Lobato, Jorge Aguilera-Iparraguirre, Timo-
thy D Hirzel, Ryan P Adams, and Al´an Aspuru-Guzik.
Automatic chemical design using a data-driven con-
arXiv preprint
tinuous representation of molecules.
arXiv:1610.02415, 2016.

Alex Graves. Generating sequences with recurrent neural

networks. arXiv preprint:1308.0850, 2013.

Shixiang Gu, Zoubin Ghahramani, and Richard E Turner.
In Advances
Neural adaptive sequential monte carlo.
in Neural Information Processing Systems (NIPS), pp.
2629–2637, 2015.

Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey
Levine. Continuous Deep Q-Learning with model-based
acceleration. In ICML, 2016.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term
memory. Neural computation, 9(8):1735–1780, 1997.

Sham M Kakade. A natural policy gradient. In Advances
in neural information processing systems (NIPS), vol-
ume 14, pp. 1531–1538, 2002.

Kappen, G´omez, and Opper. Optimal control as a graphical
model inference problem. Machine learning, 87(2):159–
182, 2012.

Diederik Kingma

and Jimmy Ba.
method for stochastic optimization.
arXiv:1412.6980, 2014.

Adam:

A
arXiv preprint

Jiwei Li, Will Monroe, Alan Ritter, and Dan Jurafsky. Deep
reinforcement learning for dialogue generation. arXiv
preprint arXiv:1606.01541, 2016.

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,
Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and
Daan Wierstra. Continuous control with deep reinforce-
ment learning. arXiv preprint arXiv:1509.02971, 2015.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Nose-
worthy, Laurent Charlin, and Joelle Pineau. How not to
evaluate your dialogue system: An empirical study of
unsupervised evaluation metrics for dialogue response
generation. In EMNLP, 2016.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex
Graves, Ioannis Antonoglou, Daan Wierstra, and Martin
Riedmiller. Playing atari with deep reinforcement learn-
ing. arXiv preprint arXiv:1312.5602, 2013.

Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike
Schuster, Yonghui Wu, Dale Schuurmans, et al. Re-
ward augmented maximum likelihood for neural struc-
In Advances In Neural Information
tured prediction.
Processing Systems, pp. 1723–1731, 2016.

Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control

Peters, M¨ulling, and Altun. Relative entropy policy search.

In AAAI. Atlanta, 2010.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and
Wojciech Zaremba. Sequence level training with recur-
rent neural networks. arXiv preprint arXiv:1511.06732,
2015.

Rawlik, Toussaint, and Vijayakumar. On stochastic opti-
mal control and reinforcement learning by approximate
inference. Proceedings of Robotics: Science and Sys-
tems VIII, 2012.

Tom Schaul, John Quan, Ioannis Antonoglou, and David
Silver. Prioritized experience replay. arXiv preprint
arXiv:1511.05952, 2015.

Schulman, Levine, Moritz, Jordan, and Abbeel. Trust re-

gion policy optimization. In ICML, 2015.

Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and
Mark P Waller. Generating focussed molecule libraries
for drug discovery with recurrent neural networks. arXiv
preprint arXiv:1701.01329, 2017.

Robert F Stengel. Stochastic optimal control. John Wiley

and Sons New York, New York, 1986.

Sturm, Santos, Ben-Tal, and Korshunova. Music tran-
scription modelling and composition using deep learn-
ing. arXiv preprint:1604.08723, 2016.

Emanuel Todorov.

Linearly-solvable markov decision
In Advances in neural information process-

problems.
ing systems (NIPS), pp. 1369–1376, 2007.

A¨aron van den Oord, Sander Dieleman, Heiga Zen,
Karen Simonyan, Oriol Vinyals, Alex Graves, Nal
Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
Wavenet: A generative model for raw audio. CoRR
abs/1609.03499, 2016.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. Cider: Consensus-based image description eval-
uation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 4566–4575,
2015.

David Weininger. Smiles, a chemical language and infor-
mation system. 1. introduction to methodology and en-
coding rules. In Proc. Edinburgh Math. SOC, volume 17,
pp. 1–14, 1970.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and
Anind K Dey. Maximum entropy inverse reinforcement
learning. In AAAI, volume 8, pp. 1433–1438. Chicago,
IL, USA, 2008.

