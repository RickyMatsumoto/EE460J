000
001
002
003
004
005
006
007
008
009
010
011
012
013
014
015
016
017
018
019
020
021
022
023
024
025
026
027
028
029
030
031
032
033
034
035
036
037
038
039
040
041
042
043
044
045
046
047
048
049
050
051
052
053
054

Leveraging Union of Subspace Structure to Improve Constrained Clustering:
Supplementary Material

Anonymous Authors1

In this document, we provide the proofs to Theorem 1 and
Corollary 1, which appear in Section 3.1 of the main docu-
ment. We also explain the optional UOS-EXPLORE initial-
ization phase of the SUPERPAC algorithm.

First consider the numerator and note that y − P1y =
1 y ∼ N (0, σ2P ⊥
P ⊥
E (cid:13)

1 ) with
1 y(cid:13)
2
(cid:13)

= σ2(D − d).

(cid:13)P ⊥

1. Proofs of Technical Results

Theorem 1. Consider two d-dimensional subspaces S1 and
S2. Let y = x + n, where x ∈ S1 and n ∼ N (0, σ2ID).
Deﬁne

µ(y) =

dist(y, S1)
dist(y, S2)

.

Then

and

(1 − ε)(cid:112)σ2(D − d)
(1 + ε)(cid:112)σ2(D − d) + dist(x, S2)2

≤ µ(y)

µ(y) ≤

(1 + ε)(cid:112)σ2(D − d)
(1 − ε)(cid:112)σ2(D − d) + dist(x, S2)2

,

with probability at least 1 − 4e−cε2(D−d), where c is an
absolute constant.

Proof. The proof relies on theorem 5.2.1 from (Vershynin,
2016), restated below.

Theorem 2. (Concentration on Gauss space) Consider a
random vector X ∼ N (0, σ2ID) and a Lipschitz function
f : RD → R. Then for every t ≥ 0,

and

Let f (z) = (cid:107)P z(cid:107)2, where P is an arbitrary projection
matrix. In this case, (cid:107)f (cid:107)Lip = 1, as f is a composition of
1-Lipschitz functions, which is also 1-Lipschitz. Further, by
Exercise 5.2.5 of (Vershynin, 2016), we can replace E (cid:107)X(cid:107)2
by
in the concentration inequality. Applying
Thm. 2 to the above, we see that

E (cid:107)X(cid:107)2
2

(cid:17)1/2

(cid:16)

P

(cid:110)(cid:12)
(cid:12)
(cid:12)

(cid:13)
(cid:13)P ⊥

1 y(cid:13)

(cid:13) − (cid:112)σ2(D − d)

(cid:111)

(cid:12)
(cid:12)
(cid:12) ≥ t

≤ 2 exp

−

(cid:18)

(cid:19)

.

ct2
σ2

(1)
2 y ∼

Similarly, for the denominator, note that y − P2y = P ⊥
N (P ⊥

2 x, σ2P ⊥
2 ) with
2 y(cid:13)
E (cid:13)
2
(cid:13)

(cid:13)P ⊥

= σ2(D − d) + γ2.

2 y is no longer centered, we let g(z) = z + P ⊥

Since P ⊥
2 x,
which also has (cid:107)g(cid:107)Lip = 1. Applying Thm. 2 to the cen-
tered random vector ¯y ∼ N (0, σ2P ⊥
2 ) with Lipschitz func-
tion h = f ◦ g, we have that

P

(cid:110)(cid:12)
(cid:12)
(cid:12)

(cid:13)
(cid:13)P ⊥

2 y(cid:13)

(cid:13) − (cid:112)σ2(D − d) + γ2

Letting t = ε(cid:112)σ2(D − d)
ε(cid:112)σ2(D − d) + γ2 in (2) yields
(1 − ε)(cid:112)σ2(D − d) ≤ (cid:13)
1 y(cid:13)

(cid:13)P ⊥

(cid:19)

(cid:18)

(cid:12)
(cid:111)
(cid:12)
(cid:12) ≥ t

≤ 2 exp

ct2
σ2
(2)
in (1) and t =

−

.

(cid:13) ≤ (1 + ε)(cid:112)σ2(D − d)

P {|f (X) − Ef (X)| ≥ t} ≤ 2 exp

−

(cid:32)

(cid:33)

,

ct2
σ2 (cid:107)f (cid:107)2

Lip

where (cid:107)f (cid:107)Lip is the Lipschitz constant of f .

*Equal contribution 1Anonymous Institution, Anonymous City,
Anonymous Region, Anonymous Country. Correspondence to:
Anonymous Author <anon.email@domain.com>.

Preliminary work. Under review by the International Conference
on Machine Learning (ICML). Do not distribute.

(cid:13)P ⊥

(1 − ε)(cid:112)σ2(D − d) + γ2 ≤ (cid:13)

2 y(cid:13)
(cid:13)
≤ (1 + ε)(cid:112)σ2(D − d) + γ2,
each with probability at least 1 − 2 exp (cid:0)−cε2(D − d)(cid:1)
(since γ > 0). Applying the union bound gives the statement
of the theorem.

Corollary 1. Suppose x1 ∈ S1 is such that

dist(x1, S2)2 = sin2(φ1) + δ

sin2(φi)

(3)

(cid:33)

(cid:32)

1
d

d
(cid:88)

i=1

055
056
057
058
059
060
061
062
063
064
065
066
067
068
069
070
071
072
073
074
075
076
077
078
079
080
081
082
083
084
085
086
087
088
089
090
091
092
093
094
095
096
097
098
099
100
101
102
103
104
105
106
107
108
109

Leveraging Union of Subspace Structure to Improve Constrained Clustering

for some small δ ≥ 0; that is, x1 is close to the intersection
of S1 and S2. Let x2 be a random point in S1 generated as
x2 = U1w where U1 is a basis for S1 and w ∼ N (0, 1
d Id).
We observe yi = xi + ni, where ni ∼ N (0, σ2), i = 1, 2.
If there exists τ > 1 such that

δ <

−

5
7

1
τ

and

(cid:18)

τ

sin2(φ1) +

σ2 (D − d)

<

sin2(φi) ,

(4)

1
6

(cid:19)

1
d

d
(cid:88)

i=1

that is, the average angle is sufﬁciently larger than the
smallest angle, then

P {µ(y1) > µ(y2)} ≥ 1 − e−c( 7

100 )2

ds − 4e−c( 1

50 )2

(D−d)

where µ(y) is deﬁned as in Thm. 1, c is an absolute constant,
and s = 1
d

i=1 sin2(φi).

(cid:80)d

Algorithm 1 UOS-EXPLORE

Input: X = {x1, x2, . . . , xN }: data, K: number of
subspaces, d: dimension of subspaces, A: afﬁnity matrix,
maxQueries: maximum number of pairwise comparisons
Estimate Labels: ˆC ← SPECTRALCLUSTERING(A,K)
Calculate Margin: Calculate margin and set

x∨ ← arg maxx∈X ˆµ(x) (most conﬁdent point)

Initialize Certain Sets: Z1 ← x∨, Z ← {Z1},

numQueries ← 0, nc ← 1

while nc < K and numQueries < maxQueries do

Obtain Test Point: Choose xT as point of maximum
margin such that ˆC(xT ) (cid:54)= ˆC(x ∈ Zk) for any k. If
no such xT exists, choose xT at random.
Assign xT to Certain Set:

Sort {Z1, · · · , Znc } in order of most likely must-
link (via subspace residual for xT ), query xT against
representatives from Zk until must-link constraint is
found or k = nc. If no must-link constraint found,
set Z ← {Z1, · · · , Znc, {xT }} and increment nc.

Proof. We have from Thm. 1 that

end while

and

µ(y2) ≤

(1 + ε)(cid:112)σ2(D − d)
(1 − ε)(cid:112)σ2(D − d) + γ2

2

(1 − ε)(cid:112)σ2(D − d)
(1 + ε)(cid:112)σ2(D − d) + γ2

1

≤ µ(y1)

with probability at least 1 − 4e−cε2(D−d). Therefore if we
get the upper bound of µ(y2) to be smaller than the lower
bound of µ(y1), we are done. Rearranging this desired
inequality we see that we need

1 < β4γ2
γ2

2 − (1 − β4)σ2(D − d).

(5)

where β = (1 − ε)/(1 + ε). Let ε be such that β4 = 5/6,
1 = sin2(φ1) + δs as in the theorem. Then we
and let γ2
wish to select δ to satisfy

5

6 γ2

2 − sin2(φ1) − 1

6 σ2(D − d)

δ <

.

(6)

s
Applying concentration with γ2
2 , we have that γ2
2 ≥ (1 −
ξ)2s with probability at least 1 − e−cξ2ds where c is an
absolute constant. Therefore taking ξ to be such that (1 −
ξ)2 = 6/7, we require

5

7 s − sin2(φ1) − 1

6 σ2(D − d)

δ <

s

=

−

5
7

1
τ

where we used the deﬁnition of τ in the theorem. To quantify
the probability we need the appropriate values for ε and ξ;
we lower bound both with simple fractions: 1/50 < ε where
((1 − ε)/(1 + ε))4 = β = 5/6 and 7/100 < ξ where (1 −
ξ)2 = 6/7. Applying the union bound with the chosen
concentration values implies that µ(y1) > µ(y2) holds with
100 )2
probability at least 1 − e−c( 7

ds − 4e−c( 1

(D−d).

50 )2

2. UOS-EXPLORE Algorithm

In this section, we describe the process of initializing the
certain sets. Note that this step is not necessary, as we
could initialize all certain sets to be empty, but we found
it led to improved performance experimentally. A main
distinction between subspace clustering and the general
clustering problem is that in the UoS model points can lie
arbitrarily far from each other but still be on or near the
same subspace. For this reason, the EXPLORE algorithm
from (Basu et al., 2004) is unlikely to quickly ﬁnd points
from different clusters in an efﬁcient manner. Here we
deﬁne an analogous algorithm for the UoS case, termed
UOS-EXPLORE, with pseudocode given in Algorithm 1.
The goal of UOS-EXPLORE is to ﬁnd K certain sets, each
containing as few points as possible (ideally a single point),
allowing us to more rapidly assign test points to certain
sets in the SUPERPAC algorithm. We begin by selecting
our test point xT as the most certain point, or the point of
maximum margin and placing it in its own certain set. We
then iteratively select xT as the point of maximum margin
that (1) is not in any certain set and (2) has a different
cluster estimate from all points in the certain sets. If no
such point exists, we choose uniformly at random from all
points not in any certain set. This point is queried against
a single representative from each certain set according to
the UoS model as above until either a must-link is found
or all set representatives have been queried, in which case
xT is added to a new certain set. This process is repeated
until either K certain sets have been created or a terminal
number of queries have been used. As points of maximum
margin are more likely to be correctly clustered than other

110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164

165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219

Leveraging Union of Subspace Structure to Improve Constrained Clustering

points in the set, we expect that by choosing points whose
estimated labels indicate they do not belong to any current
certain set, we will quickly ﬁnd a point with no must-link
constraints. In our simulations, we found that this algorithm
ﬁnds at least one point from each cluster in nearly the lower
limit of K(K − 1)/2 queries on the Yale dataset.

References

Basu, Sugato, Banerjee, Arindam, and Mooney, Raymond J.
Active semi-supervision for pairwise constrained cluster-
ing. In Proc. SIAM Int. Conf. on Data Mining, 2004.

Vershynin, Roman. A Course in High Dimensional Prob-
ability. 2016. URL www-personal.umich.edu/
˜romanv/teaching/2015-16/626/HDP-book.
pdf.

220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274

275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329

