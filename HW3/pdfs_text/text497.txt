Innovation Pursuit: A New Approach to the Subspace Clustering Problem

Mostafa Rahmani 1 George Atia 1

Abstract

This paper presents a new scalable approach,
termed Innovation Pursuit (iPursuit), to the prob-
iPursuit rests on a
lem of subspace clustering.
new geometrical idea whereby each subspace is
identiﬁed based on its novelty with respect to the
other subspaces. The subspaces are identiﬁed
consecutively by solving a series of simple lin-
ear optimization problems, each searching for a
direction of innovation in the span of the data. A
detailed mathematical analysis is provided estab-
lishing sufﬁcient conditions for the proposed ap-
proach to correctly cluster the data points. More-
over, the proposed direction search approach can
be integrated with spectral clustering to yield
a new variant of spectral-clustering-based algo-
rithms. Remarkably, the proposed approach can
provably yield exact clustering even when the
subspaces have signiﬁcant intersections. The nu-
merical simulations demonstrate that iPursuit can
often outperform the state-of-the-art subspace
clustering algorithms – more so for subspaces
with signiﬁcant intersections – along with sub-
stantial reductions in computational complexity.

1. Introduction

Principal Component Analysis (PCA) is a popular and ef-
ﬁcient procedure to approximate the data with a single low
dimensional subspace (Lerman et al., 2015). Nonetheless,
in numerous contemporary applications the data points
may originate from multiple independent sources, in which
case a union of subspaces can better model the data (Vidal,
2011). The problem of subspace clustering is concerned
with learning these low-dimensional subspaces and cluster-
ing the data points to their respective subspaces, generally
without prior knowledge about the number of subspaces
and their dimensions, nor the membership of the data points

1University of Central Florida, Orlando, Florida, USA. Corre-

spondence to: Mostafa Rahmani <mostafa@knights.ucf.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

to these subspaces. Subspace clustering naturally arises
in many machine learning and data analysis problems, in-
cluding computer vision (e.g. motion segmentation (Vidal
et al., 2008), face clustering (Ho et al., 2003)), image pro-
cessing (Yang et al., 2008) and system identiﬁcation (Vidal
et al., 2003). Numerous approaches for subspace clustering
have been studied in prior work, including statistical-based
approaches (Yang et al., 2006; Rao et al., 2010), spec-
tral clustering (Soltanolkotabi et al., 2012; Von Luxburg,
2007; Dyer et al., 2013; Elhamifar & Vidal, 2013; Heckel
& B¨olcskei, 2013; Liu et al., 2013; Chen & Lerman, 2009),
the algebraic-geometric approach (Vidal et al., 2005) and
iterative methods (Bradley & Mangasarian, 2000). We re-
fer the reader to (Vidal, 2011) for a comprehensive survey
on the topic.

This paper aims to advance the state-of-the-art research on
subspace clustering on several fronts. First, the proposed
approach – termed iPursuit – rests on a novel geometrical
idea whereby one subspace is identiﬁed at a time based on
its novelty with respect to (w.r.t.) the other subspaces. Sec-
ond, the proposed method is a provable and scalable sub-
space clustering algorithm – the computational complex-
ity of iPursuit only scales linearly in the number of sub-
spaces and quadratically in their dimensions (c.f. Section
2.5). In contrast to the spectral-clustering-based algorithms
such as (Dyer et al., 2013; Elhamifar & Vidal, 2013; Liu
et al., 2013), which need to solve an M 2
2 -dimensional op-
timization problem to build the similarity matrix (where
M2 is the number of data points), the proposed method
requires solving few M2-dimensional linear optimization
problems. This feature makes iPursuit remarkably faster
than the state-of-the-art algorithms. Third, innovation pur-
suit in the data span enables superior performance when the
subspaces have considerable intersections in comparison to
the state-of-the-art subspace clustering algorithms.

1.1. Notation and deﬁnitions

Given a matrix A, (cid:107)A(cid:107) denotes its spectral norm. For a
vector a, (cid:107)a(cid:107) denotes its (cid:96)2-norm and (cid:107)a(cid:107)1 its (cid:96)1-norm.
Given matrices {Ai}n
i=1 with equal number of rows, we
:=
use the union symbol ∪ to deﬁne the matrix

Ai

n
∪
i=1

[A1 A2
{Ai}n

... An] as the concatenation of the matrices
i=1. For a matrix D, we overload the set member-

Submission and Formatting Instructions for ICML 2017

that the given data matrix follows the following data model.

Data Model 1. The data matrix D ∈ RM1×M2 can
be represented as D = [D1 ... DN ]T, where T is an arbi-
trary permutation matrix. The columns of Di ∈ RM1×ni
lie in Si, where Si is an ri-dimensional linear subspace,
for 1 ≤ i ≤ N , and, (cid:80)N
i=1 ni = M2. Deﬁne Vi as an
In addition, deﬁne D as the
orthonormal basis for Si.
space spanned by the data, i.e., D =

Si. Moreover,
it is assumed that every subspace in the set of subspaces
{Si}N
i=1 has an innovation over the other subspaces,
to say that, for 1 ≤ i ≤ N , the subspace Si does not

N
⊕
i=1

completely lie in

Sk . In addition, the columns of D are

N
⊕
k=1
k(cid:54)=i

normalized, i.e., each column has an (cid:96)2-norm equal to one.

2.1. iPursuit: Basic idea

iPursuit is a multi-step algorithm that identiﬁes one sub-
space at a time. In each step, the data is clustered into two
subspaces. One subspace is the identiﬁed subspace and the
other one is the direct sum of the other subspaces. The
data points of the identiﬁed subspace are removed and the
algorithm is applied to the remaining data to ﬁnd the next
subspace. Accordingly, each step of the algorithm can be
interpreted as a subspace clustering problem with two sub-
spaces. Therefore, for ease of exposition we ﬁrst inves-
tigate the two-subspace scenario then extend the result to
multiple (more than two) subspaces. Thus, in this subsec-
tion, it is assumed that the data follows Data model 1 with
N = 2.

To gain some intuition, we ﬁrst consider a simple exam-
ple before stating our main result. Suppose that S1 and
S2 are not orthogonal and assume that n2 < n1. The
non-orthogonality of S1 and S2 is not a requirement, but
is merely used herein to simplify the exposition of the ba-
sic idea underlying the proposed approach. Deﬁne c∗ as
the optimal point of the following optimization problem

min
ˆc

(cid:107)ˆcT D(cid:107)0

subject to

ˆc ∈ D and (cid:107)ˆc(cid:107) = 1,

(1)

where (cid:107).(cid:107)0 is the (cid:96)0-norm. The ﬁrst constraint limits the
search space to the span of the data, and the equality con-
straint (cid:107)ˆc(cid:107) = 1 is used to avoid the trivial ˆc = 0 solution.
Assume that the columns of D1 and D2 are uniformly dis-
tributed in S1 and S2, respectively. Accordingly, with high
probability (whp) the data is not aligned along any speciﬁc
direction in S1 and S2.

The (cid:96)0-norm minimization problem (1) searches for a non-
zero vector in D that is orthogonal to the maximum number
of data points. Since c∗ has to lie in D, we claim that the
optimal point of (1) lies in I (S2 ⊥ S1) whp given the as-
sumption that the number of data points in S1 is greater

Figure 1. The subspace I (S2 ⊥ S1) is the innovation subspace
of S2 over S1. The subspace I (S2 ⊥ S1) is orthogonal to S1.

(cid:19)

Gi

(cid:18) n
⊕
i=1

said to be independent if dim

ship operator by using the notation d ∈ D to signify that
d is a column of D. A collection of subspaces {Gi}n
i=1 is
i=1 dim(Gi),
where ⊕ denotes the direct sum operator and dim(Gi) is
the dimension of Gi. Given a vector a, (cid:12)
(cid:12)a| is the vector
of absolute values of the elements of a. For a real number
a, sgn(a) denotes the sign of a. The complement of a set
L is denoted Lc. For any positive integer n, the index set
{1, . . . , n} is denoted [n].

= (cid:80)n

Consider two subspaces S1 and S2, such that S2 (cid:54)⊆ S1
and S1 (cid:54)⊆ S2. This means that each of the subspaces S1
and S2 carries some innovation w.r.t. the other. As such,
corresponding to each subspace we deﬁne an innovation
subspace capturing its novelty (innovation) w.r.t. the other
subspaces, deﬁned formally as follows.
Deﬁnition 1. Assume that V1 and V2 are two orthonor-
mal bases for S1 and S2, respectively. We deﬁne the in-
novation subspace of S2 over S1, denoted I (S2 ⊥ S1),
as the subspace spanned by (cid:0)I − V1VT
(cid:1) V2. In other
words, I (S2 ⊥ S1) is the complement of S1 in the sub-
space S1 ⊕ S2.

1

Similarly, we can also deﬁne I (S1 ⊥ S2) as the innova-
tion subspace of S1 over S2. Fig. 1 illustrates a scenario
in which the data lies in a union of a two-dimensional and
a one-dimensional subspace. Note that the innovation sub-
space of S2 over S1 is orthogonal to S1 and is the comple-
ment of S1 in S1 ⊕ S2.

2. Proposed Approach

In this section, the main geometrical idea underlying iPur-
suit is ﬁrst presented. This idea is based on a non-convex
(cid:96)0-norm minimization problem searching for a direction
of innovation in the span of the data. Then, we provide a
convex relaxation to a linear optimization problem, whose
solution is shown to yield the correct subspaces under mild
sufﬁcient conditions. Due to space limitations, the proofs
of all the theoretical results are deferred to an extended ver-
sion of this paper (Rahmani & Atia, 2015). It is assumed

Submission and Formatting Instructions for ICML 2017

than the number of data points in S2. To clarify, consider
the following cases:

I. If c∗ ∈ S1, then it would not be orthogonal to the ma-
jority of the data points in S1 given that the columns of D1
are uniformly distributed in S1. In addition, it cannot be
orthogonal to most of the data points in S2 since S1 and
S2 are not orthogonal. Since the optimal vector should be
orthogonal to the maximum number of data points, it is
highly likely that c∗ (cid:54)∈ S1. Similarly, it is highly unlikely
that the optimal point lies in S2. II. If c∗ ∈ I (S1 ⊥ S2),
then according to Deﬁnition 1, it is orthogonal to the data
points in D2. However, since it was assumed that n2 < n1,
the cost function of (1) can be decreased if c∗ lies in
I (S2 ⊥ S1) (which is orthogonal to S1). III. If c∗ does
not lie in any of the subspaces S1, S2, I (S2 ⊥ S1) and
I (S2 ⊥ S1), then it is neither orthogonal to S1 nor to S2.
Now, since the data points are distributed uniformly in the
subspaces, we see that c∗ would not be orthogonal to the
maximum number of data points.

Hence, it is highly likely that c∗ lies in I (S2 ⊥ S1).
It
follows that the columns of D corresponding to the non-
zero elements of (c∗)T D lie in S2. The following lemma
ensures that these columns span S2.
Lemma 1. The columns of D corresponding to the non-
zero elements of (c∗)T D span S2 if the following condi-
tions are satisﬁed: (i) c∗ ∈ I (S2 ⊥ S1) , (ii) D2 cannot
follow Data model 1 with N > 1, that is, the data points in
D2 do not lie in the union of lower dimensional subspaces
within S2 each with innovation w.r.t. the other subspaces.

It is important to note that the conditions of Lemma 1 are
by no means restrictive. Speciﬁcally, if the requirement (ii)
of Lemma 1 is not satisﬁed, then the problem can be viewed
as a subspace clustering problem with more than two sub-
spaces. In Section 2.4, we will investigate the clustering
problem with more than two subspaces.

Remark 1. At a high level, the innovation search prob-
lem (1) ﬁnds the most sparse vector in the row space of
D. Interestingly, ﬁnding the most sparse vector in a lin-
ear subspace has bearing on, and has been effectively used
in, other machine learning problems, including dictionary
learning and spectral estimation (Qu et al., 2014).

2.2. Convex relaxation

The (cid:96)1-norm is known to provide an efﬁcient convex relax-
ation of the (cid:96)0-norm. Thus, we relax the non-convex cost
function and rewrite (1) as

min
ˆc

(cid:107)ˆcT D(cid:107)1

subject to

ˆc ∈ D and (cid:107)ˆc(cid:107) = 1 .

(2)

Since the feasible set of (2) is non-convex, we further sub-
stitute the equality constraint with a linear constraint to

consider the following convex program

(IP) min

ˆc

(cid:107)ˆcT D(cid:107)1

s. t.

ˆc ∈ D and

ˆcT q = 1.

(3)

(IP) is the core program of iPursuit to ﬁnd a direction of
innovation. The vector q is a unit (cid:96)2-norm vector that
should not be orthogonal to D. In Section 2.6, we present a
methodology to obtain a good choice for the vector q from
the given data. However, our investigations have shown
that iPursuit performs well generally even when q is cho-
sen as a random vector in D.

2.3. Segmentation of two subspaces: Performance

guarantees

Suppose that D follows Data model 1 with N = 2, i.e.,
the data lies in a union of two subspaces. In order to show
that the optimal point of (IP) yields correct clustering, it
sufﬁces to show that the optimal point lies in I (S2 ⊥ S1)
given that condition (ii) of Lemma 1 is satisﬁed for D2 (or
lies in I (S1 ⊥ S2) given that the condition is satisﬁed for
D1). The following theorem provides sufﬁcient conditions
for the optimal point of (3) to lie in I (S2 ⊥ S1) provided
that

inf
c∈I(S2⊥S1)
cT q=1

(cid:107)cT D(cid:107)1 <

(cid:107)cT D(cid:107)1.

(4)

inf
c∈I(S1⊥S2)
cT q=1

If the inequality in (4) is reversed, then similar sufﬁcient
conditions can be established for the optimal point of (3)
to lie in I (S1 ⊥ S2). Hence, assumption (4) does not
lead to any loss of generality. The subspaces I (S2 ⊥ S1)
and I (S1 ⊥ S2) are orthogonal to S1 and S2, respectively.
Thus, (4) is equivalent to

inf
c∈I(S2⊥S1 )
cT q=1

(cid:107)cT D2(cid:107)1 <

(cid:107)cT D1(cid:107)1.

(5)

inf
c∈I(S1⊥S2)
cT q=1

Henceforth, “innovation subspace” refers to I (S2 ⊥ S1)
whenever the two-subspace scenario is considered and (5)
is satisﬁed. Theorem 2 stated next provides sufﬁcient
conditions for the optimal point of (IP) in (3) to lie in
I (S2 ⊥ S1). These conditions are characterized in terms
of the optimal solution to an oracle optimization problem
(OP), wherein the feasible set of (IP) is replaced by the in-
novation subspace. Deﬁne c2 as the optimal point of the
following optimization problem

(OP)

subject to

(cid:107)ˆcT D2(cid:107)1

min
ˆc
ˆc ∈ I (S2 ⊥ S1) and

(6)

ˆcT q = 1.

Theorem 2 establishes that c2 is the optimal point of (3).
Before we state the theorem, we deﬁne the index set L0 :=
{i ∈ [n2] : cT
2 di = 0, di ∈ D2}, with cardinality n0 =
|L0| and a complement set Lc
0, comprising the indices of
the columns of D2 orthogonal to c2.

Submission and Formatting Instructions for ICML 2017

Theorem 2. Suppose the data matrix D follows Data
model 1 with N = 2. Also, assume that condition (5) and
the requirement of Lemma 1 for D2 are satisﬁed (condi-
tion (ii) of Lemma 1). Let c2 be the optimal point of the
oracle (OP) in (6) and deﬁne α = (cid:80)
2 di)di.
Also, let P2 denote an orthonormal basis for I (S2 ⊥ S1)
and assume that q is a unit (cid:96)2-norm vector in D that is not
orthogonal to I (S2 ⊥ S1). If

sgn(cT

di∈D2
i∈Lc
0

(cid:88)

(cid:12)
(cid:12)δT di

(cid:12)
(cid:12) > (cid:107)VT

1 V2(cid:107)

(cid:18)

(cid:19)

(cid:107)α(cid:107) + n0

, and

1
2

inf
δ∈S1
(cid:107)δ(cid:107)=1

di∈D1
(cid:32)

(cid:107)qT P2(cid:107)
2(cid:107)qT V1(cid:107)

(cid:88)

(cid:12)
(cid:12)δT di

(cid:12)
(cid:12)

inf
δ∈S1
(cid:107)δ(cid:107)=1

di∈D1

(cid:33)

(cid:18)

(cid:19)

> (cid:107)VT

2 P2(cid:107)

(cid:107)α(cid:107) + n0

,

(7)

then c2 ∈ I (S2 ⊥ S1) is the optimal point of (IP) in (3),
and iPursuit clusters the data correctly.

In what follows, we provide a detailed discussion of the
signiﬁcance of the sufﬁcient conditions (7) of Theorem 2,
which reveal some interesting facts about the properties of
iPursuit.

1. Data distribution: The LHS of (7) is known as the per-
meance statistic, an efﬁcient measure of how well the data
points are distributed in a subspace (Lerman et al., 2015).
As such, the sufﬁcient conditions (7) imply that the distri-
bution and the number of the data points within the sub-
spaces are important performance factors for iPursuit. For
a set of data points Di in a subspace Si, the permeance
(cid:12)
(cid:12) .
statistic is deﬁned as P(Di, Si) = inf
u∈Si
(cid:107)u(cid:107)=1

(cid:12)
(cid:12)uT di

di∈Di

(cid:80)

From this deﬁnition, we see that the permeance statistic is
fairly small if a set of data points are aligned along a given
direction (i.e. not well distribued). In addition, having n0
on the RHS reveals that the distribution of the data points
within S2 also matters since c2 cannot be simultaneously
orthogonal to a large number of columns of D2 if the data
does not align along speciﬁc directions. Hence, according
to (7), iPursuit yields correct clustering if the data is well
distributed within the subspaces.

2. The coherency of q with I (S2 ⊥ S1): For the optimal
point of (IP) to lie in I (S2 ⊥ S1), the vector q should not
be too coherent with S1. This can be seen by observing that
if q has a small projection on I (S2 ⊥ S1) – in which case
it would be more coherent with S1 – the Euclidean norm
of any feasible point of (3) lying in I (S2 ⊥ S1) will have
to be large to satisfy the equality constraint in (IP). Such
points are less likely to be optimal in the sense of attaining
the minimum of the objective function in (3). The afore-
mentioned intuition is afﬁrmed by the analysis.
Indeed,
the factor (cid:107)qT P2(cid:107)/(cid:107)qT V1(cid:107) in the sufﬁcient conditions of
Theorem 2 and Lemma 3 indicates that the coherency of q

r1

E[(cid:107)qT P2(cid:107)]
E[(cid:107)qT V1(cid:107)] = r2−y

with the innovation subspace is an important performance
factor for iPursuit. The coherence property could have a
more serious effect on the performance of the algorithm for
non-independent subspaces, especially when the dimension
of their intersection is signiﬁcant. For instance, consider
the scenario where the vector q is chosen randomly from
D, and deﬁne y as the dimension of the intersection of S1
and S2. It follows that I (S2 ⊥ S1) has dimension r2 − y.
. Therefore, a randomly chosen
Thus,
vector q is likely to have a small projection on the inno-
vation subspace when y is large. As such, in dealing with
subspaces with signiﬁcant intersection, it may not be favor-
able to choose the vector q at random. In Section 2.6 and
section 2.7, we develop a simple technique to learn a good
choice for q from the given data. It makes iPursuit remark-
ably powerful in dealing with subspaces with intersection
as shown in the numerical results section.
Remark 2. We conjecture that if the ratios {ni/ri}N
i=1 are
sufﬁciently large, the optimal point of (2) always lies in an
innovation subspace; our investigations have shown that
the optimal point of (IP) always lies in an innovation sub-
space provided that q is sufﬁciently coherent with the in-
novation subspace regardless how large the intersections
between the subspaces are. This is particularly compelling
if the subspaces are too close, in which case spectral-
clustering-based methods can seldom construct a correct
similarity matrix (which leads to large clustering errors).
By contrast, in such cases iPursuit can yield accurate clus-
tering provided that the constraint vector is sufﬁciently co-
In Section 2.6, we
herent with the innovation subspace.
present a method to identify a coherent constraint vector.

Now, we demonstrate that the sufﬁcient conditions (7) are
not restrictive. The following lemma simpliﬁes the condi-
tions when the data points are randomly distributed in the
subspaces. In this setting, we show that the conditions are
naturally satisﬁed.
Lemma 3. Assume that the distribution of the columns of
D1 in S1 and D2 in S2 is uniformly random and consider
the same setup of Theorem 2. If
(cid:114) n1

√

− 2

n1 − t1

(cid:114) 2
π

n1
r1

r1 − 1
(cid:18)
√

> 2(cid:107)VT

1 V2(cid:107)

t2

n2 − n0 + n0

,

(cid:107)qT P2(cid:107)
(cid:107)qT V1(cid:107)

(cid:32)(cid:114) 2
π

n1
r1

√

− 2

n1 − t1

(cid:114) n1

(cid:33)

r1 − 1

(8)

(cid:19)

(cid:19)

(cid:18)

√

> 2(cid:107)VT

2 P2(cid:107)

t2

n2 − n0 + n0

,

then the optimal point of (3) lies in I (S2 ⊥ S1) with
2) − 1)(cid:1) −
probability at least 1 − exp (cid:0)− r2
, for all t2 > 1 , t1 ≥ 0.
exp

2 − log(t2

2 (t2

− t2

(cid:16)

(cid:17)

1
2

Submission and Formatting Instructions for ICML 2017

When the data points are not aligned along any speciﬁc
directions, c2 can only be simultaneously orthogonal to a
small number of columns of D2. Thus, n0 will be much
smaller than n2. The LHS of (8) has order n1 and the
n2 + n0 (which is much smaller than n2).
RHS has order
Therefore, the sufﬁcient conditions are naturally satisﬁed
when the data is well distributed within the subspaces.

√

2.4. Clustering multiple subspaces

Suppose that D follows Data Model 1 with N = m where
m > 2. Similar to (5), without loss of generality assume
that

(cid:107)cT Dm(cid:107)1 <

(cid:107)cT Dj(cid:107)1

(9)

(cid:0)

c∈I

inf
Sm⊥

m
⊕
k=1
k(cid:54)=m
cT qm=1

(cid:1)

Sk

c∈I

(cid:1)

Sk

(cid:0)

inf
Sj ⊥

m
⊕
k=1
k(cid:54)=j
cT qm=1

for all 1 ≤ j ≤ m − 1 , where qm is a unit (cid:96)2-norm in
⊕m
k=1Sk. Given (9), we expect the optimal point of (IP)
with q = qm to lie in the innovation subspace of Sm over
m
(cid:1), in which case Sm will be
Sk
⊕
k=1
k(cid:54)=m
identiﬁed. Accordingly, in this step the clustering prob-

Sk, i.e., I(cid:0)Sm ⊥

m
⊕
k=1
k(cid:54)=m

Si). Theorem
lem separates (Dm, Sm) and (
2 can be used to derive sufﬁcient conditions for the optimal
(cid:1) by substituting (D2, S2)
point to lie in I(cid:0)Sm ⊥
Sk

Di ,

m−1
∪
i=1

m−1
⊕
i=1

m
⊕
k=1
k(cid:54)=m

Si). Con-
with (Dm, Sm) and (D1, S1) with (
sequently, we can also establish sufﬁcient conditions for
exact subspace segmentation. Due to space limitations, we
defer the analysis to (Rahmani & Atia, 2015).

Di ,

m−1
∪
i=1

m−1
⊕
i=1

Table 1. Run time of different algorithms (M1 = 50, N = 3,
{ri}3

i=1 = 10, {ni}3
i=1 = M2/3)
M2
LRR
SSC
0.9 s
1.1 s
300
26 s
209 s
3000
15000 > 2 h
2800
30000 > 2 h > 2 h

iPursuit
0.14 s
0.35 s
2.78 s
10.6 s

SSC-OMP
1.6 s
56 s
5340 s
> 2 h

Remark 3. The proposed method brings about substan-
tial speedups over existing algorithms due to the following:
i) unlike existing multi-step algorithms (such as RANSAC)
which have exponential complexity in the number and di-
mension of subspaces, the complexity of iPursuit is linear
in the number of subspaces and quadratic in their dimen-
sion. In addition, while iPursuit has linear complexity in
M2, spectral-clustering-based algorithms have complexity
O(M 2
2 N ) for their spectral clustering step plus the com-
plexity of obtaining the similarity matrix; ii) more impor-
tantly, the solver of the proposed optimization problem has
O(rM2) complexity per iteration, while the other opera-
tions – whose complexity are O(r2M2) and O(r3) – sit
outside of the iterative solver. This feature makes the pro-
posed method notably faster than most of the existing al-
gorithms which solve high-dimensional optimization prob-
lems. For instance, solving the optimization problem of the
SSC algorithm has roughly O(M 3
2 + rM2) complexity per
iteration (Elhamifar & Vidal, 2013).
For instance, suppose M1 = 100, the data lies in a union of
three 10-dimensional subspaces and ni = M2/3. Table 1
compares the running time of the algorithms. One can ob-
serve that iPursuit is remarkably faster. More running time
comparisons are available in (Rahmani & Atia, 2015).

2.5. Complexity analysis

2.6. How to choose the vector q?

is

equivalent

optimization
(cid:107)aT UT D(cid:107)1

problem (3)
subject to aT UT q = 1.

Deﬁne U as an orthonormal basis for D.
Thus,
to
the
min
Fur-
a
ther, deﬁne f = UT q. This optimization problem can be
efﬁciently solved using the Alternating Direction Method
of Multipliers (ADMM) (Boyd et al., 2011). Due to space
constraints, we defer the details of the iterative solver
to (Rahmani & Atia, 2015).
The complexity of the
initialization step of the solver is O(r3) plus the com-
plexity of obtaining U. Obtaining an appropriate U has
O(r2M2) complexity by applying the clustering algorithm
to a random subset of the rows of D (with the rank of
sampled rows equal to r). In addition, the complexity of
each iteration of the solver is O(rM2). Thus, the overall
complexity is less than O((r3 + r2M2)N ) since the
number of data points remaining keeps decreasing over
the iterations. In most cases, r (cid:28) M2, hence the overall
complexity is roughly O(r2M2N ).

Our investigations have shown that iPursuit performs very
well when the subspaces are independent or have small in-
tersections even if q is chosen randomly. However, in the
more challenging scenarios in which the dimensions of the
intersections between the subspaces are signiﬁcant, ran-
domly choosing the vector q could be unfavorable since
the dimension of the innovation subspace decreases as the
dimension of the intersection increases. This motivates
the methodology described next that aims to search for a
“good” vector q. Consider the optimization problem

min
ˆq

(cid:107)ˆqT D(cid:107)2

subject to

ˆq ∈ D and

(cid:107)ˆq(cid:107) = 1, (10)

which searches for a non-zero vector in D with small pro-
jections on the columns of D. It is straightforward to show
that the optimal point of (10) is the singular vector corre-
sponding to the least non-zero singular value of D. When
the subspaces are close to each other, it is not hard to see

Submission and Formatting Instructions for ICML 2017

that the least singular vector is highly coherent with the in-
novation subspace, thus can be a good candidate for the
vector q. For subspaces with remarkable intersections, this
choice of q brings about substantial improvement in per-
formance compared to using a randomly generated q (cf.
Section 3). Clearly, when the data is noisy, we utilize the
least dominant singular vecor. In addition, when the singu-
lar values of the noisy data decay rapidly, it may be hard
to accurately estimate the rank of D, which may lead to an
unfavorable use of a singular vector corresponding to noise
as the constraint vector. Alternatively, we can choose the
data point closest to the least dominant singular vector as
our vector q. This technique makes the proposed method
robust to the presence of noise (cf. Section 2.7).

2.7. Noisy data

In the presence of additive noise, we model the data as
De = D + E , where De is the given noisy data matrix,
D is the clean data which follows Data model 1 and E rep-
resents the noise component. The rank of D is equal to
r. Thus, the singular values of De can be divided into two
subsets: the dominant singular values (the ﬁrst r singular
values) and the small singular values (or the singular val-
ues corresponding to the noise component). Estimating the
number of dominant singular values is a fairly well-studied
topic (Stoica & Selen, 2004).

Consider the optimization problem (IP) using De, i.e.,

min
ˆc

(cid:107)ˆcT De(cid:107)1

s.t.

ˆc ∈ span(De) and ˆcT q = 1. (11)

Clearly, the optimal point of (11) is very close to the sub-
space spanned by the singular vectors corresponding to the
small singular values. Thus, if ce denotes the optimal so-
lution of (11), then all the elements of cT
e De will be fairly
small and the subspaces cannot be distinguished. However,
the span of the dominant singular vectors is approximately
equal to D. Accordingly, we propose the following approx-
imation to (IP),

min
ˆc

(cid:107)ˆcT De(cid:107)1

s.t.

ˆc ∈ span(Q) and ˆcT q = 1 (12)

where Q is an orthonormal basis for the span of the dom-
inant singular vectors. The ﬁrst constraint of (12) forces
the optimal point to lie in span(Q), which serves as a
good approximation to span(D). For instance, consider
D = [D1 D2], where the columns of D1 ∈ R40×100
lie in a 5-dimensional subspace S1, and the columns of
D2 ∈ R40×100 lie in another 5-dimensional subspace S2.
Deﬁne ce and cr as the optimal points of (11) and (12), re-
spectively. Fig. 2 shows |cT
e De| and |cT
r De| with the max-
imum element scaled to one. Clearly, cT
r De can be used to
correctly cluster the data. In addition, when D is low rank,
the subspace constraint in (12) can ﬁlter out a remarkable
portion of the noise component.

Figure 2. The left plot shows the output of (11), while the right
plot shows the output of iPursuit when its search domain is re-
stricted to the subspace spanned by the dominant singular vectors
as per (12).

When the data is noisy and the singular values of D de-
cay rapidly, it may be hard to accurately estimate r. If the
dimension is incorrectly estimated, Q may contain some
singular vectors corresponding to the noise component,
wherefore the optimal point of (12) could end up lying
close to a noise singular vector. In the sequel, we present
two effective techniques to effectively avoid this undesir-
able scenario.

1. Using a data point as a constraint vector: A singu-
lar vector corresponding to the noise component is nearly
orthogonal to the entire data, i.e., has small projection on
all the data points. Thus, if the optimal vector is forced to
have strong projection on a data point, it will be unlikely for
the optimal direction to be close to a noise singular vector.
Thus, we modify (12) as follows

min
ˆc

(cid:107)ˆcT De(cid:107)1

s.t.

ˆc ∈ span(Q) and ˆcT dek = 1 , (13)

where dek is the kth column of De. The modiﬁed con-
straint in (13) ensures that the optimal point is not orthogo-
nal to dek. If dek lies in the subspace Si, the optimal point
of (13) will lie in the innovation subspace corresponding
to Si whp.
In order to determine a good data point for
the constraint vector, we leverage the principle presented
in section 2.6. Speciﬁcally, we use the data point that is
closest to the least dominant singular vector rather than the
least dominant singular vector itself.

2. Sparse representation of the optimal point: When D
is low rank, i.e., r (cid:28) min(M1, M2), any direction in the
span of the data – including the optimal direction sought by
iPursuit – can be represented as a sparse combination of the
data points. For such settings, we propose the alternative
optimization problem

(cid:107)aT QT De(cid:107)1 + γ(cid:107)z(cid:107)1

min
a,z
subject to a = QT De z and aT QT dek = 1 ,

(14)

where γ is a regularization parameter. Forcing a sparse
representation in (14) for the optimal direction averts a so-
lution that lies in close proximity with the small singular
vectors, which are normally obtained through linear com-
binations of a large number of data points. This alternative

Submission and Formatting Instructions for ICML 2017

random within the subspace. Let cr = (cid:107)qT P2(cid:107)/(cid:107)qT V1(cid:107).
Thus, cr captures the coherency of q with the innovation
subspace. Deﬁne ˆV1 and ˆV2 as orthonormal bases for the
identiﬁed subspaces. A trial is considered successful if

(cid:107)(I − V1VT

1 ) ˆV1(cid:107)F + (cid:107)(I − V2VT

2 ) ˆV2(cid:107)F ≤ 10−3 . (15)

The left plot of Fig. 3 shows the phase transition in the
plane of cr and y, where y is the dimension of the inter-
section of the two subspaces. In this ﬁgure, white desig-
nates exact identiﬁcation of the subspaces with probability
almost equal to one. As shown, the probability of correct
clustering increases if cr is increased. Remarkably, the left
plot of Fig. 3 shows that when cr is sufﬁciently large, the
algorithm yields exact segmentation even when y = 14.

formulation is particularly useful when the dimension of
the data cannot be accurately estimated. When D is not
a low rank matrix, we can set γ equal to zero. The table
of Algorithm 1 details the proposed method for noisy data
along with the used notation and deﬁnitions.

2.7.1. ERROR PROPAGATION

If κ (or ci) and the threshold co in Algorithm 1 are cho-
sen appropriately, the algorithm exhibits strong robustness
in the presence of noise. Nonetheless, if the data is too
noisy, an error incurred in one step of the algorithm may
propagate and unfavorably affect the performance in sub-
sequent steps. Two types of error could occur. The ﬁrst
type is that some data points are erroneously included in
G1 or G2. An example is when Sm is the subspace to be
identiﬁed in a given step of the algorithm (i.e., the optimal
point of (13) lies in the innovation subspace corresponding
to Sm), but few data points from the other subspaces are er-
roneously included in G1. The second type of error is that
some of the data points remain unidentiﬁed. For instance,
Sm is to be identiﬁed in a given iteration, yet not all the data
points belonging to Sm are identiﬁed. In an extended ver-
sion of this work (Rahmani & Atia, 2015), we discuss the
two main sources of error and present some techniques to
effectively neutralize their impact on subsequent iterations.
In addition, a brief discussion about handling the presence
of outliers is provided.

2.8. Subspace clustering using direction search and

spectral clustering

In (Rahmani & Atia, 2017), we showed that the direc-
tion search optimization problem (13) can be utilized to
ﬁnd a neighborhood set for the kth data point. We lever-
aged this feature to propose a new spectral-clustering-
based subspace segmentation algorithm, dubbed Direction
search based Subspace Clustering (DSC) (Rahmani & Atia,
2017). We showed that DSC often outperforms the exist-
ing spectral-clustering-based methods particularly for hard
scenarios involving high levels of noise and close sub-
spaces, and notably improves the state-of-the-art result for
the challenging problem of face segmentation using sub-
space clustering.

3. Numerical Simulations

3.1. The coherency parameter

In this experiment, we examine the impact of the coherency
of q with the innovation subspace. Here, it is assumed that
the data follows Data Model 1 with N = 2 and M1 = 50.
The dimension of the subspaces is equal to 15 and the di-
mension of their intersection varies between 0 to 14. Each
subspace contains 100 data points distributed uniformly at

Figure 3. Left: Phase transition plot in the plane of the coherency
parameter and the dimension of intersection. Right: The cluster-
ing error of iPursuit, SSC and LRR versus the dimension of the
intersection.

3.2. Clustering data in union of multiple subspaces

In this simulation, we consider the subspace clustering
problem with 15 30-dimensional subspaces {Si}15
i=1 and
M1 = 500. Each subspace contains 90 data points and the
distribution of the data within the subspaces is uniformly
random. We compare the performance of the proposed
approach to the state-of-the-art sparse subspace clustering
(SSC) (Elhamifar & Vidal, 2013) algorithm and low rank
representation (LRR) based clustering (Liu et al., 2013).
The number of replicates used in spectral clustering for
SSC and LRR is equal to 20. Deﬁne the clustering error
as the ratio of misclassiﬁed points to the total number of
data points. The right plot of Fig. 3 shows the clustering
error versus the dimension of the intersection. The dimen-
sion of intersection varies between 1 and 29. Each point in
the plot is obtained by averaging over 40 independent runs.
iPursuit is shown to yield the best performance.

3.3. Noisy data

In this section, we study the performance of the proposed
approach, SSC, LRR, SCC (Chen & Lerman, 2009), TSC
(Heckel & B¨olcskei, 2013) and SSC-OMP (Dyer et al.,

Submission and Formatting Instructions for ICML 2017

Table 2. CE (%) of algorithms on Hopkins155 dataset (Mean - Median).

N
N = 2
N = 3

SSC
1.52 - 0
4.40 - 1.56 s

LRR
2.13 - 0
4.03 - 1.43

iPursuit
3.33 - 0.27
6.91 - 2.44

SSC-OMP
16.92 - 12.77 s
27.96 - 30.98

TSC
18.44 - 16.92
28.58 - 29.67

K-ﬂats
13.62 - 10.65
14.07 - 14.18

SCC
2.06 - 0
6.37 - 0.21

Thus, the problem here is to cluster data lying in two or
three subspaces. Table 2 shows the clustering error (in per-
centage) for iPursuit, SSC, LRR, TSC, SSC-OMP and K-
ﬂats. We use the results reported in (Elhamifar & Vidal,
2013; Heckel & B¨olcskei, 2013; Vidal, 2011; Park et al.,
2014). For SSC-OMP and TSC, the number of parameters
for motion segmentation are equal to 8 and 10. One can
observe that iPursuit yields competitive results comparable
to SSC, SCC, and LRR and outperforms TSC, SSC-OMP
and K-ﬂats.

.

of (14) and deﬁne h1 =

Algorithm 1 Innovation pursuit (iPursuit) for noisy data
Initialization Set κ, ˆn and ˆN as integers greater than 1, and set
ci and co as positive real numbers less than 1.
While The number of identiﬁed subspaces is less than ˆN or the
number of the columns of De is less than ˆn.
1. Obtaining the basis for the remaining Data: Construct Q as
the orthonormal matrix formed by the dominant singular vectors
of De.
2. Choosing the vector q: Set q = the column of De closest to
the last column of Q.
3. Solve (14) and deﬁne c∗ = Qa∗, where a∗ is the optimal point
(cid:12)
e c∗(cid:12)
(cid:12)DT
(cid:12)
e c∗(cid:12)
(cid:12)
(cid:12))
(cid:12)DT
max(
4. Finding a basis for the identiﬁed subspace: Construct G1 as
the matrix consisting of the columns of De corresponding to the
elements of h1 that are greater than ci. Alternatively, construct
G1 using the columns of De corresponding to the κ largest ele-
ments of h1. Deﬁne F1 as an orthonormal basis for the dominant
left singular vectors of G1.
5. Finding a basis for the rest of the data:
Deﬁne the vector h2 whose entries are equal to the (cid:96)2-norm of the
columns of (I − F1FT
1 )De. Scale h2 as h2 := h2/ max(h2).
Construct G2 as the columns of De corresponding to the elements
of h2 greater than co. Deﬁne F2 as an orthonormal basis for the
dominant left singular vectors of of G2.
6. Find the data point belonging to the identiﬁed subspace:
Assign dei to the identiﬁed subspace if (cid:107)FT
7. Remove the data points belonging to the identiﬁed sub-
space: Update De by removing the columns corresponding to
the identiﬁed subspace.

1 dei(cid:107) ≥ (cid:107)FT

2 dei(cid:107).

End While

Acknowledgment: This work was supported by NSF
CAREER Award CCF-1552497 and NSF Grant CCF-
1320547.

Figure 4. Performance of the algorithms versus the dimension of
intersection for different noise levels.

2013) with different noise levels, and varying dimensions
of the intersection between the subspaces, which gives rise
to both low rank and high rank data matrices. It is assumed
that D follows Data model 1 with M1 = 100, M2 = 500,
N = 6 and {ri}6
i=1 = 15. The dimension of the intersec-
tion between the subspaces varies from 0 to 14. Thus, the
rank of D ranges from 20 to 90. The Noisy data is mod-
eled as De = D + E, with the elements of E sampled in-
dependently from a zero mean Gaussian distribution. Fig.
4 shows the performance of the different algorithms ver-
sus the dimension of the intersection for τ = (cid:107)E(cid:107)F
equal
(cid:107)D(cid:107)F
to 1/20, 1/10, 1/5 and 1/2. One can observe that even
with τ = 1/5, iPursuit signiﬁcantly outperforms the other
algorithms. In addition, when the data is very noisy, i.e.,
τ = 1/2, it yields better performance when the dimen-
sion of the intersection is large. SSC, LRR, and SSC-OMP
yield a better performance for lower dimension of intersec-
tion. This is explained by the fact that the rank of the data
is high when the dimension of the intersection is low, and
the subspace projection operation QT De may not always
ﬁlter out the additive noise effectively.

3.4. Real data

We apply iPursuit to the problem of motion segmentation
using the Hopkins155 (Tron & Vidal, 2007) dataset, which
contains video sequences of 2 or 3 motions.
In motion
segmentation, each motion corresponds to one subspace.

Submission and Formatting Instructions for ICML 2017

References

Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
Eckstein, Jonathan. Distributed optimization and statisti-
cal learning via the alternating direction method of mul-
tipliers. Foundations and Trends® in Machine Learning,
3(1):1–122, 2011.

Bradley, Paul S and Mangasarian, Olvi L. k-plane cluster-
ing. Journal of Global Optimization, 16(1):23–32, 2000.

Chen, Guangliang and Lerman, Gilad. Spectral curvature
clustering (scc). International Journal of Computer Vi-
sion, 81(3):317–330, 2009.

Dyer, Eva L, Sankaranarayanan, Aswin C, and Baraniuk,
Richard G. Greedy feature selection for subspace clus-
tering. The Journal of Machine Learning Research, 14
(1):2487–2517, 2013.

Elhamifar, Ehsan and Vidal, Rene. Sparse subspace cluster-
ing: Algorithm, theory, and applications. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on, 35
(11):2765–2781, 2013.

Heckel, Reinhard and B¨olcskei, Helmut. Robust sub-
arXiv preprint

space clustering via thresholding.
arXiv:1307.4891, 2013.

Ho, Jason, Yang, Ming-Hsuan, Lim, Jongwoo, Lee, Kuang-
Chih, and Kriegman, David. Clustering appearances of
In Pro-
objects under varying illumination conditions.
ceedings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR), vol-
ume 1, 2003.

Rahmani, Mostafa and Atia, George. Innovation pursuit:
A new approach to subspace clustering. arXiv preprint
arXiv:1512.00907, 2015.

Rahmani, Mostafa and Atia, George. A direction search
and spectral clustering based approach to subspace clus-
tering. arXiv preprint, 2017.

Rao, Shankar, Tron, Roberto, Vidal, Rene, and Ma, Yi.
Motion segmentation in the presence of outlying, incom-
plete, or corrupted trajectories. Pattern Analysis and Ma-
chine Intelligence, IEEE Transactions on, 32(10):1832–
1845, 2010.

Soltanolkotabi, Mahdi, Candes, Emmanuel J, et al. A geo-
metric analysis of subspace clustering with outliers. The
Annals of Statistics, 40(4):2195–2238, 2012.

Stoica, Petre and Selen, Yngve. Model-order selection: a
review of information criterion rules. Signal Processing
Magazine, IEEE, 21(4):36–47, 2004.

Tron, Roberto and Vidal, Ren´e. A benchmark for the com-
parison of 3-d motion segmentation algorithms. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 1–8, 2007.

Vidal, Ren´e, Soatto, Stefano, Ma, Yi, and Sastry, Shankar.
An algebraic geometric approach to the identiﬁcation of
a class of linear hybrid systems. In Proceedings of the
42nd IEEE Conference on Decision and Control (CDC),
volume 1, pp. 167–172, 2003.

Vidal, Rene, Ma, Yi, and Sastry, Shankar. Generalized
principal component analysis (GPCA). Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on, 27
(12):1945–1959, 2005.

Lerman, Gilad, McCoy, Michael B, Tropp, Joel A, and
Zhang, Teng. Robust computation of linear models by
convex relaxation. Foundations of Computational Math-
ematics, 15(2):363–410, 2015.

Vidal, Ren´e, Tron, Roberto, and Hartley, Richard. Mul-
tiframe motion segmentation with missing data using
powerfactorization and GPCA. International Journal of
Computer Vision, 79(1):85–105, 2008.

Liu, Guangcan, Lin, Zhouchen, Yan, Shuicheng, Sun, Ju,
Yu, Yong, and Ma, Yi. Robust recovery of subspace
structures by low-rank representation. Pattern Analysis
and Machine Intelligence, IEEE Transactions on, 35(1):
171–184, 2013.

Park, Dohyung, Caramanis, Constantine, and Sanghavi,
In Advances in
Sujay. Greedy subspace clustering.
Neural Information Processing Systems, pp. 2753–2761,
2014.

Vidal, Rene. Subspace clustering. IEEE Signal Processing

Magazine, 2(28):52–68, 2011.

Von Luxburg, Ulrike. A tutorial on spectral clustering.

Statistics and computing, 17(4):395–416, 2007.

Yang, Allen Y, Rao, Shankar R, and Ma, Yi. Robust
statistical estimation and segmentation of multiple sub-
In Computer Vision and Pattern Recognition
spaces.
Workshop, 2006. CVPRW’06. Conference on, pp. 99–99.
IEEE, 2006.

Qu, Qing, Sun, Ju, and Wright, John. Finding a sparse vec-
tor in a subspace: Linear sparsity using alternating di-
rections. In Advances in Neural Information Processing
Systems, pp. 3401–3409, 2014.

Yang, Allen Y, Wright,

John, Ma, Yi, and Sastry,
S Shankar. Unsupervised segmentation of natural im-
ages via lossy data compression. Computer Vision and
Image Understanding, 110(2):212–225, 2008.

