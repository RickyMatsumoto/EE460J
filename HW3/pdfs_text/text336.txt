Sub-sampled Cubic Regularization for Non-convex Optimization

A. Appendix

A.1. Concentration Inequalities and Sampling Schemes

For the sake of simplicity we shall drop the iteration subscript k in the following results of this section.

A.1.1. GRADIENT SAMPLING

First, we extend the Vector Bernstein inequality as it can be found in (Gross, 2011) to the average of independent, zero-
mean vector-valued random variables.

Lemma 18 (Vector Bernstein Inequality). Let x1, . . . , xn be independent vector-valued random variables with com-
mon dimension d and assume that each one is centered, uniformly bounded and also the variance is bounded above:

E [xi] = 0 and (cid:107)xi(cid:107)2 ≤ µ as well as E

(cid:104)

(cid:107)xi(cid:107)2(cid:105)

≤ σ2

Let

Then we have for 0 < (cid:15) < σ2/µ

z =

1
n

n
(cid:88)

i=1

xi.

P ( (cid:107)z(cid:107) ≥ (cid:15)) ≤ exp

−n ·

(cid:18)

(cid:19)

(cid:15)2
8σ2 +

1
4

(35)

Proof: Theorem 6 in (Gross, 2011) gives the following Vector Bernstein inequality for independent, zero-mean vector-
valued random variables

P

(cid:32) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

n=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:33)

√

xi

≥ t +

V

≤ exp

−

(cid:18)

(cid:19)

,

t2
4V

where V = (cid:80)n

i=1

E

(cid:104)

(cid:107)xi(cid:107)2(cid:105)

First, we shall deﬁne (cid:15) = t +

is the sum of the variances of the centered vectors xi.
√

V , which allows us to rewrite the above equation as

P

(cid:32) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:33)



xi

≥ (cid:15)

≤ exp

−

(cid:32)

(cid:15) −
√

√

V

V

1
4

(cid:33)2

(cid:32)

 = exp

−

1
4

(cid:18) (cid:15)
√
V

(cid:19)2(cid:33)

− 1

.

Based on the observation that

−

(cid:18) (cid:15)
√
V

(cid:19)2

− 1

≤ −

(cid:19)

(cid:18) (cid:15)2
2V

+

1
4

1
4
(cid:15)2
V

(cid:15)
√
V

⇔ −

+ 2

− 1 ≤ −

+ 1

1
4
(cid:15)2
2V

⇔ 0 ≤

⇔ 0 ≤

− 2

(cid:15)2
2V
(cid:18) (cid:15)
√
2V

(cid:15)
√
V
√

−

+ 2

(cid:19)2

2

P

(cid:32) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:33)

xi

≥ (cid:15)

≤ exp

−

+

(cid:18)

(cid:15)2
8V

(cid:19)

.

1
4

always holds, we can formulate a slightly weaker Vector Bernstein version as follows

(36)

(37)

(38)

(39)

Sub-sampled Cubic Regularization for Non-convex Optimization

Since the individual variance is assumed to be bounded above, we can write

V =

n
(cid:88)

i=1

E

(cid:104)

(cid:107)xi(cid:107)2(cid:105)

≤ nσ2.

This term also constitutes an upper bound on the variance of y = (cid:80)n
uncorrelated . However, z = 1
n
E [z] = 0, and thus

i=1 xi, because the xi are independent and thus
i=1 xi and we must account for the averaging term. Since the xi are centered we have

(cid:80)n

V ar(z) = E

(cid:104)

(cid:107)z − E [z](cid:107)2(cid:105)

= E

(cid:104)

(cid:107)z(cid:107)2(cid:105)

= E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

2
 =

E

1
n2

(cid:32) n
(cid:88)





(cid:33)(cid:124) 


xi

i=1





xj





n
(cid:88)

j=1

=

E

1
n2





(cid:88)

i,j



(cid:0)x

(cid:124)
j xi

(cid:1)

 =

1
n2

(cid:88)

i,j

E (cid:2)(cid:0)x

(cid:124)
j xi

(cid:1)(cid:3) =

(cid:124)
E [(x
i xi)] +

E [(x

(cid:124)
i xj)]



(41)

xi

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)




1
n2

n
(cid:88)

i=1



n
(cid:88)

n
(cid:88)

i=1

j(cid:54)=i

=

1
n2

n
(cid:88)

i=1

E

(cid:104)

(cid:107)xi(cid:107)2(cid:105)

≤

1
n

σ2,

where we used the fact that the expectation of a sum equals the sum of the expectations and the cross-terms E (cid:2)x
0, j (cid:54)= i because of the independence assumption. Hence, we can bound the term V ≤ 1

(cid:3) =
n σ2 for the random vector sum z.
Now, since n > 1 and (cid:15) > 0, as well as P (z > a) is falling in a and exp(−x) falling in x, we can use this upper bound on
the variance of z in (39), which gives the desired inequality

(cid:124)
j xi

P ( (cid:107)z(cid:107) ≥ (cid:15)) ≤ exp

−n ·

(cid:18)

(cid:19)

(cid:15)2
8σ2 +

1
4

This result was applied in order to ﬁnd the probabilistic bound on the deviation of the sub-sampled gradient from the full
gradient as stated in Lemma 6, for which we will give the proof next.

Proof of Lemma 6:

To apply vector Bernstein’s inequality (35) we need to center the gradients. Thus we deﬁne

and note that from the Lipschitz continuity of f (A3), we have

xi = gi(x) − ∇f (x), i = 1, . . . , |Sg|

(cid:107)xi(cid:107) = (cid:107)gi(x) − ∇f (x)(cid:107) ≤ (cid:107)gi(x)(cid:107) + (cid:107)∇f (x)(cid:107) ≤ 2κf and (cid:107)xi(cid:107)2 ≤ 4κ2

f , i = 1, . . . , |Sg|.

With σ2 := 4κ2

f and

z =

1
|Sg|

(cid:88)

i∈Sg

xi =

1
|Sg|

(cid:88)

i∈Sg

1
|Sg|

(cid:88)

i∈Sg

gi(x) −

∇f (x) = g(x) − ∇f (x)

in equation (35), we can require the probability of a deviation larger or equal to (cid:15) to be lower than some δ ∈ (0, 1]

(40)

(42)

(cid:3)

(43)

(44)

(45)

(46)

P ( (cid:107)g(x) − ∇f (x)(cid:107) > (cid:15)) ≤2d exp

−|Sg| ·

(cid:32)

(cid:33)

!
≤ δ

(cid:15)2
32κ2
f

+

1
4

!
≥ log((2d)/δ)

log ((2d)/δ) + 1/4
|Sg|

.

⇔|Sg| ·

(cid:15)2
32κ2
f

1
4

−

(cid:115)

√

⇔(cid:15) ≥ 4

2κf

Sub-sampled Cubic Regularization for Non-convex Optimization

Conversely, the probability of a deviation of

(cid:115)

√

(cid:15) < 4

2κf

log ((2d)/δ) + 1/4
|Sg|

is higher or equal to 1 − δ.

Of course, any sampling scheme that guarantees the right hand side of (16) to be smaller or equal to M times the squared
step size, directly satisﬁes the sufﬁcient gradient agreement condition (A5). Consequently, plugging the former into the
latter and rearranging for the sample size gives Theorem 7 as we shall prove now.

Proof of Theorem 7:

By use of Lemma 6 we can write

(cid:107)g(x) − ∇f (x)(cid:107) ≤ M (cid:107)s(cid:107)2

(cid:115)

√

⇔ 4

2κf

log(1/δ + 1/4)
|Sg|

≤ M (cid:107)s(cid:107)2

|Sg| ≥

32κ2

f log (1/δ + 1/4)

M 2 (cid:107)s(cid:107)4

(47)

(cid:3)

(48)

(cid:3)

Sub-sampled Cubic Regularization for Non-convex Optimization

A.1.2. HESSIAN SAMPLING

Lemma 19 (Matrix Bernstein Inequality). Let A1, .., An be independent random Hermitian matrices with common
dimension d × d and assume that each one is centered, uniformly bounded and also the variance is bounded above:

E [Ai] = 0 and (cid:107)Ai(cid:107)2 ≤ µ as well as (cid:13)

(cid:13)E (cid:2)A2

i

(cid:3)(cid:13)
(cid:13)2 ≤ σ2

Introduce the sum

Then we have

Z =

1
n

n
(cid:88)

i=1

Ai

(cid:18)

P ( (cid:107)Z(cid:107) ≥ (cid:15)) ≤ 2d · exp

−n · min{

(49)

(cid:19)

(cid:15)2
4σ2 ,

(cid:15)
2µ

}

Proof: Theorem 12 in (Gross, 2011) gives the following Operator-Bernstein inequality

n
(cid:88)

P

(cid:32) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:33)

(cid:18)

Ai

≥ (cid:15)

≤ 2d · exp

min{

(cid:15)2
4V

,

(cid:15)
2µ

(cid:19)
}

,

i=1
where V = nσ2. As well shall see, this is an upper bound on the variance of Y = (cid:80)n
and have an expectation of zero (E [Y ] = 0).

i=1 Ai since the Ai are independent

V ar(Y) =

E (cid:2)Y2(cid:3) − E [Y]2(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) =
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:34)
(

E

(cid:88)

i

Ai)2

=

E

AiAj



=

(cid:35)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)





(cid:88)

i,j



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

i,j

(cid:13)
(cid:13)
(cid:13)
E [AiAj]
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:13)
(cid:13)
(cid:13)
E [AiAj]
(cid:13)
(cid:13)
(cid:13)
where we used the fact that the expectation of a sum equals the sum of the expectations and the cross-terms E [AjAi] =
0, j (cid:54)= i because of the independence assumption.

(cid:3)(cid:13)
(cid:13) ≤ nσ2,

E [AiAi] +

(cid:13)
(cid:13)E (cid:2)A2

E (cid:2)A2

(cid:13)
(cid:13)
(cid:3)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

j(cid:54)=i

≤

=

=

i

i

i

i

i

i

However, Z = 1
n

(cid:80)n

i=1 Ai and thus

V ar(Z) = (cid:13)

(cid:13)E (cid:2)Z2(cid:3)(cid:13)

(cid:13) =

(cid:34)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E

(

1
n

n
(cid:88)

i=1

Ai)2

(cid:35)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

1
n2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E

(cid:34)

n
(cid:88)
(

i=1

Ai)2

≤

σ2.

(cid:35)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

Hence, we can bound V ≤ 1
exp(−α) decreasing in α ∈ R we have that

n σ2 for the average random matrix sum Z. Furthermore, since n > 1 and (cid:15), µ > 0 as well as

exp

−

≤ exp

−

(cid:18)

(cid:19)

(cid:15)
2µ

(cid:18)

(cid:19)

.

(cid:15)
n2µ

Together with the Operator-Bernstein inequality, (52) and (53) give the desired inequality (49).

(50)

(51)

(52)

(53)

(cid:3)

This result exhibits that sums of independent random matrices provide normal concentration near its mean in a range
determined by the variance of the sum. We apply it in order to derive the bound on the deviation of the sub-sampled
Hessian from the full Hessian as stated in Lemma 8, which we shall prove next.

Proof of Lemma 8: Bernstein’s Inequality holds as f ∈ C 2 and thus the Hessian is symmetric by Schwarz’s Theorem.
Since the expectation of the random matrix needs to be zero, we center the individual Hessians,

Xi = Hi(x) − H(x), i = 1, ..., |SH |

Sub-sampled Cubic Regularization for Non-convex Optimization

and note that now from the Lipschitz continuity of g (A3):

(cid:107)Xi(cid:107)2 ≤ 2κg, i = 1...|SH | and (cid:13)

(cid:13)X2
i

(cid:13)
(cid:13)2 ≤ 4κ2

g, i = 1...|SH |.

Hence, for (cid:15) ≤ 4κg, we are in the small deviation regime of Bernstein’s bound with a sub-gaussian tail. Then, we may
plug

into (49), to get

1
|SH |

|SH |
(cid:88)

i=1

Xi = B(x) − H(x)

P ( (cid:107)B(x) − H(x)(cid:107) ≥ (cid:15)) ≤ 2d · exp

−

(cid:18)

(cid:19)

.

(cid:15)2|SH |
16κ2
g

Finally, we shall require the probability of a deviation of (cid:15) or higher to be lower than some δ ∈ (0, 1]

which is equivalent to (cid:107)B(x) − H(x)(cid:107) staying within this particular choice of (cid:15) with probability (1 − δ), generally
perceived as high probability.

Proof of Theorem 9: Since (cid:107)Av(cid:107) ≤ (cid:107)A(cid:107)op(cid:107)v(cid:107) for every v ∈ V we have for the choice of the spectral matrix norm and
euclidean vector norm that any B that fulﬁls (cid:107)(B(x) − H(x))(cid:107) ≤ C (cid:107)s(cid:107) also satisﬁes condition A4. Furthermore

2d · exp

−

(cid:18)

(cid:19)

(cid:15)2|SH |
16κ2
g

!= δ

⇔ −

= log(δ/2d)

(cid:15)2|SH |
16κ2
g
(cid:115)

⇔ (cid:15) = 4κg

log(2d/δ)
|SH |

,

(cid:107)(B − H(x))(cid:107) ≤ C (cid:107)s(cid:107)

(cid:115)

⇔ 4κg

⇔ |SH | ≥

log(2d/δ)
|SH |
16κ2

≤ C (cid:107)s(cid:107)

g log(2d/δ)
(C (cid:107)s(cid:107))2

,

C > 0.

(54)

(55)

(cid:3)

(56)

(cid:3)

Note that there may be a less restrictive sampling conditions that satisfy A4 since condition (56) is based on the worst
case bound (cid:107)Av(cid:107) ≤ (cid:107)A(cid:107)op(cid:107)v(cid:107) which indeed only holds with equality if v happens to be (exactly in the direction of) the
largest eigenvector of A.

Finally, we shall state a Lemma which illustrates that the stepsize goes to zero as the algorithm converges. The proof can
be found in Section 5 of (Cartis et al., 2011a).

Lemma 20. Let {f (xk)} be bounded below by some finf > −∞. Also, let sk satisfy A1 and σk be bounded below by
some σinf > 0. Then we have for all successful iterations that

(cid:107)sk(cid:107) → 0, as k → ∞

(57)

Sub-sampled Cubic Regularization for Non-convex Optimization

A.1.3. ILLUSTRATION

In the top row of Figure 2 we illustrate the Hessian sample sizes that result when applying SCR with a practical version of
Theorem 9 to the datasets used in our experiments 4. In the bottom row of Figure 2, we benchmark our algorithm to the
deterministic as well as two naive stochastic versions of ARC with linearly and exponentially increasing sample sizes.

1. A9A

2. COVTYPE

3. GAUSSIAN

Figure 2. Suboptimality (top row) and sample sizes (bottom row) for different cubic regularization methods on a9a, covtype and gaus-
sian. Note that the automatic sampling scheme of SCR follows an exponential curve, which means that it can indeed save a lot of
computation in the early stage of the optimization process.

Note that both the linear and the exponential sampling schemes do not quite reach the same performance as SCR even
though they were carefully ﬁne tuned to achieve the best possible performance. Furthermore, the sampling size was
manually set to reach the full sample size at the very last iteration. This highlights another advantage of the automatic
sampling scheme that does not require knowledge of the total number of iterations.

A.2. Convergence Analysis

A.2.1. PRELIMINARY RESULTS

Proof of Lemma 10:

The lower bound σinf follows directly from Step 7 in the algorithm design (see Algorithm 1). Within the upper bound, the
constant σ0 accounts for the start value of the penalty parameter. Now, we show that as soon as some σk > 3( 2M +C+κg
),
the iteration is very successful and σk+1 < σk. Finally, γ2 allows for σk being ’close to’ the successful threshold, but
increased ’one last time’.

2

Any iteration with f (xk + sk) ≤ m(sk) yields a ρk ≥ 1 ≥ η2 and is thus very successful. From a 2nd-order Taylor

4see Section A.3 for details

Sub-sampled Cubic Regularization for Non-convex Optimization

approximation of f (xk + sk) around xk we have:

f (xk + sk) − mk(sk) = (∇f (xk) − g(xk))(cid:124)sk +

1
2
(cid:107)sk(cid:107)2 (cid:107)H(xk + tsk) − H(x)(cid:107) +

(cid:124)
k(H(xk + tsk) − Bk)sk −
s

σ
3

(cid:107)sk(cid:107)3

(cid:107)H(xk) − Bk(cid:107) (cid:107)sk(cid:107) −

1
2

σk
3

(cid:107)sk(cid:107)3

(cid:124)
ksk +
≤ e

1
2

≤ (cid:107)ek(cid:107) (cid:107)sk(cid:107) +

≤ M (cid:107)sk(cid:107)3 +

(cid:18) C + κg
2
(cid:18) C + κg
2

−

−

σk
3

σk
3
(cid:19)

(cid:19)

(cid:107)sk(cid:107)3

(cid:107)sk(cid:107)3

(cid:18) 2M + C + κg
2

=

−

σk
3

(cid:19)

(cid:107)sk(cid:107)3

Requiring the right hand side to be non-positive and solving for σk gives the desired result.

Proof of Lemma 11 : By deﬁnition of the stochastic model mk(sk) we have

(cid:124)
kg(xk) −
f (xk) − mk(sk) = − s
2
3

(cid:124)
kBksk +
s

=

1
3

(cid:124)
kBksk −
s

1
2
σk (cid:107)sk(cid:107)3

σk (cid:107)sk(cid:107)3

1
2
1
6

≥

σk (cid:107)sk(cid:107)3 ,

where we applied equation (11) ﬁrst and equation (12) secondly.

Before proving the lower bound on the stepsize (cid:107)sk(cid:107) we ﬁrst transfer the rather technical result from Lemma 4.6 in (Cartis
et al., 2011a) to our framework of stochastic gradients. For this purpose, let ek be the gradient approximation error, i.e.
ek := gk − ∇f (xk).

Lemma 21. Let f ∈ C 2, Lipschitz continuous gradients (A3) and TC (A2) hold. Then, for each (very-)successful k,
we have

(1 − κθ) (cid:107)∇f (xk+1)(cid:107) ≤ σk (cid:107)sk(cid:107)2 +
(cid:13)
(cid:18) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(H(xk + tsk) − H(xk))dt

(cid:90) 1

0

+

(cid:124)

(cid:107)(H(xk) − Bk)sk(cid:107)
(cid:107)sk(cid:107)
(cid:123)(cid:122)
=dk

+ κθκg (cid:107)sk(cid:107) + (1 + κθκg)

· (cid:107)sk(cid:107)

(60)

(cid:107)ek(cid:107)
(cid:107)sk(cid:107)

(cid:19)

(cid:125)

with κθ ∈ (0, 1) as in TC (13).

Proof: We shall start by writing

(cid:107)∇f (xk + sk)(cid:107) ≤ (cid:107)∇f (xk + sk) − ∇mk(sk)(cid:107) + (cid:107)∇mk(sk)(cid:107)
,
+ θk (cid:107)gk(xk)(cid:107)
≤ (cid:107)∇f (xk + sk) − ∇mk(sk)(cid:107)
(cid:123)(cid:122)
(cid:125)
(cid:125)
(b)

(cid:123)(cid:122)
(a)

(cid:124)

(cid:124)

where the last inequality results from TC (Eq. (13)). Now, we can ﬁnd the following bounds on the individual terms:

(a) By (5) we have

(cid:107)∇f (xk + sk) − ∇mk(cid:107) = (cid:107)∇f (xk + sk) − gk(xk) − Bksk − σksk (cid:107)sk(cid:107)(cid:107) .

(62)

(58)

(cid:3)

(cid:3)

(59)

(61)

Sub-sampled Cubic Regularization for Non-convex Optimization

We can rewrite the right-hand side by a Taylor expansion of ∇fk+1(xk + sk) around xk to get

(62) =

∇f (xk) +

H(xk + tsk)skdt − gk(xk) − Bksk − σksk (cid:107)sk(cid:107)

.

(63)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:90) 1

0

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Contrary to the case of deterministic gradients, the ﬁrst and third summand no longer cancel out. Applying the triangle
inequality repeatedly, we thus get an error term in the ﬁnal bound on (a):

(cid:107)∇f (xk + sk) − ∇mk(cid:107) ≤

H((xk + tsk) − Bk)skdt

+ σk (cid:107)sk(cid:107)2 + (cid:107)∇f (xk) − gk(xk)(cid:107)

(cid:90) 1

0
(cid:90) 1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+ σk (cid:107)sk(cid:107)2 + (cid:107)ek(cid:107) .

0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

H((xk + tsk)dt − H(xk)

· (cid:107)sk(cid:107) + (cid:107)(H(xk) − Bk)sk(cid:107)

(b) To bound the second summand, we can write

(cid:107)g(xk)(cid:107) ≤ (cid:107)∇f (xk)(cid:107) + (cid:107)ek(cid:107)

≤ (cid:107)∇f (xk + sk)(cid:107) + (cid:107)∇f (xk) − ∇f (xk + sk)(cid:107) + (cid:107)ek(cid:107)
≤ (cid:107)∇f (xk + sk)(cid:107) + κg (cid:107)sk(cid:107) + (cid:107)ek(cid:107)

Finally, using the deﬁnition of θk as in (13) (which also gives θk ≤ κθ and θk ≤ κθhk) and combining (a) and (b) we get
the above result.

Proof of Lemma 12: The conditions of Lemma 21 are satisﬁed. By multiplying dk (cid:107)sk(cid:107) out in equation (60), we get

(cid:90) 1

(1 − κθ) (cid:107)∇f (xk+1)(cid:107) ≤
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(H(xk + tsk) − H(xk))dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)

0

(cid:107)sk(cid:107) + (cid:107)(H(xk) − Bk)sk(cid:107) + κθκg (cid:107)sk(cid:107)2 + (1 + κθκg) (cid:107)ek(cid:107) + σk (cid:107)sk(cid:107)2 .

Now, applying the strong agreement conditions (A4) and (A5), as well as the Lipschitz continuity of H, we can rewrite this
as

(1 − κθ) (cid:107)∇f (xk+1)(cid:107) ≤ (

κg + C + (1 + κθκg)M + σmax + κθκg) (cid:107)sk(cid:107)2 ,

1
2

for all sufﬁciently large, successful k. Solving for the stepsize (cid:107)sk(cid:107) give the desired result.

(64)

(65)

(cid:3)

(66)

(67)

(cid:3)

A.2.2. LOCAL CONVERGENCE

Before we can study the convergence rate of SCR in a locally convex neighbourhood of a local minimizer w∗ we ﬁrst need
to establish three crucial properties:

1. a lower bound on (cid:107)sk(cid:107) that depends on (cid:107)gk(cid:107).

2. an upper bound on (cid:107)sk(cid:107) that depends on (cid:107)gk+1(cid:107).

3. an eventually full sample size

4. conditions under which all steps are eventually very successful.

With this at hand we will be able to relate (cid:107)gk+1(cid:107) to (cid:107)gk(cid:107), show that this ratio eventually goes to zero at a quadratic rate
and conclude from a Taylor expansion around gk that the iterates themselves converge as well.

Sub-sampled Cubic Regularization for Non-convex Optimization

Assumption 22 (Sampling Scheme). Let gk and Bk be sampled such that 17 and 19 hold in each iteration k. Furthermore,
for unsuccessful iterations, assume that the sample size is not decreasing.

We have already established a lower stepsize bound in Lemma 12 so let us turn our attention directly to 2.:
Lemma 23 (Upper bound on stepsize). Suppose that sk satisﬁes (11) and that the Rayleigh coefﬁcient

Rk(sk) :=

(cid:124)
kBksk
s
(cid:107)sk(cid:107)2

is positive, then

(cid:107)sk(cid:107) ≤

(cid:107)gk(cid:107) =

(cid:107)∇f (wk) + ek(cid:107) ≤

( (cid:107)∇f (wk)(cid:107) + (cid:107)ek(cid:107))

1
Rk(sk)

1
Rk(sk)

1
Rk(sk)

Proof: Given the above assumptions we can rewrite (11) as follows

(cid:124)
Rk(sk) (cid:107)sk(cid:107)2 = −s

kgk − σk (cid:107)sk(cid:107)3 ≤ (cid:107)sk(cid:107) (cid:107)gk(cid:107) ,

where we used Cauchy-Schwarz inequality as well as the fact that σk > 0, ∀k. Solving (70) for (cid:107)sk(cid:107) gives (69).

Lemma 24 (Eventually full sample size). Let {f (xk)} be bounded below by some finf > −∞. Also, let A1, A3 hold and
let gk and Bk be sampled according to A22. Then we have w.h.p. that

|Sg,k| → n and |SB,k| → n as k → ∞

The sampling schemes from Theorem 7 and Theorem 9 imply that the sufﬁcient agreement assumptions A5 and A4 hold
with high probability. Thus, we can deduce from Lemma 10 that after a certain number of consecutive unsuccessful iterates
the penalty parameter is so high (σk ≥ σsup) that we are guaranteed to ﬁnd a successful step. Consequently, the number
of successful iterations must be inﬁnite (|S| = ∞) when we consider the asymptotic convergence properties of SCR. We
are left with two possible scenarios:
(i) If the number of unsuccessful iterations is ﬁnite (|U| ≤ ∞ & |S| = ∞) we have that ∃ ˆk after which all iterates are
successful, i.e. k ∈ S, ∀ k > ˆk. From Lemma 20 we know that for all successful iterations (cid:107)sk(cid:107) → 0 as k → ∞.
Consequently, due to the sampling scheme as speciﬁed in Theorem 7 and Theorem 9, ∃ ¯k ≥ ˆk with |Sg,k| = |SB,k| =
n, ∀ k ≥ ¯k.

(ii) If the number of unsuccessful iterations is inﬁnite (|U| = ∞ & |S| = ∞) we know for the same reasons that for
the subsequence of successful iterates {k = 0, 1, . . . ∞|k ∈ S} again (cid:107)sk(cid:107) → 0, as k ∈ S → ∞ and hence ∃ ˜k with
|Sg,k| = |SB,k| = n, ∀ k ≥ ˜k ∈ S. Given that we do speciﬁcally not decrease the sample size in unsuccessful iterations
we have that |Sg,k| = |SB,k| = n, ∀ k ≥ ˜k.

As a result the sample sizes eventually equal n with high probability in all conceivable scenarios which proves the asser-
tion5.

Now that we have (asymptotic) stepsize bounds and gradient (Hessian) agreement we are going to establish that, when
converging, all SCR iterations are indeed very successful asymptotically.
Lemma 25 (Eventually successful iterations). Let f ∈ C 2, ∇f uniformly continuous and Bk bounded above. Let Bk and
gk be sampled according to A22, as well as sk satisfy (11). Furthermore, let

with ∇f (w∗) = 0 and H(w∗) positive deﬁnite. Then there exists a constant Rmin > 0 such that for all k sufﬁciently large

wk → w∗, as k → ∞,

Rk(sk) ≥ Rmin.

Furthermore, all iterations are eventually very successful w.h.p.

5We shall see that, as a result of Lemma 25, the case of an inﬁnite number of unsuccessful steps can actually not happen

(68)

(69)

(70)

(cid:3)

(71)

(cid:3)

(72)

(73)

Sub-sampled Cubic Regularization for Non-convex Optimization

Proof: Since f is continuous, the limit (72) implies that {f (wk)} is bounded below. Since H(w∗) is positive deﬁnite per
assumption, so is H(wk) for all k sufﬁciently large. Therefore, there exists a constant Rmin such that

s

(cid:124)
kH(wk)sk
(cid:107)sk(cid:107)2

> 2Rmin > 0, for all k sufﬁciently large.

As a result of Lemma 24 we have that (cid:107)ek(cid:107) → 0 as k → ∞. Hence, Lemma 23 yields (cid:107)sk(cid:107) ≤ 1/Rmin (cid:107)∇fk(cid:107) which
implies that the step size converges to zero as we approximate w∗. Consequently, we are able to show that eventually all
iterations are indeed very successful. Towards this end we need to ensure that the following quantity rk becomes negative
for sufﬁciently large k:

rk := f (wk + sk) − m(sk)
(cid:125)

(cid:124)

,
+(1 − η2) (m(sk) − f (wk))
(cid:125)

(cid:124)

(cid:123)(cid:122)
(ii)

(cid:123)(cid:122)
(i)

where η2 ∈ (0, 1) is the ”very successful” threshold.

(i) By a (second-order) Taylor approximation around f (wk) and applying the Cauchy-Schwarz inequality, we have:

f (wk + sk) − m(sk) =(∇f (w) − gk)(cid:124)sk +

(cid:124)
k((H(wk + τ sk) − Bk)sk −
s

σk
3

(cid:107)s(cid:107)3

≤ (cid:107)ek(cid:107) (cid:107)sk(cid:107) +

(cid:107)((H(wk + τ sk) − Bk)sk(cid:107) (cid:107)sk(cid:107) ,

1
2

1
2

where the term (cid:107)ek(cid:107) (cid:107)sk(cid:107) is extra compared to the case of deterministic gradients.

(ii) Regarding the second part we note that if sk satisﬁes (11), we have by the deﬁnition of Rk and equation (73) that

f (wk) − mk(sk) =

σk (cid:107)sk(cid:107)3

(cid:124)
s
kBsk +

2
3
Rmin (cid:107)sk(cid:107)2 ,

1
2
1
2

≥

which negated gives the desired bound on (ii). All together, the upper bound on rk is written as

rk ≤

(cid:107)sk(cid:107)2

1
2

(cid:18) 2 (cid:107)ek(cid:107)
(cid:107)sk(cid:107)

+

(cid:107)((H(wk + τ sk) − Bk)sk(cid:107)
(cid:107)sk(cid:107)

− (1 − η2)Rmin

.

(cid:19)

Let us add and subtract H(wk) to the second summand and apply the triangle inequality

rk ≤

(cid:107)sk(cid:107)2

1
2

(cid:18) 2 (cid:107)ek(cid:107)
(cid:107)sk(cid:107)

+

(cid:107)(H(wk + τ sk) − Hk)sk(cid:107) + (cid:107)(Hk − Bk)sk(cid:107)
(cid:107)sk(cid:107)

− (1 − η2)Rmin

.

(79)

(cid:19)

Now applying (cid:107)Av(cid:107) ≤ (cid:107)A(cid:107) (cid:107)v(cid:107) we get

rk ≤

(cid:107)sk(cid:107)2

1
2

(cid:18) 2 (cid:107)ek(cid:107)
(cid:107)sk(cid:107)

+ (cid:107)H(wk + τ sk) − Hk(cid:107) + (cid:107)(Hk − Bk)(cid:107) − (1 − η2)Rmin

.

(80)

(cid:19)

We have already established in Lemma 24 that (cid:107)ek(cid:107) → 0 and (cid:107)(Hk − Bk)(cid:107) → 0. Together with Lemma 23 and the
assumption (cid:107)∇fk(cid:107) → 0 this implies (cid:107)sk(cid:107) → 0. Furthermore, since τ ∈ [0, 1] we have that (cid:107)wk + τ sk(cid:107) ≤ (cid:107)wk + sk(cid:107) ≤
(cid:107)sk(cid:107). Hence, H(wk + τ sk) and H(wk) eventually agree. Finally, η2 < 1 and Rmin > 0 such that rk is negative for all k
sufﬁciently large, which implies that every such iteration is very successful.

Proof of Theorem 13:

From Lemma 10 we have σk ≤ σsup. Furthermore, all assumptions needed for the step size bounds of Lemma 12 and 23
hold. Finally, Lemma 25 gives that all iterations are eventually successful. Thus, we can combine the upper (69) and lower
(24) bound on the stepsize for all k sufﬁciently large to obtain

1
Rmin

( (cid:107)∇f (wk)(cid:107) + (cid:107)ek(cid:107)) ≥ (cid:107)sk(cid:107) ≥ κs

(cid:112) (cid:107)∇f (wk+1)(cid:107)

(74)

(75)

(76)

(77)

(78)

(cid:3)

(81)

Sub-sampled Cubic Regularization for Non-convex Optimization

which we can solve for the gradient norm ratio

(cid:107)∇f (wk+1)(cid:107)
(cid:107)∇f (wk)(cid:107)2 ≤

(cid:18) 1

Rminκs

(cid:18)

1 +

(cid:107)ek(cid:107)
(cid:107)∇f (wk)(cid:107)

(cid:19)(cid:19)2

.

Consequently, as long as the right hand side of (82) stays below inﬁnity, i.e. (cid:107)ek(cid:107) / (cid:107)∇f (wk)(cid:107) (cid:54)→ ∞, we have quadratic
convergence of the gradient norms. From Lemma 24 we have that (cid:107)ek(cid:107) → 0 as k → ∞ w.h.p. and furthermore κs is
bounded above by a constant and Rmin is a positive constant itself which gives quadratic convergence of the gradient norm
ratio with high probability. Finally, the convergence rate of the iterates follows from a Taylor expansion around gk.

A.2.3. FIRST ORDER GLOBAL CONVERGENCE

Note that the preliminary results Lemma 11 and 12 allow us to lower bound the function decrease of a successful step in
terms of the full gradient ∇fk+1. Combined with Lemma 10, this enables us to give a deterministic global convergence
guarantee while using only stochastic ﬁrst order information6.

Proof of Theorem 14:

We will consider two cases regarding the number of successful steps for this proof.

Case (i): SCR takes only ﬁnitely many successful steps. Hence, we have some index k0 which yields the very last
successful iteration and all further iterates stay at the same point xk0+1. That is xk0+1 = xk0+i, ∀ i ≥ 1. Let us assume
that (cid:107)∇f (xk0+1)(cid:107) = (cid:15) > 0, then

Since, furthermore, all iterations k ≥ k0 + 1 are unsuccessful σk increases by γ, such that

(cid:107)∇f (xk)(cid:107) = (cid:15), ∀ k ≥ k0 + 1.

σk → ∞ as k → ∞.

However, this is in contradiction with Lemma 10, which states that σk is bounded above. Hence, the above assumption
cannot hold and we have (cid:107)∇f (xk0+1)(cid:107) = (cid:107)∇f (x∗)(cid:107) = 0.

Case (ii): sARC takes inﬁnitely many successful steps. While unsuccessful steps keep f (xk) constant, (very) successful
steps strictly decrease f (xk) and thus the sequence {f (xk)} is monotonically decreasing. Furthermore, it is bounded
below per assumption and thus the objective values converge

f (xk) → finf , as k → ∞.

All requirements of Lemma 11 and Lemma 12 hold and we thus can use the sufﬁcient function decrease equation (31) to
write

f (xk) − finf ≥ f (xk) − f (xk+1) ≥

η1σinf κ3

s (cid:107)∇f (xk+1)(cid:107)3/2 .

1
6

Since (f (xk) − finf ) → 0 as k → ∞, σinf > 0, η1 > 0 and κ3
the result.

s ≥ 0 (as σsup < ∞), we must have (cid:107)∇f (xk)(cid:107) → 0, giving

(82)

(cid:3)

(83)

(84)

(85)

(86)

(cid:3)

A.2.4. SECOND ORDER GLOBAL CONVERGENCE AND WORST CASE ITERATION COMPLEXITY

For the proofs of Theorem 15 and Theorem 17 we refer the reader to Theorem 5.4 in (Cartis et al., 2011a) and Corollary
5.3 in (Cartis et al., 2011b). Note that, as already laid out above in the proofs of Lemma 10 and Lemma 11, the constants
involved in the convergence Theorems change due to the stochastic gradients used in our framework.

A.3. Details concerning experimental section

We here provide additional results and brieﬂy describe the baseline algorithms used in the experiments as well as the choice
of hyper-parameters. All experiments were run on a CPU with a 2.4 GHz nominal clock rate.

6Note that this result can also be proven without Lipschitz continuity of H and less strong agreement conditions as done in Corollary

2.6 in (Cartis et al., 2011a).

Sub-sampled Cubic Regularization for Non-convex Optimization

Datasets The real-world datasets we use represent very common instances of Machine Learning problems and are part of
the libsvm library (Chang & Lin, 2011), except for cifar which is from Krizhevsky & Hinton (2009). A summary of their
main characteristic can be found in table 1. The multiclass datasets are both instances of so-called image classiﬁcation
problems. The mnist images are greyscale and of size 28 × 28. The original cifar images are 32 × 32 × 3 but we converted
them to greyscale so that the problem dimensionality is comparable to mnist. Both datasets have 10 different classes, which
multiplies the problem dimensionality of the multinomial regression by 10.

type
Classiﬁcation
a9a
Classiﬁcation
a9a nc
covtype
Classiﬁcation
covtype nc Classiﬁcation
Classiﬁcation
higgs
Classiﬁcation
higgs nc
Multiclass
mnist
Multiclass
cifar

n
32, 561
32, 561
581, 012
581, 012
11, 000, 000
11, 000, 000
60, 000
50, 000

d
123
123
54
54
28
28
7, 840
10, 240

κ(H∗)
761.8
1, 946.3
3 · 109
25, 572, 903.1
1, 412.0
2, 667.7
10, 281, 848
1 · 109

λ
1e−3
1e−3
1e−3
1e−3
1e−4
1e−4
1e−3
1e−3

Table 1. Overview over the real-world datasets used in our experiments with convex and non-convex (nc) regularizer. κ(H∗) refers to
the condition number of the Hessian at the optimizer and λ is the regularization parameter applied in the loss function and its derivatives.

1. A9A

2. COVTYPE

3. HIGGS

Figure 3. Results from Section 5 over epochs. Top (bottom) row shows the log suboptimality of convex (non-convex) regularized logistic
regressions over epochs (average of 10 independent runs).

Benchmark methods

• Stochastic Gradient Descent (SGD): To bring in some variation, we select a mini-batch of the size (cid:100)n/10(cid:101) on the real
world classiﬁcation- and (cid:100)n/100(cid:101) on the multiclass problems. On the artiﬁcial datasets we only sample 1 datapoint
per iteration and update the parameters with respect to this point. We use a problem-dependent, constant step-size as
this yields faster initial convergence (Hofmann et al., 2015),(Roux et al., 2012).

• SAGA: is a variance-reduced variant of SGD that only samples 1 datapoint per iteration and uses a constant step-size.

Sub-sampled Cubic Regularization for Non-convex Optimization

• Broyden-Fletcher-Goldfarb-Shanno (BFGS) is the most popular and stable Quasi-Newton method.

• Limited-memory BFGS is a variant of BFGS which uses only the recent K iterates and gradients to construct an
approximate Hessian. We used K = 20 in our experiments. Both methods employs a line-search technique that
satisﬁes the strong Wolfe condition to select the step size.

• NEWTON is the classic version of Newton’s method which we apply with a backtracking line search.

For L-BFGS and BFGS we used the implementation available in the optimization library of scipy. All other methods are
our own implementation. The code for our implementation of SCR is publicly available on the authors’ webpage.

Initialization. All of our experiments were started from the initial weight vector w0 := (0, . . . , 0).

Choice of parameters for ARC and SCR. The regularization parameter updating is analog to the rule used in the
reported experiments of (Cartis et al., 2011a), where γ = 2. Its goal is to reduce the penalty rapidly as soon as convergence
sets in, while keeping some regularization in the non asymptotic regime. A more sophisticated approach can be found in
(Gould et al., 2012). In our experiments we start with σ0 = 1, η1 = 0.2, and η2 = 0.8 as well as an initial sample size of
5%.

Inﬂuence of dimensionality To test the inﬂuence of the dimensionality on the progress of the above applied methods we
created artiﬁcial datasets of three different sizes, labeled as gaussian s, gaussian m and gaussian l.

type
gaussian s
Classiﬁcation
gaussian m Classiﬁcation
Classiﬁcation
gaussian l

n
50, 000
50, 000
50, 000

d
100
1, 000
10, 000

κ(H ∗)
2, 083.3
98, 298.9
1, 167, 211.3

λ
1e−3
1e−3
1e−3

Table 2. Overview over the synthetic datasets used in our experiments with convex regularizer

The feature vectors X = (x1, x2, ..., xd), xi ∈ Rn were drawn from a multivariate Gaussian distribution

X ∼ N (µ, Σ)

(87)

with a mean of zero µ = (0, . . . , 0) and a covariance matrix that has reasonably uniformly distributed off-diagonal elements
in the interval (−1, 1).

As expected, the classic Newton methods suffers heavily from an increase in the dimension. The regularized Newton
methods on the other hand scale comparably very well since they only need indirect access to the Hessian via matrix-
vector products. Evidently, these methods outperform the quasi-newton approaches even in high dimensions. Among
these, the limited memory version of BFGS is signiﬁcantly faster than its original variant.

Multiclass regression In this section we leave the trust region method out because our implementation is not optimized
towards solving multi-class problems. We do not run Newton’s method or BFGS either as the above results suggests that
they are unlikely to be competitive. Furthermore, Figure 5 does not show logarithmic but linear suboptimality because
optimizing these problems to high precision takes very long and yields few additional beneﬁts. For example, the 25th SCR
iteration drove the gradient norm from 3.8 · 10−5 to 5.6 · 10−8 after building up a Krylov space of dimensionality 7800. It
took 9.47 hours and did not change any of the ﬁrst 13 digits of the loss. As can be seen, SCR provides early progress at a
comparable rate to other methods but gives the opportunity to solve the problem to high precision if needed.

Sub-sampled Cubic Regularization for Non-convex Optimization

1. GAUSSIAN S

2. GAUSSIAN M

3. GAUSSIAN L

Figure 4. Top (bottom) row shows the log suboptimality of convex regularized logistic regressions over time (epochs) (average of 10
independent runs).

Figure 5. Top (bottom) row shows suboptimality of the empirical risk of convex regularized multinominal regressions over time (epochs)

1. CIFAR

2. MNIST

