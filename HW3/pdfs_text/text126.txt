Active Heteroscedastic Regression

Kamalika Chaudhuri 1 Prateek Jain 2 Nagarajan Natarajan 2

Abstract

∈

An active learner is given a model class Θ, a large
sample of unlabeled data drawn from an under-
lying distribution and access to a labeling oracle
that can provide a label for any of the unlabeled
instances. The goal of the learner is to ﬁnd a
model θ
Θ that ﬁts the data to a given accuracy
while making as few label queries to the oracle as
possible. In this work, we consider a theoretical
analysis of the label requirement of active learn-
ing for regression under a heteroscedastic noise
model, where the noise depends on the instance.
We provide bounds on the convergence rates of
active and passive learning for heteroscedastic
regression. Our results illustrate that just like
in binary classiﬁcation, some partial knowledge
of the nature of the noise can lead to signiﬁcant
gains in the label requirement of active learning.

1. Introduction

An active learner is given a model class Θ, a large sample
of unlabeled data drawn from an underlying distribution Px
and access to a labeling oracle
which can provide a label
for any of the unlabeled instances. The goal of the learner is
to ﬁnd a model θ
Θ that ﬁts the data to a given accuracy
while making as few label queries to the oracle as possible.

O

∈

There has been a lot of theoretical literature on active learn-
ing, most of which has been in the context of binary clas-
siﬁcation in the PAC model (Balcan et al., 2009; Han-
neke, 2007; Dasgupta et al., 2007; Beygelzimer et al., 2009;
Awasthi et al., 2014; Zhang and Chaudhuri, 2014). For
classiﬁcation, the problem is known to be particularly difﬁ-
cult when there is no perfect classiﬁer in the class that best
ﬁts the labeled data induced by the oracle’s responses. Prior
work in the PAC model has shown that the difﬁculty of the
problem is alleviated when the “noise” is more benign – for

Authors listed in the alphabetical order 1University of Califor-
nia, San Diego 2Microsoft Research, India. Correspondence to:
Nagarajan Natarajan <t-nanata@microsoft.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

example, when there is a ground truth classiﬁer that induces
a labeling and the oracle’s responses are perturbed versions
of these labels (Hanneke, 2007; Awasthi et al., 2014; Zhang
and Chaudhuri, 2014; Awasthi et al., 2016) corrupted by
certain kinds of noise. In particular, signiﬁcant improve-
ments in label complexity have been obtained under what
is known as the Tsybakov noise conditions, which model
the realistic case of noise that decreases as we move fur-
ther from the decision boundary.

The case of active learning under regression however is
signiﬁcantly less well-understood. In particular, we only
have a theoretical understanding of the two extreme cases
– no noise (as in, no model mismatch) and arbitrary model
mismatch. Chaudhuri et al. (2015) show that allowing the
learner to actively select instances for labeling under re-
gression with no model mismatch can only improve the
convergence rates by a constant factor; moreover, in many
natural cases, such as when the unlabeled data is drawn
from a uniform Gaussian, there is no improvement. Sabato
and Munos (2014) look at the other extreme case – when
arbitrary model mismatch is allowed – and provide an al-
gorithm that attempts to “learn” the locations of the mis-
match through increasingly reﬁned partitions of the space,
and then learn a model accordingly. However if the model
mismatch is allowed to be arbitrary, then this algorithm
either requires an extremely reﬁned partition leading to a
very high running time, or a large number of labels. More
recently, Anava and Mannor (2016) study an online learn-
ing approach for estimating heteroscedastic variances and
provide general task-dependent regret bounds, but not ex-
act parameter recovery gaurantees.

In this paper we take a step towards closing this gap in un-
derstanding by considering active regression under a real-
istic yet more benign “noise” model – when the variance of
the label noise depends linearly on the example x. Specif-
ically, the oracle’s response on an unlabeled example x
, σ2
; here
is distributed as
β∗ is the unknown vector of regression coefﬁcients and
f ∗ is an unknown parameter vector. In classical statistics,
this framework is called heteroscedastic regression, and is
known to arise in econometric and medical applications.

x) with σx =

f ∗, x

x, β∗

N

⟨

(

⟩

⟨

⟩

While the usual least squares estimator for heteroscedas-
tic regression is still statistically consistent, we ﬁnd that

Active Heteroscedastic Regression

!

"

"

2(1/n + d2/n2)
∥
2(d/n)
f ∗
∥
∥

even in the passive learning case, optimal convergence rates
for heteroscedastic regression are not known. We thus be-
gin with a convergence analysis of heteroscedastic regres-
sion for passive learning when the distribution Px over the
unlabeled examples is a spherical Gaussian (in d dimen-
sions). We show that even in this very simple case, the
usual least squares estimator is sub-optimal, even when the
noise model f ∗ is known to the learner. Instead, we pro-
pose a weighted least squares estimator, and show that its
rate of convergence is ˜O
f ∗
when the
∥
noise model is known, and ˜O
when it needs
to be estimated from the data; here, n denotes the number
!
of labeled examples used to obtain the estimator. The lat-
ter matches the convergence rates of the least squares esti-
mator for the usual homoskedastic linear regression, where
f ∗
∥
We next turn to active heteroscedastic regression and pro-
pose a two-stage active estimator. We show that when the
noise model is known, the convergence rate of active het-
eroscedastic regression is ˜O
f ∗
, a small
∥
improvement over passive. However, in the more realistic
!
case where the noise model is unknown, the rates become
O
, which improves over the passive
estimator by a factor of d. Our results extend to the case
when the distribution Px over unlabeled examples is an ar-
bitrary Gaussian with covariance matrix Σ and the norm
used is the Σ norm. Our work illustrates that just like bi-
nary classiﬁcation, even a partial knowledge of the nature
of the model mismatch signiﬁcantly helps the label com-
plexity of active learning.

2 plays the role of the variance σ2.
∥

2(1/n+d2/n4)
∥

2(1/n + d2/n2)

f ∗
∥
!

∥

"

"

Our work is just a ﬁrst step towards a study of active max-
imum likelihood estimation under controlled yet realistic
forms of noise. There are several avenues for future work.
For simplicity, the convergence bounds we present relate to
the case when the distribution Px is a Gaussian. An open
problem is to combine our techniques with the techniques
of (Chaudhuri et al., 2015) and establish convergence rates
for general unlabeled distributions. Another interesting line
of future work is to come up with other, realistic noise mod-
els that apply to maximum likelihood estimation problems
such as regression and logistic regression, and determine
when active learning can help under these noise models.
Summary of our main results in this work is given in Ta-
ble 1. We conclude the paper presenting simulations sup-
porting our theoretical bounds as well as experiments on
real-world data.

2. Problem Setup and Preliminaries
Let x denote instances in Rd. Let Px denote a ﬁxed un-
known distribution over instances x. The response y is gen-

MODEL

NOISE
KNOWN
˜O

PASSIVE

ACTIVE

˜O

!

f ∗
∥
f ∗
∥

n + d2
n2 )
n + d2
n4 )

2( 1
∥
2( 1
∥

"

NOISE MODEL ES-
TIMATED
˜O
f ∗

2( d
n )
∥
n + d2
2( 1
"
n2 )
∥

O

f ∗

∥
!
∥
!

!
β∗

"
Table1. Summary of our results: Rates for convergence of esti-
2
mators, i.e.
2, under the heteroscedastic noise model (2).
Here, d is the data dimensionality and n is the number of labeled
!
examples used for estimation.

−

β

"

∥

∥

erated according to the model: y =
+ ηx, where
ηx denotes instance-dependent corruption, and β∗ is the
ground-truth parameter. In this work, we consider the fol-
lowing heteroscedastic model:

β∗, x

⟨

⟩

ηx ∼ N

(0, σ2(x)) ,

(1)

with a standard parametric model for heteroscedastic noise
given by a linear model:
ηx ∼ N

f ∗, x

2) ,

(2)

(0,

⟩

⟨

for some unknown f ∗
= β∗. Each response is indepen-
dently corrupted via (2). The goal is to recover β∗ using
instances drawn from Px and their responses sampled from

(

β∗, x

,

f ∗, x

2).

⟩

⟨

⟩

⟨

N
Remark 1. The noise ηx can be sampled from any sub-
Gaussian distribution with E[ηx] = 0 and bounded second
moment E[η2
σ2 (for some constant σ). For simplicity,
x]
we will consider the Gaussian model (1).

≤

Our approach is based on maximum likelihood estimator
(MLE) for regression.
In the homoscedastic setting (i.e.
σ(x)2 = σ for all x in (1)), MLE is known to give minimax
optimal rates1. The standard least squares estimator com-
puted on n iid training instances (xi, yi), i = 1, 2, . . . , n is
given by:

βLS =

n

xixT
i

1 n

−

xiyi ,

(3)

i=1
%
and is the solution to the minimization problem:

i=1
%

#

$

&

n

βLS = arg min

β

(

β, xi⟩ −

⟨

yi)2.

#

i=1
%
In the heteroscedastic setting, it is easy to show that the
standard least squares estimator is consistent.
Remark 2. Standard least squares estimator is consis-
tent for the heteroscedastic noise model (2): Assuming
Rd, i = 1, 2, . . . , n are drawn iid from the standard
xi ∈
multivariate Gaussian, we have the rate:
2 d
n
∥

2
2 = O
∥

β∗

f ∗

.

∥
$

&

1A notable exception is the Stein’s estimator that may do better

βLS −
∥
#

for high dimensional spaces (Stein et al., 1956)

̸
Active Heteroscedastic Regression

While the estimator (3) is consistent, it does not exploit
the knowledge of the noise model, and does not give bet-
ter rates even when the noise model f ∗ is known exactly.
We look at the maximum likelihood estimator for the het-
eroscedastic noise (1); which is given by the weighted least
squares estimator (or sometimes referred to as generalized
least squares estimator):

n

1 n

−

βGLS =

wixixT
i

$

i=1
%

&

i=1
%

wixiyi,

(4)

#

where wi = 1
σ(xi)2 . When the weights are known, it has
been shown that the weighted estimator is the “correct” es-
timator to study; in particular, it is the minimum variance
unbiased linear estimator (Theorem 10.7, Greene (2002)).
However, we do not know of strong learning rate guaran-
tees for the weighted least squares model in general, or in
particular for the model (2), compared to the ordinary least
squares estimator. This raises two important questions for
which we provide answers in the subsequent sections.

1. What are the rates of convergence of the maximum
likelihood estimator for the heteroscedastic model
when the noise model, aka, f ∗ is unknown?

2. Can we achieve a better label requirement via active

learning?

U

−

≤

O

β∗

β
∥

O(ϵ).

sampled i.i.d.

The problem is formally stated as follows. Given a set of
m instances
x1, x2, . . . , xm}
=
from
{
the underlying Px, a label budget n
m, and access to
that generates responses yi according to the
label oracle
β of
heteroscedastic noise model (2), we want an estimator
the regression model parameter β∗ such that the estimation
error is small, i.e.
∥2 ≤
Remark 3. Existing active regression methods (Sabato
and Munos, 2014; Chaudhuri et al., 2015) do not con-
sider the heteroscedastic noise model. Note that when f ∗
is known exactly, one can reduce heteroscedastic model to
a homoscedastic model, by scaling instances x and their
responses by 1/
. However, we still may not be able
⟨
to apply the existing active learning results to the trans-
formed problem, as the modiﬁed data distribution may no
longer satisfy required nice properties. The resulting esti-
mators do not yield advantages over passive learning, even
in simple cases such as when Px is spherical Gaussian.

f ∗, x

#

#

⟩

Id denotes the identity matrix of size d. We use
Notation.
bold letters to denote vectors and capital letters to denote
matrices.

3. Basic Idea: Noise Model is Known Exactly

To motivate our approach, we begin with the basic het-
eroscedastic setting: when f ∗ is known exactly in (2). Even

in this arguably simple setting, the rates for passive and ac-
tive learning are a priori not clear, and the exercise turns
out to be non-trivial. The results and the analysis here help
gain insights into label complexities achievable via passive
and active learning strategies.

U

f ∗, xi⟩

βGLS|
#

βGLS|
#

X) = (X T W X)−

In the standard (passive) learning setting, we sample n in-
stances uniformly from the set
and compute the maxi-
mum likelihood estimator given in (4) with weights set to
2. The procedure is given in Algorithm
wi = 1/
⟨
1. The resulting estimator is unbiased, i.e. E[
X] =
β∗. Let W denote the diagonal weighting matrix with
Wii = wi. The variance of the estimator is given by:
1. The question of interest is
Var(
βGLS is qualitatively
if and when the weighted estimator
βLS. The
better than the ordinary least squares estimator
following theorem shows that the variance of the latter, and
in turn the estimation error, can be potentially much larger;
and in particular, the difference between their estimation
errors is at least a factor of dimensionality d.
Theorem 1 (Passive Regression With Noise Oracle). Let
βGLS denote the estimator in (4) (or the output of Algo-
(0, Id) i.i.d., with label budget
rithm 1) where xi ∼ N
#
βLS denote the ordinary least squares esti-
n
mator (3). There exist positive constants C ′ and c
1 such
d
that, with probability at least 1
nc , both the statements
hold:

2d ln d and

≥

≥

−

#

#

#

β∗

2
2 ≤
∥

C ′

f ∗
∥

2
∥

β∗

2
2 = Θ
∥

f ∗
∥
$

1
n
$
2 d
n
∥

.

&

+

d2 ln n
n2

,
&

βGLS −
∥
#
βLS −
∥
#

(

β

β

−

N

Remark 4. We present the results for instances sampled
(0, Id) for clarity. The estimation error bounds
from
can be naturally extended to the case of Gaussian distri-
bution with arbitrary covariance matrix Σ.
In this case,
the bounds (in Theorem 1, for example) continue to hold
for the estimation error measured w.r.t. Σ, i.e.
−
β∗)T Σ(
β∗). Furthermore, with some calculations, we
can obtain analogous bounds for sub-Gaussian distribu-
tions, with distribution-speciﬁc constants featuring in the
#
resulting bounds.
Remark 5. In Theorem 1, when n > d, d2/n2 term is the
lower-order term, and thus, up to constants, the error of
2(1/n)
the weighted least squares estimator is at most
∥
while that of the ordinary least squares estimator is at
2(d/n). Thus, if the noise model is known, the
least
weighted least squares estimator can give a factor of d im-
provement in convergence rate.
Remark 6 (Technical challenges). The proofs of key results
in this paper involve controlling quantities such as sum of
ratios of Gaussian random variables, ratios of chi-squared

f ∗
∥

f ∗
∥

#

∥

Active Heteroscedastic Regression

random variables, etc. which do not even have expectation,
let alone higher moments; so, standard concentration ar-
guments cannot be made. However, in many of these cases,
we can show that our error bounds hold with sufﬁciently
high probability.

The following lemma is key to showing Theorem 1; the
proof sketch illustrates some of the aforementioned tech-
nical challenges. Unlike typical results in this domain,
1) by providing concentration bounds
which bound tr(A−
1) by providing lower bound on
for A, we bound tr(A−
each eigenvalue of A.

×

∈

Rn
(0, Id). Assume n

Lemma 1. Let X
i.i.d. from
N
≥
a diagonal matrix, with Wii = 1/
⟨
⟩
Rd. Let σ1 ≥
σ2 ≥ · · · ≥
X T W X. Then, with probability at least (1

d where the rows xi are sampled
2d ln d. Let W denote
xi, f

∈
σd denote the eigenvalues of

2, for ﬁxed f

d
nc ):

−

1. σd(X T W X)

2. σi(X T W X)

n
f

∥

∥

≥

2 and

C ′

≥

d

f

∥

∥

n2
2 ln n for i = 1, 2, . . . , d

1,

−

where c

1 and C ′ > 0 are constants.

≥

Proof. We give a sketch of the proof here (See Appendix
B.2 for details). To show a lower bound on the smallest
eigenvalue, we ﬁrst show that the smallest eigenvector is
very close to f , with sufﬁciently large probability. To do so,
we exploit the fact that the smallest eigenvalue is at most
2 which can be readily seen. For the second part, we
f
n/
∥
∥
consider the variational characterization of d
1st singular
value given by:

−

σd

1(X T W X) =

max

−

U :dim(U )=d

1

v

−

∈

min
v
U,
∥
∥

=1

vT X T W Xv.

n
i=1

We look at the particular subspace that is orthogonal to f ∗
to get the desired upper bound. One key challenge here is
where gi and hi
controlling quantity of the form
are iid Gaussian variables. We use a blocking technique
based on the magnitude of
, and lower bound the
⟨
quantity with just the ﬁrst block (as all the quantities in-
volved are positive). This requires proving a bound on the
order statistics of iid Gaussian random variables (Lemma 7
in Appendix A).

’
f, xi⟩

g2
i
h2
i

⟩ ≈

Theorem 1 shows that weighting “clean” instances (i.e.
0) much more than “noisy” instances yields a
f ∗, x
⟨
highly accurate estimator of β∗. But can we instead pre-
fer querying labels on instances where we know a priori
the response will be relatively noise-free? This motivates a
simple active learning solution — in principle, if we actu-
ally know f ∗, we could query the labels of instances with
low noise, and hope to get an accurate estimator. The active

learning procedure is given in Algorithm 2. Besides label
budget n, it takes another parameter τ as input, which is a
threshold on the noise level.

We state the convergence for Algorithm 2 below:

Theorem 2 (Active Regression with Noise Oracle). Con-
β of Algorithm 2, with input la-
sider the output estimator
bel budget n
= m and
with
(0, Id) i.i.d., and τ = 2n/m. Then, with probabil-
xi ∼ N
ity at least 1

2d ln d, unlabeled set
#

1/nc:

|U|

≥

U

−

β
∥

−

β∗

2
2 ≤
∥

C ′

f ∗
∥

2
∥

1
n

+

d2 ln n
m2

,

&

$

for some constants c > 1 and C ′ > 0.

#

Remark 7. We observe that the estimation error via active
learning (Theorem 2) goes to 1/n as the size of the unla-
beled examples m becomes larger. Note that O(1/n) is the
error for 1-dimensional problem and is much better than
d2/n2 we get from uniform sampling.

≥

n2 unlabeled samples, then we
Remark 8. If we have m
observe that active learning (Theorem 2) achieves a bet-
ter convergence rate compared to that of passive learning
(Theorem 1) — the lower order term in case of active learn-
ing is O( d2
n4 ) versus passive learning which is O( d2
n2 ). The
convergence is superior especially when n < d2 (as we
also observe in simulations).

Rn

Lemma 2. Let X
whose rows xi are sampled from

The proof of Theorem 2 relies on two lemmas stated below.
d denote the design matrix
such that they satisfy
Rd. Assume n
2d ln d.
. . . σd denote the eigenvalues of X T X.
1
nc ):

xi, f
|⟨
Let σ1 ≥
Then, with probability at least (1

f
τ for ﬁxed f
∥
σ2 ≥

⟩| ≤ ∥

≥

∈

∈

U

×

−

1. σd(X T X)

nτ 2,

2. σi(X T X)

1
2 n, for i = 1, 2, . . . , d

1,

−

for some constants C > 0 and c
Lemma 3. For each xi ∈ U
, for any ﬁxed z
xi, z
⟩
⟨
mτ 3
2 ):
least exp( −
z
4
∥

∈

∥

1.

≥
= m, deﬁne gi =
, where
Rd. Then, with probability at

|U|

i :

gi| ≤ ∥

z
∥

|

τ

mτ
2

.

≥

*(
(
(
(

4. Estimating Noise: Algorithms and

Guarantees

≥

≥

)

(
(
(
(

In this section, we will ﬁrst show that we can obtain a
consistent estimator of f ∗, as long as we have a reason-
ably good estimate of β∗. Let β0 denote the ordinary least

Active Heteroscedastic Regression

Algorithm 1 Passive Regression With Noise Oracle

Input: Labeling oracle
1. Choose a subset
2. Estimate
β.
Output:

L
β using (4) on

O

, instances
of size n from

U
U
, with wi = 1/

xi, i
{

=
[m]
uniformly at random from
2.

∈
f ∗, xi⟩

⟨

L

U

, label budget n, noise model f ∗
}

. Query their labels using

.

O

Algorithm 2 Active Regression With Noise Oracle

Input: Labeling oracle
1. Choose a subset

, noise model f ∗, instances

O
of size at most n from

xi, f ∗
|⟨
⟩| ≤
2. Estimate

τ . Query their labels using
β = (X T W X)−
β as

.

O

1X T W y where X

U

U

=

xi, i
{

∈

[m]

, label budget n, noise tolerance τ .
}

with expected noise up to the given tolerance τ , i.e. for all xi ∈ L
Rn, and W is a diagonal matrix with Wii =

d and y

Rn
×

,

L

∈

∈

#

#

#

2 .

1
f ∗,xi⟩
⟨
Output:

#

β.

#

squares estimator of β∗, obtained by using (3), on m1 la-
(0, Id). The largest
beled instances, chosen i.i.d. from
eigenvector of the residual-weighted empirical covariance
matrix given by:

N

1
m1

m1

i=1
%

S =

#

(yi − ⟨

xi,

β0⟩
#

)2xixT
i

.

(5)

gives a sufﬁciently good estimate of f ∗. This is established
formally in the following lemma.
Lemma 4. Let m1 = Ω(d log3 d). Then, with probabil-
ity at least 1
S in (5)
−
converges to f ∗:

, the largest eigenvector ˆf of

1
m5
1

ˆf
∥

−

f ∗

2
2 ≤
∥

C1E[

β0 −
∥
for some positive constant C1, and expectation is wrt the
randomness in the estimator β0.

2
2] + O
∥

β∗

$

#
,

d
m1 &

We ﬁrst discuss the implications of using the estimated ˆf in
order to obtain the generalized least square estimator given
in (4) and then present the active learning solution.

,

4.1. Weighted Least Squares

1X T

W X)−

We now consider a simple (passive) learning algorithm
for the setting where the noise model is estimated, based
on the weighted least squares solution discussed in Sec-
tion 3. We ﬁrst get a good estimator of f ∗ (as in Lemma
4), and then obtain the weighted least squares estimator:
β = (X T
W is the diagonal ma-
W y, where
trix of inverse noise variances obtained using the estimate
ˆf with a small additive offset γ. The procedure is presented
#
in Algorithm 3.
Remark 9. Algorithm 3 can be thought of as a special case
of the well-known iterative weighted least squares (i.e. with
just one iteration), that has been studied in the past (Car-
roll et al., 1988).

+

+

+

It is well-known heuristic to ﬁrst estimate the weights and
then obtain the weighted estimator in practice; the approach
has been widely in use for decades now in multiple com-
munities including Econometrics and Bioinformatics (Har-
vey, 1976; Greene, 2002). However, we do not know of
strong convergence rates for the solution. To our knowl-
edge, the most comprehensive analysis was done by (Car-
roll et al., 1988). Their analysis is not directly applicable to
us for reasons two-fold: (i) they focus on using a maximum
likelihood estimate of the parameters in the heteroscedastic
noise model, and does not apply to our noise model (2), and
(ii) their analysis relies the noise being smooth (for obtain-
ing tighter Taylor series approximation). More importantly,
their analysis conceals a lot of signiﬁcant factors in both d
and n, and the resulting statements about convergence rates
are not useful (See Appendix C).

Theorem 3. Consider the output estimator
rithm 3, with label budget n

β of Algo-
2d ln d and offset γ2 =

d
n . Then, with probability at least 1

1/nc:

#

≥

β
∥

−

β∗

2
2 ≤
∥

C

f ∗
∥

−
2 d ln4 n
n
∥

,

#
for some constants C > 0 and c

1.

≥

Remark 10. Note that the above result holds for the spe-
ciﬁc choice of γ. When γ = 0, we get the weighted
least squares estimator analogous to the one used in Algo-
rithm 1. However, when estimating weights with γ = 0, the
β has poor convergence rate. In par-
resulting estimator
2
ticular, we observe empirically that the error
∥
scales as O( d3

β
∥

β∗

−

#

n ).

#

4.2. Active Regression

We now show that active learning can help overcome the
inadequacy of the passive weighted least squares solution.
The proposed active regression algorithm, presented in Al-

Active Heteroscedastic Regression

#

#

#

#

|⟨

f is indeed very close to f ∗,

gorithm 4, consists of two stages. In the ﬁrst stage, we ob-
f similar to that in Algorithm 3. Note that
tain an estimate
f serves as a good proxy for
if
selecting instances whose labels are nearly noise-free. To
this end, we sample instances that have sufﬁciently small
#
τ , where τ is an input parameter to the
f , x
noise:
⟩| ≤
f is exact, then the algorithm reduces to the
algorithm. If
strategy outlined in Algorithm 2. Our algorithm follows
the strategy of using a single-round of interaction (in light
of the analysis presented in the passive learning setting) to
achieve a good estimate of the underlying β∗ akin to the ac-
tive MLE estimation algorithm studied by Chaudhuri et al.
(2015).
f denote an estimator of f ∗ satisfying
Lemma 5. Let
∆. For a given τ , let
f ∗
f
denote a set of
∥2 ≤
∥
−
2d log d instances sampled from m unlabeled in-
|L| ≥
#
f , xi⟩| ≤
stances
, and let
yi denote their corresponding labels. Consider the ordi-
nary least squares estimator obtained using
#

L
τ , for all xi ∈ L
, i.e.:

, such that

|⟨

U

#

β =

#

$ %xi∈L

1

−

xixT
i

L

xiyi .

&

%xi∈L
1
nc :

−

Then, with probability at least 1

β
∥

−

β∗

2
2 ≤
∥

C

f ∗

∥

2(τ 2 + ∆2)
∥

1
mτ 3 +

1

d
−
mτ

.

&

$
1.

#

for some constants C > 0 and c
Remark 11. The bound in the above theorem recovers the
known variance case discussed in Theorem 2, where the
estimation error ∆2 = 0 and the choice of τ = 2n
m .

≥

Compared to the passive learning error bound in Theorem
3, we hope to get leverage — as long we can choose τ suf-
ﬁciently small, and yet guarantee that the number of sam-
ples m2 in Step 4 of Algorithm 4 is sufﬁciently large. The
following theorem shows that this is indeed the case, and
that the proposed active learning solution achieves optimal
learning rate.
Theorem 4 (Active Regression with Noise Estimation).
β of Algorithm 4, with in-
Consider the output estimator
n2, m1 = n
put label budget n, unlabeled examples m
2
d
n . Then, we have, with probability at least

≥

#

and τ = 2
1/nc:
1

,

−

β
∥

−

β∗

2
2 ≤
∥

C

f ∗
∥

2
∥

1
n

+

d2
n2

.

&

$

#

for some constants C > 0 and c > 1.
Remark 12. We observe that active learning (Theorem 4)
has the same convergence rate for sufﬁciently large n, as
that of the case when f ∗ is known exactly (Theorem 2).
Note that d2/n2 and d2/m2 are lower-order terms in the
compared bounds.

Remark 13. Unlike in the case when noise model was
known (Theorem 2), here we can not do better even with
inﬁnite unlabeled examples. The source of trouble is the
f , so beyond a point even active learn-
estimation error in
ing does not provide improvement. Note that we do not
compute weighted least squares estimator in the ﬁnal step
of Algorithm 4 unlike in Algorithm 2, for the same reason.

#

5. Simulations

We now present simulations that support the convergence
bounds developed in this work. The setup is as fol-
lows. We sample unlabeled instances x1, x2, . . . , xm from
(0, Id). Labels are generated according to the het-
N
eroscedastic model: yi =
, where
⟨
gi are iid standard Gaussian random variables. We ﬁx
∥2 = 1 and d = 10. We look at how the model es-
f ∗
∥
β∗
timation error (in case of Algorithms 1 and 2)
∥
decays as a function of the label budget n (m = 2n2 for all
the simulations). We also check the estimation error of the
noise model in case of Algorithms 3 and 4.

β∗, xi⟩

f ∗, xi⟩

+ gi⟨

β
∥

−

#

∥

#

β

−

≤

d2,

β∗
∥

The results for convergence of model estimation when the
noise model is known are presented in Figure 1 (a)-(d).
In passive learning, the bounds in Theorem 1 suggest that
= O( d
n ); but once n > d2, we
when n
get a convergence of O(1/√n). We observe that the re-
sult in Figure 1 (a) closely matches the given bounds2. In
case of active learning, the bounds in Theorem 2, for the
n2, suggest that we get an error rate of
case when m
n2 ). We observe a similar phenomenon in
β
β∗
∥
the Figure 1 (b). Turning to the noise estimation setting for
passive learning, we see in Figure 1 (c) that the estimation
error of β∗ as well as f ∗ decay as
d/n (as suggested by
Theorem 3); for active learning, we see in Figure 1 (d) that
the estimation error of β∗ is noticeably better, in particular,
better than that of f ∗, and approaches 1/√n as n becomes
larger than d2.

≥
= O( d

-

−

#

∥

We also study the performance of the algorithms on two
real-world datasets from UCI: (1) WINE QUALITY with
m = 6500 and d = 11, and (2) MSD (a subset of the mil-
lion song dataset) with m = 515345 and d = 90. For each
dataset, we create a 70-30 train-test split, and learn the best
linear regressor using ordinary least squares, which forms
our β∗. We then sample labels using β∗ and a simulated
heteroscedastic noise f ∗. We compare active and passive
learning algorithms on the root mean square error (RMSE)
obtained on the test set. In Figure 1 (e), we see that ac-
tive learning with noise estimation gives a signiﬁcant re-
duction in RMSE early on for WINE QUALITY. We also
see that weighted least squares gives slight beneﬁt over or-

2For better resolution, we plot

given in the theorem statements

β∗

β

−

∥

∥

rather than

β∗

2

β

−

∥

∥

!

!

Active Heteroscedastic Regression

dinary least squares. On MSD dataset 3, again we observe
that our active learning algorithm consistently achieves a
marginal reduction in RMSE as the number of labeled ex-
amples increases.

6. Conclusions and Future Work

In conclusion, we consider active regression under a het-
eroscedastic noise model. Previous work has looked at
active regression either with no model mismatch (Chaud-
huri et al., 2015) or arbitrary model mismatch (Sabato and
Munos, 2014). In the ﬁrst case, active learning provided no
improvement even in the simple case where the unlabeled
examples were drawn from Gaussians. In the second case,
under arbitrary model mismatch, the algorithm either re-
quired a very high running time or a large number of labels.
We provide bounds on the convergence rates of active and
passive learning for heteroscedastic regression. Our results
illustrate that just like in binary classiﬁcation, some partial
knowledge of the nature of the noise has the potential to
lead to signiﬁcant gains in the label requirement of active
learning.

There are several avenues for future work. For simplicity,
the convergence bounds we present relate to the case when
the distribution Px over unlabeled examples is a Gaussian.
An open problem is to combine our techniques with the
techniques of (Chaudhuri et al., 2015) and establish con-
vergence rates for general unlabeled distributions. Another
interesting line of future work is to come up with other,
realistic noise models that apply to maximum likelihood
estimation problems such as regression and logistic regres-
sion, and determine when active learning can help under
these noise models.

3here, the response variable is the year of the song; we make

the response mean zero in our experiments

Active Heteroscedastic Regression

, unlabeled samples

Algorithm 3 Least Squares with Estimated Weights
=

Input: Labeling oracle
O
1. Draw m1 examples uniformly at random from
2. Estimate
3. Draw a subset
4. Compute
wi =
5. Set

β0 by solving y

L
#
2+γ2 ., for xi ∈ L
β = (X T

xi,
⟨
⟩
#
β by solving:

W X)−

1X T

X

U

1

f

.

β0 where X
of n examples uniformly at random from

≈

∈

∈

[m]

, label budget n, parameter m1, offset γ.
}
.

xi, i
{
and query their labels y using
U
Rm1 is the vector of labels.
d has xi as rows and y
Rm1×
∈
Rn.
d and y
Rn
×

. Form X

O

U

∈

∈

#
f as the largest eigenvector of the residual-weighted empirical covariance given in (5).

6. Estimate
#
β.
Output:

!

#

#

W y, where

W is diagonal matrix with

Wii = ˆwi.

#

+

+

+

+

Algorithm 4 Active Regression

U

, unlabeled samples

Input: Labeling oracle
O
1. Draw m1 examples uniformly at random from
2. Estimate
3. Compute
4. Choose a subset
xi ∈ L
5. Estimate
β.
Output:

β0 by solving y
f as the largest eigenvector of
#
#
of m2 = n
L
−
τ 2. Query their labels using
#
f
xi,
⟩|
β as
#

1X T y where X

β = (X T X)−

β0 where X

X

≤

≈

∈

|⟨

#

2

,

#

#

#

, label budget n, parameters m1, τ .
}

∈

=

[m]

xi, i
{
and query their labels y using
U
Rm1 .
Rm1×
S given in (5).

d and y

∈

.

O

m1 instances from

with estimated noise variance up to tolerance τ 2, i.e. for all

U

.
O
Rm2×

∈

d and y

Rm2 .

∈

d/n
1/√n
Algorithm 1

√d/n√n
1/√n
Algorithm 2

!d/n
β − β∗
∥
"
∗
f − f
∥
"

∥
∥

0.4

0.3

∥

∗

β
−
ˆβ

0.2

∥

0.1

0

0

1

0.8

0.6

0.4

0.2

0

0

r
o
r
r

E

0.2

0.15

∥

∗

β
−
ˆβ

∥

0.1

0.05

0

0

0.96

0.94

0.92

0.9

0.88

0.86

0.84

0

E
S
M
R

200
n

200
n

√d/√n
1/√n
d/n
!β − β∗
!
f − f

∗

∥
∥

∥
∥

n

2000
n

r
o
r
r
E

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

E
S
M
R

1.35

1.3

1.25

1.2

1.15

1.1

0

100

300

400

200

400

600

800

1000

100

300

400

200
n

(a) Algorithm 1 (Passive)

(b) Algorithm 2 (Active)

(c) Algorithm 3 (Passive)

Ordinary Least Squares
Algorithm 3 (Passive Learning)
Algorithm 4 (Active Learning)

Ordinary Least Squares
Algorithm 3 (Passive Learning)
Algorithm 4 (Active Learning)

100

300

400

1000

3000

4000

2000

4000

6000

8000

10000

(d) Algorithm 4 (Active)

(e) WINE QUALITY

n

(f) MSD

Figure1. Plots (a)-(b): convergence of model (β∗) estimation error, when the noise model is known. Plots (c)-(d): convergence of model
(β∗) estimation error as well as noise parameter (f ∗) estimation error, when the noise model is estimated. Plots (e)-(f): RMSE on test
data for two real-world datasets.

Active Heteroscedastic Regression

Sivan Sabato and Remi Munos. Active regression by strat-
iﬁcation. In Advances in Neural Information Processing
Systems, pages 469–477, 2014.

Charles Stein et al. Inadmissibility of the usual estimator
In
for the mean of a multivariate normal distribution.
Proceedings of the Third Berkeley symposium on mathe-
matical statistics and probability, volume 1, pages 197–
206, 1956.

Chicheng Zhang and Kamalika Chaudhuri.

Beyond
disagreement-based agnostic active learning. In Neural
Information Processing Systems (NIPS), 2014.

References

Oren Anava and Shie Mannor. Heteroscedastic sequences:
Beyond gaussianity. In Proceedings of The 33rd Interna-
tional Conference on Machine Learning, pages 755–763,
2016.

Pranjal Awasthi, Maria-Florina Balcan, and Philip M.
Long. The power of localization for efﬁciently learning
linear separators with noise. In Symposium on Theory of
Computing, STOC 2014, New York, NY, USA, May 31 -
June 03, 2014, pages 449–458, 2014.

Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab,
and Hongyang Zhang. Learning and 1-bit compressed
sensing under asymmetric noise. In Proceedings of the
29th Conference on Learning Theory, COLT 2016, New
York, USA, June 23-26, 2016, pages 152–192, 2016.

M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic
active learning. J. Comput. Syst. Sci., 75(1):78–89, 2009.

A. Beygelzimer, S. Dasgupta, and J. Langford. Importance

weighted active learning. In ICML, 2009.

Raymond J Carroll, CF Jeff Wu, and David Ruppert. The
effect of estimating weights in weighted least squares.
the American Statistical Association, 83
Journal of
(404):1045–1054, 1988.

Kamalika Chaudhuri, Sham M Kakade, Praneeth Netra-
palli, and Sujay Sanghavi. Convergence rates of ac-
tive learning for maximum likelihood estimation. In Ad-
vances in Neural Information Processing Systems, pages
1090–1098, 2015.

S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnos-

tic active learning algorithm. In NIPS, 2007.

Yehoram Gordon, Alexander Litvak, Carsten Sch¨utt, and
Elisabeth Werner. On the minimum of several random
variables. Proceedings of the American Mathematical
Society, 134(12):3665–3675, 2006.

William H Greene. Econometric analysis. Prentice Hall,

2002.

S. Hanneke. A bound on the label complexity of agnostic

active learning. In ICML, 2007.

Andrew C Harvey. Estimating regression models with mul-
tiplicative heteroscedasticity. Econometrica: Journal of
the Econometric Society, pages 461–465, 1976.

Prateek Jain and Ambuj Tewari. Alternating minimiza-
tion for regression problems with vector-valued outputs.
In Advances in Neural Information Processing Systems,
pages 1126–1134, 2015.

