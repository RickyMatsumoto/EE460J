Equivariance Through Parameter-Sharing

Siamak Ravanbakhsh 1 Jeff Schneider 1 Barnab´as P´oczos 1

Abstract

We propose to study equivariance in deep neu-
ral networks through parameter symmetries. In
particular, given a group (cid:71) that acts discretely
on the input and output of a standard neural net-
work layer φW ∶ (cid:82)M
→ (cid:82)N , we show that φW
is equivariant with respect to (cid:71)-action iff (cid:71) ex-
plains the symmetries of the network parameters
W. Inspired by this observation, we then pro-
pose two parameter-sharing schemes to induce
the desirable symmetry on W. Our procedure
for tying the parameters achieves (cid:71)-equivariance
and, under some conditions on the action of (cid:71),
it guarantees sensitivity to all other permutation
groups outside (cid:71).

Given enough training data, a multi-layer perceptron would
eventually learn the domain invariances in a classiﬁcation
task. Nevertheless, success of convolutional and recurrent
networks suggests that encoding the domain symmetries
through shared parameters can signiﬁcantly boost the gen-
eralization of deep neural networks. The same observation
can be made in deep learning for semi-supervised and un-
supervised learning in structured domains. This raises an
important question that is addressed in this paper: What
kind of priors on input/output structure can be encoded
through parameter-sharing?

This work is an attempt at answering this question, when
our priors are in the form discrete domain symmetries. To
formalize this type of prior, a family of transformations of
input and output to a neural layer are expressed as group
“action” on the input and output. The resulting neural net-
work is invariant to this action, if transformations of the in-
put within that particular family, does not change the output
(e.g., rotation-invariance). However, if the output is trans-
formed, in a predictable way, as we transform the input,
the neural layer is equivariant to the action of the group.

1School of Computer Science, Carnegie Mellon University,
5000 Forbes Ave., Pittsburgh, PA, USA 15217. Correspondence
to: Siamak Ravanbakhsh <mravanba@cs.cmu.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Our goal is to show that parameter-sharing can be used to
achieve equivariance to any discrete group action.

Application of group theory in machine learning has been
the topic of various works in the past (e.g., Kondor, 2008;
Bart´ok et al., 2010). In particular, many probabilistic in-
ference techniques have been extended to graphical models
with known symmetry groups (Raedt et al., 2016; Kerst-
ing et al., 2009; Bui et al., 2012; Niepert, 2012). Deep and
hierarchical models have used a variety of techniques to
study or obtain representations that isolate transformations
from the “content” (e.g., Hinton et al., 2011; Jayaraman
& Grauman, 2015; Lenc & Vedaldi, 2015; Agrawal et al.,
2015). The simplest method of achieving equivariance is
through data-augmentation (Krizhevsky et al., 2012; Diele-
man et al., 2015). Going beyond augmentation, several
methods directly apply the group-action, in one way or an-
other, by transforming the data or its encodings using group
members (Jaderberg et al., 2015; Anselmi et al., 2013;
Dieleman et al., 2016). An alternative path to invariance via
harmonic analysis. In particular cascade of wavelet trans-
forms is investigated in (Bruna & Mallat, 2013; Oyallon &
Mallat, 2015; Sifre & Mallat, 2013). More recently (Cohen
& Welling, 2016b) study steerable ﬁlters (e.g., Freeman
et al., 1991; Hel-Or & Teo, 1998) as a general mean for
achieving equivariance in deep networks. Invariance and
equivariance through parameter-sharing is also discussed
in several prior works (Cohen & Welling, 2016a; Gens &
Domingos, 2014).

The desirability of using parameter-sharing for this purpose
is mainly due to its simplicity and computational efﬁciency.
However, it also suggests possible directions for discov-
ering domain symmetries through regularization schemes.
Following the previous work on the study of symmetry in
deep networks, we rely on group theory and group-actions
to formulate invariances and equivariances of a function.
Due to discrete nature of parameter-sharing, our treatment
here is limited to permutation groups. Action of a permu-
tation group (cid:71) can model discrete transformations of a set
of variables, such as translation and 90○ rotation of pixels
around any center in an image. If the output of a function
transforms with a (cid:71)-action as we transform its input with a
different (cid:71)-action, the function is equivariant with respect
to action of (cid:71). For example, in a convolution layer, as we
translate the input, the feature-maps are also translated. If

Equivariance Through Parameter-Sharing

Figure 1. Summary: given a group action on input and output of a neural network layer, deﬁne a parameter-sharing for this layer that is
equivariant to these actions.
(left) (cid:71) = (cid:68)5 is a Dihedral group, acting on a 4 × 5 input image and an output vector of size 5. (cid:78) and (cid:77) denote the index set of input,
and output variables respectively. Here (cid:71) is represented using its Cayley diagram.
(middle-left) (cid:71)-action for (cid:103) ∈ (cid:71) is shown for an example input. (cid:71)-action on the input is a combination of circular shifts (blue arrows)
and vertical ﬂips (red arrows) of the 2D image. (cid:71) acts on the output indices (cid:77) only through circular shift. A permutation group (cid:71)(cid:78),(cid:77)
encodes the simultaneous “action” of (cid:71) on input and output indices.
(middle-right) The structure Ω designed using our procedure, such that its symmetries (cid:65)(cid:117)(cid:116)(Ω) subsumes the permutation group (cid:71)(cid:78),(cid:77).
(right) the same structure Ω unfolded to a bipartite form to better show the resulting parameter-sharing in the neural layer. The layer is
equivariant to (cid:71)-action: shifting the input will shift the output of the resulting neural network function, while ﬂipping the input does not
change the output.

the output does not transform at all, the function is invariant
to the action of (cid:71). Therefore, invariance is a special equiv-
ariance. In this example, different translations correspond
to the action of different members of (cid:71).

The novelty of this work is its focus on the “model sym-
metry” as a gateway to equivariance. This gives us new
theoretical guarantees for a “strict” notion of equivariance
in neural networks. The core idea is simple: consider a col-
ored bipartite graph Ω representing a neural network layer.
Edges of the same color represent tied parameters. This
neural network layer as a function is equivariant to the ac-
tions of a given group (cid:71) (and nothing more) iff the action
of (cid:71) is the symmetry group of Ω – i.e., there is a simple
bijection between parameter symmetries and equivariences
of the corresponding neural network.

The problem then boils down to designing colored bipartite
graphs with given symmetries, which constitutes a major
part of this paper. Fig. 1 demonstrates this idea.1

For the necessary background on group theory see the Ap-
pendix. In the following, Section 1 formalizes equivariance
wrt discrete group action. Section 2 relates the model sym-
metries a neural layer to its equivariance. Section 3 then
builds on this observation to introduce two procedures for
parameter-sharing that achieves a desirable equivariance.

1Throughout this paper, since we deal with ﬁnite sets, we use
circular shift and circular convolution instead of shift and convo-
lution. The two can be made identical with zero-padding of the
input.

Here, we also see how group and graph convolution as well
as deep-sets become special instances in our parameter-
sharing procedure, which provides new insight and im-
proved design in the case of group convolution. Where in-
put and output of the layer have a one-to-one mapping, we
see that the design problem reduces a well-known problem
in combinatorics.

1. Group Action and Equivariance

Let x = [x1, . . . , xN ] ∈ (cid:88)N denote a set of variables and
(cid:71) = {(cid:103)} be a ﬁnite group. The discrete action of (cid:71) on x
is in the form of permutation of indices in (cid:78) = {1, . . . , N }.
This group is a subgroup of the symmetric group (cid:83)(cid:78); the
group of all N ! permutations of N objects. We use —→(cid:78) =
[1, . . . , N ] to denote the ordered counterpart to (cid:78) and the
(cid:71)-action on this vector (cid:103)—→(cid:78) ≐ [(cid:103)1, . . . , (cid:103)N ] is a simple
permutation. Using x—→
to denote x, the discrete action of
(cid:78)
(cid:103) ∈ (cid:71) on x ∈ (cid:88)N is given by (cid:103)x—→
.

(cid:78) ≐ x

—→
(cid:78)

(cid:103)

(cid:71)-action on (cid:78) is a permutation group that is not necessar-
ily isomorphic to (cid:71) itself. (cid:71)(cid:78) ≤ (cid:71) captures the structure
of (cid:71) when it acts on (cid:78). We use (cid:103)(cid:78) to denote the image of
(cid:103) ∈ (cid:71) in (cid:71)(cid:78). (cid:71)-action is faithful iff two groups are iso-
morphic (cid:71) ≅ (cid:71)(cid:78) – that is (cid:71)-action preserves its structure.
In this case, each (cid:103) ∈ (cid:71) maps to a distinct permutation
(cid:103)—→(cid:78) ≠ (cid:103)′—→(cid:78) ∀(cid:103), (cid:103)′
∈ (cid:71). Given any (cid:71)-action on (cid:78) we can
efﬁciently obtain (cid:71)(cid:78); see Appendix.

Equivariance Through Parameter-Sharing

(cid:71)(cid:78),(cid:77) = (cid:71)(cid:78) ⊙ (cid:71)(cid:77) ≐ {((cid:103)(cid:78), (cid:103)(cid:77)) ∣ (cid:103) ∈ (cid:71)}.

φW(2(cid:78)x) =

the

Example 1.1 (Cyclic Group) Consider
cyclic
group (cid:71) = (cid:90)6 and deﬁne its action on x ∈ (cid:82)3 by
deﬁning it on the index set (cid:78) = {1, 2, 3} as (cid:103)n ≐ (cid:103) + n
mod 3 ∀(cid:103) ∈ (cid:90)6. This action is not faithful.
For
example, the action of (cid:103) = 1 and (cid:103) = 4 result in the
same permutations of variables in x; i.e., single-step
of circular shift to the right. With the above action,
the resulting permutation group (cid:71)(cid:78) is isomorphic to
(cid:90)3 < (cid:90)6.
Now consider the same group (cid:71) = (cid:90)6 with a different
action on (cid:78): (cid:103)n ≐ (cid:103) − n mod 3 ∀(cid:103) ∈ (cid:90)6, where we
replaced (+) with (−). Let ˜(cid:71)(cid:78) be the resulting permuta-
tion group. Here again ˜(cid:71)(cid:78) ≅ (cid:90)3. Although isomorphic,
˜(cid:71)(cid:78) ≠ (cid:71)(cid:78), as they are different permutation groups of (cid:78).

Consider the function φ ∶ (cid:88)N
be the action of (cid:71) on input/output index sets (cid:78) and (cid:77).

→ (cid:89) M and let (cid:71)(cid:78) and (cid:71)(cid:77)

Deﬁnition 1.1 The joint permutation group (cid:71)(cid:78),(cid:77) is a sub-
direct product (or pairing) of (cid:71)(cid:78) and (cid:71)(cid:77)

We are now ready to deﬁne equivariance and invariance.
φ(⋅) is (cid:71)(cid:78),(cid:77)-equivariant iff

(cid:103)(cid:78)φ(x) = φ((cid:103)(cid:77)x) ∀x ∈ (cid:88)N , ((cid:103)(cid:78), (cid:103)(cid:77)) ∈ (cid:71)(cid:78),(cid:77)

(1)

Moreover, if (cid:71)(cid:77) = {(cid:101)} is trivial, we have

(cid:103)(cid:78)φ(x) = φ(x) ∀x ∈ (cid:88)N , (cid:103)(cid:78) ∈ (cid:71)(cid:78)

and φ(⋅) is (cid:71)(cid:78)-invariant.
(cid:103)(cid:78) and (cid:103)(cid:77) can also be represented using permutation ma-
M ×M . Equivari-
N ×N , and G(cid:77) ∈ {0, 1}
trices G(cid:78) ∈ {0, 1}
ance relation of (1) then becomes

G(cid:77)φ(x) = φ(G(cid:78)x) ∀x ∈ (cid:88)N , (G(cid:78), G(cid:77)) ∈ (cid:71)(cid:78),(cid:77) (2)

The following observation shows that the subgroup rela-
tionship affects equivariance and invariance.
Observation 1.1 If the function φ ∶ (cid:88)N
→ (cid:89) M is (cid:71)(cid:78),(cid:77)-
equivariant, then it is also (cid:72)(cid:78),(cid:77)-equivariant for any per-
mutation group (cid:72)(cid:78),(cid:77) < (cid:71).

Example 1.2 (Reverse Convolution) Consider
the
cyclic group (cid:71) = (cid:90)6 and for (cid:103) ∈ (cid:71), deﬁne the action
on (cid:78) = {1, 2, 3} to be (cid:103)n ≐ (cid:103) + n mod 3. Also let its
action on (cid:77) = {1, . . . , 6} be (cid:103)m ≐ (cid:103) − n mod 6. In
other words, (cid:71)-action on (cid:78) performs circular shift to
the right and its action on (cid:77) shifts variables to the left.
Examples of the permutation matrix representation for

two members of (cid:71)(cid:78) and (cid:71)(cid:77) are

2(cid:78) = (

0 1 0
0 0 1
1 0 0 )

2(cid:77) =

0 0 1 0 0 0
0 1 0 0 0 0
1 0 0 0 0 0
0 0 0 0 0 1
0 0 0 0 1 0
0 0 0 1 0 0

⎛
⎜
⎝

⎞
⎟
⎠

corresponding to right and left shift on vectors of differ-
ent lengths. Now consider the function φ ∶ (cid:82)N

→ (cid:82)M

φW(x) = Wx WT

= (

0 a b 0 a b
a b 0 a b 0

b 0 a b 0 a ) ∀a, b ∈ (cid:82)

Using permutation matrices one could check the equiv-
ariance condition (2) for this function. We can show that
φ is equivariant to (cid:71)(cid:78),(cid:77). Consider 2 ∈ (cid:90)6 and its images
2(cid:78) ∈ (cid:71)(cid:78) and 2(cid:77) ∈ (cid:71)(cid:77). L.h.s. of (2) is

2(cid:77)φW(x) =

0 0 1 0 0 0
0 1 0 0 0 0
1 0 0 0 0 0
0 0 0 0 0 1
0 0 0 0 1 0
0 0 0 1 0 0

⎛
⎜
⎝

⎞
⎟
⎠

⎛
⎜
⎝

0 a b
a b 0
b 0 a
0 a b
a b 0
b 0 a

⎞
⎟
⎠

x =

b 0 a
0 a b
a b 0
b 0 a
0 a b
a b 0

⎛
⎜
⎝

⎞
⎟
⎠

x

which is equal to its r.h.s.

0 a b
a b 0
b 0 a
0 a b
a b 0
b 0 a

⎛
⎜
⎝

⎞
⎟
⎠

0 1 0
0 0 1

1 0 0 ) x =

(

b 0 a
0 a b
a b 0
b 0 a
0 a b
a b 0

⎛
⎜
⎝

⎞
⎟
⎠

x

for any x. One could verify this equality for all (cid:103) ∈ (cid:90)6.
Now consider the group (cid:72)(cid:78),(cid:77) < (cid:71)(cid:78),(cid:77), where (cid:72)(cid:78) = (cid:71)(cid:78)
and members of (cid:72)(cid:77) = {0, 2, 4}, perform left circular
shift of length 0, 2 and 4. It is easy to see that (cid:72)(cid:78),(cid:77) ≅
(cid:90)3. Moreover since (cid:72)(cid:78),(cid:77) < (cid:71)(cid:78),(cid:77), φ(⋅) above is (cid:72)(cid:78),(cid:77)-
equivariant as well. However, one prefers to characterize
the equivariance properties of φ using (cid:71)(cid:78),(cid:77) rather than
(cid:72)(cid:78),(cid:77).

The observation above suggests that (cid:71)(cid:78),(cid:77)-equivariance is
not restrictive enough. As an extreme case, a constant func-
tion φ(x) = 1 is equivariant to any permutation group
(cid:71)(cid:78),(cid:77) ≤ (cid:83)(cid:78) × (cid:83)(cid:77). In this case equivariance of φ with re-
spect to a particular (cid:71)(cid:78),(cid:77) is not very informative to us. To
remedy this, we deﬁne a more strict notion of equivariance.

→ (cid:89) M is
Deﬁnition 1.2 we say a function φ ∶ (cid:88)N
uniquely (cid:71)-equivariant iff it is (cid:71)-equivariant and it is
“not” (cid:72)-equivariant for any (cid:72) > (cid:71).

2. Symmetry Groups of a Network

Given a group (cid:71), and its discrete action through (cid:71)(cid:78),(cid:77), we
are interested in deﬁning parameter-sharing schemes for a
parametric class of functions that guarantees their unique
(cid:71)(cid:78),(cid:77)-equivariance. We start by looking at a single neural
layer and relate its unique (cid:71)(cid:78),(cid:77)-equivariance to the sym-
metries of a colored multi-edged bipartite graph that de-

Equivariance Through Parameter-Sharing

Example 2.1 (Reverse Convolution) Revisiting
the condition of
Example 1.2 we can show that
In this case σ(x) = x and the
Theorem 2.1 holds.
parameter-sharing of the matrix W is visualized below,
where we used two different line styles for a, b ∈ (cid:82).

ﬁnes parameter-sharing. We then show that the idea ex-
tends to multiple-layers.

Deﬁnition 2.1 A colored multi-edged bipartite graph Ω =
((cid:78), (cid:77), α) is a triple, where (cid:78) and (cid:77) are its two sets of
nodes, and α ∶ (cid:78) × (cid:77) → 2{1,...,C} is the edge function
that assigns multiple edge-colors from the set {1, . . . , C}
to each edge. Non-existing edges receive no color.

We are interested in the symmetries of this structure. The
set of permutations (π(cid:78), π(cid:77)) ∈ (cid:83)(cid:78) × (cid:83)(cid:77) of nodes (within
each part of the bipartite graph) that preserve all edge-
colors deﬁne the Automorphism Group (cid:65)(cid:117)(cid:116)(Ω) ≤ (cid:83)(cid:78) ×
(cid:83)(cid:77) – that is ∀(n, m) ∈ (cid:78) × (cid:77)

(π(cid:78), π(cid:77)) ∈ (cid:65)(cid:117)(cid:116)(Ω) ⇔ α(n, m) = α((π(cid:78)n, π(cid:77)m)) (3)

Alternatively, to facilitate the notation, we deﬁne the same
structure (colored multi-edged bipartite graph) as a set
of binary relations between (cid:78) and (cid:77) – that is Ω =
((cid:78), (cid:77), {∆c}1≤c≤C) where each relation is associated with
one color ∆c = {(n, m) ∣ c ∈ α(n, m)∀(n, m) ∈ (cid:78) × (cid:77)}.
This deﬁnition of structure, gives an alternative expression
for (cid:65)(cid:117)(cid:116)(Ω)

(π(cid:78), π(cid:77)) ∈ (cid:65)(cid:117)(cid:116)(Ω) ⇔

(4)

((n, m) ∈ ∆c ⇔ (π(cid:78)n, π(cid:77)m) ∈ ∆c) ∀c, n, m

The signiﬁcance of this structure is in that, it deﬁnes a
parameter-sharing scheme in a neural layer, where the same
edge-colors correspond to the same parameters. Consider
the function φ ≐ [φ1, . . . , φM ] ∶ (cid:82)N

→ (cid:82)M

φm(x; w, Ω) ≐ σ( ∑
n

∑
c∈α(n,m)

wcxn) ∀m (5)

where σ ∶ (cid:82) → (cid:82) is a strictly monotonic nonlinearity and
w = [w1, . . . wc, . . . , wC] is the parameter-vector for this
layer.

The following key theorem relates the equivariances of
φ(⋅; w, Ω) to the symmetries of Ω.

Theorem 2.1 For any w ∈ (cid:82)C s.t., wc ≠ wc′ ∀c, c′, the
function φ(⋅; w, Ω) is uniquely (cid:65)(cid:117)(cid:116)(Ω)-equivariant.

Corollary 2.2 For any (cid:72)(cid:78),(cid:77) ≤ (cid:65)(cid:117)(cid:116)(Ω),
φ(⋅; w, Ω) is (cid:72)(cid:78),(cid:77)-equivariant.

the function

The implication is that to achieve unique equivariance for
a given group-action, we need to deﬁne the parameter-
sharing using the structure Ω with symmetry group (cid:71)(cid:78),(cid:77).

In this ﬁgure, the circular shift of variables at the output
and input level to the left and right respectively, does not
change the edge-colors. For example in both cases node
1 ’s connection to nodes 3 , 6 using dashed-lines is
preserved.

Six repetitions of this action produces different permuta-
tions corresponding to six members of (cid:71)(cid:78),(cid:77). Therefore
(cid:71)(cid:78),(cid:77) ≤ (cid:65)(cid:117)(cid:116)(Ω) and according to Corollary 2.2, φ(⋅) is
(cid:71)(cid:78),(cid:77) equivariant. Moreover, using Theorem 3.3 of the
next section, we can prove that these six permutations
are the “only” edge-color preserving ones for this struc-
ture, resulting in unique equivariance.

Matrix Form. To write (5) in a matrix form, if there are
multiple edges between two nodes n, m, we need to merge
them. In general, by assigning on distinct color to any set in
the range of α ∶ (cid:78) × (cid:77) → 2{1,...,C} we can w.l.o.g. reduce
multiple edges to a single edge.
In other words we can
rewrite φ using W ∈ (cid:82)M ×N

φ(x; w; Ω) = σ(Wx) Wm,n = ∑

wc

(6)

c∈α(n,m)

Using this notation, and due to strict monotonicity of the
nonlinearity σ(⋅), Theorem 2.1 simply states that for all
((cid:103)(cid:78), (cid:103)(cid:77)) ∈ (cid:65)(cid:117)(cid:116)(Ω), x ∈ (cid:82)N and W given by (6)

G(cid:77)Wx = WG(cid:78)x.

(7)

Example 2.2 (Permutation-Equivariant Layer)
Consider all permutations of indices (cid:78) and (cid:77) = (cid:78).

We want to deﬁne a neural layer such that all permu-
tations of the input (cid:103)(cid:78) ∈ (cid:71)(cid:78) = (cid:83)(cid:78) result in the same

Equivariance Through Parameter-Sharing

permutation of the output (cid:103)(cid:77) = (cid:103)(cid:78). Consider the fol-
lowing colored bipartite graph, for a special case where
N = M = 4. It is easy to show that color-preserving
permutations of this structure are (cid:65)(cid:117)(cid:116)(Ω) = (cid:83)(cid:78) ⊙ (cid:83)(cid:78) =
{((cid:103), (cid:103)) ∣ (cid:103) ∈ (cid:83)(cid:78)} ≅ (cid:83)(cid:78): On one hand, for (π(cid:78), π(cid:77)) ∈
(cid:83)(cid:78) × (cid:83)(cid:77), having π(cid:78) = π(cid:77) clearly preserves the colors.
On the other hand, if π(cid:78) ≠ π(cid:77), there exists u ∈ (cid:78) (also in
(cid:77)) such that π(cid:78)u ≠ π(cid:77)u. Therefore (π(cid:78), π(cid:77)) does not
preserve the relation ∆ = {(n, n) ∣ n ∈ (cid:78)} correspond-
ing to dashed edges, and therefore (π(cid:78), π(cid:77)) ∉ (cid:65)(cid:117)(cid:116)(Ω).
This proves (cid:65)(cid:117)(cid:116)(Ω) = (cid:83)(cid:78) ⊙ (cid:83)(cid:78). The function (5) for
this Ω is

φ(x; w = [w1, w2], Ω) = σ(w1Ix + w211Tx).

Ravanbakhsh et al. (2016); Zaheer et al. (2017) derive
the same permutation equivariant layer, by proving the
commutativity in (7), while here it follows from Corol-
lary 2.2.

Multiple Layers. For deep networks, the equivariance of
the composition φ2 ○ φ1 to (cid:71)-action follows from that of
individual layer φ1 ∶ (cid:88)N
→ (cid:90)O. As-
suming φ1 is (cid:71)(cid:78),(cid:77)-equivariant and φ2 is (cid:71)(cid:77),(cid:79)-equivariant,
where (cid:71)-action on (cid:77) is shared between the two layers, it
follows that φ2 ○ φ1 is (cid:71)(cid:78),(cid:79)-equivariant, where (cid:71)(cid:78),(cid:79) =
(cid:71)(cid:78) ⊙ (cid:71)(cid:79). This is because ∀(cid:103) ∈ (cid:71) and x ∈ (cid:88)N

→ (cid:89) M and φ2 ∶ (cid:89) M

φ2(φ1((cid:103)(cid:78)x)) = φ2((cid:103)(cid:77)φ1(x)) = (cid:103)(cid:79)φ2(φ1(x)).

(8)

3. Structure Design

Consider the deﬁnition of neural layer (5) that employs
parameter-sharing according to Ω. Given (cid:71)-action on (cid:78)
and (cid:77), we are interested in designing structures Ω such
that (cid:65)(cid:117)(cid:116)(Ω) = (cid:71)(cid:78),(cid:77). According to the Theorem 2.1, it
then follows that φ is uniquely (cid:71)(cid:78),(cid:77)-equivariant. Here,
we give the sufﬁcient conditions and the design recipe to
achieve this.

For this we brieﬂy review some group properties that are
used in later developments.

transitivity We say that (cid:71)-action on (cid:78) is transitive iff
∀n1, n2 ∈ (cid:78), there exists at least one action (cid:103) ∈ (cid:71)
(or (cid:103)(cid:78) ∈ (cid:71)(cid:78)) such that (cid:103)n1 = n2.

regularity The group action is free or semi-regular iff
∀n1, n2 ∈ (cid:78), there is at most one (cid:103) ∈ (cid:71) such at
(cid:103)n1 = n2, and the action is regular iff it is both tran-
sitive and free – i.e., for any pair n1, n2 ∈ (cid:78), there is
uniquely one (cid:103) ∈ (cid:71) such that (cid:103)n1 = n2. Any free ac-
tion is also faithful. We use a similar terminology for
(cid:71)(cid:78). That is we call (cid:71)(cid:78) semi-regular iff ∀n1, n2 ∈ (cid:78) at

most one (cid:103)(cid:78) ∈ (cid:71)(cid:78) moves n1 to n2 and (cid:71)(cid:78) is regular if
this number is exactly one.

orbit The orbit of n ∈ (cid:78) is all the members to which it
can be moved, (cid:71)n = {(cid:103)n ∣ (cid:103) ∈ (cid:71)}. The orbits of
n ∈ (cid:78) form an equivalence relation2 This equivalence
relation partitions (cid:78) into orbits (cid:78) = ⋃1≤p≤P (cid:71)np,
where np is an arbitrary representative of the parti-
tion (cid:71)np ⊆ (cid:78). Note that the (cid:71)-action on (cid:78) is always
transitive on its orbits – that is for any n, n′
∈ (cid:71)np,
there is at least one (cid:103) ∈ (cid:71) such that n = (cid:103)n′. There-
fore, for a semi-regular (cid:71)-action, the action of (cid:71) on
the orbits (cid:71)np∀1 ≤ p ≤ P is regular.

Example 3.1 (Mirror Symmetry) Consider (cid:71) = (cid:90)2 =
{(cid:101) = 0, 1} (1 + 1 = 0) acting on (cid:78), where the
only non-trivial action is deﬁned as ﬂipping the input:
1(cid:78)[1, . . . , N ] = [N, N − 1, . . . , 1].
(cid:71) is faithful in its action on (cid:78), however (cid:71)(cid:78) is not tran-
sitive – e.g., N cannot be moved to N − 1. If N is even,
then (cid:71)-action is semi-regular. This is because otherwise
N
the element in the middle n = ⌈
2 ⌉ is moved to itself by
two different actions (cid:101), 1 ∈ (cid:71). Furthermore, if N is even,
(cid:71)-action has N
2 orbits and (cid:71)2 acts on these orbits regu-
N
larly. If N is odd, (cid:71)-action has ⌈
2 ⌉ orbits. However, its
N
action on the orbit of the middle element (cid:71) ⌈
2 ⌉ is not
regular.

In the following, Section 3.1 proposes a procedure for
parameter-sharing in a fully connected layer. Although
simple,
this design is dense and does not guarantee
“unique” of equivariance. Section 3.2 proposes an alter-
native design with sparse connections that in some set-
tings ensures unique equivariance. Section 3.3 investigates
the effect of having multiple input and output channels in
the neural layer and Section 3.4 studies a special case of
(cid:71)(cid:78) = (cid:71)(cid:77), where input and output indices have a one-to-
one mapping.

3.1. Dense Design

Consider a complete bipartite graph with (cid:78) and (cid:77) as its two
parts and edges (n, m) ∈ (cid:78) × (cid:77). The action of (cid:71)(cid:78),(cid:77) par-
titions the edges into orbits {(cid:71)(cid:78),(cid:77)(np, mq)}np,mq , where
(np, mq) is a representative edge from an orbit. Painting
each orbit with a different color gives

Ω = ((cid:78), (cid:77), {∆p,q = (cid:71)(cid:78),(cid:77)(np, mq)}).

(9)

Therefore two edges (n, m) and (n′, m′
color iff an action in (cid:71)(cid:78),(cid:77) moves one edge to the other.

) have the same

2n ∼ n′ ⇔ ∃(cid:103) s.t., n = (cid:103)n′ ⇔ n ∈ (cid:71)n′ ⇔ n′ ∈ (cid:71)n.

Equivariance Through Parameter-Sharing

Proposition 3.1 (cid:71)(cid:78),(cid:77) ≤ Ω for Ω of (9).

Corollary 3.2 φ(⋅; w, Ω), for structure (9), is equivariant
to (cid:71)(cid:78),(cid:77).

layer

Example 3.2 (Nested Subsets and Wreath Product)
The permutation-equivariant
that we saw in
Example 2.2 is useful for deﬁning neural layers for set
structure. If our data-structure is in the form of nested
subsets, then we require equivariance to permutation
of variables within each set as well as permutation of
subsets. Here, we show how to use our dense design for
this purpose.

indexing for the input

We use a special
to better
identify the exchangeability of variables. We as-
sume D subsets, each of which has d variables x =
[x1,1, . . . , x1,d, x2,1, . . . , xD,d].
The group of our interest is the wreath product (cid:83)d ≀
(cid:83)D. This type of group product can be used to build
hierarchical and nested structures with different type of
symmetries at each level. Nesting subsets corresponds to
the most basic form of such hierarchical constructions.
We use (n, n′
) for
output variables.

) to index input variables and (m, m′

The following ﬁgure shows the resulting parameter-
sharing for an example with D = 2, d = 3.

How did we arrive at this structure Ω? Recall Our
objective is to deﬁne parameter-sharing so that φW ∶
(cid:82)dD
→ (cid:82)dD is equivariant to the action of (cid:71) =
(cid:83)d ≀ (cid:83)D – i.e., permutations within sets at two lev-
els. This group-action identiﬁes three partitions of
))∀n, n′
edges (seen in the ﬁgure):
connects each variable to its counterpart (dashed or-
ange); II) ((n, n′
≠ m′ connects each
variable to other variables within the same subset; III)
((n, n′
))∀n ≠ m is the set of edges from one
subset to another. According to the Corollary 3.2 this
parameter-sharing guarantees equivariance.

I) ((n, n′

), (m, m′

), (n, m′

))∀n, n′

), (n, n′

This fully-connected design is useful when the group (cid:71)(cid:78),(cid:77)
is large; for example when dealing with (cid:83)(cid:78). However, for

smaller groups it could be very inefﬁcient in practice, as
sometimes we can achieve equivariance through a sparse
structure Ω. As an example, consider the 2D circular con-
volution layer. It is easy to show that according to this de-
sign, the convolution ﬁlter will be the same size as the input
image. While this achieves the desirable equivariance, it is
inefﬁcient and does not generalize as well as a convolution
layer with small ﬁlters. Moreover, the dense design does
not guarantee “unique” equivariance. We next show under
some conditions on (cid:71)(cid:78),(cid:77) the sparse design can produce this
stronger guarantee.

3.2. Sparse Design

Our sparse construction uses orbits and symmetric generat-
ing sets:

• Let us denote the orbits of (cid:71)-action on (cid:77) and (cid:78) by
{(cid:71)np ∣ 1 ≤ p ≤ P } and {(cid:71)mq ∣ 1 ≤ q ≤ Q} respec-
tively, where P and Q are the total number of orbits
and np,mq are (arbitrary) representative members of
orbits (cid:71)np, (cid:71)mq respectively. Note that in contrast to
previous section, here we are considering the orbit of
variables rather than the edges.

• The set (cid:65) ⊆ (cid:71) is called the generating set of (cid:71) (<
(cid:65) >= (cid:71)), iff every member of (cid:71) can be expressed as
a combination of members of (cid:65). If the generating set
is closed under inverse (cid:97) ∈ (cid:65) ⇒ (cid:97)−1
∈ (cid:65) we call it a
symmetric generating set.

Deﬁne the structure Ω as

Ω = ((cid:78), (cid:77), {∆p,q,(cid:97)}1≤p≤P, 1≤q≤Q,(cid:97)∈(cid:65))
∆p,q,(cid:97) = {((cid:103)(cid:78)(cid:97)np, (cid:103)(cid:78)mq) ∣ ((cid:103)(cid:78), (cid:103)(cid:77)) ∈ (cid:71)(cid:78),(cid:77)}.

(10)

In words, we have one color per each combination of or-
bits (p, q) and members of the generating set (cid:97) ∈ (cid:65). The
following theorem relates the symmetry group of this struc-
ture to (cid:71).

Theorem 3.3 (cid:71)(cid:78),(cid:77) ≤ (cid:65)(cid:117)(cid:116)(Ω) for Ω of (10). More-
over if (cid:71)(cid:78) and (cid:71)(cid:77) are both semi-regular, then (cid:71)(cid:78),(cid:77) =
(cid:65)(cid:117)(cid:116)(Ω).

Note that this result holds for any choice of a symmetric
generating set (cid:65) in deﬁning Ω. Therefore, in designing
sparse layers, one seeks a minimal (cid:65).

Corollary 3.4 The function φ(⋅, w, Ω), using the structure
(10) is (cid:71)(cid:78),(cid:77)-equivariant. If (cid:71)(cid:78) and (cid:71)(cid:77) are semi-regular,
this function is “uniquely” (cid:71)(cid:78),(cid:77)-equivariant.

Equivariance Through Parameter-Sharing

The important implication is that, orbits and multiple
channels are treated identically by both dense and
sparse designs.

Example 3.5 (Group Convolution) The idea of group-
convolution is studied by Cohen & Welling (2016a); see
also (Olah, 2014). The following claim relates the func-
tion of this type of layer to our sparse design.

Claim 3.5 Under the following conditions the neural
layer (5) using our sparse design (10) performs group
convolution: I) there is a bijection between the output
and (cid:71) (i.e., (cid:77) = (cid:71)) and; II) (cid:71)(cid:78) is transitive.

This also identiﬁes the limitations of group-convolution
even in the setting where (cid:77) = (cid:71): When (cid:71)(cid:78) is semi-
regular and not transitive (P > 1), group convolution
is not guaranteed to be uniquely equivariant while the
sparse parameter-sharing of (10) provides this guaran-
tee.

For demonstration consider the following example in
equivariance to mirror symmetry.

This ﬁgure shows the bipartite structure for (cid:71) = (cid:90)2 =
{0, 1} and (cid:65) = {1}. (cid:71)-action is horizontal ﬂip of the
input and the output. On the right, (cid:77) = (cid:71) while on the
left (cid:71)(cid:77)-action has two orbits. Orbits are identiﬁed by
line-style and color of the circles. In a neural layer with
this parameter-sharing, when we ﬂip the input variables
(around the mirror line) the output is also ﬂipped.

The representatives in each orbit on (cid:78) and (cid:77) is iden-
tiﬁed with a star. Note that each combination of orbits
p and q has a parameter of its own, identiﬁed with dif-
ferent edge-styles. While this construction guarantees
“unique” (cid:71)-equivariance, if instead we use the same
parameters across orbits (as suggested by the original
group convolution) we get the parameter-sharing of the
ﬁgure below middle.

Now, assuming (cid:71)-action is semi-regular on both (cid:78) and (cid:77),
using (arbitrarily chosen) representatives {np}1≤p≤P and
{mq}1≤q≤Q for orbits in (cid:78) and (cid:77), we can rewrite the ex-
pression (5) of the structured neural layer for the structure
above. Here, components of φ = [φ1, . . . , φM ] are enu-
merated for 1 ≤ q ≤ Q, (cid:103)(cid:77) ∈ (cid:71)(cid:77):

φ(cid:103)(cid:77)mq (x; w) = σ( ∑
1≤p≤P

∑
(cid:97)∈(cid:65)

wq,p,(cid:97)x(cid:103)(cid:78)(cid:97)np )

(11)

where w ∈ (cid:82)P ×Q×∣(cid:65)∣ is the set of unique parameters,
and each element φ(cid:103)(cid:77)mq depends on subset of parame-
ters {wq,p,(cid:97)}p,(cid:97) identiﬁed by q and a subset of inputs
{x(cid:97),(cid:103)(cid:78)np }p,(cid:97) identiﬁed by (cid:103)(cid:78).

Example 3.3 (Dihedral Group of Fig. 1) In the exam-
ple of Fig. 1, the number of orbits of (cid:71)-action on (cid:78) is
P = 2 and for (cid:77) this is Q = 1. The symmetric gener-
ating set is the generating set that is used in the Cayley
diagram, with the addition of inverse shift (inverse of the
blue arrow). We then used (10) to build the structure of
Fig. 1 (right).

Example 3.4 (Reverse Convolution) The parameter-
sharing structure of reverse convolution in Examples 1.2
and 2.1 is produced using our sparse design. In these
examples, both (cid:71)(cid:78) and (cid:71)(cid:77) are regular. Therefore
unique
the
equivariance.

parameter-sharing

proposed

provides

3.3. Multiple Channels

In this section, we extend our results to multiple input and
output channels. Up to this point, we considered a neural
→ (cid:82)M . Here, we want to see how
network layer φ ∶ (cid:82)N
→ (cid:82)M ×K′
to achieve (cid:71)(cid:78),(cid:77)-equivariance for φ ∶ (cid:82)N ×K
,
where K and K ′ are the number of input and output chan-
nels.

First, we extend the action of (cid:71) on (cid:78) and (cid:77) to (cid:78)K
[(cid:78), . . . , (cid:78)
·„„„„„„„„„„„„„‚„„„„„„„„„„„„„¶
Ktimes

=
, to accommodate multiple chan-

] as well as (cid:77)K′

nels. For this, simply repeat the (cid:71)-action on each compo-
nent. (cid:71)-action on multiple input channels is equivalent to
sub-direct product (cid:71)(cid:78) ⊙ . . . ⊙ (cid:71)(cid:78)
≅ (cid:71)(cid:78). The same applies
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
Ktimes

to (cid:71)(cid:77).

This repetition, multiplies the orbits of (cid:71)(cid:78), one for each
channel, so that instead of having P and Q orbits on the
input (cid:78) and output (cid:77) sets, we have K × P and K ′
× Q
orbits on the input (cid:78)K and output (cid:77)K′
. This increases the
number of parameters by a factor of K × K ′.

Equivariance Through Parameter-Sharing

In this case, the resulting neural layer has the desired
equivariance (right). However, it is equivariant to the
action of a larger group (cid:71)(cid:78),(cid:77) ≅ (cid:90)2 × (cid:90)2 > (cid:90)2, in which
1 in the second (cid:90)2 group exchanges variables across the
orbits on (cid:78) (left in ﬁgure above).

3.4. (cid:71)(cid:78) = (cid:71)(cid:77)

In semi-supervised and un-supervised applications, we of-
ten need to produce a single output yn for each input
xn∀n ∈ (cid:78) – that is (cid:78) = (cid:77). We can ensure this by hav-
ing a relation ∆c∗ = {(n, n) ∣ n ∈ (cid:78)} in Ω that guaran-
tees any (π(cid:78), π(cid:77)) ∈ (cid:65)(cid:117)(cid:116)(Ω) applies the same permutation
to (cid:78) and (cid:77) = (cid:78) – i.e., π(cid:78) = π(cid:77). The resulting structure
Ω = ((cid:78), (cid:78), {∆c}1≤c≤C ∪ {∆c∗ }} can be also interpreted as
a colored multi-edged directed graph (digraph). This is be-
cause we can collapse the two parts by identifying n ∈ (cid:78)
with n ∈ (cid:77).
Therefore, the symmetry-group of the original bipartite
structure,
is isomorphic to symmetry group of a col-
ored multi-edged digraph on (cid:78). Achieving unique (cid:71)-
equivariance then reduces to answering the following ques-
tion: when could we express a permutation group (cid:71) ≤ (cid:83)(cid:78)
as the symmetry group (cid:65)(cid:117)(cid:116)(Ω) of a colored multi-edged
digraph with N nodes?

This problem is well-studied under the class of concrete
representation problems (Babai, 1994).
Permutation
groups (cid:71) that can be expressed in this way are called 2-
closed groups (Wielandt, 1969). The recipe for achieving
(cid:71)(cid:78) ≤ (cid:65)(cid:117)(cid:116)(Ω) is similar to our dense construction of Sec-
tion 3.13 The 2-closure ¯(cid:71)(cid:78) of a group (cid:71)(cid:78) is then, the great-
est permutation group ¯(cid:71)(cid:78) ≤ (cid:83)(cid:78) with the same orbit on (cid:78)×(cid:78)
as (cid:71)(cid:78). It is known that for example semi-regular permuta-
tion groups are 2-closed ¯(cid:71)(cid:78) = (cid:71)(cid:78). This result also follows
a corollary of our Theorem 3.3 for sparse design of (10).

Multiples of ±90○ rotation is produced as the action of
cyclic group (cid:90)4 on eight input output variables – that is
(cid:78) = (cid:77) = {1, . . . , 8}. (cid:90)4-action is semi-regular with two
orbits; these orbits the two inner and outer set of four
nodes. The representatives of each orbit in our sparse de-
sign is indicated using ﬁlled circles. The generating set
consists of (cid:65) = {1, 3}, rotation by 90○ and its inverse,
rotation by 270○. Each edge in each of these ﬁgures, has
a corresponding edge in the opposite direction, within a
different relation. To avoid over-crowding the ﬁgure, we
have dropped this edge from the drawing above, unless
both edges belong to the same relation.

Example 3.7 (Graph Convolution) Consider the set-
ting where we use the (normalized) adjacency matrix
N ×N (or Laplacian) of a graph Λ, to identify
B ∈ {0, 1}
parameter-sharing in a neural network layer. For a sin-
gle input/output channel, this is often in the form of Ax,
where x ∈ (cid:82)N and A = w1B + w2I has different param-
eters for diagonal and off-diagonal values (e.g., Kipf &
Welling, 2016; Bruna et al., 2013; Henaff et al., 2015);
for multiple channels see Section 3.3. The following
corollary of Theorem 2.1 identiﬁes the equivariance of
Ax.

Corollary 3.6 Given the digraph Λ and its binary ad-
N ×N , then (w1B + w2I)x is
jacency matrix B ∈ {0, 1}
uniquely equivariant to the symmetry-group of Λ.

Since two graphs on N nodes can have identical sym-
metries, one implication of this corollary is that graph-
convolution has identical equivariances for graphs with
the same symmetry groups.

Example 3.6 (Equivariance to ×90○ Rotations)
Figure below compares the digraph representation of Ω
produced using (left) our sparse design, and (right) our
dense design.

4. Conclusion

This work is a step towards designing neural network layers
with a given equivariance and invariance properties. Our
approach was to relate the equivariance properties of the
neural layer to the symmetries of the parameter-matrix.

We then proposed two parameter-sharing scheme that
achieves equivariance wrt any discrete group-action. More-
over under some conditions, we guarantee sensitivity wrt
other group actions. This is important because even a triv-
ial constant function is invariant to all transformations. It
is therefore essential to be able to draw the line between
equivariance/invariance and sensitivity in a function. To
our knowledge, our work presents the ﬁrst results of its
kind on guarantees regarding both variance and equivari-
ance with respect to group actions.

3In a fully connected digraph, the edges that belong to the

same orbit by (cid:71)-action on (cid:78) × (cid:78), receive the same color.

Equivariance Through Parameter-Sharing

Acknowledgment

research is

This
DESC0011114 and NSF grant IIS1563887.

supported in part by DOE grant

References

Agrawal, Pulkit, Carreira, Joao, and Malik, Jitendra.
Learning to see by moving. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 37–
45, 2015.

Anselmi, Fabio, Leibo, Joel Z, Rosasco, Lorenzo, Mutch,
Jim, Tacchetti, Andrea, and Poggio, Tomaso. Unsuper-
vised learning of invariant representations in hierarchical
architectures. arXiv preprint arXiv:1311.4158, 2013.

Babai, Laszlo. Automorphism groups, isomorphism, re-
construction (chapter 27 of the handbook of combina-
torics). University of Chicago, 1994.

Bart´ok, G´abor, Szepesv´ari, Csaba, and Zilles, Sandra.
Models of active learning in group-structured state
spaces. Information and Computation, 208(4):364–384,
2010.

Bruna, Joan and Mallat, St´ephane. Invariant scattering con-
volution networks. IEEE transactions on pattern analy-
sis and machine intelligence, 35(8):1872–1886, 2013.

Bruna, Joan, Zaremba, Wojciech, Szlam, Arthur, and Le-
Cun, Yann. Spectral networks and locally connected net-
works on graphs. arXiv preprint arXiv:1312.6203, 2013.

Bui, Hung Hai, Huynh, Tuyen N, and Riedel, Sebastian.
Automorphism groups of graphical models and lifted
variational inference. arXiv preprint arXiv:1207.4814,
2012.

Cohen, Taco S and Welling, Max. Group equivariant con-
volutional networks. arXiv preprint arXiv:1602.07576,
2016a.

Cohen, Taco S and Welling, Max. Steerable cnns. arXiv

preprint arXiv:1612.08498, 2016b.

Dieleman, Sander, Willett, Kyle W, and Dambre, Joni.
Rotation-invariant convolutional neural networks for
galaxy morphology prediction. Monthly notices of the
royal astronomical society, 450(2):1441–1459, 2015.

Dieleman, Sander, De Fauw, Jeffrey, and Kavukcuoglu,
Koray. Exploiting cyclic symmetry in convolutional neu-
ral networks. arXiv preprint arXiv:1602.02660, 2016.

Freeman, William T, Adelson, Edward H, et al. The design
and use of steerable ﬁlters. IEEE Transactions on Pat-
tern analysis and machine intelligence, 13(9):891–906,
1991.

Gens, Robert and Domingos, Pedro M. Deep symmetry
networks. In Advances in neural information processing
systems, pp. 2537–2545, 2014.

Hel-Or, Yacov and Teo, Patrick C. A common framework
for steerability, motion estimation, and invariant feature
detection. In Circuits and Systems, 1998. ISCAS’98. Pro-
ceedings of the 1998 IEEE International Symposium on,
volume 5, pp. 337–340. IEEE, 1998.

Henaff, Mikael, Bruna, Joan, and LeCun, Yann. Deep
convolutional networks on graph-structured data. arXiv
preprint arXiv:1506.05163, 2015.

Hinton, Geoffrey E, Krizhevsky, Alex, and Wang, Sida D.
In International Confer-
Transforming auto-encoders.
ence on Artiﬁcial Neural Networks, pp. 44–51. Springer,
2011.

Jaderberg, Max, Simonyan, Karen, Zisserman, Andrew,
In Advances in
et al. Spatial transformer networks.
Neural Information Processing Systems, pp. 2017–2025,
2015.

Jayaraman, Dinesh and Grauman, Kristen. Learning im-
age representations equivariant to ego-motion. In Proc.
ICCV, 2015.

Kersting, Kristian, Ahmadi, Babak, and Natarajan, Sri-
raam. Counting belief propagation. In Proceedings of
the Twenty-Fifth Conference on Uncertainty in Artiﬁcial
Intelligence, pp. 277–284. AUAI Press, 2009.

Kipf, Thomas N and Welling, Max. Semi-supervised clas-
arXiv

siﬁcation with graph convolutional networks.
preprint arXiv:1609.02907, 2016.

Kondor, Risi. Group theoretical methods in machine learn-

ing. Columbia University, 2008.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in neural information processing
systems, pp. 1097–1105, 2012.

Lenc, Karel and Vedaldi, Andrea. Understanding im-
age representations by measuring their equivariance and
equivalence. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 991–999,
2015.

Niepert, Mathias. Markov chains on orbits of permutation

groups. arXiv preprint arXiv:1206.5396, 2012.

Olah, Christopher.

and group convolu-
http://colah.github.io/posts/

Groups

tions.
2014-12-Groups-Convolution/, 2014.

Equivariance Through Parameter-Sharing

Oyallon, Edouard and Mallat, St´ephane. Deep roto-
In Pro-
translation scattering for object classiﬁcation.
ceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 2865–2873, 2015.

Raedt, Luc De, Kersting, Kristian, Natarajan, Sriraam, and
Poole, David. Statistical relational artiﬁcial intelligence:
Logic, probability, and computation. Synthesis Lectures
on Artiﬁcial Intelligence and Machine Learning, 10(2):
1–189, 2016.

Ravanbakhsh, Siamak, Schneider, Jeff, and Poczos, Barn-
abas. Deep learning with sets and point clouds. arXiv
preprint arXiv:1611.04500, 2016.

Sifre, Laurent and Mallat, St´ephane. Rotation, scaling and
deformation invariant scattering for texture discrimina-
tion. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 1233–1240,
2013.

Wielandt, H. Permutation groups through invariant rela-
tions and invariant functions. Dept. of Mathematics,
Ohio State University, 1969.

Zaheer, Manzil, Kottur, Satwik, Ravanbakhsh, Siamak,
Poczos, Barnabas, Salakhutdinov, Ruslan, and Smola,
Alexander J. Deep sets. CoRR, abs/1703.06114, 2017.
URL http://arxiv.org/abs/1703.06114.

