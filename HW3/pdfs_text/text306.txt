An Adaptive Test of Independence with Analytic Kernel Embeddings

Wittawat Jitkrittum 1 Zoltán Szabó 2 Arthur Gretton 1

Abstract

A new computationally efﬁcient dependence mea-
sure, and an adaptive statistical test of independence,
are proposed. The dependence measure is the differ-
ence between analytic embeddings of the joint distri-
bution and the product of the marginals, evaluated at
a ﬁnite set of locations (features). These features are
chosen so as to maximize a lower bound on the test
power, resulting in a test that is data-efﬁcient, and
that runs in linear time (with respect to the sample
size n). The optimized features can be interpreted
as evidence to reject the null hypothesis, indicating
regions in the joint domain where the joint distri-
bution and the product of the marginals differ most.
Consistency of the independence test is established,
for an appropriate choice of features. In real-world
benchmarks, independence tests using the optimized
features perform comparably to the state-of-the-art
quadratic-time HSIC test, and outperform competing

(n) and

(n log n) tests.

O

O

1. Introduction

We consider the design of adaptive, nonparametric statistical
tests of dependence: that is, tests of whether a joint distri-
bution Pxy factorizes into the product of marginals PxPy
with the null hypothesis that H0 : X and Y are indepen-
dent. While classical tests of dependence, such as Pearson’s
correlation and Kendall’s τ , are able to detect monotonic
relations between univariate variables, more modern tests
can address complex interactions, for instance changes in
variance of X with the value of Y . Key to many recent
tests is to examine covariance or correlation between data
features. These interactions become signiﬁcantly harder to
detect, and the features are more difﬁcult to design, when
the data reside in high dimensions.

Zoltán Szabó’s ORCID ID: 0000-0001-6183-7603. Arthur Gret-
ton’s ORCID ID: 0000-0003-3169-7624. 1Gatsby Unit, Univer-
sity College London, UK. 2CMAP, École Polytechnique, France.
Correspondence to: Wittawat Jitkrittum <wittawatj@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

H

A basic nonlinear dependence measure is the Hilbert-
Schmidt Independence Criterion (HSIC), which is the
Hilbert-Schmidt norm of the covariance operator between
feature mappings of the random variables (Gretton et al.,
2005; 2008). Each random variable X and Y is mapped
k and
to a respective reproducing kernel Hilbert space
l. For sufﬁciently rich mappings, the covariance operator
H
norm is zero if and only if the variables are independent. A
second basic nonlinear dependence measure is the smoothed
difference between the characteristic function of the joint
distribution, and that of the product of marginals. When
a particular smoothing function is used, the statistic corre-
sponds to the covariance between distances of X and Y vari-
able pairs (Feuerverger, 1993; Székely et al., 2007; Székely
& Rizzo, 2009), yielding a simple test statistic based on
pairwise distances. It has been shown by Sejdinovic et al.
(2013) that the distance covariance (and its generalization
to semi-metrics) is an instance of HSIC for an appropriate
choice of kernels. A disadvantage of these feature covari-
ance statistics, however, is that they require quadratic time
to compute (besides in the special case of the distance co-
variance with univariate real-valued variables, where Huo &
(n log n) cost). Moreover, the
Székely (2016) achieve an
feature covariance statistics have intractable null distribu-
tions, and either a permutation approach or the solution of
an expensive eigenvalue problem (e.g. Zhang et al., 2011) is
required for consistent estimation of the quantiles. Several
approaches were proposed by Zhang et al. (2017) to obtain
faster tests along the lines of HSIC. These include comput-
ing HSIC on ﬁnite-dimensional feature mappings chosen as
random Fourier features (RFFs) (Rahimi & Recht, 2008),
a block-averaged statistic, and a Nyström approximation
to the statistic. Key to each of these approaches is a more
efﬁcient computation of the statistic and its threshold under
the null distribution: for RFFs, the null distribution is a
ﬁnite weighted sum of χ2 variables; for the block-averaged
statistic, the null distribution is asymptotically normal; for
Nyström, either a permutation approach is employed, or
the spectrum of the Nyström approximation to the kernel
matrix is used in approximating the null distribution. Each
(n2) cost
of these methods costs signiﬁcantly less than the
of the full HSIC (the cost is linear in n, but also depends
quadratically on the number of features retained). A poten-
tial disadvantage of the Nyström and Fourier approaches is
that the features are not optimized to maximize test power,

O

O

An Adaptive Test of Independence with Analytic Kernel Embeddings

but are chosen randomly. The block statistic performs worse
than both, due to the large variance of the statistic under the
null (which can be mitigated by observing more data).

O

In addition to feature covariances, correlation measures have
also been developed in inﬁnite dimensional feature spaces:
in particular, Bach & Jordan (2002); Fukumizu et al. (2008)
proposed statistics on the correlation operator in a repro-
ducing kernel Hilbert space. While convergence has been
established for certain of these statistics, their computa-
(n3), and test thresholds have relied
tional cost is high at
on permutation. A number of much faster approaches to
testing based on feature correlations have been proposed,
however. For instance, Dauxois & Nkiet (1998) compute
statistics of the correlation between ﬁnite sets of basis func-
tions, chosen for instance to be step functions or low order
(n). This idea
B-splines. The cost of this approach is
was extended by Lopez-Paz et al. (2013), who computed
the canonical correlation between ﬁnite sets of basis func-
tions chosen as random Fourier features; in addition, they
performed a copula transform on the inputs, with a total
(n log n). Finally, space partitioning approaches
cost of
have also been proposed, based on statistics such as the
KL divergence, however these apply only to univariate vari-
ables (Heller et al., 2016), or to multivariate variables of
low dimension (Gretton & Györﬁ, 2010) (that said, these
tests have other advantages of theoretical interest, notably
distribution-independent test thresholds).

O

O

The approach we take is most closely related to HSIC on a
ﬁnite set of features. Our simplest test statistic, the Finite
Set Independence Criterion (FSIC), is an average of covari-
ances of analytic functions (i.e., features) deﬁned on each
of X and Y . A normalized version of the statistic (NFSIC)
yields a distribution-independent asymptotic test threshold.
We show that our test is consistent, despite a ﬁnite number
of analytic features being used, via a generalization of ar-
guments in Chwialkowski et al. (2015). As in recent work
on two-sample testing by Jitkrittum et al. (2016), our test
is adaptive in the sense that we choose our features on a
held-out validation set to optimize a lower bound on the
test power. The design of features for independence testing
turns out to be quite different to the case of two-sample
testing, however: the task is to ﬁnd correlated feature pairs
on the respective marginal domains, rather than attempting
to ﬁnd a single, high-dimensional feature representation on
the tensor product of the marginals, as we would need to
do if we were comparing distributions Pxy and Qxy. While
the use of coupled feature pairs on the marginals entails a
smaller feature space dimension, it introduces signiﬁcant
complications in the proof of the lower bound, compared
with the two-sample case. We demonstrate the performance
of our tests on several challenging artiﬁcial and real-world
datasets, including detection of dependence between music
and its year of appearance, and between videos and captions.

In these experiments, we outperform competing linear and

(n log n) time tests.

O

2. Independence Criteria and Statistical Tests

We introduce two test statistics: ﬁrst, the Finite Set Inde-
pendence Criterion (FSIC), which builds on the principle
that dependence can be measured in terms of the covari-
ance between data features. Next, we propose a normalized
version of this statistic (NFSIC), with a simpler asymptotic
distribution when Pxy = PxPy. We show how to select
features for the latter statistic to maximize a lower bound
on the power of its corresponding statistical test.

2.1. The Finite Set Independence Criterion

∈ X ⊆

Rdx and Y

We begin by recalling the Hilbert-Schmidt Independence
Criterion (HSIC) as proposed in Gretton et al. (2005), since
our unnormalized statistic is built along similar lines. Con-
sider two random variables X
∈ Y ⊆
Rdy . Denote by Pxy the joint distribution between X and Y ;
Px and Py are the marginal distributions of X and Y . Let
⊗
b, c
denote the tensor product, such that (a
. As-
(cid:105)
(cid:104)
R are positive
R and l :
sume that k :
deﬁnite kernels associated with reproducing kernel Hilbert
spaces (RKHS)
HS be the
H
k Hilbert-Schmidt operators.
norm on the space of
l
H
Then, HSIC between X and Y is deﬁned as

⊗
Y × Y →

l, respectively. Let

X × X →

b) c = a

k and

→ H

(cid:107) · (cid:107)

H

HSIC(X, Y ) = (cid:13)
(cid:13)
2
µy
(cid:13)
HS
= E(x,y),(x(cid:48),y(cid:48)) [k(x, x(cid:48))l(y, y(cid:48))]

(cid:13)µxy

µx

−

⊗

+ ExE

x(cid:48) [k(x, x(cid:48))]EyEy(cid:48)[l(y, y(cid:48))]
2E(x,y) [Ex(cid:48)[k(x, x(cid:48))]Ey(cid:48)[l(y, y(cid:48))]] ,

(1)

−

⊗

l to

l(y,

X ×Y k(x,

) dPxy(x, y)

where Ex := Ex∼Px, Ey := Ey∼Py , Exy := E(x,y)∼Pxy ,
and x(cid:48) is an independent copy of x. The mean embedding of
Pxy belongs to the space of Hilbert-Schmidt operators from

·
Y l(y,

k, µxy := (cid:82)
)
∈
·
H
H
k), and the marginal mean embeddings are µx :=
l,
HS(
H
H
(cid:82)
k and µy := (cid:82)
X k(x,
) dPx(x)
·
∈
l (Smola et al., 2007). Gretton et al. (2005, Theorem 4)
H
show that if the kernels k and l are universal (Steinwart
& Christmann, 2008) on compact domains
, then
HSIC(X, Y ) = 0 if and only if X and Y are independent.
n
Pxy, an em-
Given a joint sample Zn =
i=1 ∼
(n2) time
pirical estimator of HSIC can be computed in
by replacing the population expectations in (1) with their
corresponding empirical expectations based on Zn.

) dPy(y)
·

(xi, yi)
}

∈ H

and

O

X

Y

{

Rdx and

We now propose our new linear-time dependence mea-
sure, the Finite Set Independence Criterion (FSIC). Let
Let
X ⊆
µxµy(x, y) := µx(x)µy(y) The idea is to see µxy(v, w) =
Exy[k(x, v)l(y, w)], µx(v) = Ex[k(x, v)] and µy(w) =
Ey[l(y, w)] as smooth functions, and consider a new dis-

Rdy be open sets.

Y ⊆

An Adaptive Test of Independence with Analytic Kernel Embeddings

tance between µxy and µxµy instead of a Hilbert-Schmidt
distance as in HSIC (Gretton et al., 2005). The new mea-
sure is given by the average of squared differences be-
tween µxy and µxµy, evaluated at J random test locations
VJ :=

.

(vi, wi)
}
{

FSIC2(X, Y ) :=

[µxy(vi, wi)

µx(vi)µy(wi)]2

J
i=1 ⊂ X × Y
J
(cid:88)

1
J

i=1

J
(cid:88)

i=1

=

1
J

u2(vi, wi) =

−

1
J (cid:107)

2
2,

u
(cid:107)

where

u(v, w) := µxy(v, w)
−
= Exy[k(x, v)l(y, w)]
−
= covxy[k(x, v), l(y, w)],

µx(v)µy(w)
Ex[k(x, v)]Ey[l(y, w)],

(2)

J
u := (u(v1, w1), . . . , u(vJ , wJ ))(cid:62), and
(vi, wi)
i=1
}
{
are realizations from an absolutely continuous distribution
(wrt the Lebesgue measure).

Our ﬁrst result in Proposition 2 states that FSIC(X, Y )
almost surely deﬁnes a dependence measure for the random
variables X and Y , provided that the product kernel on
is characteristic and analytic (see
the joint space
Deﬁnition 1).

X × Y

.

Deﬁnition 1 (Analytic kernels (Chwialkowski et al., 2015)).
be an open set in Rd. A positive deﬁnite kernel
Let
R is said to be analytic on its domain
k :
X × X
, f (x) := k(x, v) is an analytic function on
if for all v

X
X × X →
∈ X

X
R and
Assumption A. The kernels k :
R are bounded by Bk and Bl respectively
l :
Y × Y →
[supx,x(cid:48)∈X k(x, x(cid:48))
Bl] , and
the product kernel g((x, y), (x(cid:48), y(cid:48))) := k(x, x(cid:48))l(y, y(cid:48)) is
characteristic (Sriperumbudur et al., 2010, Deﬁnition 6),
and analytic (Deﬁnition 1) on (

Bk, supy,y(cid:48)∈Y l(y, y(cid:48))

X × X →

≤

≤

).

)

(

X × Y

×

X × Y

Proposition 2 (FSIC is a dependence measure). Assume
that assumption A holds, and that
locations
J
i=1 are drawn from an absolutely con-
VJ =
(vi, wi)
}
{
tinuous distribution η. Then, η-almost surely, it holds that
FSIC(X, Y ) = 1√
2 = 0 if and only if X and Y are
u
(cid:107)
independent.

the test

J (cid:107)

(cid:55)→

E(x,y)∼P [g((x, y),

Proof. Since g is characteristic, the mean embedding map
Πg : P
)] is injective (Sriperum-
·
budur et al., 2010, Section 3), where P is a probability
. Since g is analytic, by Lemma 10
distribution on
(Appendix), µxy and µxµy are analytic functions. Thus,
Lemma 11 (Appendix, setting Λ = Πg) guarantees that
FSIC(X, Y ) = 0
X and Y are
independent almost surely.

Pxy = PxPy

X × Y

⇐⇒

⇐⇒

If Pxy

FSIC uses µxy as a proxy for Pxy, and µxµy as a proxy for
PxPy. Proposition 2 states that, to detect the dependence
between X and Y , it is sufﬁcient to evaluate the difference
of the population joint embedding µxy and the embedding
of the product of the marginal distributions µxµy at a ﬁ-
nite number of locations (deﬁned by VJ ). The intuitive ex-
planation of this property is as follows. If Pxy = PxPy,
then u(v, w) = 0 everywhere, and FSIC(X, Y ) = 0
for any VJ .
= PxPy, then u will not be a zero
function, since the mean embedding map is injective (re-
quires the product kernel to be characteristic). Using the
same argument as in Chwialkowski et al. (2015), since k
and l are analytic, u is also analytic, and the set of roots
u(v, w) = 0
Ru :=
has Lebesgue measure zero.
}
Thus, it is sufﬁcient to draw (v, w) from an absolutely con-
tinuous distribution to have (v, w) /
Ru η-almost surely,
∈
and hence FSIC(X, Y ) > 0. We note that a characteristic
kernel which is not analytic may produce u such that Ru has
a positive Lebesgue measure. In this case, there is a positive
probability that (v, w)
Ru, resulting in a potential failure
∈
to detect the dependence.

(v, w)
{

|

(A product

characteristic

of Gaussian

The next proposition shows that Gaussian kernels k and l
yield a product kernel which is characteristic and analytic; in
other words, this is an example when Assumption A holds.
Proposition 3
kernels
=
is
exp (cid:0)
=
(x
−
exp (cid:0)
ker-
(y
−
nels on Rdx
respectively,
Then,
for positive deﬁnite matrices A and B.
g((x, y), (x(cid:48), y(cid:48))) = k(x, x(cid:48))l(y, y(cid:48)) is characteris-
Rdy )
tic and analytic on (Rdx

and analytic). Let k(x, x(cid:48))
x(cid:48))(cid:1)
and
y(cid:48))(cid:1)
be
Rdx and Rdy

l(y, y(cid:48))
Gaussian
Rdy

x(cid:48))(cid:62)A(x
y(cid:48))(cid:62)B(y

Rdy ).

(Rdx

−
−

−
−

×

×

×

×

×

Proof (sketch). The main idea is to use the fact that a Gaus-
sian kernel is analytic, and a product of Gaussian kernels is
a Gaussian kernel on the pair of variables. See the full proof
in Appendix D.

estimators

Plug-in Estimator Assume
a
observe
joint sample Zn :=
Pxy. Un-
and µxµy(v, w)
biased
are
and
(cid:91)µxµy(v, w)
j(cid:54)=i k(xi, v)l(yj, w),
respectively. A straightforward empirical estimator of
FSIC2 is then given by

that we
i.i.d.
n
(xi, yi)
i=1
∼
}
of µxy(v, w)
(cid:80)n
i=1 k(xi, v)l(yi, w)
(cid:80)n
i=1

ˆµxy(v, w)

1
n(n−1)

(cid:80)

:=

:=

1
n

{

(cid:92)FSIC2(Zn) =

ˆu(vi, wi)2,

1
J

J
(cid:88)

i=1

ˆu(v, w) := ˆµxy(v, w) − (cid:91)µxµy(v, w)

(3)

=

2
n(n − 1)

(cid:88)

i<j

h(v,w)((xi, yi), (xj, yj)),

(4)

where h(v,w)((x, y), (x(cid:48), y(cid:48)))
l(y(cid:48), w)).
k(x(cid:48), v))(l(y, w)

:=
−
For conciseness, we

1
2 (k(x, v)

−

(cid:54)
An Adaptive Test of Independence with Analytic Kernel Embeddings

∈
J ˆu(cid:62) ˆu.

RJ where ˆui := ˆu(vi, wi)

deﬁne ˆu := (ˆu1, . . . , ˆuJ )(cid:62)
so that (cid:92)FSIC2(Zn) = 1
(cid:92)FSIC2 can be efﬁciently computed in
((dx +dy)Jn) time
which is linear in n [see (3) which does not have nested
double sums], assuming that the runtime complexity of
(dy).
evaluating k(x, v) is

(dx) and that of l(y, w) is

O

O

O

⊥

X

⇐⇒

Since FSIC satisﬁes FSIC(X, Y ) = 0
Y , in
principle its empirical estimator can be used as a test statistic
for an independence test proposing a null hypothesis H0 :
“X and Y are independent” against an alternative H1 : “X
and Y are dependent.” The null distribution (i.e., distribu-
tion of the test statistic assuming that H0 is true) is challeng-
ing to obtain, however, and depends on the unknown Pxy.
This prompts us to consider a normalized version of FSIC
whose asymptotic null distribution takes a more convenient
form. We ﬁrst derive the asymptotic distribution of ˆu in
Proposition 4, which we use to derive the normalized test
statistic in Theorem 5. As a shorthand, we write z := (x, y),
t := (v, w), covz is covariance,Vz stands for variance.
Proposition 4 (Asymptotic distribution of ˆu). Deﬁne
u := (u(t1), . . . , u(tJ ))(cid:62), ˜k(x, v)
−
Ex(cid:48)k(x(cid:48), v), and ˜l(y, w)
Ey(cid:48)l(y(cid:48), w).
RJ×J be the positive semi-deﬁnite
Let Σ = [Σij]
∈
matrix with entries Σij = covz(ˆu(ti), ˆu(tj)) =
Exy[˜k(x, vi)˜l(y, wi)˜k(x, vj)˜l(y, wj)]
u(ti)u(tj). Then,
under both H0 and H1,
locations
for which Σ is full rank, and 0 <
t1, . . . , tJ
{
}
Vz[htj (z)] <
for j = 1, . . . , J, it holds that √n(ˆu
u) d

for any ﬁxed test

∞
(0, Σ).

:= k(x, v)

:= l(y, w)

−

−

−

→ N

}

t1, . . . , tJ
{

, ˆu is a one-sample second-
Proof. For a ﬁxed
order multivariate U-statistic with a U-statistic kernel ht.
Thus, by Lehmann (1999, Theorem 6.1.6) and Kowal-
ski & Tu (2008, Section 5.1, Theorem 1), it follows di-
rectly that √n(ˆu
(0, Σ) where we note that
Exy[˜k(x, v)˜l(y, w)] = u(v, w).

u) d

→ N

−

Recall from Proposition 2 that u = 0 holds almost surely un-
der H0. The asymptotic normality described in Proposition
4 implies that n(cid:92)FSIC2 = n
J ˆu(cid:62) ˆu converges in distribution
to a sum of J dependent weighted χ2 random variables.
The dependence comes from the fact that the coordinates
ˆu1 . . . , ˆuJ of ˆu all depend on the sample Zn. This null dis-
tribution is not analytically tractable, and requires a large
number of simulations to compute the rejection threshold
Tα for a given signiﬁcance value α.

2.2. Normalized FSIC and Adaptive Test

For the purpose of an independence test, we will consider
a normalized variant of (cid:92)FSIC2, which we call (cid:92)NFSIC2,
whose tractable asymptotic null distribution is χ2(J), the

chi-squared distribution with J degrees of freedom. We
then show that the independence test deﬁned by (cid:92)NFSIC2 is
consistent. These results are given in Theorem 5.
Theorem 5 (Independence test based on (cid:92)NFSIC2 is consis-
tent). Let ˆΣ be a consistent estimate of Σ based on the joint
sample Zn, where Σ is deﬁned in Proposition 4. Assume
that VJ =
η where η is absolutely contin-
uous wrt the Lebesgue measure. The (cid:92)NFSIC2 statistic is
deﬁned as ˆλn := nˆu(cid:62) (cid:16) ˆΣ + γnI
0 is a
regularization parameter. Assume that

(vi, wi)
}
{

ˆu where γn

J
i=1 ∼

(cid:17)−1

≥

1. Assumption A holds.

2. Σ is invertible η-almost surely.

3. limn→∞ γn = 0.

Then, for any k, l and VJ satisfying the assumptions,

1. Under H0, ˆλn

χ2(J) as n

d
→

.
→ ∞
R, limn→∞ P

(cid:16)ˆλn

(cid:17)

2. Under H1, for any r

= 1
η-almost surely. That is, the independence test based on
(cid:92)NFSIC2 is consistent.

≥

∈

r

Proof (sketch) . Under H0, nˆu(cid:62)( ˆΣ + γnI)−1 ˆu asymptot-
ically follows χ2(J) because √nˆu is asymptotically nor-
mally distributed (see Proposition 4). Claim 2 builds on
the result in Proposition 2 stating that u
= 0 under H1; it
follows using the convergence of ˆu to u. The full proof can
be found in Appendix E.

−

Theorem 5 states that if H1 holds, the statistic can be arbi-
trarily large as n increases, allowing H0 to be rejected for
any ﬁxed threshold. Asymptotically the test threshold Tα is
α)-quantile of χ2(J) and is independent
given by the (1
of n. The assumption on the consistency of ˆΣ is required
to obtain the asymptotic chi-squared distribution. The regu-
larization parameter γn is to ensure that ( ˆΣ + γnI)−1 can
be stably computed. In practice, γn requires no tuning, and
can be set to be a very small constant. We emphasize that J
need not increase with n for test consistency.

The next proposition states that the computational com-
plexity of the (cid:92)NFSIC2 estimator is linear in both the input
dimension and sample size, and that it can be expressed
RJ×n, L =
in terms of the K =[K ij] = [k(vi, xj)]
RJ×n matrices. In contrast to typical
[Lij] = [l(wi, yj)]
kernel methods, a large Gram matrix of size n
n is not
needed to compute (cid:92)NFSIC2.
Proposition 6 (An empirical estimator of (cid:92)NFSIC2). Let
Rn. Denote by
1n := (1, . . . , 1)(cid:62)
the element-wise
∈
matrix product. Then,

×

∈

∈

◦

(cid:54)
An Adaptive Test of Independence with Analytic Kernel Embeddings

1. ˆu = (K◦L)1n

n−1 −

(K1n)◦(L1n)
n(n−1)

.

2. A consistent estimator for Σ is ˆΣ = ΓΓ(cid:62)

n where

ξ3 := 8c1B2J, c3 := 4B2J ˜c2, ξ4 := 28B4J 2c2
1, c1 :=
4B2J√J ˜c, and c2 := 4B√J ˜c. Moreover, for sufﬁciently
large ﬁxed n, L(λn) is increasing in λn.

Γ := (K
−
ˆub = n−1 (K

n−1K1n1(cid:62)
n )
(L
n−2 (K1n)
L) 1n

n−1L1n1(cid:62)
n )
(L1n) .

−

◦

−

ˆub1(cid:62)
n ,

◦

−

◦

Assume that the complexity of the kernel evaluation is lin-
ear in the input dimension. Then the test statistic ˆλn =
nˆu(cid:62) (cid:16) ˆΣ + γnI
(J 3 + J 2n +
(dx + dy)Jn) time.

ˆu can be computed in

(cid:17)−1

O

Proof (sketch). Claim 1 for ˆu is straightforward. The ex-
pression for ˆΣ in claim 2 follows directly from the asymp-
totic covariance expression in Proposition 4. The consis-
tency of ˆΣ can be obtained by noting that the ﬁnite sample
ˆΣ
bound for P(
F > t) decreases as n increases. This
(cid:107)
(cid:107)
is implicitly shown in Appendix F.2.2 and its following
sections.

Σ

−

Although the dependency of the estimator on J is cubic, we
empirically observe that only a small value of J is required
(see Section 3). The number of test locations J relates to
of pxy and pxpy that differ
the number of regions in
(see Figure 1).

X × Y

Theorem 5 asserts the consistency of the test for any test
locations VJ drawn from an absolutely continuous distribu-
tion. In practice, VJ can be further optimized to increase
the test power for a ﬁxed sample size. Our ﬁnal theoretical
result gives a lower bound on the test power of (cid:92)NFSIC2 i.e.,
the probability of correctly rejecting H0. We will use this
lower bound as the objective function to determine VJ and
the kernel parameters. Let

F be the Frobenius norm.

(cid:107) · (cid:107)
Theorem 7 (A lower bound on the test power). Let
NFSIC2(X, Y ) := λn := nu(cid:62)Σ−1u. Let
be a kernel
K
class for k,
be a collection
be a kernel class for l, and
with each element being a set of J locations. Assume that

L

V

1. There

and Bl

exist
supk∈K supx,x(cid:48)∈X |
supl∈L supy,y(cid:48)∈Y |

ﬁnite Bk
k(x, x(cid:48))
|
l(y, y(cid:48))
Bl.
| ≤
Σ−1
2. ˜c := supk∈K supl∈L supVJ ∈V (cid:107)
Then, for any k
power satisﬁes P

F <
(cid:107)
, and λn
L(λn) where

, l
(cid:16)ˆλn
∈ K

, VJ

∈ V

≤

(cid:17)
∈ L
r
≥

≥

.
∞

≥

such
Bk

that
and

r, the test

L(λn) = 1

62e−ξ1γ2

n(λn−r)2/n

−

−

2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ2

−

2e−(cid:98)0.5n(cid:99)(λn−r)2/[ξ2n2]
/[ξ4n2(n−1)],

nn(n−1)]2

is the ﬂoor function, ξ1 :=

(cid:98)·(cid:99)
depending on only Bk and Bl, ξ2 := 72c2

1

1J 2B∗ , B∗ is a constant
2JB2, B := BkBl,

32c2

|

(cid:16)

(cid:17)

−

(cid:55)→

exp

exp

y,u]

x,u]

K
=:

L
=:

(x, v)

g
K
=

(y, w)

and
(cid:111)

the proof

for some 0 < σ2
(cid:110)

in Appendix F. To put
=

We provide
Theorem 7 into perspective,
assume that
(cid:16)
(cid:110)
(cid:111)
(cid:107)x−v(cid:107)2
σ2
[σ2
x,l, σ2
x ∈
2σ2
x
x,l < σ2
x,u <
∞
(cid:17)
(cid:107)y−w(cid:107)2
y,l, σ2
σ2
[σ2
g
y ∈
2σ2
−
(cid:55)→
L
|
y
for some 0 < σ2
y,l < σ2
y,u <
are Gaussian kernel
∞
classes. Then, in Theorem 7, B = Bk = Bl = 1,
and B∗ = 2. The assumption ˜c <
is a techni-
cal condition to guarantee that
the test power lower
bound is ﬁnite for all θ deﬁned by the feasible sets

| (cid:107)
(cid:107)
(cid:15), for all i

, and
vi
(cid:107)
=

(cid:15),r
V
wi
−
(cid:107)
g, and
V

. Let
V
2
2 +
vj
(cid:107)
=
L
g, and

:= (cid:8)VJ
2,
wi
,
≤
(cid:107)
(cid:107)
K
L
= j(cid:9). If we
2
wj
r and
2 ≥
(cid:107)
(cid:15),r for some (cid:15), r > 0, then
=
set
V
˜c <
(cid:15),r are compact. In practice, these
conditions do not necessarily create restrictions as they
almost always hold implicitly. We show in Appendix C that
the objective function used to choose VJ will discourage
any two locations to be in the same neighborhood.

K
∞

K
as

L
g,

−
g,

∞

vi

K

L

V

2

}

∈ L

g and l
y, VJ

Parameter Tuning Let θ be the collection of all tuning
parameters of the test. If k
g (i.e., Gaus-
∈ K
x, σ2
σ2
sian kernels), then θ =
. The test power
{
lower bound L(λn) in Theorem 7 is a function of λn =
nu(cid:62)Σ−1u which is the population counterpart of the test
statistic ˆλn. As in FSIC, it can be shown that λn = 0 if
and only if X are Y are independent (from Proposition 2).
According to Theorem 7, for a sufﬁciently large n, the test
power lower bound is increasing in λn. One can therefore
think of λn (a function of θ) as representing how easily the
test rejects H0 given a problem Pxy. The higher the λn, the
greater the lower bound on the test power, and thus the more
likely it is that the test will reject H0 when it is false.

(cid:55)→

In light of this reasoning, we propose to set θ by maxi-
mizing the lower bound on the test power i.e., set θ to
θ∗ = arg maxθ L(λn). Assume that n is sufﬁciently large
so that λn
L(λn) is an increasing function. Then,
arg maxθ L(λn) = arg maxθ λn. That this procedure is
also valid under H0 can be seen as follows. Under H0,
θ∗ = arg maxθ 0 will be arbitrary. Since Theorem 7 guaran-
tees that ˆλn
for any θ, the asymptotic
→ ∞
null distribution does not change by using θ∗. In practice,
λn is a population quantity which is unknown. We propose
dividing the sample Zn into two disjoint sets: training and
test sets. The training set is used to compute ˆλn (an estimate
of λn) to optimize for θ∗, and the test set is used for the ac-
tual independence test with the optimized θ∗. The splitting
is to guarantee the independence of θ∗ and the test sample
to avoid overﬁtting.

χ2(J) as n

d
→

(cid:54)
An Adaptive Test of Independence with Analytic Kernel Embeddings

(a) ˆµxy(v, w)

(b) (cid:91)µxµy(v, w)

(c) (cid:98)Σ(v, w)

(d) Statistic ˆλn(v, w)

−

∼ N

Figure 1: Illustration of (cid:92)NFSIC2.
To better understand the behaviour of (cid:92)NFSIC2, we visual-
ize ˆµxy(v, w), (cid:91)µxµy(v, w) and ˆΣ(v, w) as a function of
one test location (v, w) on a simple toy problem. In this
(0, 0.32) is an inde-
problem, Y =
pendent noise variable. As we consider only one location
(J = 1), ˆΣ(v, w) is a scalar. The statistic can be written
as ˆλn = n
. These components are
shown in Figure 1, where we use Gaussian kernels for both
X and Y , and the horizontal and vertical axes correspond
to v

(ˆµxy(v,w)−(cid:92)µxµy(v,w))2
ˆΣ(v,w)

R, respectively.

X + Z where Z

R and w

∈
(cid:91)µxµy(v, w) captures
Intuitively, ˆu(v, w) = ˆµxy(v, w)
the difference of the joint distribution and the product of
the marginals as a function of (v, w). Squaring ˆu(v, w)
and dividing it by the variance shown in Figure 1c gives the
statistic (also the parameter tuning objective) shown in Fig-
ure 1d. The latter ﬁgure illustrates that the parameter tuning
objective function can be non-convex: non-convexity arises
since there are multiple ways to detect the difference be-
tween the joint distribution and the product of the marginals.
In this case, the lower left and upper right regions equally
indicate the largest difference. A convex objective would
not be able to capture this phenomenon.

−

∈

3. Experiments

In this section, we empirically study the performance of
the proposed method on both toy (Section 3.1) and real
problems (Section 3.2). We are interested in challeng-
ing problems requiring a large number of samples, where
a quadratic-time test might be computationally infeasi-
ble. Our goal is not to outperform a quadratic-time test
with a linear-time test uniformly over all testing problems.
We will ﬁnd, however, that our test does outperform the
quadratic-time test in some cases. Code is available at
https://github.com/wittawatj/fsic-test.

We compare the proposed NFSIC with optimization (NFSIC-
opt) to ﬁve multivariate nonparametric tests. The (cid:92)NFSIC2
test without optimization (NFSIC-med) acts as a baseline,
allowing the effect of parameter optimization to be clearly

seen. For pedagogical reason, we consider the original HSIC
test of Gretton et al. (2005) denoted by QHSIC, which is a
quadratic-time test. Nyström HSIC (NyHSIC) uses a Nys-
tröm approximation to the kernel matrices of X and Y when
computing the HSIC statistic. FHSIC is another variant of
HSIC in which a random Fourier feature approximation
(Rahimi & Recht, 2008) to the kernel is used. NyHSIC and
FHSIC are studied in Zhang et al. (2017) and can be com-
(n), with quadratic dependency on the number
puted in
of inducing points in NyHSIC, and quadratic dependency
on the number of random features in FHSIC. Finally, the
Randomized Dependence Coefﬁcient (RDC) proposed in
Lopez-Paz et al. (2013) is also considered. The RDC can be
seen as the primal form (with random Fourier features) of
the kernel canonical correlation analysis of Bach & Jordan
(2002) on copula-transformed data. We consider RDC as a
linear-time test even though preprocessing by an empirical
copula transform costs

((dx + dy)n log n).

O

O

}

n

L

K

≤

−

xi

i < j

g and

We use Gaussian kernel classes
g for both
X and Y in all the methods. Except NFSIC-opt, all
other tests use full sample to conduct the independence
test, where the Gaussian widths σx and σy are set ac-
cording to the widely used median heuristic i.e., σx =
median (
), and σy is set in
xj
1
2
≤
|
(cid:107)
{(cid:107)
n
i=1. The J locations for NFSIC-
yi
the same way using
}
{
med are randomly drawn from the standard multivariate
normal distribution in each trial. For a sample of size n,
NFSIC-opt uses half the sample for parameter tuning, and
the other disjoint half for the test. We permute the sample
300 times in RDC1 and HSIC to simulate from the null
distribution and compute the test threshold. The null distri-
butions for FHSIC and NyHSIC are given by a ﬁnite sum of
weighted χ2(1) random variables given in Eq. 8 of Zhang
et al. (2017). Unless stated otherwise, we set the test thresh-
α)-quantile of
old of the two NFSIC tests to be the (1
χ2(J). To provide a fair comparison, we set J = 10, use 10
inducing points in NyHSIC, and 10 random Fourier features
in FHSIC and RDC.

−

Optimization of NFSIC-opt The parameters of NFSIC-opt
are σx, σy, and J locations of size (dx + dy)J. We treat all
the parameters as a long vector in R2+(dx+dy)J and use gra-
dient ascent to optimize ˆλn/2. We observe that initializing
VJ by randomly picking J points from the training sample
yields good performance. The regularization parameter γn
in NFSIC is ﬁxed to a small value, and is not optimized. It is
worth emphasizing that the complexity of the optimization
procedure is still linear-time.2

1We use a permutation test for RDC, following the au-
thors’ implementation (https://github.com/lopezpaz/
randomized_dependence_coefficient, referred com-
mit: b0ac6c0).

2Our claim on linear runtime (with respect to n) is for the
gradient ascent procedure to ﬁnd a local optimum for θ. We do not

An Adaptive Test of Independence with Analytic Kernel Embeddings

(a) SG (α = 0.05)

(b) SG (α = 0.05)

(c) Sin

(d) GSign

Figure 2: (a): Runtime. (b): Probability of rejecting H0 as problem parameters vary. Fix n = 4000.

Since FSIC, NyHFSIC and RDC rely on a ﬁnite-
dimensional kernel approximation, these tests are consistent
only if both the number of features increases with n. By
constrast, the proposed NFSIC requires only n to go to in-
ﬁnity to achieve consistency i.e., J can be ﬁxed. We refer
the reader to Appendix C for a brief investigation of the test
power vs. increasing J. The test power does not necessarily
monotonically increase with J.

3.1. Toy Problems

We consider three toy problems.

1. Same Gaussian (SG). The two variables are indepen-
dently drawn from the standard multivariate normal distri-
bution i.e., X
(0, Idy ) where Id
d identity matrix. This problem represents a case
is the d
×
in which H0 holds.

(0, Idx ) and Y

∼ N

∼ N

,

Y

X

∝
−

2. Sinusoid (Sin). Let pxy be the probability density of Pxy.
In the Sinusoid problem, the dependency of X and Y is char-
pxy(x, y)
acterized by (X, Y )
1 + sin(ωx) sin(ωy),
∼
π, π) and ω is the fre-
= (
where the domains of
quency of the sinusoid. As the frequency ω increases, the
drawn sample becomes more similar to a sample drawn
π, π)2). That is, the higher ω, the harder
from Uniform((
to detect the dependency between X and Y . This problem
was studied in Sejdinovic et al. (2013). Plots of the density
for a few values of ω are shown in Figures 6 and 7 in the
appendix. The main characteristic of interest in this problem
is the local change in the density function.

−

|

(cid:81)dx

∼ N

i=1 sgn(Xi), where X

In this problem, Y =
3. Gaussian Sign (GSign).
) is the
(0, Idx ), sgn(
Z
·
|
sign function, and Z
(0, 1) serves as a source of noise.
The full interaction of X = (X1, . . . , Xdx ) is what makes
the problem challenging. That is, Y is dependent on X,
yet it is independent of any proper subset of
.
}
Thus, simultaneous consideration of all the coordinates of
X is required to successfully detect the dependency.

X1, . . . , Xd

∼ N

{

We ﬁx n = 4000 and vary the problem parameters. Each
problem is repeated for 300 trials, and the sample is redrawn
each time. The signiﬁcance level α is set to 0.05. The re-

claim a linear runtime to ﬁnd a global optimum.

sults are shown in Figure 2. It can be seen that in the SG
problem (Figure 2b) where H0 holds, all the tests achieve
roughly correct type-I errors at α = 0.05. In particular,
we point out that NFSIC-opt’s rejection rate is well con-
trolled as the sample used for testing and the sample used
for parameter tuning are independent. The rejection rate
would have been much higher had we done the optimization
and testing on the same sample (i.e., overﬁtting). In the
Sin problem, NFSIC-opt achieves high test power for all
considered ω = 1, . . . , 6, highlighting its strength in detect-
ing local changes in the joint density. The performance of
NFSIC-med is signiﬁcantly lower than that of NFSIC-opt.
This phenomenon clearly emphasizes the importance of the
optimization to place the locations at the relevant regions in
. RDC has a remarkably high performance in both Sin
X ×Y
and GSign (Figure 2c, 2d) despite no parameter tuning. The
ability to simultaneously consider interacting features of
NFSIC-opt is indicated by its superior test power in GSign,
especially at the challenging settings of dx = 5, 6.

NFSIC vs. QHSIC. We observe that NFSIC-opt outper-
forms the quadratic-time QHSIC in these two problems.
QHSIC is deﬁned as the RKHS norm of the witness func-
tion u (see (2)). Intuitively, one can think of the RKHS
norm as taking into account all the locations (v, w). By
contrast, the proposed NFSIC evaluates the witness function
at J locations. If the differences in pxy and pxpy are local
(e.g., Sin problem), or there are interacting features (e.g.,
GSign problem), then only small regions in the space of
(X, Y ) are relevant in detecting the difference of pxy and
pxpy. In these cases, pinpointing exact test locations by
the optimization of NFSIC performs well. On the other
hand, taking into account all possible test locations as done
implicitly in QHSIC also integrates over regions where the
difference between pxy and pxpy is small, resulting in a
weaker indication of dependence. Whether QHSIC is better
than NFSIC depends heavily on the problem, and there is
no one best answer. If the difference between pxy and pxpy
is large only in localized regions, then the proposed linear
time statistic has an advantage. If the difference is spatially
diffuse, then QHSIC has an advantage. No existing work
has proposed a procedure to optimally tune kernel param-
eters for QHSIC; by contrast, NFSIC has a clearly deﬁned
objective for parameter tuning.

100200dxanddy100101102Time(s)50100150200250dxanddy0.040.06Type-Ierror123456ωin1+sin(ωx)sin(ωy)0.00.51.0Testpower123456dx0.00.51.0Testpower123456dx0.00.51.0TestpowerNFSIC-optNFSIC-medQHSICNyHSICFHSICRDCAn Adaptive Test of Independence with Analytic Kernel Embeddings

(a) SG. dx = dy = 250.

(b) SG. dx = dy = 250.

(c) Sin. ω = 4.

(d) GSign. dx = 4.

Figure 3: (a) Runtime. (b): Probability of rejecting H0 as n increases in the toy problems.

To investigate the sample efﬁciency of all the tests, we ﬁx
dx = dy = 250 in SG, ω = 4 in Sin, dx = 4 in GSign, and
increase n. Figure 3 shows the results. The quadratic depen-
dency on n in QHSIC makes it infeasible both in terms of
memory and runtime to consider n larger than 6000 (Fig-
ure 3a). By constrast, although not the most time-efﬁcient,
NFSIC-opt has the highest sample-efﬁciency for GSign, and
for Sin in the low-sample regime, signiﬁcantly outperform-
ing QHSIC. Despite the small additional overhead from the
optimization, we are yet able to conduct an accurate test
with n = 105, dx = dy = 250 in less than 100 seconds.
We observe in Figure 3b that the two NFSIC variants have
correct type-I errors across all sample sizes. We recall from
Theorem 5 that the NFSIC test with random test locations
will asymptotically reject H0 if it is false. A demonstration
of this property is given in Figure 3c, where the test power
of NFSIC-med eventually reaches 1 with n higher than 105.

3.2. Real Problems

We now examine the performance of our proposed test on
real problems.

Million Song Data (MSD) We consider a subset of the
Million Song Data3 (Bertin-Mahieux et al., 2011), in which
each song (X) out of 515,345 is represented by 90 features,
of which 12 features are timbre average (over all segments)
of the song, and 78 features are timbre covariance. Most of
the songs are western commercial tracks from 1922 to 2011.
The goal is to detect the dependency between each song and
its year of release (Y ). We set α = 0.01, and repeat for
300 trials where the full sample is randomly subsampled
to n points in each trial. Other settings are the same as
in the toy problems. To make sure that the type-I error
is correct, we use the permutation approach in the NFSIC
tests to compute the threshold. Figure 4b shows the test
powers as n increases from 500 to 2000. To simulate the
case where H0 holds in the problem, we permute the sample
to break the dependency of X and Y . The results are shown
in Figure 5 in the appendix.

Evidently, NFSIC-opt has the highest test power among all

3Million Song Data subset: https://archive.ics.

uci.edu/ml/datasets/YearPredictionMSD.

(a) MSD problem.

(b) Videos & Captions problem.
Figure 4: Probability of rejecting H0 as n increases in the
two real problems. α = 0.01.

the linear-time tests for all the sample sizes. Its test power
is second to only QHSIC. We recall that NFSIC-opt uses
half of the sample for parameter tuning. Thus, at n = 500,
the actual sample for testing is 250, which is relatively
small. The fact that there is a vast power gain from 0.4
(NFSIC-med) to 0.8 (NFSIC-opt) at n = 500 suggests that
the optimization procedure can perform well even at a lower
sample sizes.

Videos and Captions Our last problem is based on the
VideoStory46K4 dataset (Habibian et al., 2014). The
dataset contains 45,826 Youtube videos (X) of an aver-
age length of roughly one minute, and their corresponding
text captions (Y ) uploaded by the users. Each video is
represented as a dx = 2000 dimensional Fisher vector en-
coding of motion boundary histograms (MBH) descriptors
of Wang & Schmid (2013). Each caption is represented
as a bag of words with each feature being the frequency
of one word. After ﬁltering only words which occur in at
least six video captions, we obtain dy = 1878 words. We
examine the test powers as n increases from 2000 to 8000.
The results are given in Figure 4. The problem is sufﬁ-
ciently challenging that all linear-time tests achieve a low
power at n = 2000. QHSIC performs exceptionally well
on this problem, achieving a maximum power throughout.
NFSIC-opt has the highest sample efﬁciency among the
linear-time tests, showing that the optimization procedure is
also practical in a high dimensional setting.

4VideoStory46K dataset: https://ivi.fnwi.uva.nl/

isis/mediamill/datasets/videostory.php.

103104105Samplesizen100102Time(s)104105Samplesizen0.040.06Type-Ierror103104105Samplesizen0.00.51.0Testpower103104105Samplesizen0.00.51.0Testpower123456dx0.00.51.0TestpowerNFSIC-optNFSIC-medQHSICNyHSICFHSICRDC500100015002000Samplesizen0.000.010.02Type-IerrorNFSIC-optNFSIC-medQHSICNyHSICFHSICRDC500100015002000Samplesizen0.51.0Testpower2000400060008000Samplesizen0.51.0TestpowerAn Adaptive Test of Independence with Analytic Kernel Embeddings

Acknowledgement

We thank the Gatsby Charitable Foundation for the ﬁnancial
support. The major part of this work was carried out while
Zoltán Szabó was a research associate at the Gatsby Com-
putational Neuroscience Unit, University College London.

References

Anderson, Theodore W. An Introduction to Multivariate

Statistical Analysis. Wiley, 2003.

Bach, Francis R. and Jordan, Michael I. Kernel indepen-
dent component analysis. Journal of Machine Learning
Research, 3:1–48, 2002.

Bertin-Mahieux, Thierry, Ellis, Daniel P.W., Whitman,
Brian, and Lamere, Paul. The million song dataset. In
International Conference on Music Information Retrieval
(ISMIR), 2011.

Chwialkowski, Kacper P., Ramdas, Aaditya, Sejdinovic,
Dino, and Gretton, Arthur. Fast Two-Sample Testing
with Analytic Representations of Probability Measures.
In Advances in Neural Information Processing Systems
(NIPS), pp. 1981–1989. 2015.

Dauxois, Jacques and Nkiet, Guy Martial. Nonlinear canon-
ical analysis and independence tests. The Annals of Statis-
tics, 26(4):1254–1278, 1998.

Feuerverger, Andrey. A consistent test for bivariate depen-
dence. International Statistical Review, 61(3):419–433,
1993.

Fukumizu, Kenji, Gretton, Arthur, Sun, Xiaohai, and
Schölkopf, Bernhard. Kernel measures of conditional
dependence. In Advances in Neural Information Process-
ing Systems (NIPS), pp. 489–496, 2008.

Gretton, Arthur and Györﬁ, László. Consistent nonparamet-
ric tests of independence. Journal of Machine Learning
Research, 11:1391–1423, 2010.

Gretton, Arthur, Bousquet, Olivier, Smola, Alex, and
Schölkopf, Bernhard. Measuring Statistical Dependence
with Hilbert-Schmidt Norms. In Algorithmic Learning
Theory (ALT), pp. 63–77. 2005.

Gretton, Arthur, Fukumizu, Kenji, Teo, Choon H., Song,
Le, Schölkopf, Bernhard, and Smola, Alex J. A Kernel
Statistical Test of Independence. In Advances in Neural
Information Processing Systems (NIPS), pp. 585–592.
2008.

Habibian, Amirhossein, Mensink, Thomas, and Snoek,
Cees GM. Videostory: A new multimedia embedding
for few-example recognition and translation of events. In

ACM International Conference on Multimedia, pp. 17–26,
2014.

Heller, Ruth, Heller, Yair, Kaufman, Shachar, Brill, Barak,
and Gorﬁne, Malka. Consistent distribution-free k-
sample and independence tests for univariate random
variables. Journal of Machine Learning Research, 17
(29):1–54, 2016.

Huo, Xiaoming and Székely, Gábor J. Fast computing
for distance covariance. Technometrics, 58(4):435–447,
2016.

Jitkrittum, Wittawat, Szabó, Zoltán, Chwialkowski, Kacper,
and Gretton, Arthur. Interpretable Distribution Features
with Maximum Testing Power. In Advances in Neural
Information Processing Systems (NIPS), pp. 181–189.
2016.

Kowalski, Jeanne and Tu, Xin M. Modern Applied U-

Statistics. John Wiley & Sons, 2008.

Lehmann, Eric L. Elements of Large-Sample Theory.

Springer Science & Business Media, 1999.

Lopez-Paz, David, Hennig, Philipp, and Schölkopf, Bern-
hard. The Randomized Dependence Coefﬁcient. In Ad-
vances in Neural Information Processing Systems (NIPS),
pp. 1–9. 2013.

Rahimi, Ali and Recht, Benjamin. Random features for
large-scale kernel machines. In Advances in Neural In-
formation Processing Systems (NIPS), pp. 1177–1184.
2008.

Sejdinovic, Dino, Sriperumbudur, Bharath, Gretton, Arthur,
and Fukumizu, Kenji. Equivalence of distance-based and
RKHS-based statistics in hypothesis testing. The Annals
of Statistics, 41(5):2263–2291, 2013.

Serﬂing, Robert J. Approximation Theorems of Mathemati-

cal Statistics. John Wiley & Sons, 2009.

Smola, Alex, Gretton, Arthur, Song, Le, and Schölkopf,
Bernhard. A Hilbert space embedding for distributions.
In International Conference on Algorithmic Learning
Theory (ALT), pp. 13–31, 2007.

Sriperumbudur, Bharath K., Gretton, Arthur, Fukumizu,
Kenji, Schölkopf, Bernhard, and Lanckriet, Gert R. G.
Hilbert Space Embeddings and Metrics on Probability
Measures. Journal of Machine Learning Research, 11:
1517–1561, 2010.

Steinwart, Ingo and Christmann, Andreas. Support vector
machines. Springer Science & Business Media, 2008.

An Adaptive Test of Independence with Analytic Kernel Embeddings

Székely, Gábor J. and Rizzo, Maria L. Brownian distance
covariance. The Annals of Applied Statistics, 3(4):1236–
1265, 2009.

Székely, Gábor J., Rizzo, Maria L., and Bakirov, Nail K.
Measuring and testing dependence by correlation of dis-
tances. The Annals of Statistics, 35(6):2769–2794, 2007.

van der Vaart, Aad. Asymptotic Statistics. Cambridge Uni-

versity Press, 2000.

Wang, Heng and Schmid, Cordelia. Action recognition with
improved trajectories. In IEEE International Conference
on Computer Vision (ICCV), pp. 3551–3558, 2013.

Zhang, Kun, Peters, Jonas, Janzing, Dominik, and
Schölkopf, Bernhard. Kernel-based conditional indepen-
dence test and application in causal discovery. In Confer-
ence on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
804–813, 2011.

Zhang, Qinyi, Filippi, Sarah, Gretton, Arthur, and Sejdi-
novic, Dino. Large-Scale Kernel Methods for Indepen-
dence Testing. Statistics and Computing, pp. 1–18, 2017.

