Batched High-dimensional Bayesian Optimization
via Structural Kernel Learning (Appendix)

Zi Wang * 1 Chengtao Li * 1 Stefanie Jegelka 1 Pushmeet Kohli 2

1. Add-UCB-DPP-BBO Algorithm

We present four variants of Add-UCB-DPP-BBO in Algo-
rithm 1. The algorithm framework is general in that, one
can plug in other acquisition and quality functions other
than UCB to get different algorithms.

2. Additional experiments

In this section, we provide more details in our experiments.

2.1. Optimization of the Acquisition Functions

We decompose the acquisition function into M subacquisi-
tion functions, one for each part, and optimize those sepa-
rately. We randomly sample 10000 points in the low di-
mensional space, and then choose the one with the best
value to start gradient descent in the search space (i.e. the
range of the box on R|Am|). In practice, we observe this ap-
proach optimizes low-dimensional (< 5 dimensions) func-
tions very well. As the number of dimensions grows, the
known difﬁculties of high dimensional BO (and global non-
convex optimization) arise.

2.2. Effectiveness of Decomposition Learning

Recovering Decompositions
In Table 4, Table 2 and
Table 3, we show three quantities which may imply
the quality of the learned decompositions.
The ﬁrst
is the Rand Index of
quantity , reported in Table 4,
the decompositions learned by Gibbs sampling, namely,
(cid:80)

i<j≤D

1

z

g
i

≡z

∧zi≡zj

g
j

i<j≤D

1

z

g
i

(cid:54)=z

∧zi(cid:54)=zj

g
j

. The second

+(cid:80)
(D
2 )

quantity, reported in Table 2, is the probability of two di-
mensions being correctly grouped together by Gibbs sam-

*Equal contribution

1Computer Science and Artiﬁcial In-
telligence Laboratory, Massachusetts
Institute of Technol-
ogy, Massachusetts, USA 2DeepMind, London, UK. Cor-
respondence to: Zi Wang <ziw@csail.mit.edu>, Chengtao
Li <ctli@mit.edu>, Stefanie Jegelka <stefje@csail.mit.edu>,
Pushmeet Kohli <pushmeet@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

z

1

g
i

(cid:80)

i<j≤D
(cid:80)

pling in each iteration of Gibbs sampling after the burn-in
≡z

period, namely,
. The third quantity,
reported in Table 3, is the probability of two dimensions
being correctly separated by Gibbs sampling in each iter-
ation of Gibbs sampling after the burn-in period, namely,
(cid:80)

1zi≡zj

∧zi≡zj

i<j≤D

g
j

i<j≤D
(cid:80)

1

z

g
i

(cid:54)=z

∧zi(cid:54)=zj

g
j

i<j≤D

1zi(cid:54)=zj

.

Sensitivity Analysis for α Empirically, we found that the
quality of the learned decompositions is not very sensitive
to the scale of α (see Table 4), because the log data likeli-
hood plays a much more important role than log(|Am|+α)
when α is less than the total number of dimensions. The re-
ported results correspond to alpha = 1 for all the partitions.

BO for Synthetic Functions We show an example of a 2
dimensional function component in the additive synthetic
function in Fig. 1. Because of the numerous local max-
ima, it is very challenging to achieve the global optimum
even for 2 dimensions, let alone maximizing an additive
sum of them, only by observing their sum. The full results
of the simple and cumulative regrets for the synthetic func-
tions comparing Add-GP-UCB with known additive struc-
ture (Known), no partitions (NP), fully partitioned with one
dimension for each group (FP) and the following methods
of learning partition: Gibbs sampling (Gibbs), random
sampling the same number of partitions sampled by Gibbs
and select the one with the highest data likelihood (PL-1),
random sampling 5 partitions and select the one with the
highest data likelihood (PL-2) are shown in Fig. 3. The
learning was done every 50 iterations, starting from the ﬁrst
iteration. For D = 20, 30, it is quite obvious that when a
new partition is learned from the newly observed data (e.g.
at iteration 100 and 150), the simple regret gets a boost.

BO for Real-world functions
In addition to be 14 pa-
rameter robot pushing task, we tested on the walker func-
tion which returns the walking speed of a three-link planar
bipedal walker implemented in Matlab (Westervelt et al.,
2007). We tune 25 parameters that may inﬂuence the walk-
ing speed, including 3 sets of 8 parameters for the ODE
solver and 1 parameter specifying the initial velocity of
the stance leg. To our knowledge, this function does not

Batched High-dimensional Bayesian Optimization

Algorithm 1 Add-UCB-DPP-BBO Variants

Input: X , Ninit, Ncyc, T , B, M
Observe function values of Ninit points chosen randomly from X
Get the initial decomposition of feature space via Gibbs sampling and get corresponding Xm’s
for t = 1 to T do

if (t mod Ncyc = 0) then

Learn the decomposition via Gibbs sampling and get corresponding Xm’s

end if
Choose x0 by maximizing UCB (acquisition function) for each group and combine them
for m = 1 to M do
Compute (R(m)
t
Sample {x(m)

}i∈[B−1] ⊆ Xm via PE or DPP with kernel K(m)

)+ and K(m)

(t−1)B+1

(t−1)B+1

i

end for
Combine {x(m)
{xi}i∈[B−1]
Observe (noisy) function values for {xi} for i ∈ {0, . . . , B − 1}.

i

end for

}i∈[B−1],m∈[M ] either randomly or by maximizing UCB (quality function) without replacement to get

Table 2. Empirical posterior of any two dimensions correctly being grouped together by Gibbs sampling.

Table 1. Rand Index of the decompositions computed by Gibbs sampling.

50

150

250

350

450

0.85 ± 0.20
0.78 ± 0.06
0.88 ± 0.02
0.95 ± 0.01
0.98 ± 0.00

0.83 ± 0.23
0.85 ± 0.08
0.88 ± 0.02
0.95 ± 0.01
0.97 ± 0.00

0.71 ± 0.18
0.86 ± 0.10
0.89 ± 0.02
0.95 ± 0.01
0.97 ± 0.00

0.68 ± 0.16
0.89 ± 0.12
0.92 ± 0.02
0.95 ± 0.01
0.97 ± 0.00

0.66 ± 0.18
0.95 ± 0.06
0.95 ± 0.04
0.95 ± 0.01
0.97 ± 0.00

50

150

250

350

450

0.81 ± 0.28
0.21 ± 0.13
0.06 ± 0.06
0.02 ± 0.03
0.01 ± 0.01

0.91 ± 0.19
0.54 ± 0.25
0.11 ± 0.08
0.02 ± 0.02
0.01 ± 0.01

1.00 ± 0.03
0.68 ± 0.25
0.20 ± 0.12
0.03 ± 0.03
0.01 ± 0.01

0.97 ± 0.08
0.81 ± 0.27
0.43 ± 0.17
0.04 ± 0.03
0.01 ± 0.01

1.00 ± 0.00
0.93 ± 0.15
0.71 ± 0.22
0.06 ± 0.04
0.02 ± 0.02

Table 3. Empirical posterior of any two dimensions correctly being separated by Gibbs sampling.

50

150

250

350

450

0.30 ± 0.46
0.87 ± 0.17
0.88 ± 0.05
0.94 ± 0.02
0.98 ± 0.00
0.99 ± 0.00

0.30 ± 0.46
0.80 ± 0.27
0.89 ± 0.06
0.94 ± 0.02
0.98 ± 0.00
0.99 ± 0.00

0.90 ± 0.30
0.60 ± 0.32
0.89 ± 0.07
0.94 ± 0.02
0.98 ± 0.01
0.99 ± 0.00

0.90 ± 0.30
0.55 ± 0.29
0.91 ± 0.08
0.95 ± 0.02
0.98 ± 0.00
0.99 ± 0.00

1.00 ± 0.00
0.50 ± 0.34
0.94 ± 0.07
0.97 ± 0.02
0.98 ± 0.01
0.99 ± 0.00

N

D
5
10
20
50
100

N

dx
5
10
20
50
100

N

dx
2
5
10
20
50
100

have an additive structure. The regrets of each decompo-
sition learning methods are shown in Fig. 2. In addition
to Gibbs, we test learning decomposition via constrained
Gibbs sampling (Gibbs-L), where the maximum size of
each group of dimensions does not exceed 2. Because
the function does not have additive structure, Gibbs per-

formed poorly since it groups together many dimensions
of the input. As a result, its performance is similar to that
of no partition (NP). However, Gibbs-L appears to learn
a good decomposition with the group size limit, and man-
ages to achieve a slightly lower regret than other methods.
Gibbs, PL-1, PL-2 and FP all performed relatively well

Batched High-dimensional Bayesian Optimization

Table 4. Rand Index of the decompositions learned by Gibbs sampling for different values of α.

N

α
0.2
0.5
1
2
5

50

150

250

350

450

0.87811 ± 0.019002
0.88211 ± 0.019893
0.88211 ± 0.016947
0.88084 ± 0.016972
0.88337 ± 0.015784

0.90126 ± 0.022394
0.90305 ± 0.024574
0.90326 ± 0.024935
0.9 ± 0.023489
0.90158 ± 0.02203

0.95284 ± 0.047111
0.95295 ± 0.046232
0.95305 ± 0.043878
0.95463 ± 0.042968
0.96126 ± 0.037045

0.98811 ± 0.02602
0.98947 ± 0.025872
0.98558 ± 0.034779
0.97989 ± 0.038818
0.98716 ± 0.030949

0.98811 ± 0.026322
0.99505 ± 0.013881
0.98053 ± 0.035843
0.98832 ± 0.023592
0.99316 ± 0.015491

Figure 1. An example of a 2 dimensional function component of
the synthetic function.

in for this function, indicating that using the additive struc-
ture may beneﬁt the BO procedure even if the function it-
self is not additive.

2.3. Diverse Batch Sampling

In Fig. 4, we show the full results of the simple and the
cumulative regrets on the synthetic functions described in
Section 5.2 of the paper.

References

Westervelt, Eric R, Grizzle, Jessy W, Chevallereau, Chris-
tine, Choi, Jun Ho, and Morris, Benjamin. Feedback
control of dynamic bipedal robot locomotion, volume 28.
CRC press, 2007.

Figure 2. Simple regret of tuning the 25 parameters for optimiz-
ing the walking speed of a bipedal robot. We use the vanilla
Gibbs sampling algorithm (Gibbs) and a Gibbs sampling algo-
rithm with partition size limit set to be 2 (Gibbs-L) to compare
with partial learning (PL-1, PL-2), no partitions (NP), and fully
partitioned (FP). Gibbs-L performed slightly better than PL-2
and FP. This function does not have an additive structure, and as
a result, Gibbs does not perform well for this function because
the sizes of the groups it learned tend to be large .

10.5x1000.5x205-5101f(m)(x)t100200300400500rt11.522.533.544.5NPFPPL-1PL-2GibbsGibbs-LBatched High-dimensional Bayesian Optimization

Figure 3. The simple regrets (rt) and the averaged cumulative regrets (Rt) and for Known (ground truth partition is given), Gibbs
(using Gibbs sampling to learn the partition), PL-1 (randomly sample the same number of partitions sampled by Gibbs and select
the one with highest data likelihood), PL-2 (randomly sample 5 partitions and select the one with highest data likelihood), FP (fully
partitioned, each group with one dimension) and NP (no partition) on 10, 20, 50 dimensional functions. Gibbs achieved comparable
results to Known. Comparing PL-1 and PL-2 we can see that sampling more partitions did help to ﬁnd a better partition. But a more
principled way of learning partition using Gibbs can achieve much better performance than PL-1 and PL-2.

100200300400500t00.20.40.60.81RtD=2KnownNPFPPL-1PL-2Gibbs100200300400500t5101520253035RtD=5KnownNPFPPL-1PL-2Gibbs100200300400500t5101520253035RtD=10KnownNPFPPL-1PL-2Gibbs100200300400500t102030405060RtD=20KnownNPFPPL-1PL-2Gibbs100200300400500t020406080RtD=30KnownNPFPPL-1PL-2Gibbs200400600t20406080100120RtD=50KnownNPFPPL-1PL-2Gibbs100200300400500t-0.200.20.40.60.8rtD=2KnownNPFPPL-1PL-2Gibbs100200300400500t010203040rtD=5KnownNPFPPL-1PL-2Gibbs100200300400500t010203040rtD=10KnownNPFPPL-1PL-2Gibbs100200300400500t0102030405060rtD=20KnownNPFPPL-1PL-2Gibbs100200300400500t020406080rtD=30KnownNPFPPL-1PL-2Gibbs200400600t020406080100120rtD=50KnownNPFPPL-1PL-2GibbsAveraged Cumulative RegretSimple RegretBatched High-dimensional Bayesian Optimization

Figure 4. The simple regrets (rt) and the averaged cumulative regrets (Rt) on synthetic functions with various dimensions when the
ground truth partition is known. Four batch sampling methods (Batch-UCB-PE, Batch-UCB-DPP, Batch-UCB-PE-Fnc and
Batch-UCB-DPP-Fnc) perform comparably well and outperform random sampling (Rand) by a large gap.

Averaged Cumulative RegretSimple Regrett20406080100rt00.511.522.5D=2RandBatch-UCB-PEBatch-UCB-DPPBatch-UCB-PE-FncBatch-UCB-DPP-Fnct20406080100rt0510152025D=5t20406080100rt051015202530D=10t20406080100rt01020304050D=20t20406080100rt010203040506070D=30t20406080100rt020406080100D=50t20406080100Rt00.511.522.5D=2RandBatch-UCB-PEBatch-UCB-DPPBatch-UCB-PE-FncBatch-UCB-DPP-Fnct20406080100Rt0510152025D=5t20406080100Rt051015202530D=10t20406080100Rt01020304050D=20t20406080100Rt10203040506070D=30t20406080100Rt30405060708090100D=50