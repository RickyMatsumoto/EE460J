Estimating individual treatment effect: generalization bounds and algorithms

Uri Shalit * 1 Fredrik D. Johansson * 2 David Sontag 2 3

Abstract

There is intense interest in applying machine
learning to problems of causal inference in ﬁelds
such as healthcare, economics and education.
individual-level causal inference
In particular,
has important applications such as precision
medicine. We give a new theoretical analy-
sis and family of algorithms for predicting indi-
vidual treatment effect (ITE) from observational
data, under the assumption known as strong ig-
norability. The algorithms learn a “balanced”
representation such that the induced treated and
control distributions look similar, and we give
a novel and intuitive generalization-error bound
showing the expected ITE estimation error of a
representation is bounded by a sum of the stan-
dard generalization-error of that representation
and the distance between the treated and con-
trol distributions induced by the representation.
We use Integral Probability Metrics to measure
distances between distributions, deriving explicit
bounds for the Wasserstein and Maximum Mean
Discrepancy (MMD) distances. Experiments on
real and simulated data show the new algorithms
match or outperform the state-of-the-art.

1. Introduction

Making predictions about causal effects of actions is a cen-
tral problem in many domains. For example, a doctor de-
ciding which medication will cause better outcomes for a
patient; a government deciding who would beneﬁt most
from subsidized job training; or a teacher deciding which
study program would most beneﬁt a speciﬁc student.
In
this paper we focus on the problem of making these predic-
tions based on observational data. Observational data is

*Equal contribution

1CIMS, New York University, New
York, NY 10003 2IMES, MIT, Cambridge, MA 02142
3CSAIL, MIT, Cambridge, MA 02139.
Correspondence
to: Uri Shalit <shalit@cs.nyu.edu>, Fredrik D. Johansson
<fredrikj@mit.edu>, David Sontag <dsontag@csail.mit.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

data which contains past actions, their outcomes, and pos-
sibly more context, but without direct access to the mecha-
nism which gave rise to the action. For example we might
have access to records of patients (context), their medica-
tions (actions), and outcomes, but we do not have complete
knowledge of why a speciﬁc action was applied to a patient.

The hallmark of learning from observational data is that
the actions observed in the data depend on variables which
might also affect the outcome, resulting in confounding:
For example, richer patients might better afford certain
medications, and job training might only be given to those
motivated enough to seek it. The challenge is how to untan-
gle these confounding factors and make valid predictions.
Speciﬁcally, we work under the common simplifying as-
sumption of “no-hidden confounding”, assuming that all
the factors determining which actions were taken are ob-
served. In the examples above, it would mean that we have
measured a patient’s wealth or an employee’s motivation.

As a learning problem, estimating causal effects from ob-
servational data is different from classic learning in that in
our training data we never see the individual-level effect.
For each unit, we only see their response to one of the pos-
sible actions - the one they had actually received. This is
close to what is known in the machine learning literature as
“learning from logged bandit feedback” (Strehl et al., 2010;
Swaminathan & Joachims, 2015), with the distinction that
we do not have access to the model generating the action.

Our work differs from much work in causal inference in
that we focus on the individual-level causal effect (“c-
speciﬁc treatment effects” Shpitser & Pearl (2006); Pearl
(2015)), rather than the average or population level. Our
main contribution is to give what is, to the best of our
knowledge, the ﬁrst generalization-error1 bound for esti-
mating individual-level causal effect, where each individ-
ual is identiﬁed by its features x. The bound leads natu-
rally to a new family of representation-learning based al-
gorithms (Bengio et al., 2013), which we show to match or
outperform state-of-the-art methods on several causal ef-
fect inference tasks.

1Our use of the term generalization is different from its use in
the study of transportability, where the goal is to generalize causal
conclusion across distributions (Bareinboim & Pearl, 2016).

Estimating individual treatment effect: generalization bounds and algorithms

We frame our results using the Neyman-Rubin potential
outcomes framework (Rubin, 2011), as follows. We as-
sume that for a unit with features x ∈ X , and an action
(also known as treatment or intervention) t ∈ {0, 1}, there
are two potential outcomes: Y0 and Y1. For each unit we
only observe one of the potential outcomes, according to
if t = 0 we observe y = Y0, if
treatment assignment:
t = 1, we observe y = Y1; this is known as the consis-
tency assumption. For example, x can denote the set of lab
tests and demographic factors of a diabetic patient, t = 0
denote the standard medication for controlling blood sugar,
t = 1 denotes a new medication, and Y0 and Y1 indicate the
patient’s blood sugar level if they were to be given medica-
tions t = 0 and t = 1, respectively.
We will denote m1(x) = E [Y1|x], m0(x) = E [Y0|x].
We are interested in learning the function τ (x)
:=
E [Y1 − Y0|x] = m1(x) − m0(x). τ (x) is the expected
treatment effect of t = 1 relative to t = 0 on a unit with
characteristics x, or the Individual Treatment Effect (ITE)2.
Our goal is to ﬁnd an estimate ˆτ of τ such that some loss
function, e.g. E
, is small. For example, for a
patient with features x, we attempt to predict which of two
treatments will have a better outcome. The fundamental
problem of causal inference is that for any x in our data we
only observe Y1 or Y0, but never both.

(ˆτ − τ )2(cid:105)

(cid:104)

As mentioned above, we make an important “no-hidden
confounders” assumption,
in order to make the condi-
tional causal effect identiﬁable. We formalize this assump-
tion by using the standard strong ignorability condition:
(Y1, Y0) ⊥⊥ t|x, and 0 < p(t = 1|x) < 1 for all x. Strong
ignorability is a sufﬁcient condition for the ITE function
τ (x) to be identiﬁable (Imbens & Wooldridge, 2009; Pearl,
2015; Rolling, 2014): see proof in the supplement. The va-
lidity of strong ignorability cannot be assessed from data,
and must be determined by domain knowledge and under-
standing of the causal relationships between the variables.

One approach to the problem of estimating the function
τ (x) is by learning the two functions m0(x) and m1(x)
using samples from p(Yt|x, t). This is similar to a stan-
dard machine learning problem of learning from ﬁnite sam-
ples. However, there is an additional source of variance at
work here: For example, if mostly rich patients received
treatment t = 1, and mostly poor patients received treat-
ment t = 0, we might have an unreliable estimation of
m1(x) for poor patients. In this paper we upper bound this
additional source of variance using an Integral Probabil-
ity Metric (IPM) measure of distance between two distri-
butions p(x|t = 0), and p(x|t = 1), also known as the
control and treated distributions. In practice we use two
speciﬁc IPMs: the Maximum Mean Discrepancy (Gretton

2Also known as Conditional Average Treatment Effect, CATE.

Figure 1. Neural network architecture for ITE estimation. L is
a loss function, IPMG is an integral probability metric. Note that
only one of h0 and h1 is updated for each sample during training.

et al., 2012), and the Wasserstein distance (Villani, 2008;
Cuturi & Doucet, 2014). We show that the expected error
in learning the individual treatment effect function τ (x) is
upper bounded by the error of learning Y1 and Y0, plus the
IPM term. In the randomized controlled trial setting, where
t ⊥⊥ x, the IPM term is 0, and our bound naturally reduces
to a standard learning problem of learning two functions.

The bound we derive points the way to a family of algo-
rithms based on the idea of representation learning (Ben-
gio et al., 2013): Jointly learn hypotheses for both treated
and control on top of a representation which minimizes a
weighted sum of the factual loss (the standard supervised
machine learning objective), and the IPM distance between
the control and treated distributions induced by the repre-
sentation. This can be viewed as learning the functions m0
and m1 under a constraint that encourages better general-
ization across the treated and control populations. In the
Experiments section we apply algorithms based on neural
nets as representations and hypotheses, along with MMD
or Wasserstein distributional distances over the representa-
tion layer; see Figure 1 for the basic architecture.

In his foundational text on causality, Pearl (2009) writes:
“Whereas in traditional learning tasks we attempt to gener-
alize from one set of instances to another, the causal mod-
eling task is to generalize from behavior under one set of
conditions to [...] another set. Causal models should there-
fore be chosen by a criterion that challenges their stability
against changing conditions” [emphasis ours]. We believe
our work points the way to one such stability criterion, for
causal inference in the strongly ignorable case.

2. Related work

Much recent work in machine learning for causal infer-
ence focuses on causal discovery, with the goal of discov-
ering the underlying causal graph or causal direction from
data (Hoyer et al., 2009; Maathuis et al., 2010; Triantaﬁl-
lou & Tsamardinos, 2015; Mooij et al., 2016). We focus
on the case when the causal setup is simple and known
to be of the form (Y1, Y0) ⊥⊥ x|t, with no hidden con-
founders. Under the causal model we assume, the most
common goal of causal effect inference as used in the ap-

𝑡𝑥𝐿(ℎ&(Φ),𝑦=𝑌&)ΦIPM0(𝑝23456,𝑝2345&)ℎ6𝐿ℎ6Φ,𝑦=𝑌6………ℎ&𝑖𝑓	  𝑡=0𝑖𝑓	  𝑡=1Estimating individual treatment effect: generalization bounds and algorithms

plied sciences is to obtain the average treatment effect:
AT E = Ex∼p(x) [τ (x)]. We will brieﬂy discuss how some
standard statistical causal effect inference methods relate to
our proposed method. Note that most of these approaches
assume some form of ignorability.

One of the most widely used approaches to estimating ATE
is covariate (or back-door) adjustment, also known as the
G-computation formula (Robins, 1986; Pearl, 2009).
In
their basic version, covariate adjustment methods aim to
estimate the functions m1(x), m0(x), and are therefore
natural candidates for estimating ITE as well as ATE, us-
ing the estimates of mt(x). Most previous work on this
subject focused on asymptotic consistency (Belloni et al.,
2014; Athey et al., 2016; Chernozhukov et al., 2016), and
so far there has not been much work on the generalization-
error of such a procedure. One view of our results is that
we point out a previously unaccounted for source of vari-
ance when using covariate adjustment to estimate ITE. We
suggest a new type of regularization, by learning represen-
tations with reduced IPM distance between treated and con-
trol, enabling a new type of bias-variance trade-off.

Another widely used family of statistical methods used in
causal effect inference are weighting methods. Methods
such as inverse propensity score weighting (Austin, 2011)
re-weight the units in the observational data so as to make
the treated and control populations more comparable, and
have been used for estimating conditional effects as well
(Cole et al., 2003). The major challenge, especially in
high-dimensional cases, is controlling the variance of the
estimates (Swaminathan & Joachims, 2015). Doubly ro-
bust methods go further and combine propensity score re-
weighting and covariate adjustment in clever ways to re-
duce model bias (Funk et al., 2011).

Adapting machine learning methods for causal effect infer-
ence, and in particular for individual level treatment effect,
has gained much interest recently. For example Wager &
Athey (2015); Athey & Imbens (2016) discuss how tree-
based methods can be adapted to obtain a consistent esti-
mator with semi-parametric asymptotic convergence rate.
Recent work has also looked into how machine learning
methods can help detect heterogeneous treatment effects
when some data from randomized experiments is available
(Taddy et al., 2016; Peysakhovich & Lada, 2016). Neural
nets have also been used for this purpose, exempliﬁed in
early work by Beck et al. (2000), and more recently by
Hartford et al. (2016)’s work on deep instrumental vari-
ables. Our work differs from all the above by focusing
on the generalization-error aspects of estimating individual
treatment effect, as opposed to asymptotic consistency, and
by focusing solely on the observational study case, with no
randomized components or instrumental variables.

Our work has strong connections with work on domain

adaptation. In particular, estimating ITE requires predic-
tion of outcomes over a different distribution from the ob-
served one. Our ITE error upper bound has similarities with
generalization bounds in domain adaptation given by Ben-
David et al. (2007); Mansour et al. (2009); Ben-David et al.
(2010); Cortes & Mohri (2014). These bounds employ dis-
tribution distance metrics such as the A-distance or the dis-
crepancy metric, which are related to the IPM distance we
use. Our algorithm is similar to a recent algorithm for do-
main adaptation by Ganin et al. (2016), and in principle
other domain adaptation methods (e.g. Daum´e III (2007);
Pan et al. (2011); Sun et al. (2016)) could be adapted for
use in ITE estimation as presented here.

Finally, our paper builds on Johansson et al. (2016), where
we showed a connection between covariate shift and the
task of estimating counterfactuals. We proposed learning a
representation of the data that makes the treated and control
distributions more similar, ﬁtting a linear ridge-regression
model on top of it. We bounded the relative error of ﬁtting
a ridge-regression using the distribution with reverse treat-
ment assignment versus ﬁtting a ridge-regression using the
factual distribution. Unfortunately, the relative error bound
is not at all informative regarding the absolute quality of
the representation. In this paper we focus on a related but
more substantive task: estimating the individual treatment
effect, building on the counterfactual error term. We pro-
vide an informative bound on the absolute quality of the
representation. We also derive a much more ﬂexible family
of algorithms, including non-linear hypotheses and much
more powerful distribution metrics in the form of IPMs
such as the Wasserstein and MMD distances. Finally, we
conduct signiﬁcantly more thorough experiments including
a real-world dataset and out-of-sample performance, and
show our methods outperform previously proposed ones.

3. Estimating ITE: Error bounds

In this section we prove a bound on the expected error in
estimating the individual treatment effect for a given repre-
sentation, and a hypothesis deﬁned over that representation.
The bound is expressed in terms of (1) the expected loss
of the model when learning the observed outcomes y as a
function of x and t, denoted (cid:15)F , F standing for “Factual”;
(2) an Integral Probability Metric (IPM) distance between
the distribution of treated and control units. The term (cid:15)F
is the classic machine learning generalization-error, and in
turn can be upper bounded using the empirical error and
model complexity terms, applying standard machine learn-
ing theory (Shalev-Shwartz & Ben-David, 2014).

3.1. Problem setup

We will employ the following assumptions and notations.
The most important notations are in the Notation box in the

Estimating individual treatment effect: generalization bounds and algorithms

supplement. The space of covariates is a bounded subset
X ⊂ Rd. The outcome space is Y ⊂ R. Treatment t is a
binary variable. We assume there exists a joint distribution
p(x, t, Y0, Y1), such that (Y1, Y0) ⊥⊥ t|x and 0 < p(t =
1|x) < 1 for all x ∈ X (strong ignorability). The treated
and control distributions are the distribution of the features
x conditioned on treatment: pt=1(x) := p(x|t = 1), and
pt=0(x) := p(x|t = 0), respectively.

Throughout this paper we will discuss representation func-
tions of the form Φ : X → R, where R is the representa-
tion space. We make the following assumption about Φ:

Assumption 1. The representation Φ is a twice-
differentiable, one-to-one function. Without loss of gener-
ality we will assume that R is the image of X under Φ.
We then have Ψ : R → X as the inverse of Φ, such that
Ψ(Φ(x)) = x for all x ∈ X .

The representation Φ pushes forward the treated and con-
trol distributions into the new space R; we denote the in-
duced distribution by pΦ.
Deﬁnition 1. Deﬁne pt=1
Φ (r) :=
pΦ(r|t = 0), to be the treated and control distributions
induced over R. For a one-to-one Φ, the distributions
Φ (r) and pt=0
pt=1
Φ (r) can be obtained by the standard
change of variables formula, using the determinant of the
Jacobian of Ψ(r).

Φ (r) := pΦ(r|t = 1), pt=0

Let Φ : X → R be a representation function, and h :
R × {0, 1} → Y be an hypothesis deﬁned over the repre-
sentation space R. Let L : Y × Y → R+ be a loss func-
tion. We deﬁne two complimentary loss functions: one is
the standard machine learning loss, which we will call the
factual loss (cid:15)F , as it relates to observable quantities. The
other is the expected loss with respect to the distribution
where the treatment assignment is ﬂipped, which we call
the counterfactual loss, (cid:15)CF .
unit
expected
Deﬁnition
and
=
(x, t)
(cid:82)
Y L(Yt, h(Φ(x), t))p(Yt|x)dYt. The expected factual
and counterfactual losses of h and Φ are:

for
the
(cid:96)h,Φ(x, t)

treatment

2. The

pair

loss

is:

(cid:15)F (h, Φ) =

(cid:96)h,Φ(x, t) p(x, t) dxdt,

(cid:15)CF (h, Φ) =

(cid:96)h,Φ(x, t) p(x, 1 − t) dxdt.

(cid:90)

X ×{0,1}
(cid:90)

X ×{0,1}

If x denotes patients’ features, t a treatment, and Yt a poten-
tial outcome such as mortality, we think of (cid:15)F as measuring
how well do h and Φ predict mortality for the patients and
doctors’ actions sampled from the same distribution as our
data sample. (cid:15)CF measures how well our prediction with
h and Φ would do in a “topsy-turvy” world where the pa-
tients are the same but the doctors are inclined to prescribe

exactly the opposite treatment than the one the real-world
doctors would prescribe.

Deﬁnition 3. The expected factual treated and control
losses are:

(cid:90)

(cid:90)

(cid:15)t=1
F (h, Φ) =

(cid:96)h,Φ(x, 1) pt=1(x) dx,
X

(cid:15)t=0
F (h, Φ) =

(cid:96)h,Φ(x, 0) pt=0(x) dx.
X

For u := p(t = 1), it is immediate to show that (cid:15)F (h, Φ) =
u(cid:15)t=1
F (h, Φ).
Deﬁnition 4. The treatment effect (ITE) for unit x is:

F (h, Φ) + (1 − u)(cid:15)t=0

τ (x) := E [Y1 − Y0|x] .

Let f : X × {0, 1} → Y by an hypothesis. For example,
we could have that f (x, t) = h(Φ(x), t).

Deﬁnition 5. The treatment effect estimate of the hypothe-
sis f for unit x is:

ˆτf (x) = f (x, 1) − f (x, 0).

Deﬁnition 6. The expected Precision in Estimation of Het-
erogeneous Effect (PEHE, Hill (2011)) loss of f is:

(cid:15)PEHE(f ) =

(ˆτf (x) − τ (x))2 p(x) dx,

(1)

(cid:90)

X

When f (x, t) = h(Φ(x), t), we will also use the notation
(cid:15)PEHE(h, Φ) = (cid:15)PEHE(f ).

Our proof relies on the notion of an Integral Probability
Metric (IPM), which is a class of metrics between prob-
ability distributions (Sriperumbudur et al., 2012; M¨uller,
1997). For two probability density functions p, q deﬁned
over S ⊆ Rd, and for a function family G of functions
g : S → R, we have that

IPMG(p, q) := sup
g∈G

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

S

(cid:12)
(cid:12)
g(s)(p(s) − q(s)) ds
(cid:12)
(cid:12)

.

Integral probability metrics are always symmetric and obey
the triangle inequality, and trivially satisfy IPMG(p, p) =
0. For rich enough function families G, we also have that
IPMG(p, q) = 0 =⇒ p = q, and then IPMG is a true
metric. Examples of function families G for which IPMG
is a true metric are the family of bounded continuous func-
tions, the family of 1-Lipschitz functions (Sriperumbudur
et al., 2012), and the unit-ball of functions in a universal
reproducing kernel Hilbert space (Gretton et al., 2012).

3.2. Bounds

We ﬁrst state a Lemma bounding the counterfactual loss, a
key step in obtaining the bound on the error in estimating

Estimating individual treatment effect: generalization bounds and algorithms

individual treatment effect. We then give the main Theo-
rem. The proofs and details are in the supplement.

have been used for various machine learning tasks in recent
years (Gretton et al., 2009; 2012; Cuturi & Doucet, 2014).

Let u := p(t = 1) be the marginal probability of treatment.
By the strong ignorability assumption, 0 < u < 1.

Lemma 1. Let Φ : X → R be a one-to-one representation
function, with inverse Ψ. Let h : R × {0, 1} → Y be an
hypothesis. Let G be a family of functions g : R → Y. As-
sume there exists a constant BΦ > 0, such that for ﬁxed t ∈
{0, 1}, the per-unit expected loss functions (cid:96)h,Φ(Ψ(r), t)
(Deﬁnition 2) obey 1
BΦ

· (cid:96)h,Φ(Ψ(r), t) ∈ G. We have:

(cid:15)CF (h, Φ) ≤
(1 − u)(cid:15)t=1

F (h, Φ) + u(cid:15)t=0
F (h, Φ)
(cid:1) ,
(cid:0)pt=1
Φ , pt=0

Φ

+ BΦ · IPMG

F

F

and (cid:15)t=1

are as in Deﬁnitions 2 and 3.

where (cid:15)CF , (cid:15)t=0
Theorem 1. Under the conditions of Lemma 1, and assum-
ing the loss L used to deﬁne (cid:96)h,Φ in Deﬁnitions 2 and 3 is
the squared loss, we have:

(cid:15)PEHE(h, Φ) ≤
2(cid:0)(cid:15)CF (h, Φ) + (cid:15)F (h, Φ) − 2σ2
2(cid:0)(cid:15)t=0

(cid:1) ≤
F (h, Φ)+BΦIPMG

F (h, Φ)+(cid:15)t=1

Y

(cid:0)pt=1

Φ , pt=0

Φ

(2)
(cid:1),
(cid:1)−2σ2

Y

where (cid:15)F and (cid:15)CF are deﬁned w.r.t. the squared loss, and
σ2
Y is the variance of the outcomes Yt (see Deﬁnition A11
in Appendix for detailed deﬁnition).

The main idea of the proof is showing that (cid:15)PEHE is upper
bounded by the sum of the expected factual loss (cid:15)F and
expected counterfactual loss (cid:15)CF . However, we cannot es-
timate (cid:15)CF , since we only have samples relevant to (cid:15)F . We
therefore bound the difference (cid:15)CF − (cid:15)F using an IPM.

Choosing a small function family G makes the bound
tighter. However, choosing too small a family could re-
sult in an incomputable bound. For example, for the min-
imal choice G = {(cid:96)h,Φ(x, 0), (cid:96)h,Φ(x, 1)}, we will have to
evaluate an expectation term of Y1 over pt=0
Φ , and of Y0
over pt=1
Φ . We cannot in general evaluate these expecta-
tions, since by assumption when t = 0 we only observe
Y0, and the same for t = 1 and Y1. In addition, for some
function families there is no known way to efﬁciently com-
pute the IPM distance or its gradients. Here, we use two
function families for which there are available optimiza-
tion tools. The ﬁrst is the family of 1-Lipschitz functions,
which leads to IPM being the Wasserstein distance (Vil-
lani, 2008), denoted Wass(p, q). The second is the family
of norm-1 reproducing kernel Hilbert space (RKHS) func-
tions, leading to the MMD metric (Gretton et al., 2012), de-
noted MMD(p, q). Both the Wasserstein and MMD metrics
have consistent estimators which can be efﬁciently com-
puted for ﬁnite samples (Sriperumbudur et al., 2012), and

In order to explicitly evaluate the constant BΦ in Theorem
1, we have to make some assumptions about the elements
of the problem. For the Wasserstein case these are the loss
L, the Lipschitz constants of p(Yt|x) and h, and the con-
dition number of the Jacobian of Φ. For the MMD case,
we make assumptions about the RKHS representability and
RKHS norms of h , Φ, and the standard deviation of Yt|x.
The full details are given in the supplement, with the major
results stated in Theorems 2 and 3. In all cases we obtain
that making Φ smaller increases the constant BΦ preclud-
ing trivial solutions such as making Φ arbitrarily small.

F

F

For an empirical sample, and a family of representations
and hypotheses, we can further upper bound (cid:15)t=0
and (cid:15)t=1
by their respective empirical losses and a model complex-
ity term using standard arguments (Shalev-Shwartz & Ben-
David, 2014). The IPMs we use can be consistently es-
timated from ﬁnite samples (Sriperumbudur et al., 2012).
The negative variance term σ2
Y arises from the fact that,
following Hill (2011); Athey & Imbens (2016), we deﬁne
the error (cid:15)PEHE in terms of the conditional mean functions
mt(x), as opposed to ﬁtting the random variables Yt.

Our results hold for any given h and Φ obeying the The-
orem conditions. This immediately suggest an algorithm
in which we minimize the upper bound in Eq. (2) with re-
spect to Φ and h and either the Wasserstein or MMD IPM,
in order to minimize the error in estimating the individual
treatment effect. This leads us to Algorithm 1 below.

4. Algorithm for estimating ITE

We propose a general framework called CFR (for Counter-
factual Regression) for ITE estimation based on the theo-
retical results above. Our algorithm is an end-to-end, regu-
larized minimization procedure which ﬁts both a balanced
representation of the data and a hypothesis for the outcome.
CFR draws on the same intuition as our previous work (Jo-
hansson et al., 2016), but overcomes the following limita-
tions: a) Our previous theory requires a two-step optimiza-
tion procedure and is speciﬁc to linear hypotheses (it does
not support e.g. deep neural networks), b) The treatment in-
dicator might be washed out in the old model, if the learned
representation is high-dimensional (see discussion below).

We assume there exists a distribution p(x, t, Y0, Y1) over
X × {0, 1} × Y × Y, such that strong ignorability holds.
We further assume we have a sample from that distribution
(x1, t1, y1), . . . (xn, tn, yn), where yi ∼ p(Y1|xi) if ti = 1,
yi ∼ p(Y0|xi) if ti = 0. This standard assumption means
that the treatment assignment determines which potential
outcome we see. Our goal is to ﬁnd a representation Φ :
X → R and hypothesis h : X × {0, 1} → Y that will

Estimating individual treatment effect: generalization bounds and algorithms

min
h,Φ
(cid:107)Φ(cid:107)=1

with

and

minimize (cid:15)PEHE(f ) for f (x, t) := h(Φ(x), t).

We parameterize Φ(x) and h(Φ, t) by deep neural networks
trained jointly, see Figure 1. This allows for learning com-
plex non-linear representations and hypotheses with large
In Johansson et al. (2016), we parameterized
ﬂexibility.
h(Φ, t) with a single network, concatenating Φ and t as in-
put. In this case, if Φ is high-dimensional, the inﬂuence of
t on h might be lost during training. To combat this, we pa-
rameterize h1(Φ) = h(Φ, 1) and h0(Φ) = h(Φ, 0) as two
separate “heads” of the joint network, the former used to
estimate the outcome under treatment, and the latter under
control. This way, statistical power is shared in represen-
tation layers, while the effect of treatment is preserved in
the separate heads. Note that each sample is used to update
only the head corresponding to the observed treatment.

Our second contribution is to explicitly adjust for the bias
induced by treatment group imbalance. To this end, we
seek a representation Φ and hypothesis h that minimizes a
trade-off between predictive accuracy and imbalance in the
representation space, using the following objective:

(cid:80)n

1
n

i=1 wi · L (h(Φ(xi), ti) , yi) + λ · R(h)

+α · IPMG ({Φ(xi)}i:ti=0, {Φ(xi)}i:ti=1) ,
2u + 1−ti
wi = ti
2(1−u) , where u = 1
i=1 ti,
R is a model complexity term.

(cid:80)n

n

(3)

Note that u = p(t = 1) is simply the proportion of treated
units in the population. The weights wi compensate for the
difference in treatment group size in our sample, see The-
orem 1. IPMG(·, ·) is the (empirical) integral probability
metric w.r.t. G. For most IPMs, we cannot compute the
factor Bφ in (2), but treat it as part of the hyperparameter
α. This makes our objective sensitive to the scaling of Φ,
even for a constant α. We therefore normalize Φ through
either projection or batch-normalization with ﬁxed scale.

We refer to the model minimizing (3) with α > 0 as Coun-
terfactual Regression (CFR) and the variant without bal-
ance regularization (α = 0) as Treatment-Agnostic Rep-
resentation Network (TARNet). Both models are trained
by minimizing (3) using stochastic gradient descent, as de-
scribed in Algorithm 1. Both the prediction loss and the
penalty term IPMG(·, ·) are computed for one mini-batch
at a time. Details of how to obtain the gradient g1 with
respect to the empirical IPMs are in the supplement.

5. Experiments

Evaluating causal inference algorithms is more difﬁcult
than many machine learning tasks, since we rarely have ac-
cess to the ground truth treatment effect. Existing literature
mostly deals with this in two ways. One is by using (semi-)

Algorithm 1 CFR: Counterfactual regression with integral
probability metrics

1: Input: Factual sample (x1, t1, y1), . . . , (xn, tn, yn),
scaling parameter α > 0, loss function L (·, ·), rep-
resentation network ΦW with initial weights W, out-
come network hV with initial weights V, function
family G for IPM.

2(1−u) for i = 1 . . . n

(cid:80)n
i=1 ti
2u + 1−ti

2: Compute u = 1
n
3: Compute wi = ti
4: while not converged do
5:
6:

7:

8:

=0, {ΦW(xik )}tij
(cid:1)
j wij · L (cid:0)hV(ΦW(xij ), tij ), yij
(cid:80)
j wij · L (cid:0)hV(ΦW(xij ), tij ), yij
(cid:1)
(cid:80)

Sample mini-batch {i1, i2, . . . , im} ⊂ {1, 2, . . . , n}
Calculate the gradient of the IPM term:
g1 =∇W IPMG({ΦW(xij )}tij
Calculate the gradients of the empirical loss:
g2 = ∇V
g3 = ∇W
Obtain step size scalar or matrix η with standard
neural net methods e.g. Adam (Kingma & Ba, 2015)
[W, V] ← [W − η(αg1 + g3), V − η(g2 + 2λV)]
Check convergence criterion

1
m
1
m

=1)

9:
10:
11: end while

synthetic datasets, where the outcome or treatment assign-
ment are fully known; we use the semi-synthetic IHDP
dataset from Hill (2011). The other is using real-world
data from randomized controlled trials (RCT). The problem
with using data from RCTs is that there is no imbalance be-
tween treatment groups, making our method redundant. We
partially overcome this problem by using the Jobs dataset
from LaLonde (1986), which includes both a randomized
and a non-randomized component. We use both compo-
nents for training, but only use the randomized component
for evaluation. This alleviates, but does not solve, the issue
of a completely randomized and balanced dataset being un-
suited for our method.

We evaluate our framework CFR, and its variant with-
out balancing regularization (TARNet), in the task of es-
timating ITE and ATE. Both versions are implemented3
as feed-forward neural networks with 3 fully-connected
exponential-linear layers (Clevert et al., 2016) for the repre-
sentation and 3 for the hypothesis. Layer sizes were 200 for
all layers used for Jobs and 200 and 100 for the represen-
tation and hypothesis used for IHDP. The model is trained
using Adam (Kingma & Ba, 2015). The hypothesis param-
eters are regularized with a small (cid:96)2 weight decay. For con-
tinuous data we use mean squared loss and for binary data,
we use log-loss. While our theory does not immediately
apply to log-loss, we were curious to see how our model
performs with it. We use the Wasserstein (CFRWASS) and
the squared linear MMD (CFRMMD) distances to penalize

3https://github.com/clinicalml/cfrnet

Estimating individual treatment effect: generalization bounds and algorithms

imbalance.

We compare our method to Ordinary Least Squares with
treatment as a feature (OLS1), OLS with separate re-
gressors for each treatment (OLS2), k-nearest neighbor
(k-NN), Targeted Maximum Likelihood (TMLE), which
is a doubly robust method (Gruber & van der Laan,
2011), Bayesian Additive Regression Trees (BART) (Chip-
man et al., 2010; Chipman & McCulloch, 2016), Ran-
dom Forests (R. For.) (Breiman, 2001), Causal Forests
(C. For.) (Wager & Athey, 2015) as well as the Balanc-
ing Linear Regression (BLR) and Balancing Neural Net-
work (BNN) from Johansson et al. (2016). For classiﬁca-
tion tasks we substitute Logistic Regression (LR) for OLS.
Choosing hyperparameters for estimating PEHE is non-
trivial; we detail our general procedure using a validation
set, in subsection C.1 of the supplement.

We consider two different estimation tasks. One is within-
sample, where the task is to estimate ITE for all units in
a sample for which the (factual) outcome of one treatment
is observed. This corresponds to the common scenario in
which a cohort is selected once and not changed. This task
is non-trivial, as we never observe the ITE for any unit. The
other is out-of-sample, where the goal is to estimate ITE for
units with no observed outcomes. This corresponds to the
problem of selecting the best treatment for a new patient.
Within-sample error is computed over both the training and
validation sets, out-of-sample error over the test set.

5.1. Simulated outcome: IHDP

Hill (2011) compiled a dataset for causal effect estima-
tion based on the Infant Health and Development Program
(IHDP), in which the covariates come from a randomized
experiment studying the effects of specialist home visits on
cognitive test scores. The treatment groups have been made
imbalanced by removing a biased subset of the treated pop-
ulation. The dataset comprises 747 units (139 treated, 608
control) and 25 covariates measuring aspects of children
and their mothers. We use the simulated outcome imple-
mented as setting “A” in the NPCI package (Dorie, 2016).
Following Hill (2011), we use the noiseless outcome to
compute the true effect. We report the estimated (ﬁnite-
sample) PEHE loss (cid:15)PEHE (Eq.
1), and the absolute er-
ror in average treatment effect (cid:15)ATE = | 1
i=1(f (xi, 1) −
n
f (xi, 0)) − 1
i=1(m1(xi) − m0(xi))|. The results of
n
the experiments on IHDP are presented in Table 1 (left).
We average over 1000 realizations of the outcomes with
63/27/10 train/validation/test splits.

(cid:80)n

(cid:80)n

We also investigate the effects of increasing imbalance be-
tween the original treatment groups by constructing bi-
ased subsamples of the IHDP dataset. A logistic-regression
propensity score model is ﬁt to form estimates ˆp(t = 1|x)
of the conditional treatment probability. Then, repeatedly,

Figure 2. Out-of-sample ITE error versus IPM regularization for
CFR Wass, relative to the error at α = 0, on 500 realizations of
IHDP, with high (q = 1), medium and low (artiﬁcial) imbalance
between control and treated.

Figure 3. Policy risk on Jobs as a function of treatment inclusion
rate. Lower is better. Subjects are included in treatment in order
of their estimated treatment effect given by the various methods.
CFR Wass is similar to TARNet and is omitted to avoid clutter.

with probability q we remove the remaining control obser-
vation x that has ˆp(t = 1|x) closest to 1, and with proba-
bility 1 − q, we remove a random control observation. The
higher q, the more imbalance. For each value of q, we re-
move 347 observations from each set, leaving 400.

5.2. Real-world outcome: Jobs

The study by LaLonde (1986) is a widely used benchmark
in the causal inference community, where the treatment is
job training and the outcomes are income and employment
status after training. This dataset combines a randomized
study based on the National Supported Work program with
observational data to form a larger dataset (Smith & Todd,
2005). The presence of the randomized subgroup gives a
way to estimate the “ground truth” causal effect. The study
includes 8 covariates such as age and education, as well
as previous earnings. We construct a binary classiﬁcation
task, called Jobs, where the goal is to predict unemploy-
ment, using the feature set of Dehejia & Wahba (2002).
Following Smith & Todd (2005), we use the LaLonde ex-
perimental sample (297 treated, 425 control) and the PSID
comparison group (2490 control). There were 482 (15%)
subjects unemployed by the end of the study. We average

010-510-410-310-210-1100101Imbalancepenalty;®0.70.80.91.01.11.21.31.41.5²PEHErelativeto®=0q=0:0q=0:5q=1:00.00.20.40.60.81.0Treatmentinclusionrate0.160.180.200.220.240.260.280.300.32OutofsamplepolicyriskBARTCausalForestsCFRMMDTARNetRandompolicyEstimating individual treatment effect: generalization bounds and algorithms

Table 1. Results on IHDP and Jobs within-sample (left) and out-of-sample (right). Lower is better. †Not applicable.

Within-sample

Out-of-sample

IHDP

JOBS

IHDP

JOBS

√

(cid:15)PEHE
5.8 ± .3
2.4 ± .1
5.8 ± .3
2.1 ± .1
5.0 ± .2
2.1 ± .1
4.2 ± .2
3.8 ± .2
2.2 ± .1
.88 ± .02
.73 ± .01
.71 ± .02

OLS/LR1
OLS/LR2
BLR
k-NN
TMLE
BART
R.FOR.
C.FOR.
BNN
TARNET
CFRMMD
CFRWASS

(cid:15)ATE
.73 ± .04
.14 ± .01
.72 ± .04
.14 ± .01
.30 ± .01
.23 ± .01
.73 ± .05
.18 ± .01
.37 ± .03
.26 ± .01
.30 ± .01
.25 ± .01

RPOL
.22 ± .00
.21 ± .00
.22 ± .01
.02 ± .00
.22 ± .00
.23 ± .00
.23 ± .01
.19 ± .00
.20 ± .01
.17 ± .01
.18 ± .00
.17 ± .01

(cid:15)ATT
.01 ± .00
.01 ± .01
.01 ± .01
.21 ± .01
.02 ± .01
.02 ± .00
.03 ± .01
.03 ± .01
.04 ± .01
.05 ± .02
.04 ± .01
.04 ± .01

OLS/LR1
OLS/LR2
BLR
k-NN
TMLE
BART
R.FOR.
C.FOR.
BNN
TARNET
CFRMMD
CFRWASS

√

(cid:15)PEHE
5.8 ± .3
2.5 ± .1
5.8 ± .3
4.1 ± .2
†
2.3 ± .1
6.6 ± .3
3.8 ± .2
2.1 ± .1
.95 ± .02
.78 ± .02
.76 ± .02

(cid:15)ATE
.94 ± .06
.31 ± .02
.93 ± .05
.79 ± .05
†
.34 ± .02
.96 ± .06
.40 ± .03
.42 ± .03
.28 ± .01
.31 ± .01
.27 ± .01

RPOL
.23 ± .02
.24 ± .01
.25 ± .02
.26 ± .02
†
.25 ± .02
.28 ± .02
.20 ± .02
.24 ± .02
.21 ± .01
.21 ± .01
.21 ± .01

(cid:15)ATT
.08 ± .04
.08 ± .03
.08 ± .03
.13 ± .05
†
.08 ± .03
.09 ± .04
.07 ± .03
.09 ± .04
.11 ± .04
.08 ± .03
.09 ± .03

over 10 train/validation/test splits with ratios 56/24/20.

(cid:80)

Because all the treated subjects T were part of the original
randomized sample E, we can compute the true average
treatment effect on the treated by ATT = |T |−1 (cid:80)
i∈T yi −
|C ∩ E|−1 (cid:80)
i∈C∩E yi, where C is the control group. We
report the error (cid:15)ATT = |ATT − 1
i∈T (f (xi, 1) −
|T |
f (xi, 0))|. We cannot evaluate (cid:15)PEHE on this dataset, since
there is no ground truth for the ITE. Instead, in order
to evaluate the quality of ITE estimation, we use a mea-
sure we call policy risk. The policy risk is deﬁned as
the average loss in value when treating according to the
policy implied by an ITE estimator.
In our case, for a
model f , we let the policy be to treat, πf (x) = 1, if
f (x, 1) − f (x, 0) > λ, and to not treat, πf (x) = 0 oth-
erwise. The policy risk is RPol(πf ) = 1 − (E[Y1|πf (x) =
1] · p(πf = 1) + E[Y0|πf (x) = 0] · p(πf = 0)) which
we can estimate for the randomized trial subset of Jobs
by ˆRPol(πf = 1 − (E[Y1|πf (x) = 1, t = 1] · p(πf =
1) + E[Y0|πf (x) = 0, t = 0] · p(πf = 0)). See ﬁgure 3
for risk as a function of treatment threshold λ, aligned by
proportion of treated, and Table 1 for the risk when λ = 0.

5.3. Results

We note that indeed imbalance confers an advantage to us-
ing the IPM regularization term, as our theoretical results
the results for CFRWASS and TARNet
indicate, see e.g.
on IHDP in Table 1. We also see in Figure 2 that even
for the harder case of increased imbalance (q > 0) be-
tween treated and control, the relative gain from using our
method remains signiﬁcant. On Jobs, our proposed meth-
ods are better than or competitive with state-of-the-art, but
we don’t see a signiﬁcant gain from using IPM penalties.
This might be because we evaluate the predictions only on a
randomized subset with treatment groups distributed iden-
tically. Non-linear estimators perform signiﬁcantly better
than linear ones in terms of individual effect ((cid:15)PEHE). On
the Jobs dataset, straightforward logistic regression does

remarkably well in estimating the ATT. However, being a
linear model, LR can only ascribe a uniform policy - in
this case, “treat everyone”. The more nuanced policies
offered by non-linear methods achieve lower policy risk
in the case of Causal Forests and CFR. This emphasizes
the fact that estimating average effect and individual effect
can require different models. Speciﬁcally, while smoothing
over many units may yield a good ATE estimate, this might
signiﬁcantly hurt ITE estimation. k-nearest neighbors has
very good within-sample results on Jobs, because evalua-
tion is performed over the randomized component, but suf-
fers heavily in generalizing out of sample, as expected.

6. Conclusion

In this paper we give a meaningful and intuitive error-
bound for estimating individual treatment effect. Our
bound relates ITE estimation to the classic machine learn-
ing problem of learning from samples, along with methods
for measuring distributional distances from samples. The
bound lends itself naturally to the creation of learning al-
gorithms; we focus on using neural nets as representations
and hypotheses. We apply our theory-guided approach to
both synthetic and real-world tasks, showing that in every
case our method matches or outperforms the state-of-the-
art. Important open questions are theoretical considerations
in choosing the IPM weight α, how to best derive conﬁ-
dence intervals for our model’s predictions, and integrat-
ing our work with more complicated causal models such as
those with hidden confounding or instrumental variables.

ACKNOWLEDGMENTS

We thank Aahlad Puli for his assistance with the experi-
ments; Sanjog Misra and G¨unter J. Hitsch for suggesting
the policy risk evaluation; Jennifer Hill, Marco Cuturi and
Esteban Tabak for fruitful conversations; and Stefan Wager
for his help with the code for Causal Forests. DS and US
were supported by NSF CAREER award #1350965.

Estimating individual treatment effect: generalization bounds and algorithms

References

Athey, Susan and Imbens, Guido. Recursive partitioning for
heterogeneous causal effects. Proceedings of the National
Academy of Sciences, 113(27):7353–7360, 2016.

Athey, Susan, Imbens, Guido W, and Wager, Stefan.

Efﬁ-
cient inference of average treatment effects in high dimen-
arXiv preprint
sions via approximate residual balancing.
arXiv:1604.07125, 2016.

Austin, Peter C. An introduction to propensity score methods for
reducing the effects of confounding in observational studies.
Multivariate behavioral research, 46(3):399–424, 2011.

Bareinboim, Elias and Pearl, Judea. Causal inference and the
data-fusion problem. Proceedings of the National Academy
of Sciences, 113(27):7345–7352, 2016.

Beck, Nathaniel, King, Gary, and Zeng, Langche.

Improving
quantitative studies of international conﬂict: A conjecture.
American Political Science Review, 94(01):21–35, 2000.

Belloni, Alexandre, Chernozhukov, Victor, and Hansen, Chris-
tian. Inference on treatment effects after selection among high-
dimensional controls. The Review of Economic Studies, 81(2):
608–650, 2014.

Ben-David, Shai, Blitzer, John, Crammer, Koby, Pereira, Fer-
nando, et al. Analysis of representations for domain adapta-
tion. Advances in neural information processing systems, 19:
137, 2007.

Ben-David, Shai, Blitzer, John, Crammer, Koby, Kulesza, Alex,
Pereira, Fernando, and Vaughan, Jennifer Wortman. A theory
of learning from different domains. Machine learning, 79(1-2):
151–175, 2010.

Bengio, Yoshua, Courville, Aaron, and Vincent, Pierre. Repre-
sentation learning: A review and new perspectives. Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 35
(8):1798–1828, 2013.

Breiman, Leo. Random forests. Machine learning, 45(1):5–32,

2001.

Chernozhukov, Victor, Chetverikov, Denis, Demirer, Mert, Du-
ﬂo, Esther, Hansen, Christian, et al. Double machine learn-
arXiv preprint
ing for treatment and causal parameters.
arXiv:1608.00060, 2016.

immunodeﬁciency syndrome or death using marginal struc-
tural models. American Journal of Epidemiology, 158(7):687–
694, 2003.

Cortes, Corinna and Mohri, Mehryar. Domain adaptation and
sample bias correction theory and algorithm for regression.
Theoretical Computer Science, 519:103–126, 2014.

Cuturi, Marco and Doucet, Arnaud. Fast computation of Wasser-
In Proceedings of The 31st International

stein barycenters.
Conference on Machine Learning, pp. 685–693, 2014.

Daum´e III, Hal. Frustratingly easy domain adaptation. Confer-
ence of the Association for Computational Linguistics (ACL),
2007.

Dehejia, Rajeev H and Wahba, Sadek. Propensity score-matching
methods for nonexperimental causal studies. Review of Eco-
nomics and statistics, 84(1):151–161, 2002.

Dorie, Vincent. NPCI: Non-parametrics for Causal Inference.

https://github.com/vdorie/npci, 2016.

Funk, Michele Jonsson, Westreich, Daniel, Wiesen, Chris,
St¨urmer, Til, Brookhart, M Alan, and Davidian, Marie. Dou-
bly robust estimation of causal effects. American journal of
epidemiology, 173(7):761–767, 2011.

Ganin, Yaroslav, Ustinova, Evgeniya, Ajakan, Hana, Germain,
Pascal, Larochelle, Hugo, Laviolette, Franc¸ois, Marchand,
Mario, and Lempitsky, Victor. Domain-adversarial training
of neural networks. Journal of Machine Learning Research,
17(59):1–35, 2016. URL http://jmlr.org/papers/
v17/15-239.html.

Gretton, Arthur, Smola, Alex, Huang, Jiayuan, Schmittfull, Mar-
cel, Borgwardt, Karsten, and Sch¨olkopf, Bernhard. Covariate
shift by kernel mean matching. Dataset shift in machine learn-
ing, 3(4):5, 2009.

Gretton, Arthur, Borgwardt, Karsten M., Rasch, Malte J.,
Sch¨olkopf, Bernhard, and Smola, Alexander. A kernel two-
sample test. J. Mach. Learn. Res., 13:723–773, March 2012.
ISSN 1532-4435.

Gruber, Susan and van der Laan, Mark J. tmle: An r package for

targeted maximum likelihood estimation. 2011.

Hartford, Jason, Lewis, Greg, Leyton-Brown, Kevin, and Taddy,
Matt. Counterfactual prediction with deep instrumental vari-
ables networks. arXiv preprint arXiv:1612.09596, 2016.

Chipman, Hugh and McCulloch, Robert. BayesTree: Bayesian
Additive Regression Trees. https://cran.r-project.
org/web/packages/BayesTree, 2016.

Hill, Jennifer L. Bayesian nonparametric modeling for causal in-
ference. Journal of Computational and Graphical Statistics,
20(1), 2011.

Chipman, Hugh A, George, Edward I, and McCulloch, Robert E.
BART: Bayesian additive regression trees. The Annals of Ap-
plied Statistics, pp. 266–298, 2010.

Clevert, Djork-Arn´e, Unterthiner, Thomas, and Hochreiter, Sepp.
Fast and accurate deep network learning by exponential linear
units (elus). International Conference on Learning Represen-
tations, 2016.

Hoyer, Patrik O, Janzing, Dominik, Mooij, Joris M, Peters, Jonas,
and Sch¨olkopf, Bernhard. Nonlinear causal discovery with ad-
ditive noise models. In Advances in neural information pro-
cessing systems, pp. 689–696, 2009.

Imbens, Guido W and Wooldridge, Jeffrey M. Recent develop-
ments in the econometrics of program evaluation. Journal of
economic literature, 47(1):5–86, 2009.

Cole, Stephen R, Hern´an, Miguel A, Robins, James M, Anastos,
Kathryn, Chmiel, Joan, Detels, Roger, Ervin, Carolyn, Feld-
man, Joseph, Greenblatt, Ruth, Kingsley, Lawrence, et al. Ef-
fect of highly active antiretroviral therapy on time to acquired

Johansson, Fredrik D., Shalit, Uri, and Sontag, David. Learn-
ing representations for counterfactual inference. In Proceed-
ings of the 33rd International Conference on Machine Learn-
ing (ICML), 2016.

Estimating individual treatment effect: generalization bounds and algorithms

Kingma, Diederik and Ba, Jimmy. Adam: A method for stochas-
tic optimization. International Conference on Learning Repre-
sentations, 2015.

Strehl, Alex, Langford, John, Li, Lihong, and Kakade, Sham M.
Learning from logged implicit exploration data. In Advances in
Neural Information Processing Systems, pp. 2217–2225, 2010.

LaLonde, Robert J. Evaluating the econometric evaluations of
training programs with experimental data. The American eco-
nomic review, pp. 604–620, 1986.

Sun, Baochen, Feng, Jiashi, and Saenko, Kate. Return of frustrat-
ingly easy domain adaptation. In Thirtieth AAAI Conference
on Artiﬁcial Intelligence, 2016.

Maathuis, Marloes H, Colombo, Diego, Kalisch, Markus, and
B¨uhlmann, Peter. Predicting causal effects in large-scale sys-
tems from observational data. Nature Methods, 7(4):247–248,
2010.

Swaminathan, Adith and Joachims, Thorsten. Batch learning
from logged bandit feedback through counterfactual risk min-
imization. Journal of Machine Learning Research, 16:1731–
1755, 2015.

Taddy, Matt, Gardner, Matt, Chen, Liyun, and Draper, David. A
nonparametric bayesian analysis of heterogenous treatment ef-
fects in digital experimentation. Journal of Business & Eco-
nomic Statistics, 34(4):661–672, 2016.

Triantaﬁllou, Soﬁa and Tsamardinos, Ioannis. Constraint-based
causal discovery from multiple interventions over overlapping
Journal of Machine Learning Research, 16:
variable sets.
2147–2205, 2015.

Villani, C´edric. Optimal transport: old and new, volume 338.

Springer Science & Business Media, 2008.

Wager, Stefan and Athey, Susan.

Estimation and in-
ference of heterogeneous treatment effects using random
arXiv preprint arXiv:1510.04342. https://
forests.
github.com/susanathey/causalTree, 2015.

Mansour, Yishay, Mohri, Mehryar, and Rostamizadeh, Afshin.
Domain adaptation: Learning bounds and algorithms. 2009.

Mooij, Joris M, Peters, Jonas, Janzing, Dominik, Zscheischler,
Jakob, and Sch¨olkopf, Bernhard. Distinguishing cause from ef-
fect using observational data: methods and benchmarks. Jour-
nal of Machine Learning Research, 17(32):1–102, 2016.

M¨uller, Alfred. Integral probability metrics and their generating
classes of functions. Advances in Applied Probability, pp. 429–
443, 1997.

Pan, Sinno Jialin, Tsang, Ivor W, Kwok, James T, and Yang,
Qiang. Domain adaptation via transfer component analysis.
Neural Networks, IEEE Transactions on, 22(2):199–210, 2011.

Pearl, Judea. Causality. Cambridge university press, 2009.

Pearl, Judea. Detecting latent heterogeneity. Sociological Meth-

ods & Research, pp. 0049124115600597, 2015.

Peysakhovich, Alexander and Lada, Akos. Combining observa-
tional and experimental data to ﬁnd heterogeneous treatment
effects. arXiv preprint arXiv:1611.02385, 2016.

Robins, James. A new approach to causal inference in mortality
studies with a sustained exposure periodapplication to control
of the healthy worker survivor effect. Mathematical Modelling,
7(9-12):1393–1512, 1986.

Rolling, Craig Anthony. Estimation of Conditional Average Treat-

ment Effects. PhD thesis, University of Minnesota, 2014.

Rubin, Donald B. Causal inference using potential outcomes.

Journal of the American Statistical Association, 2011.

Shalev-Shwartz, Shai and Ben-David, Shai. Understanding ma-
chine learning: From theory to algorithms. Cambridge Uni-
versity Press, 2014.

Shpitser, Ilya and Pearl, Judea. Identiﬁcation of conditional inter-
ventional distributions. In Proceedings of the Twenty-second
Conference on Uncertainty in Artiﬁcial Intelligence, pp. 437–
444. UAI Press, 2006.

Smith, Jeffrey A and Todd, Petra E. Does matching overcome
LaLonde’s critique of nonexperimental estimators? Journal of
econometrics, 125(1):305–353, 2005.

Sriperumbudur, Bharath K, Fukumizu, Kenji, Gretton, Arthur,
Sch¨olkopf, Bernhard, Lanckriet, Gert RG, et al. On the em-
pirical estimation of integral probability metrics. Electronic
Journal of Statistics, 6:1550–1599, 2012.

