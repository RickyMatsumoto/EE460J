GSOS: Gauss-Seidel Operator Splitting Algorithm for
Multi-Term Nonsmooth Convex Composite Optimization

Li Shen 1 Wei Liu 1 Ganzhao Yuan 2 Shiqian Ma 3

Abstract
In this paper, we propose a fast Gauss-Seidel
Operator Splitting (GSOS) algorithm for ad-
dressing multi-term nonsmooth convex compos-
ite optimization, which has wide applications in
machine learning, signal processing and statistic-
s. The proposed GSOS algorithm inherits the ad-
vantage of the Gauss-Seidel technique to acceler-
ate the optimization procedure, and leverages the
operator splitting technique to reduce the com-
putational complexity. In addition, we develop
a new technique to establish the global conver-
gence of the GSOS algorithm. To be speciﬁc, we
ﬁrst reformulate the iterations of GSOS as a two-
step iterations algorithm by employing the tool of
operator optimization theory. Subsequently, we
establish the convergence of GSOS based on the
two-step iterations algorithm reformulation. At
last, we apply the proposed GSOS algorithm to
solve overlapping group Lasso and graph-guided
fused Lasso problems. Numerical experiments
show that our proposed GSOS algorithm is supe-
rior to the state-of-the-art algorithms in terms of
both efﬁciency and effectiveness.

1. Introduction

In this paper, we focus on the multi-term nonsmooth con-
vex composite optimization

min
x∈X

f (x) +

gi(x),

n
(cid:88)

i=1

(1)

where X is a linear space, gi
: X → (−∞, +∞] is
a proper, lower semicontinuous convex function for al-
l i = 1, · · · , n, and f : X → (−∞, +∞) is a continuous

1Tencent AI Lab, China 2Sun Yat-sen University, China
3The Chinese University of Hong Kong, China. Correspon-
dence to: Li Shen <mathshenli@gmail.com>, Wei Liu <wli-
u@ee.columbia.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

differentiable convex function with its gradient satisfying
the inequality that

(cid:13)∇f (x) − ∇f (y)(cid:13)
(cid:13)
2
(cid:13)

≤ (cid:10)∇f (x) − ∇f (y), x − y(cid:11). (2)

1
L

The above multi-term nonsmooth convex composite opti-
mization problem (1) covers a large class of applications
in machine learning such as simultaneous low-rank and s-
parsity (Richard et al., 2012; Zhou et al., 2013), overlap-
ping group Lasso (Zhao et al., 2009; Jacob et al., 2009;
Mairal et al., 2010), graph-guided fused Lasso (Chen et al.,
2012; Kim & Xing, 2009), graph-guided logistic regres-
sion (Chen et al., 2011; Zhong & Kwok, 2014), variation-
al image restoration (Combettes & Pesquet, 2011; Dup´e
et al., 2009; Pustelnik et al., 2011), and other types of struc-
ture regularization paradigms (Teo et al., 2010; 2007). By
introducing the multi-term nonsmooth regularization term
(cid:80)n
i=1 gi(x) such as structured sparsity (Huang et al., 2011;
Bach et al., 2012; Bach, 2010) and nonnegativity (Chen &
Plemmons, 2015; Xu & Yin, 2013), more prior informa-
tion can be included to enhance the accuracy of regulariza-
tion models. However, due to the multi-term nonsmooth
regularization term (cid:80)n
i=1 gi(x), the optimization problem
(1) is too complicated to be solved even for small n. For
n ≤ 2, some existing popular ﬁrst-order optimization
methods are accelerated proximal gradient method (Beck
& Teboulle, 2009; Nesterov, 2007), smoothing accelerat-
ed proximal gradient method (Nesterov, 2005a;b), three
operator splitting method (Davis & Yin, 2015), and some
primal-dual operator splitting methods such as majorized
alternating direction method of multiplier (ADMM) (Cui
et al., 2016; Lin et al., 2011), fast proximity method (Li &
Zhang, 2016), and so on.

On the other hand, when n ≥ 3, there also exist some al-
gorithms for solving problem (1). A directly method for
(1) is smoothing accelerated proximal gradient (S-APG)
proposed by Nesterov (Nesterov, 2005a;b). Then, Yu (Yu,
2013) proposed a new approximation method called PA-
APG for handling (1) by combining the proximal average
approximation technique and Nesterov’s acceleration tech-
nique, which has been enhanced very recently by Shen
et al. (Shen et al., 2017). Their proposed method called
APA-APG adopts an adaptive stepsize strategy. However,

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

the above mentioned methods S-APG, PA-APG and its en-
hanced version APA-APG all need a strict restriction on the
nonsmooth functions {gi(x)} that each gi(x) must be Lip-
schitz continuous. In addition, some primal-dual parallel
splitting methods (Briceno-Arias et al., 2011; Combettes
& Pesquet, 2007; 2008; Condat, 2013; V˜u, 2013) gener-
alized from traditional operator splitting, such as forward
backward splitting method (Chen & Rockafellar, 1997) and
Douglas Rachford splitting method (Eckstein & Bertsekas,
1992), can also solve the multi-term nonsmooth convex
composite optimization problem (1). Different from pri-
or work, Raguet et al. (Raguet et al., 2013) proposed an
efﬁcient primal operator splitting method called general-
ized forward backward splitting method using the classic
forward backward splitting technique, which has shown
the superiority over numerous existing primal-dual splitting
methods (Monteiro & Svaiter, 2013; Combettes & Pesquet,
2012; Chambolle & Pock, 2011) in dealing with variation-
al image restoration problems. All the above mentioned
methods for problem (1) with n ≥ 3 share a common
feature that they all split the nonsmooth composite term
(cid:80)n
i=1 gi(x) in the Jacobi iteration manner, i.e., parallelly.
This is one of the main differences between existing split-
ting methods and our proposed method in this paper.
To split the nonsmooth composite term (cid:80)n
i=1 gi(x) more
efﬁciently, we propose a novel operator splitting algorithm
to solve problem (1) by harnessing the advantage of Gauss-
Seidel iterations,
the computation of the proximal
mapping of the current function gi(x) uses the proximal
mappings of gj(x) for all j < i which have already
been computed ahead. In addition, to further improve the
algorithm’s efﬁciency, we leverage the over-relaxation
acceleration technique. What’s more, we provide a new
strategy that the over-relaxation stepsize can be determined
adaptively, ensuring a larger value to accelerate the algo-
rithm. The most important is that the convergence of our
proposed GSOS algorithm is established by a newly devel-
oped analysis technique. In detail, given an invertible linear
operator R, we ﬁrst argue that the optimal solution set
[∇f + (cid:80)n
i=1 ∂gi]−1 (0) of problem (1) can be recovered
by the zero point set (cid:2)(R∗)−1SR, ∂g+A◦∇f ◦A, NV
(0).
This is fulﬁlled through adopting the tool of operator
in which the composite operator
optimization theory,
SR, ∂g+A◦∇f ◦A, NV is generalized from the deﬁnition of
the composite monotone operator Sλ,A,B in (Eckstein
& Bertsekas, 1992). Next, by unitizing the deﬁnition of
the (cid:15)-enlargement of maximal monotone (Burachik et al.,
1998; 1997; Burachik & Svaiter, 1999; Svaiter, 2000),
we establish a key property for SR, ∂g+A◦∇f ◦A, NV ,
⊆
that
gph (cid:0)R∗[(R∗)−1SR, ∂g+A∗◦∇f ◦A, NV ][(cid:15)](cid:1).
Based on
this observation, we equivalently reformulate the GSOS
algorithm as a two-step iterations algorithm. Then, the

gph (cid:0)SR, (∂g+A∗◦∇f ◦A)[(cid:15)], NV

i.e.,

(cid:3)−1

is,

(cid:1)

global convergence of the proposed GSOS algorithm is
easily established based on this reformulation.

The closest algorithm to our proposed GSOS algorithm
is the generalized forward backward splitting method pro-
posed by Raguet et al. (Raguet et al., 2013). By carefully
selecting the scaling matrix H in the forthcoming GSOS
algorithm, it is easy to check that GSOS covers the gener-
alized forward backward splitting method as a special case.
Another highly related algorithm to our proposed GSOS al-
gorithm is the matrix splitting method (Luo & Tseng, 1991;
Yuan et al., 2016). Choosing the scaling matrix H suitably,
the proposed GSOS algorithm can inherit the advantage of
the matrix splitting technique which has shown the efﬁcien-
cy in (Yuan et al., 2016) for coping with a special class of
coordinate separable composite optimization problems.

The rest of this paper is organized as follows. In Section 2,
we ﬁrst give the deﬁnitions of some useful notations which
can make the paper much more readable. We also establish
some lemmas and propositions based on monotone opera-
tor theory (Bauschke & Combettes, 2011), which are the
key to the convergence of the GSOS algorithm. In Section
3, we present the proposed GSOS algorithm and then an-
alyze its convergence and iteration complexity. In Section
4, we conduct numerical experiments on overlapping group
Lasso and graph-guided fused Lasso problems to evaluate
the efﬁcacy of the GSOS algorithm. Finally, we draw con-
clusions in Section 5.

2. Preliminaries and Notations
Let Y = (cid:81)n
i=1 Xi be the product space of Xi with Xi = X
for all i ∈ {1, 2, · · · , n}. Let V be a linear space and V ⊥
be its complementary space with the following deﬁnitions

V = (cid:8)y ∈ Y | y1 = · · · = yn

(cid:9), V ⊥ = (cid:8)y ∈ Y |

yi = 0(cid:9).

n
(cid:88)

i

(cid:1)∗

· · ·

IX

Let IX : X → X be the identity map and EY :
X → Y be a block linear operator deﬁned as EY =
(cid:0) IX
. Let A : Y → X be a linear oper-
(cid:80)n
ator deﬁned as Ay = 1
n E ∗
i=1 yi. Hence, its
adjoint operator A∗ : X → Y is deﬁned as A∗x = 1
n EY x.
Let H, R : Y → Y be block lower triangular linear invert-
ible operators satisfying (R∗)−1 = H and H + H∗ (cid:31) 0.
Moreover, H is deﬁned as

Y y = 1
n







H1,1
...
Hn−1,1
Hn,1

· · ·
...

0
. . .
· · · Hn−1,n−1
· · ·

Hn−1,n Hn,n

0
...
0







,

(3)

where Hi,j : X → X is a linear operator for all (i, j) ∈
{1, · · · , n}. It is worthwhile to emphasize that Hi,i is also
possible to be a lower triangular linear operator satisfying

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

i,i (cid:31) 0. Next, we abuse the notation (cid:107) · (cid:107)H which

Hi,i + H∗
is induced by the inner product (cid:104)·, H·(cid:105) satisfying
(cid:107) · (cid:107)H : = (cid:112)(cid:104)·, H·(cid:105) = (cid:112)(cid:104)·, H∗·(cid:105)

with (cid:15) = L
inequalities (6)-(7), it holds that

4 (cid:107)Ax1−Ax2(cid:107)2. In addition, if f further satisﬁes

(A∗ ◦ ∇f ◦ A)(x2) ∈ (A∗ ◦ ∇f ◦ A)[(cid:15)](x1)

(9)

(cid:114)

=

(cid:104)·,

H + H∗
2

·(cid:105) = (cid:107) · (cid:107) H+H∗

.

(4)

2

with (cid:15) = 1

4 (cid:107)Ax1 − Ax2(cid:107)2

.

2(cid:98)Σ−Σ

In addition, we deﬁne the generalized proximal mapping of
a proper, lower semicontinuous convex function gi(x) with
respect to the invertible linear operator Hi,i.

i,i gi

Deﬁnition 1 For a given x, the proximal mapping denoted
(x) of a proper, lower semicontinuous con-
by ProxH−1
vex function gi with respect to an invertible linear operator
Hi,i satisfying Hi,i + H∗
i,i (cid:31) 0 is deﬁned to be the zero
point of the following inclusion equation

0 ∈ ∂gi(·) + Hi,i(· − x).

(5)

Moreover, if Hi,i is symmetric, it can be reformulated as
the following convex minimization

ProxHi,igi(x) := arg min
y∈X

gi(y) +

1
2

(cid:107)y − x(cid:107)2

.

Hi,i

Next, we recall the deﬁnition of (cid:15)-enlargement of monotone
operators (Burachik et al., 1998; 1997; Burachik & Svaiter,
1999; Svaiter, 2000), which is an effective tool for estab-
lishing the convergence of the proposed GSOS algorithm.

Deﬁnition 2 Given a maximal monotone operator T :
X ⇒ X, the (cid:15) (≥ 0)-enlargement of T is deﬁned as the
set T [(cid:15)](x) := (cid:8)v ∈ Y | (cid:104)w − v, z − x(cid:105) ≥ −(cid:15) for all z ∈
X, w ∈ T (z)(cid:9).

Recall that f (x) is a gradient Lipschitz convex function sat-
isfying inequality (2). There exits 0 (cid:22) Σ (cid:22) (cid:98)Σ (cid:22) LI such
that the following two inequalities hold for any x, x(cid:48) ∈ X

f (x) ≤ f (x(cid:48)) + (cid:104)∇f (x(cid:48)), x − x(cid:48)(cid:105) +

f (x) ≥ f (x(cid:48)) + (cid:104)∇f (x(cid:48)), x − x(cid:48)(cid:105) +

1
2
1
2

(cid:107)x − x(cid:48)(cid:107)2
(cid:98)Σ

,

(cid:107)x − x(cid:48)(cid:107)2
Σ.

(6)

(7)

Actually, when f (x) is a quadratic function, it holds Σ = (cid:98)Σ
directly in inequalities (6) and (7). The following lemma
establishes the property of the enlargement of the compos-
ite operator A∗ ◦ ∇f ◦ A with f satisfying inequalities (6)-
(7) or (2), which is an essential ingredient for reformulating
the GSOS algorithm as a two-step iterations algorithm.

Proposition 1 Assume that f is a gradient Lipschitz con-
tinuous convex function satisfying inequality (2). For any
x1, x2 ∈ Y, it holds that

(A∗ ◦ ∇f ◦ A)(x2) ∈ (A∗ ◦ ∇f ◦ A)[(cid:15)](x1)

(8)

Remark 1 Two comments are made for Proposition 1:

(1) This proposition gives two types of estimations for (cid:15) in
in (8) and (9). When f is a quadratic

(cid:0)A∗ ◦ ∇f ◦ A(cid:1)[(cid:15)]
function, it is easy to check that

1
4

(cid:107)Ax1 − Ax2(cid:107)2

≤

(cid:107)Ax1 − Ax2(cid:107)2

2(cid:98)Σ−Σ

L
4

due to (cid:98)Σ = Σ (cid:22) LI. When f is a general gradient
Lipschitz continuous function, we do not know which
estimation for (cid:15) is tighter in (8) and (9).

(2) The second part of this proposition can be regarded as
an intensiﬁed version of Lemma 2.2 in (Svaiter, 2014)
for a speciﬁed composite operator A∗ ◦ ∇f ◦ A. The
ﬁrst part of the proposition coincides with the results
by applying Lemma 2.2 in (Svaiter, 2014) for A∗ ◦
∇f ◦ A.

Next, we generalize the notation Sλ,T1,T2 in (Eckstein &
Bertsekas, 1992) for a given λ > 0 and two maximal mono-
tone operators T1, T2 as SR,T1,T2 for a given invertible lin-
ear operator R deﬁned as

(10)

(cid:111)
.

gph SR, T1, T2

(cid:110)

:=

(x1 + Ry2, x2 − x1) | y1 ∈ T1(x1),

y2 ∈ T2(x2), x1 + R∗y1 = x2 − R∗y2

By (Eckstein & Bertsekas, 1992), we know that Sλ,T1,T2 is
maximal monotone if T1 and T2 are both maximal mono-
tone. However, its generalized operator SR,T1,T2 is not
monotone unless the invertible linear operator R reduces
to be a constant. Very interesting, it can be shown that its
composition with (R∗)−1, i.e., (R∗)−1SR,T1,T2, is maxi-
mal monotone for any invertible linear operator R.

Lemma 1 For any given invertible linear operator R, op-
erator (R∗)−1SR,T1,T2 is maximal monotone if T1 and T2
are both maximal monotone operators.

Setting T1 = ∂g + A∗ ◦ ∇f ◦ A, T2 = NV , we obtain
SR, ∂g+A∗◦∇f ◦A, NV , which is deﬁned as

gph (cid:0)SR,∂g+A∗◦∇f ◦A, NV
(cid:110)
:=

(cid:1)

(x1 +Ry2, x2 −x1) | y1 ∈ (∂g + A∗ ◦ ∇f ◦ A)(x1),
(cid:111)
.

y2 ∈ NV (x2), x1 + R∗y1 = x2 − R∗y2

(11)

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

By Lemma 1, we know that (R∗)−1SR, ∂g+A∗◦∇f ◦A, NV
is maximal monotone due to the maximal monotonicity
of ∂g + A∗ ◦ ∇f ◦ A and NV . Hence, given a constan-
t (cid:15) ≥ 0, the enlargement [(R∗)−1SR, ∂g+A∗◦∇f ◦A, NV ][(cid:15)]
is well deﬁned.
In addition, based on the deﬁnition of
SR,T1,T2 again, we set T1 = ∂g + (A∗ ◦ ∇f ◦ A)[(cid:15)], or
T1 = (∂g + A∗ ◦ ∇f ◦ A)[(cid:15)] and T2 = NV in (10).
Then we have the deﬁnition of SR,∂g+(A∗◦∇f ◦A)[(cid:15)],NV or
SR,(∂g+A∗◦∇f ◦A)[(cid:15)],NV for any given invertible linear op-
erator R and constant (cid:15) ≥ 0 as follows

gph (cid:0)SR,∂g+(A∗◦∇f ◦A)[(cid:15)],NV
(cid:110)
:=

(cid:1)

(x1 +Ry2, x2 −x1)|y1 ∈ (∂g+(A∗ ◦∇f ◦A)[(cid:15)])(x1),
(cid:111)
,

y2 ∈ NV (x2), x1 + R∗y1 = x2 − R∗y2

gph (cid:0)SR,(∂g+A∗◦∇f◦A)[(cid:15)],NV
(cid:110)
:=

(cid:1)

(x1 +Ry2, x2 −x1)|y1 ∈ (∂g + A∗ ◦∇f ◦A)[(cid:15)])(x1),
(cid:111)
.

y2 ∈ NV (x2), x1 + R∗y1 = x2 − R∗y2

(12)

(13)

In the proposition below, we will establish the re-
lationships among the above mentioned three opera-
tors SR,∂g+(A∗◦∇f ◦A)[(cid:15)],NV , SR,(∂g+A∗◦∇f ◦A)[(cid:15)],NV and
[(R∗)−1SR, ∂g+A∗◦∇f ◦A, NV ][(cid:15)].

Proposition 2 Given a constant (cid:15) ≥ 0 and an invertible
linear operator R, it holds that

gph (cid:0)SR, ∂g+(A∗◦∇f ◦A)[(cid:15)], NV
(cid:1)
⊆ gph (cid:0)SR, (∂g+A∗◦∇f ◦A)[(cid:15)], NV
⊆ gph (cid:0)R∗[(R∗)−1SR, ∂g+A∗◦∇f ◦A, NV ][(cid:15)](cid:1).

(cid:1)

In the following, we establish the relationship between
the optimal solution set [∇f + (cid:80)n
i=1 ∂gi]−1 (0) of prob-
lem (1) and (cid:2)(R∗)−1SR, ∂g+A∗◦∇f ◦A, NV
(cid:3)−1
(0), which
means that we can recover the solution of problem (1)
through (cid:2)(R∗)−1SR, ∂g+A∗◦∇f ◦A, NV

(0).

(cid:3)−1

linear

Lemma 2 Let
isfy (R∗)−1 = H and H satisfy (3).
Ω = (cid:2)(R∗)−1SR, (∂g+A∗◦∇f ◦A), NV
(0).
that
(cid:34)

operators H and R sat-
Denote
It holds

(cid:35)−1

(cid:3)−1

∇f +

∂gi

(0) = (cid:0)E T

Y H∗EY

(cid:1)−1

Y H∗(cid:0)Ω(cid:1).
E T

n
(cid:88)

i=1

3. GSOS Algorithm

Algorithm 1 GSOS Algorithm

(cid:3) and θﬁx2 ∈ (cid:0) − 1, θ2

Parameters: Choose σ ∈ (0, 1), a linear operator H
satisfying (3) and a starting point z0 ∈ Z. Set θﬁx1 ∈
(cid:3), where θ1 and θ2 are
(cid:0) − 1, θ1
deﬁned via equations (14a) and (14b), respectively.
for k = 0, 1, 2, · · · , K do
(cid:0)E T
(cid:1)−1
xk := EY
for i = 1, 2 · · · , n do
(cid:16)

Y Hzk;
E T

Y HEY

yk
i

:= ProxH−1
i,i gi
(cid:80)n
i=1 xk

i,i [(cid:80)i
H−1
i ) − (cid:80)i−1

j=1 Hi,j(2xk
(cid:17)
j=1 Hi,jyk
j ]

;

1

n ∇f ( 1

n

j − zk

j ) −

as (14c) and θadap2

end for
set θadap1
k
set θk ∈ [θﬁx1, θadap1
k
k
zk+1 := zk + (1 + θk)(yk − xk);

as (14d);
] ∪ [θﬁx2, θadap2
];

k

end for
return ωK := (cid:0)E T

Y H∗EY

(cid:1)−1

Y H∗zK.
E T

In Algorithm 1, parameters θ1, θ1, θadap1
ﬁned as

k

, θadap1
k

are de-

θ1 = max

θ|(θ − σ)(H + H∗) + LA∗A (cid:22) 0

; (14a)

(cid:110)

(cid:110)

θ2 = max

θ | (θ − σ)(H + H∗)

+A∗(2(cid:98)Σ − Σ)A (cid:22) 0

θadap1
k

= σ −

θadap2
k

= σ −

;

L(cid:107)A(xk − yk)(cid:107)2
(cid:107)xk − yk(cid:107)2
H+H∗
(cid:107)A(xk − yk)(cid:107)2
(cid:107)xk − yk(cid:107)2

H+H∗

2(cid:98)Σ−Σ

.

(cid:111)

(cid:111)
;

(14b)

(14c)

(14d)






Remark 2 We make some comments on GSOS below.

j=1

(cid:80)K

(cid:0) (cid:80)K

(cid:1)−1 (cid:80)K

i=j Hijzk

(1) For the updating step of xk, we obtain xk =
j by using the
EY
i,j=1 Hij
Similarly, we have ωk =
notations H and EY .
(cid:0) (cid:80)K
(cid:1)−1 (cid:80)K
(cid:80)j
i=i H∗
j . Hence, we need
to compute the inverse of (cid:80)n
i,j=1 Hi,j. However, if
Hi,j is a lower triangular matrix operator, xk and ωk
can be obtained easily.

i,j=1 Hij

jizk

j=1

(2) By the deﬁnitions of ProxH−1

i,i gi

solve the following inclusion equation

and yk, we need to

Gk

i ∈ Hi,iyk

i + ∂gi(yk

i ),

In this section, we ﬁrst propose the Gauss-Seidel operator
splitting algorithm for solving the multi-term nonsmooth
convex composite problem (1). Then, based on the pre-
liminaries in Section 2, we establish the convergence and
iteration complexity of the GSOS algorithm.

(cid:2) (cid:80)i
i ) − (cid:80)i−1

i = H−1
i,i
(cid:80)n
i=1 xk

where Gk
j − zk
j ) −
(cid:3). Usually, it is
n ∇f ( 1
1
easy to choose a suitable Hi,i such that the solution
of the above inclusion equation has a closed form.

j=1 Hi,j(2xk

j=1 Hi,jyk
j

n

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

(3) θk is the over-relaxation stepsize for accelerating the
the computations of θadap1
are time consuming, we can set θk =

If

k

GSOS algorithm.
and θadap2
k
max{θﬁx1, θﬁx2}.

Theorem 1 indicates that (cid:107)xk − yk(cid:107) approaching to zero
In the
implies the convergence of the GSOS algorithm.
theorem below, we measure the convergence rates of two
sequences (cid:107)xk − yk(cid:107) and (cid:107)ωk − ωk+1(cid:107).

(4) When H is a diagonal matrix, i.e., Hi,j = 0 and Hi,i =
aiI with some nonnegative constant ai, and the over
relaxation stepsize θk is ﬁxed to a smaller region, the
GSOS algorithm reduces to the generalized forward
backward splitting method in (Raguet et al., 2013).

Theorem 2 Let zk be the sequence generated by the GSOS
algorithm. Then, there exists i ∈ {1, 2, · · · , k} such that

(cid:107)xi − yi(cid:107)2 ≤ O(cid:0) 1
k

(cid:1), (cid:13)

(cid:13)ωi+1 − ωi(cid:13)
2
(cid:13)

≤ O(cid:0) 1
k

(cid:1).

In the following, we reformulate the GSOS algorithm as
a two-step iterations algorithm by utilizing monotone opti-
mization theory established in Section 2, which is the key
to the convergence of the GSOS algorithm.

Proposition 3 Let g : Y → (−∞, +∞] be the func-
tion deﬁned as g(x) = (cid:80)n
i=1 gi(xi). Assume that the
sequences (xk, yk) and zk are generated by Algorithm
1 with σ ∈ (0, 1). Let vk = (R∗)−1(xk − yk) and
zk = yk + R(R∗)−1(zk − xk). Then, for all k ∈ N,
there exists (cid:15)k ≥ 0 such that the iterations in Algorithm
1 can be reformulated as the following two-step iterations
algorithm:



vk ∈ [(R∗)−1SR, ∂g+(A∗◦∇f ◦A), NV ][(cid:15)k](zk),
θk(cid:107)R∗vk(cid:107)2
R−1 + (cid:107)R∗vk + zk − zk(cid:107)2

(15a)

R−1
+2(cid:15)k ≤ σ(cid:107)zk − zk(cid:107)2

R−1,

(15b)



and zk+1 = zk − (1 + θk)R∗vk.

Remark 3 Based on Proposition 3, the GSOS algorithm
can be regarded as an inexact over-relaxed metric proximal
point algorithm for the composite inclusion

0 ∈ (R∗)−1SR,∂g+A∗◦∇f ◦A, NV (z).

By Proposition 3 and Lemma 2, we can establish the con-
vergence of the GSOS algorithm based on the relationship
∂gi]−1(0) and Ω.

between the two zero point sets [∇f+

n
(cid:80)
i=1

Theorem 1 Let {(xk, yk, zk)} be the sequence generated
by Algorithm 1. We have:

(i) for any z∗ ∈ [(R∗)−1SR,∂g+A∗◦∇f ◦A, NV ]−1(0), it

holds that

(cid:107)zk+1 − z∗(cid:107)2

R−1 ≤ (cid:107)zk − z∗(cid:107)2

R−1

(16)

− (1 − σ)(1 + θk)(cid:107)xk − yk(cid:107)2

R−1;

(ii) zk

converges

set

point
and ωk
[∇f + (cid:80)n
of problem (1).

a

point

belonging

to
zero
[(R∗)−1SR,∂g+A∗◦∇f ◦A, NV ]−1(0)
converges
to a point belonging to
i=1 ∂gi]−1 (0), i.e., the optimal solution set

to

Due to the space limit, all proofs of the propositions, lem-
mas and theorems are placed into the supplementary mate-
rial.

4. Experiments

In this section, we apply the proposed algorithm to the
overlapping group Lasso (Zhao et al., 2009; Jacob et al.,
2009; Mairal et al., 2010) and graph-guided fused Lasso
problems (Chen et al., 2012; Kim & Xing, 2009), which
can be formulated as

min

(cid:107)Sx − b(cid:107)2 +

gi(x).

(17)

1
2

K
(cid:88)

i=1

For overlapping group Lasso problem (21), gi(x) =
ναi(cid:107)xGi(cid:107) and K denotes the number of groups. For graph-
guided Lasso problem (25), gi(x) = ναij(cid:107)xi −xj(cid:107) and K
denotes the number of edges in the graph edge set E.

We describe the detailed techniques in the experimental im-
plementation for (17). Given a > 1
2 and a positive deﬁnite
operator D satisfying D (cid:23) ST S, we set

Hi,j =

(cid:26) 1
K2 D,
a
K2 D,

i ≥ j ∈ {1, 2, · · · , K};
i = j ∈ {1, 2, · · · , K}.

(18)

j=1

(cid:80)K

(cid:80)K

i,j=1 Hi,j = K(K−1)+2αK
2K2
j=1(a+K −j)zk
(cid:80)K

it easy to check that H + H∗ = A∗DA +
Hence,
K2 Diag(cid:0)EY D(cid:1) (cid:31) 0. Due to the smooth term in overlap-
2a−1
ping group Lasso (21) is quadratic, the two estimations θ2
and θadap2
in (14b) and (14d) are preferred to be used. By
k
speciﬁc H, we obtain (cid:80)K
D and
(cid:80)K
j = D
i=j Hi,jzk
j , which fur-
K2
ther imply xk = (cid:0) (cid:80)K
i=j Hi,jzk
j =
2 (cid:80)K
j=1(a+K−j)zk
K(K−1)+2aK . Moreover, by the positive deﬁniteness
it holds that (cid:80)n
j =
of Hi,i and D,
D
j . Hence, we attain ωk =
K2
2 (cid:80)K
K(K−1)+2aK . In addition, by the deﬁnition of H, we
reformulate the estimation (14b) for θk as the following
form:

j=1(a + j − 1)zk

j=1(a+j−1)zk

i,j=1 Hi,j

(cid:1)−1 (cid:80)K

i=i H∗

j,izk

(cid:80)K

(cid:80)j

j=1

j=1

j

j

θ = max

θ | EY

(cid:2)(σ − θ)D − ST S(cid:3)E ∗

Y

(cid:110)

+ (2a − 1)(cid:0)σ − θ(cid:1)Diag(EY D) (cid:23) 0

(cid:111)
.

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

Due to a ≥ 1
2 and the positive deﬁniteness of D, a sufﬁ-
cient condition satisfying the constraint in the above set is
(cid:8)(σ − θ)D − ST S (cid:23) 0, θ ≤ σ(cid:9). Hence, we have an alter-
native estimation for θ as

θ = max (cid:8)θ | (σ − θ)D − ST S (cid:23) 0, θ ≤ σ(cid:9) .

(19)

Similarly, the adaptive stepsize estimation (14d) is refor-
mulated as

1
2K2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:80)
i=1

(cid:13)
2
(cid:13)
(xk − yk
i )
(cid:13)
(cid:13)

θadap
k

= σ −

K
(cid:80)
j=1

K
(cid:80)
i=j

(xk − yk

i )T Hij(xk − yk
j )

ST S

.

(20)

Therefore, the GSOS algorithm can be speciﬁed as the fol-
lowing form for solving problem (17).

Algorithm 2 GSOS Algorithm for Solving Problem (17)

Parameters: Choose σ ∈ (0, 1), positive deﬁnite op-
erators D and Hi,j satisfying (18), and a starting point
z0 ∈ Z. Set θ as (19) and θﬁx ∈ (cid:0) − 1, θ1
for k = 0, 1, 2, · · · , do
2 (cid:80)K

(cid:3).

j=1(α+K−j)zk
K(K−1)+2αK ;

xk :=
for i = 1, 2 · · · , K do

j

:= ProxH−1

(cid:0)H−1
yk
i
K ST (Sxk − b) − (cid:80)i−1
1

i,i [(cid:80)i
j=1 Hi,jyk

i,i gi

j ](cid:1);

j=1 Hi,j(2xk − zk

j ) −

], where θadap

k

is deﬁned via (20);

end for
set θk ∈ [θﬁx
for j = 1, 2 · · · , K do
:= zk

k , θadap

k

zk+1
j
end for

j + (1 + θk)(yk

j − xk);

end for
return ωN =

2 (cid:80)K

j=1(α+j−1)zN
K(K−1)+2αK .

j

In this paper, we compare the proposed GSOS algorithm
with four state-of-the-art algorithms below.

• GFB (Raguet et al., 2013): Generalized Forward
Backward (GFB) splitting algorithm is a primal ﬁrst-
order operator splitting algorithm for solving (1) pro-
posed by Raguet et al. (Raguet et al., 2013), which
has been shown to outperform other competing algo-
rithms such as (Monteiro & Svaiter, 2013; Combettes
& Pesquet, 2012; Chambolle & Pock, 2011) for vari-
ational image restoration.

• PDM (Condat, 2013): A ﬁrst-order Primal-Dual split-
ting Method (PDM) (Condat, 2013) for solving jointly
the primal and dual formulations of large-scale convex
minimization problems involving Lipschitz, proximal
and linear composite terms.

• PA-APG (Yu, 2013): Proximal Average approximated
Accelerated Proximal Gradient (PA-APG) algorithm
(Yu, 2013) is a primal ﬁrst-order method, which uti-
lizes the proximal average technique (Bauschke et al.,
2008) to separate the multi-term nonsmooth function
in (1). It has been shown to outperform the smooth-
ing accelerated proximal gradient method (Nesterov,
2005b;a).

• APA-APG (Shen et al., 2017): An enhanced ver-
sion of PA-APG, which incorporates the Adaptive
Proximal Average approximation technique with the
Accelerated Proximal Gradient (APA-APG) method
to improve the efﬁciency of the optimization proce-
dure.

It is worthwhile to emphasize that PA-APG and APA-APG
algorithms can only be applied to a speciﬁc class of prob-
lems (1), in which the multi-term nonsmooth regularization
is Lipschitz continuous. Since the nonsmooth regulariza-
tion terms in overlapping group Lasso and graph-guided
fused Lasso are all exactly Lipschitz continuous, the two
efﬁcient solvers PG-APG (Yu, 2013) and its enhanced ver-
sion APA-APG (Shen et al., 2017) are also compared with
the GSOS algorithm to illustrate the efﬁcacy of GSOS. In
the implementation, the approximation parameter for PA-
APG is set as 1.0e − 5.

4.1. Overlapping Group Lasso

In this subsection, we apply the proposed GSOS algorithm
to the overlapping group Lasso problem, which takes the
following formal deﬁnition:

min

(cid:107)Sx − b(cid:107)2 + ν

αi(cid:107)xGi(cid:107),

(21)

1
2

K
(cid:88)

i=1

where S ∈ Rn×d is the sampling matrix, b is the noisy
observation vector, G = {G1, · · · , GK} denotes the set of
overlapping groups (Gi ⊂ {1, · · · , d} satisfying (cid:83)K
i=1 Gi =
(cid:84) Gj (cid:54)= ∅ for some i, j), xGi ∈ Rd is a
{1, · · · , d} and Gi
duplication of x with x{1,··· ,d}\Gi = 0, αi is the weight
for the i-th group, and ν is the regularization parameter
controlling group sparsity.

During the implementation of Algorithm 2, we need to cal-
culate the generalized proximal mapping of (cid:107)xGi(cid:107) in the
updating step of yk
i . By the positive deﬁniteness of Hi,i,
the calculation of yk
i in Algorithm 2 is equivalent to solv-
ing the following problem:

yk
i := arg min

(cid:107)x − bk(cid:107)2

Hi,i

+ ναi(cid:107)xGi(cid:107),

1
2

x

where bk = H−1
i,i
b)−(cid:80)i−1
j=1 Hi,jyk
j

j=1 Hi,j(2xk − zk

(cid:2) (cid:80)i
K ST (Sxk −
(cid:3). In the proposition below, given c, diag-

j ) − 1

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

Figure 1. Objective value vs. iteration on overlapping group Lasso.

Figure 2. Objective value vs. iteration on overlapping group Lasso.

onal positive deﬁnite operatpr Hi,i and group G, we solve

x∗ := arg min

(cid:107)x − c(cid:107)2

+ ν(cid:107)xG(cid:107).

Hi,i

(22)

1
2

x

When Hi,i is identity matrix I, (22) has the closed-form
solution

x∗ =

(cid:26) x∗
G,
ci,
(cid:26) (1 − ν/(cid:107)cG(cid:107))cG, (cid:107)cG(cid:107) ≥ t;

i ∈ G,
else,

else.

where x∗

G =

0,

Proposition 4 Let (Hi,i)G be the subdiagonal matrix of
Hi,i with the index set G, and t∗ be the optimal solution
of the one-dimensional optimization problem

(cid:26) 1
2

min
t≥0

(cid:10)cG, (cid:2)(Hi,i)−1

G + 2tI(cid:3)−1

cG

(cid:11) + tν2

(cid:27)

.

(23)

Hence, the optimal solution of (22) has the following form

(cid:26)

x∗ =

cG − (cid:2)I + 2t∗(Hi,i)G
ci,

(cid:3)−1

cG,

i ∈ G;
else.

(24)

Like (Chen et al., 2012; Yu, 2013), the entries of sampling
matrix S ∈ Rn×d are sampled from an i.i.d. normal dis-
tribution, and x ∈ Rd with xj = (−1)j exp−(j−1)/100 and
d = 90K + 10. Let ξ be the noise sampled from the stan-
dard normal distribution, and the noisy observation satisﬁes
b = Sx + ξ. In addition, we set ν = 1 and αi = 1
K2 for
each group Gi and the groups {Gi} are overlapped by 10
elements, that is

(cid:26) G1 = {1, · · · , 100}
· · ·

G2 = {91, · · · , 190}
GK = {d − 99, · · · , d}

(cid:27)

.

The sampling size and the number of groups (n, K) are
chosen from the following set

(n, K) ∈

(cid:26) (1000, 20),
(4000, 80),

(2000, 40),
(5000, 80),

(4000, 60),
(5000, 100)

(cid:27)

.

To further reduce the computations, in Algorithm 2 we set
Hi,i = (cid:107)ST S(cid:107)I and the over-relaxation stepsize θk as θ in
(19). Hence, the compared ﬁve solvers GSOS, GFB, PDM,
PA-APG and APA-APG have the same computational cost
in each iteration. To be fair, all the compared algorithms
start with the same initial point. The following six pic-
tures in Figures 1 and 2 display the comparisons of the ﬁve
solvers for a variety of (n, K). It is apparent that our pro-
posed GSOS algorithm shows great superiorities over the
other four solvers. The primal-dual solver PDM is slightly
faster than the primal solver GFB. PA-APG is the slowest
algorithm, because the prespeciﬁed proximal average ap-
proximation precision is 1.0e − 5 which leads to a very
small stepsize. Also, APA-APG is much faster than the
other four solvers at the ﬁrst 50 iterations. However, it is
slowed down since the stepsize used in AP-APG becomes
smaller and smaller as the iterations go on.

4.2. Graph-Guided Fused Lasso

In this subsection, we perform experiments on graph-
guided fused Lasso which is formulated as

min

(cid:107)Sx − b(cid:107)2 + ν

αij|xi − xj|,

(25)

1
2

(cid:88)

(i,j)∈E

where αij ≥ 0 is the weight for the fused term (cid:107)xi − xj(cid:107)
for all (i, j) ∈ E (E is the given graph edge set), and ν is

0200400600800100010−210−1100101102103104105(n,K) = (1000,20)  GSOSPDMGFBAPA−APGPA−APG0200400600800100010−210−1100101102103104105(n,K) = (2000,40)  GSOSPDMGFBAPA−APGPA−APG0200400600800100010−410−2100102104106(n,K) = (4000,60)  GSOSPDMGFBAPA−APGPA−APG0200400600800100010−410−2100102104106(n,K) = (4000,80)  GSOSPDMGFBAPA−APGPA−APG0200400600800100010−410−2100102104106(n,K) = (5000,80)  GSOSPDMGFBAPA−APGPA−APG0200400600800100010−410−2100102104106(n,K) = (5000,100)  GSOSPDMGFBAPA−APGPA−APGGSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

Figure 3. Objective value vs. iteration on graph-guided fused Lasso.

Figure 4. Objective value vs. iteration on graph-guided fused Lasso.

the regularization parameter.

In the implementation of Algorithm 2 for tackling graph-
guided fused Lasso (25), we need to solve the following
optimization in the updating step of yk:

x∗ := arg min

(cid:107)x − b(cid:107)2

+ ν|xi − xj|,

Hi,i

(26)

1
2

x

where Hi,i is a diagonal positive deﬁnite matrix, and b and
ν are given constants. Let hii and hjj be the i-th and j-th
diagonal elements of Hi,i, respectively.

Proposition 5 The optimal solution of (26) takes the fol-
lowing closed-form:



x∗ =



ll λ∗,
ll λ∗,

bl − h−1
bl + h−1
bl,

l = i,
l = j,
l (cid:54)= i, j,

(27)

where λ∗ is deﬁned as

λ∗ =






,

bi−bj
h−1
ii +h−1
sign (bi − bj) ν,

jj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

bi−bj
ii +h−1
h−1
bi−bj
ii +h−1
h−1

jj

jj

(cid:12)
(cid:12)
(cid:12) ≤ ν;
(cid:12)
(cid:12)
(cid:12) > ν.

In the implementation, we use the similar parameter set-
tings of S, ν as above. The dimension parameter pair (n, d)
is chosen from the following set

(n, d) ∈

(cid:26) (2000, 500), (2000, 1000), (5000, 1000),
(5000, 2000), (10000, 2000), (10000, 4000)

(cid:27)

,

and the parameter αi = 100/|E|2. Similarly, all the com-
pared algorithms start with the same initial point. The fol-
lowing six pictures in Figures 3 and 4 display the compar-
isons of the ﬁve solvers for six kinds of choices of (n, d). It

is obvious that the other four solvers GFB, PDM, AP-APG
and APA-APG are not as efﬁcient as the proposed GSOS
algorithm, which demonstrates that the Gauss-Seidel tech-
nique is very useful for addressing nonsmooth optimiza-
tion. It is worthwhile to point out that the primal solver
GFB is faster than the primal-dual solver PDM on graph-
guided fused Lasso. One possible reason is that the number
of nonsmooth terms is too large, which will lead to a large
quantity of dual variables introduced in PDM and hence
slow down the updating of primal variables.

5. Conclusions

In this paper, we proposed a novel ﬁrst-order algorithm
called GSOS for addressing multi-term nonsmooth con-
vex composite optimization. This algorithm inherits the
advantages of the Gauss-Seidel technique and the opera-
tor splitting technique, therefore being largely accelerat-
ed. We found that the GSOS algorithm includes the gen-
eralized forward backward splitting method (Raguet et al.,
2013) as a special case. In addition, we developed a new
technique to establish the global convergence and iteration
complexity of the GSOS algorithm. Last, we applied the
proposed GSOS algorithm to solve overlapping group Las-
so and graph-guided fused Lasso problems, and compared
it against several state-of-the-art algorithms. The experi-
mental results show the great superiority of the GSOS al-
gorithm in terms of both efﬁciency and effectiveness.

Acknowledgements

Yuan is supported by NSF-China (61402182).

02004006008001000102103104105106107108109(n,d) = (2000,500)  GSOSPDMGFBAPA−APGPA−APG020040060080010001021031041051061071081091010(n,d) = (2000,1000)  GSOSPDMGFBAPA−APGPA−APG020040060080010001021031041051061071081091010(n,d) = (5000,1000)  GSOSPDMGFBAPA−APGPA−APG0200400600800100010210410610810101012(n,d) = (5000,2000)  GSOSPDMGFBAPA−APGPA−APG020040060080010001031041051061071081091010(n,d) = (10000,2000)  GSOSPDMGFBAPA−APGPA−APG0200400600800100010210410610810101012(n,d) = (10000,4000)  GSOSPDMGFBAPA−APGPA−APGGSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

References

Bach, F. R. Structured sparsity-inducing norms through
submodular functions. Advances in Neural Information
Processing Systems, pp. 118–126, 2010.

Bach, F. R., Jenatton, R., Mairal, J., and Obozinski, G.
Structured sparsity through convex optimization. Sta-
tistical Science, 27(4):pgs. 450–468, 2012.

Chen, X., Lin, Q., Kim, S., Carbonell, J. G., and Xing, E. P.
Smoothing proximal gradient method for general struc-
tured sparse regression. The Annals of Applied Statistics,
6(2):719–752, 2012.

Combettes, P. L. and Pesquet, J. C. A douglas–rachford
splitting approach to nonsmooth convex variational sig-
nal recovery. IEEE Journal of Selected Topics in Signal
Processing, 1(4):564–574, 2007.

Bauschke, H. H. and Combettes, P. L. Convex Analysis and
Monotone Operator Theory in Hilbert Space. Springer
New York, 2011.

Combettes, P. L and Pesquet, J. C. A proximal decom-
position method for solving convex variational inverse
problems. Inverse problems, 24(6):065014, 2008.

Bauschke, H. H., Goebel, R., Lucet, Y., and Wang, X. The
proximal average: basic theory. SIAM Journal on Opti-
mization, 19(2):766–785, 2008.

Beck, A. and Teboulle, M. A fast iterative shrinkage-
inverse problems.
thresholding algorithm for
SIAM journal on imaging sciences, 2(1):183–202, 2009.

linear

Briceno-Arias, L. M., Combettes, P. L., Pesquet, J. C., and
Pustelnik, N. Proximal algorithms for multicomponen-
t image recovery problems. Journal of Mathematical
Imaging and Vision, 41(1-2):3–22, 2011.

Burachik, R. S. and Svaiter, B. F. ε-enlargements of max-
imal monotone operators in banach spaces. Set-Valued
Analysis, 7(2):117–132, 1999.

Burachik, R. S., Iusem, A. N., and Svaiter, B. F. Enlarge-
ment of monotone operators with applications to varia-
tional inequalities. Set-Valued and Variational Analysis,
5(2):159–180, 1997.

Burachik, R. S., Sagastiz´abal, C. A., and Svaiter, B. F. ε-
enlargements of maximal monotone operators: Theory
and applications. In Reformulation: nonsmooth, piece-
wise smooth, semismooth and smoothing methods, pp.
25–43. Springer, 1998.

Chambolle, A. and Pock, T. A ﬁrst-order primal-dual al-
gorithm for convex problems with applications to imag-
ing. Journal of Mathematical Imaging and Vision, 40(1):
120–145, 2011.

Chen, D. and Plemmons, R. J. Nonnegativity constraints in

numerical analysis. 2015.

Chen, G. H. G. and Rockafellar, R. T. Convergence rates
in forward–backward splitting. SIAM Journal on Opti-
mization, 7(2):421–444, 1997.

Chen, X., Lin, Q., Kim, S., Carbonell, J. G., and Xing,
E. P. An efﬁcient proximal gradient method for general
structured sparse learning. stat, 1050:26, 2011.

Combettes, P. L. and Pesquet, J. C. Proximal splitting
methods in signal processing. In Fixed-point algorithm-
s for inverse problems in science and engineering, pp.
185–212. Springer, 2011.

Combettes, P. L. and Pesquet, J. C. Primal-dual splitting
algorithm for solving inclusions with mixtures of com-
posite, lipschitzian, and parallel-sum type monotone op-
erators. Set-Valued and variational analysis, 20(2):307–
330, 2012.

Condat, L. A primal-dual splitting method for convex op-
timization involving lipschitzian, proximable and linear
composite terms. Journal of Optimization Theory and
Applications, 158(2):460–479, 2013.

Cui, Y., Li, X., Sun, D. F., and Toh, K. C. On the con-
vergence properties of a majorized alternating direction
method of multipliers for linearly constrained convex op-
timization problems with coupled objective functions.
Journal of Optimization Theory & Applications, 169(3):
1013–1041, 2016.

Davis, D. and Yin, W. A three-operator splitting scheme
and its optimization applications. Mathematics, 19(3):
407–12, 2015.

Dup´e, F. X., Fadili, J. M., and Starck, J. L. A proximal
iteration for deconvolving poisson noisy images using s-
parse representations. IEEE Transactions on Image Pro-
cessing, 18(2):310–321, 2009.

Eckstein, J. and Bertsekas, D. P. On the douglasłrachford
splitting method and the proximal point algorithm for
maximal monotone operators. Mathematical Program-
ming, 55(1):293–318, 1992.

Huang, J., Zhang, T., and Metaxas, D. Learning with struc-
tured sparsity. Journal of Machine Learning Research,
12(Nov):3371–3412, 2011.

Jacob, L., Obozinski, G., and Vert, J. P. Group lasso with
In International Conference
overlap and graph lasso.
on Machine Learning, ICML 2009, Montreal, Quebec,
Canada, June, pp. 433–440, 2009.

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

Kim, S. and Xing, E. P. Statistical estimation of correlat-
ed genome associations to a quantitative trait network.
PLoS Genet, 5(8):e1000587, 2009.

Svaiter, B. F. A family of enlargements of maximal
monotone operators. Set-Valued Analysis, 8(4):311–328,
2000.

Li, Q. and Zhang, N. Fast proximity-gradient algorithms
for structured convex optimization problems . Applied
& Computational Harmonic Analysis, 41(2):491–517,
2016.

Svaiter, B. F. A class of fej´er convergent algorithm-
s, approximate resolvents and the hybrid proximal-
extragradient method. Journal of Optimization Theory
and Applications, 162(1):133–153, 2014.

Teo, C. H., Smola, A., Vishwanathan, S., and Le, Q. V.
A scalable modular convex solver for regularized risk
minimization. In Proceedings of the 13th ACM SIGKD-
D international conference on Knowledge discovery and
data mining, pp. 727–736. ACM, 2007.

Teo, C. H., Vishwanthan, S., Smola, A. J., and Le, Q. V.
Bundle methods for regularized risk minimization. Jour-
nal of Machine Learning Research, 11(Jan):311–365,
2010.

V˜u, B. C. A splitting algorithm for dual monotone inclu-
sions involving cocoercive operators. Advances in Com-
putational Mathematics, 38(3):667–681, 2013.

Xu, Y. and Yin, W. A block coordinate descent method for
regularized multiconvex optimization with applications
to nonnegative tensor factorization and completion. Siam
Journal on Imaging Sciences, 6(3):1758–1789, 2013.

Yu, Y. Better approximation and faster algorithm using the
proximal average. Advances in Neural Information Pro-
cessing Systems, pp. 458–466, 2013.

Yuan, G., Zheng, W. S., and Ghanem, B. A matrix split-
ting method for composite function minimization. arXiv
preprint arXiv:1612.02317, 2016.

Zhao, P., Rocha, G., and Yu, B. The composite absolute
penalties family for grouped and hierarchical variable s-
election. The Annals of Statistics, pp. 3468–3497, 2009.

Zhong, W. and Kwok, J. T. Y. Accelerated stochastic gra-
dient method for composite regularization. In AISTATS,
pp. 1086–1094, 2014.

Zhou, K., Zha, H., and Song, L. Learning social infectivi-
ty in sparse low-rank networks using multi-dimensional
hawkes processes. In AISTATS, volume 31, pp. 641–649,
2013.

Lin, Z., Liu, R., and Su, Z. Linearized alternating direc-
tion method with adaptive penalty for low-rank repre-
sentation. Advances in Neural Information Processing
Systems, pp. 612–620, 2011.

Luo, Z. Q. and Tseng, P. On the convergence of a ma-
trix splitting algorithm for the symmetric monotone lin-
ear complementarity problem. SIAM Journal on Control
and Optimization, 29(5):1037–1060, 1991.

Mairal, J., Jenatton, R., Bach, F. R., and Obozinski, G. R.
Network ﬂow algorithms for structured sparsity. In Ad-
vances in Neural Information Processing Systems, pp.
1558–1566, 2010.

Monteiro, R. D. C. and Svaiter, B. F. Iteration-complexity
of block-decomposition algorithms and the alternating
direction method of multipliers. SIAM Journal on Opti-
mization, 23(1):475–507, 2013.

Nesterov, Y. Excessive gap technique in nonsmooth con-
vex minimization. SIAM Journal on Optimization, 16
(1):235–249, 2005a.

Nesterov, Y. Smooth minimization of non-smooth func-
tions. Mathematical programming, 103(1):127–152,
2005b.

Nesterov, Y. Gradient methods for minimizing composite
functions. Mathematical Programming, 140(2007076):
125–161, 2007.

Pustelnik, N., Chaux, C., and Pesquet, J. C. Parallel prox-
imal algorithm for image restoration using hybrid regu-
larization. IEEE Transactions on Image Processing, 20
(9):2450–2462, 2011.

Raguet, H., Fadili, J., and Peyr´e, G. A generalized forward-
backward splitting. SIAM Journal on Imaging Sciences,
6(3):1199–1226, 2013.

Richard, E., Savalle, P. A., and Vayatis, N. Estimation
of simultaneously sparse and low rank matrices. arXiv
preprint arXiv:1206.6474, 2012.

Shen, L., Liu, W., Huang, J., Jiang, Y. G., and Ma, S. Adap-
tive proximal average approximation for composite con-
vex minimization. In AAAI, 2017.

