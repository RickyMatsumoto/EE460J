Supplementary Material for “iSurvive: An Interpretable, Event-time
Prediction Model for mHealth”

Walter H. Dempsey * 1 Alexander Moreno * 2 Christy K. Scott 3 Michael L. Dennis 3 David H. Gustafson 4
Susan A. Murphy 1 James M. Rehg 2

A. Prior Work on Interpretable Latent State

Models

We highlight key differences between the present work and
an interpretable, latent state model introduced by (Lian
et al., 2014). In it, the model has one sequence of latent
(K binary events) states (e.g., progression in a movie); each
user experiences the same sequence of latent states but may
react differently, resulting in a user-speciﬁc intensity func-
tion that produces a response to the latent process. In our
model, on the other hand, the latent state process evolves
independently from user to user. Thus the participants do
not experience the same sequence of latent states. This is a
key difference for our mobile health application.

In Lian et al. (2014), interpretability is achieved post-hoc
and is not a built in feature of the model. Comparison of
latent sources to movie scenes is performed and intuitive
statements are made such as ”One source relates to en-
hanced arousal intensity during a plane crash, key plot turn-
ing points, a climax, and a surprise denouement.” We, on
the other hand, will provide a systematic method of relat-
ing one latent state to particular emissions so that the states
are easily interpreted, allowing us to make statements such
as the 30-minute probability of lapse at high risk and low
engagement is 74.4%.

B. Graphical Models for Section 2.3

The graphical model for the example discussed in Sec-
tion 2.3 is given by {S1 → O1, S1 → O2, S2 → O2},
where x → y denotes a directed arc from x to y.

*Equal contribution 1University of Michigan 2Georgia Institute
of Technology 3Lighthouse Institute 4University of Wisconsin-
Madison. Correspondence to: Alexander Moreno <alexan-
der.f.moreno@gatech.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure 1. Graphical model for sec. 2.3. Recall that s1 corresponds
to stress and s2 corresponds to craving. Then o1 is conditionally
independent of s2, given s1.

C. Details on Weighted Fisher Scoring

We have assumed complete factorization of the observation
component; therefore, we only investigate the kth observa-
tion component. Let O(n)
denote the kth observation com-
ponent for the nth participant. Then the ﬁnal term of the
expected complete log-likelihood is given by

k

N
(cid:88)

Vn(cid:88)

(cid:104)

E

n=1

v=1

log p(O(n)(tv) = ov | S(tv)) | On[t]; ˆQ, ˆΦ

(cid:105)

.

(cid:16)

S(tv) = s | On[t]; ˆQ, ˆΦ
Let γv(s) = p
pected complete log-likelihood is equal to

(cid:17)

. Then the ex-

N
(cid:88)

Vn(cid:88)

(cid:88)

n=1

v=1

s
(cid:88)

(cid:88)

=

γv(s) log p(O(n)(tv) = ov | S(tv) = s)

b(s) log p(O = o | S(tv) = s).

s

o
where (cid:80)
o is a sum over the observed values of the obser-
vation process (e.g., if the process is binary then the sum is
over o = 0 and o = 1) and b(s) = (cid:80)
n γv(s) (i.e., the
sum of all weights where the latent process is equal to the
latent state s ∈ S.

(cid:80)

v

Given weights {b(s)}s∈S, we now show how to obtain the
maximum-likelihood estimates via a weighted version of
Fisher scoring. Let ˆη0 denote the current estimate of the

!"!#$"!"!#$"$#$#Supplementary Material for “iSurvive: An Interpretable, Event-time Prediction Model for mHealth”

linear predictor, and ˆµ0 the ﬁtted value using the link func-
tion η = g(µ). Then deﬁne

z0 = ˆη0 + (y − ˆµ0)

(cid:19)

(cid:18) dη
dµ

0

where the derivative of the link is evaluated at ˆµ0. Weights
are deﬁned by

W −1

0 = ¯b

(cid:19)2

(cid:18) dη
dµ

V0

0

where V0 is the variance function evaluated at ˆµ0, and ¯b =
(b(1), . . . , b(S)) is the vector of weights associated with
the EM-algorithm. Then regress z0 on the covariates with
weight W0 to give new estimates ˆβ; from these form a new
estimate ˆη. Repeat until changes are sufﬁciently small.

D. EM Convergence – Technical Details

Here we provide technical details related to Lemma 1 from
the main body of the paper; we assume the event process
is observed via self-report and therefore can be thought of
as part of the observation process. When the event process
is measured continuously, the below theory can easily be
adjusted to include the necessary third term. We omit this
for brevity as the conclusions are not changed.

We assume the observation schedule always starts with an
observation at baseline (i.e, t1 = 0). The next observa-
tion times ti are (potentially stochastic) functions of the
observed history Hi; therefore the observation schedule
satisﬁes the restrictive conditional independence assump-
tion. The study is of length ξ and therefore the observation
schedule for each participant t is a random subset of the
interval [0, ξ].

We assume the probability of observing the sequence o =
(o(t1), . . . , o(tk)) is given by:

p(X = x; θ) =

p(S = s, O = o; θ)

=

p(s(t1))

p(s(ti) | s(ti−1); θz)

(cid:90)

s

k
(cid:89)

i=2

p(o(ti) | s(ti); θo)p(ti | Hi; ω)

(cid:90)

s

·

K
(cid:89)

i=1

and θ =
where S is a latent Markov process,
(θs, θx) parametrize the latent Markov process and the
“measurement-error models” respectively. Note that the
ﬁnal term is independent of the latent process and there-
fore factors outside the summation; under the assumption
of variational independence we see that the observation
schedule component of the joint probability will not impact
maximum likelihood estimation with respect to θ.

We observe N independent and identically distributed ob-
servation sequences, oi = (oi(ti,1), . . . , oi(ti,ki )) for i =
1, . . . , n. Then the re-scaled log-likelihood of such a sam-
ple is given by

lN (θ) =

log

p(S = s, Xi = xi; θ)

(cid:19)

=

log

ES | xi;θ

(cid:20) p(S = s, Xi = xi; θ(cid:48))
p(S = s | Xi = xi; θ(cid:48))

(cid:21)(cid:19)

.

By Jensen’s inequality we have

lN (θ) ≥

ES | xi;θ(cid:48)

log

(cid:18) p(S = s, Xi = xi; θ(cid:48))
p(S = s | Xi = xi; θ(cid:48))

(cid:19)(cid:35)

(cid:18)(cid:90)

s

(cid:18)

(cid:34) N
(cid:88)

i=1

(cid:20) N
(cid:88)

i=1

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1

1
N

1
N

−

N
(cid:88)

i=1

=

ES | xi;θ

log (p(S = s, Xi = xi; θ(cid:48)))

log (p(S = s | Xi = xi; θ(cid:48)))

(cid:21)

=QN (θ | θ(cid:48)) − HN (θ(cid:48)).

For a choice of θ(cid:48), the E-step computes the function θ →
QN (θ | θ(cid:48)). The M-step then maximizes the Q-function
for ﬁxed θ(cid:48):

MN (θ(cid:48)) = arg max
θ∈Ω

QN (θ | θ(cid:48))

For the HMM, we can decompose the Q-function as

QN (θ | θ(cid:48)) = Q(s)

N (θs | θ(cid:48)) + Q(s)

N (θx | θ(cid:48)).

That is, we can decompose the Q-function into a compo-
nent only dependent on θs and one only dependent on θx.
This implies the M-step is also decomposable and we can
deﬁne

M (s)

N (θ(cid:48)) = arg max
θs∈Ωs
N (θ(cid:48)) = arg max
θx∈Ωs

M (x)

Q(s)

N (θs | θ(cid:48)) and

Q(x)

N (θx | θ(cid:48))

Under the assumption of independent and identically dis-
tributed, the law of large numbers ensures that as the same
size N increases, the sample-based Q-function approaches
its expectation:
˜Q(θ | θ(cid:48)) = E[QN (θ | θ(cid:48))] = E (cid:2)ES | X,θ(cid:48) [log(p(S, X; θ))](cid:3)

where the outside expectation is over the distribution of o
(both the observation values and schedule). We call ˜Q the
“population Q-function”. The “population M-function” is
deﬁned as

˜M (θ(cid:48)) = arg max
θ∈Ω

˜Q(θ | θ(cid:48)).

Supplementary Material for “iSurvive: An Interpretable, Event-time Prediction Model for mHealth”

D.1. Analysis of EM Algorithm

D.1.1. POPULATION LEVEL ALGORITHM

Let θ(cid:63) denote the maximum likelihood estimate (MLE) of
the population likelihood E[lN (θ)]. Here we assume the
MLE is unique. The MLE is a ﬁxed point of the population
M -function – i.e., it automatically satisﬁes

and

θ(cid:63) = ˜M (θ(cid:63)).

We make the following conditions on the population Q-
function.
Assumption 1 (Conditions on ˜Q). We assume the follow-
ing conditions on the population Q-function for θ ∈ Ω:

Proof. By deﬁnition, the M -functions satisfy the following
optimality condition

(cid:104)∇Q(s)(M (s)(θ(cid:63)) | θ(cid:63)), M (s)(θ(cid:63)

s ) − M (s)(θ)(cid:105) ≥ 0

(cid:104)∇Q(s)(M (s)(θ) | θ), M (s)(θ(cid:63)

s ) − M (s)(θ)(cid:105) ≤ 0.

Recall the optimal θ(cid:63)
s ) = θ(cid:63)
(i.e., M (s)(θ(cid:63)

s is a ﬁxed point of the M (s)-operator
s ). Combining these inequalities yields

(cid:104)∇Q(s)(M (s)(θ(cid:63)) | θ(cid:63)) − ∇Q(s)(M (s)(θ) | θ),
s − M (s)(θ)(cid:105) ≥ 0.
θ(cid:63)

(1)

1. We assume ˜Q(s) is λs-strongly concave:

Recall λs-strong concavity, states that

˜Q(s)(θs | θ(cid:63)) − ˜Q(s)(θ(cid:48)

s | θ(cid:63)) − (cid:104)∇ ˜Q(s)(θs), θs − θ(cid:48)
s(cid:105)
λs
2

(cid:107)θs − θ(cid:48)

≤ −

s(cid:107)2
2

for θs, θ(cid:48)

s ∈ Ωs and θ(cid:63) ∈ Ω.
2. We assume ˜Q(x) is λx-strongly concave

˜Q(x)(θx | θ(cid:63)) − ˜Q(x)(θ(cid:48)

x | θ(cid:63)) − (cid:104)∇ ˜Q(x)(θx), θx − θ(cid:48)
x(cid:105)
λx
2

(cid:107)θx − θ(cid:48)

x(cid:107)2
2

≤ −

for θx, θ(cid:48)

x ∈ Ωx and θ(cid:63) ∈ Ω.
3. For each θx ∈ Ωx and θ(cid:48) ∈ Ω

(cid:107)∇x ˜Q(x)(θx | (θ(cid:63)

s , θ(cid:63)

(cid:107)∇x ˜Q(x)(θx | (θ(cid:63)

s , θ(cid:63)

x)) − ∇x ˜Q(x)(θx | (θ(cid:48)
≤ Lx,s(cid:107)θ(cid:63)
s − θ(cid:48)
s(cid:107)
x)) − ˜Q(x)(θx | (θ(cid:63)
s , θ(cid:48)
x − θ(cid:48)
≤ Lx,x(cid:107)θ(cid:63)
x(cid:107)

s, θ(cid:63)

x))(cid:107)2

x))(cid:107)2

4. For each θs ∈ Ωs and θ(cid:48) ∈ Ω

(cid:107)∇s ˜Q(z)(θs | (θ(cid:63)

s , θ(cid:63)

(cid:107)∇s ˜Q(x)(θx | (θ(cid:63)

s , θ(cid:63)

x)) − ∇x ˜Q(x)(θx | (θ(cid:48)
s − θ(cid:48)
≤ Lx,s(cid:107)θ(cid:63)
s(cid:107)
x)) − ˜Q(x)(θx | (θ(cid:63)
s , θ(cid:48)
x − θ(cid:48)
≤ Lx,x(cid:107)θ(cid:63)
x(cid:107)

s, θ(cid:63)

x))(cid:107)2

x))(cid:107)2

Q(s)(θs | θ(cid:63)) − Q(s)(θ(cid:48)

s | θ(cid:63)) − (cid:104)∇Q(s)(θs | θ(cid:63)),

θs − θ(cid:48)

s(cid:105) ≥

(cid:107)θs − θ(cid:48)

s(cid:107)2
2.

λs
2

s and adding the resulting in-

Switching places of θs and θ(cid:48)
equality to the above yields

(cid:104)∇Q(s)(θs | θ(cid:63)) − ∇Q(s)(θ(cid:48)

s | θ(cid:63)),
s(cid:105) ≥ λs(cid:107)θs − θ(cid:48)

s(cid:107)2
2.

θs − θ(cid:48)

(2)

Substitute M (s)(θ) = θs and θ(cid:63)
equation (2) yields

s = M (s)(θ(cid:63)) = θ(cid:48)

s into

(cid:104)∇Q(s)(M (s)(θ) | θ(cid:63)) − ∇Q(s)(M (s)(θ(cid:63)) | θ(cid:63)),
M (s)(θ) − M (s)(θ(cid:63))(cid:105) ≥ λs(cid:107)M (s)(θ) − M (s)(θ(cid:63))(cid:107)2
2

= λs(cid:107)M (s)(θ) − θ(cid:63)

s (cid:107)2
2.

Also, equation (1) implies

(cid:104)∇Q(s)(M (s)(θ(cid:63)) | θ(cid:63)) − ∇Q(s)(M (s)(θ) | θ(cid:63)),
s − M (s)(θ)(cid:105)
θ(cid:63)

≥ (cid:104)∇Q(s)(M (s)(θ) | θ) − ∇Q(s)(M (s)(θ) | θ(cid:63)),

s − M (s)(θ)(cid:105).
θ(cid:63)

Lemma 1. Under the above assumptions, for θ ∈ Ω and
pair (L, λ), then population M -function satisﬁes

Take the left-hand side of the above inequality. Then the
second set of assumptions leads to

(cid:107)M (θ) − θ(cid:107) ≤

(cid:107)θ − θ(cid:63)(cid:107).

(cid:19)

(cid:18) L
λ

Deﬁne L = max{Ls,x, Ls,s} + max{Lx,s, Lx,x} and λ =
min{λs, λx}. Then for any point starting θ0 ∈ Ω, the pop-
ulation EM-algorithm satisﬁes
(cid:18) L
λ

(cid:107)θ(t) − θ(cid:63)(cid:107)2 ≤

(cid:107)θ0 − θ(cid:63)(cid:107)2

(cid:19)t

for every t = 1, 2, . . ..

(cid:104)∇Q(s)(M (s)(θ(cid:63)) | θ(cid:63)) − ∇Q(s)(M (s)(θ) | θ(cid:63)),
s − M (s)(θ)(cid:105)
θ(cid:63)

≤(cid:107)∇Q(s)(M (s)(θ) | θ(cid:63)) − ∇Q(s)(M (s)(θ) | θ(cid:63))(cid:107)2
2

≤ (cid:0)Ls,x · (cid:107)θ(cid:63)

x − θx(cid:107)2

2 + Ls,s · (cid:107)θ(cid:63)

s − θs(cid:107)2
2

(cid:1)

(cid:107)θ(cid:63)

s − M (s)(θ)(cid:107)2
2

(cid:107)θ(cid:63)

s − M (s)(θ)(cid:107)2

≤ max{Ls,x, Ls,s} (cid:0)(cid:107)θ(cid:63) − θ(cid:107)2

(cid:1) (cid:107)θ(cid:63)

2

s − M (s)(θ)(cid:107)2

Supplementary Material for “iSurvive: An Interpretable, Event-time Prediction Model for mHealth”

where the second inequality is application of Cauchy-
Schwarz, the third is due to item 3 in Assumptions 1. Com-
bining the inequalities yields the equation

Iterating on this we have

(cid:107)ˆθ(t+1) − θ(cid:63)(cid:107) ≤

κs

(cid:15)(N, δ) + κt(cid:107)θ0 − θ(cid:63)(cid:107)

λs(cid:107)θ(cid:63)

s − M (s)(θ)(cid:107)2 ≤ max{Ls,x, Ls,s}(cid:107)θ(cid:63) − θ(cid:107)2

The identical argument applied to the x-component yields

λx(cid:107)θ(cid:63)

x − M (x)(θ)(cid:107)2 ≤ max{Lx,x, Lx,s}(cid:107)θ(cid:63) − θ(cid:107)2.

Combining these with λ = min(λs, λx) and L =
max{Ls,x, Ls,s} + max{Lx,x, Lx,s} yields

(cid:107)θ(cid:63) − M (θ)(cid:107)2 ≤

(cid:107)θ(cid:63) − θ(cid:107)2

(cid:19)

(cid:18) L
λ

D.2. Finite Sample-size Statistical Guarantees for EM

Algorithm

Deﬁnition 1 (Sample-to-population M -function gap). For
a particular sample size N ≥ 0 and constant δ ∈ (0, 1),
deﬁne (cid:15)(x)(N, δ) by:

p((cid:107)M (x)

N (θ) − M (x)(θ)(cid:107)2 ≥ (cid:15)(x)(N, δ)) ≤ δ

and (cid:15)(z)(N, δ) by:

sup
θ∈Ω

sup
θ∈Ω

p((cid:107)M (s)

N (θ) − M (s)(θ)(cid:107)2 ≥ (cid:15)(s)(N, δ)) ≤ δ.

Lemma 2 below is the technical version of Lemma 1 in the
main body of the paper; we end with a proof of this result.

Lemma 2. The population M -function satisﬁes Assump-
tions 1 for all θ ∈ Ω. Then for sample size sufﬁciently
large to ensure

(cid:15)x(N, δ) + (cid:15)s(N, δ) ≤ (1 − κ) r − κ max
θs∈Ωs

(cid:107)θs − θ(cid:63)
s (cid:107)

Then for θ0 ∈ Ω, with probability 1−2δ, the EM-algorithm
satisﬁes

(cid:107)ˆθ(t) − θ(cid:63)(cid:107) ≤ κt(cid:107)θ0 − θ(cid:63)(cid:107) +

((cid:15)x(N, δ) + (cid:15)s(N, δ))

1
1 − κ

Proof. Deﬁne (cid:15)(N, δ) = (cid:15)x(N, δ) + (cid:15)x(N, δ). With prob-
ability at least 1 − 2δ

(cid:107)ˆθ(t+1) − θ(cid:63)(cid:107) = (cid:107)MN (ˆθ(t)) − θ(cid:63)(cid:107)

≤ (cid:107)MN (ˆθ(t)) − ˜M (ˆθ(t))(cid:107) + (cid:107) ˜M (ˆθ(t)) − θ(cid:63)(cid:107)
≤ (cid:15)x(N, δ) + (cid:15)x(N, δ) + κ (cid:107)ˆθ(t) − θ(cid:63)(cid:107).

(cid:34)t−1
(cid:88)

(cid:35)

s=0
1
1 − κ

≤

(cid:15)(N, δ) + κt(cid:107)θ0 − θ(cid:63)(cid:107).

To complete the proof, note the sample size based inequal-
ity ensures that each iteration stays within the space Ω.

E. Additional Details for the RSS Case Study

Here we present additional details regarding the subset of
data analyzed from the recovery support studies on individ-
uals with substance use disorders (SUDs). We start with
a more detailed description of each component of the re-
duced observation O(t) = (O1(t), O2(t), O3(t), Y (t)).

• O1(t): Ordinal response to the question ”Rate the
extent to which certain feelings help with/support
your recovery.” Original response was on a 0–7 scale.
Based on studying the distribution of responses we
collapsed into a three-level ordinal response:

(0, 1, 2) → 0, (3, 4, 5) → 1, and (6) → 2.

Here 2 is translates to current feelings greatly helping
with/supporting your recovery. The fraction of obser-
vations per level is (0.10, 0.26, 0.64). Figure 2 below
presents histograms across participants of the fraction
of EMAs when a participant responds with a particu-
lar rating.

• O2(t): a 3-level ordinal variable related to EMI usage

by the following mapping:

None → 0, 1–3 times → 1, and 4+ → 2

observations
level
The
of
fraction
is (0.76, 0.15, 0.09).
Figure 3 below presents
histograms across participants of the fraction of
EMAs when a participant had a particular level of
EMI usage.

per

• O3(t): an indicator of whether the participant kept all
default answers to all questions within the self-report.
The fraction of observations per level is (0.61, 0.39).
Figure 3 below presents a histogram (across partici-
pants) of the fraction of EMAs when the participant
responded with all default answers.

• Y(t): the binary indicator of use of drugs/alcohol in
past 30 minutes. The fraction of observations per
level is (0.94, 0.06) (i.e., 6% of all EMAs record
drug/alcohol use within the ). Figure 3 below presents
a histogram (across participants) of the fraction of
EMAs when the participant responded yes to having
used drugs/alcohol within the past 30 minutes.

Supplementary Material for “iSurvive: An Interpretable, Event-time Prediction Model for mHealth”

Figure 2. Histograms across participants related to O1(t)

Figure 3. Histograms across participants related to O2(t)

We assumed 2 latent binary sources–i.e., S(t) =
(S1(t), S2(t)) with Si(t) ∈ {0, 1} for each i ∈ {1, 2}. We
describe how we think about each and then show how this
interpretation is achieved via link restriction.

• S1(t): We deﬁne this as risk. Here S1(t) is thought to
be uniquely associated with O1, the ordinal response
on how current feelings help with support a partici-
pant’s recovery.

• S2(t): We deﬁne this to be engagement. Here engage-
ment is deﬁned in a very speciﬁc manner: “thinking”
through the EMA self-report and not simply ﬁlling a
self-report with default answers. Here S2(t) is thought
to be uniquely associated with O3(t).

We now specify the models for each observation compo-
nent conditional on S(t).

• Model for O2(t): Hierarchical model dependent on
S2(t). If S2(t) = 1 then the participant is not cur-
rently engaged, and therefore the response is indepen-
dent of S1(t). In this case, the following proportional
odds model deﬁnes the relationship:

Figure 4. Histogram related to O3(t)

If S2(t) = 1 then the participant is currently engaged,
and therefore the response depends on S1(t). In this
case, the following proportional odds model deﬁnes
the relationship:

logit(p(O2(t) ≤ j | S1(t), S2(t) = 0]) = φ(1,0)

j +S1(t)φ(1)
1 .

logit(p(O2(t) ≤ j | S1(t), S2(t) = 1]) = φ(1,1)

.

j

• Model for O2(t): Proportional odds model deﬁned by

0.00.20.40.60.81.0Fraction of EMAs with rating = 0051015202530354045Frequency0.00.20.40.60.81.0Fraction of EMAs with rating = 1051015202530354045Frequency0.00.20.40.60.81.0Fraction of EMAs with rating = 2051015202530354045Frequency0.00.20.40.60.81.0Fraction of EMAs with EMI usage = 0051015202530354045Frequency0.00.20.40.60.81.0Fraction of EMAs with EMI usage = 1051015202530354045Frequency0.00.20.40.60.81.0Fraction of EMAs with EMI usage = 2051015202530354045Frequency0.00.20.40.60.81.0Fraction of EMAs with all default answers024681012141618FrequencySupplementary Material for “iSurvive: An Interpretable, Event-time Prediction Model for mHealth”

Figure 5. Histogram related to Y (t)

Figure 6. Cross-validated complete log-loss on recovery support
services study for several discriminative models and iSurvive

the following relationship:

logit(p(O2(t) ≤ j | S(t)]) = φ(2)

j

+ S1(t)φ(2)

1 + S2(t)φ(2)
2 .

• Model for O3(t): GLM with logit link function (i.e.,
logistic regression) deﬁned by the following relation-
ship:

logit(E[O3(t) | S(t)]) = φ(3)

0 + S2(t)φ(3)
2 .

• Model for Y (t): GLM with logit link function (i.e.,
logistic regression) deﬁned by the following relation-
ship:

logit(E[Y (t) | S(t)]) = φ(4)

0 + S1(t)φ(4)

1 + S2(t)φ(4)
2 .

A log-linear model can also be ﬁt with an “expo-
sure” parameter of 30-minutes. This would allow for
the user to make predictions for future self-reported
drug/alcohol use over different windows. Since this
was not necessary for the case study, we did not pur-
sue this further.

F. Additional details on Synthetic

Experiments

Table 1. Results from 10 runs of the synthetic experiments. We
deleted a single extreme outlier from the c = 2 case.

c
1
2
3

Binary MAE
0.056±0.029
0.039±0.028
0.010±0.006

Count Weights Norm Q rel. error
0.109±0.087
0.120±0.102
0.068±0.023

0.269±0.048
0.222±0.104
0.180±0.044

We assume the latent process has p = 3 sources: for ease
of understanding we consider them to be stress, craving,

and engagement. We generate a random Q matrix in the
following way: we sample from a dirichlet with parame-
ters K = 8 and α1, · · · , αK = 8. We then zero out the
diagonals, set them to be the negative of the sum of the re-
maining terms in the corresponding rows, and multiply the
entire matrix by 5. For the observations, we have three or-
dinal ratings, each having a binary response. We also have
count data to represent number of times EMIs (Ecological
Momentary Interventions) are accessed via the application.
The emission model for the ratings data is a GLM similar
to equation (3) given by

logit (E[Ok(t) | S]) = φ(cid:48)S(t)

(3)

while for EMI the counts between two observations follow
a homogeneous poisson process with mean rate parameter
λ such that

log(λ) = φ(cid:48)S(t)

(4)

We use logit emissions in order to see the effect of smart
initialization, as the lack of strong concavity far from the
true parameters can provide a challenge without it.

we

the

use

φ
=
For
EMA,
(φbaseline, φstress, φcraving, φengagement).
Each EMA
is only associated with one variable, and the true re-
lationship between the baseline parameter and a latent
variable weight is ﬁxed. For the stress question this would
be φ = c(1, −2, 0, 0), while for craving it would be
φ = c(1, 0, −2, 0), where c is a scalar. The weights are
the same magnitudes across questions. With this setup,
c becomes a parameter that controls the emission noise.
Larger values of c correspond to lower emission noise.
Particularly, c = 1, 2, 3 correspond to probabilities 0.73,
0.88, and 0.95 for observing a 0. For the EMI we set the
true weights to φ = c(0.4, 0.1, 0.2, 0.3). We set c = 3
for the true value, and c = 0.25 with the true relative
proportions for the smart initialization. When not using

0.00.20.40.60.81.0Fraction of EMAs with self-reported alcohol/drug use051015202530354045Frequency012345Window length (in days)0.10.20.30.40.50.6Brier ScoreComparison of prediction accuracyiSurviveCurrent O (Logistic)Event at prior time (Logistic)Complete (Logistic)Current O (kSVM)Event at prior time (kSVM)Complete (kSVM)Supplementary Material for “iSurvive: An Interpretable, Event-time Prediction Model for mHealth”

smart initialization, we set c = 0 and initialize Q randomly
the same way we initialized the true Q.

Table 1 shows the results from running the algorithm with
smart initialization 10 times for each of c = 1, 2, 3 with
N = 50, T = 252 and 125 iterations. We deleted a single
extreme outlier for c = 2, and found that the differences
between the accuracy of c = 1 and c = 2 were not statisti-
cally signiﬁcant at 95%, but the differences between c = 3
and either of c = 1 or c = 2 were for the binary case and
count weights. The difference for Q in the latter case was
just outside the 95% interval. For the terms shown, the bi-
nary MAE represents the mean absolute error between the
true probability of observing a 0 and the learned probabil-
ity across all states. The count weight norms represent the
norm of the difference between generating and estimated
parameters, and the Q relative error is the same as in (Liu
et al., 2015).

References

Lian, W., Rao, V.A., Eriksson, B., and Carin, L. Mod-
eling correlated arrival events with latent semi-markov
processes. In Proceedings of the 29th International Con-
ference on Machine Learning, 2014.

Liu, Y., Song, L., Li, F., Li, S., and Rehg, J. Efﬁcient
continuous-time hidden markov model for disease mod-
eling. In Proceedings for Advances in Neural Informa-
tion Processing Systems (NIPS), 2015.

