Supplementary Materials for
Understanding Synthetic Gradients and Decoupled Neural Interfaces

A. Relation to critic methods

Instead of estimating the gradient directly, one could es-
timate loss instead (thus use some trainable φ(h|θ) ≈
E[L|h]) and then use its gradient wrt. to its inputs (∂φ/∂h)
as a surrogate for the synthetic gradient. These kind
of approaches are known in Reinforcement Learning as
critic methods (Fairbank, 2014; Heess et al., 2015), but
in terms of gradient approximation they do not guarantee
any alignment between these signals if only critic is a non-
linear function. As an example lets consider a function
φ(h(xi)|θ) = L(h(xi)) for every xi, such that it is con-
stant (in terms of its output) in some (cid:15) balls around each
xi. As a consequence gradients of φ are 0 everywhere, yet
as a critic it receives no learning signal (since loss is ap-
proximated perfectly). This example shows that in general
alignment between critic gradient and true gradient can be
arbitrary, and completely independent from the loss error
itself.

B. Additional examples

Critical points

We can show an example of SG introducing new critical
points. Consider a small one-dimensional training dataset
{−2, −1, 1, 2} ⊂ R, and let us consider a simple system
where the model f : R → R is parametrised with two
scalars, a and b and produces ax + b. We train it to min-
imise L(a, b) = (cid:80)4
i=1 |axi + b|. This has a unique mini-
mum which is obtained for a = b = 0, and standard gra-
dient based methods will converge to this solution. Let us
now attach a SG module betweenf and L. This module
produces a (trainable) scalar value c ∈ R (thus it produces
a single number, independent from the input). Regardless
of the value of a, we have a critical point of the SG module
when b = 0 and c = 0. However, solutions with a = 1
and c = 0 are clearly not critical points of the original sys-
tem. Figure 6 shows the loss surface and the ﬁtting of SG
module when it introduces new critical point.

C. Proofs

Theorem 1 Let us consider linear regression trained with
a linear SG module attached between its output and the

Figure 6. Left: The loss surface with a white marker represents
critical point of the original optimisation and white line a set of
critical points of SG based one. Right: A situation when SG ﬁnds
a solution d = 0 which introduces new critical point, which is not
a critical point of the original problem.

loss.
If one chooses the learning rate of the SG module
using line search, then in every iteration there exists small
enough, positive learning rate of the main network such
that it converges to the global solution.

s=1 ∈ Rd×S be the data,

let
s=1 ∈ R1×S be the labels. Throughout the proof k

Proof. Let X = {xs}S
{ys}S
will be the iteration of training.
We denote by 1 ∈ R1×S a row vector in which every el-
ement is 1. We also follow the standard convention of in-
cluding the bias in the weight matrix by augmenting the
data X with one extra coordinate always equal to 1. Thus,
we denote ¯X = (XT |1T )T , ¯X ∈ R(d+1)×S and ¯xs-the
columns of ¯X. Using that convention, the weight matrix is
Wk ∈ R1×(d+1). We have

L =

(ys − ps

k)2 =

(ys − Wk ¯xs)2 .

k := Wk ¯xs,
ps

1
2

n
(cid:88)

i=1

arg min
W,b

L.

1
2

S
(cid:88)

s=1

Our aim is to ﬁnd

We use

∂L
∂W

=

∂L
∂p

∂p
∂W

=

S
(cid:88)

s=1

∂L
∂ps

∂ps
∂W

=

Understanding Synthetic Gradients and DNIs

Using matrix notation

S
(cid:88)

s=1

∂L
∂ps ¯xs =

S
(cid:88)

s=1

(ys − Wk ¯xs) (¯xs)T

∂L
∂p

= (cid:0)p1 − y1, . . . , pS − yS(cid:1)

We will use the following parametrization of the synthetic
gradient (cid:103)∇Lk = (αk +1)pk −(βk +1)y+γk1. The reason
for using this form instead of simply akpk + bky + ck1 is
that we are going to show that under DNI this synthetic gra-
dient will converge to the “real gradient” ∂L
∂p , which means
showing that
(αk, βk, γk) = (0, 0, 0). Thanks to this
choice of parameters αk, βk, γk we have the simple expres-
sion for the error

lim
k→∞

Ek =

(cid:13)
(cid:13)
(cid:103)∇Lk −
(cid:13)
(cid:13)

∂L
∂p

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

=

(cid:107)(αk + 1)pk − (βk + 1)y + γk1−

(cid:0)p1

k − y1, . . . , pS

k − yS(cid:1)(cid:13)
2
2 =
(cid:13)
k − βkyS + γk

(cid:1)(cid:13)
2
(cid:13)
2

(cid:13)
(cid:0)αkp1
(cid:13)

k − βky1 + γk, . . . , αkpS

Parameters αk, βk, γk will be updated using the gradient
descent minimizing the error E. We have

(αkps

k − βkys + γk)ps
k

∂E
∂α

=

S
(cid:88)

s=1

∂E
∂β

∂E
∂γ

=

S
(cid:88)

s=1

S
(cid:88)

s=1

= −

(αkps

k − βkys + γk)ys

(αkps

k − βkys + γk).

As prescribed in Jaderberg et al. (2016), we start our iter-
ative procedure from the synthetic gradient being equal to
zero and we update the parameters by adding the (negative)
gradient multiplied by a learning rate ν. This means that we
apply the iterative procedure:

α0 = −1, β0 = −1, γ0 = 0

Wk+1 =Wk − µ

((αk + 1)ps

k−

(βk + 1)ys + γk) (¯xs)T

Wk+1 = Wk − µ((αk + 1)pk − (βk + 1)y + γk1) ¯XT
αk+1 = αk − ν (cid:0)αk(cid:107)pk(cid:107)2
2 − βk(cid:104)y, pk(cid:105) + γk(cid:104)1, pk(cid:105)(cid:1)
βk+1 = βk + ν (cid:0)αk(cid:104)pk, y(cid:105) − βk(cid:107)y(cid:107)2
γk+1 = γk − ν (αk(cid:104)1, pk(cid:105) − βk(cid:104)1, y(cid:105) + Sγk)

2 + γk(cid:104)1, y(cid:105)(cid:1)

Note, that the subspace given by α = β = γ = 0 is invari-
ant under this mapping. As noted before, this corresponds
to the synthetic gradient being equal to the real gradient.
Proving the convergence of SG means showing, that a tra-
jectory starting from α0 = −1, β0 = −1, γ0 = 0 con-
verges to W = W0, α = β = γ = 0, where W0 are
the “true” weigts of the linear regression. We are actu-
ally going to prove more, we will show that W = W0,
α = β = γ = 0 is in fact a global attractor, i.e. that any
trajectory converges to that point. Denoting ω = (α, β, γ)t
we get

Wk+1 = Wk − µ((αk + 1)pk − (βk + 1)y + γk1) ¯XT
k | − yT |1T (cid:3)T (cid:2)pT
ωk+1 = ωk − ν (cid:2)pT

k | − yT |1T (cid:3) ωk

Wk+1 = Wk − µ(pk − y) ¯XT − µωT
k
k | − yT |1T (cid:3)T (cid:2)pT
ωk+1 = ωk − ν (cid:2)pT
k | − yT |1T (cid:3) we get
Denoting by Ak = (cid:2)pT

k | − yT |1T (cid:3)T ¯XT

(cid:2)pT
k | − yT |1T (cid:3) ωk.

Wk+1 = Wk − µ(pk − y) ¯XT − µωT AT
k
ωk+1 = ωk − νAT

k Akωk.

¯XT

Multiplying both sides of the ﬁrst equation by ¯X we obtain

Wk+1 ¯X = Wk ¯X − µ(pk − y) ¯XT ¯X − µωT AT
k
k Akωk.

ωk+1 = ωk − νAT

¯XT ¯X

Denote B = ¯XT ¯X. We get

pk+1 = pk − µpkB + µyB − µωT
ωk+1 = ωk − νAT

k Akωk.

k AT

k B

Denoting ek = (y − pk)T we get

ek+1 = ek − µBek + µBAkωk
ωk+1 = ωk − νAT

k Akωk.

αk+1 =αk − ν

(αkps

k − βkys + γk)ps
k

We will use the symbol ξ = Akωk. Then

βk+1 =βk + ν

(αkps

k − βkys + γk)ys

γk+1 =γk − ν

(αkps

k − βkys + γk).

ek+1 = ek − µBek + µBξk
ξk+1 = ξk − νAkAT

k ξk.

(1)

Every vector v can be uniquely expressed as a sum v =
v⊥ + v(cid:107) with ¯Xv⊥ = 0 and v(cid:107) = ¯XT θ for some θ (v(cid:107)
is a projection of v onto the linear subspace spanned by

S
(cid:88)

s=1

S
(cid:88)

s=1

S
(cid:88)

s=1

S
(cid:88)

s=1

Understanding Synthetic Gradients and DNIs

the columns of ¯X). Applying this decomposition to ek =
k + e(cid:107)
e⊥

k we get

e⊥
k+1 = e⊥
k+1 = e(cid:107)
e(cid:107)
ξk+1 = ξk − νAkAT

k − µ(Bek)⊥ + µ(Bξk)⊥
k − µ(Bek)(cid:107) + µ(Bξk)(cid:107)
k ξk.

Note now, that as B = ¯XT ¯X, for any vector v there is
(Bv)⊥ = 0, and (Bv)(cid:107) = Bv (because the operator v (cid:55)→
v(cid:107) is a projection). Moreover, Bv = Bv(cid:107). Therefore

e⊥
k+1 = e⊥
k
k − µ(Be(cid:107)
k+1 = e(cid:107)
e(cid:107)
ξk+1 = ξk − νAkAT

k) + µ(Bξk)(cid:107)
k ξk.

k does not change. Thus, we will be omitting
k is “the residue”, the small-

The value e⊥
the ﬁrst equation. Note, that e⊥
est error that can be obtained by a linear regression.
For the sake of visual appeal we will denote f = e(cid:107)
k

fk+1 = fk − µBfk + µBξk
ξk+1 = ξk − νAkAT

k ξk.

Taking norms and using (cid:107)u + v(cid:107) ≤ (cid:107)u(cid:107) + (cid:107)v(cid:107) we obtain

(cid:107)fk+1(cid:107)2 ≤ (cid:107)fk − µBfk(cid:107)2 + µ(cid:107)Bξk(cid:107)2
(cid:107)ξk+1(cid:107)2

2 = (cid:107)ξk(cid:107)2

2 − 2ν(cid:107)AT

k ξk(cid:107)2

2 + ν2(cid:107)AkAT

k ξk(cid:107)2
2.

2 = (cid:107)fk(cid:107)2

2 − 2µfkBfk +
2. As B is a constant matrix, there exists a con-
2 for any v satisfying
2 − 2µb(cid:107)fk(cid:107)2
2 +
2. Using that and (cid:107)Bξk(cid:107)2 ≤ (cid:107)B(cid:107)(cid:107)ξk(cid:107)2 we

Observe that (cid:107)fk − µBfk(cid:107)2
µ2(cid:107)Bfk(cid:107)2
stant b > 0 such that vT Bv ≥ b(cid:107)v(cid:107)2
v(cid:107) = v. Therefore (cid:107)fk − µBfk(cid:107)2
µ2(cid:107)B(cid:107)2(cid:107)fk(cid:107)2
get
(cid:107)fk+1(cid:107)2 ≤ (cid:112)
(cid:107)ξk+1(cid:107)2

1 − 2µb + µ2(cid:107)B(cid:107)2(cid:107)fk(cid:107)2 + µ(cid:107)B(cid:107)(cid:107)ξk(cid:107)2

2 + ν2(cid:107)AkAT

2 = (cid:107)ξk(cid:107)2

2 − 2ν(cid:107)AT

2 ≤ (cid:107)fk(cid:107)2

k ξk(cid:107)2
2.

k ξk(cid:107)2

Let us assume that AkAT
k ξk (cid:54)= 0. In that case the right-
hand side of the second equation is a quadratic function is
ν, whose minimum value is attained for ν = (cid:107)AT
.
(cid:107)AkAT
For so-chosen ν we have
(cid:107)fk+1(cid:107)2 ≤ (cid:112)
(cid:18)

1 − 2µb + µ2(cid:107)B(cid:107)2(cid:107)fk(cid:107)2 + µ(cid:107)B(cid:107)(cid:107)ξk(cid:107)2
(cid:107)AT

2
k ξk(cid:107)2
2

k ξk(cid:107)2

(cid:19)

(cid:107)AT
(cid:107)AkAT

k ξk(cid:107)2
2
k ξk(cid:107)2
2

k ξk(cid:107)2
2
(cid:107)ξk(cid:107)2
2

(cid:107)ξk(cid:107)2
2.

(cid:107)ξk+1(cid:107)2

2 =

1 −

Consider a space {f } ⊕ {ξ} (concatenation of vectors) with
a norm (cid:107){f } ⊕ {ξ}(cid:107)⊕ = (cid:107)f (cid:107)2 + (cid:107)ξ(cid:107)2.

(cid:107){fk+1} ⊕ {ξk+1}(cid:107)⊕ ≤

(cid:112)

1 − 2µb + µ2(cid:107)B(cid:107)2(cid:107)fk(cid:107)2 + µ(cid:107)B(cid:107)(cid:107)ξk(cid:107)2 +

(cid:115)

√

1 −

(cid:107)AT
(cid:107)AkAT

k ξk(cid:107)2
2
k ξk(cid:107)2
2

(cid:107)AT

k ξk(cid:107)2
2
(cid:107)ξk(cid:107)2
2

(cid:107)ξk(cid:107)2 ≤

Using

1 − h ≤ 1 − 1

2 h we get

(cid:107){fk+1} ⊕ {ξk+1}(cid:107)⊕ ≤ (cid:112)

1 − 2µb + µ2(cid:107)B(cid:107)2(cid:107)fk(cid:107)2+

(cid:18)

1 −

(cid:107)AT

k ξk(cid:107)2
2
k ξk(cid:107)2
2

(cid:107)AT
2(cid:107)AkAT

k ξk(cid:107)2
2
(cid:107)ξk(cid:107)2
2
1 − 2µb + µ2(cid:107)B(cid:107)2 < 1 for 0 < µ ≤ b

(cid:107)ξk(cid:107)2

+ µ

(cid:19)

(cid:107)B(cid:107)2 .

Note, that (cid:112)
Thus, for

µ < min

(cid:26) b

(cid:107)B(cid:107)2 , 1 −

(cid:107)AT
2(cid:107)AkAT

k ξk(cid:107)2
2
k ξk(cid:107)2
2

(cid:107)AT

k ξk(cid:107)2
2
(cid:107)ξk(cid:107)2
2

(cid:27)

,

for every pair {fk+1} ⊕ {ξk+1} (cid:54)= {0} ⊕ {0} (and if they
are zeros then we already converged) there is

(cid:107){fk+1} ⊕ {ξk+1}(cid:107)⊕ < (cid:107){fk} ⊕ {ξk}(cid:107)⊕.

Therefore, by Theorem 2, the error pair {fk+1} ⊕ {ξk+1}
has to converge to 0, which ends the proof in the case
AkAT
k ξk (cid:54)= 0. It remains to investigate what happens if
AkAT
k ξk = 0.
We start by observing that either ξk = 0 or AT
AkAT
ξk = Akωk. Indeed, if ξk (cid:54)= 0 there is 0 < (cid:107)Akωk(cid:107)2
k AT
ωT
k ξk.
In case ξk = 0 there is (cid:107){fk+1} ⊕ {ξk+1}(cid:107)⊕ =
(cid:107)
=
(cid:112)
1 − 2µb + µ2(cid:107)B(cid:107)2(cid:107){fk} ⊕ {ξk}(cid:107)⊕ and the theo-

k ξk (cid:54)= 0 and
k ξk (cid:54)= 0. This follows directly from the deﬁnition
2 =

k ξk and analogously 0 < (cid:107)AT

1 − 2µb + µ2(cid:107)B(cid:107)2(cid:107)fk(cid:107)2

k ξk(cid:107) = ξT

k AkAT

fk+1(cid:107)2

(cid:112)

<

rem follows.

Theorem 2. Let B be a ﬁnite-dimensional Banach space.
Let f : B → B be a continuous map such that for every
x ∈ B there is (cid:107)f (x)(cid:107) < (cid:107)x(cid:107). Then for every x there is
lim
n→∞

f n(x) = 0.

lim
n→∞

f in (x) = y}.
Proof. Let ω(x) = {y : ∃i1<i2<...
Because (cid:107)f (x)(cid:107) < (cid:107)x(cid:107), the sequence x, f (x), f 2(x), . . .
is contained in a ball of a radius (cid:107)x(cid:107), which due to a ﬁ-
nite dimensionality of B is a compact set. Thus, ω(x)
is nonempty. Moreover, from the deﬁnition, ω(x) is a
closed set, and therefore it is a compact set. Let y0 =
inf y∈ω(x) (cid:107)y(cid:107) – which we know exists, due to the com-
pactness of ω(x) and the continuity of (cid:107) · (cid:107) (Weierstraß
theorem). But for every y ∈ ω(x) there is f (y) ∈ ω(x),
thus there must be y0 = 0. By deﬁnition, for every ε, there
exists n0 such that (cid:107)f n0(x)(cid:107) < ε. Therefore, for n > n0
(cid:107)f n(x)(cid:107) < ε. Therefore, f n(x) must converge to 0.

Proposition 2. Let us assume that a SG module is trained
in each iteration in such a way that it (cid:15)-tracks true gradient,

Understanding Synthetic Gradients and DNIs

i.e. that (cid:107)SG(h, y) − ∂L/∂h(cid:107) ≤ (cid:15). If (cid:107)∂h/∂θ<h(cid:107) is upper
bounded by some K and there exists a constant δ ∈ (0, 1)
such that in every iteration (cid:15)K ≤ (cid:107)∂L/∂θ<h(cid:107) 1−δ
1+δ , then
the whole training process converges to the solution of the
original problem.

Proof. Directly from construction we get that (cid:107)∂L/∂θ<h−
ˆ∂L/ ˆ∂θ<h(cid:107) = (cid:107)(∂L/∂h−SG(h, y))∂h/∂θ<h(cid:107) ≤ (cid:15)K thus
in each iteration there exists such a vector e, that (cid:107)e(cid:107) ≤ (cid:15)K
and ˆ∂L/ ˆ∂θ<h = ∂L/∂θ<h + e. Consequently, we get
a model trained with noisy gradients, where the noise of
the gradient is bounded in norm by (cid:15)K so, directly from
assumptions, it is also upper bounded by (cid:107)∂L/∂θ<h(cid:107) 1−δ
1+δ
and we we get that the direction followed is sufﬁcient for
convergence as this means that cosine between true gradi-
ent and synthetic gradient is uniformly bounded away (by
δ) from zero (Zoutendijk, 1970; Gratton et al., 2011). At
the same time, due to Proposition 1, we know that the as-
sumptions do not form an empty set as the SG module can
stay in an (cid:15) neighborhood of the gradient, and both norm
of the synthetic gradient and (cid:107)∂h/∂θ<h(cid:107) can go to zero
around the true critical point.

Corollary 1. For a deep linear model and an MSE ob-
jective, trained with a linear SG module attached between
two of its hidden layers, there exist learning rates in each
iteration such that it converges to the critical point of the
original problem.

Proof. Denote the learning rate of the main model by µ
and learning rate of the SG module by ν > 0 and put µ =
(cid:15) max(0, (cid:107)e(cid:107) − 1/(3(cid:107)∂h/∂θ<h(cid:107))(cid:107)∂L/∂θ<h(cid:107)), where (cid:15) is
a small learning rate (for example found using line search)
and e is the error SG will make in the next iteration. The
constant 1/3 appears here as it is equal to (1 − δ)/(1 + δ)
for δ = 0.5 which is a constant from Proposition 2, which
we will need later on. Norm of e consists of the error ﬁtting
term LSG which we know, and the term depending on the
previous µ value, since this is how much the solution for the
SG problem evolved over last iteration. In such a setting,
the main model changes iff

(cid:107)e(cid:107)(cid:107)∂h/∂θ<h(cid:107) < 1/3(cid:107)∂L/∂θ<h(cid:107).

(2)

First of all, this takes place as long as ν is small enough
since the linear SG is enough to represent ∂L/∂h with ar-
bitrary precision (Proposition 1) and it is trained to do so
in a way that always converges (as it is a linear regression
ﬁtted to a linear function). So in the worst case scenario
for a few ﬁrst iterations we choose very small µ (it always
exists since in the worst case scenario µ = 0 agrees with
the inequality). Furthermore, once this happens we follow
true gradient on θ>h and a noisy gradient on θ<h. Since

the noise is equal to e∂h/∂θ<h we get that

(cid:107)e∂h/∂θ<h(cid:107) ≤ (cid:107)e(cid:107)(cid:107)∂h/∂θ<h(cid:107) < 1/3(cid:107)∂L/∂θ<h(cid:107),

which is equivalent to error for θ<h being upper bounded
by (1 − δ)/(1 + δ)(cid:107)∂L/∂h(cid:107) for δ = 0.5 which matches
assumptions of Proposition 2, thus leading to the conver-
gence of the model considered. If at any moment we lose
track of the gradient again – the same mechanism kicks in -
µ goes down for as long as the inequality (2) does not hold
again (and it has to at some point, given ν is positive and
small enough).

D. Technical details

All experiments were performed using TensorFlow (Abadi
et al., 2016). In all the experiments SG loss is the MSE
between synthetic and true gradients. Since all SGs con-
sidered were linear, weights were initialized to zeros so
initially SG produces zero gradients, and it does not affect
convergence (since linear regression is convex).

Datasets

Each of the artiﬁcial datasets is a classiﬁcation problem,
consisting of X sampled from k-dimensional Gaussian dis-
tribution with zero mean and unit standard deviation. For
k = 2 we sample 100 points and for k = 100 we sample
1000. Labels y are generated in a way depending on the
dataset name:

• lineark - we randomly sample an origin-crossing hy-
perplane (by sampling its parameters from standard
Gaussians) and label points accordingly,

• noisyk - we label points according to lineark and then

randomly swap labels of 10% of samples,

• randomk - points are labeled completely randomly.

We used one-hot encoding of binary labels to retain com-
patibility with softmax-based models, which is consistent
with the rest of experiments. However we also tested
the same things with a single output neuron and regular
sigmoid-based network and obtained analogous results.

Optimisation

Optimisation is performed using the Adam opti-
miser (Kingma & Ba, 2014) with a learning rate of
3e − 5. This applies to both main model and to SG module.

Artiﬁcial datasets

Table 2 shows results for training linear regression (shallow
MSE), 10 hidden layer deep linear regression (deep MSE),

Understanding Synthetic Gradients and DNIs

Figure 7. (top) Representation Dissimilarity Matrices for a label ordered sample from MNIST dataset pushed through 20-hidden layer
deep relu networks trained with backpropagation (top row), a single SG attached between layers 11 and 12 (2nd row), SG between
every pair of layers (3rd row), and the DFA model (4th row). Notice the moment of appearance of dark blue squares on a diagonal in
each learning method, which shows when a clear inner-class representation has been learned. For visual conﬁdence off block diagonal
elements are semi transparent. (bottom) L2 distance between diagonal elements at a given layer and the same elements at layer 20.
Dotted lines show where SGs are inserted. With a single SG module we can see that there is the representation is qualitatively different
for the ﬁrst part of the network (up to layer 11) and the rest. For fully unlocked model the representation constantly evolves through
all the layers, as opposed to backprop which has a nearly constant representation correlation from layer 9 forward. Also due to DFA
mathematical formulation it tries to solve the task as early as possible thus leading to nearly non-evolving representation correlation after
the very ﬁrst layer.

logistic regression (shallow log loss) and 10 hidden layer
deep linear classiﬁer (deep log loss). Since all these prob-
lems (after proper initialisation) converge to the global op-
tima, we report the difference between ﬁnal loss obtained
for SG enriched models and the true global optimum.

MNIST experiments

Networks used are simple feed forward networks with h
layers of 512 hidden relu units followed by batch normali-
sation layers. The ﬁnal layer is a regular 10-class softmax
layer. Inputs were scaled to [0, 1] interval, besides that there
was no preprocessing applied.

Representational Dissimilarity Matrices

In order to build RDMs for a layer h we sample 400 points
(sorted according to their label) from the MNIST dataset,
{xi}400
i=1 and record activations on each of these points,
hi = h(xi). Then we compute a matrix RDM such that
RDMij = 1 − corr(hi, hj). Consequently a perfect RDM
is a block diagonal matrix, thus elements of the same class
have a representation with high correlation and the repre-
sentations of points from two distinct classes are not cor-
related. Figure 7 is the extended version of the analogous
Figure 3 from the main paper where we show RDMs for

backpropagation, a single SG, SG in-between every two
layers, and also the DFA model, when training 20 hidden
layer deep relu network.

Linear classiﬁer/regression probes

One way of checking the degree to which the actual clas-
siﬁcation problem is solved at every layer of a feedforward
network is to attach linear classiﬁers to every hidden layer
and train them on the main task without backpropagating
through the rest of the network. This way we can make a
plot of train accuracy obtained from the representation at
each layer. As seen in Figure 8 (left) there is not much of
the difference between such analysis for backpropagation
and a single SG module, conﬁrming our claim in the paper
that despite different representations in both sections of SG
based module - they are both good enough to solve the main
problem. We can also that DFA tries to solve the classiﬁca-
tion problem bottom-up as opposed to up-bottom – notice
that for DFA we can have 100% accuracy after the very ﬁrst
hidden layer, which is not true even for backpropagation.

We also introduced a new kind of linear probe, which tries
to capture how much computation (non-linear transforma-
tions) are being used in each layer. To achieve this, we at-
tach a linear regressor module after each hidden layer and

Understanding Synthetic Gradients and DNIs

none of these two approaches provide a loss estimation ﬁ-
delity comparable with the full SG (conditioned on both
activations and labels). This gives another empirical con-
ﬁrmation for correct conditioning of the module. Secondly,
models which used only labels did not converge to a good
solutions after 100k iterations, while without the label SG
was able to do so (however it took much longer and was far
noisier).

References

Abadi, Mart´ın, Agarwal, Ashish, Barham, Paul, Brevdo,
Eugene, Chen, Zhifeng, Citro, Craig, Corrado, Greg S,
Davis, Andy, Dean, Jeffrey, Devin, Matthieu, et al. Ten-
sorﬂow: Large-scale machine learning on heterogeneous
distributed systems. arXiv preprint arXiv:1603.04467,
2016.

Fairbank, M. Value-gradient learning. PhD thesis, City

University London, UK, 2014.

Gratton, Serge, Toint, Philippe L, and Tr¨oltzsch, Anke.
How much gradient noise does a gradient-based line-
search method tolerate. Technical report, Citeseer, 2011.

Heess, N, Wayne, G, Silver, D, Lillicrap, T P, Erez, T,
and Tassa, Y. Learning continuous control policies by
stochastic value gradients. In Advances in Neural Infor-
mation Processing Systems 28: Annual Conference on
Neural Information Processing Systems 2015, December
7-12, 2015, Montreal, Quebec, Canada, pp. 2944–2952,
2015.

Jaderberg, Max, Czarnecki, Wojciech Marian, Osindero,
Simon, Vinyals, Oriol, Graves, Alex, and Kavukcuoglu,
Koray. Decoupled neural interfaces using synthetic gra-
dients. arXiv preprint arXiv:1608.05343, 2016.

Kingma, Diederik and Ba,

Jimmy.
method for stochastic optimization.
arXiv:1412.6980, 2014.

Adam:

A
arXiv preprint

Zoutendijk, G. Nonlinear programming, computational
methods. Integer and nonlinear programming, 143(1):
37–86, 1970.

dataset

model

MSE

linear2
linear100
noisy2
noisy100
random2
random100
noisy2
noisy100
random2
random100

shallow 0.00000
shallow 0.00002
shallow 0.00000
shallow 0.00002
shallow 0.00000
shallow 0.00004
0.00000
deep
0.00001
deep
0.00000
deep
0.00001
deep

log loss

0.03842
0.08554
0.00036
0.00442
0.00000
0.00003
0.00000
0.00293
0.00000
0.00004

Table 2. Differences in ﬁnal losses obtained for various mod-
els/datasets when trained with SG as compared to model trained
with backpropagation. Bolded entries denote experiments which
converged to a different solution. lineark is k dimensional, lin-
early separable dataset, noisy is linearly separable up to 10% la-
bel noise, and random has completely random labeling. Shallow
models means linear ones, while deep means 10 hidden layer deep
linear models. Reported differences are averaged across 10 differ-
ent datasets from the same distributions.

Figure 8. Left: Training accuracy at each linear classiﬁer probe.
Right: MSE for each linear regressor probe.

regress it (with MSE) to the input of the network. This
is obviously label agnostic approach, but measures how
non-linear the transformations are up to the given hidden
layer. Figure 8 (right) again conﬁrms that with a single SG
we have two parts of the network (thus results are similar
to RDM experiments) which do have slightly different be-
haviour, and again show clearly that DFA performs lots of
non-linear transformations very early on compared to all
other methods.

Loss estimation

In the main paper we show how SG modules using both ac-
tivations and labels are able to implicitly describe the loss
surface reasonably well for most of the training, with dif-
ferent datasets and losses. For completeness, we also in-
clude the same experiment for SG modules which do not
use label information (Figure 9 (a) - (d)) as well as a mod-
ule which does not use activations at all6 (Figure 9 (e) -
(h))). There are two important observations here: Firstly,

6This is more similar to a per-label stale gradient model.

Understanding Synthetic Gradients and DNIs

Single SG

Every layer SG

Single SG

Every layer SG

a) MSE, noisy linear data, no label conditioning

b) log loss, noisy linear data, no label conditioning

c) MSE, randomly labeled data, no label conditioning

d) log loss, randomly labeled data, no label conditioning

e) MSE, noisy linear data, only label conditioning

f) log loss, noisy linear data, only label conditioning

n
o
i
t
a
r
e
t
i
n
i
a
r
T

n
o
i
t
a
r
e
t
i

n
i
a
r
T

n
o
i
t
a
r
e
t
i

n
i
a
r
T

n
o
i
t
a
r
e
t
i

n
i
a
r
T

g) MSE, randomly labeled data, only label conditioning

h) log loss, randomly labeled data, only label conditioning

Figure 9. Visualisation of the true loss and the loss extracted from the SG module. In each block left plot shows an experiment with
a single SG attached and the right one with a SG after each hidden layer. Note, that in this experiment the ﬁnal loss is actually big,
thus even though the loss reassembles some part of the noise surface, the bright artifact lines are actually keeping it away from the true
solution.

