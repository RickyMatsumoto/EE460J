Improved Variational Autoencoders for Text Modeling using Dilated
Convolutions

Zichao Yang 1 Zhiting Hu 1 Ruslan Salakhutdinov 1 Taylor Berg-Kirkpatrick 1

Abstract

Recent work on generative text modeling has
found that variational autoencoders (VAE) with
LSTM decoders perform worse than simpler
LSTM language models (Bowman et al., 2015).
This negative result is so far poorly understood,
but has been attributed to the propensity of
LSTM decoders to ignore conditioning informa-
tion from the encoder.
In this paper, we ex-
periment with a new type of decoder for VAE:
a dilated CNN. By changing the decoder’s di-
lation architecture, we control the size of con-
text from previously generated words.
In ex-
periments, we ﬁnd that there is a trade-off be-
tween contextual capacity of the decoder and ef-
fective use of encoding information. We show
that when carefully managed, VAEs can outper-
form LSTM language models. We demonstrate
perplexity gains on two datasets, representing the
ﬁrst positive language modeling result with VAE.
Further, we conduct an in-depth investigation of
the use of VAE (with our new decoding archi-
tecture) for semi-supervised and unsupervised la-
beling tasks, demonstrating gains over several
strong baselines.

1. Introduction

Generative models play an important role in NLP, both in
their use as language models and because of their ability
to effectively learn from unlabeled data. By parameterz-
ing generative models using neural nets, recent work has
proposed model classes that are particularly expressive and
can pontentially model a wide range of phenomena in lan-
guage and other modalities. We focus on a speciﬁc instance

1Carnegie Mellon University. Correspondence to: Zichao

Yang <zichaoy@cs.cmu.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

of this class: the variational autoencoder1 (VAE) (Kingma
& Welling, 2013).

The generative story behind the VAE (to be described in
detail in the next section) is simple: First, a continuous la-
tent representation is sampled from a multivariate Gaus-
sian. Then, an output is sampled from a distribution pa-
rameterized by a neural decoder, conditioned on the latent
representation. The latent representation (treated as a latent
variable during training) is intended to give the model more
expressive capacity when compared with simpler neural
generative models–for example, conditional language mod-
els. The choice of decoding architecture and ﬁnal output
distribution, which connect the latent representation to out-
put, depends on the kind of data being modeled. The VAE
owes its name to an accompanying variational technique
(Kingma & Welling, 2013) that has been successfully used
to train such models on image data (Gregor et al., 2015;
Salimans et al., 2015; Yan et al., 2016).

The application of VAEs to text data has been far less suc-
cessful (Bowman et al., 2015; Miao et al., 2016). The ob-
vious choice for decoding architecture for a textual VAE
is an LSTM, a typical workhorse in NLP. However, Bow-
man et al. (2015) found that using an LSTM-VAE for text
modeling yields higher perplexity on held-out data than us-
ing an LSTM language model. In particular, they observe
that the LSTM decoder in VAE does not make effective
use of the latent representation during training and, as a re-
sult, VAE collapses into a simple language model. Related
work (Miao et al., 2016; Larochelle & Lauly, 2012; Mnih
& Gregor, 2014) has used simpler decoders that model text
as a bag of words. Their results indicate better use of la-
tent representations, but their decoders cannot effectively
model longer-range dependencies in text and thus under-
perform in terms of ﬁnal perplexity.

Motivated by these observations, we hypothesize that the
contextual capacity of the decoder plays an important role
in whether VAEs effectively condition on the latent repre-
sentation when trained on text data. We propose the use
of a dilated CNN as a decoder in VAE, inspired by the re-
cent success of using CNNs for audio, image and language

1The name VAE is often used to refer to both a model class

and an associated inference procedure.

Improved Variational Autoencoders for Text Modeling using Dilated Convolutions

modeling (van den Oord et al., 2016a; Kalchbrenner et al.,
2016a; van den Oord et al., 2016b). In contrast with prior
work where extremely large CNNs are used, we exploit the
dilated CNN for its ﬂexibility in varying the amount of con-
ditioning context. In the two extremes, depending on the
choice of dilation, the CNN decoder can reproduce a sim-
ple MLP using a bags of words representation of text, or
can reproduce the long-range dependence of recurrent ar-
chitectures (like an LSTM) by conditioning on the entire
history. Thus, by choosing a dilated CNN as the decoder,
we are able to conduct experiments where we vary contex-
tual capacity, ﬁnding a sweet spot where the decoder can
accurately model text but does not yet overpower the latent
representation.

We demonstrate that when this trade-off is correctly man-
aged, textual VAEs can perform substantially better than
simple LSTM language models, a ﬁnding consistent with
recent image modeling experiments using variational lossy
autoencoders (Chen et al., 2016). We go on to show that
VAEs with carefully selected CNN decoders can be quite
effective for semi-supervised classiﬁcation and unsuper-
vised clustering, outperforming several strong baselines
(from (Dai & Le, 2015)) on both text categorization and
sentiment analysis.

Our contributions are as follows: First, we propose the use
of a dilated CNN as a new decoder for VAE. We then empir-
ically evaluate several dilation architectures with different
capacities, ﬁnding that reduced contextual capacity leads
to stronger reliance on latent representations. By picking a
decoder with suitable contextual capacity, we ﬁnd our VAE
performs better than LSTM language models on two data
sets. We also explore the use of dilated CNN VAEs for
semi-supervised classiﬁcation and ﬁnd they perform better
than strong baselines from (Dai & Le, 2015). Finally, we
verify that the same framework can be used effectively for
unsupervised clustering.

2. Model

In this section, we begin by providing background on the
use of variational autoencoders for language modeling.
Then we introduce the dilated CNN architecture that we
will use as a new decoder for VAE in experiments. Finally,
we describe the generalization of VAE that we will use to
conduct experiments on semi-supervised classiﬁcation.

2.1. Background on Variational Autoencoders

Neural language models (Mikolov et al., 2010) typically
generate each token xt conditioned on the entire history of
previously generated tokens:

p(x) =

p(xt|x1, x2, ..., xt−1).

(1)

(cid:89)

t

State-of-the-art language models often parametrize these
conditional probabilities using RNNs, which compute an
evolving hidden state over the text which is used to predict
each xt. This approach, though effective in modeling text,
does not explicitly model variance in higher-level proper-
topic or style) and thus can
ties of entire utterances (e.g.
have difﬁculty with heterogeneous datasets.

Bowman et al. (2015) propose a different approach to gen-
erative text modeling inspired by related work on vision
(Kingma & Welling, 2013). Instead of directly modeling
the joint probability p(x) as in Equation 1, we specify a
generative process for which p(x) is a marginal distribu-
tion. Speciﬁcally, we ﬁrst generate a continuous latent
vector representation z from a multivariate Gaussian prior
pθ(z), and then generate the text sequence x from a con-
ditional distribution pθ(x|z) parameterized using a neural
net (often called the generation model or decoder). Because
this model incorporates a latent variable that modulates the
entire generation of each whole utterance, it may be better
able to capture high-level sources of variation in the data.
Speciﬁcally, in contrast with Equation 1, this generating
distribution conditions on latent vector representation z:

pθ(x|z) =

pθ(xt|x1, x2, ..., xt−1, z).

(2)

(cid:89)

t

To estimate model parameters θ we would ideally
like to maximize the marginal probability pθ(x) =
(cid:82) pθ(z)pθ(x|z)dz. However, computing this marginal is
intractable for many decoder choices. Thus, the follow-
ing variational lower bound is often used as an objective
(Kingma & Welling, 2013):

log pθ(x) = − log

pθ(z)pθ(x|z)dz

(cid:90)

≥ Eqφ(z|x)[log pθ(x|z)] − KL(qφ(z|x)||pθ(z)).

Here, qφ(z|x) is an approximation to the true posterior (of-
ten called the recognition model or encoder) and is param-
eterized by φ. Like the decoder, we have a choice of neu-
ral architecture to parameterize the encoder. However, un-
like the decoder, the choice of encoder does not change the
model class – it only changes the variational approximation
used in training, which is a function of both the model pa-
rameters θ and the approximation parameters φ. Training
seeks to optimize these parameters jointly using stochastic
gradient ascent. A ﬁnal wrinkle of the training procedure
involves a stochastic approximation to the gradients of the
variational objective (which is itself intractable). We omit
details here, noting only that the ﬁnal distribution of the
posterior approximation qφ(z|x) is typically assumed to be
Gaussian so that a re-parametrization trick can be used, and
refer readers to (Kingma & Welling, 2013).

Improved Variational Autoencoders for Text Modeling using Dilated Convolutions

2.2. Training Collapse with Textual VAEs

Together, this combination of generative model and varia-
tional inference procedure are often referred to as a vari-
ational autoencoder (VAE). We can also view the VAE
as a regularized version of the autoencoder. Note, how-
ever, that while VAEs are valid probabilistic models whose
likelihood can be evaluated on held-out data, autoen-
coders are not valid models.
If only the ﬁrst term of
the VAE variational bound Eqφ(z|x)[log pθ(x|z)] is used
as an objective, the variance of the posterior probability
qφ(z|x) will become small and the training procedure re-
duces to an autoencoder.
It is the KL-divergence term,
KL(qφ(z|x)||pθ(z)), that discourages the VAE memoriz-
ing each x as a single latent point.

While the KL term is critical for training VAEs, histor-
ically, instability on text has been evidenced by the KL
term becoming vanishingly small during training, as ob-
served by Bowman et al. (2015). When the training proce-
dure collapses in this way, the result is an encoder that has
duplicated the Gaussian prior (instead of a more interest-
ing posterior), a decoder that completely ignores the latent
variable z, and a learned model that reduces to a simpler
language model. We hypothesize that this collapse con-
dition is related to the contextual capacity of the decoder
architecture. The choice encoder and decoder depends on
the type of data. For images, these are typically MLPs or
CNNs. LSTMs have been used for text, but have resulted in
training collapse as discussed above (Bowman et al., 2015).
Here, we propose to use a dilated CNN as the decoder in-
stead. In one extreme, when the effective contextual width
of a CNN is very large, it resembles the behavior of LSTM.
When the width is very small, it behaves like a bag-of-
words model. The architectural ﬂexibility of dilated CNNs
allows us to change the contextual capacity and conduct
experiments to validate our hypothesis: decoder contextual
capacity and effective use of encoding information are di-
rectly related. We next describe the details of our decoder.

2.3. Dilated Convolutional Decoders

The typical approach to using CNNs used for text genera-
tion (Kalchbrenner et al., 2016a) is similar to that used for
images (Krizhevsky et al., 2012; He et al., 2016), but with
the convolution applied in one dimension. We take this
approach here in deﬁning our decoder.

One dimensional convolution: For a CNN to serve as
a decoder for text, generation of xt must only condition
on past tokens x<t. Applying the traditional convolution
will break this assumption and use tokens x≥t as inputs
to predict xt.
In our decoder, we avoid this by simply
shifting the input by several slots (van den Oord et al.,
2016b). With a convolution with ﬁlter size of k and using
n layers, our effective ﬁlter size (the number of past tokens

(a) VAE training graph using a dilated CNN decoder.

(b) Digram of dilated CNN decoder.

Figure 1: Our training and model architectures for textual
VAE using a dilated CNN decoder.

to condition to in predicting xt) would be (k − 1) × n + 1.
Hence, the ﬁlter size would grow linearly with the depth of
the network.

Dilation: Dilated convolution (Yu & Koltun, 2015) was
introduced to greatly increase the effective receptive ﬁeld
size without increasing the computational cost. With
dilation d, the convolution is applied so that d − 1 inputs
are skipped each step. Causal convolution can be seen
a special case with d = 1. With dilation, the effective
receptive size grows exponentially with network depth. In
Figure 1b, we show dilation of sizes of 1 and 2 in the ﬁrst
and second layer, respectively. Suppose the dilation size in
the i-th layer is di and we use the same ﬁlter size k in all
layers, then the effective ﬁlter size is (k − 1) (cid:80)
i di + 1.
The dilations are typically set
to double every layer
di+1 = 2di, so the effective receptive ﬁeld size can grow
exponentially. Hence, the contextual capacity of a CNN
can be controlled across a greater range by manipulating
the ﬁlter size, dilation size and network depth. We use this
approach in experiments.

use
in

residual
the

con-
decoder

al.,

(He

We
2016)

connection:
Residual
et
nection
to speed up convergence and enable
training of deeper models. We use
a residual block (shown to the right)
similar to that of (Kalchbrenner et al.,
2016a). We use three convolutional
layers with ﬁlter size 1×1, 1×k, 1×1,
respectively, and ReLU activation be-

LSTMzLSTMLSTMtastesreallygreatLSTMencoderCNNDecodertastesreallygreatBOSEOStastesreallygreattastesreallygreatinputembeddingdilation=1dilation=2zReLU 1x1, 512ReLU 1xk, 512convReLU 1x1, 1024+convconvImproved Variational Autoencoders for Text Modeling using Dilated Convolutions

tween convolutional layers.

can be approximated using:

Overall architecture: Our VAE architecture is shown in
Figure 1a. We use LSTM as the encoder to get the pos-
terior probability q(z|x), which we assume to be diagonal
Gaussian. We parametrize the mean µ and variance σ with
LSTM output. We sample z from q(z|x), the decoder is
conditioned on the sample by concatenating z with every
word embedding of the decoder input.

2.4. Semi-supervised VAE

In addition to conducting language modeling experiments,
we will also conduct experiments on semi-supervised clas-
siﬁcation of text using our proposed decoder. In this sec-
tion, we brieﬂy review semi-supervised VAEs of (Kingma
et al., 2014) that incorporate discrete labels as additional
variables. Given the labeled set (x, y) ∼ DL and the unla-
beled set x ∼ DU , (Kingma et al., 2014) proposed a model
whose latent representation contains continuous vector z
and discrete label y:

p(x, y, z) = p(y)p(z)p(x|y, z).

(3)

The semi-supervised VAE ﬁts a discriminative network
q(y|x), an inference network q(z|x, y) and a generative
network p(x|y, z) jointly as part of optimizing a variational
lower bound similar that of basic VAE. For labeled data
(x, y), this bound is:

log p(x, y) ≥ Eq(z|x,y)[log p(x|y, z)]

− KL(q(z|x, y)||p(z)) + log p(y)

=L(x, y) + log p(y).

For unlabeled data x, the label is treated as a latent variable,
yielding:

log p(x) ≥U (x)

= Eq(y|x)

(cid:2) Eq(z|x,y)[log p(x|y, z)]

− KL(q(z|x, y)||p(z)) + log p(y) − log q(y|x)(cid:3)
(cid:88)

q(y|x)L(x, y) − KL(q(y|x)||p(y)).

=

y

Combining the labeled and unlabeled data terms, we have
the overall objective as:

J = E(x,y)∼DL [L(x, y)] + Ex∼DU [U (x)]

+ α E(x,y)∼DL[log q(y|x)],

where α controls the trade off between generative and dis-
criminative terms.

Gumbel-softmax:
Jang et al. (2016); Maddison et al.
(2016) propose a continuous approximation to sampling
from a categorical distribution. Let u be a categorical dis-
tribution with probabilities π1, π2, ..., πc. Samples from u

yi =

exp((log(πi) + gi)/τ )
j=1 exp((log(πj) + gj)/τ )

,

(cid:80)c

(4)

where gi follows Gumbel(0, 1). The approximation is accu-
rate when τ → 0 and smooth when τ > 0. In experiments,
we use Gumbel-Softmax to approximate the samples from
p(y|x) to reduce the computational cost. As a result, we
can directly back propagate the gradients of U (x) to the
discriminator network. We anneal τ so that sample vari-
ance is small when training starts and then gradually de-
crease τ .

Unsupervised clustering:
In this section we adapt the
same framework for unsupervised clustering. We directly
minimize the objective U (x), which is consisted of two
parts: reconstruction loss and KL regularization on q(y|x).
The ﬁrst part encourages the model to assign x to label y
such that the reconstruction loss is low. We ﬁnd that the
model can easily get stuck in two local optimum: the KL
term is very small and q(y|x) is close to uniform distribu-
tion or the KL term is very large and all samples collapse
to one class. In order to make the model more robust, we
modify the KL term by:

KLy = max(γ, KL(q(y|x)|p(y)).

(5)

That is, we only minimize the KL term when it is large
enough.

3. Experiments

3.1. Data sets

Since we would like to investigate VAEs for language
modeling and semi-supervised classiﬁcation, the data sets
should be suitable for both purposes. We use two large
scale document classiﬁcation data sets: Yahoo Answer and
Yelp15 review, representing topic classiﬁcation and senti-
ment classiﬁcation data sets respectively (Tang et al., 2015;
Yang et al., 2016; Zhang et al., 2015). The original data sets
contain millions of samples, of which we sample 100k as
training and 10k as validation and test from the respective
partitions. The detailed statistics of both data sets are in Ta-
ble 1. Yahoo Answer contains 10 topics including Society
& Culture, Science & Mathematics etc. Yelp15 contains 5
level of rating, with higher rating better.

Data

classes

documents

average #w vocabulary

Yahoo
Yelp15

10
5

100k
100k

78
96

200k
90k

Table 1: Data statistics

Improved Variational Autoencoders for Text Modeling using Dilated Convolutions

Model

LSTM-LM
LSTM-VAE∗∗
LSTM-VAE∗∗ + init

Size NLL (KL)
< i
< i
< i

334.9
342.1 (0.0)
339.2 (0.0)

SCNN-LM
SCNN-VAE
SCNN-VAE + init

MCNN-LM
MCNN-VAE
MCNN-VAE + init

LCNN-LM
LCNN-VAE
LCNN-VAE + init

VLCNN-LM
VLCNN-VAE
VLCNN-VAE + init

15
15
15

63
63
63

125
125
125

187
187
187

345.3
337.8 (13.3)
335.9 (13.9)

338.3
336.2 (11.8)
334.6 (12.6)

335.4
333.9 (6.7)
332.1 (10.0)

336.5
336.5 (0.7)
335.8 (3.8)

(a) Yahoo

PPL

66.2
72.5
69.9

75.5
68.7
67.0

69.1
67.3
66.0

66.6
65.4
63.9

67.6
67.6
67.0

Model

LSTM-LM
LSTM-VAE∗∗
LSTM-VAE∗∗ + init

Size NLL (KL)
< i
< i
< i

362.7
372.2 (0.3)
368.9 (4.7)

SCNN-LM
SCNN-VAE
SCNN-VAE + init

MCNN-LM
MCNN-VAE
MCNN-VAE + init

LCNN-LM
LCNN-VAE
LCNN-VAE + init

VLCNN-LM
VLCNN-VAE
VLCNN-VAE + init

15
15
15

63
63
63

125
125
125

187
187
187

371.2
365.6 (9.4)
363.7 (10.3)

366.5
363.0 (6.9)
360.7 (9.1)

363.5
361.9 (6.4)
359.1 (7.6)

364.8
364.3 (2.7)
364.7 (2.2)

(b) Yelp

PPL

42.6
47.0
46.4

46.6
43.9
43.1

44.3
42.8
41.8

43.0
42.3
41.1

43.7
43.4
43.5

Table 2: Language modeling results on the test set. ∗∗ is from (Bowman et al., 2015). We report negative log likelihood
(NLL) and perplexity (PPL) on the test set. The KL component of NLL is given in parentheses. Size indicates the effective
ﬁlter size. VAE + init indicates pretraining of only the encoder using an LSTM LM.

3.2. Model conﬁgurations and Training details

We use an LSTM as an encoder for VAE and explore
LSTMs and CNNs as decoders. For CNNs, we explore sev-
eral different conﬁgurations. We set the convolution ﬁlter
size to be 3 and gradually increase the depth and dilation
from [1, 2, 4], [1, 2, 4, 8, 16] to [1, 2, 4, 8, 16, 1, 2, 4, 8,
16]. They represent small, medium and large model and we
name them as SCNN, MCNN and LCNN. We also explore
a very large model with dilations [1, 2, 4, 8, 16, 1, 2, 4, 8,
16, 1, 2, 4, 8, 16] and name it as VLCNN. The effective
ﬁlter size are 15, 63, 125 and 187 respectively. We use the
last hidden state of the encoder LSTM and feed it though an
MLP to get the mean and variance of q(z|x), from which
we sample z and then feed it through an MLP to get the
starting state of decoder. For the LSTM decoder, we fol-
low (Bowman et al., 2015) to use it as the initial state of
LSTM and feed it to every step of LSTM. For the CNN de-
coder, we concatenate it with the word embedding of every
decoder input.

The architecture of the Semi-supervised VAE basically fol-
lows that of the VAE. We feed the last hidden state of the
encoder LSTM through a two layer MLP then a softmax
to get q(y|x). We use Gumbel-softmax to sample y from
q(y|x). We then concatenate y with the last hidden state of
encoder LSTM and feed them throught an MLP to get the
mean and variance of q(z|y, x). y and z together are used
as the starting state of the decoder.

We use a vocabulary size of 20k for both data sets and set
the word embedding dimension to be 512. The LSTM di-
mension is 1024. The number of channels for convolutions

in CNN decoders is 512 internally and 1024 externally, as
shown in Section 2.3. We select the dimension of z from
[32, 64]. We ﬁnd our model is not sensitive to this parame-
ter.

We use Adam (Kingma & Ba, 2014) to optimize all models
and the learning rate is selected from [2e-3, 1e-3, 7.5e-4]
and β1 is selected from [0.5, 0.9]. Empirically, we ﬁnd
learning rate 1e-3 and β1 = 0.5 to perform the best. We
select drop out ratio of LSTMs (both encoder and decoder)
from [0.3, 0.5]. Following (Bowman et al., 2015), we also
use drop word for the LSTM decoder, the drop word ratio
is selected from [0, 0.3, 0.5, 0.7]. For the CNN decoder,
we use a drop out ratio of 0.1 at each layer. We do not
use drop word for CNN decoders. We use batch size of
32 and all model are trained for 40 epochs. We start to
half the learning rate every 2 epochs after epoch 30. Fol-
lowing (Bowman et al., 2015), we use KL cost annealing
strategy. We set the initial weight of KL cost term to be
0.01 and increase it linearly until a given iteration T . We
treat T as a hyper parameter and select it from [10k, 40k,
80k].

3.3. Language modeling results

The results for language modeling are shown in Table 2.
We report the negative log likelihood (NLL) and perplexity
(PPL) of the test set. For the NLL of VAEs, we decompose
it into reconstruction loss and KL divergence and report the
KL divergence in the parenthesis. To better visualize these
results, we plot the results of Yahoo data set (Table 2a) in
Figure 2.

Improved Variational Autoencoders for Text Modeling using Dilated Convolutions

Figure 2: NLL decomposition of Table 2a. Each group con-
sists of three bars, representing LM, VAE and VAE+init.
For VAE, we decompose the loss into reconstruction loss
and KL divergence, shown in blue and red respectively. We
subtract all loss values with 300 for better visualization.

We ﬁrst look at the LM results for Yahoo data set. As
we gradually increase the effective ﬁlter size of CNN from
SCNN, MCNN to LCNN, the NLL decreases from 345.3,
338.3 to 335.4. The NLL of LCNN-LM is very close to
the NLL of LSTM-LM 334.9. But VLCNN-LM is a lit-
tle bit worse than LCNN-LM, this indicates a little bit of
over-ﬁtting.

We can see that LSTM-VAE is worse than LSTM-LM in
terms of NLL and the KL term is nearly zero, which veriﬁes
the ﬁnding of (Bowman et al., 2015). When we use CNNs
as the decoders for VAEs, we can see improvement over
pure CNN LMs. For SCNN, MCNN and LCNN, the VAE
results improve over LM results from 345.3 to 337.8, 338.3
to 336.2, and 335.4 to 333.9 respectively. The improve-
ment is big for small models and gradually decreases as we
increase the decoder model contextual capacity. When the
model is as large as VLCNN, the improvement diminishes
and the VAE result is almost the same with LM result. This
is also reﬂected in the KL term, SCNN-VAE has the largest
KL of 13.3 and VLCNN-VAE has the smallest KL of 0.7.
When LCNN is used as the decoder, we obtain an opti-
mal trade off between using contextual information and la-
tent representation. LCNN-VAE achieves a NLL of 333.9,
which improves over LSTM-LM with NLL of 334.9.

We ﬁnd that if we initialize the parameters of LSTM en-
coder with parameters of LSTM language model, we can
improve the VAE results further. This indicates better
encoder model is also a key factor for VAEs to work
well. Combined with encoder initialization, LCNN-VAE
improves over LSTM-LM from 334.9 to 332.1 in NLL and
from 66.2 to 63.9 in PPL. Similar results for the sentiment
data set are shown in Table 2b. LCNN-VAE improves over
LSTM-LM from 362.7 to 359.1 in NLL and from 42.6 to
41.1 in PPL.

(a) Yahoo

(b) Yelp

Figure 3: Visualizations of learned latent representations.

Latent representation visualization: In order to visual-
ize the latent representation, we set the dimension of z to
be 2 and plot the mean of posterior probability q(z|x), as
shown in Figure 3. We can see distinct different character-
istics of topic and sentiment representation. In Figure 3a,
we can see that documents of different topics fall into dif-
ferent clusters, while in Figure 3b, documents of different
ratings form a continuum, they lie continuously on the x-
axis as the review rating increases.

Model

ACCU NLL (KL)

LSTM-VAE-Semi
SCNN-VAE-Semi
MCNN-VAE-Semi
LCNN-VAE-Semi

51.9
65.5
64.6
57.2

345.5 (9.3)
335.7 (10.4)
332.8 (7.2)
331.3 (2.7)

Table 3: Semi-supervised VAE ablation results on Yahoo.
We report both the NLL and classiﬁcation accuracy of the
test data. Accuracy is in percentage. Number of labeled
samples is ﬁxed to be 500.

3.4. Semi-supervised VAE results

Motivated by the success of VAEs for language modeling,
we continue to explore VAEs for semi-supervised learning.
Following that of (Kingma et al., 2014), we set the number
of labeled samples to be 100, 500, 1000 and 2000 respec-
tively.

Ablation Study: At ﬁrst, we would like to explore the
effect of different decoders for semi-supervised classiﬁca-
tion. We ﬁx the number of labeled samples to be 500 and
report both classiﬁcation accuracy and NLL of the test set
of Yahoo data set in Table. 5. We can see that SCNN-VAE-
Semi has the best classiﬁcation accuracy of 65.5. The ac-
curacy decreases as we gradually increase the decoder con-
textual capacity. On the other hand, LCNN-VAE-Semi has
the best NLL result. This classiﬁcation accuracy and NLL
trade off once again veriﬁes our conjecture: with small con-
textual window size, the decoder is forced to use the en-
coder information, hence the latent representation is better

NLLLSTMSCNNMCNNLCNNVLCNNLMVAEVAE+initKLImproved Variational Autoencoders for Text Modeling using Dilated Convolutions

Model

LSTM
LA-LSTM (Dai & Le, 2015)
LM-LSTM (Dai & Le, 2015)

SCNN-VAE-Semi
SCNN-VAE-Semi+init

100

10.7
20.8
46.9

55.4
63.8

500

11.9
42.2
61.3

65.6
65.4

1000

2000

Model

14.3
50.4
63.9

66.0
66.6

23.1
54.7
65.6

65.8
67.4

LSTM
LA-LSTM (Dai & Le, 2015)
LM-LSTM (Dai & Le, 2015)

SCNN-VAE-Semi
SCNN-VAE-Semi+init

100

22.6
35.2
46.9

51.4
52.6

500

25.4
46.4
54.1

53.5
57.3

1000

2000

27.9
49.8
57.2

55.3
58.9

29.9
52.2
57.7

57.4
59.8

(a) Yahoo

(b) Yelp

Table 4: Semi-supervised VAE results on the test set, in percentage. LA-LSTM and LM-LSTM come from (Dai & Le,
2015), they denotes the LSTM is initialized with a sequence autoencoder and a language model.

learned.

Comparing the NLL results of Table 5 with that of Ta-
ble 2a, we can see the NLL improves. The NLL of semi-
supervised VAE improves over simple VAE from 337.8 to
335.7 for SCNN, from 336.2 to 332.8 for MCNN, and from
333.9 to 332.8 for LCNN. The improvement mainly comes
from the KL divergence part, this indicates that better la-
tent representations decrease the KL divergence, further
improving the VAE results.

Comparison with related methods: We compare Semi-
supervised VAE with the methods from (Dai & Le, 2015),
which represent
the previous state-of-the-art for semi-
supervised sequence learning. Dai & Le (2015) pre-trains
a classiﬁer by initializing the parameters of a classiﬁer with
that of a language model or a sequence autoencoder. They
ﬁnd it improves the classiﬁcation accuracy signiﬁcantly.
Since SCNN-VAE-Semi performs the best according to Ta-
ble 5, we ﬁx the decoder to be SCNN in this part. The
detailed comparison is in Table 4. We can see that semi-
supervised VAE performs better than LM-LSTM and LA-
LSTM from (Dai & Le, 2015). We also initialize the en-
coder of the VAE with parameters from LM and ﬁnd classi-
ﬁcation accuracy further improves. We also see the advan-
tage of SCNN-VAE-Semi over LM-LSTM is greater when
the number of labeled samples is smaller. The advantage
decreases as we increase the number of labeled samples.
When we set the number of labeled samples to be 25k,
the SCNN-VAE-Semi achieves an accuracy of 70.4, which
is similar to LM-LSTM with an accuracy of 70.5. Also,
SCNN-VAE-Semi performs better on Yahoo data set than
Yelp data set. For Yelp, SCNN-VAE-Semi is a little bit
worse than LM-LSTM if the number of labeled samples is
greater than 100, but becomes better when we initialize the
encoder. Figure 3b explains this observation. It shows the
documents are coupled together and are harder to classify.
Also, the latent representation contains information other
than sentiment, which may not be useful for classiﬁcation.

3.5. Unsupervised clustering results

We also explored using the same framework for unsuper-
vised clustering. We compare with the baselines that ex-

Model

LSTM + GMM
SCNN-VAE + GMM
SCNN-VAE + init + GMM

SCNN-VAE-Unsup + init

ACCU

25.8
56.6
57.0

59.9

Table 5: Unsupervised clustering results for Yahoo data set.
We run each model 10 times and report the best results.
LSTM+GMM means we extract the features from LSTM
language model. SCNN-VAE + GMM means we use the
mean of q(z|x) as the feature. SCNN-VAE + init + GMM
means SCNN-VAE is trained with encoder initialization.

tract the feature with existing models and then run Gaussian
Mixture Model (GMM) on these features. We ﬁnd empir-
ically that simply using the features does not perform well
since the features are high dimensional. We run a PCA on
these features, the dimension of PCA is selected from [8,
16, 32]. Since GMM can easily get stuck in poor local op-
timum, we run each model ten times and report the best
result. We ﬁnd directly optimizing U (x) does not perform
well for unsupervised clustering and we need to initialize
the encoder with LSTM language model. The model only
works well for Yahoo data set. This is potentially because
Figure 3b shows that sentiment latent representations does
not fall into clusters. γ in Equation 5 is a sensitive param-
eter, we select it from the range between 0.5 and 1.5 with
an interval of 0.1. We use the following evaluation pro-
tocol (Makhzani et al., 2015): after we ﬁnish training, for
cluster i, we ﬁnd out the validation sample xn from clus-
ter i that has the best q(yi|x) and assign the label of xn
to all samples in cluster i. We then compute the test ac-
curacy based on this assignment. The detailed results are
in Table 5. We can see SCNN-VAE-Unsup + init performs
better than other baselines. LSTM+GMM performs very
bad probably because the feature dimension is 1024 and is
too high for GMM, even though we already used PCA to
reduce the dimension.

Conditional text generation With the semi-supervised
VAE, we are able to generate text conditional on the la-
bel. Due to space limitation, we only show one example of

Improved Variational Autoencoders for Text Modeling using Dilated Convolutions

1 star

2 star

3 star

4 star

5 star

the food was good but the service was horrible . took forever to get our food . we had to ask
twice for our check after we got our food . will not return .
the food was good , but the service was terrible . took forever to get someone to take our drink
order . had to ask 3 times to get the check . food was ok , nothing to write about .
came here for the ﬁrst time last night . food was good . service was a little slow . food was just
ok .
food was good , service was a little slow , but the food was pretty good . i had the grilled chicken
sandwich and it was really good . will deﬁnitely be back !
food was very good , service was fast and friendly . food was very good as well . will be back !

Table 6: Text generated by conditioning on sentiment label.

generated reviews conditioning on review rating in Table 6.
For each group of generated text, we ﬁx z and vary the la-
bel y, while picking x via beam search with a beam size of
10.

4. Related work

Variational inference via the re-parameterization trick was
initially proposed by (Kingma & Welling, 2013; Rezende
et al., 2014) and since then, VAE has been widely adopted
as generative model for images (Gregor et al., 2015; Yan
et al., 2016; Salimans et al., 2015; Gregor et al., 2016; Hu
et al., 2017b).

Our work is in line with previous works on combining
variational inferences with text modeling (Bowman et al.,
2015; Miao et al., 2016; Serban et al., 2016; Zhang et al.,
2016; Hu et al., 2017a). (Bowman et al., 2015) is the ﬁrst
work to combine VAE with language model and they use
LSTM as the decoder and ﬁnd some negative results. On
the other hand, (Miao et al., 2016) models text as bag of
words, though improvement has been found, the model can
not be used to generate text. Our work ﬁlls the gaps be-
tween them. (Serban et al., 2016; Zhang et al., 2016) ap-
plies variational inference to dialogue modeling and ma-
chine translation and found some improvement in terms of
generated text quality, but no language modeling results are
reported. (Chung et al., 2015; Bayer & Osendorfer, 2014;
Fraccaro et al., 2016) embedded variational units in every
step of a RNN, which is different from our model in using
global latent variables to learn high level features.

Our use of CNN as decoder is inspired by recent success of
PixelCNN model for images (van den Oord et al., 2016b),
WaveNet for audios (van den Oord et al., 2016a), Video
Pixel Network for video modeling (Kalchbrenner et al.,
2016b) and ByteNet for machine translation (Kalchbrenner
et al., 2016a). But in contrast to those works showing using
a very deep architecture leads to better performance, CNN
as decoder is used in our model to control the contextual
capacity, leading to better performance.

Our work is closed related the recently proposed variational
lossy autoencoder (Chen et al., 2016) which is used to pre-

dict image pixels. They ﬁnd that conditioning on a smaller
window of a pixels leads to better results with VAE, which
is similar to our ﬁnding. Much (Rezende & Mohamed,
2015; Kingma et al., 2016; Chen et al., 2016) has been done
to come up more powerful prior/posterior distribution rep-
resentations with techniques such as normalizing ﬂows. We
treat this as one of our future works. This work is largely
orthogonal and could be potentially combined with a more
effective choice of decoder to yield additional gains.

There is much previous work exploring unsupervised sen-
tence encodings, for example skip-thought vectors (Kiros
et al., 2015), paragraph vectors (Le & Mikolov, 2014), and
sequence autoencoders (Dai & Le, 2015). (Dai & Le, 2015)
applies a pretrained model to semi-supervised classiﬁcation
and ﬁnd signiﬁcant gains, we use this as the baseline for our
semi-supervised VAE.

5. Conclusion

We showed that by controlling the decoder’s contextual ca-
pacity in VAE, we can improve performance on both lan-
guage modeling and semi-supervised classiﬁcation tasks by
preventing a degenerate collapse of the training procedure.
These results indicate that more carefully characterizing
decoder capacity and understanding how it relates to com-
mon variational training procedures may represent impor-
tant avenues for unlocking future unsupervised problems.

References

Bayer, Justin and Osendorfer, Christian. Learning stochas-
tic recurrent networks. arXiv preprint arXiv:1411.7610,
2014.

Bowman, Samuel R, Vilnis, Luke, Vinyals, Oriol, Dai, An-
drew M, Jozefowicz, Rafal, and Bengio, Samy. Gener-
ating sentences from a continuous space. arXiv preprint
arXiv:1511.06349, 2015.

Chen, Xi, Kingma, Diederik P, Salimans, Tim, Duan, Yan,
Dhariwal, Prafulla, Schulman, John, Sutskever, Ilya, and
Abbeel, Pieter. Variational lossy autoencoder. arXiv
preprint arXiv:1611.02731, 2016.

Improved Variational Autoencoders for Text Modeling using Dilated Convolutions

Chung, Junyoung, Kastner, Kyle, Dinh, Laurent, Goel,
Kratarth, Courville, Aaron C, and Bengio, Yoshua. A
recurrent latent variable model for sequential data.
In
Advances in neural information processing systems, pp.
2980–2988, 2015.

Kingma, Diederik P, Mohamed, Shakir, Rezende,
Danilo Jimenez, and Welling, Max. Semi-supervised
In Advances in
learning with deep generative models.
Neural Information Processing Systems, pp. 3581–3589,
2014.

Dai, Andrew M and Le, Quoc V. Semi-supervised sequence
learning. In Advances in Neural Information Processing
Systems, pp. 3079–3087, 2015.

Kingma, Diederik P, Salimans, Tim, and Welling, Max. Im-
proving variational inference with inverse autoregressive
ﬂow. arXiv preprint arXiv:1606.04934, 2016.

Fraccaro, Marco, Sønderby, Søren Kaae, Paquet, Ul-
rich, and Winther, Ole. Sequential neural models with
In Advances in Neural Information
stochastic layers.
Processing Systems, pp. 2199–2207, 2016.

Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan R, Zemel,
Richard, Urtasun, Raquel, Torralba, Antonio, and Fidler,
Sanja. Skip-thought vectors. In Advances in neural in-
formation processing systems, pp. 3294–3302, 2015.

Gregor, Karol, Danihelka, Ivo, Graves, Alex, Rezende,
Danilo Jimenez, and Wierstra, Daan. Draw: A recur-
rent neural network for image generation. arXiv preprint
arXiv:1502.04623, 2015.

Gregor, Karol, Besse, Frederic, Rezende, Danilo Jimenez,
Danihelka, Ivo, and Wierstra, Daan. Towards concep-
In Advances In Neural Information
tual compression.
Processing Systems, pp. 3549–3557, 2016.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, pp. 770–778, 2016.

Hu, Zhiting, Yang, Zichao, Liang, Xiaodan, Salakhutdinov,
Ruslan, and Xing, Eric P. Controllable text generation.
arXiv preprint arXiv:1703.00955, 2017a.

Hu, Zhiting, Yang, Zichao, Salakhutdinov, Ruslan, and
Xing, Eric P. On unifying deep generative models. arXiv
preprint arXiv:1706.00550, 2017b.

Jang, Eric, Gu, Shixiang, and Poole, Ben. Categorical
reparameterization with gumbel-softmax. arXiv preprint
arXiv:1611.01144, 2016.

Kalchbrenner, Nal, Espeholt, Lasse, Simonyan, Karen,
Oord, Aaron van den, Graves, Alex, and Kavukcuoglu,
Koray. Neural machine translation in linear time. arXiv
preprint arXiv:1610.10099, 2016a.

Kalchbrenner, Nal, Oord, Aaron van den, Simonyan,
Karen, Danihelka, Ivo, Vinyals, Oriol, Graves, Alex,
and Kavukcuoglu, Koray. Video pixel networks. arXiv
preprint arXiv:1610.00527, 2016b.

Kingma, Diederik and Ba,

Jimmy.
method for stochastic optimization.
arXiv:1412.6980, 2014.

Adam:

A
arXiv preprint

Kingma, Diederik P and Welling, Max. Auto-encoding
arXiv preprint arXiv:1312.6114,

variational bayes.
2013.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in neural information processing
systems, pp. 1097–1105, 2012.

Larochelle, Hugo and Lauly, Stanislas. A neural autore-
gressive topic model. In Advances in Neural Information
Processing Systems, pp. 2708–2716, 2012.

Le, Quoc V and Mikolov, Tomas. Distributed representa-
tions of sentences and documents. In ICML, volume 14,
pp. 1188–1196, 2014.

concrete distribution:

Maddison, Chris J, Mnih, Andriy, and Teh, Yee Whye.
relax-
arXiv preprint

The
ation of discrete random variables.
arXiv:1611.00712, 2016.

A continuous

Makhzani, Alireza, Shlens, Jonathon, Jaitly, Navdeep,
Goodfellow, Ian, and Frey, Brendan. Adversarial autoen-
coders. arXiv preprint arXiv:1511.05644, 2015.

Miao, Yishu, Yu, Lei, and Blunsom, Phil. Neural vari-
In Proc. ICML,

ational inference for text processing.
2016.

Mikolov, Tomas, Karaﬁ´at, Martin, Burget, Lukas, Cer-
nock`y, Jan, and Khudanpur, Sanjeev. Recurrent neu-
ral network based language model. In Interspeech, vol-
ume 2, pp. 3, 2010.

Mnih, Andriy and Gregor, Karol. Neural variational in-
ference and learning in belief networks. arXiv preprint
arXiv:1402.0030, 2014.

Rezende, Danilo Jimenez and Mohamed, Shakir. Varia-
tional inference with normalizing ﬂows. arXiv preprint
arXiv:1505.05770, 2015.

Rezende, Danilo Jimenez, Mohamed, Shakir, and Wier-
stra, Daan. Stochastic backpropagation and approxi-
mate inference in deep generative models. arXiv preprint
arXiv:1401.4082, 2014.

Improved Variational Autoencoders for Text Modeling using Dilated Convolutions

Salimans, Tim, Kingma, Diederik P, Welling, Max, et al.
Markov chain monte carlo and variational inference:
Bridging the gap. In ICML, volume 37, pp. 1218–1226,
2015.

Serban, Iulian Vlad, Sordoni, Alessandro, Lowe, Ryan,
Charlin, Laurent, Pineau, Joelle, Courville, Aaron, and
Bengio, Yoshua. A hierarchical latent variable encoder-
decoder model for generating dialogues. arXiv preprint
arXiv:1605.06069, 2016.

Tang, Duyu, Qin, Bing, and Liu, Ting. Document mod-
eling with gated recurrent neural network for sentiment
classiﬁcation. In EMNLP, pp. 1422–1432, 2015.

van den Oord, A¨aron, Dieleman, Sander, Zen, Heiga, Si-
monyan, Karen, Vinyals, Oriol, Graves, Alex, Kalch-
brenner, Nal, Senior, Andrew, and Kavukcuoglu, Ko-
ray. Wavenet: A generative model for raw audio. CoRR
abs/1609.03499, 2016a.

van den Oord, Aaron, Kalchbrenner, Nal, Espeholt, Lasse,
Vinyals, Oriol, Graves, Alex, et al. Conditional im-
age generation with pixelcnn decoders. In Advances in
Neural Information Processing Systems, pp. 4790–4798,
2016b.

Yan, Xinchen, Yang, Jimei, Sohn, Kihyuk, and Lee,
Honglak. Attribute2image: Conditional image genera-
tion from visual attributes. In European Conference on
Computer Vision, pp. 776–791. Springer, 2016.

Yang, Zichao, Yang, Diyi, Dyer, Chris, He, Xiaodong,
Smola, Alex, and Hovy, Eduard. Hierarchical attention
networks for document classiﬁcation. In Proceedings of
NAACL-HLT, pp. 1480–1489, 2016.

Yu, Fisher and Koltun, Vladlen. Multi-scale context
arXiv preprint

aggregation by dilated convolutions.
arXiv:1511.07122, 2015.

Zhang, Biao, Xiong, Deyi, Su, Jinsong, Duan, Hong, and
Zhang, Min. Variational neural machine translation.
arXiv preprint arXiv:1605.07869, 2016.

Zhang, Xiang, Zhao, Junbo, and LeCun, Yann. Character-
level convolutional networks for text classiﬁcation.
In
Advances in neural information processing systems, pp.
649–657, 2015.

