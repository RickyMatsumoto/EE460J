Supplementary Material: Asynchronous Stochastic Gradient Descent with
Delay Compensation

A. Theorem 3.1 and Its Proof

Theorem 3.1:
Assume the loss function is L1-Lipschitz. If (cid:21) 2 [0; 1] make the following inequality holds,

K∑

k=1

1
(cid:27)3
k(x; wt)

2

(cid:21) 2

4Cij

(

K∑

k=1

)2

1
(cid:27)k(x; wt)

3

′

+ C

ijL2

1jϵtj

5 ;

where Cij = 1
ij =
is smaller than the MSE of G(wt) in approximating Hessian H(wt).

(cid:11) )2, C

1+(cid:21) ( uiuj (cid:12)

lilj

p

1

′

(1+(cid:21))(cid:11)(lilj )2 , and the model converges to the optimal model, then the MSE of (cid:21)G(wt)

Proof:
For simplicity, we abbreviate E(Y jx;w(cid:3)) as E, Gt as G(wt) and Ht as H(wt). First, we calculate the MSE of Gt, (cid:21)Gt
to approximate Ht for each element of Gt. We denote the element in the i-th row and j-th column of G(wt) as Gt
ij and
H(wt) as Hij(t).
The MSE of Gt

ij:

E(Gt

ij (cid:0) EH t

ij)2 = E(Gt

ij (cid:0) EGt

ij)2 + (EH t

ij (cid:0) EGt

ij)2 = E(Gt

ij)2 (cid:0) (EGt

ij)2 + ϵ2
t

(2)

The MSE of (cid:21)gij:

E((cid:21)Gt

ij (cid:0) EH t

ij)2 = (cid:21)2E(Gt
= (cid:21)2E(Gt

ij)2 + (EH t

ij (cid:0) EGt
ij)2 (cid:0) (cid:21)2(EGt

ij)2
ij (cid:0) (cid:21)EGt
ij)2 + (1 (cid:0) (cid:21))2(EGt

ij)2 + ϵ2

t + 2((cid:21) (cid:0) 1)EGt

ijϵt

The condition for E(Gt
ij

(cid:0) EH t

ij)2 (cid:21) E((cid:21)Gt
ij

(cid:0) EH t

ij)2 is

(1 (cid:0) (cid:21)2)(E(Gt

ij)2 (cid:0) (EGt

ij)2) (cid:21) 2(1 (cid:0) (cid:21))(EGt

ij)2 + 2((cid:21) (cid:0) 1)EGt

ijϵt

Inequality (4) is equivalent to

(1 + (cid:21))E(Gt

ij)2 (cid:21) 2[(EGt

ij)2 (cid:0) EGt

ijϵt]

Next we calculate E(Gt

ij)2, and (EGt

ij)2 which appear in Eqn.(5). For simplicity, we denote (cid:27)k(x; wt) as (cid:27)k, and I[Y =k]

(1)

(3)

(4)

(5)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

as zk. Then we can get:

E(gij)2 = E(Y jx;wt)

)

2

(

@
@wj

)
2

log P (Y jx; wt)

(

(

@
@wi
K∑

log P (Y jx; wt)
))4

(

(cid:0) zk
(cid:27)k

(lilj)2

k=1
K∑

k=1
K∑

)

1
(cid:27)3
k(x; wt)
(

@(cid:27)k
@wi

(cid:0) zk
(cid:27)k

k=1
(

K∑

k=1

1
(cid:27)k(x; wt)

)

(cid:1)

)2

:

(cid:21) E(Y jx;w(cid:3))
(

= (cid:11) (lilj)2

(

(cid:20) (cid:12)2 (uiuj)2

(Ehij)2 =

E(Y jx;w(cid:3))

(

))2

K∑

k=1

@(cid:27)k
@wj

(cid:0) zk
(cid:27)k

By substituting Ineq.(7) and Ineq.(8) into Ineq.(5), a sufﬁcient condition for Ineq.(5) to be satisﬁed is

[

(∑

2

Cij

)
2

]

K
k=1

1
(cid:27)k(x;wt)

′

+ C

ijL2

1jϵtj

because Gt
ij

(cid:20) L2

1. (cid:3)

∑

K
k=1

1
k(x;wt)

(cid:27)3

(cid:21)

B. Corollary 3.2 and Its Proof

Corollary 3.2: A sufﬁcient condition for inequality (1) is (cid:21) 2 [0; 1] and 9k0 2 [K] such that (cid:27)k0
[
1 (cid:0)

; 1

]
.

2

2Cij K2 and F ((cid:27)1; :::; (cid:27)K) =

2 [1 (cid:0) ∆; 1], we have for k ̸= k1 (cid:27)k 2 [0; ∆]. Therefore

∑

K
k=1

1
k(x;wt)

(cid:27)3

(cid:0) 2Cij

(∑
K
k=1

1
(cid:27)k(x;wt)

)2

(cid:0) 2C

′

ijL2
1

jϵtj. If 9k1 2 [K] such

K(cid:0)1
′
ij L2
2(Cij K2+C

1ϵt)

Proof:
Denote ∆ = K(cid:0)1
that (cid:27)k1

F ((cid:27)1; :::; (cid:27)K) (cid:21) 1

((cid:27)k1 )3 +

K (cid:0) 1
∆3

((

(cid:0) 2Cij

(

1
(cid:27)k1
)2

)2

+

K (cid:0) 1
∆

′

(cid:0) 2C
ijL2
1
)

jϵtj

(cid:21) K (cid:0) 1
∆3
(cid:21) K (cid:0) 1
∆3
(

(cid:0) 2Cij

(cid:0) 2Cij

(

K (cid:0) 1
∆
(K (cid:0) 1)2
∆2
(

+

+

1
(cid:27)2
k1
2K (cid:0) 1
(cid:27)k1 ∆

)

+

(

K (cid:0) 1
∆2
K (cid:0) 1
∆2
K (cid:0) 1
∆

(

=

1
∆
(cid:21) 1
∆
(cid:21) 1
∆2

= 0

(cid:0) 2Cij

(cid:0) 2Cij

(

(K (cid:0) 1)2
∆

+

2K (cid:0) 1
(cid:27)k1

(K (cid:0) 1)2 + 2K (cid:0) 1
∆

)

))

(cid:0) 2CijK 2 (cid:0) 2C

′

ijL2
1

jϵtj

2(K (cid:0) 1)
(cid:27)k1 ∆

′

(cid:0) 2C

ijL2
1

jϵtj

ijL2
1

jϵtj

′

(cid:0) 2C
))

′

(cid:0) 2C

ijL2
1

jϵtj

′

(cid:0) 2C

ijL2
1

jϵtj

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

(14)

(15)

where Ineq.(11) and (13) is established since (cid:27)k1 > ∆; and Eqn.(15) is established by putting ∆ =
Eqn.(14). (cid:3)

K(cid:0)1
′
ij L2
2(Cij K2+C
1

jϵtj)

in

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

C. Uniform upper bound of MSE

Lemma C.1 Assume the loss function is L1-Lipschitz, and the diagonalization error of Hessian is upper bounded by ϵD,
i.e., jjDiag(H(wt)) (cid:0) H(wt)jj (cid:20) ϵD, 1 then we have, for 8t,

mset(Diag((cid:21)G)) (cid:20) 4(cid:21)2V1 + 4(1 (cid:0) (cid:21))2L4

1 + 4ϵ2

t + 4ϵD;

where V1 is the upper bound of the variance of G(wt).

Proof:

mset(Diag((cid:21)G))

(cid:20)E∥Diag((cid:21)G(wt)) (cid:0) H(wt)∥2
(cid:20)4E∥Diag((cid:21)G(wt)) (cid:0) E(Diag((cid:21)G(wt)))∥2 + 4∥E(Diag((cid:21)G(wt))) (cid:0) E(Diag(G(wt)))∥2

+ 4∥E(Diag(G(wt))) (cid:0) E(Diag(H(wt)))∥2 + 4∥E(Diag(H(wt))) (cid:0) EH(wt)∥2

(cid:20)4(cid:21)2V1 + 4(1 (cid:0) (cid:21))2L4

1 + 4ϵ2

t + 4ϵD

D. Convergence Rate for DC-ASGD: Convex Case

∥w∥2 strongly convex. Thus, we assume that the objective function is (cid:22)-strongly convex.

DC-ASGD is a general method to compensate delay in ASGD. We ﬁrst show the convergence rate for convex loss function.
If the loss function f (w) is convex about w, we can add a regularization term (cid:26)
∥w∥2 to make the objective function
2
F (w) + (cid:26)
2
Theorem 4.1: (Strongly Convex) If f (w) is L2-smooth and (cid:22)-strongly convex about w, ∇f (w) is L3-smooth about w
and the expectation of the ∥ (cid:1) ∥2
2 norm of the delay compensated gradient is upper bounded by a constant G. By setting the
learning rate (cid:17)t = 1

(cid:22)t , DC-ASGD has convergence rate as

2G2
EF (wt) (cid:0) F (w(cid:3)) (cid:20) 2L2
t(cid:22)4

(1 + 4(cid:28) C(cid:21)) +

p
(cid:28)

2G2L2
2(cid:18)
p
(cid:22)4t
t

+

L3L3

2(cid:28) 2G3
(cid:22)6t2

;

√

where (cid:18) = 2HKLG
sampling of DC-ASGD and E(yjx;w(cid:3)).

L2
(cid:22) (1 + (cid:28) GL3
(cid:22)L2

(cid:22)

) and C(cid:21) = (1 (cid:0) (cid:21))L2

1 + ϵD, and the expectation is taking with respect to the random

Proof:
We denote gdc(wt) = g(wt) + (cid:21)g(wt) ⊙ g(wt) ⊙ (wt+(cid:28) (cid:0) wt), gh(wt) = g(wt) + Hit (wt)(wt+(cid:28) (cid:0) wt) and ∇F h(wt) =
∇F (wt) + EitHit (wt)(wt+(cid:28) (cid:0) wt). Obviously, we have Egh(wt) = ∇F h(wt). By the smoothness condition, we have

EF (wt+(cid:28) +1) (cid:0) F (w

(cid:3)

)

(cid:20) F (wt+(cid:28) ) (cid:0) F (w

(cid:20) F (wt+(cid:28) ) (cid:0) F (w

= F (wt+(cid:28) ) (cid:0) F (w

(cid:3)

(cid:3)

(cid:3)

) (cid:0) ⟨∇F (wt+(cid:28) ); wt+(cid:28) +1 (cid:0) wt+(cid:28) ⟩ +

L2
2
L2(cid:17)2
t+(cid:28) G2
2
) (cid:0) (cid:17)t+(cid:28) ⟨∇F (wt+(cid:28) ); ∇F (wt+(cid:28) )⟩ + (cid:17)t+(cid:28) ⟨∇F (wt+(cid:28) ); ∇F (wt+(cid:28) ) (cid:0) ∇F h(wt)⟩

) (cid:0) (cid:17)t+(cid:28) ⟨∇F (wt+(cid:28) ); gdc(wt)⟩ +

∥wt+(cid:28) +1 (cid:0) wt+(cid:28) ∥2

+(cid:17)t+(cid:28) ⟨∇F (wt+(cid:28) ); Egh(wt) (cid:0) gdc(wt)⟩ +

L2(cid:17)2
t+(cid:28) G2
2

Since f (w) is L2-smooth and (cid:22) strongly convex, we have

(cid:0)⟨∇F (wt+(cid:28) ); ∇F (wt+(cid:28) )⟩ (cid:20) (cid:0)(cid:22)2∥wt+(cid:28) (cid:0) w

(F (wt+(cid:28) ) (cid:0) F (w

(cid:3)

)):

(cid:3)∥2 (cid:20) (cid:0) 2(cid:22)2
L2

1(LeCun, 1987) demonstrated that the diagonal approximation to Hessian for neural networks is an efﬁcient method with no much

drop on accuracy

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

(26)

(27)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

For the term (cid:17)t+(cid:28) ⟨∇F (wt+(cid:28) ); ∇F (wt+(cid:28) ) (cid:0) ∇F h(wt)⟩, we have

(cid:17)t+(cid:28) ⟨∇F (wt+(cid:28) ); ∇F (wt+(cid:28) ) (cid:0) ∇F h(wt)⟩
(cid:20) (cid:17)t+(cid:28) ∥∇F (wt+(cid:28) )∥∥∇F (wt+(cid:28) ) (cid:0) ∇F h(wt)∥
(cid:20) (cid:17)t+(cid:28) G∥∇F (wt+(cid:28) ) (cid:0) ∇F h(wt)∥

By the smoothness condition for ∇F (w), we have

∥∇F (wt+(cid:28) ) (cid:0) ∇F h(wt)∥ (cid:20) L3
2

∥wt+(cid:28) (cid:0) wt∥2 (cid:20) L3(cid:28) G2

2

(cid:28) (cid:0)1∑

j=0

(cid:17)2
t+j

∑

Let (cid:17)t = L2

(cid:22)2t , we can get

(cid:28)
j=1 (cid:17)2
For the term (cid:17)t+(cid:28) ⟨∇F (wt+(cid:28) ); Egh(wt) (cid:0) gdc(wt)⟩, we have

(cid:28)
t(t+(cid:28) )

(cid:20) L2
(cid:22)4 (cid:1)

(cid:20) 2L2
2(cid:28)
(cid:22)4(t+(cid:28) )2 .

t+j

2

⟨∇F (wt+(cid:28) ); E(gh(wt) (cid:0) gdc(wt))⟩
(cid:20) ∥∇F (wt+(cid:28) )∥∥E((cid:21)g(wt) ⊙ g(wt) (cid:0) H(wt))(wt+(cid:28) (cid:0) wt)∥

(cid:28) (cid:0)1∑

j=0

(cid:20) 2G2L2(cid:28)

(t + (cid:28) )(cid:22)2 (C(cid:21) + ϵt);

where C(cid:21) = (1 (cid:0) (cid:21))L2

1 + ϵD.
√

√

(cid:20) G2(cid:28)

(cid:17)t+j(∥E((cid:21)g(wt) ⊙ g(wt) (cid:0) g(wt) ⊙ g(wt)∥ + ∥g(wt) ⊙ g(wt) (cid:0) Diag(H(wt))∥ + ∥Diag(H(wt)) (cid:0) H(wt)∥)

Using Lemma F.1, ϵt (cid:20) (cid:18)

(cid:20) (cid:18)

1
t

(cid:28)
t+(cid:28) . Putting inequality 27 and 31 in inequality 26, we have

EF (wt+(cid:28) +1) (cid:0) F (w

(cid:3)

) (cid:20)

(

)

1 (cid:0) 2
t + (cid:28)

(EF (wt) (cid:0) F (w
)
√

(

(cid:3)

)) +

+

2G2L2
2(cid:28)
(cid:22)4(t + (cid:28) )2

C(cid:21) + (cid:18)

(cid:28)
t + (cid:28)

+

L3L3
2(cid:28) 2G3
(cid:22)6(t + (cid:28) )3
L2

2G2
2(t + (cid:28) )2(cid:22)4

EF (wt) (cid:0) F (w

(cid:3)

2G2
) (cid:20) 2L2
t(cid:22)4

(1 + 4(cid:28) C(cid:21)) +

p

(cid:28)

2G2L2
2(cid:18)
p
(cid:22)4t
t

+

L3L3

2(cid:28) 2G3
(cid:22)6t2

:

We can get

by induction. (cid:3)

Discussion:
(1). Following the above proof steps and using ∥∇F (wt+(cid:28) ) (cid:0) ∇F (wt)∥ (cid:20) L2∥wt+(cid:28) (cid:0) wt∥, we can get the convergence
rate of ASGD is

EF (wt) (cid:0) F (w

(1 + 4(cid:28) L2) :

(cid:3)

2G2
) (cid:20) 2L2
t(cid:22)4

p

Compared the convergence rate of DC-ASGD with ASGD, the extra term 2G2L2
2(cid:18)
p
(cid:22)4t
t
2G2
than 2L2
t(cid:22)4
t is large and the term can be neglected. Then the condition for DC-ASGD outperforming ASGD is L2 > C(cid:21).

(1 + 4(cid:28) C(cid:21)) in terms of the order of t. Thus, when t is large, the extra term has smaller value. We assume that

converge to zero faster

2(cid:28) 2G3
(cid:22)6t2

+ L3L3

(cid:28)

E. Convergence Rate for DC-ASGD: Nonconvex Case

Theorem 5.1: (Nonconvex Case) Assume that Assumptions 1-4 hold. Set the learning rate

√

(cid:17)t =

2(F (w1) (cid:0) F (w(cid:3))
bT V 2L2

;

(28)

(29)

(30)

(31)

(32)
(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

where b is the mini-batch size, and V is the upper bound of the variance of the delay-compensated gradient. If T (cid:21)
maxfO(1=r4); 2D0bL2=V 2g and delay (cid:28) is upper-bounded as below,

{

√

√

√

√

(cid:28) (cid:20) min

L2V
C(cid:21)

L2T
2D0b

;

V
C(cid:21)

L2T
2D0b

;

T V
~C

L2
bD0

;

V L2T
4 ~C

T L2
2D0b

}

:

then DC-ASGD has the following ergodic convergence rate,

min
t=f1;(cid:1)(cid:1)(cid:1) ;T g

E(∥∇F (wt)∥2) (cid:20) V

√

2D0L2
bT

;

where the expectation is taken with respect to the random sampling in SGD and the data distribution P (Y jx; w(cid:3)).

Proof:
We denote gm(wt) + (cid:21)gm(wt) ⊙ gm(wt) ⊙ (wt+(cid:28) (cid:0) wt) as gdc
minibatch. From the proof the Theorem 1 in ASGD (Lian et al., 2015), we can get

m (wt) where m 2 f1; (cid:1) (cid:1) (cid:1) ; bg is the index of instances in the

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

b∑

m=1
1

A

(cid:13)
(cid:13)
2
(cid:13)

b∑

m=1

EF (wt+(cid:28) +1) (cid:0) F (wt+(cid:28) )

(cid:20) ⟨∇F (wt+(cid:28) ); wt+(cid:28) (cid:0) wt⟩ +

(cid:20) (cid:0)(cid:17)t+(cid:28) ⟨∇F (wt+(cid:28) );

Egdc

m (wt)⟩ +

L2
2

∥wt+(cid:28) +1 (cid:0) wt+(cid:28) ∥2
0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:17)2
t+(cid:28) L2
2

@

E

b∑

m=1

gdc
m (wt)

b∑

m=1

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Egdc

m (wt)

(cid:0)

∇F (wt+(cid:28) ) (cid:0)

Egdc

m (wt)

2

1

A

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

1

A

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

b∑

m=1

0

(cid:20) (cid:0) b(cid:17)t+(cid:28)

@∥∇F (wt+(cid:28) )∥2 +

2

+

(cid:17)2
t+(cid:28) L2
2

0

@

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

b∑

m=1

gdc
m (wt)

For the term T1 =

(cid:13)
(cid:13)
(cid:13)∇F (wt+(cid:28) ) (cid:0)

∑

b
m=1

Egdc

m (wt)

, by using the smooth condition of g, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

L3
2

T1 =

∇F (wt+(cid:28) ) (cid:0)

Egdc

m (wt)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:20)

∇F (wt+(cid:28) ) (cid:0) ∇F h(wt) + ∇F h(wt) (cid:0)

Egdc

m (wt)

(cid:20) 2

∥wt+(cid:28) (cid:0) wt∥2

+ 2

∇F h(wt) (cid:0)

Egdc

m (wt)

(cid:20) (L2

3(cid:25)2=2 + 2(((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2

m=1
t ))∥wt+(cid:28) (cid:0) wt∥2

b∑

m=1
b∑

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(

Thus by following the proof of ASGD, we have

E(T1) (cid:20) 4(L2

3(cid:25)2=4 + ((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2
t )

b(cid:28) (cid:17)2

t+(cid:28) V 2 + (cid:28) 2(cid:17)2

t+(cid:28)

(cid:13)
(cid:13)
(cid:13)bEgdc

m (wt)

)

(cid:13)
(cid:13)
(cid:13)

2

:

For the term T2 = E

b
m=1 gdc

m (wt)

, it has

((cid:13)
(cid:13)
(cid:13)

∑

)

(cid:13)
(cid:13)
2
(cid:13)

E(T2) (cid:20) bV 2 +

(cid:13)
(cid:13)
(cid:13)bEgdc

m (wt)

(cid:13)
(cid:13)
(cid:13)

2

:

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

By putting Ineq.(51) and Ineq.(52) in Ineq.(46), we can get

(cid:20)

(

E∥∇F (wt+(cid:28) )∥2 +

E(F (wt+(cid:28) +1) (cid:0) F (wt+(cid:28) )
(cid:0) b(cid:17)t+(cid:28)
2
(
(cid:17)2
t+(cid:28) bL2
2

+ (L2

+

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

(cid:17)2
t+(cid:28) L2
2

(cid:0) (cid:17)t+(cid:28)
2b

)

((cid:13)
(cid:13)
(cid:13)bEgdc

E

)

2

(cid:13)
(cid:13)
(cid:13)
m (wt)
)

t )b2(cid:28) (cid:17)3

1 + ϵD)2 + ϵ2
((cid:13)
(cid:13)
(cid:13)bEgdc

t+(cid:28) E

t )b(cid:28) 2(cid:17)3

t+(cid:28)

m (wt)

V 2
)
(cid:13)
(cid:13)
(cid:13)

2

+(L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2

Summarizing the Ineq.(55) from t = 1 to t + (cid:28) = T , we have

EF (wT +1) (cid:0) F (w1)

T∑

(cid:20) (cid:0) b
2

t=1
(

T∑

+

t=1

(cid:17)2
t L2
2

(

T∑

t=1

(cid:17)2
t+(cid:28) bL2
2

(cid:17)tE∥∇F (wt)∥2 +

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2

t )b2(cid:28) (cid:17)3

t+(cid:28)

V 2

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2

t )b(cid:28) 2(cid:17)3

m (wmaxft(cid:0)(cid:28);1g)

)

(cid:13)
(cid:13)
(cid:13)bEgdc

E

t (cid:0) (cid:17)t
2b

)

(cid:13)
(cid:13)
(cid:13)

2

:

By Lemma F.1 and under our assumptions, we have when t > T0, wt will goes into a strongly convex neighbourhood of
some local optimal wloc. Thus, ϵt (cid:20) ϵnc + (cid:18)

1=(t (cid:0) T0), when t > T0 and ϵt < maxs21;(cid:1)(cid:1)(cid:1) ;T0 ϵs when t < T0.

√

√

Let (cid:17)t =

2(F (w1)(cid:0)F (w(cid:3))
bT V 2L2

. It follows that

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2

t )b(cid:28) 2(cid:17)2
t

}

T∑

(cid:17)tL2
2

t=1
T∑

{

(cid:20)

t=1

(cid:17)tL2
2

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + 2ϵ2

nc)b(cid:28) 2(cid:17)2
t

+ 2b(cid:28) 2(cid:17)2

t (4T0 max

(ϵs)2 + 4(cid:18)2 log(T (cid:0) T0))

(60)

s21;(cid:1)(cid:1)(cid:1) ;T0

We ignore the log(T (cid:0) T0) term and regards ~C 2 = 4T0 maxs21;(cid:1)(cid:1)(cid:1) ;T0 (ϵs)2 + 4(cid:18)2 log(T (cid:0) T0) as a constant, which yields

T∑

(cid:17)tL2
2

t=1
T∑

{

(cid:20)

t=1

(cid:17)tL2
2

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2

t )b(cid:28) 2(cid:17)2
t

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + 2ϵ2

nc)b(cid:28) 2(cid:17)2
t

+ 2(cid:28) 2(cid:17)2

t b ~C 2

}

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + 2ϵ2

nc)b(cid:28) 2(cid:17)3

t +

t b ~C 2
2(cid:28) 2(cid:17)3
T

(cid:0) (cid:17)t
2b

)

(cid:20) 0:

(cid:17)t should be set to make

(

T∑

t=1

(cid:17)2
t L2
2

Then we can get

1
T

T∑

t=1

E∥∇F (wt)∥2

2(F (w1) (cid:0) F (w(cid:3)) + T b((cid:17)2

t L2 + 2(L2

(cid:20)

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2
bT (cid:17)t

1 + ϵD)2 + 2ϵ2

nc)b(cid:28) (cid:17)3

t )V 2 + (cid:17)3

t

~C24b(cid:28)
T

V 2

(cid:20) 2(F (w1) (cid:0) F (w(cid:3))
bT (cid:17)t

+ ((cid:17)tL2 + 2(L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + 2ϵ2

nc)b(cid:28) (cid:17)2

t )V 2 +

(cid:17)2
t

~C 24b(cid:28) V 2
T

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(61)

(62)

(63)

(64)

(65)

(66)

(67)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

(2(L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + 2ϵ2

nc)b(cid:28) (cid:17)2

t ) +

(cid:17)2
t

~C 24b(cid:28)
T

(cid:20) (cid:17)tL2

We set (cid:17)t to make

√

Thus let (cid:17)t =

2(F (w1)(cid:0)F (w(cid:3))
bT V 2L2

,

1
T

T∑

t=1

E∥∇F (wt)∥2 (cid:20) V

√

2D0L2
bT

:

And we can get the condition for T by putting (cid:17) in ineq.63 and ineq.68, we can get that
√

√

√

√

{

(cid:28) (cid:20) min

L2V
C(cid:21)

L2T
2D0b

;

V
C(cid:21)

L2T
2D0b

;

T V
~C

L2
bD0

;

V L2T
4 ~C

T L2
2D0b

}

:

(68)

(69)

(70)

F. Decreasing rate of the approximation error ϵt

Since ϵt is contained the proof of the convergence rate for DC-ASGD , in this section we will introduce a lemma which
describes the approximation error ϵt the for both convex and nonconvex cases.

Lemma F.1 Assume that the true label y is generated according to the distribution P(Y = kjx; w(cid:3)) = (cid:27)k(x; w(cid:3)) and
K
f (x; y; w) = (cid:0)
k=1(I[y=k] log (cid:27)k(x; w)). If we assume that the loss function is (cid:22)-strongly convex about w. We denote wt is
the output of DC-ASGD by using the outerproduct approximation of Hessian, we have

∑

(cid:12)
(cid:12)
(cid:12)E(x;yjw(cid:3))

@2
@w2 f (x; y; wt) (cid:0) E(x;yjw(cid:3))

@
@w

f (x; y; wt)

(cid:10)

f (x; y; wt)

@
@w

(

)

(

√

) (cid:12)
(cid:12)
(cid:12) (cid:20) (cid:18)

1
t

;

ϵt =

√

where (cid:18) = 2HKLV L2

(cid:22)2

1

(cid:22) (1 + L2+(cid:21)L2

L2

1

(cid:28) ).

If we assume that
(cid:12)
(cid:12)
(cid:12) @2P(Y =kjx;w)
@2w
put of DC-ASGD by using the outerproduct approximation of Hessian, we have

the loss function is (cid:22)-strongly convex in a neighborhood of each local optimal d(wloc; r),
(cid:12)
(cid:12)
(cid:12) (cid:20) H, 8k; x; w, each (cid:27)k(w) is L-Lipschitz continuous about w. We denote wt is the out-
1
P (Y =kjx;w)

(cid:2)

(cid:12)
(cid:12)
(cid:12)E(x;yjw(cid:3))

ϵt =

@2
@w2 f (x; y; wt) (cid:0) E(x;yjw(cid:3))

@
@w

f (x; y; wt)

(cid:10)

f (x; y; wt)

@
@w

(

)

(

√

) (cid:12)
(cid:12)
(cid:12) (cid:20) (cid:18)

1
t (cid:0) T0

+ ϵnc:

where t > T0 (cid:21) O( 1

r8 ).

Proof:

E(yjx;w(cid:3))

@2
@w2 f (x; Y; wt) = (cid:0)E(yjx;w(cid:3))

@2
@w2

(I[y=k] log (cid:27)k(x; wt))

= (cid:0)E(yjx;w(cid:3))

(cid:27)k(x; wt)I[y=k]

= (cid:0)E(yjx;w(cid:3))

= (cid:0)E(yjx;w(cid:3))

+ E(yjx;w(cid:3))

P(yjx; wt)

@
@!
P(yjx; wt)

(

K∑

k=1
(

K∏

k=1

@2
@w2 log
@2
@w2 log P(yjx; wt)
P(yjx; wt)

@2
@!2

P(yjx; wt)

@2
@!2

P(yjx; wt)

P(yjx; wt)
@2
@!2

P(yjx; wt)

P(yjx; wt)

)

)

(

(

(

@
@!

@
@!

= (cid:0)E(yjx;w(cid:3))

+ E(yjx;w(cid:3))

log P(yjx; wt)

:

)

2

)

2

)

2

= (cid:0)E(yjx;w(cid:3))

+ E(yjx;w(cid:3))

f (x; Y; wt)

:

(71)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

Since E(yjx;wt)
2001), we have

P(yjx;wt)

@2
@!2
P(yjx;wt) = 0 by the two equivalent methods to calculating ﬁsher information matrix (Friedman et al.,

(cid:12)
(cid:12)
(cid:12)
E(yjx;w(cid:3))
(cid:12)
(cid:12)

@2
@!2

P(yjx; wt)

P(yjx; wt)

@2
@!2

P(yjx; wt)

P(yjx; wt)

(cid:0) E(yjx;wt)

@2
@!2

P(yjx; wt)

P(yjx; wt)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) =

=

(cid:12)
(cid:12)
(cid:12)
E(yjx;w(cid:3))
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k=1

@2
@!2

K∑

K∑

k=1

P(Y = kjX = x; wt) (cid:2)

P(Y = kjx; w(cid:3)) (cid:0) P(Y = kjx; wt)
P(Y = kjx; wt)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20) H (cid:1)

jP(Y = kjx; w

) (cid:0) P(Y = kjx; wt)j

(cid:3)

(cid:20) HKL∥wt (cid:0) wloc∥ + HK max

(cid:3)
jP(Y = kjx; wloc) (cid:0) P(Y = kjx; w

)j

k=1;(cid:1)(cid:1)(cid:1) ;K

(cid:20) HKL∥wt (cid:0) wloc∥ + ϵnc:

For strongly convex objective functions, ϵnc = 0 and wloc = w(cid:3). The only thing we need is to prove the convergence of
DC-ASGD without using the information of ϵt like before. By the smoothness condition, we have

EF (wt+(cid:28) +1) (cid:0) F (w

(cid:3)

)

(cid:3)

(cid:3)

(cid:20) F (wt+(cid:28) ) (cid:0) F (w

) (cid:0) (cid:17)t+(cid:28) ⟨∇F (wt+(cid:28) ); Egdc(wt)⟩ +

= F (wt+(cid:28) ) (cid:0) F (w

+(cid:17)t+(cid:28) ⟨∇F (wt+(cid:28) ); ∇F (wt+(cid:28) ) (cid:0) Egdc(wt)⟩ +

) (cid:0) (cid:17)t+(cid:28) ⟨∇F (wt+(cid:28) ); ∇F (wt+(cid:28) )⟩
L2(cid:17)2
t+(cid:28) V 2
2

L2(cid:17)2
t+(cid:28) V 2
2

(cid:20) (1 (cid:0) 2(cid:17)t+(cid:28) (cid:22)2

(cid:20) (1 (cid:0) 2(cid:17)t+(cid:28) (cid:22)2

(cid:20) (1 (cid:0) 2(cid:17)t+(cid:28) (cid:22)2

L2

L2

L2

)(F (wt+(cid:28) ) (cid:0) F (w

)) + (cid:17)t+(cid:28) ∥∇F (wt+(cid:28) )∥∥∇F (wt+(cid:28) ) (cid:0) Egdc(wt)∥ +

)(F (wt+(cid:28) ) (cid:0) F (w

)) + (cid:17)t+(cid:28) V (cid:1) (L2 + (cid:21)L2

1)∥wt+(cid:28) (cid:0) wt∥ +

)(F (wt+(cid:28) ) (cid:0) F (w

)) + (cid:17)t+(cid:28) V (cid:1) (L2 + (cid:21)L2

1)∥

(cid:17)t+(cid:28) (cid:0)jgdc(wt)∥ +

(cid:28)∑

j=1

L2(cid:17)2
t+(cid:28) V 2
2

t+(cid:28) V 2
L2(cid:17)2
2

L2(cid:17)2
t+(cid:28) V 2
2

Taking expectation to the above inequality, we can get
) (cid:20) (1 (cid:0) 2(cid:17)t+(cid:28) (cid:22)2

EF (wt+(cid:28) +1) (cid:0) F (w

(cid:3)

)(EF (wt+(cid:28) ) (cid:0) F (w

)) +

(cid:20) (1 (cid:0) 2(cid:17)t+(cid:28) (cid:22)2

)(EF (wt+(cid:28) ) (cid:0) F (w

)) +

(cid:3)

(cid:3)

t+(cid:28) (L2 + (cid:21)L2
(cid:17)2

1)V 2(cid:28)

2
t+(cid:28) V 2L2
(cid:17)2
2

(1 +

L2 + (cid:21)L2
1
L2

(cid:28) ):

+

L2(cid:17)2
t+(cid:28) V 2
2

EF (wt+1) (cid:0) F (w

(cid:3)

) (cid:20)

(EF (wt) (cid:0) F (w

(cid:3)

)) +

(

)

1 (cid:0) 2
t

(

V 2L2
2
2(cid:22)4t2

1 +

L2 + (cid:21)L2
1
L2

)

(cid:28)

:

Let (cid:17)t = L2

(cid:22)2t , we have

We can get

by induction. Then we can get

EF (wt) (cid:0) F (w

(cid:3)

2V 2
) (cid:20) 2L2
t(cid:22)4

1 +

L2 + (cid:21)L2
1
L2

∥wt (cid:0) w

2V 2
(cid:3)∥2 (cid:20) 4L2
t(cid:22)5

1 +

L2 + (cid:21)L2
1
L2

(

(

)

(cid:28)

:

)

(cid:28)

:

By putting Ineq.86 into Ineq.73, we can get the result in the theorem.
For nonconvex case, if wt 2 B(wloc; r), we have E(wt(cid:0)wloc) (cid:20) 1
for nonconvex loss function f (x; y; wt), DC-ASGD has ergodic convergence rate. mint=1;(cid:1)(cid:1)(cid:1) ;T E∥ @
@wt
O(1=

T ), where the expectation is taking with respect to the stochastic sampling.

E∇F (wt) under the assumptions. Next we will prove that,
F (x; y; wt)∥2 =

p

(cid:22)

(72)

(73)

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

(cid:3)

(cid:3)

(cid:3)

L2

L2

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

Figure1. Error rates of the global model with Different (cid:21)0 w.r.t. number of effective passes on CIFAR-10

Compared with the proof of ASGD (Lian et al., 2015), DC-ASGD with Hessian approximation has

T1 = ∥∇F (wt+(cid:28) ) (cid:0) Egdc(wt)∥2

= ∥∇F (wt+(cid:28) ) (cid:0) ∇F (wt) (cid:0) (cid:21)Eg(wt) ⊙ g(wt) (cid:1) (wt+(cid:28) (cid:0) wt)∥2
(cid:20) 2∥∇F (wt+(cid:28) ) (cid:0) ∇F (wt)∥2 + 2∥(cid:21)Eg(wt) ⊙ g(wt) (cid:1) (wt+(cid:28) (cid:0) wt)∥2
(cid:20) 2(L2

1)∥wt+(cid:28) (cid:0) wt∥2;

2 + (cid:21)2L4

since L1 is the upper bound of ∇f (w) and L2 is the smooth coefﬁcient of f (w). Suppose that (cid:17) =
upper bounded as Theorem 5.1,

√

2D0
bT V 2L2

and (cid:28) is

min
t=1;(cid:1)(cid:1)(cid:1) ;T

E∥∇F (wt)∥2 (cid:20) 1
T

E∥∇F (wt)∥2 (cid:20) O(

1
T 1=2

):

T∑

t=1

Referring to a recent work of Lee et:al (Lee et al., 2016), GD with a random initialization and sufﬁciently small constant
step size converges to a local minimizer almost surely under the assumptions in Theorem 1.2. Thus, the assumption that
F (w) is (cid:22)-strongly convex in the r-neighborhood of arbitrary local minimum wloc is easily to be satistied with probability
one. By the L1-Lipschitz assumption, we have P (Y = kjx; wt) (cid:0) P (Y = kjx; wloc) (cid:20) L1∥wt (cid:0) wloc∥. By the L2-smooth
assumption, we have L2∥wt (cid:0) wloc∥2 (cid:21) ⟨∇F (wt); wt (cid:0) wloc⟩. Thus for wt 2 B(wloc; r), we have ∥∇F (wt)∥ (cid:20) L2∥wt (cid:0)
wloc∥ (cid:20) L2r. By the continuously twice differential assumption, we can assume that ∥∇F (wt)∥ (cid:20) L2∥wt (cid:0) wloc∥ (cid:20) L2r
for wt 2 B(wloc; r) and ∥∇F (wt)∥ (cid:20) L2∥wt (cid:0) wloc∥ > L2r for wt =2 B(wloc; r) without loss of generality 2. Therefore
mint=1;(cid:1)(cid:1)(cid:1) ;T E∥∇F (wt)∥2 (cid:20) L2

2r2 is a sufﬁcient condition for E∥wT (cid:0) wloc∥ (cid:20) r.

(87)

(88)

(89)

(90)

(91)

(92)

min
t=1;(cid:1)(cid:1)(cid:1) ;T0

E∥∇F (wt)∥2 (cid:20) O(

) (cid:20) r2:

1
T 1=2
0

We have T0 (cid:21) O

(

)
.

1
r4

Thus we have ﬁnished the proof for nonconvex case.

G. Experimental Results on the Inﬂuence of (cid:21)

In this section, we show how the parameter (cid:21) affect our DC-ASGD algorithm. We compare the performance of respectively
3. The results are given in Figure 1. This
sequential SGD, ASGD and DC-ASGD-a with different value of initial (cid:21)0
experiment reﬂects to the discussion in Section 5, too large value of this parameter ((cid:21)0 > 2 in this setting) will introduce
large variance and lead to a wrong gradient direction, meanwhile too small will make the compensation inﬂuence nearly
disappear. As (cid:21) decreasing, DC-ASGD will gradually degrade to ASGD. A proper (cid:21) will lead to signiﬁcant better accuracy.

2We can choose r small enough to make it satisﬁed.
3We also compare different (cid:21)0 for DC-ASGD-c and the results are very similar to DC-ASGD-a.

020406080100120140160Epochs0.000.050.100.150.20Training errorM = 8SGDAsync SGDDC-ASGD:¸0=0:5DC-ASGD:¸0=1DC-ASGD:¸0=28090100110120130140150160Epochs0.0850.0900.0950.1000.1050.1100.1150.1200.1250.130Test errorM = 8Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

H. Large Mini-batch Synchronous SGD with Delay-Compensated Gradient

In this section, we discuss how delay-compensated gradient can be used in synchronous SGD. The effective mini-batch
size in SSGD is usually enlarged M times comparing with sequential SGD. A learning rate scaling trick is commonly used
to overcome the inﬂuence of large mini-batch size in SSGD (Goyal et al., 2017): when the mini-batch size is multiplied by
M , multiply the learning rate by M . For sequential mini-batch SGD with learning rate (cid:17) we have:

where zt+j is the t + j-th minibatch.

On the other hand, taking one step with M times large mini-batch size and learning rate ^(cid:17) = M (cid:17) in synchronous SGD
yields:

wt+M = wt (cid:0) (cid:17)

g(wt+j; zt+j);

M (cid:0)1∑

j=0

^wt+1 = wt (cid:0) ^(cid:17)

g(wt; zj

t );

1
M

M (cid:0)1∑

j=0

t . The assumption g(wt+j; zt+j) (cid:25) g(wt; zj

t ) was made in synchronous SGD(Goyal et al., 2017).

t is the t-th minibatch on local machine j.

where zj
Assume that zt+j = zj
However, it often may not hold.
If we denote ~wj
t+1 = wt (cid:0) ^(cid:17) 1

M

∑

i<j g(wt; zi

t), we can unfold the summation in Eq.94 to

t+1 = ~wj
~wj+1

t+1

(cid:0) ^(cid:17)

g(wt; zj

t ); j < M;

1
M

then we have ^wt+1 = ~wM
delay-compensated gradient to update Eq.95 with:

t+1. We propose to use Eq.(5) in the main paper to compensate this assumption and apply

g(wt+j; zt+j) (cid:25) ~g( ~wj
1
M

t+1 = ~wj
~wj+1

(cid:0) ^(cid:17)

t+1

t+1; zj
~g( ~wj

t ) := g(wt; zj
t+1; zj

t ); j < M:

t ) + (cid:21)g(wt; zj

t ) ⊙ g(wt; zj

t ) ⊙ ( ~wj

t+1

)
(cid:0) wt)

;

Please note that we redeﬁne the previous ~wj+1
Choosing ~wj
induce more accurate approximation by using Taylor expansion.

t+1 according to the increasing order of ∥ ~wj

t+1

t+1 in Eq.97. For j > 1, we need to design an order to make ~wj

(cid:25) wt+j.
(cid:0) wt∥2 can be used since the smaller distance with wt will

t+1

References

Springer, Berlin, 2001.

Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert. The elements of statistical learning, volume 1. Springer series in statistics

Goyal, Priya, Dollar, Piotr, Girshick, Ross, Noordhuis, Pieter, Wesolowski, Lukasz, Kyrola, Aapo, Tulloch, Andrew, Jia, Yangqing, and

He, Kaiming. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.

LeCun, Yann. Modèles connexionnistes de lapprentissage. PhD thesis, These de Doctorat, Universite Paris 6, 1987.

Lee, Jason D, Simchowitz, Max, Jordan, Michael I, and Recht, Benjamin. Gradient descent converges to minimizers. University of

California, Berkeley, 1050:16, 2016.

Lian, Xiangru, Huang, Yijun, Li, Yuncheng, and Liu, Ji. Asynchronous parallel stochastic gradient for nonconvex optimization. In

Advances in Neural Information Processing Systems, pp. 2737–2745, 2015.

(93)

(94)

(95)

(96)

(97)

