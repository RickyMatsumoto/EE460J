Estimating the unseen from multiple populations

Aditi Raghunathan 1 Gregory Valiant 1 James Zou 1 2

Abstract

Given samples from a distribution, how many
new elements should we expect to ﬁnd if we con-
tinue sampling this distribution? This is an im-
portant and actively studied problem, with many
applications ranging from unseen species estima-
tion to genomics. We generalize this extrapola-
tion and related unseen estimation problems to
the multiple population setting, where population
j has an unknown distribution Dj from which we
observe nj samples. We derive an optimal esti-
mator for the total number of elements we expect
to ﬁnd among new samples across the popula-
tions. Surprisingly, we prove that our estimator’s
accuracy is independent of the number of popu-
lations. We also develop an efﬁcient optimiza-
tion algorithm to solve the more general problem
of estimating multi-population frequency distri-
butions. We validate our methods and theory
through extensive experiments. Finally, on a real
dataset of human genomes across multiple an-
cestries, we demonstrate how our approach for
unseen estimation can enable cohort designs that
can discover interesting mutations with greater
efﬁciency.

1. Introduction

Given samples from a distribution, many settings in ma-
chine learning and statistics involves estimating properties
of the unseen portion of the distribution, i.e. elements in
the support of the distribution that are not observed in the
samples collected so far. One important example of esti-
mating the unseen is the problem of predicting the number
of distinct new elements in additional samples collected.
This question is famously illustrated by the case of Corbet’s
butterﬂies. Alexander Corbet was a British naturalist who

1Stanford University, Stanford, CA 2Chan Zuckerberg Biohub,
San Francisco, CA. Correspondence to: Aditi Raghunathan <adi-
tir@stanford.edu>, Gregory Valiant <valiant@stanford.edu>,
James Zou <jamesz@stanford.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

spent two years in Malaya trapping butterﬂies. He found
118 rare species of butterﬂies for which he found only one
specimen, another 74 species with two specimens, 44 with
three specimens, etc. Corbet was naturally interested in
the butterﬂies that are heretofore unseen. In particular, he
wanted to estimate how many distinct new species of but-
terﬂies he can expect to discover if he were to conduct a
new expedition to Malaya—such an estimate could help
determine whether a new experiment is warranted. Good-
Toulmin, extending earlier work of Ronald Fisher, came
up with the remarkable estimate that the number of new
species Corbet can expect to ﬁnd is simply the alternat-
ing sum 118 - 74 + 44 - ... The Good-Toulmin estimator
sparked the investigation into how to estimate the discov-
ery rate of new elements and this remains an active area of
research. Estimating the discovery rate has many important
applications beyond the original species collection setting.
In genomics, for example, an important question is: given
the genetic variation already identiﬁed in the genomes of
individuals from some population (say, East Asia), how
many additional mutations do we expect to ﬁnd by se-
quencing the genomes of additional individuals from East
Asia. An accurate answer to this question can improve the
cohort design of new population sequencing experiments.

Predicting the number of new elements is a particular in-
stance of estimating the unseen. In other applications, one
may want to estimate different statistics that also depend on
the currently unobserved elements. For example, one may
want to predict how many new elements will be observed
at least twice (for reproducibility) or at most three times (if
the focus is on rare elements). More generally, one may
want to estimate the histogram of the underlying distribu-
tion, which summarizes the frequency distribution of all the
elements (see Sec. 2 for precise deﬁnition) and from which
these other statistics can be derived.

The unseen estimation literature has focused on the setting
where there is a single distribution which generate current
In practice, we
samples as well as any future samples.
often have multiple distinct distributions and we observe
varying number of samples from each distribution. In the
genomics example above, in addition to sequencing data
from East Asians, we also have genome sequences of in-
dividuals from Europe, Africa, etc. The relevant question
is: given we currently have the genomes of ni individuals

Estimating the unseen from multiple populations

from population i, i ∈ {1, ..., m}, and we have identiﬁed
all the genetic variants in this group, how many total new
mutations do we expect to ﬁnd if we sequence additional
bi individuals from population i. Moreover, given a ﬁnite
budget Nnew of new genomes that we can sequence, how
should we allocate this budget across the different popula-
tions to maximize the expected number of new mutations
oberved? Similarly, suppose Corbet had also collected but-
terﬂies in Brunei and Indonesia, in addition to Malaya.
Then he might want to know how many totally new species
he can expect to ﬁnd if he was to spend, say, another six
month in Malaya and one year in Brunei. He might also be
interested in estimating the joint frequency distribution of
butterﬂies across all three regions.

Our contributions.
In this paper, we address the gen-
eral problem of estimating the unseen when we have sam-
ples from multiple populations, each corresponding to a
potentially distinct distribution. Despite being very natu-
ral, this multi-population problem has not been systemat-
ically studied to the best of our knowledge. We derive a
multi-population generalization of the Good-Toulmin esti-
mator for the expected number of new elements. Surpris-
ingly, we prove that the accuracy of our extrapolation esti-
mator is independent of the number of populations. More-
over, it achieves the optimal super-linear extrapolation rate.
Next, we develop an efﬁcient optimization method to es-
timate the more general multi-population joint frequency
distribution. This complements our extrapolation estima-
tor, and outperforms the generalized Good-Toulmin esti-
mator in most settings. This more general approach also
enables predictions for other statistics of interest. We sys-
tematically validate these two algorithms on synthetic data
as well as real datasets from population genetics and from
English books. Moreover, we illustrate that by estimating
the joint frequency distribution, we can signiﬁcantly im-
prove the discovery power under a budget constraint.

2. Related works

The problem of estimating the properties of the unobserved
portion of a distribution, given n samples, and the related
problem of estimating the number of new domain elements
that are likely to be observed if an additional cn samples
are collected, dates back to works of I.J. Good and A. Tur-
ing (Good, 1953), and R.A. Fisher (Fisher et al., 1943).
This was quickly followed by (Good & Toulmin, 1956),
which introduced the Good-Toulmin estimator. While the
Good-Toulmin estimator is always unbiased, the variance
increases rapidly for c ≥ 1. Subsequent works, includ-
ing (Efron & Thisted, 1976) have suggested “smoothing”
approaches that tradeoff the bias and variance for this type
of approach. The recent work of Orlitsky et al. (2016) de-
scribes a clever variant that achieves good performance for

c = O(log n). This ability to accurately estimate the num-
ber of domain elements seen in a second sample of size
up to O(n log n), where n denotes the size of the original
sample, was concurrently shown via a different approach
in (Valiant & Valiant, 2016). This logarithmic factor ex-
trapolation matches the lower bounds of (Valiant & Valiant,
2011), to constant factors. The linear estimators that we
propose in Section 4 for the multiple population setting,
and their analysis, are extensions of the smoothed Good-
Toulmin estimators of (Orlitsky et al., 2016).

A different approach to this problem was proposed
by Efron & Thisted (1976), who considered a linear-
programming approach to estimating this property by im-
plicitly ﬁnding a label-less representation of the underly-
ing distribution that was consistent with the observed fre-
quency counts, then returning the support size of this dis-
tribution. This approach was adapted and rigorously an-
alyzed in (Valiant & Valiant, 2011; 2013), who showed
that it provably yields an accurate representation of the fre-
quency distribution of the underlying distribution, which
can subsequently be leveraged to yield estimates of dis-
tributional properties, including entropy, distance metrics
between distributions, and approximations for the number
of new elements that would be observed in larger samples.
Recent works (Valiant & Valiant, 2016; Zou et al., 2016)
also established that this approach can accurately estimate
the number of new elements that will be observed in sam-
ples of size up to O(n log n). Our optimization-based al-
gorithm, described in Section 5, generalizes this approach.

3. Deﬁnitions and examples

i }j=1,...,m
i=1,...,nj

Let Ω denote the domain, and D1, ..., Dm denote m prob-
ability distributions over Ω. Di represents the frequency
of elements in population i. Note that it is not restrictive
to assume that the populations share the same domain Ω
since different Di’s may have distinct supports. We model
the multi-population unseen estimation as a two stage pro-
cess. In the ﬁrst period, we observe nj independent sam-
ples from the j-th population, {X j
. This is the
seen data. In period two, which is in the future, we will
sample additional tjnj samples from the j-th population,
{Y j
. The period two samples are unseen and
we would like to estimate some statistic U ({Y j
i }).
We can think of tj ≥ 0 as the extrapolation factors. If tj
is large, then we will obtain many more samples from pop-
ulation j in the second period compared to what we have,
and the problem of estimating U could be more challeng-
ing. We can take tj as given for the purpose of estimating
U . We later discuss how we to leverage our estimator of
U to optimize the tj’s in order to maximize the number of
new discoveries. Note that in general, the nj’s and tj’s can
differ arbitrarily across the populations.

i }j=1,...,m
i=1,...,tj nj

i }, {X j

Estimating the unseen from multiple populations

A particularly important statistic is U = the total number
of new elements in {Y j
i } that are not observed in the pe-
riod one samples {X j
i }. A good estimator for this U quan-
tiﬁes the expected information gain of the second period.
In the one population setting, this statistic is the focus of
Good-Toulmin and a large number of papers. Other useful
choices of U could be the number of distinct new elements
that are observed at least twice in {Y j
i }, which could be
relevant if we want some reproducibility.

Beyond estimating these single parameters, we could also
hope to use the samples {X j
i } to estimate the histogram of
D1, ..., Dm. The multi-population histogram, deﬁned be-
low, captures all of the information about the populations,
other than the labels of the domain.
Deﬁnition 3.1 (Multi-population histogram). Given a col-
lection of m distributions D1, . . . , Dm over a common do-
main Ω, the corresponding multi-population histogram H
is a mapping from [0, 1]m \ 0m (cid:55)→ N ∪ {0}. For each
α = (α1, α2, . . . αm) ∈ [0, 1]m \ 0m, H(α) = |{y ∈ Ω |
Dj(y) = αj, 1 ≤ j ≤ m}|, where Di(y) is the probability
mass of domain element y. in the ith distribution Di.

Any symmetric multi-population statistic—one that is in-
variant to permuting the labels of the domain—is a func-
tion of only the histogram. Such statistics include distance
metrics between the distributions/populations, measures of
the entropy of the populations, and the number of new el-
ements that one is likely to observe in a second batch of
samples. The multi-population histogram is also of intrin-
sic interest; in population genetics, H is exactly the joint
frequency distribution of mutations, and reveals informa-
tion about demographic history (e.g. historical variations
in population size) and selective pressures. One beneﬁt of
focusing on the histogram is that, while it does not contain
as much information as the actual labeled distributions, it
can often be accurately recovered even when given too few
samples to learn the (labeled) distributions to any signiﬁ-
cant accuracy (Valiant & Valiant, 2011).

Both for directly predicting U and estimating H, we rely
on a label-less representation of the samples, termed the
ﬁngerprint of {X j
i }. The ﬁngerprint of the samples is the
analog of the histogram of the distributions, and captures
all the information of {X j
i } that is relevant for estimating
symmetric statistics.
Deﬁnition 3.2 (Multi-population ﬁngerprint). Given the
samples {X j
i }, its ﬁngerprint is an m-dimensional tensor
Φ whose i1...im-th entry, φi1...im, is the number of distinct
elements observed exactly ij times in the samples from pop-
ulation j. Here each ij can range from 0 to nj.
Example 3.1. Suppose we have ﬁve samples from Popu-
lation 1, (A, B, C, E, F ), and seven from Population 2,
(A, B, D, E, E, F, F ). The corresponding 2-dimensional
ﬁngerprint of this data is given by the following matrix:

0
·
1

1
1
2

2
0
2

0
1

The (1, 1) entry is 2 because A, B are observed once in
each set of samples; the (1, 0) entry is 1 because exactly
one element, C, is observed once in the samples from Pop-
ulation 1 and zero times in the samples from Population 2.
By convention, we omit the (0, 0) element.

4. A linear estimator

Unbiased estimator. Given the empirical ﬁngerprints Φ
and the extrapolation factors tj, j = 1, ..., m, we deﬁne the
following estimator

ˆU = −

(cid:88)

i1,...,im:(cid:80) ij >0

j=1







m
(cid:89)

(−tj)ij

 φi1...im.

(1)

ˆU is a weighted alternating sum of the empirical ﬁnger-
prints where the weights are determined by the extrapola-
tion factors tj.
Proposition 4.1. For any number of populations m, and
any extrapolation factors tj ≥ 0, j = 1, ...m, ˆU is an un-
biased estimator of U .

Proof of the proposition appears in Appendix 7.
ˆU is linear in the ﬁngerprint entries.
Its computational
cost is linear in the total number of period one samples,
n = (cid:80)
j nj, since there can be at most n non-zero ﬁnger-
print entries. To build more intuition for ˆU , we illustrate its
application in two simple settings.

Example 4.2. Consider the setting where all m distribu-
tion are identical, i.e. all the samples are drawn from the
same discrete distribution D. Let tj = 1, ∀j for simplicity.
After rearranging terms, ˆU can be written as

ˆU =

(−1)k+1

(cid:88)

k=1





(cid:88)

(i1,...,im):(cid:80) ij =k



φi1...im

 .

Because the populations are identical,
the sum in the
parenthesis is just the number of elements that are observed
k times from all the samples so far. Hence the general es-
timator ˆU reduces to the one dimensional Good-Toulmin
estimator when all m populations are identical.
Example 4.3. Suppose the supports of the distributions Di
are disjoint. Then the only possible non-zero ﬁngerprint
entries are φi1...im where exactly one of the ij is great than
0 and all the other ij’s are zero. For simplicity, assume
tj = 1 for all j. Then ˆU = (cid:80)k
i , where
φk
is the marginal ﬁngerprint entry of the number of el-
i
ements that are observed i times in population k. Hence

i(−1)k+1φk

(cid:80)

j=1

Estimating the unseen from multiple populations

when the populations are disjoint, the expected number of
new elements is the sum of the expected number of new ele-
ments in each population. When the populations have over-
lapping support, we have the nontrivial interaction terms
due to the cross-population ﬁngerprint entries.

General weighted linear estimator. While ˆU is unbiased,
its variance could be large if some of the extrapolation fac-
tors tj’s are greater than 1. This is because the powers of tj
appear in Eqn. 1. To address this issue, we introduce a gen-
eral class of multi-population weighted linear estimators.

ˆU W = −

(cid:88)







m
(cid:89)

(−tj)ij

 φi1,...,imW (i1, . . . , im).

ij :(cid:80) ij >0

j=1

(cid:17)

(cid:16)

j∈A ij

We focus on a particular weighting scheme, which is an
extension of that introduced in (Orlitsky et al., 2016):
L ≥ (cid:80)
W (i1, i2, . . . im) = P
where L ∼ Poi(r)
and A = {j : tj > 1} are the populations that we would
like to extrapolate beyond the original sample size.
If
tj ≤ 1 ∀j, then W = 1 and ˆU W is just the unbiased estima-
tor ˆU . The Poisson rate r is a tuning parameter that deter-
mines the bias/variance tradeoff of ˆU W . As r increases, all
the weights approaches 1 and ˆU W approaches the unbiased
estimator ˆU . As r decreases, the ﬁngerprint entries φi1...im
with some large ij’s—which are also the terms with high
variance—are weighted by a factor that is close to 0. This
reduces the total variance of ˆU W at the cost of introducing
bias. We will see how to set r as a function of the nj’s and
tj’s in order to minimize the overall estimation error. In the
rest of the paper, unless otherwise speciﬁed, we will use
ˆU W to denote the multi-population linear estimator with
Poisson weights.

Performance guarantee of the weighted estimator. We
use relative mean squared error, E

, to quan-

(cid:17)2(cid:21)

(cid:20)(cid:16) ˆU W −U
(cid:80) nj tj

tify the performance of ˆU W . This is a natural error metric,
because (cid:80) njtj is the number of samples in period two
and we care about how the error in the predicted number of
new elements scales with the number of samples. Without
loss of generality, we can relabel the populations so that
t1 = maxjtj. We are especially interested in the setting
when t1 ≥ 1 (i.e. large extrapolation).
Proposition 4.4. Suppose t1 = maxj tj ≥ 1 and the Pois-
son rate is r =

, then

log((cid:80)

j nj (tj +1))
2t1


(cid:32) ˆU W − U

(cid:80) njtj

E

(cid:33)2

 ≤

(cid:18) n1t1 + (cid:80)
n1t1

(cid:19)

j nj

n−1/t1
1

.

(2)

any (cid:15) > 0, we can achieve E

(cid:20)(cid:16) ˆU W −U
(cid:80) nj tj

(cid:17)2(cid:21)

≤ (cid:15) with

t1 = O(log n1/ log(1/(cid:15))). This means that ˆU W has low
relative error even when the largest extrapolation factor t1
is logarithmic in its initial sample size n1.
Remark 4.6 (no dependence on m). Note that the relative
error in Eqn. 2 does not depend on the number of pop-
ulations m. This is somewhat surprising since the num-
ber of terms in ˆU W potentially grows exponentially with
m and the variance of each ﬁngerprint entry φi1...im also
increases as the number of population increases. This pop-
ulation agnostic property of ˆU W guarantees its accuracy
even when m is arbitrarily large.
Remark 4.7 (lower bound). Here we have focused on a
speciﬁc form of the estimator ˆU W where the weights W
of the ﬁngerprint entries correspond to the tail probabil-
ity of Poisson distributions. A natural question is whether
there exists a different form of the weights or a different es-
timator altogether that can consistently be more accurate
than our current ˆU W . The answer is essentially no due to
the following lower bound for one population extrapola-
tion (Orlitsky et al., 2016; Valiant & Valiant, 2011): There
exists universal constants c, c(cid:48) such that for all estimators
ˆU , if the extrapolation factor t > c, then ∃ distribution such
n−c(cid:48)/t. Here n is the number of sam-
that E

(cid:20)(cid:16) ˆU −U

(cid:17)2(cid:21)

nt

>
∼

ples drawn from this distribution in period one. This lower
bound implies that in order to guarantee that the relative
error is less than (cid:15) in general, the extrapolation factor can
be at most O(log n/ log(1/(cid:15))), matching Prop. 4.4.

Outline of the proof of Prop. 4.4 (detailed analysis is in
the Appendix). To analyze the relative error, we separately
quantify the bias and variance of ˆU W in terms of nj, tj, r.
Lemma 4.8 (Bias). Let r denote the rate of the Poisson
weights, then

(cid:12)
(cid:12)
(cid:12)

(cid:12)
E[ ˆU W − U ]
(cid:12)
(cid:12) ≤


 e−r

nj(tj + 1)





(cid:88)

j∈A

Lemma 4.9 (Variance). Without loss of generality, let t1 =
maxj tj and suppose t1 ≥ 1 then

Var( ˆU W − U ) ≤

(cid:88)

nje2r(t1−1) +

njtj.

(cid:88)

j

To obtain the optimal r given in the statement of Prop. 4.4,
we set r to balance the squared bias and variance.

5. Estimating the multi-population frequency

distribution

Remark 4.5 (log extrapolation factor). Suppose the ra-
is bounded, then Prop. 4.4 guarantees that for
tio

(cid:80)

n1
j nj

While we have a linear estimator for the number of un-
seen elements in a new sample, it is challenging to con-

Estimating the unseen from multiple populations

struct good estimators of other statistics (e.g. number of
new elements observed ≥ 2) directly from the ﬁngerprints.
As discussed in Sec. 3, we can also take the less direct
approach of ﬁrst trying to estimating the true underlying
multi-population histogram. Given an accurate reconstruc-
tion of this underlying histogram, we can then estimate any
symmetric statistic of the future samples. We discuss some
of the uses of such a representation in Section 5.

Recovering the frequency distribution The core of our
algorithm to recover the multi-population histogram is a
natural extension of the single population algorithm pre-
sented in Valiant & Valiant (2011; 2013).

Estimating the multi-population histogram: Core
Approach.
Input: Multi-population ﬁngerprint Φ of samples,
Output: Two estimates, ˆHcounts and ˆHll of his-
togram corresponding to the distributions underlying
ﬁngerprint Φ.

• Compute ˆHcounts and ˆHll minimizing the fol-

lowing expressions:

ˆHcounts = arg min
H

√

1
1 + Φi

|Φi − ˆΦ(H)i|.

ˆHll = arg max
H

log poi(Φi, ˆΦ(H)i),

Where [ ˆΦ(H)]i =

H(α)

bino(αj, nj, ij).

m
(cid:89)

j=1

(cid:88)

(cid:88)

i

i

(cid:88)

α

The intuition behind these two optimization problems is the
following. The histogram corresponding to a set of dis-
tributions is an unlabeled representation of the underlying
distributions, hence it makes intuitive sense to try to re-
cover the histogram that maximizes the likelihood of the
unlabeled representation of the samples, namely the ﬁn-
gerprint Φ. Recent work (Acharya et al., 2016) provided
rigorous support for this intuition.
In general, however,
this likelihood might be difﬁculty to compute. Neverthe-
less, an efﬁciently computable proxy for this likelihood can
be obtained by treating the distribution of the ﬁngerprint,
corresponding to a histogram H, as a product distribution,
with Φi1,...,im distributed according to the Poisson dis-
tribution with appropriate expectation EH [Φi1,...,im]. The
recent central limit theorem for “Poisson Multinomials”
from (Valiant & Valiant, 2011) provides at least some cor-
roboration for the reasonableness of having a proxy for the
log-likelihood that decomposes linearly across the differ-
ent elements of Φ. The motivation for the
scaling
on the ﬁrst proxy likelihood function is that this expression
penalizes discrepancies between the observed and expected

1√

1+Φi

ﬁngerprint entries according to a rough approximation of
the standard deviation of that ﬁngerprint entry, as the vari-
ance of a Poisson random variable is equal to its expecta-
tion, and the observed ﬁngerprint entry is an approximation
for the expected ﬁngerprint entry given the true underlying
histogram.

The work (Valiant & Valiant, 2013) focused on recovering
ˆHcounts, as this optimization problem can be formulated as
a linear program, whose variables correspond to a ﬁne dis-
cretization of the potential support of the histogram. Unfor-
tunately, in the present multi-distribution setting, the num-
ber of variables required by this linear programming ap-
proach would scale exponentially with the number of dis-
tributions in question. Even for ﬁngerprints derived from
modest-sized samples from two distributions, the resulting
linear program becomes impractical.

the dimensionality of

Instead of pursuing the linear programming based ap-
proach, we instead propose a black-box optimization
approach to ﬁnding a histogram that optimizes either
In this op-
of the two proxy likelihood functions.
the opti-
timization approach,
mization problem is speciﬁed by the user, and corre-
sponds to the number of (i1, . . . , im) tuples for which
the returned histogram ˆH is nonzero. Denoting this
quantity by s,
the resulting optimization problem can
be regarded as the problem of specifying s vectors
(h1, α1,1, . . . , α1,m), . . . , (hs, αs,1, . . . , αs,m). These s
vectors are then interpreted as a histogram H with
H(αj) = hj for all j ∈ {1, . . . , s}, and H(α) = 0 for
all other vectors α.

The one additional modiﬁcation that leads to a substantial
improvement in runtime is to only evaluate the proxy likeli-
hood expressions for ﬁngerprint entries Φi1,...,im ≥ 2. The
intuition for this is two-fold. First, the number of vectors
(i1, . . . , im) for which Φi1,...,im = 0 will scale exponen-
tially with m, as opposed to scaling as some parameter
of the sample sizes; this is clearly undesirable. Second,
given that we wish to avoid evaluating the contribution to
the proxy likelihood from ﬁngerprint entries that are zero,
we must now be careful in dealing with ﬁngerprint entries
that are equal to 1. Suppose we have 1 element with true
probability i
n and suppose we observe that ﬁngerprint entry
Φi = 1, and the other ﬁngerprints near i are 0. Since we are
maximizing the likelihood that Φi = 1 (without taking into
√
i
account the nearby 0 entries), we would assign roughly
elements to probability i
n which is undesirable. Removing
the ones largely resolves this issue. Note that the Φj = 2
entries do not cause as much of an issue, as such collisions
are unlikely to occur in regions of the ﬁngerprint in which
there is not a signiﬁcant number of domain elements.

In this one-distribution example, a constraint on the total
probability mass being 1 would resolve this issue, though

Estimating the unseen from multiple populations

analogs of this issue in the multiple distribution setting can-
not be resolved in this way. Hence, we adopt the crude, but
effective approach of viewing all the empirical ﬁngerprint
entries that are equal to 1 as being reﬂective of an element
in the underlying set of distributions whose probability is
close to the empirical probability of the corresponding ele-
ment. We summarize the complete algorithm below:

Estimating the multi-population histogram: Full
Algorithm.
Input: Multi-population ﬁngerprint Φ derived from
samples from m distributions of respective sizes
n1, . . . , nm.
Output: Two estimates, ˆHcounts and ˆHll of histogram
corresponding to the distributions underlying ﬁnger-
print Φ.

• Remove ﬁngerprint entries that are 1, and add to em-

pirical portion of histogram:

1. Initialize m-distribution histogram ˆHemp to be

identically zero.

2. For each vector i = (i1, . . . , im) such that

Φi = 1, set ˆHemp( i1
n1
• Compute ˆHcounts and ˆHll minimizing the follow-

, . . . , im
nm

) = 1.

ing expressions:

ˆHcounts = arg min
H

(cid:88)

√

1
1 + Φi

|Φi − ˆΦ(H)i|.

ˆHll = arg max
H

i:Φ(i)≥2

(cid:88)

log poi(Φi, ˆΦ(H)i.

Where ˆΦ(H)i =

H(α)

bino(αj, nj, ij).

i:Φ(i)≥2

(cid:88)

α

m
(cid:89)

j=1

Subject to the constraint that, together with ˆHemp,
the total mass in all the distributions is 1. Namely
for all i ∈ {1, . . . , m},

(cid:88)

α

αi ˆHll(α) +

αi ˆH∗(α) = 1.

(cid:88)

α

• Return the concatenation of the empirical portion
of the histogram and the portion returned by the
:= ˆHcount + ˆHemp, and
optimization:
ˆHll := ˆHll + ˆHemp

ˆHcount

Leveraging ˆH for approximating the value of additional
data. An accurate representation of the histogram corre-
sponding to the multi-population distribution underlying a
given set of observations can be leveraged to estimate a
number of useful properties. These properties include es-
timating the number of new domain elements that would

likely be seen given additional samples from the popula-
tions. Speciﬁcally, given a histogram ˆH, corresponding to
m populations, we can estimate the expected number of
distinct elements that will be observed in samples from the
m populations of respective sizes n1, . . . , nm via the sim-
ple formula:

E[num observed] =

ˆH(α)

1 −

(1 − αi)ni

.

(3)

(cid:32)

(cid:33)

(cid:88)

α

m
(cid:89)

i=1

An accurate approximation to the histogram can also be
leveraged to answer many other questions about the popu-
lations that can not be readily addressed via the linear esti-
mators of Section 4. These include tasks such as estimating
the amount of data that must be collected to capture, say,
99% of the mass of the distributions in question.

6. Experiments

Evaluating the weighted linear estimator for large m.
We empirically evaluated the performance of the weighted
linear estimator ˆU W . The experiments were conducted
for three types of distributions—Uniform, Dirichlet and
Geometric—that are commonly used to evaluate extrapola-
tion algorithms. Each experiment contains m = 100 pop-
ulations. We have a total of 3000 distinct elements.
In
the Uniform setting, each population has support on 100
elements that are randomly sampled from the 3000. For
Dirichlet, each population also has support on 100 random
elements (from the 3000), and the weights on these 100 el-
ements are sampled from a Dirichlet prior. For the Geomet-
ric experiments, each population corresponds to a random
ordering of the 3000 elements and the k-th element is as-
signed probability ∝ (1 − p)kp. In period one, ten samples
are observed in each of the 100 populations. In period two,
95 randomly chosen populations have extrapolation factor
t ∈ [0, 1] and ﬁve populations have extrapolation factor
10t. This simulates the setting where we can obtain sub-
stantially more samples from a subset of the populations.

Figure 1(a, b, c) shows the results of the experiments for
Uniform, Dirichlet(1) and Geometric with p = 0.05 re-
spectively. The results for other parameter settings are
qualitatively similar. The black curves indicate the true
number of distinct new elements we expect to observe in
period two by sampling from the true underlying distribu-
tions. The red curves are the predictions of the weighted
linear estimator (shaded regions indicate one standard de-
viation across 100 experiments). In all three settings, ˆU W
provides accurate estimate with low variance when the
maximum extrapolation factor is relatively small (≤ 3).
For Uniform and Geometric distributions, the accuracy is
high up to 10 fold extrapolation. For Zipf, the bias is low
but variance becomes large for the maximum extrapolation
factor around 10. The downward bias in the predictions

Estimating the unseen from multiple populations

Figure 1. Performance of the weighted linear estimator of Prop. 4.4 for (a) Uniform, (b) Dirichlet and (c) Geometric distributions. Each
experiment contains 100 populations. The x-axis corresponds to the maximum extrapolation factor among the 100 populations (t1 in
Prop. 4.4). The black curve indicates the true number of distinct new elements that we expect to observe in the new samples, and the red
curve shows the predicted number of new elements. The red shaded region corresponds to one standard deviation over 100 independent
experiments. (d) The 3-population earthmover distance (EMD) between the recovered histograms and the true histogram corresponding
to the populations from which the samples were drawn. The blue line corresponds to the histogram of the empirical distribution of the
samples, and the red and green lines correspond to the histograms returned by our multi-population histogram estimation algorithm,
using the count-objective and likelihood objectives, respectively. Plots depict the mean and standard deviation over 5 independent runs.
The true underlying distribution is supported on 4 · 105 domain elements. (e) Estimating the number of new domain elements that will be
observed given additional samples in the same 3-population setting. Estimates are made for when the new samples are evenly distributed
among the populations (equal) and when the majority come from one population (skewed). Error bars depict one standard deviation
about the mean, calculated based on 10 independent trials.

Figure 2. (a) Estimating the total number of unique words combining three different books using hist-opt-counts. Predictions are based
on ﬁngerprints of samples of words—sampled without replacement either randomly (blue) or from a contiguous block of text (green)
from each book. Error bars depict one standard deviation over 10 independent runs. (b) Estimating the number of new mutations
that would be observed given additional samples from four different populations using hist-opt-counts. We consider different ratios of
sampling within these populations and observe the change in number of new mutations that would be observed.

is due to the weighting scheme. The relative error of the
(cid:17)2

, is 0.09, 0.08 and 0.08 for
weighted estimator,
the Uniform, Dirichlet and Geometric distributions when

(cid:16) ˆU W −U
(cid:80) nj tj

the maximum extrapolation factor is 10. This conﬁrms
the theoretical results of Prop. 4.4 on the accuracy of the
weighted linear estimator.

Estimating the unseen from multiple populations

Evaluating the histogram estimators. We ﬁrst validated
the performance of ˆHcount and ˆHll on a three population
setting with synthetic data. The true population consists of
three uniform distributions over 200k elements, whose sup-
ports have 100k elements in common, and 100k elements
unique to each distribution. In Figure 1(d), the x-axis cor-
responds to the number of samples we observe from each
population, and the y-axis indicates the earthmover dis-
tance (EMD) between ˆHcount, ˆHll and the true histogram.
As a baseline, we also compute the EMD between the em-
pirical histogram of the observed samples and the true his-
togram. ˆHcount and ˆHll performed roughly equally well
and both are substantially better than the empirical esti-
mator especially when the number of observed samples is
small. Figure 1(e) illustrates the extrapolation accuracy of
our histogram estimators. We estimated ˆHcount and ˆHll us-
ing 16K from each population, and then used Eqn. 3 to esti-
mate the number of unseen elements in additional samples.
We tested two different settings: 1) when the additional
samples are equally drawn from the three populations, and
2) a skewed mixture where 5/6 of the new samples are from
population 1 and 1/12 each are drawn from population 2
and 3. ˆHcount and ˆHll gave extremely accurate predictions.
In comparison, the weighted linear estimator ˆU W was ac-
curate for the initial extrapolations but has downward bias
when the extrapolation increases, consistent with Fig. 1(a-
c).
Additionally, we evaluate the performance of ˆHcount on a
real dataset, in which we sampled words from three books–
Hamlet (32K total words), Treasure Island (40K) and The
Sun Also Rises (72K). We used the true word frequencies
(over the entire text) as the true histogram. We sampled a
small number of words (equal in all books) either randomly
or from a contiguous block of text and used ˆHcounts to pre-
dict the total number of distinct words in total in all three
books. In Figure 2(a), the red line is the true value, and blue
and green lines are predictions based on ˆHcount derived
from samples of either random words, or words occurring
in a random contiguous block of text, respectively. We ob-
tain accurate estimates using a fraction of words (10K from
each book). The estimates based on independent samples
of words is more accurate than that based on contiguous
blocks of text—likely due to correlation in words that oc-
cur near each other.

Optimizing discovery rate. Given the estimated his-
togram ˆHcount or ˆHll, we can optimize the allocation of
new samples across the populations to maximize the num-
ber of unseen elements we can expect to discover given a
bound on (cid:80)
j tjnj. To illustrate, we obtained genome se-
quencing data of 45K individuals from the Exome Aggre-
gation Consortium (Lek et al., 2016). The individuals come
from four ancestries: Europeans, Africans, East Asians and
Latinos. We used all the observed mutations from the 45K

samples to construct a four population frequency distribu-
tion. For the experiment, we treat this as the ground truth
and sampled 105 mutations from each population to obtain
“seen” data. Suppose we have budget to sample 3 × 106
variants (10 fold extrapolation from current sample size),
how should we allocate these new samples across the four
populations in order to maximize the number of new vari-
ants discovered? We use ˆHcount to predict the extrapola-
tion curves for three scenarios: 1. all the samples are al-
located to Europeans (current genomic studies are heavily
enriched of Europeans); 2.
the samples are evenly allo-
cated across the four populations; 3) we explicitly optimize
the factors tj using ˆHcount. The dotted curves in Fig. 2(b)
correspond to the predictions, and the solid curves are the
actual numbers using the true distribution, showing good
agreement. Optimization using ˆHcount led to 10 % in-
crease in the number of new variants discovered. This is a
simplistic example (there are many other factors in the de-
sign of real cohorts) but it illustrate the potential power in
having multi-population histogram estimates. In Appendix
Fig. 3, we also show that ˆHcount gives accurate predictions
for a different statistic—the number of new variants we ex-
pect to ﬁnd at least twice in the new samples.

7. Discussion

We introduce and formalize the problem of multi-
population unseen estimation. We provide a weighted lin-
ear estimator for the number of new elements and a general
optimization algorithm to estimate the multi-population
histogram. These two approaches have complementary
strength. The weighted linear estimator ˆU W speciﬁcally
estimates the number of unseen elements.
It’s accuracy
is independent of the number of populations, m, and it is
worst-case optimal. This can be a good method especially
when m is large and the extrapolation factor is small com-
pared to log of the number of observed samples. When the
extrapolation is larger, however, ˆU W is consistently down-
ward biased due to its variance-reducing weights. For rela-
tively small number of populations (m = 2, 3, 4) and larger
extrapolation factors, the unseen predictions of our his-
togram estimators, ˆHcount and ˆHll are signiﬁcantly more
accurate than ˆU W . While both likely have comparable
worst-case performance, the linear estimator nearly always
incurs this worst-case loss and is largely incapable of ex-
trapolating beyond this worst-case logarithmic factor.
In
contrast, the histogram-based estimators seem to perform
well for much larger extrapolation factors on all of the dis-
tributions that we considered. ˆHcount and ˆHll are compu-
tationally more expensive than ˆU W , but are still tractable
for many applications—each run of our experiments took
less than 20 minutes on a single laptop.

Estimating the unseen from multiple populations

Acknowledgments

Gregory Valiant’s contributions were supported by NSF
CAREER CCF-1351108 and a Sloan Research Fellowship.
James Zou is a Chan Zuckerberg Biohub investigator and
is supported by NSF CISE-1657155.

Zou, James, Valiant, Gregory, Valiant, Paul, Karczewski,
Konrad, Chan, Siu On, Samocha, Kaitlin, Lek, Monkol,
Sunyaev, Shamil, Daly, Mark, and MacArthur, Daniel G.
Quantifying unobserved protein-coding variants in hu-
man populations provides a roadmap for large-scale se-
quencing projects. Nature Communications, 7, 2016.

References

Acharya, Jayadev, Das, Hirakendu, Orlitsky, Alon, and
Suresh, Ananda Theertha. A uniﬁed maximum likeli-
hood approach for optimal distribution property estima-
tion. arXiv preprint arXiv:1611.02960, 2016.

Efron, Bradley and Thisted, Ronald. Estimating the num-
ber of unsen species: How many words did shakespeare
know? Biometrika, pp. 435–447, 1976.

Fisher, Ronald A, Corbet, A Steven, and Williams, Car-
rington B. The relation between the number of species
and the number of individuals in a random sample of an
animal population. The Journal of Animal Ecology, pp.
42–58, 1943.

Good, IJ and Toulmin, GH. The number of new species,
and the increase in population coverage, when a sample
is increased. Biometrika, 43(1-2):45–63, 1956.

Good, Irving J. The population frequencies of species and
the estimation of population parameters. Biometrika, pp.
237–264, 1953.

Lek, Monkol, Karczewski, Konrad J, Minikel, Eric V,
Samocha, Kaitlin E, Banks, Eric, Fennell, Timothy,
ODonnell-Luria, Anne H, Ware, James S, Hill, An-
drew J, Cummings, Beryl B, et al. Analysis of protein-
coding genetic variation in 60,706 humans. Nature, 536
(7616):285–291, 2016.

Orlitsky, Alon, Suresh, Ananda Theertha, and Wu, Yihong.
Optimal prediction of the number of unseen species.
Proceedings of the National Academy of Sciences, pp.
201607774, 2016.

Valiant, Gregory and Valiant, Paul. Estimating the unseen:
an n/log (n)-sample estimator for entropy and support
size, shown optimal via new clts. In Proceedings of the
forty-third annual ACM symposium on Theory of com-
puting, pp. 685–694. ACM, 2011.

Valiant, Gregory and Valiant, Paul. Instance optimal learn-
ing of discrete distributions. In Proceedings of the 48th
Annual ACM SIGACT Symposium on Theory of Comput-
ing, pp. 142–155. ACM, 2016.

Valiant, Paul and Valiant, Gregory. Estimating the unseen:
improved estimators for entropy and other properties. In
Advances in Neural Information Processing Systems, pp.
2157–2165, 2013.

A. Proof of Prop. 4.1 and Prop. 4.4

Estimating the unseen from multiple populations

Proof of Prop. 4.1. For each element x ∈ X , let λx,j = njpx,j, where px,j is the probability of x in population j. We have

The ﬁrst term in the sum is the probability that x is not observed in period one and the second term is the probability that
x is observed at least once in period two. Taylor expand the second term followed by Binomial expansion gives

E[U ] =

e− (cid:80)

j λx,j

1 − e− (cid:80)

j tj λx,j

(cid:16)

(cid:17)

.

(cid:88)

x

E[U ] =

(cid:88)

e− (cid:80)

j λx,j

(−1)i+1

((cid:80)

j tjλx,j)i

∞
(cid:88)

i=1

= −

(cid:88)

e− (cid:80)

j λx,j (cid:88)

x

x

i!

m
(cid:89)

m
(cid:89)

j=1

(−tjλx,j)ij
ij!

(−tjλx,j)ij
ij!

i1,...,im:(cid:80) ij >0

j=1

(cid:88)

e− (cid:80)

j λx,j

x




m
(cid:89)


 E[φi1...im ].

(−tj)ij

i1,...,im:(cid:80) ij >0

= −

= −

(cid:88)

(cid:88)

i1,...,im:(cid:80) ij >0

j=1

It’s easy to see that ˆU is an unbiased estimator of the last expression.

Weighting the ﬁngerprints reduces the variance of the estimator at the cost of introducing bias. We analyze the bias and
variance of ˆU W separately. The proof follows the strategy of the analysis for the one population setting in (Orlitsky et al.,
2016).
Lemma A.1 (Lemma 4.8 restated). Let n = (cid:80)m
rate of the Poisson weights, then

j=1 nj denote the total number of samples in period one and r denote the

(cid:12)
(cid:12)
(cid:12)

(cid:12)
E[ ˆU W − U ]
(cid:12)
(cid:12) ≤


 e−r

nj(tj + 1)





(cid:88)

j∈A

Proof. For each element x, its contribution to E[ ˆU W ] can be written as

= −e− (cid:80)m

j=1 λx,j





(cid:88)

(cid:89)

(−tj)ij

−e− (cid:80)m

j=1 λx,j

(cid:88)

(−tj)ij

P

L ≥





ij

 − 1



(cid:88)

j∈A





i1,...,im




m
(cid:89)

j=1

ij :j(cid:54)∈A

j(cid:54)∈A


e− (cid:80)




λij
x,j
ij!









λij
x,j
ij!

(cid:88)

(cid:89)

j∈A

ij :j∈A


(cid:88)

j∈A

∞
(cid:88)

i=0

P(L ≥ i)
i!

λij
x,j
ij!


i






= −e− (cid:80)m

j=1 λx,j

j(cid:54)∈A tj λx,j

−

tjλx,j



− 1









(−tj)ij

P

L ≥

ij



 − 1



(cid:88)

j∈A

We use the following two facts from (Orlitsky et al., 2016).

Fact 1 For all y > 0 and for any random variable L,

(cid:12)
(cid:12)
(cid:12)
−
(cid:12)
(cid:12)

∞
(cid:88)

i=0

P(L ≥ i)
i!

(−y)i + e−y

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ max
s≤y

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

(cid:20) (−s)L
L!

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)1 − e−y(cid:1) .

Fact 2

If L ∼ Poi(r), then

Estimating the unseen from multiple populations

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

(cid:20) (−s)L
L!

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ e−r.

Therefore, the contribution of x to E[ ˆU W − U ] is

e− (cid:80)m

j=1 λx,j −(cid:80)

j(cid:54)∈A tj λx,j

tjλx,j



+ e− (cid:80)

j∈A tj λx,j

(cid:16)

1 − e− (cid:80)

j∈A tj λx,j

(cid:17)

e−r




−

∞
(cid:88)

i=0

P(L ≥ i)
i!



−

(cid:88)

j∈A



i




 ≤

where we used Facts 1 and 2 with y = (cid:80)

j∈A tjλx,j and the fact that e− (cid:80)m

j=1 λx,j −(cid:80)

j(cid:54)∈A tj λx,j ≤ 1.

Now summing over x ∈ X , we have

(cid:12)
(cid:12)
(cid:12)

(cid:12)
E[ ˆU W − U ]
(cid:12)
(cid:12) ≤

1 − e− (cid:80)

j∈A tj λx,j

(cid:17)

e−r

1 − e− (cid:80)

j∈A(tj +1)λx,j

(cid:17)

e−r

(cid:88)

(cid:16)

x
(cid:88)

(cid:16)

x
(cid:88)

x





(cid:88)

j∈A

≤

=

≤

= (cid:0)E[U A] + E[ΦA

+](cid:1) e−r

 e−r

nj(tj + 1)

(cid:104)
e− (cid:80)

j∈A λx,j

(cid:16)

1 − e− (cid:80)

j∈A tj λx,j

(cid:17)

+ 1 − e− (cid:80)

j∈A λx,j

(cid:105)

e−r

where ΦA
of new elements observed in period two for j ∈ A.

+ is the total number of distinct elements observed in period one for subpopulations j ∈ A and U A is the number

Lemma 4.8 quantiﬁes the bias of the weighted estimator. Next we quantify its variance.

Lemma A.2 (Lemma 4.9 restated). Without loss of generality, let t1 = maxj tj and suppose t1 ≥ 1 then

Var( ˆU W − U ) ≤ ne2r(t1−1) +

(cid:88)

j

njtj.

Proof. Let Nx,j be the random variable corresponding to the number of times x is found in population j during period
one. Let N (cid:48)
x,j be the random variable corresponding to the number of times x is found in population j during period two.
Deﬁne h(i1, ..., im) = − (cid:81)m

(cid:16)
L ≥ (cid:80)

(cid:17)

j=1(−tij )ij P

j∈A ij

.

For every element x, its contribution to Var( ˆU W − U ) is

1Nx,j =ij

 h(i1, ..., im) −



1Nx,j =0



1 −

1N (cid:48)

x,j =0





















Var

(cid:88)

(cid:89)













j

i1,...,im


(cid:88)

(cid:89)







j

j

i1,...,im

i1,...,im

≤ E

1Nx,j =ij

 h(i1, ..., im) −



1Nx,j =0



1 −

1N (cid:48)

x,j =0












2

= E

(cid:88)

(cid:89)

1Nx,j =ij


 h(i1, ..., im)2 +













1Nx,j =0



1 −

1N (cid:48)

x,j =0



 .



(cid:89)

j



(cid:89)

j

(cid:89)

j

(cid:89)

j

(cid:89)

j

(cid:89)

j

Estimating the unseen from multiple populations

The last equality follows because the cross-term vanishes since the events Nx,j = 0, ∀j and Nx,j = ij, (cid:80)
disjoint. Summing over all x gives

j ij > 0 are

Var( ˆU W − U ) ≤ E[Φ+]

h(i1, ..., im)2 + E[U ]

sup
i1,...,im

≤ n sup

h(i1, ..., im)2 +

njtj.

i1,...,im

(cid:88)

j

(4)

(5)

Moreover we have

|h(i1, ..., im)| =





(cid:89)

tij
j

j




 P

L ≥



ij



(cid:88)

j∈A



(cid:80)

j∈A ij

≤ t

1

P

L ≥



ij



(cid:88)

j

≤ er(t1−1)

where we have used the following fact:

Fact 3

If L ∼ Poi(r) and t ≥ 1, then for all i > 0

Note that only tmax ≥ 1 is assumed here; the other tj’s could be less than 1.

tiP(L ≥ i) ≤ er(t−1).

Putting the last two lemmas together, we have

Lemma A.3. Let t1 = maxj tj ≥ 1, then

(cid:104)
( ˆU W − U )2(cid:105)

E

≤ ne2r(t1−1) +

njtj +

nj(tj + 1)



e−2r

(cid:88)

j





(cid:88)

j∈A


2

Because t1 ≥ 1, Lemma A.3 implies that

( ˆU W − U )2(cid:105)
(cid:104)

E

≤ (n +

(cid:88)

j

njtj)e2r(t1−1) + (n +

njtj)2e−2r.

(cid:88)

j

The two terms on the RHS are equal when r =

. Using this value of r, we have

log(n+(cid:80)

j nj tj )

2tmax


(cid:32) ˆU W − U

(cid:80) njtj

E

(cid:33)2

 ≤

(cid:19)2

(cid:18) n + t1 ¯n
t ¯n

(n + t1 ¯n)−1/t1

≤

≤

(cid:18) n + t1 ¯n
t1 ¯n
(cid:18) n + t1n1
t1n1

(cid:19)2

¯n−1/t1

(cid:19)2

n−1/t1
1

where ¯n ≡ (cid:80)

j njtj/t1. This completes the proof of Prop. 4.4.

Estimating the unseen from multiple populations

Figure 3. Estimating the number of new mutations that would be observed two or more times given additional samples from four different
populations using hist-opt-counts. We consider different ratios of sampling within these four populations and observe the change in
number of new mutations that would be observed for a ﬁxed total number of new samples.

B. Multi-population Earth Mover’s Distance

We deﬁne a natural distance metric on multi-population histograms, which is a measure of the extent to which the corre-
sponding distributions are similar, up to a relabeling of the elements:
Deﬁnition B.1. Given two m-population histograms, H, H (cid:48), the multi-population earthmover distance dW (H, H (cid:48)) is
deﬁned as the minimum over all schemes of moving the histogram elements in H to yield H (cid:48), where the cost of moving c
2m (cid:107)α − α(cid:48)(cid:107)1 = (cid:80)m
histogram elements from α ∈ [0, 1]m to α(cid:48) is c 1
i|. To ensure that such a scheme exists, we
regard there being an inﬁnitude of elements that occur with probability zero in all populations, H(0) = H (cid:48)(0) = ∞.

i=1 |αi − α(cid:48)

Note that for all pairs of histograms H, H (cid:48) it holds that dW (H, H (cid:48)) ∈ [0, 1], with dW (H, H (cid:48)) = 0 if and only if the
distributions corresponding to H and H (cid:48) are identical, up to relabeling the domain elements. The following example
illustrates the above deﬁnition:

Example B.1. Consider a 3 population distribution corresponding to a three uniform distributions over 2n elements, where
n of the elements are common to all 3 populations, and the other elements are unique. This corresponds to histogram H
deﬁned by H(1/2n, 1/2n, 1/2n) = n, H(1/2n, 0, 0) = n, H(0, 1/2n, 0) = n, H(0, 0, 1/2n) = n. Consider a second
histogram H (cid:48) corresponding to three uniform distributions over a common set of n elements, H (cid:48)(1/n, 1/n, 1/n) = n, and
H (cid:48)(α) = 0 for all α (cid:54)= (1/n, 1/n, 1/n). The EMD

dW (H, H (cid:48)) =

1
2 · 3

(cid:18)

n

3
2n

+ 3n

(cid:19)

1
2n

=

1
2

,

Since we can make H (cid:48) from H by moving n histogram elements from (1/2n, 1/2n, 1/2n) to (1/n, 1/n, 1/n) at a per-unit-
cost of (cid:107)( 1
2n , and then moving the remaining 3n elements of H to (0, 0, 0) at a per-unit-cost
of 1/2n.

n )(cid:107)1 = 3

2n ) − ( 1

2n , 1

2n , 1

n , 1

n , 1

C. Additional experiments
We tested the prediction accuracy of ˆHcount on a different statistic: the number of elements we expect to ﬁnd at least twice
in the new samples, see Fig. 3.

