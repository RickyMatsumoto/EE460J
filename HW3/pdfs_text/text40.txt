Wasserstein Generative Adversarial Networks

A. Why Wasserstein is indeed weak

We now introduce our notation. Let X ⊆ Rd be a compact set (such as [0, 1]d the space of images). We deﬁne Prob(X ) to
be the space of probability measures over X . We note

Cb(X ) = {f : X → R, f is continuous and bounded}

Note that if f ∈ Cb(X ), we can deﬁne (cid:107)f (cid:107)∞ = maxx∈X |f (x)|, since f is bounded. With this norm, the space (Cb(X ), (cid:107) ·
(cid:107)∞) is a normed vector space. As for any normed vector space, we can deﬁne its dual

Cb(X )∗ = {φ : Cb(X ) → R, φ is linear and continuous}

and give it the dual norm (cid:107)φ(cid:107) = supf ∈Cb(X ),(cid:107)f (cid:107)∞≤1 |φ(f )|.
With this deﬁnitions, (Cb(X )∗, (cid:107) · (cid:107)) is another normed space. Now let µ be a signed measure over X , and let us deﬁne the
total variation distance

(cid:107)µ(cid:107)T V = sup
A⊆X

|µ(A)|

δ(Pr, Pθ) := (cid:107)Pr − Pθ(cid:107)T V

where the supremum is taken over the Borel sets in X . Since the total variation is a norm, then if we have Pr and Pθ two
probability distributions over X ,

is a distance in Prob(X ) (called the total variation distance).

We can consider

Φ : (Prob(X ), δ) → (Cb(X )∗, (cid:107) · (cid:107))

where Φ(P)(f ) := Ex∼P[f (x)] is a linear function over Cb(X ). The Riesz Representation theorem ((Kakutani, 1941),
Theorem 10) tells us that Φ is an isometric immersion. This tells us that we can effectively consider Prob(X ) with the
total variation distance as a subset of Cb(X )∗ with the norm distance. Thus, just to accentuate it one more time, the total
variation over Prob(X ) is exactly the norm distance over Cb(X )∗.

Let us stop for a second and analyze what all this technicality meant. The main thing to carry is that we introduced a
distance δ over probability distributions. When looked as a distance over a subset of Cb(X )∗, this distance gives the norm
topology. The norm topology is very strong. Therefore, we can expect that not many functions θ (cid:55)→ Pθ will be continuous
when measuring distances between distributions with δ. As we will show later in Theorem 2, δ gives the same topology as
the Jensen-Shannon divergence, pointing to the fact that the JS is a very strong distance, and is thus more propense to give
a discontinuous loss function.

Now, all dual spaces (such as Cb(X )∗ and thus Prob(X )) have a strong topology (induced by the norm), and a weak*
topology. As the name suggests, the weak* topology is much weaker than the strong topology. In the case of Prob(X ),
the strong topology is given by the total variation distance, and the weak* topology is given by the Wasserstein distance
(among others) (Villani, 2009).

B. Assumption deﬁnitions

Assumption 1. Let g : Z × Rd → X be locally Lipschitz between ﬁnite dimensional vector spaces. We will denote gθ(z)
it’s evaluation on coordinates (z, θ). We say that g satisﬁes assumption 1 for a certain probability distribution p over Z if
there are local Lipschitz constants L(θ, z) such that

Ez∼p[L(θ, z)] < +∞

C. Proofs of things

Proof of Theorem 1. Let θ and θ(cid:48) be two parameter vectors in Rd. Then, we will ﬁrst attempt to bound W (Pθ, Pθ(cid:48)), from
where the theorem will come easily. The main element of the proof is the use of the coupling γ, the distribution of the joint
(gθ(Z), gθ(cid:48)(Z)), which clearly has γ ∈ Π(Pθ, Pθ(cid:48)).

By the deﬁnition of the Wasserstein distance, we have

Wasserstein Generative Adversarial Networks

W (Pθ, Pθ(cid:48)) ≤

(cid:107)x − y(cid:107) dγ

(cid:90)

X ×X

= E(x,y)∼γ[(cid:107)x − y(cid:107)]
= Ez[(cid:107)gθ(z) − gθ(cid:48)(z)(cid:107)]

If g is continuous in θ, then gθ(z) →θ→θ(cid:48) gθ(cid:48)(z), so (cid:107)gθ − gθ(cid:48)(cid:107) → 0 pointwise as functions of z. Since X is compact, the
distance of any two elements in it has to be uniformly bounded by some constant M , and therefore (cid:107)gθ(z) − gθ(cid:48)(z)(cid:107) ≤ M
for all θ and z uniformly. By the bounded convergence theorem, we therefore have

Finally, we have that

proving the continuity of W (Pr, Pθ).

W (Pθ, Pθ(cid:48)) ≤ Ez[(cid:107)gθ(z) − gθ(cid:48)(z)(cid:107)] →θ→θ(cid:48) 0

|W (Pr, Pθ) − W (Pr, Pθ(cid:48))| ≤ W (Pθ, Pθ(cid:48)) →θ→θ(cid:48) 0

Now let g be locally Lipschitz. Then, for a given pair (θ, z) there is a constant L(θ, z) and an open set U such that
(θ, z) ∈ U , such that for every (θ(cid:48), z(cid:48)) ∈ U we have

(cid:107)gθ(z) − g(cid:48)

θ(z(cid:48))(cid:107) ≤ L(θ, z)((cid:107)θ − θ(cid:48)(cid:107) + (cid:107)z − z(cid:48)(cid:107))

By taking expectations and z(cid:48) = z we

Ez[(cid:107)gθ(z) − gθ(cid:48)(z)(cid:107)] ≤ (cid:107)θ − θ(cid:48)(cid:107)Ez[L(θ, z)]

whenever (θ(cid:48), z) ∈ U . Therefore, we can deﬁne Uθ = {θ(cid:48)|(θ(cid:48), z) ∈ U }. It’s easy to see that since U was open, Uθ is as
well. Furthermore, by assumption 1, we can deﬁne L(θ) = Ez[L(θ, z)] and achieve

|W (Pr, Pθ) − W (Pr, Pθ(cid:48))| ≤ W (Pθ, Pθ(cid:48)) ≤ L(θ)(cid:107)θ − θ(cid:48)(cid:107)

for all θ(cid:48) ∈ Uθ, meaning that W (Pr, Pθ) is locally Lipschitz. This obviously implies that W (Pr, Pθ) is everywhere
continuous, and by Radamacher’s theorem we know it has to be differentiable almost everywhere.

The counterexample for item 3 of the Theorem is indeed Example 1.

Proof of Corollary 1. We begin with the case of smooth nonlinearities. Since g is C 1 as a function of (θ, z) then for any
ﬁxed (θ, z) we have L(θ, Z) ≤ (cid:107)∇θ,xgθ(z)(cid:107) + (cid:15) is an acceptable local Lipschitz constant for all (cid:15) > 0. Therefore, it
sufﬁces to prove

Ez∼p(z)[(cid:107)∇θ,zgθ(z)(cid:107)] < +∞

If H is the number of layers we know that ∇zgθ(z) = (cid:81)H
k=1 WkDk where Wk are the weight matrices and Dk is are the
diagonal Jacobians of the nonlinearities. Let fi:j be the application of layers i to j inclusively (e.g. gθ = f1:H ). Then,
f1:k−1(z). We recall that if L is the Lipschitz constant of the nonlinearity, then
∇Wk gθ(z) =
(cid:107)Di(cid:107) ≤ L and (cid:107)f1:k−1(z)(cid:107) ≤ (cid:107)z(cid:107)Lk−1 (cid:81)k−1

i=1 Wi. Putting this together,

i=k+1 WiDi

(cid:16)(cid:16)(cid:81)H

Dk

(cid:17)

(cid:17)

(cid:107)∇z,θgθ(z)(cid:107) ≤ (cid:107)

WiDi(cid:107) +

WiDi

Dk

f1:k−1(z)(cid:107)

(cid:32)(cid:32) H
(cid:89)

(cid:33)

(cid:33)

≤ LH

(cid:107)Wi(cid:107) +

(cid:107)z(cid:107)LH

(cid:107)Wi(cid:107)

i=k+1

(cid:32)k−1
(cid:89)

(cid:33) (cid:32) H
(cid:89)

(cid:33)

(cid:107)Wi(cid:107)

H
(cid:89)

i=1

K
(cid:89)

i=H

H
(cid:88)

k=1

(cid:107)

H
(cid:88)

k=1

If C1(θ) = LH (cid:16)(cid:81)H

(cid:17)
i=1 (cid:107)Wi(cid:107)

and C2(θ) = (cid:80)H

k=1 LH (cid:16)(cid:81)k−1

i=1 (cid:107)Wi(cid:107)

i=1
(cid:17) (cid:16)(cid:81)H

i=k+1

(cid:17)

i=k+1 (cid:107)Wi(cid:107)

then

Ez∼p(z)[(cid:107)∇θ,zgθ(z)(cid:107)] ≤ C1(θ) + C2(θ)Ez∼p(z)[(cid:107)z(cid:107)] < +∞

ﬁnishing the proof

Proof of Theorem 2.

Wasserstein Generative Adversarial Networks

1.

• (δ(Pn, P) → 0 ⇒ JS(Pn, P) → 0) — Let Pm be the mixture distribution Pm = 1
2

P (note that Pm
depends on n). It is easily veriﬁed that δ(Pm, Pn) ≤ δ(Pn, P), and in particular this tends to 0 (as does δ(Pm, P)).
We now show this for completeness. Let µ be a signed measure, we deﬁne (cid:107)µ(cid:107)T V = supA⊆X |µ(A)|. for all
Borel sets A. In this case,

Pn + 1
2

Let fn = dPn
dPm
every Borel set A we have Pn(A) ≤ 2Pm(A). If A = {fn > 3} then we get

be the Radon-Nykodim derivative between Pn and the mixture. Note that by construction for

which implies Pm(A) = 0. This means that fn is bounded by 3 Pm(and therefore Pn and P)-almost everywhere.
We could have done this for any constant larger than 2 but for our purposes 3 will suﬁce.
Let (cid:15) > 0 ﬁxed, and An = {fn > 1 + (cid:15)}. Then,

Therefore,

δ(Pm, Pn) = (cid:107)Pm − Pn(cid:107)T V

Pn − Pn(cid:107)T V

P +

1
1
2
2
(cid:107)P − Pn(cid:107)T V

δ(Pn, P) ≤ δ(Pn, P)

Pn(A) =

fn dPm ≥ 3Pm(A)

= (cid:107)

=

=

1
2
1
2

(cid:90)

A

Pn(An) =

fn dPm ≥ (1 + (cid:15))Pm(An)

(cid:90)

An

(cid:15)Pm(An) ≤ Pn(An) − Pm(An)
≤ |Pn(An) − Pm(An)|
≤ δ(Pn, Pm)
≤ δ(Pn, P).

≤

≤

≤

1
(cid:15)
1
(cid:15)
(cid:18) 1
(cid:15)

δ(Pn, P) + δ(Pn, Pm)

δ(Pn, P) + δ(Pn, P)

(cid:19)

+ 1

δ(Pn, P)

Which implies Pm(Am) ≤ 1

(cid:15) δ(Pn, P). Furthermore,

Pn(An) ≤ Pm(An) + |Pn(An) − Pm(An)|

We now can see that

KL(Pn(cid:107)Pm) =

(cid:90)

log(fn) dPn
(cid:90)

≤ log(1 + (cid:15)) +

log(fn) dPn

An

≤ log(1 + (cid:15)) + log(3)Pn(An)

≤ log(1 + (cid:15)) + log(3)

(cid:19)

+ 1

δ(Pn, P)

(cid:18) 1
(cid:15)

Wasserstein Generative Adversarial Networks

Taking limsup we get 0 ≤ lim sup KL(Pn(cid:107)Pm) ≤ log(1 + (cid:15)) for all (cid:15) > 0, which means KL(Pn(cid:107)Pm) → 0.
In the same way, we can deﬁne gn = dP
dPm

, and

2Pm({gn > 3}) ≥ P({gn > 3}) ≥ 3Pm({gn > 3})

meaning that Pm({gn > 3}) = 0 and therefore gn is bounded by 3 almost everywhere for Pn, Pm and P. With
the same calculation, Bn = {gn > 1 + (cid:15)} and

so Pm(Bn) ≤ 1

(cid:15) δ(P, Pm) → 0, and therefore P(Bn) → 0. We can now show

so we achieve 0 ≤ lim sup KL(P(cid:107)Pm) ≤ log(1 + (cid:15)) and then KL(P(cid:107)Pm) → 0. Finally, we conclude

JS(Pn, P) =

KL(Pn(cid:107)Pm) +

KL(P(cid:107)Pm) → 0

1
2

• (JS(Pn, P) → 0 ⇒ δ(Pn, P) → 0) — by a simple application of the triangular and Pinsker’s inequalities

we get

P(Bn) =

gn dPm ≥ (1 + (cid:15))Pm(Bn)

(cid:90)

Bn

KL(P(cid:107)Pm) =

(cid:90)

log(gn) dP
(cid:90)

≤ log(1 + (cid:15)) +

log(gn) dP

≤ log(1 + (cid:15)) + log(3)P(Bn)

Bn

1
2

δ(Pn, P) ≤ δ(Pn, Pm) + δ(P, Pm)

(cid:114) 1
2

KL(Pn(cid:107)Pm) +
≤
≤ 2(cid:112)JS(Pn, P) → 0

(cid:114) 1
2

KL(P(cid:107)Pm)

δ(Pn, P) ≤

KL(Pn(cid:107)P) → 0

δ(P, Pn) ≤

KL(P(cid:107)Pn) → 0

(cid:114) 1
2
(cid:114) 1
2

2. This is a long known fact that W metrizes the weak* topology of (C(X ), (cid:107) · (cid:107)∞) on Prob(X ), and by deﬁnition this

is the topology of convergence in distribution. A proof of this can be found (for example) in (Villani, 2009).

3. This is a straightforward application of Pinsker’s inequality

4. This is trivial by recalling the fact that δ and W give the strong and weak* topologies on the dual of (C(X ), (cid:107) · (cid:107)∞)

when restricted to Prob(X ).

Proof of Theorem 3. Let us deﬁne

V ( ˜f , θ) = Ex∼Pr [ ˜f (x)] − Ex∼Pθ [ ˜f (x)]

= Ex∼Pr [ ˜f (x)] − Ez∼p(z)[ ˜f (gθ(z))]

where ˜f lies in F = { ˜f : X → R , ˜f ∈ Cb(X ), (cid:107) ˜f (cid:107)L ≤ 1} and θ ∈ Rd.

Since X is compact, we know by the Kantorovich-Rubinstein duality (Villani, 2009) that there is an f ∈ F that attains the
value

Let us deﬁne X ∗(θ) = {f ∈ F : V (f, θ) = W (Pr, Pθ)}. By the above point we know then that X ∗(θ) is non-empty. We
know by a simple envelope theorem ((Milgrom & Segal, 2002), Theorem 1) that

Wasserstein Generative Adversarial Networks

W (Pr, Pθ) = sup
˜f ∈F

V ( ˜f , θ) = V (f, θ)

∇θW (Pr, Pθ) = ∇θV (f, θ)

for any f ∈ X ∗(θ) when both terms are well-deﬁned.

Let f ∈ X ∗(θ), which we knows exists since X ∗(θ) is non-empty for all θ. Then, we get

∇θW (Pr, Pθ) = ∇θV (f, θ)

= ∇θ[Ex∼Pr [f (x)] − Ez∼p(z)[f (gθ(z))]
= −∇θEz∼p(z)[f (gθ(z))]

under the condition that the ﬁrst and last terms are well-deﬁned. The rest of the proof will be dedicated to show that

−∇θEz∼p(z)[f (gθ(z))] = −Ez∼p(z)[∇θf (gθ(z))]

(4)

when the right hand side is deﬁned. For the reader who is not interested in such technicalities, he or she can skip the rest
of the proof.

Since f ∈ F, we know that it is 1-Lipschitz. Furthermore, gθ(z) is locally Lipschitz as a function of (θ, z). Therefore,
f (gθ(z)) is locally Lipschitz on (θ, z) with constants L(θ, z) (the same ones as g). By Radamacher’s Theorem, f (gθ(z))
has to be differentiable almost everywhere for (θ, z) jointly. Rewriting this, the set A = {(θ, z) : f ◦ g is not differentiable}
has measure 0. By Fubini’s Theorem, this implies that for almost every θ the section Aθ = {z : (θ, z) ∈ A} has measure 0.
Let’s now ﬁx a θ0 such that the measure of Aθ0 is null (such as when the right hand side of equation (4) is well deﬁned).
For this θ0 we have ∇θf (gθ(z))|θ0 is well-deﬁned for almost any z, and since p(z) has a density, it is deﬁned p(z)-a.e. By
assumption 1 we know that

Ez∼p(z)[(cid:107)∇θf (gθ(z))|θ0 (cid:107)] ≤ Ez∼p(z)[L(θ0, z)] < +∞

so Ez∼p(z)[∇θf (gθ(z))|θ0] is well-deﬁned for almost every θ0. Now, we can see

Ez∼p(z)[f (gθ(z))] − Ez∼p(z)[f (gθ0 (z))] − (cid:104)(θ − θ0), Ez∼p(z)[∇θf (gθ(z))|θ0 ](cid:105)
(cid:107)θ − θ0(cid:107)

(5)

= Ez∼p(z)

(cid:20) f (gθ(z)) − f (gθ0(z)) − (cid:104)(θ − θ0), ∇θf (gθ(z))|θ0 (cid:105)
(cid:107)θ − θ0(cid:107)

(cid:21)

By differentiability, the term inside the integral converges p(z)-a.e. to 0 as θ → θ0. Furthermore,

(cid:107)

f (gθ(z)) − f (gθ0 (z)) − (cid:104)(θ − θ0), ∇θf (gθ(z))|θ0(cid:105)
(cid:107)θ − θ0(cid:107)

(cid:107)

≤

(cid:107)θ − θ0(cid:107)L(θ0, z) + (cid:107)θ − θ0(cid:107)(cid:107)∇θf (gθ(z))|θ0 (cid:107)
(cid:107)θ − θ0(cid:107)

and since Ez∼p(z)[2L(θ0, z)] < +∞ by assumption 1, we get by dominated convergence that Equation 5 converges to 0 as
θ → θ0 so

∇θEz∼p(z)[f (gθ(z))] = Ez∼p(z)[∇θf (gθ(z))]
for almost every θ, and in particular when the right hand side is well deﬁned. Note that the mere existance of the left hand
side (meaning the differentiability a.e. of Ez∼p(z)[f (gθ(z))]) had to be proven, which we just did.

≤ 2L(θ0, z)

D. Related Work

Wasserstein Generative Adversarial Networks

There’s been a number of works on the so called Integral Probability Metrics (IPMs) (M¨uller, 1997). Given F a set of
functions from X to R, we can deﬁne

dF (Pr, Pθ) = sup
f ∈F

Ex∼Pr [f (x)] − Ex∼Pθ [f (x)]

(6)

as an integral probability metric associated with the function class F. It is easily veriﬁed that if for every f ∈ F we have
−f ∈ F (such as all examples we’ll consider), then dF is nonnegative, satisﬁes the triangular inequality, and is symmetric.
Thus, dF is a pseudometric over Prob(X ).

While IPMs might seem to share a similar formula, as we will see different classes of functions can yield to radically
different metrics.

• By the Kantorovich-Rubinstein duality (Villani, 2009), we know that W (Pr, Pθ) = dF (Pr, Pθ) when F is the set of
1-Lipschitz functions. Furthermore, if F is the set of K-Lipschitz functions, we get K · W (Pr, Pθ) = dF (Pr, Pθ).

• When F is the set of all continuous functions bounded between -1 and 1, we retrieve dF (Pr, Pθ) = δ(Pr, Pθ) the total
variation distance (M¨uller, 1997). This already tells us that going from 1-Lipschitz to 1-Bounded functions drastically
changes the topology of the space, and the regularity of dF (Pr, Pθ) as a loss function (as by Theorems 1 and 2).

• Energy-based GANs (EBGANs) (Zhao et al., 2016) can be thought of as the generative approach to the total variation
distance. This connection is stated and proven in depth in Appendix E. At the core of the connection is that the
discriminator will play the role of f maximizing equation (6) while its only restriction is being between 0 and m for
some constant m. This will yield the same behaviour as being restricted to be between −1 and 1 up to a constant
scaling factor irrelevant to optimization. Thus, when the discriminator approaches optimality the cost for the generator
will aproximate the total variation distance δ(Pr, Pθ).
Since the total variation distance displays the same regularity as the JS, it can be seen that EBGANs will suffer
from the same problems of classical GANs regarding not being able to train the discriminator till optimality and thus
limiting itself to very imperfect gradients.

• Maximum Mean Discrepancy (MMD) (Gretton et al., 2012) is a speciﬁc case of integral probability metrics when
F = {f ∈ H : (cid:107)f (cid:107)H ≤ 1} for H some Reproducing Kernel Hilbert Space (RKHS) associated with a given kernel
k : X × X → R. As proved on (Gretton et al., 2012) we know that MMD is a proper metric and not only a
pseudometric when the kernel is universal. In the speciﬁc case where H = L2(X , m) for m the normalized Lebesgue
measure on X , we know that {f ∈ Cb(X ), (cid:107)f (cid:107)∞ ≤ 1} will be contained in F, and therefore δ(Pr, Pθ) ≤ dF (Pr, Pθ),
so the regularity of the MMD distance as a loss function will be at least as bad as the one of the total variation.
Nevertheless this is a very extreme case, since we would need a very powerful kernel to approximate the whole L2.
However, even Gaussian kernels are able to detect tiny noise patterns as recently evidenced by (Sutherland et al.,
2017). This points to the fact that especially with low bandwidth kernels, the distance might be close to a saturating
regime similar as with total variation or the JS. This obviously doesn’t need to be the case for every kernel, and
ﬁguring out how and which different MMDs are closer to Wasserstein or total variation distances is an interesting
topic of research.

The great aspect of MMD is that via the kernel trick there is no need to train a separate network to maximize equation
(6) for the ball of a RKHS. However, this has the disadvantage that evaluating the MMD distance has computational
cost that grows quadratically with the amount of samples used to estimate the expectations in (6). This last point makes
MMD have limited scalability, and is sometimes inapplicable to many real life applications because of it. There are
estimates with linear computational cost for the MMD (Gretton et al., 2012) which in a lot of cases makes MMD very
useful, but they also have worse sample complexity.

• Generative Moment Matching Networks (GMMNs) (Li et al., 2015; Dziugaite et al., 2015) are the generative counter-
part of MMD. By backproping through the kernelized formula for equation (6), they directly optimize dMM D(Pr, Pθ)
(the IPM when F is as in the previous item). As mentioned, this has the advantage of not requiring a separate network
to approximately maximize equation (6). However, GMMNs have enjoyed limited applicability. Partial explanations
for their unsuccess are the quadratic cost as a function of the number of samples and vanishing gradients for low-
bandwidth kernels. Furthermore, it may be possible that some kernels used in practice are unsuitable for capturing

Wasserstein Generative Adversarial Networks

very complex distances in high dimensional sample spaces such as natural images. This is properly justiﬁed by the
fact that (Ramdas et al., 2014) shows that for the typical Gaussian MMD test to be reliable (as in it’s power as a statis-
tical test approaching 1), we need the number of samples to grow linearly with the number of dimensions. Since the
MMD computational cost grows quadratically with the number of samples in the batch used to estimate equation (6),
this makes the cost of having a reliable estimator grow quadratically with the number of dimensions, which makes it
very inapplicable for high dimensional problems. Indeed, for something as standard as 64x64 images, we would need
minibatches of size at least 4096 (without taking into account the constants in the bounds of (Ramdas et al., 2014)
which would make this number substantially larger) and a total cost per iteration of 40962, over 5 orders of magnitude
more than a GAN iteration when using the standard batch size of 64.

That being said, these numbers can be a bit unfair to the MMD, in the sense that we are comparing empirical sample
complexity of GANs with the theoretical sample complexity of MMDs, which tends to be worse. However, in the
original GMMN paper (Li et al., 2015) they indeed used a minibatch of size 1000, much larger than the standard 32 or
64 (even when this incurred in quadratic computational cost). While estimates that have linear computational cost as
a function of the number of samples exist (Gretton et al., 2012), they have worse sample complexity, and to the best
of our knowledge they haven’t been yet applied in a generative context such as in GMMNs.

On another great line of research, the recent work of (Montavon et al., 2016) has explored the use of Wasserstein distances
in the context of learning for Restricted Boltzmann Machines for discrete spaces. The motivations at a ﬁrst glance might
seem quite different, since the manifold setting is restricted to continuous spaces and in ﬁnite discrete spaces the weak
and strong topologies (the ones of W and JS respectively) coincide. However, in the end there is more in commmon than
not about our motivations. We both want to compare distributions in a way that leverages the geometry of the underlying
space, and Wasserstein allows us to do exactly that.

Finally, the work of (Genevay et al., 2016) shows new algorithms for calculating Wasserstein distances between different
distributions. We believe this direction is quite important, and perhaps could lead to new ways to evaluate generative
models.

E. Energy-based GANs optimize total variation

Wasserstein Generative Adversarial Networks

In this appendix we show that under an optimal discriminator, energy-based GANs (EBGANs) (Zhao et al., 2016) optimize
the total variation distance between the real and generated distributions.

Energy-based GANs are trained in a similar fashion to GANs, only under a different loss function. They have a discrimi-
nator D who tries to minimize

LD(D, gθ) = Ex∼Pr [D(x)] + Ez∼p(z)[[m − D(gθ(z))]+]

for some m > 0 and [x]+ = max(0, x) and a generator network gθ that’s trained to minimize

LG(D, gθ) = Ez∼p(z)[D(gθ(z))] − Ex∼Pr [D(x)]

Very importantly, D is constrained to be non-negative, since otherwise the trivial solution for D would be to set everything
to arbitrarily low values. The original EBGAN paper used only Ez∼p(z)[D(gθ(z))] for the loss of the generator, but this is
obviously equivalent to our deﬁnition since the term Ex∼Pr [D(x)] does not dependent on θ for a ﬁxed discriminator (such
as when backproping to the generator in EBGAN training) and thus minimizing one or the other is equivalent.
We say that a measurable function D∗ : X → [0, +∞) is optimal for gθ (or Pθ) if LD(D∗, gθ) ≤ LD(D, gθ) for all other
measurable functions D. We show that such a discriminator always exists for any two distributions Pr and Pθ, and that
under such a discriminator, LG(D∗, gθ) is proportional to δ(Pr, Pθ). As a simple corollary, we get the fact that LG(D∗, gθ)
attains its minimum value if and only if δ(Pr, Pθ) is at its minimum value, which is 0, and Pr = Pθ (Theorems 1-2 of
(Zhao et al., 2016)).
Theorem 4. Let Pr be a the real data distribution over a compact space X . Let gθ : Z → X be a measurable function
(such as any neural network). Then, an optimal discriminator D∗ exists for Pr and Pθ, and

LG(D∗, gθ) =

δ(Pr, Pθ)

m
2

Proof. First, we prove that there exists an optimal discriminator. Let D : X → [0, +∞) be a measurable function, then
D(cid:48)(x) := min(D(x), m) is also a measurable function, and LD(D(cid:48), gθ) ≤ LD(D, gθ). Therefore, a function D∗ : X →
[0, +∞) is optimal if and only if D∗(cid:48) is. Furthermore, it is optimal if and only if LD(D∗, gθ) ≤ LD(D, gθ) for all D :
X → [0, m]. We are then interested to see if there’s an optimal discriminator for the problem min0≤D(x)≤m LD(D, gθ).

Note now that if 0 ≤ D(x) ≤ m we have

Therefore, we know that

LD(D, gθ) = Ex∼Pr [D(x)] + Ez∼p(z)[[m − D(gθ(z))]+]

= Ex∼Pr [D(x)] + Ez∼p(z)[m − D(gθ(z))]
= m + Ex∼Pr [D(x)] − Ez∼p(z)[D(gθ(z))]
= m + Ex∼Pr [D(x)] − Ex∼Pθ [D(x)]

inf
0≤D(x)≤m

LD(D, gθ) = m +

inf
0≤D(x)≤m

Ex∼Pr [D(x)] − Ex∼Pθ [D(x)]

= m +

inf
2 ≤D(x)≤ m

2

= m +

inf
−1≤f (x)≤1

− m
m
2

Ex∼Pr [D(x)] − Ex∼Pθ [D(x)]

Ex∼Pr [f (x)] − Ex∼Pθ [f (x)]

The interesting part is that

inf
−1≤f (x)≤1

Ex∼Pr [f (x)] − Ex∼Pθ [f (x)] = −δ(Pr, Pθ)

(7)

and there is an f ∗ : X → [−1, 1] such that Ex∼Pr [f ∗(x)] − Ex∼Pθ [f ∗(x)] = −δ(Pr, Pθ). This is a long known fact, found
for example in (Villani, 2009), but we prove it later for completeness. In that case, we deﬁne D∗(x) = m
2 . We

2 f ∗(x) + m

then have 0 ≤ D(x) ≤ m and

Wasserstein Generative Adversarial Networks

LD(D∗, gθ) = m + Ex∼Pr [D∗(x)] − Ex∼Pθ [D∗(x)]

= m +

Ex∼Pr [D∗(x)] − Ex∼Pθ [f ∗(x)]

m
2
m
2
inf
0≤D(x)≤m

= m −

δ(Pr, Pθ)

=

LD(D, gθ)

This shows that D∗ is optimal and LD(D∗, gθ) = m − m

2 δ(Pr, Pθ). Furthermore,

LG(D∗, gθ) = Ez∼p(z)[D∗(gθ(z))] − Ex∼Pr [D∗(x)]

= −LD(D∗, gθ) + m
δ(Pr, Pg)

=

m
2

concluding the proof.

For completeness, we now show a proof for equation (7) and the existence of said f ∗ that attains the value of the inﬁmum.
Take µ = Pr − Pθ, which is a signed measure, and (P, Q) its Hahn decomposition. Then, we can deﬁne f ∗ := 1Q − 1P .
By construction, then

Ex∼Pr [f ∗(x)]−Ex∼Pθ [f ∗(x)] =

f ∗ dµ = µ(Q)−µ(P ) = −(µ(P )−µ(Q)) = −(cid:107)µ(cid:107)T V = −(cid:107)Pr−Pθ(cid:107)T V = −δ(Pr, Pθ)

(cid:90)

Furthermore, if f is bounded between -1 and 1, we get

|Ex∼Pr [f (x)] − Ex∼Pθ [f (x)]| = |

f dPr −

f dPθ|

(cid:90)

(cid:90)

(cid:90)

= |

f dµ|

(cid:90)

(cid:90)

≤

|f | d|µ| ≤

1 d|µ|

= |µ|(X ) = (cid:107)µ(cid:107)T V = δ(Pr, Pθ)

Since δ is positive, we can conclude Ex∼Pr [f (x)] − Ex∼Pθ [f (x)] ≥ −δ(Pr, Pθ).

F. Generator’s cost during normal GAN training

Wasserstein Generative Adversarial Networks

Figure 9: Cost of the generator during normal GAN training, for an MLP generator (upper left) and a DCGAN generator (upper
right). Both had a DCGAN discriminator. Both curves have increasing error. Samples get better for the DCGAN but the cost of the
generator increases, pointing towards no signiﬁcant correlation between sample quality and loss. Bottom: M LP with both generator
and discriminator. The curve goes up and down regardless of sample quality. All training curves were passed through the same median
ﬁlter as in Figure 4.

G. Further comments on clipping and batch normalization

In this appendix we provide further informal insights into the behaviour of weight clipping and batch normalization in the
context of GANs and WGANs.

G.1. Weight clipping

One may wander what would happen if one were to use weight clipping in a standard GAN. Disregarding the use of the
cross-entropy vs difference loss, the central difference would be the use of a sigmoid in the end of the discriminator. This
brings into place the use of the Dudley metric:

dF (Pr, Pθ) = sup
f ∈F

Ex∼Pr [f (x)] − Ex∼Pθ [f (x)]

where

F = {f : X → R, f continuous and (cid:107)f (cid:107)∞ + (cid:107)f (cid:107)L ≤ 1}

This is similar to the class of 1-Lipschitz functions, only we restrict how high the values of f can be. This metric is easily
shown to be equivalent to the one with

F (cid:48)(cid:48) = {f : X → R, f continuous and (cid:107)f (cid:107)∞ ≤ 1, (cid:107)f (cid:107)L ≤ K}

or the one with

F (cid:48) = {f : X → R, f continuous and 0 ≤ f ≤ 1, (cid:107)f (cid:107)L ≤ K}

(8)

This last family is essentially the family of functions we would achieve by adding a sigmoid to the critic of the WGAN,
moving us closer to the standard GAN realm. An easy and very interesting result is that dF and dF (cid:48) have the same topology
as the Wasserstein distance (Villani, 2009), which hints that adding clipping to a GAN lands us closer to a WGAN than
the standard GAN (since the cost function between distributions has the exact same regularity as that of a WGAN, and
drastically different from a normal GAN, see Theorems 1, 2).

Wasserstein Generative Adversarial Networks

That being said, while the topology of Wasserstein and the Dudley metric is the same, some things are not. There are many
distances that yield the weak topology, but Wasserstein has a number of differences against the rest. These are perfectly
explained in pages 110 and 111 of (Villani, 2009), but we highlight the main idea here. At the core of it, Wasserstein is
better at representing long distances between samples. The saturation behaviour of the sigmoid, or what happens when K
in (8) is large, shows that if the real samples are far away from the fake ones, f can saturate at 1 in the real and 0 in the fake
constantly, providing no usable gradient. Thus, Dudley and Wasserstein have the same behaviour in close samples, but
Wasserstein avoids saturations and provides gradients even when samples are far away. However, if K (i.e. the clipping)
is small enough (such as the 0.01 we use in practice), the saturating regime will never enter in place, so Dudley and
Wasserstein will behave in the same way.

To conclude, if the clipping is small enough, the network is quite literally a WGAN, and if it’s large it will saturate and fail
to take into account information between samples that are far away (much like a normal GAN when the discriminator is
trained till optimum).

As to the similarities between the difference vs cross-entropy on the loss of the discriminator or critic: if the supports of
Pr and Pθ are essentially disjoint (which was shown to happen in (Arjovsky & Bottou, 2017) in the usual setting of low
dimensional supports), with both cost functions the f will simply be trained to attain the maximum value possible in the
real and the minimum possible in the fake, without surpassing the Lipschitz constraint. Therefore, CE and the difference
might behave more similarly than we expect in the typical ‘learning low dimensional distributions’ setting, provided we
have a strong Lipschitz constraint.

G.2. Batch normalization

It is not clear that batch normalization (BN) is a Lipschitz function with a constant independent of the parameters, since
we are dividing by the standard deviation, which depends on the inputs and the parameters. In this subsection we explain
why BN still behaves in a Lipschitz way, and ﬁts in with the theoretical support of our method.
Let x be the input of a BN layer. If there is a positive c ∈ R for which V (x)1/2 > c (the variance is uniformly bounded
below during training), then this c ∈ R becomes a Lipschitz constant on the BN layer that’s independent of the model
parameters, as we wanted. For V (x) to not be bounded below as training progresses, it has to go arbitrarily close to 0. In
this case, x has to converge (up to taking a subsequence) to it’s mean, so the term x − E[x] in the numerator of batchnorm
x−E[x]
V [x]1/2+(cid:15) comes the constant 0 (which is obviously 1-Lipschitz) due to the (cid:15) in the division.
will go to 0, and therefore
This will also further render the activation x inactive, which the network has no incentive to do.

While this argument is handwavy, one can formalize it and prove very simple bounds that depend only on (cid:15). By increasing
(cid:15) one can enforce a stronger Lipschitz constant, and we could have for example clamped the denominator of the BN to
attain a value large enough. However, in practice in all our runs the variance never surpassed low thresholds, and this
clamping of the BN division was simply never set into effect. Thus, we empirically never saw a break in the Lipschitness
of our BN layers.

H. Sheets of samples

Wasserstein Generative Adversarial Networks

Figure 10: WGAN algorithm: generator and critic are DCGANs.

Figure 11: Standard GAN procedure: generator and discriminator are DCGANs.

Wasserstein Generative Adversarial Networks

Figure 12: WGAN algorithm: generator is a DCGAN without batchnorm and constant ﬁlter size. Critic is a DCGAN.

Figure 13: Standard GAN procedure: generator is a DCGAN without batchnorm and constant ﬁlter size. Discriminator is a DCGAN.

Wasserstein Generative Adversarial Networks

Figure 14: WGAN algorithm: generator is an MLP with 4 hidden layers of 512 units, critic is a DCGAN.

Figure 15: Standard GAN procedure: generator is an MLP with 4 hidden layers of 512 units, discriminator is a DCGAN.

