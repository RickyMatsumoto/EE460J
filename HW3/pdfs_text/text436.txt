Coupling Distributed and Symbolic Execution for Natural Language Queries

Lili Mou 1 Zhengdong Lu 2 Hang Li 3 Zhi Jin 1

Abstract

Query:

Building neural networks to query a knowledge
base (a table) with natural language is an emerg-
ing research topic in deep learning. An execu-
tor for table querying typically requires multi-
ple steps of execution because queries may have
complicated structures. In previous studies, re-
searchers have developed either fully distributed
executors or symbolic executors for table query-
ing. A distributed executor can be trained in
an end-to-end fashion, but is weak in terms of
execution efﬁciency and explicit interpretability.
A symbolic executor is efﬁcient in execution,
but is very difﬁcult to train especially at initial
stages. In this paper, we propose to couple dis-
tributed and symbolic execution for natural lan-
guage queries, where the symbolic executor is
pretrained with the distributed executor’s inter-
mediate execution results in a step-by-step fash-
ion. Experiments show that our approach signiﬁ-
cantly outperforms both distributed and symbolic
executors, exhibiting high accuracy, high learn-
ing efﬁciency, high execution efﬁciency, and high
interpretability.

1. Introduction

Using natural language to query a knowledge base is an
important task in NLP and has wide applications in ques-
tion answering (QA) (Yin et al., 2016a), human-computer
conversation (Wen et al., 2017), etc. Table 1 illustrates an
example of a knowledge base (a table) and a query “How
long is the game with the largest host country size?” To
answer the question, we should ﬁrst ﬁnd a row with the

1Key Laboratory of High Conﬁdence Software Technolo-
gies (Peking University), MoE; Software Institute, Peking Uni-
versity, China 2DeeplyCurious.ai 3Noah’s Ark Lab, Huawei
Technologies. Work done when the ﬁrst author was an
L.M. <double
intern at Huawei.
power.mou@gmail.com>, Z.L. <luz@DeeplyCurious.ai>, H.L.
<HangLi.HL@huawei.com>, Z.J. <zhijin@sei.pku.edu.cn>.

Correspondence to:

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

How long is the game with the largest host country size?

Knowledge base (table):

Year

City

Area

· · ·

Duration

2000
2004
2008
2012
2016 Rio de Janeiro

Sydney
Athens
Beijing
London

200
250
350
300
200

· · ·
· · ·
· · ·
· · ·
· · ·

30
20
25
35
40

· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·

Table 1. An example of a natural language query and a knowledge
base (table).

largest value in the column Area, and then select the value
of the chosen row with the column being Duration.

A typical approach to table querying is to convert a natural
language sentence to an “executable” logic form, known
as semantic parsing. Traditionally, building a semantic
parser requires extensive human engineering of explicit
features (Berant et al., 2013; Pasupat & Liang, 2015).

With the fast development of deep learning, an increasingly
number of studies use neural networks for semantic pars-
ing. Dong & Lapata (2016) and Xiao et al. (2016) apply
sequence-to-sequence (seq2seq) neural models to gen-
erate a logic form conditioned on an input sentence, but
the training requires groundtruth logic forms, which are
costly to obtain and speciﬁc to a certain dataset. In real-
istic settings, we only assume groundtruth denotations1 are
available, and that we do not know execution sequences or
intermediate execution results. Liang et al. (2017) train a
seq2seq network by REINFORCE policy gradient. But it
is known that the REINFORCE algorithm is sensitive to the
initial policy; also, it could be very difﬁcult to get started at
early stages.

Yin et al. (2016b) propose a fully distributed neural en-
quirer, comprising several neuralized execution layers of
ﬁeld attention, row annotation, etc. The model can be
trained in an end-to-end fashion because all components
are differentiable. However, it lacks explicit interpreta-
tion and is not efﬁcient in execution due to intensive ma-
trix/vector operation during neural processing.

Neelakantan et al. (2016) propose a neural programmer

1A denotation refers to an execution result.

Coupling Distributed and Symbolic Execution for Natural Language Queries

by deﬁning a set of symbolic operators (e.g., argmax,
greater than); at each step, all possible execution re-
sults are fused by a softmax layer, which predicts the prob-
ability of each operator at the current step. The step-by-step
fusion is accomplished by weighted average and the model
is trained with mean square error. Hence, such approaches
work with numeric tables, but may not be suited for other
operations like string matching.
It also suffers from the
problem of “exponential numbers of combinatorial states,”
as the model explores the entire space at a time by step-by-
step weighted average.

In this paper, we propose to couple distributed and sym-
bolic execution for natural language queries. By “sym-
bolic execution,” we mean that we deﬁne symbolic op-
erators and keep discrete intermediate execution results.2
Our intuition rises from the observation that a fully dis-
tributed/neuralized executor also exhibits some (imperfect)
symbolic interpretation. For example, the ﬁeld attention
gadget in Yin et al. (2016b) generally aligns with column
selection. We therefore use the distributed model’s in-
termediate execution results as supervision signals to pre-
train a symbolic executor. Guided by such imperfect step-
by-step supervision, the symbolic executor learns a fairly
meaningful initial policy, which largely alleviates the cold
start problem of REINFORCE. Moreover, the improved
policy can be fed back to the distributed executor to im-
prove the neural network’s performance.

We evaluated the proposed approach on the QA dataset
in Yin et al. (2016b). Our experimental results show that
the REINFORCE algorithm alone takes long to get started.
Even if it does, it is stuck in poor local optima. Once
pretrained by imperfect supervision signals, the symbolic
executor can recover most execution sequences, and also
achieves the highest denotation accuracy. It should be em-
phasized that, in our experiment, neither the distributed ex-
ecutor nor the symbolic executor is aware of groundtruth
execution sequences, and that the entire model is trained
with weak supervision of denotations only.

To the best of our knowledge, we are the ﬁrst to couple
distributed and symbolic execution for semantic parsing.
Our study also sheds light on neural sequence prediction in
general.

2. Approach

In Subsection 2.1, we introduce the fully distributed neu-
ral executor, which is mostly based on Yin et al. (2016b).
For symbolic execution, we design a set of operators that

2The sense of symbolic execution here should not be confused
with “symbolic execution of a program” (see https://en.
wikipedia.org/wiki/Symbolic_execution for ex-
ample).

Figure 1. An overview of the coupled distributed and symbolic
executors.

are complete to the task at hand; at each execution step, a
neural network predicts a particular operator and possibly
arguments (Subsection 2.2).

Subsection 2.3 provides a uniﬁed view of distributed and
symbolic execution (Figure 1). We explain how the sym-
bolic executor is pretrained by the distributed one’s inter-
mediate execution results, and then further trained with the
REINFORCE algorithm.

2.1. Distributed Executor

The distributed executor makes full use of neural networks
for table querying. By “distributed,” we mean that all se-
mantic units (including words in the query, entries in the
table, and execution results) are represented as distributed,
real-valued vectors and processed by neural networks. One
of the most notable studies of distributed semantics is word
embeddings, which map discrete words to vectors as mean-
ing representations (Mikolov et al., 2013).

The distributed executor consists of the following main
components.

• Query encoder. Words are mapped to word em-
beddings and a bi-directional recurrent neural net-
work (RNN) aggregates information over the sen-
tence. RNNs’ last states in both directions are con-
catenated as the query representation (detonated as q).
• Table encoder. All table cells are also represented as
embeddings. For a cell c (e.g., Beijing in Table 1)
with its column/ﬁeld name being f (e.g., City), the
cell vector is the concatenation of the embeddings of
c and f , further processed by a feed-forward neural
network (also known as multi-layer perceptron). We
denote the representation of a cell as c.

• Executor. As shown in Figure 1a, the neural network
comprises several steps of execution. In each execu-
tion step, the neural network selects a column by soft-

NeuralExecutorIntermediateResultOperator1Operator2OperatornOperator/ArgumentPredictorOperator/ArgumentPredictorOperator/ArgumentPredictor. . .DifferentiableNon-differentiableImperfectstep-by-stepsupervisionIntermediateResultNeuralExecutorIntermediateResultOperator1Operator2OperatornIntermediateResultNeuralExecutorOutputOperator1Operator2Operatorn. . .Output. . .(a)(b)QueryCoupling Distributed and Symbolic Execution for Natural Language Queries

where [i] indexes a row of the selected column.

The current row annotation is computed by another MLP,
based on the query q, previous execution results r(t−1)
,
previous global information g(t−1), as well as the selected
row in the current step c(t)
select[i] (the selection is in a soft
manner), i.e.,

i

r(t)
i = MLP

q, g(t−1), r(t−1)

i

, c(t)

select[i]

(cid:16)(cid:104)

(cid:105)(cid:17)

(5)

As said, the last execution layer applies a softmax classiﬁer
over all cells to select an answer. Similar to Equation 3,
the weights of softmax are not associated with positions,
but the cell embeddings. In other words, the probability of
choosing the i-th row, j-th column is
(cid:110)

(cid:17)(cid:111)

(cid:16)

exp

MLP
(cid:110)

(cid:80)

(cid:80)

i(cid:48)

j(cid:48) exp

MLP

q, g(t−1), r(t−1)

(cid:16)

i
q, g(t−1), r(t−1)

, cij

i(cid:48)

(cid:17)(cid:111)

, ci(cid:48)j(cid:48)

pij =

(6)

In this way, the neural executor is invariant with respect to
the order of rows and columns. While order-sensitive ar-
chitectures (e.g., convolutional/recurrent neural networks)
might also model a table by implicitly ignoring such or-
der information, the current treatment is more reasonable,
which also better aligns with symbolic interpretation.

2.2. Symbolic Executor

The methodology of designing a symbolic executor is to
deﬁne a set of primitive operators for the task, and then to
use a machine learning model to predict the operator se-
quence and its arguments.

Our symbolic executor is different from the neural pro-
grammer (Neelakantan et al., 2016) in that we keep
discrete/symbolic operators as well as execution results,
whereas Neelakantan et al. (2016) fuse execution results
by weighted average.

2.2.1. PRIMITIVE OPERATORS

We design six operators for symbolic execution, which are
complete as they cover all types of queries in our scenario.
Similar to the distributed executor, the result of one-step
symbolic execution is some information for a row; here,
we use a 0-1 boolean scalar, indicating whether a row is
selected or not after a particular step of execution. Then
a symbolic execution step takes previous results as input,
with a column/ﬁeld being the argument. The green boxes
in Figure 1b illustrate the process and Table 2 summarizes
our primitive operator set.

In Table 1, for example,
the ﬁrst step of execution is
argmax over the column Area, with previous (initial)
row selection being all ones. This step yields a single

Figure 2. A single step of distributed execution.

max attention, and annotates each row with a vector,
i.e., an embedding (Figure 2). The row vector can be
intuitively thought of as row selection in query execu-
tion, but is represented by distributed semantics here.3
In the last step of execution, a softmax classiﬁer is ap-
plied to the entire table to select a cell as the answer.
Details are further explained as below.

Let r(t−1)
be the previous step’s row annotation result,
i
where the subscript i indexes a particular row. We sum-
marize global execution information (denoted as g(t−1)) by
max-pooling the row annotation r(t−1), i.e.,
(cid:111)

(cid:110)

g(t−1) = MaxPooli

r(t−1)
i

(1)

In the current execution step, we ﬁrst compute a distribu-
tion p(t)
f over all ﬁelds as “soft” ﬁeld selction. The compu-
tation is based on the query q, the previous global informa-
tion g(t−1), and the ﬁeld name embeddings f , i.e.,
(cid:16)
MLP (cid:0)[q; fj; g(t−1)](cid:1)(cid:17)
exp (cid:8)MLP (cid:0)[q; fj; g(t−1)](cid:1)(cid:9)
j(cid:48) exp (cid:8)MLP (cid:0)[q; fj(cid:48); g(t−1)](cid:1)(cid:9)

= softmax

p(t)
fj

(2)

(3)

(cid:80)

=

where [·; ·; · · · ] denotes vector concatenation; MLP refers
to a multi-layer perceptron.

Here, the weights of softmax are ﬁeld embeddings (rather
than parameters indexed by positions). In this way, there is
no difference if one shufﬂes table columns. Besides, for a
same ﬁeld name, its embedding is shared among all training
samples containing this ﬁeld (but different tables may have
different ﬁelds).

We represent the selected cell in each row as the sum of
all cells in that row, weighted by soft ﬁeld selection p(t−1)
.
Formally, for the i-th row, we have

f

c(t)
select[i] =

p(t)
fj

cij

(cid:88)

j

(4)

3In a pilot experiment, we tried a gating mechanism to indi-
cate the results of row selection in hopes of aligning symbolic
table execution. However, our preliminary experiments show that
such gates do not exhibit much interpretation, but results in per-
formance degradation. The distributed semantics provide more
information than a 1-bit gate for a row.

Soft column selectionDistributed row annotationSelected columnCoupling Distributed and Symbolic Execution for Natural Language Queries

Explanation

Choose a row whose value of a particular column is mentioned in the query
Choose the row from previously selected candidate rows with the minimum value in a particular column
Choose the row from previously selected candidate rows with the maximum value in a particular column

Operator
select row
argmin
argmax
greater than Choose rows whose value in a particular column is greater than a previously selected row
less than
select value Choose the value of a particular column and of the previously selected row
EOE

Choose rows whose value in a particular column is less than a previously selected row

Terminate, indicating the end of execution

Table 2. Primitive operators for symbolic execution.

row (cid:104)2008, Beijing, · · · , 350, · · · , 25(cid:105). The second execu-
tion operator is select value, with an argument col-
umn=Duration, yielding the result 25. Then the execu-
tor terminates (EOE).

Stacked with multiple steps of primitive operators, the ex-
ecutor can answer fairly complicated questions like “How
long is the last game which has smaller country size than
the game whose host country GDP is 250?” In this exam-
ple, the execution sequence is

1. select row: select the row where the column is

GDP and the value is mentioned in the query.

2. less than: select rows whose country size is less

than that of the previously selected row.

3. argmax: select the row whose year is the largest

among previously selected rows.

4. select value: choose the value of the previously

selected row with the column being Duration.

Likewise, another Jordan-type RNN selects a ﬁeld. The
only difference lies in the weight of the output softmax,
i.e., wi in Equation 8 is substituted with the embedding of
a ﬁeld/column name f , given by
ﬁeld h(t−1)
h(t)
ﬁeld = sigmoid(W (rec)
ﬁeld )
(cid:111)
j h(t)
p(t)
f (cid:62)
= softmax
fj

(10)

(9)

ﬁeld

(cid:110)

Training a symbolic executor without step-by-step super-
vision signals is non-trivial. A typical training method is
reinforcement learning in a trial-and-error fashion. How-
ever, for a random initial policy, the probability of recov-
ering an accurate execution sequence is extremely low.
Given a 10 × 10 table, for example, the probability is
1/(64 · 104) ≈ 7.7 × 10−8; the probability of obtaining an
accurate denotation is 1%, which is also very low. There-
fore, symbolic executors are not efﬁcient in learning.

Then the execution terminates. In our scenario, the execu-
tion is limited to four steps (EOE excluded) as such queries
are already very complicated in terms of logical depth.

2.3. A Uniﬁed View

We now have two worlds of execution:

2.2.2. OPERATOR AND ARGUMENT PREDICTORS

We also leverage neural models, in particular RNNs, to pre-
dict the operator and its argument (a selected ﬁeld/column).
Let h(t−1) be the previous state’s hidden vectors. The cur-
rent hidden state is

h(t)
op = sigmoid(W (rec)

op h(t−1)

op

)

(7)

op

where W (rec)
is weight parameters. (A bias term is omitted
in the equation for simplicity.) The initial hidden state is
the query embedding, i.e., h(0)

op = q.

The predicted probability of an operator i is given by

p(t)
opi

= softmax

w(out)
opi

(cid:62)h(t)
op

(cid:110)

(cid:111)

(8)

• The distributed executor is end-to-end learnable, but
it is of low execution efﬁciency because of intensive
matrix/vector multiplication during neural informa-
tion processing. The fully neuralized execution also
lacks explicit interpretation.

• The symbolic executor has high execution efﬁciency
and explicit interpretation. However, it cannot be
trained in an end-to-end manner, and suffers from the
cold start problem of reinforcement learning.

We propose to combine the two worlds by using the dis-
tributed executor’s intermediate execution results to pre-
train the symbolic executor for an initial policy; we then
use the REINFORCE algorithm to improve the policy. The
well-trained symbolic executor’s intermediate results can
also be fed back to the distributed executor to improve per-
formance.

The operator with the largest predicted probability is se-
lected for execution.

2.3.1. DISTRIBUTED → SYMBOLIC

Our RNN here does not have input, because the execution
sequence is not dependent on the result of the previous ex-
ecution step. Such architecture is known as a Jordan-type
RNN (Jordan, 1997; Mesnil et al., 2013).

We observe that the ﬁeld attention in Equation 3 generally
aligns with column selection in Equation 10. We therefore
pretrain the column selector in the symbolic executor with
labels predicted by a fully neuralized/distributed executor.

Coupling Distributed and Symbolic Execution for Natural Language Queries

Such pretraining can obtain up to 70% accurate ﬁeld selec-
tion and largely reduce the search space during reinforce-
ment learning.

Formally, the operator predictor (Equation 8) and argu-
ment predictor (Equation 10) in each execution step are
the actions (denoted as A) in reinforcement learning ter-
If we would like to pretrain m actions
minologies.
a1, a2, · · · , am ∈ A, the cost function of a particular data
sample is

J = −

ˆt(i)
j

log p(i)
j

(11)

m
(cid:88)

n(i)
label(cid:88)

i=1

j=1

where n(j)
label is the number of labels (possible candidates)
for the j-th action. p(i) ∈ Rn(i)
label is the predicted prob-
ability by the operator/argument predictors in Figure 1b.
ˆt(i) ∈ Rn(i)
label is the induced action from the fully distributed
model in Figure 1a. In our scenario, we only pretrain col-
umn predictors.

After obtaining a meaningful, albeit imperfect, initial pol-
icy, we apply REINFORCE (Williams, 1992) to improve
the policy.

We deﬁne a binary reward R indicating whether the ﬁnal
result of symbolic execution matches the groundtruth de-
notation. The loss function of a policy is the negative ex-
pected reward where actions are sampled from the current
predicted probabilities

J = −Ea1,a2,··· ,an∼θ[R(a1, a2, · · · , an)]

(12)

The partial derivative for a particular sampled action is

∂J
∂oi

= ˜R · (pi − 1ai)

(13)

where pi is the predicted probability of all possible actions
at the time step i, 1ai is a onehot representation of a sam-
pled action ai, and oi is the input (also known as logit) of
softmax. ˜R is the adjusted reward, which will be described
shortly.

To help the training of REINFORCE, we have two tricks:

• We balance exploration and exploitation with a small
probability (cid:15).
In other words, we sample an action
from the predicted action distribution with probability
1 − (cid:15) and from a uniform distribution over all possible
actions with probability (cid:15). The small fraction of uni-
form sampling helps the model to escape from poor
local optima, as it continues to explore the entire ac-
tion space during training.

• We adjust the reward by subtracting the mean reward,
averaged over sampled actions for a certain data point.
This is a common practice for REINFORCE (Ranzato
et al., 2016). We also truncate negative rewards as

zero to prevent gradient from being messed up by in-
correct execution. This follows the idea of “reward-
inaction,” where unsuccessful trials are ignored (Sec-
tion 2.4 in Sutton & Barto (1998)). The adjusted re-
ward is denoted as ˜R in Equation 13.

Notice that these tricks are applied to both coupled training
and baselines for fairness.

2.3.2. DISTRIBUTED → SYMBOLIC → DISTRIBUTED

After policy improvement by REINFORCE, we could fur-
ther feed back the symbolic executor’s intermediate results
to the distributed one, akin to the step-by-step supervision
setting in Yin et al. (2016b). The loss is a combination of
denotation cross entropy loss Jdenotation and ﬁeld attention
cross entropy loss Jﬁelds. (Jﬁelds is similar to Equation 11,
and details are not repeated). The overall training objective
is J = Jdenotation + λJﬁelds, where λ is a hyperparameter
balancing the two factors.

As will be seen in Section 3.3.5, feeding back intermedi-
ate results improves the distributed executor’s performance.
This shows the distributed and symbolic worlds can indeed
be coupled well.

3. Experiments

3.1. Dataset

We evaluated our approach on a QA dataset
in Yin
et al. (2016b). The dataset comprises 25k different ta-
bles and queries for training; validation and test sets con-
tain 10k samples, respectively, and do not overlap with the
training data. Each table is of size 10 × 10, but differ-
ent samples have different tables; the queries can be di-
vided into four types: SelectWhere, Superlative,
WhereSuperlative, and NestQuery, requiring 2–4
execution steps (EOE excluded).

We have both groundtruth denotation and execution actions
(including operators and ﬁelds), as the dataset is synthe-
sized by complicated rules and templates for research pur-
poses. However, only denotations are used as labels during
training, which is a realistic setting; execution sequences
are only used during testing. For the sake of simplicity, we
presume the number of execution steps is known a priori
during training (but not during testing). Although we have
such (little) knowledge of execution, it is not a limitation of
our approach and out of our current focus. One can easily
design a dummy operator to ﬁll an unnecessary step or one
can also train a discriminative sentence model to predict the
number of execution steps if a small number of labels are
available.

We chose the synthetic datasets because it is magnitudes
larger than existing resources (e.g., WEBQUESTIONS).

Coupling Distributed and Symbolic Execution for Natural Language Queries

Query type
SelectWhere
Superlative
WhereSuperlative
NestQuery
Overall

SEMPRE
SEMPRE
93.8
97.8
34.8
34.4
65.2

96.2
98.9
80.4
60.5
84.0

† Distributed†

Denotation

Symbolic Coupled Distributed
99.6
100.0
99.9
100.0
99.8

99.2
100.0
51.9
52.5
75.8

–
–
–
–
–

Execution

Symbolic Coupled

99.1
100.0
0.0
0.0
49.5

99.6
100.0
91.0
100.0
97.6

Table 3. Accuracies (in percentage) of the SEMPRE tookit, the distributed neural enquirer, the symbolic executor, and our coupled
approach. †Results reported in Yin et al. (2016b).

The process of data synthesizing also provides intermedi-
ate execution results for in-depth analysis. Like BABI for
machine comprehension, our dataset and setting are a pre-
requisite for general semantic parsing. The data are avail-
able at out project website4; the code for data generation
can also be downloaded to facilitate further development
of the dataset.

3.2. Settings

The symbolic executor’s settings were generally derived
from Yin et al. (2016b) so that we can have a fair com-
parison.5 The dimensions of all layers were in the range of
20–50; the learning algorithm was AdaDelta with default
hyperparameters.

For the pretraining of the symbolic executor, we applied
maximum likelihood estimation for 40 epochs to column
selection with labels predicted by the distributed executor.
We then used the REINFORCE algorithm to improve the
policy, where we generated 10 action samples for each data
point with the exploration probability (cid:15) being 0.1.

When feeding back to the distributed model, we chose λ
from {0.1, 0.5, 1} by validation to balance denotation error
and ﬁeld attention error. 0.5 outperforms the rest.

Besides neural networks, we also included the SEMPRE
system as a baseline for comparison. The results are re-
ported in Yin et al. (2016b), where a SEMPRE version that
is specially optimized for table query is adopted (Pasupat
& Liang, 2015). Thus it is suited in our scenario.

3.3. Results

3.3.1. OVERALL PERFORMANCE

Table 3 presents the experimental results of our coupled ap-
proach as well as baselines. Because reinforcement learn-
ing is much more noisy to train, we report the test accuracy

4https://sites.google.com/site/

coupleneuralsymbolic/

5One exception is the query RNN’s hidden states. Yin
et al. (2016b) used 300d BiRNN, but we found it more likely
to overﬁt in the symbolic setting, and hence we used 50d. This
results in slower training, more rugged error surfaces, but higher
peak performance.

corresponding to highest validation accuracy among three
different random initializations (called trajectories). This
is also known as a restart strategy for non-convex optimiza-
tion.

As we see, both distributed and symbolic executors out-
perform the SEMPRE system, showing that neural net-
works can capture query information more effectively than
human-engineered features. Further, the coupled approach
also signiﬁcantly outperforms both of them.

If trained solely by REINFORCE, the symbolic execu-
tor can recover the execution sequences for simple ques-
tions (SelectWhere and Superlative). However,
for more complicated queries, it only learns last one or two
steps of execution and has trouble in recovering early steps,
even with the tricks in Section 2.3.1. This results in low
execution accuracy but near 50% denotation accuracy be-
cause, in our scenario, we still have half chance to obtain
an accurate denotation even if the nested (early) execution
is wrong—the ultimate result is either in the candidate list
or not, given a wrong where-clause execution.

By contrast, the coupled training largely improves the sym-
bolic executor’s performance in terms of all query types.

3.3.2. INTERPRETABILITY

The accuracy of execution is crucial to the interpretability
of a model. We say an execution is accurate, if all actions
(operators and arguments) are correct. As shown above, an
accurate denotation does not necessarily imply an accurate
execution.

We ﬁnd that coupled training recovers most correct exe-
cution sequences, that the symbolic executor alone cannot
recover complicated cases, and that a fully distributed en-
quirer does not have explicit interpretations of execution.

The results demonstrate high interpretability of our ap-
proach, which is helpful for human understanding of ex-
ecution processes.

3.3.3. LEARNING EFFICIENCY

We plot in Figure 3 the validation learning curves of the
symbolic executor, trained by either reinforcement learning

Coupling Distributed and Symbolic Execution for Natural Language Queries

Fully

Our approach

Distributed Op/Arg Pred. Symbolic Exe.† Total
2.65
0.44

13.86
1.05

2.65
0.44

0.002

CPU
GPU

Table 4. Execution efﬁciency. We present the running time (in
seconds) of the test set, containing 10k samples. †The symbolic
execution is assessed in C++ implementation. Others are imple-
mented in Theano, including the fully distributed model as well
as the operator and argument predictors.

Training Method
End-to-end (w/ denotation labels)†
Step-by-step (w/ execution labels)†
Feeding back

Accuracy (%)

84.0
96.4
96.5

Table 5. The accuracy of a fully distributed model, trained by dif-
ferent methods. In the last row, we ﬁrst train a distributed executor
and feed its intermediate execution results to the symbolic one;
then the symbolic executor’s intermediate results are fed back to
the distributed one. †Reported in Yin et al. (2016b).

distributed real-valued vector, resulting in intensive matrix-
vector operations. The symbolic executor only needs a neu-
ral network to predict actions (operators and arguments),
and thus is more lightweight. Further, we observe the exe-
cution itself is blazingly fast, implying that, compared with
distributed models, our approach could achieve even more
efﬁciency boost with a larger table or more complicated
operation.

3.3.5. FEEDING BACK

We now feed back the well-trained symbolic executor’s in-
termediate results to the fully distributed one. As our well-
trained symbolic executor has achieved high execution ac-
curacy, this setting is analogous to strong supervision with
step-by-step groundtruth signals, and thus it also achieves
similar performance,6 shown in Table 5.

We showcase the distributed executor’s ﬁeld attention7 in
Figure 4.
If trained in an end-to-end fashion, the neural
network exhibits interpretation in the last three steps of this
example, but in the early step, the ﬁeld attention is incorrect
(also more uncertain as it scatters a broader range). Af-
ter feeding back the symbolic executor’s intermediate re-
sults as step-by-step supervision, the distributed executor
exhibits near-perfect ﬁeld attention.

This experiment further conﬁrms that the distributed and

6We even have 0.1% performance boost compared with the
step-by-step setting, but we think it should be better explained as
variance of execution.

7The last neural executor is a softmax layer over all cells. We

marginalize over rows to obtain the ﬁeld probability.

Figure 3. Validation learning curves.
(a) Symbolic executor
trained by REINFORCE (RL) only. (b) Symbolic executor with
40-epoch pretraining using a distributed executor in a supervised
learning (SL) fashion. Both settings have three trajectories with
different random initializations. Dotted lines: Denotation accu-
racy. Solid lines: Execution accuracy.

alone or our coupled approach.

Figure 3a shows that the symbolic executor is hard to train
by REINFORCE alone: one trajectory obtains near-zero
execution accuracy after 2000 epochs; the other two take
200 epochs to get started to escape from initial plateaus.
Even if they achieve ∼50% execution accuracy (for simple
query types) and ∼75% denotation accuracy, they are stuck
in the poor local optima.

Figure 3b presents the learning curves of the symbolic ex-
ecutor pretrained with intermediate ﬁeld attention of the
distributed executor. Since the operation predictors are still
hanging after pretraining, the denotation accuracy is near
0 before reinforcement learning. However, after only a
few epochs of REINFORCE training, the performance in-
creases sharply and achieves high accuracy gradually.

Notice that we have 40 epochs of (imperfectly) supervised
pretraining. However, its time is negligible compared with
reinforcement learning because in our experiments REIN-
FORCE generates 10 samples and hence is theoretically 10
times slower. The results show that our coupled approach
has much higher learning efﬁciency than a pure symbolic
executor.

3.3.4. EXECUTION EFFICIENCY

Table 4 compares the execution efﬁciency of a distributed
executor and our coupled approach. All neural networks
are implemented in Theano with a TITAN Black GPU and
Xeon e7-4820v2 (8-core) CPU; symbolic execution is as-
sessed in C++ implementation. The comparison makes
sense because the Theano platform is not specialized in
symbolic execution, and fortunately, execution results do
not affect actions in our experiment. Hence they can be
easily disentangled.

As shown in the table, the execution efﬁciency of our ap-
proach is 2–5 times higher than the distributed executor,
depending on the implementation. The distributed execu-
tor is when predicting because it maps every token to a

Coupling Distributed and Symbolic Execution for Natural Language Queries

Query: How many people watched the earliest game whose host country GDP is larger than the game in Cape Town?

Figure 4. Distributed executor’s intermediate results of ﬁeld attention. Top: Trained in an end-to-end fashion (a–d). Bottom: One-round
co-training of distributed and symbolic executors (e–h). The red plot indicates incorrect ﬁeld attention.

symbolic worlds can indeed be coupled well. In more com-
plicated applications, there could also be possibilities in it-
eratively training one model by leveraging the other in a
co-training fashion.

4. Related Work and Discussions

Neural execution has recently aroused much interest in
the deep learning community. Besides SQL-like execu-
tion as has been extensively discussed in previous sec-
tions, neural Turing machines (Graves et al., 2014) and
neural programmer-interpreters (Reed & De Freitas, 2016)
are aimed at more general “programs.” The former is a
“distributed analog” to Turing machines with soft operators
(e.g., read, write, and address); its semantics, how-
ever, cannot be grounded to actual operations. The latter
learns to generate an execution trace in a fully supervised,
step-by-step manner.

Another related topic is incorporating neural networks with
external (hard) mechanisms. Hu et al. (2016) propose to
better train a neural network by leveraging the classiﬁca-
tion distribution induced from a rule-based system. Lei
et al. (2016) propose to induce a sparse code by REIN-
FORCE to make neural networks focus on relevant infor-
mation. In machine translation, Mi et al. (2016) use align-
ment heuristics to train the attention signal of neural net-
works in a supervised manner. In these studies, researchers
typically leverage external hard mechanisms to improve
neural networks’ performance.

The uniqueness of our work is to train a fully neural-
ized/distributed model ﬁrst, which takes advantage of its
differentiability, and then to guide a symbolic model to
achieve a meaningful initial policy. Further trained by rein-
forcement learning, the symbolic model’s knowledge can

improve neural networks’ performance by feeding back
step-by-step supervision. Our work sheds light on neu-
ral sequence prediction in general, for example, explor-
ing word alignment (Mi et al., 2016) or chunking informa-
tion (Zhou et al., 2017) in machine translation by coupling
neural and external mechanisms.

5. Conclusion and Future Work

In this paper, we have proposed a coupled view of
distributed and symbolic execution for natural language
queries. By pretraining with intermediate execution re-
sults of a distributed executor, we manage to accelerate
the symbolic model’s REINFORCE training to a large ex-
tent. The well-trained symbolic executor could also guide
a distributed executor to achieve better performance. Our
proposed approach takes advantage of both distributed and
symbolic worlds, achieving high interpretability, high exe-
cution efﬁciency, high learning efﬁciency, as well as high
accuracy.

As a pilot study, our paper raises several key open ques-
tions: When do neural networks exhibit symbolic interpre-
tations? How can we better transfer knowledge between
distributed and symbolic worlds?

In future work, we would like to design interpretable op-
erators in the distributed model to better couple the two
worlds and to further ease the training with REINFORCE.
We would also like to explore different ways of transferring
knowledge, e.g., distilling knowledge from the action dis-
tributions (rather than using the max a posteriori action),
or sampling actions by following the distributed model’s
predicted distribution during symbolic one’s Monte Carlo
policy gradient training (REINFORCE).

Coupling Distributed and Symbolic Execution for Natural Language Queries

Pasupat, Panupong and Liang, Percy. Compositional se-
In ACL-

mantic parsing on semi-structured tables.
IJCNLP, pp. 1470–1480, 2015.

Ranzato, MarcAurelio, Chopra, Sumit, Auli, Michael, and
Zaremba, Wojciech. Sequence level training with recur-
rent neural networks. In ICLR, 2016.

Reed, Scott and De Freitas, Nando. Neural programmer-

interpreters. In ICLR, 2016.

Sutton, Richard S and Barto, Andrew G. Reinforcement
Learning: An Introduction. MIT Press Cambridge, 1998.

Wen, Tsung-Hsien, Vandyke, David, Mrkˇsi´c, Nikola, Ga-
sic, Milica, Rojas Barahona, Lina M., Su, Pei-Hao,
Ultes, Stefan, and Young, Steve. A network-based end-
to-end trainable task-oriented dialogue system. In EACL,
pp. 438–449, 2017.

Williams, Ronald J. Simple statistical gradient-following
learning.

algorithms for connectionist reinforcement
Machine Learning, 8(3):229–256, 1992.

Xiao, Chunyang, Dymetman, Marc, and Gardent, Claire.
Sequence-based structured prediction for semantic pars-
ing. In ACL, pp. 1341–1350, 2016.

Yin, Jun, Jiang, Xin, Lu, Zhengdong, Shang, Lifeng, Li,
Hang, and Li, Xiaoming. Neural generative question an-
swering. In IJCAI, pp. 2972–2978, 2016a.

Yin, Pengcheng, Lu, Zhengdong, Li, Hang, and Kao, Ben.
Neural enquirer: Learning to query tables with natural
language. In IJCAI, pp. 2308–2314, 2016b.

Zhou, Hao, Tu, Zhaopeng, Huang, Shujian, Liu, Xiaohua,
Li, Hang, and Chen, Jiajun. Chunk-based bi-scale de-
coder for neural machine translation. In ACL, 2017.

Acknowledgments

We thank Pengcheng Yin and Jiatao Gu for helpful dis-
cussions; we also thank the reviewers for insightful com-
ments. This research is partially supported by the National
Basic Research Program of China (the 973 Program) under
Grant Nos. 2014CB340301 and 2015CB352201, and the
National Natural Science Foundation of China under Grant
Nos. 614201091, 61232015 and 61620106007.

References

Berant, Jonathan, Chou, Andrew, Frostig, Roy, and Liang,
Percy. Semantic parsing on Freebase from question-
answer pairs. In EMNLP, pp. 1533–1544, 2013.

Dong, Li and Lapata, Mirella. Language to logical form

with neural attention. In ACL, pp. 33–43, 2016.

Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural
Turing machines. arXiv preprint arXiv:1410.5401, 2014.

Hu, Zhiting, Ma, Xuezhe, Liu, Zhengzhong, Hovy, Eduard,
and Xing, Eric. Harnessing deep neural networks with
logic rules. In ACL, pp. 2410–2420, 2016.

Jordan, Michael I. Serial order: A parallel distributed pro-
cessing approach. Advances in Psychology, 121:471–
495, 1997.

Lei, Tao, Barzilay, Regina, and Jaakkola, Tommi. Ratio-
In EMNLP, pp. 107–117,

nalizing neural predictions.
2016.

Liang, Chen, Berant, Jonathan, Le, Quoc, Forbus, Ken-
neth D, and Lao, Ni. Neural symbolic machines: Learn-
ing semantic parsers on Freebase with weak supervision.
In ACL (to appear), 2017.

Mesnil, Gr´egoire, He, Xiaodong, Deng, Li, and Bengio,
Yoshua. Investigation of recurrent-neural-network archi-
tectures and learning methods for spoken language un-
derstanding. In INTERSPEECH, pp. 3771–3775, 2013.

Mi, Haitao, Sankaran, Baskaran, Wang, Zhiguo, and Itty-
cheriah, Abe. Coverage embedding models for neural
machine translation. In EMNLP, pp. 955–960, 2016.

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed representations of
words and phrases and their compositionality. In NIPS,
pp. 3111–3119, 2013.

Neelakantan, Arvind, Le, Quoc V, and Sutskever, Ilya.
Neural programmer: Inducing latent programs with gra-
dient descent. In ICLR, 2016.

