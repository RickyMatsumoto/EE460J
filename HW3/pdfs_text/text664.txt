Supplementary Material:
Adaptive Consensus ADMM for Distributed Optimization

Zheng Xu 1 Gavin Taylor 2 Hao Li 1 M´ario A. T. Figueiredo 3 Xiaoming Yuan 4 Tom Goldstein 1

This is the supplemental material for Adaptive Consen-
sus ADMM (ACADMM) (Xu et al., 2017c). We provide
details of proofs and experimental settings, in addition to
more results. Our proof generalizes the variational inequal-
ity approach in (He et al., 2000; He & Yuan, 2012; 2015;
Xu et al., 2017b).

1. Proof of lemmas

1.1. Proof of Lemma 1 (17)

Proof. By using the updated dual variable λk+1 in (10), VI
(15) can be rewritten as

∀v, g(v) − g(vk+1) − (Bv − Bvk+1)T λk+1 ≥ 0.

(S1)

Similarly, in the previous iteration,

∀v, g(v) − g(vk) − (Bv − Bvk)T λk ≥ 0.

(S2)

Let y = y∗, z = z∗ in VI (S4), and y = yk+1, z = zk+1 in
VI (13), and sum the two equalities together to get

(∆z∗

k+1)T Ω(∆z+
(∆z∗

k , T k) ≥

k+1)T (F (z∗) − F (zk+1)).

(S5)

Since F (z) is monotone,
negative. Now, substitute Ω(∆z+

the right hand side is non-
k , T k) into (S5) to get
k+1)T T k(B∆v+
k )
k+1)T (T k)−1∆λ+
+ (∆λ∗

k ≥ 0.

(S6)

− (A∆u∗

If we use the feasibility constraint of optimal solution
(Au∗ + Bv∗ = b) and the dual update formula (10), we
have

T kA∆u∗

k+1 = ∆λ+
Substitute this into (S6) yields

k − T kB∆v∗

k+1.

(S7)

(B∆v∗

k + (∆λ∗

k+1)T T kB∆v+

k+1)T (T k)−1∆λ+
k
k )T ∆λ+
≥ (B∆v+
The proof (18) is concluded by applying (17) to (S8).

k

(S8)

Let v = vk in (S1) and v = vk+1 in (S2), and sum the two
inequalities together. We conclude

1.3. Proof of Lemma 1 (19)

(Bvk+1 − Bvk)T (λk+1 − λk) ≥ 0.

(S3)

Proof.

(cid:107)∆z∗

k(cid:107)2

1.2. Proof of Lemma 1 (18)

Proof. VI (16) can be rewritten as

φ(y) − φ(yk+1)+

(z − zk+1)T (cid:0)F (zk+1) + Ω(∆z+

k , T k)(cid:1) ≥ 0,

(S4)

where Ω(∆z+

k , T k) = (−AT T kB∆v+

k ; 0; (T k)−1∆λ+

k ).

1University of Maryland, College Park 2United States Naval
Academy, Annapolis, 3Universidade de Lisboa, Portugal 4Hong
Kong Baptist University, Hong Kong. Correspondence to: Zheng
Xu <xuzhustc@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

H k = (cid:107)z∗ − zk(cid:107)2
H k
= (cid:107)z∗ − zk+1 + zk+1 − zk(cid:107)2
= (cid:107)∆z∗
= (cid:107)∆z∗

k (cid:107)2
H k
H k + (cid:107)∆z+

k+1 + ∆z+
k+1(cid:107)2

H k

+ 2(∆z∗

k (cid:107)2
H k
k+1)T H k∆z+
k
k (cid:107)2

H k .

≥ (cid:107)∆z∗

k+1(cid:107)2

H k + (cid:107)∆z+

(S9)

(S10)

(S11)

(S12)

(S13)

Eq. (18) is used for the inequality in (S13), and Eq. (19)
is derived by rearranging the order of (cid:107)∆z∗
H k ≥
(cid:107)∆z∗
H k .

H k + (cid:107)∆z+

k+1(cid:107)2

k (cid:107)2

k(cid:107)2

1.4. Proof of Lemma 2

Proof. Applying the observation

(a − b)T H(c − d) =

((cid:107)a − d(cid:107)2

H − (cid:107)a − c(cid:107)2

H )

1
2
((cid:107)c − b(cid:107)2

+

1
2

H − (cid:107)c − d(cid:107)2

H ),

(S14)

Supplementary Material for Adaptive Consensus ADMM

Hence

(˜zk+1 − z)T H k∆z+

k = (˜zk+1 − z)H k(zk+1 − zk)

(S15)

(cid:107)∆z∗

k+1(cid:107)2

H k ≤(1 + (ηk)2)(cid:107)∆z∗

k(cid:107)2

H k−1

((cid:107)˜zk+1 − zk(cid:107)2

H k − (cid:107)˜zk+1 − zk+1(cid:107)2

H k )+

((cid:107)zk+1 − z(cid:107)2

H k − (cid:107)zk − z(cid:107)2

H k ).

(S16)

H k = (cid:107)˜zk+1 − zk + zk − zk+1(cid:107)2
=(cid:107)˜zk+1 − zk(cid:107)2
H k −

H k + (cid:107)∆z+
k (cid:107)2
2(˜zk+1 − zk)T H k∆z+
k ,

H k (S17)

1 (cid:107)2
Let z(cid:48) = z∗ in Lemma 3, we have

η (cid:107)∆z∗

H 0 < ∞.

(S18)

(cid:107)z − z∗(cid:107)2

H k ≤ (1 + (ηk)2)(cid:107)z − z∗(cid:107)2

H k−1

we have

=

1
2

1
2

and get

We now consider

(cid:107)˜zk+1 − zk+1(cid:107)2

k
(cid:89)

t=1
∞
(cid:89)

≤

≤

t=1
= C Π

k
(cid:89)

t=1
∞
(cid:89)

≤

≤

t=1
= C Π

(1 + (ηt)2)(cid:107)∆z∗

1 (cid:107)2
H 0

(1 + (ηt)2)(cid:107)∆z∗

1 (cid:107)2
H 0

(1 + (ηt)2)(cid:107)z − z∗(cid:107)2

H 0

(1 + (ηt)2)(cid:107)z − z∗(cid:107)2

H 0

η (cid:107)z − z∗(cid:107)2

H 0 < ∞.

(cid:107)˜zk+1 − zk(cid:107)2

=2(˜zk+1 − zk)T H k∆z+

H k − (cid:107)˜zk+1 − zk+1(cid:107)2
H k
k (cid:107)2
H k .

k − (cid:107)∆z+

We then substitute ∆z+

k with M k(˜zk+1 − zk) in (12),

(cid:107)˜zk+1 − zk(cid:107)2

H k − (cid:107)˜zk+1 − zk+1(cid:107)2
H k

=(˜zk+1 − zk)T (2I − M k)T H kM k(˜zk+1 − zk)
=(cid:107)ˆλk+1 − λk(cid:107)2

(T k)−1 ≥ 0.

Combining (S16) and (S23), we conclude

(˜zk+1−z)T H k∆z+

k ≥

((cid:107)zk+1−z(cid:107)2

Hk −(cid:107)zk−z(cid:107)2

Hk ). (S24)

1
2

Let z(cid:48) = zk in Lemma 3, we have

(cid:107)z − zk(cid:107)2

H k ≤ (1 + (ηk)2)(cid:107)z − zk(cid:107)2

H k−1 .

(S39)

Then we have
l
(cid:88)

((cid:107)z − zk(cid:107)2

H k − (cid:107)z − zk(cid:107)2

H k−1)

(ηk)2(cid:107)z − zk(cid:107)2

H k−1

(S31)

(S32)

(S33)

(S34)

(S35)

(S36)

(S37)

(S38)

(S40)

(S41)

(S42)

1.5. Proof of Lemma 3

(ηk)2(cid:107)z − z∗ + z∗ − zk(cid:107)2

H k−1

Proof. Assumption 1 implies (22), which suggests the di-
agonal matrices T k and T k−1 satisfy

k=1

l
(cid:88)

k=1

l
(cid:88)

k=1

l
(cid:88)

k=1

l
(cid:88)

k=1
∞
(cid:88)

≤

=

≤

≤

≤

(ηk)2((cid:107)z − z∗(cid:107)2

H k−1 + (cid:107)∆z∗

k(cid:107)2

H k−1 )

(S43)

(ηk)2(C Π

η (cid:107)z − z∗(cid:107)2

H 0 + C Π

η (cid:107)∆z∗

1 (cid:107)2

H 0)

(S44)

(ηk)2(C Π

η (cid:107)z − z∗(cid:107)2

H 0 + C Π

η (cid:107)∆z∗

1 (cid:107)2

H 0)

(S45)

k=1
η (C Π
=C Σ
η C Π
=C Σ

η (cid:107)z − z∗(cid:107)2
η ((cid:107)z − z∗(cid:107)2

H 0 + C Π
H 0 + (cid:107)∆z∗

η (cid:107)∆z∗
1 (cid:107)2

1 (cid:107)2
H 0)
H 0 ) < ∞.

(S46)

(S47)

Then we have

T k ≤(1 + (ηk)2)T k−1
(T k)−1 ≤(1 + (ηk)2)(T k−1)−1.

(cid:107)z − z(cid:48)(cid:107)2
=(cid:107)B(v − v(cid:48))(cid:107)2

H k

T k + (cid:107)λ − λ(cid:48)(cid:107)2

(T k)−1
T k−1+

≤(1 + (ηk)2)((cid:107)B(v − v(cid:48))(cid:107)2

(cid:107)λ − λ(cid:48)(cid:107)2

(T k−1)−1)

≤(1 + (ηk)2)(cid:107)z − z(cid:48)(cid:107)2

H k−1.

(S19)

(S20)

(S21)

(S22)

(S23)

(S25)

(S26)

(S27)

(S28)

(S29)

The inequality (S25) is used to get from (S27) to (S28).

DRS in Section 5.1

1.7. Proof of equivalence of generalized ADMM and

1.6. Proof of Lemma 4

Proof. From (27) we know

(cid:107)∆z+

k (cid:107)2

H k +(cid:107)∆z∗

k+1(cid:107)2

H k ≤ (1+(ηk)2)(cid:107)∆z∗

k(cid:107)2

H k−1 .

(S30)

Proof. The optimality condition for ADMM step (8) is
0 ∈ ∂f (uk+1) − AT (λk + T k(b − Auk+1 − Bvk)
),
(cid:125)

(cid:124)

(cid:123)(cid:122)
ˆλk+1

(S48)

Supplementary Material for Adaptive Consensus ADMM

Figure 1: ACADMM is robust to the correlation threshold hyper-
parameter (cid:15)cor.

Figure 2: ACADMM is robust to the convergence threshold Ccg.

which is equivalent to AT ˆλk+1 ∈ ∂f (uk+1). By ex-
ploiting properties of the Fenchel conjugate (Rockafellar,
1970), we get uk+1 ∈ ∂f ∗(AT ˆλk+1). A similar argu-
ment using the optimality condition for (9) leads to vk+1 ∈
∂g∗(BT λk+1). Recalling the deﬁnition of ˆf , ˆg in (42), we
arrive at

Auk+1 − b ∈ ∂ ˆf (ˆλk+1) and Bvk+1 ∈ ∂ˆg(λk+1). (S49)

We can then use simple algebra to verify λk, ˆλk+1 in (10)
and ∂ ˆf (ˆλk+1), ∂ˆg(λk+1) in (S49) satisfy the generalized
DRS steps (43, 44).

1.8. Proposition for proof in Section 5.2

Proposition 1 (Spectral DRS (Xu et al., 2017a)). Suppose
the Douglas-Rachford splitting steps are used,

0 ∈ (ˆλk+1 − λk)/τ k + ∂ ˆf (ˆλk+1) + ∂ˆg(λk)
0 ∈ (λk+1 − λk)/τ k + ∂ ˆf (ˆλk+1) + ∂ˆg(λk+1),

(S50)

(S51)

and assume the subgradients are locally linear,

∂ ˆf (ˆλ) = α ˆλ + Ψ and

∂ˆg(λ) = β λ + Φ,

(S52)

where α, β ∈ R, Ψ, Φ ⊂ Rp. Then, the minimal residual
of ˆf (λk+1)+ ˆg(λk+1) is obtained by setting τ k = 1/
α β.

√

2. More experimental results

Figure 3: ACADMM is robust to regularizer parameter ρ in EN
regression problem.

3.1. Sampling data matrices from Gaussian(s)

For Synthetic1, on each compute node i, we create a data
matrix Di ∈ Rni×d with ni samples and d features using
a standard normal distribution. For Synthetic2, we build 10
Gaussian feature sets {Di}. On each node, we then ran-
domly choose an index ji, and randomly select two Gaus-
sian parameters µ1, . . . , µ10 ∈ R and σ1, . . . , σ10 ∈ R. We
then introduce heterogeneity across nodes by computing

Di ← Di ∗ σji + µji.

(S53)

We provide more experimental results demonstrating the
robustness of ACADMM in Fig. 1, Fig. 2 and Fig. 3.

3.2. Correlation for Elastic Net regression

3. Synthetic problems in experiments

We provide the details of the synthetic data used in our ex-
periments.

Following standard method used to test elastic net regres-
sion in (Zou & Hastie, 2005), we introduce correlations
into the datasets. We start by building a random Gaus-
sian dataset Di on each node. We then select the number
of active features as 0.6d. Then we randomly select three

00.20.40.60.81Correlation threshold101102103IterationsENReg-S1ENReg-S2Logreg-S1Logreg-S2SVM-S1SVM-S21021041061081010Convergence constant paramter101102103IterationsENReg-S1ENReg-S2Logreg-S1Logreg-S2SVM-S1SVM-S210-210-1100101102Regularizer101102103IterationsENRegression-Synthetic1CADMMRB-ADMMAADMMCRB-ADMMACADMMSupplementary Material for Adaptive Consensus ADMM

Xu, Zheng, Figueiredo, Mario AT, Yuan, Xiaoming, Studer,
Christoph, and Goldstein, Thomas. Adaptive relaxed
ADMM: Convergence theory and practical implementa-
tion. CVPR, 2017b.

Xu, Zheng, Taylor, Gavin, Li, Hao, Figueiredo, Mario AT,
Yuan, Xiaoming, and Goldstein, Tom. Adaptive consen-
sus ADMM for distributed optimization. ICML, 2017c.

Zou, Hui and Hastie, Trevor. Regularization and variable
selection via the elastic net. Journal of the Royal Statis-
tical Society: Series B (Statistical Methodology), 67(2):
301–320, 2005.

Gaussian vectors vi,1, vi,2, vi,3 ∈ Rni. We then compute

∀j ∈{1, 2, . . . , 0.2d},

Di[:, j] ← Di[:, j] + vi,1,
∀j ∈{0.2d + 1, 0.2d + 2, . . . , 0.4d},
Di[:, j] ← Di[:, j] + vi,2,
∀j ∈{0.4d + 1, 0.4d + 2, . . . , 0.6d},
Di[:, j] ← Di[:, j] + vi,3,

(S54)

(S55)

(S56)

where Di[:, j] denotes the jth column of Di.

3.3. Regression measurement

We use a groundtruth vector x ∈ Rd, where the ﬁrst 0.6d
features are 1 and the rest are 0, and generate measurements
for the regression problem as

Dix = ci

(S57)

where Di is random Gaussian.

3.4. Classiﬁcation labels

For classiﬁcation problems, we add a constant dconst to the
active features on half of the feature vectors stored on each
node. This means we compute

Di[0.5ni : ni, 1 : 0.6d] ← Di[0.5ni : ni, 1 : 0.6d]+dconst.

We then create a ground truth label vector ci ∈ Rni, which
contains 1 for the permuted feature vectors, and −1 for the
rest.

References

He, Bingsheng and Yuan, Xiaoming. On the o(1/n) conver-
gence rate of the douglas-rachford alternating direction
method. SIAM Journal on Numerical Analysis, 50(2):
700–709, 2012.

He, Bingsheng and Yuan, Xiaoming. On non-ergodic con-
vergence rate of Douglas-Rachford alternating direction
method of multipliers. Numerische Mathematik, 130:
567–577, 2015.

He, Bingsheng, Yang, Hai, and Wang, Shengli. Alternating
direction method with self-adaptive penalty parameters
for monotone variational inequalities. Jour. Optim. The-
ory and Appl., 106(2):337–356, 2000.

Rockafellar, R. Convex Analysis. Princeton University

Press, 1970.

Xu, Zheng, Figueiredo, Mario AT, and Goldstein, Thomas.
Adaptive ADMM with spectral penalty parameter selec-
tion. AISTATS, 2017a.

