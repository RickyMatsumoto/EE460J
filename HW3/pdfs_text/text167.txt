Logarithmic Time One-Against-Some

A. Proof of theorem 2

Proof. For the ﬁxed tree at timestep t (there have been
1 previous splits) with a ﬁxed partition function in
t
−
the nodes, the weighted entropy of class labels is Wt =
(cid:80)

{n∈Leaves} pnHn.

When we split the tth node, the weak learning assumption
implies entropy decreases by γ according to:
(cid:18) pl
pn

pr
pn

Hl +

+ γ

Hn

Hr

≥

(cid:19)

where γ is the advantage of the weak learner. Hence,

Algorithm

Parameter

Default Value

Binary

Learning Rate
Loss

1
logistic

Recall Tree

Max Depth
Num Candidates
Depth Penalty (λ)

log2(#classes)
4 log2(#classes)
1

Table 3. Algorithm hyperparameters for various algorithms. “Bi-
nary” refers to hyperparameters inherited via reduction to binary
classiﬁcation.

Wt

Wt+1 = pnHn

plHl

prHr

pnγ

−

−

≥

−

B. Datasets

We can bound pn according to

max
n

pn

≥

1
t

which implies

γ
Wt+1 ≥
t
This can be solved recursively to get:

Wt

−

Wt+1 ≤

γ

t
(cid:88)

i=1

1
i

γ ln(t + 1)

γ ln(t + 1)

W1 −
W1 −
≤
= H1 −

where the second inequality follows from lower bounding
a harmonic series, and H1 is the marginal Shannon entropy
of the class labels.

To ﬁnish the proof, we bound the multiclass loss in terms
of the average entropy. For any leaf node n we can assign
the most likely label, y = arg maxi πni so the error rate is
(cid:15)n = 1

πny.

−

Wt+1 =

(cid:88)

(cid:88)

pn

πni ln

1
πni

1
πny

i

i

(cid:88)

pn

πni ln

1

1

(cid:15)n

−

pn ln

pn(cid:15)n

{n∈Leaves}
(cid:88)

{n∈Leaves}
(cid:88)

{n∈Leaves}
(cid:88)

{n∈Leaves}

≥

=

≥

= (cid:15)

Putting these inequalities together we have:

(cid:15)

H1 −

≤

γ ln(t + 1)

ALOI (Geusebroek et al., 2005) is a color image collection
of one-thousand small objects recorded for scientiﬁc pur-
poses (Geusebroek et al., 2005). We use the same train-test
split and representation as Choromanska et. al. (Choroman-
ska & Langford, 2015).

Imagenet consists of features extracted from intermediate
layers of a convolutional neural network trained on the IL-
VSRC2012 challenge dataset. This dataset was originally
developed to study transfer learning in visual tasks (Oquab
et al., 2014); more details are at http://www.di.ens.
fr/willow/research/cnn/. We utilize a predictor
linear in this representation.

LTCB is the Large Text Compression Benchmark, con-
sisting of the ﬁrst billion bytes of a particular Wikipedia
dump (Mahoney, 2009). Originally developed to study text
compression, it is now commonly used as a language mod-
eling benchmark where the task is to predict the next word
in the sequence. We limit the vocabulary to 80000 words
plus a single out-of-vocabulary indicator; utilize a model
linear in the 6 previous unigrams, the previous bigram, and
the previous trigram; and utilize a 90-10 train-test split on
entire Wikipedia articles.

ODP(Bennett & Nguyen, 2009) is a multiclass dataset de-
rived from the Open Directory Project. We utilize the same
train-test split and labels from (Choromanska & Langford,
2015). Speciﬁcally there is a ﬁxed train-test split of 2:1,
the representation of a document is a bag of words, and
the class label is the most speciﬁc category associated with
each document.

C. Experimental Methodology

Default Performance Methodology Hyperparameter
selection can be computationally burdensome for large data
sets, which is relevant to any claims of decreased training
times. Therefore we report results using the default val-
ues indicated in Table 3. For the larger data sets (Ima-
genet, ODP), we do a single pass over the training data;

Logarithmic Time One-Against-Some

Table 2. Empirical comparison summary. When OAA training is accelerated using parallelism and gradient subsampling, wall clock
times are parenthesized. Training times are for defaults, i.e., without hyperparameter optimization. Asterisked LOMTree results are
from (Choromanska & Langford, 2015).

Training Time

Inference Time
per example

Dataset

Method

ALOI

Imagenet

LTCB

ODP

OAA
Recall Tree
LOMTree

OAA
Recall Tree
LOMTree

OAA
Recall Tree
LOMTree

OAA
Recall Tree
LOMTree

Test Error

Default

Tuned

12.2% 12.1%
11.4%
8.6%
21.4% 16.5%∗

84.7% 82.2%
91.1% 88.4%
96.7% 90.1%∗

78.7% 76.8%
78.0% 77.6%
78.4%

-

91.2% 90.6%
96.2% 93.8%
95.4% 93.5%∗

571s
1972s
112s

446d (20.4h)
71.4h
14.0h

764d (19.1h)
4.8h
4.3h

133d (1.3h)
1.5h
0.6h

67µs
194µs
28µs

118ms
4ms
0.56ms

3600µs
76µs
51µs

560ms
1.9ms
0.52ms

for the smaller data set (ALOI), we do multiple passes over
the training data, monitoring a 10% held-out portion of the
training data to determine when to stop optimizing.

Tuned Performance Methodology For tuned perfor-
mance, we use random search over hyperparameters, tak-
ing the best result over 59 probes. For the smaller data set
(ALOI), we optimize validation error on a 10% held-out
subset of the training data. For the larger data sets (Im-
agenet, ODP), we optimize progressive validation loss on
the initial 10% of the training data. After determining hy-
perparameters we retrain with the entire training set and
report the resulting test error.

When available we report published LOMtree results, al-
though they utilized a different method for optimizing hy-
perparameters.

Timing Measurements All timings are taken from the
same 24 core xeon server machine. Furthermore, all algo-
rithms are implemented in the Vowpal Wabbit toolkit and
therefore share ﬁle formats, parser, and binary classiﬁca-
tion base learner implying differences are attributable to
the different reductions. Our baseline OAA implementa-
tion is mature and highly tuned: it always exploits vector-
ization, and furthermore can optionally utilize multicore
training and negative gradient subsampling to accelerate
training. For the larger datasets these latter features were
necessary to complete the experiments: estimated unaccel-
erated training times are given, along with wall clock times
in parenthesis.

