Self-Paced Co-training

Fan Ma 1 Deyu Meng * 1 Qi Xie 1 Zina Li 1 Xuanyi Dong 2

In this supplementary material, we present the condition of
(cid:15)-expanding with respect to the proposed serial co-training
process, and give the proof that SPaCo is an efﬁcient PAC
learning algorithm if such condition is satisﬁed.

Notation and Deﬁnition: We assume that examples are
drawn from some distributions D over an instance space
X = X1 × X2, where X1 and X2 correspond to two
different “views” of examples. Let c denote the target
function, and let X + and X − (for simplicity we assume
we are doing binary classiﬁcation) denote the positive and
negative regions of X, respectively . For i ∈ 1, 2, let
X +
i = {xj ∈ Xi : ci(xj) = 1}, so we can think of X +
as X +
i . Let D+ and D−
denote the marginal distribution of D over X + and X −,
respectively.

i = Xi − X +

2 , and let X −

1 × X +

For S1 ⊆ X1 and S2 ⊆ X2, let boldface Si denote the
event that an example (cid:104)x1, x2(cid:105) has xi ∈ Si. The P (Sn
i )
denotes the possibility mass on example for which we are
conﬁdent under ith view in the nth training round. Be-
low we give the deﬁnition of (cid:15)-expanding afﬁxing marks of
training round.

Deﬁnition 1 (Balcan et al., 2004) Let X + denote the pos-
itive region and D+ denote the distribution over X +, and
Xi(i = 1, 2) is the training data set in the ith view. For
S1 ⊆ X1 and S2 ⊆ X2, the D+ is (cid:15)-expanding if the fol-
lowing inequality holds:

P (S1 ⊕ S2) ≥ (cid:15) min(P (S1 ∧ S2), P ( ¯S1 ∧ ¯S2)),

(1)

where P (S1 ∧ S2) denotes the probability of examples for
being conﬁdent in both views, and P (S1 ⊕ S2) denotes
the probability of examples for being conﬁdent in only one
view.

Proof

To present training order of classiﬁer under each view, we
add superscript for distinguishing the order of iteration.
The reivsed deﬁnition is:

1Xi’an Jiaotong University, Xi’an, China 2University of Tech-
nology Sydney, Sydney, Australia. Correspondence to: Deyu
Meng <dymeng@xjtu.edu.cn>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Deﬁnition 2 D+ is (cid:15)-expanding in the serial training pro-
cess if

P (Sn

i ⊕ Sn−1

3−i ) ≥ (cid:15) min(P (Sn−1

3−i ∧ Sn

i ), P (Sn−1

3−i ∧ Sn

i ))
(2)

This (cid:15)-expanding deﬁnition is the same as that deﬁned in
(Balcan et al., 2004) except for the round mark in each
view. When D+ satisﬁes (cid:15)-expanding in every training
round and there are sufﬁcient unlabeled instances, classi-
ﬁers under each view can acquire arbitrary accuracy with
probability 1 − δ after enough training rounds as described
in Theorem 1.

Theorem 1 Let (cid:15)f in and δf in be the desired accuracy and
conﬁdence parameters. Suppose that serial (cid:15)-expanding
condition is satisﬁed in each training round, then we can
achieve error rate (cid:15)f in with probability 1−δf in by running
the SPaCo for N = O( 1
(cid:15) log 1
) rounds, each time
(cid:15)f in
running algorithm A1 and algorithm A2 with accuracy and
conﬁdence parameters set to (cid:15)·(cid:15)f in
2N respectively.

and δf in

(cid:15) · 1
pinit

+ 1

8

Similar to proof in (Balcan et al., 2004), we begin by stating
two lemmas that will be useful for the analysis. For both
lemmas, let Sn
i , and all probabilities are with the
respect to D+.

i ⊆ X +

3−i ∨ Sn−1

Lemma1 Suppose P (Sn
P (Sn
Sn−1
i
Sn−1
i

3−i|Sn
) ≥ 1 − (cid:15)
)

i

8 , then P (Sn+1

i

i

3−i ∧ Sn−1
) ≥ 1 − (cid:15)
∧ Sn

) ≤ P (Sn
8 and P (Sn+1
i
3−i) ≥ (1 + (cid:15)

3−i ∧ Sn−1
),
i
|Sn
3−i ∨
2 )P (Sn
3−i ∧

) + P (Sn

3−i, Sn

3−i ∨ Sn−1

i

)

∧ Sn

3−i)
3−i ∨ Sn−1
, Sn
3−i ∨ Sn−1
i
)(1 + (cid:15))P (Sn

i
)

P (Sn+1
i
≥ P (Sn+1
i
− P (Sn
(cid:15)
4
(cid:15)
2

≥ (1 −

≥ (1 +

)P (Sn

3−i ∧ Sn−1

i

)

3−i ∧ Sn−1

i

)

(3)

Lemma2 Suppose P (Sn
let γ = 1 − P (Sn
1− γ(cid:15)
8 and P (Sn
3−i) ≥ (1 + (cid:15)
Sn

3−i ∧ Sn−1
3−i|Sn
2 )P (Sn

3−i ∨Sn−1
i
3−i ∧ Sn−1

3−i∧Sn−1
i

), if P (Sn+1
i
) > 1− γ(cid:15)
)

) > P (S

i

i

n
3−i∧Sn−1
i
3−i ∨ Sn−1
|Sn
i
8 , then P (Sn+1
i

) and
) >
∧

Self-Paced Co-training

Proof

) + P (Sn

3−i ∧ Sn−1

i

)

γ = P (Sn

3−i ⊕ Sn−1

i
≥ (1 + (cid:15))P (Sn
≥ (1 + (cid:15))(1 − P (Sn

3−i ∧ Sn−1

i

)
3−i ∨ Sn−1

i

))

(4)

From inequality 4 we can get P (Sn
Thus

3−i ∨ Sn−1

i

) ≥ 1 − γ

1+(cid:15) .

P (Sn+1
i

∧ Sn

3−i) ≥ (1 −

)(1 −

γ
1 + (cid:15)

)

)

γ(cid:15)
8
3−i ∧ Sn−1
)P (Sn

i

)

γ(cid:15)
4

γ(cid:15)
8

≥ (1 +

≥ (1 − γ)(1 +

(5)

From Lemma 1 and Lemma 2, we present that with ﬁne
tuned conﬁdence condition, classiﬁers trained in a serial
way possess same character compared with classiﬁers built
paralleled after each iteration. Therefore, we conclude
that with the modiﬁed (cid:15)-expanding condition fulﬁlled, after
same number of iterations, classiﬁers trained serially can
achieve same error rate with same conﬁdence as shown in
the original (cid:15)-expanding theorem (Balcan et al., 2004).

References

Balcan, Maria-Florina, Blum, Avrim, and Yang, Ke. Co-
training and expansion: Towards bridging theory and
practice. In Advances in neural information processing
systems, pp. 89–96, 2004.

