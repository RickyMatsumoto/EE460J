Wasserstein Generative Adversarial Networks

Martin Arjovsky 1 Soumith Chintala 2 L´eon Bottou 1 2

Abstract
We introduce a new algorithm named WGAN,
an alternative to traditional GAN training.
In
this new model, we show that we can improve
the stability of learning, get rid of problems like
mode collapse, and provide meaningful learning
curves useful for debugging and hyperparameter
searches. Furthermore, we show that the cor-
responding optimization problem is sound, and
provide extensive theoretical work highlighting
the deep connections to different distances be-
tween distributions.

1. Introduction

The problem this paper is concerned with is that of unsu-
pervised learning. Mainly, what does it mean to learn a
probability distribution? The classical answer to this is to
learn a probability density. This is often done by deﬁning
a parametric family of densities (Pθ)θ∈Rd and ﬁnding the
one that maximized the likelihood on our data: if we have
real data examples {x(i)}m
i=1, we would solve the problem
m
(cid:88)

max
θ∈Rd

1
m

i=1

log Pθ(x(i))

If the real data distribution Pr admits a density and Pθ is the
distribution of the parametrized density Pθ, then, asymp-
totically, this amounts to minimizing the Kullback-Leibler
divergence KL(Pr(cid:107)Pθ).

For this to make sense, we need the model density Pθ to
exist. This is not the case in the rather common situation
where we are dealing with distributions supported by low
dimensional manifolds. It is then unlikely that the model
manifold and the true distribution’s support have a non-
negligible intersection (see (Arjovsky & Bottou, 2017)),
and this means that the KL distance is not deﬁned (or sim-
ply inﬁnite).

1Courant Institute of Mathematical Sciences, NY 2Facebook
AI Research, NY. Correspondence to: Martin Arjovsky <marti-
narjovsky@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

The typical remedy is to add a noise term to the model dis-
tribution. This is why virtually all generative models de-
scribed in the classical machine learning literature include
a noise component.
In the simplest case, one assumes a
Gaussian noise with relatively high bandwidth in order to
cover all the examples. It is well known, for instance, that
in the case of image generation models, this noise degrades
the quality of the samples and makes them blurry. For ex-
ample, we can see in the recent paper (Wu et al., 2016)
that the optimal standard deviation of the noise added to
the model when maximizing likelihood is around 0.1 to
each pixel in a generated image, when the pixels were al-
ready normalized to be in the range [0, 1]. This is a very
high amount of noise, so much that when papers report the
samples of their models, they don’t add the noise term on
which they report likelihood numbers. In other words, the
added noise term is clearly incorrect for the problem, but is
needed to make the maximum likelihood approach work.
Rather than estimating the density of Pr which may not ex-
ist, we can deﬁne a random variable Z with a ﬁxed dis-
tribution p(z) and pass it through a parametric function
gθ : Z → X (typically a neural network of some kind)
that directly generates samples following a certain distribu-
tion Pθ. By varying θ, we can change this distribution and
make it close to the real data distribution Pr. This is use-
ful in two ways. First of all, unlike densities, this approach
can represent distributions conﬁned to a low dimensional
manifold. Second, the ability to easily generate samples is
often more useful than knowing the numerical value of the
density (for example in image superresolution or semantic
segmentation when considering the conditional distribution
of the output image given the input image). In general, it
is computationally difﬁcult to generate samples given an
arbitrary high dimensional density (Neal, 2001).

Variational Auto-Encoders (VAEs) (Kingma & Welling,
2013) and Generative Adversarial Networks (GANs)
(Goodfellow et al., 2014) are well known examples of this
approach. Because VAEs focus on the approximate likeli-
hood of the examples, they share the limitation of the stan-
dard models and need to ﬁddle with additional noise terms.
GANs offer much more ﬂexibility in the deﬁnition of the
objective function, including Jensen-Shannon (Goodfellow
et al., 2014), and all f -divergences (Nowozin et al., 2016)
as well as some exotic combinations (Huszar, 2015). On

Wasserstein Generative Adversarial Networks

the other hand, training GANs is well known for being del-
icate and unstable, for reasons theoretically investigated in
(Arjovsky & Bottou, 2017).

In this paper, we direct our attention on the various ways to
measure how close the model distribution and the real dis-
tribution are, or equivalently, on the various ways to deﬁne
a distance or divergence ρ(Pθ, Pr). The most fundamen-
tal difference between such distances is their impact on the
convergence of sequences of probability distributions. A
sequence of distributions (Pt)t∈N converges if and only if
there is a distribution P∞ such that ρ(Pt, P∞) tends to zero,
something that depends on how exactly the distance ρ is
deﬁned. Informally, a distance ρ induces a weaker topol-
ogy when it makes it easier for a sequence of distribution
to converge.1 Section 2 clariﬁes how popular probability
distances differ in that respect.

In order to optimize the parameter θ, it is of course desir-
able to deﬁne our model distribution Pθ in a manner that
makes the mapping θ (cid:55)→ Pθ continuous. Continuity means
that when a sequence of parameters θt converges to θ, the
distributions Pθt also converge to Pθ. However, it is essen-
tial to remember that the notion of the convergence of the
distributions Pθt depends on the way we compute the dis-
tance between distributions. The weaker this distance, the
easier it is to deﬁne a continuous mapping from θ-space to
Pθ-space, since it’s easier for the distributions to converge.
The main reason we care about the mapping θ (cid:55)→ Pθ to be
continuous is as follows. If ρ is our notion of distance be-
tween two distributions, we would like to have a loss func-
tion θ (cid:55)→ ρ(Pθ, Pr) that is continuous, and this is equivalent
to having the mapping θ (cid:55)→ Pθ be continuous when using
the distance between distributions ρ.

The contributions of this paper are:

• In Section 2, we provide a comprehensive theoretical
analysis of how the Earth Mover (EM) distance be-
haves in comparison to popular probability distances
and divergences used in the context of learning distri-
butions.

• In Section 3, we deﬁne a form of GAN called
Wasserstein-GAN that minimizes a reasonable and ef-
ﬁcient approximation of the EM distance, and we the-
oretically show that the corresponding optimization
problem is sound.

• In Section 4, we empirically show that WGANs cure
the main training problems of GANs.
In particular,
training WGANs does not require maintaining a care-
ful balance in training of the discriminator and the

generator, does not require a careful design of the net-
work architecture either, and also reduces the mode
dropping that is typical in GANs. One of the most
compelling practical beneﬁts of WGANs is the ability
to continuously estimate the EM distance by training
the discriminator to optimality. Because they correlate
well with the observed sample quality, plotting these
learning curves is very useful for debugging and hy-
perparameter searches.

2. Different Distances

We now introduce our notation. Let X be a compact metric
set, say the space of images [0, 1]d, and let Σ denote the
set of all the Borel subsets of X . Let Prob(X ) denote the
space of probability measures deﬁned on X . We can now
deﬁne elementary distances and divergences between two
distributions Pr, Pg ∈ Prob(X ):

• The Total Variation (TV) distance

δ(Pr, Pg) = sup
A∈Σ

|Pr(A) − Pg(A)| .

• The Kullback-Leibler (KL) divergence

KL(Pr(cid:107)Pg) =

log

(cid:90)

(cid:19)

(cid:18) Pr(x)
Pg(x)

Pr(x)dµ(x) ,

where both Pr and Pg are assumed to admit densities
with respect to a same measure µ deﬁned on X .2 The
KL divergence is famously assymetric and possibly
inﬁnite when there are points such that Pg(x) = 0
and Pr(x) > 0.

• The Jensen-Shannon (JS) divergence

JS(Pr, Pg) = KL(Pr(cid:107)Pm) + KL(Pg(cid:107)Pm) ,

where Pm is the mixture (Pr + Pg)/2. This diver-
gence is symmetrical and always deﬁned because we
can choose µ = Pm.

• The Earth-Mover (EM) distance or Wasserstein-1
(cid:2) (cid:107)x − y(cid:107) (cid:3) ,

W (Pr, Pg) =

E(x,y)∼γ

(1)

inf
γ∈Π(Pr,Pg)

where Π(Pr, Pg) is the set of all joint distributions
γ(x, y) whose marginals are respectively Pr and Pg.
Intuitively, γ(x, y) indicates how much “mass” must
be transported from x to y in order to transform the
distributions Pr into the distribution Pg. The EM dis-
tance then is the “cost” of the optimal transport plan.

1More exactly, the topology induced by ρ is weaker than that
induced by ρ(cid:48) when the set of convergent sequences under ρ is a
superset of that under ρ(cid:48).

2Recall that a probability distribution Pr ∈ Prob(X ) admits
a density Pr(x) with respect to µ, that is, ∀A ∈ Σ, Pr(A) =
(cid:82)
A Pr(x)dµ(x), if and only it is absolutely continuous with re-
spect to µ, that is, ∀A ∈ Σ, µ(A) = 0 ⇒ Pr(A) = 0 .

Wasserstein Generative Adversarial Networks

Figure 1: These plots show ρ(Pθ, P0) as a function of θ when ρ is the EM distance (left plot) or the JS divergence (right plot). The EM
plot is continuous and provides a usable gradient everywhere. The JS plot is not continuous and does not provide a usable gradient.

The following example illustrates how apparently simple
sequences of probability distributions converge under the
EM distance but do not converge under the other distances
and divergences deﬁned above.

Example 1 (Learning parallel lines). Let Z ∼ U [0, 1] the
uniform distribution on the unit interval. Let P0 be the dis-
tribution of (0, Z) ∈ R2 (a 0 on the x-axis and the random
variable Z on the y-axis), uniform on a straight vertical line
passing through the origin. Now let gθ(z) = (θ, z) with θ
a single real parameter. It is easy to see that in this case,

• W (P0, Pθ) = |θ|,
(cid:40)

• JS(P0, Pθ) =

log 2
0

if θ (cid:54)= 0 ,
if θ = 0 ,

• KL(Pθ(cid:107)P0) = KL(P0(cid:107)Pθ) =

(cid:40)

+∞
0

if θ (cid:54)= 0 ,
if θ = 0 ,

• and δ(P0, Pθ) =

(cid:40)

1
0

if θ (cid:54)= 0 ,
if θ = 0 .

When θt → 0, the sequence (Pθt)t∈N converges to P0 un-
der the EM distance, but does not converge at all under
either the JS, KL, reverse KL, or TV divergences. Figure 1
illustrates this for the case of the EM and JS distances.

Example 1 gives a case where we can learn a probability
distribution over a low dimensional manifold by doing gra-
dient descent on the EM distance. This cannot be done with
the other distances and divergences because the resulting
loss function is not even continuous. Although this simple
example features distributions with disjoint supports, the
same conclusion holds when the supports have a non empty
intersection contained in a set of measure zero. This hap-
pens to be the case when two low dimensional manifolds
intersect in general position (Arjovsky & Bottou, 2017).

Since the Wasserstein distance is much weaker than the JS

distance,3 we can now ask whether W (Pr, Pθ) is a contin-
uous loss function on θ under mild assumptions:
Theorem 1. Let Pr be a ﬁxed distribution over X . Let
Z be a random variable (e.g Gaussian) over another
space Z. Let Pθ denote the distribution of gθ(Z) where
g : (z, θ) ∈ Z × Rd (cid:55)→ gθ(z) ∈ X . Then,

1. If g is continuous in θ, so is W (Pr, Pθ).

2. If g is locally Lipschitz and satisﬁes regularity as-
then W (Pr, Pθ) is continuous every-

sumption 1,
where, and differentiable almost everywhere.

3. Statements 1-2 are false for the Jensen-Shannon di-

vergence JS(Pr, Pθ) and all the KLs.

As a consequence, learning by minimizing the EM distance
makes sense (at least in theory) for neural networks:
Corollary 1. Let gθ be any feedforward neural network4
parameterized by θ, and p(z) a prior over z such that
Ez∼p(z)[(cid:107)z(cid:107)] < ∞ (e.g. Gaussian, uniform, etc.). Then
assumption 1 is satisﬁed and therefore W (Pr, Pθ) is con-
tinuous everywhere and differentiable almost everywhere.

Both proofs are given in Appendix C.

All this indicates that EM is a much more sensible cost
function for our problem than at least the Jensen-Shannon
divergence. The following theorem describes the relative
strength of the topologies induced by these distances and
divergences, with KL the strongest, followed by JS and TV,
and EM the weakest.
Theorem 2. Let P be a distribution on a compact space X
and (Pn)n∈N be a sequence of distributions on X . Then,
considering all limits as n → ∞,

3Appendix A explains to the mathematically inclined reader
why this happens and how we arrived to the idea that Wasserstein
is what we should really be optimizing.

4By a feedforward neural network we mean a function com-
posed of afﬁne transformations and componentwise Lipschitz
nonlinearities (such as the sigmoid, tanh, elu, softplus, etc). A
similar but more technical proof is required for ReLUs.

Wasserstein Generative Adversarial Networks

1. The following statements are equivalent

• δ(Pn, P) → 0 with δ the total variation distance.
• JS(Pn, P) → 0 with JS the Jensen-Shannon di-

vergence.

2. The following statements are equivalent

• W (Pn, P) → 0.
• Pn

D−→ P where D−→ represents convergence in

distribution for random variables.

3. KL(Pn(cid:107)P) → 0 or KL(P(cid:107)Pn) → 0 imply the state-

ments in (1).

4. The statements in (1) imply the statements in (2).

Proof. See Appendix C

This highlights the fact that the KL, JS, and TV distances
are not sensible cost functions when learning distributions
supported by low dimensional manifolds. However the EM
distance is sensible in that setup. This leads us to the next
section where we introduce a practical approximation of
optimizing the EM distance.

3. Wasserstein GAN

Again, Theorem 2 points to the fact that W (Pr, Pθ) might
have nicer properties when optimized than JS(Pr, Pθ).
However, the inﬁmum in (1) is highly intractable. On the
other hand, the Kantorovich-Rubinstein duality (Villani,
2009) tells us that

W (Pr, Pθ) = sup

Ex∼Pr [f (x)] − Ex∼Pθ [f (x)]

(2)

(cid:107)f (cid:107)L≤1

where the supremum is over all the 1-Lipschitz functions
f : X → R. Note that if we replace (cid:107)f (cid:107)L ≤ 1 for
(cid:107)f (cid:107)L ≤ K (consider K-Lipschitz for some constant K),
then we end up with K · W (Pr, Pg). Therefore, if we have
a parameterized family of functions {fw}w∈W that are all
K-Lipschitz for some K, we could consider solving the
problem

max
w∈W

Ex∼Pr [fw(x)] − Ez∼p(z)[fw(gθ(z)]

(3)

and if the supremum in (2) is attained for some w ∈ W
(a pretty strong assumption akin to what’s assumed when
proving consistency of an estimator), this process would
yield a calculation of W (Pr, Pθ) up to a multiplicative
constant. Furthermore, we could consider differentiat-
ing W (Pr, Pθ) (again, up to a constant) by back-proping
through equation (2) via estimating Ez∼p(z)[∇θfw(gθ(z))].
While this is all intuition, we now prove that this process is
principled under the optimality assumption.

Algorithm 1 WGAN, our proposed algorithm. All exper-
iments in the paper used the default values α = 0.00005,
c = 0.01, m = 64, ncritic = 5.
Require: : α, the learning rate. c, the clipping parameter.
m, the batch size. ncritic, the number of iterations of the
critic per generator iteration.

Require: : w0, initial critic parameters. θ0, initial genera-

tor’s parameters.

1: while θ has not converged do
for t = 0, ..., ncritic do
2:
Sample {x(i)}m
3:
Sample {z(i)}m
4:
gw ← ∇w[ 1
5:
m

i=1 ∼ Pr a batch from the real data.
i=1 ∼ p(z) a batch of priors.
(cid:80)m

i=1 fw(x(i))
− 1
m

(cid:80)m

i=1 fw(gθ(z(i)))]

w ← w + α · RMSProp(w, gw)
w ← clip(w, −c, c)

end for
Sample {z(i)}m
gθ ← −∇θ
θ ← θ − α · RMSProp(θ, gθ)

i=1 fw(gθ(z(i)))

1
m

i=1 ∼ p(z) a batch of prior samples.
(cid:80)m

6:
7:
8:
9:
10:
11:
12: end while

Theorem 3. Let Pr be any distribution. Let Pθ be the dis-
tribution of gθ(Z) with Z a random variable with density p
and gθ a function satisfying assumption 1. Then, there is a
solution f : X → R to the problem

max
(cid:107)f (cid:107)L≤1

Ex∼Pr [f (x)] − Ex∼Pθ [f (x)]

and we have

∇θW (Pr, Pθ) = −Ez∼p(z)[∇θf (gθ(z))]

when both terms are well-deﬁned.

Proof. See Appendix C

Now comes the question of ﬁnding the function f that
solves the maximization problem in equation (2). To
roughly approximate this, something that we can do is
train a neural network parameterized with weights w ly-
ing in a compact space W and then backprop through
Ez∼p(z)[∇θfw(gθ(z))], as we would do with a typical
GAN. Note that the fact that W is compact implies that all
the functions fw will be K-Lipschitz for some K that only
depends on W and not the individual weights, therefore
approximating (2) up to an irrelevant scaling factor and the
capacity of the ‘critic’ fw. In order to have parameters w lie
in a compact space, something simple we can do is clamp
the weights to a ﬁxed box (say W = [−0.01, 0.01]l) after
each gradient update. The Wasserstein Generative Adver-
sarial Network (WGAN) procedure is described in Algo-
rithm 1.

Wasserstein Generative Adversarial Networks

Figure 2: Different methods learning a mixture of 8 gaussians spread in a circle. WGAN is able to learn the distribution without mode
collapse. An interesting fact is that the WGAN (much like the Wasserstein distance) seems to capture ﬁrst the low dimensional structure
of the data (the approximate circle) before matching the speciﬁc bumps in the density. Green: KDE plots. Blue: samples from the model.

is differentiable almost everywhere. For the JS, as the dis-
criminator gets better the gradients get more reliable but
the true gradient is 0 since the JS is locally saturated and
we get vanishing gradients, as can be seen in Figure 1 of
this paper and Theorem 2.4 of (Arjovsky & Bottou, 2017).
In Figure 3 we show a proof of concept of this, where we
train a GAN discriminator and a WGAN critic till optimal-
ity. The discriminator learns very quickly to distinguish
between fake and real, and as expected provides no reliable
gradient information. The critic, however, can’t saturate,
and converges to a linear function that gives remarkably
clean gradients everywhere. The fact that we constrain the
weights limits the possible growth of the function to be at
most linear in different parts of the space, forcing the opti-
mal critic to have this behaviour.

Perhaps more importantly, the fact that we can train the
critic till optimality makes it impossible to collapse modes
when we do. This is due to the fact that mode collapse
comes from the fact that the optimal generator for a ﬁxed
discriminator is a sum of deltas on the points the discrimi-
nator assigns the highest values, as observed by (Goodfel-
low et al., 2014) and highlighted in (Metz et al., 2016).

In the following section we display the practical beneﬁts of
our new algorithm, and we provide an in-depth comparison
of its behaviour and that of traditional GANs.

Figure 3: Optimal discriminator and critic when learning to dif-
ferentiate two Gaussians. As we can see, the traditional GAN
discriminator saturates and results in vanishing gradients. Our
WGAN critic provides very clean gradients on all parts of the
space.

The fact that the EM distance is continuous and differen-
tiable a.e. means that we can (and should) train the critic
till optimality. The argument is simple, the more we train
the critic, the more reliable gradient of the Wasserstein we
get, which is actually useful by the fact that Wasserstein

Wasserstein Generative Adversarial Networks

Figure 4: Training curves and samples at different stages of training. We can see a clear correlation between lower error and better
sample quality. Upper left: the generator is an MLP with 4 hidden layers and 512 units at each layer. The loss decreases constistently
as training progresses and sample quality increases. Upper right: the generator is a standard DCGAN. The loss decreases quickly
and sample quality increases as well. In both upper plots the critic is a DCGAN without the sigmoid so losses can be subjected to
comparison. Lower half: both the generator and the discriminator are MLPs with substantially high learning rates (so training failed).
Loss is constant and samples are constant as well. The training curves were passed through a median ﬁlter for visualization purposes.

4. Empirical Results

4.2. Experimental Procedure for Image Generation

We run experiments on image generation using our
Wasserstein-GAN algorithm and show that there are sig-
niﬁcant practical beneﬁts to using it over the formulation
used in standard GANs. We claim two main beneﬁts:

• a meaningful loss metric that correlates with the gen-

erator’s convergence and sample quality

• improved stability of the optimization process

We run experiments on image generation. The target dis-
tribution to learn is the LSUN-Bedrooms dataset (Yu et al.,
2015) – a collection of natural images of indoor bedrooms.
Our baseline comparison is DCGAN (Radford et al., 2015),
a GAN with a convolutional architecture trained with the
standard GAN procedure using the − log D trick (Goodfel-
low et al., 2014). The generated samples are 3-channel im-
ages of 64x64 pixels in size. We use the hyper-parameters
speciﬁed in Algorithm 1 for all of our experiments.

4.1. Mixtures of Gaussians

4.3. Meaningful loss metric

In (Metz et al., 2016) the authors presented a simple mix-
ture of Gaussians experiments that served a very speciﬁc
purpose.
In this mixture, the mode collapse problem of
GANs is easy to visualize, since a normal GAN would ro-
tate between the different modes of the mixture, and fail
to capture the whole distribution. In 2 we show how our
WGAN algorithm approximately ﬁnds the correct distribu-
tion, without any mode collapse.

An interesting thing is that the WGAN ﬁrst seems to learn
to match the low-dimensional structure of the data (the ap-
proximate circle), before zooming in on the speciﬁc bumps
of the true density. Similar to the Wasserstein distance, it
looks like WGAN gives more importance to matching the
low dimensional supports rather than the speciﬁc ratios be-
tween the densities.

Because the WGAN algorithm attempts to train the critic f
(lines 2–8 in Algorithm 1) relatively well before each gen-
erator update (line 10 in Algorithm 1), the loss function at
this point is an estimate of the EM distance, up to constant
factors related to the way we constrain the Lipschitz con-
stant of f .

Our ﬁrst experiment illustrates how this estimate correlates
well with the quality of the generated samples. Besides
the convolutional DCGAN architecture, we also ran exper-
iments where we replace the generator or both the generator
and the critic by 4-layer ReLU-MLP with 512 hidden units.

Figure 4 plots the evolution of the WGAN estimate (3) of
the EM distance during WGAN training for all three archi-
tectures. The plots clearly show that these curves correlate

Wasserstein Generative Adversarial Networks

Figure 5: JS estimates for an MLP generator (upper left) and a DCGAN generator (upper right) trained with the standard GAN
procedure. Both had a DCGAN discriminator. Both curves have increasing error. Samples get better for the DCGAN but the JS estimate
increases or stays constant, pointing towards no signiﬁcant correlation between sample quality and loss. Bottom: M LP with both
generator and discriminator. The curve goes up and down regardless of sample quality. All training curves were passed through the
same median ﬁlter as in Figure 4.

well with the visual quality of the generated samples.

To our knowledge, this is the ﬁrst time in GAN litera-
ture that such a property is shown, where the loss of the
GAN shows properties of convergence. This property is
extremely useful when doing research in adversarial net-
works as one does not need to stare at the generated sam-
ples to ﬁgure out failure modes and to gain information on
which models are doing better over others.

However, we do not claim that this is a new method to
quantitatively evaluate generative models yet. The con-
stant scaling factor that depends on the critic’s architecture
means it’s hard to compare models with different critics.
Even more, in practice the fact that the critic doesn’t have
inﬁnite capacity makes it hard to know just how close to
the EM distance our estimate really is. This being said, we
have succesfully used the loss metric to validate our exper-
iments repeatedly and without failure, and we see this as a
huge improvement in training GANs which previously had
no such facility.

In contrast, Figure 5 plots the evolution of the GAN esti-
mate of the JS distance during GAN training. More pre-
cisely, during GAN training, the discriminator is trained to
maximize

L(D, gθ) = Ex∼Pr [log D(x)] + Ex∼Pθ [log(1 − D(x))]

which is is a lower bound of 2JS(Pr, Pθ) − 2 log 2. In the
ﬁgure, we plot the quantity 1
2 L(D, gθ) + log 2, which is a
lower bound of the JS distance.

This quantity clearly correlates poorly the sample quality.
Note also that the JS estimate usually stays constant or goes
up instead of going down.
In fact it often remains very
close to log 2 ≈ 0.69 which is the highest value taken by
the JS distance. In other words, the JS distance saturates,
the discriminator has zero loss, and the generated samples
are in some cases meaningful (DCGAN generator, top right
plot) and in other cases collapse to a single nonsensical im-
age (Goodfellow et al., 2014). This last phenomenon has
been theoretically explained in (Arjovsky & Bottou, 2017)
and highlighted in (Metz et al., 2016).

When using the − log D trick (Goodfellow et al., 2014),
the discriminator loss and the generator loss are different.
Figure 9 in Appendix F reports the same plots for GAN
training, but using the generator loss instead of the discrim-
inator loss. This does not change the conclusions.

Finally, as a negative result, we report that WGAN train-
ing becomes unstable at times when one uses a momentum
based optimizer such as Adam (Kingma & Ba, 2014) (with
β1 > 0) on the critic, or when one uses high learning rates.
Since the loss for the critic is nonstationary, momentum
based methods seemed to perform worse. We identiﬁed
momentum as a potential cause because, as the loss blew up
and samples got worse, the cosine between the Adam step
and the gradient usually turned negative. The only places
where this cosine was negative was in these situations of
instability. We therefore switched to RMSProp (Tieleman
& Hinton, 2012) which is known to perform well even on
very nonstationary problems (Mnih et al., 2016).

Wasserstein Generative Adversarial Networks

Figure 6: Algorithms trained with a DCGAN generator. Left: WGAN algorithm. Right: standard GAN formulation. Both algorithms
produce high quality samples.

Figure 7: Algorithms trained with a generator without batch normalization and constant number of ﬁlters at every layer (as opposed
to duplicating them every time as in (Radford et al., 2015)). Aside from taking out batch normalization, the number of parameters is
therefore reduced by a bit more than an order of magnitude. Left: WGAN algorithm. Right: standard GAN formulation. As we can see
the standard GAN failed to learn while the WGAN still was able to produce samples.

Figure 8: Algorithms trained with an MLP generator with 4 layers and 512 units with ReLU nonlinearities. The number of parameters
is similar to that of a DCGAN, but it lacks a strong inductive bias for image generation. Left: WGAN algorithm. Right: standard GAN
formulation. The WGAN method still was able to produce samples, lower quality than the DCGAN, and of higher quality than the MLP
of the standard GAN. Note the signiﬁcant degree of mode collapse in the GAN MLP.

4.4. Improved stability

5. Related Work

One of the beneﬁts of WGAN is that it allows us to train
the critic till optimality. When the critic is trained to com-
pletion, it simply provides a loss to the generator that we
can train as any other neural network. This tells us that we
no longer need to balance generator and discriminator’s ca-
pacity properly. The better the critic, the higher quality the
gradients we use to train the generator.

We observe that WGANs are more robust than GANs when
one varies the architectural choices for the generator in cer-
tain ways. We illustrate this by running experiments on
three generator architectures: (1) a convolutional DCGAN
generator, (2) a convolutional DCGAN generator without
batch normalization and with a constant number of ﬁlters
(the capacity of the generator is drastically smaller than that
of the discriminator), and (3) a 4-layer ReLU-MLP with
512 hidden units. The last two are known to perform very
poorly with GANs. We keep the convolutional DCGAN ar-
chitecture for the WGAN critic or the GAN discriminator.

Figures 6, 7, and 8 show samples generated for these three
architectures using both the WGAN and GAN algorithms.
We refer the reader to Appendix H for full sheets of gener-
ated samples. Samples were not cherry-picked.

In no experiment did we see evidence of mode collapse
for the WGAN algorithm.

We refer the reader to Appendix D for the connections to
the different integral probability metrics (M¨uller, 1997).

The recent work of (Montavon et al., 2016) has explored
the use of Wasserstein distances in the context of learn-
ing for Restricted Boltzmann Machines for discrete spaces.
Even though the motivations at a ﬁrst glance might seem
quite different, at the core of it both our works want to com-
pare distributions in a way that leverages the geometry of
the underlying space, which Wasserstein allows us to do.

Finally, the work of (Genevay et al., 2016) shows new al-
gorithms for calculating Wasserstein distances between dif-
ferent distributions. We believe this direction is quite im-
portant, and perhaps could lead to new ways to evaluate
generative models.

6. Conclusion

We introduced an algorithm that we deemed WGAN, an
alternative to traditional GAN training. In this new model,
we showed that we can improve the stability of learning,
get rid of problems like mode collapse, and provide mean-
ingful learning curves useful for debugging and hyperpa-
rameter searches. Furthermore, we showed that the corre-
sponding optimization problem is sound, and provided ex-
tensive theoretical work highlighting the deep connections
to other distances between distributions.

Wasserstein Generative Adversarial Networks

Acknowledgments

We would like to thank Mohamed Ishmael Belghazi, Emily
Denton, Ian Goodfellow, Ishaan Gulrajani, Alex Lamb,
David Lopez-Paz, Eric Martin, musyoku, Maxime Oquab,
Aditya Ramesh, Ronan Riochet, Uri Shalit, Pablo Sprech-
mann, Arthur Szlam, Ruohan Wang, for helpful comments
and advice.

References

Arjovsky, Martin and Bottou, L´eon. Towards principled
methods for training generative adversarial networks. In
International Conference on Learning Representations,
2017.

Dziugaite, Gintare Karolina, Roy, Daniel M., and Ghahra-
mani, Zoubin.
Training generative neural networks
via maximum mean discrepancy optimization. CoRR,
abs/1505.03906, 2015.

Genevay, Aude, Cuturi, Marco, Peyr´e, Gabriel, and Bach,
Francis. Stochastic optimization for large-scale optimal
transport. In Lee, D. D., Sugiyama, M., Luxburg, U. V.,
Guyon, I., and Garnett, R. (eds.), Advances in Neural In-
formation Processing Systems 29, pp. 3440–3448. Cur-
ran Associates, Inc., 2016.

Goodfellow, Ian J., Pouget-Abadie, Jean, Mirza, Mehdi,
Xu, Bing, Warde-Farley, David, Ozair, Sherjil,
Courville, Aaron, and Bengio, Yoshua. Generative ad-
versarial nets. In Advances in Neural Information Pro-
cessing Systems 27, pp. 2672–2680. Curran Associates,
Inc., 2014.

Gretton, Arthur, Borgwardt, Karsten M., Rasch, Malte J.,
Sch¨olkopf, Bernhard, and Smola, Alexander. A ker-
nel two-sample test. J. Mach. Learn. Res., 13:723–773,
2012. ISSN 1532-4435.

Huszar, Ferenc. How (not) to train your generative model:
CoRR,

Scheduled sampling, likelihood, adversary?
abs/1511.05101, 2015.

Kakutani, Shizuo. Concrete representation of abstract (m)-
spaces (a characterization of the space of continuous
functions). Annals of Mathematics, 42(4):994–1024,
1941.

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for
stochastic optimization. CoRR, abs/1412.6980, 2014.

International Conference on Machine Learning (ICML-
15), pp. 1718–1727. JMLR Workshop and Conference
Proceedings, 2015.

Metz, Luke, Poole, Ben, Pfau, David, and Sohl-Dickstein,
Jascha. Unrolled generative adversarial networks. Corr,
abs/1611.02163, 2016.

Milgrom, Paul and Segal, Ilya. Envelope theorems for ar-
bitrary choice sets. Econometrica, 70(2):583–601, 2002.
ISSN 1468-0262.

Mnih, Volodymyr, Badia, Adri`a Puigdom`enech, Mirza,
Mehdi, Graves, Alex, Lillicrap, Timothy P., Harley, Tim,
Silver, David, and Kavukcuoglu, Koray. Asynchronous
In Proceed-
methods for deep reinforcement learning.
ings of the 33nd International Conference on Machine
Learning, ICML 2016, New York City, NY, USA, June
19-24, 2016, pp. 1928–1937, 2016.

Montavon, Gr´egoire, M¨uller, Klaus-Robert, and Cuturi,
Marco. Wasserstein training of restricted boltzmann ma-
chines. In Lee, D. D., Sugiyama, M., Luxburg, U. V.,
Guyon, I., and Garnett, R. (eds.), Advances in Neural In-
formation Processing Systems 29, pp. 3718–3726. Cur-
ran Associates, Inc., 2016.

M¨uller, Alfred. Integral probability metrics and their gen-
erating classes of functions. Advances in Applied Prob-
ability, 29(2):429–443, 1997.

Neal, Radford M. Annealed importance sampling. Statis-
tics and Computing, 11(2):125–139, April 2001. ISSN
0960-3174.

Nowozin, Sebastian, Cseke, Botond, and Tomioka, Ryota.
f-gan: Training generative neural samplers using varia-
tional divergence minimization. pp. 271–279, 2016.

Radford, Alec, Metz, Luke, and Chintala, Soumith. Unsu-
pervised representation learning with deep convolutional
generative adversarial networks. CoRR, abs/1511.06434,
2015.

Ramdas, Aaditya, Reddi, Sashank J., Poczos, Barn-
abas, Singh, Aarti, and Wasserman, Larry.
On
the high-dimensional power of linear-time kernel two-
sample testing under mean-difference alternatives. Corr,
abs/1411.6314, 2014.

Kingma, Diederik P. and Welling, Max. Auto-encoding

variational bayes. CoRR, abs/1312.6114, 2013.

Li, Yujia, Swersky, Kevin, and Zemel, Rich. Generative
moment matching networks. In Proceedings of the 32nd

Sutherland, Dougal J, Tung, Hsiao-Yu, Strathmann, Heiko,
De, Soumyajit, Ramdas, Aaditya, Smola, Alex, and
Gretton, Arthur. Generative models and model criticism
via optimized maximum mean discrepancy. In Interna-
tional Conference on Learning Representations, 2017.

Wasserstein Generative Adversarial Networks

Tieleman, T. and Hinton, G. Lecture 6.5—RmsProp: Di-
vide the gradient by a running average of its recent mag-
nitude. COURSERA: Neural Networks for Machine
Learning, 2012.

Villani, C´edric.
Grundlehren
Springer, Berlin, 2009.

Optimal Transport: Old and New.
der mathematischen Wissenschaften.

Wu, Yuhuai, Burda, Yuri, Salakhutdinov, Ruslan, and
On the quantitative analy-
CoRR,

Grosse, Roger B.
sis of decoder-based generative models.
abs/1611.04273, 2016.

Yu, Fisher, Zhang, Yinda, Song, Shuran, Seff, Ari, and
Xiao, Jianxiong. LSUN: Construction of a large-scale
image dataset using deep learning with humans in the
loop. Corr, abs/1506.03365, 2015.

Zhao,

Junbo, Mathieu, Michael, and LeCun, Yann.
Energy-based generative adversarial network. Corr,
abs/1609.03126, 2016.

