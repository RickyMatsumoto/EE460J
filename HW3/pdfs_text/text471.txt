Count-BasedExplorationwithNeuralDensityModelsGeorgOstrovski1MarcG.Bellemare1A¨aronvandenOord1R´emiMunos1AbstractBellemareetal.(2016)introducedthenotionofapseudo-count,derivedfromadensitymodel,togeneralizecount-basedexplorationtonon-tabularreinforcementlearning.Thispseudo-countwasusedtogenerateanexplorationbonusforaDQNagentandcombinedwithamixedMonteCarloupdatewassufﬁcienttoachievestateoftheartontheAtari2600gameMon-tezuma’sRevenge.Weconsidertwoquestionsleftopenbytheirwork:First,howimportantisthequalityofthedensitymodelforexploration?Second,whatroledoestheMonteCarloupdateplayinexploration?WeanswertheﬁrstquestionbydemonstratingtheuseofPixelCNN,anad-vancedneuraldensitymodelforimages,tosup-plyapseudo-count.Inparticular,weexaminetheintrinsicdifﬁcultiesinadaptingBellemareetal.’sapproachwhenassumptionsaboutthemodelareviolated.Theresultisamorepracticalandgen-eralalgorithmrequiringnospecialapparatus.WecombinePixelCNNpseudo-countswithdifferentagentarchitecturestodramaticallyimprovethestateoftheartonseveralhardAtarigames.OnesurprisingﬁndingisthatthemixedMonteCarloupdateisapowerfulfacilitatorofexplorationinthesparsestofsettings,includingMontezuma’sRevenge.1.IntroductionExplorationistheprocessbywhichanagentlearnsaboutitsenvironment.Inthereinforcementlearningframework,thisinvolvesreducingtheagent’suncertaintyabouttheenvironment’stransitiondynamicsandattainablerewards.Fromatheoreticalperspective,explorationisnowwell-understood(e.g.Strehl&Littman,2008;Jakschetal.,2010;Osbandetal.,2016),andBayesianmethodshave1DeepMind,London,UK.Correspondenceto:GeorgOstro-vski<ostrovski@google.com>.Proceedingsofthe34thInternationalConferenceonMachineLearning,Sydney,Australia,PMLR70,2017.Copyright2017bytheauthor(s).beensuccessfullydemonstratedinanumberofsettings(Deisenroth&Rasmussen,2011;Guezetal.,2012).Ontheotherhand,practicalalgorithmsforthegeneralcaseremainscarce;fullyBayesianapproachesareusuallyintractableinlargestatespaces,andthecount-basedmethodtypicaloftheoreticalresultsisnotapplicableinthepresenceofvaluefunctionapproximation.Recently,Bellemareetal.(2016)proposedthenotionofpseudo-countasareasonablegeneralizationofthetabu-larsettingconsideredinthetheoryliterature.Thepseudo-countisdeﬁnedintermsofadensitymodelρtrainedonthesequenceofstatesexperiencedbyanagent:ˆN(x)=ρ(x)ˆn(x),whereˆn(x)canbethoughtofasatotalpseudo-countcom-putedfromthemodel’srecodingprobabilityρ0(x),theprobabilityofxcomputedimmediatelyaftertrainingonx.Asapracticalapplicationtheauthorsusedthepseudo-countsderivedfromthesimpleCTSdensitymodel(Belle-mareetal.,2014)toincentivizeexplorationinAtari2600agents.Oneofthemainoutcomesoftheirworkwassub-stantialempiricalprogressontheinfamouslyhardgameMONTEZUMA’SREVENGE.Theirmethodcriticallyhingedonseveralassumptionsregardingthedensitymodel:1)themodelshouldbelearning-positive,i.e.theprobabilityassignedtoastatexshouldincreasewithtraining;2)itshouldbetrainedon-line,usingeachsampleexactlyonce;and3)theeffectivemodelstep-sizeshoulddecayatarateofn−1.PartoftheirempiricalsuccessalsoreliedonamixedMonteCarlo/Q-Learningupdaterule,whichpermittedfastpropagationoftheexplorationbonuses.Inthispaper,wesetouttoanswerseveralresearchques-tionsrelatedtothesemodellingchoicesandassumptions:1.Towhatextentdoesabetterdensitymodelgiverisetobetterexploration?2.Cantheabovemodellingassumptionsberelaxedwithoutsacriﬁcingexplorationperformance?3.WhatroledoesthemixedMonteCarloupdateplayinsuccessfullyincentivizingexploration?Count-BasedExplorationwithNeuralDensityModelsInparticular,weexploretheuseofPixelCNN(vandenOordetal.,2016b;a),astate-of-the-artneuraldensitymodel.Weexaminethechallengesposedbythisapproach:Modelchoice.Performingtwoevaluationsandonemodelupdateateachagentstep(tocomputeρ(x)andρ0(x))canbeprohibitivelyexpensive.Thisrequiresthedesignofasimpliﬁed–yetsufﬁcientlyexpressiveandaccurate–Pix-elCNNarchitecture.Modeltraining.ACTSmodelcannaturallybetrainedfromsequentiallypresented,correlateddatasamples.Traininganeuralmodelinthisonlinefashionrequiresmorecarefulattentiontotheoptimizationproceduretopre-ventoverﬁttingandcatastrophicforgetting(French,1999).Modeluse.Thetheoryofpseudo-countsrequirestheden-sitymodel’srateoflearningtodecayovertime.Optimiza-tionofaneuralmodel,however,imposesconstraintsonthestep-sizeregimewhichcannotbeviolatedwithoutdeterio-ratingeffectivenessandstabilityoftraining.Theconceptofintrinsicmotivationhasmadearecentresur-genceinreinforcementlearningresearch,ingreatpartduetoadissatisfactionwith(cid:15)-greedyandBoltzmannpoli-cies.Ofnote,Tangetal.(2016)maintainanapproximatecountbymeansofhashtablesoverfeatures,whichinthepseudo-countframeworkcorrespondstoahash-basedden-sitymodel.Houthooftetal.(2016)usedasecond-orderTaylorapproximationofthepredictiongaintodriveexplo-rationincontinuouscontrol.Asresearchmovestowardsevermorecomplexenvironments,weexpectthetrendto-wardsmoreintrinsicallymotivatedsolutionstocontinue.2.Background2.1.Pseudo-CountandPredictionGainHerewebrieﬂyintroducenotationandresults,referringthereaderto(Bellemareetal.,2016)fortechnicaldetails.LetρbeadensitymodelonaﬁnitespaceX,andρn(x)theprobabilityassignedbythemodeltoxafterbeingtrainedonasequenceofstatesx1,...,xn.Assumeρn(x)>0forallx,n.Therecodingprobabilityρ0n(x)isthentheprobabilitythemodelwouldassigntoxifitweretrainedonthatsamexonemoretime.Wecallρlearning-positiveifρ0n(x)≥ρn(x)forallx1,...,xn,x∈X.Thepredictiongain(PG)ofρisPGn(x)=logρ0n(x)−logρn(x).(1)Alearning-positiveρimpliesPGn(x)≥0forallx∈X.Forlearning-positiveρ,wedeﬁnethepseudo-countasˆNn(x)=ρn(x)(1−ρ0n(x))ρ0n(x)−ρn(x),derivedfrompostulatingthatasingleobservationofx∈Xshouldleadtoaunitincreaseinpseudo-count:ρn(x)=ˆNn(x)ˆn,ρ0n(x)=ˆNn(x)+1ˆn+1,whereˆnisthepseudo-counttotal.Thepseudo-countgen-eralizestheusualstatevisitationcountfunctionNn(x).Undercertainassumptionsonρn,pseudo-countsgrowapproximatelylinearlywithrealcounts.Crucially,thepseudo-countcanbeapproximatedusingthepredictiongainofthedensitymodel:ˆNn(x)≈(cid:16)ePGn(x)−1(cid:17)−1.Itsmainuseistodeﬁneanexplorationbonus.Weconsiderareinforcementlearning(RL)agentinteractingwithanen-vironmentthatprovidesobservationsandextrinsicrewards(seeSutton&Barto,1998,forathoroughexpositionoftheRLframework).Totherewardatstepnweaddthebonusr+(x):=(ˆNn(x))−1/2,whichincentivizestheagenttotrytore-experiencesur-prisingsituations.Quantitiesrelatedtopredictiongainhavebeenusedforsimilarpurposesintheintrinsicmoti-vationliterature(Lopesetal.,2012),wheretheymeasureanagent’slearningprogress(Oudeyeretal.,2007).Al-thoughthepseudo-countbonusisclosetothepredictiongain,itisasymptoticallymoreconservativeandsupportedbystrongertheoreticalguarantees.2.2.DensityModelsforImagesTheCTSdensitymodel(Bellemareetal.,2014)isbasedonthenamesakealgorithm,ContextTreeSwitching(Ve-nessetal.,2012),aBayesianvariable-orderMarkovmodel.Initssimplestform,themodeltakesasinputa2Dimageandassignstoitaprobabilityaccordingtotheproductoflocation-dependentL-shapedﬁlters,wherethepredictionofeachﬁlterisgivenbyaCTSalgorithmtrainedonpastimages.InBellemareetal.(2016),thismodelwasap-pliedto3-bitgreyscale,42×42downsampledAtari2600frames(Fig.1).TheCTSmodelpresentsadvantagesintermsofsimplicityandperformancebutislimitedinex-pressiveness,scalability,anddataefﬁciency.Inrecentyears,neuralgenerativemodelsforimageshaveachievedimpressivesuccessesintheirabilitytogeneratediverseimagesinvariousdomains(Kingma&Welling,2013;Rezendeetal.,2014;Gregoretal.,2015;Good-fellowetal.,2014).Inparticular,vandenOordetal.(2016b;a)introducedPixelCNN,afullyconvolutionalneu-ralnetworkcomposedofresidualblockswithmultiplica-tivegatingunits,whichmodelspixelprobabilitiescondi-tionalonpreviouspixels(intheusualtop-lefttobottom-rightraster-scanorder)byusingmaskedconvolutionﬁl-Count-BasedExplorationwithNeuralDensityModelsFigure1.Atariframepreprocessing(Bellemareetal.,2016).ters.Thismodelachievedstate-of-the-artmodellingperfor-manceonstandarddatasets,pairedwiththecomputationalefﬁciencyofaconvolutionalfeed-forwardnetwork.2.3.Multi-StepRLMethodsAdistinguishingfeatureofreinforcementlearningisthattheagent“learnsonthebasisofinterimestimates”(Sutton,1996).Forexample,theQ-LearningupdateruleisQ(x,a)←Q(x,a)+α[r(x,a)+γmaxa0Q(x0,a0)−Q(x,a)]|{z}δ(x,a),linkingtherewardrandnext-statevaluefunctionQ(x0,a0)tothecurrentstatevaluefunctionQ(x,a).Thisparticularformisthestochasticupdaterulewithstep-sizeαandin-volvestheTD-errorδ.Intheapproximatereinforcementlearningsetting,suchaswhenQ(x,a)isrepresentedbyaneuralnetwork,thisupdateisconvertedintoalosstobeminimized,mostcommonlythesquaredlossδ2(x,a).Itiswellknownthatbetterperformance,bothintermsoflearningefﬁciencyandapproximationerror,isattainedbymulti-stepmethods(Sutton,1996;Tsitsiklis&vanRoy,1997).Thesemethodsinterpolatebetweenone-stepmeth-ods(Q-Learning)andtheMonte-CarloupdateQ(x,a)←Q(x,a)+α"∞Xt=0γtr(xt,at)−Q(x,a)#|{z}δMC(x,a),wherex0,a0,x1,a1,...isasamplepaththroughtheen-vironmentbeginningin(x,a).ToachievetheirsuccessonthehardestAtari2600games,Bellemareetal.(2016)usedthemixedMonte-Carloupdate(MMC)Q(x,a)←Q(x,a)+α[(1−β)δ(x,a)+βδMC(x,a)],withβ∈[0,1].Thischoicewasmadefor“computa-tionalandimplementationalsimplicity”,andisaparticu-larlycoarsemulti-stepmethod.Abettermulti-stepmethodistherecentRetrace(λ)algorithm(Munosetal.,2016).Retrace(λ)usesaproductoftruncatedimportancesam-plingratiosc1,c2,...toreplaceδwiththeerrortermδRETRACE(x,a):=∞Xt=0γt tYs=1cs!δ(xt,at),effectivelymixinginTD-errorsfromallfuturetimesteps.Munosetal.showedthatRetrace(λ)issafe(doesnotdi-vergewhentrainedondatafromanarbitrarybehaviourpol-icy),andefﬁcient(makesthemostofmulti-stepreturns).3.UsingPixelCNNforExplorationAsmentionedintheIntroduction,thetheoryofusingden-sitymodelsforexplorationmakesseveralassumptionsthattranslateintoconcreterequirementsforanimplementation:(a)Thedensitymodelshouldbetrainedcompletelyon-line,i.e.exactlyonceoneachstateexperiencedbytheagent,inthegivensequentialorder.(b)Thepredictiongain(PG)shoulddecayataraten−1toensurethatpseudo-countsgrowapproximatelylin-earlywithrealcounts.(c)Thedensitymodelshouldbelearning-positive.Simultaneously,apartlycompetingsetofrequirementsareposedbythepracticalitiesoftraininganeuraldensitymodelandusingitaspartofanRLagent:(d)Forstability,efﬁciency,andtoavoidcatastrophicfor-gettinginthecontextofadriftingdatadistribution,itisadvantageoustotrainaneuralmodelinmini-batches,drawnrandomlyfromadiversedataset.(e)Foreffectivetraining,acertainoptimizationregime(e.g.aﬁxedlearningrateschedule)hastobefollowed.(f)Thedensitymodelmustbecomputationallylightweight,toallowcomputingthePG(twomodelevaluationsandoneupdate)aspartofeverytrainingstepofanRLagent.WeinvestigatehowtobestresolvethesetensionsinthecontextoftheArcadeLearningEnvironment(Bellemareetal.,2013),asuiteofbenchmarkAtari2600games.3.1.DesigningaSuitableDensityModelDrivenby(f)andaimingforanagentwithcomputationalperformancecomparabletoDQN,wedesignaslimvari-antofthePixelCNNnetwork.Itscoreisastackof2gatedresidualblockswith16featuremaps(comparedto15resid-ualblockswith128featuremapsinvanillaPixelCNN).AswasdonewiththeCTSmodel,imagesaredownsampledto42×42andquantizedto3-bitgreyscale.SeeAppendixAfortechnicaldetails.Count-BasedExplorationwithNeuralDensityModelsFigure2.Left:PixelCNNloglossonFREEWAY,whentrainedonline,onarandompermutation(singleuseofeachframe)oronrandomlydrawnsamples(withreplacement,potentiallyusingsameframemultipletimes)fromthestatesequence.Tosimulatetheeffectofnon-stationarity,theagent’spolicychangesevery4Kupdates.Alltrainingmethodsshowqualitativelysimilarlearningprogressandstability.Middle:PixelCNNloglossoverﬁrst6KtrainingframesonPONG.Verticaldashedlinesindicateepisodeends.Thecoincidinglossspikesarethedensitymodel’s‘surprise’uponobservingthedistinctivegreenframethatsometimesoccursattheepisodestart.Right:DQN-PixelCNNtrainingperformanceonMONTEZUMA’SREVENGEaswevarylearningrateandPGdecayschedules.Figure3.Modellossaveragedover10Kframes,after1Mtrainingframes,forconstant,n−1,andn−1/2learningrateschedules.Thesmallestlossisachievedbyaconstantlearningrateof10−3.3.2.TrainingtheDensityModelInsteadofusingrandomizedmini-batches,wetrainthedensitymodelcompletelyonlineonthesequenceofexperi-encedstates.Empiricallywefoundthatwithminortuningofoptimizationhyper-parameterswecouldtrainthemodelasrobustlyonatemporallycorrelatedsequenceofstatesasonasequencewithrandomizedorder(Fig.2(left)).Besidessatisfyingthetheoreticalrequirement(a),com-pletelyonlinetrainingofthedensitymodelhastheadvan-tagethatρ0n=ρn+1,sothatthemodelupdateperformedforcomputingthePGneednotbereverted1.Anothermoresubtlereasonforavoidingmini-batchup-datesofthedensitymodel(despite(d))isapracticalop-timizationissue.The(necessarilyonline)computationofthePGinvolvesamodelupdateandhencetheuseofanoptimizer.Advancedoptimizersusedwithdeepneuralnet-works,liketheRMSPropoptimizer(Tieleman&Hinton,2012)usedinthiswork,arestateful,trackingrunningav-eragesofe.g.meanandvarianceofthemodelparameters.Ifthemodelisadditionallytrainedfrommini-batches,thetwostreamsofupdatesmayshowdifferentstatisticalchar-1TheCTSmodelallowsqueryingthePGcheaply,withoutin-curringanactualupdateofmodelparameters.acteristics(e.g.differentgradientmagnitudes),invalidatingtheassumptionsunderlyingtheoptimizationalgorithmandleadingtoslowerorunstabletraining.Todetermineasuitableonlinelearningrateschedule,wetrainthemodelonasequenceof1Mframesofexperienceofarandom-policyagent.Wecomparethelossachievedbytrainingproceduresfollowingconstantordecayinglearn-ingrateschedules,seeFig.3.Thelowestﬁnaltraininglossisachievedbyaconstantlearningrateof0.001orade-cayinglearningrateof0.1·n−1/2.Wesettledourchoiceontheconstantlearningratescheduleasitshowedgreaterrobustnesswithrespecttothechoiceofinitiallearningrate.PixelCNNrapidlylearnsasensibledistributionoverstatespace.Fig.2(left)showsthemodel’slossdecayingasitlearnstoexploitimageregularities.Spikesinitslossfunctionquicklystarttocorrespondtovisuallymeaning-fulevents,suchasthestartsofepisodes(Fig.2(middle)).Avideoofearlydensitymodeltrainingisprovidedinhttp://youtu.be/T6iaa8Z4eyE.Figure4.Samplesafter25Ksteps.Left:CTS,right:PixelCNN.3.3.ComputingthePseudo-CountFromtheprevioussectionweobtainaparticularlearningrateschedulethatcannotbearbitrarilymodiﬁedwithoutdeterioratingthemodel’strainingperformanceorstability.ToachievetherequiredPGdecay(b),weinsteadreplacePGnbycn·PGnwithasuitablydecayingsequencecn.Count-BasedExplorationwithNeuralDensityModelsInexperimentscomparingactualagentperformanceweempiricallydeterminedthatinfacttheconstantlearningrate0.001,pairedwithaPGdecaycn=c·n−1/2,obtainsthebestexplorationresultsonhardexplorationgameslikeMONTEZUMA’SREVENGE,seeFig.2(right).Weﬁndthemodeltoberobustacross1-2ordersofmagnitudeforthevalueofc,andinformallydeterminec=0.1tobeasen-sibleconﬁgurationforachievinggoodresultsonabroadrangeofAtari2600games(seealsoSection7).Regarding(c),itishardtoensurelearning-positivenessforadeepneuralmodel,andanegativePGcanoccurwhen-evertheoptimizer‘overshoots’alocallossminimum.Asaworkaround,wethresholdthePGvalueat0.Tosumma-rize,thecomputedpseudo-countisˆNn(x)=(cid:16)exp(cid:16)c·n−1/2·(PGn(x))+(cid:17)−1(cid:17)−1.4.ExplorationinAtari2600GamesHavingdescribedourpseudo-countfriendlyadaptationofPixelCNN,wenowstudyitsperformanceonAtarigames.Tothisendweaugmenttheenvironmentrewardwithapseudo-countexplorationbonus,yieldingthecombinedre-wardr(x,a)+(ˆNn(x))−1/2.Asusualforneuralnetwork-basedagents,weensurethetotalrewardliesin[−1,1]byclippinglargervalues.4.1.DQNwithPixelCNNExplorationBonusOurﬁrstsetofexperimentsprovidesthePixelCNNexplo-rationbonustoaDQNagent(Mnihetal.,2015)2.Ateachagentstep,thedensitymodelreceivesasingleframe,withwhichitsimultaneouslyupdatesitsparametersandoutputsthePG.WerefertothisagentasDQN-PixelCNN.TheDQN-CTSagentwecompareagainstisderivedfromtheonein(Bellemareetal.,2016).Forbettercompara-bility,itistrainedinthesameonlinefashionasDQN-PixelCNN,i.e.thePGiscomputedwheneverwetrainthedensitymodel.Bycontrast,theoriginalDQN-CTSqueriedthePGattheendofeachepisode.Unlessstatedotherwise,wealwaysusethemixedMonteCarloupdate(MMC)fortheintrinsicallymotivatedagents3,butregularQ-LearningforthebaselineDQN.Fig.5showstrainingcurvesofDQNcomparedtoDQN-2UnlikeBellemareetal.weuseregularQ-LearninginsteadofDoubleQ-Learning(vanHasseltetal.,2016),asourearlyexper-imentsshowednosigniﬁcantadvantageofDoubleDQNwiththePixelCNN-basedexplorationreward.3TheuseofMMCinareplay-basedagentposesaminorcom-plication,astheMCreturnisnotavailableforreplayuntiltheendofanepisode.Forsimplicity,inourimplementationwedisregardthisdetailandsettheMCreturnto0fortransitionsfromthemostrecentepisode.Figure5.DQN,DQN-CTSandDQN-PixelCNNonhardexplo-rationgames(top)andeasierones(bottom).Figure6.Improvements(in%ofAUC)ofDQN-PixelCNNandDQN-CTSoverDQNin57Atarigames.Annotationsindicatethenumberofhardexplorationgameswithpositive(right)andnegative(left)improvement,respectively.CTSandDQN-PixelCNN.OnthefamousMONTEZUMA’SREVENGE,bothintrinsicallymotivatedagentsvastlyout-performthebaselineDQN.Onotherhardexplorationgames(PRIVATEEYE;orVENTURE,appendixFig.15),DQN-PixelCNNachievesstateoftheartresults,substan-tiallyoutperformingDQNandDQN-CTS.Theothertwogamesshown(ASTEROIDS,BERZERK)poseeasierexplo-rationproblems,wheretherewardbonusshouldnotpro-videlargeimprovementsandmayhaveanegativeeffectbyskewingtherewardlandscape.Here,DQN-PixelCNNbe-havesmoregracefullyandstilloutperformsDQN-CTS.Wehypothesizethisisduetoaqualitativedifferencebetweenthemodels,seeSection5.OverallPixelCNNprovidestheDQNagentwithalargeradvantagethanCTS,andoftenacceleratesorstabilizestrainingevenwhennotaffectingpeakperformance.OutofCount-BasedExplorationwithNeuralDensityModels57Atarigames,DQN-PixelCNNoutperformsDQN-CTSin52gamesbymaximumachievedscore,and51byAUC(methodologyinAppendixB).SeeFig.6forahighlevelcomparison(appendixFig.15forfulltraininggraphs).Thegreatestgainsfromusingeitherexplorationbonusareob-servedingamescategorizedashardexplorationgamesinthe‘taxonomyofexploration’in(Bellemareetal.,2016,reproducedinAppendixD),speciﬁcallyinthemostchal-lengingsparserewardgames(e.g.MONTEZUMA’SRE-VENGE,PRIVATEEYE,VENTURE).4.2.AMulti-StepRLAgentwithPixelCNNEmpiricalpractitionersknowthattechniquesbeneﬁcialforoneagentarchitectureoftencanbedetrimentalforadif-ferentalgorithm.TodemonstratethewideapplicabilityofthePixelCNNexplorationbonus,wealsoevaluateitwiththemorerecentReactoragent4(Gruslysetal.,2017).Thisreplay-basedactor-criticagentrepresentsitspolicyandvaluefunctionbyarecurrentneuralnetworkand,cru-cially,usesthemulti-stepRetrace(λ)algorithmforpolicyevaluation,replacingtheMMCweuseinDQN-PixelCNN.Toreduceimpactoncomputationalefﬁciencyofthisagent,wesub-sampleintrinsicrewards:weperformupdatesofthePixelCNNmodelandcomputetherewardbonuson(randomlychosen)25%ofallsteps,leavingtheagent’sre-wardunchangedonothersteps.WeusethesamePGdecayscheduleof0.1n−1/2,withnthenumberofmodelupdates.Figure7.Reactor/Reactor-PixelCNNandDQN/DQN-PixelCNNtrainingperformance(averagedover3seeds).TrainingcurvesfortheReactor/Reactor-PixelCNNagentcomparedtoDQN/DQN-PixelCNNareshowninFig.7.ThebaselineReactoragentissuperiortotheDQNagent,obtaininghigherscoresandlearningfasterinabout50outof57games.ItisfurtherimprovedonalargefractionofgamesbythePixelCNNexplorationreward,seeFig.8(fulltraininggraphsinappendixFig.16).Theeffectoftheexplorationbonusisratheruniform,yield-ingimprovementsonabroadrangeofgames.Inparticu-4Theexactagentvariantisreferredtoas‘β-LOO’withβ=1.lar,Reactor-PixelCNNenjoysbettersampleefﬁciency(intermsofareaunderthecurve,AUC)thanvanillaReactor.Wehypothesizethat,likeotherpolicygradientalgorithms,Reactorgenerallysuffersfromweakerexplorationthanitsvalue-basedcounterpartDQN.Thisaspectismuchhelpedbytheexplorationbonus,boostingtheagent’ssampleefﬁ-ciencyinmanyenvironments.Figure8.Improvements(in%ofAUC)ofReactor-PixelCNNoverReactorin57Atarigames.However,onhardexplorationgameswithsparserewards,Reactorseemsunabletomakefulluseoftheexplorationbonus.Webelievethisisbecause,inverysparsesettings,thepropagationofrewardinformationacrosslonghori-zonsbecomescrucial.TheMMCtakesoneextremeofthisview,directlylearningfromtheobservedreturns.TheRetrace(λ)algorithm,ontheotherhand,hasaneffectivehorizonwhichdependsonλand,critically,thetruncatedimportancesamplingratio.Thisratioresultsinthediscard-ingoftrajectorieswhichareoff-policy,i.e.unlikelyunderthecurrentpolicy.WehypothesizethattheverygoaloftheRetrace(λ)algorithmtolearncautiouslyiswhatpreventsitfromtakingfulladvantageoftheexplorationbonus!5.QualityoftheDensityModelPixelCNNcanbeexpectedtobemoreexpressiveandaccu-ratethanthelessadvancedCTSmodel,andindeed,sam-plesgeneratedaftertrainingaresomewhathigherquality(Fig.4).However,wearenotusingthegenerativefunctionofthemodelswhencomputinganexplorationbonus,andabettergenerativemodeldoesnotnecessarilygiverisetobetterprobabilityestimates(Theisetal.,2016).Figure9.PGonMONTEZUMA’SREVENGE(logscale).Count-BasedExplorationwithNeuralDensityModelsInFig.9wecomparethePGproducedbythetwomod-elsthroughout5Ktrainingsteps.PixelCNNconsistentlyproducesPGslowerthanCTS.Moreimportantly,itsPGsaresmoother,exhibitinglessvariancebetweensucces-sivestates,whileshowingmorepronouncedpeaksatcer-taininfrequentevents.Thisyieldsarewardbonusthatislessharmfulineasyexplorationgames,whileprovidingastrongsignalinthecaseofnovelorrareevents.AnotherdistinguishingfeatureofPixelCNNisitsnon-decayingstep-size.Theper-stepPGnevercompletelyvan-ishes,asthemodeltracksthemostrecentdata.Thispro-videsanunexpectedbeneﬁt:theagentremainsmildlysur-prisedbysigniﬁcantstatechanges,e.g.switchingroomsinMONTEZUMA’SREVENGE.Thesepersistentrewardsactasmilestonesthattheagentlearnstoreturnto.Thisisil-lustratedinFig.10,depictingtheintrinsicrewardoverthecourseofanepisode.Theagentroutinelyrevisitstheright-handsideofthetorchroom,notbecauseitleadstorewardbutjustto“takeinthesights”.Avideooftheepisodeisprovidedathttp://youtu.be/232tOUPKPoQ.5Figure10.IntrinsicrewardinMONTEZUMA’SREVENGE.Lastly,PixelCNN’sconvolutionalnatureisexpectedtobebeneﬁcialforitssampleefﬁciency.InAppendixCwecom-paretoaconvolutionalCTSandconﬁrmthatthisexplainspart,butnotallofPixelCNN’sadvantageovervanillaCTS.6.ImportanceoftheMonteCarloReturnLikeforDQN-CTS,thesuccessofDQN-PixelCNNhingesontheuseofthemixedMonteCarloupdate.Thetransientandvanishingnatureoftheexplorationrewardsrequiresthelearningalgorithmtolatchontotheserapidly.TheMMCservesthisendasasimplemulti-stepmethod,help-ingtopropagaterewardinformationfaster.AnadditionalbeneﬁtliesinthefactthattheMonteCarloreturnhelpsbridginglonghorizonsinenvironmentswhererewardsarefarapartandencounteredrarely.Ontheotherhand,itis5AnotheragentvideoonthegamePRIVATEEYEcanbefoundathttp://youtu.be/kNyFygeUa2E.Figure11.Top:gameswhereMMCcompletelyexplainstheim-proved/decreasedperformanceofDQN-PixelCNNcomparedtoDQN.Bottom-left:MMCandPixelCNNshowadditivebeneﬁts.Bottom-right:hardexploration,sparserewardgame–onlycom-biningMMCandPixelCNNbonusachievestrainingprogress.importanttonotethattheMonteCarloreturn’son-policynatureincreasesvarianceinthelearningalgorithm,andcanpreventthealgorithm’sconvergencetotheoptimalpolicywhentrainingoff-policy.Itcanthereforebeexpectedtoadverselyaffecttrainingperformanceinsomegames.TodistilltheeffectoftheMMConperformance,wecom-pareallfourcombinationsofDQNwith/withoutPixelCNNexplorationbonusandwith/withoutMMC.Fig.11showstheperformanceofthesefouragentvariants(graphsforallgamesareshowninFig.17).Thesegameswerepickedtoillustrateseveralcommonlyoccurringcases:•MMCspeedsuptrainingandimprovesﬁnalperfor-mancesigniﬁcantly(examples:BANKHEIST,TIMEPILOT).Inthesegames,MMCaloneexplainsmostoralloftheimprovementofDQN-PixelCNNoverDQN.•MMChurtsperformance(examples:MS.PAC-MAN,BREAKOUT).Heretoo,MMCaloneexplainsmostofthedifferencebetweenDQN-PixelCNNandDQN.•MMCandPixelCNNrewardbonushaveacompound-ingeffect(example:H.E.R.O.).Mostimportantly,thesituationisratherdifferentwhenwerestrictourattentiontothehardestexplorationgameswithsparserewards.HerethebaselineDQNagentfailstomakeanytrainingprogress,andneitherMonteCarloreturnnortheexplorationbonusaloneprovideanysigniﬁcantbeneﬁt.Theircombinationhowevergrantstheagentrapidtrainingprogressandallowsittoachievehighperformance.Oneeffectoftheexplorationbonusinthesegamesistoprovideadenserrewardlandscape,enablingtheagenttolearnmeaningfulpolicies.Duetothetransientnatureoftheexplorationbonus,theagentneedstobeabletolearnfromthisrewardsignalfasterthanregularone-stepmeth-odsallow,andMMCprovestobeaneffectivesolution.Count-BasedExplorationwithNeuralDensityModelsFigure12.DQN-PixelCNN,hardexplorationgames,differentPGscalesc·n−1/2·PGn(c=0.1,1,10)(5seedseach).7.PushingtheLimitsofIntrinsicMotivationInthissectionweexploretheideaofa‘maximallycurious’agent,whoserewardfunctionisdominatedbytheexplo-rationbonus.ForthatweincreasethePGscale,previouslychosenconservativelytoavoidadverseeffectsoneasyex-plorationgames.Fig.12showsDQN-PixelCNNperformanceonthehardestexplorationgameswhenthePGscaleisincreasedby1-2ordersofmagnitude.Thealgorithmseemsfairlyrobustacrossawiderangeofscales:themaineffectofincreasingthisparameteristotradeoffexploration(seekingmaximalreward)withexploitation(optimizingthecurrentpolicy).Asexpected,ahigherPGscaletranslatestostrongerex-ploration:severalrunsobtainrecordpeakscores(900inGRAVITAR,6,600inMONTEZUMA’SREVENGE,39,000inPRIVATEEYE,1,500inVENTURE)surpassingthestateoftheartbyasubstantialmargin(forpreviouslypublishedresults,seeAppendixD).Aggressivescalingspeedsuptheagent’sexplorationandachievespeakperformancerapidly,butcanalsodeteriorateitsstabilityandlong-termperfor-mance.Notethatinpractice,becauseofthenon-decayingstep-sizethePGdoesnotvanish.Afterrewardclipping,anoverlyinﬂatedexplorationbonuscanthereforebecomeessentiallyconstant,nolongerprovidingausefulintrinsicmotivationsignaltotheagent.Anotherwayofcreatinganentirelycuriosity-drivenagentistoignoretheenvironmentrewardaltogetherandtrainbasedontheexplorationrewardonly,seeFig.13.Remark-ably,thecuriositysignalaloneissufﬁcienttotrainahigh-performingagent(measuredbyenvironmentreward!).Itisworthnotingthatagentswithexplorationbonusseemto‘neverstopexploring’:fordifferentseeds,theagentsmakelearningprogressatverydifferenttimesduringtrain-ing,aqualitativedifferencetovanillaDQN.8.ConclusionWedemonstratedtheuseofPixelCNNforexplorationandshowedthatitsgreateraccuracyandexpressivenesstrans-lateintoamoreusefulexplorationbonusthanthatobtainedfrompreviousmodels.Whilethecurrenttheoryofpseudo-Figure13.DQN-PixelCNNtrainedfromintrinsicrewardonly(3seedsforeachconﬁguration).countsputsstringentrequirementsonthedensitymodel,wehaveshownthatPixelCNNcanbeusedinasimplerandmoregeneralsetup,andcanbetrainedcompletelyonline.Italsoprovestobewidelycompatiblewithbothvalue-functionandpolicy-basedRLalgorithms.InadditiontopushingthestateoftheartonthehardestexplorationproblemsamongtheAtari2600games,Pixel-CNNimprovesspeedoflearningandstabilityofbaselineRLagentsacrossawiderangeofgames.Thequalityofitsrewardbonusisevidencedbythefactthatonsparserewardgames,thissignalalonesufﬁcestolearntoachievesigniﬁ-cantscores,creatingatrulyintrinsicallymotivatedagent.OuranalysisalsorevealstheimportanceoftheMonteCarloreturnforeffectiveexploration.Thecomparisonwithmoresophisticatedbutﬁxed-horizonmulti-stepmethodsshowsthatitssigniﬁcanceliesbothinfasterlearninginthecontextofausefulbuttransientrewardfunction,aswellasbridgingrewardgapsinenvironmentswhereextrinsicandintrinsicrewardsare,orquicklybecome,extremelysparse.Count-BasedExplorationwithNeuralDensityModelsAcknowledgementsTheauthorsthankTomSchaul,OlivierPietquin,IanOs-band,SriramSrinivasan,TejasKulkarni,AlexGraves,CharlesBlundell,andShimonWhitesonforinvaluablefeedbackontheideaspresentedhere,andAudrunasGruslysespeciallyforprovidingtheReactoragent.ReferencesBellemare,Marc,Veness,Joel,andTalvitie,Erik.Skipcontexttreeswitching.ProceedingsoftheInternationalConferenceonMachineLearning,2014.Bellemare,MarcG.,Naddaf,Yavar,Veness,Joel,andBowling,Michael.Thearcadelearningenvironment:Anevaluationplatformforgeneralagents.JournalofArtiﬁcialIntelligenceResearch,47:253–279,2013.Bellemare,MarcG.,Srinivasan,Sriram,Ostrovski,Georg,Schaul,Tom,Saxton,David,andMunos,R´emi.Uni-fyingcount-basedexplorationandintrinsicmotivation.AdvancesinNeuralInformationProcessingSystems,2016.Deisenroth,MarcP.andRasmussen,CarlE.PILCO:Amodel-basedanddata-efﬁcientapproachtopolicysearch.InProceedingsoftheInternationalConferenceonMachineLearning,2011.French,RobertM.Catastrophicforgettinginconnectionistnetworks.Trendsincognitivesciences,3(4):128–135,1999.Goodfellow,Ian,Pouget-Abadie,Jean,Mirza,Mehdi,Xu,Bing,Warde-Farley,David,Ozair,Sherjil,Courville,Aaron,andBengio,Yoshua.Generativeadversarialnets.InAdvancesinNeuralInformationProcessingSystems,2014.Gregor,Karol,Danihelka,Ivo,Graves,Alex,Rezende,Danilo,andWierstra,Daan.Draw:Arecurrentneuralnetworkforimagegeneration.InProceedingsoftheIn-ternationalConferenceonMachineLearning,2015.Gruslys,Audrunas,Azar,MohammadGheshlaghi,Belle-mare,MarcG.,andMunos,R´emi.TheReactor:Asample-efﬁcientactor-criticarchitecture.arXivpreprintarXiv:1704.04651,2017.Guez,Arthur,Silver,David,andDayan,Peter.Efﬁcientbayes-adaptivereinforcementlearningusingsample-basedsearch.InAdvancesinNeuralInformationPro-cessingSystems,2012.Houthooft,Rein,Chen,Xi,Duan,Yan,Schulman,John,DeTurck,Filip,andAbbeel,Pieter.Variationalinfor-mationmaximizingexploration.InAdvancesinNeuralInformationProcessingSystems(NIPS),2016.Jaksch,Thomas,Ortner,Ronald,andAuer,Peter.Near-optimalregretboundsforreinforcementlearning.Jour-nalofMachineLearningResearch,11:1563–1600,2010.Kingma,DiederikP.andWelling,Max.Auto-encodingvariationalbayes.InProceedingsoftheInternationalConferenceonLearningRepresentations,2013.Lopes,Manuel,Lang,Tobias,Toussaint,Marc,andOudeyer,Pierre-Yves.Explorationinmodel-basedre-inforcementlearningbyempiricallyestimatinglearningprogress.InAdvancesinNeuralInformationProcessingSystems,2012.Mnih,Volodymyr,Kavukcuoglu,Koray,Silver,David,Rusu,AndreiA,Veness,Joel,Bellemare,MarcG.,Graves,Alex,Riedmiller,Martin,Fidjeland,AndreasK.,Ostrovski,Georg,etal.Human-levelcontrolthroughdeepreinforcementlearning.Nature,518(7540):529–533,2015.Munos,R´emi,Stepleton,Tom,Harutyunyan,Anna,andBellemare,MarcG.Safeandefﬁcientoff-policyrein-forcementlearning.InAdvancesinNeuralInformationProcessingSystems,2016.Osband,Ian,vanRoy,Benjamin,andWen,Zheng.Gen-eralizationandexplorationviarandomizedvaluefunc-tions.InProceedingsoftheInternationalConferenceonMachineLearning,2016.Oudeyer,Pierre-Yves,Kaplan,Fr´ed´eric,andHafner,Ver-enaV.Intrinsicmotivationsystemsforautonomousmentaldevelopment.IEEETransactionsonEvolution-aryComputation,11(2):265–286,2007.Rezende,DaniloJimenez,Mohamed,Shakir,andWier-stra,Daan.Stochasticbackpropagationandapproximateinferenceindeepgenerativemodels.InProceedingsofTheInternationalConferenceonMachineLearning,2014.Strehl,AlexanderL.andLittman,MichaelL.Ananalysisofmodel-basedintervalestimationforMarkovdecisionprocesses.JournalofComputerandSystemSciences,74(8):1309–1331,2008.Sutton,RichardS.Generalizationinreinforcementlearn-ing:Successfulexamplesusingsparsecoarsecoding.InAdvancesinNeuralInformationProcessingSystems,1996.Sutton,RichardS.andBarto,AndrewG.Reinforcementlearning:Anintroduction.MITPress,1998.Tang,Haoran,Houthooft,Rein,Foote,Davis,Stooke,Adam,Chen,Xi,Duan,Yan,Schulman,John,DeTurck,Count-BasedExplorationwithNeuralDensityModelsFilip,andAbbeel,Pieter.#Exploration:Astudyofcount-basedexplorationfordeepreinforcementlearn-ing.arXivpreprintarXiv:1611.04717,2016.Theis,Lucas,vandenOord,A¨aron,andBethge,Matthias.Anoteontheevaluationofgenerativemodels.InPro-ceedingsoftheInternationalConferenceonLearningRepresentations,2016.Tieleman,TijmenandHinton,Geoffrey.RMSProp:dividethegradientbyarunningaverageofitsrecentmagni-tude.COURSERA.Lecture6.5ofNeuralNetworksforMachineLearning,2012.Tsitsiklis,JohnN.andvanRoy,Benjamin.Ananalysisoftemporal-differencelearningwithfunctionapproxima-tion.IEEETransactionsonAutomaticControl,42(5):674–690,1997.vandenOord,Aaron,Kalchbrenner,Nal,Espeholt,Lasse,Vinyals,Oriol,Graves,Alex,etal.ConditionalimagegenerationwithPixelCNNdecoders.InAdvancesinNeuralInformationProcessingSystems,2016a.vandenOord,Aaron,Kalchbrenner,Nal,andKavukcuoglu,Koray.Pixelrecurrentneuralnet-works.InProceedingsoftheInternationalConferenceonMachineLearning,2016b.vanHasselt,Hado,Guez,Arthur,andSilver,David.DeepreinforcementlearningwithDoubleQ-learning.InPro-ceedingsoftheAAAIConferenceonArtiﬁcialIntelli-gence,2016.Veness,Joel,Ng,KeeSiong,Hutter,Marcus,andBowling,MichaelH.Contexttreeswitching.InProceedingsoftheDataCompressionConference,2012.Wang,Ziyu,Schaul,Tom,Hessel,Matteo,vanHasselt,Hado,Lanctot,Marc,anddeFreitas,Nando.Duelingnetworkarchitecturesfordeepreinforcementlearning.InProceedingsofThe33rdInternationalConferenceonMachineLearning,pp.1995–2003,2016.