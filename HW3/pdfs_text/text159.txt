Supplementary Material
Random Feature Expansions for Deep Gaussian Processes

Kurt Cutajar 1 Edwin V. Bonilla 2 Pietro Michiardi 1 Maurizio Filippone 1

A. Additional Experiments

Using the experimental set-up described in Section 4, Figure 1 demonstrates how the competing models perform with
regards to the RMSE (or error rate) and MNLL metric when two hidden layers are incorporated into the competing models.
The results follow a similar progression to those reported in Figure 3 of the main paper. The DGP-ARC and DGP-RBF
models both continue to perform well after introducing this additional layer. However, the results for the regularized DNN
are notably inferior, and the degree of overﬁtting is also much greater. To this end, the MNLL obtained for the MNIST
dataset is not shown in the plot as it was vastly inferior to the values obtained using the other methods. DGP-EP was also
observed to have low scalability in this regard whereby it was not possible to obtain sensible results for the MNIST dataset
using this conﬁguration.

REGRESSION

CLASSIFICATION

Powerplant
(n = 9568, d=4)

RMSE

Protein
(n = 45730, d=9)
RMSE

Spam
(n = 4061, d=57)
Error rate

EEG
(n = 14979, d=14)
Error rate

MNIST
(n = 60000, d=784)
Error rate

0:5

0:4

0:3

0:2

1

0:5

2

0

2

0:85

0:8

0:75

0:7

0:65

1:2

1:1

0:4

0:3

0:2

0:1

0

1

4

2

0

0:4

0:2

0

2

0:6

0:4

0:2

0:3

0:2

0:1

0

3

1

0:8

0:6

0:4

0:2

0

3

2:5
3
log10(sec)

MNLL

3:5

2

2:5

3

3:5

1:5

2

2:5

3

3:5

log10(sec)

MNLL

log10(sec)

MNLL

2:5

3

log10(sec)

3:5

MNLL

3:5

4
log10(sec)

4:5

MNLL

2:5
3
log10(sec)

1

2

3:5

2:5

3

log10(sec)

3:5

1

1:5

2

2:5

3

3:5

2

log10(sec)

2:5

3

log10(sec)

3:5

3:5

4
log10(sec)

4:5

Figure1. Progression of RMSE and MNLL over time for competing models. Results are shown for conﬁgurations having 2 hidden layers.
There is no plot for DGP-EP on MNIST because the model did not produce sensible results within the allocated time.

In Section 3.3, we outlined the different strategies for treating Ω, namely ﬁxing them or treating them variationally, where
we observed that the constructed DGP model appears to work best when these are treated variationally while ﬁxing the
randomness in their computation throughout the learning process (VAR-FIXED). In Figures 2 and 3, we compare these three
approaches on the complete set of datasets reported in the main experiments for one and two hidden layers, respectively.
Once again, we conﬁrm that the performance obtained using the VAR-FIXED strategy yields more consistent results than the
alternatives. This is especially pertinent to the classiﬁcation datasets, where the obtained error rate is markedly superior.
However, the variation of the model constructed with the ARC-COSINE kernel and optimized using VAR-FIXED appears to
be susceptible to some overﬁtting for higher dimensional datasets (SPAM and MNIST), which is expected given that we are
optimizing several covariance parameters (ARD). This would motivate trying to be variational about (cid:2) too.

1Department of Data Science, EURECOM, France
2School of Computer Science and Engineering, University of New South Wales, Australia

dgp-rbfdgp-arcdgp-epdnnvar-gp0:8

0:7

1:2

1:1

1

0:85

0:8

0:75

0:7

0:65

1:2

1:1

1

0:5

0:4

0:3

0:2

2

1

0:5

0

2

0:5

0:4

0:3

0:2

1

0:5

2

0

2

0:1

0:05

1

0:8

0:6

0:4

0:2

0:4

0:3

0:2

0:1

1

0:8

0:6

0:4

0:2

Random Feature Expansions for Deep Gaussian Processes

REGRESSION

CLASSIFICATION

Powerplant
(n = 9568, d=4)

RMSE

Protein
(n = 45730, d=9)
RMSE

Spam
(n = 4061, d=57)

Error rate

EEG
(n = 14979, d=14)
Error rate

MNIST
(n = 60000, d=784)
Error rate

3:5

2

2:5

3

3:5

1

1:5

2:5
3
log10(sec)

MNLL

log10(sec)

MNLL

3

3:5

0

2

2

2:5

log10(sec)

MNLL

2:5

3

log10(sec)

3:5

MNLL

3:5

4
log10(sec)

4:5

MNLL

0:4

0:2

0:6

0:4

0:2

0:4

0:2

0

2

0:6

0:4

0:2

0:15

0:1

0:05

0

3

1

0:5

0

3

8

6

4

2

0

0:3

0:2

0:1

0

3

2:5
3
log10(sec)

3:5

2

2:5

3

log10(sec)

3:5

1

1:5

2

2:5

log10(sec)

2:5

3

log10(sec)

3:5

3:5

4
log10(sec)

4:5

3

3:5

0

2

Figure2. Progression of RMSE and MNLL over time for different optimisation strategies for DGP-ARC and DGP-RBF models. Results are
shown for conﬁgurations having 1 hidden layer.

REGRESSION

CLASSIFICATION

Powerplant
(n = 9568, d=4)

RMSE

Protein
(n = 45730, d=9)
RMSE

Spam
(n = 4061, d=57)
Error rate

EEG
(n = 14979, d=14)
Error rate

MNIST
(n = 60000, d=784)
Error rate

(cid:1)10(cid:0)2

2:5
3
log10(sec)

MNLL

3:5

2

2:5

3

3:5

1

1:5

2

2:5

3

3:5

log10(sec)

MNLL

log10(sec)

MNLL

2:5

3

log10(sec)

3:5

3

3:5

4
log10(sec)

4:5

MNLL

MNLL

2:5
3
log10(sec)

3:5

2

2:5

3
log10(sec)

3:5

1

1:5

2

2:5

3

3:5

2

log10(sec)

2:5

3
log10(sec)

3:5

3:5

4
log10(sec)

4:5

Figure3. Progression of RMSE and MNLL over time for different optimisation strategies for DGP-ARC and DGP-RBF models. Results are
shown for conﬁgurations having 2 hidden layers.

B. Comparison with MCMC

Figure 4 shows a comparison between the variational approximation and MCMC for a two-layer DGP model applied to
a regression dataset. The dataset has been generated by drawing 50 data points from N (yjh(h(x)); 0:01), with h(x) =
2x exp((cid:0)x2). We experiment with two different levels of precision in the DGP approximation by using 10 and 50 ﬁxed
spectral frequencies, respectively, so as to assess the impact on the number of random features on the results. For MCMC,
covariance parameters are ﬁxed to the values obtained by optimizing the variational lower bound on the marginal likelihood
in the case of 50 spectral frequencies.

We obtained samples from the posterior over the latent variables at each layer using MCMC techniques. In the case of a
Gaussian likelihood, it is possible to integrate out the GP at the last layer, thus obtaining a model that only depends on the
GP at the ﬁrst. As a result, the collapsed DGP model becomes a standard GP model whose latent variables can be sampled
using various MCMC samplers developed in the literature of MCMC for GPs. Here we employ Elliptical Slice Sampling
(Murray et al., AISTATS, 2010) to draw samples from the posterior over the latent variables at the ﬁrst layer, whereas latent
variables at the second can be sampled directly from a multivariate Gaussian distribution. More details on the MCMC
sampler are reported at the end of this section.

The plots depicted in Figure 4 illustrate how the MCMC approach explores two modes of the posterior of opposite sign.

dgp-epdgp-rbf-var-resampleddgp-rbf-prior-fixeddgp-rbf-var-fixeddgp-arc-var-resampleddgp-arc-prior-fixeddgp-arc-var-fixedvar-gpdgp-epdgp-rbf-var-resampleddgp-rbf-prior-fixeddgp-rbf-var-fixeddgp-arc-var-resampleddgp-arc-prior-fixeddgp-arc-var-fixedvar-gpRandom Feature Expansions for Deep Gaussian Processes

Figure4. Comparison of MCMC and variational inference of a two-layer DGP with a single GP in the hidden layer and a Gaussian
likelihood. The posterior over the latent functions is based on 100 MCMC samples and 100 samples from the variational posterior.

This is due to the output function being invariant to the ﬂipping of the sign of the weights at the two layers. Conversely, the
variational approximation over W accurately identiﬁes one of the two modes of the posterior. The overall approximation is
accurate in the case of more random Fourier features, whereas in the case of less, the approximation is unsurprisingly char-
acterized by out-of-sample oscillations. The variational approximation seems to result in larger uncertainty in predictions
compared to MCMC; we attribute this to the factorization of the posterior over all the weights.

B.1. Details of MCMC sampler for a two-layer DGP with a Gaussian likelihood

We give details of the MCMC sampler that we used to draw samples from the posterior over latent variables in DGPs. In the
experiments, we regard this as the gold-standard against which we compare the quality of the proposed DGP approximation
and inference. For the sake of tractability, we assume a two-layer DGP with a Gaussian likelihood, and we ﬁx the hyper-
parameters of the GPs. Without loss of generality, we assume Y to be univariate and the hidden layer to be composed of a
single GP. The model is therefore as follows:
(cid:12)
(cid:12)
(cid:12)F (2); (cid:21)

= N

(

)

)

(

Y

p

(

p

Y
(cid:12)
(cid:12)
(cid:12)F (1); (cid:18)(1)

(cid:12)
(cid:12)
(cid:12)X; (cid:18)(0)

)

)

F (2)
(

p

F (1)

(cid:12)
(cid:12)
(cid:12)F (2); (cid:21)I
(

(cid:12)
(cid:12)
(cid:12)0; K
(cid:12)
(cid:12)
(cid:12)0; K

(

(

= N

F (2)

= N

F (1)

))

F (1); (cid:18)(1)
))

(

X; (cid:18)(0)

with (cid:21), (cid:18)(1), and (cid:18)(0) ﬁxed. In the model speciﬁcation above, we denoted by K
the co-
variance matrices obtained by applying the covariance function with parameters (cid:18)(1), and (cid:18)(0) to all pairs of F (1) and X,
respectively.

F (1); (cid:18)(1)

X; (cid:18)(0)

and K

(

)

(

)

Given that the likelihood is Gaussian, it is possible to integrate out F (2) analytically

(

p

Y

(cid:12)
(cid:12)
(cid:12)F (1); (cid:21); (cid:18)(1)

)

∫

(

=

p

Y

)

(cid:12)
(cid:12)
(cid:12)F (2); (cid:21)

(

p

F (2)

(cid:12)
(cid:12)
(cid:12)F (1); (cid:18)(1)

)

dF (2)

obtaining the more compact model speciﬁcation:

p

(

(cid:12)
(cid:12)
(cid:12)F (1); (cid:21); (cid:18)(1)
Y
(cid:12)
(
(cid:12)
(cid:12)X; (cid:18)(0)

F (1)

p

)

)

= N

Y

(

(

(

(cid:12)
(cid:12)
(cid:12)0; K
(cid:12)
(cid:12)
(cid:12)0; K

= N

F (1)

X; (cid:18)(0)

)

)

F (1); (cid:18)(1)
(

))

+ (cid:21)I

For ﬁxed hyper-parameters, these expressions reveal that the observations are distributed as in the standard GP regression
case, with the only difference that the covariance is now parameterized by GP distributed random variables F (1). We can
interpret these variables as some sort of hyper-parameters, and we can attempt to use standard MCMC methods to samples
from their posterior.

In order to develop a sampler for all latent variables, we factorize their full posterior as follows:
)

)

(

(

(

(cid:12)
(cid:12)
(cid:12)Y; X; (cid:21); (cid:18)(1); (cid:18)(0)

= p

F (2)

(cid:12)
(cid:12)
(cid:12)Y; F (1); (cid:21); (cid:18)(1)

(cid:12)
(cid:12)
(cid:12)Y; X; (cid:21); (cid:18)(1); (cid:18)(0)

)

p

F (1)

p

F (2); F (1)

which suggest a Gibbs sampling strategy to draw samples from the posterior where we iterate

Layer 1−202−10−50510Layer 2−101llllllllllllllllllllllllllllllllllllllllllllllllll−10−50510llllllllllllllllllllllllllllllllllllllllllllllllll−10−50510llllllllllllllllllllllllllllllllllllllllllllllllllVariational − 10 RFFVariational − 50 RFFMCMCLayer 1Layer 2Random Feature Expansions for Deep Gaussian Processes

1. sample from p

F (1)

2. sample from p

F (2)

(

(

)

(cid:12)
(cid:12)Y; X; (cid:21); (cid:18)(1); (cid:18)(0)
(cid:12)
)
(cid:12)Y; F (1); (cid:21); (cid:18)(1)

Step 1. can be done by setting up a Markov chain with invariant distribution given by:

(

p

F (1)

(cid:12)
(cid:12)
(cid:12)Y; X; (cid:21); (cid:18)(1); (cid:18)(0)

)

(

/ p

Y

(cid:12)
(cid:12)
(cid:12)F (1); (cid:21); (cid:18)(1)

)

(

p

F (1)

)

(cid:12)
(cid:12)
(cid:12)X; (cid:18)(0)

We can interpret this as a GP model, where the likelihood now assumes a complex form because of the nonlinear way in
which the likelihood depends on F (1). Because of this interpretation, we can attempt to use any of the samplers developed
in the literature of GPs to obtain samples from the posterior over latent variables in GP models.

Step 2. can be done directly given that the posterior over F (2) is available in closed form and it is Gaussian:

(

p

F (2)

(cid:12)
(cid:12)
(cid:12)Y; F (1); (cid:21); (cid:18)(1)

)

(

(

(cid:12)
(cid:12)
(cid:12)
(cid:12)K (1)

= N

F (2)

K (1) + (cid:21)I

Y; K (1) (cid:0) K (1)

K (1) + (cid:21)I

K (1)

)(cid:0)1

(

)(cid:0)1

)

where we have deﬁned

(

)

K (1) := K

F (1); (cid:18)(1)

C. Derivation of the lower bound

For the sake of completeness, here is a detailed derivation of the lower bound that we use in variational inference to learn
the posterior over W and optimize (cid:2), assuming Ω ﬁxed:

log[p(Y jX; Ω; (cid:2))] = log

p(Y jX; W; Ω; (cid:2))p(W)dW

D. Learning Ω variationally

Deﬁning (cid:9) = fW; Ωg, we can attempt to employ variational inference to treat the spectral frequencies Ω variationally as
well as W. In this case, the detailed derivation of the lower bound is as follows:
]
p(Y jX; (cid:9); (cid:2))p((cid:9)j(cid:2))d(cid:9)

log [p(Y jX; (cid:2))] = log

[∫

[∫

[∫

[

]

]

q(W)dW
]

= log

p(Y jX; W; Ω; (cid:2))p(W)
q(W)

= log

Eq(W)
(

[

(cid:21) Eq(W)

log

p(Y jX; W; Ω; (cid:2))p(W)
q(W)

p(Y jX; W; Ω; (cid:2))p(W)
q(W)

])

= Eq(W) (log[p(Y jX; W; Ω; (cid:2))]) + Eq(W)

p(W)
q(W)
= Eq(W) (log[p(Y jX; W; Ω; (cid:2))]) (cid:0) DKL[q(W)jjp(W)]

log

(

[

])

= log

[∫

[

p(Y jX; (cid:9); (cid:2))p((cid:9)j(cid:2))
q((cid:9))

]

q((cid:9))d(cid:9)
]

= log

Eq((cid:9))
(

[

(cid:21) Eq((cid:9))

log

p(Y jX; (cid:9); (cid:2))p((cid:9)j(cid:2))
q((cid:9))

p(Y jX; (cid:9); (cid:2))p((cid:9)j(cid:2))
q((cid:9))

])

= Eq((cid:9)) (log[p(Y jX; (cid:9); (cid:2))]) + Eq((cid:9))

p((cid:9)j(cid:2))
q((cid:9))
= Eq((cid:9)) (log[p(Y jX; (cid:9); (cid:2))]) (cid:0) DKL[q((cid:9))jjp((cid:9)j(cid:2))]

log

(

[

])

Random Feature Expansions for Deep Gaussian Processes

Again, assuming a factorized prior over all weights across layers

p((cid:9)j(cid:18)) =

p(Ω(l)j(cid:18)(l))p(W (l)) =

Nh(cid:0)1∏

l=0

∏

(

) ∏

(

)

q

Ω(l)
ij

q

W (l)
ij

,

ijl

ijl

we optimize the variational lower bound using variational inference following the mini-batch approach with the reparame-
terization trick explained in the main paper. The variational parameters then become the mean and the variance of each of
the approximating factors
(

)

)

(

q

W (l)
ij
)
(

q

Ω(l)
ij

= N

m(l)

ij ; (s2)(l)
)

ij

(

= N

ij ; ((cid:12)2)(l)
(cid:22)(l)

ij

,

,

(1)

(2)

(3)

and we optimize the lower bound with respect to the variational parameters m(l)

ij ; (s2)(l)

ij ; (cid:22)(l)

ij ; ((cid:12)2)(l)
ij .

E. Expression for the DKL divergence between Gaussians

Given p1(x) = N ((cid:22)1; (cid:27)2

1) and p2(x) = N ((cid:22)2; (cid:27)2

2), the KL divergence between the two is:

DKL (p1(x)∥p2(x)) =

log

[

(

)

1
2

(cid:27)2
2
(cid:27)2
1

(cid:0) 1 +

+

(cid:27)2
1
(cid:27)2
2

((cid:22)1 (cid:0) (cid:22)2)2
(cid:27)2
2

]

F. Distributed Implementation

Our model is easily amenable to a distributed implementation using asynchronous distributed stochastic gradient descent
(Chilimbi et al., USENIX, 2014). Our distributed setting, based on TensorFlow, includes one or more Parameter servers
(PS), and a number of Workers. The latter proceed asynchronously using randomly selected batches of data: they fetch fresh
model parameters from the PS, compute the gradients of the lower bound with respect to these parameters, and push those
gradients back to the PS, which update the model accordingly. Given that workers compute gradients and send updates
to PS asynchronously, the discrepancy between the model used to compute gradients and the model actually updated can
degrade training quality. This is exacerbated by a large number of asynchronous workers, as noted in Chen et al., arXiv
1604.00981, (2016).

We focus our experiments on the MNIST dataset, and study how training time and error rates evolve with the number of
workers introduced in our system. The parameters for the model are identical to those reported for the previous experi-
ments, except that we ﬁx the number of Monte Carlo samples to one throughout.

)
h
(
0
1
g
o
l

e
m

i
t
g
n
i
n
i
a
r
T

0:8

0:6

0:4

0:2

0

MNIST

0:1

8 (cid:1) 10(cid:0)2

6 (cid:1) 10(cid:0)2

4 (cid:1) 10(cid:0)2

e
t
a
R

r
o
r
r
E

2 (cid:1) 10(cid:0)2

1

5
Workers

0

10

Figure5. Comparison of training time and error rate for asynchronous DGP-RBF with 2 PS, and 1, 5 and 10 workers.

We report the results in Figure 5. The training time decreases in proportion to the number of workers, albeit sub-linearly,
whereas the error rate remains largely unchanged when varying the number of workers. When using a single PS, we
noticed that increasing the number of workers led to lower gains in speed of execution, and the behavior of the error rate

TrainingtimeErrorRateRandom Feature Expansions for Deep Gaussian Processes

over iterations was more erratic than in the case of two PS. We attribute these effects to the greater overheads on the
communication cost when using a single PS, as well as the more involved handling of the coordination of multiple workers.
The work in Chen et al., arXiv 1604.00981, (2016) corroborates our ﬁndings, and motivates further work in the direction
of alleviating this issue.

