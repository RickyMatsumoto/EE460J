Unsupervised Learning by Predicting Noise

Piotr Bojanowski 1 Armand Joulin 1

Abstract

Convolutional neural networks provide visual
features that perform well in many computer vi-
sion applications. However, training these net-
works requires large amounts of supervision; this
paper introduces a generic framework to train
such networks, end-to-end, with no supervision.
We propose to ﬁx a set of target representations,
called Noise As Targets (NAT), and to constrain
the deep features to align to them. This domain
agnostic approach avoids the standard unsuper-
vised learning issues of trivial solutions and col-
lapsing of features. Thanks to a stochastic batch
reassignment strategy and a separable square loss
function, it scales to millions of images. The
proposed approach produces representations that
perform on par with state-of-the-art unsupervised
methods on ImageNet and PASCAL VOC.

1. Introduction

In recent years, convolutional neural networks, or con-
vnets (Fukushima, 1980; LeCun et al., 1989) have pushed
the limits of computer vision (Krizhevsky et al., 2012; He
et al., 2016), leading to important progress in a variety of
tasks, like object detection (Girshick, 2015) or image seg-
mentation (Pinheiro et al., 2015). Key to this success is
their ability to produce features that easily transfer to new
domains when trained on massive databases of labeled im-
ages (Razavian et al., 2014; Oquab et al., 2014) or weakly-
supervised data (Joulin et al., 2016). However, human an-
notations may introduce unforeseen bias that could limit
the potential of learned features to capture subtle informa-
tion hidden in a vast collection of images.

Several strategies exist to learn deep convolutional features
with no annotation (Donahue et al., 2016). They either
try to capture a signal from the source as a form of self-
supervision (Doersch et al., 2015; Wang & Gupta, 2015) or

1Facebook AI Research. Correspondence to: Piotr Bojanowski

<bojanowski@fb.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

learn the underlying distribution of images (Vincent et al.,
2010; Goodfellow et al., 2014). While some of these ap-
proaches obtain promising performance in transfer learn-
ing (Donahue et al., 2016; Wang & Gupta, 2015), they do
not explicitly aim to learn discriminative features. Some at-
tempts were made with retrieval based approaches (Doso-
vitskiy et al., 2014) and clustering (Yang et al., 2016; Liao
et al., 2016), but they are hard to scale and have only been
tested on small datasets. Unfortunately, as in the supervised
case, a lot of data is required to learn good representations.

In this work, we propose a discriminative framework de-
signed to learn deep architectures on large datasets. Our
approach is general, but we focus on convnets since they
require millions of images to produce good features. Sim-
ilar to self-organizing maps (Kohonen, 1982; Martinetz &
Schulten, 1991), we map deep features to a set of prede-
ﬁned representations in a low dimensional space. As op-
posed to these approaches, we aim to learn the features in a
end-to-end fashion, which traditionally suffers from a fea-
ture collapsing problem. Our approach deals with this is-
sue by ﬁxing the target representations and aligning them to
our features. These representations are sampled from a un-
informative distribution and we use this Noise As Targets
(NAT). Our approach also shares some similarities with
standard clustering approches like k-means (Lloyd, 1982)
or discriminative clustering (Bach & Harchaoui, 2007).

In addition, we propose an online algorithm able to scale
to massive image databases like ImageNet (Deng et al.,
Importantly, our approach is barely less efﬁcient
2009).
to train than standard supervised approaches and can re-
use any optimization procedure designed for them. This
is achieved by using a quadratic loss as in (Tygert et al.,
2017) and a fast approximation of the Hungarian algo-
rithm. We show the potential of our approach by training
end-to-end on ImageNet a standard architecture, namely
AlexNet (Krizhevsky et al., 2012) with no supervision.

We test the quality of our features on several image classi-
ﬁcation problems, following the setting of Donahue et al.
(2016). We are on par with state-of-the-art unsupervised
and self-supervised learning approaches while being much
simpler to train and to scale.

The paper is organized as follows: after a brief review of
the related work in Section 2, we present our approach in

Unsupervised Learning by Predicting Noise

Section 3. We then validate our solution with several ex-
periments and comparisons with standard unsupervised and
self-supervised approaches in Section 4.

2. Related work

Several approaches have been recently proposed to tackle
the problem of deep unsupervised learning (Coates & Ng,
2012; Mairal et al., 2014; Dosovitskiy et al., 2014). Some
of them are based on a clustering loss (Xie et al., 2016;
Yang et al., 2016; Liao et al., 2016), but they are not tested
at a scale comparable to that of supervised convnet train-
ing. Coates & Ng (2012) uses k-means to pre-train con-
vnets, by learning each layer sequentially in a bottom-up
fashion. In our work, we train the convnet end-to-end with
a loss that shares similarities with k-means. Closer to our
work, Dosovitskiy et al. (2014) proposes to train convnets
by solving a retrieval problem. They assign a class per im-
age and its transformation.
In contrast to our work, this
approach can hardly scale to more than a few hundred of
thousands of images, and requires a custom-tailored archi-
tecture while we use a standard AlexNet.

Another traditional approach for learning visual representa-
tions in an unsupervised manner is to deﬁne a parametrized
mapping between a predeﬁned random variable and a set
of images. Traditional examples of this approach are varia-
tional autoencoders (Kingma & Welling, 2013), generative
adversarial networks (Goodfellow et al., 2014), and to a
lesser extent, noisy autoencoders (Vincent et al., 2010). In
our work, we are doing the opposite; that is, we map images
to a predeﬁned random variable. This allows us to re-use
standard convolutional networks and greatly simpliﬁes the
training.

Generative adversarial networks. Among those ap-
proaches, generative adversarial networks (GANs) (Good-
fellow et al., 2014; Denton et al., 2015; Donahue et al.,
2016) share another similarity with our approach, namely
they are explicitly minimizing a discriminative loss to learn
their features. While these models cannot learn an inverse
mapping, Donahue et al. (2016) recently proposed to add
an encoder to extract visual features from GANs. Like
ours, their encoder can be any standard convolutional net-
work. However, their loss aims at differentiating real and
generated images, while we are aiming directly at differ-
entiating between images. This makes our approach much
simpler and faster to train, since we do not need to learn the
generator nor the discriminator.

Self-supervision. Recently, a lot of work has explored
self-supervison:
leveraging supervision contained in the
input signal (Doersch et al., 2015; Noroozi & Favaro,
In the same vein as
2016; Pathak et al., 2016).

word2vec (Mikolov et al., 2013), Doersch et al. (2015)
show that spatial context is a strong signal to learn visual
features. Noroozi & Favaro (2016) have further extended
this work. Others have shown that temporal coherence in
videos also provides a signal that can be used to learn pow-
erful visual features (Agrawal et al., 2015; Jayaraman &
Grauman, 2015; Wang & Gupta, 2015). In particular, Wang
& Gupta (2015) show that such features provide promis-
ing performance on ImageNet.
In contrast to our work,
these approaches are domain dependent since they require
explicit derivation of weak supervision directly from the
input.

Autoencoders. Many have also used autoencoders with
a reconstruction loss (Bengio et al., 2007; Ranzato et al.,
2007; Masci et al., 2011). The idea is to encode and de-
code an image, while minimizing the loss between the de-
coded and original images. Once trained, the encoder pro-
duces image features and the decoder can be used to gen-
erate images from codes. The decoder is often a fully con-
nected network (Ranzato et al., 2007) or a deconvolutional
network (Masci et al., 2011; Zhao et al., 2016) but can
be more sophisticated, like a PixelCNN network (van den
Oord et al., 2016).

Self-organizing map. This family of unsupervised meth-
ods aims at learning a low dimensional representation of
the data that preserves certain topological properties (Ko-
honen, 1982; Vesanto & Alhoniemi, 2000). In particular,
Neural Gas (Martinetz & Schulten, 1991) aligns feature
vectors to the input data. Each input datum is then assigned
to one of these vectors in a winner-takes-all manner. These
feature vectors are in spirit similar to our target representa-
tions and we use a similar assignment strategy. In contrast
to our work, the target vectors are not ﬁxed and aligned to
the input vectors. Since we primarly aim at learning the
input features, we do the opposite.

Discriminative clustering. Many methods have been
proposed to use discriminative losses for clustering (Xu
et al., 2004; Bach & Harchaoui, 2007; Krause et al., 2010;
Joulin & Bach, 2012).
In particular, Bach & Harchaoui
(2007) shows that the ridge regression loss could be use
to learn discriminative clusters.
It has been successfully
applied to several computer vision applications, like ob-
ject discovery (Joulin et al., 2010; Tang et al., 2014) or
video/text alignment (Bojanowski et al., 2013; 2014; Ra-
manathan et al., 2014). In this work, we show that a similar
framework can be designed for neural networks. As op-
posed to Xu et al. (2004), we address the empty assignment
problems by restricting the set of possible reassignments to
permutations rather than using global linear constrains the
assignments. Our assignments can be updated online, al-
lowing our approach to scale to very large datasets.

Unsupervised Learning by Predicting Noise

Choosing the loss function.
In the supervised setting, a
popular choice for the loss (cid:96) is the softmax function. How-
ever, computing this loss is linear in the number of targets,
making it impractical for large output spaces (Goodman,
2001). While there are workarounds to scale these losses to
large output spaces, Tygert et al. (2017) has recently shown
that using a squared (cid:96)2 distance works well in many su-
pervised settings, as long as the ﬁnal activations are unit
normalized. This loss only requires access to a single tar-
get per sample, making its computation independent of the
number of targets. This leads to the following problem:

min
θ

min
Y ∈Rn×d

1
2n

(cid:107)fθ(X) − Y (cid:107)2
F ,

(2)

where we still denote by fθ(X) the unit normalized fea-
tures.

Using ﬁxed target representations. Directly solving the
problem deﬁned in Eq. (2) would lead to a representation
collapsing problem: all the images would be assigned to
the same representation (Xu et al., 2004). We avoid this
issue by ﬁxing a set of k predeﬁned target representations
and matching them to the visual features. More precisely,
the matrix Y is deﬁned as the product of a matrix C con-
taining these k representations and an assignment matrix P
in {0, 1}n×k, i.e.,

Y = P C.

(3)

Note that we can assume that k is greater than n with
no loss of generality (by duplicating representations oth-
erwise). Each image is assigned to a different target and
each target can only be assigned once. This leads to a set
P of constraints for the assignment matrices:

P = {P ∈ {0, 1}n×k | P 1k ≤ 1n, P (cid:62)1n = 1k}.

(4)

This formulation forces the visual features to be diversiﬁed,
avoiding the collapsing issue at the cost of ﬁxing the target
representations. Predeﬁning these targets is an issue if their
number k is small, which is why we are interested in the
case where k is at least as large as the number n of images.

Choosing the target representations. Until now, we
have not discussed the set of target representations stored
in C. A simple choice for the targets would be to take
k elements of the canonical basis of Rd.
If d is larger
than n, this formulation would be similar to the framework
of Dosovitskiy et al. (2014), and is impractical for large
n. On the other hand, if d is smaller than n, this formula-
tion is equivalent to the discriminative clustering approach
of Bach & Harchaoui (2007). Choosing such targets makes
very strong assumptions on the nature of the underlying
problem. Indeed, it assumes that each image belongs to a
unique class and that all classes are orthogonal. While this
assumption might be true for some classiﬁcation datasets, it

Figure 1. Our approach takes a set of images, computes their deep
features with a convolutional network and matches them to a set of
predeﬁned targets from a low dimensional space. The parameters
of the network are learned by aligning the features to the targets.

3. Method

In this section, we present our model and discuss its re-
lations with several clustering approaches including k-
means. Figure 1 shows an overview of our approach. We
also show that it can be trained on massive datasets using
an online procedure. Finally, we provide all the implemen-
tation details.

3.1. Unsupervised learning

We are interested in learning visual features with no su-
pervision. These features are produced by applying a
parametrized mapping fθ to the images. In the presence
of supervision, the parameters θ are learned by minimiz-
ing a loss function between the features produced by this
mapping and some given targets, e.g., labels. In absence of
supervision, there is no clear target representations and we
thus need to learn them as well. More precisely, given a
set of n images xi, we jointly learn the parameters θ of the
mapping fθ, and some target vectors yi:

min
θ

1
n

n
(cid:88)

i=1

min
yi∈Rd

(cid:96)(fθ(xi), yi),

(1)

where d is the dimension of target vectors. In the rest of
the paper, we use matrix notations, i.e., we denote by Y the
matrix whose rows are the target representations yi, and by
X the matrix whose rows are the images xi. With a slight
abuse of notation, we denote by fθ(X) the n × d matrix of
features whose rows are obtained by applying the function
fθ to each image independently.

Target spaceFeaturesAssignmentImagescjPf(X)CNNUnsupervised Learning by Predicting Noise

does not generalize to large image collections nor capture
subtle similarities between images belonging to different
classes.

Algorithm 1 Stochastic optimization of Eq. (5).
Require: T batches of images, λ0 > 0

for t = {1, . . . , T } do

Since our features are unit normalized, another natural
choice is to uniformly sample target vectors on the (cid:96)2 unit
sphere. Note that the dimension d will then directly inﬂu-
ence the level of correlation between representations, i.e.,
the correlation is inversely proportional to the square root
of d. Using this Noise As Targets (NAT), Eq. (2) is now
equivalent to:

max
θ

max
P ∈P

Tr (cid:0)P Cfθ(X)(cid:62)(cid:1) .

This problem can be interpreted as mapping deep features
to a uniform distribution over a manifold, namely the d-
dimension (cid:96)2 sphere. Using k predeﬁned representations is
a discrete approximation of this manifold that justiﬁes the
restriction of the mapping matrices to the set P of 1-to-1
assignment matrices. In some sense, we are optimizing a
crude approximation of the earth mover’s distance between
the distribution of deep features and a given target distribu-
tion (Rubner et al., 1998).

Relation to clustering approaches. Using the same no-
tations as in Eq. (5), several clustering approaches share
similarities with our method. In the linear case, spherical
k-means minimizes the same loss function w.r.t. P and C,
i.e.,

max
C

max
P ∈Q

tr (cid:0)P CX T (cid:1) .

The main difference is the set Q of assignment matrices:

Q = {P ∈ {0, 1}n×k | P 1k = 1n}.

This set only guarantees that each data point is assigned
to a single target representation. Once we jointly learn the
features and the assignment, this set does not prevent the
collapsing of the data points to a single target representa-
tion.

Another similar clustering approach is Diffrac (Bach &
Harchaoui, 2007). Their loss is equivalent to ours in the
case of unit normalized features. Their set R of assignment
matrices, however, is different:

R = {P ∈ {0, 1}n×k | P (cid:62)1n ≥ c1k},

where c > 0 is some ﬁxed parameter. While restricting
the assignment matrices to this set prevents the collapsing
issue, it introduces global constraints that are not suited
for online optimization. This makes their approach hard
to scale to large datasets.

3.2. Optimization

In this section, we describe how to efﬁciently optimize the
cost function described in Eq. (5). In particular, we explore

Obtain batch b and representations r
Compute fθ(Xb)
Compute P ∗ by minimizing Eq. (2) w.r.t. P
Compute ∇θL(θ) from Eq. (2) with P ∗
Update θ ← θ − λt∇θL(θ)

end for

(5)

approximated updates of the assignment matrix that are
compatible with online optimization schemes, like stochas-
tic gradient descent (SGD).

Updating the assignment matrix P . Directly solving
for the optimal assignment requires to evaluate the dis-
tances between all the n features and the k representations.
In order to efﬁciently solve this problem, we ﬁrst reduce
the number k of representations to n. This limits the set P
to the set of permutation matrices, i.e.,

P = {P ∈ {0, 1}n×n | P 1n = 1n, P (cid:62)1n = 1n}.

(6)

Restricting the problem deﬁned in Eq. (5) to this set, the
linear assignment problem in P can be solved exactly with
the Hungarian algorithm (Kuhn, 1955), but at the pro-
hibitive cost of O(n3).

Instead, we perform stochastic updates of the matrix. Given
a batch of samples, we optimize the assignment matrix P
on its restriction to this batch. Given a subset B of b dis-
tinct images, we only update the b × b square sub matrix
PB obtained by restricting P to these b images and their
corresponding targets. In other words, each image can only
be re-assigned to a target that was previously assigned to
another image in the batch. This procedure has a complex-
ity of O(b3) per batch, leading to an overall complexity of
O(nb2), which is linear in the number of data points. We
perform this update before updating the parameters θ of our
features, in an on-line manner. Note that this simple proce-
dure would not have been possible if k > n; we would have
had to also consider the k − n unassigned representations.

Stochastic gradient descent. Apart from the update of
the assignment matrix P , we use the same optimization
scheme as standard supervised approaches, i.e., SGD with
batch normalization (Ioffe & Szegedy, 2015). As noted
by Tygert et al. (2017), batch normalization plays a cru-
cial role when optimizing the l2 loss, as it avoids exploding
gradients. For each batch b of images, we ﬁrst perform a
forward pass to compute the distance between the images
and the corresponding subset of target representations r.
The Hungarian algorithm is then used on these distances to
obtain the optimal reassignments within the batch. Once

Unsupervised Learning by Predicting Noise

Softmax

Square loss

4. Experiments

ImageNet

59.2

58.4

Table 1. Comparison between the softmax and the square loss for
supervised object classiﬁcation on ImageNet. The architecture
is an AlexNet. The features are unit normalized for the square
loss (Tygert et al., 2017). We report the accuracy on the validation
set.

the assignments are updated, we use the chain rule in order
to compute the gradients of all our parameters. Our opti-
mization algorithm is summarized in Algorithm 1.

3.3. Implementation details

Our experiments solely focus on learning visual features
with convnets. All details required to train these architec-
tures with our approach are described below. Most of them
are standard tricks used in the supervised setting.

Deep features. To ensure a fair empirical comparison
with previous work, we follow Wang & Gupta (2015) and
use an AlexNet architecture. We train it end to end using
our unsupervised loss function. We subsequently test the
quality of the learned visual feature by re-training a classi-
ﬁer on top. During transfer learning, we consider the output
of the last convolutional layer as our features as in Raza-
vian et al. (2014). We use the same multi-layer perceptron
(MLP) as in Krizhevsky et al. (2012) for the classiﬁer.

in practice

Pre-processing. We observe
that pre-
processing the images greatly helps the quality of our
learned features. As in Ranzato et al. (2007), we use im-
age gradients instead of the images to avoid trivial solu-
tions like clustering according to colors. Using this pre-
processing is not surprising since most hand-made features
like SIFT or HoG are based on image gradients (Lowe,
1999; Dalal & Triggs, 2005).
In addition to this pre-
processing, we also perform all the standard image trans-
formations that are commonly applied in the supervised
setting (Krizhevsky et al., 2012), such as random cropping
and ﬂipping of images.

Optimization details. We project the output of the net-
work on the (cid:96)2 sphere as in Tygert et al. (2017). The net-
work is trained with SGD with a batch size of 256. Dur-
ing the ﬁrst t0 batches, we use a constant step size. Af-
ter t0 batches, we use a linear decay of the step size, i.e.,
lt =
. Unless mentioned otherwise, we permute
the assignments within batches every 3 epochs. For the
transfer learning experiments, we follow the guideline de-
scribed in Donahue et al. (2016).

l0
1+γ[t−t0]+

We perform several experiments to validate different design
choices in NAT. We then evaluate the quality of our fea-
tures by comparing them to state-of-the-art unsupervised
approaches on several auxiliary supervised tasks, namely
object classiﬁcation on ImageNet and object classiﬁcation
and detection of PASCAL VOC 2007 (Everingham et al.,
2010).

Transfering the features.
In order to measure the quality
of our features, we measure their performance on transfer
learning. We freeze the parameters of all the convolutional
layers and overwrite the parameters of the MLP classiﬁer
with random Gaussian weights. We precisely follow the
training and testing procedure that is speciﬁc to each of the
datasets following Donahue et al. (2016).

Datasets and baselines. We use the training set of Im-
ageNet to learn our convolutional network (Deng et al.,
2009). This dataset is composed of 1, 281, 167 images that
belong to 1, 000 object categories. For the transfer learn-
ing experiments, we also consider PASCAL VOC 2007. In
addition to fully supervised approaches (Krizhevsky et al.,
2012), we compare our method to several unsupervised
approaches, i.e., autoencoder, GAN and BiGAN as re-
ported in Donahue et al. (2016). We also compare to self-
supervised approaches, i.e., Agrawal et al. (2015); Doersch
et al. (2015); Pathak et al. (2016); Wang & Gupta (2015)
and Zhang et al. (2016). Finally we compare to state-of-
the-art hand-made features, i.e., SIFT with Fisher Vectors
(SIFT+FV) (S´anchez et al., 2013). They reduce the Fisher
Vectors to a 4, 096 dimensional vector with PCA, and apply
an 8, 192 unit 3-layer MLP on top.

4.1. Detailed analysis

In this section, we validate some of our design choices,
like the loss function, representations and the inﬂuences of
some parameters on the quality of our features. All the ex-
periments are run on ImageNet.

Softmax versus square loss. Table 1 compares the per-
formance of an AlexNet trained with a softmax and a
square loss. We report the accuracy on the validation set.
The square loss requires the features to be unit normal-
ized to avoid exploding gradients. As previously observed
by Tygert et al. (2017), the performances are similar, hence
validating our choice of loss function.

Effect of image preprocessing.
In supervised classi-
image pre-processing is not frequently used,
ﬁcation,
and transformations that remove information are usually
avoided.
In the unsupervised case, however, we observe
that is it is preferable to work with simpler inputs as

Unsupervised Learning by Predicting Noise

clean

high-pass

sobel

acc@1

59.7

58.5

57.4

Table 2. Performance of supervised models with various image
pre-processings applied. We train an AlexNet on ImageNet, and
report classiﬁcation accuracy.

it avoids learning trivial features.
In particular, we ob-
serve that using grayscale image gradients greatly helps our
method, as mentioned in Sec. 3.
In order to verify that
this preprocessing does not destroy crucial information, we
propose to evaluate its effect on supervised classiﬁcation.
We also compare with high-pass ﬁltering. Table 2 shows
the impact of this preprocessing methods on the accuracy
of an AlexNet on the validation set of ImageNet. None
of these pre-processings degrade the perform signiﬁcantly,
meaning that the information related to gradients are suf-
ﬁcient for object classiﬁcation. This experiment conﬁrms
that such pre-processing does not lead to a signiﬁcant drop
in the upper bound performance for our model.

Continuous versus discrete representations. We com-
pare our choice for target vectors to those commonly used
for clustering, i.e., elements of the canonical basis of a k
dimensional space. Such a representation makes a strong
assumption on the structure of the problem, that it can be
linearly separated in k different classes. This holds for Im-
ageNet, giving a fair advantage to this discrete representa-
tion. We test this representation with k in {103, 104, 105},
which is a range well-suited for the 1, 000 classes of Im-
ageNet. The matrix C contains n/k replications of k ele-
ments of the canonical basis. This assumes that the clusters
are balanced, which is veriﬁed on ImageNet.

We compare these cluster-like representations to our con-
tinuous target vectors on the transfer task on ImageNet. Us-
ing discrete targets achieves an accuracy of 22%, which is
signiﬁcantly worse that our best performance, i.e., 36.0%.
A possible explanation is that binary vectors induce sharp
discontinuous distances between representations. Such dis-
tances are hard to optimize over and may result in early
convergence to poorer local minima.

Evolution of the features.
In this experiment, we are in-
terested in understanding how the quality of our features
evolves with the optimization of our cost function. Dur-
ing the unsupervised training, we freeze the network every
20 epochs and learn a MLP classiﬁer on top. We report
the accuracy on the validation set of ImageNet. Figure 2
shows the evolution of the performance on this transfer task
as we optimize for our unsupervised approach. The train-
ing performance improves monotonically with the epochs
of the unsupervised training. This suggests that optimizing

Figure 2. On the left, we measure the accuracy on ImageNet after
training the features with our unsupervised approach as a function
of the number of epochs. The performance improves with longer
unsupervised training. On the right, we measure the accuracy on
ImageNet after training the features with different permutation
rates There is a clear trade-off with an optimum at permutations
performed every 3 epochs.

our objective function correlates with learning transferable
features, i.e., our features do not destroy useful class-level
information. On the other hand, the test accuracy seems
to saturate after a hundred epochs. This suggests that the
MLP is overﬁtting rapidly on pre-trained features.

Effect of permutations. Assigning images to their target
representations is a the main feature of our approach. In
this experiment, we want to understand how frequently we
should update this assignment. Updating the assignment,
even partially, is costly and may not be required to achieve
good performance. Figure 2 shows the transfer accuracies
on ImageNet as a function of the frequency of these up-
dates. The model is quite robust to choice of frequency,
with a test accuracy always above 30%. Interestingly, the
accuracy actually degrades slightly with high frequency. A
possible explanation is that the network overﬁts rapidly to
its own output, leading to relatively worse features. In prac-
tice, we observe that updating the assignment matrix every
3 epochs offers a good trade-off between performance and
accuracy. When training the network and keeping P ﬁxed,
we obtain a baseline test accuracy of 26.4%.

Visualizing the ﬁlters. Figure 4 shows a comparison be-
tween the ﬁrst convolutional layer of an AlexNet trained
with and without supervision. Both take grayscale gradient
images as input. The visualization are obtained by com-
posing the Sobel ﬁltering with the ﬁlters of the ﬁrst layer
of the AlexNet. Unsupervised ﬁlters are slightly less sharp
than their supervised counterpart, but still maintain edge
and orientation information.

Nearest neighbor queries. Our loss optimizes a distance
between features and ﬁxed vectors. This means that look-
ing at the distance between features should provide some
information about the type of structure that our model cap-

50100150200epochofunsupervisedtraining2030405060accuracytransfertrainacctransfertestacc510permutationperiod2030405060accuracyUnsupervised Learning by Predicting Noise

Figure 3. Images and their 3 nearest neighbors in ImageNet according to our model using an (cid:96)2 distance. The query images are shown on
the top row, and the nearest neighbors are sorted from the closer to the further. Our features seem to capture global distinctive structures.

4.2. Comparison with the state of the art

We report results on the transfer task both on ImageNet and
PASCAL VOC 2007. The model is trained on ImageNet.

ImageNet classiﬁcation.
In this experiment, we evaluate
the quality of our features for the object classiﬁcation task
of ImageNet. In this setup, we build the unsupervised fea-
tures on images that correspond to predeﬁned image cate-
gories. Even though we do not have access to labels, the
data itself is biased towards these classes. In order to eval-
uate the features, we freeze the layers up to the last convo-
lutional layer and train the classiﬁer with supervision. This
experimental setting follows Noroozi & Favaro (2016).

We compare our model with several self-supervised (Wang
& Gupta, 2015; Doersch et al., 2015; Zhang et al., 2016)
and one unsupervised approach, i.e., Donahue et al. (2016).
Note that self-supervised approaches use losses speciﬁcally
designed for visual features. Like BiGANs (Donahue et al.,
2016), NAT does not make any assumption about the do-
main but of the structure of its features. Table 3 compares
NAT with these approaches.

Among unsupervised approaches, NAT compares favor-
ably to BiGAN (Donahue et al., 2016). Interestingly, the
performance of NAT are slightly better than self-supervised
methods, even though we do not explicitly use domain-
speciﬁc clues in images or videos to guide the learning.
While all the models provide performance in the 30 − 36%
range, it is not clear if they all learn the same features. Fi-
nally, all the unsupervised deep features are outperformed

Figure 4. Filters form the ﬁrst layer of an AlexNet trained on Im-
ageNet with supervision (left) or with NAT (right). The ﬁlters
are in grayscale, since we use grayscale gradient images as input.
This visualization shows the composition of the gradients with the
ﬁrst layer.

tures. Given a query image x, we compute its feature fθ(x)
and search for its nearest neighbors according to the (cid:96)2 dis-
tance. Figure 3 shows images and their nearest neighbors.

The features capture relatively complex structures in im-
ages. Objects with distinctive structures, like trunks or
fruits, are well captured by our approach. However, this
information is not always related to image labels. For ex-
ample, the image of bird on the sea is matched to images
more related to the sea or the sky rather than the bird.

Unsupervised Learning by Predicting Noise

Method

Acc@1

Random (Noroozi & Favaro, 2016)

SIFT+FV (S´anchez et al., 2013)

Wang & Gupta (2015)
Doersch et al. (2015)
Zhang et al. (2016)
1Noroozi & Favaro (2016)

BiGAN (Donahue et al., 2016)

NAT

12.0

55.6

29.8
30.4
35.2
38.1

32.2

36.0

Table 3. Comparison of the proposed approach to state-of-the-art
unsupervised feature learning on ImageNet. A full multi-layer
perceptron is retrained on top of the features. We compare to sev-
eral self-supervised approaches and an unsupervised approach,
i.e., BiGAN (Donahue et al., 2016). 1Noroozi & Favaro (2016)
uses a signiﬁcantly larger amount of features than the original
AlexNet. We report classiﬁcation accuracy.

by hand-made features, in particular Fisher Vectors with
SIFT descriptors. This baseline uses a slightly bigger MLP
for the classiﬁer and its performance can be improved by
2.2% by bagging 8 such models. This difference of 20%
in accuracy shows that unsupervised deep features are still
quite far from the state-of-the-arts among all unsupervised
features.

Transferring to PASCAL VOC 2007. We carry out a
second transfer experiment on the PASCAL VOC dataset,
on the classiﬁcation and detection tasks. The model is
trained on ImageNet. Depending on the task, we ﬁnetune
all layers in the network, or solely the classiﬁer, follow-
ing Donahue et al. (2016). In all experiments, the parame-
ters of the convolutional layers are initialized with the ones
obtained with our unsupervised approach. The parame-
ters of the classiﬁcation layers are initialized with gaussian
weights. We get rid of batch normalization layers and use
a data-dependent rescaling of the parameters (Kr¨ahenb¨uhl
et al., 2015). Table 4 shows the comparison between our
model and other unsupervised approaches. The results for
other methods are taken from Donahue et al. (2016) except
for Zhang et al. (2016).

As with the ImageNet classiﬁcation task, our performance
is on par with self-supervised approaches, for both de-
tection and classiﬁcation. Among purely unsupervised
approaches, we outperform standard approaches like au-
toencoders or GANs by a large margin. Our model also
performs slightly better than the best performing BiGAN
model (Donahue et al., 2016). These experiments conﬁrm
our ﬁndings from the ImageNet experiments. Despite its
simplicity, NAT learns feature that are as good as those ob-
tained with more sophisticated and data-speciﬁc models.

Classiﬁcation Detection

fc6-8

all

Trained layers

ImageNet labels

Agrawal et al. (2015)
Pathak et al. (2016)
Wang & Gupta (2015)
Doersch et al. (2015)
Zhang et al. (2016)

Autoencoder
GAN
BiGAN (Donahue et al., 2016)

NAT

78.9

31.0
34.6
55.6
55.1
61.5

16.0
40.5
52.3

56.7

79.9

54.2
56.5
63.1
65.3
65.6

53.8
56.4
60.1

65.3

all

56.8

43.9
44.5
47.4
51.1
46.9

41.9
-
46.9

49.4

Table 4. Comparison of the proposed approach to state-of-the-art
unsupervised feature learning on VOC 2007 Classiﬁcation and de-
tection. We either ﬁx the features after conv5 or we ﬁne-tune the
whole model. We compare to several self-supervised and an un-
supervised approaches. The GAN and autoencoder baselines are
from Donahue et al. (2016). We report mean average prevision as
customary on PASCAL VOC.

5. Conclusion

This paper presents a simple unsupervised framework to
learn discriminative features. By aligning the output of a
neural network to low-dimensional noise, we obtain fea-
tures on par with state-of-the-art unsupervised learning ap-
proaches. Our approach explicitly aims at learning discrim-
inative features, while most unsupervised approaches target
surrogate problems, like image denoising or image genera-
tion. As opposed to self-supervised approaches, we make
very few assumptions about the input space. This makes
our appproach very simple and fast to train. Interestingly, it
also shares some similarities with traditional clustering ap-
proaches as well as retrieval methods. While we show the
potential of our approach on visual data, it will be interest-
ing to try other domains. Finally, this work only considers
simple noise distributions and alignment methods. A pos-
sible direction of research is to explore target distributions
and alignments that are more informative. This also would
strengthen the relation between NAT and methods based on
distribution matching like the earth mover distance.

Acknowledgement. We greatly thank Herv´e J´egou for
his help throughout the development of this project. We
also thank Allan Jabri, Edouard Grave, Iasonas Kokkinos,
L´eon Bottou, Matthijs Douze and the rest of FAIR for their
support and helpful discussion. Finally, we thank Richard
Zhang, Jeff Donahue and Florent Perronnin for their help.

Unsupervised Learning by Predicting Noise

References

Agrawal, P., Carreira, J., and Malik, J. Learning to see by

moving. In ICCV, 2015.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y. Generative adversarial nets. In NIPS, 2014.

Goodman, J. Classes for fast maximum entropy training.

Bach, F. and Harchaoui, Z. Diffrac: a discriminative and

ﬂexible framework for clustering. In NIPS, 2007.

In ICASSP, 2001.

Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.
Greedy layer-wise training of deep networks. In NIPS,
2007.

Bojanowski, P., Bach, F., Laptev, I., Ponce, J., Schmid, C.,
and Sivic, J. Finding actors and actions in movies. In
ICCV, 2013.

Bojanowski, P., Lajugie, R., Bach, F., Laptev, I., Ponce,
J., Schmid, C., and Sivic, J. Weakly supervised action
labeling in videos under ordering constraints. In ECCV,
2014.

Coates, A. and Ng, A. Learning feature representations
with k-means. In Neural Networks: Tricks of the Trade.
Springer, 2012.

Dalal, N. and Triggs, B. Histograms of oriented gradients

for human detection. In CVPR, 2005.

Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., and
Fei-Fei, L. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009.

Denton, E. L., Chintala, S., and Fergus, R. Deep generative
image models using a laplacian pyramid of adversarial
networks. In NIPS, 2015.

Doersch, C., Gupta, A., and Efros, A. Unsupervised visual
representation learning by context prediction. In CVPR,
2015.

Donahue, J., Kr¨ahenb¨uhl, P., and Darrell, T. Adversar-
ial feature learning. arXiv preprint arXiv:1605.09782,
2016.

Dosovitskiy, A., Springenberg, J., Riedmiller, M., and
Brox, T. Discriminative unsupervised feature learning
with convolutional neural networks. In NIPS, 2014.

Everingham, M., Van Gool, L., Williams, C. K. I., Winn,
J., and Zisserman, A. The PASCAL visual object classes
(VOC) challenge. IJCV, 2010.

Fukushima, K. Neocognitron: A self-organizing neural
network model for a mechanism of pattern recognition
unaffected by shift in position. Biological Cybernetics,
36:193–202, 1980.

Girshick, R. Fast r-cnn. In CVPR, 2015.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-

ing for image recognition. In CVPR, 2016.

Ioffe, S. and Szegedy, C. Batch normalization: Accelerat-
ing deep network training by reducing internal covariate
shift. arXiv preprint arXiv:1502.03167, 2015.

Jayaraman, D. and Grauman, K. Learning image represen-

tations tied to ego-motion. In ICCV, 2015.

Joulin, A. and Bach, F. A convex relaxation for weakly

supervised classiﬁers. In ICML, 2012.

Joulin, A., Bach, F., and Ponce, J. Discriminative clustering

for image co-segmentation. In CVPR, 2010.

Joulin, A., van der Maaten, L., Jabri, A., and Vasilache, N.
Learning visual features from large weakly supervised
data. In ECCV, 2016.

Kingma, D. and Welling, M. Auto-encoding variational

bayes. arXiv preprint arXiv:1312.6114, 2013.

Kohonen, T. Self-organized formation of topologically cor-

rect feature maps. Biological cybernetics, 1982.

Kr¨ahenb¨uhl, Philipp, Doersch, Carl, Donahue,

and Darrell, Trevor.
of convolutional neural networks.
arXiv:1511.06856, 2015.

Data-dependent

Jeff,
initializations
arXiv preprint

Krause, A., Perona, P., and Gomes, R. G. Discriminative
clustering by regularized information maximization. In
NIPS, 2010.

Krizhevsky, A., Sutskever, I., and Hinton, G.

Imagenet
classiﬁcation with deep convolutional neural networks.
In NIPS, 2012.

Kuhn, H. W. The hungarian method for the assignment
problem. Naval research logistics quarterly, 2(1-2):83–
97, 1955.

LeCun, Y., Boser, B., Denker, J. S., Henderson, D.,
Howard, R. E., Hubbard, W., and Jackel, L.D. Handwrit-
ten digit recognition with a back-propagation network.
In NIPS, 1989.

Liao, R., Schwing, A., Zemel, R., and Urtasun, R. Learning

deep parsimonious representations. In NIPS, 2016.

Lloyd, S. Least squares quantization in pcm. Transactions

on information theory, 28(2):129–137, 1982.

Unsupervised Learning by Predicting Noise

Lowe, D. Object recognition from local scale-invariant fea-

tures. In ICCV, 1999.

Mairal, J., Koniusz, P., Harchaoui, Z., and Schmid, C. Con-

volutional kernel networks. In NIPS, 2014.

Martinetz, T. and Schulten, K. A” neural-gas” network

learns topologies. 1991.

Masci, J., Meier, U., Cires¸an, D., and Schmidhuber,
J. Stacked convolutional auto-encoders for hierarchical
feature extraction. In ICANN, 2011.

Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efﬁcient
estimation of word representations in vector space. arXiv
preprint arXiv:1301.3781, 2013.

Noroozi, M. and Favaro, P. Unsupervised learning of visual
representations by solving jigsaw puzzles. arXiv preprint
arXiv:1603.09246, 2016.

van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals,
O., and Graves, A. Conditional image generation with
pixelcnn decoders. In NIPS, 2016.

Vesanto, J. and Alhoniemi, E. Clustering of the self-
organizing map. Transactions on neural networks, 2000.

Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Man-
zagol, P.-A. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local de-
noising criterion. JMLR, 11(Dec):3371–3408, 2010.

Wang, X. and Gupta, A. Unsupervised learning of visual

representations using videos. In ICCV, 2015.

Xie, J., Girshick, R., and Farhadi, A. Unsupervised deep

embedding for clustering analysis. In ICML, 2016.

Xu, L., Neufeld, J., Larson, B., and Schuurmans, D. Maxi-

mum margin clustering. In NIPS, 2004.

Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning
and transferring mid-level image representations using
convolutional neural networks. In CVPR, 2014.

Yang, J., Parikh, D., and Batra, D. Joint unsupervised learn-
ing of deep representations and image clusters. In CVPR,
2016.

Zhang, R., Isola, P., and Efros, A. Colorful image coloriza-

tion. In ECCV, 2016.

Zhao, J., Mathieu, M., Goroshin, R., and LeCun, Y.
In Workshop at

Stacked What-Where Auto-encoders.
ICLR, 2016.

Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., and
Efros, A. Context encoders: Feature learning by inpaint-
ing. In CVPR, 2016.

Pinheiro, P. O., Collobert, R., and Dollar, P. Learning to

segment object candidates. In NIPS, 2015.

Ramanathan, V., Joulin, A., Liang, P., and Fei-Fei, L. Link-
ing people in videos with their names using coreference
resolution. In ECCV, 2014.

Ranzato, M. A., Huang, F. J., Boureau, Y. L., and LeCun,
Y. Unsupervised learning of invariant feature hierarchies
with applications to object recognition. In CVPR, 2007.

Razavian, A. Sharif, Azizpour, H., Sullivan, J., and Carls-
son, S. CNN features off-the-shelf: an astounding base-
line for recognition. In arXiv 1403.6382, 2014.

Rubner, Y., Tomasi, C., and Guibas, L. J. A metric for
distributions with applications to image databases.
In
ICCV, 1998.

S´anchez, J., Perronnin, F., Mensink, T., and Verbeek, J.
Image classiﬁcation with the ﬁsher vector: Theory and
practice. IJCV, 105(3):222–245, 2013.

Tang, K., Joulin, A., Li, L.-J., and Fei-Fei, L. Co-

localization in real-world images. In CVPR, 2014.

Tygert, M., Chintala, S., Szlam, A., Tian, Y., and Zaremba,
W. Scale-invariant learning and convolutional networks.
Applied and Computational Harmonic Analysis, 42(1):
154–166, 2017.

