The Statistical Recurrent Unit

Junier B. Oliva 1 Barnab´as P´oczos 1 Jeff Schneider 1

Abstract

Sophisticated gated recurrent neural network ar-
chitectures like LSTMs and GRUs have been
shown to be highly effective in a myriad of appli-
cations. We develop an un-gated unit, the statisti-
cal recurrent unit (SRU), that is able to learn long
term dependencies in data by only keeping mov-
ing averages of statistics. The SRU’s architec-
ture is simple, un-gated, and contains a compara-
ble number of parameters to LSTMs; yet, SRUs
perform favorably to more sophisticated LSTM
and GRU alternatives, often outperforming one
or both in various tasks. We show the efﬁcacy
of SRUs as compared to LSTMs and GRUs in an
unbiased manner by optimizing respective archi-
tectures’ hyperparameters for both synthetic and
real-world tasks.

1. Introduction

The analysis of sequential data has long been a staple in
machine learning. Domain areas like natural language
(Zaremba et al., 2014; Vinyals et al., 2015), speech (Graves
et al., 2013; Graves & Jaitly, 2014), music (Chung et al.,
2014), and video (Donahue et al., 2015) processing have
recently garnered much attention. While the study of se-
quences itself is broad and may be extended to general
functional analysis (Ramsay & Silverman, 2002), most re-
cent success has been from neural network based models,
especially from recurrent architectures.

Recurrent networks are dynamical systems that represent
time recursively. For example, the simple recurrent unit
(Elman, 1990) contains a hidden state that itself depends on
the previous hidden state. However, training such networks
has been observed to be difﬁcult in practice due to explod-
ing and vanishing gradients when propagating error gradi-
ents through time (Hochreiter et al., 2001). While explod-

1Machine Learning Department, Carnegie Mellon University,
Pittsburgh, PA, USA. Correspondence to: Junier B. Oliva <jo-
liva@cs.cmu.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ing gradients can be mitigated with techniques like gradient
clipping and normalization (Pascanu et al., 2013), vanish-
ing gradients may be harder to deal with. As a result, so-
phisticated gated architectures like Long-Short Term Mem-
ory (LSTM) networks (Hochreiter & Schmidhuber, 1997)
and Gated Recurrent Unit (GRU) networks (Cho et al.,
2014) have been developed. These gated architectures con-
tain “memory cells” along with gates to control how much
they decay through time thereby aiding the networks’ abil-
ity to learn long term dependencies in sequences.

Notwithstanding,
there are still challenges in capturing
long term dependencies in gated architectures (Le et al.,
In this paper we present a simple un-gated ar-
2015).
chitecture, the Statistical Recurrent Unit, that often out-
performs these more complicated alternatives. Although
the SRU keeps only simple moving averages of summary
statistics, its novel architecture makes it more adept than
previous gated units for capturing long term information in
sequences and comparing them across different windows
of time. For instance, the SRU, unlike traditional recur-
rent units, can obtain a multitude of viewpoints of the past
by simple linear combinations of only a few averages. We
shall illustrate the efﬁcacy of the SRU below using both
real-world and synthetic sequential data tasks.

The structure of the paper is as follows: ﬁrst we detail the
architecture of the SRU as well as provide several key in-
tuitions and insights for its design; after, we describe our
experiments comparing the SRU to popular gated alter-
natives, and we perform a “dissective” study of the SRU,
gaining further understanding of the unit by exploring how
various hyper-parameters affect performance; ﬁnally, we
discuss conclusions from our study.

2. Model

The SRU maintains long term sequential dependencies in a
rather intuitive fashion–through summary statistics. As the
name implies, statisticians often employ summary statistics
when trying to represent a dataset. Quite naturally then, we
look to an algorithm that itself learns to represent data seen
previously in much the same vein as a neural statistician
(Edwards & Storkey, 2016).

Of course, unlike with unordered i.i.d. samples, simply
averaging statistics of sequential points will lose valuable

The Statistical Recurrent Unit

temporal information. The SRU maintains sequential infor-
mation in two ways: ﬁrst, we generate recurrent statistics
that depend on a context of previously seen data; second,
we generate moving averages at several scales, allowing
the model to distinguish the type of data seen at different
points in the past. We expound on these methods for creat-
ing temporally-aware statistics below.

We shall see that the statistical design of the SRU yields a
powerful yet simple model that is able to analyze sequential
data and, on the ﬂy, create summary statistics for learning
over sequences. Furthermore, through the use of ReLUs
and exponential moving averages, the SRU is able to mit-
igate vanishing gradient issues that are common to many
recurrent units.

2.1. Recurrent Statistics

(cid:80)T

We consider an input sequence of real valued points
x1, x2, . . . , xT ∈ Rd. As seen in the second row of Ta-
ble 1, we can compute a vector of statistics φ(xi) ∈ RD
for each point. Here, each vector φ(xi) is independent of
other points xj for j (cid:54)= i. One may then average these
vectors as µ = 1
i=1 φ(xi) to produce summary statis-
T
tics of the sequence. This approach amounts to treating
the sequence as a set of i.i.d. points drawn form some dis-
tribution and marginalizing out time. Clearly, here one
will lose temporal information that will be useful for many
sequence related ML tasks.
It is interesting to note that
global average pooling operations have gained a lot of re-
cent traction in convolutional networks (Lin et al., 2013;
Iandola et al., 2016). Analogously to the i.i.d. statistic ap-
proach, global averaging will lose spatial information, yet
the high-level summary statistics provide an effective rep-
resentation. Still, not marginalizing out time should pro-
vide a more robust approach for sequence tasks, thus we
consider the following methods for producing statistics.

First, we provide temporal information whilst still utilizing
averages through recurrent statistics that also depend on the
values of previous points (see third row of Table 1). That
is, we compute our statistics on the ith point xi not only
as a function of xi, but also as a function of the previous
statistics of xi−1, (cid:126)γi−1 (which itself depends on (cid:126)γi−2, etc.):

ing illustrative example where xi ∈ R+ and statistics

(cid:126)γi = (0, . . . , 0, T xi, 0, . . .)

(cid:126)γi+1 = (0, . . . , 0, 0, T xi+1, 0, . . .).

(2)

(3)

(cid:80)T

That is, one records the ith input in the ith index. When av-
eraged the statistics will be 1
i=1 (cid:126)γi = (x1, x2, . . .), i.e.
T
the complete sequence. Such recurrent statistics will un-
doubtedly suffer from the curse of dimensionality. Hence,
we consider a more restrictive model of recurrent statistics
which we expound on below (6).

Second, we provide even more temporal information by
considering summary statistics at multiple scales. As a sim-
ple hypothetical example, consider taking multiple means
across separate time windows (for instance taking means
over indices 1-10, then over indices 11-20, etc.). Such an
approach (4) will illustrate how summary statistics evolve
through time.

, φ11, . . . , φ20
φ1, . . . , φ10
(cid:123)(cid:122)
(cid:125)
(cid:125)
(cid:123)(cid:122)
(cid:124)
µ11:20
µ1:10

(cid:124)

, . . . .

(4)

In practice, we shed light on the dynamics of statistics
through time by using several averages of the same sum-
mary statistics. The SRU will use exponential moving aver-
ages µi = α(cid:126)γi +(1−α)µi−1 to compute means; hence, we
consider multiple weights by taking the exponential means
at various scales α1, . . . , αm as shown in the last row of
Table 1. Later we show that this multi-scaled approach is
capable of a combinatorial number of viewpoints of past
statistics through simple linear combinations.

Table 1. Methods for keeping statistics of sequences.

inputs

i.i.d.
statistics

recurrent
statistics

x1, x2, . . . , xT

φ(x1), φ(x2), . . . , φ(xT )

γ (x1, (cid:126)γ0) , γ(x2, (cid:126)γ1), . . . , γ(xT , (cid:126)γT −1)

recurrent
multi-scaled
statistics

αT −1
1

γ(x1,(cid:126)γ0), αT −2

1

γ(x2,(cid:126)γ1), ...

αT −1

m γ(x1,(cid:126)γ0), αT −2

...
m γ(x2,(cid:126)γ1), ...

(cid:126)γ1 = γ(x1, (cid:126)γ0), (cid:126)γ2 = γ(x2, (cid:126)γ1), . . .

(1)

2.2. Update Equations

where γ(·, ·) is a function for producing statistics given the
current point and previous statistics, and (cid:126)γ0 is a constant
initial vector for convention. We note that from a general
standpoint if given a ﬂexible model and enough dimen-
sions, then recurrent summary statistics like (1) can per-
fectly encode ones sequence. Take for instance the follow-

We have discussed in broad terms how one may create
temporally-aware summary statistics through multi-scaled
recurrent statistics. Below, we cover speciﬁcally how the
SRU creates and uses summary statistics for sequences.

Recall that our input is a sequence of ordered points:
x1, x2, . . . , xt ∈ Rd. Throughout, we apply an element-

The Statistical Recurrent Unit

wise non-linearity f (·), which we take to be the ReLU
(Jarrett et al., 2009; Nair & Hinton, 2010): f (·) =
max(·, 0). The SRU operates via exponential moving av-
erages, µ(α) ∈ Rs (7), kept at various scales α ∈ A =
{α1, . . . , αm}, where αi ∈ [0, 1). These moving averages,
µ(α), are of recurrent statistics ϕ (6) that are dependent
not only on the current input but also on features of av-
erages, r (5). The moving averages are then concatenated
as µ = (µ(α1), . . . , µ(αm)) and used to create an output o
(8) that is fed upwards in the network.

Figure 1. Graphical representation of the SRU. Solid lines indi-
cate a dependence on the current value of a node. Dashed lines
indicate a dependence on the previous value of a node. We see
that both the current point xt as well as a summary of the previ-
ous data rt are used to make statistics ϕt, which in turn are used in
moving averages µt, ﬁnally an output ot is feed-forward through
the rest of the network.

We detail the update equations for the SRU below (and in
Figure 1):

rt = f
(cid:16)

ϕt = f

∀α ∈ A, µ(α)
(cid:16)

ot = f

(cid:16)

W (r)µt−1 + b(r)(cid:17)
W (ϕ)rt + W (x)xt + b(ϕ)(cid:17)
t = αµ(α)

t−1 + (1 − α)ϕt

W (o)µt + b(o)(cid:17)

.

(5)

(6)

(7)

(8)

In practiced we noted that it sufﬁces to use only a few α’s
such as A = {0, 0.25, 0.5, 0.9, 0.99}.

It is worth noting that previous work has considered cap-
turing recurrent information at various timescales in the
past. For instance, Koutnik et al. (2014) considers an RNN
scheme that divides the hidden state into different mod-
ules for use at different frequencies. Furthermore, expo-
nential averages in recurrent units have been considered
previously, e.g. (Mikolov et al., 2015; Bengio et al., 2013;
Jaeger et al., 2007). However, such works are more akin to
un-gated GRUs since they consider only one scale per fea-
ture, limiting the views available per statistic to just one.
The use of ReLUs in recurrent units has also been recently
explored by Le et al. (2015), however there no statistics
are kept and their use is limited to the simple RNN when
initialized in a special manner.

2.3. Intuitions from Mean Map Embeddings

The design of the SRU is deliberately chosen to allow for
long term dependencies to be learned. To better elucidate
the design and its intuition, let us take a brief excursion to
another use of (summary) statistics in machine learning for
the representation of data: mean map embeddings (MMEs)
of distributions (Smola et al., 2007). At its core, the con-
cept of MMEs is that one may embed, and thereby rep-
resent, a distribution through statistics (such as moments).
The MME for a distribution D given a positive semideﬁnite
kernel k is:

µ[D] = EX∼D [φk(X)] ,

(9)

where φk are the reproducing kernel Hilbert space (RKHS)
features of k, which may be inﬁnite dimensional. To rep-
resent a set Y = {y1, . . . , yn} iid∼ D one would use an
empirical mean version of the MME:

µ[Y ] =

φk(yi).

(10)

1
n

n
(cid:88)

i=1

Numerous works have shown success in representing dis-
tributions and sets through MMEs (Muandet et al., 2016).
One interpretation for the design of SRUs is that we are
modifying MME’s for use on sequences. Of course, one
way of applying MMEs directly on sequences is to simply
ignore the non-i.i.d. nature of sequences and treat points as
comprising a set. This however loses important sequential
information, as previously mentioned. Below we discuss
the speciﬁc modiﬁcations we make from traditional MMEs
and the beneﬁts they yield.

2.3.1. DATA-DRIVEN STATISTICS

First, we note the clear analogue between the mean embed-
ding of a set Y , µ[Y ] (10), and the moving average µ(α) (7).
The moving averages µ(α) are clearly serving as summary
statistics of previously seen data. However, the statistics
we are averaging for µ(α), ϕ (6), are not comprised of a-
priori RKHS features as is typical with MMEs, but rather
are learned non-linear features. This has the beneﬁt of us-
ing data-driven statistics, and may be interpreted as using a
linear kernel in the learned features.

2.3.2. RECURSIVE STATISTICS FROM THE PAST

Second, recall that typical MMEs use statistics that depend
only on a single point x, φk(x). As aforementioned this
is ﬁne for i.i.d. data, however it loses sequential informa-
tion when averaged. Instead, we wish to assign statistics
that depend on the data we have seen so far, since it pro-
vides context for one’s current point in the sequence. For
instance, one may want to have a statistic that keeps track
of the difference between the current point and the mean

The Statistical Recurrent Unit

of previous data. We provide a context based on previ-
ous data by making the statistics considered at time t, ϕt
(6), a function not only of xt but also of {x1, . . . , xt−1}
through rt (5). rt may be interpreted as a condensation of
the sequence seen so far, and allows us to keep sequential
information even through an averaging operation.

2.3.3. MULTI-SCALED STATISTICS

Third, the use of multi-scaled moving averages of statistics
gives the SRU a simple and powerful rich view of past data
that is unique to this recurrent unit. In short, by keeping
moving averages at different scales {α1, . . . , αm}, we are
able to uncover differences in statistics at various times in
the past. Note that we may unroll moving averages as:

t = (1 − α) (cid:0)ϕt + αϕt−1 + α2ϕt−2 + . . .(cid:1)
µ(α)

(11)

Thus, a smaller α weighs current statistics more than
older statistics; hence, a concatenated vector µ =
(µ(α1), . . . , µ(αm)) itself provides a multi-scale view of
statistics through time (see Figure 2). For instance, keep-
ing statistics for short and long terms pasts already yields
information on the evolution of the sequence through time.

Figure 2. We may unroll the moving average updates as (11). To
visualize the different emphasis in the past that varying α has on
statistics we plot the values of weights in moving averages (i.e.
αi) for 100 points in the past across rows. We see that alpha
values closer to 0 focus only on the recent past, where values close
to 1 maintain an emphasis on the distant past as well.

2.4. Viewpoints of the Past

An interesting and useful property of keeping multiple
scales for each statistic is that one can obtain a combinato-
rial number of viewpoints of the past through simple linear
combinations of ones statistics. For instance, for properly
chosen wj, wk ∈ R, wjµ(αj ) −wkµ(αk) provides an aggre-
gate of statistics from the past for αj > αk (Figure 3). Of
course, more complicated linear combinations may be per-
formed to obtain richer viewpoints that are comprised of
multiple windows. Furthermore, by using a linear projec-
tion of our statistics µt, as we do with ot (8), we are able to
compute output features of combined viewpoints of several
statistics.

This kind of multi-viewpoint perspective of previously seen
data is difﬁcult to produce in traditional gated recurrent

Figure 3. We visualize the power of taking linear combinations
of µ(α)’s for providing different viewpoints into past data. In row
1 we show the effective weights that would be used for weighing
statistics ϕt if one considers .001−1µ(.999) − .01−1µ(.99); we
see that this is equivalent to considering only statistics from the
distant past. Similarly, we show the effective weights when taking
.01−1µ(.99) − .1−1µ(.9) and .1−1µ(.9) − .5−1µ(.5) on rows 2 and
3 respectively. We see that these linear combinations amount to
considering viewpoints concentrated at various points in the past.
Lastly its worth noting that more complicated linear combinations
may lead to even richer views on previous statistics; for instance,
we show .001−1µ(.999) −.01−1µ(.99) + .5
.09 µ(.9) on row 4, which
concentrates on the statistics of the distant and very recent past,
but de-emphasizes statistics of data from less recent past.

units since they must encode where in the sequence they
currently are and then store an activation on separate nodes
per each viewpoint for future use. SRUs, on the other hand,
only need to take simple linear combinations to capture var-
ious viewpoints in the past. For example, as shown above,
statistics from just the distant past are available via a sim-
ple subtraction of two moving averages (Figure 3, row 1).
Such a windowed view would require a gated unit to learn
to stop averaging after a certain point in the sequence, and
the corresponding statistic would not yield any information
outside of this window. In contrast, each statistic kept by
the SRU provides a combinatorial number of varying per-
spectives in the past through linear combinations and their
multi-scaled nature.

2.5. Vanishing Gradients

As previously mentioned, it has been shown that vanish-
ing gradients make learning recurrent units difﬁcult due
to an inability to propagate error gradients through time.
Notwithstanding its simple un-gated structure, the SRU
features several safeguards to alleviate vanishing gradients.
First, units and statistics are comprised of ReLUs. ReLUs
have been observed to be easier to train for general deep
networks (Nair & Hinton, 2010) and have had success in
recurrent units (Le et al., 2015). Intuitively, ReLUs allow
for the propagation on error on positive inputs without sat-
uration and vanishing gradients as with traditional sigmoid
units. The ability of the SRU to use ReLUs (without any
special initialization) makes it especially adept at learning
long term dependencies through time.

Furthermore, the explicit moving average of statistics al-

The Statistical Recurrent Unit

lows for longer term learning. Consider the following
(cid:105)
derivative of the error signal E w.r.t. an element
of the unit’s moving averages when [ϕt]k = 0:

(cid:104)
µ(α)
t−1

k

(cid:104)

∂E
µ(α)
t−1

(cid:105)

∂

k

=

(cid:105)

(cid:104)
µ(α)
t

∂

(cid:104)
µ(α)
t−1

∂

k
(cid:105)

k

∂E
(cid:104)
µ(α)
t

(cid:105)

∂

k

= α

∂E
(cid:104)
µ(α)
t

(cid:105)

∂

.

k

That is, the factor α directly controls the decay of the error
signal through time. Thus, by including an α explicitly near
1 (i.e. 0.999), the decay for that moving average can be
made minuscule for the lengths of sequences in ones data.
Also, it is interesting to note that, with a large α near 1,
SRUs with ReLUs can implement part of the functionality
of a gate (“remembering”) by carrying through the previous
moving average [µ(α)
t−1]k when the corresponding statistic
[ϕt]k has be zeroed out (7). The other functionality of a
gate (forgetting) can be had by including an α near 0; if the
ReLU statistic is not zeroed out, then the moving average
for a small α will “forget” the previous value.

3. Experiments

We compared the performance of the SRU1 to two popular
gated recurrent units, the GRU and LSTM unit. All ex-
periments were performed in Tensorflow (Abadi et al.,
2016) and used the standard implementations of GRUCell
and BasicLSTMCell for GRUs and LSTMs respectively.
In order to perform a fair, unbiased comparison of the re-
current units and their hyper-parameters, which greatly af-
fect performance (Bergstra & Bengio, 2012), we used the
Hyperopt (Bergstra et al., 2015) hyper-parameter opti-
mization package. We believe that such an approach gives
each algorithm a fair shot to succeed without injecting bi-
ases from experimenters or imposing gross restrictions on
architectures considered.

In all experiments we used SGD for optimization using gra-
dient clipping (Pascanu et al., 2013) with a norm of 1 on all
algorithms. Unless otherwise speciﬁed 100 trials were per-
formed to search over the following hyper-parameters on a
validation set: one, initial learning rate the ini-
tial learning rate used for SGD, in range of [exp(−10), 1];
two, lr decay the multiplier to multiply the learning
rate by every 1k iterations, in range of [0.8, 0.999]; three,
dropout keep rate, percent of output units that are
kept during dropout, in range (0, 1]; four, num units
number of units for recurrent unit, in {1, . . . , 256}. In ad-
dition, the following two parameters were searched over
for the SRU: num stats, the dimensionality of ϕ (6),
in {1, . . . , 256}; summary dims, the dimensionality of
r (5), in {1, . . . , 64}.

1See

https://github.com/junieroliva/

recurrent for code.

3.1. Synthetic Recurrent Unit Generated Data

First we provide evidence that traditional gated units have
difﬁculties capturing the same type of multi-scale recurrent
statistic based dependencies that the SRU offers. We show
the relative inefﬁciency of traditional gated units at learn-
ing long term dependencies of statistics by considering 1d
synthetic data from a ground truth SRU.

iid∼ N (0, 1002), and xt
We begin the sequences with x1
is the results of a projection of ot. We generate a total of
176 points per sequence for 3200 training sequences, 400
validation sequences, and 400 testing sequences.

The ground truth statistical recurrent unit has three statis-
tics φt (6): the positive part of inputs (x)+, the negative
part of inputs (x)−, and an internal statistic, z. We use
i=1 = {0.0, 0.5, 0.9, 0.99, 0.999}. Denote µ(α)
α ∈ {αi}5
+ ,
µ(α)
− , µ(α)
as the moving averages using α for each re-
spective statistic. The internal statistic z does not get used
(through rt (5)) in updating the statistics for (x)+ or (x)−.
z is itself updated as:

z

zt = (zt−1)+
(cid:16)
+ − µ(α5)
µ(α4)

+

+ − 0.01

−µ(α4)

− + µ(α5)

− − 0.01

(cid:16)

−

−µ(α4)

+ + µ(α5)

+ − 0.05

− − µ(α5)
µ(α4)

− − 0.05

(cid:17)

(cid:16)

−

+
(cid:17)

+

(cid:16)

+

where each of the summands are rt features. Furthermore
we have ot ∈ R15 (8):

ot = (cid:0)(xt)+, −(xt)−, vT

1 µt, . . . , vT

13µt

(cid:1) ,

where vj’s where initialized and ﬁxed as (vj)k
N (0, ( 1
100 )2). Finally the next point is generated as:
xt+1 = (xt)+ − (xt)− + wT ot,3:,

where w was initialized and ﬁxed as (w)k
ot,3: are the last 13 dimensions of ot.

iid∼ N (0, 1), and

After the ground truth SRU was constructed we generated
the training, validation, and testing sequences. As can be
seen in Figure 4, the sequences follow a simple pattern:
at the start negative values are quickly pushed to zero and
positive values follow a parabolic line until hitting zero, at
which point they slope downward depending on initial val-
ues. While simple, it is clear that trained recurrent units
must be able to hold long-term information since all se-
quences converge at one point and future behaviour de-
pends on initial values.

is,

that

(cid:80)175

We look to minimize the mean of squared errors
the loss we consider per sequence is
(MSE);
1
t=1 |xt+1 − pt|2, where pt is the output of the net-
175
work after being fed xt. We conducted 100 trials of hyper-
parameter optimization as described above and obtained
the following results in Table 2.

(cid:17)

(cid:17)

+

+

,

iid∼

The Statistical Recurrent Unit

Figure 4. 25 sequences generated from the ground truth SRU
model.

Table 2. MSEs for synthetically generated dataset.

SRU GRU
0.62
21.72

LSTM
161.62

Error

Figure 5. Right: example MNIST 28 × 28 image, which is taken
as a pixel-by-pixel sequence of length 784 unrolled as shown in
yellow. Left: example pixel sequences for 0, 1, and 2 digit images.

Not surprisingly, the SRU performs far better than tradi-
tional gated recurrent units. This suggests that the types
of long-term statistical relationships captured by the SRU
are indeed different than those of traditional recurrent units.
As previously mentioned, the SRU is able to obtain a mul-
titude of different views from its statistics, a task that tradi-
tional units achieve less efﬁciently since they must devote
one whole memory cell per viewpoint and statistic. As we
show below, the SRU is able to outperform traditional gated
units in long term problems even for real data that is not
generated from its model class.

3.2. MNIST Image Classiﬁcation

Next we explore the ability of recurrent units to use long-
term dependencies in ones data with a synthetic task us-
ing a real dataset. It has been observed that LSTMs per-
form poorly in classifying a long pixel-by-pixel sequence
of MNIST digits (Le et al., 2015). In this synthetic task,
each 28×28 gray-scale MNIST digit image is ﬂattened and
observed as a sequence {x1, . . . , x784}, where xi ∈ [0, 1]
(see Figure 5). The task is, based on the output observed af-
ter feeding x784 through the network, to classify the digit of
the corresponding image in {0, . . . , 9}. Hence, we project
the output after x784 of each recurrent unit to 10 dimen-
sions and use a softmax activation.

We report the hyper-parameter optimized results below in
Table 3; due to resource constraints each trial consisted
only of 10K training iterations. We see that the SRU is
able to out-perform both GRUs and LSTMs. Given the
long length and dependencies of pixel sequences in this
experiment, it is not surprising that SRUs’ abilities to cap-
ture long-term dependencies are aiding it to achieve a much
lower error.

Table 3. Test error rate for MNIST pixel sequence classiﬁcation.

Error Rate

SRU GRU LSTM
0.11
0.48
0.28

3.2.1. DISSECTIVE STUDY

the architecture.

Next, we study the behavior of the statistical recur-
rent unit with a dissective study where we vary sev-
We consider
eral parameters of
num stats=200;
variants to a base model with:
r dims=60; num units=200. We keep the parameters
initial learning rate, lr decay ﬁxed at the the
optimal values found (0.1, 0.99 respectively) unless we ﬁnd
no learning, in which case we also try learning rates of 0.01
and 0.001.

The need for multi-scaled recurrent statistics. Recall
that we designed the statistics used by the SRU expressly
to capture long term time dependencies in sequences. We
did so both with recurrent statistics, i.e. statistics that them-
selves depend on previous points’ statistics, and with multi-
scaled averages. We show below that both of these time-
dependent design choices are vital to capturing long term
dependencies in data. Furthermore, we show that the use
of ReLU statistics lends itself to better learning.

We explored the impact that time-dependent statistics had
on learning by ﬁrst considering naive i.i.d. summary statis-
tics for sequences. This was achieved by using r dims=0
and α ∈ A = {0.99999}. Here no past-dependent context
is used for statistics, i.e. we used i.i.d.-type statistics as is
typical for unordered sets. Furthermore, the use of a single
scale α near 1 means that all of the points’ statistics will
be weighted nearly identically (11) regardless of index. We
optimized the SRU when using no recurrent statistics and
a single scale (iid), when using recurrent statistics with a
single scale (recur), and when using no recurrent statis-
tics with multiple scales (multi). We report errors below
in Table 4.

Table 4. Test error rate for MNIST pixel sequence classiﬁcation.

Error Rate

iid
0.88

recur
0.88

multi
0.63

The Statistical Recurrent Unit

Table 7. Test error rate varying number of units.
num stats
10

units

10

50

50

Error Rate

0.88

0.32

0.15

0.15

Predictably, we cannot learn by simply keeping i.i.d. type
statistics of pixel values at a single scale. Furthermore, we
ﬁnd that only using recurrent statistics (recur) in the SRU
is not enough. It is interesting to note, however, that keep-
ing i.i.d. statistics at multiple scales is able to predict digits
with limited success. This lends evidence for the need of
both recurrent statistics and multiple scales.

Next, we explored the effects of the scales at which
we keep our statistics by varying from α ∈ A =
{0.0, 0.5, 0.9, 0.99, 0.999} considering α ∈ A =
{0.0, 0.5, 0.9}, α ∈ A = {0.0, 0.5, 0.9, 0.99}. We see in
Table 5 that additional, longer scales aid our learning for
this dataset. This is not very surprising given the long term
nature of the pixel sequences.

Table 5. Test error rate for MNIST pixel sequence classiﬁcation.

A
Error Rate

{0.0, 0.5, 0.9}
0.79

{0.0, 0.5, 0.9, 0.99}
0.21

Lastly, we considered the use of non-ReLU statistics by
changing the element-wise non-linearity f (·) (5)-(8) to be
the hyperbolic tangent f (·) = tanh(·). We postulated that
the use of ReLUs would help our learning since they have
been observed to better handle the problem of vanishing
gradients. We ﬁnd evidence of this when swapping ReLUs
for hyperbolic tangent units in SRUs: we get an error rate
of 0.18 when using hyperbolic tangent units. Although the
previous uses of ReLUs in RNN required careful initializa-
tion (Le et al., 2015), SRUs are able to use ReLUs for better
learning without any special considerations.

Dimension of recurrent summary. Next we explore the
effect of varying the number of dimensions used for the re-
current summary of statistics rt (5). We consider r dims
in {5, 20, 240}. As previously discussed rt provides a con-
text based on past data so that the SRU may produce non-
i.i.d. statistics as it moves along a sequences. As one would
expect the dimensionality of rt will limit the information
ﬂow from the past and values that are too small will hinder
performance. It is also interesting to see that after enough
dimensions, there are diminishing returns to adding more.

Table 6. Test error rate varying recurrent summary rt.

r dims
Error Rate

5
0.25

20
0.20

240
0.10

3.3. Polyphonic Music Modeling

Henceforth we consider real data and sequence learning
tasks. First, we used the polyphonic music datasets from
Boulanger-Lewandowski et al. (2012). Each time-step is a
binary vector representing the notes played at the respective
time-step. Since we were required to predict binary vectors
we used the element-wise sigmoid σ. I.e., the binary vec-
tor of notes xt+1 was modeled as σ (pt), where pt is the
output after feeding xt (and previous values x1, . . . , xt−1)
through the recurrent network.

It is interesting to note in Table 8 that the SRU is able to out-
perform one of the traditional gated units in every dataset
and it outperforms both in two datasets.

Table 8. Test negative log-likelihood for polyphonic music data.

SRU
Data set
8.260
JSB
Muse
6.336
Nottingham 3.362
7.737
Piano

GRU LSTM
8.393
8.548
6.293
6.429
3.359
3.386
7.931
7.929

3.4. Electronica-Genre Music MFCC

In the following experiment we modeled the Mel fre-
quency cepstrum coefﬁcients (MFCCs) in a dataset of
nearly 18 000 scraped 30s sound clips of electronica-genre
songs. MFCCs are perceptually based spectral features
positioned logarithmically on the mel scale, which ap-
proximates the human auditory system’s response (M¨uller,
2007). We looked to model the 13 real-valued coefﬁcients
using the recurrent units, by modeling xt+1 as a projection
of the output of a recurrent unit after being fed x1, . . . , xt.

Table 9. Test-set MSEs of MFCC Music data.

SRU
1.176

GRU LSTM
1.183
2.080

Error

As can be seen in Table 9, SRUs again are outperforming
gated architectures and are especially beating GRUs by a
wider margin.

Number of statistics and outputs. Finally, we vary the
number of statistics num stats, and outputs units. In-
terestingly the SRU seems robust to the number of outputs
propagated in the network. However, performance is con-
siderably affected by the number of statistics considered.

3.5. Climate Data

Next we consider weather data prediction using the North
America Regional Reanalysis (NARR) Project (NAR). The
dataset provides a long-term set of consistent climate data
on a regional scale for the North American domain. The

The Statistical Recurrent Unit

period of the reanalyses is from October 1978 to the present
and analyses were made 8 times daily (3 hour intervals).

We take our input sequences to be year-long sequences of
weather variables in a location for the year 2006. I.e. an in-
put sequence will be a 2920 length sequence of weather
variables at a given lat/lon coordinate. We considered
the following 7 variables: pres10m, 10 m pressure (pa);
tcdc, total cloud cover (%); rh2m, relative humidity
2m (%); tmpsfc, surface temperature (k); snod, snow
depth surface (m); ugrd10m, u component of wind 10m
above ground; vgrd10m, v component of wind 10m above
ground. The variables were standardized, see Figure 6 for
example sequences.

Figure 7. Example player/ball x, y positions for two plays.

dataset contains long term dependencies that the SRU is
able to exploit.

4. Discussion

We believe that the use of summary statistics has been
under-explored in modern recurrent units. Although recent
studies in convolutional networks have considered global
average pooling, which is essentially using high-level sum-
mary statistics to represent images, there has been little ex-
ploration of summary statistics for modern recurrent net-
works. To this end we introduce the Statistical Recurrent
Unit, a novel architecture that seeks to capture long term
dependencies in data using only simple moving averages
and rectiﬁed-linear units.

The SRU was motivated by the success of mean-map em-
beddings for representing unordered datasets, and may be
interpreted as an alteration of MMEs for sequential data.
The main modiﬁcations are as follows: ﬁrst, the SRU uses
data-driven statistics unlike typical MMEs, which will use
RKHS features from an a-priori selected class of kernels;
second, SRUs will use recurrent statistics that are depen-
dent not only on a current point, but on previous points’
statistics through a condensation of kept moving averages;
third, the SRU will keep moving averages at various scales.
We provide evidence that the combination of these modi-
ﬁcations yield much better results than any one of them in
isolation.

The resulting recurrent unit is especially adept for captur-
ing long term dependencies in data and readily has access
to a combinatorial number of viewpoints of past windows
through simple linear combinations. Moreover, it is in-
teresting to note that even though the SRU is gate-less, it
may implement part of both “remembering” and “forget-
ting” functionalities through ReLUs and moving averages.

We showed empirically that the SRU is better equipped that
traditional gated units for long term dependencies via syn-
thetic and real-world data experiments.

Figure 6. Two example sequences for weather variables at distinct
locations for the year 2006.

Below we see results using 51 200 training location se-
quences and 6 400 validation and testing instances. Again,
we look to model the next point in a sequence as a pro-
jection of the output of the recurrent unit after feeding the
previous points. One may see in Table 10 that SRUs and
LSTMs perform nearly identically; perhaps the cyclical na-
ture of climate data was beneﬁcial to the gated units.

Table 10. Test MSEs for weather data.

SRU
0.465

GRU LSTM
0.466
0.487

Error

3.6. SportVu NBA Tracking data

Finally, we look to predict the positions of National Basket-
ball Association (NBA) players based on previous court po-
sitions during a play. Optical tracking data for this project
were provided by STATS LLC from their SportVU product
and obtained from (NBA). The data are composed of x and
y coordinates for each of the ten players and the ball. We
again minimize the squared norm of errors for predictions.

Table 11. Test-set MSEs of NBA data.

SRU
34.505

GRU
329.921

LSTM
296.908

Error

We observed a large margin of improvement for SRUs over
gated architectures in Table 11 that is reminiscent of the
synthetic data experiment in §3.1. This suggests that this

research is

This
DESC0011114 and NSF grant IIS1563887.

supported in part by DOE grant

Acknowledgements

The Statistical Recurrent Unit

References

Ncep north american regional

reanalysis.

https:

//data.noaa.gov/dataset/ncep-north-
american-regional-reanalysis-narr-
for-1979-to-present. Accessed: 2016-10-17.

Nba movement data.

https://github.com/
sealneaward/nba-movement-data. Accessed:
2016-10-17.

Abadi, Mart´ın, Barham, Paul, Chen, Jianmin, Chen,
Zhifeng, Davis, Andy, Dean, Jeffrey, Devin, Matthieu,
Ghemawat, Sanjay, Irving, Geoffrey, Isard, Michael,
et al. Tensorﬂow: A system for large-scale machine
In Proceedings of the 12th USENIX Sympo-
learning.
sium on Operating Systems Design and Implementation
(OSDI). Savannah, Georgia, USA, 2016.

Bengio, Yoshua, Boulanger-Lewandowski, Nicolas, and
Pascanu, Razvan. Advances in optimizing recurrent
networks. In Acoustics, Speech and Signal Processing
(ICASSP), 2013 IEEE International Conference on, pp.
8624–8628. IEEE, 2013.

Bergstra, James and Bengio, Yoshua. Random search
for hyper-parameter optimization. Journal of Machine
Learning Research, 13(Feb):281–305, 2012.

Edwards, Harrison and Storkey, Amos. Towards a neural
statistician. arXiv preprint arXiv:1606.02185, 2016.

Elman, Jeffrey L. Finding structure in time. Cognitive sci-

ence, 14(2):179–211, 1990.

Graves, Alex and Jaitly, Navdeep. Towards end-to-end
speech recognition with recurrent neural networks.
In
ICML, volume 14, pp. 1764–1772, 2014.

Graves, Alex, Mohamed, Abdel-rahman, and Hinton, Ge-
offrey. Speech recognition with deep recurrent neu-
ral networks. In Acoustics, speech and signal process-
ing (icassp), 2013 ieee international conference on, pp.
6645–6649. IEEE, 2013.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-
term memory. Neural computation, 9(8):1735–1780,
1997.

Hochreiter, Sepp, Bengio, Yoshua, Frasconi, Paolo, and
Schmidhuber, J¨urgen. Gradient ﬂow in recurrent nets:
the difﬁculty of learning long-term dependencies, 2001.

Iandola, Forrest N, Han, Song, Moskewicz, Matthew W,
Ashraf, Khalid, Dally, William J, and Keutzer, Kurt.
Squeezenet: Alexnet-level accuracy with 50x fewer
arXiv preprint
parameters and¡ 0.5 mb model size.
arXiv:1602.07360, 2016.

Bergstra, James, Komer, Brent, Eliasmith, Chris, Yamins,
Dan, and Cox, David D. Hyperopt: a python library for
model selection and hyperparameter optimization. Com-
putational Science & Discovery, 8(1):014008, 2015.

Jaeger, Herbert, Lukoˇseviˇcius, Mantas, Popovici, Dan, and
Siewert, Udo. Optimization and applications of echo
state networks with leaky-integrator neurons. Neural
networks, 20(3):335–352, 2007.

Boulanger-Lewandowski, Nicolas, Bengio, Yoshua, and
Vincent, Pascal. Modeling temporal dependencies in
high-dimensional sequences: Application to polyphonic
arXiv preprint
music generation and transcription.
arXiv:1206.6392, 2012.

Cho, Kyunghyun, Van Merri¨enboer, Bart, Bahdanau,
Dzmitry, and Bengio, Yoshua. On the properties of neu-
ral machine translation: Encoder-decoder approaches.
arXiv preprint arXiv:1409.1259, 2014.

Chung, Junyoung, Gulcehre, Caglar, Cho, KyungHyun,
and Bengio, Yoshua. Empirical evaluation of gated re-
current neural networks on sequence modeling. arXiv
preprint arXiv:1412.3555, 2014.

Donahue, Jeffrey, Anne Hendricks, Lisa, Guadarrama,
Sergio, Rohrbach, Marcus, Venugopalan, Subhashini,
Saenko, Kate, and Darrell, Trevor. Long-term recur-
rent convolutional networks for visual recognition and
In Proceedings of the IEEE conference
description.
on computer vision and pattern recognition, pp. 2625–
2634, 2015.

Jarrett, Kevin, Kavukcuoglu, Koray, LeCun, Yann, et al.
What
is the best multi-stage architecture for object
recognition? In Computer Vision, 2009 IEEE 12th In-
ternational Conference on, pp. 2146–2153. IEEE, 2009.

Koutnik, Jan, Greff, Klaus, Gomez, Faustino, and Schmid-
huber, Juergen. A clockwork rnn. pp. 1863–1871, 2014.

Le, Quoc V, Jaitly, Navdeep, and Hinton, Geoffrey E. A
simple way to initialize recurrent networks of rectiﬁed
linear units. arXiv preprint arXiv:1504.00941, 2015.

Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in

network. arXiv preprint arXiv:1312.4400, 2013.

Mikolov, Tomas, Joulin, Armand, Chopra, Sumit, Mathieu,
Michael, and Ranzato, Marc’Aurelio. Learning longer
memory in recurrent neural networks. arXiv preprint
arXiv:1412.7753, 2015.

Muandet, Krikamol, Fukumizu, Kenji, Sriperumbudur,
Bharath, and Sch¨olkopf, Bernhard. Kernel mean em-
bedding of distributions: A review and beyonds. arXiv
preprint arXiv:1605.09522, 2016.

The Statistical Recurrent Unit

M¨uller, Meinard. Information retrieval for music and mo-

tion, volume 2. Springer, 2007.

Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units
improve restricted boltzmann machines. In Proceedings
of the 27th international conference on machine learning
(ICML-10), pp. 807–814, 2010.

Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.
On the difﬁculty of training recurrent neural networks.
ICML (3), 28:1310–1318, 2013.

Ramsay, J.O. and Silverman, B.W. Applied functional data
analysis: methods and case studies, volume 77. Springer
New York:, 2002.

Smola, Alex, Gretton, Arthur, Song, Le, and Sch¨olkopf,
Bernhard. A hilbert space embedding for distributions.
In International Conference on Algorithmic Learning
Theory, pp. 13–31. Springer, 2007.

Vinyals, Oriol, Kaiser, Łukasz, Koo, Terry, Petrov, Slav,
Sutskever, Ilya, and Hinton, Geoffrey. Grammar as a
In Advances in Neural Information
foreign language.
Processing Systems, pp. 2773–2781, 2015.

Zaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol.
Recurrent neural network regularization. arXiv preprint
arXiv:1409.2329, 2014.

