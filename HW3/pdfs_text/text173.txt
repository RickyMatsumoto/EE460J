Consistency Analysis for Binary Classiﬁcation Revisited

Krzysztof Dembczy ´nski 1 Wojciech Kotłowski 1 Oluwasanmi Koyejo 2 Nagarajan Natarajan 3

Abstract

Statistical learning theory is at an inﬂection point
enabled by recent advances in understanding and
optimizing a wide range of metrics. Of partic-
ular interest are non-decomposable metrics such
as the F-measure and the Jaccard measure which
cannot be represented as a simple average over
examples. Non-decomposability is the primary
source of difﬁculty in theoretical analysis, and
interestingly has led to two distinct settings and
notions of consistency.
In this manuscript we
analyze both settings, from statistical and algo-
rithmic points of view, to explore the connec-
tions and to highlight differences between them
for a wide range of metrics. The analysis com-
plements previous results on this topic, clariﬁes
common confusions around both settings, and
provides guidance to the theory and practice of
binary classiﬁcation with complex metrics.

1. Introduction

Real-world applications of binary classiﬁcation to complex
decision problems have led to the design of a wide range of
evaluation metrics (Choi & Cha, 2010). Prominent exam-
ples include area under the ROC curve (AUC) for imbal-
anced labels (Menon et al., 2013), F-measure for informa-
tion retrieval (Lewis, 1995), and precision at the top (Kar
et al., 2014; 2015; Jasinska et al., 2016). To this end, sev-
eral algorithms have been proposed for optimizing many of
these metrics, primarily focusing on large-scale learning,
without a conscious emphasis on statistical consequences
of choosing models and their asymptotic behavior (Kar
et al., 2015; Joachims, 2005). Wide use of such complex
metrics has also re-invigorated research into their theoret-
ical properties, which can then serve as a guide to prac-

Authors listed in the alphabetical order 1Institute of Computing
Science, Poznan University of Technology, Poland 2Department
of Computer Science, University of
Illinois at Urbana-
Champaign, USA 3Microsoft Research, India. Correspondence
to: Wojciech Kotłowski <wkotlowski@cs.put.poznan.pl>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

tice (Koyejo et al., 2014a; Narasimhan et al., 2014a; Dem-
bczy´nski et al., 2012; Waegeman et al., 2014; Natarajan
et al., 2016).

Complex evaluation metrics for binary classiﬁcation are
best described as set metrics, or non-decomposable met-
rics – as, in general, the evaluation for a set of predictions
cannot be decomposed into the average of individual in-
stance evaluations. This is in contrast to decomposable
metrics such as accuracy which are deﬁned as the empir-
ical average of the instance evaluations. This property is
the primary source of difﬁculty in theoretical analysis, and
interestingly has led to two distinct settings and notions of
consistency. On one hand, Population Utility (PU) focuses
on estimation – so a consistent PU classiﬁer is one which
correctly estimates the population optimal utility as the size
of the training set (equiv. test set) increases. The PU ap-
proach has strongest roots in classical statistical analysis
which often deals with asymptotically optimal estimation.
On the other hand, Expected Test Utility (ETU) focuses on
generalization. Thus, the consistent ETU classiﬁer is one
which optimizes the expected prediction error over test sets
of a pre-deﬁned size. The ETU approach has strongest
roots in statistical machine learning which prizes general-
ization as the primary goal. Importantly, these distinctions
are irrelevant when the metric is a linear function of the
confusion matrix e.g. (weighted) accuracy and other linear
metrics. To the best of our knowledge, this dichotomy was
ﬁrst explicitly noted by Ye et al. (2012) in the context of
F-measure.1 Like in Ye et al. (2012), our goal is not to ad-
judicate the correctness of either approach, but instead to
explore deep connections, and highlight signiﬁcant differ-
ences between both approaches for a wide range of metrics.

Contributions: We present a variety of results compar-
ing and contrasting the PU and ETU approaches for con-
sistent classiﬁcation:

• We show that for a wide range of metrics, PU and
ETU are asymptotically equivalent with respect to the
size of the test set, subject to a certain p-Lipschitzness

1Note that Ye et al. (2012) termed the two approaches Em-
pirical Utility Maximization (EUM) and Decision Theoretic Ap-
proach (DTA), respectively. We have instead chosen the more de-
scriptive names Population Utility (PU) and Expected Test Utility
(ETU).

Consistency Analysis for Binary Classiﬁcation Revisited

condition which is satisﬁed by many metrics of inter-
est. This further implies asymptotic equivalence of the
Bayes optimal classiﬁers (Section 3.1). Similar results
were previously only known for F-measure.

• We provide lower bounds for the difference between
PU and ETU metrics for ﬁnite test sets, and for cer-
tain metrics – thereby highlighting the difference be-
tween PU and ETU consistent classiﬁers with small
test sets (Section 3.2).

• We analyze approximate ETU classiﬁcation using low
order Taylor approximations, showing that the ap-
proximation can be computed with effectively linear
complexity, yet achieves low error under standard as-
sumptions (Section 4.1).

• We consider the effects of model mis-speciﬁcation
and ﬁnd that ETU may be more sensitive than PU,
but this may be alleviated by properly calibrating the
estimated probabilities (Section 4.2).

In addition, we present experimental results using simu-
lated and real data to evaluate our theoretical claims (Sec-
tion 5).

2. Preliminaries and Problem Setup

We consider the binary classiﬁcation problem, where the
input is a feature vector x ∈ X, and the output is a label y ∈
{0, 1}. We assume the examples (x, y) are generated i.i.d.
according to P(x, y). A classiﬁer is a mapping h : X →
{0, 1}. We let 1C denote the indicator function i.e. equal
to one if C is satisﬁed, and zero otherwise.
Given a distribution P and a binary classiﬁer h, deﬁne:

TP(h) = P(h = 1, y = 1), TN(h) = P(h = 0, y = 0),
FP(h) = P(h = 1, y = 0), FN(h) = P(h = 0, y = 1),

which are entries of the so-called confusion matrix, namely
true positives, true negatives, false positives and false neg-
atives. In this paper, we are interested in optimizing perfor-
mance metrics Φ(h, P) (we use explicit dependence on P
because we will also consider the empirical version of Φ)
that are functions of the above four quantities. However,
since the entries of the confusion matrix are interdepen-
dent, it sufﬁces to only use their three independent combi-
nations. Following Natarajan et al. (2016), we parametrize
Φ(h, P) = Φ(u(h), v(h), p) by means of:

u(h) = TP(h),

v(h) = P(h = 1), and p = P(y = 1).

Metric

Deﬁnition

Φ(u, v, p)

Accuracy

TP + TN

AM

Fβ

Jaccard

G-Mean

AUC

TP/2

TP+FN + TN/2
(1+β2)TP
(1+β2)TP+β2FN+FP

TN+FP

TP
TP+FP+FN
(cid:113)

TP·TN
(TP+FN)(TN+FP)

FP·FN
(TP+FN)(FP+TN)

1 + 2u − v − p

u+p(1−v−p)
p(1−p)

(1+β2)u
β2p+v
p+v−2u
p+v−u

u(1−v−p+u)
p(1−p)

(v−u)(p−u)
p(1−p)

Table 1. Examples of performance metrics.

with explicit parameterization Φ(u, v, p). Throughout the
paper we assume Φ(u, v, p) is bounded from above and
from below.2

2.1. Formal Deﬁnitions of PU and ETU

Deﬁnition 1 (Population Utility (PU)). Given a distribu-
tion P and classiﬁer h, the PU of h for a performance met-
ric Φ is deﬁned as Φ(u(h), v(h), p). We let h∗
PU denote any
maximizer of the PU,

h∗
PU ∈ argmax

Φ(u(h), v(h), p) .

h

In words, the PU is obtained by taking the value of metric
Φ evaluated at the expected confusion matrix of h over P.
Thus, one can think of the PU as evaluating the classiﬁer h
on a “single test set of inﬁnite size” drawn i.i.d. from P.

In contrast, ETU evaluates the expected utility for a ﬁxed-
size test set. Formally, given a sample S = {(xi, yi)}n
i=1 of
size n, generated i.i.d. from P, we let (cid:98)u(h), (cid:98)v(h), (cid:98)p denote
the corresponding empirical quantities:

(cid:98)u(h) =

h(xi)yi, (cid:98)v(h) =

h(xi), (cid:98)p =

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

yi,

1
n

n
(cid:88)

i=1

value

empirical

of metric Φ is

and
the
Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p).
Deﬁnition 2 (Expected Test Utility (ETU)). Let x =
(x1, . . . , xn) ∈ X n be an arbitrary sequence of inputs.
Given a distribution P and a classiﬁer h, the ETU of h for
a performance metric Φ conditioned on x is deﬁned as:3

then

Ey|x

(cid:104)

Φ(cid:0)

(cid:98)u(h), (cid:98)v(h), (cid:98)p(cid:1)(cid:105)

,

As argued by Natarajan et al. (2016), any metric being a
function of the confusion matrix can be parameterized in
this way. Table 1 lists popular examples of such metrics

3The conditional expectation y|x is deﬁned up to a zero-
measure set (over x), but this does not create any problems as
we always consider x being sampled from the data distribution.

2In fact, for essentially all metrics used in practice it holds

0 ≤ Φ(u, v, p) ≤ 1.

Consistency Analysis for Binary Classiﬁcation Revisited

where the expectation over y = (y1, . . . , yn) is with re-
spect to the conditional distribution P(y|x) i.i.d. over the
examples. We let h∗
ETU(x) denote any maximizer of the
ETU,

h∗
ETU(x) ∈ argmax

Ey|x

h

(cid:104)

Φ(cid:0)

(cid:98)u(h), (cid:98)v(h), (cid:98)p(cid:1)(cid:105)

.

One can think of ETU as evaluating the classiﬁer h on “in-
ﬁnitely many test sets of size n” drawn i.i.d. from P. We
will see (in Section 4) that the optimal predictions (in both
PU and ETU approaches) can be accurately estimated us-
ing the conditional probabilities P(yi|xi). In practice, we
ﬁrst obtain an estimator of the conditional probability and
then compute the optimal predictions on test data based on
their conditional probability estimates.

(cid:104)
Φ(cid:0)

Remark 1. More generally, ETU optimizes the expected
utility Ey,x
. However, clearly, it is suf-
ﬁcient to analyze the predictions at any given x (Natarajan
et al., 2016) as in Deﬁnition 2.

(cid:98)u(h), (cid:98)v(h), (cid:98)p(cid:1)(cid:105)

2.2. Well-behaved Performance Metrics

The two frameworks treat the metrics as utility measures
(i.e., they are to be maximized). Further, it is reasonable to
expect that Φ(h, P) is non-decreasing in true positive and
true negative rates (and indeed, virtually all performance
measures used in practice behave this way). As shown by
Natarajan et al. (2016), such monotonicity in true positive
and true negative rates implies another property, called TP
monotonicity, which is better suited to the parameterization
employed here.

Deﬁnition 3 (TP monotonicity). Φ(u, v, p) is said to be
TP monotonic if for any v, p and u1 > u2, it holds that
Φ(u1, v, p) > Φ(u2, v, p).

It is easy to verify that all measures in Table 1 are TP mono-
tonic.

A contribution in this work is to develop a notion of regu-
larity for metrics, that helps establish statistical connections
between the two frameworks and their optimal classiﬁers.
We call it p-Lipschitzness, deﬁned next.

Deﬁnition 4 (p-Lipschitzness). Φ(u, v, p) is said to be
p-Lipschitz if:

|Φ(u, v, p)−Φ(u(cid:48), v(cid:48), p(cid:48))| ≤ Up|u−u(cid:48)|+Vp|v−v(cid:48)|+Pp|p−p(cid:48)|,

for any feasible u, v, p, u(cid:48), v(cid:48), p(cid:48). The Lipschitz constants
Up, Vp, Pp are allowed to depend on p, in contrast to the
standard Lipschitz functions.

The rationale behind p-Lipschitzness is that we want to
control the change in value of the measure under small

changes in their arguments. This property turns out to be
essential to show equivalence between ETU and PU ap-
proaches. On the other hand, if we simply used a standard
deﬁnition of Lipschitz function (with global constants),
it would not be satisﬁed by many interesting measures.
Hence, we weaken the Lipschitz property by allowing the
constant to vary as a function of p. One can also show that
general linear-fractional performance metrics studied in
(Koyejo et al., 2014a; Narasimhan et al., 2015; Kotłowski
& Dembczy´nski, 2016) satisfy p-Lipschitzness under mild
conditions (Appendix A).

Proposition 1. All measures in Table 1 are p-Lipschitz.

Proof. We only give a proof for Fβ-measure here (See Ap-
pendix A for the rest). For ease, let us denote Fβ(u, v, p) by
Fβ and Fβ(u(cid:48), v(cid:48), p(cid:48)) by F (cid:48)
β. Let ∆u = u−u(cid:48), ∆v = v−v(cid:48),
∆p = p − p(cid:48). We have:

|Fβ − F (cid:48)

β| = (1 + β2)

|u(β2p(cid:48) + v(cid:48)) − u(cid:48)(β2p + v)|
(β2p + v)(β2p(cid:48) + v(cid:48))

= (1 + β2)

(cid:18)

≤

1 + β2
β2p+v

|∆u(β2p(cid:48) + v(cid:48)) − u(cid:48)β2∆p − u(cid:48)∆v|
(β2p + v)(β2p(cid:48) + v(cid:48))

|∆u| +

β2u(cid:48)
β2p(cid:48) +v(cid:48) |∆p| +

u(cid:48)

β2p(cid:48) +v(cid:48) |∆v|

(cid:19)

.

Since u(cid:48) ≤ min{p(cid:48), v(cid:48)}, we have
1, and thus we can choose Up = Vp = Pp = 1+β2
β2p .

β2u(cid:48)
β2p(cid:48)+v(cid:48) ≤ 1,

u(cid:48)

β2p(cid:48)+v(cid:48) ≤

As for an example of a metric which is not p-Lipschitz,
consider the precision deﬁned as Φ(u, v, p) = u
v . Indeed,
if v is close to zero, choosing v(cid:48) = 2v, u(cid:48) = u and p(cid:48) = p
gives:

Φ(u, v, p) − Φ(u(cid:48), v(cid:48), p(cid:48)) =

u
2v

,

which can be arbitrarily large for sufﬁciently small v, while
the difference |v − v(cid:48)| = v is small. As it turns out in Sec-
tion 3.2, this pathological behavior of the precision metric
is responsible for a large deviation between PU and ETU,
which suggests that p-Lipschitzness is in some sense nec-
essary to establish connections.

3. Equivalence of PU and ETU

Most of
the existing literature on optimizing non-
decomposable classiﬁcation metrics focus on one of the
two approaches in isolation. In this section, we show that
the two approaches are in fact asymptotically equivalent,
for a range of well-behaved metrics. Informally, given a
distribution P and a performance metric Φ, our ﬁrst result is
that for sufﬁciently large n, the PU of the associated h∗
is arbitrarily close to that of h∗
PU is arbitrarily close to that of h∗
h∗

ETU
PU, and likewise, the ETU of
ETU. In contrast, we also

Consistency Analysis for Binary Classiﬁcation Revisited

show that the PU and ETU optimal classiﬁers may suffer
differences for small samples.

We are now ready to state the result. Proofs omitted in the
main text are supplied in the Appendix.

3.1. Asymptotic Equivalence

The intuition behind the equivalence lies in the observation
that the optimal classiﬁers under the two approaches ex-
hibit a very simple, similar form, under mild assumptions
on the distribution (Koyejo et al., 2014b; Narasimhan et al.,
2014b; Natarajan et al., 2016). Let η(x) := P(y = 1|x) de-
note the conditional probability of positive class as a func-
tion of x. The following lemma shows that for any ﬁxed
classiﬁer h that thresholds η(x), and sufﬁciently large sam-
ple size n, its performance measured with respect to PU
and ETU are close, in particular, differ by a factor that de-
cays as fast as ˜O(1/
n). In fact, the result holds uniformly
over all such binary classiﬁers.
Lemma 1. Let H = {h | h = 1η(x)≥τ , τ ∈ [0, 1]}, be
the class of thresholded binary decision functions. Let
Φ be a performance metric which is p-Lipschitz. Then,
with probability at least 1 − δ over a random sample
i=1 of size n generated i.i.d. from P, it holds
S = {(xi, yi)}n
uniformly over all h ∈ H,

√

(cid:12)
(cid:12)Φ(u(h),v(h), p(h)) − Ey|x
(cid:12)

(cid:2)Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p(h))(cid:3) (cid:12)
Lp√
n

log 4
δ
2n

+ 3Lp

(cid:115)

+

(cid:12)
(cid:12)

,

(cid:114)

≤ 4Lp

2 log(n + 1)
n

where (cid:98)u(h), (cid:98)v(h), (cid:98)p(h) are empirical quantities evaluated
on S, and Lp = max{Up, Vp, Pp}.
Remark 2. Lemma 1 generalizes the result obtained by Ye
et al. (2012) for Fβ-measure to arbitrary p-Lipschitz met-
rics. Furthermore, using more careful bounding technique,
we are able to get a better dependence on the sample size
n, essentially ˜O(1/
n) (neglecting logarithmic terms). In
fact, this dependence cannot be improved any further in
general (See Appendix B).

√

The uniform convergence result in Lemma 1 enables the
ﬁrst main result of this work. In particular, the convergence
holds when the optimal classiﬁers with respect to ETU
and PU are of the thresholded form, i.e. h∗
PU ∈ H, and
h∗
ETU(x) ∈ H almost surely (with respect to random sam-
ple of inputs x), where H = {h | h = 1η(x)≥τ , τ ∈ [0, 1]}
is the class of threshold functions on function η(x). Sev-
eral recent results have shown that the optimal classiﬁer for
many popular metrics (including all metrics in Table 1) in-
deed has the thresholded form (Narasimhan et al., 2014a;
Lewis, 1995), under a mild condition related to continuity
of the distribution of η(x) (See the proof of Theorem 1 in
Appendix B.2 for details):

Assumption 1. The random variable η(x) has a density
(with respect to the Lebesgue measure) on [0, 1].

Theorem 1. Let Φ be a performance metric that is TP
monotonic and p-Lipschitz, and P be a distribution satis-
fying Assumption 1. Consider the ETU optimal classiﬁer
h∗
ETU (Deﬁnition 2) and the PU optimal classiﬁer h∗
PU (Def-
inition 1). Then, for any given (cid:15) and δ, we can choose
n large enough (in Deﬁnition 2 of ETU), such that, with
probability at least 1 − δ over the random choice of the
sample of inputs x, we have:

(cid:12)
(cid:12)Φ(cid:0)u(h∗
(cid:12)

ETU(x)),v(h∗

ETU(x)), p(cid:1)

− Φ(cid:0)u(h∗

PU), v(h∗

PU), p(cid:1)(cid:12)
(cid:12)
(cid:12) ≤ (cid:15).

Similarly, for large enough n, with probability 1 − δ,

Ey|x

(cid:104)

Φ(cid:0)

(cid:12)
(cid:12)
(cid:12)

−Ey|x

ETU(x)), (cid:98)v(h∗
(cid:98)u(h∗
(cid:104)
Φ(cid:0)
PU), (cid:98)v(h∗
(cid:98)u(h∗

ETU(x)), (cid:98)p(cid:1)(cid:105)
PU), (cid:98)p(cid:1)(cid:105) (cid:12)
(cid:12)
(cid:12) ≤ (cid:15).

Remark 3. In essence, Theorem 1 suggests that, for large
sample sizes, the optimal in the sense of one approach
gives an accurate estimate (or a proxy) of the optimal in
the sense of the other approach. Our characterization of
p-Lipschitzness is key to showing the equivalence.

3.2. Finite Sample Regime

The aforementioned result is asymptotic; to elucidate the
point, we now give an example where optimal classiﬁers
corresponding to PU and ETU differ.
It is important to
be aware of such extremities, especially when one applies
a learned model to test data of modest sample sizes. The
way we argue a lower bound is by specifying a metric and
a distribution, such that on a randomly obtained test set of
modest size, say m, the gap in the empirical metric com-
puted on the test data for the two optimal classiﬁers can be
large. As one is typically primarily interested in the em-
pirical metric on a given test set, focusing on the empirical
metric ensures fairness and forbids favoring either deﬁni-
tion.

Example. For some constant α > 0, consider the (ad-
justed) empirical precision metric deﬁned as:

ΦPrec((cid:98)u(h(x)), (cid:98)v(h(x)), p) = (cid:98)u(h(x))
(cid:98)v(h(x)) + α

.

1
1+α ]; Furthermore, it is p-Lipschitz,
Note that ΦPrec ∈ [0,
with Lipschitz constant Vp ∝ 1
α (see Deﬁnition 4). Thus,
choosing very small values of α implies very high Lipschitz
constant, and in turn the metric becomes less “stable”. To
establish the desired lower bound, we choose a small 0 <

Consistency Analysis for Binary Classiﬁcation Revisited

α (cid:28) 1. Let {X1, X2, X3} denote a partition of the instance
space X , i.e. ∪3
i=1Xi = X and Xi ∩ Xj = 0, for any pair
(i, j). Consider the joint distribution P deﬁned as:

P(y = 1|x ∈ X1) = 1

P(y = 1|x ∈ X3) = 0,

P(y = 1|x ∈ X2) = 1 − (cid:15) = 1 −

(1)

√

α,
P(X2) = 1 − (cid:15)2.

P(X1) + P(X3) = (cid:15)2

,

,

for some 1 (cid:29) (cid:15)2 > 0 and note that the distribution is de-
ﬁned to be dependent on our choice of α. The last line in
the above set of equations suggests that the distribution has
a small region where labels are deterministically positive
or negative, but overwhelmingly positive elsewhere.
Theorem 2. Let x = {x1, x2, . . . , xn} denote a set of
from the distribution P. Let y =
instances drawn i.i.d.
{y1, y2, . . . , yn} denote their labels drawn from the same
distribution. With probability at least (1 − (cid:15)2 − (cid:15)n),

ΦPrec((cid:98)u(h∗

ETU(x)), (cid:98)v(h∗
PU(x)), (cid:98)v(h∗

ΦPrec((cid:98)u(h∗

ETU(x)), ˆp) −

PU(x)), ˆp) ≥

1
n(1 + α)

.

n

(cid:80)n

i=1 yi, (cid:98)u0 = 0

i = 0, ∀i ∈ [n], (cid:98)p = 1

Algorithm 1 Approximate ETU Consistent Classiﬁer
1: Input: Φ and sorted estimates of ηi, i = 1, 2, . . . , n
2: Init s∗
3: Set Φ0 = Φ(0, 0, (cid:98)p)
4: for k = 1, 2, . . . , n do
5:
6:
7: end for
8: k∗ ← arg maxk=0,...,n Φk.
9: return s∗ s.t. s∗

Set (cid:98)uk = (k−1)(cid:98)uk−1+ηk
Set Φk = Φ((cid:98)uk, (cid:98)vk, (cid:98)p) (via Lemmas 2 or 3)

i ← 1 for i ∈ [k∗].

, (cid:98)vk = k

n

k

measure, which is time consuming – requiring O(n2) in
general. Our goal here is to approximate this term, so that
it can be computed in O(n) time, then the whole procedure
can be implemented in amortized time O(n).

Fix a binary classiﬁer h : X → {0, 1} and the input sample
x = (x1, . . . , xn). Let (cid:98)u(h), (cid:98)v(h), (cid:98)p denote the empirical
quantities, as deﬁned in Section 3.1. Furthermore, we de-
ﬁne semi-empirical quantities:

4. Algorithms: Optimization and Conditional

(cid:101)u(h) =

h(xi)η(xi),

and (cid:101)p =

η(xi)

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

Probability Estimation

Characterization of the optimal classiﬁer as a thresholding
of the conditional probability yields simple and efﬁcient
PU consistent estimators. The idea is to ﬁrst obtain an es-
timator for the conditional probability using training data,
and then search for an optimal threshold on a separate vali-
dation set (Narasimhan et al., 2014b; Koyejo et al., 2014b).
Threshold search can be efﬁciently implemented in linear
time (assuming probabilities are pre-sorted). In contrast,
although a similar thresholding characterization exists for
ETU (Natarajan et al., 2016), evaluation and prediction re-
quire the computation of an expensive expectation (Deﬁni-
tion 2). For general metrics, there is an O(n3) procedure
to determine the optimal test set labeling (Jansche, 2007;
Chai, 2005; Natarajan et al., 2016), and the procedure can
be sped up to O(n2) in some special cases (Ye et al., 2012;
Natarajan et al., 2016). Here, we consider an approxi-
mation to ETU that requires only O(n) computation, yet
achieves error O(n−3/2) compared to exact optimization.

4.1. Approximation Algorithms

Recall that ETU seeks to ﬁnd the classiﬁer of the form:

h∗
ETU(x) = argmax

Ey|x

(cid:2)Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p)(cid:3) .

h

Following (Lewis, 1995; Natarajan et al., 2016) we know
that when Φ is TP monotonic, it sufﬁces to sort observa-
tions in decreasing order according to η(x) and assign pos-
itive labels to top k of them, for k = 0, . . . , n. Unfortu-
nately, for each k, we need to calculate the expected utility

(there is no need to deﬁne (cid:101)v(h)). Note that (cid:101)u(h) =
Ey|x

(cid:98)u(h)(cid:3), and (cid:101)p = Ey|x [(cid:98)p].
(cid:2)

Zeroth-order approximation. Our ﬁrst approximation
is based on Taylor-expanding the measure up to the second
order:

Lemma 2. If Φ is twice-differentiable in (u, p) and all its
second-order derivatives are bounded by constant A, then:

(cid:12)
(cid:12)Ey|x

(cid:2)Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p)(cid:3) − Φ((cid:101)u(h), (cid:98)v(h), (cid:101)p)(cid:12)

(cid:12) ≤

A
2n

.

We note that the ﬁrst order terms vanish in the Taylor ap-
proximation (proof in Appendix). This constitutes a sim-
ple, yet powerful method for approximating ETU utility.
Algorithm 1 outlines the resulting algorithm. As shown,
the classiﬁer can be computed in O(n) time overall, assum-
ing the data is already sorted according to η(xi) (otherwise,
the procedure is dominated by sorting time O(n log n)).
We note that (Lewis, 1995) proposed a similar ﬁrst order
approximation, albeit without any rigorous guarantee.

Second order approximation. Naturally, we can get a
better approximation by Taylor-expanding the measure up
to the third order.

Lemma 3. Assume Φ is three times differentiable in (u, p)
and assume all its third-order derivatives are bounded by
up, ∇2
constant B. Let ∇2
pp denote the second-order

uu, ∇2

Consistency Analysis for Binary Classiﬁcation Revisited

derivative terms evaluated at ((cid:101)u, (cid:101)p), and likewise deﬁne
∇2

pp. We then have:

up, ∇2

(cid:12)
(cid:12)Ey|x

(cid:2)Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p)(cid:3) − Φappr(h)(cid:12)

(cid:12) ≤

B
3n3/2

,

where:

and

Φappr(h) = Φ((cid:101)u(h), (cid:98)v(h), (cid:101)p)
1
(∇2
2

uu + 2∇2

+

up)su + ∇2

ppsp,

sp :=

η(xi)(1 − η(xi)),

su :=

h(xi)η(xi)(1 − η(xi)).

1
n2

1
n2

n
(cid:88)

i=1
n
(cid:88)

i=1

Theorem 3 (Consistency). Given n instances x =
(x1, x2, . . . , xn), sort them in decreasing order of η(xi).
For 0 ≤ k ≤ n, let s(k) denote the vector with positions
corresponding to top k of the sorted instances set to 1,
and 0 otherwise.
(a) Suppose ﬁrst order derivatives are
bounded by A, let:

h∗
a = arg max
s(k)

Φ((cid:101)u(s(k)), (cid:98)v(s(k)), (cid:101)p),

We have:

Φ(h∗

ETU) − Φ((cid:101)u(h∗

a), (cid:98)v(h∗

a), (cid:101)p) ≤

A
2n

.

(b) Suppose second order derivatives are bounded by B,
let:

h∗
b = arg max
s(k)

Φappr(s(k)),

where Φappr(h) is deﬁned in Lemma 3. We have:

Φ(h∗

ETU) − Φappr(h∗

b ) ≤

2B
3n3/2

.

As before, the approximation can be computed in O(n) to-
tal time. We could also expand the function up to orders
higher than the third order, and get better approximations
(still with O(n) computation if the order of the expansion
is independent of n) at the cost of an even more compli-
cated approximation formula. In experiments, we ﬁnd that
on real datasets with test data sets of size 100 or more, even
the zeroth order approximation is highly accurate.

4.2. Conditional Probability Estimation and Model

Misspeciﬁcation

classiﬁer is a threshold function on η(x). In practice, one
employs some probability estimation procedure and gets
(cid:98)η(x), which we call a model.4 Then, one uses (cid:98)η(x) as if
it were a true conditional probability η(x) to obtain PU or
ETU classiﬁers. Note that since η(x) is unknown, and we
only have access to (cid:98)η(x), the best we can hope for is to
choose the optimal threshold on (cid:98)η(x) (for PU) or choose
the optimal number of test set observations k to be clas-
siﬁed as positive after sorting them according to (cid:98)η(x) (for
ETU). Next, we investigate these ﬁnite sample effects in
practical PU and ETU procedures. For this analysis, we
treat (cid:98)η(x) as given and ﬁxed, make no other assumptions
on how it was obtained. Let (cid:98)H = {h | h = 1
(cid:98)η(x)≥τ , τ ∈
[0, 1]} denote the class of binary threshold functions on
(cid:98)η(x).
Consider PU ﬁrst, and let h∗ be the PU-optimal classiﬁer
from (cid:98)H, i.e.:

h∗ = argmax

Φ(u(h), v(h), p).

h∈ (cid:98)H

In practice, however, one does not have access to P, and
thus u(h), v(h), p cannot be computed.
Instead, given
(cid:98)η(x), one uses a validation sample S = {(xi, yi)}n
i=1 to
choose a threshold on (cid:98)η(x) (and thus, a classiﬁer from (cid:98)H),
by directly optimizing the empirical version of the metric
on S:

(cid:98)h = argmax

Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p).

h∈ (cid:98)H

We would like to assess how close is (cid:98)h to h∗. By following
the proof of Lemma 1 (which never assumes the class H
is based on thresholding η(x)), it is easy to show that with
high probability,

(cid:12)Φ(u((cid:98)h), v((cid:98)h), p) − Φ(u(h∗), v(h∗), p)(cid:12)
(cid:12)

(cid:12) ≤ O

(cid:16) 1
√
n

(cid:17)

.

Thus, if we have a sufﬁciently large validation sample at
our disposal, we can set the threshold which maximizes
the empirical version of the metric, and our performance
n) close to the performance of
is guaranteed to be (cid:101)O(1/
the Φ-optimal classiﬁer from (cid:98)H. In other words, PU does
not require to know the true distribution in order to select
the best classiﬁer in (cid:98)H, only a sufﬁciently large validation
sample is required.

√

In contrast, ETU procedure is inherently based on using
(cid:98)η(x) as a replacement for η(x) (which we do not know)
to decide upon label assignments. Let x = (x1, . . . , xn)
be the input sample of size n. Assume for simplicity the
distribution of η(x) and (cid:98)η(x) are continuous on [0, 1], so
that for any i (cid:54)= j, η(xi) (cid:54)= η(xj) with probability one, and

So far, we assumed that we have access to the true class
conditional density η(x) = P(y = 1|x) and the resulting

4For instance, (cid:98)η(x) could be obtained from logistic regression

or neural network with soft-max function on the ﬁnal layer.

Consistency Analysis for Binary Classiﬁcation Revisited

similarly for (cid:98)η. Then, given x and (cid:98)η, the ETU procedure
chooses the classiﬁer of the form:

(cid:98)h = argmax

h∈ (cid:98)H

Ey∼(cid:98)η(x)

(cid:2)Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p)(cid:3) .

Likewise, the optimal ETU classiﬁer in (cid:98)H is given by:

h∗ = argmax

Ey∼η(x)

(cid:2)Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p)(cid:3) ,

h∈ (cid:98)H

i.e. by deﬁnition, the optimal classiﬁer in the restricted
class H involves the expectation with respect to the true
(cid:2)Φ((cid:98)u(h), (cid:98)v(h), (cid:98)p)(cid:3), so
η. Let us denote ΦETU = Ey∼η(x)
that h∗ maximizes ΦETU. In the supplementary material,
we show that under some mild assumptions on Φ:

Ex

(cid:104)(cid:12)
(cid:12)ΦETU((cid:98)h) − ΦETU(h∗)(cid:12)
(cid:12)

(cid:105)

≤ ˜O

(cid:17)

(cid:16) 1
√
n

+ Pp|p − p

(cid:98)η|

+ sup
h∈ (cid:98)H

Up|u(h) − u

(cid:98)η(h)|,

(cid:98)η = E (cid:2)

(cid:98)η(x)(cid:3) and u

(cid:98)η(h) = E (cid:2)h(x)(cid:98)η(x)(cid:3), are the
where p
quantities corresponding to p and u(h), which were cal-
culated by replacing the conditional probability η with its
estimate (cid:98)η. Thus, while for the PU procedure, the differ-
ence between (cid:98)h and h∗ diminishes as n grows, it is not
the case of ETU, as there are two bias terms |p − p
(cid:98)η| and
(cid:98)η(h)| which do not depend on n. These terms cor-
|u(h)−u
respond to using incorrect conditional probability (cid:98)η while
selecting the classiﬁer, and are present even if the sample
size tends to inﬁnity. Thus, it seems crucial for the success
of ETU procedure to have (cid:98)η calibrated with respect to the
true distribution.

A popular choice for class probability estimation is to use
logistic regression. However, if the model is misspeciﬁed,
which happens often in practice, the aforementioned dis-
cussion suggests that the desired ETU solution may not be
achieved. Therefore, we need to learn the class probability
function more carefully. Here, we consider two variants.

The ﬁrst is to use the Isotron algorithm.
In case of the
generalized linear model, i.e. P(y|x) = γ(w∗, x) for some
unknown link function γ and model w∗, Kalai & Sastry
(2009) proposed a simple and elegant algorithm (see Ap-
pendix E) that alternatively learns w∗ and the link function
γ (approximated by a piecewise linear function). It prov-
ably learns the model under certain assumptions on P. The
model w∗ and link function γ are learned using training
data, and at prediction time, the link function and the scores
of training data (i.e., xT
i w) are used to calibrate the class
probabilities η(x) of test instances.

We also consider using a recalibrated logistic model, i.e.,
we ﬁrst estimate the class probabilities via standard logis-
tic regression, and recalibrate the probabilities by running
one update of the γ function in Isotron algorithm (which

essentially solves a quadratic problem known as the Pool
of Adjacent Violators). At test time, we use the learnt γ
and the logistic model to estimate η(x) for test instances.

5. Experiments

We empirically evaluate the effectiveness and accuracy of
ETU approximations introduced in Section 4.1, on syn-
thetic as well as real datasets. We also show on several
benchmark datasets that, by carefully calibrating the con-
ditional probabilities in ETU, we can improve the classiﬁ-
cation performance.

5.1. Convergence of Approximations

We consider F1 and Jaccard metrics from Table 1. We
sample conditional probabilities ηi for n instances from the
uniform distribution. The optimal predictions (see Deﬁni-
tion 2) are obtained using Algorithm 1 of (Natarajan et al.,
2016) (which is equivalent to searching over 2n possible
label vectors). Then we compute the approximate optimal
predictions using the ﬁrst and the second order approxima-
tions discussed in Section 4.1. For each metric, we measure
the deviation between the true and the approximate optimal
values with increasing sample size in Figure 1. We ob-
serve linear convergence for the ﬁrst order approximation
and quadratic convergence for the second order approxi-
mation. This suggests that the bounds in Theorem 3 indeed
can be improved for some metrics, if not in general.

(a) F1

(b) Jaccard

Figure 1. Convergence of approximations demonstrated on syn-
thetic data.

5.2. Approximations on Real Data

We report results on seven multiclass and multilabel bench-
mark datasets: (1) LETTERS: 16000 train, 4000 test in-
stances, (2) SCENE: 1137 train, 1093 test (3) YEAST: 1500
train, 917 test (4) WEBPAGE: 6956 train, 27824 test (5)
IMAGE: 1300 train, 1010 test (6) BREAST CANCER: 463
train, 220 test instances, (7) SPAMBASE: 3071 train, 1530
test instances.5 In case of multiclass datasets, we report re-
sults (using one-vs-all classiﬁers) averaged over classes (as

5See (Koyejo et al., 2014b; Ye et al., 2012) for details.

n020406080100|F(app)1 - F1*|10-610-410-2Second order approximationFirst order approximationn020406080100|Jacc(app) - Jacc*|10-610-510-410-310-2Second order approximationFirst order approximationConsistency Analysis for Binary Classiﬁcation Revisited

DATASET

LETTERS (26)
SCENE (6)
YEAST (14)
WEB PAGE
SPAMBASE
IMAGE
BREAST CANCER

Exact Approx (ﬁrst) Approx (second)
F1
F1
0.5666
0.5666
0.6916
0.6917
0.4493
0.4494
0.6336
0.6336
0.8448
0.8457
0.8542
0.8542
0.9660
0.9660

F1
0.5666
0.6916
0.4493
0.6336
0.8448
0.8542
0.9660

Exact Approx (ﬁrst) Approx (second)
Jaccard
0.4273
0.5374
0.3242
0.4637
0.7314
0.7455
0.9342

Jaccard
0.4272
0.5376
0.3242
0.4637
0.7326
0.7455
0.9342

Jaccard
0.4273
0.5374
0.3242
0.4637
0.7314
0.7455
0.9342

Table 2. Comparison of ETU approximation methods: F1 and Jaccard metrics deﬁned in Table 1. Reported values correspond to
performance evaluated on heldout data (higher values are better). For multiclass datasets (number of classes indicated in parenthesis),
average performance over classes is reported. Evidently, the ETU approximations are accurate to at least 4 decimal digits in several
cases.

DATASET

LETTERS (26)
SCENE (6)
YEAST (14)
WEB PAGE
SPAMBASE
IMAGE
BREAST CANCER

Logistic
F1
0.5666
0.6916
0.4493
0.6336
0.8448
0.8542
0.9660

Isotron Recalibrated
Logistic: F1
0.5722
0.6809
0.4629
0.6756
0.8905
0.8569
0.9799

F1
0.5905
0.6222
0.4666
0.7362
0.7825
0.8683
0.9669

PU
F1
0.5745
0.4397
0.4730
0.6809
0.8839
0.8581
0.9766

Logistic
Jaccard
0.4273
0.5374
0.3242
0.4637
0.7314
0.7455
0.9342

Isotron
Jaccard
0.4447
0.4673
0.3399
0.5825
0.5729
0.7673
0.9359

Recalibrated
Logistic: Jaccard
0.4304
0.5362
0.3414
0.5037
0.7837
0.7704
0.9605

PU
Jaccard
0.4318
0.4318
0.3422
0.5194
0.8003
0.7623
0.9481

Table 3. Modeling conditional probabilities in ETU: Logistic model vs calibrated model using Isotron: F1 and Jaccard metrics deﬁned
in Table 1. Reported values correspond to performance evaluated on heldout data (higher values are better). For multiclass datasets
(number of classes indicated in parenthesis), average performance over classes is reported.

in Natarajan et al. (2016)).

We compare the exact ETU optimal, computed using the
algorithm of Natarajan et al. (2016), with the approxima-
tions. The results for F1 and Jaccard metrics are presented
in Table 2. The results convincingly show that the approx-
imations are highly accurate, and almost always indistin-
guishable from optimizing true metrics, on real datasets.
Note that even the ﬁrst-order approximation (in fact, this is
zeroth-order, as the ﬁrst order term is zero; see Section 4)
achieves high accuracy, as the test set sizes are relatively
large.

5.3. Model Misspeciﬁcation

We now study how class probability estimation (CPE) and
model misspeciﬁcation affects the performances of PU
and ETU approaches, on the seven benchmark datasets.
We compare four methods: (a) ETU with logistic regres-
sion based CPE, (b) ETU with Isotron based CPE (dis-
cussed in Section 4.1), (c) ETU with recalibrated logis-
tic regression based CPE (discussed in Section 4.1), and
(d) PU using logistic regression based CPE followed by
threshold tuning on validation set (Koyejo et al., 2014b).
Additional comparisons to structured SVM (Joachims,
2005) and other classiﬁers are available in previously pub-
lished work by others (Koyejo et al., 2014b; Natarajan
et al., 2016), and are omitted here.

The results are presented in Table 3. We observe that the
logistic model (column 1) is insufﬁcient for many of the
datasets. The results improve in several cases using the
estimated generalized linear model with Isotron (column
2). However, there is a confounding factor that the two al-
gorithms are very different, and noticed improvement may
not necessarily be due to better CPE. To isolate this, re-
calibrated logistic model results are presented in column
3. The results are in general much better than the stan-
dard logistic model, which suggests that it is indeed the
case of model misspeciﬁcation in these datasets. Finally,
we present the results with PU algorithm in column 4. We
ﬁnd that the results closely match that of the recalibrated
logistic model (except in the case of SCENE dataset); thus,
correcting for model misspeciﬁcation helps demonstrate
the theorized asymptotic equivalence of PU and ETU ap-
proaches in practice.

6. Conclusions and Future Work

We have presented new results which elucidate the rela-
tionship between the two notions of consistency for com-
plex binary classiﬁcation metrics. Next, we plan to ex-
plore surrogates to further improve training efﬁciency non-
decomposable metrics. We will also extend to more com-
plex prediction problems such as multilabel classiﬁcation,
where a similar dichotomy exists.

Consistency Analysis for Binary Classiﬁcation Revisited

Acknowledgments

W. Kotłowski has been supported by the Polish National
Science Centre under Grant No. 2013/11/D/ST6/03050.

References

Chai, Kian Ming Adam.

Expectation of F-measures:
tractable exact computation and some empirical observa-
tions of its properties. In Proceedings of the 28th Annual
Intl. ACM SIGIR Conf. on Research and Development in
Information Retrieval, pp. 593–594. ACM, 2005.

Choi, Seung-Seok and Cha, Sung-Hyuk. A survey of bi-
nary similarity and distance measures. Journal of Sys-
temics, Cybernetics and Informatics, pp. 43–48, 2010.

Dembczy´nski, Krzysztof, Waegeman, Willem, Cheng,
Weiwei, and H¨ullermeier, Eyke. On label dependence
and loss minimization in multi-label classiﬁcation. Ma-
chine Learning, 88(1-2):5–45, 2012.

Jansche, Martin. A maximum expected utility framework
for binary sequence labeling. In Annual Meeting of the
Association of Computational Linguistics, volume 45,
pp. 736, 2007.

Jasinska, Kalina, Dembczynski, Krzysztof, Busa-Fekete,
R´obert, Pfannschmidt, Karlson, Klerx, Timo, and
H¨ullermeier, Eyke. Extreme f-measure maximization
In Proceedings of
using sparse probability estimates.
the 33nd International Conference on Machine Learn-
ing, ICML 2016, New York City, NY, USA, June 19-24,
2016, pp. 1435–1444, 2016.

Joachims, Thorsten. A support vector method for multivari-
ate performance measures. In Proceedings of the 22nd
Intl. Conf. on Machine Learning, pp. 377–384. ACM,
2005.

Kalai, Adam and Sastry, Ravi. The Isotron algorithm:
High-dimensional isotonic regression. In Conference on
Learning Theory (COLT), 2009.

Kar, Purushottam, Narasimhan, Harikrishna, and Jain, Pra-
teek. Online and stochastic gradient methods for non-
decomposable loss functions. In Advances in Neural In-
formation Processing Systems, pp. 694–702, 2014.

Kar, Purushottam, Narasimhan, Harikrishna, and Jain, Pra-
teek. Surrogate functions for maximizing precision at
the top. In Proceedings of the 32nd International Con-
ference on Machine Learning (ICML-15), pp. 189–198,
2015.

Koyejo, Oluwasanmi, Natarajan, Nagarajan, Ravikumar,
Pradeep K., and Dhillon, Inderjit S. Consistent binary
classiﬁcation with generalized performance metrics. In
Neural Information Processing Systems (NIPS), 2014a.

Koyejo, Oluwasanmi O, Natarajan, Nagarajan, Ravikumar,
Pradeep K, and Dhillon, Inderjit S. Consistent binary
classiﬁcation with generalized performance metrics. In
Advances in Neural Information Processing Systems, pp.
2744–2752, 2014b.

Lewis, David D. Evaluating and optimizing autonomous
In Proceedings of the 18th
text classiﬁcation systems.
Intl. ACM SIGIR Conf. on Research and Development in
Information Retrieval, pp. 246–254. ACM, 1995.

Menon, Aditya, Narasimhan, Harikrishna, Agarwal, Shiv-
ani, and Chawla, Sanjay. On the statistical consistency
of algorithms for binary classiﬁcation under class imbal-
ance. In Proceedings of the 30th Intl. Conf. on Machine
Learning, pp. 603–611, 2013.

Mohri, Mehryar, Rostamizadeh, Afshin, and Talwalkar,
Ameet. Foundations of Machine Learning. The MIT
Press, 2012.

Narasimhan, Harikrishna, Vaish, Rohit, and Agarwal, Shiv-
ani. On the statistical consistency of plug-in classiﬁers
for non-decomposable performance measures. In Neural
Information Processing Systems (NIPS), 2014a.

Narasimhan, Harikrishna, Vaish, Rohit, and Agarwal, Shiv-
ani. On the statistical consistency of plug-in classiﬁers
In Ad-
for non-decomposable performance measures.
vances in Neural Information Processing Systems, pp.
1493–1501, 2014b.

Narasimhan, Harikrishna, Ramaswamy, Harish, Saha,
Aadirupa, and Agarwal, Shivani. Consistent multiclass
algorithms for complex performance measures. In Pro-
ceedings of the 32nd Intl. Conf. on Machine Learning,
pp. 2398–2407, 2015.

Natarajan, Nagarajan, Koyejo, Oluwasanmi, Ravikumar,
Pradeep, and Dhillon, Inderjit. Optimal classiﬁcation
with multivariate losses. In Proceedings of The 33rd In-
ternational Conference on Machine Learning, pp. 1530–
1538, 2016.

Waegeman, Willem, Dembczynski, Krzysztof, Jachnik,
Arkadiusz, Cheng, Weiwei, and H¨ullermeier, Eyke. On
the bayes-optimality of F-measure maximizers. Journal
of Machine Learning Research, 15:3333–3388, 2014.

Kotłowski, Wojciech and Dembczy´nski, Krzysztof. Sur-
rogate regret bounds for generalized classiﬁcation per-
formance metrics. Machine Learning Journal, DOI
10.1007/s10994-016-5591-7, 2016.

Ye, Nan, Chai, Kian Ming A, Lee, Wee Sun, and Chieu,
Hai Leong. Optimizing F-measures: a tale of two ap-
proaches. In Proceedings of the Intl. Conf. on Machine
Learning, 2012.

