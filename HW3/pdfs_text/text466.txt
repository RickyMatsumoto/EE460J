Algebraic Variety Models for High-Rank Matrix Completion

Greg Ongie 1 Rebecca Willett 2 Robert D. Nowak 2 Laura Balzano 1

Abstract
We consider a generalization of low-rank matrix
completion to the case where the data belongs to
an algebraic variety, i.e., each data point is a solu-
tion to a system of polynomial equations. In this
case the original matrix is possibly high-rank,
but it becomes low-rank after mapping each col-
umn to a higher dimensional space of monomial
features. Many well-studied extensions of lin-
ear models, including afﬁne subspaces and their
union, can be described by a variety model, as
well as a rich class of nonlinear quadratic and
higher degree curves and surfaces. We study the
sampling requirements for matrix completion un-
der a variety model with a focus on a union of
afﬁne subspaces. We also propose an efﬁcient
matrix completion algorithm that minimizes a
convex or non-convex surrogate of the rank of
the matrix of monomial features, using the well-
known “kernel trick” to avoid working directly
with the high-dimensional monomial matrix. We
show the proposed algorithm is able to recover
synthetically generated data up to the predicted
sampling complexity bounds, and outperforms
standard low rank matrix completion and sub-
space clustering algorithms in experiments with
real data.

1. Introduction

Work in the last decade on matrix completion has shown
that it is possible to leverage linear structure in order to in-
terpolate missing values in a low-rank matrix (Candes &
Recht, 2012). The high-level idea of this work is that if
the data deﬁning the matrix belongs to a structure having
fewer degrees of freedom than the entire dataset, that struc-
ture provides redundancy that can be leveraged to complete

1Department of EECS, University of Michigan, Ann Arbor,
Michigan, USA 2Department of ECE, University of Wiscon-
sin, Madison, Wisconsin, USA. Correspondence to: Greg Ongie
<gongie@umich.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

the matrix. The assumption that the matrix is low-rank is
equivalent to assuming the data lies on (or near) a low-
dimensional linear subspace.

It is of great interest to generalize matrix completion to
exploit low-complexity nonlinear structures in the data.
Several avenues have been explored in the literature, from
generic manifold learning (Lee et al., 2013), to unions of
subspaces (Eriksson et al., 2012; Elhamifar & Vidal, 2013),
to low-rank matrices perturbed by a nonlinear monotonic
function (Ganti et al., 2015; Song et al., 2016).
In each
case missing data has been considered, but there lacks a
clear, unifying framework for these ideas.

In this work we study the problem of completing a ma-
trix whose columns belong to an algebraic variety, i.e., the
set of solutions to a system of polynomial equations (Cox
et al., 2015). This is a strict generalization of the linear
(or afﬁne) subspace model, which can be written as the set
of points satisfying a system of linear equations. Unions
of subspaces and unions of afﬁne spaces are also algebraic
varieties. Plus, a much richer class of non-linear curves,
surfaces, and their unions, are captured by a variety model.

The matrix completion problem using a variety model can
be formalized as follows. Let X = (cid:2)x1, . . . , xs
(cid:3) ∈ Rn×s
be a matrix of s data points where each column xi ∈ Rn.
Deﬁne φd : Rn → RN as the mapping that sends the
vector x = (x1, ..., xn) to the vector of all monomials in
x1, ..., xn of degree at most d, and let φd(X) denote the
matrix that results after applying φd to each column of X,
which we call the lifted matrix. We will show the lifted ma-
trix is rank deﬁcient if and only if the columns of X belong
to an algebraic variety. This motivates the following matrix
completion approach:

rank φd( ˆX) such that PΩ( ˆX) = PΩ(X)

(1)

min
ˆX

where PΩ(·) represents a projection that restricts to some
observation set Ω ⊂ {1, . . . , n} × {1, . . . , s}. The rank of
φd( ˆX) depends on the choice of the polynomial degree d
and the underlying “complexity” of the variety, in a sense
we will make precise. Figure 1 shows three examples of
datasets that have low-rank in the lifted space for different
polynomial degrees d.

In this work we investigate the factors that inﬂuence the
sampling complexity of varieties as well as algorithms for

Algebraic Variety Models for High-Rank Matrix Completion

completion. The challenges are (a) to characterize varieties
having low-rank (and therefore few degrees of freedom) in
the lifted space, i.e., determine when φd(X) is low-rank,
(b) devise efﬁcient algorithms for solving (1) that can ex-
ploit these few degrees of freedom in a matrix completion
setting, and (c) determine the trade-offs relative to exist-
ing matrix completion approaches. This work contributes
considerable progress towards these goals.

certain deterministic conditions. However, these methods
and theory are restricted to low-rank linear models. A great
deal of real data exhibit nonlinear structure, and so it is of
interest to generalize this approach. Work in that direc-
tion has dealt with union of subspaces models (Eriksson
et al., 2012; Yang et al., 2015; Elhamifar, 2016; Pimentel-
Alarc´on et al., 2016a; Pimentel-Alarcon & Nowak, 2016),
locally linear approximations (Lee et al., 2013), as well as
low-rank models perturbed by an arbitrary nonlinear link
function (Ganti et al., 2015; Song et al., 2016; Rao et al.,
2017). In this paper we instead seek a more general model
that captures both linear and nonlinear structure. The va-
riety model has as instances low-rank subspaces and their
union as well as quadratic and higher degree curves and
surfaces.

Work on kernel PCA (cf., (Sanguinetti & Lawrence, 2006;
Nguyen & Torre, 2009)) leverage similar geometry to
ours. In Kernel Spectral Curvature Clustering (Chen et al.,
2009), the authors similarly consider clustering of data
points via subspace clustering in a lifted space using ker-
nels. These works are algorithmic in nature, with promis-
ing numerical experiments, but do not systematically con-
sider missing data or analyze relative degrees of freedom.

This paper also has close ties to algebraic subspace clus-
tering (ASC) (Vidal et al., 2003; 2005; 2016; Tsakiris &
Vidal, 2015), also known as generalized PCA. Similar to
our approach, the ASC framework models unions of sub-
spaces as an algebraic variety, and makes use of monomial
liftings of the data to identify the subspaces. Characteriza-
tions of the rank of data belonging to union of subspaces
under the monomial lifting are used in the ASC framework
(Vidal et al., 2016) based on results in (Derksen, 2007).
The difference of the results in (Derksen, 2007) and those
in Prop. 1 is that ours hold for monomial liftings of all de-
grees d, not just d ≥ k, where k is the number of subspaces.
Also, the main focus of ASC is to recover unions of sub-
spaces or unions of afﬁne spaces, whereas we consider data
belonging to a more general class of algebraic varieties. Fi-
nally, the ASC framework has not been adapted to the case
of missing data, which is the main focus of this work.

2. Variety Models

As a toy example to illustrate our approach, consider a ma-
trix

X =

(cid:18)x1,1 x1,2
x2,1 x2,2

· · ·
· · ·

(cid:19)

x1,6
x2,6

∈ R2×6

whose six columns satisfy the quadratic equation

c0+c1 x1,i+c2 x2,i+c3 x2

1,i+c4 x1,ix2,i+c5 x2

2,i = 0 (2)

for i = 1, . . . , 6 and some unknown constants c0, ..., c5 that
are not all zero. Generically, X will be full rank. However,

Figure 1. Data belonging to algebraic varieties in R3. The original
data is full rank, but a nonlinear embedding of the matrix to a
feature space consisting of monomials of degree at most d is low-
rank with rank R, indicating the data has few degrees of freedom.

Our main contributions are as follows. We identify bounds
on the rank of a matrix φd(X) when the columns of the
data matrix X belong to an algebraic variety. We study
how many entries of such a matrix should be observed in
order to recover the full matrix from an incomplete sample.
We show as a case study that monomial representations
produce low-rank representations of unions of subspaces,
and we characterize the rank. The standard union of sub-
space representation as a discrete collection of individual
subspaces is inherently non-smooth in nature, whereas the
algebraic variety allows for a purely continuous parameter-
ization. This leads to a general algorithm for completion of
a data matrix whose columns belong to a variety. The al-
gorithm’s performance is showcased on data simulated as a
union of subspaces, a union of low-dimensional parametric
surfaces, and real data from a motion segmentation dataset
and a motion capture dataset. The simulations show that
the performance of our algorithm matches our predictions
and outperforms other methods. In addition, the analysis of
the degrees of freedom associated with the proposed repre-
sentations introduces several new research avenues at the
intersection of nonlinear algebraic geometry and random
matrix theory.

1.1. Related work

There has been a great deal of research activity on matrix
completion problems since (Candes & Recht, 2012), where
the authors showed that one can recover an incomplete ma-
trix from few entries using a convex relaxation of the rank
minimization optimization problem. For example, it is now
known that only O(rn) entries are necessary and sufﬁcient
(Pimentel-Alarc´on et al., 2016b) for almost every n × n
rank r matrix as long as the measurement pattern satisﬁes

-0.5100.5d=2,R=500.50-1-0.5-1101d=2,R=7010-1-1-1100.5d=3,R=5010-1-0.5Algebraic Variety Models for High-Rank Matrix Completion

suppose we vertically expand each column of the matrix to
make a 6 × 6 matrix

When X = [x1, ..., xs] is an n × s matrix, we use φd(X)
to denote the N × s matrix [φd(x1), ..., φd(xs)].

Y =







1
x1,1
x2,1
x2

x2

2,1

1
x1,2
x2,2
x2

x2

2,2

···
···
···
···

···

1
x1,6
x2,6
x2

x2

2,6







1,1

1,2
x1,1x2,1 x1,2x2,2 ··· x1,6x2,6

1,6

∈ R6×6,

1,i, x1,ix2,i, x2

i.e., we augment each column of X with a 1 and with the
quadratic monomials x2
2,i. This allows us
to re-express the polynomial equation (2) as the matrix-
vector product Y T c = 0 where c = (c0, c1, .., c5)T .
In other words, Y is rank deﬁcient. Suppose, for exam-
ple, that we are missing entry x1,1 of X. Since X is
full rank, there is no way to uniquely complete the miss-
ing entry by leveraging linear structure alone.
Instead,
we ask: Can we complete x1,1 using the linear structure
present in Y ? Due to the missing entry x1,1, the ﬁrst col-
umn of Y will having the following pattern of missing en-
tries: (1, −, x2,1, −, −, x2
2,1)T . However, assuming the ﬁve
complete columns in Y are linearly independent, we can
uniquely determine the nullspace vector c up to a scalar
multiple. Then from (2) we have

The problem we consider is this: can we complete a par-
tially observed matrix X under the assumption that φd(X)
is low-rank? This can be posed as the optimization prob-
lem given above in (1). We give a practical algorithm for
solving a relaxation of (1) in Section 4. Similar to previ-
ous work cited above on using polynomial feature maps,
our method leverages the kernel trick for efﬁcient compu-
tations. The success of this optimization and its relaxations
will depend on many factors, but clearly the rank of φd(X)
and the number of sampled entries will play an important
role. The number of samples, rank, and dimensions all
grow in the mapping to feature space, but they grow at dif-
ferent rates depending on the underlying geometry; it is not
immediately obvious what conditions on the geometry and
sampling rates impact our ability to determine the missing
entries. In the remainder of this section, we show how to
relate the rank of φd(X) to the underlying variety, and we
study the sampling requirements necessary for the comple-
tion of the matrix in feature space.

c3 x2

1,1 + (c1 + c4 x2,1)x1,1 = −c0 − c2 x2,1 − c5 x2

2,1.

2.2. Rank properties

In general, this equation will yield at most two possibil-
ities for x1,1. Moreover, there are conditions where we
can uniquely recover x1,1, namely when c3 = 0 and
c1 + c4 x2,1 (cid:54)= 0.

This example shows that even without a priori knowledge
of the particular polynomial equation satisﬁed by the data,
it is possible to uniquely recover missing entries in the orig-
inal matrix by leveraging induced linear structure in the
matrix of expanded monomials. We now show how to con-
siderably generalize this example to the case of data be-
longing to an arbitrary algebraic variety.

2.1. Formulation
Let X = (cid:2)x1, . . . , xs
(cid:3) ∈ Rn×s be a matrix of s data points
where each column xi ∈ Rn. Deﬁne φd : Rn → RN as
the mapping that sends the vector x = (x1, ..., xn) to the
vector of all monomials in x1, ..., xn of degree at most d:

φd(x) = (xα)|α|≤d ∈ RN

1 · · · xαn

where α = (α1, ..., αn) is a multi-index of non-negative
integers, with xα := xα1
n , and |α| := α1+· · ·+αn.
In the context of kernel methods in machine learning, the
map φd is often called a polynomial feature map (Muller
et al., 2001). Borrowing this terminology, we call φd(x) a
feature vector, the entries of φd(x) features, and the range
of φd feature space. Note that the number of features is
given by N = N (n, d) = (cid:0)n+d
(cid:1), the number
of unique monomials in n variables of degree at most d.

(cid:1) = (cid:0)n+d

n

d

To better understand what determines the rank of the ma-
trix φd(X), we introduce some additional notation and
concepts from algebraic geometry. Let R[x] denote the
space of all polynomials with real coefﬁcients in n vari-
ables x = (x1, ..., xn). We model a collection of data as
belonging to a real (afﬁne) algebraic variety (Cox et al.,
2015), which is deﬁned as the common zero set of a sys-
tem of polynomials P ⊂ R[x]:

V (P ) = {x ∈ Rn : f (x) = 0 for all f ∈ P }.

Suppose the variety V (P ) is deﬁned by the ﬁnite set of
polynomials P = {f1, ..., fq}, where each fi has degree at
most d. Let C ∈ RN ×q be the matrix whose columns are
given by the vectorized coefﬁcients (cα,i)|α|≤d of the poly-
nomials fi(x), i = 1, ..., q in P . Then the columns of X
belong to the variety V (P ) if and only if φd(X)T C = 0.
In particular, assuming the columns of C are linearly inde-
pendent, this shows that φd(X) has rank ≤ min(N − q, s).
In particular, when the number of data points s > N − q,
then φd(X) is rank deﬁcient.

However, the exact rank of φd(X) could be much smaller
than min(N − q, s), especially when the degree d is large.
This is because the coefﬁcients c of any polynomial that
vanishes at every column of X satisﬁes φd(X)T c = 0.
We will ﬁnd it useful to identify this space of coefﬁcients
with a ﬁnite dimensional vector space of polynomials. Let
Rd[x] be the space of all polynomials in n real variables of
degree at most d. We deﬁne the vanishing ideal of degree
d corresponding to a set X ⊂ Rn, denoted by Id(X ), to be

Algebraic Variety Models for High-Rank Matrix Completion

subspace of polynomials belonging to Rd[x] that vanish at
all points in X :

Id(X ) := {f ∈ Rd[x] : f (x) = 0 for all x ∈ X }.

We also deﬁne the non-vanishing ideal of degree d corre-
sponding to X, denoted by Sd(X ), to be the orthogonal
complement of Id(X ) in Rd[x]:

Sd(X ) := {g ∈ Rd[x] : (cid:104)f, g(cid:105) = 0 for all f ∈ Id(X )},

where the inner product (cid:104)f, g(cid:105) of polynomials f, g ∈ Rd[x]
is deﬁned as the inner product of their coefﬁcient vectors.
Hence, the rank R of φd(X) can expressed in terms of the
dimension of non-vanishing ideal of degree d correspond-
ing to X = {x1, ...., xs}, the set of all columns of X.
Speciﬁcally, we have rank φd(X) = min(R, s) where

R = dim Sd(X ) = N − dim Id(X ) .

In general the dimension of the space Id(X ) or Sd(X ) is
difﬁcult to determine when X is an arbitrary set of points.
However, if we assume X is a subset of a variety V , then
Id(V ) ⊆ Id(X ) and hence

rank φd(X) ≤ dim Sd(V ).

In certain cases dim Sd(V ) can be computed exactly or
bounded using properties of the polynomials deﬁning V .
For example, it is possible to compute the dimension of
Sd(V ) directly from a Gr¨obner basis for the vanishing
ideal associated with V (Cox et al., 2015).
In Section 3
we show how to bound the dimension of Sd(V ) in the case
where V is a union of subspaces.

2.3. Sampling rate

Informally, the degrees of freedom of a class of objects is
the minimum number of free variables needed to describe
an element in that class uniquely. For example, a n×s rank
r matrix has r(n + s − r) degrees of freedom: nr parame-
ters to describe r linearly independent columns making up
a basis of the column space, and r(s − r) parameters to de-
scribe the remaining s − r columns in terms of this basis.
It is impossible to uniquely complete a matrix in this class
if we sample fewer than this many entries.

We can make a similar argument to specify the minimum
number of samples needed to uniquely complete a matrix
that is low-rank when mapped to feature space. First, we
characterize how missing entries of the data matrix trans-
late to missing entries in feature space. For simplicity, we
will assume a sampling model where we sample a ﬁxed
number of entries m from each column of the original data
matrix. Let x ∈ Rn represent a single column of the data
matrix, and Ω ⊂ {1, ..., n} with m = |Ω| denote the in-
dices of the sampled entries of x. The pattern of revealed

entries in φd(x) corresponds to the set of multi-indices:

{α = (α1, ..., αn) : |α| ≤ d, αi = 0 for all i ∈ Ωc},

which has the same cardinality as the set of all monomials
(cid:1). If we call
of degree at most d in m variables, i.e., (cid:0)m+d
this quantity M , then the ratio of revealed entries in φd(x)
to the feature space dimension is

d

M
N

=

(cid:1)
(cid:0)m+d
d
(cid:1) =
(cid:0)n+d
d

(m + d)(m + d − 1) · · · (m + 1)
(n + d)(n + d − 1) · · · (n + 1)

,

which is on the order of ( m
we have the bounds

n )d for small d. More precisely,

(cid:17)d

(cid:16) m
n

≤

M
N

≤

(cid:18) m + d
n

(cid:19)d

.

(3)

In total, observing m entries per column of the data matrix
translates to M entries per column in feature space. Sup-
pose the N × s lifted matrix φd(X) is rank R. By the
preceding discussion, we need least R(N + s − R) entries
of the feature space matrix φd(X) to complete it uniquely
among the class of all N × s matrices of rank R. Hence, at
minimum we need to satisfy

M s ≥ R(N + s − R).

(4)

Let m0 denote the minimal value of m such that M =
(cid:1) achieves the bound (4), and set M0 = (cid:0)m0+d
(cid:1). Di-
(cid:0)m+d
d
viding (4) through by the feature space dimension N and s
gives

d

M0
N

≥

(cid:18) R
N

(cid:19) (cid:18) N + s − R

(cid:19)

s

=

(cid:18) R
s

+

R
N

(cid:18)

1 −

R
s

(cid:19)(cid:19)

,

(5)

and so from (3) we see we can guarantee this bound with

ρ0 :=

m0
n

≥

(cid:18) R
s

+

R
N

(cid:18)

1 −

R
s

(cid:19)(cid:19) 1

d

,

(6)

and this in fact will result in tight satisfaction of (5) because
(M0/N )

d ≈ m0/n for small d and large n.

1

At one extreme where the matrix φd(X) is full rank, then
R/s = 1 or R/N = 1 and according to (6) we need
ρ0 ≈ 1, i.e., full sampling of every data column. At
the other extreme where instead we have many more data
points than the feature space rank, R/s (cid:28) 1, then (6) gives
the asymptotic bound ρ0 ≈ (R/N )

1
d .

The above discussion bounds the degrees of freedom of
a matrix that is rank-R in feature space. Of course, the
proposed variety model has potentially fewer degrees of
freedom than this, because additionally the columns of the
lifted matrix are constrained to lie in the image of the fea-
ture map. We use the above bound only as a rule of thumb

Algebraic Variety Models for High-Rank Matrix Completion

for sampling requirements on our matrix. Furthermore, we
note that sample complexities for standard matrix comple-
tion often require that locations are observed uniformly at
random, whereas in our problem the locations of obser-
vations in the lifted space will necessarily be structured.
However, there is recent work that shows matrix com-
pletion can suceed without these assumptions (Pimentel-
Alarc´on et al., 2016b; Chen et al., 2014) that gives reason
to believe random samples in the original space may allow
completion in the lifted space, and our empirical results in
Section 5 support this rationale.

3. Case Study: Union of Afﬁne Subspaces

A union of afﬁne subspaces can be modeled as an
For example, with (x, y, z) ∈ R3,
algebraic variety.
the plane z = 1 and the line
the union of
x = y is the zero-set of the quadratic polynomial
In general, if A1, A2 ⊂ Rn are afﬁne
(z − 1)(x − y).
spaces of dimension r1 and r2, respectively,
then we
can write A1 = {x : fi(x) = 0 for i = 1, ..., n − r1} and
A2 = {x : gi(x) = 0 for i = 1, ..., n − r2} where the fi
and gi are afﬁne functions. The union A ∪ B can be ex-
pressed as the common zero set of all possible products of
the fi and gi, i.e., A1 ∪ A2 is the common zero set of a
system of (n − r1)(n − r2) quadratic equations. Similarly,
a union of k afﬁne subspaces of dimensions r1, ..., rk is a
variety described by a system of (cid:81)k
i=1(n − ri) polynomial
equations of degree k.

In this section we establish bounds on the feature space
rank for data belonging to a union of afﬁne subspaces. We
will make use of the following lemma that shows the di-
mension of a vanishing ideal is ﬁxed under an afﬁne change
of variables:
Lemma 1. Let T : Rn → Rn be an afﬁne change of vari-
ables, i.e., T (x) = Ax + b, where b ∈ Rn and A ∈ Rn×n
is invertible. Then for any S ⊂ Rn,

dim Id(S) = dim Id(T (S)).

(7)

We omit the proof for brevity, but the result is elemen-
tary and relies on the fact the degree of a polynomial is
unchanged under an afﬁne change of variables. Our next
result establishes a bound on the feature space rank for a
single afﬁne subspace:
Proposition 1. If the columns of a matrix X n×s belong to
an afﬁne subspace of dimension at most r, then

rank φd(X) ≤

,

for all d ≥ 1.

(8)

(cid:19)

(cid:18)r + d
d

Proof. By Lemma 1, dim Id(A) is preserved under an
afﬁne transformation of A. Note that we can always ﬁnd
an afﬁne change of variables y = Ax + c with invert-
ible A ∈ Rn×n and c ∈ Rn such that in the coordinates

y = (y1, ..., yn) the variety A becomes

A = {(y1, . . . , yr, 0, . . . , 0) : y1, ..., yr ∈ R}.

(9)

For any polynomial f (y) = (cid:80)
|α|≤d cαyα, the only mono-
mial terms in f (y) that do not vanish on A are those of
the form yα1
r . Furthermore, any polynomial in just
these monomials that vanishes on all of A must be the zero
polynomial, since the y1, ..., yr are free variables. Hence,

1 · · · yαr

Sd(A) = span{yα1

1 · · · yαr
r

: α1 + · · · + αr ≤ d}

(10)

d

(cid:1), proving the claim.

i.e., the non-vanishing ideal coincides with the space of
polynomials in r variables of degree at most d, which has
dimension (cid:0)r+d
We note that for s sufﬁciently large, the bound in (8) be-
comes an equality, provided the data points are distributed
generically within the afﬁne subspace, meaning they are
not the solution to additional non-trivial polynomial equa-
tions of degree at most d.

d

Proposition 1 shows that points belonging to a single afﬁne
subspace of dimension r are mapped to a linear subspace of
(cid:1) under φd. Therefore, if the columns of a
dimension (cid:0)r+d
data matrix are drawn from a union of k afﬁne subspaces of
dimension r, their image under φd will belong to a union of
(cid:1). The
k linear subspaces each of dimension at most (cid:0)r+d
linear span of this union has dimension at most k(cid:0)r+d
(cid:1),
which yields the following result:
Proposition 2. If the columns of a matrix X n×s belong to
a union of k afﬁne subspaces each of dimension at most r,
then

d

d

rank φd(X) ≤ k

for all d ≥ 1.

(11)

(cid:18)r + d
d

(cid:19)
,

In some cases the bound (11) is (nearly) tight. For exam-
ple, if the data lies on the union of two r-dimensional afﬁne
subspaces A and B that are mutually orthogonal, one can
show1 rank φd(X) = 2(cid:0)r+d
(cid:1) − 1. Empirically, we observe
that the bound in (11) is order-optimal with respect to k, r,
and d.
In this case, the feature space rank to dimension
ratio is R/N = O(k (cid:0) r
). Recall that the minimum sam-
pling rate is approximately (R/N ) 1
d for s (cid:29) R. Hence the
mininum number of samples per column m should be

(cid:1)d

n

d

m ≈ O(k

1
d r).

(12)

This rate is favorable to low-rank matrix completion ap-
proaches, which need m = O(kr) for a union of k sub-
spaces having dimension r. At ﬁrst glance, this bound sug-
gests it is always better to take the degree d as large as
possible. However, this is only true for sufﬁciently large s.

1The rank is one less than the bound in (11) because Sd(A) ∩
Sd(B) has dimension one, coinciding with the space of constant
polynomials.

Algebraic Variety Models for High-Rank Matrix Completion

To take advantage of the improved sampling rate implied
by (12), according to (6) we need the number of data vec-
tors per subspace to be O(rd). In other words, our model
is able to accommodate more subspaces with larger d but
at the expense of requiring exponentially more data points
per subspace. Note that if the number of data points is suf-
ﬁciently large, we could take d = log k and require only
m ≈ O(r) observed entries per column. In this case, for
moderately sized k (e.g., k ≤ 20) we should choose d = 2
or 3. In fact, we ﬁnd that for these values of d we get excel-
lent empirical results for the recovery of union of subspaces
data, as shown in Section 5.

4. Algorithm

There are several existing matrix completion algorithms
that could potentially be adapted to solve a relaxation of
the rank minimization problem (1), such as singular value
thresholding (Cai et al., 2010), or alternating minimization
(Jain et al., 2013). However, these approaches do not easily
lend themselves to “kernelized” implementations, i.e., ones
that do not require forming the high-dimensional lifted ma-
trix φd(X) explicitly, but instead make use of the efﬁciently
computable kernel function for polynomial feature maps 2

kd(x, y) := φd(x)T φd(y) = (xT y + 1)d.

(13)

For matrices X = [x1, ..., xs], Y = [y1, ..., ys] ∈ Rn×s,
we use kd(X, Y ) to denote the matrix whose (i, j)-th entry
is kd(xi, yj), or equivalently,

kd(X, Y ) := φd(X)T φd(Y ) = (X T Y + 1)(cid:12)d,

(14)

where 1 ∈ Rs×s is the matrix of all ones, and (·)(cid:12)d de-
notes the entrywise d-th power of a matrix. A kernelized
implentation is critical for even modest sizes of d, since the
number of rows of the lifted matrix scales exponentially
with d.

One class of algorithm that kernelizes very naturally is the
iterative reweighted least squares (IRLS) approach of (For-
nasier et al., 2011; Mohan & Fazel, 2012) for low-rank ma-
trix completion. The algorithm also has the advantage of
being able to accommodate the non-convex Schatten-p re-
laxation of the rank penalty, in addition to the convex nu-
clear norm relaxation. Speciﬁcally, we use an IRLS ap-
proach to solve the following variety-based matrix comple-
tion (VMC) optimization problem:

min
X

(cid:107)φd(X)(cid:107)p
Sp

s.t. PΩ(X) = PΩ(X0),

(VMC)

2Strictly speaking, kd is not kernel associated with the poly-
nomial feature map φd as deﬁned in (2.1). Instead, it is the kernel
√
of the related map ˜φd(x) := {
cαxα : |α| ≤ d} where cα are
appropriately chosen multinomial coefﬁcients.

Algorithm 1 Kernelized IRLS to solve (VMC).
Require: Initialize X = X0, γ = γ0. Choose η,γmin.

while not converged do

Step 1: Inverse power of kernel matrix
K ← kd(X, X)
(V , S) = eig(K).
W ← V (S + γI)

2 −1V T

p

2

Step 2: Projected gradient descent step
τ ← γ1− p
X ← X − τ X(W (cid:12) kd−1(X, X))
X ← PΩ(X0) + PΩc (X)
γ ← max{γ/η, γmin}

end while

where (cid:107)Y (cid:107)Sp is the Schatten-p quasi-norm deﬁned as

(cid:107)Y (cid:107)Sp := (cid:0) (cid:80)

i σi(Y )p(cid:1) 1
with σi(Y ) denoting the ith singular value of Y . Algo-
rithm 1 gives the pseudo-code of the proposed IRLS algo-
rithm for solving (VMC), which we derive below.

p , 0 < p ≤ 1

(15)

First, consider the simpler problem of minimizing the
Schatten-p norm of a matrix variable Y belonging to a con-
straint set C. The main idea behind the IRLS approach is
re-express the Schatten-p quasi-norm as

(cid:107)Y (cid:107)p
Sp

= tr[(Y T Y )

2 ] = tr[(Y T Y )W ],

p

(16)

p

where W := (Y T Y )
2 −1. Note if W is treated as con-
stant, then (16) is a smooth, quadratic function of Y . This
motivates the following iterative approach:

Wn = (Y T
Yn+1 = arg min

n Yn + γn)

tr[(Y T Y )Wn].

p
2 −1

Y ∈C

Here γn is a sequence of smoothing parameters satisfying
γn → γmin as n → ∞, where γmin is close to zero, which
is included to improve numerical stability and avoid local
minima; this is equivalent to minimizing a smooth approx-
imation of the Schatten-p cost (Mohan & Fazel, 2012).

Making the substitution Y = φd(X) in the above deriva-
tion, gives the following approach for solving (VMC):

Wn = (k(Xn, Xn) + γnI)

p
2 −1

Xn+1 = arg min

tr[k(X, X)Wn] s.t. PΩ(X) = PΩ(X0)

X

Rather than ﬁnding the exact minimum in the X up-
following the approach
date, which could be costly,
in (Mohan & Fazel, 2012), we instead take a sin-
gle projected gradient descent step to update X. A
the gradient of
straightforward calculation shows that

Algebraic Variety Models for High-Rank Matrix Completion

(a) Union of Subspaces

(b) Parametric Data

Figure 2. Phase transitions for matrix completion of synthetic variety data. In (a) we simulate data belonging to a union of k subspaces
for varying k. In (b) we simulate data belonging union of few parametric curves and surfaces having known feature space rank R. We
randomly undersample each column of the data matrix at the rate m/n. The grayscale values 0–1 indicate the fraction of random trials
where the columns of the data matrix were successfully recovered up to the speciﬁed percentage (white is success, black is failure). In
all ﬁgures the red dashed line indicates the predicted minimal sampling rate ρ0 = m0/n determined by (4).

the objective F (X) = tr[k(X, X)W ]
is given by
∇F (X) = X(W (cid:12) kd−1(X, X)), where (cid:12) denotes an
entry-wise product. Hence a projected gradient step with
step-size τn > 0 is given by

˜Xn = Xn − τnXn(Wn (cid:12) kd−1(Xn, Xn))
Xn = PΩ(X0) + PΩc( ˜Xn).

Similar to (Mohan & Fazel, 2012), one can show that ev-
ery limit point of the above iterates converges to a station-
ary point of a smoothed Schatten-p cost for appropriate
choices of step-sizes τn. Heuristics are given in (Mohan
& Fazel, 2012) for updating the smoothing parameter γn,
which we adopt as well. Speciﬁcally, we set γn = γ0/ηn,
where γ0 and η are user-deﬁned parameters, and update
τn = γ1−p/2
. The appropriate choice of γ0 and η will
depend on the scaling and spectral properties of the data.
Empirically, we ﬁnd that setting γ0 = (0.1)dλmax, where
λmax is the largest eigenvalue of the kernel matrix obtained
from the initialization, and η = 1.01 work well in a vari-
ety of settings. For all our experiments in Section 5 we ﬁx
p = 1/2, which was found to give the best matrix recovery
results for synthetic data. We also use a zero-ﬁlled initial-
ization X0 in all cases.

n

5. Numerical Experiments

5.1. Empirical validation of sampling bounds

In Figure 2 we report the results of two experiments to vali-
date the predicted minimum sampling rate ρ0 in (4) on syn-
thetic variety data.
In the ﬁrst experiment we generated
n × s data matrices whose columns belong to a union of k
subspaces each of dimension r (with n = 15, s = 100k,

r = 3). In the second experiment we generated data ma-
trices of size 20 × 300 whose columns belong to a union
of randomly generated parametric surfaces of low dimen-
sion, where we sorted each dataset by its empirically de-
termined feature space rank R. For both experiments, we
undersampled each column of the matrix taking m entries
uniformly at random at various values of k and R, and then
attempted to recover the missing entries using our proposed
IRLS algorithm for VMC (Algorithm 1 with p = 1/2) for
d = 2, 3. For the union of subspaces data, we also com-
pare with low-rank matrix completion in the original ma-
trix domain via nuclear norm minimization (LRMC) and
non-convex Schatten-1/2 minimization (LRMC-NCVX),
implemented using Algorithm 1 with a linear kernel (d = 1
in (13)). We said a column was successfully recovered if
(cid:107)x − x0(cid:107)/(cid:107)x0(cid:107) ≤ 10−5, where x is the recovered column
and x0 is the original column. For each pair of parameters
(m, k) or (m, R) we perform 10 random trials to determine
the probability of successful recovery.

Consistent with our theory, VMC is successful at recover-
ing most of the data columns above the predicted minimum
sampling rate, substantially extending the range of recov-
ery over LRMC. While VMC often fails to recover 100% of
the columns near the predicted rate, in fact a large propor-
tion of the columns (%99–%90) are still successfully com-
pleted. Sometimes the recovery dips below the predicted
rate (e.g., VMC, d = 2 in Fig. 2(a) and VMC, d = 3 in
Fig. 2(b)). However, since the predicted rate relies on what
is likely an over-estimate of the true degrees of freedom, it
is not surprising that the VMC algorithm occasionally suc-
ceeds below this rate, too.

Algebraic Variety Models for High-Rank Matrix Completion

VMC approach outperforms LRMC-NCVX for appropri-
ately chosen degree d. In particular, VMC with d = 2, 3
perform similar for small missing rates, but VMC d = 2
gives lower completion error over d = 3 for large missing
rates, consistent with the results in Figure 2.

5.2. Motion segmentation of real data

In Figure 3 we apply VMC to the problem of motion seg-
mentation (Kanatani, 2001) with missing data using the
Hopkins 155 dataset (Tron & Vidal, 2007). This data con-
sists of several feature points tracked across frames of the
video. We reproduce the experimental setting in (Yang
et al., 2015), and simulate high-rank data by undersam-
pling frames of the dataset. We simulate missing trajec-
tories by sampling uniformly at random from the feature
points across all frames. To obtain a clustering we ﬁrst
completed the missing entries using VMC and then ran
the sparse subspace clustering (SSC) algorithm (Elhami-
far & Vidal, 2009) on the result, calling this VMC+SSC.
A similar approach of standard LRMC followed by SSC
(LRMC+SSC) provides a consistent baseline for subspace
clustering with missing data (Yang et al., 2015; Elham-
ifar, 2016). We also compare against SSC with entry-
wise zeroﬁll (SSC-EWZF) (Yang et al., 2015). We ﬁnd
the VMC+SSC approach gives similar or lower clustering
error than LRMC+SCC for low missing rates. Likewise,
VMC+SSC also substantially outperforms SSC-EWZF for
high missing rates. Unlike SSC-EWZF and the other al-
gorithms in (Yang et al., 2015), VMC+SSC also succeeds
in setting where the data is low-rank (i.e., when all frames
are retained). This is because the performance of VMC is
similar to standard LRMC in the low-rank setting.

Figure 3. Subspace clustering error on Hopkins 155 dataset for
varying rates of missing data and undersampling of frames.

5.3. Completion of motion capture data

In Figure 4 we demonstrate VMC for completing time-
series trajectories from motion capture sensors using a
dataset from the CMU Mocap database3 (subject 56, trial
6). Empirically, this dataset has been shown to be locally
low-rank over the time frames corresponding to each sep-
arate activity, and can be modeled as a union of subspaces
(Elhamifar, 2016). The data had measurements from n =
62 sensors at s = 6784 time instants. We randomly under-
sampled the columns of this matrix and attempt to complete
the data using VMC, LRMC, and LRMC-NCVX and mea-
sure the resulting completion error: (cid:107)X − X0(cid:107)F /(cid:107)X0(cid:107)F ,
where X is the recovered matrix and X0 is the original
matrix. Similar to results on synthetic data, we ﬁnd the

3http://mocap.cs.cmu.edu

Figure 4. Completion error on CMU Mocap dataset using the pro-
posed VMC approach compared with convex and non-convex
LRMC algorithms.

6. Conclusion

We introduce a matrix completion approach that general-
izes low-rank matrix completion to a much wider class of
variety models, including data belonging to a union of sub-
spaces. We present a hypothesized sampling complexity
bound for the completion of a matrix whose columns be-
long to an algebraic variety. A surprising result of our
analysis that that a union of k afﬁne subspaces of dimen-
sion r should be recoverable from O(rk1/d) measurements
per column, provided we have O(rd) data points (columns)
per subspace, where d is the degree of the feature space
map. In particular, if we choose d = log k, then we need
only O(r) measurements per column as long as we have
O(rlog k) columns per subspace. We additionally introduce
an efﬁcient algorithm based on an iterative reweighted least
squares approach that realizes these hypothesized bounds
on synthetic data, and reaches state-of-the-art performance
on for matrix completion on several real high-rank datasets.

Our algorithm can easily accommodate other smooth ker-
nels, including the popular Gaussian RBF kernel (Muller
et al., 2001). A similar optimization formulation to ours
was presented in the recent pre-print (Garg et al., 2016) us-
ing Gaussian RBF kernels in place of polynomial kernels,
showing good empirical results in a matrix completion con-
text. However, analysis of the sample complexity in this
case is complicated by the fact that a feature space repre-
sentation for Gaussian RBF kernel is necessarily inﬁnite-
dimensional. Understanding the sample requirements in
this case would be an interesting avenue for future work.
Acknowledgements

For this work, Balzano and Ongie were supported in part
by ARO grant W911NF-14-1-0634. Willett and Nowak
were supported in part by NSF IIS-1447449, NSF CCF-
0353079, and NIH 1 U54 AI117924-01, and Nowak also
by AFOSR FA9550-13-1-0138.

00.10.20.30.40.5missing rate00.10.20.30.40.5clustering errorall framesSSC-EWZFLRMC+SSCVMC+SSC,d=2VMC+SSC,d=300.10.20.30.40.5missing rate00.10.20.30.40.5clustering error6 frames00.10.20.30.40.5missing rate00.10.20.30.40.5clustering error3 frames0.10.20.30.40.50.60.7missing rate00.20.4completion errorLRMCLRMC-NCVXVMC, d=2VMC, d=3Algebraic Variety Models for High-Rank Matrix Completion

References

Cai, Jian-Feng, Cand`es, Emmanuel J, and Shen, Zuowei. A
singular value thresholding algorithm for matrix comple-
tion. SIAM Journal on Optimization, 20(4):1956–1982,
2010.

Candes, Emmanuel and Recht, Benjamin. Exact matrix
completion via convex optimization. Communications
of the ACM, 55(6):111–119, 2012.

Chen, Guangliang, Atev, Stefan, and Lerman, Gilad. Ker-
In Computer
nel spectral curvature clustering (kscc).
Vision Workshops (ICCV Workshops), 2009 IEEE 12th
International Conference on, pp. 765–772. IEEE, 2009.

Chen, Yudong, Bhojanapalli, Srinadh, Sanghavi, Sujay,
and Ward, Rachel. Coherent matrix completion. In Pro-
ceedings of The 31st International Conference on Ma-
chine Learning, pp. 674–682, 2014.

Cox, David A., Little, John, and O’Shea, Donal.

Ideals,
Varieties, and Algorithms. Springer International Pub-
lishing, 2015.

Derksen, Harm. Hilbert series of subspace arrangements.
Journal of pure and applied algebra, 209(1):91–98,
2007.

Elhamifar, Ehsan. High-rank matrix completion and clus-
tering under self-expressive models. In Advances in Neu-
ral Information Processing Systems, pp. 73–81, 2016.

Elhamifar, Ehsan and Vidal, Ren´e. Sparse subspace clus-
In Computer Vision and Pattern Recognition,
tering.
2009. CVPR 2009. IEEE Conference on, pp. 2790–2797.
IEEE, 2009.

Elhamifar, Ehsan and Vidal, Ren´e. Sparse subspace clus-
tering: Algorithm, theory, and applications. IEEE trans-
actions on pattern analysis and machine intelligence, 35
(11):2765–2781, 2013.

Eriksson, Brian, Balzano, Laura, and Nowak, Robert D.
High-rank matrix completion. In AISTATS, pp. 373–381,
2012.

Fornasier, Massimo, Rauhut, Holger, and Ward, Rachel.
Low-rank matrix recovery via iteratively reweighted
SIAM Journal on Opti-
least squares minimization.
mization, 21(4):1614–1640, oct 2011. doi: 10.1137/
100811404.

Ganti, Ravi Sastry, Balzano, Laura, and Willett, Rebecca.
Matrix completion under monotonic single index mod-
els. In Advances in Neural Information Processing Sys-
tems, pp. 1873–1881, 2015.

Garg, Ravi, Eriksson, Anders, and Reid, Ian. Non-linear
dimensionality regularizer for solving inverse problems.
arXiv preprint arXiv:1603.05015, 2016.

Jain, Prateek, Netrapalli, Praneeth, and Sanghavi, Su-
jay. Low-rank matrix completion using alternating min-
imization. In Proceedings of the forty-ﬁfth annual ACM
symposium on Theory of computing, pp. 665–674. ACM,
2013.

Kanatani, Ken-ichi. Motion segmentation by subspace
In Computer Vision,
separation and model selection.
2001. ICCV 2001. Proceedings. Eighth IEEE Interna-
tional Conference on, volume 2, pp. 586–591. IEEE,
2001.

Lee, Joonseok, Kim, Seungyeon, Lebanon, Guy, and
Singer, Yoram. Local low-rank matrix approximation.
ICML (2), 28:82–90, 2013.

Mohan, Karthik and Fazel, Maryam. Iterative reweighted
algorithms for matrix rank minimization. The Journal of
Machine Learning Research, 13(1):3441–3473, 2012.

Muller, K-R, Mika, Sebastian, Ratsch, Gunnar, Tsuda,
Koji, and Scholkopf, Bernhard. An introduction to
kernel-based learning algorithms. IEEE Transactions on
Neural Networks, 12(2):181–201, 2001.

Nguyen, Minh H and Torre, Fernando. Robust kernel prin-
cipal component analysis. In Advances in Neural Infor-
mation Processing Systems, pp. 1185–1192, 2009.

Pimentel-Alarc´on, D, Balzano, L, Marcia, R, Nowak, R,
and Willett, R. Group-sparse subspace clustering with
missing data. In Statistical Signal Processing Workshop
(SSP), 2016 IEEE, pp. 1–5. IEEE, 2016a.

Pimentel-Alarcon, Daniel and Nowak, Robert.

The
information-theoretic requirements of subspace cluster-
ing with missing data. In Proceedings of The 33rd Inter-
national Conference on Machine Learning, pp. 802–810,
2016.

Pimentel-Alarc´on, Daniel L, Boston, Nigel, and Nowak,
Robert D. A characterization of deterministic sampling
patterns for low-rank matrix completion. IEEE Journal
of Selected Topics in Signal Processing, 10(4):623–636,
2016b.

Rao, Nikhil, Ganti, Ravi, Balzano, Laura, Willett, Rebecca,
and Nowak, Robert. On learning high dimensional struc-
In Proceedings of the 31st
tured single index models.
AAAI conference on artiﬁcial intelligence, 2017.

Sanguinetti, Guido and Lawrence, Neil D. Missing data
In European Conference on Machine

in kernel PCA.
Learning, pp. 751–758. Springer, 2006.

Algebraic Variety Models for High-Rank Matrix Completion

Song, Dogyoon, Lee, Christina E, Li, Yihua, and Shah, De-
vavrat. Blind regression: Nonparametric regression for
latent variable models via collaborative ﬁltering. In Ad-
vances in Neural Information Processing Systems, pp.
2155–2163, 2016.

Tron, Roberto and Vidal, Ren´e. A benchmark for the com-
parison of 3-d motion segmentation algorithms. In Com-
puter Vision and Pattern Recognition, 2007. CVPR’07.
IEEE Conference on, pp. 1–8. IEEE, 2007.

Tsakiris, Manolis C and Vidal, Ren´e. Algebraic clustering
of afﬁne subspaces. arXiv preprint arXiv:1509.06729,
2015.

Vidal, Ren´e, Soatto, Stefano, Ma, Yi, and Sastry, Shankar.
An algebraic geometric approach to the identiﬁcation of
a class of linear hybrid systems. In Decision and Con-
trol, 2003. Proceedings. 42nd IEEE Conference on, vol-
ume 1, pp. 167–172. IEEE, 2003.

Vidal, Ren´e, Ma, Yi, and Sastry, Shankar. Generalized
principal component analysis (GPCA). IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 27
(12):1945–1959, 2005.

Vidal, Ren´e, Ma, Yi, and Sastry, Shankar. Generalized
Principal Component Analysis. Springer New York,
2016.

Yang, Congyuan, Robinson, Daniel, and Vidal, Ren´e.
Sparse subspace clustering with missing entries. In Pro-
ceedings of The 32nd International Conference on Ma-
chine Learning, pp. 2463–2472, 2015.

