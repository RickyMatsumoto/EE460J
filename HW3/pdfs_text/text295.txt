Density Level Set Estimation on Manifolds with DBSCAN

Heinrich Jiang 1

Abstract

We show that DBSCAN can estimate the con-
nected components of the λ-density level set {x :
f (x) ≥ λ} given n i.i.d. samples from an un-
known density f . We characterize the regular-
ity of the level set boundaries using parameter
β > 0 and analyze the estimation error under the
Hausdorff metric. When the data lies in RD we
obtain a rate of (cid:101)O(n−1/(2β+D)), which matches
known lower bounds up to logarithmic factors.
When the data lies on an embedded unknown d-
dimensional manifold in RD, then we obtain a
rate of (cid:101)O(n−1/(2β+d·max{1,β})). Finally, we pro-
vide adaptive parameter tuning in order to attain
these rates with no a priori knowledge of the in-
trinsic dimension, density, or β.

1. Introduction

DBSCAN (Ester et al., 1996) is one of the most popu-
lar clustering algorithms amongst practitioners and has had
profound success in a wide range of data analysis applica-
tions. However, despite this, its statistical properties have
not been fully understood. The goal of this work is to give
a theoretical analysis of the procedure and to the best of
our knowledge, provide the ﬁrst analysis of density level-
set estimation on manifolds. We also contribute ideas to
related areas that may be of independent interest.

DBSCAN aims at discovering clusters which turn out to
be the high-density regions of the dataset. It takes in two
hyperparameters: minPts and ε.
It deﬁnes a point as a
core-point if there are at least minPts sample points in its ε-
radius neighborhood. The points within the ε-radius neigh-
borhood of a core-point are said to be directly reachable
from that core-point. Then, a point q is reachable from a
core-point p if there exists a path from q to p where each
point is directly reachable from the next point. It is now
clear that this deﬁnition of reachable gives a partitioning of

1Google.

Correspondence to: Heinrich Jiang <hein-

rich.jiang@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

the dataset (and remaining points not reachable from any
core-point are considered noise). This partitioning is the
clustering that is returned by DBSCAN.

The problem of analyzing DBSCAN has recently been ex-
plored in (Sriperumbudur & Steinwart, 2012). Their analy-
sis is for a modiﬁed version of DBSCAN and is not focused
on estimating a ﬁxed density level. Their results have many
desirable properties, but are not immediately applicable for
what this paper tries to address. Using recent developments
in topological data analysis along with some tools we de-
velop in this paper, we show that it is now possible to ana-
lyze the original procedure.

The clusters DBSCAN aims at discovering can be viewed
as approximations of the connected components of the level
sets {x : f (x) ≥ λ} where f is the density and λ is
some density level. We provide the ﬁrst comprehensive
analysis in tuning minPts and ε to estimate the density
level set for a particular level. Here, the density level λ
is known to the algorithm while the density remains un-
known. Density level set estimation has been studied ex-
tensively. e.g., (Carmichael et al., 1968; Hartigan, 1975;
Polonik, 1995; Cuevas & Fraiman, 1997; Walther, 1997;
Tsybakov et al., 1997; Baıllo et al., 2001; Cadre, 2006; Wil-
lett & Nowak, 2007; Biau et al., 2008; Rigollet & Vert,
2009; Maier et al., 2009; Singh et al., 2009; Rinaldo &
Wasserman, 2010; Steinwart, 2011; Rinaldo et al., 2012;
Steinwart et al., 2015; Chen et al., 2016; Jiang, 2017).
However approaches that obtain state-of-art consistency re-
sults are largely unpractical (i.e. unimplementable). Our
work shows that in actuality, DBSCAN, a procedure known
for decades and has since been used widely, can achieve
the strongest known results. Also, unlike much of the ex-
isting work, we show that DBSCAN can also recover the
connected components of the level sets separately and bi-
jectively.

Our work begins with the insight that DBSCAN behaves
like an ε-neighborhood graph, which is different from, but
related to the k-nearest neighbor graph. The latter has
been heavily used for cluster-tree estimation (Chaudhuri &
Dasgupta, 2010; Stuetzle & Nugent, 2010; Kpotufe & von
Luxburg, 2011; Chaudhuri et al., 2014; Jiang & Kpotufe,
2017) and in this paper we adapt some of these ideas for
ε-neighborhood graphs.

Density Level Set Estimation on Manifolds with DBSCAN

Cluster-tree estimation aims at discovering the hierarchi-
cal tree structure of the connected-components as the levels
vary. Balakrishnan et al. (2013) extends results by Chaud-
huri & Dasgupta (2010) to the setting where the data lies
on a lower dimensional manifold and provide consistency
results depending on the lower dimension and independent
of the ambient dimension. Here we are instead interested
in how to set minPts and ε in order to estimate a partic-
ular level and provide rates on the Hausdorff distance er-
ror. This is different from works on cluster tree estimation
which focuses on how to recover the tree structure rather
than recovering a particular level. In that regard, we also
require density estimation bounds in order to get a handle
on the true density-levels and the empirical ones.

Dasgupta & Kpotufe (2014) gives us optimal high-
probability ﬁnite-sample k-NN density estimation bounds
which hold uniformly;
this is key to obtaining optimal
level-set estimation rates under the Hausdorff error. Much
of the previous works on density level-set estimation, e.g.
(Rigollet & Vert, 2009) provide rates under risk measures
such as symmetric set difference. These metrics are consid-
erably weaker than the Hausdorff metric; the latter is a uni-
form guarantee. There are such bounds for the histogram
density estimator. This allowed Singh et al. (2009) to ob-
tain optimal rates under Hausdorff metric, while having
a fully adaptive procedure. This was a signiﬁcant break-
through for level set estimation, as discussed by Chazal
et al. (2015). We believe this to be the strongest consis-
tency results obtained thus far. However, a downside is that
the histogram density estimator has little practical value.
Here, aided with the desired bounds on the k-NN density
estimator, we can actually obtain similar results to Singh
et al. (2009) but with the clearly practical DBSCAN.

We extend the k-NN density estimation results of Dasgupta
& Kpotufe (2014) to the manifold case, as the bulk our
analysis is about the more general case that the data lies on
a manifold. Density-based procedures perform poorly in
high-dimensions since the number of samples required in-
creases exponentially in the dimension– the so called curse
of dimensionality. Thus, the consequences of handling the
manifold case are of practical signiﬁcance. Since the esti-
mation rates we obtain depend only on the intrinsic dimen-
sion, it explains why DBSCAN can do well in high dimen-
sions if the data has low intrinsic dimension (i.e. the man-
ifold hypothesis). Given the modern capacity of systems
to collect data of increasing complexity, it has become ever
more important to understand the feasibility of practical
algorithms in high dimensions.

To analyze DBSCAN, we write minPts and ε in terms of
the d, unknown manfold dimension; k, which controls the
density estimator; and λ, which determines which level to
estimate. We assume knowledge of λ with the goal of es-

timating the λ-level set of the density. We give a range of
k in terms of n and corresponding consistency guarantees
and estimation rates for such choices. We then adaptively
tune d and k in order to attain close to optimal performance
with no a priori knowledge of the distribution. Adaptivity
is highly desirable because it allows for automatic tuning of
the hyper-parameters, which is a core tenet of unsupervised
learning. To solve for the unknown dimension, we use an
estimator from Farahmand et al. (2007), which we show to
have considerably better ﬁnite-sample behavior than previ-
ously thought. More details and discussion of related works
is in the main text. We then provide a new method of choos-
ing k such that it will asymptotically approach a value that
provides near-optimal level set estimation rates.

2. Overview

We start by analyzing the procedure under the manifold
assumption. The end of the paper will discuss the full-
dimensional setting. The bulk of our contribution lies in
analyzing the former situation, while the analysis of the lat-
ter uses a subset of those techniques.

• Section 3 proves that the clusters returned by DB-
SCAN are close to the connected components of cer-
tain ε-neighborhood graphs (Lemma 2). This is sig-
niﬁcant because these graphs can be shown to estimate
density level sets.

• Section 4 introduces the manifold setting and provides
supporting results including k-nearest neighbor den-
sity estimation bounds (Lemma 5 and Lemma 6) that
are useful later on.

• Section 5 provides a range of parameter settings un-
der which for each true cluster, there exists a corre-
sponding cluster returned by DBSCAN (Lemma 7 and
Lemma 8), and a rate for the Hausdorff distance be-
tween them (Theorem 1).

• Section 6 shows how one can apply DBSCAN a sec-
ond time to remove false clusters from the ﬁrst appli-
cation, thus completing a bijection between the esti-
mates and the true clusters (Theorem 2).

• Section 7 explains how to adaptively tune the parame-
ters so that they fall within the theoretical ranges. The
main contributions of this section are a stronger result
about a known k-nearest neighbor based approach to
estimating the unknown dimension (Theorem 3) and a
new way to tune k to approach an optimal choice of k
(Theorem 4).

• Section 8 gives the result when the data lives in RD

without the manifold assumption.

Density Level Set Estimation on Manifolds with DBSCAN

3. The connection to neighborhood graphs

This section is dedicated towards the understanding of
the clusters produced by DBSCAN. The algorithm can be
found in (Ester et al., 1996) and is not shown here since
Lemma 1 characterizes what DBSCAN returns.

We have n i.i.d. samples X = {x1, ..., xn} drawn from a
distribution F over RD.
Deﬁnition 1. Deﬁne the k-NN radius of x ∈ RD as

rk(x) := inf{r > 0 : |X ∩ B(x, r)| ≥ k},

where B(x, r) denotes the Euclidean ball of radius r cen-
tered at x. Let G(k, ε) denote the ε-neighborhood level
graph of X with vertices {x ∈ X : rk(x) ≤ ε} and an
edge between x and x(cid:48) iff ||x − x(cid:48)|| ≤ ε.

Remark 1. This is slightly different from ε-neighborhood
graph, which includes all vertices. Here we exclude ver-
tices below certain empirical density level (i.e. rk(x) > ε).

The next deﬁnition is relevant to DBSCAN and is from (Es-
ter et al., 1996) but in the notation of Deﬁnition 1.

Deﬁnition 2. The following is with respect to ﬁxed ε > 0
and minPts ∈ N.

Proof. Take any K ∈ K. Each point in K is a core-point
and by Lemma 1 and the deﬁnition of density-reachable,
each point in K belongs to the same C ∈ C. Thus, K ⊆
{x ∈ C : rk(x) ≤ ε}. Next we show that K = {x ∈ C :
rk(x) ≤ ε}.

Suppose there exists core-point x ∈ C but x /∈ K and let
y ∈ K. By Lemma 1, there exists core-point c ∈ C such
that all points in C are directly reachable from c. Then
there exists a path of core-points from x to c with pairwise
edges of length at most ε. The same holds for c to y. Thus
there exists such a path of core-points from x to y, which
means that x, y are in the same CC of G(minP ts, ε), con-
tradicting the assumption that x /∈ K and y ∈ K. Thus,
in fact K = {x ∈ C : rk(x) ≤ ε}. The result now fol-
lows since C consists of points that are at most ε from its
core-points.

We can now see that DBSCAN’s clusterings can be viewed
as the connected components (CCs) of an appropriate (cid:15)-
neighborhood level graph. Using a neighborhood graph to
approximate the level-set has been studied in (Rinaldo &
Wasserman, 2010). The difference is that they use a kernel
density estimator instead of a k-NN density estimator and
study the convergence properties under different settings.

• p is a core-point if rminPts(p) ≤ ε.
• q is directly density-reachable from p if |p − q| ≤ ε

4. Manifold Setting

4.1. Setup

and p is a core-point.

• q is density-reachable from p if there exists a sequence
q = p1, p2, ..., pm = p such that pi is directly density-
reachable from pi+1 for i = 1, .., m − 1.

The following result is paraphrased from Lemmas 1 and 2
from (Ester et al., 1996), which characterizes the clusters
learned by DBSCAN.

Lemma 1. (Ester et al., 1996) Let C be the clusters re-
turned by DBSCAN(minPts, ε). For any core-point x, there
exists C ∈ C with x ∈ C. On the other hand, for any
C ∈ C, there exists core-point x such that C = {x(cid:48)
:
x(cid:48) is density-reachable from x}.

relating the ε-
We now show the following result
neighborhood level graphs and the clusters obtained from
DBSCAN. Such an interpretation of DBSCAN has been
given in previous works such as Campello et al. (2015).

Lemma 2 (DBSCAN and ε-neighborhood level graphs).
Let C be the clusters obtained from DBSCAN(minP ts, ε)
on X.
Let K be the connected components of
G(minP ts, ε). Then, there exists a one-to-one corre-
spence between C and K such that if C ∈ C and K ∈ K
correspond, then

K ⊆ C ⊆ ∪x∈KB(x, ε) ∩ X.

We make the following regularity assumptions which are
standard among works on manifold learning e.g. (Baraniuk
& Wakin, 2009; Genovese et al., 2012; Balakrishnan et al.,
2013).

Assumption 1. F is supported on M where:

• M is a d-dimensional smooth compact Riemannian
manifold without boundary embedded in compact sub-
set X ⊆ RD.

• The volume of M is bounded above by a constant.

• M has condition number 1/τ , which controls the cur-

vature and prevents self-intersection.

Let f be the density of F with respect to the uniform mea-
sure on M .

Assumption 2. f is continuous and bounded.

4.2. Basic Supporting Bounds

The following result bounds the empirical mass of Eu-
clidean balls to the true mass under f . It is a direct con-
sequence of Lemma 6 of Balakrishnan et al. (2013).

Lemma 3 (Uniform convergence of empirical Euclidean
balls (Lemma 6 of Balakrishnan et al. (2013))). Let N be
a minimal ﬁxed set such that each point in M is at most dis-
tance 1/n from some point in N . There exists a universal

Density Level Set Estimation on Manifolds with DBSCAN

constant C0 such that the following holds with probability
at least 1 − δ. For all x ∈ X ∪ N ,

Fix λ0 > 0 and δ > 0 and suppose that k ≥ C 2
there exists constant C1 ≡ C1(λ0, d, τ ) such that if

δ,n. Then

F(B) ≥ Cδ,n

⇒ Fn(B) > 0

F(B) ≥

+ Cδ,n

⇒ Fn(B) ≥

F(B) ≤

− Cδ,n

⇒ Fn(B) <

.

√

d log n
n

√

k
n
√
k
n
√

k
n
k
n

k
n
k
n

d log n, Fn is the empirical

where Cδ,n = C0 log(2/δ)
distribution, and k ≥ Cδ,n.
Remark 2. For the rest of the paper, many results are qual-
iﬁed to hold with probability at least 1−δ. This is precisely
the event in which Lemma 3 holds.
Remark 3. If δ = 1/n, then Cδ,n = O((log n)3/2).

Next, we need the following bound on the volume of the
intersection Euclidean ball and M ; this is required to get
a handle on the true mass of the ball under F in later ar-
guments. The upper and lower bounds follow from Chazal
(2013) and Lemma 5.3 of Niyogi et al. (2008). The proof
is given in the appendix.

Lemma 4 (Ball Volume). If 0 < r < min{τ /4d, 1/τ },
and x ∈ M then

vdrd(1 − τ 2r2) ≤ vold(B(x, r) ∩ M ) ≤ vdrd(1 + 4dr/τ ).

where vd is the volume of a unit ball in Rd and vold is the
volume w.r.t. the uniform measure on M .

4.3. k-NN Density Estimation

Here, we establish density estimation rates for the k-NN
density estimator in the manifold setting. This builds on
work in density estimation on manifolds e.g.
(Hendriks,
1990; Pelletier, 2005; Ozakin & Gray, 2009; Kim & Park,
2013; Berry & Sauer, 2017); thus, it may be of independent
interest. The estimator is deﬁned as follows

Deﬁnition 3 (k-NN Density Estimator).

fk(x) :=

k
n · vd · rk(x)d .

The following extends previous work of Dasgupta &
Kpotufe (2014) to the manifold case. The proofs can be
found in the appendix.

Lemma 5 (fk upper bound). Suppose that Assumptions 1
and 2 hold. Deﬁne the following which charaterizes how
much the density increases locally in M :

(cid:40)

(cid:41)

f (x(cid:48)) − f (x) ≤ (cid:15)

.

ˆr((cid:15), x) := sup

r :

sup
x(cid:48)∈B(x,r)∩M

k ≤ C1 · C 2d/(2+d)

δ,n

· n2/(2+d),

then the following holds with probability at least 1 − δ uni-
formly in (cid:15) > 0 and x ∈ X with f (x) + (cid:15) ≥ λ0:

fk(x) <

1 + 3 ·

· (f (x) + (cid:15)),

(cid:18)

(cid:19)

Cδ,n√
k

√
provided k satisﬁes vd · ˆr((cid:15), x)d ·(f (x)+(cid:15)) ≥ k
k
n .
Lemma 6 (fk lower bound). Suppose that Assumptions 1
and 2 hold. Deﬁne the following which charaterizes how
much the density decreases locally in M :

n −Cδ,n

(cid:40)

(cid:41)

f (x) − f (x(cid:48)) ≤ (cid:15)

.

ˇr((cid:15), x) := sup

r :

sup
x(cid:48)∈B(x,r)∩M

Fix λ0 > 0 and 0 < δ < 1 and suppose k ≥ Cδ,n. Then
there exists constant C2 ≡ C2(λ0, d, τ ) such that if

k ≤ C2 · C 2d/(4+d)

δ,n

· n4/(4+d),

then with probability at least 1 − δ, the following holds
uniformly for all (cid:15) > 0 and x ∈ X with f (x) − (cid:15) ≥ λ0:

fk(x) ≥

1 − 3 ·

· (f (x) − (cid:15)),

(cid:18)

(cid:19)

Cδ,n√
k

√

k
n

(cid:17)
.

(cid:16) k

n + Cδ,n

provided k satisﬁes vd · ˇr((cid:15), x)d · (f (x) − (cid:15)) ≥
4
3
Remark 4. We will often bound the density of points with
low density. In low-density regions, there is less data and
thus we require more points to get a tight bound. However,
in many cases a tight bound is not necessary; thus the pur-
poses of (cid:15) is to allow some slack. The higher the (cid:15), the
easier it is for the lemma conditions to be satisiﬁed. In par-
ticular, if f is α-H¨older continuous (i.e. |f (x) − f (x(cid:48))| ≤
Cα|x − x(cid:48)|α), we have ˆr((cid:15), x), ˇr((cid:15), x) ≥ ((cid:15)/Cα)1/α.

5. Consistency and Rates

5.1. Level-Set Conditions

Much of the results will depend on the behavior of level
set boundaries. Thus, we require sufﬁcient drop-off at the
boundaries, as well as separation between the CCs at a par-
ticular level set. We give the following notion of separation.
Deﬁnition 4. A, A(cid:48) are r-separated in M if there exists a
set S such that every path from A to A(cid:48) intersects S and
supx∈M ∩(S+B(0,r)) f (x) < inf x∈A∪A(cid:48) f (x).

Deﬁne the following shorthands for distance from a point to
a set, the intersection of M with a neighborhood around a
set under the Euclidean distance, and the largest Euclidean
distance from a point in a set to its closest sample point.

Density Level Set Estimation on Manifolds with DBSCAN

Deﬁnition 5. d(x, A) := inf x(cid:48)∈A |x − x(cid:48)|, C ⊕r := {x ∈
M : d(x, C) ≤ r}, rn(C) := supc∈C d(c, X).

The remainder of this section will be to show that DB-
SCAN(minPts, ε) with

We have the following mild assumptions which ensures
that the CCs can be separated from the rest of the density
by sufﬁciently wide valleys and there is sufﬁcient decay
around the level set boundaries.

Assumption 3 (Separation Conditions). Let λ > 0 and
Cλ be a CCs of {x ∈ M : f (x) ≥ λ}. There exists
ˇCβ, ˆCβ, β, rs, rc > 0 and 0 < λ0 < λ such that the fol-
lowing holds:

For each C ∈ Cλ, there exists AC, a connected component
of M λ0 := {x ∈ M : f (x) ≥ λ0} such that:

• AC separates C by a valley: AC does not intersect
with any other CC in Cλ; AC and M λ0 \AC are rs-
separated by some SC.

• C ⊕rc ⊆ AC.
• β-regularity: For x ∈ C ⊕rc\C, we have

ˇCβ · d(x, C)β ≤ λ − f (x) ≤ ˆCβ · d(x, C)β.

Remark 5. We can choose any 0 < β < ∞. The β-
regularity assumption appears in e.g. (Singh et al., 2009).
This is very general and also allows us to make a separate
global smoothness assumption.

Remark 6. We currently characterize the smoothness w.r.t.
the Euclidean distance. One could alternatively use the
geodesic distance on M , dM (p, q). It follows from Propo-
sition 6.3 of Niyogi et al. (2008) that when |p − q| < τ /4,
we have |p − q| ≤ dM (p, q) ≤ 2|p − q|. Since the dis-
tances we deal in our analysis with are of such small order,
these distances can thus essentially be treated as equiva-
lent. We use the Euclidean distance throughout the paper
for simplicity.

Remark 7. For the rest of this paper, it will be understood
that Assumptions 1, 2, and 3 hold.

We can deﬁne a region which isolates C away from other
clusters of {x ∈ M : f (x) ≥ λ}.
Deﬁnition 6. XC := {x : ∃ a path P from x to x(cid:48) ∈
C such that P ∩ SC = ∅}.

5.2. Parameter Settings

Fix λ > 0 and δ > 0. Let k satisfy the following

Kl · (log n)2 ≤ k ≤ Ku · (log n)2d/(2+d) · n2β(cid:48)/(2β(cid:48)+d),

(cid:32)

minPts = k, ε =

k

(cid:33)1/d

√

n · vd · (λ − λ · C 2

δ,n/

k)

will consistently estimate each CC of {x ∈ M : f (x) ≥
λ}. Throughout the text, we denote (cid:99)Cλ as the clusters re-
turned by DBSCAN under this setting.

5.3. Separation and Connectedness

Take C ∈ Cλ. We show that DBSCAN will return an esti-
mated CC (cid:98)C, such that (cid:98)C does not contain any points out-
side of XC. Then, we show that (cid:98)C contains all the sample
points in C. The proof ideas used are similar to that of stan-
dard results in cluster trees estimation; they can be found
in the appendix.
Lemma 7 (Separation). There exists Kl sufﬁciently large
and Ku sufﬁciently small such that the following holds with
probability at least 1 − δ. Let C ∈ Cλ. There exists (cid:98)C ∈ (cid:99)Cλ
such that (cid:98)C ⊆ XC.
Lemma 8 (Connectedness). There exists Kl sufﬁciently
large and Ku sufﬁciently small such that the following
holds with probability at least 1 − δ. Let C ∈ Cλ. If there
exists (cid:98)C ∈ (cid:99)Cλ such that (cid:98)C ⊆ XC, then C ⊕rn(C) ∩ X ⊆ (cid:98)C.
Remark 8. These results allow C to have any dimension
between 0 to d since we reason with C ⊕rn(C), which con-
tains samples, instead of simply C.

5.4. Hausdorff Error

We give the estimation rate under the Hausdorff metric.
Deﬁnition 7 (Hausdorff Distance).

dHaus(A, A(cid:48)) = max{sup
x∈A

d(x, A(cid:48)), sup
x(cid:48)∈A(cid:48)

d(x(cid:48), A)}.

Theorem 1. There exists Kl sufﬁciently large and Ku sufﬁ-
ciently small such that the following holds with probability
at least 1 − δ. For each C ∈ Cλ, there exists (cid:98)C ∈ (cid:99)Cλ such
that

dHaus(C, (cid:98)C) ≤ 2 · (4λ/ ˇCβ)1/β · C 2/β

δ,n · k−1/2β.

Proof. For Kl and Ku appropriately chosen, we have
Lemma 7 and Lemma 8 hold. Thus we have for C ∈ Cλ,
there exists (cid:98)C ∈ (cid:99)Cλ such that

C ⊕rn(C) ∩ X ⊆ (cid:98)C ⊆

(cid:91)

B(x, ε) ∩ M.

x∈XC ∩X
C2
δ,n
√
k

fk(x)≥λ−

λ

where β(cid:48)
:= min{1, β}, and Kl and Ku are positive
constants depending on δ, ˇCβ, ˆCβ, β, τ, d, ||f ||∞, λ0, rs, rc
which are implicit in the proofs later in this section.

(cid:17)1/β

(cid:16) 4λ·C2
δ,n
√
ˇCβ ·
k

Deﬁne ¯r :=
. We show that dHaus(C, (cid:98)C) ≤ ¯r,
which involves two directions to show from the Hausdroff

Density Level Set Estimation on Manifolds with DBSCAN

metric: that maxx∈ (cid:98)C d(x, C) ≤ ¯r and supx∈C d(x, (cid:98)C) ≤
¯r.

We start by proving maxx∈ (cid:98)C d(x, C) ≤ ¯r. Deﬁne r0 =
¯r/2. We have

r0 =

(cid:33)1/β

(cid:32) 4 · C 2
δ,n
√
ˇCβ ·
k

1
2

(cid:18) k

(cid:19)1/d

≥

vdnλ0

≥ ε,

where the ﬁrst inequality holds when Ku is chosen sufﬁ-
ciently small, and the last inequality holds because λ0 <
λ. Hence r0 + ε ≤ ¯r. Therefore, it sufﬁces to
λ −
show

C2
δ,n√
k

sup
x∈(XC \C⊕r0 )∩X

fk(x) < λ −

C 2
δ,n√
k

λ.

We have that for x ∈ (XC\C ⊕r0/2) ∩ X, f (x) ≤ λ −
ˇCβ(r0/2)β := λ(cid:48). Thus, for any x ∈ (XC\C ⊕r0) ∩ X and
letting (cid:15) = λ(cid:48) − f (x), we have

ˆr((cid:15), x) ≥ r0/2 ≥ (4λ0Cδ,n/(

k · ˇCβ))1/β/2.

√

For Ku chosen sufﬁciently small, the last equation will be
large enough (i.e. of order (k/vdnλ)1/d) so that the con-
ditions of Lemma 5 hold. Thus, applying this for each
x ∈ (XC\C ⊕r0 ) ∩ X, we obtain

sup
x∈(XC \C⊕r0 )∩X

fk(x) <

1 + 3

(λ − ˇCβ(r0/2)β).

(cid:18)

(cid:19)

Cδ,n√
k

We have the r.h.s.
appropriately and the ﬁrst direction follows.

is at most λ −

C2
δ,n√
k

λ for Ku chosen

We now turn to the other direction, that supx∈C d(x, (cid:98)C) ≤
¯r. Let x ∈ C. Then there exists sample point x(cid:48) ∈
B(x, rn(C)) by deﬁnition of rn and we have that x(cid:48) ∈ (cid:98)C.
Finally, rn(C) ≤ ¯r for Kl sufﬁciently large, and thus
|x(cid:48) − x| ≤ ¯r. The result follows.
Remark 9. When taking k ≈ n2β(cid:48)/(2β(cid:48)+d), we obtain the
error rate of dHaus(C, (cid:98)C) ≈ n−1/(2β+d·max{1,β}), ignoring
logarithmic factors. When 0 < β ≤ 1, this matches the
known lower bound established in Theorem 4 of Tsybakov
et al. (1997). However, we do not obtain this rate when
β > 1. In this case, the density estimation error will be of
order at least n−1/(2+d) due in part to the error from re-
solving the geodesic balls with Euclidean balls. This does
not arise in the full dimensional setting, which will be de-
scribed later.

6. Removal of False Clusters

show how a second application of DBSCAN (Algorithm 1)
can remove the false clusters discovered by the ﬁrst appli-
cation of DBSCAN with no additional parameters. This
gives us the other direction, that each estimate in (cid:98)Cλ corre-
sponds to a true CC in Cλ, and thus DBSCAN can identify
with a one-to-one correspondence each CC of the level-set.

Algorithm 1 DBSCAN False CC Removal
As in Section 5.2, let minPts = k and

(cid:18)

ε =

(cid:19)1/d

.

k

√

δ,n/

k)

n·vd·(λ−λ·C2
(cid:18)

(cid:19)1/d

k

.

k)

Deﬁne ˜ε :=

n·vd·(λ−λ·C2

δ,n/ 3√
Let (cid:98)Cλ be the clusters returned by DBSCAN(minPts, ε).
Let (cid:98)Dλ be the clusters returned by DBSCAN(minPts, ˜ε).
Let (cid:101)Cλ be the clusters obtained by merging clusters from
(cid:98)Cλ which are subsets of the same cluster in (cid:98)Dλ .
Return (cid:101)Cλ.

We state our result below. The proof is less involved and is
in the appendix.

Theorem 2 (Removal of False CC Estimates). Deﬁne γ =
λ − supx∈M \(∪C∈Cλ XC ) f (x), which is positive. There ex-
ists Kl sufﬁciently large and Ku sufﬁciently small depend-
ing on γ in addition to the constants mentioned in Sec-
tion 5.2 so that the following holds with probability at least
1 − δ. For all (cid:98)C ∈ (cid:101)Cλ, there exists C ∈ Cλ such that

dHaus(C, (cid:98)C) ≤ 2 · (4λ/ ˇCβ)1/β · C 2/β

δ,n · k−1/2β.

7. Adaptive Parameter Tuning

In this section, we show how to obtain the near optimal
rates by estimating d and adaptively choosing k such that
k ≈ n2β(cid:48)/(2β(cid:48)+d) without knowledge of β.

7.1. Determining d

Knowing the manifold dimension d is necessary to tune the
parameters as described in Section 5.2. There has been
much work done on estimating the intrinsic dimension as
many learning procedures (including this one) require d as
an input. Such work in intrinsic dimension estimation in-
clude (Kegl, 2002; Levina & Bickel, 2004; Hein & Audib-
ert, 2005). Pettis et al. (1979) and more recently Farahmand
et al. (2007) take a k-nearest neighbor approach. We work
with the estimate of a dimension at a point proposed in the
latter work:

ˆd(x) =

log 2
log(r2k(x)/rk(x))

.

The result of Theorem 1 guarantees us that for each C ∈
Cλ, there exists (cid:98)C ∈ (cid:98)Cλ that estimates it. In this section, we

The main result of Farahmand et al. (2007) gives a high-
probability bound for a single sample X1 ∈ X. Here we

Density Level Set Estimation on Manifolds with DBSCAN

give a high-probability bound under more mild smoothness
assumptions which hold uniformly for all samples above
some density-level given our new knowledge of k-NN den-
sity estimation rates. This may be of independent interest.
Theorem 3. Suppose that f is α-H¨older continuous for
some 0 < α ≤ 1. Choose ¯λ0 > 0 and δ > 0. Then there ex-
ists constants C1, C2 depending on δ, Cα, α, τ, d, ¯λ0 such
that if k satisﬁes

C1 · (log n)2 ≤ k ≤ C2 · n2α/(2α+d),

then with probability at least 1 − δ,

| ˆd(x) − d| ≤ 20d · ||f ||∞ ·

Cδ,n√
k

,

uniformly for all x ∈ X with fk(x) ≥ ¯λ0.
Proof. We have for x ∈ X such that if fk(x) ≥ ¯λ0, then
f (x) ≥ λ0 := ¯λ0/2 by Lemma 5 for C1 chosen appropri-
ately large and C2 chosen appropriately small.

ˆd(x) =

log 2
log(r2k(x)/rk(x))

=

d log 2
log 2 + log(fk(x)/f2k(x))

.

We now try to get a handle on fk(x)/f2k(x) and show it
is sufﬁciently close to 1. Applying Lemma 5 and 6 with
(cid:15) = Cδ,n√
f (x) and C1, C2 appropriately chosen so that the
k
conditions for the two Lemmas hold (remember that here
we have ˆr((cid:15), x), ˇr((cid:15), x) ≥ ((cid:15)/Cα)1/α), we obtain

√
√

k)(1 − Cδ,n/
k)(1 + Cδ,n/

k) · f (x)
k) · f (x)

√
√

fk(x)
f2k(x)

≥

(1 − 3Cδ,n/
(1 + 3Cδ,n/
Cδ,n√
k

,

≥ 1 − 9 ·

where the last inequality holds when C1 is chosen sufﬁ-
ciently large so that Cδ,n/
k is sufﬁciently small. On the
other hand, we similarly obtain (for C1 and C2 appropri-
ately chosen):

√

√
√

k)(1 + Cδ,n/
k)(1 − Cδ,n/

k) · f (x)
k) · f (x)

√
√

fk(x)
f2k(x)

≤

(1 + 3Cδ,n/
(1 − 3Cδ,n/
Cδ,n√
k

.

≤ 1 + 9 ·

It is now clear that by the expansion log(1 − r) = −r −
r2/2 − r3/3 − · · · , and for Kl chosen sufﬁcently large so
that Cδ,n/

√

k is sufﬁciently small, we have
Cδ,n√
k

(cid:18) fk(x)
f2k(x)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 10 ·

log

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

The result now follows by combining this with the earlier
established expression for ˆd(x), as desired.

Remark 10. In Farahmand et al. (2007), it is the case that
α = 1; under this setting, we match their bound with an
error rate of n1/(2+d) with k ≈ n2/(2+d) being the optimal
choice for k (ignoring log factors).

7.2. Determining k

After determining d, the next parameter we look at is k.
In particular, to obtain the optimal rate, we must choose
k ≈ n2β(cid:48)/(2β(cid:48)+d) without knowledge of β. We present a
consistent estimator for β.

We need the following deﬁnition. The ﬁrst character-
izes how much f varies in balls of a certain radius along
the boundaries of the λ-level set (where ∂Cλ denotes the
boundary of Cλ). The second is meant to be an estimate of
the ﬁrst, which can be computed from the data alone. The
ﬁnal is our estimate of β.

Dr = inf

x0∈∂Cλ

sup
x∈B(x0,r)

|λ − f (x)|

ˆDr,k =

min
x0∈X
B(x0,r)∩X(cid:54)=∅
ˆβ = logr( ˆDr,k)

max
x∈B(x0,r)∩X

|λ − fk(x)|

The next is a result of how ˆDr,k estimates Dr.
Lemma 9. Suppose that f is α-H¨older continuous for
some 0 < α ≤ 1. Let k = (cid:98)(log n)5(cid:99) and r = 1/
log n.
Then there exists positive constants ˜C and N depending on
d, τ, α, Cα, λ0, ||f ||∞, rc such that when n ≥ N , then the
following holds with probability at least 1 − 1/n.

√

|Dr − ˆDr,k| ≤ ˜C/(log n)2.

Proof sketch. Suppose that the value of Dr is attained at
x0 = p and the value of ˆDr,k is attained at x0 = q. Let
y, z be the points that maximize |λ − f (x)| on B(p, r) and
B(q, r), respectively. Let ˆy, ˆz be the sample points that
maximize |λ − fk(x)| on B(p, r) and B(q, r), respectively.
Now, we have

Dr − ˆDr,k = |λ − f (y)| − |λ − fk(ˆz)|
≤ |λ − f (z)| − |λ − fk(ˆz)| ≤ |f (z) − fk(ˆz)|
≤ max{f (z) − fk(z), fk(ˆz) − f (ˆz)}.

Now let z(cid:48) be the closest sample point to z in B(q, r). Then,

≤ max{f (z(cid:48)) − fk(z(cid:48)), fk(ˆz) − f (ˆz)} + |f (z) − f (z(cid:48))|
+ |fk(z) − fk(z(cid:48))| ≤
max
x∈X,f (x)≥λ0
+ Cα|z − z(cid:48)|α + |fk(z) − fk(z(cid:48))|.

|f (x) − fk(x)|

On the other hand, we have

ˆDr,k − Dr = |λ − fk(ˆz)| − |λ − f (y)|
≤ |λ − fk(ˆy)| − |λ − f (y)| ≤ |f (y) − fk(ˆy)|
≤ max{f (y) − fk(y), fk(ˆy) − f (ˆy)}.

Let y(cid:48) be the closest sample point to y in B(p, r). Then,
≤ max{f (y(cid:48)) − fk(y(cid:48)), fk(ˆy) − f (ˆy)} + |f (y) − f (y(cid:48))|
+ |fk(y) − fk(y(cid:48))| ≤
max
x∈X,f (x)≥λ0
+ Cα|y − y(cid:48)|α + |fk(y) − fk(y(cid:48))|.

|f (x) − fk(x)|

Density Level Set Estimation on Manifolds with DBSCAN

it

sufﬁces

to bound maxx∈X,f (x)≥λ0 |f (x) −
Thus
fk(x)|, |y − y(cid:48)|, |z − z(cid:48)|, |fk(y) − fk(y(cid:48))|, |fk(z) − fk(z(cid:48))|.
take δ = 1/n and use Lemma 5 and 6 for
First
maxx∈X,f (x)≥λ0 |f (x) − fk(x)|. Using Lemma 3, we can
show that rn := |y − y(cid:48)| (cid:46) (log n/n)1/d. Next we bound
|fk(y) − fk(y(cid:48))|. y(cid:48) ∈ X so we have guarantees on its fk
value. Note that rk(y(cid:48)) − rn ≤ rk(y) ≤ rk(y(cid:48)) + rn.
Let rk = rk(y(cid:48)). This implies that fk(y(cid:48))(rk/(rk +
rn))d ≤ fk(y) ≤ fk(y(cid:48))(rk/(rk − rn))d. Now since
rk ≈ (k/n)1/d, we have |fk(y) − fk(y(cid:48))| (cid:46) log n/k. The
same holds for the bounds related to z, z(cid:48).

Theorem 4 ( ˆβ → β in probability). Suppose f is α-
H¨older continuous for some α with 0 < α ≤ β(cid:48). Let
√
k = (cid:98)(log n)5(cid:99) and r = 1/
log n. Then for all (cid:15) > 0,

(cid:16)

P

| ˆβ − β| ≥ (cid:15)

= 0.

(cid:17)

lim
n→∞

N(cid:15),δ,f ≡ N ((cid:15), δ, f ) and Cδ ≡ Cδ(δ, f ) such that the fol-
lowing holds. If n ≥ N(cid:15),δ,f , then with probability at least
1 − δ simulatenously for each C ∈ Cλ, there exists (cid:98)C ∈ (cid:99)Cλ
such that

dHaus(C, (cid:98)C) ≤ Cδ · n−

1

2β+d max{1,β} +(cid:15).

Moreover, using Algorithm 2, there is a one-to-one corre-
spondence between Cλ and (cid:99)Cλ.

8. Full Dimensional Setting

Here we instead take f to be the density of F over the uni-
form measure on RD. Let

(cid:32)

minPts = k, ε =

k

(cid:33)1/D

√

,

n · vD · (λ − λ · C 2

δ,n/

k)

Proof. Based on the β-regularity assumption, we have for
r < rc:

ˇCβrβ ≤ Dr ≤ ˆCβrβ.

where k satisﬁes

Combining this with Lemma 9, we have with probability at
least 1 − 1/

n that

√

ˇCβrβ − ˜C/(log n)2 ≤ ˆDr,k ≤ ˆCβrβ + ˜C/(log n)2.

Thus with probability at least 1 − 1/n,

β − ˆβ ≥

β − ˆβ ≤

log(1 − ˜C/( ˆDr,k · (log n2)))
log r
log(1 + ˜C/( ˆDr,k · (log n2)))
log r

−

+

log ˆCβ
log r
log ˇCβ
log r

.

It is clear that these expressions go to 0 as n → ∞ and the
result follows.

Remark 11. We can then take k = n ˆβ(cid:48)/(2 ˆβ(cid:48)+d) with
ˆβ(cid:48) = min{1, ˆβ − (cid:15)0} for some (cid:15)0 > 0 so that ˆβ(cid:48) < β(cid:48) for n
sufﬁciently large and thus k lies in the allowed ranges de-
scribed in Section 5.2 asymptotically. The settings of ε and
MinPts are implied by this choice of k and our estimate of
d.

7.3. Rates with Data-driven Tuning

Putting this all together, along with Theorems 1 and 2,
gives us the following consequence about level set recov-
ery with adaptive tuning. It shows that we can obtain rates
arbitrarily close to those obtained as if the smoothness pa-
rameter β and intrinsic dimension were known.

Corollary 1. Suppose that 0 < δ < 1 and f is α-
H¨older continuous for some 0 < α ≤ 1 and suppose
the data-driven choices of parameters described in Re-
mark 11 are used for DBSCAN. For any (cid:15) > 0, there exists

Kl · (log n)2 ≤ k ≤ Ku · (log n)2D/(2+D) · n2β/(2β+D),

and Kl and Ku are positive constants depending
δ, ˇCβ, ˆCβ, β, τ, D, ||f ||∞, λ0, rs, rc.

Then Theorem 1 and 2 hold (replacing d with D in Al-
gorithm 1) for this setting of DBSCAN and thus taking
k ≈ n2β/(2β+D) gives us the optimal estimation rate of
O(n−1/(2β+D)). A straightforward modiﬁcation of Corol-
lary 1 also holds. This is discussed further in the Appendix.

9. Conclusion

We proved that DBSCAN can obtain Hausdorff level-set
recovery rates of (cid:101)O(n−1/(2β+D)) when the data is in RD,
and (cid:101)O(n−1/(2β+d·max{1,β})) when the data lies on an em-
bedded d-dimensional manifold. The former rate is op-
timal up to log factors and the latter matches known d-
dimensional lower bounds for 0 < β ≤ 1 up to log fac-
tors. Moreover, we provided a fully data-driven procedure
to tune the parameters to attain these rates.

This shows that the procedure’s ability to recover density
level sets matches the strongest known consistency results
attained for this problem. Furthermore, we developed the
necessary tools and give the ﬁrst analysis of density level-
set estimation on manifolds, let alone with a practical pro-
cedure such as DBSCAN.

Our density estimation errors however cannot converge
faster than (cid:101)O(n−1/(2+d)), which is due in part to the error
from resolving geodesic balls with Euclidean balls. Thus
it remains an open problem whether the manifold level-set
rates are minimax optimal when β > 1.

Density Level Set Estimation on Manifolds with DBSCAN

Acknowledgements

The author is grateful to Samory Kpotufe for insightful dis-
cussions and to the anonymous reviewers for their useful
feedback.

References

Baıllo, Amparo, Cuesta-Albertos, Juan A, and Cuevas, An-
tonio. Convergence rates in nonparametric estimation of
level sets. Statistics & probability letters, 53(1):27–35,
2001.

Balakrishnan, S., Narayanan, S., Rinaldo, A., Singh, A.,
and Wasserman, L. Cluster trees on manifolds. In Ad-
vances in Neural Information Processing Systems, pp.
2679–2687, 2013.

Baraniuk, Richard G and Wakin, Michael B. Random pro-
jections of smooth manifolds. Foundations of computa-
tional mathematics, 9(1):51–77, 2009.

Berry, Tyrus and Sauer, Timothy. Density estimation on
manifolds with boundary. Computational Statistics &
Data Analysis, 107:1–17, 2017.

Biau, G´erard, Cadre, Benoˆıt, and Pelletier, Bruno. Exact
rates in density support estimation. Journal of Multi-
variate Analysis, 99(10):2185–2207, 2008.

Cadre, Benoıt. Kernel estimation of density level sets.
Journal of multivariate analysis, 97(4):999–1023, 2006.

Campello, Ricardo JGB, Moulavi, Davoud, Zimek, Arthur,
and Sander, J¨org. Hierarchical density estimates for
data clustering, visualization, and outlier detection.
ACM Transactions on Knowledge Discovery from Data
(TKDD), 10(1):5, 2015.

Carmichael, J., George, G., and Julius, R. Finding natural

clusters. Systematic Zoology, 1968.

Chaudhuri, K. and Dasgupta, S. Rates for convergence for
the cluster tree. In Advances in Neural Information Pro-
cessing Systems, 2010.

Chaudhuri, K., Dasgupta, S., Kpotufe, S., and von
Luxburg, U. Consistent procedures for cluster tree esti-
mation and pruning. IEEE Transactions on Information
Theory, 2014.

Chen, Yen-Chi, Genovese, Christopher R, and Wasserman,
Larry. Density level sets: Asymptotics, inference, and
visualization. Journal of the American Statistical Asso-
ciation, (just-accepted), 2016.

Cuevas, A. and Fraiman, R. A plug-in approach to support
estimation. Annals of Statistics, pp. 2300–2312, 1997.

Dasgupta, S. and Kpotufe, S. Optimal rates for k-nn density
and mode estimation. In Advances in Neural Information
Processing Systems, 2014.

Ester, M., Kriegel, H., Sander, J., and Xu, X. A density-
based algorithm for discovering clusters in large spatial
databases with noise. KDD, 96(34):226–231, 1996.

Farahmand, A., Szepesvari, C., and Audibert, J. Manifold-

adaptive dimension estimation. ICML, 2007.

Genovese,

Christopher,

Perone-Paciﬁco, Marco,
Verdinelli, Isabella, and Wasserman, Larry. Mini-
max manifold estimation. Journal of machine learning
research, 13(May):1263–1291, 2012.

Hartigan, J. Clustering algorithms. Wiley, 1975.

Hein, Matthias and Audibert, Jean-Yves. Intrinsic dimen-
sionality estimation of submanifolds in rd. ICML, 2005.

Hendriks, Harrie. Nonparametric estimation of a probabil-
ity density on a riemannian manifold using fourier ex-
pansions. The Annals of Statistics, pp. 832–849, 1990.

Jiang, Heinrich. Uniform convergence rates for kernel den-
International Conference on Machine

sity estimation.
Learning (ICML), 2017.

Jiang, Heinrich and Kpotufe, Samory. Modal-set estima-
tion with an application to clustering. Proceedings of the
20th International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS), 2017.

Kegl, Balazs. Intrinsic dimension estimation using packing

numbers. NIPS, 2002.

Kim, Yoon Tae and Park, Hyun Suk. Geometric struc-
tures arising from kernel density estimation on rieman-
nian manifolds. Journal of Multivariate Analysis, 114:
112–126, 2013.

Kpotufe, S. and von Luxburg, U. Pruning nearest neighbor
In International Conference on Machine

cluster trees.
Learning, 2011.

Chazal, F. An upper bound for the volume of geodesic balls

in submanifolds of euclidean spaces. 2013.

Levina, Elizaveta and Bickel, Peter J. Maximum likelihood

estimation of intrinsic dimension. NIPS, 2004.

Chazal, Fr´ed´eric, Glisse, Marc, Labru`ere, Catherine, and
Michel, Bertrand. Convergence rates for persistence dia-
gram estimation in topological data analysis. Journal of
Machine Learning Research, 16:3603–3635, 2015.

Maier, Markus, Hein, Matthias, and von Luxburg, Ulrike.
Optimal construction of k-nearest-neighbor graphs for
identifying noisy clusters. Theoretical Computer Sci-
ence, 410(19):1749–1764, 2009.

Density Level Set Estimation on Manifolds with DBSCAN

Niyogi, Partha, Smale, Stephen, and Weinberger, Shmuel.
Finding the homology of submanifolds with high conﬁ-
dence from random samples. Discrete & Computational
Geometry, 39(1-3):419–441, 2008.

Ozakin, Arkadas and Gray, Alexander G. Submanifold

density estimation. pp. 1375–1382, 2009.

Pelletier, Bruno. Kernel density estimation on riemannian
manifolds. Statistics & probability letters, 73(3):297–
304, 2005.

Pettis, K., Bailey, T., and Jain, A. An intrinsic dimension-
IEEE

ality estimator from near-neighbor information.
Transactions on PAMI, 1979.

Polonik, Wolfgang. Measuring mass concentrations and
estimating density contour clusters-an excess mass ap-
proach. The Annals of Statistics, pp. 855–881, 1995.

Rigollet, P. and Vert, R. Fast rates for plug-in estimators of
density level sets. Bernoulli, 15(4):1154–1178, 2009.

Rinaldo, Alessandro and Wasserman, Larry. Generalized
density clustering. The Annals of Statistics, pp. 2678–
2722, 2010.

Rinaldo, Alessandro, Singh, Aarti, Nugent, Rebecca, and
Wasserman, Larry. Stability of density-based cluster-
ing. Journal of Machine Learning Research, 13(Apr):
905–948, 2012.

Singh, Aarti, Scott, Clayton, Nowak, Robert, et al. Adap-
tive hausdorff estimation of density level sets. The An-
nals of Statistics, 37(5B):2760–2782, 2009.

Sriperumbudur, Bharath K and Steinwart, Ingo. Consis-
tency and rates for clustering with dbscan. pp. 1090–
1098, 2012.

Steinwart, I. Adaptive density level set clustering. In 24th

Annual Conference on Learning Theory, 2011.

Steinwart, Ingo et al. Fully adaptive density-based cluster-
ing. The Annals of Statistics, 43(5):2132–2167, 2015.

Stuetzle, Werner and Nugent, Rebecca. A generalized sin-
gle linkage method for estimating the cluster tree of a
density. Journal of Computational and Graphical Statis-
tics, 19(2):397–418, 2010.

Tsybakov, Alexandre B et al. On nonparametric estimation
of density level sets. The Annals of Statistics, 25(3):948–
969, 1997.

Walther, Guenther. Granulometric smoothing. The Annals

of Statistics, pp. 2273–2299, 1997.

Willett, RM and Nowak, Robert D. Minimax optimal level-
set estimation. IEEE Transactions on Image Processing,
16(12):2965–2979, 2007.

