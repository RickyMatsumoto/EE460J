OptNet: Supplementary Material

Brandon Amos J. Zico Kolter

A. MNIST Experiment

B. Denoising Experiment Details

Figure 7 shows the error of the fully connected network
on the denoising task and Figure 8 shows the error of the
OptNet ﬁne-tuned TV solution.

In this section we consider the integration of QP OptNet
layers into a traditional fully connected network for the
MNIST problem. The results here show only very marginal
improvement if any over a fully connected layer (MNIST,
after all, is very fairly well-solved by a fully connected net-
work, let alone a convolution network). But our main point
of this comparison is simply to illustrate that we can in-
clude these layers within existing network architectures and
efﬁciently propagate the gradients through the layer.

Speciﬁcally we use a FC600-FC10-FC10-SoftMax fully
connected network and compare it to a FC600-FC10-
Optnet10-SoftMax network, where the numbers after each
layer indicate the layer size. The OptNet layer in this case
includes only inequality constraints and the previous layer
is only used in the linear objective term p(zi) = zi. To keep
Q (cid:31) 0, we use a Cholesky factorization Q = LLT + (cid:15)I
and directly learn L (without any information from the pre-
vious layer). We also directly learn A and G, and to ensure
a feasible solution always exists, we select some learnable
z0 and s0 and set b = Az0 and h = Gz0 + s0.

Figure 6 shows that the results are similar for both networks
with slightly lower error and less variance in the OptNet
network.

Figure 6. Training performance on MNIST; top: fully connected
network; bottom: OptNet as ﬁnal layer.)

Figure 7. Error of the fully connected network for denoising

Figure 8. Error rate from ﬁne-tuning the TV solution for denois-
ing

C. Representational power of the QP OptNet

layer

This section contains proofs for those results we highlight
in Section 3.2. As mentioned before, these proofs are all
quite straightforward and follow from well-known proper-
ties, but we include them here for completeness.

C.1. Proof of Theorem 1

Proof. The fact that an OptNet layer is subdifferentiable
from strictly convex QPs (Q (cid:31) 0) follows directly from
the well-known result that the solution of a strictly convex
QP is continuous (though not everywhere differentiable).
Our proof essentially just boils down to showing this fact,

050100150200Epoch0.00.51.01.52.02.53.03.54.0ErrorTrainTest050100150200Epoch0.00.51.01.52.02.53.03.54.0ErrorTrainTest050100150200Epoch1520253035404550ErrorTrainTest01020304050Epoch101112131415161718ErrorTrainTestOptNet: Supplementary Material

though we do so by explicitly showing that there is a unique
solution to the Jacobian equations (6) that we presented
earlier, except on a measure zero set. This measure zero
set consists of QPs with degenerate solutions, points where
inequality constraints can hold with equality yet also have
zero-valued dual variables. For simplicity we assume that
A has full row rank, but this can be relaxed.

From the complementarity condition, we have that at a pri-
mal dual solution (z(cid:63), λ(cid:63), ν(cid:63))

(Gz(cid:63) − h)i < 0 → λ(cid:63)
i = 0
i > 0 → (Gz(cid:63) − h)i = 0
λ(cid:63)

(13)

(i.e., we cannot have both these terms non-zero).

First we consider the (typical) case where exactly one of
(Gz(cid:63) − h)i and λ(cid:63)
i is zero. Then the KKT differential ma-
trix

D(λ(cid:63))G D(Gz(cid:63) − h)

(14)





Q

A





AT
0
0

(the left hand side of (6)) is non-singular. To see this, note
that if we let I be the set where λ(cid:63)

i > 0, then the matrix


AT
0
0

 =

(15)

D(λ(cid:63))GI D(Gz(cid:63) − h)I





Q

A





Q
D(λ(cid:63))GI
A

0
I AT
GT
0
0
0
0





GT

0

GT
I

is non-singular (scaling the second block by D(λ(cid:63))−1 gives
a standard KKT system (Boyd & Vandenberghe, 2004, Sec-
tion 10.4), which is nonsingular for invertible Q and [GT
I
AT ] with full column rank, which must hold due to our
condition on A and the fact that there must be less than n
total tight constraints at the solution. Also note that for any
i (cid:54)∈ I, only the D(Gz(cid:63) −h)ii term is non-zero for the entire
row in the second block of the matrix. Thus, if we want to
solve the system





Q

A

D(λ(cid:63))GI D(Gz(cid:63) − h)I

GT
I

0

AT
0
0















 =



a
b
 (16)
c

z
λ
ν

we simply ﬁrst set λi = bi/(Gz(cid:63) − h)i for i (cid:54)∈ I and then
solve the nonsingular system





Q
D(λ(cid:63))GI
A

I AT
GT
0
0
0
0













 =



z
λI
ν

a − GT
bI
c.



¯I λ ¯I

 (17)

Alternatively, suppose that we have both λ(cid:63)
i = 0 and
(Gz(cid:63) − h)i = 0. Then although the KKT matrix is now
singular (any row for which λ(cid:63)
i = 0 and (Gz(cid:63) − h)i = 0

will be all zero), there still exists a solution to the system
(6), because the right hand side is always in the range of
D(λ(cid:63)) and so will also be zero for these rows. In this case
there will no longer be a unique solution, corresponding to
the subdifferentiable but not differentiable case.

C.2. Proof of Theorem 2

Proof. The proof that an OptNet layer can represent any
piecewise linear univariate function relies on the fact that
we can represent any such function in “sum-of-max” form

f (x) =

wi max{aix + b, 0}

(18)

k
(cid:88)

i=1

where wi ∈ {−1, 1}, ai, bi ∈ R (to do so, simply proceed
left to right along the breakpoints of the function adding
a properly scaled linear term to ﬁt the next piecewise sec-
tion). The OptNet layer simply represents this function di-
rectly.

That is, we encode the optimization problem

minimize
z∈R,t∈Rk

(cid:107)t(cid:107)2

2 + (z − wT t)2

subject to aix + bi ≤ ti,

i = 1, . . . , k

(19)

Clearly, the objective here is minimized when z = wT t,
and t is as small as possible, meaning each t must either be
at its bound aix + b ≤ ti or, if aix + b < 0, then ti = 0 will
be the optimal solution due to the objective function. To
obtain a multivariate but elementwise function, we simply
apply this function to each coordinate of the input x.

To see the speciﬁc case of a ReLU network, note that the
layer

z = max{W x + b, 0}

(20)

is simply equivalent to the OptNet problem

minimize
z

(cid:107)z − W x − b(cid:107)2
2

subject to z ≥ 0.

(21)

C.3. Proof of Theorem 3

Proof. The ﬁnal theorem simply states that a two-layer
ReLU network (more speciﬁcally, a ReLU followed by a
linear layer, which is sufﬁcient to achieve a universal func-
tion approximator), can often require exponentially many
more units to approximate a function speciﬁed by an Opt-
Net layer. That is, we consider a single-output ReLU net-
work, much like in the previous section, but deﬁned for
multi-variate inputs.

f (x) =

wi max{aT

i x + b, 0}

(22)

m
(cid:88)

i=1

OptNet: Supplementary Material

yet it does not seem possible to represent this in closed form
the closed form solution of such a
as a simple network:
projection operator requires sorting or ﬁnding a particular
median term of the data (Duchi et al., 2008), which is not
feasible with a single layer for any form of network that
we are aware of. Yet for simplicity we stated the theorem
above using just ReLU networks and a straightforward ex-
ample that works even in two dimensions.

Figure 9. Creases for a three-term pointwise maximum (left), and
a ReLU network (right).

Although there are many functions that such a network can-
not represent, for illustration we consider a simple case of
a maximum of three linear functions

f (cid:48)(x) = max{aT

1 x, aT

2 x, aT

3 x}

(23)

To see why a ReLU is not capable of representing this func-
tion exactly, even for x ∈ R2, note that any sum-of-max
function, due to the nature of the term max{aT
i x + bi, 0} as
stated above must have “creases” (breakpoints in the piece-
wise linear function), than span the entire input space; this
is in contrast to the max terms, which can have creases that
only partially span the space. This is illustrated in Figure
9. It is apparent, therefore, that the two-layer ReLU cannot
exactly approximate the three maximum term (any ReLU
network would necessarily have a crease going through one
of the linear region of the original function). Yet this max
function can be captured by a simple OptNet layer

minimize
z

z2

subject to aT

i x ≤ z, i = 1, . . . , 3.

(24)

The fact that the ReLU network is a universal function
approximator means that the we are able to approximate
the three-max term, but to do so means that we require
a dense covering of points over the input space, choose
an equal number of ReLU terms, then choose coefﬁcients
such that we approximate the underlying function on this
points; however, for a large enough radius this will require
an exponential size covering to approximate the underlying
function arbitrarily closely.

Although the example here in this proof is quite simple
(and perhaps somewhat limited, since for example the func-
tion can be exactly approximated using a “Maxout” net-
work), there are a number of other such functions for which
we have been unable to ﬁnd any compact representation.
For example, projection of a point on to the simplex is eas-
ily written as the OptNet layer

minimize
z

(cid:107)z − x(cid:107)2
2

subject to z ≥ 0, 1T z = 1

(25)

aT1xaT2xaT3x