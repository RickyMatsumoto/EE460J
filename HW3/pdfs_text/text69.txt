Dynamic Word Embeddings

Robert Bamler 1 Stephan Mandt 1

Abstract

We present a probabilistic language model for
time-stamped text data which tracks the se-
mantic evolution of individual words over time.
The model represents words and contexts by
latent trajectories in an embedding space. At
each moment in time,
the embedding vectors
are inferred from a probabilistic version of
word2vec (Mikolov et al., 2013b). These em-
bedding vectors are connected in time through
a latent diffusion process. We describe two
scalable variational inference algorithms—skip-
gram smoothing and skip-gram ﬁltering—that al-
low us to train the model jointly over all times;
thus learning on all data while simultaneously al-
lowing word and context vectors to drift. Experi-
mental results on three different corpora demon-
strate that our dynamic model infers word em-
bedding trajectories that are more interpretable
and lead to higher predictive likelihoods than
competing methods that are based on static mod-
els trained separately on time slices.

1. Introduction

Language evolves over time and words change their mean-
ing due to cultural shifts, technological inventions, or po-
litical events. We consider the problem of detecting shifts
in the meaning and usage of words over a given time span
based on text data. Capturing these semantic shifts requires
a dynamic language model.

Word embeddings are a powerful tool for modeling se-
mantic relations between individual words (Bengio et al.,
2003; Mikolov et al., 2013a; Pennington et al., 2014; Mnih
& Kavukcuoglu, 2013; Levy & Goldberg, 2014; Vilnis &
McCallum, 2014; Rudolph et al., 2016). Word embed-

1Disney Research,

PA 15213, USA. Correspondence
<Robert.Bamler@disneyresearch.com>,
<Stephan.Mandt@disneyresearch.com>.

4720 Forbes Avenue, Pittsburgh,
Robert Bamler
Stephan Mandt

to:

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

dings model the distribution of words based on their sur-
rounding words in a training corpus, and summarize these
statistics in terms of low-dimensional vector representa-
tions. Geometric distances between word vectors reﬂect
semantic similarity (Mikolov et al., 2013a) and difference
vectors encode semantic and syntactic relations (Mikolov
et al., 2013c), which shows that they are sensible represen-
tations of language. Pre-trained word embeddings are use-
ful for various supervised tasks, including sentiment anal-
ysis (Socher et al., 2013b), semantic parsing (Socher et al.,
2013a), and computer vision (Fu & Sigal, 2016). As un-
supervised models, they have also been used for the ex-
ploration of word analogies and linguistics (Mikolov et al.,
2013c).

Word embeddings are currently formulated as static mod-
els, which assumes that the meaning of any given word is
the same across the entire text corpus. In this paper, we
propose a generalization of word embeddings to sequential
data, such as corpora of historic texts or streams of text in
social media.

Current approaches to learning word embeddings in a dy-
namic context rely on grouping the data into time bins
and training the embeddings separately on these bins (Kim
et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016).
This approach, however, raises three fundamental prob-
lems. First, since word embedding models are non-convex,
training them twice on the same data will lead to different
results. Thus, embedding vectors at successive times can
only be approximately related to each other, and only if the
embedding dimension is large (Hamilton et al., 2016). Sec-
ond, dividing a corpus into separate time bins may lead to
training sets that are too small to train a word embedding
model. Hence, one runs the risk of overﬁtting to few data
whenever the required temporal resolution is ﬁne-grained,
as we show in the experimental section. Third, due to the
ﬁnite corpus size the learned word embedding vectors are
subject to random noise. It is difﬁcult to disambiguate this
noise from systematic semantic drifts between subsequent
times, in particular over short time spans, where we expect
only minor semantic drift.

In this paper, we circumvent these problems by introducing
a dynamic word embedding model. Our contributions are
as follows:

Dynamic Word Embeddings

Figure 1. Evolution of the 10 words that changed the most in cosine distance from 1850 to 2008 on Google books, using skip-gram
ﬁltering (proposed). Red (blue) curves correspond to the ﬁve closest words at the beginning (end) of the time span, respectively.

• We derive a probabilistic state space model where
word and context embeddings evolve in time accord-
ing to a diffusion process. It generalizes the skip-gram
model (Mikolov et al., 2013b; Barkan, 2017) to a dy-
namic setup, which allows end-to-end training. This
leads to continuous embedding trajectories, smoothes
out noise in the word-context statistics, and allows us
to share information across all times.

• We propose two scalable black-box variational in-
ference algorithms (Ranganath et al., 2014; Rezende
et al., 2014) for ﬁltering and smoothing. These al-
gorithms ﬁnd word embeddings that generalize bet-
ter to held-out data. Our smoothing algorithm carries
out efﬁcient black-box variational inference for struc-
tured Gaussian variational distributions with tridiago-
nal precision matrices, and applies more broadly.

• We analyze three massive text corpora that span over
long periods of time. Our approach allows us to auto-
matically ﬁnd the words whose meaning changes the
most. It results in smooth word embedding trajecto-
ries and therefore allows us to measure and visual-
ize the continuous dynamics of the entire embedding
cloud as it deforms over time.

Figure 1 exempliﬁes our method. The plot shows a ﬁt of
our dynamic skip-gram model to Google books (we give
details in section 5). We show the ten words whose mean-
ing changed most drastically in terms of cosine distance
over the last 150 years. We thereby automatically dis-
cover words such as “computer” or “radio” whose meaning
changed due to technological advances, but also words like

“peer” and “notably” whose semantic shift is less obvious.

Our paper is structured as follows. In section 2 we discuss
related work, and we introduce our model in section 3. In
section 4 we present two efﬁcient variational inference al-
gorithms for our dynamic model. We show experimental
results in section 5. Section 6 summarizes our ﬁndings.

2. Related Work

Probabilistic models that have been extended to latent time
series models are ubiquitous (Blei & Lafferty, 2006; Wang
et al., 2008; Sahoo et al., 2012; Gultekin & Paisley, 2014;
Charlin et al., 2015; Ranganath et al., 2015; Jerfel et al.,
2017), but none of them relate to word embeddings. The
closest of these models is the dynamic topic model (Blei &
Lafferty, 2006; Wang et al., 2008), which learns the evo-
lution of latent topics over time. Topic models are based
on bag-of-word representations and thus treat words as
symbols without modelling their semantic relations. They
therefore serve a different purpose.

Mikolov et al. (2013a;b) proposed the skip-gram model
with negative sampling (word2vec) as a scalable word em-
bedding approach that relies on stochastic gradient de-
scent. This approach has been formulated in a Bayesian
setup (Barkan, 2017), which we discuss separately in sec-
tion 3.1. These models, however, do not allow the word
embedding vectors to change over time.

Several authors have analyzed different statistics of text
data to analyze semantic changes of words over time (Mi-
halcea & Nastase, 2012; Sagi et al., 2011; Kim et al., 2014;

date0.00.20.40.60.81.0cosinedistancetextphotographsprefacereferencesmemorandumproductivitydiminishingelasticityaggregateutility1.marginaldateexactaccuratesamplingobservercleversoftwareusermachinedeviceprinter2.computerdatequamautquodArizonaaufEffectsEffectInﬂuenceAmerpp.3.versusdatenominationoffendercustodyassignmentvotingwillingnessloyaltydedicationadherencedevotion4.commitmentdateperipheralbasaloscortexnuclearTVtelephonenewspapersphonecomputer5.radio1850190019502000date0.00.20.40.60.81.0cosinedistanceobjectiveperceptionsubjectiveverbverbspossibilitiespotentiallypossibilityriskslikelihood6.potential1850190019502000datemateriallyfaithfullyeffectuallyclearlyabundantlyespeciallyparticularlyincludingnotableexempliﬁed7.notably1850190019502000datenoblemanlawyerknightmembernobilityclassroomcognitivenetworksteacherparent8.peer1850190019502000dateemphaticallysigniﬁcantgesturessmiledsharplyconsiderablysubstantiallygreatlymateriallyslightly9.signiﬁcantly1850190019502000dateprocessingPlanningFreudenzymespecializedcomputerWebtechnologyapplicationsdesign10.softwareDynamic Word Embeddings

Figure 2. a) Bayesian skip-gram model (Barkan, 2017). b) The
dynamic skip-gram model (proposed) connects T copies of the
Bayesian skip-gram model via a latent time series prior on the
embeddings.

Kulkarni et al., 2015; Hamilton et al., 2016). None of them
explicitly model a dynamical process; instead, they slice
the data into different time bins, ﬁt the model separately
on each bin, and further analyze the embedding vectors in
post-processing. By construction, these static models can
therefore not share statistical strength across time. This
limits the applicability of static models to very large cor-
pora.

Most related to our approach are methods based on word
embeddings. Kim et al. (2014) ﬁt word2vec separately on
different time bins, where the word vectors obtained for
the previous bin are used to initialize the algorithm for the
next time bin. The bins have to be sufﬁciently large and the
found trajectories are not as smooth as ours, as we demon-
strate in this paper. Hamilton et al. (2016) also trained
word2vec separately on several large corpora from differ-
ent decades. If the embedding dimension is large enough
(and hence the optimization problem less non-convex), the
authors argue that word embeddings at nearby times ap-
proximately differ by a global rotation in addition to a small
semantic drift, and they approximately compute this ro-
tation. As the latter does not exist in a strict sense, it is
difﬁcult to distinguish artifacts of the approximate rotation
from a true semantic drift. As discussed in this paper, both
variants result in trajectories which are noisier.1

3. Model

We propose the dynamic skip-gram model, a generaliza-
tion of the skip-gram model (word2vec) (Mikolov et al.,
2013b) to sequential text data. The model ﬁnds word em-
bedding vectors that continuously drift over time, allowing
to track changes in language and word usage over short and
long periods of time. Dynamic skip-gram is a probabilistic
model which combines a Bayesian version of the skip-gram
model (Barkan, 2017) with a latent time series. It is jointly

1 Rudolph & Blei (2017) independently developed a similar
model, using a different likelihood model. Their approach uses
a non-Bayesian treatment of the latent embedding trajectories,
which makes the approach less robust to noise when the data per
time step is small.

trained end-to-end and scales to massive data by means of
approximate Bayesian inference.

The observed data consist of sequences of words from a
ﬁnite vocabulary of size L. In section 3.1, all sequences
(sentences from books, articles, or tweets) are considered
time-independent; in section 3.2 they will be associated
with different time stamps. The goal is to maximize the
probability of every word that occurs in the data given its
surrounding words within a so-called context window. As
detailed below, the model learns two vectors ui, vi ∈ Rd
for each word i in the vocabulary, where d is the embed-
ding dimension. We refer to ui as the word embedding
vector and to vi as the context embedding vector.

3.1. Bayesian Skip-Gram Model

The Bayesian skip-gram model (Barkan, 2017) is a prob-
abilistic version of word2vec (Mikolov et al., 2013b) and
forms the basis of our approach. The graphical model is
shown in Figure 2a). For each pair of words i, j in the
vocabulary, the model assigns probabilities that word i ap-
pears in the context of word j. This probability is σ(u(cid:62)
i vj)
with the sigmoid function σ(x) = 1/(1 + e−x). Let zij ∈
{0, 1} be an indicator variable that denotes a draw from that
probability distribution, hence p(zij = 1) = σ(u(cid:62)
i vj). The
generative model assumes that many word-word pairs (i, j)
are uniformly drawn from the vocabulary and tested for be-
ing a word-context pair; hence a separate random indicator
zij is associated with each drawn pair.

Focusing on words and their neighbors in a context win-
dow, we collect evidence of word-word pairs for which
zij = 1. These are called the positive examples. De-
note n+
ij the number of times that a word-context pair
(i, j) is observed in the corpus. This is a sufﬁcient statis-
tic of the model, and its contribution to the likelihood is
p(n+
ij . However, the generative pro-
cess also assumes the possibility to reject word-word pairs
if zij = 0. Thus, one needs to construct a ﬁctitious sec-
ond training set of rejected word-word pairs, called nega-
tive examples. Let the corresponding counts be n−
ij. The
total likelihood of both positive and negative examples is
then

ij|ui, vj) = σ(u(cid:62)

i vj)n+

p(n+, n−|U, V ) =

σ(u(cid:62)

i vj)n+

ij σ(−u(cid:62)

i vj)n−
ij .

(1)

L
(cid:89)

i,j=1

Above we used the antisymmetry σ(−x) = 1 − σ(x). In
our notation, dropping the subscript indices for n+ and n−
denotes the entire L × L matrices, U = (u1, · · · , uL) ∈
Rd×L is the matrix of all word embedding vectors, and V
is deﬁned analogously for the context vectors. To con-
struct negative examples, one typically chooses n−
ij ∝
P (i)P (j)3/4 (Mikolov et al., 2013b), where P (i) is the

Dynamic Word Embeddings

frequency of word i in the training corpus. Thus, n− is
well-deﬁned up to a constant factor which has to be tuned.

Deﬁning n± = (n+, n−) the combination of both positive
and negative examples, the resulting log likelihood is

log p(n±|U, V ) =

L
(cid:88)

i,j=1

(cid:0)n+

ij log σ(u(cid:62)

i vj) + n−

ij log σ(−u(cid:62)

i vj)(cid:1).

(2)

This is exactly the objective of the (non-Bayesian) skip-
gram model, see (Mikolov et al., 2013b). The count ma-
trices n+ and n− are either pre-computed for the entire
corpus, or estimated based on stochastic subsamples from
the data in a sequential way, as done by word2vec. Barkan
(2017) gives an approximate Bayesian treatment of the
model with Gaussian priors on the embeddings.

3.2. Dynamic Skip-Gram Model

The key extension of our approach is to use a Kalman ﬁl-
ter as a prior for the time-evolution of the latent embed-
dings (Welch & Bishop, 1995). This allows us to share
information across all times while still allowing the em-
beddings to drift.

Notation. We consider a corpus of T documents which
were written at time stamps τ1 < . . . < τT . For each time
step t ∈ {1, . . . , T } the sufﬁcient statistics of word-context
pairs are encoded in the L × L matrices n+
t , n−
t of positive
ij,t and n−
and negative counts with matrix elements n+
ij,t,
respectively. Denote Ut = (u1,t, · · · , uL,t) ∈ Rd×L the
matrix of word embeddings at time t, and deﬁne Vt corre-
spondingly for the context vectors. Let U, V ∈ RT ×d×L
denote the tensors of word and context embeddings across
all times, respectively.

Model. The graphical model is shown in Figure 2b). We
consider a diffusion process of the embedding vectors over
time. The variance σ2
t of the transition kernel is
σ2
t = D(τt+1 − τt),
where D is a global diffusion constant and (τt+1 −τt) is the
time between subsequent observations (Welch & Bishop,
1995). At every time step t, we add an additional Gaussian
prior with zero mean and variance σ2
0 which prevents the
embedding vectors from growing very large, thus
p(Ut+1|Ut) ∝ N (Ut, σ2

t ) N (0, σ2
Computing the normalization, this results in
(cid:18)

0).

(3)

(4)

(cid:19)

Ut+1|Ut ∼ N

Vt+1|Vt ∼ N

Ut
t /σ2
1 + σ2
0
Vt
t /σ2
1 + σ2
0

,

,

1
t + σ−2
σ−2
0
1
t + σ−2
σ−2
0

I

I

,

.

(cid:19)

(cid:18)

(5)

(6)

In practice, σ0 (cid:29) σt, so the damping to the origin is very
weak. This is also called Ornstein-Uhlenbeck process (Uh-
lenbeck & Ornstein, 1930). We recover the Wiener process
for σ0 → ∞, but σ0 < ∞ prevents the latent time series
from diverging to inﬁnity. At time index t = 1, we deﬁne
p(U1|U0) ≡ N (0, σ2

0I) and do the same for V1.

Our joint distribution factorizes as follows:

p(n±, U, V ) =

p(Ut+1|Ut) p(Vt+1|Vt)

T −1
(cid:89)

t=0

T
(cid:89)

L
(cid:89)

×

t=1

i,j=1

p(n±

ij,t|ui,t, vj,t)

(7)

The prior model enforces that the model learns embedding
vectors which vary smoothly across time. This allows to as-
sociate words unambiguously with each other and to detect
semantic changes. The model efﬁciently shares informa-
tion across the time domain, which allows to ﬁt the model
even in setups where the data at every given point in time
are small, as long as the data in total are large.

4. Inference

We discuss two scalable approximate inference algorithms.
Filtering uses only information from the past; it is required
in streaming applications where the data are revealed to
us sequentially. Smoothing is the other inference method,
which learns better embeddings but requires the full se-
quence of documents ahead of time.

In Bayesian inference, we start by formulating a joint dis-
tribution (Eq. 7) over observations n± and parameters U
and V , and we are interested in the posterior distribution
over parameters conditioned on observations,

p(U, V |n±) =

p(n±, U, V )
(cid:82) p(n±, U, V ) dU dV

(8)

The problem is that the normalization is intractable. In vari-
ational inference (VI) (Jordan et al., 1999; Blei et al., 2016)
one sidesteps this problem and approximates the posterior
with a simpler variational distribution qλ(U, V ) by mini-
mizing the Kullback-Leibler (KL) divergence to the poste-
rior. Here, λ summarizes all parameters of the variational
distribution, such as the means and variances of a Gaussian,
see below. Minimizing the KL divergence is equivalent to
optimizing the evidence lower bound (ELBO) (Blei et al.,
2016),

L(λ) = Eqλ[log p(n±, U, V )]−Eqλ[log qλ(U, V )].

(9)

For a restricted class of models, the ELBO can be com-
puted in closed-form (Hoffman et al., 2013). Our model is

Dynamic Word Embeddings

non-conjugate and requires instead black-box VI using the
reparameterization trick (Rezende et al., 2014; Kingma &
Welling, 2014).

4.1. Skip-Gram Filtering

In many applications such as streaming, the data arrive se-
quentially. Thus, we can only condition our model on past
and not on future observations. We will ﬁrst describe in-
ference in such a (Kalman) ﬁltering setup (Kalman et al.,
1960; Welch & Bishop, 1995).

In the ﬁltering scenario, the inference algorithm iteratively
updates the variational distribution q as evidence from each
time step t becomes available. We thereby use a variational
distribution that factorizes across all times, q(U, V ) =
(cid:81)T
t=1 q(Ut, Vt) and we update the variational factor at a
given time t based on the evidence at time t and the approx-
imate posterior of the previous time step. Furthermore, at
every time t we use a fully-factorized distribution:

L
(cid:89)

i=1

q(Ut, Vt) =

N (ui,t; µui,t, Σui,t) N (vi,t; µvi,t.Σvi,t),

The variational parameters are the means µui,t, µvi,t ∈ Rd
and the covariance matrices Σui,t and Σvi,t, which we re-
strict to be diagonal (mean-ﬁeld approximation).

We now describe how we sequentially compute q(Ut, Vt)
and use the result to proceed to the next time step. As other
Markovian dynamical systems, our model assumes the fol-
lowing recursion,

p(Ut, Vt|n±

1:t) ∝ p(n±

t |Ut, Vt) p(Ut, Vt|n±

1:t−1).

(10)

Within our variational approximation, the ELBO (Eq. 9)
therefore separates into a sum of T terms, L = (cid:80)
t Lt with

Lt = E[log p(n±

t |Ut, Vt)] + E[log p(Ut, Vt|n±

1:t−1)]

prior is a fully factorized distribution p(Ut, Vt|n±
(cid:81)L
i=1 N (ui,t; ˜µui,t, ˜Σui,t) N (vi,t; ˜µvi,t, ˜Σvit) with
(cid:0)Σui,t−1 + σ2
t I(cid:1)−1

(cid:104)(cid:0)Σui,t−1 + σ2

˜µui,t = ˜Σui,t

˜Σui,t =

+ (1/σ2

t I(cid:1)−1

µui,t−1;

0)I

(cid:105)−1

.

1:t−1) ≈

(13)

Analogous update equations hold for ˜µvi,t and ˜Σvi,t. Thus,
the second contribution in Eq. 11 (the prior) yields a closed-
form expression. We can therefore compute its gradient.

4.2. Skip-Gram Smoothing

In contrast to ﬁltering, where inference is conditioned on
past observations until a given time t, (Kalman) smoothing
performs inference based on the entire sequence of obser-
vations n±
1:T . This approach results in smoother trajectories
and typically higher likelihoods than with ﬁltering, because
evidence is used from both future and past observations.

Besides the new inference scheme, we also use a different
variational distribution. As the model is ﬁtted jointly to all
time steps, we are no longer restricted to a variational distri-
bution that factorizes in time. For simplicity we focus here
on the variational distribution for the word embeddings U ;
the context embeddings V are treated identically. We use a
factorized distribution over both embedding space and vo-
cabulary space,

q(U1:T ) =

q(uik,1:T ).

(14)

L
(cid:89)

d
(cid:89)

i=1

k=1

In the time domain, our variational approximation is struc-
tured. To simplify the notation we now drop the indices
for words i and embedding dimension k, hence we write
q(u1:T ) for q(uik,1:T ) where we focus on a single factor.
This factor is a multivariate Gaussian distribution in the
time domain with tridiagonal precision matrix Λ,

− E[log q(Ut, Vt)],

(11)

q(u1:T ) = N (µ, Λ−1)

(15)

where all expectations are taken under q(Ut, Vt). We com-
pute the entropy term −E[log q] in Eq. 11 analytically and
estimate the gradient of the log likelihood by sampling
from the variational distribution and using the reparam-
eterization trick (Kingma & Welling, 2014; Salimans &
Kingma, 2016). However, the second term of Eq. 11, con-
taining the prior at time t, is still intractable. We approxi-
mate the prior as

p(Ut, Vt|n±
E

1:t−1) ≡

p(Ut−1,Vt−1|n±

1:t−1)
≈ Eq(Ut−1,Vt−1)

(cid:2)p(Ut, Vt|Ut−1, Vt−1)(cid:3)
(cid:2)p(Ut, Vt|Ut−1, Vt−1)(cid:3).

(12)

The remaining expectation involves only Gaussians and
can be carried-out analytically. The resulting approximate

Both the means µ = µ1:T and the entries of the tridiago-
nal precision matrix Λ ∈ RT ×T are variational parameters.
This gives our variational distribution the interpretation of a
posterior of a Kalman ﬁlter (Blei & Lafferty, 2006), which
captures correlations in time.

We ﬁt the variational parameters by training the model
jointly on all time steps, using black-box VI and the repa-
rameterization trick. As the computational complexity of
an update step scales as Θ(L2), we ﬁrst pretrain the model
by drawing minibatches of L(cid:48) < L random words and
L(cid:48) random contexts from the vocabulary (Hoffman et al.,
2013). We then switch to the full batch to reduce the sam-
pling noise. Since the variational distribution does not fac-
torize in the time domain we always include all time steps
{1, . . . , T } in the minibatch.

Dynamic Word Embeddings

We also derive an efﬁcient algorithm that allows us to es-
timate the reparametrization gradient using Θ(T ) time and
memory, while a naive implementation of black-box varia-
tional inference with our structured variational distribution
would require Θ(T 2) of both resources. The main idea is to
parametrize Λ = B(cid:62)B in terms of its Cholesky decompo-
sition B, which is bidiagonal (Kılıc¸ & Stanica, 2013), and
to express gradients of B−1 in terms of gradients of B. We
use mirror ascent (Ben-Tal et al., 2001; Beck & Teboulle,
2003) to enforce positive deﬁniteness of B. The algorithm
is detailed in our supplementary material.

5. Experiments

We evaluate our method on three time-stamped text cor-
pora. We demonstrate that our algorithms ﬁnd smoother
embedding trajectories than methods based on a static
model. This allows us to track semantic changes of indi-
vidual words by following nearest-neighbor relations over
time. In our quantitative analysis, we ﬁnd higher predictive
likelihoods on held-out data compared to our baselines.

Algorithms and Baselines. We report results from our
proposed algorithms from section 4 and compare against
baselines from section 2:

• SGI denotes the non-Bayesian skip-gram model
with independent random initializations of word vec-
tors (Mikolov et al., 2013b). We used our own imple-
mentation of the model by dropping the Kalman ﬁl-
tering prior and point-estimating embedding vectors.
Word vectors at nearby times are made comparable by
approximate orthogonal transformations, which corre-
sponds to Hamilton et al. (2016).

• SGP denotes the same approach as above, but with
word and context vectors being pre-initialized with the
values from the previous year, as in Kim et al. (2014).

• DSG-F: dynamic skip-gram ﬁltering (proposed).

• DSG-S: dynamic skip-gram smoothing (proposed).

Data and preprocessing. Our three corpora exemplify
opposite limits both in the covered time span and in the
amount of text per time step.

2008 (the latest available). In 1800, the size of the data
is approximately ∼ 7 · 107 words. We used the 5-gram
counts, resulting in a context window size of 4.

2. We used the “State of the Union” (SoU) addresses of
U.S. presidents, which spans more than two centuries,
resulting in T = 230 different time steps and approx-
imately 106 observed words.3 Some presidents gave
both a written and an oral address; if these were less
than a week apart we concatenated them and used the
average date. We converted all words to lower case
and constructed the positive sample counts n+
ij using
a context window size of 4.

3. We used a Twitter corpus of news tweets for 21 ran-
domly drawn dates from 2010 to 2016. The median
number of tweets per day is 722. We converted all
tweets to lower case and used a context window size
of 4, which we restricted to stay within single tweets.

Hyperparameters. The vocabulary for each corpus was
constructed from the 10,000 most frequent words through-
out the given time period. In the Google books corpus, the
number of words per year grows by a factor of 200 from the
year 1800 to 2008. To avoid that the vocabulary is domi-
nated by modern words we normalized the word frequen-
cies separately for each year before adding them up.

ij n+

ij n−

ij,t/ (cid:80)

For the Google books corpus, we chose the embedding
dimension d = 200, which was also used in Kim et al.
(2014). We set d = 100 for SoU and Twitter, as d = 200
resulted in overﬁtting on these much smaller corpora. The
ratio η = (cid:80)
ij,t of negative to positive word-
context pairs was η = 1. The precise construction of the
matrices n±
is explained in the supplementary material.
t
We used the global prior variance σ2
0 = 1 for all corpora
and all algorithms, including the baselines. The diffusion
constant D controls the time scale on which information
is shared between time steps. The optimal value for D
depends on the application. A single corpus may exhibit
semantic shifts of words on different time scales, and the
optimal choice for D depends on the time scale in which
one is interested. We used D = 10−3 per year for Google
books and SoU, and D = 1 per year for the Twitter corpus,
which spans a much shorter time range. In the supplemen-
tary material, we provide details of the optimization proce-
dure.

1. We used data from the Google books corpus2 (Michel
et al., 2011) from the last two centuries (T = 209).
This amounts to 5 million digitized books and approx-
imately 1010 observed words. The corpus consists of
n-gram tables with n ∈ {1, . . . , 5}, annotated by year
of publication. We considered the years from 1800 to

2http://storage.googleapis.com/books/

ngrams/books/datasetsv2.html

Qualitative results. We show that our approach results in
smooth word embedding trajectories on all three corpora.
We can automatically detect words that undergo signiﬁcant
semantic changes over time.

Figure 1 in the introduction shows a ﬁt of the dynamic
skip-gram ﬁltering algorithm to the Google books corpus.

3http://www.presidency.ucsb.edu/sou.php

Dynamic Word Embeddings

Figure 3. Word embeddings over a sequence of years trained on
Google books, using DSG-F (proposed, top row) and compared
to the static method by Hamilton et al. (2016) (bottom). We used
dynamic t-SNE (Rauber et al., 2016) for dimensionality reduc-
tion. Colored lines in the second to fourth column indicate the tra-
jectories from the previous year. Our method infers smoother tra-
jectories with only few words that move quickly. Figure 4 shows
that these effects persist in the original embedding space.

Here, we show the ten words whose word vectors change
most drastically over the last 150 years in terms of co-
sine distance. Figure 3 visualizes word embedding clouds
over four subsequent years of Google books, where we
compare DSG-F against SGI. We mapped the normal-
ized embedding vectors to two dimensions using dynamic
t-SNE (Rauber et al., 2016) (see supplement for details).
Lines indicate shifts of word vectors relative to the preced-
ing year. In our model only few words change their position
in the embedding space rapidly, while embeddings using
SGI show strong ﬂuctuations, making the cloud’s motion
hard to track.

Figure 4 visualizes the smoothness of the trajectories di-
rectly in the embedding space (without the projection to
two dimensions). We consider differences between word
vectors in the year 1998 and the subsequent 10 years.
In more detail, we compute histograms of the Euclidean
distances ||uit − ui,t+δ|| over the word indexes i, where
δ = 1, . . . , 10 (as discussed previously, SGI uses a global
rotation to optimally align embeddings ﬁrst). In our model,
embedding vectors gradually move away from their origi-
nal position as time progresses, indicating a directed mo-
tion. In contrast, both baseline models show only little di-
rected motion after the ﬁrst time step, suggesting that most
temporal changes are due to ﬁnite-size ﬂuctuations of n±
ij,t.
Initialization schemes alone, thus, seem to have a minor
effect on smoothness.

Our approach allows us to detect semantic shifts in the us-
age of speciﬁc words. Figures 5 and 1 both show the cosine
distance between a given word and its neighboring words
(colored lines) as a function of time. Figure 5 shows results
on all three corpora and focuses on a comparison across
methods. We see that DSG-S and DSG-F (both proposed)

Figure 4. Histogram of distances between word vectors in the
year 1998 and their positions in subsequent years (colors).
DSG-F (top panel) displays a continuous growth of these dis-
In contrast, in
tances over time, indicating a directed motion.
SGP (middle) (Kim et al., 2014) and SGI (bottom) (Hamilton
et al., 2016), the distribution of distances jumps from the ﬁrst to
the second year but then remains largely stationary, indicating ab-
sence of a directed drift; i.e. almost all motion is random.

result in trajectories which display less noise than the base-
lines SGP and SGI. The fact that the baselines predict zero
cosine distance (no correlation) between the chosen word
pairs on the SoU and Twitter corpora suggests that these
corpora are too small to successfully ﬁt these models, in
contrast to our approach which shares information in the
time domain. Note that as in dynamic topic models, skip-
gram smoothing (DSG-S) may diffuse information into the
past (see ”presidential” to ”clinton-trump” in Fig. 5).

Quantitative results. We show that our approach gener-
alizes better to unseen data. We thereby analyze held-out
predictive likelihoods on word-context pairs at a given time
t, where t is excluded from the training set,

log p(n±

t | ˜Ut, ˜Vt).

(16)

1
|n±
t |
(cid:0)n+

i,j

t | = (cid:80)

(cid:1) denotes the total num-
Above, |n±
ij,t + n−
ber of word-context pairs at time τt. Since inference is dif-
ferent in all approaches, the deﬁnitions of word and con-
text embedding matrices ˜Ut and ˜Vt in Eq. 16 have to be
adjusted:

ij,t

• For SGI and SGP, we did a chronological pass
through the time sequence and used the embeddings
˜Ut = Ut−1 and ˜Vt = Vt−1 from the previous time
step to predict the statistics n±

ij,t at time step t.
• For DSG-F, we did the same pass to test n±

ij,t. We
thereby set ˜Ut and ˜Vt to be the modes Ut−1, Vt−1 of
the approximate posterior at the previous time step.

• For DSG-S, we held out 10%, 10% and 20% of the
documents from the Google books, SoU, and Twitter
corpora for testing, respectively. After training, we
estimated the word (context) embeddings ˜Ut ( ˜Vt) in

Dynamic Word Embeddings

Figure 5. Smoothness of word embedding trajectories, compared across different methods. We plot the cosine distance between two
words (see captions) over time. High values indicate similarity. Our methods (DSG-S and DSG-F) ﬁnd more interpretable trajectories
than the baselines (SGI and SGP). The different performance is most pronounced when the corpus is small (SoU and Twitter).

Figure 6. Predictive log-likelihoods (Eq. 16) for two proposed versions of the dynamic skip-gram model (DSG-F & DSG-S) and two
competing methods SGI (Hamilton et al., 2016) and SGP (Kim et al., 2014) on three different corpora (high values are better).

Eq. 16 by linear interpolation between the values of
Ut−1 (Vt−1) and Ut+1 (Vt+1) in the mode of the vari-
ational distribution, taking into account that the time
stamps τt are in general not equally spaced.

The predictive likelihoods as a function of time τt are
shown in Figure 6. For the Google Books corpus (left panel
in ﬁgure 6), the predictive log-likelihood grows over time
with all four methods. This must be an artifact of the cor-
pus since SGI does not carry any information of the past.
A possible explanation is the growing number of words per
year from 1800 to 2008 in the Google Books corpus. On
all three corpora, differences between the two implementa-
tions of the static model (SGI and SGP) are small, which
suggests that pre-initializing the embeddings with the pre-
vious result may improve their continuity but seems to have
little impact on the predictive power. Log-likelihoods for
the skip-gram ﬁlter (DSG-F) grow over the ﬁrst few time
steps as the ﬁlter sees more data, and then saturate. The
improvement of our approach over the static model is par-
ticularly pronounced in the SoU and Twitter corpora, which
are much smaller than the massive Google books corpus.
There, sharing information between across time is crucial
because there is little data at every time slice. Skip-gram
smoothing outperforms skip-gram ﬁltering as it shares in-

formation in both time directions and uses a more ﬂexible
variational distribution.

6. Conclusions

We presented the dynamic skip-gram model: a Bayesian
probabilistic model that combines word2vec with a latent
continuous time series. We showed experimentally that
both dynamic skip-gram ﬁltering (which conditions only
on past observations) and dynamic skip-gram smoothing
(which uses all data) lead to smoothly changing embedding
vectors that are better at predicting word-context statistics
at held-out time steps. The beneﬁts are most drastic when
the data at individual time steps is small, such that ﬁtting a
static word embedding model is hard. Our approach may
be used as a data mining and anomaly detection tool when
streaming text on social media, as well as a tool for his-
torians and social scientists interested in the evolution of
language.

Acknowledgements

We would like to thank Marius Kloft, Cheng Zhang,
Andreas Lehrmann, Brian McWilliams, Romann Weber,
Michael Clements, and Ari Pakman for valuable feedback.

date0.00.51.0cos. distance"computer" to "accurate"Google Booksdate"community" to "nature""State of the Union" addresses2010201120122013201420152016date"presidential" to "barack"TwitterDSG-SDSG-FSGISGP18001850190019502000date0.00.51.0cos. distance"computer" to "machine"18001850190019502000date"community" to "transportation"2010201120122013201420152016date"presidential" to "clinton-trump"18001850190019502000date of test books−0.56−0.55−0.54−0.53−0.52normalized log(predictive likelihood)Google booksDSG-SDSG-FSGISGP18001850190019502000date of test SoU address−0.85−0.80−0.75−0.70−0.65"State of the Union" addressesDSG-SDSG-FSGISGP2010201120122013201420152016date of test tweets−0.85−0.80−0.75−0.70−0.65TwitterDSG-SDSG-FSGISGP18001820−0.56−0.55−0.5419802000−0.53−0.522015−0.845−0.840−0.83518821884−0.770−0.768Dynamic Word Embeddings

References

Barkan, Oren. Bayesian Neural Word Embedding. In Pro-
ceedings of the Thirty-First AAAI Conference on Artiﬁ-
cial Intelligence, 2017.

Beck, Amir and Teboulle, Marc. Mirror Descent and Non-
linear Projected Subgradient Methods for Convex Opti-
mization. Operations Research Letters, 31(3):167–175,
2003.

Ben-Tal, Aharon, Margalit, Tamar, and Nemirovski,
Arkadi. The Ordered Subsets Mirror Descent Optimiza-
tion Method with Applications to Tomography. SIAM
Journal on Optimization, 12(1):79–108, 2001.

Bengio, Yoshua, Ducharme, R´ejean, Vincent, Pascal, and
Jauvin, Christian. A Neural Probabilistic Language
Model. Journal of Machine Learning Research, 3:1137–
1155, 2003.

Blei, David M and Lafferty, John D. Dynamic Topic Mod-
els. In Proceedings of the 23rd International Conference
on Machine Learning, pp. 113–120. ACM, 2006.

Blei, David M., Kucukelbir, Alp, and McAuliffe, Jon D.
Variational Inference: A Review for Statisticians. arXiv
preprint arXiv:1601.00670, 2016.

Charlin, Laurent, Ranganath, Rajesh, McInerney, James,
and Blei, David M. Dynamic Poisson Factorization.
In Proceedings of the 9th ACM Conference on Recom-
mender Systems, pp. 155–162, 2015.

Semi-Supervised
Fu, Yanwei and Sigal, Leonid.
In Proceedings of
Vocabulary-Informed Learning.
the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5337–5346, 2016.

Gultekin, San and Paisley, John. A Collaborative Kalman
Filter for Time-Evolving Dyadic Processes. In Proceed-
ings of the 2nd International Conference on Data Min-
ing, pp. 140–149, 2014.

Hamilton, William L, Leskovec, Jure, and Jurafsky, Dan.
Diachronic word embeddings reveal statistical laws of
In Proceedings of the 54th Annual
semantic change.
Meeting of the Association for Computational Linguis-
tics, pp. 1489–1501, 2016.

Hoffman, Matthew D, Blei, David M, Wang, Chong, and
Paisley, John William. Stochastic Variational Inference.
Journal of Machine Learning Research, 14(1):1303–
1347, 2013.

Jerfel, Ghassen, Basbug, Mehmet E, and Engelhardt, Bar-
bara E. Dynamic Compound Poisson Factorization. In
Artiﬁcial Intelligence and Statistics, 2017.

Jordan, Michael

I, Ghahramani, Zoubin,

Jaakkola,
Tommi S, and Saul, Lawrence K. An Introduction to
Variational Methods for Graphical Models. Machine
learning, 37(2):183–233, 1999.

Kalman, Rudolph Emil et al. A New Approach to Linear
Filtering and Prediction Problems. Journal of Basic En-
gineering, 82(1):35–45, 1960.

Kılıc¸, Emrah and Stanica, Pantelimon. The Inverse of
Banded Matrices. Journal of Computational and Applied
Mathematics, 237(1):126–135, 2013.

Kim, Yoon, Chiu, Yi-I, Hanaki, Kentaro, Hegde, Dar-
shan, and Petrov, Slav. Temporal Analysis of Language
In Proceedings of
Through Neural Language Models.
the ACL 2014 Workshop on Language Technologies and
Computational Social Science, pp. 61–65, 2014.

Kingma, Diederik P and Welling, Max. Auto-Encoding
In Proceedings of the 2nd Interna-
Variational Bayes.
tional Conference on Learning Representations (ICLR),
2014.

Kulkarni, Vivek, Al-Rfou, Rami, Perozzi, Bryan, and
Skiena, Steven. Statistically Signiﬁcant Detection of
In Proceedings of the 24th Inter-
Linguistic Change.
national Conference on World Wide Web, pp. 625–635,
2015.

Levy, Omer and Goldberg, Yoav. Neural Word Embedding
as Implicit Matrix Factorization. In Advances in Neural
Information Processing Systems, pp. 2177–2185, 2014.

Michel,

Jean-Baptiste,

Shen, Yuan Kui, Aiden,
Aviva Presser, Veres, Adrian, Gray, Matthew K,
Pickett, Joseph P, Hoiberg, Dale, Clancy, Dan, Norvig,
Peter, Orwant, Jon, et al. Quantitative Analysis of
Culture Using Millions of Digitized Books. Science,
331(6014):176–182, 2011.

Mihalcea, Rada and Nastase, Vivi. Word Epoch Dis-
ambiguation: Finding how Words Change Over Time.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Short Papers-
Volume 2, pp. 259–263, 2012.

Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jef-
frey. Efﬁcient Estimation of Word Representations in
Vector Space. arXiv preprint arXiv:1301.3781, 2013a.

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed Representations of
Words and Phrases and their Compositionality. In Ad-
vances in Neural Information Processing Systems 26, pp.
3111–3119. 2013b.

Dynamic Word Embeddings

Socher, Richard, Bauer, John, Manning, Christopher D,
and Ng, Andrew Y. Parsing with Compositional Vector
Grammars. In ACL (1), pp. 455–465, 2013a.

Socher, Richard, Perelygin, Alex, Wu, Jean Y, Chuang, Ja-
son, Manning, Christopher D, Ng, Andrew Y, and Potts,
Christopher. Recursive Deep Models for Semantic Com-
positionality over a Sentiment Treebank. In Proceedings
of the 2013 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), volume 1631, pp.
1642, 2013b.

Uhlenbeck, George E and Ornstein, Leonard S. On the
Theory of the Brownian Motion. Physical Review, 36
(5):823, 1930.

Vilnis, Luke and McCallum, Andrew. Word Representa-
In Proceedings of the
tions via Gaussian Embedding.
2nd International Conference on Learning Representa-
tions (ICLR), 2014.

Wang, Chong, Blei, David, and Heckerman, David. Con-
tinuous time dynamic topic models. In Proceedings of
the Twenty-Fourth Conference on Uncertainty in Artiﬁ-
cial Intelligence, pp. 579–586, 2008.

Welch, Greg and Bishop, Gary. An Introduction to the

Kalman Filter. 1995.

Mikolov, Tomas, Yih, Wen-tau, and Zweig, Geoffrey. Lin-
guistic Regularities in Continuous Space Word Repre-
In Proceedings of the 2013 Conference of
sentations.
the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies
(NAACL-HLT-2013), pp. 746–751, 2013c.

Mnih, Andriy and Kavukcuoglu, Koray. Learning Word
Embeddings Efﬁciently with Noise-Contrastive Estima-
tion. In Advances in Neural Information Processing Sys-
tems, pp. 2265–2273, 2013.

Pennington,

Jeffrey, Socher, Richard, and Manning,
Christopher D. Glove: Global Vectors for Word Rep-
resentation. In EMNLP, volume 14, pp. 1532–43, 2014.

Ranganath, Rajesh, Gerrish, Sean, and Blei, David M.
Black Box Variational Inference. In AISTATS, pp. 814–
822, 2014.

Ranganath, Rajesh, Perotte, Adler J, Elhadad, No´emie, and
Blei, David M. The Survival Filter: Joint Survival Anal-
In UAI, pp. 742–751,
ysis with a Latent Time Series.
2015.

Rauber, Paulo E., Falc˜ao, Alexandre X., and Telea, Alexan-
dru C. Visualizing Time-Dependent Data Using Dy-
namic t-SNE. In EuroVis 2016 - Short Papers, 2016.

Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,
Daan. Stochastic Backpropagation and Approximate In-
ference in Deep Generative Models. In The 31st Interna-
tional Conference on Machine Learning (ICML), 2014.

Rudolph, Maja and Blei, David. Dynamic Bernoulli
Embeddings for Language Evolution. arXiv preprint
arXiv:1703.08052, 2017.

Rudolph, Maja, Ruiz, Francisco, Mandt, Stephan, and Blei,
David. Exponential Family Embeddings. In Advances
in Neural Information Processing Systems, pp. 478–486,
2016.

Sagi, Eyal, Kaufmann, Stefan, and Clark, Brady. Trac-
ing Semantic Change with Latent Semantic Analysis.
Current Methods in Historical Semantics, pp. 161–183,
2011.

Sahoo, Nachiketa, Singh, Param Vir, and Mukhopadhyay,
Tridas. A Hidden Markov Model for Collaborative Fil-
tering. MIS Quarterly, 36(4):1329–1356, 2012.

Salimans, Tim and Kingma, Diederik P. Weight Normaliza-
tion: A Simple Reparameterization to Accelerate Train-
In Advances in Neural
ing of Deep Neural Networks.
Information Processing Systems, pp. 901–901, 2016.

