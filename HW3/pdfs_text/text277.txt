Supplementary Material: Deep Generative Models for Relational Data with
Side Information

1. Proof of Lemma 1

We can compute E[I{Aij = 0}] as

E[I{Aij = 0}] = p(Xij = 0)

= Ezi,zj ,Λ

p(Xij = 0|zik1, zjk2 , Λk1k2)

= Ezi,zj ,Λ

exp(−Λk1k2zik1 zjk2)









k

k,(cid:96) and Γ(m)
k and b((cid:96))

rive the Gibbs sampler updates for the hyper-parameters
γk1, ξ, Γ(w)

, in closed form.

Sample w((cid:96))
k : We consider the update of layer-1
weights w(1)
as an example, and assume the side informa-
k
tion is available (which is the more general case). Weights
for the other layers can be sampled in a similar manner.

Given the P´olya-Gamma auxiliary variables α(1)
terior for w(1)

k will be w(1)

k ∼ N (µ(w)

, V(w)
k

k , the pos-
), where

k

(cid:18)

≥ exp

Ezi,zj ,Λ

log

exp(−Λk1k2zik1zjk2)



K
(cid:89)

k1,k2=1

= Ezi,zj ,Λ

−

Λk1k2 zik1zjk2







(cid:19)

µ(w)
k

V(w)
k

= V(w)

(Z(2))T (z(2)

k

1N − diag(α(1)

k )(Smk + b(1)

k 1N ))

= ((Z(2))T diag(α(1)

k −

1
2
k )Z(2) + (Γ(w)

k,(cid:96) )−1)−1

(1)

In the above, 1N is a vector of length N with all entries
being 1, and α(1)
ik is drawn from the
P´olya-Gamma distribution

+ , each entry α(1)

k ∈ RN

K
(cid:89)

k1,k2

K
(cid:89)

k1,k2=1












K
(cid:88)

k1,k2=1

where the inequality step follows from Jensen’s inequal-
(Zhou, 2015), we have
ity.
+ γ2
E
. Then the last
a
γ2
b ck1 k2
line in Equation (1) can be written as

Following Lemma 1 in
= ζγc
k1,k2=1 Λk1k2

(cid:104)(cid:80)K

γbck1k2

(cid:105)



K
(cid:88)

Ezi,zj ,Λ

−

Λk1k2 zik1zjk2





(cid:18)

= exp

−

k1,k2=1

(cid:20) ζγc
γbck1k2

+

γ2
a
γ2
b ck1k2

(cid:21)

E

(cid:104)

z(1)
ik1

z(1)
jk2

i z(1)
z(1)

j

Based on Equation (1) and (3), the expected number of ze-
ros in A is lower bounded by

N
(cid:88)

E[

i,j=1

I{Aij = 0}] ≥ N 2Ezi,zj ,Λ

Λk1k2 zik1 zjk2




−

K
(cid:88)

k1,k2=1

(cid:32)

(cid:34)

= N 2 exp

−

ζγc
γbck1 k2

+

γ2
a
γ2
b ck1k2

(cid:35)

E

z

(1)
i

z

(1)
j

(cid:33)

(cid:105)

(cid:104)
z(1)
ik1

z(1)
jk2

(3)

2. HYPERPARAMETER INFERENCE
We sample w((cid:96))
k and mk leveraging the P´olya-Gamma
augmentation (Polson et al., 2013). This enables us to de-

k , b((cid:96))

. Correspondence to: Changwei Hu <changweih@yahoo-
inc.com>, Piyush Rai <piyush@cse.iitk.ac.in>, Lawrence Carin
<lcarin@duke.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

α(1)
ik ∼ PG(1, mT

k si + (w(1)

k )(cid:62)z(2)

i + b(1)
k )

Conditioned on these PG variables, the posterior over b((cid:96))
k
will also be a Gaussian.
Sample mk: Akin to the way we sample w((cid:96))
k , the side in-
formation based regression weights mk can also be sam-
pled using the P´olya-Gamma scheme (using the layer 1
PG variables α(1)
k ). The posterior will be a Gaussian
, V(m)
mk ∼ N (µ(m)
k

), where

k

(2)

(cid:105)(cid:19)

µ(m)
k

= V(m)

k ST (z(2)

1N − diag(α(1)

k )(Z(2)w(1)

k + b(1)

k 1N ))



V(m)
k

= ((ST diag(α(1)

)−1)−1

k −

1
2
k )S + (Γ(m)

k

Sample γk1 : γk1 can be sampled as

γk1 ∼ Gamma(γa+(cid:96)k1k2 ,

γb − (cid:80)

k2

ξδk1k2 γ

1
1−δk1k2
k2

ln(

ck1 k2

Qk1 k2

+ck1k2

)

)

where (cid:96)k1· = (cid:80)
nese Restaurant Table (CRT) distribution (Zhou, 2015)

(cid:96)k1k2 with (cid:96)k1k2 drawn from the Chi-

k2

(cid:96)k1k2 ∼ CRT(X··k1k2, gk1k2)

Sample ξ: The hyperparameter ξ can be sampled as

ξ ∼ Gamma(ξa +

(cid:96)kk,

(cid:88)

k

ξb − (cid:80)

1
k γk ln(

)

ckk
Qkk+ckk

)

Supplementary Material: Deep Generative Models for Relational Data with Side Information

Sample Γ(w)
matrix Γ(w)

k,(cid:96) , Γ(m)
k
k,(cid:96) is sampled as

: Each diagonal entry of the precision

Γ(w)

k,(cid:96) ∼Gamma(a+

K(cid:96)+1
2

,

1
diag((b + 0.5(w((cid:96))
k )T w((cid:96))

k )1K(cid:96)+1)

)

where a and b are the scale and rate parameters for the prior
of Γ(w)

can be sampled similarly.

k,(cid:96) respectively. Γ(m)

k

References

Polson, Nicholas G, Scott, James, and Windle, Jesse.
Bayesian inference for logistic models using p´olya–
gamma latent variables. Journal of the American Sta-
tistical Association, 108(504):1339–1349, 2013.

Zhou, Mingyuan. Inﬁnite edge partition models for over-
In
lapping community detection and link prediction.
AISTATS, 2015.

