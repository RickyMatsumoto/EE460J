Bayesian Optimisation with Continuous Approximations

Appendix

A. Some Ancillary Material

A.1. Review of GP-UCB

We present a review of the GP-UCB algorithm of Srinivas et al. (2010) which we build on in this work. Here we will assume
f ∼ GP(0, κ) where κ : X 2 → R is a radial kernel deﬁned on the domain X . The algorithm is given below.

Algorithm 2 GP-UCB
Input: kernel κ.

• D0 ← ∅, (µ0, σ0) ← (0, κ1/2).
• for t = 1, 2, . . .

1. xt ← argmaxx∈X µt−1(x) + β1/2
2. yt ← Query f at xt.
3. Perform Bayesian posterior updates to obtain µt, σt

t σt−1(x)

(Srinivas et al., 2010)

See (1).

To present the theoretical results for GP-UCB, we begin by deﬁning the Maximum Information Gain (MIG) which
characterises the statistical difﬁculty of GP bandits.
Deﬁnition 2. (Maximum Information Gain (Srinivas et al., 2010)) Let f ∼ GP(0, φX ). Consider any A ⊂ Rd and let
A(cid:48) = {x1, . . . , xn} ⊂ A be a ﬁnite subset. Let fA(cid:48), (cid:15)A(cid:48) ∈ Rn such that (fA(cid:48))i = f (xi) and ((cid:15)A(cid:48))i ∼ N (0, η2). Let
yA(cid:48) = fA(cid:48) + (cid:15)A(cid:48). Denote the Shannon Mutual Information by I. The Maximum Information Gain of A is

Ψn(A) =

max
A(cid:48)⊂A,|A(cid:48)|=n

I(yA(cid:48); fA(cid:48)).

Next, we will need the following regularity conditions on the kernel. It is satisﬁed for four times differentiable kernels such
as the SE kernel and Mat´ern kernel when ν > 2 (Ghosal & Roy, 2006).
Assumption 3. Let f ∼ GP(0, κ), where κ : X 2 → R is a stationary kernel. The partial derivatives of f satisﬁes the
following condition. There exist constants a, b > 0 such that,

for all J > 0, and for all i ∈ {1, . . . , d}, P

(cid:18)

(cid:12)
(cid:12)
(cid:12)

∂f (x)
∂xi

(cid:12)
(cid:12)
(cid:12) > J

sup
x

(cid:19)

≤ ae−(J/b)2

.

The following theorem is a bound on the simple regret Sn (2) for GP-UCB.
Theorem 4. ((Srinivas et al., 2010)) Let f ∼ GP(0, κ), where X = [0, 1]d, f : X → R and the kernel κ satisﬁes
Assumption 3). At each query, we have noisy observations y = f (x) + (cid:15) where (cid:15) ∼ N (0, η2). Denote C1 = 8/ log(1 + η−2).

Pick a failure probability δ ∈ (0, 1) and run GP-UCB with βt = 2 log

(cid:17)

(cid:16) 2π2t2
3δ

+ 2d log

t2bdr

. The following

(cid:18)

(cid:19)

(cid:113) 4ad
δ

holds with probability > 1 − δ,

A.2. Some Technical Results

for all n ≥ 1,

Sn ≤

(cid:114)

C1βnΨn(X )
n

+

π2
6

.

Here we present some technical lemmas we will need for our analysis.
Lemma 5 (Gaussian Concentration). Let Z ∼ N (0, 1). Then P(Z > (cid:15)) ≤ 1
Lemma 6 (Mutual Information in GP, (Srinivas et al., 2010) Lemma 5.3). Let f ∼ GP(0, κ), f : X → R and we observe
y = f (x) + (cid:15) where (cid:15) ∼ N (0, η2). Let A be a ﬁnite subset of X and fA, yA be the function values and observations on this
set respectively. Then the Shannon Mutual Information I(yA; fA) is,

2 exp(−(cid:15)2/2).

I(yA; fA) =

log(1 + η−2σ2

t−1(xt)).

1
2

n
(cid:88)

t=1

Bayesian Optimisation with Continuous Approximations

where σ2

t−1 is the posterior GP variance after observing the ﬁrst t − 1 points.

Our next result is a technical lemma taken from Kandasamy et al. (2016a). It will be used in controlling the posterior
variance of our f and g GPs.
Lemma 7 (Posterior Variance Bound (Kandasamy et al., 2016a)). Let f ∼ (0, κ), f : U → R where κ(u, u(cid:48)) =
κ0φ((cid:107)u − u(cid:48)(cid:107)) and φ is a radial kernel. Upon evaluating f at u we observe y = f (u) + (cid:15) where (cid:15) ∼ N (0, η2). Let u1 ∈ U
and suppose we have s observations at u1 and no observations elsewhere. Then the posterior variance κ(cid:48) (see (1)) at all
u ∈ U satisﬁes,

κ(cid:48)(u, u) ≤ κ0(1 − φ2((cid:107)u − u1(cid:107))) +

η2/s
1 + η2
κ0s

.

Proof: The proof is in Section C.0.1 of Kandasamy et al. (2016a) who prove this result as part of a larger proof.

B. Analysis

We will ﬁrst state a formal version of Theorem 1. Recall from the main text where we stated that most evaluations at z• are
inside the following set Xρ.

Xρ = {x ∈ X : f(cid:63) − f (x) ≤ 2ρ

κ0(cid:107)ξ(cid:107)∞}.

√

This is not entirely accurate as it hides a dilation that arises due to a covering argument in our proofs. Precisely, we will
show that after n queries at any ﬁdelity, BOCA will use most of the z• evaluations in Xρ,n deﬁned below using Xρ.

Here B2(x, (cid:15)) is an L2 ball of radius (cid:15) centred at x. Xρ,n is a dilation of Xρ by
n → ∞, Xρ,n approaches Xρ at a polynomial rate. We now state our main theorem below.

Xρ,n = (cid:8) x ∈ X : B2

(cid:0)x,

d/nα/2d(cid:1) ∩ Xρ,n (cid:54)= ∅(cid:9)

√

√

d/nα/2d. Notice that for all α > 0, as

(10)

Theorem 8. Let Z = [0, 1]p and X = [0, 1]d. Let g ∼ GP(0, κ) where κ is of the form (3). Let φX satisfy Assumption 3
with some constants a, b > 0. Pick δ ∈ (0, 1) and run BOCA with

βt = 2 log

+ 4d log(t) + max

0 , 2d log

brd log

(cid:26)

(cid:18)

(cid:18) 6ad
δ

(cid:19)(cid:19)(cid:27)

.

(cid:19)

(cid:18) π2t2
2δ

Then, for all α ∈ (0, 1) there exists ρ, Λ0 such that with probability at least 1 − δ we have for all Λ ≥ Λ0,

(cid:115)

S(Λ) ≤

2C1β2nΛ Ψ2nΛ (Xρ,n)
nΛ

+

(cid:115)

2C1β2nΛ Ψ2nα

(X )

Λ

n2−α
Λ

+

π2
6nΛ

.

Here C1 = 8/ log(1 + η2) is a constant and nΛ = (cid:98)Λ/λ(z•)(cid:99). ρ satisﬁes ρ > ρ0 = max{2, 1 + (cid:112)(1 + 2/α)/(1 + d)}.

In addition to the dilation, Theorem 1 in the main text also suppresses the constants and polylog terms. The next three
subsections are devoted to proving the above theorem. In Section B.1 we describe some discretisations for Z and X which
we will use in our proofs. Section B.2 gives some lemmas we will need and Section B.3 gives the proof.

B.1. Set Up & Notation

Notation: Let U ⊂ Z × X . Tn(U ) will denote the number of queries by BOCA at points (z, x) ∈ U within n time steps.
When A ⊂ Z and B ⊂ X , we will overload notation to denote Tn(A, B) = Tn(A × B). For z ∈ Z, [> z] will denote the
ﬁdelities which are more expensive than z, i.e. [> z] = {z(cid:48) ∈ Z : λ(z(cid:48)) > λ(z)}.

We will require a fairly delicate set up before we can prove Theorem 8. Let α > 0. All sets described in the rest of this
subsection are deﬁned with respect to α. First deﬁne

˜Hn = {(z, x) ∈ Z × X : f(cid:63) − f (x) < 2ρβ1/2

n

κ0ξ(z)},

√

(11)

(12)

(13)

(14)

Bayesian Optimisation with Continuous Approximations

where recall from (4), ξ(z) = (cid:112)1 − φ2
of ˜Hn in the X space, i.e.

Z ((cid:107)z − z•(cid:107)) is the information gap function. We next deﬁne H(cid:48)

n to be an L2 dilation

H(cid:48)

n = {(z, x) ∈ Z × X : B2

(cid:0)x,

d/nα/2d(cid:1) ∪ ˜Hn (cid:54)= ∅}.

√

Finally, we deﬁne Hn to be the intersection of H(cid:48)
(cid:110)

Hn = H(cid:48)

n ∩

(z, x) ∈ Z × X : ξ(z) > (cid:107)ξ(cid:107)∞/β1/2

n

(cid:111)
.

n with all ﬁdelities satisfying the third condition in (7). That is,

In our proof we will use the second condition in (7) to control the number of queries in Hn.

To control the number of queries outside Hn we ﬁrst introduce a
a sufﬁcient covering would be an equally spaced grid having n α
Ai,n ⊂ X to be the points in X which are closest to ai,n in X . Therefore Fn = {Ai,n}n
Now deﬁne Qt : 2X → 2Z to be the following function which maps subsets of X to subsets of Z.

2d points per side. Let {ai,n}n

2n

α
2

α
2

i=1 be the points in the covering.

i=1 is a partition of X .

√

d
α
2d

-covering of the space X of size nα/2. If X = [0, 1]d,

(cid:110)

Qt(A) =

z ∈ Z : ∀ x ∈ A,

f(cid:63) − f (x) ≥ 2ρβ1/2

t

√

κ0ξ(z)

(cid:111)
.

That is, Qt maps A ⊂ X to ﬁdelities where the information gap ξ is smaller than (f(cid:63) − f (x))/(2ρβ1/2
we deﬁne θt : 2X → Z, to be the cheapest ﬁdelity in Qt(A) for a subset A ∈ X .

t

) for all x ∈ A. Next

We will see that BOCA will not query inside an Ai,n ∈ Fn at ﬁdelities larger than θt(Ai,n) too many times (see Lemma 12).
That is, Tn([> θn(Ai,n)], Ai,n) will be small. We now deﬁne Fn as follows,

θt(A) = arginf
z∈Qt(A)

λ(z).

Fn =

(cid:91)

[> θn(Ai,n)] × Ai,n.

Ai,n⊂X \Xρ,n

That is, we ﬁrst choose Ai,n’s that are completely outside Xρ,n and take their cross product with ﬁdelities more expensive
than θt(Ai,n). By design of the above sets, and using the third condition in (7) we can bound the total number of queries as
follows,

n = Tn(Z, X ) ≤ Tn({z•}, Xρ,n) + Tn(Fn) + Tn(Hn)

We will show that the last two terms on the right hand side are small for BOCA and consequently, the ﬁrst term will be large.
But ﬁrst, we establish a series of technical results which will be useful in proving theorem 8.

B.2. Some Technical Lemmas

The ﬁrst lemma proves that the UCB ϕt in (6) upper bounds f (xt) on all the domain points {xt}t≥1 chosen for evaluation.

Lemma 9. Let βt > 2 log(π2t2/2δ). Then, with probability > 1 − δ/3, we have

∀ t ≥ 1,

|f (xt) − µt−1(xt)| ≤ β1/2

t σt−1(xt).

Proof: This is a straightforward argument using Lemma 5 and the union bound. At t ≥ 1,

(cid:16)

P

|f (x) − µt−1(x)| > β1/2

t σt−1(x)

(cid:17)

(cid:104)

(cid:104)
E

= E

|f (x) − µt−1(x)| > β1/2

t σt−1(x)

(cid:104)

(cid:16)

= E

PZ∼N (0,1)

|Z| > β1/2

t

(cid:17)(cid:105)

≤ exp

(cid:105)(cid:105)

(cid:12)
(cid:12)
(cid:12) Dt−1
(cid:17)

=

(cid:16) −βt
2

2δ
π2t2 .

In the ﬁrst step we have conditioned w.r.t Dt−1 = {(zi, xi, yi)}t−1
N (µt−1(x), σ2

t−1(x)). The statement follows via a union bound over all t ≥ 0 and the fact that (cid:80)

i=1 which allows us to use Lemma 5 as f (x)|Dt−1 ∼

t t−2 = π2/6.

Next we show that the GP sample paths are well behaved and that ϕt(x) upper bounds f (x) on a sufﬁciently dense subset at
each time step. For this we use the following lemma.

Bayesian Optimisation with Continuous Approximations

Lemma 10. Let βt be as given in Theorem 8. Then for all t, there exists a discretisation Gt of X of size (t2brd(cid:112)6ad/δ)d
such that the following hold.

• Let [x] be the closest point to x ∈ X in the discretisation. With probability > 1 − δ/6, we have

∀ t ≥ 1,

∀ x ∈ X ,

|f (x) − f ([x]t)| ≤ 1/t2.

• With probability > 1 − δ/3, for all t ≥ 1 and for all a ∈ Gt, |f (a) − µt−1(a)| ≤ β1/2

t σt−1(a).

Proof: The ﬁrst part of the proof, which we skip here, uses the regularity condition for φX in Assumption 3 and mimics the
argument in Lemmas 5.6, 5.7 of Srinivas et al. (2010). The second part mimics the proof of Lemma 9 and uses the fact that
βt > 2 log(|Gt|π2t2/2δ).

The discretisation in the above lemma is different to the coverings introduced in Section B.1. The next lemma is about the
information gap function in (4).
Lemma 11. Let g ∼ GP(0, κ), g : Z × X → R and κ is of the form (3). Suppose we have s observations from g. Let
z ∈ Z and x ∈ X . Then τt−1(z, x) < α implies σt−1(x) < α +

κ0ξ(z).

√

Proof: The proof uses the observation that for radial kernels, the maximum difference between the variances at two points u1
and u2 occurs when all s observations are at u2 or vice versa. Now we use u1 = (z, x) and u2 = (z•, x) and apply Lemma 7
to obtain τ 2
when all observations are at

. However, As τ 2

t−1(z, x) = η2/s
1+ η2
sκ0

t−1(z•, x) ≤ κ0(1 − φZ ((cid:107)z• − z(cid:107)))2 + η2/s
1+ η2
sκ0
t−1(z•, x), we have σ2

(z, x) and noting that σ2
t−1(x) = τ 2
situation characterised the maximum difference between σ2
observation set. The proof is completed using the elementary inequality a2 + b2 ≤ (a + b)2 for a, b > 0.

t−1(z, x). Since the above
t−1(z, x), this inequality is valid for any general

t−1(x) ≤ κ0(1 − φZ ((cid:107)z• − z(cid:107)))2 + τ 2

t−1(x) and τ 2

We are now ready to prove Theorem 8. The plan of attack is as follows. We will analyse BOCA after n time steps and
bound the number of plays at ﬁdelities z (cid:54)= z• and outside Xρ,n at z•. Then we will show that for sufﬁciently large Λ, the
number of random plays N is bounded by 2nΛ with high probability. Finally we use techniques from Srinivas et al. (2010),
speciﬁcally the maximum information gain, to control the simple regret. However, unlike them we will obtain a tighter
bound as we can control the regret due to the sets Xρ,n and X \ Xρ,n separately.

B.3. Proof of Theorem 8

Let α > 0 be given. We invoke the sets Xρ,n, Hn, Fn in equations (10), (11), (14) for the given α. The following lemma
establishes that for any A ⊂ X , we will not query inside A at ﬁdelities larger than θt(A) (13) too many times. The proof is
given in Section B.3.1.

Lemma 12. Let A ⊂ X which does not contain the optimum. Let ρ, βt be as given in Theorem 8. Then for all u >
max{3, (2(ρ − ρ0)η)−2/3}, we have

(cid:16)

P

Tn([> θt(A)], A) > u

≤

(cid:17)

δ
π2

1
u1+4/α

To bound T (Fn), we will apply Lemma 12 with u = nα/2 on all Ai,n ∈ Fn satisfying Ai,n ⊂ X \ Xρ,n. Since Xρ ⊂ Xρ,n,
Ai,n does not contain the optimum. As Fn is the union of such sets (14), we have for all n (larger than a constant),

(cid:16)
P(T (Fn) > nα) ≤ P

∃Ai,n ⊂ X \ Xρ,n, Tn([> θt(Ai,n)], Ai,n) > nα/2(cid:17)
Tn([> θt(Ai,n)], Ai,n) > nα/2(cid:17)
δ
(cid:88)
π2

≤ |Fn|

P

(cid:16)

1
nα/2+2

≤

δ
π2

1
n2

≤

Ai,n∈Fn
Ai,n⊂X \Xρ,n

Now applying the union bound over all n, we get P(∀ n ≥ 1, T (Fn) > nα) ≤ δ/6.

Now we will bound the number of plays in Hn using the second condition in (7). We begin with the following Lemma. The
proof mimics the argument in Lemma 11 of Kandasamy et al. (2016a) who prove a similar result for GPs deﬁned on just the
domain, i.e. f ∼ GP(0, κ) where f : X → R.

Bayesian Optimisation with Continuous Approximations

Lemma 13. Let A ⊂ Z × X and the L2 diameter of A in X be DX and that in Z be DZ . Suppose we have n evaluations
of g of which s are in A. Then for any (z, x) ∈ A, the posterior variance τ (cid:48)2 satisﬁes,

τ (cid:48)2(z, x) ≤ κ0(1 − φ2

Z (DZ )φ2

X (DX )) +

η2
s

.

Let λr = λmin/λ(z•) where λmin = minz∈Z λ(z). If the maximum posterior variance in a certain region is smaller than
γ(z), then we will not query within that region by the second condition in (7). Further by the third condition, since we
will only query at ﬁdelities satisfying ξ(z) > (cid:107)ξ(cid:107)∞/β1/2
n , it is sufﬁcient to show that the posterior variance is bounded by
∞λ2q
κ0(cid:107)ξ(cid:107)2
r /βn at time n to prove that we will not query again in that region. For this we can construct a covering of Hn such
that 1 − φ2
Z (DZ )φ2
r /βn. For any A ⊂ Z × X , the covering number, which we denote Ωn(A) of this
construction will typically be poly-logarithmic in n (See Remark 15 below). Now if there are
+ 1 queries inside a
ball in this covering, the posterior variance, by Lemma 13 will be smaller than κ0(cid:107)ξ(cid:107)2
r /βn. Therefore, we will not query
≤ C3vol(Hn) polylog(n)
any further inside this ball. Hence, the total number of queries in Hn is Tn(Hn) ≤ C2Ωn(Hn) βn
λ2q
poly(λr)
r
for appropriate constants C2, C3. (Also see Remark 16).

X (DX ) < 1

2βnη2
r (cid:107)ξ(cid:107)2

2 (cid:107)ξ(cid:107)2

∞λ2q

∞λ2q

∞κ0

λ2q

Next, we will argue that the number of queries for sufﬁciently large Λ, is bounded by nΛ/2 where, recall nΛ = (cid:98)Λ/λ(z•)(cid:99).
This simply follows from the bounds we have for Tn(Fn) and Tn(Hn).

Tn(Z \ {z•}, X ) ≤ Tn(Fn) + Tn(Hn) ≤ nα + O(polylog(n)).

Since the right hand side is sub-linear in n, we can ﬁnd n0 such that for all n0, n/2 is larger than the right hand side.
Therefore for all n ≥ n0, Tn({z•}, X ) > n/2. Since our bounds hold with probability > 1 − δ for all n we can invert the
above inequality to bound N , the random number of queries after capital Λ. We have N ≤ 2Λ/λ(z•). We only need to
make sure that N ≥ n0 which can be guaranteed if Λ > Λ0 = n0λ(z•).

The ﬁnal step of the proof is to bound the simple regret after n time steps in BOCA. This uses techniques that are now
standard in GP bandit optimisation, so we only provide an outline. We will need the following Lemma, whose proof is given
in Section B.3.2.
Lemma 14. Assume that we have queried g at n points, (zt, xt)n
σt−1 denote the posterior variance of f at time t, i.e. after t − 1 queries. Then, (cid:80)
Here Ψs(A) is the MIG of φX after s queries to A as given in Deﬁnition 2.

t=1 of which s points are in {z•} × A for any A ⊂ X . Let
σ2
log(1+η−2) Ψs(A).
t−1(xt) ≤

xt∈A,zt=z•

2

We now deﬁne the quantity Rn below. Readers familiar with the GP bandit literature might see that it is similar to the notion
of cumulative regret, but we only consider queries at z•.

Rn =

f(cid:63) − f (xt) =

f(cid:63) − f (xt) +

f(cid:63) − f (xt).

(15)

(cid:88)

zt=z•
xt∈Xρ,n

(cid:88)

zt=z•
xt /∈Xρ,n

For any A ⊂ X we can use Lemmas 9, 10, and 14 and the Cauchy Schwartz inequality to obtain,

f(cid:63) − f (xt) ≤

C1Tn(z•, A)βnΨTn(z•,A)(A) +

(cid:113)

(16)

1
t2 .

(cid:88)

zt=z•
xt∈A

n
(cid:88)

t=1
zt=z•

(cid:88)

zt=z•
xt∈A

For the ﬁrst term in (15), we set A = Xρ,n in (16) and use the trivial bound Tn(z•, Xρ,n) ≤ n. For the second term we note
that {z•} × (X \ Xρ,n) ⊂ Fn and hence, Tn(z•, X \ Xρ,n) ≤ Tn(Fn) ≤ nα. As A ⊂ B =⇒ Ψn(A) ≤ Ψn(B), we have
Rn ≤ (cid:112)C1nβnΨn(Xρ,n) + (cid:112)C1nαβnΨnα (X ) + π2/6. Now, using the fact that N ≤ 2nΛ for large enough N we have,

(cid:113)

(cid:113)

RN ≤

2C1nΛβ2nΛ Ψ2nΛ (Xρ,n) +

2αC1nα

Λβ2nΛ Ψ2nα

Λ

(X ) +

π2
6

.

The theorem now follows from the fact that S(Λ) ≤ 1
Lemmas 9, 10 and the bound on Tn(Fn), the summation of whose probabilities are bounded by δ.

N RN by deﬁnition and that N ≥ nΛ. The failure instances arise out of

Bayesian Optimisation with Continuous Approximations

Remark 15 (Construction of covering for the SE kernel). We demonstrate that such a construction is always possible using
the SE kernel. Using the inequality e−x ≥ 1 − x for x > 0 we have,

1 − φ2

X (DX )φ2

Z (DZ ) <

D2
X
h2
X

+

D2
Z
h2
Z

DX = DZ =

h
2

(cid:107)ξ(cid:107)∞
β1/2
n

λq
r,

√

where DZ , DX will be the L2 diameters of the balls in the covering. Now let h = min{hZ , hX } and choose

Z (z)φ2

via which we have 1 − φ2
2 ξ(
results on covering numbers, we can show that the size of this covering will be log(n)
possible for Mat´ern kernels, but the exponent on log(n) will be worse.
Remark 16 (Choice of q for SE kernel). From the arguments in our proof and Remark 15, we have that the number

r /βn as stated in the proof. Noting that βn (cid:16) log(n), using standard
. A similar argument is

2 /λq(d+p)
r

X (x) < 1

p)2λ2q

d+p

d+p+2
2

(cid:16) λ(z•)
λmin

(cid:17)q(p+d+2)

of plays in a set S ⊂ (Z × X ) is T (S) ≤ vol(S) log(n)
. However, we chose to work work
λmin mostly to simplify the proof. It is not hard to see that for A ⊂ X and B ⊂ Z if λ(z) ≈ λ(cid:48) for all z ∈ B, then
. As the capital spent in this region is λ(cid:48)Tn(A, B), by picking
Tn(B, A) ≈ vol(B × A) log(n)
q = 1/(p + d + 2) we ensure that the capital expended for a certain A ⊂ X at all ﬁdelities is roughly the same, i.e. for any
A, the capital density in ﬁdelities z such that λ(z) < λ(θt(A)) will be roughly the same. Kandasamy et al. (2016c) showed
that doing so achieved a nearly minimax optimal strategy for cumulative regret in K-armed bandits. While it is not clear
that this is the best strategy for optimisation under GP assumptions, it did reasonably well in our experiments. We leave it to
future work to resolve this.

(cid:16) λ(z•)
λ(cid:48)

(cid:17)q(p+d+2)

d+p+2
2

B.3.1. PROOF OF LEMMA 12

For brevity, we will denote θ = θt(A). We will invoke the discretisation Gt used in Lemma 10 via which we have
ϕt([x(cid:63)]t) ≥ f(cid:63) − 1/t2 for all t ≥ 1. Let b = argmaxx∈A ϕt(x) be the maximiser of the upper conﬁdence bound ϕt in A at
time t. Now note that, xt ∈ A =⇒ ϕt(b) > ϕt([x(cid:63)]t) =⇒ ϕt(b) > f(cid:63) − 1/t2. We therefore have,
P(cid:0)Tn([> θ], A) > u(cid:1) ≤ P(cid:0)∃t : u + 1 ≤ t ≤ n, ϕt(b) > f(cid:63) − 1/t2 ∧ τt−1(θ, b) < γ(θ))

n
(cid:88)

≤

t=u+1

P(cid:0)µt−1(b) − f (b) > f(cid:63) − f (b) − β1/2

t σt−1(b) − 1/t2 ∧ τt−1(θ, b) < γ(θ)(cid:1)

(17)

We now note that

τt−1(θ, b) < γ(θ) =⇒ σt−1(b) < γ(θ) +

κ0ξ(θ) ≤ 2

√

√

κ0ξ(θ) ≤

1
β1/2
t ρ
κ0ξ(θ)(λ(z)/λ(z•))1/(p+d+2) ≤

(f(cid:63) − f (b)).

√

√

√

κ0ξ(θ) and
κ0ξ(θ). Now plugging this back

The ﬁrst step uses Lemma 11. The second step uses the fact that γ(θ) =
the last step uses the deﬁnition of Qt(A) in (12) whereby we have f(cid:63) − f (x) ≥ 2ρβ1/2
into (17), we can bound each term in the summation by,

t

P(cid:0)µt−1(b) − f (b) > (ρ − 1)β1/2

t σt−1(b) − 1/t2(cid:1) ≤ PZ∼N (0,1)
(cid:19)(ρ0−1)2
(cid:19)

≤

exp

1
2

(cid:18) (ρ0 − 1)2
2

βt

≤

(cid:18) 2δ
π2

1
2

t−(ρ0−1)2(2+2d) ≤

δ
π2 t−(ρ0−1)2(2+2d).

(cid:16)

Z > (ρ0 − 1)β1/2

t

(cid:17)

In the ﬁrst step we have used the following facts, t > u ≥ max{3, (2(ρ − ρ0)η)−2/3}, π2/2δ > 1 and σt−1(b) > η/
conclude,

(ρ − ρ0)

η(cid:112)4 log(t)
√
t

>

1
t2 =⇒ (ρ − ρ0) ·

(cid:115)

2 log

(cid:18) π2t2
2δ

(cid:19)

η
√

·

>

t

1
t2 =⇒ (ρ − ρ0)β1/2

t σt−1(b) >

1
t2 .

The second step of (18) uses Lemma 5, the third step uses the conditions on β1/2
as given in theorem 8 and the last step
uses the fact that π2/2δ > 1. Now plug (18) back into (17). The result follows by bounding the sum by an integral and
noting that ρ0 > 2 and ρ0 ≥ 1 + (cid:112)(1 + 2/α)/(1 + d).

t

(18)

√

t to

Bayesian Optimisation with Continuous Approximations

B.3.2. PROOF OF LEMMA 14

Let As = {u1, u2, . . . , us} be the queries in {z•} × A in the order they were queried. Now, assuming that we have queried
g only inside {z•} × A, denote by ˜σt−1(·), the posterior standard deviation after t − 1 such queries. Then,

(cid:88)

t:xt∈A,zt=z•

σ2
t−1(xt) ≤

˜σ2
t−1(ut) ≤

s
(cid:88)

t=1

s
(cid:88)

t=1

η2 ˜σ2

t−1(ut)
η2

≤

s
(cid:88)

t=1

log(1 + η−2 ˜σ2

t−1(ut))

log(1 + η−2)

≤

2
log(1 + η−2)

I(yAs; fAs ).

Queries outside {z•} × A will only decrease the variance of the GP so we can upper bound the ﬁrst sum by the posterior
variances of the GP with only the queries in {z•} × A. The third step uses the inequality u2/v2 ≤ log(1 + u2)/ log(1 + v2).
The result follows from the fact that Ψs(A) maximises the mutual information among all subsets of size s.

C. Addendum to Experiments

C.1. Implementation Details

We describe some of our implementation details below.

Domain and Fidelity space: Given a problem with arbitrary domain X and Z, we mapped them to [0, 1]d and [0, 1]p by
appropriately linear transforming the coordinates.

Initialisation: Following recommendations in Brochu et al. (2010) all GP methods were initialised with uniform random
queries with Λ/10 capital, where Λ is the total capital used in the experiment. For GP-UCB and GP-EI all queries were
initialised at z• whereas for the multi-ﬁdelity methods, the ﬁdelities were picked at random from the available ﬁdelities.

GP Hyper-parameters: Except in the ﬁrst two experiments of Fig. 3, the GP hyper-parameters were learned after
initialisation by maximising the GP marginal likelihood (Rasmussen & Williams, 2006) and then updated every 25 iterations.
We use an SE kernel for both φX and φZ and instead of using one bandwidth for the entire ﬁdelity space and domain, we
learn a bandwidth for each dimension separately. We learn the kernel scale, bandwidths and noise variance using marginal
likelihood. The mean of the GP is set to be the median of the observations.

Choice of βt: βt, as speciﬁed in Theorem 8 has unknown constants and tends to be too conservative in practice (Srinivas
et al., 2010). Following the recommendations in Kandasamy et al. (2015) we set it to be of the correct “order”; precisely,
βt = 0.5d log(2(cid:96)t + 1). Here, (cid:96) is the effective L1 diameter of X and is computed by scaling each dimension by the inverse
of the bandwidth of the SE kernel for that dimension.

Maximising ϕt: We used the DiRect algorithm (Jones et al., 1993).

Fidelity selection: Since we only worked in low dimensional ﬁdelity spaces, the set Zt was constructed in practice by
obtaining a ﬁnely sampled grid of Z and then ﬁltering out those which satisﬁed the 3 conditions in (7). In the second
condition of (7), the threshold γ(z) can be multiplied up to a constant factor, i.e cγ(z) without affecting our theoretical
results. In practice, we started with c = 1 but we updated it every 20 iterations via the following rule: if the algorithm has
queried z• more than 75% of the time in the last 20 iterations, we decrease it to c/2 and if it queried less than 25% of the
time we increase it to 2c. But the c value is always clipped inbetween 0.1 and 20. In practice we observed that the value for
c usually stabilised around 1 and 8 although in some experiments it shot up to 20. Changing c this way resulted in slightly
better performance in practice.

C.2. Description of Synthetic Functions

The following are the synthetic functions used in the paper.

GP Samples: For the GP samples in the ﬁrst two experiments of Figure 3 we used an SE kernel with bandwidth 0.1 for φX .
For φZ we used bandwidths 1 and 0.01 for the ﬁrst and second experiments respectively. The function was constructed by
obtaining the GP function values on a 50 × 50 grid in the two dimensional Z × X space and then interpolating for evaluations
in between via bivariate splines. For both experiments we used η2 = 0.05 and the cost function λ(z) = 0.2 + 6z2.

Bayesian Optimisation with Continuous Approximations

Currin exponential function: The domain is the two dimensional unit cube X = [0, 1]2 and the ﬁdelity was Z = [0, 1]
with z• = 1. We used λ(z) = 0.1 + z2, η2 = 0.5 and,

(cid:18)

g(z, x) =

1 − 0.1(1 − z) exp

(cid:18) −1
2x2

(cid:19)(cid:19) (cid:18) 2300x3

1 + 1900x2
1 + 500x2

1 + 2092x1 + 60
1 + 4x1 + 20

100x3

(cid:19)

.

j=1 Aij(xj − Pij)2(cid:1). Here A, P are given below
i(z)) exp (cid:0) − (cid:80)3
Hartmann functions: We used g(z, x) = (cid:80)4
for the 3 and 6 dimensional cases and α = [1.0, 1.2, 3.0, 3.2]. Then α(cid:48)
i was set as α(cid:48)
i(z) = 0.1(1 − zi) if i ≤ p for
i = 1, 2, 3, 4. We constructed the p = 4 and p = 2 Hartmann functions for the 3 and 6 dimensional cases respectively
this way. When z = z• = 1p, this reduces to the usual Hartmann function commonly used as a benchmark in global
optimisation.
For the 3 dimensional case we used λ(z) = 0.05 + (1 − 0.05)z3

i=1(αi − α(cid:48)

2, η2 = 0.01 and,

1z2

A =

, P = 10−4 ×







10
3
0.1 10
10
3
0.1 10







30
35
30
35







3689
4699
1091
381

1170
4387
8732
5743







2673
7470
5547
8828

.

For the 3 dimensional case we used λ(z) = 0.05 + (1 − 0.05)z3

4, η2 = 0.05 and,

A =







10
0.05
3
17

3
10
3.5
8

17
17
1.7
0.05

3.5 1.7
8
0.1
17
10
0.1
10







8
14
8
14

, P = 10−4 ×

1696
4135
1451
8828

5569
8307
3522
8732

124
3736
2883
5743

8283
1004
3047
1091







5886
9991
6650
381

.

3 z1
2z1.5
1z2


1312
2329
2348
4047





Borehole function: This function was taken from (Xiong et al., 2013). We ﬁrst let,

f2(x) =

log(x2/x1)

1 +

2πx3(x4 − x6)
(cid:16)
2x7x3

log(x2/x1)x2

1x8

5x3(x4 − x6)

(cid:17) ,

+ x3
x5

f1(x) =

log(x2/x1)

1.5 +

(cid:16)

2x7x3

log(x2/x1)x2

1x8

+ x3
x5

(cid:17) .

Then we deﬁne g(z, x) = zf2(x) + (1 − z)f1(x).
=
[0.05, 0.15; 100, 50K; 63.07K, 115.6K; 990, 1110; 63.1, 116; 700, 820; 1120, 1680; 9855, 12045] and Z = [0, 1]
with z• = 1. We used λ(z) = 0.1 + z1.5 for the cost function and η2 = 5 for the noise variance.

The domain of

function is X

the

Branin function: We use the following function where X = [[−5, 10], [0, 15]]2 and Z = [0, 1]3.

g(z, x) = a(x2 − b(z1)x2

1 + c(z2)x1 − r)2 + s(1 − t(z)) cos(x1) + s,

where a = 1, b(z1) = 5.1/(4π2)−0.01(1−z1) c(z2) = 5/π−0.1(1−z2), r = 6, s = 10 and t(z3) = 1/(8π)+0.05(1−z3).
At z = z• = 1p, this becomes the standard Branin function used as a benchmark in global optimisation. We used
λ(z) = 0.05 + z3

for the cost function and η2 = 0.05 for the noise variance.

1z2

2z1.5
3

