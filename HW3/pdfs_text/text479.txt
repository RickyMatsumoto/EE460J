Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence
and Cluster Recovery

Ashkan Panahi 1 Devdatt Dubhashi 2 Fredrik D. Johansson 3 Chiranjib Bhattacharyya 4

Abstract

Standard clustering methods such as K–means,
Gaussian mixture models, and hierarchical clus-
tering, are beset by local minima, which are
sometimes drastically suboptimal. Moreover the
number of clusters K must be known in advance.
The recently introduced sum–of–norms (SON)
or Clusterpath convex relaxation of k-means and
hierarchical clustering shrinks cluster centroids
toward one another and ensure a unique global
minimizer. We give a scalable stochastic incre-
mental algorithm based on proximal iterations to
solve the SON problem with convergence guar-
antees. We also show that the algorithm recov-
ers clusters under quite general conditions which
have a similar form to the unifying proximity
condition introduced in the approximation algo-
rithms community (that covers paradigm cases
such as Gaussian mixtures and planted partition
models). We give experimental results to conﬁrm
that our algorithm scales much better than previ-
ous methods while producing clusters of compa-
rable quality.

1. Introduction

Clustering is perhaps the most fundamental problem in un-
supervised learning. Many clustering algorithms have been
proposed in the literature (Jain et al., 1999), including K-
means, spectral clustering, Gaussian mixture models and
hierarchical clustering, to solve problems with respect to
a wide range of cluster shapes. However, much research
has pointed out that these methods all suffer from instabil-
ities. For example, the formulation of K-means is NP-hard
and the typical way to solve it is the Lloyds method, which

1ECE, North Carolina State University, Raleigh, NC 2CSE,
Chalmers University of Technology, G¨oteborg, Sweden 3IMES,
MIT, Cambridge, MA 4CSA, IISc, Bangalore, India. Correspon-
dence to: Ashkan Panahi <panahi1986@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

requires randomly initializing the clusters. However, one
needs to know the number of clusters in advance and dif-
ferent initializations may lead to signiﬁcantly different ﬁnal
cluster results.

Lindsten et al. (2011) and Hocking et al. (2011) proposed
the following convex optimization procedure for clustering,
called SON (“Sum of norms” clustering) by the former and
Clusterpath by the latter;

min
{ui∈Rm}

1
2

n
(cid:88)

i=1

(cid:107)xi − ui(cid:107)2

2 + λ

(cid:107)ui − uj(cid:107)2

(1)

(cid:88)

i<j

The main idea of the formulation is that if input data points
xi and xj belong to the same cluster, then their correspond-
ing centroids ui and uj should be forced to be the same.
Intuitively, this is due to the fact that the second term is a
regularization term that enforces zeroes in the vector con-
sisting of entries (cid:107)ui − uj(cid:107) and can be seen as a general-
ization of the fused Lasso penalty. From another point of
view, the regularization term can be seen as an (cid:96)1,2 norm,
i.e., the sum of (cid:96)2 norms. Such a group norm is known to
encourage block sparse solutions (Bach et al., 2012). Thus
for many pairs (i, j), we expect to enforce ui = uj.

Lindsten et al. (2011) used an off–the–shelf convex solver,
CVX to generate solution paths. Hocking et al. (2011) in-
troduced three distinct algorithms for the three most com-
monly encountered norms. For the (cid:96)1 norm, the objec-
tive function separates, and they solve the convex cluster-
ing problem by the exact path following method designed
for the fused lasso. For the (cid:96)1 and (cid:96)2 norms, they employ
subgradient descent in conjunction with active sets. Re-
cently, Chi & Lange (2015); Chen et al. (2015) introduce
two similar generic frameworks for minimizing the convex
clustering objective function with an arbitrary norm. One
approach solves the problem by the alternating direction
method of multipliers (ADMM), while the other solves it
by the alternating minimization algorithm (AMA). How-
ever both algorithms have issues with scalablity.

Moreover, none of these papers provide any theoretical
guarantees about the cluster recovery property of the algo-
rithm. The ﬁrst theoretical result on cluster recovery was
shown by Zhu et al. (2010): if the samples are drawn from

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

two cubes, each being one cluster, then SON can provably
recover the clusters provided that the distance between the
two cubes is larger than a threshold which depends (lin-
early) on the size of the cube and the ratio of numbers of
samples in each cluster. Unfortunately, the conditions for
recovery represent an extremely narrow special case: only
two clusters which both have to be cubes. Moreover in
their paper, there is no algorithm or analysis of the speed of
convergence. No other theoretical guarantees for SON are
known previously.

Here we develop a new algorithm in the spirit of recent
advances in stochastic methods for large scale optimization
(Bottou et al., 2016) to solve the optimization problem (1).
We give a convergence analysis and provide quite general
cluster recovery guarantees.

There has been a ﬂurry of advances (Johnson & Zhang,
2013; Defazio et al., 2014; Schmidt et al., 2016) in devel-
oping algorithms for solving optimization problems for the
case when the objective consists of the sum of two con-
vex functions: one is the average of a large number of
smooth component functions, and the other is a general
convex function that admits a proximal mapping (and the
whole objective function is strongly convex). The opti-
mization (1) is of this form but here we exploit the structure
of (1) further by observing that the second function can also
be split into component functions. This results in an in-
cremental algorithm with proximal iterations consisting of
very simple and natural steps. Our algorithm can be seen
as a special case of the methods of Bertsekas (2011). We
compute the proximal operator in closed form to yield very
simple and cheap iterations. Using the fact that the prox-
imal operator is non-expansive, we reﬁne and strengthen
Bertsekas’ convergence results. The stochastic incremental
nature of our algorithm makes it highly suited to large scale
problems (Bottou et al., 2016) in contrast to the methods in
Chi & Lange (2015); Chen et al. (2015).

We show that the SON formualation (1) provides strong
cluster recovery properties that go far beyond the special
case considered in Zhu et al. (2010). Our cluster recovery
conditions are similar in spirit to the unifying general con-
ditions recently formulated in A. Kumar (2010); P.Awasthi
(2012) of the form that the means of the clusters are well–
separated, i.e., the distance between the means of any two
clusters is at least Ω(k) standard deviations (the notion of
standard deviations is based on the spectral norm of the ma-
trix whose rows represent the difference between a point
and the mean of the cluster to which it belongs). Besides
containing the result of Zhu et al. (2010) as a special case,
the condition essentially recovers the well known cluster
recovery conditions for paradigm examples such as mix-
tures of Gaussians and planted partition models. The algo-
rithms in A. Kumar (2010); P.Awasthi (2012) are based on

an SVD-based initialization followed by applying Lloyd’s
K–means algorithm, so K must be known in advance. Our
method does not need to know K and is independent of any
initialization.

A summary of our contributions are:

• We develop a new incremental proximal algorithm for

the SON optimization problem (1).

• We give a convergence analysis for our algorithm
that reﬁnes and strengthens the analysis in Bertsekas
(2011).

• We show that the SON formulation (1) provides strong
cluster recovery guarantees that is far more general
than previously known recovery results, essentially
similar to the recently discovered unifying center sep-
aration conditions.

• We give experimental results giving evidence that our
algorithm produces clusters of comparable quality to
previous methods but scales much better to large scale
problems.

2. Related Work

The SON formulation ﬁrst appeared in (Lindsten et al.,
2011) and in closely related forms in Hocking et al. (Hock-
ing et al., 2011). Lindsten et al (Lindsten et al., 2011) used
an off–the–shelf convex solver, CVX to generate solution
paths. Hocking et al.
(Hocking et al., 2011) introduced
three distinct algorithms for the three most commonly en-
countered norms. For the (cid:96)1 norm, the objective function
separates, and they solve the convex clustering problem
by the exact path following method designed for the fused
lasso. For the (cid:96)1 and (cid:96)2 norms, they employ subgradient
descent in conjunction with active sets. Neither provides
any theoretical results on cluster recovery. Chi et al (Chi
& Lange, 2015; Chen et al., 2015) introduce two similar
generic frameworks for minimizing the convex clustering
objective function with an arbitrary norm. One approach
solves the problem by the alternating direction method of
multipliers (ADMM), while the other solves it by the alter-
nating minimization algorithm (AMA). The ﬁrst (and only)
theoretical results on cluster recovery are in (Zhu et al.,
2010) but this is a very simple special case of exactly two
cube shaped clusters that are well separated. This work
also does not develop a specialized algorithm for the SON
formulation.

3. Cluster Recovery

To express our results, we ﬁrst review few deﬁnitions:
Deﬁnition 1. Take a ﬁnite set X = {x1, x2, . . . , xn} ⊂
Rm and its partitioning V = {V1, V2, . . . , VK}, where each

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

Vk is a subset of X. We say that a map φ on X perfectly
recovers V when φ(xi) = φ(xj) is equivalent to xi, xj
belonging to the same cluster, or in other words, there exist
distinct vectors v1, v2, . . . , vK such that φ(xi) = vα holds
whenever xi ∈ Vα.
Deﬁnition 2. For any set S ⊂ Rm, its diameter is deﬁned
as

D(S) = sup{(cid:107)x − y(cid:107)2 | x, y ∈ S}.
Moreover, for any ﬁnite set T ⊂ Rm we deﬁne its separa-
tion as

d(T ) = min{(cid:107)x − y(cid:107)2 | x, y ∈ S, x (cid:54)= y}

and its Euclidean centroid as

c(T ) =

x

(cid:80)
x∈T
|V |

.

Finally, for any family of mutually disjoint ﬁnite sets T =
{Ti ⊂ Rm}, we deﬁne C(T ) = {c(Ti)}.
Deﬁnition 3. Take a ﬁnite set X = {x1, x2, . . . , xn} ⊂
Rm and its partitioning V = {V1, V2, . . . , VK}. We call a
partitioning W = {W1, W2, . . . , WL} of X a coarsening
of V if each partition Wl is obtained by taking the union of
a number of partitions Vk. Further, W is called the trivial
coarsening of V if W has exactly one element, i.e. W =
{X}. Otherwise, it is called a non-trivial coarsening.

Based on the above deﬁnitions, our result can be explained
as follows:
Theorem 1. Consider a ﬁnite set X = {xi ∈ Rm |
i = 1, 2, ..., n} of vectors and its partitioning V =
{V1, V2, . . . , VK}. Take the SON optimization in (1). De-
note its optimal solution by {¯ui} and deﬁne the map φ :
xi → φ(xi) = ¯ui.

d(C(V))
√
K
2n
then the map φ perfectly recovers V.

D(V )
|V |

max
V ∈V

≤ λ ≤

,

1. If

2. If

max
V ∈V

D(V )
|V |

≤ λ ≤ max
V ∈V

(cid:107)c(X) − c(V )(cid:107)2
|X| − |V |

,

then the map φ perfectly recovers a non-trivial coars-
ening of V.

Proof. We introduce associated centroid optimization:

min
{vα∈Rm}

1
2

K
(cid:88)

i=1

(cid:107)vα − cα(cid:107)2

2nα + λ

nαnβ(cid:107)cα − cα(cid:107)2

(cid:88)

α(cid:54)=β

(2)

where

xi

(cid:80)
i∈Vα
nα

cα =

We prove the following results, which clearly imply Theo-
rem 1:

1. Suppose that for every α ∈ [K],

max
i,j∈Vα

(cid:107)xi − xj(cid:107)

≤ λ.

nα

Then, ui = vα for i ∈ Vα is a global solution of the
SON clustering.

2. If all cαs are distinct and

≥ λ where d =

d
√

2n

K

(cid:107)cα − cβ(cid:107), then all centroids vα are distinct.

min
α(cid:54)=β

3. If max

α

(cid:107)cα−c(cid:107)
n−nα

≥ λ where c =

xi/n, then at least

n
(cid:80)
i=1

two centroids vα are distinct.

To prove the above, notice that the solution of the centroid
optimization satisﬁes

cα − vα = λ

nβzα,β

(cid:88)

β

where (cid:107)zα,β(cid:107) ≤ 1, zα,β = −zβ,α and whenever vα (cid:54)= vβ,
the relation zα,β = vα−vβ
holds. Now, for the solution
(cid:107)vα−vβ (cid:107)2
ui = vα for i ∈ Vα, deﬁne

z(cid:48)
ij =

(cid:26) zα,β
xi−xj
λnα

α (cid:54)= β
α = β

,

where i ∈ Vα, j ∈ Vβ. It is easy to see that (cid:107)z(cid:48)
ij = −z(cid:48)
z(cid:48)
ui−uj
(cid:107)ui−uj (cid:107)2

ji and whenever ui (cid:54)= uj, we have that z(cid:48)
. Further for each i,

ij(cid:107)2 ≤ 1,
ij =

(cid:88)

λ

z(cid:48)
i,j = λ

(cid:88)

zα,βnβ +

j

β

(cid:88)

j∈Vα

xi − xj
nα

= cα − vα + xi − cα = xi − vα = xi − ui

This shows that the local optimality conditions for the SON
optimization holds and proves item a.

For item b, denote the solution of the centroid optimization
by vα(λ) and notice that the solution of SON consists of
distinct elements vα = cα and is continuous at λ = 0.
Hence, vα:s remain distinct in an interval λ ∈ [0, λ1).
Take λ0 as the supremum of all possible λ1:s. Hence, the
solution in λ ∈ [0, λ0) contains distinct element and at
λ = λ0 contains two equal elements (otherwise, one can
extend [0, λ0) to some [0, λ0 +(cid:15)), which is against λ being
supremum). Now, notice that for λ ∈ [0 λ0) the objective

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

function is smooth at the optimal point. Hence, vα(λ) is
differentiable and satisﬁes
(cid:20) dvα
dλ

= H −1 ∂g
∂λ

δ =

(3)

(cid:21)

α

where [. ]α and [. ]α,β denote block vectors and block ma-
trices respectively. Moreover, H and g are the Hessian and
the gradient of the objective function at the optimal point. It
is possible, by explicitly expanding H and g, to show that
K (see the supplementary material for more
(cid:107)δ(cid:107)2 ≤ n
detailed derivations).

√

Hence,

(cid:13)
(cid:13)
(cid:13)
(cid:13)

dvα
dλ

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

√

≤ (cid:107)δ(cid:107)2 ≤

Kn

This yields for λ < λ0 to

(cid:107)vα(λ) − vβ(λ)(cid:107)2 =

cα − cβ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:18) dvα
dλ

−

dvβ
dλ

(cid:19)

dλ

λ
(cid:90)

0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≥ (cid:107)cα − cβ(cid:107)2 −

λ
(cid:90)

0

(cid:13)
(cid:13)
(cid:13)
(cid:13)

dvα
dλ

−

dvβ
dλ

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

dλ

√

≥ d − 2nλ

K

Since at λ = λ0, we have that vα = vβ for some α (cid:54)= β, we
K. this proves
get that d − 2nλ0
item b.

K ≤ 0 or λ0 ≥ d/2n

√

√

For item c, Take a value of λ, where v1 = v2 = . . . = vK.
It is simple to see that in this case vα = c. The optimality
condition leads to

c − cα = λ

zα,βnβ

(cid:88)

β(cid:54)=α

Hence, (cid:107)c − cα(cid:107)2 ≤ λ(n − nα). This proves item c.

Remark 1. The study in Zhu et al. (2010) establishes some
results for the special case of two clusters in rectangular
boxes. In this special case, we observe that our result im-
proves theirs.

Proof. Consider the notation in Zhu et al. (2010) with two
clusters V1, V2 and notice that λ = α/2 (α denotes reg-
ularization parameter in Zhu et al. (2010)). Moreover,
D(Vi) ≤ (cid:107)si(cid:107) as (cid:107)si(cid:107) is the diameter of the rectangle sur-
rounding Vi. We observe that

w1,2
n

≥ D(Vi)

2n3−i

(cid:17)

(cid:16) ni−1
n2
i
n3−i + ni

+ 1

≥

D(Vi)
ni

for i = 1, 2, which shows that the condition λ ≥ w1,2
n in
Zhu et al. (2010) is tighter than λ ≥ max D(V )/|V | in
ours. On the other hand,

(cid:107)c(Vi) − c(X)(cid:107)
n − ni

=

=

(cid:107)

n1+n2

(cid:107)c(Vi) − c(V1)n1+c(V2)n2
n − ni
(cid:107)c(V1) − c(V2)(cid:107)2
n

(4)

=

d(C(V))
n
Hence, the condition λ ≤ d(C(V))
same as our condition λ ≤ (cid:107)c(Vi)−c(X)(cid:107)

n

.

n−ni

in Zhu et al. (2010) is the

Remark 2. The second result in Theorem 1 reﬂects a hierar-
chical structure in the SON clusters: Under weaker condi-
tion than the ﬁrst part, SON may merge some clusters and
provide larger clusters than the true ones. In a recursive
way, SON clustering can be applied to each of these large
clusters to reﬁne them, which improves the guarantees in
Theorem 1. We postpone careful study of this method to
future work.

3.1. Comparison with Center Separation Conditions

Recently, there have been a number of theoretical results of
the form that if we have data points generated by a mix-
ture of K probability distributions, then one can cluster
the data points into the K clusters, one corresponding to
each component, provided the means of the different com-
ponents are well–separated. There are different notions of
well-separated, but mainly, the (best known) results can be
qualitatively stated as: “If the means of every pair of densi-
ties are at least poly(K) standard deviations apart, then we
can learn the mixture in polynomial time.”. These results
generally make heavy use of the generative model and par-
ticular properties of the distributions (Indeed, many of them
specialize to Gaussians or independent Bernoulli trials).

Kumar and Kannan (A. Kumar, 2010) and Awasthi and
Sheffet (P.Awasthi, 2012) uniﬁed these into a general de-
terministic condition which can be roughly stated as fol-
lows: “If the means of every pair of clusters are at least
Ω(K) times standard deviations apart, then we can learn
the mixture in polynomial time.” Here the spectral norm of
the matrix A − C scaled by 1√
n plays the role of standard
deviation, where A is the data matrix and C is the matrix of
cluster centers. More formally, for any two distinct clusters
α, β,

(cid:107)c(Vα) − c(Vβ)(cid:107)2 ≥ K

(cid:107)A − C(cid:107) (5)

(cid:18) 1
√

nα

+

1
√
nβ

(cid:19)

Our condition is similar in spirit:

(cid:107)c(Vα) − c(Vβ)(cid:107)2 ≥

K

d(Vα)

(6)

√

(cid:18) n
nα

(cid:19)

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

If nα ≥ wn for all clusters α, then this becomes

3.1.3. REGULAR AND DIRECTED CLUSTERS

(cid:107)c(Vα) − c(Vβ)(cid:107)2 ≥

d(Vα).

(7)

√

K
w

In the sequel, we specialize the above discussion in a num-
ber of examples and provide an explicit comparison of our
result with the center separation condition. In some cases,
our condition is slightly tighter than the center separation
guarantees, but we remind that the latter is obtained by
applying K-means and a SVD-based initialization, which
can be intractable in large problems, while our techniques
scales with the problem size more suitably.

3.1.1. MIXTURES OF GAUSSIANS

Suppose we have a mixture of K Gaussians in d dimen-
sions with mixture weights w1, · · · , wK, let w := mini wi
and let µ1, · · · , µK denote their means respectively. If we
have n = Ω(poly(d/w)) points sampled from this mixture
distribution, then with high probability, the center separa-
tion condition is satisﬁed if:

(cid:107)µr − µs(cid:107) ≥

polylog(d/w).

cKσ
√
w

Here σ is the maximum variance in any direction of any of
the Gaussians. Our cluster recovery condition (7) is satis-
ﬁed if:

(cid:107)µr − µs(cid:107) ≥

polylog(n).

cKσ
w

3.1.2. PLANTED PARTITION MODEL

In the planted partition model of McSherry, a set of n points
is implicitly partitioned into K groups. There is an (un-
known) K × K matrix of probabilities P . We are given a
graph G on these n points, where an edge between two
vertices from groups r and s is present with probability
Pr,s. We can consider these n points x1, · · · , xn ∈ Rn
where coordinate j in xi is 1 if (i, j) ∈ G and 0 otherwise.
The center µr of cluster r has in coordinate j the value
Pr,ψ(j), where ψ(j) is the cluster vertex j belongs to. Ku-
mar and Kannan show that the center separation condition
holds with probability at least 1 − δ if:

(cid:107)µr − µs(cid:107) ≥ cσ2K(

+ log

1
w

n
δ

)

where c is a large constant, w is such that every group has
size at least w · n and σ2 := maxr,s Pr,s. Our center sepa-
ration condition (7) is satisﬁed if:

Besides the stochastic models, we take a closer look at the
result in A. Kumar (2010) and identify deterministic cases
where the SON has better performance than the proved
bounds for K-means. These cases essentially guarantee that
the term (cid:107)A−C(cid:107) in (5) remains large and the bound therein
becomes highly restrictive:

Deﬁnition 4. We
{V1, V2, . . . , VK}
(δ, γ)−expanded if

say
of X

that
=

a

partition V
{x1, x2, . . . , xn}

=
is

|{x ∈ V | (cid:107)x − c(V )(cid:107)2 ≥ δ}| ≥ γ|V |.

We further say that this partition is (w, D, (cid:15), γ)-regular if
for all V ∈ V we have D(V ) ≥ D, |V | ≥ wn and it is
((cid:15)D, γ)−expanded.

Deﬁnition 5. We say that a set X = {x1, x2, . . . , xn} is
θ−directed if there exists a unit vector v ∈ Rm such that

(cid:88)

|vT (x − c(X))|2
(cid:107)x − c(X)(cid:107)2
2

≥ θ|X|

x∈X\{c(X)}

For a (w, D, (cid:15), γ)-regular partition, the bound in (5) implies
d(C(V)) ≥ 2cK(cid:15)D

mwn . This is because

γn

√

√

(cid:107)A − C(cid:107)2 = σmax

δiδT
i

≥

(cid:33)

Tr

(cid:19)

δiδT
i

(cid:18) n
(cid:80)
i=1
m

(cid:32) n
(cid:88)

i=1

(8)

n
(cid:80)
i=1

(cid:107)δi(cid:107)2
2

m

=

= γ(cid:15)2D2 n
m

where δi = xi − c(Vα) for i ∈ Vα. Notice that our condi-
tions can be implied by d(C(V)) ≥ 2nD
. Hence,SON
wn
can improve K-means if m ≤ wKc2γ(cid:15)2, which means that
the number of clusters K is large and the smallest fraction
of cluster size w is Ω(1).

K

√

If the (w, D, (cid:15), γ)-regular partition is further θ−directed
we may improve the previous bounds as

σmax

(cid:33)

δiδT
i

≥

(cid:32) n
(cid:88)

i=1

(cid:88)

x∈X

|vT δi|2 ≥ γ(cid:15)2D2nθ

Hence (5) implies d(C(V)) ≥ 2cK(cid:15)D
. This means that
√
wn
SON improves K-means if wK ≥ 1
c2(cid:15)2γθ , i.e. the number
of clusters is higher than a ﬁxed value.

γθn

√

4. Stochastic Splitting Algorithm

(cid:107)µr − µs(cid:107) ≥ c

σ2K
w

√

n

Our implementation is identical to the so-called proximal-
based incremental technique in Bertsekas (2011), which is

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

= Tλµ(ui + µxi, uj + µxj),

(9)

holds for every l, n with probability 1.

performed in a way that it requires little amount of calcu-
lations (precisely O(m) and independent of other param-
eters) in each iteration. The proximal-based incremental
method is a variant of the stochastic gradient technique, in
problems where many terms in the objective function are
not differentiable, and the local gradient steps are replaced
by local proximal operators. To perform the proximal-
based incremental method, we ﬁrst write the SON objective
function as

Φ(u1, u2, . . . , un) =

φij(ui, uj)

where

φij(ui, uj) =

(cid:107)xi −ui(cid:107)2

2 +

(cid:107)xj −uj(cid:107)2

2 +λ(cid:107)ui −uj(cid:107).

1
2n

Then, we introduce and explicitly calculate the proximal
operator Π(µ)

ij of φij with step size µ as

arg min
i,u(cid:48)
u(cid:48)
j

φij(u(cid:48)

i, u(cid:48)

Π(µ)
j) + 1

ij (ui, uj) =
2µ (cid:107)u(cid:48)

i − ui(cid:107)2

2 + 1

2µ (cid:107)u(cid:48)

j − uj(cid:107)2
2

(cid:88)

i<j

1
2n

where we also introduce the pairwise soft-thresholding op-
erator Tη(y, z) =

(cid:40) (cid:16)

y + η z−y
(cid:107)z−y(cid:107)2
(cid:0) y+z
(cid:1)
2 , y+z

2

, z + η y−z
(cid:107)y−z(cid:107)2

(cid:17)

(cid:107)y − z(cid:107) ≥ 2η

(cid:107)y − z(cid:107) < 2η

,

(10)
and the ﬁnal equality is obtained by the local optimality
conditions and straightforward calculations. Our algorithm
simply consists in iteratively applying randomly selected
proximal operators. This is depicted in Algorithm 1.

Algorithm 1 Stochastic Splitting Algorithm
Input: The data vectors {xk}n
{µk}∞
Initialization: Set u1, u2, . . . , un arbitrarily (we use
u1 = u2 = . . . = un = 0)
for k = 1, 2, . . . do

k=1 and step sizes

k=1

Select a pair (i, j) with i < j uniformly randomly.
Update (ui, uj) ← Π(µk)

(ui, uj)

ij

end for

4.1. Convergence Analysis

Convergence of proximal-based incremental method is
discussed in Bertsekas (2011). We further elaborate
on the convergence by further exploitation of the non-
expansiveness property of proximal operators. This allows
us to complement the result in Bertsekas (2011) in the fol-
lowing two directions: First, we establish convergence in

the probability sense (uniform convergence), while the re-
sult in Bertsekas (2011) is pointwise. Second, we prove
guaranteed speed of convergence with probability one. We
present these results by the following theorem. In this theo-
rem, we consider ﬁxed data dimension m and bounded data
vectors (i.e. (cid:107)xk(cid:107) ≤ C for some absolute constant C).
Theorem 2.

∞
(cid:80)
0

1. Assume that {µk} is non-increasing

µk = ∞ and

k < ∞. Then, the sequence Uk converges to ˜U
µ2

∞
(cid:80)
0
in the following strong probability sense:

∀(cid:15) > 0;

lim
k→∞

Pr

sup
l≥k

(cid:18)

(cid:107)Ul − ˜U(cid:107)2

F > (cid:15)

= 0 (11)

(cid:19)

2. Take µk = µ1

kα for k = 1, 2, . . . and 2

3 < α < 1. For

sufﬁciently small values of (cid:15) > 0 the relation

(cid:107)Ul − ˜U(cid:107)2

F = O

(cid:18) n4

(cid:19)

l3α−2−(cid:15)

Proof. We skip many steps in our proof for lack of space.
These steps can be found in the supplement. Denote by Uk
a matrix where the ith column is the value of ui at the kth
iteration. Deﬁne

ψµ(U) = E (Uk+1 | Uk = U, µk = µ) ,

(12)

Starting from ¯U0 = U0 (the initialization of the algo-
rithm), we deﬁne the characteristic sequence { ¯Uk}∞
k=0 by
the following iteration:

¯Uk+1 = ψµk ( ¯Uk)

Our proof is based on the following two results, which we
prove in the supplementary material:

i We have that

(cid:32)

Pr

sup
k

(cid:107)Uk − ¯Uk(cid:107)2

F +

µ2

l > λ

≤

∞
(cid:88)

l=k

(cid:33)

µ2
k

∞
(cid:80)
k=0
λ

(13)

ii Deﬁne ˜U as the unique optimal solution of the SON
optimization and suppose that {µk} is a non-increasing
sequence. There exists a universal constant a such that
(cid:107) ¯Uk − ˜U(cid:107)2

F is upper bounded by

a

k−1
(cid:88)

l=0

µ2
l e

− 2
n2

k−1
(cid:80)
s=l+1

µs

+ (cid:107)U0 − ˜U(cid:107)2
Fe

− 2
n2

k−1
(cid:80)
s=0

µs

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

To prove Theorem 2, deﬁne U k = { ¯Uk
quence obtained by starting from ¯Uk

l=0 as the se-
0 = Uk and applying

l }∞

¯Uk

l+1 = ψµl+k ( ¯Uk
l )

Take arbitrary (non-zero) positive numbers (cid:15), δ. Take λ
such that λ ≥ 2
δ

µ2
l . Take some values l0, k which we
specialize later. Now, we deﬁne two outcomes H1 and H2:

∞
(cid:80)
l=0

H1 : ∀k ≥ 0; (cid:107)Uk − ˜U(cid:107)2

F ≤ λ

H2 : ∀l ≥ 0; (cid:107) ¯Uk

(cid:15)
4
From item (i), it is simple to see that Pr(H c
1) and Pr(H c
2)
are less than δ/2. Furthermore we can show by (ii) that
under H1 ∩ H2 and suitable l0, k:

l − Ul+k(cid:107) ≤

∀l > l0; (cid:107)Ul+k − ˜U(cid:107)2

2 ≤ 2((cid:107)Ul+k − ¯Uk

l (cid:107)2

F +(cid:107) ¯Uk

l − ˜U(cid:107)2
F)

≤ 2(

+

) = (cid:15)

(cid:15)
4

(cid:15)
4

This is detailed in the supplement. We conclude that

Pr( sup

l>l0+k

(cid:107)Ul − ˜U(cid:107)2

2 > (cid:15)) ≤ Pr(H c

1) + Pr(H c

2) ≤ δ

which proves part (1) of Theorem.
For part (2), deﬁne kr = rγ, λr = r−β, where γ = 1− (cid:15)
1−α ,
β < γ(2α − 1) − 1, and the outcomes:

2

Qr : sup
l≥0

(cid:107)Ul+kr − ¯Ukr

l (cid:107)2

F > λr.

By item (i), we have that

∞
(cid:80)
r=1
Borel-Cantelli lemma, Qc
, Qc
r0+2, . . . simultane-
r0
ously hold for some r0 with probability 1. For simplicity
and without loss of generality, we assume that r0 = 0 as it
does not affect the asymptotic rate. Then for any r > 0, we
have that

Pr(Qr) < ∞. Hence by
r0+1, Qc

(cid:107)Ul+kr − ¯Ukr

l (cid:107)2

F ≤ λr

sup
l≥0

In particular,

(cid:107)Ukr+1 − ¯Ukr
lr

(cid:107)2
F ≤ λr

where lr = kr+1 − kr. From item (ii), we also conclude
that

(cid:107) ¯Ukr
lr

− ˜U(cid:107)2

F ≤ A

lr−1
(cid:88)

t=0

1
(t + kr)2α e

−2a

lr −1
(cid:80)
s=t+1

1
(s+kr )α

+(cid:107)Ukr − ˜U(cid:107)2
Fe

−2a

lr −1
(cid:80)
s=0

1
(s+kr )α

Figure 2. Running times of the exact SON clustering algorithm
(implemented in CVX) and stochastic splitting for samples from
a mixture of two Gaussians with increasing sample size.

where we introduce µ1 = bn2 and A = 4an4b2 for sim-
plicity. This leads to

(cid:107)Ukr+1 − ˜U(cid:107)2

F ≤ LeLk1−α

r −Lk1−α

r+1 (cid:107)Ukr − ˜U(cid:107)2

F+

2λr + A

lr(cid:88)

t=0

1

(t + kr)2α eL(kr+t)1−α−Lk1−α

r+1

where L is a suitable constant with different values at dif-
ferent occurrences. Postponing few more steps to the sup-
plementary material, we obtain that

(cid:107)Ukr − ˜U(cid:107)2

F ≤

L log r
rβ− (cid:15)

2

≤

L
rβ−(cid:15)

Take kr < l ≤ kr+1. We obtain that

(cid:107)Ul − ˜U(cid:107)2

2 ≤ 2((cid:107)Ukr − ˜U(cid:107)2

2 + (cid:107)Ukr − Ul(cid:107)2
2)

≤ 2λr +

L
rβ−(cid:15) ≤
By taking β = γ(2α − 1) − 1, we obtain part (2).

L
rβ−(cid:15) ≤

L
β−(cid:15)
γ

l

5. Experiments

We evaluate the proposed stochastic splitting algorithm in
the task of clustering points generated by Gaussians mix-
ture models. We compare the results to the exact algorithm
proposed by Lindsten et al. (2011) in terms of a) the qual-
ity of the produced clustering and b) the time spent solving
the optimization problem. The results of both algorithms
are dense embeddings of the points that are then thresh-
olded to form clusters. The clusters are the largest subsets
of nodes such that the maximum pairwise distance within
the subset is less than τ . The stochastic splitting algorithm
is implemented as in Algorithm 1. We observed in practice

n100200300400500600t(s)020406080100120 Exact Stochastic SplittingClustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

(a) σ2 = 0.01

(b) σ2 = 0.02

(c) σ2 = 0.05

(d) σ2 = 0.1

Figure 1. Adjusted Rand index for different choices of λ. Each plot represents quality of the clustering produced by solving the SON
objective exactly or with stochastic splitting. The different plots represent clustering of 200 samples from a mixture of two Gaussians in
R2 with ﬁxed separation d =

2 and variance σ2.

√

that a heuristic for adaptively setting the step-size improved
robustness and rate of convergence. Speciﬁcally, the step
size was reduced by a constant factor whenever the average
change in the objective over successive rounds in a small
window was positive. If the same average was negative,
but small in absolute value, the step size was increased by
a small constant factor.

√

The data is generated from Gaussian mixture models with
two components in R2 where the means are separated by
2 and the variance σ2 is varied. The number of sam-
d =
ples is also varied, to illustrate the computational gains of
the stochastic splitting method. As pointed out by Lindsten
et al. (2011), the choice of the regularization parameter λ
is perhaps the most challenging hurdle in applying SON
clustering. Choosing λ too high might result in a single
large cluster, and choosing it too low may cause each point
to be represented by its own cluster. While this problem
is of great importance in applications, we focus on the rel-
ative performance of the Lindsten et al. (2011) algorithm
(CVX) and stochastic splitting (SS). We report the adjusted
Rand index (Rand, 1971) as measure of cluster quality, and
would like to emphasize that this does not rely on identify-
ing the number of clusters beforehand.

Results The results of the experiments are presented in
Figures 1 and 2. We see in Figure 1 that the quality of the
clustering produced by the stochastic splitting algorithm is
comparable to that of the exact algorithm. This pattern is
consistant across choices of σ, where a high σ implies low
sample separation between the clusters. We also note that
the range of λ for which the stochastic splitting algorithm
achieves as good result as the exact algorithm is less wide
than for the exact. We believe this is due to the stochastic
nature of the algorithm which makes the resulting embed-
ding clusters less separated than in the exact version. De-
viations from the optimal embedding could be magniﬁed
by the thresholding step, effectively making the stochastic
algorithm more sensitive to the choice of threshold, and in
effect the quality more sensitive to the choice of λ. In these

experiments, the same threshold was used for both algo-
rithms, but tailored choices could be considered given an
appropriate selection criterion.

Furthermore, we see in Figure 2 that the running time of
the stochastic splitting algorithm is lower than that of the
exact algorithm, and grows signiﬁcantly slower. While the
stochastic splitting algorithm could in principle be imple-
mented in time constant in the number of samples, and in-
stead determined by the number of iterations, the adaptive
stepsize used to improve performance requires evaluation
of the objective value which scales with the number of sam-
ples. This could be improved by subsampling the terms in
the objective function, but this was not done here.

6. Conclusions

We developed a stochastic incremental algorithm based on
proximal iterations for the SON convex relaxtion of clus-
tering that is highly suited to large scale problems and gave
an analysis of its convergence propertis. We also gave quite
general theoretical guaranteees for exact recovery of clus-
ters similar to the unifying proximity condition in approxi-
mation algorithms that covers paradigm models for cluster-
ing data.

It has not escaped our attention that our algorithm can eas-
ily be adapted to incorporate similarity weights as used in
Chi & Lange (2015); Chen et al. (2015); Hocking et al.
(2011) and that it is amenable to acceleration using vari-
ance reduction and other techniques. The cluster recovery
conditions can also be extended to cover almost perfect re-
covery i.e. correctly clustering all except a small fraction
of points. A more complete experimental evaluation of our
algorithm and comparison to others will be included in a
longer version of the paper.

Acknowledgements

This work is supported in part by the Swedish Foundation
for Strategic Research (SSF).

6#10-30246810AdjustedRand00.20.40.60.811.2 Exact Stochastic Splitting6#10-30246810AdjustedRand00.20.40.60.811.2 Exact Stochastic Splitting6#10-30246810AdjustedRand00.20.40.60.811.2 Exact Stochastic Splitting6#10-30246810AdjustedRand00.20.40.60.811.2 Exact Stochastic SplittingClustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

References

A. Kumar, R. Kannan. Clustering with spectral norm and the k-

means algorithm. In FOCS, 2010.

Bach, F., Jenatton, R., Mairal, J., and Obozinski, G. Optimiza-
tion with sparsity-inducing penalties. Foundation and Trends
in Machine Learning, 1(4):1–106, 2012.

Bertsekas, D. Incremental proximal methods for large scale con-

vex optimization. Math. Program., 129(163), 2011.

Bottou, L´eon, Curtis, Frank E., and Nocedal, Jorge. Optimiza-
tion methods for large-scale machine learning. Technical re-
port, arXiv:1606.04838, June 2016. URL http://leon.
bottou.org/papers/tr-optml-2016.

Chen, Gary K., Chi, Eric C., Ranola, John M.O., and Lange,
Kenneth. Convex clustering: An attractive alternative to
hierarchical clustering. PLoS Computational Biology, 11(5):
e1004228, 2015. doi: 10.1371/journal.pcbi.1004228. URL
http://journals.plos.org/ploscompbiol/
article?id=10.1371/journal.pcbi.1004228.

Chi, Eric C. and Lange, Kenneth. Splitting methods for con-
Journal of Computational and Graphical
vex clustering.
Statistics, 24(4):994–1013, 2015.
doi: 10.1080/10618600.
2014.948181. URL http://dx.doi.org/10.1080/
10618600.2014.948181.

Defazio, A., F. Bach, F., and Lacoste-Julien, S. A fast incremental
gradient method with support for non-strongly convex compos-
ite objectives. In NIPS, pp. 1646–1654, 2014.

Hocking, T., Vert, J-P., Bach, F., and Joulin., A. Clusterpath:
an algorithm for clustering using convex fusion penalties. In
ICML, 2011.

Jain, A. K., Murty, M. N., and Flynn, P. J. Data clustering: A
review. ACM Comput. Surv., 31(3):264–323, September 1999.
ISSN 0360-0300. doi: 10.1145/331499.331504. URL http:
//doi.acm.org/10.1145/331499.331504.

Johnson, R. and Zhang, T. Accelerating stochastic gradient de-
scent using predictive variance reduction. In NIPS, pp. 315–
323, 2013.

Lindsten, F., Ohlsson, H., and Ljung, L. Clustering using sum-of-
norms regularization: With application to particle ﬁlter output
computation. 2011.

P.Awasthi, O Sheffet. Improved spectral norm bounds for cluster-

ing. In APPROX-RANDOM, 2012.

Rand, William M. Objective criteria for the evaluation of cluster-
ing methods. Journal of the American Statistical association,
66(336):846–850, 1971.

Schmidt, M., Roux, N. Le, and Bach, F. Minimizing ﬁnite sums
with the stochastic average gradient. Mathematical Program-
ming, 2016.

Zhu, C., Xu, H., Leng, C., and Yan, S. Convex optimization pro-

cedure for clustering: Theoretical revisit. In NIPS, 2010.

