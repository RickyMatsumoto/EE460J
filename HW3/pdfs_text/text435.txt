Regularising Non-linear Models Using Feature Side-information

6. Appendix

6.1. Modiﬁed Backpropogation

For notion simplicity, we consider stochastic gradient de-
scent. The objective function we want to minimize is as
following:

E = L(y, φ(x)) + λ1

∂φ(x)

||

∂xi −

∂φ(x)
∂xj

||

2Sij (12)

ij
!

Notice that the objective function includes derivative of the
learned function with respect to the input features, if we use
neural network to learn the model, the conventional back-
propagation algorithm can’t be applied directly. Therefore,
we developed a modiﬁed version of the backpropagation
algorithm to ﬁnd the gradient of the objective.
We keep the notation consistent with the notation used in
the book of (Bishop, 1995). n is the total layers (including
input and out put layer) number of the network, ak is the
pre-activation units in layer k, k1 is the number of hidden
units in hidden layer k, m is the number of output units,
and h(x) stands for the non-linear activation function.

ak = wkzk

−

z0 = x
1 + bk
zk = h(ak)
φ(x) = zn

(13)

If the network only has one hidden layer, we can derive
derivative of the regularizer with respect to weights using
δ and (15). When hidden layer’s number is more than one,
we need to introduce two more term, one to the backward
path and one to the forward path: Deﬁne Gk as the jaco-
bian of pre-activation unit at layer k with respect to pre-
activation at ﬁrst hidden layer, note layer k = 1 corre-
sponding to ﬁrst hidden layer.

Gk

mg =

∂ak
m
∂a1
g ∀

k = 1, 2, 3, ..., n

(18)

We know that:

G1

mg =

∂a1
m
∂a1
g

=

(

1 if m=g
0 others

(19)

And Gk for all k can be achieved during forward path by
the following forward propagation equation and G1

Gk

mg =

W k

mlGk

lg h′(ak

−

1

l

1

)

−

k = 2, 3, ..., n (20)

∀

!l

Deﬁne Bk which gives the derivative of the δk with respect
to the pre-activation units in the ﬁrst hidden layers:

Bk

ljg =

∂δk
lj
∂a1
g ∀

k = 1, 2, ..., n

(21)

To ﬁnd the gradient of (12), we deﬁne δk as the Jacobian
of the learned function with respect to pre-activations at the
layer k:

We know that:

Bn

ljg =

= h′′(an

l )1ljGn
lg

(22)

∂δn
lj
∂a1
g

(14)

Bk for all k can be obtained by the following propagating
equation during backward path using Bn as following:

δk =

∂φ1
∂ak
1
∂φ1
∂ak
2

...

∂φ1
∂ak
k1

⎡

⎢
⎢
⎢
⎢
⎢
⎣

∂φ2
∂ah
1
∂φ2
∂ak
2

...

∂φ2
∂ak
k1

.

· · ·

· · ·
...

· · ·

∂φm
∂ak
1
∂φm
∂ak
2

∂φm
∂ak
k1

⎤

⎥
⎥
⎥
⎥
⎥
⎦

δk for all k can be achieved by the following backpropaga-
tion equation.

δk = ((Wk+1)T δk+1)

h′(ak)

k = 1, 2, ..., n

1 (15)

∀
stands for the element wise multiplication of a

Where
column vector to every column of the matrix.

⊙

⊙

−

h′(an
1 )
0
...
0

0
h′(an
2 )
...
0

0
0

...
...
...
... h′(an

m)

(16)

⎤

⎥
⎥
⎦

Deﬁning the term δ in such a away, we can rewrite the reg-
ularizer term in equation (12) as following:

(W1(:, i))

W1(:, j))T δ1

2Sij

(17)

−

||

δn = ⎡

⎢
⎢
⎣

||

ij
!

ljg = h′′(ak
Bk

l )Gk
lg

p δk+1

pj W k+1

pl + h′(ak
l )

p W k+1

pl Bk+1

pjg

(23)

)

∀

k = 1, 2, ..., n

1

−

)

Finally, the gradient of the regularizer, i.e. second term of
the equation (12), can be calculated as follwoing:
For k = 1, i.e. ﬁrst hidden layer:

∂R
= 4λ1
∂W 1
lm
j(W1(:, k)
)

s Sms

j(W1(:, m)
−
W 1(:, s))T δ1(:, j)

)

−

W1(:, s))Tδ1(:, j)δ1(lj)

g(W 1(g, k)

W 1(g, s))B1

ljgz0
m)

−

(24)

)

+2λ1

ks Sks

For k = 2, ..., n:

)

)

∂R
∂W k
lm

= 2λ1

ks Sks

g(W 1(g, k)
)

W 1(g, s))(zk
)

−

j(W1(:, k)
m Bk
1
−

−
ljg + δk

W1(:, s))T δ1(:, j)

ljh′(ak

m )Gk
1
−

1
mg )
−

(25)

)

Regularising Non-linear Models Using Feature Side-information

Gradient with respect to bias term, for all k = 1, ..., n:

∂R
∂bk
lm

= 2λ1

Sks

(W1(:, k)

W1(:, s))T δ1(:, j)

−

!ks

j
!

(W 1(g, k)

W 1(g, s))Bk
ljg

−

g
!

(26)

The gradient of the ﬁrst part of the objective which is some
loss function we chose, is same as in the standard Back-
propagation algorithm, here we just need to rewrite it in
terms of the newly deﬁned δ. For example, if we use sig-
moid on all layers as activation function and cross entropy
loss, we have the following:

E =

(yi log φ(x)i + (1

yi) log(1

φ(x)i))

−

−

m

−

i=1
!

∂E

∂Wk = δk φ

φ(1

y
φ)

−
−

(zk

1)T

−

∂E

∂bk = δk φ

φ(1

y
φ)

−
−

(27)

(28)

(29)

Now we can ﬁnd the gradient of the loss with respect
to weights in all layers. Compared to the conventional
back propagation algorithm, except we have δ term which
is deﬁned differently than the conventional backprop
algorithm, we have one more extra term Bk to add to the
backward path and one more term Gh to the forward path.

