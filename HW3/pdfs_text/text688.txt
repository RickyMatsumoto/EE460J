Supplementary Material: Continual Learning Through Synaptic Intelligence

Friedemann Zenke * 1 Ben Poole * 1 Surya Ganguli 1

A. Split CIFAR-10/100 CNN architecture

For our CIFAR-10/100 experiments, we used the default
CIFAR-10 CNN from Keras:

Operation Kernel

Stride

Filters Dropout Nonlin.

3x32x32 input
Convolution
Convolution
MaxPool
Convolution
Convolution
MaxPool
Dense

Task 1: Dense

. . . : Dense

Task µ: Dense

3 × 3
3 × 3

3 × 3
3 × 3

1 × 1
1 × 1
2 × 2
1 × 1
1 × 1
2 × 2

ReLU
ReLU

ReLU
ReLU

ReLU

0.25

0.25
0.5

32
32

64
64

m

m

m

512

Table 1. Split CIFAR10/100 model architecture and hyperparam-
eters. m: number of splits.

B. Additional split CIFAR–10 experiments

As an additional experiment, we trained a CNN (4 convo-
lutional, followed by 2 dense layers with dropout; cf. main
text) on the split CIFAR-10 benchmark. We used the same
multi-head setup as in the case of split MNIST using Adam
(η = 1 × 10−3, β1 = 0.9, β2 = 0.999, minibatch size
256). First, we trained the network for 60 epochs on the
ﬁrst 5 categories (Task A). At this point the training accu-
racy was close to 1. Then the optimizer was reset and the
network was trained for another 60 epochs on the remain-
ing 5 categories (Task B). We ran identical experiments for
both the control case (c = 0) and the case in which consol-
idation was active (c > 0). All experiments were repeated
n = 10 times to quantify the uncertainty on the validation
set accuracy.

After training on both Task A and B, the network with con-

*Equal contribution 1Stanford University. Correspondence
to: Friedemann Zenke <fzenke@stanford.edu>, Ben Poole
<poole@cs.stanford.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure 1. Classiﬁcation accuracy for the split CIFAR-10 bench-
mark after being trained on Task A and B. Blue: Validation error,
without consolidation (c = 0). Green: Validation error, with con-
solidation (c = 0.1). Note that chance-level in this benchmark is
0.2. Error bars correspond to SD (n=10).

solidation performed signiﬁcantly better on both tasks than
the control network without consolidation (Fig. 1). While
the large performance difference on Task A can readily be
explained by the fact that consolidation alleviates the prob-
lem of catastrophic forgetting — the initial motivation for
our model — the small but signiﬁcant difference (≈ 4.5%)
in validation accuracy on Task B suggests that consolida-
tion also improves transfer learning. The network with-
out consolidation is essentially ﬁne-tuning a model which
has been pre-trained on the ﬁrst ﬁve CIFAR-10 categories.
In contrast to that, by leveraging the knowledge about the
optimization of Task A stored at the individual synapses,
the network with consolidation solves a different optimiza-
tion problem which makes the network generalize better on
Task B. This signiﬁcant effect was observed consistently
for different values of c in the range 0.1 < c < 10.

C. Comparison of path integral approach to

other metrics

Prior approaches toward measuring the sensitivity of pa-
rameters in a network have primarily focused on local met-
rics related to the curvature of the objective function at the
ﬁnal parameters (Martens, 2016). The Hessian is one pos-
sible metric, but it can be negative deﬁnite and comput-
ing even the diagonal adds additional overhead over stan-
dard backpropagation (Martens et al., 2012). An alterna-

Task ATask B0.500.751.00Validation accuracyFine tuningConsolidationContinual Learning Through Synaptic Intelligence

tive choice is the Fisher information (see for instance Kirk-
patrick et al. (2017)):

F = Ex∼D,y∼pθ(y|x)

(cid:34)(cid:18) ∂ log pθ(y|x)
∂θ

(cid:19) (cid:18) ∂ log pθ(y|x)

(cid:19)T (cid:35)

∂θ

While the Fisher information has a number of desirable
properties (Pascanu & Bengio, 2013), it requires comput-
ing gradients using labels sampled from the model distribu-
tion instead of the data distribution, and thus would require
at least one additional backpropagation pass to compute on-
line. For efﬁciency, the Fisher is often replaced with an
approximation, the empirical Fisher (Martens, 2016), that
uses labels sampled from the data distribution and can be
computed directly from the gradient of the objective at the
current parameters:

¯F = E(x,y)∼D

= E(x,y)∼D

(cid:34)(cid:18) ∂ log pθ(y|x)
∂θ
(cid:2)g(θ)g(θ)T (cid:3)

(cid:19) (cid:18) ∂ log pθ(y|x)

(cid:19)T (cid:35)

∂θ

The diagonal of the empirical Fisher yields a very similar
formula to our local importance measure ω in Eq. 3 under
gradient descent dynamics. However, the empirical Fisher
is computed at a single parameter value θ whereas the path
integral is computed over a trajectory θ(t). This yields an
important difference in the behavior of these metrics: for
a quadratic the empirical Fisher at the minimum will be 0
while the path integral will be proportional to the diago-
nal of the Hessian. Thus the path integral based approach
yields an efﬁcient algorithm with no additional gradients
required that still recovers a meaningful estimate of the cur-
vature.

References

Kirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil,
Veness, Joel, Desjardins, Guillaume, Rusu, Andrei A.,
Milan, Kieran, Quan, John, Ramalho, Tiago, Grabska-
Barwinska, Agnieszka, Hassabis, Demis, Clopath, Clau-
dia, Kumaran, Dharshan, and Hadsell, Raia. Overcom-
ing catastrophic forgetting in neural networks. PNAS, pp.
201611835, March 2017. ISSN 0027-8424, 1091-6490.
doi: 10.1073/pnas.1611835114.

Martens, James. Second-order optimization for neural net-

works. PhD thesis, University of Toronto, 2016.

Martens, James, Sutskever, Ilya, and Swersky, Kevin. Esti-
mating the hessian by back-propagating curvature. arXiv
preprint arXiv:1206.6464, 2012.

Pascanu, Razvan and Bengio, Yoshua. Revisiting nat-
arXiv preprint
for deep networks.

ural gradient
arXiv:1301.3584, 2013.

