A. Alias Sampler

Canopy — Fast Sampling with Cover Trees

A key component is the alias sampler of (Walker, 1977). Given an arbitrary discrete probability distribution on n outcomes, it
allows for O(1) sampling once an O(n) preprocessing step has been performed. Hence, drawing n observations a distribution
over n outcomes costs an amortized O(1) per sample. Given probabilities πi with π ∈ Pn the algorithm proceeds as follows:

• Decompose {1, . . . n} into sets L, H with i ∈ L if πi < n−1 and i ∈ H otherwise.
• For each i ∈ L pick some j ∈ H.

– Append the triple (i, j, πi) to an array A
– Set residual π(cid:48)
– If π(cid:48)

j := πj + πi − n−1

j > n−1 return π(cid:48)

j to H, otherwise to L.

Preprocessing takes O(n) computation and memory since we remove one element at a time from L.

• To sample from the array pick u ∼ U (0, 1) uniformly at random.
• Choose the tuple (i, j, πi) at position (cid:98)un(cid:99).
• If u − n−1(cid:98)un(cid:99) < πi return i, else return j.

This step costs O(1) operations and it follows by construction that i is returned with probability πi. Now we need a data
structure that will allow us to sample many objects in bulk without the need to inspect each item individually. Cover trees
satisfy this requirement.

B. Rejection Sampling

B.1. Flat Clusters

The proof for the proposed rejection sampler in case of sampling a cluster for a single observation x is as follows. If we
approximate p(x|θz) by some qz such that

then it follows that a sampler drawing z from

e−(cid:15)p(x|θz) ≤ qz ≤ e(cid:15)p(x|θz)

z ∼

(cid:80)

qzp(z)
z(cid:48) qz(cid:48)p(z(cid:48))

and then accepting with probability e−(cid:15)q−1
z p(x|θz) will draw from p(z|x). To prove this, we begin by computing the
probability of this sampler r(z) to return a particular value z. The sampler returns z when it (a) samples and accepts z, or (b)
samples any value, rejects it to proceed to next iteration of sampling. Using γ = (cid:80)
z(cid:48) p(x|θz(cid:48))p(z(cid:48))
to denote normalization for proposal and true posterior respectively, we have:

z(cid:48) qz(cid:48)p(z(cid:48)) and γT = (cid:80)

e−(cid:15)q−1

z p(x|θz) +

(1 − e−(cid:15)q−1

z(cid:48) p(x|θz(cid:48)))

qz(cid:48)p(z(cid:48))
γ

r(z)

p(z)p(x|θz) +

qz(cid:48)p(z(cid:48)) − r(z)

p(x|θz(cid:48))p(z(cid:48))

e−(cid:15)
γ

(cid:88)

z(cid:48)

(cid:88)

z(cid:48)
(cid:88)

z(cid:48)

r(z)
γ

p(z)p(x|θz) + r(z) − r(z)

e−(cid:15)
γ

γT

r(z) =

qzp(z)
γ

e−(cid:15)
γ

=

=

e−(cid:15)
γ
p(z)p(x|θz)
γT

r(z) =

Hence the procedure will draw from the true posterior p(z|x).

B.2. Clusters Arranged in Cover Tree

We now extend the above proof strategy when the clusters are arranged in a cover tree, thereby proving the correctness of
our rejection sampler in Sec. 3.2.2.

(13)

(14)

(15)

Canopy — Fast Sampling with Cover Trees

Similar to previous case, we approximate p(x|θz) for z in level i by some qz such that

e−(cid:15)i p(x|θz) ≤ qz ≤ e(cid:15)ip(x|θz).

Note that approximation error (cid:15)i now depends on the location of the cluster in the cover tree. To be speciﬁc, if the cluster z
is located at level i, then (cid:15)i = 2i(cid:107) ¯φ(x)(cid:107). Also, we assume the path to reach the node z starting from its (grand) parent at
level ˆı is given by T = [T (ˆı), T (ˆı − 1), ..., T (i)], with T (i) = z.

To prove the correctness of our rejection sampler in Sec. 3.2.1, we simply show that probability of this sampler to return a
particular value z is equal to the true posterior. The sampler returns z when it (a) reaches the corresponding node in the
cover tree and accepts it, or (b) rejects or exits to proceed to next iteration of sampling. So, the probability of this sampler to
return z is given by:

r(z) =

+

r(z)

A(z)
(cid:124) (cid:123)(cid:122) (cid:125)
Probability of the
sampler accepting z

E
(cid:124)(cid:123)(cid:122)(cid:125)
Probability of the
sampler rejecting or exiting

We calculate these individual terms beginning with the probability of sampler accepting z. Using γ as deﬁned in (10) and
γT = (cid:80)

z(cid:48) p(x|θz(cid:48))p(z(cid:48)), we have:


A(z) =

e(cid:15)ˆıβ(ˆı, T (ˆı))p(x|θT (ˆı))
γ
(cid:124)
(cid:125)
(cid:123)(cid:122)
Selecting the ﬁrst parent (Step 4)










ˆı
(cid:89)

(cid:18)

1 −

(cid:124)

i(cid:48)=i+1
(cid:124) (cid:123)(cid:122) (cid:125)
The loop
(Step 5)

p(T (i(cid:48)))p(x|θT (i(cid:48)))
e(cid:15)i(cid:48) β(i(cid:48), T (i(cid:48)))p(x|θT (i(cid:48)))

(cid:123)(cid:122)
Rejecting the nodes (Step 5iii)



e(cid:15)i(cid:48)−1 β(i(cid:48) − 1, T (i(cid:48) − 1))p(x|θT (i(cid:48)−1))



e(cid:15)i(cid:48) β(i(cid:48), T (i(cid:48)))p(x|θT (i(cid:48))) − p(T (i(cid:48)))p(x|θT (i(cid:48)))


(cid:125)
(cid:123)(cid:122)
(cid:124)

Selecting the next node (Step 5ii)

(cid:19)

(cid:125)



×

p(z)p(x|θz)
e(cid:15)i β(i, T (i))p(x|θT (i))
(cid:125)
(cid:123)(cid:122)
(cid:124)
Accepting node z

=

=

=

e(cid:15)ˆıβ(ˆı, T (ˆı))p(x|θT (ˆı))
γ

e(cid:15)ˆıβ(ˆı, T (ˆı))p(x|θT (ˆı))
γ
p(z)p(x|θz)
γ

(by telescoping)

(cid:34) ˆı
(cid:89)

e(cid:15)i(cid:48)−1β(i(cid:48) − 1, T (i(cid:48) − 1))p(x|θT (i(cid:48)−1))
e(cid:15)i(cid:48) β(i(cid:48), T (i(cid:48)))p(x|θT (i(cid:48)))

p(z)p(x|θz)
e(cid:15)iβ(i, T (i))p(x|θT (i))

(cid:35)

i(cid:48)=i+1
(cid:20) e(cid:15)i β(i, T (i))p(x|θT (i))
e(cid:15)ˆıβ(ˆı, T (ˆı))p(x|θT (ˆı))

(cid:21)

p(z)p(x|θz)
e(cid:15)iβ(i, T (i))p(x|θT (i))

Next, the probability of rejecting or exiting from the sampler is one minus probability of accepting any node z, i.e.

Plugging back the acceptance and exit probabilities into (17):

E = 1 −

A(z(cid:48))

(cid:88)

z(cid:48)∈Z
(cid:88)

z(cid:48)∈Z
γT
γ

= 1 −

= 1 −

p(z(cid:48))p(x|θz(cid:48))
γ

r(z) = A(z) + r(z)E

=

=

p(z)p(x|θz)
γ
p(z)p(x|θz)
γ
p(z)p(x|θz)
γT

r(z) =

(cid:18)

(cid:19)

+ r(z)

1 −

+ r(z) − r(z)

γT
γ
γT
γ

(16)

(17)

(18)

(19)

(20)

Canopy — Fast Sampling with Cover Trees

(a) Expansion rate

(b) Separation property

(c) Covering property

Figure 6. Illustration of various properties of covering tree.

Hence the procedure will draw from the true posterior p(z|x).

The above describes a rejection sampler that keeps on upper-bounding the probability of accepting a particular parameter or
any of its children. It is as aggressive as possible at retaining tight lower bounds on the acceptance probability such that
not too much effort is wasted in traversing the cover tree to he bottom. In other words, we attempt to reject as quickly as
possible. Some computational considerations are in order:

1. The computationally most expensive part is to compute the inner products
2. As soon as we compute this value for a particular ˜θz we cache it at the corresponding vertex of the cover tree.
3. To avoid expensive bookkeeping we attach to each vertex two variables: the value of the last compute inner product

.

(cid:68) ˜φ(x), ˜θz

(cid:69)

and the observation ID of x that it is associated with. +

C. Cover Trees

Cover Trees (Beygelzimer et al., 2006) and their improved version (Izbicki & Shelton, 2015) form a hierarchical data
structure that allows fast retrieval in logarithmic time. The key properties for the purpose of this paper are that it allows for
O(n log n) construction time, O(log n) retrieval, and that it only depends polynomially on the expansion rate (Karger &
Ruhl, 2002) of the underlying space, which we refer to as c. Moreover, the degree of all internal nodes is well controlled,
thus giving guarantees for retrieval (as exploited in (Beygelzimer et al., 2006)), and for sampling (as we will be using in this
paper).

The expansion rate of a set, due to (Karger & Ruhl, 2002) captures several key properties.

Deﬁnition 2 (Expansion Rate) Denote by Bρ(r) a ball of radius of r centered at ρ. Then a set S has a (l, c) expansion
rate iff all r > 0 and ρ ∈ S satisfy

|Bρ(r) ∩ S| ≥ l =⇒ |Bρ(2r) ∩ S| ≤ c |Bρ(r) ∩ S| .

(21)

In the following we set l = O(log |S|), thus referring to c simply as the expansion rate of S.

Cover trees are deﬁned as an inﬁnite succession of levels Si with i ∈ Z. Each level i contains (a nested subset of) the data
with the following properties:

• Nesting property: Si ⊆ Si−1.
• Separation property: All x, x(cid:48) ∈ Si satisfy (cid:107)x − x(cid:48)(cid:107) ≥ 2i.
• All x ∈ Si−1 have a parent in x(cid:48) ∈ Si, possibly with x = x(cid:48), with (cid:107)x − x(cid:48)(cid:107) ≤ 2i.
• As a consequence, the subtree for any x ∈ Si has distance at most 2i+1 from x.

Clearly we need to reperesent each x only once, namely in terms of Si with the largest i for which x ∈ Si holds. This data
structure has a number of highly desirable properties, as proved in (Beygelzimer et al., 2006). We list the most relevant ones
below:

• The depth of the tree in terms of its explicit representation is at most O(c2 log n).

Canopy — Fast Sampling with Cover Trees

• The maximum degree of any node is O(c4).
• Insertion & removal take at most O(c6 log n) time.
• Retrieval of the nearest neighbor takes at most O(c12 log n) time.
• The time to construct the tree is O(c6n log n).

The fast lookup of cover tree is built upon the implicit assumption in terms of the distinguishability of parameters θz, which
we also borrow in Canopy. This is related to the issue that if we had many choices of θz that, a-priori, all looked quite
relevant yet distinct, we would have no efﬁcient means of evaluating them short of testing all by brute force. Note that this
could be achieved, e.g. by using the fast hash approximation of a sampler in (Ahmed et al., 2012). This is complementary to
the present paper.

D. Theoretical Analysis

Some more conclusions we can make about the algorithm Canopy I:

Remark 3 (Rejection Sampler) The same reasoning yields a rejection sampler since

p(z|¯x)
p(z|x)

≥ e−(cid:107)φ(x)−φ(¯x)(cid:107)(cid:107)θz(cid:107) ≥ e−2¯+1L.

(22)

Here we may bound each term (and the normalization) in computing p(z|x) appropriately.

Remark 4 The efﬁciency of the sampler increases as the sample size m increases. In particular, an increase of m by O(c4)
is guaranteed to decrease ¯ by 1, thus increasing the acceptance probability π from π to
π. This follows from the fact that
each node in the cover tree has at most O(c4) children.

√

Remark 5 There is no need to build a cover tree to a level beyond ¯ since we do not exploit the improvement. This could be
used to remove the logarithmic dependence O(n log n) in constructing the cover tree and reduce it to O(n¯).

E. Feature Extraction

E.1. Denoising Autoencoder for MNIST

The autoencoder consists of an encoder with fully connected layers of size (28x28)-1000-500-250-30 and a symmetric
decoder. The thirty units in the code layer were linear and all the other units were logistic. The network was trained on the 8
million images using mean square error loss.

E.2. Denoising Autoencoder for CIFAR100

The autoencoder consists of an encoder with convolutional layers of size (3x32x32)-(64, 5, 5)-(32, 5, 5)-(16, 4, 4) and having
a 2x2 max pooling after each convolutional layer. The decoder is symmetric with max pooling replaced by upsampling. The
256 units in the code layer were linear and all the other internal units were RelU while the ﬁnal layer was sigmoid. The
network was trained on the 50 thousand images using mean square error loss.

E.3. ResNet for ImageNet

We use the state of the art deep convolutional neural network (DCNN), based on the ResNet (”Residual Network”)
architecture (He et al., 2015; 2016). ResNet consists of small building blocks of layers which learn the residual functions
with reference to the input. It is demonstrated that ResNet is able to train networks that are substantially deeper without the
problem of noisy backpropagation gradient. For feature extraction We use a 200 layer ResNet that is trained on a task of
classiﬁcation on ImageNet. In the process, the network learned which high-level visual features (and combinations of those
features) are important. After training the model, we remove the ﬁnal classiﬁcation layer of the network and extract from the
next-to-last layer of the DCNN, as the representation of the input image which is of dimension 2048.

F. Further Experimental Results

Canopy — Fast Sampling with Cover Trees

Clusters Method

Clusters Method

10

100

10

100

100

500

100

500

EM
SEM
Canopy I
Canopy II

EM
SEM
Canopy I
Canopy II

EM
SEM
Canopy I
Canopy II

EM
SEM
Canopy I
Canopy II

EM
SEM
Canopy I
Canopy II

EM
SEM
Canopy I
Canopy II

EM
SEM
Canopy I
Canopy II

EM
SEM
Canopy I
Canopy II

s/iter
39.588 ± 1.801
7.124 ± 0.241
7.453 ± 0.255
7.534 ± 0.320

512.185 ± 13.295
10.085 ± 0.162
6.882 ± 0.174
6.483 ± 0.298

LLH
3.04 ×107
3.04 ×107
1.49 ×107
1.49 ×107

3.27 ×107
3.34 ×107
2.02 ×107
1.91 ×107

s/iter
6.595 ± 0.230
0.943 ± 0.037
0.932 ± 0.027
1.008 ± 0.053

56.640 ± 1.060
4.006 ± 0.050
1.220 ± 0.025
1.015 ± 0.029

LLH (×107)
-4.35 ×105
-4.35 ×105
-4.35 ×105
-4.35 ×105

-3.93 ×105
-3.93 ×105
-3.96 ×105
-3.97 ×105

s/iter
78.019 ± 10.702
1.055 ± 0.095
1.027 ± 0.095
1.190 ± 0.099

407.764 ± 18.160
6.486 ± 0.613
2.745 ± 0.225
1.908 ± 0.152

s/iter
12.589 ± 0.255
0.491 ± 0.022
0.315 ± 0.014
0.313 ± 0.124

62.520 ± 1.135
2.276 ± 0.112
0.963 ± 0.061
0.333 ± 0.101

LLH
2.86 ×106
2.93 ×106
3.20 ×106
2.99 ×106

3.37 ×106
3.39 ×106
3.38 ×106
3.17 ×106

LLH
5.45 ×105
5.46 ×105
5.01 ×105
5.00 ×105

6.94 ×105
6.92 ×105
6.25 ×105
6.20 ×105

Random I

Random II

KMeans++

CoverTree

MNIST8m - Direct

Purity
32.39%
32.33%
42.12%
42.85%

LLH
3.05 ×107
3.03 ×107
1.49 ×107
1.49 ×107

53.20%
53.19%
53.39%
60.19%
MNIST8m - Embedding

3.26 ×107
3.34 ×107
2.04 ×107
1.90 ×107

Purity
30.76%
30.65%
40.51%
40.69%

53.24%
53.21%
53.53%
61.09%

LLH
3.04 ×107
3.04 ×107
1.49 ×107
1.49 ×107

3.28 ×107
3.34 ×107
2.01 ×107
1.90 ×107

Purity
30.81%
30.61%
40.41%
40.95%

52.45%
52.42%
53.88%
60.61%

LLH
3.05 ×107
3.04 ×107
1.50 ×107
1.50 ×107

3.32 ×107
3.33 ×107
2.02 ×107
1.90 ×107

Random I

Random II

KMeans++

CoverTree

LLH (×107)
Purity
58.43% -4.36 ×105
58.43% -4.35 ×105
58.78% -4.36 ×105
58.78% -4.35 ×105

LLH (×107)
Purity
63.14% -4.35 ×105
62.05% -4.36 ×105
61.61% -4.35 ×105
62.30% -4.36 ×105

LLH (×107)
Purity
63.19% -4.34 ×105
61.44% -4.35 ×105
64.46% -4.35 ×105
61.69% -4.35 ×105

83.95% -3.94 ×105
83.99% -3.93 ×105
83.44% -3.96 ×105
82.77% -3.97 ×105
CIFAR100 - Direct

Purity
14.27%
14.08%
12.98%
12.87%

LLH
3.03 ×106
2.93 ×106
3.36 ×106
3.08 ×106

25.19%
25.14%
22.35%
22.68%
CIFAR100 - Embedding

3.31 ×106
3.30 ×106
3.50 ×106
3.18 ×106

Purity
12.38%
12.21%
12.34%
12.50%

19.17%
18.97%
20.07%
22.26%

LLH
5.50 ×105
5.53 ×105
5.04 ×105
5.02 ×105

6.96 ×105
6.93 ×105
6.21 ×105
6.16 ×105

82.33% -3.94 ×105
83.37% -3.94 ×105
83.20% -3.97 ×105
83.21% -3.97 ×105

83.44% -3.94 ×105
83.05% -3.95 ×105
83.48% -3.96 ×105
82.66% -3.97 ×105

Purity
13.31%
14.12%
12.43%
13.23%

24.70%
24.33%
22.14%
22.83%

Purity
12.14%
11.57%
11.96%
11.97%

18.93%
18.64%
19.19%
21.61%

LLH
3.09 ×106
2.86 ×106
3.21 ×106
3.28 ×106

3.27 ×106
3.36 ×106
3.45 ×106
3.19 ×106

LLH
5.50 ×105
5.45 ×105
4.99 ×105
4.99 ×105

6.86 ×105
6.85 ×105
6.14 ×105
6.12 ×105

Purity
13.84%
14.75%
13.55%
13.72%

26.03%
26.16%
24.03%
24.91%

Purity
12.25%
12.72%
13.16%
13.01%

21.13%
21.16%
21.57%
23.18%

LLH
3.09 ×106
3.00 ×106
3.25 ×106
3.08 ×106

3.31 ×106
3.22 ×106
3.44 ×106
3.19 ×106

LLH
5.46 ×105
5.47 ×105
5.06 ×105
5.02 ×105

6.86 ×105
6.85 ×105
6.24 ×105
6.18 ×105

Clusters Method

Random I

Random II

KMeans++

CoverTree

Clusters Method

Random I

Random II

KMeans++

CoverTree

Purity
30.50%
31.69%
42.84%
42.59%

53.10%
53.52%
52.69%
60.29%

Purity
63.22%
60.58%
58.78%
58.78%

82.77%
83.44%
83.22%
82.66%

Purity
14.19%
14.90%
12.91%
12.87%

25.59%
25.39%
22.31%
22.71%

Purity
12.59%
12.68%
12.30%
12.29%

21.05%
21.20%
20.04%
22.25%

Table 1. Comparison of ESCA, Canopy I and Canopy II on cluster purity and loglikelihood on real, benchmark datasets–MNIST8m and
CIFAR-100. Additionally, standard deviations are shown for 5 runs.

F.1. Image Clustering

We sample images from varied sized clusters, as described below, to study the semantic concept they usually represent: (a)
> 10k members: As our dataset is extracted from Flickr, a photo sharing platform, it is heavily biased towards everyday
objects like humans, ﬂowers, birds, etc. We found several consistent clusters containing people (sitting, standing, crowd). (b)
> 5 but < 10k members: These contains less common semantic groups like swings, transmission lines, etc, out of which
some are absent as explicit concepts in underlying Resnet model. (c) < 5 members: We found around 15% small sized
clusters which are typically outliers containing less than 5 images. Fig. 7 contains more examples.

Canopy — Fast Sampling with Cover Trees

Figure 7. Illustration of concepts captured by clustering images in the feature space extracted by ResNet (He et al., 2015; 2016). Figure
shows four closest images of seven more randomly selected clusters (one in each row) possibly denoting the semantic concepts of ‘electrical
transmission lines’, ‘image with text’, ‘lego toys’, ‘lightening’, ‘Aurora’, ‘buggy’ and ‘eyes’. Few of the concepts are discovered by
clustering as Resnet received supervision only for 1000 categories (for example does not include label ‘lightening’, ‘thunder’, or ‘storm’).
Full set of 1000 imagenet label can be seen at http://image-net.org/challenges/LSVRC/2014/browse-synsets.

Canopy — Fast Sampling with Cover Trees

G. Graphical Explanation

We now present insights about our approach graphically.

MotivationLatent variable models (LVM), such as Mixture Models, Latent DirichletAllocation, are popular tools in statistical data analysis.They are used in diverse fields ranging from text, images, to user modelling and content recommendations.Inference is often slow2/37-20000-17500-15000-12500-100003.20E+011.28E+025.12E+022.05E+038.19E+03EMESCACanopy ICanopy IILog-likelihoodTime (s/iter)Inference StrategyInference using Gibbs sampling, stochastic EM, or stochastic variationalmethods requires drawing from3/37    z LocalGlobalInference StrategyInference using Gibbs sampling, stochastic EM, or stochastic variationalmethods requires drawing fromAssume exponential family, i.e.4/37    z LocalGlobalInsightsFor example assume we have following data:5/37InsightsFor example assume we have following data:Two key observationsPoints close by will have similar posteriorsNo need to consider clusters far away6/37InsightsFor example assume we have following data:Two key observationsPoints close by will have similar posteriorsNo need to consider clusters far awayTwo tools to exploit the observationsCover treesMetropolis Hasting sampling7/37Canopy — Fast Sampling with Cover Trees

Cover TreeCover tree is a hierarchical data structure 8/37Cover TreeCover tree is a hierarchical data structure Covering property:9/37Cover TreeCover tree is a hierarchical data structure Covering property:Separating property:10/37Computational Cost of Cover TreesDoes not depend on the dimension of the datac: Expansion rate of data or Hausdorffdimension(special case of fractal dimension)11/37InsightsFor example assume we have following data:Two key observationsPoints close by will have similar posteriorsNo need to consider clusters far awayTwo tools to exploit the observationsCover treesMetropolis Hasting sampling12/37Metropolis Hasting Sampling13/37Enables us to construct sound sampler that incorporates our intuitions Accept/RejectSamplefrom pAcceptance probabilityAn easy to draw distributionOnly need to look at a few probabilities!Canopy — Fast Sampling with Cover Trees

How to Design a Good Proposal?For example assume we have following data:14/37How to Design a Good Proposal?For example assume we have following data:Suppose for each point xwe can find surrogates15/37How to Design a Good Proposal?For example assume we have following data:Suppose for each point xwe can find surrogatesThen           becomes a good proposal forCompute alias table and re-use for many pointsCost for sampling from proposal given alias table is O(1)16/37OutlineBackgroundLatent Variable ModelsCover treeMetropolis HastingsCanopy: Proposed MethodModerate number of clustersLarge number of clustersExperimental ResultsSynthetic dataImages17/37Canopy I –Method 1Build a cover tree on data points –Cost18/37123456789Cover Tree:Data:Canopy I –Method 2Build a cover tree on data points –CostPick an accuracy level   having              elements19/37Cover Tree:123456789Data:Canopy — Fast Sampling with Cover Trees

Canopy I –Method 3Build a cover tree on data points –CostPick an accuracy level   having              elements20/37Cover Tree:123456789Data:Canopy I –Method 4Build a cover tree on data points –CostPick an accuracy level   having              elements21/37Cover Tree:123456789Data:Surrogates:Canopy I –Method 5Build a cover tree on data points –CostPick an accuracy level   having              elementsBuild alias tables for             –Cost 22/37Cover Tree:123456789Data:Surrogates:Canopy I –Method 6For each observation x perform Metropolis-Hastings23/37Data:Surrogates:Sample fromCanopy I –Method 7For each observation x perform Metropolis-Hastings24/37Data:Surrogates:Sample fromPropose in O(1)Canopy I –Method 8For each observation x perform Metropolis-Hastings25/37Data:Surrogates:Sample fromPropose in O(1)Accept/RejectCanopy — Fast Sampling with Cover Trees

Canopy I –Method 9For each observation x perform Metropolis-Hastings26/37Data:Surrogates:Sample fromPropose in O(1)Accept/RejectCanopy I –Method 10For each observation x perform Metropolis-HastingsFor exponential families:27/37Data:Surrogates:Sample fromPropose in O(1)Accept/RejectLarge Number of ClustersUsing first insight, cost reducedWhen moderate number of clusters, e.g. Choose Then alias table will be used at least Ktimes –full amortization!Total cost When there many clusters, e.g.  Either high overhead of memory and computation,Or granularity in xthat is less precise than desiredUse second insight: not all clusters are relevantApply cover trees not only to observations but also to the clusters themselves!28/37Canopy II –Method 1Build a cover tree on cluster parameters –CostPick an accuracy level 29/5Cover Tree:Cluster Parameters:124536789ABCDEFCanopy II –Method 230/5124536789ABCDEFThe nodes at the selected accuracy level    act as coarse approximation to the posterior(Sec 3.2.2 of paper)Canopy II –Method 331/5124536789ABCDEFThe nodes at the selected accuracy level    act as coarse approximation to the posteriorTreat this as a proposal for a rejection sampler(Sec 3.2.2 of paper)Canopy — Fast Sampling with Cover Trees

Canopy II –Method 432/5124536789ABCDEFThe nodes at the selected accuracy level    act as coarse approximation to the posteriorTreat this as a proposal for a rejection samplerSample from the proposal(Sec 3.2.2 of paper)Canopy II –Method 533/5If the proposed sample is accepted, exitElse descend down the tree, and obtain a finer proposal around the region of interest(Sec 3.2.2 of paper)124536789ABCDEFCanopy II –Method 634/5124536789ABCDEFIf the proposed sample is accepted, exitElse descend down the tree, and obtain a finer proposal around the region of interest(Sec 3.2.2 of paper)Canopy II –Method 735/5124536789ABCDEF∞Sampler is as aggressive as possible in rejecting early on such that not much effort is wasted in traversing the treeThe deeper we descend into the tree, the less likely we rejectIn worst case the cost isCanopy II –Full PictureUsing both the trees allows to  deal with an aggregate of clustersand dataThis leads to a much smaller observation groupEmploy a MHscheme as beforeWe propose from adistribution where both observations and clusters are grouped36/51234567891235679ClustersDataCanopy II –Descending both TreesRecursively descend in both the trees while samplingUntil number of observations for a given cluster is too smallThen use the rejection sampler as describer earlierFinally perform a MH accept/reject stepAcceptance probabilities are as high as in previous caseThis reduces total cost 37/5