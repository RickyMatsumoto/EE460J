A. Extended Introductory Discussion

A.2. Omitted Preliminary Details

Differentially Private Ordinary Least Squares

Due to space constraint, a few details from the introduc-
tory parts (Sections 1,2) were omitted. We bring them in
this appendix. We especially recommend the uninformed
reader to go over the extended OLS background we pro-
vide in Appendix A.3.

A.1. Proof Of Privacy of Algorithm 1

Theorem A.1. Algorithm 1 is ((cid:15), δ)-differentially private.

Proof. The proof of the theorem is based on the fact
the Algorithm 1 is the result of composing the differen-
tially private Propose-Test-Release algorithm of (Dwork &
Lei, 2009) with the differentially private analysis of the
Johnson-Lindenstrauss transform of (Sheffet, 2015).

(cid:17)
(cid:16)(cid:112)2r ln(4/δ) + 2 ln(4/δ)

More speciﬁcally, we use Theorem B.1 from (Sheffet,
2015) that states that given a matrix A whose all of its
singular values at greater than T ((cid:15), δ) where T ((cid:15), δ)2 =
2B2
, publishing RA is ((cid:15), δ)-
(cid:15)
differentially private for a r-row matrix R whose entries
sampled are i.i.d normal Gaussians. Since we have that all
of the singular values of A(cid:48) are greater than w (as speciﬁed
in Algorithm 1), outputting RA(cid:48) is ((cid:15)/2, δ/2)-differentially
private. The rest of the proof boils down to showing that
(i) the if-else-condition is ((cid:15)/2, 0)-differentially private and
that (ii) w.p. ≤ δ/2 any matrix A whose smallest singu-
lar value is smaller than w passes the if-condition (step 3).
If both these facts hold, then knowing whether we pass
the if-condition or not is ((cid:15)/2)-differentially private and
the output of the algorithm is ((cid:15)/2, δ)-differentially private,
hence basic composition gives the overall bound of ((cid:15), δ)-
differential privacy.

To prove (i) we have that for any pair of neighboring matri-
ces A and B that differ only on the i-th row, denoted aaai and
bbbi resp., we have BTB − bbbibbbT
i . Applying
Weyl’s inequality we have

i = ATA − aaaiaaaT

σmin(BTB) ≤ σmin(BTB − bbbibbbT

i ) + σmax(bbbibbbT
i )
i ) + σmax(bbbibbbT
i )

≤ σmin(ATA) + σmax(aaaiaaaT
≤ σmin(ATA) + 2B2

hence |σmin(A)2−σmin(B)2| ≤ 2B2, so adding Lap( 4B2
(cid:15) )
is ((cid:15)/2)-differentially private.

] ≤ δ

To prove (ii), note that by standard tail-bounds on the
−
Laplace distribution we have that Pr[Z
4B2 ln(1/δ)
2 . Therefore, w.p. 1 − δ/2 it holds that
(cid:15)
any matrix A that passes the if-test of the algorithm must
have σmin(A)2 > w2. Also note that a similar argu-
ment shows that for any 0 < β < 1, any matrix A s.t.
σmin(A)2 > w2 + 4B2 ln(1/β)
passes the if-condition of
the algorithm w.p. 1 − β.

<

(cid:15)

Linear Algebra and Pseudo-Inverses. Given a matrix M
we denote its SVD as M = U SV T with U and V being
orthonormal matrices and S being a non-negative diagonal
matrix whose entries are the singular values of M . We use
σmax(M ) and σmin(M ) to denote the largest and smallest
singular value resp. Despite the risk of confusion, we stick
to the standard notation of using σ2 to denote the variance
of a Gaussian, and use σj(M ) to denote the j-th singular
value of M . We use M + to denote the Moore-Penrose in-
verse of M , deﬁned as M + = V S−1U T where S−1 is a
matrix with S−1

j,j = 1/Sj,j for any j s.t. Sj,j > 0.

denotes

the Gaussian

2πσ2)−1 exp(− x−µ

The Gaussian Distribution.
A univariate Gaus-
sian N (µ, σ2)
distribution
whose mean is µ and variance σ2, with PDF(x) =
√
(
2σ2 ). Standard concentration bounds
on Gaussians give that Pr[x > µ + 2σ(cid:112)ln(1/ν)] < ν
for any ν ∈ (0, 1
e ). A multivariate Gaussian N (µµµ, Σ)
for some positive semi-deﬁnite Σ denotes the multi-
variate Gaussian distribution where the mean of the
j-th coordinate is the µj and the co-variance between
coordinates j and k is Σj,k. The PDF of such Gaus-
sian is deﬁned only on the subspace colspan(Σ),
where for every x ∈ colspan(Σ) we have PDF(xxx) =
(cid:16)
2 (xxx − µµµ)TΣ+(xxx − µµµ)(cid:1)
(2π)rank(Σ) · ˜det(Σ)
˜det(Σ) is the multiplication of all non-zero sin-
and
gular values of Σ.
A matrix Gaussian distribution
denoted N (Ma×b, U, V ) has mean M , variance U
on its rows and variance V on its columns. For full
rank U and V it holds that PDFN (M,U,V )(X) =
(2π)−ab/2(det(U ))−b/2(det(V ))−a/2
·
2 trace (cid:0)V −1(X − M )TU −1(X − M )(cid:1)).
exp(− 1
In
our case, we will only use matrix Gaussian distributions
with N (Ma×b, Ia×a, V ) and so each row in this matrix is
an i.i.d sample from a b-dimensional multivariate Gaussian
N ((M )j→, V ).

exp (cid:0)− 1

(cid:17)−1/2

We will repeatedly use the rules regarding linear operations
on Gaussians. That in, for any c, it holds that cN (µ, σ2) =
N (c · µ, c2σ2). For any C it holds that C · N (µµµ, Σ) =
N (Cµµµ, CΣC T). And for any C is holds that N (M, U, V ) ·
C = N (M C, U, C TV C). In particular, for any ccc (which
can be viewed as a b × 1-matrix) it holds that N (M, U, V ) ·
ccc = N (Mccc, U, cccT V ccc) = N (Mccc, cccT V ccc · U ).

We will also require the following proposition.
Proposition A.2. Given σ2, λ2 s.t. 1 ≤ σ2
λ2 ≤ c2 for some
constant c, let X and Y be two random Gaussians s.t. X ∼
N (0, σ2) and Y ∼ N (0, λ2). It follows that 1
c PDFY (x) ≤
PDFX (x) ≤ cPDFcY (x) for any x.
Corollary A.3. Under the same notation as in Proposi-
tion A.2, for any set S ⊂ R it holds that 1
c Prx←Y [x ∈
S] ≤ Prx←X [x ∈ S] ≤ cPrx←cY [x ∈ S] =

Differentially Private Ordinary Least Squares

cPrx←Y [x ∈ S/c]

Proof. The proof is mere calculation.

PDFX (x)
PDFcY (x)

=

(cid:114)

c2λ2
σ2

·

(

exp(− x2
exp(− x2
1
c2λ2 −
2σ2 )
2λ2 )
λ2 − 1

x2
2
exp(− x2
exp(− x2
2 ( 1

≤ c · exp(

(cid:114)

=

λ2
σ2 ·
≥ c−1 exp( x2

PDFX (x)
PDFY (x)

2σ2 )
2c2λ2 )
1
σ2 )) ≤ c · exp(0) = c

σ2 )) ≥ exp(0)

c = c−1

.

(cid:16)

(cid:17)−

1 + x2
k

The Tk-distribution, where k
The Tk-Distribution.
is referred to as the degrees of freedom of the distribu-
tion, denotes the distribution over the reals created by in-
dependently sampling Z ∼ N (0, 1) and (cid:107)ζ(cid:107)2 ∼ χ2
k,
Z√
Its PDF is given by
and taking the quantity

(cid:107)ζ(cid:107)2/k
k+1
2 .
PDFTk (x) ∝
It is a known fact that
as k increases, Tk becomes closer and closer to a normal
Gaussian. The T -distribution is often used to determine
suitable bounds on the rate of converges, as we illustrate
in Section A.3. As the T -distribution is heavy-tailed, ex-
isting tail bounds on the T -distribution (which are of the
form: if τν = C(cid:112)k((1/ν)2/k − 1) for some constant C
then (cid:82) ∞
PDFTk (x)dx < ν) are often cumbersome to work
τν
with. Therefore, in many cases in practice, it common to
assume ν = Θ(1) (most commonly, ν = 0.05) and use
existing tail-bounds on normal Gaussians.

Differential Privacy facts.
It is known (Dwork et al.,
2006b) that if ALG outputs a vector in Rd such that for
any A and A(cid:48) it holds that (cid:107)ALG(A) − ALG(A(cid:48))(cid:107)1 ≤ B,
then adding Laplace noise Lap(1/(cid:15)) to each coordinate of
the output of ALG(A) satisﬁes (cid:15)-differential privacy. Sim-
ilarly, (2006b) showed that if for any neighboring A and
2 ≤ ∆2 then adding
A(cid:48) it holds that (cid:107)ALG(A) − ALG(A(cid:48))(cid:107)2
Gaussian noise N (0, ∆2 · 2 ln(2/δ)
) to each coordinate of
the output of ALG(A) satisﬁes ((cid:15), δ)-differential privacy.

(cid:15)2

Another standard result (Dwork et al., 2006a) gives that the
composition of the output of a ((cid:15)1, δ1)-differentially private
algorithm with the output of a ((cid:15)2, δ2)-differentially private
algorithm results in a ((cid:15)1 +(cid:15)2, δ1 +δ2)-differentially private
algorithm.

A.3. Detailed Background on Ordinary Least Squares

For the unfamiliar reader, we give a short description of the
model under which OLS operates as well as the conﬁdence
bounds one derives using OLS. This is by no means an ex-

haustive account of OLS and we refer the interested reader
to (Rao, 1973; Muller & Stewart, 2006).

Given n observations {(xxxi, yi)}n
i=1 where for all i we have
xxxi ∈ Rp and yi ∈ R, we assume the existence of a p-
dimensional vector βββ ∈ Rp s.t. the label yi was derived by
yi = βββTxxxi + ei where ei ∼ N (0, σ2) independently (also
known as the homoscedastic Gaussian model). We use the
matrix notation where X denotes the (n × p)-matrix whose
rows are xxxi, and use yyy, eee ∈ Rn to denote the vectors whose
i-th entry is yi and ei resp. To simplify the discussion, we
assume X has full rank.

The parameters of the model are therefore βββ and σ2, which
we set to discover. To that end, we minimize minzzz (cid:107)yyy −
Xzzz(cid:107)2 and solve

ˆβββ = (X TX)−1X Tyyy = (X TX)−1X T(Xβββ+eee) = βββ+X +eee

that ˆβββ ∼
As eee ∼ N (000n, σ2In×n),
N (βββ, σ2(X TX)−1), or alternatively, that for every coor-
ˆβββ ∼ N (βj, σ2(X TX)−1
dinate j it holds that ˆβj = eeeT
j,j ).
j
∼ N (0, 1). In addition, we de-
Hence we get

it holds

(cid:113)

ˆβj −βj
(X TX)−1
j,j

σ
note the vector

ζζζ = y − X ˆβββ = (Xβββ + eee) − X(βββ + X +eee) = (I − XX +)eee

and since XX + is a rank-p (symmetric) projection matrix,
we have ζζζ ∼ N (0, σ2(I − XX +)). Therefore, (cid:107)ζζζ(cid:107)2 is
equivalent to summing the squares of (n − p) i.i.d samples
from N (0, σ2).
In other words, the quantity (cid:107)ζζζ(cid:107)2/σ2 is
sampled from a χ2-distribution with (n − p) degrees of
freedom.

We sidetrack from the OLS discussion to give the following
bounds on the l2-distance between βββ and ˆβββ, as the next
claim shows.

Claim A.4. For any 0 < ν < 1/2, the following holds w.p.
≥ 1−ν over the randomness of the model (the randomness
over eee)

(cid:107)βββ − ˆβββ(cid:107)2 = (cid:107)X +eee(cid:107)2

= O (cid:0)σ2 log(p/ν) · (cid:107)X +(cid:107)2

(cid:1)

F

(cid:107)ˆβββ(cid:107)2 = (cid:107)βββ + X +eee(cid:107)2
(cid:16)

= O(

n−p (cid:107)ζζζ(cid:107)2 − σ2(cid:12)

1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) = O(

(cid:107)βββ(cid:107) + σ · (cid:107)X +(cid:107)F · (cid:112)log(p/ν)
(cid:113) ln(1/ν)
n−p )

(6)

(cid:17)2

)

Proof. Since eee ∼ N (000n, σ2In×n)
then X +eee ∼
N (000n, σ2(X TX)−1). Denoting the SVD decomposition
(X TX)−1 = V SV T with S denoting the diagonal ma-
trix whose entries are σ−2
min(X), we have
that V TX +eee ∼ N (000n, σ2S). And so, each coordi-
nate of V TX +eee is distributed like an i.i.d Gaussian. So

max(X), . . . , σ−2

Differentially Private Ordinary Least Squares

i

i σ−2

w.p. ≥ 1 − ν/2 non of these Gaussians is a factor of
O(σ(cid:112)ln(p/ν)) greater than its standard deviation. And so
w.p. ≥ 1 − ν/2 it holds that (cid:107)X +eee(cid:107)2 = (cid:107)V TX +eee(cid:107)2 ≤
(X)(cid:1)).
O(σ2 log(p/ν) (cid:0)(cid:80)
(X) =
trace((X TX)−1) = trace(X +(X +)T) = (cid:107)X +(cid:107)2
F , the
bound of (6) is proven.
The bound on (cid:107)ˆβββ(cid:107)2 is an immediate corollary of (6) using
the triangle inequality.8 The bound on (cid:107)ζζζ(cid:107)2 follows from
tail bounds on the χ2
n−p distribution, as detailed in Sec-
tion 2.

Since (cid:80)

i σ−2

i

Returning to OLS, it is important to note that ˆβββ and ζζζ are
(Note, ˆβββ depends solely on
independent of one another.
X +eee = (X +X)X +eee = X +PUeee, whereas ζζζ depends on
(I − XX +)eee = PU ⊥eee. As eee is spherically symmetric, the
two projections are independent of one another and so ˆβββ is
independent of ζζζ.) As a result of the above two calcula-
tions, we have that the quantity

(βj) def=

t ˆβj

ˆβj −βj
(X TX)−1

(cid:113)

j,j · (cid:107)ζζζ(cid:107)

√

n−p

=

ˆβj −βj
(X TX)−1
j,j

(cid:113)

σ

(cid:46) (cid:107)ζζζ(cid:107)
√
σ

n−p

is distributed like a T -distribution with (n − p) degrees of
freedom. Therefore, we can compute an exact probability
estimation for this quantity. That is, for any measurable
S ⊂ R we have

Pr

(cid:104)ˆβββ and ζζζ satisfying t ˆβj

(βj) ∈ S

=

PDFTn−p (x)dx

(cid:105)

(cid:90)

S

The importance of the t-value t(βj) lies in the fact that it
can be fully estimated from the observed data X and y (for
any value of βj), which makes it a pivotal quantity. There-
fore, given X and yyy, we can use t(βj) to describe the like-
lihood of any βj — for any z ∈ R we can now give an
estimation of how likely it is to have βj = z (which is
PDFTn−p (t(z))). The t-values enable us to perform mul-
titude of statistical inferences. For example, we can say
which of two hypotheses is more likely and by how much
(e.g., we are 5-times more likely that the hypothesis βj = 3
is true than the hypothesis βj = 14 is true); we can com-
pare between two coordinates j and j(cid:48) and report we are
more conﬁdent that βj > 0 than βj(cid:48) > 0; or even compare
among the t-values we get across multiple datasets (such
as the datasets we get from subsampling rows from a sin-
gle dataset).

In particular, we can use t(βj) to α-reject unlikely values
of βj. Given 0 < α < 1, we denote cα as the number for
which the interval (−cα, cα) contains a probability mass
of 1 − α from the Tn−p-distribution. And so we derive a

8Observe, though eee is spherically symmetric, and is likely to
be approximately-orthogonal to βββ, this does not necessarily hold
for X +eee which isn’t spherically symmetric. Therefore, we result
to bounding the l2-norm of ˆβββ using the triangle bound.

corresponding conﬁdence interval Iα centered at ˆβj where
βj ∈ Iα with conﬁdence of level of 1 − α.

We comment as to the actual meaning of this conﬁdence
interval. Our analysis thus far applied w.h.p to a vector yyy
derived according to this model. Such X and yyy will re-
sult in the quantity t ˆβj
(βj) being distributed like a Tn−p-
distribution — where βj is given as the model parameters
and ˆβj is the random variable. We therefore have that guar-
antee that for X and yyy derived according to this model, the

event Eα

def= ˆβj ∈

βj ± cα ·

(cid:18)

(cid:113)

(X TX)−1

j,j · (cid:107)ζζζ(cid:107)2

n−p

(cid:19)

hap-

pens w.p. 1 − α. However, the analysis done over a given
dataset X and yyy (once yyy has been drawn) views the quan-
(βj) with ˆβj given and βj unknown. Therefore the
tity t ˆβj
event Eα either holds or does not hold. That is why the
alternative terms of likelihood or conﬁdence are used, in-
stead of probability. We have a conﬁdence level of 1 − α
that indeed βj ∈ ˆβj ± cα ·
n−p , because this
event does happen in 1−α fraction of all datasets generated
according to our model.

j,j · (cid:107)ζζζ(cid:107)2

(X TX)−1

(cid:113)

√

ˆβj
(cid:113)

(0) =

def= t ˆβj

n−p
(X TX)−1
j,j

Rejecting the Null Hypothesis. One important implica-
tion of the quantity t(βj) is that we can refer speciﬁcally to
the hypothesis that βj = 0, called the null hypothesis. This
quantity, t0
, represents how

(cid:107)ζζζ(cid:107)
large is ˆβj relatively to the empirical estimation of standard
deviation σ. Since it is known that as the number of degrees
of freedom of a T -distribution tends to inﬁnity then the T -
distribution becomes a normal Gaussian, it is common to
think of t0 as a sample from a normal Gaussian N (0, 1).
This allows us to associate t0 with a p-value, estimating the
event “βj and ˆβj have different signs.” Formally, we deﬁne
p0 = (cid:82) ∞
e−x2/2dx. It is common to reject the null
|t0|
hypothesis when p0 is sufﬁciently small (typically, below
0.05).9

1√

2π

1√

Speciﬁcally, given α ∈ (0, 1/2), we say we α-reject the
null hypothesis if p0 < α. Let τα be the number s.t.
Φ(τα) = (cid:82) ∞
e−x2/2dx = α.
(Standard bounds
τα
give that τα < 2(cid:112)ln(1/α).) This means we α-reject
the null hypothesis if t0 > τα or t0 < −τα, meaning if
| ˆβj| > τα

(X TX)−1
j,j

(cid:113)

(cid:107)ζζζ(cid:107)
n−p .

2π

√

We can now lower bound the number of i.i.d sample points
needed in order to α-reject the null hypothesis. This bound
will be our basis for comparison — between standard OLS
and the differentially private version.10

9Indeed, it is more accurate to associate with t0 the value
(cid:82) ∞
|t0| PDFTn−p (x)dx and check that this value is < α. However,
as most uses take α to be a constant (often α = 0.05), asymptoti-
cally the threshold we get for rejecting the null hypothesis are the
same.

10This theorem is far from being new (except for maybe fo-

Differentially Private Ordinary Least Squares

Theorem A.5 (Theorem 2.2 restated.). Fix any positive
deﬁnite matrix Σ ∈ Rp×p and any ν ∈ (0, 1
2 ). Fix pa-
rameters βββ ∈ Rp and σ2 and a coordinate j s.t. βj (cid:54)= 0.
Let X be a matrix whose n rows are i.i.d samples from
N (000, Σ), and yyy be a vector where yi − (Xβββ)i is sampled
i.i.d from N (0, σ2). Fix α ∈ (0, 1). Then w.p. ≥ 1 − ν
we have that the (1 − α)-conﬁdence interval is of length
(cid:112)σ2/(nσmin(Σ))) provided n ≥ C1(p + ln(1/ν))
O(cα
for some sufﬁciently large constant C1. Furthermore, there
exists a constant C2 such that w.p. ≥ 1 − α − ν we (cor-
rectly) reject the null hypothesis provided

(cid:40)

n ≥ max

C1(p + ln(1/ν)), C2

(cid:41)

σ2
β2
j

·

c2
α + τ 2
α
σmin(Σ)

cα

the

denotes
PDFTn−p (x)dx = 1 − α.

Here
which
(cid:82) cα
(If we are content
−cα
with approximating Tn−p with a normal Gaussian than
one can set cα ≈ τα < 2(cid:112)ln(1/α).)

number

for

(cid:113)

(X TX)−1
j,j

Proof. The discussion above shows that w.p. ≥ 1 − α
(cid:113)
(cid:107)ζζζ(cid:107)2
we have |βj − ˆβj| ≤ cα
n−p ; and in or-
der to α-reject the null hypothesis we must have | ˆβj| >
(cid:107)ζζζ(cid:107)2
τα
n−p . Therefore, a sufﬁcient condition for
OLS to α-reject the null-hypothesis is to have n large
(cid:107)ζζζ(cid:107)2
n−p . We there-

enough s.t. |βj| > (cα + τα)
fore argue that w.p.≥ 1 − ν this inequality indeed holds.

(X TX)−1
j,j

(X TX)−1
j,j

(cid:113)

√

We assume each row of X i.i.d vector xxxi ∼ N (000p, Σ), and
recall that according to the model (cid:107)ζζζ(cid:107)2 ∼ σ2χ2(n − p).
Straightforward concentration bounds on Gaussians and on
the χ2-distribution give:
(i) W.p. ≤ α it holds that (cid:107)ζζζ(cid:107) > σ (
(This is part of the standard OLS analysis.)
√
(ii) W.p. ≤ ν it holds that σmin(X TX) ≤ σmin(Σ)(
√
p + (cid:112)2 ln(2/ν)))2. (Rudelson & Vershynin, 2009)
(
Therefore, due to the lower bound n = Ω(p +
ln(1/ν)), w.p.≥ 1 − ν − α we have that none
In such a case we have
of
(cid:113)
)

n − p + 2 ln(2/α))).

these events hold.

j,j ≤ (cid:112)σmax((X TX)−1) = O(
n − p).

This implies that

(X TX)−1

nσmin(Σ)

1√

n −

√

and (cid:107)ζζζ(cid:107) = O(σ
conﬁdence interval of

the
length of

j,j · (cid:107)ζζζ(cid:107)2

(X TX)−1

cα

n−p = O

cα
; and that in
order to α-reject that null-hypothesis it sufﬁces to have
|βj| = Ω
bound on n, we see that this inequality holds.

. Plugging in the lower

(cα + τα)

(cid:113) σ2

nσmin(Σ)

nσmin(Σ)

(cid:16)

(cid:17)

level 1 − α has
(cid:113) σ2
(cid:17)

(cid:16)

(cid:113)

We comment that for sufﬁciently large constants C1, C2,

cusing on the setting where every row in X is sampled from an
i.i.d multivariate Gaussians), it is just stated in a non-standard
way, discussing solely the power of the t-test in OLS. For further
discussions on sample size calculations see (Muller & Stewart,
2006).

it holds that all the constants hidden in the O- and Ω-
notations of the proof are close to 1.
I.e., they are all
within the interval (1 ± η) for some small η > 0 given
C1, C2 ∈ Ω(η−2).

B. Projecting the Data using Gaussian
Johnson-Lindenstrauss Transform

B.1. Main Theorem Restated and Further Discussion

Theorem B.1 (Theorem 3.1 restated.). Let X be a n × p
matrix, and parameters βββ ∈ Rp and σ2 are such that
we generate the vector yyy = Xβββ + eee with each coordi-
nate of eee sampled independently from N (0, σ2). Assume
σmin(X) ≥ C · w and that n is sufﬁciently large s.t.
all of the singular values of the matrix [X; yyy] are greater
than C · w for some large constant C, and so Algorithm 1
projects the matrix A = [X; yyy] without altering it, and pub-
lishes [RX; Ryyy].
Fix ν ∈ (0, 1/2) and r = p + Ω(ln(1/ν)). Fix coordinate
j. Then w.p. ≥ 1 − ν we have that deriving ˜βββ, ˜ζζζ and ˜σ2 as
follows

˜βββ = (X TRTRX)−1(RX)T(Ryyy) = βββ + (RX)+Reee
˜ζζζ = 1√
= 1√
r
r
r − p

r Ryyy − 1√
(cid:0)I − (RX)(X TRTRX)−1(RX)T)(cid:1) Reee

r (RX)˜βββ

(cid:107)˜ζζζ(cid:107)2

˜σ2 =

then the pivot quantity

˜t(βj) =

˜βj − βj

(cid:113)

˜σ

(X TRTRX)−1
j,j

has a distribution D satisfying e−aPDFTr−p (x) ≤
PDFD(x) ≤ eaPDFTr−p (e−ax) for any x ∈ R, where we
denote a = r−p
n−p .

then 1√

Comparison with Existing Bounds. Sarlos’ work (2006)
utilizes the fact that when r, the numbers of rows in R,
r R is a Johnson-Lindenstrauss
is large enough,
matrix. Speciﬁcally, given r and ν ∈ (0, 1) we denote

Sarlos’ work (Sarl´os, 2006)

), and so r = O( p ln(p) ln(1/ν)

η2
r (cid:107)RXzzz − Ryyy(cid:107)2.

(cid:113) p ln(p) ln(1/ν)
η = Ω(
r
Let us denote ˜βββ = arg minzzz
In
this
(Theo-
rem 12(3)) guarantees that w.p. ≥ 1 − ν we have
(cid:17)
(cid:107)ˆβββ − ˜βββ(cid:107)2 ≤ η(cid:107)ζζζ(cid:107)/σmin(X) = O
.
Na¨ıvely bounding | ˆβj − ˜βj| ≤ (cid:107)ˆβββ − ˜βββ(cid:107) and using the
conﬁdence interval for ˆβββj − βββj
from Section A.311

rσmin(X TX) (cid:107)ζζζ(cid:107)

(cid:16)(cid:113) p log(p) log(1/ν)

setting,

).

1

11Where we approximate cα,

the tail bound of the Tn−p-
distribution with the tail bound on a Gaussian, i.e., use the ap-
proximation cα ≈ O((cid:112)ln(1/α)).

Differentially Private Ordinary Least Squares

gives a conﬁdence interval of level 1 − (α + ν) cen-
tered at ˜βj with length of O
(cid:19)

(cid:17)
rσmin(X TX) (cid:107)ζζζ(cid:107)

(cid:16)(cid:113) p ln(p) log(1/ν)

+

(cid:18)(cid:113)

=

O

log(1/α)

(X TX)−1
n−p (cid:107)ζζζ(cid:107)
j,j
(cid:16)(cid:113) p ln(p) log(1/ν)+log(1/α)

(cid:17)

.

(cid:107)ζζζ(cid:107)

rσmin(X TX)

O
This implies that
our conﬁdence interval has decreased its degrees of
freedom from n − p to roughly r/p ln(p), and furthermore,
that it no longer depends on (X T X)−1
j,j but rather on
1/σmin(X TX). It is only due to the fact that we rely on
Gaussians and by mimicking carefully the original proof
that we can deduce that the ˜t-value has (roughly) r − p
degrees of freedom and depends solely on (X T X)−1
j,j .

(In the worst case, we have that (X TX)−1
j,j is proportional
to σmin(X TX)−1, but it is not uncommon to have matrices
where the former is much larger than the latter.) As men-
tioned in the introduction, alternative techniques ((Chaud-
huri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for
ﬁnding a DP estimator βββdp of the linear regression give a
data-independent12 bound of (cid:107)βββdp − ˆβββ(cid:107) = ˜O(p/(cid:15)). Such
bounds are harder to compare with the interval length given
by Corollary 3.2. Indeed, as we discuss in Section 3 un-
der “Rejecting the null-hypothesis,” enough samples from
a multivariate Gaussian whose covariance-matrix is well
conditioned give a bound which is well below the worst-
upper bound of O(p/(cid:15)). (Yet, it is possible that these tech-
niques also do much better on such “well-behaved” data.)
What the works of Sarlos and alternative works regrading
differentially private linear regression do not take into ac-
count are questions such as generating a likelihood for βj
nor do they discuss rejecting the null hypothesis.

B.2. Proof of Theorem 3.1
We now turn to our analysis of ˜βββ and ˜ζζζ, where our goal
is to show that the distribution of the ˜t-values as spec-
iﬁed in Theorem 3.1 is well-approximated by the Tr−p-
distribution. For now, we assume the existence of ﬁxed
vectors βββ ∈ Rp and eee ∈ Rn s.t. yyy = Xβββ + eee. (Later,
we will return to the homoscedastic model where each co-
ordinate of eee is sampled i.i.d from N (0, σ2) for some σ2.)
In other words, we ﬁrst examine the case where R is the
sole source of randomness in our estimation. Based on the
assumption that eee is ﬁxed, we argue the following.

In

our model,

given X and
that ˜βββ ∼
∼

Claim B.2.
the output M = RX, we have
N (cid:0)βββ + X +eee, (cid:107)PU ⊥eee(cid:107)2(M TM )−1(cid:1)
and
(cid:17)
(Ir×r − M (M TM )−1M T)
Where
N
PU ⊥ denotes the projection operator onto the subspace
i.e., PU = XX + and
orthogonal
PU ⊥ = (Ir×r − XX +).

(cid:16)
000n, (cid:107)PU ⊥eee(cid:107)2

to colspan(X);

˜ζζζ

r

.

12In other words, independent of X, ζζζ.

is

matrix

sampled

Proof. The
R
from
N (0r×p, Ir×r, Ip×p). Given X and RX = M , we
learn the projection of each row in R onto the subspace
spanned by the columns of X. That is, denoting uuuT as
the i-th row of R and vvvT as the i-th row of M , we have
that X Tuuu = vvv. Recall, initially uuu ∼ N (000n, In×n) –
a spherically symmetric Gaussian. As a result, we can
denote uuu = PUuuu × PU ⊥uuu where the two projections are
independent samples from N (000n, PU ) and N (000n, PU ⊥ )
resp. However, once we know that vvv = X Tuuu we have that
PUuuu = X(X TX)−1X Tuuu = X(X TX)−1vvv so we learn
PUuuu exactly, whereas we get no information about PU ⊥
so PU ⊥uuu is still sampled from a Gaussian N (000n, PU ⊥ ).
As we know for each row of R that uuuTPU = vvvTX +, we
therefore have that

R = RPU + RPU ⊥ = M X + + RPU ⊥

where RPU ⊥ ∼ N (0r×n, Ir×r, PU ⊥ ). From here on, we
just rely on the existing results about the linearity of Gaus-
sians.

R ∼ N (M X +, Ir×r, PU ⊥ )

⇒ Reee ∼ N (M X +eee, (cid:107)PU ⊥eee(cid:107)2Ir×r)
⇒ M +Reee ∼ N (X +eee, (cid:107)PU ⊥eee(cid:107)2(M TM )−1)

so ˜βββ = βββ + M +Reee implies ˜βββ ∼ N (βββ +
X +eee, (cid:107)PU ⊥eee(cid:107)2(M TM )−1).
=
1√
r (Ir×r
−
then we
have ˜ζζζ
(Ir×r − M M +)M = 0r×p.

∼ N (000r, (cid:107)PU ⊥eee(cid:107)2

M (M TM )−1M T)Reee

(Ir×r − M M +))

And

as

as

˜ζζζ

r

Claim B.2 was based on the assumption that eee is ﬁxed.
However, given X and yyy there are many different ways to
assign vectors βββ and eee s.t. yyy = Xβββ + eee. However, the
distributions we get in Claim B.2 are unique. To see that,
recall Equations (1) and (2): βββ + X +eee = X +yyy = ˆβββ and
PU ⊥eee = PU ⊥yyy = (I − XX +)yyy = ζζζ. We therefore have
˜βββ ∼ N (ˆβββ, (cid:107)ζζζ(cid:107)2(M TM )−1) and ˜ζζζ ∼ N (000n, (cid:107)ζζζ(cid:107)2
r (I −
M M +)). We will discuss this further, in Section 4, where
we will not be able to better analyze the explicit distribu-
tions of our estimators. But in this section, we are able to
argue more about the distributions of ˜βββ and ˜ζζζ.

So far we have considered the case that eee is ﬁxed, whereas
our goal is to argue about the case where each coordinate
of eee is sampled i.i.d from N (0, σ2). To that end, we now
switch to an intermediate model, in which PUeee is sam-
pled from a multivariate Gaussian while PU ⊥eee is ﬁxed as
some arbitrary vector of length l. Formally, let Dl denote
the distribution where PUeee ∼ N (0, σ2PU ) and PU ⊥eee is
ﬁxed as some speciﬁc vector whose length is denoted by
(cid:107)PU ⊥eee(cid:107) = l.
Claim B.3. Under
in
Claim B.2, given that eee ∼ Dl, we have that

same assumptions as

the

Differentially Private Ordinary Least Squares

˜βββ
˜ζζζ ∼ N

∼
(cid:16)
000n, l2

N (cid:0)βββ, σ2(X T X)−1 + l2(M TM )−1(cid:1)
r (I − M M +)

(cid:17)
.

and

Proof. Recall, ˜βββ = βββ + M +Reee = βββ + M +(M X + +
RPU ⊥ )eee = βββ + X +eee + M +R(PU ⊥eee). Now, under the
assumption eee ∼ Dl we have that β is the sum of two inde-
pendent Gaussians:

βββ + X +eee ∼ N (βββ, σ2 (cid:0)X + · PU · (X +)T(cid:1))
= N (βββ, σ2(X TX)−1)
∼ N (000r, (cid:107)PU ⊥eee(cid:107)2Ir×r)

RPU ⊥eee
⇒ M +Reee ∼ N (000p, (cid:107)PU ⊥eee(cid:107)2(M TM )−1)

(cid:17)

(cid:16)
000n, (cid:107)PU ⊥eee(cid:107)2
(I − M M +)
(cid:16)
000n, l2

Summing the two independent Gaussians’ means and
variances gives the distribution of ˜βββ. Furthermore,
in
Claim B.2 we have already established that for any ﬁxed
eee we have ˜ζζζ ∼ N
r
eee ∼ Dl we still have ˜ζζζ ∼ N
. (It is
easy to verify that the same chain of derivations is applica-
ble when eee ∼ Dl.)
Corollary B.4. Given that eee ∼ Dl we have that ˜βj ∼
j,j + l2(M TM )−1
N (βj, σ2(X TX)−1
j,j ) for any coordinate
j, and that (cid:107)˜ζζζ(cid:107)2 ∼ l2
r−p.

r (I − M M +)

. Hence, for

r · χ2

(cid:17)

Proof. The corollary follows immediately from the fact
˜βββ, and from the deﬁnition of the χ2-
that βj = eeeT
j
distribution, as ˜ζζζ is a spherically symmetric Gaussian de-
ﬁned on the subspace colspan(M )⊥ of dimension r −
p.

To continue, we need the following claim.
Claim B.5. Given X and M = RX, and given that eee ∼ Dl
we have that ˜βββ and ˜ζζζ are independent.

r P ˜U ⊥ Reee = 1√

Proof. Recall, ˜βββ = βββ + X +eee + M +R(PU ⊥eee). And
so, given X, M and a speciﬁc vector PU ⊥eee we have
that the distribution of ˜βββ depends on (i) the projection
of eee on U = colspan(X) and on (ii) the projection of
each row in R onto ˜U = colspan(M ). The distribu-
tion of ˜ζζζ = 1√
r P ˜U ⊥ (M X + + RPU ⊥ )eee =
1√
r P ˜U ⊥ RPU ⊥eee depends on (i) the projection of eee onto U ⊥
(which for the time being is ﬁx to some speciﬁc vector of
length l) and on (ii) the projection of each row in R onto
˜U ⊥. Since PUeee is independent from PU ⊥eee, and since for
any row uuuT of R we have that P ˜Uuuu is independent of P ˜U ⊥uuu,
and since eee and R are chosen independently, we have that
˜βββ and ˜ζζζ are independent.
Formally, consider any pair of coordinates ˜βj and ˜ζk, and
we have

˜βj − βj = eeeT

j X +eee + eeeT

j M +(RPU ⊥eee)

˜ζk

= eeeT

k P ˜U ⊥ (RPU ⊥eee)

Recall, we are given X and M = RX. Therefore, we know
PU and P ˜U . And so

Cov[ ˜βj, ˜ζk]
= E[( ˜βj − βj)(˜ζk − 0)]
= E[eeeT

j X +eee(RPU ⊥eee)TP ˜U ⊥eeek]
j M +(RPU ⊥eee)(RPU ⊥eee)TP ˜U ⊥eeek]

+ E[eeeT

= eeeT

+ eeeT

= eeeT

+ eeeT

= eeeT

j X +E[eeeeeeTPU ⊥ ]E[RT]P ˜U ⊥eeek
j M +E[(RPU ⊥eee)(RPU ⊥eee)T]P ˜U ⊥eeek
j X +E[eeeeeeTPU ⊥ ] (cid:0)(M X +)T + E[(RPU ⊥ )T](cid:1) P ˜U ⊥eeek
(cid:1) P ˜U ⊥eeek
j M + (cid:0)(cid:107)PU ⊥eee(cid:107)2Ir×r
j X +E[eeeeeeTPU ⊥ ](X +)T (cid:0)M TP ˜U ⊥
(cid:1) eeek

(cid:1) eeek + 0

(cid:0)M +P ˜U ⊥

+ l2 · eeeT
j
= 0 + 0 + 0 = 0

And as ˜βββ and ˜ζζζ are Gaussians, having their covariance = 0
implies independence.

Having established that ˜βββ and ˜ζζζ are independent Gaussians
and speciﬁed their distributions, we continue with the proof
of Theorem 3.1. We assume for now that there exists some
small a > 0 s.t.

l2(M TM )−1

j,j ≤ σ2(X T X)−1

j,j + l2(M TM )−1
j,j
≤ e2a · l2(M TM )−1
j,j

(7)

due to Corollary A.3,

Then,
butions N1 = N (0,
N (0, σ2(X TX)−1
S ⊂ R it holds that13

j,j + l2(M TM )−1

l2(M TM )−1

denoting the distri-
j,j ) and N2 =
j,j ), we have that for any

e−aPr ˜βj ∼N1

[S] ≤ Pr ˜βj ∼N2

[S] ≤ eaPr ˜βj ∼N1

[S/ea]

(8)

More speciﬁcally, denote the function

˜t(ψ, (cid:107)ξξξ(cid:107), βj)

=

=

(cid:113) r

(cid:107)ξξξ(cid:107)

ψ − βj
r−p (M TM )−1
(cid:46) (cid:107)ξξξ(cid:107)

j,j

ψ − βj
(M TM )−1
j,j

(cid:113)

l

(cid:113) r
r−p

l

and observe that when we sample ψ, ξξξ independently s.t.
ψ ∼ N (βj,
r−p then
˜t(ψ, (cid:107)ξξξ(cid:107), βj) is distributed like a T -distribution with r − p

j,j ) and (cid:107)ξξξ(cid:107)2 ∼ l2

l2(M TM )−1

r χ2

13In fact, it is possible to use standard techniques from differ-
ential privacy, and argue a similar result — that the probabilities
of any event that depends on some function f (βj) under βj ∼ N1
and under βj ∼ N2 are close in the differential privacy sense.

Differentially Private Ordinary Least Squares

degrees of freedom. And so, for any τ > 0 we have that
under such way to sample ψ, ξξξ we have Pr[˜t(ψ, (cid:107)ξξξ(cid:107), βj) >
τ ] = 1 − CDFTr−p (τ ).
For any τ ≥ 0 and for any non-negative real value z let Sτ
z
denote the suitable set of values s.t.

Pr


ψ∼N (βj , l2(M TM )−1
j,j )
l2
r χ2

(cid:107)ξξξ(cid:107)2∼

r−p






[˜t(ψ, (cid:107)ξξξ(cid:107), βj) > τ ]

=

PDF l2

r χ2

r−p

(z) ·

Pr
{ψ−βj ∼N (0, l2(M TM )−1

j,j )}

[Sτ

z ] dz


∞
(cid:90)

0

That is, Sτ

z =

(cid:16)

τ · z

(cid:113) r

r−p (M TM )−1

j,j , ∞

(cid:17)

.

We now use Equation (8) (Since N (0, l2(M TM )−1
precisely N1) to deduce that

j,j ) is

Pr


j,j +σ2(X TX)−1
ψ∼N (βj , l2(M TM )−1
j,j )
l2
r χ2

(cid:107)ξξξ(cid:107)2∼

r−p






[˜t(ψ, (cid:107)ξξξ(cid:107), βj) > τ ]


(cid:90) ∞

=

0

(cid:90) ∞

0
(cid:90) ∞

≤ ea

(∗)
= ea

= eaPr


PDF l2

(z)

r χ2

r−p

ψ − βj ∼ N (0, l2(M TM )−1

j,j + σ2(X TX)−1
j,j )

Pr

[Sτ

z ]dz

PDF l2

(z)

Pr
ψ−βj ∼N (0, l2(M TM )−1
j,j )

r χ2

r−p

[Sτ

z /ea]dz

PDF l2

(z)

Pr
ψ−βj ∼N (0, l2(M TM )−1
j,j )

r χ2

r−p

0

[Sτ /ea
z

]dz

[˜t(ψ, (cid:107)ξξξ(cid:107), βj) > τ /ea]

ψ∼N (βj , l2(M TM )−1
j,j )
l2
r χ2

(cid:107)ξξξ(cid:107)2∼

r−p








= ea (cid:0)1 − CDFTr−p (τ /ea)(cid:1)

z /c =
for any c > 0, since it is a non-negative interval.

where the equality (∗) follows from the fact that Sτ
Sτ /c
z
Analogously, we can also show that

Pr


ψ∼N (βj , l2(M TM )−1
j,j +σ2(X TX)−1
j,j )
l2
r χ2

(cid:107)ξξξ(cid:107)2∼

r−p



≥ e−aPr


ψ∼N (βj , l2(M TM )−1
j,j )
l2
r χ2
(cid:107)ξξξ(cid:107)2∼
= e−a (cid:0)1 − CDFTr−p (τ )(cid:1)

r−p








[˜t(ψ, (cid:107)ξξξ(cid:107), βj) > τ ]





[˜t(ψ, (cid:107)ξξξ(cid:107), βj) > τ ]

In other words, we have just shown that for any in-
terval I = (τ, ∞) with τ ≥ 0 we have that
[˜t(ψ, (cid:107)ξξξ(cid:107), βj) ∈ I]
Pr





j,j +σ2(X TX)−1
ψ∼N (βj , l2(M TM )−1
j,j )
l2
r χ2

(cid:107)ξξξ(cid:107)2∼

r−p





is lower bounded by ea (cid:82)
bounded by ea (cid:82)
I/ea

I

PDFTr−p (z)dz. We can now repeat

the same argument for I = (τ1, τ2) with 0 ≤ τ1 <
τ2 (using an analogous deﬁnition of Sτ1,τ2
), and again

z

r χ2

l2(M TM )−1

for any I = (τ1, τ2) with τ1 < τ2 ≤ 0, and deduce
that the PDF of the function ˜t(ψ, (cid:107)ξξξ(cid:107), βj) at x — where
j,j + σ2(X TX)−1
we sample ψ ∼ N (βj,
j,j )
and (cid:107)ξξξ(cid:107)2 ∼ l2
r−p independently — lies in the range
(cid:0)e−aPDFTr−p (x), eaPDFTr−p (x/ea)(cid:1). And so, using
Corollary B.4 and Claim B.5, we have that when eee ∼ Dl,
the distributions of ˜βj and (cid:107)˜ζζζ(cid:107)2 are precisely as stated
above, and so we have that the distribution of ˜t(βj) def=
˜t( ˜βj, (cid:107)˜ζζζ(cid:107), βj) has a PDF that at the point x is “sandwiched”
between e−aPDFTr−p (x) and eaPDFTr−p (x/ea).
Next, we aim to argue that this characterization of the
PDF of ˜t(βj) still holds when e ∼ N (000n, σ2In×n).
It would be convenient to think of eee as a sample in
N (000n, σ2PU ) × N (000n, σ2PU ⊥ ). (So while in Dl we have
PUeee ∼ N (000n, σ2PU ) but PU ⊥eee is ﬁxed, now both PUeee and
PU ⊥eee are sampled from spherical Gaussians.) The reason
why the above still holds lies in the fact that ˜t(βj) does not
depend on l. In more details:

Preee∼N (000n,σ2In×n)

(cid:2)˜t(βj) ∈ I(cid:3)

Preee∼N (000n,σ2In×n)

(cid:2)˜t(βj) ∈ I | PU ⊥eee = vvv(cid:3)PDFPU ⊥eee(vvv)dvvv

(cid:90)

vvv
(cid:90)

vvv
(cid:90)

vvv
(cid:32)

=

=

≤

=

Pr
eee∼Dl
(cid:32)

ea

(cid:2)˜t(βj) ∈ I | l = (cid:107)vvv(cid:107)(cid:3) PDFPU ⊥eee(vvv)dvvv
(cid:90)

(cid:33)

PDFTr−p (z)dz

PDFPU ⊥eee(vvv)dvvv

I/ea

PDFTr−p (z)dz

PDFPU ⊥eee(vvv)dvvv

(cid:33) (cid:90)

vvv

(cid:90)

ea

(cid:90)

I/ea

I/ea

= ea

PDFTr−p (z)dz

where the last transition is possible precisely because ˜t is
independent of l (or (cid:107)vvv(cid:107)) — which is precisely what makes
this t-value a pivot quantity. The proof of the lower bound
is symmetric.

is

lower

(cid:2)˜t(βj) ∈ I(cid:3)

if Equation (7)
To conclude, we have shown that
then for every interval I ⊂ R we have that
holds,
Preee∼N (000n,σ2In×n)
bounded
e−aPrz∼Tr−p [z ∈ I]
and
by
by
eaPrz∼Tr−p [z ∈ (I/ea)].
So to conclude the proof
of Theorem 3.1, we need to show that w.h.p such a as in
Equation (7) exists.
Claim B.6. In the homoscedastic model with Gaussian
noise, if both n and r satisfy n, r ≥ p + Ω(log(1/ν)), then
j,j ≥ l2(M TM )−1
we have that σ2(X T X)−1
and

j,j +l2(M TM )−1

bounded

upper

j,j

Using (1 + 2(r−p)
n−p ) ≤ e
from plugging a = r−p

2(r−p)
n−p , Theorem 3.1 now follows

n−p to our above discussion.

PDFTr−p (z)dz and upper

σ2(X T X)−1

j,j +l2(M TM )−1

j,j ≤ (1+ 2(r−p)

n−p )·l2(M TM )−1

j,j

Differentially Private Ordinary Least Squares

Proof. The lower bound is immediate from non-negativity
of σ2 and of (X TX)−1
j,j = (cid:107)(X TX)−1/2eeej(cid:107)2. We there-
fore prove the upper bound.
First, observe that l2 = (cid:107)PU ⊥eee(cid:107)2 is sampled from σ2·χ2
n−p
as U ⊥ is of dimension n − p. Therefore, it holds that w.p.
≥ 1 − ν/2 that

σ2 (cid:16)√

n − p − (cid:112)2 ln(2/ν)

(cid:17)2

≤ l2

4

≥ 1 − ν/2 it holds

j,j ≤ (r − p)(X TRTRX)−1

and assuming n > p+100 ln(2/ν) we therefore have σ2 ≤
3(n−p) l2.
Secondly, we argue that when r > p + 300 ln(4/ν)
that
we have that w.p.
4 (X TX)−1
3
j,j . To see this, ﬁrst
observe that by picking R ∼ N (0r×n, Ir×r, In×n) the
distribution of the product RX ∼ N (0r×d, Ir×r, X TX)
to picking Q ∼ N (0r×d, Ir×r, Id×d)
is identical
There-
the
taking
and
is
fore,
distribution
the
=
identical
(X TX)−1/2(QTQ)−1(X TX)−1/2.
Denoting
vvv = (X TX)−1/2eeej we have (cid:107)vvv(cid:107)2 = (X TX)−1
j,j .
Claim A.1 from (Sheffet, 2015) gives that w.p. ≥ 1 − ν/2
we have

product Q(X TX)1/2.
of
(cid:0)(X TX)1/2QTQ(X TX)1/2(cid:1)−1

(X TRT RX)−1

to

(r − p) · eeeT
j
= vvvT( 1

(cid:16)

(X TX)1/2QTQ(X TX)1/2(cid:17)−1

r−p QTQ)−1vvv ≥ 3

4vvvTvvv = 3

eeej
4 (X TX)−1

j,j

which implies the required.

Combining the two inequalities we get:

σ2(X TX)−1
j,j

≤ 16l2(r−p)
n−p
≤ 2(r−p)

(X TRTRX)−1
j,j
n−p l2(X TRTRX)−1

j,j

and as we denote M = RX we are done.

We comment that our analysis in the proof of Claim B.6
implicitly assumes r (cid:28) n (as we do think of the pro-
jection R as dimensionality reduction), and so the ratio
r−p
n−p is small. However, a similar analysis holds for r
which is comparable to n — in which we would argue that
σ2(X T X)−1

j,j +l2(M TM )−1

j,j

∈ [1, 1 + η] for some small η.

σ2(X T X)−1

B.3. Proof of Theorem 3.3

Theorem B.7 (Theorem 3.3 restated.). Fix a positive deﬁ-
nite matrix Σ ∈ Rp×p. Fix parameters βββ ∈ Rp and σ2 > 0
and a coordinate j s.t. βj (cid:54)= 0. Let X be a matrix whose n
rows are sampled i.i.d from N (000p, Σ). Let yyy be a vector s.t.
yi−(Xβββ)i is sampled i.i.d from N (0, σ2). Fix ν ∈ (0, 1/2)
and α ∈ (0, 1/2). Then there exist constants C1, C2, C3
and C4 such that when we run Algorithm 1 over [X; yyy] with

parameter r w.p. ≥ 1 − ν we correctly α-reject the null hy-
pothesis using ˜p0 (i.e., w.p. ≥ 1 − ν Algorithm 1 returns
matrix unaltered and we can estimate ˜t0 and verify that
indeed ˜p0 < α · e−

r−p
n−p ) provided

r ≥ p + max

C1

, C2 ln(1/ν)

(cid:40)

α + ˜τ 2
σ2(˜c2
α)
β2
j σmin(Σ)

(cid:41)

and

where
∞
(cid:82)

r−p
n−p
˜cα/e
∞
(cid:82)

r−p
n−p

˜τα/e

(cid:26)

n ≥ max

r, C3

w2
min{σmin(Σ), σ2}

, C4(p + ln(1/ν))

(cid:27)

˜cα,

˜τα

denote

the

PDFTr−p (x)dx

=

numbers
r−p
2 e−
n−p

α

s.t.

and

PDFN (0,1)(x)dx = α

r−p
n−p resp.

2 e−

Proof. First we need to use the lower bound on n to show
that indeed Algorithm 1 does not alter A, and that various
quantities are not far from their expected values. Formally,
we claim the following.

Proposition B.8. Under the same lower bounds on n and r
as in Theorem 3.3, w.p. 1−α−ν we have that Theorem 3.1
holds and also that

˜ζζζ(cid:107)2 = Θ( r−p

r (cid:107)PU ⊥eee(cid:107)2) = Θ( r−p

r (n − p)σ2)

and

(X TRTRX)−1

j,j = Θ( 1

r−p (X TX)−1
j,j )

Proof of Proposition B.8. First, we need to argue that we
have enough samples as to have the gap σ2
min([X; y]) − w2
sufﬁciently large.

Since xxxi ∼ N (0, Σ), and yi = βββTxxxi + ei with ei ∼
N (0, σ2), we have that the concatenation (xxxi ◦ yi) is also
sampled from a Gaussian. Clearly, E[yi] = βββTE[xxxi] +
E[ei] = 0. Similarly, E[xi,jyi] = E[xi,j · (βββTxxxi +
i ] + E[(cid:107)Xβββ(cid:107)2] = σ2 +
i ] = E[e2
ei)] = (Σβββ)j and E[y2
E[βββTX TXβββ] = σ2 + βββTΣβββ. Therefore, each row of A is
an i.i.d sample of N (000p+1, ΣA), with

ΣA =

(cid:18) Σ
βββTΣ

(cid:19)

Σβββ
σ2+βββTΣβββ

Denote λ2 = σmin(Σ). Then, to argue that σmin(ΣA)
is large we use the lower bound from (Ma & Zarowski,
1995)
(Theorem 3.1) combining with some simple
arithmetic manipulations to deduce that σmin(ΣA) ≥
min{σmin(Σ), σ2}.

Differentially Private Ordinary Least Squares

Having established a lower bound on σmin(ΣA), it fol-
lows that with n = Ω(p ln(1/ν)) i.i.d draws from
N (000p+1, ΣA) we have w.p. ≤ ν/4 that σmin(ATA) =
o(n) · min{σmin(Σ), σ2}. Conditioned on σmin(ATA) =
Ω(nσmin(ΣA)) = Ω(w2) being large enough, we have that
w.p. ≤ ν/4 over the randomness of Algorithm 1 the matrix
A does not pass the if-condition and the output of the algo-
rithm is not RA. Conditioned on Algorithm 1 outputting
RA, and due to the lower bound r = p + Ω(ln(1/ν)),
we have that the result of Theorem 3.1 does not hold w.p.
≤ α + ν/4. All in all we deduce that w.p. ≥ 1 − α − 3ν/4
the result of Theorem 3.1 holds. And since we argue Theo-
rem 3.1 holds, then the following two bounds that are used
in the proof14 also hold:

(X TRTRX)−1
(cid:107)PU ⊥eee(cid:107)2 = Θ((n − p)σ2)

j,j = Θ( 1

r−p (X TX)−1
j,j )

χ2

in the proof of Theorem 3.1 we argue that
Lastly,
for a given PU ⊥eee the length (cid:107)˜ζζζ(cid:107)2 is distributed like
(cid:107)PU ⊥eee(cid:107)2
r−p. Appealing again to the fact that r = p +
r
Ω(ln(1/ν) we have that w.p. ≥ ν/4 it holds that (cid:107)˜ζζζ(cid:107)2 >
2(r − p) (cid:107)PU ⊥eee(cid:107)2
. Plugging in the value of (cid:107)PU ⊥eee(cid:107)2 con-
cludes the proof of the proposition.

r

and

Based on Proposition B.8, we now show that we indeed
reject the null-hypothesis (as we should). When Theo-
rem 3.1 holds, reject the null-hypothesis iff ˜p0 < α ·
e−
we reject that null-hypothesis when | ˜βj| > e
(cid:113)
˜σ

r−p
n−p which holds iff |˜t0| > e

r−p
n−p ˜τα. This implies

(X TRTRX)−1

r−p
n−p ˜τα ·
this bound is based
| ˜βj − βj| =

on Corollary 3.2 that determines that

Note that

j,j ).

(cid:18)
e

O

r−p
n−p ˜cα · ˜σ

(cid:113)

(cid:19)

(X TRTRX)−1
j,j )

. And so we have that

and

w.p. ≥ 1 − ν we α-reject the null hypothesis when it holds

that |βj| > 3(˜cα + ˜τα)· ˜σ

(X TRTRX)−1

j,j ) ≥ e

(cid:113)

r−p
n−p (˜cα +

(cid:113)

˜τα)˜σ

(X TRTRX)−1

j,j ) (due to the lower bound n ≥ r).

Based on the bounds stated above we have that
√

˜σ = (cid:107)˜ζζζ(cid:107)

(cid:113) r

(cid:113) r−p
r

(cid:113) r

r−p = Θ(σ

n − p

r−p ) = Θ(σ

n − p)

√

and that

(X TRTRX)−1

j,j = Θ( 1

r−p (X TX)−1

j,j ) = O

(cid:16) 1

r−p ·

1
nσmin(Σ)

(cid:17)

And so, a sufﬁcient condition for rejecting the null-
hypothesis is to have

(cid:18)

|βj|

= Ω

(˜cα + ˜τα)σ

(cid:114) n − p
r − p

·

(cid:19)

(cid:113) 1

nσmin(Σ)

= Ω(e

r−p
n−p (˜cα + ˜τα)˜σ

(cid:113)

(X TRTRX)−1

j,j ))

which, given the lower bound r = p + Ω
indeed holds.

(cid:16) (˜cα+˜τα)2σ2
j σmin(Σ)

β2

(cid:17)

C. Projected Ridge Regression

In this section we deal with the case that our matrix does
not pass the if-condition of Algorithm 1. In this case, the
matrix is appended with a d × d-matrix which is wId×d.
(cid:21)
Denoting A(cid:48) =

we have that the algorithm’s

(cid:20)

A
w · Id×d

output is RA(cid:48).

Similarly to before, we are going to denote d = p + 1 and
decompose A = [X; yyy] with X ∈ Rn×p and yyy ∈ Rn, with
the standard assumption of yyy = Xβββ + eee and ei sampled
i.i.d from N (0, σ2).15 We now need to introduce some ad-
ditional notation. We denote the appended matrix and vec-
tors X (cid:48) and yyy(cid:48) s.t. A(cid:48) = [X (cid:48); yyy(cid:48)]. Meaning:

X (cid:48) =





X
wIp×p
000T
p





yyy(cid:48) =





yyy
000p
w


 = X (cid:48)βββ +









eee
−wβββ
w

def= X (cid:48)βββ + eee(cid:48)

And so we respectively denote R = [R1; R2; R3] with
R1 ∈ Rr×n, R2 ∈ Rr×p and R3 ∈ Rr×1 (so R3 is a
vector denoted as a matrix). Hence:

M (cid:48) = RX (cid:48) = R1X + wR2

Ryyy(cid:48) = RX (cid:48)βββ +Reee(cid:48) = R1yyy +wR3 = R1Xβββ +R1eee+wR3

And so, using the output RA(cid:48) of Algorithm 1, we solve
the linear regression problem derived from 1√
r RX (cid:48) and
1√
r Ryyy(cid:48). I.e., we set

βββ(cid:48)

1

r (cid:107)Ryyy(cid:48) − RX (cid:48)zzz(cid:107)2

= arg min
= (X (cid:48)TRTRX (cid:48))−1(RX (cid:48))T(Ryyy(cid:48))

zzz

Sarlos’ results (2006) regarding the Johnson Lindenstrauss
transform give that, when R has sufﬁciently many rows,
solving the latter optimization problem gives a good ap-
proximation for the solution of the optimization problem

βββR = arg minzzz (cid:107)yyy(cid:48) − X (cid:48)zzz(cid:107)2 = arg minzzz

(cid:0)(cid:107)yyy − Xzzz(cid:107)2 + w2(cid:107)zzz(cid:107)2(cid:1)

15Just as before, it is possible to denote any single column as yyy

14More accurately, both are bounds shown in Claim B.6.

and any subset of the remaining columns as X.

Differentially Private Ordinary Least Squares

The latter problem is known as the Ridge Regression prob-
lem. Invented in the 60s (Tikhonov, 1963; Hoerl & Ken-
nard, 1970), Ridge Regression is often motivated from
the perspective of penalizing linear vectors whose coefﬁ-
cients are too large.
It is also often applied in the case
where X doesn’t have full rank or is close to not hav-
ing full-rank. That is because the Ridge Regression prob-
lem is always solvable. One can show that the minimizer
βββR = (X TX + w2Ip×p)−1X Tyyy is the unique solution of
the Ridge Regression problem and that the RHS is always
deﬁned (even when X is singular).

The original focus of Ridge Regression is on penalizing
βββR for having large coefﬁcients. Therefore, Ridge Re-
gression actually poses a family of linear regression prob-
lems: minzzz (cid:107)y − Xzzz(cid:107) + λ(cid:107)zzz(cid:107)2, where one may set λ to be
any non-negative scalar. And so, much of the literature on
Ridge Regression is devoted to the art of ﬁne-tuning this
penalty term — either empirically or based on the λ that
yields the best risk: (cid:107)E[βββR] − βββ(cid:107)2 + Var(βββR).16 Here we
propose a fundamentally different approach for the choice
of the normalization factor — we set it so that solution of
the regression problem would satisfy ((cid:15), δ)-differential pri-
vacy (by projecting the problem onto a lower dimension).

While the solution of the Ridge Regression problem might
have smaller risk than the OLS solution, it is not known
how to derive t-values and/or reject the null hypothesis un-
der Ridge Regression (except for using X to manipulate
βββR back into ˆβββ = (X TX)−1X Tyyy and relying on OLS).
In fact, prior to our work there was no need for such analy-
sis! For conﬁdence intervals one could just use the standard
OLS, because access to X and yyy was given.

Therefore, much for the same reason, we are unable to de-
rive t-values under projected Ridge Regression.17 Clearly,
there are situations where such conﬁdence bounds simply
cannot be derived. (Consider for example the case where
X = 0n×p and yyy is just i.i.d draws from N (0, σ2), so
obviously [X; y] gives no information about βββ.) Nonethe-
less, under additional assumptions about the data, our work
can give conﬁdence intervals for βj, and in the case where
the interval doesn’t intersect the origin — assure us that
sign(β(cid:48)

j) = sign(βj) w.h.p.

Clearly, Sarlos’ work (2006) gives an upper bound on the
distance (cid:107)βββ(cid:48) −βββR(cid:107). However, such distance bound doesn’t
come with the coordinate by coordinate conﬁdence guar-
antee we would like to have. In fact, it is not even clear
from Sarlos’ work that E[βββ(cid:48)] = βββR (though it is obvious
to see that E[(X (cid:48)TRTRX (cid:48))]βββR = E[(RX (cid:48))TRyyy(cid:48)]). Here,

16Ridge Regression, as opposed to OLS, does not yield an un-

biased estimator. I.e., E[βββR] (cid:54)= βββ.

17Note: The na¨ıve approach of using RX (cid:48) and Ryyy(cid:48) to interpo-
late RX and Ryyy and then apply Theorem 3.1 using these estima-
tions of RX and Ryyy ignores the noise added from appending the
matrix A into A(cid:48), and it is therefore bound to produce inaccurate
estimations of the t-values.

we show that E[βββ(cid:48)] = ˆβββ which, more often than not, does
not equal βββR.

Comment about notation. Throughout this section we as-
sume X is of full rank and so (X TX)−1 is well-deﬁned. If
X isn’t full-rank, then one can simply replace any occur-
rence of (X TX)−1 with X +(X +)T. This makes all our
formulas well-deﬁned in the general case.

C.1. Running OLS on the Projected Data

In this section, we analyze the projected Ridge Regression,
under the assumption (for now) that eee is ﬁxed. That is, for
now we assume that the only source of randomness comes
from picking the matrix R = [R1; R2; R3]. As before, we
analyze the distribution over βββ(cid:48) (see Equation (9)), and the
value of the function we optimize at βββ(cid:48). Denoting M (cid:48) =
RX (cid:48), we can formally express the estimators:

βββ(cid:48) = (M (cid:48)TM (cid:48))−1M (cid:48)TRyyy(cid:48)
ζζζ (cid:48) = 1√

r (Ryyy(cid:48) − RX (cid:48)βββ(cid:48))

(9)
(10)

Claim C.1. Given that yyy = Xβββ +eee for a ﬁxed eee, and given
X and M (cid:48) = RX (cid:48) = R1X + wR2 we have that

βββ(cid:48) ∼ N

(cid:16)
βββ + X +eee,

(w2((cid:107)βββ + X +eee(cid:107)2 + 1) + (cid:107)PU ⊥eee(cid:107)2)(M (cid:48)TM (cid:48))−1(cid:17)
(cid:16)
000r,

ζζζ (cid:48) ∼ N

w2((cid:107)βββ+X +eee(cid:107)2+1)+(cid:107)PU ⊥eee(cid:107)2
r

(Ir×r − M (cid:48)M (cid:48)+)

(cid:17)

and furthermore, βββ(cid:48) and ζζζ (cid:48) are independent of one another.

Proof. First, we write βββ(cid:48) and ζζζ (cid:48) explicitly, based on eee and
projection matrices:

βββ(cid:48)

ζζζ (cid:48)

= (M (cid:48)TM (cid:48))−1M (cid:48)TRyyy(cid:48)
= M (cid:48)+(R1X)βββ + M (cid:48)+(R1eee + wR3)
= 1√
= 1√
= 1√

r (Ryyy(cid:48) − RX (cid:48)βββ(cid:48))
r (Ir×r − M (cid:48)M (cid:48)+)Reee(cid:48)
r PU (cid:48)⊥ (R1eee − wR2βββ + wR3)

with U (cid:48) denoting colspan(M (cid:48)) and PU (cid:48)⊥ denoting the pro-
jection onto the subspace U (cid:48)⊥.

Again, we break eee into an orthogonal composition: eee =
PUeee + PU ⊥eee with U = colspan(X) (hence PU = XX +)
and U ⊥ = colspan(X)⊥. Therefore,

βββ(cid:48) = M (cid:48)+(R1X)βββ + M (cid:48)+(R1XX +eee + R1PU ⊥eee + wR3)
= M (cid:48)+(R1X)(βββ + X +eee) + M (cid:48)+(R1PU ⊥eee + wR3)

Differentially Private Ordinary Least Squares

whereas ζζζ (cid:48) is essentially

r (Ir×r − M (cid:48)M (cid:48)+)(R1XX +eee + R1PU ⊥eee − wR2βββ + wR3)
1√
(∗)
= 1√

r (Ir×r − M (cid:48)M (cid:48)+)·

(R1XX +eee + R1PU ⊥eee + (M (cid:48) − wR2)βββ + wR3)

= 1√

r (Ir×r − M (cid:48)M (cid:48)+)·

(R1X(βββ + X +eee) + R1PU ⊥eee + wR3)
where equality (∗) holds because (I − M (cid:48)M (cid:48)+)M (cid:48)vvv = 000
for any vvv.

We now aim to describe the distribution of R given that we
know X (cid:48) and M (cid:48) = RX (cid:48). Since

M (cid:48) = R1X + wR2 + 0 · R3 = R1X(X +X) + wR2

= (R1PU )X + wR2

then M (cid:48) is independent of R3 and independent of R1PU ⊥ .
Therefore, given X and M (cid:48) the induced distribution over
R3 remains R3 ∼ N (000r, Ir×r), and similarly, given X and
M (cid:48) we have R1PU ⊥ ∼ N (0r×n, Ir×r, PU ⊥ ) (rows remain
independent from one another, and each row is distributed
like a spherical Gaussian in colspan(X)⊥). And so, we
have that R1X = R1PU X = M (cid:48) − wR2, which in turn
implies:

R1X ∼ N (cid:0)M (cid:48), Ir×r, w2 · Ip×p

(cid:1)

multiplying this random matrix with a vector, we get
R1X(βββ+X +eee) ∼ N (M (cid:48)βββ + M (cid:48)X +eee, w2(cid:107)βββ + X +eee(cid:107)2Ir×r)

and multiplying this random vector with a matrix we get
M (cid:48)+R1X(βββ+X +eee) ∼ N (βββ + X +eee, w2(cid:107)βββ + X +eee(cid:107)2(M (cid:48)TM )−1)

I.e.,
M (cid:48)+R1X(βββ +X +eee) ∼ (cid:107)βββ +X +eee(cid:107)·N (uuu, w2(M (cid:48)TM )−1)

where uuu denotes a unit-length vector in the direction of βββ +
X +eee.

Similar to before we have

RPU ⊥ ∼ N (0r×n, Ir×r, PU ⊥)

⇒ M (cid:48)+(RPU ⊥eee) ∼ N (000d, (cid:107)PU ⊥ e(cid:107)2(M (cid:48)TM (cid:48))−1)

wR3 ∼ N (000r, w2Ir×r)

⇒ M (cid:48)+(wR3) ∼ N (000d, w2(M (cid:48)+M (cid:48))−1)

Therefore, the distribution of βββ(cid:48), which is the sum of the 3
independent Gaussians, is as required.
Also, ζζζ (cid:48) = 1√
r PU (cid:48)⊥ (R1X(βββ + X +eee) + R1PU ⊥eee + wR3)
is the sum of 3 independent Gaussians, which implies its
distribution is
(cid:16) 1√

r PU (cid:48)⊥ M (cid:48)(βββ + X +eee),

N

1

r (w2((cid:107)βββ + X +eee(cid:107)2 + 1) + (cid:107)PU ⊥eee(cid:107)2)PU (cid:48)⊥

(cid:1)

r (w2((cid:107)βββ + X +eee(cid:107)2 + 1) + (cid:107)PU ⊥eee(cid:107)2)PU (cid:48)⊥

I.e., N (cid:0)000r, 1
as PU (cid:48)⊥M (cid:48) = 0r×r.
Finally, observe that βββ(cid:48) and ζζζ (cid:48) are independent as the for-
mer depends on the projection of the spherical Gaussian
R1X(β + X +eee) + R1PU ⊥eee + wR3 on U (cid:48), and the latter
depends on the projection of the same multivariate Gaus-
sian on U (cid:48)⊥.

Observe that Claim C.1 assumes eee is given. This may seem
somewhat strange, since without assuming anything about
eee there can be many combinations of βββ and eee for which
yyy = Xβββ + eee. However, we always have that βββ + X +eee =
X +yyy = ˆβββ. Similarly, it is always the case the PU ⊥eee =
(I − XX +)yyy = ζζζ. (Recall OLS deﬁnitions of ˆβββ and ζζζ in
Equation (1) and (2).) Therefore, the distribution of βββ(cid:48) and
ζζζ (cid:48) is unique (once yyy is set):

βββ(cid:48) ∼ N

(cid:16)ˆβββ, (w2((cid:107)ˆβββ(cid:107)2 + 1) + (cid:107)ζζζ(cid:107)2)(M (cid:48)TM (cid:48))−1(cid:17)
(cid:32)

(cid:33)

ζζζ (cid:48) ∼ N

000r,

w2((cid:107)ˆβββ(cid:107)2 + 1) + (cid:107)ζζζ(cid:107)2
r

(Ir×r − M (cid:48)M (cid:48)+)

And so for a given dataset [X; yyy] we have that βββ(cid:48) serves as
an approximation for ˆβββ.

j,j

j,j

=

(cid:107)ζζζ(cid:48)(cid:107)

(cid:113) r

that

that

j − ˆβj
β(cid:48)

r−p ·(M (cid:48)TM (cid:48))−1

r−p ·(M (cid:48)TM (cid:48))−1

An immediate corollary of Claim C.1 is
any ﬁxed eee it holds
j −(βj +(X +eee)j )
β(cid:48)
(cid:113) r

for
the quantity t(cid:48)(βj) =
is dis-

(cid:107)ζζζ(cid:48)(cid:107)
tributed like a Tr−p-distribution. Therefore, the following
theorem follows immediately.
Theorem C.2. Fix X ∈ Rn×p and yyy ∈ R. Deﬁne ˆβββ =
X +yyy and ζ = (I − XX +)yyy. Let RX (cid:48) and Ryyy(cid:48) denote
the result of applying Algorithm 1 to the matrix A = [X; yyy]
when the algorithm appends the data with a w · I matrix.
Fix a coordinate j and any α ∈ (0, 1/2). When computing
βββ(cid:48) and ζζζ (cid:48) as in Equations (9) it and (10), we have that w.p.
≥ 1 − α it holds that

(cid:16)

ˆβj ∈

j ± c(cid:48)
β(cid:48)

α(cid:107)ζζζ (cid:48)(cid:107)

r−p · (M (cid:48)TM (cid:48))−1

j,j

(cid:113) r

(cid:17)

α denotes the number such that (−c(cid:48)

where c(cid:48)
1 − α mass of the Tr−p-distribution.

α, c(cid:48)

α) contains

Note that Theorem C.2, much like the rest of the discus-
sion in this Section, builds on yyy being ﬁxed, which means
j serves as an approximation for ˆβj. Yet our goal is to
β(cid:48)
argue about similarity (or proximity) between β(cid:48)
j and βj.
To that end, we combine the standard OLS conﬁdence in-
terval — which says that w.p. ≥ 1 − α over the ran-
domness of picking eee in the homoscedastic model we have

(cid:17)

|βj − ˆβj| ≤ cα(cid:107)ζζζ(cid:107)
n−p — with the conﬁdence in-
terval of Theorem C.2 above, and deduce that w.p. ≥ 1 − α

(cid:114)

(X TX)−1
j,j

Differentially Private Ordinary Least Squares

we have that |β(cid:48)

j − βj| is at most



O

cα

(cid:113)

√

(cid:107)ζζζ(cid:107)

(X TX)−1
j,j

n − p

(cid:113)

(cid:107)ζζζ (cid:48)(cid:107)

r(M (cid:48)TM (cid:48))−1
j,j
√

r − p





+ c(cid:48)
α

(cid:113)

(cid:107)ζζζ(cid:48)(cid:107)
√
r−p

r(M (cid:48)TM (cid:48))−1

(11)
18And so, in the next section, our goal is to give con-
ditions under which the interval of Equation (11) isn’t
much larger in comparison to the interval
length of
c(cid:48)
j,j we get from Theorem C.2; and
α
more importantly — conditions that make the interval of
Theorem C.2 useful and not too large. (Note, in expecta-
tion (cid:107)ζζζ(cid:48)(cid:107)
(w2 + w2(cid:107)ˆβββ(cid:107)2 + (cid:107)ζζζ(cid:107)2)/r. So, for
example, in situations where (cid:107)ˆβββ(cid:107) is very large, this interval
isn’t likely to inform us as to the sign of βj.)

r−p is about

(cid:113)

√

Motivating Example. A good motivating example for the
discussion in the following section is when [X; yyy] is a strict
submatrix of the dataset A. That is, our data contains many
variables for each entry (i.e., the dimensionality d of each
entry is large), yet our regression is made only over a mod-
est subset of variables out of the d. In this case, the least
singular value of A might be too small, causing the al-
gorithm to alter A; however, σmin(X TX) could be sufﬁ-
ciently large so that had we run Algorithm 1 only on [X; yyy]
we would not alter the input. (Indeed, a differentially pri-
vate way for ﬁnding a subset of the variables that induce a
submatrix with high σmin is an interesting open question,
partially answered — for a single regression — in the work
of Thakurta and Smith (Thakurta & Smith, 2013).) Indeed,
the conditions we specify in the following section depend
on σmin( 1
n X TX), which, for a zero-mean data, the mini-
mal variance of the data in any direction. For this motivat-
ing example, indeed such variance isn’t necessarily small.

C.2. Conditions for Deriving a Conﬁdence Interval for

Ridge Regression

Looking at the interval speciﬁed in Equation (11), we now
give an upper bound on the the random quantities in this
interval: (cid:107)ζζζ(cid:107), (cid:107)ζζζ (cid:48)(cid:107), and (M (cid:48)TM (cid:48))−1
j,j . First, we give bound
that are dependent on the randomness in R (i.e., we con-
tinue to view eee as ﬁxed).
Proposition C.3. For any ν
if we
have r = p + Ω(ln(1/ν)) then with probability ≥

∈ (0, 1/2),

(cid:114)

18Observe that w.p. ≥ 1 − α over the randomness of eee

(XTX)−1
j,j
n−p

(cid:113) r

we have that |βj − ˆβj| ≤ cα(cid:107)ζζζ(cid:107)
, and w.p. ≥
j − ˆβj| ≤
1 − α over the randomness of R we have that |β(cid:48)
r−p · (M (cid:48)TM (cid:48))−1
c(cid:48)
α(cid:107)ζζζ (cid:48)(cid:107)
j,j . So technically, to give a (1 − α)-
conﬁdence interval around β(cid:48)
j that contains βj w.p. ≥ 1 − α, we
need to use cα/2 and c(cid:48)
α resp. To avoid
overburdening the reader with what we already see as too many
parameters, we switch to asymptotic notation.

α/2 instead of cα and c(cid:48)

1 − ν over the randomness of R we have (r −
j,j = Θ (cid:0)(w2Ip×p + X TX)−1
p)(M (cid:48)TM )−1
r−p =
Θ( w2+w2(cid:107)ˆβββ(cid:107)2+(cid:107)ζζζ(cid:107)2

(cid:1) and (cid:107)ζζζ(cid:48)(cid:107)2

).

j,j

r

Proof. The former bound follows from known results on
the Johnson-Lindenstrauss transform (as were shown in the
proof of Claim B.6). The latter bound follows from stan-
dard concentration bounds of the χ2-distribution.

Plugging in the result of Proposition C.3 to Equation (11)
we get that w.p. ≥ 1 − ν the difference |β(cid:48)
j − βj| is at most

(cid:16)

O

cα

√

(cid:113)

(cid:107)ζζζ(cid:107)
n − p
(cid:115)

(X TX)−1
j,j

+ c(cid:48)
α

w2 + w2(cid:107)ˆβββ(cid:107)2 + (cid:107)ζζζ(cid:107)2
r − p

(cid:113)

(w2Ip×p + X TX)−1
j,j

(cid:17)

(12)

We will also use the following proposition.

Proposition C.4.

(cid:18)

(X TX)−1

j,j ≤

1 +

(cid:19)

w2
σmin(X TX)

(w2Ip×p + X TX)−1
j,j

Proof. We have that

(X TX)−1
= (X TX)−1(X TX + w2Ip×p)(X TX + w2Ip×p)−1
= (X TX + w2Ip×p)−1 + w2(X TX)−1(X TX + w2Ip×p)−1
= (Ip×p + w2(X TX)−1)(X TX + w2Ip×p)−1
= (X TX + w2Ip×p)−1/2·

(Ip×p + w2(X TX)−1)·

(X TX + w2Ip×p)−1/2

where the latter holds because (Ip×p + w2(X TX)−1) and
(X TX + w2Ip×p)−1 are diagonalizable by the same ma-
trix V (the same matrix for which (X TX) = V S−1V T).
Since we have (cid:107)Ip×p + w2(X TX)−1(cid:107) = 1 + w2
min(X) , it is
σ2
clear that (Ip×p + w2(X TX)−1) (cid:22) (1 + w2
min(X) )Ip×p.
σ2
We deduce that (X TX)−1
j,j = eeeT
j (X TX)−1eeej ≤ (1 +
min(X) )(X TX + w2Ip×p)−1
j,j .

w2

σ2

Based on Proposition C.4 we get from Equation (12) that

Differentially Private Ordinary Least Squares

|β(cid:48)

j − βj| is at most
(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:107)ζζζ(cid:107)2(1 +

(cid:16)

O(

cα

w2
σmin(X TX) )

+

n − p

(cid:115)

c(cid:48)
α

w2 + w2(cid:107)ˆβββ(cid:107)2 + (cid:107)ζζζ(cid:107)2
r − p

(cid:17)(cid:113)

Proof. Based on the above discussion, we aim to show that
in the homoscedastic model (where each coordinate ei ∼
N (0, σ2) independently) w.p. ≥ 1 − ν it holds that the
magnitude of βj is greater than

(w2Ip×p + X TX)−1
j,j )

c(cid:48)
α(1 + η)

(cid:115)

w2 + w2(cid:107)ˆβββ(cid:107)2 + (cid:107)ζζζ(cid:107)2
r − p

(cid:113)

(w2Ip×p + X TX)−1
j,j

(13)

(14)
∈

And so, if it happens to be the case that exists some small
η > 0 for which ˆβββ, ζζζ and w2 satisfy
w2
σmin(X TX) )

(cid:107)ζζζ(cid:107)2(1 +

(cid:32)

(cid:33)

≤ η2

w2 + w2(cid:107)ˆβββ(cid:107)2 + (cid:107)ζζζ(cid:107)2
r − p

n − p

we
then
(cid:16)
j ± O((1 + η) · c(cid:48)
β(cid:48)

1 − α.19

(cid:113) r

have
α(cid:107)ζζζ (cid:48)(cid:107)
Moreover,
(cid:113) w2+w2(cid:107)ˆβββ(cid:107)2+(cid:107)ζζζ(cid:107)2
r−p

Pr[βj
that
(cid:17)
r−p · (M (cid:48)TM (cid:48))−1
j,j )
in this case |βj| >
if
(w2Ip×p + X TX)−1
j,j
j) = sign(βj)] ≥ 1 − α. This is precisely

(cid:113)

≥

]

c(cid:48)
α(1 + η)
then Pr[sign(β(cid:48)
what Claims C.5 and C.6 below do.
If
Claim C.5.
s.t.
(cid:18)

n − p
r3/2 · B2 ln(1/δ)

Ω

(cid:15)

and n2

>

η

there
≥

exists
2
η2 (r − p)
1
1
η2σmin(
n X TX)
(cid:17)
(cid:113) r
r−p · (M (cid:48)TM (cid:48))−1
α(cid:107)ζζζ (cid:48)(cid:107)
j,j )

(cid:19)

·

,

]

then Pr[βj

0
=

∈

≥

(cid:16)

β(cid:48)
j ± O((1 + η) · c(cid:48)
1 − α.

Proof. Based on the above discussion, it is enough to ar-
gue that under the conditions of the claim, the constraint
of Equation (14) holds. Since we require η2
n−p then
it is evident that (cid:107)ζζζ(cid:107)2
2(r−p) . So we now show that
(cid:107)ζζζ(cid:107)2
w2
n−p ·
2(r−p) under the conditions of the
claim, and this will show the required. All that is left is
some algebraic manipulations. It sufﬁces to have:

σmin(X TX) ≤ η2(cid:107)ζζζ(cid:107)2

n−p ≤ η2(cid:107)ζζζ(cid:107)2

2 ≥ r−p

η2
2 · n−p

r−p σmin(X TX) ≥ η2

r σmin( 1

n X TX)

2 · n2
32B2√

r ln(8/δ)
(cid:15)
which holds for n2 ≥ r3/2 · 64B2 ln(1/δ)
as we assume to hold.

σmin( 1

(cid:15)η2

≥

≥ w2

n X TX)−1,

Claim C.6. Fix ν ∈ (0, 1
(ii) (cid:107)βββ(cid:107)2 = Ω(σ2(cid:107)X +(cid:107)2

(cid:18)

Ω

(c(cid:48)

α)2(1+η)2
β2
j

(cid:18)

1 + (cid:107)βββ(cid:107)2 +

2 ). If (i) n = p + Ω(ln(1/ν)),
F ln( p
ν )) and (iii) r − p =
(cid:19)(cid:19)
, then in the

σ2
1
n X TX)

σmin(

homoscedastic model, with probability ≥ 1−ν −α we have
that sign(βj) = sign(β(cid:48)

j).

19We assume n ≥ r so cα < c(cid:48)

α as the Tn−p-distribution is

closer to a normal Gaussian than the Tr−p-distribution.

To show this, we invoke Claim A.4 to argue that w.p. ≥
1 − ν we have (i) (cid:107)ζζζ(cid:107)2 ≤ 2σ2(n − p) (since n = p +
Ω(ln(1/ν))), and (ii) (cid:107)ˆβββ(cid:107)2 ≤ 2(cid:107)βββ(cid:107)2 (since (cid:107)βββ − ˆβββ(cid:107)2 ≤
F ln( p
σ2(cid:107)X +(cid:107)2
ν ) whereas (cid:107)βββ(cid:107)2 = Ω(σ2(cid:107)X +(cid:107)2
ν ))).
We also use the fact that (w2Ip×p + X TX)−1
j,j ≤ (w2 +
σ−1
min(X TX)), and then deduce that

F ln( p

(cid:115)

(1 + η)c(cid:48)
α

w2 + w2(cid:107)ˆβββ(cid:107)2 + (cid:107)ζζζ(cid:107)2
r − p

(cid:113)

(w2Ip×p + X TX)−1
j,j

(cid:115)

(cid:115)

≤

≤

(1 + η)c(cid:48)
α
√
r − p

(1 + η)c(cid:48)
α
√
r − p

2

w2(1 + (cid:107)βββ(cid:107)2) + σ2(n − p)
w2 + σmin(X TX)

2(1 + (cid:107)βββ(cid:107)2) +

2σ2(n − p)
σmin(X TX)

≤ |βj|

due to our requirement on r − p.

Observe, out of the 3 conditions speciﬁed in Claim C.6,
condition (i) merely guarantees that the sample is large
enough to argue that estimations are close to their expect
value; and condition (ii) is there merely to guarantee that
(cid:107)ˆβββ(cid:107) ≈ (cid:107)βββ(cid:107).
It is condition (iii) which is non-trivial to
hold, especially together with the conditions of Claim C.5
that pose other constraints in regards to r, n, η and the var-
ious other parameters in play. It is interesting to compare
the requirements on r to the lower bound we get in The-
orem 3.3 — especially the latter bound. The two bounds
are strikingly similar, with the exception that here we also
require r − p to be greater than 1+(cid:107)βββ(cid:107)2
. This is part of the
unfortunate effect of altering the matrix A: we cannot give
conﬁdence bounds only for the coordinates j for which β2
j
is very small relative to (cid:107)βββ(cid:107)2.

β2
j

In summary, we require to have n = p + Ω(ln(1/ν)) and
that X contains enough sample points to have (cid:107)ˆβββ(cid:107) compa-
rable to (cid:107)βββ(cid:107), and then set r and η such that (it is convenient
to think of η as a small constant, say, η = 0.1)

• r − p = O(η2(n − p)) (which implies r = O(n))

(cid:16)

• r = O(

η2

(cid:15)n2

B2 ln(1/δ) σmin( 1

(cid:17)
n X TX)

2
3 )

• r − p = Ω( 1+(cid:107)βββ(cid:107)2

+ σ2
β2
j

· σ−1

min( 1

n X TX))

β2
j

Differentially Private Ordinary Least Squares

to have that the (1 − α)-conﬁdence interval around β(cid:48)
j
does not intersect the origin. Once again, we comment
that these conditions are sufﬁcient but not necessary, and
furthermore — even with these conditions holding — we
do not make any claims of optimality of our conﬁdence
bound. That is because from Proposition C.4 onwards our
discussion uses upper bounds that do not have correspond-
ing lower bounds, to the best of our knowledge.

D. Conﬁdence Intervals for “Analyze Gauss”

Algorithm

To complete the picture, we now analyze the “Analyze
Gauss” algorithm of Dwork et al (Dwork et al., 2014).
Algorithm 2 works by adding random Gaussian noise to
ATA, where the noise is symmetric with each coordi-
nate above the diagonal sampled i.i.d from N (0, ∆2) with
∆2 = O
.20 Using the same notation for a
sub-matrix of A as [X; yyy] as before, with X ∈ Rn×p and
y ∈ Rn, we denote the output of Algorithm 2 as


B4 log(1/δ)



(cid:16)

(cid:17)

(cid:15)2






(cid:94)
X TX (cid:103)X Tyyy

(cid:103)yyyTX (cid:103)yyyTyyy












=

X TX + N

X Tyyy + nnn

yyyTX + nnnT

yyyTyyy + m







(15)
where N is a symmetric p × p-matrix, nnn is a p-dimensional
vector and m is a scalar, whose coordinates are sampled
i.i.d from N (0, ∆2).

Using the output of Algorithm 2, it is simple to derive ana-
logues of ˆβββ and (cid:107)ζζζ(cid:107)2 (Equations (1) and (2))

(cid:16)(cid:94)
X TX

(cid:17)−1

(cid:101)βββ =

(cid:103)X Tyyy = (cid:0)X TX + N (cid:1)−1

(X T yyy + nnn)

(cid:103)(cid:107)ζζζ(cid:107)2 = (cid:103)yyyTyyy − 2 (cid:93)yyyT X (cid:101)βββ + (cid:101)βββ
(cid:94)
= (cid:103)yyyT yyy − (cid:93)yyyT X
X TX

T (cid:94)

X TX (cid:101)βββ
−1 (cid:93)X T yyy

(16)

(17)

We now argue that it is possible to use (cid:101)βj and (cid:103)(cid:107)ζζζ(cid:107)2 to get a
conﬁdence interval for βj under certain conditions.
Theorem D.1. Fix α, ν ∈ (0, 1
2 ). Assume that there exists
2 ) s.t. σmin(X TX) > ∆(cid:112)p ln(1/ν)/η. Under the
η ∈ (0, 1
homoscedastic model, given βββ and σ2, if we assume also
that (cid:107)βββ(cid:107) ≤ B and (cid:107)ˆβββ(cid:107) = (cid:107)(X TX)−1X Tyyy(cid:107) ≤ B, then
w.p. ≥ 1 − α − ν it holds that |βj − (cid:101)βj| it at most

(cid:16)

O

ρ ·

(cid:115)(cid:18)(cid:94)
X TX

−1

j,j + ∆(cid:112)p ln(1/ν) ·

(cid:94)
X TX

j,j

−2

(cid:19)

ln(1/α)

(cid:114)

+ ∆

(cid:94)
X TX

−2
j,j · ln(1/ν) · (B

√

(cid:17)

p + 1)

where ρ is such that ρ2 is w.h.p an upper bound on σ2,
deﬁned as

ρ2 def=

(cid:18)

√

ln(4/α)

1
√
n−p−2
(cid:18)
∆ B2√

p
1−η

(cid:18)

(cid:103)(cid:107)ζζζ(cid:107)2 − C ·

(cid:19)2

·

for some large constant C.

(cid:112)ln(1/ν) + ∆2(cid:107)

(cid:94)
X TX

−1

(cid:107)F · ln(p/ν)

(cid:19)(cid:19)

We comment that in practice, instead of using ρ, it might
be better to use the MLE of σ2, namely:

σ2 def= 1
n−p

(cid:18)

(cid:103)(cid:107)ζζζ(cid:107)2 + ∆2(cid:107)

(cid:94)
X TX

−1

(cid:19)

(cid:107)F

instead of ρ2, the upper bound we derived for σ2. (Replac-
ing an unknown variable with its MLE estimator is a com-
mon approach in applied statistics.) Note that the assump-
tion that (cid:107)βββ(cid:107) ≤ B is fairly benign once we assume each
row has bounded l2-norm. The assumption (cid:107)ˆβββ(cid:107) ≤ B sim-
ply assumes that ˆβββ is a reasonable estimation of βββ, which is
likely to hold if we assume that X TX is well-spread. The
assumption about the magnitude of the least singular value
of X TX is therefore the major one. Nonetheless, in the
case we considered before where each row in X is sampled
i.i.d from N (000, Σ), this assumption merely means that n is
large enough s.t. n = ˜Ω(

√
∆
p ln(1/ν)
η·σmin(Σ) ).

In order to prove Theorem D.1, we require the following
proposition.
Proposition D.2. Fix any ν ∈ (0, 1
2 ). Fix any matrix
M ∈ Rp×p. Let vvv ∈ Rp be a vector with each coordinate
sampled independently from a Gaussian N (0, ∆2). Then
(cid:107)Mvvv(cid:107) > ∆ · (cid:107)M (cid:107)F
we have that Pr
< ν.

(cid:105)
(cid:112)2 ln(2p/ν)

(cid:104)

Proof. Given M , we have that Mvvv ∼ N (000, ∆2 · M M T).
Denoting M ’s singular values as sv1, . . . , svp, we can ro-
tate Mvvv without affecting its l2-norm and infer that (cid:107)Mvvv|2
is distributed like a sum on p independent Gaussians, each
sampled from N (0, ∆2 · sv2
i ). Standard union bound gives
that w.p. ≥ 1 − ν non of the p Gaussians exceeds its stan-
dard deviation by a factor of (cid:112)2 ln(2p/ν). Hence, w.p.
≥ 1 − ν it holds that (cid:107)Mvvv(cid:107)2 ≤ 2∆2 (cid:80)
i ln(2p/ν) =
2∆2 · trace(M M T) · ln(2p/ν).

i sv2

20It is easy to see that the l2-global sensitivity of the mapping
A (cid:55)→ ATA is ∝ B4. Fix any A1, A2 that differ on one row
which is some vector vvv with (cid:107)vvv(cid:107) = B in A1 and the all zero
vector in A2. Then GS2
2 = (cid:107)AT
F =
trace(vvvvvvT · vvvvvvT) = (vvvTvvv)2 = B4.

F = (cid:107)vvvvvvT (cid:107)2

1 A1 − AT

2 A2(cid:107)2

Our proof also requires the use of the following equality,
that holds for any invertible A and any matrix B s.t. I +
B · A−1 is invertible:

(A + B)−1 = A−1 − A−1 (cid:0)I + BA−1(cid:1)−1

BA−1

Differentially Private Ordinary Least Squares

In our case, we have

−1

(cid:94)
X TX
= (X TX + N )−1
= (X TX)−1 − (X TX)−1(cid:0)I + N (X TX)−1(cid:1)−1
= (X TX)−1 (cid:16)
def= (X TX)−1 (cid:0)I − Z · (X TX)−1(cid:1)

I − (cid:0)I + N (X TX)−1(cid:1)−1

we

σ2

(cid:32)

can bound the variance of
2(cid:33)
.

−1
j,j + (cid:107)N (cid:107) ·

(cid:94)
X TX

(cid:94)
X TX

j→

−1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:94)
X TX

−1
j→X Teee by

Appealing to

N(X TX)−1

N (X TX)−1(cid:17)



(cid:32)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

O



Gaussian concentration bounds, we have that w.p.
≥ 1 − α/2 the absolute value of this Gaussian is at most

−1

(cid:94)
X TX

j,j + ∆(cid:112)p ln(1/ν) ·

(cid:94)
X TX

−1

j→

σ2 ln(1/α)

2(cid:33)

(cid:13)
(cid:13)
(cid:13)
(cid:13)



.

(18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Proof of Theorem D.1. Fix ν > 0. First, we apply to stan-
dard results about Gaussian matrices, such as (Tao, 2012)
(used also by (Dwork et al., 2014) in their analysis), to see
that w.p. ≥ 1 − ν/6 we have (cid:107)N (cid:107) = O(∆(cid:112)p ln(1/ν)).
And so, for the remainder of the proof we ﬁx N subject to
having bounded operator norm. Note that by ﬁxing N we

(cid:94)
X TX.

ﬁx

To bound

(cid:94)
X TX

−1
j→nnn note that nnn ∼ N (000, ∆2I) is sam-
(cid:94)
X TX. We therefore have that
(cid:94)
X TX

). Gaussian concentra-

−1

j→

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

pled independently of
(cid:94)
X TX

−1
j→nnn ∼ N (0, ∆2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

tion bounds give that w.p ≥ 1−ν/6 we have |
(cid:112)ln(1/ν)

(cid:94)
X TX

∆

O

(cid:18)

(cid:19)

−1

.

j→

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:94)
X TX

−1
j→nnn| =

Recall that in the homoscedastic model, yyy = Xβββ + eee with
each coordinate of eee sampled i.i.d from N (0, σ2). We
therefore have that

Plugging this into our above bounds on all terms that appear
in Equation (19) we have that w.p. ≥ 1 − ν/2 − α/2 we
have that

(cid:12)
(cid:12)
(cid:12) (cid:101)βj − βj

(cid:12)
(cid:12)
(cid:12) is at most

(cid:101)βββ =

(cid:94)
X TX

(cid:94)
X TX

=

−1

−1

(X Tyyy + nnn) =

−1

(cid:94)
X TX

(

(cid:94)
X TX − N )βββ +
−1

(cid:94)
X TX
−1

= βββ −

(cid:94)
X TX

Nβββ +

(cid:94)
X TX

X Teee +

(cid:94)
X TX

(X TXβββ + X Teee + nnn)
−1

−1

X Teee +

nnn

O

(cid:94)
X TX
−1

nnn

Denoting the j-th row of

(cid:94)
X TX

−1

(cid:94)
X TX

as

−1
j→ we deduce:

(cid:101)βj = βj −

(cid:94)
X TX

−1
j→Nβββ +

(cid:94)
X TX

−1
j→X Teee +

(cid:94)
X TX

−1
j→nnn

(cid:19)
· B∆(cid:112)p ln(1/ν)

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:94)
X TX

−1

j→

(cid:13)
(cid:13)
(cid:13)
(cid:13)



(cid:32)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

+ O

σ

−1

(cid:94)
X TX

j,j + ∆(cid:112)p ln(1/ν) ·

(cid:94)
X TX

−1

j→

(cid:13)
(cid:13)
(cid:13)
(cid:13)





ln(1/α)

2(cid:33)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:18)

+ O

∆

(cid:94)
X TX

−1

j→

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:19)
(cid:112)ln(1/ν)

(19)

of

the

term

(cid:107)N (cid:107)(cid:107)βββ(cid:107)

=

Note that due to the symmetry of
(cid:13)
(cid:94)
(cid:13)
X TX
(cid:13)
(cid:13)

(cid:94)
X TX

j→

=

−1

−2
j,j (the (j, j)-coordinate of the ma-

(cid:94)
X TX we have

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
−2

(cid:94)
X TX

trix

), thus | (cid:101)βj − βj| is at most

We
(cid:94)
X TX
(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

O

na¨ıvely
−1
j→Nβββ

(cid:94)
X TX

−1

j→

(cid:13)
(cid:13)
(cid:13)
(cid:13)

by

bound

the
(cid:94)
X TX

(cid:13)
(cid:13)
(cid:13)
(cid:13)
· B∆(cid:112)p ln(1/ν)

size
(cid:13)
−1
(cid:13)
(cid:13)
(cid:13)

j→
(cid:19)

.

To

bound

(cid:94)
X TX

sen

independently

−1
j→X Teee
of

N (000, σ2I) we
−1

have

000, σ2 · eeeT
j

(cid:94)
X TX

· X TX ·

(cid:94)
X TX

eeej

.

note

that

eee

is

cho-

(cid:94)
X TX

and
−1
(cid:94)
j→X Teee
X TX
(cid:19)
−1

since

∼

Since

∼

(cid:18)

eee

N

we have

(cid:16)

O

σ ·

(cid:115)(cid:18)(cid:94)
X TX

−1

j,j + ∆(cid:112)p ln(1/ν) ·

(cid:94)
X TX

j,j

−2

(cid:19)

ln(1/α)

(cid:114)

+ ∆

(cid:94)
X TX

−2
j,j · ln(1/ν) · (B

√

p + 1)

(cid:17)

(20)

−1

(cid:94)
X TX

−1

(cid:94)
X TX

· X TX ·
−1

(cid:94)
X TX

=

−1

(cid:94)
X TX

=

· (

(cid:94)
X TX − N ) ·
−1

(cid:94)
X TX

−

· N ·

(cid:94)
X TX

−1

−1

(cid:94)
X TX

All of the terms appearing in Equation (20) are known
(cid:94)
X TX, except for σ — which is a parameter of the
given
model. Next, we derive an upper bound on σ which we can
then plug into Equation (20) to complete the proof of the
theorem and derive a conﬁdence interval for βj.

Differentially Private Ordinary Least Squares

Recall Equation (17), according to which we have

(cid:103)(cid:107)ζζζ(cid:107)2 = (cid:103)yyyT yyy − (cid:93)yyyT X
(18)= yyyTyyy + m

(cid:94)
X TX

−1 (cid:93)X T yyy

− (yyyTX + nnnT)(X TX)−1(I − Z · (X TX)−1)(X Tyyy + nnn)

= yyyTyyy + m

− yyyTX(X TX)−1X Tyyy
+ yyyTX(X TX)−1Z(X TX)−1X Tyyy
− 2yyyTX(X TX)−1nnn
+ 2yyyTX(X TX)−1Z(X TX)−1nnn
− nnnT(X TX)−1(I − Z · (X TX)−1)nnn

Recall that ˆβββ = (X TX)−1X Tyyy, and so we have

= yyyT (cid:0)I − X(X TX)−1X T(cid:1) yyy + m − ˆβββ
(I − Z(X TX)−1)nnn − nnnT(cid:94)
X TX

− 2ˆβββ

T

T

Z ˆβββ
−1

nnn

(21)

and of course, both nnn and m are chosen independently of
(cid:94)
X TX and yyy.

Before we bound each term in Equation (21), we ﬁrst give
a bound on (cid:107)Z(cid:107). Recall, Z = (cid:0)I + N (X TX)−1(cid:1)−1
N .
Recall our assumption (given in the statement of Theo-
(cid:112)p ln(1/ν). This im-
rem D.1) that σmin(X TX) ≥ ∆
η
plies that (cid:107)N (X TX)−1(cid:107) ≤ (cid:107)N (cid:107)·σmin(X TX)−1 = O(η).
Hence

(cid:107)Z(cid:107) ≤ ((cid:107)I +N (X TX)−1(cid:107))−1 ·(cid:107)N (cid:107) = O

√

(cid:18)

∆

(cid:19)

p ln(1/ν)
1−η

(cid:17)

(cid:16) η
1−η

Moreover, this implies that (cid:107)Z(X TX)−1(cid:107) ≤ O

and that (cid:107)I − Z(X TX)−1(cid:107) ≤ O

(cid:16) 1
1−η

(cid:17)

.

Armed with these bounds on the operator norms of Z and
(I − Z(X TX)−1) we bound the magnitude of the different
terms in Equation (21).

• The term yyyT (I − XX +) yyy is the exact term from
the standard OLS, and we know it is distributed like
σ2 · χ2
n−p distribution. Therefore, it is greater than
√
n − p − 2(cid:112)ln(4/α))2 w.p. ≥ 1 − α/2.
σ2(
• The scalar m sampled from m ∼ N (0, ∆2) is

bounded by O(∆(cid:112)ln(1/ν)) w.p. ≥ 1 − ν/8.

• Since we assume (cid:107)ˆβββ(cid:107) ≤ B, the term ˆβββ

T

Z ˆβββ is upper
(cid:19)

(cid:18)

√

B2∆

p ln(1/ν)

1−η

.

bounded by B2(cid:107)Z(cid:107) = O

• Denote zzzTnnn = 2ˆβββ

(I − Z(X TX)−1)nnn. We thus have
that zzzTnnn ∼ N (0, ∆2(cid:107)zzz(cid:107)2) and that its magnitude is at

T

most O(∆ · (cid:107)zzz(cid:107)(cid:112)ln(1/ν)) w.p. ≥ 1 − ν/8. We can
upper bound (cid:107)zzz(cid:107) ≤ 2(cid:107)ˆβββ(cid:107) (cid:107)I − Z(X TX)−1(cid:107) =
O( B
1−η ), and so this term’s magnitude is upper

bounded by O

(cid:18)

∆·B

ln(1/ν)

√

1−η

(cid:19)
.

• Given our assumption about the least singular value
of X TX and with the bound on (cid:107)N (cid:107), we have that
(cid:94)
X TX) ≥ σmin(X TX) − (cid:107)N (cid:107) > 0 and so
σmin(
(cid:94)
X TX is a PSD. Therefore,
the symmetric matrix
the term nnnT(cid:94)
nnn(cid:107)2 is strictly
X TX
positive. Applying Proposition D.2 we have that
w.p. ≥ 1 − ν/8 it holds that nnnT(cid:94)
X TX

(cid:94)
X TX

nnn = (cid:107)

nnn ≤

−1/2

−1

−1

(cid:18)

O

∆2(cid:107)

(cid:94)
X TX

−1

(cid:107)F · ln(p/ν)

(cid:19)
.

Plugging all of the above bounds into Equation (21) we get
that w.p. ≥ 1 − ν/2 − α/2 it holds that

(cid:18)

σ2 ≤

√

(cid:18)

(cid:103)(cid:107)ζζζ(cid:107)2 + O

ln(4/α)

1
√
n−p−2
(cid:18)
(1 + B2√

1−η

(cid:19)2

·

p+B

)∆(cid:112)ln(1/ν) + ∆2(cid:107)

(cid:94)
X TX

−1

(cid:107)F · ln(p/ν)

(cid:19)(cid:19)

and indeed, the RHS is the deﬁnition of ρ2 in the statement
of Theorem D.1.

E. Experiment: Additional Figures

To complete our discussion about the experiments we have
conducted, we attach here additional ﬁgures, plotting both
the t-value approximations we get from both algorithms,
and the “high-level decision” of whether correctly reject or
not-reject the null hypothesis (and with what sign). First,
we show the distribution of the t-value approximation for
coordinates that should be rejected, in Figure 2, and then
the decision of whether to reject or not based on this t-value
— and whether it was right, conservative (we didn’t reject
while we needed to) or wrong (we rejected with the wrong
sign, or rejected when we shouldn’t have rejected) in Fig-
ure 3. As one can see, Algorithm 1 has far lower t-values
(as expected) and therefore is much more conservative. In
fact, it tends to not-reject coordinate 1 of the real-data even
on the largest value of n (Figure 3c).

However, because Algorithm 1 also has much smaller
variance, it also does not reject when it ought to not-
reject, whereas Algorithm 2 erroneiously rejects the null-
hypotheses. This can be seen in Figures 4 and 5.

Differentially Private Ordinary Least Squares

(a) Synthetic data, coordinate β1 = 0.5

(a) Synthetic data, coordinate β1 = 0.5

(b) Synthetic data, coordinate β2 = −0.25

(b) Synthetic data, coordinate β2 = −0.25

(c) real-life data, coordinate β1 = 14.07

(c) real-life data, coordinate β1 = 14.07

Figure 2. The distribution of the t-value approximations from se-
lected experiments on synthetic and real-life data where the null
hypothesis should be rejected

Figure 3. The correctness of our decision to reject the null-
hypothesis based on the approximated t-value where the null hy-
pothesis should be rejected

Differentially Private Ordinary Least Squares

(a) Synthetic data, coordinate β3 = 0

(a) Synthetic data, coordinate β3 = 0

(b) real-life data, coordinate β2 = 0.57

(b) Synthetic data, coordinate β2 = 0.57

Figure 4. The distribution of the t-value approximations from se-
lected experiments on synthetic and real-life data when the null
hypothesis is (essentially) true

Figure 5. The correctness of our decision to reject the null-
hypothesis based on the approximated t-value when the null hy-
pothesis is (essentially) true

