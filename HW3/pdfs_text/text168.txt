Language Modeling with Gated Convolutional Networks

Yann N. Dauphin 1 Angela Fan 1 Michael Auli 1 David Grangier 1

Abstract

The pre-dominant approach to language mod-
eling to date is based on recurrent neural net-
works. Their success on this task is often linked
to their ability to capture unbounded context.
In this paper we develop a ﬁnite context ap-
proach through stacked convolutions, which can
be more efﬁcient since they allow paralleliza-
tion over sequential tokens. We propose a novel
simpliﬁed gating mechanism that outperforms
Oord et al. (2016b) and investigate the impact
of key architectural decisions. The proposed ap-
proach achieves state-of-the-art on the WikiText-
103 benchmark, even though it features long-
term dependencies, as well as competitive re-
sults on the Google Billion Words benchmark.
Our model reduces the latency to score a sen-
tence by an order of magnitude compared to a
recurrent baseline. To our knowledge, this is the
ﬁrst time a non-recurrent approach is competitive
with strong recurrent models on these large scale
language tasks.

1. Introduction

Statistical language models estimate the probability distri-
bution of a sequence of words by modeling the probability
of the next word given preceding words, i.e.

P (w0, . . . , wN ) = P (w0)

P (wi|w0, . . . , wi−1),

N
(cid:89)

i=1

where wi are discrete word indices in a vocabulary. Lan-
guage models are a critical part of systems for speech
recognition (Yu & Deng, 2014) and machine translation
(Koehn, 2010).

Recently, neural networks (Bengio et al., 2003; Mikolov
et al., 2010; Jozefowicz et al., 2016) have been shown to

1Facebook AI Research. Correspondence to: Yann N. Dauphin

<ynd@fb.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

outperform classical n-gram language models (Kneser &
Ney, 1995; Chen & Goodman, 1996). These classical mod-
els suffer from data sparsity, which makes it difﬁcult to rep-
resent large contexts and thus, long-range dependencies.
Neural language models tackle this issue by embedding
words in continuous space over which a neural network is
applied. The current state of the art for language model-
ing is based on long short term memory networks (LSTM;
Hochreiter et al., 1997) which can theoretically model ar-
bitrarily long dependencies.

In this paper, we introduce new gated convolutional net-
works and apply them to language modeling. Convolu-
tional networks can be stacked to represent large context
sizes and extract hierarchical features over larger and larger
contexts with more abstractive features (LeCun & Bengio,
1995). This allows them to model long-term dependen-
cies by applying O( N
k ) operations over a context of size N
and kernel width k. In contrast, recurrent networks view
the input as a chain structure and therefore require a linear
number O(N ) of operations.

Analyzing the input hierarchically bears resemblance to
classical grammar formalisms which build syntactic tree
structures of increasing granuality, e.g., sentences consist
of noun phrases and verb phrases each comprising further
internal structure (Manning & Sch¨utze, 1999; Steedman,
2002). Hierarchical structure also eases learning since the
number of non-linearities for a given context size is reduced
compared to a chain structure, thereby mitigating the van-
ishing gradient problem (Glorot & Bengio, 2010).

Modern hardware is well suited to models that are highly
parallelizable. In recurrent networks, the next output de-
pends on the previous hidden state which does not enable
parallelization over the elements of a sequence. Convolu-
tional networks, however, are very amenable to this com-
puting paradigm since the computation of all input words
can be performed simultaneously (§2).

Gating has been shown to be essential for recurrent neural
networks to reach state-of-the-art performance (Jozefow-
icz et al., 2016). Our gated linear units reduce the vanish-
ing gradient problem for deep architectures by providing a
linear path for the gradients while retaining non-linear ca-
pabilities (§5.2).

Language Modeling with Gated Convolutional Networks

We show that gated convolutional networks outperform
other recently published language models such as LSTMs
trained in a similar setting on the Google Billion Word
Benchmark (Chelba et al., 2013). We also evaluate the abil-
ity of our models to deal with long-range dependencies on
the WikiText-103 benchmark for which the model is con-
ditioned on an entire paragraph rather than a single sen-
tence and we achieve a new state-of-the-art on this dataset
(Merity et al., 2016). Finally, we show that gated linear
units achieve higher accuracy and converge faster than the
LSTM-style gating of Oord et al. (2016; §4, §5).

2. Approach

In this paper we introduce a new neural language model
that replaces the recurrent connections typically used in re-
current networks with gated temporal convolutions. Neu-
ral language models (Bengio et al., 2003) produce a repre-
sentation H = [h0, . . . , hN ] of the context for each word
w0, . . . , wN to predict the next word P (wi|hi). Recurrent
neural networks f compute H through a recurrent function
hi = f (hi−1, wi−1) which is an inherently sequential pro-
cess that cannot be parallelized over i.1

Our proposed approach convolves the inputs with a func-
tion f to obtain H = f ∗ w and therefore has no tempo-
ral dependencies, so it is easier to parallelize over the in-
dividual words of a sentence. This process will compute
each context as a function of a number of preceding words.
Compared to recurrent networks, the context size is ﬁnite
but we will demonstrate both that inﬁnite contexts are not
necessary and our models can represent large enough con-
texts to perform well in practice (§5).

Figure 1 illustrates the model architecture. Words are rep-
resented by a vector embedding stored in a lookup table
D|V|×e where |V| is the number of words in the vocabulary
and e is the embedding size. The input to our model is a
sequence of words w0, . . . , wN which are represented by
word embeddings E = [Dw0, . . . , DwN ]. We compute the
hidden layers h0, . . . , hL as

hl(X) = (X ∗ W + b) ⊗ σ(X ∗ V + c)

(1)

where m, n are respectively the number of input and output
feature maps and k is the patch size, X ∈ RN ×m is the
input of layer hl (either word embeddings or the outputs of
previous layers), W ∈ Rk×m×n, b ∈ Rn, V ∈ Rk×m×n,
c ∈ Rn are learned parameters, σ is the sigmoid function
and ⊗ is the element-wise product between matrices.

When convolving inputs, we take care that hi does not
contain information from future words. We address this
by shifting the convolutional inputs to prevent the kernels

1Parallelization is usually done over multiple sequences in-

stead.

Figure 1. Architecture of the gated convolutional network for lan-
guage modeling.

from seeing future context (Oord et al., 2016a). Speciﬁ-
cally, we zero-pad the beginning of the sequence with k − 1
elements, assuming the ﬁrst input element is the beginning
of sequence marker which we do not predict and k is the
width of the kernel.

The output of each layer is a linear projection X ∗ W + b
modulated by the gates σ(X ∗ V + c). Similar to LSTMs,
these gates multiply each element of the matrix X ∗ W + b
and control the information passed on in the hierarchy.
We dub this gating mechanism Gated Linear Units (GLU).
Stacking multiple layers on top of the input E gives a repre-
sentation of the context for each word H = hL◦. . .◦h0(E).
We wrap the convolution and the gated linear unit in a pre-
activation residual block that adds the input of the block to

Input sentenceTextThe    cat    sat    on    the    mat    .w0     w1     w2     w3     w4     w5     w6Lookup TableE = DwiConvolutionA = E∗W + bGatingH0 = A⊗σ(B) σSoftmaxY = softmax(WHL)B = E∗V + cStack L - 1 Convolution+Gating BlocksLanguage Modeling with Gated Convolutional Networks

the output (He et al., 2015a). The blocks have a bottleneck
structure for computational efﬁciency and each block has
up to 5 layers.

The simplest choice to obtain model predictions is to use
a softmax layer, but this choice is often computationally
inefﬁcient for large vocabularies and approximations such
as noise contrastive estimation (Gutmann & Hyv¨arinen)
or hierarchical softmax (Morin & Bengio, 2005) are pre-
ferred. We choose an improvement of the latter known as
adaptive softmax which assigns higher capacity to very fre-
quent words and lower capacity to rare words (Grave et al.,
2016a). This results in lower memory requirements as well
as faster computation at both training and test time.

3. Gating Mechanisms

Gating mechanisms control the path through which infor-
mation ﬂows in the network and have proven to be use-
ful for recurrent neural networks (Hochreiter & Schmidhu-
ber, 1997). LSTMs enable long-term memory via a sep-
arate cell controlled by input and forget gates. This al-
lows information to ﬂow unimpeded through potentially
many timesteps. Without these gates, information could
easily vanish through the transformations of each timestep.
In contrast, convolutional networks do not suffer from the
same kind of vanishing gradient and we ﬁnd experimentally
that they do not require forget gates.

Therefore, we consider models possessing solely output
gates, which allow the network to control what informa-
tion should be propagated through the hierarchy of lay-
ers. We show this mechanism to be useful for language
modeling as it allows the model to select which words or
features are relevant for predicting the next word. Par-
allel to our work, Oord et al. (2016b) have shown the
effectiveness of an LSTM-style mechanism of the form
tanh(X∗W+b)⊗σ(X∗V+c) for the convolutional mod-
eling of images. Later, Kalchbrenner et al. (2016) extended
this mechanism with additional gates for use in translation
and character-level language modeling.

Gated linear units are a simpliﬁed gating mechanism based
on the work of Dauphin & Grangier (2015) for non-
deterministic gates that reduce the vanishing gradient prob-
lem by having linear units coupled to the gates. This retains
the non-linear capabilities of the layer while allowing the
gradient to propagate through the linear unit without scal-
ing. The gradient of the LSTM-style gating of which we
dub gated tanh unit (GTU) is

∇[tanh(X) ⊗ σ(X)] = tanh(cid:48)(X)∇X ⊗ σ(X)
+σ(cid:48)(X)∇X ⊗ tanh(X).

(2)

Notice that it gradually vanishes as we stack layers because
of the downscaling factors tanh(cid:48)(X) and σ(cid:48)(X). In con-

trast, the gradient of the gated linear unit

∇[X ⊗ σ(X)] = ∇X ⊗ σ(X) + X ⊗ σ(cid:48)(X)∇X (3)

has a path ∇X ⊗ σ(X) without downscaling for the ac-
tivated gating units in σ(X). This can be thought of
as a multiplicative skip connection which helps gradients
ﬂow through the layers. We compare the different gating
schemes experimentally in Section §5.2 and we ﬁnd gated
linear units allow for faster convergence to better perplexi-
ties.

4. Experimental Setup

4.1. Datasets

We report results on two public large-scale language mod-
eling datasets. First, the Google Billion Word dataset
(Chelba et al., 2013) is considered one of the largest lan-
guage modeling datasets with almost one billion tokens and
a vocabulary of over 800K words. In this dataset, words
appearing less than 3 times are replaced with a special un-
known symbol. The data is based on an English corpus
of 30, 301, 028 sentences whose order has been shufﬂed.
Second, WikiText-103 is a smaller dataset of over 100M
tokens with a vocabulary of about 200K words (Merity
et al., 2016). Different from GBW, the sentences are con-
secutive which allows models to condition on larger con-
texts rather than single sentences. For both datasets, we
add a beginning of sequence marker <S > at the start of
each line and an end of sequence marker </S> at the end
of each line. On the Google Billion Word corpus each
sequence is a single sentence, while on WikiText-103 a
sequence is an entire paragraph. The model sees <S>
and </S > as input but only predicts the end of sequence
marker </S>. We evaluate models by computing the per-
i − log p(wi|...,wi−1) on the standard held out
plexity e
test portion of each dataset.

(cid:80)N

1
N

4.2. Training

We implement our models in Torch (Collobert et al., 2011)
and train on Tesla M40 GPUs. The majority of our models
are trained on single GPU, as we focused on identifying
compact architectures with good generalization and efﬁ-
cient computation at test time. We trained larger models
with an 8-GPU setup by copying the model onto each GPU
and dividing the batch such that each worker computes
1/8th of the gradients. The gradients are then summed us-
ing Nvidia NCCL. The multi-GPU setup allowed us to train
models with larger hidden units.

We train using Nesterov’s momentum (Sutskever et al.,
2013). While the cost in terms of memory is storing an-
other vector of the size of the parameters, it increases the
speed of convergence signiﬁcantly with minimal additional

Language Modeling with Gated Convolutional Networks

GCNN-13

GCNN-14B

GCNN-9

GCNN-8B

GCNN-8

GCNN-14

[4, 1268] × 1

(cid:34)

4, 1268
4, 1268

(cid:35)

× 12

Google Billion Word
128

[4, 807] × 1

(cid:34)

4, 807
4, 807

(cid:35)

× 4

wikitext-103
280

[4, 900] × 1

[6, 850] × 3

[4, 900] × 7

[1, 850] × 1








[5, 512] × 1
1, 128
5, 128
1, 512


 × 3
















1, 512
5, 512
1, 1024

1, 1024
5, 1024
1, 2048

1, 1024
5, 1024
1, 4096








 × 3


 × 6


 × 1


















[1, 512] × 1
1, 128
5, 128
1, 512


 × 3


 × 3






 × 1

1, 256
5, 256
1, 512

1, 1024
1, 1024
1, 2048

[5, 850] × 4

[1, 850] × 1

[4, 850] × 3

[4, 1024] × 1
[4, 2048] × 1
10k,20k,200k

Name
Dataset
Lookup
Conv1

Conv2.x

Conv3.x

Conv4.x

Conv5.x

Conv6.x
Conv7.x
AdaSoftmax

10k,40k,200k

4k,40k,200k

2k,10k,50k

Table 1. Architectures for the models. The residual building blocks are shown in brackets with the format [k, n]. “B” denotes bottleneck
architectures.

computation compared to standard stochastic gradient de-
scent. The speed of convergence was further increased with
gradient clipping (Pascanu et al., 2013) and weight normal-
ization (Salimans & Kingma, 2016).

Pascanu et al. (2013) argue for gradient clipping because it
prevents the gradient explosion problem that characterizes
RNNs. However, gradient clipping is not tied to RNNs, as
it can be derived from the general concept of trust region
methods. Gradient clipping is found using a spherical trust
region

∆θ∗ = argmin
s. t. (cid:107)∆θ(cid:107)≤(cid:15)

f (θ) + ∇f T ∆θ

= − max((cid:107)∇f (cid:107), (cid:15))

(4)

∇f
(cid:107)∇f (cid:107)

.

Empirically, our experiments converge signiﬁcantly faster
with the use of gradient clipping even though we do not use
a recurrent architecture.

In combination, these methods led to stable and fast con-
vergence with comparatively large learning rates such as 1.

4.3. Hyper-parameters

We found good hyper-parameter conﬁgurations by cross-
validating with random search on a validation set. For
the number of residual
model architecture, we select
blocks between {1, . . . , 10},
the size of the embed-
dings with {128, . . . , 256}, the number of units between
{128, . . . , 2048}, and the kernel width between {3, . . . , 5}.

In general, ﬁnding a good architecture was simple and the
rule of thumb is that the larger the model, the better the per-
formance. In terms of optimization, we initialize the lay-
ers of the model with the Kaiming initialization (He et al.,
2015b), with the learning rate sampled uniformly in the
interval [1., 2.], the momentum set to 0.99, and clipping
set to 0.1. Good hyper-parameters for the optimizer are
quite straightforward to ﬁnd and the optimal values do not
change much between datasets.

5. Results

LSTMs and recurrent networks are able to capture long
term dependencies and are fast becoming cornerstones in
natural language processing. In this section, we compare
strong LSTM and RNN models from the literature to our
gated convolutional approach on two datasets.

We ﬁnd the GCNN outperforms the comparable LSTM re-
sults on Google billion words. To accurately compare these
approaches, we control for the same number of GPUs and
the adaptive softmax output model (Grave et al., 2016a), as
these variables have a signiﬁcant inﬂuence on performance.
In this setting, the GCNN reaches 38.1 test perplexity while
the comparable LSTM has 39.8 perplexity (Table 2).

Further, the GCNN obtains strong performance with much
greater computational efﬁciency. Figure 2 shows that our
approach closes the previously signiﬁcant gap between
models that use the full softmax and models with the usu-
ally less accurate hierarchical softmax. Thanks to the adap-

Language Modeling with Gated Convolutional Networks

Model
Sigmoid-RNN-2048 (Ji et al., 2015)
Interpolated KN 5-Gram (Chelba et al., 2013)
Sparse Non-Negative Matrix LM (Shazeer et al., 2014)
RNN-1024 + MaxEnt 9 Gram Features (Chelba et al., 2013)
LSTM-2048-512 (Jozefowicz et al., 2016)
2-layer LSTM-8192-1024 (Jozefowicz et al., 2016)
BIG GLSTM-G4 (Kuchaiev & Ginsburg, 2017)
LSTM-2048 (Grave et al., 2016a)
2-layer LSTM-2048 (Grave et al., 2016a)
GCNN-13
GCNN-14 Bottleneck

Test PPL Hardware
1 CPU
100 CPUs
-
24 GPUs
32 GPUs
32 GPUs
8 GPUs
1 GPU
1 GPU
1 GPU
8 GPUs

68.3
67.6
52.9
51.3
43.7
30.6
23.3∗
43.9
39.8
38.1
31.9

Table 2. Results on the Google Billion Word test set. The GCNN outperforms the LSTMs with the same output approximation.

Model
LSTM-1024 (Grave et al., 2016b)
GCNN-8
GCNN-14

Test PPL Hardware
1 GPU
1 GPU
4 GPUs

48.7
44.9
37.2

Table 3. Results for single models on the WikiText-103 dataset.

lion Word, the average sentence length is quite short —
only 20 words. We evaluate on WikiText-103 to determine
if the model can perform well on a dataset where much
larger contexts are available. On WikiText-103, an input se-
quence is an entire Wikipedia article instead of an individ-
ual sentence - increasing the average length to 4000 words.
However, the GCNN outperforms LSTMs on this problem
as well (Table 3). The GCNN-8 model has 8 layers with
800 units each and the LSTM has 1024 units. These results
show that GCNNs can model enough context to achieve
strong results.

We evaluated on the Gigaword dataset following Chen et al.
(2016) to compare with fully connected models. We found
that the fully connected and convolutional network reach
respectively 55.6 and 29.4 perplexity. We also ran pre-
liminary experiments on the much smaller Penn tree bank
dataset. When we score the sentences independently, the
GCNN and LSTM have comparable test perplexity with
108.7 and 109.3 respectively. However, it is possible to
achieve better results by conditioning on previous sen-
tences. Unlike the LSTM, we found that the GCNN over-
ﬁts on this quite small dataset and so we note the model is
better suited to larger scale problems.

5.1. Computational Efﬁciency

Computational cost is an important consideration for lan-
guage models. Depending on the application, there are a
number of metrics to consider. We measure the throughput

Figure 2. In comparison to the state-of-the-art (Jozefowicz et al.,
2016) which uses the full softmax, the adaptive softmax approxi-
mation greatly reduces the number of operations required to reach
a given perplexity.

tive softmax, the GCNN only requires a fraction of the op-
erations to reach the same perplexity values. The GCNN
outperforms other single model state-of-the-art approaches
except the much larger LSTM of Jozefowicz et al. (2016),
a model which requires more GPUs and the much more
computationally expensive full softmax.
In comparison,
the largest model we have trained reaches 31.9 test per-
plexity compared to the 30.6 of that approach, but only re-
quires training for 2 weeks on 8 GPUs compared to 3 weeks
of training on 32 GPUs for the LSTM. Note that these re-
sults can be improved by either using mixtures of experts
(Shazeer et al., 2017) or ensembles of these models.

Another relevant concern is if the GCNN’s ﬁxed context
size can thoroughly model long sequences. On Google Bil-

∗appeared after submission

02004006008001000MFlops303540455055Test PerplexityLSTM+SoftmaxGCNN+AdaSoftmaxLanguage Modeling with Gated Convolutional Networks

Figure 3. Learning curves on WikiText-103 (left) and Google Billion Word (right) for models with different activation mechanisms.
Models with gated linear units (GLU) converge faster and to a lower perplexity.

LSTM-2048
GCNN-9
GCNN-8 Bottleneck

Throughput

(CPU)
169
121
179

(GPU)
45,622
29,116
45,878

Responsiveness
(GPU)
2,282
29,116
45,878

Table 4. Processing speed in tokens/s at test time for an LSTM
with 2048 units and GCNNs achieving 43.9 perplexity on Google
Billion Word. The GCNN with bottlenecks improves the respon-
siveness by 20 times while maintaining high throughput.

of a model as the number of tokens that can be processed
per second. Throughput can be maximized by processing
many sentences in parallel to amortize sequential opera-
tions. In contrast, responsiveness is the speed of process-
ing the input sequentially, one token at a time. Through-
put is important because it indicates the time required to
process a corpus of text and responsiveness is an indicator
of the time to ﬁnish processing a sentence. A model can
have low responsiveness but high throughput by evaluating
many sentences simultaneously through batching. In this
case, such a model is slow in ﬁnishing processing individ-
ual sentences, but can process many sentences at a good
rate.

We evaluate the throughput and responsiveness for mod-
els that reach approximately 43.9 perplexity on the Google
Billion Word benchmark. We consider the LSTM with
2048 units in Table 2, a GCNN-8Bottleneck with 7 Resnet
blocks that have a bottleneck structure as described by (He
et al., 2015a) and a GCNN-8 without bottlenecks. A bot-
tleneck block wedges a k > 1 convolution between two
k = 1 layers. This designs reduces computational cost by
reducing and increasing dimensionality with the k = 1 lay-
ers so that the convolution operates in a lower dimensional
space. Our results show that the use of bottleneck blocks is
important to maintaining computational efﬁciency.

The throughput of the LSTM is measured by using a large
batch of 750 sequences of length 20, resulting in 15, 000 to-
kens per batch. The responsiveness is the average speed to
process a sequence of 15, 000 contiguous tokens. Table 4
shows that the throughput for the LSTM and the GCNN
are similar. The LSTM performs very well on GPU be-
cause the large batch size of 750 enables high paralleliza-
tion over different sentences. This is because the LSTM
implementation has been thoroughly optimized and uses
cuDNN, whereas the cuDNN implementation of convolu-
tions is not been optimized for the 1-D convolutions we use
in our model. We believe much better performance can be
achieved by a more efﬁcient 1-D cuDNN convolution. Un-
like the LSTM, the GCNN can be parallelized both over
sequences as well as across the tokens of each sequence,
allowing the GCNN to have 20x higher responsiveness.

5.2. Gating Mechanisms

In this section, we compare the gated linear unit with
other mechanisms as well as to models without gating.
We consider the LSTM-style gating mechanism (GTU)
tanh(X ∗ W + b) ⊗ σ(X ∗ V + c) of (Oord et al., 2016b)
and networks that use regular ReLU or Tanh activations.
Gating units add parameters, so for fair comparison, we
carefully cross-validate models with a comparable number
of parameters. Figure 3 (left) shows that GLU networks
converge to a lower perplexity than the other approaches
on WikiText-103. Similar to gated linear units, the ReLU
has a linear path that lets the gradients easily pass through
the active units. This translates to much faster convergence
for both the ReLU and the GLU. On the other hand, neither
Tanh nor GTU have this linear path, and thus suffer from
the vanishing gradient problem. In the GTU, both the in-
puts as well as the gating units can cut the gradient when
the units saturate.

Comparing the GTU and Tanh models allows us to measure

05101520253035Epochs4550556065707580Test PerplexityTanhReLUGTUGLU050100Hours40455055606570Test PerplexityReLUGTUGLULanguage Modeling with Gated Convolutional Networks

Figure 4. Test perplexity as a function of context for Google Billion Word (left) and Wiki-103 (right). We observe that models with
bigger context achieve better results but the results start diminishing quickly after a context of 20.

hl(X) = (X ∗ W + b) ⊗ (X ∗ V + c).

the effect of gating since the Tanh model can be thought of
as a GTU network with the sigmoid gating units removed.
The results (Figure 3, left) show that the gating units make
a vast difference and provide useful modeling capabilities,
as there is a large difference in the performance between
GTU and Tanh units. Similarly, while ReLU unit is not
an exact ablation of the gating units in the GLU, it can be
seen as a simpliﬁcation ReLU(X) = X ⊗ (X > 0) where
the gates become active depending on the sign of the input.
Also in this case, GLU units lead to lower perplexity.

In Figure 3 (right) we repeat the same experiment on the
larger Google Billion Words dataset. We consider a ﬁxed
time budget of 100 hours because of the considerable train-
ing time required for this task. Similar to WikiText-103,
the gated linear units achieve the best results on this prob-
lem. There is a gap of about 5 perplexity points between
the GLU and ReLU which is similar to the difference be-
tween the LSTM and RNN models measured by (Jozefow-
icz et al., 2016) on the same dataset.

5.3. Non-linear Modeling

The experiments so far have shown that the gated linear
unit beneﬁts from the linear path the unit provides com-
pared to other non-linearities. Next, we compare networks
with GLUs to purely linear networks and networks with
bilinear layers in order to measure the impact of the non-
linear path provided by the gates of the GLU. One mo-
tivation for this experiment is the success of linear mod-
els on many natural language processing tasks (Manning
& Sch¨utze, 1999). We consider deep linear convolutional
networks where the layers lack the gating units of the GLU
and take the form hl(X) = X ∗ W + b. Stacking sev-
eral layers on top of each other is simply a factorization of
the model which remains linear up to the softmax, at which
point it becomes log-linear. Another variation of GLUs are
bilinear layers (Mnih & Hinton, 2007) which take the form

Figure 5. Learning curves on Google Billion Word for models
with varying degrees of non-linearity.

Figure 5 shows that GLUs perform best, followed by bilin-
ear layers and then linear layers. Bilinear layers improve
over linear ones by more than 40 perplexity points, and the
GLU improves another 20 perplexity points over the bilin-
ear model. The linear model performs very poorly at per-
plexity 115 even compared to 67.6 of a Kneser-Ney 5-gram
model, even though the former has access to more con-
text. Surprisingly, the introduction of the gated linear units
is enough to reach 61 perplexity on Google Billion Word,
which surpasses both Kneser-Ney 5-gram models and the
non-linear neural model of (Ji et al., 2015).

5.4. Context Size

Figure 4 shows the impact of context size for the gated
CNN. We tried different combinations of network depth
and kernel widths for each context size and chose the best
performing one for each size. Generally, larger contexts

10203040506070Context3032343638404244Test Perplexity510152025Context405060708090Test Perplexity050100Hours406080100120140Test PerplexityLinearBilinearGLULanguage Modeling with Gated Convolutional Networks

improve accuracy but returns drastically diminish with win-
dows larger than 40 words, even for WikiText-103 where
we may condition on an entire Wikipedia article. This
means that the unlimited context offered by recurrent mod-
els is not strictly necessary for language modeling. Fur-
thermore, this ﬁnding is also congruent with the fact that
good performance with recurrent networks can be obtained
by truncating gradients after only 40 timesteps using trun-
cated back propagation through time. Figure 4 also shows
that WikiText-103 beneﬁts much more from larger context
size than Google Billion Word as the performance degrades
more sharply with smaller contexts. WikiText-103 pro-
vides much more context than Google Billion Word where
the average sentence size is 20. However, while the average
size of the documents is close to 4000 tokens, we ﬁnd that
strong performance can be achieved with a context size as
low as 30 tokens.

5.5. Training

In this section, we perform an ablation study of the impact
of weight normalization and gradient clipping. We sepa-
rately cross-validate the hyper-parameters of each conﬁgu-
ration to make the comparison fair. Due to the high cost of
each of these experiments, we only consider a single itera-
tion over the training data. Figure 6 shows that both meth-
ods signiﬁcantly speed up convergence. Weight normal-
ization in particular improves the speed by over two times.
This speedup is partly due to the ability to use much larger
learning rates (1 instead of 0.01) than would otherwise be
possible. Both clipping and weight normalization add com-
putational overhead, but it is minor compared to the large
gains in convergence speed.

Figure 6. Effect of weight normalization and gradient clipping on
Google Billion Word.

6. Conclusion

We introduce a convolutional neural network for language
modeling with a novel gating mechanism. Compared to
recurrent neural networks, our approach builds a hierarchi-
cal representation of the input words that makes it easier
to capture long-range dependencies, similar in spirit to the
tree-structured analysis of linguistic grammar formalisms.
The same property eases learning since features are passed
through a ﬁxed number of layers and non-linearities, un-
like for recurrent networks where the number of processing
steps differs depending on the position of the word in the
input. The results show that our gated convolutional net-
work achieves a new state of the art on WikiText-103. On
the Google Billion Word benchmark, we show competitive
results can be achieved with signiﬁcantly fewer resources.

Acknowledgments

We would like to thank Ben Graham, Jonas Gehring,
Edouard Grave, Armand Joulin and Ronan Collobert for
helpful discussions.

References

Bengio, Yoshua, Ducharme, R´ejean, Vincent, Pascal, and Jauvin,
journal of

Christian. A neural probabilistic language model.
machine learning research, 3(Feb):1137–1155, 2003.

Chelba, Ciprian, Mikolov, Tomas, Schuster, Mike, Ge, Qi, Brants,
Thorsten, Koehn, Phillipp, and Robinson, Tony. One billion
word benchmark for measuring progress in statistical language
modeling. arXiv preprint arXiv:1312.3005, 2013.

Chen, Stanley F and Goodman, Joshua. An empirical study of
smoothing techniques for language modeling. In Proceedings
of the 34th annual meeting on Association for Computational
Linguistics, pp. 310–318. Association for Computational Lin-
guistics, 1996.

Chen, Wenlin, Grangier, David, and Auli, Michael. Strategies
for training large vocabulary neural language models. CoRR,
abs/1512.04906, 2016.

Collobert, Ronan, Kavukcuoglu, Koray, and Farabet, Clement.
Torch7: A Matlab-like Environment for Machine Learning. In
BigLearn, NIPS Workshop, 2011. URL http://torch.ch.

Dauphin, Yann N and Grangier, David.

butions with linearizing belief networks.
arXiv:1511.05622, 2015.

Predicting distri-
arXiv preprint

Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty
of training deep feedforward neural networks. The handbook
of brain theory and neural networks, 2010.

Grave, E., Joulin, A., Ciss´e, M., Grangier, D., and J´egou, H.
Efﬁcient softmax approximation for GPUs. ArXiv e-prints,
September 2016a.

Grave, E., Joulin, A., and Usunier, N.

Improving Neural Lan-
guage Models with a Continuous Cache. ArXiv e-prints, De-
cember 2016b.

4000080000120000160000Updates5060708090100110120130140Test PerplexityWithout ClippingWithout WeightNormWith BothLanguage Modeling with Gated Convolutional Networks

Gutmann, Michael and Hyv¨arinen, Aapo. Noise-contrastive esti-
mation: A new estimation principle for unnormalized statisti-
cal models.

Oord, Aaron van den, Kalchbrenner, Nal, and Kavukcuoglu,
arXiv preprint

Pixel recurrent neural networks.

Koray.
arXiv:1601.06759, 2016a.

Oord, Aaron van den, Kalchbrenner, Nal, Vinyals, Oriol, Espe-
holt, Lasse, Graves, Alex, and Kavukcuoglu, Koray. Condi-
tional image generation with pixelcnn decoders. arXiv preprint
arXiv:1606.05328, 2016b.

Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua. On the
difﬁculty of training recurrent neural networks. In Proceedings
of The 30th International Conference on Machine Learning,
pp. 1310–1318, 2013.

Salimans, Tim and Kingma, Diederik P. Weight normalization: A
simple reparameterization to accelerate training of deep neural
networks. arXiv preprint arXiv:1602.07868, 2016.

Shazeer, Noam, Pelemans, Joris, and Chelba, Ciprian. Skip-gram
language modeling using sparse non-negative matrix probabil-
ity estimation. arXiv preprint arXiv:1412.1454, 2014.

Shazeer, Noam, Mirhoseini, Azalia, Maziarz, Krzysztof, Davis,
Andy, Le, Quoc V., Hinton, Geoffrey E., and Dean, Jeff. Out-
rageously large neural networks: The sparsely-gated mixture-
of-experts layer. CoRR, abs/1701.06538, 2017. URL http:
//arxiv.org/abs/1701.06538.

Steedman, Mark. The syntactic process. 2002.

Sutskever, Ilya, Martens, James, Dahl, George E, and Hinton, Ge-
offrey E. On the importance of initialization and momentum in
deep learning. 2013.

Yu, Dong and Deng, Li. Automatic Speech Recognition: A Deep
Learning Approach. Springer Publishing Company, Incorpo-
rated, 2014. ISBN 1447157788, 9781447157786.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
Deep residual learning for image recognition. arXiv preprint
arXiv:1512.03385, 2015a.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 1026–1034,
2015b.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term

memory. Neural computation, 9(8):1735–1780, 1997.

Ji, Shihao, Vishwanathan, SVN, Satish, Nadathur, Anderson,
Michael J, and Dubey, Pradeep. Blackout: Speeding up recur-
rent neural network language models with very large vocabu-
laries. arXiv preprint arXiv:1511.06909, 2015.

Jozefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer,
Noam, and Wu, Yonghui. Exploring the limits of language
modeling. arXiv preprint arXiv:1602.02410, 2016.

Kalchbrenner, Nal, Espeholt, Lasse, Simonyan, Karen, van den
Oord, Aaron, Graves, Alex, and Kavukcuoglu, Koray. Neural
Machine Translation in Linear Time. arXiv, 2016.

Kneser, Reinhard and Ney, Hermann. Improved backing-off for
m-gram language modeling. In Acoustics, Speech, and Signal
Processing, 1995. ICASSP-95., 1995 International Conference
on, volume 1, pp. 181–184. IEEE, 1995.

Koehn, Philipp. Statistical Machine Translation. Cambridge Uni-
versity Press, New York, NY, USA, 1st edition, 2010. ISBN
0521874157, 9780521874151.

Kuchaiev, Oleksii and Ginsburg, Boris. Factorization tricks for
LSTM networks. CoRR, abs/1703.10722, 2017. URL http:
//arxiv.org/abs/1703.10722.

LeCun, Yann and Bengio, Yoshua. Convolutional networks for
images, speech, and time series. The handbook of brain theory
and neural networks, 3361(10):1995, 1995.

Manning, Christopher D and Sch¨utze, Hinrich. Foundations of

statistical natural language processing, 1999.

Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer Sen-

tinel Mixture Models. ArXiv e-prints, September 2016.

Mikolov, Tom´aˇs, Martin, Karaﬁ´at, Burget, Luk´aˇs, Cernock´y, Jan,
and Khudanpur, Sanjeev. Recurrent Neural Network based
Language Model. In Proc. of INTERSPEECH, pp. 1045–1048,
2010.

Mnih, Andriy and Hinton, Geoffrey. Three new graphical models
for statistical language modelling. In Proceedings of the 24th
international conference on Machine learning, pp. 641–648.
ACM, 2007.

Morin, Frederic and Bengio, Yoshua. Hierarchical probabilistic
neural network language model. In Aistats, volume 5, pp. 246–
252. Citeseer, 2005.

