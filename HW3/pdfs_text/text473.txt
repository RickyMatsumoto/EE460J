Dictionary Learning Based on Sparse Distribution Tomography

Pedram Pad * 1 Farnood Salehi * 2 Elisa Celis 2 Patrick Thiran 2 Michael Unser 1

Abstract
We propose a new statistical dictionary learning
algorithm for sparse signals that is based on an
α-stable innovation model. The parameters of
the underlying model—that is, the atoms of the
dictionary, the sparsity index α and the disper-
sion of the transform-domain coefﬁcients—are
recovered using a new type of probability distri-
bution tomography. Speciﬁcally, we drive our
estimator with a series of random projections of
the data, which results in an efﬁcient algorithm.
Moreover, since the projections are achieved us-
ing linear combinations, we can invoke the gen-
eralized central limit theorem to justify the use
of our method for sparse signals that are not nec-
essarily α-stable. We evaluate our algorithm by
performing two types of experiments: image in-
painting and image denoising. In both cases, we
ﬁnd that our approach is competitive with state-
of-the-art dictionary learning techniques. Beyond
the algorithm itself, two aspects of this study are
interesting in their own right. The ﬁrst is our sta-
tistical formulation of the problem, which uniﬁes
the topics of dictionary learning and independent
component analysis. The second is a generaliza-
tion of a classical theorem about isometries of
(cid:96)p-norms that constitutes the foundation of our
approach.

1. Introduction

The problem of ﬁnding the mixing matrix A from a set of
observation vectors y in the model

y = Ax

(1)

is only solvable if one can beneﬁt from strong hypotheses
on the signal vector x. For instance, one may assume that

*Equal contribution 1Biomedical Imaging Group, EPFL, Lau-
sanne, Switzerland 2Computer Communications and Applications
Laboratory 3, EPFL, Lausanne, Switzerland. Correspondence to:
Pedram Pad <pedram.pad@epﬂ.ch>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

the entries of x are statistically independent, which results
in a class of methods refered to as independent component
analysis (ICA) (Hyvärinen et al., 2004). A more recent
trend is to assume that the vector x is sparse, so that the
recovery can be recast as a deterministic dictionary learning
problem, the prototypical example being sparse component
analysis (SCA) (Gribonval & Lesage, 2006; Aharon et al.,
2006; Spielman et al., 2012). Extensive research has been
conducted on these problems in the past three decades.

Prior work: In the literature, ICA precedes SCA and can
be traced back to (Herault & Jutten, 1986). In fact, ICA con-
stitutes the non-Gaussian generalization of the much older
principal component analysis (PCA), which is widely used
in classical signal processing. ICA is usually formalized
as an optimization problem involving a cost function that
measures the independence of the estimated xi (i.e., the
entries of the vector x). A common measure of indepen-
dence, which is inspired by information theory, is the mutual
information of the entries of x. However, due to its com-
putational complexity, other measures such as the kurtosis,
which measures the non-Gaussianity of the components, are
often used (Hyvärinen & Oja, 2000; Naik & Kumar, 2011)
(except in special cases such as the analysis of stable AR(1)
processes by (Pad & Unser, 2015)). The main drawback of
ICA is that the system (1) needs to be determined; i.e., A
should be square—otherwise the complexity is so high that
the methods can only be implemented for small problems
(Lathauwer et al., 2007; Lathauwer & Castaing, 2008).

SCA, on the other hand, is usually achieved by putting
constraints on the sparsity of the representation or by opti-
mizing a sparsity-promoting cost function. Thanks to the
emergence of very efﬁcient algorithms, SCA has found
wide use in different applications (see (Mairal et al., 2010;
Marvasti et al., 2012)). The underlying framework for
SCA is deterministic—this is the primary difference with
ICA, which aims to decouple signals that are realizations of
stochastic processes.

α-stable distributions: In this paper, we aim to achieve the
best of both worlds: the use of a statistical formulation—in
the spirit of ICA—with a restriction to a parametric class
of stochastic models that is well adapted to the notion of
sparsity. Speciﬁcally, we assume that the entries of the
vector x are random variables that are i.i.d. symmetric-α-

Dictionary Learning Based on Sparse Distribution Tomography

We also show that the proposed algorithm provides an efﬁ-
cient estimator of the spectral measure of a stable random
vector. An enabling component of our method is a new
theorem that generalizes a classical result about isometries
of (cid:96)p-norms.

Organization: In the next section, we brieﬂy review SαS
random variables and present our mathematical model. In
Section 3, we establish our main result which then yields an
algorithm for ﬁnding the matrix A as well as the sparsity
index α. In Section 4, we present the simulation results and
compare their performance with existing algorithms. In Sec-
tion 5, we summarize the paper and give some suggestions
for future work.

2. Preliminaries and problem formulation

We begin by recalling some basic properties of symmetric-
α-stable random variables. We then proceed with the formu-
lation of the estimation problem that we solve in Section 3.
The notation that we use throughout the paper is as follows:
we use italic symbols for random variables, capital boldface
symbols for matrices and lowercase boldface symbols for
vectors. Thus, X is a deterministic matrix, X is a random
matrix and x is a random variable. Likewise, x and x denote
a random and a deterministic vector respectively.

2.1. Symmetric-α-stable random variables

For any α ∈ (0, 2] and γ > 0, a random variable X with
characteristic function

(cid:98)pX (ω) = exp(−γ|ω|α)

(2)

is called a symmetric-α-stable (SαS) random variable with
dispersion γ and stability parameter α (Nikias & Shao,
1995). This class of random variables is a generalization of
the Gaussian model: For α = 2, X is a Gaussian random
variable with zero mean and variance 2γ. As their name
suggests, α-stable variables share the property of stability
under linear combination (Nikias & Shao, 1995); i.e., if
X1, . . . , Xn are n i.i.d. copies of X and a1, . . . , an ∈ R are
n real numbers, then the random variable

Y = a1X1 + · · · + anXn

has the same distribution as

(cid:0)|a1|α + · · · + |an|α(cid:1) 1

α X.

(3)

(4)

In other words, the random variable Y is an SαS random
α where (cid:107)a(cid:107)α = (cid:0)|a1|α +
variable with dispersion γ (cid:107)a(cid:107)α
· · · + |an|α(cid:1) 1
α is the α-(pseudo)norm of the vector a =
(a1, . . . , an). This property makes SαS random variables
convenient for the study of linear systems.

Figure 1. Illustration of the effect of α on the sparsity of the signal.
Two realizations of i.i.d. SαS signals.

stable. The family of α-stable distributions is a general-
ization of the Gaussian probability density function (PDF).
Since α-stability is preserved through linear transforma-
tion, this class of models has a central position in the study
of stochastic processes (Samoradnitsky & Taqqu, 1994;
Nikias & Shao, 1995; Shao & Nikias, 1993). The family is
parametrized by α ∈ (0, 2], which controls the rate of decay
of the distribution. The extreme case of α = 2 corresponds
to the Gaussian distribution—the only non-sparse member
of the family. By contrast, the other members of the SαS
family for α < 2 are heavy-tailed with unbounded variance.
This property implies that an i.i.d. sequence of such ran-
dom variables generates a sparse signal (Amini et al., 2011;
Gribonval et al., 2012). By decreasing α, the distribution
becomes more heavy-tailed and thus the signal becomes
more sparse (the effect of α is illustrated in Figure 1).

This class of random variables has also been widely used
in practice. Typical applications include: modeling of ul-
trasound RF signals, (Achim et al., 2015), signal detection
theory (Kuruoglu et al., 1998), communications (Middleton,
1999), image processing (Achim & Kuruoglu, 2005), audio
processing (Georgiou et al., 1999), sea surface (Gallagher,
2001), network trafﬁc (Resnick, 1997), and ﬁnance (Nolan,
2003; Ling, 2005).

Main contributions: Our main contribution in this paper
is a new dictionary learning algorithm based on the signal
modeling mentioned above. The proposed method has the
following advantages:

1. all parameters can be estimated from the data (it is

hyperparameter-free),

2. it learns the dictionary without the need to recover the

signal x, and

3. it is fast and remarkably robust.

Once the matrix A is estimated, it is then possible to efﬁ-
ciently recover x by using standard procedures (Bickson &
Guestrin, 2010).

(a)α=10255075100-50050(b)α=1.4SampleIndex0255075100SampleValue-20020Dictionary Learning Based on Sparse Distribution Tomography

The other property of SαS random variables with α < 2 is
their heavy-tailed PDF. When α < 2, we have

lim
|x|→∞

|x|1+αpX (x) = C(α, γ),

(5)

where pX is the PDF of X and C(α, γ) is a positive con-
stant that depends on α and γ (Nikias & Shao, 1995). This
implies that the variance of SαS random variables is un-
bounded for α < 2. Also, note that a smaller α results in
heavier tails.

Inﬁnite-variance random variables are considered to be ap-
propriate candidates for sparse signals (Amini et al., 2011;
Gribonval et al., 2012). Because an i.i.d. sequence of heavy-
tailed random variables has most of its energy concentrated
on a small fraction of samples, they are good candidates to
model signals that exhibit sparse behavior.

Yet, the truly fundamental aspect of α-stable random vari-
ables is their role in the generalized central limit theorem.
As we know, the limit distribution of normalized sums of
i.i.d. ﬁnite-variance random variables are Gaussian. Like-
wise, any properly normalized sum of heavy-tailed i.i.d.
random variables converges to an α-stable random variable
whee the α depends on the weight of their tail (Meerschaert
& Schefﬂer, 2001). This implies that a linear combination
of a large number of samples of a sparse signal is well
represented by α-stable random variables.

2.2. Problem formulation

Our underlying signal model is

y = Ax

3.1. New cost function for sparse SαS signals

(cid:1)

Recall that, the random vector y (see Equations (2) and
(6)) is an m-dimensional α-stable vector with characteristic
function

(cid:98)py(ω) = exp (cid:0)−γ(cid:107)A(cid:62)ω(cid:107)α
(7)
for ω ∈ Rm. Thus, knowing (cid:107)A(cid:62)u(cid:107)α for all u ∈ S m−1,
where S m−1 is the (m − 1)-dimensional unit sphere, i.e.,
S m−1 = {u ∈ Rm | (cid:107)u(cid:107)2 = 1} ,
is equivalent to knowing the distribution of y. Note that
u(cid:62)y = u(cid:62)Ax (see Equations (3) and (4)) is an SαS ran-
dom variable with dispersion

(8)

α

γ(u) = γ(cid:107)A(cid:62)u(cid:107)α
α.

(9)

Thus, knowing the dispersion of the marginal distributions
of y for all u ∈ S m−1 is equivalent to knowing the dis-
tribution of y. In other words, in the case of SαS random
vectors, knowing their marginal dispersions is equivalent to
knowing the Radon transform of their PDFs or, equivalently,
their joint characteristic function (Helgason, 2010). Due to
the relationship between the Radon transform and the ﬁeld
of tomography, we call our algorithm sparse distribution
tomography (SparsDT).

Another interesting fact is that, in the non-Gaussian case
(α < 2), knowing the marginal dispersions of y, i.e., γ(u),
identiﬁes the matrix A uniquely, up to negation and per-
mutation of the columns. Formally, we have the following
theorem, which is proved in Appendix A:

Theorem 1 Let A be an m × n matrix where columns are
pairwise linearly independent. If α ∈ (0, 2) and B is an
m × n matrix for which we have

(6)

where x is an unknown n × 1 random vector with SαS i.i.d.
entries and α < 2, y is an m × 1 observation vector and A
is an m × n dictionary matrix,. We are given K realizations
of y; namely, y1, . . . , yK, and our goal is to estimate A.

3. Dictionary learning for SαS signals

In the problem of dictionary learning, the maximum in-
formation that we can asymptotically try to retrieve from
y1, . . . , yK is the exact distribution of y. However, even if
we knew y, identifying A is still not tractable in general—
for instance, in the case of Gaussian vectors, A is only
identiﬁable up to right-multiplication by a unitary matrix.
Moreover, obtaining an acceptable estimate of the distribu-
tion of y requires, in general, a vast amount of data and
processing power (since it is a m-dimensional function with
m possibly large). In this section, we leverage the property
of stability under linear combination of SαS random vari-
ables explained in Section 2.1 to propose a new algorithm
to estimate A for the dictionary learning problem stated in
Section 2.2.

(cid:107)A(cid:62)u(cid:107)α

(10)
for all u ∈ Rm, then B is equal to A up to negation and
permutation of its columns.

α = (cid:107)B(cid:62)u(cid:107)α
α

Remark 1 This theorem can be seen as a generalization of
the result in (Rolewicz, 1985) that states that the isometries
of (cid:96)p-norms are generalized permutation matrices (permu-
tation matrices with some of their rows negated). To the
best of our knowledge, this result is novel and could be of
independent interest.

This theorem suggests that in order to ﬁnd A all we need is
to ﬁnd γ(u) for u ∈ Rm. Intuitively, we can say that as A
has a ﬁnite number of parameters (entries), A is identiﬁable
based on the knowledge of γ(u) for an appropriate ﬁnite
set of vectors u = u1, . . . , uL (for some L ≥ mn). We can
then solve the set of non-linear equations

γ(cid:107)B(cid:62)u1(cid:107)α

α = γ(u1),

...

(11)

γ(cid:107)B(cid:62)uL(cid:107)α

α = γ(uL),

Dictionary Learning Based on Sparse Distribution Tomography

for B to obtain A.
Now, the problem is to ﬁnd γ(u) for a given u ∈ Rm. Recall
that γ(u) is the dispersion of the SαS random variable uT y.
As y1, . . . , yK are realizations of y, uT y1, . . . , uT yK are
realizations of uT y. There is a rich literature on the estima-
tion of the parameters of α-stable random variables through
their realizations, see, e.g, (Nikias & Shao, 1995). We use
the estimation from (Achim et al., 2015) in the following
equation

log ˆγ(u) =

log |uT yk| − (α − 1)ψ(1)

(12)

α
K

K
(cid:88)

k=1

where ψ is the digamma function (ψ(1) is the negative of the
Euler-Mascheroni constant and is approximately 0.5572),
and ˆγ(u) denotes the estimation of γ(u). Note that ˆγ(u)
tends to γ(u) when the number of observations, K, tends
to inﬁnity. This means that we can obtain the exact value of
γ(u) asymptotically.

However, non-exact values for γ(u(cid:96)), for (cid:96) = 1, . . . , L
(which is the case when K is ﬁnite), can lead to the non-
existence of a solution for the system of equations (11).
To overcome this problem, instead of solving this system
of equations exactly, we minimize the following objective
function

merged into the learned dictionary. Recall that there are
well-known methods for estimating α from data; among
which we use

ˆα(u) =

(cid:32)

6
π2K

K
(cid:88)

k=1

(cid:0)log |u(cid:62)yk| − log ˆκ(u)(cid:1)2

−

(cid:33)− 1

2

1
2

(15)

from (Achim et al., 2015), where

log ˆκ(u) =

log |u(cid:62)yk|.

(16)

This gives us an estimate for α for any given u ∈ Rm.
Hence, the estimated value of α is the average over all
ˆα(u(cid:96)) for (cid:96) = 1, . . . , L, i.e.,

ˆα =

ˆα(u(cid:96)).

(17)

Now, using this estimate, Equation (12) becomes

log ˆγ(u) = ˆα log ˆκ(u) − (ˆα − 1)ψ(1).

(18)

We also replace α with ˆα in Equation (13) which results in
a parameter-free cost function. This is in contrast with most
existing cost functions that have parameters one must set.

1
K

K
(cid:88)

k=1

1
L

L
(cid:88)

(cid:96)=1

E(B) =

d (cid:0)γ(cid:107)B(cid:62)u(cid:96)(cid:107)α

α, ˆγ(u(cid:96))(cid:1)

(13)

3.2. Learning algorithm

1
L

L
(cid:88)

(cid:96)=1

=

1
αL

L
(cid:88)

(cid:96)=1

(cid:12)
(cid:12)log(γ(cid:107)B(cid:62)u(cid:96)(cid:107)α

α) − log(ˆγ(u(cid:96)))(cid:12)
(cid:12)

where log ˆγ(u1), . . . , log ˆγ(uL) are L numbers calculated
from (12). The cost function d(a, b) = 1
α | log a − log b| is
a continuous positive function1 from R2 to R, whose global
minimum is 0 and is reached over the line a = b. When
ˆγ(u) = γ(u), B = A minimizes E(B). Thus, if ˆγ(u) is
close enough to γ(u), due to the continuity of d, we expect
that the minimizer of E will be close to A. Therefore, our
approach to dictionary learning is to solve

∇E(B) =

1
αL

L
(cid:88)

(cid:96)=1

(14)

To solve the minimization problem in Equation (14), we
propose a variation on a gradient-descent algorithm with
an adaptive step size that has a changing cost function. To
do so, we ﬁrst derive the gradient of E at B. Using matrix
calculus (see Appendix B), we ﬁnd that

sgn (cid:0)log(γ(cid:107)B(cid:62)u(cid:96)(cid:107)α

α) − log ˆγ(u(cid:96))(cid:1) ·

(19)

∇(cid:107)B(cid:62)u(cid:96)(cid:107)α
α
(cid:107)B(cid:62)u(cid:96)(cid:107)α
α

where sgn(·) is the sign function (i.e., sgn(e) = 1 if e > 0
and sgn(e) = 0 otherwise) and

(cid:98)A = argmin

E(B)

B

B

1
αL

L
(cid:88)

(cid:96)=1

= argmin

(cid:12)
(cid:12)log (cid:0)γ(cid:107)B(cid:62)u(cid:96)(cid:107)α

α

(cid:1) − log ˆγ(u(cid:96))(cid:12)
(cid:12) .

∇(cid:107)B(cid:62)u(cid:107)α

α = α

1 u(cid:1) (cid:12)

1 u(cid:12)
α−1
(cid:12)

u(cid:62)


sgn (cid:0)b(cid:62)



sgn (cid:0)b(cid:62)

(cid:12)b(cid:62)
...
(cid:12)b(cid:62)

n u(cid:1) (cid:12)

n u(cid:12)
α−1
(cid:12)

u(cid:62)



(cid:62)





(20)

The only parameter that needs to be set now is the stability
parameter α. Note that the dispersion parameter γ in Equa-
tion (14) does not need to be set as it will be automatically

1In our simulations we also implemented other natural can-
didates for d(a, b) and all of them gave approximately the same
performance. Due to the limited space, we do not present results
for other cost functions.

where bi is the ith column of B.

The cost function in Equation (13) is non-convex in B. In
order to avoid getting trapped in local minima, we iteratively
change the cost function inside the gradient descent algo-
rithm. The idea is that instead of keeping u1, . . . , uL ﬁxed
throughout the optimization process, we regenerate them
randomly with a uniform distribution on Rm after some

Dictionary Learning Based on Sparse Distribution Tomography

iterations of steepest descent. We repeat this process until
convergence. Note that (11) holds for any u1, . . . , uL and
thus changing this set does not change the end result of (11).

Remark 2 Using this idea always results in convergence
to the global minimum in our computer simulations. A
plausible explanation of this phenomenon is that each set of
u1, . . . , uL yields a non-convex cost function with different
local minima. Yet they all have the same global minimum.
Therefore, switching between them during the optimization
process prevents getting trapped in any of the local minima,
which ultimately results in ﬁnding the global minimum of
the cost function.

The pseudocode of our dictionary learning method is given
in Algorithm 1. There, η is the step size of the gradient
descent that increases or decreases by factors of κ+ or κ−
upon taking a good or poor step. The adaptive step size is
especially helpful for α ≤ 1, where the cost function is not
smooth. The algorithm does not depend on the exact choice
of convergence criteria.

Remark 3 Algorithm 1 can also be seen as an efﬁcient
method for estimating the spectral measure of stable random
vectors. In fact, the problem of estimating A from a set of
realizations of y can also be seen as parameter estimation
for a stable random vector y with a symmetric distribution
around the origin. Such random vectors are parametrized
by a measure Γ on S m−1 that is called the spectral measure.
In our problem, we have Γ(·) = (cid:80)n
i=1 (cid:107)ai(cid:107)α δai(·) where
ai
and ai is the ith
the δais are unit point masses at
(cid:107)ai(cid:107)2
column of A. Some methods have been proposed to solve
this problem, e.g., (Nolan et al., 2001). However, they tend
to be computationally intractable for dimensions greater
than 3.

Remark 4 According to the generalized central limit theo-
rem, under some mild conditions, the distribution of the sum
of symmetric heavy-tailed random variables tends to a SαS
distribution as the number of summands tends to inﬁnity.
This means that we can represent u(cid:62)y = u(cid:62)Ax with an
SαS random variable for large enough n irrespective of the
distribution of the xis provided that the latter are heavy
tailed. Therefore, we expect Algorithm 1 to ﬁnd applica-
tions for other classes of sparse signals, provided that n is
sufﬁciently large.

4. Empirical results

In this section, we analyze the performance of the proposed
algorithm SparsDT and compare it with existing methods.
Recall that the actual dictionary is A and the learned dictio-
nary is (cid:98)A. We run two types of the experiments: We ﬁrst
test the algorithm on synthetic SαS data and then we test it
on real images.

Algorithm 1 SparseDT
1: initialize: η > 0
2: initialize: κ+ ≥ 1 and κ− ≤ 1
3: initialize: generate b1, . . . , bn ∼ N (0, Im×m) and

initialize: generate u1, . . . , uL ∼ N (0, Im×m)
estimate ˆα from (15)
E ← E(B)
repeat

B ← (cid:2)b1| . . . |bn

(cid:3)

Bold ← B
Eold ← E
B ← B − η ∇E(B)
E ← E(B)
if E ≤ Eold then
η ← κ+ · η

4: repeat
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: until B converges

B ← Bold
E ← Eold
η ← κ− · η

end if

else

return B

4.1. Benchmarks

until B converges (for this choice of u1, . . . , uL)

We compare our algorithm with three commonly used al-
gorithms that are available in the Python package SPAMS2.
These constrained optimization problems3 are as follows:

1. (cid:96)2/(cid:96)1: Maximizing the data ﬁdelity while controling
the sparsity of representation with parameter λ1:

(cid:98)A(cid:96)2/(cid:96)1 = argmin

(cid:107)yk − Bxk(cid:107)2
2

K
(cid:88)

1
2K

B

k=1
s.t. (cid:107)xi(cid:107)1 ≤ λ1.

2. (cid:96)1/(cid:96)2: Maximizing the sparsity of representation while

controling the data ﬁdelity with parameter λ2:

(cid:98)A(cid:96)1/(cid:96)2 = argmin

B

1
2K

K
(cid:88)

k=1

(cid:107)xk(cid:107)1

s.t. (cid:107)yk − Bxk(cid:107)2 ≤ λ2.

3. (cid:96)1 + (cid:96)2: Combining sparsity and data ﬁdelity in the

cost function using Lagrange multipliers:

(cid:98)A(cid:96)1+(cid:96)2 = argmin

B

1
2K

K
(cid:88)

k=1

(cid:107)yk − Bxk(cid:107)2
2

+ λ3(cid:107)xk(cid:107)1 + λ4(cid:107)xk(cid:107)2
2.

2http://spams-devel.gforge.inria.fr/
3Other cost functions are also available in the package SPAMS,

but those retained here yield the best results in our experiments.

Dictionary Learning Based on Sparse Distribution Tomography

Comparison metrics

Algorithm % found Avg. time (s)

SparsDT
(cid:96)2/(cid:96)1
(cid:96)1/(cid:96)2
(cid:96)1 + (cid:96)2

100
50
75
94

5.19
0.07
95.75
19.89

Table 1. Performance of Algorithms on SαS Signals. α = 1.2,
A16×24 matrix, K = 500.

Comparison against benchmarks: We compare SparsDT
against the (cid:96)2/(cid:96)1, (cid:96)1/(cid:96)2 and (cid:96)1 + (cid:96)2 methods described
above. We compare the algorithms with regard to their
success rate (i.e., the percentage of the dictionaries found
by the algorithm), and the time that they take to ﬁnd the
dictionary (in the cases of success only). We again take
m = 16, n = 24 and generate 100 random matrices A. In
Table 1, the results for α = 1.2 and K = 500 are given.
Finally, in Figure 3 we compare the algorithms success
rate for different α, we take m = 16, n = 24, and K =
1000. These results indicate that SparsDT outperforms the
other methods in the rate of success. Also, its average
learning time is typically much less than the others, except
for (cid:96)2/(cid:96)1 which does not ﬁnd the correct dictionary at best
in 10% of the time. The range of α that was observed in
our experiments is α ∈ [1, 1.6], which is also the range
where our algorithm works well and which is interesting
for many applications including image processing. We do
not recommend using the method for α > 1.7 because
the convergence properties degrade as we get closer to 2
(a larger value of K is then needed to reach high success
rates).

4.3. Experimental results for real images

Since images often have sparse representations, we apply
our algorithm to problems in image processing applications.
Our experiments are missing pixel recovery (in-painting)
and denoising, based on dictionary learning. We use a

Figure 3. Impact of α ∈ [1, 1.6] on the success rate of the algo-
rithms.

Figure 2. Impact of the number of samples K on the average cor-
relation for A16×24.

One of the challenges in utilizing these benchmarks is de-
termining the regularization parameters λ1, . . . , λ4. In our
experiments, the regularization parameters are optimized
(by grid search) in order to maximize the performance of
each of the benchmarks above. This is in contrast to our
algorithm, which has no regularization parameter to tune.

4.2. Experimental results for synthetic data

We ﬁrst test the algorithms on synthetic data. In order to
quantify the performance of the algorithms, we use several
metrics. One is the average correlation of the dictionar-
ies. Speciﬁcally, we calculate the correlation between all
columns of (cid:98)A and A, and then match each column of (cid:98)A
with one of the columns of A (a one-to-one map) such that
the average correlation between the corresponding columns
is maximized. Additionally, we say that the dictionary is
found if the average correlation of the columns is larger than
0.97.

Effect of K: We demonstrate the effect of the number
of samples K on the performance of our proposed algo-
rithm SparsDT. Intuitively, the precision of the estimation
increases with the number of samples K, and, as K goes
to inﬁnity, the estimation error goes to zero, which ulti-
mately gives the exact A. We demonstrate this effect with
the following experiment: We take m = 16, n = 24 and
α = 1 and 1.5. Then, for each K, we run the experiment
for 50 random matrices A, and, for each case, we run Algo-
rithm 1 with both exact and estimated α (from (17)). The
results are depicted in Figure 2, where the vertical axis is
the average correlation of the estimated dictionary with the
exact one, and the horizontal axis is the number of sam-
ples K. Interestingly, the performance of the algorithm is
almost the same when using the exact or estimated value
of α, which suggests that the estimator of α is robust and
accurate. Moreover, we see that the average correlation is
an increasing function of K, as expected. Also note that the
convergence is faster for α = 1, which corresponds to the
setting with more sparsity.

K50100150200250300350400450500AvgCorrolation(%)7580859095100exactα=1estimatedα=1exactα=1.5estimatedα=1.51.01.11.21.31.41.51.6α020406080100%founddictionary‘2/‘1‘1/‘2‘1+‘2SparsDTDictionary Learning Based on Sparse Distribution Tomography

Algorithm PSNR (dB)

SparsDT
(cid:96)2/(cid:96)1
(cid:96)1/(cid:96)2
(cid:96)1 + (cid:96)2

29.61
28.98
28.74
28.98

Table 2. Performance of different methods for denoising images
contaminated by Gaussian noise.

database of face images provided by AT&T4 and crop them
to have size 112 × 91 so we can chop each image to 208
patches of size 7 × 7, which correspond to yi in our model.

In this situation, the data is not exactly SαS, so we must
adapt our choice of u in Step 5 of Algorithm 1. Speciﬁcally,
in Equation (17) we eliminate projection vectors u that
result in α greater than 2 (as α is required to be less than 2).
In addition, we only select u that results in an α close to our
estimated ˆα in (17). The number of iterations are chosen
such that all algorithms converge.

Missing pixel recovery: In this experiment, we reconstruct
an image from 50% of its pixels. We take out the image
shown in Figure 4, remove 50% of its pixels uniformly
at random, and learn the dictionary using the patches of
the other images in the collection. We assume 248 atoms
in the dictionary. Then, using the learned dictionary, we
reconstruct the image using orthogonal matching pursuit
(for a detailed analysis see (Sahoo & Makur, 2015)). The
results for different dictionary learning methods are depicted
in Figure 4; SparsDT outperforms existing methods both
visually and in term of PSNR.

Image denoising: In this experiment, we use the dictionar-
ies learned in the previous experiment to denoise the image
in Figure 4. More precisely, we add Gaussian noise with
standard deviation 10 to the original image and use orthog-
onal matching pursuit to denoise it. The performance of
each method in PSNR can be seen in Table 2. As we see,
SparsDT outperforms the other methods by at least 0.6 dB.

5. Summary and future work

In this paper, we consider a stochastic generation model of
sparse signals that involves an SαS innovation. Then, by
designing an estimator of the spectral measure of so-deﬁned
stable random vectors, we propose a new dictionary learn-
ing algorithm. The proposed algorithm (SparsDT) turns
out to be quite robust; it works well on sparse real-world
signals, even when these do not rigorously follow the SαS
model. This surprising fact can be explained by invoking
the generalized central limit theorem. We validate SparsDT
on several image-processing tasks and found it to outper-
form popular dictionary learning methods often signiﬁcantly.

4www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html

Moreover, SparsDT has no parameter to tune, contrary to
other algorithms.

Extending this work to non-symmetric α-stable random vari-
ables is a possible direction of future research. Given the
excellent numerical behavior of the algorithm, it is of inter-
est to get a good handle on the accuracy of the estimation in
terms of the number of samples and the dimensionality of
signals.

A. Proof of Theorem 1

Denote the jth column of A and B by aj and bj, respec-
tively. Also, denote the set of indices j for which bj (cid:54)= 0
by B ⊆ {1, . . . , n}. Note that due to the assumption of the
pairwise linear independence of columns of A, aj (cid:54)= 0 for
all j ∈ {1, . . . , n}. Since

(cid:13)A(cid:62)u(cid:13)
(cid:13)
α
α =
(cid:13)

(cid:12)
(cid:12)u(cid:62)aj

(cid:12)
(cid:12)

α

=

(cid:12)
(cid:12)u(cid:62)bj

(cid:12)
(cid:12)

α

= (cid:13)

(cid:13)B(cid:62)u(cid:13)
α
α .
(cid:13)

(cid:88)

j∈B

n
(cid:88)

j=1

for all u ∈ Rm, the partial derivatives of any order of the
two side of the equation are also equal. In particular, we
have

∂d
∂ui

d

(cid:13)A(cid:62)u(cid:13)
(cid:13)
α
α =
(cid:13)

∂d
∂ui

d

(cid:13)B(cid:62)u(cid:13)
(cid:13)
(cid:13)

α

α

(21)

for all i = 1, . . . , m and d ∈ N, where ui is the ith entry of
u.

First we prove the theorem for 0 < α ≤ 1. In (21), we set
d = 1 and obtain

n
(cid:88)

j=1

=

(cid:88)

j∈B

(cid:12)
(cid:12)u(cid:62)aj

(cid:12)
(cid:12)

α−1

sgn (cid:0)u(cid:62)aj

(cid:1) aij

(cid:12)
(cid:12)u(cid:62)bj

(cid:12)
(cid:12)

α−1

sgn (cid:0)u(cid:62)bj

(cid:1) bij.

(22)

Exploiting this equation, we prove the following lemma:

Lemma 1 Under the assumptions of Theorem 1, for any
j(cid:48) ∈ {1, . . . , n}, there exists j ∈ B and tj(cid:48) (cid:54)= 0, such that
tj(cid:48)aj(cid:48) = bj.

Proof: Take i(cid:48) such that ai(cid:48)j(cid:48)
1, . . . , n, deﬁne

(cid:54)= 0. Also, for all r =

r = (cid:8)u ∈ Rm|u(cid:62)ar = 0(cid:9)
Ka
which is an (m − 1)-dimensional subspace of Rm. Since
for any j (cid:54)= j(cid:48), aj(cid:48) and aj are linearly independent, the
subspace Ka
j is (m − 2)-dimensional. This implies
that their (m − 1)-dimensional Lebesgue measure is zero;
(cid:1). Since
and the same holds for the union (cid:83)

j(cid:48) ∩ Ka

(cid:0)Ka

(23)

j(cid:48) ∩ Ka
j

j(cid:54)=j(cid:48)

Ka

j(cid:48) \

Ka

j = Ka

j(cid:48) \

(cid:0)Ka

j(cid:48) ∩ Ka
j

(cid:1) ,

(24)

(cid:91)

j(cid:54)=j(cid:48)

(cid:91)

j(cid:54)=j(cid:48)

Dictionary Learning Based on Sparse Distribution Tomography

Figure 4. Comparing the proposed method with the existing ones for recovering missing pixels.

j(cid:54)=j(cid:48) Ka

we conclude that the (m − 1)-dimensional Lebesgue mea-
sure of Ka
j is inﬁnity.
j(cid:48) \ (cid:83)

j(cid:48) \ (cid:83)
j(cid:54)=j(cid:48) Ka
Note that any u ∈ Ka
j is only orthogonal to
aj(cid:48) and not to any other column of A. This yields that if
we set i = i(cid:48) in the left-hand side of (22), for any u ∈
j(cid:48) \ (cid:83)
Ka
j , the only discontinuous term at u is the
j(cid:48)th one (because the function |x|α−1sgn(x) has a single
point of discontinuity at x = 0). As a result, the sum itself
j(cid:48) \ (cid:83)
is discontinuous over Ka
j . Hence, the same
should hold for the right-hand side of the equation.

j(cid:54)=j(cid:48) Ka

j(cid:54)=j(cid:48) Ka

Similar to (23), deﬁne

r = (cid:8)u ∈ Rm|u(cid:62)br = 0(cid:9) .
Kb
The set of discontinuity points of the right-hand side of (22)
is a subset of (cid:83)

j. Therefore, we have

j∈B Kb

(25)

Also, since all pairs of columns of A are linearly inde-
pendent, the correspondence between a column of A and
a column of B that are linearly dependent is one-to-one.
Thus, we can simplify (22) to be

(1 − |tj|) (cid:12)

(cid:12)u(cid:62)aj

(cid:12)
α−1
(cid:12)

sgn (cid:0)u(cid:62)aj

(cid:1) aij = 0,

(28)

n
(cid:88)

j=1

which holds for all u. This implies that the left hand-side of
the above equation is a continuous function. However, as we
saw in the proof of the lemma, every u ∈ Ka
j(cid:54)=j(cid:48) Ka
j
is a discontinuity point of the left-hand unless 1 − |t(cid:48)
j| = 0
which completes the proof for the case of 0 < α ≤ 1.

j(cid:48) \ (cid:83)

For the case of 1 < α < 2, we set d = 2 in (21) and obtain

n
(cid:88)

j=1

(cid:12)
(cid:12)u(cid:62)aj

(cid:12)
(cid:12)

α−2

a2
ij =

(cid:12)
(cid:12)u(cid:62)bj

(cid:12)
(cid:12)

α−2

b2
ij.

(29)

n
(cid:88)

j=1

Ka

j(cid:48) \

Ka

j ⊆

(cid:91)

j(cid:54)=j(cid:48)

Kb
j

(cid:91)

j∈B

(26)

Replacing (22) by (29), the same reasoning as for 0 < α ≤ 1
works to prove the theorem for 1 < α < 2.

which can also be written as

Ka

j(cid:48) \

Ka

j ⊆

(cid:0)Ka

j(cid:48) ∩ Kb
j

(cid:1)

(27)

(cid:91)

j(cid:54)=j(cid:48)

(cid:91)

j∈B

j(cid:48) ∩ Kb

Now, if none of the columns of B is linearly dependent to
aj(cid:48), all Ka
j will be (m − 2)-dimensional spaces, and
their (m − 1)-dimesnional Lebesgue measure is zero. This
implies that the (m − 1)-dimensional Lebesgue measure of
the right-hand side of (27) is also zero, which contradicts
the result after (24). Therefore, there exists a j ∈ B such
that bj is linearly dependent to aj(cid:48), which completes the
(cid:3)
proof of the lemma.

The ﬁrst consequence of Lemma 1 is that none of the
columns of B is the zero vector and thus B = {1, . . . , n}.

B. Derivation of the gradient of E(B)

To calculate the gradient of E(B), we ﬁrst calculate the
gradient of (cid:107)B(cid:62)u(cid:107)α
α using the deﬁnition of the gradient, i.e.

(cid:104)∇(cid:107)B(cid:62)u(cid:107)α

α, C(cid:105) =

(cid:13)
(cid:13)(B(cid:62) − (cid:15)C(cid:62))u(cid:13)
(cid:13)

= α

j u sgn (cid:0)b(cid:62)
c(cid:62)

j u(cid:1) (cid:12)

α−1

.

α

α

(cid:12)
(cid:12)
(cid:12)(cid:15)=0
j u(cid:12)
(cid:12)

(cid:12)b(cid:62)

∂
∂(cid:15)

n
(cid:88)

j=1

Here, (cid:104)D, C(cid:105) = tr(D(cid:62)C) is the standard inner product on
the space of matrices. Writing the last equation in the matrix
form, we obtain (20). Now, using the fact d
dx |log x| =
sgn(log x) 1
x and the chain rule for differentiation yields
(19).

Originalimage50%missingpixelsSparsDTPSNR28.91dB`2=`1PSNR27.6dB`1=`2PSNR26.03dB`1+`2PSNR26.48dBDictionary Learning Based on Sparse Distribution Tomography

Acknowledgements

The research was partially supported by the Hasler Founda-
tion under Grant 16009, by the European Research Council
under Grant 692726 (H2020-ERC Project GlobalBioIm)
and by the SNF Project Grant (205121 163385).

References

Achim, A. and Kuruoglu, E. Image denoising using bivariate
α-stable distributions in the complex wavelet domain.
IEEE Signal Processing Letters, 12(1):17–20, 2005.

Hyvärinen, A. and Oja, E. Independent component analysis:
algorithms and applications. Neural networks, 13(4):
411–430, 2000.

Hyvärinen, A., Karhunen, J., and Oja, E.

Independent
component analysis, volume 46. John Wiley & Sons,
2004.

Kuruoglu, E. E., Fitzgerald, W. J., and Rayner, P. J. Near
optimal detection of signals in impulsive noise modeled
IEEE
with a symmetric/spl alpha/-stable distribution.
Communications Letters, 2(10):282–284, 1998.

Achim, A., Basarab, A., Tzagkarakis, G., Tsakalides, P.,
and Kouamé, D. Reconstruction of ultrasound RF echoes
modeled as stable random variables. IEEE Transactions
on Computational Imaging, 1(2):86–95, June 2015.

Lathauwer, L. De and Castaing, J. Blind identiﬁcation of
underdetermined mixtures by simultaneous matrix diago-
nalization. IEEE Transactions on Signal Processing, 56
(3):1096–1105, 2008.

Aharon, M., Elad, M., and Bruckstein, A. K-svd: An algo-
rithm for designing overcomplete dictionaries for sparse
representation. IEEE Transactions on signal processing,
54(11):4311–4322, 2006.

Lathauwer, L. De, Castaing, J., and Cardoso, J. Fourth-order
cumulant-based blind identiﬁcation of underdetermined
mixtures. IEEE Transactions on Signal Processing, 55
(6):2965–2973, 2007.

Amini, A., Unser, M., and Marvasti, F. Compressibility
of deterministic and random inﬁnite sequences. IEEE
Transactions on Signal Processing, 59(11):5193–5201,
November 2011.

Ling, S. Self-weighted least absolute deviation estimation
for inﬁnite variance autoregressive models. Journal of the
Royal Statistical Society: Series B (Statistical Methodol-
ogy), 67(3):381–393, 2005.

Bickson, D. and Guestrin, C. Inference with multivariate
heavy-tails in linear models. In Advances in Neural In-
formation Processing Systems, pp. 208–216, 2010.

Gallagher, C. A method for ﬁtting stable autoregressive
models using the autocovariation function. Statistics &
probability letters, 53(4):381–390, 2001.

Georgiou, P., Tsakalides, P., and Kyriakakis, C. Alpha-
stable modeling of noise and robust time-delay estimation
in the presence of impulsive noise. IEEE transactions on
multimedia, 1(3):291–301, 1999.

Gribonval, R. and Lesage, S. A survey of sparse component
analysis for blind source separation: principles, perspec-
tives, and new challenges. In ESANN’06 proceedings-
14th European Symposium on Artiﬁcial Neural Networks,
pp. 323–330. d-side publi., 2006.

Gribonval, R., Cevher, V., and Davies, M. E. Compress-
ible distributions for high-dimensional statistics. IEEE
Transactions on Information Theory, 58(8):5016–5034,
August 2012.

Helgason, S. Integral Geometry and Radon Transforms.

Springer, 2010.

Herault, J. and Jutten, C. Space or time adaptive signal
processing by neural network models. In Neural networks
for computing, volume 151, pp. 206–211. AIP Publishing,
1986.

Mairal, J., Bach, F., Ponce, J., and Sapiro, G. Online learn-
ing for matrix factorization and sparse coding. Journal of
Machine Learning Research, 11(Jan):19–60, 2010.

Marvasti, F., Amini, A., Haddadi, F., Soltanolkotabi, M.,
Khalaj, B.H., Aldroubi, A., Sanei, S., and Chambers, J. A
uniﬁed approach to sparse signal processing. EURASIP
journal on advances in signal processing, 2012(1):1,
2012.

Meerschaert, M. and Schefﬂer, H. Limit distributions for
sums of independent random vectors: Heavy tails in the-
ory and practice, volume 321. John Wiley & Sons, 2001.

Middleton, D. Non-gaussian noise models in signal process-
ing for telecommunications: new methods an results for
class a and class b noise models. IEEE Transactions on
Information Theory, 45(4):1129–1149, 1999.

Naik, G. and Kumar, D. An overview of independent com-
ponent analysis and its applications. Informatica, 35(1),
2011.

Nikias, C. L. and Shao, M. Signal Processing with Alpha-
Stable Distributions and Applications. Wiley, New York,
1995.

Nolan, JP. Modeling ﬁnancial data with stable distribu-
tions. Handbook of Heavy Tailed Distributions in Finance,
Handbooks in Finance: Book, 1:105–130, 2003.

Dictionary Learning Based on Sparse Distribution Tomography

Nolan, JP., Panorska, AK., and McCulloch, JH. Estimation
of stable spectral measures. Mathematical and Computer
Modelling, 34(9):1113–1122, 2001.

Pad, P. and Unser, M. Optimality of operator-like wavelets
for representing sparse AR(1) processes. IEEE Transac-
tions on Signal Processing, 63(18):4827–4837, Septem-
ber 2015.

Resnick, S. Heavy tail modeling and teletrafﬁc data: special
invited paper. The Annals of Statistics, 25(5):1805–1869,
1997.

Rolewicz, S. Metric Linear Spaces. Mathematics and
its applications (D. Reidel Publishing Company).: East
European series. D. Reidel, 1985.

Sahoo, S. and Makur, A. Signal recovery from random
measurements via extended orthogonal matching pursuit.
IEEE Trans. Signal Processing, 63(10):2572–2581, 2015.

Samoradnitsky, G. and Taqqu, M. Stable non-Gaussian ran-
dom processes: stochastic models with inﬁnite variance,
volume 1. CRC press, 1994.

Shao, M. and Nikias, C. L. Signal processing with frac-
tional lower order moments: stable processes and their
applications. Proceedings of the IEEE, 81(7):986–1010,
Jul 1993.

Spielman, D., Wang, H., and Wright, J. Exact recovery of
sparsely-used dictionaries. In COLT, pp. 37–1, 2012.

