Depth-Width Tradeoffs in Approximating Natural Functions with Neural
Networks

Itay Safran 1 Ohad Shamir 1

Abstract
We provide several new depth-based separation
results for feed-forward neural networks, proving
that various types of simple and natural functions
can be better approximated using deeper net-
works than shallower ones, even if the shallower
networks are much larger. This includes indi-
cators of balls and ellipses; non-linear functions
which are radial with respect to the L1 norm; and
smooth non-linear functions. We also show that
these gaps can be observed experimentally: In-
creasing the depth indeed allows better learning
than increasing width, when training neural net-
works to learn an indicator of a unit ball.

1. Introduction

Deep learning, in the form of artiﬁcial neural networks, has
seen a dramatic resurgence in the past recent years, achiev-
ing great performance improvements in various ﬁelds of
artiﬁcial intelligence such as computer vision and speech
recognition. While empirically successful, our theoretical
understanding of deep learning is still limited at best.

An emerging line of recent works has studied the expres-
sive power of neural networks: What functions can and
cannot be represented by networks of a given architecture
(see related work section below). A particular focus has
been the trade-off between the network’s width and depth:
On the one hand, it is well-known that large enough net-
works of depth 2 can already approximate any continuous
target function on [0, 1]d to arbitrary accuracy (Cybenko,
1989; Hornik, 1991). On the other hand, it has long been
evident that deeper networks tend to perform better than
shallow ones, a phenomenon supported by the intuition that
depth, providing compositional expressibility, is necessary
for efﬁciently representing some functions. Moreover, re-

1Weizmann Institute of Science, Rehovot, Israel. Corre-
spondence to: Itay Safran <itay.safran@weizmann.ac.il>, Ohad
Shamir <ohad.shamir@weizmann.ac.il>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

cent empirical evidence suggests that standard feedforward
deep networks are harder to optimize than shallower net-
works which lead to worse training error and testing error
(He et al., 2015).

To demonstrate the power of depth in neural networks, a
clean and precise approach is to prove the existence of
functions which can be expressed (or well-approximated)
by moderately-sized networks of a given depth, yet cannot
be approximated well by shallower networks, even if their
size is much larger. However, the mere existence of such
functions is not enough: Ideally, we would like to show
such depth separation results using natural, interpretable
functions, of the type we may expect neural networks to
successfully train on. Proving that depth is necessary for
such functions can give us a clearer and more useful insight
into what various neural network architectures can and can-
not express in practice.

In this paper, we provide several contributions to this
emerging line of work. We focus on standard, vanilla feed-
forward networks (using some ﬁxed activation function,
such as the popular ReLU), and measure expressiveness
directly in terms of approximation error, deﬁned as the ex-
pected squared loss with respect to some distribution over
the input domain. In this setting, we show the following:

• We prove that the indicator of the Euclidean unit ball,
x (cid:55)→ 1 ((cid:107)x(cid:107) ≤ 1) in Rd, which can be easily approx-
imated to accuracy (cid:15) using a 3-layer network with
O(d2/(cid:15)) neurons, cannot be approximated to an ac-
curacy higher than O(1/d4) using a 2-layer network,
unless its width is exponential in d. In fact, we show
the same result more generally, for any indicator of
an ellipsoid x (cid:55)→ 1 ((cid:107)Ax + b(cid:107) ≤ r) (where A is a
non-singular matrix and b is a vector). The proof is
based on a reduction from the main result of (Eldan &
Shamir, 2016), which shows a separation between 2-
layer and 3-layer networks using a more complicated
and less natural radial function.

• We prove that any L1

(cid:55)→
f ((cid:107)x(cid:107)1), where x ∈ Rd and f : R → R is
piecewise-linear, cannot be approximated to accuracy
(cid:15) by a depth 2 ReLU network of width less than

function x

radial

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

˜Ω(min{1/(cid:15), exp(Ω(d))}). In contrast, such functions
can be represented exactly by 3-layer ReLU networks.

• We show that this depth/width trade-off can also be
observed experimentally: Speciﬁcally, that when us-
ing standard backpropagation to learn the indicators
of the L1 and L2 unit balls, 3-layer nets give signif-
icantly better performance compared to 2-layer nets
(even if much larger). Our theoretical results indi-
cate that this gap in performance is due to approxi-
mation error issues. This experiment also highlights
the fact that our separation result is for a natural func-
tion that is not just well-approximated by some 3-layer
network, but can also be learned well from data using
standard methods.

• Finally, we prove that any member of a wide fam-
ily of non-linear and twice-differentiable functions
(including for instance x (cid:55)→ x2 in [0, 1]), which
can be approximated to accuracy (cid:15) using ReLU net-
works of depth and width O(poly(log(1/(cid:15)))), can-
not be approximated to similar accuracy by constant-
depth ReLU networks, unless their width is at least
Ω(poly(1/(cid:15))). We note that a similar result ap-
peared online concurrently and independently of ours
in (Yarotsky, 2016; Liang & Srikant, 2016), but the
setting is a bit different (see related work below for
more details).

RELATED WORK

The question of studying the effect of depth in neural net-
work has received considerable attention recently, and stud-
ied under various settings. Many of these works con-
sider a somewhat different setting than ours, and hence are
not directly comparable. These include networks which
are not plain-vanilla ones (e.g.
(Cohen et al., 2016; De-
lalleau & Bengio, 2011; Martens & Medabalimi, 2014)),
measuring quantities other than approximation error (e.g.
(Bianchini & Scarselli, 2014; Poole et al., 2016)), focus-
ing only on approximation upper bounds (e.g.
(Shaham
et al., 2016)), or measuring approximation error in terms
of L∞-type bounds, i.e. supx |f (x) − ˜f (x))| rather than
L2-type bounds Ex(f (x) − ˜f (x))2 (e.g. (Yarotsky, 2016;
Liang & Srikant, 2016)). We note that the latter distinc-
tion is important: Although L∞ bounds are more common
in the approximation theory literature, L2 bounds are more
natural in the context of statistical machine learning prob-
lems (where we care about the expected loss over some dis-
tribution). Moreover, L2 approximation lower bounds are
stronger, in the sense that an L2 lower bound easily trans-
lates to a lower bound on L∞ lower bound, but not vice
versa1.

1To give a trivial example, ReLU networks always express
continuous functions, and therefore can never approximate a dis-

A noteworthy paper in the same setting as ours is (Tel-
garsky, 2016), which proves a separation result between
the expressivity of ReLU networks of depth k and depth
o (k/ log (k)) (for any k).
This holds even for one-
dimensional functions, where a depth k network is shown
to realize a saw-tooth function with exp(O(k)) oscilla-
tions, whereas any network of depth o (k/ log (k)) would
require a width super-polynomial in k to approximate it by
more than a constant.
In fact, we ourselves rely on this
construction in the proofs of our results in section 5. On
the ﬂip side, in our paper we focus on separation in terms
of the accuracy or dimension, rather than a parameter k.
Moreover, the construction there relies on a highly oscil-
latory function, with Lipschitz constant exponential in k
almost everywhere. In contrast, in our paper we focus on
simpler functions, of the type that are likely to be learnable
from data using standard methods.

Our separation results in Sec. 5 (for smooth non-linear
functions) are closely related to those of (Yarotsky, 2016;
Liang & Srikant, 2016), which appeared online concur-
rently and independently of our work, and the proof ideas
are quite similar. However, these papers focused on L∞
bounds rather than L2 bounds. Moreover, (Yarotsky, 2016)
considers a class of functions different than ours in their
positive results, and (Liang & Srikant, 2016) consider net-
works employing a mix of ReLU and threshold activations,
whereas we consider a purely ReLU network.

Another relevant and insightful work is (Poggio et al.,
2016), which considers width vs. depth and provide gen-
eral results on expressibility of functions with a composi-
tional nature. However, the focus there is on worse-case ap-
proximation over general classes of functions, rather than
separation results in terms of speciﬁc functions as we do
here, and the details and setting is somewhat orthogonal to
ours.

2. Preliminaries

In general, we let bold-faced letters such as x =
(x1, . . . , xd) denote vectors, and capital letters denote ma-
trices or probabilistic events. (cid:107)·(cid:107) denotes the Euclidean
norm, and (cid:107)·(cid:107)1 the 1-norm. 1 (·) denotes the indicator
function. We use the standard asymptotic notation O(·) and
Ω(·) to hide constants, and ˜O(·) and ˜Ω(·) to hide constants
and factors logarithmic in the problem parameters.

Neural Networks. We consider feed-forward neural net-
works, computing functions from Rd to R. The network
is composed of layers of neurons, where each neuron com-
putes a function of the form x (cid:55)→ σ(w(cid:62)x + b), where w

continuous function such as x (cid:55)→ 1 (x ≥ 0) in an L∞ sense, yet
can easily approximate it in an L2 sense given any continuous
distribution.

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

1 x + b1), . . . , σ(w(cid:62)

is a weight vector, b is a bias term and σ : R (cid:55)→ R is a
non-linear activation function, such as the ReLU function
σ(z) = [z]+ = max{0, z}. Letting σ(W x + b) be a short-
hand for (cid:0)σ(w(cid:62)
n x + bn)(cid:1), we deﬁne a
layer of n neurons as x (cid:55)→ σ(W x + b). By denoting the
output of the ith layer as Oi, we can deﬁne a network of
arbitrary depth recursively by Oi+1 = σ(Wi+1Oi + bi+1),
where Wi, bi represent the matrix of weights and bias of
the ith layer, respectively. Following a standard convention
for multi-layer networks, the ﬁnal layer h is a purely linear
function with no bias, i.e. Oh = Wh · Oh−1. We deﬁne the
depth of the network as the number of layers l, and denote
the number of neurons ni in the ith layer as the size of the
layer. We deﬁne the width of a network as maxi∈{1,...,l} ni.
Finally, a ReLU network is a neural network where all the
non-linear activations are the ReLU function. We use “2-
layer” and “3-layer” to denote networks of depth 2 and 3.
In particular, in our notation a 2-layer ReLU network has
the form

n1(cid:88)

x (cid:55)→

vi · [w(cid:62)

i x + bi]+

i=1
parameters

some

d-
for
dimensional vectors w1, . . . , wn1. Similarly, a 3-layer
ReLU network has the form

v1, b1, . . . , vn1 , bn1

and

n2(cid:88)

i=1

ui


n1(cid:88)


j=1

vi,j

(cid:2)w(cid:62)

i,jx + bi,j

(cid:3)
+

+ ci





+

for some parameters {ui, vi,j, bi,j, ci, wi,j}.

Approximation error. Given some function f on a domain
X endowed with some probability distribution (with den-
sity function µ), we deﬁne the quality of its approximation
by some other function ˜f as (cid:82)
X (f (x) − ˜f (x))2µ(x)dx =
Ex∼µ[(f (x) − ˜f (x))2]. We refer to this as approxima-
tion in the L2-norm sense. In one of our results (Thm. 6),
we also consider approximation in the L∞-norm sense, de-
ﬁned as supx∈X |f (x) − ˜f (x)|. Clearly, this upper-bounds
the (square root of the) L2 approximation error deﬁned
above, so as discussed in the introduction, lower bounds
on the L2 approximation error (w.r.t. any distribution) are
stronger than lower bounds on the L∞ approximation error.

approximated with 3-layer networks, no 2-layer network
can approximate it to high accuracy w.r.t. any distribution,
unless its width is exponential in the dimension. This is
formally stated in the following theorem:

Theorem 1 (Inapproximability with 2-layer networks).
The following holds for some positive universal constants
c1, c2, c3, c4, and any network employing an activation
function satisfying Assumptions 1 and 2 in Eldan & Shamir
(2016): For any d > c1, and any non-singular ma-
trix A ∈ Rd×d, b ∈ Rd and r ∈ (0, ∞), there ex-
ists a continuous probability distribution γ on Rd, such
that for any function g computed by a 2-layer network of
width at most c3 exp(c4d), and for the function f (x) =
1 ((cid:107)Ax + b(cid:107) ≤ r), we have

(cid:90)

Rd

(f (x) − g(x))2 · γ(x)dx ≥

c2
d4 .

We note that the assumptions from (Eldan & Shamir, 2016)
are very mild, and apply to all standard activation functions,
including ReLU, sigmoid and threshold. For complete-
ness, the fully stated assumptions are presented in Subsec-
tion A.1

The formal proof of Thm. 1 (provided below) is based on a
reduction from the main result of (Eldan & Shamir, 2016),
which shows the existence of a certain radial function (de-
pending on the input x only through its norm) and a prob-
ability distribution which cannot be expressed by a 2-layer
network, whose width is less than exponential in the di-
mension d to more than constant accuracy. A closer look
at the proof reveals that this function (denoted as ˜g) can
be expressed as a sum of Θ(d2) indicators of L2 balls of
various radii. We argue that if we could have accurately
approximated a given L2 ball indicator with respect to all
distributions, then we could have approximated all the in-
dicators whose sum add up to ˜g, and hence reach a contra-
diction. By a linear transformation argument, we show the
same contradiction would have occured if we could have
approximated the indicators of an non-degenerate ellipse
with respect to any distribution. The formal proof is pro-
vided below:

3. Indicators of L2 Balls and Ellipsoids

We begin by considering one of the simplest possible func-
tion classes on Rd, namely indicators of L2 balls (and more
generally, ellipsoids). The ability to compute such func-
tions is necessary for many useful primitives, for exam-
ple determining if the distance between two points in Eu-
clidean space is below or above some threshold (either with
respect to the Euclidean distance, or a more general Maha-
lanobis distance). In this section, we show a depth separa-
tion result for such functions: Although they can be easily

Proof of Thm. 1. Assume by contradiction that for f as de-
scribed in the theorem, and for any distribution γ, there
exists a 2-layer network ˜fγ of width at most c3 exp(c4d),
such that

(cid:90)

x∈Rd

(cid:16)

f (x) − ˜fγ(x)

(cid:17)2

γ(x)dx ≤ (cid:15) ≤

c2
d4 .

Let ˆA and ˆb be a d × d non-singular matrix and vector re-
spectively, to be determined later. We begin by performing
a change of variables, y = ˆAx+ ˆb ⇐⇒ x = ˆA−1(y− ˆb),

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

dx =

(cid:90)

(cid:12)
(cid:12)
(cid:12)det
(cid:16)

(cid:16) ˆA−1(cid:17)(cid:12)
(cid:12)
(cid:12) · dy, which yields
(cid:17)(cid:17)

(cid:16) ˆA−1 (cid:16)

f

y − ˆb

− ˜fγ

y∈Rd

(cid:16) ˆA−1 (cid:16)

y − ˆb

(cid:17)(cid:17)

· γ

(cid:12)
(cid:12)
(cid:12)det

·

(cid:17)(cid:17)(cid:17)2

y − ˆb

(cid:16) ˆA−1 (cid:16)
(cid:16) ˆA−1(cid:17)(cid:12)
(cid:12)
(cid:12) · dy ≤ (cid:15). (1)

In particular, let us choose the distribution γ deﬁned as
γ(z) = | det( ˆA)| · µ( ˆAz + ˆb), where µ is the (continuous)
distribution used in the main result of (Eldan & Shamir,
2016) (note that γ is indeed a distribution, since (cid:82)
z γ (z) =
(det( ˆA)) (cid:82)
z µ( ˆAz+ ˆb)dz, which by the change of variables
x = ˆAz + ˆb, dx = | det( ˆA)|dz equals (cid:82)
x µ(x)dx = 1).
Plugging the deﬁnition of γ in Eq. (1), and using the fact
that | det( ˆA−1)| · | det( ˆA)| = 1, we get

(cid:90)

y∈Rd

(cid:16)

f

(cid:16) ˆA−1 (cid:16)

y − ˆb

(cid:17)(cid:17)

− ˜fγ

(cid:16) ˆA−1 (cid:16)

y − ˆb

(cid:17)(cid:17)(cid:17)2

· µ (y) dy ≤ (cid:15).

(2)

Letting z > 0 be an arbitrary parameter, we now pick ˆA =
r A and ˆb = z
z
r b. Recalling the deﬁnition of f as x (cid:55)→
1 ((cid:107)Ax + b(cid:107) ≤ r), we get that

(cid:90)

(cid:16)

y∈Rd

1 ((cid:107)y(cid:107) ≤ z) − ˜fγ

A−1 (cid:16)

(cid:16) r
z

y −

b

z
r

(cid:17)(cid:17)(cid:17)2

z A−1 (cid:0)y − z
(cid:0) r

· µ (y) dy ≤ (cid:15).
(3)
r b(cid:1)(cid:1) expresses a 2-layer net-
Note that ˜fγ
work composed with a linear transformation of the input,
and hence can be expressed in turn by a 2-layer network
(as we can absorb the linear transformation into the param-
eters of each neuron in the ﬁrst layer). Therefore, letting
y f 2(y)dy denote the norm in L2(µ) func-
(cid:107)f (cid:107)L2(µ) =
tion space, we showed the following: For any z > 0, there
exists a 2-layer network ˜fz such that
(cid:17)(cid:13)
(cid:13)
(cid:13)L2(µ)

1 ((cid:107)·(cid:107) ≤ z) − ˜fz (·)

(cid:113)(cid:82)

(cid:13)
(cid:13)
(cid:13)

(4)

√

≤

(cid:15).

(cid:16)

With this key result in hand, we now turn to complete the
proof. We consider the function ˜g from (Eldan & Shamir,
2016), for which it was proven that no 2-layer network can
approximate it w.r.t. µ to better than constant accuracy,
unless its width is exponential in the dimension d. In par-
ticular ˜g can be written as

˜g(x) =

(cid:15)i · 1 ((cid:107)x(cid:107) ∈ [ai, bi]) ,

n
(cid:88)

i=1

where [ai, bi] are disjoint intervals, (cid:15)i ∈ {−1, +1}, and
n = Θ(d2) where d is the dimension. Since ˜g can also be
written as

n
(cid:88)

i=1

(cid:15)i (1 ((cid:107)x(cid:107) ≤ bi) − 1 ((cid:107)x(cid:107) ≤ ai)) ,

we get by Eq. (4) and the triangle inequality that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

˜g(·) −

(cid:15)i · ( ˜fbi(·) − ˜fai(·)

≤

(cid:18)(cid:13)
(cid:16)
(cid:13)
(cid:13)

|(cid:15)i|

n
(cid:88)

i=1

1 ((cid:107)·(cid:107) ≤ bi) − ˜fbi

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)L2(µ)
(cid:17)(cid:13)
(cid:13)
(cid:13)L2(µ)

+

(cid:13)
(cid:13)
(cid:13)1 ((cid:107)·(cid:107) ≤ ai) − ˜fai(·)
(cid:13)
(cid:13)
(cid:13)L2(µ)

(cid:19)

√

≤ 2n

(cid:15).

However, since a linear combination of 2n 2-layer neural
networks of width at most w is still a 2-layer network, of
width at most 2nw, we get that (cid:80)n
i=1 (cid:15)i · ( ˜fbi(·) − ˜fai(·))
is a 2-layer network, of width at most Θ(d2) · c3 exp(c4d),
(cid:15) =
which approximates ˜g to an accuracy of less than 2n
√
Θ(d2) · (cid:112)c2/d4 = Θ(1) ·
c2. Hence, by picking c2, c3, c4
sufﬁciently small, we get a contradiction to the result of
(Eldan & Shamir, 2016), that no 2-layer network of width
smaller than c exp(cd) (for some constant c) can approx-
imate ˜g to more than constant accuracy, for a sufﬁciently
large dimension d.

√

i=1 x2

To complement Thm. 1, we also show that such indica-
tor functions can be easily approximated with 3-layer net-
works. The argument is quite simple: Using an activation
such as ReLU or Sigmoid, we can use one layer to approx-
imate any Lipschitz continuous function on any bounded
interval, and in particular x (cid:55)→ x2. Given a vector x ∈ Rd,
we can apply this construction on each coordinate xi seper-
ately, hence approximating x (cid:55)→ (cid:107)x(cid:107)2 = (cid:80)d
i . Sim-
ilarly, we can approximate x (cid:55)→ (cid:107)Ax + b(cid:107) for arbitrary
ﬁxed matrices A and vectors b. Finally, with a 3-layer net-
work, we can use the second layer to compute a continuous
approximation to the threshold function z (cid:55)→ 1 (z ≤ r).
Composing these two layers, we get an arbitrarily good ap-
proximation to the function x (cid:55)→ 1 ((cid:107)Ax + b(cid:107) ≤ r) w.r.t.
any continuous distribution, with the network size scaling
polynomially with the dimension d and the required accu-
racy.
In the theorem below, we formalize this intuition,
where for simplicity we focus on approximating the indi-
cator of the unit ball:
Theorem 2 (Approximability with 3-layer networks).
Given δ > 0, for any activation function σ satisfying As-
sumption 1 in Eldan & Shamir (2016) and any continuous
probability distribution µ on Rd, there exists a constant cσ
dependent only on σ, and a function g expressible by a 3-
(cid:111)
,
layer network of width at most max
such that the following holds:

8cσd2/δ, cσ

(cid:112)1/2δ

(cid:110)

(cid:90)

Rd

(g (x) − 1 ((cid:107)x(cid:107)2 ≤ 1))2 µ (x) dx ≤ δ,

where cσ is a constant depending solely on σ.

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

The proof of the theorem appears in the supplementary ma-
terial

3.1. An Experiment

In this subsection, we empirically demonstrate that indica-
tor functions of L2 balls are indeed easier to learn with a
3-layer network, compared to a 2-layer network (even if
the 2-layer network is signiﬁcantly larger). This indicates
that the depth/width trade-off for indicators of balls, pre-
dicted by our theory, can indeed be observed experimen-
tally. Moreover, it highlights the fact that our separation
result is for simple natural functions, that can be learned
reasonably well from data using standard methods.

For our experiment, we sampled 5 · 105 data instances in
R100, with a direction chosen uniformly at random and a
norm drawn uniformly at random from the interval [0, 2].
To each instance, we associated a target value computed
according to the target function f (x) = 1 ((cid:107)x(cid:107)2 ≤ 1). An-
other 5 · 104 examples were generated in a similar manner
and used as a validation set.

We trained 5 ReLU networks on this dataset:

• One 3-layer network, with a ﬁrst hidden layer of size
100, a second hidden layer of size 20, and a linear
output neuron.

• Four 2-layer networks, with hidden layer of sizes
100, 200, 400 and 800, and a linear output neuron.

Training was performed with backpropagation, using the
TensorFlow library. We used the squared loss (cid:96)(y, y(cid:48)) =
(y − y(cid:48))2 and batches of size 100. For all networks, we
chose a momentum parameter of 0.95, and a learning rate
starting at 0.1, decaying by a multiplicative factor of 0.95
every 1000 batches, and stopping at 10−4.

The results are presented in Fig. 1. As can be clearly seen,
the 3-layer network achieves signiﬁcantly better perfor-
mance than the 2-layer networks. This is true even though
some of these networks are signiﬁcantly larger and with
more parameters (for example, the 2-layer, width 800 net-
work has ˜80K parameters, vs. ˜10K parameters for the 3-
layer network). This gap in performance is the exact oppo-
site of what might be expected based on parameter counting
alone. Moreover, increasing the width of the 2-layer net-
works exhibits diminishing returns: The performance im-
provement in doubling the width from 100 to 200 is much
larger than doubling the width from 200 to 400 or 400 to
800. This indicates that one would need a much larger 2-
layer network to match the 3-layer, width 100 network’s
performance. Thus, we conclude that the network’s depth
indeed plays a crucial role, and that 3-layer networks are
inherently more suitable to express indicator functions of
the type we studied.

Figure 1. The experiment results, depicting the network’s root
mean square error over the training set (top) and validation set
(bottom), as a function of the number of batches processed. Best
viewed in color.

4. L1 Radial Functions; ReLU Networks

Having considered functions depending on the L2 norm,
we now turn to consider functions depending on the L1
norm. Focusing on ReLU networks, we will show a cer-
tain separation result holding for any non-linear function,
which depends on the input x only via its 1-norm (cid:107)x(cid:107)1.
Theorem 3. Let f : [0, ∞) (cid:55)→ R be a function such that
for some r, δ > 0 and (cid:15) ∈ (0, 1/2),

inf
a,b∈R

Ex uniform on [r,(1+(cid:15))r][(f (x) − (ax − b))2] > δ .

Then there exists a distribution γ over {x : (cid:107)x(cid:107)1 ≤ (1 +
(cid:15))r}, such that if a 2-layer ReLU network F (x) satisﬁes

(cid:90)

x

(f ((cid:107)x(cid:107)1) − F (x))2 γ(x)dx ≤ δ/2,

then its width must be at least ˜Ω(min {1/(cid:15), exp(Ω(d))})
(where the ˜Ω notation hides constants and factors logarith-
mic in (cid:15), d).

The proof appears in the supplementary material. We note
that δ controls how ‘linearly inapproximable’ is f in a nar-
row interval (of width (cid:15)) around r, and that δ is gener-
ally dependent on (cid:15). To give a concrete example, suppose
that f (z) = [z − 1]+, which cannot be approximated by
a linear function to an accuracy better than O((cid:15)2) in an (cid:15)-
neighborhood of 1. By taking r = 1− (cid:15)
2 and δ = O((cid:15)2), we
get that no 2-layer network can approximate the function

Batch number (x1000)020406080100120140160180200RMSE (training set)0.120.140.160.180.20.220.240.260.280.33-layer, width 1002-layer, width 1002-layer, width 2002-layer, width 4002-layer, width 800Batch number (x1000)020406080100120140160180200RMSE (validation set)0.150.20.250.33-layer, width 1002-layer, width 1002-layer, width 2002-layer, width 4002-layer, width 800Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

[(cid:107)x(cid:107)1 − 1]+ (at least with respect to some distribution), un-
less its width is ˜Ω(min {1/(cid:15), exp(Ω(d))}). On the ﬂip side,
f ((cid:107)x(cid:107)1) can be expressed exactly by a 3-layer, width 2d
ReLU network: x (cid:55)→ [(cid:80)d
i=1([xi]+ + [−xi]+) − 1]+, where
the output neuron is simply the identity function. The same
argument would work for any piecewise-linear f . More
generally, the same kind of argument would work for any
function f exhibiting a non-linear behavior at some points:
Such functions can be well-approximated by 3-layer net-
works (by approximating f with a piecewise-linear func-
tion), yet any approximating 2-layer network will have a
lower bound on its size as speciﬁed in the theorem.

Intuitively, the proof relies on showing that any good 2-
layer approximation of f ((cid:107)x(cid:107)1) must capture the non-
linear behavior of f close to “most” points x satisfying
(cid:107)x(cid:107)1 ≈ r. However, a 2-layer ReLU network x (cid:55)→
(cid:80)N
j=1 aj [(cid:104)wj, x(cid:105) + bj]+ is piecewise linear, with non-
linearities only at the union of the N hyperplanes ∪j{x :
(cid:104)wj, x(cid:105) + bj = 0}. This implies that “most” points x
s.t. (cid:107)x(cid:107)1 ≈ r must be (cid:15)-close to a hyperplane {x :
(cid:104)wj, x(cid:105) + bj = 0}. However, the geometry of the L1 ball
{x : (cid:107)x(cid:107) = r} is such that the (cid:15) neighborhood of any sin-
gle hyperplane can only cover a “small” portion of that ball,
yet we need to cover most of the L1 ball. Using this and
an appropriate construction, we show that required number
of hyperplanes is at least 1/(cid:15), as long as (cid:15) > exp(−O(d))
(and if (cid:15) is smaller than that, we can simply use one neu-
ron/hyperplane for each of the 2d facets of the L1 ball, and
get a covering using 2d neurons/hyperplanes). The formal
proof appears in the supplementary material.

We note that the bound in Thm. 3 is of a weaker nature
than the bound in the previous section, in that the lower
bound is only polynomial rather than exponential (albeit
w.r.t. different problem parameters: (cid:15) vs. d). Nevertheless,
we believe this does point out that L1 balls also pose a ge-
ometric difﬁculty for 2-layer networks, and conjecture that
our lower bound can be considerably improved: Indeed, at
the moment we do not know how to approximate a function
such as x (cid:55)→ [(cid:107)x(cid:107)1 − 1]+ with 2-layer networks to better
than constant accuracy, using less than Ω(2d) neurons.

Finally, we performed an experiment similar to the one pre-
sented in Subsection 3.1, where we veriﬁed that the bounds
we derived are indeed reﬂected in differences in empiri-
cal performance, when training 2-layer nets versus 3-layer
nets. The reader is referred to Sec. B for the full details of
the experiment and its results.

5. C 2 Nonlinear Functions; ReLU Networks

In this section, we establish a depth separation result for ap-
proximating continuously twice-differentiable (C 2) func-
tions using ReLU neural networks. Unlike the previous re-

sults in this paper, the separation is for depths which can be
larger than 3, depending on the required approximation er-
ror. Also, the results will all be with respect to the uniform
distribution µd over [0, 1]d. As mentioned earlier, the re-
sults and techniques in this section are closely related to the
independent results of (Yarotsky, 2016; Liang & Srikant,
2016), but our emphasis is on L2 rather than L∞ approx-
imation bounds, and we focus on somewhat different net-
work architectures and function classes.

Clearly, not all C 2 functions are difﬁcult to approximate
(e.g. a linear function can be expressed exactly with a 2-
layer network). Instead, we consider functions which have
a certain degree of non-linearity, in the sense that its Hes-
sians are non-zero along some direction, on a signiﬁcant
portion of the domain. Formally, we make the following
deﬁnition:

Deﬁnition 1. Let µd denote the uniform distribution on
[0, 1]d. For a function f : [0, 1]d → R and some λ > 0,
denote

σλ (f ) =

sup
v∈Sd−1, U ∈U s.t. v(cid:62)H(f )(x)v≥λ ∀x∈U

µd (U ) ,

where Sd−1 = {x : (cid:107)x(cid:107)2 = 1} is the d-dimensional unit
hypersphere, and U is the set of all connected and measur-
able subsets of [0, 1]d.

In words, σλ (f ) is the measure (w.r.t. the uniform distri-
bution on [0, 1]d) of the largest connected set in the domain
of f , where at any point, f has curvature at least λ along
some ﬁxed direction v. The “prototypical” functions f we
are interested in is when σλ(f ) is lower bounded by a con-
it is 1 if f is strongly convex). We stress that
stant (e.g.
our results in this section will hold equally well by consid-
ering the condition v(cid:62)H(f )(x)v ≤ −λ as well, however
for the sake of simplicity we focus on the former condition
appearing in Def. 1. Our goal is to show a depth separa-
tion result inidividually for any such function (that is, for
any such function, there is a gap in the attainable error be-
tween deeper and shallower networks, even if the shallow
network is considerably larger).

As usual, we start with an inapproximability result. Speciﬁ-
cally, we prove the following lower bound on the attainable
approximation error of f , using a ReLU neural network of
a given depth and width:
Theorem 4. For any C 2 function f : [0, 1]d → R, any
λ > 0, and any function g on [0, 1]d expressible by a ReLU
network of depth l and maximal width m, it holds that

(cid:90)

[0,1]d

(f (x) − g(x)2µd (x) dx ≥

c · λ2 · σ5
λ
(2m)4l

,

where c > 0 is a universal constant.

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

The theorem conveys a key tradeoff between depth and
width when approximating a C 2 function using ReLU net-
works: The error cannot decay faster than polynomially in
the width m, yet the bound deteriorates exponentially in the
depth l. As we show later on, this deterioration does not
stem from the looseness in the bound: For well-behaved f ,
it is indeed possible to construct ReLU networks, where the
approximation error decays exponentially with depth.

The proof of Thm. 4 appears in the supplementary mate-
rial, and is based on a series of intermediate results. First,
we show that any strictly curved function (in a sense sim-
ilar to Deﬁnition 1) cannot be well-approximated in an
L2 sense by piecewise linear functions, unless the num-
ber of linear regions is large. To that end, we ﬁrst estab-
lish some necessary tools based on Legendre polynomi-
als. We then prove a result speciﬁc to the one-dimensional
case, including an explicit lower bound if the target func-
tion is quadratic (Thm. 9) or strongly convex or concave
(Thm. 10). We then expand the construction to get an er-
ror lower bound in general dimension d, depending on the
number of linear regions in the approximating piecewise-
linear function. Finally, we note that any ReLU network
induces a piecewise-linear function, and bound the number
of linear regions induced by a ReLU network of a given
width and depth (using a lemma borrowed from (Telgar-
sky, 2016)). Combining this with the previous lower bound
yields Thm. 4.

We now turn to complement this lower bound with an ap-
proximability result, showing that with more depth, a wide
family of functions to which Thm. 4 applies can be ap-
proximated with exponentially high accuracy. Speciﬁcally,
we consider functions which can be approximated using a
moderate number of multiplications and additions, where
the values of intermediate computations are bounded (for
example, a special case is any function approximable by a
moderately-sized Boolean circuit, or a polynomial).

The key result to show this is the following, which implies
that the multiplication of two (bounded-size) numbers can
be approximated by a ReLU network, with error decaying
exponentially with depth:
Theorem 5. Let f : [−M, M ]2 → R, f (x, y) = x · y and
let (cid:15) > 0 be arbitrary. Then exists a ReLU neural network
(cid:1)(cid:7) + 9
g of width 4 (cid:6)log (cid:0) M
satisfying

(cid:1)(cid:7) + 13 and depth (cid:6)2 log (cid:0) M

(cid:15)

(cid:15)

sup
(x,y)∈[−M,M ]2

|f (x, y) − g (x, y)| ≤ (cid:15).

The idea of the construction is that depth allows us to com-
pute highly-oscillating functions, which can extract high-
order bits from the binary representation of the inputs.
Given these bits, one can compute the product by a pro-
cedure resembling long multiplication, as shown in Fig. 2,

Figure 2. ReLU approximation of the function x (cid:55)→ x2 obtained
by extracting 5 bits. The number of linear segments grows expo-
nentially with the number of bits and the approximating network
size.

and formally proven as follows:

Proof of Thm. 5. We begin by observing that by using a
simple linear change of variables on x, we may assume
without loss of generality that x ∈ [0, 1], as we can just
rescale x to the interval [0, 1], and then map it back to its
original domain [−M, M ], where the error will multiply by
a factor of 2M . Then by requiring accuracy (cid:15)
2M instead of
(cid:15), the result will follow.

The key behind the proof is that performing bit-wise oper-
ations on the ﬁrst k bits of x ∈ [0, 1] yields an estimation
of the product to accuracy 21−kM . Let x = (cid:80)∞
i=1 2−ixi
be the binary representation of x where xi is the ith bit of
x, then

=

2−ixi · y +

2−ixi · y.

(5)

2−ixi · y

≤

2−i · y

= 2−k |y| ≤ 21−kM,

x · y =

2−ixi · y

∞
(cid:88)

i=1

k
(cid:88)

i=1

But since
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞
(cid:88)

i=k+1

Eq. (5) implies

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞
(cid:88)

i=k+1

∞
(cid:88)

i=k+1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

k
(cid:88)

i=1

x · y −

2−ixi · y

≤ 21−kM.

Requiring that 22−kM ≤ (cid:15)
2M , it sufﬁces to show the
existence of a network which approximates the function
2 , where k = 2 (cid:6)log (cid:0) 8M
(cid:1)(cid:7).
(cid:80)k

i=1 2−ixi · y to accuracy (cid:15)

(cid:15)

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

This way both approximations will be at most (cid:15)
in the desired accuracy of (cid:15).

2 , resulting

Before specifying the architecture which extracts the ith bit
of x, we ﬁrst describe the last 2 layers of the network. Let
the penultimate layer comprise of k neurons, each receiv-
ing both y and xi as input, and having the set of weights
(cid:0)2−i, 1, −1(cid:1). Thus, the output of the ith neuron in the
penultimate layer is

(cid:2)2−iy + xi − 1(cid:3)

+ = 2−ixiy.

Let the ﬁnal single output neuron have the set of weights
(1, . . . , 1, 0) ∈ Rk+1, this way, the output of the network
will be (cid:80)k

i=1 2−ixi · y as required.

We now specify the architecture which extracts the ﬁrst
most signiﬁcant k bits of x. In Telgarsky (2016), the au-
thor demonstrates how the composition of the function

ϕ (x) = [2x]+ − [4x − 2]+

with itself i times, ϕi, yields a highly oscillatory triangle
wave function in the domain [0, 1]. Furthermore, we ob-
serve that ϕ (x) = 0 ∀x ≤ 0, and thus ϕi (x) = 0 ∀x ≤ 0.
Now, a linear shift of the input of ϕi by 2−i−1, and com-
posing the output with

σδ (x) =

x −

+

(cid:20) 1
2δ

1
4δ

(cid:21)

1
2

+

−

(cid:20) 1
2δ

x −

−

1
4δ

(cid:21)

1
2

,

+

which converges to 1[x≥0.5] (x) as δ → 0, results in an ap-
(cid:0)ϕi (cid:0)x − 2−i−1(cid:1)(cid:1) . We stress
proximation of x (cid:55)→ xi: σδ
that choosing δ such that the network approximates the bit-
2 will require δ to be of mag-
wise product
nitude 1
(cid:15) , but this poses no problem as representing such a
(cid:1) bits, which is also the magnitude
number requires log (cid:0) 1
of the size of the network, as suggested by the following
analysis.

to accuracy (cid:15)

(cid:15)

Next, we compute the size of the network required to im-
plement the above approximation. To compute ϕ only two
neurons are required, therefore ϕi can be computed using
i layers with 2 neurons in each, and ﬁnally composing this
with σδ requires a subsequent layer with 2 more neurons.
To implement the ith bit extractor we therefore require a
network of size 2×(i + 1). Using dummy neurons to prop-
agate the ith bit for i < k, the architecture extracting the
k most signiﬁcant bits of x will be of size 2k × (k + 1).
Adding the ﬁnal component performing the multiplication
estimation will require 2 more layers of width k and 1 re-
spectively, and an increase of the width by 1 to propagate
y to the penultimate layer, resulting in a network of size
(2k + 1) × (k + 1).

Thm. 5 shows that multiplication can be performed very
accurately by deep networks. Moreover, additions can be

computed by ReLU networks exactly, using only a sin-
gle layer with 4 neurons: Let α, β ∈ R be arbitrary, then
(x, y) (cid:55)→ α · x + β · y is given in terms of ReLU summation
by

α [x]+ − α [−x]+ + β [y]+ − β [−y]+ .

Repeating these arguments, we see that any function which
can be approximated by a bounded number of operations
involving additions and multiplications, can also be ap-
proximated well by moderately-sized networks. This is
formalized in the following theorem, which provides an ap-
proximation error upper bound (in the L∞ sense, which is
stronger than L2 for upper bounds):

Theorem 6. Let Ft,M,(cid:15) be the family of functions on the
domain [0, 1]d with the property that f ∈ Ft,M,(cid:15) is ap-
proximable to accuracy (cid:15) with respect to the inﬁnity norm,
using at most t operations involving weighted addition,
(x, y) (cid:55)→ α · x + β · y, where α, β ∈ R are ﬁxed; and mul-
tiplication, (x, y) (cid:55)→ x · y, where each intermediate com-
putation stage is bounded in the interval [−M, M ]. Then
there exists a universal constant c, and a ReLU network g
(cid:1) + t2 log (M )(cid:1), such
of width and depth at most c (cid:0)t log (cid:0) 1
that

(cid:15)

sup
x∈[0,1]d

|f (x) − g (x)| ≤ 2(cid:15).

As discussed in Sec. 2, this type of L∞ approximation
bound implies an L2 approximation bound with respect
to any distribution. The proof of the theorem appears in
Sec. A.

Combining Thm. 4 and Thm. 6, we can state the follow-
ing corollary, which formally shows how depth can be ex-
ponentially more valuable than width as a function of the
target accuracy (cid:15):
Corollary 1. Suppose f ∈ C 2 ∩Ft((cid:15)),M ((cid:15)),(cid:15), where t ((cid:15)) =
O (poly (log (1/(cid:15)))) and M ((cid:15)) = O (poly (1/(cid:15))). Then
approximating f to accuracy (cid:15) in the L2 norm using a ﬁxed
depth ReLU network requires width at least poly(1/(cid:15)),
whereas there exists a ReLU network of depth and width at
most p (log (1/(cid:15))) which approximates f to accuracy (cid:15) in
the inﬁnity norm, where p is a polynomial depending solely
on f .

Proof. The lower bound follows immediately from Thm. 4.
For the upper bound, observe that Thm. 6 implies an (cid:15) ap-
proximation by a network of width and depth at most

(cid:16)

(cid:17)
t ((cid:15)/2) log (2/(cid:15)) + (t ((cid:15)/2))2 log (M ((cid:15)/2))

,

c

which by the assumption of Corollary 1, can be bounded by
p (log (1/(cid:15))) for some polynomial p which depends solely
on f .

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

Shaham, Uri, Cloninger, Alexander,

and Coifman,
Ronald R. Provable approximation properties for deep
neural networks. Applied and Computational Harmonic
Analysis, 2016.

Telgarsky, Matus. Beneﬁts of depth in neural networks.

arXiv preprint arXiv:1602.04485, 2016.

Yarotsky, Dmitry. Error bounds for approximations with
deep relu networks. arXiv preprint arXiv:1610.01145,
2016.

Acknowledgements

This research is supported in part by an FP7 Marie Curie
CIG grant, Israel Science Foundation grant 425/13, and
the Intel ICRI-CI Institute. We would like to thank Shai
Shalev-Shwartz for some illuminating discussions, and
Eran Amar for his valuable help with the experiments.

References

Bianchini, M. and Scarselli, F. On the complexity of shal-
In ESANN,

low and deep neural network classiﬁers.
2014.

Cohen, Nadav, Sharir, Or, and Shashua, Amnon. On the
expressive power of deep learning: A tensor analysis. In
29th Annual Conference on Learning Theory, pp. 698–
728, 2016.

Cybenko, George. Approximation by superpositions of a
sigmoidal function. Mathematics of control, signals and
systems, 2(4):303–314, 1989.

Delalleau, O. and Bengio, Y. Shallow vs. deep sum-product

networks. In NIPS, pp. 666–674, 2011.

Eldan, Ronen and Shamir, Ohad. The power of depth for
In 29th Annual Confer-

feedforward neural networks.
ence on Learning Theory, pp. 907–940, 2016.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep residual learning for image recognition. arXiv
preprint arXiv:1512.03385, 2015.

Hornik, Kurt. Approximation capabilities of multilayer
feedforward networks. Neural networks, 4(2):251–257,
1991.

Liang, Shiyu and Srikant, R. Why deep neural networks?

arXiv preprint arXiv:1610.04161, 2016.

Martens, J. and Medabalimi, V. On the expressive ef-
arXiv preprint

ﬁciency of sum product networks.
arXiv:1411.7717, 2014.

Poggio, Tomaso, Mhaskar, Hrushikesh, Rosasco, Lorenzo,
Miranda, Brando, and Liao, Qianli.
Why and
when can deep–but not shallow–networks avoid the
arXiv preprint
curse of dimensionality:
arXiv:1611.00740, 2016.

a review.

Poole, Ben, Lahiri, Subhaneil, Raghu, Maithreyi, Sohl-
Dickstein, Jascha, and Ganguli, Surya. Exponential
expressivity in deep neural networks through transient
In Advances In Neural Information Processing
chaos.
Systems, pp. 3360–3368, 2016.

