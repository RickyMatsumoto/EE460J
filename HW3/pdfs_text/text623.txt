Sketched Ridge Regression: Optimization Perspective,
Statistical Perspective, and Model Averaging

Shusen Wang 1 Alex Gittens 2 Michael W. Mahoney 1

Abstract

We address the statistical and optimization im-
pacts of using classical sketch versus Hessian
sketch to solve approximately the Matrix Ridge
Regression (MRR) problem. Prior research has
considered the effects of classical sketch on least
squares regression (LSR), a strictly simpler prob-
lem. We establish that classical sketch has a
similar effect upon the optimization properties
of MRR as it does on those of LSR—namely,
it recovers nearly optimal solutions. In contrast,
Hessian sketch does not have this guarantee; in-
stead, the approximation error is governed by a
subtle interplay between the “mass” in the re-
sponses and the optimal objective value. For
both types of approximations, the regularization
in the sketched MRR problem gives it signif-
icantly different statistical properties from the
sketched LSR problem.
In particular, there is
a bias-variance trade-off in sketched MRR that
is not present in sketched LSR. We provide up-
per and lower bounds on the biases and vari-
ances of sketched MRR; these establish that the
variance is signiﬁcantly increased when classical
sketches are used, while the bias is signiﬁcantly
increased when using Hessian sketches. Empiri-
cally, sketched MRR solutions can have risks that
are higher by an order-of-magnitude than those
of the optimal MRR solutions. We establish the-
oretically and empirically that model averaging
greatly decreases this gap. Thus, in the dis-
tributed setting, sketching combined with model
averaging is a powerful technique that quickly
obtains near-optimal solutions to the MRR prob-

1International Computer Science Institute and Department of
Statistics, University of California at Berkeley, USA 2Department
of Computer Science, Rensselaer Polytechnic Institute, USA.
Correspondence to: Shusen Wang <shusen@berkeley.edu>,
Alex Gittens <gittea@rpi.edu>, Michael W. Mahoney <mma-
honey@stat.berkeley.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

lem while greatly mitigating the statistical risks
incurred by sketching.

1. Introduction

1 ; . . . , xT

Regression is one of the most fundamental problems in ma-
chine learning. The simplest and most thoroughly studied
regression model is least squares regression (LSR). Given
n ] ∈ Rn×d and responses y =
features X = [xT
[y1, . . . , yn]T ∈ Rn, the LSR problem minw (cid:107)Xw − y(cid:107)2
2
can be solved in O(nd2) time using the QR decomposition
or in O(ndt) time using accelerated gradient descent algo-
rithms. Here, t is the number of iterations, which depends
on the initialization, the condition number of X, and the
stopping criterion.

This paper considers the n (cid:29) d problem, where there
is much redundancy in X. Matrix sketching, as used
within Randomized Linear Algebra (RLA) (Mahoney,
2011; Woodruff, 2014), works by reducing the size of X
without losing too much information; this operation can
be modeled as taking actual rows or linear combinations
of the rows of X with a sketching matrix S to form the
sketch ST X. Here S ∈ Rn×s satisﬁes d < s (cid:28) n so that
ST X generically has the same rank but much fewer rows
as X. Sketching has been used to speed up LSR (Drineas
et al., 2006; 2011; Clarkson & Woodruff, 2013; Meng &
Mahoney, 2013; Nelson & Nguyˆen, 2013) by solving the
sketched LSR problem minw (cid:107)ST Xw − ST y(cid:107)2
2 instead
of the original LSR problem. Solving sketched LSR costs
either O(sd2 + Ts) time using the QR decomposition or
O(sdt + Ts) time using accelerated gradient descent al-
gorithms, where t is as deﬁned previously1 and Ts is the
time cost of sketching. For example, Ts = O(nd log s)
when S is the subsampled randomized Hadamard trans-
form (Drineas et al., 2011), and Ts = O(nd) when S is
a CountSketch matrix (Clarkson & Woodruff, 2013).

There has been much work in RLA on analyzing the
quality of sketched LSR with different sketching methods
and different objectives; see the reviews (Mahoney, 2011;

1The condition number of XT SST X is very close to that of

XT X, and thus the number of iterations t is almost unchanged.

Sketched Ridge Regression

Table 1. The time cost of the solutions to MRR. Here Ts(X) and
Ts(Y) denote the time cost of forming the sketches ST X ∈
Rs×d and ST Y ∈ Rs×m.

Optimal
Classical
Hessian

Deﬁnition
(2)
(3)
(4)

Time
O(nd2 + nmd)
O(sd2 + smd) + Ts(X) + Ts(Y)
O(sd2 + nmd) + Ts(X)

Woodruff, 2014) and the references therein.

The concept of sketched LSR originated in the theoreti-
cal computer science literature, e.g., (Drineas et al., 2006;
2011), where the behavior of sketched LSR was studied
from an optimization perspective. Let w(cid:63) be the optimal
LSR solution and ˜w be the solution to sketched LSR. This
line of work established that if s = O(d/(cid:15) + poly(d)),
then the objective function value (cid:107)X ˜w − y(cid:13)
2
2 is at most (cid:15)
(cid:13)
times worse than (cid:107)Xw(cid:63) − y(cid:13)
2
2. These works also bounded
(cid:13)
(cid:107) ˜w − w(cid:63)(cid:107)2
2 in terms of the difference in the objective func-
tion values and the condition number of XT X.

A more recent line of work has studied sketched LSR from
a statistical perspective: (Ma et al., 2015; Raskutti & Ma-
honey, 2016; Pilanci & Wainwright, 2015; Wang et al.,
2016b) considered statistical properties of sketched LSR
like the bias and variance. In particular, Pilanci & Wain-
wright (2015) showed that sketched LSR has much higher
variance than the optimal solution.

Both of these perspectives are important and of practical
interest. The optimization perspective is relevant when the
data can be taken as deterministic values. The statistical
perspective is relevant in machine learning and statistics
applications where the data are random, and the regression
coefﬁcients are therefore themselves random variables.

In practice, regularized regression, e.g., ridge regression
and LASSO, exhibit more attractive bias-variance trade-
offs and generalization errors than vanilla LSR. Further-
more, the matrix generalization of LSR, where multiple
responses are to be predicted, is often more useful than
LSR. However, the properties of sketched regularized ma-
trix regression are largely unknown. Hence, the question:
How, if at all, does our understanding of the optimiza-
tion and statistical properties of sketched LSR generalize
to sketched regularized regression? We answer this ques-
tion for sketched matrix ridge regression (MRR).
Recall that X is n × d. Let Y ∈ Rn×m denote the matrix
of corresponding responses. We study the MRR problem
(cid:13)XW − Y(cid:13)
(cid:13)
2
(cid:13)
F

f (W) (cid:44) 1
n

+ γ(cid:107)W(cid:107)2
F

(cid:111)
,

(1)

(cid:110)

min
W

LSR is a special case of MRR, with m = 1 and γ = 0. The
optimal solution W(cid:63) can be obtained in O(nd2 + nmd)
time using a QR decomposition of X. Sketching can be
applied to MRR in two ways:

Wc = (XT SST X + nγId)†(XT SST Y),
Wh = (XT SST X + nγId)†XT Y.

(3)

(4)

Following the convention of Pilanci & Wainwright (2015);
Wang et al. (2016a), we call Wc classical sketch and Wh
Hessian sketch, which approximate the optimal solution
W(cid:63). Table 1 lists the time costs of the three solutions to
MRR.

1.1. Main Results and Contributions

We ﬁrst study classical and Hessian sketches from the op-
timization perspective. Theorems 1 and 2 show that

• Classical sketch achieves relative error in the objective
value. With sketch size s = ˜O(d/(cid:15)), the objective
satisﬁes f (Wc) ≤ (1 + (cid:15))f (W(cid:63)).

• Hessian sketch does not achieve relative error in the
In particular, if 1
F is much
objective value.
larger than f (W(cid:63)), then f (Wh) can be far larger than
f (W(cid:63)).

n (cid:107)Y(cid:107)2

• For both classical and Hessian sketch, the relative
quality of approximation improves as the regulariza-
tion parameter γ increases.

We then study classical and Hessian sketches from the sta-
tistical perspective, by modeling Y = XW0 + Ξ as the
sum of a true linear model and random noise, decompos-
ing the risk R(W) = E(cid:107)XW − XW0(cid:107)2
F into bias and
variance terms, and bounding these terms. We draw the
following conclusions (see Theorems 4, 5, 6):

s

• The bias of the classical sketch can be nearly as
small as that of the optimal solution. The variance
is Θ(cid:0) n
(cid:1) times that of the optimal solution; this bound
is optimal. Therefore over-regularization, i.e., large
γ, should be used to supress the variance.
(As γ
increases, the bias increases, and the variance de-
creases.)

• Since Y is not sketched with Hessian sketch, the vari-
ance of Hessian sketch can be close to the optimal so-
lution. However, Hessian sketch has high bias, espe-
cially when nγ is small compared to (cid:107)X(cid:107)2
2. This indi-
cates that over-regularization is necessary for Hessian
sketch to have low bias.

which has optimal solution

W(cid:63) = (XT X + nγId)†XT Y.

(2)

Here, (·)† denotes the Moore-Penrose inversion operation.

Our empirical evaluations bear out these theoretical results.
In particular, in Section 4, we show in Figure 2 that even
when the regularization parameter γ is ﬁne-tuned, the risks
of classical sketch and Hessian sketch are worse than that

Sketched Ridge Regression

of the optimal solution by an order of magnitude. This is
an empirical demonstration of the fact that the near-optimal
properties of sketching from the optimization perspective
are much less relevant in a statistical setting than its sub-
optimal statistical properties.

We propose to use model averaging, which averages the
solutions of g sketched MRR problems, to attain lower op-
timization and statistical errors. Without ambiguity, we de-
note classical and Hessian sketches with model averaging
by Wc and Wh, respectively. Theorems 7, 8, 10, 11 give
the following results:

• Classical Sketch. Assume the sketch size s = ˜O( d
(cid:15) )
and (cid:15) ≤ 1
g ; then the bound on f (Wc) − f (W(cid:63)) is
proportional to (cid:15)
(cid:15)2 ) and (cid:15)2 ≤
1
g ; the bias does not increase; the variance bound is
proportional to 1
g .

g . Assume that s = ˜O( d

• Hessian Sketch. Assume that s = ˜O( d

(cid:15) ) and (cid:15) ≤ 1
g2 ;
then the bound on f (Wh) − f (W(cid:63)) is proportional
g2 . Assume that s = ˜O( d
to (cid:15)
(cid:15)2 ); the variance does
not increase; if, additionally, (cid:15) ≤ 1
g and nγ is much
smaller than the squared spectral norm of X, then the
bias bound is proportional to (cid:15)
g .

Note that classical sketch with uniform sampling and
model averaging is very well known as bagging (Breiman,
1996) (or pasting (Breiman, 1999) or bootstrap aggregat-
ing). Different from bagging, our model averaging ap-
proach is not limited to uniform sampling.

Classical sketch with model averaging has three immediate
applications. In the single-machine setting,

• Classical sketch with model averaging offers a way to
improve the statistical performance in the presence of
heavy noise. Assume the sketch size is s = ˜O(
nd).
As g grows larger than n
s , the variance of the averaged
solution can be even lower than the optimal solution.
See Remark 1 for further discussion. This observation
is in accordance with the observation that bagging re-
duces variance.

√

In the distributed setting,
the feature-response pairs
(x1, y1), · · · , (xn, yn) ∈ Rd × Rm are divided among g
machines. Assuming that the data have been shufﬂed ran-
domly, each machine contains a sketch constructed by uni-
formly sampled rows from the dataset without replacement.
In this setting, the model averaging procedure will com-
municate the g local models only once to return the ﬁnal
estimate; this process has very low communication com-
plexity and latency, and it suggests two further applications
of classical sketch with model averaging:

• Model Averaging for Machine Learning.

If a low-
precision solution is acceptable, the averaged solution
can be used in lieu of distributed numerical optimiza-
tion algorithms requiring multiple rounds of commu-
nication. If n
g is big enough compared to d and the row
coherence of X is small, then “one-shot” model aver-
aging has bias and variance comparable to the optimal
solution.

• Model Averaging for Optimization.

If a high-
precision solution to MRR is required, then an iter-
ative numerical optimization algorithm must be used.
The cost of such numerical optimization algorithms
heavily depends on the quality of the initialization.2
A good initialization saves lots of iterations. The av-
eraged model is provably close to the optimal solution,
so model averaging provides a high-quality initializa-
tion for more expensive algorithms.

1.2. Prior Work

The body of work on sketched LSR mentioned earlier
(Drineas et al., 2006; 2011; Clarkson & Woodruff, 2013;
Meng & Mahoney, 2013; Nelson & Nguyˆen, 2013) shares
many similarities with our results. However, the theories of
sketched LSR developed from the optimization perspective
do not obviously extend to MRR, and the statistical analy-
sis of LSR and MRR differ: among other differences, LSR
is unbiased while MRR has a nontrivial bias and therefore
has a bias-variance tradeoff that must be considered.

Lu et al. (2013) has considered a different application of
sketching to ridge regression: they assume d (cid:29) n, reduce
the number of features in X using sketching, and conduct
statistical analysis. Our setting differs in that we consider
n (cid:29) d, reduce the number of samples by sketching, and
allow for multiple responses.

The model averaging analyzed in this paper is similar in
spirit to the AVGM algorithm of (Zhang et al., 2013). When
classical sketch is used with uniform row sampling without
replacement, our model averaging procedure is a special
case of AVGM. However, our results do not follow from
those of (Zhang et al., 2013): ﬁrst, we make no assumption
on the data, whereas they assumed x1, · · · , xn are i.i.d.
from an unknown distribution; second, our results apply
to many other sketching ensembles than uniform sampling
without replacement; and third, we provide both optimiza-
tion and statistical perspectives, whereas they provide only
a statistical perspective. Our results clearly indicate that the

2For example,
≤ θt

(cid:107)W(t)−W(cid:63)(cid:107)2
F
(cid:107)W(0)−W(cid:63)(cid:107)2
F

the conjugate gradient method satisﬁes

1; the stochastic block coordinate descent (Tu

2. Here W(t) is the
et al., 2016) satisﬁes
output of the t-th iteration; θ1, θ2 ∈ (0, 1) depend on the condi-
tion number of XT X + nγId and some other factors.

Ef (W(t))−f (W(cid:63))
f (W(0))−f (W(cid:63))

≤ θt

Sketched Ridge Regression

performance critically depends on the row coherence of X;
this dependence is not captured in (Zhang et al., 2013). For
similar reasons, our work is different from the divide-and-
conquer kernel ridge regression algorithm of (Zhang et al.,
2015).

Iterative Hessian sketch has been studied by Pilanci &
Wainwright (2015); Wang et al. (2016a). By way of com-
parison, all the algorithms in this paper are “one-shot”
rather than iterative. Upon completion of this paper, we no-
ticed the contemporary works (Avron et al., 2016; Thanei
et al., 2017). Avron et al. (2016) studied classical sketch
from the optimization perspective, and Thanei et al. (2017)
studied LSR with model averaging.

1.3. Paper Organization

Section 2 deﬁnes our notation and introduces the sketching
schemes we consider. Section 3 presents our theoretical
results. Section 4 conducts experiments to verify our the-
ories and demonstrates the usefulness of model averaging.
Proofs of our claims and more empirical evaluations can be
found in the technical report version (Wang et al., 2017).

2. Preliminaries

Throughout, we take In to be the n × n identity matrix and
0 to be a vector or matrix of all zeroes of the appropriate
size. Given a matrix A = [aij], the i-th row is denoted
by ai:, and a:j denotes the j-th column. The Frobenius
and spectral norms of A are written as, respectively, (cid:107)A(cid:107)F
and (cid:107)A(cid:107)2. The set {1, 2, · · · , n} is written [n]. Let O, Ω,
and Θ be the standard asymptotic notation. Let ˜O conceal
logarithm factors.
Throughout, we ﬁx X ∈ Rn×d as our matrix of features.
We set ρ = rank(X) and write the SVD of X as X =
UΣVT , where U, Σ, V are respectively n × ρ, ρ × ρ,
and d × ρ matrices. We let σ1 ≥ · · · ≥ σρ > 0 be the
singular values of X. The Moore-Penrose inverse of X
is deﬁned by X† = VΣ−1UT . The row leverage scores
of X are li = (cid:107)u:i(cid:107)2
2 for i ∈ [n]. The row coherence of
X is µ(X) = n
2. Throughout, we let µ be
shorthand for µ(X).

ρ maxi (cid:107)u:i(cid:107)2

Matrix sketching turns big matrices into smaller ones with-
out losing too much information useful in tasks like linear
regression. We denote the process of sketching a matrix
X ∈ Rn×d by X(cid:48) = ST X. Here, S ∈ Rn×s is called a
sketching matrix and X(cid:48) ∈ Rs×d is called a sketch of X.
In practice, except for Gaussian projection (where the en-
tries of S are i.i.d. sampled from N (0, 1/s)), the sketching
matrix S is not formed explicitly. Matrix sketching can be
accomplished by random sampling or random projection.

Random sampling corresponds to sampling rows of X

i.i.d. with replacement according to given row sampling
probabilities p1, · · · , pm ∈ (0, 1). The corresponding (ran-
dom) sketching matrix S ∈ Rn×s has exactly one non-zero
entry per column, whose position indicates the index of the
selected row; in practice, this S is not explicitly formed.
Uniform sampling ﬁxes p1 = · · · = pn = 1
n . Leverage
score sampling sets pi proportional to the (exact or approx-
imate (Drineas et al., 2012)) row leverage scores li of X. In
practice shrinked leverage score sampling can be a better
choice than leverage score sampling (Ma et al., 2015). The
sampling probabilities of shrinked leverage score sampling
are deﬁned by pi = 1
2

+ 1
n

(cid:1).3

(cid:0)

li
(cid:80)n
j=1 lj

Gaussian projection is also well-known as the prototypi-
cal Johnson-Lindenstrauss transform (Johnson & Linden-
strauss, 1984). Let G ∈ Rm×s be a standard Gaus-
sian matrix, i.e., each entry is sampled independently from
N (0, 1). The matrix S = 1√
s G is a Gaussian projection
matrix. It takes O(nds) time to apply S ∈ Rn×s to any
n × d dense matrix, which makes Gaussian projection in-
efﬁcient relative to other forms of sketching.

Subsampled randomized Hadamard transform (SRHT)
(Drineas et al., 2011; Lu et al., 2013; Tropp, 2011) is
a more efﬁcient alternative to Gaussian projection. Let
Hn ∈ Rn×n be the Walsh-Hadamard matrix with +1 and
−1 entries, D ∈ Rn×n be a diagonal matrix with diagonal
entries sampled uniformly from {+1, −1}, and P ∈ Rn×s
be the uniform row sampling matrix deﬁned above. The
n DHnP ∈ Rn×s is an SRHT matrix, and
matrix S = 1√
can be applied to any n × d matrix in O(nd log s) time.
In practice, the subsampled randomized Fourier transform
(SRFT) (Woolfe et al., 2008) is often used in lieu of the
SRHT, because the SRFT exists for all values of n, whereas
Hn exists only for some values of n. Their performance
and theoretical analyses are very similar.
CountSketch can be applied to any X ∈ Rn×d in O(nd)
time (Charikar et al., 2004; Clarkson & Woodruff, 2013;
Meng & Mahoney, 2013; Nelson & Nguyˆen, 2013; Pham
& Pagh, 2013; Weinberger et al., 2009). Though more ef-
ﬁcient to apply, CountSketch requires a bigger sketch size
than Gaussian projections, SRHT, and leverage score sam-
pling to attain the same theoretical guarantees. The readers
can refer to (Woodruff, 2014) for a detailed description of
CountSketch.

3. Main Results

Sections 3.1 and 3.2 analyze sketched MRR from, re-
spectively, optimization and statistical perspectives. Sec-

3In fact, pi can be any convex combination of

(Ma et al., 2015). We use the weight 1
sions extend in a straightforward manner to other weightings.

and 1
n
2 for simplicity; our conclu-

li
j=1 lj

(cid:80)n

Sketched Ridge Regression

tions 3.3 and 3.4 capture the impacts of model averaging
on, respectively, the optimization and statistical properties
of sketched MRR.

We described six sketching methods in Section 2. For sim-
plicity, in this section, we refer to leverage score sampling,
shrinked leverage score sampling, Gaussian projection, and
SRHT as the four sketching methods; and we will men-
tion explicitly uniform sampling and CountSketch. The no-
tation deﬁned in Table 2 are used throughout.

Table 2. The commonly used notation.

Notation
X ∈ Rn×d
Y ∈ Rn×m
UΣVT
µ
γ

β
S ∈ Rn×s

Deﬁnition
each row is a data sample (feature vector)
each row contains the corresponding responses
the SVD of X
the row coherence of X
the regularization parameter
β = (cid:107)X(cid:107)2
(cid:107)X(cid:107)2
a sketching matrix

2
2+nγ

≤ 1

3.1. Sketched MRR: Optimization Perspective

Theorem 1 shows that f (Wc), the objective value of clas-
sical sketch, is very close to the optimal objective value
f (W(cid:63)). The approximation quality improves as γ in-
creases.
Theorem 1 (Classical Sketch). For the four sketching
methods with s = ˜O(cid:0) βd
(cid:1), uniform sampling with s =
O(cid:0)µ βd log d
(cid:1), the in-
(cid:15)
equality

(cid:1), and CountSketch with s = O(cid:0) βd2

(cid:15)

(cid:15)

f (Wc) − f (W(cid:63)) ≤ (cid:15) f (W(cid:63))

holds with probability at least 0.9.

n (cid:107)Y(cid:107)2

n (cid:107)Y(cid:107)2

The corresponding guarantee for the performance of Hes-
sian sketch is given in Theorem 2. It is weaker than the
guarantee for classical sketch, especially when 1
F is
far larger than f (W(cid:63)). If Y is nearly noiseless—Y is well-
explained by a linear combination of the columns of X—
and γ is small, then f (W(cid:63)) is close to zero, and conse-
quently f (W(cid:63)) can be far smaller than 1
F . There-
fore, in this case which is ideal for MRR, f (Wh) is not
close to f (W(cid:63)) and our theory suggests Hessian sketch
does not perform as well as classical sketch. This is veri-
ﬁed by our experiments, which show that unless γ is big or
a large portion of Y is outside the column space of X, the
ratio f (Wh)
Theorem 2 (Hessian Sketch). For the four sketching meth-
ods with s = ˜O(cid:0) β2d
(cid:1), uniform sampling with s =
O(cid:0) µβ2d log d
), the in-
(cid:15)
equality

(cid:1), and CountSketch with s = O( β2d2

f (W(cid:63)) can be large.

(cid:15)

(cid:15)

f (Wh) − f (W(cid:63)) ≤ (cid:15)

(cid:16) (cid:107)Y(cid:107)2
F

n − f (W(cid:63))

(cid:17)

.

holds with probability at least 0.9.

These two results imply that f (Wc) and f (Wh) can be
close to f (W(cid:63)). When this is the case, curvature of the
objective function ensures that the sketched solutions Wc
and Wh are close to the optimal solution W(cid:63). Lemma 3
studies the Mahalanobis distance (cid:107)M(W − W(cid:63))(cid:107)2
F . Here
M is any non-singular matrix; in particular, it can be the
identity matrix or (XT X)1/2.
Lemma 3. Let f be the objective function of MRR deﬁned
in (1), W ∈ Rd×m be arbitrary, and W(cid:63) be the optimal
solution deﬁned in (2). For any non-singular matrix M, the
Mahalanobis distance satisﬁes
1
n

(cid:2)(XT SST X + nγId)1/2M−1(cid:3) .

(cid:13)M(W − W(cid:63))(cid:13)
(cid:13)
2
(cid:13)
F

f (W) − f (W(cid:63))

σ2

≤

min

By choosing M = (XT X)1/2, we can bound 1
XW(cid:63)(cid:107)2

n (cid:107)XW −
F in terms of the difference in the objective values:

(cid:13)XW − XW(cid:63)(cid:13)
(cid:13)
(cid:13)

1
n

2
F

≤ β(cid:2)f (W) − f (W(cid:63))(cid:3).

With Lemma 3, we can directly apply Theorems 1 or 2 to
F or 1
bound 1

n (cid:107)XWh − XW(cid:63)(cid:107)2
F .

n (cid:107)XWc − XW(cid:63)(cid:107)2

3.2. Sketched MRR: Statistical Perspective

We consider the following ﬁxed design model. Let X ∈
Rn×d be the observed feature matrix, W0 ∈ Rd×m be the
true and unknown model, Ξ ∈ Rn×m contain unknown
random noise, and

Y = XW0 + Ξ

(5)

be the observed responses. We make the following standard
weak assumptions on the noise:
E[Ξ] = 0

and E[ΞΞT ] = ξ2In.

We observe X and Y and seek to estimate W0.
We can evaluate the quality of the estimate by the risk:

R(W) = 1
n

E(cid:13)

(cid:13)XW − XW0

(cid:13)
(cid:13)

2
F

,

(6)

where the expectation is taken w.r.t. the noise Ξ. We study
the risk functions R(W(cid:63)), R(Wc), and R(Wh) in the fol-
lowing.
Theorem 4 (Bias-Variance Decomposition). We consider
the data model described in this subsection. Let W be W(cid:63),
Wc, or Wh, as deﬁned in (2), (3), (4), respectively; then
the risk function can be decomposed as

R(W) = bias2(W) + var(W).

Recall the SVD of X: X = UΣVT . The bias and variance
terms can be written as
bias(cid:0)W(cid:63)(cid:1) = γ

√

2

,

(cid:13)
(cid:13)
(cid:13)F

(cid:13)
(cid:13)(Σ2 + nγIρ)−1ΣVT W0
(cid:13)
n
(cid:13)
(cid:0)Iρ + nγΣ−2(cid:1)−1(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
√
(cid:13)
(cid:13)
(cid:13)
(cid:0)UT SST U + nγΣ−2(cid:1)†UT SST (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:0)ΣUT SST UΣ + nγIρ

n

F

,

2

F

,

(cid:1)†ΣVT W0

(cid:13)
(cid:13)
(cid:13)F

,

var(cid:0)W(cid:63)(cid:1) =

ξ2
n
bias(cid:0)Wc(cid:1) = γ

var(cid:0)Wc(cid:1) =

ξ2
n

Sketched Ridge Regression

bias(cid:0)Wh(cid:1) = γ

√

(cid:13)
(cid:13)
(cid:13)

n

(cid:16)
Σ−2 + UT SST U−Iρ

(cid:17)

nγ

· (cid:0)UT SST U + nγΣ−2(cid:1)†ΣVT W0
(cid:0)UT SST U + nγΣ−2(cid:1)†(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

ξ2
n

F

(cid:13)
(cid:13)
(cid:13)F

,

var(cid:0)Wh(cid:1) =

Theorem 5 provides upper and lower bounds on the bias
and variance of the classical sketch. In particular, we see
that that bias(Wc) is within a factor of (1±(cid:15)) of bias(W(cid:63)).
However, var(Wc) is Θ( n
Theorem 5 (Classical Sketch). For Gaussian projection
and SRHT sketching with s = ˜O( d
(cid:15)2 ), uniform sampling
with s = O(µ d log d
(cid:15)2 ), the
inequalities

), or CountSketch with s = O( d2

s ) times worse than var(W(cid:63)).

(cid:15)2

1 − (cid:15) ≤

≤ 1 + (cid:15),

bias(Wc)
bias(W(cid:63))
var(Wc)
var(W(cid:63))

n
s

(1 − (cid:15))

≤

≤ (1 + (cid:15))

n
s

hold with probability at least 0.9.
For shrinked leverage score sampling with s = O( d log d
),
these inequalities, except for the lower bound on the vari-
ance,4 hold with probability at least 0.9.

(cid:15)2

Theorem 6 establishes similar upper and lower bounds on
the bias and variance of Hessian sketch. The situation is the
reverse of that with classical sketch: the variance of Wh is
close to that of W(cid:63) if s is large enough, but as the regular-
ization parameter γ goes to zero, bias(Wh) becomes much
larger than bias(W(cid:63)).
Theorem 6 (Hessian Sketch). For the four sketching
methods with s = ˜O( d
(cid:15)2 ), uniform sampling with s =
O(µ d log d
(cid:15)2 ), the in-
equalities

), and CountSketch with s = O( d2

(cid:15)2

bias(Wh)
bias(W(cid:63))

≤ (1 + (cid:15))

1 +

(cid:16)

(cid:15)(cid:107)X(cid:107)2
2
nγ

(cid:17)

,

1 − (cid:15) ≤

≤ 1 + (cid:15)

var(Wh)
var(W(cid:63))

hold with probability at least 0.9. Further assume that
ρ ≥ nγ
σ2

(cid:15) . Then

bias(Wh)
bias(W(cid:63))

≥

1
1 + (cid:15)

(cid:16) (cid:15)σ2
ρ
nγ

(cid:17)

− 1

holds with probability at least 0.9.

The lower bound on the bias shows that Hessian sketch
can suffer from a much higher bias than the optimal so-
lution. The gap between bias(Wh) and bias(W(cid:63)) can be

4For shrinked leverage score sampling, (cid:107)S(cid:107)2

2 does not enjoy
nontrivial lower bound. This is why we do not have a lower bound
on the variance.

lessened by increasing the regularization parameter γ, but
such over-regularization increases the baseline bias(W(cid:63))
It is also worth mentioning that unlike bias(W(cid:63))
itself.
and bias(Wc), bias(Wh) is not monotonically increasing
with γ, as is empirically veriﬁed in Figure 2.

In sum, our theories show that classical and Hessian
sketches are not statistically comparable to the optimal so-
lutions: classical sketch has too high a variance, and Hes-
sian sketch has too high a bias for reasonable amounts of
regularization. In practice, the regularization parameter γ
should be tuned to optimize the prediction accuracy. Our
experiments in Figure 2 show that even with ﬁne-tuned γ,
the risks of classical and Hessian sketches can be higher
than the risk of the optimal solution by an order of magni-
tude. Formally speaking, minγ R(Wc) (cid:29) minγ R(W(cid:63))
and minγ R(Wh) (cid:29) minγ R(W(cid:63)) hold in practice.

Our empirical study in Figure 2 suggests classical and Hes-
sian sketches both require over-regularization, i.e., setting
γ larger than what is best for the optimal solution W(cid:63). For-
mally speaking, argminγ R(Wc) > argminγ R(W(cid:63)) and
argminγ R(Wh) > argminγ R(W(cid:63)). Although this is the
case for both types of sketches, the underlying explanations
are different. Classical sketch has a high variance, so a
large γ is required to supress the variance (its variance is
non-increasing with γ). Hessian sketch has very high bias
when γ is small, so a reasonably large γ is necessary to
lower its bias.

3.3. Model Averaging: Optimization Perspective

We consider model averaging as an approach to increas-
ing the accuracy of sketched MRR solutions. The model
averaging procedure is straightforward: one independently
draws g sketching matrices S1, · · · , Sg ∈ Rn×s, uses these
to form g sketched MRR solutions, denoted by {Wc
i=1
or {Wh
i=1, and averages these solutions to obtain the ﬁ-
nal estimate Wc = 1
i=1 Wh
i=1 Wc
i .
g
Practical applications of model averaging are enumerated
in Section 1.1.

i or Wh = 1
g

i }g

i}g

(cid:80)g

(cid:80)g

Theorems 7 and 8 present guarantees on the optimization
accuracy of using model averaging to combine the classical
or Hessian sketch solutions. We can contrast these with the
guarantees provided for sketched MRR in Theorems 1 and
2. For classical sketch with model averaging, we see that
when (cid:15) ≤ 1
g , the bound on f (Wh)−f (W(cid:63)) is proportional
to (cid:15)/g. From Lemma 3 we can see that the distances from
Wc to W(cid:63) also decreases accordingly.
Theorem 7 (Classical Sketch with Model Averaging). For
the four methods, let s = ˜O(cid:0) βd
(cid:1); for uniform sampling, let
s = O(cid:0) µβd log d

(cid:1). Then the inequality

(cid:15)

(cid:15)

f (Wc) − f (W(cid:63)) ≤

g + β2(cid:15)2(cid:17)
(cid:16) (cid:15)

f (W(cid:63))

Sketched Ridge Regression

Figure 1. Empirical study of classical sketch and Hessian sketch from optimization perspective. The x-axis is the regularization param-
eter γ (log-scale); the y-axis is the objective function values (log-scale).

holds with probability at least 0.8.

β2 ≤ 1
For Hessian sketch with model averaging, if (cid:15)
g2 , then
the bound on f (Wh) − f (W(cid:63)) is proportional to (cid:15)
g2 .

Theorem 8 (Hessian Sketch with Model Averaging). For
the four methods let s = ˜O(cid:0) β2d
(cid:1), and for uniform sam-
pling let s = O(cid:0) µβ2d log d

(cid:1), then the inequality

(cid:15)

(cid:15)

f (Wh) − f (W(cid:63)) ≤ (cid:0) (cid:15)

g2 + (cid:15)2

β2

(cid:1) (cid:0) (cid:107)Y(cid:107)2
F

n − f (W(cid:63))(cid:1).

holds with probability at least 0.8.

3.4. Model Averaging: Statistical Perspective

Model averaging also has the salutatory property of reduc-
ing the risks of the classical and Hessian sketch solutions.
Our ﬁrst result conducts a bias-variance decomposition for
the averaged solution of sketched MRR.

Theorem 9 (Bias-Variance Decomposition). We consider
the ﬁxed design model (5). The risk function deﬁned in (6)
can be decomposed as

R(W) = bias2(W) + var(W).

The bias and variance terms are

bias(cid:0)Wh(cid:1) = γ

√

n

(cid:13)
(cid:13)
(cid:13)

1
g

g
(cid:88)

i=1

(cid:0)Σ−2 +

UT SiST
i U − Iρ
nγ

(cid:1)

· (cid:0)UT SiST

i U + nγΣ−2(cid:1)†ΣVT W0

(cid:13)
(cid:13)
(cid:13)F

,

var(cid:0)Wh(cid:1) =

ξ2
n

(cid:13)
(cid:13)
(cid:13)

1
g

g
(cid:88)

i=1

(cid:0)UT SiST

i U + nγΣ−2(cid:1)†(cid:13)
(cid:13)
(cid:13)

2

F

.

Theorems 10 and 11 provide upper bounds on the bias
and variance of model-averaged sketched MRR for, respec-
tively, classical sketch and Hessian sketch. We can contrast
them with Theorems 5 and 6 to see the statistical beneﬁts
of model averaging.
Theorem 10 (Classical Sketch with Model Averaging).
For shrinked leverage score sampling, Gaussian projec-
tion, SRHT with s = ˜O(cid:0) d
(cid:1), or uniform sampling with
(cid:15)2
s = O(cid:0) µd log d

(cid:15)2

(cid:1), the inequalities
bias(Wc)
bias(W(cid:63))
var(Wc)
var(W(cid:63))

≤ 1 + (cid:15),

n
s

≤

(cid:16)(cid:113) 1+(cid:15)/g

(cid:17)2

g + (cid:15)

hold with probability at least 0.8.
Remark 1. From this result, we see that if (cid:15) ≤ 1√
the variance is proportional to 1

g , then

g = O

and

(cid:17)

(cid:16) n
s

g . If g and s are at least
s = ˜O(cid:0)√

nd(cid:1),

then the risk R(Wc) is close to R(W(cid:63)). If g and s are
larger, then the variance var(Wc) can even be even lower
than var(W(cid:63)).

bias(cid:0)Wc(cid:1) = γ

(cid:0)ΣUT SiST

i UΣ + nγIρ

(cid:1)†ΣVT W0

(cid:13)
(cid:13)
(cid:13)F

,

var(cid:0)Wc(cid:1) =

(cid:0)UT SiST

i U + nγΣ−2(cid:1)†UT SiST

i

√

n

(cid:13)
(cid:13)
(cid:13)

1
g

g
(cid:88)

i=1

ξ2
n

(cid:13)
(cid:13)
(cid:13)

1
g

g
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)

2

F

,

Theorem 11 shows that model averaging decreases the bias
of Hessian sketch without increasing the variance. For Hes-
sian sketch without model averaging, recall that bias(Wh)

ξ=10−3ξ=10−2ξ=10−1Wcγ10-1210-1010-810-610-410-2Objective Function10-710-610-510-410-310-2γ10-1210-1010-810-610-410-2Objective Function10-410-310-2γ10-1210-1010-810-610-410-2Objective Function0.0050.010.020.05Whγ10-1210-1010-810-610-410-2Objective Function10-710-610-510-410-310-2γ10-1210-1010-810-610-410-2Objective Function10-410-310-2γ10-1210-1010-810-610-410-2Objective Function0.0050.010.020.0510-1410-1310-73e-0710-63e-0610-53e-050.00010.00030.0010.0030.01Uniform SamplingLeverage SamplingShrinkage Lev. SamplingGaussian ProjectionSRFTCount SketchOptimal SolutionSketched Ridge Regression

Figure 2. Empirical study of classical sketch and Hessian sketch from statistical perspective. The x-axis is the regularization parameter
γ (log-scale); the y-axes are respectively bias2, variance, and risk (log-scale). We annotate the minimum risks and optimal γ in the plots.

is larger than bias(W(cid:63)) by a factor of O((cid:107)X(cid:107)2
2/(nγ)).
Theorem 11 shows that model averaging reduces this ra-
tio by a factor of (cid:15)

g when (cid:15) ≤ 1
g .

Theorem 11 (Hessian Sketch with Model Averaging). For
the four methods with s = ˜O(cid:0) d
(cid:1), or uniform sampling
(cid:15)2
with s = O(cid:0) µd log d

(cid:1), the inequalities

(cid:15)2

bias(Wh)
bias(W(cid:63))
var(Wh)
var(W(cid:63))

≤ 1 + (cid:15)

≤ 1 + (cid:15) + (cid:0) (cid:15)
g

+ (cid:15)2(cid:1) (cid:107)X(cid:107)2
nγ

2

,

hold with probability at least 0.8.

We conducted experiments on synthetic data to verify The-
orems 5 and 6 and to show the effects of classical and Hes-
sian sketching on the bias and variance. We set the noise
intensity to be ξ = 0.1. In Figure 2, we plot the analytical
expressions for the squared bias, variance, and risk stated in
Theorem 4 against the regularization parameter γ. The re-
sults of this experiment match our theory: classical sketch
magniﬁed the variance, and Hessian sketch increased the
bias. Even if γ is ﬁne-tuned, the risks of classical and Hes-
sian sketches can be much higher than those of the optimal
solution.5 Our experiments also indicate that classical and
Hessian sketches require setting γ larger than the best reg-
ularization parameter for the optimal solution W(cid:63).

4. Sketched Ridge Regression Experiments

5. Conclusions

Following (Ma et al., 2015; Yang et al., 2016), we
constructed X ∈ Rn×d to have condition number
κ(XT X) = 1012 and high row coherence, ﬁxed w0 =
[10.2d; 0.110.6d; 10.2d], and set y = Xw0 + ε ∈ Rn, where
the entries of ε ∈ Rn were i.i.d. sampled from N (0, ξ2).
The details of this data model are given in the technical re-
port version (Wang et al., 2017). Let S ∈ Rn×s be any of
the six sketching methods considered in this paper. We ﬁx
n = 105, d = 500, and s = 5, 000. Because the analyti-
cal expressions involve the random sketching matrix S, we
randomly generate S, repeat this procedure 10 times, and
report the averaged results.

2 + γ(cid:107)w(cid:107)2

In Figure 1, we plot the objective function value f (w) =
1
n (cid:107)Xw − y(cid:107)2
2 against γ, under different settings
of noise intensity ξ. The results verify our theory: classical
sketch wc is always close to optimal; Hessian sketch wh
is much worse than the optimal when γ is small and y is
mostly in the column space of X.

We studied sketched matrix ridge regression (MRR) from
optimization and statistical perspectives. Using classical
sketch, by taking a large enough sketch, one can obtain
an (cid:15)-accurate approximate solution. Counterintuitively and
in contrast to classical sketch, the relative error of Hes-
sian sketch increases as the responses Y are better approx-
imated by linear combinations of the columns of X. Both
classical and Hessian sketches can have statistical risks that
are worse than the risk of the optimal solution by an order
of magnitude. We proposed the use of model averaging
to attain better optimization and statistical properties. We
have shown that model averaging leads to substantial im-
provements in the theoretical error bounds, suggesting ap-
plications in distributed optimization and machine learning.

5In the experiment yielding Figure 2, Hessian sketch had
lower risk than classical sketch. This is not generally true: if we
used a smaller ξ, so that the variance is dominated by bias, then
classical sketch results in lower risks than Hessian sketch.

Bias2VarRisk=Bias2+VarWcγ10-1210-1010-810-610-410-2Bias210-1210-1110-1010-910-810-710-610-510-410-310-2γ10-1210-1010-810-610-410-2Variance10-1210-1110-1010-910-810-710-610-510-410-310-2γ10-1210-1010-810-610-410-2Risk10-1210-1110-1010-910-810-710-610-510-410-310-2the min risk of the classical sketchassicthe min risk of the optimal solutioneoeopt10-6optimal γ for the optimal solutionoptimal γ for the classical sketch10-8tionsicala Whγ10-1210-1010-810-610-410-2Bias210-1210-1110-1010-910-810-710-610-510-410-310-2γ10-1210-1010-810-610-410-2Variance10-1210-1110-1010-910-810-710-610-510-410-310-2γ10-1210-1010-810-610-410-2Risk10-1210-1110-1010-910-810-710-610-510-410-310-2the min risk of the Hessian sketchesssiathe min risk of the optimal solutioneoeop10-6optimal γ for the optimal solutionoptimal γ for the Hessian sketch8tionssian10-1410-1310-73e-0710-63e-0610-53e-050.00010.00030.0010.0030.01Uniform SamplingLeverage SamplingShrinkage Lev. SamplingGaussian ProjectionSRFTCount SketchOptimal SolutionSketched Ridge Regression

Acknowledgements

We thank the anonymous reviewers for their helpful sug-
gestions. We thank the Army Research Ofﬁce and the De-
fense Advanced Research Projects Agency for partial sup-
port of this work.

References

Avron, Haim, Clarkson, Kenneth L., and Woodruff, David P.
Sharper bounds for regression and low-rank approximation
with regularization. arXiv preprint arXiv:1611.03225, 2016.

Breiman, Leo. Bagging predictors. Machine Learning, 24(2):

123–140, 1996.

Breiman, Leo. Pasting small votes for classiﬁcation in large
databases and on-line. Machine Learning, 36(1-2):85–103,
1999.

Charikar, Moses, Chen, Kevin, and Farach-Colton, Martin. Find-
ing frequent items in data streams. Theoretical Computer Sci-
ence, 312(1):3–15, 2004.

Clarkson, Kenneth L. and Woodruff, David P. Low rank approx-
imation and regression in input sparsity time. In Annual ACM
Symposium on theory of computing (STOC), 2013.

Drineas, Petros, Mahoney, Michael W., and Muthukrishnan, S.
Sampling algorithms for (cid:96)2 regression and applications. In An-
nual ACM-SIAM Symposium on Discrete Algorithm (SODA),
2006.

Drineas, Petros, Mahoney, Michael W., Muthukrishnan, S., and
Sarl´os, Tam´as. Faster least squares approximation. Numerische
Mathematik, 117(2):219–249, 2011.

Drineas, Petros, Magdon-Ismail, Malik, Mahoney, Michael W.,
and Woodruff, David P. Fast approximation of matrix coher-
ence and statistical leverage. Journal of Machine Learning Re-
search, 13:3441–3472, 2012.

Johnson, William B. and Lindenstrauss, Joram. Extensions of
Lipschitz mappings into a Hilbert space. Contemporary math-
ematics, 26(189-206), 1984.

Lu, Yichao, Dhillon, Paramveer, Foster, Dean P, and Ungar,
Lyle. Faster ridge regression via the subsampled randomized
Hadamard transform. In Advances in Neural Information Pro-
cessing Systems (NIPS), 2013.

Ma, Ping, Mahoney, Michael W, and Yu, Bin. A statistical per-
spective on algorithmic leveraging. Journal of Machine Learn-
ing Research, 16(1):861–911, 2015.

Mahoney, Michael W. Randomized algorithms for matrices and
data. Foundations and Trends in Machine Learning, 3(2):123–
224, 2011.

Pham, Ninh and Pagh, Rasmus. Fast and scalable polynomial ker-
nels via explicit feature maps. In ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD),
2013.

Pilanci, Mert and Wainwright, Martin J. Iterative Hessian sketch:
Fast and accurate solution approximation for constrained least-
squares. Journal of Machine Learning Research, pp. 1–33,
2015.

Raskutti, Garvesh and Mahoney, Michael W. A statistical per-
spective on randomized sketching for ordinary least-squares.
Journal of Machine Learning Research, 17(214):1–31, 2016.

Thanei, Gian-Andrea, Heinze, Christina, and Meinshausen, Nico-
arXiv

lai. Random projections for large-scale regression.
preprint arXiv:1701.05325, 2017.

Tropp, Joel A. Improved analysis of the subsampled randomized
Hadamard transform. Advances in Adaptive Data Analysis, 3
(01n02):115–126, 2011.

Tu, Stephen, Roelofs, Rebecca, Venkataraman, Shivaram, and
Recht, Benjamin. Large scale kernel learning using block co-
ordinate descent. arXiv preprint arXiv:1602.05310, 2016.

Wang, Jialei, Lee, Jason D, Mahdavi, Mehrdad, Kolar, Mladen,
and Srebro, Nathan. Sketching meets random projection in
the dual: A provable recovery algorithm for big and high-
dimensional data. arXiv preprint arXiv:1610.03045, 2016a.

Wang, Shusen, Gittens, Alex, and Mahoney, Michael W. Sketched
ridge regression: Optimization perspective, statistical perspec-
tive, and model averaging. arXiv preprint arXiv:1702.04837,
2017.

Wang, Yining, Yu, Adams Wei, and Singh, Aarti.

Com-
putationally feasible near-optimal subset selection for linear
arXiv preprint
regression under measurement constraints.
arXiv:1601.02068, 2016b.

Weinberger, Kilian, Dasgupta, Anirban, Langford, John, Smola,
Alex, and Attenberg, Josh. Feature hashing for large scale mul-
titask learning. In International Conference on Machine Learn-
ing (ICML), 2009.

Woodruff, David P. Sketching as a tool for numerical linear alge-
bra. Foundations and Trends R(cid:13) in Theoretical Computer Sci-
ence, 10(1–2):1–157, 2014.

Woolfe, Franco, Liberty, Edo, Rokhlin, Vladimir, and Tygert,
Mark. A fast randomized algorithm for the approximation of
matrices. Applied and Computational Harmonic Analysis, 25
(3):335–366, 2008.

Yang, Jiyan, Meng, Xiangrui, and Mahoney, Michael W.

Im-
plementing randomized matrix algorithms in parallel and dis-
tributed environments. Proceedings of the IEEE, 104(1):58–
92, 2016.

Meng, Xiangrui and Mahoney, Michael W. Low-distortion sub-
space embeddings in input-sparsity time and applications to ro-
bust linear regression. In Annual ACM Symposium on Theory
of Computing (STOC), 2013.

Zhang, Yuchen, Duchi, John C., and Wainwright, Martin J.
Communication-efﬁcient algorithms for statistical optimiza-
tion. Journal of Machine Learning Research, 14:3321–3363,
2013.

Nelson, John and Nguyˆen, Huy L. Osnap: Faster numerical lin-
ear algebra algorithms via sparser subspace embeddings.
In
IEEE Annual Symposium on Foundations of Computer Science
(FOCS), 2013.

Zhang, Yuchen, Duchi, John, and Wainwright, Martin. Divide
and conquer kernel ridge regression: a distributed algorithm
with minimax optimal rates. Journal of Machine Learning Re-
search, 16:3299–3340, 2015.

