Risk Bounds for Transferring Representations With and Without Fine-Tuning

Daniel McNamara 1 Maria-Florina Balcan 2

Abstract
A popular machine learning strategy is the trans-
fer of a representation (i.e. a feature extraction
function) learned on a source task to a target task.
Examples include the re-use of neural network
weights or word embeddings. We develop sufﬁ-
cient conditions for the success of this approach.
If the representation learned from the source task
is ﬁxed, we identify conditions on how the tasks
relate to obtain an upper bound on target task risk
via a VC dimension-based argument. We then
consider using the representation from the source
task to construct a prior, which is ﬁne-tuned us-
ing target task data. We give a PAC-Bayes tar-
get task risk bound in this setting under suitable
conditions. We show examples of our bounds
using feedforward neural networks. Our results
motivate a practical approach to weight transfer,
which we validate with experiments.

1. Introduction

A widely used machine learning technique is the transfer
of a representation learned from a source task, for which
labeled data is abundant, to a target task, for which labeled
data is scarce. This may be effective if the tasks approxi-
mately share an intermediate representation. For example:

• features learned from an image of a human face to pre-
dict age may also be useful for predicting gender

• word embeddings learned to predict word contexts

may also be useful for part of speech tagging

• features learned from ﬁnancial data to predict loan de-
fault may also be useful for predicting insurance fraud.

Often a representation is learned by a different organization
that may have greater access to data, computational and hu-
man resources. Examples are the Google word2vec pack-
age (Mikolov et al., 2013), and downloadable pre-trained

1The Australian National University and Data61, Can-
berra, ACT, Australia 2Carnegie Mellon University, Pitts-
burgh, PA, USA. Correspondence to: Daniel McNamara
<daniel.mcnamara@anu.edu.au>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

neural networks.1 Under this ‘representation-as-a-service’
model, a user may expect to access the representation itself,
as well as information about its performance on the source
task data on which it was trained. We aim to convert this
into a guarantee of the usefulness of the representation on
other tasks, which is known in advance without the effort
or cost of testing the representation on the target task(s).
Our analysis also covers the case where the source task is
constructed from unlabeled data, as in neural network un-
supervised pre-training.

We consider two approaches to transferring a representa-
tion learned from a source task to a target task, as shown in
Figure 1. We may either treat the representation as ﬁxed, or
we may narrow the class of representations considered on
the target task, which we refer to as ﬁne-tuning. The ﬁxed
option may be attractive when very little labeled target task
data is available and hence overﬁtting is a strong concern,
while the advantage of ﬁne-tuning is relatively greater hy-
pothesis class expressiveness.

Let X, Y and Z be sets known as the input, output and
feature spaces respectively. Let F be a class of represen-
tations, where f : X → Z for f ∈ F . Let G be a class
of specialized classiﬁers, where g : Z → Y for g ∈ G.
Let the hypothesis class H := {h : ∃f ∈ F, g ∈ G
such that h = g ◦ f }. Let hS, hT : X → Y be the la-
beling functions and PS, PT be the input distributions for
source task S and target task T respectively. We consider
the setting Y = {−1, 1}. Let the risk of a hypothesis
h on S and T be RS(h) := Ex∼PS [hS(x) (cid:54)= h(x)] and
RT (h) := Ex∼PT [hT (x) (cid:54)= h(x)] respectively. Let ˆRS(h)
and ˆRT (h) be the corresponding empirical (i.e.
training
set) risks. We have mS labelled points for S and mT la-
belled points for T . Let dH be the VC dimension of H.

The remainder of the paper is structured as follows. In Sec-
tion 2 we introduce related work. In Sections 3 and 4 we
analyze the cases where the transferred representation is
ﬁxed and ﬁne-tuned respectively.
In Section 5 we apply
the results and use them to motivate and test a practical ap-
proach to weight transfer in neural networks. We conclude
in Section 6 and defer more involved proofs to Section 7.

1See
http://code.google.com/archive/p/
word2vec,
http://caffe.berkeleyvision.org/
model_zoo and http://vlfeat.org/matconvnet/
pretrained for examples.

Risk Bounds for Transferring Representations With and Without Fine-Tuning

2. Background
Empirical studies have shown the success of transferring
representations between tasks (Donahue et al., 2014; Hoff-
man et al., 2014; Girshick et al., 2014; Socher et al., 2013;
Bansal et al., 2014). Word embeddings learned on a source
task have been shown (Qu et al., 2015) to perform better
than unigram features on target tasks such as part of speech
tagging, and comparably or better than embeddings ﬁne-
tuned on the target task. Yosinski et al.
(2014) learned
neural network weights using half of the ImageNet classes,
and then learned the other classes with a neural network
initialized with these weights, ﬁnding a beneﬁt compared
to random initialization only with target task ﬁne-tuning.
The transfer of representations, both with and without ﬁne-
tuning, is widely and successfully used.

Previous work on domain adaptation (Ben-David et al.,
2010; Mansour et al., 2009; Germain et al., 2013) has con-
sidered learning a hypothesis h on S and re-using it on
T , bounding RT (h) using RS(h) (measured with labeled
source data) and some notion of similarity between PS and
PT (measured with additional unlabeled target data). Such
results motivate a joint optimization using labeled source
and unlabeled target data (Ganin et al., 2016; Long et al.,
2015) to learn separate mappings fS, fT : X → Z which
make the induced distributions in the feature space Z sim-
ilar, and a hypothesis g : Z → Y learned from the source
labels which can be re-used on T . This approach assumes
the tasks become the same if their input distributions can be
aligned. We consider a relaxation where the tasks are more
weakly related but some representation step can be trans-
ferred. We consider learning f : X → Z on S, re-using it
on T , and then learning gT : Z → Y from a small amount
of labeled target data. Given the widespread use of ‘down-
loadable’ representations, where f and gT are learned sep-
arately and there is no joint optimization over source and
target data, this is a realistic setting.

Work on lifelong learning relates the past performance of
a representation over many tasks to its expected future
performance. For a representation f ∈ F we construct
G ◦ f := {g ◦ f : g ∈ G}. Suppose there is a distribu-
tion over tasks, known as an environment. Assume several
tasks from this environment have been sampled, and that
for each task some hypothesis in G ◦ f has been selected
and its empirical risk evaluated. Previous work has pro-
vided bounds on the difference between the average em-
pirical risk and the expected risk of the best hypothesis in
G ◦ f for a new task drawn from the environment. Such
bounds have been given by measuring the complexity of
F and G using covering numbers (Baxter, 2000), a vari-
ant of the growth function (Galanti et al., 2016), and a
distribution-dependent measure known as Gaussian com-
plexity (Maurer et al., 2016). All of these bounds rely on

Learn representation
from scratch
F

Transfer representation
without ﬁne-tuning
F

Transfer representation
with ﬁne-tuning

f

ˆf

f

F

ˆF

ˆf

f

Figure 1. A comparison of approaches to learning a representation
on a target task, where the search space in each case is the shaded
area. Learning from scratch, we search a representation class F
for a good representation f ∈ F . Without ﬁne-tuning, we ﬁx a
representation ˆf learned from the source task. With ﬁne-tuning,
we narrow the search to ˆF ⊆ F near ˆf , which still contains f .

known past performance on a large number of tasks.2 In
practice, however, representations such as neural network
weights or word embeddings are often learned using only a
single source task, which is the setting we consider.

3. Representation Fixed by Source Task

Suppose labeled source data is abundant, labeled target data
is scarce, and we believe the tasks share a representation.
A natural approach to leveraging the source data is to learn
ˆgS ◦ ˆf ∈ H on S, from which we assume we may ex-
tract ˆf ∈ F ,3 then conduct empirical risk minimization
over G ◦ ˆf := {g ◦ ˆf : g ∈ G} on T yielding ˆgT ◦ ˆf .
Theorem 1 upper-bounds RT (ˆgT ◦ ˆf ) using four terms: a
function ω measuring a transferrability property obtained
analytically from the problem setting, the empirical risk
ˆRS(ˆgS ◦ ˆf ), the generalization error of a hypothesis in H
learned from mS samples, and the generalization error of
a hypothesis in G learned from mT samples. The value
of the theorem is that if ω(R) = O(R), ˆRS(ˆgS ◦ ˆf ) is a
small constant, mS (cid:29) mT and dH (cid:29) dG,4 we improve on
the VC dimension-based bound for learning T from scratch
by avoiding the generalization error of a hypothesis in H
learned from mT samples. Furthermore, we do not settle
for bounding RT (ˆgT ◦ ˆf ) in terms of ˆRT (ˆgT ◦ ˆf ), which
may be large. The theorem can be used to select S given

2Pentina & Lampert (2014) extend this analysis to stochas-
tic hypotheses (i.e. distributions over deterministic hypotheses),
where for each task we learn a posterior given a prior and training
data. The quality of the prior affects the learner’s performance.
The study proposes using source tasks to learn a ‘hyperposterior’,
a distribution over priors which is sampled to give a prior for each
task. Such a hyperposterior may focus the learner on a represen-
tation shared across tasks. The study gives a PAC-Bayes bound
on the expected risk of using a hyperposterior to learn a new task
drawn from the environment, in terms of the average empirical
risk obtained using the hyperposterior to learn the source tasks.

3This is not possible with knowledge of ˆgS ◦ ˆf alone, but in
the case of feedforward neural networks which we focus on, ˆf is
known if the weights learned on S are known.

4We have mS (cid:29) mT if labeled source task data is abundant
while labeled target task data is scarce, and dH (cid:29) dG if we sim-
plify target task learning by substantially reducing the hypothesis
space to be searched.

Risk Bounds for Transferring Representations With and Without Fine-Tuning

Learn T
from scratch

Learn ˆgS ◦ ˆf
on S

Transfer ˆf from S,
learn ˆgT on T

ˆf

ˆf

ˆgS

ˆgT

several options. While we refer to ω in a general form, we
give an example in Section 3.1 and expect that others exist.5
Theorem 1. Let ω : R → R be a non-decreasing func-
tion. Suppose PS, PT , hS, hT , ˆf , G have the property that
RT (g ◦ ˆf ) ≤ ω(RS(ˆgS ◦ ˆf )). Let ˆgT :=
∀ˆgS ∈ G, min
g∈G
ˆRT (g ◦ ˆf ). Then with probability at least 1 − δ

arg min
g∈G

over pairs of training sets for tasks S and T , RT (ˆgT ◦ ˆf )
≤ ω( ˆRS(ˆgS ◦ ˆf ) + 2
) +
(cid:113) 2dG log(2emT /dG)+2 log(8/δ)
mT

(cid:113) 2dH log(2emS /dH )+2 log(8/δ)
mS

4

.

Proof. Let g∗

T := arg min

RT (g ◦ ˆf ). With probability at

g∈G

least 1 − δ,
RT (ˆgT ◦ ˆf )

≤ ˆRT (ˆgT ◦ ˆf ) + 2

≤ ˆRT (g∗

T ◦ ˆf ) + 2

≤ RT (g∗

T ◦ ˆf ) + 4

(cid:113) 2dG log(2emT /dG)+2 log(8/δ)
mT
(cid:113) 2dG log(2emT /dG)+2 log(8/δ)
mT
(cid:113) 2dG log(2emT /dG)+2 log(8/δ)
mT

≤ ω(RS(ˆgS ◦ ˆf )) + 4

≤ ω( ˆRS(ˆgS ◦ ˆf ) + 2

(cid:113) 2dG log(2emT /dG)+2 log(8/δ)
4
mT

.

(cid:113) 2dG log(2emT /dG)+2 log(8/δ)
mT
(cid:113) 2dH log(2emS /dH )+2 log(8/δ)
mS

) +

(cid:113) 2d log(2em/d)+2 log(4/δ)
m

Using m training points and a hypothesis class of VC di-
mension d, with probability at least 1 − δ, for all hypothe-
ses h simultaneously, the risk R(h) and empirical risk ˆR(h)
satisfy |R(h)− ˆR(h)| ≤ 2
(Mohri
et al., 2012). For G this yields the ﬁrst and third inequalities
with probability at least 1 − δ
2 . For H, because ω is non-
decreasing, this yields the ﬁfth inequality with probability
at least 1 − δ
2 . Applying the union bound achieves the de-
sired result. The second inequality is by the deﬁnition of ˆgT
and the fourth inequality follows from our assumption.

3.1. Neural Network Example with Fixed

Representation

In Theorem 2, we give an example of the property required
by Theorem 1 which is speciﬁc to a particular problem set-
ting. We consider a neural network with a single hidden
layer (see Figure 2). We propose transferring the lower-
level weights (corresponding to ˆf ) learned on S, so that
only the upper-level weights (corresponding to G) have to
be learned on T . We want to show ˆf is also useful for T .

5We deﬁne ω by relating RS(ˆgS ◦ ˆf ) to min
g∈G

RT (g ◦ ˆf ), since
we expect this may be feasible analytically as in our example in
Section 3.1. However, because we only observe ˆRS(ˆgS ◦ ˆf ), in
Theorem 1 we use this to bound RS(ˆgS ◦ ˆf ) and then apply ω.

Figure 2. Neural network example learning T from scratch (left)
and with weights transferred from S (right). Thin blue and thick
red lines show weights trained on S and T respectively. Under
certain assumptions using weight transfer yields low risk on T .

To do this, we assume that some lower-level weights per-
form well on both tasks, which is clearly a necessary con-
dition for the speciﬁc ˆf we are transferring to perform well
on both tasks. We also assume PS and PT have the rel-
ative rotation invariance property and that the upper-level
weights have ﬁxed magnitude. This is so that a point x for
which ˆf (x) contributes to the risk on T cannot be ‘hidden’
from the risk of using ˆf on S, either through low PS(x)
or low magnitude upper-level weights. Hence RS(ˆgS ◦ ˆf )
reliably indicates the usefulness of ˆf on T .
Let X = Rn and Z = Rk. Let F be the function class such
that f (x) = [a(w1 · x), . . . , a(wk · x)], where wi ∈ Rn
for 1 ≤ i ≤ k, a : R → R is an odd function6 and
· is the dot product. Let G be the function class such that
g(z) = sign(v · z), where v ∈ {−1, 1}k. Suppose ∃f ∈
F, gS, gT ∈ G such that max[RS(gS ◦f ), RT (gT ◦f )] ≤ (cid:15).
Let ˆf (x) := [a( ˆw1 · x), . . . , a( ˆwk · x)]. Given wi and
ˆwi, pick nonzero constants αi and βi such that ||wi|| =
||αi ˆwi − βiwi|| and wi · (αi ˆwi − βiwi) = 0. Let M be a
2k×n matrix with rows w1, α1 ˆw1−β1w1, . . . , wk, αk ˆwk−
βkwk. Suppose M is full rank.7 Suppose ∀x, x(cid:48) such that
||M x|| = ||M x(cid:48)||, PT (x) ≤ cPS(x(cid:48)) for some c ≥ 1,
which we call relative rotation invariance and implies PS
and PT have the same support. If M is an orthogonal ma-
trix then ∀x, x(cid:48) such that ||x|| = ||x(cid:48)||, PT (x) ≤ cPS(x(cid:48)).8

Theorem 2. Let ω(R) := cR + (cid:15)(1 + c). Then ∀ˆgS ∈ G,
min
g∈G

RT (g ◦ ˆf ) ≤ ω(RS(ˆgS ◦ ˆf )).

6i.e. a(−x) = −a(x). Examples are tanh, sign and identity.
7To see that this condition is necessary, consider the following
example where M is not full rank. Let n = 4, k = 2, hS =
sign(x1) and hT = sign(x2). For f (x) = [x1 + x2, x1 − x2],
gS(z) = sign(z1 + z2) and gT (z) = sign(z1 − z2), we have
RS(gS ◦ f ) = RT (gT ◦ f ) = 0. On S we learn ˆf (x) = [x1 +
x3, x1 − x3] and ˆgS(z) = sign(z1 + z2), so that RS(ˆgS ◦ ˆf ) = 0
but in general min
g∈G

RT (g ◦ ˆf ) > 0 since ˆf ignores x2.

8For example, PS and PT are spherical Gaussians. For a zero-
mean multivariate Gaussian distribution this is achieved by the
whitening transformation x → Λ−1/2U T x, where the columns
of U and entries of the diagonal matrix Λ are the eigenvectors and
eigenvalues of the distribution’s covariance matrix respectively.

Risk Bounds for Transferring Representations With and Without Fine-Tuning

4. Representation Fine-Tuned on Target Task

4.1. Neural Network Example with Fine-Tuning

Consider learning ˆgS ◦ ˆf on S, and then using ˆf and
RS(ˆgS ◦ ˆf ) to ﬁnd ˆF ⊆ F , as in Figure 1. Let ˜hg◦f be
a stochastic hypothesis (i.e. a distribution over H) asso-
ciated with g ◦ f (e.g. g ◦ f is the mode of ˜hg◦f ). We
propose learning T with the hypothesis class ˜HG◦ ˆF :=
{˜hg◦f : f ∈ ˆF , g ∈ G} and the prior ˜hˆgS ◦ ˆf . Learn-
ing T from scratch we assume that we would instead use
˜HG◦F := {˜hg◦f : f ∈ F, g ∈ G} and some ﬁxed prior
˜h0 ∈ ˜HG◦F . Let RT (˜h) := E
x∼PT ,h∼˜h[hT (x) (cid:54)= h(x)]
and compute ˆRT (˜h) on the training set distribution of T .
In Theorem 3 we show that if ˆF is ‘small enough’ so that
all ˜h ∈ ˜HG◦ ˆF have a small KL divergence from ˜hˆgS ◦ ˆf ,
we may apply a PAC-Bayes bound to the generalization
error of hypotheses in ˜HG◦ ˆF involving four terms: a func-
tion ω measuring a transferrability property, the empirical
risk ˆRS(ˆgS ◦ ˆf ), the generalization error of a hypothesis
in H learned from mS points, and a weak dependence on
mT . The value of the theorem is that if ω(R) = O(R),
ˆRS(ˆgS ◦ ˆf ) is a small constant, and mS (cid:29) mT , we improve
on the PAC-Bayes bound for ˜HG◦F and ˜h0.9 ˆF is useful if
it is also ‘large enough’ in the sense that ∃˜hgT ◦f ∈ ˜HG◦ ˆF
such that RT (˜hgT ◦f ) ≤ (cid:15). Here ω quantiﬁes how large the
ˆF we search on T must be in order to be ‘large enough’,
in terms of RS(ˆgS ◦ ˆf ). While in general such an ˆF and ω
may not exist, we give an example in Section 4.1.
Theorem 3. Let ω : R → R be non-decreasing. Sup-
pose given ˆf ∈ F and RS(ˆgS ◦ ˆf ) estimated from S,
it is possible to construct ˆF with the property ∀˜h ∈
˜HG◦ ˆF , KL(˜h||˜hˆgS ◦ ˆf ) ≤ ω(RS(ˆgS ◦ ˆf )). Then with
probability at least 1 − δ over pairs of training sets for
tasks S and T , ∀˜h ∈ ˜HG◦ ˆF , RT (˜h) ≤ ˆRT (˜h) +
(cid:114)
(cid:113) 2dH log(2emS /dH )+2 log(8/δ)
mS
2(mT −1)

ω( ˆRS (ˆgS ◦ ˆf )+2

)+log 2mT /δ

.

Proof. With probability at least 1 − δ,
RT (˜h)

(cid:114)

≤ ˆRT (˜h) +

KL(˜h||˜hˆgS ◦ ˆf )+log 2mT /δ
2(mT −1)

≤ ˆRT (˜h) +

(cid:113) ω(RS (ˆgS ◦ ˆf ))+log 2mT /δ
2(mT −1)

.

The ﬁrst inequality holds with probability at least 1 − δ
2
(Shalev-Shwartz & Ben-David, 2014). The second inequal-
ity holds by assumption. Furthermore, RS(ˆgS ◦ ˆf ) ≤
ˆRS(ˆgS ◦ ˆf ) + 2
with prob-
ability at least 1 − δ
2 (Mohri et al., 2012) and ω is non-
decreasing. The result follows from the union bound.

(cid:113) 2dH log(2emS /dH )+2 log(8/δ)
mS

9Using the restricted deterministic hypothesis class G ◦ ˆF :=
{h : ∃f ∈ ˆF , g ∈ G such that h = g ◦ f } and a VC dimension-
based bound may not improve on H, since possibly dG◦ ˆF = dH .

We transfer and ﬁne-tune weights in a feedforward neu-
ral network with one hidden layer to instantiate the prop-
erty required by Theorem 3. We learn a deterministic hy-
pothesis of this type on S and obtain k estimated lower-
level weight vectors ˆwi. Learning T we now consider only
lower-level weights near ˆwi, corresponding to ˆF . On T we
learn a stochastic hypothesis formed by taking a determin-
istic network and adding independent sources of spherical
Gaussian noise to the lower-level weights and sign-ﬂipping
noise to the upper-level weights. The KL divergence be-
tween two of the stochastic hypotheses is expressed using
the angles between their lower-level weights10 and a quan-
tity computable from their upper-level weights.
We want to prove that we can construct such an ˆF to suc-
cessfully learn T . To do this, we assume some lower-level
weights wi perform well on both S and T . We make ˆF
‘small enough’ by only including lower-level weights with
small angles to ˆwi, and ‘large enough’ by using the risk
observed using ˆwi on S to provide an upper bound on the
angle between each pair wi and ˆwi. Our assumptions en-
sure that poor ˆwi cannot be ‘hidden’ from the risk on S, ei-
ther through low PS density in the region of disagreement
between wi and ˆwi, or through low magnitude higher-level
weights. Hence we know that searching ˆF will include wi.
Let X = Rn and Z = Rk, where k is odd.
Let F be the function class
such that f (x) =
[sign(w1 · x), . . . , sign(wk · x)], where wi ∈ Rn
Let G be the function class
for 1 ≤ i ≤ k.
such that g(z) = sign(v · z), where v ∈ {−1, 1}k.
Let Bv be a distribution on {−1, 1}k such that for

p1(v(cid:48)

k
(cid:81)
i=1

1, . . . , w(cid:48)

j =−vj ), where

v(cid:48) ∼ Bv, P r(v(cid:48)) =

k
(cid:81)
j=1
p ∈ [0.5, 1]. Let ˜hg◦f := g(cid:48) ◦ f (cid:48) such that v(cid:48), w(cid:48)

j =vj )(1 − p)1(v(cid:48)

k ∼
N (wi, σ2I). Suppose ∃f ∈ F, gS, gT ∈ G such
Bv
that max[RS(gS ◦ f ), RT (˜hgT ◦f )] ≤ (cid:15). Let ˆf (x) :=
[sign( ˆw1 · x), . . . , sign( ˆwk · x)], θ(wi, ˆwi) be the angle
between wi and ˆwi, and assume ∀i, || ˆwi|| = 1. Deﬁne M
as in Section 3.1. Let PS have the rotation invariance prop-
erty ∀x, x(cid:48) such that ||M x|| = ||M x(cid:48)||, PS(x) ≤ cPS(x(cid:48))
for some c ≥ 1.
Theorem 4. Given ˆf and RS(ˆgS ◦ ˆf ) estimated from S,
let θmax := π(cid:112)2(k − 1)c(RS(ˆgS ◦ ˆf ) + (cid:15)) and ˆF :=
{f ∈ F : ∀i, ||wi|| = 1 ∧ |θ(wi, ˆwi)| ≤ θmax}. Let
p
ω(R) := k
1−p .
Then ∃˜hgT ◦f ∈ ˜HG◦ ˆF such that RT (˜hgT ◦f ) ≤ (cid:15) and
∀˜h ∈ ˜HG◦ ˆF , KL(˜h||˜hˆgS ◦ ˆf ) ≤ ω(RS(ˆgS ◦ ˆf )).

σ2 [1 − cos θmax] + k[2p − 1 + (1 − p)k] log2

10Assuming that the lower-level weight vectors are of ﬁxed
magnitude, which is no loss of model expressiveness since we
use the sign activation function at the hidden layer.

Risk Bounds for Transferring Representations With and Without Fine-Tuning

5. Applications

We show the utility of the risk bounds, and present a novel
technique and experiments motivated by our theorems.

5.1. Using the Risk Bounds

The results described yield tighter bounds on risk when
transferring representations from S, compared to learning
T from scratch. Examples are shown in Figure 3.11

We set δ = 0.05. For the top part, we use the example
from Section 3.1 and set n = 10, k = 5. Learning T
from scratch with H, we use the bound from Mohri et al.
(2012) used previously. The VC dimension of a network
of |E| edges using the sign activation is O(|E| log |E|)
(Shalev-Shwartz & Ben-David, 2014), where in our case
|E| = nk+k. We use dH = |E| log |E| in the chart. Trans-
ferring a representation from S to T without ﬁne-tuning,
we consider the limit (cid:15) → 0, ˆRS(ˆgS ◦ ˆf ) → 0, mS → ∞,
and hence ω(·) → 0 by Theorem 2. Furthermore, dG ≤ k
since G is ﬁnite and hence dG ≤ log2 |G| (Shalev-Shwartz
& Ben-David, 2014). We use the bound from Theorem 1.

10 , k = 499, p = 2

For the bottom part, we use the example from Section 4.1
and set σ2 = 1
3 . Learning T from
scratch we use the stochastic hypothesis class {˜hg◦f : f ∈
F such that ∀i||wi|| = 1, g ∈ G} and a prior ˜h0 where
∀i wi = 0 and v ∈ {−1, 1}k is arbitrary.12 Hence we
have the bound KL(˜h||˜h0) ≤ 10k + k
3 , which becomes
tight for large k. We apply the PAC-Bayes bound (Shalev-
Shwartz & Ben-David, 2014) used previously. Transferring
a representation from S and ﬁne-tuning on T , we consider
the limit (cid:15) → 0, ˆRS(ˆgS ◦ ˆf ) → 0, mS → ∞. We have
KL(˜h||˜hˆgS ◦ ˆf ) ≤ k
3 by Theorem 4. We use the bound from
Theorem 3.

5.2. Fine-Tuning through Regularization
We relax the hard constraint on ˆF from Section 4.1 by us-
ing a modiﬁed loss function, which we ﬁnd performs better
in practice. Let yi and ˆyi be the label and prediction re-
spectively for the ith training point. In a fully-connected
feedforward neural network with l layers of weights, let
W (j) be the jth weight matrix, ˆW (j) be its estimate from
S (excluding weights for bias units in both cases), and ||·||2
be the entry-wise 2 norm. A typical loss function (1) used
for training is composed of the sum of training set log loss
and L2 regularization on the weights.

11Note that VC dimension risk bounds are known for being
rather loose, while PAC-Bayesian bounds are tighter and hence
yield non-trivial results in higher dimensions with fewer samples.
12This class is as expressive as ˜HG◦F but by setting ||wi|| = 1
the KL divergence of all hypotheses from any prior is bounded,
allowing a fair comparison to ˜HG◦ ˆF . The choice of ˜h0 minimizes
worst case KL divergence to a hypothesis in the class.

Figure 3. A comparison of risk bounds compared to learning T
from scratch, without ﬁne-tuning (top) and with ﬁne-tuning (bot-
tom). The two charts use different parameters (see Section 5.1).
m
(cid:88)

l
(cid:88)

[−yi log ˆyi − (1 − yi) log(1 − ˆyi)] +

(||W (j)||2
2)

λ
2

j=1

i=1

(1)

We replace the regularization penalty with (2).13

l
(cid:88)

j=1

[

λ1(j)
2

||W (j) − ˆW (j)||2

2 +

||W (j)||2
2]

(2)

λ2(j)
2

This penalizes estimates of W far from the representation
learned on S. Since we expect the tasks to share a low-
level representation (e.g. edge detectors for vision, word
embeddings for text) but be distinct at higher levels (e.g.
image components for vision, topics for text), we set λ1(·)
to be a decreasing function, while λ2(·) controls standard
L2 regularization. The technique is novel to our knowl-
edge, although other approaches to transferring regulariza-
tion between tasks exist (Evgeniou & Pontil, 2004; Raina
et al., 2006; Argyriou et al., 2008; Ghifary et al., 2014).

5.3. Experiments

We experiment on basic image and text classiﬁcation
tasks.14 We show that learning algorithms motivated by
our theoretical results can help to overcome a scarcity of
labeled target task data. Note that we do not replicate the
conditions speciﬁed in our theorems, nor do we attempt ex-
tensive tuning to achieve state-of-the-art performance.

13Basing our approach on (1), we follow the convention that
weights connected to bias units are excluded from the regular-
ization penalty. However, the inclusion of these weights in the
||W (j) − ˆW (j)|| term of (2) is a plausible variant.

14The MNIST and 20 Newgroups datasets are available at
http://yann.lecun.com/exdb/mnist and http://
qwone.com/˜jason/20Newsgroups respectively.

Risk Bounds for Transferring Representations With and Without Fine-Tuning

We randomly partition label classes into sets S+ and S−,
where |S+| = |S−|.15 We construct T+ by randomly pick-
ing from S+ up to γ := |S+∩T+|
, then randomly picking
|S+|
from S− such that |T+| = |T−|. We let S be the task of
distinguishing between S+ and S− and T be that of dis-
tinguishing T+ and T−. Constructing S+ and T+ as dis-
junctions of classes means that the class labels are a perfect
representation shared between S and T .

We compare the accuracy on T of four options:

Table 1. Evaluation of transferring representations. Entries are the
test set accuracy of the technique (row) for the task (column) av-
eraged over 10 trials, with the best result for each task bolded.

TECHNIQUE

MNIST, γ =
0.8

1

0.6

NEWSGROUPS, γ =
1
0.8
0.6

BASE
FINE-TUNE ˆf
FIX ˆf
FIX ˆgS ◦ ˆf

88.4
91.9
87.5
67.4

87.9
93.9
92.3
85.6

87.9
95.4
97.3
98.1

62.6
62.3
52.2
55.5

63.2
72.3
69.6
70.7

66.1
83.3
83.3
83.6

• learn T from scratch (BASE)

• transfer ˆf from S, ﬁne-tune f and train g on T using

6. Conclusion

(2) (FINE-TUNE ˆf )

• transfer ˆf from S and ﬁx, train g on T (FIX ˆf )16

• transfer ˆgS ◦ ˆf from S and ﬁx (FIX ˆgS ◦ ˆf ).17

We use λ1(1) = λ2(2) = λ := 1,18 λ1(2) = λ2(1) = 0,
mT = 500 and the sigmoid activation function. For
MNIST we use raw pixel intensities, a 784 × 50 × 1 net-
work and mS = 50000. For NEWSGROUPS we use TF-IDF
weighted counts of most frequent words, a 2000 × 50 × 1
network and mS = 15000. We use conjugate gradient op-
timization with 200 iterations.

The results are shown in Table 1.19 When the tasks are non-
identical, FINE-TUNE ˆf is mostly the strongest but per-
forms better on MNIST. FIX ˆf outperforms BASE when
γ ≥ 0.8 and hence the tasks are similar. While FIX ˆf out-
performs FIX ˆgS ◦ ˆf when the tasks are non-identical on
MNIST, on NEWSGROUPS there is no evidence of beneﬁt.
When the tasks are identical, FIX ˆgS ◦ ˆf is the strongest.

It appears that learning an MNIST digit requires a dense
weight vector and so ˆW (1) tends to encode single digits,
which helps transferrability. However, it appears that since
we may learn a newsgroup with a sparse weight vector,
ˆW (1) tends to encode disjunctions of newsgroups which
somewhat reduces transferrability. When transferring rep-
resentations does work, ﬁne-tuning using the regularization
penalty proposed in (2) improves performance.

15For MNIST there are 10 label classes and for 20 Newgroups
there are 20.
In both cases the classes are approximately bal-
anced. Note that we ignore the hierarchical structure of the 20
Newsgroups classes, which likely contributes to the lower accu-
racies reported for all methods for this dataset relative to MNIST.
16i.e. logistic regression with L2 regularization and ˆf ﬁxed.
17Used to isolate the beneﬁt of transferring ˆf rather than ˆgS ◦ ˆf .
18We explored tuning λ to lift the performance of BASE on
MNIST, but found that the results did not materially improve. Po-
tentially λ1(j) and λ2(j) in (2) could be tuned with cross valida-
tion on the target task.

19For γ = 1, hS = hT . We do not consider γ < 0.5, since that
is equivalent to 1 − γ with the deﬁnitions of T+ and T− swapped.

We developed sufﬁcient conditions for the successful trans-
fer of representations both with and without ﬁne-tuning.
This is a step towards a principled explanation of the em-
pirical success achieved by such techniques. A promising
direction for future work is generalizing the neural network
architectures considered (e.g. using multiple hidden layers)
and relaxing the distributional assumptions required. Fur-
thermore, in the ﬁne-tuning case it may be possible to upper
bound the target task generalization error of hypotheses in
G ◦ ˆF := {h : ∃f ∈ ˆF , g ∈ G such that h = g ◦ f } us-
ing another measure such as the Rademacher complexity of
G ◦ ˆF , eliminating the need for stochastic hypotheses.

We proposed a novel form of regularization for neu-
ral network training motivated by our theoretical results,
which penalizes divergence from source task weights and
is stricter for lower-level weights. We validated this tech-
nique through applications to image and text classiﬁcation.
Future directions include experiments on more challenging
tasks using deeper and more tailored network architectures
(e.g. convolutional neural networks).

7. Additional Proofs

We provide complete proofs of Theorems 2 and 4. For
brevity, we drop the explicit dependence of f , ˆf , hS and
hT on x in our notation where the meaning is clear.

7.1. Proof of Theorem 2

Proof. Let gS(z) := sign(vS · z), gT (z) := sign(vT · z),
ˆgS(z) := sign(ˆvS · z), ˆgT (z) := sign(d ∗ ˆvS · z), where
d := vS ∗ vT ∈ {−1, 1}k and ∗ is the elementwise product.
It is sufﬁcient to show RT (ˆgT ◦ ˆf ) ≤ cRS(ˆgS ◦ ˆf )+(cid:15)(1+c).
RT (ˆgT ◦ ˆf )
= P rx∼PT (hT d ∗ ˆvS · ˆf ≤ 0)
≤ P rx∼PT (hT d ∗ vS · f ≤ 0, d ∗ vS · f d ∗ ˆvS · ˆf ≥ 0) +
P rx∼PT (hT d ∗ vS · f ≥ 0, d ∗ vS · f d ∗ ˆvS · ˆf ≤ 0)
≤ P rx∼PT (hT d ∗ vS · f ≤ 0)+
P rx∼PT (d ∗ vS · f d ∗ ˆvS · ˆf ≤ 0)

Risk Bounds for Transferring Representations With and Without Fine-Tuning

≤ (cid:15) + P rx∼PT (d ∗ vS · f d ∗ ˆvS · ˆf ≤ 0)
≤ (cid:15) + cP rx∼PS (vS · f ˆvS · ˆf ≤ 0)
≤ (cid:15) + c[P rx∼PS (hS ˆvS · ˆf ≤ 0, hSvS · f ≥ 0)+
P rx∼PS (hS ˆvS · ˆf ≥ 0, hSvS · f ≤ 0)]
≤ (cid:15) + c[P rx∼PS (hS ˆvS · ˆf ≤ 0) + P rx∼PS (hSvS · f ≤ 0)]
≤ cRS(ˆgS ◦ ˆf ) + (cid:15)(1 + c).

The third and ﬁnal inequalities are due to the shared repre-
sentation assumption in the problem statement. The fourth
inequality holds by Lemma 1. The remaining lines apply
simple rules of probability.

Lemma 1. Suppose ∀x, x(cid:48) such that ||M x|| = ||M x(cid:48)||,
PT (x) ≤ cPS(x(cid:48)). Let f, ˆf ∈ F , v, ˆv, d ∈ {−1, 1}k. Then
P rx∼PT (d ∗ v · f d ∗ ˆv · ˆf ≤ 0) ≤ cP rx∼PS (v · f ˆv · ˆf ≤ 0).

Proof. Suppose there is an invertible map Rn → Rn yield-
ing x(cid:48) on input x, such that ∀x, ||M x|| = ||M x(cid:48)|| and
d ∗ v · f (x)d ∗ ˆv · ˆf (x) = v · f (x(cid:48))ˆv · ˆf (x(cid:48)). Then the result
follows since PT (x) ≤ cPS(x(cid:48)) by assumption. Further-
more, if M is an orthogonal matrix, ||x|| = ||x(cid:48)||.
Such a map is x(cid:48) := (M T M )−1M T ˜d ∗ (M x), where ˜d :=
[d1, d1, . . . , dk, dk]. We have ∀i, wi · x(cid:48) = diwi · x and
(αi ˆwi−βiwi)·x(cid:48) = di(αi ˆwi−βiwi)·x, and hence ˆwi·x(cid:48) =
di ˆwi · x for αi, βi (cid:54)= 0. Therefore:
d ∗ v · f (x)d ∗ ˆv · ˆf (x)

= v · d ∗ f (x)ˆv · d ∗ ˆf (x)

= v · f (x(cid:48))ˆv · d ∗ ˆf (x)

= v · f (x(cid:48))ˆv · ˆf (x(cid:48)).

max
|θ(wi, ˆwi)|
√
i
π

2(k−1)

≤ P rx∼P (vS · f vS · ˆf ≤ 0)
≤ P rx∼P (vS · f ˆvS · ˆf ≤ 0)
≤ cP rx∼PS (vS · f ˆvS · ˆf ≤ 0)
≤ c[P rx∼PS (hSvS · f ≤ 0, hS ˆvS · ˆf ≥ 0)+
P rx∼PS (hSvS · f ≥ 0, hS ˆvS · ˆf ≤ 0)]
≤ c[P rx∼PS (hSvS · f ≤ 0) + P rx∼PS (hS ˆvS · ˆf ≤ 0)]
≤ c[(cid:15) + RS(ˆgS ◦ ˆf )].

The ﬁrst inequality holds by Lemma 2. The second in-
equality holds by Lemma 3, using the fact ∀i, wi · ˆwi ≥ 0.
The third inequality uses the rotation invariance of PS. The
following two lines use basic laws of probability. The ﬁnal
inequality uses the assumption RS(gS ◦ f ) ≤ (cid:15).

Proof of ∀˜h ∈ ˜HG◦ ˆF , KL(˜h||˜hˆgS ◦ ˆf ) ≤ ω(RS(ˆgS ◦ ˆf )).
For any ˜hg◦f ∈ ˜HG◦ ˆF , KL(˜hg◦f ||˜hˆgS ◦ ˆf )

[KL(N (wi, σ2I)||N ( ˆwi, σ2I))] + KL(Bv||BˆvS ).

The KL divergence of a product distribution is the sum of
the KL divergences of its component distributions. We up-
per bound both terms and apply the deﬁnition of ω.

KL(N (wi, σ2I)||N ( ˆwi, σ2I))

= 1
2σ2

||wi − ˆwi||2

=

k
(cid:80)
i=1

k
(cid:80)
i=1

k
(cid:80)
i=1

k
(cid:80)
i=1

= 1
2σ2

= 1
σ2

k
(cid:80)
i=1

The ﬁrst equality is a property of the elementwise and dot
products. For the second equality, a(wi·x(cid:48)) = a(diwi·x) =
dia(wi · x) since a is an odd function. Similarly, for the
third equality a( ˆwi · x(cid:48)) = a(di ˆwi · x) = dia( ˆwi · x).

7.2. Proof of Theorem 4
Proof of ∃˜hgT ◦f ∈ ˜HG◦ ˆF such that RT (˜hgT ◦f ) ≤ (cid:15).
Recall that wi are the weight vectors for f and ˆwi are
those for ˆf . Observe that for any wi such that wi · ˆwi < 0,
we have −wi · ˆwi > 0 and −visign(−wi · x) =
visign(wi · x). Combining this with the assumption
max[RS(gS ◦ f ), RT (gT ◦ f )] ≤ (cid:15), we con-

min
f ∈F,gS ,gT ∈G
clude ∃f ∈ F, gS, gT ∈ G such that ∀i, wi · ˆwi ≥ 0 and
max[RS(gS ◦ f ), RT (˜hgT ◦f )] ≤ (cid:15).
Let gS(z) := sign(vS · z) and ˆgS(z) := sign(ˆvS · z). Let
P be a rotation invariant distribution for c = 1. To prove
˜hgT ◦f ∈ ˜HG◦ ˆF , by the deﬁnition of ˜HG◦ ˆF it is sufﬁcient
to show ∀i, |θ(wi, ˆwi)| ≤ π(cid:112)2(k − 1)c(RS(ˆgS ◦ ˆf ) + (cid:15)).

(||wi||2 + || ˆwi||2 − 2||wi|||| ˆwi|| cos |θ(wi, ˆwi)|)

(1 − cos |θ(wi, ˆwi)|)

≤ k

σ2 [1 − cos(π(cid:112)2(k − 1)c(RS(ˆgS ◦ ˆf ) + (cid:15)))].

The ﬁrst equality uses the KL divergence of Gaussian dis-
tributions. The second equality uses the law of cosines. The
third equality is because ∀i, ||wi|| = || ˆwi|| = 1 by con-
struction. The inequality follows by the deﬁnition of ˆF and
the fact that 1 − cos |θ| is non-decreasing for |θ| ∈ [0, π].

KL(Bv||BˆvS )

≤

k
(cid:80)
i=1

(cid:0)k
i

(cid:1)pi(1 − p)k−i log2

pi(1−p)k−i
(1−p)ipk−i

= k[2p − 1 + (1 − p)k] log2
The ﬁrst inequality uses the deﬁnition of Bv to express
KL(Bv||BˆvS ). The equality is a simpliﬁcation.

p
1−p .

Risk Bounds for Transferring Representations With and Without Fine-Tuning

Lemma 2. Suppose k is odd, v ∈ {−1, 1}k, f, ˆf ∈ F such
that ∀i, wi · ˆwi ≥ 0 and P is rotation invariant with c = 1.
Then

≤ P rx∼P (v · f v · ˆf ≤ 0).

max
|θ(wi, ˆwi)|
√
i
π

2(k−1)

Proof. Let v−j := [v1, . . . , vj−1, vj+1, . . . , vk] and deﬁne
f−j and ˆf−j similarly. Let P r(·) := P rx∼P (·).
P r(v · f v · ˆf ≤ 0)

≥ P r(v · f v · ˆf < 0)
≥ P r(v−j · f−j = 0)P r(v · f v · ˆf < 0|v−j · f−j = 0)

= P r(v−j · f−j = 0)
P r(vjfjv−j · ˆf−j + fj

ˆfj < 0|v−j · f−j = 0)

= P r(v−j · f−j = 0)
[P r(vjfjv−j · ˆf−j < −1, fj
P r(vjfjv−j · ˆf−j < 1, fj

ˆfj = 1|v−j · f−j = 0)+

ˆfj = −1|v−j · f−j = 0)]

ˆfj = −1|v−j · f−j = 0)+

ˆfj = −1|v−j · f−j = 0)]

≥ 0.

ˆfj = −1|v−j · f−j = 0)+
ˆfj = −1|v−j · f−j = 0)]
ˆfj = −1|v−j · f−j = 0)
ˆfj = −1)

≥ P r(v−j · f−j = 0)
[P r(vjfjv−j · ˆf−j < −1, fj
P r(vjfjv−j · ˆf−j < 1, fj

= P r(v−j · f−j = 0)
[P r(vjfjv−j · ˆf−j < −1, fj
P r(vjfjv−j · ˆf−j > −1, fj

= P r(v−j · f−j = 0)P r(fj

= P r(v−j · f−j = 0)P r(fj
= (cid:0)k−1

2 )k−1 |θ(wj , ˆwj )|

(cid:1)( 1

π

k−1
2

≥ 2k−1√

2(k−1)

2 )k−1 |θ(wj , ˆwj )|
( 1

π

≥

max
|θ(wi, ˆwi)|
√
i
π

2(k−1)

.

The third inequality follows since P is rotation invariant
and wj · ˆwj ≥ 0. The third and ﬁfth equalities use rotation
invariance. The ﬁnal equality uses rotation invariance and
the fact that k is odd. The fourth inequality is a standard
lower bound for the central binomial coefﬁcient. The other
lines use basic simpliﬁcations and laws of probability.
Lemma 3. Suppose k is odd, v, ˆv ∈ {−1, 1}k, f, ˆf ∈ F
such that ∀i, wi · ˆwi ≥ 0 and P is rotation invari-
ant with c = 1. Then P rx∼P (v · f v · ˆf ≤ 0) ≤
P rx∼P (v · f ˆv · ˆf ≤ 0).
Proof. Let P r(·) := P rx∼P (·) and E[·] := Ex∼P [·]. Let
P r( ˜f ) := P rx∼P ([f1(x) ˆf1(x), . . . , fk(x) ˆfk(x)] = ˜f ).
Let d := ˆv ∗ v and ∆(x) := 1(v · f (x)ˆv · ˆf (x) ≤ 0) −
1(v · f (x)v · ˆf (x) ≤ 0). Assume ˆv (cid:54)= v (if ˆv = v then
1( ˜fi = 1) and
the lemma clearly holds). Let a( ˜f ) :=
i. Let ˜F := { ˜f ∈ {−1, 1}k : a( ˜f ) >

k
(cid:80)
i=1

let l := min
a(d ∗ ˜f ) ∨ (a( ˜f ) = a(d ∗ ˜f ) ∧ ˜fl = 1)}.

i:di=−1

Let Φ(a) := 1

2k−1

(cid:98)k/2(cid:99)
(cid:80)
b=0

b
(cid:80)

(cid:0)a
j

(cid:1)(cid:0)k−a
b−j

(cid:1). The

j=(cid:100)a/2+b/2−k/4(cid:101)

term b counts coordinates where vi
j counts those where vifi = sign(v · f ) and fi = ˆfi.
P r(v · f ˆv · ˆf ≤ 0) − P r(v · f v · ˆf ≤ 0)

ˆfi = sign(v · f ), while

[P r( ˜f ) − P r(d ∗ ˜f )]E[∆| ˜f ]

P r( ˜f )E[∆| ˜f ] + P r(d ∗ ˜f )E[∆|d ∗ ˜f ]

= E[1(v · f ˆv · ˆf ≤ 0)] − E[1(v · f v · ˆf ≤ 0)]
= E[∆]
= (cid:80)
˜f ∈ ˜F
= (cid:80)
˜f ∈ ˜F
= (cid:80)
˜f ∈ ˜F
[P r(v · f v · ˆf ≤ 0|d ∗ ˜f ) − P r(v · f v · ˆf ≤ 0| ˜f )]
= (cid:80)
˜f ∈ ˜F

[P r( ˜f ) − P r(d ∗ ˜f )][Φ(a(d ∗ ˜f )) − Φ(a( ˜f ))]

[P r( ˜f ) − P r(d ∗ ˜f )]

The second equality uses linearity of expectation. The third
equality uses the law of total expectation and the deﬁnition
of ˜F .
The fourth equality holds since E[∆|d ∗ ˜f ]
= (cid:80)

P r(f |d ∗ ˜f )E[∆|d ∗ ˜f , f ]

P r(f |d ∗ ˜f )E[∆| ˜f , f ]

f ∈{−1,1}k
= − (cid:80)

f ∈{−1,1}k

= − (cid:80)

f ∈{−1,1}k

rotation invariance of P .

P r(f | ˜f )E[∆| ˜f , f ] = −E[∆| ˜f ] due to the

The ﬁfth equality holds by expanding ∆, linearity of ex-
pectation, and a similar argument to the previous equality
to show P r(v · f ˆv · ˆf ≤ 0| ˜f ) = P r(v · f v · ˆf ≤ 0|d ∗ ˜f ).

The sixth equality holds by the rotation invariance of P and
the fact that k is odd.

For the ﬁnal inequality, the right hand term is non-negative
since a( ˜f ) ≥ a(d ∗ ˜f ) and Φ is non-increasing. The left
hand term is also non-negative due to the rotation invari-
ance assumption and the fact that ∀i, wi · ˆwi ≥ 0.
Acknowledgements

Daniel McNamara was a visitor at Carnegie Mellon Uni-
versity during the period of this research, supported by a
Fulbright Postgraduate Scholarship.

This work was supported in part by NSF grants CCF-
1422910, CCF-1535967, IIS-1618714, and a Microsoft Re-
search Faculty Fellowship.

We thank the anonymous reviewers for their useful com-
ments.

Risk Bounds for Transferring Representations With and Without Fine-Tuning

References

Argyriou, Andreas, Evgeniou, Theodoros, and Pontil, Mas-
similiano. Convex multi-task feature learning. Machine
Learning, 73(3):243–272, 2008.

Bansal, Mohit, Gimpel, Kevin, and Livescu, Karen. Tai-
loring continuous word representations for dependency
parsing. In Association for Computational Linguistics,
pp. 809–815, 2014.

Hoffman, Judy, Guadarrama, Sergio, Tzeng, Eric S, Hu,
Ronghang, Donahue, Jeff, Girshick, Ross, Darrell,
Trevor, and Saenko, Kate. LSDA: Large scale detection
through adaptation. In Advances in Neural Information
Processing Systems, pp. 3536–3544, 2014.

Long, Mingsheng, Cao, Yue, Wang, Jianmin, and Jordan,
Michael I. Learning transferable features with deep
In International Conference on
adaptation networks.
Machine Learning, pp. 97–105, 2015.

Baxter, Jonathan. A model of inductive bias learning. Jour-
nal of Artiﬁcial Intelligence Research, 12(3):149–198,
2000.

Mansour, Yishay, Mohri, Mehryar, and Rostamizadeh, Af-
shin. Domain adaptation: Learning bounds and algo-
rithms. In Conference on Learning Theory, 2009.

Ben-David, Shai, Blitzer, John, Crammer, Koby, Kulesza,
Alex, Pereira, Fernando, and Vaughan, Jennifer Wort-
man. A theory of learning from different domains. Ma-
chine Learning, 79(1):151–175, 2010.

Maurer, Andreas, Pontil, Massimiliano, and Romera-
Paredes, Bernardino. The beneﬁt of multitask represen-
tation learning. Journal of Machine Learning Research,
17(81):1–32, 2016.

Donahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman,
Judy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. De-
CAF: a deep convolutional activation feature for generic
visual recognition. In International Conference on Ma-
chine Learning, pp. 647–655, 2014.

Evgeniou, Theodoros and Pontil, Massimiliano. Regular-
ized multitask learning. In International Conference on
Knowledge Discovery and Data Mining, pp. 109–117,
2004.

Galanti, Tomer, Wolf, Lior, and Hazan, Tamir. A theoret-
ical framework for deep transfer learning. Information
and Inference, 2016.

Ganin, Yaroslav, Ustinova, Evgeniya, Ajakan, Hana, Ger-
main, Pascal, Larochelle, Hugo, Laviolette, Franc¸ois,
Marchand, Mario, and Lempitsky, Victor. Domain-
adversarial training of neural networks. Journal of Ma-
chine Learning Research, 17(59):1–35, 2016.

Germain, Pascal, Habrard, Amaury, Laviolette, Franc¸ois,
and Morvant, Emilie. A PAC-Bayesian approach for do-
main adaptation with specialization to linear classiﬁers.
In International Conference on Machine Learning, pp.
738–746, 2013.

Ghifary, Muhammad, Kleijn, W Bastiaan, and Zhang,
Mengjie. Domain adaptive neural networks for object
recognition. In Paciﬁc Rim International Conference on
Artiﬁcial Intelligence, pp. 898–904. Springer, 2014.

Girshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik,
Jitendra. Rich feature hierarchies for accurate object de-
tection and semantic segmentation. In IEEE Conference
on Computer Vision and Pattern Recognition, pp. 580–
587, 2014.

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S., and Dean, Jeff. Distributed representations of
In Ad-
words and phrases and their compositionality.
vances in Neural Information Processing Systems, pp.
3111–3119, 2013.

Mohri, Mehryar, Rostamizadeh, Afshin, and Talwalkar,
Ameet. Foundations of Machine Learning. MIT Press,
2012.

Pentina, Anastasia and Lampert, Christoph H. A PAC-
Bayesian bound for Lifelong Learning. In International
Conference on Machine Learning, pp. 991–999, 2014.

Qu, Lizhen, Ferraro, Gabriela, Zhou, Liyuan, Hou, Wei-
wei, Schneider, Nathan, and Baldwin, Timothy. Big data
small data, in domain out-of domain, known word un-
known word: the impact of word representation on se-
quence labelling tasks. In Conference on Computational
Natural Language Learning, pp. 89–93, 2015.

Raina, Rajat, Ng, Andrew Y., and Koller, Daphne. Con-
In
structing informative priors using transfer learning.
International Conference on Machine Learning, pp.
713–720, 2006.

Shalev-Shwartz, Shai and Ben-David, Shai. Understanding
Machine Learning: From Theory to Algorithms. Cam-
bridge University Press, 2014.

Socher, Richard, Ganjoo, Milind, Manning, Christopher D,
and Ng, Andrew. Zero-shot learning through cross-
modal transfer. In Advances in Neural Information Pro-
cessing Systems, pp. 935–943, 2013.

Yosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson,
Hod. How transferable are features in deep neural net-
works? In Advances in Neural Information Processing
Systems, pp. 3320–3328, 2014.

