Supplemental Material: Scaling Up Sparse Support Vector Machines by
Simultaneous Feature and Sample Reduction

Weizhong Zhang * 1 2 Bin Hong * 1 3 Wei Liu 2 Jieping Ye 3 Deng Cai 1 Xiaofei He 1 Jie Wang 3
1State Key Lab of CAD&CG, Zhejiang University, China
2 Tencent AI Lab, Shenzhen, China, 3 University of Michigan, USA

In this supplement, we ﬁrst present the detailed proofs of all the theorems in the main text and then report the rest experi-
ment results which are omitted in the experiment section due to the space limitation.

A. Proof for Theorem 1
Proof. of Theorem 1:
(i) : Let ¯X = (¯x1, ¯x2, ..., ¯xn) and z = 1 − ¯XT w, the primal problem (P∗) then is equivalent to

min
w∈Rp,z∈Rn

α
2

s.t. z = 1 − ¯XT w.

||w||2 + β||w||1 +

(cid:96)([z]i),

1
n

n
(cid:88)

i=1

The Lagrangian then becomes

L(w, z, θ) =

||w||2 + β||w||1 +

(cid:96)([z]i) +

(cid:104)1 − ¯XT w − z, θ(cid:105)

1
n

n
(cid:88)

i=1

1
n

=

||w||2 + β||w||1 −

1
n

+

(cid:104) ¯Xθ, w(cid:105)
(cid:125)

n
(cid:88)

i=1

1
n
(cid:124)

(cid:123)(cid:122)
:=f1(w)

(cid:96)([z]i) −

(cid:104)z, θ(cid:105)

+

(cid:104)1, θ(cid:105)

1
n

1
n

(cid:123)(cid:122)
:=f2(z)

(cid:125)

α
2

α
2
(cid:124)

0 ∈ ∂wL(w, z, θ) = ∂wf1(w) = αw −

¯Xθ + β∂||w||1 ⇔

¯Xθ ∈ αw + β∂||w||1 ⇒ w =

Sβ(

1
n
1
¯Xθ)
n

1
α

We ﬁrst consider the subproblem minw L(w, z, θ):

1
n

α
2

By substituting (19) into f1(w), we get

Then, we consider the problem minz L(w, z, θ):

f1(w) =

||w||2 + β||w||1 − (cid:104)αw + β∂||w||1, w(cid:105) = −

||w||2 = −

||Sβ(

¯Xθ)||2.

(20)

α
2

1
2α

1
n






1

− 1
n [θ]i,
γn [z]i − 1
n − 1

n [θ]i,

1

n [θ]i,

if [z]i < 0,
if 0 ≤ [z]i ≤ γ,
if [z]i > γ.

0 =∇[z]i L(w, z, θ) = ∇[z]if2(z) =

⇒[θ]i =






0,
1
γ [z]i,
1,

if [z]i < 0,
if 0 ≤ [z]i ≤ γ,
if [z]i > γ.

Thus, we have

(17)

(18)

(19)

(21)

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

Combining Eq. (17), Eq. (20) and Eq. (22), we obtain the dual problem:

f2(z) =

(cid:26) − γ

2n ||θ||2,
−∞,

if [θ]i ∈ [0, 1], ∀i ∈ [n],
otherwise .

min
θ∈[0,1]n

1
2α

1
n

||Sβ(

¯Xθ)||2 +

||θ||2 −

(cid:104)1, θ(cid:105)

γ
2n

1
n

(ii) : From Eq. (19) and Eq. (21), we get the KKT conditions:

(22)

(23)

w∗(α, β) =

¯XT θ∗(α, β))

1
n

1
α

Sβ(



[θ∗(α, β)]i =

1

0,
γ (1 − (cid:104)¯xi, w∗(α, β)(cid:105)),
1,



if 1 − (cid:104)¯xi, w∗(α, β)(cid:105) < 0,
if 0 ≤ 1 − (cid:104)¯xi, w∗(α, β)(cid:105) ≤ γ,
if 1 − (cid:104)¯xi, w∗(α, β)(cid:105) > γ.

i = 1, ..., n.

The proof is complete.

B. Proof for Lemma 1
Proof. of Theorem 1:
1) It is the conclusion of the analysis above.
2) After feature screening, the primal problem (P∗) is scaled into:

min
˜w∈R| ˆF c |

α
2

1
n

n
(cid:88)

i=1

|| ˜w||2 + β|| ˜w||1 +

(cid:96)(1 − (cid:104)[¯xi] ˆF c, ˜w(cid:105)),

(scaled-P ∗-1)

Thus, we can easily derive out the dual problem of (scaled-P ∗-1):

min
˜θ∈[0,α]n

˜D(˜θ; α, β) =

1
2α

||Sβ(

1
n ˆF c[ ¯X]˜θ)||2 +

γ
2n

||˜θ||2 −

(cid:104)1, ˜θ(cid:105).

1
n

and also the KKT conditions:

1
n ˆF c [ ¯X]˜θ∗(α, β))

˜w∗(α, β) =

[˜θ∗(α, β)]i =

1
α

Sβ(





1

0,
γ (1 − (cid:104)[¯xi] ˆF c, ˜w∗(α, β)),
1,

if 1 − (cid:104)[¯xi] ˆF c, ˜w∗(α, β)(cid:105) < 0,
if 0 ≤ 1 − (cid:104)[¯xi] ˆF c , ˜w∗(α, β) ≤ γ,
if 1 − (cid:104)[¯xi] ˆF c, ˜w∗(α, β) > γ,

(scaled-D∗-1)

(scaled-KKT-1)

(scaled-KKT-2)

Then, it is obvious that ˜w∗(α, β) = [w∗(α, β)] ˆF c, since essentially, problem (scaled-P ∗-1) can be derived by substituting
0 to the weights for the eliminated features in problem (P∗) and optimize over the rest weights.
Since the solutions w∗(α, β) and θ∗(α, β) satisfy the conditions KKT-1 and KKT-2 and (cid:104)[¯xi] ˆF c, ˜w∗(α, β)(cid:105) =
(cid:104)¯xi, w∗(α, β)(cid:105) for all i , we know ˜w∗(α, β) and θ∗(α, β) satisfy the conditions scaled-KKT-1 and scaled-KKT-2. So
they are the solutions of problems (scaled-P ∗-1) and (scaled-D∗-1). Thus, due to the uniqueness of the solution of prob-
lem (scaled-D∗-1), we have

θ∗(α, β) = ˜θ∗(α, β)

(24)

From 1) we have, [˜θ∗(α, β)] ˆRc = 0 and [˜θ∗(α, β)] ˆLc = 1. Therefore, from the dual problem (scaled-D∗), we can see that
[˜θ∗(C, α)] ˆDc can be recovered from the following problem:

min
ˆθ∈[0,1]| ˆDc |

1
2α

1
n

||Sβ(

ˆG1

ˆθ +

ˆG21)||2 +

||ˆθ||2 −

(cid:104)1, ˆθ(cid:105),

γ
2n

1
n

1
n

Since [˜θ∗(α, β)] ˆDc = [θ∗(α, β)] ˆDc , the proof is therefore completed.

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

C. Proof for Lemma 2
Proof. Due to the α-strong convexity of the objective P (w; α, β), we have

P (w∗(α0, β0); α, β0) ≥ P (w∗(α, β0); α, β0) +

||w∗(α0, β0) − w∗(α, β0)||2

P (w∗(α, β0); α0, β0) ≥ P (w∗(α0, β0); α0, β0) +

||w∗(α0, β0) − w∗(α, β0)||2

which are equivalent to

α
2

α0
2

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

n
(cid:88)

1
n

i=1

1
n

n
(cid:88)

i=1

||w∗(α0, β0)||2 + β0||w∗(α0, β0)||1 +

(cid:96)(1 − (cid:104)¯xi, w∗(α0, β0)(cid:105))

≥

||w∗(α, β0)||2 + β0||w∗(α, β0)||1 +

(cid:96)(1 − (cid:104)¯xi, w∗(α, β0)(cid:105))

||w∗(α0, β0) − w∗(α, β0)||2

||w∗(α, β0)||2 + β0||w∗(α, β0)||1 +

(cid:96)(1 − (cid:104)¯xi, w∗(α, β0)(cid:105))

≥

||w∗(α0, β0)||2 + β0||w∗(α0, β0)||1 +

(cid:96)(1 − (cid:104)¯xi, w∗(α0, β0)(cid:105))

α
2

α
2

+

α0
2

α0
2

+

α
2

α0
2

||w∗(α0, β0) − w∗(α, β0)||2

Adding the above two inequalities together, we get

α − α0
2

||w∗(α0, β0)||2 ≥

||w∗(α, β0)||2 +

||w∗(α0, β0) − w∗(α, β0)||2

α − α0
2

α0 + α
2

⇒||w∗(α, β0) −

w∗(α0, β0)||2 ≤

||w∗(α0, β0)||2

(25)

α0 + α
2α

(α − α0)2
4α2

Substitute the prior that [w∗(α, β0)] ˆF = 0 into (25), we get

||[w∗(α, β0)] ˆF c −
(α − α0)2
4α2

≤

α0 + α
2α

[w∗(α0, β0)] ˆF c ||2
(α0 + α)2
4α2

||w∗(α0, β0)||2 −

||[w∗(α0, β0)] ˆF ||2.

The proof is complete.

D. Proof for Lemma 3
Proof. Firstly, we need to extend the deﬁnition of D(θ; α, β) to Rn:

˜D(θ; α, β) =

(cid:26) D(θ; α, β),
+∞,

if θ ∈ [0, 1]n,
otherwise

(26)

Due to the strong convexity of objective ˜D(θ; α, β), we have

˜D(θ∗(α0, β0), α, β0) ≥ ˜D(θ∗(α, β0), α, β0) +

γ
2n
˜D(θ∗(α, β0), α0, β0) ≥ ˜D(θ∗(α0, β0), α0, β0) +

||θ∗(α0, β0) − θ∗(α, β0)||2,
γ
2n

||θ∗(α0, β0) − θ∗(α, β0)||2.

Since θ∗(α0, β0), θ∗(α, β0) ∈ [0, 1]n, the above inequalities are equivalent to

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

≥

1
2α
1
2α

+

1
n
1
n

¯XT θ∗(α0, β0))||2 +

γ
2n
γ
2n
||θ∗(α0, β0) − θ∗(α, β0)||2,

¯XT θ∗(α, β0))||2 +

||Sβ0(

||Sβ0(
γ
2n

||θ∗(α0, β0)||2 −

||θ∗(α, β0)||2 −

(cid:104)1, θ∗(α0, β0)(cid:105)

1
n
(cid:104)1, θ∗(α, β0)(cid:105)

||Sβ0 (

¯XT θ∗(α, β0))||2 +

||θ∗(α, β0)||2 −

(cid:104)1, θ∗(α, β0)(cid:105)

1
2α0
1
2α0

1
n
1
n

1
n

1
n

1
n

≥

||Sβ0 (

¯XT θ∗(α0, β0))||2 +

||θ∗(α0, β0)||2 −

(cid:104)1, θ∗(α0, β0)(cid:105) +

||θ∗(α0, β0) − θ∗(α, β0)||2.

γ
2n

Adding the above two inequalities, we get

γ(α − α0)
2n
γ(α − α0)
2n

≥

That is equivalent to

||θ∗(α0, β0)||2 −

(cid:104)1, θ∗(α0, β0)

||θ∗(α, β0)||2 −

(cid:104)1, θ∗(α, β0)(cid:105) +

||θ∗(α0, β0) − θ∗(α, β0)||2

γ(α0 + α)
2n

γ
2n
γ
2n

α − α0
n
α − α0
n

||θ∗(α, β0)||2 − (cid:104)

α − α0
γα

1 +

α0 + α
α

θ∗(α0, β0), θ∗(α, β0)(cid:105)

≤ −

||θ∗(α0, β0)||2 −

(cid:104)1, θ∗(α0, β0)(cid:105)

α − α0
γα

α0
α

That is

||θ∗(α, β0) − (

α − α0
2γα

1 +

α0 + α
2α

θ∗(α0, β0))||2 ≤ (

)2||θ∗(α0, β0) −

1||2

α − α0
2α

1
γ

Substitute the priors that [θ∗(α, β0)] ˆR = 0 and [θ∗(α, β0)] ˆL = 1 into (28), we have

||[θ∗(α, β0)] ˆDc − (

α − α0
2γα

1 +

α0 + α
2α

[θ∗(α0, β0)] ˆDc)||2

≤(

α − α0
2α
α − α0
2γα

− ||

)2||θ∗(α0, β0) −

1 +

α0 + α
2α

1||2 − ||

1
γ
[θ∗(α0, β0)] ˆR||2.

(2γ − 1)α + α0
2γα

1 −

α0 + α
2α

[θ∗(α0, β0)] ˆL||2

The proof is complete.

E. Proof for Lemma 4
Before the proof of Lemma 4, we should prove that the optimization problem in (1) is equivalent to

si(α, β0) = max
θ∈Θ

(cid:26) 1
n

|(cid:104)[¯xi] ˆDc , θ(cid:105) + (cid:104)[¯xi] ˆL, 1(cid:105)|

, i ∈ ˆF c.

(cid:27)

To avoid notational confusion, we denote the feasible region Θ in (1) as ˜Θ. Then,

max
θ∈ ˜Θ

= max
θ∈ ˜Θ

= max
θ∈Θ

(cid:26)(cid:12)
1
(cid:12)
(cid:12)
n
(cid:12)
(cid:26) 1
n
(cid:26) 1
n

[ ¯Xθ]i

(cid:27)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= max
θ∈Θ

(cid:26) 1
n

(cid:27)

(cid:12)
(cid:12)¯xiθ(cid:12)
(cid:12)

(cid:12)
(cid:12)[¯xi] ˆDc[θ] ˆDc + [¯xi] ˆL[θ] ˆL + [¯xi] ˆR[θ] ˆR

(cid:12)
(cid:12)

(cid:27)

|(cid:104)[¯xi] ˆDc, [θ] ˆDc(cid:105) + (cid:104)[¯xi] ˆL, 1(cid:105)|

= si(α, β0).

(cid:27)

(27)

(28)

(29)

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

The last equation holds since [θ] ˆL = 1, [θ] ˆR = 0 and [θ ˆDc ] ∈ Θ.

Proof. of Lemma 4:

si(α, β0) = max

{
θ∈B(c,r)

1
n

|(cid:104)[¯xi] ˆDc, θ(cid:105) + (cid:104)[¯xi] ˆL, 1(cid:105)|}.

1
n

= max

{
η∈B(0,r)
1
n

=

|(cid:104)[¯xi] ˆDc, c(cid:105) + (cid:104)[¯xi] ˆL, 1(cid:105) + (cid:104)[¯xi] ˆDc , η(cid:105)|}

(cid:0)|(cid:104)[¯xi] ˆDc , c(cid:105) + (cid:104)[¯xi] ˆL, 1| + (cid:107)[¯xi] ˆDc(cid:107)r(cid:1)

The last equality holds since −(cid:107)[¯xi] ˆDc(cid:107)r ≤ (cid:104)[¯xi] ˆDc, η(cid:105)| ≤ (cid:107)[¯xi] ˆDc(cid:107)r. The proof is complete.

F. Proof for Theorem 4
Proof. (1) It can be obtained from the the rule (R1).
(2) It is from the deﬁnition of ˆF.

G. Proof for Lemma 5
Firstly, we need to point out that the optimization problems in (2) and (3) are equivalent to the problems:

ui(α, β0) = max
w∈W

li(α, β0) = min
w∈W

{1 − (cid:104)[¯xi] ˆF c, w(cid:105)}, i ∈ ˆDc,
{1 − (cid:104)[¯xi] ˆF c , w(cid:105)}, i ∈ ˆDc

(30)

(31)

They follow from the fact that [w] ˆF c ∈ W and

Proof. of Lemma 5:

{1 − (cid:104)w, ¯xi(cid:105)}

={1 − (cid:104)[w] ˆF c , [¯xi] ˆF c(cid:105) − (cid:104)[w] ˆF , [¯xi] ˆF (cid:105)}
={1 − (cid:104)[w] ˆF c , [¯xi] ˆF c(cid:105)} (since [w] ˆF = 0).

ui(α, β0) = max

{1 − (cid:104)[¯xi] ˆF c, w(cid:105)}

w∈B(c,r)

= max

η∈B(0,r)

{1 − (cid:104)[¯xi] ˆF c , c(cid:105) − (cid:104)[¯xi] ˆF c, η(cid:105)}

=1 − (cid:104)[¯xi] ˆF c, c(cid:105) + max

{−(cid:104)[¯xi] ˆF c , η(cid:105)}

η∈B(0,r)

=1 − (cid:104)[¯xi] ˆF c, c(cid:105) + (cid:107)[¯xi] ˆF c (cid:107)r

li(α, β0) = min

{1 − (cid:104)[¯xi] ˆF c, w(cid:105)}

w∈B(c,r)

= min

η∈B(0,r)

{1 − (cid:104)[¯xi] ˆF c, c(cid:105) − (cid:104)[¯xi] ˆF c, η(cid:105)}

=1 − (cid:104)[¯xi] ˆF c , c(cid:105) + min

{−(cid:104)[¯xi] ˆF c, η(cid:105)}

η∈B(0,r)

=1 − (cid:104)[¯xi] ˆF c , c(cid:105) − (cid:107)[¯xi] ˆF c(cid:107)r

The proof is complete.

H. Proof for Theorem 5
Proof. (1) It can be obtained from the the rule (R2).

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

(2) It is from the deﬁnitions of ˆR and ˆL.

I. Proof for Theorem 2
Proof. of Theorem 2:
We prove this theorem by verifying that the solutions w∗(α, β) = 0 and θ∗(α, β) = 1 satisfy the conditions KKT-1 and
KKT-2.
Firstly, since β ≥ βmax = || 1
n
KKT-1.
Then, for all i ∈ [n], we have

¯X1) = 0. Thus w∗(α, β) = 0 and θ∗(α, β) = 1 satisfy the condition

¯X1||∞, we have Sβ( 1
n

1 − (cid:104)¯xi, w∗(α, β)(cid:105) = 1 − 0 > γ.

Thus w∗(α, β) = 0 and θ∗(α, β) = 1 satisfy the condition KKT-2. Hence, they are the solutions for the primal problem
(P∗) and the dual problem (D∗), respectively.

J. Proof for Theorem 3
Proof. of Theorem 3:
Similar with the proof of Theorem 2, we prove this theorem by verifying that the solutions w∗(α, β) = 1
and θ∗(α, β) = 1 satisfy the conditions KKT-1 and KKT-2.

α Sβ( 1

n

¯Xθ∗(α, β))

1. Case 1: αmax(β) ≤ 0. Then for all α > 0, we have

{1 − (cid:104)¯xi, w∗(α, β)(cid:105)}

{1 −

(cid:104)¯xi, Sβ(

1
α

(cid:104)¯xi, Sβ(

max
i∈[n]

1
n
1
n

¯Xθ∗(α, β))(cid:105)} = min
i∈[n]
1
α

¯X1)(cid:105) = 1 − (1 − γ)

αmax(β)

{1 −

(cid:104)¯xi, Sβ(

¯X1)(cid:105)}

1
α

1
n

Then, L = [n] and w∗(α, β) = 1
n
Hence, they are the optimal solution for the primal and dual problems (P∗) and (D∗).

α Sβ( 1

¯Xθ∗(α, β)) and θ∗(α, β) = 1 satisfy the conditions KKT-1 and KKT-2.

2. Case 2: αmax(β) > 0. Then for any α ≥ αmax(β), we have

{1 − (cid:104)¯xi, w∗(α, β)(cid:105)}

{1 −

(cid:104)¯xi, Sβ(

1
α

(cid:104)¯xi, Sβ(

max
i∈[n]

1
n
1
n

¯Xθ∗(α, β))(cid:105)} = min
i∈[n]
1
α

¯X1)(cid:105) = 1 − (1 − γ)

{1 −

(cid:104)¯xi, Sβ(

¯X1)(cid:105)}

1
α

1
n

αmax(β) ≥ 1 − (1 − γ) = γ.

Thus, E ∪ L = [n] and w∗(α, β) = 1
α Sβ( 1
n
Hence, they are the optimal solution for the primal and dual problems (P∗) and (D∗).

¯Xθ∗(α, β)) and θ∗(α, β) = 1 satisfy the conditions KKT-1 and KKT-2.

The proof is complete.

K. Proof for Theorem 6
Proof. of Theorem 6:
(1) Given the reference solutions pair w∗(αi−1,j, βj) and θ∗(αi−1,j, βj), if we do ISS ﬁrst in SIFS and apply ISS and IFS
for inﬁnite times. If after p times of triggering, no new inactive features or samples are identiﬁed, then we can denote the
sequence of ˆF, ˆR and ˆL as:

0 = ˆRA
ˆF A

0 = ˆLA

0 = ∅ ISS−→ ˆF A

1 , ˆRA

1 , ˆLA

1

IF S−→ ˆF A

2 , ˆRA

2 , ˆLA

2

ISS−→ ... ˆF A

p , ˆRA

p , ˆLA

p

IF S/ISS

−→ ...

(32)

min
i∈[n]

= min
i∈[n]
1
α

=1 −

≥1 > γ

min
i∈[n]

= min
i∈[n]
1
α

=1 −

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

with ˆF A

p = ˆF A

p+1 = ˆF A

p+2 = ..., ˆRA

p = ˆRA

p+1 = ˆRA

p+2 = ..., and ˆLA

p = ˆLA

p+1 = ˆLA

p+2 = ...

(33)

In the same way, if we do IFS ﬁrst in SIFS and no new inactive feature or samples are identiﬁed after q times of triggering
of ISS and IFS, then the sequence can be denoted as:

0 = ˆRB
ˆF B

0 = ˆLB

0 = ∅ IF S−→ ˆF B

1 , ˆRB

1 , ˆLB

1

ISS−→ ˆF B

2 , ˆRB

2 , ˆLB

2

IF S−→ ... ˆF B

q , ˆRB

q , ˆLB

q

IF S/ISS

−→ ...

with ˆF B

q = ˆF B

q+1 = ˆF B

q+2 = ..., ˆRB

q = ˆRB

q+1 = ˆRB

q+2 = ..., and ˆLB

q = ˆLB

q+1 = ˆLB

q+2 = ...

We ﬁrst prove that ˆF B

k ⊆ ˆF A

k+1, ˆRB

k ⊆ ˆRA

k+1 and ˆLB

k ⊆ ˆLA

k+1 hold for all k ≥ 0 by induction.

0 ⊆ ˆF A
1 , ˆRB
k+1 and ˆLB

k ⊆ ˆF A
k+1 ⊆ ˆRA
k ⊆ ˆF A

1) When k = 0, the equalities ˆF B
2) If ˆF B
k ⊆ ˆRA
ˆF A
k+2, ˆRB
Thus, ˆF B
Similar with the analysis in (1), we can also prove that ˆF A
Combine (1) and (2), we can get

k+1, ˆRB
k+2 and ˆLB
k ⊆ ˆRA

k+1 ⊆ ˆLA
k+1 and ˆLB

k+2 hold.
k ⊆ ˆLA

0 ⊆ ˆRA
k ⊆ ˆLA

k+1, ˆRB

k+1 hold for all k ≥ 0.

0 ⊆ ˆLA

1 and ˆLB
k+1 hold, by the synergy effect of ISS and IFS, we have ˆF B

1 hold since ˆF B

0 = ˆRB

0 = ˆLB

0 = ∅.

k+1 ⊆

k ⊆ ˆF B

k+1, ˆRA

k ⊆ ˆRB

k+1 and ˆLA

k ⊆ ˆLB

k+1 hold for all k ≥ 0.

ˆF B
0 ⊆ ˆF A
ˆF A
0 ⊆ ˆF B
0 ⊆ ˆRA
ˆRB
ˆRA
0 ⊆ ˆRB
0 ⊆ ˆLA
ˆLB
ˆLA
0 ⊆ ˆLB

1 ⊆ ˆF B
1 ⊆ ˆF A
1 ⊆ ˆRB
1 ⊆ ˆRA
1 ⊆ ˆLB
1 ⊆ ˆLA

2 ⊆ ˆF A
2 ⊆ ˆF B
2 ⊆ ˆRA
2 ⊆ ˆRA
2 ⊆ ˆLA
2 ⊆ ˆLB

3 ....
3 ....
3 ....
3 ....
3 ....
3 ....

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

by the ﬁrst equality of (33), (36) and (37), we can get ˆF A
(2) If p is odd, then by (36), (38 and (40), we have ˆF A
Else if p is even, then by (37), (39) and (41), we have ˆF A
Do the same analysis for q, we can get p ≤ q + 1.
Hence, |p − q| ≤ 1.
The proof is complete.

p = ˆF B

q . Similarly, we can get ˆRA
p ⊆ ˆLB

p+1 and ˆLA

p ⊆ ˆRB

p = ˆLB
q and ˆLA
p = ˆRB
q .
p+1. Thus q ≤ p + 1.

p+1, ˆRA

p ⊆ ˆF B

p ⊆ ˆF B

p+1, ˆRA

p ⊆ ˆRB

p+1 and ˆLA

p ⊆ ˆLB

p+1. Thus q ≤ p + 1.

L. Experiment Result
L.1. Veriﬁcation of the Synergy Effect
Here, we verify the synergy effect between ISS and IFS in SIFS from the experiment results on the dataset real-sim. In
Fig. 4, SIFS performs ISS (sample screening) ﬁrst, while in Fig. 5, it performs IFS (feature screening) ﬁrst. All the rejection
ratios (Fig. 4(a)-(d)) of the 1st triggering of IFS when SIFS performs ISS ﬁrst are much higher than (at least equal to) those
(Fig. 5(a)-(d)) when SIFS performs IFS ﬁrst. In turn, all the rejection ratios (Fig. 5(e)-(h)) of the 1st triggering of ISS when
SIFS performs IFS ﬁrst are also much higher than those (Fig. 4(e)-(h)) when SIFS performs ISS ﬁrst. This demonstrates
that the screening result of ISS can reinforce the capability of IFS and vice versa, which is the so called synergy effect. At
last, in Fig. 5 and Fig. 4, we can see that the overall rejection ratios at the end of SIFS are the same, so no matter which (ISS
or IFS) we perform ﬁrst in SIFS, SIFS has the same screening performances in the end. This is consistent with Theorem 6.

L.2. The Rest Experiment Result
Below, we report the rejection ratios of SIFS on syn1 (Fig. 6), syn3 (Fig. 7), rcv1-train (Fig. 8), rcv1-test(Fig. 9), url
(Fig. 10) and kddb (Fig. 11), which are omitted in the main text due to the space limitation.

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 4. Rejection ratios of SIFS on real-sim when it performs ISS ﬁrst (ﬁrst row: Feature Screening, second row: Sample Screening).

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 5. Rejection ratios of SIFS on real-sim when it performs IFS ﬁrst(ﬁrst row: Feature Screening, second row: Sample Screening).

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 6. Rejection ratios of SIFS on syn1 (ﬁrst row: Feature Screening, second row: Sample Screening).

α/αmax0.010.030.10.41Rejection Ratio0.9850.990.9951Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.9850.990.9951Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.970.980.991Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.9850.990.9951Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.960.981Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.960.981Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.960.981Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.9850.990.9951Trigger 1Trigger 2Trigger 3Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 7. Rejection ratios of SIFS on syn3 (ﬁrst row: Feature Screening, second row: Sample Screening).

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 8. Rejection ratios of SIFS on rcv1-train dataset (ﬁrst row: Feature Screening, second row: Sample Screening).

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 9. Rejection ratios of SIFS on rcv1-test dataset (ﬁrst row: Feature Screening, second row: Sample Screening).

α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.9850.990.9951Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio0.990.9951Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio0.990.9951Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio0.990.9951Round 1Round 2Round 3Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 10. Rejection ratios of SIFS on url dataset (ﬁrst row: Feature Screening, second row: Sample Screening).

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 11. Rejection ratios of SIFS on kddb dataset (ﬁrst row: Feature Screening, second row: Sample Screening).

α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3