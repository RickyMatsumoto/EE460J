On Kernelized Multi-armed Bandits

Sayak Ray Chowdhury 1 Aditya Gopalan 1

Abstract

We consider the stochastic bandit problem with
a continuous set of arms, with the expected re-
ward function over the arms assumed to be ﬁxed
but unknown. We provide two new Gaussian
process-based algorithms for continuous bandit
optimization – Improved GP-UCB (IGP-UCB)
and GP-Thomson sampling (GP-TS), and derive
corresponding regret bounds. Speciﬁcally, the
bounds hold when the expected reward function
belongs to the reproducing kernel Hilbert space
(RKHS) that naturally corresponds to a Gaus-
sian process kernel used as input by the algo-
rithms. Along the way, we derive a new self-
normalized concentration inequality for vector-
valued martingales of arbitrary, possibly inﬁnite,
dimension. Finally, experimental evaluation and
comparisons to existing algorithms on synthetic
and real-world environments are carried out that
highlight the favorable gains of the proposed
strategies in many cases.

1. Introduction

Optimization over large domains under uncertainty is an
important subproblem arising in a variety of sequential de-
cision making problems, such as dynamic pricing in eco-
nomics (Besbes & Zeevi, 2009), reinforcement learning
with continuous state/action spaces (Kaelbling et al., 1996;
Smart & Kaelbling, 2000), and power control in wireless
communication (Chiang et al., 2008). A typical feature of
such problems is a large, or potentially inﬁnite, domain of
decision points or covariates (prices, actions, transmit pow-
ers), together with only partial and noisy observability of
the associated outcomes (demand, state/reward, communi-
cation rate); reward/loss information is revealed only for
decisions that are chosen. This often makes it hard to bal-

1Department of Electrical Communication Engineer-
India.
Indian Institute of Science, Bengaluru, 560012,
Sayak Ray Chowdhury <srchowd-

ing,
Correspondence
hury@ece.iisc.ernet.in>.

to:

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ance exploration and exploitation, as available knowledge
must be transferred efﬁciently from a ﬁnite set of obser-
vations so far to estimates of the values of inﬁnitely many
decisions. A classic case in point is that of the canonical
stochastic MAB with ﬁnitely many arms, where the effort
to optimize scales with the total number of arms or deci-
sions; the effect of this is catastrophic for large or inﬁnite
arm sets.

With suitable structure in the values or rewards of arms,
however, the challenge of sequential optimization can be
efﬁciently addressed. Parametric bandits, especially lin-
early parameterized bandits (Rusmevichientong & Tsitsik-
lis, 2010), represent a well-studied class of structured de-
cision making settings. Here, every arm corresponds to a
known, ﬁnite dimensional vector (its feature vector), and
its expected reward is assumed to be an unknown linear
function of its feature vector. This allows for a large, or
even inﬁnite, set of arms all lying in space of ﬁnite dimen-
sion, say d, and a rich line of work gives algorithms that
attain sublinear regret with a polynomial dependence on
the dimension, e.g., Conﬁdence Ball (Dani et al., 2008),
OFUL (Abbasi-Yadkori et al., 2011) (a strengthening of
Conﬁdence Ball) and Thompson sampling for linear ban-
dits (Agrawal & Goyal, 2013)1 The insight here is that even
though the number of arms can be large, the number of un-
known parameters (or degrees of freedom) in the problem
is really only d, which makes it possible to learn about the
values of many other arms by playing a single arm.

A different approach to modelling bandit problems with a
continuum of arms is via the framework of Gaussian pro-
cesses (GPs) (Rasmussen & Williams, 2006). GPs are
a ﬂexible class of nonparametric models for expressing
uncertainty over functions on rather general domain sets,
which generalize multivariate Gaussian random vectors.
GPs allow tractable regression for estimating an unknown
function given a set of (noisy) measurements of its values
at chosen domain points. The fact that GPs, being distribu-
tions on functions, can also help quantify function uncer-
tainty makes it attractive for basing decision making strate-
gies on them. This has been exploited to great advantage to

1Roughly, for rewards bounded in [−1, 1], these algorithms
(cid:17)
, where ˜O (·) hides polylog(T )

√

(cid:16)

T

d

achieve optimal regret ˜O
factors.

On Kernelized Multi-armed Bandits

build nonparametric bandit algorithms, such as GP-UCB
(Srinivas et al., 2009), GP-EI and GP-PI (Hoffman et al.,
2011). In fact, GP models for bandit optimization, in terms
of their kernel maps, can be viewed as the parametric linear
bandit paradigm pushed to the extreme, where each feature
vector associated to an arm can have inﬁnite dimension 2.

Against this backdrop, our work revisits the problem of
bandit optimization with stochastic rewards. Speciﬁcally,
we consider stochastic multiarmed bandit (MAB) problems
with a continuous arm set, and whose (unknown) expected
reward function is assumed to lie in a reproducing kernel
Hilbert space (RKHS), with bounded RKHS norm – this
effectively enforces smoothness on the function3. We make
the following contributions-

• We design a new algorithm – Improved Gaussian
Process-Upper Conﬁdence Bound (IGP-UCB) – for
stochastic bandit optimization. The algorithm can be
viewed as a variant of GP-UCB (Srinivas et al., 2009),
but uses a signiﬁcantly reduced conﬁdence interval
width resulting in an order-wise improvement in re-
gret compared to GP-UCB. IGP-UCB also shows a
markedly improved numerical performance over GP-
UCB.

(cid:17)

(cid:16)

√

• We develop a nonparametric version of Thompson
sampling, called Gaussian Process Thompson sam-
pling (GP-TS), and show that enjoys a regret bound
of ˜O
. Here, T is the total time horizon and
γT is a quantity depending on the RKHS containing
the reward function. This is, to our knowledge, the
ﬁrst known regret bound for Thompson sampling in
the agnostic setup with nonparametric structure.

dT

γT

• We prove a new self-normalized concentration in-
equality for inﬁnite-dimensional vector-valued mar-
tingales, which is not only key to the design and
analysis of the IGP-UCB and GP-TS algorithms, but
also potentially of independent interest. The inequal-
ity generalizes a corresponding self-normalized bound
for martingales in ﬁnite dimension proven by Abbasi-
Yadkori et al. (2011).

• Empirical comparisons of the algorithms developed
above, with other GP-based algorithms, are presented,
over both synthetic and real-world setups, demonstrat-
ing performance improvements of the proposed algo-
rithms, as well as their performance under misspeciﬁ-
cation.

2The completion of the linear span of all feature vectors (im-
ages of the kernel map) is precisely the reproducing kernel Hilbert
space (RKHS) that characterizes the GP.

3Kernels, and their associated RKHSs,

2. Problem Statement

We consider the problem of sequentially maximizing a
ﬁxed but unknown reward function f : D → R over a
(potentially inﬁnite) set of decisions D ⊂ Rd, also called
actions or arms. An algorithm for this problem chooses, at
each round t, an arm xt ∈ D, and subsequently observes
a reward yt = f (xt) + εt, which is a noisy version of the
function value at xt. The arm xt is chosen causally de-
pending upon the arms played and rewards obtained upto
round t − 1, denoted by the history Ht−1 = {(xs, ys) : s =
1, . . . , t−1}. We assume that the noise sequence {εt}∞
t=1 is
conditionally R-sub-Gaussian for a ﬁxed constant R ≥ 0,
i.e.,

∀t ≥ 0, ∀λ ∈ R, E

(cid:104)

eλεt (cid:12)

(cid:105)

(cid:12) Ft−1

≤ exp

(cid:19)

(cid:18) λ2R2
2

,

(1)

where Ft−1 is the σ-algebra generated by the random vari-
ables {xs, εs}t−1
s=1 and xt.This is a mild assumption on the
noise (it holds, for instance, for distributions bounded in
[−R, R]) and is standard in the bandit literature (Abbasi-
Yadkori et al., 2011; Agrawal & Goyal, 2013).

Regret. The goal of an algorithm is to maximize its cu-
mulative reward or alternatively minimize its cumulative
regret – the loss incurred due to not knowing f ’s maxi-
mum point beforehand. Let x(cid:63) ∈ argmaxx∈D f (x) be
a maximum point of f (assuming the maximum is at-
tained). The instantaneous regret incurred at time t is
rt = f (x(cid:63)) − f (xt), and the cumulative regret in a time
horizon T (not necessarily known a priori) is deﬁned to be
RT = (cid:80)T
t=1 rt. A sub-linear growth of RT in T signiﬁes
that RT /T → 0 as T → ∞, or vanishing per-round regret.

Regularity Assumptions. Attaining sub-linear regret is
impossible in general for arbitrary reward functions f and
domains D, and thus some regularity assumptions are in
order. In what follows, we assume that D is compact. The
smoothness assumption we make on the reward function f
is motivated by Gaussian processes4 and their associated
reproducing kernel Hilbert spaces (RKHSs, see Sch¨olkopf
& Smola (2002)). Speciﬁcally, we assume that f has small
norm in the RKHS of functions D → R, with positive
semi-deﬁnite kernel function k : D ×D → R. This RKHS,
denoted by Hk(D), is completely speciﬁed by its kernel
function k(·, ·) and vice-versa, with an inner product (cid:104)·, ·(cid:105)k
obeying the reproducing property: f (x) = (cid:104)f, k(x, ·)(cid:105)k for
all f ∈ Hk(D). In other words, the kernel plays the role
of delta functions to represent the evaluation map at each
point x ∈ D via the RKHS inner product. The RKHS
norm (cid:107)f (cid:107)k = (cid:112)(cid:104)f, f (cid:105)k is a measure of the smoothness5
4Other work has also studied continuum-armed bandits with
weaker smoothness assumptions such as Lipschitz continuity –
see Related work for details and comparison.

5One way to see this is that

for every element g in
|g(x) − g(y)| = |(cid:104)g, k(x, ·) − k(y, ·)(cid:105)| ≤

the RKHS,
(cid:107)g(cid:107)k (cid:107)k(x, ·) − k(y, ·)(cid:107)k by Cauchy-Schwarz.

On Kernelized Multi-armed Bandits

of f , with respect to the kernel function k, and satisﬁes:
f ∈ Hk(D) if and only if (cid:107)f (cid:107)k < ∞.
We assume a known bound on the RKHS norm of the un-
known target function6: (cid:107)f (cid:107)k ≤ B. Moreover, we assume
bounded variance by restricting k(x, x) ≤ 1, for all x ∈ D.
Two common kernels that satisfy bounded variance prop-
erty are Squared Exponential and Mat´ern, deﬁned as

kSE(x, x(cid:48)) = exp

kM at´ern(x, x(cid:48)) =

(cid:16)

− s2/2l2(cid:17)
√

,

(cid:16) s

21−ν
Γ(ν)

2ν
l

(cid:17)ν

(cid:16) s

Bν

√

(cid:17)
,

2ν
l

where l > 0 and ν > 0 are hyperparameters, s =
(cid:107)x − x(cid:48)(cid:107)2 encodes the similarity between two points
x, x(cid:48) ∈ D, and Bν(·) is the modiﬁed Bessel function. Gen-
erally the bounded variance property holds for any station-
ary kernel, i.e. kernels for which k(x, x(cid:48)) = k(x − x(cid:48)) for
all x, x(cid:48) ∈ Rd. These assumptions are required to make
the regret bounds scale-free and are standard in the litera-
ture (Agrawal & Goyal, 2013). Instead if k(x, x) ≤ c or
(cid:107)f (cid:107)k ≤ cB, then our regret bounds would increase by a
factor of c.

3. Algorithms

Design philosophy. Both the algorithms we propose
use Gaussian likelihood models for observations, and
Gaussian process (GP) priors for uncertainty over re-
ward functions. A Gaussian process over D, denoted
by GPD(µ(·), k(·, ·)), is a collection of random variables
(f (x))x∈D, one for each x ∈ D, such that every ﬁnite
sub-collection of random variables (f (xi))m
i=1 is jointly
Gaussian with mean E [f (xi)] = µ(xi) and covariance
E [(f (xi) − µ(xi))(f (xj) − µ(xj))] = k(xi, xj), 1 ≤
i, j ≤ m, m ∈ N. The algorithms use GPD(0, v2k(·, ·)),
v > 0, as an initial prior distribution for the unknown re-
ward function f over D, where k(·, ·) is the kernel func-
tion associated with the RKHS Hk(D) in which f is as-
sumed to have ‘small’ norm at most B. The algorithms
also assume that the noise variables εt = yt − f (xt)
are drawn independently, across t, from N (0, λv2), with
λ ≥ 0. Thus, the prior distribution for each f (x), is as-
sumed to be N (0, v2k(x, x)), x ∈ D. Moreover, given
a set of sampling points At = (x1, . . . , xt) within D, it
follows under the assumption that the corresponding vec-
tor of observed rewards y1:t = [y1, . . . , yt]T has the mul-
tivariate Gaussian distribution N (0, v2(Kt + λI)), where
Kt = [k(x, x(cid:48))]x,x(cid:48)∈At is the kernel matrix at time t. Then,
by the properties of GPs, we have that y1:t and f (x) are
jointly Gaussian given At:

(cid:21)
(cid:20)f (x)
y1:t

(cid:18)

∼ N

0,

(cid:20)v2k(x, x)
v2kt(x)

v2kt(x)T
v2(Kt + λI)

(cid:21)(cid:19)

,

6This is analogous to the bound on the weight θ typically as-

sumed in regret analyses of linear parametric bandits.

where kt(x) = [k(x1, x), . . . , k(xt, x)]T . Therefore con-
ditioned on the history Ht, the posterior distribution over f
is GPD(µt(·), v2kt(·, ·)), where

µt(x) = kt(x)T (Kt + λI)−1y1:t,

(2)
kt(x, x(cid:48)) = k(x, x(cid:48)) − kt(x)T (Kt + λI)−1kt(x(cid:48)),(3)
(4)

σ2
t (x) = kt(x, x).

Thus for every x ∈ D, the posterior distribution of f (x),
given Ht, is N (µt(x), v2σ2
t (x)).

Remark. Note that the GP prior and Gaussian likelihood
model described above is only an aid to algorithm design,
and has nothing to do with the actual reward distribution
or noise model as in the problem statement (Section 2).
The reward function f is a ﬁxed, unknown, member of the
RKHS Hk(D), and the true sequence of noise variables εt
is allowed to be a conditionally R-sub-Gaussian martingale
difference sequence (Equation 1). In general, thus, this rep-
resents a misspeciﬁed prior and noise model, also termed
the agnostic setting by Srinivas et al. (2009).

The proposed algorithms, to follow, assume the knowledge
of only the sub-Gaussianity parameter R, kernel function k
and upper bound B on the RKHS norm of f . Note that v, λ
are free parameters (possibly time-dependent) that can be
set speciﬁc to the algorithm.

3.1. Improved GP-UCB (IGP-UCB) Algorithm

We introduce the IGP-UCB algorithm (Algorithm 1), that
uses a combination of the current posterior mean µt−1(x)
and standard deviation vσt−1(x) to (a) construct an upper
conﬁdence bound (UCB) envelope for the actual function
f over D, and (b) choose an action to maximize it. Specif-
ically it chooses, at each round t, the action

xt = argmax

µt−1(x) + βtσt−1(x),

(5)

x∈D

with the scale parameter v set to be 1. Such a rule
trades off exploration (picking points with high uncertainty
σt−1(x)) with exploitation (picking points with high re-
ward µt−1(x)), with βt = B + R(cid:112)2(γt−1 + 1 + ln(1/δ))
being the parameter governing the tradeoff, which we later
show is related to the width of the conﬁdence interval for f
at round t. δ ∈ (0, 1) is a free conﬁdence parameter used
by the algorithm, and γt is the maximum information gain
at time t, deﬁned as:

γt := max

I(yA; fA).

A⊂D:|A|=t

Here, I(yA; fA) denotes the mutual information between
fA = [f (x)]x∈A and yA = fA + εA, where εA ∼
N (0, λv2I) and quantiﬁes the reduction in uncertainty
about f after observing yA at points A ⊂ D. γt is
a problem dependent quantity and can be found given
the knowledge of domain D and kernel function k. For

On Kernelized Multi-armed Bandits

a compact subset D of Rd, γT is O((ln T )d+1) and
O(T d(d+1)/(2ν+d(d+1)) ln T ), respectively, for the Squared
Exponential and Mat´ern kernels (Srinivas et al., 2009), de-
pending only polylogarithmically on the time T .

Algorithm 1 Improved-GP-UCB (IGP-UCB)

Input: Prior GP (0, k), parameters B, R, λ, δ.
for t = 1, 2, 3 . . . T do

Set βt = B + R(cid:112)2(γt−1 + 1 + ln(1/δ)).
Choose xt = argmax

µt−1(x) + βtσt−1(x).

x∈D
Observe reward yt = f (xt) + εt.
Perform update to get µt and σt using 2, 3 and 4.

end for

Discussion. Srinivas et al. (2009) have proposed the GP-
UCB algorithm, and Valko et al. (2013) the KernelUCB
algorithm, for sequentially optimizing reward functions ly-
ing in the RKHS Hk(D). Both algorithms play an arm
at time t using the rule: xt = argmaxx∈D µt−1(x) +
˜βtσt−1(x). GP-UCB uses the exploration parameter ˜βt =
(cid:113)
2B2 + 300γt−1 ln3(t/δ), with λ set to σ2, where σ is
additionally assumed to be a known, uniform (i.e., almost-
sure) upper bound on all noise variables εt (Srinivas et al.,
2009, Theorem 3). Compared to GP-UCB, IGP-UCB (Al-
gorithm 1) reduces the width of the conﬁdence interval by
a factor roughly O(ln3/2 t) at every round t, and, as we
will see, this small but critical adjustment leads to much
better theoretical and empirical performance compared to
GP-UCB. In KernelUCB, ˜βt is set as η/λ1/2, where η is
the exploration parameter and λ is the regularization con-
stant. Thus IGP-UCB can be viewed as a special case of
KernelUCB where η = βt.

3.2. Gaussian Process Thompson Sampling (GP-TS)

Our second algorithm, GP-TS (Algorithm 2),
inspired
by the success of Thompson sampling for standard and
parametric bandits (Agrawal & Goyal, 2012; Kaufmann
et al., 2012; Gopalan et al., 2014; Agrawal & Goyal,
2013), uses the time-varying scale parameter vt = B +
R(cid:112)2(γt−1 + 1 + ln(2/δ)) and operates as follows. At
each round t, GP-TS samples a random function ft(·) from
the GP with mean function µt−1(·) and covariance function
v2
t kt−1(·, ·). Next, it chooses a decision set Dt ⊂ D, and
7. We call it GP-
plays the arm xt ∈ Dt that maximizes ft
Thompson-Sampling as it falls under the general frame-
work of Thompson Sampling, i.e., (a) assume a prior on the
underlying parameters of the reward distribution, (b) play
the arm according to the prior probability that it is optimal,

7If Dt = D for all t, then this is simply exact Thompson
sampling. For technical reasons, however, our regret bound is
valid when Dt is chosen as a suitable discretization of D, so we
include Dt as an algorithmic parameter.

and (c) observe the outcome and update the prior. However,
note that the prior is nonparametric in this case.

Algorithm 2 GP-Thompson-Sampling (GP-TS)
Input: Prior GP (0, k), parameters B, R, λ, δ.
for t = 1, 2, 3 . . . , do

Set vt = B + R(cid:112)2(γt−1 + 1 + ln(2/δ)).
Sample ft(·) from GPD(µt−1(·), v2
Choose the current decision set Dt ⊂ D.
Choose xt = argmax

ft(x).

t kt−1(·, ·)).

x∈Dt
Observe reward yt = f (xt) + εt.
Perform update to get µt and kt using 2 and 3.

end for

4. Main Results

We begin by presenting two key concentration inequalities
which are essential in bounding the regret of the proposed
algorithms.

t=1 be an Rd-valued discrete time
Theorem 1 Let {xt}∞
stochastic process predictable with respect to the ﬁltration
{Ft}∞
t=0, i.e., xt is Ft−1-measurable ∀t ≥ 1. Let {εt}∞
t=1
be a real-valued stochastic process such that for some R ≥
0 and for all t ≥ 1, εt is (a) Ft-measurable, and (b) R-sub-
Gaussian conditionally on Ft−1. Let k : Rd×Rd → R be a
symmetric, positive-semideﬁnite kernel, and let 0 < δ ≤ 1.
For a given η > 0, with probability at least 1 − δ, the fol-
lowing holds simultaneously over all t ≥ 0:

(cid:107)ε1:t(cid:107)2

(cid:112)det((1 + η)I + Kt)
δ

((Kt+ηI)−1+I)−1 ≤ 2R2 ln

.
(6)
(Here, Kt denotes the t × t matrix Kt(i, j) = k(xi, xj),
1 ≤ i, j ≤ t and for any x ∈ Rt and A ∈ Rt×t, (cid:107)x(cid:107)A :=
√
xT Ax). Moreover, if Kt is positive deﬁnite ∀t ≥ 1 with
probability 1, then the conclusion above holds with η = 0.

Theorem 1 represents a self-normalized concentration in-
equality: the ‘size’ of the increasing-length sequence {εt}t
of martingale differences is normalized by the growing
quantity ((Kt + ηI)−1 + I)−1 that explicitly depends on
the sequence. The following lemma helps provide an al-
ternative, abstract, view of the self-normalized process of
Theorem 1, based on the feature space representation in-
duced by a kernel.

Lemma 1 Let k : Rd × Rd → R be a symmetric, positive-
semideﬁnite kernel, with associated feature map ϕ : Rd →
Hk and the reproducing kernel Hilbert space8 (RKHS) Hk.

8Such a pair (ϕ, Hk) always exists, see e.g., Rasmussen &

Williams (2006).

On Kernelized Multi-armed Bandits

Letting St = (cid:80)t
mensional) matrix9 Vt = I+(cid:80)t
whenever Kt is positive deﬁnite, that

s=1 εsϕ(xs) and the (possibly inﬁnite di-
s=1 ϕ(xs)ϕ(xs)T , we have,

(cid:107)ε1:t(cid:107)(K−1

,

t +I)−1 = (cid:107)St(cid:107)V −1
(cid:13)
(cid:13)
(cid:13)V −1/2
(cid:13)
(cid:13)
(cid:13)Hk

t

t

t

St

:=

denotes the norm of

St in the RKHS Hk.

where (cid:107)St(cid:107)V −1
V −1/2
t
(cid:3) =
Observe that St is Ft-measurable and also E (cid:2)St
St−1. The process {St}t≥0 is thus a martingale with val-
ues10 in the RKHS H, which can possibly be inﬁnite-
dimensional, and moreover, whose deviation is measured
by the norm weighted by V −1
, which is itself derived from
St. Theorem 1 represents the kernelized generalization
of the ﬁnite-dimensional result of Abbasi-Yadkori et al.
(2011), and we recover their result under the special case
of a linear kernel: ϕ(x) = x for all x ∈ Rd.

(cid:12)
(cid:12) Ft−1

t

We remark that when ϕ is a mapping to a ﬁnite-dimensional
Hilbert space, the argument of Abbasi-Yadkori et al. (2011,
Theorem 1) can be lifted to establish Theorem 1, but
inﬁnite-dimensional
it breaks down in the generalized,
RKHS setting, as the self-normalized bound in their pa-
per has an explicit, growing dependence on the feature di-
mension. Speciﬁcally, the method of mixtures (de la Pena
et al., 2009) or Laplace method, as dubbed by Maillard
(2016), fails to hold in inﬁnite dimension. The primary rea-
son for this is that the mixture distribution for ﬁnite dimen-
sional spaces can be chosen independently of time, but in a
nonparametric setup like ours, where the dimensionality of
the self-normalizing factor (cid:0)K −1
itself grows with
time, the use of (random) stopping times, precludes using
time-dependent mixtures. We get around this difﬁculty by
applying a novel ‘double mixture’ construction, in which a
pair of mixtures on (a) the space of real-valued functions on
Rd, i.e., the support of a Gaussian process, and (b) on real
sequences is simultaneously used to obtain a more general
result, of potentially independent interest.

t + I(cid:1)−1

Our next result shows that how the posterior mean is con-
centrated around the unknown reward function f .

Theorem 2 Under the same hypotheses as those of Theo-
rem 1, let D ⊂ Rd, and f : D → R be a member of the
RKHS of real-valued functions on D with kernel k, with
RKHS norm bounded by B. Then, with probability at least
1 − δ, the following holds for all x ∈ D and t ≥ 1:
(cid:17)
B + R(cid:112)2(γt−1 + 1 + ln(1/δ))

|µt−1(x) − f (x)| ≤

(cid:16)

σt−1(x),

posterior distribution deﬁned as in Equation 2, 3, 4, with λ
set to 1 + η and η = 2/T .

Theorem 3.5 of Maillard (2016) states a similar result on
the estimation of the unknown reward function from the
RKHS. We improve upon it in the sense that the conﬁdence
bound in Theorem 2 is simultaneous over all x ∈ D, while
the bound has been shown only for a single, ﬁxed x in the
Kernel Least-squares setting. We are able to achieve this
result by virtue of Theorem 1.

4.1. Regret Bound of IGP-UCB

Theorem 3 Let δ ∈ (0, 1), (cid:107)f (cid:107)k ≤ B and εt is condition-
ally R-sub-Gaussian. Running IGP-UCB for a function f
lying in the RKHS Hk(D), we obtain a regret bound of
with high probability. More pre-
O

γT + γT )

T (B

(cid:16)√

√

(cid:17)

cisely, with probability at least 1 − δ, RT = O
(cid:17)
(cid:112)T γT (γT + ln(1/δ))
.

(cid:16)

√

B

T γT +

√

T (B

Improvement over GP-UCB. Srinivas et al. (2009), in
the course of analyzing the GP-UCB algorithm, show
that when the reward function lies in the RKHS Hk(D),
(cid:16)√
(cid:17)
γT + γT ln3/2(T ))
GP-UCB obtains regret O
with high probability (see Theorem 3 therein for the ex-
act bound). Furthermore, they assume that the noise εt
is uniformly bounded by σ, while our sub-Gaussianity as-
sumption (see Equation 1) is slightly more general, and
we are able to obtain a O(ln3/2 T ) multiplicative factor
improvement in the ﬁnal regret bound thanks to the new
self-normalized inequality (Theorem 1). Additionally, in
our numerical experiments, we observe a signiﬁcantly im-
proved performance of IGP-UCB over GP-UCB, both on
synthetically generated function, and on real-world sensor
measurement data (see Section 6).

Comparison with KernelUCB. Valko et al. (2013) show
(cid:112) ˜dT ),
that the cumulative regret of KernelUCB is ˜O(
where ˜d, deﬁned as the effective dimension, measures, in
a sense, the number of principal directions over which
the projection of the data in the RKHS is spread. They
show that ˜d is at least as good as γT , precisely γT ≥
Ω( ˜d ln ln T ) and thus the regret bound of KernelUCB is
roughly ˜O(
γT factor better than IGP-
UCB. However, KernelUCB requires the number of actions
to be ﬁnite, so the regret bound is not applicable for inﬁnite
or continuum action spaces.

T γT ), which is

√

√

where γt−1 is the maximum information gain after t − 1
rounds and µt−1(x), σ2
t−1(x) are mean and variance of

4.2. Regret Bound of GP-TS

9More formally, Vt : Hk → Hk is the linear operator deﬁned
s=1 ϕ(xs)(cid:104)ϕ(xs), z(cid:105) ∀z ∈ Hk.

by Vt(z) = z + (cid:80)t

10We ignore issues of measurability here.

For technical reasons, we will analyze the following ver-
sion of GP-TS. At each round t, the decision set used
by GP-TS is restricted to be a unique discretization Dt

On Kernelized Multi-armed Bandits

of D with the property that |f (x) − f ([x]t)| ≤ 1/t2
for all x ∈ D, where [x]t is the closest point to x in
Dt. This can always be achieved by choosing a com-
pact and convex domain D ⊂ [0, r]d and discretization
Dt with size |Dt| = (BLrdt2)d such that (cid:107)x − [x]t(cid:107)1 ≤
rd/BLrdt2 = 1/BLt2 for all x ∈ D, where L =
(cid:17)1/2

|p=q=x

. This implies, for every x ∈

(cid:16) ∂2k(p,q)
∂pj ∂qj

ilar O(ln3/2 T ) factor improvement, as obtained by IGP-
UCB over GP-UCB, was achieved in the linear parametric
setting by (Abbasi-Yadkori et al., 2011) in the OFUL al-
gorithm, over its predecessor ConﬁdenceBall (Dani et al.,
2008). Finally we see that the for linear bandit problem
with inﬁnitely many actions, IGP-UCB attains the infor-
T ) (see (Dani et al.,
mation theoretic lower bound of Ω(d
2008)), but GP-TS is a factor of

d away from it.

√

√

sup
j∈[d]

sup
x∈D
D,

|f (x) − f ([x]t)| ≤ (cid:107)f (cid:107)k L (cid:107)x − [x]t(cid:107)1 ≤ 1/t2,

(7)

as any f ∈ Hk(D) is Lipschitz continuous with constant
(cid:107)f (cid:107)k L (De Freitas et al., 2012, Lemma 1).

Theorem 4 (Regret bound for GP-TS) Let δ ∈ (0, 1),
D ⊂ [0, r]d be compact and convex, (cid:107)f (cid:107)k ≤ B and
{εt}t a conditionally R-sub-Gaussian sequence. Run-
ning GP-TS for a function f lying in the RKHS Hk(D)
and with decision sets Dt chosen as above, with prob-
the regret of GP-TS satis-
ability at
ﬁes RT = O
T γT +
(cid:17)(cid:17)
B(cid:112)T ln(2/δ)

(cid:16)(cid:112)(γT + ln(2/δ))d ln(BdT )
.

least 1 − δ,

(cid:16)√

√

√

d factor away from the bound ˜O(γT

Comparison with IGP-UCB. Observe that regret scal-
ing of GP-TS is ˜O(γT
dT ) which is a multiplicative
√
T ) obtained for
IGP-UCB and similar behavior is reﬂected in our simula-
tions on synthetic data. The additional multiplicative fac-
tor of (cid:112)d ln(BdT ) in the regret bound of GP-TS is es-
sentially a consequence of discretization. How to remove
this extra logarithmic dependency, and make the analysis
discretization-independent, remains an open question.

Remark. The regret bound for GP-TS is inferior compared
to IGP-UCB in terms of the dependency on dimension d,
but to the best of our knowledge, Theorem 4 is the ﬁrst
(frequentist) regret guarantee of Thompson Sampling in the
agnostic, non-parametric setting of inﬁnite action spaces.

√

Linear Models and a Matching Lower Bound.
If the
mean rewards are perfectly linear, i.e.
if there exists a
θ ∈ Rd such that f (x) = θT x for all x ∈ D, then we
are in the parametric setup, and one way of casting this
in the kernelized framework is by using the linear kernel
k(x, x(cid:48)) = xT x(cid:48). For this kernel, γT = O(d ln T ), and the
regret scaling of IGP-UCB is ˜O(d
T ) and that of GP-TS
is ˜O(d3/2
T ), which recovers the regret bounds of their
linear, parametric analogues OFUL (Abbasi-Yadkori et al.,
2011) and Linear Thompson sampling (Agrawal & Goyal,
2013), respectively. Moreover, in this case ˜d = d, thus
d factor away from that of Ker-
the regret of IGP-UCB is
nelUCB. But the regret bound of KernelUCB also depends
on the number of arms N , and if N is exponential in d,
then it also suffers ˜O(d
T ) regret. We remark that a sim-

√

√

√

5. Overview of Techniques

We brieﬂy outline here the key arguments for all the theo-
rems in Section 4. See Chowdhury & Gopalan (2017) for
complete proofs.

Proof Sketch for Theorem 1.
It is convenient to as-
sume that Kt, the induced kernel matrix at time t, is in-
vertible, since this is where the crux of the argument lies.
First we show that for any function g : D → R and
for all t ≥ 0, thanks to the sub-Gaussian property (1),
is a non-
the process
negative super-martingale with respect to the ﬁltration Ft,
:= [g(x1), . . . , g(xt)]T and in fact satisﬁes
where g1:t
E [M g
t ] ≤ 1. The chief difﬁculty is to handle the behav-
ior of Mt at a (random) stopping time, since the sizes of
quantities such as ε1:t at the stopping time will be random.

(cid:111)
2 (cid:107)g1:t(cid:107)2)

t := exp(εT

1:tg1:t − 1

M g

(cid:110)

t

i.e.,

the space RD.

We next construct a mixture martingale Mt by mix-
ing M g
t over g drawn from an independent GPD(0, k)
Gaussian process, which is a measure over a large
Then, by a
space of functions,
change of measure argument, we show that this induces
a mixture distribution which is essentially N (0, Kt) over
any desired ﬁnite dimension t,
thus obtaining Mt =
(cid:16) 1
2 (cid:107)ε1:t(cid:107)2
1√
. Next from the fact
(I+K−1
that E [Mτ ] ≤ 1 and from Markov’s inequality, for any
δ ∈ (0, 1), we obtain

det(I+Kt)

exp

)−1

(cid:17)

t

(cid:104)

P

(cid:107)ε1:τ (cid:107)2

(K−1

τ +I)−1 > 2 ln

(cid:16)(cid:112)det(I + Kτ )/δ

(cid:17)(cid:105)

≤ δ.

Finally, we lift this bound for all t through a standard stop-
ping time construction as in Abbasi-Yadkori et al. (2011).

i.e.

λ = 1.

(cid:12)kt(x)T (Kt + I)−1ε1:t

Proof Sketch for Theorem 2. Here we sketch the
special case of η = 0,
Ob-
serve that |µt(x) − f (x)| is upper bounded by sum of
two terms, P := (cid:12)
(cid:12)
(cid:12) and Q :=
(cid:12)
(cid:12)kt(x)T (Kt + I)−1f1:t − f (x)(cid:12)
(cid:12). Now we observe that
t Φt + I)−1ϕ(x) and use this obser-
σ2
t (x) = ϕ(x)T (ΦT
vation to show that P = (cid:12)
(cid:12)
t Φt + I)−1ΦT
t ε1:t
(cid:12)
and Q = (cid:12)
(cid:12)ϕ(x)T (ΦT
(cid:12), which are in turn up-
and (cid:107)f (cid:107)k σt(x)
per bounded by the terms σt(x) (cid:107)St(cid:107)V −1
respectively. Then the result follows using Theorem 1,
along with the assumption that (cid:107)f (cid:107)k ≤ B and the fact that
1
2 ln(det(I + Kt)) ≤ γt a.s. when Kt is invertible.

t Φt + I)−1f (cid:12)

(cid:12)ϕ(x)T (ΦT

t

On Kernelized Multi-armed Bandits

Proof Sketch for Theorem 3. First from Theorem 2 and
the choice of xt in Algorithm 1, we show that the instanta-
neous regret rt at round t is upper bounded by 2βtσt−1(xt)
with probability at least 1 − δ. Then the result follows by
bounding the term (cid:80)T

t=1 σt−1(xt) by O(

T γT ).

√

Proof Sketch for Theorem 4. We follow a similar ap-
proach given in Agrawal & Goyal (2013) to prove the re-
gret bound of GP-TS. First observe that from our choice of
discretization sets Dt, the instantaneous regret at round t
is given by rt = f (x(cid:63)) − f ([x(cid:63)]t) + f ([x(cid:63)]t) − f (xt) ≤
1
t2 + ∆t(xt), where ∆t(x) := f ([x(cid:63)]t) − f (x) and [x(cid:63)]t
is the closest point to x(cid:63) in Dt. Now at each round t, af-
ter an action is chosen, our algorithm improves the conﬁ-
dence about true reward function f , via an update of µt(·)
and kt(·, ·). However, if we play a suboptimal arm, the re-
gret suffered can be much higher than the improvement of
our knowledge. To overcome this difﬁculty, at any round
t, we divide the arms (in the present discretization Dt)
into two groups: saturated arms, St, deﬁned as those with
∆t(x) > ctσt−1(x) and unsaturated otherwise, where ct
is an appropriate constant. The idea is to show that the
probability of playing a saturated arm is small and then
bound the regret of playing an unsaturated arm in terms
of standard deviation. This is useful because the inequality
(cid:80)T
T γT ) allows us to bound the total

t=1 σt−1(xt) ≤ O(

√

regret due to unsaturated arms.

(cid:48)

(cid:48)

(cid:104)
xt ∈ Dt \ St

First we lower bound the probability of playing an unsatu-
rated arm at round t. We deﬁne a ﬁltration F
t−1 as the his-
tory Ht−1 up to round t − 1 and prove that for “most” (in
t−1, P
≥
a high probability sense) F
√
p−1/t2, where p = 1/4e
π. This observation, along with
concentration bounds for ft(x) and f (x) and “smoothness”
of f , allow us to show that the expected regret at round
t is upper bounded in terms of σt−1(xt), i.e.
in terms
of regret due to playing an unsaturated arm. More pre-
cisely, we show that for “most” F
≤

(cid:12)
(cid:12) F

(cid:48)
t−1

(cid:104)
rt

t−1, E

(cid:12)
(cid:12) F

(cid:48)
t−1

(cid:105)

(cid:105)

(cid:48)

(cid:105)

(cid:104)

E

(cid:12) F

11ct
p

+ 2B+1

, and use it to prove

t2
p σt−1(xt) − 2B+1

σt−1(xt) (cid:12)
(cid:48)
t−1
that Xt (cid:39) rt − 11ct
; t ≥ 1 is a
super-martingale difference sequence adapted to ﬁltration
{F
t }t≥1. Now, using the Azuma-Hoeffding inequality,
along with the bound on (cid:80)T
t=1 σt−1(xt), we obtain the de-
sired high-probability regret bound.

t2

(cid:48)

6. Experiments

In this section we provide numerical results on both syn-
thetically generated test functions and functions from real-
world data. We compare GP-UCB, IGP-UCB and GP-TS
with GP-EI and GP-PI11.

11GP-EI and PI perform similarly and thus are not separately

distinguishable in the plots.

(a)

(b)

Figure 1. Cumulative regret for functions lying in the RKHS cor-
responding to (a) Squared Exponential kernel and (b) Mat´ern ker-
nel.

(a)

(b)

Figure 2. Cumulative regret for functions lying in the GP corre-
sponding to (a) Squared Exponential kernel and (b) Mat´ern ker-
nel.

Synthetic Test Functions. We use the following procedure
to generate test functions from the RKHS. First we sample
100 points uniformly from the interval [0, 1] and use that as
our decision set. Then we compute a kernel matrix K on
those points and draw reward vector y ∼ N (0, K). Finally,
the mean of the resulting posterior distribution is used as
the test function f . We set noise parameter R2 to be 1%
of function range and use λ = R2. We used Squared Ex-
ponential kernel with lengthscale parameter l = 0.2 and
Mat´ern kernel with parameters ν = 2.5, l = 0.2. Pa-
rameters βt, ˜βt, vt of IGP-UCB, GP-UCB and GP-TS are
chosen as given in Section 3, with δ = 0.1, B2 = f T Kf
and γt set according to theoretical upper bounds for corre-
sponding kernels. We run each algorithm for T = 30000
iterations, over 25 independent trials (samples from the
RKHS) and plot the average cumulative regret along with
standard deviations (Figure 1). We see a signiﬁcant im-
provement in the performance of IGP-UCB over GP-UCB.
In fact IGP-UCB performs the best in the pool of competi-
tors, while GP-TS also fares reasonably well compared to
GP-UCB and GP-EI/GP-PI.

We next sample 25 random functions from the GP (0, K)
and perform the same experiment (Figure 2) for both ker-
nels with exactly same set of parameters. The relative per-
formance of all methods is similar to that in the previous
experiment, which is the arguably harder “agnostic” setting
of a ﬁxed, unknown target function.

Standard Test Functions. We consider 2 well-known

00.511.522.533.5x 10400.511.522.533.54x 104RoundsCumulativeRegret  GP-PIGP-EIGP-TSGP-UCBIGP-UCB00.511.522.533.5x 10400.511.522.5x 104RoundsCumulativeRegret  GP-PIGP-EIGP-TSGP-UCBIGP-UCB00.511.522.533.5x 10400.511.522.53x 104RoundsCumulativeRegret  GP-PIGP-EIGP-TSGP-UCBIGP-UCB00.511.522.533.5x 10400.511.522.533.5x 104RoundsCumulativeRegret  GP-PIGP-EIGP-TSGP-UCBIGP-UCBOn Kernelized Multi-armed Bandits

(a)

(b)

(a)

(b)

Figure 3. Cumulative regret for (a) Rosenbrock and (b) Hartman3
benchmark function.

Figure 4. Cumulative regret plots for (a) temperature data and (b)
light sensor data.

synthetic benchmark functions for Bayesian Optimization:
Rosenbrock and Hartman3 (see Azimi et al. (2012) for ex-
act analytical expressions). We sample 100 d points uni-
formly from the domain of each benchmark function, d be-
ing the dimension of respective domain, as the decision set.
We consider the Squared Exponential kernel with l = 0.2
and set all parameters exactly as in previous experiment.
The cumulative regret for 25 independent trials on Rosen-
brock and Hartman3 benchmarks is shown in Figure 3. We
see GP-EI/PI perform better than the rest, while IGP-UCB
and GP-TS show competitive performance. Here no al-
gorithm is aware of the underlying kernel function, hence
we conjecture that the UCB- and TS- based algorithms are
somewhat less robust on the choice of kernel than EI/PI.

Temperature Sensor Data. We use temperature data12
collected from 54 sensors deployed in the Intel Berkeley
Research lab between February 28th and April 5th, 2004
with samples collected at 30 second intervals. We tested all
algorithms in the context of learning the maximum read-
ing of the sensors collected between 8 am to 9 am. We
take measurements of ﬁrst 5 consecutive days (starting Feb.
28th 2004) to learn algorithm parameters. Following Srini-
vas et al. (2009), we calculate the empirical covariance ma-
trix of the sensor measurements and use it as the kernel
matrix in the algorithms. Here R2 is set to be 5% of the
average empirical variance of sensor readings and other al-
gorithm parameters is set similarly as in the previous exper-
iment with γt = 1 (found via cross-validation). The func-
tions for testing consist of one set of measurements from all
sensors in the two following days and the cumulative regret
is plotted over all such test functions. From Figure 4, we
see that IGP-UCB and GP-UCB performs the same, while
GP-TS outperforms all its competitors.

Light Sensor Data. We take light sensor data collected
in the CMU Intelligent Workplace in Nov 2005, which is
available online as Matlab structure13 and contains loca-
tions of 41 sensors, 601 train samples and 192 test samples.

12http://db.csail.mit.edu/labdata/labdata.

html

13http://www.cs.cmu.edu/˜guestrin/Class/

10708-F08/projects/lightsensor.zip

We compute the kernel matrix, estimate the noise and set
other algorithm parameters exactly as in the previous ex-
periment. Here also GP-TS is found to perform better than
the others, with IGP-UCB performing better than GP-EI/PI
(Figure 4).

Related work. An alternative line of work pertaining to
X -armed bandits (Kleinberg et al., 2008; Bubeck et al.,
2011; Carpentier & Valko, 2015; Azar et al., 2014) stud-
ies continuum-armed bandits with smoothness structure.
For instance, Bubeck et al. (2011) show that with a Lip-
schitzness assumption on the reward function, algorithms
based on discretizing the domain yield nontrivial regret
d+1
d+2 ) in Rd. Other Bayesian
guarantees, of order Ω(T
approaches to function optimization are GP-EI (Moˇckus,
1975), GP-PI (Kushner, 1964), GP-EST (Wang et al.,
2016) and GP-UCB, including the contextual (Krause &
Ong, 2011), high-dimensional (Djolonga et al., 2013; Wang
et al., 2013), time-varying (Bogunovic et al., 2016) safety-
aware (Gotovos et al., 2015), budget-constraint (Hoffman
et al., 2013) and noise-free (De Freitas et al., 2012) set-
tings. Other relevant work focuses on best arm identiﬁca-
tion problem in the Bayesian setup considering pure explo-
ration (Gr¨unew¨alder et al., 2010). For Thompson sampling
(TS), Russo & Van Roy (2014) analyze the Bayesian regret
of TS, which includes the case where the target function is
sampled from a GP prior. Our work obtains the ﬁrst fre-
quentist regret of TS for unknown, ﬁxed functions from an
RKHS.

7. Conclusion

For bandit optimization, we have improved upon the exist-
ing GP-UCB algorithm, and introduced a new GP-TS al-
gorithm. The proposed algorithms perform well in practice
both on synthetic and real-world data. An interesting case
is when the kernel function is also not known to the algo-
rithms a priori and needs to be learnt adaptively. Moreover,
one can consider classes of time varying functions from the
RKHS, and general reinforcement learning with GP tech-
niques. There are also important questions on computa-
tional aspects of optimizing functions drawn from GPs.

00.511.522.533.5x 1040100020003000400050006000700080009000RoundsCumulativeRegret  GP-PIGP-EIGP-TSGP-UCBIGP-UCB00.511.522.533.5x 104010002000300040005000600070008000RoundsCumulativeRegret  GP-PIGP-EIGP-TSGP-UCBIGP-UCB00.511.522.533.5x 104010002000300040005000600070008000RoundsCumulativeRegret  GP-PIGP-EIGP-TSGP-UCBIGP-UCB00.511.522.533.5x 104−0.500.511.522.5x 104RoundsCumulativeRegret  GP-PIGP-EIGP-TSGP-UCBIGP-UCBOn Kernelized Multi-armed Bandits

Acknowledgement

Aditya Gopalan was supported by the DST INSPIRE fac-
ulty grant IFA13-ENG-69. The authors are grateful to O.-
A. Maillard for helpful discussions, and to anonymous re-
viewers for providing useful comments.

References

Abbasi-Yadkori, Yasin, P´al, D´avid, and Szepesv´ari, Csaba.
Improved algorithms for linear stochastic bandits. In Ad-
vances in Neural Information Processing Systems, pp.
2312–2320, 2011.

Agrawal, Shipra and Goyal, Navin. Analysis of thompson
sampling for the multi-armed bandit problem. In COLT,
pp. 39–1, 2012.

Dani, Varsha, Hayes, Thomas P, and Kakade, Sham M.
Stochastic linear optimization under bandit feedback. In
COLT, pp. 355–366, 2008.

De Freitas, Nando, Smola, Alex, and Zoghi, Masrour.
Exponential regret bounds for gaussian process ban-
arXiv preprint
dits with deterministic observations.
arXiv:1206.6457, 2012.

de la Pena, Victor H, Lai, Tze Leung, and Shao, Qi-Man.
Self-normalized processes. probability and its applica-
tions, 2009.

Djolonga, Josip, Krause, Andreas, and Cevher, Volkan.
High-dimensional gaussian process bandits. In Advances
in Neural Information Processing Systems, pp. 1025–
1033, 2013.

Agrawal, Shipra and Goyal, Navin. Thompson sampling
for contextual bandits with linear payoffs. In ICML, pp.
127–135, 2013.

Gopalan, Aditya, Mannor, Shie, and Mansour, Yishay.
Thompson sampling for complex online problems.
In
ICML, volume 14, pp. 100–108, 2014.

Azar, Mohammad Gheshlaghi, Lazaric, Alessandro, and
Brunskill, Emma. Online stochastic optimization under
In ICML, pp. 1557–1565,
correlated bandit feedback.
2014.

Azimi, Javad, Jalali, Ali, and Fern, Xiaoli. Hybrid batch
bayesian optimization. arXiv preprint arXiv:1202.5597,
2012.

Besbes, Omar and Zeevi, Assaf. Dynamic pricing without
knowing the demand function: Risk bounds and near-
optimal algorithms. Operations Research, 57(6):1407–
1420, 2009.

Bogunovic, Ilija, Scarlett, Jonathan, and Cevher, Volkan.
Time-varying gaussian process bandit optimization.
arXiv preprint arXiv:1601.06650, 2016.

Bubeck, S´ebastien, Munos, R´emi, Stoltz, Gilles, and
Szepesv´ari, Csaba. X-armed bandits. Journal of Ma-
chine Learning Research, 12(May):1655–1695, 2011.

Carpentier, Alexandra and Valko, Michal. Simple regret for
inﬁnitely many armed bandits. In ICML, pp. 1133–1141,
2015.

Gotovos, Alkis, CH, ETHZ, and Burdick, Joel W. Safe
exploration for optimization with gaussian processes.
2015.

Gr¨unew¨alder, Steffen, Audibert, Jean-Yves, Opper, Man-
fred, and Shawe-Taylor, John. Regret bounds for gaus-
sian process bandit problems. In AISTATS, pp. 273–280,
2010.

Hoffman, Matthew D, Brochu, Eric, and de Freitas, Nando.
Portfolio allocation for bayesian optimization. In UAI,
pp. 327–336, 2011.

Hoffman, Matthew W, Shahriari, Bobak, and de Freitas,
Nando. Exploiting correlation and budget constraints
arXiv
in bayesian multi-armed bandit optimization.
preprint arXiv:1303.6746, 2013.

Kaelbling, Leslie Pack, Littman, Michael L, and Moore,
Andrew W. Reinforcement learning: A survey. Journal
of artiﬁcial intelligence research, 4:237–285, 1996.

Kaufmann, Emilie, Korda, Nathaniel, and Munos, R´emi.
Thompson sampling: An asymptotically optimal ﬁnite-
time analysis. In International Conference on Algorith-
mic Learning Theory, pp. 199–213. Springer, 2012.

Chiang, Mung, Hande, Prashanth, Lan, Tian, and Tan,
Chee Wei. Power control in wireless cellular networks.
Foundations and Trends in Networking, 2(4):381–533,
2008. ISSN 1554-057X. doi: 10.1561/1300000009.

Kleinberg, Robert, Slivkins, Aleksandrs, and Upfal, Eli.
Multi-armed bandits in metric spaces. In Proceedings of
the fortieth annual ACM symposium on Theory of com-
puting, pp. 681–690. ACM, 2008.

Chowdhury, Sayak Ray and Gopalan, Aditya.

kernelized multi-armed bandits.
arXiv:1704.00445, 2017.

On
arXiv preprint

Krause, Andreas and Ong, Cheng S. Contextual gaussian
process bandit optimization. In Advances in Neural In-
formation Processing Systems, pp. 2447–2455, 2011.

On Kernelized Multi-armed Bandits

Kushner, Harold J. A new method of locating the maxi-
mum point of an arbitrary multipeak curve in the pres-
ence of noise. Journal of Basic Engineering, 86(1):97–
106, 1964.

Maillard, Odalric-Ambrym. Self-normalization techniques

for streaming conﬁdent regression. 2016.

Moˇckus, J. On bayesian methods for seeking the ex-
In Optimization Techniques IFIP Technical

tremum.
Conference, pp. 400–404. Springer, 1975.

Rasmussen, Carl Edward and Williams, Christopher KI.

Gaussian processes for machine learning. 2006.

Rusmevichientong, Paat and Tsitsiklis, John N. Linearly
parameterized bandits. Math. Oper. Res., 35(2):395–
411, May 2010.

Russo, Daniel and Van Roy, Benjamin. Learning to opti-
mize via posterior sampling. Mathematics of Operations
Research, 39(4):1221–1243, 2014.

Sch¨olkopf, Bernhard and Smola, Alexander J. Learning
with kernels: support vector machines, regularization,
optimization, and beyond. MIT press, 2002.

Smart, William D and Kaelbling, Leslie Pack. Practical
reinforcement learning in continuous spaces. In ICML,
pp. 903–910, 2000.

Srinivas, Niranjan, Krause, Andreas, Kakade, Sham M, and
Seeger, Matthias. Gaussian process optimization in the
bandit setting: No regret and experimental design. arXiv
preprint arXiv:0912.3995, 2009.

Valko, Michal, Korda, Nathaniel, Munos, R´emi, Flaounas,
Finite-time analysis
arXiv preprint

Ilias, and Cristianini, Nelo.
of kernelised contextual bandits.
arXiv:1309.6869, 2013.

Wang, Zi, Zhou, Bolei, and Jegelka, Stefanie. Optimization
as estimation with gaussian processes in bandit settings.
In International Conf. on Artiﬁcial and Statistics (AIS-
TATS), 2016.

Wang, Ziyu, Zoghi, Masrour, Hutter, Frank, Mathe-
son, David, Freitas, N, et al. Bayesian optimization
in high dimensions via random embeddings. AAAI
Press/International Joint Conferences on Artiﬁcial Intel-
ligence, 2013.

