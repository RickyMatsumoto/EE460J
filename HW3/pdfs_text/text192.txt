Neural Audio Synthesis of Musical Notes
with WaveNet Autoencoders

Jesse Engel * 1 Cinjon Resnick * 1 Adam Roberts 1 Sander Dieleman 2 Mohammad Norouzi 1 Douglas Eck 1
Karen Simonyan 2

Abstract

Generative models in vision have seen rapid
progress due to algorithmic improvements and
the availability of high-quality image datasets. In
this paper, we offer contributions in both these ar-
eas to enable similar progress in audio modeling.
First, we detail a powerful new WaveNet-style
autoencoder model that conditions an autoregres-
sive decoder on temporal codes learned from
the raw audio waveform. Second, we introduce
NSynth, a large-scale and high-quality dataset of
musical notes that is an order of magnitude larger
than comparable public datasets. Using NSynth,
we demonstrate improved qualitative and quanti-
tative performance of the WaveNet autoencoder
over a well-tuned spectral autoencoder baseline.
Finally, we show that the model learns a mani-
fold of embeddings that allows for morphing be-
tween instruments, meaningfully interpolating in
timbre to create new types of sounds that are re-
alistic and expressive.

1. Introduction

Audio synthesis is important for a large range of appli-
cations including text-to-speech (TTS) systems and mu-
sic generation. Audio generation algorithms, know as
vocoders in TTS and synthesizers in music, respond to
higher-level control signals to create ﬁne-grained audio
waveforms. Synthesizers have a long history of being
hand-designed instruments, accepting control signals such
as ‘pitch’, ‘velocity’, and ﬁlter parameters to shape the
tone, timbre, and dynamics of a sound (Pinch et al., 2009).
In spite of their limitations, or perhaps because of them,
synthesizers have had a profound effect on the course of
music and culture in the past half century (Punk, 2014).

*Equal contribution 1Google Brain 2DeepMind. Correspon-

dence to: Jesse Engel <jesseengel@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

In this paper, we outline a data-driven approach to audio
synthesis. Rather than specifying a speciﬁc arrangement
of oscillators or an algorithm for sample playback, such as
in FM Synthesis or Granular Synthesis (Chowning, 1973;
Xenakis, 1971), we show that it is possible to generate new
types of expressive and realistic instrument sounds with a
neural network model. Further, we show that this model
can learn a semantically meaningful hidden representation
that can be used as a high-level control signal for manipu-
lating tone, timbre, and dynamics during playback.

Explicitly, our two contributions to advance the state of
generative audio modeling are:

• A WaveNet-style autoencoder that learns temporal
hidden codes to effectively capture longer term struc-
ture without external conditioning.

• NSynth: a large-scale dataset for exploring neural au-

dio synthesis of musical notes.

The primary motivation for our novel autoencoder structure
follows from the recent advances in autoregressive models
like WaveNet (van den Oord et al., 2016a) and SampleRNN
(Mehri et al., 2016). They have proven to be effective at
modeling short and medium scale (∼500ms) signals, but
rely on external conditioning for longer-term dependencies.
Our autoencoder removes the need for that external condi-
tioning. It consists of a WaveNet-like encoder that infers
hidden embeddings distributed in time and a WaveNet de-
coder that uses those embeddings to effectively reconstruct
the original audio. This structure allows the size of an em-
bedding to scale with the size of the input and encode over
much longer time scales.

Recent breakthroughs in generative modeling of images
(Kingma & Welling, 2013; Goodfellow et al., 2014;
van den Oord et al., 2016b) have been predicated on the
availability of high-quality and large-scale datasets such as
MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011),
CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Deng
et al., 2009). While generative models are notoriously hard
to evaluate (Theis et al., 2015), these datasets provide a
common test bed for consistent qualitative and quantitative

Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders

Figure 1. Models considered in this paper. For both models, we optionally condition on pitch by concatenating the hidden embedding
with a one-hot pitch representation. 1a. Baseline spectral autoencoder: Each block represents a nonlinear 2-D convolution with stride
(s), kernel size (k), and channels (#). 1b. The WaveNet autoencoder: Downsampling in the encoder occurs only in the average pooling
layer. The embeddings are distributed in time and upsampled with nearest neighbor interpolation to the original resolution before biasing
each layer of the decoder. ‘NC’ indicates non-causal convolution. ‘1x1’ indicates a 1-D convolution with kernel size 1. See Section 2.1
for further details.

evaluation, such as with the use of the Inception score (Sal-
imans et al., 2016).

2. Models

2.1. WaveNet Autoencoder

We recognized the need for an audio dataset that was as ap-
proachable as those in the image domain. Audio signals
found in the wild contain multi-scale dependencies that
prove particularly difﬁcult to model (Raffel, 2016; Bertin-
Mahieux et al., 2011; King et al., 2008; Thickstun et al.,
2016), leading many previous efforts at data-driven audio
synthesis to focus on more constrained domains such as
texture synthesis or training small parametric models (Sar-
roff & Casey, 2014; McDermott et al., 2009).

Inspired by the large, high-quality image datasets, NSynth
is an order of magnitude larger than comparable public
It consists of ∼300k four-
datasets (Humphrey, 2016).
second annotated notes sampled at 16kHz from ∼1k har-
monic musical instruments.

After introducing the models and describing the dataset,
we evaluate the performance of the WaveNet autoencoder
over a baseline convolutional autoencoder model trained
on spectrograms. We examine the tasks of reconstruction
and interpolation, and analyze the learned space of embed-
dings. For qualitative evaluation, we include supplemental
audio ﬁles for all examples mentioned in this paper. De-
spite our best efforts to convey analysis in plots, listening
to the samples is essential to understanding this paper and
we strongly encourage the reader to listen along as they
read.

WaveNet (van den Oord et al., 2016a) is a powerful gener-
ative approach to probabilistic modeling of raw audio. In
this section we describe our novel WaveNet autoencoder
structure. The primary motivation for this approach is to
attain consistent long-term structure without external con-
ditioning. A secondary motivation is to use the learned en-
codings for applications such as meaningful audio interpo-
lation.

Recalling the original WaveNet architecture described in
(van den Oord et al., 2016a), at each step a stack of di-
lated convolutions predicts the next sample of audio from a
ﬁxed-size input of prior sample values. The joint probabil-
ity of the audio x is factorized as a product of conditional
probabilities:

p(x) =

p(xi|x1, ..., xN −1)

N
(cid:89)

i=1

Unconditional generation from this model manifests as
“babbling” due to the lack of longer term structure (see
Supplemental for an audio example). However, (van den
Oord et al., 2016a) showed in the context of speech that
long-range structure can be enforced by conditioning on
temporally aligned linguistic features.

Our autoencoder removes the need for that external con-

Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders

ditioning. It works by taking raw audio waveform as input
from which the encoder produces an embedding Z = f (x).
Next, we causally shift the same input and feed it into the
decoder, which reproduces the input waveform. The joint
probablity is now:

p(x) =

p(xi|x1, ..., xN −1, f (x))

N
(cid:89)

i=1

We could parameterize Z as a latent variable p(Z|x) that
we would have to marginalize over (Gulrajani et al., 2016),
but in practice we have found this to be less effective. As
discussed in (Chen et al., 2016), this may be due to the
decoder being so powerful that it can ignore the latent vari-
ables unless they encode a much larger context that’s oth-
erwise inaccessible.

Note that the decoder could completely ignore the deter-
ministic encoding and degenerate to a standard uncon-
ditioned WaveNet. However, because the encoding is a
strong signal for the supervised output, the model learns
to utilize it.

During inference, the decoder autoregressively generates a
single output sample at a time conditioned on an embed-
ding and a starting palette of zeros. The embedding can be
inferred deterministically from audio or drawn from other
points in the embedding space, e.g. through interpolation
or analogy (White, 2016).

Figure 1b depicts the model architecture in more detail.
The temporal encoder model is a 30-layer nonlinear resid-
ual network of dilated convolutions followed by 1x1 con-
volutions. Each convolution has 128 channels and precedes
a ReLU nonlinearity. The output feed into another 1x1
convolution before downsampling with average pooling to
get the encoding Z. We call it a ‘temporal encoding’ be-
cause the result is a sequence of hidden codes with sepa-
rate dimensions for time and channel. The time resolution
depends on the stride of the pooling. We tune the stride,
keeping total size of the embedding constant (∼32x com-
pression). In the trade-off between temporal resolution and
embedding expressivity, we ﬁnd a sweet spot at a stride
of 512 (32ms) with 16 dimensions per timestep, yielding
a 125x16 embedding for each NSynth note. We addition-
ally explore models that condition on global attributes by
utilizing a one-hot pitch embedding.

The WaveNet decoder model is similar to that presented in
(van den Oord et al., 2016a). We condition it by biasing ev-
ery layer with a different linear projection of the temporal
embeddings. Since the decoder does not downsample any-
where in the network, we upsample the temporal encodings

to the original audio rate with nearest neighbor interpola-
tion. As in the original design, we quantize our input audio
using 8-bit mu-law encoding and predict each output step
with a softmax over the resulting 256 values.

This WaveNet autoencoder is a deep and expressive net-
work, but has the trade-off of being limited in temporal
context to the chunk-size of the training audio. While
this is sufﬁcient for consistently encoding the identity of
a sound and interpolating among many sounds, achieving
larger context would be better and is an area of ongoing
research.

2.2. Baseline: Spectral Autoencoder

As a point of comparison, we set out to create a straight-
forward yet strong baseline for the our neural audio syn-
thesis experiments.
Inspired by image models (Vincent
et al., 2010), we explore convolutional autoencoder struc-
tures with a bottleneck that forces the model to ﬁnd a com-
pressed representation for an entire note. Figure 1a shows
a block diagram of our baseline architecture. The convo-
lutional encoder and decoder are each 10 layers deep with
2x2 strides and 4x4 kernels. Every layer is followed by
a leaky-ReLU (0.1) nonlinearity and batch normalization
(Ioffe & Szegedy, 2015). The number of channels grows
from 128 to 1024 before a linear fully-connected layer cre-
ates a single 19841 dimensional hidden vector (Z) to match
that of the WaveNet autoencoder.

Given the simplicity of the architecture, we examined a
range of input representations. Using the raw waveform as
input with a mean-squared error (MSE) cost proved difﬁ-
cult to train and highlighted the inadequacy of the indepen-
dent Gaussian assumption. Spectral representations such
as the real and imaginary components of the Fast Fourier
Transform (FFT) fared better, but suffered from low per-
ceptual quality despite achieving low MSE cost. We found
that training on the log magnitude of the power spectra,
peak normalized to be between 0 and 1, correlated better
with perceptual distortion.

We also explored several representations of phase, includ-
ing instantaneous frequency and circular normal cost func-
tions (see Supplemental), but in each case independently
estimating phase and magnitude led to poor sample quality
due to phase errors. We ﬁnd a large improvement by es-
timating only the magnitude and using a well established
iterative technique to reconstruct the phase (Grifﬁn & Lim,
1984). To get the best results, we used a large FFT size
(1024) relative to the hop size (256) and ran the algorithm
for 1000 iterations. As a ﬁnal heuristic, we weighted the
MSE loss, starting at 10 for 0Hz and decreasing linearly to

1This size was aligned with a WaveNet autoencoder that had

a pooling stride of 1024 and a 62x32 embedding.

Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders

1 at 4000Hz and above. At the expense of some precision
in timbre, this created more phase coherence for the funda-
mentals of notes, where errors in the linear spectrum lead
to a larger relative error in frequency.

2.3. Training

We train all models with stochastic gradient descent with
an Adam optimizer (Kingma & Ba, 2014). The baseline
models commonly use a learning rate of 1e-4, while the
WaveNet models use a schedule, starting at 2e-4 and de-
scending to 6e-5, 2e-5, and 6e-6 at iterations 120k, 180k,
and 240k respectively. The baseline models train asyn-
chronously for 1800k iterations with a batch size of 8. The
WaveNet models train synchronously for 250k iterations
with a batch size of 32.

3. The NSynth Dataset

To evaluate our WaveNet autoencoder model, we wanted
an audio dataset that let us explore the learned embed-
dings. Musical notes are an ideal setting for this study as
we hypothesize that the embeddings will capture structure
such as pitch, dynamics, and timbre. While several smaller
datasets currently exist (Goto et al., 2003; Romani Picas
et al., 2015), deep networks train better on abundant, high-
quality data, motivating the development of a new dataset.

3.1. A Dataset of Musical Notes

NSynth consists of 306 043 musical notes, each with a
unique pitch, timbre, and envelope. For 1006 instruments
from commercial sample libraries, we generated four sec-
ond, monophonic 16kHz audio snippets, referred to as
notes, by ranging over every pitch of a standard MIDI pi-
ano (21-108) as well as ﬁve different velocities2 (25, 50, 75,
100, 127). The note was held for the ﬁrst three seconds and
allowed to decay for the ﬁnal second. Some instruments
are not capable of producing all 88 pitches in this range,
resulting in an average of 65.4 pitches per instrument. Fur-
thermore, the commercial sample packs occasionally con-
tain duplicate sounds across multiple velocities, leaving an
average of 4.75 unique velocities per pitch.

3.2. Annotations

We also annotated each of the notes with three additional
pieces of information based on a combination of human
evaluation and heuristic algorithms:

• Source: The method of sound production for the
note’s instrument. This can be one of ‘acoustic’ or

2MIDI velocity is similar to volume control and they have a
direct relationship. For physical intuition, higher velocity corre-
sponds to pressing a piano key harder.

‘electronic’ for instruments that were recorded from
acoustic or electronic instruments, respectively, or
‘synthetic’ for synthesized instruments.

• Family: The high-level family of which the note’s in-
strument is a member. Each instrument is a member
of exactly one family. See Supplemental for the com-
plete list.

• Qualities: Sonic qualities of the note. See Supple-
mental for the complete list of classes and their co-
occurrences. Each note is annotated with zero or more
qualities.

3.3. Availability

in
is

dataset will

a
available

serialized
for

full NSynth
available

be made
format
download

The
licly
publication.
http://download.magenta.tensorflow.org/hans
as TFRecord ﬁles split into training and holdout sets. Each
note is represented by a serialized TensorFlow Example
protocol buffer containing the note and annotations.
Details of the format can be found in the README.

pub-
after
at

4. Evaluation

We evaluate and analyze our models on the tasks of note
reconstruction, instrument interpolation, and pitch interpo-
lation.

Audio is notoriously hard to represent visually. Magnitude
spectrograms capture many aspects of a signal for analyt-
ics, but two spectrograms that appear very similar to the
eye can correspond to audio that sound drastically different
due to phase differences. We have included supplemental
audio examples of every plot and encourage the reader to
listen along as they read.

in our analysis we present examples as
That said,
plots of the constant-q transform (CQT) (Brown, 1991;
Sch¨orkhuber & Klapuri, 2010), which is useful because it is
shift invariant to changes in the fundamental frequency. In
this way, the structure and envelope of the overtone series
(higher harmonics) determines the dynamics and timbre of
a note, regardless of its base frequency. However, due to
the logarithmic binning of frequencies, transient noise-like
impulses appear as rainbow “pyramidal spikes” rather than
straight broadband lines. We display CQTs with a pitch
range of 24-96 (C2-C8), hop size of 256, 40 bins per oc-
tave, and a ﬁlter scale of 0.8.

As phase plays such an essential part in sample quality,
we have attempted to show both magnitude and phase on
the same plot. The intensity of lines is proportional to
the log magnitude of the power spectrum while the color

Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders

Figure 2. Reconstructions of notes from three different instruments. Each note is displayed as a CQT spectrum with time on the hori-
zontal axis and pitch on the vertical axis. Intensity of lines is proportional to the log magnitude of the power spectrum and the color is
given by the instantaneous frequency. See Section 4.1 for details.

is given by the derivative of the unrolled phase (‘instan-
taneous frequency’) (Boashash, 1992). We display the
derivative of the phase because it creates a solid continu-
ous line for a harmonic of a consistent frequency. We can
understand this because if the instantaneous frequency of
a harmonic (fharm) and an FFT bin (fbin) are not exactly
equal, each timestep will introduce a constant phase shift,
∆φ = (fbin − fharm) hopsize
samplerate .

4.1. Reconstruction

Figure 2 displays CQT spectrograms for notes from 3 dif-
ferent instruments in the holdout set, where the original
note spectrograms are on the ﬁrst column and the model
reconstruction spectrograms are on the second and third
columns. Each note has a similar structure with some noise
on onset, a fundamental frequency with a series of harmon-
ics, and a decay. For all the WaveNet models, there is a
slight built-in distortion due to the compression of the mu-
law encoding. It is a minor effect for many samples, but
is more pronounced for lower frequencies. Using different
representations without this distortion is an ongoing area of
research.

While each spectrogram matches the general contour of the
original note, we can hear a pronounced difference in sam-
ple quality that we can ascribe to certain features. For the
Glockenspiel, we can see that the WaveNet autoencoder
reproduces the magnitude and phase of the fundamental
(solid blue stripe, (A)), and also the noise on attack (ver-
tical rainbow spike (B)). There is a slight error in the fun-

damental as it starts a little high and quickly descends to
the correct pitch (C). In contrast, the baseline has a more
percussive, multitonal sound, similar to a bell or gong. The
fundamental is still present, but so are other frequencies,
and the phases estimated from the Grifﬁn-Lim procedure
are noisy as indicated by the blurred horizontal rainbow
texture (D).

The electric piano has a more clearly deﬁned harmonic se-
ries (the horizontal rainbow solid lines, (E)) and a noise on
the beginning and end of the note (vertical rainbow spikes,
(F)). Listening to the sound, we hear that it is slightly dis-
torted, which promotes these upper harmonics. Both the
WaveNet autoencoder and the baseline produce spectro-
grams with similar shapes to the original, but with different
types of phase artifacts. The WaveNet model has sufﬁcient
phase structure to model the distortion, but has a slight wa-
vering of the instantaneous frequency of some harmonics,
as seen in the color change in harmonic stripes (G). In con-
trast, the baseline lacks the structure in phase to maintain
the punchy character of the original note, and produces a
duller sound that is slightly out of tune. This is represented
in the less brightly colored harmonics due to phase noise
(H).

The ﬂugelhorn displays perhaps the starkest difference be-
tween the two models. The sound combines rich harmon-
ics (many lines), non-tonal wind and lip noise (background
color), and vibrato - oscillation of pitch that results in a
corresponding rainbow of color in all of the harmonics.
While the WaveNet autoencoder does not replicate the ex-

Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders

Figure 3. CQT spectrograms of linear interpolations between three different notes from instruments in the holdout set. For the original
spectrograms, the raw audio is linearly mixed. For the models, samples are generated from linear interpolations in embedding space.
See Section 4.2 for details.

act trace of the vibrato (I), it creates a very similar spec-
trogram with oscillations in the instantaneous frequency at
all levels synced across the harmonics (J). This results in
a rich and natural sounding reconstruction with all three
aspects of the original sound. The baseline, by compari-
son, is unable to model such structure. It creates a more or
less correct harmonic series, but the phase has lots of ran-
dom perturbations. Visually this shows up as colors which
are faded and speckled with rainbow noise (K), which con-
trasts with the bright colors of the original and WaveNet
examples. Acoustically, this manifests as an unappealing
buzzing sound laid over an inexpressive and consistent se-
ries of harmonics. The WaveNet model also produces a few
inaudible discontinuities visually evidenced by the vertical
rainbow spikes (L).

4.1.1. QUANTITATIVE COMPARISON

Inspired by the use of the Inception Score for images (Sal-
imans et al., 2016), we train a multi-task classiﬁcation net-
work to perform a quantitative comparison of the model re-
constructions by predicting pitch and quality labels on the
NSynth dataset (details in the Supplemental). The network
conﬁguration is the same as the baseline encoder and test-
ing is done on reconstructions of a randomly chosen subset
of 4096 examples from the held-out set.

The results in Table 1 conﬁrm our qualititive observation
that the WaveNet reconstructions are of superior quality.
The classiﬁer is ∼70% more successful at extracting pitch
from the reconstructed WaveNet samples than the baseline

Table 1. Classiﬁcation accuracy of a deep nonlinear pitch and
quality classiﬁer on reconstructions of a test set.

PITCH

QUALITY

ORIGINAL AUDIO
WAVENET RECON
BASELINE RECON

91.6%
79.6%
46.9%

90.1%
88.9%
85.2%

and several points higher for predicting quality informa-
tion, giving an accuracy roughly equal to the original au-
dio.

4.2. Interpolation in Timbre and Dynamics

Given the limited factors of variation in the dataset, we
know that a successful embedding space (Z) should span
the range of timbre and dynamics in its reconstructions.
In Figure 3, we show reconstructions from linear interpo-
lations (0.5:0.5) in the Z space among three different in-
struments and additionally compare these to interpolations
in the original audio space. The latter are simple super-
positions of the individual instruments’ spectrograms. This
is perceptually equivalent to the two instruments being
played at the same time.

In contrast, we ﬁnd that the generative models fuse aspects
of the instruments. As we saw in Section 4.1, the WaveNet
autoencoder models the data much more realistically than
the baseline, so it is no surprise that it also learns a man-
ifold of codes that yield more perceptually interesting re-

Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders

constructions.

For example, in the interpolated note between the bass and
ﬂute (Figure 3, column 2), we can hear and see that both the
baseline and WaveNet models blend the harmonic struc-
ture of the two instruments while imposing the amplitude
envelope of the bass note onto the upper harmonics of the
ﬂute note. However, the WaveNet model goes beyond this
to create a dynamic mixing of the overtones in time, even
jumping to a higher harmonic at the end of the note (A).
This sound captures expressive aspects of the timbre and
dynamics of both the bass and ﬂute, but is distinctly sep-
arate from either original note. This contrasts with the in-
terpolation in audio space, where the dynamics and timbre
of the two notes is independent. The baseline model also
introduces phase distortion similar to those in the recon-
structions of the bass and ﬂute.

We see this phenomenon again in the interpolation between
ﬂute and organ (Figure 3, column 4). Both models also
seem to create new harmonic structure, rather than just
overlay the original harmonics. The WaveNet model adds
additional harmonics as well as a sub-harmonic to the orig-
inal ﬂute note, all while preserving phase relationships (B).
The resulting sound has the breathiness of a ﬂute, with the
upper frequency modulation of an organ. By contrast, the
lack of phase structure in the baseline leads to a new har-
monic yet dull sound lacking a unique character.

The WaveNet model additionally has a tendency to exag-
gerate amplitude modulation behavior, while the baseline
suppresses it. If we examine the original organ sound (Fig-
ure 3, column 5), we can see a subtle modulation signi-
ﬁed by the blue harmonics periodically fading to black (C).
The baseline model misses this behavior completely as it is
washed out. Conversely, the WaveNet model ampliﬁes the
behavior, adding in new harmonics not present in the orig-
inal note and modulating all the harmonics. This is seen in
the ﬁgure by four vertical black stripes that align with the
four modulations of the original signal (D).

4.3. Entanglement of Pitch and Timbre

By conditioning on pitch during training, we hypothesize
that we should be able to generate multiple pitches from a
single Z vector that preserve the identity of timbre and dy-
namics. Our initial attempts were unsuccessful, as it seems
our models had learned to ignore the conditioning variable.
We investigate this further with classiﬁcation and correla-
tion studies.

4.3.1. PITCH CLASSIFICATION FROM Z

One way to study the entanglement of pitch and Z is to con-
sider the pitch classiﬁcation accuracy from embeddings. If
training with pitch conditioning disentangles the represen-

Table 2. Classiﬁcation accuracy (in percentage) of a linear pitch
classiﬁer trained on learned embeddings. The decoupling of pitch
and embedding becomes more pronounced at smaller embedding
sizes as shown by the larger relative decrease in classiﬁcation ac-
curacy.

Z
SIZE

1984
1984
1024
512
256
128
64

WAVENET
BASELINE
BASELINE
BASELINE
BASELINE
BASELINE
BASELINE

NO PITCH
COND.

PITCH
COND.

RELATIVE
CHANGE

58.1
63.8
57.4
63.2
57.7
58.2
59.8

40.5
55.2
42.1
21.8
21.0
21.2
15.2

-30.4
-13.5
-26.7
-65.5
-63.6
-63.6
-74.6

tation of pitch and timbre, then we would expect a linear
pitch classiﬁer trained on the embeddings to drop in accu-
racy. To test this, we train a series of baseline autoencoder
models with different embedding sizes, both with and with-
out pitch conditioning. For each model, we then train a lo-
gistic regression pitch classiﬁer on its embeddings and test
on a random sample of 4096 held-out embeddings.

The ﬁrst two rows of Table 2 demonstrate that the baseline
and WaveNet models decrease in classiﬁcation accuracy by
13-30% when adding pitch conditioning during training.
This is indicative a reduced presence of pitch information
in the latent code and thus a decoupling of pitch and timbre
information. Further, as the total embedding size decreases
below 512, the accuracy drop becomes much more pro-
nounced, reaching a 75% relative decrease. This is likely
due to the greater expressivity of larger embeddings, where
there is less to be gained from utilizing the pitch condi-
tioning. However, as the embedding size decreases, so too
does reconstruction quality. This is more pronounced for
the WaveNet models, which have farther to fall in terms of
sample quality.

As a proof of principle, we ﬁnd that for a baseline model
with an embedding size of 128, we are able to success-
fully balance reconstruction quality and response to condi-
tioning. Figure 4 demonstrates two octaves of a C major
chord created from a single embedding of an electric pi-
ano note, but conditioned on different pitches. The result-
ing harmonic structure of the original note is only partially
preserved across the range. As we shift the pitch upwards,
a sub-harmonic emerges (A) such that the pitch +12 note
is similar to the original except that the harmonics of the
octave are accentuated in amplitude. This aligns with our
pitch classiﬁcation results, where we ﬁnd that pitches are
most commonly confused with those one octave away (see
Supplemental). These errors can account for as much as
20% absolute classiﬁcation error.

Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders

Figure 4. Conditioning on pitch. CQT spectrograms of a baseline (128 hidden dimensions) reconstruction of a single note of electric
piano from the holdout set. By holding Z constant and conditioning on different pitches, we can play two octaves of a C major chord
from a single embedding. The original pitch (MIDI C60) is dashed in white for comparison. See Section 4.3.1 for details.

5. Conclusion and Future Directions

In this paper, we have introduced a WaveNet autoencoder
model that captures long term structure without the need for
external conditioning and demonstrated its effectiveness on
the new NSynth dataset for generative modeling of audio.

The WaveNet autoencoder that we describe is a powerful
representation for which there remain multiple avenues of
exploration.
It builds upon the ﬁne-grained local under-
standing of the original WaveNet work and provides access
to a useful hidden space. However, due to memory con-
straints, it is unable to fully capture global context. Over-
coming this limitation is an important open problem.

NSynth was inspired by image recognition datasets that
have been core to recent progress in deep learning. We en-
courage the broader community to use NSynth as a bench-
mark and entry point into audio machine learning. We
also view NSynth as a building block for future datasets
and envision a high-quality multi-note dataset for tasks like
generation and transcription that involve learning complex
language-like dependencies.

A huge thanks to Hans Bernhard with the dataset, Colin
Raffel for crucial conversations, and Sageev Oore for
thoughtful analysis.

References

Figure 5. Correlation of embeddings across pitch for three differ-
ent instruments and the average across all instruments. These em-
beddings were taken from a WaveNet model trained without pitch
conditioning.

Acknowledgments

4.3.2. Z CORRELATION ACROSS PITCH

We can gain further insight into the relationship between
timbre and pitch by examining the correlation of WaveNet
embeddings among pitches for a given instrument. Figure 5
shows correlations for several instruments across their en-
tire 88 note range at velocity 127. We see that each instru-
ment has a unique partitioning into two or more registers
over which notes of different pitches have similar embed-
dings. Even the average over all instruments shows a broad
distinction between high and low registers. On reﬂection,
this is unsurprising as the timbre and dynamics of an in-
strument can vary dramatically across its range.

Bertin-Mahieux, Thierry, Ellis, Daniel PW, Whitman,
Brian, and Lamere, Paul. The million song dataset. In
ISMIR, volume 2, pp. 10, 2011.

Boashash, Boualem. Estimating and interpreting the in-
stantaneous frequency of a signal. i. fundamentals. Pro-
ceedings of the IEEE, 80(4):520–538, 1992.

Brown, Judith C. Calculation of a constant q spectral trans-
form. The Journal of the Acoustical Society of America,
89(1):425–434, 1991.

Chen, Xi, Kingma, Diederik P., Salimans, Tim, Duan, Yan,

Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders

Dhariwal, Prafulla, Schulman, John, Sutskever, Ilya, and
Abbeel, Pieter. Variational lossy autoencoder. CoRR,
abs/1611.02731, 2016. URL http://arxiv.org/
abs/1611.02731.

Chowning, John M. The synthesis of complex audio spec-
tra by means of frequency modulation. Journal of the
audio engineering society, 21(7):526–534, 1973.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-
ImageNet: A Large-Scale Hierarchical Image

Fei, L.
Database. In CVPR09, 2009.

Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,
Bing, Warde-Farley, David, Ozair, Sherjil, Courville,
Aaron, and Bengio, Yoshua. Generative adversarial nets.
In Advances in neural information processing systems,
pp. 2672–2680, 2014.

McDermott, Josh H, Oxenham, Andrew J, and Simoncelli,
Eero P. Sound texture synthesis via ﬁlter statistics. In
Applications of Signal Processing to Audio and Acous-
tics, 2009. WASPAA’09. IEEE Workshop on, pp. 297–
300. IEEE, 2009.

Mehri, Soroush, Kumar, Kundan, Gulrajani, Ishaan, Ku-
mar, Rithesh, Jain, Shubham, Sotelo, Jose, Courville,
Aaron C., and Bengio, Yoshua. Samplernn: An uncondi-
tional end-to-end neural audio generation model. CoRR,
abs/1612.07837, 2016. URL http://arxiv.org/
abs/1612.07837.

Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco,
Alessandro, Wu, Bo, and Ng, Andrew Y. Reading dig-
its in natural images with unsupervised feature learning.
In NIPS workshop on deep learning and unsupervised
feature learning, volume 2011, pp. 5, 2011.

Goto, Masataka, Hashiguchi, Hiroki, Nishimura, Takuichi,
and Oka, Ryuichi. Rwc music database: Music genre
database and musical instrument sound database. 2003.

Pinch, Trevor J, Trocco, Frank, and Pinch, TJ. Analog
days: The invention and impact of the Moog synthesizer.
Harvard University Press, 2009.

Grifﬁn, Daniel and Lim, Jae. Signal estimation from mod-
IEEE Transactions
iﬁed short-time fourier transform.
on Acoustics, Speech, and Signal Processing, 32(2):236–
243, 1984.

Gulrajani, Ishaan, Kumar, Kundan, Ahmed, Faruk, Taiga,
Adrien Ali, Visin, Francesco, V´azquez, David, and
Courville, Aaron C. Pixelvae: A latent variable model
for natural images. CoRR, abs/1611.05013, 2016. URL
http://arxiv.org/abs/1611.05013.

Humphrey, Eric J. Minst, a collection of musical
sound datasets, 2016. URL https://github.com/
ejhumphrey/minst-dataset/.

Ioffe, Sergey and Szegedy, Christian. Batch normalization:
Accelerating deep network training by reducing inter-
nal covariate shift. CoRR, abs/1502.03167, 2015. URL
http://arxiv.org/abs/1502.03167.

King, Simon, Clark, Robert AJ, Mayo, Catherine, and
Karaiskos, Vasilis. The blizzard challenge 2008. 2008.

Kingma, Diederik P. and Ba, Jimmy. Adam: A method
for stochastic optimization. CoRR, abs/1412.6980, 2014.
URL http://arxiv.org/abs/1412.6980.

Kingma, Diederik P and Welling, Max. Auto-encoding
arXiv preprint arXiv:1312.6114,

variational bayes.
2013.

Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple

layers of features from tiny images. 2009.

LeCun, Yann, Cortes, Corinna, and Burges, Christo-
pher JC. The mnist database of handwritten digits, 1998.

Punk, Daft. Giorgio by morodor, 2014. URL https://
www.youtube.com/watch?v=zhl-Cs1-sG4.

Raffel, Colin. Learning-Based Methods for Comparing Se-
quences, with Applications to Audio-to-MIDI Alignment
and Matching. PhD thesis, COLUMBIA UNIVERSITY,
2016.

Romani Picas, Oriol, Parra Rodriguez, Hector, Dabiri,
Dara, Tokuda, Hiroshi, Hariya, Wataru, Oishi, Koji, and
Serra, Xavier. A real-time system for measuring sound
In Audio Engineer-
goodness in instrumental sounds.
ing Society Convention 138. Audio Engineering Society,
2015.

Salimans, Tim, Goodfellow, Ian J., Zaremba, Wojciech,
Cheung, Vicki, Radford, Alec, and Chen, Xi. Improved
techniques for training gans. CoRR, abs/1606.03498,
URL http://arxiv.org/abs/1606.
2016.
03498.

Sarroff, Andy M and Casey, Michael A. Musical audio syn-
thesis using autoencoding neural nets. In ICMC, 2014.

Sch¨orkhuber, Christian and Klapuri, Anssi. Constant-q
In 7th Sound
transform toolbox for music processing.
and Music Computing Conference, Barcelona, Spain,
pp. 3–64, 2010.

Theis, Lucas, Oord, A¨aron van den, and Bethge, Matthias.
A note on the evaluation of generative models. arXiv
preprint arXiv:1511.01844, 2015.

Thickstun, John, Harchaoui, Zaid, and Kakade, Sham.
In preprint,
Learning features of music from scratch.
https://arxiv.org/abs/1611.09827, 2016.

Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders

van den Oord, A¨aron, Dieleman, Sander, Zen, Heiga, Si-
monyan, Karen, Vinyals, Oriol, Graves, Alex, Kalch-
brenner, Nal, Senior, Andrew W., and Kavukcuoglu, Ko-
ray. Wavenet: A generative model for raw audio. CoRR,
abs/1609.03499, 2016a. URL http://arxiv.org/
abs/1609.03499.

van

den Oord, A¨aron, Kalchbrenner, Nal,

Pixel
Kavukcuoglu, Koray.
CoRR, abs/1601.06759, 2016b.
works.
http://arxiv.org/abs/1601.06759.

and
recurrent neural net-
URL

Vincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Ben-
gio, Yoshua, and Manzagol, Pierre-Antoine. Stacked
denoising autoencoders: Learning useful representations
in a deep network with a local denoising criterion. Jour-
nal of Machine Learning Research, 11(Dec):3371–3408,
2010.

White, Tom. Sampling generative networks: Notes on a
few effective techniques. CoRR, abs/1609.04468, 2016.
URL http://arxiv.org/abs/1609.04468.

Xenakis, Iannis. Formalized music. 1971.

