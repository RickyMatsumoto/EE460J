A Richer Theory of Convex Constrained Optimization
with Reduced Projections and Improved Rates

Tianbao Yang 1 Qihang Lin 1 Lijun Zhang 2

Abstract
This paper focuses on convex constrained opti-
mization problems, where the solution is subject
to a convex inequality constraint. In particular,
we aim at challenging problems for which both
projection into the constrained domain and a lin-
ear optimization under the inequality constraint
are time-consuming, which render both projected
gradient methods and conditional gradient meth-
ods (a.k.a.
the Frank-Wolfe algorithm) expen-
sive. In this paper, we develop projection reduced
optimization algorithms for both smooth and
non-smooth optimization with improved conver-
gence rates under a certain regularity condition
of the constraint function. We ﬁrst present a gen-
eral theory of optimization with only one pro-
jection.
Its application to smooth optimization
with only one projection yields O(1/(cid:15)) iteration
complexity, which improves over the O(1/(cid:15)2)
iteration complexity established before for non-
smooth optimization and can be further reduced
under strong convexity. Then we introduce a lo-
cal error bound condition and develop faster al-
gorithms for non-strongly convex optimization at
the price of a logarithmic number of projections.
In particular, we achieve an iteration complex-
ity of (cid:101)O(1/(cid:15)2(1−θ)) for non-smooth optimization
and (cid:101)O(1/(cid:15)1−θ) for smooth optimization, where
θ ∈ (0, 1] appearing the local error bound con-
dition characterizes the functional local growth
rate around the optimal solutions. Novel applica-
tions in solving the constrained (cid:96)1 minimization
problem and a positive semi-deﬁnite constrained
distance metric learning problem demonstrate
that the proposed algorithms achieve signiﬁcant
speed-up compared with previous algorithms.

1The University of Iowa, Iowa City, IA 52242, USA 2National
Key Laboratory for Novel Software Technology, Nanjing Univer-
sity, Nanjing 210023, China. Correspondence to: Tianbao Yang
<tianbao-yang@uiowa.edu>.

This is the long version of our paper appearing in the Proceedings
of the 34 th International Conference on Machine Learning, Syd-
ney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).

1. Introduction

In this paper, we aim at solving the following convex con-
strained optimization problem:

min
x∈Rd

f (x),

s.t. c(x) ≤ 0,

(1)

where f (x) is a smooth or non-smooth convex function and
c(x) is a lower-semicontinuous and convex function. The
problem can ﬁnd applications in machine learning, signal
processing, statistics, marketing optimization, and etc. For
example, in distance metric learning one needs to learn a
positive semi-deﬁnite (PSD) matrix such that similar ex-
amples are close to each other and dissimilar examples are
far from each other (Weinberger et al., 2006; Xing et al.,
2003), where the positive semi-deﬁnite constraint can be
cast into a convex inequality constraint. Another example
arising in compressive sensing is to minimize the (cid:96)1 norm
of high-dimensional vector subject to a measurement con-
straint (Cand`es & Wakin, 2008). Although general interior-
point methods can be applied to solve the problem with lin-
ear convergence, they suffer from exceedingly high com-
putational cost per-iteration. Another solution is to em-
ploy the projected gradient (PG) method (Nesterov, 2004)
or the conditional gradient (CG) method (Frank & Wolfe,
1956), where the PG method needs to compute the pro-
jection into the constrained domain at each iteration and
CG needs to solve a linear optimization problem under
the constraint. However, for many constraints (e.g., PSD,
quadratic constraints) both projection into the constrained
domain and the linear optimization under the constraint are
time-consuming, which restrict their capabilities to solving
these problems.

Recently, there emerges a new direction towards address-
ing the challenge of expensive projection that is to reduce
the number of projections. In the seminal paper (Mahdavi
et al., 2012), the authors have proposed two algorithms with
only one projection at the end of iterations for non-smooth
convex and strongly convex optimization, respectively. The
idea of both algorithms is to move the constraint function
into the objective function and to control the violation of
constraint for intermediate solutions. While their devel-
oped algorithms enjoy an optimal convergence rate for non-
smooth optimization (i.e., O(1/(cid:15)2) iteration complexity)

Convex Constrained Optimization with Reduced Projections and Improved Rates

and a close-to-optimal convergence rate for strongly con-
vex optimization (i.e., (cid:101)O(1/(cid:15)) 1), there still lack of theory
and algorithms with reduced projections and faster rates
for smooth convex optimization and for convex optimiza-
tion without strong convexity assumptions.

In this paper, we make signiﬁcant contributions by devel-
oping a richer theory of convex constrained optimization
with reduced projections and faster rates. To be speciﬁc,

• we develop a general framework and theory of opti-
mization with only one projection, where any favorable
smooth or non-smooth convex optimization algorithms
can be employed to solve the intermediate augmented
unconstrained objective function. We discuss in full de-
tails the applicability of the proposed algorithms to prob-
lems with polyhedral, quadratic or PSD constraints.
• Applying the general theory to smooth convex opti-
mization 2 with Nesterov’s accelerated gradient meth-
ods yields an iteration complexity of O(1/(cid:15)) with only
one projection. In addition, when equipped with an opti-
mal algorithm for strongly convex optimization the gen-
eral theory implies the optimal iteration complexity of
O(1/(cid:15)) for strongly convex optimization with only one
projection. For smooth and strongly convex optimiza-
tion, the general theory implies an iteration complexity
of O(1/(cid:15)β) where β ∈ (1/2, 1) with only one projection
and a sufﬁciently large number of iterations.

• Building on the general framework and theory, we fur-
ther develop an improved theory with faster convergence
rates for non-strongly convex optimization at the price
of a logarithmic number of projections. In particular, we
show that under a mild local error bound condition, the
iteration complexities can be reduced to (cid:101)O(1/(cid:15)2(1−θ))
for non-smooth optimization and (cid:101)O(1/(cid:15)1−θ) for smooth
optimization, where θ ∈ (0, 1] is a constant in the local
error bound condition that characterizes the local growth
rate of functional values. To our knowledge, these are the
best convergence results with only a logarithmic number
of projections for non-strongly convex optimization. We
also demonstrate their effectiveness for solving compres-
sive sensing and distance metric learning problems.

2. Related Work

The issue of high projection cost in projected gradient
descent has received increasing attention in recent years.
Most studies are based on the Frank-Wolfe technique that
eschews the projection in favor of a linear optimization
over the constrained domain (Jaggi, 2013; Hazan & Kale,
2012; Lacoste-Julien et al., 2013; Garber & Hazan, 2015).
It happens that for many bounded domains (e.g., bounded

1where (cid:101)O() suppresses a logarithmic factor.
2where the constraint function is assumed to be smooth.

balls for vectors and matrices, a PSD constraint with a
bounded trace norm) the linear optimization over the con-
strained domain is much cheaper than projection into the
constrained domain (Jaggi, 2013). However, there still ex-
ist many constraints that render both projection into the
constrained domain and linear optimization under the con-
straint are comparably expensive. Examples include poly-
hedral constraints, quadratic constraints and a PSD con-
straint 3.

To tackle these complex constraints, the idea of optimiza-
tion with a reduced number of projections was explored in
several studies since (Mahdavi et al., 2012). In a recent pa-
per (Chen et al., 2016), the authors show that for stochastic
strongly convex optimization, the optimal convergence rate
can be achieved using a logarithmic number of projections.
In contrast, the developed theory in this paper implies that
only one projection is sufﬁcient to achieve the optimal con-
vergence rate for strongly convex optimization, and a log-
arithmic number of projections can be used to accelerate
convergence rates for non-strongly convex optimization.
Cotter et al. (2016) proposed a stochastic algorithm for
solving heavily constrained problems with many constraint
functions by extending the work of (Mahdavi et al., 2012).
Nonetheless, their focus is not to improve the convergence
rates. Zhang et al. (2013) studied the smooth and strongly
convex optimization and they proposed a stochastic algo-
rithm with O(κ log(T )) projections and proved an O(1/T )
convergence rate, where κ is the condition number and T
is the total number of iterations. Nonetheless, if the con-
dition number is high the number of projections could be
very large. In addition, their algorithm utilizes the mini-
batch to avoid frequent projections in stochastic optimiza-
tion, which is different from the present paper.

We note that several recent works also exploit different
forms of error bound conditions to improve the conver-
gence (Wang & Lin, 2014; So, 2013; Hou et al., 2013; Zhou
et al., 2015; Yang & Lin, 2016; Xu et al., 2016). Most
notably, the technique used in our work is closely related
to (Yang & Lin, 2016). However, for constrained optimiza-
tion problems the methods in (Yang & Lin, 2016) still need
to conduct projections at each iteration.

Finally, we comment on the differences between the pro-
posed methods and the classical penalty methods that also
move the constraint into the objective using a penalty func-
tion (Bertsekas, 1996). The major differences are that (i)
the classical penalty methods typically require solving each
subproblem exactly while our methods do not require that;
and (ii) the classical penalty methods typically guarantee
asymptotic convergence while our methods have explicit
convergence rates.

3Indeed, a linear optimization over a PSD constraint is ill-

posed because the PSD domain is unbounded.

Convex Constrained Optimization with Reduced Projections and Improved Rates

3. Preliminaries

Let Ω = {x ∈ Rd : c(x) ≤ 0} denote the constrained do-
main, Ω∗ denote the optimal solution set and f∗ denote the
optimal objective value. We denote by ∇f (x) the gradient
and by ∂f (x) the subgradient of a smooth or non-smooth
function, respectively. When f (x) is a non-smooth func-
tion, we consider the problem as non-smooth constrained
optimization. When both f (x) and c(x) are smooth, we
consider the problem as smooth constrained optimization.
A function f (x) is L-smooth if it has a Lipschitz continu-
ous gradient, i.e., (cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107), where
(cid:107) · (cid:107) denotes the Euclidean norm. A function f (x) is µ-
strongly convex if it satisﬁes f (x) ≥ f (y) + ∂f (y)(cid:62)(x −
y) + µ

2 (cid:107)x − y(cid:107)2.

In the sequel, dist(x, Ω) denotes the distance of x to a set
Ω, i.e., dist(x, Ω) = minu∈Ω (cid:107)x − u(cid:107). Let [s]+ be a hinge
operator that is deﬁned as [s]+ = s if s ≥ 0, and [s]+ = 0
if s < 0.

Throughout the paper, we make the the following assump-
tions to facilitate the development of our algorithms and
theory.

Assumption 1. For a convex minimization problem (1), we
assume (i) there exists a positive value ρ > 0 such that

(2)

(3)

min
c(x)=0
v∈∂c(x),v(cid:54)=0

(cid:107)v(cid:107) ≥ ρ,

or more generally there exists a constant ρ > 0 for any
x ∈ Rd, such that x(cid:92) = arg minu∈Rd,c(u)≤0 (cid:107)u − x(cid:107)2
satisﬁes

(cid:107)x(cid:92) − x(cid:107) ≤ [c(x)]+/ρ.

(ii) there exists a strictly feasible solution such that c(x) <
0; (iii) both f (x) and c(x) are deﬁned everywhere and are
Lipschitz continuous with their Lipschitz constants denoted
by G and Gc, respectively.
We make several remarks about the assumptions. The in-
equality in (2) is introduced in (Mahdavi et al., 2012),
which is to ensure the distance from the ﬁnal solution be-
fore projection to constrained domain Ω is not too large.
Note that the inequality in (3) is a more general condition
than (2) as seen from the following lemma.
Lemma 1. For any x ∈ Rd, let x(cid:92) = arg minc(u)≤0 (cid:107)u −
x(cid:107)2. If (2) holds, then (3) holds.

The above lemma is implicit in the proof of (Mahdavi et al.,
2012). We will provide more discussions about Assump-
tion 1(i) - the key assumption, and exhibit the value of ρ for
a number of commonly seen constraints (e.g., polyhedral,
quadratic and PSD constraints). To make the presentation
more ﬂuent, we postpone these discussions to Section 6.
The strict feasibility assumption (ii) allows us to explore
the KKT condition of the projection problem shown below.

Assumption (iii) imposes mild Lipschitz continuity condi-
tions on both f (x) and c(x).

Traditional projected gradient descent methods need
to solve the following projection at each iteration
ΠΩ[x] = arg minc(u)≤0 (cid:107)u − x(cid:107)2. Conditional gradi-
ent methods (a.k.a.
the Frank-Wolfe technique) need to
solve the following linear optimization at each iteration
minu∈Rd,c(u)≤0 u(cid:62)∇f (x). For many constraint functions
(see Section 6), solving the projection problem and the lin-
ear optimization could be very expensive.

4. A General Theory of Optimization with

only one projection

In this section, we extend the idea of only one projection
proposed in (Mahdavi et al., 2012) to a general theory, and
then present optimization algorithms with only one projec-
tion for non-smooth and smooth optimization, respectively.
To tackle the constraint, we introduce a penalty function
hγ(x) parameterized by γ, which obeys the following cer-
tiﬁcate: there exist constants C ≥ 0 and λ > G/ρ such
that

hγ(x) ≥ λ[c(x)]+, ∀x
hγ(x) ≤ Cγ, ∀x such that c(x) ≤ 0.
From the above condition, it is clear that γ ≥ 0. It is no-
table that the penalty function hγ(x) will also depend on
λ; however it will be set to a constant value, thus the de-
pendence on λ is omitted. We will construct such a penalty
function hγ(x) for non-smooth and smooth optimization in
next two subsections. We propose to optimize the follow-
ing augmented objective function

(4)

min
x∈Rd

Fγ(x) = f (x) + hγ(x).

(5)

We can employ any applicable optimization algorithms to
optimize Fγ(x) pretending that there is no constraint, and
ﬁnally obtain a solution (cid:98)xT that is not necessarily feasi-
ble. In order to obtain a feasible solution, we perform one
projection to get (cid:101)xT = ΠΩ((cid:98)xT ). The following theorem
allows us to convert the convergence of (cid:98)xT for Fγ(x) to
that of (cid:101)xT for f (x).
Theorem 1. Let A be any iterative optimization algorithm
applied to minx Fγ(x) with T iterations, which starts with
x1 and returns (cid:98)xT as the ﬁnal solution, such that the fol-
lowing convergence of (cid:98)xT holds for any x ∈ Rd
Fγ((cid:98)xT ) − Fγ(x) ≤ BT (γ; x, x1),
where BT (γ; x, x1) → 0 when T → ∞. Suppose that
Assumption 1 hold, then

(6)

f ((cid:101)xT ) − f (x∗) ≤

λρ
λρ − G

(Cγ + BT (γ; x∗, x1)),

(7)

where (cid:101)xT = ΠΩ[(cid:98)xT ] and x∗ is an optimal solution to (1).

Convex Constrained Optimization with Reduced Projections and Improved Rates

Remark: It is worth mentioning that we omit some con-
stant factors in the convergence bound BT (γ; x, x1) that
are irrelevant to our discussions. The notation BT (γ; x, x1)
emphasizes that it is a function of γ and depends on x1
and a target solution x and it will be referred to as BT . In
the next several subsections, we will see that by carefully
choosing the penalty function hγ(x) we are able to provide
nice convergence for smooth and non-smooth optimization
with only one projection. In the above theorem, we assume
the optimization algorithm A is deterministic. However, a
similar result can be easily extended to a stochastic opti-
mization algorithm A.

Proof. First, we consider c((cid:98)xT ) ≤ 0, which implies that
(cid:98)xT = (cid:101)xT . Due to the certiﬁcate of hγ(x), Fγ((cid:101)xT ) ≥
f ((cid:101)xT ) and Fγ(x∗) ≤ f (x∗) + Cγ. Hence f ((cid:101)xT ) ≤
Fγ((cid:98)xT ) ≤ Fγ(x∗) + BT (γ; x1, x∗) ≤ f (x∗) + Cγ +
BT (γ; x1, x∗). Then (7) follows due to λρ/(λρ − G) ≥ 1.
Next, we assume c((cid:98)xT ) > 0. Inequality (6) implies that
f ((cid:98)xT ) + λ[c((cid:98)xT )]+ ≤ f (x∗) + Cγ + BT (γ; x∗, x1). (8)
By Assumption 1(i), we have [c((cid:98)xT )]+ ≥ ρ(cid:107)(cid:98)xT − (cid:101)xT (cid:107).
Combined with (8) we have

λρ(cid:107)(cid:98)xT − (cid:101)xT (cid:107) ≤ f (x∗) − f ((cid:98)xT ) + Cγ + BT (γ; x∗, x1)

≤ G(cid:107)(cid:98)xT − (cid:101)xT (cid:107) + Cγ + BT (γ; x∗, x1),

where the last inequality follows that fact f (x∗)−f ((cid:98)xT ) ≤
f (x∗) − f ((cid:101)xT ) + f ((cid:101)xT ) − f ((cid:98)xT ) ≤ G(cid:107)(cid:98)xT − (cid:101)xT (cid:107) because
the Lipschitz property and f (x∗) ≤ f ((cid:101)xT ). Therefore we
have

(cid:107)(cid:98)xT − (cid:101)xT (cid:107) ≤

Cγ + BT (γ; x∗, x1, )
λρ − G

.

Finally, we obtain

f ((cid:101)xT ) − f (x∗) ≤ f ((cid:101)xT ) − f ((cid:98)xT ) + f ((cid:98)xT ) − f (x∗)

≤ G(cid:107)(cid:98)xT − (cid:101)xT (cid:107) + Cγ + BT (γ; x∗, x1)

≤

λρ
λρ − G

(Cγ + BT (γ; x∗, x1)).

4.1. Non-smooth Optimization

Since an optimal convergence rate for general non-smooth
optimization with only one projection has been attained
in (Mahdavi et al., 2012), in this subsection we present an
optimal convergence result for strongly convex problems.
For non-smooth optimization, we can choose

h(x) = λ[c(x)]+,

and hence γ = 0. We will use deterministic subgradi-
ent descent as an example to demonstrate the convergence
for f (x), though many other optimization algorithms de-
signed for non-smooth optimization are applicable (e.g.,

the stochastic subgradient method). The update of subgra-
dient descent method is given by the following

xt+1 = xt − ηt∂F (xt),

t = 1, . . . , T,

(9)

αT

(cid:80)T

where ηt is an appropriate step size. If f (x) is µ-strongly
convex, the step size can be set as ηt = 1/(µt) and
the ﬁnal solution can be computed by the α-sufﬁx aver-
aging (cid:98)xT = 1
t=(1−α)T +1 xt with α > 0 (Rakhlin
et al., 2012), or by the polynomial decay averaging with
(cid:98)xt = (1 − s+1
s+t xt and s ≥ 1 (Shamir & Zhang,
2013). Both schemes can attain BT = O(1/(µT )) for
the convergence of F (x) when f (x) is µ-strongly convex.
Combining this with Theorem 1, we have the following
convergence result with the proof omitted due to its sim-
plicity.

s+t )(cid:98)xt−1 + s+1

Corollary 2. Suppose that Assumption 1 holds and f (x) is
µ-strongly convex. Set F (x) = f (x) + λ[c(x)]+ with λ ≥
G/ρ. Let (9) run for T iterations with ηt = 1/(µt). Let (cid:98)xT
be computed by α-sufﬁx averaging or the polynomial decay
averaging. Then with only one projection (cid:101)xT = ΠΩ((cid:98)xT ),
we achieve

f ((cid:101)xT ) − f∗ ≤

λρ
λρ − G

(G + λGc)2O(1)
µT

.

Remark: We note that the O(1/(µT )) is also achieved for
strongly convex optimization in (Zhang et al., 2013; Chen
et al., 2016) but with a logarithmic number of projections.
In contrast, Corollary 2 implies only one projection is suf-
ﬁcient to achieve the optimal convergence for strongly con-
vex optimization.

4.2. Smooth Optimization
For smooth optimization, we consider both f (x) and c(x)
to be smooth 4. Let the smoothness parameter of f (x) and
c(x) be Lf and Lc, respectively.
In order to ensure the
augmented function Fγ(x) to be still a smooth function,
we construct the following penalty function

hγ(x) = γ ln (1 + exp (λc(x)/γ)) .

(10)

The following proposition shows that hγ(x) is a smooth
function and obeys the condition in (4).

Proposition 1. Suppose c(x) is Lc-smooth and Gc-
Lipschitz continuous. The penalty function in (10) is a
(λLc + λ2G2
4γ )-smooth function and satisﬁes (i) hγ(x) ≥
λ[c(x)]+ and (ii) hγ(x) ≤ γ ln 2, ∀x such that c(x) ≤ 0.

c

Then Fγ(x) is a smooth function and its smoothness pa-
rameter is given by LF = Lf + λLc + λ2G2
4γ . Next, we will

c

4it can be extended to when f (x) is non-smooth but its proxi-

mal mapping can be easily solved.

Convex Constrained Optimization with Reduced Projections and Improved Rates

establish the convergence for f (x) using Nesterov’s opti-
mal accelerated gradient (NAG) methods. The update of
one variant of NAG can be written as follows

xt+1 = yt − ∇Fγ(yt)/LF
yt+1 = xt+1 + βt+1(xt+1 − xt),

(11)

(cid:16)

(cid:17)
)

(cid:113) µ
LF

LF exp(−T

where the value of βt can be set to different values de-
pending on whether f (x) is strongly convex or not (see
Corollary 3). Previous work have established the conver-
gence of (cid:98)xT = xT for Fγ(x), in particular BT = O( LF
T 2 )
for smooth non-strongly convex optimization and BT =
O
for smooth and strongly convex
optimization. By combining these results with Theorem 1
and appropriately setting γ, we can achieve the following
convergence of (cid:101)xT for f (x).
holds,
Corollary
3.
dist(y0, Ω∗) ≤ D, f (x) is Lf -smooth and c(x) is
Lc-smooth. Set Fγ(x) = f (x) + hγ(x) with λ > G/ρ
and hγ(x) being (10). Let (11) run for T iterations and
(cid:101)xT = ΠΩ(xT ).
• If f (x) is convex, we can set γ =

that Assumption

Suppose

λGcD
√

, βt =

1

(T +1)

2 ln 2

τt−1−1
τt

, where τt =

f ((cid:101)xT )−f∗ ≤

λρ
λρ − G

√

1+

1+4τ 2
2
λGcD

t−1
√

(cid:34)

with τ0 = 1, and achieve
(cid:35)
(Lf + λLc)D2
(T + 1)2

2 ln 2

+

T + 1

• If f (x) is µ-strongly convex, we can set γ = 1
√
√

α ∈ (1/2, 1) and βt =

, and achieve

√
√

T 2α with

f ((cid:101)xT ) − f∗ ≤ O

LF −
LF +

µ
µ
(cid:18) 1

(cid:19)

,

1
T 4α

T 2α +
(cid:17) 1

c/4

as long as T ≥

(cid:16) Lf +λLc+λ2G2

µ

2(1−α) (4α ln T )

1
1−α .

Remark: The convergence results above indicate an
O(1/(cid:15)) iteration complexity for smooth optimization and
O(1/(cid:15)1/(2α)) with α ∈ (1/2, 1) for smooth and strongly
convex optimization with only one projection. All omitted
proofs can be found in (Yang et al., 2017).

5. Improved Convergence for Non-strongly

Convex Optimization

In this section, we will develop improved convergence for
non-strongly convex optimization at a price of a logarith-
mic number of projections by considering an additional
condition on the target problem. To facilitate the presen-
tation, we ﬁrst introduce some notations. The (cid:15)-sublevel
set S(cid:15) and (cid:15)-level set L(cid:15) of the problem (1) are denoted by
S(cid:15) = {x ∈ Ω : f (x) ≤ f∗ + (cid:15)}, and L(cid:15) = {x ∈ Ω :
f (x) = f∗ + (cid:15)}, respectively. Let x†
(cid:15) denote the closest
point in the (cid:15)-sublevel set S(cid:15) to x ∈ Ω, i.e.,
(cid:107)u − x(cid:107)2,

f (u) ≤ f∗ + (cid:15).

s.t.

x†

(12)

(cid:15) = arg min
u∈Ω

Let x∗ denote the closest optimal solution in Ω∗ to x, i.e.,
x∗ = arg minu∈Ω∗ (cid:107)u − x(cid:107)2.

In this section, we will make the following additional as-
sumption about the problem (1).
Assumption 2. For a convex minimization problem (1), we
assume (i) there exist x0 ∈ Ω and (cid:15)0 ≥ 0 such that f (x0)−
minx∈Ω f (x) ≤ (cid:15)0; (ii) Ω∗ is a non-empty convex compact
set; (iii) the optimization problem (1) satisﬁes a local error
bound condition, i.e., there exist θ ∈ (0, 1] and σ > 0 such
that for any x ∈ S(cid:15) we have dist(x, Ω∗) ≤ σ(f (x) −
f∗)θ where Ω∗ denotes the optimal set and f∗ denotes the
optimal value.
Remark: we would like to remark that the new assumption
only imposes mild conditions on the problem. In particular,
Assumption 2 (i) supposes there is a lower bound of the op-
timal value f∗, which usually holds in machine learning ap-
plications where the objective function if non-negative; As-
sumption 2 (ii) ensures that S(cid:15) is also bounded (Rockafel-
lar, 1970), therefore the σ in the local error bound is ﬁnite,
which can be easily satisﬁed for a norm regularized or con-
straint problems; the local error bound condition holds for
a broad family of functions (e.g., semi-algebraic functions
or real subanalytic functions (Jerome Bolte, 2015; Yang &
Lin, 2016)). In Section 7, we will also demonstrate several
applications of the improved algorithms proposed in this
section by establishing the local error bound condition.

Although the local error bound condition is much weaker
than the strong convexity assumption, below we will pro-
pose novel algorithms leveraging this condition with faster
convergence and only a logarithmic number of projections.

5.1. Non-smooth Optimization

To establish an improved convergence for non-smooth opti-
mization, we develop a new algorithm shown in Algorithm
1 based on subgradient descent (GD) method, to which
we refer as LoPGD. The algorithm runs for K epochs and
each epoch employs GD for minimizing F (x) = f (x) +
λ[c(x)]+ with a feasible solution xk−1 ∈ Ω as a starting
point and t iterations of updates. At the end of each epoch,
the averaged solution (cid:98)xk is projected into the constrained
domain Ω and the solution xk will be used as the starting
point for next epoch. The step size ηk is decreased by half
every epoch starting from a given value η1. The theorem
below establishes the iteration complexity of LoPGD and
also exhibits the values of K, t and η1. To simplify nota-
λρ−G and ¯G = G + λGc.
tions, we let p = λρ
Theorem 4. Suppose Assumptions 1 and 2 hold. Let η1 =
2p ¯G2 , K = (cid:100)log2((cid:15)0/(cid:15))(cid:101) and t = 4σ2p2 ¯G2
in Algorithm 1,
where θ and σ are constants appearing in the local error
bound condition. Then f (xK) − f∗ ≤ 2(cid:15).
Remark: Since the projection is only conducted at the
end of each epoch and the total number of epochs is at

(cid:15)2(1−θ)

(cid:15)0

Convex Constrained Optimization with Reduced Projections and Improved Rates

1 = xk−1

Let xk
for s = 1, 2, . . . , t − 1 do

Algorithm 1 LoPGD
1: INPUT: K ∈ N+ , t ∈ N+, η1
2: Initialization: x0 ∈ Ω, (cid:15)0
3: for k = 1, 2, . . . , K do
4:
5:
6:
7:
8:
9:
10: end for

end for
Let (cid:98)xk = (cid:80)t
Let xk = ΠΩ[(cid:98)xk] and ηk+1 = ηk/2

s − ηk∂F (xk
s )

s+1 = xk

Update xk

s=1 xk

s /t

Algorithm 2 LoPNAG
1: INPUT: K ∈ N+ , t1, . . . , tK ∈ N+, γ1
2: Initialization: x0 ∈ Ω, (cid:15)0
3: for k = 1, 2, . . . , K do
4:
5:
6:

Let yk
for s = 0, 1, 2, . . . , tk − 1 do

0 = xk−1

Update xk
Update yk

s+1 = yk
s+1 = xk

∇Fγk (xk
s )

s − 1
Lk
s+1 + βs+1(xk

s+1 − xk
s )

7:
8:
9:
10: end for

end for
Let (cid:98)xk = xk

tk , xk = ΠΩ[(cid:98)xk] and γk+1 = γk/2

most K = (cid:100)log2((cid:15)0/(cid:15))(cid:101), so the total number of projec-
tions is only a logarithmic number K. The iteration com-
plexity in Theorem 4 is (cid:101)O(1/(cid:15)2(1−θ)) that improves the
standard result of O(1/(cid:15)2) without strong convexity. With
θ = 1/2, we can achieve (cid:101)O(1/(cid:15)) iteration complexity with
only O(log(1/(cid:15))) projections.

5.2. Smooth Optimization

τs

Similar to non-smooth optimization, we also develop a new
algorithm based on NAG shown in Algorithm 2, where
Fγ(x) is deﬁned using hγ(x) in (10), Lk = LFγk
is the
smoothness parameter of Fγk and βs = τs−1−1
, s = 1, . . . ,
is a sequence with τs updated as in Corollary 3. We refer
to this algorithm as LoPNAG. The key idea is to use to a
sequence of reducing values for γk instead of using a small
value as in Corollary 3, and solve each augmented uncon-
strained problem Fγk (x) approximately with one projec-
tion. The theorem below exhibits the iteration complexity
of LoPNAG and reveals the values of K, γ1 and t1, . . . , tK.
To simplify notations, we let ¯L = Lf + λLc.
Theorem 5. Suppose Assumptions 1 and 2 hold
is Lf -smooth and c(x)
and f (x)
is Lc-smooth.
6p ln 2 , K = (cid:100)log2((cid:15)0/(cid:15))(cid:101) and tk =
Let γ1 =
18 ln 2, (cid:112)12(Lf + λLc)(cid:15)0/2k−1}
σ
(cid:15)1−θ max{λGcp
in Algorithm 2, where θ and σ are constants appearing in
the local error bound condition. Then f (xK) − f∗ ≤ 2(cid:15).
Remark:
It is not difﬁcult to show that the total number
of iterations is bounded by (cid:101)O(1/(cid:15)1−θ), which improves the
one in Corollary 3 without strong convexity. If f (x) is a

(cid:15)0
√

simple non-smooth function whose proximal mapping can
be easily computed (e.g., (cid:96)1 norm), we can replace step
6 in Algorithm 2 by a proximal mapping to handle f (x),
which gives the same convergence result in Theorem 5. An
example is presented in Section 7 for compressive sensing
with θ = 1/2.

6. Discussion of Assumption 1 (i)

One might note that a key condition for developing the the-
ory with reduced projections is Assumption 1 (i). Although
Mahdavi et al. (2012) has brieﬂy mentioned that the con-
dition can be satisﬁed for a PSD cone or a Polytope (a
bounded polyhedron), their discussion lacks of details in
particular on the value of ρ in (2) or (3). Below, we discuss
the condition in details about three types of constraints.

Polyhedral constraints. First, we show that when c(x)
is a polyhedral function, i.e., its epigraph is a polyhedron
(not necessarily bounded), the inequality (3) is satisﬁed. To
this end, we explore the polyhedral error bound (PEB) con-
dition (Gilpin et al., 2012; Yang & Lin, 2016). In particu-
lar, if we consider an optimization problem, minx∈Rd h(x),
where the epigraph of h(x) is polyhedron. Let H∗ denote
the optimal set and h∗ denote the optimal value of the prob-
lem above. The PEB says that there exists ρ > 0 such that
for any x ∈ Rd

dist(x, H∗) ≤ (h(x) − h∗)/ρ.

(13)

To show that the inequality (3) holds for a polyhedral
function c(·), we can consider the optimization problem
minx∈Rd [c(x)]+. The optimal set of the above problem
is given by H∗ = {x ∈ Rd : c(x) ≤ 0}. For any x
such that c(x) > 0, let x(cid:92) = arg minc(u)≤0 (cid:107)u − x(cid:107)2
be the closest point in the optimal set to x. There-
fore if c(·) is a polyhedral function so does [c(x)]+, by
the PEB condition (13) there exists a ρ > 0 such that
(cid:107)x − x(cid:92)(cid:107) ≤ ([c(x)]+ − minx[c(x)]+)/ρ = [c(x)]+/ρ.
Let us consider a concrete example, where the problem has
a set of afﬁne inequalities c(cid:62)
i x − bi ≤ 0, i = 1, . . . , m.
There are two methods to encode this into a single con-
straint function c(x) ≤ 0. The ﬁrst method is to use
c(x) = max1≤i≤m c(cid:62)
i x − bi, which is a polyhedral func-
tion and therefore satisﬁes (3). The second method is to
use c(x) = (cid:107)[Cx − b]+(cid:107), where [a]+ = max(0, a) and
C = (c1, . . . , cm)(cid:62). Thus [c(x)]+ = (cid:107)[Cx − b]+(cid:107). The
inequality (3) is then guaranteed by Hoffman’s bound and
the parameter ρ is given by the minimum non-zero eigen-
value of C (cid:62)C (Wang & Lin, 2014). Note that the pro-
jection onto a polyhedron is a linear constrained quadratic
programming problem, and the linear optimization over a
polyhedron is a linear programming problem. Both have
polynomial time complexity that would be high if m and d
are large (Karmarkar, 1984; Kozlov et al., 1980).

Convex Constrained Optimization with Reduced Projections and Improved Rates

Quadratic constraint. A quadratic constraint can take
the form of (cid:107)Ax − y(cid:107)2 ≤ τ , where A ∈ Rm×d and
y ∈ Rm. Such a constraint appears in compressive sens-
ing (Cand`es & Wakin, 2008)5, where the goal is to re-
construct a sparse high-dimensional vector x from a small
number of noisy measurements y = Ax + ε ∈ Rm with
m (cid:28) d. The corresponding optimization problem is

minx∈Rd

(cid:107)x(cid:107)1,

s.t. (cid:107)Ax − y(cid:107)2 ≤ τ.

(14)

where τ ≥ (cid:107)ε(cid:107)2 is an upper bound on the magnitude of the
noise. To check the Assumption 1(i), we note that c(x) =
(cid:107)Ax−y(cid:107)2 −τ and ∇c(x) = A(cid:62)(Ax−y). Let us consider
that A has a full row rank 6 and denote by v = Ax − y,
τ and
then on the boundary c(x) = 0 we have (cid:107)v(cid:107) =
(cid:107)A(cid:62)v(cid:107) ≥ (cid:112)τ λmin(AA(cid:62)), where λmin(AA(cid:62)) > 0 is the
minimum eigenvalue of AA(cid:62) ∈ Rm×m. Therefore the
Assumption 1(i) is satisﬁed with ρ = (cid:112)τ λmin(AA(cid:62)). It
is notable that the projection and the linear optimization
under the quadratic constraint require solving a quadratic
programming problem and therefore could be expensive.

√

PSD constraint. A PSD constraint X (cid:23) 0 for X ∈ Rd×d
can be written as an inequality constraint −λmin(X) ≤ 0,
where λmin(X) denotes the minimum eigen-value of X.
The subgradient of c(X) = −λmin(X) when λmin(X) =
0 is given by Conv{−uu(cid:62)|(cid:107)u(cid:107) = 1, Xu = 0}, i.e., the
convex hull of the outer products of normalized vectors in
the null space of the matrix X. In (Yang et al., 2017), we
show that if the dimension of the null space of X is r with
1 ≤ r ≤ d, the norm of the subgradient of c(X) on the
r ≥ 1√
boundary c(X) = 0 is lower bounded by ρ = 1√
.
d
Finally, we note that computing a subgradient of [c(X)]+
only needs to compute one eigen-vector corresponding to
the smallest eigen-value. In contrast, both projection and
linear optimization under a PSD constraint could be very
expensive for high-dimensional problems. In particular, the
projection onto a PSD domain needs to conduct a singular
value decomposition. The linear optimization over a PSD
cone is ill-posed due to that PSD cone is not compact (the
solution is either 0 or inﬁnity). One may add an artiﬁcial
constraint on the upper bound of the eigen-values. Accord-
ing to (Jaggi, 2013), the time complexity for solving this
linear optimization problem approximately up to an accu-
racy level (cid:15)(cid:48) is O(N d1.5/(cid:15)(cid:48)2.5) with N being the number
of non-zeros in the gradient and (cid:15)(cid:48) decreasing iteratively
required in the Frank-Wolfe method, which could be much
more expensive especially for high-dimensional problems
and in later iterations than computing the ﬁrst eigen-pairs
at each iteration in our methods.

5Here we use the square constraint to make it a smooth func-
tion so that the proposed algorithms for smooth optimization are
applicable by using proximal gradient mapping to handle the (cid:96)1
norm.

7. Applications
7.1. Compressive Sensing

We ﬁrst consider a compressive sensing problem in (14).
Becker et al. (2011) proposed an optimization algorithm
based on the Nesterov’s smoothing and the Nesterov’s op-
timal method for the smoothed problem, known as NESTA.
It needs to perform the projection into the domain (cid:107)Ax −
y(cid:107)2 ≤ τ at every iteration and has an iteration complexity
of O(1/(cid:15)). In contrast, the presented algorithm with only
one projection in Section 4.2 using Nesterov’s accelerated
proximal gradient method (Beck & Teboulle, 2009) to solve
the unconstrained problem enjoys an iteration complexity
of O(1/(cid:15)). Moreover, we present a theorem below showing
that the problem (14) satisﬁes the local error bound con-
dition with θ = 1/2, and hence the presented LoPNAG
(cid:15)) iteration complexity with only a loga-
enjoys an (cid:101)O(1/
rithmic number of projections.
Theorem 6. Let f (x) = (cid:107)x(cid:107)1, c(x) = (cid:107)Ax − y(cid:107)2 − τ ,
Ω∗ denote the optimal set and f∗ be the optimal solution
to (14). Assume that there exists x0 such that (cid:107)Ax0−y(cid:107)2 <
τ and 0 (cid:54)∈ Ω∗. Then for any (cid:15) > 0, x ∈ Rd such that
c(x) ≤ 0 and f (x) ≤ f∗ + (cid:15), there exists 0 < σ < ∞
such that dist(x, Ω∗) ≤ σ(f (x) − f∗)1/2. Hence, LoP-
NAG can have an iteration complexity of (cid:101)O(1/
(cid:15)) with
only O(log(1/(cid:15))) projections.

√

√

Next, we demonstrate the effectiveness of the LoPNAG for
solving the compressive sensing problem in (14) by com-
paring with NESTA. We generate a synthetic data for test-
ing. In particular, we generate a random measurement ma-
trix A ∈ Rm×d with m = 1000 and d = 5000. The en-
tries of the matrix A are generated independently with the
uniform distribution over the interval [−1, +1]. The vector
x∗ ∈ Rd is generated with the same distribution at 100 ran-
domly chosen coordinates. The noise ε ∈ Rm is a dense
vector with independent random entries with the uniform
distribution over the interval [−ζ, ζ], where ζ is the noise
magnitude and is set to 0.01. Finally the vector y was ob-
tained as y = Ax∗ + ε.

We use the Matlab package of NESTA 7. For fair compari-
son, we also use the projection code in the NESTA package
for conducting projection. To handle the unknown smooth-
ness parameter in the proposed algorithm, we use the back-
tracking technique (Beck & Teboulle, 2009). The param-
eter γ is initially set to 0.001 and decreased by half every
5000 iterations after a projection and the target smoothing
parameter in NESTA is set to 10−5. For the value of λ in
LoPNAG, we tune it from its theoretical value to several
smaller values and choose the one that yields the fastest
convergence. We report the results in Table 7.1, which
include different number of iterations, the corresponding

7http://statweb.stanford.edu/˜candes/

6which is reasonable because m (cid:28) d.

nesta/

Convex Constrained Optimization with Reduced Projections and Improved Rates

Table 1. LoPNAG vs. NESTA for solving the compressive sensing problem.

LoPNAG

Iters - Projs Rec. Err.
0.018017
5000 - 1
0.018038
10000 - 2
0.018043
15000 - 3
0.018043
20000 - 4
0.018043
25000 - 5

Objective
52.042878
52.042418
52.042358
52.042358
52.042358

Time (s)
18.04
35.88
53.09
70.24
87.32

Iters - Projs
1000 - 2000
3000 - 6000
5000 - 10000
8000 - 16000
10000 - 20000

NESTA

Rec. Err.
0.137798
0.018669
0.018659
0.018657
0.018657

Objective
52.703275
52.050051
52.050046
52.050045
52.050044

Time (s)
48.49
93.84
245.23
404.72
501.65

Table 2. LoPGD vs. OPGD and PGD for solving the considered distance metric learning problem.

LoPGD

Iters - Projs Objective
1000 - 1
2000 - 2
4000 - 4
6000 - 6
8000 - 8

0.0953
0.0695
0.0494
0.0428
0.0405

Time (h)
0.22
0.43
0.87
1.33
1.89

OPGD

Iters - Projs Objective
1000 - 1
2000 - 1
4000 - 1
6000 - 1
8000 - 1

0.1707
0.1583
0.1469
0.1398
0.1343

Time (h)
0.20
0.40
0.80
1.22
1.64

Iters - Projs
1000 - 1000
2000 - 2000
4000 - 4000
6000 - 6000
8000 - 8000

PGD
Objective
0.1491
0.1278
0.1072
0.0957
0.0879

Time (h)
7.97
15.46
29.39
43.36
57.43

number of projections, the recovery error of the found so-
lution compared to the underlying true sparse solution, the
objective value (i.e., the (cid:96)1 norm of the found solution) and
the running time. Note that each iteration of NESTA re-
quires two projections because it maintains two extra se-
quence of solutions. From the results, we can see that LoP-
NAG converges signiﬁcantly faster than NESTA. Even with
only one projection, we are able to obtain a better solution
than that of NESTA after running 10000 iterations.

7.2. High-dimensional Distance Metric Learning

Consider the following distance metric learning problem:

min
A(cid:23)0

1
2|E|

(cid:88)

(i,j)∈E

(1 − yij − (cid:107)xi − xj(cid:107)2

A)2 + τ (cid:107)A(cid:107)off

1 , (15)

1 = (cid:80)

where E denotes all pairs of training examples, yij = 1
indicates xi, xj belong to the same class and yij = −1
A = z(cid:62)Az
indicates they belong to different classes, (cid:107)z(cid:107)2
and (cid:107)A(cid:107)off
i(cid:54)=j |Aij|. We note that such a formu-
lation is useful for high dimensional problems due to the
(cid:96)1 regularizer. A similar formulation with different forms
of loss function has been adopted in literature (Qi et al.,
2009). We consider the square loss because it gives us
faster convergence with a logarithmic number of projec-
tions by LoPGD. Due to the presence of the non-smooth
PSD constraint and the (cid:96)1 regularizer, Nesterov’s accel-
erated proximal gradient methods can not be applied ef-
ﬁciently to solving (15) and the augmented unconstrained
problem. Nevertheless, we can apply the proposed LoPGD
method for solving the problem with a logarithmic number
of projections. Regarding the constant θ in the local error
bound condition for (15), it still remains an open problem.
Nonetheless, a local error bound condition with θ = 0.5
might be established under certain regularity condition of
the problem (Zhou & So, 2015; Cui et al., 2017). For ex-
ample, Cui et al. (2017) provided a direct analysis of a lo-
cal error bound condition with θ = 0.5 for a class of con-
strained convex symmetric matrix optimization problems

regularized by nonsmooth spectral functions (including the
indicator function of a PSD constraint). They established
sufﬁcient conditions (Theorem 16) for a local error bound
condition with θ = 0.5 to hold, which reduces to a regular-
ity condition for (15) depending on the optimal solutions of
the problem. A thorough analysis of the regularity condi-
tion is much more involved and left as an open problem.

Next, we demonstrate the empirical performance of
LoPGD for solving (15). We use the colon-cancer data
available on libsvm web portal, which has 2000 features
and 62 examples. Fourty examples are used as training
examples to generate 780 pairs to learn the distance met-
ric. The regularization parameter is set to τ = 0.001. We
compare LoPGD, gradient descent method with only one
projection (referred to as OPGD), and standard projected
GD (referred to PGD). The step size in PGD and OPGD
t, where t is the iteration index. We use the
is set to η0/
same tuned initial step size for all algorithms. The number
of iterations per-epoch in LoPGD is set to 1000. The pe-
nalization parameter λ in both OPGD and LoPGD is tuned
and set to 10. In Table 2, we report the objective values, the
#of iterations/projections, and running time across the ﬁrst
8000 iterations. We can see that LoPGD converges dramat-
ically faster than PGD and also much faster than OPGD.

√

8. Conclusion
We have developed a general theory of optimization with
only one projection for a family of inequality constrained
convex optimization problems. It yields an improved iter-
ation complexity for smooth optimization compared with
non-smooth optimization. By exploring the local error
bound condition, we further develop new algorithms with a
logarithmic number of projections and achieve better con-
vergence for both smooth and non-smooth optimization
without strong convexity assumption. Applications in com-
pressive sensing and distance metric learning demonstrate
the effectiveness of the proposed improved algorithms.

Convex Constrained Optimization with Reduced Projections and Improved Rates

Acknowledgements

We are grateful to all anonymous reviewers for their help-
ful comments. T. Yang is partially supported by National
Science Foundation (IIS-1463988, IIS-1545995). L. Zhang
thanks the support from NSFC (61603177) and JiangsuSF
(BK20160658).

References

Beck, Amir and Teboulle, Marc. A fast iterative shrinkage-
inverse problems.

thresholding algorithm for
linear
SIAM J. Img. Sci., 2:183–202, 2009.

Becker, Stephen, Bobin, J´erˆome, and Cand`es, Emmanuel J.
Nesta: A fast and accurate ﬁrst-order method for sparse
recovery. SIAM J. Img. Sci., 4:1–39, 2011. ISSN 1936-
4954.

Bertsekas, Dimitri P. Constrained Optimization and La-
grange Multiplier Methods (Optimization and Neural
Computation Series). Athena Scientiﬁc, 1 edition, 1996.
ISBN 1886529043.

Cand`es, Emmanuel J. and Wakin, Michael B. An introduc-
tion to compressive sampling. IEEE Signal Processing
Magazine, 25(2):21 –30, 2008.

Chen, Jianhui, Yang, Tianbao, Lin, Qihang, Zhang, Lijun,
and Chang, Yi. Optimal stochastic strongly convex op-
timization with a logarithmic number of projections. In
Proceedings of the Thirty-Second Conference on Uncer-
tainty in Artiﬁcial Intelligence (UAI), 2016.

Cotter, Andrew, Gupta, Maya R., and Pfeifer, Jan. A light
In Proceedings of
touch for heavily constrained SGD.
the 29th Conference on Learning Theory (COLT), pp.
729–771, 2016.

Cui, Ying, Ding, Chao, and Zhao, Xinyuan. Quadratic
growth conditions for convex matrix optimization prob-
lems associated with spectral functions. CoRR, 2017.

Frank, Marguerite and Wolfe, Philip.

An algorithm
for quadratic programming. Naval Research Logistics
(NRL), 3:149–154, 1956.

Garber, Dan and Hazan, Elad. Faster rates for the frank-
wolfe method over strongly-convex sets. In Proceedings
of the 32nd International Conference on Machine Learn-
ing (ICML), pp. 541–549, 2015.

Hazan, Elad and Kale, Satyen. Projection-free online learn-
ing. In Proceedings of the International Conference on
Machine Learning (ICML), 2012.

Hou, Ke, Zhou, Zirui, So, Anthony Man-Cho, and Luo,
Zhi-Quan. On the linear convergence of the proxi-
mal gradient method for trace norm regularization.
In
Advances in Neural Information Processing Systems
(NIPS), pp. 710–718, 2013.

Jaggi, Martin. Revisiting frank-wolfe: Projection-free
sparse convex optimization. In Proceedings of the Inter-
national Conference on Machine Learning (ICML), pp.
427–435, 2013.

Jerome Bolte, Trong Phong Nguyen, Juan Peypouquet
Bruce Suter. From error bounds to the complexity of
ﬁrst-order descent methods for convex functions. CoRR,
abs/1510.08234, 2015.

Karmarkar, N. A new polynomial-time algorithm for linear
In Proceedings of the Sixteenth Annual
programming.
ACM Symposium on Theory of Computing, pp. 302–311,
1984.

Kozlov, M.K., Tarasov, S.P., and Khachiyan, L.G. Polyno-
miale Loesbarkeit der konvexen quadratischen Program-
mierung. Zh. Vychisl. Mat. Mat. Fiz., 20:1319–1323,
1980. ISSN 0044-4669.

Lacoste-Julien, Simon, Jaggi, Martin, Schmidt, Mark, and
Pletscher, Patrick. Block-coordinate frank-wolfe opti-
mization for structural svms. In Proceedings of the Inter-
national Conference on Machine Learning (ICML), pp.
53–61, 2013.

Mahdavi, M., Yang, T., Jin, R., and Zhu, S. Stochastic
gradient descent with only one projection. In Advances
in Neural Information Processing Systems (NIPS), pp.
503–511, 2012.

Nesterov, Yurii. Introductory lectures on convex optimiza-
tion: a basic course, volume 87 of Applied optimization.
Kluwer Academic Publishers, 2004.

Qi, Guo-Jun, Tang, Jinhui, Zha, Zheng-Jun, Chua, Tat-
Seng, and Zhang, Hong-Jiang. An efﬁcient sparse metric
learning in high-dimensional space via l1-penalized log-
determinant regularization. In Proceedings of the 26th
international conference on Machine learning (ICML),
pp. 106, 2009.

Gilpin, Andrew, Pe˜na, Javier, and Sandholm, Tuomas.
First-order algorithm with log(1/epsilon) convergence
for epsilon-equilibrium in two-person zero-sum games.
Math. Program., 133(1-2):279–298, 2012.

Rakhlin, Alexander, Shamir, Ohad, and Sridharan, Karthik.
Making gradient descent optimal for strongly convex
stochastic optimization. In Proceedings of the 29th inter-
national conference on Machine learning (ICML), 2012.

Convex Constrained Optimization with Reduced Projections and Improved Rates

Rockafellar, R.T. Convex Analysis. Princeton mathematical

series. Princeton University Press, 1970.

Shamir, Ohad and Zhang, Tong. Stochastic gradient de-
scent for non-smooth optimization: Convergence re-
sults and optimal averaging schemes. In Proceedings of
the 30th International Conference on Machine Learning
(ICML), pp. 71–79, 2013.

So, Anthony Man-Cho. Non-asymptotic convergence anal-
ysis of inexact gradient methods for machine learning
without strong convexity. CoRR, abs/1309.0113, 2013.

Wang, Po-Wei and Lin, Chih-Jen. Iteration complexity of
feasible descent methods for convex optimization. Jour-
nal of Machine Learning Research, 15(1):1523–1548,
2014.

Weinberger, Kilian Q., Blitzer,

and Saul,
large
Lawrence K.
In Advances
margin nearest neighbor classiﬁcation.
in Neural Information Processing Systems (NIPS), pp.
1473–1480, 2006.

Distance metric learning for

John,

Xing, E., Ng, A., Jordan, M., and Russell, S. Distance
metric learning with application to clustering with side-
In Advances in Neural Information Pro-
information.
cessing Systems (NIPS), volume 15, pp. 505–512, 2003.

Xu, Yi, Yan, Yan, Lin, Qihang, and Yang, Tianbao. Ho-
motopy smoothing for non-smooth problems with lower
complexity than o(1/(cid:15)). In Advances in Neural Informa-
tion Processing Systems (NIPS), pp. 1208–1216, 2016.

Yang, Tianbao and Lin, Qihang. Rsg: Beating subgradi-
ent method without smoothness and strong convexity.
CoRR, abs/1512.03107, 2016.

Yang, Tianbao, Lin, Qihang, and Zhang, Lijun. A richer
theory of convex constrained optimization with reduced
projections and improved rates (the long version of icml
paper). CoRR, arXiv:1608.03487, 2017.

Zhang, Lijun, Yang, Tianbao, Jin, Rong, and He, Xiaofei.
O(logt) projections for stochastic optimization of smooth
In Proceedings of the
and strongly convex functions.
International Conference on Machine Learning (ICML),
pp. 1121–1129, 2013.

Zhou, Zirui and So, Anthony Man-Cho. A uniﬁed approach
to error bounds for structured convex optimization prob-
lems. arXiv:1512.03518, 2015.

Zhou, Zirui, Zhang, Qi, and So, Anthony Man-Cho. L1p-
norm regularization: Error bounds and convergence rate
In Proceedings of the
analysis of ﬁrst-order methods.
32nd International Conference on Machine Learning,
(ICML), pp. 1501–1510, 2015.

