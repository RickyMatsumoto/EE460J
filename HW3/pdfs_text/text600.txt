Supplementary Material for Magnetic Hamiltonian Monte Carlo

1 Section 3 and 4 Proofs

Here we provide proofs for results discussed in Section 3 of the main text regarding non-canonical dynamics.

Lemma 1. The map ΦA

τ,H (θ, p) deﬁned by integrating the non-canonical Hamiltonian system

(cid:21)

(cid:20)θ(t)
p(t)

d
dt

= A∇θ,pH(θ(t), p(t))

with initial conditions (θ, p) for time τ , where A ∈ M2n×2n is any invertible, antisymmetric matrix induces
a ﬂow on the coordinates (θ, p) that is still energy-conserving (∂τ H(ΦA
τ,H (θ, p)) = 0) and symplectic with
respect to A ([∇θ,pΦτ,H (θ, p)]>A−1[∇θ,pΦτ,H (θ, p)] = A−1) which also implies volume-preservation of the
ﬂow.

Proof. The proofs of both results simply uses the antisymmetry of A.

Energy-Conservation – Simply, we have that:

∂τ H(Φτ,H (θ, p)) = ∇θ,pH(Φτ,H (θ, p))∂τ Φτ,H (θ, p) =
∇θ,pH(Φτ,H (θ, p))>A∇θ,pH(Φτ,H (θ, p)) = 0

using the antisymmetry of A and symmetry of ∇θ,pH(Φτ,H (θ, p))∇θ,pH(Φτ,H (θ, p))>.

Symplecticness – We must show that the Jacobian of the ﬂow generated by the dynamics preserves the

non-canonical structure matrix A, which amounts to showing:

[∇θ,pΦτ,H (θ, p)]>
{z
}
|
F (τ )>

A−1 [∇θ,pΦτ,H (θ, p)]
}

|

{z
F (τ )

= A−1

d
dτ

F (τ ) = A∇θ,p∇θ,pH(Φτ,H (θ, p))F (τ )

F (0)A−1F (0) = A−1

where we deﬁne F (τ ) = ∇θ,pΦτ,H (θ, p) as the time-evolving Jacobian of the ﬂow. First, note that F (τ ) can
be equivalently described as the solution to the diﬀerential equation:

with the initial condition F (0) = I2d (the Jacobian for the identity map at t = 0). Trivially, we have:

Then note that:

(F (τ )>A−1F (τ )) =

d
dτ
F (τ )>A−1A∇θ,p∇θ,pH(Φτ,H (θ, p))F (τ ) + F (τ )>∇θ,p∇θ,pH(Φτ,H (θ, p))A>A−1F (τ )
= F (τ )>∇θ,p∇θ,pH(Φτ,H (θ, p))F (τ ) − F (τ )>∇θ,p∇θ,pH(Φτ,H (θ, p))F (τ ) = 0

as desired by simply using A> = −A.

Time-Reversibility – However, crucially it is not the case that the Hamilton equations are time-reversible

in the traditional sense of canonical Hamiltonian dynamics.

1

(1)

(2)

(3)

(4)

(5)

(6)

Lemma 2. If (θ(t), p(t)) is a solution to the non-canonical dynamics:

then (eθ(t), ep(t)) = (θ(−t), −p(−t)) is a solution to the modiﬁed non-canonical dynamics:

(cid:21)

(cid:20)θ(t)
p(t)

d
dt

=

(cid:21)
(cid:20) E
F
−F> G
{z
A

|

}

(cid:20)∇θH(θ(t), p(t))
∇pH(θ(t), p(t))

(cid:21)

d
dt

(cid:21)

(cid:20)
eθ(t)
ep(t)

=

"

(cid:21)
(cid:20) −E
F
−F> −G
{z
eA

|

}

#

H(eθ(t), p(t))
∇
eθ
epH(eθ(t), ep(t))
∇

(7)

(8)

if H(θ, p) = H(θ, −p). In particular if E = G = 0 then A = eA, which reduces to the traditional time-reversal
symmetry of canonical Hamiltonian dynamics.

Proof. A direct calculation yields

(cid:21)

=

(cid:20) ˜θ(t)
˜p(t)

(cid:21)
(cid:20)− d
dt θ(−t)
d
dt p(−t)

d
dt
(cid:20) −E∇˜θH(˜θ(t)) + F∇˜pH(˜θ(t))
−F>∇˜θH(˜θ(t)) − G∇˜pH(˜θ(t))

(cid:20) −E∇θH(θ(−t)) − F∇pH(θ(−t))
−F>∇θH(θ(−t)) + G∇pH(θ(−t))
(cid:20)∇˜θH(˜θ(t))
∇˜pH(˜θ(t))

=

=

(cid:21)

(cid:21)

(cid:21)

=

(cid:20) −E
(cid:21)
F
−F> −G
{z
˜A

|

}

1.1 Non-Canonical Dynamics Variable Augmentation

As remarked in the main text it is necessary to ﬂip the E and G matrices at the end of a deterministic
simulation of the Hamiltonian dynamics in order to render the proposal time-reversible which is in turn
necessary to satisfy detailed balance. This is crucial for the correctness of the algorithm especially when an
approximate simulation of the dynamics is used (as is always often the case).

In particular, say that we wish to use ΦA

τ,H (θ, p) as a transition kernel with ﬁxed, non-zero values of E = E0
and G = G0. We ﬁrst augment the state-space by placing a symmetric, binary distribution independently over
E and G such that p(E = E0) = p(E = −E0) = 1/2 and p(G = G0) = p(G = −G0) = 1/2, independently of
θ, p:

ρ(θ, p, E, G) ∝ e−H(θ,p)p(E)p(G).

(9)

Importantly, this augmentation leaves the distribution over θ, p intact when E and G are marginalized out.
Just as applying the momentum ﬂip operator, Φp : (θ, p, E, G) → (θ, −p, E, G), is a deterministic, energy-
preserving, volume-preserving transformation, the E, G ﬂip operators, ΦE : (θ, p, E, G) → (θ, p, −E, G)
and ΦG : (θ, p, E, G) → (θ, p, E, −G), are also deterministic, energy-preserving, volume-preserving trans-
formations that leave (9) invariant for this particular augmentation with p(E) and p(G). We can now
build a self-inverse operator eΦA
τ,H (θ, p) plus
ΦE ◦ ΦG ◦ Φp, a ﬂip of p, E, G, as:

τ,H (θ, p), composed of simulating the Hamiltonian ﬂow as ΦA

eΦA

τ,H (θ, p) = ΦE ◦ ΦG ◦ Φp ◦ ΦA

τ,H (θ, p)

(10)

Now we have constructed a deterministic, self-inverse map eΦA
proposal for a reversible MCMC algorithm.

τ,H (θ, p). eΦA

τ,H (θ, p) can now be used as the

An important point to note is that our choice of variable augmentation strategy, namely augmenting with
binary distribution, is certainly not unique. However, it is perhaps the most natural and simplest choice
which avoids the repetitive computation of matrix exponentials/diagonalizations since the approximate ﬂow
detailed in Section 2.2 will only need to compute matrix exponentials once upfront for ±G.

2

1.2 Mass Preconditioning Proofs

A common variation on standard HMC dynamics is to set the kinetic energy term in the Hamiltonian H(θ, p)
to 1
2 p>M−1p for some symmetric positive-deﬁnite matrix M, and sample the initial momentum variable
p from the corresponding distribution N (0, M). However, we can contextualize preconditioning using a
non-canonical A matrix in the following manner:

Lemma 3. i) Preconditioned HMC with momentum variable p ∼ N (0, M) in the (θ, p) coordinates, is
exactly equivalent to simulating non-canonical HMC with p0 = M−1/2p ∼ N (0, I) and the non-canonical
matrix:

A =

(cid:20)
0
−(M1/2)>

(cid:21)

M1/2
0

and then transforming back to (θ, p) coordinates using p = M1/2p0. Here M1/2 is a Cholesky factor for M.
ii) On the other hand if we apply a change of basis (via an invertible matrix F) to our coordinates
θ0 = F−1θ, simulate HMC in the (θ0, p) coordinates, and transform back to the original basis using F, this is
exactly equivalent to non-canonical HMC with

Proof. We ﬁrst prove the equivalence regarding the change of basis in momentum space. Under the M mass
matrix variant of HMC, p is drawn from a N (0, M) distribution, and the dynamics of θ, p are then given by

A =

(cid:21)

(cid:20) 0
F
−F> 0

(cid:21)

(cid:20)θ
p

d
dt

=

(cid:21)

(cid:20) M−1p
−∇θU (θ)

Denoting the upper-triangular Cholesky factor of M−1 by M−1/2, and introducing a new variable p0 =
M−1/2p, we obtain the following dynamics for the joint variable (θ, p0):

(cid:21)

(cid:20) θ
p0

d
dt

=

d
dt

(cid:20)
(cid:21)
θ
M−1/2p

=

(cid:20) (M−1/2)>p0
−M−1/2∇θU (θ)

(cid:21)

(cid:20)
0
−M−1/2

=

(M−1/2)>
0

(cid:21)

(cid:21) (cid:20)∇θU (θ)
p0

Further, note that if the marginal distribution of p is N (0, M), then under this change of variables p0 has the
marginal distribution N (0, I). Thus, simulating canonical HMC with a non-identity mass matrix is equivalent
to making a change of basis in momentum space, simulating non-canonical HMC with a particular choice of
non-canonical A matrix, and ﬁnally reverting back to the original basis.

We now prove the equivalence regarding the change of basis in θ space, which follows similarly. Consider
non-canonical HMC on the state-momentum pair (θ, p), with the antisymmetric matrix A taking the particular
form

The states θ, p obtained from this algorithm are equal to those obtained by ﬁrst changing basis to θ0 = F−1θ,
then simulating standard HMC dynamics for the pair (θ0, p) with respect to the Hamiltonian

and then reverting to the original basis as θ = Fθ0. To see this, ﬁrst note that if we denote the distribution
on the coordinates θ corresponding to the potential U by π(θ) = e−U (θ), then the corresponding distribution
on the coordinates θ0 is given by π0, where

The corresponding potential U 0 is therefore given by

A =

(cid:19)

(cid:18) 0

F
−F> 0

H 0(θ0, p) = U 0(θ0) +

p>p

= U (Fθ) +

p>p

1
2
1
2

π0(θ0) = det(F)π(Fθ0)

U 0(θ0) = U (Fθ0)

3

Running canonical HMC dynamics targeting the Hamiltonian H 0 yields the dynamics:

(cid:21)

(cid:20)θ0
p

d
dt

=

(cid:20)
(cid:21)
p
−∇θ0U (θ0)

But note then that the dynamics of the original coordinates are then given by:
(cid:20)θ
(cid:20)
(cid:21)
Fp
−∇θ0U (θ0)
p
(cid:21) (cid:20)∇θU (θ)
(cid:21)
(cid:20) 0
F
−F> 0
p

Fp
−∇θ0U (Fθ)

(cid:20)Fθ0
p

d
dt

d
dt

=

=

=

=

=

(cid:21)

(cid:20)

(cid:21)

(cid:21)

(cid:20)
Fp
−(F>)∇θU (θ)

(cid:21)

which are exactly a special case of non-canonical HMC dynamics described above.

2 Magnetic HMC (MHMC)

Here we provide proofs related to the dynamics of magnetic HMC and it’s symplectic integration scheme.

2.1 Non-Canonical Dynamics and Magnetism

We ﬁrst establish the connection between the particular subcase of non-canonical Hamiltonian dynamics
where

and Newton’s law for a charged particle coupled to a magnetic ﬁeld.

Lemma 4. In 3-dimensions the non-canonical Hamiltonian dynamics, with Hamiltonian H(θ, p) = U (θ) +
1
2 p>p, correspond to simulating the diﬀerential equations:
d
dt

(cid:20)∇θU (θ)
p

(cid:20)∇θH
∇pH

(cid:20)θ
p

(11)

≡

=

(cid:21)

(cid:21)

(cid:21)

(cid:21)

(cid:21)

where

are equivalent to the Newtonian mechanics of a charged particle (with unit mass and charge) coupled to a

magnetic ﬁeld ~B =

 which take the form:







b1
b2
b3

where θ is simply a 3-dimensional vector and × the cross-product.

Proof. If we let θ and p denote our position and momentum coordinates in 3 dimensions then Newton’s law
for a charged particle in a magnetic ﬁeld (with m = q = 1) is:

A =

(cid:21)

(cid:20) 0
I
−I G

(cid:20) 0
I
−I G
{z
A

|

}

(cid:20) 0
I
−I G
{z
A

|

}





G =

0
b3
−b2

−b3
0
b1





b2
−b1
0

d2θ
dt2 = −∇θU (θ) +

dθ
dt

× ~B

d2θ
dt2 = −∇θU (θ) +

dθ
dt

× ~B

4

Deﬁning momentum canonically as dθ
(cid:20)θ
p

(cid:20)
−∇θU (θ) + dθ

d
dt

=

p

(cid:21)

dt = p we have:
(cid:20)

(cid:21)
dt × ~B

=

p
−∇θU (θ) + Gp

(cid:21)

≡

(cid:21)
(cid:20)∇θU (θ)
p

(cid:20) 0
I
−I G
{z
A

|

(cid:21)

}

(12)

(13)

(14)

We now show that the dynamics used in magnetic HMC cannot be reproduced by simply choosing a

diﬀerent smooth Hamiltonian, H 0(θ, p) and using the canonical A matrix:

A =

(cid:21)

(cid:20) 0
I
−I 0

to generate the dynamics.

Lemma 5. The non-canonical Hamiltonian dynamics with magnetic A and Hamiltonian H(θ, p) = U (θ) +
1
2 p>p cannot be obtained using canonical Hamiltonian dynamics for any choice of smooth Hamiltonian.
Proof. Consider the ODEs corresponding to non-canonical dynamics with magnetic A and H(θ, p) =
U (θ) + 1

2 p>p:

(cid:21)

(cid:20)θ
p

d
dt

(cid:20) 0
I
−I G

(cid:21)
(cid:21) (cid:20)∇θU (θ)
p

(cid:20)

=

=

p
−∇θU (θ) + Gp

(cid:21)

.

Assume, to obtain a contradiction, that these canonical Hamiltonian dynamics can be reproduced for some
choice of smooth H 0(θ, p) and canonical A matrix (i.e. E = G = 0, F = I):
(cid:20) 0
I
−I 0

(cid:21)
(cid:21) (cid:20)∇θH 0(θ, p)
∇pH 0(θ, p)

p
−∇θU (θ) + Gp

(cid:20)θ
p

d
dt

(16)

=

=

(cid:21)

(cid:20)

(cid:21)

.

This implies:

(cid:20)∇pH 0(θ, p)
∇θH 0(θ, p)

(cid:21)

(cid:20)

=

p
∇θU (θ) − Gp

(cid:21)

=⇒

(cid:21)
(cid:20)∇θ∇pH 0(θ, p)
∇p∇θH 0(θ, p)

(cid:21)

(cid:20) 0
−G

.

=

However, as long as the 2nd-order mixed partial derivatives are continuous they must be equal; so the
conclusion follows.

2.2 Symplectic Integrator for Magnetic Dynamics

We begin by considering the symmetric splitting:

H(θ, p) = U (θ)/2
| {z }
H1(θ)

+ pT p/2
| {z }
H2(p)

+ U (θ)/2
| {z }
H1(θ)

The corresponding non-canonical dynamics for the sub-Hamiltonians H1(θ) and H2(p) are:

and

(cid:21)

(cid:20)θ
p

d
dt

=

(cid:20) E
F
−F> G
{z
A

|

(cid:21)

}

(cid:20)∇θU (θ)/2
0

(cid:21)

=

(cid:21)
(cid:20) E∇θU (θ)/2
−F>∇θU (θ)/2

(cid:21)

(cid:20)θ
p

d
dt

=

(cid:21)
(cid:20) E
F
−F> G
{z
A

}

|

(cid:21)

(cid:20)0
p

=

(cid:21)

(cid:20) Fp
Gp

.

We denote the corresponding ﬂows by ΦA
(cid:15),H2(p) respectively. The ﬂow in (19) is generally not
explicitly tractable unless we take E = 0 – in which case it is solved by an Euler translation as for standard
Hamiltonian dynamics.

(cid:15),H1(θ) and ΦA

Crucially, the ﬂow in (20) is a linear diﬀerential equation and hence analytically integrable. Without loss
of generality, we restrict ourselves to the case F = I (the case for general F follows similarly). The dynamics
associated with the ﬂow H2(p) introduced in Lemma 4 become
(cid:20) p(t)
Gp(t)

(cid:20)θ(t)
p(t)

=

(cid:21)

(cid:21)

d
dt

5

(15)

(17)

(18)

(19)

(20)

with initial condition (θ0, p0). Using the power series representation of the matrix exponential, the second
diﬀerential equation for p may be integrated analytically to yield the following ﬂow in p-space:

Substituting this result into the diﬀerential equation for θ yields

p(t) = exp(Gt)p0

dθ
dt

= exp(Gt)p0

If G is invertible then once again using the power series representation of the matrix exponential and
rearranging yields the solution

Φ(cid:15),H2(p)

(cid:21)

(cid:20)θ
p

=

(cid:21)
(cid:20)θ + G−1(exp(G(cid:15)) − I)p
exp(G(cid:15))p

If G is not invertible, then slightly more care must be taken to ﬁrst diagonalize G and separate its
invertible/singular components. Since G is strictly antisymmetric it can be written as iH where H is a
Hermitian matrix. Thus it can be diagonalized over C as:

G = (cid:2)UΛ U0

(cid:3)

(cid:20)Λ 0
0 0

(cid:21) (cid:20)U >
Λ
U >
0

(cid:21)

where Λ is a diagonal submatrix consisting of the nonzero eigenvalues of G. (cid:2)UΛ U0
matrices where the columns of UΛ are the eigenvectors of G corresponding to its nonzero eigenvalues while
the columns of U0 are the eigenvectors of G corresponding to its zero eigenvalues. Even if G is not invertible
we still have:

are unitary

(cid:3) and

(cid:21)

(cid:20)U >
Λ
U >
0

However it is more convenient to represent the matrix exponential as:

p(t) = exp(Gt)p0

exp(Gt) = (cid:2)UΛ U0

(cid:3)

(cid:20)exp(Λt) 0
I
0

(cid:21)

(cid:21) (cid:20)U >
Λ
U >
0

Substituting this result into the diﬀerential equation for θ, this representation of exp(Gt) implies the
non-identity block can be handled as in the invertible case while the I block follows trivially to give:

θ(t) = θ0 + (cid:2)UΛ U0

(cid:3)

(cid:20)Λ−1(exp(Λt) − I) 0
tI
0

(cid:21)

(cid:21) (cid:20)U >
Λ
U >
0

p0

Note that if G = 0 then the ﬂow map will simply reduce to an Euler translation as in ordinary HMC. We
can also combine the ideas of Section 1.2 to obtain a preconditioned, magnetic HMC algorithm corresponding
to a general A-matrix of the form

Dealing with a non-zero E becomes more subtle, since the corresponding sub-Hamiltonian is no longer
exactly integrable under the splitting construction. In order to exactly integrate this sub-block a more costly
implicit integrator is needed.

2.3 Integration Error of Magnetic HMC

Since we are using a symmetric, leapfrog splitting scheme for magnetic HMC that exactly integrates each
sub-Hamiltonian we obtain identical error scaling to the leapfrog integrator applied to canonical HMC. Indeed,
symplectic integrators are well-known to have many nice error properties in general and so perhaps this result
is not so surprising [3].

(cid:18) 0

F
−F> G

(cid:19)

6

Lemma 6. The symplectic leapfrog-like integrator for magnetic HMC will have the same local (∼ O((cid:15)3))
and global (∼ O((cid:15)2)) error scaling (over τ ∼ L
(cid:15) steps), as the canonical leapfrog integrator of standard HMC
if the Hamiltonian is separable.

Proof. Note that for the parametrization of A corresponding to magnetic HMC the Hamiltonian vector ﬁeld
~H = ∇pH∇θ + (−∇θ + G∇pH)∇p ≡ ~A + ~B will generate the exact ﬂow corresponding to exactly simulating
the dynamics. We obtain an O((cid:15)3) local error by simply exploiting the separability of the Hamiltonian.
The leapfrog integration scheme splits the Hamiltonian as: H(θ, p) = H1(θ) + H2(p) + H1(θ) and exactly
integrates each sub-Hamiltonian so:

Φfrog

(cid:15),H = Φ(cid:15),H1(θ) ◦ Φ(cid:15),H2(p) ◦ Φ(cid:15),H1(θ) = exp

(cid:17)
~B

(cid:16) (cid:15)
2

◦ exp

(cid:16)

(cid:17)
(cid:15) ~A

◦ exp

(cid:17)
~B

(cid:16) (cid:15)
2

Via repeated applications of the Baker-Campbell-Hausdorﬀ formula [3] obtain:

exp

(cid:17)
~B

◦ exp

(cid:15) ~A +

~B +

[ ~A, ~B]

+ O((cid:15)3) =

(cid:19)

(cid:15)2
2

(cid:16) (cid:15)
2
(cid:18) (cid:15)
2

(cid:18)

(cid:18)

(cid:15)
2

(cid:15)2
4

(cid:15)
2
(cid:15)2
2
(cid:15)2
4

exp

~B + (cid:15) ~A +

~B +

[ ~A, ~B] +

~B, (cid:15) ~A +

~B +

[ ~A, ~B]]

+ O((cid:15)3) =

(cid:15)
2

(cid:15)2
2

(cid:19)

exp

(cid:15) ~H +

[ ~A, ~B] +

[~B, ~A] +

[~B, ~B]

+ O((cid:15)3) = exp

(cid:16)

(cid:17)
(cid:15) ~H

+ O((cid:15)3)

[

(cid:15)
2

1
2
(cid:15)2
8

(cid:19)

where we have used the antisymmetry of the commutator. The global error scaling, for an integration time of
τ = L

(cid:15) follows straightforwardly:

(cid:16)

exp

(cid:17)
~B

(cid:16) (cid:15)
2

◦ exp

(cid:16)

(cid:17)
(cid:15) ~A

◦ exp

(cid:17)(cid:17)L

(cid:16) (cid:15)
2

~B

Φfrog

τ,H =
(cid:16)

=

exp
(cid:16)

= exp

= exp

= exp

(cid:17)

(cid:16)

(cid:15) ~H

(cid:17)
(cid:15)L ~H
(cid:17)
τ ~H
(cid:17)
τ ~H

(cid:16)

(cid:16)

(cid:17)L

+ O((cid:15)3)

+ L(cid:15)O((cid:15)2)

+ τ O((cid:15)2)

+ O((cid:15)2)

(21)

(22)

(23)

(24)

(25)

(26)

(27)

(28)

(29)

(30)

as desired.

3 Section 6 Experimental Details

Here we provide relevant experimental details for some of the Experiments presented in the main text.

In both experiments the reported autocorrelation measures are averaged over all coordinates as well as over
100 independent runs of the HMC/MHMC chains.

3.1 Gaussians

3.1.1

2D Gaussian

For the uncorrelated, ill-conditioned 2D Gaussian experiment presented in the main text the magnetic G
component only has one non-zero parameter which was set to g = .2.

7

3.1.2

10D Gaussian

For the uncorrelated, ill-conditioned 10D Gaussian experiment presented in the main text, the G matrix was
set to encourage the ﬂow of momentum between the directions of large marginal variance with covariance
eigenvalues 106 and the remaining 8 directions of directions of small marginal variance with covariance
eigenvalues of 1. We denote the directions of large marginal variance as x1, x2, and the other 8 directions
of directions of small marginal variance as xi. G was set such that G1i = G2i = g, Gi1 = Gi2 = −g and
G12 = G21 = 0 for g = .2.

3.2 Mixture of Gaussians

The superior mixing of MHMC relative to HMC in this example holds true for a wide range of ((cid:15), L) settings
as we can see by looking at the maximum mean discrepancy as a function of the number of samples in both
Figures 1 and 2.

Figure 1: Left: MMD vs. Number of Samples for ((cid:15) = 1.5, L = 33). The acceptance rate was ∼ .74 for all g.
Right: MMD vs. Number of Samples for ((cid:15) = 1.9, L = 40). The acceptance rate was ∼ .43 for all g = 0 and
∼ .34 for all non-zero g. In both diagrams g denotes the non-zero component of the magnetic ﬁeld.

Figure 2: Left: MMD vs. Number of Samples for ((cid:15) = 1.0, L = 50). The acceptance rate was ∼ .87 for all g.
Right: MMD vs. Number of Samples for ((cid:15) = 0.5, L = 110). The acceptance rate was ∼ .95 for all g. In both
diagrams g denotes the non-zero component of the magnetic ﬁeld.

We found that tuning the parameters ((cid:15), L) via Bayesian optimization often resulted in worse performance
for ordinary HMC since the values found for ((cid:15), L) were too conservative to encourage exploration between
both modes. Moreover, more aggressive choices for ((cid:15), L) for ordinary HMC led to a sharp drop in acceptance
rate and signiﬁcantly worse performance.

8

2000400060008000100001200014000NumberofSamples051015202530MaximumMeanDiscrepancyg=0.0g=0.05g=0.1g=0.152000400060008000100001200014000NumberofSamples051015202530MaximumMeanDiscrepancyg=0.0g=0.05g=0.1g=0.152000400060008000100001200014000NumberofSamples051015202530MaximumMeanDiscrepancyg=0.0g=0.05g=0.1g=0.152000400060008000100001200014000NumberofSamples051015202530MaximumMeanDiscrepancyg=0.0g=0.05g=0.1g=0.153.3 Gaussian Funnel

In this additional experiment, we consider the Gaussian funnel of [4] with density

p(x, v) = Πn

i=1N (xi|0, e−v)N (v|0, 32)

in 10+1 dimensions (i.e. n = 10). This density illustrates the pathological correlation present in many
hierarchical models between x, a vector of low-level parameters, and v, a hyperparameter controlling their
variability. As noted in [1, 5], Riemannian HMC methods, which incorporate local curvature information of
the target, are well-suited to this problem as they help the dynamics traverse the energy surface which rapidly
changes as a v varies. HMC (as well as MHMC) do not exploit curvature information and will have more
diﬃculty exploring the v direction due to the rapid variation in density – see [1] for a detailed discussion.
Despite this diﬃculty, we might intuitively expect that introducing a “curl” term into the entries of G
which couple each xi and v could increase exploration of the dynamics since these variables are nonlinearly
correlated. In order to encourage the periodic ﬂow of momentum between the marginal direction v and the
coordinates xi the G matrix was set such that Gvi = g, Giv = −g, Gij = 0 with g = .2. To investigate this,
we generated 10000 samples from both HMC and MHMC, discarding 1000 burn-in samples and computed
the minimum eﬀective sample size across x and v and bias in the moments of the v parameter similar to the
set-up in [5] for various (cid:15), L (see Table 1). We report results averaged over 100 diﬀerent runs of the Markov
chains.

Table 1: Comparison of HMC and MHMC targeting the Gaussian funnel for a variety of leapfrog steps

algorithm

settings

time (s) min ESS(x, v) min ESS(x, v)/s MSE(E[v], E[v2])

HMC

(cid:15) = 0.05, L = 100

MHMC

(cid:15) = 0.05, L = 100

HMC

(cid:15) = 0.05, L = 300

MHMC

(cid:15) = 0.05, L = 300

225

270

705

837

414, 85

463, 97

1342, 118

1554, 122

1.84, 0.38

1.71, 0.36

1.90, 0.17

1.86, 0.15

.59, 1.57

.29, 1.17

.35, 1.15

.15, 1.05

We ﬁnd that adding the magnetic ﬁeld component decreases the bias in the moments and marginally
increases the ESS, although both samplers struggle to explore the full target density, as the relatively low
ESS ﬁgures indicate. Further details and experiments are provided in the Appendix.

Recall the density of the Gaussian funnel p(x, v) = Πn

i=1N (xi|0, e−v)N (v|0, 32). In order to encourage
the periodic ﬂow of momentum between the marginal direction v and the coordinates xi the G matrix was
set such that Gvi = g, Giv = −g, Gij = 0 with g = .2. Moreover the reported results were averaged over 100
diﬀerent runs of the Markov chains.

4 MHMC Proposals and Dynamics

In this section, we provide illustrations of the proposal distributions of MHMC in simple low-dimensional
settings, to aid intuition and demonstrate the divergence of its behaviour from standard HMC.

4.1 Gaussian Densities

We ﬁrst consider the case of an isotropic Gaussian target, and illustrate the proposal distribution of standard

HMC, as well as MHMC with a variety of settings for the skew-symmetric matrix A =

- see Figure

3. As in previous sections, we denote the oﬀ-diagonal element of the G matrix by g. We also provide proposal
plots for an anisotropic Gaussian target distribution - see Figure 4.

(cid:19)

(cid:18)E F
F G

4.2 Banana Density

We also provide proposal illustrations for the banana density of [2], as shown in Figure 5.

9

(a) Standard HMC (g=0)

(b) MHMC (g=0.5)

(c) MHMC (g=1.0)

(d) MHMC (g=2.0)

(e) MHMC (g=3.0)

(f) MHMC (g=4.0)

Figure 3: HMC and MHMC proposals for an isotropic Gaussian target.

(a) Standard HMC (g=0)

(b) MHMC (g=0.5)

(c) MHMC (g=1.0)

(d) MHMC (g=2.0)

(e) MHMC (g=3.0)

(f) MHMC (g=4.0)

Figure 4: HMC and MHMC proposals for an anisotropic Gaussian target.

10

(a) Standard HMC (g=0)

(b) MHMC (g=0.1)

(c) MHMC (g=0.2)

(d) MHMC (g=0.3)

(e) MHMC (g=0.4)

(f) MHMC (g=0.5)

Figure 5: HMC and MHMC proposals for the banana density target.

11

References

Boca Raton, FL, 2015.

[1] Michael Betancourt and Mark Girolami. Hamiltonian Monte Carlo for Hierarchical Models. CRC Press,

[2] Heikki Haario, Eero Saksman, and Johanna Tamminen. Adaptive proposal distribution for random walk

metropolis algorithm. Computational Statistics, 14(3), 1999.

[3] Ernst Hairer, Marlis Hochbruck, Arieh Iserles, and Christian Lubich. Geometric Numerical Integration.

Oberwolfach Reports, pages 805–882, 2006.

[4] Radford M. Neal. Slice Sampling: Rejoinder, 2003.

[5] Yichuan Zhang and Charles Sutton. Semi-separable Hamiltonian Monte Carlo for Inference in Bayesian

Hierarchical Models. In Advances in Neural Information Processing Systems, pages 10–18, 2014.

12

