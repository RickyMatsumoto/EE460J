1

Interpretation of Laplacian based models as MPNNs

Another family of models deﬁned in Defferrard et al. (2016), Bruna et al. (2013), Kipf
& Welling (2016) can be interpreted as MPNNs. These models generalize the notion of
convolutions a general graph G with N nodes. In the language of MPNNs, these mod-
els tend to have very simple message functions and are typically applied to much larger
graphs such as social network data. We closely follow the notation deﬁned in Bruna
et al. (2013) equation (3.2). The model discussed in Defferrard et al. (2016) (equation
5) and Kipf & Welling (2016) can be viewed as special cases. Given an adjacency ma-
trix W ∈ RN ×N we deﬁne the graph Laplacian to be L = In − D−1/2W D−1/2 where
D is the diagonal degree matrix with Dii = deg(vi). Let V denote the eigenvectors of
L, ordered by eigenvalue. Let σ be a real valued nonlinearity (such as ReLU). We now
deﬁne an operation which transforms an input vector x of size N × d1 to a vector y of
size N × d2 (the full model can deﬁned as stacking these operations).

yj = σ

V Fi,jV T xi

(j = 1 . . . d2)

(1)

(cid:33)

(cid:32) d1(cid:88)

i=1

Here yj and xi are all N dimensional vectors corresponding to a scalar feature at each
node. The matrices Fi,j are all diagonal N × N matrices and contain all of the learned
parameters in the layer. We now expand equation 1 in terms of the full N × d1 vector
x and N × d2 vector y, using v and w to index nodes in the graph G and i, j to
index the dimensions of the node states. In this way xw,i denotes the i’th dimension
of node w, and yv,j denotes the j’th dimension of node v, furthermore we use xw to
denote the d1 dimensional vector for node state w, and yv to denote the d2 dimensional
vector for node v. Deﬁne the rank 4 tensor ˜L of dimension N × N × d1 × d2 where
˜Lv,w,i,j = (V Fi,jV T )v,w. We will use ˜Li,j to denote the N × N dimensional matrix
where ( ˜Li,j)v,w = ˜Lv,w,i,j and ˜Lv,w to denote the d1 × d2 dimensional matrix where
( ˜Lv,w)i,j = ˜Lv,w,i,j. Writing equation 1 in this notation we have

yv,j = σ

˜Lv,w,i,jxw,i

yj = σ

yv = σ

(cid:33)

˜Li,jxi

(cid:32) d1(cid:88)

i=1





d1,N
(cid:88)

i=1,w=1

(cid:33)

˜Lv,wxw

.

(cid:32) N
(cid:88)

w=1





1

Relabelling yv as ht+1
sage passing update in an MPNN where M (ht
σ(mt+1
v

and xw as ht

).

v

w this last line can be interpreted as the mes-
) =

w) = ˜Lv,wht

w and U (ht

v, mt+1
v

v, ht

1.1 The special case of Kipf and Welling (2016)

Motivated as a ﬁrst order approximation of the graph laplacian methods, Kipf & Welling
(2016) propose the following layer-wise propagation rule:

H l+1 = σ

(cid:16) ˜D−1/2 ˜A ˜D−1/2H lW l(cid:17)

(2)

Here ˜A = A + IN where A is the real valued adjacency matrix for an undirected graph
G. Adding the identity matrix IN corresponds to adding self loops to the graph. Also
˜Dii = (cid:80)
˜Aij denotes the degree matrix for the graph with self loops, W l ∈ RD×D
is a layer-speciﬁc trainable weight matrix, and σ(·) denotes a real valued nonlinearity.
Each H l is a RN ×D dimensional matrix indicating the D dimensional node states for
the N nodes in the graph.

j

In what follows, given a matrix M we use M(v) to denote the row in M indexed
by v (v will always correspond to a node in the graph G). Let L = ˜D−1/2 ˜A ˜D−1/2. To
get the updated node state for node v we have

(v) = σ (cid:0)L(v)H lW l(cid:1)
H l+1
(cid:32)
(cid:88)

= σ

LvwH l

(w)W l

(cid:33)

w

Relabelling the row vector H l+1
equation is equivalent to

(v) as an N dimensional column vector ht+1

v

the above

(cid:32)

(cid:33)

ht+1
v = σ

(W l)T (cid:88)

Lvwht
w

w

(3)

which is equivalent to a message function

Mt(ht

v, ht

w) = Lvwht

w = ˜Avw(deg(v)deg(w))−1/2ht
w,

and update function

Ut(ht

v, mt+1
v

) = σ((W t)T mt+1).

Note that the Lvw are all scalar valued, so this model corresponds to taking a certain
weighted average of neighboring nodes at each time step.

2 A More Detailed Description of the Quantum Prop-

erties

First there the four atomization energies.

• Atomization energy at 0K U0 (eV): This is the energy required to break up the
molecule into all of its constituent atoms if the molecule is at absolute zero. This
calculation assumes that the molecules are held at ﬁxed volume.

2

• Atomization energy at room temperature U (eV): Like U0, this is the energy

required to break up the molecule if it is at room temperature.

• Enthalpy of atomization at room temperature H (eV): The enthalpy of atomiza-
tion is similar in spirit to the energy of atomization, U . However, unlike the
energy this calculation assumes that the constituent molecules are held at ﬁxed
pressure.

• Free energy of atomization G (eV): Once again this is similar to U and H, but
assumes that the system is held at ﬁxed temperature and pressure during the
dissociation.

Next there are properties related to fundamental vibrations of the molecule:

• Highest fundamental vibrational frequency ω1 (cm−1): Every molecule has fun-
damental vibrational modes that it can naturally oscillate at. ω1 is the mode that
requires the most energy.

• Zero Point Vibrational Energy (ZPVE) (eV): Even at zero temperature quan-
tum mechanical uncertainty implies that atoms vibrate. This is known as the
zero point vibrational energy and can be calculated once the allowed vibrational
modes of a molecule are known.

Additionally, there are a number of properties that concern the states of the electrons
in the molecule:

• Highest Occupied Molecular Orbital (HOMO) εHOMO (eV): Quantum mechanics
dictates that the allowed states that electrons can occupy in a molecule are dis-
crete. The Pauli exclusion principle states that no two electrons may occupy the
same state. At zero temperature, therefore, electrons stack in states from lowest
energy to highest energy. HOMO is the energy of the highest occupied electronic
state.

• Lowest Unoccupied Molecular Orbital (LUMO) εLUMO (eV): Like HOMO, LUMO

is the lowest energy electronic state that is unoccupied.

• Electron energy gap ∆ε (eV): This is the difference in energy between LUMO
and HOMO. It is the lowest energy transition that can occur when an electron
is excited from an occupied state to an unoccupied state. ∆ε also dictates the
longest wavelength of light that the molecule can absorb.

Finally, there are several measures of the spatial distribution of electrons in the molecule:

• Electronic Spatial Extent (cid:104)R2(cid:105) (Bohr2): The electronic spatial extent is the sec-
ond moment of the charge distribution, ρ(r), or in other words (cid:104)R2(cid:105) = (cid:82) drr2ρ(r).
• Norm of the dipole moment µ (Debye): The dipole moment, p(r) = (cid:82) dr(cid:48)p(r(cid:48))(r−
r(cid:48)), approximates the electric ﬁeld far from a molecule. The norm of the dipole
moment is related to how anisotropically the charge is distributed (and hence the
strength of the ﬁeld far from the molecule). This degree of anisotropy is in turn

3

Table 1: Chemical Accuracy For Each Target
DFT Error Chemical Accuracy
Target
.1
mu
alpha
.4
HOMO 2.0
LUMO 2.6
1.2
gap
-
R2
.0097
ZPVE
.1
U0
.1
U
.1
H
.1
G
.34
Cv
28
Omega

.1
.1
.043
.043
.043
1.2
.0012
.043
.043
.043
.043
.050
10.0

related to a number of material properties (for example hydrogen bonding in wa-
ter causes the dipole moment to be large which has a large effect on dynamics
and surface tension).

• Norm of the static polarizability α (Bohr3): α measures the extent to which
a molecule can spontaneously incur a dipole moment in response to an external
ﬁeld. This is in turn related to the degree to which i.e. Van der Waals interactions
play a role in the dynamics of the medium.

2.1 Chemical Accuracy and DFT Error

In Table 1 we list the mean absolute error numbers for chemical accuracy. These are the
numbers used to compute the error ratios of all models in the tables. The mean absolute
errors of our models can thus be calculated as (Error Ratio) × (Chemical Accuracy).
We also include some estimates of the mean absolute error for the DFT calculation to
the ground truth. Both the estimates of chemical accruacy and DFT error were provided
in Faber et al. (2017).

3 Additional Results

In Table 2 we compare the performance of the best architecture (edge network + set2set
output) on different sized training sets. It is surprising how data efﬁcient this model is
on some targets. For example, on both R2 and Omega our models are equal or better
with 11k samples than the best baseline is with 110k samples.

In Table 3 we compare the performance of several models trained without spatial
information. The left 4 columns show the results of 4 experiments, one where we
train the GG-NN model on the sparse graph, one where we add virtual edges (ve), one
where we add a master node (mn), and one where we change the graph level output

4

Table 2: Results from training the edge network + set2set model on different sized
training sets (N denotes the number of training samples)

Target
N=11k N=35k N=58k N=82k N=110k
mu
1.28
2.76
alpha
HOMO 2.33
LUMO 2.18
3.53
gap
0.28
R2
2.52
ZPVE
1.24
U0
1.05
U
1.14
H
1.23
G
1.99
Cv
0.28
Omega

0.44
1.26
1.34
1.19
2.07
0.21
1.69
0.58
0.60
0.65
0.64
0.93
0.24

0.32
1.09
1.19
1.10
1.84
0.21
1.68
0.62
0.52
0.53
0.49
0.87
0.15

0.55
1.59
1.50
1.47
2.34
0.22
1.78
0.69
0.69
0.64
0.62
1.24
0.25

0.30
0.92
0.99
0.87
1.60
0.15
1.27
0.45
0.45
0.39
0.44
0.80
0.19

to a set2set output (s2s). In general, we ﬁnd that it’s important to allow the model to
capture long range interactions in these graphs.

In Table 4 we compare GG-NN + towers + set2set output (tow8) vs a baseline GG-
NN + set2set output (GG-NN) when distance bins are used. We do this comparison in
both the joint training regime (j) and when training one model per target (i). For joint
training of the baseline we used 100 trials with d = 200 as well as 200 trials where d
was chosen randomly in the set {43, 73, 113, 153}, we report the minimum test error
across all 300 trials. For individual training of the baseline we used 100 trials where
d was chosen uniformly in the range [43, 200]. The towers model was always trained
with d = 200 and k = 8, with 100 tuning trials for joint training and 50 trials for
individual training. The towers model outperforms the baseline model on 12 out of 13
targets in both individual and joint target training.

In Table 5 right 2 columns compare the edge network (enn) with the pair message
network (pm) in the joint training regime (j). The edge network consistently outper-
forms the pair message function across most targets.

In Table 6 we compare our MPNNs with different input featurizations. All models
use the Set2Set output and GRU update functions. The no distance model uses the
matrix multiply message function, the distance models use the edge neural network
message function.

References

Bruna, Joan, Zaremba, Wojciech, Szlam, Arthur, and LeCun, Yann. Spectral networks
and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.

Defferrard, Micha¨el, Bresson, Xavier, and Vandergheynst, Pierre. Convolutional neural

5

Table 3: Comparison of models when distance information is excluded

GG-NN ve
Target
3.94
mu
alpha
2.43
HOMO 1.80
1.73
LUMO
2.48
gap
14.74
R2
5.93
ZPVE
1.98
U0
2.48
U
2.19
H
2.13
G
1.96
Cv
1.28
Omega
3.47
Average

3.76
2.07
1.60
1.48
2.33
17.11
3.21
0.89
0.93
0.86
0.85
1.61
1.05
2.90

mn
4.02
2.01
1.67
1.48
2.23
13.16
3.26
0.90
0.99
0.95
1.02
1.63
0.78
2.62

s2s
3.81
2.04
1.71
1.41
2.26
13.67
3.02
0.72
0.79
0.80
0.74
1.71
0.78
2.57

networks on graphs with fast localized spectral ﬁltering.
Information Processing Systems, pp. 3837–3845, 2016.

In Advances in Neural

Faber, Felix, Hutchison, Luke, Huang, Bing, Gilmer, Justin, Schoenholz, Samuel S.,
Dahl, George E., Vinyals, Oriol, Kearnes, Steven, Riley, Patrick F., and von
Lilienfeld, O. Anatole. Fast machine learning models of electronic and ener-
getic properties consistently reach approximation errors better than dft accuracy.
https://arxiv.org/abs/1702.05532, 2017.

Kipf, T. N. and Welling, M. Semi-Supervised Classiﬁcation with Graph Convolutional

Networks. ArXiv e-prints, September 2016.

6

Table 4: Towers vs Vanilla Model (no explicit hydrogen)

GG-NN-j
Target
2.73
mu
alpha
1.66
HOMO 1.33
1.28
LUMO
1.73
gap
6.07
R2
4.07
ZPVE
1.00
U0
1.01
U
1.01
H
0.99
G
Cv
1.40
0.66
Omega
1.92
Average

tow8-j GG-NN-i
2.47
1.50
1.19
1.12
1.55
6.16
3.43
0.86
0.88
0.88
0.85
1.27
0.62
1.75

2.16
1.47
1.27
1.24
1.78
4.78
2.70
0.71
0.65
0.68
0.66
1.27
0.57
1.53

tow8-i
2.23
1.34
1.20
1.11
1.68
4.11
2.54
0.55
0.52
0.50
0.50
1.09
0.50
1.37

Table 5: Pair Message vs Edge Network in joint training
pm-j
Target
2.10
mu
alpha
2.30
HOMO 1.20
1.46
LUMO
1.80
gap
10.87
R2
16.53
ZPVE
3.16
U0
3.18
U
3.20
H
2.97
G
2.17
Cv
0.83
Omega
3.98
Average

enn-j
2.24
1.48
1.30
1.20
1.75
2.41
3.85
0.92
0.93
0.93
0.92
1.28
0.74
1.53

7

Table 6: Performance With Different Input Information
no distance
dist + exp hydrogen
Target
0.30
3.81
mu
0.92
alpha
2.04
0.99
HOMO 1.71
0.87
1.41
LUMO
1.60
2.26
gap
0.15
13.67
R2
1.27
3.02
ZPVE
0.45
0.72
U0
0.45
0.79
U
0.39
0.80
H
0.44
0.74
G
0.80
1.71
Cv
0.19
0.78
Omega
0.68
2.57
Average

distance
0.95
1.18
1.10
1.06
1.74
0.57
2.57
0.55
0.55
0.59
0.55
0.99
0.41
0.98

8

