Latent Feature Lasso

Ian E.H. Yen 1 Wei-Cheng Lee 2 Sung-En Chang 2 Arun S. Suggala 1 Shou-De Lin 2 Pradeep Ravikumar 1

Abstract

The latent feature model (LFM), proposed in
(Grifﬁths & Ghahramani, 2005), but possibly
with earlier origins, is a generalization of a mix-
ture model, where each instance is generated not
from a single latent class but from a combina-
tion of latent features. Thus, each instance has
an associated latent binary feature incidence vec-
tor indicating the presence or absence of a fea-
ture. Due to its combinatorial nature, inference
of LFMs is considerably intractable, and accord-
ingly, most of the attention has focused on non-
parametric LFMs, with priors such as the Indian
Buffet Process (IBP) on inﬁnite binary matrices.
Recent efforts to tackle this complexity either
still have computational complexity that is expo-
nential, or sample complexity that is high-order
polynomial w.r.t.
the number of latent features.
In this paper, we address this outstanding prob-
lem of tractable estimation of LFMs via a novel
atomic-norm regularization, which gives an algo-
rithm with polynomial run-time and sample com-
plexity without impractical assumptions on the
data distribution.

1. Introduction

Latent variable models are widely used in unsupervised
learning, in part because they provide compact and inter-
pretable representations of the distribution over the ob-
served data. The most common and simplest such latent
variable model is a mixture model, which associates each
observed object with a latent class. However, in many
real-world applications, observations are better described
by a combination of latent features than a single latent
class. Accordingly, admixture or mixed membership mod-
els have been proposed (Airoldi et al., 2014), that in the
simplest settings, assign each object to a convex combi-

1Carnegie Mellon University, U.S.A.

Correspondence to:

2National Taiwan
Ian E.H. Yen

University, Taiwan.
<eyan@cs.cmu.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

nation of latent classes. For instance, a document object
could be modeled as a convex combination of topic ob-
jects. There are many settings however where a convex
combination might be too restrictive, and the objects are
better modeled as simply a collection of latent classes. An
example is web image, which can often described by mul-
tiple tags rather than a single class, or even by a convex
combination of tag objects. Another example is the model
of user, who might have multiple interests in the context of
a recommendation system, or be involved in multiple com-
munities in a social network. With such settings in mind,
(Grifﬁths & Ghahramani, 2005) proposed a latent feature
model (LFM), where each observed object can be repre-
sented by a binary vector that indicates the presence or ab-
sence of each of a collection of latent features. Their pro-
posed model extended earlier models with a similar ﬂavor
for specialized settings, such as (Ueda & Saito, 2003) for
bag of words models for text. The latent feature model
can also be connected to sparse PCA models (d’Aspremont
et al., 2007; Jolliffe et al., 2003) by considering a point-
wise product of the binary feature incidence vector with
another real-valued vector. As (Grifﬁths & Ghahramani,
2005) showed, LFM handily outperforms clustering as an
efﬁcient and interpretable data representation, particularly
in settings where the object can be naturally represented as
a collection of latent features or parts.

However, the estimation (inference) of an LFM from data
is difﬁcult, due to the combinatorial nature of the binary
feature incidence vectors. Indeed, with N samples, and K
latent features, the number of possible binary matrices con-
sisting of the N binary feature incidence vectors is 2N K.
And not in the least, the log-likelihood of LFM is not a
concave function of its parameters.

Given that the ﬁnite feature case seems intractable, right
from the outset, attention has focused on the nonparamet-
ric inﬁnite feature case, where a prior known as the Indian
Buffet Process (IBP) has been proposed for the inﬁnite bi-
nary matrices consisting of the feature incidence vectors
given inﬁnite set of latent features (Grifﬁths & Ghahra-
mani, 2011). While the IBP prior provides useful structure,
inference remains a difﬁcult problem, and in practice, one
often relies on local search methods (Broderick et al., 2013)
to ﬁnd an estimate of parameters, or employ Markov Chain
Monte Carlo (MCMC) (Doshi-Velez & Ghahramani, 2009)

Latent Feature Lasso

or variational methods (Doshi-Velez et al., 2009) to obtain
an approximate posterior distribution. However, none of
these approaches can provide guarantees on the quality of
solution in polynomial time.

Note that both in the mixture model, as well as the admix-
ture model cases, the parametric variants have been hugely
popular alongside or perhaps even more so than the non-
parametric variants e.g. clustering procedures based on ﬁ-
nite number of clusters, or topic models with a ﬁnite num-
ber of topics. This is in part because the parametric variants
have a lower model complexity, which might be desired
under certain settings, and also have simpler inference pro-
cedures. However, in the LFM case, the parametric vari-
ant has received very little attention, which might suggest
the relatively lesser popularity for LFMs when compared
to mixture or admixture/topic models.

Accordingly, in this paper, we consider the question of
computationally tractable estimation of parametric LFMs.
In the nonparametric setting with an IBP prior, (Tung &
Smola, 2014) have proposed the use of spectral methods,
which bypasses the problem of non-concave log-likelihood
by estimating the moments derived from the model, and
then recovers parameters by solving a system of equations.
Their spectral methods based procedure produces consis-
tent estimates of LFMs in polynomial time, however with
a sample complexity that has a high-order (more than six-
order) polynomial dependency on the number of latent fea-
tures and the occurrence probability of each feature. More-
over, the application of spectral methods requires knowl-
edge of the distribution, which results in non-robustness
to model mis-speciﬁcation in practice. Under a noiseless
setting, (Slawski et al., 2013) leveraged identiﬁability con-
ditions under which the solution is unique, to propose an
algorithm for a parametric LFM. Their algorithm is guar-
anteed to recover the parameters in the noiseless setting,
but with the caveat that it has a computational complexity
that is exponential in the number of latent features.

We note that even under the assumption of an nonparamet-
ric LFM, speciﬁcally an Indian Buffet Process with Linear
Gaussian Observations, deriving its MAP point estimate
under low-variance asymptotics following the approach of
MAD-Bayes Asymptotics (Broderick et al., 2013) yields an
objective similar to that of a parametric LFM with an ad-
ditional term that is linear in the number of latent features.
Thus, developing computationally tractable approaches for
parametric LFMs would be broadly useful.

In this work, we propose the Latent Feature Lasso, a novel
convex estimation procedure for the estimation of a Latent
Feature Model using atomic-norm regularization. We con-
struct a greedy algorithm with strong optimization guar-
antees for the estimator by relating each greedy step to a
MAX-CUT like problem. We also provide a risk bound

for the estimator under general data distribution settings,
which trades off between risk and sparsity, and has a sam-
ple complexity linear in the number of components and di-
mension. Under the noiseless setting, we also show that
Latent Feature Lasso estimator recovers the parameters of
LFM under an identiﬁability condition similar to (Slawski
et al., 2013).

2. Problem Setup

A Latent Feature Model represents data as a combination
of latent features. Let x ∈ RD be an observed random
vector that is generated as:

x = W T z + e,

where z ∈ {0, 1}K is a latent binary feature incidence
vector that denotes the presence or absence of K features,
W ∈ RK×D is an unknown matrix of K latent features
of dimension D, and e ∈ RD is an unknown noise vec-
tor. We say that the model is biased when E[e|z] =
E[x|z] − W T z (cid:54)= 0, and which we allow in our analysis.
Suppose we observe N samples of the random vector x. It
will be useful in the sequel to collate the various vectors
corresponding to the N samples into matrices. We collate
the observations into a matrix X ∈ RN ×D, the N latent
incidence vectors into a matrix Z ∈ {0, 1}N ×K, and the
noise vectors into an N × D matrix (cid:15). We thus obtained the
vectorized form of the model as X = ZW + (cid:15).

Most existing works on LFM make two strong assump-
tions. The ﬁrst is that the model has zero bias E[e|z] =
0 (Tung & Smola, 2014; Broderick et al., 2013; Grifﬁths
& Ghahramani, 2011; Slawski et al., 2013; Zoubin, 2013;
Doshi-Velez & Ghahramani, 2009; Doshi-Velez et al.,
2009; Hayashi & Fujimaki, 2013). The second common
but strong class of assumptions is distributional (Hayashi
& Fujimaki, 2013; Tung & Smola, 2014):

p(x|z) = N (W T z, σ2I) , p(z) = Bern(π),

where Bern(π) denotes the distribution of K independent
Bernoulli with zk ∼ Bern(πk).
In the Nonparametric
Bayesian setting (Grifﬁths & Ghahramani, 2011; Zoubin,
2013; Doshi-Velez et al., 2009; Doshi-Velez & Ghahra-
mani, 2009; Broderick et al., 2013), one replaces Bern(π)
with an Indian Buffet Process IBP(α) over the N × K + bi-
nary incidence matrix Z ∈ {0, 1}N ×K+
where K + can be
inferred from data instead of being speciﬁed a-priori. We
note that both classes of assumptions need not hold in prac-
tice: the zero bias assumption E[x|z] = W T z is stringent
given the linearity of the model, while the Bernoulli and
IBP distributional assumptions are also restrictive, in part
since they assume independence between the presence of
two features zik and zik(cid:48). Our method and analyses do not
impose either of these assumptions.

Latent Feature Lasso

It is useful to contrast the different estimation goals rang-
ing over the LFM literature. In the Bayesian approach line
of work (Grifﬁths & Ghahramani, 2011; Broderick et al.,
2013; Zoubin, 2013; Doshi-Velez & Ghahramani, 2009;
Hayashi & Fujimaki, 2013), the goal is to infer the posterior
distribution P (Z, W |X) given X. The line of work using
Spectral Methods (Tung & Smola, 2014) on the other hand
aim to estimate p(z), p(x|z) in turn by estimating parame-
ters (π, W ). In some other work (Slawski et al., 2013), they
aim to estimate W , leaving the distribution of z unmodeled.
In this paper, we focus on the more realistic setting where
we make no assumption on p(x) except that of bounded-
ness, and aim to ﬁnd an LFM W ∗ that minimizes the risk

r(W ) := E[ min

(cid:107)x − W T z(cid:107)2].

(1)

1
2

z∈{0,1}K

where the expectation is over the random observation x.

3. Latent Feature Lasso

We ﬁrst consider the non-convex formulation that was also
previously studied in (Broderick et al., 2013) as asymp-
totics of the MAP estimator of IBP Linear-Gaussian model:

min
K∈N,Z∈{0,1}N ×K ,W ∈RK×D

1
2N

(cid:107)X − ZW (cid:107)2

F + λK. (2)

The estimation problem in (Slawski et al., 2013) could also
be cast in the above form with λ = 0 and K treated as a
ﬁxed hyper-parameter, while (Broderick et al., 2013) treats
K as a variable and controls it through λ. (2) is a combi-
natorial optimization of N × K + 1 integer variables. In
the following we develop a tight convex approximation to
(2) with (cid:96)2 regularization on W , by introducing a type of
atomic norm (Chandrasekaran et al., 2012).

For a ﬁxed K, Z, consider the minimization over W of the
(cid:96)2 regularized version of (2)

min
W ∈RK×D

1
2N

(cid:107)X − ZW (cid:107)2

F +

(cid:107)W (cid:107)2
F ,

(3)

τ
2

which is a convex minimization problem. Applying La-
grangian duality to (3) results in the following dual form

max
A∈RN ×D

(cid:26) −1
2N 2τ

tr(AAT M ) −

L∗(xi, −Ai,:)

. (4)

(cid:27)

1
N

Algorithm 1 A Greedy Algorithm for Latent Feature Lasso

0: A =, c = 0.

for t = 1...T do

Find a greedy atom zzT by solving (8).
Add zzT to an active set A.

1:
2:
3: Minimize (7) w.r.t. coordinates in A via updates (9).
j |cj = 0} from A.
4:

Eliminate {zjzT

end for.

of the objective when optimized over A. The objective in
(2) for a ﬁxed K could thus be simply reformulated as a
minimization of this dual-derived objective g(M ). It can
be seen that g(M ) is a convex function w.r.t. M since it
is the maximum of linear functions of M . The key caveat
however is the combinatorial structure on M since it has
the form M = ZZ T , Z ∈ {0, 1}N ×K. We address this
caveat by introducing the following atomic norm

(cid:107)M (cid:107)S := min
c≥0

ca s.t. M =

caa.

(5)

K
(cid:88)

a∈S

(cid:88)

a∈S

with S := {zzT |z ∈ {0, 1}N }. Note (cid:107)M (cid:107)S = (cid:80)
a∈S ca =
K when ca in (5) are constrained at integer value {0, 1},
and it serves a convex approximation to K similar to the
(cid:96)1-norm used in Lasso for the approximation of cardinality.
This results in the following Latent Feature Lasso estimator

min
M

{g(M ) + λ(cid:107)M (cid:107)S} .

(6)

4. Algorithm

The estimator (6) seems intractable at ﬁrst sight in part
since the atomic norm involves a set S of 2N atoms. In
this section, we study a variant of approximate greedy co-
ordinate descent method for tractably solving problem (6).
We begin by rewriting the optimization problem (6) as an
(cid:96)1-regularized problem with ¯K = 2N − 1 coordinates, by
expanding the matrix M in terms of the ¯K atoms underly-
ing the atomic norm (cid:107).(cid:107)S :



g



¯K
(cid:88)

j=1

min
c∈R ¯K
+






(cid:124)

(cid:124)



(cid:125)

(cid:123)(cid:122)
f (c)

(cid:123)(cid:122)
F (c)






(cid:125)

cjzjzT
j



+λ(cid:107)c(cid:107)1

(7)

where M := ZZ T , A ∈ RN ×D are dual variables that
satisfy W ∗ = 1
N Z ∗A∗ at the optimum of (3) and (4), and
L∗(x, α) = (cid:104)x, α(cid:105) + 1
2 (cid:107)α(cid:107)2 is the convex conjugate of
square loss L(x, ξ) = 1
2 (cid:107)x−ξ(cid:107)2 w.r.t. its second argument.
Let G(M, A) denote the objective in (4) for any ﬁxed M ,
and let g(M ) = maxA G(M, A) denote the optimal value

where {zj} ¯K
j=1 enumerates all possible {0, 1}N patterns
except the 0 vector. Our overall algorithm is depicted in
Algorithm 1. In each iteration, it ﬁnds
j∗ := arg max

−∇jf (x)

= arg max

(cid:104)−∇g(M ), zjzT
j (cid:105)

j

j

(8)

Latent Feature Lasso

approximately with a constant approximation ratio via a
reduction to a MAX-CUT-like problem (see Section 4.1).
An active set A is maintained to contain all atoms zjzT
j
with non-zero coefﬁcients cj and the atom returned by the
greedy search (8). Then we minimize (7) over coordinates
in A by a sequence of proximal updates:

cr+1 ←

(cid:20)
cr −

(cid:21)

∇f (cr) + λ
γ|A|

+

, r = 1...T2

where γ is the Lipschitz-continuous constant of
coordinate-wise gradient ∇cj f (c).

(9)

the

Computing cooordinate-wise gradients. By Danskin’s
Theorem, the gradient of function f (c) takes the form

∇cj f (c) = zjA∗A∗T zj/(2N 2τ ),

(10)

which in turn requires ﬁnding the maximizer A∗ of (4).

Computing A∗. By taking advantage of the strong dual-
ity between (4) and (3), the maximizer A∗ can be found by
ﬁnding the minimizer W ∗ of

min
W

1
2N

(cid:107)X − ZAW (cid:107)2

F +

(cid:107)Wk,:(cid:107)2

(11)

(cid:88)

k∈A

τ
2ck

and computing A∗ = (X − ZAW ∗), where ZA denotes
N × |A| matrix of columns taking from the active atom
basis {zk}k∈A.

Computing W ∗. There is a closed-form solution W ∗ to
(11) of the form

W ∗ = (Z T

AZA + N τ diag−1(cA))−1Z T

AX.

(12)

An efﬁcient way of computing (12) is to maintain Z T
AZA
and Z T
AX whenever the active set of atoms A changes.
This has a cost of O(N DKA) for a bound KA on the active
size, which however is almost neglectable compared to the
other costs when amortized over iterations. Then the eval-
uation of (12) would cost only O(K 3
AD) for each
evaluation of different c. Similarly the matrix computation
of (10) can be made more efﬁcient as ∇cf (c) ∝

A + K 2

diag((Z T

AX − Z T

AZAW ∗)(Z T
can be computed in O(K 2D + K 3) via the maintenance of
Z T

AZAW ∗)T )

AX − Z T

AZA, Z T

AX.

The output of Algorithm 1 is the coefﬁcient vector c, and
with the resulting latent feature matrix W (c) given by
(12). Since the solution could contain many atoms of small
weight ck. In practice, we perform a rounding procedure
that ranks atoms according to the score {ck(cid:107)Wk,:(cid:107)2}k∈A
and then pick top K atoms as the output Z ∗, and solve a
simple least-squares problem to obtain the corresponding
W ∗.

4.1. Greedy Atom Generation

A key step to the greedy algorithm (Algorithm 1) is to ﬁnd
the direction (8) of steepest descent, which however is a
convex maximization problem with binary constraints that
in general cannot be exactly solved in polynomial time.
Fortunately in this section, we show that (8) is equiva-
lent to a MAX-CUT-like Boolean Quadratic Maximiza-
tion problem that has efﬁcient Semideﬁnite relaxation with
constant approximation guarantee. Furthermore, the result-
ing Semideﬁnite Programming (SDP) problem is of special
structure that allows iterative method of complexity linear
to the matrix size (Boumal et al., 2016; Wang & Kolter,
2016).

In particular,
mization problem

let C=∇g(M )=A∗A∗T /(2τ N ) the maxi-

max
z∈{0,1}N

(cid:104)C, zzT (cid:105)

(13)

can be reduced to an optimization problem over variables
taking values in {−1, 1} via the transformation y = 2z − 1,
which results in the problem

max
y∈{−1,1}N

1
4

(cid:0)(cid:104)C, yyT (cid:105) + 2(cid:104)C, 1yT (cid:105) + (cid:104)C, 11T (cid:105)(cid:1) . (14)

where 1 denotes N -dimensional vector of all 1s. By in-
troducing a dummy variable y0, (14) can be expressed as

max
(y0;y)∈{−1,1}N +1

(cid:20) y0
y

1
4

(cid:21)T (cid:20) 1T C1 1T C
C
C1

(cid:21) (cid:20) y0
y

(cid:21)

.

(15)
Note that one can ensure ﬁnding a solution with y0 = 1 by
ﬂipping signs of the solution vector to (15), since this does
not change the quadratic form objective value. Denote the
quadratic form matrix in (15) be ˆC. Problem of form (15)
is a MAXCUT-like Boolean Quadratic problem, for which
there is SDP relaxation of the form

(cid:104) ˆC, Y (cid:105)

max
Y ∈SN
s.t.

Y (cid:23) 0, diag(Y ) = 1

(16)

rounding from which guarantees a solution ˆy to (15) satis-
fying

h − h(ˆy) ≤ ρ(h − h)

(17)

for ρ = 2/5 (Nesterov et al., 1997), where h(y) denotes the
objective function of (15) and h, h denote the maximum,
minimum value achievable by some y ∈ {−1, 1}N +1 re-
spectively. Note this result holds for any symmetric matrix
ˆC. Since our problem has a positive-semideﬁnite matrix ˆC,
h = 0 and thus

−∇ˆjf (c) = h(ˆy) ≥ µh = µ(−∇j∗ f (c))

(18)

Latent Feature Lasso

for µ = 1 − ρ = 3/5, where ˆj is coordinate selected by
rounding from a solution of (16) and j∗ is the exact maxi-
mizer of (8).

Finally, it is noteworthy that, although solving a general
SDP is computationally expensive, SDP of the form (16)
has been shown to allow much faster solver that has linear
the matrix size nnz( ˆC) (Boumal et al., 2016;
cost w.r.t.
Wang & Kolter, 2016). In our implementation we adopt
the method of (Wang & Kolter, 2016) due to its strong em-
pirical performance.

5. Analysis

5.1. Convergence Analysis

The aim of this section is to show the convergence of Al-
gorithm 1 under the approximation of greedy atom gen-
eration. In particular, we show the multiplicative approx-
imation error incurred in the step (8) only contributes an
additive approximation error proportional to λ, as stated in
the following theorem.

Theorem 1. The greedy algorithm proposed (Algorithm 1)
satisﬁes

F (ct) − F (c∗) ≤

2γ(cid:107)c∗(cid:107)2
1
µ2

1
t

+

2(1 − µ)
µ

,

λ(cid:107)c∗(cid:107)1
(cid:125)

(cid:124)

(cid:123)(cid:122)
∆(λ)

where c∗ is any reference solution, µ = 3/5 is the approxi-
mation ratio given by (18) and γ is the Lipschitz-continuous
constant of coordinate-wise gradient ∇jf (c), ∀j ∈ [K].

The theorem thus shows that the iterates converge sub-
linearly to within statistical precision λ of any reference
solution c∗ scaled in main by its (cid:96)1 norm (cid:107)c∗(cid:107)1. In the fol-
lowing theorem, we show that, with the additional assump-
tion that F (c) is strongly convex over a restricted support
set A∗, one can get a bound in terms of the (cid:96)0-norm of a
reference solution c∗ with support A∗.
Theorem 2. Let A∗ ∈ [ ¯K] be a support set and c∗ :=
arg minc:supp(c)=A∗ F (c∗). Suppose F (c) is strongly con-
vex on A∗ with parameter β. The solution given by Algo-
rithm 1 satisﬁes

F (cT ) − F (c∗) ≤

4γ(cid:107)c∗(cid:107)0
βµ2

(cid:18) 1
T

(cid:19)

+

2(1 − µ)λ
µ

2(cid:107)c∗(cid:107)0
β

.

(cid:115)

Let ¯K = 2N be the size of the atomic set. Any target latent
structure Z ∗W ∗ can be expressed as ZD(c∗) ˜W ∗ where Z
is an N × ¯K dictionary matrix, D(c∗) is a ¯K × ¯K diagonal
matrix of diagonal elements Dkk = (cid:112)c∗
k = 1 for
columns corresponding to Z ∗ and c∗
k = 0 for the others,
and ˜W ∗ is W ∗ padded with 0 on rows in {k | ck = 0}.

k with c∗

Then since (cid:107)c∗(cid:107)1 = (cid:107)c∗(cid:107)0 = K ∗, Theorem 2 shows that
our algorithm has an iteration complexity of O(K/(cid:15)) to
achieve (cid:15) error, with an additional error term proportional
to λ

K due to the approximation made in (18).

√

5.2. Risk Analysis

In this section, we investigate the performance of the out-
put from Algorithm 1 in terms of the population risk r(·)
deﬁned in (1). Given coefﬁcients c with support A obtained
from algorithm (1) for T iterations, we construct the weight
matrix by ˆW = diag(
AA∗,
where A∗ is the maximizer of (4) as a function of c. It can
be seen that ˆW satisﬁes

cA)W ∗ with W ∗(cA) = 1

N Z T

√

F (c) =

(cid:107)X − ZA ˆW (cid:107)2 +

(cid:107) ˆW (cid:107)2

F + λ(cid:107)cA(cid:107)1.

(19)

1
2N

τ
2

The following theorem gives a risk bound for ˆW . Without
loss of generality, we assume x is bounded and scaled such
that (cid:107)x(cid:107) ≤ 1.
Theorem 3. Let ˆW = diag(
cA)W ∗(cA) be the weight
matrix obtained from T iterations of Algorithm 1, and ¯W
be the minimizer of the population risk (1) with K compo-
nents and (cid:107) ¯W (cid:107)F ≤ R. We then have the following bound
on population risk: r( ˆW ) ≤ r( ¯W ) + (cid:15) with probability
1 − ρ for

√

T ≥

4γ
µ2β

(

K
(cid:15)

) and N = Ω(

log(

DK
(cid:15)3

RK
(cid:15)ρ

)),

with λ, τ chosen appropriately as functions of N .

Note the output of Algorithm 1 has number of components
ˆK bounded by number of iterations T . Therefore, The-
orem (3) gives us a trade-off between risk and sparsity—
one can guarantee to achieve (cid:15)-suboptimal risk compared
to the optimal solution of size K, via O(K/(cid:15)) compo-
nents and ˜O(DK/(cid:15)3) samples. Notice the result (3) is ob-
tained without any distributional assumption on p(x) and
p(z) except that of boundedness. Comparatively, the the-
oretical result obtained from Spectral Method (Tung &
Smola, 2014) requires the knowledge/assumption of the
distribution p(x|z), p(z), which is sensitive to model mis-
speciﬁcation in practice.

5.3. Identiﬁability

It is noteworthy that the true parameters (Z ∗, W ∗) might
In particular, it is possible to have
not be identiﬁable.
(Z, W ) (cid:54)= (Z ∗, W ∗) with ZW = Z ∗W ∗, in which case
it is impossible to recover the true parameters (Z ∗, W ∗)
from Θ∗ = Z ∗W ∗. The following theorem introduces con-
ditions that ensure uniqueness of the factorization Θ∗ =
Z ∗W ∗.
Theorem 4. Let Θ∗ = Z ∗W ∗ be of rank K. If

Latent Feature Lasso

λ ≤ ¯λ and solve (20) to obtain a solution (c, W ) of (21),
which satisﬁes the following theorem.
Theorem 5. Let (c, W ) be a solution to (21), and
(ZS, WS) be columns of Z and rows of W corresponding
to the set of non-zero indexes S of c respectively. Suppose
the identiﬁability condition in Theorem 4 holds and WS has
full row-rank. Then

{Z:,j}j∈S = {Z ∗

:,j}K

j=1 , {Wj,:}j∈S = {W ∗

j,:}K

j=1

Note since we can choose an arbitrarily small λ ≤ ¯λ to
ﬁnd a solution of (21). The approximation error due to
approximate atom generation can be reduced to arbitrarily
small.

5.5. Parameter Recovery under Noise

In the noisy setting, parameter recovery is more tricky.
When the model is unbiased (i.e. E[x|z] = (W ∗)T z) , by
appealing to well-known results in high-dimensional esti-
mation (Negahban et al., 2009), we can achieve a bound on
the (cid:96)2 norm of the error ˆc−c∗, where c∗ is coefﬁcient vector
corresponding to the ground-truth parameter (W ∗, Z ∗).

√

We defer the resulting Theorem 8 to Section 7.8 in the Ap-
pendix. The theorem bounds the (cid:96)2 error (cid:107)ˆc − c∗(cid:107)2 as
K ∗ ρn, where ρn is a term capturing the noise-
(1/κn)
level, κn is a term capturing the restricted strong convexity
of the objective, and deﬁned in detail in Section 7.8.

However, extending this bound on (cid:107)c−c∗(cid:107) to derive bounds
on (cid:107)Z − Z ∗(cid:107) and (cid:107)W − W ∗(cid:107) is a delicate matter that we
defer to future work, in part due to the exponential size
¯K = 2N of the atomic set, and since the size of Z grows
with N . In particular, in the following theorem, we show
that it is in general not possible to estimate Z ∗ accurately
even with a large number of samples.
Theorem 6. Let K = D = 1. Consider the following
noise model: X = Z ∗W ∗ + E, where Z ∗ ∈ {0, 1}N ,
W ∗ ∈ R and ∀i ∈ [N ], Ei are i.i.d random variables with
Ei ∼ N (0, 1). Moreover, suppose we know the true pa-
rameter W ∗ = 1. Then the Latent Feature Model estimator
for Z ∗ given by:

ˆZ = argmin
Z∈{0,1}N

1
2N

(cid:107)X − ZW ∗(cid:107)2

(22)

satisﬁes the following:

E((cid:107) ˆZ − Z ∗(cid:107)2

2) ≥ cN,

for some positive constant c.

Figure 1: The frequency of failures of condition in Theo-
rem 4 out of 100 trials, for a spectrum of i.i.d. Bernoulli
parameter p and different K. We use algorithm proposed
in (Slawski et al., 2013) to check the condition efﬁciently.

1. Z ∗:N × K and W ∗:K × D are both of rank K.

2. span(Z ∗) ∩ {0, 1}N \ {0} = {Z ∗

:,j}K

j=1.

Then for any rank-K matrices Z:N × K and W :K ×
D, ZW = Θ∗ implies {Z:,j}K
j=1 and
{Wj,:}K

j=1 = {Z ∗

j=1 = {W ∗

:,j}K

j,:}K

j=1.

The conditions in Theorem 4 are similar to that discussed
in (Slawski et al., 2013), where an additional afﬁne con-
straint on W is considered. For random binary matrix of
binary value {−1, +1} instead of {0, 1}, the conditions
are known to hold with high probability when entries are
i.i.d. Bernoulli(0.5) (Tao & Vu, 2007; Kahn et al., 1995).
Here we also conduct numerical experiments for matrices
of i.i.d. Bernoulli(p) with a wide range of p. Results in
Figure 1 shows that the probability with which such condi-
tion fails is almost 0 when p ≥ 0.1, while it increases when
p becomes smaller than 0.1.

5.4. Parameter Recovery without Noise

Let the true parameters be (Z ∗, W ∗) with (cid:107)W ∗(cid:107)2
F = R.
We can ﬁnd some τ (R) such that the estimator (6) is equiv-
alent to solving the following problem:

min

c∈R ¯K

+ ,W ∈R ¯K×D

1
2N

s.t.

(cid:107)W (cid:107)2

F ≤ R.

(cid:107)X − Zdiag(c)W (cid:107)2 + λ(cid:107)c(cid:107)1

where diag(c) is a diagonal matrix with diagkk(c) =
ck.
In the noiseless setting ((cid:15) = 0), one can ﬁnd a feasible
solution to the following problem

min

c∈R ¯K

+ ,W ∈R ¯K×D

(cid:107)c(cid:107)1

(20)

√

(21)

s.t.

ZD(c)W = X, (cid:107)W (cid:107)2

F ≤ R,

6. Experiments

which is equivalent to problem (20) with any λ ≤ ¯λ for
some ¯λ > 0. One can thus choose an arbitrarily small

In this section, we compare our proposed method with
other state-of-the-art approaches on both synthetic and real

10-1100p0102030405060708090100number-of-failuresFail-ProbK=10K=15K=20K=5Latent Feature Lasso

Figure 2: From left to right, each column are results for Syn0 (K=4), Syn2 (K=14), Syn3 (K=35) and Syn1 (K=35)
respectively. The ﬁrst row shows the Hamming loss between the ground-truth binary assignment matrix Z ∗ and the
recovered ones ˆZ. The second row shows RMSE between Θ∗ = Z ∗W ∗ and the estimated ˆΘ = ˆZ ˆW .

Figure 3: From left to right are results for Tabletop, Mnist1k, YaleFace and Yeast, where Spectral Method does not appear
in the plots for YaleFace and Yeast due to a much higher RMSE, and Variational method reports a runtime error when
running on the YaleFace data set.

Table 1: Data statistics.

Dataset
Syn0
Syn1
Syn2
Syn3
Tabletop
Mnist1k
YaleFace
Yeast

N
100
1000
1000
1000
100
1000
165
1500

D
196
1000
900
900
8560
777
2842
104

K
4
35
14
35
4
n/a
n/a
n/a

σ
0
0.01
0.1
0.1
n/a
n/a
n/a
n/a

k,:)

nnz(W ∗
≤ 8
1000
49
36
n/a
n/a
n/a
n/a

data sets. The dataset statistics are listed in Table 1. For
the synthetic data experiments, we used a benchmark sim-
ulated dataset Syn0 that was also used in (Broderick et al.,
2013; Tung & Smola, 2014). But since this has only a
small number of latent features (K = 4), to make the task
more challenging, we created additional synthetic datasets
(which we denote Syn1, Syn2, Syn3) with more latent fea-

tures. Figure 4 shows example of our synthetic data, where
we reshape dimension D into an image and pick a con-
tiguous region. Each pixel W (k, j) in the region is set as
N (0, σ2), while pixels not in the region are set to 0.
In
the examples of Figure 4, the region has size nnz(W (k, :
))=36. Note the problem becomes harder when the region
size nnz(W (k, :)), number of features K, or noise level σ
becomes larger. For real data, we use a benchmark Table-
top data set constructed by (Grifﬁths & Ghahramani, 2005),
where there is a ground-truth number of features K = 4 for
the 4 objects on the table. We also take two standard multi-
label (multiclass) classiﬁcation data sets Yeast and Mnist1k
from the LIBSVM repository 1, and one Face data set Yale-
Face from the Yale Face database 2.

Given the estimated factorization (Z, W ), we use the fol-
lowing 3 evaluation metrics to compare different algo-
rithms:

1https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
2http://vision.ucsd.edu/content/yale-face-database

0246810K00.050.10.150.20.250.30.350.4Hamming-ErrSyn0-Hamming05101520K00.050.10.150.20.250.30.350.40.450.5Hamming-ErrSyn2-Hamming01020304050K00.050.10.150.20.250.30.350.40.45Hamming-ErrSyn3-Hamming01020304050K00.050.10.150.20.250.30.350.40.45Hamming-ErrSyn1-Hamming0246810K0.050.10.150.20.25RMSESyn0-RMSE05101520K0.050.10.150.20.25RMSESyn2-RMSE01020304050K0.050.10.150.20.250.30.35RMSESyn3-RMSE01020304050K00.511.522.533.5RMSESyn1-RMSEBP-MeansLatentLassoMCMCMF-BinarySpectralVariational0246810K0.0250.030.0350.040.0450.050.0550.060.065RMSETabletop-RMSEnoisy01020304050K0.120.140.160.180.20.220.240.260.28RMSEMnist1k-RMSEnoisy01020304050K0.090.10.110.120.130.140.150.160.170.18RMSEYaleFace-RMSEnoisy01020304050K0.0550.060.0650.070.0750.080.0850.090.095RMSEYeast-RMSEnoisyBP-MeansLatentLassoMCMCMF-BinarySpectralVariationalLatent Feature Lasso

(e.g. Syn0), most of methods perform reasonably well.
However, when the number of features becomes slightly
larger (i.e. K = 35 in Syn1, Syn3), most of algorithms
lose their ability of recovering the hidden structure, and
when they fail to do so, they can hardly ﬁnd a good ap-
proximation to Θ∗ = Z ∗W ∗ even using a much larger
number of components up to 50. We found the proposed
LatentLasso method turns out to be the only method that
can still recover the desired hidden structure on the Syn1
and Syn3 data sets, which gives 0 RMSE and Hamming
Error. On Syn2 (K = 14) data set, MF-Binary and La-
tentLasso are the only two methods that achieve 0 RMSE
and Hamming-Error. However, MF-Binary has a complex-
ity growing exponential with K, which results in its failure
on Syn1 and Syn3 due to a running time more than one
day when K > 30. The proposed LatentLasso algorithm
actually runs signiﬁcantly faster than other methods in our
experiments. For example, on the Syn1 dataset (N=1000,
D=1000, K=35), the runtime of LatentLasso is 398s, while
MCMC, Variational, MF-Binary and BP-Means all take
more than 10000s to obtain their best results reported in
the Figures. We provide a comparison of the time com-
plexities of all compared methods in Section 7.1 in the Ap-
pendix. Our overall lower time complexity is also corrobo-
rated empirically by our experiments. We also observe that
LatentLasso is the only method that has RMSE and Ham-
ming error monotonically decreasing with K. On Syn0 and
Tabletop which have ground-truth K = 4, we found most
of algorithms could become unstable when trying to use
a number of components K larger than the ground truth.
Among all algorithms, Spectral, Variational methods are
the most unstable, while BP-Means and MCMC are more
stable possibly due to the large number of random re-trials
employed in their procedures.

On real data sets, the LFM model assumption might not
hold, and might serve at best as an approximation to the
ground-truth. Even in such cases, we found that our La-
tentLasso method ﬁnds a better approximation than other
existing approaches, especially when using a larger num-
ber of components K. We conjecture that for local search
methods, the performance breakdown for larger K is pos-
sibly due to an exponentially increased number of local
optimums, which makes strategies such as random restarts
less effective for methods such as BP-Means and MCMC.
On the other hand, the Spectral Method simply has a sam-
ple complexity bound with a high-order polynomial depen-
dency on K, which makes the estimation error increase dra-
matically as K becomes larger.

Figure 4: Sample images from the synthetic data we cre-
ated (i.e. Syn1, Syn2, Syn3). The ﬁrst row shows observa-
tions Xi,:, and the second row shows latent features Wk,:.

• Hamming-Error: minS:|S|=K

(cid:107)Z:,S −Z∗(cid:107)2
F
N K

.

• RMSE: (cid:107)Z∗W ∗−ZW (cid:107)F

√

.

N D

• RMSEnoisy: (cid:107)X−ZW (cid:107)F

√

.

N D

where the ﬁrst two can only be applied when the ground
truth Z ∗ are W ∗ are given. For real data, we can only eval-
uate the noisy version of RMSE, which can be interpreted
as trying to ﬁnd a best approximation to the observation X
via a factorization with binary components.

(a)
The methods in comparison are listed as follows:
MCMC: An accelerated version of the Collapsed Gibbs
sampler for the Indian Buffet Process (IBP) model (Doshi-
Velez & Ghahramani, 2009). We adopted the implemen-
tation published by 3. We ran it with 25 random restarts
and recorded the best results for each K. (b) Variational:
A Variational approximate inference method for IBP pro-
posed in (Doshi-Velez et al., 2009). We used implementa-
tion published by the author 4. (c) MF-Binary: A Matrix
Factorization with the Binary Components model (Slawski
et al., 2013), which has recovery guarantees in the noise-
less case but has a O(K2K) complexity and thus cannot
scale to K > 30 on our machine. We use the implemen-
tation published by the author 5. (d) BP-Means: A local
search method that optimizes a MAD-Bayes Latent Fea-
ture objective function (Broderick et al., 2013). We used
code provided by the author 6. We ran it with 100 random
restarts and recorded the best result. (e) Spectral: Spectral
Method for IBP Linear Gaussian model proposed in (Tung
& Smola, 2014). We used code from the author. The im-
plementation has a memory requirement that restricts its
use to K < 14. (f) LatentLasso: The proposed Latent
Feature Lasso method (Algorithm 1).

The results are shown in Figure 2 and 3. On synthetic data,
we observe that, when the number of features K is small

3https://github.com/davidandrzej/PyIBP
4http://mloss.org/software/view/185/
5https://sites.google.com/site/

slawskimartin/code

6https://github.com/tbroderick/bp-means

Latent Feature Lasso

Acknowledgements P.R. acknowledges the support of
ARO via W911NF-12-1-0390 and NSF via IIS-1149803,
IIS-1447574, and DMS-1264033, and NIH via R01
GM117594-01. S.D. Lin acknowledges the support of
AOARD and MOST via 104-2628-E-002-015-MY3.

References

Airoldi, Edoardo M, Blei, David, Erosheva, Elena A, and
Fienberg, Stephen E. Handbook of Mixed Member-
ship Models and Their Applications. Chapman and
Hall/CRC, 2014.

Boumal, Nicolas, Voroninski, Vlad, and Bandeira, Afonso.
The non-convex burer-monteiro approach works on
In Advances in Neural
smooth semideﬁnite programs.
Information Processing Systems, pp. 2757–2765, 2016.

Broderick, Tamara, Kulis, Brian, and Jordan, Michael I.
Mad-bayes: Map-based asymptotic derivations from
bayes. In ICML (3), pp. 226–234, 2013.

Chandrasekaran, Venkat, Recht, Benjamin, Parrilo,
Pablo A, and Willsky, Alan S. The convex geometry of
linear inverse problems. Foundations of Computational
mathematics, 12(6):805–849, 2012.

d’Aspremont, Alexandre, El Ghaoui, Laurent, Jordan,
Michael I, and Lanckriet, Gert RG. A direct formulation
for sparse pca using semideﬁnite programming. SIAM
review, 49(3):434–448, 2007.

Doshi-Velez, Finale and Ghahramani, Zoubin. Accelerated
sampling for the indian buffet process. In Proceedings
of the 26th annual international conference on machine
learning, pp. 273–280. ACM, 2009.

Doshi-Velez, Finale, Miller, Kurt T, Van Gael, Jurgen, Teh,
Yee Whye, and Unit, Gatsby. Variational inference for
In Proceedings of the Intl.
the indian buffet process.
Conf. on Artiﬁcial Intelligence and Statistics, volume 12,
pp. 137–144, 2009.

Grifﬁths, Thomas L and Ghahramani, Zoubin. Inﬁnite la-
tent feature models and the indian buffet process.
In
NIPS, volume 18, pp. 475–482, 2005.

Grifﬁths, Thomas L and Ghahramani, Zoubin. The indian
buffet process: An introduction and review. Journal of
Machine Learning Research, 12(Apr):1185–1224, 2011.

Hayashi, Kohei and Fujimaki, Ryohei. Factorized asymp-
totic bayesian inference for latent feature models. In Ad-
vances in Neural Information Processing Systems, pp.
1214–1222, 2013.

Jolliffe, Ian T, Trendaﬁlov, Nickolay T, and Uddin, Mu-
dassir. A modiﬁed principal component technique based

on the lasso. Journal of computational and Graphical
Statistics, 12(3):531–547, 2003.

Kahn, Jeff, Koml´os, J´anos, and Szemer´edi, Endre. On the
probability that a random±1-matrix is singular. Journal
of the American Mathematical Society, 8(1):223–240,
1995.

Negahban, Sahand, Yu, Bin, Wainwright, Martin J, and
Ravikumar, Pradeep K. A uniﬁed framework for high-
dimensional analysis of m-estimators with decompos-
In Advances in Neural Information
able regularizers.
Processing Systems, pp. 1348–1356, 2009.

Nesterov, Yurii et al.

Quality of semideﬁnite relax-
ation for nonconvex quadratic optimization. Universit´e
Catholique de Louvain. Center for Operations Research
and Econometrics [CORE], 1997.

Shaban, Amirreza, Farajtabar, Mehrdad, Xie, Bo, Song, Le,
and Boots, Byron. Learning latent variable models by
improving spectral solutions with exterior point method.
In UAI, pp. 792–801, 2015.

Slawski, Martin, Hein, Matthias, and Lutsik, Pavlo. Matrix
In Advances in
factorization with binary components.
Neural Information Processing Systems, pp. 3210–3218,
2013.

Tao, Terence and Vu, Van. On the singularity probability
of random bernoulli matrices. Journal of the American
Mathematical Society, 20(3):603–628, 2007.

Tung, Hsiao-Yu and Smola, Alexander J. Spectral meth-
ods for indian buffet process inference. In Advances in
Neural Information Processing Systems, pp. 1484–1492,
2014.

Ueda, Naonori and Saito, Kazumi. Parametric mixture
models for multi-labeled text. Advances in neural in-
formation processing systems, pp. 737–744, 2003.

Wang, Po-Wei and Kolter, J Zico. The mixing method for
maxcut-sdp problem. NIPS LHDS Workshop., 2016.

Wu, Lingfei, Yen, Ian EH, Chen, Jie, and Yan, Rui. Re-
visiting random binning features: Fast convergence and
strong parallelizability. In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, pp. 1265–1274. ACM, 2016.

Zhao, Han and Poupart, Pascal. A sober look at spectral

learning. arXiv preprint arXiv:1406.4631, 2014.

Zoubin, Ghahramani. Scaling the indian buffet process via
In Proceedings of the 30th
submodular maximization.
International Conference on Machine Learning (ICML-
13), pp. 1013–1021, 2013.

