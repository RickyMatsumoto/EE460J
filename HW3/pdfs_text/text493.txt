On the Expressive Power of Deep Neural Networks

Appendix

Here we include the full proofs from sections in the paper.

A. Proofs and additional results from Section 2.1

Proof of Theorem 2

Proof. We show inductively that FA(x; W ) partitions the input space into convex polytopes via hyperplanes. Consider
the image of the input space under the ﬁrst hidden layer. Each neuron v(1)
deﬁnes hyperplane(s) on the input space:
letting W (0)
the bias, we have the hyperplane W (0)
i x + bi = 0 for a ReLU and hyperplanes
W (0)
i x + bi = ±1 for a hard-tanh. Considering all such hyperplanes over neurons in the ﬁrst layer, we get a hyperplane
arrangement in the input space, each polytope corresponding to a speciﬁc activation pattern in the ﬁrst hidden layer.

be the ith row of W (0), b(0)

i

i

i

Now, assume we have partitioned our input space into convex polytopes with hyperplanes from layers ≤ d − 1. Consider
v(d)
and a speciﬁc polytope Ri. Then the activation pattern on layers ≤ d − 1 is constant on Ri, and so the input to v(d)
i
on Ri is a linear function of the inputs (cid:80)
j λjxj + b and some constant term, comprising of the bias and the output of
saturated units. Setting this expression to zero (for ReLUs) or to ±1 (for hard-tanh) again gives a hyperplane equation,
but this time, the equation is only valid in Ri (as we get a different linear function of the inputs in a different region.) So
the deﬁned hyperplane(s) either partition Ri (if they intersect Ri) or the output pattern of v(d)
is also constant on Ri. The
theorem then follows.

i

i

This implies that any one dimensional trajectory x(t), that does not ‘double back’ on itself (i.e. reenter a polytope it has
previously passed through), will not repeat activation patterns. In particular, after seeing a transition (crossing a hyperplane
to a different region in input space) we will never return to the region we left. A simple example of such a trajectory is a
straight line:

Corollary 1. Transitions and Output Patterns in an Afﬁne Trajectory For any afﬁne one dimensional trajectory x(t) =
x0 + t(x1 − x0) input into a neural network FW , we partition R (cid:51) t into intervals every time a neuron transitions. Every
interval has a unique network activation pattern on FW .

Generalizing from a one dimensional trajectory, we can ask how many regions are achieved over the entire input – i.e. how
many distinct activation patterns are seen? We ﬁrst prove a bound on the number of regions formed by k hyperplanes in
Rm (in a purely elementary fashion, unlike the proof presented in (Stanley, 2011))
Theorem 5. Upper Bound on Regions in a Hyperplane Arrangement Suppose we have k hyperplanes in Rm - i.e. k
equations of form αix = βi. for αi ∈ Rm, βi ∈ R. Let the number of regions (connected open sets bounded on some sides
by the hyperplanes) be r(k, m). Then

r(k, m) ≤

m
(cid:88)

i=0

(cid:19)

(cid:18)k
i

Proof of Theorem 5

Proof. Let the hyperplane arrangement be denoted H, and let H ∈ H be one speciﬁc hyperplane. Then the number of
regions in H is precisely the number of regions in H − H plus the number of regions in H ∩ H. (This follows from the
fact that H subdivides into two regions exactly all of the regions in H ∩ H, and does not affect any of the other regions.)

In particular, we have the recursive formula

r(k, m) = r(k − 1, m) + r(k − 1, m − 1)

We now induct on k + m to assert the claim. The base cases of r(1, 0) = r(0, 1) = 1 are trivial, and assuming the claim

On the Expressive Power of Deep Neural Networks

for ≤ k + m − 1 as the induction hypothesis, we have

r(k − 1, m) + r(k − 1, m − 1) ≤

m
(cid:88)

(cid:19)

(cid:18)k − 1
i

+

m−1
(cid:88)

(cid:19)

(cid:18)k − 1
i

i=0
(cid:18)k − 1
0

(cid:19)

+

d−1
(cid:88)

i=0
(cid:18)k − 1
i

(cid:19)

+

(cid:19)

(cid:18)k − 1
i + 1

≤

≤

(cid:19)

(cid:18)k
0

+

m−1
(cid:88)

i=0

i=0
(cid:18) k

(cid:19)

i + 1

where the last equality follows by the well known identity

(cid:19)

(cid:18)a
b

+

(cid:18) a

(cid:19)

b + 1

=

(cid:19)

(cid:18)a + 1
b + 1

This concludes the proof.

Proof of Theorem 1

With this result, we can easily prove Theorem 1 as follows:

Proof. First consider the ReLU case. Each neuron has one hyperplane associated with it, and so by Theorem 5, the ﬁrst
hidden layer divides up the inputs space into r(k, m) regions, with r(k, m) ≤ O(km).

Now consider the second hidden layer. For every region in the ﬁrst hidden layer, there is a different activation pattern in
the ﬁrst layer, and so (as described in the proof of Theorem 2) a different hyperplane arrangement of k hyperplanes in an
m dimensional space, contributing at most r(k, m) regions.

In particular, the total number of regions in input space as a result of the ﬁrst and second hidden layers is ≤ r(k, m) ∗
r(k, m) ≤ O(k2m). Continuing in this way for each of the n hidden layers gives the O(kmn) bound.

A very similar method works for hard tanh, but here each neuron produces two hyperplanes, resulting in a bound of
O((2k)mn).

B. Proofs and additional results from Section 2.2

Proof of Theorem 3

B.1. Notation and Preliminary Results

Difference of points on trajectory Given x(t) = x, x(t + dt) = x + δx in the trajectory, let δz(d) = z(d)(x + δx) − z(d)(x)

Parallel and Perpendicular Components: Given vectors x, y, we can write y = y⊥ + y(cid:107) where y⊥ is the component of y
perpendicular to x, and y(cid:107) is the component parallel to x. (Strictly speaking, these components should also have a subscript
x, but we suppress it as the direction with respect to which parallel and perpendicular components are being taken will be
explicitly stated.)

This notation can also be used with a matrix W , see Lemma 1.

Before stating and proving the main theorem, we need a few preliminary results.
Lemma 1. Matrix Decomposition Let x, y ∈ Rk be ﬁxed non-zero vectors, and let W be a (full rank) matrix. Then, we
can write

W = (cid:107)W(cid:107) + (cid:107)W⊥ + ⊥W(cid:107) + ⊥W⊥

such that

On the Expressive Power of Deep Neural Networks

(cid:107)W⊥x = 0
yT ⊥W(cid:107) = 0

⊥W⊥x = 0
yT ⊥W⊥ = 0

i.e. the row space of W is decomposed to perpendicular and parallel components with respect to x (subscript on right),
and the column space is decomposed to perpendicular and parallel components of y (superscript on left).

Proof. Let V, U be rotations such that V x = (||x|| , 0..., 0)T and U y = (||y|| , 0...0)T . Now let ˜W = U W V T , and let
˜W = (cid:107) ˜W(cid:107) + (cid:107) ˜W⊥ + ⊥ ˜W(cid:107) + ⊥ ˜W⊥, with (cid:107) ˜W(cid:107) having non-zero term exactly ˜W11, (cid:107) ˜W⊥ having non-zero entries exactly
˜W1i for 2 ≤ i ≤ k. Finally, we let ⊥ ˜W(cid:107) have non-zero entries exactly ˜Wi1, with 2 ≤ i ≤ k and ⊥ ˜W⊥ have the remaining
entries non-zero.

If we deﬁne ˜x = V x and ˜y = U y, then we see that

(cid:107) ˜W⊥ ˜x = 0
˜yT ⊥ ˜W(cid:107) = 0

⊥ ˜W⊥ ˜x = 0
˜yT ⊥ ˜W⊥ = 0

as ˜x, ˜y have only one non-zero term, which does not correspond to a non-zero term in the components of ˜W in the equations.
Then, deﬁning (cid:107)W(cid:107) = U T (cid:107) ˜W(cid:107)V , and the other components analogously, we get equations of the form

(cid:107)W⊥x = U T (cid:107) ˜W⊥V x = U T (cid:107) ˜W⊥ ˜x = 0

Observation 1. Given W, x as before, and considering W(cid:107), W⊥ with respect to x (wlog a unit vector) we can express
them directly in terms of W as follows: Letting W (i) be the ith row of W , we have

i.e. the projection of each row in the direction of x. And of course

W(cid:107) =






((W (0))T · x)x
...
((W (k))T · x)x






W⊥ = W − W(cid:107)

The motivation to consider such a decomposition of W is for the resulting independence between different components, as
shown in the following lemma.

Lemma 2. Independence of Projections Let x be a given vector (wlog of unit norm.)
Wij ∼ N (0, σ2), then W(cid:107) and W⊥ with respect to x are independent random variables.

If W is a random matrix with

Proof. There are two possible proof methods:

(a) We use the rotational invariance of random Gaussian matrices, i.e. if W is a Gaussian matrix, iid entries N (0, σ2),
and R is a rotation, then RW is also iid Gaussian, entries N (0, σ2). (This follows easily from afﬁne transformation
rules for multivariate Gaussians.)
Let V be a rotation as in Lemma 1. Then ˜W = W V T is also iid Gaussian, and furthermore, ˜W(cid:107) and ˜W⊥ partition
the entries of ˜W , so are evidently independent. But then W(cid:107) = ˜W(cid:107)V T and W⊥ = ˜W⊥V T are also independent.

(b) From the observation note that W(cid:107) and W⊥ have a centered multivariate joint Gaussian distribution (both consist of
linear combinations of the entries Wij in W .) So it sufﬁces to show that W(cid:107) and W⊥ have covariance 0. Because
both are centered Gaussians, this is equivalent to showing E(< W(cid:107), W⊥ >) = 0. We have that

E(< W(cid:107), W⊥ >) = E(W(cid:107)W T

⊥ ) = E(W(cid:107)W T ) − E(W(cid:107)W T
(cid:107) )

On the Expressive Power of Deep Neural Networks

As any two rows of W are independent, we see from the observation that E(W(cid:107)W T ) is a diagonal matrix, with the
ith diagonal entry just ((W (0))T · x)2. But similarly, E(W(cid:107)W T
(cid:107) ) is also a diagonal matrix, with the same diagonal
entries - so the claim follows.

In the following two lemmas, we use the rotational invariance of Gaussians as well as the chi distribution to prove results
about the expected norm of a random Gaussian vector.
Lemma 3. Norm of a Gaussian vector Let X ∈ Rk be a random Gaussian vector, with Xi iid, ∼ N (0, σ2). Then

E [||X||] = σ

√

2

Γ((k + 1)/2)
Γ(k/2)

Proof. We use the fact that if Y is a random Gaussian, and Yi ∼ N (0, 1) then ||Y || follows a chi distribution. This means
that E(||X/σ||) =
2Γ((k + 1)/2)/Γ(k/2), the mean of a chi distribution with k degrees of freedom, and the result
follows by noting that the expectation in the lemma is σ multiplied by the above expectation.

√

We will ﬁnd it useful to bound ratios of the Gamma function (as appear in Lemma 3) and so introduce the following
inequality, from (Kershaw, 1983) that provides an extension of Gautschi’s Inequality.

Theorem 6. An Extension of Gautschi’s Inequality For 0 < s < 1, we have

(cid:17)1−s

(cid:16)

x +

s
2

≤

Γ(x + 1)
Γ(x + s)

(cid:32)

(cid:18)

2 (cid:33)1−s

(cid:19) 1

≤

x −

+

s +

1
2

1
4

We now show:
Lemma 4. Norm of Projections Let W be a k by k random Gaussian matrix with iid entries ∼ N (0, σ2), and x, y two
given vectors. Partition W into components as in Lemma 1 and let x⊥ be a nonzero vector perpendicular to x. Then

(a)

E (cid:2)(cid:12)
(cid:12)

(cid:12)
(cid:12)⊥W⊥x⊥

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:3) = ||x⊥|| σ

√

2

Γ(k/2)
Γ((k − 1)/2

≥ ||x⊥|| σ

2

(cid:19)1/2

√

(cid:18) k
2

−

3
4

(b) If 1A is an identity matrix with non-zeros diagonal entry i iff i ∈ A ⊂ [k], and |A| > 2, then

E (cid:2)(cid:12)
(cid:12)

(cid:12)
(cid:12)1A

⊥W⊥x⊥

(cid:12)
(cid:12)

(cid:12)
(cid:3) ≥ ||x⊥|| σ
(cid:12)

√

2

Γ(|A|/2)
Γ((|A| − 1)/2)

≥ ||x⊥|| σ

2

√

(cid:18) |A|
2

−

3
4

(cid:19)1/2

Proof.

(a) Let U, V, ˜W be as in Lemma 1. As U, V are rotations, ˜W is also iid Gaussian. Furthermore for any ﬁxed W ,

with ˜a = V a, by taking inner products, and square-rooting, we see that

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
˜W ˜a
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) = ||W a||. So in particular

E (cid:2)(cid:12)
(cid:12)

(cid:12)
(cid:12)⊥W⊥x⊥

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:3) = E

⊥ ˜W⊥ ˜x⊥

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

But from the deﬁnition of non-zero entries of ⊥ ˜W⊥, and the form of ˜x⊥ (a zero entry in the ﬁrst coordinate), it follows
that ⊥ ˜W⊥ ˜x⊥ has exactly k − 1 non zero entries, each a centered Gaussian with variance (k − 1)σ2 ||x⊥||2. By Lemma
3, the expected norm is as in the statement. We then apply Theorem 6 to get the lower bound.

(b) First note we can view 1A

⊥W⊥ = ⊥1AW⊥. (Projecting down to a random (as W is random) subspace of ﬁxed size
|A| = m and then making perpendicular commutes with making perpendicular and then projecting everything down
to the subspace.)

So we can view W as a random m by k matrix, and for x, y as in Lemma 1 (with y projected down onto m dimensions),
we can again deﬁne U, V as k by k and m by m rotation matrices respectively, and ˜W = U W V T , with analogous

On the Expressive Power of Deep Neural Networks

properties to Lemma 1. Now we can ﬁnish as in part (a), except that ⊥ ˜W⊥ ˜x may have only m − 1 entries, (depending
on whether y is annihilated by projecting down by1A) each of variance (k − 1)σ2 ||x⊥||2.

Lemma 5. Norm and Translation Let X be a centered multivariate Gaussian, with diagonal covariance matrix, and µ a
constant vector.

E(||X − µ||) ≥ E(||X||)

Proof. The inequality can be seen intuitively geometrically: as X has diagonal covariance matrix, the contours of the pdf
of ||X|| are circular centered at 0, decreasing radially. However, the contours of the pdf of ||X − µ|| are shifted to be
centered around ||µ||, and so shifting back µ to 0 reduces the norm.

A more formal proof can be seen as follows: let the pdf of X be fX (·). Then we wish to show

(cid:90)

x

||x − µ|| fX (x)dx ≥

||x|| fX (x)dx

(cid:90)

x

Now we can pair points x, −x, using the fact that fX (x) = fX (−x) and the triangle inequality on the integrand to get

(cid:90)

|x|

(||x − µ|| + ||−x − µ||) fX (x)dx ≥

||2x|| fX (x)dx =

(||x|| + ||−x||) fX (x)dx

(cid:90)

|x|

(cid:90)

|x|

(1)

(**)

B.2. Proof of Theorem
We use v(d)
at layer d, and φ the non-linearity. The weights and bias are called W (d) and b(d) respectively. So we have the relations

to denote the ith neuron in hidden layer d. We also let x = z(0) be an input, h(d) be the hidden representation

i

h(d) = W (d)z(d) + b(d),

z(d+1) = φ(h(d)).

Proof. We ﬁrst prove the zero bias case. To do so, it is sufﬁcient to prove that
(cid:33)d+1


(cid:32) √
√

(cid:12)
(cid:12)
(cid:12)δz(d+1)(t)
(cid:12)
(cid:12)
(cid:12)

≥ O

(cid:104)(cid:12)
(cid:12)
(cid:12)





(cid:12)
(cid:12)
(cid:12)

E

(cid:105)

σk
σ + k

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)δz(0)(t)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

as integrating over t gives us the statement of the theorem.

For ease of notation, we will suppress the t in z(d)(t).

We ﬁrst write

W (d) = W (d)

⊥ + W (d)

(cid:107)

where the division is done with respect to z(d). Note that this means h(d+1) = W (d)
(maps to 0) z(d).

(cid:107) z(d) as the other component annihilates

We can also deﬁne AW (d)
saturated. Letting Wi denote the ith row of matrix W , we now claim that:

= {i : i ∈ [k], |h(d+1)

i

(cid:107)

| < 1} i.e. the set of indices for which the hidden representation is not

E

W (d)

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)δz(d+1)(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

= E

E

W (d)
(cid:107)

W (d)
⊥

((W (d)

⊥ )iδz(d) + (W (d)

(cid:107)

)iδz(d))2

(*)














(cid:88)

i∈A

W

(d)
(cid:107)







1/2






Indeed, by Lemma 2 we ﬁrst split the expectation over W (d) into a tower of expectations over the two independent parts
of W to get

E

W (d)

(cid:104)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)δz(d+1)(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

= E

E

W (d)
(cid:107)

W (d)
⊥

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)φ(W (d)δz(d))
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

On the Expressive Power of Deep Neural Networks

But conditioning on W (d)
φ(W (d)δz(d)) with the sum in the term on the right hand side of the claim.

in the inner expectation gives us h(d+1) and AW (d)

(cid:107)

(cid:107)

, allowing us to replace the norm over

Till now, we have mostly focused on partitioning the matrix W (d). But we can also set δz(d) = δz(d)
⊥ where the
perpendicular and parallel are with respect to z(d). In fact, to get the expression in (**), we derive a recurrence as below:

(cid:107) + δz(d)

E

W (d)

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d+1)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

≥ O

(cid:33)

(cid:32) √
√

σk
σ + k

E

W (d)

(cid:12)
(cid:104)(cid:12)
(cid:12)δz(d)
(cid:12)
(cid:12)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:105)

To get this, we ﬁrst need to deﬁne ˜z(d+1) = 1A

h(d+1) - the latent vector h(d+1) with all saturated units zeroed out.

W

(d)
(cid:107)

We then split the column space of W (d) = ⊥W (d) + (cid:107)W (d), where the split is with respect to ˜z(d+1). Letting δz(d+1)
the part perpendicular to z(d+1), and A the set of units that are unsaturated, we have an important relation:

⊥

be

Claim

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d+1)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

⊥W (d)δz(d)1A

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(where the indicator in the right hand side zeros out coordinates not in the active set.)

To see this, ﬁrst note, by deﬁnition,

δz(d+1)
⊥

= W (d)δz(d) · 1A − (cid:104)W (d)δz(d) · 1A, ˆz(d+1)(cid:105)ˆz(d+1)

where the ˆ· indicates a unit vector.

Similarly

⊥W (d)δz(d) = W (d)δz(d) − (cid:104)W (d)δz(d), ˆ˜z(d+1)(cid:105)ˆ˜z(d+1)

(1)

(2)

Now note that for any index i ∈ A, the right hand sides of (1) and (2) are identical, and so the vectors on the left hand side
agree for all i ∈ A. In particular,

Now the claim follows easily by noting that

Returning to (*), we split δz(d) = δz(d)
cancellation, we have

δz(d+1)
⊥

· 1A = ⊥W (d)δz(d) · 1A
(cid:12)
(cid:12)
(cid:12)
(cid:12)δz(d+1)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≥
(cid:12)
⊥ = (cid:107)W (d)
(cid:107) , W (d)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12).
(cid:12)
⊥ + ⊥W (d)

(cid:12)
(cid:12)δz(d+1)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
⊥ + δz(d)

· 1A

(cid:12)
(cid:12)
(cid:12)

⊥

⊥

⊥ (and W (d)

(cid:107)

analogously), and after some

E

W (d)

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)δz(d+1)(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:105)

= E

E

W (d)
(cid:107)

W (d)
⊥

(⊥W (d)

⊥ + (cid:107)W (d)

⊥ )iδz(d)

⊥ + (⊥W (d)

(cid:107) + (cid:107)W (d)

(cid:107)

)iδz(d)
(cid:107)

(cid:17)2







1/2



















(cid:88)

(cid:16)

i∈A

W

(d)
(cid:107)

We would like a recurrence in terms of only perpendicular components however, so we ﬁrst drop the (cid:107)W (d)
(which
can be done without decreasing the norm as they are perpendicular to the remaining terms) and using the above claim, have

⊥ , (cid:107)W (d)

(cid:107)

E

W (d)

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d+1)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

≥ E

E

W (d)
(cid:107)

W (d)
⊥

(⊥W (d)

⊥ )iδz(d)

⊥ + (⊥W (d)

(cid:107)

)iδz(d)
(cid:107)














(cid:88)

(cid:16)

i∈A

W

(d)
(cid:107)

(cid:17)2







1/2






On the Expressive Power of Deep Neural Networks

But in the inner expectation, the term ⊥W (d)
we have

(cid:107) δz(d)

(cid:107)

is just a constant, as we are conditioning on W (d)

. So using Lemma 5

(cid:107)














E

W (d)
⊥

(cid:88)

(cid:16)

i∈A

W

(d)
(cid:107)

(⊥W (d)

⊥ )iδz(d)

⊥ + (⊥W (d)

(cid:107)

)iδz(d)
(cid:107)

1/2

(cid:17)2












≥ E

W (d)
⊥














(cid:88)

(cid:16)

(⊥W (d)

⊥ )iδz(d)

⊥

i∈A

W

(d)
(cid:107)

(cid:17)2







1/2






We can then apply Lemma 4 to get














E

W (d)
⊥

(cid:88)

(cid:16)

(⊥W (d)

⊥ )iδz(d)

⊥

i∈A

W

(d)
(cid:107)

1/2

(cid:17)2







≥






√

2

σ
√
k

(cid:113)2|AW (d)
(cid:107)
2

| − 3

E

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

The outer expectation on the right hand side only affects the term in the expectation through the size of the active set of
units. For ReLUs, p = P(h(d+1)
| < 1), and noting that we get a non-zero
norm only if |AW (d)

| ≥ 2 (else we cannot project down a dimension), and for |AW (d)

> 0) and for hard tanh, we have p = P(|h(d+1)

| ≥ 2,

i

i

(cid:107)

(cid:107)

√

2

(cid:113)2|AW (d)
(cid:107)
2

| − 3

≥

(cid:113)

1
√
2

|AW (d)

|

(cid:107)

we get

E

W (d)

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d+1)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:105)
(cid:12)
(cid:12)

≥





k
(cid:88)

j=2

1
√
2

(cid:18)k
j

(cid:19)
pj(1 − p)k−j σ
√
k


 E

(cid:112)j

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

We use the fact that we have the probability mass function for an (k, p) binomial random variable to bound the

j term:

√

k
(cid:88)

j=2

(cid:18)k
j

(cid:19)
pj(1 − p)k−j σ
√
k

(cid:112)j = −

(cid:18)k
1

(cid:19)
p(1 − p)k−1 σ
√
k

+

k
(cid:88)

j=0

(cid:19)

(cid:18)k
j

pj(1 − p)k−j σ
√
k

(cid:112)j

√

= −σ

kp(1 − p)k−1 + kp ·

σ
√
k

k
(cid:88)

j=1

(cid:19)

1
√
j

(cid:18)k − 1
j − 1

pj−1(1 − p)k−j

But by using Jensen’s inequality with 1/

x, we get

√

k
(cid:88)

j=1

(cid:19)

1
√
j

(cid:18)k − 1
j − 1

pj−1(1 − p)k−j ≥

(cid:113)(cid:80)k

j=1 j(cid:0)k−1

j−1

1
(cid:1)pj−1(1 − p)k−j

=

1
(cid:112)(k − 1)p + 1

where the last equality follows by recognising the expectation of a binomial(k − 1, p) random variable. So putting together,
we get

E

W (d)

(cid:104)(cid:12)
(cid:12)
(cid:12)δz(d+1)
(cid:12)
(cid:12)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

≥

1
√
2

(cid:32)

√

−σ

kp(1 − p)k−1 + σ ·

√

(cid:33)

kp
(cid:112)1 + (k − 1)p

E

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

From here, we must analyse the hard tanh and ReLU cases separately. First considering the hard tanh case:

To lower bound p, we ﬁrst note that as h(d+1)

i

is a normal random variable with variance ≤ σ2, if A ∼ N (0, σ2)

(a)

(b)

P(|h(d+1)
i

| < 1) ≥ P(|A| < 1) ≥

1
√
2π

σ

On the Expressive Power of Deep Neural Networks

where the last inequality holds for σ ≥ 1 and follows by Taylor expanding e−x2/2 around 0. Similarly, we can also show
that p ≤ 1
σ .

So this becomes

E

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)δz(d+1)(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)



≥



= O





1
√
2
(cid:32) √
√

(cid:33)

σk
σ + k

E

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

√

σk

1
(2π)1/4

(cid:113)

√

σ

2π + (k − 1)

√

(cid:18)

−

k

1 −

(cid:19)k−1



 E

1
σ

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

Finally, we can compose this, to get

E

(cid:12)
(cid:104)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)δz(d+1)(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)



≥







1
√
2

1
(2π)1/4

√

σk

(cid:113)

√

σ

2π + (k − 1)

√

(cid:18)

−

k

1 −



d+1

(cid:19)k−1




1
σ

c · ||δx(t)||

(c)

with the constant c being the ratio of ||δx(t)⊥|| to ||δx(t)||. So if our trajectory direction is almost orthogonal to x(t)
(which will be the case for e.g. random circular arcs, c can be seen to be ≈ 1 by splitting into components as in Lemma 1,
and using Lemmas 3, 4.)

The ReLU case (with no bias) is even easier. Noting that for random weights, p = 1/2, and plugging in to equation (a), we
get

E

W (d)

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d+1)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

≥

(cid:32)

−σ

√
k
2k + σ ·

1
√
2

√

k
(cid:112)2(k + 1)

(cid:33)

E

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

(d)

But the expression on the right hand side has exactly the asymptotic form O(σ

k/

k + 1), and we ﬁnish as in (c).

√

√

Result for non-zero bias
In fact, we can easily extend the above result to the case of non-zero bias. The insight is to
note that because δz(d+1) involves taking a difference between z(d+1)(t + dt) and z(d+1)(t), the bias term does not enter
at all into the expression for δz(d+1). So the computations above hold, and equation (a) becomes

E

W (d)

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d+1)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

≥

(cid:32)

1
√
2

√

−σw

kp(1 − p)k−1 + σw ·

√

(cid:33)

kp
(cid:112)1 + (k − 1)p

E

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

For ReLUs, we require h(d+1)
N (0, σ2
through results in the same asymptotic behavior as without bias.

b ) and
w) respectively. But with p ≥ 1/4, this holds as the signs for w, b are purely random. Substituting in and working

> 0 where the bias and weight are drawn from N (0, σ2

i + b(d+1)
z(d)

= w(d+1)
i

i

i

For hard tanh, not that as h(d+1)

i

is a normal random variable with variance ≤ σ2

w + σ2

b (as equation (b) becomes

P(|h(d+1)
i

| < 1) ≥

1
w + σ2
b )

√

2π

(cid:112)(σ2

This gives Theorem 3

E

(cid:104)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)δz(d+1)(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

≥ O





σw
w + σ2

(σ2

b )1/4

·

√
(cid:113)(cid:112)σ2

k

w + σ2

b + k


 E

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)δz(d)
(cid:12)

⊥

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

On the Expressive Power of Deep Neural Networks

Figure 12. The ﬁgure above shows trajectory growth with different initialization scales as a trajectory is propagated through a fully
connected network for MNIST, with Relu activations. Note that as described by the bound in Theorem 3 we see that trajectory growth
is 1) exponential in depth 2) increases with initialization scale and width, 3) increases faster with scale over width, as expected from σw
compared to (cid:112)k/(k + 1) in the Theorem.

Statement and Proof of Upper Bound for Trajectory Growth for Hard Tanh Replace hard-tanh with a linear
coordinate-wise identity map, h(d+1)
= (W (d)z(d))i + bi. This provides an upper bound on the norm. We also then
recover a chi distribution with k terms, each with standard deviation σw
k

1
2

,

i

E

(cid:104)(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)δz(d+1)(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:105)

√

≤

2

(cid:12)
(cid:12)

(cid:12)δz(d)(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Γ ((k + 1)/2)
Γ (k/2)
(cid:19) 1

(cid:18) k + 1
k

2 (cid:12)
(cid:12)
(cid:12)

2

(cid:12)
(cid:12)
(cid:12)

σw
k 1
(cid:12)δz(d)(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ,

≤ σw

(2)

(3)

where the second step follows from (Laforgia and Natalini, 2013), and holds for k > 1.

Proof of Theorem 4

i

i

1 . This has as input (cid:80)k

| = 1. Furthermore, as signs for z(d−1)

i1
and W (d−1)
i1
as sensitive to v(d−1)
= 1. For a particular input, we can deﬁne v(d)

Proof. For σb = 0:
For hidden layer d < n, consider neuron v(d)
we assume that |z(d−1)
assume wlog that z(d−1)
wlog −1) will induce a transition in node v(d)
X = Wi1 ∼ N (0, σ2/k) and (cid:80)
computation, we instead look at P(|X| > |Y |), where Y ∼ N (0, σ2).
But this is the same as computing P(|X|/|Y | > 1) = P(X/Y < −1) + P(X/Y > 1). But the ratio of two centered
independent normals with variances σ2
2 follows a Cauchy distribution, with parameter σ1/σ2, which in this case is
1/

. As we are in the large σ case,
are both completely random, we can also
if v(d−1)
transitioning (to
i
1 . A sufﬁcient condition for this to happen is if |Wi1| ≥ | (cid:80)
j(cid:54)=i Wj1|. But
j(cid:54)=i Wj1 = Y (cid:48) ∼ N (0, (k − 1)σ2/k). So we want to compute P(|X| > |Y (cid:48)|). For ease of

1, σ2
k. Substituting this in to the cdf of the Cauchy distribution, we get that

i=1 W (d−1)

z(d−1)
i

√

1

i

i

(cid:19)

P

(cid:18) |X|
|Y |

2
π

> 1

= 1 −

arctan(

k)

√

2468101214Network depth100101102103104105106Trajectory lengthTrajectory length growth with increasing depthw50 scl3w1000 scl3w50 scl5w500 scl5w100 scl8w700 scl8w300 scl10w700 scl10On the Expressive Power of Deep Neural Networks

(a)

(c)

(b)

(d)

Figure 13. The exponential growth of trajectory length with depth, in a random deep network with hard-tanh nonlinearities. A circular
trajectory is chosen between two random vectors. The image of that trajectory is taken at each layer of the network, and its length
measured. (a,b) The trajectory length vs. layer, in terms of the network width k and weight variance σ2
w, both of which determine its
growth rate. (c,d) The average ratio of a trajectory’s length in layer d + 1 relative to its length in layer d. The solid line shows simulated
data, while the dashed lines show upper and lower bounds (Theorem 3). Growth rate is a function of layer width k, and weight variance
σ2
w.

024681012Depth100101102103104105Trajectory LengthTrajectory Length, k=32σ2w=0.5σ2w=1σ2w=4σ2w=16σ2w=64024681012Depth100101102Trajectory LengthTrajectory Length, σ2w=4k=1k=4k=16k=64k=2560100200300400500600Width2024681012||δxd+1||||δxd||Perturbation Growthσ2w=1σ2w=16σ2w=64050100150200250300σ2w505101520||δxd+1||||δxd||Perturbation Growthk=2k=32k=512On the Expressive Power of Deep Neural Networks

Finally, using the identity arctan(x) + arctan(1/x) and the Laurent series for arctan(1/x), we can evaluate the right hand
side to be O(1/

k). In particular

√

P

(cid:18) |X|
|Y |

(cid:19)

> 1

≥ O

(cid:19)

(cid:18) 1
√
k

√

(c)

i

k neurons in the layer below.
might ﬂip very quickly from say −1 to 1, the gradation in the transition ensures
will transition at distinct times, we get the desired growth rate in expectation as

This means that in expectation, any neuron in layer d will be sensitive to the transitions of
Using this, and the fact the while v(d−1)
that neurons in layer d sensitive to v(d−1)
follows:
Let T (d) be a random variable denoting the number of transitions in layer d. And let T (d)
number of transitions of neuron i in layer d. Note that by linearity of expectation and symmetry, E (cid:2)T (d)(cid:3) = (cid:80)
kE

be a random variable denoting the

T (d)
i

=

E

(cid:105)

(cid:104)

(cid:105)

i

i

i

(cid:104)
T (d)
1

(cid:105)

(cid:104)
T (d+1)
1

i 1(1,i)T (d)
Now, E
being sensitive to neuron i in layer d.

≥ E

i

(cid:104)(cid:80)

(cid:105)

(cid:104)

= kE

1(1,1)T (d)

1

(cid:105)

But by the independence of these two events, E

side is O(1/

k) by (c), so putting it all together, E

√

(cid:105)

(cid:104)
1(1,1)T (d)
1
(cid:105)
(cid:104)
T (d+1)
1
√

= E (cid:2)1(1,1)
kE

√

(cid:104)
T (d)
1

(cid:3) · E
(cid:105)
.

≥

(cid:105)

(cid:104)

T (d)
1

Written in terms of the entire layer, we have E (cid:2)T (d+1)(cid:3) ≥

kE (cid:2)T (d)(cid:3) as desired.

where 1(1,i) is the indicator function of neuron 1 in layer d + 1

. But the ﬁrt time on the right hand

b /σ2

w), by noting that Y ∼ N (0, σ2

w + σ2

b ). This results in a growth rate of form

For σb > 0:

We replace

√

(cid:113)

O(

k/

1 + σ2
b
σ2
w

).

√

k with (cid:112)k(1 + σ2

B.3. Dichotomies: a natural dual

Our measures of expressivity have mostly concentrated on sweeping the input along a trajectory x(t) and taking measures
of FA(x(t); W ). Instead, we can also sweep the weights W along a trajectory W (t), and look at the consequences (e.g.
binary labels – i.e. dichotomies), say for a ﬁxed set of inputs x1, ..., xs.

In fact, after random initialization, sweeping the ﬁrst layer weights is statistically very similar to sweeping the input along
a trajectory x(t). In particular, letting W (cid:48) denote the ﬁrst layer weights, for a particular input x0, x0W (cid:48) is a vector, each
coordinate is iid, ∼ N (0, ||x0||2σ2
w). Extending this observation, we see that (providing norms are chosen appropriately),
x0W (cid:48) cos(t) + x1W (cid:48) sin(t) (ﬁxed x0, x1, W ) has the same distribution as x0W (cid:48)
1).

1 sin(t) (ﬁxed x0, W (cid:48)

0 cos(t) + x0W (cid:48)

0, W (cid:48)

So we expect that there will be similarities between results for sweeping weights and for sweeping input trajectories, which
we explore through some synthetic experiments, primarily for hard tanh, in Figures 15, 16. We ﬁnd that the proportionality
of transitions to trajectory length extends to dichotomies, as do results on the expressive power afforded by remaining
depth.

For non-random inputs and non-random functions, this is a well known question upper bounded by the Sauer-Shelah lemma
(Sauer, 1972). We discuss this further in Appendix ??. In the random setting, the statistical duality of weight sweeping
and input sweeping suggests a direct proportion to transitions and trajectory length for a ﬁxed input. Furthermore, if the
xi ∈ S are sufﬁciently uncorrelated (e.g. random) class label transitions should occur independently for each xi Indeed,
we show this in Figure 14.

C. Addtional Experiments from Section 3

Here we include additional experiments from Section 3

On the Expressive Power of Deep Neural Networks

(a)

(b)

Figure 14. We sweep the weights W of a layer through a trajectory W (t) and count the number of labellings over a set of datapoints.
When W is the ﬁrst layer, this is statistically identical to sweeping the input through x(t) (see Appendix). Thus, similar results are
observed, with exponential increase with the depth of an architecture, and much slower increase with width. Here we plot the number
of classiﬁcation dichotomies over s = 15 input vectors achieved by sweeping the ﬁrst layer weights in a hard-tanh network along a
one-dimensional great circle trajectory. We show this (a) as a function of depth for several widths, and (b) as a function of width for
several depths. All networks were generated with weight variance σ2

w = 8, and bias variance σ2

b = 0.

024681012141618Remaining Depth dr100101102103104105Unique PatternsDichotomies vs. Remaining Depthk=2k=8k=32k=128k=5120100200300400500600Width k100101102103104105Unique PatternsDichotomies vs. Widthdr=1dr=3dr=5dr=7dr=9dr=11dr=13dr=15dr=17On the Expressive Power of Deep Neural Networks

Figure 15. Expressive power depends only on remaining network depth. Here we plot the number of dichotomies achieved by sweeping
the weights in different network layers through a 1-dimensional great circle trajectory, as a function of the remaining network depth.
The number of achievable dichotomies does not depend on the total network depth, only on the number of layers above the layer swept.
All networks had width k = 128, weight variance σ2
w = 8, number of datapoints s = 15, and hard-tanh nonlinearities. The blue dashed
line indicates all 2s possible dichotomies for this random dataset.

0246810121416Remaining Depth dr101102103104105Unique DichotomiesDichotomies vs. Remaining DepthLayer swept = 1Layer swept = 4Layer swept = 8Layer swept = 12All dichotomiesOn the Expressive Power of Deep Neural Networks

Figure 16. Here we plot the number of unique dichotomies that have been observed as a function of the number of transitions the network
has undergone. Each datapoint corresponds to the number of transitions and dichotomies for a hard-tanh network of a different depth,
with the weights in the ﬁrst layer undergoing interpolation along a great circle trajectory W (0)(t). We compare these plots to a random
walk simulation, where at each transition a single class label is ﬂipped uniformly at random. Dichotomies are measured over a dataset
consisting of s = 15 random samples, and all networks had weight variance σ2
w = 16. The blue dashed line indicates all 2s possible
dichotomies.

100101102103104105Transitions100101102103104105Unique DichotomiesDichotomies vs. Transitionsk=2k=8k=32k=128k=512All dichotomiesRandom walkOn the Expressive Power of Deep Neural Networks

Figure 17. We repeat a similar experiment in Figure 7 with a fully connected network on CIFAR-10, and mostly observe that training
lower layers again leads to better performance, although, as expected, overall performance is impacted by training only a single layer.
The networks had width k = 200, weight variance σ2
w = 1, and hard-tanh nonlinearities. We again only train from the second hidden
layer on so that the number of parameters remains ﬁxed. The theory only applies to training error (the ability to ﬁt a function), and
generalisation accuracy remains low in this very constrained setting.

0100200300400500Epoch Number0.20.30.40.50.6AccuracyTrain Accuracy Against Epoch0100200300400500Epoch NumberTest Accuracy Against Epochlay 2lay 3lay 4lay 5lay 6lay 7lay 8On the Expressive Power of Deep Neural Networks

Figure 18. Training increases the trajectory length for smaller initialization values of σw. This experiment plots the growth of trajectory
length as a circular interpolation between two MNIST datapoints is propagated through the network, at different train steps. Red indicates
the start of training, with purple the end of training. We see that the training process increases trajectory length, likely to increase the
expressivity of the input-output map to enable greater accuracy.

