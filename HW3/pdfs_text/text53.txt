Distributed and Provably Good Seedings for k-Means in Constant Rounds

Olivier Bachem 1 Mario Lucic 1 Andreas Krause 1

Abstract
The k-means++ algorithm is the state of the art
algorithm to solve k-Means clustering problems
as the computed clusterings are O(log k) com-
petitive in expectation. However, its seeding step
requires k inherently sequential passes through
the full data set making it hard to scale to mas-
sive data sets. The standard remedy is to use the
k-means(cid:107) algorithm which reduces the number
of sequential rounds and is thus suitable for a dis-
tributed setting. In this paper, we provide a novel
analysis of the k-means(cid:107) algorithm that bounds
the expected solution quality for any number of
rounds and oversampling factors greater than k,
the two parameters one needs to choose in prac-
tice. In particular, we show that k-means(cid:107) pro-
vides provably good clusterings even for a small,
constant number of iterations. This theoretical
ﬁnding explains the common observation that
k-means(cid:107) performs extremely well in practice
even if the number of rounds is low. We further
provide a hard instance that shows that an addi-
tive error term as encountered in our analysis is
inevitable if less than k − 1 rounds are employed.

1. Introduction

Over the last several years, the world has witnessed the
emergence of data sets of an unprecedented scale across
different scientiﬁc disciplines. This development has cre-
ated a need for scalable, distributed machine learning algo-
rithms to deal with the increasing amount of data. In this
paper, we consider large-scale clustering or, more specif-
ically, the task of ﬁnding provably good seedings for k-
Means in a massive data setting.

Seeding — the task of ﬁnding initial cluster centers —
is critical to ﬁnding good clusterings for k-Means.
In
fact, the seeding step of the state of the art algorithm
k-means++ (Arthur & Vassilvitskii, 2007) provides the

1Department of Computer Science, ETH Zurich. Correspon-

dence to: Olivier Bachem <olivier.bachem@inf.ethz.ch>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

theoretical guarantee on the solution quality while the sub-
sequent reﬁnement using Lloyd’s algorithm (Lloyd, 1982)
only guarantees that the quality does not deteriorate. While
the k-means++ seeding step guarantees a solution that is
O(log k) competitive with the optimal solution in expecta-
tion, it also requires k inherently sequential passes through
the data set. This makes it unsuitable for the massive data
setting where the data set is distributed across machines and
computation has to occur in parallel.

(2012) propose the
As a remedy, Bahmani et al.
k-means(cid:107) algorithm which produces seedings for k-
Means with a reduced number of sequential iterations.
Whereas k-means++ only samples a single cluster cen-
ter in each of k rounds, k-means(cid:107) samples in expectation
(cid:96) points in each of t iterations. Provided t is small enough,
this makes k-means(cid:107) suitable for a distributed setting as
the number of synchronizations is reduced.

Our contributions. We provide a novel analysis of
k-means(cid:107) that bounds the expected solution quality for
any number of rounds t and any oversampling factor (cid:96) ≥ k,
the two parameters that need to be chosen in practice. Our
bound on the expected quantization error includes both a
“traditional” multiplicative error term based on the opti-
mal solution as well as a scale-invariant additive error term
based on the variance of the data. The key insight is that
this additive error term vanishes at a rate of (cid:0) k
if t or (cid:96) is
increased. This shows that k-means(cid:107) provides provably
good clusterings even for a small, constant number of iter-
ations and explains the commonly observed phenomenon
that k-means(cid:107) works very well even for small t.

(cid:1)t

e(cid:96)

We further provide a hard instance on which k-means(cid:107)
provably incurs an additive error based on the variance of
the data and for which an exclusively multiplicative error
guarantee cannot be achieved. This implies that an additive
error term such as the one in our analysis is in fact neces-
sary if less than k − 1 rounds are employed.

2. Background & related work

k-Means clustering. Let X denote a set of points in Rd.
The k-Means clustering problem is to ﬁnd a set C of k
cluster centers in Rd that minimizes the quantization error

φX (C) =

d(x, C)2 =

(cid:88)

x∈X

(cid:88)

x∈X

(cid:107)x − q(cid:107)2
2.

min
q∈C

Distributed and Provably Good Seedings for k-Means in Constant Rounds

Algorithm 1 k-means++ seeding
Require: weighted data set (X , w), number of clusters k
1: C ← sample single x ∈ X with probability
2: for i = 2, . . . , k do
3:

wx
x(cid:48)∈X wx(cid:48)

wx d(x,C)2
x(cid:48) ∈X wx(cid:48) d(x(cid:48),C)2

Sample x ∈ X with probability
C ← C ∪ {x}

(cid:80)

(cid:80)

4:
5: Return C

Algorithm 2 k-means(cid:107) overseeding
Require: data set X , # rounds t, oversampling factor (cid:96)
1: C ← sample a point uniformly at random from X
2: for i = 1, 2, . . . , t do
3:
4:
5:

Add x to C (cid:48) with probability min

C (cid:48) ← ∅
for x ∈ X do

(cid:16)

1, (cid:96) d(x,C)2
φX (C)

(cid:17)

We denote the optimal quantization error by φOPT(X )
while the variance of the data is deﬁned as Var(X ) =
φX ({µ(X )}) where µ(X ) is the mean of X .

k-means++ seeding. Given a data set X and any set of
cluster centers C ⊂ X , the D2-sampling strategy selects a
new center by sampling each point x ∈ X with probability

p(x) =

d(x, C)2
x(cid:48)∈X d(x(cid:48), C (cid:48))2 .

(cid:80)

The seeding step of k-means++ (Arthur & Vassilvitskii,
2007), detailed for potentially weighted data sets in Algo-
rithm 1, selects an initial cluster center uniformly at ran-
dom and then sequentially adds k − 1 cluster centers us-
ing D2 sampling whereby C is always the set of previously
sampled centers. Arthur & Vassilvitskii (2007) show that
the solution quality φk-means++ of k-means++ seeding is
bounded in expectation by

E[φk-means++] ≤ 8 (log2 k + 2) φOPT(X ).

The computational complexity of k-means++ seeding is
O(nkd) where n is the number of data points and d the di-
mensionality. Unfortunately, the iterations in k-means++
seeding are inherently sequential and, as a result, the algo-
rithm requires k full passes through the data. This makes
the algorithm unsuitable for the distributed setting.

k-means(cid:107) seeding. As a remedy, Bahmani et al. (2012)
propose the algorithm k-means(cid:107) which aims to reduce
the number of sequential iterations. The key component
of k-means(cid:107) is detailed in Algorithm 2 in what we call
k-means(cid:107) overseeding: First, a data point is sampled as
the ﬁrst cluster center uniformly at random. Then, in each
of t sequential rounds, each data point x ∈ X is inde-
pendently sampled with probability min
and
added to the set of sampled centers C at the end of the
round. The parameter (cid:96) ≥ 1 is called the oversampling fac-
tor and determines the expected number of sampled points
in each iteration.

1, (cid:96) d(x,C)2
φX (C)

(cid:16)

(cid:17)

At the end of Algorithm 2, one obtains an oversampled
solution with t(cid:96) cluster centers in expectation. The full
k-means(cid:107) seeding algorithm as detailed in Algorithm 3
reduces such a solution to k centers as follows: First, each
of the centers in the oversampled solution is weighted by
the number of data points which are closer to it than the

C ← C ∪ C (cid:48)

6:
7: Return C

Algorithm 3 k-means(cid:107) seeding
Require: data set X , # rounds t, oversampling factor (cid:96)
1: B ← Result of Algorithm 2 applied to (X , t, (cid:96))
2: for c ∈ B do
3:

Xc ← points x ∈ X whose closest center in B is c
(ties broken arbitrarily but consistently)
wc ← |Xc|

4:
5: C ← Result of Algorithm 1 applied to (B, w)
6: Return C

other centers. Then, k-means++ seeding is run on the
weighted oversampled solution to produce a set of k ﬁ-
nal centers. The total computational complexity of Algo-
rithm 3 is O(nt(cid:96)d) in expectation.

The key intuition behind k-means(cid:107) is that, if we choose
a large oversampling factor (cid:96), the number of rounds t can
be small — certainly much smaller than k, preferably even
constant. The step in lines 4 and 5 in Algorithm 2 can be
distributed over several machines and after each round the
set C can be synchronized. Due to the low number of syn-
chronizations (i.e., rounds), Algorithm 2 can be efﬁciently
run in a distributed setting.1

Other related work. Celebi et al. (2013) provide an
overview over different seeding methods for k-Means. D2-
sampling and k-means++ style algorithms have been in-
dependently studied by both Ostrovsky et al. (2006) and
Arthur & Vassilvitskii (2007). This research direction has
led to polynomial time approximation schemes based on
D2-sampling (Jaiswal et al., 2014; 2015), constant factor
approximations based on sampling more than k centers
(Ailon et al., 2009; Aggarwal et al., 2009) and the analy-
sis of hard instances (Arthur & Vassilvitskii, 2007; Brun-
sch & R¨oglin, 2011). Recently, algorithms to approximate
k-means++ seeding based on Markov Chain Monte Carlo
have been proposed by Bachem et al. (2016b;a). Finally,
k-means++ has been used to construct coresets — small
data set summaries — for k-Means clustering (Lucic et al.,
2016; Bachem et al., 2015; Fichtenberger et al., 2013; Ack-
ermann et al., 2012) and Gaussian mixture models (Lucic
et al., 2017).

1A popular choice is the MLLib library of Apache Spark

(Meng et al., 2016) which uses k-means(cid:107) by default.

Distributed and Provably Good Seedings for k-Means in Constant Rounds

3. Intuition and key results

In this section, we provide the intuition and the main results
behind our novel analysis of k-means(cid:107) and defer the for-
mal statements and the formal proofs to Section 4.

3.1. Solution quality of k-means(cid:107)

Solution quality of Algorithm 2. We ﬁrst consider Algo-
rithm 2 as it largely determines the ﬁnal solution quality.
Algorithm 3 with its use of k-means++ to obtain the ﬁnal
k cluster centers, only adds an additional O(log k) factor
as shown in Theorem 1. Our key result is Lemma 4 (see
Section 4) which guarantees that, for (cid:96) ≥ k, the expected
error of solutions computed by Algorithm 2 is at most

by at most 1
2 per round.2 This means that, given the anal-
ysis in Bahmani et al. (2012), one would always obtain a
constant additive error for a constant number of rounds t,
even as (cid:96) is increased.

Guarantee for Algorithm 3. Our main result — Theo-
rem 1 — bounds the expected quality of solutions produced
by Algorithm 3. As in Bahmani et al. (2012), one loses an-
other factor of O(ln k) compared to (1) due to Algorithm 3.
Theorem 1. Let k ∈ N, t ∈ N and (cid:96) ≥ k. Let X be a data
set in Rd and C be the set returned by Algorithm 3. Then,

E[φX (C)] ≤ O

ln k

Var(X )+O(ln k)φOPT(X ).

(cid:33)

(cid:19)t

(cid:32)(cid:18) k
e(cid:96)

E[φX (C)] ≤ 2

Var(X ) + 26φOPT(X ).

(1)

3.2. A hard instance for k-means(cid:107)

(cid:19)t

(cid:18) k
e(cid:96)

The ﬁrst term may be regarded as a scale-invariant addi-
tive error: It is additive as it does not depend on the opti-
mal quantization error φOPT(X ). It is scale-invariant since
both the variance and the quantization error are scaled by
λ2 if we scale the data set X by λ > 0. The second term is a
“traditional” multiplicative error term based on the optimal
quantization error.

Given a ﬁxed oversampling factor (cid:96), the additive error term
decreases exponentially if the number of rounds t is in-
creased. Similarly, for a ﬁxed number of rounds t, it de-
creases polynomially at a rate O(cid:0) 1
(cid:1) if the over sampling
(cid:96)t
factor (cid:96) is increased. This result implies that even for a con-
stant number of rounds one may obtain good clusterings by
increasing the oversampling factor (cid:96). This explains the em-
pirical observation that often even a low number of rounds
t is sufﬁcient and that increasing (cid:96) increases the solution
quality (Bahmani et al., 2012). The practical implications
of this result are non-trivial: Even for the choice of t = 5
and (cid:96) = 5k one retains at most 0.0004% of the variance as
an additive error. Furthermore, state of the art uniform de-
viation bounds for k-Means include a similar additive error
term (Bachem et al., 2017).

Comparison to previous result. Bahmani et al. (2012)
show the following result:
re-
For α =
turned by Algorithm 2 with t rounds.
exp (cid:0)−(1 − e−(cid:96)/(2k))(cid:1) ≈ e− (cid:96)
2k , Corollary 3 of Bahmani
et al. (2012) bounds the expected quality of C by

Let C be the set

E[φX (C)] ≤

(cid:19)t

(cid:18) 1 + α
2

ψ +

16
1 − α

φOPT(X ),

(2)

where ψ denotes the quantization error of X based on the
ﬁrst, uniformly sampled center in k-means(cid:107). The key dif-
ference compared to our result is as follows: First, even as
we increase (cid:96), the factor α is always non-negative. Hence,
regardless of the choice of (cid:96), the additive ψ term is reduced

We consider the case t < k − 1 which captures the scenario
where k-means(cid:107) is useful in practice as for t ≥ k one
may simply use k-means++ instead.
Theorem 2. For any β > 0, k ∈ N, t < k − 1 and (cid:96) ≥ 1,
there exists a data set X of size 2(t + 1) such that

E[φX (C)] ≥

(4(cid:96)t)−t Var(X ),

1
4

where C is the output of Algorithm 2 or Algorithm 3 ap-
plied to X with t and (cid:96). Furthermore,

Var(X ) > 0, φOPT(X ) = 0 and n∆2 ≤ β

where ∆ is the largest distance between any points in X .

Theorem 2 shows that there exists a data set on which
k-means(cid:107) provably incurs a non-negligible error even if
the optimal quantization error is zero. This implies that
k-means(cid:107) with t < k − 1 cannot provide a multiplica-
tive guarantee on the expected quantization error for gen-
eral data sets. We thus argue that an additive error bound
such as the one in Theorem 1 is required. We note that the
upper bound in (1) and the lower bound in Theorem 2 ex-
hibit the same 1
(cid:96)t dependence on the oversampling factor (cid:96)
for a given number of rounds t.

Furthermore, Theorem 2 implies that, for general data
sets, k-means(cid:107) cannot achieve the multiplicative error
of O(log k) in expectation as claimed by Bahmani et al.
(2012).3 In particular, if the optimal quantization error is

2Note that E[ψ] ≤ 2 Var(X ) (Arthur & Vassilvitskii, 2007).
3To see this, let ψ = φX (c1) be the quantization error of the
ﬁrst sampled center in Algorithm 2 and choose β small enough
such that the choice of t ∈ O(log ψ) leads to t < k − 1. For
X in Theorem 2, φOPT(X ) = 0 which implies that the desired
multiplicative guarantee would require E[φX (C)] = 0. However,
the non-negligible, additive error in Theorem 2 and Var(X ) > 0
implies that E[φX (C)] > 0.

Distributed and Provably Good Seedings for k-Means in Constant Rounds

zero, then k-means(cid:107) would need to return a solution with
quantization error zero. While we are guaranteed to remove
a constant fraction of the error in each round, the number
of required iterations may be unbounded.

where A denotes the complement of A. For convenience,
let G0 be the event that occurs with probability one.

Let (a1, a2, . . . , a|A|) be any enumeration of A. For i =
1, 2, . . . , m and j = 1, 2, . . . , |A|+1, deﬁne the event

4. Theoretical analysis

Proof of Theorem 1. The proof is divided into four
steps: First, we relate k-means(cid:107)-style oversampling to
k-means++-style D2-sampling in Lemmas 1 and 2. Sec-
ond, we analyze a single iteration of Algorithm 2 in
Lemma 3. Third, we bound the expected solution quality
of Algorithm 2 in Lemma 4. Finally, we use this to bound
the expected solution quality of Algorithm 3 in Theorem 1.
Lemma 1. Let A be a ﬁnite set and let f : 2A → R be
a set function that is non-negative and monotonically de-
creasing, i.e., f (V ) ≥ f (U ) ≥ 0, for all V ⊆ U .

Let P be a probability distribution where, for each a ∈ A,
Ea denotes an independent event that occurs with proba-
bility qa ∈ [0, 1]. Let C be the set of elements a ∈ A for
which the event Ea occurs.

Let Q be the probability distribution on A where a single
a ∈ A is sampled with probability qa/(cid:80)
a∈A qa.
Then, with ∅ denoting the empty set, we have that

EP [f (C)] ≤ EQ[f ({a})] + e− (cid:80)

a∈A qa f (∅).

Proof. To prove the claim, we ﬁrst construct a series of
sub-events of the events {Ea}a∈A and then use them to
recursively bound EP [f (C)].
Let m ∈ N. For each a ∈ A, let ia be an indepen-
dent random variable drawn uniformly at random from
{1, 2, . . . , m}. For each a ∈ A and i = 1, 2, . . . , m, let
Fai be an independent event that occurs with probability

P[Fai] =

(cid:16)

1 −

(cid:17)i−1

.

qa
m

For each a ∈ A and i = 1, 2, . . . , m, denote by Eai the
event that occurs if i = ia and both Ea and Fai occur. By
design, all these events are independent and thus

P[Eai] = P[Ea]P[Fai]P[ia = i] =

(cid:16)

qa
m

1 −

qa
m

(cid:17)i−1

(3)

for each a ∈ A and i = 1, 2, . . . , m. Furthermore, for any
a, a(cid:48) ∈ A with a (cid:54)= a(cid:48) and any i, i(cid:48) ∈ {1, 2, . . . , m}, the
events Eai and Ea(cid:48)i(cid:48) are independent.

For i = 1, 2, . . . , m let Gi be the event that none of the
events {Eai(cid:48)}a∈A,i(cid:48)≤i occur, i.e.,

Gi =

(cid:92)

(cid:92)

Eai(cid:48)

i(cid:48)≤i

a∈A

Gi,j = Gi−1 ∩

(cid:92)

Eaj(cid:48) i.

0<j(cid:48)<j

We note that by deﬁnition Gi,1 = Gi−1 and Gi,|A|+1 = Gi
for i = 1, 2, . . . , m.

For i = 1, 2, . . . , m and j = 1, 2, . . . , |A|, we have

E[f (C)|Gi,j] =P(cid:2)Eaj i | Gi,j

+ P

(cid:104)
Eaj(cid:48) i | Gij

(cid:3)E(cid:2)f (C) | Eaj i ∩ Gij
E[f (C) | Gi,j+1].

(cid:105)

(cid:3)

We now bound the individual terms. The event Gi,j im-
plies that the events {Eaj i(cid:48)}i(cid:48)<i did not occur. Further-
more, Eaj i is independent of the events {Eaj(cid:48) i(cid:48)}
i(cid:48)=1,2,...,m
for j(cid:48) (cid:54)= j. Hence, we have

P(cid:2)Eaj i | Gi,j

(cid:3) = P

Eaj i | G0 ∩

Eaj i(cid:48)

(cid:34)

(cid:35)

(cid:92)

i(cid:48)<i

(4)

(5)

P(cid:2)Eaj i

(cid:3)

=

=

=

P(cid:2)G0 ∩ (cid:84)

i(cid:48)<i Eaj i(cid:48)

(cid:3)

P(cid:2)Eaj i

(cid:3)

P(cid:2)Eaj i

i(cid:48)<i Eaj i(cid:48)
(cid:3)
P(cid:2)Eaj i(cid:48)

i(cid:48)<i

(cid:3)

(cid:3) ,

1 − P(cid:2)(cid:83)

1 − (cid:80)

where the last equality follows since the events {Eaj i(cid:48)}i(cid:48)<i
(cid:3) is
are disjoint. Using (3), we observe that (cid:80)
i(cid:48)<i
a sum of a ﬁnite geometric series and we have

P(cid:2)Eaj i(cid:48)

P(cid:2)Eaj i(cid:48)

(cid:3) =

(cid:88)

i(cid:48)<i

(cid:16)

qa
m

(cid:88)

i(cid:48)<i

1 −

qa
m

(cid:17)i(cid:48)−1

=

qa
m

= 1 −

(cid:1)i−1
(cid:1)

m

1 − (cid:0)1 − qa
1 − (cid:0)1 − qa
m
(cid:17)i−1
(cid:16)
qa
m

1 −

.

Together with (3) and (5), this implies

P(cid:2)Eaj i | Gi,j

(cid:3) =

qa
m

(cid:0)1 − qa
(cid:0)1 − qa

m

(cid:1)i−1
(cid:1)i−1 =

m

qa
m

.

(6)

The event Eaj i implies that C contains aj. Hence, since f
is monotonically decreasing, we have

E(cid:2)f (C) | Eaj i ∩ Gij

(cid:3) ≤ f ({aj}).

Distributed and Provably Good Seedings for k-Means in Constant Rounds

Using (4) and (6), this implies

E[f (C)|Gi,j] ≤

f ({aj})+

1 −

E[f (C) | Gi,j+1].

(cid:16)

(cid:17)

qaj
m

Applying this result iteratively for j = 1, 2, . . . , |A| implies

If EQ[f ({a})] = 0, the contradiction follows directly
from (7). Otherwise, EQ[f ({a})] > 0 implies that there
exists an (cid:15) > 0 such that

EP [f (C)] > (1 + (cid:15))EQ[f ({a})] + e− (cid:80)

a∈A qa f (∅).

(8)

qaj
m

|A|
(cid:88)

j=1


+



E[f (C)|Gi,1] =





qaj
m

(cid:18)

(cid:89)

1 −

j(cid:48)<j

qaj(cid:48)
m



(cid:19)

 f ({aj})

|A|
(cid:89)

(cid:16)

j=1

1 −

qaj
m

(cid:17)


 E(cid:2)f (C) | Gi,|A|+1

(cid:3).

Note that 0 ≤ 1 − qa
non-negative. This implies that for i = 1, 2, . . . , m

m ≤ 1 for all a ∈ A and that f is

E[f (C)|Gi,1] ≤

f ({a}) + c · E(cid:2)f (C)|Gi,|A|+1

(cid:3)

(cid:88)

a∈A

qa
m

where

(cid:89)

(cid:16)

1 −

c =

(cid:17)

.

qa
m

a∈A

Since Gi,1 = Gi−1 and Gi,|A|+1 = Gi, we have for i =
1, 2, . . . , m

E[f (C)|Gi−1] ≤

f ({a}) + c · E[f (C)|Gi].

(cid:88)

a∈A

qa
m

Applying this result iteratively, we obtain

E[f (C)] ≤

f ({a}) + cm · f (∅).

(cid:33)

ci−1

(cid:32) m
(cid:88)

i=1

(cid:88)

a∈A

qa
m

Since 0 ≤ c ≤ 1, we have

m
(cid:88)

i=1

ci−1 ≤

ci−1 =

1
1 − c

.

∞
(cid:88)

i=1

For x ∈ [−1, 0] it holds that log(1 + x) ≤ x and hence

(cid:32)

= exp

m

(cid:16)

log

1 −

(cid:88)

a∈A

(cid:33)

(cid:17)

qa
m

= e− (cid:80)

a∈A qa .

cm =

(cid:89)

(cid:16)

1 −

(cid:17)m

qa
m

a∈A

(cid:32)

≤ exp

−m

(cid:33)

(cid:88)

a∈A

qa
m

This implies that

E[f (C)] ≤

1
1 − c

(cid:88)

a∈A

qa
m

By deﬁnition, we have

(cid:89)

(cid:16)

1 −

c =

a∈A

(cid:17)

qa
m

= 1 −

(cid:88)

a∈A

qa
m

+ o

(cid:19)

.

(cid:18) 1
m

Thus, there exists a m(cid:15) ∈ N sufﬁciently large such that

c = 1 −

(cid:88)

a∈A

qa
m(cid:15)

+ o

(cid:19)

(cid:18) 1
m (cid:15)

≤ 1 −

1
1 + (cid:15)

(cid:88)

a∈A

qa
m(cid:15)

.

Together with (7), this implies

E[f (C)] ≤

1 + (cid:15)

(cid:80)

qa
m(cid:15)

a∈A

(cid:88)

a∈A

qa
m (cid:15)

f ({a}) + e− (cid:80)

a∈A qa · f (∅)

= (1 + (cid:15))EQ[f ({a})] + e− (cid:80)

a∈A qa f (∅).

which is a contradiction to (8) and thus proves the claim.

Lemma 2 extends Lemma 1 to k-means(cid:107)-style sampling
probabilities of the form qa = min (1, (cid:96)pa).
Lemma 2. Let (cid:96) ≥ 1. Let A be a ﬁnite set and let f : 2A →
R be a set function that is non-negative and monotonically
decreasing, i.e., f (V ) ≥ f (U ) ≥ 0, for all V ⊆ U . For
each a ∈ A, let pa ≥ 0 and (cid:80)
Let P be the probability distribution where, for each a ∈ A,
Ea denotes an independent event that occurs with probabil-
ity qa = min (1, (cid:96)pa). Let C be the set of elements a ∈ A
for which the event Ea occurs.

a∈A pa ≤ 1.

Let Q be the probability distribution on A where a single
a ∈ A is sampled with probability pa/(cid:80)
a∈A pa.
Then, with ∅ denoting the empty set, we have that

EP [f (C)] ≤ 2EQ[f ({a})] + e−(cid:96) (cid:80)

a∈A pa f (∅).

Proof. Let A1 be the set of elements a ∈ A such that (cid:96)pa ≤
1 and A2 the set of elements a ∈ A such that (cid:96)pa > 1. By
deﬁnition, every element in A2 is sampled almost surely,
i.e., A2 ⊆ C. This implies that almost surely

f ({a}) + e− (cid:80)

a∈A qa · f (∅). (7)

f (C) = f (A2 ∪ (C ∩ A1)) .

(9)

We show the main claim by contradiction. Assume that

If |A1|= 0, the result follows trivially since

EP [f (C)] > EQ[f ({a})] + e− (cid:80)

a∈A qa f (∅).

EP [f (C)] = f (A2) = EQ[f ({a})].

Distributed and Provably Good Seedings for k-Means in Constant Rounds

pa. Lemma 1 ap-

a∈A2

Since g(∅) ≤ f (∅), it follows that

Since A2 is nonempty and pa ≥ 1
that p2 ≥ 1
(cid:96) . This implies

(cid:96) for all a ∈ A2, it follows

which proves the main claim.

Similarly, if |A2|= 0, the result follows directly from
Lemma 1 with qa = (cid:96)pa. For the remainder of the proof,
we may thus assume that both A1 and A2 are non-empty.

For a ∈ A1, let qa = (cid:96)pa and deﬁne the non-negative and
monotonically decreasing function

g(C) = f (A2 ∪ C) .

Let p1 = (cid:80)
a∈A1
plied to A1, qa and g implies that

pa and p2 = (cid:80)

EP [f (C)] = E[g(C)] ≤

g({a}) + e−(cid:96)p1g(∅).

(cid:88)

a∈A1

pa
p1

Let

and deﬁne

d = (cid:0)1 − e−(cid:96)p2(cid:1) e−(cid:96)p1

α =

p2
p1 + p2

−

p1
p1 + p2

d.

By design, α ≤ 1. Furthermore

(cid:96)p1 ≥ log (cid:96)p1.

e(cid:96)p1 ≥ (cid:96)p1 ≥

p1
p2

.

Since 0 ≤ (cid:0)1 − e−(cid:96)p2(cid:1) ≤ 1, we have

p2 ≥ p1e−(cid:96)p1 ≥ p1

(cid:0)1 − e−(cid:96)p2(cid:1) e−(cid:96)p1 = p1d.

Hence,

α =

p2
p1 + p2

−

p1
p1 + p2

(cid:0)1 − e−(cid:96)p2(cid:1) e−(cid:96)p1 ≥ 0.

Since α ∈ [0, 1] and g({a}) ≤ g(∅) for any a ∈ A1, we
may write (10), i.e.,

EP [f (C)] ≤ (1 − α)

g({a}) + (cid:0)α + e−(cid:96)p1(cid:1) g(∅).

(cid:88)

a∈A1

pa
p1

(11)

By deﬁnition, we have

1 − α = 1 −

p2
p1 + p2

+

p1
p1 + p2

d =

p1
p1 + p2

(1 + d).

Since g({a}) ≤ f ({a}), we thus have

(1 − α)

g({a}) ≤ (1 + d)

f ({a}).

(cid:88)

a∈A1

pa
p1

(cid:88)

a∈A1

pa
p1 + p2

Similarly, we have

α + e−(cid:96)p1 =

d + e−(cid:96)p1

p2
p1 + p2
p2
p1 + p2

−

+ d

p1
p1 + p2
p2
p1 + p2

=

= (1 + d)

+ e−(cid:96)(p1+p2).

p2
p1 + p2

− d + e−(cid:96)p1

(cid:0)α + e−(cid:96)p1 (cid:1) g(∅) ≤ (1+d)

g(∅)+e−(cid:96)(p1+p2)f (∅).

p2
p1 + p2

(10)

(13)
Since g(∅) = f (A2) and thus g(∅) ≤ f ({a}) for all a ∈
A2, we have

p2g(∅) =

pag(∅) ≤

paf ({a}).

(14)

(cid:88)

a∈A2

(cid:88)

a∈A2

Combining (11), (12), (13), and (14) leads to
EP [f (C)] ≤ (1 + d)EQ[f ({a})] + e−(cid:96) (cid:80)

a∈A pa f (∅).

Since p1 ≥ 0, we have

1 + d = 1 + (cid:0)1 − e−(cid:96)p2 (cid:1) e−(cid:96)p1 ≤ 2

Lemma 3 bounds the solution quality after each iteration of
Algorithm 2 based on the solution before the iteration.
Lemma 3. Let k ∈ N and (cid:96) ≥ 1. Let X be a data set in Rd
and denote by φOPT(X ) the optimal k-Means clustering
cost. Let C denote the set of cluster centers at the beginning
of an iteration in Algorithm 2 and C (cid:48) the random set added
in the iteration. Then, it holds that

E[φX (C ∪ C (cid:48))] ≤

φX (C) + 16φOPT(X ).

(cid:19)

(cid:18) k
e(cid:96)

Proof. The proof relies on applying Lemma 2 to each clus-
ter of the optimal solution. Let OPT denote any clustering
achieving the minimal cost φOPT(X ) on X . We assign all
the points x ∈ X to their closest cluster center in OPT
with ties broken arbitrarily but consistently. For c ∈ OPT
we denote by Xc the subset of X assigned to c. For each
c ∈ OPT, let

C (cid:48)

c = C (cid:48) ∩ Xc.

By deﬁnition, a ∈ Xc is included in C (cid:48)

c with probability

(cid:18)

qa = min

1,

(cid:80)

(cid:96) d(a, C)2
a(cid:48)∈X d(a(cid:48), C)2

(cid:19)

.

For each c ∈ OPT, we deﬁne the monotonically decreas-
ing function fc : 2Xc → R≥0 to be

(12)

fc(C (cid:48)

c) = φXc(C ∪ C (cid:48)

c).

Distributed and Provably Good Seedings for k-Means in Constant Rounds

For each c ∈ OPT, Lemma 2 applied to Xc, C (cid:48)
implies

c and fc

Lemma 4. Let k ∈ N, t ∈ N and (cid:96) ≥ k. Let X be a data
set in Rd and C be the random set returned by Algorithm 2.
Then,

E[fc(C (cid:48)

c)] ≤2

(cid:88)

a∈Xc

d(a, C)2

(cid:80)

a(cid:48)∈Xc

d(a(cid:48), C)2 fc({a})

−(cid:96)

+ e

(cid:80)

(cid:80)

a∈Xc
a(cid:48) ∈X

d(a,C)2
d(a(cid:48),C)2 fc(∅).

(15)

E[φX (C)] ≤ 2

Var(X ) + 26φOPT(X ).

(cid:19)t

(cid:18) k
e(cid:96)

Since fc({a}) = φXc(C ∪ {a}), the ﬁrst term is equivalent
to sampling a single element from Xc using D2 sampling.
Hence, by Lemma 3.3 of Arthur & Vassilvitskii (2007) we
have for all c ∈ OPT

d(a, C)2

(cid:88)

a∈Xc

(cid:80)

a(cid:48)∈Xc

d(a(cid:48), C)2 fc({a}). ≤ 8φOPT(Xc).

(16)

where

For each c ∈ OPT, we further have

Proof. The algorithm starts with a uniformly sampled ini-
tial cluster center c1. We iteratively apply Lemma 3 for
each of the t rounds to obtain

E[φX (C)] ≤

E[φX ({c1})] + 16stφOPT(X ) (19)

(cid:19)t

(cid:18) k
e(cid:96)

st =

t
(cid:88)

i=1

(cid:18) k
e(cid:96)

(cid:19)i−1

.

−(cid:96)

e

(cid:80)

(cid:80)

a∈Xc
a(cid:48) ∈X

d(a,C)2
d(a(cid:48) ,C)2 fc(∅) = e−(cid:96)ucucφX (C).

For (cid:96) ≥ k, we have 0 ≤ k

e(cid:96) ≤ 1/e and hence

uc =

a∈Xc

(cid:80)
d(a, C)2
a(cid:48)∈X d(a(cid:48), C)2 =
(cid:80)

φXc (C)
φX (C)

.

log (cid:96)uc ≤ (cid:96)uc − 1 ⇐⇒ (cid:96)uc ≤

e(cid:96)uc
e

where

We have that

which implies

st ≤

t
(cid:88)

i=1

1
ei−1 ≤

∞
(cid:88)

i=0

1
ei =

1
1 − 1/e

.

(20)

By Lemma 3.2 of Arthur & Vassilvitskii (2007), we have
that E[φX ({c1})] ≤ 2 Var(X ). Together with (19), (20)
and 16/(1 − 1/e) ≈ 25.31 < 26, this implies the required
result.

e−(cid:96)uc ucφX (C) ≤

φX (C).

(17)

1
e(cid:96)

With Lemma 4, we are further able to bound the solution
quality of Algorithm 3 and prove Theorem 1.

Combining (15), (16) and (17), we obtain

E[fc(C (cid:48)

c)] ≤16φOPT(Xc) +

φX (C).

(18)

1
e(cid:96)

Proof of Theorem 1. Let B be the set returned by Algo-
rithm 2. For any x ∈ X , let bx denote its closest point
in B with ties broken arbitrarily. By the triangle inequality
and since (|a|+|b|)2 ≤ 2a2 + 2b2, for any x ∈ X

Since

and

we thus have

E[φX (C ∪ C (cid:48))] ≤

E[fc(C (cid:48)

c)]

(cid:88)

c∈OPT

φX (OPT) =

φXc(OPT),

(cid:88)

c∈OPT

E[φX (C ∪ C (cid:48))] ≤

φX (C) + 16φOPT(X )

(cid:19)

(cid:18) k
e(cid:96)

which concludes the proof.

d(x, C)2 ≤ 2 d(x, bx)2 + 2 d(bx, C)2

and hence

E[φX (C)] =

(cid:88)

d(x, C)2

x∈X

(cid:88)

≤ 2

x∈X

d(x, bx)2 + 2

d(bx, C)2

(21)

(cid:88)

x∈X

= 2φX (B) + 2

wx d(x, C)2.

(cid:88)

x∈B

An iterated application of Lemma 3 allows us to bound the
solution quality of Algorithm 2 in Lemma 4.

Let OPTX be the optimal k-Means clustering solution on
X and OPT(B,w) the optimal solution on the weighted set
(B, w). By Theorem 1.1 of Arthur & Vassilvitskii (2007),

Distributed and Provably Good Seedings for k-Means in Constant Rounds

k-means++ produces an α = 8(log2 k + 2) approxima-
tion to the optimal solution. This implies that

wx d(x, C)2 ≤ α

wx d(cid:0)x, OPT(B,w)

(cid:1)2

(cid:88)

x∈B

≤ α

wx d(x, OPTX )2

(22)

= α

d(bx, OPTX )2.

(cid:88)

x∈B
(cid:88)

x∈B
(cid:88)

x∈X

By the triangle inequality and since (|a|+|b|)2 ≤ 2a2+2b2,
it holds for any x ∈ X that

d(bx, OPTX )2 ≤ 2 d(x, bx)2 + 2 d(x, OPTX )2

and hence
(cid:88)

x∈X

d(bx, OPTX )2 ≤ 2φX (B) + 2φOPT(X ).

(23)

Combining (21), (22) and (23), we obtain

E[φX (C)] ≤ 2(1 + α)φX (B) + 2αφOPT(X ).

Finally, by Lemma 4, we have

E[φX (C)] ≤(32 log2 k + 68)

Var(X )

(cid:19)t

(cid:18) k
e(cid:96)

+ (432 log2 k + 916)φOPT(X ).

Proof of Theorem 2. For this proof, we explicitly con-
struct a data set: Let β(cid:48) > 0 and consider points in one-
dimensional Euclidean space. For i = 1, 2, . . . , t, set

(cid:113)

xi =

β(cid:48)(4(cid:96)t)1−i − β(cid:48)(4(cid:96)t)−i

as well as

(cid:113)

xt+1 =

β(cid:48)(4(cid:96)t)−t.

Let the data set X consist of the t+1 points {xi}t=1,2,...,t+1
as well as t + 1 points at the origin. Since t < k − 1,
the optimal k-Means clustering solution consists of t + 2
points placed at each of the {xi}i=1,2,...t+1 and at 0. By
design, this solution has a quantization error of zero and the
variance is nonzero, i.e., φOPT(X ) = 0 and Var(X ) > 0
as claimed.
Choose β(cid:48) = β
2(t+1) . The maximal distance ∆ between
any two points in X is bounded by ∆ = d(0, x1)2 ≤ β(cid:48).
Since n = 2(t + 1), this implies ψ ≤ n∆2 ≤ β as claimed.

For i = 1, 2, . . . , t, let Ci consist of 0 and all xj with j < i.
By design, we have d(0, Ci)2 = 0 as well as d(xj, Ci)2 =
0 for j < i. For j ≥ i, we have d(xj, Ci)2 = d(xj, 0)2.
For any i = 1, 2, . . . , t + 1, we thus have

d(xj, 0)2 = β(cid:48)(4(cid:96)t)1−i.

(cid:88)

j≥i

Consider a single iteration of Algorithm 2 where C = Ci.
In this case, all points in Xj with j < i are added to C (cid:48)
with probability zero and for j > i each point xj is added
to C (cid:48) with probability

(cid:32)

min

1,

(cid:33)

(cid:96) d(xj, 0)2
j(cid:48)≥i d(xj(cid:48), 0)2

(cid:80)

=

(cid:96) d(xj, 0)2
β(cid:48)(4(cid:96)t)1−i .

By the union bound, the probability that any of the points
in (cid:83)

j>i{xj} are sampled is bounded by

(cid:88)

j>i

(cid:96) d(xj, 0)2
β(cid:48)(4(cid:96)t)1−i =

1
4t

.

The point xi is not sampled with probability at most

1 − min

1,

(cid:32)

(cid:33)

(cid:96) d(xi, 0)2
j(cid:48)≥i d(xj(cid:48), 0)2

(cid:80)

(cid:18)

= 1 − min

1, (cid:96) −

(cid:19)

1
4t

≤

1
4t

.

By the union bound, a single iteration of Algorithm 2 with
C = Ci hence samples exactly the set C (cid:48) = {xi} with
(cid:1).
probability at least (cid:0)1 − 1

2t

In Algorithm 2, the ﬁrst center is sampled uniformly at ran-
dom from X . Since half of the elements in X are placed at
0, with probability at least 1
2 , the ﬁrst center is at 0 or equiv-
alently C = C1. With probability (cid:0)1 − 1
(cid:1)t
2 , we then
sample exactly the points x1, x2, . . . , xt in the t subsequent
iterations. Hence, with probability at least 1
4 , the solution
produced by Algorithm 2 consists of 0 and all xi except
xt+1. Since xt+1 is closest to 0, this implies

≥ 1

2t

E[φX (C)] ≥

d(xt+1, 0)2 =

β(cid:48)(4(cid:96)t)−t.

(24)

1
4

The variance of X is bounded by a single point at 0, i.e.,

Var(X ) ≤ φX ({0}) =

d(xj, 0)2 = β(cid:48).

1
4

(cid:88)

j≥1

Together with (24), we have that

E[φX (C)] ≥

(4(cid:96)t)−t Var(X ).

1
4

The same result extends to the output of Algorithm 3 as it
always picks a subset of the output of Algorithm 2.

Acknowledgements

This research was partially supported by SNSF NRP 75,
ERC StG 307036, a Google Ph.D. Fellowship and an IBM
Ph.D. Fellowship. This work was done in part while An-
dreas Krause was visiting the Simons Institute for the The-
ory of Computing.

Distributed and Provably Good Seedings for k-Means in Constant Rounds

Fichtenberger, Hendrik, Gill´e, Marc, Schmidt, Melanie,
Schwiegelshohn, Chris, and Sohler, Christian. Bico:
In Euro-
Birch meets coresets for k-means clustering.
pean Symposium on Algorithms, pp. 481–492. Springer,
2013.

Jaiswal, Ragesh, Kumar, Amit, and Sen, Sandeep. A
simple D2-sampling based PTAS for k-means and other
clustering problems. Algorithmica, 70(1):22–46, 2014.

Jaiswal, Ragesh, Kumar, Mehul, and Yadav, Pulkit.

Im-
proved analysis of D2-sampling based PTAS for k-means
and other clustering problems. Information Processing
Letters, 115(2):100–103, 2015.

Lloyd, Stuart. Least squares quantization in PCM. IEEE
Transactions on Information Theory, 28(2):129–137,
1982.

Lucic, Mario, Bachem, Olivier, and Krause, Andreas.
Strong coresets for hard and soft Bregman clustering
with applications to exponential family mixtures.
In
International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), May 2016.

Lucic, Mario, Faulkner, Matthew, Krause, Andreas, and
Feldman, Dan. Training mixture models at scale via
coresets. To appear in Journal of Machine Learning Re-
search (JMLR), 2017.

Meng, Xiangrui, Bradley, Joseph, Yuvaz, B, Sparks, Evan,
Venkataraman, Shivaram, Liu, Davies, Freeman, Jeremy,
Tsai, D, Amde, Manish, Owen, Sean, et al. Mllib: Ma-
chine learning in Apache Spark. Journal of Machine
Learning Research (JMLR), 17(34):1–7, 2016.

Ostrovsky, Rafail, Rabani, Yuval, Schulman, Leonard J,
and Swamy, Chaitanya. The effectiveness of Lloyd-type
In Symposium on
methods for the k-means problem.
Foundations of Computer Science (FOCS), pp. 165–176.
IEEE, 2006.

References

Ackermann, Marcel R, M¨artens, Marcus, Raupach,
Christoph, Swierkot, Kamil, Lammersen, Christiane,
and Sohler, Christian. StreamKM++: A clustering al-
gorithm for data streams. Journal of Experimental Algo-
rithmics (JEA), 17:2–4, 2012.

Aggarwal, Ankit, Deshpande, Amit, and Kannan, Ravi.
Adaptive sampling for k-means clustering. In Approx-
imation, Randomization, and Combinatorial Optimiza-
tion. Algorithms and Techniques, pp. 15–28. Springer,
2009.

Ailon, Nir, Jaiswal, Ragesh, and Monteleoni, Claire.
Streaming k-means approximation. In Advances in Neu-
ral Information Processing Systems, pp. 10–18, 2009.

Arthur, David and Vassilvitskii, Sergei. k-means++: The
advantages of careful seeding. In Symposium on Discrete
Algorithms (SODA), pp. 1027–1035. SIAM, 2007.

Bachem, Olivier, Lucic, Mario, and Krause, Andreas.
Coresets for nonparametric estimation - the case of DP-
means. In International Conference on Machine Learn-
ing (ICML), 2015.

Bachem, Olivier, Lucic, Mario, Hassani, Hamed, and
Krause, Andreas. Fast and provably good seedings for
k-means. In Advances in Neural Information Processing
Systems (NIPS), pp. 55–63, 2016a.

Bachem, Olivier, Lucic, Mario, Hassani, S. Hamed, and
Krause, Andreas. Approximate k-means++ in sublinear
In Conference on Artiﬁcial Intelligence (AAAI),
time.
February 2016b.

Bachem, Olivier, Lucic, Mario, Hassani, S. Hamed, and
Krause, Andreas. Uniform deviation bounds for k-
In To appear in International Con-
means clustering.
ference on Machine Learning (ICML), 2017.

Bahmani, Bahman, Moseley, Benjamin, Vattani, Andrea,
Kumar, Ravi, and Vassilvitskii, Sergei. Scalable K-
Means++. Very Large Data Bases (VLDB), 5(7):622–
633, 2012.

Brunsch, Tobias and R¨oglin, Heiko. A bad instance for k-
In International Conference on Theory and
means++.
Applications of Models of Computation, pp. 344–352.
Springer, 2011.

Celebi, M Emre, Kingravi, Hassan A, and Vela, Patricio A.
A comparative study of efﬁcient initialization methods
for the k-means clustering algorithm. Expert Systems
with Applications, 40(1):200–210, 2013.

