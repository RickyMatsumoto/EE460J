Supplemental Materials for: Stochastic
Gradient MCMC Methods for Hidden Markov
Models

Yi-An Ma, Nicholas J. Foti, Emily B. Fox

1 Gradient of the Posterior

For the hidden Markov model (HMM), the posterior distribution of all hyperpa-
rameters θ can be calculated by the Bayes rule, where

p(θ|y) ∝ p(y|θ)p(θ).

Since

p(y, x|θ) = π0(x0)

Axt,xt−1 ·

p(yt|xt),

T
(cid:89)

T
(cid:89)

t=1
where y = (y1, · · · yT ) denotes the data as real valued vector, and x = (x1, · · · xT )
as discrete valued vector with xt ∈ {1, · · · K}, ∀t. We can directly marginalize
out the hidden variables, x, with matrix multiplication as

t=1

p(y|θ) = 1T

T P (yT )A · · · P (y1)A π0,

where P (yT ) is a diagonal matrix and Pi,j(yt) = p(yt|xt = i)δi,j; 1T
T = (1, · · · , 1)
is a row vector of k ones (T denotes transpose); (π0)i = π0(x0 = i). Hence the
same as Eq. (2), (3) and (8) of the main paper, the posterior distribution is:

p(θ|y) = 1T

T P (yT )A · · · P (y1)A π0 · p(θ).

When we divide the whole sequence into subsequences of

yτ,L = (yτ −L, . . . , yτ , . . . , yτ +L),

1

the posterior can be rewritten as:

p(θ|y) ∝ 1T (cid:89)

P (yτ,L)π0 · p(θ),

(1)

yτ,L∈S

where S is the minimum set of yτ,L covering y.

We can then use gradient information of the posterior distribution to construct

MCMC algorithms. The gradient of the log-posterior distribution is:

∂ ln p(θ|y)
∂θi

=

|S|
(cid:88)

τ =1

1TP (y|S|,L)A · · ·

∂ (P (yτ,L)A)
∂θi
1TP (y|S|,L)A · · · P (yτ,L)A · · · P (y1,L)Aπ0

· · · P (y1,L)Aπ0

+

∂ ln p(θ)
∂θi

.

Denote qT
Then

τ +L+1 = 1T

T P (yT )A · · · P (yt+1)A and πτ −L−1 = P (yt−1)A · · · P (y1)Aπ0.

∂U (θ)
∂θi

= −

∂ ln p(y|θ)
∂θi

−

∂p(θ)
∂θi
∂P (yτ )
∂θi
qT
τ +L+1P (yτ )πτ −L−1

τ +L+1

πτ −L−1

qT

(cid:88)

= −

yτ ∈ (cid:101)S

−

∂ ln p(θ)
∂θi

,

(2)

as shown in Eq. (11) of the main paper.

2 Lyapunov Exponent

The question of buffer length is equivalent to: for two random vectors π and π∗,
what’s the expected length of LB such that after the application of P (yLB), π and
π∗ will synchronize? This is a question of random dynamical systems and can be
answered through deﬁning the Lyapunov exponent.

We ﬁrst transform π through stereographic projection into K − 1 dimensions
and denote as: r. Then operator P (yt)A[ · ] is projected to new space and the
equivalent dynamics over r becomes: Fyt. We deﬁne the Lyapunov exponent L
through the projected random dynamics Fyt as

(cid:90)

L =

Ω×RK−1

ln ||∇rFy(r)||dµydµr,

(3)

2

where y ∈ Ω. Measure µy corresponds to the distribution of the data yt, and µr is
the invariant measure of r under the dynamics of P (yt)A, which will be estimated
through sampling.

Once the Lyapunov exponent L is calculated, we can set the buffer length:

B =

ln

1
L

(cid:19)

,

(cid:18) δ
δ0

(4)

where δ = 10−3 is the error tolerance and δ0 = 2 is the maximum initial error for
probability vectors.

3 Subsequence Sampling Procedure

We use the following sampling procedure to obtain the subsequences used to
compute stochastic gradient estimates. In order to enforce the non-overlapping
mixing-time constraint between adjacent subsequences, we sample them sequen-
tially. This results in the following form for the probability of the minibatch (cid:101)S:
p( (cid:101)S) = (cid:81)R−1
n=0 L/|Sn|, where |S0| = T , |Sn| = |Sn−1| − (ν + 2B + 2L) − Loverlap.
The quantity Loverlap is calculated as follows:

L0

overlap = |τn|,

LT

overlap = |T − τn|,

LL

overlap =

LR

overlap =

n−1
min
n(cid:48)=1,τn(cid:48) <τn

n−1
min
n(cid:48)=1,τn(cid:48) >τn

{|τn − τn(cid:48)|} − L − B,

{|τn − τn(cid:48)|} − L − B.

overlap, LT

overlap, LL

If min{L0
overlap} ≥ 2ν + 3L + 3B, the minimum number
of observations required to ﬁt an entire subsequence while respecting minimum
gap ν, Loverlap = 0. Otherwise, Loverlap equals to the sum of all the above terms
that are less than 2ν + 3L + 3B.

overlap, LR

Since T (cid:29) L, B, ν, then p( (cid:101)S) provides the correct probability of the mini-

batch (cid:101)S.

3

Figure 1: Synthetic experiments with hard-to-capture dynamics. Diagonally dominant
(DD) (left) and reversed cycles (RC) (right) experiments. First Row: The emission dis-
tributions corresponding to 8 different states. Arrows in the RC case indicate the Markov
transition structure with transition between bridge states as dashed arrows. Second Row:
Decrease of error in transition matrix estimation versus runtime. Comparisons are made
for SG-RLD algorithms with estimated buffer, without buffer, and treating data as i.i.d.
All of the experiments use a constant computation budget by varying the number of sub-
chains, | ˜S|, with the length of the subchains, L.

4 Detailed Descriptions of Experiments

4.1 Evaluating Buffer Effectiveness

The ﬁrst data set, diagonally dominant (DD) consists of a Markov chain that
heavily self-transitions. Most subchains in a minibatch thus contain redundan-
t information with observations generated from the same latent state. Although
transitions are rarely observed, the emission means are set to be distinct so that

4

Diagonally Dominant Reversed Cycles 02004000123runtime (sec)|| A − Atrue ||F  01002003000123runtime (sec)  this example is likelihood-dominated and highly identiﬁable. See Fig. 1 (top left).
For this data we choose L = 2 and | (cid:101)S| = 10 subsequences in order to incorpo-
rate observations from distant parts of the observation sequence. This corresponds
to an extreme setting where each gradient is based only on 5 observations. The
transition matrix and emission parameters used for this experiment were:

ADD =

.999 .001

.999 .001

.999 .001















0
0
0
0
0
0
.001

0
0
0
0
0
0

0
0
0
0
0

0

0
0
0
0
0

.999 .001

.999 .001

0
0

0
0
0
0

0
0
0

0
0
0

0
0
0
0

0
0

.999 .001

.999 .001
.999

0

0
0
0
0
0
0















.

µDD = {(0, 20); (20, 0); (−30, −30); (30, −30); (−20, 0); (0, −20); (30, 30); (−30, 30); }

and ΣDD = I for all states.

The second dataset we consider contains two reversed cycles (RC): the Markov
chain strongly transitions from states 1 → 2 → 3 → 1 and 5 → 7 → 6 → 5 with
a small probability of transiting between cycles via bridge states 4 and 8. See
Fig. 1 (top right). The emission means for the two cycles are very similar but
occur in reverse order with respect to the transitions. The emission variance is
larger, making states 1 and 5, 2 and 6, 3 and 7 indiscernible by themselves. Tran-
sition information in observing long enough dynamics is thus crucial to identify
between states 1, 2, 3 and 5, 6, 7. Therefore, we set L = 5 and | (cid:101)S| = 4. Note that
same amount of data are used in the calculation of the gradient. The transition
matrix and emission parameters were:

ARC =















.01
0
.99 .01
.99
0
0
0
0
0
0
0
0
0
0
0

.85 0
0
0
0
0
.15 0
0
0
0
0

0
0
0
0
0
0
0
0
0
1 .01
0 .99 .01
.99
0
0
0
0
0















.

1
0
0
0
0
0
0
0
.85 0
0
0
0
0
.15 0

5

µ = {(−50, 0); (30, −30); (30, 30); (−100, −10); (40, −40); (−65, 0); (40, 40); (100, 10)} ,

and ΣRC = 20 ∗ I for all states.

We use a non-conjugate ﬂat prior to demonstrate the ﬂexibility of our algo-
rithm. We initialize with a short run of k-means clustering to ensure that different
states have different emission parameters.

4.2 Non-conjugate Emission Distribution

For the non-conjugate experiment, we used the following transition matrix:

(cid:18) .1 .9
.9 .1

(cid:19)

.

For emission probability, we use a log-normal distribution: pk(y) ∝ e
with parameters: µ1 = 0, µ2 = 4; σ1 = σ2 = 2.

In the non-conjugate model, we use the following priors on the emission pa-

rameters: µ1, µ2, σ1, σ2 ∼ N (0, 1).

−

ln(y − µk)2
2σ2
k

6

