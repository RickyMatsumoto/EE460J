Parseval Networks: Improving Robustness to Adversarial Examples

Moustapha Cisse 1 Piotr Bojanowski 1 Edouard Grave 1 Yann Dauphin 1 Nicolas Usunier 1

Abstract

We introduce Parseval networks, a form of deep
neural networks in which the Lipschitz constant
of linear, convolutional and aggregation layers
is constrained to be smaller than 1. Parseval
networks are empirically and theoretically mo-
tivated by an analysis of the robustness of the
predictions made by deep neural networks when
their input is subject to an adversarial perturba-
tion. The most important feature of Parseval net-
works is to maintain weight matrices of linear
and convolutional layers to be (approximately)
Parseval tight frames, which are extensions of
orthogonal matrices to non-square matrices. We
describe how these constraints can be maintained
efﬁciently during SGD. We show that Parse-
val networks match the state-of-the-art in terms
of accuracy on CIFAR-10/100 and Street View
House Numbers (SVHN), while being more ro-
bust than their vanilla counterpart against adver-
sarial examples. Incidentally, Parseval networks
also tend to train faster and make a better usage
of the full capacity of the networks.

1. Introduction

Deep neural networks achieve near-human accuracy on
many perception tasks (He et al., 2016; Amodei et al.,
2015). However, they lack robustness to small alterations
of the inputs at test time (Szegedy et al., 2014).
Indeed
when presented with a corrupted image that is barely dis-
tinguishable from a legitimate one by a human, they can
predict incorrect labels, with high-conﬁdence. An adver-
sary can design such so-called adversarial examples, by
adding a small perturbation to a legitimate input to max-
imize the likelihood of an incorrect class under constraints
on the magnitude of the perturbation (Szegedy et al., 2014;
Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2015; Pa-

1Facebook AI Research. Correspondence to: Moustapha Cisse

<moustaphacisse@fb.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

pernot et al., 2016a). In practice, for a signiﬁcant portion of
inputs, a single step in the direction of the gradient sign is
sufﬁcient to generate an adversarial example (Goodfellow
et al., 2015) that is even transferable from one network to
another one trained for the same problem but with a differ-
ent architecture (Liu et al., 2016; Kurakin et al., 2016).

The existence of transferable adversarial examples has two
undesirable corollaries. First, it creates a security threat
for production systems by enabling black-box attacks (Pa-
pernot et al., 2016a). Second, it underlines the lack of ro-
bustness of neural networks and questions their ability to
generalize in settings where the train and test distributions
can be (slightly) different as is the case for the distributions
of legitimate and adversarial examples.

Whereas the earliest works on adversarial examples already
suggested that their existence was related to the magnitude
of the hidden activations gradient with respect to their in-
puts (Szegedy et al., 2014), they also empirically assessed
that standard regularization schemes such as weight de-
cay or training with random noise do not solve the prob-
lem (Goodfellow et al., 2015; Fawzi et al., 2016). The cur-
rent mainstream approach to improving the robustness of
deep networks is adversarial training. It consists in gen-
erating adversarial examples on-line using the current net-
work’s parameters (Goodfellow et al., 2015; Miyato et al.,
2015; Moosavi-Dezfooli et al., 2015; Szegedy et al., 2014;
Kurakin et al., 2016) and adding them to the training data.
This data augmentation method can be interpreted as a ro-
bust optimization procedure (Shaham et al., 2015).

In this paper, we introduce Parseval networks, a layerwise
regularization method for reducing the network’s sensitiv-
ity to small perturbations by carefully controlling its global
Lipschitz constant. Since the network is a composition of
functions represented by its layers, we achieve increased
robustness by maintaining a small Lipschitz constant (e.g.,
1) at every hidden layer; be it fully-connected, convolu-
tional or residual. In particular, a critical quantity govern-
ing the local Lipschitz constant in both fully connected and
convolutional layers is the spectral norm of the weight ma-
trix. Our main idea is to control this norm by parameter-
izing the network with parseval tight frames (Kovaˇcevi´c &
Chebira, 2008), a generalization of orthogonal matrices.

The idea that regularizing the spectral norm of each weight

Parseval Networks

matrix could help in the context of robustness appeared as
early as (Szegedy et al., 2014), but no experiment nor al-
gorithm was proposed, and no clear conclusion was drawn
on how to deal with convolutional layers. Previous work,
such as double backpropagation (Drucker & Le Cun, 1992)
has also explored jacobian normalization as a way to im-
prove generalization. Our contribution is twofold. First, we
provide a deeper analysis which applies to fully connected
networks, convolutional networks, as well as Residual net-
works (He et al., 2016). Second, we propose a computa-
tionally efﬁcient algorithm and validate its effectiveness on
standard benchmark datasets. We report results on MNIST,
CIFAR-10, CIFAR-100 and Street View House Numbers
(SVHN), in which fully connected and wide residual net-
works were trained (Zagoruyko & Komodakis, 2016) with
Parseval regularization. The accuracy of Parseval networks
on legitimate test examples matches the state-of-the-art,
while the results show notable improvements on adversar-
ial examples. Besides, Parseval networks train signiﬁcantly
faster than their vanilla counterpart.

In the remainder of the paper, we ﬁrst discuss the previous
work on adversarial examples. Next, we give formal deﬁni-
tions of the adversarial examples and provide an analysis of
the robustness of deep neural networks. Then, we introduce
Parseval networks and its efﬁcient training algorithm. Sec-
tion 5 presents experimental results validating the model
and providing several insights.

2. Related work

Early papers on adversarial examples attributed the vulner-
ability of deep networks to high local variations (Szegedy
et al., 2014; Goodfellow et al., 2015). Some authors ar-
gued that this sensitivity of deep networks to small changes
in their inputs is because neural networks only learn the
discriminative information sufﬁcient to obtain good accu-
racy rather than capturing the true concepts deﬁning the
classes (Fawzi et al., 2015; Nguyen et al., 2015).

Strategies to improve the robustness of deep networks in-
clude defensive distillation (Papernot et al., 2016b), as well
as various regularization procedures such as contractive
networks (Gu & Rigazio, 2015). However, the bulk of
recent proposals relies on data augmentation (Goodfellow
et al., 2015; Miyato et al., 2015; Moosavi-Dezfooli et al.,
2015; Shaham et al., 2015; Szegedy et al., 2014; Kurakin
et al., 2016). It uses adversarial examples generated online
during training. As we shall see in the experimental sec-
tion, regularization can be complemented with data aug-
mentation; in particular, Parseval networks with data aug-
mentation appear more robust than either data augmenta-
tion or Parseval networks considered in isolation.

3. Robustness in Neural Networks

We consider a multiclass prediction setting, where we have
Y classes in Y = {1, ..., Y }. A multiclass classiﬁer is a
function ˆg : (x ∈ RD, W ∈ W) (cid:55)→ argmax¯y∈Y g¯y(x, W ),
where W are the parameters to be learnt, and g¯y(x, W ) is
the score given to the (input, class) pair (x, ¯y) by a function
g : RD × W → RY . We take g to be a neural network,
represented by a computation graph G = (N , E), which is
a directed acyclic graph with a single root node, and each
node n ∈ N takes values in Rd(n)
out and is a function of its
children in the graph, with learnable parameters W (n):

n : x (cid:55)→ φ(n)(cid:0)W (n), (cid:0)n(cid:48)(x)(cid:1)

n(cid:48):(n,n(cid:48))∈E

(cid:1) .

(1)

The function g we want to learn is the root of G. The
training data ((xi, yi))m
i=1 ∈ (X × Y)m is an i.i.d. sam-
ple of D, and we assume X ⊂ RD is compact. A function
(cid:96) : RY × Y → R measures the loss of g on an example
(x, y); in a single-label classiﬁcation setting for instance, a
common choice for (cid:96) is the log-loss:

(cid:96)(cid:0)g(x, W ), y(cid:1) = −gy(x, W ) + log (cid:0) (cid:88)

eg ¯y(x,W )(cid:1). (2)

¯y∈Y

√

The arguments that we develop below depend only on the
Lipschitz constant of the loss, with respect to the norm of
interest. Formally, we assume that given a p-norm of inter-
est (cid:107).(cid:107)p, there is a constant λp such that

∀z, z(cid:48) ∈ RY , ∀¯y ∈ Y, |(cid:96)(z, ¯y)−(cid:96)(z(cid:48), ¯y)| ≤ λp(cid:107)z −z(cid:48)(cid:107)p .

2 and λ∞ ≤ 2.
For the log-loss of (2), we have λ2 ≤
In the next subsection, we deﬁne adversarial examples and
the generalization performance of the classiﬁer. Then, we
make the relationship between robustness to adversarial ex-
amples and the lipschitz constant of the networks.

3.1. Adversarial examples

Given an input (train or test) example (x, y), an adversarial
example is a perturbation of the input pattern ˜x = x + δx
where δx is small enough so that ˜x is nearly undistinguish-
able from x (at least from the point of view of a human
annotator), but has the network predict an incorrect label.
Given the network parameters and structure g(., W ) and a
p-norm, the adversarial example is formally deﬁned as

˜x = argmax

˜x:(cid:107)˜x−x(cid:107)p≤(cid:15)

(cid:96)(cid:0)g(˜x, W ), y(cid:1) ,

(3)

where (cid:15) represents the strength of the adversary. Since the
optimization problem above is non-convex, Shaham et al.
(2015) propose to take the ﬁrst order taylor expansion of
x (cid:55)→ (cid:96)(g(x, W ), y) to compute δx by solving

˜x = argmax

˜x:(cid:107)˜x−x(cid:107)p≤(cid:15)

(cid:0)∇x(cid:96)(g(x, W ), y)(cid:1)T

(˜x − x) .

(4)

Parseval Networks

If p = ∞, then ˜x = x + (cid:15)sign(∇x(cid:96)(g(x, W ), y)). This is
the fast gradient sign method. For the case p = 2, we ob-
tain ˜x = x + (cid:15)∇x(cid:96)(g(x, W ), y). A more involved method
is the iterative fast gradient sign method, in which several
gradient steps of (4) are performed with a smaller stepsize
to obtain a local minimum of (3).

supremum over x0 ∈ X of the Lipschitz constant for (cid:107).(cid:107)p
of the function (1n(cid:48)(cid:48) = n(cid:48) is 1 if n(cid:48)(cid:48) = n(cid:48) and 0 otherwise):

x (cid:55)→ φ(n)(cid:0)W (n), (cid:0)n(cid:48)(cid:48)(x0+1n(cid:48)(cid:48) = n(cid:48)(x−x0))(cid:1)

n(cid:48)(cid:48):(n,n(cid:48)(cid:48))∈E

(cid:1) .

The Lipschitz constant of n, denoted by Λ(n)

p

satisﬁes:

3.2. Generalization with adversarial examples

In the context of adversarial examples, there are two differ-
ent generalization errors of interest:

L(W ) = E

(cid:2)(cid:96)(g(x, W ), y)(cid:3),

Ladv(W, p, (cid:15)) = E

(x,y)∼D

(x,y)∼D

(cid:2) max
˜x:(cid:107)˜x−x(cid:107)p≤(cid:15)

(cid:96)(g(˜x, W ), y)(cid:3).

By deﬁnition, L(W ) ≤ Ladv(W, p, (cid:15)) for every p and (cid:15) > 0.
Reciprocally, denoting by λp and Λp the Lipschitz constant
(with respect to (cid:107).(cid:107)p) of (cid:96) and g respectively, we have:

Ladv(W, p, (cid:15)) ≤ L(W )
+ E

(cid:2) max
˜x:(cid:107)˜x−x(cid:107)p≤(cid:15)

(x,y)∼D

|(cid:96)(g(˜x, W ), y) − (cid:96)(g(x, W ), y)|(cid:3)

≤ L(W ) + λpΛp(cid:15) .

This suggests that the sensitivity to adversarial examples
can be controlled by the Lipschitz constant of the network.
In the robustness framework of (Xu & Mannor, 2012),
the Lipschitz constant also controls the difference between
the average loss on the training set and the generalization
performance. More precisely, let us denote by Cp(X , γ)
the covering number of X using γ-balls for (cid:107).(cid:107)p. Using
M = supx,W,y (cid:96)(g(x, W ), y), Theorem 3 of (Xu & Man-
nor, 2012) implies that for every δ ∈ (0, 1), with probabil-
ity 1 − δ over the i.i.d. sample ((xi, yi)m

i=1, we have:

L(W ) ≤

(cid:96)(g(xi, W ), yi)

1
m

m
(cid:88)

i=1

+ λpΛpγ + M

(cid:114)

2Y Cp(X , γ

2 ) ln(2) − 2 ln(δ)

.

m

Since covering numbers of a p-norm ball in RD increases
exponentially with RD, the bound above suggests that it is
critical to control the Lipschitz constant of g, for both good
generalization and robustness to adversarial examples.

3.3. Lipschitz constant of neural networks

From the network structure we consider (1), for every node
n ∈ N , we have (see below for the deﬁnition of Λ(n,n(cid:48))
(cid:88)

):

p

Λ(n,n(cid:48))
p

(cid:107)n(cid:48)(x) − n(cid:48)(˜x)(cid:107)p ,

(cid:107)n(x) − n(˜x)(cid:107)p ≤

n(cid:48):(n,n(cid:48))∈E

p

for any Λ(n,n(cid:48))
that is greater than the worst case variation
of n with respect to a change in its input n(cid:48)(x). In par-
ticular we can take for Λ(n,n(cid:48))
any value greater than the

p

Λ(n)

p ≤

(cid:88)

Λ(n,n(cid:48))
p

Λ(n(cid:48))
p

(5)

n(cid:48):(n,n(cid:48))∈E

Thus, the Lipschitz constant of the network g can grow ex-
ponentially with its depth. We now give the Lipschitz con-
stants of standard layers as a function of their parameters:

Linear layers: For layer n(x) = W (n)n(cid:48)(x) where n(cid:48) is
the unique child of n in the graph, the Lipschitz constant
for (cid:107).(cid:107)p is, by deﬁnition, the matrix norm of W (n) induced
by (cid:107).(cid:107)p, which is usually denoted (cid:107)W (n)(cid:107)p and deﬁned by

(cid:107)W (n)(cid:107)p = sup

(cid:107)W (n)z(cid:107)p .

z:(cid:107)z(cid:107)p=1

2

2 = (cid:107)W (n)(cid:107)2Λ(n(cid:48))

Then Λ(n)
, where (cid:107)W (n)(cid:107)2, called the
spectral norm of W (n), is the maximum singular value
of W (n). We also have Λ(n)
∞ , where
j |W (n)
(cid:107)W (n)(cid:107)∞ = maxi
| is the maximum 1-norm of
the rows. W (n).

∞ = (cid:107)W (n)(cid:107)∞Λ(n(cid:48))

(cid:80)

ij

Convolutional layers: To simplify notation, let us con-
sider convolutions on 1D inputs without striding, and we
take the width of the convolution to be 2k + 1 for k ∈ N.
To write convolutional layers in the same way as linear lay-
ers, we ﬁrst deﬁne an unfolding operator U , which pre-
pares the input z, denoted by U (z). If the input has length
T with din inputs channels, the unfolding operator maps
z For a convolution of the unfolding of z considered as a
T × (2k + 1)din matrix, its j-th column is:

Uj(z) = [zj−k; ...; zj+k] ,

where “;” is the concatenation along the vertical axis (each
zi is seen as a column din-dimensional vector), and zi = 0
if i is out of bounds (0-padding). A convolutional layer
with dout output channels is then deﬁned as

n(x) = W (n) ∗ n(cid:48)(x) = W (n)U (n(cid:48)(x)) ,

where W (n) is a dout × (2k + 1)din matrix. We thus have
Λ(n)
2 ≤ (cid:107)W (cid:107)2(cid:107)U (n(cid:48)(x))(cid:107)2. Since U is a linear operator
that essentially repeats its input (2k + 1) times, we have
(cid:107)U (n(cid:48)(x)) − U (n(cid:48)(˜x))(cid:107)2
2 ≤ (2k + 1)(cid:107)n(cid:48)(x) − n(cid:48)(˜x)(cid:107)2
2,
2k + 1(cid:107)W (cid:107)2Λ(n(cid:48))
so that Λ(n)
. Also, (cid:107)U (n(cid:48)(x)) −
2 ≤
U (n(cid:48)(˜x))(cid:107)∞ = (cid:107)n(cid:48)(x) − n(cid:48)(˜x)(cid:107)∞, and so for a convolu-
tional layer, Λ(n)

∞ ≤ (cid:107)W (n)(cid:107)∞Λ(n(cid:48))
∞ .

√

2

Parseval Networks

Aggregation layers/transfer functions: Layers that per-
form the sum of their inputs, as in Residual Netowrks (He
et al., 2016), fall in the case where the values Λ(n,n(cid:48))
in (5)
come into play. For a node n that sums its inputs, we have
Λ(n,n(cid:48))
p ≤ (cid:80)
. If n is
a tranfer function layer (e.g., an element-wise application
of ReLU) we can check that Λ(n)
, where n(cid:48) is the
input node, as soon as the Lipschitz constant of the transfer
function (as a function R → R) is ≤ 1.

n(cid:48):(n,n(cid:48))∈E Λ(n(cid:48))

= 1, and thus Λ(n)

p ≤ Λ(n(cid:48))

p

p

p

p

4. Parseval networks

Parseval regularization, which we introduce in this section,
is a regularization scheme to make deep neural networks
robust, by constraining the Lipschitz constant (5) of each
hidden layer to be smaller than one, assuming the Lipschitz
constant of children nodes is smaller than one. That way,
we avoid the exponential growth of the Lipschitz constant,
and a usual regularization scheme (i.e., weight decay) at
the last layer then controls the overall Lipschitz constant
of the network. To enforce these constraints in practice,
Parseval networks use two ideas: maintaining orthonormal
rows in linear/convolutional layers, and performing convex
combinations in aggregation layers. Below, we ﬁrst explain
the rationale of these constraints and then describe our ap-
proach to efﬁciently enforce the constraints during training.

4.1. Parseval Regularization

Orthonormality of weight matrices: For linear layers,
we need to maintain the spectral norm of the weight matrix
at 1. Computing the largest singular value of weight ma-
trices is not practical in an SGD setting unless the rows
of the matrix are kept orthogonal. For a weight matrix
W ∈ Rdout×din with dout ≤ din, Parseval regulariza-
tion maintains W T W ≈ Idout×dout, where I refers to
the identity matrix. W is then approximately a Parseval
tight frame (Kovaˇcevi´c & Chebira, 2008), hence the name
of Parseval networks. For convolutional layers, the ma-
trix W ∈ Rdout×(2k+1)din is constrained to be a Parse-
val tight frame (with the notations of the previous section),
and the output is rescaled by a factor (2k + 1)−1/2. This
maintains all singular values of W to (2k + 1)−1/2, so that
2 ≤ Λ(n(cid:48))
Λ(n)
2 where n(cid:48) is the input node. More generally,
keeping the rows of weight matrices orthogonal makes it
possible to control both the spectral norm and the (cid:107).(cid:107)∞ of
a weight matrix through the norm of its individual rows.
Robustness for (cid:107).(cid:107)∞ is achieved by rescaling the rows so
that their 1-norm is smaller than 1. For now, we only ex-
perimented with constraints on the 2-norm of the rows, so
we aim for robustness in the sense of (cid:107).(cid:107)2.

Remark 1 (Orthogonality is required). Without orthogo-
nality, constraints on the 2-norm of the rows of weight ma-

trices are not sufﬁcient to control the spectral norm. Parse-
val networks are thus fundamentally different from weight
normalization (Salimans & Kingma, 2016).

Aggregation Layers:
In parseval networks, aggregation
layers do not make the sum of their inputs, but rather take
a convex combination of them:

n(x) =

(cid:88)

α(n,n(cid:48))n(cid:48)(x)

n(cid:48):(n,n(cid:48))∈E

with (cid:80)
n(cid:48):(n,n(cid:48))∈E α(n,n(cid:48)) = 1 and α(n,n(cid:48)) ≥ 0. The pa-
rameters α(n,n(cid:48)) are learnt, but using (5), these constraint
guarantee that Λ(n)
p ≤ 1 as soon as the children satisfy the
inequality for the same p-norm.

4.2. Parseval Training

Orthonormality constraints: The ﬁrst signiﬁcant differ-
ence between Parseval networks and its vanilla counter-
part is the orthogonality constraint on the weight matrices.
This requirement calls for an optimization algorithm on the
manifold of orthogonal matrices, namely the Stiefel man-
ifold. Optimization on matrix manifolds is a well-studied
topic (see (Absil et al., 2009) for a comprehensive survey).
The simplest ﬁrst-order geometry approaches consist in op-
timizing the unconstrained function of interest by moving
in the direction of steepest descent (given by the gradient
of the function) while at the same time staying on the man-
ifold. To guarantee that we remain in the manifold after
every parameter update, we need to deﬁne a retraction op-
erator. There exist several pullback operators for embedded
submanifolds such as the Stiefel manifold based for exam-
ple on Cayley transforms (Absil et al., 2009). However,
when learning the parameters of neural networks, these
methods are computationally prohibitive. To overcome this
difﬁculty, we use an approximate operator derived from the
following layer-wise regularizer of weight matrices to en-
sure their parseval tightness (Kovaˇcevi´c & Chebira, 2008):

Rβ(Wk) =

(cid:107)W (cid:62)

k Wk − I(cid:107)2
2.

β
2

Optimizing Rβ(Wk) to convergence after every gradient
descent step (w.r.t the main objective) guarantees us to stay
on the desired manifold but this is an expensive procedure.
Moreover, it may result in parameters that are far from the
ones obtained after the main gradient update. We use two
approximations to make the algorithm more efﬁcient: First,
we only do one step of descent on the function Rα(Wk).
The gradient of this regularization term is ∇Wk Rβ(Wk) =
β(WkW (cid:62)
k − I)Wk. Consequently, after every main update
we perform the following secondary update:

Wk ← (1 + β)Wk − βWkW (cid:62)

k Wk.

Parseval Networks

Algorithm 1 Parseval Training
Θ = {Wk, αk}K
while e ≤ E do

k=1, e ← 0

Sample a minibatch {(xi, yi)}B
for k ∈ {1, . . . , K} do

i=1.

Compute the gradient: GWk ← ∇Wk (cid:96)(Θ, {(xi, yi)}),
Gαk ← ∇αk (cid:96)(Θ, {(xi, yi)}).
Update the parameters:
Wk ← Wk − (cid:15) · GWk
αk ← αk − (cid:15) · Gαk .
if hidden layer then

Sample a subset S of rows of Wk.
Projection:
WS ← (1 + β)WS − βWSW (cid:62)
S WS.
αk ← argminγ∈∆K−1 (cid:107)αK − γ(cid:107)2
2

e ← e + 1.

Optionally, instead of updating the whole matrix, one can
randomly select a subset S of rows and perform the update
from Eq. (4.2) on the submatrix composed of rows indexed
by S. This sampling based approach reduces the overall
complexity to O(|S|2d). Provided the rows are carefully
sampled, the procedure is an accurate Monte Carlo ap-
proximation of the regularizer loss function (Drineas et al.,
2006). The optimal sampling probabilities, also called sta-
tistical leverages are approximately equal if we start from
an orthogonal matrix and (approximately) stay on the man-
ifold throughout the optimization since they are propor-
tional to the eigenvalues of W (Mahoney et al., 2011).
Therefore, we can sample a subset of columns uniformly
at random when applying this projection step.

While the full update does not result in an increased over-
head for convolutional layers, the picture can be very dif-
ferent for large fully connected layers making the sampling
approach computationally more appealing for such layers.
We show in the experiments that the weight matrices result-
ing from this procedure are (quasi)-orthogonal. Also, note
that quasi-orthogonalization procedures similar to the one
described here have been successfully used previously in
the context of learning overcomplete representations with
independent component analysis (Hyvärinen & Oja, 2000).

Convexity constraints in aggregation layers:
In Parse-
val networks, aggregation layers output a convex combina-
tion of their inputs instead of e.g., their sum as in Residual
networks (He et al., 2016). For an aggregation node n of
the network, let us denote by α = (α(n,n(cid:48)))n(cid:48):(n,n(cid:48))∈E the
K-size vector of coefﬁcients used for the convex combina-
tion output by the layer. To ensure that the Lipschitz con-
stant at the node n is such that Λ(n)
p ≤ 1, the constraints
of 4.1 call for a euclidean projection of α onto the positive
simplex after a gradient update:

α∗ = argmin
γ∈∆K−1

(cid:107)α − γ(cid:107)2
2 ,

Figure 1. Sample images from the CIFAR-10 dataset, with corre-
sponding adversarial examples. We show the original image and
adversarial versions for SNR values of 24.7, 12.1 and 7.8.

where ∆K−1 = {γ ∈ RK|1(cid:62)γ = 1, γ ≥ 0}. This is a
well studied problem (Michelot, 1986; Pardalos & Kovoor,
Its solution is
1990; Duchi et al., 2008; Condat, 2016).
i = max(0, αi − τ (α)), with τ : RK →
of the form: α∗
R the unique function satisfying (cid:80)
i(xi − τ (α)) = 1 for
every x ∈ RK. Therefore, the solution essentially boils
down to a soft thresholding operation. If we denote α1 ≥
α2 ≥ . . . αK the sorted coefﬁcients and k(α) = max{k ∈
(1, . . . , K)|1+kαk > (cid:80)
j≤k αj}, the optimal thresholding
is given by (Duchi et al., 2008):

τ (α) =

((cid:80)

j≤k(α) αj) − 1

k(α)

the complexity of

the projection is
Consequently,
O(K log(K)) since it is only dominated by the sorting
of the coefﬁcients and is typically cheap because aggre-
gation nodes will only have few children in practice (e.g.
If the number of children is large, there exist efﬁ-
2).
cient linear time algorithms for ﬁnding the optimal thresh-
olding τ (α) (Michelot, 1986; Pardalos & Kovoor, 1990;
Condat, 2016). In this work, we use the method detailed
above (Duchi et al., 2008) to perform the projection of the
coefﬁcient α after every gradient update step.

5. Experimental evaluation

We evaluate the effectiveness of Parseval networks on
well-established image classiﬁcation benchmark datasets
namely MNIST, CIFAR-10, CIFAR-100 (Krizhevsky,
2009) and Street View House Numbers (SVHN) (Netzer
et al.). We train both fully connected networks and wide
residual networks. The details of the datasets, the models,
and the training routines are summarized below.

Parseval Networks

5.1. Datasets

CIFAR. Each of the CIFAR datasets is composed of 60K
natural scene color images of size 32 × 32 split between
50K training images and 10K test images. CIFAR-10 and
CIFAR-100 have respectively 10 and 100 classes. For these
two datasets, we adopt the following standard preprocess-
ing and data augmentation scheme (Lin et al., 2013; He
et al., 2016; Huang et al., 2016a; Zagoruyko & Komodakis,
2016): Each training image is ﬁrst zero-padded with 4 pix-
els on each side. The resulting image is randomly cropped
to produce a new 32 × 32 image which is subsequently
horizontally ﬂipped with probability 0.5. We also normal-
ize every image with the mean and standard deviation of
its channels. Following the same practice as (Huang et al.,
2016a), we initially use 5K images from the training as a
validation set. Next, we train de novo the best model on
the full set of 50K images and report the results on the test
set. SVHN The Street View House Number dataset is a
set of 32 × 32 color digit images ofﬁcially split into 73257
training images and 26032 test images. Following common
practice (Zagoruyko & Komodakis, 2016; He et al., 2016;
Huang et al., 2016a;b), we randomly sample 10000 images
from the available extra set of about 600K images as a val-
idation set and combine the rest of the pictures with the
ofﬁcial training set. We divide the pixel values by 255 as
a preprocessing step and report the test set performance of
the best performing model on the validation set.

5.2. Models and Implementation details

ConvNet Models. For the CIFAR and SVHN datasets, we
trained wide residual networks (Zagoruyko & Komodakis,
2016) as they perform on par with standard resnets (He
et al., 2016) while being faster to train thanks to a reduced
depth. We used wide resnets of depth 28 and width 10 for
both CIFAR-10 and CIFAR-100. For SVHN we used wide
resnet of depth 16 and width 4. For each architecture, we
compare Parseval networks with the vanilla model trained
with standard regularization both in the adversarial and the
non-adversarial training settings.

ConvNet Training. We train the networks with stochas-
tic gradient descent using a momentum of 0.9. On CIFAR
datasets, the initial learning rate is set to 0.1 and scaled
by a factor of 0.2 after epochs 60, 120 and 160, for a to-
tal number of 200 epochs. We used mini-batches of size
128. For SVHN, we trained the models with mini-batches
of size 128 for 160 epochs starting with a learning rate
of 0.01 and decreasing it by a factor of 0.1 at epochs 80
and 120. For all the vanilla models, we applied by default
weight decay regularization (with parameter λ = 0.0005)
together with batch normalization and dropout since this
combination resulted in better accuracy and increased ro-
bustness in preliminary experiments. The dropout rate use

is 0.3 for CIFAR and 0.4 for SVHN. For Parseval regular-
ized models, we choose the value of the retraction parame-
ter to be β = 0.0003 for CIFAR datasets and β = 0.0001
for SVHN based on the performance on the validation set.
In all cases, We also adversarially trained each of the mod-
els on CIFAR-10 and CIFAR-100 following the guidelines
in (Goodfellow et al., 2015; Shaham et al., 2015; Kurakin
et al., 2016). In particular, we replace 50% of the examples
of every minibatch by their adversarially perturbed version
generated using the one-step method to avoid label leak-
ing (Kurakin et al., 2016). For each mini-batch, the magni-
tude of the adversarial perturbation is obtained by sampling
from a truncated Gaussian centered at 0 with standard de-
viation 2.

Fully Connected Model. We also train feedforward net-
works composed of 4 fully connected hidden layers of size
2048 and a classiﬁcation layer. The input to these networks
are images unrolled into a C × 1024 dimensional vector
where C is the number of channels. We used these models
on MNIST and CIFAR-10 mainly to demonstrate that the
proposed approach is also useful on non-convolutional net-
works. We compare a Parseval networks to vanilla models
with and without weight decay regularization. For adver-
sarially trained models, we follow the guidelines previously
described for the convolutional networks.

Fully Connected Training. We train the models with SGD
and divide the learning rate by two every 10 epochs. We use
mini-batches of size 100 and train the model for 50 epochs.
We chose the hyperparameters on the validation set and re-
train the model on the union of the training and validation
sets. The hyperparameters are β, the size of the row subset
S, the learning rate and its decrease rate. Using a subset
S of 30% of all the rows of each of weight matrix for the
retraction step worked well in practice.

5.3. Results

5.3.1. (QUASI)-ORTHOGONALITY.

We ﬁrst validate that Parseval training (Algorithm 1) indeed
yields (near)-orthonormal weight matrices. To do so, we
analyze the spectrum of the weight matrices of the different
models by plotting the histograms of their singular values,
and compare these histograms for Parseval networks to net-
works trained using standard SGD with and without weight
decay (SGD-wd and SGD).

The histograms representing the distribution of singular
values at layers 1 and 4 for the fully connected network (us-
ing S = 30%) trained on the dataset CIFAR-10 are shown
in Fig. 2 (the ﬁgures for convolutional networks are sim-
ilar). The singular values obtained with our method are
tightly concentrated around 1. This experiment conﬁrms
that the weight matrices produced by the proposed opti-

Parseval Networks

Table 1. Classiﬁcation accuracy of the models on CIFAR-10 and
CIFAR-100 with the (combination of) various regularization
scheme. (cid:15) represents here the value of the signal to noise ratio
(SNR). At (cid:15) = 30, an adversarially perturbed image is percepti-
ble by a human. For each dataset, the top 3 rows report results for
non-adversarial training and the bottom 3 rows report results for
adversarial training.

Clean

(cid:15) ≈ 50

(cid:15) ≈ 45

(cid:15) ≈ 40

(cid:15) ≈ 33

Parseval(OC)

Parseval(OC)

Model

Vanilla

Parseval

Vanilla

Parseval

Vanilla

Parseval

Vanilla

Parseval(OC)

0
1
-
R
A
F
I
C

0
0
1
-
R
A
F
I
C

Parseval(OC)

Parseval

N Vanilla
H
V
S

Parseval(OC)

Parseval

95.63

95.82

96.28

95.49

95.59

96.08

79.70

81.07

80.72

79.23

80.34

80.19

98.38

97.91

98.13

90.16

91.85

93.03

91.17

92.31

92.51

65.76

70.33

72.43

67.06

69.27

73.41

97.04

97.55

97.86

85.97

88.56

90.40

88.90

90.00

90.05

57.27

63.78

66.41

62.53

62.93

67.16

95.18

96.35

96.19

76.62

78.79

81.76

86.75

87.02

86.89

44.62

49.97

55.41

56.71

53.21

58.86

92.71

93.73

93.55

67.21

61.38

69.10

84.87

85.23

84.53

34.49

32.99

41.19

51.78

52.60

39.56

88.11

89.09

88.47

adversarial training (SGD-wd-da) on CIFAR-10. Combin-
ing Parseval Networks and adversarial training results in
the most robust method on MNIST.

ResNets. Table 1 summarizes the results of our experi-
ments with wide residual Parseval and vanilla networks on
CIFAR-10, CIFAR-100 and SVHN. In the table, we denote
Parseval(OC) the Parseval network with orthogonality con-
straint and without using a convex combination in aggrega-
tion layers. Parseval indicates the conﬁguration where both
of the orthogonality and convexity constraints are used.
We ﬁrst observe that Parseval networks outperform vanilla
ones on all datasets on the clean examples and match the
state of the art performances on CIFAR-10 (96.28%) and
SVHN (98.44%). On CIFAR-100, when we use Parse-
val wide Resnet of depth 40 instead of 28, we achieve
an accuracy of 81.76%.
In comparison, the best perfor-
mance achieved by a vanilla wide resnet (Zagoruyko & Ko-
modakis, 2016) and a pre-activation resnet (He et al., 2016)
are respectively 81.12% and 77.29%. Therefore, our pro-
posal is a useful regularizer for legitimate examples. Also
note that in most cases, Parseval networks combining both
the orthogonality constraint and the convexity constraint is
superior to use the orthogonality constraint solely.

The results presented in the table validate our most impor-
tant claim: Parseval networks signiﬁcantly improve the ro-
bustness of vanilla models to adversarial examples. When
no adversarial training is used, the gap in accuracy be-

Figure 2. Histograms of the singular values of the weight matrices
at layers 1 and 4 of our network in CIFAR-10.

Figure 3. Performance of the models for various magnitudes of
adversarial noise on MNIST (left) and CIFAR-10 (right).

mization procedure are (almost) orthonormal. The distribu-
tion of the singular values of the weight matrices obtained
with SGD has a lot more variance, with nearly as many
small values as large ones. Adding weight decay to stan-
dard SGD leads to a sparse spectrum for the weight matri-
ces, especially in the higher layers of the network suggest-
ing a low-rank structure. This observation has motivated
recent work on compressing deep neural networks (Denton
et al., 2014).

5.3.2. ROBUSTNESS TO ADVERSARIAL NOISE.

We evaluate the robustness of the models to adversarial
noise by generating adversarial examples from the test set,
for various magnitudes of the noise vector. Following com-
mon practice (Kurakin et al., 2016), we use the fast gradient
sign method to generate the adversarial examples (using
(cid:107).(cid:107)∞, see Section 3.1). Since these adversarial examples
transfer from one network to the other, the fast gradient
sign method allows to benchmark the network for reason-
able settings where the opponent does not know the net-
work. We report the accuracy of each model as a function
of the magnitude of the noise. To make the results easier
to interpret, we compute the corresponding Signal to Noise
Ratio (SNR). For an input x and perturbation δx, the SNR
is deﬁned as SNR(x, δx) = 20 log10
. We show some
adversarial examples in Fig. 1.

(cid:107)x(cid:107)2
(cid:107)δx(cid:107)2

Fully Connected Nets. Figure 3 depicts a comparison of
Parseval and vanilla networks with and without adversar-
ial training at various noise levels. On both MNIST and
CIFAR-10, Parseval networks consistently outperforms
weight decay regularization. In addition, it is as robust as

024singularvalues050010001500frequencysgdsgd-wdparseval024singularvalues0500100015002000frequencysgdsgd-wdparseval20406080SNR050100accuracysgdsgd-wdsgd-wd-daparsevalparseval-da20406080SNR0204060accuracysgdsgd-wdsgd-wd-daparsevalparseval-daParseval Networks

Table 2. Number of dimensions (in % of the total dimension) nec-
essary to capture 99% of the covariance of the activations.

SGD-wd

SGD-wd-da

Parseval

all

class

all

class

all

class

Layer 1
Layer 2
Layer 3
Layer 4

72.6
1.5
0.5
0.5

34.7
1.3
0.5
0.4

73.6
1.5
0.4
0.4

34.7
1.3
0.4
0.4

89.0
82.6
81.9
56.0

38.4
38.2
30.6
19.3

tween the two methods is signiﬁcant (particularly in the
high noise scenario). For an SNR value of 40, the best
Parseval network achieves 55.41% accuracy while the best
vanilla model is at 44.62%. When the models are adversar-
ially trained, Parseval networks remain superior to vanilla
models in most cases.
Interestingly, adversarial training
only slightly improves the robustness of Parseval networks
in low noise setting (e.g. SNR values of 45-50) and some-
times even deteriorates it (e.g. on CIFAR-10). In contrast,
combining adversarial training and Parseval networks is an
effective approach in the high noise setting. This result
suggests that thanks to the particular form of regularizer
(controlling the Lipschitz constant of the network), Parse-
val networks achieves robustness to adversarial examples
located in the immediate vicinity of each data point. There-
fore, adversarial training only helps for adversarial exam-
ples found further away from the legitimate patterns. This
observation holds consistently across all our datasets.

(cid:80)n

Better use of capacity Given the distribution of singu-
lar values observed in Figure 2, we want to analyze the
intrinsic dimensionality of the representation learned by
the different networks at every layer. To that end, we use
the local covariance dimension (Dasgupta & Freund, 2008)
which can be measured from the covariance matrix of the
data. For each layer k of the fully connected network,
we compute the activation’s empirical covariance matrix
1
i=1 φk(x)φk(x)(cid:62) and obtain its sorted eigenvalues
n
σ1 ≥ · · · ≥ σd. For each method and each layer, we select
the smallest integer p such that (cid:80)p
i=1 σi.
This gives us the number of dimensions that we need to
explain 99% of the covariance. We can also compute the
same quantity for the examples of each class, by only con-
sidering in the empirical estimation of the covariance of the
examples xi such that yi = c. Table 2 report these numbers
for all examples and the per-class average on CIFAR-10.

i=1 σi ≥ 0.99 (cid:80)d

Table 2 shows that the local covariance dimension of all
the data is consistently higher for Parseval networks than
all the other approaches at any layer of the network. SGD-
wd-da contracts all the data in very low dimensional spaces
at the upper levels of the network by using only 0.4% of the
total dimension (layer 3 and 4) while Parseval networks use
about 81% and 56% at of the whole dimension respectively

Figure 4. Learning curves of Parseval wide resnets and Vanilla
wide resnets on CIFAR-10 (right) and CIFAR-100 (left). Parseval
networks converge faster than their vanilla counterpart.

in the same layers. This is intriguing given that SGD-wd-da
also increases the robustness of the network, apparently not
in the same way as Parseval networks. For the average local
covariance dimension of the classes, SGD-wd-da contracts
each class into the same dimensionality as it contracts all
the data at the upper layers of the network. For Parseval,
the data of each class is contracted in about 30% and 19%
of the overall dimension. These results suggest that Parse-
val contracts the data of each class in a lower dimensional
manifold (compared to the intrinsic dimensionality of the
whole data) hence making classiﬁcation easier.

faster convergence Parseval networks converge signiﬁ-
cantly faster than vanilla networks trained with batch nor-
malization and dropout as depicted by ﬁgure 4. Thanks to
the orthogonalization step following each gradient update,
the weight matrices are well conditioned at each step dur-
ing the optimization. We hypothesize this is the main ex-
planation of this phenomenon. For convolutional networks
(resnets), the faster convergence is not obtained at the ex-
pense of larger wall-time since the cost of the projection
step is negligible compared to the total cost of the forward
pass on modern GPU architecture thanks to the small size
of the ﬁlters.

6. Conclusion

We introduced Parseval networks, a new approach for
learning neural networks that are intrinsically robust to ad-
versarial noise. We proposed an algorithm that allows us to
optimize the model efﬁciently. Empirical results on three
classiﬁcation datasets with fully connected and wide resid-
ual networks illustrate the performance of our approach.
As a byproduct of the regularization we propose, the model
trains faster and makes a better use of its capacity. Further
investigation of this phenomenon is left to future work.

Acknowledgements

The authors would like to thank M.A. Ranzato, Y. Tian, A.
Bordes and F. Perronnin for their valuable feedback on this
work.

0100200epochs01020304050errorsgdparseval0100200epochs01020304050errorsgdparsevalParseval Networks

References

Absil, P-A, Mahony, Robert, and Sepulchre, Rodolphe.
Optimization algorithms on matrix manifolds. Princeton
University Press, 2009.

Amodei, Dario, Anubhai, Rishita, Battenberg, Eric, Case,
Carl, Casper, Jared, Catanzaro, Bryan, Chen, Jingdong,
Chrzanowski, Mike, Coates, Adam, Diamos, Greg, et al.
Deep speech 2: End-to-end speech recognition in english
and mandarin. arXiv preprint arXiv:1512.02595, 2015.

Condat, Laurent. Fast projection onto the simplex and the\
pmb {l} _\ mathbf {1} ball. Mathematical Program-
ming, 158(1-2):575–585, 2016.

Dasgupta, Sanjoy and Freund, Yoav. Random projection
trees and low dimensional manifolds. In Proceedings of
the fortieth annual ACM symposium on Theory of com-
puting, pp. 537–546. ACM, 2008.

Denton, Emily L, Zaremba, Wojciech, Bruna, Joan, Le-
Cun, Yann, and Fergus, Rob. Exploiting linear structure
within convolutional networks for efﬁcient evaluation. In
Adv. NIPS, 2014.

Drineas, Petros, Kannan, Ravi, and Mahoney, Michael W.
Fast monte carlo algorithms for matrices i: Approximat-
ing matrix multiplication. SIAM Journal on Computing,
36(1):132–157, 2006.

Drucker, Harris and Le Cun, Yann. Improving generaliza-
tion performance using double backpropagation. IEEE
Transactions on Neural Networks, 3(6):991–997, 1992.

Duchi, John, Shalev-Shwartz, Shai, Singer, Yoram, and
Chandra, Tushar. Efﬁcient projections onto the l 1-ball
for learning in high dimensions. In Proceedings of the
25th international conference on Machine learning, pp.
272–279. ACM, 2008.

Fawzi, Alhussein, Fawzi, Omar, and Frossard, Pascal.
Analysis of classiﬁers’ robustness to adversarial pertur-
bations. arXiv preprint arXiv:1502.02590, 2015.

Fawzi, Alhussein, Moosavi-Dezfooli, Seyed-Mohsen, and
Frossard, Pascal. Robustness of classiﬁers: from adver-
sarial to random noise. In Advances in Neural Informa-
tion Processing Systems, pp. 1624–1632, 2016.

Goodfellow, Ian J, Shlens, Jonathon, and Szegedy, Chris-
tian. Explaining and harnessing adversarial examples.
In Proc. ICLR, 2015.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, pp. 770–778, 2016.

Huang, Gao, Liu, Zhuang, Weinberger, Kilian Q, and
van der Maaten, Laurens. Densely connected convo-
arXiv preprint arXiv:1608.06993,
lutional networks.
2016a.

Huang, Gao, Sun, Yu, Liu, Zhuang, Sedra, Daniel, and
Weinberger, Kilian Q. Deep networks with stochastic
depth. In European Conference on Computer Vision, pp.
646–661. Springer, 2016b.

Hyvärinen, Aapo and Oja, Erkki. Independent component
analysis: algorithms and applications. Neural networks,
2000.

Kovaˇcevi´c, Jelena and Chebira, Amina. An introduction to
frames. Foundations and Trends in Signal Processing,
2008.

Krizhevsky, Alex. Learning multiple layers of features

from tiny images, 2009.

Kurakin, Alexey, Goodfellow, Ian, and Bengio, Samy.
Adversarial machine learning at scale. arXiv preprint
arXiv:1611.01236, 2016.

Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in

network. arXiv preprint arXiv:1312.4400, 2013.

Liu, Yanpei, Chen, Xinyun, Liu, Chang, and Song,
Dawn. Delving into transferable adversarial examples
and black-box attacks. CoRR, abs/1611.02770, 2016.
URL http://arxiv.org/abs/1611.02770.

Mahoney, Michael W et al. Randomized algorithms for
matrices and data. Foundations and Trends R(cid:13) in Ma-
chine Learning, 3(2):123–224, 2011.

Michelot, Christian. A ﬁnite algorithm for ﬁnding the pro-
jection of a point onto the canonical simplex of? n.
Journal of Optimization Theory and Applications, 50(1):
195–200, 1986.

Miyato, Takeru, Maeda, Shin-ichi, Koyama, Masanori,
Nakae, Ken, and Ishii, Shin. Distributional smoothing
with virtual adversarial training. In Proc. ICLR, 2015.

Moosavi-Dezfooli, Seyed-Mohsen, Fawzi, Alhussein, and
Frossard, Pascal. Deepfool: a simple and accurate
method to fool deep neural networks. arXiv preprint
arXiv:1511.04599, 2015.

Gu, Shixiang and Rigazio, Luca. Towards deep neural net-
work architectures robust to adversarial examples.
In
ICLR workshop, 2015.

Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco,
Alessandro, Wu, Bo, and Ng, Andrew Y. Reading digits
in natural images with unsupervised feature learning.

Parseval Networks

Nguyen, Anh, Yosinski, Jason, and Clune, Jeff. Deep neu-
ral networks are easily fooled: High conﬁdence predic-
tions for unrecognizable images. In Proc. CVPR, 2015.

Papernot, Nicolas, McDaniel, Patrick, Goodfellow, Ian,
Jha, Somesh, Berkay Celik, Z, and Swami, Anan-
thram. Practical black-box attacks against deep learn-
ing systems using adversarial examples. arXiv preprint
arXiv:1602.02697, 2016a.

Papernot, Nicolas, McDaniel, Patrick, Wu, Xi, Jha,
Somesh, and Swami, Ananthram. Distillation as a de-
fense to adversarial perturbations against deep neural
networks. In Security and Privacy (SP), 2016 IEEE Sym-
posium on, pp. 582–597. IEEE, 2016b.

Pardalos, Panos M and Kovoor, Naina. An algorithm for a
singly constrained class of quadratic programs subject to
upper and lower bounds. Mathematical Programming,
46(1):321–328, 1990.

Salimans, Tim and Kingma, Diederik P. Weight normaliza-
tion: A simple reparameterization to accelerate training
of deep neural networks. In Advances in Neural Infor-
mation Processing Systems, pp. 901–901, 2016.

Shaham, Uri, Yamada, Yutaro, and Negahban, Sahand. Un-
derstanding adversarial training: Increasing local stabil-
ity of neural nets through robust optimization. arXiv
preprint arXiv:1511.05432, 2015.

Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya,
Bruna, Joan, Erhan, Dumitru, Goodfellow, Ian, and Fer-
gus, Rob. Intriguing properties of neural networks. In
Proc. ICLR, 2014.

Xu, Huan and Mannor, Shie. Robustness and generaliza-

tion. Machine learning, 2012.

Zagoruyko, Sergey and Komodakis, Nikos. Wide residual

networks. arXiv preprint arXiv:1605.07146, 2016.

