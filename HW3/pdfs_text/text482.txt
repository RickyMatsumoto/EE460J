Asynchronous Distributed Variational Gaussian Process for Regression

Hao Peng 1 Shandian Zhe 1 Xiao Zhang 1 Yuan Qi 2

Abstract
Gaussian processes (GPs) are powerful non-
parametric function estimators. However, their
applications are largely limited by the expensive
computational cost of the inference procedures.
Existing stochastic or distributed synchronous
variational inferences, although have alleviated
this issue by scaling up GPs to millions of sam-
ples, are still far from satisfactory for real-world
large applications, where the data sizes are of-
ten orders of magnitudes larger, say, billions. To
solve this problem, we propose ADVGP, the ﬁrst
Asynchronous Distributed Variational Gaussian
Process inference for regression, on the recent
large-scale machine learning platform, PARAM-
ETERSERVER. ADVGP uses a novel, ﬂexible
variational framework based on a weight space
augmentation, and implements the highly efﬁ-
cient, asynchronous proximal gradient optimiza-
tion. While maintaining comparable or bet-
ter predictive performance, ADVGP greatly im-
proves upon the efﬁciency of the existing vari-
ational methods. With ADVGP, we effortlessly
scale up GP regression to a real-world applica-
tion with billions of samples and demonstrate
an excellent, superior prediction accuracy to the
popular linear models.

1. Introduction

Gaussian processes (GPs) (Rasmussen & Williams, 2006)
are powerful non-parametric Bayesian models for func-
tion estimation. Without imposing any explicit parametric
form, GPs merely induce a smoothness assumption via the
deﬁnition of covariance function, and hence can ﬂexibly in-
fer various, complicated functions from data. In addition,
GPs are robust to noise, resist overﬁtting and produce un-
certainty estimations. However, a crucial bottleneck of GP

1Purdue University, West Lafayette,

IN, USA 2Ant Fi-
Correspondence to: Hao Peng

nancial Service Group.
<pengh@alumni.purdue.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

models is their expensive computational cost: exact GP in-
ference requires O(n3) time complexity and O(n2) space
complexity (n is the number of training samples), which
limits GPs to very small applications, say, a few hundreds
of samples.

To mitigate this limitation, many approximate inference al-
gorithms have been developed (Williams & Seeger, 2001;
Seeger et al., 2003; Qui˜nonero-Candela & Rasmussen,
2005; Snelson & Ghahramani, 2005; Deisenroth & Ng,
2015). Most methods use sparse approximations. Basi-
cally, we ﬁrst introduce a small set of inducing points; and
then we develop an approximation that transfers the expen-
sive computations from the entire large training data, such
as the covariance and inverse covariance matrix calcula-
tions, to the small set of the inducing points. To this end,
a typical strategy is to impose some simpliﬁed modeling
assumption. For example, FITC (Snelson & Ghahramani,
2005) makes a fully conditionally independent assumption.
Recently, Titsias (2009) proposed a more principled, vari-
ational sparse approximation framework, where the induc-
ing points are also treated as variational parameters. The
variational framework is less prone to overﬁtting and often
yields a better inference quality (Titsias, 2009; Bauer et al.,
2016). Based on the variational approximation, Hensman
et al. (2013) developed a stochastic variational inference
(SVI) algorithm, and Gal et al. (2014) used a tight varia-
tional lower bound to develop a distributed inference algo-
rithm with the MAPREDUCE framework.

While SVI and the distributed variational inference have
successfully scaled up GP models to millions of samples
(O(106)), they are still insufﬁcient for real-world large-
scale applications, in which the data sizes are often orders
of magnitude larger, say, over billions of samples (O(109)).
Speciﬁcally, SVI (Hensman et al., 2013) sequentially pro-
cesses data samples and requires too much time to complete
even one epoch of training. The distributed variational al-
gorithm in (Gal et al., 2014) uses the MAPREDUCE frame-
work and requires massive synchronizations during train-
ing, where a large amount of time is squandered when the
MAPPERS or REDUCERS are waiting for each other, or the
failed nodes are restarted.

To tackle this problem, we propose Asynchronous
Distributed Variational Gaussian Process inference (AD-

Asynchronous Distributed Variational Gaussian Process for Regression

VGP), which enables GP regression on applications with
(at least) billions of samples. To the best of our knowledge,
this is the ﬁrst variational inference that scales up GPs to
this level. The contributions of our work are summarized
as follows: ﬁrst, we propose a novel, general variational
GP framework using a weight space augmentation (Section
3). The framework allows ﬂexible constructions of fea-
ture mappings to incorporate various low-rank structures
and to fulﬁll different variational model evidence lower
bounds (ELBOs). Furthermore, due to the simple stan-
dard normal prior of the random weights, the framework
enables highly efﬁcient, asynchronous proximal gradient-
based optimization, with convergence guarantees as well
as fast, element-wise and parallelizable variational poste-
rior updates. Second, based on the new framework, we
develop a highly efﬁcient, asynchronous variational infer-
ence algorithm in the recent distributed machine learning
platform, PARAMETERSERVER (Li et al., 2014a) (Section
4). The asynchronous algorithm eliminates an enormous
amount of waiting time caused by the synchronous coordi-
nation, and fully exploits the computational power and net-
work bandwidth; as a result, our new inference, ADVGP,
greatly improves on both the scalability and efﬁciency of
the prior variational algorithms, while still maintaining a
similar or better inference quality. Finally, in a real-world
application with billions of samples, we effortlessly train
a GP regression model with ADVGP and achieve an ex-
cellent prediction accuracy, with 15% improvement over
the popular linear regression implemented in Vowpal Wab-
bit (Agarwal et al., 2014), the state-of-the-art large-scale
machine learning software widely used in industry.

2. Gaussian Processes Review

1 , . . . , x(cid:62)

In this paper, we focus on Gaussian process (GP) regres-
sion. Suppose we aim to infer an underlying function
f : Rd → R from an observed dataset D = {X, y}, where
n ](cid:62) is the input matrix and y is the output
X = [x(cid:62)
vector. Each row of X, namely xi (1 ≤ i ≤ n), is a d-
dimensional input vector. Correspondingly, each element
of y, namely yi, is an observed function value corrupted by
some random noise. Note that the function f can be highly
nonlinear. To estimate f from D, we place a GP prior over
f . Speciﬁcally, we treat the collection of all the function
values as one realization of the Gaussian process. There-
fore, the ﬁnite projection of f over the inputs X, i.e., f =
[f (x1), . . . , f (xn)] follows a multivariate Gaussian distri-
(cid:1), where ¯f = [ ¯f (x1), . . . , ¯f (xn)]
bution: f ∼ N (cid:0)f |¯f , Knn
are the mean function values and Knn is the n × n covari-
ance matrix. Each element of Knn is a covariance func-
tion k(·, ·) of two input vectors, i.e., [Knn]i,j = k(xi, xj).
We can choose any symmetric positive semideﬁnite ker-
nel as the covariance function, e.g.,
the ARD kernel:
2 (xi − xj)(cid:62)diag(η)(xi − xj)(cid:1),
k(xi, xj) = a2

0 exp (cid:0)− 1

where η = [1/a2
1, ..., 1/a2
the zero mean function, namely ¯f (·) = 0.

d]. For simplicity, we usually use

Given f , we use an isotropic Gaussian model to sample the
observed noisy output y: p(y|f ) = N (y|f , β−1I). The
joint probability of GP regression is
p(y, f |X) = N (cid:0)f |0, Knn

(cid:1)N (y|f , β−1I).

(1)

Further, we can obtain the marginal distribution of y,
namely the model evidence, by marginalizing out f :

p(y|X) = N (cid:0)y|0, Knn + β−1I(cid:1).

(2)

The inference of GP regression aims to estimate the appro-
priate kernel parameters and noise variance from the train-
ing data D = {X, y}, such as {a0, η} in ARD kernel and
β−1. To this end, we can maximize the model evidence in
(2) with respect to those parameters. However, to maximize
(2), we need to calculate the inverse and the determinant of
the n × n matrix Knn + β−1I to evaluate the multivariate
Gaussian term. This will take O(n3) time complexity and
O(n2) space complexity and hence is infeasible for a large
number of samples, i.e., large n.

For prediction, given a test input x∗, since the test output f ∗
and training output f can be treated as another GP projec-
tion on X and x∗, the joint distribution of f ∗ and f is also a
multivariate Gaussian distribution. Then by marginalizing
out f , we can obtain the posterior distribution of f ∗:

p(f ∗|x∗, X, y) = N (f ∗|α, v),

where

n∗(Knn + β−1I)−1y,

α = k(cid:62)
v = k(x∗, x∗) − k(cid:62)

n∗(Knn + β−1I)−1kn∗,

and kn∗ = [k(x∗, x1), . . . , k(x∗, xn)](cid:62). Note that the cal-
culation also requires the inverse of Knn + β−1I and hence
takes O(n3) time complexity and O(n2) space complexity.

3. Variational Framework Using Weight

Space Augmentation

Although GPs allow ﬂexible function inference, they have
a severe computational bottleneck. The training and predic-
tion both require O(n3) time complexity and O(n2) space
complexity (see (2), (4) and (5)), making GPs unrealistic
for real-world, large-scale applications, where the number
of samples (i.e., n) are often billions or even larger. To
address this problem, we propose ADVGP that performs
highly efﬁcient, asynchronous distributed variational in-
ference and enables the training of GP regression on ex-
tremely large data. ADVGP is based on a novel variational
GP framework using a weight space augmentation, which
is discussed below.

(3)

(4)

(5)

Asynchronous Distributed Variational Gaussian Process for Regression

First, we construct an equivalent augmented model by in-
troducing an m × 1 auxiliary random weight vector w
(m (cid:28) n). We assume w is sampled from the standard
normal prior distribution: p(w) = N (w|0, I). Given w,
we sample an n × 1 latent function values f from

p(f |w) = N (f |Φw, Knn − ΦΦ(cid:62)),

(6)

where Φ is an n × m matrix: Φ = [φ(x1), . . . , φ(xn)](cid:62).
Here φ(·) represents a feature mapping that maps the orig-
inal d-dimensional input into an m-dimensional feature
space. Note that we need to choose an appropriate φ(·)
to ensure the covariance matrix in (6) is symmetric posi-
tive semideﬁnite. Flexible constructions of φ(·) enable us
to fulﬁll different variational model evidence lower bounds
(ELBO) for large-scale inference, which we will discuss
more in Section 5.

Given f , we sample the observed output y from the
isotropic Gaussian model p(y|f ) = N (y|f , β−1I). The
joint distribution of our augmented model is then given by

p(y, f , w|X)

=N (w|0, I)N (f |Φw, Knn − ΦΦ(cid:62))N (y|f , β−1I). (7)

This model is equivalent to the original GP regression—
when we marginalize out w, we recover the joint distribu-
tion in (1); we can further marginalize out f to recover the
model evidence in (2). Note that our model is distinct from
the traditional weight space view of GP regression (Ras-
mussen & Williams, 2006): the feature mapping φ(·) is not
equivalent to the underlying (nonlinear) feature mapping
induced by the covariance function (see more discussions
in Section 5). Instead, φ(·) is deﬁned for computational
purpose only—that is, to construct a tractable variational
evidence lower bound (ELBO), shown as follows.

Now, we derive the tractable ELBO based on the weight
space augmented model in (7). The derivation is similar
to (Titsias, 2009; Hensman et al., 2013). Speciﬁcally, we
ﬁrst consider the conditional distribution p(y|w). Because
log p(y|w) = log (cid:82) p(y|f )p(f |w)df = log(cid:104)p(y|f )(cid:105)p(f |w),
where (cid:104)·(cid:105)p(θ) denotes the expectation under the distribu-
tion p(θ), we can use Jensen’s inequality to obtain a lower
bound:

log p(y|w) = log(cid:104)p(y|f )(cid:105)p(f |w) ≥ (cid:104)log p(y|f )(cid:105)p(f |w)

=

n
(cid:88)

i=1

log N (yi|φ(cid:62)(xi)w, β−1) −

(8)

β
2

˜kii,

where ˜kii is the ith diagonal element of Knn − ΦΦ(cid:62).

Next, we introduce a variational posterior q(w) to construct

the variational lower bound of the log model evidence,

log p(y) = log

(cid:28) p(y|w)p(w)
q(w)

(cid:29)

q(w)

≥ (cid:104)log p(y|w)(cid:105)q(w) − KL(q(w)(cid:107)p(w)).

(9)

where KL(·(cid:107)·) is the Kullback–Leibler divergence. Re-
placing log p(y|w) in (9) by the right side of (8), we obtain
the following lower bound,

log p(y) ≥ L = −KL (q(w)(cid:107)p(w))

+

n
(cid:88)

i=1

(cid:10)log N (yi|φ(cid:62)(xi)w, β−1)(cid:11)

q(w) −

β
2

˜kii.

(10)

Note that this is a variational lower bound: the equality is
obtained when ΦΦT = Knn and q(w) = p(w|y). To
achieve equality, we need to set m = n and have φ(·)
map the d-dimensional input into an n-dimensional feature
space. In order to reduce the computational cost, however,
we can restrict m to be very small and choose any family
of mappings φ(·) that satisfy Knn − ΦΦ(cid:62) (cid:23) 0. The ﬂex-
ible choices of φ(·) allows us to explore different approx-
imations in a uniﬁed variational framework. For example,
in our practice, we introduce an m × d inducing matrix
Z = [z1, . . . , zm](cid:62) and deﬁne

φ(x) = L(cid:62)km(x),

(11)

i.e.,

mm = LL(cid:62).
mmK(cid:62)

where km(x) = [k(x, z1), . . . , k(x, zm)](cid:62) and L is
the lower triangular Cholesky factorization of the inverse
kernel matrix over Z,
[Kmm]i,j = k(zi, zj) and
It can be easily veriﬁed that ΦΦ(cid:62) =
K−1
KnmK−1
nm, where Knm is the cross kernel matrix
between X and Z, i.e., [Knm]ij = k(xi, zj). Therefore
Knn − ΦΦ is always positive semideﬁnite, because it can
be viewed as a Schur complement of Knn in the block ma-
(cid:105)
. We discuss other choices of φ(·) in
trix
Section 5.

(cid:104) Kmm K(cid:62)
nm
Knm Knn

4. Delayed Proximal Gradient Optimization

for ADVGP

A major advantage of our variational GP framework is the
capacity of using the asynchronous, delayed proximal gra-
dient optimization supported by PARAMETERSERVER (Li
et al., 2014b), with convergence guarantees and scalabil-
ity to huge data. PARAMETERSERVER is a well-known,
general platform for asynchronous machine learning algo-
rithms for extremely large applications. It has a bipartite ar-
chitecture where the computing nodes are partitioned into
two classes: server nodes store the model parameters and
worker nodes the data. PARAMETERSERVER assumes the
learning procedure minimizes a non-convex loss function

Asynchronous Distributed Variational Gaussian Process for Regression

with the following composite form:

L(θ) =

(cid:88)r

k=1

Gk(θ) + h(θ)

(12)

where θ are the model parameters. Here Gk(θ) is a (possi-
bly non-convex) function associated with the data in worker
k and therefore can be calculated by worker k indepen-
dently; h(θ) is a convex function with respect to θ.

To efﬁciently minimize the loss function in (12), PARAM-
ETERSERVER uses a delayed proximal gradient updating
method to perform asynchronous optimization. To illus-
trate it, let us ﬁrst review the standard proximal gradient de-
scent. Speciﬁcally, for each iteration t, we ﬁrst take a gradi-
ent descent step according to (cid:80)
k Gk(θ) and then perform
a proximal operation to project θ toward the minimum
of h(·), i.e., θ(t+1) = Proxγt[θ(t) − γt
k ∇Gk(θ(t))],
where γt is the step size. The proximal operation is deﬁned
as

(cid:80)

Proxγt[θ] = argmin

h(θ∗) +

(cid:107)θ∗ − θ(cid:107)2
2.

(13)

θ∗

1
2γt

The standard proximal gradient descent guarantees to ﬁnd
a local minimum solution. However, the computation is
inefﬁcient, even in parallel:
in each iteration, the server
nodes wait until the worker nodes ﬁnish calculating each
∇Gk(θ(t)); then the workers wait for the servers to ﬁnish
the proximal operation. This synchronization wastes much
time and computational resources. To address this issue,
PARAMETERSERVER uses a delayed proximal gradient up-
dating approach to implement asynchronous computation.

(cid:80)

Speciﬁcally, we set a delay limit τ ≥ 0. At any itera-
tion t, the servers do not enforce all the workers to ﬁn-
ish iteration t; instead, as long as each worker has ﬁn-
ished an iteration no earlier than t − τ , the servers will
proceed to perform the proximal updates, i.e., θ(t+1) =
Proxγt[θ(t)−γt
k ∇Gk(θ(tk))] (t−τ ≤ tk ≤ t), and no-
tify all the workers with the new parameters θ(t+1). Once
received the updated parameters, the workers compute and
push the local gradient to the servers immediately. Obvi-
ously, this delay mechanism can effectively reduce the wait
between the server and worker nodes. By setting differ-
ent τ , we can adjust the degree of the asynchronous com-
putation: when τ = 0, we have no asynchronization and
return to the standard, synchronous proximal gradient de-
scent; when τ = ∞, we are totally asynchronous and there
is no wait at all.

A highlight is that given the composite form of the non-
convex loss function in (12), the above asynchronous de-
layed proximal gradient descent guarantees to converge ac-
cording to Theorem 1.
Theorem 1. (Li et al., 2013) Assume the gradient of the
function Gk is Lipschitz continuous, that is, there is a con-
stant Ck such that (cid:107)∇Gk(θ) − ∇Gk(θ(cid:48))(cid:107) ≤ Ck||θ − θ(cid:48)||

for any θ, θ(cid:48), and k = 1, ..., r. Deﬁne C = (cid:80)r
k=1 Ck.
Also, assume we allow a maximum delay for the updates by
τ and a signiﬁcantly-modiﬁed ﬁlter on pulling the param-
eters with threshold O(t−1). For any (cid:15) > 0, the delayed
proximal gradient descent converges to a stationary point
if the learning rate γt satisﬁes γt ≤ ((1 + τ )C + (cid:15))−1.

Now, let us return to our variational GP framework. A
major beneﬁt of our framework is that the negative varia-
tional evidence lower bound (ELBO) for GP regression has
the same composite form as (12). Thereby we can apply
the asynchronous proximal gradient descent for GP infer-
ence on PARAMETERSERVER. Speciﬁcally, we explicitly
assume q(w) = N (w|µ, Σ) and obtain the negative varia-
tional ELBO (see (10))

(cid:88)n

−L =

gi + h

i=1

where

gi = − log N (yi|φ(cid:62)(xi)µ, β−1) +

φ(cid:62)(xi)Σφ(xi)

β
2

+

˜kii,

β
2
(cid:0)− ln |Σ| − m + tr(Σ) + µ(cid:62)µ(cid:1) .

h =

1
2

Instead of directly updating Σ, we consider U, the upper
triangular Cholesky factor of Σ, i.e., Σ = U(cid:62)U. This not
only simpliﬁes the proximal operation but also ensures the
positive deﬁniteness of Σ during computation. The partial
derivatives of gi with respect to µ and U are

∂gi
∂µ
∂gi
∂U

= β (cid:0)−yiφ(xi) + φ(xi)φ(cid:62)(xi)µ(cid:1) ,

= βtriu[Uφ(xi)φ(cid:62)(xi)],

where triu[·] denotes the operator that keeps the upper tri-
angular part of a matrix but leaves any other element zero.It
can be veriﬁed that the partial derivatives of gi with respect
to µ and U are Lipschitz continuous and h is also convex
with respect to µ and U. According to Theorem 1, mini-
mizing −L (i.e., maximizing L) with respect to the varia-
tional parameters, µ and U, using the asynchronous proxi-
mal gradient method can guarantee convergence. For other
parameters, such as kernel parameters and inducing points,
h is simply a constant. As a result, the delayed proximal
updates for these parameters reduce to the delayed gradient
descent optimization such as in (Agarwal & Duchi, 2011).

We now present the details of ADVGP implementation on
PARAMETERSERVER. We ﬁrst partition the data for r
workers and allocate the model parameters (such as the ker-
nel parameters, the parameters of q(w) and the inducing
points Z) to server nodes. At any iteration t, the server
nodes aggregate all the local gradients and perform the

(14)

(15)

(16)

(17)

Asynchronous Distributed Variational Gaussian Process for Regression

i∈Dk

∇g(tk)
i

k = (cid:80)

proximal operation in (13), as long as each worker k has
computed and pushed the local gradient on its own data
subset Dk for some prior iteration tk (t − τ ≤ tk ≤ t),
∇G(tk)
. Note that the proximal opera-
tion is only performed for the parameters of q(w), namely
µ and U; since h is constant for the other model parame-
ters, such as the kernel parameters and the inducing points,
their gradient descent updates remain unchanged. Mini-
mizing (13) by setting the derivatives to zero, we obtain the
proximal updates for each element in µ and U:

µ(t+1)
i
U (t+1)
ij

= µ(cid:48)(t+1)
i
= U (cid:48)(t+1)
ij

/(1 + γt),

/(1 + γt),
(cid:113)

U (cid:48)(t+1)
ii

+

U (t+1)
ii

=

(U (cid:48)(t+1)
ii
2(1 + γt)

(18)

(19)

)2 + 4(1 + γt)γt

,

(20)

where

µ(cid:48)(t+1)
i

= µ(t)

i − γt

U (cid:48)(t+1)
ij

= U (t)

ij − γt

(cid:88)r

k=1

(cid:88)r

k=1

,

∂G(tk)
k
∂µ(tk)
i
∂G(tk)
k
∂U (tk)
ij

.

The proximal operation comprises of element-wise, closed-
form computations, therefore making the updates of the
variational posterior q(w) highly parallelizable and efﬁ-
cient. The gradient calculation for the other parameters,
including the kernel parameters and inducing points, al-
though quite complicated, is pretty standard and we give
the details in the supplementary material (Appendix A). Fi-
nally, ADVGP is summarized in Algorithm 1.

Algorithm 1 Delayed Proximal Gradient for ADVGP
Worker k at iteration tk
1: Block until servers have new parameters ready.
2: Pull the parameters from servers and update the current

k

on data Dk.

k
to servers.

version (or iteration) tk.
3: Compute the gradient ∇G(tk)
4: Push the gradient ∇G(tk)
Servers at iteration t
1: if Each worker k completes iteration tk ≥ t − τ then
Aggregate gradients to obtain ∇G(t) = (cid:80) ∇G(tk)
2:
Update µ and U using (18), (19) and (20).
3:
Update the other parameters using gradient descent.
4:
Notify all blocked workers of the new parameters
5:
and the version (i.e., t + 1).
Proceed to iteration t + 1.

k

.

6:
7: end if

5. Discussion and Related Work

Exact GP inference requires computing the full covari-
ance matrix (and its inverse), and therefore is infeasible
for large data. To reduce the computational cost, many
sparse GP inference methods use a low-rank structure to
approximate the full covariance. For example, Williams
& Seeger (2001); Peng & Qi (2015) used the Nystr¨om ap-
proximation; Bishop & Tipping (2000) used relevance vec-
tors, constructed from covariance functions evaluated on a
small subset of the training data. A popular family of sparse
GPs introduced a small set of inducing inputs and targets,
viewed as statistical summary of the data, and deﬁne an ap-
proximate model by imposing some conditional indepen-
dence between latent functions given the inducing targets;
the inference of the inexact model is thereby much easier.
Qui˜nonero-Candela & Rasmussen (2005) provided a uni-
ﬁed view of those methods, such as SoR (Smola & Bartlett,
2001), DTC (Seeger et al., 2003), PITC (Schwaighofer &
Tresp, 2003) and FITC (Snelson & Ghahramani, 2005).

Despite the success of those methods, their inference pro-
cedures often exhibit undesirable behaviors, such as under-
estimation of the noise and clumped inducing inputs (Bauer
et al., 2016). To obtain a more favorable approximation,
Titsias (2009) proposed a variational sparse GP framework,
where the approximate posteriors and the inducing inputs
are both treated as variational parameters and estimated by
maximizing a variational lower bound of the true model
evidence. The variational framework is less prone to over-
ﬁtting and often yields a better inference quality (Titsias,
2009; Bauer et al., 2016). Based on Titsias (2009)’s work,
Hensman et al. (2013) developed a stochastic variational in-
ference for GP (SVIGP) by parameterizing the variational
distributions explicitly. Gal et al. (2014) reparameterized
the bound of Titsias (2009) and developed a distributed op-
timization algorithm with MAPREDUCE framework. Fur-
ther, Dai et al. (2014) developed a GPU acceleration using
the similar formulation, and Matthews et al. (2017) devel-
oped GPﬂow library, a TensorFlow implementation that ex-
ploit GPU hardwares.

To further enable GPs on real-world, extremely large appli-
cations, we proposed a new variational GP framework us-
ing a weight space augmentation. The proposed augmented
model, introducing an extra random weight vector w with
standard normal prior, is distinct from the traditional GP
weight space view (Rasmussen & Williams, 2006) and the
recentering tricks used in GP MCMC inferences (Murray &
Adams, 2010; Filippone et al., 2013; Hensman et al., 2015).
In the conventional GP weight space view, the weight vec-
tor is used to combine the nonlinear feature mapping in-
duced by the covariance function and therefore can be inﬁ-
nite dimensional; in the recentering tricks, the weight vec-
tor is used to reparameterize the latent function values, to

Asynchronous Distributed Variational Gaussian Process for Regression

dispose of the dependencies on the hyper-parameters, and
to improve the mixing rate. In our framework, however, the
weight vector w has a ﬁxed, much smaller dimension than
the number of samples (m (cid:28) n), and is used to introduce
an extra feature mapping φ(·) — φ(·) plays the key role
to construct a tractable variational model evidence lower
bound (ELBO) for large scale GP inference.

The advantages of our framework are mainly twofold.
First, by using the feature mapping φ(·), we are ﬂexible
to incorporate various low rank structures, and meanwhile
still cast them into a principled variational inference frame-
work. For example, in addition to (11), we can deﬁne

(21)

φ(x) = diag(λ)−1/2Q(cid:62)km(x),
where Q are λ are eigenvectors and eigenvalues of Kmm.
Then φ(·) is actually a scaled Nystr¨om approximation for
eigenfunctions of the kernel used in GP regression. This
actually fulﬁlls a variational version of the EigenGP ap-
proximation (Peng & Qi, 2015). Further, we can extend
(21) by combining q Nystr¨om approximations. Suppose
we have q groups of inducing inputs {Z1, . . . , Zq}, where
each Zl consists of ml inducing inputs. Then the feature
mapping can be deﬁned by

φ(x) =

q−1/2diag(λl)−1/2Q(cid:62)

l kml (x),

(22)

q
(cid:88)

l=1

where λl and Ql are the eigenvalues and eigenvectors of
the covariance matrix for Zl. This leads to a variational
sparse GP based on the ensemble Nystr¨om approxima-
tion (Kumar et al., 2009). It can be trivially veriﬁed that
both (21) and (22) satisﬁed Knn − ΦΦ(cid:62) (cid:23) 0 in (6).

In addition, we can also relate ADVGP to GP models with
pre-deﬁned feature mappings, for instance, Relevance Vec-
tor Machines (RVMs) (Bishop & Tipping, 2000), by setting
φ(x) = diag(α1/2)km(x), where α is an m × 1 vector.
Note that to ensure Knn −ΦΦ(cid:62) (cid:23) 0, we have to add some
constraint over the range of each αi in α.

The second major advantage of ADVGP is that our varia-
tional ELBO is consistent with the composite non-convex
loss form favored by PARAMETERSERVER, therefore we
can utilize the highly efﬁcient, distributed asynchronous
proximal gradient descent in PARAMETERSERVER to scale
up GPs to extremely large applications (see Section 6.3).
Furthermore,
the simple element-wise and closed-form
proximal operation enables exceedingly efﬁcient and par-
allelizable variational posterior update on the server side.

6. Experiments

6.1. Predictive Performance

First, we evaluated the inference quality of ADVGP in
terms of predictive performance. To this end, we used the

US Flight data1 (Hensman et al., 2013), which recorded the
arrival and departure time of the USA commercial ﬂights
between January and April in 2008. We performed two
groups of tests: in the ﬁrst group, we randomly chose 700K
samples for training; in the second group, we randomly se-
lected 2M training samples. Both groups used 100K sam-
ples for testing. We ensured that the training and testing
data are non-overlapping.

We compared ADVGP with two existing scalable varia-
tional inference algorithms: SVIGP (Hensman et al., 2013)
and DistGP (Gal et al., 2014). SVIGP employs an online
training, and DistGP performs a distributed synchronous
variational inference. We ran all the methods on a com-
puter node with 16 CPU cores and 64 GB memory. While
SVIGP uses a single CPU core, DistGP and ADVGP use
all the CPU cores to perform parallel inference. We used
ARD kernel for all the methods, with the same initializa-
tion of the kernel parameters. For SVIGP, we set the mini-
batch size to 5000, consistent with (Hensman et al., 2013).
For DistGP, we tested two optimization frameworks:
lo-
cal gradient descent (DistGP-GD) and L-BFGS (DistGP-
LBFGS). For ADVGP, we initialized µ = 0, U = I, and
used ADADELTA (Zeiler, 2012) to adjust the step size for
the gradient descent before the proximal operation. To
choose an appropriate delay τ , we sampled another set of
training and test data, based on which we tuned τ from
{0, 8, 16, 24, 32, 40}. These tunning datasets do not over-
lap the test data in the evaluation. Note that when τ = 0,
the computation is totally synchronous; larger τ results in
more asynchronous computation. We chose τ = 32 as it
produced the best performance on the tunning datasets.

Table 1 and Table 2 report the root mean square errors (RM-
SEs) of all the methods using different numbers of inducing
points, i.e., m ∈ {50, 100, 200}. As we can see, ADVGP
exhibits better or comparable prediction accuracy in all the
cases. Therefore, while using asynchronous computation,
ADVGP maintains the same robustness and quality for in-
ference. Furthermore, we examined the prediction accu-
racy of each method along with the training time, under
the settings m ∈ {100, 200}. Figure 1 shows that during
the same time span, ADVGP achieves the highest perfor-
mance boost (i.e., RMSE is reduced faster than the com-
peting methods), which demonstrates the efﬁciency of AD-
VGP. It is interesting to see that in a short period of time
since the beginning, SVIGP reduces RMSE as fast as AD-
VGP; however, after that, RMSE of SVIGP is constantly
larger than ADVGP, exhibiting an inferior performance. In
addition, DistGP-LBFGS converges earlier than both AD-
VGP and SVIGP. However, RMSE of DistGP-LBFGS is
larger than both ADVGP and SVIGP at convergence. This
implies that the L-BFGS optimization converged to a sub-

1http://stat-computing.org/dataexpo/2009/

Asynchronous Distributed Variational Gaussian Process for Regression

(A) n = 700K, m = 100

(B) n = 700K, m = 200

(C) n = 2M, m = 100

(D) n = 2M, m = 200

Figure 1. Root mean square errors (RMSEs) for US ﬂight data as a function of training time.

optimal solution.

Table 1. Root mean square errors (RMSEs) for 700K/100K US
Flight data.

Method
Prox GP
GD Dist GP
LBFG Dist GP
SVIGP

m = 50 m = 100 m = 200
32.6143
32.7543
32.9080
32.6521
32.8069
32.9411
32.8729
33.2263
33.0707
32.7802
32.9499
33.1054

Table 2. RMSEs for 2M/100K US Flight data.

Method
Prox GP
GD Dist GP
LBFG Dist GP
SVIGP

m = 50 m = 100 m = 200
35.7017
35.8347
36.1156
35.7971
35.9487
36.0142
36.0749
36.1676
35.9809
35.8599
35.9517
36.2019

In our experiment,

We also studied how the delay limit τ affects the perfor-
mance of ADVGP. Practically, when many machines are
used, some worker may always be slower than the oth-
ers due to environmental factors, e.g., unbalanced work-
loads. To simulate this scenario, we intentionally intro-
duced a latency by assigning each worker a random sleep
time of 0, 10 or 20 seconds at initialization; hence a
worker would pause for its given sleep time before each
iteration.
the average per-iteration
running time was only 0.176 seconds; so the fastest
worker could be hundreds of iterations ahead of the slow-
est one in the asynchronous setting. We examined τ =
0, 5, 10, 20, 40, 80, 160 and plotted RMSEs as a function of
time in Figure 2. Since RMSE of the synchronous case
(τ = 0) is much larger than the others, we do not show it
in the ﬁgure. When τ is larger, ADVGP’s performance is
more ﬂuctuating. Increasing τ , we ﬁrst improved the pre-
diction accuracy due to more efﬁcient CPU usage; however,
later we observed a decline caused by the excessive asyn-
chronization that impaired the optimization. Therefore, to
use ADVGP for workers at various paces, we need to care-
fully choose the appropriate delay limit τ .

Figure 2. Root mean square errors (RMSEs) as a function of time
for different delay limits τ

6.2. Scalability

Next, we examined the scalability of our asynchronous
inference method, ADVGP. To this end, we used the
700K/100K dataset and compared with the synchronous
inference algorithm DistGP (Gal et al., 2014). For a fair
comparison, we used the local gradient descent version of
DistGP, i.e., DistGP-GD. We conducted two experiments
on 4 c4.8xlarge instances of Amazon EC2 cloud, where
we set the number of inducing points m = 100.
In the
ﬁrst experiment, we ﬁxed the size of the training data, and
increased the number of CPU cores from 4 to 128. We
examined the per-iteration running time of both ADVGP
and DistGP-GD. Figure 3(A) shows that while both de-
creasing with more CPU cores, the per-iteration running
time of ADVGP is much less than that of DistGP-GD. This
demonstrates the advantage of ADVGP in computational
efﬁciency.
In addition, the per-iteration running time of
ADVGP decays much more quickly than that of DistGP-
GD as the number of cores approaches 128. This implies
that even the communication cost becomes dominant, the
asynchronous mechanism of ADVGP still effectively re-
duces the latency and maintains a high usage of the com-
putational power. In the second experiment, we simultane-
ously increased the number of cores and the size of train-
ing data. We started from 87.5K samples and 16 cores

0100020003000400032.533.534.535.5Time (s)RMSE  ADVGPDistGP−GDDistGP−LBFGSSVIGP02500500075001000032.533.534.535.5Time (s)RMSE  ADVGPDistGP−GDDistGP−LBFGSSVIGP00.511.52x 10435.536.537.538.5Time (s)RMSE  ADVGPDistGP−GDDistGP−LBFGSSVIGP01.534.56x 10435.536.537.538.5Time (s)RMSE  ADVGPDistGP−GDDistGP−LBFGSSVIGP020004000600080001000032.53333.53434.535Time (s)RMSE  τ =5τ =10τ =20τ =40τ =80τ =160Asynchronous Distributed Variational Gaussian Process for Regression

and gradually increased them to 700K samples and 128
cores. As shown in Figure 3(B), the average per-iteration
time of DistGP-GD grows linearly; in contrast, the aver-
age per-iteration time of ADVGP stays almost constant.
We speculate that without synchronous coordination, AD-
VGP can fully utilize the network bandwidth so that the
increased amount of messages, along with the growth of
the data size, affect little the network communication efﬁ-
ciency. This demonstrates the advantage of asynchronous
inference from another perspective.

processes. The delay limit τ was selected as 20. We used
Vowpal Wabbit to train a linear regression model, with de-
fault settings. We also took the average traveling time over
the training data to obtain a simple mean prediction. In Fig-
ure 4(A), we report RMSEs of the linear regression and the
mean prediction, as well as the GP regression along with
running time. As we can see, ADVGP greatly outperforms
the competing methods. Only after 6 minutes, ADVGP
has improved RMSEs of the linear regression and the mean
prediction by 9% and 41%, respectively; the improvements
continued for about 30 minutes. Finally, ADVGP reduced
the RMSEs of the linear regression and the mean prediction
by 22% and 49%, respectively. The RMSEs are {ADVGP:
333.4, linear regression: 424.8, mean-prediction: 657.7}.

(A)

(B)

Figure 3. Scalability tests on 700K US ﬂight data.
(A) Per-
iteration time as a function of available cores in log-scale. (B)
Per-iteration time when scaling the computational resources pro-
portionally to dataset size.

6.3. NYC Taxi Traveling Time Prediction

Finally, we applied ADVGP for an extremely large prob-
lem: the prediction of the taxi traveling time in New York
city. We used the New York city yellow taxi trip dataset
2, which consist of 1.21 billions of trip records from Jan-
uary 2009 to December 2015. We excluded the trips that
are outside the NYC area or more than 5 hours. The aver-
age traveling time is 764 seconds and the standard deriva-
tion is 576 seconds. To predict the traveling time, we used
the following 9 features: time of the day, day of the week,
day of the month, month, pick-up latitude, pick-up longi-
tude, drop-off latitude, drop-off longitude, and travel dis-
tance. We used Amazon EC2 cloud, and ran ADVGP on
multiple Amazon c4.8xlarge instances, each with 36 vC-
PUs and 60 GB memory. We compared with the linear
regression model implemented in Vowpal Wabbit (Agarwal
et al., 2014). Vowpal Wabbit is a state-of-the-art large scale
machine learning software package and has been used in
many industrial-scale applications, such as click-through-
rate prediction (Chapelle et al., 2014).

We ﬁrst randomly selected 100M training samples and
500K test samples. We set m = 50 and initialized the
inducing points as the the K-means cluster centers from a
subset of 2M training samples. We trained a GP regression
model with ADVGP, using 5 Amazon instances with 200

2http://www.nyc.gov/html/tlc/html/about/

trip_record_data.shtml

(A) 100M training samples

(B) 1B training samples

Figure 4. RMSE as a function of training time on NYC Taxi Data.

To further verify the advantage of GP regression in ex-
tremely large applications, we used 1B training and 1M
testing samples. We used 50 inducing points, initialized
by the K-means cluster centers from a 1M training subset.
We ran ADVGP using 28 Amazon instances with 1000 pro-
cesses and chose τ = 100. As shown in Figure 4(B), the
RMSE of GP regression outperforms the linear models by a
large margin. After 12 minutes, ADVGP has improved the
RMSEs of the linear regression and the mean prediction by
8% and 40%, respectively; the improvement kept growing
for about 1.5 hours. At the convergence, ADVGP outper-
forms the linear regression and the mean prediction by 15%
and 44%, respectively. The RMSEs are {ADVGP: 309.7,
linear regression: 362.8, mean-prediction: 556.3}. In addi-
tion, the average per-iteration time of ADVGP is only 0.21
seconds. These results conﬁrm the power of the nonlinear
regression in extremely large real-world scenarios, compar-
ing with linear models, while the latter are much easier to
be scaled up and hence more popular.

7. Conclusion

We have presented ADVGP, an asynchronous, distributed
variational inference algorithm for GP regression, which
enables real-world extremely large applications. ADVGP
is based on a novel variational GP framework, which allows
ﬂexible construction of low rank approximations and can
relate to many sparse GP models.

Number of cores110100Time per iteration (s)10-1100101ADVGPDistGP-GDNumber of coresDataset size×10502468Time per iteration (s)00.511.52ADVGPDistGP-GD03264961280200040006000300400500600700Time (s)RMSE  ADVGPMeanVowpal Wabbit0200040006000300400500600Time (s)RMSE  ADVGPMeanVowpal WabbitAsynchronous Distributed Variational Gaussian Process for Regression

References

Agarwal, Alekh and Duchi, John C. Distributed delayed
stochastic optimization. In Advances in Neural Informa-
tion Processing Systems 24, pp. 873–881. Curran Asso-
ciates, Inc., 2011.

Agarwal, Alekh, Chapelle, Oliveier, Dud´ık, Miroslav, and
Langford, John. A reliable effective terascale linear
learning system. Journal of Machine Learning Research,
15:1111–1133, 2014.

Bauer, Matthias, van der Wilk, Mark, and Rasmussen,
Carl Edward. Understanding probabilistic sparse Gaus-
sian process approximations. In Advances in Neural In-
formation Processing Systems 29, pp. 1525–1533, 2016.

Bishop, Christopher M. and Tipping, Michael E. Varia-
tional relevance vector machines. In Proceedings of the
16th Conference in Uncertainty in Artiﬁcial Intelligence
(UAI), 2000.

Chapelle, Olivier, Manavoglu, Eren, and Rosales, Romer.
Simple and scalable response prediction for display ad-
vertising. ACM Transactions on Intelligent Systems and
Technology (TIST), 5(4):61:1–61:34, December 2014.
ISSN 2157-6904.

Dai, Zhenwen, Damianou, Andreas, Hensman, James, and
Lawrence, Neil D. Gaussian process models with paral-
lelization and GPU acceleration. In NIPS Workshop on
Software Engineering for Machine Learning, 2014.

Deisenroth, Marc and Ng, Jun W. Distributed Gaussian
In Proceedings of the 32nd International
processes.
Conference on Machine Learning, pp. 1481–1490, 2015.

Filippone, Maurizio, Zhong, Mingjun, and Girolami, Mark.
A comparative evaluation of stochastic-based inference
methods for gaussian process models. Machine Learn-
ing, 93(1):93–114, 2013.

Gal, Yarin, van der Wilk, Mark, and Rasmussen, Carl. Dis-
tributed variational inference in sparse Gaussian process
In Advances in
regression and latent variable models.
Neural Information Processing Systems 27, pp. 3257–
3265, 2014.

Hensman, James, Fusi, Nicolo, and Lawrence, Neil D.
In Proceedings of
Gaussian processes for big data.
the Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), 2013.

Hensman, James, Matthews, Alexander G, Filippone, Mau-
rizio, and Ghahramani, Zoubin. Mcmc for variationally
sparse gaussian processes. In Advances in Neural Infor-
mation Processing Systems, pp. 1648–1656, 2015.

Kumar, Sanjiv, Mohri, Mehryar, and Talwalkar, Ameet.
Ensemble Nystr¨om method. In Bengio, Y., Schuurmans,
D., Lafferty, J. D., Williams, C. K. I., and Culotta, A.
(eds.), Advances in Neural Information Processing Sys-
tems 22, pp. 1060–1068. Curran Associates, Inc., 2009.

Li, Mu, Andersen, David G, and Smola, Alexander J. Dis-
In NIPS
tributed delayed proximal gradient methods.
Workshop on Optimization for Machine Learning, 2013.

Li, Mu, Andersen, David G, Park, Jun Woo, Smola,
Alexander J, Ahmed, Amr, Josifovski, Vanja, Long,
James, Shekita, Eugene J, and Su, Bor-Yiing. Scal-
ing distributed machine learning with the parameter
server. In 11th USENIX Symposium on Operating Sys-
tems Design and Implementation (OSDI 14), pp. 583–
598, 2014a.

Li, Mu, Andersen, David G, Smola, Alexander, and Yu,
Kai. Communication efﬁcient distributed machine learn-
In Neural Information
ing with the parameter server.
Processing Systems 27, 2014b.

Matthews, Alexander G de G, van der Wilk, Mark, Nick-
son, Tom, Fujii, Keisuke, Boukouvalas, Alexis, Le´on-
Villagr´a, Pablo, Ghahramani, Zoubin, and Hensman,
James. GPﬂow: A Gaussian process library using Ten-
Journal of Machine Learning Research, 18
sorFlow.
(40):1–6, 2017.

Murray, Iain and Adams, Ryan P. Slice sampling covari-
ance hyperparameters of latent gaussian models. In Ad-
vances in Neural Information Processing Systems 24, pp.
1732–1740, 2010.

Peng, Hao and Qi, Yuan. EigenGP: Sparse Gaussian pro-
cess models with adaptive eigenfunctions. In Proceed-
ings of the 24th International Joint Conference on Arti-
ﬁcial Intelligence, pp. 3763–3769, 2015.

Qui˜nonero-Candela, Joaquin and Rasmussen, Carl Ed-
ward. A unifying view of sparse approximate Gaussian
process regression. The Journal of Machine Learning
Research, 6:1939–1959, 2005.

Rasmussen, Carl E. and Williams, Christopher K. I. Gaus-
sian Processes for Machine Learning. The MIT Press,
2006.

Schwaighofer, Anton and Tresp, Volker. Transductive and
inductive methods for approximate Gaussian process re-
gression. In Advances in Neural Information Processing
Systems 15, pp. 953–960. MIT Press, 2003.

Seeger, Matthias, Williams, Christopher, and Lawrence,
Neil. Fast forward selection to speed up sparse Gaussian
process regression. In Proceedings of the Ninth Interna-
tional Workshop on Artiﬁcial Intelligence and Statistics,
2003.

Asynchronous Distributed Variational Gaussian Process for Regression

Smola, Alexander J. and Bartlett, Peter L. Sparse greedy
Gaussian process regression. In Advances in Neural In-
formation Processing Systems 13. MIT Press, 2001.

Snelson, Edward and Ghahramani, Zoubin. Sparse Gaus-
In Advances in
sian processes using pseudo-inputs.
Neural Information Processing Systems, pp. 1257–1264,
2005.

Titsias, Michalis K. Variational learning of inducing vari-
In Proceedings of
ables in sparse Gaussian processes.
the Twelfth International Conference on Artiﬁcial Intel-
ligence and Statistics, pp. 567–574, 2009.

Williams, Christopher and Seeger, Matthias. Using the
In Ad-
Nystr¨om method to speed up kernel machines.
vances in Neural Information Processing Systems 13, pp.
682–688, 2001.

Zeiler, Matthew D. ADADELTA: an adaptive learning rate

method. arXiv:1212.5701, 2012.

