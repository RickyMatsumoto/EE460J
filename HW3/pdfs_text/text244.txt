Behavior Policy Gradient Supplemental Material

Josiah P. Hanna 1 Philip S. Thomas 2 3 Peter Stone 1 Scott Niekum 1

A. Proof of Theorem 1

In Appendix A, we give the full derivation of our primary theoretical contribution — the importance-sampling (IS) variance
gradient. We also present the variance gradient for the doubly-robust (DR) estimator.

We ﬁrst derive an analytic expression for the gradient of the variance of an arbitrary, unbiased off-policy policy evalua-
tion estimator, OPE(H, θ). Importance-sampling is one such off-policy policy evaluation estimator. From our general
derivation we derive the gradient of the variance of the IS estimator and then extend to the DR estimator.

A.1. Variance Gradient of an Unbiased Off-Policy Policy Evaluation Method

We ﬁrst present a lemma from which ∂

∂θ MSE[IS(H, θ)] and ∂

∂θ MSE[DR(H, θ)] can both be derived.

Lemma 1 gives the gradient of the mean squared error (MSE) of an unbiased off-policy policy evaluation method.

∂
∂θ

MSE[OPE(H, θ)] = E

(cid:34)
OPE(H, θ)2(

L
(cid:88)

∂
∂θ

t=0

log πθ(At|St)) +

OPE(H, θ)2

H ∼ πθ

∂
∂θ

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Proof. We begin by decomposing Pr(H|π) into two components—one that depends on π and the other that does not. Let

Lemma 1.

and

wπ(H) :=

π(At|St),

L
(cid:89)

t=0

p(H) := Pr(H|π)/wπ(H),

for any π such that H ∈ supp(π) (any such π will result in the same value of p(H)). These two deﬁnitions mean that
Pr(H|π) = p(H)wπ(H).

The MSE of the OPE estimator is given by:

MSE[OPE(H, θ)] = Var[OPE(H, θ)] + (E[OPE(H, θ)] − ρ(πe))2
(cid:125)

(cid:124)

.

(cid:123)(cid:122)
bias2

Since the OPE estimator is unbiased, i.e., E[OPE(H, θ)] = ρ(πe), the second term is zero and so:

MSE(OPE(H, θ)) = Var(OPE(H, θ))
=E (cid:2)OPE(H, θ)2(cid:12)
=E (cid:2)OPE(H, θ)2(cid:12)

(cid:12)H ∼ πθ
(cid:12)H ∼ πθ

(cid:3) − E[OPE(H, θ)|H ∼ πθ]2
(cid:3) − ρ(πe)2

To obtain the MSE gradient, we differentiate MSE(OPE(H, θ)) with respect to θ:

Behavior Policy Gradient: Supplemental Material

∂
∂θ

MSE[OPE(H, θ)] =

(cid:2)E (cid:2)OPE(H, θ)2(cid:12)

(cid:12)H ∼ πθ

(cid:3) − ρ(πe)2(cid:3)

EH∼πθ

(cid:2)OPE(H, θ)2(cid:3)

Pr(H|θ) OPE(H, θ)2

(cid:88)

H

∂
∂θ
∂
∂θ
∂
∂θ

(cid:88)

H
(cid:88)

H

=

=

=

=

Pr(H|θ)

∂
∂θ

OPE(H, θ)2 + OPE(H, θ)2 ∂
∂θ

Pr(H|θ)

Pr(H|θ)

OPE(H, θ)2 + OPE(H, θ)2p(H)

(1)

∂
∂θ

wπθ (H)

∂
∂θ

Consider the last factor of the last term in more detail:

∂
∂θ

wπθ (H) =

πθ(At|St)

∂
∂θ

L
(cid:89)

t=0

(cid:32) L
(cid:89)

t=0

(a)=

πθ(At|St)

(cid:33) (cid:32) L
(cid:88)

t=0

(cid:33)

∂
∂θ πθ(At|St)
πθ(At|St)

=wπθ (H)

log (πθ(At|St)) ,

L
(cid:88)

t=0

∂
∂θ

where (a) comes from the multi-factor product rule. Continuing from (1) we have that:

∂
∂θ

MSE(OPE(H, θ)) =E

OPE(H, θ)2

log (πθ(At|St)) +

OPE(H, θ)2

(cid:34)

L
(cid:88)

t=0

∂
∂θ

∂
∂θ

(cid:12)
(cid:12)
(cid:12)
H ∼ πθ
(cid:12)
(cid:12)

(cid:35)

.

We now use Lemma 1 to prove the Behavior Policy Gradient Theorem which is our main theoretical contribution.

A.2. Behavior Policy Gradient Theorem

Theorem 1.

MSE[IS(H, θ)] = E

− IS(H, θ)2

(cid:34)

∂
∂θ

L
(cid:88)

t=0

∂
∂θ

(cid:12)
(cid:12)
(cid:12)
log πθ(At|St)
(cid:12)
(cid:12)

(cid:35)

H ∼ πθ

where the expectation is taken over H ∼ πθ.

Proof. We ﬁrst derive ∂
Lemma 1.

∂θ IS(H, θ)2. Theorem 1 then follows directly from using ∂

∂θ IS(H, θ)2 as ∂

∂θ OPE(H, θ)2 in

Behavior Policy Gradient: Supplemental Material

IS(H, θ)2 =

∂
∂θ

IS(H, θ)2 =

(cid:19)2

g(H)

(cid:18) wπe
wθ
(cid:18) wπe (H)
wθ(H)

∂
∂θ

(cid:19)2

g(H)

=2 · g(H)

wπe (H)
wθ(H)

∂
∂θ

g(H)

(cid:19)

wπe (H)
wθ(H)

(a)= − 2 · g(H)

wπe (H)
wθ(H)

g(H)

wπe (H)
wθ(H)

(cid:19) L
(cid:88)

t=0

∂
∂θ

log πθ(At|St)

= − 2 IS(H, θ)2

log πθ(At|St)

(cid:18)

(cid:18)

L
(cid:88)

t=0

∂
∂θ

where (a) comes from the multi-factor product rule and using the likelihood-ratio trick (i.e.,

∂
∂θ πθ (A|S)
πθ (A|S) = log πθ(A|S))

Substituting this expression into Lemma 1 completes the proof:

MSE[IS(H, θ)] = E

− IS(H, θ)2

(cid:34)

∂
∂θ

L
(cid:88)

t=0

∂
∂θ

(cid:12)
(cid:12)
(cid:12)
log πθ(At|St)
(cid:12)
(cid:12)

(cid:35)

H ∼ πθ

A.3. Doubly Robust Estimator

Our ﬁnal theoretical result is a corollary to the Behavior Policy Gradient Theorem: an extension of the IS variance gradient
to the Doubly Robust (DR) estimator. Recall that for a single trajectory DR is given as:

DR(H, θ) := ˆvπe (S0) +

(Rt − ˆqπe (St, At) + ˆvπe (St+1))

L
(cid:88)

t=0

γt wπe,t
wθ,t

where ˆvπe is the state-value function of πe under an approximate model, ˆqπe is the action-value function of πe under the
model, and wπ,t := (cid:81)t

j=0 π(Aj|Sj).

The gradient of the mean squared error of the DR estimator is given by the following corollary to the Behavior Policy
Gradient Theorem:
Corollary 1.

∂
∂θ

MSE [DR(H, θ)] = E[(DR(H, θ)2

log πθ(At|St) − 2 DR(H, θ)(

log πθ(Ai|Si))]

L
(cid:88)

t=0

∂
∂θ

L
(cid:88)

t=0

γtδt

wπe,t
wθ,t

t
(cid:88)

i=0

∂
∂θ

where δt = Rt − ˆq(St, At) + ˆv(St+1) and the expectation is taken over H ∼ πθ.

Proof. As with Theorem 1, we ﬁrst derive ∂
∂
∂θ OPE(H, θ)2 in Lemma 1.

∂θ DR(H, θ)2. Corollary 1 then follows directly from using ∂

∂θ DR(H, θ)2 as

DR(H, θ)2 =

ˆvπe(S0) +

(Rt − ˆqπe(St, At) + ˆvπe (St+1))

(cid:32)

L
(cid:88)

t=0

γt wπe,t
wθ,t

(cid:33)2

Behavior Policy Gradient: Supplemental Material

∂
∂θ

(cid:32)

∂
∂θ

L
(cid:88)

t=0

γt wπe,t
wθ,t

DR(H, θ)2 =

ˆvπe (S0) +

(Rt − ˆqπe (St, At) + ˆvπe (St+1))

=2 DR(H, θ)

ˆvπe (S0) +

(Rt − ˆqπe(St, At) + ˆvπe (St+1))

(cid:32)

∂
∂θ

L
(cid:88)

t=0

γt wπe,t
wθ,t

L
(cid:88)

t=0

γt wπe,t
wθ,t

(cid:33)

(cid:33)2

t
(cid:88)

i=0

∂
∂θ

= − 2 DR(H, θ)(

(Rt − ˆqπe (St, At) + ˆvπe (St+1))

log πθ(Ai|Si))

Thus the DR(H, θ) gradient is:

(cid:34)

= E

DR(H, θ)2

L
(cid:88)

t=0

∂
∂θ

log πθ(At|St) − 2 DR(H, θ)(

(Rt − ˆqπe (St, At) + ˆvπe (St+1))

L
(cid:88)

t=0

γt wπe,t
wθ,t

t
(cid:88)

i=0

∂
∂θ

(cid:12)
(cid:12)
(cid:12)
log πθ(Ai|Si))
(cid:12)
(cid:12)

(cid:35)

H ∼ πθ

The expression for the DR behavior policy gradient is more complex than the expression for the IS behavior policy gradient.
Lowering the variance of DR involves accounting for the covariance of the sum of terms. Intuitively, accounting for the
covariance increases the complexity of the expression for the gradient.

B. BPG’s Off-Policy Estimates are Unbiased

This appendix proves that the estimate of BPG is an unbiased estimate of ρ(πe). If only trajectories from a single θi were
used then clearly IS(·, θi) is an unbiased estimate of ρ(πe). The difﬁculty is that the BPG’s estimate at iteration n depends
on all θi for i = 1 . . . n and each θi is not independent of the others. Nevertheless, we prove here that BPG produces
an unbiased estimate of ρ(πe) at each iteration. Speciﬁcally, we will show that E [IS(Hn, θn|θ0 = θe)] is an unbiased
estimate of ρ(πe), where the IS estimate is conditioned on θ0 = θe. To make the dependence of θi on θi−1 explicit, we
will write f (Hi−1) := θi where Hi−1 ∼ πθi−1 . Notice that, even though BPG’s off-policy estimates are unbiased, they
are not statistically independent. This means that concentration inequalities, like Hoeffding’s inequality, cannot be applied
directly. We conjecture that the conditional independence properties of BPG (speciﬁcally that Hi is independent of Hi−1
given θi), are sufﬁcient for Hoeffding’s inequality to be applicable.

E [IS(Hn, θn|θ = θe)] =

Pr(h0|θ0)

Pr(h1|f (h0)) · · ·

Pr(hn|f (hn−1)) IS(hn)

(cid:88)

h0

(cid:88)

h1

(cid:88)

hn
(cid:124)

(cid:123)(cid:122)
ρ(πe)

(cid:125)

=ρ(πe)

Pr(h0|θ0)

Pr(h1|f (h0)) · · ·

(cid:88)

h0

(cid:88)

h1

=ρ(πe)

C. Supplemental Experiment Description

This appendix contains experimental details in addition to the details contained in Section 5 of the paper.

Gridworld: This domain is a 4x4 Gridworld with a terminal state with reward 10 at (3, 3), a state with reward −10 at
(1, 1), a state with reward 1 at (1, 3), and all other states having reward −1. The action set contains the four cardinal direc-
tions and actions move the agent in its intended direction (except when moving into a wall which produces no movement).
The agent begins in (0,0), γ = 1, and L = 100. All policies use softmax action selection with temperature 1 where the
probability of taking an action a in a state s is given by:

π(a|s) =

eθsa
a(cid:48) eθsa(cid:48)

(cid:80)

Behavior Policy Gradient: Supplemental Material

We obtain two evaluation policies by applying REINFORCE to this task, starting from a policy that selects actions uni-
formly at random. We then select one evaluation policy from the early stages of learning – an improved policy but still far
from converged –, π1, and one after learning has converged, π2. We run our set of experiments once with πe := π1 and a
second time with πe := π2. The ground truth value of ρ(πe) is computed with value iteration for both πe.

Stochastic Gridworld: The layout of this Gridworld is identical to the deterministic Gridworld except the terminal state
is at (9, 9) and the +1 reward state is at (1, 9). When the agent moves, it moves in its intended direction with probability
0.9, otherwise it goes left or right with equal probability. Noise in the environment increases the difﬁculty of building an
accurate model from trajectories.

Continuous Control: We evaluate BPG on two continuous control tasks: Cart-pole Swing Up and Acrobot. Both tasks
are implemented within RLLAB (Duan et al., 2016) (full details of the tasks are given in Appendix 1.1). The single task
modiﬁcation we make is that in Cart-pole Swing Up, when a trajectory terminates due to moving out of bounds we give
a penalty of −1000. This modiﬁcation increases the variance of πe. We use γ = 1 and L = 50. Policies are represented
as conditional Gaussians with mean determined by a neural network with two hidden layers of 32 tanh units each and
a state-independent diagonal covariance matrix. In Cart-pole Swing Up, πe was learned with 10 iterations of the TRPO
algorithm (Schulman et al., 2015) applied to a randomly initialized policy. In Acrobot, πe was learned with 60 iterations.
The ground truth value of ρ(πe) in both domains is computed with 1,000,000 Monte Carlo roll-outs.

Domain Independent Details
estimate from Theorem 1. The baseline is bi = E (cid:2)− IS(H)2(cid:12)

(cid:12)H ∼ θi−1

(cid:3) and our new gradient estimate is:

In all experiments we subtract a constant control variate (or baseline) in the gradient

(cid:34)

E

(− IS2 −bi)

L
(cid:88)

t=0

∂
∂θ

(cid:12)
(cid:12)
(cid:12)
log πθ(At|St)
(cid:12)
(cid:12)

(cid:35)

H ∼ πθ

Adding or subtracting a constant does not change the gradient in expectation since bi · E
BPG with a baseline has lower variance so that the estimated gradient is closer in direction to the true gradient.

t=0

(cid:104)(cid:80)L

(cid:105)
∂
∂θ log πθ(At|St)

= 0.

We use batch sizes of 100 trajectories per iteration for Gridworld experiments and size 500 for the continuous control tasks.
The step-size parameter was determined by a sweep over [10−2, 10−6]

Early Stopping Criterion In all experiments we run BPG for a ﬁxed number of iterations.
In general, BPS can
continue for a ﬁxed number of iterations or until the variance of the IS estimator stops decreasing. The true variance
is unknown but can be estimated by sampling a set of k trajectories with θi and computing the uncentered variance:
1
j=0 OPE(Hj, θj)2. This measure can be used to empirically evaluate the quality of each θ or determine when a BPS
k
algorithm should terminate behavior policy improvement.

(cid:80)k

References

Duan, Yan, Chen, Xi, Houthooft, Rein, Schulman, John, and Abbeel, Pieter. Benchmarking deep reinforcement learning

for continuous control. In In Proceedings of the 33rd International Conference on Machine Learning, 2016.

Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan, Michael, and Abbeel, Pieter. Trust region policy optimization.

In International Conference on Machine Learning, ICML, 2015.

