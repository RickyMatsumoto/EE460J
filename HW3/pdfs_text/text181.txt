Supplementary Material for
Being Robust (in High Dimensions) Can Be Practical

Ilias Diakonikolas
CS, USC
diakonik@usc.edu

Gautam Kamath
EECS & CSAIL, MIT
g@csail.mit.edu

Daniel M. Kane
CSE & Math, UCSD
dakane@cs.ucsd.edu

Jerry Li
EECS & CSAIL, MIT
jerryzli@mit.edu

Ankur Moitra
Math & CSAIL, MIT
moitra@mit.edu

Alistair Stewart
CS, USC
alistais@usc.edu

1 Omitted Details from Section 3

1.1 Robust Mean Estimation for Sub-Gaussian Distributions

In this section, we use our ﬁlter technique to give a near sample-optimal computationally eﬃcient
algorithm to robustly estimate the mean of a sub-gaussian density with a known covariance matrix,
thus proving Theorem 3.1.

We emphasize that the algorithm and its analysis is essentially identical to the ﬁltering algorithm
given in Section 8.1 of [DKK+16] for the case of a Gaussian N (µ, I). The only diﬀerence is a
weaker deﬁnition of the “good set of samples” (Deﬁnition 2) and a simple concentration argument
(Lemma 1) showing that a random set of uncorrupted samples of the appropriate size is good
with high probability. Given these, the analysis of this subsection follows straightforwardly from
the analysis in Section 8.1 of [DKK+16] by plugging in the modiﬁed parameters. For the sake of
completeness, we provide the details below.

We start by formally deﬁning sub-gaussian distributions:

Deﬁnition 1. A distribution P on R with mean µ, is sub-gaussian with parameter ν > 0 if

EX∼P [exp(λ(X − µ))] ≤ exp(νλ2/2)

for all λ ∈ R. A distribution P on Rd with mean vector µ is sub-gaussian with parameter ν > 0,
if for all unit vectors v, the one-dimensional random variable v · X, X ∼ P , is sub-gaussian with
parameter ν.

We will use the following simple fact about the concentration of sub-gaussian random variables:

Fact 1. If P is sub-gaussian on Rd with mean vector µ and parameter ν > 0, then for any unit
vector v ∈ Rd we have that PrX∼P [|v · (X − µ)| ≥ T ] ≤ exp(−t2/2ν).

The following theorem is a high probability version of Theorem 3.1:

Theorem 2. Let G be a sub-gaussian distribution on Rd with parameter ν = Θ(1), mean µG,
covariance matrix I, and ε, τ > 0. Let S(cid:48) be an ε-corrupted set of samples from G of size
Ω((d/ε2)poly log(d/ετ )). There exists an eﬃcient algorithm that, on input S(cid:48) and ε > 0, returns a
mean vector (cid:98)µ so that with probability at least 1 − τ we have (cid:107)(cid:98)µ − µG(cid:107)2 = O(ε(cid:112)log(1/ε)).

1

Notation. We will denote µS = 1
|S|
sample mean and modiﬁed sample covariance matrix of the set S.

X∈S X and MS = 1
|S|

(cid:80)

(cid:80)

X∈S(X − µG)(X − µG)T for the

We start by deﬁning our modiﬁed notion of good sample, i.e, a set of conditions on the uncor-

rupted set of samples under which our algorithm will succeed.

Deﬁnition 2. Let G be an identity covariance sub-gaussian in d dimensions with mean µG and
covariance matrix I and ε, τ > 0. We say that a multiset S of elements in Rd is (ε, τ )-good with
respect to G if the following conditions are satisﬁed:

(i) For all x ∈ S we have (cid:107)x − µG(cid:107)2 ≤ O((cid:112)d log(|S|/τ )).

(ii) For every aﬃne function L : Rd → R such that L(x) = v · (x − µG) − T , (cid:107)v(cid:107)2 = 1, we have

that |PrX∈uS[L(X) ≥ 0] − PrX∼G[L(X) ≥ 0]| ≤

ε
T 2 log(d log( d

ετ ))

.

(iii) We have that (cid:107)µS − µG(cid:107)2 ≤ ε.

(iv) We have that (cid:107)MS − I(cid:107)2 ≤ ε.

We show in the following subsection that a suﬃciently large set of independent samples from G

is (ε, τ )-good (with respect to G) with high probability. Speciﬁcally, we prove:

Lemma 1. Let G be sub-gaussian distribution with parameter ν = Θ(1) and with identity covariance,
and ε, τ > 0. If the multiset S is obtained by taking Ω((d/ε2)poly log(d/ετ )) independent samples
from G, it is (ε, τ )-good with respect to G with probability at least 1 − τ.

We require the following deﬁnition that quantiﬁes the extent to which a multiset has been

corrupted:

Deﬁnition 3. Given ﬁnite multisets S and S(cid:48) we let ∆(S, S(cid:48)) be the size of the symmetric diﬀerence
of S and S(cid:48) divided by the cardinality of S.

The starting point of our algorithm will be a simple NaivePrune routine (Section 4.3.1 of
[DKK+16]) that removes obvious outliers, i.e., points which are far from the mean. Then, we
iterate the algorithm whose performance guarantee is given by the following:

Proposition 1. Let G be a sub-gaussian distribution on Rd with parameter ν = Θ(1), mean µG,
covariance matrix I, ε > 0 be suﬃciently small and τ > 0. Let S be an (ε, τ )-good set with respect
to G. Let S(cid:48) be any multiset with ∆(S, S(cid:48)) ≤ 2ε and for any x, y ∈ S(cid:48), (cid:107)x − y(cid:107)2 ≤ O((cid:112)d log(d/ετ )).
There exists a polynomial time algorithm Filter-Sub-Gaussian-Unknown-Mean that, given S(cid:48)
and ε > 0, returns one of the following:
(i) A mean vector (cid:98)µ such that (cid:107)(cid:98)µ − µG(cid:107)2 = O(ε(cid:112)log(1/ε)).
(ii) A multiset S(cid:48)(cid:48) ⊆ S(cid:48) such that ∆(S, S(cid:48)(cid:48)) ≤ ∆(S, S(cid:48))−ε/α, where α def= d log(d/ετ ) log(d log( d

ετ )).

We start by showing how Theorem 2 follows easily from Proposition 1.

Proof of Theorem 2. By the deﬁnition of ∆(S, S(cid:48)), since S(cid:48) has been obtained from S by corrupting
an ε-fraction of the points in S, we have that ∆(S, S(cid:48)) ≤ 2ε. By Lemma 1, the set S of uncorrupted
samples is (ε, τ )-good with respect to G with probability at least 1 − τ. We henceforth condition on
this event.

Since S is (ε, τ )-good, all x ∈ S have (cid:107)x − µG(cid:107)2 ≤ O((cid:112)d log |S|/τ ). Thus, the NaivePrune
procedure does not remove from S(cid:48) any member of S. Hence, its output, S(cid:48)(cid:48), has ∆(S, S(cid:48)(cid:48)) ≤ ∆(S, S(cid:48))

2

and for any x ∈ S(cid:48)(cid:48), there is a y ∈ S with (cid:107)x − y(cid:107)2 ≤ O((cid:112)d log |S|/τ ). By the triangle inequality,
for any x, z ∈ S(cid:48)(cid:48), (cid:107)x − z(cid:107)2 ≤ O((cid:112)d log |S|/τ ) = O((cid:112)d log(d/ετ )).

Then, we iteratively apply the Filter-Sub-Gaussian-Unknown-Mean procedure of Propo-
sition 1 until it terminates returning a mean vector µ with (cid:107)(cid:98)µ − µG(cid:107)2 = O(ε(cid:112)log(1/ε)). We claim
that we need at most O(α) iterations for this to happen. Indeed, the sequence of iterations results
in a sequence of sets S(cid:48)
i) ≤ ∆(S, S(cid:48))−i·ε/α. Thus, if we do not output the empirical
mean in the ﬁrst 2α iterations, in the next iteration there are no outliers left and the algorithm
terminates outputting the sample mean of the remaining set.

i, so that ∆(S, S(cid:48)

1.1.1 Algorithm Filter-Sub-Gaussian-Unknown-Mean: Proof of Proposition 1

In this subsection, we describe the eﬃcient algorithm establishing Proposition 1 and prove its
correctness. Our algorithm calculates the empirical mean vector µS(cid:48) and empirical covariance matrix
Σ. If the matrix Σ has no large eigenvalues, it returns µS(cid:48). Otherwise, it uses the eigenvector v∗
corresponding to the maximum magnitude eigenvalue of Σ and the mean vector µS(cid:48) to deﬁne a
ﬁlter. Our eﬃcient ﬁltering procedure is presented in detailed pseudocode below.

Algorithm 1 Filter algorithm for a sub-gaussian with unknown mean and identity covariance
1: procedure Filter-Sub-Gaussian-Unknown-Mean(S(cid:48), ε, τ )
input: A multiset S(cid:48) such that there exists an (ε, τ )-good S with ∆(S, S(cid:48)) ≤ 2ε
output: Multiset S(cid:48)(cid:48) or mean vector (cid:98)µ satisfying Proposition 1
2:

Compute the sample mean µS(cid:48) = EX∈uS(cid:48)[X] and the sample covariance matrix Σ , i.e.,

Σ = (Σi,j)1≤i,j≤d with Σi,j = EX∈uS(cid:48)[(Xi − µS(cid:48)

i )(Xj − µS(cid:48)

j )].

Compute approximations for the largest absolute eigenvalue of Σ − I, λ∗ := (cid:107)Σ − I(cid:107)2, and

3:

4:

5:

6:

the associated unit eigenvector v∗.

if (cid:107)Σ − I(cid:107)2 ≤ O(ε log(1/ε)), then return µS(cid:48).
end if
Let δ := 3(cid:112)ε(cid:107)Σ − I(cid:107)2. Find T > 0 such that

(cid:104)

Pr
X∈uS(cid:48)

|v∗ · (X − µS(cid:48)

)| > T + δ

> 8 exp(−T 2/2ν) + 8

(cid:105)

ε
T 2 log (cid:0)d log( d

ετ )(cid:1) .

return the multiset S(cid:48)(cid:48) = {x ∈ S(cid:48) : |v∗ · (x − µS(cid:48))| ≤ T + δ}.

7:
8: end procedure

1.1.2 Proof of Correctness of Filter-Sub-Gaussian-Unknown-Mean

By deﬁnition, there exist disjoint multisets L, E, of points in Rd, where L ⊂ S, such that S(cid:48) =
(S \ L) ∪ E. With this notation, we can write ∆(S, S(cid:48)) = |L|+|E|
. Our assumption ∆(S, S(cid:48)) ≤ 2ε is
equivalent to |L| + |E| ≤ 2ε · |S|, and the deﬁnition of S(cid:48) directly implies that (1 − 2ε)|S| ≤ |S(cid:48)| ≤
(1 + 2ε)|S|. Throughout the proof, we assume that ε is a suﬃciently small constant.

|S|

We deﬁne µG, µS, µS(cid:48), µL, and µE to be the means of G, S, S(cid:48), L, and E, respectively.
Our analysis will make essential use of the following matrices:

• MS(cid:48) denotes EX∈uS(cid:48)[(X − µG)(X − µG)T ],
• MS denotes EX∈uS[(X − µG)(X − µG)T ],

• ML denotes EX∈uL[(X − µG)(X − µG)T ], and

3

• ME denotes EX∈uE[(X − µG)(X − µG)T ].

Our analysis will hinge on proving the important claim that Σ−I is approximately (|E|/|S(cid:48)|)ME.
This means two things for us. First, it means that if the positive errors align in some direction
(causing ME to have a large eigenvalue), there will be a large eigenvalue in Σ − I. Second, it says
that any large eigenvalue of Σ − I will correspond to an eigenvalue of ME, which will give an explicit
direction in which many error points are far from the empirical mean.

Useful Structural Lemmas. We begin by noting that we have concentration bounds on G and
therefore, on S due to its goodness.

Fact 3. Let w ∈ Rd be any unit vector, then for any T > 0, PrX∼G
2 exp(−T 2/2ν) and PrX∈uS

(cid:2)|w · (X − µG)| > T (cid:3) ≤ 2 exp(−T 2/2ν) +

(cid:2)|w · (X − µG)| > T (cid:3) ≤
ε
T 2 log(d log( d

.

ετ ))

Proof. The ﬁrst line is Fact 1, and the former follows from it using the goodness of S.

By using the above fact, we obtain the following simple claim:

Claim 1. Let w ∈ Rd be any unit vector, then for any T > 0, we have that:

[|w · (X − µS(cid:48)

)| > T + (cid:107)µS(cid:48)

− µG(cid:107)2] ≤ 2 exp(−T 2/2ν).

Pr
X∼G

and

Pr
X∈uS

[|w · (X − µS(cid:48)

)| > T + (cid:107)µS(cid:48)

− µG(cid:107)2] ≤ 2 exp(−T 2/2ν) +

ε
T 2 log (cid:0)d log( d

ετ )(cid:1) .

Proof. This follows from Fact 3 upon noting that |w · (X − µS(cid:48))| > T + (cid:107)µS(cid:48) − µG(cid:107)2 only if
|w · (X − µG)| > T .

We can use the above facts to prove concentration bounds for L. In particular, we have the

following lemma:

Lemma 2. We have that (cid:107)ML(cid:107)2 = O (log(|S|/|L|) + ε|S|/|L|).

Proof. Since L ⊆ S, for any x ∈ Rd, we have that

|S| · Pr

(X = x) ≥ |L| · Pr

(X = x) .

X∈uS

X∈uL

(1)

Since ML is a symmetric matrix, we have (cid:107)ML(cid:107)2 = max(cid:107)v(cid:107)2=1 |vT MLv|. So, to bound (cid:107)ML(cid:107)2 it
suﬃces to bound |vT MLv| for unit vectors v. By deﬁnition of ML, for any v ∈ Rd we have that

|vT MLv| = EX∈uL[|v · (X − µG)|2].

4

For unit vectors v, the RHS is bounded from above as follows:

EX∈uL

(cid:2)|v · (X − µG)|2(cid:3) = 2

(cid:2)|v · (X − µG)| > T (cid:3) T dT

[|v · (X − µG)| > T ]T dT

|S|
|L|

· Pr

X∈uS

(cid:2)|v · (X − µG)| > T (cid:3)

T dT

(cid:27)

(cid:90) ∞

0
(cid:90) O(

Pr
X∈uL
√

d log(d/ετ ))

0
(cid:90) O(

√

d log(d/ετ ))

Pr
X∈uL
(cid:26)

min

1,

ν log(|S|/|L|)

T dT

√

(cid:90) O(
√
4

d log(d/ετ ))

(cid:16)

ν log(|S|/|L|)

= 2

≤ 2

0
√
(cid:90) 4

(cid:28)

0

(cid:28) log(|S|/|L|) + ε · |S|/|L| ,

+ (|S|/|L|)

exp(−T 2/2ν) +

ε
T 2 log (cid:0)d log( d

ετ )(cid:1)

(cid:17)

T dT

where the second line follows from the fact that (cid:107)v(cid:107)2 = 1, L ⊂ S, and S satisﬁes condition (i) of
Deﬁnition 2; the third line follows from (1); and the fourth line follows from Fact 3.

As a corollary, we can relate the matrices MS(cid:48) and ME, in spectral norm:

Corollary 1. We have that MS(cid:48) − I = (|E|/|S(cid:48)|)ME + O(ε log(1/ε)), where the O(ε log(1/ε)) term
denotes a matrix of spectral norm O(ε log(1/ε)).

Proof. By deﬁnition, we have that |S(cid:48)|MS(cid:48) = |S|MS − |L|ML + |E|ME. Thus, we can write

MS(cid:48) = (|S|/|S(cid:48)|)MS − (|L|/|S(cid:48)|)ML + (|E|/|S(cid:48)|)ME
= I + O(ε) + O(ε log(1/ε)) + (|E|/|S(cid:48)|)ME ,

where the second line uses the fact that 1 − 2ε ≤ |S|/|S(cid:48)| ≤ 1 + 2ε, the goodness of S (condition (iv)
in Deﬁnition 2), and Lemma 2. Speciﬁcally, Lemma 2 implies that (|L|/|S(cid:48)|)(cid:107)ML(cid:107)2 = O(ε log(1/ε)).
Therefore, we have that

MS(cid:48) = I + (|E|/|S(cid:48)|)ME + O(ε log(1/ε)) ,

as desired.

We now establish a similarly useful bound on the diﬀerence between the mean vectors:

Lemma 3. We have that µS(cid:48) −µG = (|E|/|S(cid:48)|)(µE −µG)+O(ε(cid:112)log(1/ε)), where the O(ε(cid:112)log(1/ε))
term denotes a vector with (cid:96)2-norm at most O(ε(cid:112)log(1/ε)).

Proof. By deﬁnition, we have that

|S(cid:48)|(µS(cid:48)

− µG) = |S|(µS − µG) − |L|(µL − µG) + |E|(µE − µG).

Since S is a good set, by condition (iii) of Deﬁnition 2, we have (cid:107)µS − µG(cid:107)2 = O(ε). Since
1 − 2ε ≤ |S|/|S(cid:48)| ≤ 1 + 2ε, it follows that (|S|/|S(cid:48)|)(cid:107)µS − µG(cid:107)2 = O(ε). Using the valid inequality
(cid:17)
(cid:16)(cid:112)log(|S|/|L|) + (cid:112)ε|S|/|L|
(cid:107)ML(cid:107)2 ≥ (cid:107)µL − µG(cid:107)2
.
Therefore,

2 and Lemma 2, we obtain that (cid:107)µL − µG(cid:107)2 ≤ O

(|L|/|S(cid:48)|)(cid:107)µL − µG(cid:107)2 ≤ O

(cid:16)

(cid:17)
(|L|/|S|)(cid:112)log(|S|/|L|) + (cid:112)ε|L|/|S|

= O(ε(cid:112)log(1/ε)) .

5

In summary,

µS(cid:48)

− µG = (|E|/|S(cid:48)|)(µE − µG) + O(ε(cid:112)log(1/ε)) ,

as desired. This completes the proof of the lemma.

By combining the above, we can conclude that Σ − I is approximately proportional to ME.

More formally, we obtain the following corollary:

Corollary 2. We have Σ − I = (|E|/|S(cid:48)|)ME + O(ε log(1/ε)) + O(|E|/|S(cid:48)|)2(cid:107)ME(cid:107)2, where the
additive terms denote matrices of appropriately bounded spectral norm.

Proof. By deﬁnition, we can write Σ − I = MS(cid:48) − I − (µS(cid:48) − µG)(µS(cid:48) − µG)T . Using Corollary 1 and
Lemma 3, we obtain:

Σ − I = (|E|/|S(cid:48)|)ME + O(ε log(1/ε)) + O((|E|/|S(cid:48)|)2(cid:107)µE − µG(cid:107)2
= (|E|/|S(cid:48)|)ME + O(ε log(1/ε)) + O(|E|/|S(cid:48)|)2(cid:107)ME(cid:107)2 ,

2) + O(ε2 log(1/ε))

where the second line follows from the valid inequality (cid:107)ME(cid:107)2 ≥ (cid:107)µE − µG(cid:107)2
proof.

2. This completes the

Case of Small Spectral Norm. We are now ready to analyze the case that the mean vector µS(cid:48)
is returned by the algorithm in Step 4. In this case, we have that λ∗ def= (cid:107)Σ − I(cid:107)2 = O(ε log(1/ε)).
Hence, Corollary 2 yields that

(|E|/|S(cid:48)|)(cid:107)ME(cid:107)2 ≤ λ∗ + O(ε log(1/ε)) + O(|E|/|S(cid:48)|)2(cid:107)ME(cid:107)2 ,

which in turns implies that

(|E|/|S(cid:48)|)(cid:107)ME(cid:107)2 = O(ε log(1/ε)) .

On the other hand, since (cid:107)ME(cid:107)2 ≥ (cid:107)µE − µG(cid:107)2

2, Lemma 3 gives that

(cid:107)µS(cid:48)

− µG(cid:107)2 ≤ (|E|/|S(cid:48)|)(cid:112)(cid:107)ME(cid:107)2 + O(ε(cid:112)log(1/ε)) = O(ε(cid:112)log(1/ε)).

This proves part (i) of Proposition 1.

Case of Large Spectral Norm. We next show the correctness of the algorithm when it returns
a ﬁlter in Step 6.

We start by proving that if λ∗ def= (cid:107)Σ − I(cid:107)2 > Cε log(1/ε), for a suﬃciently large universal
constant C, then a value T satisfying the condition in Step 6 exists. We ﬁrst note that that (cid:107)ME(cid:107)2
is appropriately large. Indeed, by Corollary 2 and the assumption that λ∗ > Cε log(1/ε) we deduce
that

(|E|/|S(cid:48)|)(cid:107)ME(cid:107)2 = Ω(λ∗) .

Moreover, using the inequality (cid:107)ME(cid:107)2 ≥ (cid:107)µE − µG(cid:107)2

2 and Lemma 3 as above, we get that

(cid:107)µS(cid:48)

− µG(cid:107)2 ≤ (|E|/|S(cid:48)|)(cid:112)(cid:107)ME(cid:107)2 + O(ε(cid:112)log(1/ε)) ≤ δ/2 ,

where we used the fact that δ def=

ελ∗ > C(cid:48)ε(cid:112)log(1/ε).

√

Suppose for the sake of contradiction that for all T > 0 we have that

(cid:104)
|v∗ · (X − µS(cid:48)

Pr
X∈uS(cid:48)

)| > T + δ

≤ 8 exp(−T 2/2ν) + 8

(cid:105)

ε
T 2 log (cid:0)d log( d

ετ )(cid:1) .

(2)

(3)

6

Using (3), we obtain that for all T > 0 we have that

(cid:2)|v∗ · (X − µG)| > T + δ/2(cid:3) ≤ 8 exp(−T 2/2ν) + 8

Pr
X∈uS(cid:48)

ε
T 2 log (cid:0)d log( d

ετ )(cid:1) .

(4)

Since E ⊆ S(cid:48), for all x ∈ Rd we have that |S(cid:48)| PrX∈uS(cid:48)[X = x] ≥ |E| PrY ∈uE[Y = x]. This fact
combined with (4) implies that for all T > 0

(cid:2)|v∗ · (X − µG)| > T + δ/2(cid:3) (cid:28) (|S(cid:48)|/|E|)

exp(−T 2/2ν) +

Pr
X∈uE

(cid:32)

ε
T 2 log (cid:0)d log( d

ετ )(cid:1)

(cid:33)

.

(5)

We now have the following sequence of inequalities:

(cid:2)|v∗ · (X − µG)|2(cid:3) = 2
(cid:107)ME(cid:107)2 = EX∈uE
√

0

Pr
X∈uE

(cid:90) ∞

(cid:2)|v∗ · (X − µG)| > T (cid:3) T dT

(cid:90) O(

d log(d/ετ ))

0
(cid:90) O(

√

d log(d/ετ ))

= 2

≤ 2

Pr
X∈uE
(cid:26)

min

1,

(cid:2)|v∗ · (X − µG)| > T (cid:3) T dT

|S(cid:48)|
|E|

· Pr

X∈uS(cid:48)

(cid:2)|v∗ · (X − µG)| > T (cid:3)

T dT

(cid:27)

√

0
(cid:90) 4

0

(cid:28)

ν log(|S(cid:48)|/|E|)+δ

T dT + (|S(cid:48)|/|E|)

(cid:90) O(
√
4
(cid:28) log(|S(cid:48)|/|E|) + δ2 + O(1) + ε · |S(cid:48)|/|E|
(cid:28) log(|S(cid:48)|/|E|) + ελ∗ + ε · |S(cid:48)|/|E| .

Rearranging the above, we get that

√

d log(d/ετ ))

(cid:16)

ν log(|S(cid:48)|/|E|)+δ

exp(−T 2/2ν) +

ε
T 2 log (cid:0)d log( d

ετ )(cid:1)

(cid:17)

T dT

(|E|/|S(cid:48)|)(cid:107)ME(cid:107)2 (cid:28) (|E|/|S(cid:48)|) log(|S(cid:48)|/|E|) + (|E|/|S(cid:48)|)ελ∗ + ε = O(ε log(1/ε) + ε2λ∗).

Combined with (2), we obtain λ∗ = O(ε log(1/ε)), which is a contradiction if C is suﬃciently large.
Therefore, it must be the case that for some value of T the condition in Step 6 is satisﬁed.

The following claim completes the proof:

Claim 2. Fix α def= d log(d/ετ ) log(d log( d

ετ )). We have that ∆(S, S(cid:48)(cid:48)) ≤ ∆(S, S(cid:48)) − 2ε/α .

Proof. Recall that S(cid:48) = (S \ L) ∪ E, with E and L disjoint multisets such that L ⊂ S. We can
similarly write S(cid:48)(cid:48) = (S \ L(cid:48)) ∪ E(cid:48), with L(cid:48) ⊇ L and E(cid:48) ⊂ E. Since

∆(S, S(cid:48)) − ∆(S, S(cid:48)(cid:48)) =

|E \ E(cid:48)| − |L(cid:48) \ L|
|S|

,

it suﬃces to show that |E \ E(cid:48)| ≥ |L(cid:48) \ L| + ε|S|/α. Note that |L(cid:48) \ L| is the number of points rejected
by the ﬁlter that lie in S ∩ S(cid:48). Note that the fraction of elements of S that are removed to produce
S(cid:48)(cid:48) (i.e., satisfy |v∗ · (x − µS(cid:48))| > T + δ) is at most 2 exp(−T 2/2ν) + ε/α. This follows from Claim 1
and the fact that T = O((cid:112)d log(d/ετ )).

Hence, it holds that |L(cid:48) \ L| ≤ (2 exp(−T 2/2ν) + ε/α)|S|. On the other hand, Step 6 of the
algorithm ensures that the fraction of elements of S(cid:48) that are rejected by the ﬁlter is at least

7

8 exp(−T 2/2ν) + 8ε/α). Note that |E \ E(cid:48)| is the number of points rejected by the ﬁlter that lie in
S(cid:48) \ S. Therefore, we can write:

|E \ E(cid:48)| ≥ (8 exp(−T 2/2ν) + 8ε/α)|S(cid:48)| − (2 exp(−T 2/2ν) + ε/α)|S|

≥ (8 exp(−T 2/2ν) + 8ε/α)|S|/2 − (2 exp(−T 2/2ν) + ε/α)|S|
≥ (2 exp(−T 2/2ν) + 3ε/α)|S|
≥ |L(cid:48) \ L| + 2ε|S|/α ,

where the second line uses the fact that |S(cid:48)| ≥ |S|/2 and the last line uses the fact that |L(cid:48) \L|/|S| ≤
2 exp(−T 2/2ν) + ε/α. Noting that log(d/ετ ) ≥ 1, this completes the proof of the claim.

1.1.3 Proof of Lemma 1

Proof. Let N = Ω((d/ε2)poly log(d/ετ )) be the number of samples drawn from G. For (i), the
probability that a coordinate of a sample is at least (cid:112)2ν log(N d/3τ ) is at most τ /3dN by Fact 1. By
a union bound, the probability that all coordinates of all samples are smaller than (cid:112)2ν log(N d/3τ )
is at least 1 − τ /3. In this case, (cid:107)x(cid:107)2 ≤ (cid:112)2νd log(N d/3τ ) = O((cid:112)dν log(N ν/τ )).

After translating by µG, we note that (iii) follows immediately from Lemmas 4.3 of [DKK+16]
and (iv) follows from Theorem 5.50 of [Ver10], as long as N = Ω(ν4d log(1/τ )/ε2), with probability
at least 1 − τ /3. It remains to show that, conditioned on (i), (ii) holds with probability at least
1 − τ /3.

To simplify some expressions, let δ := ε/(log(d log d/ετ )) and R = C(cid:112)d log(|S|/τ ). We need to

show that for all unit vectors v and all 0 ≤ T ≤ R that

(cid:12)
(cid:12)
Pr
(cid:12)
(cid:12)
X∈uS

[|v · (X − µG)| > T ] − Pr
X∼G

(cid:12)
(cid:12)
[|v · (X − µG) > T ≥ 0]
(cid:12)
(cid:12)

≤

δ
T 2 .

(6)

Firstly, we show that for all unit vectors v and T > 0

(cid:12)
(cid:12)
Pr
(cid:12)
(cid:12)
X∈uS

[|v · (X − µG)| > T ] − Pr
X∼G

(cid:12)
(cid:12)
[|v · (X − µG)| > T ≥ 0]
(cid:12)
(cid:12)

≤ δ/4ν ln(1/δ)

with probability at least 1 − τ /6. Since the VC-dimension of the set of all halfspaces is d + 1, this
follows from the VC inequality [DL01], since we have more than Ω(d/(δ/(4ν log(1/δ))2) samples.
We thus only need to consider the case when T ≥ (cid:112)4ν ln(1/δ).
Lemma 4. For any ﬁxed unit vector v and T > (cid:112)4ν ln(1/δ), except with probability exp(−N δ/6Cν),
we have that

[|v · (X − µG)| > T ] ≤

Pr
X∈uS

δ
CT 2 ,

where C = 8.

Proof. Let E be the event that |v · (X − µG)| > T . Since G is sub-gaussian, Fact 1 yields that
PrG[E] = PrY ∼G[|v · (X − µG)| ≤ exp(−T 2/2ν). Note that, thanks to our assumption on T , we
have that T ≤ exp(T 2/4ν)/2C, and therefore T 2 PrG[E] ≤ exp(−T 2/4ν)/2C ≤ δ/2C.

8

Consider ES[exp(t2/3ν · N PrS[E])]. Each individual sample sample Xi for 1 ≤ i ≤ N , is an

independent copy of Y ∼ G, and hence:

(cid:20)

ES

exp(T 2/3ν · N Pr
S

[E])

= ES

(cid:21)

(cid:34)
exp(T 2/3ν ·

(cid:35)
1Xi∈E)

n
(cid:88)

i=1

=

=

N
(cid:89)

i=1
N
(cid:89)

i=1

(cid:34)

(cid:35)

EXi

exp(T 2/3ν ·

1Xi∈E)

n
(cid:88)

i=1

exp(T 2/3ν · Pr
G

[E])

≤ exp (cid:0)N T 2/3ν · exp(−T 2/2ν)(cid:1)
≤ exp(N/3ν · δ/2C) = exp(N δ/6Cν) .

Note that if PrS[E] > δ
this happens with probability at most exp(−N δ/6Cν).

CνT 2 , then exp(T 2/3ν · N PrS[E]) = exp(N δ/3Cν). By Markov’s inequality,

Now let C be a 1/2-cover in Euclidean distance for the set of unit vectors of size 2O(d). By a

union bound, for all v(cid:48) ∈ C and T (cid:48) a power of 2 between (cid:112)4ν ln(1/δ) and R, we have that

[|v(cid:48) · (X − µG)| > T (cid:48)] ≤

Pr
X∈uS

δ
8T 2

except with probability

2O(d) log(R) exp(−N δ/6Cν) = exp (O(d) + log log R − N δ/6Cν) ≤ τ /6 .

However, for any unit vector v and (cid:112)4ν ln(1/δ) ≤ T ≤ R, there is a v(cid:48) ∈ C and such a T (cid:48) such
that for all x ∈ Rd, we have |v · (X − µG)| ≥ |v(cid:48) · (X − µG)|/2, and so |v(cid:48) · (X − µG)| > 2T (cid:48) implies
|v(cid:48) · (X − µG)| > T.

Then, by a union bound, (6) holds simultaneously for all unit vectors v and all 0 ≤ T ≤ R, with

probability a least 1 − τ /3. This completes the proof.

1.2 Robust Mean Estimation Under Second Moment Assumptions

In this section, we use our ﬁltering technique to give a near sample-optimal computationally eﬃcient
algorithm to robustly estimate the mean of a density with a second moment assumption. We show:

Theorem 4. Let P be a distribution on Rd with unknown mean vector µP and unknown covariance
matrix ΣP (cid:22) I. Let S be an ε-corrupted set of samples from P of size Θ((d/ε) log d). Then there
exists an algorithm that given S, with probability 2/3, outputs (cid:98)µ with (cid:107)(cid:98)µ − µP (cid:107)2 ≤ O(
ε) in time
poly(d/ε).

√

Note that Theorem 3.2 follows straightforwardly from the above (divide every sample by σ, run

the algorithm of Theorem 4, and multiply its output by σ).

As usual in our ﬁltering framework, the algorithm will iteratively look at the top eigenvalue
and eigenvector of the sample covariance matrix and return the sample mean if this eigenvalue
is small (Algorithm 2). The main diﬀerence between this and the ﬁlter algorithm for the sub-
gaussian case is how we choose the threshold for the ﬁlter. Instead of looking for a violation of
a concentration inequality, here we will choose a threshold at random (with a bias towards higher

9

thresholds). The reason is that, in this setting, the variance in the direction we look for a ﬁlter in
needs to be a constant multiple larger – instead of the typical ˜Ω(ε) relative for the sub-gaussian
case. Therefore, randomly choosing a threshold weighted towards higher thresholds suﬃces to throw
out more corrupted samples than uncorrupted samples in expectation. Although it is possible to
reject many good samples this way, the algorithm still only rejects a total of O(ε) samples with high
probability.

We would like our good set of samples to have mean close to that of P and bounded variance

in all directions. This motivates the following deﬁnition:

Deﬁnition 4. We call a set S ε-good for a distribution P with mean µP and covariance ΣP (cid:22) I if
the mean µS and covariance ΣS of S satisfy (cid:107)µS − µP (cid:107)2 ≤

ε and (cid:107)ΣS(cid:107)2 ≤ 2.

√

However, since we have no assumptions about higher moments, it may be be possible for outliers
to aﬀect our sample covariance too much. Fortunately, such outliers have small probability and do
not contribute too much to the mean, so we will later reclassify them as errors.

Lemma 5. Let S be N = Θ((d/ε) log d) samples drawn from P . Then, with probability at least
9/10, a random X ∈u S satisﬁes

(i) (cid:107)E[X] − µP (cid:107)2 ≤

√

ε/3,

(ii) Pr

(cid:104)
(cid:107)X − µP (cid:107)2 ≥ 80(cid:112)d/ε

(cid:105)

≤ ε/160,

(iii) E

(cid:104)
(cid:107)X − µP (cid:107)2 · 1

√

(cid:105)

√

≤

ε/2, and

(cid:107)X−µP (cid:107)2≥80

d/ε

(iv)

(X − µP )(X − µP )T · 1

(cid:104)

(cid:13)
(cid:13)
(cid:13)

E

(cid:107)X−µP (cid:107)2≤80

d/ε

√

(cid:105)(cid:13)
(cid:13)
(cid:13)2

≤ 3/2.

Proof. For (i), note that

ES[(cid:107)E[X] − µP (cid:107)2

2] =

ES[(E[X]i − µP

i )2] ≤ d/N ≤ ε/360 ,

(cid:88)

i

and so by Markov’s inequality, with probability at least 39/40, we have (cid:107)E[X] − µP (cid:107)2

2 ≤ ε/9.

For (ii), similarly to (i), note that

E[(cid:107)Y − µP (cid:107)2

2] =

E[(Yi − µP

i )2] ≤ d ,

(cid:88)

i

for Y ∼ P . By Markov’s inequality, Pr[(cid:107)Y − µP (cid:107)2 ≥ 80(cid:112)d/ε] ≤ ε/160 with probability at least
39/40.

For (iii), note that by an application of the Cauchy-Schwarz inequality

E[|Y − µP |1

(cid:107)Y −µP (cid:107)2≥80

d/ε

√

] ≤

E[(Y − µP )2] Pr[(cid:107)Y − µP (cid:107)2 ≥ 80(cid:112)d/ε] ≤

√

ε/80 .

(cid:113)

Thus,

ES[E[|X − µP |1

(cid:107)X−µP (cid:107)2≥80

d/ε

√

]] ≤

ε/80 ,

√

and by Markov’s inequality, with probability at least 39/40

(cid:104)
|X − µP |1

E

√

(cid:105)

√

≤

ε/2 .

(cid:107)X−µP (cid:107)2≥80

d/ε

For (iv), we require the following Matrix Chernoﬀ bound:

10

Lemma 6 (Part of Theorem 5.1.1 of [T+15]). Consider a sequence of d × d positive semi-deﬁnite
random matrices Xk with (cid:107)Xk(cid:107)2 ≤ L for all k. Let µmax = (cid:107)(cid:80)
k

E[Xk](cid:107)2. Then, for θ > 0,

and for any δ > 0,

E

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

Xk

k

(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤ (eθ − 1)µmax/θ + L log(d)/θ ,

Pr

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

Xk

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:35)

≥ (1 + δ)µmax

≤ d(eδ/(1 + δ)1+δ)µmax/L .

We apply this lemma with Xk = (xk − µP )(xk − µP )T 1
that (cid:107)Xk(cid:107)2 ≤ (80)2d/ε = L and that µmax ≤ N (cid:107)ΣP (cid:107)2 ≤ N .
Suppose that µmax ≤ N/80. Then, taking θ = 1, we have

(cid:107)xk−µP (cid:107)2≤80

d/ε

√

for {x1, . . . , xN } = S. Note

(cid:13)
(cid:13)
E[
(cid:13)
(cid:13)
(cid:13)

(cid:88)

Xk

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

] ≤ (e − 1)N/80 + O(d log(d)/ε) .

By Markov’s inequality, except with probability 39/40, we have (cid:107) (cid:80)
3N/2, for N a suﬃciently high multiple of d log(d)/ε.

Suppose that µmax ≥ N/80, then we take δ = 1/2 and obtain

k Xk(cid:107)2 ≤ N + O(d log(d)/ε) ≤

Pr

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

Xk

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:35)
≥ 3µmax2

≤ d(e3/2/(5/2)3/2)N ε/20d .

For N a suﬃciently high multiple of d log(d)/ε, we get that Pr[(cid:107)(cid:80)
µmax ≤ N , we have with probability at least 39/40, (cid:107)(cid:80)
k Xk(cid:107)2 /N = (cid:107)E[1

Noting that (cid:107)(cid:80)

√

(cid:107)X−µP (cid:107)2≤80

d/ε

k Xk(cid:107)2 ≤ 3N/2.

union bound, (i)-(iv) all hold simultaneously with probability at least 9/10.

(X − µP )(X − µP )T ](cid:107)2, we obtain (iv). By a

k Xk(cid:107)2 ≥ 3µmax/2] ≤ 1/40. Since

Now we can get a 2ε-corrupted good set from an ε-corrupted set of samples satisfying Lemma

5, by reclassifying outliers as errors:

Lemma 7. Let S = R ∪ E \ L, where R is a set of N = Θ(d log d/ε) samples drawn from P
and E and L are disjoint sets with |E|, |L| ≤ ε. Then, with probability 9/10, we can also write
S = G ∪ E(cid:48) \ L(cid:48), where G ⊆ R is ε-good, L(cid:48) ⊆ L and E(cid:48) ⊆ E(cid:48) has |E(cid:48)| ≤ 2ε|S|.
Proof. Let G = {x ∈ R : (cid:107)x(cid:107)2 ≤ 80(cid:112)d/ε}. Since R satisﬁes (ii) of Lemma 5, |G|−|R| ≤ ε|R|/160 ≤
ε|S|. Thus, E(cid:48) = E ∪ (R \ G) has |E(cid:48)| ≤ 3ε/2. Note that (iv) of Lemma 5 for R in terms of G is
exactly |G|(cid:107)ΣG(cid:107)2/|R| ≤ 3/2, and so (cid:107)ΣG(cid:107)2 ≤ 3|R|/2|G| ≤ 2.

It remains to check that (cid:107)µG−µP (cid:107)2 ≤

ε. But note that (iii) of Lemma 5 is exactly EX∈uR[(cid:107)X −

√

µP (cid:107)21X∈R\G] ≤

√

ε/2, and we have
(cid:12)|G|EX∈uG[(cid:107)X − µP (cid:107)2] − |R|EX∈uR[(cid:107)X − µP (cid:107)2](cid:12)
(cid:12)

√

(cid:12) ≤ |R|

ε/2 ,

and since by (i), EX∈uR[(cid:107)X − µP (cid:107)2] ≤

ε/3, it follows that EX∈uG(cid:48)[(cid:107)X − µP (cid:107)2] ≤

√

√

ε.

An iteration of FilterUnder2ndMoment may throw out more samples from G than corrupted
samples. However, in expectation, we throw out many more corrupted samples than from the good
set:

11

Algorithm 2 Filter under second moment assumptions
1: function FilterUnder2ndMoment(S)
2:

Compute µS, ΣS, the mean and covariance matrix of S.
Find the eigenvector v∗ with highest eigenvalue λ∗ of ΣS.
if λ∗ ≤ 9 then
return µS

4:

5:

3:

else

6:

7:

8:

9:

end if

10:
11: end function

Draw Z from the distribution on [0, 1] with probability density function 2x.
Let T = Z max{|v∗ · x − µS| : x ∈ S}.
Return the set S(cid:48) = {x ∈ S : |v∗ · (X − µS)| < T }.

Proposition 2. If we run FilterUnder2ndMoment on a set S = G ∪ E \ L for some ε-good set G
and disjoint E, L with |E| ≤ 2ε|S|, |L| ≤ 9ε|S|, then either it returns µS with (cid:107)µS − µP (cid:107)2 ≤ O(
ε),
or else it returns a set S(cid:48) ⊂ S with S(cid:48) = G ∪ E(cid:48) \ L(cid:48) for disjoint E(cid:48) and L(cid:48). In the latter case we
have EZ[|E(cid:48)| + 2|L(cid:48)|] ≤ |E| + 2|L|.

√

For D ∈ {G, E, L, S}, let µD be the mean of D and MD be the matrix EX∈uD[(X−µS)(X−µS)T ].

Lemma 8. If G is an ε-good set with x ≤ 40(cid:112)d/ε for x ∈ S ∪ G, then (cid:107)MG(cid:107)2 ≤ 2(cid:107)µG − µS(cid:107)2

2 + 2 .

Proof. For any unit vector v, we have

vT MGv = EX∈uG[(v · (X − µS))2]

= EX∈uG[(v · (X − µG) + v · (µP − µG))2]
= vT ΣGv + (v · (µG − µS))2
≤ 2 + 2(cid:107)µG − µS(cid:107)2

2 .

Lemma 9. We have that |L|(cid:107)ML(cid:107)2 ≤ 2|G|(1 + (cid:107)µG − µS(cid:107)2

2) .

Proof. Since L ⊆ G, for any unit vector v, we have

|L|vT MLv = |L|EX∈uL[(v · (X − µS))2]
≤ |G|EX∈uG[(v · (X − µS))2]
≤ 2|G|(1 + (cid:107)µG − µS(cid:107)2

2) .

Lemma 10. (cid:107)µG − µS(cid:107)2 ≤ (cid:112)2ε(cid:107)MS(cid:107)2 + 12

√

ε.

Proof. We have that |E|ME ≤ |S|MS + |L|ML and so

|E|(cid:107)ME(cid:107)2 ≤ |S|(cid:107)MS(cid:107)2 + 2|G|(1 + (cid:107)µG − µS(cid:107)2

2) .

By Cauchy Schwarz, we have that (cid:107)ME(cid:107)2 ≥ (cid:107)µE − µS(cid:107)2

2, and so

(cid:112)|E|(cid:107)µE − µS(cid:107)2 ≤

(cid:113)

|S|(cid:107)MS(cid:107)2 + 2|G|(1 + (cid:107)µG − µS(cid:107)2

2) .

12

By Cauchy-Schwarz and Lemma 9, we have that

(cid:112)|L|(cid:107)µL − µS(cid:107)2 ≤ (cid:112)|L|(cid:107)ML(cid:107)2 ≤

(cid:113)

2|G|(1 + (cid:107)µG − µS(cid:107)2

2) .

Since |S|µS = |G|µG + |E|µE − |L|µL and |S| = |G| + |E| − |L|, we get

|G|(µG − µS) = |E|(µE − µS) − |L|(µE − µS) .

Substituting into this, we obtain

(cid:113)

|G|(cid:107)µG − µS(cid:107)2 ≤
√

Since for x, y > 0,

x + y ≤

x +

y, we have

√

√

|E||S|(cid:107)MS(cid:107)2 + 2|E||G|(1 + (cid:107)µG − µS(cid:107)2

2) +

2|L||G|(1 + (cid:107)µG − µS(cid:107)2

2) .

(cid:113)

|G|(cid:107)µG − µS(cid:107)2 ≤ (cid:112)|E||S|(cid:107)MS(cid:107)2 + ((cid:112)2|E||G| + (cid:112)2|L||G|)(1 + (cid:107)µG − µS(cid:107)2) .

Since ||G| − |S|| ≤ ε|S| and |E| ≤ 2ε|S|, |L| ≤ 9ε|S|, we have

(cid:107)µG − µS(cid:107)2 ≤ (cid:112)2ε(cid:107)MS(cid:107)2 + (6
√

√

ε)(1 + (cid:107)µG − µS(cid:107)2) .

Moving the (cid:107)µG − µS(cid:107)2 terms to the LHS, using 6

ε ≤ 1/2, gives

(cid:107)µG − µS(cid:107)2 ≤ (cid:112)2ε(cid:107)MS(cid:107)2 + 12

√

ε .

Since λ∗ = (cid:107)MS(cid:107)2, the correctness if we return the empirical mean is immediate.

Corollary 3. If λ∗ ≤ 9, we have that (cid:107)µG − µS(cid:107)2 = O(

ε).

√

From now on, we assume λ∗ > 9. In this case we have (cid:107)µG − µS(cid:107)2

2 ≤ O(ελ∗). Using Lemma 8,

for suﬃciently small ε. Thus, we have that

(cid:107)MG(cid:107)2 ≤ 2 + O(ελ∗) ≤ 2 + λ∗/5

Now we can show that in expectation, we throw out many more corrupted points from E than

v∗T MSv∗ ≥ 4v∗T MGv∗ .

(7)

we have

from G \ L:

Lemma 11. Let S(cid:48) = G ∪ E(cid:48) \ L(cid:48) for disjoint E(cid:48), L(cid:48) be the set of samples returned by the iteration.
Then we have EZ[|E(cid:48)| + 2|L(cid:48)|] ≤ |E| + 2|L|.
Proof. Let a = maxx∈S |v∗ · x − µS|. Firstly, we look at the expected number of samples we reject:

EZ[|S(cid:48)|] − |S| = EZ

|S| Pr

X∈uS

(cid:21)
[|X − µS| ≥ aZ]

(cid:20)

(cid:90) 1

0
(cid:90) a

= |S|

= |S|

Pr
X∈uS

Pr
X∈uS

(cid:2)|v∗ · (X − µS)| ≥ ax(cid:3) 2xdx

(cid:2)|v∗ · (X − µS)| ≥ T (cid:3) (2T /a)dT

0
= |S|EX∈uS
= (|S|/a) · v∗T MSv∗ .

(cid:2)(v∗ · (X − µS))2(cid:3) /a

13

Next, we look at the expected number of false positive samples we reject, i.e., those in L(cid:48) \ L.

EZ[|L(cid:48)|] − |L| = EZ

(|G| − |L|) Pr

(cid:20)

(cid:20)

(cid:21)
(cid:2)|X − µS| ≥ T (cid:3)

X∈uG\L

(cid:21)
[|v∗ · (X − µS)| ≥ aZ]

≤ EZ

|G| Pr

X∈uG

= |G|

= |G|

≤ |G|

(cid:90) 1

0
(cid:90) a

0
(cid:90) ∞

Pr
X∈uG

Pr
X∈uG

Pr
X∈uG

[|v∗ · (X − µS)| ≥ ax]2x dx

[|v∗ · (X − µS)| ≥ T ](2T /a) dT

[|v∗ · (X − µS)| ≥ T ](2T /a) dT

0
= |G|EX∈uG
= (|G|/a) · v∗T MGv∗ .

(cid:2)(v∗ · (X − µS))2(cid:3) /a

Using (7), we have |S|v∗T MSv∗ ≥ 4|G|v∗T MGv∗ and so EZ[S(cid:48)] − S ≥ 3(EZ[L(cid:48)] − L). Now consider
that |S(cid:48)| = |G|+|E(cid:48)|−|L(cid:48)| = |S|−|E|+|E(cid:48)|+|L|−|L(cid:48)|, and thus |S(cid:48)|−|S| = |E|−|E(cid:48)|+|L(cid:48)|−|L|. This
yields that |E|−EZ[|E(cid:48)|] ≥ 2(EZ[L(cid:48)]−L), which can be rearranged to EZ[|E(cid:48)|+2|L(cid:48)|] ≤ |E|+2|L|.

Proof of Proposition 2. If λ∗ ≤ 9, then we return the mean in Step 5, and by Corollary 3, (cid:107)µS −
µP (cid:107)2 ≤ O(

ε).

√

If λ∗ > 9, then we return S(cid:48). Since at least one element of S has |v∗ · X| = maxx∈S |v∗ · X|,
whatever value of Z is drawn, we still remove at least one element, and so have S(cid:48) ⊂ S. By Lemma
11, we have EZ[|E(cid:48)| + 2|L(cid:48)|] ≤ |E| + 2|L|.

Proof of Theorem 4. Our input is a set S of N = Θ((d/ε) log d) ε-corrupted samples so that with
probability 9/10, S is a 2ε-corrupted set of ε-good samples for P by Lemmas 5 and 7. We have
a set S = G ∪ E(cid:48) \ L, where G(cid:48) is an ε-good set, |E| ≤ 2ε, and |L| ≤ ε. Then, we iteratively
apply FilterUnder2ndMoment until it outputs an approximation to the mean. Since each
iteration removes a sample, this must happen within N iterations. The algorithm takes at most
poly(N, d) = poly(d, 1/ε) time.

√

As long as we can show that the conditions of Proposition 2 hold in each iteration, it ensures that
(cid:107)µS − µP (cid:107)2 ≤ O(
ε). However, the condition that |L| ≤ 9ε|S| need not hold in general. Although
in expectation we reject many more samples in E than G, it is possible that we are unlucky and
reject many samples in G, which could make L large in the next iteration. Thus, we need a bound
on the probability that we ever have |L| > 9ε.

We analyze the following procedure: We iteratively run FilterUnder2ndMoment starting
with a set Si ∪ Ei \ Li of samples with S0 = S and producing a set Si+1 = G ∪ Ei+1 \ Li+1. We stop
if we output an approximation to the mean or if |Li+1| ≥ 13ε|S|. Since we do now always satisfy
the conditions of Proposition 2, this gives that EZ[|Ei+1| + |Li+1|] = |Ei| + 2|Li|. This expectation
is conditioned on the state of the algorithm after previous iterations, which is determined by Si.
Thus, if we consider the random variables Xi = |Ei| + 2|Li|, then we have E[Xi+1|Si] ≤ Xi, i.e.,
the sequence Xi is a sub-martingale with respect to Xi. Using the convention that Si+1 = Si, if
we stop in less than i iterations, and recalling that we always stop in N iterations, the algorithm
fails if and only if |LN | > 9ε|S|. By a simple induction or standard results on sub-martingales, we
have E[XN ] ≤ X0. Now X0 = |E0| + 2|L0| ≤ 3ε|S|. Thus, E[XN ] ≤ 3ε|S|. By Markov’s inequality,
except with probability 1/6, we have XN ≤ 18ε|S|. In this case, |LN | ≤ XN /2 ≤ 9ε|S|. Therefore,
the probability that we ever have |Li| > 9ε is at most 1/6.

14

By a union bound, the probability that the uncorrupted samples satisfy Lemma 5 and Proposi-
tion 2 applies to every iteration is at least 9/10 − 1/6 ≥ 2/3. Thus, with at least 2/3 probability,
the algorithm outputs a vector (cid:98)µ with (cid:107)(cid:98)µ − µP (cid:107)2 ≤ O(

ε).

√

1.3 Robust Covariance Estimation

In this subsection, we give a near sample-optimal eﬃcient robust estimator for the covariance of
a zero-mean Gaussian density, thus proving Theorem 3.3. Our algorithm is essentially identical
to the ﬁltering algorithm given in Section 8.2 of [DKK+16]. As in Section 1.1 the only diﬀerence
is a weaker deﬁnition of the “good set of samples” (Deﬁnition 5) and a concentration argument
(Lemma 3) showing that a random set of uncorrupted samples of the appropriate size is good with
high probability. Given these, the analysis of this subsection follows straightforwardly from the
analysis in Section 8.2 of [DKK+16] by plugging in the modiﬁed parameters.

The algorithm Filter-Gaussian-Unknown-Covariance to robustly estimate the covariance

of a mean 0 Gaussian in [DKK+16] is as follows:

Algorithm 3 Filter algorithm for a Gaussian with unknown covariance matrix.
1: procedure Filter-Gaussian-Unknown-Covariance(S(cid:48), ε, τ )
input: A multiset S(cid:48) such that there exists an (ε, τ )-good set S with ∆(S, S(cid:48)) ≤ 2ε
output: Either a set S(cid:48)(cid:48) with ∆(S, S(cid:48)(cid:48)) < ∆(S, S(cid:48)) or the parameters of a Gaussian G(cid:48) with

dT V (G, G(cid:48)) = O((cid:15) log(1/(cid:15))).

matrix Σ(cid:48).

Let C > 0 be a suﬃciently large universal constant.
Let Σ(cid:48) be the matrix EX∈uS(cid:48)[XX T ] and let G(cid:48) be the mean 0 Gaussian with covariance

2:

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

if there is any x ∈ S(cid:48) so that xT (Σ(cid:48))−1x ≥ Cd log(|S(cid:48)|/τ ) then

return S(cid:48)(cid:48) = S(cid:48) − {x : xT (Σ(cid:48))−1x ≥ Cd log(|S(cid:48)|/τ )}.

end if
Compute an approximate eigendecomposition of Σ(cid:48) and use it to compute Σ(cid:48)−1/2
Let x(1), . . . , x(|S(cid:48)|) be the elements of S(cid:48).
For i = 1, . . . , |S(cid:48)|, let y(i) = Σ(cid:48)−1/2x(i) and z(i) = y⊗2
(i) .
Let TS(cid:48) = −I (cid:91)I (cid:91)T + (1/|S(cid:48)|) (cid:80)|S(cid:48)|
i=1 z(i)zT
(i).
Approximate the top eigenvalue λ∗ and corresponding unit eigenvector v∗ of TS(cid:48)..
Let p∗(x) = 1√
2
if λ∗ ≤ (1 + C(cid:15) log2(1/(cid:15)))QG(cid:48)(p∗) then

((Σ(cid:48)−1/2x)T v∗(cid:93)(Σ(cid:48)−1/2x) − tr(v∗(cid:93)))

return G(cid:48)

end if
Let µ be the median value of p∗(X) over X ∈ S(cid:48).
Find a T ≥ C(cid:48) so that

(|p∗(X) − µ| ≥ T + 4/3) ≥ Tail(T, d, ε, τ )

Pr
X∈uS(cid:48)

return S(cid:48)(cid:48) = {X ∈ S(cid:48) : |p∗(X) − µ| < T }.

17:
18: end procedure

In [DKK+16], we take Tail(T, d, ε, τ ) = 12 exp(−T )+3(cid:15)/(d log(N/τ ))2, where N = Θ((d log(d/ετ ))6/ε2)

is the number of samples we took there.

15

To get a near sample-optimal algorithms, we will need a weaker deﬁnition of a good set. To
use this, we will need to weaken the tail bound in the algorithm to Tail(T, d, ε, τ ) = ε/(T 2 log2(T )),
when T ≥ 10 log(1/ε). For T ≤ 10 log(1/ε), we take Tail(T, d, ε, τ ) = 1 so that we always choose
T ≥ 10 log(1/ε). It is easy to show that the integrals of this tail bound used in the proofs of Lemma
8.19 and Claim 8.22 of [DKK+16] have similar bounds. Thus, our analysis here will sketch that
these tail bounds hold for a set of Ω(d2 log5(d/ετ )/ε2) samples from the Guassian.

Firstly, we state the new, weaker, deﬁnition of a good set:

Deﬁnition 5. Let G be a Gaussian in Rd with mean 0 and covariance Σ. Let (cid:15) > 0 be suﬃciently
small. We say that a multiset S of points in Rd is ε-good with respect to G if the following hold:

1. For all x ∈ S, xT Σ−1x < d + O(

d log(d/ε)).

√

2. We have that (cid:107)Σ−1/2Cov(S)Σ−1/2 − I(cid:107)F = O(ε).

3. For all even degree-2 polynomials p, we have that Var(p(S)) = Var(p(G))(1 + O(ε)).

4. For p an even degree-2 polynomial with E[p(G)] = 0 and Var(p(G)) = 1, and for any T >

10 log(1/ε) we have that

(|p(x)| > T ) ≤ ε/(T 2 log2(T )).

Pr
x∈uS

It is easy to see that the algorithm and analysis of

[DKK+16] can be pushed through using the
above weaker deﬁnition. That is, if S is a good set, then G can be recovered to ˜O(ε) error from an
ε-corrupted version of S. Our main task will be to show that random sets of the appropriate size
are good with high probability.

Proposition 3. Let N be a suﬃciently large constant multiple of d2 log5(d/ε)/ε2. Then a set S of
N independent samples from G is ε-good with respect to G with high probability.

Proof. First, note that it suﬃces to prove this when G = N (0, I).
Condition 1 follows by standard concentration bounds on (cid:107)x(cid:107)2
2.
Condition 2 follows by estimating the entry-wise error between Cov(S) and I.
Condition 3 is slightly more involved. Let {pi} be an orthonormal basis for the set of even,
degree-2, mean-0 polynomials with respect to G. Deﬁne the matrix Mi,j = Ex∈uS[pi(x)pj(x)] − δi,j.
This condition is equivalent to (cid:107)M (cid:107)2 = O(ε). Thus, it suﬃces to show that for every v with (cid:107)v(cid:107)2 = 1
that vT M v = O(ε). It actually suﬃces to consider a cover of such v’s. Note that this cover will be
of size 2O(d2). For each v, let pv = (cid:80)
i vipi. We need to show that Var(pv(S)) = 1 + O(ε). We can
show this happens with probability 1 − 2−Ω(d2), and thus it holds for all v in our cover by a union
bound.

Condition 4 is substantially the most diﬃcult of these conditions to prove. Naively, we would
want to ﬁnd a cover of all possible p and all possible T , and bound the probability that the desired
condition fails. Unfortunately, the best a priori bound on Pr(|p(G)| > T ) are on the order of
exp(−T ). As our cover would need to be of size 2d2 or so, to make this work with T = d, we would
require on the order of d3 samples in order to make this argument work.

However, we will note that this argument is suﬃcient to cover the case of T < 10 log(1/ε) log2(d/ε).
Fortunately, most such polynomials p satisfy much better tail bounds. Note that any even, mean
zero polynomial p can be written in the form p(x) = xT Ax − tr(A) for some matrix A. We call
A the associated matrix to p. We note by the Hanson-Wright inequality that Pr(|p(G)| > T ) =
exp(−Ω(min((T /(cid:107)A(cid:107)F )2, T /(cid:107)A(cid:107)2))). Therefore, the tail bounds above are only as bad as described
when A has a single large eigenvalue. To take advantage of this, we will need to break p into parts
based on the size of its eigenvalues. We begin with a deﬁnition:

16

Deﬁnition 6. Let Pk be the set of even, mean-0, degree-2 polynomials, so that the associated matrix
A satisﬁes:

1. rank(A) ≤ k
√

2. (cid:107)A(cid:107)2 ≤ 1/

k.

Note that for p ∈ Pk that |p(x)| ≤ |x|2/
Importantly, any polynomial can be written in terms of these sets.

k +

k.

√

√

Lemma 12. Let p be an even, degree-2 polynomial with E[p(G)] = 0, Var(p(G)) = 1. Then if
t = (cid:98)log2(d)(cid:99), it is possible to write p = 2(p1 + p2 + . . . + p2t + pd) where pk ∈ Pk.

Proof. Let A be the associated matrix to p. Note that (cid:107)A(cid:107)F = Varp = 1. Let Ak be the matrix
corresponding to the top k eigenvalues of A. We now let p1 be the polynomial associated to A1/2,
p2 be associated to (A2 − A1)/2, p4 be associated to (A4 − A2)/2, and so on.
It is clear that
p = 2(p1 + p2 + . . . + p2t + pd). It is also clear that the matrix associated to pk has rank at most
k. If the matrix associated to pk had an eigenvalue more than 1/
k, it would need to be the case
that the k/2nd largest eigenvalue of A had size at least 2/
k. This is impossible since the sum of
the squares of the eigenvalues of A is at most 1.

√

√

This completes our proof.

We will also need covers of each of these sets Pk.

Lemma 13. For each k, there exists a set Ck ⊂ Pk so that

1. For each p ∈ Pk there exists a q ∈ Ck so that (cid:107)p(G) − q(G)(cid:107)2 ≤ (ε/d)2.

2. |Ck| = 2O(dk log(d/ε)).

√

Proof. We note that any such p is associated to a matrix A of the form A = (cid:80)k
i , for
k] and vi orthonormal. It suﬃces to let q correspond to the matrix A(cid:48) = (cid:80)k
i=1 µiwiwT
λi ∈ [0, 1/
i
for with |λi − µi| < (ε/d)3 and |vi − wi| < (ε/d)3 for all i. It is easy to let µi and wi range over
covers of the interval and the sphere with appropriate errors. This gives a set of possible q’s of
size 2O(dk log(d/ε)) as desired. Unfortunately, some of these q will not be in Pk as they will have
eigenvalues that are too large. However, this is easily ﬁxed by replacing each such q by the closest
element of Pk. This completes our proof.

i=1 λivivT

We next will show that these covers are suﬃcient to express any polynomial.

Lemma 14. Let p be an even degree-2 polynomial with E[p(G)] = 0 and Var(p(G)) = 1. It is
possible to write p as a sum of O(log(d)) elements of some Ck plus another polynomial of L2 norm
at most ε/d.

Proof. Combining the above two lemmas we have that any such p can be written as

p = (q1 + p1) + (q2 + p2) + . . . (q2t + p2t) + (qd + pd) = q1 + q2 + . . . + q2t

+ qd + p(cid:48) ,

where qk above is in Ck and (cid:107)pk(G)(cid:107)2 < (ε/d)2. Thus, p(cid:48) = p1 + p2 + . . . + p2t + pd has (cid:107)p(cid:48)(G)(cid:107)2 ≤
(ε/d). This completes the proof.

17

The key observation now is that if |p(x)| ≥ T for (cid:107)x(cid:107)2 ≤ (cid:112)d/ε, then writing p = q1 + q2 + q4 +
. . . + qd + p(cid:48) as above, it must be the case that |qk(x)| > (T − 1)/(2 log(d)) for some k. Therefore,
to prove our main result, it suﬃces to show that, with high probability over the choice of S, for
any T ≥ 10 log(1/ε) log2(d/ε) and any q ∈ Ck for some k, that Prx∈uS(|q(x)| > T /(2 log(d))) <
ε/(2T 2 log2(T ) log(d)). Equivalently, it suﬃces to show that for T ≥ 10 log(1/ε) log(d/ε) it holds
Prx∈uS(|q(x)| > T /(2 log(d))) < ε/(2T 2 log2(T ) log2(d)). Note that this holds automatically for
T > (d/ε), as p(x) cannot possibly be that large for (cid:107)x(cid:107)2 ≤ (cid:112)d/ε. Furthermore, note that losing a
constant factor in the probability, it suﬃces to show this only for T a power of 2.

Therefore,

kε (cid:29) T (cid:29)
it suﬃces to show for every k ≤ d, every q ∈ Ck and every d/
log(1/ε) log(d/ε) that with probability at least 1 − 2−Ω(dk log(d/ε)) over the choice of S we have
that Prx∈uS(|q(x)| > T ) (cid:28) ε/(T 2 log4(d/ε)). However, by the Hanson-Wright inequality, we have
that

√

√

Pr(|q(G)| > T ) = exp(−Ω(min(T 2, T

k))) < (ε/(T 2 log4(d/ε)))2 .

Therefore, by Chernoﬀ bounds, the probability that more than a ε/(T 2 log4(d/ε))-fraction of the
elements of S satisfy this property is at most

exp(−Ω(min(T 2, T

√

k))|S|ε/(T 2 log4(d/ε))) = exp(−Ω(|S|ε/(log4(d/ε)) min(1,
≤ exp(−Ω(|S|ε2/(log4(d/ε))k/d))
≤ exp(−Ω(dk log(d/ε))) ,

√

k/T )))

as desired.

This completes our proof.

2 Omitted Details from Section 5

2.1 Full description of the distributions for experiments

Here we formally describe the distributions we used in our experiments. In all settings, our goal
was to ﬁnd noise distributions so that noise points were not “obvious” outliers, in the sense that
there is no obvious pointwise pruning process which could throw away the noise points, which still
gave the algorithms we tested the most diﬃculty. We again remark that while other algorithms had
varying performances depending on the noise distribution, it seemed that the performance of ours
was more or less unaﬀected by it.

Distribution for the synthetic mean experiment Our uncorrupted points were generated by
N (µ, I), where µ is the all-ones vector. Our noise distribution is given as

where Π1 is the product distribution over the hypercube where every coordinate is 0 or 1 with
probability 1/2, and Π2 is a product distribution where the ﬁrst coordinate is ether 0 or 12 with equal
probability, the second coordinate is −2 or 0 with equal probability, and all remaining coordinates
are zero.

Distribution for the synthetic covariance experiment For the isotropic synthetic covariance
experiment, our uncorrupted points were generated by N (0, I), and the noise points were all zeros.
For the skewed synthetic covariance experiment, our uncorrupted points were generated by N (0, I +

N =

Π1 +

Π2 ,

1
2

1
2

18

10e1eT
1 ), where e1 is the ﬁrst unit vector, and our noise points were generated as follows: we took
a ﬁxed random rotation of points of the form Yi ∼ Π, where Π is a product distribution whose
ﬁrst d/2 coordinates are ±0.5 with probability 1/2, and whose next d/2 − 1 coordinates are each
0.8 × Ai, where for each coordinate i, Ai is an independent random integer between −2 and 2, and
whose last coordinate is a uniformly random integer between [−10, 10].

Setup for the semi-synthetic geographic experiment We took the 20 dimensional data
from [NJB+08], which was diagonalized, and randomly rotated it. This was to simulate the higher
dimensional case, since the singular vectors that [NJB+08] obtained did not seem to be sparse or
analytically sparse. Our noise was distributed as Π, where Π is a product distribution whose ﬁrst
d/2 coordinates are each uniformly random integers between 0 and 2 and whose last d/2 coordinates
are each uniformly randomly either 2 or 3, all scaled by a factor of 1/24.

2.2 Comparison with other robust PCA methods on semi-synthetic data

In addition to comparing our results with simple pruning techniques, as we did in Figure 3 in the
main text, we also compared our algorithm with implementations of other robust PCA techniques
from the literature with accessible implementations. In particular, we compared our technique with
RANSAC-based techniques, LRVCov, two SDPs ([CLMW11, XCS10]) for variants of robust PCA,
and an algorithm proposed by [CLMW11] to speed up their SDP based on alternating descent. For
the SDPs, since black box methods were too slow to run on the full data set (as [CLMW11] mentions,
black-box solvers for the SDPs are impractical above perhaps 100 data points), we subsample the
data, and run the SDP on the subsampled data. For each of these methods, we ran the algorithm
on the true data points plus noise, where the noise was generated as described above. We then take
the estimate of the covariance it outputs, and project the data points onto the top two singular
values of this matrix, and plot the results in Figure 1.

Similar results occurred for most noise patterns we tried. We found that only our algorithm
and LRVCov were able to reasonably reconstruct Europe, in the presence of this noise. It is hard to
judge qualitatively which of the two maps generated is preferable, but it seems that ours stretches
the picture somewhat less than LRVCov.

19

Figure 1: Comparison with other robust methods on the Europe semi-synthetic data. From left to
right, top to bottom: the original projection without noise, what our algorithm recovers, RANSAC,
LRVCov, the ADMM method proposed by [CLMW11], the SDP proposed by [XCS10] with subsam-
pling, and the SDP proposed by [CLMW11] with subsampling.

20

-0.2-0.100.10.20.3-0.15-0.1-0.0500.050.10.150.2XCS Projection-0.2-0.100.10.20.3-0.15-0.1-0.0500.050.10.150.2CLMW SDP Projection-0.2-0.100.10.20.3-0.15-0.1-0.0500.050.10.150.2CLMW ADMM Projection-0.2-0.100.10.20.3-0.15-0.1-0.0500.050.10.150.2RANSAC Projection-0.2-0.100.10.20.3-0.15-0.1-0.0500.050.10.150.2LRV Projection-0.2-0.100.10.20.3-0.15-0.1-0.0500.050.10.150.2Original Data-0.2-0.100.10.20.3-0.2-0.15-0.1-0.0500.050.10.15Filter ProjectionReferences

[CLMW11] E. J. Candès, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? J.

ACM, 58(3):11, 2011.

[DKK+16]

I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robust
estimators in high dimensions without the computational intractability. In Proceedings
of FOCS’16, 2016. Full version available at https://arxiv.org/pdf/1604.06443.pdf.

[DL01]

L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer
Series in Statistics, Springer, 2001.

[NJB+08]

J. Novembre, T. Johnson, K. Bryc, Z. Kutalik, A. R. Boyko, A. Auton, A. Indap, K. S.
King, S. Bergmann, M. R. Nelson, et al. Genes mirror geography within europe. Nature,
456(7218):98–101, 2008.

[T+15]

J. A. Tropp et al. An introduction to matrix concentration inequalities. Foundations
and Trends in Machine Learning, 8(1-2):1–230, 2015.

[Ver10]

R. Vershynin. Introduction to the non-asymptotic analysis of random matrices, 2010.

[XCS10]

H. Xu, C. Caramanis, and S. Sanghavi. Robust pca via outlier pursuit. In Advances in
Neural Information Processing Systems, pages 2496–2504, 2010.

21

