Supplementary Material for
Adversarial Variational Bayes: Unifying Variational Autoencoders and
Generative Adversarial Networks

Lars Mescheder 1

Sebastian Nowozin 2

Andreas Geiger 1 3

Abstract
In the main text we derived Adversarial Varia-
tional Bayes (AVB) and demonstrated its useful-
ness both for black-box Variational Inference and
for learning latent variable models. This doc-
ument contains proofs that were omitted in the
main text as well as some further details about
the experiments and additional results.

To apply our method in practice, we need to obtain unbi-
ased gradients of the ELBO. As it turns out, this can be
achieved by taking the gradients w.r.t. a ﬁxed optimal dis-
criminator. This is a consequence of the following Propo-
sition:
Proposition 2. We have

Eqφ(z|x) (∇φT ∗(x, z)) = 0.

(3.6)

I. Proofs

This section contains the proofs that were omitted in the
main text.

The derivation of AVB in Section 3.1 relies on the fact that
we have an explicit representation of the optimal discrim-
inator T ∗(x, z). This was stated in the following Proposi-
tion:
Proposition 1. For pθ(x | z) and qφ(z | x) ﬁxed, the opti-
mal discriminator T ∗ according to the objective in (3.3) is
given by

T ∗(x, z) = log qφ(z | x) − log p(z).

(3.4)

Proof. As in the proof of Proposition 1 in Goodfellow et al.
(2014), we rewrite the objective in (3.3) as

(cid:90)

(cid:0)pD(x)qφ(z | x) log σ(T (x, z))

+ pD(x)p(z) log(1 − σ(T (x, z))(cid:1)dxdz.

(I.1)

This integral is maximal as a function of T (x, z) if and only
if the integrand is maximal for every (x, z). However, the
function

t (cid:55)→ a log(t) + b log(1 − t)

(I.2)

attains its maximum at t = a

σ(T ∗(x, z)) =

a+b , showing that
qφ(z | x)
qφ(z | x) + p(z)

or, equivalently,

Proof. By Proposition 1,

Eqφ(z|x) (∇φT ∗(x, z))

= Eqφ(z|x) (∇φ log qφ(z | x)) .

(I.5)

For an arbitrary family of probability densities qφ we have

Eqφ (∇φ log qφ) =

qφ(z)

(cid:90)

(cid:90)

∇φqφ(z)
qφ(z)

dz

= ∇φ

qφ(z)dz = ∇φ1 = 0.

(I.6)

Together with (8.5), this implies (3.6).

In Section 3.3 we characterized the Nash-equilibria of the
two-player game deﬁned by our algorithm. The follow-
ing Proposition shows that in the nonparametric limit for
T (x, z) any Nash-equilibrium deﬁnes a global optimum of
the variational lower bound:
Proposition 3. Assume that T can represent any function
of two variables. If (θ∗, φ∗, T ∗) deﬁnes a Nash-equilibrium
of the two-player game deﬁned by (3.3) and (3.7), then

T ∗(x, z) = log qφ∗ (z | x) − log p(z)

(3.8)

and (θ∗, φ∗) is a global optimum of the variational lower
bound in (2.4).

(I.3)

Proof. If (θ∗, φ∗, T ∗) deﬁnes a Nash-equilibrium, Propo-
sition 1 shows (3.8). Inserting (3.8) into (3.5) shows that
(φ∗, θ∗) maximizes

T ∗(x, z) = log qφ(z | x) − log p(z).

(I.4)

EpD(x)Eqφ(z|x)

(cid:0)− log qφ∗ (z | x) + log p(z)

+ log pθ(x | z)(cid:1)

(I.7)

as a function of φ and θ. A straightforward calculation
shows that (8.7) is equal to

II. Adaptive Contrast

Adversarial Variational Bayes

L(θ, φ) + EpD(x)KL(qφ(z | x), qφ∗ (z | x))

(I.8)

where

L(θ, φ) := EpD(x)

− KL(qφ(z | x), p(z))

(cid:104)

(cid:105)
+ Eqφ(z|x) log pθ(x | z)

(I.9)

is the variational lower bound in (2.4).

Notice that (8.8) evaluates to L(θ∗, φ∗) when we insert
(θ∗, φ∗) for (θ, φ).

Assume now, that (θ∗, φ∗) does not maximize the varia-
tional lower bound L(θ, φ). Then there is (θ(cid:48), φ(cid:48)) with

L(θ(cid:48), φ(cid:48)) > L(θ∗, φ∗).

(I.10)

Inserting (θ(cid:48), φ(cid:48)) for (θ, φ) in (8.8) we obtain

L(θ(cid:48), φ(cid:48)) + EpD(x)KL(qφ(cid:48)(z | x), qφ∗ (z | x)),

(I.11)

which is strictly bigger than L(θ∗, φ∗), contradicting the
fact that (θ∗, φ∗) maximizes (8.8). Together with (3.8), this
proves the theorem.

In Section 4 we derived a variant of AVB that contrasts the
current inference model with an adaptive distribution rather
than the prior. This leads to Algorithm 2. Note that we do
not consider the µ(k) and σ(k) to be functions of φ and
therefore do not backpropagate gradients through them.

Algorithm 2 Adversarial Variational Bayes with Adaptive
Constrast (AC)
1: i ← 0
2: while not converged do
3:
4:
5:
6:
7:
8:

Sample {x(1), . . . , x(m)} from data distrib. pD(x)
Sample {z(1), . . . , z(m)} from prior p(z)
Sample {(cid:15)(1), . . . , (cid:15)(m)} from N (0, 1)
Sample {η(1), . . . , η(m)} from N (0, 1)
for k = 1, . . . , m do

z(k)
φ , µ(k), σ(k) ← encoderφ(x(k), (cid:15)(k))
¯z(k)
φ ←
end for
Compute θ-gradient (eq. 3.7):

z(k)
φ −µ(k)
σ(k)

9:
10:
11:

gθ ← 1
m

(cid:80)m

k=1 ∇θ log pθ

x(k), z(k)
φ

(cid:16)

(cid:16)

(cid:17)

(cid:17)

12:

Compute φ-gradient (eq. 3.7):

gφ ← 1
m

(cid:80)m

k=1 ∇φ

(cid:2)−Tψ

+ log pθ
Compute ψ-gradient (eq. 3.3) :

13:

x(k), ¯z(k)
+ 1
φ
(cid:16)
x(k), z(k)
φ

φ (cid:107)2

2 (cid:107)¯z(k)
(cid:17)(cid:3)

gψ ← 1
m

(cid:80)m

k=1 ∇ψ

(cid:104)

(cid:16)

σ(Tψ(x(k), ¯z(k)
φ

log
+ log (cid:0)1 − σ(Tψ(x(k), η(k))(cid:1)(cid:105)

(cid:17)

14:

Perform SGD-updates for θ, φ and ψ:
θ ← θ + hi gθ, φ ← φ + hi gφ, ψ ← ψ + hi gψ
i ← i + 1

15:
16: end while

III. Architecture for MNIST-experiment

To apply Adaptive Contrast to our method, we have to be
able to efﬁciently estimate the moments of the current in-
ference model qφ(z | x). To this end, we propose a network
architecture like in Figure 8. The ﬁnal output z of the net-
work is a linear combination of basis noise vectors where
the coefﬁcients depend on the data point x, i.e.

zk =

vi,k((cid:15)i)ai,k(x).

(III.1)

m
(cid:88)

i=1

The noise basis vectors vi((cid:15)i) are deﬁned as the out-
put of small fully-connected neural networks fi acting on
normally-distributed random noise (cid:15)i, the coefﬁcient vec-

Adversarial Variational Bayes

(cid:15)1

...

(cid:15)m

f1

...

fm

v1

...

vm

x

∗

...

∗

a1

...

am

+

g

z

Figure 8. Architecture of the network used for the MNIST-
experiment

Figure 9. Independent samples for a model trained on celebA.

(a) Training data

(b) Random samples

tors ai(x) are deﬁned as the output of a deep convolutional
neural network g acting on x.

The moments of the zi are then given by

E(zk) =

E[vi,k((cid:15)i)]ai,k(x).

(III.2)

Var(zk) =

Var[vi,k((cid:15)i)]ai,k(x)2.

(III.3)

m
(cid:88)

i=1
m
(cid:88)

i=1

By estimating E[vi,k((cid:15)i)] and Var[vi,k((cid:15)i)] via sampling
once per mini-batch, we can efﬁciently compute the mo-
ments of qφ(z | x) for all the data points x in a single
mini-batch.

IV. Additional Experiments

celebA We also used AVB (without AC) to train a deep
convolutional network on the celebA-dataset (Liu et al.,
2015) for a 64-dimensional latent space with N (0, 1)-prior.
For the decoder and adversary we use two deep convolu-
tional neural networks acting on x like in Radford et al.
(2015). We add the noise (cid:15) and the latent code z to each
hidden layer via a learned projection matrix. Moreover, in
the encoder and decoder we use three RESNET-blocks (He
et al., 2015) at each scale of the neural network. We add
the log-prior log p(z) explicitly to the adversary T (x, z),
so that it only has to learn the log-density of the inference
model qφ(z | x).

The samples for celebA are shown in Figure 9. We see
that our model produces visually sharp images of faces. To
demonstrate that the model has indeed learned an abstract
representation of the data, we show reconstruction results
and the result of linearly interpolating the z-vector in the la-
tent space in Figure 10. We see that the reconstructions are
reasonably sharp and the model produces realistic images
for all interpolated z-values.

Figure 10. Interpolation experiments for celebA

MNIST To evaluate how AVB with adaptive contrast
compares against other methods on a ﬁxed decoder archi-
tecture, we reimplemented the methods from Maaløe et al.
(2016) and Kingma et al. (2016). The method from Maaløe
et al. (2016) tries to make the variational approximation
to the posterior more ﬂexible by using auxiliary variables,
the method from Kingma et al. (2016) tries to improve the
variational approximation by employing an Inverse Autore-
gressive Flow (IAF), a particularly ﬂexible instance of a
normalizing ﬂow (Rezende & Mohamed, 2015). In our ex-
periments, we compare AVB with adaptive contrast to a
standard VAE with diagonal Gaussian inference model as
well as the methods from Maaløe et al. (2016) and Kingma
et al. (2016).

In our ﬁrst experiment, we evaluate all methods on training
a decoder that is given by a fully-connected neural network
with ELU-nonlinearities and two hidden layers with 300
units each. The prior distribution p(z) is given by a 32-
dimensional standard-Gaussian distribution.

that
The results are shown in Table 3a. We observe,
both AVB and the VAE with auxiliary variables achieve
a better (approximate) ELBO than a standard VAE. When
evaluated using AIS, both methods result in similar log-

Adversarial Variational Bayes

ELBO

AIS

AVB + AC
VAE
auxiliary VAE
VAE + IAF

≈ −85.1 ± 0.2 −83.7 ± 0.3
−85.0 ± 0.3
−83.8 ± 0.3
−84.9 ± 0.3

−88.9 ± 0.2
−88.0 ± 0.2
−88.9 ± 0.2

reconstr. error
59.3 ± 0.2
62.2 ± 0.2
62.1 ± 0.2
62.3 ± 0.2

(a) fully-connected decoder (dim(z) = 32)

ELBO

AIS

AVB + AC
VAE
auxiliary VAE
VAE + IAF

≈ −93.8 ± 0.2 −89.7 ± 0.3
−89.9 ± 0.4
−89.7 ± 0.3
−89.7 ± 0.3

−94.9 ± 0.2
−95.0 ± 0.2
−94.4 ± 0.2

reconstr. error
76.4 ± 0.2
76.7 ± 0.2
76.8 ± 0.2
76.1 ± 0.2

(b) convolutional decoder (dim(z) = 8)

ELBO

AIS

AVB + AC
VAE
auxiliary VAE
VAE + IAF

≈ −82.7 ± 0.2 −81.7 ± 0.3
−81.9 ± 0.3
−81.6 ± 0.3
−82.1 ± 0.4

−85.7 ± 0.2
−85.6 ± 0.2
−85.5 ± 0.2

reconstr. error
57.0 ± 0.2
59.4 ± 0.2
59.6 ± 0.2
59.6 ± 0.2

(c) convolutional decoder (dim(z) = 32)

likelihoods. However, AVB results in a better reconstruc-
tion error than an auxiliary variable VAE and a better (ap-
proximate) ELBO. We observe that our implementation of
a VAE with IAF did not improve on a VAE with diagonal
Gaussian inference model. We suspect that this due to op-
timization difﬁculties.

In our second experiment, we train a decoder that is given
by the shallow convolutional neural network described in
Salimans et al. (2015) with 800 units in the last fully-
connected hidden layer. The prior distribution p(z) is given
by either a 8-dimensional or a 32-dimensional standard-
Gaussian distribution.

The results are shown in Table 3b and Table 3c. Even
though AVB achieves a better (approximate) ELBO and
a better reconstruction error for a 32-dimensional latent
space, all methods achieve similar log-likelihoods for this
decoder-architecture, raising the question if strong infer-
ence models are always necessary to obtain a good genera-
tive model. Moreover, we found that neither auxiliary vari-
ables nor IAF did improve the ELBO. Again, we believe
this is due to optimization challenges.

