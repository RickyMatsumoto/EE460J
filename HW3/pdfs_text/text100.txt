A. Derivation of the Block-Diagonal Hessian Recursion

Practical Gauss-Newton Optimisation for Deep Learning

The diagonal blocks of the pre-activation Hessian of a feedforward neural network can be related to each other via the
recursion in (8). Starting from its deﬁnition in (6) we can derive the recursion:

Hλ =

∂
∂hλ
b

∂E
∂hλ
a

=

(cid:88)

=

W λ+1
i,a

∂
∂hλ
b
(cid:32)

(cid:88)

i

∂E
∂hλ+1
i

∂E
∂hλ+1
i

∂aλ
a
∂hλ
a

∂hλ+1
i
∂hλ
a
(cid:33)

(cid:88)

=

i

∂
∂hλ
b
(cid:32)

(cid:88)

=

W λ+1
i,a

i

∂
∂hλ
b


i

i

(cid:88)

=

W λ+1
i,a



∂aλ
a
∂hλ
a

(cid:88)

∂2E

∂hλ+1
j

j

∂hλ+1
i
(cid:33)

∂hλ+1
j
∂hλ
b

+

∂E
∂hλ+1
i

δa,b





∂2aλ
a
2
∂hλ
a

(cid:32)

∂E
∂hλ+1
i

∂hλ+1
i
∂aλ
a

∂aλ
a
∂hλ
a

(cid:33)

∂aλ
a
∂hλ
a

∂2E
b ∂hλ+1

i

∂hλ

+

∂E
∂hλ+1
i

∂2aλ
a
a∂hλ
b

∂hλ

(cid:33)

= δa,b

= δa,b

∂2aλ
a
2
∂hλ
a
∂2aλ
a
2
∂hλ
a

(cid:32)

(cid:88)

i

∂E
∂aλ
a

W λ+1
i,a

∂E
∂hλ+1
i

(cid:88)

+

W λ+1
i,a

∂aλ
a
∂hλ
a

∂2E

∂hλ+1
j

∂hλ+1
i

W λ+1
j,b

∂aλ
b
∂hλ
b

(cid:88)

+

W λ+1
i,a

i,j

∂aλ
a
∂hλ
a

i,j

∂2E

∂hλ+1
j

∂hλ+1
i

∂aλ
b
∂hλ
b

W λ+1
j,b

Hence the pre-activation Hessian can be written in matrix notation as

Hλ = BλW T

λ+1Hλ+1Wλ+1Bλ + Dλ

where we deﬁne the diagonal matrices

[Bλ]a,a(cid:48) = δa,a(cid:48)

= δa,a(cid:48)f (cid:48)(hλ
a)

[Dλ]a,a(cid:48) = δa,a(cid:48)

= δa,a(cid:48)f (cid:48)(cid:48)(hλ
a)

∂E
∂xλ
a

∂aλ
a
∂hλ
a
∂2aλ
a
2
∂hλ
a

∂E
∂xλ
a

f (cid:48) and f (cid:48)(cid:48) are the ﬁrst and second derivatives of the transfer function f respectively.

Note that this is a special case of a more general recursion for calculating a Hessian (Gower & Gower, 2016).

B. Implementation Details

Second-order optimisation methods are based on ﬁnding some positive semi-deﬁnite quadratic approximation to the func-
tion of interest around the current value of the parameters. For the rest of the appendix we deﬁne ˆf to be a local quadratic
approximation to f given a positive semi-deﬁnite curvature matrix C:

f (θ + δ) ≈ f (θ) + δT∇θf +

δTCδ = ˆf (δ; C)

1
2

The curvature matrix depends on the speciﬁc optimisation method and will often be only an estimate. For notational
simplicity, the dependence of ˆf on θ is omitted. Setting C to the true Hessian matrix of f would make ˆf the exact second-
order Taylor expansion of the function around θ. However, when f is a nonlinear function, the Hessian can be indeﬁnite,
which leads to an ill-conditioned quadratic approximation ˆf . For this reason, C is usually chosen to be positive-semi
deﬁnite by construction, such as the Gauss-Newton or the Fisher matrix. In the experiments discussed in the paper, C
can be either the full Gauss-Newton matrix ¯G, obtained from running Conjugate Gradient as in (Martens, 2010), or a
block diagonal approximation to it, denoted by (cid:101)G. The analysis below is independent of whether this approximation is
based on KFLR, KFRA, KFAC or if it is the exact block-diagonal part of ¯G, hence there will be no reference to a speciﬁc
approximation.

Damping plays an important role in second-order optimisation methods. It can improve the numerical stability of the
quadratic approximation and can also be related to trust region methods. Hence, we will introduce two damping coefﬁcients
- one for ¯G and one for (cid:101)G. In practice, an additional weight decay term is often applied to neural network models. As a
result and following the presentation in (Martens & Grosse, 2015), a total of three extra parameters are introduced:

(28)

(29)

(30)

(31)

(32)

Practical Gauss-Newton Optimisation for Deep Learning

• A L2 regularisation on θ with weight η

2 , which implies an additive diagonal term to both ¯G and (cid:101)G

• A damping parameter τ added to the full Gauss-Newton matrix ¯G

• A separate damping parameter γ added to the approximation matrix (cid:101)G

Subsequently, for notational convenience, we deﬁne ¯C = ¯G + (τ + η)I, which is the curvature matrix obtained when using
the full Gauss-Newton matrix in the quadratic approximation (32). Similarly, (cid:101)C = (cid:101)G + (γ + η)I is the curvature matrix
obtained when using any of the block-diagonal approximations.

B.1. Inverting the Approximate Curvature Matrix

The Gauss-Newton method calculates its step direction by multiplying the gradient with the inverse of the curvature matrix,
in this case (cid:101)C. This gives the unmodiﬁed step direction:

Since (cid:101)C is a block diagonal matrix (each block corresponds to the parameters of a single layer) the problem naturally
factorises to solving L independent linear systems:

For all of the approximate methods under consideration – KFLR, KFRA and KFAC – the diagonal blocks (cid:101)Gλ have a
Kronecker factored form (cid:101)Qλ ⊗ (cid:101)Gλ, where (cid:101)Qλ = E [Qλ] and (cid:101)Gλ denotes the approximation to E [Gλ] obtained from the
method of choice. Setting k = (η + γ) implies:

(cid:101)δ = (cid:101)C −1∇θf

(cid:101)δλ = (cid:101)C −1

λ ∇Wλf

(cid:101)Cλ = (cid:101)Qλ ⊗ (cid:101)Gλ + kI ⊗ I

The exact calculation of (34) given the structural form of (cid:101)Cλ requires the eigen decomposition of both matrices (cid:101)Qλ and (cid:101)Gλ,
see (Martens & Grosse, 2015). However, the well known Kronecker identity (A ⊗ B)−1vec (V ) = A−1V B−1 motivates
the following approximation:

(cid:101)Qλ ⊗ (cid:101)Gλ + kI ⊗ I ≈

(cid:101)Qλ + ω

(cid:16)

√

(cid:17)

(cid:16)

kI

⊗

(cid:101)Gλ + ω−1

√

(cid:17)

kI

The optimal setting of ω can be found analytically by bounding the norm of the approximation’s residual, namely:

R(ω) = (cid:101)Qλ ⊗ (cid:101)Gλ + kI ⊗ I −
√

(cid:16)

√

= −ω−1
√

k (cid:101)Qλ ⊗ I − ω

k(cid:101)Gλ ⊗ I
√

||R(π)|| ≤ ω−1

k|| (cid:101)Qλ ⊗ I|| + ω

k||(cid:101)Gλ ⊗ I||

(cid:101)Qλ + ω

√

(cid:17)

(cid:16)

kI

⊗

(cid:101)Gλ + ω−1

√

(cid:17)

kI

Minimising the right hand side with respect to ω gives the solution

(cid:115)

ω =

|| (cid:101)Qλ ⊗ I||
||I ⊗ (cid:102)Gλ||

(33)

(34)

(35)

(36)

(37)

(38)

The choice of the norm is arbitrary, but for comparative purposes with previous work on KFAC, we use the trace norm
in all of our experiments. Importantly, this approach is computationally cheaper as it requires solving only two linear
systems per layer, compared to an eigen decomposition and four matrix-matrix multiplications for the exact calculation.
Alternatively, one can consider this approach as a special form of damping for Kronecker factored matrices.

Practical Gauss-Newton Optimisation for Deep Learning

B.2. Choosing the Step Size

The approximate block diagonal Gauss-Newton update can be signiﬁcantly different from the full Gauss-Newton update.
It is therefore important, in practice, to choose an appropriate step size, especially in cases where the curvature matrices
are estimated from mini-batches rather than the full dataset. The step size is calculated based on the work in (Martens &
Grosse, 2015), using the quadratic approximation ˆf (δ; ¯C) from (32), induced by the full Gauss-Newton matrix. Given the
initial step direction (cid:101)δ from (33) a line search is performed along that direction and the resulting optimal step size is used
for the ﬁnal update.

α∗ = arg min

ˆf (α(cid:101)δ; ¯C) = arg min

f (θ) + α(cid:101)δT∇f +

α2(cid:101)δT ¯C(cid:101)δ

α

α

1
2

This can be readily solved as

δ∗ = α∗(cid:101)δ = −

(cid:101)δT∇f
(cid:101)δT ¯C(cid:101)δ

(cid:101)δ

where

(cid:101)δT ¯C(cid:101)δ = (cid:101)δT ¯G(cid:101)δ + (τ + η)(cid:101)δT(cid:101)δ

B.3. Adaptive Damping

B.3.1. τ

ρ =

f (θ + δ∗) − f (θ)
ˆf (δ∗; ¯C) − ˆf (0; ¯C)

The term (cid:101)δT ¯Gδ can be calculated efﬁciently (without explicitly evaluating ¯G) using the R-operator (Pearlmutter, 1994).
The ﬁnal update of the approximate GN method is δ∗. Notably, the damping parameter γ affects the resulting update
direction (cid:101)δ, whilst τ affects only the step size along that direction.

In order to be able to correctly adapt τ to the current curvature of the objective we use a Levenberg-Marquardt heuristic
based on the reduction ratio ρ deﬁned as

This quantity measures how well the quadratic approximation matches the true function. When ρ < 1 it means that the
true function f has a lower value at θ + δ∗ (and thus the quadratic underestimates the curvature), while in the other case
the quadratic overestimates the curvature. The Levenberg-Marquardt method introduces the parameter ωτ < 1. When
ρ > 0.75 the τ parameter is multiplied by ωτ , when ρ < 0.75 it is divided by ωτ . In order for this to not introduce a
signiﬁcant computational overhead (as it requires an additional evaluation of the function – f (θ + δ∗)) we adapt τ only
every Tτ iterations. For all experiments we used ωτ = 0.95Tτ and Tτ = 5.

B.3.2. γ
The role of γ is to regularise the approximation of the quadratic function ˆf (δ, (cid:101)C) induced by the approximate Gauss-
Newton to that induced by the full Gauss-Newton ˆf (δ, ¯C). This is in contrast to τ , which regularises the quality of the
latter approximation to the true function. γ can be related to how well the approximate curvature matrix (cid:101)C reﬂects the full
Gauss-Newton matrix. The main reason for having two parameters is because we have two levels of approximations, and
each parameter independently affects each one of them:

f (θ + δ)

τ
≈ ˆf (δ; ¯C)

γ

≈ ˆf (δ; (cid:101)C)

The parameter γ is updated greedily. Every Tγ iterations the algorithm computes the update δ∗ for each of {ωγγ, γ, ω−1
γ γ, }
and some scaling factor ωγ < 1. From these three values the one that minimises ˆf (δ; ¯C) is selected. Similar to the previous
section, we use ωγ = 0.95Tγ and Tγ = 20 across all experiments.

(39)

(40)

(41)

(42)

(43)

Practical Gauss-Newton Optimisation for Deep Learning

B.4. Parameter Averaging

Compared with stochastic ﬁrst-order methods (for example stochastic gradient descent), stochastic second-order methods
do not exhibit any implicit averaging. To address this, Martens & Grosse (2015) introduce a separate value (cid:98)θt which tracks
the moving average of the parameter values θt used for training:

Importantly, (cid:98)θt has no effect or overhead on training as it is not used for the updates on θt. The extra parameter βt is chosen
such that in the initial stage when t is small, (cid:98)θt is the exact average of the ﬁrst t parameter values of θ:

(cid:98)θt = βt (cid:98)θt−1 + (1 − βt)θt

βt = min(0.95, 1 − 1/t)

Another factor playing an important role in stochastic second-order methods is the mini-batch size m. In Martens & Grosse
(2015), the authors concluded that because of the high signal to noise ratio that arises close to the minimum, in practice
one should use increasingly larger batch sizes for KFAC as the optimisation proceeds. However, our work does not focus
on this aspect and all of the experiments are conducted using a ﬁxed batch size of 1000.

For a general probability distribution pθ(x) parametrised by θ, the Fisher matrix can be expressed in two equivalent forms
(Martens, 2014):

C. The Fisher Matrix and KFAC

C.1. The Fisher Matrix

¯F = E

(cid:104)

∇θ log pθ(x)∇θ log pθ(x)T(cid:105)

pθ(x)

= −E [∇∇ log pθ(x)]pθ(x)

By construction the Fisher matrix is positive semi-deﬁnite. Using the Fisher matrix in place of the Hessian to form the
parameter update ¯F −1g is known as Natural Gradient (Amari, 1998).

In the neural network setting, the model speciﬁes a conditional distribution pθ(y|x), and the Fisher matrix is given by

¯F = E

= E

(cid:104)

(cid:104)

∇θ log pθ(x, y)∇θ log pθ(x, y)T(cid:105)
∇θ log pθ(y|x)∇θ log pθ(y|x)T(cid:105)

pθ(x,y)

pθ(x,y)

Using the chain rule ∇θ log pθ(y|x) = J hL

∇hL log pθ(y|x) and deﬁning

T

θ

FL ≡ ∇hL log pθ(y|x)∇hL log pθ(y|x)T

the Fisher can be calculated as:

¯F = E

J hL
θ

∇hL log pθ(y|x)∇hL log pθ(y|x)TJ hL
θ

(cid:104)

(cid:104)

T

T

= E

J hL
θ

FLJ hL
θ

(cid:105)

pθ(x,y)

(cid:105)

pθ(x,y)

C.2. Equivalence between the Gauss-Newton and Fisher Matrices

The expected Gauss-Newton matrix is given by

(cid:104)

¯G = E

T

J hL
θ

HLJ hL

θ

(cid:105)

(cid:104)

= E

J hL
θ

TE [HL]p(y|x) J hL

θ

(cid:105)

p(x,y)

p(x)

The equation is reminiscent of (12) and in Appendix C.2 we discuss the conditions under which the Fisher and the Gauss-
Newton matrix are indeed equivalent.

(44)

(45)

(46)

(47)

(48)

(49)

(50)

Practical Gauss-Newton Optimisation for Deep Learning

Using that E [FL] = E [HL] which follows from (46) and the fact that the Jacobian J hL
matrix can be expressed as:

θ

is independent of y, the Fisher

¯F = E

(cid:104)

T

J hL
θ

FLJ hL
θ

(cid:105)

(cid:104)

= E

J hL
θ

TE [FL]pθ(y|x) J hL

θ

(cid:105)

p(x)

= E

(cid:104)
J hL
θ

TE [HL]pθ(y|x) J hL

θ

(cid:105)

p(x)

pθ(x,y)

Hence the Fisher and Gauss-Newton matrices matrices are equivalent when E [HL]p(y|x) = E [HL]pθ(y|x). Since the
model distribution pθ(y|x) and the true data distribution p(y|x) are not equal, a sufﬁcient condition for the expectations to
be equal is HL being independent of y. Although this might appear restrictive, if hL parametrises the natural parameters
of an exponential family distribution this independence holds (Martens, 2014). To show this, consider

log p(y|x, θ) = log h(y) + T (y)Tη(x, θ) − log Z(x, θ) = log h(y) + T (y)ThL − log Z(hL)

where h is the base measure, T is the sufﬁcient statistic, Z is the partition function and η are the natural parameters. Taking
the gradient of the log-likelihood with respect to hL

Assuming that the objective is the negative log-likelihood as in Section 2.1 and differentiating again

∇hL log p(y|x, θ) = T (y) − ∇hL log Z(hL)

HL = ∇∇hL log Z(hL)

which is indeed independent of y. This demonstrates that in many practical supervised learning problems in Machine
Learning, the Gauss-Newton and Natural Gradient methods are equivalent.
The parameter update for these approaches is then given by computing ¯G−1g or ¯F −1g, where g is the gradient of the
objective with respect to all parameters. However, the size of the linear systems is prohibitively large in the case of neural
networks, thus it is computationally infeasible to solve them exactly. As shown in (Schraudolph, 2002), matrix-vector
products with ¯G can be computed efﬁciently using the R-operator (Pearlmutter, 1994). The method does not need to
compute ¯G explicitly, at the cost of approximately twice the computation time of a standard backward pass. This makes
it suitable for iterative methods such as conjugate gradient for approximately solving the linear system. The resulting
method is called ‘Hessian-free’, with promising results in deep feedforward and recurrent neural networks (Martens, 2010;
Martens & Sutskever, 2011). Nevertheless, the convergence of the conjugate gradient step may require many iterations by
itself, which can signiﬁcantly increase the computation overhead compared to standard methods. As a result, this approach
can have worse performance per clock time compared to a well-tuned ﬁrst-order method (Sutskever et al., 2013). This
motivates the usage of approximate Gauss-Newton methods instead.

C.3. The Fisher Approximation to E [Gλ] and KFAC

The key idea in this approach is to use the fact that the Fisher matrix is an expectation of the outer product of gradients and
that it is equal to the Gauss-Newton matrix (Section C.2). This is independent of whether the Gauss-Newton matrix is with
respect to Wλ or hλ, so we can write the pre-activation Gauss-Newton as

E [Gλ]p(x,y) = E

T

J hL
hλ

HLJ hL
hλ

(cid:105)

p(x)

= E

J hL
hλ

T E [FL]pθ(y|x) J hL
(cid:105)

hλ

T

(cid:105)

p(x)

= E

J hL
hλ

FLJ hL
hλ

pθ(x,y)

(cid:104)

(cid:104)

(cid:104)

(cid:104)

(cid:104)

T

= E

= E

∇hL log pθ(y|x)∇hL log pθ(y|x)TJ hL
hλ

J hL
hλ
∇hλ log pθ(y|x)∇hλ log pθ(y|x)T(cid:105)

pθ(x,y)

(cid:105)

pθ(x,y)

where the ﬁrst equality follows from (54) and the second one from (51) in the supplement.

(51)

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

Practical Gauss-Newton Optimisation for Deep Learning

We stress here that the resulting expectation is over the model distribution pθ(x, y) and not the data distribution p(x, y). In
order to approximate (59) the method proceeds by taking Monte Carlo samples of the gradients from the model conditional
distribution pθ(y|x).

The KFAC approximation presented in (Martens & Grosse, 2015) is analogous to the above approach, but it is derived
in a different way. The authors directly focus on the parameter Fisher matrix. Using the fact that J hλ
λ−1 ⊗ I and
Wλ
J hλ
Wλ

v = aλ−1 ⊗ v, the blocks of the Fisher matrix become:

= aT

T

(cid:2) ¯F (cid:3)

λ,β = E

(cid:104)

(cid:104)

∇Wλ log pθ(y|x)∇Wβ log pθ(y|x)T(cid:105)
(aλ−1 ⊗ ∇hλ log pθ(y|x)) (cid:0)aβ−1 ⊗ ∇hβ log pθ(y|x)(cid:1)T(cid:105)
∇hλ log pθ(y|x)∇hβ log pθ(y|x)T(cid:17)(cid:105)
(cid:104)(cid:16)

aλ−1aT

pθ(x,y)

⊗

(cid:17)

(cid:16)

β−1

= E

= E

pθ(x,y)

pθ(x,y)

This equation is equivalent to our result in (16) 13. In (Martens & Grosse, 2015) the authors similarly approximate the
expectation of the Kronecker products by the product of the individual expectations, which makes the second term equal
to the pre-activation Gauss-Newton as in (59).

C.4. Differences between KFAC and KFRA

It is useful to understand the computational complexity of both KFAC and KFRA and the implications of the approxi-
mations. In order to not make any additional assumptions about the underlying hardware or mode (serial or parallel) of
execution, we denote with Omm(m, n, p) the complexity of a matrix matrix multiplication of an m × n and n × p matrices
and with Oel(m, n) the complexity of an elementwise multiplication of two matrices of size m × n.

KFRA We need to backpropagate the matrix E [Gλ] of size Dλ × Dλ, where Dλ is the dimensionality of the layer.
For each layer, this requires two matrix-matrix multiplications with Wλ and single element wise multiplication (this
is due to Aλ being diagonal, which allows for such a simpliﬁcation). The overall complexity of the procedure is
2Omm(Dλ, Dλ, Dλ−1) + Oel(Dλ−1, Dλ−1).

KFAC We need to draw samples from pθ(y|x) for each datapoint x and backprogate the corresponding gradients through
the network (this is in addition to the gradients of the objective function). This requires backpropagating a matrix of
size Dλ−1 × N S, where S denotes the number of samples taken per datapoint. Per layer, the method requires also two
matrix-matrix multiplications (one with Wλ and the outer product of C s
λ) and a single element wise multiplication.
The overall complexity of the procedure is Omm(N S, Dλ, Dλ−1) + Oel(N S, Dλ−1) + Omm(Dλ−1, N S, Dλ−1).

There are several observations which we deem important. Firstly, if N = 1, KFRA is no longer an approximate method,
but computes the exact Gλ matrix. Secondly, if S = 1 and Dλ ∼ N then the two methods have similar computational
complexity. If we assume that the complexity scales linearly in S, in the extreme case of S = N and Dλ ∼ N , it is
possible to execute KFRA independently for each datapoint providing the exact value Gλ for the same computational cost,
while KFAC would nevertheless still be an approximation.

D. The Rank of the Monte Carlo Gauss-Newton

Using the deﬁnition of the sample Gauss-Newton matrix in (12) we can infer that its rank is bounded by the rank of HL:

G ≡ J hL

θ

T

HLJ hL

θ ⇒ rank(G) ≤ rank(HL)

This does not provide any bound on the rank of the “true” Gauss-Newton matrix, which is an expectation of the above.
However, for any practical algorithm which approximates the expectations via N Monte Carlo samples as:

¯G = E [G] ≈

1
N

(cid:88)

Gn

n

13Under the condition that the Fisher and Gauss-Newton matrices are equivalent, see Section C.2

(60)

(61)

(62)

(63)

(64)

Practical Gauss-Newton Optimisation for Deep Learning

(cid:80)

it provides a bound on the rank of the resulting matrix. Using the sub-additive property of the rank operator, it follows
that rank( 1
n Gn) ≤ rank(HL)N . Similarly, the approximate Fisher matrix computed in KFAC will have a rank
N
bounded by N S, where S is the number of samples taken per data point (usually one). This provides an explanation for
the results in Section 5.2 for binary classiﬁcation, since the last layer output in this problem is a scalar, thus its rank is
1. Hence, both the Gauss-Newton and the Fisher for a mini-batch have a rank bounded by the mini-batch size N . This
leads to the conclusion that in such a situation the curvature information provided from a Monte-Carlo estimate is not
sufﬁcient to render the approximate Gauss-Newton methods competitive against well-tuned ﬁrst order methods, although
we observe that in the initial stages they are still better. In some of the more recent works on KFAC the authors use
momentum terms in conjunction with the second-order updates or do not rescale by the full Gauss-Newton. This leaves
space for exploration and more in depth research on developing techniques that can robustly and consistently improve the
performance of second-order methods for models with a small number of outputs and small batch sizes.

E. Absence of Smooth Local Maxima for Piecewise Linear Transfer Functions

In order to show that the Hessian of a neural network with piecewise linear transfer functions can have no differentiable
strict local maxima, we ﬁrst establish that all of its diagonal blocks are positive semi-deﬁnite.
Lemma 1. Consider a neural network as deﬁned in (1). If the second derivative of all transfer functions fλ for 1 ≤ λ ≤ L
is zero where deﬁned, and if the Hessian of the error function w.r.t. the outputs of the network is positive semi-deﬁnite, then
all blocks

Hλ =

∂2E
∂vec (Wλ)∂vec (Wλ)

on the diagonal of the Hessian — corresponding to the weights Wλ of single layer — are positive semi-deﬁnite.

Proof. By the deﬁnition in (10) Dλ
∀λ Dλ = 0. Using the recursive equation (8) we can analyze the quadratic form vTHλv:

i,j = δi,jf (cid:48)(cid:48)(hλ

i,j). From the assumption of the lemma, f (cid:48)(cid:48) = 0 for all layers, hence

where we used the fact that by deﬁnition Bλ is a diagonal matrix, thus it is symmetric and Bλ = BT

λ. Deﬁning

Hλ = BλW T

λ+1Hλ+1Wλ+1Bλ + Dλ
= (Wλ+1Bλ)T Hλ+1 (Wλ+1Bλ)

˜v = Wλ+1Bλv

yields

hence

Hλ+1 ≥ 0 ⇒ Hλ ≥ 0

vTHλv = (Wλ+1Bλv)T Hλ+1 (Wλ+1Bλv)

= ˜vTHλ+1˜v

It follows by induction that if HL is positive semi-deﬁnite, all of the pre-activation Hessian matrices are positive semi-
deﬁnite as well.

Using the proof that the blocks Hλ can be written as a Kronecker product in (7), we can analyze the quadratic form of the
Hessian block diagonals:

(65)

(66)

(67)

(68)

(69)

(70)

vec (V )THλvec (V ) = vec (V )T (cid:104)(cid:16)
= vec (V )Tvec

(cid:105)

(cid:17)

aλaT
λ
(cid:16)
HλV aλaT
λ

⊗ Hλ
(cid:17)

vec (V )

(cid:16)

(cid:16)

= trace

V THλV aλaT
λ

= trace

λV THλV aλ
aT

= (V aλ)T Hλ (V aλ)

(cid:17)

(cid:17)

Hλ ≥ 0 ⇒ Hλ ≥ 0

Practical Gauss-Newton Optimisation for Deep Learning

The second line follows from the well known identity (A ⊗ B)vec (V ) = vec (cid:0)BV AT(cid:1). Similarly, the third line follows
from the fact that vec (A)Tvec (B) = trace (cid:0)ATB(cid:1). The fourth line uses the fact that trace (AB) = trace (BA) when the
product AB is a square matrix. This concludes the proof of the lemma.

• If we ﬁx the weights of all layers but one, the error function becomes locally convex, wherever the second derivatives

This lemma has two implications:

of all transfer functions in the network are deﬁned.

• The error function can have no differentiable strict local maxima.

We formalise the proof of the second proposition below:

Lemma 2. Under the same assumptions as in Lemma 1, the overall objective function E has no differentiable strict local
maxima with respect to the parameters θ of the network.

Proof. For a point to be a strict local maximum, all eigenvalues of the Hessian at this location would need to be simul-
taneously negative. However, as the trace of a matrix is equal to the sum of the eigenvalues it is sufﬁcient to prove that
trace (H) ≥ 0.

The trace of the full Hessian matrix is equal to the sum of the diagonal elements, so it is also equal to the sum of the
traces of the diagonal blocks. Under the assumptions in Lemma 1, we showed that all of the diagonal blocks are positive
semi-deﬁnite, hence their traces are non-negative. It immediately follows that:

trace (H) =

trace (Hλ) ≥ 0

L
(cid:88)

λ=1

(71)

Therefore, it is impossible for all eigenvalues of the Hessian to be simultaneously negative. As a corollary it follows that
all strict local maxima must lie in the non-differentiable boundary points of the nonlinear transfer functions.

Practical Gauss-Newton Optimisation for Deep Learning

F. Additional Figures

F.1. CPU Benchmarks

(a) CURVES

(b) FACES

(c) MNIST

Figure 4. Optimisation performance on the CPU. These timings are obtained with a previous implementation in Arrayﬁre, different to
the one used for the ﬁgures in the main text. For the second-order methods, the asterisk indicates the use of the approximate inversion
as described in Section B.1. The error function on all three datasets is binary cross-entropy.

Practical Gauss-Newton Optimisation for Deep Learning

F.2. Comparison of the Alignment of the Approximate Updates with the Gauss-Newton Update

(a) Block-diagonal Gauss-Newton

(b) Full Gauss-Newton

Figure 5. CURVES: Cosine similarity between the update vector per layer, given by the corresponding approximate method, (cid:101)δλ with
that for the block-diagonal GN (a) and the full vector with that from the full GN matrix (b). The optimal value is 1.0. The ∗ indicates
approximate inversion in (36). The x-axis is the number of iterations. Layers one to four are in the top; ﬁve to eight in the bottom row.
The trajectory of parameters we follow is the one generated by KFRA∗.

(a) Block-diagonal Gauss-Newton

(b) Full Gauss-Newton

Figure 6. FACES: Cosine similarity between the update vector per layer, given by the corresponding approximate method, (cid:101)δλ with that
for the block-diagonal GN (a) and the full vector with that from the full GN matrix (b). The optimal value is 1.0. The ∗ indicates
approximate inversion in (36). The x-axis is the number of iterations. Layers one to four are in the top; ﬁve to eight in the bottom row.
The trajectory of parameters we follow is the one generated by KFRA∗.

To gain insight into the quality of the approximations that are made in the second-order methods under consideration, we
compare how well the KFAC and KFRA parameter updates (cid:101)δ are aligned with updates obtained from using the regularised
block diagonal GN and the full GN matrix. Additionally we check how using the approximate inversion of the Kronecker
factored curvature matrices discussed in Appendix B impacts the alignment.

In order to ﬁnd the updates for the full GN method we use conjugate gradients and the R-operator and solve the linear
system ¯Gδ = ∇θf as in (Martens, 2010). For the block diagonal GN method we use the same strategy, however the
method is applied independently for each separate layer of the network, see Appendix B.

We compared the different approaches for batch sizes of 250, 500 and 1000. However, the results did not differ signiﬁcantly.
We therefore show results only for a batch size of 1000. In Figures 5 to 7, subﬁgure 6a plots the cosine similarity between
the update vector (cid:101)δλ for a speciﬁc layer, given by the corresponding approximate method, and the update vector when
using the block diagonal GN matrix on CURVES, FACES and MNIST. Throughout the optimisation, compared to KFAC,
the KFRA update has better alignment with the exact GN update. Subﬁgure 6b shows the same similarity for the whole
update vector (cid:101)δ, however in comparison with the update vector given by the full GN update. Additionally, we also show the
similarity between the update vector of the block diagonal GN and the full GN approach in those plots. There is a decay
in the alignment between the block-diagonal and full GN updates towards the end of training on FACES, however this is
most likely just due to the conjugate gradients being badly conditioned and is not observed on the other two datasets.

Practical Gauss-Newton Optimisation for Deep Learning

(a) Block-diagonal Gauss-Newton

(b) Full Gauss-Newton

Figure 7. MNIST: Cosine similarity between the update vector per layer, given by the corresponding approximate method, (cid:101)δλ with that
for the block-diagonal GN (a) and the full vector with that from the full GN matrix (b). The optimal value is 1.0. The ∗ indicates
approximate inversion in (36). The x-axis is the number of iterations. Layers one to four are in the top; ﬁve to eight in the bottom row.
The trajectory of parameters we follow is the one generated by KFRA∗.

After observing that KFRA generally outperforms KFAC, it is not surprising to see that its updates are beter aligned with
both the block diagonal and the full GN matrix.

Considering that (for exponential family models) both methods differ only in how they approximate the expected GN
matrix, gaining a better understanding of the impact of the separate approximations on the optimisation performance could
lead to an improved algorithm.

G. Algorithm for a Single Backward Pass

Practical Gauss-Newton Optimisation for Deep Learning

Algorithm 1 Algorithm for KFRA parameter updates excluding heuristic updates for η and γ

Input: minibatch X, weight matrices W1:L, transfer functions f1:L, true outputs Y , parameters η and γ
- Forward Pass -
a0 = X
for λ = 1 to L do
hλ = Wλaλ−1
aλ = fλ(hλ)

end for
- Derivative and Hessian of the objective -
(cid:12)
dL = ∂E
(cid:12)
(cid:12)hL
∂hL
(cid:12)
(cid:102)GLλ = E [HL]
(cid:12)
(cid:12)hL
- Backward pass -
for λ = L to 1 do

λ−1 + ηWλ

- Update for Wλ -
gλ = 1
N dλaT
(cid:101)Qλ = 1
N aλ−1aT
(cid:114)
T r( (cid:101)Qλ)∗dim((cid:101)G λ)
T r((cid:101)G λ)∗dim( (cid:101)Qλ)
γ + η

ω =

λ−1

√

k =
(cid:101)δλ = ( (cid:101)Qλ + ωk)−1gλ((cid:101)Gλ + ω−1k)−1
if λ > 1 then

- Propagate gradient and approximate pre-activation Gauss-Newton -
Aλ−1 = f (cid:48)(hλ−1)
dλ−1 = W T
(cid:101)Gλ−1 = (W T

λ dλ (cid:12) Aλ−1
λ (cid:101)GλWλ) (cid:12) (cid:0) 1

N Aλ−1AT

λ−1

(cid:1)

(using the R-op from (Pearlmutter, 1994))

end if
end for
v = J hL
θ (cid:101)δ
(cid:101)δT ¯G(cid:101)δ = vTHLv
(cid:101)δT ¯C(cid:101)δ = (cid:101)δT ¯G(cid:101)δ + (τ + η)||(cid:101)δ||2
2
α∗ = − (cid:101)δT∇f
(cid:101)δT ¯C(cid:101)δ
δ∗ = α∗(cid:101)δ
for λ = 1 to L do

Wλ = Wλ + δ∗λ

end for

