On the Sampling Problem for Kernel Quadrature

Franc¸ois-Xavier Briol 1 2 Chris J. Oates 3 4 Jon Cockayne 1 Wilson Ye Chen 5 Mark Girolami 2 4

Abstract

The standard Kernel Quadrature method for nu-
merical integration with random point sets (also
called Bayesian Monte Carlo) is known to con-
verge in root mean square error at a rate de-
termined by the ratio s/d, where s and d en-
code the smoothness and dimension of the in-
tegrand. However, an empirical investigation
reveals that the rate constant C is highly sen-
sitive to the distribution of the random points.
In contrast to standard Monte Carlo integration,
for which optimal importance sampling is well-
understood, the sampling distribution that min-
imises C for Kernel Quadrature does not admit
a closed form. This paper argues that the practi-
cal choice of sampling distribution is an impor-
tant open problem. One solution is considered; a
novel automatic approach based on adaptive tem-
pering and sequential Monte Carlo. Empirical re-
sults demonstrate a dramatic reduction in integra-
tion error of up to 4 orders of magnitude can be
achieved with the proposed method.

1. INTRODUCTION

Consider approximation of the Lebesgue integral

Π(f ) =

f dΠ

(1)

(cid:90)

X

where Π is a Borel measure deﬁned over X ⊆ Rd and
f is Borel measurable. Deﬁne P(f ) to be the set of
Borel measures Π(cid:48) such that f ∈ L2(Π(cid:48)), meaning that
X f 2dΠ(cid:48) < ∞, and assume Π ∈ P(f ). In
(cid:107)f (cid:107)2
situations where Π(f ) does not admit a closed-form, Monte

L2(Π(cid:48)) = (cid:82)

1University of Warwick, Department of Statistics. 2Imperial
College London, Department of Mathematics. 3Newcastle Uni-
versity, School of Mathematics and Statistics 4The Alan Tur-
ing Institute for Data Science 5University of Technology Sydney,
School of Mathematical and Physical Sciences. Correspondence
to: Franc¸ois-Xavier Briol <f-x.briol@warwick.ac.uk>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Carlo (MC) methods can be used to estimate the numerical
value of Eqn. 1. A classical research problem in computa-
tional statistics is to reduce the MC estimation error in this
context, where the integral can, for example, represent an
expectation or marginalisation over a random variable of
interest.

The default MC estimator comprises of

ˆΠMC(f ) =

f (xj),

1
n

n
(cid:88)

j=1

where xj are sampled identically and independently (i.i.d.)
from Π. Then we have a root mean square error (RMSE)
bound

(cid:113)

E[ ˆΠMC(f ) − Π(f )]2 ≤

CMC(f ; Π)
√
n

,

where CMC(f ; Π) = Std(f ; Π) and the expectation is with
respect to the joint distribution of the {xj}n
j=1. For set-
tings where the Lebesgue density of Π is only known up to
normalising constant, Markov chain Monte Carlo (MCMC)
methods can be used; the rate-constant CMC(f ; Π) is then
related to the asymptotic variance of f under the Markov
chain sample path.

Considerations of computational cost place emphasis on
methods to reduce the rate constant CMC(f ; Π). For the
MC estimator, this rate constant can be made smaller via
importance sampling (IS): f (cid:55)→ f · dΠ/dΠ(cid:48) where an
optimal choice Π(cid:48) ∈ P(f · dΠ/dΠ(cid:48)),
that minimises
Std(f · dΠ/dΠ(cid:48); Π(cid:48)), is available in explicit closed-form
(see Robert and Casella, 2013, Thm. 3.3.4). However, the
RMSE remains asymptotically gated at O(n−1/2).

The default Kernel Quadrature (KQ) estimate comprises of

ˆΠ(f ) =

wjf (xj),

(2)

n
(cid:88)

j=1

where the xj ∼ Π(cid:48) are independent (or arise from
a Markov chain) and supp(Π) ⊆ supp(Π(cid:48)).
In con-
trast to MC, the weights {wj}n
j=1 in KQ are in general
non-uniform, real-valued and depend on {xj}n
j=1. The
KQ nomenclature derives from the (symmetric, positive-
deﬁnite) kernel k : X × X → R that is used to con-
struct an interpolant ˆf (x) = (cid:80)n
j=1 βjk(x, xj) such that

On the Sampling Problem for Kernel Quadrature

ˆf (xj) = f (xj) for j = 1, . . . , n. The weights wj in Eqn.
2 are implicitly deﬁned via the equation ˆΠ(f ) = (cid:82)
ˆf dΠ.
The KQ estimator is identical to the posterior mean in
Bayesian Monte Carlo (O’Hagan, 1991; Rasmussen and
Ghahramani, 2002), and its relationship with classical nu-
merical quadrature rules has been studied (Diaconis, 1988;
S¨arkk¨a et al., 2015).

X

Under regularity conditions, Briol et al. (2015b) estab-
lished the following RMSE bound for KQ:

(cid:113)

E[ ˆΠ(f ) − Π(f )]2 ≤

C(f ; Π(cid:48))
ns/d−(cid:15)

,

(s > d/2)

where both the integrand f and each argument of the kernel
k admit continuous mixed weak derivatives of order s and
(cid:15) > 0 can be arbitrarily small. An information-theoretic
lower bound on the RMSE is O(n−s/d−1/2) (Bakhvalov,
1959). The faster convergence of the RMSE, relative to
MC, can lead to improved precision in applications. Akin
to IS, the samples {xj}n
j=1 need not be draws from Π in or-
der for KQ to provide consistent estimation (since Π is en-
coded in the weights wj). Importantly, KQ can be viewed
as post-processing of MC samples; the kernel k can be
reverse-engineered (e.g. via cross-validation) and does not
need to be speciﬁed up-front.

One notable disadvantage of KQ methods is that little is
known about how the rate constant C(f ; Π(cid:48)) depends on
the choice of sampling distribution Π(cid:48). In contrast to IS,
no general closed-form expression has been established for
an optimal distribution Π(cid:48) for KQ (the technical meaning
of ‘optimal’ is deﬁned below). Moreover, limited practical
guidance is available on the selection of the sampling dis-
tribution (an exception is Bach, 2015, as explained in Sec.
2.4) and in applications it is usual to take Π(cid:48) = Π.
This choice is convenient but leads to estimators that are
not efﬁcient, as we demonstrate in dramatic empirical ex-
amples in Sec. 2.3.

The main contributions of this paper are twofold. First, we
formalise the problem of optimal sampling for KQ as an
important and open challenge in computational statistics.
To be precise, our target is an optimal sampling distribution
for KQ, deﬁned as

Π∗ ∈ arg min

sup
f ∈F

Π(cid:48)

(cid:113)

E[ ˆΠ(f ) − Π(f )]2.

(3)

for some functional class F to be speciﬁed. In general a
(possibly non-unique) optimal Π∗ will depend on F and,
unlike for IS, also on the kernel k and the number of sam-
ples n.

Second, we propose a novel and automatic method for se-
lection of Π(cid:48) that is rooted in approximation of the unavail-
able Π∗. In brief, our method considers candidate sampling

distributions of the form Π(cid:48) = Π1−t
0 Πt for t ∈ [0, 1] and
Π0 a reference distribution on X . The exponent t is chosen
such that Π(cid:48) minimises an empirical upper bound on the
RMSE. The overall approach is facilitated with an efﬁcient
sequential MC (SMC) sampler and called SMC-KQ. In par-
ticular, the approach (i) provides practical guidance for se-
lection of Π(cid:48) for KQ, (ii) offers robustness to kernel mis-
speciﬁcation, and (iii) extends recent work on computing
posterior expectations with kernels obtained using Stein’s
method (Oates et al., 2017).

The paper proceeds as follows: Empirical results in Sec.
2 reveal that the RMSE for KQ is highly sensitive to the
choice of Π(cid:48). The proposed approach to selection of Π(cid:48) is
contained in Sec. 3. Numerical experiments, presented in
Sec. 4, demonstrate that dramatic reductions in integration
error (up to 4 orders of magnitude) can be achieved with
SMC-KQ. Lastly, a discussion is provided in Sec. 5.

2. BACKGROUND

This section presents an overview of KQ (Sec. 2.1 and 2.2),
empirical (Secs. 2.3) and theoretical (Sec. 2.4) results on
the choice of sampling distribution, and discusses kernel
learning for KQ (Sec. 2.5).

2.1. Overview of Kernel Quadrature

We now proceed to describe KQ: Recall the approximation
ˆf to f ; an explicit form for the coefﬁcients βj is given as
β = K−1f , where Ki,j = k(xi, xj) and fj = f (xj). It is
assumed that K−1 exists almost surely; for non-degenerate
kernels, this corresponds to Π having no atoms. From the
above deﬁnition of KQ,

ˆΠ(f ) =

βj

k(x, xj)Π(dx).

n
(cid:88)

j=1

(cid:90)

X

Deﬁning zj = (cid:82)
X k(·, xj)dΠ leads to the estimate in Eqn.
2 with weights w = K−1z. Pairs (Π, k) for which the
zj have closed form are reported in Table 1 of Briol et al.
(2015b). Computation of these weights incurs a compu-
tational cost of at most O(n3) and can be justiﬁed when
either (i) evaluation of f forms the computational bottle-
neck, or (ii) the gain in estimator precision (as a function in
n) dominates this cost (i.e. whenever s/d > 3 + 1/2).

Notable contributions on KQ include Diaconis (1988);
O’Hagan (1991); Rasmussen and Ghahramani (2002) who
introduced the method and Huszar and Duvenaud (2012);
Osborne et al. (2012a;b); Gunter et al. (2014); Bach (2015);
Briol et al. (2015a;b); S¨arkk¨a et al. (2015); Kanagawa
et al. (2016); Liu and Lee (2017) who provided conse-
quent methodological extensions. KQ has been applied
to a wide range of problems including probabilistic ODE

On the Sampling Problem for Kernel Quadrature

solvers (Kersting and Hennig, 2016), reinforcement learn-
ing (Paul et al., 2016), ﬁltering (Pr¨uher and ˇSimandl, 2015)
and design of experiments (Ma et al., 2014).

Several characterisations of the KQ estimator are known
and detailed below. Let H denote the Hilbert space charac-
terised by the reproducing kernel k, and denote its norm as
(cid:107) · (cid:107)H (Berlinet and Thomas-Agnan, 2011). Then we have
the following: (a) The function ˆf is the minimiser of (cid:107)g(cid:107)H
over g ∈ H subject to g(xj) = f (xj) for all j = 1, . . . , n.
(b) The function ˆf is the posterior mean for f under the
Gaussian process prior f ∼ GP(0, k) conditioned on data
f and ˆΠ(f ) is the mean of the implied posterior marginal
over Π[f ]. (c) The weights w are characterised as the min-
imiser over γ ∈ Rn of

en(γ; {xj}n

j=1) = sup

(cid:107)f (cid:107)H=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

j=1

(cid:12)
(cid:12)
(cid:12)
γjf (xj) − Π(f )
(cid:12)
(cid:12)

,

the maximal error in the unit ball of H. These character-
isations connect KQ to (a) non-parametric regression, (b)
probabilistic integration and (c) quasi-Monte Carlo (QMC)
methods (Dick and Pillichshammer, 2010). The scattered
data approximation literature (Sommariva and Vianello,
2006) and the numerical analysis literature (where KQ is
known as the ‘empirical interpolation method’; Eftang and
Stamm, 2012; Kristoffersen, 2013) can also be connected
to KQ. However, our search of all of these literatures did
not yield guidance on the optimal selection of the sampling
distribution Π(cid:48) (with the exception of Bach (2015) reported
in Sec. 2.4).

2.2. Over-Reliance on the Kernel

In Osborne et al. (2012a); Huszar and Duvenaud (2012);
Gunter et al. (2014); Briol et al. (2015a), the selection of xn
was approached as a greedy optimisation problem, wherein
the maximal integration error en(w; {xj}n
j=1) was min-
imised, given the location of the previous {xj}n−1
j=1 . This
approach has demonstrated considerable success in appli-
cations. However, the error criterion en is strongly de-
pendant on the choice of kernel k and the sequential op-
timisation approach is vulnerable to kernel misspeciﬁca-
tion. In particular, if the intrinsic length scale of k is “too
small” then the {xj}n
j=1 all cluster around the mode of
Π, leading to poor integral estimation (see Fig. 5 in the
Appendix). Related work on sub-sample selection, such
as leverage scores (Bach, 2013), can also be non-robust to
mis-speciﬁed kernels. The partial solution of online kernel
learning requires a sufﬁcient number n of data and is not
always practicable in small-n regimes that motivate KQ.

This paper considers sampling methods as a robust alter-
native to optimisation methods. Although our method also
makes use of k to select Π(cid:48), it reverts to Π(cid:48) = Π in the

limit as the length scale of k is made small. In this sense,
sampling offers more robustness to kernel mis-speciﬁcation
than optimisation methods, at the expense of a possible
(non-asymptotic) decrease in precision in the case of a
well-speciﬁed kernel. This line of research is thus com-
plementary to existing work. However, we emphasise that
robustness is an important consideration for general appli-
cations of KQ in which kernel speciﬁcation may be a non-
trivial task.

2.3. Sensitivity to the Sampling Distribution

To date, we are not aware of a clear demonstration of the
acute dependence of the performance of the KQ estimator
on the choice of distribution Π(cid:48). It is therefore important to
illustrate this phenomenon in order to build intuition.
Consider the toy problem with state space X = R, target
distribution Π = N(0, 1), a single test function f (x) =
1 + sin(2πx) and kernel k(x, x(cid:48)) = exp(−(x − x(cid:48))2). For
this problem, consider a range of sampling distributions of
the form Π(cid:48) = N(0, σ2) for σ ∈ (0, ∞). Fig. 1 plots

ˆRn,σ =

( ˆΠn,m,σ(f ) − Π(f ))2,

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
M

M
(cid:88)

m=1

an empirical estimate for the RMSE where ˆΠn,m,σ(f ) is
the mth of M independent KQ estimates for Π(f ) based
on n samples drawn from the distribution Π(cid:48) with standard
deviation σ (M = 1000). In this case Π(f ) = 1 is avail-
able in closed-form. It is seen that the ‘obvious’ choice of
σ = 1, i.e. Π(cid:48) = Π, is sub-optimal. The intuition here
is that ‘extreme’ samples xi from the tails of Π are rather
informative for building the interpolant ˆf underlying KQ;
we should therefore over-sample these values via a heavier-
tailed Π(cid:48). The same intuition is used for column sampling
and to construct leverage scores (Mahoney, 2011; Drineas
et al., 2012).

2.4. Established Results

Here we recall the main convergence results to-date on KQ
and discuss how these relate to choices of sampling distri-
bution. To reduce the level of detail below, we make several
assumptions at the outset:

Assumption on the domain: The domain X will either be
Rd itself or a compact subset of Rd that satisﬁes an ‘interior
cone condition’, meaning that there exists an angle θ ∈
(0, π/2) and a radius r > 0 such that for every x ∈ X
there exists (cid:107)ξ(cid:107)2 = 1 such that the cone {x + λy : y ∈
Rd, (cid:107)y(cid:107)2 = 1, yT ξ ≥ cos θ, λ ∈ [0, r]} is contained in
X (see Wendland, 2004, for background).

Assumption on the kernel: Consider the integral opera-
tor Σ : L2(Π) → L2(Π), with (Σf )(x) deﬁned as the

On the Sampling Problem for Kernel Quadrature

Figure 1. The performance of kernel quadrature is sensitive to
the choice of sampling distribution. Here the test function was
f (x) = 1 + sin(2πx), the target measure was N(0, 1), while n
samples were generated from N(0, σ2). The kernel k(x, x(cid:48)) =
exp(−(x − x(cid:48))2) was used. Notice that the values of σ that min-
imise the root mean square error (RMSE) are uniformly greater
than σ = 1 (dashed line) and depend on the number n of samples
in general.

Bochner integral (cid:82)
X f (x(cid:48))k(x, x(cid:48))Π(dx(cid:48)). Assume that
(cid:82)
X k(x, x)Π(dx) < ∞, so that Σ is self-adjoint, positive
semi-deﬁnite and trace-class (Simon, 1979). Then, from an
extension of Mercer’s theorem (K¨onig, 1986) we have a de-
composition k(x, x(cid:48)) = (cid:80)∞
m=1 µmem(x)em(x(cid:48)), where
µm and em(x) are the eigenvalues and eigenfunctions of
Σ. Further assume that H is dense in L2(Π).

The ﬁrst result is adapted and extended from Thm. 1 in
Oates et al. (2016).
Theorem 1. Assume that Π(cid:48) admits a density π(cid:48) deﬁned
on a compact domain X . Assume that π(cid:48) > c for some
c > 0. Let x1, . . . , xm be ﬁxed and deﬁne the Euclidean
ﬁll distance

hm = sup
x∈X

min
j=1,...,m

(cid:107)x − xj(cid:107)2.

Let xm+1, . . . , xn be independent draws from Π(cid:48). Assume
k gives rise to a Sobolev space Hs(Π). Then there exists
h0 > 0 such that, for hm < h0,

(cid:113)

E[ ˆΠ(f ) − Π(f )]2 ≤ C(f )n−s/d+(cid:15)

for all (cid:15) > 0. Here C(f ) = ck,Π(cid:48),(cid:15)(cid:107)f (cid:107)H for some constant
0 < ck,Π(cid:48),(cid:15) < ∞ independent of n and f .

All proofs are reserved for the Appendix. The main con-
tribution of Thm. 1 is to establish a convergence rate for
KQ when using importance sampling distributions. A sim-
ilar result appeared in Thm. 1 of Briol et al. (2015b) for
samples from Π (see the Appendix) and was extended to
MCMC samples in Oates et al. (2016). An extension to

Figure 2. The performance of kernel quadrature is sensitive to the
choice of kernel. Here the same set-up as Fig. 1 was used with
n = 75. The kernel k(x, x(cid:48)) = exp(−(x − x(cid:48))2/(cid:96)2) was used
for various choices of parameter (cid:96) ∈ (0, ∞). The root mean
square error (RMSE) is sensitive to choice of (cid:96) for all choices of
σ, suggesting that online kernel learning could be used to improve
over the default choice of (cid:96) = 1 and σ = 1 (dashed lines).

the case of a mis-speciﬁed kernel was considered in Kana-
gawa et al. (2016). However a limitation of this direction
of research is that it does not address the question of how
to select Π(cid:48).

The second result that we present is a consequence of the
recent work of Bach (2015), who considered a particular
choice of Π(cid:48) = ΠB, depending on a ﬁxed λ > 0, via the
density πB(x; λ) ∝ (cid:80)∞
µm
µm+λ e2
m(x). The following is
adapted from Prop. 1 in Bach (2015):
Theorem 2. Let x1, . . . , xn ∼ ΠB be independent and
λ > 0. For δ ∈ (0, 1) and n ≥ 5d(λ) log 16d(λ)
,
d(λ) = (cid:80)∞

µm
µm+λ , we have that

m=1

m=1

δ

| ˆΠ(f ) − Π(f )| ≤ 2λ1/2(cid:107)f (cid:107)H,

with probability greater than 1 − δ.

√

Some remarks are in order:
(i) Bach (2015, Prop. 3)
showed that, for ΠB, integration error scales at an opti-
mal rate in n up to logarithmic terms and, after n samples,
µn. (ii) The distribution ΠB is obtained from
is of size
minimising an upper bound on the integration error, rather
than the error itself. It is unclear to us how well ΠB ap-
proximates an optimal sampling distribution for KQ. (iii)
In general ΠB is hard to compute. For the speciﬁc case
X = [0, 1]d, H equal to Hs(Π) and Π uniform, the dis-
tribution ΠB is also uniform (and hence independent of n;
see Sec. 4.4 of Bach (2015)). However, even for the simple
example of Sec. 2.3, ΠB does not appear to have a closed
form (details in Appendix). An approximation scheme was
proposed in Sec. 4.2 of Bach (2015) but the error of this
scheme was not studied.

Optimal sampling for approximation in (cid:107) · (cid:107)L2(Π) with
weighted least squares (not in the kernel setting) was

On the Sampling Problem for Kernel Quadrature

considered in Hampton and Doostan (2015); Cohen and
Migliorati (2016).

2.5. Goals

Our ﬁrst goal was to formalise the sampling problem for
KQ; this is now completed. Our second goal was to de-
velop a novel automatic approach to selection of Π(cid:48), called
SMC-KQ; full details are provided in Sec. 3.

Also, observe that the integrand f will in general belong
to an inﬁnitude of Hilbert spaces, while for KQ a single
kernel k must be selected. This choice will affect the per-
formance of the KQ estimator; for example, in Fig. 2, the
problem of Sec. 2.3 was reconsidered based on a class of
kernels k(x, x(cid:48)) = exp(−(x − x(cid:48))2/(cid:96)2) parametrised by
(cid:96) ∈ (0, ∞). Results showed that, for all choices of σ pa-
rameter, the RMSE of KQ is sensitive to choice of (cid:96). In par-
ticular, the default choice of (cid:96) = 1 is not optimal. For this
reason, an extension that includes kernel learning, called
SMC-KQ-KL, is proposed in Sec. 3.

3. METHODS

In this section the SMC-KQ and SMC-KQ-KL methods are
presented. Our aim is to explain in detail the main compo-
nents (SMC, temp, crit) of Alg. 1. To this end, Secs. 3.1
and 3.2 set up our SMC sampler to target tempered distri-
butions, while Sec. 3.3 presents a heuristic for the choice
of temperature schedule. Sec. 3.4 extends the approach to
kernel learning and Sec. 3.5 proposes a novel criterion to
determine when a desired error tolerance is reached.

3.1. Thermodynamic Ansatz

To begin, consider f , k and n as ﬁxed. The following
ansatz is central to our proposed SMC-KQ method: An op-
timal distribution Π∗ (in the sense of Eqn. 3) can be well-
approximated by a distribution of the form

Πt = Π1−t

0 Πt,

t ∈ [0, 1]

(4)

for a speciﬁc (but unknown) ‘inverse temperature’ param-
eter t = t∗. Here Π0 is a reference distribution to be spec-
iﬁed and which should be chosen to be un-informative in
practice. It is assumed that all Πt exist (i.e. can be nor-
malised). The motivation for this ansatz stems from Sec.
2.3, where Π = N(0, 1) and Πt = N(0, σ2) can be cast
in this form with t = σ−1 and Π0 an (improper) uniform
distribution on R. In general, tempering generates a class
of distributions which over-represent extreme events rela-
tive to Π (i.e. have heavier tails). This property has the
potential to improve performance for KQ, as demonstrated
in Sec. 2.3.

The ansatz of Eqn. 4 reduces the non-parametric sampling
problem for KQ to the one-dimensional parametric prob-

lem of selecting a suitable t ∈ [0, 1]. The problem can
be further simpliﬁed by focusing on a discrete temperature
ladder {ti}T
i=0 such that t0 = 0, ti < ti+1 and tT = 1.
Discussion of the choice of ladder is deferred to Sec. 3.3.
This reduced problem, where we seek an optimal index
i∗ ∈ {0, . . . , T }, is still non-trivial as no closed-form ex-
pression is available for the RMSE at each candidate ti.
To overcome this impasse a novel approach to estimate the
RMSE is presented in Sec. 3.5.

3.2. Convex Ansatz (SMC)

The proposed SMC-KQ algorithm requires a second ansatz,
namely that the RMSE is convex in t and possesses a global
minimum in the range t ∈ (0, 1). This second ansatz (borne
out in numerical results in Fig. 1) motivates an algorithm
that begins at t0 = 0 and tracks the RMSE until an increase
is detected, say at ti; at which point the index i∗ = i − 1 is
taken for KQ.

To realise such an algorithm, this paper exploited SMC
methods (Chopin, 2002; Del Moral et al., 2006). Here, a
particle approximation {(wj, xj)}N
j=1 to Πt0 is ﬁrst ob-
tained where xj are independent draws from Π0, wj =
N −1 and N (cid:29) n. Then, at iteration i, the particle ap-
proximation to Πti−1 is re-weighted, re-sampled and sub-
ject to a Markov transition, to deliver a particle approxi-
mation {(w(cid:48)
j=1 to Πti. This ‘re-sample-move’ al-
gorithm, denoted SMC, is standard but, for completeness,
pseudo-code is provided as Alg. 2 in the Appendix.

j)}N

j, x(cid:48)

j}N

At iteration i, a subset of size n is drawn from the unique1
elements in {x(cid:48)
j=1, from the particle approximation to
Πti, and proposed for use in KQ. A criterion crit, deﬁned
in Sec. 3.5, is used to determine whether the resultant KQ
error has increased relative to Πti−1.
If this is the case,
then the distribution Πti−1 from the previous iteration is
taken for use in KQ. Otherwise the algorithm proceeds to
ti+1 and the process repeats. In the degenerate case where
the RMSE has a minimum at tT , the algorithm defaults to
standard KQ with Π(cid:48) = Π.

Both ansatz of the SMC-KQ algorithm are justiﬁed through
the strong empirical results presented in Sec. 4.

3.3. Choice of Temperature Schedule (temp)

The choice of temperature schedule {ti}T
i=0 inﬂuences sev-
eral aspects of SMC-KQ: (i) The SMC approximation to
Πti is governed by the “distance” (in some appropriate
metric) between Πti−1 and Πti.
(ii) The speed at which
the minimum t∗ can be reached is linear in the number of

1This ensures that kernel matrices have full rank. It does not
introduce bias into KQ, since in general Π(cid:48) need not equal Π.
However, to keep notation clear, we do not make this operation
explicit.

On the Sampling Problem for Kernel Quadrature

temperatures between 0 and t∗. (iii) The precision of KQ
depends on the approximation t∗ ≈ ti∗ . Factors (i,iii) mo-
tivate the use of a ﬁne schedule with T large, while (ii)
motivates a coarse schedule with T small.

For this work, a temperature schedule was used that is
well suited to both (i) and (ii), while a strict constraint
ti − ti−1 ≤ ∆ was imposed on the grid spacing to ac-
knowledge (iii). The speciﬁc schedule used in this work
was determined based on the conditional effective sample
size of the current particle population, as proposed in the
recent work of Zhou et al. (2016). Full details are presented
in Algs. 4 and 5 in the Appendix.

3.4. Kernel Learning

In Sec. 2.5 we demonstrated the beneﬁt of kernel learning
for KQ. From the Gaussian process characterisation of KQ
from Sec. 2.1, it follows that kernel parameters θ can be
estimated, conditional on a vector of function evaluations
f , via maximum marginal likelihood:

θ(cid:48) ← arg max

p(f |θ) = arg min

f (cid:62)K−1

θ f + log |Kθ|.

θ

θ

In SMC-KQ-KL, the function evaluations f are obtained
at the ﬁrst2 n (of N ) states {xj}n
j=1 and the parameters θ
are updated in each iteration of the SMC. This demands re-
peated function evaluation; this burden can be reduced with
less frequent parameter updates and caching of all previous
function evaluations. The experiments in Sec. 4 assessed
both SMC-KQ and SMC-KQ-KL in terms of precision per
total number of function evaluations, so that the additional
cost of kernel learning was taken into account.

3.5. Termination Criterion (crit)

The SMC-KQ-KL algorithm is designed to track the RMSE
as t is increased. However, the RMSE is not available in
closed form. In this section we derive a tight upper bound
on the RMSE that is used for the crit component in Alg.
1.

From the worst-case characterisation of KQ presented in
Sec. 2.1, we have an upper bound

| ˆΠ(f ) − Π(f )| ≤ en(w; {xj}n
term en(w; {xj}n

denoted henceforth as
The
en({xj}n
j=1), can be com-
puted in closed form (see the Appendix). This motivates

j=1) (since w depends on {xj}n

j=1),

j=1)(cid:107)f (cid:107)H.

(5)

2This is a notational convention and is without loss of gener-
ality. In this paper these states were a random sample (without
replacement) of size n, though stratiﬁed sampling among the N
states could be used. More sophisticated alternatives that also in-
volve the kernel k, such as leverage scores, were not considered,
since in general these (i) introduce a vulnerability to mis-speciﬁed
kernels and (ii) require manipulation of a N × N kernel matrix
(Patel et al., 2015).

Algorithm 1 SMC Algorithm for KQ

function SMC-KQ(f, Π, k, Π0, ρ, n, N )
input f (integrand)
input Π (target disn.)
input k (kernel)
input Π0 (reference disn.)
input ρ (re-sample threshold)
input n (num. func. evaluations)
input N (num. particles)
i ← 0; ti ← 0; Rmin ← ∞
x(cid:48)
w(cid:48)
R ← crit(Π, k, {x(cid:48)
j=1) (est’d error)
while test(R < Rmin) and ti < 1 do

j ∼ Π0 (initialise states ∀j ∈ 1 : N )
j ← N −1 (initialise weights ∀j ∈ 1 : N )
j}N

i ← i + 1; Rmin ← R
j=1 ← {(w(cid:48)
{(wj, xj)}N
ti ← temp({(wj, xj)}N
j)}N
{(w(cid:48)
(next particle approx.)
R ← crit(Π, k, {x(cid:48)

j, x(cid:48)

j}N

j)}N

j, x(cid:48)
j=1, ti−1, ρ) (next temp.)

j=1

j=1 ← SMC({(wj, xj)}N

j=1, ti, ti−1, ρ)

j=1) (est’d error)

X k(·, xj)dΠ (kernel mean eval. ∀j ∈ 1 : n)

end while
fj ← f (xj) (function eval. ∀j ∈ 1 : n)
zj ← (cid:82)
Kj,j(cid:48) ← k(xj, xj(cid:48)) (kernel eval. ∀j, j(cid:48) ∈ 1 : n)
ˆΠ(f ) ← z(cid:62)K−1f (eval. KQ estimator)
return ˆΠ(f )

the following upper bound on MSE:

E[ ˆΠ(f ) − Π(f )]2 ≤ E[en({xj}n

(cid:124)

j=1)2]
(cid:125)

(cid:107)f (cid:107)2
H
(cid:124) (cid:123)(cid:122) (cid:125)
(∗∗)

(cid:123)(cid:122)
(∗)

(6)

The term (∗) can be estimated with the bootstrap approxi-
mation

E[en({xj}n

j=1)2] =

M
(cid:88)

en({ ˜xm,j}n

j=1)2

m=1

M

=: R2

j=1.

where ˜xm,j are independent draws from {xj}N
In
SMC-KQ the term (∗∗) is an unknown constant and the
statistic R, an empirical proxy for the RMSE, is monitored
at each iteration. The algorithm terminates once an increase
in this statistic occurs. For SMC-KQ-KL the term (∗∗) is
non-constant as it depends on the kernel hyper-parameters;
then (∗∗) can in addition be estimated as (cid:107) ˆf (cid:107)2
H = w(cid:62)Kθw
and we monitor the product of R and (cid:107) ˆf (cid:107)H, with termina-
tion when an increase is observed (c.f. test, deﬁned in
the Appendix).

Full pseudo-code for SMC-KQ is provided as Alg. 1, while
SMC-KQ-KL is Alg. 9 in the Appendix. To summarise, we
have developed a novel procedure, SMC-KQ (and an exten-
sion SMC-KQ-KL), designed to approximate the optimal

On the Sampling Problem for Kernel Quadrature

KQ estimator based on the unavailable optimal distribution
in Eqn. 3 where F is the unit ball of H. Earlier empiri-
cal results in Sec. 2.3 suggest that SMC-KQ has potential
to provide a powerful and general algorithm for numeri-
cal integration. The additional computational cost of opti-
mising the sampling distribution does however have to be
counterbalanced with the potential gain in error, and so this
method will mainly be of practical interest for problems
with expensive integrands or complex target distributions.
The following section reports experiments designed to test
this claim.

4. RESULTS

Here we compared SMC-KQ (and SMC-KQ-KL) against
the corresponding default approaches KQ (and KQ-KL) that
are based on Π(cid:48) = Π. Sec. 4.1 below reports an assessment
in which the true value of integrals is known by design,
while in Sec. 4.2 the methods were deployed to solve a
parameter estimation problem involving differential equa-
tions.

4.1. Simulation Study

To continue our illustration from Sec. 2, we investigated
the performance of SMC-KQ and SMC-KQ-KL for inte-
gration of f (x) = 1 + sin(2πx) against the distribution
Π = N(0, 1). Here the reference distribution was taken to
be Π0 = N(0, 82). All experiments employed SMC with
N = 300 particles, random walk Metropolis transitions
(Alg. 3), the re-sample threshold ρ = 0.95 and a maxi-
mum grid size ∆ = 0.1. Dependence of the subsequent
results on the choice of Π0 was investigated in Fig. 10 in
the Appendix.

Fig. 3 (top) reports results for SMC-KQ against KQ, for
ﬁxed length-scale (cid:96) = 1. Corresponding results for
SMC-KQ-KL against KQ-KL are shown in the bottom plot.
It was observed that SMC-KQ (resp. SMC-KQ-KL) out-
performed KQ (resp. KQ-KL) in the sense that, on a per-
function-evaluation basis, the MSE achieved by the pro-
posed method was lower than for the standard method.
The largest reduction in MSE achieved was about 8 orders
of magnitude (correspondingly 4 orders of magnitude in
RMSE). A fair approximation to the σ = 2 method, which
is approximately optimal for n = 75 (c.f. results in Fig.
1), was observed. The termination criterion in Sec. 3.5 was
observed to be a good approximation to the optimal tem-
perature t∗ (Fig. 9 in Appendix). As an aside, we note that
the MSE was gated at 10−16 for all methods due to numer-
ical condition of the kernel matrix K (a known feature of
the Gaussian kernel used in this experiment).

The investigation was extended to larger dimensions (d = 3
and d = 10) and more complex integrands f in the Ap-

Figure 3. Performance on for the running illustration of Figs. 1
and 2. The top plot shows SMC-KQ against KQ, whilst the bottom
plot illustrates the versions with kernel learning.

pendix. In all cases, considerable improvements were ob-
tained using SMC-KQ over KQ.

4.2. Inference for Differential Equations

Consider the model given by dx/dt = f (t|θ) with solution
x(t|θ) depending on unknown parameters θ. Suppose we
can obtain observations through the following noise model
(likelihood): y(ti) = x(ti|θ) + ei at times 0 = t1 < . . . <
tn where we assume ei ∼ N (0, σ2) for known σ > 0. Our
goal is to estimate x(T |θ) for a ﬁxed (potentially large)
T > 0. To do so, we will use a Bayesian approach and
specify a prior p(θ), then obtain samples from the poste-
rior π(θ) := p(θ|y) using MCMC. The posterior predictive
mean is then deﬁned as: Π(cid:0)x(T |·)(cid:1) = (cid:82) x(T |θ)π(θ)dθ,
and this can be estimated using an empirical average from
the posterior samples. This type of integration problem is
particularly challenging as the integrand requires simulat-
ing from the differential equation at each iteration. Further-
more, the larger T or the smaller the grid, the longer the
simulation will be and the higher the computational cost.

For a tractable test-bed, we considered Hooke’s law, given

On the Sampling Problem for Kernel Quadrature

5. DISCUSSION

In this paper we formalised the optimal sampling prob-
lem for KQ. A general, practical solution was proposed,
based on novel use of SMC methods. Initial empirical re-
sults demonstrate performance gains relative to standard
approach of KQ with Π(cid:48) = Π. A more challenging exam-
ple based on parameter estimation for differential equations
was used to illustrate the potential of SMC-KQ for Bayesian
computation in combination with Stein’s method.

Our methods were general but required user-speciﬁed
choice of an initial distribution Π0. For compact state
spaces X we recommend taking Π0 to be uniform. For
non-compact spaces, however, there is a degree of ﬂex-
ibility here and default solutions, such as wide Gaussian
distributions, necessarily require user input. However, the
choice of Π0 is easier than the choice of Π(cid:48) itself, since Π0
is not required to be optimal. In our examples, improved
performance (relative to standard KQ) was observed for a
range of reference distributions Π0.

A main motivation for this research was to provide an al-
ternative to optimisation-based KQ that alleviates strong
dependence on the choice of kernel (Sec. 2.2). This pa-
per provides essential groundwork toward that goal, in de-
veloping sampling-based methods for KQ in the case of
complex and expensive integration problems. An empiri-
cal comparison of sampling-based and optimisation-based
methods is reserved for future work.

Two extensions of this research are identiﬁed: First, the
curse of dimension that is intrinsic to standard Sobolev
spaces can be alleviated by demanding ‘dominating mixed
smoothness’; our methods are compatible with these (es-
sentially tensor product) kernels (Dick et al., 2013). Sec-
ond, the use of sequential QMC (Gerber and Chopin, 2015)
can be considered, motivated by further orders of magni-
tude reduction in numerical error observed for determinis-
tic point sets (see Fig. 13 in the Appendix).

ACKNOWLEDGEMENTS

FXB was supported by the EPSRC grant [EP/L016710/1].
CJO & MG we supported by the Lloyds Register Foun-
dation Programme on Data-Centric Engineering. WYC
was supported by the ARC Centre of Excellence in Math-
ematical and Statistical Frontiers. MG was supported
by the EPSRC grants [EP/J016934/3, EP/K034154/1,
EP/P020720/1], an EPSRC Established Career Fellowship,
the EU grant [EU/259348], a Royal Society Wolfson Re-
search Merit Award. FXB, CJO, JC & MG were also sup-
ported by the SAMSI working group on Probabilistic Nu-
merics.

Figure 4. Comparison of SMC-KQ and KQ on the ODE inverse
problem. The top plot illustrates the physical system, the mid-
dle plot shows observations of the ODE, whilst the bottom plot
illustrates the superior performance of SMC-KQ against KQ.

by the following second order homogeneous ODE given by

θ5

d2x
dt2 + θ4

dx
dt

+ θ3x = 0,

with initial conditions x(0) = θ1 and x(cid:48)(0) = θ2. This
equation represents the evolution of a mass on a spring with
friction (Robinson, 2004, Chapter 13). More precisely, θ3
denotes the spring constant, θ4 the damping coefﬁcient rep-
resenting friction and θ5 the mass of the object. Since this
differential equation is an overdetermined system we ﬁxed
θ5 = 1. In this case, if θ2
4 ≤ 4θ3, we get a damped oscilla-
tory behaviour as presented in Fig. 4 (top). Data were gen-
erated with σ = 0.4, (θ1, θ2, θ3, θ4) = (1, 3.75, 2.5, 0.5).
with log-normal priors with scale equal to 0.5 for all pa-
rameters.

To implement KQ under an unknown normalisation con-
stant for Π, we followed Oates et al. (2017) and made use
of a Gaussian kernel that was adapted with Stein’s method
(see the Appendix for details). The reference distribution
Π0 was an wide uniform prior on the hypercube [0, 10]4.
Brute force computation was used to obtain a benchmark
value for the integral. For the SMC algorithm, an indepen-
dent lognormal transition kernel was used at each iteration
with parameters automatically tuned to the current set of
particles. Results in Fig. 4 demonstrate that SMC-KQ out-
performs KQ for these integration problems. These results
improve upon those reported in Oates et al. (2016) for a
similar integration problem based on parameter estimation
for differential equations.

On the Sampling Problem for Kernel Quadrature

REFERENCES

F. Bach. Sharp analysis of low-rank kernel matrix approx-

imations. In Proc. I. Conf. Learn. Theory, 2013.

F. Bach. On the equivalence between kernel quadrature
rules and random features. arXiv:1502.06800, 2015.

N. S. Bakhvalov. On approximate computation of integrals.
Vestnik MGU, Ser. Math. Mech. Astron. Phys. Chem., 4:
3–18, 1959. In Russian.

A. Berlinet and C. Thomas-Agnan. Reproducing kernel
Hilbert spaces in probability and statistics. Springer Sci-
ence & Business Media, 2011.

F-X. Briol, C. J. Oates, M. Girolami, and M. A. Osborne.
Frank-Wolfe Bayesian quadrature: Probabilistic integra-
tion with theoretical guarantees. In Adv. Neur. Inf. Proc.
Sys., 2015a.

T. Gunter, R. Garnett, M. Osborne, P. Hennig, and
S. Roberts. Sampling for inference in probabilistic mod-
els with fast Bayesian quadrature. In Adv. Neur. Inf. Proc.
Sys., 2014.

J. Hampton and A. Doostan. Coherence motivated sam-
pling and convergence analysis of least squares polyno-
mial Chaos regression. Comput. Methods Appl. Mech.
Engrg., 290:73–97, 2015.

A. Hinrichs. Optimal importance sampling for the approxi-
mation of integrals. J. Complexity, 26(2):125–134, 2010.

F. Huszar and D. Duvenaud. Optimally-weighted herding
is Bayesian quadrature. In Uncert. Artif. Intell., 2012.

M. Kanagawa, B. Sriperumbudur, and K. Fukumizu. Con-
vergence guarantees for kernel-based quadrature rules in
misspeciﬁed settings. In Adv. Neur. Inf. Proc. Sys., 2016.

F-X. Briol, C. J. Oates, M. Girolami, M. A. Osborne, and
D. Sejdinovic. Probabilistic integration: A role for statis-
ticians in numerical analysis? arXiv:1512.00933, 2015b.

H. Kersting and P. Hennig. Active uncertainty calibration
In Proc. Conf. Uncert. Artif.

in bayesian ode solvers.
Intell., 2016.

N. Chopin. A sequential particle ﬁlter method for static

models. Biometrika, 89(3):539–552, 2002.

A. Cohen and G. Migliorati. Optimal weighted least-

squares methods. arXiv:1608.00512, 2016.

P. Del Moral, A. Doucet, and A. Jasra. Sequential monte
carlo samplers. J. R. Stat. Soc. Ser. B. Stat. Methodol.,
68:411–436, 2006.

P. Diaconis. Bayesian Numerical Analysis, volume IV of
Statistical Decision Theory and Related Topics, pages
163–175. Springer-Verlag, New York, 1988.

J. Dick and F. Pillichshammer. Digital nets and sequences:
Discrepancy Theory and Quasi–Monte Carlo Integra-
tion. Cambridge University Press, 2010.

H. K¨onig. Eigenvalues of compact operators with appli-
cations to integral operators. Linear Algebra Appl., 84:
111–122, 1986.

S. Kristoffersen. The empirical interpolation method. Mas-
ter’s thesis, Department of Mathematical Sciences, Nor-
wegian University of Science and Technology, 2013.

Q. Liu and J. D. Lee. Black-Box Importance Sampling. I.

Conf. Artif. Intell. Stat., 2017.

Y. Ma, R. Garnett, and J. Schneider. Active Area Search
via Bayesian Quadrature. I. Conf Artif. Intell. Stat., 33,
2014.

M. W. Mahoney. Randomized algorithms for matrices and
data. Found. Trends Mach. Learn., 3(2):123–224, 2011.

J. Dick, F. Y. Kuo, and I. H. Sloan. High-dimensional in-
tegration: The quasi-Monte Carlo way. Acta Numerica,
22:133–288, 2013.

C. J. Oates, J. Cockayne, F-X. Briol, and M. Girolami.
Convergence Rates for a Class of Estimators Based on
Stein’s Identity. arXiv:1603.03220, 2016.

P. Drineas, M. Magdon-Ismail, M. W. Mahoney, and D. P.
Woodruff. Fast approximation of matrix coherence and
statistical leverage. J. Mach. Learn. Res., 13:3475–3506,
2012.

J. L. Eftang and B. Stamm. Parameter multi-domain ‘hp’
empirical interpolation. I. J. Numer. Methods in Eng., 90
(4):412–428, 2012.

M. Gerber and N. Chopin. Sequential quasi Monte Carlo.

J. R. Statist. Soc. B, 77(3):509–579, 2015.

C. J. Oates, M. Girolami, and N. Chopin. Control Func-
tionals for Monte Carlo Integration. J. R. Stat. Soc. Ser.
B. Stat. Methodol., 2017. To appear.

A. O’Hagan. Bayes-Hermite quadrature. J. Statist. Plann.

Inference, 29:245–260, 1991.

M. A. Osborne, D. Duvenaud, R. Garnett, C. E. Ras-
mussen, S. Roberts, and Z. Ghahramani. Active learning
of model evidence using Bayesian quadrature. In Adv.
Neur. Inf. Proc. Sys., 2012a.

On the Sampling Problem for Kernel Quadrature

M. A. Osborne, R. Garnett, S. Roberts, C. Hart, S. Aigrain,
and N. Gibson. Bayesian quadrature for ratios. In Proc.
I. Conf. Artif. Intell. Stat., 2012b.

R. Patel, T. A. Goldstein, E. L. Dyer, A.Mirhoseini, and
R. G. Baraniuk. OASIS: Adaptive Column Sampling for
Kernel Matrix Approximation. arXiv:1505.05208, 2015.

S. Paul, K. Ciosek, M. A. Osborne, and S. Whiteson. Al-
ternating Optimisation and Quadrature for Robust Rein-
forcement Learning. arXiv:1605.07496, 2016.

L. Plaskota, G.W. Wasilkowski, and Y. Zhao. New averag-
ing technique for approximating weighted integrals. J.
Complexity, 25(3):268–291, 2009.

J. Pr¨uher and M. ˇSimandl. Bayesian Quadrature in Non-
linear Filtering. In 12th I. Conf. Inform. Control Autom.
Robot., 2015.

C. E. Rasmussen and Z. Ghahramani. Bayesian Monte

Carlo. In Adv. Neur. Inf. Proc. Sys., 2002.

C. Robert and G. Casella. Monte Carlo statistical methods.

Springer Science & Business Media, 2013.

J. C. Robinson. An introduction to ordinary differential

equations. Cambridge University Press, 2004.

S. S¨arkk¨a, J. Hartikainen, L. Svensson, and F. Sandblom.
On the relation between Gaussian process quadratures
and sigma-point methods. arXiv:1504.05994, 2015.

T. Shi, M. Belkin, and B. Yu.

Data spectroscopy:
Eigenspaces of convolution operators and clustering.
Ann. Statist., 37(6):3960–3984, 2009.

B. Simon. Trace Ideals and Their Applications. Cambridge

University Press, 1979.

A. Smola, A. Gretton, L. Song, and B. Sch¨olkopf. A
Hilbert space embedding for distributions. In Algorith-
mic Learn. Theor., pages 13–31, 2007.

A. Sommariva and M. Vianello. Numerical cubature on
scattered data by radial basis functions. Computing, 76
(3-4):295–310, 2006.

N. M. Temme. Special Functions: An Introduction to the
Classical Functions of Mathematical Physics. Wiley,
New York, 1996.

H. Wendland. Scattered Data Approximation. Cambridge

University Press, 2004.

Y. Zhou, A. M. Johansen, and J. A. D. Aston. Towards
Automatic Model Comparison: An Adaptive Sequential
Monte Carlo Approach. J. Comput. Graph. Statist., 25
(3):701–726, 2016.

