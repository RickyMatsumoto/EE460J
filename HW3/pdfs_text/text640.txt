Beyond Filters: Compact Feature Map for Portable Deep Model

Yunhe Wang 1 Chang Xu 2 Chao Xu 1 Dacheng Tao 2

Abstract
Convolutional neural networks (CNNs) have
shown extraordinary performance in a number of
applications, but they are usually of heavy design
for the accuracy reason. Beyond compressing the
ﬁlters in CNNs, this paper focuses on the redun-
dancy in the feature maps derived from the large
number of ﬁlters in a layer. We propose to ex-
tract intrinsic representation of the feature maps
and preserve the discriminability of the features.
Circulant matrix is employed to formulate the
feature map transformation, which only requires
O(d log d) computation complexity to embed a
d-dimensional feature map. The ﬁlter is then re-
conﬁgured to establish the mapping from origi-
nal input to the new compact feature map, and
the resulting network can preserve intrinsic infor-
mation of the original network with signiﬁcantly
fewer parameters, which not only decreases the
online memory for launching CNN but also ac-
celerates the computation speed. Experiments on
benchmark image datasets demonstrate the supe-
riority of the proposed algorithm over state-of-
the-art methods.

1. Introduction

Tremendous power of convolutional neural networks
(CNNs) have been well demonstrated in a wide variety
of computer vision applications, from image classiﬁca-
tion (Simonyan & Zisserman, 2015) and object detec-
tion (Ren et al., 2015) to image segmentation (Long et al.,
2015). Meanwhile, there is a recent consensus that there are

1Key Laboratory of Machine Perception (MOE) and Coop-
erative Medianet Innovation Center, School of EECS, Peking
2UBTech Sydney
University, Beijing 100871, P.R. China.
AI Institute, School of IT, FEIT, The University of Syd-
ney, Darlington, NSW 2008, Australia.
Correspondence
to: Yunhe Wang <wangyunhe@pku.edu.cn>, Chang Xu
<c.xu@sydney.edu.au>, Chao Xu <xuchao@cis.pku.edu.cn>,
Dacheng Tao <dacheng.tao@sydney.edu.au>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

signiﬁcant redundancy in most of existing convolutional
neural networks. For instance, the ResNet-50 (He et al.,
2015) with some 50 convolutional layers needs over 95MB
memory for storage and over 3.8 × 109 times of ﬂoating
number multiplications for calculating each image, and af-
ter discarding more than 75% of its weights, the network
still works as usual (Wang et al., 2016).

Admittedly, a heavy neural network is extremely difﬁcult
to train and to use in mobile terminal apps due to the lim-
ited memory and computational resource. Lots of methods
have been developed to reduce the amount of parameters in
CNNs (Liu et al., 2015) to obtain a considerable compres-
sion ratio. (Han et al., 2015) discarded subtle weights in
a pre-trained network and constructed a sparse neural net-
work with less computational complexity. Subsequently,
(Wang et al., 2016) further studied the redundancy on all
weights and their underlying connections in the DCT fre-
quency domain, which achieves higher compression and
speed-up ratios. (Wen et al., 2016) excavated subtle con-
nections in different aspects and (Figurnov et al., 2016) re-
ﬁned the conventional convolution neurons as locally con-
nection on the input data in order to reduce the computa-
tional cost. In addition, there are a variety of techniques for
compressing convolution ﬁlters, e.g., pruning (Han et al.,
2016; Hu et al., 2016; Li et al., 2016), quantization and bi-
narization (Arora et al., 2014; Rastegari et al., 2016; Cour-
bariaux & Bengio, 2016), matrix approximation (Cheng
et al., 2015), and matrix decomposition (Denton et al.,
2014).

Although these methods obtained promising performance
to reduce the storage of convolution ﬁlters, the memory us-
age introduced by ﬁlters is still huge in the stage of online
inference. This is because except convolution ﬁlters, we
have to store feature maps (output data) of different layers
for the subsequent processes, e.g., over 97MB memory is
required for storing feature maps of one single image when
running a ResNet-50 (He et al., 2015) without batch nor-
malization layers, and a batch consisting of 32 instances
consumes some 3.2GB GPU memory. However, existing
compression methods tend to directly compress the ﬁlters
in one step and rarely consider the signiﬁcant demand of
feature maps on the storage and computational cost.

In a CNN, the size of a convolution ﬁlter is usually much

Beyond Filters: Compact Feature Map for Portable Deep Model

smaller than the number of ﬁlters in a convolutional layer.
Given a convolutional layer with 1024 ﬁlters of size 3 × 3,
any 3×3 patch in the input data will be mapped into a 1024-
dimensional space R9 → R1024. The size of the space
to describe this small patch has broken up more than 100
folds, which leads to the redundancy of the feature map.
We are therefore motivated to discover the intrinsic repre-
sentations of the redundant feature maps via dimensionality
reduction. Circulant matrix is employed to formulate the
feature map transformation considering its low space com-
plexity and high mapping speed. Based on the obtained
compact feature maps, we re-formulate the convolution ﬁl-
ters to establish its connections with the input data. In sum-
mary, the proposed approach makes the following contribu-
tions:

• We propose to excavate the intrinsic information and
decrease the redundancy in feature maps derived from
a large number of ﬁlters in each layer, and then the net-
work architecture is upgraded to produce a new com-
pact network with fewer ﬁlters but the similar discrim-
inativeness.

• We devise to learn a circulant matrix for projection
which is exactly a diagonal matrix in the Fourier do-
main and thus yields a high speed for training and low
complexity for mapping.

• Experiments demonstrate that, compared to the orig-
inal heavy network, the learned portable counterpart
network achieves a comparable accuracy, but has sig-
niﬁcantly lower memory usage and computational
cost.

2. Feature Map Dimensionality Reduction

Here we will ﬁrst introduce some preliminaries of CNNs
and then develop feature map dimensionality reduction
method.

For a convolutional layer L in a pre-trained convolution
neural network N whose input data and output feature
maps are X ∈ RH×W ×c and Y ∈ RH (cid:48)×W (cid:48)×d, respec-
tively, where H, W , H (cid:48), W (cid:48) are widths and heights of X
and Y , c is the channel number (i.e., the number of convo-
lution ﬁlters in the previous layer) and d is the number of
convolution ﬁlters in this layer. These convolution ﬁlters
can be denoted as a tensor, i.e., F ∈ Rs1×s2×c×d, where s1
and s2 are the width and the height of ﬁlters, respectively.
Taking the neural network as a powerful feature extractor,
the convolutional layer then becomes a mapping from the
patch x ∈ Rs1×s2 to feature map y ∈ Rd×1.

Generally, d is much larger than s = s1 × s2, e.g., d = 256
and s = 1 in the second layer in ResNet-50, and there are
even some layers with d > 2000. Admittedly, using such
a long d-dimensional vector to represent a s1 × s2 area is
heavy and redundant. To decrease the storage and com-

putation cost of the feature map, we attempt to discover
the compact representations of the feature maps. Many
sophisticated methods such as locally linear embedding
(LLE, (Roweis & Saul, 2000)), principle component analy-
sis (PCA, (Wold et al., 1987; Pan & Jiashi, 2017)), isomet-
ric feature mapping (Isomap, (Tenenbaum et al., 2000)),
locality preserving projection (LLP, (He & Niyogi, 2004)),
and other excellent dimensionality methods (Pan et al.,
2016; Xu et al., 2014), can be applied for dimensionality
reduction. Low-dimensional representations produced by
these methods can inherit intrinsic information of original
high-dimensional data, so that the performance of the trans-
formed features can be maintained, and enhanced in some
cases. We thus proceed to develop an exclusive feature
map dimensionality reduction method for the deep network
compression problem.

Dividing X into q = H (cid:48) × W (cid:48) patches and vectoriz-
ing them, we have X = [vec(x1), ..., vec(xq)] ∈ Rsc×q.
Accordingly, we reformulate feature maps and convolu-
tion ﬁlters as Y = [vec(Y1), ..., vec(Yd)] ∈ Rq×d and
F = [vec(F1), ..., vec(Fd)] ∈ Rsc×d, respectively. Thus,
the conventional convolution operation of the given layer L
can be rewritten as:

Y = XT F,

(1)

where the i-th column in Y corresponds to the convolution
responses of all patches through the i-th ﬁlter in F. Note
that, when we consider the entire dataset, q should be ad-
ditionally multiplied by the number of samples which is an
extreme large number. The most compact representation of
a CNN should have no correlation between feature maps
derived from different convolution ﬁlters. In other words,
feature maps from different ﬁlters should be independent
of each other as far as possible. The independence (or re-
dundancy) in Y can be measured by

Θ(Y) = ||YT Y ◦ (1 − I)||2
F ,

(2)

where || · ||F is the Frobenius norm for matrices, 1 is a
full one matrix, ◦ is the element-wise product, and I is
an identity matrix. Denote the reduced feature maps as
˜Y = φ(Y) ∈ Rq× ˜d, where ˜d ≤ d and φ(·) can be
either a linear (Wold et al., 1987) or non-linear transfor-
mation (Roweis & Saul, 2000). However, since there are
numerous training samples in real word image datasets
(e.g., ImageNet (Russakovsky et al., 2015)), computational
complexities of the nonlinear transformation will be great.
We thus use the linear transformation instead, i.e., ˜Y =
φ(Y) = YP T , where P ∈ R ˜d×d is the projection matrix.
An optimal transformation should generate the new repre-
sentations which occupy more information of the original
input and have less internal correlation. Based on the mea-
surement in Fcn. 2, the optimal projection P can be solved

Beyond Filters: Compact Feature Map for Portable Deep Model

by minimizing the following function:

min
P,c

||P YT YP T − C||2

F , s.t. C = diag(c),

(3)

where C = diag(c) is a diagonal matrix, whose functional-
ity is equal to the identity matrix in Fcn. 2.

Deep neural network enjoys great popularity due to its ex-
cellent capability of learning effective features for exam-
ples. An optimal projection should thus not only remove
redundancy between feature maps, but also preserve the
discriminability of these features. If images from different
categories are well separated from each other in the feature
space, classiﬁers will more easily accomplish the classiﬁca-
tion task. To maintain the accuracy of the original network
and its representation capability, we propose to preserve
the distances between feature maps and form the following
objective function for seeking the compact feature maps:

||P YT YP T − C||2

F + λ||D( ˜Y) − D(Y)||

min
P,c

(4)

s.t.

˜Y = YP T , C = diag(c),

where D(Y) = D ∈ Rq×q and Dij is the Euclidean dis-
tance between the i-th column and the j-th column of Y.

3. Optimal Feature Map Learning

The above section has proposed a feature map dimension-
ality reduction model. However, calculating the distance
matrix D = D(·) is inefﬁcient and memory consuming
since the column length of Y corresponds to the number
of training samples and is rather large in practice. For ex-
ample, there are over 1.2×106 images in the ILSVRC-2012
dataset and there are up to 4096 ﬁlters of a network layer.
The size of D will be larger than 4 × 109, which is inconve-
nient for distance calculation. In this section, we will pro-
pose an alternative feature map dimensionality reduction
approach, which consists of two steps: distance preserva-
tion and sparse approximation.

In fact, distances between feature maps can be easily pre-
served if P is orthogonal, i.e., P P T = I, where I is
an identity matrix with size d × d. For any two fea-
ture maps y1, y2, ∈ Rd, we have ||y1P T ||2 = ||y1||2 and
||y1P T − y2P T ||2
2. We thus reformulate
Fcn. 4 as

2 = ||y1 − y2||2

||P YT YP T − C||2
F ,

min
P

s.t. C = diag(c), P P T = I.

(5)

The orthogonal transformation P learned by Fcn. 5 is able
to extract the intrinsic representation and preserve distances
between feature maps, but the dimensionality has not been
reduced since P is a square matrix. Hence at the second

stage, we propose to use a sparse matrix to approximate
the representation generated by P ,

1
2

min
˜Y

|| ˜Y − YP T ||2

F + λ|| ˜Y||2,1,

(6)

where || ˜Y||2,1 = (cid:80)
i || ˜Yi|| and ˜Yi is the i-th column in
˜Y. || · ||2,1 is (cid:96)2,1-norm which is a widely used regulariza-
tion (Nie et al., 2010; Liu et al., 2010) for removing useless
columns in ˜Y and the closed form solution of Fcn. 6 is

˜Yi =

(cid:26) ||ui||−λ

||ui|| ui,
0,

if λ < ||ui||
otherwise

where ui = YP T
is the i-th column in P .
i
Zero columns in ˜Y can be discarded to achieve the low-
dimensional representations.

and Pi

By combining Fcn. 6 and Fcn. 5, we can obtain a uniﬁed
model as follow

min
P,c

||P YT YP T − C||2

F + β||c||1

s.t. C = diag(c), P P T = I,

(7)

(8)

where || · ||1 is the (cid:96)1 norm to make c sparse, so that some
small valued columns in ˜Y = YP T will be discarded. β
is a weight parameter which controls the sparsity of ˜Y and
implicitly inﬂuence the resulting dimensionality of the new
feature map of this layer.

Considering there are d × d variables in P and d can be
up to 4096, Fcn. 8 cannot be efﬁciently optimized w.r.t. P .
Since each frequency coefﬁcient corresponds to a Fourier
base with different textures, circulant matrices have com-
plex internal structures and strong diversity thus can be uti-
lized for approximating huge matrices (Cheng et al., 2015).
We therefore propose using a circulant matrix (Gray, 2006;
Henriques et al., 2015; 2014) to formulate P , which then
only has d variables in the Fourier frequency domain. We
propose the following model to learn the projection for gen-
erating the compact low-dimensional feature maps:

min
p,c

||P YT YP T − C||2

F + α||P P T − I||2

F + β||c||1,

s.t. P = circ(p), C = diag(c),

(9)

where α is the weight for relaxing the equality constrain
in Fcn. 5, and P = circ(p) is a circulant matrix. For the
d × 1 vector p = (p1, ..., pd)T , we can refer to it as the base
sample, and the cyclic shift operator can be deﬁned as:

circ(p) :=

.

(10)











p1
p2
...
pd−1
pd

pd
p1

p2

pd−1

. . .
pd

p1
. . .
. . .

p3

. . .
. . .
p2











p2
p3
...
pd
p1

Beyond Filters: Compact Feature Map for Portable Deep Model

(13)

4. CNN Layer Reconstruction

Given the fact that all circulant matrices are made diagonal
by the discrete Fourier transform (DFT (Bracewell, 1965)),
P and P T can be expressed as

P =

SH diag(ˆp) S, P T =

S diag(ˆp) SH ,

(11)

1
d

1
d

where S is the DFT transform matrix which is constant,
the DFT is a linear transform, and SH S = dI. ˆp is the
frequency representation of p, i.e.,

ˆp = F(p) = Sp,

(12)

and its inverse discrete Fourier transform (IDFT) is p =
F −1(ˆp) = 1
d SH (ˆp). In following illustrations, we will use
a hat ˆ to denote the DFT frequency representations.

Since any two bases in S are orthogonal thus it can hold the
Euclidean distance between any two vectors. For any two
d-dimensional samples in Y we have

||SyT

1 ||2

2 =

||y1||2
2,

||SyT

1 − SyT

2 ||2

2 =

||y1 − y2||2
2.

1
d
1
d

In addition, the most elegant property of the circulant ma-
trix is that the projection in the original domain is equiv-
alent to vector element wise product in the Fourier do-
main (Oppenheim et al., 1999), which is beneﬁcial for sig-
niﬁcantly decreasing the computational complexity, i.e.,

F(P yT ) = F(p) ◦ F(y)T ,
F(yP T ) = F(y) ◦ F(p)T ,

(14)

where ◦ denotes the element wise product. Since DFT and
IDFT can be efﬁciently computed in O(d log d) with the
fast Fourier transform (FFT, (Bracewell, 1965)), the projec-
tion for generating low-dimensional feature maps is only
O(d log d), compared with the O(d2) complexity of the
original dense matrix multiplication. Considering the ef-
ﬁcient computation over circulant matrix in the frequency
domain, we propose to use the frequency optimizing ap-
proach to obtain the optimal feature maps representation
˜Y = YP T . We optimize Fcn. 9 by alternatively ﬁxing p
and c, and leave the optimization details in the supplemen-
tary materials for the limited page length.

Given the optimal ˆp and c, the transformation for reducing
the dimensionality of feature maps can be written as

P =

diag(¯c)SH ˆP S,

(15)

1
d

where ¯ci = 0, if ci = 0, and ˜ci = 1, otherwise, the
transformation P in Fcn. 9 is a row sparse matrix, and the
rows with all zeros can be discarded to reformulate a com-
pact transformation matrix P ˜d with ˜d rows according to ¯c.

Therefore the feature map matrix Y can be transformed as
˜Y = YP T
. This projection is exactly a linear transform,
˜d
if we only take one convolutional layer into consideration,
i.e., the input data matrix X is ﬁxed, we can explicitly in-
clude the ﬁlter matrix F into the dimensionality reduction
procedure, i.e.,

˜Y = YP T

˜d = XFT P T

˜d = X ˜FT .

(16)

Hence, we can also directly reduce the number of convolu-
tion ﬁlters after obtaining the optimal projection matrix P ˜d.
Fixing the ﬁlter size as s1 × s2 × c, we can reconstruct con-
volutional layers with smaller ﬁlters ˜F ∈ Rs1×s2× ˜d. Based
on the above analysis, we have the following proposition:

Proposition 1. Given a convolutional layer L with d ﬁl-
ters, i.e., its feature map dimensionality is d. For the d-
dimensional feature of any sample through L, the proposed
method for solving its low-dimensional embedding has
space complexity O(d), and time complexity O(d log d).

Section 3 proposes an effective approach for learning com-
pact feature maps of a given convolutional layer. In the on-
line inference, it is impossible to ﬁrst calculate the original
high-dimensional feature maps, and then project them into
the low-dimensional space. To conserve the online compu-
tation resource, we thus aim to establish the mapping di-
rectly from the input data to the compact feature map.

The dimensionality of the feature map for the i-th convolu-
tional layer Li has been reduced by Fcn. 16, and the num-
ber of convolution ﬁlters of Li has also been reduced from
d to ˜d (where ˜d (cid:28) d). For the following convolutional layer
L(i+1), the size of the input data ˜X has becomes H ×W × ˜d
and we have ˜X ∈ Rs ˜d×q, which leads original ﬁlters can no
longer be used for calculating. Thus, we propose minimiz-
ing the following function for reconstructing convolution
ﬁlters of this layer:

|| ˜Y − ˜XT ˜F||2

F + γ|| ˜F||2
F ,

(17)

min
˜F

where ˜Y is the compact feature maps of L(i+1) after apply-
ing Fcn. 16 and γ is a weight parameter for balancing the
two terms. Note that the second term can be regarded as
a weight decay regularization in the training of neural net-
works (Krizhevsky et al., 2012). Fcn. 17 can be efﬁciently
solved by the following closed form solution:

˜F = ( ˜X ˜XT + γI)−1 ˜X ˜Y,

(18)

where I is an identity matrix. However, when the scale of
the dataset is enormous, we cannot construct the two huge
matrices ˜X and ˜Y through all instances in the dataset. The

Beyond Filters: Compact Feature Map for Portable Deep Model

Algorithm 1 CNN Layer Reconstruction Method.
Input: A pre-trained convolutional neural network N learned
through a dataset X with k convolutional layers: L1, ..., Lk,
weight parameter γ, learning rate η.

1: Calculate feature maps of each layer by using the original

network: {Y1, ...Yk} ← N (X );

Learn the projection Pi by solving Fcn. 9;
Calculate new feature maps: ˜Yi ← YiP T
i ;

2: for i = 1 to k − 1 do
3:
4:
5: end for
6: Keep feature maps of the k-th layer: ˜Yk ← Yk;
7: Construct a new network ˜N according to { ˜Y1, ... ˜Yk} and
initialize convolution ﬁlters { ˜F1, ... ˜Fk} by random values
from the standard normal distribution;

8: repeat
9:
10:
11:
12:

Randomly select a batch Xj from X ;
for i = 1 to k do

Generate input data ˜Xi of Li exploiting ˜N ;
Estimate the new ﬁlter matrix (Fcn. 20):
˜Fi ← ˜Fi − η∂L( ˜Fi)/∂ ˜Fi;
Convert ˜Fi into ﬁlter data and ﬁll it in ˜N ;

13:
end for
14:
15: until Convergence;
Output: The new convolutional neural network ˜N .

mini-batch strategy is adopted for updating ˜F iteratively.
The loss function of ˜F can be directly formulated as

L( ˜F) = Tr( ˜FT ˜XT ˜X ˜F)

− 2Tr( ˜FT ˜X ˜Y) + γTr( ˜FT ˜F),

(19)

and the gradient of L( ˜F) is

∂L( ˜F)
∂ ˜F

By using stochastic gradient descent (SGD), ˜F can be up-
dated as

˜F = ˜F − η

∂L( ˜F)
∂ ˜F

,

(21)

where η is the learning rate.

It is worth mentioning that input data of the ﬁrst layer of
the original network N and feature map of the last layer
(closely related to the number of classiﬁcation labels) are
kept unchanged. As for other intermediate convolutional
layers and fully connected layers, we can generate compact
feature maps ˜Y from the original feature maps Y. Then,
calculate the input data ˜X using the compressed network
˜N and estimate the ﬁlter matrix ˜F. The detailed ﬁlters up-
dating procedure can be found in Alg. 1.

According to Proposition 1, for a d-dimensional feature,
the complexity of the proposed feature map dimensionality
reduction method with the help of circulant matrix is only
O(d log d). Compared to O(d2) of other traditional linear
projection methods, the proposed scheme is of great beneﬁt

for conducting experiments on large scale datasets. More-
over, since we only need to store a d-dimensional vector
for one layer, the proposed method also have an obvious
advantage on the space complexity for learning a portable
version of neural networks with a larger number of layers
(e.g., ResNet (He et al., 2015)).

Discussion. There are some works investigating the in-
trinsic information of feature maps in the original neural
network to learn a new thinner and deeper neural network.
(Hinton et al., 2015) ﬁrst built a thinner neural network and
then made its feature map of the fully connected layer sim-
ilar to that of the original pre-trained networks, thus en-
hanced the accuracy of the new network. (Romero et al.,
2015) further extended this work to a general model which
minimizes the difference between the feature map of an ar-
bitrary layer in the smaller network and the feature map of
a given layer in the original network, yields a thinner and
deeper network with some accuracy decline.

The difference between these methods and the proposed
method is two-fold:
(i) These methods rebuild a new
student network with less parameters while the proposed
method outputs a compact CNN based on the original net-
work itself, which inherits the well-designed architectures;
(ii) The performance of the newly learned student network
will be declined, since it is only inﬂuenced by the infor-
mation from one or several layers of the teacher network.
By contrast, the proposed method excavates redundancy in
feature maps of every layer and preserves the distances be-
tween examples to guarantee the accuracy of the CNN.

A novel dimensionality reduction method for learning a
portable neural network has been proposed in Alg. 1. Com-
pared with the original heavy network N , the new network
˜N has the same depth but less convolution ﬁlters per layer.
In this section, we will further analyze the memory usage
and computation cost of ˜N and calculate the compression
ratio and speed-up ratio theoretically.

i×W (cid:48)

i ×di and F ∈ Rs2

Speed-up ratio. Consider the i-th convolutional layer Li in
the original network N with its output feature map and con-
volution ﬁlters are Yi ∈ RH (cid:48)
i ×ci×di,
respectively. We only discuss square ﬁlters and the conclu-
sion can be straightforwardly extended to non-square ﬁlters
as well. Wherein, ci = di−1 is the channel number of ﬁl-
ters in Li and d0 = 3 (RGB color images). The feature
map and convolution ﬁlters in the corresponding layer ˜Li
i × ˜di and
in the learned compact network are Yi ∈ RH (cid:48)
i ×˜ci× ˜di, respectively. Generally, weights and fea-
F ∈ Rs2
ture maps are stored in 32-bit ﬂoating values whose multi-
plications are much more expensive than additions. Com-
plexities of other auxiliary layers (e.g., pooling, Relu, etc..)

i×W (cid:48)

= 2 ˜X ˜XT ˜F − 2 ˜X ˜Y + 2γ ˜F.

(20)

5. Analysis on Compression Performance

Beyond Filters: Compact Feature Map for Portable Deep Model

(a) Accuracy of the reconstructed model.

(b) Model accuracy after ﬁne-tuning.

(c) Compression and speed-up ratios.

Figure 1. The performance of the proposed CNN compression method with different β.

have been discarded since they only account for a subtle
proportion of the overall complexity. Hence, considering
the major multiplications, the speed-up ratio of the compact
network for this layer compared with the original network
is

rs =

i di−1diH (cid:48)
s2
˜di−1
˜diH (cid:48)
s2
i

iW (cid:48)
i
iW (cid:48)
i

=

di−1di
˜di
˜di−1

.

(22)

It is obvious that the speed-up ratio for one convolutional
layer relates to numbers of ﬁlters in this layer and the pre-
vious layer. Thus, if we only keep 1/2 ﬁlters per layer, the
speed-up ratio will be increased to 4×.

Compression ratio. The online memory can be divided
into two major parts, feature maps of different layers and
ﬁlters of convolutional layers. Although we can remove
feature maps of a layer after it has already been used for
calculating the following layer, the the memory allocation
and release will increase the time consumption. More-
over, if a layer is connected with several other convolu-
tional layers, we have to store feature maps of previous
layers when doing online inference (e.g., the second con-
volutional layer and the ﬁfth convolutional layer will be
combined in ResNet-50 (He et al., 2015), and we have to
preserve these feature maps before merging them). For a
given convolutional layer Li, the compression ratio of the
proposed method is

rc =

i di−1di + diH (cid:48)
s2
˜di + ˜diH (cid:48)
˜di−1
s2
i

iW (cid:48)
i
iW (cid:48)
i

,

(23)

which is simultaneously affected by the current layer and
the pervious layer. Meanwhile, the memory for storing fea-
ture maps of other layers, such as pooling layers and Relu
layers, will be reduced. We will further illustrate the de-
tailed compression ratio and speed-up ratio of the proposed
method in the following section experimentally.

6. Experiments

Baselines and Models. Several effective approaches for
compressing deep neural networks were selected for com-
parison: SVD (Denton et al., 2014), XNOR-Net (Rastegari

et al., 2016), Pruning (Han et al., 2016), Perforation (Fig-
urnov et al., 2016), and CNNpack (Wang et al., 2016), and
we denoted the proposed method as RedCNN. The eval-
uation was conducted on the MNIST and ILSVRC2012
datasets (Russakovsky et al., 2015). We ﬁrst tested the
performance of the proposed method and analyzed impacts
of parameters on the MNIST dataset using LeNet (LeCun
et al., 1998), then compared the proposed method with two
benchmark CNNs (VGGNet-16 (Simonyan & Zisserman,
2015) and ResNet-50 (He et al., 2015)) on the ILSVRC
2012 dataset (Russakovsky et al., 2015) which has more
than 1 million nature images. All methods were imple-
mented using MatConvNet (Vedaldi & Lenc, 2015) and
ran on NVIDIA K40 cards. Filters and data in CNNs were
stored and calculated as 32 bit ﬂoating-point values.

Impact of parameters. The hyper-parameter γ in the pro-
posed reconstruction method (Alg. 1) controls the weight
decay regularization and makes weights in new convolu-
tion ﬁlters not too large. We set γ as 0.0005 empiri-
cally.
In addition, the proposed dimensionality method
(Fcn. 9) has two hyper-parameters, i.e., α and β. We ﬁrst
tested their impact on the network accuracy by conducting
an experiment using a LeNet for classifying the MNIST
dataset (Vedaldi & Lenc, 2015), where the network has four
convolutional layers of size 5 × 5 × 1 × 20, 5 × 5 × 20 × 50,
4 × 4 × 50 × 500, and 1 × 1 × 500 × 10, respectively, and its
accuracy is 99.2%. α is used for enforcing the projection
matrix P to be orthogonal and is set to be 1.5, experimen-
tally. β is directly related to the sparsity of P , and it effects
compression and speed-up ratios of the proposed method.
Although a larger β will produce a smaller network, it also
leads to a larger distortion on the Euclidean distances be-
tween samples. To have a clear illustration, we reported the
compression performance by ranging different β, as shown
in Fig. 1.

From Fig. 1 (a), we found that the compact network recon-
structed by using Alg. 1 can also hold a considerable accu-
racy (e.g., 78% when β = 0.06), which demonstrates that
the proposed method can preserve the intrinsic information
of the original CNN. Moreover, the accuracy decline can
be rebounded (98.9% when β = 0.06) after ﬁne-tuning as

75.078.081.084.00.010.040.070.1Accuracy(%)b98.999.099.199.20.010.040.070.1Accuracy (%)b36912150.010.040.070.1ratio(×)βcompressionspeed-upBeyond Filters: Compact Feature Map for Portable Deep Model

shown in Fig. 1 (b). However, a network that was directly
trained with such a small architecture can only achieve a
92.8% accuracy. In addition, although the impact of β is
sensitive but monotonous, a larger β enhances compres-
sion and speed-up ratios simultaneously, but decreases the
accuracy of CNNs as well. The value of β can be easily ad-
justed according to the demand and restrictions of devices.
In our experiments, we set β = 0.06 which provides the
best trade-off between compression performance and accu-
racy, i.e., rc = 11.3×, rs = 8.7×, with 99.16% accuracy.
In this case, the layers in the compact convolution network
is of the size 5×5×1×5, 5×5×5×20, 4×4×20×96, and
1×1×96×10, respectively. The resulting compact network
only occupies around 130KB memory. The MATLAB ﬁle
of the compressed network and the demo code can be found
in https://github.com/YunheWang/RedCNN.

Deep Neural Networks Compression on ISLVRC-2012.
We next employed the proposed RedCNN for CNN com-
pression on the ImageNet ILSVRC-2012 dataset (Rus-
sakovsky et al., 2015), which contains over 1.2M train-
ing images and 50k validation images. We evaluated the
compression performance on three widely used models:
AlexNet (Krizhevsky et al., 2012), which has more than
61M parameters and a top-5 accuracy of 80.8%; VGGNet-
16 (Simonyan & Zisserman, 2015), which has over 138M
parameters and a top-5 accuracy of 90.1%; and ResNet-
50 (He et al., 2015) which has more than 150 layers with
54 convolutional layers, and a top-5 accuracy of 92.9%. It
is worth mentioning that there are considerable ﬁlters in
the ResNet-50, and thus the network has less redundancy
and it is hard for further compression. We ﬁrst begin our
experiment with the AlexNet dataset, and the detailed ex-
perimental results were shown in Tab. 1.

Table 1. Compression statistics for AlexNet.

Layer
conv1
conv2
conv3
conv4
conv5
fc6
fc7
fc8
Total

Memory
1.24MB
1.88MB
3.62MB
2.78MB
1.85MB
144MB
64.02MB
15.63MB
235.03MB

rc
Mult.
1.05×108
2.4×
10.0× 2.23×108
15.5× 1.49×108
1.12×108
3.0×
0.74×108
1.2×
0.37×108
3.5×
72.0× 0.16×108
23.4× 0.04×108
5.06× 7.24×108

rs
2.4×
15.4×
21.2×
3.6×
1.2×
3.5×
72.1×
23.5×
4.31×

From Tab. 1, we found that the proposed method achieved
a 5.1× compression ratio and a 4.3× speed-up ratio for
AlexNet. Then, we reported the compression result of
VGGNet-16 in Tab. 2.

It can be seen from Tab. 2, we obtained a 6.19× compres-
sion ratio and a 9.63× speed-up ratio on VGGNet-16. In
addition, the compression ratio and the speed-up ratio on
ResNet-50 are 4.14× and 5.82×, respectively. Note that
the compression ratio we reported here is calculated by

Table 2. Compression statistics for VGG-16 Net.

Layer
conv1 1
conv1 2
conv2 1
conv2 2
conv3 1
conv3 2
conv3 3
conv4 1
conv4 2
conv4 3
conv5 1
conv5 2
conv5 3
fc6
fc7
fc8
Total

Memory
12.26MB
12.39MB
6.41MB
6.69MB
4.19MB
5.31MB
5.31MB
6.03MB
10.53MB
10.53MB
9.38MB
9.38MB
9.38MB
392MB
64MB
15.62MB
579.46MB

rc
Mult.
0.11×109
10.7×
2.41×109
1.8×
1.20×109
3.3×
2.41×109
4.0×
1.20×109
7.2×
2.41×109
2.7×
2.41×109
2.9×
1.20×109
41.3×
2.41×109
16.5×
2.41×109
3.9×
0.60×109
7.8×
0.60×109
18.5×
0.60×109
23.4×
0.41×109
8.6×
0.16×108
3.3×
0.41×107
2.2×
6.19× 2.04 × 1010

rs
10.7×
19.0×
5.7×
12.0×
21.9×
10.2×
4.2×
56.2×
70.1×
5.1×
8.0×
21.3×
26.7×
8.6×
3.3×
2.2×
9.63×

Fcn. 23, which contains both convolution ﬁlters and fea-
ture maps. More compression results of these three CNN
models can be found in the supplementary material.

Comparison with state-of-the-art methods. Tab. 3 de-
tails the compression results of the proposed method and
several state-of-the-art methods on three benchmark deep
CNN models. Since comparison methods do not change
the number of ﬁlters of the original neural network, feature
map compression ratios of these methods are both equal to
1. Thus, we reported the compression ratio of ﬁlters rc1
and feature maps rc2 separately for a fair comparison. For
a convolutional neural network with layers, its compression
ratios is calculated as

rc1 =

(cid:80)p

(cid:80)p

i=1 s2
i=1 s2
i

i di−1di
˜di−1
˜di

,

rc2 =

(cid:80)p

(cid:80)p

i=1 diH (cid:48)
˜diH (cid:48)

iW (cid:48)
i
iW (cid:48)
i

i=1

.

(24)

Tab. 3 also shows the cost of various models for processing
one image, i.e., storage of ﬁlters, memory usage of feature
maps, and multiplications for calculating convolutions. It
is obvious that feature maps accounting a considerable pro-
portion of memory usage of the whole network, and the
proposed RedCNN can provide signiﬁcant compression ra-
tios rc2 on every network. Although we can remove the
feature map of a layer after calculation for saving memory,
frequently allocating and releasing is also time consuming.

It can be seen from Tab. 3, RedCNN clearly achieves the
best performance in terms of both the speed-up ratio (rs)
and the feature map compression ratio (rc1).
In addi-
tion, convolution ﬁlter compression ratios of the proposed
method is lower than those of pruning (Han et al., 2016)
and CNNpack (Wang et al., 2016). These two compari-
son methods employed quantization approaches (i.e., one-
dimensional k-means clustering), and thus 32-bit ﬂoating
values can be converted into about 8-bit values without af-

Beyond Filters: Compact Feature Map for Portable Deep Model

Table 3. An overall comparison of state-of-the-art methods for deep neural network compression and speed-up, where rc1 is the com-
pression ratio of convolution ﬁlters, rc2 is the compression ratio of feature maps, and rs is the speed-up ratio.

Model

Evaluation Original

AlexNet
Filters (232 MB)
Maps (2.5MB)
Multiplications
(7.24 × 108)

VGGNet-16
Filters (572 MB)
Maps (52 MB)
Multiplications
(1.54 × 1010)

ResNet-50
Filters (97 MB)
Maps (40 MB)
Multiplications
(5.82 × 109)

rc1
rc2
rs
top-1 err
top-5 err
rc1
rc2
rs
top-1 err
top-5 err
rc1
rc2
rs
top-1 err
top-5 err

1×
1×
1×
41.8%
19.2%
1×
1×
1×
28.5%
9.9%
1×
1×
1×
24.6%
7.7%

SVD
5×
1×
2×

XNOR
32×
32×
64×
44.0% 56.8%
20.5% 31.8%

-
-
-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-
-
-

Pruning
35×
1×
-
42.7%
19.7%
49×
1×
3.5×
31.1%
10.9%
-
-
-
-
-

Perforation
1.7×
1×
2×
44.7%
-
1.7×
1×
1.9×
31.0%
-
-
-
-
-
-

CNNpack
39×
1×
25×
41.6%
19.2%
46×
1×
9.4×
29.7%
10.4%
12.2×
1×
4×
-
7.8%

RedCNN
5.12×
2.45×
4.31×
42.1%
19.3%
6.87×
3.07×
9.63×
29.3%
10.2%
4.35×
3.71×
5.82×
25.7%
8.2%

fecting the accuracy of the original network. If we adopt
this similar strategy, the convolution ﬁlter compression ra-
tio rc1 of the proposed scheme can be further multiplied a
factor of around 4×, e.g., we can obtain an almost 17.4×
ﬁlter compression ratio on ResNet-50 model, which is su-
perior to all the other comparison methods. However, 8-bit
(or other unconventional format) value cannot be directly
used in generic devices (e.g., GPU cards, mobile phones),
and thus we did not try them in the experiments of this
In summary, the proposed RedCNN can achieve
paper.
considerable compression and speed-up ratios, which can
make existing deep models portable.

were signiﬁcantly reduced. The results are extremely en-
couraging, e.g., the compressed ResNet can recognize over
500 images per second. This efﬁciency can also be inher-
ited into the ﬁne-tuning process, therefore, the compressed
networks can be quickly adjusted when applied them to a
new dataset. In addition, the practical speed-up ratios of
runtimes were slightly lower than the corresponding theo-
retical speed-up ratios rs due to the costs incurred by data
transmission, pooling, padding, etc.. Note that, the runtime
reported here is a bit higher than that in (Vedaldi & Lenc,
2015), due to different conﬁgurations and hardware envi-
ronments.

Table 4. Runtime of the proposed deep neural network compres-
sion algorithm on different models per image.

7. Conclusions and Discussions

time
LeNet
AlexNet
VGGNet-16
ResNet-50

Original
0.17 ms
1.28 ms
16.67 ms
9.03 ms

RedCNN speed-up
0.04 ms
0.74 ms
3.38 ms
1.84 ms

4.3×
1.7×
4.9×
4.9×

Runtime.
In fact, most of comparison methods cannot
signiﬁcantly accelerate the deep network for various addi-
tional operations. For example, (Han et al., 2016) needs
to decode the CSR data before testing, which slows down
the online inference and will not achieve the comparable
compression and speed-up ratios with those of the pro-
posed algorithm in practice. Since the proposed compres-
sion method directly re-conﬁgures the network into a more
compact form, and does not require other additional sup-
port for realizing the network speed-up, the runtime of the
compressed models for processing images will be reduced
signiﬁcantly. To explicitly demonstrate the superiority of
the proposed method, we compared runtimes for recogniz-
ing images by benchmark CNN models before and after
applying the proposed method, and showed the experimen-
tal results in Tab. 4.

We found that runtimes of these models after compression

Compression methods for learning portable CNNs are ur-
gently required so that neural networks can be used on mo-
bile devices. Besides convolution ﬁlters, the storage of fea-
ture maps also accounts for a larger proportion of the on-
line memory usage, we thus no longer search useless con-
nections or weights of ﬁlters. In this paper, we present a
feature map dimensionality reduction method by excavat-
ing and removing redundancy in feature maps generated by
different ﬁlters. Although the portable network learned by
our approach has signiﬁcantly fewer parameters, its fea-
ture maps can also preserve intrinsic information of the
original network. Experiments conducted on benchmark
datasets and models show that the proposed method can
achieve considerable compression ratio and speed-up ra-
tios simultaneously without affecting the classiﬁcation ac-
curacy of the original CNN. In addition, the compressed
network generated by exploiting the proposed method is
still a regular CNN with 32-bit ﬂoat values which does not
have any decoding or other procedures for online inference.

Acknowledgements

We thank supports of NSFC 61375026 and 2015BAF15B00, and
ARC Projects: FT-130101457, DP-140102164, LP-150100671.

Beyond Filters: Compact Feature Map for Portable Deep Model

References

Arora, Sanjeev, Bhaskara, Aditya, Ge, Rong, and Ma, Tengyu.
Provable bounds for learning some deep representations.
ICML, 2014.

Bracewell, Ron. The fourier transform and iis applications. New

York, 5, 1965.

Cheng, Yu, Yu, Felix X, Feris, Rogerio S, Kumar, Sanjiv, Choud-
hary, Alok, and Chang, Shi-Fu. An exploration of parameter
redundancy in deep networks with circulant projections.
In
CVPR, 2015.

Courbariaux, Matthieu and Bengio, Yoshua. Binarynet: Training
deep neural networks with weights and activations constrained
to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.

Denton, Emily L, Zaremba, Wojciech, Bruna, Joan, LeCun, Yann,
and Fergus, Rob. Exploiting linear structure within convolu-
tional networks for efﬁcient evaluation. In NIPS, 2014.

Figurnov, Michael, Vetrov, Dmitry, and Kohli, Pushmeet. Perfo-
ratedcnns: Acceleration through elimination of redundant con-
volutions. NIPS, 2016.

Gray, Robert M. Toeplitz and circulant matrices: A review. now

publishers inc, 2006.

Han, Song, Pool, Jeff, Tran, John, and Dally, William. Learning
both weights and connections for efﬁcient neural network. In
NIPS, 2015.

Han, Song, Mao, Huizi, and Dally, William J. Deep compres-
sion: Compressing deep neural networks with pruning, trained
quantization and huffman coding. 2016.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
Deep residual learning for image recognition. arXiv preprint
arXiv:1512.03385, 2015.

He, Xiaofei and Niyogi, Partha. Locality preserving projections.

In NIPS, 2004.

Henriques, Joao F, Martins, Pedro, Caseiro, Rui F, and Batista,
Jorge. Fast training of pose detectors in the fourier domain. In
NIPS, 2014.

Henriques, Jo˜ao F, Caseiro, Rui, Martins, Pedro, and Batista,
Jorge. High-speed tracking with kernelized correlation ﬁlters.
IEEE TPAMI, 37(3):583–596, 2015.

Hinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff. Distill-
arXiv preprint

ing the knowledge in a neural network.
arXiv:1503.02531, 2015.

Hu, Hengyuan, Peng, Rui, Tai, Yu-Wing, and Tang, Chi-
Keung. Network trimming: A data-driven neuron pruning
approach towards efﬁcient deep architectures. arXiv preprint
arXiv:1607.03250, 2016.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Im-
agenet classiﬁcation with deep convolutional neural networks.
In NIPS, 2012.

LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Li, Hao, Kadav, Asim, Durdanovic, Igor, Samet, Hanan, and
Graf, Hans Peter. Pruning ﬁlters for efﬁcient convnets. arXiv
preprint arXiv:1608.08710, 2016.

Liu, Baoyuan, Wang, Min, Foroosh, Hassan, Tappen, Marshall,
and Pensky, Marianna. Sparse convolutional neural networks.
In CVPR, 2015.

Liu, Guangcan, Lin, Zhouchen, and Yu, Yong. Robust subspace
segmentation by low-rank representation. In ICML, 2010.

Long, Jonathan, Shelhamer, Evan, and Darrell, Trevor. Fully
convolutional networks for semantic segmentation. In CVPR,
2015.

Nie, Feiping, Huang, Heng, Cai, Xiao, and Ding, Chris H. Ef-
ﬁcient and robust feature selection via joint (cid:96)2,1 norms mini-
mization. In NIPS, 2010.

Oppenheim, Alan V, Schafer, Ronald W, and Buck, John R.
Discrete-time signal processing. Pren- tice Hall Upper Saddle
River, 1999.

Pan, Zhou and Jiashi, Feng. Outlier-robust tensor pca. In CVPR,

2017.

Pan, Zhou, Zhouchen, Lin, and Chao, Zhang.

Integrated low-
rank-based discriminative feature learning for recognition.
IEEE TNNLS, 27(5):1080–1093, 2016.

Rastegari, Mohammad, Ordonez, Vicente, Redmon, Joseph, and
Farhadi, Ali. Xnor-net: Imagenet classiﬁcation using binary
convolutional neural networks. ECCV, 2016.

Ren, Shaoqing, He, Kaiming, Girshick, Ross, and Sun, Jian.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, 2015.

Romero, Adriana, Ballas, Nicolas, Kahou, Samira Ebrahimi,
Chassang, Antoine, Gatta, Carlo, and Bengio, Yoshua. Fitnets:
Hints for thin deep nets. In ICLR, 2015.

Roweis, Sam T and Saul, Lawrence K. Nonlinear dimensionality
reduction by locally linear embedding. Science, 290(5500):
2323–2326, 2000.

Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan,
Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, An-
drej, Khosla, Aditya, Bernstein, Michael, et al.
Imagenet
IJCV, 115(3):211–
large scale visual recognition challenge.
252, 2015.

Simonyan, Karen and Zisserman, Andrew. Very deep convolu-
tional networks for large-scale image recognition. ICLR, 2015.

Tenenbaum, Joshua B, De Silva, Vin, and Langford, John C. A
global geometric framework for nonlinear dimensionality re-
duction. science, 290(5500):2319–2323, 2000.

Vedaldi, Andrea and Lenc, Karel. Matconvnet: Convolutional
neural networks for matlab. In Proceedings of the 23rd Annual
ACM Conference on Multimedia Conference, 2015.

Wang, Yunhe, Xu, Chang, You, Shan, Tao, Dacheng, and Xu,
Chao. Cnnpack: Packing convolutional neural networks in the
frequency domain. In NIPS, 2016.

Wen, Wei, Wu, Chunpeng, Wang, Yandan, Chen, Yiran, and Li,
Hai. Learning structured sparsity in deep neural networks. In
NIPS, 2016.

Wold, Svante, Esbensen, Kim, and Geladi, Paul. Principal com-
ponent analysis. Chemometrics and intelligent laboratory sys-
tems, 2(1-3):37–52, 1987.

Xu, Chang, Tao, Dacheng, Xu, Chao, and Rui, Yong. Large-
margin weakly supervised dimensionality reduction. In ICML,
2014.

