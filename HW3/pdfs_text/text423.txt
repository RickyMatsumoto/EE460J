Deletion-Robust Submodular Maximization:
Data Summarization with “the Right to be Forgotten”

Baharan Mirzasoleiman 1 Amin Karbasi 2 Andreas Krause 1

Abstract
How can we summarize a dynamic data stream
when elements selected for the summary can be
deleted at any time? This is an important chal-
lenge in online services, where the users gener-
ating the data may decide to exercise their right
to restrict the service provider from using (part
of) their data due to privacy concerns. Moti-
vated by this challenge, we introduce the dy-
namic deletion-robust submodular maximization
problem. We develop the ﬁrst resilient streaming
algorithm, called ROBUST-STREAMING, with a
constant factor approximation guarantee to the
optimum solution. We evaluate the effectiveness
of our approach on several real-world applica-
tions, including summarizing (1) streams of geo-
coordinates (2); streams of images; and (3) click-
stream log data, consisting of 45 million feature
vectors from a news recommendation task.

1. Introduction

Streams of data of massive and increasing volume are gen-
erated every second, and demand fast analysis and efﬁcient
storage, including massive clickstreams, stock market data,
image and video streams, sensor data for environmental or
health monitoring, to name a few. To make efﬁcient and re-
liable decisions we usually need to react in real-time to the
data. However, big and fast data makes it difﬁcult to store,
analyze, or make predictions. Therefore, data summariza-
tion – mining and extracting useful information from large
data sets – has become a central topic in machine learning
and information retrieval.

A recent body of research on data summarization relies on
utility/scoring functions that are submodular. Intuitively,
submodularity (Krause & Golovin, 2013) states that select-

1ETH Zurich, Switzerland 2Yale University, New Haven,
USA. Correspondence to: Baharan Mirzasoleiman <baha-
ranm@inf.ethz.ch>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ing any given data point earlier helps more than selecting it
later. Hence, submodular functions can score both diversity
and representativeness of a subset w.r.t. the entire dataset.
Thus, many problems in data summarization require max-
imizing submodular set functions subject to cardinality
(or more complicated hereditary constraints). Numer-
ous examples include exemplar-based clustering (Dueck &
Frey, 2007), document (Lin & Bilmes, 2011) and corpus
summarization (Sipos et al., 2012), recommender systems
(El-Arini & Guestrin, 2011), search result diversiﬁcation
(Rakesh Agrawal, 2009), data subset selection (Wei et al.,
2015), and social networks analysis (Kempe et al., 2003).

Classical methods, such as the celebrated greedy algorithm
(Nemhauser et al., 1978) or its accelerated versions (Mirza-
soleiman et al., 2015; Badanidiyuru & Vondr´ak, 2014)
require random access to the entire data, make multiple
passes, and select elements sequentially in order to pro-
duce near optimal solutions. Naturally, such solutions can-
not scale to large instances. The limitations of central-
ized methods inspired the design of streaming algorithms
that are able to gain insights from data as it is being col-
lected (Badanidiyuru et al., 2014; Chakrabarti & Kale,
2014; Chekuri et al., 2015; Mirzasoleiman et al., 2017).

While extracting useful information from big data in real-
time promises many beneﬁts, the development of more
sophisticated methods for extracting, analyzing and using
personal information has made privacy a major public is-
sue. Various web services rely on the collection and com-
bination of data about individuals from a wide variety of
sources. At the same time, the ability to control the in-
formation an individual can reveal about herself in online
applications has become a growing concern.

The “right to be forgotten” (with a speciﬁc mandate for
protection in the European Data Protection Regulation
(2012), and concrete guidelines released in 2014) allows
individuals to claim the ownership of their personal infor-
mation and gives them the authority to their online activ-
ities (videos, photos, tweets, etc). As an example, con-
sider a road trafﬁc information system that monitors trafﬁc
speeds, travel times and incidents in real time. It combines
the massive amount of control messages available at the
cellular network with their geo-coordinates in order to gen-

Deletion-Robust Submodular Maximization

erate the area-wide trafﬁc information service. Some con-
sumers, while using the service and providing data, may
not be willing to share information about speciﬁc locations
in order to protect their own privacy. With the right to be
forgotten, an individual can have certain data deleted from
online database records so that third parties (e.g., search en-
gines) can no longer trace them (Weber, 2011). Note that
the data could be in many forms, including a) user’s posts
to an online social media, b) visual data shared by wear-
able cameras (e.g., Google Glass), c) behavioral patterns or
feedback obtained from clicking on advertisement or news.

In this paper, we propose the ﬁrst framework that offers in-
stantaneous data summarization while preserving the right
of an individual to be forgotten. We cast this problem as
an instance of robust streaming submodular maximization
where the goal is to produce a concise real-time summary
in the face of data deletion requested by users. We develop
ROBUST-STREAMING, a method that for a generic stream-
ing algorithm STREAMINGALG with approximation guar-
antee α, ROBUST-STREAMING outputs a robust solution,
against any m deletions from the summary at any given
time, while preserving the same approximation guarantee.
To the best of our knowledge, ROBUST-STREAMING is the
ﬁrst algorithm with such strong theoretical guarantees. Our
experimental results also demonstrate the effectiveness of
ROBUST-STREAMING on several practical applications.

2. Background and Related Work

Several streaming algorithms for submodular maximiza-
tion have been recently developed. For monotone func-
tions, Gomes & Krause (2010) ﬁrst developed a multi-
pass algorithm with 1/2−(cid:15) approximation guarantee sub-
ject to a cardinality constraint k, using O(k) memory, un-
der strong assumptions on the way the data is generated.
Later, Badanidiyuru et al. (2014) proposed the ﬁrst sin-
gle pass streaming algorithm with 1/2 − (cid:15) approximation
under a cardinality constraint. They made no assump-
tions on the order of receiving data points, and only re-
quire O(k log k/(cid:15)) memory. Following the same line of in-
quiry, Chakrabarti & Kale (2014) developed a single pass
algorithm with 1/4p approximation guarantee for handling
more general constraints such as intersections of p ma-
troids. The required memory is unbounded and increases
polylogarithmically with the size of the data. For general
submodular functions, Chekuri et al. (2015) presented a
randomized algorithm subject to a broader range of con-
straints, namely p-matchoids. Their method gives a (2 −
o(1))/(8+e)p approximation using O(k log k/(cid:15)2) memory
(k is the size of the largest feasible solution). Very recently,
Mirzasoleiman et al. (2017) introduced a (4p − 1)/4p(8p +
2d − 1)-approximation algorithm under a p-system and d
knapsack constraints, using O(pk log2(k)/(cid:15)2) memory.

An important requirement, which frequently arises in prac-
is robustness. Krause et al. (2008) proposed the
tice,
problem of robust submodular observation selection, where
we want to solve max|A|≤k mini∈[(cid:96)] fi(A), for normal-
ized monotonic fi. Submodular maximization of f robust
against m deletions can be cast as an instance of the above
problem: max|A|≤k min|B|≤m f (A\B). The running time,
however, will be exponential in m. Recently, Orlin et al.
(2016) developed an algorithm with an asymptotic guar-
antee 0.285 for deletion-robust submodular maximization
k) deletions. The results can be im-
under up to m = o(
proved for only 1 or 2 deletions.

√

The aforementioned approaches aim to construct solutions
that are robust against deletions in a batch mode way, with-
out being able to update the solution set after each deletion.
To the best of our knowledge, this is the ﬁrst to address the
general deletion-robust submodular maximization problem
in the streaming setting. We also highlight the fact that our
method does not require m, the number of deletions, to be
bounded by k, the size of the largest feasible solution.

Very recently, submodular optimization over sliding win-
dows has been considered, where we want to maintain a
solution that considers only the last W items (Epasto et al.,
2017; Jiecao et al., 2017). This is in contrast to our setting,
where the guarantee is with respect to all the elements re-
ceived from the stream, except those that have been deleted.
The sliding window model can be easily incorporated into
our solution to get a robust sliding window streaming algo-
rithm with the possibility of m deletions in the window.

3. Deletion-Robust Model

We review the static submodular data summarization prob-
lem. We then formalize a novel dynamic variant, and con-
straints on time and memory that algorithms need to obey.

3.1. Static Submodular Data Summarization

In static data summarization, we have a large but ﬁxed
dataset V of size n, and we are interested in ﬁnding a
summary that best represents the data. The representa-
tiveness of a subset is deﬁned based on a utility func-
tion f : 2V → R+ where for any A ⊂ V the function
f (A) quantiﬁes how well A represents V . We deﬁne the
marginal gain of adding an element e ∈ V to a summary
A ⊂ V by ∆(e|A) = f (A ∪ {e}) − f (A). In many data
summarization applications, the utility function f satisﬁes
submodularity, i.e., for all A ⊆ B ⊆ V and e ∈ V \ B,

∆(e|A) ≥ ∆(e|B).

Many data summarization applications can be cast as an
instance of a constrained submodular maximization:

OPT = max
A∈I

f (A),

(1)

Deletion-Robust Submodular Maximization

where I ⊂ 2V is a given family of feasible solutions.
We will denote by A∗ the optimal solution, i.e. A∗ =
arg maxA∈I f (A). A common type of constraint is a car-
dinality constraint, i.e., I = {A ⊆ 2V , s.t., |A| ≤ k}.
Finding A∗ even under cardinality constraint is NP-hard,
for many classes of submodular functions (Feige, 1998).
However, a seminal result by Nemhauser et al. (1978) states
that for a non-negative and monotone submodular function
a simple greedy algorithm that starts with the empty set
S0 = ∅, and at each iteration augments the solution with the
element with highest marginal gain, obtains a (1 − 1/e) ap-
proximation to the optimum solution. For small, static data,
the centralized greedy algorithm or its accelerated variants
produce near-optimal solutions. However, such methods
fail to scale to truly large problems.

3.2. Dynamic Data: Additions and Deletions

In dynamic deletion-robust submodular maximization
problem, the data V is generated at a fast pace and in real-
time, such that at any point t in time, a subset Vt ⊆ V of
the data has arrived. Naturally, we assume that V1 ⊆ V2 ⊆
· · · ⊆ Vn, with no assumption made on the order or the size
of the datastream. Importantly, we allow data to be deleted
dynamically as well. We use Dt to refer to data deleted by
time t, where again D1 ⊆ D2 ⊆ · · · ⊆ Dn. Without loss
of generality, below we assume that at every time step t ex-
actly one element et ∈ V is either added or deleted, i.e.,
|Dt \ Dt−1| + |Vt \ Vt−1| = 1. We now seek to solve a
dynamic variant of Problem (1)
OPTt = max
At∈It

f (At) s.t. It = {A : A ∈ I ∧ A ⊆ Vt \ Dt}.

(2)
Note that in general a feasible solution at time t might not
be a feasible solution at a later time t(cid:48). This is particularly
important in practical situations where a subset of the ele-
ments Dt should be removed from the solution. We do not
make any assumptions on the order or the size of the data
stream V , but we assume that the total number of deletions
is limited to m , i.e., |Dn| ≤ m.

3.3. Dealing with Limited Time and Memory

In principle, we could solve Problem (2) by repeatedly – at
every time t – solving a static Problem (1) by restricting the
ground set V to Vt \ Dt. This is impractical even for mod-
erate problem sizes. For large problems, we may not even
be able to ﬁt Vt into the main memory of the computing
device (space constraints). Moreover, in real-time appli-
cations, one needs to make decisions in a timely manner
while the data is continuously arriving (time constraints).

We hence focus on streaming algorithms which may main-
tain a limited memory Mt ⊂ Vt \ Dt, and must have an
updated feasible solution {At | At ⊆ Mt, At ∈ It} to out-
put at any given time t. Ideally, the capacity of the memory

|Mt| should not depend on t and Vt. Whenever a new el-
ement is received, the algorithm can choose 1) to insert it
into its memory, provided that the memory does not ex-
ceed a pre-speciﬁed capacity bound, 2) to replace it with
one or a subset of elements in the memory (in the preemp-
tive model), or otherwise 3) the element gets discarded and
cannot be used later by the algorithm. If the algorithm re-
ceives a deletion request for a subset Dt ⊂ Vt at time t
(in which case It will be updated to accommodate this re-
quest) it has to drop Dt from Mt in addition to updating
At to make sure that the current solution is feasible (all
subsets A(cid:48)
t ⊂ Vt that contain an element from Dt are infea-
sible, i.e., A(cid:48)
t /∈ It). To account for such losses, the stream-
ing algorithm can only use other elements maintained in its
memory in order to produce a feasible candidate solution,
i.e. At ⊆ Mt ⊆ ((Vt \ Vt−1) ∪ Mt−1) \ Dt. We say that
the streaming algorithm is robust against m deletions, if it
can provide a feasible solution At ∈ It at any given time t
such that f (At) ≥ τ OPTt for some constant τ > 0. Later,
we show how robust streaming algorithms can be obtained
by carefully increasing the memory and running multiple
instances of existing streaming methods simultaneously.

4. Example Applications

We now discuss three concrete applications, with their
submodular objective functions f , where the size of the
datasets and the nature of the problem often require a
deletion-robust streaming solution.

4.1. Summarizing Click-stream and Geolocation Data

There exists a tremendous opportunity of harnessing preva-
lent activity logs and sensing resources. For instance, GPS
traces of mobile phones can be used by road trafﬁc infor-
mation systems (such as Google trafﬁc, TrafﬁcSense, Nav-
igon) to monitor travel times and incidents in real time. In
another example, stream of user activity logs is recorded
while users click on various parts of a webpage such as ads
and news while browsing the web, or using social media.
Continuously sharing all collected data is problematic for
several reasons. First, memory and communication con-
straints may limit the amount of data that can be stored and
transmitted across the network. Second, reasonable privacy
concerns may prohibit continuous tracking of users.

In many such applications, the data can be described in
terms of a kernel matrix K which encodes the similarity
between different data elements. The goal is to select a
small subset (active set) of elements while maintaining a
certain diversity. Very often, the utility function boils down
to the following monotone submodular function (Krause &
Golovin, 2013) where α > 0 and KS,S is the principal sub-
matrix of K indexed by the set S.

f (S) = log det(I + αKS,S)

(3)

Deletion-Robust Submodular Maximization

In light of privacy concerns, it is natural to consider partic-
ipatory models that empower users to decide what portion
of their data could be made available. If a user decides not
to share, or to revoke information about parts of their ac-
tivity, the monitoring system should be able to update the
summary to comply with users’ preferences. Therefore, we
use ROBUST-STREAMING to identify a robust set of the k
most informative data points by maximizing Eq. (3).

4.2. Summarizing Image Collections

Given a collection of images, one might be interested
in ﬁnding a subset that best summarizes and represents
the collection. This problem has recently been addressed
via submodular maximization. More concretely, Tschi-
atschek et al. (2014) designed several submodular objec-
tives f1, . . . , fl, which quantify different characteristics
that good summaries should have, e.g., being representa-
tive w.r.t. commonly reoccurring motives. Each function
either captures coverage (including facility location, sum-
coverage, and truncated graph cut, or rewards diversity
(such as clustered facility location, and clustered diversity).
Then, they optimize a weighted combination of such func-
tions
l
(cid:88)

fw(A) =

wifi(A),

(4)

i=1
where weights are non-negative, i.e., wi ≥ 0, and learned
via a large-margin structured prediction. We use their
learned mixtures of submodular functions in our image
summarization experiments. Now, consider a situation
where a user wants to summarize a large collection of her
photos. If she decides to delete some of the selected pho-
tos in the summary, she should be able to update the re-
sult without processing the whole collection from scratch.
ROBUST-STREAMING can be used as an appealing method.

5. Robust-Streaming Algorithm

In this section, we ﬁrst elaborate on why naively increas-
ing the solution size does not help. Then, we present our
main algorithm, ROBUST-STREAMING, for deletion-robust
streaming submodular maximization. Our approach builds
on the following key ideas: 1) simultaneously construct-
ing non-overlapping solutions, and 2) appropriately merg-
ing solutions upon deleting an element from the memory.

5.1. Increasing the Solution Size Does Not Help

One of the main challenges in designing streaming solu-
tions is to immediately discover whether an element re-
ceived from the data stream at time t is good enough to be
added to the memory Mt. This decision is usually made
based on the added value or marginal gain of the new el-
ement which in turn depends on the previously chosen el-
ements in the memory, i.e., Mt−1. Now, let us consider

the opposite scenario when an element e should be deleted
from the memory at time t. Since now we have a smaller
context, submodularity guarantees that the marginal gains
of the elements added to the memory after e was added,
could have only increased if e was not part of the stream
(diminishing returns). Hence, if some elements had large
marginal values to be included in the memory before the
deletion, they still do after the deletion. Based on this in-
tuition, a natural idea is to keep a solution of a bigger size,
say m+k (rather than k) for at most m deletions. However,
this idea does not work as shown by the following example.

Bad Example (Coverage): Consider a collection of n
subsets V = {B1, . . . , Bn}, where Bi ⊆ {1, . . . , n}, and a
coverage function f (A) = | ∪i∈A Bi|, A ⊆ V . Suppose we
receive B1 = {1, . . . , n}, and then Bi = {i} for 2 ≤ i ≤ n
from the stream. Streaming algorithms that select elements
according to their marginal gain and are allowed to pick
k + m elements, will only pick up B1 upon encounter (as
other elements provide no gain), and return An = {B1} af-
ter processing the stream. Hence, if B1 is deleted after the
stream is received, these algorithms return the empty set
An = ∅ (with f (An) = 0). An optimal algorithm which
knows that element B1 will be deleted, however, will return
set An = {B2, . . . , Bk+2}, with value f (An) = k + 1.
Hence, standard streaming algorithms fail arbitrarily badly
even under a single deletion (i.e., m = 1), even when we
allow them to pick sets larger than k.

In the following we show how we can solve the above issue
by carefully constructing not one but multiple solutions.

5.2. Building Multiple Solutions

As stated earlier, the existing one-pass streaming algo-
rithms for submodular maximization work by identifying
elements with marginal gains above a carefully chosen
threshold. This ensures that any element received from the
stream which is fairly similar to the elements of the solu-
tion set is discarded by the algorithm. Since elements are
chosen as diverse as possible, the solution may suffer dra-
matically in case of a deletion.

One simple idea is to try to ﬁnd m (near) duplicates for each
element e in the memory, i.e., ﬁnd e(cid:48) such that f (e(cid:48)) = f (e)
and ∆(e(cid:48)|e) = 0 (Orlin et al., 2016). This way if we face m
deletions we can still ﬁnd a good solution. The drawback
is that even one duplicate may not exist in the data stream
(see the bad example above), and we may not be able to
recover for the deleted element. Instead, what we will do
is to construct non-overlapping solutions such that once we
experience a deletion, only one solution gets affected.

In order to be robust against m deletions, we run a cas-
cading chain of r instances of STREAMINGALGs as fol-
lows. Let Mt = M (1)
denote the con-

, . . . , M (r)

, M (2)
t

t

t

Deletion-Robust Submodular Maximization

its memory M (j)
\ {ed}, we start a
new cascade from j+1-th instance, STREAMINGALG(j+1).

, namely, R(j)

t ← M (j)

t

t

t

The key reason why the above algorithm works is that the
guarantee provided by the streaming algorithm is indepen-
dent of the order of receiving the data elements. Note that
at any point in time, the ﬁrst instance i of the algorithm
with M (i)
(cid:54)= null has processed all the elements from the
stream Vt (not necessarily in the order the stream is origi-
nally received) except the ones deleted by time t, i.e., Dt.
Therefore, we can guarantee that STREAMINGALG (i) pro-
vides us with its inherent α-approximation guarantee for
reading Vt \ Dt. More precisely, f (S(i)
t ) ≥ αOPTt, where
OPTt is the optimum solution for the constrained optimiza-
tion problem (2) when we have m deletions.

In case of adversary deletions, there will be one deletion
from the solution of m instances of STREAMINGALG in
the chain. Therefore, having r = m + 1 instances, we will
remain with only one STREAMINGALG that gives us the
desired result. However, as shown later in this section, if
the deletions are i.i.d. (which is often the case in practice),
and we have m deletions in expectation, we need r to be
much smaller than m + 1. Finally, note that we do not need
to assume that m ≤ k where k is the size of the largest
feasible solution. The above idea works for arbitrary m ≤ n.

t

t

,

, S(1)
t

The pseudocode of ROBUST-STREAMING is given in Al-
It uses r ≤ m + 1 instances of STREAMIN-
gorithm 1.
GALG as subroutines in order to produce r solutions. We
denote by S(1)
. . . , S(r)
the solutions of the r
STREAMINGALG s at any given time t. We assume that
an instance i of STREAMINGALG(i) receive an input ele-
ment and produces a solution S(i)
based on the input. It
may also change its memory content M (i)
, and discard a
set R(i)
. Among all the remained solutions (i.e., the ones
t
that are not ”null”), it returns the ﬁrst solution in the chain,
i.e. the one with the lowest index.

t

t

Theorem 1 Let STREAMINGALG be a 1-pass streaming
algorithm that achieves an α-approximation guarantee for
the constrained maximization problem (2) with an update
time of T , and a memory of size M when there is no dele-
tion. Then ROBUST-STREAMING uses r ≤ m + 1 in-
stances of STREAMINGALG s to produce a feasible solu-
tion St ∈ It (now It encodes deletions in addition to con-
straints) such that f (St) = αOPTt as long as no more than
m elements are deleted from the data stream. Moreover,
ROBUST-STREAMING uses a memory of size rM , and has
worst case update time of O(r2M T ), and average update
time of O(rT ).

The proofs can be found in the appendix. In Table 1 we
combine the result of Theorem 1 with the existing stream-
ing algorithms that satisfy our requirements.

Figure 1: ROBUST-STREAMING uses r instances of a
generic STREAMINGALG to construct r non-overlapping
, M (2)
memories at any given time t, i.e., M (1)
, . . . , M (r)
.
t
t
Each instance produces a solution S(i)
and the solution re-
turned by ROBUST-STREAMING is the ﬁrst valid solution
St = {S(i)
t

|i = min j ∈ [1 · · ·r], M (i)

(cid:54)= null}.

t

t

t

tent of their memories at time t. When we receive a
new element e ∈ Vt from the data stream at time t, we
pass it to the ﬁrst instance of STREAMINGALG(1).
If
STREAMINGALG(1) discards e, the discarded element is
cascaded in the chain and is passed to its successive al-
gorithm, i.e. STREAMINGALG(2).
If e is discarded by
STREAMINGALG(2), the cascade continues and e is passed
to STREAMINGALG(3). This process continues until either
e is accepted by one of the instances or discarded for good.
Now, let us consider the case where e is accepted by the
i-th instance, SIEVE-STREAMING(i), in the chain. As dis-
cussed in Section 3.3, STREAMINGALG may choose to dis-
card a set of points R(i)
t ⊂ M (i)
from its memory before
inserting e, i.e., M (i)
. Note that R(i)
t
is empty, if e is inserted and no element is discarded from
M (i)
, we start a new
t
cascade from (i + 1)-th instance, STREAMINGALG (i+1).

. For every discarded element r ∈ R(i)
t

t ∪ {e} \ R(i)

t ← M (i)

t

t

Note that in the worst case, every element of the stream can
go once through the whole chain during the execution of the
algorithm, and thus the processing time for each element
scales linearly by r. An important observation is that at any
given time t, all the memories M (1)
con-
tain disjoint sets of elements. Next, we show how this data
structure leads to a deletion-robust streaming algorithm.

, · · · ,M (r)

, M (2)
t

t

t

5.3. Dealing with Deletions

t

t ← M (j)

Equipped with the above data structure shown in Fig. 1,
we now demonstrate how deletions can be treated. As-
sume an element ed is being deleted from the memory
of the j-th instance of STREAMINGALG(j) at time t, i.e.,
M (j)
\ {ed}. As discussed in Section 5.1, the
solution of the streaming algorithm can suffer dramatically
from a deletion, and we may not be able to restore the qual-
ity of the solution by substituting similar elements. Since
there is no guarantee for the quality of the solution after
a deletion, we remove STREAMINGALG (j) from the chain
by making R(j)
t = null and for all the remaining elements in

…Data Streamdiscarded forever12r12r2RrR1R1M2MrM1S2SrSDeletion-Robust Submodular Maximization

6. Experiments

∀i ∈ [1 · · · r]

t = ∅

t = 0, S(i)

if {Dt \ Dt−1} (cid:54)= ∅ then
ed ← {Dt \ Dt−1}
Delete(ed)

Algorithm 1 ROBUST-STREAMING
Input: data stream Vt, deletion set Dt, r ≤ m+1.
Output: solution St at any time t.
1: t = 1, M (i)
2: while ({Vt \ Vt−1} ∪ {Dt \ Dt−1} (cid:54)= ∅) do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12: end while

et ← {Vt \ Vt−1}
Add(1, et)

end if
t = t + 1
St = (cid:8)S(i)

| i = min{j ∈ [1 · · · r], M (j)

else

t

t

(cid:54)= null}(cid:9)

] =STREAMINGALG (i)(e)

13: function Add(i, R)
14:
15:

for e ∈ R do
, M (i)
t

, S(i)
t

[R(i)
t
if R(i)
(cid:54)= ∅ and i < r then
t
Add(i + 1, R(i)
t )

16:

17:
18:
19:
20: end function

end if
end for

24:

21: function Delete(e)
for i = 1 to r do
22:
if e ∈ M (i)
then
23:
R(i)
t = M (i)
t \ {e}
M (i)
t ← null
Add(i + 1, R(i)
t )
return

25:

t

26:
27:
28:
29:
30: end function

end if
end for

Theorem 2 Assume each element of the stream is deleted
with equal probability p = m/n, i.e., in expectation we
have m deletions from the stream. Then, with probability
1 − δ, ROBUST-STREAMING provides an α-approximation
as long as

r ≥

(cid:18) 1

(cid:19)k

1 − p

log (cid:0)1/δ(cid:1).

Theorem 2 shows that for ﬁxed k, δ and p, a constant num-
ber r of STREAMINGALGs is sufﬁcient to support m = pn
(expected) deletions independently of n.
In contrast, for
adversarial deletions, as analyzed in Theorem 1, pn + 1
copies of STREAMINGALG are required, which grows lin-
early in n. Hence, the required dependence of r on m is
much milder for random than adversarial deletions. This is
also veriﬁed by our experiments in Section 6.

We address the following questions: 1) How much can
ROBUST-STREAMING recover and possibly improve the
performance of STREAMINGALG in case of deletions? 2)
How much does the time of deletions affect the perfor-
mance? 3) To what extent does deleting representative
vs. random data points affect the performance? To this
end, we run ROBUST-STREAMING on the applications we
described in Section 4, namely, image collection summa-
rization, summarizing stream of geolocation sensor data,
as well as summarizing a clickstream of size 45 million.

Throughout this section we consider the following stream-
ing algorithms: SIEVE-STREAMING (Badanidiyuru et al.,
2014), STREAM-GREEDY (Gomes & Krause, 2010), and
STREAMING-GREEDY (Chekuri et al., 2015). We allow
all streaming algorithms,
including the non-preemptive
SIEVE-STREAMING, to update their solution after each
deletion. We also consider a stronger variant of SIEVE-
STREAMING, called EXTSIEVE, that aims to pick k ·r el-
ements to protect for deletions, i.e., is allowed the same
memory as ROBUST-STREAMING. After the deletions, the
remaining solution is pruned to k elements.

To compare the effect of deleting representative elements to
the that of deleting random elements from the stream, we
use two stochastic variants of the greedy algorithm, namely,
STOCHASTIC-GREEDY (Mirzasoleiman et al., 2015) and
RANDOM-GREEDY (Buchbinder et al., 2014). This way
we introduce randomness into the deletion process in a
principled way. Hence, we have:

STOCHASTIC-GREEDY (SG): Similar to the the greedy
algorithm, STOCHASTIC-GREEDY starts with an empty set
and adds one element at each iteration until obtains a solu-
tion of size m. But in each step it ﬁrst samples a random set
R of size (n/m) log(1/(cid:15)) and then adds an element from
R to the solution which maximizes the marginal gain.

RANDOM-GREEDY (RG): RANDOM-GREEDY iteratively
selects a random element from the top m elements with
the highest marginal gains, until ﬁnds a solution of size m.

For each deletion method, the m data points are deleted
either while receiving the data (where the steaming algo-
rithms have the chance to update their solutions by select-
ing new elements) or after receiving the data (where there
is no chance of updating the solution with new elements).
Finally, the performance of all algorithms are normalized
against the utility obtained by the centralized algorithm
that knows the set of deleted elements in advance.

6.1. Image Collection Summarization

We ﬁrst apply ROBUST-STREAMING to a collection of
100 images from Tschiatschek et al. (2014). We used

Deletion-Robust Submodular Maximization

Algorithm
ROBUST + SIEVE-STREAMING [BMKK’14]
ROBUST + f -MSM [CK’14]

Problem
Mon. Subm.
Mon. Subm.

ROBUST + STREAMING-GREEDY [CGQ’15] Non-mon. Subm.
ROBUST + STREAMING-LOCAL SEARCH
[MJK’17]

Non-mon. Subm.

Constraint
Cardinality
p-matroids
p-matchoid
p-system +
d knapsack

Appr. Fact.
1/2 − (cid:15)
1
4p
(1−ε)(2−o(1))
(8+e)p

(1−ε)(4p−1)
4p(8p+2d−1)

Memory
O(mk log k/(cid:15))
O(mk(log |V |)O(1))
O(mk log k/(cid:15)2)

O(mpk log2 k/(cid:15)2)

Table 1: ROBUST-STREAMING combined with 1-pass streaming algorithms can make them robust against m deletions.

the weighted combination of 594 submodular functions ei-
ther capturing coverage or rewarding diversity (c.f. Sec-
tion 4.2). Here, despite the small size of the dataset, com-
puting the weighted combination of 594 functions makes
the function evaluation considerably expensive.

Fig. 2a compares the performance of SIEVE-STREAMING
with its robust version ROBUST-STREAMING for r = 3 and
solution size k=5. Here, we vary the number m of deletions
from 1 to 20 after the whole stream is received. We see
that ROBUST-STREAMING maintains its performance by
updating the solution after deleting subsets of data points
imposed by different deletion strategies.
It can be seen
that, even for a larger number m of deletions, ROBUST-
STREAMING, run with parameter r < m, is able to return a
solution competitive with the strong centralized benchmark
that knows the deleted elements beforehand. For the image
collection, we were not able to compare the performance of
STREAM-GREEDY with its robust version due to the pro-
hibitive running time. Fig. 2b shows an example of an up-
dated image summary returned by ROBUST-STREAMING
after deleting the ﬁrst image from the summary.

(a) Images

(b) Images

Performance of ROBUST-STREAMING vs
Figure 2:
SIEVE-STREAMING for different deletion strategies (SG,
RG), at the end of stream, on a collection of 100 images.
Here we ﬁx k = 5 and r = 3. a) performance of ROBUST-
STREAMING and SIEVE-STREAMING normalized by the
utility obtained by greedy that knows the deleted elements
beforehand. b) updated solution returned by ROBUST-
STREAMING after deleting the ﬁrst images in the summary.

6.2. Summarizing a stream of geolocation data

Next we apply ROBUST-STREAMING to the active set se-
lection objective described in Section 4.1. Our dataset con-

i,j/h2) with h = 1500.

sists of 3,607 geolocations, collected during a one hour bike
ride around Zurich (Fatio, 2015). For each pair of points i
and j we used the corresponding (latitude, longitude) coor-
dinates to calculate their distance in meters di,j and chose
a Gaussian kernel Ki,j = exp(−d2
Fig. 3e shows the dataset where red and green tri-
angles show a summary of size 10 found by SIEVE-
STREAMING, and the updated summary provided by
ROBUST-STREAMING with r = 5 after deleting m = 70%
of the datapoints. Fig. 3a and 3c compare the perfor-
mance of SIEVE-STREAMING with its robust version when
the data is deleted after or during the stream, respectively.
As we see, ROBUST-STREAMING provides a solution very
close to the hindsight centralized method. Fig. 3b and
3d show similar behavior for STREAM-GREEDY. Note
that deleting data points via STOCHASTIC-GREEDY or
RANDOM-GREEDY are much more harmful on the qual-
ity of the solution provided by STREAM-GREEDY. We re-
peated the same experiment by dividing the map into grids
of length 2km. We then considered a partition matroid by
restricting the number of points selected from each grid
to be 1. The red and green triangles in Fig. 3f are the
summary found by STREAMING-GREEDY and the updated
summary provided by ROBUST-STREAMING after deleting
the shaded area in the ﬁgure.

6.3. Large scale click through prediction

For our large-scale experiment we consider again the active
set selection objective, described in Section 4.1. We used
Yahoo! Webscope data set containing 45,811,883 user click
logs for news articles displayed in the Featured Tab of the
Today Module on Yahoo! Front Page during the ﬁrst ten
days in May 2009 (Yahoo, 2012). For each visit, both the
user and shown articles are associated with a feature vector
of dimension 6. We take their outer product, resulting in a
feature vector of size 36.

The goal was to predict the user behavior for each displayed
article based on historical clicks. To do so, we considered
the ﬁrst 80% of the data (for the ﬁst 8 days) as our training
set, and the last 20% (for the last 2 days) as our test set.
We used Vowpal-Wabbit (Langford et al., 2007) to train a
linear classiﬁer on the full training set. Since only 4% of
the data points are clicked, we assign a weight of 10 to each

Number of deletions51015Normalized objective value0.860.880.90.920.940.960.98Robust-RGRobust-SGSieve-SGSieve-RGExtSieve-RGExtSieve-SGDeletion-Robust Submodular Maximization

ran ROBUST-STREAMING on each machine to ﬁnd a sum-
mary of size k/15, and merged the results to obtain the ﬁ-
nal summary of size k. We then start deleting the data uni-
formly at random until we left with only 1% of the data, and
trained another classiﬁer on the remaining elements from
the summary.

4a

the

compares

performance

Fig.
of ROBUST-
STREAMING for a ﬁxed active set of size k = 10, 000,
and r = 2 with random selection, randomly selecting
equal numbers of clicked and not-clicked vectors, and
using SIEVE-STREAMING for selecting equal numbers of
clicked and not-clicked data points. The y-axis shows the
improvement in AUC score of the classiﬁer trained on a
summary obtained by different algorithms over random
guessing (AUC=0.5), normalized by the AUC score of the
classiﬁer trained on the whole training data. To maximize
fairness, we let other baselines select a subset of r.k
elements before deletions. Fig. 4b shows the same quantity
for r = 5.
It can be seen that a slight increase in the
amount of memory helps boosting the performance for all
the algorithms. However, ROBUST-STREAMING beneﬁts
from the additional memory the most, and can almost
recover the performance of the classiﬁer trained on the full
training data, even after 99% deletion.

(a) SIEVE-STREAMING, at end (b) STREAM-GREEDY, at end

(c) SIEVE-STREAMING, during (d) STREAM-GREEDY, during

(e) Cardinality constraints

(f) Matroid constraints

Figure 3: ROBUST-STREAMING vs SIEVE-STREAMING
and STREAM-GREEDY for different deletion strategies
(SG, RG) on geolocation data. We ﬁx k = 20 and r = 5.
a) and c) show the performance of robustiﬁed SIEVE-
STREAMING, whereas b) and d) show performance for ro-
bustiﬁed STREAM-GREEDY. a) and b) consider the per-
formance after deletions at the end of the stream, while c)
and d) consider average performance while deletions hap-
pen during the stream. e) red and green triangles show a set
of size 10 found by SIEVE-STREAMING and the updated
solution found by ROBUST-STREAMING where 70% of the
points are deleted. f) set found by STREAMING-GREEDY,
constrained to pick at most 1 point per grid cell (matroid
constraint). Here r = 5, and we deleted the shaded area.

clicked vector. The AUC score of the trained classiﬁer on
the test set was 65%. We then used ROBUST-STREAMING
and SIEVE-STREAMING to ﬁnd a representative subset of
size k consisting of k/2 clicked and k/2 not-clicked ex-
amples from the training data. Due to the massive size of
the dataset, we used Spark on a cluster of 15 quad-core
machines with 32GB of memory each. We partitioned the
training data to the machines keeping its original order. We

(a) Yahoo! Webscope, r = 2

(b) Yahoo! Webscope, r = 5

Figure 4: ROBUST-STREAMING vs random unbalanced
and balanced selection and SIEVE-STREAMING selecting
equal numbers of clicked and not-clicked data points, on
45,811,883 feature vectors from Yahoo! Webscope data.
We ﬁx k = 10, 000 and delete 99% of the data points.

7. Conclusion

We have developed the ﬁrst deletion-robust streaming algo-
rithm – ROBUST-STREAMING – for constrained submod-
ular maximization. Given any single-pass streaming al-
gorithm STREAMINGALG with α-approximation guaran-
tee, ROBUST-STREAMING outputs a solution that is robust
against m deletions. The returned solution also satisﬁes an
α-approximation guarantee w.r.t. to the solution of the op-
timum centralized algorithm that knows the set of m dele-
tions in advance. We have demonstrated the effectiveness
of our approach through an extensive set of experiments.

Number of deletions20406680100120Normalized objective value0.930.9350.940.9450.950.9550.960.9650.97Robust-RGSieve-SGRobust-SGExtSieve-SGExtSieve-RGSieve-RGNumber of deletions020406080100120Normalized objective value0.650.70.750.80.850.90.951Robust-SGRobust-RGExtSieve-SGExtSieve-RGSieve-RGSieve-SGNumber of deletions020406080100120Normalized objective value0.750.7510.7520.7530.7540.7550.7560.7570.758ExtSieve-RGSieve-SGSieve-RGExtSieve-SGRobust-RGRobust-SGNumber of deletions020406080100120Normalized objective value0.740.750.760.770.780.790.80.810.82Robust-RGExtSieve-RGExtSieve-SGRobust-SGSieve-RGSieve-SGRandomRandEqualExtSieveRobustNormalized AUC improvement00.10.20.30.40.50.60.70.80.91RandomRandEqualExtSieveRobustNormalized AUC improvement00.10.20.30.40.50.60.70.80.91Deletion-Robust Submodular Maximization

Acknowledgements

This research was supported by ERC StG 307036, a Mi-
crosoft Faculty Fellowship, DARPA Young Faculty Award
(D16AP00046), Simons-Berkeley fellowship and an ETH
Fellowship. This work was done in part while Amin Kar-
basi, and Andreas Krause were visiting the Simons Institute
for the Theory of Computing.

References

Babaei, Mahmoudreza, Mirzasoleiman, Baharan, Jalili,
Mahdi, and Safari, Mohammad Ali. Revenue maximiza-
tion in social networks through discounting. Social Net-
work Analysis and Mining, 3(4):1249–1262, 2013.

Badanidiyuru, Ashwinkumar and Vondr´ak, Jan. Fast algo-
rithms for maximizing submodular functions. In SODA,
2014.

Badanidiyuru, Ashwinkumar, Mirzasoleiman, Baharan,
Karbasi, Amin, and Krause, Andreas. Streaming sub-
modular maximization: Massive data summarization on
the ﬂy. In KDD, 2014.

Buchbinder, Niv, Feldman, Moran, Naor, Joseph Sefﬁ, and
Schwartz, Roy. Submodular maximization with cardi-
nality constraints. In SODA, 2014.

Chakrabarti, Amit and Kale, Sagar. Submodular maximiza-
tion meets streaming: Matchings, matroids, and more.
IPCO, 2014.

Chekuri, Chandra, Gupta, Shalmoli, and Quanrud, Kent.
Streaming algorithms for submodular function maxi-
mization. In ICALP, 2015.

Dueck, Delbert and Frey, Brendan J. Non-metric afﬁnity
propagation for unsupervised image categorization.
In
ICCV, 2007.

guidelines.

http://data.consilium.europa.
eu/doc/document/ST-9565-2015-INIT/en/
pdf.

Jiecao, Chen, Nguyen, Huy L., and Zhang, Qin. Submod-
ular optimization over sliding windows. 2017. preprint,
https://arxiv.org/abs/1611.00129.

Kempe, David, Kleinberg, Jon, and Tardos, ´Eva. Maximiz-
ing the spread of inﬂuence through a social network. In
KDD, 2003.

Krause, Andreas and Golovin, Daniel.

Submodular
In Tractability: Practical
function maximization.
Approaches to Hard Problems. Cambridge University
Press, 2013.

Krause, Andreas, McMahon, H Brendan, Guestrin, Carlos,
and Gupta, Anupam. Robust submodular observation
selection. Journal of Machine Learning Research, 2008.

Langford, John, Li, Lihong, and Strehl, Alex. Vowpal wab-

bit online learning project, 2007.

Lin, Hui and Bilmes, Jeff. A class of submodular functions
for document summarization. In NAACL/HLT, 2011.

Mirzasoleiman, Baharan, Badanidiyuru, Ashwinkumar,
Karbasi, Amin, Vondrak, Jan, and Krause, Andreas.
Lazier than lazy greedy. In AAAI, 2015.

Mirzasoleiman, Baharan, Jegelka, Stefanie, and Krause,
Andreas. Streaming non-monotone submodular maxi-
mization: Personalized video summarization on the ﬂy.
2017. preprint, https://arxiv.org/abs/1706.
03583.

Nemhauser, George L., Wolsey, Laurence A., and Fisher,
Marshall L. An analysis of approximations for maxi-
mizing submodular set functions - I. Mathematical Pro-
gramming, 1978.

El-Arini, Khalid and Guestrin, Carlos. Beyond keyword
search: Discovering relevant scientiﬁcliterature. In KDD,
2011.

Orlin, James B, Schulz, Andreas S, and Udwani, Rajan.
Robust monotone submodular function maximization.
IPCO, 2016.

Epasto, Alessandro, Lattanzi, Silvio, Vassilvitskii, Sergei,
and Zadimoghaddam, Morteza. Submodular optimiza-
tion over sliding windows. In WWW, 2017.

Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson
Samuel Ieong. Diversifying search results. In WSDM,
2009.

Fatio, Philipe. https://refind.com/fphilipe/

topics/open-data, 2015.

Feige, Uriel. A threshold of ln n for approximating set

cover. Journal of the ACM, 1998.

Gomes, Ryan and Krause, Andreas. Budgeted nonparamet-

ric learning from data streams. In ICML, 2010.

Regulation, European Data Protection. http://ec.
europa.eu/justice/data-protection/
document/review2012/com_2012_11_en.
pdf, 2012.

Sipos, Ruben, Swaminathan, Adith, Shivaswamy, Pannaga,
and Joachims, Thorsten. Temporal corpus summariza-
tion using submodular word coverage. In CIKM, 2012.

Deletion-Robust Submodular Maximization

Tschiatschek, Sebastian, Iyer, Rishabh K, Wei, Haochen,
and Bilmes, Jeff A. Learning mixtures of submodular
functions for image collection summarization. In NIPS,
2014.

Weber, R.

The right

pandora’s box?
issues/jipitec-2-2-2011/3084, 2011.

to be forgotten: More than a
https://www.jipitec.eu/

Wei, Kai, Iyer, Rishabh, and Bilmes, Jeff. Submodularity
In ICML,

in data subset selection and active learning.
2015.

Yahoo. Yahoo! academic relations. r6a, yahoo! front page
today module user click log dataset, version 1.0, 2012.
URL http://Webscope.sandbox.yahoo.com.

