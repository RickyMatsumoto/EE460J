Spectral Learning from a Single Trajectory under Finite-State Policies

Borja Balle 1 Odalric-Ambrym Maillard 2

Abstract

We present spectral methods of moments for
learning sequential models from a single trajec-
tory, in stark contrast with the classical litera-
ture that assumes the availability of multiple i.i.d.
trajectories. Our approach leverages an efﬁcient
SVD-based learning algorithm for weighted au-
tomata and provides the ﬁrst rigorous analysis
for learning many important models using depen-
dent data. We state and analyze the algorithm un-
der three increasingly difﬁcult scenarios: proba-
bilistic automata, stochastic weighted automata,
and reactive predictive state representations con-
trolled by a ﬁnite-state policy. Our proofs in-
clude novel tools for studying mixing properties
of stochastic weighted automata.

1. Introduction

Spectral methods of moments are a powerful tool for de-
signing provably correct learning algorithms for latent vari-
able models. Successful applications of this approach in-
clude polynomial-time algorithms for learning topic mod-
els (Anandkumar et al., 2012; 2014), hidden Markov mod-
els (Hsu et al., 2012; Siddiqi et al., 2010; Anandkumar
et al., 2014), mixtures of Gaussians (Anandkumar et al.,
2014; Hsu & Kakade, 2013), predictive state representa-
tions (Boots et al., 2011; Hamilton et al., 2014; Bacon et al.,
2015; Langer et al., 2016), weighted automata (Bailly,
2011; Balle et al., 2011; Balle & Mohri, 2012; Balle et al.,
2014a;b; Glaude & Pietquin, 2016), and weighted context-
free grammars (Bailly et al., 2010; Cohen et al., 2013;
2014). All these methods can be split into two classes de-
pending on which spectral decomposition they rely on. The
ﬁrst class includes algorithms based on an Singular Value
Decomposition (SVD) decomposition of a matrix contain-

1Amazon Research, Cambridge, UK (work done at
- Nord Europe, Vil-
Lancaster University)
leneuve d’Ascq, France.
Correspondence to: Borja Balle
<pigem@amazon.co.uk>, Odalric-Ambrym Maillard <odal-
ric.maillard@inria.fr>.

2Inria Lille

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ing (estimated) moments of the target distribution (e.g. Hsu
et al. (2012); Boots et al. (2011); Balle et al. (2014a)). The
other class includes algorithms relying on symmetric ten-
sor decomposition methods (e.g. Anandkumar et al. (2014);
Hsu & Kakade (2013)). The advantage of tensor methods is
that their output is always a proper probabilistic model. On
the other hand, SVD methods, which do not always output
a probabilistic model, provide learning algorithms for mod-
els which are provably non-learnable using tensor meth-
ods. A notable example is the class of stochastic weighted
automata that do not admit a probabilistic parametrization
(Jaeger, 2000; Denis & Esposito, 2008).

language processing (NLP) and reinforcement
Natural
learning (RL) are the two main application domains of
spectral learning. For example, SVD methods for learn-
ing weighted context-free grammars have proved very suc-
cessful in language-related problems (Cohen et al., 2013;
Luque et al., 2012). In the context of RL, efﬁcient SVD
methods for learning predictive state representations were
proposed in (Boots et al., 2011; Hamilton et al., 2014). A
recent application of tensor methods to RL is given in (Az-
izzadenesheli et al., 2016), where the authors use a spectral
algorithm to obtain a PAC-RL learning result for POMDP
under memory-less policies. All these results have in com-
mon that they provide learning algorithms for models over
sequences. However, there is a fundamental difference be-
tween the nature of data in NLP and RL. With the excep-
tion of a few problems, most of NLP “safely” relies on
the assumption that i.i.d. data from the target distribution
is available. In RL, however, the most general scenario as-
sumes that the learner can only collect a single continuous
trajectory of data while all existing analysis of the SVD
method for sequential models1 rely on the i.i.d. assumption
(Hsu et al., 2012; Balle & Mohri, 2015; Glaude & Pietquin,
2016). Regarding tensor methods, (Azizzadenesheli et al.,
2016) gave the ﬁrst analysis under dependent data satisfy-
ing certain mixing conditions.

The purpose of this paper is to provide the ﬁrst rigorous
analyses of spectral SVD methods for learning sequential
models from non-i.i.d. data. We provide efﬁcient algo-
rithms with provable guarantees for learning several se-
quential models from a single trajectory. Speciﬁcally, we

1See (Thon & Jaeger, 2015) for a survey of sequential models

learnable with SVD methods.

Spectral Learning from a Single Trajectory under Finite-State Policies

consider three models: probabilistic automata, stochastic
weighted automata, and PSRs under control from a ﬁnite-
state policy. The ﬁrst two results extend existing results in
the literature for i.i.d. data (Hsu et al., 2012; Balle et al.,
2014a). The last result is an analog of the environment
learning result for POMDP – not the whole RL result –
of (Azizzadenesheli et al., 2016), with the difference that
our analysis provides guarantees under a much larger set
of policies (ﬁnite-state, as opposed to memory-less). This
result can also be interpreted as an extension of the batch-
based PSR learning algorithm from (Boots et al., 2011)
to the non-i.i.d. case, although they do not provide ﬁnite-
sample guarantees. Our analysis is especially relevant since
the single trajectory spectral algorithm we analyze has been
used previously without an explicit instantiation or analy-
sis. For example, (Kulesza et al., 2015; Shaban et al., 2015)
present experiments with datasets containing single or few
long trajectories which are broken into short subsequences
and given as input to an spectral algorithm designed for
i.i.d. data. A more detailed list of our contributions is as
follows:

(1) A single-trajectory spectral-learning algorithm for
probabilistic automata whose sample complexity de-
pends on the mixing properties of the target automaton
(Section 3).

(2) An extension of this result showing that the same algo-
rithm also learns stochastic weighted automata (Sec-
In this case the analysis is more involved,
tion 4).
and requires a novel notion of mixing for stochastic
weighted automata and tools from the theory of linear-
invariant cones (Berman & Plemmons, 1994).

(3) A generalization of the algorithm that learns reactive
PSR controlled by a ﬁnite-state stochastic policy (Sec-
tion 5). We provide for this algorithm ﬁnite-sample
bounds under a simple exploration assumption.

The most important tool in our analysis is a concentration
inequality for functions of dependent random variables.
These inequalities depend on the mixing coefﬁcients of the
underlying process. We provide technical estimates for the
relevant mixing coefﬁcients in each of the three cases listed
above. Our goal for future work is to extend (3) to prove
a PAC-RL for PSR under ﬁnite-state policies. We also
think that the tools we develop to prove (2) can be used to
improve the sample complexity of algorithms for learning
stochastic weighted automata in the i.i.d. case.

In Section 2, we start by recalling several facts about
weighted automata, spectral learning, and mixing that will
play a role in the sequel. For space reasons, most of our
proofs are deferred to the Supplementary Material.

2. Background

Let Σ be a ﬁnite alphabet, Σ(cid:63) denote the set of words of
ﬁnite length on Σ, Σω the set of all inﬁnite words on Σ, and
(cid:15) be the empty word. Given two sets of words U, V ⊂ Σ(cid:63)
we write U · V to denote the set of words {uv|u ∈ U, v ∈ V}
obtained by concatenating all words in U with all words in
V. Let P(Σω) be the set of probability distributions over
Σω. A member ρ ∈ P(Σω) is called a stochastic process
and a random inﬁnite word ξ ∼ ρ is called a trajectory.

Weighted and probabilistic automata A weighted ﬁ-
nite automaton (WFA) with n states is a tuple A =
(cid:104)α, β, {Aσ}σ∈Σ(cid:105) where α, β ∈ Rn are vectors of initial and
ﬁnal weights respectively, and Aσ ∈ Rn×n are matrices of
transition weights. A weighted automaton A computes a
function fA : Σ(cid:63) → R given by fA(w) = α(cid:62)Awβ where
Aw = Aw1 · · · Awt for w = w1 · · · wt. A WFA is minimal
if there does not exist another WFA with less states com-
puting the same function. A WFA A is stochastic if there
exists a stochastic process ρ such that for every w ∈ Σ(cid:63),
fA(w) = P[ξ ∈ wΣω]; that is, A provides a representa-
tion for the probabilities of preﬁxes under the distribution
of ρ. A weighted automaton is irreducible if the labelled
directed graph with n vertices obtained by adding a tran-
sition from i to j with label σ whenever Aσ(i, j) (cid:54)= 0 is
strongly connected. It can be shown that irreducibility im-
plies minimality, and that almost all WFA are irreducible in
the sense that the set of irreducible WFA are dense on the
set of all WFA (Balle et al., 2017).
A probabilistic ﬁnite automaton (PFA) is a stochastic WFA
A = (cid:104)α, β, {Aσ}(cid:105) where the weights have a probabilistic
interpretation. Namely, α is a probability distribution over
[n], Aσ(i, j) is the probability of emitting symbol σ and
transitioning to state j starting from state i, and β(i) = 1
for all i ∈ [n]. It is immediate to check that a PFA satisfy-
ing these conditions induces a stochastic process. However
not all stochastic WFA admit an equivalent PFA (Jaeger,
2000; Denis & Esposito, 2008).
If A is a PFA, then the matrix A = (cid:80)
σ∈Σ Aσ yields the
Markov kernel A(i, j) = P[j | i] on the state space [n] after
marginalizing over the observations. It is easily checked
that A is row-stochastic, and thus Aβ = β. Furthermore,
for every distribution α0 ∈ Rn over [n] we have α(cid:62)
0 A = α1
for some other probability distribution α1 over [n]. In the
case of PFA irreducibility coincides with the usual concept
of irreducibility of the Markov chain induced by A.

Hankel matrices and spectral learning The Hankel ma-
trix of a function f : Σ(cid:63) → R is the inﬁnite matrix
Hf ∈ RΣ(cid:63)×Σ(cid:63)
with entries Hf (u, v) = f (uv). Given ﬁ-
nite sets U, V ⊂ Σ(cid:63), H U ,V
f ∈ RU ×V denotes the restriction
of matrix Hf to preﬁxes in U and sufﬁxes in V.
Fliess’ theorem (Fliess, 1974) states that a Hankel matrix

Spectral Learning from a Single Trajectory under Finite-State Policies

Hf has ﬁnite rank n if and only if there exists a WFA A
with n states such that f = fA. This implies that a WFA A
with n states is minimal if and only if n = rank(HfA ). The
spectral learning algorithm for WFA (Balle et al., 2014a)
provides a mechanism for recovering such a WFA from
a ﬁnite sub-block H U ,V
of Hf such that: 1) (cid:15) ∈ U ∩ V,
2) there exists a set U (cid:48) such that U = U (cid:48) ∪ (∪σ∈ΣU (cid:48)σ),
3) rank(Hf ) = rank(H U (cid:48),V
). A pair (U, V) that satis-
ﬁes these conditions is called a complete basis for f . The
pseudo-code of this algorithm is given below:

f

f

Algorithm 1: Spectral Learning for WFA

Input: number of states n, Hankel matrix H U ,V
Find U (cid:48) such that U = U (cid:48) ∪ (∪σ∈ΣU (cid:48)σ)
Let H(cid:15) = H U (cid:48),V
Compute the rank n SVD H(cid:15) ≈ U DV (cid:62)
Let hV = H {(cid:15)},V and take α = V (cid:62)hV
Let hU (cid:48) = H U (cid:48),{(cid:15)} and take β = D−1U (cid:62)hU (cid:48)
foreach σ ∈ Σ do

Let Hσ = H U (cid:48)σ,V and take Aσ = D−1U (cid:62)HσV

return A = (cid:104)α, β, {Aσ}(cid:105)

The main strength of Algorithm 1 is its robustness to noise.
Speciﬁcally, if only an approximation (cid:98)H U ,V of the Han-
kel matrix is known, then the error between the target au-
tomaton A and the automaton (cid:98)A learned from (cid:98)H U ,V can
be controlled in terms of the error (cid:107)H U ,V − (cid:98)H U ,V (cid:107)2; see
(Hsu et al., 2012) for a proof in the HMM case and (Balle,
2013) for a proof in the general WFA case. These tedious
but now standard arguments readily reduce the problem of
learning WFA via spectral learning to that of estimating the
corresponding Hankel matrix.

Classical applications of spectral learning assume one has
access to i.i.d. samples from a stochastic process ρ. In this
setting one can obtain a sample S = (ξ(1), . . . , ξ(N )) con-
taining N ﬁnite-length trajectories from ρ, and use them to
estimate a Hankel matrix (cid:98)H U ,V

as follows:

S

(cid:98)H U ,V
S

(u, v) =

I{ξ(i) ∈ uvΣω} .

1
N

N
(cid:88)

i=1

S

] = H U ,V

If ρ = ρA for some stochastic WFA, then obviously
ES[ (cid:98)H U ,V
and a large sample size N will pro-
fA
vide a good approximation (cid:98)H U ,V
of H U ,V
. Explicit con-
centration bounds for Hankel matrices bounding the error
(cid:107)H U ,V

(cid:107)2 can be found in (Denis et al., 2016).

fA − (cid:98)H U ,V

fA

S

S

In this paper we consider the more challenging setup where
we only have access to a sample S = {ξ} of size N = 1
from ρ. In particular, we show it is possible to replace the
empirical average above by a C´esaro average and still use
the spectral learning algorithm to recover the transition ma-

trices of a stochastic WFA. To obtain a ﬁnite-sample anal-
ysis of this single-trajectory learning algorithm we prove
concentration results for C´esaro averages of Hankel matri-
ces. Our analysis relies on concentration inequalities for
functions of dependent random variables which depend on
mixing properties of the underlying process.

Mixing and concentration Let ρ ∈ P(Σω) be a stochastic
process and ξ = x1x2 · · · a random word drawn from ρ.
For 1 (cid:54) s < t (cid:54) T and u ∈ Σs we let ρt:T (·|u) denote
the distribution of xt · · · xT conditioned on x1 · · · xs = u.
With this notation we deﬁne the quantity

ηt(u, σ, σ(cid:48)) = (cid:107)ρt:T (·|uσ) − ρt:T (·|uσ(cid:48))(cid:107)T V
for any u ∈ Σs−1, and σ, σ(cid:48) ∈ Σ. Then the η-mixing
coefﬁcients of ρ at horizon T are given by

ηs,t =

sup
u∈Σs−1,σ,σ(cid:48)∈Σ

ηt(u, σ, σ(cid:48)) .

Mixing coefﬁcients are useful in establishing concentration
properties of functions of dependent random variables. The
Lipschitz constant of a function g : ΣT → R with respect
to the Hamming distance is deﬁned as

(cid:107)g(cid:107)Lip = sup |g(w) − g(w(cid:48))| ,

where the supremum is taken over all pairs of words
w, w(cid:48) ∈ ΣT differing in exactly one symbol. The follow-
ing theorem proved in (Chazottes et al., 2007; Kontorovich
et al., 2008) provides a concentration inequality for Lips-
chitz functions of weakly dependent random variables.

Theorem 1 Let ρ ∈ P(Σω) and ξ = x1x2 · · · ∼ ρ. Suppose
g : ΣT → R satisﬁes (cid:107)g(cid:107)Lip (cid:54) 1 and let Z = g(x1, . . . , xT ).
(cid:80)T
Let
t=s+1 ηs,t, where ηs,t are
the η-mixing coefﬁcients of ρ at horizon T . Then the fol-
lowing holds for any ε > 0:

ηρ = 1+max1<s<T

Pξ [Z − EZ > εT ] (cid:54) exp

(cid:19)

(cid:18) −2ε2T
η2
ρ

,

with an identical bound for the other tail.

Theorem 1 shows that the mixing coefﬁcient ηρ is a key
quantity in order to control the concentration of a func-
In fact, upper-bounding ηρ
tion of dependent variables.
in terms of geometric ergodicity coefﬁcients of a latent
variable stochastic process enables (Kontorovich & Weiss,
2014) to analyze the concentration of functions of HMMs
and (Azizzadenesheli et al., 2016) to provide PAC guar-
antees for an RL algorithm for POMDP based on spectral
tensor decompositions. Our Lemma 2 uses a similar but
more reﬁned bounding strategy that directly applies when
the transition and observation processes are not condition-
ally independent. Lemma 4 reﬁnes this strategy further to
control ηρ for stochastic WFA (for which there may be no
underlying Markov stochastic process in general). To the
best of our knowledge this yields the ﬁrst concentration re-
sults for the challenging setting of stochastic WFA.

Spectral Learning from a Single Trajectory under Finite-State Policies

3. Single-Trajectory Spectral Learning of PFA

In this section we focus on the problem of learning the tran-
sition structure of a PFA A using single trajectory is gen-
erated by A. We provide a spectral learning algorithm for
this problem and a ﬁnite-sample analysis consisting of a
concentration bound for the error on the Hankel matrix es-
timated by the algorithm. We assume the learner has access
to a single inﬁnite-length trajectory ξ ∼ ρA that is progres-
sively uncovered. The algorithm uses a length t preﬁx from
ξ to estimate a Hankel matrix whose entries are C´esaro av-
erages. This Hankel matrix is then processed by the usual
spectral learning algorithm to recover an approximation to
an automaton with transition weights equivalent to those of
A. We want to analyze the quality of the model learned by
the algorithm after observing the ﬁrst t symbols from ξ.

We start by showing that C´esaro averages provide a consis-
tent mechanism for learning the transition structure of A.
Then we proceed to analyze the accuracy of the Hankel es-
timation step. As discussed in Section 2, this is enough to
obtain ﬁnite-sample bounds for learning PFA. The general
case of stochastic WFA is considered in Section 4.

3.1. Learning with C´esaro Averages is Consistent
Let A = (cid:104)α, β, {Aσ}(cid:105) be a PFA computing a function fA :
Σ(cid:63) → R and deﬁning a stochastic process ρA ∈ P(Σω). For
convenience we drop the subscript and just write f and ρ.
Since we only have access to a single trajectory ξ from ρ we
cannot obtain an approximation of the Hankel matrix for f
by averaging over multiple i.i.d. trajectories. Instead, we
compute C´esaro averages over the trajectory ξ to obtain a
Hankel matrix whose expectation is related to A as follows.
For any t ∈ N let ¯ft
: Σ(cid:63) → R be the function
given by ¯ft(w) = (1/t) (cid:80)t−1
s=0 f (Σsw), where f (Σsw) =
(cid:80)
u∈Σs f (uw). We shall sometimes write fs(w) =
f (Σsw). Using the deﬁnition of the function computed by
a WFA it is easy to see that

(cid:88)

f (uw) =

(cid:88)

α(cid:62)AuAwβ = α(cid:62)AsAwβ ,

u∈Σs

u∈Σs
where A = (cid:80)
σ Aσ is the Markov kernel on the state space
t = (1/t) (cid:80)t−1
s=0 α(cid:62)As we get
of A. Thus, introducing ¯α(cid:62)
¯ft(w) = ¯α(cid:62)
t Awβ. Since α is a probability distribution, A
is a Markov kernel, and probability distributions are closed
by convex combinations, then ¯αt is also a probability dis-
tribution over [n]. Thus, we have just proved the following:

Lemma 1 (Consistency) The C´esaro average of f over
t steps, ¯ft, is computed by the probabilistic automaton
¯At = (cid:104)¯αt, β, {Aσ}(cid:105). In particular, A and ¯At have the same
number of states and the same transition probability matri-
ces. Furthermore, if A is irreducible then ¯At is minimal.

The irreducibility claim follows from (Balle et al., 2017).

For convenience, in the sequel we write ¯H U ,V
(U, V)-block of the Hankel matrix Hf¯At
Remark 1 The irreducible condition simply ensures there
is a unique stationary distribution, and that the Hankel ma-
trix of ¯At has the same rank as the Hankel matrix of A
(otherwise it could be smaller).

for the

.

t

3.2. Spectral Learning Algorithm

Algorithm 2 describes the estimation of the empirical Han-
kel matrix (cid:98)H U ,V
from the ﬁrst t+L symbols of a single tra-
t,ξ
jectory using the corresponding C´esaro averages. To avoid
cumbersome notations, in the sequel we may drop super
and subscripts when not needed and write (cid:98)Ht or (cid:98)H when U,
V, and ξ are clear from the context. Note that by Lemma 1
the expectation E[ (cid:98)H] over ξ ∼ ρ is equal to the Hankel ma-
trix ¯Ht of the function ¯ft computed by the PFA ¯At.

Algorithm 2: Single Trajectory Spectral Learning
(Generative Case)

Input: number of states n, length t, preﬁxes

U ⊂ Σ(cid:63), sufﬁxes V ⊂ Σ(cid:63)

Let L = maxw∈U ·V |w|
Sample trajectory ξ = x1x2 · · · xt+L · · · ∼ ρ
foreach u ∈ U and v ∈ V do

Let (cid:98)H(u, v) = 1
t

(cid:80)t−1
s=0

I{xs+1:s+|uv| = uv}

Apply the spectral algorithm to (cid:98)H with rank n

3.3. Concentration Results
Now we proceed to analyze the error (cid:98)Ht − ¯Ht in the Han-
kel matrix estimation inside Algorithm 2. In particular, we
provide concentration bounds that depend on the length t,
the mixing coefﬁcient ηρ of the process ρ, and the struc-
ture of the basis (U, V). The main result of this section is
the matrix-wise concentration bound Theorem 3 where we
control the spectral norm of the error matrix. For compari-
son we also provide a simpler entry-wise bound and recall
the equivalent matrix-wise bound in the i.i.d. setting.

Before trying to bound the concentration of the errors us-
ing Theorem 1 we need to analyze the mixing coefﬁcient
of the process generated by a PFA. This is the goal of the
following result, whose proof is provided in Appendix A.

Lemma 2 (η-mixing for PFA) Let A be PFA and assume
that it is (C, θ)-geometrically mixing in the sense that for
some constants C > 0, θ ∈ (0, 1) we have

∀t ∈ N, µA

t = sup
α,α(cid:48)

(cid:107)αAt − α(cid:48)At(cid:107)1
(cid:107)α − α(cid:48)(cid:107)1

(cid:54) Cθt ,

where the supremum is over all probability vectors. Then
we have ηρA

(cid:54) C/(θ(1 − θ)).

Spectral Learning from a Single Trajectory under Finite-State Policies

Remark 2 A sufﬁcient condition for the geometric con-
trol of µA
t is that A admits a spectral gap. In this case θ
can be chosen to be the modulus of the second eigenvalue
|λ2(A)| < 1 of the transition kernel A.

Before the main result of this section we provide a con-
centration result for each individual entry of the estimated
Hankel matrix as a warmup (see Appendix D).

Theorem 2 (Single-trajectory, entry-wise) Let A be a
(C, θ)-geometrically mixing PFA and ξ ∼ ρA a trajectory
of observations. Then for any u ∈ U, v ∈ V and δ ∈ (0, 1),

(cid:34)
(cid:98)H U ,V

P

t,ξ (u, v)− ¯H U ,V

t

(u, v) (cid:62)

(cid:114)

(cid:16)

|uv|C
θ(1 − θ)

1 +

|uv| − 1
t

(cid:17) log(1/δ)
2t

(cid:35)

(cid:54) δ ,

with an identical bound for the other tail.

A naive way to handle the concentration of the whole Han-
kel matrix is to control the Frobenius norm (cid:107) (cid:98)Ht − ¯Ht(cid:107)F
by taking a union bound over all entries using Theorem 2.
However, the resulting concentration bound would scale as
(cid:112)|U||V|. To have better dependency with the dimension
(the matrix has dimension |U| × |V|) can split the empiri-
cal Hankel matrix (cid:98)H into blocks containing strings of the
same length (as suggested by the dependence of the bound
above on |uv|). We thus introduce the maximal length
L = maxw∈U ·V |w|, and the set U(cid:96) = {u ∈ U : |u| = (cid:96)}
for any (cid:96) ∈ N. We use these to deﬁne the quantity
nU = |{(cid:96) ∈ [0, L] : |U(cid:96)| > 0}|, and introduce likewise
V(cid:96), nV with obvious deﬁnitions. With this notation we can
now state the main result of this section.

Theorem 3 (Single-trajectory, matrix-wise) Let A be as
in Theorem 2. Let m = (cid:80)
¯ft(uv) be the proba-
bility mass and d = min{|U||V|, 2nU nV } be the effective
dimension. Then, for all δ ∈ (0, 1) we have

u∈U ,v∈V

(cid:34)

P

(cid:107) (cid:98)H U ,V

t,ξ − ¯H U ,V

t

(cid:107)2 (cid:62)

(cid:32)√

L+

(cid:114)

(cid:16)

+

2LC
θ(1−θ)

1+

L−1
t

(cid:17) d ln(1/δ)
2t

(cid:33)(cid:114) 2m
t

(cid:114) 2C
1 − θ
(cid:35)

(cid:54) δ .

Theorem 4 (Theorem 7 in (Denis et al., 2014)) Let A be
a stochastic WFA with stopping probabilities and S =
(ξ(1), . . . , ξ(N )) be an i.i.d. sample of size N from the dis-
tribution ρA ∈ P(Σ(cid:63)). Let m = (cid:80)
u∈U ,v∈V fA(uv). Then,
for all c > 0 we have

(cid:34)

P

(cid:107) (cid:98)H U ,V

S − H U ,V

fA (cid:107)2 >

(cid:35)

(cid:114) 2cm
N

+

2c
3N

(cid:54)

2c
ec − c − 1

.

3.4. Sketch of the Proof of Theorem 3

In this section we sketch the main steps of the proof of
Theorem 3 (the full proof is given in Appendices A and D).
We focus on highlighting the main difﬁculties and paving
the path for the extension of Theorem 3 to stochastic WFA
given in Section 4.

t

t,ξ − ¯H U ,V

The key of the proof is to study the function g(ξ) =
(cid:107) (cid:98)H U ,V
(cid:107)2, in view of applying Theorem 1. To
this end, we ﬁrst control the η-mixing coefﬁcients using
Lemma 2. The next step is to control the Lipschitz con-
stant (cid:107)g(cid:107)Lip. This part is not very difﬁcult and we derive
after a few careful steps the bound (cid:107)g(cid:107)Lip (cid:54) L

d/t.

√

The second and most interesting part of the proof is about
the control of E[g(ξ)]. Let us give some more details.
Decomposition step We control (cid:107) (cid:98)H U ,V
Frobenius norm and get

t − ¯H U ,V

(cid:107)2 by its

t

E[(cid:107) (cid:98)H U ,V

t − ¯H U ,V

t

(cid:107)2]2 (cid:54) (cid:88)

|w|U ,V E[( (cid:98)ft(w)− ¯ft(w))2] ,

w∈U ·V

where we introduced |w|U ,V = |{(u, v) ∈ U × V :
(cid:80)t
uv = w}|, and (cid:98)ft(w) = 1
s=1bs(w) using the short-
t
hand notation bs(w) = I{xs . . . xs+|w|−1 = w}. Also,
(cid:80)t
¯ft(w) = E[ (cid:98)ft(w)] = 1
s=1 fs(w), where fs(w) =
ρA(Σs−1wΣω). This implies that we have a sum of vari-
ances, where each of the terms can be written as

t

E[( (cid:98)ft(w) − ¯ft(w))2] =

(cid:32) t

(cid:33)2

1
t2

E



(cid:88)

s=1

bs(w)

 −

1
t2

(cid:32) t

(cid:88)

s=1

(cid:33)2

fs(w)

.

Remark 3 Note that quantity nU nV in d can be exponen-
tially smaller than |U||V|. Indeed, for U = V = Σ(cid:54)L/2 we
have |U||V| = Θ(|Σ|L) while nU nV = Θ(L2).

For comparison, we recall a state-of-the-art concentration
bound for estimating the Hankel matrix of a stochastic lan-
guage2 from N i.i.d. trajectories.

2A stochastic language is a probability distribution over Σ(cid:63).

Slicing step An important observation is that each prob-
ability term satisﬁes fs(w) = α(cid:62)As−1Awβ because of
the PFA assumption on ρA. Furthermore, it follows from
A being a PFA that (cid:80)
|w|=l fs(w) = 1 for all s and l.
This suggests that we group the terms in the sum over
W = U · V by length, so we write Wl = W ∩ Σl and deﬁne
Ll = maxw∈Wl |w|U ,V the maximum number of ways to
write a string of length l in W as a concatenation of a preﬁx
in U and a sufﬁx in V. Note that we always have Ll (cid:54) l+1.

Spectral Learning from a Single Trajectory under Finite-State Policies

A few more steps lead to the following bound

4. Extension to Stochastic WFA

(cid:107)2]2 (cid:54)

(cid:20) t

(cid:88)

s=1

t

t − ¯H U ,V
E[(cid:107) (cid:98)H U ,V
∞
1
(cid:88)
t2

(cid:88)

l=0

w∈Wl

(cid:88)

(cid:16)

+2

1(cid:54)s<s(cid:48)(cid:54)t

|w|U ,V

(1−fs(w))fs(w)

E[bs(w)bs(cid:48)(w)]−fs(w)fs(cid:48)(w)

(cid:17)(cid:21)

.

(1)

We control the ﬁrst term in (1) using

∞
(cid:88)

(cid:88)

l=0

w∈Wl

t
(cid:88)

s=1

|w|U ,V

(1−fs(w))fs(w) (cid:54) t

(cid:88)

¯ft(uv) .

u∈U ,v∈V

Cross terms Regarding the remaining “cross”-term in (1)
we ﬁx w ∈ Wl and obtain the equation

E[bs(w)bs(cid:48)(w)] − fs(w)fs(cid:48)(w)
As(cid:48)−s
w − Awβα(cid:62)

= α(cid:62)

s−1

(cid:16)

(cid:17)

s(cid:48)−1Aw

β ,

(2)

w

x∈Σs(cid:48)−s

w = (cid:80)

s = α(cid:62)As and transition
Ax corresponding to the “event”

where we introduced the vectors α(cid:62)
matrix As(cid:48)−s
w = wΣs(cid:48)−s ∩ Σs(cid:48)−sw. We now discuss two cases.
Σs(cid:48)−s
First control If s(cid:48) − s < l, we use the simplifying fact that
w ⊂ wΣs(cid:48)−s to upper bound (2) by
Σs(cid:48)−s
(cid:16)

(cid:17)

α(cid:62)

As(cid:48)−s − βα(cid:62)

s−1Aw
= fs(w)(1 − fs(cid:48)(w)) (cid:54) fs(w) .

s(cid:48)−1Aw

β

Second control When s(cid:48) − s (cid:62) |w| = l we have Σs(cid:48)−s
wΣs(cid:48)−s−lw and As(cid:48)−s
(2) and bound it using H¨older’s inequality as follows:

w =
w = AwAs(cid:48)−s−lAw. Thus, we rewrite

α(cid:62)

s−1Aw

As(cid:48)−s−l −βα(cid:62)

s(cid:48)−1

Awβ (cid:54)

(cid:17)

(cid:16)

(3)

(cid:107)α(cid:62)

s−1Aw(cid:107)1(cid:107)As(cid:48)−s−l − βα(cid:62)

s(cid:48)−1(cid:107)∞(cid:107)Awβ(cid:107)∞ .

Using Lemma 6 in Appendix A we bound the induced norm
as (cid:107)As(cid:48)−s−l − βα(cid:62)
is the
mixing coefﬁcient deﬁned in Lemma 2. Also, it holds that
(cid:107)Awβ(cid:107)∞ (cid:54) 1. Finally, since α(cid:62)
s−1Aw is a sub-distribution
over states, we have the key equalities

s(cid:48)−s−l, where µA

s(cid:48)−1(cid:107)∞ (cid:54) 2µA

t

|w|U ,V (cid:107)α(cid:62)

s−1Aw(cid:107)1 =

|w|U ,V α(cid:62)

s−1Awβ (4)

|w|U ,V fs(w) =

fs(uv) .

u∈U ,v∈V:uv∈Wl

The proof is concluded by collecting the previous bounds,
plugging them into (1), and using Lemma 2 to get

(cid:18)

E[g(ξ)]2 (cid:54)

2L − 1 +

4C
1 − θ

(cid:19) m
t

.

(5)

(cid:88)

w∈Wl

(cid:88)

=

w∈Wl

(cid:88)

w∈Wl
(cid:88)

We now generalize the results in previous section to the
case where the distribution over ξ is generated by a stochas-
tic weighted automaton that might not have a probabilistic
representation. The key observation is that Algorithm 2
can learn stochastic WFA without any change, and the con-
sistency result in Lemma 1 extends verbatim to stochastic
WFA. However, the proof of the concentration bound in
Theorem 3 requires further insights into the mixing proper-
ties of stochastic WFA. Before describing the changes re-
quired in the proof, we discuss some important geometric
properties of stochastic WFA.

4.1. The State-Space Geometry of SWFA
Recall that a stochastic WFA (SWFA) A = (cid:104)α, β, {Aσ}(cid:105)
deﬁnes a stochastic process ρA and computes a function
fA such that fA(w) = P[ξ ∈ wΣω], where ξ ∼ ρA. It is
immediate to check that this implies that the weights of A
satisfy the properties: (i) α(cid:62)Axβ (cid:62) 0 for all x ∈ Σ(cid:63), and
(ii) α(cid:62)Atβ = (cid:80)
|w|=t α(cid:62)Awβ = 1 for all t (cid:62) 0, where
A = (cid:80)
σ∈Σ Aσ. Without loss of generality we assume
throughout this section that A is a minimal SWFA of di-
mension n, meaning that any SWFA computing the same
probability distribution than A must have dimension at least
Importantly, the weights in α, β, and Aσ are not re-
n.
quired to be non-negative in this deﬁnition. Nonetheless,
it follows from these properties that β is an eigenvector of
A of eigenvalue 1 exactly like in the case of PFA. We now
introduce further facts about the geometry of SWFA.
A minimal SWFA A is naturally associated with a proper
(i.e. pointed, closed, and solid) cone in K ⊂ Rn called the
backward cone (Jaeger, 2000), and characterized by the fol-
lowing properties: 1) β ∈ K, 2) AσK ⊆ K for all σ ∈ Σ, and
3) α(cid:62)v (cid:62) 0 for all v ∈ K. Condition 2) says that every tran-
sition matrix Aσ leaves K invariant, and in particular the
backward vector Awβ belongs to K for all w ∈ Σ(cid:63).
The vector of ﬁnal weights β plays a singular role in the
geometry of the state space of a SWFA. This follows from
facts about the theory of invariant cones (Berman & Plem-
mons, 1994) which provides a generalization of the clas-
sical Perron–Frobenius theory of non-negative matrices to
arbitrary matrices. We recall from (Berman & Plemmons,
1994) that a norm on Rn can be associated with every vec-
tor in the interior of K. In particular, we will take the norm
associated with the ﬁnal weights β ∈ K. This norm, de-
noted by (cid:107) · (cid:107)β, is completely determined by its unit ball
Bβ = {v ∈ Rn : −β (cid:54)K v (cid:54)K β}, where u (cid:54)K v means
v − u ∈ K. In particular, (cid:107)v(cid:107)β = inf{r (cid:62) 0 : v ∈ rBβ}.
Induced and dual norms are derived from (cid:107) · (cid:107)β as usual.
When A is a PFA one can take K to be the cone of vectors in
Rn with non-negative entries, in which case β = (1, . . . , 1)
and (cid:107) · (cid:107)β reduces to (cid:107) · (cid:107)∞ (Berman & Plemmons, 1994).
The following result shows that (cid:107) · (cid:107)β provides the right

Spectral Learning from a Single Trajectory under Finite-State Policies

generalization to SWFA of the norm (cid:107) · (cid:107)∞ used in the Sec-
ond control step of the proof for PFA (see Appendix B).

Lemma 3 For any w ∈ Σ(cid:63): (i) (cid:107)Awβ(cid:107)β (cid:54) 1, and (ii)
(cid:107)α(cid:62)Aw(cid:107)β,∗ = α(cid:62)Awβ.

It is also natural to consider mixing coefﬁcients for stochas-
tic processes generated by SWFA in terms of the dual β-
norm. This provides a direct analog to Lemma 2 for PFA:

Lemma 4 (η-mixing for SWFA) Let A be SWFA and as-
sume that it is (C, θ)-geometrically mixing in the sense that
for some C (cid:62) 0, θ ∈ (0, 1),

µA
t =

sup
0 β=α(cid:62)

1 β=1

α0,α1:α(cid:62)

(cid:107)α(cid:62)

1 At(cid:107)β,(cid:63)

0 At − α(cid:62)
(cid:107)α0 − α1(cid:107)β,(cid:63)

(cid:54) Cθt .

Then the η-mixing coefﬁcient satisﬁes ηρA

(cid:54) C/θ(1 − θ).

is that A admits a spectral gap.

Remark 4 A sufﬁcient condition for the geometric con-
trol of µA
In this case
t
θ can be chosen to be the modulus of the second eigen-
value |λ2(A)| < 1 of A. Another sufﬁcient condition is that
θ = γβ(A) < 1, where

γβ(A) = sup

: ν s.t. ||ν||β,(cid:63) (cid:54)= 0, ν(cid:62)β = 0

.

(cid:27)

(cid:26) ||Aν||β,(cid:63)
||ν||β,(cid:63)

4.2. Concentration of Hankel Matrices for SWFA

We are now ready to extend the proof of Theorem 3 to
SWFA. Using that both PFA and SWFA deﬁne probabil-
ity distributions over preﬁxes it follows that any argument
in Section 3.4 that only appeals to the function computed
by the automaton can remain unchanged. Therefore, the
only arguments that need to be revisited are described in
the Second control step.
In particular, we must provide
versions of (3) and (4) for SWFA.

Recalling that H¨older’s inequality can be applied with
any pair of dual norms, we start by replacing the norms
(cid:107) · (cid:107)∞ and (cid:107) · (cid:107)1 in (3) with the cone-norms (cid:107) · (cid:107)β and
(cid:107) · (cid:107)β,(cid:63) respectively. Next we use Lemma 3 to obtain,
for any w ∈ Σ(cid:63), the bound (cid:107)Awβ(cid:107)β (cid:54) 1 and the equa-
tion (cid:107)α(cid:62)Aw(cid:107)β,∗ = α(cid:62)Awβ which are direct analogs
of the results used for PFA. Then it only remains to re-
late the β-norm of As(cid:48)−s−l − βα(cid:62)
s(cid:48)−1 to the mixing co-
efﬁcients µA
t . Applying Lemma 8 in Appendix A yields
(cid:107)As(cid:48)−s−l − βα(cid:62)
s(cid:48)−s−l. Thus we obtain for
SWFA exactly the same concentration result that we ob-
tained for empirical Hankel matrices estimated from a sin-
gle trajectory of observations generated by a PFA.

s(cid:48)−1(cid:107)β (cid:54) 2µA

Theorem 5 (Single-trajectory, SWFA) Let A be a SWFA
that is (C, θ)-geometrically mixing with the deﬁnition in
Lemma 4. Then the concentration bound in Theorem 3 also
holds for trajectories ξ ∼ ρA.

5. The Controlled Case
This section describes the ﬁnal contribution of the paper:
a generalization of our analysis of spectral learning from
a single trajectory the case of dynamical systems under
ﬁnite-state control. We consider discrete-time dynamical
systems with ﬁnite set of observations O and ﬁnite set of
actions A, and let Σ = O ×A. We assume the learner has
access to a single trajectory ξ = (ot, at)t(cid:62)1 in Σω. The tra-
jectory is generated by coupling an environment deﬁning a
distribution over observations conditioned on actions and
a policy deﬁning a distribution over actions conditioned
on observations. Assuming the joint action-observation
distribution can be represented by a stochastic WFA is
equivalent to saying that the environment corresponds to
a POMDP or PSR, and the policy has ﬁnite memory.
To ﬁx some notation we assume the environment is repre-
sented by a conditional3 stochastic WFA A = (cid:104)α, β, {Aσ}(cid:105)
This implies the semantics fA(w) =
with n states.
P[o1 · · · ot|a1 · · · at] for the function computed by A,
where w = w1 · · · wt with wi = (oi, au). For any w ∈ Σ(cid:63)
we shall write wA = a1 · · · at and wO = o1 · · · ot. We also
assume there is a stochastic policy π represented by a con-
ditional PFA Aπ = (cid:104)απ, βπ, {Πσ}(cid:105) with k states; that is,
fAπ (w) = π(wA|wO) = P[a1 · · · at|o1 · · · ot]. In partic-
ular, Aπ represents a stochastic policy that starts in a state
s1 ∈ [k] sampled according to απ(i) = P[s1 = i], and at
each time step samples an action and changes state accord-
ing to Πo,a(i, j) = P[st+1 = j, at = a|ot = o, st = i].
The trajectory ξ observed by the learner is generated by
the stochastic process ρ ∈ P(Σω) obtained by coupling
A and Aπ. A standard construction in the theory of
weighted automata (Berstel & Reutenauer, 1988) shows
that this process can be computed by the product automa-
ton B = A ⊗ Aπ = (cid:104)α⊗, β⊗, {Bσ}(cid:105), where α⊗ = α ⊗ απ,
β⊗ = β ⊗ βπ, and Bo,a = Ao,a ⊗ Πo,a. It is easy to verify
that B is a stochastic WFA with nk states computing the
function fB(w) = fA(w)fAπ (w) = P[ξ ∈ wΣω].
At this point, the spectral algorithm from Section 4 could
be used to learn B directly from a trajectory ξ ∼ ρB. How-
ever, since the agent interacting with environment A knows
the policy π, we would like to leverage this information to
learn directly a model of the environment. This approach
is formalized in Algorithm 3, which provides a single-
trajectory version of the algorithm in (Bowling et al., 2006)
for learning PSR from non-blind policies with i.i.d. data.
The main difference with Algorithm 2 is that in the reactive
case we need a smoothing parameter κ that will prevent the
entries in the empirical Hankel matrix (cid:98)H to grow unbound-
edly plus that the policy π satisﬁes an exploration assump-
tion. κ plays a similar role in our analysis as the smoothing
parameter introduced in (Denis et al., 2014) for learning

3Such WFA are also called reactive predictive state represen-

tations in the RL literature.

Spectral Learning from a Single Trajectory under Finite-State Policies

Algorithm 3: Single Trajectory Spectral Learning
(Reactive Case)

Input: number of states n, length t, U,V ⊂ (A×O)(cid:63),

policy π, smoothing coefﬁcient κ

Let L = maxw∈U ·V |w|
Sample trajectory o1a1o2a2 · · · ot+Lat+L using π
foreach u ∈ U and v ∈ V do

Let (cid:98)H(u, v) =
(cid:80)t−1
s=0

1
t

I{os+1as+1···os+|uv|as+|uv|=uv}
κsπ(a1···as+|uv||o1···os+|uv|)

Apply the spectral algorithm to (cid:98)H with rank n

stochastic languages from factor estimates in the i.i.d. case.
The difference is that in our case the smoothing parameter
must satisfy κε > 1, where ε is the exploration probability
of the policy π provided by the following assumption.

Assumption 1 (Exploration) There exists some ε > 0
for each w ∈ Σ(cid:63) the policy π satisﬁes
such that
π(wA|wO) (cid:62) ε|w|. In particular, at every time step each
action a ∈ A is picked with probability at least ε.

Before moving to the next section, where we provide ﬁnite-
sample concentration results for the Hankel matrix esti-
mated by Algorithm 3, we show that Algorithm 3 is consis-
tent, that is it learns in expectation a WFA whose transition
matrices are equivalent to those of the environment A. The
proof of the following lemma is provided in Appendix E.
Lemma 5 The Hankel matrix (cid:98)H = (cid:98)H U ,V
t,ξ computed in Al-
, where ˜H U ,V
gorithm 3 satisﬁes E[ (cid:98)H U ,V
is
a block of the Hankel matrix corresponding to the stochas-
tic WFA ˜At = (cid:104)˜αt, β, {Aσ}(cid:105) where we introduced the modi-
˜αt = (1/t) (cid:80)t−1
s=0 α(cid:62)(A/κ)s. We denote by
ﬁed vector
˜ft the function computed by ˜At.

t,ξ ] = ˜H U ,V

t

t

5.1. Concentration Results

t

t,ξ − ˜H U ,V

Broadly speaking, a concentration bound for the estima-
tion error (cid:107) (cid:98)H U ,V
(cid:107)2 can be obtained by following a
proof strategy similar to the ones used in Theorems 3 and 5.
However, almost all the bounds used in the previous proofs
need to be reworked to account for (i) the effect of the extra
dependencies introduced by the policy π, and (ii) the fact
that the target automaton A to be learned is not a stochas-
tic WFA in the sense of Section 4 but rather a conditional
stochastic WFA.

Point (i) is addressed in our proof by introducing a “nor-
malized” reference process ρ¯A corresponding to the cou-
pling ¯A = A ⊗ Aunif between the environment A and
the uniform random policy that at each step takes each ac-
tion independently with probability 1/|A|. Assuming the
smoothing parameter satisﬁes κε > 1 for some exploration

t

parameter ε (cf. Assumption 1), then 1/κ (cid:54) 1/|A|. This
observation is used, for example, to bound some variance
terms in E[g(ξ)] by replace occurrences of ˜ft with ˜f unif
,
the function computed by taking the C´esaro average of the
ﬁrst t steps of ¯A. Ultimately, this makes our bound depend
not only on the mixing properties of ρB, but also on those
of the normalized process ρ¯A induced by the SWFA ¯A. In-
cidentally, this argument is also used to address point (ii):
by bounding quantities involving κ by quantities computed
by a SWFA we can use again the arguments sketched in
Section 4.1.
Pursuing the ideas above, and assuming that ¯A is ( ¯C, ¯θ)-
geometrically mixing, we obtain the following bound
which can be compared to the one in (5):

E[g(ξ)]2 (cid:54)

˜m
tεL(1 − 1/(κε)2)

+

2 ¯m
tε2L

(cid:18)

L +

(cid:19)

¯C
1 − ¯θ

,

where L = maxw∈U ·V |w|, ˜m = (cid:80)
¯m = (cid:80)
˜f unif
t

(uv).

u∈U ,v∈V

u∈U ,v∈V

˜ft(uv), and

Theorem 6 Suppose that B is (C, θ)-geometrically mixing
and ¯A is ( ¯C, ¯θ)-geometrically mixing. Suppose π satis-
ﬁes Assumption 1 and the smoothing coefﬁcient κ satisﬁes
κε > 1. Let d = (cid:80)
w∈U ·V |w|U ,V , and deﬁne L, ˜m, ¯m as
above. Then for any δ ∈ (0, 1) we have

P

(cid:107) (cid:98)H U ,V

t,ξ − ˜H U ,V

t

(cid:107)2 >

(cid:115)

˜m
tεL(1 − κ−2ε−2)

+

(cid:34)

(cid:115)

2 ¯m
tε2L

(cid:18)

L+

(cid:19)

¯C
1− ¯θ

+

C
θ(1−θ)εL

(cid:114)

2d ln(1/δ)
t

(cid:35)

(cid:54) δ .

On a ﬁnal note we remark that the dependence on εL might
be unavoidable due to inherent increase in variance pro-
duced by importance sampling estimators.

6. Conclusion

We present the ﬁrst rigorous analysis of single-trajectory
SVD-based spectral
learning algorithms for sequential
models with latent variables. Our analysis highlights the
role of mixing properties of WFA and their relation with the
geometry of the underlying state space. In the controlled
case we obtain a result for control with ﬁnite-state poli-
cies, a much more general class than previously considered
memoryless policies. In future work we will use our results
to get upper conﬁdence bounds on the predictions made by
the learned environment with the goal of solving the full
RL problem for PSR with complex control policies.

Spectral Learning from a Single Trajectory under Finite-State Policies

Acknowledgements

O.-A. M. acknowledges the support of the French Agence
Nationale de la Recherche (ANR), under grant ANR-16-
CE40-0002 (project BADASS).

References

Anandkumar, Anima, Foster, Dean P, Hsu, Daniel J,
Kakade, Sham M, and Liu, Yi-Kai. A spectral algorithm
for latent dirichlet allocation. In Advances in Neural In-
formation Processing Systems, pp. 917–925, 2012.

Anandkumar, Animashree, Ge, Rong, Hsu, Daniel J,
Kakade, Sham M, and Telgarsky, Matus. Tensor decom-
positions for learning latent variable models. Journal of
Machine Learning Research, 15(1):2773–2832, 2014.

Azizzadenesheli, Kamyar, Lazaric, Alessandro,

and
Anandkumar, Animashree. Reinforcement learning of
pomdps using spectral methods. In 29th Annual Confer-
ence on Learning Theory, pp. 193–256, 2016.

Bacon, Pierre-Luc, Balle, Borja, and Precup, Doina. Learn-
ing and planning with timing information in markov
In Proceedings of the Thirty-First
decision processes.
Conference on Uncertainty in Artiﬁcial Intelligence, pp.
111–120. AUAI Press, 2015.

Bailly, R., Habrard, A., and Denis, F. A spectral approach
for probabilistic grammatical inference on trees. In Al-
gorithmic Learning Theory, pp. 74–88, 2010.

Bailly, Rapha¨el. Quadratic weighted automata: Spectral
In Proceed-
algorithm and likelihood maximization.
ings of the 3rd Asian Conference on Machine Learning,
ACML 2011, Taoyuan, Taiwan, November 13-15, 2011,
pp. 147–163, 2011.

Balle, B. Learning Finite-State Machines: Algorithmic and
Statistical Aspects. PhD thesis, Universitat Polit`ecnica
de Catalunya, 2013.

Balle, Borja and Mohri, Mehryar. Spectral learning of gen-
eral weighted automata via constrained matrix comple-
tion. In Advances in neural information processing sys-
tems, pp. 2168–2176, 2012.

Balle, Borja and Mohri, Mehryar. Learning weighted au-
tomata. In International Conference on Algebraic Infor-
matics, pp. 1–21. Springer, 2015.

Balle, Borja, Quattoni, Ariadna, and Carreras, Xavier. A
spectral learning algorithm for ﬁnite state transducers.
In Joint European Conference on Machine Learning
and Knowledge Discovery in Databases, pp. 156–171.
Springer, 2011.

Balle, Borja, Carreras, Xavier, Luque, Franco M, and Quat-
toni, Ariadna. Spectral learning of weighted automata.
Machine learning, 96(1-2):33–63, 2014a.

Balle, Borja, Hamilton, William L, and Pineau, Joelle.
Methods of moments for learning stochastic languages:
Uniﬁed presentation and empirical comparison.
In
ICML, pp. 1386–1394, 2014b.

Balle, Borja, Gourdeau, Pascale, and Panangaden, Prakash.
Bisimulation metrics for weighted automata. In 44rd In-
ternational Colloquium on Automata, Languages, and
Programming, ICALP, 2017.

Berman, Abraham and Plemmons, Robert J. Nonnegative
matrices in the mathematical sciences. SIAM, 1994.

Berstel, Jean and Reutenauer, Christophe. Rational Series

and Their Languages. Springer, 1988.

Boots, Byron, Siddiqi, Sajid M, and Gordon, Geoffrey J.
Closing the learning-planning loop with predictive state
representations. The International Journal of Robotics
Research, 30(7):954–966, 2011.

Bowling, Michael, McCracken, Peter, James, Michael,
Neufeld, James, and Wilkinson, Dana. Learning pre-
dictive state representations using non-blind policies. In
Proceedings of the 23rd international conference on Ma-
chine learning, pp. 129–136. ACM, 2006.

Chazottes, J-R, Collet, Pierre, K¨ulske, Christof, and Redig,
Frank. Concentration inequalities for random ﬁelds via
coupling. Probability Theory and Related Fields, 137
(1-2):201–225, 2007.

Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., and
Ungar, L. Experiments with spectral learning of latent-
variable PCFGs. In Proceedings of NAACL, 2013.

Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., and
Ungar, L. Spectral learning of latent-variable PCFGs:
Algorithms and sample complexity. Journal of Machine
Learning Research, 2014.

Denis, Franc¸ois, Gybels, Mattias, and Habrard, Amaury.
Dimension-free concentration bounds on hankel matri-
ces for spectral learning. Journal of Machine Learning
Research, 17(31):1–32, 2016.

Denis, Franc¸ois and Esposito, Yann. On rational stochastic
languages. Fundamenta Informaticae, 86(1, 2):41–77,
2008.

Denis, Franc¸ois, Gybels, Mattias, and Habrard, Amaury.
Dimension-free concentration bounds on hankel matri-
ces for spectral learning. In ICML, pp. 449–457, 2014.

Spectral Learning from a Single Trajectory under Finite-State Policies

Fliess, M. Matrices de Hankel. Journal de Math´ematiques

Rosenthal, Jeffrey S. Convergence rates for markov chains.

Pures et Appliqu´ees, 53, 1974.

Siam Review, 37(3):387–405, 1995.

Glaude, Hadrien and Pietquin, Olivier. Pac learning of
probabilistic automaton based on the method of mo-
ments. In Proceedings of The 33rd International Con-
ference on Machine Learning, pp. 820–829, 2016.

Shaban, Amirreza, Farajtabar, Mehrdad, Xie, Bo, Song, Le,
and Boots, Byron. Learning latent variable models by
improving spectral solutions with exterior point method.
In UAI, pp. 792–801, 2015.

Siddiqi, S. M., Boots, B., and Gordon, G. J. Reduced-rank

hidden Markov models. AISTATS, 2010.

Thon, Michael and Jaeger, Herbert. Links between multi-
plicity automata, observable operator models and pre-
dictive state representations–a uniﬁed learning frame-
work. Journal of Machine Learning Research, 16:103–
147, 2015.

Hamilton, William L, Fard, Mahdi Milani, and Pineau,
Joelle. Efﬁcient learning and planning with compressed
Journal of Machine Learning Re-
predictive states.
search, 15(1):3395–3439, 2014.

Hsu, Daniel and Kakade, Sham M. Learning mixtures of
spherical Gaussians: moment methods and spectral de-
compositions. In Innovations in Theoretical Computer
Science, 2013.

Hsu, Daniel, Kakade, Sham M, and Zhang, Tong. A spec-
tral algorithm for learning hidden markov models. Jour-
nal of Computer and System Sciences, 78(5):1460–1480,
2012.

Jaeger, Herbert. Observable operator models for discrete
stochastic time series. Neural Computation, 12(6):1371–
1398, 2000.

Kontorovich, Aryeh and Weiss, Roi. Uniform chernoff and
dvoretzky-kiefer-wolfowitz-type inequalities for markov
chains and related processes. Journal of Applied Proba-
bility, 51(04):1100–1113, 2014.

Kontorovich, Leonid Aryeh, Ramanan, Kavita, et al. Con-
centration inequalities for dependent random variables
via the martingale method. The Annals of Probability,
36(6):2126–2158, 2008.

Kontoyiannis, Ioannis and Meyn, Sean P. Geometric er-
godicity and the spectral gap of non-reversible markov
chains. Probability Theory and Related Fields, 154(1-
2):327–339, 2012.

Kulesza, Alex, Jiang, Nan, and Singh, Satinder. Low-rank
spectral learning with weighted loss functions. In Artiﬁ-
cial Intelligence and Statistics, pp. 517–525, 2015.

Langer, Lucas, Balle, Borja, and Precup, Doina. Learn-
In Pro-
ing multi-step predictive state representations.
ceedings of the Twenty-Fifth International Joint Confer-
ence on Artiﬁcial Intelligence, IJCAI 2016, New York,
NY, USA, 9-15 July 2016, pp. 1662–1668, 2016.

Luque, Franco M, Quattoni, Ariadna, Balle, Borja, and
Carreras, Xavier. Spectral learning for non-deterministic
In Proceedings of the 13th Con-
dependency parsing.
ference of the European Chapter of the Association for
Computational Linguistics, pp. 409–419. Association
for Computational Linguistics, 2012.

