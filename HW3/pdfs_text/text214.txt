Differentiable Programs with Neural Libraries

Alexander L. Gaunt 1 Marc Brockschmidt 1 Nate Kushman 1 Daniel Tarlow 2

Abstract
We develop a framework for combining differen-
tiable programming languages with neural net-
works. Using this framework we create end-to-
end trainable systems that learn to write inter-
pretable algorithms with perceptual components.
We explore the beneﬁts of inductive biases for
strong generalization and modularity that come
from the program-like structure of our models. In
particular, modularity allows us to learn a library
of (neural) functions which grows and improves
as more tasks are solved. Empirically, we show
that this leads to lifelong learning systems that
transfer knowledge to new tasks more effectively
than baselines.

1. Introduction

Recently, there has been much work on learning algorithms
using neural networks. Following the idea of the Neural Tur-
ing Machine (Graves et al., 2014), this work has focused on
extending neural networks with interpretable components
that are differentiable versions of traditional computer com-
ponents, such as external memories, stacks, and discrete
functional units. However, trained models are not easily
interpreted as the learned algorithms are embedded in the
weights of a monolithic neural network. In this work we ﬂip
the roles of the neural network and differentiable computer
architecture. We consider interpretable controller architec-
tures which express algorithms using differentiable pro-
gramming languages (Gaunt et al., 2016; Riedel et al., 2016;
Bunel et al., 2016). In our framework, these controllers can
execute discrete functional units (such as those considered
by past work), but also have access to a library of trainable,
uninterpretable neural network functional units. The sys-
tem is end-to-end differentiable such that the source code
representation of the algorithm is jointly induced with the
parameters of the neural function library. In this paper we

1Microsoft Research, Cambridge, UK 2Google Brain, Montr´eal,
Canada (work done while at Microsoft). Correspondence to:
Alexander L. Gaunt <algaunt@microsoft.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

explore potential advantages of this class of hybrid model
over purely neural systems, with a particular emphasis on
lifelong learning systems that learn from weak supervision.

We concentrate on perceptual programming by example
(PPBE) tasks that have both algorithmic and perceptual ele-
ments to exercise the traditional strengths of program-like
and neural components. Examples of this class of task in-
clude navigation tasks guided by images or natural language
(see Fig. 1) or handwritten symbol manipulation (see Sec. 3).
Using an illustrative set of PPBE tasks we aim to emphasize
two speciﬁc beneﬁts of our hybrid models:

First, the source code representation in the controller allows
modularity: the neural components are small functions that
specialize to different tasks within the larger program struc-
ture. It is easy to separate and share these functional units to
transfer knowledge between tasks. In contrast, the absence
of well-deﬁned functions in purely neural solutions makes
effective knowledge transfer more difﬁcult, leading to prob-
lems such as catastrophic forgetting in multitask and life-
long learning (McCloskey & Cohen, 1989; Ratcliff, 1990).
In our experiments, we consider a lifelong learning setting
in which we train the system on a sequence of PPBE tasks
that share perceptual subtasks.

Second, the source code representation enforces an induc-
tive bias that favors learning solutions that exhibit strong
generalization. For example, once a suitable control ﬂow
structures (e.g., a for loop) for a list manipulation prob-
lem was learned on short examples, it trivially generalizes
to lists of arbitrary length. In contrast, although some neu-
ral architectures demonstrate a surprising ability to general-
ize, the reasons for this generalization are not fully under-
stood (Zhang et al., 2017) and generalization performance
invariably degrades as inputs become increasingly distinct
from the training data.

This paper is structured as follows. We ﬁrst present a lan-
guage, called NEURAL TERPRET (NTPT), for specifying
hybrid source code/neural network models (Sec. 2), and
then introduce a sequence of PPBE tasks (Sec. 3). Our
NTPT models and purely neural baselines are described
in Sec. 4 and 5 respectively. The experimental results are
presented in Sec. 6.

Differentiable Programs with Neural Libraries

Figure 1: Components of an illustrative NTPT program for learning loopy programs that measure path length (path len)
through a maze of street sign images. The learned program (parameterized by instr and goto) must control the position
(X, Y) of an agent on a grid of (W×H) street sign images each of size (w×h). The agent has a single register of memory
(reg) and learns to interpret street signs using the LOOK neural function. Our system produces a solution consisting of a
correctly inferred program and a trained neural network (see Supplementary Material). Learnable components are shown in
blue and the NTPT extensions to the TERPRET language are highlighted. The red path on the img grid shows the desired
behavior and is not provided at training time.

2. Building hybrid models

The TERPRET language (Gaunt et al., 2016) provides a sys-
tem for constructing differentiable program interpreters that
can induce source code operating on basic data types (e.g.
integers) from input-output examples. We extend this lan-
guage with the concept of learnable neural functions. These
can either be embedded inside the differentiable interpreter
as mappings from integer to integer or (as we emphasize in
this work) can act as learnable interfaces between percep-
tual data represented as ﬂoating point Tensors and the dif-
ferentiable interpreter’s integer data type. Below we brieﬂy
review the TERPRET language and describe the NEURAL
TERPRET extensions.

2.1. TERPRET

TERPRET programs specify a differentiable interpreter by
deﬁning the relationship between Inputs and Outputs
via a set of inferrable Params (that deﬁne an executable
program) and Vars (that store intermediate results). TER-
PRET requires all of these variables to range over bounded
integers. The model is made differentiable by a compilation
step that lifts the relationships between integers speciﬁed
by the TERPRET code to relationships between marginal

distributions over integers in ﬁnite ranges. Fig. 1 illustrates
an example application of the language.

TERPRET can be translated into a TensorFlow (Abadi et al.,
2015) computation graph which can then be trained using
standard methods. For this, two key features of the language
need to be translated:

• Function

is

The

jk Iijkµx

application.
statement
z.set to(foo(x, y))
into
i = (cid:80)
µz
k where µa represents the
marginal distribution for the variable a and I is an
indicator tensor 1[i = foo(j, k)]. This approach
extends to all functions mapping any number of
integer arguments to an integer output.

translated

j µy

• Conditional

statements The
statements
x == 0: z.set to(a); elif x == 1:
1 µb.
z.set to(b) are translated to µz = µx
Statements switching between more than two cases
follow a similar pattern, with details given in (Gaunt
et al., 2016).

0 µa + µx

if

# Discrete operations@Runtime([max_int], max_int)defINC(a):return(a + 1) % max_int@Runtime([max_int], max_int)defDEC(a):return(a -1) % max_int@Runtime([W, 5], W)defMOVE_X(x, dir):ifdir== 1: return(x + 1) % W # →elifdir== 3: return(x -1) % W # ←else: returnx@Runtime([H, 5], H)defMOVE_Y(y, dir):ifdir== 2: return(y -1) % H # ↑elifdir== 4: return(y + 1) % H # ↓else: returny# Helper functions@Runtime([5],2)defeq_zero(dir):return1 ifdir== 0 else0# Learned operations@Learn([Tensor(w,h)],5,hid_sizes=[256,256])defLOOK(img):pass# constantsmax_int= 15; n_instr= 3; T = 45W = 5; H = 3; w = 28; h = 28# variablesimg_grid= InputTensor(w, h)[W, H]init_X= Input(W)init_Y= Input(H)final_X= Output(W)final_Y= Output(H)path_len= Output(max_int)is_halted_at_end= Output(2)instr= Param(4)[n_instr]goto= Param(n_instr)[n_instr]X = Var(W)[T]Y = Var(H)[T]dir= Var(5)[T]reg= Var(max_int)[T]instr_ptr= Var(n_instr)[T]is_halted= Var(2)[T]# initializationX[0].set_to(init_X)Y[0].set_to(init_Y)dir[0].set_to(1)reg[0].set_to(0)instr_ptr[0].set_to(0)fort inrange(T -1):is_halted[t].set_to(eq_zero(dir[t]))ifis_halted[t] == 1:   # halteddir[t + 1].set_to(dir[t])X[t + 1].set_to(X[t])Y[t + 1].set_to(Y[t])reg[t + 1].set_to(reg[t])instr_ptr[t + 1].set_to(instr_ptr[t])elifis_halted[t] == 0: # not haltedwithinstr_ptr[t] asi:ifinstr[i] == 0:                  # INCreg[t + 1].set_to(INC(reg[t]))elifinstr[i] == 1:                # DECreg[t + 1].set_to(DEC(reg[t]))else:reg[t + 1].set_to(reg[t])ifinstr[i] == 2:                  # MOVEX[t + 1].set_to(MOVE_X(X[t], dir[t]))Y[t + 1].set_to(MOVE_Y(Y[t], dir[t]))else:X[t + 1].set_to(X[t])Y[t + 1].set_to(Y[t])ifinstr[i] == 3:                  # LOOKwith X[t] as x:with Y[t] as y:dir[t + 1].set_to(LOOK(img_grid[y,x]))else:dir[t + 1].set_to(dir[t])instr_ptr[t + 1].set_to(goto[i])final_X.set_to(X[T -1])final_Y.set_to(X[T -1])path_len.set_to(reg[T –1])is_halted_at_end.set_to(ishalted[T -2])Instruction SetDeclaration & initializationExecution modelInput-output data setimg_grid=init_X= 0init_Y= 1final_X= 4final_Y= 2path_len= 7instr= [3,2,0]goto= [1,2,0]L0if not halted:dir= LOOKhalt if dir==0gotoL1L1if not halted:MOVE(dir)gotoL2L2if not halted:reg= INC(reg)gotoL0SolutionLOOK:=Differentiable Programs with Neural Libraries

Figure 2: Overview of tasks in the (a) ADD2X2, (b) APPLY2X2 and (c) MATH scenarios. ‘A’ denotes the APPLY operator
which replaces the ? tiles with the selected operators and executes the sum. We show two MATH examples of different
length.

2.2. NEURAL TERPRET

To handle perceptual data, we relax the restriction that all
variables need to be ﬁnite integers. We introduce a new
ﬂoating point Tensor type whose dimensions are ﬁxed
at declaration, and which is suitable for storing percep-
tual data. Additionally, we introduce learnable functions
that can process integer or tensor variables. A learnable
function is declared using @Learn([d1, . . . , dD], dout,
hid sizes=[(cid:96)1, . . . , (cid:96)L]), where the ﬁrst component
speciﬁes the dimensions (resp. ranges) d1, . . . , dD of the
input tensors (resp. integers) and the second speciﬁes the di-
mension of the output. NTPT compiles such functions into
a fully-connected feed-forward neural network whose lay-
out is controlled by the hid sizes component (specifying
the number neurons in each layer). The inputs of the func-
tion are simply concatenated. Tensor output is generated by
learning a mapping from the last hidden layer, and ﬁnite
integer output is generated by a softmax layer producing a
distribution over integers up to the declared bound. Learn-
able parameters for the generated network are shared across
every use of the function in the NTPT program, and as they
naturally ﬁt into the computation graph for the remaining
TERPRET program, the whole system is trained end-to-end.
We illustrate an example NTPT program for learning navi-
gation tasks in a maze of street signs (Stallkamp et al., 2011)
in Fig. 1.

3. A Lifetime of PPBE Tasks

Motivated by the hypothesis that the modularity of the
source code representation beneﬁts knowledge transfer, we
devise a sequence of PPBE tasks to be solved by sharing
knowledge between tasks. Our tasks are based on algorith-
mic manipulation of handwritten digits and mathematical
operators.

In early tasks the model learns to navigate simple 2 × 2
grids of images, and to become familiar with the concepts
of digits and operators from a variety of weak supervision.
Despite their simplicity, these challenges already pose prob-
lems for purely neural lifelong learning systems.

The ﬁnal task in the learning lifetime is more complex and
designed to test generalization properties: the system must
learn to compute the results of variable-length mathematical
expressions expressed using handwritten symbols. The algo-
rithmic component of this task is similar to arithmetic tasks
presented to contemporary Neural GPU models (Kaiser &
Sutskever, 2016; Price et al., 2016). The complete set of
tasks is illustrated in Fig. 2 and described in detail below.

ADD2X2 scenario: The ﬁrst scenario in Fig. 2(a) uses of
a 2 × 2 grid of MNIST digits. We set 4 tasks based on this
grid: compute the sum of the digits in the (1) top row, (2) left
column, (3) bottom row, (4) right column. All tasks require
classiﬁcation of MNIST digits, but need different programs
to compute the result. As training examples, we supply only
a grid and the resulting sum. Thus, we never directly label
an MNIST digit with its class.

APPLY2X2 scenario: The second scenario in Fig. 2(b)
presents a 2 × 2 grid of of handwritten arithmetic opera-
tors. Providing three auxiliary random integers d1, d2, d3,
we again set 4 tasks based on this grid, namely to evalu-
ate the expression1 d1 op1 d2 op2 d3 where (op1, op2)
are the operators represented in the (1) top row, (2) left col-
umn, (3) bottom row, (4) right column. In comparison to
the ﬁrst scenario, the dataset of operators is relatively small
and consistent2, making the perceptual task of classifying
operators considerably easier.

MATH scenario: The ﬁnal task in Fig. 2(c) requires com-
bination of the knowledge gained from the weakly labeled
data in the ﬁrst two scenarios to execute a handwritten arith-
metic expression.

1Note that for simplicity, our toy system ignores operator prece-
dence and executes operations from left to right - i.e. the sequence
in the text is executed as ((d1 op1 d2) op2 d3).

2200 handwritten examples of each operator were collected
from a single author to produce a training set of 600 symbols and
a test set of 200 symbols from which to construct random 2 × 2
grids.

++810117++(a)145AAA3A1443?2?(b)= 2(c)= 11(a)

(b)

4.2. 2 × 2 model

Differentiable Programs with Neural Libraries

# initialization :
R0 = READ
# program :
R1 = MOVE EAST
R2 = MOVE SOUTH
R3 = SUM(R0 , R1)
R4 = NOOP
return R3

# initialization :
R0 = InputInt [0]
R1 = InputInt [1]
R2 = InputInt [2]
R3 = READ
# program :
R4 = MOVE EAST
R5 = MOVE SOUTH
R6 = APPLY(R0 , R1 , R4)
R7 = APPLY(R6 , R2 , R5)
return R7

Figure 3: Example solutions for the tasks on the right
columns of the (a) ADD2X2 and (b) APPLY2X2 scenarios.
The read head is initialized READing the top left cell and
any auxiliary InputInts are loaded into memory. Instruc-
tions and arguments shown in black must be learned.

4. Models

We study two kinds of NTPT model. First, for navigating
the introductory 2 × 2 grid scenarios, we create a model
which learns to write simple straight-line code. Second, for
the MATH scenario we ask the system to use a more com-
plex language which supports loopy control ﬂow (note that
the baselines will also be specialized between the 2 × 2
scenarios and the MATH scenario). Knowledge transfer is
achieved by deﬁning a library of 2 neural network functions
shared across all tasks and scenarios. Training on each task
should produce a task-speciﬁc source code solution (from
scratch) and improve the overall usefulness of the shared
networks. All models are included in Supplementary Mate-
rial, and below we outline further details of the models.

4.1. Shared components

We refer to the 2 networks in the shared library as net 0
and net 1. Both networks have similar architectures: they
take a 28 × 28 monochrome image as input and pass this
sequentially through two fully connected layers each with
256 neurons and ReLU activations. The last hidden vector
is passed through a fully connected layer and a softmax to
produce a 10 dimensional output (net 0) or 4 dimensional
output (net 1) to feed to the differentiable interpreter (the
output sizes are chosen to match the number of classes of
MNIST digits and arithmetic operators respectively).

One restriction that we impose is that when a new task is
presented, no more than one new untrained network can
be introduced into the library (i.e. in our experiments the
very ﬁrst task has access to only net 0, and all other tasks
have access to both nets). This restriction is imposed be-
cause if a differentiable program tries to make a call to one
of N untrained networks based on an unknown parameter
net choice = Param(N), then the system effectively
sees the N nets together with the net choice parameter
as one large untrained network, which cannot usefully be
split apart into the N components after training.

For the 2 × 2 scenarios we build a model capable of writing
short straight line algorithms with up to 4 instructions. The
model consists of a read head containing net 0 and net 1
which are connected to a set of registers each capable of
holding integers in the range 0, . . . , M , where M = 18.
The head is initialized reading the top left cell of the 2 × 2
grid. At each step in the program, one instruction can be
executed, and lines of code are constructed by choosing an
instruction and addresses of arguments for that instruction.
We follow (Feser et al., 2016) and allow each line to store
its result in a separate immutable register. For the ADD2X2
scenario the instruction set is:

• NOOP: a trivial no-operation instruction.

• MOVE NORTH, MOVE EAST, MOVE SOUTH,

MOVE WEST:
translate the head (if possible) and
return the result of applying the neural network chosen
by net choice to the image in the new cell.

• ADD(·, ·): accepts two register addresses and returns

the sum of their contents.

The parameter net choice is to be learned and decides
which of net 0 and net 1 to apply. In the APPLY2X2
scenario we extend the ADD instruction to APPLY(a, b,
op) which interprets the integer stored at op as an arith-
metic operator and computes3 a op b. In addition, for the
APPLY2X2 scenario we initialize three registers with the
auxiliary integers supplied with each 2 × 2 operator grid
[see Fig. 2(b)]. In total, this model exposes a program space
of up to ∼ 1012 syntactically distinct programs.

4.3. MATH model

The ﬁnal task investigates the synthesis of more complex,
loopy control ﬂow. A natural solution to execute the expres-
sion on the tape is to build a loop with a body that alternates
between moving the head and applying the operators [see
Fig. 4(b)]. This loopy solution has the advantage that it gen-
eralizes to handle arbitrary length arithmetic expressions.

Fig. 4(a) shows the basic architecture of the interpreter used
in this scenario. We provide a set of three blocks each con-
taining the instruction MOVE or APPLY, an address, a reg-
ister and a net choice. A MOVE instruction increments
the position of the head and loads the new symbol into a
block’s register using either net 0 or net 1 as determined
by the block’s net choice. After executing the instruc-
tion, the interpreter executes a GOTO IF statement which
checks whether the head is over the end of the tape and if not

3All operations are performed modulo (M + 1) and division

by zero returns M .

(a)

(b)

Differentiable Programs with Neural Libraries

1. Indep.: Each task is handled by an independent col-

umn with no mechanism for transfer.

2. Progressive Neural Network (PNN): We follow
(Rusu et al., 2016) and build lateral connections link-
ing each task speciﬁc column to columns from tasks
appearing earlier in the learning lifetime. Weights in
all columns except the active task’s column are frozen
during a training update. Note that the number of lay-
ers in each column must be identical to allow lateral
connections, meaning we cannot tune the architecture
separately for each task.

3. Multitask neural network (MTNN): We split the col-
umn into a shared perceptual part and a task speciﬁc
part. The perceptual part consists of net 0 and net 1
embedding networks (note that we use a similar sym-
metry breaking technique mentioned in Sec. 4.1 to en-
courage specialization of these networks to either digit
or operator recognition respectively).

The task-speciﬁc part consists of a neural network that
maps the perceptual embeddings to a 19 dimensional
output. Note that unlike PNNs, the precise architecture
of the task speciﬁc part of the MTNN can be tuned for
each individual task. We consider two MTNN architec-
tures:

(a) MTNN-1: All task-speciﬁc parts are 3 layer net-

works comparable to the PNN case.

(b) MTNN-2: We manually tune the number of layers
for each task and ﬁnd best performance when the
task speciﬁc part contains 1 hidden layer for the
ADD2X2 tasks and 3 layers for the APPLY2X2
tasks.

5.2. MATH baselines

For the MATH task, we build purely neural baselines which
(1) have previously been shown to offer competitive gener-
alization performance for some tasks with sequential inputs
of varying length (2) are able to learn to execute arithmetic
operations and (3) are easily integrated with the library of
perceptual networks learned in the 2 × 2 tasks. We consider
two models fulﬁlling these criteria: an LSTM and a Neural
GPU.

For the LSTM, at each image in the mathematical expres-
sion the network takes in the embeddings of the current
symbol from net 0 and net 1, updates an LSTM hidden
state and then proceeds to the next symbol. We make a clas-
siﬁcation of the ﬁnal answer using the last hidden state of
the LSTM. Our best performance is achieved with a 3 layer
LSTM with 1024 elements in each hidden state and dropout
between layers.

Figure 4: Overview of the MATH model. (a) The general
form of a block in the model. Blue elements are learnable.
(b) A loop-based solution to the task in the MATH scenario.

then it passes control to the block speciﬁed by goto addr,
otherwise control passes to a halt block which returns a
chosen register value and exits the program. This model
describes a space of ∼ 106 syntactically distinct programs.

5. Baselines

To evaluate the merits of including the source code structure
in NTPT models, we build baselines that replace the differ-
entiable program interpreter with neural networks, thereby
creating purely neural solutions to the lifelong PPBE tasks.
We specialize these neural baselines for the 2 × 2 task (with
emphasis on lifelong learning) and for the MATH task (with
emphasis on generalization).

5.1. 2 × 2 baselines

We deﬁne a column as the following neural architecture (see
Fig. 5(a)):

• Each of the images in the 2 × 2 grid is passed through
an embedding network with 2 layers of 256 neurons (cf.
net 0/1) to produce a 10-dimensional embedding.
The weights of the embedding network are shared
across all 4 images.

• These 4 embeddings are concatenated into a 40-
dimensional vector and for the APPLY2X2 the auxil-
iary integers are represented as one-hot vectors and
concatenated with this 40-dimensional vector.

• This is then passed through a network consisting of
3 hidden layers of 128 neurons to produce a 19-
dimensional output.

We construct 3 different neural baselines derived from this
column architecture (see Fig. 5):

ifis MOVE:pos++else:Ri= APPLY(Ri= READ(,,))Labeli:GOTO_IF halt:returninstrinet_choiceiarg1iarg2iarg3igoto_addrinstrjLabelj:return_addrL0:MOVER0= READ(net_0)GOTO_IFL1L1:R1= APPLY(R1, R0, R2)GOTO_IFL2L2:MOVER2= READ(net_1)GOTO_IFL0halt:returnR1Differentiable Programs with Neural Libraries

Figure 5: Cartoon illustration of all models used in the 2 × 2 experiments. See text for details.

For the Neural GPU, we use the implementation from the
original authors4 (Kaiser & Sutskever, 2016).

6. Experiments

In this section we report results illustrating the key beneﬁts
of NTPT for the lifelong PPBE tasks in terms of knowledge
transfer (Sec. 6.1) and generalization (Sec. 6.2).

6.1. Lifelong Learning

Demonstration of lifelong learning requires a series of tasks
for which there is insufﬁcient data to learn independent so-
lutions to all tasks and instead, success requires transferring
knowledge from one task to the next. Empirically, we ﬁnd
that training any of the purely neural baselines or the NTPT
model on individual tasks from the ADD2X2 scenario with
only 1k distinct 2 × 2 examples produces low accuracies of
around 40 ± 20% (measured on a held-out test set of 10k ex-
amples). Since none of our models can satisfactorily solve
an ADD2X2 task independently in this small data regime,
we can say that any success on these tasks during a lifetime
of learning can be attributed to successful knowledge trans-
fer. In addition, we check that in a data rich regime (e.g.
≥4k examples) all of the baseline models and NTPT can
independently solve each task with >80% accuracy. This
indicates that the models all have sufﬁcient capacity to rep-
resent satisfactory solutions, and the challenge is to ﬁnd
these solutions during training.

We train on batches of data drawn from a time-evolving
probability distribution over all 8 tasks in the 2×2 scenarios
(see the top of Fig. 6(a)). During training, we observe the
following key properties of the knowledge transfer achieved
by NTPT:

4available

at https://github.com/tensorflow/

models/tree/master/neural_gpu

Reverse transfer: Fig. 6(a) focuses on the performance of
NTPT on the ﬁrst task (ADD2X2:top). The red bars indicate
times where the the system was presented with an example
from this task. Note that even when we have stopped pre-
senting examples, the performance on this task continues to
increase as we train on later tasks - an example of reverse
transfer. We verify that this is due to continuous improve-
ment of net 0 in later tasks by observing that the accuracy
on the ADD2X2:top task closely tracks measurements of
the accuracy of net 0 directly on the digit classiﬁcation
task.

Avoidance of catastrophic forgetting: Fig. 6(b) shows the
performance of the NTPT on the remaining ADD2X2 tasks.
Both Fig. 6(a) and (b) include results for the MTNN-2 base-
line (the best baseline for these tasks). Note that whenever
the dominant training task swaps from an ADD2X2 task to
an APPLY2X2 task the baseline’s performance on ADD2X2
tasks drops. This is because the shared perceptual network
becomes corrupted by the change in task - an example of
catastrophic forgetting. To try to limit the extent of catas-
trophic forgetting and make the shared components more
robust, we have a separate learning rate for the perceptual
networks in both the MTNN baseline and NTPT which is
100 fold smaller than the learning rate for the task-speciﬁc
parts. With this balance of learning rates we ﬁnd empirically
that NTPT does not display catastrophic forgetting, while
the MTNN does.

Final performance: Fig. 6(c) focuses on the ADD2X2:left
and APPLY2X2:left tasks to illustrate the relative perfor-
mance of all the baselines described in Sec. 5. Note that
although PNNs are effective at avoiding catastrophic forget-
ting, there is no clear overall winner between the MTNN
and PNN baselines. NTPT learns faster and to a higher ac-
curacy than all baselines for all the tasks considered here.
For clarity we only plot results for the *:left tasks: the other
tasks show similar behavior and the accuracies for all tasks
at the end of the lifetime of learning are presented in Fig. 7.

concatconcatconcatR0 = REAR1 = MOVR2 = SUMR3 = NOOR4 = NOOreturn RR0 = InpR1 = InpR2 = InpR3 = MOVR4 = MOVR5 = APPR0 = REAR1 = MOVR2 = MOVR3 = SUMR4 = NOOreturn RLibrary(4,3,2)(b) PNN(d) NTPTTASK 1TASK 2TASK 3concat(4,3,2)(a) indep.concat(4,3,2)(c) MTNN2562561012812812819Differentiable Programs with Neural Libraries

(a)

(b)

(c)

Figure 6: Lifelong learning with NTPT. (a) top: the sequential learning schedule for all 8 tasks, bottom: performance of
NTPT (solid) and the MTNN-2 baseline (dashed) on the ﬁrst ADD2X2 task. (b) performance on the remaining ADD2X2
tasks. (c) Performance of all the baselines on the *:left tasks.

task

indep PNN MTNN-1 MTNN-2 NTPT

2 top
X
left
2
D
D
A

35% 35%
32% 36%
bottom 34% 33%
32% 35%
right

2 top
X
left
2
Y
L
P
P
A

38% 39%
39% 51%
bottom 39% 48%
39% 51%
right

26%
38%
40%
44%

40%
41%
41%
42%

24%
47%
56%
60%

38%
39%
40%
37%

87%
87%
86%
86%

98%
100%
100%
100%

Figure 7: Final accuracies on all 2 × 2 tasks for all models
at the end of lifelong learning

Figure 8: Generalization behavior on MATH expressions.
Solid dots indicate expression lengths used in training. We
show results on (a) a simpler non-perceptual MATH task
(numbers in parentheses indicate parameter count in each
model) and (b) the MATH task including perception.

6.2. Generalization

In the ﬁnal experiment we take net 0/1 from the end of
the NTPT 2 × 2 training and start training on the MATH
scenario. For the NTPT model we train on arithmetic ex-
pressions containing only 2 digits. The known difﬁculty
in training differentiable interpreters with free loop struc-
ture (Gaunt et al., 2016) is revealed by the fact that only
2/100 random restarts converge on a correct program in a
global optimum of the loss landscape. We detect conver-
gence by a rapid increase in the accuracy on a validation
set (typically occurring after around 30k training examples).
Once the correct program is found, continuing to train the

model mainly leads to further improvement in the accuracy
of net 0, which saturates at 97.5% on the digit classiﬁ-
cation task. The learned source code provably generalizes
perfectly to expressions containing any number of digits,
and the only limitation on the performance on long expres-
sions comes from the repeated application of the imperfect
net 0.

To pick a strong baseline for the MATH problem, we ﬁrst
perform a preliminary experiment with two simpliﬁcations:
(1) rather than expecting strong generalization from just 2-
digit training examples, we train candidate baselines with
supervision on examples of up to 5 digits and 4 opera-
tors, and (2) we remove the perceptual component of the
task, presenting the digits and operators as one-hot vectors
rather than images. Fig. 8(a) shows the generalization per-
formance of the LSTM and Neural GPU (512-ﬁlter) base-
lines in this simpler setting after training to convergence.5
Based on these results, we restrict attention to the LSTM
baseline and return to the full task including the perceptual
component. In the full MATH task, we initialize the embed-
ding networks of each model using net 0/1 from the end
of the NTPT 2 × 2 training. Fig. 8(b) shows generalization
of the NTPT and LSTM models on expressions of up to 16
digits (31 symbols) after training to convergence. We ﬁnd
that even though the LSTM shows surprisingly effective
generalization when supplied supervision for up to 5 digits,
NTPT trained on only 2-digit expressions still offers better
results.

7. Related work

Lifelong Machine Learning. We operate in the
paradigm of Lifelong Machine Learning (LML) (Thrun,
1994; 1995; Thrun & O’Sullivan, 1996; Silver et al., 2013;

5Note that (Price et al., 2016) also ﬁnd poor generalization
performance for a Neural GPU applied to the similar task of eval-
uating arithmetic expressions involving binary numbers.

0128256384512training example (1000s)0.00.51.0accuracy0.00.51.0probability0128256384512training example (1000s)ADD2x2: top rowADD2x2: left columnADD2x2: bottom rowADD2x2: right columnAPPLY2x2 tasks0128256384512training example (1000s)0.00.51.0APPLY2x2:leftaccuracyindep.PNNMTNN-1MTNN-2NTPT0.00.51.0ADD2x2:leftaccuracy82.887.18090100051015accuracy (%)digits in expressionLSTM - 2digitLSTM - 5digitNTPT - 2digit25.092.8100050100accuracy (%)Neural GPU (43.8M)LSTM       (21.1M)TerpreT       (32)(a)(b)Differentiable Programs with Neural Libraries

Chen et al., 2015), where a learner is presented a sequence
of different tasks and the aim is to retain and re-use knowl-
edge from earlier tasks to more efﬁciently and effectively
learn new tasks. This is distinct from related paradigms of
multitask learning (where a set of tasks is presented rather
than in sequence (Caruana, 1997; Kumar & Daume III,
2012; Luong et al., 2015; Rusu et al., 2016)), transfer learn-
ing (transfer of knowledge from a source to target domain
without notion of knowledge retention (Pan & Yang, 2010)),
and curriculum learning (training a single model for a single
task of varying difﬁculty (Bengio et al., 2009)).

The challenge for LML with neural networks is the problem
of catastrophic forgetting: if the distribution of examples
changes during training, then neural networks are prone to
forget knowledge gathered from early examples. Solutions
to this problem involve instantiating a knowledge reposi-
tory (KR) either directly storing data from earlier tasks or
storing (sub)networks trained on the earlier tasks with their
weights frozen. This knowledge base allows either (1) re-
hearsal on historical examples (Robins, 1995), (2) rehearsal
on virtual examples generated by the frozen networks (Sil-
ver & Mercer, 2002; Silver & Poirier, 2006) or (3) creation
of new networks containing frozen sub networks from the
historical tasks (Rusu et al., 2016; Shultz & Rivest, 2001)

To frame our approach in these terms, our KR contains
partially-trained neural network classiﬁers which we call
from learned source code. Crucially, we never freeze the
weights of the networks in the KR: all parts of the KR can
be updated during the training of all tasks - this allows us to
improve performance on earlier tasks by continuing training
on later tasks (so-called reverse transfer). Reverse transfer
has been demonstrated previously in systems which assume
that each task can be solved by a model parameterized by an
(uninterpretable) task-speciﬁc linear combination of shared
basis weights (Ruvolo & Eaton, 2013). The representation
of task-speciﬁc knowledge as source code, learning from
weak supervision, and shared knowledge as a deep neural
networks distinguishes this work from the linear model used
in (Ruvolo & Eaton, 2013).

Neural Networks Learning Algorithms. Recently, ex-
tensions of neural networks with primitives such as memory
and discrete computation units have been studied to learn
algorithms from input-output data (Graves et al., 2014; We-
ston et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al.,
2015; Kurach et al., 2015; Kaiser & Sutskever, 2016; Reed
& de Freitas, 2016; Bunel et al., 2016; Andrychowicz &
Kurach, 2016; Zaremba et al., 2016; Graves et al., 2016;
Riedel et al., 2016; Gaunt et al., 2016; Feser et al., 2016).
A dominant trend in these works is to use a neural network
controller to managing differentiable computer architecture.
We ﬂip this relationship, and in our approach, a differen-
tiable interpreter acts as the controller that can make calls
to neural network components.

The methods above, with the exception of (Reed & de Fre-
itas, 2016) and (Graves et al., 2016), operate on inputs of
(arrays of) integers. However, (Reed & de Freitas, 2016)
requires extremely strong supervision, where the learner
is shown all intermediate steps to solving a problem;
our learner only observes input-output examples. (Reed &
de Freitas, 2016) also show the performance of their sys-
tem in a multitask setting. In some cases, additional tasks
harm performance of their model and they freeze parts of
their model when adding to their library of functions. Only
(Bunel et al., 2016), (Riedel et al., 2016) and (Gaunt et al.,
2016) aim to consume and produce source code that can
be provided by a human (e.g. as sketch of a solution) or
returned to a human (to potentially provide feedback).

8. Discussion

We have presented NEURAL TERPRET, a framework for
building end-to-end trainable models that structure their so-
lution as a source code description of an algorithm which
may make calls into a library of neural functions. Experi-
mental results show that these models can successfully be
trained in a lifelong learning context, and they are resistant
to catastrophic forgetting; in fact, they show that even af-
ter instances of earlier tasks are no longer presented to the
model, performance still continues to improve.

Our experiments concentrated on two key beneﬁts of the
hybrid representation of task solutions as source code and
neural networks. First, the source code structure imposes
modularity which can be seen as focusing the supervision.
If a component is not needed for a given task, then the differ-
entiable interpreter can choose not to use it, which shuts off
any gradients from ﬂowing to that component. We speculate
that this could be a reason for the models being resistant to
catastrophic forgetting, as the model either chooses to use
a classiﬁer, or ignores it (which leaves the component un-
changed). The second beneﬁt is that learning programs im-
poses a bias that favors learning models that exhibit strong
generalization. Additionally, the source code representation
has the advantage of being interpretable by humans, allow-
ing veriﬁcation and incorporation of domain knowledge de-
scribing the shape of the problem through the source code
structure.

The primary limitation of this design is that it is known
that differentiable interpreters are difﬁcult to train on prob-
lems signiﬁcantly more complex than those presented here
(Kurach et al., 2015; Neelakantan et al., 2016; Gaunt et al.,
2016). However, if progress can be made on more robust
training of differentiable interpreters (perhaps extending
ideas in (Neelakantan et al., 2016) and (Feser et al., 2016)),
then we believe there to be great promise in using hybrid
models to build large lifelong learning systems.

Differentiable Programs with Neural Libraries

References

Abadi, Mart´ın, Agarwal, Ashish, Barham, Paul, Brevdo,
Eugene, Chen, Zhifeng, Citro, Craig, Corrado, Greg S.,
Davis, Andy, Dean, Jeffrey, Devin, Matthieu, Ghemawat,
Sanjay, Goodfellow, Ian, Harp, Andrew, Irving, Geof-
frey, Isard, Michael, Jia, Yangqing, Jozefowicz, Rafal,
Kaiser, Lukasz, Kudlur, Manjunath, Levenberg, Josh,
Man´e, Dan, Monga, Rajat, Moore, Sherry, Murray, Derek,
Olah, Chris, Schuster, Mike, Shlens, Jonathon, Steiner,
Benoit, Sutskever, Ilya, Talwar, Kunal, Tucker, Paul, Van-
houcke, Vincent, Vasudevan, Vijay, Vi´egas, Fernanda,
Vinyals, Oriol, Warden, Pete, Wattenberg, Martin, Wicke,
Martin, Yu, Yuan, and Zheng, Xiaoqiang. TensorFlow:
Large-scale machine learning on heterogeneous systems,
2015. URL http://tensorflow.org/. Software
available from tensorﬂow.org.

Andrychowicz, Marcin and Kurach, Karol. Learning ef-
ﬁcient algorithms with hierarchical attentive memory.
arXiv preprint arXiv:1602.03218, 2016.

Bengio, Yoshua, Louradour, J´erˆome, Collobert, Ronan, and
Weston, Jason. Curriculum learning. In Proceedings of
the 26th Annual International Conference on Machine
Learning (ICML), pp. 41–48, 2009.

Bunel, Rudy, Desmaison, Alban, Kohli, Pushmeet, Torr,
Philip H. S., and Kumar, M. Pawan. Adaptive neu-
ral compilation. CoRR, abs/1605.07969, 2016. URL
http://arxiv.org/abs/1605.07969.

Caruana, Rich. Multitask learning. Machine Learning, 28:

41–75, 1997.

Chen, Zhiyuan, Ma, Nianzu, and Liu, Bing. Lifelong learn-
In Proceedings of the
ing for sentiment classiﬁcation.
53rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pp. 750–756, 2015.

Feser, John K., Brockschmidt, Marc, Gaunt, Alexander L.,
and Tarlow, Daniel. Neural functional programming.
2016. Submitted to ICLR 2017.

Gaunt, Alexander L., Brockschmidt, Marc, Singh, Rishabh,
Kushman, Nate, Kohli, Pushmeet, Taylor, Jonathan, and
Tarlow, Daniel. Terpret: A probabilistic programming
language for program induction. CoRR, abs/1608.04428,
URL http://arxiv.org/abs/1608.
2016.
04428.

Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural
turing machines. CoRR, abs/1410.5401, 2014. URL
http://arxiv.org/abs/1410.5401.

Graves, Alex, Wayne, Greg, Reynolds, Malcolm, Harley,
Tim, Danihelka, Ivo, Grabska-Barwi´nska, Agnieszka,

Colmenarejo, Sergio G´omez, Grefenstette, Edward, Ra-
malho, Tiago, Agapiou, John, et al. Hybrid computing
using a neural network with dynamic external memory.
Nature, 2016.

Grefenstette, Edward, Hermann, Karl Moritz, Suleyman,
Mustafa, and Blunsom, Phil. Learning to transduce with
In Proceedings of the 28th Con-
unbounded memory.
ference on Advances in Neural Information Processing
Systems (NIPS), pp. 1828–1836, 2015.

Joulin, Armand and Mikolov, Tomas.

Inferring algorith-
mic patterns with stack-augmented recurrent nets.
In
Advances in Neural Information Processing Systems 2,
[NIPS Conference, Denver, Colorado, USA, November
27-30, 1989], pp. 190–198, 2015.

Kaiser, Łukasz and Sutskever, Ilya. Neural GPUs learn al-
gorithms. In Proceedings of the 4th International Con-
ference on Learning Representations (ICLR), 2016. URL
http://arxiv.org/abs/1511.08228.

Kumar, Abhishek and Daume III, Hal. Learning task group-
ing and overlap in multi-task learning. arXiv preprint
arXiv:1206.6417, 2012.

Kurach, Karol, Andrychowicz, Marcin, and Sutskever, Ilya.
Neural random-access machines. In Proceedings of the
4th International Conference on Learning Representa-
tions 2016, 2015. URL http://arxiv.org/abs/
1511.06392.

Luong, Minh-Thang, Le, Quoc V, Sutskever, Ilya, Vinyals,
Oriol, and Kaiser, Lukasz. Multi-task sequence to se-
quence learning. In International Conference on Learn-
ing Representations (ICLR), 2015.

McCloskey, Michael and Cohen, Neal J. Catastrophic inter-
ference in connectionist networks: The sequential learn-
ing problem. Psychology of learning and motivation, 24:
109–165, 1989.

Neelakantan, Arvind, Le, Quoc V., and Sutskever, Ilya. Neu-
ral programmer: Inducing latent programs with gradient
descent. In Proceedings of the 4th International Confer-
ence on Learning Representations 2016, 2016.

Pan, Sinno Jialin and Yang, Qiang. A survey on transfer
learning. IEEE Transactions on knowledge and data en-
gineering, 22(10):1345–1359, 2010.

Price, Eric, Zaremba, Wojciech, and Sutskever, Ilya. Exten-
sions and limitations of the neural GPU. 2016. Submitted
to ICLR 2017.

Ratcliff, Roger. Connectionist models of recognition mem-
ory: constraints imposed by learning and forgetting func-
tions. Psychological review, 97(2):285, 1990.

Differentiable Programs with Neural Libraries

Weston, Jason, Chopra, Sumit, and Bordes, Antoine. Mem-
ory networks. In Proceedings of the 3rd International
Conference on Learning Representations 2015, 2014.
URL http://arxiv.org/abs/1410.3916.

Zaremba, Wojciech, Mikolov, Tomas, Joulin, Armand, and
Fergus, Rob. Learning simple algorithms from examples.
In Proceedings of the 33nd International Conference on
Machine Learning, ICML 2016, pp. 421–429, 2016.

Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Recht, Ben-
jamin, and Vinyals, Oriol. Understanding deep learning
requires rethinking generalization. In International Con-
ference on Learning Representations, 2017.

Reed, Scott E. and de Freitas, Nando. Neural programmer-

interpreters. 2016.

Riedel, Sebastian, Bosnjak, Matko, and Rockt¨aschel, Tim.
Programming with a differentiable forth interpreter.
CoRR, abs/1605.06640, 2016. URL http://arxiv.
org/abs/1605.06640.

Robins, Anthony. Catastrophic forgetting, rehearsal and
pseudorehearsal. Connection Science, 7(2):123–146,
1995.

Rusu, Andrei A, Rabinowitz, Neil C, Desjardins, Guillaume,
Soyer, Hubert, Kirkpatrick, James, Kavukcuoglu, Koray,
Pascanu, Razvan, and Hadsell, Raia. Progressive neural
networks. arXiv preprint arXiv:1606.04671, 2016.

Ruvolo, Paul and Eaton, Eric. Ella: An efﬁcient lifelong

learning algorithm. ICML (1), 28:507–515, 2013.

Shultz, Thomas R and Rivest, Francois. Knowledge-based
cascade-correlation: Using knowledge to speed learning.
Connection Science, 13(1):43–72, 2001.

Silver, Daniel L and Mercer, Robert E. The task rehearsal
method of life-long learning: Overcoming impoverished
data. In Conference of the Canadian Society for Com-
putational Studies of Intelligence, pp. 90–101. Springer,
2002.

Silver, Daniel L and Poirier, Ryan. Machine life-long learn-

ing with csmtl networks. In AAAI, 2006.

Silver, Daniel L, Yang, Qiang, and Li, Lianghao. Lifelong
machine learning systems: Beyond learning algorithms.
In AAAI Spring Symposium: Lifelong Machine Learning,
pp. 49–55, 2013.

Stallkamp, Johannes, Schlipsing, Marc, Salmen, Jan, and
Igel, Christian.
The German Trafﬁc Sign Recogni-
tion Benchmark: A multi-class classiﬁcation competition.
In IEEE International Joint Conference on Neural Net-
works, pp. 1453–1460, 2011.

Thrun, Sebastian. A lifelong learning perspective for mobile
robot control. In Proceedings of IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pp.
23–30, 1994.

Thrun, Sebastian. Is learning the n-th thing any easier than
learning the ﬁrst? In Advances in Neural Information
Processing Systems 8 (NIPS), pp. 640–646, 1995.

Thrun, Sebastian and O’Sullivan, Joseph. Discovering struc-
ture in multiple learning tasks: The TC algorithm.
In
Machine Learning, Proceedings of the Thirteenth Inter-
national Conference (ICML), pp. 489–497, 1996.

