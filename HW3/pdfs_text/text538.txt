Differentially Private Ordinary Least Squares

Or Sheffet 1

Abstract
Linear regression is one of the most prevalent
techniques in machine learning; however, it is
also common to use linear regression for its ex-
planatory capabilities rather than label predic-
tion. Ordinary Least Squares (OLS) is often used
in statistics to establish a correlation between
an attribute (e.g. gender) and a label (e.g.
in-
come) in the presence of other (potentially corre-
lated) features. OLS assumes a particular model
that randomly generates the data, and derives t-
values — representing the likelihood of each real
value to be the true correlation. Using t-values,
OLS can release a conﬁdence interval, which is
an interval on the reals that is likely to contain
the true correlation; and when this interval does
not intersect the origin, we can reject the null hy-
pothesis as it is likely that the true correlation
is non-zero. Our work aims at achieving sim-
ilar guarantees on data under differentially pri-
vate estimators. First, we show that for well-
spread data, the Gaussian Johnson-Lindenstrauss
Transform (JLT) gives a very good approxima-
tion of t-values; secondly, when JLT approxi-
mates Ridge regression (linear regression with
l2-regularization) we derive, under certain con-
ditions, conﬁdence intervals using the projected
data; lastly, we derive, under different conditions,
conﬁdence intervals for the “Analyze Gauss” al-
gorithm (Dwork et al., 2014).

1. Introduction

Since the early days of differential privacy, its main goal
was to design privacy preserving versions of existing tech-
niques for data analysis. It is therefore no surprise that sev-
eral of the ﬁrst differentially private algorithms were ma-
chine learning algorithms, with a special emphasis on the
ubiquitous problem of linear regression (Kasiviswanathan

1Computing Science Dept., University of Alberta, Edmonton
AB, Canada. This work was done when the author was at Har-
vard University, supported by NSF grant CNS-123723. Corre-
spondence to: Or Sheffet <osheffet@ualberta.ca>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

et al., 2008; Chaudhuri et al., 2011; Kifer et al., 2012; Bass-
ily et al., 2014). However, all existing body of work on
differentially private linear regression measures utility by
bounding the distance between the linear regressor found
by the standard non-private algorithm and the regressor
found by the privacy-preserving algorithm. This is moti-
vated from a machine-learning perspective, since bounds
on the difference in the estimators translate to error bounds
on prediction (or on the loss function). Such bounds are
(highly) interesting and non-trivial, yet they are of little use
in situations where one uses linear regression to establish
correlations rather than predict labels.

In the statistics literature, Ordinary Least Squares (OLS)
is a technique that uses linear regression in order to infer
the correlation between a variable and an outcome, espe-
cially in the presence of other factors. And so, in this pa-
per, we draw a distinction between “linear regression,” by
which we refer to the machine learning technique of ﬁnding
a speciﬁc estimator for a speciﬁc loss function; and “Ordi-
nary Least Squares,” by which we refer to the statistical in-
ference done assuming a speciﬁc model for generating the
data and that uses linear regression. Many argue that OLS
is the most prevalent technique in social sciences (Agresti
& Finlay, 2009). Such works make no claim as to the la-
bels of a new unlabeled batch of samples. Rather they aim
to establish the existence of a strong correlation between
the label and some feature. Needless to say, in such works,
the privacy of individuals’ data is a concern.

In order to determine that a certain variable xj is positively
(resp. negatively) correlated with an outcome y, OLS as-
sumes a model where the outcome y is a noisy version of
a linear mapping of all variables: y = βββ · xxx + e (with e
denoting random Gaussian noise) for some predetermined
and unknown βββ. Then, given many samples (xxxi, yi) OLS
establishes two things: (i) when ﬁtting a linear function
to best predict y from xxx over the sample (via computing
ˆβββ = (cid:0)(cid:80)
i yixxxi)) the coefﬁcient ˆβj is positive
(resp. negative); and (ii) inferring, based on ˆβj, that the
true βj is likely to reside in R>0 (resp. R<0). In fact, the
crux in OLS is by describing βj using a probability distri-
bution over the reals, indicating where βj is likely to fall,
derived by computing t-values. These values take into ac-
count both the variance in the data as well as the variance of
the noise e.1 Based on this probability distribution one can

i xxxixxxT
i

((cid:80)

(cid:1)−1

1For example, imagine we run linear regression on a certain

Differentially Private Ordinary Least Squares

deﬁne the α-conﬁdence interval — an interval I centered
at ˆβj whose likelihood to contain βj is 1 − α. Of particular
importance is the notion of rejecting the null-hypothesis,
where the interval I does not contain the origin, and so
one is able to say with high conﬁdence that βj is positive
(resp. negative). Further details regarding OLS appear in
Section 2.

In this work we give the ﬁrst analysis of statistical infer-
ence for OLS using differentially private estimators. We
emphasize that the novelty of our work does not lie in the
differentially-private algorithms, which are, as we discuss
next, based on the Johnson-Lindenstrauss Transform (JLT)
and on additive Gaussian noise and are already known to
be differentially private (Blocki et al., 2012; Dwork et al.,
2014). Instead, the novelty of our work lies in the analy-
ses of the algorithms and in proving that the output of the
algorithms is useful for statistical inference.

The Algorithms. Our ﬁrst algorithm (Algorithm 1) is an
adaptation of Gaussian JLT. Proving that this adaptation
remains ((cid:15), δ)-differentially private is straightforward (the
proof appears in Appendix A.1). As described, the al-
gorithm takes as input a parameter r (in addition to the
other parameters of the problem) that indicates the number
of rows in the JL-matrix. Later, we analyze what should
one set as the value of r. Our second algorithm is taken

Algorithm 1 Outputting a private Johnson-Lindenstrauss
projection of a matrix.

Input: A matrix A ∈ Rn×d and a bound B > 0 on the
l2-norm of any row in A.
Privacy parameters: (cid:15), δ > 0.
Parameter r indicating the number of rows in the result-
ing matrix.
Set w s.t. w2 = 8B2
(cid:15)
Sample Z ∼ Lap(4B2/(cid:15)) and let σmin(A) denote the
smallest singular value of A.
if σmin(A)2 > w2 + Z + 4B2 ln(1/δ)

(cid:16)(cid:112)2r ln(8/δ) + 2 ln(8/δ)

then

(cid:17)

.

(cid:15)

Sample a (r×n)-matrix R whose entries are i.i.d sam-
ples from a normal Gaussian.
return RA and “matrix unaltered”.

else

end if

Let A(cid:48) denote the result of appending A with the d×d-
matrix wId×d.
Sample a (r × (n + d))-matrix R whose entries are
i.i.d samples from a normal Gaussian.
returnRA(cid:48) and “matrix altered”.

(X, yyy) which results in a vector ˆβββ with coordinates ˆβ1 = ˆβ2 =
0.1. Yet while the column X1 contains many 1s and (−1)s, the
column X2 is mostly populated with zeros.
In such a setting,
OLS gives that it is likely to have β1 ≈ 0.1, whereas no such
guarantees can be given for β2.

verbatim from the work of Dwork et al (2014). We de-

Algorithm 2 “Analyze Gauss” Algorithm of Dwork et
al (2014).

Input: A matrix A ∈ Rn×d and a bound B > 0 on the
l2-norm of any row in A.
Privacy parameters: (cid:15), δ > 0.
N ← symmetric (d × d)-matrix with upper triangle en-
(cid:16)
tries sampled i.i.d from N
return ATA + N .

0, 2B4 ln(2/δ)
(cid:15)2

(cid:17)

.

liberately focus on algorithms that approximate the 2nd-
moment matrix of the data and then run hypothesis-testing
by post-processing the output, for two reasons. First, they
enable sharing of data2 and running unboundedly many
hypothesis-tests. Since, we do not deal with OLS based
on the private single-regression ERM algorithms (Chaud-
huri et al., 2011; Bassily et al., 2014) as such inference re-
quires us to use the Fisher-information matrix of the loss
function — but these algorithms do not minimize a private
loss-function but rather prove that outputting the minimizer
of the perturbed loss-function is private. This means that
differentially-private OLS based on these ERM algorithms
requires us to devise new versions of these algorithms,
making this a second step in this line of work... (After ﬁrst
understanding what we can do using existing algorithms.)
We leave this approach — as well as performing private hy-
pothesis testing using a PTR-type algorithm (Dwork & Lei,
2009) (output merely reject / don’t-reject decision with-
out justiﬁcation), or releasing only relevant tests judging
by their p-values (Dwork et al., 2015) — for future work.

Our Contribution and Organization. We analyze the
performances of our algorithms on a matrix A of the form
A = [X; yyy], where each coordinate yi is generated accord-
ing to the homoscedastic model with Gaussian noise, which
is a classical model in statistics. We assume the existence
of a vector βββ s.t. for every i we have yi = βββTxxxi + ei and
ei is sampled i.i.d from N (0, σ2).3

We study the result of running Algorithm 1 on such data
in the two cases: where A wasn’t altered by the algorithm
and when A was appended by the algorithm. In the former
case, Algorithm 1 boils down to projecting the data under
a Gaussian JLT. Sarlos (2006) has already shown that the
JLT is useful for linear regression, yet his work bounds
the l2-norm of the difference between the estimated re-

2Researcher A collects the data and uses the approximation of
the 2nd-moment matrix to test some OLS hypothesis; but once
the approximation is published researcher B can use it to test for
a completely different hypothesis.

3This model may seem objectionable. Assumptions like the
noise independence, 0-meaned or sampled from a Gaussian dis-
tribution have all been called into question in the past. Yet due to
the prevalence of this model we see ﬁt to initiate the line of work
on differentially private Least Squares with this Ordinary model.

Differentially Private Ordinary Least Squares

gression before and after the projection. Following Sarlos’
work, other works in statistics have analyzed compressed
linear regression (Zhou et al., 2007; Pilanci & Wainwright,
2014a;b). However, none of these works give conﬁdence
intervals based on the projected data, presumably for three
reasons. Firstly, these works are motivated by computa-
tional speedups, and so they use fast JLT as opposed to our
analysis which leverages on the fact that our JL-matrix is
composed of i.i.d Gaussians. Secondly, the focus of these
works is not on OLS but rather on newer versions of linear
regression, such as Lasso or when βββ lies in some convex
set. Lastly, it is evident that the smallest conﬁdence inter-
val is derived from the data itself. Since these works do
not consider privacy applications, (actually, (Zhou et al.,
2007; Pilanci & Wainwright, 2014a) do consider privacy
applications of the JLT, but quite different than differential
privacy) they assume the analyst has access to the data it-
self, and so there was no need to give conﬁdence intervals
for the projected data. Our analysis is therefore the ﬁrst, to
the best of our knowledge, to derive t-values — and there-
fore achieve all of the rich expressivity one infers from t-
values, such as conﬁdence bounds and null-hypotheses re-
jection — for OLS estimations without having access to X
itself. We also show that, under certain conditions, the sam-
ple complexity for correctly rejecting the null-hypothesis
increases from a certain bound N0 (without privacy) to a
bound of N0 + ˜O(
n ATA)/(cid:15)) with privacy (where
κ(M ) denotes the condition number of the matrix M .) This
appears in Section 3.

N0 · κ( 1

√

(cid:0)(cid:80)

In Section 4 we analyze the case Algorithm 1 does append
the data and the JLT is applied to A(cid:48). In this case, solving
the linear regression problem on the projected A(cid:48) approxi-
mates the solution for Ridge Regression (Tikhonov, 1963;
Hoerl & Kennard, 1970).
In Ridge Regression we aim
i(yi − zzzTxxxi)2 + w2(cid:107)zzz(cid:107)2(cid:1), which means
to solve minzzz
we penalize vectors whose l2-norm is large. In general, it
is not known how to derive t-values from Ridge regression,
and the literature on deriving conﬁdence intervals solely
from Ridge regression is virtually non-existent.
Indeed,
prior to our work there was no need for such calculations,
as access to the data was (in general) freely given, and so
deriving conﬁdence intervals could be done by appealing
back to OLS. We too are unable to derive approximated
t-values in the general case, but under additional assump-
tions about the data — which admittedly depend in part on
(cid:107)βββ(cid:107) and so cannot be veriﬁed solely from the data — we
show that solving the linear regression problem on RA(cid:48) al-
lows us to give conﬁdence intervals for βj, thus correctly
determining the correlation’s sign.

In Section 5 we discuss the “Analyze Gauss” algo-
rithm (Dwork et al., 2014) that outputs a noisy version of
a covariance of a given matrix using additive noise rather
than multiplicative noise. Empirical work (Xi et al., 2011)
shows that Analyze Gauss’s output might be non-PSD if
the input has small singular values, and this results in truly

bad regressors. Nonetheless, under additional conditions
(that imply that the output is PSD), we derive conﬁdence
bounds for Dwork et al’s “Analyze Gauss” algorithm. Fi-
nally, in Section 6 we experiment with the heuristic of
computing the t-values directly from the outputs of Algo-
rithms 1 and 2. We show that Algorithm 1 is more “con-
servative” than Algorithm 2 in the sense that it tends to
not reject the null-hypothesis until the number of exam-
ples is large enough to give a very strong indication of re-
jection. In contrast, Algorithm 2 may wrongly rejects the
null-hypothesis even when it is true.

Discussion. Some works have already looked at the in-
tersection of differentially privacy and statistics (Dwork &
Lei, 2009; Smith, 2011; Chaudhuri & Hsu, 2012; Duchi
et al., 2013; Dwork et al., 2015) (especially focusing on ro-
bust statistics and rate of convergence). But only a handful
of works studied the signiﬁcance and power of hypotheses
testing under differential privacy, without arguing that the
noise introduced by differential privacy vanishes asymp-
totically (Vu & Slavkovic, 2009; Uhler et al., 2013; Wang
et al., 2015; Rogers et al., 2016). These works are exper-
imentally promising, yet they (i) focus on different statis-
tical tests (mostly Goodness-of-Fit and Independence test-
ing), (ii) are only able to prove results for the case of simple
hypothesis-testing (a single hypothesis) with an efﬁcient
data-generation procedure through repeated simulations —
a cumbersome and time consuming approach. In contrast,
we deal with a composite hypothesis (we simultaneously
reject all βββs with sign(βj) (cid:54)= sign( ˆβj)) by altering the
conﬁdence interval (or the critical region).

One potential reason for avoiding conﬁdence-interval anal-
ysis for differentially private hypotheses testing is that it
does involve re-visiting existing results. Typically, in sta-
tistical inference the sole source of randomness lies in the
underlying model of data generation, whereas the estima-
tors themselves are a deterministic function of the dataset.
In contrast, differentially private estimators are inherently
random in their computation. Statistical inference that con-
siders both the randomness in the data and the randomness
in the computation is highly uncommon, and this work, to
the best of our knowledge, is the ﬁrst to deal with random-
ness in OLS hypothesis testing. We therefore strive in our
analysis to separate the two sources of randomness — as
in classic hypothesis testing, we use α to denote the bound
on any bad event that depends solely on the homoscedas-
tic model, and use ν to bound any bad event that depends
on the randomized algorithm.4 (Thus, any result which is
originally of the form “α-reject the null-hypothesis” is now
converted into a result “(α+ν)-reject the null hypothesis”.)

4Or any randomness in generating the feature matrix X which
standard OLS theory assumes to be ﬁxed, see Theorems 2.2
and 3.3.

Differentially Private Ordinary Least Squares

2. Preliminaries and OLS Background

Notation. Throughout this paper, we use lower-case let-
ters to denote scalars (e.g., yi or ei); boldboldbold characters to
denote vectors; and UPPER-case letters to denote matri-
ces. The l-dimensional all zero vector is denoted 000l, and
the l × m-matrix of all zeros is denoted 0l×m. We use eee
to denote the speciﬁc vector yyy − Xβββ in our model; and
though the reader may ﬁnd it a bit confusing but hopefully
clear from the context — we also use eeej and eeek to denote
elements of the natural basis (unit length vector in the di-
rection of coordinate j or k). We use (cid:15), δ to denote the pri-
vacy parameters of Algorithms 1 and 2, and use α and ν to
denote conﬁdence parameters (referring to bad events that
hold w.p. ≤ α and ≤ ν resp.) based on the homoscedastic
model or the randomized algorithm resp. We also stick to
the notation from Algorithm 1 and use w to denote the posi-
(cid:17)
(cid:16)(cid:112)2r ln(8/δ) + ln(8/δ)
tive scalar for which w2 = 8B2
(cid:15)
throughout this paper. We use standard notation for SVD
composition of a matrix (M = U ΣV T), its singular values
and its Moore-Penrose inverse (M +).

The Gaussian distribution.
A univariate Gaussian
N (µ, σ2) denotes the Gaussian distribution whose mean
is µ and variance σ2. Standard concentration bounds on
Gaussians give that Pr[x > µ + 2σ(cid:112)ln(2/ν)] < ν
for any ν ∈ (0, 1
e ). A multivariate Gaussian N (µµµ, Σ)
for some positive semi-deﬁnite Σ denotes the multivari-
ate Gaussian distribution where the mean of the j-th co-
ordinate is the µj and the covariance between coordinates
j and k is Σj,k. The PDF of such Gaussian is deﬁned
only on the subspace colspan(Σ). A matrix Gaussian dis-
tribution, denoted N (Ma×b, Ia×a, V ) has mean M , in-
dependence among its rows and variance V for each of
its columns. We also require the following property of
Gaussian random variables: Let X and Y be two random
Gaussians s.t. X ∼ N (0, σ2) and Y ∼ N (0, λ2) where
1 ≤ σ2
λ2 ≤ c2 for some c, then for any S ⊂ R we have
1
c Prx←Y [x ∈ S] ≤ Prx←X [x ∈ S] ≤ cPrx←Y [x ∈ S/c]
(see Proposition A.2).

Additional Distributions. We denote by Lap(σ) the
Laplace distribution whose mean is 0 and variance is 2σ2.
The χ2
k-distribution, where k is referred to as the de-
grees of freedom of the distribution, is the distribution
over the l2-norm squared of the sum of k independent
normal Gaussians. That is, given i.i.d X1, . . . , Xk ∼
def= (X1, X2, . . . , Xk) ∼
N (0, 1) it holds that ζζζ
N (000k, Ik×k), and (cid:107)ζζζ(cid:107)2 ∼ χ2
k. Existing tail bounds on
the χ2
k distribution (Laurent & Massart, 2000) give that
(cid:104)
(cid:107)ζζζ(cid:107)2 ∈ (
The Tk-
Pr
distribution, where k is referred to as the degrees of free-
dom of the distribution, denotes the distribution over the
reals created by independently sampling Z ∼ N (0, 1) and

k ± (cid:112)2 ln(2/ν))2(cid:105)

≥ 1 − ν.

√

k, and taking the quantity Z/(cid:112)(cid:107)ζ(cid:107)2/k. It is a
(cid:107)ζ(cid:107)2 ∼ χ2
k→∞→ N (0, 1), thus it is a common prac-
known fact that Tk
tice to apply Gaussian tail bounds to the Tk-distribution
when k is sufﬁciently large.

Differential Privacy. In this work, we deal with input in
the form of a n × d-matrix with each row bounded by a
l2-norm of B. Two inputs A and A(cid:48) are called neighbors if
they differ on a single row.
Deﬁnition 2.1 ((Dwork et al., 2006a)). An algorithm ALG
which maps (n × d)-matrices into some range R is ((cid:15), δ)-
differential privacy it holds that Pr[ALG(A) ∈ S] ≤
e(cid:15)Pr[ALG(A(cid:48)) ∈ S] + δ for all neighboring inputs A and
A(cid:48) and all subsets S ⊂ R.

Background on OLS. For the unfamiliar reader, we give
here a very brief overview of the main points in OLS. Fur-
ther details, explanations and proofs appear in Section A.3.

We are given n observations {(xxxi, yi)}n
i=1 where ∀i, xxxi ∈
Rp and yi ∈ R. We assume the existence of βββ ∈ Rp s.t.
the label yi was derived by yi = βββTxxxi + ei where ei ∼
N (0, σ2) independently (also known as the homoscedastic
Gaussian model). We use the matrix notation where X de-
notes the (n × p)- feature matrix and yyy denotes the labels.
We assume X has full rank.

The parameters of the model are therefore βββ and σ2, which
we set to discover. To that end, we minimize minzzz (cid:107)yyy −
Xzzz(cid:107)2 and have

ˆβββ = (X TX)−1X Tyyy = (X TX)−1X T(Xβββ + eee) = βββ + X +eee (1)
ζζζ = yyy − X ˆβββ = (Xβββ + eee) − X(βββ + X +eee) = (I − XX +)eee (2)

And then for any coordinate j the t-value, which
def=
is
is

the quantity t(βj)

,

ˆβj −βj
(X TX)−1
to Tn−p-distribution.

(cid:113)

j,j · (cid:107)ζζζ(cid:107)

√

n−p

(cid:105)

Pr

distributed

= (cid:82)
S

according
(cid:104)ˆβββ and ζζζ satisfying t(βj) ∈ S

I.e.,
PDFTn−p (x)dx
for any measurable S ⊂ R. Thus t(βj) describes the
likelihood of any βj — for any z ∈ R we can now give
an estimation of how likely it is to have βj = z (which
is PDFTn−p (t(z))), and this is known as t-test for the
value z. In particular, given 0 < α < 1, we denote cα
as the number for which the interval (−cα, cα) contains
a probability mass of 1 − α from the Tn−p-distribution.
And so we derive a corresponding conﬁdence interval Iα
centered at ˆβj where βj ∈ Iα with conﬁdence of level of
1 − α.

√

ˆβj
(cid:113)

n−p
(X TX)−1
j,j

Of particular importance is the quantity t0

def= t(0) =
,since if there is no correlation between xj

(cid:107)ζζζ(cid:107)
and y then the likelihood of seeing ˆβj depends on the ratio
of its magnitude to its standard deviation. As mentioned
k→∞→ N (0, 1), then rather than viewing
earlier, since Tk

Differentially Private Ordinary Least Squares

2π

1√

this t0 as sampled from a Tn−p-distribution, it is common
to think of t0 as a sample from a normal Gaussian N (0, 1).
This allows us to associate t0 with a p-value, estimating the
event “βj and ˆβj have different signs.” Speciﬁcally, given
α ∈ (0, 1/2), we α-reject the null hypothesis if p0 < α.
Let τα be the number s.t. Φ(τα) = (cid:82) ∞
e−x2/2dx = α.
τα
This means we α-reject the null hypothesis when |t0| > τα.
We now lower bound the number of i.i.d sample points
needed in order to α-reject the null hypothesis. This bound
is our basis for comparison between standard OLS and the
differentially private version.5
Theorem 2.2. Fix any positive deﬁnite matrix Σ ∈ Rp×p
2 ). Fix parameters βββ ∈ Rp and σ2
and any ν ∈ (0, 1
(cid:54)= 0. Let X be a ma-
and a coordinate j s.t. βj
trix whose n rows are i.i.d samples from N (000, Σ), and
yyy be a vector where yi − (Xβββ)i is sampled i.i.d from
N (0, σ2). Fix α ∈ (0, 1). Then w.p. ≥ 1 − α − ν we
have that OLS’s (1 − α)-conﬁdence interval has length
(cid:112)σ2/(nσmin(Σ))) provided n ≥ C1(p + ln(1/ν))
O(cα
for some sufﬁciently large constant C1. Furthermore,
there exists a constant C2 such that w.p. ≥ 1 − α − ν
OLS (correctly) rejects the null hypothesis provided n ≥
, where cα
max
is the number for which (cid:82) cα
−cα

α+τ 2
· c2
α
σmin(Σ)
PDFTn−p (x)dx = 1 − α.

C1(p + ln(1/ν)), p + C2

σ2
β2
j

(cid:111)

(cid:110)

3. OLS over Projected Data

In this section we deal with the output of Algorithm 1
in the special case where Algorithm 1 outputs matrix
unaltered and so we work with RA.

To clarify, the setting is as follows. We denote A = [X; yyy]
the column-wise concatenation of the (n × (d − 1))-matrix
X with the n-length vector yyy.
(Clearly, we can denote
any column of A as yyy and any subset of the remaining
columns as the matrix X.) We therefore denote the output
RA = [RX; Ryyy] and for simplicity we denote M = RX
and p = d − 1. We denote the SVD decomposition of
X = U ΣV T. So U is an orthonormal basis for the column-
span of X and as X is full-rank V is an orthonormal basis
for Rp. Finally, in our work we examine the linear regres-
sion problem derived from the projected data. That is, we
denote
˜βββ = (X TRTRX)−1(RX)T(Ryyy) = βββ + (RX)+Reee

(3)

˜σ2 =

r
r − p

(cid:107)˜ζζζ(cid:107)2 , with

˜ζζζ = 1√

r Ryyy − 1√

r (RX)˜βββ

(4)

We now give our main theorem, for estimating the t-values
based on ˜βββ and ˜σ.

5Theorem 2.2 also illustrates how we “separate” the two
sources of privacy. In this case, ν bounds the probability of bad
events that depend to sampling the rows of X, and α bounds the
probability of a bad event that depends on the sampling of the yyy
coordinates.

Theorem 3.1. Let X be a (n × p)-matrix, and parame-
ters βββ ∈ Rp and σ2 are such that we generate the vector
yyy = Xβββ + eee with each coordinate of eee sampled indepen-
dently from N (0, σ2). Assume Algorithm 1 projects the
matrix A = [X; yyy] without altering it. Fix ν ∈ (0, 1/2)
and r = p + Ω(ln(1/ν)). Fix coordinate j. Then we have
that w.p. ≥ 1 − ν deriving ˜βββ and ˜σ2 as in Equations (3)
and (4), the pivot quantity ˜t(βj) =
has a

˜βj −βj

(cid:113)

˜σ

(X TRTRX)−1
j,j

distribution D satisfying e−aPDFTr−p (x) ≤ PDFD(x) ≤
eaPDFTr−p (e−ax) for any x ∈ R, where we denote a =
r−p
n−p .

The implications of Theorem 3.1 are immediate: all esti-
mations one can do based on the t-values from the true data
X, yyy, we can now do based on ˜t modulo an approximation
factor of exp( r−p
n−p ). In particular, Theorem 3.1 enables us
to deduce a corresponding conﬁdence interval based on ˜βββ.
Corollary 3.2. In the same setting as in Theorem 3.1, w.p.
≥ 1 − ν we have the following. Fix any α ∈ (0, 1
2 ). Let ˜cα
denote the number s.t. the interval (˜cα, ∞) contains α
2 e−a
probability mass of the Tr−p-distribution. Then Pr[βj ∈
(cid:16) ˜βj ± ea · ˜cα · ˜σ

(X TRTRX)−1
j,j

] ≥ 1 − α. 6

(cid:113)

(cid:17)

We compare the conﬁdence interval of Corollary 3.2
to the conﬁdence interval of the standard OLS model,
j,j . As R is a JL-
whose length is cα
matrix, known results regarding the JL transform give
that (cid:107)˜ζζζ(cid:107) = Θ ((cid:107)ζζζ(cid:107)), and that
j,j =

(r − p)(X TRTRX)−1

(X TX)−1

(cid:107)ζζζ(cid:107)
n−p

(cid:113)

(cid:113)

√

(X TX)−1
j,j

(cid:17)

.

(cid:16)(cid:113)

Θ

(cid:113)

We
= (cid:107)˜ζζζ(cid:107)
(cid:113)

√

therefore
(cid:113)

√

have

that

(cid:17)

=

(X TRTRX)−1
j,j

˜σ
(cid:113) r·(n−p)

(r−p)2 · Θ
for which r

(X TRTRX)−1
j,j
(cid:16) (cid:107)ζζζ(cid:107)
√
n−p

r
r−p
(X TX)−1
. So for values of r
j,j
r−p = Θ(1) we get that the conﬁdence interval
(cid:16) ˜cα
of Theorem 3.1 is a factor of Θ
-larger than
cα
the standard OLS conﬁdence interval. Observe that when
α = Θ(1), which is the common case, the dominating
factor is (cid:112)(n − p)/(r − p). This bound intuitively makes
sense: we have contracted n observations to r observa-
tions, hence our model is based on conﬁdence intervals
derived from Tr−p rather than Tn−p.

(cid:113) n−p
r−p

(cid:17)

In the supplementary material we give further discussion,
in which we compare our work to the more straight-forward
bounds one gets by “plugging in” Sarlos’ work (2006); and
we also compare ourselves to the bounds derived from al-
ternative works in differentially private linear regression.

6Moreover,

note ˜dα s.t
ability mass of
(cid:113)
(cid:16) ˜βj ± ˜dα · ˜σ

this

interval

is
the interval ( ˜dα, ∞) contains α
2 e

essentially optimal:
r−p
n−p prob-
Then Pr[βj ∈

the Tr−p-distribution.

de-

(X TRTRX)−1
j,j

] ≤ 1 − α.

(cid:17)

Differentially Private Ordinary Least Squares

Rejecting the Null Hypothesis. Due to Theorem 3.1,
we can mimic OLS’ technique for rejecting the null hy-
and re-
pothesis.

I.e., we denote ˜t0 =

˜βj

(cid:113)

˜σ

(X TRTRX)−1
j,j

ject the null-hypothesis if indeed the associated ˜p0, denot-
r−p
ing p-value of the slightly truncated e−
n−p ˜t0, is below
r−p
α · e−
n−p . Much like Theorem 2.2 we now establish a
lower bound on n so that w.h.p we end up (correctly) re-
jecting the null-hypothesis.

Theorem 3.3. Fix a positive deﬁnite matrix Σ ∈ Rp×p.
Fix parameters βββ ∈ Rp and σ2 > 0 and a coordinate j
s.t. βj (cid:54)= 0. Let X be a matrix whose n rows are sampled
i.i.d from N (000p, Σ). Let yyy be a vector s.t. yi − (Xβββ)i
is sampled i.i.d from N (0, σ2). Fix ν ∈ (0, 1/2) and
α ∈ (0, 1/2). Then there exist constants C1, C2, C3
and C4 such that when we run Algorithm 1 over [X; yyy]
with parameter r w.p. ≥ 1 − α − ν we (correctly)
reject
the null hypothesis using ˜p0 (i.e., Algorithm 1
returns matrix unaltered and we can estimate ˜t0
r−p
indeed ˜p0 < α · e−
n−p ) provided
and verify that
(cid:111)
σ2(˜c2
α+˜τ 2
α)
, and n ≥
j σmin(Σ) , C2 ln(1/ν)
C1
r ≥ p + max
β2
(cid:111)
min{σmin(Σ),σ2} , C4p ln(1/ν)

r, C3

where

max

w2

(cid:110)

(cid:110)

˜cα, ˜τα deﬁned s.t. PrX∼Tr−p [X > ˜cα/e

PrX∼N (0,1)[X > ˜τα/e

r−p
n−p ] = α

2 e−

r−p
n−p .

r−p
n−p ] =

3.1. Setting the Value of r, Deriving a Bound on n

(cid:17)

β2

(cid:16) (˜cα+˜τα)2σ2
j σmin(Σ)

Comparing the lower bound on n given by Theorem 3.3 to
the bound of Theorem 2.2, we have that the data-dependent
bound of Ω
should now hold for r rather than
n. Yet, Theorem 3.3 also introduces an additional depen-
dency between n and r: we require n = Ω( w2
σmin(Σ) )
(since otherwise we do not have σmin(A) (cid:29) w and Algo-
rithm 1 might alter A before projecting it) and by deﬁnition
w2 is proportional to (cid:112)r ln(1/δ)/(cid:15). This is precisely the
focus of our discussion in this subsection. We would like
to set r’s value as high as possible — the larger r is, the
more observations we have in RA and the better our conﬁ-
dence bounds (that depend on Tr−p) are — while satisfying
n = Ω(

σ2 + w2

√
r
(cid:15)·min{σ2,σmin(Σ)} ).
that

if each sample point

is drawn i.i.d xxx ∼
Recall
N (000p, Σ), then each sample (xxxi ◦ yi) is sampled from
N (000p+1, ΣA) for ΣA deﬁned in the proof of Theorem 3.3,

that is: ΣA =

(cid:18)

Σ
βββTΣ

(cid:19)

Σβββ
σ2+βββTΣβββ

rem 3.3 gives the lower bound r − p = Ω
and the following lower bounds on n: n ≥ r and

n = Ω

√

(cid:18)

B2(

r ln(1/δ)+ln(1/δ))
(cid:15)σmin(ΣA)

(cid:19)

, which means r =

. So, Theo-
(cid:16) σ2(˜cα+˜τα)2
j σmin(Σ)

β2

(cid:17)

min(ΣA)

(cid:110)
n, (cid:15)2σ2

B4 ln(1/δ) (n − ln(1/δ))2(cid:111)
min
minates in the following corollary.
Corollary 3.4. Denoting (cid:94)LB2.2 = σ2(˜cα+˜τα)2

. This discussion cul-

thus conclude that if n − p ≥ Ω

(cid:18)

Ω

B2 ln(1/δ)
(cid:15)σmin(ΣA) ·

(cid:113)

(cid:19)

(cid:94)LB2.2

holds by setting r = min

, then the result of Theorem 3.3
B4 ln(1/δ) (n − ln(1/δ))2(cid:111)
(cid:110)
n, (cid:15)2σ2
.

min(ΣA)

j σmin(Σ) , we
β2
(cid:17)
and n =

(cid:16)(cid:94)LB2.2

It is interesting to note that when we know ΣA, we also
have a bound on B. Recall ΣA,
the variance of the
Gaussian (xxx ◦ y). Since every sample is an independent
draw from N (000p+1, ΣA) then we have an upper bound of
B2 ≤ log(np)σmax(ΣA). So our lower bound on n (using
κ(ΣA) to denote the condition number of ΣA) is given by
(cid:19)(cid:27)
(cid:18)

(cid:26)

(cid:113)

n ≥ max

(cid:16)(cid:94)LB2.2

(cid:17)

Ω

, ˜Ω

κ(ΣA) ln(1/δ)
(cid:15)

·

(cid:94)LB2.2

.

Observe, overall this result is similar in nature to many
other results in differentially private learning (Bassily et al.,
2014) which are of the form “without privacy, in order to
achieve a total loss of ≤ η we have a sample complexity
bound of some Nη; and with differential privacy the sam-
ple complexity increases to Nη + Ω((cid:112)Nη/(cid:15)).” However,
there’s a subtlety here worth noting. (cid:94)LB2.2 is proportional
σmin(ΣA) . The additional
to
dependence on σmax follows from the fact that differential
privacy adds noise proportional to the upper bound on the
norm of each row.

σmin(ΣA) but not to κ(ΣA) = σmax(ΣA)

1

4. Projected Ridge Regression

We now turn to deal with the case that our matrix does not
pass the if-condition of Algorithm 1. In this case, the ma-
trix is appended with a d × d-matrix which is wId×d. De-
noting A(cid:48) =

we have that the algorithm’s

(cid:20)

(cid:21)

A
w · Id×d

output is RA(cid:48). Similarly to before, we are going to denote
d = p + 1 and decompose A = [X; yyy] with X ∈ Rn×p and
yyy ∈ Rn, with the standard assumption of yyy = Xβββ + eee and
ei sampled i.i.d from N (0, σ2). We now need to introduce
some additional notation. We denote the appended matrix
and vectors X (cid:48) and yyy(cid:48) s.t. A(cid:48) = [X (cid:48); yyy(cid:48)]. And so, using the
output RA(cid:48) of Algorithm 1, we solve the linear regression
problem derived from 1√
r RX (cid:48) and 1√

r Ryyy(cid:48). I.e., we set

βββ(cid:48) = (X (cid:48)TRTRX (cid:48))−1(RX (cid:48))T(Ryyy(cid:48))
ζζζ (cid:48) = 1√

r (Ryyy(cid:48) − RX (cid:48)βββ(cid:48))

(5)

Sarlos’ results (2006) regarding the Johnson Linden-
strauss transform give that, when R has sufﬁciently
many rows, solving the latter optimization problem gives
a good approximation for
the opti-
mization problem βββR = arg minzzz (cid:107)yyy(cid:48) − X (cid:48)zzz(cid:107)2 =
(cid:0)(cid:107)yyy − Xzzz(cid:107)2 + w2(cid:107)zzz(cid:107)2(cid:1). The latter problem is
arg minzzz

the solution of

Differentially Private Ordinary Least Squares

known as the Ridge Regression problem. Invented in the
60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Re-
gression is often motivated from the perspective of penaliz-
ing linear vectors whose coefﬁcients are too large. It is also
often applied in the case where X doesn’t have full rank
or is close to not having full-rank: one can show that the
minimizer βββR = (X TX + w2Ip×p)−1X Tyyy is the unique
solution of the Ridge Regression problem and that the RHS
is always well-deﬁned.

While the solution of the Ridge Regression problem might
have smaller risk than the OLS solution, it is not known
how to derive t-values and/or reject the null hypothesis un-
der Ridge Regression (except for using X to manipulate
βββR back into ˆβββ = (X TX)−1X Tyyy and relying on OLS).
In fact, prior to our work there was no need for such analy-
sis! For conﬁdence intervals one could just use the standard
OLS, because access to X and yyy was given.

Therefore, much for the same reason, we are unable to de-
rive t-values under projected Ridge Regression.7 Clearly,
there are situations where such conﬁdence bounds simply
cannot be derived.Nonetheless, under additional assump-
tions about the data, our work can give conﬁdence intervals
for βj, and in the case where the interval doesn’t intersect
the origin — assure us that sign(β(cid:48)
j) = sign(βj) w.h.p.
This is detailed in the supplementary material.

To give an overview of our analysis, we ﬁrst discuss a
model where eee = yyy − Xβββ is ﬁxed (i.e., the data is ﬁxed
and the algorithm is the sole source of randomness), and
prove that in this model β(cid:48)β(cid:48)β(cid:48) is as an approximation to ˆβββ.
Theorem 4.1. Fix X ∈ Rn×p and yyy ∈ R. Deﬁne ˆβββ =
X +yyy and ζ = (I − XX +)yyy. Let RX (cid:48) = M (cid:48) and Ryyy(cid:48)
denote the result of applying Algorithm 1 to the matrix A =
[X; yyy] when the algorithm appends the data with a w · I
matrix. Fix a coordinate j and any α ∈ (0, 1/2). When
computing βββ(cid:48) and ζζζ (cid:48) as in (5), we have that w.p. ≥ 1 −
(cid:17)
α it holds that ˆβj ∈
r−p · (M (cid:48)TM (cid:48))−1
j ± c(cid:48)
β(cid:48)
α denotes the number such that (−c(cid:48)
where c(cid:48)
α) contains
1 − α mass of the Tr−p-distribution.

α(cid:107)ζζζ (cid:48)(cid:107)

(cid:113) r

α, c(cid:48)

j,j

(cid:16)

However, our goal remains to argue that β(cid:48)
j serves as a good
approximation for βj. To that end, we combine the stan-
dard OLS conﬁdence interval — which says that w.p. ≥
1 − α over the randomness of picking eee in the homoscedas-

(cid:114)

(X TX)−1
j,j

tic model we have |βj − ˆβj| ≤ cα(cid:107)ζζζ(cid:107)
n−p — with
the conﬁdence interval of Theorem 4.1 above, and denot-
(cid:107)ζζζ(cid:48)(cid:107)
(X TX)−1
r(M (cid:48)TM (cid:48))−1
ing I = cα
√
j,j
r−p
we have that Pr[|β(cid:48)
j − βj| = O(I)] ≥ 1 − α. And

j,j + c(cid:48)
α

(cid:107)ζζζ(cid:107)
n−p

(cid:113)

(cid:113)

√

7Note: The na¨ıve approach of using RX (cid:48) and Ryyy(cid:48) to interpo-
late RX and Ryyy and then apply Theorem 3.1 using these estima-
tions of RX and Ryyy ignores the noise added from appending the
matrix A into A(cid:48), and therefore leads to inaccurate estimations of
the t-values.

so, in summary, in Section C we give conditions under
which the length of the interval I is dominated by the
c(cid:48)
j,j factor derived from Theorem 4.1.
α

r(M (cid:48)TM (cid:48))−1

(cid:107)ζζζ(cid:48)(cid:107)
√
r−p

(cid:113)

5. Conﬁdence Intervals for “Analyze Gauss”

In this section we analyze the “Analyze Gauss” algorithm
of Dwork et al (2014). Algorithm 2 works by adding ran-
dom Gaussian noise to ATA, where the noise is symmetric
with each coordinate above the diagonal sampled i.i.d from
N (0, ∆2) with ∆2 = O
. Using the same no-
tation for a sub-matrix of A as [X; yyy] as before, we denote

B4 log(1/δ)

(cid:17)

(cid:16)

(cid:15)2

the output of Algorithm 2 as

(cid:94)
X TX (cid:103)X Tyyy

. Thus,















(cid:103)yyyTX (cid:103)yyyTyyy
(cid:17)−1
(cid:16)(cid:94)
X TX

(cid:103)X Tyyy and

we approximate βββ and (cid:107)ζζζ(cid:107) by (cid:101)βββ =
T (cid:94)
(cid:103)(cid:107)ζζζ(cid:107)2 = (cid:103)yyyTyyy − 2 (cid:93)yyyT X (cid:101)βββ + (cid:101)βββ
X TX (cid:101)βββ resp. We now argue
that it is possible to use (cid:101)βj and (cid:103)(cid:107)ζζζ(cid:107)2 to get a conﬁdence
interval for βj under certain conditions.
Theorem 5.1. Fix α, ν ∈ (0, 1
2 ). Assume that there exists
2 ) s.t. σmin(X TX) > ∆(cid:112)p ln(1/ν)/η. Under the
η ∈ (0, 1
homoscedastic model, given βββ and σ2, if we assume also
that (cid:107)βββ(cid:107) ≤ B and (cid:107)ˆβββ(cid:107) = (cid:107)(X TX)−1X Tyyy(cid:107) ≤ B, then
w.p. ≥ 1 − α − ν it holds that

(cid:12)
(cid:12)
(cid:12)βj − (cid:101)βj

(cid:12)
(cid:12)
(cid:12) is at most
(cid:19)

−2

(cid:16)

O

ρ ·

(cid:115)(cid:18)(cid:94)
X TX

−1

j,j + ∆(cid:112)p ln(1/ν) ·

(cid:94)
X TX

j,j

ln(1/α)

(cid:114)

+ ∆

(cid:94)
X TX

−2
j,j · ln(1/ν) · (B

√

(cid:17)

p + 1)

where ρ is w.h.p an upper bound on σ (details appear in
the Supplementary material).

Note that the assumptions that (cid:107)βββ(cid:107) ≤ B and (cid:107)ˆβββ(cid:107) ≤ B
are fairly benign once we assume each row has bounded
l2-norm. The key assumption is that X TX is well-spread.
Yet in the model where each row in X is sampled i.i.d
from N (000, Σ), this assumption merely means that n is large
enough — namely, that n = ˜Ω(

√
∆
p ln(1/ν)
η·σmin(Σ) ).

6. Experiment: t-Values of Output

Goal. We set to experiment with the outputs of Algo-
rithms 1 and 2. While Theorem 3.1 guarantees that com-
puting the t-value from the output of Algorithm 1 in the
matrix unaltered case does give a good approxima-
tion of the t-value – we were wondering if by computing
the t-value directly from the output we can (a) get a good
approximation of the true (non-private) t-value and (b) get
the same “higher-level conclusion” of rejecting the null-
hypothesis. The answers are, as ever, mixed. The two main

Differentially Private Ordinary Least Squares

get fairly high (in magnitude) t-values. As the result, we
falsely reject the null-hypothesis based on the t-value of
Analyze Gauss quite often, even for large values of n. This
is shown in Figure 1b. Additional ﬁgures (including plot-
ting the distribution of the t-value approximations) appear
in the supplementary material.

The results show that t-value approximations that do not
take into account the inherent randomness in the DP-
algorithms lead to erroneous conclusions. One approach
would be to follow the more conservative approach we ad-
vocate in this paper, where Algorithm 1 may allow you to
get true approximation of the t-values and otherwise re-
ject the null-hypothesis only based on the conﬁdence inter-
val (of Algorithm 1 or 2) not intersecting the origin. An-
other approach, which we leave as future work, is to re-
place the T -distribution with a new distribution, one that
takes into account the randomness in the estimator as well.
This, however, has been an open and long-standing chal-
lenge since the ﬁrst works on DP and statistics (see (Vu
& Slavkovic, 2009; Dwork & Lei, 2009)) and requires we
move into non-asymptotic hypothesis testing.

(a) Synthetic data, coordinate β1

(b) Synthetic data, coordinate β3

Figure 1. Correctly and Wrongly Rejecting the Null-Hypothesis

observations we do notice is that both algorithms improve
as the number of examples increases, and that Algorithm 1
is more conservative then Algorithm 2.

Setting. We tested both algorithms in two settings. The
ﬁrst is over synthetic data. Much like the setting in Theo-
rems 2.2 and 3.3, X was generated using p = 3 indepen-
dent normal Gaussian features, and yyy was generated using
the homoscedastic model. We chose βββ = (0.5, −0.25, 0)
so the ﬁrst coordinate is twice as big a the second but
of opposite sign, and moreover, yyy is independent of the
3rd feature. The variance of the label is also set to 1,
and so the variance of the homosedastic noise equals to
σ2 = 1 − (0.5)2 − (−0.25)2. The number of observations
n ranges from n = 1000 to n = 100000.

The second setting is over real-life data. We ran the two
algorithms over diabetes dataset collected over ten years
(1999-2008) taken from the UCI repository (Strack et al.,
2014). We truncated the data to 4 attributes: sex (binary),
age (in buckets of 10 years), number medications (numeric,
0-100), and a diagnosis (numeric, 0-1000). Naturally, we
added a 5th column of all-1 (intercept). Omitting any en-
try with missing or non-numeric values on these nine at-
tributes we were left with N = 91842 entries, which we
shufﬂed and fed to the algorithm in varying sizes — from
n = 30, 000 to n = 90, 000. Running OLS over the entire
N observation yields β ≈ (14.07, 0.54, −0.22, 482.59),
and t-Values of (10.48, 1.25, −2.66, 157.55).

The Algorithms. We ran a version of Algorithm 1 that uses
a DP-estimation of σmin, and ﬁnds the largest r the we can
use without altering the input, yet if this r is below 25 then
it does alter the input and approximates Ridge regression.
We ran Algorithm 2 verbatim. We set (cid:15) = 0.25 and δ =
10−6. We repeated each algorithm 100 times.

Results. We plot the t-values we get from Algorithms 1
and 2 and decide to reject the null-hypothesis based on t-
value larger than 2.8 (which corresponds to a fairly conser-
vative p-value of 0.005). Not surprisingly, as n increases,
the t-values become closer to their expected value – the t-
value of Analyze Gauss is close to the non-private t-value
and the t-value from Algorithm 1 is a factor of (cid:112) r
n smaller
as detailed above (see after Corollary 3.2). As a result,
when the null-hypothesis is false, Analyze Gauss tends to
produce larger t-values (and thus reject the null-hypothesis)
for values of n under which Algorithm 1 still does not re-
ject, as shown in Figure 1a. This is exacerbated in real
data setting, where its actual least singular value (≈ 500) is
fairly small in comparison to its size (N = 91842).

However, what is fairly surprising is the case where the
null-hypothesis should not be rejected — since βj = 0
(in the synthetic case) or its non-private t-value is close
to 0 (in the real-data case). Here, the Analyze Gauss’ t-
value approximation has fairly large variance, and we still

Differentially Private Ordinary Least Squares

Acknowledgements

The bulk of this work was done when the author was a
postdoctoral fellow at Harvard University, supported by
NSF grant CNS-123723; and also an unpaid collaborator
on NSF grant 1565387. The author wishes to wholeheart-
edly thank Prof. Salil Vadhan, for his tremendous help in
shaping this paper. The author would also like to thank
Prof. Jelani Nelson and the members of the “Privacy Tools
for Sharing Research Data” project at Harvard Univer-
sity (especially James Honaker, Vito D’Orazio, Vishesh
Karwa, Prof. Kobbi Nissim and Prof. Gary King) for many
helpful discussions and suggestions; as well as Abhradeep
Thakurta for clarifying the similarity between our result.
Lastly the author thanks the anonymous referees for many
helpful suggestions in general and for a reference to (Ull-
man, 2015) in particular.

References

Agresti, A. and Finlay, B. Statistical Methods for the Social

Sciences. Pearson P. Hall, 2009.

Bassily, R., Smith, A., and Thakurta, A. Private empirical
risk minimization: Efﬁcient algorithms and tight error
bounds. In FOCS, 2014.

Blocki, J., Blum, A., Datta, A., and Sheffet, O. The
Johnson-Lindenstrauss transform itself preserves differ-
ential privacy. In FOCS, 2012.

Chaudhuri, Kamalika and Hsu, Daniel J. Convergence rates
for differentially private statistical estimation. In ICML,
2012.

Chaudhuri, Kamalika, Monteleoni, Claire, and Sarwate,
Anand D. Differentially private empirical risk minimiza-
tion. Journal of Machine Learning Research, 12, 2011.

Duchi, John C., Jordan, Michael I., and Wainwright, Mar-
tin J. Local privacy and statistical minimax rates.
In
FOCS, pp. 429–438, 2013.

Dwork, C. and Lei, J. Differential privacy and robust statis-

tics. In STOC, 2009.

Dwork, Cynthia, Kenthapadi, Krishnaram, McSherry,
Frank, Mironov, Ilya, and Naor, Moni. Our data, our-
selves: Privacy via distributed noise generation. In EU-
ROCRYPT, 2006a.

Dwork, Cynthia, Mcsherry, Frank, Nissim, Kobbi, and
Smith, Adam. Calibrating noise to sensitivity in private
data analysis. In TCC, 2006b.

Dwork, Cynthia, Talwar, Kunal, Thakurta, Abhradeep, and
Zhang, Li. Analyze gauss - optimal bounds for pri-
vacy preserving principal component analysis. In STOC,
2014.

Dwork, Cynthia, Su, Weijie, and Zhang, Li. Private false
discovery rate control. CoRR, abs/1511.03803, 2015.

Hoerl, A. E. and Kennard, R. W. Ridge regression: Biased
estimation for nonorthogonal problems. Technometrics,
12:55–67, 1970.

Kasiviswanathan, S., Lee, H., Nissim, K., Raskhodnikova,
S., and Smith, A. What can we learn privately? In FOCS,
2008.

Kifer, Daniel, Smith, Adam D., and Thakurta, Abhradeep.
Private convex optimization for empirical risk minimiza-
tion with applications to high-dimensional regression. In
COLT, 2012.

Laurent, B. and Massart, P. Adaptive estimation of a
quadratic functional by model selection. The Annals of
Statistics, 28(5), 10 2000.

Ma, E. M. and Zarowski, Christopher J. On lower bounds
for the smallest eigenvalue of a hermitian positive-
deﬁnite matrix. IEEE Transactions on Information The-
ory, 41(2), 1995.

Muller, Keith E. and Stewart, Paul W. Linear Model The-
ory: Univariate, Multivariate, and Mixed Models. John
Wiley & Sons, Inc., 2006.

Pilanci, M. and Wainwright, M. Randomized sketches of
convex programs with sharp guarantees. In ISIT, 2014a.

Pilanci, Mert and Wainwright, Martin J. Iterative hessian
sketch: Fast and accurate solution approximation for
constrained least-squares. CoRR, abs/1411.0347, 2014b.

Rao, C. Radhakrishna. Linear statistical inference and its

applications. Wiley, 1973.

Rogers, Ryan M., Vadhan, Salil P., Lim, Hyun-Woo, and
Gaboardi, Marco. Differentially private chi-squared hy-
pothesis testing: Goodness of ﬁt and independence test-
ing. In ICML, pp. 2111–2120, 2016.

Rudelson, Mark and Vershynin, Roman. Smallest singular
value of a random rectangular matrix. Comm. Pure Appl.
Math, pp. 1707–1739, 2009.

Sarl´os, T. Improved approx. algs for large matrices via ran-

dom projections. In FOCS, 2006.

Sheffet, O. Private approximations of the 2nd-moment ma-
trix using existing techniques in linear regression. CoRR,
abs/1507.00056, 2015. URL http://arxiv.org/
abs/1507.00056.

Smith, Adam D. Privacy-preserving statistical estimation
with optimal convergence rates. In STOC, pp. 813–822,
2011.

Differentially Private Ordinary Least Squares

Strack, B., DeShazo, J., Gennings, C., Olmo, J., Ventura,
S., Cios, K., and Clore, J. Impact of HbA1c measure-
ment on hospital readmission rates: Analysis of 70,000
clinical database patient records. BioMed Research In-
ternational, 2014:11 pages, 2014.

Tao, T. Topics in Random Matrix Theory. American Math-

ematical Soc., 2012.

Thakurta, Abhradeep and Smith, Adam. Differentially pri-
vate feature selection via stability arguments, and the ro-
bustness of the lasso. In COLT, 2013.

Tikhonov, A. N. Solution of incorrectly formulated prob-
lems and the regularization method. Soviet Math. Dokl.,
4, 1963.

Uhler, Caroline, Slavkovic, Aleksandra B., and Fien-
berg, Stephen E. Privacy-preserving data sharing for
Journal of Privacy
genome-wide association studies.
and Conﬁdentiality, 2013. Available at: http://
repository.cmu.edu/jpc/vol5/iss1/6.

Ullman, J. Private multiplicative weights beyond linear

queries. In PODS, 2015.

Vu, D. and Slavkovic, A. Differential privacy for clinical
trial data: Preliminary evaluations. In ICDM, 2009.

Wang, Yue, Lee, Jaewoo, and Kifer, Daniel. Differ-
entially private hypothesis testing, revisited. CoRR,
abs/1511.03376, 2015.

Xi, B., Kantarcioglu, M., and Inan, A. Mixture of gaus-
sian models and bayes error under differential privacy.
In CODASPY. ACM, 2011.

Zhou, S., Lafferty, J., and Wasserman, L. Compressed re-

gression. In NIPS, 2007.

