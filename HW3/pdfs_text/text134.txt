000
001
002
003
004
005
006
007
008
009
010
011
012
013
014
015
016
017
018
019
020
021
022
023
024
025
026
027
028
029
030
031
032
033
034
035
036
037
038
039
040
041
042
043
044
045
046
047
048
049
050
051
052
053
054

Supplementary Materials:
Dueling Bandits with Weak Regret

June 12, 2017

Abstract

in this iteration by Lemma 0.1 is

This note contains supplementary materials
for Dueling Bandits with Weak Regret.

0. Gambler’s Ruin Lemma

In our analysis of WS-W, we will use results from a
special case of the Gambler’s ruin problem (Karlin,
1968), stated as follows: suppose a gambler has m
dollars initially. In each of a sequence of rounds, he
loses 1 dollar with probability q (cid:54)= 1
2 and wins 1 dollar
with probability 1 − q. He stops playing when he has
either m + 1 dollars or has no money left. We have the
following result, with a proof available on Page 73 of
Karlin (1968).

Lemma 0.1 (Gambler’s Ruin Lemma). In the gam-
bler’s ruin problem: (1) the probability that the gam-
bler reaches m + 1 dollars before reaching 0 dol-

lars is qm =
−1
of steps before the gambler stops playing is m

; (2) the expected number

1−2q −

−1

q )m
( 1−q
q )m+1
( 1−q

m+1
1−2q

−1

q )m
( 1−q
q )m+1
( 1−q

−1

.

Observe that the conditional distribution of T(cid:96),k and
the winner of iteration k round (cid:96), given the two arms
being pulled, is given by the result above for the Gam-
bler’s ruin problem. We leverage this in our proof.

1. Proof of Lemma 1

Proof. Suppose we are comparing arm i versus arm j
in this iteration with i > j and arm i is the incumbent.
Then we know C(t(cid:96),k − 1, i) = (N − 1)((cid:96) − 1) + k − 1
and C(t(cid:96),k − 1, j) = −(cid:96) + 1. We will keep playing these
two arms until C(t(cid:96),k + T(cid:96),k − 1, i) = (N − 1)((cid:96) − 1) + k
or C(t(cid:96),k + T(cid:96),k − 1, j) = (N − 1)((cid:96) − 1) + k. Further,
since the winning probability of arm i over arm j is
pi,j over this period, we know the dynamics of this
iteration are the same as those of the Gambler’s Ruin
problem. Denote E = C(t(cid:96),k −1, i)−C(t(cid:96),k −1, j)+1 =
N l+k−N . Then the expected length of time we spend

1

(cid:16) 1−pi,j
pi,j
(cid:16) 1−pi,j
pi,j

(cid:17)E

− 1

(cid:17)E+1

− 1

E
1 − 2pi,j

−

E + 1
1 − 2pi,j

≤

E
1 − 2pi,j

≤

E
2p − 1

.

The proof of second statement is similar. Using the
same notation but now supposing pi,j ≥ p > 1
2 , we
have that the expected length of time we spend in this
iteration is

E
1 − 2pi,j

−

E + 1
1 − 2pi,j

(cid:16) 1−pi,j
pi,j
(cid:16) 1−pi,j
pi,j

(cid:17)E

− 1

(cid:17)E+1

− 1

−

E + 1
1 − 2pi,j

pi,j(1 − pi,j)E − (1 − pi,j)E+1
(1 − pi,j)n+1 − pE+1

i,j

=

≤

1
2pi,j − 1
1
2p − 1

.

2. Proof of Lemma 2

In this section, we prove Lemma 2 from the main
paper. This section is structured as follows: In sec-
tion 2.1, we provide two bounds for the incumbent’s
losing and winning probability; In section 2.2, we con-
sider a version of the problem in which better and
worse incumbents have constant (but diﬀerent) win-
ning probabilities and provide a upper bound for the
number of worse incumbents in a round before a better
incumbent loses ; In section 2.3, we use the results from
the previous subsection to bound the expected number
of iterations with a worse incumbent in a single round
before a better incumbent loses, starting from within a
round; In section 2.4, we prove a similar bound on the
expected number of iterations with a worse incumbent
in this and future rounds before a better incumbent
loses, starting from the beginning of a round; In sec-
tion 2.5, we complete the proof of Lemma 2.

055
056
057
058
059
060
061
062
063
064
065
066
067
068
069
070
071
072
073
074
075
076
077
078
079
080
081
082
083
084
085
086
087
088
089
090
091
092
093
094
095
096
097
098
099
100
101
102
103
104
105
106
107
108
109

Throughout this section, we use a one to one cor-
respondence between n and ((cid:96), k) deﬁned by n =
((cid:96) − 1)(N − 1) + k, 0 ≤ k ≤ N − 1 and (cid:96) = (cid:100)n/(N − 1)(cid:101).
We also denote p∗ = 2p−1
.

p

N ((cid:96) − 1) + k. We have

1 − qE = 1 −

2.1. Bounds on Win and Loss Probabilities

We ﬁrst prove the following two lemmas, which give

• a lower bound for the probability that a worse

incumbent loses an iteration;

• an upper bound for the probability that a better

incumbent loses an iteration.

(cid:17)E

− 1

(cid:17)E+1

− 1

(cid:16) 1−pi,j
pi,j
(cid:16) 1−pi,j
pi,j
(cid:17)E

[1 − 1−p
p ]

(cid:17)E+1

(cid:16) 1−pi,j
pi,j
(cid:19)E

(cid:16) 1−pi,j
pi,j

1 −

=

≤

(cid:18) 1 − pi,j
pi,j

≤

(cid:18) 1 − p
p

(cid:19)E

.

Lemma 2.1. In iteration k of round (cid:96) conditioned on
the identities of the incumbent and the challenger, if
the incumbent is worse than the challenger, then the
incumbent loses the iteration with conditional proba-
bility at least p∗ = 2p−1

.

p

2.2. Deﬁnition and Upper Bound for g(b, m)

In this section, we deﬁne a function g(b, m) as follows.
First, we deﬁne g(0, m) = 0 for any m. We deﬁne
g(b, m) for other integers b, m satisfying m > 0 and
0 ≤ b ≤ m recursively, as follows:

=

1 − 2pi,j
1 − pi,j

(1 − p∗)g(b − 1, m − 1)

(1)

Proof. Let i be the incumbent and j be the challenger,
with i > j. C(i, t(cid:96),k) ≥ 0 and C(j, t(cid:96),k) ≤ 0. Let
E = C(i, t(cid:96),k) + |C(j, t(cid:96),k)| + 1. The probability that
arm i loses this iterations is the same as 1 − qE in the
Gambler’s Ruin Lemma, Lemma 0.1, with q = pi,j <
0.5. This probability is:

1 − qE = 1 −

(cid:17)E

− 1

(cid:17)E+1

− 1

(cid:16) 1−pj,i
pi,j
(cid:16) 1−pi,j
pi,j
(cid:17)E+1

(cid:16) 1−pi,j
pi,j

−

(cid:17)E

(cid:17)E+1

(cid:16) 1−pi,j
pi,j

(cid:16) 1−pi,j
pi,j

≥

≥

2p − 1
p

.

Lemma 2.2. In iteration k of round (cid:96) conditioned on
the identities of the incumbent and the challenger, if
the incumbent is better than the challenger, then the
incumbent loses the iteration with conditional proba-

bility at most

, where E = N ((cid:96) − 1) + k.

(cid:17)E

(cid:16) 1−p
p

Proof. This proof is similar to the previous one. Sup-
pose we are pulling arm i and j with i < j and i is the
incumbent. Then we know C(t(cid:96),k − 1, i) = (N − 1)((cid:96) −
1) + k − 1 and C(t(cid:96),k − 1, j) = −(cid:96) + 1. The probability
that arm i loses is equal to 1 − qE from the gambler’s
ruin problem, where E = (N −1)((cid:96)−1)+k −1+(cid:96)−1 =

=

+

(cid:48)

p∗g(b

, m − 1) +

g(b, m − 1)

(1 − p∗)g(b − 1, m − 1)

=

+

(cid:48)

p∗g(b

, m − 1) +

g(b, m − 1)

m−1
(cid:88)

b(cid:48) =b

1
m

m − b
m

g(b, m)

b
m

+

b
m

+

b−1
(cid:88)

b(cid:48) =0

1
m

b−1
(cid:88)

b(cid:48) =0

1
m

b−1
(cid:88)

b(cid:48) =0

1
m

b
m

Intuitively, g(b, m) is the expected number of future
iterations in which the incumbent is worse than the
challenger, starting with m arms that have not dueled
yet b of which are better than the incumbent, when
we stop counting when we reach the end of the round
or when an incumbent loses to a worse challenger, in
a simpliﬁed problem in which worse incumbents beat
better challengers with probability p∗. In our problem,
this probability is not p∗, but is bounded below by this
quantity, and in the next section we will show that
g(b, m) is an upper bound on an analogous quantity in
our problem.

We prove the following result about g.

Lemma 2.3. For 0 ≤ b ≤ m ≤ N − 1, we have

g(b, m) = g(b, b) ≤

log(b) + 1
p∗

.

110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164

Proof. Given the boundary conditions g(0, m) = 0 for
all m, we know Equation 1 has a unique solution. In
this proof,

Therefore,

• We ﬁrst assume g(b, m) = g(b, b) for all b ≤ m

and solve for g(b, m);

• Then we show that this g(b, m) is indeed the so-
lution for Equation 1, verifying that g(b, m) is as
claimed;

• Finally, we show g(b, m) ≤ log(b)+1

.

p∗

First, we solve for g(b, m) with the assumption that
g(b, m) = g(b, b) for b ≤ m. Setting m = b in Equa-
tion 1 provides

g(b, b) = 1 +

+ (1 − p∗)g(b − 1, b − 1).

b−1
(cid:88)

b(cid:48) =0

(cid:48)

, b)

p∗g(b
b

(2)

Thus, we know

(cid:48)

p∗g(b

, b + 1)

b−1
(cid:88)

b(cid:48) =1

=

b−1
(cid:88)

b(cid:48) =1

(cid:48)

p∗g(b

, b)

=b [g(b, b) − 1 − (1 − p∗)g(b − 1, b − 1)] .

Therefore, Equation 2 becomes

=1 +

g(b + 1, b + 1)
b
b + 1
p∗g(b, b)
b + 1

+

[g(b, b) − 1 − (1 − p∗)g(b − 1, b − 1)]

+ (1 − p∗g(b, b).

Re-organizing the terms, we have

g(b + 1, b + 1) − g(b, b)
b
b + 1

1
b + 1

+

=

(1 − p∗)[g(b, b) − g(b − 1, b − 1)].

g(b, b) =

F (k)

b
(cid:88)

k=1

b
(cid:88)

k=1

=

(cid:20) 1
k

+

1 − p∗
k

+ · · ·

(1 − p∗)k−1
k

(cid:21)

.

Thus, if g(b, m) = g(b, b) for all b ≤ m, we know

g(b, m) =

b
(cid:88)

k=1

(cid:20) 1
k

+

1 − p∗
k

+ · · ·

(1 − p∗)k−1
k

(cid:21)

.

Now we verify that this is the correct solution. We
prove this by induction on b. For b = 1, Equation 1
becomes

g(1, m) =

+

g(1, m − 1).

1
m

m − 1
m

Since g(1, 1) = 1, it is easy to check g(1, 2) = g(1, 3) =
· · · = g(1, N − 1) = 1.

Suppose this g(b, m) = g(b, b) are true for all b ≤ m,
b ≤ k. For b = k + 1, Equation 1 becomes

g(k + 1, m)

=

k + 1
m

+

k
(cid:88)

b(cid:48) =0

p∗
m

(cid:48)
g(b

, m − 1) +

m − k − 1
m

g(k + 1, m − 1)

+

k + 1
m

(1 − p∗)g(k, m − 1)

=

k + 1
m

+

k
(cid:88)

b(cid:48) =0

p∗
m

(cid:48)
g(b

(cid:48)
, b

) +

m − k − 1
m

+

k + 1
m

(1 − p∗)g(k, k).

g(k + 1, m − 1)

To show g(k + 1, m) does not depend on m, we need to
prove the following equation is true for m = k + 2, k +
3, · · · , N − 1.

k + 1
m

+

k
(cid:88)

b(cid:48) =0

p∗
m

(cid:48)
g(b

(cid:48)
, b

) +

(1 − p∗)g(k, k)

k + 1
m

Denote F (b) = g(b, b)−g(b−1, b−1). We know F (1) =
1. Thus, we have

=

k + 1
m

g(k + 1, m − 1)

F (b) =

+

(1 − p∗)F (b − 1)

1
b
1
b
1
b

b − 1
b
1 − p∗
b
1 − p∗
b

=

+

(1 − p∗)2F (b − 2)

+

b − 2
b

=

+

+ · · ·

(1 − p∗)b−1
b

.

⇐⇒ k + 1 +

p∗g(b

(cid:48)

(cid:48)
, b

) + (k + 1)(1 − p∗)g(k, k)

k
(cid:88)

b(cid:48) =0
=(k + 1)g(k + 1, m − 1)

(3)

We ﬁrst check Equation 3 when m = k + 2. Starting

165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219

from the left hand side, we have

the round. Formally, we deﬁne this quantity as:

k + 1 +

g(b

(cid:48)

(cid:48)
, b

) + (k + 1)(1 − p∗)g(k, k)

k
(cid:88)

b(cid:48) =0

=k + 1 + (k + 1)[g(k + 1, k + 1) − 1 − (1 − p∗)g(k, k)]
(4)

where

+ (k + 1)(1 − p∗)g(k, k)

=(k + 1)g(k + 1, k + 1),

h(i, n, A) = E

(cid:34) σ−1
(cid:88)

(cid:35)
B(n(cid:48))|A, in = i

,

n(cid:48)=n

• Conditioning on A is understood to mean that
we are conditoning on C(n − 1, j) = −(cid:96) + 1 ∀ j /∈
A ∪ {in}, and C(n − 1, j) = −(cid:96) ∀ j ∈ A, where
(cid:96) = (cid:100)n/(N − 1)(cid:101) is the round in which iteration n
resides. In other words, it is understood to mean
that A contains the set of arms that have not yet
dueled in this round.

• σ = min {n(cid:48) > n : J(n(cid:48)) = 1, n(cid:48) = N (cid:100)n/(N − 1)(cid:101)}
where J(n) is an indicator that equals 1 when a
better incumbent loses at iteration n, i.e., σ is
the ﬁrst time that either a better incumbent loses
or the round ends.

Lemma 2.4. For any i, (cid:96), k and A, we have

h(i, n, A) ≤ g(b, m) ≤

log(N ) + 1
p∗

,

where m = N − k and b = |{u ∈ A : u < i}|.

Proof. Denote qi,j(n) as the probability that incum-
bent arm i will beat challenger j at time n. We ﬁrst
write a recursive expression for h(i, n, A) that applies
when n is not divisible by N :

h(i, n, A) =

(cid:88)

(cid:20)

1 +

qi,j(n)
N − k

h(i, n + 1, A ∪ {j})

h(j, n + 1, A ∪ {i})

(cid:21)

{j∈A:i>j}

1 − qi,j(n)
N − k

(cid:88)

+

+

{j∈A:i<j}

qi,j(n)
N − k

h(i, n + 1, A ∪ j). (5)

When n is divisible by N − 1, the only allowed value
of A is ∅ and h(i, n, ∅) = 0.

We then prove the desired result via induction on the
number of iterations in the round, i.e., on n (mod N −
1). When n (mod N − 1) = 0, we have h(i, n, ∅) = 0,
b = 0, and g(b, m) = 0. Thus the result holds in this
case.

Then suppose the result holds for all n with a partic-
ular value of n (mod N − 1) and we show it holds for
n − 1.

Applying the induction hypothesis to the right-hand

which equals to the right hand side. Equation (4) fol-
lows from Equation (2) (Equation (2) holds because
g(b, m) = g(b, b) for all b ≤ k).

Again, by induction, we know (3) is true for all m =
k + 2, · · · , N − 1 and thus we concludes our induction.

We have shown that g(b, m) = g(b, b) for all b ≤ m.

Finally, we prove g(b, b) = g(b, m) ≤ log(b)+1
because

p∗

. This is

g(b, m) =g(b, b)
(cid:20) 1
k

b
(cid:88)

=

k=1

b
(cid:88)

k=1

b
(cid:88)

(cid:20) 1
k

1
k

≤

=

≤

k=1
log(b) + 1
p∗

,

+

+

1 − p∗
k

1 − p∗
k

+ · · ·

(1 − p∗)k−1
k

+ · · ·

(1 − p∗)b−1
k

(cid:21)

(cid:21)

(cid:2)1 + (1 − p∗) + · · · + (1 − p∗)b−1(cid:3)

which concludes our proof.

2.3. Bound on the Number of Iterations in
One Round with a Worse Incumbent,
Starting from Within the Round

Let B(n) denote an indicator function that equals 1
if we have a better incumbent at the nth iteration.
The deﬁnition of B(n) is very similar to B((cid:96), k) ex-
cept B((cid:96), k) tracks both round and iteration number.
Similarly, we use ¯B(n) = 1−B(n) to denote an indica-
tor function that equals 1 if we have a worse incumbent
at the nth iteration.

Let h(i, n, A) be the expected number of iterations
with an incumbent that is worse than the challenger,
between iteration n and the ﬁrst time that a better in-
cumbent loses to a challenger or the round ends, given
that the incumbent arm at iteration n is i and A is
the set of arms that have not yet previously dueled in

220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274

side of (5), we have

h(i, n, A) ≤

(cid:88)

(cid:20)

1 +

qi,j(n)
m

g(bi,j, m − 1)

(cid:21)

g(bj,j, m − 1)

{j∈A:i>j}

1 − qi,j(n)
m

(cid:88)

+

+

{j∈A:i<j}

qi,j(n)
m

g(bi,j, m − 1),

(6)

where bu,j = #{u

(cid:48)

(cid:48)
∈ A \ {j} : u

< u}.

Consider the summand in the ﬁrst sum in (6), drop-
ping the constants 1 and 1
m ,
qi,j(n)g(bi,j, m − 1) + (1 − qi,j(n))g(bj,j, m − 1). (7)

This is increasing in qi,j(n) when i > j since bi,j >
bj,j, and since g(b, m) is increasing in b. Since i is an
incumbent that is worse than the challenger when i >
j, Lemma 2.1 shows that qi,j(n) ≤ 1−p∗ = 1− 2p−1
in
this situation. Thus, this summand is bounded above
by (1 − p∗)g(bi,j, m − 1) + p∗g(bj,j, m − 1).

p

Substituting this into (6), along with the inequality
qi,j(n) ≤ 1 in the last term, we have

h(i, n, A)

(cid:88)

≤

{j∈A:i>j}

(cid:88)

+

{j∈A:i<j}

=

+

b
m

b
m

+

m − b
m
=g(b, m)

(cid:20)

1 +

1 − p∗
m

g(bi,j, m−1) +

g(bj,j, m−1)

(cid:21)

p∗
m

1
m

g(bi,j, m − 1)

(1 − p∗)g(b − 1, m − 1) +

(cid:48)

g(b

, m − 1)

b−1
(cid:88)

b(cid:48) =0

p∗
m

g(b, m − 1)

In the second to last line we have used that {bi,j :
j ∈ A, i > j} = {0, . . . , b − 1} and bi,j = b − 1 when
i > j; bi,j = b when i < j; and that the cardinality
of {j ∈ A : i > j} and {j ∈ A : i < j} are b and
m − b respectively. In the last line we have used the
recursive deﬁnition of g(b, m) in terms of g(·, m − 1).

This shows the ﬁrst inequality in the statement of the
lemma. The second inequality follows directly from
Lemma 2.3.

2.4. Bound on the Number of Iterations with
a Worse Incumbent, Starting from a
Round Beginning

Denote f (i, (cid:96)) to be the expected number of iterations
with a worse incumbent in this and future rounds,

stopping as soon as a better incumbent loses, giving
that we have arm i as the incumbent at the start of
round (cid:96).

Lemma 2.5. For any i and (cid:96), we have

f (i, (cid:96)) ≤

log(N ) + 1
(p∗)2

.

Proof. Let U (i, (cid:96)) denote the expected number of it-
erations in round (cid:96) with a worse incumbent before a
better incumbent loses. We use V ((cid:96)) to denote an in-
dicator which equals to 1 if a better incumbent does
not lose in the round (cid:96). Then for i > 1,

f (i, (cid:96)) = U (i, (cid:96)) + E[f (Z((cid:96)), (cid:96) + 1)V ((cid:96))|Z((cid:96) − 1) = i].

The ﬁrst term is bounded by Lemma 2.4 by

U (i, (cid:96)) ≤

log(N ) + 1
p∗

,

for all i and (cid:96).

For the second term, since f (Z((cid:96)), (cid:96) + 1) = 0 when
Z((cid:96)) = 1, we know the second term is bounded by

E[f (Z((cid:96)), (cid:96) + 1)V ((cid:96))|Z((cid:96) − 1) = i]

≤E[f (Z((cid:96)), (cid:96) + 1)|Z((cid:96)) (cid:54)= 1, V ((cid:96)) = 1, Z((cid:96) − 1) = i]

× P (Z((cid:96)) (cid:54)= 1, V ((cid:96))|Z((cid:96) − 1) = i).

Let sj = P (Z((cid:96)) = j|Z((cid:96)) (cid:54)= 1, V ((cid:96)), Z((cid:96) − 1) = i) to
be the probability distribution over the integers from
2 through N . Then we know

E[f (Z((cid:96)), (cid:96) + 1)|Z((cid:96)) (cid:54)= 1, V ((cid:96)) = 1, Z((cid:96) − 1) = i]
N
(cid:88)

=

sjf (j, (cid:96) + 1)

j=2

≤ max
j=2,,N

f (j, (cid:96) + 1).

Further, since if arm 1 wins its ﬁrst duel as a chal-
lenger (which happens with probability at least p∗),
then either Z((cid:96)) = 1 (it wins all subsequent duel in
the round) or V ((cid:96)) = 0 (it loses a subsequent duel),
we have P (Z((cid:96)) (cid:54)= 1, V ((cid:96))|Z((cid:96) − 1) = i) ≤ 1 − p∗.

Thus, we know

f (i, (cid:96)) ≤

log(N ) + 1
p∗

+ (1 − p∗) max

f (j, (cid:96) + 1).

j=2,··· ,N

Let f ((cid:96)) = maxj=2,··· ,N f (j, (cid:96)). Then,

f ((cid:96)) ≤

log(N ) + 1
p∗

+ (1 − p∗)f ((cid:96) + 1).

275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329

Thus,

Let (cid:96)k = (cid:100)τk/(N − 1)(cid:101). Then,

f (1) ≤

+ (1 − p∗)f (2)

log(N ) + 1
p∗
log(N ) + 1
p∗
log(N ) + 1
(p∗)2

.

≤

=

(1 + (1 − p∗) + (1 − p∗)2 + · · · )

2.5. Completing the Proof of Lemma 3

With the lemmas in the preceding subsections estab-
lished, we now complete the proof of Lemma 2.

Proof. Let τ0 = 0 and τk = {n > τk−1 : J(n) =
1}. The expected number of iterations with a worse
incumbent is

=E

1{τk < ∞}

1{n < τk+1} ¯B(n)

(cid:34) ∞
(cid:88)

(cid:35)
¯B(n)

E

n=0
∞
(cid:88)

k=0

=

∞
(cid:88)

k=0

P (τk < ∞)E

∞
(cid:88)

n=τk
(cid:34) ∞
(cid:88)

n=τk

(cid:35)
1{n < τk+1} ¯B(n)|τk < ∞

where we have used Tonelli’s theorem to exchange the
expectation of an inﬁnite sum of non-negative terms
with an inﬁnite sum of expectations of the same terms.

Conditioning on the history available at time τk, we
have that the inner expectation can be written as,

1{n < τk+1} ¯B(n)|τk < ∞

(cid:35)

(cid:34) ∞
(cid:88)

E

n=τk

(cid:34)

(cid:34) ∞
(cid:88)

=E

E

n=τk

1{n < τk+1} ¯B(n)|Hτk , τk < ∞

|τk < ∞

,

(cid:35)

(cid:35)

where Hn is the sigma algebra generated by (C(i, s) :
s < t(cid:96),k(cid:48), i = 1, . . . , N ), where (cid:96) = n (mod N − 1),
k(cid:48) = (cid:100)n/(N − 1)(cid:101), and Hτk is the ﬁltration (Hn : n)
stopped at τk.

break

further

term
We
1{n < τk+1} ¯B(n)|Hτk , τk < ∞(cid:3)
E (cid:2)(cid:80)∞
two
parts: the part that occurs during the round in which
τk resides, and the part that occurs in future rounds.

inner

into

this

n=τk

(cid:34) ∞
(cid:88)

E

n=τk
(cid:34) (cid:96)kN
(cid:88)

=E

+E

n=τk
(cid:34) ∞
(cid:88)

1{n < τk+1} ¯B(n)|Hτk , τk < ∞

1{n < τk+1} ¯B(n)|Hτk , τk < ∞

1{n < τk+1} ¯B(n)|Hτk , τk < ∞

(cid:35)

(cid:35)

(cid:35)

n=(cid:96)kN +1
log(N ) + 1
p∗

≤

≤

2(log(N ) + 1)
(p∗)2

+

log(N ) + 1
(p∗)2

(cid:104)(cid:80)(cid:96)kN
n=τk

1{n < τk+1} ¯B(n)|Hτk , τk < ∞

where the second to last inequality relies on Lemma 2.4
to show E
is
bounded above by log(N )+1
show E (cid:2)(cid:80)∞
bounded above by log(N )+1

and Lemma 2.5 to
n=(cid:96)kN +1 1{n < τk+1} ¯B(n)|Hτk , τk < ∞(cid:3) is

p∗

(cid:105)

.

(p∗)2

Thus,

(cid:35)

¯B(n)

(cid:34) ∞
(cid:88)

E

n=0

≤

2(log(N ) + 1)
(p∗)2

∞
(cid:88)

k=0

P (τk < ∞).

Now we bound P (τk < ∞) for a ﬁxed k. Based on
Lemma 2.2, we know J(n) is a Bernoulli random vari-

able with success rate less than

(this is be-

cause of Lemma 2.2 and n = (N −1)((cid:96)−1)+k < E), in-
dependent across n. Let Qn denote a Bernoulli random
variable with success rate

. Then we know:

(cid:17)n

(cid:17)n

(cid:16) 1−p
p

P (τk < ∞) ≤ P

J(i) ≥ k

(cid:16) 1−p
p
(cid:32) ∞
(cid:88)

i=1
(cid:32) ∞
(cid:88)

i=1

(cid:33)

(cid:33)

≤ P

Qi ≥ k

.

Let Wm = (cid:80)m
i=1 Qi, which follows a Poisson Bernoulli
distribution, and let W = limm→∞ Wm. W follows a
Poisson distribution with parameter (cid:80)∞
i=1
1−p
2p−1 (Theorem 4, Wang (1993)). Thus,

(cid:16) 1−p
p

(cid:17)i

=

(cid:35)

¯B(n)

≤

(cid:34) ∞
(cid:88)

E

n=0

2(log(N ) + 1)
(p∗)2

∞
(cid:88)

k=0

P (W ≥ k)

=

2p2(1 − p)
(2p − 1)3 (log(N ) + 1)

2p2

≤

(2p − 1)3 (log(N ) + 1)

330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384

3. Proof of Lemma 3

Proof. It is easy to see that at the last iteration which
has a worse incumbent, the better arm is always arm
1. Thus, we only consider C(t, 1) in this proof. At the
end of the (cid:96)th round, if C(t(cid:96)+1 − 1, 1) < 0, we know
C(t(cid:96)+1 − 1, 1) = −(cid:96).

Let us consider a simple random walk W(t) such that
W (t + 1) = W (t) + 1 with probability p > 1
2 and
W (t + 1) = W (t) − 1 with probability 1 − p for t ≥ 1.
If we denote p∗
(cid:96) = P (∃t∗, W (t∗) = −(cid:96)) for (cid:96) > 0, then
(cid:16) 1−p
p

it is easy to calculate that p∗

(cid:96) =

(cid:17)(cid:96)

.

Now let us consider C(t, 1). If we pull arm 1 with some
other arm i at time t, then C(t, 1) = C(t − 1, 1) + 1
happens with probability p1,i > p and C(t, 1) = C(t −
1, 1) − 1 with probability 1 − p1,i < 1 − p. If we do not
pull arm 1 at time t, then C(t, 1) = C(t − 1, 1) with
probability 1.

Deﬁne τ1 = 1 and τk = mint{t > τk−1, C(t, 1) (cid:54)=
C(τk−1, 1)}, for k = 1, 2, · · · ,. Because τk is a non-
decreasing right continuous stopping time, we know it
is a valid random change of time (Barndorﬀ-Nielsen &
Shiryaev, 2015). Deﬁne R(k) a new stochastic process
where R(k) = C(τk, 1). Then we know at every time
k, R(k) = R(k − 1) + 1 with probability greater or
equal to p and R(k) = R(k − 1) − 1 with probability
less than 1-p. Deﬁne p(cid:96) = P (∃t∗, R(t∗) = −(cid:96)), then
it is easy to prove p(cid:96) ≤ p∗
(cid:96) =
analysis and induction (we leave the proof as an ex-
ercise for the reader), which means P (∃t∗, C(t∗, 1) =

using ﬁrst step

(cid:16) 1−p
p

(cid:17)(cid:96)

−(cid:96)) ≤

(cid:16) 1−p
p

(cid:17)(cid:96)

.

4. Proof of Lemma 4

Proof. To show the ﬁrst claimed equation, we have:

E[B((cid:96), k)T(cid:96),k ¯D((cid:96))]

=E[B((cid:96), k)T(cid:96),k| ¯D((cid:96)) = 1]P ( ¯D((cid:96)) = 1).

(8)

term E[B((cid:96), k)T(cid:96),k| ¯D((cid:96)) = 1] can be
The ﬁrst
bounded by writing it as E[B((cid:96), k)T(cid:96),k| ¯D((cid:96)) = 1] =
E[E[B((cid:96), k)T(cid:96),k| ¯D((cid:96)) = 1, A((cid:96), k)]| ¯D((cid:96)) = 1], where
A((cid:96), k) denotes the pair of arms being pulled in it-
eration k round (cid:96).
We focus on the inner term E[B((cid:96), k)T(cid:96),k| ¯D((cid:96)) =
1, A((cid:96), k)]. B((cid:96), k) is observable given A((cid:96), k).
If
B((cid:96), k) = 0 then this inner term is 0.
If B((cid:96), k) =
1 then this inner term is E[T(cid:96),k|A((cid:96), k)] (where we
note that T(cid:96),k is conditionally independent of ¯D((cid:96))
given A((cid:96), k)) and is bounded above by 1/(2p − 1)
In both cases, the inner term is
by Lemma 1.

bounded above by 1/(2p − 1), and we have that
E[B((cid:96), k)T(cid:96),k| ¯D((cid:96)) = 1] ≤ 1/(2p − 1).

Thus, we have that (8) is bounded above by

1
2p − 1

P ( ¯D((cid:96)) = 1) ≤

1
2p − 1

(cid:18) 1 − p
p

(cid:19)(cid:96)−1

,

where the ﬁnal inequality follows from Lemma 3 and
the fact that ¯D((cid:96)) = 1 implies L ≥ (cid:96) − 1.

To show the second claimed equation, we use the same
proof technique used for the ﬁrst and get:

E[B((cid:96), k)T(cid:96),kV ((cid:96), k)] ≤

P (V ((cid:96), k) = 1).

1
2p − 1

Now we just need to compute P (V ((cid:96), k) = 1). Given
C(t(cid:96)−1, 1) = (N −1)((cid:96)−1) at the beginning of round (cid:96),
it loses only if there exists a t0 ≥ t(cid:96) and C(1, t0) = −(cid:96).
Using the results from Lemma 3, we know P (V ((cid:96), k) =

. This completes the proof of the second

(cid:17)(cid:96)

(cid:16) 1−p
1) ≤
p
claimed equation.

5. Proof of Lemma 5

Proof. For the ﬁrst inequality, we know

(cid:35)

¯B((cid:96), k)T(cid:96),k ¯D((cid:96))

(cid:34)N −1
(cid:88)

E

k=1

=

N −1
(cid:88)

k=1

E (cid:2)E[ ¯B((cid:96), k)T(cid:96),k|D((cid:96)) = 0] ¯D((cid:96))(cid:3) .

(9)

Moreover,

E[ ¯B((cid:96), k)T(cid:96),k|D((cid:96)) = 0]

=E[T(cid:96),k|B((cid:96), k) = 0, D((cid:96)) = 0]P (B((cid:96), k) = 0|D((cid:96)) = 0)

≤

N (cid:96)
2p − 1

P (B((cid:96), k) = 0|D((cid:96)) = 0),

the

last

where
from applying
equation follows
Lemma 1 and iterated conditional expectation. Thus,
we know

(9) =

P (B((cid:96), k) = 0|D((cid:96)) = 0)E[ ¯D((cid:96))]

N −1
(cid:88)

k=1

N −1
(cid:88)

k=1

N (cid:96)
2p − 1

N (cid:96)
2p − 1

≤

P (B((cid:96), k) = 0|D((cid:96)) = 0)

(cid:18) 1 − p
p

≤

(cid:19)(cid:96)−1 2N (cid:96)p2

(2p − 1)4 (log(N ) + 1),

(cid:19)(cid:96)−1

(cid:18) 1 − p
p

(10)

385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439

where equation (10) is because Lemma 2.

The proof of the second inequality follows very simi-
larly, and is omitted.

7. Preference Matrices

6. Proof of Theorem 2

In this section, we prove the cumulative expected weak
regret of WS-W is bounded by O(N 2) in the Condorcet
winner setting. First, we want to give an example to il-
lustrate why our algorithm will not have O(N log(N ))
regret under the Condorcet winner setting.

In the Condorcet winner setting, Lemma 2 is no longer
true. Here is a counter example to illustrate why
Lemma 2 does not hold true anymore. Suppose we
have N = 3k + 1 arms in total, which includes a
Condorcet winner arm and three types of other arms:
k type-A arms, k type-B arms and k type-C arms.
Among these arms, we assume the user prefers type-
A arms than type-B arms, type-B arms than type-C
arms and type-C arms than type-A arms. Among each
type of arms, there is a total order. In this setting, the
expected number of iterations with a worse incumbent
is O(N ) instead of O(log(N )), which means Lemma 2
is no longer true.

Now we start our proof for Theorem 2.

Proof. In the Condorcent winner setting, Lemmas 3
and 4 hold, but as explained earlier, Lemma 2 does
not. Because the proof of Lemma 5 utilizes Lemma 2,
Lemma 5 also no longer holds.

On the other hand, since we can have at most N − 1
iterations in a round, we know the following state-
ment is true: the conditional expected number of it-
erations with a worse incumbent is bounded by N in
each round. Thus, we know Lemma 5 now becomes:

(cid:34)N −1
(cid:88)

(cid:35)
¯B((cid:96), k)T(cid:96),k ¯D((cid:96))

E

E

k=1
(cid:34)N −1
(cid:88)

k=1

¯B((cid:96), k)T(cid:96),kV ((cid:96), k)

≤

≤

(cid:35)

(cid:18) 1 − p
p

(cid:19)(cid:96)−1 N 2(cid:96)
2p − 1

,

(cid:18) 1 − p
p

(cid:19)(cid:96) N 2(cid:96)
2p − 1

.

Thus, following the same reasoning as in the proof of
Theorem 1, we know the expected weak regret in the
Condorcet winner setting is bounded by

N R
(2p − 1)2 +

pN 2
(2p − 1)3 ,

which concludes our proof.

In the sushi experiment, the user’s preference matrix
is given by Figure 1.

In the MSLR experiment, the ranker’s preference ma-
trix is given by:









0.5
0.465
0.387
0.243
0.235

0.535
0.5
0.420
0.276
0.262

0.613
0.580
0.5
0.341
0.331

0.757
0.727
0.659
0.5
0.490









0.765
0.738
0.669
0.510
0.5

8. Condorcet Winner Experiment

In the main paper, we considered numerical examples
in which the arms have a total order. This is common
in the dueling bandits literature, where even work that
considers more general settings theoretically test their
methods on problems that satisfy the total order as-
sumption (Komiyama et al., 2016; Urvoy et al., 2013).

In this section, we consider an additional example that
has a Condorcet winner but does not have a total order
among arms. The example has a cyclic struture, and
is similar to the cyclic example in Komiyama et al.
(2015).

The preference matrix is:







0.5
0.4
0.4
0.4

0.6
0.5
0.4
0.6

0.6
0.6
0.5
0.4







0.6
0.4
0.6
0.5

In the above example, arm 1 is the Condorcet winner.
Arm 2 beats arm 3, arm 3 beats arm 4 and arm 4 beats
arm 2.

Again, we consider both binary strong regret and the
utility-based strong regret. The utility-based strong
regret is deﬁned the same as the other two experi-
ments. The result is summarized in Figure 2. WS-S
outperforms all benchmarks considered in all time pe-
riods on binary regret, and outperforms them all in all
time periods except T = 102 on utility-based regret.

9. Sensitivity Analysis

In this section, we conduct a sensitivity analysis of β
In this analysis,
in WS-S using the MSLR dataset.

440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494





























0.5
0.488
0.378
0.345
0.302
0.274
0.289
0.292
0.251
0.2
0.259
0.217
0.153
0.183
0.146
0.132

0.512
0.5
0.398
0.317
0.348
0.224
0.337
0.317
0.262
0.291
0.214
0.198
0.17
0.15
0.129
0.127

0.622
0.602
0.5
0.472
0.446
0.467
0.466
0.409
0.427
0.407
0.339
0.295
0.266
0.328
0.213
0.178

0.655
0.683
0.528
0.5
0.447
0.381
0.434
0.359
0.325
0.313
0.335
0.304
0.197
0.177
0.204
0.156

0.698
0.652
0.554
0.553
0.5
0.487
0.476
0.482
0.392
0.462
0.357
0.39
0.305
0.328
0.319
0.225

0.726
0.776
0.533
0.619
0.513
0.5
0.487
0.441
0.425
0.379
0.409
0.299
0.298
0.213
0.171
0.189

0.711
0.663
0.534
0.566
0.524
0.513
0.5
0.441
0.447
0.387
0.436
0.393
0.297
0.265
0.264
0.199

0.708
0.683
0.591
0.641
0.518
0.559
0.559
0.5
0.444
0.473
0.438
0.42
0.332
0.195
0.223
0.233

0.749
0.738
0.573
0.675
0.608
0.575
0.553
0.556
0.5
0.488
0.452
0.458
0.388
0.214
0.29
0.315

0.8
0.709
0.593
0.687
0.538
0.621
0.613
0.527
0.512
0.5
0.457
0.421
0.387
0.282
0.315
0.253

0.741
0.786
0.661
0.665
0.643
0.591
0.564
0.562
0.548
0.543
0.5
0.436
0.375
0.382
0.298
0.316

0.783
0.802
0.705
0.696
0.61
0.701
0.607
0.58
0.542
0.579
0.564
0.5
0.458
0.356
0.3
0.267

0.847
0.83
0.734
0.803
0.695
0.702
0.703
0.668
0.612
0.613
0.625
0.542
0.5
0.423
0.393
0.404

0.817
0.85
0.672
0.823
0.672
0.787
0.735
0.805
0.786
0.718
0.618
0.644
0.577
0.5
0.422
0.363

0.854
0.871
0.787
0.796
0.681
0.829
0.736
0.777
0.71
0.685
0.702
0.7
0.607
0.578
0.5
0.414





























0.868
0.873
0.822
0.844
0.775
0.811
0.801
0.767
0.685
0.747
0.684
0.733
0.596
0.637
0.586
0.5

Figure 1: User’s preference matrix for the Sushi experiment

(a) Cyclic dataset with utility-based strong regret

(b) Cyclic dataset with binary strong regret

Figure 2: Comparison of the strong regret between WS-S and 7 benchmarks on the cyclic dataset. WS-S
outperforms all benchmarks in all settings studied.

we choose β = 1.01, 1.05, 1.1, 1.2, 1.5 respectively and
compare them with RMED and RUCB. The result is
summarized in Figure 3.

ever, as long as β is within a reasonable range, WS-S
can outperform existing state-of-art algorithms.

References

Barndorﬀ-Nielsen, Ole E and Shiryaev, Albert.
Change of time and change of measure, volume 21.
World Scientiﬁc Publishing Co Inc, 2015.

Karlin, Samuel. A First Course In Stochastic Pro-

cesses. Academic Press, 1968.

Komiyama, Junpei, Honda, Junya, Kashima, Hisashi,
and Nakagawa, Hiroshi. Regret lower bound and
optimal algorithm in dueling bandit problem.
In
COLT, pp. 1141–1154, 2015.

Komiyama, Junpei, Honda, Junya, and Nakagawa,
Hiroshi. Copeland dueling bandit problem: Re-
gret lower bound, optimal algorithm, and com-
putationally eﬃcient algorithm.
arXiv preprint
arXiv:1605.01677, 2016.

Urvoy, Tanguy, Clerot, Fabrice, F´eraud, Raphael, and
Naamane, Sami. Generic exploration and k-armed
voting bandits. In ICML (2), pp. 91–99, 2013.

Figure 3: Sensitivity Analysis

Based on Figure 3, WS-S with β = 1.05, 1.1, 1.2 out-
performs RMED and RUCB. When β = 1.01, we
spend too much time on the exploration period and
do not exploit enough. Similarly, WS-S with β = 1.5
over exploits and does not explore enough.
In both
cases, WS-S underperforms RMED and RUCB. How-

Wang, Y.H. On the number of success in independent

trials. Statistica Sinica, 1993.

495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549

