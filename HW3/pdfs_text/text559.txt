High-Dimensional Structured Quantile Regression

Vidyashankar Sivakumar 1 Arindam Banerjee 1

Abstract

Quantile regression aims at modeling the condi-
tional median and quantiles of a response vari-
In this
able given certain predictor variables.
work we consider the problem of linear quantile
regression in high dimensions where the num-
ber of predictor variables is much higher than
the number of samples available for parameter
estimation. We assume the true parameter to
have some structure characterized as having a
small value according to some atomic norm R(·)
and consider the norm regularized quantile re-
gression estimator. We characterize the sam-
ple complexity for consistent recovery and give
non-asymptotic bounds on the estimation error.
While this problem has been previously consid-
ered, our analysis reveals geometric and statisti-
cal characteristics of the problem not available in
prior literature. We perform experiments on syn-
thetic data which support the theoretical results.

1 Introduction

Considerable advances have been made over the past
decade on ﬁtting high-dimensional structured linear mod-
els when the number of samples n is much smaller than
the ambient dimensionality p (Banerjee et al., 2014; Ne-
gahban et al., 2012; Chandrasekaran et al., 2012). Most
of the advances have been made for linear models: yi =
(cid:104)xi, θ∗(cid:105) + ωi, i = 1, . . . , n, where θ∗ ∈ Rp is assumed
to be structured, e.g., sparse, group sparse, etc. Estima-
tion of such structured θ is usually done using Lasso-type
regularized estimators (Negahban et al., 2012; Banerjee
et al., 2014) or Dantzig-type constrained estimators (Chan-
drasekaran et al., 2012; Chatterjee et al., 2014); other re-
lated estimators have also been explored (Hsu & Sabato,

1Department of Computer Science & Engineering, University
of Minnesota, Twin Cities. Correspondence to: Vidyashankar
Sivakumar <sivak017@umn.edu>, Arindam Banerjee <baner-
jee@cs.umn.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

2016; Vershynin, 2015). Such models have been extended
to generalized linear models (Banerjee et al., 2014; Ne-
gahban et al., 2012), matrix completion (Cand`es & Recht,
2009), vector auto-regressive models (Melnyk & Banerjee,
2016) among others.

(τ |xi) = (cid:104)xi, θ∗

In this paper, we consider the problem of structured quan-
tile regression in high-dimensions, which can be posed as
follows: given the response variable yi and covariates xi
the τ th conditional quantile function of yi given xi is given
by: F −1
τ (cid:105), τ ∈ (0, 1) for some structured
yi|xi
θ∗
τ whose structure can be captured by a suitable atomic
norm R(·), e.g., l1-norm for sparsity, l1/l2 norm for group
sparsity, etc. Here F −1
(·) is the inverse of the conditional
yi|xi
distribution function of yi given xi. We consider the fol-
lowing regularized estimator for the structured quantile re-
gression problem:

ˆθn := argminθ∈Rp Lτ (θ) + λnR(θ)

:= argminθ∈Rp

ρτ (yi − (cid:104)xi, θ(cid:105)) + λnR(θ) ,

1
n

n
(cid:88)

i=1

(1)

where ρτ (u) = (τ − I(u ≤ 0))u is the asymmetric abso-
lute deviation function (Koenker, 2005), I(·) is the indica-
tor function. The goal is to get nonasymptotic bounds on
the estimation error (cid:107)ˆθ − θ∗(cid:107)2.

Many previous papers analyze the asymptotic performance
of the estimator in (1) (Li & Zhu, 2008; Kai et al., 2011;
Wu & Liu, 2009; Zou & Yuan, 2008; Wang et al., 2012). In
the non-asymptotic setting of interest in this paper, special
cases of the estimator in (1) have been studied in recent lit-
erature (Belloni & Chernozhukov, 2011; Kato, 2011; Fan
et al., 2014a), primarily focusing on speciﬁc norms like
the l1-norm and nonoverlapping group sparse norm.
In
contrast, our formulation and analysis is applicable to any
atomic norm R(·) giving considerable ﬂexibility in choos-
ing a suitable structure for real world problems, e.g., hier-
archical sparsity, k-support norm, OWL norm etc. More
recently (Alquier et al., 2017) consider the more general
problem of norm regularized regression with Lipschitz loss
functions which includes (1) as a special case. They derive
similar results to ours for bounds on the estimation error,
but their analysis differs signiﬁcantly and, in our opinion,
does not leverage and highlight the key geometric and sta-

High-Dimensional Structured Quantile Regression

tistical characteristics of the problem.

In the setting of norm regularized regression with square
loss, including the widely studied Lasso estimator (Tibshi-
rani, 1996; Negahban et al., 2012; Banerjee et al., 2014),
the sample complexity n0 of the estimator gets determined
by a certain restricted strong convexity (RSC) property
which simpliﬁes to the restricted eigenvalue (RE) condi-
tion on the matrix X T X (Bickel et al., 2009; Negahban
et al., 2012); in the noiseless setting, i.e., when ωi = 0,
the sample complexity determines a phase transition phe-
nomenon so that the probability of recovering the struc-
tured θ∗ is minimal when n < n0, and one can exactly
recover θ∗ with high probability when n > n0. Our work
gives an equivalent sample complexity characterization for
structured quantile regression, which was not available in
prior work. The challenge in characterizing RSC in the
context of quantile regression stems partly from the non-
smoothness of the objective, so one has to work with sub-
gradients. However, the unique aspect stems from the ge-
ometry of quantile regression, or as the authoritative book
on the topic puts it: “How quantile regression works?”
(Koenker, 2005)[Section 2.2]. In quantile regression, the
n samples get divided into three subsets: ν samples which
get exactly interpolated, i.e., yi = (cid:104)xi, ˆθ(cid:105), (n − ν)τ sam-
ples which lie below the curve, i.e., yi < (cid:104)xi, ˆθ(cid:105), and
(n − ν)(1 − τ ) samples which lie above the curve, i.e.,
yi > (cid:104)xi, ˆθ(cid:105). Note that when ν = n all samples are in-
terpolated, the loss is zero and the same ˆθ is a solution for
all quantiles τ . Quantile regression is clearly not working.
The Number of InterPolated Samples (NIPS) ν is an im-
portant quantity, inherent to structure in θ∗, and determines
the sample complexity of structured quantile regression (1).
In fact, we show that when n > ν, the RSC condition as-
sociated with the estimator in (1) is satisﬁed. When there
is no structure in θ∗, then ν = O(p), and quantile regres-
sion needs n > O(p) samples to work. However, when θ∗
has structure, such as sparsity or group sparsity, ν can be
substantially smaller than p. Speciﬁcally we show that ν
is of the order of square of Gaussian width of the error set
(Talagrand, 2014; Chandrasekaran et al., 2012) for a class
of atomic norms which includes l1, l1/l2 group sparse, k-
support (Argyriou et al., 2012) and the OWL (Bogdan et al.,
2013) norms. The Gaussian width as a measure of com-
plexity of a set has been extensively used in prior literature
(Chandrasekaran et al., 2012; Tropp, 2015; Banerjee et al.,
2014). For example, when θ∗ is sparse with s non-zero
entries, we show that ν = cs log p.

When n > ν and the RSC condition is satisﬁed, build-
ing on recent developments in high-dimensional estima-
tion (Negahban et al., 2012; Banerjee et al., 2014), we show
that choosing λn ≥ 2R∗(∇Lτ (θ∗))), where R∗(·) is the
dual norm of R(·), leads to non-asymptotic bounds on the
estimation error (cid:107)ˆθn − θ∗(cid:107)2. While the speciﬁcation of

λn looks complex, with its dependency on the dual norm
and its dependency on θ∗, we show it is sufﬁcient to set
λn based on the Gaussian width (Talagrand, 2014) of the
unit norm ball for R(·) (Banerjee et al., 2014; Sivakumar
et al., 2015)‘. Our analysis and results on the estimation
error bound for quantile regression, interestingly, has the
same order as that for regularized least squares regression
for general norms (Banerjee et al., 2014).
In contrast to
the least squares loss the quantile loss is more robust as
the estimation error is independent of the two norm of the
noise. We discuss results for the l1, l1/l2 group sparse and
k-support norms as examples, precisely characterizing the
sample complexity for recovery and non-asymptotic error
bounds. Speciﬁcally, our results for the l1-norm matches
those from existing literature on sparse quantile regres-
sion (Belloni & Chernozhukov, 2011).

The rest of the paper is organized as follows. In Section 2,
we discuss the problem formulation along with assump-
tions, review the general framework for analyzing regu-
larized estimation problems and discuss the three atomic
norms used as examples throughout the paper.
In Sec-
tion 3, we analyze the number of interpolated samples and
establish precise sample complexities for a class of atomic
norms in terms of the Gaussian widths of sets. In Section 4
we establish key ingredients of the analysis and provide the
main bound. We present experimental results in Section 5
and conclude in Section 6.

2 Background and Preliminaries

Problem formulation: We outline the assumptions on the
data and estimator. Similar conditions are present in all
prior literature on quantile regression and we refer to Sec-
tion 2.5 in Belloni & Chernozhukov (2011) for examples
of data satisfying the conditions.
We consider data is generated as y = Xθ + ω, X ∈ Rn×p
is the design matrix, θ ∈ Rp and ω ∈ Rn is the noise. We
assume subGaussian design matrices X ∈ Rn×p which in-
cludes the class of all bounded random variables. Note that
this is not a restrictive assumption as to avoid the quan-
tile crossing phenomenon the covariate space has to be
bounded. Quantile crossing is when the value of the τ1th
quantile is greater than the τ2th quantile for some τ1 < τ2
(See Section 2.5 in (Koenker, 2005)). We do not make any
assumptions on the noise vector ω ∈ Rn. More speciﬁcally
the noise can be sampled from a heavy tailed distribution,
can be heteroscedastic as in the location-scale model where
ωi = (cid:104)xi, η(cid:105) · (cid:15)i, η ∈ Rp, (cid:15)i is noise independent of xi, bi-
modal and so on and so forth. Note that this setting is more
general than for the least squares loss.

We consider a parametric quantile regression model where
the τ th conditional quantile function of the response vari-

High-Dimensional Structured Quantile Regression

able yi given any xi ∈ Rp is given by,

F −1
yi|xi

(τ |xi) = (cid:104)xi, θ∗

τ (cid:105), θ∗

τ ∈ Rp, τ ∈ (0, 1) ,

(2)

where F −1
is the inverse of the conditional distribution
yi|xi
function of yi given xi. We will assume the conditional
density of yi evaluated at the conditional quantile (cid:104)xi, θ∗
τ (cid:105)
is bounded away from zero uniformly for all τ , that is,
fyi|xi((cid:104)xi, θ∗
τ (cid:105)) > f > 0 for all τ and all xi. The goal
is to estimate parameter ˆθτ close to θ∗
τ using n observa-
tions of the data when n < p. The estimator in this paper
belongs to the family of regularized estimators and is of the
form:

ˆθλn,τ := argminθ∈Rp Lτ (θ) + λnR(θ) ,

(3)

(cid:80)n

where Lτ (θ) = 1
i=1 ρτ (yi −(cid:104)xi, θ(cid:105)), ρτ (·) is the quan-
n
tile loss function and R(·) is any atomic norm. Examples
of atomic norms we consider in this paper are the l1, l1/l2
nonoverlapping group sparse norm and the k-support norm.
We present all results assuming any single τ ∈ (0, 1) and
going forward drop the subscripts from θ∗

τ and ˆθλn,τ .

Gaussian Width: All results will be in terms of the Gaus-
sian width of suitable sets. For any set A ∈ Rp, the Gaus-
sian width of the set A is deﬁned as (Gordon, 1985; Chan-
drasekaran et al., 2012):

w(A) = Eg

(cid:104)g, u(cid:105)

.

(4)

(cid:21)

(cid:20)

sup
u∈A

where the expectation is over g ∼ N (0, Ip×p). Gaus-
sian widths have been widely used in prior literature on
structured estimation (Chandrasekaran et al., 2012; Baner-
jee et al., 2014; Sivakumar et al., 2015; Tropp, 2015).

High-dimensional estimation: Our analysis is built on
developments over the past decade for high-dimensional
structured regression for linear and generalized linear mod-
els using both regularized as well as constrained estimators
(Candes & Tao, 2007; Bickel et al., 2009; Chandrasekaran
et al., 2012; Negahban et al., 2012; Banerjee et al., 2014).
For the regularized formulation considered in this work
Banerjee et al. (2014); Negahban et al. (2012) have estab-
lished a generalized analysis framework when the loss is
least squares or more generally the maximum likelihood
estimator for generalized linear models. We give a brief
overview of the main components of the analysis.

1. Regularization parameter: In Banerjee et al. (2014);
Negahban et al. (2012) the regularization parameter is as-
sumed to satisfy the following assumption,

λn ≥ 2R∗(∇Lτ (θ∗)) .

(5)

With ΩR = {u|R(u) ≤ 1} denoting the unit norm ball,
Banerjee et al. (2014) prove that with high probability a

value λn = O(w(ΩR)) satisﬁes the above condition for
subGaussian design matrices, noise and the least squares
loss.

2. Error set: The assumption on the regularization pa-
rameter ensures that the error vector ∆ = ˆθ − θ∗ lies in the
following error set (Banerjee et al., 2014),

(cid:26)

C :=

∆ | R(θ∗ + ∆) ≤ R(θ∗) +

R(∆)

.

(6)

(cid:27)

1
2

3. The norm compatibility constant: It is deﬁned as fol-
lows (Negahban et al., 2012; Banerjee et al., 2014),

Ψ(C) = sup
u∈C

R(u)
(cid:107)u(cid:107)2

.

4. Restricted Strong Convexity (RSC): In Banerjee et al.
(2014); Negahban et al. (2012) the loss function is shown to
satisfy the following RSC condition with high probability
once the number of samples is of the order of the square of
the Gaussian width of the error set, that is, n = O(w2(C)).

inf
u∈C

δL(θ∗, u) = inf
u∈C
≥ κ(cid:107)u(cid:107)2 .

(L(θ∗ + u) − L(θ∗) − (cid:104)∇L(θ∗), u(cid:105))

For the squared loss, the RSC condition simpliﬁes to the
Restricted Eigenvalue (RE) condition (Bickel et al., 2009)

inf
u∈C

1
n

(cid:107)Xu(cid:107)2

2 ≥ κ(cid:107)u(cid:107)2
2 .

(7)

(8)

(9)

5. Recovery Bounds: When RSC and bounds on the
regularization parameter are satisﬁed Banerjee et al. (2014)
prove the following deterministic error bound,

(cid:107)∆(cid:107)2 = (cid:107)ˆθ − θ∗(cid:107)2 ≤ c

Ψ(C)w(ΩR)
κ

.

(10)

where c is any constant.

Atomic Norms: We consider the class of atomic norms
for the regularizer. Mathematically consider a set A ⊆ Rp
the collection of atoms that is compact, centrally symmetric
about the origin (that is, a ∈ A =⇒ −a ∈ A). Let (cid:107)θ(cid:107)A
denote the gauge of A,

R(θ) = (cid:107)θ(cid:107)A = inf{t > 0 : θ ∈ tconv(A)}
(cid:88)

(cid:88)

= inf{

ca : θ =

caa, ca ≥ 0, ∀a ∈ A} .

a∈A

a∈A

(11)

(12)

For example when A = {±ei}p
i=1 yields (cid:107)θ(cid:107)A = (cid:107)θ(cid:107)1.
Although the atomic set A may contain uncountably many
vectors, we assume A can be decomposed as a union of m
sets, A = Ai∪A2∪. . . Am similar to the setting considered

High-Dimensional Structured Quantile Regression

in Chen & Banerjee (2015). Such a decomposition assump-
tion is satisﬁed by many popular atomic norms like the l1,
l1/l2 group sparse norms, k-support norm, OWL norm etc.
Throughout the paper we will illustrate our results on the
following norms.

1. l1 norm: For the l1 norm we will consider that θ∗ is an
s-sparse vector, that is, (cid:107)θ∗(cid:107)0 = s.

l1/l2 nonoverlapping group sparse norm:
2.
Let
{G1, G2, . . . , GNG } denote a collection of groups which are
blocks of any vector θ ∈ Rp. Let θNG denote a vector with
coordinates θi
= θi if i ∈ GNG , else θi
= 0. The max-
imum size of any group is l = max
|Gi|. The norm
is given as R(θ) = (cid:80)NG
i=1 (cid:107)θi(cid:107)2. Let SG ⊆ {1, 2, . . . , NG}
with cardinality |SG| = sG. We consider the true parameter
θ∗ ∈ Rp is sG-sparse, that is, θ∗
NG

= (cid:126)0, ∀NG /∈ SG.

i∈[1,...,NG ]

NG

NG

3. k-support norm: The k-support norm can be expressed
as an inﬁmum convolution given by (Argyriou et al., 2012),

R(θ) = inf

(cid:80)

i ui=θ

(cid:40)

(cid:88)

i

(cid:107)ui(cid:107)2

| (cid:107)ui(cid:107)0 ≤ k

.

(13)

(cid:41)

Clearly it is an atomic norm for which A = {a ∈
(cid:1)
| (cid:107)a(cid:107)0 ≤ k, (cid:107)a(cid:107)2 = 1} and A is a union of (cid:0)p
Rp
subsets, that is, A = A1 ∪ A2 ∪ . . . A(p
k). More results
on the k-support norm can be looked up in Chatterjee et al.
(2014); Chen & Banerjee (2015). We consider the setting
where (cid:107)θ∗(cid:107)0 = s and k < s. For the results we require
the Gaussian widths of the unit norm ball, error set and the
norm compatibility constants for the norms. We provide
them below for reference. All values are in order notation.

k

Norm
l1
l1/l2
k-sp

w(ΩR)
√
log p
c
√
l + log NG
c
c(cid:112)k + k log(cid:100) p
k (cid:101)

w(C)
√
s log p
c
lsG + sG log NG
c(cid:112)s + s log(cid:100) p
k (cid:101)

√
c

Ψ(C)
√
s
c
√
c
sG
c(cid:112)2s/k

3 Number of InterPolated Samples (NIPS)

We begin with intuitions on the geometry of the problem.
In the high sample, low dimension setting n >> p, when
R(θ) = 0, the quantile loss is a linear program and hence
the solutions are at the vertices, that is, where any p of the
n samples are interpolated. Mathematically we deﬁne the
quantity Z = {i : yi = (cid:104)xi, ˆθ(cid:105) = (cid:104)xi, θ∗ + u(cid:105), u ∈ Rp}
and note that ν = sup
In the high di-
u∈Rp

|Z| = O(p).

mensional setting considered in this paper n < p and
hence with R(θ) = 0 the number of interpolated sam-
ples is ν = n. From an optimization perspective there
are multiple such solutions and all solutions are optimal
for all quantile parameters τ . But practically quantile re-
gression is not working. Now introducing a regularizer

with a suitable choice for the regularization parameter en-
sures that the error vector lies in a restricted subset of Rp,
C := (cid:8)u (cid:12)
2 R(u) (cid:9) ⊆ Rp. We are
(cid:12)R(θ∗ + u) ≤ R(θ∗) + 1
|Z|, Z = {i :
now interested in characterizing ν = sup
u∈C

yi = (cid:104)xi, ˆθ(cid:105) = (cid:104)xi, θ∗ + u(cid:105), u ∈ C}, that is, the maximum
number of interpolated samples with the error restricted to
a particular subset of Rp. Again if ν = n, quantile regres-
sion is not working. Since there are no restrictions on the
number of non-zero elements in the error vector u a ﬁrst
crude estimate will be ν ≤ min{n, p, (cid:107)u(cid:107)0}, which im-
plies quantile regression will not work unless we have a
minimum of p samples. But intuitively ν should depend on
properties of the error set C, which the initial crude estimate
is failing to take advantage of.

Below we state a result which reinforces the intuition of the
relation between ν and the properties of the set C. Specif-
ically we show that for the types of atomic norms consid-
ered in this work (which includes all popularly known vec-
tor norms) the number of interpolated samples does not ex-
ceed the product of the square of the norm compatibility
constant and the square of the Gaussian width of the unit
norm ball. For the norms considered, this is precisely the
square of the Gaussian width of the error set C. For exam-
ple for the l1 norm for an s-sparse vector this evaluates to
an upper bound of ν = O(s log p). While the result state-
ment considers subGaussian design matrices, the result will
also hold for design matrices sampled from heavy-tailed
distributions using arguments similar to Lecu´e & Mendel-
son (2014); Sivakumar et al. (2015).

Theorem 3.1 Consider X has isotropic subGaussian rows
and θ∗ is an s-sparse vector that can be written as a linear
combination of k atoms from an atomic set of cardinality
m,

θ∗ =

k
(cid:88)

i=1

ciai, ai ∈ A, ci ≥ 0, |A| = m .

(14)

For the regularized quantile regression problem penalized
with the atomic norm R(θ) = (cid:107)θ(cid:107)A,

ˆθ = arg min
θ∈Rp

Lτ (θ)+λR(θ) = arg min
θ∈Rp

ρτ (θ)+λR(θ) ,

1
n

n
(cid:88)

i=1

(15)
2 R(u)(cid:9) denote the
let C := (cid:8)u|R(θ∗ + u) ≤ R(θ∗) + 1
error set, let λ ≥ R∗(∇Lτ (θ∗)) and let n ≥ (c1s +
c2Ψ2(C)w(A)) where Ψ(C) = sup
is the norm
u∈C

(cid:107)u(cid:107)A
(cid:107)u(cid:107)2
in the error set, w(A) is the
compatibility constant
Gaussian width of the unit norm ball and c1 and c2
are some constants. Then with probability atleast 1 −
exp(−c2k1 log(em)) − 2 exp(−ηw2(C)) the number of in-

High-Dimensional Structured Quantile Regression

terpolated samples,

ν = sup
u∈C

|{i : yi = (cid:104)xi, θ∗ +u(cid:105), u ∈ C}| ≤ cΨ2(C)w2(A) ,

(16)

where c is a constant.

To understand the intuition consider the case of the l1 norm.
For an error vector lying in a particular s-dimensional sub-
space the maximum number of interpolated samples is
O(s) with high probability. Extending the argument to all
such s-dimensional subspaces by a union bound argument
on the (cid:0)p
(cid:1) subspaces, the maximum number of interpolated
samples when the error vector is any s-sparse vector in p-
dimensional space is O(s log p). Finally the argument is
extended to all vectors in the error set using the power-
ful Maurey’s empirical approximation argument previously
employed in Sivakumar et al. (2015); Lecu´e & Mendelson
(2014); Rudelson & Zhou (2013).

s

Suprisingly in prior literature on structured high dimen-
sional quantile regression, the importance of ν has not been
explicitly discussed. This intuition about the importance of
ν also shows up in an elegant form in the analysis of the
RSC condition in Section 4.2.

Below we provide results for the number of interpolated
samples for the l1, l1/l2 nonoverlapping group sparse and
For the l1/l2 nonoverlapping group
k-support norms.
sparse norm and the k-support norm we ﬁrst illustrate that
they are atomic norms. The results then follow from sub-
stituting known values for the norm compatibility constant
and Gaussian width of unit norm ball for the different
norms. For computation of these quantities for any general
norm we refer the interested reader to work in Vershynin
(2015); Chen & Banerjee (2015).

in the same direction of βij, otherwise cij = 0. Therefore
the group sparse norm is an atomic norm with a hierarchi-
cal set structure with the number of elements m = NG in
the outer set with each element of the outer set itself being
a set of an inﬁnite number of elements with any one ele-
ment chosen for a particular vector θ, that is, cij (cid:54)= 0 for
only one j amongst an inﬁnite number of j’s.

Corollary 2 Consider the l1/l2 nonoverlapping group-
sparse norm with n > (c1sGl + c2sG log NG). With high
probability,

ν = sup
u∈C

|{i : yi = (cid:104)xi, θ∗ + u(cid:105)}| ≤ c(lsG + sG log NG) ,

(19)

for some constant c.

For

the
{(cid:80)

i (cid:107)ui(cid:107)2

inf
i ui=θ

(cid:80)
expressed as,

k-support
|

norm

=
(cid:107)ui(cid:107)0 ≤ k}, can be similarly

(cid:107)θ(cid:107)sp
k

θ =

cijβij

(20)

(p
k)
(cid:88)

(cid:88)

i=1

j

where βij is a unit vector in k-dimensional subspace i. The
difference compared to the nonoverlapping group sparse
norm is that many of the cij’s can now be non zero in
the inner sum. This is comparably more complex than the
group-sparse norm where the inner set becomes a singleton
for some θ, but in terms of the analysis nothing changes.

Corollary 3 Consider the k-support norm with n > (c1s+
c2s log(cid:100) p

k (cid:101)). With high probability,

Corollary 1 For the l1 norm with θ∗ being an s-sparse
vector, when n > (c1s + c2s log p) then with high prob-
ability,

ν = sup
u∈C

|{i : yi = (cid:104)xi, θ∗ + u(cid:105)}| ≤ c(s + s log(cid:100)p/k(cid:101)) ,

(21)

for some constant c.

ν = sup
u∈C

|{i : yi = (cid:104)xi, θ∗ + u(cid:105)}| ≤ cs log p ,

(17)

for some constant c.

Before applying the result to the nonoverlapping group
sparse norm note that for any vector θ ∈ Rp,

θ =

cijβij ,

(18)

NG
(cid:88)

(cid:88)

i=1

j

where βij is any unit norm vector in subspace deﬁned by
the group i. For any group i, let θi denote the vector con-
structed from θ such that it has component k, θk = 0 if
k /∈ i. Now by deﬁnition of atomic norm cij = (cid:107)θi(cid:107)2 for θi

4 Structured Quantile Regression

In this section, we present results for the key components
in the general analysis framework of Banerjee et al. (2014)
which we brieﬂy described in Section 2 of the paper. We
start with results on the regularization parameter by ana-
lyzing equation (5) before establishing sample complexity
bounds when the restricted strong convexity condition in
equation (8) is satisﬁed. Finally an l2 bound on the error
is obtained using (10). We will consider subGaussian de-
sign matrices throughout. All results are in terms of Gaus-
sian widths of sets and the norm compatibility constant.
Results for l1, l1/l2-nonoverlapping group sparse and k-
support norms are given for illustration purposes.

High-Dimensional Structured Quantile Regression

4.1 Regularization Parameter λn

We analyze the bound in equation (5). In prior literature a
bound has been established speciﬁcally for the l1 norm in
Belloni & Chernozhukov (2011) (See Theorem 1). Below
we consider the case of any general atomic norm and ob-
tain a result in terms of the Gaussian width of the unit norm
ball. The analysis follows from a similar result for the reg-
ularization parameter in Banerjee et al. (2014) for the least
squares case.

Theorem 4.1 Let X ∈ Rn×p be a design matrix with in-
dependent isotropic subGaussian rows with subGaussian
≤ κ. Deﬁne ΩR = {u : R(u) ≤ 1} the unit
norm |||xi|||ψ2
(cid:107)u(cid:107)2/R(u). Then the following
norm ball and let φ = sup

holds

u

E [R∗(∇Lτ (θ∗))] ≤ c

(cid:112)τ (1 − τ )w(ΩR)
√
n

,

(22)

where c is any ﬁxed constant depending only on the sub-
Gaussian norm κ. Moreover with probability atleast 1 −
− 2 exp (cid:0)−2t2(cid:1)

(cid:17)2(cid:19)

(cid:16) τ

−

(cid:18)

c1 exp

c2φκ

R∗(∇Lτ (θ∗)) ≤ c

(cid:112)τ (1 − τ )w(ΩR) + t
√
n

,

(23)

where c1, c2, t are absolute constants.

A major difference to the least squares loss setting, is the
independence of the regularization parameter to assump-
tions on the noise vector (see for example Theorem 3 and
Theorem 4 in Banerjee et al. (2014) where the noise is ex-
plicitly assumed to be subGaussian and homoscedastic and
the noise enters the analysis through properties of (cid:107)ω(cid:107)2).
This gives the ﬂexibility of considering noise vectors which
are heavy tailed or heteroscedastic. Indeed the most inter-
esting applications of quantile regression arise in such set-
tings.

Below we provide bounds for the regularization parame-
ter for different norms by substituting known values of the
Gaussian width for the unit norm balls. The result for the l1
norm matches with Theorem 1 in Belloni & Chernozhukov
(2011) for the regularization parameter.

Corollary 6 If R(·) is the k-support norm, with high prob-
ability,

R∗(∇Lτ (θ∗)) ≤

c(cid:112)τ (1 − τ )(cid:112)k + k log(cid:100) p

k (cid:101) + t

.

√

n

4.2 Restricted Strong Convexity (RSC)

The loss needs to satisfy the RSC condition in equation (8).
Prior literature on structured quantile regression has not
discussed the RSC condition explicitly, though Fan et al.
(2014b) considers it for the quantile huber loss function.

We start by providing an intuition for the RSC formulation
for the quantile loss. The RSC condition equation (8) on
the error set C evaluates to the following,

inf
u∈C

1
n

n
(cid:88)

(cid:90) (cid:104)xi,u(cid:105)

i=1

0

Let ν = sup
u∈C

|Z| = sup
u∈C

(I(yi − (cid:104)xi, θ∗(cid:105) ≤ z) − I(yi − (cid:104)xi, θ∗(cid:105) ≤ 0)) dz .

(25)
|{i|yi = (cid:104)xi, θ∗ + u(cid:105)}| is the num-

ber of interpolated samples. For any n < p if the model
can interpolate all points, that is, ν = n then (25) eval-
In general, as shown in Section 3, ν gets
uates to zero.
determined by the structure. For example for the l1 norm
ν = O(s log p) rather than the ambient dimensionality p.
Thus, the sum over n points in (25) simply reduces to the
sum over the (n − ν) points which are not interpolated, and
will ensure the RSC condition when n > ν. The intuition
of the NIPS property of Section 3 thus shows up elegantly
in the RSC condition.

(I(ξi ≤ z) − I(ξi ≤ 0))

let ξi = yi − (cid:104)xi, θ∗(cid:105), vi =
consider
and

In equation (25),
(cid:82) (cid:104)xi,u(cid:105)
0
(cid:80)n
1
n

i=1 E[vi],

1
n

n
(cid:88)

i=1

E[vi] =

(Fi(ξi + z) − Fi(ξ))

fi(ξi)zdz + o(1)

(cid:90) (cid:104)xi,u(cid:105)

(cid:90) (cid:104)xi,u(cid:105)

0

0

n
(cid:88)

i=1
n
(cid:88)

1
n

1
n

i=1
n
(cid:88)

i=1

1
2n

f
2n

=

=

≥

fi(ξi)(cid:104)xi, u(cid:105)2 + o(1)

(cid:107)Xu(cid:107)2

2 ≥

f κ(cid:107)u(cid:107)2
2
2

.

Corollary 4 If R(·) is the l1 norm, with high probability

R∗(∇Lτ (θ∗)) ≤ c

(cid:112)τ (1 − τ )
√

√

n

log p + t

.

(24)

Corollary 5 If R(·) is the l1/l2 nonoverlapping group
sparse norm, with high probability,

R∗(∇Lτ (θ∗)) ≤

c(cid:112)τ (1 − τ )(cid:112)l + log NG + t
√
n

.

The ﬁrst line follows from the deﬁnition of the cumula-
tive distribution function, the second line by a simple Tay-
lor series expansion, the last line by the assumption that
f ≤ fi(ξi), ∀i and (1/n)(cid:107)Xu(cid:107)2
2 ≥ κ, where κ is the re-
stricted eigenvalue (RE) constant. The RE condition is sat-
isﬁed as the sample complexity bounds for satisfying the
NIPS property is same as the RE condition. More generally
RSC is a condition on the minimum eigenvalue of the Jaco-
bian matrix 1
i=1 fi(ξi)(cid:104)xi, u(cid:105)2 restricted to the error set
n

(cid:80)n

High-Dimensional Structured Quantile Regression

C. This quantity has also been considered in prior literature
(see Section 4.2 in Koenker (2005) and the proof in page
121, also see condition D.1 in Belloni & Chernozhukov
(2011)). While the above analysis is in expectation of the
quantity vi, we state the following result giving large devi-
ation bounds for the above quantity.

Theorem 4.2 Consider X ∈ Rn×p has subGaussian rows.
Let 0 < f < fi((cid:104)xi, θ∗(cid:105)) be a uniform lower bound on the
conditional density for all xi in the support of x. Let κ de-
note the RE constant satisfying 1
2. Let the
number of samples n > cw2(C) where w(C) is the Gaus-
sian width of the error set C and c is some constant. Then
(cid:17)
1f ξ
with probability atleast 1 − exp(−τ 2/2) − exp
2 n
where φ1, ξ < 1 and τ are constants,

2 ≥ κ(cid:107)u(cid:107)2

n (cid:107)Xu(cid:107)2

−

φ2

(cid:16)

δLτ (θ∗, u) ≥ c1κf (cid:107)u(cid:107)2
2 .

inf
u∈C

(26)

where c1 < 0 is a constant.

Below we provide results for different norms. The sample
complexity for the l1 norm matches the result in Belloni &
Chernozhukov (2011) (see equation 2.10).

Corollary 7 For the l1 norm with n > cs log p with high
probability the following RSC condition is satisﬁed,

δLτ (θ∗, u) ≥ cκf (cid:107)u(cid:107)2
2 .

inf
u∈C

Corollary 8 For the l1/l2 nonoverlapping group sparse
norm with n > c(lsG + sG log NG) with high probability
the following RSC condition is satisﬁed,

δLτ (θ∗, u) ≥ κf (cid:107)u(cid:107)2
2 .

inf
u∈C

(28)

The two norm of the error depends on the two terms
(cid:112)τ (1 − τ ) and f . The (cid:112)τ (1 − τ ) term is minimized at
the tails and hence has the effect of reducing the estimation
error. But typically this is dominated by the lower bound
on the density f term which makes the estimate less precise
in regions of low density. This is to be expected as there
are very few samples to make a very precise estimate in
low density regions. While similar observations are made
in page 72 of Koenker (2005), the results are asymptotic
while we show non-asymptotic recovery bounds. Another
aspect we reiterate here is the independence of the results
from the form of the noise. All results make no assump-
tions on the noise apart from an assumption on the lower
bound of the noise density.

Below we provide recovery bounds for the different norms
we consider in the paper.

√

Corollary 10 For
c1

log p

√

τ (1−τ )
√

n

bility

the

l1

norm when

λn

≥

and n > c2s log p with high proba-

(cid:107)∆(cid:107)2 ≤ c

√

s log p
√
n
f κ

Corollary 11 For the l1/l2 nonoverlapping group sparse
√

√

(27)

norm when λn >
sG log NG) with high probability

n

c1

τ (1−τ )
√

l+log NG

and n > c(lsG +

(cid:107)∆(cid:107)2 ≤ c

(cid:112)lsG + sG log NG
√
f κ

n

√

Corollary 12 For the k-support norm when λn >
c1
k (cid:101) with high

and n > cs + s log(cid:100) p

k+k log(cid:100) p
n

√
√

τ (1−τ )

k (cid:101)

Corollary 9 For the k-support norm with n > c(s +
s log(cid:100) p
k (cid:101)) with high probability the following RSC condi-
tion is satisﬁed,

probability

δLτ (θ∗, u) ≥ cκf (cid:107)u(cid:107)2
2 .

inf
u∈C

(29)

(cid:107)∆(cid:107)2 ≤ c

(cid:112)s + s log(cid:100) p
k (cid:101)
√
f κ

n

4.3 Recovery Bounds

Following the general framework outlined in Banerjee et al.
(2014) (see Theorem 2), we state the following high proba-
bility bound on the two norm of the error vector ∆ = ˆθ−θ∗.

Theorem 4.3 For the quantile regression problem, when
, n > c2w2(C) for some constants

τ (1−τ )w(ΩR)

√

c1

√

λn ≥
n
c1, c2 then with high probability,

(cid:107)∆(cid:107)2 ≤ c

(cid:112)τ (1 − τ )Ψ(C)w(ΩR)
f κ

.

(30)

where Ψ(C) is the norm compatibility constant in the error
set.

5 Experiments

We perform simulations with synthetic data.

5.1 Phase Transition

Data is generated as y = Xθ∗ + ω.
θ∗ =
] ∈ Rp for the l1 norm and
[1, 1, 1, 1, 1, 1
, 0, 0, . . . , 0
(cid:124)
(cid:125)
(cid:123)(cid:122)
(cid:124)
(cid:125)
(cid:123)(cid:122)
p-6
6
θ∗ = [1, . . . , 1
(cid:124) (cid:123)(cid:122) (cid:125)
5

]
, . . . , 0, . . . , 0
(cid:124) (cid:123)(cid:122) (cid:125)
5
for the l1/l2 group sparse norm with p ∈ [500, 750, 1000].
The noise ωi ∼ N (0, 0.25), ∀i ∈ [n] is Gaussian with
zero mean and 0.25 variance. The design matrix X ∼

, 0, . . . , 0
(cid:124) (cid:123)(cid:122) (cid:125)
5

, 1, . . . , 1
(cid:124) (cid:123)(cid:122) (cid:125)
5

, 1, . . . , 1
(cid:124) (cid:123)(cid:122) (cid:125)
5

(31)

(32)

(33)

High-Dimensional Structured Quantile Regression

documentation Li et al. (2015). The code was implemented
in Python. The plots in Figure 1 clearly show a phase tran-
sition for both the l1 and l1/l2 group sparse norms for all
quantiles exemplifying the NIPS property described earlier.

5.2 Robustness

(cid:124)

We showcase the robustness enjoyed by quantile regres-
sion over ordinary least squares estimation against heavy-
tailed noise and outliers. We consider the l1 norm with
] ∈ Rp. For
y = Xθ∗ + ω. θ∗ = [1, 1, 1, 1, 1, 1
, 0, 0, . . . , 0
(cid:125)
(cid:123)(cid:122)
(cid:124)
(cid:125)
494

(cid:123)(cid:122)
6
heavy-tailed noise we consider the student t-distribution
with different degrees of freedom, with lower degrees of
freedom corresponding to heavier tailed data. To show the
robustness to outliers we randomly pick a certain percent-
age of samples from the dataset and multiply the noise
by 10, that is, ωi = 10 ∗ ωi for a certain proportion
of the dataset. We vary the proportion of contamination
from 2.5% to 15%. We ﬁx n = 200 for this simulation.
Again for both exercises, we run 100 simulations and plot
the mean and standard deviation of the estimation error
(cid:107)ˆθ − θ∗(cid:107)2. The plots in Figure 2 show 1. the estimation
error against varying degrees of freedom of the student t-
distribution and 2. estimation error against the percent con-
tamination. The observations are in agreement with con-
ventional wisdom on robustness of the quantile regression
estimator to heavy-tailed noise and outliers.

6 Conclusions

The paper presents a general framework for the analysis
of non-asymptotic error and structured recovery for norm
regularized quantile regression for any atomic norm. Our
results are based on extending the general analysis frame-
work outlined in Banerjee et al. (2014); Negahban et al.
(2012) using insights from the geometry of the problem. In
particular we introduce the Number of InterPolated Sam-
ples (NIPS) as critical for determining the sample complex-
ity for consistent recovery. We prove that once the number
of samples crosses the NIPS threshold, we start recover-
ing the true parameter. This phase transition phenomena
for norm regularized quantile regression problems has not
been discussed in prior literature. We also prove that NIPS
is of the order of square of the Gaussian width of the er-
ror set for many atomic norms - which is the same order as
that for regularized least squares regression and match re-
sults from previous work for the l1 norm (Belloni & Cher-
nozhukov, 2011).

Acknowledgements: We thank reviewers for their valu-
able comments. This work was supported by NSF grants
IIS-1563950,
IIS-1422557,
CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711,
NASA grant NNX12AQ39A.

IIS-1447566,

IIS-1447574,

Figure 1: Probability of recovering true parameter versus the
rescaled sample size for l1 norm (top) and l1/l2 group sparse
norm (bottom). There is a sharp phase transition when the number
of samples exceeds NIPS

Figure 2: Estimation error of Lasso and l1-penalized quantile
regression against different degrees of freedom of the student
t-distribution noise (top) and against percentage contamination
(bottom). Quantile regression is robust to heavy-tailed noise and
outliers

N (0, Ip×p) is multivariate Gaussian with identity covari-
ance. We vary n = [10, 20, 30, . . . , 120, 130]. For each
n we generate 100 datasets with the probability of success
deﬁned as the fraction of times we are able to faithfully es-
timate the true parameter. For p = 500 we run simulations
for τ ∈ [0.1, 0.5, 0.9] and for p ∈ [750, 1000] we run sim-
ulations only for τ = 0.5. For the optimization, we use the
Alternating Direction Method of Multipliers (Boyd et al.,
2010). The details of the updates can be found in the ﬂare

0.00.51.01.52.02.53.03.5Rescaled sample size n/s log p0.00.20.40.60.81.0Probability of success0.2 quantile,p=5000.5 quantile,p=5000.8 quantile,p=5000.5 quantile,p=7500.5 quantile,p=10000.00.51.01.52.02.53.03.54.0Rescaled sample size n=(msG+sGNG)0.00.20.40.60.81.0Probability of success0.2 quantile,p=5000.5 quantile,p=5000.8 quantile,p=5000.5 quantile,p=7500.5 quantile,p=100012345Student t-distribution dof0.51.01.52.02.53.03.54.04.55.0Estimation errorLassol1 quantile regression2468101214Percent contamination0.20.40.60.81.01.21.4Estimation errorLassol1 quantile regressionHigh-Dimensional Structured Quantile Regression

References

Alquier, P., Cottet, V., and Lecue, G. Estimation Bounds
and Sharp Oracle Inequalities of Regularized Procedures
with Lipschitz Loss Functions. arXiv:1702.01402, 2017.

Argyriou, Andreas, Foygel, Rina, and Srebro, Nathan.
Sparse Prediction with the k-Support Norm. In Neural
Information Processing Systems (NIPS), apr 2012.

Banerjee, Arindam, Chen, Sheng, Fazayeli, Farideh, and
Sivakumar, Vidyashankar. Estimation with Norm Reg-
ularization. In Neural Information Processing Systems
(NIPS), 2014.

Belloni, Alexandre and Chernozhukov, Victor.

l1-
Penalized Quantile Regression in High-Dimensional
Sparse Models. The Annals of Statistics, 39(1):82–130,
2011.

Bickel, Peter J., Ritov, Yaacov, and Tsybakov, Alexan-
dre B. Simultaneous analysis of Lasso and Dantzig se-
lector. The Annals of Statistics, 37(4):1705–1732, 2009.
ISSN 0090-5364.

Bogdan, Malgorzata, Berg, Ewout van den, Su, Weijie, and
Emmanuel, Candes. Statistical Estimation and Testing
via the Sorted L1 Norm. arXiv:1310.1969, 2013.

Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
Eckstein, Jonathan. Distributed Optimization and Statis-
tical Learning via the Alternating Direction Method of
Multipliers. Foundations and Trends in Machine Learn-
ing, 3(1):1–122, 2010. ISSN 1935-8237.

Cand`es, Emmanuel J. and Recht, Benjamin. Exact Matrix
Completion via Convex Optimization. Foundations of
Computational Mathematics, 9(6):717–772, 2009. ISSN
1615-3375.

Candes, Emmanuel J. and Tao, Terence. The Dantzig se-
lector : statistical estimation when p is much larger than
n. The Annals of Statistics, 35(6):2313–2351, 2007.

Chandrasekaran, Venkat, Recht, Benjamin, Parrilo,
Pablo A., and Willsky, Alan S. The Convex Geometry
of Linear Inverse Problems. Foundations of Computa-
tional Mathematics, 12(6):805–849, 2012.

Chatterjee, Soumyadeep, Chen, Sheng, and Banerjee,
Arindam. Generalized Dantzig Selector: Application to
the k-support Norm. In Advances in Neural Information
Processing Systems, 2014.

Chen, Sheng and Banerjee, Arindam. Structured Estima-
tion with Atomic Norms: General Bounds and Appli-
cations. In Advances in Neural Information Processing
Systems, 2015.

Fan, Jianqing, Fan, Yingying, and Barut, Emre. Adaptive
Robust Variable Selection. Annals of Statistics, 42(4):
324–351, 2014a.

Fan, Jianqing, Li, Quefeng, and Wang, Yuyan. Ro-
bust Estimation of High-Dimensional Mean Regression.
arXiv:1410.2150, 2014b.

Gordon, Yehoram. Some Inequalitites for Gaussian Pro-
cesses and Applications. Israel Journal of Mathematics,
50(4):265–289, 1985.

Hsu, Daniel and Sabato, Sivan. Loss Minimization and
Parameter Estimation with Heavy Tails. Journal of Ma-
chine Learning Research, 17(18):1–40, 2016.

Kai, B., Li, R., and Zou, H. New Efﬁcient Estima-
tion and Variable Selection Methods for Semiparametric
Varying-Coefﬁcient Partially Linear Models. The Annals
of Statistics, 39:305–332, 2011.

Kato, Kengo. Group Lasso for High Dimensional Sparse
Quantile Regression Models. arXiv:1103.1458, 2011.

Koenker, Roger. Quantile Regression. Cambridge Univer-

sity Press, 2005.

Lecu´e, Guillaume and Mendelson, Shahar. Sparse recov-
ery under weak moment assumptions. arXiv:1401.2188,
2014.

Li, Xingguo, Zhao, Tuo, Yuan, Xiaoming, and Liu, Han.
The ﬂare package for high dimensional linear regression
and precision matrix estimation in r. Journal of Machine
Learning Research, 16(1):553–557, 2015.

Li, Y. J. and Zhu, J. L1-norm Quantile Regression. Journal
of Computational and Graphical Statistics, 17:163–185,
2008.

Melnyk, Igor and Banerjee, Arindam. Estimating Struc-
In International

tured Vector Autoregressive Model.
Conference on Machine Learning (ICML), 2016.

Negahban, Sahand N., Ravikumar, Pradeep, Wainwright,
Martin J., and Yu, Bin. A Uniﬁed Framework for High-
Dimensional Analysis of M -Estimators with Decompos-
able Regularizers. Statistical Science, 27(4):538–557,
2012. ISSN 0883-4237.

Rudelson, Mark and Zhou, Shuheng. Reconstruction from
anisotropic random measurements. IEEE Transactions
on Information Theory, 59(6):3434–3447, jun 2013.

Sivakumar, Vidyashankar, Banerjee, Arindam, and Raviku-
mar, Pradeep. Beyond Sub-Gaussian Measurements:
High-Dimensional Structured Estimation with Sub-
In Advances in Neural Informa-
Exponential Designs.
tion Processing Systems, 2015.

High-Dimensional Structured Quantile Regression

Talagrand, Michel. Upper and Lower Bounds of Stochastic

Processes. Springer, 2014.

Tibshirani, Robert. Regression Shrinkage and Selection via
the Lasso. Journal of the Royal Statistical Society, 58(1):
267–288, 1996.

Tropp, Joel A. Convex recovery of a structured signal from
independent random linear measurements. In Sampling
Theory - a Renaissance. To appear, may 2015.

Vershynin, Roman. Estimation in High Dimensions: A ge-
ometric perspective. In Sampling Theory, a Renaissance,
pp. 3–66. Birkhauser, Basel, 2015.

Wang, Lan, Wu, Yichao, and Li, Runze. Quantile Regres-
sion for Analyzing Heterogeneity in Ultra-high Dimen-
sion. Journal of the American Statistical Association,
107:214–222, 2012.

Wu, Y. C. and Liu, Y. F. Variable Selection in Quantile

Regression. Statistica Sinica, 19:801–817, 2009.

Zou, H. and Yuan, M. Composite Quantile Regression and
the Oracle Model Selection Theory. The Annals of Statis-
tics, 36:1108–1126, 2008.

