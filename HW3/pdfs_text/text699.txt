Multi-Class Optimal Margin Distribution Machine

Teng Zhang 1 Zhi-Hua Zhou 1

Abstract

Recent studies disclose that maximizing the min-
imum margin like support vector machines does
not necessarily lead to better generalization per-
formances, and instead,
it is crucial to opti-
mize the margin distribution. Although it has
been shown that for binary classiﬁcation, char-
acterizing the margin distribution by the ﬁrst-
and second-order statistics can achieve superior
performance.
It still remains open for multi-
class classiﬁcation, and due to the complexity
of margin for multi-class classiﬁcation, optimiz-
ing its distribution by mean and variance can
also be difﬁcult.
In this paper, we propose
mcODM (multi-class Optimal margin Distribu-
tion Machine), which can solve this problem ef-
ﬁciently. We also give a theoretical analysis for
our method, which veriﬁes the signiﬁcance of
margin distribution for multi-class classiﬁcation.
Empirical study further shows that mcODM al-
ways outperforms all four versions of multi-class
SVMs on all experimental data sets.

1. Introduction

Support vector machines (SVMs) and Boosting have
been two mainstream learning approaches during the past
decade. The former (Cortes & Vapnik, 1995) roots in the
statistical learning theory (Vapnik, 1995) with the central
idea of searching a large margin separator, i.e., maximiz-
ing the smallest distance from the instances to the classi-
ﬁcation boundary in a RKHS (reproducing kernel Hilbert
It is noteworthy that there is also a long history
space).
of applying margin theory to explain the latter (Freund &
Schapire, 1995; Schapire et al., 1998), due to its tending to
be empirically resistant to over-ﬁtting (Reyzin & Schapire,
2006; Wang et al., 2011; Zhou, 2012).

1National Key Laboratory for Novel Software Technology,
Nanjing University, Nanjing 210023, China. Correspondence to:
Zhi-Hua Zhou <zhouzh@lamda.nju.edu.cn>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Recently, the margin theory for Boosting has ﬁnally been
defended (Gao & Zhou, 2013), and has disclosed that the
margin distribution rather than a single margin is more cru-
cial to the generalization performance.
It suggests that
there may still exist large space to further enhance for
SVMs. Inspired by this recognition, (Zhang & Zhou, 2014;
2016) proposed a binary classiﬁcation method to optimize
margin distribution by characterizing it through the ﬁrst-
and second-order statistics, which achieves quite satisfac-
tory experimental results. Later, (Zhou & Zhou, 2016) ex-
tends the idea to an approach which is able to exploit un-
labeled data and handle unequal misclassiﬁcation cost. A
brief summary of this line of early research can be found
in (Zhou, 2014).

Although it has been shown that for binary classiﬁcation,
optimizing the margin distribution by maximizing the mar-
gin mean and minimizing the margin variance simultane-
ously can get superior performance, it still remains open for
multi-class classiﬁcation. Moreover, the margin for multi-
class classiﬁcation is much more complicated than that for
binary class classiﬁcation, which makes the resultant opti-
mization be a difﬁcult non-differentiable non-convex pro-
gramming. In this paper, we propose mcODM (multi-class
Optimal margin Distribution Machine) to solve this prob-
lem efﬁciently. For optimization, we relax mcODM into a
series of convex quadratic programming (QP), and extend
the Block Coordinate Descent (BCD) algorithm (Tseng,
2001) to solve the dual of each QP. The sub-problem of
each iteration of BCD is also a QP. By exploiting its special
structure, we derive a sorting algorithm to solve it which is
much faster than general QP solvers. We further provide
a generalization error bound based on Rademacher com-
plexity, and further present the analysis of the relationship
between generalization error and margin distribution for
multi-class classiﬁcation. Extensive experiments on twenty
two data sets show the superiority of our method to all four
versions of multi-class SVMs.

The rest of this paper is organized as follows. Section 2
introduces some preliminaries. Section 3 formulates the
problem. Section 4 presents the proposed algorithm. Sec-
tion 5 discusses some theoretical analyses. Section 6 re-
ports on our experimental studies and empirical observa-
tions. Finally Section 7 concludes with future work.

Multi-Class Optimal Margin Distribution Machine

2. Preliminaries

We denote by X 2 Rd the instance space and Y = [k]
the label set, where [k] = f1; : : : ; kg. Let D be an un-
known (underlying) distribution over X (cid:2) Y. A training
set S = f(x1; y1); (x2; y2); : : : ; (xm; ym)g 2 (X (cid:2) Y)m
is drawn identically and independently (i.i.d.) according to
distribution D. Let ϕ : X 7! H be a feature mapping as-
sociated to some positive deﬁnite kernel (cid:20). For multi-class
classiﬁcation setting, the hypothesis is deﬁned based on k
weight vectors w1; : : : ; wk 2 H, where each weight vector
wl; l 2 Y deﬁnes a scoring function x 7! w⊤
l ϕ(x) and the
label of instance x is the one resulting in the largest score,
i.e., h(x) = argmaxl2Y w⊤
l ϕ(x). This decision function
naturally leads to the following deﬁnition of the margin for
a labeled instance (x; y):
(cid:13)h(x; y) = w⊤

w⊤

l ϕ(x):

y ϕ(x) (cid:0) max
l̸=y

Thus h misclassiﬁes (x; y) if and only if it produces a neg-
ative margin for this instance.
Given a hypothesis set H of functions mapping X to Y
and the labeled training set S, our goal is to learn a func-
tion h 2 H such that the generalization error R(h) =
E(x;y)(cid:24)D[1h(x)̸=y] is small.

3. Formulation

To design optimal margin distribution machine for multi-
class classiﬁcation, we need to understand how to optimize
the margin distribution. (Gao & Zhou, 2013) proved that, to
characterize the margin distribution, it is important to con-
sider not only the margin mean but also the margin vari-
ance. Inspired by this idea, (Zhang & Zhou, 2014; 2016)
proposed the optimal margin distribution machine for bi-
nary classiﬁcation, which characterizes the margin distri-
bution according to the ﬁrst- and second-order statistics,
that is, maximizing the margin mean and minimizing the
margin variance simultaneously. Speciﬁcally, let (cid:22)(cid:13) denote
the margin mean, and the optimal margin distribution ma-
chine can be formulated as:

min
w;(cid:22)(cid:13);(cid:24)i;ϵi

Ω(w) (cid:0) (cid:17)(cid:22)(cid:13) +

((cid:24)2

i + ϵ2
i )

m∑

(cid:21)
m

i=1
s.t. (cid:13)h(xi; yi) (cid:21) (cid:22)(cid:13) (cid:0) (cid:24)i;

(cid:13)h(xi; yi) (cid:20) (cid:22)(cid:13) + ϵi; 8i;

where Ω(w) is the regularization term to penalize the
model complexity, (cid:17) and (cid:21) are trading-off parameters, (cid:24)i
and ϵi are the deviation of the margin (cid:13)h(xi; yi) to the mar-
m
i=1((cid:24)2
i )=m is exactly the
gin mean. It’s evident that
margin variance.

i + ϵ2

∑

By scaling w which doesn’t affect the ﬁnal classiﬁcation
results, the margin mean can be ﬁxed as 1, then the de-

viation of the margin of (xi; yi) to the margin mean is
j(cid:13)h(xi; yi) (cid:0) 1j, and the optimal margin distribution ma-
chine can be reformulated as

m∑

min
w;(cid:24)i;ϵi

Ω(w) +

(cid:24)2
i + (cid:22)ϵ2
(cid:21)
i
(1 (cid:0) (cid:18))2
m
s.t. (cid:13)h(xi; yi) (cid:21) 1 (cid:0) (cid:18) (cid:0) (cid:24)i;

i=1

(cid:13)h(xi; yi) (cid:20) 1 + (cid:18) + ϵi; 8i:

where (cid:22) 2 (0; 1] is a parameter to trade off two differ-
ent kinds of deviation (larger or less than margin mean).
(cid:18) 2 [0; 1) is a parameter of the zero loss band, which can
control the number of support vectors, i.e., the sparsity of
the solution, and (1 (cid:0) (cid:18))2 in the denominator is to scale the
second term to be a surrogate loss for 0-1 loss.

For multi-class classiﬁcation, let the regularization term
H=2 and combine with the deﬁnition
Ω(w) =
of margin, and we arrive at the formulation of mcODM,

∑
k
l=1

∥wl∥2

min
wl;(cid:24)i;ϵi

k∑

1
2
s.t. w⊤
yi
w⊤
yi

∥wl∥2

H +

l=1
ϕ(xi) (cid:0) max
l̸=yi
ϕ(xi) (cid:0) max
l̸=yi

m∑

(cid:24)2
i + (cid:22)ϵ2
i
(1 (cid:0) (cid:18))2

(cid:21)
m
i=1
w⊤
l ϕ(xi) (cid:21) 1 (cid:0) (cid:18) (cid:0) (cid:24)i;

(1)

w⊤

l ϕ(xi) (cid:20) 1 + (cid:18) + ϵi; 8i:

where (cid:21), (cid:22) and (cid:18) are the parameters for trading-off de-
scribed previously.

4. Optimization

Due to the max operator in the second constraint, mcODM
is a non-differentiable non-convex programming, which is
quite difﬁcult to solve directly.

In this section, we ﬁrst relax mcODM into a series of con-
vex quadratic programming (QP), which can be much eas-
ier to handle. Speciﬁcally, at each iteration, we recast the
ﬁrst constraint as k (cid:0) 1 linear inequality constraints:

w⊤
yi

ϕ(xi) (cid:0) w⊤

l ϕ(xi) (cid:21) 1 (cid:0) (cid:18) (cid:0) (cid:24)i; l ̸= yi;

and replace the second constraint with

w⊤
yi

ϕ(xi) (cid:0) Mi (cid:20) 1 + (cid:18) + ϵi;

where Mi = maxl̸=yi (cid:22)w⊤
l ϕ(xi) and (cid:22)wl is the solution to
the previous iteration. Then we can repeatedly solve the
following convex QP problem until convergence:

min
wl;(cid:24)i;ϵi

1
2
s.t. w⊤
yi
w⊤
yi

k∑

∥wl∥2

H +

m∑

(cid:21)
m

(cid:24)2
i + (cid:22)ϵ2
i
(1 (cid:0) (cid:18))2

i=1

l=1
ϕ(xi) (cid:0) w⊤
ϕ(xi) (cid:0) Mi (cid:20) 1 + (cid:18) + ϵi; 8i:

l ϕ(xi) (cid:21) 1 (cid:0) (cid:18) (cid:0) (cid:24)i; 8l ̸= yi;

(2)

Multi-Class Optimal Margin Distribution Machine

(cid:21) 0; l ̸= yi for the
Introduce the lagrange multipliers (cid:16) l
i
ﬁrst k (cid:0) 1 constraints and (cid:12)i (cid:21) 0 for the last constraint
respectively, the Lagrangian function of Eq. 2 leads to

L(wl; (cid:24)i; ϵi; (cid:16) l

i; (cid:12)i)

k∑

=

1
2

∥wl∥2

H +

(cid:21)
m

m∑

i=1

(cid:24)2
i + (cid:22)ϵ2
i
(1 (cid:0) (cid:18))2

l=1
m∑

∑

l̸=yi

i=1
m∑

i=1

(cid:0)

+

i(w⊤
(cid:16) l
yi

ϕ(xi) (cid:0) w⊤

l ϕ(xi) (cid:0) 1 + (cid:18) + (cid:24)i)

(cid:12)i(w⊤
yi

ϕ(xi) (cid:0) Mi (cid:0) 1 (cid:0) (cid:18) (cid:0) ϵi);

By setting the partial derivations of variables fwl; (cid:24)i; ϵig to
zero, we have
0

1

m∑

∑

wl =

@(cid:14)yi;l

(cid:16) s
i

(cid:0) (1 (cid:0) (cid:14)yi;l)(cid:16) l
i

(cid:0) (cid:14)yi;l(cid:12)i

A ϕ(xi);

i ; : : : ; (cid:11)k

Speciﬁcally, we sequentially select a group of k + 1 vari-
ables (cid:11)1
i ; (cid:12)i associated with instance xi to mini-
mize, while keeping other variables as constants, and repeat
this procedure until convergence.

Algorithm 1 below details the kenrel mcODM.

1; : : : ; (cid:11)k

1; : : : ; (cid:11)1

m; : : : ; (cid:11)k

m] and

Algorithm 1 Kenrel mcODM
1: Input: Data set S.
2: Initialize (cid:11)⊤ = [(cid:11)1

(cid:12)⊤ = [(cid:12)1; : : : ; (cid:12)m] as zero vector.

for i = 1; : : : ; m do
∑
Mi   maxl̸=yi

3: while (cid:11) and (cid:12) not converge do
4:
5:
6:
7:
8: end while
9: Output: (cid:11), (cid:12).

m
j=1((cid:11)l
j

end for
Solve Eq. 5 by block coordinate descent method.

(cid:0) (cid:14)yj ;l(cid:12)j)(cid:20)(xj; xi).

i=1
m(1 (cid:0) (cid:18))2
2(cid:21)

s̸=yi
∑

l̸=yi

(cid:24)i =

(cid:16) l
i;

ϵi =

m(1 (cid:0) (cid:18))2
2(cid:21)(cid:22)

(cid:12)i:

(3)

4.1. Solving the sub-problem

where (cid:14)yi;l equals 1 when yi = l and 0 otherwise. We
further simplify the expression of wl as

wl =

((cid:11)l
i

(cid:0) (cid:14)yi;l(cid:12)i)ϕ(xi);

(4)

m∑

i=1

(cid:17) (cid:0)(cid:16) l

i for 8l ̸= yi and (cid:11)yi
by deﬁning (cid:11)l
(cid:16) s
i and
i
substituting Eq. 4 and Eq. 3 into the Lagrangian function,
then we have the following dual problem

s̸=yi

(cid:17)

i

∑

min
i;(cid:11)yi
(cid:11)l

i ;(cid:12)i

k∑

1
2

∥wl∥2

H +

l=1
m(1 (cid:0) (cid:18))2
4(cid:21)(cid:22)

+

m∑

m(1 (cid:0) (cid:18))2
4(cid:21)

m∑

i=1

((cid:11)yi

i )2

i + (1 (cid:0) (cid:18))
(cid:12)2

m∑

∑

i=1

l̸=yi

(cid:11)l
i

+ (Mi + 1 + (cid:18))

(cid:12)i

(5)

i=1

m∑

i=1

k∑

s.t.

i = 0; 8i;
(cid:11)l

(cid:20) 0; 8i; 8l ̸= yi;

l=1
(cid:11)l
i
(cid:12)i (cid:21) 0; 8i:

The sub-problem in step 7 of Algorithm 1 is also a con-
vex QP with k + 1 variables, which can be accomplished
by some standard QP solvers. However, by exploiting its
special structure, i.e., only a small quantity of cross terms
are involved, we can derive an algorithm to solve this sub-
problem just by sorting, which can be much faster than gen-
eral QP solvers.

Note that all variables except (cid:11)1
we have the following sub-problem:

i ; : : : ; (cid:11)k

i ; (cid:12)i are ﬁxed, so

min
i;(cid:11)yi
(cid:11)l

i ;(cid:12)i

∑

l̸=yi

A
2

∑

l̸=yi

D
2

((cid:11)l

i)2 +

Bl(cid:11)l

i +

((cid:11)yi

i )2 (cid:0) A(cid:11)yi

i (cid:12)i

E
2

(cid:12)2
i + F (cid:12)i

i +

+ Byi(cid:11)yi
k∑

(cid:11)l

i = 0;

s.t.

(cid:20) 0; 8l ̸= yi;

l=1
(cid:11)l
i
(cid:12)i (cid:21) 0:

∑
∑

where A = (cid:20)(xi; xi), Bl =
(cid:14)yj ;l(cid:12)j) + 1 (cid:0) (cid:18) for 8l ̸= yi, Byi =
(cid:14)yj ;yi(cid:12)j), D = A + m(1(cid:0)(cid:18))2
F (cid:17) Mi + 1 + (cid:18) (cid:0) Byi.
The KKT conditions of Eq. 6 indicate that there are scalars
(cid:23), (cid:26)l and (cid:17) such that

j̸=i (cid:20)(xi; xj)((cid:11)l
j
j̸=i (cid:20)(xi; xj)((cid:11)yi
j
and

, E = A + m(1(cid:0)(cid:18))2

2(cid:21)(cid:22)

2(cid:21)

(cid:0)
(cid:0)

(6)

(7)

(8)

The objective function in Eq. 5 involves m(k + 1) vari-
ables in total, so it is not easy to optimize with respect
to all the variables simultaneously. Note that all the con-
straints can be partitioned into m disjoint sets, and the i-th
set only involves (cid:11)1
i ; (cid:12)i, so the variables can be
divided into m decoupled groups and an efﬁcient block co-
ordinate descent algorithm (Tseng, 2001) can be applied.

i ; : : : ; (cid:11)k

k∑

l=1
(cid:11)l
i

(cid:11)l

i = 0;

(cid:20) 0; 8l ̸= yi;

Multi-Class Optimal Margin Distribution Machine

(9)

(10)

(11)
(12)

(13)

(14)

(15)

(16)

(17)

(18)

i = 0; (cid:26)l (cid:21) 0; 8l ̸= yi;
i + Bl (cid:0) (cid:23) + (cid:26)l = 0; 8l ̸= yi;

(cid:12)i (cid:21) 0;
(cid:26)l(cid:11)l
A(cid:11)l
(cid:17)(cid:12)i = 0; (cid:17) (cid:21) 0;
(cid:0) A(cid:11)yi
D(cid:11)yi
i

i + E(cid:12)i + F (cid:0) (cid:17) = 0;
(cid:0) (cid:23) = 0:
(cid:0) A(cid:12)i + Byi

According to Eq. 8, Eq. 10 and Eq. 11 are equivalent to

A(cid:11)l

i + Bl (cid:0) (cid:23) = 0;
Bl (cid:0) (cid:23) (cid:20) 0;

if (cid:11)l
if (cid:11)l

i < 0; 8l ̸= yi;
i = 0; 8l ̸= yi:

In the same way, Eq. 12 and Eq. 13 are equivalent to

(cid:0)A(cid:11)yi

i + E(cid:12)i + F = 0;
i + F (cid:21) 0;

(cid:0)A(cid:11)yi

if (cid:12)i > 0;
if (cid:12)i = 0:

Thus KKT conditions turn to Eq. 7 - Eq. 9 and Eq. 14 -
Eq. 18. Note that

(cid:11)l
i

(cid:17) min

0;

(

)

(cid:23) (cid:0) Bl
A

; 8l ̸= yi;

(19)

satisﬁes KKT conditions Eq. 8 and Eq. 15 - Eq. 16 and
(

)

(cid:12)i (cid:17) max

0;

(cid:0) F

A(cid:11)yi
i
E

;

(20)

satisﬁes KKT conditions Eq. 9 and Eq. 17 - Eq. 18. By
substituting Eq. 20 into Eq. 14, we obtain

D(cid:11)yi

i + Byi

(cid:0) (cid:23) = max

0;

(A(cid:11)yi
i

(cid:0) F )

:

(21)

(

A
E

)

(cid:20) F , according to Eq. 20 and Eq. 21, we
(cid:20) F ,

Let’s consider the following two cases in turn.
Case 1: A(cid:11)yi
i
D . Thus, A (cid:23)(cid:0)Byi
i = (cid:23)(cid:0)Byi
have (cid:12)i = 0 and (cid:11)yi
which implies that (cid:23) (cid:20) Byi + DF
A .
Case 2: A(cid:11)yi
have (cid:12)i = A(cid:11)yi
A E(cid:23)(cid:0)AF (cid:0)EByi
DE(cid:0)A2

i > F , according to Eq. 20 and Eq. 21, we
. Thus,
> F , which implies that (cid:23) > Byi + DF
A .
The remaining task is to ﬁnd (cid:23) such that Eq. 7 holds. With
Eq. 7 and Eq. 19, it can be shown that

i = E(cid:23)(cid:0)AF (cid:0)EByi

> 0 and (cid:11)yi

DE(cid:0)A2

(cid:0)F

i
E

D

(cid:23) =

∑

AByi
D +
D + jflj(cid:11)l
A
∑

i<0 Bl
l:(cid:11)l
i < 0gj

(cid:23) =

AEByi +A2F

DE(cid:0)A2 +
DE(cid:0)A2 + jflj(cid:11)l

AE

i<0 Bl

l:(cid:11)l
i < 0gj

; Case 1;

(22)

; Case 2:

(23)

In both cases,
∑

i<0 Bl)=(Q + jflj(cid:11)l

l:(cid:11)l

the optimal (cid:23) takes the form of (P +
i < 0gj), where P and Q are some

constants. (Fan et al., 2008) showed that it can be found
by sorting fBlg for 8l ̸= yi in decreasing order and then
sequentially adding them into an empty set (cid:8), until
∑

(cid:23)(cid:3) =

P +

l2(cid:8) Bl

Q + j(cid:8)j

(cid:21) max
l̸2(cid:8)

Bl:

(24)

Note that the Hessian matrix of the objective function of
Eq. 6 is positive deﬁnite, which guarantees the existence
and uniqueness of the optimal solution, so only one of
Eq. 22 and Eq. 23 can hold. We can ﬁrst compute (cid:23)(cid:3) ac-
cording to Eq. 24 for Case 1, and then check whether the
constraint of (cid:23) is satisﬁed. If not, we further compute (cid:23)(cid:3)
for Case 2. Algorithm 2 summarizes the pseudo-code for
solving the sub-problem.

Algorithm 2 Solving the sub-problem
1: Input: Parameters A, B = fB1; : : : ; Bkg, D; E; F .
2: Initialize ^B   B, then swap ^B1 and ^Byi, and sort

^Bnf ^B1g in decreasing order.

(cid:23)   (cid:23) + ^Bi.
i   i + 1.

3: i   2, (cid:23)   AByi=D.
4: while i (cid:20) k and (cid:23)=(i (cid:0) 2 + A=D) < ^Bi do
5:
6:
7: end while
8: if (cid:23) (cid:20) Byi + DF=A then
9:
10:
11:
12: else
13:
14:

  min(0; ((cid:23) (cid:0) Bl)=A); l ̸= yi.
  ((cid:23) (cid:0) Byi)=D.

(cid:11)l
i
(cid:11)yi
i
(cid:12)i   0.

i   2, (cid:23)   (AE ^B1 + A2F )=(DE (cid:0) A2).
while i (cid:20) k and (cid:23)=(i (cid:0) 2 + AE=(DE (cid:0) A2)) < ^Bi
do

(cid:23)   (cid:23) + ^Bi.
i   i + 1.

15:
16:
17:
18:
19:
20:
21: end if
22: Output: (cid:11)1

end while
(cid:11)l
i
(cid:11)yi
i
(cid:12)i   (A(cid:11)yi
i

(cid:0) F )=E.

i ; : : : ; (cid:11)k

i ; (cid:12)i.

  min(0; ((cid:23) (cid:0) Bl)=A); l ̸= yi.
  (E(cid:23) (cid:0) AF (cid:0) EByi)=(DE (cid:0) A2).

4.2. Speedup for linear kernel

In section 4.1, the proposed method can efﬁciently deal
with kernel mcODM. However, the computation of Mi in
step 5 of Algorithm 1 and the computation of parameters
(cid:22)Bl in Algorithm 2 both involve the kernel matrix, whose
inherent computational cost takes O(m2) time, so it might
be computational prohibitive for large scale problems.

When linear kernel is used, these problems can be allevi-
ated. According to Eq. 4, w is spanned by the training
instance so it lies in a ﬁnite dimensional space under this

Multi-Class Optimal Margin Distribution Machine

circumstance. By storing w1; : : : ; wk explicitly, the com-
putational cost of Mi = maxl̸=yi w⊤
l xi can be much less.
∑
j̸=i x⊤
Moreover, note that (cid:22)Bl =
(cid:0) (cid:14)yj ;l(cid:12)j) =
i xj((cid:11)l
∑
j
(cid:22)(cid:12)i) = w⊤
l xi(cid:0)
(cid:0)(cid:14)yi;l
i xi((cid:22)(cid:11)l
i xj((cid:22)(cid:11)l
i
j

(cid:22)(cid:12)j)(cid:0)x⊤

j=1 x⊤

(cid:0)(cid:14)yj ;l

m

A((cid:11)l
i

(cid:0) (cid:14)yi;l(cid:12)i), so (cid:22)Bl can also be computed efﬁciently.

5. Analysis

In this section, we study the statistical property of mcODM.
To present the generalization bound of mcODM, we need
to introduce the following loss function (cid:8),

(cid:8)(z) = 1z(cid:20)0 +

10<z(cid:20)1(cid:0)(cid:18);

(z (cid:0) 1 + (cid:18))2
(1 (cid:0) (cid:18))2

(cid:13)h;(cid:18)(x; y) = w⊤

y ϕ(x) (cid:0) max
l2Y

fw⊤

l ϕ(x) (cid:0) (1 (cid:0) (cid:18))1l=yg;

where 1((cid:1)) is the indicator function that returns 1 when
As can be
the argument holds, and 0 otherwise.
seen, (cid:13)h;(cid:18)(x; y) is a lower bound of (cid:13)h(x; y) and
(cid:8)((cid:13)h;(cid:18)(x; y)) = (cid:8)((cid:13)h(x; y)).
7!
Theorem 1. Let H = f(x; y) 2 X (cid:2) [k]
w⊤
H (cid:20) (cid:3)2g be the hypothesis space
of mcODM, where ϕ : X 7! H is a feature mapping
induced by some positive deﬁnite kernel (cid:20). Assume that
S (cid:18) fx : (cid:20)(x; x) (cid:20) r2g, then for any (cid:14) > 0, with prob-
ability at least 1 (cid:0) (cid:14), the following generalization bound
holds for any h 2 H,

∑
k
l=1

y ϕ(x)j

∥wl∥2

R(h) (cid:20) 1
m

m∑

i=1

(cid:8)((cid:13)h(xi; yi)) +

√

√

16r(cid:3)
1 (cid:0) (cid:18)

2(cid:25)k
m

+ 3

ln 2
(cid:14)
2m

:

Proof. Let ~H(cid:18) be the family of hypotheses mapping X (cid:2)
Y 7! R deﬁned by ~H(cid:18) = f(x; y) 7! (cid:13)h;(cid:18)(x; y) : h 2 Hg,
with McDiarmid inequality (McDiarmid, 1989), yields the
following inequality with probability at least 1 (cid:0) (cid:14),

E[(cid:8)((cid:13)h;(cid:18)(x; y))] (cid:20) 1
m

m∑

i=1

(cid:8)((cid:13)h;(cid:18)(xi; yi))

+ 2RS((cid:8) ◦ ~H(cid:18)) + 3

; 8h 2 ~H(cid:18):

√

ln 2
(cid:14)
2m

Note that (cid:8)((cid:13)h;(cid:18)) = (cid:8)((cid:13)h), R(h) = E[1(cid:13)h(x;y)(cid:20)0] (cid:20)
E[1(cid:13)h;(cid:18)(x;y)(cid:20)0] (cid:20) E[(cid:8)((cid:13)h;(cid:18)(x; y))] and (cid:8)(z) is
2
1(cid:0)(cid:18) -
Lipschitz function, by using Talagrand’s lemma (Mohri
et al., 2012), we have

R(h) (cid:20) 1
m

m∑

i=1

(cid:8)((cid:13)h(xi; yi)) +

√

4RS( ~H(cid:18))
1 (cid:0) (cid:18)

+ 3

ln 2
(cid:14)
2m

:

According to Theorem 7 of (Lei et al., 2015), we have
RS( ~H(cid:18)) (cid:20) 4r(cid:3)

2(cid:25)k=m and proves the stated result.

√

Theorem 1 shows that we can get a tighter generalization
bound for smaller r(cid:3) and smaller (cid:18). Since (cid:13) (cid:20) 2r(cid:3), so
the former can be viewed as an upper bound of the margin.
Besides, 1 (cid:0) (cid:18) is the lower bound of the zero loss band of
mcODM. This veriﬁes that better margin distribution (i.e.,
larger margin mean and smaller margin variance) can yield
better generalization performance, which is also consistent
with the work of (Gao & Zhou, 2013).

6. Empirical Study

In this section, we empirically evaluate the effectiveness
of our method on a broad range of data sets. We ﬁrst
introduce the experimental settings and compared meth-
ods in Section 6.1, and then in Section 6.2, we compare
our method with four versions of multi-class SVMs, i.e.,
mcSVM (Weston & Watkins, 1999; Crammer & Singer,
2001; 2002), one-versus-all SVM (ovaSVM), one-versus-
one SVM (ovoSVM) (Ulrich, 1998) and error-correcting
output code SVM (ecocSVM) (Dietterich & Bakiri, 1995).
In addition, we also study the inﬂuence of the number of
classes on generalization performance and margin distri-
bution in Section 6.3. Finally, the computational cost is
presented in Section 6.4.

6.1. Experimental Setup

We evaluate the effectiveness of our proposed methods on
twenty two data sets. Table 1 summarizes the statistics of
these data sets. The data set size ranges from 150 to more
than 581,012, and the dimensionality ranges from 4 to more
than 62,061. Moreover, the number of class ranges from 3
to 1,000, so these data sets cover a broad range of prop-
erties. All features are normalized into the interval [0; 1].
For each data set, eighty percent of the instances are ran-
domly selected as training data, and the rest are used as
testing data. For each data set, experiments are repeated
for 10 times with random data partitions, and the average
accuracies as well as the standard deviations are recorded.

mcODM is compared with four versions of multi-class
SVMs, i.e., ovaSVM, ovoSVM, ecocSVM and mcSVM.
These four methods can be roughly classiﬁed into two
groups. The ﬁrst group includes the ﬁrst three meth-
ods by converting the multi-class classiﬁcation problem
into a set of binary classiﬁcation problems. Specially,
ovaSVM consists of learning k scoring functions hl : X 7!
f(cid:0)1; +1g; l 2 Y, each seeking to discriminate one class
l 2 Y from all the others, as can be seen it need train k
SVM models. Alternatively, ovoSVM determines the scor-
ing functions for all the combinations of class pairs, so it
need train k(k (cid:0) 1)=2 SVM models. Finally, ecocSVM
is a generalization of the former two methods. This tech-
nique assigns to each class l 2 Y a code word with length
c, which serves as a signature for this class. After training

Multi-Class Optimal Margin Distribution Machine

Table1. Characteristics of experimental data sets.

ID Dataset

#Instance

#Feature

#Class

ID Dataset

#Instance

#Feature

#Class

1
2
3
4
5
6
7
8
9
10
11

iris
wine
glass
svmguide2
svmguide4
vehicle
vowel
segment
dna
satimage
usps

150
178
214
391
612
846
990
2,310
3,186
6,435
9,298

4
13
9
20
10
18
10
19
180
36
256

3
3
6
3
6
4
11
7
3
6
10

sector
pendigits
news20
letter
protein
shuttle
connect-4

12
13
14
15
16
17
18
19 mnist
20
21
22

aloi
rcv1
covtype

9,619
10,992
19,928
20,000
24,387
58,000
67,557
70,000
108,000
534,135
581,012

55,197
16
62,061
16
357
9
126
780
128
47,236
54

105
10
20
26
3
7
3
10
1,000
53
7

c binary SVM models h1((cid:1)); : : : ; hc((cid:1)), the class predicted
for each testing instance is the one whose signatures is the
closest to [h1(x); : : : ; hc(x)] in Hamming distance. The
weakness of these methods is that they may produce un-
classiﬁable regions and their computational costs are usu-
ally quite large in practice, which can be observed in the
following experiments. On the other hand, mcSVM be-
It directly determines all the
longs to the second group.
scroing functions at once, so the time cost is usually less
than the former methods. In addition, the unclassiﬁable re-
gions are also resolved.

For all the methods, the regularization parameter (cid:21) for
mcODM or C for binary SVM and mcSVM is selected
by 5-fold cross validation from [20; 22; : : : ; 220].
For
mcODM, the regularization parameters (cid:22) and (cid:18) are both se-
lected from [0:2; 0:4; 0:6; 0:8]. For ecocSVM, the exhaus-
tive codes strategy is employed, i.e., for each class, we con-
struct a code of length 2k(cid:0)1 (cid:0) 1 as the signature. All the
selections for parameters are performed on training sets.

6.2. Results

Table 2 summarizes the detailed results on twenty two
data sets. As can be seen, the overall performance of
our method is superior or highly competitive to the other
compared methods. Speciﬁcally, mcODM performs signif-
icantly better than mcSVM/ovaSVM/ovoSVM/ecocSVM
on 17/19/18/17 over 22 data sets respectively, and achieves
the best accuracy on 20 data sets. In addition, as can be
seen, in comparing with other four methods which don’t
consider margin distribution, the win/tie/loss counts show
that mcODM is always better or comparable, almost never
worse than it.

6.3. Inﬂuence of the Number of Classes

In this section we study the inﬂuence of the number of
classes on generalization performance and margin distri-
bution, respectively.

6.3.1. GENERALIZATION PERFORMANCE

Figure 1 plots the generalization performance of all the ﬁve
methods on data set segment, and similar observation can
be found for other data sets. As can be seen, when the num-
ber of classes is less than four, all methods perform quite
well. However, as the ﬁfth class is added, the generaliza-
tion performance of other four methods decreases drasti-
cally. This might be attributable to the introduction of some
noisy data, which SVM-style algorithms are very sensitive
to since they optimize the minimum margin. On the other
hand, our method considers the whole margin distribution,
so it can be robust to noise and relatively more stable.

Figure1. Generalization performance of all the ﬁve methods on
data set segment with the increase of the number of classes.

6.3.2. MARGIN DISTRIBUTION

Figure 2 plots the frequency histogram of margin distribu-
tion produced by mcSVM, ovaSVM and mcODM on data
set segment as the number of classes increases from two
to seven. As can be seen, when the number of classes is
less than four, all methods can achieve good margin distri-
bution, whereas with the increase of the number of classes,
the other two methods begin to produce negative margins.
At the same time, the distribution of our method becomes

2345670.880.900.920.940.960.981.00generalization performance#Class mcSVM ovaSVM ovoSVM ecocSVM mcODMMulti-Class Optimal Margin Distribution Machine

Table2. Accuracy (mean(cid:6)std.) comparison on twenty two data sets. Linear kernel is used. The best accuracy on each data set is bolded.
(cid:15)/◦ indicates the performance of mcODM is signiﬁcantly better/worse than the compared methods (paired t-tests at 95% signiﬁcance
level). The average rank and top1 times are listed in the third and second row from the bottom. The win/tie/loss counts for mcODM are
summarized in the last row. ovoSVM and ecocSVM did not return results on some data sets in 48 hours.

Dataset

iris
wine
glass
svmguide2
svmguide4
vehicle
vowel
segment
dna
satimage
usps
sector
pendigits
news20
letter
protein
shuttle
connect-4
mnist
aloi
rcv1
covtype

mcSVM
84.7(cid:6)5.0(cid:15)
96.5(cid:6)2.6(cid:15)
62.0(cid:6)7.6(cid:15)
80.9(cid:6)2.6(cid:15)
86.6(cid:6)3.2(cid:15)
80.5(cid:6)2.1(cid:15)
65.4(cid:6)2.1
95.3(cid:6)0.9(cid:15)
95.1(cid:6)0.6(cid:15)
81.8(cid:6)0.5(cid:15)
95.1(cid:6)0.3
93.3(cid:6)0.4(cid:15)
94.8(cid:6)0.3(cid:15)
83.3(cid:6)0.3(cid:15)
76.9(cid:6)0.4(cid:15)
68.7(cid:6)0.3
97.2(cid:6)0.1(cid:15)
75.7(cid:6)0.1(cid:15)
92.5(cid:6)0.2(cid:15)
90.1(cid:6)0.2(cid:15)
92.8(cid:6)0.1
71.7(cid:6)0.1

Avg. Rank

2.45

mcODM: w/t/l

17/5/0

ovaSVM
82.0(cid:6)5.0(cid:15)
97.0(cid:6)2.4
57.8(cid:6)2.1(cid:15)
81.5(cid:6)2.3(cid:15)
77.2(cid:6)3.1(cid:15)
78.5(cid:6)2.9(cid:15)
44.1(cid:6)2.6(cid:15)
92.3(cid:6)0.7(cid:15)
95.1(cid:6)0.7(cid:15)
80.1(cid:6)0.7(cid:15)
94.2(cid:6)0.3(cid:15)
93.1(cid:6)0.5(cid:15)
91.8(cid:6)0.5(cid:15)
83.0(cid:6)0.3(cid:15)
64.9(cid:6)1.0(cid:15)
68.7(cid:6)0.4
89.9(cid:6)2.8(cid:15)
75.7(cid:6)0.1(cid:15)
91.5(cid:6)0.2(cid:15)
82.2(cid:6)0.2(cid:15)
92.7(cid:6)0.1
71.2(cid:6)0.1(cid:15)

3.18

19/3/0

ovoSVM
73.0(cid:6)7.1(cid:15)
96.8(cid:6)3.1
62.4(cid:6)6.0(cid:15)
74.6(cid:6)5.4(cid:15)
77.1(cid:6)6.9(cid:15)
73.5(cid:6)3.5(cid:15)
71.1(cid:6)4.9
93.2(cid:6)2.2(cid:15)
93.0(cid:6)1.0(cid:15)
77.4(cid:6)4.2(cid:15)
94.5(cid:6)0.6(cid:15)
89.9(cid:6)0.6(cid:15)
95.7(cid:6)0.8(cid:15)
69.4(cid:6)0.4(cid:15)
75.7(cid:6)1.2(cid:15)
68.1(cid:6)0.3(cid:15)
97.4(cid:6)0.1(cid:15)
75.7(cid:6)0.2(cid:15)
90.9(cid:6)0.5(cid:15)
N/A
92.1(cid:6)0.1(cid:15)
72.7(cid:6)0.1

3.45

18/3/0

ecocSVM
73.3(cid:6)4.2(cid:15)
91.6(cid:6)2.7(cid:15)
60.4(cid:6)4.0(cid:15)
79.0(cid:6)3.6(cid:15)
74.2(cid:6)4.3(cid:15)
76.9(cid:6)2.5(cid:15)
43.1(cid:6)2.0(cid:15)
90.7(cid:6)0.8(cid:15)
90.9(cid:6)1.1(cid:15)
76.0(cid:6)1.4(cid:15)
92.3(cid:6)0.7(cid:15)
N/A
87.6(cid:6)0.7(cid:15)
N/A
N/A
66.5(cid:6)0.5(cid:15)
84.2(cid:6)0.0(cid:15)
75.1(cid:6)0.2(cid:15)
87.6(cid:6)0.2(cid:15)
N/A
N/A
70.6(cid:6)0.1(cid:15)

4.82

17/0/0

mcODM

87.3(cid:6)3.4
98.4(cid:6)1.9
68.2(cid:6)7.3
84.1(cid:6)1.8
87.8(cid:6)3.5
85.6(cid:6)6.6
65.6(cid:6)3.9
96.7(cid:6)1.3
95.6(cid:6)0.7
89.1(cid:6)8.0
95.1(cid:6)0.6
93.6(cid:6)0.6
96.7(cid:6)0.9
83.6(cid:6)0.3
82.7(cid:6)1.6
70.2(cid:6)2.3
99.6(cid:6)0.0
94.4(cid:6)7.6
95.2(cid:6)0.3
91.4(cid:6)0.3
93.0(cid:6)1.1
72.8(cid:6)1.6

1.09

“sharper”, which prevents the instance with small margin,
so our method can still perform relatively well as the num-
ber of classes increases, which is also consistent with the
observation from Figure 1.

6.4. Time Cost

We compare the single iteration time cost of our method
with mcSVM, ovaSVM, ovoSVM on all the data sets ex-
cept aloi, on which ovoSVM could not return results in 48
hours. All the experiments are performed with MATLAB
2012b on a machine with 8(cid:2)2.60 GHz CPUs and 32GB
main memory. The average CPU time (in seconds) on each
data set is shown in Figure 3. The binary SVM used in
ovaSVM, ovoSVM and mcSVM are both implemented by
the LIBLINEAR (Fan et al., 2008) package. It can be seen
that for small data sets, the efﬁciency of all the methods are
similar, however, for data sets with more than ten classes,
e.g., sector and rcv1, mcSVM and mcODM, which learn
all the scroing functions at once, are much faster than
ovaSVM and ovoSVM, owing to the inefﬁciency of binary-
decomposition as discussed in Section 6.1. Note that LIB-
LINEAR are very fast implementations of binary SVM and
mcSVM, and this shows that our method is also computa-
tionally efﬁcient.

7. Conclusions

Recent studies disclose that for binary class classiﬁcation,
maximizing the minimum margin does not necessarily lead
to better generalization performances, and instead, it is
crucial to optimize the margin distribution. However, it
remains open to the inﬂuence of margin distribution for
multi-class classiﬁcation. We try to answer this question
in this paper. After maximizing the margin mean and min-
imizing the margin variance simultaneously, the resultant
optimization is a difﬁcult non-differentiable non-convex
programming. We propose mcODM to solve this problem
efﬁciently. Extensive experiments on twenty two data sets
validate the superiority of our method to four versions of
multi-class SVMs. In the future it will be interesting to ex-
tend mcODM to more general learning settings, i.e., multi-
label learning and structured learning.

Acknowledgements

This research was supported by the NSFC (61333014)
and the Collaborative Innovation Center of Novel Software
Technology and Industrialization. Authors want to thank
reviewers for helpful comments, and thank Dr. Wei Gao
for reading a draft.

Multi-Class Optimal Margin Distribution Machine

Figure2. Frequency histogram of the margin distribution produced by mcSVM, ovaSVM and mcODM on data set segment as the
number of classes increase from two to seven.

Figure3. Single iteration time cost of mcSVM, ovaSVM, ovoSVM and mcODM on all the data sets except aloi.

0.70.80.91.01.11.21.31.41.501020304050Frequencymargin mcSVM ovaSVM mcODM2 class0.020.040.060.080.100.120.1401020304050Frequencymargin mcSVM ovaSVM mcODM3 class-0.020.000.020.040.060.080.100.12050100150200Frequencymargin mcSVM ovaSVM mcODM4 class-0.020.000.020.040.060.080.100.120100200300Frequencymargin mcSVM ovaSVM mcODM5 class-0.04-0.020.000.020.040.060.080.100.120100200300Frequencymargin mcSVM ovaSVM mcODM6 class-0.020.000.020.040.060.080.100.120100200300Frequencymargin mcSVM ovaSVM mcODM7 classiriswineglasssvmguide2svmguide4vehiclevowelsegmentdnasatimageuspssectorpendigitsnews20letterproteinshuttleconnect-4mnistrcv1covtype1E-30.010.11101001000CPU time (sec) mcSVM ovaSVM ovoSVM mcODMMulti-Class Optimal Margin Distribution Machine

Ulrich, H. G. Kreel. Pairwise classiﬁcation and support
vector machines. In Schlkopf, B., Burges, C. J. C., and
Smola, A. J. (eds.), Advances in Kernel Methods: Sup-
port Vector Machines, pp. 255–268, Cambridge, MA,
December 1998. MIT Press.

Vapnik, V. The Nature of Statistical Learning Theory.

Springer-Verlag, New York, 1995.

Wang, L. W., Sugiyama, M., Yang, C., Zhou, Z.-H., and
Feng, J. A reﬁned margin analysis for boosting al-
gorithms via equilibrium margin. Journal of Machine
Learning Research, 12:1835–1863, 2011.

Weston, J. and Watkins, C. Multi-class support vector ma-
chines. In Proceedings of the European Symposium on
Artiﬁcial Neural Networks, Brussels, Belgium, 1999.

Zhang, T. and Zhou, Z.-H. Large margin distribution ma-
chine. In Proceedings of the 20th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining, pp.
313–322, New York, NY, 2014.

Zhang, T. and Zhou, Z.-H. Optimal margin distribution

machine. CoRR, abs/1604.03348, 2016.

Zhou, Y.-H. and Zhou, Z.-H. Large margin distribution
IEEE
learning with cost interval and unlabeled data.
Transactions on Knowledge and Data Engineering, 28
(7):1749–1763, 2016.

Zhou, Z.-H. Ensemble Methods: Foundations and Algo-

rithms. CRC Press, Boca Raton, FL, 2012.

Zhou, Z.-H. Large margin distribution learning.

In Pro-
ceedings of the 6th IAPR International Workshop on Ar-
tiﬁcial Neural Networks in Pattern Recognition, pp. 1–
11, Montreal, Canada, 2014.

References

Cortes, C. and Vapnik, V. Support-vector networks. Ma-

chine Learning, 20(3):273–297, 1995.

Crammer, K. and Singer, Y. On the algorithmic implemen-
tation of multiclass kernel-based vector machines. Jour-
nal of Machine Learning Research, 2:265–292, 2001.

Crammer, K. and Singer, Y. On the learnability and de-
sign of output codes for multiclass problems. Machine
Learning, 47(2):201–233, 2002.

Dietterich, T. G. and Bakiri, G. Solving multiclass learning
problems via error-correcting output codes. Journal of
Artiﬁcial Intelligence Research, 2(2):263–286, 1995.

Fan, R. E., Chang, K. W., Hsieh, C. J., Wang, X. R., and
Lin, C. J. Liblinear: A library for large linear classiﬁ-
cation. Journal of Machine Learning Research, 9:1871–
1874, 2008.

Freund, Y. and Schapire, R. E. A decision-theoretic gener-
alization of on-line learning and an application to boost-
ing. In Proceedings of the 2nd European Conference on
Computational Learning Theory, pp. 23–37, Barcelona,
Spain, 1995.

Gao, W. and Zhou, Z.-H. On the doubt about margin ex-
planation of boosting. Artiﬁcial Intelligence, 203:1–18,
2013.

Lei, Y., rn Dogan, Binder, A., and Kloft, M. Multi-
class svms: From tighter data-dependent generalization
bounds to novel algorithms.
In Cortes, C., Lawrence,
N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.),
Advances in Neural Information Processing Systems 28,
pp. 2026–2034. Curran Associates, Inc., 2015.

McDiarmid, C. On the method of bounded differences.

Surveys in Combinatorics, 141(1):148–188, 1989.

Mohri, M., Rostamizadeh, A., and Talwalkar, A. Founda-
tions of Machine Learning. MIT Press, Cambridge, MA,
2012.

Reyzin, L. and Schapire, R. E. How boosting the mar-
gin can also boost classiﬁer complexity. In Proceedings
of 23rd International Conference on Machine Learning,
pp. 753–760, Pittsburgh, PA, 2006.

Schapire, R. E., Freund, Y., Bartlett, P. L., and Lee, W. S.
Boosting the margin: a new explanation for the effectives
of voting methods. Annuals of Statistics, 26(5):1651–
1686, 1998.

Tseng, P. Convergence of a block coordinate descent
method for nondifferentiable minimization. Journal of
Optimization Theory and Applications, 109(3):475–494,
2001.

