A Simulated Annealing Based Inexact Oracle
for Wasserstein Loss Minimization

Jianbo Ye 1 James Z. Wang 1 Jia Li 2

Abstract

Learning under a Wasserstein loss, a.k.a. Wasser-
stein loss minimization (WLM), is an emerging
research topic for gaining insights from a large
set of structured objects. Despite being concep-
tually simple, WLM problems are computation-
ally challenging because they involve minimiz-
ing over functions of quantities (i.e. Wasserstein
distances) that themselves require numerical al-
gorithms to compute.
In this paper, we intro-
duce a stochastic approach based on simulated
annealing for solving WLMs. Particularly, we
have developed a Gibbs sampler to approximate
effectively and efﬁciently the partial gradients of
a sequence of Wasserstein losses. Our new ap-
proach has the advantages of numerical stability
and readiness for warm starts. These character-
istics are valuable for WLM problems that of-
ten require multiple levels of iterations in which
the oracle for computing the value and gradient
of a loss function is embedded. We applied the
method to optimal transport with Coulomb cost
and the Wasserstein non-negative matrix factor-
ization problem, and made comparisons with the
existing method of entropy regularization.

1. Introduction

An oracle is a computational module in an optimization
procedure that is applied iteratively to obtain certain char-
acteristics of the function being optimized. Typically, it
calculates the value and gradient of loss function l(x, y). In
the vast majority of machine learning models, where those
loss functions are decomposable along each dimension
, y) or
(e.g., Lp norm, KL divergence, or hinge loss),

rxl(

·

1College of Information Sciences and Technology, The Penn-
sylvania State University, University Park, PA. 2Department of
Statistics, The Pennsylvania State University, University Park,
PA.. Correspondence to: Jianbo Ye <jxy198@ist.psu.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

·

) is computed in O(m) time, m being the complex-
ryl(x,
ity of outcome variables x or y. This part of calculation is
often negligible compared with the calculation of full gra-
dient with respect to the model parameters. But this is no
longer the case in learning problems based on Wasserstein
distance due to the intrinsic complexity of the distance.
We will call such problems Wasserstein loss minimization
(WLM). Examples of WLMs include Wasserstein barycen-
ters (Li & Wang, 2008; Agueh & Carlier, 2011; Cuturi &
Doucet, 2014; Benamou et al., 2015; Ye & Li, 2014; Ye
et al., 2017b), principal geodesics (Seguy & Cuturi, 2015),
nonnegative matrix factorization (Rolet et al., 2016; San-
dler & Lindenbaum, 2009), barycentric coordinate (Bon-
neel et al., 2016), and multi-label classiﬁcation (Frogner
et al., 2015).

Wasserstein distance is deﬁned as the cost of matching two
probability measures, originated from the literature of op-
timal transport (OT) (Monge, 1781). It takes into account
the cross-term similarity between different support points
of the distributions, a level of complexity beyond the usual
vector data treatment, i.e., to convert the distribution into a
vector of frequencies. It has been promoted for comparing
sets of vectors (e.g. bag-of-words models) by researchers
in computer vision, multimedia and more recently natural
language processing (Kusner et al., 2015; Ye et al., 2017a).
However, its potential as a powerful loss function for ma-
chine learning has been underexplored. The major obstacle
is a lack of standardized and robust numerical methods to
solve WLMs. Even to empirically better understand the
advantages of the distance is of interest.

As a long-standing consensus, solving WLMs is challeng-
ing (Cuturi & Doucet, 2014). Unlike the usual optimiza-
tion in machine learning where the loss and the (partial)
gradient can be calculated in linear time, these quantities
are non-smooth and hard to obtain in WLMs, requiring so-
lution of a costly network transportation problem (a.k.a.
OT). The time complexity, O(m3 log m), is prohibitively
high (Orlin, 1993). In contrast to the Lp or KL counter-
parts, this step of calculation elevates from a negligible
fraction of the overall learning problem to a dominant por-
tion, preventing the scaling of WLMs to large data. Re-
cently, iterative approximation techniques have been devel-
oped to compute the loss and the (partial) gradient at com-

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

plexity O(m2/") (Cuturi, 2013; Wang & Banerjee, 2014).
However, nontrivial algorithmic efforts are needed to in-
corporate these methods into WLMs because WLMs often
require multi-level loops (Cuturi & Doucet, 2014; Frogner
et al., 2015). Speciﬁcally, one must re-calculate through
many iterations the loss and its partial gradient in order to
update other model dependent parameters.

We are thus motivated to seek for a fast inexact oracle
that (i) runs at lower time complexity per iteration, and
(ii) accommodates warm starts and meaningful early stops.
These two properties are equally important for efﬁciently
obtaining adequate approximation to the solutions of a se-
quence of slowly changing OTs. The second property en-
sures that the subsequent OTs can effectively leverage the
solutions of the earlier OTs so that the total computational
time is low. Approximation techniques with low complex-
ity per iteration already exist for solving a single OT, but
they do not possess the second property. In this paper, we
introduce a method that uses a time-inhomogeneous Gibbs
sampler as an inexact oracle for Wasserstein losses. The
Markov chain Monte Carlo (MCMC) based method natu-
rally satisﬁes the second property, as reﬂected by the in-
tuition of physicists that MCMC samples can efﬁciently
“remix from a previous equilibrium.”

We propose a new optimization approach based on Sim-
ulated Annealing (SA) (Kirkpatrick et al., 1983; Corana
et al., 1987) for WLMs where the outcome variables are
treated as probability measures. SA is especially suitable
for the dual OT problem, where the usual Metropolis sam-
pler can be simpliﬁed to a Gibbs sampler. To our knowl-
edge, existing optimization techniques used on WLMs are
different from MCMC. In practice, MCMC is known to
easily accommodate warm start, which is particularly use-
ful in the context of WLMs. We name this approach Gibbs-
OT for short. The algorithm of Gibbs-OT is as simple
and efﬁcient as the Sinkhorn’s algorithm — a widely ac-
cepted method to approximately solve OT (Cuturi, 2013).
We show that Gibbs-OT enjoys improved numerical sta-
bility and several algorithmic characteristics valuable for
general WLMs. By experiments, we demonstrate the ef-
fectiveness of Gibbs-OT for solving optimal transport with
Coulomb cost (Benamou et al., 2016) and the Wasserstein
non-negative matrix factorization (NMF) problem (Sandler
& Lindenbaum, 2009; Rolet et al., 2016).

2. Related Work

Recently, several methods have been proposed to overcome
the aforementioned difﬁculties in solving WLMs. Rep-
resentatives include entropic regularization (Cuturi, 2013;
Cuturi & Doucet, 2014; Benamou et al., 2015) and Breg-
man ADMM (Wang & Banerjee, 2014; Ye et al., 2017b).
The main idea is to augment the original optimization prob-

lem with a strongly convex term such that the regularized
objective becomes a smooth function of all its coordinat-
ing parameters. Neither the Sinkhorn’s algorithm nor Breg-
man ADMM can be readily integrated into a general WLM.
Based on the entropic regularization of primal OT, Cuturi &
Peyr´e (2016) recently showed that the Legendre transform
of the entropy regularized Wasserstein loss and its gradi-
ent can be computed in closed form, which appear in the
ﬁrst-order condition of some complex WLM problems. Us-
ing this technique, the regularized primal problem can be
converted to an equivalent Fenchel-type dual problem that
has a faster numerical solver in the Euclidean space (Ro-
let et al., 2016). But this methodology can only be applied
to a certain class of WLM problems of which the Fenchel-
type dual has closed forms of objective and full gradient.
In contrast, the proposed SA-based approach directly deals
with the dual OT problem without assuming any particular
mathematical structure of the WLM problem, and hence is
more ﬂexible to apply.

More recent approaches base on solving the dual OT prob-
lems have been proposed to calculate and optimize the
Wasserstein distance between a single pair of distributions
with very large support sets — often as large as the size
of an entire machine learning dataset (Montavon et al.,
2016; Genevay et al., 2016; Arjovsky et al., 2017). For
these methods, scalability is achieved in terms of the sup-
port size. Our proposed method has a different focus on
calculating and optimizing Wasserstein distances between
many pairs all together in WLMs, with each distribution
having a moderate support size (e.g., dozens or hundreds).
We aim at scalability for the scenarios when a large set of
distributions have to be handled simultaneously, that is, the
optimization cannot be decoupled on the distributions. In
addition, existing methods have no on-the-ﬂy mechanism
to control the approximation quality at a limited number of
iterations.

3. Preliminaries of Optimal Transport

In this section, we present notations, mathematical back-
grounds, and set up the problem of interest.

q

2

def.=

Deﬁnition 3.1 (Optimal Transportation, OT). Let p
2
 m2 , where  m is the set of m-dimensional
 m1 , q
simplex:  m
q,
. The set of trans-
}
h
portation plans between p and q is deﬁned as ⇧(p, q) def.=
· m2 = p; Z T
Z
. Let
{
M
be the matrix of costs. The optimal trans-
port cost between p and q with respect to M is

· m1 = q;

Rm1⇥
m1⇥
R
+

m2 : Z
m2

Rm
+ :

2
2

= 1

2

}

{

i

W (p, q) def.= min

⇧(p,q)h

Z

2

Z, M

.

i

(1)

In particular, ⇧(

,

) is often called the coupling set.

·

·

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

Now we relate primal version of (discrete) OT to a vari-
ant of its dual version. One may refer to Villani (2003) for
the general background of the Kantorovich-Rubenstein du-
ality. In particular, our formulation introduces an auxiliary
parameter CM for the sake of mathematical soundness in
deﬁning Boltzmann distributions.

Deﬁnition 3.2 (Dual Formulation of OT). Let CM > 0, de-
note vector [g1, . . . , gm1 ]T by g, and vector [h1, . . . , hm2 ]T
by h. We deﬁne the dual domain of OT by

⌦(M ) def.=

f = [g; h]

m1+m2

n
CM < gi  

 

hj 

R

2
Mi,j, 1

i





 
m1, 1
 

j





m2

.

o

(2)

Informally, for a sufﬁciently large CM (subject to p, q, M ),
the LP problem Eq. (1) can be reformulated as 1

W (p, q) = max

p, g

⌦(M )h

f

2

q, h

.

i

i   h

(3)

2

@W/@p and

Let the optimum set be ⌦⇤(M ). Then any optimal point
⌦⇤(M ) constructs a (projected) subgradi-
f ⇤ = (g⇤, h⇤)
2
ent such that g⇤
@W/@q . The main
computational difﬁculty of WLMs comes from the fact that
(projected) subgradient f ⇤ is not efﬁciently solvable.
Note that ⌦(M ) is an unbound set in Rm1+m2 . In order
to constrain the feasible region to be bounded, we alterna-
tively deﬁne

h⇤

 

2

⌦0(M ) =

f = [g; h]
{

2

⌦(M )

g1 = 0
}

.

|

(4)

One can show that the maximization in ⌦(M ) as Eq. (3)
to the maximization in ⌦0(M ) because
is equivalent
q, m2 i
p, m1 i

=

h

h

.

4. Simulated Annealing for Optimal
Transport via Gibbs Sampling

Following the basic strategy outlined in the seminal pa-
per of simulated annealing (Kirkpatrick et al., 1983), we
present the deﬁnition of Boltzmann distribution supported
on ⌦0(M ) below which, as we will elaborate, links the
dual formulation of OT to a Gibbs sampling scheme (Al-
gorithm 1 below).

Deﬁnition 4.1 (Boltzmann Distribution of OT). Given a
temperature parameter T > 0, the Boltzmann distribution

1However, for any proper M and strictly positive p, q, there
exists CM such that the optimal value of primal problem is equal
to the optimal value of the dual problem. This modiﬁcation is
solely for an ad-hoc treatment of a single OT problem. In general
cases of (p, q, M ), when CM is pre-ﬁxed, the solution of Eq. (3)
may be suboptimal.

of OT is a probability measure on ⌦0(M )
such that

✓

Rm1+m2 

1

p(f ; p, q)

exp

/

1
T



(

p, g

h

i   h

q, h

)

.

i

 

(5)

It is a well-deﬁned probability measure for an arbitrary ﬁ-
nite CM > 0.

The basic concept behind SA states that the samples from
the Boltzmann distribution will eventually concentrate at
the optimum set of its deriving problem (e.g. W (p, q)) as
T
0. However, since the Boltzmann distribution is of-
ten difﬁcult to sample, a practical convergence rate remains
mostly unsettled for speciﬁc MCMC methods.

!

Because ⌦(M ) deﬁned by Eq. (2) (also ⌦0) has a con-
ditional independence structure among variables, a Gibbs
sampler can be naturally applied to the Boltzmann distri-
bution deﬁned by Eq. (5). We summarize this result below.

Proposition 4.1. Given any f = (g; h)
CM > 0, we have for any i and j,

2

⌦0(M ) and any

(6)

(7)

(8)

(9)

gi 
hj  

Ui(h)

Lj(g)

def.=

min
1
j
m2


def.= max
m1
1



i

(Mi,j + hj) ,

(gi  

Mi,j) .

and

gi >

Li(h)

hj <

Uj(g)
b

j

def.= max
1
m2


def.= max
m1
1



i

CM + hj) ,

(

 

(CM + gi) .

b

Here Ui = Ui(h) and Lj = Lj(g) are auxiliary variables.
Suppose f follows the Boltzmann distribution by Eq. (5),
gi’s are conditionally independent given h, and likewise
hj’s are also conditionally independent given g. Further-
more, it is immediate from Eq. (5) that each of their con-
ditional probabilities within its feasible region (subject to
CM ) satisﬁes

h)

exp

p(gi|
p(hj|

/

/

g)

exp

gipi
T

⇣

⌘
hjqj
T

 

✓
m1 and 1

,

Li(h) < gi 
b
, Lj(g)
◆



Ui(h),

(10)

hj <

Uj(g),

(11)

j



m2.

where 2
i

Remark 1. As CM !
1
!
m2, one can ap-

 1
g)
h) and p(hj|
proximate the conditional probability p(gi|
b
by exponential distributions.

1
m1 and 1
b


Uj(g)

. For 2

Li(h)


,

!
j

and







+

+

i

b

By Proposition. 4.1, our proposed time-inhomogeneous
Gibbs sampler is given in Algorithm 1. Speciﬁcally in Al-
gorithm 1, the variable g1 is ﬁxed to zero by the deﬁnition

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

Algorithm 1 Gibbs Sampling for Optimal Transport
Given f (0)
 m2 , and
2
T (1), . . . , T (2N ) > 0, for t = 1, . . . , N , we deﬁne the fol-
lowing Markov chain

 m1 and q

⌦0(M ), p

2

2

1. Randomly sample

✓1, . . . ,✓ m2

i.i.d.
⇠
For j = 1, 2, . . . , m2, let

Exponential(1).

L(t)
j
h(t)
j

(

:= max1
:= L(t)

i

m1





j + ✓j ·

g(t
i
T (2t
⇣

 

1)

 

 
1)/qj

Mi,j

⌘

(12)

[. . . , z2t

 

approximate the Wasserstein gradient. In practice, we ﬁnd
this bound helps one quickly select the beginning tempera-
ture of Gibbs-OT algorithm.
Deﬁnition 4.2 (Notations for Auxiliary Statistics). Besides
the Gibbs coordinates g and h, the Gibbs-OT sampler nat-
urally introduces two auxiliary variables, U and L. Let
T
L(t) =
and U(t) =
Likewise, denote the collection of g(t)
j by vectors
g(t) and h(t) respectively. The following sequence of aux-
iliary statistics

1 , . . . , U (t)
U (t)
m1
and h(t)
h

1 , . . . , L(t)
L(t)
m2

h

i

i

T

.

i

1, z2t, z2t+1, . . . , ] def.=
L(t)
L(t)
U(t)

U(t

. . . ,

1)

,

 

,

L(t+1)
U(t)

, . . .

(14)

 







 


for t = 1, . . . , N is also a Markov chain. They can be re-
deﬁned equivalently by specifying the transition probabili-
zn) for n = 1, . . . , 2N , a.k.a., the conditional
ties p(zn+1
|
p.d.f. p(U(t)
U(t))
for t = 1, . . . , N

L(t)) for t = 1, . . . , N and p(L(t+1)

1.

 

 

|

|

 

One may notice that the alternative representation converts
the Gibbs sampler to one whose structure is similar to a
hidden Markov model, where the g, h chain is conditional
independent given the U, L chain and has (factored) expo-
nential emission distributions. We will use this equivalent
representation in Appendix A and develop analysis based
on the U, L chain accordingly.
Remark 4. We now consider the function
V (x, y) def.=

p, x

q, y

,

h

i   h

i

and deﬁne a few additional notations. Let V (Ut0 , Lt) be
denoted by V (zt+t0 ), where t0 = t or t
If g, h are
independently resampled according to Eq. (12) and (13),
we will have the inequalities that

 

1.

E [V (g, h)

zn]

V (zn) .

|
Both V (z) and V (g, h) converges to the exact
W (p, q) at
p(f ; p, q) as T

loss
the equilibrium of Boltzmann distribution

0. 2



!

5. Gibbs-OT: An Inexact Oracle for WLMs

In this section, we introduce a non-standard SA approach
for the general WLM problems. The main idea is to replace
the standard Boltzmann energy with an asymptotic consis-
tent upper bound, outlined in our previous section. Let

R(✓) :=

W (pi(✓), qi(✓))

|D|

i=1
X

2The conditional quantity V (zn)

zn is the sum
of two Gamma random variables: Gamma(m1, 1/T (2t)) +
Gamma(m2, 1/T (2t0+1)) where t0 = t or t0 = t

V (g, h)

 

1.

|

 

2. Randomly sample

✓2, . . . ,✓ m1

i.i.d.
⇠
For i = (1), 2, . . . , m1, let

Exponential(1).

U (t)
i
g(t)
i

(

:= min1
:= U (t)

j

m2





i  

✓i ·

Mi,j + h(t)
j

T (2t)/pi
⇣

⌘

(13)

of ⌦0(M ). But we have found in experiments that by cal-
culating U (t)
and sampling g(t)
in Algorithm 1 according
1
1
to Eq. (13), one can still generate MCMC samples from
⌦(M ) such that the energy quantity
con-
verges to the same distribution as that of MCMC samples
from ⌦0(M ). Therefore, we will not assume g1 = 0 from
now on and develop analysis solely for the unconstrained
version of Gibbs-OT.

i   h

q, h

p, g

h

i

Figure 1 illustrates the behavior of the proposed Gibbs sam-
pler with a cooling schedule at different temperatures. As
T decreases along iterations, the 95% percentile band for
sample f becomes thinner and thinner.
Remark 2. Algorithm 1 does not specify the actual cool-
ing schedule, nor does the analysis of the proposed Gibbs
sampler in Theorem A.2. We have been agnostic here for a
reason. In the SA literature, cooling schedules with guaran-
teed optimality are often too slow to be useful in practice.
To our knowledge, the guaranteed rate of SA approach is
worse than the combinatorial solver for OT. As a result, a
well-accepted practice of SA for many complicated opti-
mization problems is to empirically adjust cooling sched-
ules, a strategy we take for our experiments.
Remark 3. Although the exact cooling schedule is not spec-
iﬁed, we still provide a quantitative upper bound of the cho-
sen temperature T at different iterations in Appendix A
Eq. (24). One can calculate such bound at the cost of
m log m at certain iterations to check whether the current
temperature is too high for the used Gibbs-OT to accurately

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

U
L
 f(1)
 f(2)

1.5

0.5

2

1

0

-0.5

-1

-1.5

-2

U
L
 f(1)
 f(2)

1.5

0.5

2

1

0

-0.5

-1

-1.5

-2

U
L
 f(1)
 f(2)

2.5

1.5

0.5

2

1

0

-0.5

-1

-1.5

-2

-2.5

0

0.2

0.4

0.6

0.8

0.2

0.4

0.6

0.8

0.2

0.4

0.6

0.8

1

-2.5

0

1

-2.5

0

1

(a) 20 iterations

(b) 40 iterations

(c) 60 iterations

Figure 1. The Gibbs sampling of the proposed SA method. From left to right is an illustrative example of a simple 1D optimal trans-
20, 40, 60
portation problem with Coulomb cost and plots of variables for solving this problem at different number of iterations
using the inhomogeneous Gibbs sampler. Particularly, the 95% percentile of the exponential distributions are marked by the gray area.

2{

}

represents a
be our prototyped objective function, where
dataset, pi, qi are prototyped probability densities for rep-
resenting the i-th instance. We now discuss how to solve
min✓

⇥ R(✓).

D

2

To minimize the Wasserstein losses W (p, q) approxi-
mately in such WLMs, we propose to instead optimize
its asymptotic consistent upper bound E[V (z)] at equilib-
rium of Boltzmann distribution p(f ; p, q) using its stochas-
@V (z)/@q .
@V (z)/@p and
tic gradients: U
Therefore, one can calculate the gradient approximately:

 

L

2

2

r✓R

⇡

[J✓(pi(✓))Ui  

J✓(qi(✓))Li]

|D|

i=1
X

·

where J✓(
) is the Jacobian, Ui, Li are computed from
Algorithm 1 for the problem W (pi, qi) respectively. To-
gether with the iterative updates of model parameters ✓,
one gradually anneals the temperature T . The equilibrium
of p(f ; p, q) becomes more and more concentrated. We as-
sume the inexact oracle at a relatively higher temperature
is adequate for early updates of the model parameters, but
sooner or later it becomes necessary to set T smaller to bet-
ter approximate the exact loss.

i

, h(t)
j

It is well known that the variance of stochastic gradient
usually affects the rate of convergence. The reason to re-
place V (g, h) with V (z) as the inexact oracle (for some
T > 0) is motivated by the same intuition. The variances
of MCMC samples g(t)
of Algorithm 1 can be very
large if pi/T and qj/T are small, making the embedded
ﬁrst-order method inaccurate unavoidably. But we ﬁnd the
variances of max/min statistics U (t)
j are much smaller.
Fig. 1 shows an example. The bias introduced in the re-
placement is also well controlled by decreasing the tem-
perature parameter T . For the sake of efﬁciency, we use
a very simple convergence diagnostics in the practice of
Gibbs-OT. We check the values of V (z(2t)) such that the

, L(t)

i

Markov chain is roughly considered mixed if every ⌧ iter-
ation the quantity V (z(2t)) (almost) stops increasing (⌧=5
by default), say, for some t,

V (z(2t))

V (z(2(t

 

⌧ ))) < 0.01⌧T

V (z(2t)),

 
we terminate the Gibbs iterations.

·

6. Applications of Gibbs-OT

6.1. Toy OT Examples

1D Case with Euclidean Cost. We ﬁrst illustrate the dif-
ferences between the approximate primal solutions com-
puted by different methods by replicating a toy example
in (Benamou et al., 2015). The toy example calculates the
OT between two 1D two-mode distributions. We visualize
their solved coupling as a 2D image in Fig. 2 at the budgets
in terms of different number of iterations. Given their dif-
ferent convergence behaviors, when one wants to compro-
mise with using pre-converged primal solutions in WLMs,
he or she has to account for the different results computed
by different numerical methods, even though they all aim
at the Wasserstein loss.

As a note, Sinkhorn, B-ADMM and Gibbs-OT share the
same computational complexity per iteration. The differ-
ence in their actual CPU time comes from the different
arithmetic operations used. B-ADMM may be the slowest
because it requires log() and exp() operations. When
memory efﬁciency is of concern, both the implementa-
tions of Sinkhorn and Gibbs-OT can be modiﬁed to take
only O(m1 + m2) additional memory besides the space for
caching the cost matrix M .

Two Electrons with Coulomb Cost in DFT. In quantum
mechanics, Coulomb cost (or electron-electron Coulomb
repulsion) is an important energy functional in Density
Functional Theory (DFT). Numerical methods that solve

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

l

=
1

l

=
1
0

l

=
5
0

l

=
2
0
0

l

=
1
0
0
0

l

=
5
0
0
0

IBP, rho=0.1/N

IBP, rho=0.5/N

IBP, rho=2.0/N

B-ADMM

SimulAnn

Figure 2. A simple example for OT between two 1D distribu-
tion: The solutions by Iterative Bregman Projection, B-ADMM,
and Gibbs-OT are shown in pink, while the exact solution
by linear programming is shown in green.
Images in the
rows from top to bottom present results at different iterations
1, 10, 50, 200, 1000, 5000
; The left three columns are by IBP
{
with " =
, where [0, 1] is discretized with
}
N = 128 uniformly spaced points. The fourth column is by B-
ADMM (with default parameter ⌧0 = 2.0). The last column is
the proposed Gibbs-OT, with a geometric cooling schedule. With
a properly selected cooling schedule, one can achieve fast conver-
gence of OT solution without comprising much solution quality.

0.1/N, 0.5/N, 2/N

}

{

|

y

x

 

the multi-marginal OT problem with unbounded costs re-
mains an open challenge in DFT (Benamou et al., 2016).
We consider two uniform densities on 1D domain [0, 1]
with Coulomb cost c(x, y) = 1/
which has analytic
|
solutions. Coulumb cost is different from the usual metric
cost in the OT literature, which is unbounded and singu-
lar at x = y. As observed in (Benamou et al., 2016), the
entropic regularized primal solution becomes more concen-
trated at boundaries, which is not physically plausible. This
effect is not observed in the Gibbs-OT solution as shown in
Appendix Fig. 3. As shown by Fig 1, the variables U, V
in computation are always in bounded range (with an over-
whelming probability), thus the algorithm does not endure
any numerical difﬁculties.

For entropic regularization (Benamou et al., 2015; 2016),
we empirically select the minimal " which does not cause
numerical overﬂow before 5000 iterations (in which " =

0.5/N ). For Gibbs-OT, we use a geometric temperature
scheme such that T = 2.0(1/l4)n/l/N at the n-th iteration,
where l is the max iteration number. For the unbounded
Coulomb cost, Bregman ADMM (Wang & Banerjee, 2014)
does not converge to a solution close to the true optimum.

6.2. Wasserstein NMF

We now illustrate how the proposed Gibbs-OT can be used
as a ready-to-plugin inexact oracle for a typical WLM —
Wasserstein NMF (Sandler & Lindenbaum, 2009; Rolet
et al., 2016). The data parallelization of this framework is
natural because the Gibbs-OT samplers subject to different
instances are independent.

K

 k}
{

k=1  (i)

Problem Formulation. Given a set of discrete proba-
i=1 (data) over Rd, we want to es-
n
 i}
bility measures
{
K
k=1, such that for each  i,
timate a model ⇥=
there exists a membership vector  (i)
 K:  i ⇡
k  k, where each  k is again a discrete prob-
ability measure to be estimated. Therefore, Wasserstein
P
NMF reads min⇥,⌅
, where
⌅= (  (1), . . . ,  (n)) is the collection of membership vec-
tors, and W is the Wasserstein distance. One can write the
problem by plugging Eq. (3) in the dual formulation:

k=1  (i)

n
i=1 W

k  k

 i,

P

P

2

⇣

⌘

K

min
⇥,⌅

max
fi}
{

n
i=1

F =

X

s.t.  k =

w(i), hii

i

n

i=1

h

h

w(i), gii   h
v(k)
i  xi ,

m
b
i=1
K

 (i)
k  k ,

k=1
 (i),  i)

X
M (

⌦

 (i) =

X

fi 2
b

⇣

,

⌘

(15)

(16)

(17)

(18)

2

b
w(i)
 m is the weight vector of discrete prob-
where
 (i) and w(i)
 mi is the weight vec-
ability measure
tor of  (i). M (
) denotes the transportation cost matrix
,
b
·
·
between the supports of two measures. The global opti-
b
mization solves all three sets of variables (⇥, ⌅, F ). In the
sequel, we assume support points of
are shared and pre-ﬁxed.

 k}
{

m
k=1 —

xi}
{

m
i=1

2

Algorithm. At every epoch, one updates variables either
sequentially (indexed by i) or all together. It is done by
ﬁrst executing the Gibbs-OT oracle subject to the i-th in-
stance and then updating v(k) and the membership vec-
tor  (i) accordingly at a chosen step size  > 0. At the
end of each epoch, the temperature parameter T is adjusted
n
T := T
i=1 mi . For
each instance i, the algorithm proceeds with the following
q
steps iteratively:

, where ¯m = 1
n

1
m+ ¯m

P

 

⇣

⌘

1

1. Initiate from the last computed U/V sample subject
to instance i, execute the Gibbs-OT Gibbs sampler at

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

Figure 3. The recovered primal solutions for two uniform 1D distribution with Coulumb cost. The approximate solutions are shown in
pink, while the exact solution by linear programming is shown in green. Top row: entropic regularization with " = 0.5/N . Bottom row:
Gibbs-OT. Images in the rows from left to right present results at different max iterations

1, 10, 50, 200, 1000, 2000, 5000

.

{

}

constant temperature T until a mixing criterion is met,
and get Ui.

2. For k = 1, . . . , K, update v(k)

 m based on gradi-
ent  (i)
k Ui using the iterates of online mirror descent
(MD) subject to the step-size   (Beck & Teboulle,
2003).

2

3. Also update the membership vector  (i)
v(K), Uii
v(1), Uii

 K based
2
)T using the
on gradient (
, . . . ,
iterates of accelerated mirror descent (AMD) with
restarts subject to the same step-size   (Krichene
et al., 2015).

h

h

We note that the practical speed-ups we achieved via the
above procedure is the warm-start feature in Step 1. If one
uses a black-box OT solver, this dimension of speed-ups is
not viable.

Results. We investigate the empirical convergence of the
proposed Wasserstein NMF method by two datasets: one
is a subset of MNIST handwritten digit images which con-
tains 200 digits of “5”, and the other is the ORL 400-face
dataset. Our results are based on a C/C++ implementation
with vectorization. In particular, we set K = 40,  = 2.0
for both datasets. The learned components are visual-
ized together with alternative approaches (smoothed W-
NMF (Rolet et al., 2016) and regular NMF) in Appendix
Figs. 4 and 5. From these ﬁgures, we observe that our
learned components using Gibbs-OT are shaper than the
smoothed W-NMF. This can be explained by the fact that
Gibbs-OT can potentially push for higher quality of ap-
proximation by gradually annealing the temperature. We
also observe that the learned components might possess
some salt-and-pepper noise. This is because the Wasser-
stein distance by deﬁnition is not very sensitive to the sub-
pixel displacements. On a single-core of a 3.3 GHz Intel

Core i5 CPU, the average time spent for each epoch for
these two datasets are 0.84 seconds and 16.8 seconds, re-
spectively. It is about two magnitude faster than fully solv-
ing all OTs via a commercial LP solver 3.

7. Discussions

The solution of primal OT (Monge-Kantorovich problem)
have many direct interpretations, where the solved trans-
port is a coupling between two measures. Hence, it could
be well motivated to consider regularizing the solution on
the primal domain in those problems (Cuturi, 2013). Mean-
while, the solution of dual OT can be meaningful in its
own right. For instance, in ﬁnance, the dual solution is
directly interpreted as the vanilla prices implementing ro-
bust static super-hedging strategies. The entropy regular-
ized OT, under the Fenchel-type dual, provides a smoothed
unconstrained dual problem as shown in (Cuturi & Peyr´e,
2016). In this paper, we develop Gibbs-OT, whose solu-
tions respect the dual feasibility of OT and are subject to a
different regularization effect as explained by (Abernethy
& Hazan, 2015). It is a numerical stable and computational
suitable oracle to handle WLM.

Acknowledgement. This material is based upon work sup-
ported by the National Science Foundation under Grant
Nos. ECCS-1462230 and DMS-1521092. The authors
would also like to thank anonymous reviewers for their
valuable comments.

3We use the specialized network ﬂow solver in Mosek
(https://www.mosek.com) for the computation, which is
found faster than general simplex or IPM solver at moderate prob-
lem scale.

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

Figure 4. NMF components learned by different methods (K =
40) on the 200 digit “5” images. Top: regular NMF; Middle:
W-NMF with entropic regularization (" = 1/100, ⇢1 = ⇢2 =
1/200); Bottom: W-NMF using Gibbs-OT. It is observed that the
components of W-NMF with entropic regularization are smoother
than those optimized with Gibbs-OT.

References

Abernethy, Jacob and Hazan, Elad. Faster convex optimiza-
tion: Simulated annealing with an efﬁcient universal bar-
rier. arXiv preprint arXiv:1507.02528, 2015.

Agueh, Martial and Carlier, Guillaume. Barycenters in the
Wasserstein space. SIAM J. Math. Analysis, 43(2):904–
924, 2011.

Figure 5. NMF components learned by different methods (K =
40) on the ORL face images. Top: regular NMF; Middle: W-
NMF with entropic regularization (" = 1/100, ⇢1 = ⇢2 =
1/200); Bottom: W-NMF using Gibbs-OT, in which the salt and
pepper noises are observed due to the fact that Wasserstein dis-
tance is insensitive to the subpixel mass displacement (Cuturi &
Peyr´e, 2016).

Wasserstein GAN. arXiv preprint arXiv:1701.07875,
2017.

Arjovsky, Martin, Chintala, Soumith, and Bottou, L´eon.

Beck, Amir and Teboulle, Marc. Mirror descent and non-

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

linear projected subgradient methods for convex opti-
mization. Operations Research Letters, 31(3):167–175,
2003.

Benamou, Jean-David, Carlier, Guillaume, Cuturi, Marco,
Nenna, Luca, and Peyr´e, Gabriel. Iterative Bregman pro-
jections for regularized transportation problems. SIAM J.
on Scientiﬁc Computing, 37(2):A1111–A1138, 2015.

Benamou, Jean-David, Carlier, Guillaume, and Nenna,
Luca. A numerical method to solve multi-marginal opti-
mal transport problems with Coulomb cost. In Splitting
Methods in Communication, Imaging, Science, and En-
gineering, pp. 577–601. Springer, 2016.

Bonneel, Nicolas, Peyr´e, Gabriel, and Cuturi, Marco.
Wasserstein barycentric coordinates: Histogram regres-
sion using optimal transport. ACM Trans. on Graphics,
35(4), 2016.

Corana, Angelo, Marchesi, Michele, Martini, Claudio, and
Ridella, Sandro. Minimizing multimodal functions of
continuous variables with the simulated annealing algo-
rithm corrigenda for this article is available here. ACM
Trans. on Mathematical Software, 13(3):262–280, 1987.

Cuturi, Marco. Sinkhorn distances: Lightspeed computa-
tion of optimal transport. In Advances in Neural Infor-
mation Processing Systems, pp. 2292–2300, 2013.

Cuturi, Marco and Doucet, Arnaud. Fast computation of
In Proc. Int. Conf. Machine

Wasserstein barycenters.
Learning, pp. 685–693, 2014.

Cuturi, Marco and Peyr´e, Gabriel. A smoothed dual ap-
proach for variational Wasserstein problems. SIAM J. on
Imaging Sciences, 9(1):320–343, 2016.

Kusner, Matt, Sun, Yu, Kolkin, Nicholas, and Weinberger,
Kilian. From word embeddings to document distances.
In Proc. of the Int. Conf. on Machine Learning, pp. 957–
966, 2015.

Li, Jia and Wang, James Z. Real-time computerized an-
notation of pictures. IEEE Trans. Pattern Analysis and
Machine Intelligence, 30(6):985–1002, 2008.

Monge, Gaspard. M´emoire sur la th´eorie des d´eblais et des

remblais. De l’Imprimerie Royale, 1781.

Montavon, Gr´egoire, M¨uller, Klaus-Robert, and Cuturi,
Marco. Wasserstein training of restricted boltzmann ma-
chines. In Lee, D. D., Sugiyama, M., Luxburg, U. V.,
Guyon, I., and Garnett, R. (eds.), Advances in Neural In-
formation Processing Systems 29, pp. 3711–3719. 2016.

Orlin, James B. A faster strongly polynomial minimum
cost ﬂow algorithm. Operations Research, 41(2):338–
350, 1993.

Rolet, Antoine, Cuturi, Marco, and Peyr´e, Gabriel. Fast
dictionary learning with a smoothed Wasserstein loss. In
AISTAT, 2016.

Sandler, Roman and Lindenbaum, Michael. Nonnegative
matrix factorization with earth mover’s distance metric.
In Proc. of the Conf. on Computer Vision and Pattern
Recognition, pp. 1873–1880. IEEE, 2009.

Seguy, Vivien and Cuturi, Marco. Principal geodesic anal-
ysis for probability measures under the optimal transport
metric. In Advances in Neural Information Processing
Systems, pp. 3294–3302, 2015.

Villani, C´edric. Topics in Optimal Transportation. Num-

ber 58. American Mathematical Soc., 2003.

Frogner, Charlie, Zhang, Chiyuan, Mobahi, Hossein,
Araya, Mauricio, and Poggio, Tomaso A. Learning with
a Wasserstein loss. In Advances in Neural Information
Processing Systems, pp. 2044–2052, 2015.

Wang, Huahua and Banerjee, Arindam. Bregman alter-
nating direction method of multipliers. In Advances in
Neural Information Processing Systems, pp. 2816–2824,
2014.

Genevay, Aude, Cuturi, Marco, Peyr´e, Gabriel, and Bach,
Francis. Stochastic optimization for large-scale optimal
transport. In Lee, D. D., Sugiyama, M., Luxburg, U. V.,
Guyon, I., and Garnett, R. (eds.), Advances in Neural In-
formation Processing Systems 29, pp. 3440–3448. 2016.

Kirkpatrick, Scott, Gelatt, C. Daniel, Jr, and Vecchi,
Mario P. Optimization by simmulated annealing. Sci-
ence, 220(4598):671–680, 1983.

Krichene, Walid, Bayen, Alexandre, and Bartlett, Peter L.
Accelerated mirror descent in continuous and discrete
time. In Advances in Neural Information Processing Sys-
tems, pp. 2827–2835, 2015.

Ye, Jianbo and Li, Jia. Scaling up discrete distribution clus-
tering using admm. In Proc. of the Int. Conf. on Image
Processing, pp. 5267–5271. IEEE, 2014.

Ye, Jianbo, Li, Yanran, Wu, Zhaohui, Wang, James Z, Li,
Wenjie, and Li, Jia. Determining gains acquired from
word embedding quantitatively using discrete distribu-
In Proc. of the Annual Meeting of the
tion clustering.
Association for Computational Linguistics, 2017a.

Ye, Jianbo, Wu, Panruo, Wang, James Z, and Li, Jia.
Fast discrete distribution clustering using Wasserstein
barycenter with sparse support. IEEE Trans. on Signal
Processing, 65(9):2317–2332, 2017b.

