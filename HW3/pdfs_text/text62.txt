The Shattered Gradients Problem:
If resnets are the answer, then what is the question?

David Balduzzi 1 Marcus Frean 1 Lennox Leary 1 JP Lewis 1 2 Kurt Wan-Duo Ma 1 Brian McWilliams 3

Abstract

A long-standing obstacle to progress in deep
learning is the problem of vanishing and ex-
ploding gradients. Although, the problem has
largely been overcome via carefully constructed
initializations and batch normalization, archi-
tectures incorporating skip-connections such as
highway and resnets perform much better than
standard feedforward architectures despite well-
chosen initialization and batch normalization. In
this paper, we identify the shattered gradients
problem. Speciﬁcally, we show that the cor-
relation between gradients in standard feedfor-
ward networks decays exponentially with depth
resulting in gradients that resemble white noise
whereas, in contrast, the gradients in architec-
tures with skip-connections are far more resis-
tant to shattering, decaying sublinearly. Detailed
empirical evidence is presented in support of the
analysis, on both fully-connected networks and
convnets. Finally, we present a new “looks lin-
ear” (LL) initialization that prevents shattering,
with preliminary experiments showing the new
initialization allows to train very deep networks
without the addition of skip-connections.

1. Introduction

Deep neural networks have achieved outstanding perfor-
mance (Krizhevsky et al., 2012; Szegedy et al., 2015; He
et al., 2016b). Reducing the tendency of gradients to van-
ish or explode with depth (Hochreiter, 1991; Bengio et al.,
1994) has been essential to this progress.

Combining careful initialization (Glorot & Bengio, 2010;

*Equal

contribution

ton, New Zealand 2SEED, Electronic Arts
search, Z¨urich, Switzerland.
Balduzzi <dbalduzzi@gmail.com>,
<brian@disneyresearch.com>.

1Victoria University of Welling-
3Disney Re-
Correspondence to: David
Brian McWilliams

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

He et al., 2015) with batch normalization (Ioffe & Szegedy,
2015) bakes two solutions to the vanishing/exploding gra-
dient problem into a single architecture. The He initial-
ization ensures variance is preserved across rectiﬁer lay-
ers, and batch normalization ensures that backpropagation
through layers is unaffected by the scale of the weights
(Ioffe & Szegedy, 2015).

It is perhaps surprising then that residual networks (resnets)
still perform so much better than standard architectures
when networks are sufﬁciently deep (He et al., 2016a;b).
This raises the question: If resnets are the solution, then
what is the problem? We identify the shattered gradient
problem: a previously unnoticed difﬁculty with gradients
in deep rectiﬁer networks that is orthogonal to vanishing
and exploding gradients. The shattering gradients problem
is that, as depth increases, gradients in standard feedfor-
ward networks increasingly resemble white noise. Resnets
dramatically reduce the tendency of gradients to shatter.

Our analysis applies at initialization. Shattering should de-
crease during training. Understanding how shattering af-
fects training is an important open problem.

Terminology. We refer to networks without skip con-
nections as feedforward nets—in contrast to residual nets
(resnets) and highway nets. We distinguish between the
real-valued output of a rectiﬁer and its binary activation:
the activation is 1 if the output is positive and 0 otherwise.

1.1. The Shattered Gradients Problem

The ﬁrst step is to simply look at the gradients of neural net-
works. Gradients are averaged over minibatches, depend
on both the loss and the random sample from the data, and
are extremely high-dimensional, which introduces multiple
confounding factors and makes visualization difﬁcult (but
see section 4). We therefore construct a minimal model
designed to eliminate these confounding factors. The min-
imal model is a neural network fW : R → R taking scalars
to scalars; each hidden layer contains N = 200 rectiﬁer
neurons. The model is not intended to be applied to real
data. Rather, it is a laboratory where gradients can be iso-
lated and investigated.

We are interested in how the gradient varies, at initializa-

The Shattered Gradients Problem

e
s
i
o
N

s
t
n
e
i
d
a
r
G

s
e
c
i
r
t
a
m
e
c
n
a
i
r
a
v
o
C

(a) 1-layer feedforward. (b) 24-layer feedforward.

(c) 50-layer resnet.

(d) Brown noise.

(e) White noise.

Figure 1: Comparison between noise and gradients of rectiﬁer nets with 200 neurons per hidden layer. Columns
d–e: brown and white noise. Columns a–c: Gradients of neural nets plotted for inputs taken from a uniform grid. The
24-layer net uses mean-centering. The 50-layer net uses batch normalization with β = 0.1, see Eq. (2).

tion, as a function of the input:

dfW
dx

(x(i)) where x(i) ∈ [−2, 2] is in a

(1)

1-dim grid of M = 256 “data points”.

= ∂fW
∂nj

Updates during training depend on derivatives with respect
to weights, not inputs. Our results are relevant because, by
∂nj
the chain rule, ∂fW
. Weight updates thus de-
∂wij
∂wij
pend on ∂fW
—i.e. how the output of the network varies
∂nj
with the output of neurons in one layer (which are just in-
puts to the next layer).
The top row of ﬁgure 1 plots dfW
dx (x(i)) for each point x(i)
in the 1-dim grid. The bottom row shows the (absolute
|(g − ¯g)(g − ¯g)(cid:62)|/σ2
value) of the covariance matrix:
g
where g is the 256-vector of gradients, ¯g the mean, and σ2
g
the variance.

If all the neurons were linear then the gradient would be
a horizontal line (i.e. the gradient would be constant as a
function of x). Rectiﬁers are not smooth, so the gradients
are discontinuous.

Gradients of shallow networks resemble brown noise.
Suppose the network has a single hidden layer: fw,b(x) =
w(cid:62)ρ(x · v − b). Following Glorot & Bengio (2010),
weights w and biases b are sampled from N (0, σ2) with
σ2 = 1

N . Set v = (1, . . . , 1).

Figure 1a shows the gradient of the network for inputs
x ∈ [−2, 2] and its covariance matrix. Figure 1d shows
a discrete approximation to brownian motion: BN (t) =
(cid:80)t
N ). The plots are strikingly
similar: both clearly exhibit spatial covariance structure.
The resemblance is not coincidental: section A1 applies

s=1 Ws where Ws ∼ N (0, 1

Donsker’s theorem to show the gradient converges to brow-
nian motion as N → ∞.

Gradients of deep networks resemble white noise. Fig-
ure 1b shows the gradient of a 24-layer fully-connected rec-
tiﬁer network. Figure 1e shows white noise given by sam-
ples Wk ∼ N (0, 1). Again, the plots are strikingly similar.

Since the inputs lie on a 1-dim grid, it makes sense to
compute the autocorrelation function (ACF) of the gradi-
ent. Figures 2a and 2d compare this function for feed-
forward networks of different depth with white and brown
noise. The ACF for shallow networks resembles the ACF
of brown noise. As the network gets deeper, the ACF
quickly comes to resemble that of white noise.

Theorem 1 explains this phenomenon. We show that corre-
lations between gradients decrease exponentially 1
2L with
depth in feedforward rectiﬁer networks.

Training is difﬁcult when gradients behave like white
noise. The shattered gradient problem is that the spatial
structure of gradients is progressively obliterated as neural
nets deepen. The problem is clearly visible when inputs
are taken from a one-dimensional grid, but is difﬁcult to
observe when inputs are randomly sampled from a high-
dimensional dataset.

Shattered gradients undermine the effectiveness of algo-
rithms that assume gradients at nearby points are sim-
ilar such as momentum-based and accelerated methods
(Sutskever et al., 2013; Balduzzi et al., 2017). If dfW
be-
dnj
haves like white noise, then a neuron’s effect on the output
of the network (whether increasing weights causes the net-
work to output more or less) becomes extremely unstable

2.01.51.00.50.00.51.01.52.0input0.50.00.51.01.52.0gradient2.01.51.00.50.00.51.01.52.0input0.200.150.100.050.000.050.100.150.20gradient2.01.51.00.50.00.51.01.52.0input0.51.01.52.02.53.03.54.04.5gradient2.01.51.00.50.00.51.01.52.00.80.60.40.20.00.20.42.01.51.00.50.00.51.01.52.032101234050100150200250050100150200250050100150200250050100150200250050100150200250050100150200250050100150200250050100150200250050100150200250050100150200250The Shattered Gradients Problem

(a) Feedforward nets.

(b) Resnets (β = 1.0).

(c) Resnets (β = 0.1).

(d) White and brown noise.

Figure 2: Autocorrelation Function (ACF). Comparison of the ACF between white and brown noise, and feedforward
and resnets of different depths. Average over 20 runs.

making learning difﬁcult.

Gradients of deep resnets lie in between brown and
white noise.
Introducing skip-connections allows much
deeper networks to be trained (Srivastava et al., 2015; He
et al., 2016b;a; Greff et al., 2017). Skip-connections signif-
icantly change the correlation structure of gradients. Fig-
ure 1c shows the concrete example of a 50-layer resnet
which has markedly more structure than the equivalent
feedforward net (ﬁgure 1b). Figure 2b shows the ACF of
resnets of different depths. Although the gradients become
progressively less structured, they do not whiten to the ex-
tent of the gradients in standard feedforward networks—
there are still correlations in the 50-layer resnet whereas in
the equivalent feedforward net, the gradients are indistin-
guishable from white noise. Figure 2c shows the dramatic
effect of recently proposed β-rescaling (Szegedy et al.,
2016):
the ACF of even the 50 layer network resemble
brown-noise.

Theorem 3 shows that correlations between gradients decay
sublinearly with depth 1√
for resnets with batch normal-
L
ization. We also show, corollary 1, that modiﬁed highway
networks (where the gates are scalars) can achieve a depth
independent correlation structure on gradients. The analy-
sis explains why skip-connections, combined with suitable
rescaling, preserve the structure of gradients.

1.2. Outline

Section 2 shows that batch normalization increases neural
efﬁciency. We explore how batch normalization behaves
differently in feedforward and resnets, and draw out facts
that are relevant to the main results.

The main results are in section 3. They explain why gra-
dients shatter and how skip-connections reduce shatter-
ing. The proofs are for a mathematically amenable model:
fully-connected rectiﬁer networks with the same number of
hidden neurons in each layer. Section 4 presents empirical
results which show gradients similarly shatter in convnets
for real data. It also shows that shattering causes average

gradients over minibatches to decrease with depth (relative
to the average variance of gradients).

Finally, section 5 proposes the LL-init (“looks linear initial-
ization”) which eliminates shattering. Preliminary experi-
ments show the LL-init allows training of extremely deep
networks (∼200 layers) without skip-connections.

1.3. Related work

Carefully initializing neural networks has led to a series
of performance breakthroughs dating back (at least) to the
unsupervised pretraining in Hinton et al. (2006); Bengio
et al. (2006). The insight of Glorot & Bengio (2010) is
that controlling the variance of the distributions from which
weights are sampled allows to control how layers progres-
sively amplify or dampen the variance of activations and
error signals. More recently, He et al. (2015) reﬁned the
approach to take rectiﬁers into account. Rectiﬁers effec-
tively halve the variance since, at initialization and on av-
erage, they are active for half their inputs. Orthogonalizing
weight matrices can yield further improvements albeit at a
computational cost (Saxe et al., 2014; Mishkin & Matas,
2016). The observation that the norms of weights form a
random walk was used by Sussillo & Abbott (2015) to tune
the gains of neurons.

In short, it has proven useful to treat weights and gradients
as random variables, and carefully examine their effect on
the variance of the signals propagated through the network.
This paper presents a more detailed analysis that considers
correlations between gradients at different datapoints.

The closest work to ours is Veit et al. (2016), which shows
resnets behave like ensembles of shallow networks. We
provide a more detailed analysis of the effect of skip-
connections on gradients. A recent paper showed resnets
have universal ﬁnite-sample expressivity and may lack spu-
rious local optima (Hardt & Ma, 2017) but does not explain
why deep feedforward nets are harder to train than resnets.
An interesting hypothesis is that skip-connections improve
performance by breaking symmetries (Orhan, 2017).

51015Lag00.51Autocorrelation24102450Depth51015Lag00.51Autocorrelation51015Lag00.51Autocorrelation51015Lag00.51AutocorrelationBrown noiseWhite noiseThe Shattered Gradients Problem

The increased efﬁciency comes at a price. The raster plot
for feedforward networks resembles static television noise:
the spatial structure is obliterated. Resnets (Figure 4c) ex-
hibit a compromise where neurons are utilized efﬁciently
but the spatial structure is also somewhat preserved. The
preservation of spatial structure is quantiﬁed via the conti-
guity histograms which counts long runs of consistent acti-
vation. Resnets maintain a broad distribution of contiguity
even with deep networks whereas batch normalization on
feedforward nets shatters these into small sections.

3. Analysis

This section analyzes the correlation structure of gradients
in neural nets at initialization. The main ideas and results
are presented; the details provided in section A3.

Perhaps the simplest way to probe the structure of a ran-
dom process is to measure the ﬁrst few moments: the mean,
variance and covariance. We investigate how the correla-
tion between typical datapoints (deﬁned below) changes
with network structure and depth. Weaker correlations
correspond to whiter gradients. The analysis is for fully-
connected networks. Extending to convnets involves (sig-
niﬁcant) additional bookkeeping.

Proof strategy. The covariance deﬁnes an inner product
on the vector space of real-valued random variables with
mean zero and ﬁnite second moment. It was shown in Bal-
duzzi et al. (2015); Balduzzi (2016) that the gradients in
neural nets are sums of path-weights over active paths, see
section A3. The ﬁrst step is to observe that path-weights are
orthogonal with respect to the variance inner product. To
express gradients as linear combinations of path-weights is
thus to express them over an orthogonal basis.

Working in the path-weight basis reduces computing the
covariance between gradients at different datapoints to
counting the number of co-active paths through the net-
work. The second step is to count co-active paths and adjust
for rescaling factors (e.g. due to batch normalization).

The following assumption is crucial to the analysis:
Assumption 1 (typical datapoints). We say x(i) and x(j)
are typical datapoints if half of neurons per layer are active
for each and a quarter per layer are co-active for both. We
assume all pairs of datapoints are typical.

The assumption will not hold for every pair of datapoints.
Figure 3 shows the assumption holds, on average, under
batch normalization for both activations and coactivations.
The initialization in He et al. (2015) assumes datapoints
activate half the neurons per layer. The assumption on
co-activations is implied by (and so weaker than) the as-
sumption in Choromanska et al. (2015) that activations are
Bernoulli random variables independent of the inputs.

Figure 3: Activations and coactivations in feedforward
networks. Plots are averaged over 100 fully connected rec-
tiﬁer networks with 100 hidden units per layer. Without
BN: solid. With BN: dotted. Activations (green): Propor-
tion of inputs for which neurons in a given layer are active,
on average. Coactivations (blue): Proportion of distinct
pairs of inputs for which neurons are active, on average.

2. Observations on batch normalization

Batch normalization was introduced to reduce covariate
shift (Ioffe & Szegedy, 2015). However, it has other ef-
fects that are less well-known – and directly impact the
correlation structure of gradients. We investigate the effect
of batch normalization on neuronal activity at initialization
(i.e. when it mean-centers and rescales to unit variance).

We ﬁrst investigate batch normalization’s effect on neural
activations. Neurons are active for half their inputs on aver-
age, ﬁgure 3, with or without batch normalization. Figure 3
also shows how often neurons are co-active for two inputs.
With batch normalization, neurons are co-active for 1
4 of
distinct pairs of inputs, which is what would happen if acti-
vations were decided by unbiased coin ﬂips. Without batch
normalization, the co-active proportion climbs with depth,
suggesting neuronal responses are increasingly redundant.
Resnets with batch normalization behave the same as feed-
forward nets (not shown).

Figure 4 takes a closer look. It turns out that computing the
proportion of inputs causing neurons to be active on av-
erage is misleading. The distribution becomes increasingly
bimodal with depth. In particular, neurons are either always
active or always inactive for layer 50 in the feedforward net
without batch normalization (blue histogram in ﬁgure 4a).
Batch normalization causes most neurons to be active for
half the inputs, blue histograms in ﬁgures 4b,c.

Neurons that are always active may as well be linear. Neu-
rons that are always inactive may as well not exist. It fol-
lows that batch normalization increases the efﬁciency with
which rectiﬁer nonlinearities are utilized.

1020304050layer0.250.50proportionactivationscoactivationswithout batch normalizationwith batch normalizationThe Shattered Gradients Problem

2

h
t
p
e
D

0
1

h
t
p
e
D

0
5

h
t
p
e
D

(a) Feedforward net without batch norm.

(b) Feedforward with batch normalization.

(c) Resnet with batch normalization.

Raster-plots: Activations of hidden units (y-axis) for inputs
Figure 4: Activation of rectiﬁers in deep networks.
indexed by the x-axis. Left histogram (activation per unit): distribution of average activation levels per neuron. Right
histogram (contiguity): distribution of “contiguity” (length of contiguous sequences of 0 or 1) along rows in the raster plot.

Correlations between gradients. Weight updates in a
neural network are proportional to

a) The variance of the gradient at x(i) is Cfnn(i) = 1.

b) The covariance is Cfnn(i, j) = 1
2L .

∆wjk ∝

#mb
(cid:88)

P
(cid:88)

i=1

p=1

∂(cid:96)
∂fp

∂fp
∂nk

∂nk
∂wjk

(cid:0)x(i)(cid:1).

where fp is the pth coordinate of the output of the network
and nk is the output of the kth neuron. The derivatives ∂(cid:96)
∂fp
and ∂nk
do not depend on the network’s internal struc-
∂wjk
ture. We are interested in the middle term ∂fp
, which
∂nk
does. It is mathematically convenient to work with the sum
(cid:80)P
p=1 fp over output coordinates of the network. Section 4
shows that our results hold for convnets on real-data with
the cross-entropy loss. See also remark A2.
Deﬁnition 1. Let ∇i := (cid:80)P
∂fp
∂n (x(i)) be the deriva-
tive with respect to neuron n given input x(i) ∈ D. For
each input x(i), the derivative ∇i is a real-valued random
variable. It has mean zero since weights are sampled from
distributions with mean zero. Denote the covariance and
correlation of gradients by

p=1

C(i, j) = E[∇i ∇j] and R(i, j) =

(cid:113)

E[∇i ∇j]
E[∇2

i ] · E[∇2
j ]

,

where the expectations are w.r.t the distribution on weights.

3.1. Feedforward networks

Without loss of generality, pick a neuron n separated from
the output by L layers. The ﬁrst major result is
Theorem 1 (covariance of gradients in feedforward nets).
Suppose weights are initialized with variance σ2 = 2
N fol-
lowing He et al. (2015). Then

Part (a) recovers the observation in He et al. (2015) that
setting σ2 = 2
N preserves the variance across layers in rec-
tiﬁer networks. Part (b) is new. It explains the empirical
observation, ﬁgure 2a, that gradients in feedforward nets
whiten with depth.
Intuitively, gradients whiten because
the number of paths through the network grows exponen-
tially faster with depth than the fraction of co-active paths,
see section A3 for details.

3.2. Residual networks

The residual modules introduced in He et al. (2016a) are

xl = xl−1 + WlρBN

(cid:16)

(cid:17)
VlρBN (xl−1)

where ρBN (a) = ρ(BN (a)) and ρ(a) = max(0, a) is the
rectiﬁer. We analyse the stripped-down variant
xl = α · (cid:0)xl−1 + β · WlρBN (xl−1)(cid:1)

(2)

where α and β are rescaling factors. Dropping VlρBN
makes no essential difference to the analysis. The β-
rescaling was introduced in Szegedy et al. (2016) where
it was observed setting β ∈ [0.1, 0.3] reduces instability.
We include α for reasons of symmetry.

Theorem 2 (covariance of gradients in resnets). Consider
a resnet with batch normalization disabled and α = β =
1. Suppose σ2 = 2

N as above. Then

a) The variance of the gradient at x(i) is Cres(i) = 2L.

1Mhidden unitsinput0.00.51.0actvn/unit1Mcontiguity1Mhidden unitsinput0.00.51.0actvn/unit1Mcontiguity1Mhidden unitsinput0.00.51.0actvn/unit1McontiguityThe Shattered Gradients Problem

b) The covariance is Cres(i, j) = (cid:0) 3

(cid:1)L

.

2

The correlation is Rres(i, j) = (cid:0) 3

(cid:1)L

.

4

The theorem implies there are two problems in resnets
without batch normalization: (i) the variance of gradients
grows and (ii) their correlation decays exponentially with
depth. Both problems are visible empirically.

3.3. Rescaling in Resnets

A solution to the exploding variance of resnets is to rescale
layers by α = 1√
2

which yields

Cres
α=

√

2(i) = 1 and Rres

α=

√

2(i, j) =

(cid:19)L

(cid:18) 3
4

and so controls the variance but the correlation between
gradients still decays exponentially with depth. Both theo-
retical predictions hold empirically.

In practice, α-rescaling is not used. Instead, activations are
rescaled by batch normalization (Ioffe & Szegedy, 2015)
and, more recently, setting β ∈ [0.1, 0.3] per Szegedy et al.
(2016). The effect is dramatic:
Theorem 3 (covariance of gradients in resnets with BN and
rescaling). Under the assumptions above, for resnets with
batch normalization and β-rescaling,

a) the variance is Cres

β,BN(i) = β2(L − 1) + 1;

b) the covariance1 is Cres

β,BN(i, j) ∼ β

L; and

√

the correlation is Rres

β,BN(i, j) ∼ 1
√
β

L

.

The theorem explains the empirical observation, ﬁgure 2a,
that gradients in resnets whiten much more slowly with
depth than feedforward nets. It also explains why setting
β near zero further reduces whitening.

L

2L to 1√

Batch normalization changes the decay of the correlations
from 1
. Intuitively, the reason is that the variance
of the outputs of layers grows linearly, so batch normal-
ization rescales them by different amounts. Rescaling by
β introduces a constant factor. Concretely, the model pre-
dicts using batch normalization with β = 0.1 on a 100-
layer resnet gives typical correlation Rres
0.1,BN(i, j) = 0.7.
Setting β = 1.0 gives Rres
1.0,BN(i, j) = 0.1. By contrast, a
100-layer feedforward net has correlation indistinguishable
from zero.

3.4. Highway networks

Highway networks can be thought of as a generalization of
resnets, that were in fact introduced slightly earlier (Srivas-

1See section A3.4 for exact computations.

tava et al., 2015; Greff et al., 2017). The standard highway
network has layers of the form

xl = (cid:0)1 − T (xl−1)(cid:1) · xl−1 + T (xl−1) · H(xl−1)

where T (·) and H(·) are learned gates and features respec-
tively. Consider the following modiﬁcation where γ1 and
1 + γ2
γ2 are scalars satisfying γ2

2 = 1:

xl = γ1 · xl−1 + γ2 · Wlρ(xl−1)

The module can be recovered by judiciously choosing α
and β in equation (2). However, it is worth studying in its
own right:

Corollary 1 (covariance of gradients in highway net-
works). Under the assumptions above, for modiﬁed high-
way networks with γ-rescaling,

a) the variance of gradients is CHN

γ (i) = 1; and

b) the correlation is RHN

γ (i, j) = (cid:0)γ2

1 + 1

2 γ2

2

(cid:1)L

.

1 − 1
In particular, if γ1 =
correlation between gradients does not decay with depth

L and γ2 =

L then the

(cid:113)

(cid:113) 1

lim
L→∞

RHN

γ (i, j) =

1
√
e

.

The tradeoff is that the contributions of the layers becomes
increasingly trivial (i.e. close to the identity) as L → ∞.

4. Gradients shatter in convnets

In this section we provide empirical evidence that the main
results also hold for deep convnets using the CIFAR-10
dataset. We instantiate feedforward and resnets with 2,
4, 10, 24 and 50 layers of equivalent size. Using a slight
modiﬁcation of the “bottleneck” architecture in He et al.
(2016a), we introduce one skip-connection for every two
convolutional layers and both network architectures use
batch normalization.

Figures 5a and b compare the covariance of gradients in
the ﬁrst layer of feedforward and resnets (β = 0.1) with a
minibatch of 256 random samples from CIFAR-10 for net-
works of depth 2 and 50. To highlight the spatial structure
of the gradients, the indices of the minibatches were re-
ordered according to a k-means clustering (k = 10) applied
to the gradients of the two-layer networks. The same per-
mutation is used for all networks within a row. The spatial
structure is visible in both two-layer networks, although it
is more apparent in the resnet. In the feedforward network
the structure quickly disappears with depth. In the resnet,
the structure remains apparent at 50 layers.

The Shattered Gradients Problem

t
e
n
s
e
R

d
r
a
w
r
o
f
d
e
e
F

(a) Depth = 2

(b) Depth = 50

(c) Relative effective rank.

(d) Gradient norm.

Figure 5: Results on CIFAR-10. Figures a–b show the covariance matrices for a single minibatch for feedforward- and
resnets. Figures c–d show the relative effective rank and average norms of the gradients averaged over 30 minibatches.

To quantify this effect we consider the “whiteness” of the
gradient using relative effective rank. Let ∆ be the ma-
trix whose columns are the gradients with respect to the
input, for each datapoint x(i) in a minibatch. The effective
rank is r(∆) = tr(∆(cid:62)∆)/(cid:107)∆(cid:107)2
2 and measures the intrin-
sic dimension of a matrix (Vershynin, 2012). It is bounded
above by the rank of ∆—a matrix with highly correlated
columns and therefore more structure will have a lower ef-
fective rank. We are interested in the effective rank of the
covariance matrix of the gradients relative to a “white” ma-
trix Y of the same dimensions with i.i.d. Gaussian entries.
The relative effective rank r(∆)/r(Y) measures the simi-
larity between the second moments of ∆ and Y.

Figure 5c shows that the relative effective rank (averaged
over 30 minibatches) grows much faster as a function of
depth for networks without skip-connections. For resnets,
the parameter β slows down the rate of growth of the effec-
tive rank as predicted by theorem 3.

Figure 5d shows the average (cid:96)2-norm of the gradient in
each coordinate (normalized by the standard deviation
computed per minibatch). We observe that this quantity
decays much more rapidly as a function of depth for feed-
forward networks. This is due to the effect of averaging
increasingly whitening gradients within each minibatch.
In other words, the noise within minibatches overwhelms
the signal. The phenomenon is much less pronounced in
resnets.

Taken together these results conﬁrm the results in section 3
for networks with convolutional layers and show that the
gradients in resnets are indeed more structured than those
in feedforward nets and therefore do not vanish when av-
eraged within a minibatch. This phenomena allows for the
training of very deep resnets.

5. The “looks linear” initialization

Shattering gradients are not a problem for linear networks,
see remark after equation (1). Unfortunately, linear net-
works are not useful since they lack expressivity.

The LL-init combines the best of linear and rectiﬁer nets by
initializing rectiﬁers to look linear. Several implementa-
tions are possible; see Zagoruyko & Komodakis (2017) for
related architectures yielding good empirical results. We
use concatenated rectiﬁers or CReLUs (Shang et al., 2016):
(cid:18) ρ(x)
ρ(−x)

x (cid:55)→

(cid:19)

The key observation is that initializing weights with a mir-
rored block structure yields linear outputs

(cid:0)W −W(cid:1) ·

(cid:19)

(cid:18) ρ(x)
ρ(−x)

= Wρ(x) − Wρ(−x) = Wx.

The output will cease to be linear as soon as weight updates
cause the two blocks to diverge.

An alternative architecture is based on the PReLU intro-
duced in He et al. (2015):

PReLU: ρp(x) =

(cid:40)

if x > 0

x
ax else.

Setting a = 1 at initialization obtains a different kind of
LL-init. Preliminary experiments, not shown, suggest that
the LL-init is more effective on the CReLU-based architec-
ture than PReLU. The reason is unclear.

Orthogonal convolutions. A detailed analysis of learn-
ing in linear neural networks by Saxe et al. (2014) showed,
theoretically and experimentally, that arbitrarily deep linear
networks can be trained when initialized with orthogonal
weights. Motivated by these results, we use the LL-init in
conjunction with orthogonal weights.

1020304050Depth00.20.40.6Relative Effective RankFeedforward net=0.1=0.3=1.01020304050Depth0.060.120.180.24Gradient normThe Shattered Gradients Problem

y
c
a
r
u
c
c
A

0.85

0.75

0.65

0.55

0.45

0.35

0.25

CReLU w/ LL
Resnet
CReLU w/o LL
ReLU
Linear

6

14

30

54

102

198

Depth

Figure 6: CIFAR-10 test accuracy. Comparison of test ac-
curacy between networks of different depths with and with-
out LL initialization.

6. Conclusion

We brieﬂy describe how we orthogonally initialize a kernel
K of size A × B × 3 × 3 where A ≥ B. First, set all the
entries of K to zero. Second, sample a random matrix W
of size (A × B) with orthonormal columns. Finally, set
K[:, :, 2, 2] := W. The kernel is used in conjunction with
strides of one and zero-padding.

5.1. Experiments

We investigated the performance of the LL-init on very
deep networks, evaluated on CIFAR-10. The aim was not
to match the state-of-the-art, but rather to test the hypothe-
sis that shattered gradients adversely affect training in very
deep rectiﬁer nets. We therefore designed an experiment
where (concatenated) rectiﬁer nets are and are not shattered
at initialization. We ﬁnd that the LL-init allows to train sig-
niﬁcantly deeper nets, which conﬁrms the hypothesis.

We compared a CReLU architecture with an orthogonal
LL-init against an equivalent CReLU network, resnet, and a
standard feedforward ReLU network. The other networks
were initialized according to He et al. (2015). The archi-
tectures are thin with the number of ﬁlters per layer in the
ReLU networks ranging from 8 at the input layer to 64, see
section A4. Doubling with each spatial extent reduction.
The thinness of the architecture makes it particularly difﬁ-
cult for gradients to propagate at high depth. The reduction
is performed by convolutional layers with strides of 2, and
following the last reduction the representation is passed to
a fully connected layer with 10 neurons for classiﬁcation.
The numbers of ﬁlters per layer of the CReLU models were
2 to achieve parameter parity
adjusted by a factor of 1/
with the ReLU models. The Resnet version of the model is
the same as the basic ReLU model with skip-connections
after every two modules following He et al. (2016a).

√

Updates were performed with Adam (Kingma & Ba, 2015).
Training schedules were automatically determined by an
auto-scheduler that measures how quickly the loss on the
training set has been decreasing over the last ten epochs,
and drops the learning rate if a threshold remains crossed
for ﬁve measurements in a row. Standard data augmenta-
tion was performed; translating up to 4 pixels in any direc-
tion and ﬂipping horizontally with p = 0.5.

Results are shown in ﬁgure 6. Each point is the mean of
10 trained models. The ReLU and CReLU nets performed
steadily worse with depth; the ReLU net performing worse
than the linear baseline of 40% at the maximum depth of
198. The feedforward net with LL-init performs compa-
rably to a resnet, suggesting that shattered gradients are a
large part of the problem in training very deep networks.

The representational power of rectiﬁer networks depends
on the number of linear regions into which it splits the in-
put space. It was shown in Montufar et al. (2014) that the
number of linear regions can grow exponentially with depth
(but only polynomially with width). Hence deep neural
networks are capable of far richer mappings than shallow
ones (Telgarsky, 2016). An underappreciated consequence
of the exponential growth in linear regions is the prolifera-
tion of discontinuities in the gradients of rectiﬁer nets.

This paper has identiﬁed and analyzed a previously un-
noticed problem with gradients in deep networks:
in a
randomly initialized network, the gradients of deeper lay-
ers are increasingly uncorrelated. Shattered gradients play
havoc with the optimization methods currently in use2 and
may explain the difﬁculty in training deep feedforward
networks even when effective initialization and batch nor-
malization are employed. Averaging gradients over mini-
batches becomes analogous to integrating over white noise
– there is no clear trend that can be summarized in a single
average direction. Shattered gradients can also introduce
numerical instabilities, since small differences in the input
can lead to large differences in gradients.

Skip-connections in combination with suitable rescaling
reduce shattering. Speciﬁcally, we show that the rate at
which correlations between gradients decays changes from
exponential for feedforward architectures to sublinear for
resnets. The analysis uncovers a surprising and (to us at
least) unexpected side-effect of batch normalization. An
alternate solution to the shattering gradient problem is to
design initializations that do not shatter such as the LL-
init. An interesting future direction is to investigate hybrid
architectures combining the LL-init with skip connections.

2Note that even the choice of a step size in SGD typically re-
ﬂects an assumption about the correlation scale of the gradients.

The Shattered Gradients Problem

References

Balduzzi, David. Deep Online Convex Optimization with Gated

Games. In arXiv:1604.01952, 2016.

Balduzzi, David, Vanchinathan, Hastagiri,

and Buhmann,
Joachim. Kickback cuts Backprop’s red-tape: Biologically
plausible credit assignment in neural networks. In AAAI Con-
ference on Artiﬁcial Intelligence (AAAI), 2015.

Balduzzi, David, McWilliams, Brian, and Butler-Yeoman, Tony.
Neural Taylor Approximations: Convergence and Exploration
in Rectiﬁer Networks. In ICML, 2017.

Bengio, Y, Lamblin, P, Popovici, D., and Larochelle, H. Greedy

Layer-Wise Training of Deep Networks. In NIPS, 2006.

Bengio, Yoshua, Simard, P, and Frasconi, P. Learning long-term
IEEE Trans.

dependencies with gradient descent is difﬁcult.
Neur. Net., 5(2):157–166, 1994.

Choromanska, A, Henaff, M, Mathieu, M, Arous, G B, and Le-
Cun, Y. The loss surface of multilayer networks. In Journal of
Machine Learning Research: Workshop and Conference Pro-
ceeedings, volume 38 (AISTATS), 2015.

Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty
In AISTATS,

of training deep feedforward neural networks.
2010.

Greff, Klaus, Srivastava, Rupesh Kumar, and Schmidhuber, Juer-
gen. Highway and Residual Networks learn Unrolled Iterative
Estimation. In ICLR, 2017.

Hardt, Moritz and Ma, Tengyu. Identity Matters in Deep Learn-

ing. In ICLR, 2017.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
Delving Deep into Rectiﬁers: Surpassing Human-Level Per-
formance on ImageNet Classiﬁcation. In ICCV, 2015.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
In ECCV,

Identity Mappings in Deep Residual Networks.
2016a.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
In CVPR,

Deep Residual Learning for Image Recognition.
2016b.

Hinton, GE, Osindero, S, and Teh, Y W. A Fast Learning Al-
gorithm for Deep Belief Nets. Neural Computation, 18:1527–
1554, 2006.

Hochreiter, Sepp. Untersuchungen zu dynamischen neuronalen
Netzen. Master’s thesis, Technische Universit¨at M¨unchen,
1991.

Ioffe, Sergey and Szegedy, Christian. Batch normalization: Ac-
celerating deep network training by reducing internal covariate
shift. In ICML, 2015.

Kingma, Diederik P and Ba, Jimmy Lei. Adam: A method for

stochastic optimization. In ICLR, 2015.

Mishkin, D and Matas, J. All you need is a good init. In ICLR,

2016.

Montufar, Guido F, Pascanu, Razvan, Cho, Kyunghyun, and Ben-
gio, Yoshua. On the number of linear regions of deep neural
networks. In Advances in neural information processing sys-
tems, pp. 2924–2932, 2014.

Orhan, A Emin.

Skip Connections as Effective Symmetry-

Breaking. In arXiv:1701.09175, 2017.

Saxe, Andrew M, McClelland, James L, and Ganguli, Surya. Ex-
act solutions to the nonlinear dynamics of learning in deep lin-
ear neural networks. In ICLR, 2014.

Shang, Wenling, Sohn, Kihyuk, Almeida, Diogo, and Lee,
Honglak. Understanding and Improving Convolutional Neural
Networks via Concatenated Rectiﬁed Linear Units. In ICML,
2016.

Srivastava, Rupesh Kumar, Greff, Klaus, and Schmidhuber, Juer-

gen. Highway Networks. In arXiv:1505.00387, 2015.

Sussillo, David and Abbott, L F. Random Walk Initialization for
Training Very Deep Feedforward Networks. In ICLR, 2015.

Sutskever, Ilya, Martens, James, Dahl, George, and Hinton, Ge-
offrey. On the importance of initialization and momentum in
deep learning. In Proceedings of the 30th International Confer-
ence on Machine Learning (ICML-13), pp. 1139–1147, 2013.

Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre,
Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Van-
houcke, Vincent, and Rabinovich, Andrew. Going Deeper With
Convolutions. In CVPR, 2015.

Szegedy, Christian,

Ioffe, Sergey, and Vanhoucke, Vincent.
Inception-v4, Inception-ResNet and the Impact of Residual
Connections on Learning. In arXiv:1602.07261, 2016.

Telgarsky, Matus. Beneﬁts of depth in neural networks. In COLT,

2016.

Veit, Andreas, Wilber, Michael J, and Belongie, Serge. Residual
Networks Behave Like Ensembles of Relatively Shallow Net-
works. In NIPS, 2016.

Vershynin, Roman. Introduction to the non-asymptotic analysis of
random matrices. In Compressed sensing, pp. 210–268. Cam-
bridge Univ Press, 2012.

Zagoruyko, Sergey and Komodakis, Nikos. DiracNets: Train-
ing Very Deep Neural Networks Without Skip-Connections. In
arXiv:1706.00388, 2017.

APPENDIX

A1. Backprop and Brownian Motion

Brownian motion is a stochastic process {Bt : t ≥ 0} such
that

Krizhevsky, A, Sutskever, I, and Hinton, G E. Imagenet classiﬁ-
cation with deep convolutional neural networks. In Advances
in Neural Information Processing Systems (NIPS), 2012.

• B0 = 0

• (Bt2 − Bt1) ∼ N (0, t2 − t1) for any 0 ≤ t1 < t2.

The Shattered Gradients Problem

• (Bt2 − Bt1 ) and (Bt4 − Bt3 ) are independent for any

0 ≤ t1 < t2 ≤ t3 < t4.

Finally, we can write the derivative as a function of the frac-
tion t ∈ [0, 1] of neurons that are active:

• the sample function t (cid:55)→ Bt(ω) is continuous for al-

most all ω.

Some important properties of Brownian motion are that

• Bt ∼ N (0, t). In particular E[Bt] = 0 and Var[Bt] =

t.

• E[BtBs] = min(t, s) for any 0 ≤ s, t.

The following well known theorem shows how Brownian
motion arises as an inﬁnite limit of discrete random walks:

Theorem (Donsker). Let X1, . . . , be i.i.d. random vari-
ables with mean 0 and variance 1. Let SN = (cid:80)N
i=1 Xi.
Then the rescaled random walk

d
dx

fw,b(t) =

wi.

(cid:98)N t(cid:99)
(cid:88)

i=1

The result follows by Donsker’s theorem since the weights
are sampled from N (0, 1

N ).

A2. The Karhunen-Loeve theorem

Let {Xt : t ∈ [0, 1]} be a stochastic process with E[Xt] =
0 for all t. The covariance function is

K(s, t) = Cov(Xs, Xt) = E[XsXt].

Deﬁne the associated integral operator TK : L2(R) →
L2(R) as

B(N )

t =

S(cid:98)N t(cid:99)√
N

for

t ∈ [0, 1]

(A1)

TK(φ)(t) =

K(t, s)φ(t) ds

(cid:90) 1

0

converges weakly limn→∞ B(N ) = B to Brownian motion
Bt∈[0,1] on the interval [0, 1].

We are now in a position to demonstrate the connection
between the gradients and Brownian motion.

Proposition A1. Suppose weights are sampled from a dis-
tribution with mean zero and variance σ2 = 1
N per Glorot
& Bengio (2010). Then the derivative of fW,b, suitably
reparametrized, converges weakly to Brownian motion as
N → ∞.

Proof. The derivative of the neural net with respect to its
input is:

fw,b(x) =

wi.

(A2)

d
dx

(cid:88)

x>bi

If we vary x, then the (A2) is a random walk that jumps
at points sampled from a Gaussian.
In contrast, discrete
Brownian motion, (A1), jumps at uniformly spaced points
in the unit interval.

Relabel the neurons so the biases are ordered

b1 ≤ b2 ≤ · · · ≤ bN

without loss of generality. A rectiﬁer is active if its output
is nonzero. Let A(x) = {i : x > bi} denote the vec-
tor of hidden neurons that are active for input x. Ordering
the neurons by their bias terms means the derivative only
depends on |A(x)|, the number of active neurons:

d
dx

fw,b(x) =

wi.

|A(x)|
(cid:88)

i=1

If K(t, s) is continuous in t and s then, by Mercer’s theo-
rem, the operator TK has orthonormal basis of eigenvectors
ei(t) with associated eigenvalues λi.
Theorem (Karhunen-Loeve). Let

Then E[Fi] = 0, E[FiFj] = 0 for i (cid:54)= j, Var[Fi] = λi, and

Fi =

Xtei(t) dt

(cid:90) 1

0

Xt =

Fiei(t)

∞
(cid:88)

i=1

under uniform convergence in the mean with respect to t.

For example, the eigenvectors and eigenfunctions of Brow-
nian motion, with K(s, t) = min(s, t), are

√

(cid:18)

(cid:19)

ek(t) =

2 sin

(k −

)πt

and λk =

1
2

1
(k − 1
2 )2π2

.

A3. Details of the Analysis

Neural functional analysis. Functional analysis stud-
ies functions and families of functions in vector spaces
equipped with a topological structure such as a metric. A
fundamental tool is to expand a function in terms of an or-
thonormal basis f (x) = (cid:80)
k αkek(x) where the basis sat-
isﬁes (cid:104)ej(x), ek(x)(cid:105) = 1j=k. A classical example is the
Fourier expansion; a more recent example is wavelets.

A powerful tool for analyzing random processes based on
the same philosophy is the Karhunen-Loeve transform. The

The Shattered Gradients Problem

idea is to represent random processes as linear combina-
tions of orthogonal vectors or functions. For example, prin-
cipal component analysis is a special case of the Karhunen-
Loeve transform.

is

The weights of a neural network at initialization are random
variables. We can therefore model the output of the neural
network as a random process indexed by datapoints. That
is, for each x(i) ∈ D, the output fW(x(i)) is a random vari-
able. Similarly, the gradients of the neural network form a
random process indexed by the data.

The main technical insight underlying the analysis below
is that path-weights (Balduzzi et al., 2015; Balduzzi, 2016)
provide an orthogonal basis relative to the inner product on
random variables given by the covariance. The role played
by path-weights in the analysis of gradients is thus analo-
gous to the role of sin, cos and exp in Fourier analysis.

A3.1. Covariance structure of path-sums

Lemma A2 below shows that gradients are sums over prod-
ucts of weights, where the products are “zeroed out” if any
neuron along the path is inactive. In this section we develop
a minimal mathematical model of path-sums that allows us
to compute covariances and correlations between gradients
in rectiﬁer networks.

To keep things simple, the model is of a network of L lay-
ers each of which contains N neurons. Let W be a random
(N, N, L−1)-tensor with entries given by independent ran-
dom variables with mean zero and variance σ2. A path is a
sequence of numbers α = (α1, . . . αL) ∈ [N ]L. The path-
weight α is Wα := (cid:81)L−1
l=1 W[αl, αl+1, l], the product of
the weights along the path.

Path-weights are random variables. The expected weight of
a path is zero. Paths are uncorrelated unless they coincide
exactly:

E[Wα] = 0 and E[WαWβ] =

(cid:40)

σ2(L−1)
0

if α = β
else.

(A3)

Finally, the path-sum under conﬁguration A is the sum of
the weights of all active paths:

|A ∩ B| :=

Aα · Bα.

(cid:88)

α∈[N ]L

pW(A) =

Wα · Aα.

(cid:88)

α∈[N ]L

Lemma A1. Path-sums have mean zero, E[pW(A)] = 0,
and covariance

E[pW(A) · pW(B)] = |A ∩ B| · σ2(L−1).

A special case is the variance:
E[pW(A)2] = |A| · σ2(L−1).

Proof. The mean is zero since E[Wα] = 0. The cross-
terms E[Wα1 · Wβ] vanish for α (cid:54)= β by Eq. (A3), so the
covariance simpliﬁes as

E[pW(A)pW(B)] =

E[WαWβ] · AαBβ

(cid:88)

α,β∈[N ]L
(cid:88)

=

α∈[N ]L

E[W2

α] · AαBα

and the result follows.

A3.2. Gradients are path-sums

Consider a network of L + 1 layers number 0, 1, . . . L,
where each layer contains N neurons. Let

sL = xL,1 + · · · + xL,N

be the sum of the outputs of the neurons in the last layer.
Lemma A2. The derivative

∂sL
∂x0,i

=

(cid:88)

Wα · A(x)α.

α=(i,α1,...,αL)∈{i}×[N ]L

is the sum of the weights of all active paths from neuron i
to the last layer.

Remark A1. Equation (A3) implies that path-weights are
orthogonal under the inner product given by covariance.

Proof. Direct computation.

An activation conﬁguration A is a binary N × L-matrix.
Path α is active under conﬁguration A if all neurons along
the path are active, i.e. if Aα = (cid:81)L
l=1 A[αl, l] = 1, oth-
erwise the path is inactive. The number of active paths in
conﬁguration A is

|A| :=

(cid:88)

Aα.

α∈[N ]L

The number of co-active paths in conﬁgurations A and B

Remark A2. The setup of lemma A2 is quite speciﬁc. It
is chosen for mathematical convenience.
In particular,
the numerical coincidence that multiplying the number of
paths by the variance of the paths yields exactly one, when
weights are initialized according to (He et al., 2015), makes
the formulas easier on the eye.

The theorems are concerned with the large-scale behavior
of the variance and covariance as a function of the number
of layers (e.g. exponential versus sublinear decay). Their
broader implications—but not the precise quantities—are
robust to substantial changes to the setup.

The Shattered Gradients Problem

Proof of theorem 1.

Proof. Lemma A2 implies that gradients are sums over
path-weights.

a) By lemma A1 the gradient decomposes as a sum over
active paths. There are N L paths through the network.
If half the neurons per layer are active, then there are
|A(xi)| = ( N
2 )L active paths. Each path is a product of
L weights and so has covariance σ2L = ( 2
N )L. Thus, by
lemma A1

The number of co-active paths shared by conﬁgurations A
and B on the layers in F is

|A ∩ B|F =

A[αi, Fi] · B[αi, Fi].

(cid:88)

α∈[N ]|F |

Lemma A3. The covariance between two residual path-
sums is

E[rW(A) · rW(B)] =

|A ∩ B|F · σ2(|F |−1)

(cid:88)

F ⊂[L+1]

(cid:18) N
2

(cid:19)L

(cid:19)L

(cid:18) 2
N

·

L
(cid:89)

l=1

=

(1) = 1.

Proof. Direct computation.

A3.4. Residual gradients

b) The number of coactive neurons per layer is N
there are |A(xi) ∩ A(xj)| = ( N
lemma A1 the covariance is

4 and so
4 )L coactive paths. By

(cid:18) 1
2

N
2

(cid:19)L

(cid:19)L

(cid:18) 2
N

·

=

(cid:19)

L
(cid:89)

l=1

(cid:18) 1
2

=

1
2L

as required.

A3.3. Covariance structure of residual path-sums

Derivatives in resnets are path-sums as before. However,
skip-connections complicate their structure. We adapt the
minimal model in section A3.1 to resnets as follows.

A residual path is a pair ˜α = (F, α) where F ⊂ [L] and
α ∈ [N ]|F |. The subset F speciﬁes the layers that are
not skipped; α speciﬁes the neurons in those layers. The
length of the path is l( ˜α) = |F |. Let P res denote the set
of all residual paths. Let Fi denote the ith element of F ,
listed from smallest to largest. Given weight tensor W as
in section A3.1, the weight of path ˜α is

˜W ˜α =

(cid:40)
1
(cid:81)|F |−1

i=1 W[αi, αi+1, Fi]

if F = ∅
else.

Remark A3. We adopt the convention that products over
empty index sets equal one.

Path ˜α is active under conﬁguration A if ˜A ˜α = 1 where

˜A ˜α =

A[αi, Fi]

|F |
(cid:89)

i=1

The residual path-sum under conﬁguration A is

rW(A) =

˜W ˜α · ˜A ˜α.

(cid:88)

˜α∈P res

Restricting to F = [L] recovers the deﬁnitions for standard
feedforward networks in section A3.1.

Proof of theorem 2. The theorem is proved in the setting
of lemma A2, see remark A2 for justiﬁcation.

Proof. a) By lemma A3 the variance is

E[rW(A)2] = σ2 ·

(cid:19)|F |

(cid:18) N
2

σ2(|F |−1)

(cid:88)

F ⊂[L]
(cid:19)

=

(cid:18) 2
N

(cid:88)

·

F ⊂[L]

(cid:18) N
2

(cid:19)|F |

(cid:19)|F |−1

(cid:18) 2
N

·

=

L
(cid:88)

l=0

(cid:19)

(cid:18)L
l

= 2L

where the ﬁnal equality follows from the binomial theorem.

b) For the covariance we obtain

E[rW(A) · rW(B)] = σ2 ·

(cid:88)

(cid:19)|F |

σ2(|F |−1)

(cid:18) N
4

(cid:19)|F |

F ⊂[L]
(cid:18) 1
2

(cid:88)

F ⊂[L]

=

=

L
(cid:88)

l=0
(cid:18)

=

1 +

(cid:19)L

1
2

(cid:19)l

(cid:18)L
l

(cid:19) (cid:18) 1
2

by the binomial theorem.

A convenient way to intuit the computations is to think of
each layer as contributing (1+1) to the variance and (1+ 1
2 )
to the covariance:

Cres(i) =

(1 + 1) = 2L and

Cres(i, j) =

1 +

=

(cid:19)

1
2

(cid:19)L

(cid:18) 3
2

L
(cid:89)

l=1

L
(cid:89)

l=1

(cid:18)

Proof of theorem 3 when β = 1. The theorem is proved
in the setting of lemma A2, see remark A2 for justiﬁcation.

Since (cid:0)1 + 1

(cid:1) ∼

2l

1 + 1

2l−1

(cid:17)

, rewrite as

The Shattered Gradients Problem

Proof. a) Theorem 2 implies that each additional layer
(without batch normalization) doubles the contribution to
the variance of gradients, which we write schematically as
vl+1 = 2vl = 2l, the variance of the (l+1)st layer is double
the lth layer.

Batch normalization changes the schema to

(cid:16)

1
2l

L−1
(cid:89)

(cid:18)

1 +

(cid:19)2

2L−2
(cid:89)

(cid:18)

∼

1 +

= 2L − 1

(cid:19)

1
l

l=1

and so (cid:81)L−1
l=1

(cid:0)1 + 1

(cid:1) ∼

2l

l=1
√

L.

(cid:0)1 + 1
Numerically we ﬁnd (cid:81)L−1
l=1
a good approximation for large L.

2l

(cid:113) 4

(cid:1) ∼

π (L + 1) to be

vl+1 = vl + 1

Proof of theorem 3 for general β. The theorem is proved
in the setting of lemma A2, see remark A2 for justiﬁcation.

where vl is the variance of the previous layer and +1 is
added to account for additional variance generated by the
non-skip connection (which is renormalized to have unit-
variance). The variance of active path sums through (l + 1)
layers is therefore

Proof. a) The introduction of β-rescaling changes the
schema to

(cid:18)

vl+1 = vl

1 +

β2
β2(l − 1) + 1

(cid:19)

.

vl+1 = l + 1.

(A4)

The proof then follows from the observation that

Finally the variance of gradient Cres

BN (xi) = vL = L.

b) The above schema for batch normalization can be writ-
ten

vl+1 = vl +

= vl

1 +

vl
l

(cid:18)

(cid:19)

1
l

where the rescaling factor 1
previous layer per Eq. (A4). Unrolling yields

l is the expected variance of the

vL =

1 +

= L.

L−1
(cid:89)

(cid:18)

l=1

(cid:19)

1
l

L−1
(cid:89)

(cid:18)

1 +

l=1

β2
β2(l − 1) + 1

(cid:19)

= β2(L − 1) + 1.

b) The covariance is given by

L−1
(cid:89)

(cid:18)

1 +

l=1

1
2

β2
β2(l − 1) + 1

(cid:19)

√

∼ β

L

by similar working to when β = 1.

A3.5. Highway gradients

Proof of corollary 1. The theorem is proved in the setting
of lemma A2, see remark A2 for justiﬁcation.

Taking into account
normalization to the lth-layer rescales by 1√
module can be written in expectation as

the fact

that applying batch-
, the resnet

l

Proof. The variance is given by

L
(cid:89)

(cid:16)

(cid:17)

γ2
1 + γ2
2

= 1

xl+1 = xl + ρBN (Wl+1xl) = xl +

ρ(Wl+1xl)
√
l

.

l=1
and the covariance by

The contribution of each (non-skip) layer to the covariance
is half its contribution to the variance since we assume the
two inputs are co-active on a quarter of the neurons per
layer. The covariance is therefore given by

L−1
(cid:89)

(cid:18)

l=1

1 +

·

1
2

1
l

(cid:19)

√

∼

2L

as required.

To intuit the approximation, observe that

L−1
(cid:89)

(cid:18)

l=1

1 +

·

1 +

(cid:19)

(cid:18)

1
2l

1
2l − 1

(cid:19)

=

2L−2
(cid:89)

(cid:18)

(cid:19)

1
l

1 +

= 2L−1

l=1

L
(cid:89)

(cid:18)

l=1

(cid:19)

(cid:18)

γ1 +

γ2

=

γ2
1 +

1
2

(cid:19)L

1
2

γ2
2

by analogous working to the previous theorems.

Setting γ1 =

1 − 1

L and γ2 =

L obtains

(cid:113) 1

(cid:113)

CHN
γ

(xi, xj) =

1 −

(cid:19)L

1
L

+

1
L

1
2
(cid:19)L

1
2

1
L

(cid:18)

(cid:18)

=

1 −

L−→
∞

e− 1

2

by standard properties of the constant e.

A4. Details on architecture for ﬁgure 6

The Shattered Gradients Problem

r modules with 8 ﬁlters each
Downsampling module with 16 ﬁlters
r − 1 modules with 16 ﬁlters each
Downsampling module with 32 ﬁlters
r − 1 modules with 32 ﬁlters each
Downsampling module with 64 ﬁlters
r − 1 modules with 64 ﬁlters each
Downsampling module with 64 ﬁlters
Flattening layer
FC layer to output (width 10)

