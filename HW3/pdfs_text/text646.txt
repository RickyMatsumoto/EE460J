Unifying Task Speciﬁcation in Reinforcement Learning

Martha White 1

Abstract
Reinforcement learning tasks are typically speci-
ﬁed as Markov decision processes. This formal-
ism has been highly successful, though speciﬁca-
tions often couple the dynamics of the environ-
ment and the learning objective. This lack of mod-
ularity can complicate generalization of the task
speciﬁcation, as well as obfuscate connections
between different task settings, such as episodic
and continuing. In this work, we introduce the
RL task formalism, that provides a uniﬁcation
through simple constructs including a generaliza-
tion to transition-based discounting. Through a
series of examples, we demonstrate the generality
and utility of this formalism. Finally, we extend
standard learning constructs, including Bellman
operators, and extend some seminal theoretical
results, including approximation errors bounds.
Overall, we provide a well-understood and sound
formalism on which to build theoretical results
and simplify algorithm use and development.

1. Introduction

Reinforcement learning is a formalism for trial-and-error
interaction between an agent and an unknown environment.
This interaction is typically speciﬁed by a Markov decision
process (MDP), which contains a transition model, reward
model, and potentially discount parameters γ specifying a
discount on the sum of future values in the return. Domains
are typically separated into two cases: episodic problems
(ﬁnite horizon) and continuing problems (inﬁnite horizon).
In episodic problems, the agent reaches some terminal state,
and is teleported back to a start state. In continuing prob-
lems, the agent interaction is continual, with a discount to
ensure a ﬁnite total reward (e.g., constant γ < 1).

This formalism has a long and successful tradition, but is
limited in the problems that can be speciﬁed. Progressively
there have been additions to specify a broader range of ob-

1Department of Computer Science, Indiana University. Corre-

spondence to: Martha White <martha@indiana.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

jectives, including options (Sutton et al., 1999), state-based
discounting (Sutton, 1995; Sutton et al., 2011) and inter-
est functions (Reza and Sutton, 2010; Sutton et al., 2016).
These generalizations have particularly been driven by off-
policy learning and the introduction of general value func-
tions for Horde (Sutton et al., 2011; White, 2015), where
predictive knowledge can be encoded as more complex pre-
diction and control tasks. Generalizations to problem speciﬁ-
cations provide exciting learning opportunities, but can also
reduce clarity and complicate algorithm development and
theory. For example, options and general value functions
have signiﬁcant overlap, but because of different terminol-
ogy and formalization, the connections are not transparent.
Another example is the classic divide between episodic and
continuing problems, which typically require different con-
vergence proofs (Bertsekas and Tsitsiklis, 1996; Tsitsiklis
and Van Roy, 1997; Sutton et al., 2009) and different algo-
rithm speciﬁcations.

In this work, we propose a formalism for reinforcement
learning task speciﬁcation that uniﬁes many of these gener-
alizations. The focus of the formalism is to separate the spec-
iﬁcation of the dynamics of the environment and the speci-
ﬁcation of the objective within that environment. Though
natural, this represents a signiﬁcant change in the way tasks
are currently speciﬁed in reinforcement learning and has
important ramiﬁcations for simplifying implementation, al-
gorithm development and theory. The paper consists of two
main contributions. First, we demonstrate the utility of this
formalism by showing uniﬁcation of previous tasks speci-
ﬁed in reinforcement learning, including options, general
value functions and episodic and continuing, and further
providing case studies of utility. We demonstrate how to
specify episodic and continuing tasks with only modiﬁca-
tions to the discount function, without the addition of states
and modiﬁcations to the underlying Markov decision pro-
cess. This enables a uniﬁcation that signiﬁcantly simpliﬁes
implementation and easily generalizes theory to cover both
settings. Second, we prove novel contraction bounds on the
Bellman operator for these generalized RL tasks, and show
that previous bounds for both episodic and continuing tasks
are subsumed by this more general result. Overall, our goal
is to provide an RL task formalism that requires minimal
modiﬁcations to previous task speciﬁcation, with signiﬁcant
gains in simplicity and uniﬁcation across common settings.

Unifying Task Speciﬁcation in Reinforcement Learning

2. Generalized problem formulation

We assume the agent interacts with an environment for-
malized by a Markov decision process (MDP): (S, A, Pr)
where S is the set of states, n = |S|; A is the set of actions;
and Pr : S × A × S → [0, 1] is the transition probability
function where Pr(s, a, s(cid:48)) is the probability of transitioning
from state s into state s(cid:48) when taking action a. A reinforce-
ment learning task (RL task) is speciﬁed on top of these
transition dynamics, as the tuple (P, r, γ, i) where

1. P is a set of policies π : S × A → [0, 1];

2. the reward function r : S × A × S → R speciﬁes

reward received from (s, a, s(cid:48));

3. γ : S ×A×S → [0, 1] is a transition-based discount

function1;

4. i : S → [0, ∞) is an interest function that speciﬁes the

user deﬁned interest in a state.

Each task could have different reward functions within the
same environment. For example, in a navigation task within
an ofﬁce, one agent could have the goal to navigate to the
kitchen and the other the conference room. For a reinforce-
ment learning task, whether prediction or control, a set or
class of policies is typically considered. For prediction (pol-
icy evaluation), we often select one policy and evaluate its
long-term discounted reward. For control, where a policy is
learned, the set of policies may consist of all policies param-
eterized by weights that specify the action-value from states,
with the goal to ﬁnd the weights that yield the optimal pol-
icy. For either prediction or control in an RL task, we often
evaluate the return of a policy: the cumulative discounted
reward obtained from following that policy

Gt =

γ(st+j, at+j, st+1+j)

 Rt+1+i



∞
(cid:88)

i−1
(cid:89)





i=0

j=0

j=0 γ(st+j, at+j, st+1+j) = γi

where (cid:81)−1
j=0 γ(st+j, at+j, st+1+j) := 1. Note that this
subsumes the setting with a constant discount γc ∈ [0, 1),
by using γ(s, a, s(cid:48)) = γc for every s, a, s(cid:48) and so giving
(cid:81)i−1
c = 1
for i = 0. As another example, the end of the episode,
γ(s, a, s(cid:48)) = 0, making the product of these discounts
zero and so terminating the recursion. We further ex-
plain how transition-based discount enables speciﬁcation of
episodic tasks and discuss the utility of the generalization to
transition-based discounting throughout this paper. Finally,
the interest function i speciﬁes the degree of importance

c for i > 0 and γ0

1We describe a further probabilistic generalization in Appendix
A; much of the treatment remains the same, but the notation be-
comes cumbersome and the utility obfuscated.

of each state for the task. For example, if an agent is only
interested in learning an optimal policy for a subset of the
environment, the interest function could be set to one for
those states and to zero otherwise.

We ﬁrst explain the speciﬁcation and use of such tasks, and
then deﬁne a generalized Bellman operator and resulting
algorithmic extensions and approximation bounds.

2.1. Unifying episodic and continuing speciﬁcation

The RL task speciﬁcation enables episodic and continuing
problems to be easily encoded with only modiﬁcation to the
transition-based discount. Previous approaches, including
the absorbing state formulation (Sutton and Barto, 1998b)
and state-based discounting (Sutton, 1995; Reza and Sutton,
2010; Sutton et al., 2011)(van Hasselt, 2011, Section 2.1.1),
require special cases or modiﬁcations to the set of states
and underlying MDP, coupling task speciﬁcation and the
dynamics of the environment.

We demonstrate how transition-based discounting seam-
lessly enables episodic or continuing tasks to be speciﬁed in
an MDP via a simple chain world. Consider the chain world
with three states s1, s2 and s3 in Figure 1. The start state is
s1 and the two actions are right and left. The reward is -1 per
step, with termination occurring when taking action right
from state s3, which causes a transition back to state s1.
The discount is 1 for each step, unless speciﬁed otherwise.
The interest is set to 1 in all states, which is the typical case,
meaning performance from each state is equally important.

Figure 1a depicts the classical approach to specifying
episodic problems using an absorbing state, drawn as a
square. The agent reaches the goal—transitioning right from
state s3—then forever stays in the absorbing state, receiving
a reward of zero. This encapsulates the deﬁnition of the
return, but does not allow the agent to start another episode.
In practice, when this absorbing state is reached, the agent
is “teleported" to a start state to start another episode. This
episodic interaction can instead be represented the same way
as a continuing problem, by specifying a transition-based
discount γ(s3, right, s1) = 0. This deﬁnes the same return,
but now the agent simply transitions normally to a start state,
and no hypothetical states are added.

To further understand the equivalence, consider the updates
made by TD (see equation (3)). Assume linear function
approximation with feature function x : S → Rd, with
weights w ∈ Rd. When the agent takes action right from
s3, the agent transitions from s3 to s1 with probability one
and so γt+1 = γ(s3, right, s1) = 0. This correctly gives
δt = rt+1 + γt+1x(s1)(cid:62)w − x(s3)(cid:62)w = rt+1 − x(s3)(cid:62)w

and correctly clears the eligibility trace for the next step

et+1 = λt+1γt+1et + x(s1) = x(s1).

Unifying Task Speciﬁcation in Reinforcement Learning

The stationary distribution is also clearly equal to the origi-
nal episodic task, since the absorbing state is not used in the
computation of the stationary distribution.

Another strategy is to still introduce hypothetical states, but
use state-based γ, as discussed in Figure 1c. Unlike ab-
sorbing states, the agent does not stay indeﬁnitely in the
hypothetical state. When the agent goes right from s3, it
transitions to hypothetical state s4, and then transition deter-
ministically to the start state s1, with γs(s4) = 0. As before,
we get the correct update, because γt+1 = γs(s4) = 0. Be-
cause the stationary distribution has some non-zero probabil-
ity in the hypothetical state s4, we must set x(s4) = x(s1)
(or x(s4) = 0). Otherwise, the value of the hypothetical
state will be learned, wasting function approximation re-
sources and potentially modifying the approximation quality
of the value in other states. We could have tried state-based
discounting without adding an additional state s4. How-
ever, this leads to incorrect value estimates, as depicted in
Figure 1d; the relationship between transition-based and
state-based is further discussed in Appendix B.1. Overall,
to keep the speciﬁcation of the RL task and the MDP sepa-
rate, transition-based discounting is necessary to enable the
uniﬁed speciﬁcation of episodic and continuing tasks.

2.2. Options as RL tasks

The options framework (Sutton et al., 1999) generically
covers a wide range of settings, with discussion about macro-
actions, option models, interrupting options and intra-option
value learning. These concepts at the time merited their
own language, but with recent generalizations can be more
conveniently cast as RL subtasks.

Proposition 1. An option, deﬁned as the tuple (Sutton et al.,
1999, Section 2) (π, β, I) with policy π : S × A → [0, 1],
termination function β : S → [0, 1] and an initiation set
I ⊂ S from which the option can be run, can be equivalently
cast as an RL task.

s1

s2

s3

s4

1

(a) Absorbing state formulation.

s1

s2

s3

γ(s3, right, s1) = 0

(b) Transition-based termination, γ(s3, right, s1) = 0.

γs(s4) = 0

s1

s2

s3

s4

(c) State-based termination with γs(s4) = 0.

1

s2

s1

s3

γs(s1) = 0 or γs(s3) = 0

(d) Incorrect state-based termination.
Figure 1: Three different ways to represent episodic prob-
lems as continuing problems. For (c), the state-based dis-
count cannot represent the episodic chain problem with-
out adding states. To see why, consider the two cases
for representing termination: γs(s1) = 0 or γs(s3) = 0.
For simplicity, assume that π(s, right) = 0.75 for all
states s ∈ {s1, s2, s3} and transitions are deterministic. If
γs(s3) = 0, then the value for taking action right from s2 is
r(s2, right, s3) + γs(s3)vπ(s3) = −1 and the value for tak-
ing action right from s3 is r(s3, right, s1)+γs(s1)vπ(s1) (cid:54)=
−1, which are both incorrect. If γs(s1) = 0, then the value
of taking action right from s3 is −1 + γs(s1)vπ(s1) = −1,
which is correct. However, the value of taking action left
from s2 is −1 + γs(s1)vπ(s1) = −1, which is incorrect.

This proof is mainly deﬁnitional, but we state it as an
explicit proposition for clarity. The discount function
γ(s, a, s(cid:48)) = 1 − β(s(cid:48)) for all s, a, s(cid:48) speciﬁes termina-
tion. The interest function, i(s) = 1 if s ∈ I and i(s) = 0
otherwise, focuses learning resources on the states of inter-
est. If a value function for the policy is queried, it would
only make sense to query it from these states of interest.
If the policy for this option is optimized for this interest
function, the policy should only be run starting from s ∈ I,
as elsewhere will be poorly learned. The rewards for the RL
task correspond to the rewards associated with the MDP.

RL tasks generalize options, by generalizing termination
conditions to transition-based discounting and by providing
degrees of interest rather than binary interest. Further, the
policies associated with RL subtasks can be used as macro-

actions, to specify a semi-Markov decision process (Sutton
et al., 1999, Theorem 1).

2.3. General value functions

In a similar spirit of abstraction as options, general value
functions were introduced for single predictive or goal-
oriented questions about the world (Sutton et al., 2011).
The idea is to encode predictive knowledge in the form
of value function predictions: with a collection or horde
of prediction demons, this constitutes knowledge (Sutton
et al., 2011; Modayil et al., 2014; White, 2015). The work
on Horde (Sutton et al., 2011) and nexting (Modayil et al.,
2014) provide numerous examples of the utility of the types
of questions that can be speciﬁed by general value functions,
and so by RL tasks, because general value functions can

Unifying Task Speciﬁcation in Reinforcement Learning

naturally can be speciﬁed as an RL task.

The generalization to RL tasks provide additional bene-
ﬁts for predictive knowledge. The separation into under-
lying MDP dynamics and task speciﬁcation is particularly
useful in off-policy learning, with the Horde formalism,
where many demons (value functions) are learned off-policy.
These demons share the underlying dynamics, and even fea-
ture representation, but have separate prediction and control
tasks; keeping these separate from the MDP is key for avoid-
ing complications (see Appendix B.2). Transition-based
discounts, over state-based discounts, additionally enable
the prediction of a change, caused by transitioning between
states. Consider the taxi domain, described more fully in
Section 3, where the agent’s goal is to pick up and drop
off passengers in a grid world with walls. The taxi agent
may wish to predict the probability of hitting a wall, when
following a given policy. This can be encoded by setting
γ(s, a, s) = 0 if a movement action causes the agent to
remain in the same state, which occurs when trying to move
through a wall. In addition to episodic problems and hard ter-
mination, transition-based questions also enable soft termi-
nation for transitions. Hard termination uses γ(s, a, s(cid:48)) = 0
and soft termination γ(s, a, s(cid:48)) = (cid:15) for some small positive
value (cid:15). Soft terminations can be useful for incorporating
some of the value of a policy right after the soft termination.
If two policies are equivalent up to a transition, but have
very different returns after the transition, a soft termination
will reﬂect that difference. We empirically demonstrate the
utility of soft termination in the next section.

3. Demonstration in the taxi domain

To better ground this generalized formalism and provide
some intuition, we provide a demonstration of RL task spec-
iﬁcation. We explore different transition-based discounts in
the taxi domain (Dietterich, 2000; Diuk et al., 2008). The
goal of the agent is to take a passenger from a source plat-
form to a destination platform, depicted in Figure 2. The
agent receives a reward of -1 on each step, except for suc-
cessful pickup and drop-off, giving reward 0. We modify
the domain to include the orientation of the taxi, with ad-
ditional cost for not continuing in the current orientation.
This encodes that turning right, left or going backwards are
more costly than going forwards, with additional negative
rewards of -0.05, -0.1 and -0.2 respectively. This additional
cost is further multiplied by a factor of 2 when there is a
passenger in the vehicle. For grid size g and the number
of pickup/dropoff locations l, the full state information is
a 5-tuple: (x position of taxi ∈ {1, . . . , g}, y position of
taxi ∈ {1, . . . , g}, location of passenger ∈ {1, . . . , l + 1},
location of destination ∈ {1, . . . , l}, orientation of car
∈ {N, E, S, W } ). The location of the passenger can be in
one of the pickup/drop-off locations, or in the taxi. Optimal

policies and value functions are computed iteratively, with
an extensive number of iterations.

Figure 2 illustrates three policies for one part of the taxi do-
main, obtained with three different discount functions. The
optimal policy is learned using a soft-termination, which
takes into consideration the importance of approaching the
passenger location with the right orientation to minimize
turns after picking up the passenger. A suboptimal policy is
in fact learned with hard termination, as the policy prefers to
greedily minimize turns to get to the passenger. For further
details, refer to the caption in Figure 2.

We also compare to a constant γ, which corresponds to
an average reward goal, as demonstrated in Equation (8).
The table in Figure 2(e) summarizes the results. Though in
theory it should in fact recognize the relative values of orien-
tation before and after picking up a passenger, and obtain the
same solution as the soft-termination policy, in practice we
ﬁnd that numerical imprecision actually causes a suboptimal
policy to be learned. Because most of the rewards are nega-
tive per step, small differences in orientation can be more
difﬁcult to distinguish amongst for an inﬁnite discounted
sum. This result actually suggests that having multiple sub-
goals, as one might have with RL subtasks, could enable
better chaining of decisions and local evaluation of the op-
timal action. The utility of learning with a smaller γc has
been previously described (Jiang et al., 2015), however, here
we further advocate that enabling γ that provides subtasks
is another strategy to improve learning.

4. Objectives and algorithms

With an intuition for the speciﬁcation of problems as RL
tasks, we now turn to generalizing some key algorithmic
concepts to enable learning for RL tasks. We ﬁrst generalize
the deﬁnition of the Bellman operator for the value function.
For a policy π : S × A → [0, 1], deﬁne Pπ, Pπ,γ ∈ Rn×n
and rπ, vπ ∈ Rn, indexed by states s, s(cid:48) ∈ S,

Pπ(s, s(cid:48)) :=

π(s, a)Pr(s, a, s(cid:48))

(cid:88)

a∈A

(cid:88)

a∈A

(cid:88)

a∈A

(cid:88)

s(cid:48)∈S

(cid:88)

s(cid:48)∈S

Pπ,γ(s, s(cid:48)) :=

π(s, a)Pr(s, a, s(cid:48))γ(s, a, s(cid:48))

rπ(s) :=

π(s, a)

Pr(s, a, s(cid:48))r(s, a, s(cid:48))

vπ(s) := rπ(s) +

Pπ,γ(s, s(cid:48))vπ(s(cid:48)).

where vπ(s) is the expected return, starting from a state
s ∈ S. To compute a value function that satisﬁes this recur-
sion, we deﬁne a Bellman operator. The Bellman operator
has been generalized to include state-based discounting and
a state-based trace parameter2 (Sutton et al., 2016, Eq. 29).

2A generalization to state-based trace parameters has been
considered (Sutton, 1995; Sutton and Barto, 1998b; Reza and

Unifying Task Speciﬁcation in Reinforcement Learning

(e)

PICKUP

TOTAL
AND DROPOFF
TRANS-SOFT 7.74 ± 0.03
TRANS-HARD 7.73 ± 0.03
STATE-BASED 0.00 ± 0.00
γc = 0.1 0.00 ± 0.00
γc = 0.3 0.02 ± 0.01
γc = 0.5 0.04 ± 0.01
γc = 0.6 0.03 ± 0.01
γc = 0.7 7.12 ± 0.03
γc = 0.8 7.34 ± 0.03
γc = 0.9 3.52 ± 0.06
γc = 0.99 0.01 ± 0.01

ADDED COST
FOR TURNS
5.54 ± 0.01
5.83 ± 0.01
18.8 ± 0.02
2.48 ± 0.01
2.49 ± 0.01
2.51 ± 0.01
2.49 ± 0.01
4.52 ± 0.01
4.62 ± 0.01
4.57 ± 0.02
2.45 ± 0.01

Figure 2: (a) The taxi domain, where the pickup/drop-off platforms are at (0,0), (0,4), (3,0) and (4,4). The Passenger P is at the source
platform (4,4), outlined in black. The Car starts in (2,3), with orientation E as indicated the arrow, needs to bring the passenger to
destination D platform at (3,0), outlined in blue. In (b) - (d), there are simulated trajectories for policies learned using hard and soft
termination.
(b) The optimal strategy, with γ(Car in source, Pickup, P in Car) = 0.1 and a discount 0.99 elsewhere. The sequence of taxi locations are
(3, 3), (3, 4), (4, 4), (4, 4) with Pickup action, (4, 3), (4, 2), (4, 1), (4, 0), (3, 0). Successful pickup and drop-off with total reward −7.7.
(c) For γ(Car in source, Pickup, P in Car) = 0, the agent does not learn the optimal strategy. The agent minimizes orientation cost to
the subgoal, not accounting for orientation after picking up the passenger. Consequently, it takes more left turns after pickup, resulting
in more total negative reward. The sequence of locations are (3, 3), (4, 3), (4, 4), (4, 4) with Pickup action, (3, 4), (3, 3), (3, 2), (3, 1),
(3, 0). Successful pickup and drop-off with total reward −8.
(d) For state-based γ(Car in source and P in Car) = 0, the agent remains around the source and does not complete a successful drop-off.
The sequence of locations are (3, 3), (4, 3), (4, 4), (4, 4) with Pickup action, (4, 3), (4, 4), (4, 3).... The agent enters the source and
pickups up the passenger. When it leaves to location (4,3), its value function indicates better value going to (4,4) because the negative
return will again be cutoff by γ(Car in source and P in Car) = 0, even without actually performing a pickup. Since the cost to get to the
destination is higher than the −2.6 return received from going back to (4, 4), the agent stays around (4, 4) indeﬁnitely.
(e) Number of successful passenger pickup and dropoff, as well as additional cost incurred from turns, over 100 steps, with 5000 runs,
reported for a range of constant γc and the policies in Figure 2. Due to numerical imprecision, several constant discounts do not get close
enough to the passenger to pickup or drop-off. The state-based approach, that does not add additional states for termination, oscillates
after picking up the passenger, and so constantly gets negative reward.

We further generalize the deﬁnition to the transition-based
setting. The trace parameter λ : S × A × S → [0, 1] in-
ﬂuences the ﬁxed point and provides a modiﬁed (biased)
return, called the λ-return; this parameter is typically mo-
tivated as a bias-variance trade-off parameter (Kearns and
Singh, 2000). Because the focus of this work is on gen-
eralizing the discount, we opt for a simple constant λc in
the main body of the text; we provide generalizations to
transition-based trace parameters in the appendix.
The generalized Bellman operator T(λ) : Rn → Rn is
π + Pλ

T(λ)v := rλ

∀v ∈ Rn

πv,

(1)

π := (I − λcPπ,γ)−1 Pπ,γ(1 − λc)
where Pλ
π := (I − λcPπ,γ)−1 rπ
rλ

(2)

To see why this is the deﬁnition of the Bellman operator,
we deﬁne the expected λ-return, vπ,λ ∈ Rn for a given
approximate value function, given by a vector v ∈ Rn.

vπ,λ(s) := rπ(s)+

Pπ,γ(s, s(cid:48)) [(1−λc)v(s(cid:48))+λcvπ,λ(s(cid:48))]

(cid:88)

s(cid:48)∈S

= rπ(s) + (1 − λc)Pπ,γ(s, :)v + λcPπ,γ(s, :)vπ,λ.

Sutton, 2010; Sutton et al., 2014; Yu, 2012).

Continuing the recursion, we obtain3

vπ,λ =

(λcPπ,γ)i

(rπ + (1 − λc)Pπ,γv)

(cid:35)

(cid:34) ∞
(cid:88)

i=0

= (I − λcPπ,γ)−1 (rπ + (1 − λc)Pπ,γv) = T(λ)v

The ﬁxed point for this formula satisﬁes T(λ)v = v for the
Bellman operator deﬁned in Equation (1).

To see how this generalized Bellman operator modiﬁes the
algorithms, we consider the extension to temporal differ-
ence algorithms. Many algorithms can be easily gener-
alized by replacing γc or γs(st+1) with transition-based
γ(st, at, st+1). For example, the TD algorithm is gen-
eralized by setting the discount on each step to γt+1 =
γ(st, at, st+1),

wt+1 = wt + αtδtet

(cid:46) for some step-size αt

δt := rt+1 + γt+1x(st+1)(cid:62)w − x(st)(cid:62)w
et = γtλcet−1 + x(st).

(3)

3For a matrix M with maximum eigenvalue less than 1,
(cid:80)∞
i=0 Mi = (I − M)−1. We show in Lemma 3 that Pπ,γ satis-
ﬁes this condition, implying λcPπ,γ satisﬁes this condition and so
this inﬁnite sum is well-deﬁned.

012340123434343434-1.1-1.2-1-1-1-1.2-1.2-1.4-1-1.2-1-1-1.4-1-1.2-1.4Car(a)(b)(c)(d)Unifying Task Speciﬁcation in Reinforcement Learning

The generalized TD ﬁxed-point, under linear function ap-
proximation, can be expressed as a linear system Aw = b

A = X(cid:62)D(I − λcPπ,γ)−1(I − Pπ,γ)X
b = X(cid:62)D(I − λcPπ,γ)−1rπ

where each row in X ∈ Rn×d corresponds to features for
a state, and D ∈ Rn×n is a diagonal weighting matrix.
Typically, D = diag(dµ), where dµ ∈ Rn is the stationary
distribution for the behavior policy µ : S × A → [0, 1]
generating the stream of interaction. In on-policy learning,
dµ = dπ. With the addition of the interest function, this
weighting changes to D = diag(dµ ◦ i), where ◦ denotes
element-wise product (Hadamard product). More recently,
a new algorithm, emphatic TD (ETD) (Mahmood et al.,
2015; Sutton et al., 2016) speciﬁed yet another weighting,
D = M where M = diag(m) with m = (I − Pλ
π)−1(dµ ◦
i).
Importantly, even for off-policy sampling, with this
weighting, A is guaranteed to be positive deﬁnite. We show
in the next section that the generalized Bellman operator for
both the on-policy and emphasis weighting is a contraction,
and further in the appendix that the emphasis weighting
with a transition-based trace function is also a contraction.

5. Generalized theoretical properties

In this section, we provide a general approach to incorporat-
ing transition-based discounting into approximation bounds.
Most previous bounds have assumed a constant discount.
For example, ETD was introduced with state-based γs; how-
ever, (Hallak et al., 2015) analyzed approximation error
bounds of ETD using a constant discount γc. By using ma-
trix norms on Pπ,γ, we generalize previous approximation
bounds to both the episodic and continuing case.

Deﬁne the set of bounded vectors for the general space
of value functions V = {v ∈ Rn : (cid:107)v(cid:107)Dµ < ∞}. Let
Fv ⊂ V be a subspace of possible solutions, e.g., Fv =
{Xw|w ∈ Rd, (cid:107)w(cid:107)2 < ∞}.

A1. The action space A and state space S are ﬁnite.

A2. For polices µ, π : S × A → [0, 1], there exist unique
invariant distributions dµ, dπ such that dπPπ = dπ
and dµPµ = dµ. This assumption is typically satisﬁed
by assuming an ergodic Markov chain for the policy.

A3. There exist transition s, a, s(cid:48) such that γ(s, a, s(cid:48)) < 1
and π(s, a)P (s, a, s(cid:48)) > 0. This assumptions states
that the policy reaches some part of the space where
the discount is less than 1.

A4. Assume for any v ∈ Fv, if v(s) = 0 for all s ∈ S
where i(s) > 0, then v(s) = 0 for all s ∈ S s.t. i(s) =
0. For linear function approximation, this requires
F = span{x(s) : s ∈ S, i(s) (cid:54)= 0}.

√

πD1/2(cid:13)

v(cid:62)Dv, if we can take the
For weighted norm (cid:107)v(cid:107)D =
square root of D, the induced matrix norm is (cid:107)Pλ
π(cid:107)D =
(cid:13)
(cid:13)D1/2Pλ
(cid:13)sp, where the spectral norm (cid:107)·(cid:107)sp is the
largest singular value of the matrix. For simplicity of nota-
tion below, deﬁne sD := (cid:107)Pλ
π(cid:107)D. For any diagonalizable,
nonnegative matrix D, the projection ΠD : V → Fv onto
Fv exists and is deﬁned ΠDz = argminv∈Fv (cid:107)z − v(cid:107)D.

5.1. Approximation bound

We ﬁrst prove that the generalized Bellman operator in
Equation 1 is a contraction. We extend the bound from
(Tsitsiklis and Van Roy, 1997; Hallak et al., 2015) for con-
stant discount and constant trace parameter, to the general
transition-based setting. The normed difference to the true
value function could be deﬁned by multiple weightings. A
well-known result is that for D = Dπ the Bellman opera-
tor is a contraction for constant γc and λc (Tsitsiklis and
Van Roy, 1997); recently, this has been generalized for a
variant of ETD to M, still with constant parameters (Hallak
et al., 2015). We extend this result for transition-based γ for
both Dπ and the transition-based emphasis matrix M.
Lemma 1. For D = Dπ or D = M,

sD = (cid:107)Pλ

π(cid:107)D < 1.

Proof: For D = M: let ξ ∈ Rn be the vector of row sums
for Pλ

π1 = ξ. Then for any v ∈ V, with v (cid:54)= 0,

π: Pλ

(cid:107)Pλ

πv(cid:107)2

M =

m(s)

Pλ

π(s, s(cid:48))v(s(cid:48))

(cid:33)2

(cid:32)

(cid:88)

s(cid:48)∈S

(cid:88)

s∈S

(cid:88)

s∈S

(cid:88)

s∈S
(cid:88)

s(cid:48)∈S

=

≤

=

m(s)ξ(s)2

(cid:32)

Pλ

π(s, s(cid:48))
ξ(s)

(cid:88)

s(cid:48)∈S

(cid:33)2

v(s(cid:48))

m(s)ξ(s)2 (cid:88)

Pλ

π(s, s(cid:48))
ξ(s)

v(s(cid:48))2

s(cid:48)∈S
m(s)ξ(s)Pλ

π(s, s(cid:48))

v(s(cid:48))2 (cid:88)

s∈S

= v(cid:62) diag (cid:0)(m ◦ ξ)(cid:62)Pλ

(cid:1) v

π

where the ﬁrst inequality follows from Jensen’s inequality,
because Pλ
π(s, :) is normalized. Now because ξ has entries
that are less than 1, because the row sums of Pλ
π are less
than 1 as shown in Lemma 4, and because each of the values
in the above product are nonnegative,

π

π

(cid:1) v
(cid:1) v
π − I) + m(cid:62)(cid:1) v

v(cid:62) diag (cid:0)(m ◦ ξ)(cid:62)Pλ
≤ v(cid:62) diag (cid:0)m(cid:62)Pλ
= v(cid:62) diag (cid:0)m(cid:62)(Pλ
= v(cid:62) diag (cid:0)−(dπ ◦ i)(cid:62) + m(cid:62)(cid:1) v
= v(cid:62) diag (cid:0)m(cid:62)(cid:1) v − v(cid:62) diag (cid:0)(dπ ◦ i)(cid:62)(cid:1) v
< (cid:107)v(cid:107)2
M

s
(cid:88)

a
(cid:88)

s

a

(cid:32) ∞
(cid:88)

k=0

∞
(cid:88)

k=0

Unifying Task Speciﬁcation in Reinforcement Learning

The last inequality is a strict inequality because dπ ◦ i has
at least one positive entry where v has a positive entry.
Otherwise, if v(s) = 0 everywhere with i(s) > 0, then
v = 0, which we assumed was not the case.

π(cid:107)M := maxv∈Rn,v(cid:54)=0

Therefore, (cid:107)Pλ
πv(cid:107)M < (cid:107)v(cid:107)M for any v (cid:54)= 0, giving
(cid:107)Pλ
< 1. This exact same
proof follows through verbatim for the generalization of Pλ
π
to transition-based trace λ.

πv(cid:107)M

(cid:107)v(cid:107)M

(cid:107)Pλ

For D = Dπ: Again, we use Jensen’s inequality, but now
rely on the property dπPπ = dπ. Because of Assumption
A3, for some s < 1, for any non-negative v+,
(cid:88)

(cid:88)

dπ(s)Pr(s, a, :) ◦ γ(s, a, :)v+

dπPπ,γv+ =

where the last inequality follows from Lemma 1. By the
Banach Fixed Point theorem, because the Bellman operator
(cid:4)
is a contraction under D, it has a unique ﬁxed point.

Theorem 1. If D satisﬁes sD < 1, then there exists v ∈ Fv
such that ΠDT(λ)v = v, and the error to the true value
function is bounded as

(cid:107)v − v∗(cid:107)D ≤ (1 − sD)−1(cid:107)ΠDv∗ − v∗(cid:107)D.

(4)

For constant discount γc ∈ [0, 1) and constant trace param-
eter λc ∈ [0, 1], this bound reduces to the original bound
(Tsitsiklis and Van Roy, 1997, Lemma 6):

(1 − sD)−1 ≤

1 − γcλc
1 − γc

.

≤ s

dπ(s)Pr(s, a, :)v+ = sdπv.

Therefore, because vectors Pπ,γv+ are also non-negative,

dπPλ

πv+ = dπ

(Pπ,γλc)kPπ,γ(1 − λc)

v+

Proof: Let v be the unique ﬁxed point of ΠDT(λ), which
exists by Lemma 2.

(cid:33)

(cid:107)v − v∗(cid:107)D ≤ (cid:107)v − ΠDv∗(cid:107)D + (cid:107)ΠDv∗ − v∗(cid:107)D

≤ (1 − λc)

(sλc)kdπPπ,γv+

≤ (1 − λc)(1 − sλc)−1sdπv+

and so
(cid:107)Pλ

πv(cid:107)2

Dπ

≤

(cid:88)

dπ(s)ξ(s)2 (cid:88)

s∈S
(cid:88)

s(cid:48)∈S
(cid:88)

=

≤

v(s(cid:48))2 (cid:88)

s∈S

v(s(cid:48))2 (cid:88)

Pλ

π(s, s(cid:48))
ξ(s)

v(s(cid:48))2

s(cid:48)∈S
dπ(s)ξ(s)Pλ

π(s, s(cid:48))

dπ(s)Pλ

π(s, s(cid:48))

s(cid:48)∈S
≤ s(1−λc)
1−λcs

(cid:88)

s∈S
d(s(cid:48))v(s(cid:48))2

s(cid:48)∈S

≤ s−sλc

1−λcs (cid:107)v(cid:107)2
Dπ
1−λcs < 1 since s < 1.

where s−sλc
Lemma 2. Under assumptions A1-A3, the Bellman oper-
ator T(λ) in Equation (1) is a contraction under a norm
weighted by D = Dπ or D = Mπ, i.e., for v ∈ V

(cid:4)

(cid:107)T(λ)v(cid:107)D < (cid:107)v(cid:107)D.

Further, because the projection ΠD is a contraction,
ΠDT(λ) is also a contraction and has a unique ﬁxed point
ΠDT(λ)v = v for v ∈ Fv.

Proof: Because any vector v can be written v = v1 − v2,

(cid:107)T(λ)v(cid:107)D = (cid:107)T(λ)(v1 − v2)(cid:107)D = (cid:107)Pλ

π(v1 − v2)(cid:107)D

π(cid:107)D(cid:107)v(cid:107)D

≤ (cid:107)Pλ
< (cid:107)v(cid:107)D

= (cid:107)ΠT(λ)v − ΠDv∗(cid:107)D + (cid:107)ΠDv∗ − v∗(cid:107)D
≤ (cid:107)T(λ)v − v∗(cid:107)D + (cid:107)ΠDv∗ − v∗(cid:107)D
= (cid:107)T(λ)(v − v∗)(cid:107)D + (cid:107)ΠDv∗ − v∗(cid:107)D
= (cid:107)Pλ
π(v − v∗)(cid:107)D + (cid:107)ΠDv∗ − v∗(cid:107)D
π(cid:107)D(cid:107)v − v∗(cid:107)D + (cid:107)ΠDv∗ − v∗(cid:107)D
= (cid:107)Pλ
= sD(cid:107)v − v∗(cid:107)D + (cid:107)ΠDv∗ − v∗(cid:107)D

where the second inequality is due to (cid:107)ΠT(λ)v(cid:107)D ≤
(cid:107)T(λ)v(cid:107)D, the second equality due to T(λ)v∗ = v∗ and
the third equality due to T(λ)v − T(λ)v∗ = Pλ
π(v − v∗)
because the rπ cancels. By rearranging terms, we get

(1 − sD)(cid:107)v − v∗(cid:107)Dπ ≤ (cid:107)Πv∗ − v∗(cid:107)Dπ

and since sD < 1, we get the ﬁnal result.

For constant γc < 1 and λc, because Pπ,γ = γPπ

sD = (cid:107)Pλ

π(cid:107)D

= (cid:107)D1/2

cλi
γi

cPi
π

γc(1 − λc)PπD1/2(cid:107)2

(cid:33)

(cid:32) ∞
(cid:88)

i=0

≤ γc(1 − λc)

cλi
γi

c(cid:107)D1/2Pi+1

π D1/2(cid:107)2

∞
(cid:88)

i=0
∞
(cid:88)

i=0
∞
(cid:88)

i=0

= γc(1 − λc)

cλi
γi

c(cid:107)Pi+1

π (cid:107)D

≤ γc(1 − λc)

cλi
γi
c

=

γc(1 − λc)
1 − γcλc

(cid:4)

Unifying Task Speciﬁcation in Reinforcement Learning

We provide generalizations to transition-based trace param-
eters in the appendix, for the emphasis weighting, and also
discuss issues with generalizing to state-based termination
for a standard weighting with dπ. We show that for any
transition-based discounting function λ : S×A×S → [0, 1],
the above contraction results hold under emphasis weight-
ing. We then provide a general form for an upper bound
on (cid:107)Pλ
π(cid:107)Dπ for transition-based discounting, based on the
contraction properties of two matrices within Pλ
π. We fur-
ther provide an example where the Bellman operator is
not a contraction even under the simpler generalization to
state-based discounting, and discuss the requirements for
the transition-based generalizations to ensure a contraction
with weighting dπ. This further motivates the emphasis
weighting as a more ﬂexible scheme for convergence un-
der general setting—both off-policy and transition-based
generalization.

5.2. Properties of TD algorithms

Using this characterization of Pλ
π, we can re-examine pre-
vious results for temporal difference algorithms that either
used state-based or constant discounts.

Convergence of Emphatic TD for RL tasks. We can ex-
tend previous convergence results for ETD, for learning
value functions and action-value functions, for the RL task
formalism. For policy evaluation, ETD and ELSTD, the
least-squares version of ETD that uses the above deﬁned
A and b with D = M, have both been shown to converge
with probability one (Yu, 2015). As an important compo-
nent of this proof is convergence in expectation, which relies
on A being positive deﬁnite. In particular, for appropriate
step-sizes αt (see (Yu, 2015)), if A is positive deﬁnite, the
iterative update is convergent wt+1 = wt + αt(b − Awt).
For the generalization to transition-based discounting, con-
vergence in expectation extends for the emphatic algorithms.
We provide these details in the appendix for completeness,
with theorem statement and proof in Appendix F and pseu-
docode in Appendix D.

Convergence rate of LSTD(λ). Tagorti and Scherrer
(2015) recently provided convergence rates for LSTD(λ)
for continuing tasks, for some γc < 1. These results can be
extended to the episodic setting with the generic treatment
of Pλ
π. For example, in (Tagorti and Scherrer, 2015, Lemma
1), which describes the sensitivity of LSTD, the proof ex-
tends by replacing the matrix (1 − λc)γcPπ(I − λcγcPπ)−1
(which they call M in their proof) with the generalization
Pλ
in the bound
rather than 1−λcγc
. Further, this generalizes convergence
1−γc
rate results to emphatic LSTD, since M satisﬁes the re-
quired convergence properties, with rates dictated by sM
rather than sDµ for standard LSTD.

π, resulting instead in the constant

1
1−sD

Insights into sD. Though the generalized form enables
uniﬁed episodic and continuing results, the resulting bound
parameter sD is more difﬁcult to interpret than for constant
γc, λc. With λc increasing to one, the constant 1−γcλc
in
1−γc
the upper bound decreased to one. For γc decreasing to zero,
the bound also decreases to one. These trends are intuitive,
as the problem should be simpler when γc is small, and bias
should be less when λc is close to one. More generally,
however, the discount can be small or large for different
transitions, making it more difﬁcult to intuit the trend.

To gain some intuition for sD, consider a random policy in
the taxi domain, with sD summarized in Table 1. As λc
goes to one, sD goes to zero and so (1 − sD)−1 goes to one.
Some outcomes of note are that 1) hard or soft termination
for the pickup results in the exact same sD; 2) for a constant
gamma of γc = 0.99, the episodic discount had a slightly
smaller sD; and 3) increasing λc has a much stronger effect
than including more terminations. Whereas, when we added
random terminations, so that from 1% and 10% of the states,
termination occurred on at least one path within 5 steps or
even more aggressively on every path within 5 steps, the
values of sD were similar.

λc

0.0

0.5

0.99

γc = 0.99

EPISODIC TAXI

0.9
0.999
0.989 0.979 0.903 0.483 0.086
0.990 0.980 0.908 0.497 0.090
0.989 0.978 0.898 0.467 0.086
1% SINGLE PATH
10% SINGLE PATH 0.987 0.975 0.887 0.439 0.086
0.978 0.956 0.813 0.304 0.042
0.898 0.815 0.468 0.081 0.009

10% ALL PATHS

1% ALL PATHS

Table 1: The sD values for increasing λc, with discount
settings described in the text.

6. Discussion and conclusion

The goal of this paper is to provide intuition and examples
of how to use the RL task formalism. Consequently, to avoid
jarring the explanation, technical contributions were not em-
phasized, and in some cases included only in the appendix.
For this reason, we would like to highlight and summa-
rize the technical contributions, which include 1) the intro-
duction of the RL task formalism, and of transition-based
discounts; 2) an explicit characterization of the relation-
ship between state-based and transition-based discounting;
and 3) generalized approximation bounds, applying to both
episodic and continuing tasks; and 4) insights into—and
issues with—extending contraction results for both state-
based and transition-based discounting. Through intuition
from simple examples and fundamental theoretical exten-
sions, this work provides a relatively complete characteri-
zation of the RL task formalism, as a foundation for use in
practice and theory.

Unifying Task Speciﬁcation in Reinforcement Learning

Richard S Sutton, Hamid Maei Reza, Doina Precup, and
Shalab Bhatnagar. Fast gradient-descent methods for
temporal-difference learning with linear function approx-
imation. International Conference on Machine Learning,
2009.

Richard S Sutton, Joseph Modayil, Michael Delp, Thomas
Degris, Patrick M Pilarski, Adam White, and Doina Pre-
cup. Horde: A scalable real-time architecture for learning
knowledge from unsupervised sensorimotor interaction.
In International Conference on Autonomous Agents and
Multiagent Systems, 2011.

Richard S Sutton, Ashique Rupam Mahmood, Doina Precup,
and Hado van Hasselt. A new Q(lambda) with interim for-
ward view and Monte Carlo equivalence. In International
Conference on Machine Learning, 2014.

Richard S Sutton, Ashique Rupam Mahmood, and Martha
White. An emphatic approach to the problem of off-policy
temporal-difference learning. The Journal of Machine
Learning Research, 2016.

Manel Tagorti and Bruno Scherrer. On the Rate of Conver-
gence and Error Bounds for LSTD(λ). In International
Conference on Machine Learning, 2015.

Johnathan N Tsitsiklis and Benjamin Van Roy. An analysis
of temporal-difference learning with function approxima-
tion. IEEE Transactions on Automatic Control, 1997.

Hado Philip van Hasselt. Insights in Reinforcement Learn-

ing. PhD thesis, Hado van Hasselt, 2011.

Harm van Seijen and Rich Sutton. True online TD(lambda).
In International Conference on Machine Learning, 2014.

Adam White. Developing a predictive approach to knowl-

edge. PhD thesis, University of Alberta, 2015.

Huizhen Yu. Least Squares Temporal Difference Methods:
An Analysis under General Conditions. SIAM Journal on
Control and Optimization, 2012.

Huizhen Yu. On convergence of emphatic temporal-
difference learning. In Annual Conference on Learning
Theory, 2015.

Acknowledgements

Thanks to Hado van Hasselt for helpful discussions about
transition-based discounting, and probabilistic discounts.

References

Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic

programming. Athena Scientiﬁc Press, 1996.

Thomas G Dietterich. Hierarchical Reinforcement Learning
with the MAXQ Value Function Decomposition. Journal
of Artiﬁcial Intelligence Research, 2000.

Carlos Diuk, Andre Cohen, and Michael L Littman. An
object-oriented representation for efﬁcient reinforcement
learning. In International Conference on Machine Learn-
ing, 2008.

Assaf Hallak, Aviv Tamar, Rémi Munos, and Shie Mannor.
Generalized Emphatic Temporal Difference Learning:
Bias-Variance Analysis. CoRR abs/1509.05172, 2015.

Nan Jiang, Alex Kulesza, Satinder P Singh, and Richard L
Lewis. The Dependence of Effective Planning Horizon
on Model Accuracy. International Conference on Au-
tonomous Agents and Multiagent Systems, 2015.

Michael J Kearns and Satinder P Singh. Bias-Variance
error bounds for temporal difference updates. In Annual
Conference on Learning Theory, 2000.

Ashique Rupam Mahmood, Huizhen Yu, Martha White, and
Richard S Sutton. Emphatic temporal-difference learning.
In European Workshop on Reinforcement Learning, 2015.

Joseph Modayil, Adam White, and Richard S Sutton. Multi-
timescale nexting in a reinforcement learning robot.
Adaptive Behavior - Animals, Animats, Software Agents,
Robots, Adaptive Systems, 2014.

Hamid Maei Reza and Richard S Sutton. GQ (λ): A gen-
eral gradient algorithm for temporal-difference prediction
learning with eligibility traces. In AGI, 2010.

Richard S Sutton. TD models: Modeling the world at a
mixture of time scales. In International Conference on
Machine Learning, 1995.

Richard S Sutton and A G Barto. Introduction to reinforce-

ment learning. MIT Press, 1998a.

Richard S Sutton and A G Barto. Reinforcement Learning:

An Introduction. MIT press, 1998b.

Richard S Sutton, Doina Precup, and Satinder Singh. Be-
tween MDPs and semi-MDPs: A framework for temporal
abstraction in reinforcement learning. Artiﬁcial intelli-
gence, 1999.

Unifying Task Speciﬁcation in Reinforcement Learning

=

π(s, a)Pr(s, a, s(cid:48))E[r|s, a, s(cid:48)]

state-based discounting

(cid:88)

a,s(cid:48)
(cid:88)

a,s(cid:48)

+

(cid:88)

a,s(cid:48)

A. More general formulation with

probabilistic discounts

In the introduction of transition-based discounting, we could
have instead assumed that we had a more general probabil-
ity model: Pr(r, γ|s, a, s(cid:48)). Now, both the reward and dis-
count are not just functions of states and action, but also are
stochastic. This generalization in fact, does not much alter
the treatment in this paper. This is because, when taking the
expectations for value function, the Bellman operator and
the A matrix, we are left again with γ(s, a, s(cid:48)). To see why,

vπ(s) =

π(s, a)Pr(s, a, s(cid:48))E[r + γvπ(s(cid:48))|s, a, s(cid:48)]

π(s, a)Pr(s, a, s(cid:48))E[γ|s, a, s(cid:48)]vπ(s(cid:48))

= rπ(s) +

Pπ,γ(s, s(cid:48))vπ(s(cid:48))

(cid:88)

s(cid:48)

for γ(s, a, s(cid:48)) = E[γ|s, a, s(cid:48)].

B. Relationship between state-based and

transition-based discounting

In this section, we show that for any MDP with transition-
based discounting, we can construct an equivalent MDP
with state-based discounting. The MDPs are equivalent in
the sense that learned policies and value functions learned
in either MDP would have equal values when evaluated
on the states in the original transition-based MDP. This
equality ignores practicality of learning in the larger induced
state-based MDP, and at the end of this section, we discuss
advantages of the more compact transition-based MDP.

B.1. Equivalence result

The equivalence is obtained by introducing hypothetical
states for each transition. The key is then to prove that
the stationary distribution for the state-based MDP, with
additional hypothetical states, provides the same solution
even with function approximation. For each triplet s, a, s(cid:48),
add a new hypothetical state fsas(cid:48), with set F comprised of
these additional states. Each transition now goes through
a hypothetical state, fsas(cid:48), and allows the discount in the
hypothetical state to be set to γ(s, a, s(cid:48)). The induced state-
based MDP has state set ¯S = S ∪ F with | ¯S| = |A|n2 + n.
We deﬁne the other models in the proof in Appendix B.3.

The choice of action in the hypothetical states is irrelevant.
To extend the policy π, we arbitrarily choose that the policy
uniformly selects actions when in the hypothetical states
and deﬁne ¯π(s, a) = π(s, a) for s ∈ S and ¯π(s, a) = 1/|A|
otherwise. For linear function approximation, we also need

to assume x(fsas(cid:48)) = x(s(cid:48)) for fsas(cid:48) ∈ F.
Theorem 2. For a given transition-based MDP
(Pr, r, S, A, γ) and policy π, assume that the station-
ary distribution dπ exists. Deﬁne state-based MDP
(¯Pr, ¯r, ¯S, A, ¯γs) with extended ¯π, all as above. Then the
stationary distribution ¯dπ for ¯π exists and satisﬁes
¯dπ(s)

(cid:80)

i∈S

¯dπ(i) = dπ(s).

(5)

∀s ∈ S, ¯vπ(s) = vπ(s) and ¯π(s, a) = π(s, a) for all
s ∈ S, a ∈ A with π = argminπ
s∈S dπ(s)vπ(s); ¯π =
argminπ

¯dπ(s)¯vπ(s)

(cid:80)

(cid:80)

i∈ ¯S

B.2. Advantages of transition-based discounting over

Though the two have equal representational abilities, there
are several disadvantages of state-based discounting that
compound to make the more general transition-based dis-
count strictly more desirable. The disadvantages of using an
induced state-based MDP, rather than the original transition-
based MDP, arises from the addition of states and include
the following.

Compactness.
In the worst-case, for a transition-based
MDP with n states, the induced state-based MDP can have
|A|n2 + n states.

Problem deﬁnition changes for different discounts. For
the same underlying MDP, multiple learners with different
discount functions would have different induced state-based
MDPs. This complicates code and reduces opportunities for
sharing variables and computation.

Overhead. Additional states must be stored, with additional
algorithmic updates in those non-states, or cases to avoid
these updates, and the need to carefully set features for
hypothetical states. This overhead is both computational as
well as conceptual, as it complicates the code.

Stationary distribution. This distribution superﬂuously
includes hypothetical states and requires renormalization to
obtain the stationary distribution for the original transition-
based MDP.

Off-policy learning. In off-policy learning, one goal is to
learn many value functions with different discounts (White,
2015). As mentioned above, these learners may have differ-
ent induced state-based MDPs, which complicates imple-
mentation and even theory. To theoretically characterize a
set of off-policy learners, it would be necessary to consider
different induced state-based MDPs. Further, sharing infor-
mation, such as the features, is again complicated by using
induced state-based MDP rather than a single transition-
based MDP, with varying discount functions.

Speciﬁcation of algorithms. Often algorithms are intro-
duced either for the episodic case (e.g., true-online TD (van

Unifying Task Speciﬁcation in Reinforcement Learning

Seijen and Sutton, 2014)) or the continuing case (e.g., the
lower-variance version of ETD (Hallak et al., 2015)). When
kept separately, with explicit loops over episodes, the algo-
rithm itself is different (e.g., Sarsa (Sutton and Barto, 1998a,
Figure 8.8)); or, if a state-based approach is used, fake states
and fake transitions would have to be explicitly added to
make the update the same for continuing or episodic. For
the generalized formulation, the only difference is the γt+1
that is passed to the algorithm; the algorithm itself remains
exactly the same in the two settings. As a minor example,
for episodic problems, there is typically an explicit (error-
prone) step to clear traces; with generalized discounting, the
traces are automatically cleared at the end of an episode by
γt+1.

Experimental design. When presenting results for episodic
and continuing problem, often the former uses number of
episodes and the later number of steps. In reality both simply
consist of a trajectory of information, with the former having
γt+1 = 0 for some steps. A uniﬁed view with number of
steps enables more consistent presentation of results across
domains. Related to this difference, a common but rarely
discussed decision when implementing episodic tasks is the
cut-off for the maximum number of steps in an episode. If
set too small, an algorithm that takes longer to reach the
goal in the ﬁrst few episodes, but then learns more quickly
afterwards, could be unfairly penalized. Instead learning
could be limited to some maximum number of steps to
constrain learning time similarly for both continuing and
episodic problems, where multiple episodes could occur
within that maximum number of steps.

B.3. Proof of Theorem 2

This prove illustrates the representability relationship be-
tween transition-based discounting and state-based discount-
ing. This equivalence could be obtained more compactly if
γ(s, a, s(cid:48)) is not different for every s(cid:48); however, the proof
becomes much more involved. Since our main goal is to sim-
ply show a representability result, we opt for interpretability.
Note that, in addition, the result in Theorem 2 ﬁlls a gap
in the previous theory, which indicated that state-based dis-
counting could be used to represent episodic problems, but
did not explicitly demonstrate that the stationary distribution
would be equivalent (see (Yu, 2015)).
Deﬁne transition probabilities ¯Pr : ¯S × ¯S → [0, 1]

¯Pr(i, a, j) =






Pr(i, a, s(cid:48))
1
0

i ∈ S, j = fias(cid:48)
j ∈ S, i = fsaj
otherwise

rewards

(cid:26) r(i, a, s(cid:48))

¯r(i, a, j) =

0

i ∈ S, j = fias(cid:48)
otherwise

and state-based discount function ¯γs : ¯S → [0, 1]

(cid:26) γ(s, a, s(cid:48))

¯γs(i) =

1

i = fsas(cid:48)
otherwise

a

Theorem 2 For
transition-based MDP
given
(Pr, r, S, A, γ) and policy π, assume that the station-
ary distribution dπ exists. Deﬁne state-based MDP
(¯Pr, ¯r, ¯S, A, ¯γs) with extended ¯π, all as above. Then the
stationary distribution ¯dπ for ¯π exists and satisﬁes

¯dπ(s)

(cid:80)

i∈S

¯dπ(i)

= dπ(s)

(6)

and for all s ∈ S, ¯vπ(s) = vπ(s).
Proof: Deﬁne matrix ¯Pπ ∈ R(n+|A|n2)×(n+|A|n2) where
¯Pπ(i, j) = (cid:80)
a∈A ¯π(i, a)¯Pr(i, a, j), giving

π(i, a) Pr(i, a, s(cid:48))
1
0

i ∈ S, j = fias(cid:48)
i = fsaj, a ∈ A, j ∈ S
otherwise

¯Pπ(i, j) =

Deﬁne






1
c

¯dπ(i) :=

(cid:26) dπ(i)

dπ(s)π(s, a) Pr(s, a, s(cid:48))

i ∈ S
i = fsas(cid:48)

where c > 0 is a normalizer to ensure that 1(cid:62) ¯dπ = 1. Now
we need to show that ¯dπ ¯Pπ = ¯dπ. For any j ∈ S,

¯dπ ¯Pπ(:, j) =

dπ(s) ¯Pπ(s, j) +

(cid:88)

(cid:17)
¯dπ(f ) ¯Pπ(f, j)

1
c

(cid:16) (cid:88)

s∈S

f ∈F

Case 1: j ∈ S
For the ﬁrst component, because ¯Pπ(s, j) = 0 by deﬁnition
of ¯Pr, we get

dπ(s) ¯Pπ(s, j) = 0

(cid:88)

s∈S

For the second component, because ¯Pπ(fsaj, j) = 1,

(cid:88)

fsas(cid:48) ∈F

¯dπ(fsas(cid:48)) ¯Pπ(fsas(cid:48), j)

¯dπ(fsaj) ¯Pπ(fsaj, j)

(cid:88)

fsaj ∈F
(cid:88)

fsaj ∈F
(cid:88)

s∈S
(cid:88)

=

=

=

=

s∈S
= dπ(j)

¯dπ(fsaj)

(cid:88)

a∈A

dπ(s)Pπ(s, j)

dπ(s)

π(s, a)Pr(s, a, j)

Unifying Task Speciﬁcation in Reinforcement Learning

where the last line follows from the deﬁnition of the station-
ary distribution. Therefore, for j ∈ S

With this equivalence, it is clear that

¯dπ ¯Pπ(:, j) =

dπ(j) = ¯dπ(j)

1
c

Case 2: j = fsas(cid:48) ∈ F
For the ﬁrst component, because ¯Pπ(i, fsas(cid:48)) = 0 for all
i (cid:54)= s and because ¯Pπ(s, fsas(cid:48)) = π(s, a)Pr(s, a, s(cid:48)) by
construction,

¯dπ(i)¯vπ(i)

(cid:88)

i∈ ¯S
1
c

=

=

=

=

1
c

1
c

2
c

(cid:88)

s∈S

(cid:88)

s∈S
(cid:88)

s∈S
(cid:88)

s∈S

dπ(s)vπ(s) +

dπ(s)Pπ(s, s(cid:48))vπ(s(cid:48))

dπ(s)vπ(s) +

dπ(s)Pπ(s, s(cid:48))vπ(s(cid:48))

dπ(s)vπ(s) +

dπ(s(cid:48))vπ(s(cid:48))

1
c

1
c

1
c

(cid:88)

fss(cid:48) ∈F
(cid:88)

(cid:88)

s(cid:48)∈S

s∈S
(cid:88)

s(cid:48)∈S

dπ(i) ¯Pπ(i, fsas(cid:48)) = dπ(s) ¯Pπ(s, fsas(cid:48))

(cid:88)

i∈S

dπ(s)vπ(s)

= dπ(s)π(s, a)Pr(s, a, s(cid:48))
= c ¯dπ(fsas(cid:48)).

Therefore, optimizing either results in the same policy. (cid:4)

For the second component, because ¯Pπ(f, j) = 0 for all
f, j ∈ F, we get

control

C. Discounting and average reward for

¯dπ(f ) ¯Pπ(f, j) = 0.

(cid:88)

f ∈F

Therefore, for j = fss(cid:48) ∈ F, ¯dπ ¯Pπ(:, j) = ¯dπ(j).
Finally, clearly by normalizing the ﬁrst component of ¯dπ
over s ∈ S, we get the same proportion across states as in
dπ, satisfying (6).

To see why ¯vπ(s) = vπ(s) for all s ∈ S, ﬁrst notice that

¯rπ(i) =

(cid:26) rπ(i)
0

i ∈ S
otherwise

and for any fsas(cid:48) ∈ F

¯vπ(fsas(cid:48)) = 0 +

(cid:88)

¯Pπ(fsas(cid:48), j)¯γs(j)¯vπ(j)

The common wisdom is that discounting is useful for asking
predictive questions, but for control, the end goal is average
reward. One of the main reasons for this view is that it has
been previously shown that, for a constant discount, optimiz-
ing the expected return is equivalent to optimizing average
reward. This can be easily seen by expanding the expected
return weighting according to the stationary distribution for
a policy, given constant discount γc < 1,

dπvπ = dπ(rπ + Pπ,γvπ)

(7)

= dπrπ + γcdπPπvπ
= dπrπ + γcdπvπ

1
1 − γc

=⇒ dπvπ =

dπrπ.

(8)

Therefore, the constant γc < 1 simply scales the aver-
age reward objective, so optimizing either provides the
same policy. This argument, however, does not extend to
transition-based discounting, because γ(s, a, s(cid:48)) can signif-
icantly change weighting in returns in a non-uniform way,
affecting the choice of the optimal policy. We demonstrate
this in the case study for the taxi domain in Section 3.

j∈ ¯S
= ¯vπ(s(cid:48)).

(cid:88)

j∈ ¯S

(cid:88)

fsas(cid:48) ∈F
(cid:88)

(cid:88)

s(cid:48)∈S

a∈A

Now for any s ∈ S,

¯vπ(s) = ¯rπ(s) +

¯Pπ(s, j)¯γs(j)¯vπ(j)

D. Algorithms

= rπ(s) +

¯Pπ(s, fsas(cid:48))¯γs(fsas(cid:48))¯vπ(fsas(cid:48))

= rπ(s) +

Pr(s, a, s(cid:48))γ(s, a, s(cid:48))¯vπ(s(cid:48))

Therefore, because it satisﬁes the same ﬁxed point equation,
¯vπ(s) = vπ(s) for all s ∈ S.

We show how to write generalized pseudo-code for two al-
gorithms: true-online TD (λ) and ELSTDQ(λ). We choose
these two algorithms because they generally demonstrate
how one would extend to transition-based γ, and further
previously had a few unclear points in their implementation.
For TO-TD, the pseudo-code has been given for episodic
tasks (van Seijen and Sutton, 2014), rather than more gen-
erally, and has treated vold carefully at the beginning of
episodes, which is not necessary. LSTDQ has typically only

Unifying Task Speciﬁcation in Reinforcement Learning

E. Lemmas

Algorithm 1 True-online TD(λ)
vold ← 0

w ← 0, e ← 0,
Obtain initial x0
while agent interacting with environment, t = 0, 1, . . .
do

Obtain next feature vector xt+1, reward rt+1 and

discount γt+1
ˆv = w(cid:62)xt
ˆv(cid:48) = w(cid:62)xt+1
δ ← rt+1 + γt+1ˆv(cid:48) − ˆv
e ← e + xt
w ← w + α(δ + ˆv − vold)e − α(ˆv − vold)xt
vold ← ˆv(cid:48)
e ← γt+1λt+1e − αγt+1λt+1(e(cid:62)xt+1)xt+1

return w

been written for a batch of data, without importance sam-
pling; we provide an ELSTDQ variant here with importance
sampling, where LSTDQ is a special case using M = 1.

There are a few other implementation details that merit clar-
iﬁcation. We use the notation γt+1 for γ(st, at, st+1), and
λt+1 for λ(st, at, st+1). Further, unlike previous pseudo-
code, we do not reinitialize vold specially at the start of an
episode (i.e., when γt+1 = 0). This is because the value of
vold is not relevant for the next step after γt+1 = 0. The eligi-
bility trace is zeroed, and so α(δ+ˆv−vold)e−α(ˆv−vold)x =
αδx. Finally, for both algorithms, we stage the updates to
the traces. This is to avoid saving both γt, λt and γt+1, λt+1
across timesteps.

Algorithm 2 ELSTDQ(λ)

A ← 0, b ← 0, e ← 0
F ← 0, M ← 0
Obtain initial action-value feature vector x0 (implicitly
x(s0, a0)) and action a0
while agent interacting with environment, t = 0, 1, . . .
do

Obtain next action-value feature vector xt+1, action

at+1 reward rt+1 and discount γt+1

ρt+1 ← π(xt+1,at+1)
µ(xt+1,at+1)
F ← γt+1F + i(xt)
M ← λt+1i(xt) + (1 − λt+1)F
e ← e + M xt
A ← A + e (xt − ρt+1γt+1xt+1)(cid:62)
b ← b + ert+1
e ← γt+1λt+1ρt+1e
F ← ρt+1F

return A−1b

// The solution w to the linear system

To bound the maximum eigenvalues of the discount-
weighted transition matrix, we ﬁrst provide the following
lemma. This lemma is independently interesting, in that it
explicitly veriﬁes the previous Assumption 2.1 (Yu, 2015).

Lemma 3. Under Assumption A3, the maximum eigenvalue
of Pπ,γ is less than 1:

r(Pπ,γ) < 1.

Proof:
Part 1: We ﬁrst show that r( ˜M) < 1 for

˜M =

(cid:26) Mkl − δ
Mkl

if i = k, j = l
otherwise.

for any i, j, and any 0 < δ < Mij.

For M = Pπ, we know that r(M) = 1 and that, by assump-
tion, Pπ is irreducible. We know that ˜M is still irreducible,
because the connectivity is not changed (since no additional
entries are zeroed). By the Perron-Frobenius theorem, we
know that the eigenvector x that corresponds to the maxi-
mum eigenvalue r( ˜M) has strictly positive entries and so
δxixj > 0. Therefore,

x(cid:62) ˜Mx =

(cid:88)

xkMklxl + xi(Mij − δ)xj

k,l:(k,l)(cid:54)=(i,j)
= x(cid:62)Mx − δxixj
< x(cid:62)Mx.

We know that x(cid:62)Mx ≤ 1, because r(M) = 1 and x is a
unit vector. Therefore, using the fact that r( ˜M) = x(cid:62) ˜Mx
by Courant-Fischer-Weyl4, we get

r( ˜M) = x(cid:62) ˜Mx < 1.

Part 2: Next we show that further reducing entries, even
to zero values, will not increase the maximum eigenvalue.
This follows simply from the fact that non-negative matrices
are guaranteed to have a non-negative eigenvector x that
corresponds to the maximum eigenvalue.

To see why, for notational convenience, we now let M be
the matrix where entry i, j in Pπ was reduced by δ. Let ˜M
further reduce an entry by δ, now potentially to a minimum
value of 0, so that ˜M is guaranteed to be non-negative (rather
than strictly positive). Using the same argument as above,

4The Courant-Fischer-Weyl min-max principle states that
the maximum eigenvalue r(M) of a matrix M corresponds to
argmaxx:x(cid:54)=0 x(cid:62)Mx/(x(cid:62)x), where the corresponding x that
gives the maximum is an eigenvector of M. Therefore, for this x,
2 = x(cid:62)x = 1.
x(cid:62)Mx = r(M) and (cid:107)x(cid:107)2

we obtain that for the non-negative eigenvector of ˜M

For t > 0: Assume that

Unifying Task Speciﬁcation in Reinforcement Learning

x(cid:62) ˜Mx = x(cid:62)Mx − δxixj
≤ r(M)

t
(cid:88)

k=0

(Pπ,γ,λ)kPπ,γ,1−λ1 ≤ 1.

because δxixj ≥ 0. Therefore, with further reduction,
r( ˜M) cannot increase and so r( ˜M) ≤ r(M) < 1.

Then

Part 3: Finally, we can see that for any γ and Pπ as given
under Assumptions 2 and 3, M = Pπ,γ satisﬁes the above
(cid:4)
construction.

Now we additionally provide deﬁnitions for the extension to
transition-based discounts. To do so, we will need to deﬁne

Pπ,γ,1−λ(s, s(cid:48)) :=

π(s, a)Pr(s, a, s(cid:48))γ(s, a, s(cid:48))(1 − λ(s, a, s(cid:48)))

(cid:88)

a∈A
(cid:88)

a∈A

= Pπ,γ − Pπ,γ,λ

t+1
(cid:88)

k=0

(Pπ,γ,λ)kPπ,γ,1−λ1

= Pπ,γ,1−λ1 +

(cid:34)t+1
(cid:88)

(cid:35)
(Pπ,γ,λ)kPπ,γ,1−λ1

k=1

(cid:34) t

(cid:88)

k=0

(cid:35)

≤ Pπ,γ,1−λ1 + Pπ,γ,λ1
= Pπ,γ1
≤ 1

Pπ,γ,λ(s, s(cid:48)) :=

π(s, a)Pr(s, a, s(cid:48))γ(s, a, s(cid:48))λ(s, a, s(cid:48))

= Pπ,γ,1−λ1 + Pπ,γ,λ

(Pπ,γ,λ)kPπ,γ,1−λ1

Then we obtain the following generalized deﬁnition of Pλ
π
and necessary properties for convergence within ETD.

Lemma 4. Under Assumption A3, I − Pπ,γ,λ is non-
singular and the matrix

Pλ

π = (I − Pπ,γ,λ)−1 Pπ,γ,1−λ

is non-negative and has rows that sum to no greater than 1,

0 ≤ Pλ

π ≤ 1

and

Pλ

π1 ≤ 1.

Proof:

By Lemma 3, we know r(Pπ,γ) < 1.
Because
0 ≤ λ(s, a, s(cid:48)) ≤ 1 for all (s, a, s(cid:48)), this means that
r(Pπ,γ,1−λ) < 1. Therefore I − Pπ,γ,λ is non-singular
and so Pλ

π is well-deﬁned.

Notice that I − Pπ,γ,λ is a non-singular M-matrix, since the
maximum eigenvalue of Pπ,γ,λ is less than one and entry-
wise Pπ,γ,λ ≥ 0. Therefore, the inverse of I − Pπ,γ,λ is
positive, making (I − Pπ,γ,λ)−1 Pπ,γ,1−λ a positive matrix.
The fact that the matrix has entries that are less than or equal
to 1 follows from showing Pλ

π1 ≤ 1 below.

completing the proof.

(cid:4)

F. Convergence of emphatic algorithms for

the RL task formalism

We start with convergence in expectation of ETD for
transition-based discounts. The results for state-based
MDPs should automatically extend to transition-based
MDPs, due to the equivalence proved in Section B.1. How-
ever, in an effort to similarly generalize the writing of the
theoretical analysis to the more general transition-based
MDP setting, as we did for algorithms and implementation,
we explicitly extend the proof for transition-based MDPs.

Theorem 3. Assume the value function is approximated
using linear function approximation: v(s) = x(s)(cid:62)w. For
X with linearly independent columns (i.e. linearly indepen-
dent features), with an interest function i : S → (0, ∞) and
π)−1(d ◦ i), the matrix A
M = diag(m) for m = (I − Pλ
is positive deﬁnite.

A := X(cid:62)M(I − Pπ,γ,λ)−1(I − Pπ,γ)X
π)X

= X(cid:62)M(I − Pλ

To show that the matrix rows always sum to less than 1, we
use a simple inductive argument. Since

is positive deﬁnite.

Pλ

π =

(Pπ,γ,λ)kPπ,γ,1−λ

∞
(cid:88)

k=0

Proof:

First, we write an equivalent deﬁnition for Pλ
π,

show that

for

every

t,

Pλ

π = (I − Pπ,γ,λ)−1 Pπ,γ,1−λ

we
(cid:80)t

simply

need
k=0(Pπ,γ,λ)kPπ,γ,1−λ1 ≤ 1.
For the base case, t = 0: clearly

to

Pπ,γ,1−λ1 ≤ 1

= (I − Pπ,γ,λ)−1 (Pπ,γ − Pπ,γ,λ)
= (I − Pπ,γ,λ)−1 (Pπ,γ − I + I − Pπ,γ,λ)
= I − (I − Pπ,γ,λ)−1(I − Pπ,γ).

Unifying Task Speciﬁcation in Reinforcement Learning

Since X is a full rank matrix, to prove that

A = X(cid:62)M(I − Pλ

π)X

is positive deﬁnite, we need to prove that M(I − Pλ
positive deﬁnite.

π) is

As in (Sutton et al., 2016, Theorem 1), (Yu, 2015, Propo-
sition C.1), we need to show that for M(I − Pλ
π), (a) the
diagonal entries are nonnegative, (b) the off-diagonal entries
are nonpositive (c) its row sums are nonnegative and (d)
the columns sums are are positive. The requirements (a)
- (c) follow from Lemma 4, because M is a non-negative
diagonal weighting matrix. To show (d), ﬁrst if i(s) > 0 for
all s ∈ S, the vector of columns sums is

Further, this MDP with the assumptions on the subspace
produced by the state-action features satisﬁes the conditions
of Theorem 3, and so Aq is also positive deﬁnite.

Similarly, the other properties of the Bellman operator
and the weighted norm on Pλ,q
extend, giving a unique
ﬁxed point for the action-value Bellman operator Pλ,q
and
(cid:107)Pλ,q

π (cid:107)Mq < 1.

π

π

Corollary 1. Assume the action-value function is approxi-
mated using linear function approximation: x(s, a)(cid:62)w. For
X with linearly independent columns (i.e. linearly indepen-
dent features), Aq is positive deﬁnite.

G. Issues with transition-based trace without

1(cid:62)M(I − Pλ

π) = m(cid:62)(I − Pλ

π) = (dπ ◦ i)(cid:62)

emphatic weighting

which always has positive entries.

Otherwise, if i(s) = 0 for some s ∈ S, we can prove that
M(I − Pλ
π) is positive deﬁnite using the same argument
as in (Yu, 2015, Corollary C.1). The proof nicely encap-
sulates Pλ
π generically as a matrix Q. We simply have to
ensure that the inverse of I − Pλ
π has
entries less than or equal to 1, both of which were showed
in Lemma 4. The ﬁrst condition is to have well-deﬁned ma-
trices, and the second to ensure that Q has a block-diagonal
structure. Therefore, under Assumption 4, we can follow
the same proof as (Yu, 2015, Corollary C.1) to ensure that
(cid:4)
A is positive deﬁnite.

π exists and that Pλ

For the proofs for ELSTDQ, the main difference is in using
action-value functions. We construct the augmented space,
with states ¯S = S × A and

A natural goal is to similarly generalize the contraction
properties of Pλ
π under the weighting dπ, from constant λc
to transition-based trace. To do so, unlike under emphatic
weighting, we need to restrict the set of possible trace func-
tions. Notice that, because of Assumption A3, for some
sλ < 1 and s1−λ < 1, for any non-negative v+,

dπ(s)Pr(s, a, :) ◦ γ(s, a, :) ◦ λ(s, a, :)v+

≤ sλ

dπ(s)Pr(s, a, :)v+ = sλdπv+

dπPπ,γ,λv+
(cid:88)

(cid:88)

=

s

a
(cid:88)

(cid:88)

s

a

and similarly

Pπ,γ,q((s, a), (s, a(cid:48))) := P (s, a, s(cid:48))γ(s, a, s(cid:48))π(s(cid:48), a(cid:48))
Pπ,γ,λ,q((s, a), (s, a(cid:48))) := P (s, a, s(cid:48))γ(s, a, s(cid:48))λ(s, a, s(cid:48))π(s(cid:48), a(cid:48))

dπPπ,γ,1−λv+ ≤ s1−λdπv+.

giving

Then

iq((s, a)) := i(s)

dµ,q((s, a)) := dµ(s)µ(s, a)

rq((s, a)) :=

Pr(s, a, s(cid:48))r(s, a, s(cid:48))

(cid:88)

s(cid:48)∈S

:= (I − Pπ,γ,q)−1(Pπ,γ,q − Pπ,γ,λ,q)

Pλ,q
π
Mq := diag (cid:0)dµ,q ◦ iq(I − Pλ,q

π )−1(cid:1) .

Aq := X(cid:62)
bq := X(cid:62)

q Mq(I − Pλ,q
q Mq(I − Pπ,γ,q)rq.

π )X

projected Bellman

as
is
The
ΠMq T (λ,q)q = rq + Pλ,q
π q where ELSTD(λ) con-
verges to the projected Bellman operator ﬁxed point
ΠMq T (λ,q)q = q.

operator

deﬁned

ple.

The generalized bound on the dπ weighted norm is given in
the following lemma.

Lemma 5.

(cid:107)Pλ

π(cid:107)Dπ ≤

s1−λ
1 − sλ

.

Now, the norm is only a contraction if s1−λ < 1 − sλ. As
we have seen, for constant trace, this inequality holds, since
s1−λ = s(1 − λ) and sλ = sλ for some s < 1. In general,
however, there are instances where this is not true. We
provide such an example below.5

Consider a 2-state MDP, with uniform probabilities of tran-
sitioning and uniform policy, and so dπ = [0.5, 0.5]. Let
γc = 0.99 and set λ to be 0.9 when entering state s1 and 0

5Thanks to an anonymous reviewer for pointing out this exam-

Unifying Task Speciﬁcation in Reinforcement Learning

when entering state s2. Then for any v+,
(cid:20) 0.9
0

dπPπ,γ,λv+ = γcdπPπ

(cid:21)

v+

0
0
(cid:21)

(cid:20) 0.9
0
0
0
(cid:20) 1.0 0
0
0

v+

(cid:21)

v+

= γcdπ

= γc0.9dπ

≤ γc0.9dπv+

where for v+ = [v 0](cid:62) for any v ≥ 0, this bound is tight.
Similarly,

dπPπ,γ,1−λv+ = γcdπPπ

(cid:20) 0.1
0

(cid:21)

v+

0
1.0

≤ γcdπv+.

where for v+ = [0 v](cid:62) for any v ≥ 0, this bound is tight.

Therefore, sλ = 0.9γc and s1−λ = γc, and so we get
1 − sλ = 0.9γc < s1−λ = γc, which makes the upper
bound in the above lemma 1.¯1. Computing Pλ
π,

Pλ

π =

(cid:20) 0.0893
0.0893

0.8927
0.8927

(cid:21)

.

we can see that this is not a contraction.

