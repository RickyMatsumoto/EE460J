Hyperplane Clustering via Dual Principal Component Pursuit

Manolis C. Tsakiris 1 Ren´e Vidal 1

Abstract

State-of-the-art methods
for clustering data
drawn from a union of subspaces are based on
sparse and low-rank representation theory and
convex optimization algorithms. Existing re-
sults guaranteeing the correctness of such meth-
ods require the dimension of the subspaces to be
small relative to the dimension of the ambient
space. When this assumption is violated, as is,
e.g., in the case of hyperplanes, existing meth-
ods are either computationally too intensive (e.g.,
algebraic methods) or lack sufﬁcient theoretical
support (e.g., K-Hyperplanes or RANSAC). In
this paper we provide theoretical and algorithmic
contributions to the problem of clustering data
from a union of hyperplanes, by extending a re-
cent subspace learning method called Dual Prin-
cipal Component Pursuit (DPCP) to the multi-
hyperplane case. We give theoretical guarantees
under which, the non-convex (cid:96)1 problem asso-
ciated with DPCP admits a unique global min-
imizer equal to the normal vector of the most
dominant hyperplane.
Inspired by this insight,
we propose sequential (RANSAC-style) and it-
erative (K-Hyperplanes-style) hyperplane learn-
ing DPCP algorithms, which, via experiments on
synthetic and real data, are shown to outperform
or be competitive to the state-of-the-art.

1. Introduction

1.1. Hypeprlane clustering

Subspace clustering, the problem of clustering data drawn
from a union of linear subspaces, is an important problem
in machine learning, pattern recognition and computer vi-
sion (Vidal et al., 2016). A particular case of this prob-
lem is hyperplane clustering, which arises when the data

1Center for Imaging Science, Johns Hopkins University, Bal-
timore, MD, USA. Correspondence to: Manolis C. Tsakiris
<m.tsakiris@jhu.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

lie in a union of hyperplanes, as in, e.g., projective motion
segmentation (Vidal et al., 2006), 3D point cloud analysis
(Sampath & Shan, 2010) and hybrid system identiﬁcation
(Vidal et al., 2003; Bako, 2011). Even though in some ways
hyperplane clustering is simpler than general subspace
clustering, since, e.g., the dimensions of the subspaces
are equal and known a priori, modern self-expressiveness-
based methods (Liu et al., 2013; Lu et al., 2012; Elhamifar
& Vidal, 2013; Wang et al., 2013; You et al., 2016), in prin-
ciple do not apply in this case, because they require small
relative subspace dimensions d/D, where d, D are the di-
mensions of the subspace and ambient space, respectively.

From a theoretical point of view, one of the most appro-
priate methods for hyperplane clustering is Algebraic Sub-
space Clustering (ASC) which gives closed-form solutions
by means of factorization or differentiation of polynomi-
als (Vidal et al., 2005). However, the main drawback of
ASC is its exponential complexity1, which makes it im-
practical in many settings. Another method that is theoret-
ically justiﬁable for clustering hyperplanes is Spectral Cur-
vature Clustering (SCC) (Chen & Lerman, 2009), which
computes a D-fold afﬁnity between all D-tuples of points
in the dataset. As in the case of ASC, SCC has combina-
torial complexity and becomes cumbersome for large D.
On the other hand, the intuitive classical method of K-
Hyperplanes (KH) (Bradley & Mangasarian, 2000), which
alternates between assigning clusters and ﬁtting a new nor-
mal vector to each cluster with PCA, is perhaps the most
practical method for hyperplane clustering, since it is sim-
ple to implement and it is robust to noise. However, KH
is sensitive to outliers and is guaranteed to converge only
to a local minimum; hence multiple restarts are in gen-
eral required. Median K-Flats (MKF) (Zhang et al., 2009)
shares a similar objective function as KH, but uses the (cid:96)1-
norm instead of the (cid:96)2-norm, in an attempt to gain robust-
ness to outliers. The minimization is done via a stochastic
gradient descent scheme, and searches directly for a ba-
sis of each subspace, which makes it slower to converge
for hyperplanes. Finally, any single robust subspace learn-
ing method suitable for high relative dimensions, such as
RANSAC (Fischler & Bolles, 1981) or REAPER (Lerman
et al., 2015), can be applied either i) in a sequential fashion

1The issue of robustness to noise for ASC has been recently

addressed in Tsakiris & Vidal 2015b; 2017c.

Hyperplane Clustering via Dual Principal Component Pursuit

by ﬁrst learning the most dominant hyperplane, removing
the points lying close to it, learning the second most domi-
nant hyperplane, and so on, or ii) in an iterative fashion, by
assigning points to clusters, ﬁtting a hyperplane per cluster,
reassigning the points to new clusters and so on.

1.2. Dual principal component pursuit

Dual Principal Component Pursuit (DPCP) (Tsakiris &
Vidal, 2015a; 2017a) is an (cid:96)1 single subspace learning
method, which aims at recovering the orthogonal comple-
ment of the subspace in the presence of outliers, and as such
it is particularly suited for hyperplanes. DPCP searches for
the normal vector to a hyperplane by solving a non-convex
(cid:96)1 minimization problem on the sphere, or a recursion of
linear programming relaxations, and under certain condi-
tions, the normal to the hyperplane is the unique global so-
lution to this non-convex (cid:96)1 problem, as well as the limit
point of the LP recursion. Motivated by the robustness of
DPCP to outliers, one could naively use it for hyperplane
clustering by recovering the normal vector to a hyperplane
one at a time, while treating points from other hyperplanes
as outliers. However, such a scheme is not a priori guar-
anteed to succeed because the assumptions in the theorems
of correctness of DPCP assume that outliers are uniformly
distributed on the sphere, an assumption which is violated
when the data come from a union of hyperplanes.

1.3. Paper contributions

In this paper we provide a theoretical analysis of the non-
convex (cid:96)1 DPCP problem for data drawn from a union
of hyperplanes. We show that as long as the hyperplanes
are sufﬁciently separated, the dominant hyperplane is sufﬁ-
ciently dominant and the points are uniformly distributed
(in a deterministic sense) inside their associated hyper-
planes, the normal vector of the dominant hyperplane is the
unique (up to sign) global minimizer of the DPCP problem.
This suggests a DPCP-based sequential hyperplane learn-
ing algorithm, which uses DPCP to compute a dominant
hyperplane, then a second dominant hyperplane and so on.
Experiments on synthetic data show that this DPCP-based
algorithm signiﬁcantly improves over similar sequential al-
gorithms, which are based on RANSAC or REAPER. Fi-
nally, 3D plane clustering experiments on real 3D point
clouds show that an iterative (KH-style) DPCP scheme is
very competitive to RANSAC, which is the predominant
state-of-the-art method for such applications.

2. Preliminaries

2.1. Data model and the hyperplane clustering problem

Consider given a collection X = [x1, . . . , xN ] ∈ RD×N
of N points of the unit sphere SD−1 of RD, that lie in a

union (arrangement) A of n hyperplanes H1, . . . , Hn of
RD, i.e., X ⊂ A = (cid:83)n
i=1 Hi, where each hyperplane Hi
is the set of points of RD that are orthogonal to a normal
vector bi ∈ SD−1, i.e., Hi = (cid:8)x ∈ RD : x(cid:62)bi = 0(cid:9) , i ∈
[n] := {1, . . . , n}. We assume that the data X lie in gen-
eral position in A, by which we mean two things. First, we
mean that there are no linear relations among the points
other than the ones induced by their membership to the
hyperplanes. In particular, every (D − 1) points coming
from Hi form a basis for Hi and any D points of X that
come from at least two distinct Hi, Hi(cid:48) are linearly inde-
pendent. Second, we mean that the points X uniquely de-
ﬁne the hyperplane arrangement A, in the sense that A is
the only arrangement of n hyperplanes that contains X .
This can be veriﬁed computationally by checking that there
is only one up to scale homogeneous polynomial of degree
n that ﬁts the data, see Vidal et al. 2005; Tsakiris & Vidal
2017c for details. We assume that for every i ∈ [n], pre-
cisely Ni points of X , denoted by X i = [x(i)
1 , . . . , x(i)
],
Ni
belong to Hi, with (cid:80)n
i=1 Ni = N . With that notation,
X = [X 1, . . . , X n]Γ, where Γ is an unknown permuta-
tion matrix, indicating that the hyperplane membership of
the points is unknown. Moreover, we assume an ordering
N1 ≥ N2 ≥ · · · ≥ Nn, and we refer to H1 as the domi-
nant hyperplane. After these preparations, the problem of
hyperplane clustering can be stated as follows: given the
data X , ﬁnd the number n of hyperplanes associated to X ,
a normal vector to each hyperplane, and a clustering of the
data X = (cid:83)n
i=1 X i according to hyperplane membership.

2.2. Review of dual principal component pursuit

Dual Principal Component Pursuit (DPCP) (Tsakiris & Vi-
dal, 2017a) is a robust single subspace learning method.
Given unlabeled data X , which consist of inliers in a sin-
gle subspace S of RD of dimension d < D, together with
outliers to the subspace, DPCP computes a basis for the
orthogonal complement of the inlier subspace S. The key
idea of DPCP is to identify a single hyperplane H with nor-
mal vector b that is maximal with respect to the data X .
Such a maximal hyperplane is deﬁned by the property that
it must contain a maximal number of points NH from the
dataset, i.e., NH(cid:48) ≤ NH for any other hyperplane H(cid:48) of
RD. Notice that such a maximal hyperplane can be charac-
terized as a solution to the combinatorial problem

(cid:13)
(cid:13)X (cid:62)b
(cid:13)

(cid:13)
(cid:13)
(cid:13)0

min
b

s.t. b (cid:54)= 0,

(1)

(cid:13)
(cid:13)X (cid:62)b
(cid:13)

(cid:13)
(cid:13)
(cid:13)0

is the number of non-zero entries of X (cid:62)b,
since
which is precisely the number of data points in X that lie
outside the hyperplane deﬁned by b. If S is a hyperplane,
i.e., dim S = D−1, and if there are at least dim S +1 = D
inliers, it is straightforward to show that (1) has a unique
up to scale global minimizer, the normal vector to the inlier

Hyperplane Clustering via Dual Principal Component Pursuit

hyperplane. Since (1) is hard to solve, we relax it to

(cid:13)
(cid:13)X (cid:62)b
(cid:13)

(cid:13)
(cid:13)
(cid:13)1

min
b

s.t. (cid:107)b(cid:107)2 = 1,

(2)

which is still challenging to solve, since it is a non-smooth
non-convex optimization problem on the sphere. Problem
(2) has appeared several times in the literature (Sp¨ath &
Watson, 1987; Spielman et al., 2013; Qu et al., 2014; Sun
In fact, Sp¨ath & Watson 1987 proved the
et al., 2015).
following fascinating result.

Proposition 1 (Sp¨ath & Watson, 1987) Let X be a D × N
matrix of rank D. Then any global minimizer of (2) must
be orthogonal to D −1 linearly independent columns of X .

Proposition 1 establishes an encouraging property of prob-
lem (2) towards recovering the normal vector of the inlier
hyperplane as its global minimizer. Indeed, it would suf-
ﬁce that the D − 1 linearly independent points of X that
a global minimizer is orthogonal to, be points of the inlier
hyperplane. Even though a priori it is not clear under what
conditions this is the case, Tsakiris & Vidal 2015a provided
an answer, informally stated as follows.

Proposition 2 (Tsakiris & Vidal, 2015a) Suppose that the
inliers are sufﬁciently uniformly distributed (in a determin-
istic sense deﬁned in Grabner et al. 1997) inside the inter-
section of the inlier hyperplane and the (unit) sphere, and
that the outliers are sufﬁciently uniformly distributed on the
sphere. Then (2) has a unique up to sign global minimizer,
equal to the normal vector of the inlier hyperplane.

It was further shown in Tsakiris & Vidal 2015a that under
the conditions of Proposition 2, and assuming that ˆn0 is a
unit (cid:96)2-norm vector sufﬁciently far from the inlier hyper-
plane, the recursion of linear programs

nk+1 := argmin
b(cid:62) ˆnk=1

(cid:13)
(cid:13)X (cid:62)b
(cid:13)

(cid:13)
(cid:13)
(cid:13)1

,

(3)

will converge in a ﬁnite number of iterations to the global
minimizer of (2).2

2.3. Hyperplane clustering via DPCP?

Given the discussion in §2.2, the DPCP problem (2) seems
a natural mechanism towards retrieving the normal vectors
to the hyperplanes associated with a dataset X lying in
a hyperplane arrangement.
Indeed, one may be tempted
to solve problem (2) for such a dataset with the hope of
obtaining a unique global minimizer, which is orthogonal
to one of the underlying hyperplanes. In this case, points

2Remarkably, (3) was ﬁrst proposed in Sp¨ath & Watson 1987
as a means of solving (2), and it was established that it converges
in a ﬁnite number of iterations to a critical point of (2).

coming from the remaining hyperplanes are treated as out-
liers. Such an idea would give rise to sequential and iter-
ative DPCP hyperplane clustering algorithms as described
in §1.1. A sufﬁcient condition for the correctness of this
procedure would be that the global minimizer of the DPCP
problem (2) be orthogonal to the inlier subspace. However,
the conditions for this to be the case do not immediately
follow from the work of Tsakiris & Vidal 2015a, since in
the case of hyperplane clustering the outliers lie in a union
of n − 1 hyperplanes, and thus can not be uniformly dis-
tributed on the sphere, as the conditions of Tsakiris & Vidal
2015a require. The rest of the paper is devoted to provid-
ing such theoretical guarantees (§3), as well as introducing
DPCP-based hyperplane clustering algorithms (§4).

3. Theoretical Contributions

3.1. Theoretical analysis of the continuous problem

As it turns out, one can gain important insights about the
analysis of the DPCP problem (2) for data in a hyperplane
arrangement, by ﬁrst analyzing a certain continuous prob-
lem. To see what that problem is, let ˆHi = Hi ∩ SD−1, and
note ﬁrst that for any b ∈ SD−1 we have

1
Ni

Ni(cid:88)

j=1

(cid:90)

(cid:12)
(cid:12)b(cid:62)x(i)
(cid:12)

j

(cid:12)
(cid:12)
(cid:12) (cid:39)

x∈ ˆHi

(cid:12)
(cid:12)b(cid:62)x
(cid:12)

(cid:12)
(cid:12)
(cid:12) dµ ˆHi

,

(4)

(cid:13)
(cid:13)X (cid:62)
(cid:13)
i b

(cid:13)
(cid:13)
(cid:13)1

where the LHS of (4) is precisely 1
and can be
Ni
viewed as an approximation ((cid:39)) via the point set X i of
the integral on the RHS of (4), with µ ˆHi
denoting the uni-
form measure on ˆHi. Letting θi be the principal angle be-
tween b and bi, i.e., the unique angle θi ∈ [0, π/2] such
that cos(θi) = |b(cid:62)bi|, and πHi : RD → Hi the orthogonal
projection onto Hi, we have for any x ∈ Hi that

b(cid:62)x = b(cid:62)πHi(x) = (πHi(b))(cid:62) x
= h(cid:62)

i,bx = sin(θi)ˆh

(cid:62)
i,bx,

(5)

(6)

with hi,b := πHi(b) and ˆhi,b := hi,b/ (cid:107)hi,b(cid:107)2. Hence,
(cid:20)(cid:90)

(cid:90)

(cid:21)

(cid:12)
(cid:12)b(cid:62)x
(cid:12)

(cid:12)
(cid:12)
(cid:12) dµ ˆHi

=

(cid:12)
(cid:12)
(cid:12)

ˆh

(cid:62)
i,bx

(cid:12)
(cid:12)
(cid:12) dµ ˆHi

sin(θi)

(7)

x∈ ˆHi
(cid:20)(cid:90)

=

x∈SD−2

x∈ ˆHi
(cid:21)

|x1|dµSD−2

sin(θi) = c sin(θi),

(8)

c being the average height of the unit hemisphere of RD−1.
=
We can now view the objective function of (2),

(cid:13)
(cid:13)X (cid:62)b
(cid:13)

(cid:13)
(cid:13)
(cid:13)1

n
(cid:88)

i=1

(cid:13)
(cid:13)X (cid:62)
(cid:13)
i b

(cid:13)
(cid:13)
(cid:13)1

=



Ni



n
(cid:88)

i=1

1
Ni

Ni(cid:88)

j=1

(cid:12)
(cid:12)b(cid:62)x(i)
(cid:12)

j

(cid:12)
(cid:12)
(cid:12)



 ,

(9)

Hyperplane Clustering via Dual Principal Component Pursuit

as an approximation via X of the function J (b) :=

1. If N1 = · · · = Nn, then B∗ = {±b1, . . . , ±bn}.

(cid:18)(cid:90)

n
(cid:88)

i=1

Ni

x∈ ˆHi

(cid:12)
(cid:12)b(cid:62)x
(cid:12)

(cid:12)
(cid:12)
(cid:12) dµ ˆHi

(cid:19)

(8)=

n
(cid:88)

i=1

Ni c sin(θi).

(10)

2. If N1 = · · · = N(cid:96) > N(cid:96)+1 ≥ · · · Nn, for some (cid:96) ∈

[n − 1], then B∗ = {±b1, . . . , ±b(cid:96)}.

In that sense, the continuous counterpart of problem (2) is

min
b∈SD−1

J (b) := N1 sin(θ1) + · · · + Nn sin(θn).

(11)

Note that sin(θi) is the distance between the line
spanned by b and the line spanned by bi.3 Hence,
every global minimizer b∗ of problem (11) minimizes
the sum of the weighted distances of Span(b∗) from
Span(b1), . . . , Span(bn), and thus represents a weighted
geometric median of these lines. Even though medians in
Riemmannian/Grassmannian manifolds are an active sub-
ject of research (Draper et al., 2014; Ghalieh & Hajja,
1996), we are not aware of any literature that studies (11).
The advantage of working with (11) instead of (2), is that
the global minimizers of (11) depend solely on the weights
Ni as well as on the geometry of the arrangement, captured
by the principal angles θij between bi and bj. In contrast,
the global minimizers of the discrete problem (2) will in
principle also depend on the distribution of the points X .
From that perspective, understanding when problem (11)
has as unique solution ±b1, is essential for understanding
the potential of (2) for hyperplane clustering. Towards that
end, we next provide a series of results pertaining to (11).4
The ﬁrst conﬁguration that we examine is that of two hy-
perplanes. In that case the weighted geometric median of
the two lines spanned by the normals to the hyperplanes
always corresponds to one of the two normals:

Proposition 3 Consider an arrangement of two hyper-
planes in RD with normal vectors b1, b2 and weights N1 ≥
N2. Then the set B∗ of global minimizers of (11) satisﬁes:

1. If N1 = N2, then B∗ = {±b1, ±b2}.

2. If N1 > N2, then B∗ = {±b1}.

When N1 > N2, problem (11) recovers the normal b1 to
the dominant hyperplane, irrespectively of how separated
the two hyperplanes are, since, according to Proposition
3, the principal angle θ12 between b1, b2 does not play a
role. The continuous problem (11) is equally favorable in
recovering normal vectors as global minimizers in another
extreme situation, where the arrangement consists of up to
D perfectly separated (orthogonal) hyperplanes:

Proposition 4 Consider n ≤ D hyperplanes in RD with
orthogonal normal vectors b1, . . . , bn, and weights N1 ≥
N2 ≥ · · · ≥ Nn. Then the set B∗ of global minimizers of
(11) can be characterized as follows:

3Recall that θi is a principal angle, i.e., θi ∈ [0, π/2].
4All proofs can be found at Tsakiris & Vidal 2017b.

Propositions 3 and 4 are not hard to prove, since for two hy-
perplanes the objective function is strictly concave, while
for orthogonal hyperplanes it is separable. In contrast, the
problem becomes harder for n > 2 arbitrary hyperplanes.
Even when n = 3, characterizing the global minimizers of
(11) as a function of the geometry and the weights seems
challenging. Nevertheless, when the three hyperplanes are
equiangular and their weights are equal, the symmetry of
the conﬁguration allows us to analytically characterize the
median as a function of the angle of the arrangement.

Proposition 5 Consider three hyperplanes of RD, with
normal vectors b1, b2, b3 s.t. b(cid:62)
i bj = cos(θ) > 0, i (cid:54)= j,
and N1 = N2 = N3. Then the set B∗ of global minimizers
of (11) satisﬁes the following phase transition:

1. If θ > 60◦, then B∗ = {±b1, ±b2, ±b3}.

2. If θ = 60◦, then B∗ = {±b1, ±b2, ±b3, ±µ}.

3. If θ < 60◦, then B∗ = {±µ},

where µ := (b1 + b2 + b3)/ (cid:107)b1 + b2 + b3(cid:107)2.

Proposition 5, whose proof uses nontrivial arguments from
spherical and algebraic geometry, is particularly enlighten-
ing, since it suggests that the global minimizers of (11) are
associated to the normals of the underlying arrangement
when the hyperplanes are sufﬁciently separated, while oth-
erwise they seem to be capturing the median hyperplane of
the arrangement. This is in striking similarity with the re-
sults regarding the Fermat point of planar and spherical tri-
angles (Ghalieh & Hajja, 1996). However, when the sym-
metry in Theorem 5 is removed, our proof technique no
longer applies, and the problem seems even harder. Even
so, one intuitively expects an interplay between the angles
and the weights of the arrangement under which, if the hy-
perplanes are sufﬁciently separated and H1 is sufﬁciently
dominant, then (11) should have a unique global minimizer
equal to b1. Our next theorem formalizes this intuition.

Theorem 6 Consider n ≥ 3 hyperplanes in RD, with
normals b1, . . . , bn of pairwise principal angles θij and
weights Ni. Deﬁne an (n − 1) × (n − 1) matrix Θ with
(i − 1, j − 1) entry given by NiNj cos(θij), 1 < i, j ≤ n,
and maximum eigenvalue σmax(Θ). If

(cid:112)

N1 >

α2 + β2, where

(12)

α :=

Ni sin(θ1i) −

N 2

i − σmax (Θ) ≥ 0,

(13)

(cid:88)

i>1

(cid:115)(cid:88)

i>1

Hyperplane Clustering via Dual Principal Component Pursuit

(cid:115)(cid:88)

β :=

N 2

i + 2

(cid:88)

i>1

i(cid:54)=j, i,j>1

Θi−1,j−1, and

(14)

γ := min
j(cid:54)=1

(cid:88)

i(cid:54)=j

(cid:88)

i>1

Ni sin(θij) −

Ni sin(θ1i) > 0,

(15)

then problem (11) admits a unique up to sign global mini-
mizer b∗ = ±b1.

Let us provide some intuition about the meaning of the
quantities α, β and γ in Theorem 6. To begin with, the
ﬁrst term in α is precisely equal to J (b1), while the sec-
ond term in α can be shown to be a lower bound on the
objective function N2 sin(θ2) + · · · + Nn sin(θn), if one
discards hyperplane H1. Moving on, the quantity β/N1
admits a nice geometric interpretation: cos−1 (β/N1) is a
lower bound on how small the principal angle of a critical
point b† (cid:54)= ±b1 from b1 can be. Interestingly, the larger
N1, the larger this minimum angle is, which shows that
critical hyperplanes H† that are distinct from H1, must be
sufﬁciently separated from H1. Finally, the second term
in γ is J (b1), while the ﬁrst term is the smallest objective
value that corresponds to b = bi, i > 1, and so (15) simply
guarantees that J (b1) < J (bi), ∀i > 1. Next, condition
N1 > (cid:112)α2 + β2 of Theorem 6 is easier to satisfy when
H1 is close to the rest of the hyperplanes (which leads to
small α), while the rest of the hyperplanes are sufﬁciently
separated5 (which leads to small α and small β). Regard-
i>1 Ni ≥ (cid:112)α2 + β2 and so if
less, one can show that
2 (cid:80)
N1 >
i>1 Ni, then any global minimizer of (11) has
to be one of the normals, irrespectively of what the angles
θij are. Finally, condition (15) is consistent with condi-
tion (12) in that it requires H1 to be close to Hi, ∀i > 1,
and Hi, Hj to be sufﬁciently separated for i, j > 1. Once
again, (15) can always be satisﬁed irrespectively of the θij,
by choosing N1 sufﬁciently large, since only the positive
term in the deﬁnition of γ depends on N1.

2 (cid:80)

√

√

3.2. Theoretical analysis of the discrete problem

We now study6 the discrete formulation of DPCP, i.e., prob-
lem (2), for the case where X = [X 1, . . . , X n]Γ, with X i
being Ni points in Hi, as described in §2.1. For any i ∈ [n]
and b ∈ SD−1, we can write the quantity

as

(cid:13)
(cid:13)X (cid:62)
(cid:13)
i b

(cid:13)
(cid:13)
(cid:13)1

Ni(cid:88)

j=1

(cid:12)
(cid:12)b(cid:62)x(i)
(cid:12)

j

(cid:12)
(cid:12) = b(cid:62)
(cid:12)

Ni(cid:88)

j=1

Sign

(cid:16)

(cid:17)

b(cid:62)x(i)
j

x(i)
j

(16)

= Ni b(cid:62)xi,b, xi,b :=

Sign

(cid:16)

(cid:17)

b(cid:62)x(i)
j

x(i)
j

(17)

1
Ni

Ni(cid:88)

j=1

5We emphasize that the interpretation of close and sufﬁciently

separated is relative to N1 and θ12, . . . , θ1n.

6More detailed arguments and proofs can be found in Tsakiris

& Vidal 2017b.

with xi,b being the average point of X i with respect to the
orthogonal projection hi,b := πHi(b) of b onto Hi. xi,b
can be viewed as an approximation to the vector integral

(cid:90)

x∈ ˆHi

Sign(b(cid:62)x)x dµ ˆHi

= c ˆhi,b.

(18)

This leads us to deﬁne the maximal approximation error
(cid:13)
(cid:13)
(cid:13)xi,b − c (cid:98)hi,b

(cid:15)i := max
b∈SD−1

(cid:13)
(cid:13)
(cid:13)2

,

(19)

as b ranges over the entire unit sphere SD−1. Intuitively, the
more uniformly distributed the points X i are inside ˆHi, the
smaller (cid:15)i is. This intuition can be formalized by means of
the spherical cap discrepancy (Grabner et al., 1997; Grab-
ner & Tichy, 1993) of X i, given by

SD(X i) := sup

(cid:17)

(cid:16)

x(i)
j

IC

− µ ˆHi

(C)

.

(20)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
Ni

Ni(cid:88)

j=1

C

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

In (20) the supremum is taken over all spherical caps C of
∼= SD−2, where a spherical cap is the inter-
the sphere ˆHi
section of SD−2 with a half-space of RD−1, and IC(·) is the
indicator function of C, which takes the value 1 inside C and
zero otherwise. SD(X i) is a deterministic measure of the
uniformity of the point set X i. By adjusting an argument
of Harman 2010, one can show that

√

(cid:15)i ≤

5SD(X i), ∀i ∈ [n],

(21)

which conﬁrms that uniformly distributed points X i cor-
respond to small (cid:15)i. We note here that SD(X i) decreases
with a rate of (Dick, 2014; Beck, 1984)

(cid:112)log(Ni)N

− 1
i

2 − 1

2(D−2)

.

(22)

To state the main theorem of this section (Theorem 8) we
need a deﬁnition.

Deﬁnition 7 For a set Y = [y1, . . . , yL] ⊂ SD−1 and
positive integer K ≤ L, deﬁne RY,K to be the maxi-
mum circumradius among the circumradii of all polytopes
(cid:110)(cid:80)K
, where j1, . . . , jK are dis-
tinct integers in [L], and the circumradius of a closed
bounded set is the minimum radius among all spheres that
contain the set. We now deﬁne the quantity of interest as

i=1 αji yji : αji ∈ [−1, 1]

(cid:111)

R :=

max
K1+···+Kn=D−1
0≤Ki≤D−2

n
(cid:88)

i=1

RX i,Ki.

(23)

We note that it is always the case that RX i,Ki ≤ Ki, with
this upper bound achieved when X i contains Ki colinear
points. Combining this fact with the constraint (cid:80)
i Ki =
D − 1 in (23), we get that R ≤ D − 1, and the more uni-
formly distributed are the points X inside the hyperplanes,
the smaller R is (even though R does not go to zero).

Hyperplane Clustering via Dual Principal Component Pursuit

Theorem 8 Let b∗ be a global minimizer of (2) with X =
[X 1, . . . , X n]Γ, and suppose that c >

2(cid:15)1. If

√

(cid:113)

N1 >

¯α2 + ¯β2, where
(cid:32)

¯α := α + c−1

(cid:15)1N1 + 2

(cid:15)iNi

, and

(25)

(cid:33)

(cid:88)

i>1

¯β := β + c−1 (cid:16)

R +

(cid:88)

(cid:17)

,

(cid:15)iNi

(24)

(26)

with α, β as in Theorem 6, and if

(cid:32)

¯γ := γ − c−1

(cid:15)1N1 + (cid:15)2N2 + 2

(cid:15)iNi

> 0,

(27)

(cid:33)

(cid:88)

i>2

then problem (2) has a unique minimizer ±b1.

(cid:112)

¯α2 + ¯β2, ¯γ >
Notice the similarity of conditions N1 >
0 of Theorem 8 with conditions N1 > (cid:112)α2 + β2, γ > 0
of Theorem 6. In fact ¯α > α, ¯β > β and ¯γ < γ, which im-
plies that the conditions of Theorem 8 are strictly stronger
than those of Theorem 6. This is no surprise since, as we
have already remarked, the global minimizers of (2) depend
not only on the geometry {θij} and the weights {Ni} of the
hyperplane arrangement, but also on the distribution of the
data points (parameters (cid:15)i and R). In contrast though to
condition (12) of Theorem 6, N1 now appears in both sides
of condition (24) of Theorem 8, which is however harm-
less: under the assumption c >
2(cid:15)1, (24) is equivalent
to the positivity of a quadratic polynomial in N1, whose
leading coefﬁcient is positive, and hence (24) can always
be satisﬁed for sufﬁciently large N1. Another interesting
connection of Theorem 6 to Theorem 8, is that assuming
limNi→∞ SD(X i) = 0, Theorem 6 can be seen as a limit
version of Theorem 8: dividing (24) and (27) by N1, letting
N1, . . . , Nn go to inﬁnity while keeping each ratio Ni/N1
ﬁxed, recalling that R ≤ D − 1, and noting that in view of
(21) we have limNi→∞ (cid:15)i = 0, we see that in the limit we
recover the conditions of Theorem 6.

√

Finally, a theorem of the same ﬂavor gives conditions under
which (3) converges in a ﬁnite number of iterations to b1 or
−b1; see Theorem 7 in Tsakiris & Vidal 2017b.

4. Algorithmic Contributions

4.1. DPCP via iteratively reweighted least squares

Sp¨ath & Watson 1987; Tsakiris & Vidal 2015a propose
solving the non-convex problem (2) by means of the re-
cursion of convex optimization problems (3), referred to as
DPCP-r. This is computationally equivalent to a recursion
of linear programs, which can be solved efﬁciently by an
optimized LP solver such as GUROBI. However, these lin-
ear programs are in principle not sparse, which may render

the running time of this approach prohibitive for big-data
applications. To alleviate this issue, we solve (2) by stan-
dard Iteratively Reweighted Least Squares (IRLS) applied
to (cid:96)1 minimization problems (Cand`es et al., 2008; Char-
trand & Yin, 2008; Daubechies et al., 2010; Lerman et al.,
2015). The resulting algorithm, referred to as DPCP-IRLS,
is dramatically faster than solving DPCP-r by GUROBI:
a MATLAB implementation on a standard MacBook Pro
with a dual core 2.5GHz processor and a total of 4GB cache
memory is able to handle 6000 points of R1000 in about
one minute, while in such a regime DPCP-r seems, as of
now, inapplicable. Moreover, the performance of DPCP-
IRLS, investigated in §5, suggests that DPCP-IRLS con-
verges most of the time to a global minimizer of (2); the
theoretical justiﬁcation of this claim is ongoing research.

4.2. Hyperplane clustering algorithms via DPCP

Sequential Hyperplane Learning (SHL) via DPCP.
Since at its core DPCP is a single subspace learning method
(Tsakiris & Vidal, 2015a), we may as well use it to learn n
hyperplanes in the same way that RANSAC (Fischler &
Bolles, 1981) is used: learn one hyperplane from the entire
dataset, remove the points close to it, then learn a second
hyperplane, remove the points close to it, and so on. The
main weakness of this approach is well known, and con-
sists of its sensitivity to a thresholding parameter, which is
necessary in order to be able to remove points.

1 xj

(cid:12)
(cid:12)b(cid:62)
(cid:12)

To alleviate the need of knowing a good threshold, we pro-
pose to replace the process of removing points by a process
of appropriately weighting the points. In particular, sup-
pose we solve the DPCP problem (2) on the entire dataset
X and obtain a unit (cid:96)2-norm vector b1. Now, instead of
removing the points of X that are close to the hyperplane
with normal vector b1 (which would require a threshold
parameter), we weight each and every point xj of X by its
(cid:12)
(cid:12)
distance
(cid:12) from that hyperplane. Then to compute a
second hyperplane with normal b2 we apply DPCP on the
(cid:111)
weighted dataset
. To compute a third hyper-
plane, the weight of point xj is chosen as the smallest dis-
tance of xj from the already computed two hyperplanes,
(cid:111)
i.e., DPCP is now applied to
.
After n hyperplanes have been computed, the clustering
of the points is obtained based on their distances to the n
hypeprlanes. We note here that the theoretical correctness
of this weighted sequential scheme does not follow auto-
matically from the theory presented in this paper, since the
latter applies only to unit (cid:96)2-norm points; studying DPCP
for weighted points is ongoing research.

(cid:110)(cid:12)
(cid:12)b(cid:62)
(cid:12)

mini=1,2

(cid:12)
(cid:12)
(cid:12) xj

(cid:12)
(cid:12)b(cid:62)
(cid:12)

1 xj

i xj

(cid:110)(cid:16)

xj

(cid:12)
(cid:12)
(cid:12)

(cid:17)

Iterative Hyperplane Learning (IHL) via DPCP. An-
other way to do hyperplane learning and clustering via
DPCP is to modify the classic K-Hyperplanes, which we

Hyperplane Clustering via Dual Principal Component Pursuit

will be referring to as IHL-SVD (Bradley & Mangasarian,
2000; Tseng, 2000; Zhang et al., 2009) (see §1.1) by com-
puting the normal vector of each cluster by DPCP, instead
of, e.g., SVD; see §5.2 for more details. The resulting al-
gorithm, IHL-DPCP, minimizes (up to a local minimum)
the sum of the distances of the points to the estimated hy-
perplane arrangement, which corresponds to replacing the
(cid:96)2-norm in the objective of IHL-SVD with the (cid:96)1-norm,
precisely as in the case of MKF (Zhang et al., 2009).

5. Experimental Evaluation

5.1. Experiments using synthetic data

We evaluate SHL-DPCP (§4) using synthetic data, and
compare it with similar algorithms, where instead of solv-
ing the DPCP problem (2) (either via DPCP-r or via DPCP-
IRLS), one uses REAPER or RANSAC.7 For fairness,
RANSAC does not remove any points as it sequentially
learns the hyperplanes, rather it selects them randomly us-
ing the probability distribution induced by weights deﬁned
in a similar way as in §4. Moreover, it is conﬁgured to
run at least as long as DPCP-r, which uses a maximum
of 20 iterations in (3), while REAPER and DPCP-IRLS
use a maximum of 100 iterations and convergence accu-
racy 10−3. The ambient dimension is set to D = 4, 9, 30,
as inspired by major applications where hyperplane ar-
rangements appear, e.g., D = 4 in 3D point cloud anal-
ysis (in homogeneous coordinates), and D = 9 in two-
view geometry (Cheng et al., 2015). For each choice of D
we randomly generate n = 2, 3, 4 hyperplanes and sam-
ple them as follows. Given n, we set N = 300n, with
Ni = αi−1Ni−1, i > 1, where α ∈ (0, 1] is a parameter
that controls the balancing of the clusters: α = 1 means
the clusters are perfectly balanced, while smaller values
of α lead to less balanced clusters. We set α = 0.6 (for
α = 0.8, 1 see Tsakiris & Vidal 2017b). Each cluster is
sampled from a zero-mean unit-variance Gaussian distri-
bution with support in the corresponding hyperplane. To
make the experiment more realistic, we corrupt points from
each hyperplane by adding white Gaussian noise of devia-
tion σ = 0.01 with support in the direction orthogonal to
the hyperplane. Moreover, we corrupt the dataset by adding
M/(M + N ) = 10% outliers sampled from a standard
zero-mean unit-variance Gaussian distribution supported in
the ambient space, where M is the number of outliers.

The left column of Figure 1 plots the clustering accuracy
over 50 independent experiments as a function of the rela-
tive dimension (D − 1)/D and the number of hyperplanes
n. As expected, the performance degrades as either the rel-

7We have compared with methods such as SCC or MKF, how-
ever we do not report on these methods since they perform signif-
icantly more inferior to RANSAC, REAPER or DPCP.

ative dimension or the number of hyperplanes increases.
There are at least two interesting things to notice. First,
RANSAC is the best method when D = 4 irrespectively of
the number of hyperplanes, since for such a low ambient di-
mension the probability that D − 1 = 3 randomly selected
points lie in the same hyperplane is very high. Indeed, for
D = 4 RANSAC’s accuracy ranges from 99% (n = 2) to
97% (n = 4), as opposed to (for n = 4) REAPER (56%) or
even DPCP-IRLS (89%) and DPCP-r (94%). On the other
hand, DPCP-r is overall the best method with an 86% accu-
racy in the challenging scenario (D −1)/D = 0.97, n = 4,
as opposed to 81% for DPCP-IRLS, 42% for REAPER and
28% for RANSAC. The right column of Figure 1 plots the
clustering accuracy as a function of n and of the percent-
age of outliers, for D = 9 and additive noise as before.
Evidently, DPCP-r and DPCP-IRLS are the best methods,
with, e.g., a clustering accuracy of 97% and 94% respec-
tively for n = 2 and 50% outliers, as opposed to 66% for
RANSAC and 74% for REAPER.

5.2. Experiments using real kinect data

In this section we explore various hyperplane clustering al-
gorithms using the benchmark dataset NYUdepthV2 (Sil-
berman et al., 2012). This dataset consists of 1449 RGBd
data instances acquired using the Microsoft kinect sensor.
Each instance corresponds to an indoor scene, and consists
of the 480 × 640 × 3 RGB data together with depth data
for each pixel. The depth data can be used to reconstruct a
3D point cloud associated to the scene. In this experiment
we use such 3D point clouds to learn plane arrangements
and segment the pixels of the corresponding images based
on their plane membership. This is an important problem
in robotics, where estimating the geometry of a scene is
essential for successful robot navigation.

In such 3D applications RANSAC is the predominant state-
of-the-art method, since the probability of sampling three
points from the dominant plane is very large. Thus we com-
pare a sequential hyperplane learning RANSAC algorithm
(SHL-RANSAC), which uses a threshold (0.1, 0.01, 0.001)
for removing points, to iterative K-Hyperplane-like algo-
rithms based on SVD, REAPER, RANSAC and DPCP-
IRLS, to be referred to as IHL-SVD, IHL-REAPER, and
so on. These algorithms randomly initialize n hyperplanes,
they cluster the points according to their distance to these
hyperplanes, they reﬁne the hyperplanes by ﬁtting a new
hyperplane at each cluster, re-assign points based on the
new hyperplanes, and so on, until the objective function
converges or 100 iterations are reached. We use 10 inde-
pendent restarts, and we control the running time of SHL-
RANSAC and IHL-RANSAC to be not less than that of
IHL-DPCP-IRLS.

The algorithms do not operate on the raw 3D data, rather on
standard superpixel representations of the data, where each

Hyperplane Clustering via Dual Principal Component Pursuit

Table 1. Clustering error in % of 3D planes from Kinect data
without (CRF(0)) and with (CRF(1)) spatial smoothing.

method
SHL-RANSAC
IHL-RANSAC
IHL-SVD
IHL-REAPER
IHL-DPCP-IRLS

n = 4
CRF(0) CRF(1)
14.07
22.78
10.71
16.80
14.40
21.85
13.71
20.94
13.64
20.77

n = 9
CRF(0) CRF(1)
17.47
29.42
22.78 14.24
16.71
26.22
16.27
25.52
16.10
25.38

pairwise potentials, yj ∈ {1, . . . , n} is the plane label of
3D point xj, which is the variable to optimize over, CBj,k
is the length of the common boundary between superpixels
j, and k and Nj indexes the neighbors of xj. The param-
eter λ in (28) is set to the inverse of twice the maximal
row-sum of the pairwise matrix, in order to achieve a bal-
ance between unary and pairwise terms. Minimization of
(28) is done via Graph-Cuts (Boykov et al., 2001).

Since NYUdpethV2 does not come with a ground truth an-
notation based on plane membership, we manually anno-
tated 92 of the 1449 scenes in the dataset, in which dom-
inant planes such as ﬂoors, walls, ceilings, tables and so
on are present. Table 1 shows the clustering errors of var-
ious algorithms on these 92 annotated scenes for the iden-
tiﬁcation of the ﬁrst n dominant planes of each scene8,
where for SHL-RANSAC the error is averaged over the
three different choices of a threshold. As expected, the
clustering error increases for all methods as the number of
planes to be identiﬁed increases. Again as expected, the
performance of all algorithms improves signiﬁcantly if one
includes spatial smoothing. Notice that the best method
is IHL-RANSAC, and not the sequential SHL-RANSAC,
which seems a rather interesting ﬁnding. On the other hand,
the rest of the methods seem to perform similarly to each
other, with IHL-SVD being slightly inferior, since it is less
robust to outliers, and IHL-DPCP-IRLS being overall the
second best method.

6. Conclusions

In this paper we extended the framework of Dual Principal
Component Pursuit (DPCP) to the case of data lying in a
union of hyperplanes. We provided theoretical conditions
under which the normal vector of the dominant hyperplane
is the unique global minimizer of the non-convex (cid:96)1 DPCP
optimization problem. Moreover, we proposed a fast im-
plementation of DPCP, as well as DPCP-based hyperplane
clustering algorithms, which were shown to outperform or
be competitive to state-of-the-art algorithms.

(a) RANSAC

(b) RANSAC

(c) REAPER

(d) REAPER

(e) DPCP-IRLS

(f) DPCP-IRLS

(g) DPCP-r

(h) DPCP-r

Figure 1. Sequential hyperplane learning: Left (right) column
shows clustering accuracy (white corresponds to 1, black to 0)
as a function of the number of hyperplanes n and the relative di-
mension d/D (percentage M/(N + M ) of outliers).

superpixel is represented by its median 3D point, weighted
by the size of the superpixel. Moreover, since 3D planes in
an indoor seen usually do not pass through a common ori-
gin, the algorithms work with homogeneous coordinates.
Finally, the algorithms as described so far are purely geo-
metric, in the sense that they do not take into account the
spatial coherence of the 3D point cloud (nearby points are
likely to lie in the same plane), and so we expect their out-
put segmentation to be spatially incoherent. To associate a
spatially smooth image segmentation to each algorithm, we
use the normal vectors b1, . . . , bn that the algorithm pro-
duced to minimize a Conditional-Random-Field (Sutton &
McCallum, 2006) energy function E(y1, . . . , yN ) :=

N
(cid:88)

j=1

|b(cid:62)

yj xj| + λ

CBj,k exp

−

(cid:88)

k∈Nj

(cid:18)

(cid:19)

(cid:107)xj − xk(cid:107)2
2
2σ2
d

δ(yj (cid:54)= yk).

In (28) the ﬁrst and second terms are known as unary and

(28)

8If the scene has m < n annotated planes, then the cluster-
ing error is computed only with respect to the ﬁrst m dominant
clusters identiﬁed by the algorithm.

234n0.750.890.97d/D234n0.10.30.5M/(N+M)234n0.750.890.97d/D234n0.10.30.5M/(N+M)234n0.750.890.97d/D234n0.10.30.5M/(N+M)234n0.750.890.97d/D234n0.10.30.5M/(N+M)Hyperplane Clustering via Dual Principal Component Pursuit

Acknowledgements

This work was supported by NSF grants 1447822 and
1618637. The ﬁrst author thanks Dr. Glyn Harman for his
help in deriving equation (21), and Dr. Bijan Afsari for in-
teresting conversations on geometric medians in nonlinear
manifolds. The authors also thank two of the anonymous
reviewers for providing constructive comments.

References

Bako, L.

Identiﬁcation of switched linear systems via
sparse optimization. Automatica, 47(4):668–677, 2011.

Beck, J. Sums of distances between points on a sphere—an
application of the theory of irregularities of distribution
to discrete geometry. Mathematika, 31(01):33–41, 1984.

Boykov, Y., Veksler, O., and Zabih, R. Fast approximate
energy minimization via graph cuts. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 23(11):
1222–1239, 2001.

Bradley, P. S. and Mangasarian, O. L. k-plane cluster-
ing. Journal of Global Optimization, 16(1):23–32, 2000.
ISSN 0925-5001.

Cand`es, E., Wakin, M., and Boyd, S. Enhancing sparsity by
reweighted (cid:96)1 minimization. Journal of Fourier Analysis
and Applications, 14(5):877–905, 2008.

Chartrand, R. and Yin, W.

Iteratively reweighted algo-
rithms for compressive sensing. In 2008 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Pro-
cessing, pp. 3869–3872. IEEE, 2008.

Chen, G. and Lerman, G. Spectral curvature clustering
International Journal of Computer Vision, 81

(SCC).
(3):317–330, 2009. ISSN 0920-5691.

Cheng, Y., Lopez, J. A., Camps, O., and Sznaier, M. A con-
vex optimization approach to robust fundamental ma-
trix estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 2170–
2178, 2015.

Daubechies, I., DeVore, R., Fornasier, M., and G¨unt¨urk,
C. S. Iteratively reweighted least squares minimization
for sparse recovery. Communications on Pure and Ap-
plied Mathematics, 63(1):1–38, 2010.

Dick, J. Applications of geometric discrepancy in numeri-
cal analysis and statistics. Applied Algebra and Number
Theory, 2014.

Draper, B., Kirby, M., Marks, J., Marrinan, T., and Peter-
son, C. A ﬂag representation for ﬁnite collections of
subspaces of mixed dimensions. Linear Algebra and its
Applications, 451:15–32, 2014.

Elhamifar, E. and Vidal, R. Sparse subspace clustering:
Algorithm, theory, and applications. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 35(11):
2765–2781, 2013.

Fischler, M. A. and Bolles, R. C. RANSAC random sam-
ple consensus: A paradigm for model ﬁtting with ap-
plications to image analysis and automated cartography.
Communications of the ACM, 26:381–395, 1981.

Ghalieh, K. and Hajja, M. The fermat point of a spherical
triangle. The Mathematical Gazette, 80(489):561–564,
1996.

Grabner, P. J. and Tichy, R.F. Spherical designs, dis-
crepancy and numerical integration. Math. Comp., 60
(201):327–336, 1993. ISSN 0025-5718. doi: 10.2307/
2153170. URL http://dx.doi.org/10.2307/
2153170.

Grabner, P. J., Klinger, B., and Tichy, R.F. Discrepancies of
point sequences on the sphere and numerical integration.
Mathematical Research, 101:95–112, 1997.

Harman, G. Variations on the koksma-hlawka inequality.

Uniform Distribution Theory, 5(1):65–78, 2010.

Lerman, G., McCoy, M. B., Tropp, J. A., and Zhang, T.
Robust computation of linear models by convex relax-
ation. Foundations of Computational Mathematics, 15
(2):363–410, 2015.

Liu, G., Lin, Z., Yan, S., Sun, J., and Ma, Y. Robust recov-
ery of subspace structures by low-rank representation.
IEEE Transactions on Pattern Analysis and Machine In-
telligence, 35(1):171–184, Jan 2013.

Lu, C-Y., Min, H., Zhao, Z-Q., Zhu, L., Huang, D-S., and
Yan, S. Robust and efﬁcient subspace segmentation via
In European Conference on
least squares regression.
Computer Vision, 2012.

Qu, Q., Sun, J., and Wright, J. Finding a sparse vector in
a subspace: Linear sparsity using alternating directions.
In Advances in Neural Information Processing Systems,
pp. 3401–3409, 2014.

Sampath, A. and Shan, J. Segmentation and reconstruc-
tion of polyhedral building roofs from aerial lidar point
clouds. Geoscience and Remote Sensing, IEEE Transac-
tions on, 48(3):1554–1567, 2010.

Silberman, N., Kohli, P., Hoiem, D., and Fergus, R. Indoor
segmentation and support inference from rgbd images.
In European Conference on Computer Vision, 2012.

Sp¨ath, H. and Watson, G.A. On orthogonal linear (cid:96)1 ap-
proximation. Numerische Mathematik, 51(5):531–543,
1987.

Hyperplane Clustering via Dual Principal Component Pursuit

Spielman, D.A., Wang, H., and Wright, J. Exact recovery
of sparsely-used dictionaries. In Proceedings of the 23d
international joint conference on Artiﬁcial Intelligence,
pp. 3087–3090. AAAI Press, 2013.

You, C., Li, C.-G., Robinson, D., and Vidal, R. Oracle
based active set algorithm for scalable elastic net sub-
space clustering. In IEEE Conference on Computer Vi-
sion and Pattern Recognition, pp. 3928–3937, 2016.

Zhang, T., Szlam, A., and Lerman, G. Median k-ﬂats for
hybrid linear modeling with many outliers. In Workshop
on Subspace Methods, pp. 234–241, 2009.

Sun, J., Qu, Q., and Wright, J. Complete dictionary re-
covery using nonconvex optimization. In Proceedings of
the 32nd International Conference on Machine Learning
(ICML-15), pp. 2351–2360, 2015.

Sutton, C. and McCallum, A. An introduction to condi-
tional random ﬁelds for relational learning, volume 2.
Introduction to statistical relational learning. MIT Press,
2006.

Tsakiris, M. C. and Vidal, R. Dual principal component

pursuit. arXiv:1510.04390v2 [cs.CV], 2017a.

Tsakiris, M. C. and Vidal, R. Hyperplane clustering via
arXiv:1706.01604

dual principal component pursuit.
[cs.CV], 2017b.

Tsakiris, M. C. and Vidal, R. Filtrated algebraic subspace
clustering. SIAM Journal on Imaging Sciences, 10(1):
372–415, 2017c.

Tsakiris, M.C. and Vidal, R. Dual principal component pur-
suit. In ICCV Workshop on Robust Subspace Learning
and Computer Vision, pp. 10–18, 2015a.

Tsakiris, M.C. and Vidal, R. Filtrated spectral algebraic
subspace clustering. In ICCV Workshop on Robust Sub-
space Learning and Computer Vision, pp. 28–36, 2015b.

Tseng, P. Nearest q-ﬂat to m points. Journal of Optimiza-
tion Theory and Applications, 105(1):249–252, 2000.

Vidal, R., Soatto, S., Ma, Y., and Sastry, S. An algebraic
geometric approach to the identiﬁcation of a class of lin-
ear hybrid systems. In IEEE Conference on Decision and
Control, pp. 167–172, 2003.

Vidal, R., Ma, Y., and Sastry, S. Generalized Principal
IEEE Transactions on
Component Analysis (GPCA).
Pattern Analysis and Machine Intelligence, 27(12):1–15,
2005.

Vidal, R., Ma, Y., Soatto, S., and Sastry, S. Two-view
multibody structure from motion. International Journal
of Computer Vision, 68(1):7–25, 2006.

Vidal, R., Ma, Y., and Sastry, S. Generalized Principal

Component Analysis. Springer Verlag, 2016.

Wang, Y-X., Xu, H., and Leng, C. Provable subspace clus-
tering: When LRR meets SSC. In Neural Information
Processing Systems, 2013.

