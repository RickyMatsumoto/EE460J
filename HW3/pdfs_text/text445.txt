Understanding the Representation and Computation of Multilayer
Perceptrons: A Case Study in Speech Recognition

Tasha Nagamine 1 Nima Mesgarani 1

Abstract

Despite the recent success of deep learning, the
nature of the transformations they apply to the
input features remains poorly understood. This
study provides an empirical framework to study
the encoding properties of node activations in
various layers of the network, and to construct
the exact function applied to each data point in
the form of a linear transform. These methods are
used to discern and quantify properties of feed-
forward neural networks trained to map acoustic
features to phoneme labels. We show a selec-
tive and nonlinear warping of the feature space,
achieved by forming prototypical functions to ac-
count for the possible variation of each class.
This study provides a joint framework where the
properties of node activations and the functions
implemented by the network can be linked to-
gether.

1. Introduction

In recent years, deep learning has achieved remarkable
performance on a variety of tasks in machine learning
(D. Andor & Collins, 2016; D. Silver & Hassabis, 2016;
K. He & Sun, 2016), including automatic speech recog-
nition (G. E. Dahl & Acero, 2012; G. Hinton & Kings-
bury, 2012; A.-R. Mohamed & Hinton, 2010; W. Xiong
& Zweig, 2016). Despite these successes, our understand-
ing of deep neural networks (DNNs) and the nature of their
computation and representations lags far behind these per-
formance gains. This has motivated a number of recent
studies aimed at better understanding the computational
principles of deep learning in the hope of gaining intuitions
that may lead to improved models.

It has been established that networks with at least one

1Columbia University, New York, NY, USA. Correspondence

to: Nima Mesgarani <nima@ee.columbia.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

hidden layer are universal approximators (Cybenko, 1989;
K. Hornik & White, 1989). Several recent theoretical stud-
ies have proven that deeper architectures are able to more
efﬁciently solve problems than shallow models, given a
limited number of parameters (Eldan & Shamir, 2015; Lin
& Tegmark, 2016). Other studies have focused on networks
with rectiﬁed linear units, and have shown that deeper net-
works are able to learn more complex functions by splitting
the input space into exponentially more linear response re-
gions than equivalent shallow models (G. Montufar & Ben-
gio, 2014; R. Pascanu & Bengio, 2013). Finally, recent suc-
cesses using deep residual networks and subsequent anal-
yses show the effectiveness of extremely deep represen-
tations in supervised learning tasks (K. He & Sun, 2016;
A. Veit & Belongie, 2016).

At the same time, there is a growing body of empirical stud-
ies that aim to understand the behavior of neural networks
by developing mathematical methods and techniques for
visualization (Zeiler & Fergus, 2013; J. Yosinski & Lipson,
2016), as well as studies of instability in the face of adver-
sarial examples (C. Szegedy & Fergus, 2013; A. Nguyen &
Clune, 2015) and contraction and separation properties of
these models (Mallat, 2016). In the ﬁeld of speech recogni-
tion, several studies explored the representations of speech
learned by feed-forward networks used in acoustic model-
ing (A.-R. Mohamed & Penn, 2012; T. Nagamine & Mes-
garani, 2015; 2016). Finally, an architecture-independent
method was proposed to summarize the complexity of the
parameters learned in feed-forward networks (S. Wang &
Aslan, 2016).

In this study, we aim to bridge the gap between these theo-
retical and empirical studies by providing methods for an-
alyzing both the properties of activations in each layer of
the network and estimating the exact linear function that is
applied to each data point. We discern and quantify prop-
erties of the network representation and computation to de-
termine 1) what aspects of the feature space are encoded
in different layers (internal representation) and 2) how the
network transforms the feature space to achieve catego-
rization (network function). Our method thus provides a
joint framework in which the properties of node activations
are directly linked to the properties of the function that is

Visualizing and Understanding Multilayer Perceptron Models: A Case Study in Speech Processing

learned, which stands in contrast to previous studies that
focus on a speciﬁc property of the network. These meth-
ods are easily extensible to other applications for MLPs, as
well as other feed-forward network architectures.

2. Methods

2.1. Deep neural network acoustic models

Neural network acoustic models for phone recognition
were trained on the Wall Street Journal speech corpus
(J. Garofolo & Pallett, 1993; 1994). The corpus includes
approximately 80 hours of read speech training data. We
report test accuracies using the eval93 set. Input features to
all models were extracted using the Kaldi speech recogni-
tion toolkit (D. Povey & Vesely, 2011) and consisted of 23-
dimensional log-Mel ﬁlter bank coefﬁcients (25 ms win-
dow, 10 ms frame rate) with applied mean and variance nor-
malization, spliced over 11 context frames. Models were
trained on HMM state targets consisting of monophones
with one state per phone (40 output states, corresponding
to 39 phonemes and one silence state). All alignments for
training were obtained using the WSJ s5 recipe in Kaldi.

To ensure the generality of our ﬁndings, we explored two
network architectures using 256 and 2048 nodes per hid-
den layer with ﬁve hidden layers. In this work, we analyze
models using rectiﬁed linear units (ReLUs) because it has
been shown that ReLU networks are faster to train, are less
sensitive to initialization than sigmoid units, and improve
performance in speech recognition tasks (G. E. Dahl &
Hinton, 2013; A. L. Maas & Ng, 2013; M. D. Zeiler & Hin-
ton, 2013). For the networks with 2048 nodes per hidden
layer, we adopted batch normalization (Ioffe & Szegedy,
2015) and dropout (N. Srivastava & Salakhutdinov, 2014)
as regularization methods, with a 20% dropout rate on input
and hidden layers. All models were trained using Theano
(J. Bergstra & Bengio, 2010). Hyperparameters were de-
termined using grid search. Networks with 256 and 2048
nodes per hidden layer were trained with 25 and 100 epochs
of backpropagation, respectively. Because the goal of this
study is to examine the transformation from features to
phone posteriors of MLP acoustic models (not language
models), we report performance using frame-wise classi-
ﬁcation accuracy rather than word error rate.

2.2. Analyzing the learned network functions

To analyze and compare neural networks, we extended the
methods used in (S. Wang & Aslan, 2016) to construct an
extended data Jacobian matrix (EDJM) for the nodes in a
neural network. The basic concept underlying the EDJM
is that a neural network with layers of subsequent nonlin-
ear transformations can be mathematically reproduced by
ﬁnding a per-data point linear system mapping input to out-

Table 1. Frame-wise accuracy of acoustic models on WSJ dataset.

LAYERS NODES

5
5

256
2048

TR284
EVAL93
DEV93
81.43% 80.45% 80.56%
84.53% 81.70% 81.91%

put. More formally, consider a dataset D consisting of in-
puts X = [x1, ..., xN ] and outputs Y = [y1, ..., yN ]. For
a deep neural network model with λ layers and arbitrary
nonlinear function φ mapping inputs xn to predicted output
ˆyn = φλ(W λφλ−1(W λ−1φλ−2(. . . W 1xn))), the data Ja-
cobian matrix (DJM) of any output unit can be constructed
by ﬁnding the gradient of the node with respect to each of
the inputs using the following equation:

DJMθ (xi) =

∂h1
i
∂xi

=

...

∂ ˆyi
∂hλ−1
i
(cid:1)

∂ ˆyi
∂xi
∂φλ (cid:0)hλ−1
i
∂hl−1
i

∂hλ−1
i
∂hλ−2
i
∂hλ−1
i
∂φλ−1 (cid:0)hλ−2

i

=

(1)

(cid:1) ...

∂φ1 (xi)
∂xi

.

Here, h(cid:96)
i represents the activation of layer (cid:96) for data point
i, while θ indicates dependence on network parameters.
Now assuming a linear output function and rectiﬁed linear
units (ReLU) with a nonlinearity of the form φReLU (z) =
max (0, z), where z is the weighted input to the node, this
equation can be written:

DJMθ (xi) = W λ

λ−1

∂hλ−1
i
λ−2 hλ−2
∂W λ−1

i

W λ−1
λ−2

∂hλ−2
i
λ−3 hλ−3
∂W λ−2

i

...W 1
IN.

(2)

This method can be easily extended to any node in any hid-
den layer (cid:96) < λ by replacing λ with (cid:96) in Eq. 2. Intuitively,
this is equivalent to ﬁnding the unique linear mapping func-
tion ˆW for a network node for each individual data point.
Using the piece-wise linear property of the rectiﬁed linear
function, the previous equation can be easily simpliﬁed be-
cause taking the gradient with respect to any node activa-
tion is equivalent to setting rows of the weight matrix to 0
for zero-activation data points:
W (cid:96)
0,

if h(cid:96)
otherwise.

(cid:96)−1 (xi) [m, n] =

(cid:96)−1 [m, n] ,

i [m] > 0

ˆW (cid:96)

(3)

(cid:40)

Thus, for a feed-forward ReLU network, ﬁnding the DJM
for point i for a given node is mathematically equivalent to
setting selected rows of each weight matrix to 0 and doing
a simple matrix multiplication:

ˆyi = W λ

λ−1φ

λ−2 φ (cid:0). . . W 1

(cid:16)

W λ−1
(cid:16)
(cid:16) ˆW λ−1

λ−2

(cid:1)(cid:17)

INxi
(cid:17)(cid:17)

= ˆW λ

λ−1

. . . ˆW 1

INxi

(4)

= DJMθ (xi) .

Calculating the EDJM for the whole dataset over ev-
ery node in a layer results in a tensor of dimension
[N × din × dout], where for each data point i ∈ [1, ..., N ],
we have constructed a linear map from input to output. This
method, outlined in Eq. 1, is applicable to any network

Visualizing and Understanding Multilayer Perceptron Models: A Case Study in Speech Processing

Figure 1. For a single data point xi, given a network with Re-
LUs and a linear output, the sample-dependent linear transform
SLT(xi) can be found by setting rows of the weight matrices to
zero according to Eq. 3.

with a nonlinearity that is differentiable with respect to its
inputs (e.g., sigmoid and hyperbolic tangent), in which the
gradients can be interpreted as linear approximations to the
network function for a given data point. For clarity, for
the rest of this paper we will refer to the EDJM of individ-
ual nodes as the sample-dependent linear transform (SLT).
The SLT for a given dataset has a ﬁxed dimensionality in-
dependent of network architecture, providing a useful tool
for comparing different networks.

2.3. Quantifying complexity with SVD

In a neural network, both activations and the SLT of nodes
can be written as a two-dimensional matrix of the form
M = (mi,j) ∈ RN ×F . In the case of activations H(cid:96) in
layer (cid:96), N is the number of data points in dataset D, and F
is dimensionality of the layer. In the case of the SLT, we
perform decompositions on the SLT of individual nodes,
where F is the dimension of the inputs. Given a 2D ma-
trix, we can then perform matrix factorization using singu-
lar value decomposition (SVD) of the form M = UΣV∗.

The singular values (sorted by decreasing order) of the di-
agonal of Σ deﬁne the weights given to the orthonormal
basis functions deﬁned by U and V. Because the ﬁrst di-
mension of the activation matrix is deﬁned over data points,
the singular values of SLT(cid:96)
m can serve thus as a metric of
the complexity (diversity) of the learned network function.
Consider the case where rank(SLT(cid:96)
m) = 1. This means
that for node m in layer (cid:96), the linear system from input to
output is the same for all data points and the function for
this node is linear. At the other extreme, a uniform distri-
bution of singular values suggests that the function learned
for each data point is drastically different. Thus, the relative
values of the SVD spectra for SLT matrices can serve as a
metric of nonlinearity for the system. Similarly, the distri-
bution of singular values of H(cid:96) indicate how uniformly the
nodes within a layer respond to different data points.
In general, the matrices H(cid:96) and SLT(cid:96)
n are full-rank. Thus,
we quantify the shape of the distribution of the SVD
spectrum by normalizing by its maximum value, where
max (Σ) = σ1, and computing the area under the curve
(AUC). We can compute this score for the ﬁrst D singular
values σ of the spectrum using the following equation:

Figure 2. Unsupervised hierarchical clustering of activations in
each hidden layer across classes (rows) and nodes (columns, 2048
nodes/layer) for hidden layers 1 (A), 3 (B), and 5 (C). (D) Number
of clusters of nodes (columns) in each hidden layer based on a dis-
tance cutoff criterion (d < 0.5, correlation distance) for networks
with 256 and 2048 nodes/layer.

AUC =

1
max (Σ)

σD − σ1
2D

D
(cid:88)

i=1

(σi+1 + σi)

(5)

Because the AUC is computed after normalizing the spec-
trum so that its maximum value is 1, intuitively, this metric
quantiﬁes the non-uniformity of the singular values of the
spectrum. Larger values signify greater uniformity in the
distribution of singular values and thus a higher degree of
complexity.

3. Results: Analysis of node activations

We begin by considering the network activations H, which
can be interpreted as nonlinearly transformed feature repre-
sentations of the network inputs. To characterize this repre-
sentation, we studied both the properties of the activations
of individual nodes (local encoding) and the population of
nodes in each layer (global encoding).

3.1. Visualizing the global feature encoding

In a supervised learning task, hidden layers of a neural net-
work extract representations with meaningful, class-based
distinctions. To study these distinctions, for each hidden
layer, we grouped node activations to the WSJ eval93 set
by their corresponding label. To quantify the overall pat-
tern of selectivity in hidden layer (cid:96) with M nodes, we com-
pute the average activation hm for every node m for each
distinct label k ∈ {1, ..., K}. This results in a vector of
dimension [K × 1] for each node that characterizes its aver-

xiyi = SLT(xi) xiWIN1W1 2W2OUT^^^............time (s)00.11SLT(xi)frequencySLT(xi) = WIN(xi) W1 (xi) W2    (xi)1^2^OUT^..^.Visualizing and Understanding Multilayer Perceptron Models: A Case Study in Speech Processing

Table 2. Classiﬁcation accuracy using linear discriminant analysis
(LDA) trained on the WSJ dev93 set.

LAYER
FEATS
HL1
HL2
HL3
HL4
HL5

NNODES = 256

NNODES = 2048

DEV93

EVAL93

DEV93
EVAL93
60.18% 56.04% 60.18% 56.04%
67.38% 64.40% 74.58% 68.75%
70.15% 67.83% 77.34% 72.08%
72.68% 70.59% 79.88% 75.65%
75.06% 73.63% 81.81% 78.63%
76.78% 75.65% 82.70% 80.04%

1
Nk

age response to each class, which we call a class selectivity
vector (CSV), where each element is calculated as follows:
Nk(cid:88)

hm (xi) ∀(yi = k).

(6)

CSVm[k] =

i=1
Concatenating the CSVs into a matrix of dimension [K ×
M ] summarizes the overall selectivity of a hidden layer,
where rows correspond to classes and columns correspond
to individual nodes. Examining this matrix reveals pat-
terns in both the individual node and population coding of
classes within a layer. To visualize these selectivity pat-
terns, we performed an unsupervised hierarchical cluster-
ing on columns (nodes) based on the similarity of their
CSVs, and rows (phonemes) based on the similarity of their
overall activation pattern over nodes in a layer (UPGMA,
correlation distance) in hidden layers 1, 3, and 5 of the
2048 node/layer network (Fig. 2A-C). For visualization
purposes, the dendrogram showing clusters of nodes was
truncated at a set cutoff distance of 0.5.

Examining these vectors reveals nodes with various types
of response proﬁle, including selectivity to both individual
classes and groups of classes with shared features. For ex-
ample, each layer has a group of nodes dedicated to encod-
ing only silence (by far the most common label in the train-
ing set). The remaining classes, corresponding to speech
sounds, tend to be encoded jointly, with groups of nodes
encoding phoneme classes are predominantly organized by
phonetic features such as manner and place of articulation.
However, other nodes exhibit selectivity patterns not easily
explained by acoustic or phonetic feature distinctions.

We observe that in deeper layers, individual nodes tend
to become less broadly selective to classes, evidenced by
sparser selectivity patterns in their CSVs. The result is that
in deeper representations, the average activations of nodes
to individual classes tend to become differentiated from one
another. This is quantiﬁed in the top dendrograms showing
the clustering of nodes in Fig. 2A-C, where we observe
that the use of a distance cutoff criterion (correlation dis-
tance, d > 0.5) results in a larger number of branches in
each subsequent hidden layer. This holds true for networks
with 256 nodes and 2048 nodes per layer (Fig. 2D). This
shows that neural networks are successful at using progres-
sive hidden layer representations to decorrelate their inputs

Figure 3. (A) Explained variance ratio (EVR) for LDA model
trained on features and activations from hidden layers. (B) SVD
spectra for LDA-transformed activations, separated by class and
averaged. (C) AUC for spectra in (A) and (B) for networks with
256 and 2048 nodes/layer.

in order to extract useful features for classiﬁcation.

3.2. Deep representations are more class-discriminable

Next, we wanted to directly examine the discriminability
of classes in each layer of the network. Measuring the dis-
tance between classes directly from the activations, how-
ever, may produced biased results. Between networks and
even within networks, activations in different layers suf-
fer from the problem that they 1) may have different di-
mensionalities, or 2) are drawn from different distributions
(e.g., layers may exhibit differing amounts of sparsity, or
contain “dead” nodes).

To remedy this, we applied linear discriminant analysis
(LDA) as a supervised dimensionality-reduction technique
(SVD solver) on input features and hidden layer activa-
tions. Doing so allows us to project hidden representations
of a layer H(cid:96) onto a set of K − 1 orthonormal bases (cid:101)H(cid:96)
that maximally retain class discriminability and are more
easily comparable between layers and networks. Due to
the large size of the training set, we used the WSJ dev93 to
train the LDA models, noting that the development set was
not used in the training of any of the neural network mod-
els. Table 2 shows the classiﬁcation accuracy using LDA
on input features and hidden layer activations for networks
with 256 and 2048 nodes/layer. We observe two main pat-
terns. First, for both networks, representations of classes
are more separable in each subsequent hidden layer. Sec-
ond, given layers of equal depth, class discriminability is
greater for the larger network.

By examining the ratio of explained variance of each LDA
model, we can see that more dimensions are needed to ex-
plain a ﬁxed percentage of the variance in deeper layers
(Fig. 3A), meaning that in deeper layers, the representa-
tion contains more meaningfully discriminant dimensions.
However, we also wanted to investigate properties of the

singular value order03910-410010-210-310-1singular value order03900.5featsHL1HL5HL4HL3HL2ABexplained variance ratioSVD spectraCnormalizedsingular valuenormalizedsingular valuefeatsHL1HL5HL4HL3HL2Visualizing and Understanding Multilayer Perceptron Models: A Case Study in Speech Processing

a simple linear classiﬁer present a less difﬁcult problem to
the neural network than those that are incorrect and thus
linearly inseparable. We denote the LDA-transformed acti-
vations of hidden layers (cid:96) to correct and incorrect groups as
(cid:101)H(cid:96)
INC (inseparable), respectively. We
also consider the whole dataset, which we simply denote
as (cid:101)H(cid:96). Furthermore, we ﬁlter examples in each of these
groups by considering only data points that are classiﬁed
correctly at the output layer.

COR (separable) and (cid:101)H(cid:96)

To determine if the feature transformation that takes place
between hidden layers is applied non-uniformly across ex-
amples, we examined the relative location in the represen-
tational space of the hidden layers for three example classes
when they are easily confusable (/p,t,k/) and when they
are seldom confused (/t,n,s/) in the input features (confu-
sion matrix shown in Fig. 5D). Fig. 5A-B visualizes the
Euclidean distance between data points in (cid:101)H(cid:96) using mul-
tidimensional scaling (MDS) (Kruskal & Wish, 1978) for
classes /t,n,s/ and /p,t,k/. Individual class label is shown
by color, while labeled points shown in black represent the
centroid of each class. The distances between classes for
each hidden layer are overlaid at the right. Comparison of
the relative distances between the classes in Fig. 5A versus
B shows that the network nonlinearities expand the space
between the more confusable classes (/p,t,k/) more than for
classes that are more linearly separable (/t,s,n/).

While this analysis shows that the hidden layer repre-
sentations (cid:98)H(cid:96) between more overlapping categories are
warped more nonlinearly, we also investigated whether this
non-uniform and selective nonlinear transformation occurs
within each class as well. Fig. 5C tracks the relative lo-
cation of data points contained in (cid:101)H(cid:96)
INC as they propagate
through the network. Compared to Fig. 5B, we see that
there is a rapid nonlinear increase in the relative distance
between these more difﬁcult examples of the same class,
which occurs more strongly than for the examples in (cid:101)H(cid:96).
This observation is quantiﬁed in Fig. 5E, where we show
the relative expansion (increase in distance as compared to
the distance between examples in the LDA-transformed in-
put features (cid:101)X) of (cid:101)H(cid:96)
INC for both /p,t,k/ and /t,n,s/.

COR and (cid:101)H(cid:96)

With the intuition gained from considering a subset of
classes, we next quantify this ﬁnding for all classes in Fig.
5F for networks with both 256 and 2048 nodes/layer. How-
ever, instead of focusing on the relative expansion between
three classes, we consider the expansion of each class to
its most frequently confused class at the output layer. Our
results show that over all classes, linearly inseparable ex-
amples undergo a greater degree of expansion than linearly
separable ones. The observed non-uniform, nonlinear, and
focused stretching of the feature space illustrates the power
of a multilayer neural network to nonlinearly expand spe-
ciﬁc parts of the feature space that are critical for discrim-

Figure 4. Visualization of activations (cid:101)H on the WSJ eval93 set in
hidden layers 3 and 5 (2048 nodes/layer) using t-SNE. For visu-
alization, data points were aggregated using K-means clustering.

representation within a class. We did so by separating the
transformed activations (cid:101)H(cid:96) into separate matrices for each
class k, then performing SVD on each matrix individually
in each layer. Figure 3B shows the resultant SVD spec-
In deeper layers, on aver-
tra, averaged over all classes.
age the spectra require fewer singular values to reach the
same ratio of explained variance, indicating that class sep-
arability is accompanied by a normalization of activations
within classes (to represent a single class, fewer orthogonal
bases are required). These results are quantiﬁed in Figure
3C using the AUC score from Eq. 5. For the LDA mod-
els’ explained variance ratio (EVR), we observe that the
AUC score tends to increase with layer depth, while the
LDA-transformed activations of individual classes exhibit
smaller AUC scores in deeper layers. Taken together, these
results suggest that representations learned in hidden layers
of a neural network become increasingly discriminative, in
part through normalization of within-class variability.

This normalization is visualized in Fig. 4 for a network
with 2048 nodes/layer, where t-SNE (van der Maaten &
Hinton, 2008) is used to project (cid:101)H(cid:96) for (cid:96) = 3, 5 into 2D
(Euclidean distance). To reduce the dimensionality of the
problem, we used K-means clustering to cluster data points
of the same class, choosing the number of clusters for each
class as Nk
10 , where Nk is the number of data points of class
k in the WSJ eval93 set. The color and symbol (inter-
national phonetic alphabet) of each cluster indicate class
membership; the size of each point is proportional to the
number of data points in the cluster as determined by K-
means. We observe a tighter clustering of classes in deeper
layers, echoing the quantiﬁcation in Fig. 3.

3.3. Representations are transformed non-uniformly

In this section, we show that the normalization shown in
the previous section is greater for data points that are “dif-
ﬁcult” to classify. We accomplish this by ﬁrst dividing
the WSJ eval93 set into two groups based on whether the
data points were classiﬁed correctly or incorrectly using the
LDA model trained on the input features. This is based on
the intuition that examples that are classiﬁed correctly in

Visualizing and Understanding Multilayer Perceptron Models: A Case Study in Speech Processing

Figure 5. First two MDS dimensions of LDA-transformed features and hidden layer activations for (A) three dissimilar classes, /t/, /n/,
and /s/, (B) three similar classes, /p/, /t/, and /k/, and (C) inseparable examples of /p,t,k/ (in (cid:101)H(cid:96)
INC). For visualization purposes, data points
were aggregated using K-means clustering; the size of each point is proportional to the number of examples in that cluster. Centroids for
each class are shown in black text. (D) Confusion matrix for LDA classiﬁer trained on input features for classes in (A-C). (E-F) Average
relative expansion of centroids of linearly separable and inseparable (LDA, input features) data points, relative to input features for (E)
classes /t,n,s/ (left) and /p,t,k/, (F) all classes for a neural network with 2048 nodes/layer (left) and 256 nodes/layer (right).

ination of overlapping categories, while at the same time
applying more linear transformations to the parts of the fea-
ture space which are less overlapping.

4. Results: Analysis of the network function

The function of a ReLU network can be interpreted as a col-
lection of sample-dependent linear transformations (SLTs)
of the input features. In this section, we directly charac-
terize the properties of this function and examine the con-
tribution of different layers to the resulting computation of
the network. Because the size of the SLT for a layer of a
neural network is very large, we restrict the analyses in this
section to the network with 256 nodes/layer.

4.1. Networks learn clusters of functions

We start by looking at the sample-dependent linear trans-
forms (SLTs) mapping inputs to hidden and output layer
representations of the neural networks. Fig. 6A visual-
izes the SLT for the output node for class /r/ (SLTout
/r/)
for all correctly classiﬁed data points with label /r/ in the
WSJ eval93 set, sorted by clustering (UPGMA, Euclidean
distance). Here, columns represent data points, and rows
are the linear weights (templates) applied to each sample to
map it to the output node. We see that while the network

can potentially learn a different template for each sample,
the SLTs tend to cluster in groups (with several broad clus-
ters separated by dashed black lines), revealing similarities
between linear transforms that are applied to subsets of data
points. The distance between each SLT for class /r/ is vi-
sualized using MDS in Fig. 6B, while the average linear
mapping function for each cluster is shown in Fig. 6C.

4.2. Deeper layers encode more diverse functions

The number of distinct SLTs that are applied to data
points from the same class shows the diversity of tem-
plates learned for that class. This diversity of templates can
also be interpreted as the complexity of the function that
is learned for a given class. With this intuition, to quan-
titatively analyze a network we perform SVD on the SLT
of node n in layer (cid:96) and keep the singular value spectrum
contained in the diagonal of the matrix Σ(cid:96)
n (normalized by
the maximum value).

Because we are interested in the transformations occurring
in deep networks at each layer, we applied this analysis on
each of the individual nodes of the output and hidden lay-
ers of the network. For each node, if we retain the vector
of singular values and take their average, we see that given
the same dataset, deeper layers also encode more complex
functions (7A, top left). In this analysis we neglect the ﬁrst

Visualizing and Understanding Multilayer Perceptron Models: A Case Study in Speech Processing

Finally, we examined the SLTs for more difﬁcult examples
in the dataset. Recall from section 3.3 that the represen-
tational space was transformed more for linearly insepa-
rable data points than for separable ones. We replicated
this result in the SLT space by performing an SVD for in-
dividual nodes by splitting the data points into the anal-
ogous linearly separable and inseparable groups SLT(cid:96)
and SLT(cid:96)
INC based on classiﬁcation accuracy from the LDA
model trained on input features for each hidden layer (cid:96).
We then calculated the score for the difference between
the spectra Σ(cid:96)
n,COR, averaged over nodes n, using
area under the curve (AUC). The resulting positive values
shown in Fig. 7D show that the diversity of SLTs learned
for the difﬁcult examples is consistently greater that the
easier ones. Therefore, the nonuniform warping of the fea-
ture space is achieved by an increase in the number of SLTs
that are learned, resulting in division of the feature space
into a greater number of linear sub-regions to accommo-
date these more subtle class distinctions.

n,INC − Σ(cid:96)

COR

4.3. Visualizing the SLT from input to output

the functions
We showed in the previous section that
learned in subsequent layers of deep neural networks are
more complex. Here, we seek to visualize the learned func-
tions that deﬁne the separate classes. Because we are inter-
ested in the features important for class distinctions, for the
remainder of this section we consider the subset of data
points in the eval93 set that are classiﬁed correctly at the
output layer of the network. For each output node, we cal-
culate the SLT which is a matrix of dimension [Nk × F ],
where Nk is the number of data points belonging to class
k. To visualize the learned linear templates at the output
layer, we ﬁnd the centroid over data points Nk and com-
pute the mean of the SLT and corresponding input features
over the 50 samples closest to the centroid. Results for se-
lected classes are shown in Fig. 8A, where the average in-
put features (left, outlined in red) and corresponding mean
SLT (right, outlined in blue) are presented side-by-side.

A standard MLP trained for supervised learning must nor-
malize sources of variability in the feature space in order to
perform a successful classiﬁcation. In a phoneme recogni-
tion task, there are two main sources of variability within
a class. The ﬁrst source comes from differences in pro-
nunciations of phones, or allophonic variations of the same
phoneme (Ladefoged & Johson, 2010). The second source
of variability comes from the fact that the network must
learn that time-shifted variants of the same inputs belong
to the same class. In the previous section, we showed that
the network equivalent sample-dependent linear systems
are quite diverse at the output layer, evidenced by the large
AUC score of the SVD spectra of output nodes. To visual-
ize this diversity of templates, which uncover the possible
variations of features for each class, we used unsupervised

Figure 6. (A) SLTs of correctly classiﬁed data points with the la-
(B) Pairwise distances between
bel /r/, clustered by similarity.
individual SLTs in (A) projected into 2D using MDS. Color indi-
cates cluster assignment. (C) Average SLT for clusters deﬁned in
(A).

hidden layer because the SLT of each node is bimodal (con-
tains only values of W 1
i or 0 for a data point, meaning that
rank(SLT1
n) = 1). In deeper layers of the network, the
AUC score from Eq. 5 is larger, indicating that more or-
thogonal bases are needed to explain a ﬁxed percentage of
the variance. For a comparison of deep vs. shallow net-
works with the same number of parameters, see (S. Wang
& Aslan, 2016).

While this analysis shows that on average individual nodes
encode more diverse SLTs in deep layers, we also wanted
to conﬁrm that this resulted in a more complex population
coding of classes. To do this, we performed a similar anal-
ysis, this time doing one SVD for each class on the matrix
containing the SLT of all nodes in a layer to instances of
that class (reshaped to size [Nk × (F × D)]). In this way,
we perform SVD on all the nodes in a layer in response
to each classes. Fig. 7B shows the resultant score calcu-
lated from SVD on the SLT, averaged over classes, plot-
ted against the scores from Fig. 3C calculated similarly
from the activations. We observe a strong negative correla-
tion, indicating that the increased normalization that occurs
within classes in the activations of deeper layers is due to
an increase in the diversity of SLTs that create more linear
response regions in the network.

Figure 7. (A) SVD spectra of SLTs for hidden layers computed
over individual nodes (ΣNODES) and input dimensions (ΣINPUTS),
averaged. Below are histograms of AUC scores over nodes/input
dimensions for the average spectra. (B) AUC score from SVD
spectra calculated from activations vs. SLT. (C) Area under the
curve (AUC) for Σ(cid:96)
n,COR calculated from SLT of nodes.

n,INC − Σ(cid:96)

0data points with label /r/A11000253MDS 1MDS 20Bfreq.time(freq. × time)featuresCAHL1HL2HL3HL4HL5010000.5singular value orderΣINPUTSlayersdeeperoutHL2HL3HL4HL5010000.5singular value orderΣNODESlayersdeeper0412345layerHL4HL3HL2HL1HL5AUC5.58.5AUCactivation0100AUCSLT0450455400AUCAUCBCVisualizing and Understanding Multilayer Perceptron Models: A Case Study in Speech Processing

Figure 8. (A) Features (red) and corresponding SLT (blue), averaged over the WSJ eval93 set for selected classes. (B) Unsupervised
hierarchical clustering for SLT (red), shown with corresponding features (blue). Number of data points in each cluster is shown below
the features and SLT. (C) Comparison of average SLT for selected output nodes (columns) to data points of selected classes (rows).

hierarchical clustering (UPGMA, Euclidean distance). The
clustering of SLTs for one example class (phonemes = /t/)
is shown in Figure 8B. In this ﬁgure, the mean SLT of each
group is outlined in blue and the corresponding average
features are outlined in red; below each feature/SLT pair
are the number of data points in that cluster.

This clustering analysis shows that the network learns to
emphasize different feature dimensions depending on what
variation of each class is presented to the network. For ex-
ample, consider the cluster of /t/ highlighted and starred
in purple in Fig. 8B. This cluster represents a ﬂapped allo-
phone of /t/ (such as in the American English pronunciation
of “water”). The average SLT for this variation of /t/ shows
absence of power in high frequency bands (compare to the
average features at the top). This cluster has a vastly differ-
ent spectral proﬁle than the cluster of /t/ shown above, and
we can see that the corresponding mean SLT for this cluster
emphasizes different parts of the feature space. This anal-
ysis thus provides a data driven method to discovering the
variations present in phonemes, which has been the subject
of linguistic debate for decades (Stevens, 2000).

In the previous two analyses, we considered the mapping
of data points of each class to the corresponding nodes in
the output. However, it is equally important to form func-
tions that provide evidence against a data point belonging
to other classes. Because one can ﬁnd the SLTs for each
sample to all output nodes, this method makes it possible
to also investigate the resolution of confusable features of
similar classes. Fig. 8C examines these properties through
the visualization of the output nodes corresponding to two
frequently confused class pairs (/s/ and /z/; /m/ and /n/) and
the average SLT of these nodes to both classes. What we
observe is that for easily confused pairs, there are shared
features (shown in black), but also distinguishing features
between the classes (shown in red). For example, the main
difference between phonemes /s/ and /z/ are the low fre-
quencies (red arrow), which are negatively weighted for
the /s/ node and positively weighted for the /z/ node. This
is consistent with the input features for /s/ and /z/ in Fig.
8A, where we see the voiced characteristics of /z/ give the
inputs energy in low frequencies.

5. Discussion

In this study, we introduce novel techniques for jointly an-
alyzing the internal representation and the transformations
of features that are used by a multilayer perceptron. Us-
ing networks trained to map acoustic features of speech to
phoneme categories, we determined the encoding proper-
ties of each phoneme by the individual and population of
nodes in hidden layers of the network. We found a pro-
gressive non-uniform and nonlinear transformation of the
feature space which increases the discriminant dimensions
of overlapping instances of different classes.

To study how the network achieves categorization of its in-
puts, we proposed a method by which the exact function
that is applied to every data point can be constructed in
the form of a linear transformation of the input features
(sample-dependent linear transform, or SLT). We found
that while the network can potentially learn unique func-
tions for every data point, instead the network applies sim-
ilar functions to clusters of data points. These clusters of
shared functions exemplify the prototypical variations of
each class, suggesting a computational strategy to explic-
itly model variability. More generally, analyzing the prop-
erties of these functions provides a data-driven feature in-
terpretation method, which can be used for feature selec-
tion, discovering dependencies between features, or ana-
lyzing the variability of each class.

Overall, our study provides a novel and intuitive account
of how deep neural networks perform classiﬁcation, and
provides a qualitative and quantitative method for compar-
ison of networks with various sizes and architectures. Al-
though the results here are presented in a phoneme recogni-
tion task, these methods are easily extensible to other appli-
cations trained on alternative datasets or architectures (e.g.,
very deep networks, bottlenecks, convolutional networks,
or any other network where the activation function is dif-
ferentiable with respect to the inputs). Moreover, analyzing
the SLTs allows one to investigate the encoding properties
of misclassiﬁed data points, which can provide intuitions
that aid in devising improved feature extraction and classi-
ﬁcation techniques.

Visualizing and Understanding Multilayer Perceptron Models: A Case Study in Speech Processing

References

A. L. Maas, A. Y. Hannun and Ng, A. Y. Rectiﬁer non-
linearities improve neural network acoustic models. In
International Conference on Machine Learning, Atlanta,
GA, 2013.

A. Nguyen, J. Yosinkski and Clune, J. Deep neural net-
works are easily fooled: High conﬁdence predictions for
unrecognizable images. In CVPR, Boston, MA, 2015.

A.-R. Mohamed, G. E. Dahl and Hinton, G. Acoustic mod-
eling using deep belief networks. IEEE Transactions on
Audio, Speech, and Language Processing, 20(1):14–22,
2010.

A.-R. Mohamed, G. Hinton and Penn, G. Understanding
how deep belief networks perform acoustic modelling.
In ICASSP, Kyoto, Japan, 2012.

A. Veit, M. J. Wilber and Belongie, S. Residual networks
behave like ensembles of relatively shallow networks.
In Advances in Neural Information Processing Systems,
Barcelona, Spain, 2016.

C. Szegedy, W. Zaremba, I. Sutskever J. Bruna D. Erhan
Intriguing properties of

I. Goodfellow and Fergus, R.
neural networks. arXiv:1312.6199, 2013.

Cybenko, G. Approximation by superpositions of a sig-
moidal function. Mathematics of Control, Signals, and
Systems, 2:303–314, 1989.

D. Andor, C. Alberti, D. Weiss A. Severyn A. Presta K.
Ganchev S. Petrov and Collins, M. Globally normalized
arXiv:1603.06042,
transition-based neural networks.
2016.

D. Povey, A. Ghoshal, G. Boulianne L. Burget O. Glem-
bek N. Goel M. Hannemann P. Motlicek Y. Qian P.
Schwarz J. Silovsky G. Stemmer and Vesely, K. The
In IEEE Workshop
kaldi speech recognition toolkit.
on Automatic Speech Recognition and Understanding
(ASRU), Waikoloa, HI, 2011.

D. Silver, A. Huang, C. J. Maddison A. Guez L. Sifre G.
van den Driessche J. Schrittwieser I. Antonoglou V. Pan-
neershelvam M. Lanctot S. Dieleman D. Grewe J. Nham
N. Kalchbrenner I. Sutskever T. Lillicrap M. Leach K.
Kavukcuoglu T. Graepel and Hassabis, D. Mastering the
game of go with deep neural networks and tree search.
Nature, 529(7587):484–489, 2016.

Eldan, R. and Shamir, O. The power of depth for feedfor-

Audio, Speech, and Language Processing, 20(1):30–42,
2012.

G. E. Dahl, T. N. Sainath and Hinton, G. E.

Improving
deep neural networks for lvcsr using rectiﬁed linear units
In ICASSP, pp. 8609–8613, Vancouver,
and dropout.
Canada, 2013.

G. Hinton, L. Deng, D. Yu G. E. Dahl A. Mohamed N.
Jaitly A. Senior V. Vanhoucke P. Nguyen T. N. Sainath
and Kingsbury, B. Deep neural networks for acoustic
modeling in speech recognition: the shared views of four
research groups. IEEE Signal Processing Magazine, pp.
82–97, 2012.

G. Montufar, R. Pascanu, K. Cho and Bengio, Y. On the
number of linear regions of deep neural networks.
In
Neural Information Processing Systems, pp. 1–9, Mon-
treal, Canada, 2014.

Ioffe, S. and Szegedy, C. Batch normalization: Accelerat-
ing deep network training by reducing internal covariate
shift. arXiv:1502.03167, 2015.

J. Bergstra, O. Breuleux, F. F. Bastien P. Lamblin R. Pas-
canu G. Desjardins J. Turian D. Warde-Farley and Ben-
gio, Y. Theano: a cpu and gpu math compiler in python.
In Proceedings of the Python for Scientiﬁc Computing
Conference (SciPy), Austin, TX, 2010.

J. Garofolo, D. Graff, D. Paul and Pallett, D. CSR-I (WSJ0)
Complete. Linguistic Data Consortium, Philadelphia,
1993.

J. Garofolo, D. Graff, D. Paul and Pallett, D. CSR-
Linguistic Data Consortium,

II (WSJ1) Complete.
Philadelphia, 1994.

J. Yosinski, J. Clune, A. Nguyen T. Fuchs and Lipson, H.
Understanding neural networks through deep visualiza-
tion. arXiv:1506.06579, 2016.

K. He, X. Zhang, S. Ren and Sun, J. Deep residual learn-
ing for image recognition. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp.
770–778, Las Vegas, USA, 2016.

K. Hornik, M. Stinchcombe and White, H. Multilayer feed-
forward networks are universal approximators. Neural
Networks, 2(5):359–366, 1989.

Kruskal, Joseph B. and Wish, Myron. Multidimensional

Scaling. Sage Publications, Newbury Park, 1978.

ward neural networks. arXiv:1512.03965, 2015.

Ladefoged, P. and Johson, K. A Course in Phonetics.

G. E. Dahl, D. Yu, L. Deng and Acero, A. Context-
dependent pre-trained deep neural networks for large-
IEEE Transactions on
vocabulary speech recognition.

Wadsworth Publishing, Boston, MA, 2010.

Lin, H. W. and Tegmark, M. Why does deep and cheap

learning work so well? arXiv:1608.08225, 2016.

Visualizing and Understanding Multilayer Perceptron Models: A Case Study in Speech Processing

M. D. Zeiler, M. Ranzato, R. Monga M. Mao K. Yang Q. V
Le P. Nguyen A. Senior V. Vanhoucke J. Dean and Hin-
ton, G. E. On rectiﬁed linear units for speech processing.
In ICASSP, pp. 3517–3521, Vancouver, Canada, 2013.

Mallat, S. Understanding deep convolutional networks.

Phil. Trans. R. Soc, 374(2065), 2016.

N. Srivastava, G. E. Hinton, A. Krizhevsky I. Sutskever and
Salakhutdinov, R. Dropout: A simple way to prevent
neural networks from overﬁtting. Journal of Marchine
Learning Research, 15:1929–1958, 2014.

R. Pascanu, G. Montufar and Bengio, Y. On the number
of response regions of deep feedforward networks with
piecewise linear activations. arXiv:1312.6098v5, 2013.

S. Wang, A. Mohamed, R. Caruana J. Bilmes M. Plilipose
M. Richardson K. Geras G. Urban and Aslan, O. Anal-
ysis of deep neural networks with the extended data ja-
cobian matrix. In International Conference on Machine
Learning, New York, NY, 2016.

Stevens, K. N. Acoustic Phonetics. The MIT Press, 2000.

T. Nagamine, M. L. Seltzer and Mesgarani, N. Exploring
how deep neural networks form phonemic categories.
In INTERSPEECH, pp. 1912–1916, Dresden, Germany,
2015.

T. Nagamine, M. L. Seltzer and Mesgarani, N. On the
role of nonlinear transformations in deep neural network
acoustic models. In INTERSPEECH, pp. 803–807, San
Francisco, CA, 2016.

van der Maaten, L.J.P. and Hinton, G.E. Visualizing
high-dimensional data using t-sne. Journal of Marchine
Learning Research, 9:2579–2605, 2008.

W. Xiong, J. Droppo, X. Huang F. Seide M. Seltzer A. Stol-
cke D. Yu and Zweig, G. The microsoft 2016 conver-
sational speech recognition system. arXiv:1609.03528,
2016.

Zeiler, M. D. and Fergus, R. Visualizing and understanding

convolutional networks. arXiv:1311.2901, 2013.

