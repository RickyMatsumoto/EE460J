Tensor Balancing on Statistical Manifold

Mahito Sugiyama 1 2 Hiroyuki Nakahara 3 Koji Tsuda 4 5 6

Abstract
We solve tensor balancing, rescaling an N th or-
der nonnegative tensor by multiplying N ten-
sors of order N (cid:0) 1 so that every Ô¨Åber sums to
one. This generalizes a fundamental process of
matrix balancing used to compare matrices in a
wide range of applications from biology to eco-
nomics. We present an efÔ¨Åcient balancing al-
gorithm with quadratic convergence using New-
ton‚Äôs method and show in numerical experiments
that the proposed algorithm is several orders of
magnitude faster than existing ones. To theo-
retically prove the correctness of the algorithm,
we model tensors as probability distributions in
a statistical manifold and realize tensor balanc-
ing as projection onto a submanifold. The key to
our algorithm is that the gradient of the manifold,
used as a Jacobian matrix in Newton‚Äôs method,
can be analytically obtained using the M¬®obius in-
version formula, the essential of combinatorial
mathematics. Our model is not limited to ten-
sor balancing, but has a wide applicability as it
includes various statistical and machine learning
models such as weighted DAGs and Boltzmann
machines.

1. Introduction

Matrix balancing is the problem of rescaling a given square
nonnegative matrix A 2 Rn(cid:2)n
to a doubly stochastic ma-
(cid:21)0
trix RAS, where every row and column sums to one, by
multiplying two diagonal matrices R and S. This is a
fundamental process for analyzing and comparing matri-
ces in a wide range of applications, including input-output
analysis in economics, called the RAS approach (Parikh,
1979; Miller & Blair, 2009; Lahr & de Mesnard, 2004),
seat assignments in elections (Balinski, 2008; Akartunalƒ± &

1National Institute of Informatics 2JST PRESTO 3RIKEN
Brain Science Institute 4Graduate School of Frontier Sciences,
The University of Tokyo 5RIKEN AIP 6NIMS. Correspondence
to: Mahito Sugiyama <mahito@nii.ac.jp>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure1. Overview of our approach.

Knight, 2016), Hi-C data analysis (Rao et al., 2014; Wu &
Michor, 2016), the Sudoku puzzle (Moon et al., 2009), and
the optimal transportation problem (Cuturi, 2013; Frogner
et al., 2015; Solomon et al., 2015). An excellent review of
this theory and its applications is given by Idel (2016).

The standard matrix balancing algorithm is the Sinkhorn-
Knopp algorithm (Sinkhorn, 1964; Sinkhorn & Knopp,
1967; Marshall & Olkin, 1968; Knight, 2008), a special
case of Bregman‚Äôs balancing method (Lamond & Stewart,
1981) that iterates rescaling of each row and column until
convergence. The algorithm is widely used in the above
applications due to its simple implementation and theo-
retically guaranteed convergence. However, the algorithm
converges linearly (Soules, 1991), which is prohibitively
slow for recently emerging large and sparse matrices. Al-
though Livne & Golub (2004) and Knight & Ruiz (2013)
tried to achieve faster convergence by approximating each
step of Newton‚Äôs method, the exact Newton‚Äôs method with
quadratic convergence has not been intensively studied yet.

Another open problem is tensor balancing, which is a gen-
eralization of balancing from matrices to higher-order mul-
tidimentional arrays, or tensors. The task is to rescale an
N th order nonnegative tensor to a multistochastic tensor,
in which every Ô¨Åber sums to one, by multiplying (N (cid:0) 1)th
order N tensors. There are some results about mathemat-
ical properties of multistochastic tensors (Cui et al., 2014;
Chang et al., 2016; Ahmed et al., 2003). However, there
is no result for tensor balancing algorithms with guaran-
teed convergence that transforms a given tensor to a multi-
stochastic tensor until now.

Every fibersums to 1Given tensor AMultistochastic tensor A‚ÄôSubmanifold ùì¢(Œ≤)Probabilitydistribution PStatistical manifold ùì¢(dually flat Riemannian manifold)ProjectionTensor balancingProjecteddistribution PŒ≤Projecteddistribution PŒ≤Tensor Balancing on Statistical Manifold

Here we show that Newton‚Äôs method with quadratic con-
vergence can be applied to tensor balancing while avoid-
ing solving a linear system on the full tensor. Our strat-
egy is to realize matrix and tensor balancing as projec-
tion onto a dually Ô¨Çat Riemmanian submanifold (Figure 1),
which is a statistical manifold and known to be the es-
sential structure for probability distributions in information
geometry (Amari, 2016). Using a partially ordered out-
come space, we generalize the log-linear model (Agresti,
2012) used to model the higher-order combinations of bi-
nary variables (Amari, 2001; Ganmor et al., 2011; Naka-
hara & Amari, 2002; Nakahara et al., 2003), which allows
us to model tensors as probability distributions in the sta-
tistical manifold. The remarkable property of our model is
that the gradient of the manifold can be analytically com-
puted using the M¬®obius inversion formula (Rota, 1964), the
heart of combinatorial mathematics (Ito, 1993), which en-
ables us to directly obtain the Jacobian matrix in Newton‚Äôs
method. Moreover, we show that (n (cid:0) 1)N entries for the
size nN of a tensor are invariant with respect to one of the
two coordinate systems of the statistical manifold. Thus
the number of equations in Newton‚Äôs method is O(nN (cid:0)1).

The remainder of this paper is organized as follows: We
begin with a low-level description of our matrix balancing
algorithm in Section 2 and demonstrate its efÔ¨Åciency in nu-
merical experiments in Section 3. To guarantee the correct-
ness of the algorithm and extend it to tensor balancing, we
provide theoretical analysis in Section 4. In Section 4.1, we
introduce a generalized log-linear model associated with a
partial order structured outcome space, followed by intro-
ducing the dually Ô¨Çat Riemannian structure in Section 4.2.
In Section 4.3, we show how to use Newton‚Äôs method to
compute projection of a probability distribution onto a sub-
manifold. Finally, we formulate the matrix and tensor bal-
ancing problem in Section 5 and summarize our contribu-
tions in Section 6.

2. The Matrix Balancing Algorithm
Given a nonnegative square matrix A = (aij) 2 Rn(cid:2)n
task of matrix balancing is to Ô¨Ånd r; s 2 Rn that satisfy

(cid:21)0 , the

(RAS)1 = 1;

(RAS)T 1 = 1;

(1)

where R = diag(r) and S = diag(s). The balanced matrix
A‚Ä≤ = RAS is called doubly stochastic, in which each entry
a‚Ä≤
ij = aijrisj and all the rows and columns sum to one.
The most popular algorithm is the Sinkhorn-Knopp algo-
rithm, which repeats updating r and s as r = 1=(As) and
s = 1=(AT r). We denote by [n] = f1; 2; : : : ; ng hereafter.

In our algorithm, instead of directly updating r and s, we
update two parameters (cid:18) and (cid:17) deÔ¨Åned as

log pij =

(cid:18)i‚Ä≤j‚Ä≤;

(cid:17)ij =

pi‚Ä≤j‚Ä≤

(2)

‚àë

‚àë

i‚Ä≤(cid:20)i

j‚Ä≤(cid:20)j

‚àë

‚àë

i‚Ä≤(cid:21)i

j‚Ä≤(cid:21)j

6
6
6
6
6
6
6
6
6
4

(cid:18)(t+1)
11
...
(cid:18)(t+1)
1n
(cid:18)(t+1)
21
...
(cid:18)(t+1)
n1

7
7
7
7
7
7
7
7
7
5

7
7
7
7
7
7
7
7
7
5

6
6
6
6
6
6
6
6
6
4

(cid:18)(t)
11
...
(cid:18)(t)
1n
(cid:18)(t)
21
...
(cid:18)(t)
n1

@(cid:17)(t)
ij
@(cid:18)(t)
i‚Ä≤j‚Ä≤

Figure2. Matrix balancing with two parameters (cid:18) and (cid:17).

‚àë

‚àë

ij aij so that

for each i; j 2 [n], where we normalized entries as pij =
aij=
ij pij = 1. We assume for simplic-
ity that each entry is strictly larger than zero. The assump-
tion will be removed in Section 5.
The key to our approach is that we update (cid:18)(t)
ij with i = 1
or j = 1 by Newton‚Äôs method at each iteration t = 1; 2; : : :
while Ô¨Åxing (cid:18)ij with i; j Ã∏= 1 so that (cid:17)(t)
ij satisÔ¨Åes the fol-
lowing condition (Figure 2):
(cid:17)(t)
i1 = (n (cid:0) i + 1)=n;

(cid:17)(t)
1j = (n (cid:0) j + 1)=n:

Note that the rows and columns sum not to 1 but to 1=n due
to the normalization. The update formula is described as
2

3

3

2

3

2

=

(cid:0) J (cid:0)1

;

(3)

6
6
6
6
6
6
6
6
6
4

(cid:17)(t)
11

(cid:17)(t)
1n
(cid:17)(t)
21

(cid:17)(t)
n1

(cid:0) (n (cid:0) 1 + 1)=n
...
(cid:0) (n (cid:0) n + 1)=n
(cid:0) (n (cid:0) 2 + 1)=n
...
(cid:0) (n (cid:0) n + 1)=n

7
7
7
7
7
7
7
7
7
5

where J is the Jacobian matrix given as

J(ij)(i‚Ä≤j‚Ä≤) =

= (cid:17)maxfi;i‚Ä≤g maxfj;j‚Ä≤g (cid:0)n2(cid:17)ij(cid:17)i‚Ä≤j‚Ä≤; (4)

which is derived from our theoretical result in Theorem 3.
Since J is a (2n(cid:0)1)(cid:2)(2n(cid:0)1) matrix, the time complexity
of each update is O(n3), which is needed to compute the
inverse of J.
After updating to (cid:18)(t+1)
and (cid:17)(t+1)
ij
by Equation (2). Since this update does not ensure the
‚àë
condition
as
and recompute p(t+1)
(cid:18)(t+1)
11
and (cid:17)(t+1)
ij

ij
= (cid:18)(t+1)
(cid:0) log
11
for each i; j 2 [n].

= 1, we again update (cid:18)(t+1)
‚àë

, we can compute p(t+1)

ij p(t+1)

ij p(t+1)

11

ij

ij

ij

ij

By iterating the above update process in Equation (3) until
convergence, A = (aij) with aij = npij becomes doubly
stochastic.

3. Numerical Experiments

We evaluate the efÔ¨Åciency of our algorithm compared to the
two prominent balancing methods, the standard Sinkhorn-
Knopp algorithm (Sinkhorn, 1964) and the state-of-the-art

a11a12a13a14a21a22a23a24a31a32a33a34a41a42a43a44Œ∑11Œ∑12Œ∑13Œ∑14Œ∑21Œ∏22Œ∏23Œ∏24Œ∑31Œ∏32Œ∏33Œ∏34Œ∑41Œ∏42Œ∏43Œ∏44MatrixConstraints for balancingInvariantTensor Balancing on Statistical Manifold

Figure3. Results on Hessenberg matrices. The BNEWT algo-
rithm (green) failed to converge for n (cid:21) 200.

Figure5. Results on Trefethen matrices. The BNEWT algorithm
(green) failed to converge for n (cid:21) 200.

is clearly the fastest: It is three to Ô¨Åve orders of magnitude
faster than the standard Sinkhorn-Knopp algorithm (plotted
in red). Although the BNEWT algorithm (plotted in green)
is competitive if n is small, it suddenly fails to converge
whenever n (cid:21) 200, which is consistent with results in the
original paper (Knight & Ruiz, 2013) where there is no re-
sult for the setting n (cid:21) 200 on the same matrix. Moreover,
our method converges around 10 to 20 steps, which is about
three and seven orders of magnitude smaller than BNEWT
and Sinkhorn-Knopp, respectively, at n = 100.

To see the behavior of the rate of convergence in detail, we
plot the convergence graph in Figure 4 for n = 20, where
we observe the slow convergence rate of the Sinkhorn-
Knopp algorithm and unstable convergence of the BNEWT
algorithm, which contrasts with our quick convergence.

Trefethen Matrix. Next, we collected a set of Trefethen
matrices from a collection website2, which are nonnega-
tive diagonal matrices with primes. Results are plotted in
Figure 5, where we observe the same trend as before: Our
algorithm is the fastest and about four orders of magnitude
faster than the Sinkhorn-Knopp algorithm. Note that larger
matrices with n > 300 do not have total support, which
is the necessary condition for matrix balancing (Knight &
Ruiz, 2013), while the BNEWT algorithm fails to converge
if n = 200 or n = 300.

4. Theoretical Analysis

In the following, we provide theoretical support to our al-
gorithm by formulating the problem as a projection within
a statistical manifold, in which a matrix corresponds to an
element, that is, a probability distribution, in the manifold.

We show that a balanced matrix forms a submanifold and
matrix balancing is projection of a given distribution onto
the submanifold, where the Jacobian matrix in Equation (4)
is derived from the gradient of the manifold.

2http://www.cise.ufl.edu/research/sparse/

Figure4. Convergence graph on H20.

algorithm BNEWT (Knight & Ruiz, 2013), which uses
Newton‚Äôs method-like iterations with conjugate gradients.
All experiments were conducted on Amazon Linux AMI
release 2016.09 with a single core of 2.3 GHz Intel Xeon
CPU E5-2686 v4 and 256 GB of memory. All methods
were implemented in C++ with the Eigen library and
compiled with gcc 4.8.31. We have carefully implemented
BNEWT by directly translating the MATLAB code pro-
vided in (Knight & Ruiz, 2013) into C++ with the Eigen
library for fair comparison, and used the default parame-
ters. We measured the residual of a matrix A‚Ä≤ = (a‚Ä≤
ij) by
the squared norm ‚à•(A‚Ä≤1 (cid:0) 1; A‚Ä≤T 1 (cid:0) 1)‚à•2, where each en-
try a‚Ä≤
ij is obtained as npij in our algorithm, and ran each
of three algorithms until the residual is below the tolerance
threshold 10(cid:0)6.

Hessenberg Matrix. The Ô¨Årst set of experiments used a
Hessenberg matrix, which has been a standard benchmark
for matrix balancing (Parlett & Landis, 1982; Knight &
Ruiz, 2013). Each entry of an n (cid:2) n Hessenberg matrix
Hn = (hij) is given as hij = 0 if j < i (cid:0) 1 and hij = 1
otherwise. We varied the size n from 10 to 5; 000, and
measured running time (in seconds) and the number of it-
erations of each method.

Results are plotted in Figure 3. Our balancing algorithm
with the Newton‚Äôs method (plotted in blue in the Ô¨Ågures)

1An implementation of algorithms for matrices and third
https://github.com/

order
is available at:
mahito-sugiyama/newton-balancing

tensors

matrices/

nNumber of iterations1050500500010nRunning time (sec.)105050050001081061041021041021010‚Äì210‚Äì4Newton (proposed)SinkhornBNEWTNumber of iterationsResidual0500100015002000250030001010‚Äì110‚Äì310‚Äì510‚Äì7SinkhornNewtonBNEWTnNumber of iterations20100200300nRunning time (sec.)201002003001010910610310510310110‚Äì110‚Äì3Newton (proposed)SinkhornBNEWTTensor Balancing on Statistical Manifold

4.1. Formulation

We introduce our log-linear probabilistic model, where the
outcome space is a partially ordered set, or a poset (Gierz
et al., 2003). We prepare basic notations and the key math-
ematical tool for posets, the M¬®obius inversion formula, fol-
lowed by formulating the log-linear model.

4.1.1. M ¬®OBIUS INVERSION

A poset (S; (cid:20)), the set of elements S and a partial order
(cid:20) on S, is a fundamental structured space in computer
science. A partial order ‚Äú(cid:20)‚Äù is a relation between el-
ements in S that satisÔ¨Åes the following three properties:
For all x; y; z 2 S, (1) x (cid:20) x (reÔ¨Çexivity), (2) x (cid:20) y,
y (cid:20) x ) x = y (antisymmetry), and (3) x (cid:20) y,
y (cid:20) z ) x (cid:20) z (transitivity). In what follows, S is al-
ways Ô¨Ånite and includes the least element (bottom) ? 2 S;
that is, ? (cid:20) x for all x 2 S. We denote S n f?g by S+.

Rota (1964) introduced the M¬®obius inversion formula on
posets by generalizing the inclusion-exclusion principle.
Let (cid:16) : S (cid:2) S ! f0; 1g be the zeta function deÔ¨Åned as
{

1
0
The M¬®obius function (cid:22) : S (cid:2)S ! Z satisÔ¨Åes (cid:16)(cid:22) = I, which
is inductively deÔ¨Åned for all x; y with x (cid:20) y as

if s (cid:20) x;
otherwise:

(cid:16)(s; x) =

(cid:22)(x; y) =

x(cid:20)s<y (cid:22)(x; s)

8
<

:

‚àë

1
(cid:0)
0

if x = y;
if x < y;
otherwise:

From the deÔ¨Ånition, it follows that
‚àë

‚àë

(cid:16)(s; y)(cid:22)(x; s) =

(cid:22)(x; s) = (cid:14)xy;

s2S
‚àë

s2S

(cid:16)(x; s)(cid:22)(s; y) =

(cid:22)(s; y) = (cid:14)xy

(5)

x(cid:20)s(cid:20)y
‚àë

x(cid:20)s(cid:20)y

with the Kronecker delta (cid:14) such that (cid:14)xy = 1 if x = y and
(cid:14)xy = 0 otherwise. Then for any functions f , g, and h with
the domain S such that
‚àë

‚àë

g(x) =

(cid:16)(s; x)f (s) =

f (s);

h(x) =

(cid:16)(x; s)f (s) =

f (s);

f is uniquely recovered with the M¬®obius function:

s2S
‚àë

s2S

‚àë

s2S

s(cid:20)x
‚àë

s(cid:21)x

‚àë

s2S

‚àë

A probability vector is treated as a mapping p : S ! (0; 1)
x2S p(x) = 1, where every entry p(x) is as-
such that
sumed to be strictly larger than zero.

Using the zeta and the M¬®obius functions, let us introduce
two mappings (cid:18) : S ! R and (cid:17) : S ! R as
‚àë

(cid:18)(x) =

(cid:22)(s; x) log p(s);

(cid:17)(x) =

(cid:16)(x; s)p(s) =

p(s):

From the M¬®obius inversion formula, we have
‚àë

‚àë

log p(x) =

(cid:16)(s; x)(cid:18)(s) =

(cid:18)(s);

‚àë

s(cid:21)x

s(cid:20)x

p(x) =

(cid:22)(x; s)(cid:17)(s):

s2S
‚àë

s2S

s2S
‚àë

s2S

(6)

(7)

(8)

(9)

They are generalization of the log-linear model (Agresti,
2012) that gives the probability p(x) of an n-dimensional
binary vector x = (x1; : : : ; xn) 2 f0; 1gn as
‚àë

‚àë

‚àë

log p(x) =

(cid:18)ixi +

(cid:18)ijxixj +

(cid:18)ijkxixjxk

i

i<j

i<j<k

+ (cid:1) (cid:1) (cid:1) + (cid:18)1:::nx1x2 : : : xn (cid:0)  ;

where (cid:18) = ((cid:18)1; : : : ; (cid:18)12:::n) is a parameter vector,   is a
normalizer, and (cid:17) = ((cid:17)1; : : : ; (cid:17)12:::n) represents the ex-
pectation of variable combinations such that

(cid:17)i = E[xi] = Pr(xi = 1);
(cid:17)ij = E[xixj] = Pr(xi = xj = 1); i < j; : : :
(cid:17)1:::n = E[x1 : : : xn] = Pr(x1 = (cid:1) (cid:1) (cid:1) = xn = 1):

They coincide with Equations (8) and (7) when we let
S = 2V with V = f1; 2; : : : ; ng, each x 2 S as the set
of indices of ‚Äú1‚Äù of x, and the order (cid:20) as the inclusion re-
lationship, that is, x (cid:20) y if and only if x (cid:18) y. Nakahara
et al. (2006) have pointed out that (cid:18) can be computed from
p using the inclusion-exclusion principle in the log-linear
model. We exploit this combinatorial property of the log-
linear model using the M¬®obius inversion formula on posets
and extend the log-linear model from the power set 2V to
any kind of posets (S; (cid:20)). Sugiyama et al. (2016) studied a
relevant log-linear model, but the relationship with M¬®obius
inversion formula has not been analyzed yet.

f (x) =

(cid:22)(s; x)g(s);

f (x) =

(cid:22)(x; s)h(s):

4.2. Dually Flat Riemannian Manifold

This is called the M¬®obius inversion formula and is at the
heart of enumerative combinatorics (Ito, 1993).

4.1.2. LOG-LINEAR MODEL ON POSETS

We consider a probability vector p on (S; (cid:20)) that gives a
discrete probability distribution with the outcome space S.

We theoretically analyze our log-linear model introduced in
Equations (6), (7) and show that they form dual coordinate
systems on a dually Ô¨Çat manifold, which has been mainly
studied in the area of information geometry (Amari, 2001;
Nakahara & Amari, 2002; Amari, 2014; 2016). Moreover,
we show that the Riemannian metric and connection of our
model can be analytically computed in closed forms.

Tensor Balancing on Statistical Manifold

In the following, we denote by (cid:24) the function (cid:18) or (cid:17) and by
‚àá the gradient operator with respect to S+ = S n f?g, i.e.,
(‚àáf ((cid:24)))(x) = @f =@(cid:24)(x) for x 2 S+, and denote by S the
set of probability distributions speciÔ¨Åed by probability vec-
tors, which forms a statistical manifold. We use uppercase
letters P; Q; R; : : : for points (distributions) in S and their
lowercase letters p; q; r; : : : for the corresponding probabil-
ity vectors treated as mappings. We write (cid:18)P and (cid:17)P if they
are connected with p by Equations (6) and (7), respectively,
and abbreviate subscripts if there is no ambiguity.

4.2.1. DUALLY FLAT STRUCTURE

We show that S has the dually Ô¨Çat Riemannian structure
induced by two functions (cid:18) and (cid:17) in Equation (6) and (7).
We deÔ¨Åne  ((cid:18)) as

 ((cid:18)) = (cid:0)(cid:18)(?) = (cid:0) log p(?);

(10)

which corresponds to the normalizer of p. It is a convex
function since we have

 ((cid:18)) = log

exp

(cid:18)(s)

0

@

‚àë

?<s(cid:20)x

1

A

‚àë

x2S

‚àë

from log p(x) =
endre transformation to  ((cid:18)) given as

?<s(cid:20)x (cid:18)(s) (cid:0)  ((cid:18)). We apply the Leg-

(

(cid:18)‚Ä≤(cid:17) (cid:0)  ((cid:18)‚Ä≤)

)
; (cid:18)‚Ä≤(cid:17) =

‚àë

(cid:18)‚Ä≤(x)(cid:17)(x): (11)

œÜ((cid:17)) = max

(cid:18)‚Ä≤

x2S+

Then œÜ((cid:17)) coincides with the negative entropy.

Theorem 1 (Legendre dual).
‚àë

œÜ((cid:17)) =

p(x) log p(x):

x2S

Proof. From Equation (5), we have
0

‚àë

‚àë

@

(cid:18)‚Ä≤(cid:17) =

(cid:22)(s; x) log p‚Ä≤(s)

p(s)

‚àë

x2S+
‚àë

?<s(cid:20)x

s(cid:21)x
p(x) ( log p‚Ä≤(x) (cid:0) log p‚Ä≤(?) ) :

=

1

A

x2S+

Thus it holds that

(cid:18)‚Ä≤(cid:17) (cid:0)  ((cid:18)‚Ä≤) =

p(x) log p‚Ä≤(x):

(12)

‚àë

x2S

Hence it is maximized with p(x) = p‚Ä≤(x).

Proof. They can be directly derived from our deÔ¨Ånitions
(Equations (6) and (11)) as

‚àë

=

‚àë

y(cid:21)x exp

y2S exp
(
@
@(cid:17)(x)

@ ((cid:18))
@(cid:18)(x)

@œÜ((cid:17))
@(cid:17)(x)

=

)
?<s(cid:20)y (cid:18)(s)

(‚àë

(‚àë

?<s(cid:20)y (cid:18)(s)
)

(cid:18)(cid:17) (cid:0)  ((cid:18))

= (cid:18)(x):

‚àë

s(cid:21)x

) =

p(s) = (cid:17)(x);

Moreover, we can conÔ¨Årm the orthogonality of (cid:18) and (cid:17) as

[

E

@ log p(s)
@(cid:18)(x)

@ log p(s)
@(cid:17)(y)

]

‚àë

s2S

=

(cid:16)(x; s)(cid:22)(s; y) = (cid:14)xy:

The last equation holds from Equation (5), hence the
M¬®obius inversion directly leads to the orthogonality.

The Bregman divergence is known to be the canonical di-
vergence (Amari, 2016, Section 6.6) to measure the differ-
ence between two distributions P and Q on a dually Ô¨Çat
manifold, which is deÔ¨Åned as

D [P; Q] =  ((cid:18)P ) + œÜ((cid:17)Q) (cid:0) (cid:18)P (cid:17)Q:

‚àë

In our case, since we have œÜ((cid:17)Q) =
and (cid:18)P (cid:17)Q(cid:0) ((cid:18)P ) =
and Equation (12), it is given as

x2S q(x) log q(x)
x2S q(x) log p(x) from Theorem 1

‚àë

D [P; Q] =

q(x) log

q(x)
p(x)

;

‚àë

x2S

which coincides with the Kullback‚ÄìLeibler divergence (KL
divergence) from Q to P : D [P; Q] = DKL [Q; P ].

4.2.2. RIEMANNIAN STRUCTURE

Next we analyze the Riemannian structure on S and show
that the M¬®obius inversion formula enables us to compute
the Riemannian metric of S.
Theorem 3 (Riemannian metric). The manifold (S; g((cid:24)))
is a Riemannian manifold with the Riemannian metric g((cid:24))
such that for all x; y 2 S+
‚àë

[
(cid:16)(x; s)(cid:16)(y; s)p(s) (cid:0) (cid:17)(x)(cid:17)(y)

if (cid:24) = (cid:18);

]

(cid:22)(s; x)(cid:22)(s; y)p(s)(cid:0)1

if (cid:24) = (cid:17):

gxy((cid:24)) =

8
>>><
>>>:

s2S
‚àë

s2S

Proof. Since the Riemannian metric is deÔ¨Åned as
g((cid:17)) = ‚àá‚àáœÜ((cid:17));

g((cid:18)) = ‚àá‚àá ((cid:18));

Since they are connected with each other by the Legendre
transformation, they form a dual coordinate system ‚àá ((cid:18))
and ‚àáœÜ((cid:17)) of S (Amari, 2016, Section 1.5), which coin-
cides with (cid:18) and (cid:17) as follows.

when (cid:24) = (cid:18) we have

gxy((cid:18)) =

@2
@(cid:18)(x)@(cid:18)(y)

 ((cid:18)) =

(cid:17)(y)

@
@(cid:18)(x)
0

Theorem 2 (dual coordinate system).

‚àá ((cid:18)) = (cid:17); ‚àáœÜ((cid:17)) = (cid:18):

(13)

=

@
@(cid:18)(x)

‚àë

s2S

(cid:16)(y; s) exp

@

(cid:18)(u) (cid:0)  ((cid:18))

‚àë

?<u(cid:20)s

1

A

Tensor Balancing on Statistical Manifold

Then the Riemannian (Levi‚ÄìChivita) connection (cid:0)((cid:24)) with
respect to (cid:24), which is deÔ¨Åned as

{

(

)

=

(cid:16)(x; s)(cid:16)(y; s)p(s) (cid:0) jSj(cid:17)(x)(cid:17)(y):

‚àë

s2S

@2
@(cid:17)(x)@(cid:17)(y)

‚àë

s(cid:20)y

@
@(cid:17)(x)
‚àë

=

=

s2S

When (cid:24) = (cid:17), it follows that

gxy((cid:17)) =

œÜ((cid:17)) =

(cid:18)(y)

(cid:22)(s; y) log

(cid:22)(s; u)(cid:17)(u)

1

A

@
@(cid:17)(x)
0

‚àë

@

u(cid:21)s

(cid:22)(s; x)(cid:22)(s; y)p(s)(cid:0)1:

Since g((cid:24)) coincides with the Fisher information matrix,

log p(s)

log p(s)

= gxy((cid:18));

[

[

E

E

@
@(cid:18)(x)
@
@(cid:17)(x)

@
@(cid:18)(y)
@
@(cid:17)(y)

]

]

log p(s)

log p(s)

= gxy((cid:17)):

+

1
2

(cid:0)xyz((cid:24)) =

@gxz((cid:24))
@(cid:24)(y)

(cid:0) @gxy((cid:24))
@(cid:24)(z)

@gyz((cid:24))
@(cid:24)(x)
for all x; y; z 2 S+, can be analytically obtained.
Theorem 4 (Riemannian connection). The Riemannian
connection (cid:0)((cid:24)) on the manifold (S; g((cid:24))) is given in the
following for all x; y; z 2 S+,
1
2

)
(cid:16)(y; s) (cid:0) (cid:17)(y)

‚àë

)(

8

(

(cid:16)(x; s) (cid:0) (cid:17)(x)
)
(
(cid:16)(z; s) (cid:0) (cid:17)(z)

p(s)

if (cid:24) = (cid:18);

(cid:0)xyz((cid:24)) =

(cid:22)(s; x)(cid:22)(s; y)(cid:22)(s; z)p(s)(cid:0)2 if (cid:24) = (cid:17):

>>>>>><
>>>>>>:

s2S

‚àë

s2S

(cid:0) 1
2

Proof. Connections (cid:0)xyz((cid:18)) and (cid:0)xyz((cid:17)) can be obtained
by directly computing @gyz((cid:18))=@(cid:18)(x) and @gyz((cid:17))=@(cid:17)(x),
respectively.

4.3. The Projection Algorithm

Projection of a distribution onto a submanifold is essen-
tial; several machine learning algorithms are known to be
formulated as projection of a distribution empirically esti-
mated from data onto a submanifold that is speciÔ¨Åed by the
target model (Amari, 2016). Here we deÔ¨Åne projection of
distributions on posets and show that Newton‚Äôs method can
be applied to perform projection as the Jacobian matrix can
be analytically computed.

4.3.1. DEFINITION

Let S((cid:12)) be a submanifold of S such that

S((cid:12)) = fP 2 S j (cid:18)P (x) = (cid:12)(x); 8x 2 dom((cid:12))g (14)

speciÔ¨Åed by a function (cid:12) with dom((cid:12)) (cid:18) S+. Projection
of P 2 S onto S((cid:12)), called m-projection, which is deÔ¨Åned
as the distribution P(cid:12) 2 S((cid:12)) such that

{

(cid:18)P(cid:12) (x) = (cid:12)(x)
(cid:17)P(cid:12) (x) = (cid:17)P (x)

if x 2 dom((cid:12));
if x 2 S+ n dom((cid:12));

is the minimizer of the KL divergence from P to S((cid:12)):

P(cid:12) = argmin
Q2S((cid:12))

DKL[P; Q]:

The dually Ô¨Çat structure with the coordinate systems (cid:18) and
(cid:17) guarantees that the projected distribution P(cid:12) always ex-
ists and is unique (Amari, 2009, Theorem 3). Moreover,
the Pythagorean theorem holds in the dually Ô¨Çat manifold,
that is, for any Q 2 S((cid:12)) we have

DKL[P; Q] = DKL[P; P(cid:12)] + DKL[P(cid:12); Q]:
We can switch (cid:17) and (cid:18) in the submanifold S((cid:12)) by chang-
ing DKL[P; Q] to DKL[Q; P ], where the projected distri-
bution P(cid:12) of P is given as

(cid:18)P(cid:12) (x) = (cid:18)P (x)
(cid:17)P(cid:12) (x) = (cid:12)(x)
This projection is called e-projection.

if x 2 S+ n dom((cid:12));
if x 2 dom((cid:12));

Example 1 (Boltzmann machine). Given a Boltzmann ma-
chine represented as an undirected graph G = (V; E) with
a vertex set V and an edge set E (cid:18) ffi; jg j i; j 2 V g.
The set of probability distributions that can be modeled by
a Boltzmann machine G coincides with the submanifold
S B = fP 2 S j (cid:18)P (x) = 0 if jxj > 2 or x Ã∏2 Eg;
with S = 2V . Let ^P be an empirical distribution esti-
mated from a given dataset. The learned model is the m-
projection of the empirical distribution ^P onto S B, where
the resulting distribution P(cid:12) is given as

{

(cid:18)P(cid:12) (x) = 0
(cid:17)P(cid:12) (x) = (cid:17) ^P (x)

if jxj > 2 or x Ã∏2 E;
if jxj = 1 or x 2 E:

4.3.2. COMPUTATION

Here we show how to compute projection of a given prob-
ability distribution. We show that Newton‚Äôs method can be
used to efÔ¨Åciently compute the projected distribution P(cid:12) by
iteratively updating P (0)
(cid:12) = P as P (0)
(cid:12) ; : : : until
converging to P(cid:12).

(cid:12) ; P (1)

(cid:12) ; P (2)

Let us start with the m-projection with initializing P (0)
(cid:12) =
P . In each iteration t, we update (cid:18)(t)
(x) for all x 2 dom(cid:12)
P(cid:12)
while Ô¨Åxing (cid:17)(t)
(x) = (cid:17)P (x) for all x 2 S+ n dom((cid:12)),
P(cid:12)
which is possible from the orthogonality of (cid:18) and (cid:17). Using
Newton‚Äôs method, (cid:17)(t+1)
(
‚àë

(x) should satisfy

P(cid:12)

)

(

)

(cid:18)(t)
P(cid:12)

(x) (cid:0) (cid:12)(x)

Jxy
+
y2dom((cid:12))

(cid:17)(t+1)
P(cid:12)

(y) (cid:0) (cid:17)(t)
P(cid:12)

(y)

= 0;

Tensor Balancing on Statistical Manifold

for every x 2 dom((cid:12)), where Jxy is an entry of the
jdom((cid:12))j (cid:2) jdom((cid:12))j Jacobian matrix J and given as
‚àë

(cid:22)(s; x)(cid:22)(s; y)p(t)

(cid:12) (s)(cid:0)1

Jxy =

@(cid:18)(t)
P(cid:12)
@(cid:17)(t)
P(cid:12)

(x)

(y)

=

s2S

from Theorem 3. Therefore, we have the update formula
for all x 2 dom((cid:12)) as
(x) = (cid:17)(t)
P(cid:12)

(y) (cid:0) (cid:12)(y)

(cid:17)(t+1)
P(cid:12)

(x) (cid:0)

J (cid:0)1
xy

(cid:18)(t)
P(cid:12)

‚àë

(

)

:

y2dom((cid:12))

In e-projection, update (cid:17)(t)
(x) for x 2 dom((cid:12)) while Ô¨Åx-
P(cid:12)
ing (cid:18)(t)
(x) = (cid:18)P (x) for all x 2 S+ n dom((cid:12)). To ensure
P(cid:12)
(cid:17)(t)
(?) = 1, we add ? to dom((cid:12)) and (cid:12)(?) = 1. We
P(cid:12)
update (cid:18)(t)
P(cid:12)
(x) = (cid:18)(t)
P(cid:12)

(x) at each step t as

(y) (cid:0) (cid:12)(y)

(cid:18)(t+1)
P(cid:12)

J ‚Ä≤(cid:0)1

(x) (cid:0)

(cid:17)(t)
P(cid:12)

‚àë

xy

(

)

;

J ‚Ä≤

xy =

y2dom((cid:12))

@(cid:17)(t)
P(cid:12)
@(cid:18)(t)
P(cid:12)

(x)

(y)

=

‚àë

s2S

(cid:16)(x; s)(cid:16)(y; s)p(t)
(cid:12) (s)
(cid:0) jSj(cid:17)(t)
(x)(cid:17)(t)
P(cid:12)
P(cid:12)
(?) as it is not

(y):

In this case, we also need to update (cid:18)(t)
P(cid:12)
guaranteed to be Ô¨Åxed. Let us deÔ¨Åne
(

)

‚Ä≤(t+1)
p
(cid:12)

(x) = p(t)

(cid:12) (x)

‚àè

exp

s2dom((cid:12))

exp

(s)

(cid:18)(t+1)
P(cid:12)
(
(cid:18)(t)
P(cid:12)

(s)

) (cid:16)(s; x):

Since we have

p(t+1)
(cid:12)

(x) =

) p

‚Ä≤(t+1)
(cid:12)

(x);

)

(

exp

exp

(?)

(cid:18)(t+1)
P(cid:12)
(
(cid:18)(t)
P(cid:12)

(?)

it follows that
(cid:18)(t+1)
P(cid:12)

(?) (cid:0) (cid:18)(t)
P(cid:12)
(

(?)

(
(cid:18)(t)
P(cid:12)

= (cid:0) log

exp

(?)

+

)

‚Ä≤(t+1)
(cid:12)

p

(x)

;

)

‚àë

x2S+

The time complexity of each iteration is O(jdom((cid:12))j3),
which is required to compute the inverse of the Jacobian
matrix.

Global convergence of the projection algorithm is always
guaranteed by the convexity of a submanifold S((cid:12)) deÔ¨Åned
in Equation (14). Since S((cid:12)) is always convex with respect
to the (cid:18)- and (cid:17)-coordinates, it is straightforward to see that
our e-projection is an instance of the Bregman algorithm
onto a convex region, which is well known to always con-
verge to the global solution (Censor & Lent, 1981).

5. Balancing Matrices and Tensors

Now we are ready to solve the problem of matrix and tensor
balancing as projection on a dually Ô¨Çat manifold.

5.1. Matrix Balancing

Recall that the task of matrix balancing is to Ô¨Ånd r; s 2 Rn
that satisfy (RAS)1 = 1 and (RAS)T 1 = 1 with R =
diag(r) and S = diag(s) for a given nonnegative square
matrix A = (aij) 2 Rn(cid:2)n
(cid:21)0 .

Let us deÔ¨Åne S as

S = f(i; j) j i; j 2 [n] and aij Ã∏= 0g;
(15)
where we remove zero entries from the outcome space S as
our formulation cannot treat zero probability, and give each
probability as p((i; j)) = aij=
ij aij. The partial order
(cid:20) of S is naturally introduced as

‚àë

x = (i; j) (cid:20) y = (k; l) , i (cid:20) j and k (cid:20) l;

(16)
resulting in ? = (1; 1). In addition, we deÔ¨Åne (cid:19)k;m for
each k 2 [n] and m 2 f1; 2g such that

(cid:19)k;m = minf x = (i1; i2) 2 S j im = k g;
where the minimum is with respect to the order (cid:20). If (cid:19)k;m
does not exist, we just remove the entire kth row if m = 1
or kth column if m = 2 from A. Then we switch rows and
columns of A so that the condition

(cid:19)1;m (cid:20) (cid:19)2;m (cid:20) (cid:1) (cid:1) (cid:1) (cid:20) (cid:19)n;m
(17)
is satisÔ¨Åed for each m 2 f1; 2g, which is possible for any
matrices. Since we have
{ ‚àë
n
j=1 p((k; j))
‚àë
n
i=1 p((i; k))

(cid:17)((cid:19)k;m) (cid:0) (cid:17)((cid:19)k+1;m) =

if m = 1;
if m = 2

if the condition (17) is satisÔ¨Åed, the probability distribution
is balanced if for all k 2 [n] and m 2 f1; 2g

(cid:17)((cid:19)k;m) =

n(cid:0)k+1
n

:

Therefore, we obtain the following result.

Matrix balancing as e-projection:
Given a matrix A 2 Rn(cid:2)n with its normalized probabil-
ity distribution P 2 S such that p((i; j)) = aij=
ij aij.
DeÔ¨Åne the poset (S; (cid:20)) by Equations (15) and (16) and let
S((cid:12)) be the submanifold of S such that

‚àë

S((cid:12)) = fP 2 S j (cid:17)P (x) = (cid:12)(x) for all x 2 dom((cid:12))g;

where the function (cid:12) is given as

dom((cid:12)) = f(cid:19)k;m 2 S j k 2 [n]; m 2 f1; 2gg;

(cid:12)((cid:19)k;m) =

n(cid:0)k+1
n

:

Matrix balancing is the e-projection of P onto the subman-
ifold S((cid:12)), that is, the balanced matrix (RAS)=n is the
distribution P(cid:12) such that

{

(cid:18)P(cid:12) (x) = (cid:18)P (x)
(cid:17)P(cid:12) (x) = (cid:12)(x)

if x 2 S+ n dom((cid:12));
if x 2 dom((cid:12));

which is unique and always exists in S, thanks to its dually
Ô¨Çat structure. Moreover, two balancing vectors r and s are

exp

(cid:18)P(cid:12) ((cid:19)k;m) (cid:0) (cid:18)P ((cid:19)k;m)
‚àë

k=1

for every i 2 [n] and r = rn=

ij aij.

ri
ai

if m = 1;
if m = 2;

‚ñ†

)

{

=

(

i‚àë

Tensor Balancing on Statistical Manifold

5.2. Tensor Balancing

Next, we generalize our approach from matrices to tensors.
For an N th order tensor A = (ai1i2:::iN ) 2 Rn1(cid:2)n2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nN
and a vector b 2 Rnm, the m-mode product of A and b is
deÔ¨Åned as

(A (cid:2)m b)i1:::im(cid:0)1im+1:::iN

=

ai1i2:::iN bim :

nm‚àë

im=1

We deÔ¨Åne tensor balancing as follows: Given a tensor A 2
Rn1(cid:2)n2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nN with n1 = (cid:1) (cid:1) (cid:1) = nN = n, Ô¨Ånd (N (cid:0) 1)
order tensors R1; R2; : : : ; RN such that

A‚Ä≤ (cid:2)m 1 = 1 (2 Rn1(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nm(cid:0)1(cid:2)nm+1(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nN )
‚àë
n

for all m 2 [N ], i.e.,
entry a‚Ä≤

im=1 a‚Ä≤
i1i2:::iN of the balanced tensor A‚Ä≤ is given as
‚àè
a‚Ä≤
i1i2:::iN

= ai1i2:::iN

i1:::im(cid:0)1im+1:::iN

i1i2:::iN

Rm

:

(18)

= 1, where each

m2[N ]
A tensor A‚Ä≤ that satisÔ¨Åes Equation (18) is called multi-
stochastic (Cui et al., 2014). Note that this is exactly the
same as the matrix balancing problem if N = 2.

It is straightforward to extend matrix balancing to tensor
balancing as e-projection onto a submanifold. Given a ten-
sor A 2 Rn1(cid:2)n2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nN with its normalized probability
distribution P such that

p(x) = ai1i2:::iN

aj1j2:::jN

(19)

/ ‚àë

j1j2:::jN

‚àë

for all x = (i1; i2; : : : ; iN ). The objective is to obtain
im=1 p(cid:12)((i1; : : : ; iN )) = 1=(nN (cid:0)1) for all
P(cid:12) such that
m 2 [N ] and i1; : : : ; iN 2 [n]. In the same way as matrix
balancing, we deÔ¨Åne S as

n

{

S =

(i1; i2; : : : ; iN ) 2 [n]N

(cid:12)
(cid:12) ai1i2:::iN

}

Ã∏= 0

with removing zero entries and the partial order (cid:20) as
x = (i1 : : : iN ) (cid:20) y = (j1 : : : jN ) , 8m 2 [N ]; im (cid:20) jm:
In addition, we introduce (cid:19)k;m as

(cid:19)k;m = minf x = (i1; i2; : : : ; iN ) 2 S j im = k g:

and require the condition in Equation (17).

Tensor balancing as e-projection:
Given a tensor A 2 Rn1(cid:2)n2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nN with its normalized
probability distribution P 2 S given in Equation (19). The
submanifold S((cid:12)) of multistochastic tensors is given as
S((cid:12)) = fP 2 S j (cid:17)P (x) = (cid:12)(x) for all x 2 dom((cid:12))g;

where the domain of the function (cid:12) is given as

dom((cid:12)) = f (cid:19)k;m j k 2 [n]; m 2 [N ] g

and each value is described using the zeta function as

(cid:12)((cid:19)k;m) =

(cid:16)((cid:19)k;m; (cid:19)l;m)

1
nN (cid:0)1 :

‚àë

l2[n]

Tensor balancing is the e-projection of P onto the subman-
ifold S((cid:12)), that is, the multistochastic tensor is the distri-
bution P(cid:12) such that

{

(cid:18)P(cid:12) (x) = (cid:18)P (x)
(cid:17)P(cid:12) (x) = (cid:12)(x)

if x 2 S+ n dom((cid:12));
if x 2 dom((cid:12));

which is unique and always exists in S, thanks to its dually
Ô¨Çat structure. Moreover, each balancing tensor Rm is

Rm

0

i1:::im(cid:0)1im+1:::iN
‚àë

im‚Ä≤‚àë

= exp

@

m‚Ä≤Ã∏=m

k=1

(cid:18)P(cid:12) ((cid:19)k;m‚Ä≤) (cid:0) (cid:18)P ((cid:19)k;m‚Ä≤)

1

A

for every m 2 [N ] and R1 = R1nN (cid:0)1=
to recover a multistochastic tensor.

j1:::jN

aj1:::jN
‚ñ†

‚àë

Our result means that the e-projection algorithm based on
Newton‚Äôs method proposed in Section 4.3 converges to the
unique balanced tensor whenever S((cid:12)) Ã∏= ‚àÖ holds.

6. Conclusion

In this paper, we have solved the open problem of tensor
balancing and presented an efÔ¨Åcient balancing algorithm
using Newton‚Äôs method. Our algorithm quadratically con-
verges, while the popular Sinkhorn-Knopp algorithm lin-
early converges. We have examined the efÔ¨Åciency of our
algorithm in numerical experiments on matrix balancing
and showed that the proposed algorithm is several orders
of magnitude faster than the existing approaches.

We have analyzed theories behind the algorithm, and
proved that balancing is e-projection in a special type of
a statistical manifold, in particular, a dually Ô¨Çat Rieman-
nian manifold studied in information geometry. Our key
Ô¨Ånding is that the gradient of the manifold, equivalent to
Riemannian metric or the Fisher information matrix, can be
analytically obtained using the M¬®obius inversion formula.

Our information geometric formulation can model several
machine learning applications such as statistical analysis
on a DAG structure. Thus, we can perform efÔ¨Åcient learn-
ing as projection using information of the gradient of man-
ifolds by reformulating such models, which we will study
in future work.

Acknowledgements

The authors sincerely thank Marco Cuturi for his valu-
able comments.
This work was supported by JSPS
KAKENHI Grant Numbers JP16K16115, JP16H02870
(MS), JP26120732 and JP16H06570 (HN). The research
of K.T. was supported by JST CREST JPMJCR1502,
RIKEN PostK, KAKENHI Nanostructure and KAKENHI
JP15H05711.

Tensor Balancing on Statistical Manifold

References

2012.

Agresti, A. Categorical data analysis. Wiley, 3 edition,

Ahmed, M., De Loera, J., and Hemmecke, R. Polyhedral
Cones of Magic Cubes and Squares, volume 25 of Algo-
rithms and Combinatorics, pp. 25‚Äì41. Springer, 2003.

Akartunalƒ±, K. and Knight, P. A. Network models and
biproportional rounding for fair seat allocations in the
UK elections. Annals of Operations Research, pp. 1‚Äì19,
2016.

the National Academy of Sciences, 108(23):9679‚Äì9684,
2011.

Gierz, G., Hofmann, K. H., Keimel, K., Lawson, J. D., Mis-
love, M., and Scott, D. S. Continuous Lattices and Do-
mains. Cambridge University Press, 2003.

Idel, M. A review of matrix scaling and sinkhorn‚Äôs normal
form for matrices and positive maps. arXiv:1609.06349,
2016.

Ito, K. (ed.). Encyclopedic Dictionary of Mathematics. The

MIT Press, 2 edition, 1993.

Amari, S.

Information geometry on hierarchy of proba-
bility distributions. IEEE Transactions on Information
Theory, 47(5):1701‚Äì1711, 2001.

Knight, P. A. The Sinkhorn‚ÄìKnopp algorithm: Conver-
gence and applications. SIAM Journal on Matrix Analy-
sis and Applications, 30(1):261‚Äì275, 2008.

Amari, S. Information geometry and its applications: Con-
In Nielsen, F.
vex function and dually Ô¨Çat manifold.
(ed.), Emerging Trends in Visual Computing: LIX Fall
Colloquium, ETVC 2008, Revised Invited Papers, pp.
75‚Äì102. Springer, 2009.

Amari, S.

Information geometry of positive measures
and positive-deÔ¨Ånite matrices: Decomposable dually Ô¨Çat
structure. Entropy, 16(4):2131‚Äì2145, 2014.

Amari, S.

Information Geometry and Its Applications.

Springer, 2016.

Balinski, M. Fair majority voting (or how to eliminate ger-
rymandering). American Mathematical Monthly, 115(2):
97‚Äì113, 2008.

Censor, Y. and Lent, A. An iterative row-action method for
interval convex programming. Journal of Optimization
Theory and Applications, 34(3):321‚Äì353, 1981.

Chang, H., Paksoy, V. E., and Zhang, F. Polytopes of
stochastic tensors. Annals of Functional Analysis, 7(3):
386‚Äì393, 2016.

Cui, L.-B., Li, W., and Ng, M. K. Birkhoff‚Äìvon Neumann
theorem for multistochastic tensors. SIAM Journal on
Matrix Analysis and Applications, 35(3):956‚Äì973, 2014.

Cuturi, M. Sinkhorn distances: Lightspeed computation of
In Advances in Neural Information

optimal transport.
Processing Systems 26, pp. 2292‚Äì2300, 2013.

Frogner, C., Zhang, C., Mobahi, H., Araya, M., and Pog-
gio, T. A. Learning with a Wasserstein loss. In Advances
in Neural Information Processing Systems 28, pp. 2053‚Äì
2061, 2015.

Ganmor, E., Segev, R., and Schneidman, E. Sparse low-
order interaction network underlies a highly correlated
and learnable neural population code. Proceedings of

Knight, P. A. and Ruiz, D. A fast algorithm for matrix
balancing. IMA Journal of Numerical Analysis, 33(3):
1029‚Äì1047, 2013.

Lahr, M. and de Mesnard, L. Biproportional techniques
in input-output analysis: Table updating and structural
analysis. Economic Systems Research, 16(2):115‚Äì134,
2004.

Lamond, B. and Stewart, N. F. Bregman‚Äôs balancing
method. Transportation Research Part B: Methodologi-
cal, 15(4):239‚Äì248, 1981.

Livne, O. E. and Golub, G. H. Scaling by binormalization.

Numerical Algorithms, 35(1):97‚Äì120, 2004.

Marshall, A. W. and Olkin, I. Scaling of matrices to achieve
speciÔ¨Åed row and column sums. Numerische Mathe-
matik, 12(1):83‚Äì90, 1968.

Miller, R. E. and Blair, P. D. Input-Output Analysis: Foun-
dations and Extensions. Cambridge University Press, 2
edition, 2009.

Moon, T. K., Gunther, J. H., and Kupin, J. J. Sinkhorn
solves sudoku. IEEE Transactions on Information The-
ory, 55(4):1741‚Äì1746, 2009.

Nakahara, H. and Amari, S. Information-geometric mea-
sure for neural spikes. Neural Computation, 14(10):
2269‚Äì2316, 2002.

Nakahara, H., Nishimura, S., Inoue, M., Hori, G., and
Amari, S. Gene interaction in DNA microarray data is
decomposed by information geometric measure. Bioin-
formatics, 19(9):1124‚Äì1131, 2003.

Nakahara, H., Amari, S., and Richmond, B. J. A com-
parison of descriptive models of a single spike train by
information-geometric measure. Neural computation, 18
(3):545‚Äì568, 2006.

Tensor Balancing on Statistical Manifold

Parikh, A. Forecasts of input-output matrices using the
R.A.S. method. The Review of Economics and Statistics,
61(3):477‚Äì481, 1979.

Parlett, B. N. and Landis, T. L. Methods for scaling to dou-
bly stochastic form. Linear Algebra and its Applications,
48:53‚Äì79, 1982.

Rao, S. S. P., Huntley, M. H., Durand, N. C., Stamenova,
E. K., Bochkov, I. D., Robinson, J. T., Sanborn, A. L.,
Machol, I., Omer, A. D., Lander, E. S., and Aiden, E. L.
A 3D map of the human genome at kilobase resolution
reveals principles of chromatin looping. Cell, 159(7):
1665‚Äì1680, 2014.

Rota, G.-C. On the foundations of combinatorial theory I:
Theory of M¬®obius functions. Z. Wahrseheinlichkeitsthe-
orie, 2:340‚Äì368, 1964.

Sinkhorn, R. A relationship between arbitrary positive ma-
trices and doubly stochastic matrices. The Annals of
Mathematical Statistics, 35(2):876‚Äì879, 06 1964.

Sinkhorn, R. and Knopp, P. Concerning nonnegative ma-
trices and doubly stochastic matrices. PaciÔ¨Åc Journal of
Mathematics, 21(2):343‚Äì348, 1967.

Solomon, J., de Goes, F., Peyr¬¥e, G., Cuturi, M., Butscher,
A., Nguyen, A., Du, T., and Guibas, L. Convolutional
Wasserstein distances: EfÔ¨Åcient optimal transportation
on geometric domains. ACM Transactions on Graphics,
34(4):66:1‚Äì66:11, 2015.

Soules, G. W. The rate of convergence of sinkhorn bal-
ancing. Linear Algebra and its Applications, 150:3‚Äì40,
1991.

Sugiyama, M., Nakahara, H., and Tsuda, K. Information
In 2016 IEEE In-
decomposition on structured space.
ternational Symposium on Information Theory, pp. 575‚Äì
579, July 2016.

Wu, H.-J. and Michor, F. A computational strategy to adjust
for copy number in tumor Hi-C data. Bioinformatics, 32
(24):3695‚Äì3701, 2016.

