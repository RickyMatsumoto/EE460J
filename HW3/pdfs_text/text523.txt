Optimal Algorithms for Smooth and Strongly Convex
Distributed Optimization in Networks

Kevin Scaman 1 Francis Bach 2 S´ebastien Bubeck 3 Yin Tat Lee 3 Laurent Massouli´e 1

Abstract

In this paper, we determine the optimal conver-
gence rates for strongly convex and smooth dis-
tributed optimization in two settings: central-
ized and decentralized communications over a
network. For centralized (i.e. master/slave) al-
gorithms, we show that distributing Nesterov’s
is optimal and
accelerated gradient descent
√
κg(1 +
achieves a precision ε > 0 in time O(
∆τ ) ln(1/ε)), where κg is the condition num-
ber of the (global) function to optimize, ∆ is
the diameter of the network, and τ (resp. 1)
is the time needed to communicate values be-
tween two neighbors (resp. perform local com-
putations). For decentralized algorithms based
on gossip, we provide the ﬁrst optimal algorithm,
called the multi-step dual accelerated (MSDA)
method, that achieves a precision ε > 0 in time
γ ) ln(1/ε)), where κl is the con-
O(
dition number of the local functions and γ is the
(normalized) eigengap of the gossip matrix used
for communication between nodes. We then ver-
ify the efﬁciency of MSDA against state-of-the-
art methods for two problems: least-squares re-
gression and classiﬁcation by logistic regression.

κl(1 + τ√

√

1. Introduction

Given the numerous applications of distributed optimiza-
tion in machine learning, many algorithms have recently
emerged, that allow the minimization of objective functions
(cid:80)n
f deﬁned as the average 1
i=1 fi of functions fi which
n
are respectively accessible by separate nodes in a network
(Nedic & Ozdaglar, 2009; Boyd et al., 2011; Duchi et al.,
2012; Shi et al., 2015). These algorithms typically alter-

1MSR-INRIA Joint Center, Palaiseau, France 2INRIA, Ecole
Normale Sup´erieure, Paris, France 3Theory group, Microsoft Re-
search, Redmond, United States. Correspondence to: Kevin Sca-
man <kevin.scaman@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

nate local incremental improvement steps (such as gradient
steps) with communication steps between nodes in the net-
work, and come with a variety of convergence rates (see
for example Shi et al. (2014; 2015); Jakoveti´c et al. (2015);
Nedich et al. (2016)).

Two main regimes have been looked at: (a) centralized
where communications are precisely scheduled and (b) de-
centralized where communications may not exhibit a pre-
cise schedule. In this paper, we consider these two regimes
for objective functions which are smooth and strongly-
convex and for which algorithms are linearly (exponen-
tially) convergent. The main contribution of this paper is
to propose new and matching upper and lower bounds of
complexity for this class of distributed problems.

The optimal complexity bounds depend on natural quan-
tities in optimization and network theory. Indeed, (a) for
a single machine the optimal number of gradient steps to
optimize a function is proportional to the square root of
the condition number (Nesterov, 2004), and (b) for mean
estimation, the optimal number of communication steps is
proportional to the diameter of the network in centralized
problems or to the square root of the eigengap of the Lapla-
cian matrix in decentralized problems (Boyd et al., 2006).
As shown in Section 3, our lower complexity bounds hap-
pen to be combinations of the two contributions above.

These lower complexity bounds are attained by two sepa-
rate algorithms. In the centralized case, the trivial distri-
bution of Nesterov’s accelerated gradient attains this rate,
while in the decentralized case, as shown in Section 4, the
rate is achieved by a dual algorithm. We compare favorably
our new optimal algorithms to existing work in Section 5.

Related Work. Decentralized optimization has been ex-
tensively studied and early methods such as decentral-
ized gradient descent (Nedic & Ozdaglar, 2009; Jakoveti´c
et al., 2014) or decentralized dual averaging (Duchi et al.,
2012) exhibited sublinear convergence rates. More re-
cently, a number of methods with provable linear con-
vergence rates were developed, including EXTRA (Shi
et al., 2015; Mokhtari & Ribeiro, 2016), augmented La-
grangians (Jakoveti´c et al., 2015), and more recent ap-
proaches (Nedich et al., 2016). The most popular of

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

such approaches is the distributed alternating direction
method of multipliers (D-ADMM) (Boyd et al., 2011;
Wei & Ozdaglar, 2012; Shi et al., 2014) and has led to
a large number of variations and extensions.
In a dif-
ferent direction, second order methods were also investi-
gated (Mokhtari et al., 2016; Tutunov et al., 2016). How-
ever, to the best of our knowledge, the ﬁeld still lacks
a coherent theoretical understanding of the optimal con-
vergence rates and its dependency on the characteristics
In several related ﬁelds,
of the communication network.
complexity lower bounds were recently investigated, in-
cluding the sequential optimization of a sum of functions
(Arjevani & Shamir, 2016a;b), distributed optimization in
ﬂat (i.e. totally connected) networks (Shamir, 2014; Arje-
vani & Shamir, 2015), or distributed stochastic optimiza-
tion (Shamir & Srebro, 2014).

2. Distributed Optimization Setting

2.1. Optimization Problem

Let G = (V, E) be a connected simple (i.e. undirected)
graph of n computing units and diameter ∆, each having
access to a function fi(θ) over θ ∈ Rd. We consider mini-
mizing the average of the local functions

¯f (θ) =

min
θ∈Rd

1
n

n
(cid:88)

i=1

fi(θ)

(1)

in a distributed setting. More speciﬁcally, we assume that:

1. Each computing unit can compute ﬁrst-order charac-
teristics, such as the gradient of its own function or
its Fenchel conjugate. By renormalization of the time
axis, and without loss of generality, we assume that
this computation is performed in one unit of time.

2. Each computing unit can communicate values (i.e.
vectors in Rd) to its neighbors. This communication
requires a time τ (which may be smaller or greater
than 1).

These actions may be performed asynchronously and in
parallel, and each node i possesses a local version of the
parameter, which we refer to as θi. Moreover, we as-
sume that each function fi is α-strongly convex and β-
smooth, and we denote by κl = β
α ≥ 1 the local condition
number. We also denote by αg, βg and κg, respectively,
the strong convexity, smoothness and condition number
of the average (global) function ¯f . Note that we always
have κg ≤ κl, while the opposite inequality is, in gen-
eral, not true (take for example f1(θ) = 1{θ < 0}θ2 and
f2(θ) = 1{θ > 0}θ2 for which κl = +∞ and κg = 1).
However, the two quantities are close (resp. equal) when
the local functions are similar (resp. equal) to one another.

2.2. Decentralized Communication

A large body of literature considers a decentralized ap-
proach to distributed optimization based on the gossip al-
gorithm (Boyd et al., 2006; Nedic & Ozdaglar, 2009; Duchi
et al., 2012; Wei & Ozdaglar, 2012). In such a case, com-
munication is represented as a matrix multiplication with a
matrix W verifying the following constraints:

1. W is an n × n symmetric matrix,

2. W is positive semi-deﬁnite,

3. The kernel of W is the set of constant vectors:

Ker(W ) = Span(1), where 1 = (1, ..., 1)(cid:62),

4. W is deﬁned on the edges of the network: Wij (cid:54)= 0

only if i = j or (i, j) ∈ E.

The third condition will ensure that the gossip step con-
verges to the average of all the vectors shared between the
nodes. We will denote the matrix W as the gossip matrix,
since each communication step will be represented using
it. Note that a simple choice for the gossip matrix is the
Laplacian matrix L = D − A, where A is the adjacency
matrix of the network and D = diag (cid:0)(cid:80)
(cid:1). However,
in the presence of large degree nodes, weighted Laplacian
matrices are usually a better choice, and the problem of op-
timizing these weights is known as the fastest distributed
consensus averaging problem and is investigated by Xiao
& Boyd (2004); Boyd et al. (2009).

i Aij

We will denote by λ1(W ) ≥ · · · ≥ λn(W ) = 0 the spec-
trum of the gossip matrix W , and its (normalized) eigengap
the ratio γ(W ) = λn−1(W )/λ1(W ) between the second
smallest and the largest eigenvalue. Equivalently, this is
the inverse of the condition number of W projected on the
space orthogonal to the constant vector 1. This quantity
will be the main parameter describing the connectivity of
the communication network in Section 3.3 and Section 4.

3. Optimal Convergence Rates

In this section, we prove oracle complexity lower bounds
for distributed optimization in two settings: strongly con-
vex and smooth functions for centralized (i.e. master/slave)
and decentralized algorithms based on a gossip matrix W .

In the ﬁrst setting, we show that distributing accelerated
gradient descent matches the optimal convergence rate,
while, in the second setting, the algorithm proposed in Sec-
tion 4 is shown to be optimal. Note that we will use the
notation g(ε) = Ω(f (ε)) for ∃C > 0 s.t. ∀ε > 0, g(ε) ≥
Cf (ε), and will, for simplicity, omit the additive terms that
do not depend on the precision ε in Corollary 1 and Corol-
lary 2.

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

3.1. Black-box Optimization Procedures

The lower bounds provided hereafter depend on a new no-
tion of black-box optimization procedures for the problem
in Eq. (1), where we consider distributed algorithms veri-
fying the following constraints:

1. Local memory: each node i can store past values in
a (ﬁnite) internal memory Mi,t ⊂ Rd at time t ≥ 0.
These values can be accessed and used at time t by
the algorithm run by node i, and are updated either
by local computation or by communication (deﬁned
below), that is, for all i ∈ {1, ..., n},

Mi,t ⊂ Mcomp

i,t ∪ Mcomm

i,t

.

(2)

2. Local computation: each node i can, at time t, com-
pute the gradient of its local function ∇fi(θ) or its
Fenchel conjugate ∇f ∗
i (θ) for a value θ ∈ Mi,t in the
node’s internal memory, that is, for all i ∈ {1, ..., n},

Mcomp

i,t = Span ({θ, ∇fi(θ), ∇f ∗

i (θ) : θ ∈ Mi,t−1}) .
(3)
3. Local communication: each node i can, at time t,
share a value to all or part of its neighbors, that is, for
all i ∈ {1, ..., n},

Mcomm
i,t

= Span

(cid:18) (cid:91)

(cid:19)
.

Mj,t−τ

(4)

(i,j)∈E

4. Output value: each node i must, at time t, specify one
vector in its memory as local output of the algorithm,
that is, for all i ∈ {1, ..., n},

θi,t ∈ Mi,t.

(5)

Hence, a black-box procedure will return n output values—
one for each node of the network—and our analysis will
focus on ensuring that all local output values are converg-
ing to the optimal parameter of Eq. (1). Moreover, we will
say that a black-box procedure uses a gossip matrix W if
the local communication is achieved by multiplication of a
vector with W . For simplicity, we assume that all nodes
start with the simple internal memory Mi,0 = {0}. Note
that communications and local computations may be per-
formed in parallel and asynchronously.

3.2. Centralized Algorithms

√

√

In this section, we show that, for any black-box optimiza-
κg ln(1/ε)) gradient steps and
tion procedure, at least Ω(
Ω(∆
κg ln(1/ε)) communication steps are necessary to
achieve a precision ε > 0, where κg is the global condi-
tion number and ∆ is the diameter of the network. These
lower bounds extend the communication complexity lower
bounds for totally connected communication networks of

√

Arjevani & Shamir (2015), and are natural since at least
κg ln(1/ε)) steps are necessary to solve a strongly
Ω(
convex and smooth problem up to a ﬁxed precision, and
at least ∆ communication steps are required to transmit a
message between any given pair of nodes.

In order to simplify the proofs of the following theorems,
and following the approach of Bubeck (2015), we will
consider the limiting situation d → +∞. More speciﬁ-
cally, we now assume that we are working in (cid:96)2 = {θ =
(θk)k∈N : (cid:80)
k < +∞} rather than Rd.
Theorem 1. Let G be a graph of diameter ∆ > 0 and
size n > 0, and βg ≥ αg > 0. There exists n functions
: (cid:96)2 → R such that ¯f is αg strongly convex and βg
fi
smooth, and for any t ≥ 0 and any black-box procedure
one has, for all i ∈ {1, ..., n},

k θ2

¯f (θi,t) − ¯f (θ∗) ≥

where κg = βg/αg.

(cid:18)

1 −

αg
2

4
√
κg

(cid:19)1+ t

1+∆τ

(cid:107)θi,0 − θ∗(cid:107)2,

(6)

The proof of Theorem 1 relies on splitting the function
used by Nesterov to prove oracle complexities for strongly
convex and smooth optimization (Nesterov, 2004; Bubeck,
2015) on two nodes at distance ∆. One can show that most
dimensions of the parameters θi,t will remain zero, and lo-
cal gradient computations may only increase the number
of non-zero dimensions by one. Finally, at least ∆ com-
munication rounds are necessary in-between every gradi-
ent computation, in order to share information between the
two nodes. The detailed proof is available as supplemen-
tary material.
Corollary 1. For any graph of diameter ∆ and any black-
box procedure, there exists functions fi such that the time
to reach a precision ε > 0 is lower bounded by
(cid:16)

(cid:19)(cid:19)

(cid:17)

(cid:18)√

Ω

κg

1 + ∆τ

ln

,

(7)

(cid:18) 1
ε

This optimal convergence rate is achieved by distributing
Nesterov’s accelerated gradient descent on the global func-
tion. Computing the gradient of ¯f is performed by send-
ing all the local gradients ∇fi to a single node (denoted
as master node) in ∆ communication steps (which may
involve several simultaneous messages), and then return-
ing the new parameter θt+1 to every node in the network
(which requires another ∆ communication steps). In prac-
tice, summing the gradients can be distributed by comput-
ing a spanning tree (with the root as master node), and ask-
ing for each node to perform the sum of its children’s gra-
dients before sending it to its parent. Standard methods as
described by Bertsekas & Tsitsiklis (1989) can be used for
performing this parallelization of gradient computations.

This algorithm has three limitations: ﬁrst, the algorithm is
not robust to machine failures, and the central role played

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

by the master node also means that a failure of this particu-
lar machine may completely freeze the procedure. Second,
and more generally, the algorithm requires precomputing a
spanning tree, and is thus not suited to time-varying graphs,
in which the connectivity between the nodes may change
through time (e.g. in peer-to-peer networks). Finally, the
algorithm requires every node to complete its gradient com-
putation before aggregating them on the master node, and
the efﬁciency of the algorithm thus depends on the slow-
est of all machines. Hence, in the presence of non-uniform
latency of the local computations, or the slow down of a
speciﬁc machine due to a hardware failure, the algorithm
will suffer a signiﬁcant drop in performance.

3.3. Decentralized Algorithms

The gossip algorithm (Boyd et al., 2006) is a standard
method for averaging values across a network when its con-
nectivity may vary through time. This approach was shown
to be robust against machine failures, non-uniform laten-
cies and asynchronous or time-varying graphs, and a large
body of literature extended this algorithm to distributed op-
timization (Nedic & Ozdaglar, 2009; Duchi et al., 2012;
Wei & Ozdaglar, 2012; Shi et al., 2015; Jakoveti´c et al.,
2015; Nedich et al., 2016; Mokhtari et al., 2016).

The convergence analysis of decentralized algorithms usu-
ally relies on the spectrum of the gossip matrix W used
for communicating values in the network, and more specif-
ically on the ratio between the second smallest and the
largest eigenvalue of W , denoted γ.
In this section, we
show that, with respect to this quantity and κl, reaching a
κl ln(1/ε)) gradient steps
precision ε requires at least Ω(
(cid:17)
γ ln(1/ε)
communication steps, by exhibiting

(cid:16)(cid:113) κl

and Ω

√

a gossip matrix such that a corresponding lower bound ex-
ists.

Theorem 2. Let α, β > 0 and γ ∈ (0, 1]. There exists a
gossip matrix W of eigengap γ(W ) = γ, and α-strongly
convex and β-smooth functions fi : (cid:96)2 → R such that, for
any t ≥ 0 and any black-box procedure using W one has,
for all i ∈ {1, ..., n},

¯f (θi,t) − ¯f (θ∗) ≥

(cid:18)

1 −

3α
2

16
√
κl

(cid:19)1+ t
1+ τ
√
5

γ (cid:107)θi,0 − θ∗(cid:107)2,

where κl = β/α is the local condition number.

The proof of Theorem 2 relies on the same technique as
that of Theorem 1, except that we now split the two func-
tions on a subset of a linear graph. These networks have
γ, and we can thus
the appreciable property that ∆ ≈ 1/
use a slightly extended version of Theorem 1 to derive the
desired result. The complete proof is available as supple-
mentary material.

√

Corollary 2. For any γ > 0, there exists a gossip matrix
W of eigengap γ and α-strongly convex, β-smooth func-
tions such that, with κl = β/α, for any black-box proce-
dure using W the time to reach a precision ε > 0 is lower
bounded by

(cid:18)√

(cid:18)

Ω

κl

1 +

τ
√
γ

(cid:19)

ln

(cid:18) 1
ε

(cid:19)(cid:19)

.

(9)

√

√

γ =

We will see in the next section that this lower bound is met
for a novel decentralized algorithm called multi-step dual
accelerated (MSDA) and based on the dual formulation of
the optimization problem. Note that these results provide
optimal convergence rates with respect to κl and γ, but do
not imply that γ is the right quantity to consider on gen-
√
γ may indeed be very large
eral graphs. The quantity 1/
compared to ∆, for example for star networks, for which
n. However, on many simple net-
∆ = 2 and 1/
works, the diameter ∆ and the eigengap of the Laplacian
√
γ. For exam-
matrix are tightly connected, and ∆ ≈ 1/
√
γ ≈ 2n/π, for
ple, for linear graphs, ∆ = n − 1 and 1/
√
γ = 1, and
totally connected networks, ∆ = 1 and 1/
γ ≥
for regular networks, 1/
(Alon & Milman,
1985). Finally, note that the case of totally connected net-
works corresponds to a previous complexity lower bound
on communications proven by Arjevani & Shamir (2015),
and is equivalent to our result for centralized algorithms
with ∆ = 1.

∆
2 ln2 n

√

√

2

4. Optimal Decentralized Algorithms

In this section, we present a simple framework for solving
the optimization problem in Eq. (1) in a decentralized set-
ting, from which we will derive several variants, including
a synchronized algorithm whose convergence rate matches
the lower bound in Corollary 2. Note that the naive ap-
proach of distributing each (accelerated) gradient step by
gossiping does not lead to a linear convergence rate, as the
number of gossip steps has to increase with the number of
iterations to ensure the linear rate is preserved. We begin
with the simplest form of the algorithm, before extending
it to more advanced scenarios.

4.1. Single-Step Dual Accelerated Method

(8)

A standard approach for solving Eq. (1) (see Boyd et al.
(2011); Jakoveti´c et al. (2015)) consists in rewriting the op-
timization problem as

min
θ∈Rd

¯f (θ) = min

θ1=···=θn

fi(θi).

(10)

1
n

n
(cid:88)

i=1

Furthermore, the equality constraint θ1 = · · · = θn is
W = 0, where Θ = (θ1, . . . , θn) and
equivalent to Θ
W is a gossip matrix verifying the assumptions described

√

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

γ
γ

√
√

κl−
κl+

λ1(W ) , µ =

Rn×n, η = α

Algorithm 1 Single-Step Dual Accelerated method
Input: number of iterations T > 0, gossip matrix W ∈
√
√
Output: θi,T , for i = 1, ..., n
1: x0 = 0, y0 = 0
2: for t = 0 to T − 1 do
θi,t = ∇f ∗
3:
yt+1 = xt − ηΘtW
4:
xt+1 = (1 + µ)yt+1 − µyt
5:
6: end for

i (xi,t), for all i = 1, ..., n

√

W exists and is deﬁned as

in Section 2. Note that, since W is positive semi-deﬁnite,
√
W = V (cid:62)Σ1/2V , where
W = V (cid:62)ΣV is the singular value decomposition of W .
W = 0 implies that each row of Θ is con-
The equality Θ
√
W ) = Span(1)), and is thus equivalent
stant (since Ker(
to θ1 = · · · = θn. This leads to the following primal ver-
sion of the optimization problem:

√

(11)

(12)

(13)

(14)

where F (Θ) = (cid:80)n
i=1 fi(θi). Since Eq. (11) is a convex
problem, it is equivalent to its dual optimization problem:

min
Θ∈Rd×n : Θ

√

W =0

F (Θ),

√

−F ∗(λ

W ),

max
λ∈Rd×n

where F ∗(y) = supx∈Rd×n (cid:104)y, x(cid:105) − F (x) is the Fenchel
conjugate of F , and (cid:104)y, x(cid:105) = tr(y(cid:62)x) is the standard scalar
product between matrices.

The optimization problem in Eq. (12) is unconstrained and
convex, and can thus be solved using a variety of convex
optimization techniques. The proposed single-step dual ac-
celerated (SSDA) algorithm described in Alg. (1) uses Nes-
terov’s accelerated gradient descent, and can be thought of
as an accelerated version of the distributed augmented La-
grangian method of Jakoveti´c et al. (2015) for ρ = 0. The
algorithm is derived by noting that a gradient step of size
η > 0 for Eq. (12) is

λt+1 = λt − η∇F ∗(λt
√

√

√

W )

W ,

and the change of variable yt = λt

W leads to

yt+1 = yt − η∇F ∗(yt)W.

This equation can be interpreted as gossiping the gra-
dients of the local conjugate functions ∇f ∗
i (yi,t), since
∇F ∗(yt)ij = ∇f ∗
j (yj,t)i.
Theorem 3. The iterative scheme in Alg. (1) converges to
Θ = θ∗1(cid:62) where θ∗ is the solution of Eq. (1). Further-
more, the time needed for this algorithm to reach any given
precision ε > 0 is

This theorem relies on proving that the condition number
of the dual objective function is upper bounded by κl
γ , and
noting that the convergence rate for accelerated gradient
descent depends on the square root of the condition number
(see, e.g., Bubeck (2015)). A detailed proof is available as
supplementary material.

4.2. Multi-Step Dual Accelerated Method

The main problem of Alg. (1) is that it always performs the
same number of gradient and gossip steps. When commu-
nication is cheap compared to local computations (τ (cid:28) 1),
it would be preferable to perform more gossip steps than
gradient steps in order to propagate the local gradients fur-
ther than the local neighborhoods of each node. This can be
achieved by replacing W by PK(W ) in Alg. (1), where PK
is a polynomial of degree at most K. If PK(W ) is itself a
gossip matrix, then the analysis of the previous section can
be applied and the convergence rate of the resulting algo-
rithm depends on the eigengap of PK(W ). Maximizing
this quantity for a ﬁxed K leads to a common accelera-
tion scheme known as Chebyshev acceleration (Auzinger,
2011; Arioli & Scott, 2014) and the choice

PK(x) = 1 −

TK(c2(1 − x))
TK(c2)

,

(16)

where c2 = 1+γ
1−γ and TK are the Chebyshev polynomials
(Auzinger, 2011) deﬁned as T0(x) = 1, T1(x) = x, and,
for all k ≥ 1,

Tk+1(x) = 2xTk(x) − Tk−1(x).

(17)

Finally, verifying that this particular choice of PK(W ) is
indeed a gossip matrix, and taking K = (cid:98) 1√
γ (cid:99) leads to
Alg. (2) with an optimal convergence rate with respect to γ
and κl.
Theorem 4. The iterative scheme in Alg. (2) converges to
Θ = θ∗1(cid:62) where θ∗ is the solution of Eq. (1). Further-
more, the time needed for this algorithm to reach any given
precision ε > 0 is
(cid:18)√

(cid:19)(cid:19)

(cid:18)

(cid:19)

O

κl

1 +

.

(18)

τ
√
γ

ln

(cid:18) 1
ε

The proof of Theorem 4 relies on standard properties of
Chebyshev polynomials that imply that, for the particular
choice of K = (cid:98) 1√
≤ 2. Hence,
Theorem 3 applied to the gossip matrix W (cid:48) = PK(W )
gives the desired convergence rate. The complete proof is
available as supplementary material.

γ (cid:99), we have

γ(PK (W ))

1√

4.3. Discussion and Further Developments

(cid:18)

O

(1 + τ )

(cid:114) κl
γ

ln

(cid:18) 1
ε

(cid:19)(cid:19)

.

(15)

We now discuss several extensions to the proposed algo-
rithms.

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

γ

K =

1−γ , c3 =
√
1 )2 , µ = (1+cK
1 )
√
(1+cK
1 )

√
Rn×n, c1 = 1−
√
1+
(cid:106) 1√

Algorithm 2 Multi-Step Dual Accelerated method
Input: number of iterations T > 0, gossip matrix W ∈
γ , c2 = 1+γ
2
(1+γ)λ1(W ) ,
(cid:107)
κl−1+cK
, η = α(1+c2K
1 )
1
κl+1−cK
(1+cK
γ
1
Output: θi,T , for i = 1, ..., n
1: x0 = 0, y0 = 0
2: for t = 0 to T − 1 do
θi,t = ∇f ∗
3:
yt+1 = xt − η ACCELERATEDGOSSIP(Θt,W ,K)
4:
xt+1 = (1 + µ)yt+1 − µyt
5:
6: end for

i (xi,t), for all i = 1, ..., n

tion αg. However, their convergence rates are not optimal,
and it is still an open question to know if a rate close to
O
can be achieved with a decen-
tralized algorithm.

(cid:17)
γ ) ln(1/ε)

κg(1 + τ√

(cid:16)√

Asynchronous Setting. Accelerated stochastic gradient
descent such as SVRG (Johnson & Zhang, 2013) or SAGA
(Defazio et al., 2014) can be used on the dual problem in
Eq. (12) instead of accelerated gradient descent, in order
to obtain an asynchronous algorithm with a linear conver-
gence rate. The details and exact convergence rate of such
an approach are left as future work.

7: procedure ACCELERATEDGOSSIP(x,W ,K)
8: a0 = 1, a1 = c2
9: x0 = x, x1 = c2x(I − c3W )
10: for k = 1 to K − 1 do
11:
12:
13: end for
14: return x0 − xK
aK
15: end procedure

ak+1 = 2c2ak − ak−1
xk+1 = 2c2xk(I − c3W ) − xk−1

i (xi,t).

i (xi,t) = argminθ fi(θ) − x(cid:62)

Computation of ∇f ∗
In practice, it may be hard
to apply the dual algorithm when conjugate functions are
hard to compute. We now provide three potential solutions
to this problem: (1) warm starts may be used for the op-
timization problem ∇f ∗
i,tθ
by starting from the previous iteration θi,t−1. This will
drastically reduce the number of steps required for con-
vergence. (2) SSDA and MSDA can be extended to com-
posite functions of the form fi(θ) = gi(Biθ) + c(cid:107)θ(cid:107)2
2 for
Bi ∈ Rmi×d and gi smooth, and for which we know how
to compute the proximal operator. This allows applications
in machine learning such as logistic regression. See sup-
plementary material for details. (3) Beyond the composite
case, one can also add a small (well-chosen) quadratic term
to the dual, and by applying accelerated gradient descent on
the corresponding primal, get an algorithm that uses primal
gradient computations and achieves almost the same guar-
antee as SSDA and MSDA (off by a log(κl/γ) factor).

Local vs. Global Condition Number. MSDA and SSDA
depend on the worst strong convexity of the local functions
mini αi, which may be very small. A simple trick can be
used to depend on the average strong convexity. Using the
proxy functions gi(θ) = fi(θ)−(αi − ¯α)(cid:107)θ(cid:107)2
2 instead of fi,
where ¯α = 1
i αi is the average strong convexity, will
n
improve the local condition number from κl = maxi βi
to
mini αi

(cid:80)

κ(cid:48)
l =

maxi βi − αi
¯α
Several algorithms, including EXTRA (Shi et al., 2015)
and DIGing (Nedich et al., 2016), have convergence rates
that depend on the strong convexity of the global func-

− 1.

(19)

5. Experiments

In this section, we compare our new algorithms, single-step
dual accelerated (SSDA) descent and multi-step dual accel-
erated (MSDA) descent, to standard distributed optimiza-
tion algorithms in two settings:
least-squares regression
and classiﬁcation by logistic regression. Note that these ex-
periments on simple generated datasets are made to assess
the differences between existing state-of-the-art algorithms
and the ones provided in Section 4, and do not address the
practical implementation details nor the efﬁciency of the
compared algorithms on real-world distributed platforms.
The effect of latency, machine failures or variable commu-
nication time is thus left for future work.

5.1. Competitors and Setup

We compare SSDA and MSDA to four state-of-the-art dis-
tributed algorithms that achieve linear convergence rates:
distributed ADMM (D-ADMM) (Shi et al., 2014), EXTRA
(Shi et al., 2015), a recent approach named DIGing (Nedich
et al., 2016), and the distributed version of accelerated gra-
dient descent (DAGD) described in Section 3.2 and shown
to be optimal among centralized algorithms.

When available in the literature, we used the optimal pa-
rameters for each algorithm (see Theorem 2 by Shi et al.
(2014) for D-ADMM and Remark 3 by Shi et al. (2015)
for EXTRA). For the DIGing algorithm, the parameters
provided by Nedich et al. (2016) are very conservative,
and lead to a very slow convergence. We thus manually
optimized the parameter for this algorithm. The experi-
ments are simulated using a generated dataset consisting of
10, 000 samples randomly distributed to the nodes of a net-
work of size 100. In order to assess the effect of the con-
nectivity of the network, we ran each experiment on two
networks: one 10 × 10 grid and an Erd¨os-R´enyi random
network of average degree 6. The quality metric used in
this section is the maximum approximation error

et = max
i∈V

¯f (θi,t) − ¯f (θ∗),

(20)

where θ∗ is the optimal parameter of the optimization prob-

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

(a) high communication time: τ = 10

(a) high communication time: τ = 10

(b) low communication time: τ = 0.1

(b) low communication time: τ = 0.1

Figure 1. Maximum approximation error for least-squares regres-
sion on an Erd¨os-R´enyi random network of average degree 6
(n = 100).

Figure 2. Maximum approximation error for least-squares regres-
sion on a 10 × 10 grid graph (n = 100).

lem in Eq. (1). The computation time attributes a unit time
per local computation and a time τ per communication.

5.2. Least-squares Regression

The regularized least-squares regression problem consists
in solving the optimization problem

min
θ∈Rd

1
m

(cid:107)y − X (cid:62)θ(cid:107)2

2 + c(cid:107)θ(cid:107)2
2,

(21)

where X ∈ Rd×m is a matrix containing the m data points,
and y ∈ Rm is a vector containing the m associated val-
ues. The task is thus to minimize the empirical quadratic
error between a function yi = g(Xi) of d variables and its
linear regression ˆg(Xi) = X (cid:62)
i θ on the original dataset (for
i = 1, ..., m), while smoothing the resulting approximation
by adding a regularizer c(cid:107)θ(cid:107)2
2. For our experiments, we
ﬁxed c = 0.1, d = 10, and sampled m = 10, 000 Gaussian
random variables Xi ∼ N (0, 1) of mean 0 and variance 1.
1)+ξi
The function to regress is then yi = X (cid:62)
i
where ξi ∼ N (0, 1/4) is an i.i.d. Gaussian noise of vari-
ance 1/4. These data points are then distributed randomly
and evenly to the n = 100 nodes of the network. Note

1+cos(X (cid:62)
i

that the choice of function to regress y does not impact the
Hessian of the objective function, and thus the convergence
rate of the optimization algorithms.

Figure 1 and Figure 2 show the performance of the com-
pared algorithms on two networks: a 10 × 10 grid graph
and an Erd¨os-R´enyi random graph of average degree 6. All
algorithms are linearly convergent, although their conver-
In all
gence rates scale on several orders of magnitude.
experiments, the centralized optimal algorithm DAGD has
the best convergence rate, while MSDA has the best con-
vergence rate among decentralized methods. In all our ex-
periments, MSDA outperforms SSDA, indicating that per-
forming several communication rounds per gradient itera-
tion never degrades the efﬁciency of the algorithm, while
signiﬁcantly improving it when τ (cid:29) 1.

5.3. Logistic Classiﬁcation

The logistic classiﬁcation problem consists in solving the
optimization problem

min
θ∈Rd

1
m

m
(cid:88)

i=1

(cid:16)

ln

1 + e−yi·X (cid:62)

i θ(cid:17)

+ c(cid:107)θ(cid:107)2
2,

(22)

050010001500200010−10100time (t)max. approximation error (et)  D−ADMMEXTRADIGingDAGDSSDAMSDA05010015020010−10100time (t)max. approximation error (et)  020004000600080001000010−1010−5100time (t)max. approximation error (et)  0200400600800100010−1010−5100time (t)max. approximation error (et)  Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

(a) high communication time: τ = 10

(a) high communication time: τ = 10

(b) low communication time: τ = 0.1

(b) low communication time: τ = 0.1

Figure 3. Maximum approximation error for logistic classiﬁcation
on an Erd¨os-R´enyi random network of average degree 6 (n =
100).

Figure 4. Maximum approximation error for logistic classiﬁcation
on a 10 × 10 grid graph (n = 100).

where X ∈ Rd×m is a matrix containing m data points,
and y ∈ {−1, 1}m is a vector containing the m class as-
signments. The task is thus to classify a dataset by learning
a linear classiﬁer mapping data points Xi to their associated
class yi ∈ {−1, 1}. For our experiments, we ﬁxed c = 0.1,
d = 10, and sampled m = 10, 000 data points, 5, 000 for
the ﬁrst class and 5, 000 for the second. Each data point
Xi ∼ N (yi1, 1) is a Gaussian random variable of mean
yi1 and variance 1, where yi = 21{i ≤ 5, 000} − 1 is
the true class of Xi. These data points are then distributed
randomly and evenly to the n = 100 nodes of the network.

Figure 3 and Figure 4 show the performance of the com-
pared algorithms for logistic classiﬁcation on two net-
works: a 10 × 10 grid graph and an Erd¨os-R´enyi random
graph of average degree 6. As for least-squares regression,
all algorithms are linearly convergent, and their conver-
gence rates scale on several orders of magnitude. In this
case, the centralized optimal algorithm DAGD is outper-
formed by MSDA, although the two convergence rates are
relatively similar. Again, when the communication time is
smaller than the computation time (τ (cid:29) 1), performing
several communication rounds per gradient iteration will

improve the efﬁciency of the algorithm and MSDA sub-
stantially outperforms SSDA. Note that, in Figure 4(a), D-
ADMM requires 383 iterations to reach the same error ob-
tained after only 10 iterations of SSDA, demonstrating a
substantial improvement over state-of-the-art methods.

6. Conclusion

In this paper, we derived optimal convergence rates for
strongly convex and smooth distributed optimization in two
settings: centralized and decentralized communications in
a network. For the decentralized setting, we introduced
the multi-step dual accelerated (MSDA) algorithm with a
provable optimal linear convergence rate, and showed its
high efﬁciency compared to other state-of-the-art methods,
including distributed ADMM and EXTRA. The simplic-
ity of the approach makes the algorithm extremely ﬂexible,
and allows for future extensions, including time-varying
networks and an analysis for non-strongly-convex func-
tions. Finally, extending our complexity lower bounds to
time delays, variable computational speeds of local sys-
tems, or machine failures would be a notable addition to
this work.

0200040006000800010−10100time (t)max. approximation error (et)  D−ADMMEXTRADIGingDAGDSSDAMSDA020040060080010−10100time (t)max. approximation error (et)  0200040006000800010−10100time (t)max. approximation error (et)  020040060080010−10100time (t)max. approximation error (et)  Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

References

Alon, N. and Milman, V. D. λ1, isoperimetric inequalities
for graphs, and superconcentrators. Journal of Combi-
natorial Theory, series B, 38:73–88, 1985.

Arioli, M. and Scott, J. Chebyshev acceleration of itera-
tive reﬁnement. Numerical Algorithms, 66(3):591–608,
2014.

Arjevani, Yossi and Shamir, Ohad. Communication com-
plexity of distributed convex learning and optimization.
In Advances in Neural Information Processing Systems
28, pp. 1756–1764, 2015.

Arjevani, Yossi and Shamir, Ohad. On the iteration com-
plexity of oblivious ﬁrst-order optimization algorithms.
In 33nd International Conference on Machine Learning,
pp. 908–916, 2016a.

Arjevani, Yossi and Shamir, Ohad. Dimension-free itera-
tion complexity of ﬁnite sum optimization problems. In
Advances in Neural Information Processing Systems 29,
pp. 3540–3548, 2016b.

Auzinger, W. Iterative Solution of Large Linear Systems.

Lecture notes, TU Wien, 2011.

Bertsekas, Dimitri P. and Tsitsiklis, John N. Parallel and
distributed computation : numerical methods. Prentice-
Hall International, 1989.

Boyd, Stephen, Ghosh, Arpita, Prabhakar, Balaji, and
Shah, Devavrat.
Randomized gossip algorithms.
IEEE/ACM Transactions on Networking (TON), 14(SI):
2508–2530, 2006.

Boyd, Stephen, Diaconis, Persi, Parrilo, Pablo, and Xiao,
Lin. Fastest mixing markov chain on graphs with sym-
metries. SIAM Journal on Optimization, 20(2):792–819,
2009.

Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
Eckstein, Jonathan. Distributed optimization and statisti-
cal learning via the alternating direction method of mul-
tipliers. Foundations and Trends in Machine Learning,
3(1):1–122, 2011.

Bubeck, S´ebastien. Convex optimization: Algorithms and
complexity. Foundations and Trends in Machine Learn-
ing, 8(3-4):231–357, 2015.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.
SAGA: A fast incremental gradient method with support
In Ad-
for non-strongly convex composite objectives.
vances in Neural Information Processing Systems 27, pp.
1646–1654, 2014.

Duchi, John C, Agarwal, Alekh, and Wainwright, Mar-
tin J. Dual averaging for distributed optimization: Con-
vergence analysis and network scaling. IEEE Transac-
tions on Automatic control, 57(3):592–606, 2012.

Jakoveti´c, Duˇsan, Xavier, Joao, and Moura, Jos´e MF. Fast
distributed gradient methods. IEEE Transactions on Au-
tomatic Control, 59(5):1131–1146, 2014.

Jakoveti´c, Duˇsan, Moura, Jos´e MF, and Xavier, Joao. Lin-
ear convergence rate of a class of distributed augmented
lagrangian algorithms. IEEE Transactions on Automatic
Control, 60(4):922–936, 2015.

Johnson, Rie and Zhang, Tong. Accelerating stochastic
gradient descent using predictive variance reduction. In
Advances in Neural Information Processing Systems 26,
pp. 315–323, 2013.

Mokhtari, A., Shi, W., Ling, Q., and Ribeiro, A. A de-
centralized second-order method with exact linear con-
vergence rate for consensus optimization. IEEE Trans-
actions on Signal and Information Processing over Net-
works, 2(4):507–522, 2016.

Mokhtari, Aryan and Ribeiro, Alejandro. DSA: Decen-
tralized double stochastic averaging gradient algorithm.
Journal of Machine Learning Research, 17(1):2165–
2199, 2016.

Nedic, Angelia and Ozdaglar, Asuman. Distributed subgra-
dient methods for multi-agent optimization. IEEE Trans-
actions on Automatic Control, 54(1):48–61, 2009.

Nedich, A., Olshevsky, A., and Shi, W. Achieving geomet-
ric convergence for distributed optimization over time-
varying graphs. ArXiv e-prints, 2016.

Nesterov, Yurii. Introductory lectures on convex optimiza-
tion : a basic course. Kluwer Academic Publishers,
2004.

Shamir, Ohad. Fundamental limits of online and distributed
algorithms for statistical learning and estimation. In Ad-
vances in Neural Information Processing Systems 27, pp.
163–171, 2014.

Shamir, Ohad and Srebro, Nathan. Distributed stochas-
tic optimization and learning. In 52nd Annual Allerton
Conference on Communication, Control, and Computing
(Allerton), pp. 850–857. IEEE, 2014.

Shi, Wei, Ling, Qing, Yuan, Kun, Wu, Gang, and Yin,
Wotao. On the linear convergence of the ADMM in de-
centralized consensus optimization. IEEE Transactions
on Signal Processing, 62(7):1750–1761, 2014.

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

Shi, Wei, Ling, Qing, Wu, Gang, and Yin, Wotao. EXTRA:
An exact ﬁrst-order algorithm for decentralized consen-
sus optimization. SIAM Journal on Optimization, 25(2):
944–966, 2015.

Tutunov, R., Ammar, H. B., and Jadbabaie, A. A distributed
newton method for large scale consensus optimization.
ArXiv e-prints, 2016.

Wei, Ermin and Ozdaglar, Asuman. Distributed alternating
direction method of multipliers. In 51st Annual Confer-
ence on Decision and Control (CDC), pp. 5445–5450.
IEEE, 2012.

Xiao, Lin and Boyd, Stephen. Fast linear iterations for dis-
tributed averaging. Systems & Control Letters, 53(1):
65–78, 2004.

