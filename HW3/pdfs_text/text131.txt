Supplementary Material to Robust Structured Estimation with Single-Index
Models

Sheng Chen 1 Arindam Banerjee 1

Abstract

we are unable to distinguish between them, as both can be
solution to (S.1) for any samples.

In this supplementary material, we present the
deferred proofs of the results in the main paper.

1. Proof of Claim 1

Statement of Claim 1: Suppose that each element xi of x
is sampled i.i.d. from Rademacher distribution, i.e., P(xi =
1) = P(xi = −1) = 0.5. Under model (3) with noise
ϵ = 0, there exists a (cid:22)(cid:18) ∈ Sp−1 together with a monotone
(cid:22)f , such that supp( (cid:22)(cid:18)) = supp((cid:18)∗) and yi = (cid:22)f (⟨ (cid:22)(cid:18), xi⟩)
for data {(xi, yi)}n
i=1 with arbitrarily large sample size n,
while ∥ (cid:22)(cid:18) − (cid:18)∗∥2 > δ for some constant δ.

In the noiseless setting with unknown f ∗, provid-
Proof:
ed that S , supp((cid:18)∗) is given and |S| = s, the estimation
of (cid:18)∗ is simpliﬁed as

s.t. sign

⟨(cid:18)S, xiS − xj S ⟩

= sign(yi − yj),

(S.1)

(

Find (cid:18)S ∈ Ss−1

)

∀ 1 ≤ i < j ≤ n ,

any of whose solution (cid:18) can be true (cid:18)∗ on the premise
that no other information is available, since there always
exists a monotone f satisfying f (⟨(cid:18), xi⟩) = yi. Given
the distribution of x, xiS − xj S only has 3s possibilities
even if n → +∞. We denote the feasible set of (S.1) by
C, which is basically an intersection of Ss−1 and at most
min{n(n − 1), 3p} halfspaces (or hyperplanes if yi = yj).
Depending on the 3 different values of each sign(yi − yj),
this feasible set C has at most 3min{n(n−1);3p} possibili-
ties, which is ﬁnite, and the union of them should be Ss−1.
When s ≥ 2 and the constant δ is small enough, we can
always ﬁnd a C, in which there exist two different points
away by δ. Specify them as (cid:18)∗S and (cid:22)(cid:18)S respectively, and

1Department of Computer Science & Engineering, Univer-
sity of Minnesota-Twin Cities, Minnesota, USA. Correspon-
dence to: Sheng Chen <shengc@cs.umn.edu>, Arindam Baner-
jee <banerjee@cs.umn.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright
2017 by the author(s).

2. Proof of Lemma 1

Statement of Lemma 1: Suppose the distribution of y in
model (1) depends on x through ⟨(cid:18)∗, x⟩ and we deﬁne ac-
cordingly

bi (z1, . . . , zm; (cid:18)∗) =
(S.2)
E [qi (y1, . . . , ym) |⟨(cid:18)∗, x1⟩ = z1, . . . , ⟨(cid:18)∗, xm⟩ = zm] ,

With x being standard Gaussian N (0, I), u deﬁned in (4)
satisﬁes

E [u ((x1, y1), . . . , (xm, ym))] = β(cid:18)∗ ,

(S.3)

∑

m
i=1

E[bi (g1, . . . , gm; (cid:18)∗) · gi], and

where β =
g1, . . . , gm are i.i.d. standard Gaussian.
Proof: Let (cid:18)⊥ be any vector orthogonal to (cid:18)∗. For
convenience, we use the shorthand notation u for
u ((x1, y1), . . . , (xm, ym)). Then we have

]

⟨Eu, (cid:18)⊥⟩ = E

qi (y1, . . . , ym) · ⟨xi, (cid:18)⊥⟩

[

m∑

i=1

E [qi (y1, . . . , ym) · ⟨xi, (cid:18)⊥⟩]

E [⟨xi, (cid:18)⊥⟩ · E [qi (y1, . . . , ym) |x1, . . . , xm]] ((cid:3))

As xi follows N (0, I), ⟨xi, (cid:18)∗⟩ and ⟨xi, (cid:18)⊥⟩ are two zero-
mean independent Gaussian random variables. Since the
distribution of yi depends on x only via ⟨(cid:18)∗, xi⟩, we can
split the expectation and obtain

((cid:3)) =

E [⟨xi, (cid:18)⊥⟩ · bi (⟨(cid:18)∗, x1⟩, . . . , ⟨(cid:18)∗, xm⟩; (cid:18)∗)]

=

=

m∑

i=1
m∑

i=1

m∑

i=1
m∑

=

i=1
= 0 .

E [⟨xi, (cid:18)⊥⟩] · E [bi (⟨(cid:18)∗, x1⟩, . . . , ⟨(cid:18)∗, xm⟩; (cid:18)∗)]

Supplementary Material to Robust Structured Estimation with Single-Index Models

Hence u has to point towards either (cid:18)∗ or −(cid:18)∗, and note
that

(Vershynin, 2012), the sub-Gaussian norm of ui1;:::;im sat-
isﬁes

⟨Eu, (cid:18)∗⟩ =

E [qi (y1, . . . , ym) · ⟨xi, (cid:18)∗⟩]

∥ui1;:::;im

∥
 2

= sup

v∈Sp−1

qj (y11 , . . . , yim) · ⟨xj, v⟩

m∑

i=1
m∑

i=1
m∑

i=1

=

=

E [bi (⟨(cid:18)∗, x1⟩, . . . , ⟨(cid:18)∗, xm⟩; (cid:18)∗) · ⟨xi, (cid:18)∗⟩]

E [bi (g1, . . . , gm; (cid:18)∗) · gi] = β

We complete the proof by recalling that ∥(cid:18)∗∥2 = 1, thus
Eu = β(cid:18)∗.

thus we know ∥⟨ui1;:::;im, v − w⟩∥ 2
By Lemma 2, we have

≤ κm · ∥v − w∥2.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

 2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

m∑

j=1

m∑

j=1

≤ sup
v∈Sp−1

|⟨xj, v⟩|

≤ m ·

sup
v∈Sp−1

∥|⟨xj, v⟩|∥

≤ κm ,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

 2

 2

^u
β

P (|Zv − Zw| > δ) = P

v − w,

− (cid:18)∗

)

⟩(cid:12)
(cid:12)
(cid:12)
(cid:12) > δ

((cid:12)
⟨
(cid:12)
(cid:12)
(cid:12)

∑

((cid:12)
(cid:12)
(cid:12)
(cid:12)

= P

(n − m)!
n!

1≤i1;:::;im≤n
i1̸=:::̸=im
)
(cid:12)
(cid:12)
(cid:12)
(cid:12) > δ

− ⟨v − w, (cid:18)∗⟩

1
β

· ⟨ui1;:::;im , v − w⟩

(

(

≤ 2 exp

−C

≤ 2 exp

−C ′ ·

)

⌊

⌋

·

n
m

β2δ2
m2κ2 · ∥v − w∥2
2
nβ2δ2
m3κ2 · ∥v − w∥2
2

)

,

where we set C ′ = C/2. Therefore we can conclude
that {Zv} has sub-Gaussian incremental w.r.t.
the metric
s(v, w) , κm 3
2 · ∥v − w∥2/β
n. Now applying Lemma
3 to {Zv}, we obtain
(

√

3. Proof of Theorem 1

We ﬁrst provide a lemma that is useful for bounding the
Gaussian width of unions of sets, which originates in
Maurer et al. (2014).

Lemma A (Lemma 2 in Maurer et al. (2014)) Let M >
4, A1, · · · , AM ⊂ Rp, and A = ∪mAm. The Gaussian
width of A satisﬁes

w(A) ≤ max
1≤m≤M

w(Am) + 2 sup
z∈A

∥z∥2

√

log M (S.4)

Statement of Theorem 1: Suppose that the optimization
(9) can be solved to global minimum. Then the following
error bound holds for the minimizer ^(cid:18) with probability at
least 1 − C ′′ exp

)
−w2 (AK((cid:18)∗))

(

,

(cid:13)
(cid:13)
(cid:13) ^(cid:18) − (cid:18)∗

(cid:13)
(cid:13)
(cid:13)
2

≤ Cκm 3
β

2

· w(AK((cid:18)∗)) + C ′
√
n

,

(S.5)

P

sup
v;w∈AK∪{0}

|Zv − Zw| ≥ C1

γ2 (AK ∪ {0}, s)

(

))

where κ is the sub-Gaussian norm of a standard Gaussian
random variable, and C, C ′, C ′′ are all absolute constan-
t. Proof: We use the shorthand notation AK for the set
AK((cid:18)∗). As ^(cid:18) attains the global minimum of (9), we have
⟨

⟩

⟨ ^(cid:18) − (cid:18)∗, ^u⟩ ≥ 0 ⇐⇒
⟨

^(cid:18) − (cid:18)∗,

− (cid:18)∗ + (cid:18)∗

≥ 0

=⇒ ⟨ ^(cid:18), (cid:18)∗⟩ ≥ 1 −

^(cid:18) − (cid:18)∗,

− (cid:18)∗

^u
β

^u
β

⟩

⟨

≥ 1 − ∥ ^(cid:18) − (cid:18)∗∥2 ·

sup
v∈AK∪{0}

⟩

v,

^u
β

− (cid:18)∗

In order to bound the supremum above, we use the result
from generic chaining. We deﬁne the stochastic process
{Zv = ⟨v, ^u/β − (cid:18)∗⟩}v∈AK∪{0}. First, we need to check
the process has sub-Gaussian incremental. For simplici-
ty, we denote u ((xi1, yi1 ), . . . , (xim , yim)) by ui1;:::;im.
By the deﬁnitions and properties of sub-Gaussian norm

=⇒ P

+ δ · diam (AK ∪ {0}, s)
(

sup
v∈AK∪{0}
))

+ 2δ

2

√

|Zv| ≥ C1κm 3
β
n
)
(

≤ C2 exp

−δ2

(

≤ C2 exp

)

(
−δ2

·

γ2 (AK ∪ {0}, ∥ · ∥2)

Using Lemma 4 γ2 (AK ∪ {0}, ∥ · ∥2) ≤ C0 ·
w (AK ∪ {0}) and taking δ = w (AK ∪ {0}), we
get

⟩

⟨

v,

^u
β

− (cid:18)∗

≤ sup

|Zv|

v∈AK∪{0}

· w (AK ∪ {0}) ≤ C3κm 3

2

sup
v∈AK∪{0}

≤ C3κm 3
√
n
β

2

· w (AK) + C4
√
n

β
)
(
−w2 (AK)

with probability at least 1 − C2 exp
. The last
inequality follows from Lemma A. Now we turn to the

Supplementary Material to Robust Structured Estimation with Single-Index Models

quantity ∥ ^(cid:18) − (cid:18)∗∥2,

∥ ^(cid:18) − (cid:18)∗∥2
2

(

≤ 2 − 2⟨ ^(cid:18), (cid:18)∗⟩
1 − ∥ ^(cid:18) − (cid:18)∗∥2 · C3κm 3

2

≤ 2 − 2

)

· w (AK) + C4
√
n

≤ ∥ ^(cid:18) − (cid:18)∗∥2 · 2C3κm 3

2

β

β
· w (AK) + C4
√
n

.

We ﬁnish the proof by letting C = 2C3, C ′ = C4 and
C ′′ = C2.

4. Proof of Theorem 2

Statement of Theorem 2: Deﬁne the following set for any
ρ > 1,

A(cid:26) ((cid:18)∗) = cone

{

(cid:12)
(cid:12)
(cid:12) ∥v + (cid:18)∗∥ ≤ ∥(cid:18)∗∥ +

v

} ∩

Sp−1

∥v∥
ρ

(S.6)
√
If we set λ = ρ ∥^u − β(cid:18)∗∥∗ = O(ρm3=2w(B∥·∥)/
n)
and it satisﬁes λ < ∥^u∥∗, then with probability at least
1 − C ′ exp

, ^(cid:18) in (10) satisﬁes

(
−w2

B∥·∥

))

(

(cid:13)
(cid:13)
(cid:13) ^(cid:18) − (cid:18)∗

(cid:13)
(cid:13)
(cid:13)
2

≤ C(1 + ρ)κm 3
β

2

·

(cid:9) (A(cid:26)((cid:18)∗)) · w

√

n

)

(
B∥·∥

,

where (cid:9) (A(cid:26)((cid:18)∗)) = supv∈A(cid:26)((cid:18)∗)
{v | ∥v∥ ≤ 1} is the unit ball of norm ∥ · ∥.
Proof: Based on the optimality of ^(cid:18), we have

(S.7)
∥v∥ and B∥·∥ =

−⟨^u, ^(cid:18)⟩ + λ∥ ^(cid:18)∥ ≤ −⟨^u, (cid:18)∗⟩ + λ∥(cid:18)∗∥ =⇒
⟨β(cid:18)∗ − ^u − β(cid:18)∗, ^(cid:18)⟩ + λ∥ ^(cid:18)∥
≤ ⟨β(cid:18)∗ − ^u − β(cid:18)∗, (cid:18)∗⟩ + λ∥(cid:18)∗∥ =⇒
β(1 − ⟨(cid:18)∗, ^(cid:18)⟩) ≤ ⟨^u − β(cid:18)∗, ^(cid:18) − (cid:18)∗⟩ + λ(∥(cid:18)∗∥ − ∥ ^(cid:18)∥)

Since ⟨(cid:18)∗, ^(cid:18)⟩ ≤ 1, we have

⟨^u − β(cid:18)∗, ^(cid:18) − (cid:18)∗⟩ + λ

(

)
∥(cid:18)∗∥ − ∥ ^(cid:18)∥

≥ 0 =⇒

∥ ^(cid:18)∥ ≤ ∥(cid:18)∗∥ +

· ⟨^u − β(cid:18)∗, ^(cid:18) − (cid:18)∗⟩

Therefore it follows that
1 − ⟨(cid:18)∗, ^(cid:18)⟩ ≤ ⟨ ^u
β
((cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ ∥ ^(cid:18) − (cid:18)∗∥2

− (cid:18)∗, ^(cid:18) − (cid:18)∗⟩ +
(cid:13)
(cid:13)
(cid:13)
(cid:13)

λ
β
∥ ^(cid:18) − (cid:18)∗∥
∥ ^(cid:18) − (cid:18)∗∥2

^u
β

∗

·

(

)
∥(cid:18)∗∥ − ∥ ^(cid:18)∥

)

+

·

λ
β

∥ ^(cid:18) − (cid:18)∗∥
∥ ^(cid:18) − (cid:18)∗∥2

≤ (1 + ρ)∥ ^(cid:18) − (cid:18)∗∥2 ·

− (cid:18)∗

·

sup
v∈A(cid:26)((cid:18)∗)

∥v∥

= (1 + ρ)∥ ^(cid:18) − (cid:18)∗∥2 ·

− (cid:18)∗

· (cid:9) (A(cid:26)((cid:18)∗))

− (cid:18)∗
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

^u
β

^u
β

(cid:13)
(cid:13)
(cid:13) ^u
(cid:12)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
∗
(cid:13)
(cid:13)
(cid:13)
(cid:13)
∗

(cid:13)
(cid:13)
(cid:13)

(S.8)

− (cid:18)∗
⟨

⟩

− (cid:18)∗

(cid:13)
(cid:13)
(cid:13)
∗

= supv∈B∥·∥

∗
− (cid:18)∗, v

. We ﬁrst rewrite it as

Now we try to bound
(cid:13)
(cid:13)
(cid:13) ^u
^u
. Construct the s-
(cid:12)
(cid:12)
tochastic process {Zv = ⟨v, ^u/β − (cid:18)∗⟩}v∈B∥·∥, and it is
not difﬁcult to verify that {Zv} has sub-Gaussian incre-
mental using the proof in Theorem 1. Now applying Lem-
ma 3 and 4, we have
⟨

⟩

sup
v∈B∥·∥

^u
β

− (cid:18)∗, v

sup
v;w∈B∥·∥

·

=

1
2
≤ C1κm 3
β

2

w

·

(

|Zv − Zw|
)
(
B∥·∥
√
n
(

))

,

(S.9)

with probability at least 1 − C ′ exp
fore we know that λ satisﬁes

−w2

B∥·∥

. There-

(

)

ρm3=2w(B∥·∥)
√
n

λ = O

If ^(cid:18) = 0 is the minimizer, the ﬁrst-order optimality should
hold, i.e.,

^u ∈ λ · ∂∥0∥ =⇒ ∥^u∥∗ ≤ λ

Hence if λ < ∥^u∥∗, 0 cannot be the minimizer, which
means that the minimum of (10) must be negative. So we
can assert that ∥ ^(cid:18)∥2 = 1, otherwise we can normalize ^(cid:18) to
get a smaller objective value. Combining (S.8) and (S.9),
we ﬁnally get

∥ ^(cid:18) − (cid:18)∗∥ =

2 − 2⟨ ^(cid:18), (cid:18)∗⟩
∥ ^(cid:18) − (cid:18)∗∥
≤ Cmκ(1 + ρ)
β

·

(cid:9) (A(cid:26)((cid:18)∗)) · w

B∥·∥

(

)

,

√

n

where the equality uses the fact that ∥ ^(cid:18)∥2 = 1.

1
λ
1
λ
1
ρ

≤ ∥(cid:18)∗∥ +

· ∥^u − β(cid:18)∗∥∗∥ ^(cid:18) − (cid:18)∗∥

5. Proof of Corollary 1

= ∥(cid:18)∗∥ +

∥ ^(cid:18) − (cid:18)∗∥ =⇒ ^(cid:18) − (cid:18)∗ ∈ A(cid:26)((cid:18)∗)

Statement of Corollary 1: Assume that {(xi, yi)}n
i=1 fol-
low 1-bit CS model in (2) and ^u is given as (14). For any

Supplementary Material to Robust Structured Estimation with Single-Index Models

s-sparse (cid:18)∗, with high probability, ^(cid:18) produced by both (15)
and (17) (i.e., ^(cid:18)ks and ^(cid:18)ps) satisfy

Proof: We rearrange the terms inside the summation of
(21) based on π↓,

(cid:13)
(cid:13)
(cid:13) ^(cid:18) − (cid:18)∗

(cid:13)
(cid:13)
(cid:13)
2

≤ O

(√

)

s log p
n

(S.10)

the k-support norm estimator,

the cone

Proof: For
AK((cid:18)∗) is given by
{

AK((cid:18)∗) = cone

^(cid:18) − (cid:18)∗

(cid:12)
(cid:12)
(cid:12) ∥ ^(cid:18)∥0 ≤ s, ∥ ^(cid:18)∥2 ≤ 1

} ∩

Sp−1

=⇒ AK((cid:18)∗) ⊆ S = {v | ∥v∥0 ≤ 2s} ∩ Sp−1

Using (19) from (Chen & Banerjee, 2015), we have

w(AK((cid:18)∗)) ≤ w(S) ≤ O

(√

)

s log p

.

By Theorem 1, the error of k-support norm estimator satis-
ﬁes

(cid:13)
(cid:13)
(cid:13) ^(cid:18)ks − (cid:18)∗

(cid:13)
(cid:13)
(cid:13)
2

≤ O

(√

)

s log p
n

For the passive algorithm, if we choose ρ = 2, the restricted
norm compatibility (cid:9) (A(cid:26)((cid:18)∗)) for L1 norm satisﬁes

(cid:9) (A(cid:26)((cid:18)∗)) ≤ 4

√
s

(S.11)

according to the results
in (Negahban et al., 2012;
Banerjee et al., 2014). Chen & Banerjee (2015) also show
that the Gaussian width of the L1-norm ball is bounded by

w(BL1 ) ≤ O

(√

)

log p

.

(S.12)

Now combining (S.11), (S.12) and Theorem 2, we can con-
clude that

(cid:13)
(cid:13)
(cid:13) ^(cid:18)ps − (cid:18)∗

(cid:13)
(cid:13)
(cid:13)
2

≤ O

(√

)

,

s log p
n

which completes the proof.

6. Proof of Proposition 1

Statement of Proposition 1: Given {(xi, yi)}n
be the permutation of {1, . . . , n} such that y(cid:25)
. . . > y(cid:25)

. Then we have

↓
1

↓
n

i=1, let π↓
>
> y(cid:25)

↓
2

^h =

2
n(n − 1)

n∑

i=1

(n + 1 − 2i) · x(cid:25)

↓
i

(S.13)

∑

1≤i;j≤n
i̸=j
∑

1≤i;j≤n
i̸=j

n∑

∑

j̸=(cid:25)

↓
i

^h =

1
n(n − 1)

=

2
n(n − 1)

=

2
n(n − 1)

=

2
n(n − 1)

i=1

n∑

i=1

sign(yi − yj) · (xi − xj)

sign(yi − yj) · xi

(

)

sign

− yj

y(cid:25)

↓
i

· x(cid:25)

↓
i

(n + 1 − 2i) · x(cid:25)

,

↓
i

where the last
(i − 1) yj larger than and (n − i) smaller than y(cid:25)
∑

inequality uses the fact

that

)

(

there are
, thus

↓
i

sign

j̸=(cid:25)

↓
i

− yj

y(cid:25)

↓
i

= (n−i)−(i−1) = n+1−2i.

7. Proof of Proposition 2

Statement of Proposition 2: For s-fused-sparse (cid:18)∗, the
Gaussian width of set AK((cid:18)∗) with K = {(cid:18) | |F ((cid:18))| ≤
s, ∥(cid:18)∥2 = 1} satisﬁes

w(AK((cid:18)∗)) ≤ O(

s log p)

(S.14)

√

Proof: Deﬁne the following sets

{

Ti;j =

αu ∈ Rp

(cid:12)
(cid:12)
(cid:12) u1 = . . . = ui−1 = uj+1 = . . . = up = 0,

ui = . . . = uj =

1√

j − i + 1

√

}

, |α| ≤

2s + 1

(S.15)

(S.16)

T =

Ti;j

∪

i≤j

]

√

For each Ti;j, its Gaussian width can be calculated as

[

w(Ti;j) = E
√

=

=

⟨v, g⟩

sup
v∈Ti;j
2s + 1 · E |g| = O(

2s + 1 · E [|⟨u, g⟩|]
√

2s + 1) ,

where u is deﬁned in (S.15) and g is a standard Gaussian
random variable. We apply Lemma A to T , and obtain

√

((

)

)

p
2

+ p

w(T ) ≤ max
i≤j
√

w(Ti;j) + 2 sup
z∈T
√

∥z∥2

log
√

≤ O(

2s + 1) + O(

2s + 1 ·

log p)

√

= O(

s log p)

Supplementary Material to Robust Structured Estimation with Single-Index Models

(cid:12)
(cid:12)
(cid:12) v = ^(cid:18) − (cid:18)∗, ^(cid:18) ∈ K

Next we show that AK((cid:18)∗) ⊆ conv(T ). Since K =
|F ((cid:18))| ≤ s, ∥(cid:18)∥2 = 1} and AK((cid:18)∗) =
{(cid:18) |
} ∩
{
Sp−1 by deﬁnition, we
cone
v
have |F(v)| ≤ 2s for any v ∈ AK((cid:18)∗). Suppose |F(v)| =
t ≤ 2s and F(v) = {i1, i2, . . . , it}. For simplicity, we al-
so let i0 = 0 and it+1 = p. Then any v ∈ AK((cid:18)∗) can be
written as a convex combination of t + 2 points in T . To
see this, we rewrite v as

v =

vir+1:ir+1 =

t∑

r=0

√

t∑

r=0
(

∥vir+1:ir+1
√
t + 1

∥2

·

t + 1vir+1:ir+1
∥vir+1:ir+1
)

∥2

+

1 −

t∑

r=0

∥vir+1:ir+1
√
t + 1

∥2

· 0 ,

(S.17)

where vir+1:ir+1 is obtained from v by keeping the entries
from index ir + 1 to ir+1 while zeroing out the rest. Let
uir+1:ir+1 =

t+1vir +1:ir+1
∥2
∥vir +1:ir+1

√

, and we have
√

√

∥uir+1:ir+1
∥2 =
=⇒ uir+1:ir+1

t + 1 ≤
∈ Tir+1:ir+1

2s + 1
⊆ T .

It follows from ∥v∥2 = 1 that

√

t∑

r=0

∥vir+1:ir+1
√
t + 1

∥2

≤

(t + 1)

∥vir+1:ir+1

∥2
2

= 1

∑
t
r=0
√
t + 1

=⇒ 1 −

t∑

r=0

∥vir+1:ir+1
√
t + 1

∥2

≥ 0

Hence (S.17) is indeed a convex combination of t+2 points
in T , which implies AK((cid:18)∗) ⊆ conv(T ). Finally, by the
properties of Gaussian width, we conclude that
w(AK((cid:18)∗)) ≤ w(conv(T )) = w(T ) ≤ O(

s log p)

√

in which C is an absolute constant.

Proof: Our proof is based on Hoeffding’s decomposition
for U -statistics. For simplicity, we use U as shorthand for
Un;m(h). Given a permutation π of {1, . . . , n}, deﬁne

⌋−1∑
⌊ n

m

(
z(cid:25)mk+1 , . . . , z(cid:25)m(k+1)

)

h

,

W(cid:25) =

1⌊

⌋

n
m

∑

k=0
The U -statistic can be rewritten as U = 1
(cid:25) W(cid:25), and the
n!
summation is over all possible permutations of {1, . . . , n}.
As no copy of z appears more than twice in a single W(cid:25),
W(cid:25) is an average of ⌊ n
⌋ independent sub-Gaussian ran-
m
dom variables. Hence the ψ2-norm of its centered version
satisﬁes ∥W(cid:25) − EW(cid:25)∥ 2
⌋. Using Chernoff
≤ cκ/
technique, we have for any t > 0,
P (U − EU > δ) ≤ e−t(cid:14) · E [exp(t(U − EU ))]

⌊ n
m

√

[

[

(

t
n!

∑

(cid:25)

∑

1
n!

=e−t(cid:14) · E

exp

(W(cid:25) − EU )

≤e−t(cid:14) · E

exp (t(W(cid:25) − EU ))

(cid:25)
=e−t(cid:14) · E [exp (t(W(cid:25) − EW(cid:25)))]
−tδ + ct2 · κ2

≤ exp

(

)

⌊

⌋

,

n
m

)]

]

(S.20)

where the second inequality is obtained via Jensen’s in-
equality and the last one follows the moment generating
function bound for centered sub-Gaussian random variable.
n
δ/2cκ2 to minimize right-hand side of
Choosing t =
m
(S.20), we obtain

⌋

⌊

P (U − EU > δ) ≤ exp

−C

(

⌊

⌋

n
m

· δ2
κ2

)

,

where C = 1/2c. To complete the proof, we just need to
repeat the argument above for P (U − EU < −δ).

8. Proof of Lemma 2

Statement of Lemma 2: Deﬁne the U -statistic

References

Un;m(h) =

(n − m)!
n!

∑

1≤i1;:::;im≤n
i1̸=i2̸=:::̸=im

h (zi1 , . . . , zim)

(S.18)
with order m and kernel h : Rd×m 7→ R based on n in-
dependent copies of random vector z ∈ Rd, denoted by
z1, · · · , zn. If h(·, . . . , ·) is sub-Gaussian with ∥h∥ 2
≤ κ,
then the following inequality holds for Un;m(h) with any
δ > 0,

P (|Un;m(h) − EUn;m(h)| > δ) ≤ 2 exp

−C

Banerjee, A., Chen, S., Fazayeli, F., and Sivakumar, V. Es-
timation with norm regularization. In Advances in Neu-
ral Information Processing Systems (NIPS), 2014.

Chen, S. and Banerjee, A. Structured estimation with atom-
ic norms: General bounds and applications. In Proceed-
ings of the 28th International Conference on Neural In-
formation Processing Systems, 2015.

(

)

,

⌊

⌋

n
m

· δ2
κ2
(S.19)

Maurer, A., Pontil, M., and Romera-Paredes, B. An In-
equality with Applications to Structured Sparsity and
Multitask Dictionary Learning. In Conference on Learn-
ing Theory (COLT), 2014.

Supplementary Material to Robust Structured Estimation with Single-Index Models

Negahban, S., Ravikumar, P., Wainwright, M. J., and Yu,
B. A uniﬁed framework for the analysis of regularized
M -estimators. Statistical Science, 27(4):538–557, 2012.

Vershynin, R. Introduction to the non-asymptotic analysis
of random matrices. In Eldar, Y. and Kutyniok, G. (ed-
s.), Compressed Sensing, chapter 5, pp. 210–268. Cam-
bridge University Press, 2012.

