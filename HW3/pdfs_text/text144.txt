Nearly Optimal Robust Matrix Completion

Yeshwanth Cherapanamjeri 1 Kartik Gupta 1 Prateek Jain 1

Abstract
In this paper, we consider the problem of Ro-
bust Matrix Completion (RMC) where the goal
is to recover a low-rank matrix by observing
a small number of its entries out of which a
few can be arbitrarily corrupted. We propose a
simple projected gradient descent-based method
to estimate the low-rank matrix that alternately
performs a projected gradient descent step and
cleans up a few of the corrupted entries us-
ing hard-thresholding. Our algorithm solves
RMC using nearly optimal number of observa-
tions while tolerating a nearly optimal number
of corruptions. Our result also implies signif-
icant improvement over the existing time com-
plexity bounds for the low-rank matrix comple-
tion problem. Finally, an application of our re-
sult to the robust PCA problem (low-rank+sparse
matrix separation) leads to nearly linear time (in
matrix dimensions) algorithm for the same; ex-
isting state-of-the-art methods require quadratic
time. Our empirical results corroborate our the-
oretical results and show that even for moderate
sized problems, our method for robust PCA is an
order of magnitude faster than the existing meth-
ods.

1. Introduction

In this paper, we study the Robust Matrix Completion
(RMC) problem where the goal is to recover an underlying
low-rank matrix by observing a small number of sparsely
corrupted entries. Formally,

RMC:

Find rank-r matrix L∗ ∈ Rm×n

using Ω and PΩ(L∗) + S∗,

(1)

where Ω ⊆ [m] × [n] is the set of observed entries
(throughout the paper we assume that m ≤ n), S∗ de-
notes the sparse corruptions of the observed entries, i.e.,

1Microsoft Research India. Correspondence to: Prateek Jain

<prajain@microsoft.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Supp(S∗) ⊂ Ω and sampling operator PΩ : Rm×n →
Rm×n is deﬁned as:

(PΩ(A))ij =

(cid:40)

Aij,
0,

if (i, j) ∈ Ω
otherwise.

(2)

RMC is an important problem with several applications.
It is used to model recommendation systems with outliers
and to perform PCA under gross outliers as well as era-
sures (Jalali et al., 2011).
In addition, it is a strict gen-
eralization of the following two fundamental problems in
machine learning:

Matrix Completion (MC): Matrix completion is the task
of completing a low rank matrix given a subset of en-
tries of the matrix. It is widely used in recommender sys-
tems and also ﬁnds applications in system identiﬁcation
and global positioning. Note that it is a special case of
the RMC problem where the matrix S∗ = 0. State-of-the-
art result for MC uses nuclear norm minimization and re-
quires |Ω| ≥ µ2nr2 log2 n under standard µ-incoherence
assumption (see Section 3). But it requires O(m2n) time
in general. The best sample complexity result for a non-
convex iterative method (with at most logarithmic depen-
dence on the condition number of L∗) achieve exact recov-
ery when |Ω| ≥ µ6nr5 log2 n and needs O(|Ω|r) computa-
tional steps.

Robust PCA (RPCA): RPCA aims to decompose a
sparsely corrupted low rank matrix into its low rank and
sparse components. It corresponds to another special case
of RMC where the whole matrix is observed. State-of-
the-art results for RPCA shows exact recovery of a rank-r,
µ-incoherent L∗ (see Assumption 1, Section 3) if at most
ρ = O
fraction of the entries in each row/column of
S∗ are corrupted (Hsu et al., 2011; Netrapalli et al., 2014).
Moreover, St-NcRPCA algorithm (Netrapalli et al., 2014)
solves the problem in time O(mnr2).

(cid:16) 1
µ2r

(cid:17)

Therefore, an efﬁcient solution to the RMC problem im-
plies efﬁcient solutions to both the MC and RPCA prob-
lems.
In this work, we attempt to answer the following
open question (assuming m ≤ n):

Can RMC be solved exactly by using |Ω| = O(rn log n)
observations out of which O( 1
µ2r ) fraction of the observed

Nearly Optimal Robust Matrix Completion

entries in each row/column are corrupted?

Note that both |Ω| (for uniformly random Ω) and ρ val-
ues mentioned in the question above denote the informa-
tion theoretic limits. Hence, the goal is to solve RMC for
nearly-optimal number of samples and nearly-optimal frac-
tion of corruptions.

The existing state-of-the-art results for RMC with opti-
mal ρ = 1
µ2r fraction of corrupted entries, either require
at least a constant fraction of the entries of L∗ to be ob-
served (Chen et al., 2011; Cand`es et al., 2011) or require
restrictive assumptions like the support of corruptions be-
ing uniformly random (Li, 2013). (Klopp et al., 2014) also
considers RMC problem but studies the noisy setting and
does not provide exact recovery bounds. Moreover, most
of the existing methods for RMC use convex relaxation for
both low-rank and sparse components, and in general ex-
hibit large time complexity (O(m2n)).

Under standard assumptions on L∗, S∗, Ω and for n =
O(m), we answer the above question in afﬁrmative albeit
with |Ω| which is O(r) (ignoring log factors) larger than
the optimal sample complexity (see Theorem 1). In partic-
ular, we propose a simple projected gradient (PGD) style
method for RMC that alternately cleans up corrupted en-
tries by hard-thresholding and performs a projected gradi-
ent descent update onto the space of low rank matrices; our
method’s computational complexity is also nearly optimal
(O(|Ω|r + (m + n)r2 + r3)). Our algorithm is based on
projected gradient descent for estimating L∗ and alternat-
ing projection on the set of sparse matrices for estimating
S∗. Note that projection is onto non-convex sets of low-
rank matrices (for L∗) and sparse matrices (for S∗), hence
standard convex analysis techniques cannot be used for our
algorithm.

Several recent results (Jain & Netrapalli, 2015; Netrapalli
et al., 2014; Jain et al., 2014; Hardt & Wootters, 2014; Blu-
mensath, 2011) show that under certain assumptions, pro-
jection onto non-convex sets indeed lead to provable algo-
rithms with fast convergence to the global optima. How-
ever, as explained in Section 3, RMC presents a unique
set of challenges as we have to perform error analysis with
the errors arising due to missing entries as well as sparse
corruptions, both of which interact among themselves as
well. In contrast, MC and RPCA only handle noise aris-
ing from one of the two sources. To overcome this chal-
lenge, we perform error analysis by segregating the effects
due to random sampling and sparse corruptions and lever-
age their unique structure to obtain our result (See proof
of Lemma 14). Another consequence of our careful er-
ror analysis is improved results for the MC as well as the
RPCA problem.

Our empirical results on synthetic data demonstrates effec-
tiveness of our method. We also apply our method to the
foreground background separation problem and ﬁnd that
our method is an order of magnitude faster than the state-
of-the-art method (St-NcRPCA) while achieving similar
accuracy.

In a concurrent and independent work, (Yi et al., 2016)
also studied the RMC problem and obtained similar results.
They study an alternating gradient descent style algorithm
while our algorithm is based on low-rank projected gradi-
ent descent. Our sample complexity, corruption tolerance
as well as time complexity differ along certain critical pa-
rameters: a) Sample complexity: Our sample complexity
bound is dependent only logarithmically on κ, the condi-
tion number of the matrix L∗ (see Table 1). On the other
hand, result of (Yi et al., 2016) depends quadratically on κ,
which can be signiﬁcantly large. Another difference is that
our sample complexity bound depends logarithmically on
the ﬁnal error (cid:15) (deﬁned as (cid:15) = (cid:107)L − L∗(cid:107)2); which for typ-
ical ﬁnite precision computation only introduces an extra
constant factor. b) Corruption tolerance: Our result allows
the fraction of corrupted entries to be information theoretic
optimal (up to a constant) O( 1
µ2r ), while the result of (Yi
et al., 2016) allows only O
min
frac-
tion of corrupted entries. c) Time Complexity: As a con-
sequence of the sample complexity bounds, running time
of the method by (Yi et al., 2016) depends quintically on κ
whereas our algorithm only has a polylogarithmic depen-
dence.

1
µ2κ2r

1
√
µ2r

rκ ,

(cid:17)(cid:17)

(cid:16)

(cid:16)

In summary, this paper’s main contributions are:
(a) RMC: We propose a nearly linear time method that
solves RMC with |Ω| = O(nr2 log2 n log2 (cid:107)M (cid:107)2/(cid:15)) ran-
dom entries and with optimal fraction of corruptions upto
constant factors (ρ = O( 1
µ2r )).
(b) Matrix Completion: Our result improves upon the
existing linear time algorithm’s sample complexity by an
O(r3) factor, and time complexity by O(r4) factor, al-
though with an extra O(log (cid:107)L∗(cid:107)/(cid:15)) factor in both time and
sample complexity.
(c) RPCA: We present a nearly linear time (O(nr3)) al-
gorithm for RPCA under optimal fraction of corruptions,
improving upon O(mnr2) time complexity of the existing
methods.

Notations: We assume that M = L∗ + (cid:101)S∗ and PΩ(M ) =
PΩ(L∗) + S∗, i.e., S∗ = PΩ( (cid:101)S∗). (cid:107)v(cid:107)p denotes (cid:96)p norm
of a vector v; (cid:107)v(cid:107) denotes (cid:96)2 norm of v. (cid:107)A(cid:107)2, (cid:107)A(cid:107)F ,
(cid:107)A(cid:107)∗ denotes the operator, Frobenius, and nuclear norm of
A, respectively; by default (cid:107)A(cid:107) = (cid:107)A(cid:107)2. Operator PΩ is
given by (2), operators Pk(A) and HT ζ(A) are deﬁned in
Section 2. σi(A) denotes i-th singular value of A and σ∗
i
denotes the i-th singular value of L∗.

Nearly Optimal Robust Matrix Completion

Paper Organization: We present our main algorithm in
Section 2 and our main results in Section 3. We also present
an overview of the proof in Section 3. Section 4 presents
our empirical result. Due to lack of space, we present most
of the proofs and useful lemmas in the appendix.

2. Algorithm

In this section we present our algorithm for solving the
RMC (Robust Matrix Completion) problem: given Ω and
PΩ(M ) where M = L∗ + (cid:101)S∗ ∈ Rm×n, rank(L∗) ≤ r,
(cid:107) (cid:101)S∗(cid:107)0 ≤ s and S∗ = PΩ( (cid:101)S∗), the goal is to recover L∗.
To this end, we focus on solving the following non-convex
optimization problem:

(L∗, S∗) = arg min
L,S

(cid:107)PΩ(M ) − PΩ(L) − S(cid:107)2
F

s.t., rank(L) ≤ r, PΩ(S) = S, (cid:107)S(cid:107)0 ≤ s.

(3)

For the above problem, we propose a simple iterative algo-
rithm that combines projected gradient descent (for L) with
alternating projections (for S). In particular, we maintain
iterates L(t) and S(t), where L(t) is the current low-rank
approximation of L∗ and S(t) is the current sparse approx-
imation of S∗. L(t+1) is computed using gradient descent
step for objective (3) and then projecting back onto the set
of rank k matrices. That is,

L(t+1) = Pk

L(t) +

PΩ(M − L(t) − S(t))

,

(4)

(cid:19)

(cid:18)

1
p

where Pk(A) denotes projection of A onto the set of rank-
k matrices and can be computed efﬁciently using SVD of
A, p = |Ω|
mn . S(t+1) is computed by projecting the residual
PΩ(M − L(t+1)) onto set of sparse matrices using a hard-
thresholding operator, i.e.,

S(t+1) = HT ζ(M − L(t+1)),

(5)

where HT ζ : Rm×n → Rm×n is the hard thresholding
operator deﬁned as: (HT ζ(A))ij = Aij if |Aij| ≥ ζ and
0 otherwise. Intuitively, a better estimate of the sparse cor-
ruptions for each iteration will reduce the noise of the pro-
jected gradient descent step and a better estimate of the low
rank matrix will enable better estimation of the sparse cor-
ruptions. Hence, under correct set of assumptions, the al-
gorithm should recover L∗, S∗ exactly.

Unfortunately, just the above two simple iterations cannot
handle problems where L∗ has poor condition number, as
the intermediate errors can be signiﬁcantly larger than the
smallest singular values of L∗, making recovery of the cor-
responding singular vectors challenging. To alleviate this
issue, we propose an algorithm that proceeds in stages. In

the q-th stage, we project L(t) onto set of rank-kq matrices.
Rank kq is monotonic w.r.t. q. Under standard assump-
tions, we show that we can increase kq in a manner such
that after each stage (cid:13)
(cid:13)∞ decreases by at least a
constant factor. Hence, the number of stages is only log-
arithmic in the condition number of L∗. See Algorithm 1
(PG-RMC ) for a pseudo-code of the algorithm.

(cid:13)L(t) − L∗(cid:13)

We require an upper bound of the ﬁrst singular value for our
algorithm to work. Speciﬁcally, we require σ = O (σ∗
1).
Alternatively, we can also obtain an estimate of σ∗
1 by using
the thresholding technique from (Yi et al., 2016) although
this requires an estimate of the number of corruptions in
each row and column. We also use a simpliﬁed version of
Algorithm 5 from (Hardt & Wootters, 2014) to form inde-
pendent sets of samples for each iteration which is required
for our theoretical analysis. Our algorithm has an “outer
loop” (see Line 6) which sets rank kq of iterates L(t) ap-
propriately (see Line 7). We then update L(t) and S(t) in
the “inner loop” using (4), (5). We set threshold for the
hard-thresholding operator using singular values of current
gradient descent update (see Line 12). Note that, we divide
Ω uniformly into Q · T sets, where Q is an upper bound on
the number of outer iterations and T is the number of inner
iterations. This division ensures independence across iter-
ates that is critical to application of standard concentration
bounds; such division is a standard technique in the ma-
trix completion related literature (Jain & Netrapalli, 2015;
Hardt & Wootters, 2014; Recht, 2011). Also, η is a tunable
parameter which should be less than one and is smaller for
“easier” problems.

Note that updating S(t) requires O(|Ω| · r + (m + n) · r)
computational steps. Computation of L(t+1) requires com-
puting SVD for projection Pr, which can be computed in
O(|Ω| · r + (m + n) · r2 + r3) time (ignoring log factors);
see (Jain et al., 2010) for more details. Hence, the compu-
tational complexity of each step of the algorithm is linear
in |Ω| · r (assuming |Ω| ≥ r · (m + n)). As we show in
the next section, the algorithm exhibits geometric conver-
gence rate under standard assumptions and hence the over-
all complexity is still nearly linear in |Ω| (assuming r is just
a constant).

Rank based Stagewise algorithm: We also provide a
rank-based stagewise algorithm (R-RMC) where the outer
loop increments kq by one at each stage, i.e., the rank is q
in the q-th stage. Our analysis extends for this algorithm as
well, however, its time and sample complexity trades off a
factor of O(log(σ1/(cid:15))) from the complexity of PG-RMC
with a factor of r (rank of L∗). We provide the detailed
algorithm in Appendix 5.3 due to lack of space (see Algo-
rithm 3).

Nearly Optimal Robust Matrix Completion

Algorithm 1 (cid:98)L = PG-RMC (Ω, PΩ(M ), (cid:15), r, µ, η, σ)
1: Input: Observed entries Ω, Matrix PΩ(M ) ∈ Rm×n,
convergence criterion (cid:15), target rank r, incoherence pa-
rameter µ, thresholding parameter η, estimate of ﬁrst
singular value σ
2: T ← 10 log 20µ2nrσ
, Q ← T
3: Partition Ω into Q · T + 1 subsets {Ω0} ∪ {Ωq,t : q ∈

(cid:15)

[Q], t ∈ [T ]} with p set to

Ω

QT +1 using algorithm 2

|Ω0| PΩ0(M − HT ζ(M ))

4: L(0) = 0, ζ (0) ← ησ
5: M (0) = mn
6: k0 ← 0, q ← 0
7: while σkq+1(M (0)) > (cid:15)
8:

2ηn do

q ← q + 1,
(cid:12)
(cid:12)
(cid:12)
(cid:12)

kq ←

{i : σi(M (0)) ≥

σkq−1+1(M (0))
2
for Iteration t = 0 to t = T do

(cid:12)
(cid:12)
(cid:12)
(cid:12)

}

S(t) = HT ζ(PΩq,t(M − L(t)))
M (t) = L(t) − mn
L(t+1) = Pkq (M (t))
ζ (t+1) ← η

σkq+1(M (t)) + (cid:0) 1

(cid:16)

2

|Ωq,t| PΩq,t(L(t) + S(t) − M )

(cid:1)t−2

σkq (M (t))

(cid:17)

end for
S(0) = S(T ), L(0) = L(T +1), M (0) = M (T ),
ζ (0) = ζ (T +1)

9:

10:
11:
12:

13:

14:

15:
16:

17: end while
18: Return: L(T )

Algorithm 2 {Ω1, . . . , ΩT } = SplitSamples(Ω, p, T )
1: Input: Random samples with probability T p Ω, Re-
quired Sampling Probability p, Number of Sets T
2: Output: T independent sets of entries {Ω1, . . . , ΩT }

sampled with sampling probability p

3: p(cid:48) ← 1 − (1 − p)T
4: Ω(cid:48) be sampled from Ω with each entry being included

independently with probability p(cid:48)/p

5: for r = 1 to r = T do
(T
r)pr(1−p)T −r
qr ←
6:
p(cid:48)
7: end for
8: Initialize Ωt ← {} for t ∈ {1, . . . , T }
9: for Sample s ∈ Ω(cid:48) do
10:
11:
12:
13: end for

Draw r ∈ {1, . . . , T } with probability qr
Draw a random subset S of size r from {1, . . . , T }
Add s to Ωi for i ∈ S

3. Analysis

We now present our analysis for both of our algorithms PG-
RMC (Algorithm 1) and R-RMC (Algorithm 3). In gen-
eral the problem of Robust PCA with Missing Entries (3)
is harder than the standard Matrix Completion problem and
hence is NP-hard (Hardt et al., 2014). Hence, we need to

(cid:13)e(cid:62)

(cid:13)e(cid:62)

m , (cid:13)

i U ∗(cid:13)

j V ∗(cid:13)
(cid:13)2

impose certain (by now standard) assumptions on L∗, (cid:101)S∗,
and Ω to ensure tractability of the problem:
Assumption 1. Rank and incoherence of L∗: L∗ ∈
Rm×n is a rank-r incoherent matrix, i.e., (cid:13)
(cid:13)2 ≤
≤ µ(cid:112) r
µ(cid:112) r
n , ∀i ∈ [m], ∀j ∈ [n], where
L∗ = U ∗Σ∗(V ∗)(cid:62) is the SVD of L∗.
Assumption 2. Sampling (Ω): Ω is obtained by sampling
each entry with probability p = |Ω|
mn .
Assumption 3. Sparsity of (cid:101)S∗, S∗: We assume that at
most ρ ≤ c
µ2r fraction of the elements in each row and
column of (cid:101)S∗ are non-zero for a small enough constant c.
Moreover, we assume that Ω is independent of (cid:101)S∗. Hence,
S∗ = PΩ( (cid:101)S∗) also has p · ρ fraction of the entries in ex-
pectation.

Assumptions 1, 2 are standard assumptions in the prov-
able matrix completion literature (Cand`es & Recht, 2009;
Recht, 2011; Jain & Netrapalli, 2015), while Assumptions
1, 3 are standard assumptions in the robust PCA (low-
rank+sparse matrix recovery) literature (Chandrasekaran
et al., 2011; Cand`es et al., 2011; Hsu et al., 2011). Hence,
our setting is a generalization of both the standard and pop-
ular problems and as we show later in the section, our result
can be used to meaningfully improve the state-of-the-art for
both of these problems.

We ﬁrst present our main result for Algorithm 1 under the
assumptions given above.
Theorem 1. Let Assumptions 1, 2 and 3 on L∗, (cid:101)S∗ and
Ω hold respectively. Let m ≤ n, n = O(m), and let the
number of samples |Ω| satisfy:

E[|Ω|] ≥ Cα2µ4r2n log2 (n) log2

(cid:18) µ2rσ1
(cid:15)

(cid:19)

,

2 , Algorithm 1 with η = 4µ2r

where C is a global constant. Then, with probability at
least 1 − n− log α
m , at most
O(log((cid:107)M (cid:107)2/(cid:15)))) outer iterations and O(log( µ2r(cid:107)M (cid:107)2
))
inner iterations, outputs a matrix ˆL such that:
(cid:13)
ˆL − L∗(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤ (cid:15).

(cid:15)

Note that the number of samples matches information the-
oretic bound upto O(r log n log2 σ∗
1/(cid:15)) factor. Also, the
number of allowed corruptions in (cid:101)S∗ also matches the
known lower bounds (up to a constant factor) and cannot
be improved upon information theoretically.

We now present our result for the rank based stagewise al-
gorithm (Algorithm 3).
Theorem 2. Under Assumptions 1, 2 and 3 on L∗, (cid:101)S∗ and
Ω respectively and Ω satisfying:

E[|Ω|] ≥ Cα2µ4r3n log2 (n) log

(cid:18) µ2rσ1
(cid:15)

(cid:19)

,

Nearly Optimal Robust Matrix Completion

for a large enough constant C, then Algorithm 3 with η set
to 4µ2r
≤ (cid:15), w.p.
≥ 1 − n− log α
2 .

m outputs a matrix ˆL such that:

ˆL − L∗(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)

Notice that the sample complexity of Algorithm 3 has an
additional multiplicative factor of O(r) when compared to
that of Algorithm 1, but shaves off a factor of O(log(κ)).
Similarly, computational complexity of Algorithm 3 also
trades off a O(log κ) factor for O(r) factor from the com-
putational complexity of Algorithm 1.

Result for Matrix Completion: Note that for (cid:101)S∗ = 0, the
RMC problem with Assumptions 1,2 is exactly the same as
the standard matrix completion problem and hence, we get
the following result as a corollary of Theorem 1:
Corollary 1 (Matrix Completion). Suppose we observe Ω
and PΩ(L∗) where Assumptions 1,2 hold for L∗ and Ω.
Also, let E[|Ω|] ≥ Cα2µ4r2n log2 n log2 σ1/(cid:15) and m ≤
2 , Algorithm 1 outputs ˆL s.t.
n. Then, w.p. ≥ 1 − n− log α
(cid:107) ˆL − L∗(cid:107)2 ≤ (cid:15).

Table 1 compares our sample and time complexity bounds
for low-rank MC. Note that our sample complexity is
nearly the same as that of nuclear-norm methods while the
running time of our algorithm is signiﬁcantly better than
the existing results that have at most logarithmic depen-
dence on the condition number of L∗.

Result for Robust PCA: Consider the standard Robust
PCA problem (RPCA), where the goal is to recover L∗
from M = L∗ + (cid:101)S∗. For RPCA as well, we can randomly
sample |Ω| entries from M , where Ω satisﬁes the assump-
tion required by Theorem 1. This leads us to the following
corollary:
Corollary 2 (Robust PCA). Suppose we observe M =
L∗ + (cid:101)S∗, where Assumptions 1, 3 hold for L∗ and
(cid:101)S∗. Generate Ω ∈ [m] × [n] by sampling each en-
try independently with probability p, s.t., E[|Ω|] ≥
Cα2µ4r2n log2 n log2 σ1/(cid:15). Let m ≤ n. Then, w.p.
2 , Algorithm 1 outputs ˆL s.t. (cid:107) ˆL − L∗(cid:107)2 ≤ (cid:15).
≥ 1 − n− log α

Hence, using Theorem 1, we will still be able to recover L∗
but using only the sampled entries. Moreover, the running
time of the algorithm is only O(µ4nr3 log2 n log2(σ1/(cid:15))),
i.e., we are able to solve RPCA problem in time almost lin-
ear in n. To the best of our knowledge, the existing state-
of-the-art methods for RPCA require at least O(n2r) time
to perform the same task (Netrapalli et al., 2014; Gu et al.,
2016). Similarly, we don’t need to load the entire data ma-
trix in memory, but we can just sample the matrix and work
with the obtained sparse matrix with at most linear number
of entries. Hence, our method signiﬁcantly reduces both
run-time and space complexities, and as demonstrated em-
pirically in Section 4 can help scale our algorithm to very
large data sets without losing accuracy.

3.1. Proof Outline for Theorem 1

We now provide an outline of our proof for Theorem 1
and motivate some of our proof techniques; the proof of
Theorem 2 follows similarly. Recall that we assume that
M = L∗ + (cid:101)S∗ and deﬁne S∗ = PΩ( (cid:101)S∗). Similarly, we de-
ﬁne (cid:101)S(t) = HT ζ(M − L(t)). Critically, S(t) = PΩ( (cid:101)S(t))
(see Line 9 of Algorithm 1), i.e., (cid:101)S(t) is the set of iterates
that we “could” obtain if entire M was observed. Note that
we cannot compute (cid:101)S(t), it is introduced only to simplify
our analysis.

We ﬁrst re-write the projected gradient descent step for
L(t+1) as described in (4):

L(t+1) = Pkq

(cid:16)

L∗ + ( (cid:101)S∗ − (cid:101)S(t))
(cid:125)

(cid:124)

+

I −

(cid:19)

PΩq,t
p

(cid:18)

(cid:124)

(cid:123)

(cid:122)
(L(t) − L∗) +( (cid:101)S(t) − (cid:101)S∗))
(
(cid:125)

(cid:17)

(6)

(cid:123)(cid:122)
E1

E2
(cid:125)(cid:124)

(cid:123)(cid:122)
E3

That is, L(t+1) is obtained by rank-kq SVD of a perturbed
version of L∗: L∗ + E1 + E3. As we perform entrywise
thresholding to reduce (cid:107) (cid:101)S∗ − (cid:101)S(t)(cid:107)∞, we need to bound
(cid:107)L(t+1) −L∗(cid:107)∞. To this end, we use techniques from (Jain
& Netrapalli, 2015), (Netrapalli et al., 2014) that explicitly
model singular vectors of L(t+1) and argue about the inﬁn-
ity norm error using a Taylor series expansion. However,
in our case, such an error analysis requires analyzing the
following key quantities (H = E1 + E3):

∀1 ≤ j, s.t., j even :

∀1 ≤ j, s.t., j odd :

Aj := max
q∈[n]

Bj := max
q∈[m]

Cj := max
q∈[n]

Dj := max
q∈[m]

(cid:107)e(cid:62)
q

(cid:0)H (cid:62)H(cid:1) j

2 V ∗(cid:107)2

(cid:107)e(cid:62)
q

(cid:0)HH (cid:62)(cid:1) j

2 U ∗(cid:107)2,

(cid:107)e(cid:62)

q H (cid:62) (cid:0)HH (cid:62)(cid:1)(cid:98) j
2 (cid:99)

U ∗(cid:107)2

(cid:107)e(cid:62)

q H (cid:0)H (cid:62)H(cid:1)(cid:98) j
2 (cid:99)

V ∗(cid:107)2.

(7)

Note that E1 = 0 in the case of standard RPCA which was
analyzed in (Netrapalli et al., 2014), while E3 = 0 in the
case of standard MC which was considered in (Jain & Ne-
In contrast, in our case both E1 and E3
trapalli, 2015).
are non-zero. Moreover, E3 is dependent on random vari-
able Ω. Hence, for j ≥ 2, we will get cross terms between
E3 and E1 that will also have dependent random variables
which precludes application of standard Bernstein-style tail
bounds. To overcome this issue, we ﬁrst carefully segregate
the errors arising due to the randomness in the sampling
process and the deterministic sparse corruptions in (cid:101)S∗. We
do this by introducing (cid:101)S(t) which is the sparse iterate we

Nearly Optimal Robust Matrix Completion

Table 1: Comparison of PG-RMC and R-RMC with Other Matrix Completion Methods

Nuclear norm (Recht, 2011)

SVP (Jain & Netrapalli, 2015)

Sample Complexity
O (cid:0)µ2rn log2 n(cid:1)
O (cid:0)µ4r5n log3 n(cid:1)
O (cid:0)nµ4r9 log3 (κ) log2 n(cid:1)

Alt. Min. (Hardt & Wootters, 2014)
Alt. Grad. Desc. (Sun & Luo, 2015) O (cid:0)nrκ2 max{µ2 log n, µ4r6κ4}(cid:1)

R-RMC (This Paper)

PG-RMC (This Paper)

(cid:16)
(cid:16) σ∗
µ4r3n log2 (n) log
1
(cid:15)
µ4r2n log2 (n) log2 (cid:16) σ∗

(cid:16)

O

O

(cid:17)(cid:17)

(cid:17)(cid:17)

1
(cid:15)

(cid:15)

(cid:1)

Computational Complexity
O (cid:0)n3 log 1
(cid:15) )(cid:1)
O (cid:0)µ4r7n log3 n log( 1
O (cid:0)nµ4r13 log3 (κ) log2 n(cid:1)
O (cid:0)n2r6κ4 log (cid:0) 1
(cid:1)(cid:1)
(cid:15)
(cid:16) σ∗
µ4r4n log2 (n) log
1
(cid:15)
µ4r3n log2 (n) log2 (cid:16) σ∗

(cid:16)

(cid:16)

1
(cid:15)

(cid:17)(cid:17)

(cid:17)(cid:17)

O

O

would have obtained had the whole matrix been observed.
This allows us to decompose the error term into the sum of
E1 and E3 where E1 represents the error due to the sparse
corruptions and E3 represents the error arising from the
randomness in the sampling procedure. We then incorpo-
rate this decomposition into a careful combinatorial-style
argument similar to that of (Erdos et al., 2013; Jain & Ne-
trapalli, 2015) to bound the above given quantity. That is,
we can provide the following key lemma:
Lemma 1. Let L∗, Ω, and (cid:101)S∗ satisfy Assumptions 1, 2 and
3 respectively. Let L∗ = U ∗Σ∗(V ∗)(cid:62) be the singular value
decomposition of L∗. Furthermore, suppose that in the tth
iteration of the qth stage, (cid:101)S(t) deﬁned as HTζ(M − L(t))
satisﬁes Supp( (cid:101)S(t)) ⊆ Supp( (cid:101)S∗), then we have:

max{Aa, Ba, Ca, Da} ≤ µ

ρn (cid:107)E1(cid:107)∞

(cid:18)

(cid:114) r
m

antee in the q-th stage:

(cid:107)L(T ) − L∗(cid:107)∞ ≤

σ∗
kq+1

(cid:107)E1(cid:107)∞ + (cid:107)E2(cid:107)∞ ≤

σ∗
kq+1.

4µ2r
m
20µ2r
m

Moreover, by using sparsity of (cid:101)S∗ and the special structure
of E3 (See Lemma 7), we have: (cid:107)E1 + E3(cid:107)2 ≤ c · σ∗
kq+1,
where c is a small constant.

Now, the outer iteration sets the next stage’s rank kq+1 as:
kq+1 = |{i : σi(L∗ + E1 + E3) ≥ 0.5 · σkq+1(L∗ + E1 +
E3)}|. Using the bound on (cid:107)E1 + E3(cid:107)2 and Weyl’s eigen-
value perturbation bound (Lemma 2), we have: σ∗
≤
0.6 σ∗
1/(cid:15))) “outer” itera-
tions, Algorithm 1 converges to an (cid:15)-approximate solution
to L∗.

kq+1. Hence, after Q = O(log(σ∗

kq+1+1

+c

(cid:114) n
p

((cid:107)E1 − E2(cid:107)∞) log n

,

(cid:19)a

4. Experiments

∀c > 0, ∀0 ≤ j ≤ log n w.p. ≥ 1 − n−2 log c
4 +4, where
E1, E2 and E3 are deﬁned in (6), Aa, Ba, Ca, Da are de-
ﬁned in (7).

Remark: We would like to note that even for the stan-
i.e., when E1 = 0, we obtain bet-
dard MC setting,
ter bound than that of (Jain & Netrapalli, 2015) as we
i (E3)qU (cid:107)2 directly rather than the
can bound maxi (cid:107)eT
i (E3)quj(cid:107) bound that (Jain & Netra-
r maxi (cid:107)eT
weaker
palli, 2015) uses.

√

Now, using Lemmas 1 and 7 and by using a hard-
thresholding argument we can bound (cid:107)L(t+1) − L∗(cid:107)∞ ≤
kq+1 + (cid:0) 1
2µ2r
σ∗
m (σ∗
) (see Lemma 9) in the q-th stage.
kq
Hence, after O(log(σ∗
1/(cid:15))) “inner” iterations, we can guar-

(cid:1)t

2

In this section we discuss the performance of Algorithm
1 on synthetic data and its use in foreground background
separation. The goal of the section is two-fold: a) to
demonstrate practicality and effectiveness of Algorithm 1
for the RMC problem, b) to show that Algorithm 1 in-
deed solves RPCA problem in signiﬁcantly smaller time
than that required by the existing state-of-the-art algorithm
(St-NcRPCA (Netrapalli et al., 2014)). To this end, we use
synthetic data as well as video datasets where the goal is to
perform foreground-background separation (Cand`es et al.,
2011).

We implemented our algorithm in MATLAB and the re-
sults for the synthetic data set were obtained by averaging
over 20 runs. We obtained a matlab implementation of St-
NcRPCA (Netrapalli et al., 2014) from the authors of (Ne-
trapalli et al., 2014). Note that if the sampling probability
is p = 1, then our method is similar to St-NcRPCA; the
key difference being how the rank is selected in each stage.

Nearly Optimal Robust Matrix Completion

We also implemented the Alternating Minimzation based
algorithm from (Gu et al., 2016). However, we found it
to be an order of magnitude slower than Algorithm 1 on
the foreground-background separation task. For example,
on the escalator video, the algorithm did not converge in
less than 150 seconds despite discounting for the expen-
sive sorting operation in the truncation step. On the other
hand, our algorithm ﬁnds the foreground in about 8 sec-
onds.

√

(cid:13)2 /

(cid:13)M − S(t)(cid:13)

(cid:13)M − S(t)(cid:13)

Parameters. The algorithm has three main parameters: 1)
threshold ζ, 2) incoherence µ and 3) sampling probabil-
ity p (E[|Ω|] = p · mn). In the experiments on synthetic
data we observed that keeping ζ ∼ µ (cid:13)
n
speeds up the recovery while for background extraction
keeping ζ ∼ µ (cid:13)
(cid:13)2 /n gives a better quality out-
put. The value of µ for real world data sets was ﬁgured
out using cross validation while for the synthetic data the
same value was used as used in data generation. The sam-
pling probability for the synthetic data could be kept as low
as (θ =)2r log2(n)/n while for the real world data set we
get good results for p = 0.05. We deﬁne effective sam-
ple size as the ratio between the sampling probability and
θ. Also, rather than splitting samples, we use the entire
set of observed entries to perform our updates (see Algo-
rithm 1).

Synthetic data. We generate M = L∗ + (cid:101)S∗ of two sizes,
where L∗ = U V (cid:62) ∈ R2000×2000 (and R5000×5000) is a
random rank-5 (and rank-10 respectively) matrix with in-
coherence ≈ 1. (cid:101)S∗ is generated by considering a uni-
(cid:13)
(cid:13)
from [m] × [n] where
formly random subset of size
every entry is i.i.d.
from the uniform distribution in
r
r√
[
mn ]. This is the same setup as used in (Cand`es
mn ,
√
2
et al., 2011).

(cid:13) (cid:101)S∗(cid:13)
(cid:13)
(cid:13)0

Figure 1 (a) plots recovery error ((cid:107)L − L∗(cid:107)F ) vs com-
putational time for our PG-RMC method (with different
sampling probabilities) as well as the St-NcRPCA algo-
rithm. Note that even for very small values of sampling
p, we can achieve the same recovery error as when using
signiﬁcantly small values. For example, our method with
p = 0.1 achieve 0.01 error ((cid:107)L − L∗(cid:107)F ) in ≈ 2.5s while
St-NcRPCA method requires ≈ 10s to achieve the same
accuracy. Note that we do not compare against the convex
relaxation based methods like IALM from (Cand`es et al.,
2011), as (Netrapalli et al., 2014) shows that St-NcRPCA
is signiﬁcantly faster than IALM and several other convex
relaxation solvers.

Figure 1 (b) plots time required to achieve different recov-
ery errors ((cid:107)L − L∗(cid:107)F ) as the sampling probability p in-
creases. As expected, we observe a linear increase in the
run-time with p. Interestingly, for very small values of p,
In this regime,
we observe an increase in running time.

(a)

(c)

(b)

(d)

(e)

Figure 1: Performance of PG-RMC on synthetic data. 1a:
time vs error for various sampling probabilities; time taken
by St-NcRPCA 1b: sampling probability vs time for con-
stant error; time taken decreases with decreasing sampling
probability upto an extent and then increases 1c: time vs
rank for constant error 1d:
incoherence vs time for con-
stant error 1e: success probability vs effective sample size
for various matrix sizes

(cid:107)PΩ(M )(cid:107)2
p

becomes very large (as p doesn’t satisfy the sam-
pling requirements). Hence, the increase in the number of
iterations (T ≈ log (cid:107)PΩ(M )(cid:107)2
) dominates the decrease in
per iteration time complexity.

p(cid:15)

Figure 1 (c), (d) plot computation time required by our
method (PG-RMC , Algorithm 1) versus rank and inco-
herence, respectively. As expected, as these two problem
parameters increase, our method requires more time. Note
that our run-time dependence on rank seems to be linear,
while our results require O(r3) time. This hints at the pos-
sibility of further improving the computational complexity
analysis of our algorithm.

Figure 1 (e) plots the effective sample size against suc-
cess probability. We see that the probability of recovering
the underlying low rank matrix undergoes a rapid phase

Time(s)51015log||L∗−ˆL||F-6-4-202n=5000,µ=1,r=10,ρ=0.01p = 0.05p = 0.1St-NcRPCASampling probability0.10.150.20.250.3Time(s)4681012n=2000,µ=1,r=5,ρ=0.01err = 1e-5err = 1e-3err = 1e-1Rank5101520Time(s)010203040n=2000,r=5,ρ=0.01,p=0.5err = 1e-1err = 1e-3err = 1e-5µ0.511.52Time(s)246810n=2000,r=5,ρ=0.01,p=0.1err = 1e-5err = 1e-3err = 1e-1effective sample size00.51success probability00.51r = 5, µ = 1, ρ = 0.01n = 1000n = 2000n = 3000Nearly Optimal Robust Matrix Completion

eral videos. Figure 2 (a), (d) show one frame each from two
such videos (a shopping center video, a restaurant video).
Figure 2 (b), (d) show the extracted background from the
two videos by using our method (PG-RMC , Algorithm 1)
with probability of sampling p = 0.05. Figure 2 (c), (f)
compare objective function value for different p values.
Clearly, PG-RMC can recover the true background with
p as small as 0.05. We also observe an order of magni-
tude speedup (≈ 5x) over St-NcRPCA (Netrapalli et al.,
2014). We present results on the video Escalator in Ap-
pendix 5.5.

Conclusion. In this work, we studied the Robust Matrix
Completion problem. For this problem, we provide exact
recovery of the low-rank matrix L∗ using nearly optimal
number of observations as well as nearly optimal fraction
of corruptions in the observed entries. Our RMC result
is based on a simple and efﬁcient PGD algorithm that has
nearly linear time complexity as well. Our result improves
state-of-the-art sample and run-time complexities for the
related Matrix Completion as well as Robust PCA prob-
lem. For Robust PCA, we provide ﬁrst nearly linear time
algorithm under standard assumptions.

Our sample complexity depends on (cid:15), the desired accu-
racy in L∗. Removing this factor will be an interesting
future work. Moreover, improving dependence of sample
complexity on r (from r2 to r) also represents an impor-
tant direction. Finally, similar to foreground background
separation, we would like to explore more applications of
RMC/RPCA.

References

Bhatia, Rajendra. Matrix Analysis. Springer, 1997.

Blumensath, Thomas. Sampling and reconstructing signals
IEEE Trans. Infor-
from a union of linear subspaces.
mation Theory, 57(7):4660–4671, 2011. doi: 10.1109/
URL http://dx.doi.org/
TIT.2011.2146550.
10.1109/TIT.2011.2146550.

Cand`es, Emmanuel J. and Recht, Benjamin. Exact ma-
trix completion via convex optimization. Foundations
of Computational Mathematics, 9(6):717–772, Decem-
ber 2009.

Cand`es, Emmanuel J., Li, Xiaodong, Ma, Yi, and Wright,
John. Robust principal component analysis? J. ACM, 58
(3):11, 2011.

Chandrasekaran, Venkat, Sanghavi, Sujay, Parrilo,
Pablo A., and Willsky, Alan S. Rank-sparsity inco-
herence for matrix decomposition. SIAM Journal on
Optimization, 21(2):572–596, 2011.

(a)

(c)

(b)

(d)

(e)

(f)

Figure 2: PG-RMC on Shopping video. 2a: a video frame
2c: an extracted background frame 2e: time vs error for dif-
ferent sampling probabilities; PG-RMC takes 38.7s while
St-NcPCA takes 204.4s. PG-RMC on Restaurant video.
2b: a video frame 2d: an extracted background frame
2f: time vs error for different sampling probabilities; PG-
RMC takes 7.9s while St-NcPCA takes 27.8s

transition from 0 to 1 when sampling probability crosses
∼ r log2 n/n.

We also study phase transition for different values of sam-
pling probability p. Figure 3 (a) in Appendix 5.5 show a
phase transition phenomenon where beyond p > .06 the
probability of recovery is almost 1 while below it, it is al-
most 0.

Foreground-background separation. We also applied
our technique to the problem of foreground-background
separation. We use the usual method of stacking up the
vectorized video frames to construct a matrix. The back-
ground, being static, will form the low rank component
while the foreground is considered to be the noise.

We applied our PG-RMC method (with varying p) to sev-

Time(s)0100200300log||M−ˆL−ˆS||F-20-1001020µ=1,r=5p = 0.05p = 0.1St-NcRPCATime(s)0102030log||M−ˆL−ˆS||F-10-5051015µ=1,r=5p = 0.01p = 0.05p = 0.1St-NcRPCANearly Optimal Robust Matrix Completion

Montreal, Quebec, Canada, pp. 685–693, 2014. URL
http://papers.nips.cc/paper/5293-on-
iterative-hard-thresholding-methods-
for-high-dimensional-m-estimation.

Jalali, Ali, Ravikumar, Pradeep, Vasuki, Vishvas, and
Sanghavi, Sujay. On learning discrete graphical models
In Proceedings of
using group-sparse regularization.
the Fourteenth International Conference on Artiﬁcial
Intelligence and Statistics, AISTATS 2011, Fort Laud-
erdale, USA, April 11-13, 2011, pp. 378–387, 2011.
http://www.jmlr.org/proceedings/
URL
papers/v15/jalali11a/jalali11a.pdf.

Klopp, Olga, Lounici, Karim, and Tsybakov, Alexan-
arXiv preprint

dre B. Robust matrix completion.
arXiv:1412.8132, 2014.

Li, Xiaodong. Compressed sensing and matrix comple-
tion with constant proportion of corruptions. Construc-
tive Approximation, 37(1):73–99, 2013.
ISSN 1432-
0940. doi: 10.1007/s00365-012-9176-9. URL http:
//dx.doi.org/10.1007/s00365-012-9176-9.

Netrapalli, Praneeth, U N, Niranjan, Sanghavi, Sujay,
Anandkumar, Animashree, and Jain, Prateek. Non-
convex robust pca.
In Ghahramani, Z., Welling,
M., Cortes, C., Lawrence, N. D., and Weinberger,
K. Q. (eds.), Advances in Neural Information Process-
ing Systems 27, pp. 1107–1115. Curran Associates, Inc.,
URL http://papers.nips.cc/paper/
2014.
5430-non-convex-robust-pca.pdf.

Recht, Benjamin. A simpler approach to matrix com-
Journal of Machine Learning Research,
pletion.
12:3413–3430, 2011. URL http://dl.acm.org/
citation.cfm?id=2185803.

Sun, Ruoyu and Luo, Zhi-Quan. Guaranteed matrix com-
pletion via nonconvex factorization. In IEEE 56th An-
nual Symposium on Foundations of Computer Science,
FOCS 2015, Berkeley, CA, USA, 17-20 October, 2015,
pp. 270–289, 2015. doi: 10.1109/FOCS.2015.25. URL
http://dx.doi.org/10.1109/FOCS.2015.25.

Yi, Xinyang, Park, Dohyung, Chen, Yudong, and Carama-
nis, Constantine. Fast algorithms for robust PCA via
gradient descent. CoRR, abs/1605.07784, 2016. URL
http://arxiv.org/abs/1605.07784.

Chen, Yudong, Jalali, Ali, Sanghavi, Sujay, and Carama-
nis, Constantine. Low-rank matrix recovery from errors
and erasures. In 2011 IEEE International Symposium on
Information Theory Proceedings, ISIT 2011, St. Peters-
burg, Russia, July 31 - August 5, 2011, pp. 2313–2317,
2011. doi: 10.1109/ISIT.2011.6033975. URL http:
//dx.doi.org/10.1109/ISIT.2011.6033975.

Erdos, L´aszl´o, Knowles, Antti, Yau, Horng-Tzer, and Yin,
Jun. Spectral statistics of Erdos–R´enyi graphs I: Local
semicircle law. The Annals of Probability, 41(3B):2279–
2375, 2013.

Gu, Quanquan, Wang, Zhaoran Wang, and Liu, Han. Low-
rank and sparse structure pursuit via alternating min-
In Proceedings of the Nineteenth Interna-
imization.
tional Conference on Artiﬁcial Intelligence and Statis-
tics, AISTATS 2016, C´adiz, Spain, May 9-11, 2016,
2016. URL http://jmlr.org/proceedings/
papers/v51/gu16.html.

Hardt, Moritz and Wootters, Mary.

Fast matrix com-
In Proceedings
pletion without the condition number.
of The 27th Conference on Learning Theory, COLT
2014, Barcelona, Spain, June 13-15, 2014, pp. 638–678,
2014. URL http://jmlr.org/proceedings/
papers/v35/hardt14a.html.

Computational

Hardt, Moritz, Meka, Raghu, Raghavendra, Prasad, and
limits for matrix
Weitz, Benjamin.
In Proceedings of The 27th Confer-
completion.
ence on Learning Theory, COLT 2014, Barcelona,
Spain, June 13-15, 2014, pp. 703–725, 2014. URL
http://jmlr.org/proceedings/papers/
v35/hardt14b.html.

Hsu, Daniel, Kakade, Sham M, and Zhang, Tong. Robust
matrix decomposition with sparse corruptions. Informa-
tion Theory, IEEE Transactions on, 57(11):7221–7234,
2011.

Jain, Prateek and Netrapalli, Praneeth. Fast exact ma-
In Proceedings
trix completion with ﬁnite samples.
of The 28th Conference on Learning Theory, COLT
2015, Paris, France, July 3-6, 2015, pp. 1007–1034,
2015. URL http://jmlr.org/proceedings/
papers/v40/Jain15.html.

Jain, Prateek, Meka, Raghu, and Dhillon, Inderjit S. Guar-
anteed rank minimization via singular value projection.
In NIPS, pp. 937–945, 2010.

Jain, Prateek, Tewari, Ambuj, and Kar, Purushottam. On it-
erative hard thresholding methods for high-dimensional
m-estimation. In Advances in Neural Information Pro-
cessing Systems 27: Annual Conference on Neural Infor-
mation Processing Systems 2014, December 8-13 2014,

