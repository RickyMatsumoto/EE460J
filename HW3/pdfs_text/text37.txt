Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Yossi Arjevani 1 Ohad Shamir 1

Abstract
Finite-sum optimization problems are ubiquitous
in machine learning, and are commonly solved
using ﬁrst-order methods which rely on gra-
dient computations. Recently, there has been
growing interest in second-order methods, which
rely on both gradients and Hessians.
In prin-
ciple, second-order methods can require much
fewer iterations than ﬁrst-order methods, and
hold the promise for more efﬁcient algorithms.
Although computing and manipulating Hessians
is prohibitive for high-dimensional problems in
general, the Hessians of individual functions in
ﬁnite-sum problems can often be efﬁciently com-
puted, e.g.
because they possess a low-rank
structure. Can second-order information indeed
be used to solve such problems more efﬁciently?
In this paper, we provide evidence that the an-
swer – perhaps surprisingly – is negative, at
least in terms of worst-case guarantees. We also
discuss what additional assumptions and algo-
rithmic approaches might potentially circumvent
this negative result.

1. Introduction

We consider ﬁnite-sum problems of the form

min
w∈W

F (w) =

fi(w),

(1)

1
n

n
(cid:88)

i=1

where W is a closed convex subset of some Euclidean or
Hilbert space, each fi is convex and µ-smooth, and F is
λ-strongly convex1. Such problems are ubiquitous in ma-
chine learning, for example in order to perform empirical
risk minimization using convex losses.

1Department of Computer Science and Applied Mathemat-
ics, Weizmann Institute of Science, Rehovot, Israel. Corre-
spondence to: Yossi Arjevani <yossi.arjevani@weizmann.ac.il>,
Ohad Shamir <ohad.shamir@weizmann.ac.il>.

To study the complexity of this and other optimization
problems,
it is common to consider an oracle model,
where the optimization algorithm has no a-priori informa-
tion about the objective function, and obtains information
from an oracle which provides values and derivatives of the
function at various domain points (Nemirovsky and Yudin,
1983). The complexity of the algorithm is measured in
terms of the number of oracle calls required to optimize
the function to within some prescribed accuracy.

Existing lower bounds for ﬁnite-sum problems show that
using a ﬁrst-order oracle, which given a point w and index
i = 1, . . . , n returns fi(w) and ∇fi(w), the number of
oracle queries required to ﬁnd an (cid:15)-optimal solution is at
least of order

(cid:18)

Ω

n +

(cid:114) nµ
λ

log

(cid:19)(cid:19)

,

(cid:18) 1
(cid:15)

either under algorithmic assumptions or assuming the di-
mension is sufﬁciently large2 (Agarwal and Bottou, 2014;
Lan, 2015; Woodworth and Srebro, 2016; Arjevani and
Shamir, 2016a). This is matched (up to log factors) by ex-
isting approaches, and cannot be improved in general.

An alternative to ﬁrst-order methods are second-order
methods, which also utilize Hessian information. A pro-
totypical example is the Newton method, which given a
(single) function F , performs iterations of the form

wt+1 = wt − αt

(cid:0)∇2F (w)(cid:1)−1

∇F (w),

(2)

where ∇F (w), ∇2F (w) are the gradient and the Hessian
of F at w, and αt is a step size parameter. Second-order
methods can have extremely fast convergence, better than
those of ﬁrst-order methods (i.e. quadratic instead of lin-
ear). Moreover, they can be invariant to afﬁne transforma-
tions of the objective function, and provably independent of
its strong convexity and smoothness parameters (assuming
self-concordance) (Boyd and Vandenberghe, 2004).
e.g.
A drawback of these methods, however, is that they can
be computationally prohibitive. In the context of machine

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1For a twice-differentiable function f , it is µ-smooth and λ-

strongly convex if λI (cid:22) ∇2f (w) (cid:22) µI for all w ∈ W.

2Depending on how (cid:15)-optimality is deﬁned precisely, and
where the algorithm is assumed to start, these bounds may have
additional factors inside the log. For simplicity, we present the ex-
isting bounds assuming (cid:15) is sufﬁciently small, so that a log(1/(cid:15))
term dominates.

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

learning, we are often interested in high-dimensional prob-
lems (where the dimension d is very large), and the Hes-
sians are d × d matrices which in general may not even ﬁt
into computer memory. However, for optimization prob-
lems as in Eq. (1), the Hessians of individual fi often have
a special structure. For example, a very common special
case of ﬁnite-sum problems in machine learning is empiri-
cal risk minimization for linear predictors, where

fi(w) = (cid:96)i((cid:104)w, xi(cid:105)),

where xi is a training instance and (cid:96)i is some loss func-
tion. In that case, assuming (cid:96)i is twice-differentiable, the
Hessian has the rank-1 form (cid:96)(cid:48)(cid:48)
i . Therefore,
the memory and computational effort involved with stor-
ing and manipulating the Hessian of this function is merely
linear (rather than quadratic) in d. Thus, it is tractable even
for high-dimensional problems.

i ((cid:104)w, xi(cid:105))xix(cid:62)

Building on this, several recent papers proposed and
analyzed second-order methods for ﬁnite-sum problems,
which utilize Hessians of the individual functions fi (see
for instance Erdogdu and Montanari (2015); Agarwal et al.
(2016); Pilanci and Wainwright (2015); Roosta-Khorasani
and Mahoney (2016a;b); Bollapragada et al. (2016); Xu
et al. (2016) and references therein). These can all be
viewed as approximate Newton methods, which replace the
actual Hessian ∇2F (w) = 1
i=1 ∇2fi(w) in Eq. (2) by
n
some approximation, based for instance on the Hessians of
a few individual functions fi sampled at random. One may
hope that such methods can inherit the favorable properties
of second-order methods, and improve on the performance
of commonly used ﬁrst-order methods.

(cid:80)n

In this paper, we consider the opposite direction, and study
lower bounds on the number of iterations required by algo-
rithms using second-order (or possibly even higher-order)
information, focusing on ﬁnite-sum problems which are
strongly-convex and smooth. We make the following con-
tributions:

• First, as a more minor contribution, we prove that
in the standard setting of optimizing a single smooth
and strongly convex function, second-order informa-
tion cannot improve the oracle complexity compared
to ﬁrst-order methods (at least in high dimensions).
Although this may seem unexpected at ﬁrst, the reason
is that the smoothness constraint must be extended to
higher-order derivatives, in order for higher-order in-
formation to be useful. We note that this observation
in itself is not new, and is brieﬂy mentioned (with-
out proof) in Nemirovsky and Yudin (1983, Section
7.2.6). Our contribution here is in providing a clean,
explicit statement and proof of this result.

• We then turn to present our main results, which state

(perhaps surprisingly) that under some mild algorith-
mic assumptions, and if the dimension is sufﬁciently
large, the oracle complexity of second-order methods
for ﬁnite-sum problems is no better than ﬁrst-order
methods, even if the ﬁnite-sum problem is composed
of quadratics (which are trivially smooth to any order).

• Despite this pessimistic conclusion, our

results
also indicate what assumptions and algorithmic ap-
In
proaches might be helpful in circumventing it.
particular, it appears that better, dimension-dependent
performance may be possible,
if the dimension is
moderate and the n individual functions in Eq. (1)
are accessed adaptively, in a manner depending on
the functions rather than ﬁxed in advance (e.g. sam-
pling them from a non-uniform distribution depend-
ing on their Hessians, as opposed to sampling them
uniformly at random). This provides evidence to
the necessity of adaptive sampling schemes, and a
dimension-dependent analysis, which indeed accords
with some recently proposed algorithms and deriva-
tions, e.g. (Agarwal et al., 2016; Xu et al., 2016). We
note that the limitations arising from oblivious opti-
mization schemes (in a somewhat stronger sense) was
also explored in (Arjevani and Shamir, 2016a;b).

The paper is structured as follows: We begin in Sec. 2 with
a lower bound for algorithms utilizing second-order infor-
mation, in the simpler setting where there is a single func-
tion F to be optimized, rather than a ﬁnite-sum problem.
We then turn to provide our main lower bounds in Sec. 3,
and discuss their applicability to some existing approaches
in Sec. 4. We conclude in Sec. 5, where we also discuss
possible approaches to circumvent our lower bounds. The
formal proofs of our results appear in Appendix A.

2. Strongly Convex and Smooth Optimization

with a Second-Order Oracle

Before presenting our main results for ﬁnite-sum optimiza-
tion problems, we consider the simpler problem of mini-
mizing a single strongly-convex and smooth function F (or
equivalently, Eq. (1) when n = 1), and prove a result which
may be of independent interest.

To formalize the setting, we follow a standard oracle
model, and assume that the algorithm does not have a-
priori information on the objective function F , except the
strong-convexity parameter λ and smoothness parameter
µ. Instead, it has access to an oracle, which given a point
w ∈ W, returns values and derivatives of F at w (either
∇F (w) for a ﬁrst-order oracle, or ∇F (w), ∇2F (w) for a
second-order oracle). The algorithm sequentially queries
the oracle using w1, w2, . . . , wT −1, and returns the point
wT . Our goal is to lower bound the number of oracle calls

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

T , required to ensure that wT is an (cid:15)-suboptimal solution.

Given a ﬁrst-order oracle and a strongly convex and smooth
objective in sufﬁciently high dimensions, it is well-known
that the worst-case oracle complexity is

Ω((cid:112)µ/λ · log(1/(cid:15)))

(Nemirovsky and Yudin, 1983). What if we replace this by
a second-order oracle, which returns both ∇2F (w) on top
of F (w), ∇F (w)?

Perhaps unexpectedly, it turns out that this additional infor-
mation does not substantially improve the worst-case ora-
cle complexity bound, as evidenced by the following theo-
rem:

Theorem 1. For any µ, λ such that µ > 8λ > 0, any
(cid:15) ∈ (0, 1), and any deterministic algorithm, there ex-
ists a µ-smooth, λ strongly-convex function F on Rd (for
d = ˜O((cid:112)µ/λ), hiding factors logarithmic in µ, λ, (cid:15)),
such that the number of calls T to a second-order ora-
cle, required to ensure that F (wT ) − minw∈Rd F (w) ≤
(cid:15) · (F (0) − minw∈Rd F (w)), must be at least

(cid:18)(cid:114) µ
8λ

c

(cid:19)

− 1

· log

(cid:18) (λ/µ)3/2
c(cid:48)(cid:15)

(cid:19)

,

where c, c(cid:48) are positive universal constants.

(cid:15)

For sufﬁciently large µ
λ and small (cid:15), this complexity lower
bound is Ω (cid:0)(cid:112) µ
(cid:1)(cid:1), which matches existing lower
λ · log (cid:0) 1
and upper bounds for optimizing strongly-convex and
smooth functions using ﬁrst-order methods. As mentioned
earlier, the observation that such ﬁrst-order oracle bounds
can be extended to higher-order oracles is also brieﬂy men-
tioned (without proof) in Nemirovsky and Yudin (1983,
Section 7.2.6). Also, the theorem considers deterministic
algorithms (which includes standard second-order meth-
ods, such as the Newton method), but otherwise makes no
assumption on the algorithm. Generalizing this result to
randomized algorithms should be quite doable, based on
the techniques developed in Woodworth and Srebro (2016).
We leave a formal derivation to future work.

Although this result may seem surprising at ﬁrst, it has
a simple explanation: In order for Hessian information,
which is local in nature, to be useful, there should be some
regularity constraint on the Hessian, which ensures that it
cannot change arbitrarily quickly as we move around the
domain. A typical choice for a constraint of this kind is
Lipschitz continuity which dictates that

(cid:107)∇2F (w) − ∇2F (w(cid:48))(cid:107) ≤ L(cid:107)w − w(cid:48)(cid:107),

for some constant L.
Indeed, the construction relies on
a function which does not have Lipschitz Hessians: It is

based on a standard lower bound construction for ﬁrst-
order oracles, but the function is locally “ﬂattened” in cer-
tain directions around points which are to be queried by
the algorithm. This is done in such a way, that the Hessian
observed by the algorithm does not provide more informa-
tion than the gradient, and cannot be used to improve the
algorithm’s performance.

3. Second-Order Oracle Complexity Bounds

for Finite-Sum Problems

We now turn to study ﬁnite-sum problems of the form
given in Eq. (1), and provide lower bounds on the num-
ber of oracle calls required to solve them, assuming a
second-order oracle. To adapt the setting to a ﬁnite-sum
problem, we assume that the second-order oracle is given
both a point w and an index i ∈ {1, . . . , n}, and re-
turns {fi(w), ∇fi(w), ∇2fi(w)}. The algorithm itera-
tively produces and queries the oracle with point-index
pairs {(wt, it)}T
t=1, with the goal of making the subopti-
mality (or expected suboptimality, if the algorithm is ran-
domized) smaller than (cid:15) using a minimal number of oracle
calls T .

In fact, the lower bound construction we use is such that
each function fi is quadratic. Unlike the construction of
the previous section, such functions have a constant (and
hence trivially Lipschitz) Hessian. Moreover, since any p-
order derivative of a quadratic for p > 2 is zero, this means
that our lower bounds automatically hold even if the ora-
cle provides p-th order derivatives at any w, for arbitrarily
large p.

However, in order to provide a lower bound using quadratic
functions, it is necessary to pose additional assumptions
on the structure of the algorithm (unlike Thm. 1 which is
purely information-based). To see why, note that with-
out computational constraints, the algorithm can simply
query the Hessians and gradients of each fi(w) at w = 0,
take the average to get ∇F (0) = 1
i=1 ∇fi(0) and
n
∇2F (0) = 1
i=1 ∇2fi(0), and return the exact op-
n
timum, which for quadratics equals −∇2F (0)−1∇F (0).
Therefore, with second-order information, the best possible
information-based lower bound for quadratics is no better
than Ω(n). This is not a satisfying bound, since in order
to attain it we need to invert the (possibly high-rank) d × d
matrix ∇2F (0). Therefore, if we are interested in bounds
for computationally-efﬁcient algorithms, we need to forbid
such operations.

(cid:80)n

(cid:80)n

Speciﬁcally, we will consider two algorithmic assumptions,
which are stated below (their applicability to existing algo-
rithms is discussed in the next section). The ﬁrst assump-
tion constrains the algorithm to query and return points
w which are computable using linear-algebraic manipula-

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

tions of previous points, gradients and Hessians. Moreover,
these manipulations can only depend on (at most) the last
(cid:98)n/2(cid:99) Hessians returned by the oracle. As discussed previ-
ously, this assumption is necessary to prevent the algorithm
from computing and inverting the full Hessian of F , which
is computationally prohibitive. Formally, the assumption is
the following:

Assumption 1 (Linear-Algebraic Computations). wt be-
longs to the set Wt ⊆ Rd, deﬁned recursively as follows:
W1 = {0}, and Wt+1 is the closure of the set of vectors
derived from Wt ∪ {∇fit(wt)} by a ﬁnite number of oper-
ations of the following form:

• w, w(cid:48) → αw + α(cid:48)w(cid:48), where α, α(cid:48) are arbitrary

scalars.

• w → Hw, where H is any d × d matrix which has the

same block-diagonal structure as

t
(cid:88)

ατ ∇2fiτ (wτ ),

(3)

τ =max{1,t−(cid:98)n/2(cid:99)+1}

for some arbitrary {ατ }.

The ﬁrst bullet allows to take arbitrary linear combinations
of previous points and gradients, and already covers stan-
dard ﬁrst-order methods and their variants. As to the sec-
ond bullet, by “same block-diagonal structure”, we mean
that if the matrix in Eq. (3) can be decomposed to r di-
agonal blocks of size d1, . . . , dr in order, then H can also
be decomposed into r blocks of size d1, . . . , dr in order
(note that this does not exclude the possibility that each
such block is composed of additional sub-blocks). To give
a few examples, if we let Ht be the matrix in Eq. (3), then
we may have:

• H = Ht,

• H = H −1

t

if Ht is invertible, or its pseudoinverse,

• H = (Ht+D)−1 (where D is some arbitrary diagonal

matrix, possibly acting as a regularizer),

between oracle calls. However, crucially, all these oper-
ations can be performed starting from a linear combina-
tion of at most (cid:98)n/2(cid:99) recent Hessians. As mentioned ear-
lier, this is necessary, since if we could compute the av-
erage of all Hessians, then we could implement the New-
ton method. The assumption that the algorithm only “re-
members” the last (cid:98)n/2(cid:99) Hessians is also realistic, as ex-
isting computationally-efﬁcient methods seek to use much
fewer than n individual Hessians at a time. We note that the
choice of (cid:98)n/2(cid:99) is rather arbitrary, and can be replaced by
αn for any constant α ∈ (0, 1). Also, the way the assump-
tion is formulated, the algorithm is assumed to be initial-
ized at the origin 0. However, this is merely for simplicity,
and can be replaced by any other ﬁxed vector (the lower
bound will hold by shifting the constructed “hard” function
appropriately).

The second (optional) assumption we will consider con-
strains the indices chosen by the algorithm to be oblivious,
in the following sense:

Assumption 2 (Index Obliviousness). The
indices
i1, i2, . . . chosen by the algorithm are independent of
f1, . . . , fn.

To put this assumption differently, the indices may just as
well be chosen before the algorithm begins querying the
oracle. This can include, for instance, sampling functions
fi uniformly at random from f1, . . . , fn, and performing
deterministic passes over f1, . . . , fn in order. As we will
see later on, this assumption is not strictly necessary, and
can be removed at the cost of a somewhat weaker result.
Nevertheless, the assumption covers all optimal ﬁrst-order
algorithms, as well as most second-order methods we are
aware of (see Sec. 4 for more details).

With these assumptions stated, we can ﬁnally turn to
present the main result of this section:

Theorem 2. For any n > 1, any µ > λ > 0, any
(cid:15) ∈ (0, c) (for some universal constant c > 0), and any
(possibly randomized) algorithm satisfying Assumptions 1
and 2, there exists µ-smooth, λ-strongly convex quadratic
functions f1, . . . , fn : Rd → R (for d = ˜O(1 + (cid:112)µ/λn),
hiding factors logarithmic in n, µ, λ, (cid:15)), such that the num-
ber of calls T to a second-order oracle, so that

E

(cid:20)
F (wT ) − min
w∈Rd

(cid:21)

(cid:18)

F (w)

≤ (cid:15) ·

F (0) − min
w∈Rd

F (w)

,

(cid:19)

• H is a truncated SVD decomposition of Ht (or again,
Ht + D or (Ht + D)−1 for some arbitrary diagonal
matrix D) or its pseudoinverse.

must be at least

Moreover, for quadratic functions, it is easily veriﬁed that
the assumption also allows prox operations (i.e. returning
arg minw fi(w) + ρ
2 (cid:107)w − w(cid:48)(cid:107)2 for some ρ, i and previ-
ously computed point w(cid:48)). Also, note that the assumption
places no limits on the number of such operations allowed

(cid:18)

Ω

n +

(cid:114) nµ
λ

· log

(cid:18) (λ/µ)3/2√
(cid:15)

(cid:19)(cid:19)

n

.

Comparing this with the (tight) ﬁrst-order oracle complex-
ity bounds discussed in the introduction, we see that the

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

lower bound is the same up to log-factors, despite the avail-
ability of second-order information. In particular, the lower
bound exhibits none of the favorable properties associated
with full second-order methods, which can compute and in-
vert Hessians of F : Whereas the full Newton method can
attain O(log log(1/(cid:15))) rates, and be independent of µ, λ if
F satisﬁes a self-concordance property (Boyd and Vanden-
berghe, 2004), here we only get a linear O(log(1/(cid:15))) rate,
and there is a strong dependence on µ, λ, even though the
function is quadratic and hence self-concordant.

The proof of the theorem is based on a randomized con-
struction, which can be sketched as follows: We choose
indices j1, . . . , jd−1 ∈ {1, . . . , n} independently and uni-
formly at random, and deﬁne

fi(w) = a · w2

1 + ˆa ·

1jl=i(wl − wl+1)2

d−1
(cid:88)

l=1

+ ¯a · w2

d − ˜a · w1 +

(cid:107)w(cid:107)2,

λ
2

where 1A is the indicator function of the event A, and
a, ˆa, ¯a, ˜a are parameters chosen based on λ, µ, n. The aver-
age function F (w) = 1
n

i=1 fi(w) equals

(cid:80)n

F (w) = a·w2

1+

(wl−wl+1)2+¯a·w2

d−˜a·w1+

(cid:107)w(cid:107)2.

λ
2

ˆa
n

·

d−1
(cid:88)

l=1

By setting the parameters appropriately, it can be shown
that F is λ-strongly convex and each fi is µ-smooth. More-
over, the optimum of F has the form (q, q2, q3, . . . , qd) for
√
√

q =

,

κ − 1
κ + 1

where

κ =

+ 1

µ
λ − 1
n

(4)

is the so-called condition number of F . The proof is based
on arguing that after T oracle calls, the points computable
by any algorithm satisfying Assumptions 2 and 1 must
have 0 values at all coordinates larger than some lT , hence
the squared distance of wT from the optimum must be at
least (cid:80)d
i=lT +1 q2i, which leads to our lower bound. Thus,
the proof revolves around upper bounding lT . We note
that a similar construction of F was used in some pre-
vious ﬁrst-order lower bounds under algorithmic assump-
tions (e.g. Nesterov (2013); Lan (2015), as well as Arjevani
and Shamir (2015) in a somewhat different context). The
main difference is in how we construct the individual func-
tions fi, and in analyzing the effect of second-order rather
than just ﬁrst-order information.

To upper bound lT , we let lt (where t = 1, . . . , T ) be
the largest non-zero coordinate in wt, and track how lt

increases with t. The key insight is that if w1, . . . , wt−1
are zero beyond some coordinate l, then any linear com-
binations of them, as well as multiplying them by matri-
ces based on second-order information, as speciﬁed in As-
sumption 1, will still result in vectors with zeros beyond co-
ordinate l. The only way to “advance” and increase the set
of non-zero coordinates is by happening to query the func-
tion fjl . However, since the indices of the queried func-
tions are chosen obliviously, whereas each jl is chosen uni-
formly at random, the probability of this happening is quite
small, of order 1/n. Moreover, we show that even if this
event occurs, we are unlikely to “advance” by more than
O(1) coordinates at a time. Thus, the algorithm essentially
needs to make Ω(n) oracle calls in expectation, in order to
increase the number of non-zero coordinates by O(1). It
can be shown that the number of coordinates needed to get
an (cid:15)-optimal solution is ˜Ω((cid:112)µ/nλ·log(1/(cid:15))) (hiding some
log-factors). Therefore, the total number of oracle calls is
about n times larger, namely ˜Ω((cid:112)nµ/λ · log(1/(cid:15))). To
complete the theorem, we also provide a simple and sepa-
rate Ω(n) lower bound, which holds since each oracle call
gives us information on just one of the n individual func-
tions f1, . . . , fn, and we need some information on most of
them in order to get a close-to-optimal solution.

When considering non-oblivious (i.e., adaptive) algo-
rithms, the construction used in Thm. 2 fails as soon as the
algorithm obtains the Hessians of all the individual func-
tions (potentially, after n oracle queries). Indeed, knowing
the Hessians of fi, one can devise an index-schedule which
gains at least one coordinate at every iteration (by query-
ing the function which holds the desired 2 × 2 block), as
opposed to O(1/n) on average in the oblivious case. Nev-
ertheless, as mentioned before, we can still provide a result
similar to Thm. 2 even if the indices are chosen adaptively,
at the cost of a much larger dimension:
Theorem 3. Thm. 2 still holds if one omits Assumption 2,
and with probability 1 rather than in expectation, at the
cost of requiring an exponentially larger dimensionality of

√

(cid:16)

˜O

1+

µ/λn

(cid:17)

.

d = n

The proof is rather straightforward: Making the depen-
dence on the random indices j1, . . . , jd−1 explicit,
the
quadratic construction used in the previous theorem can be
written as

F j1,...,jd−1(w) =

f j1,...,jd−1
i

(w)

1
n

n
(cid:88)

i=1

=

1
n

n
(cid:88)

i=1

w(cid:62)Aj1,...,jd−1
i

w − ˜a(cid:104)e1, w(cid:105) +

(cid:107)w(cid:107)2

λ
2

some d × d matrix Aj1,...,jd−1

for
dependent on
j1, . . . , jd−1, and a ﬁxed parameter ˜a. Now, we create n
huge block-diagonal matrices A1, . . . , An, where each Ai

i

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

contains Aj1,...,jd−1
for each of the nd−1 possible choices
i
of j1, . . . , jd−1 along its diagonal (in some canonical or-
der), and one huge vector

e = e1 + ed+1 + . . . + e(nd−1−1)d+1.

We then let

F (w) =

fi(w)

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

=

w(cid:62)Aiw − ˜a(cid:104)e, w(cid:105) +

(cid:107)w(cid:107)2.

λ
2

This function essentially combines all nd−1 problems
F j1,...,jd−1 simultaneously, where each F j1,...,jd−1 is em-
bedded in a disjoint set of coordinates. Due to the block-
diagonal structure of each Ai, this function inherits the
strong-convexity and smoothness properties of the original
construction. Moreover, to optimize this function, the algo-
rithm needs to “solve” all nd−1 problems simultaneously,
using the same choice of indices i1, i2, . . .. Using a com-
binatorial argument which parallels the probabilistic argu-
ment in the proof of Thm. 2, we can show that no matter
how these indices are chosen, the average number of non-
zero coordinates of the iterates cannot grow too rapidly, and
lead to the same bound as in Thm. 2. Since the construc-
tion is deterministic, and applies no matter how the indices
are chosen, the lower bound holds deterministically, rather
than in expectation as in Thm. 2.

Lastly, it is useful to consider how the bounds stated in
Thm. 2 and Thm. 3 differ when the dimension d is ﬁxed
and ﬁnite. Inspecting the proofs of both theorems reveals
that in both cases the suboptimality, as a function of the
iteration number T , has a linear convegence rate bounded
from below by

E

(cid:20) F (wT ) − F (w(cid:63))
F (0) − F (w(cid:63))

(cid:21)

≥ Ω(1)

(cid:18) √
√

κ − 1
κ + 1

(cid:19)O( T

n )

(5)

(where κ is as deﬁned in Eq. (4), and Ω(1) hides dependen-
cies on the problem parameters, but is independent of T ).
However, whereas the bound established in Thm. 2 is valid
for O(d) number of iterations, Thm. 3 applies to a much re-
stricted range of roughly log(d)/ log(n) iterations. This in-
dicate that adaptive optimization algorithms might be able
to gain a super-linear convergence rate after a signiﬁcantly
smaller number of iterations in comparison to oblivious al-
gorithms (see (Arjevani and Shamir, 2016a) for a similar
discussion regarding ﬁrst-order methods). That being said,
trading obliviousness for adaptivity may increase the per-
iteration cost and reduce numerical stability.

4. Comparison to Existing Approaches

As discussed in the introduction, there has been a recent
burst of activity involving second-order methods for solv-
ing ﬁnite-sum problems, relying on Hessians of individ-
ual functions fi. In this section, we review the main algo-
rithmic approaches and compare them to our results. The
bottom line is that most existing approaches satisfy the as-
sumptions stated in Sec. 3, and therefore our lower bounds
will apply, at least in a worst-case sense. A possible ex-
ception to this is the Newton sketch algorithm (Pilanci and
Wainwright, 2015), which relies on random projections,
but on the ﬂip side is computationally expensive.

Turning to the details, existing approaches are based on tak-
ing the standard Newton iteration for such problems,

wt+1 = wt − αt
(cid:32)

= wt − αt

(cid:0)∇2F (wt)(cid:1)−1

1
n

n
(cid:88)

i=1

∇2fi(wt)

∇F (wt)
(cid:33)−1 (cid:32)

(cid:33)

∇fi(wt)

,

1
n

n
(cid:88)

i=1

the

inverse

Hessian

term
replacing
and
i=1 ∇2fi(w)(cid:1)−1
(cid:0) 1
(cid:80)n
(and sometimes the vector term
n
(cid:80)n
1
i=1 ∇fi(w) as well) by some approximation which
n
is computationally cheaper to compute. One standard and
well-known approach is to use only gradient information
to construct such an approximation, leading to the family
of quasi-Newton methods (Nocedal and Wright, 2006).
However, as they rely on ﬁrst-order rather than second-
order information, they are orthogonal to the topic of our
work, and are already covered by existing complexity
lower bounds for ﬁrst-order oracles.

Turning to consider Hessian approximation techniques us-
ing second-order information, perhaps the simplest and
most intuitive approach is sampling: Since the Hessian
equals the average of many individual Hessians,

∇2F (w) =

∇2fi(w),

1
n

n
(cid:88)

i=1

we can approximate it by taking a sample S of indices in
{1, . . . , n} uniformly at random, compute the Hessians of
the corresponding individual functions, and use the approx-
imation

∇2F (w) ≈

∇2fi(w).

1
|S|

(cid:88)

i∈S

If |S| is large enough, then by concentration of measure
arguments, this sample average should be close to the ac-
tual Hessian ∇2F (w). On the other hand, if |S| is not too
large, then the resulting matrix is easier to invert (e.g. be-
cause it has a rank of only O(|S|), if each individual Hes-
sian has rank O(1), as in the case of linear predictors).
Thus, one can hope that the right sample size will lead

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

to computational savings. There have been several rigor-
ous studies of such “subsampled Newton” methods, such
as Erdogdu and Montanari (2015); Roosta-Khorasani and
Mahoney (2016a;b); Bollapragada et al. (2016) and refer-
ences therein. However, our lower bound in Thm. 2 holds
for such an approach, since it satisﬁes both Assumption 2
and 1. As expected, the existing worst-case complexity up-
per bounds are no better than our lower bound.

(Xu et al., 2016) recently proposed a subsampled New-
ton method, together with non-uniform sampling, which
assigns more weight to individual functions which are
deemed more “important”. This is measured via proper-
ties of the Hessians of the functions, such as their norms or
via leverage scores. This approach breaks Assumption 2,
as the sampled indices are now chosen in a way dependent
on the individual functions. However, our lower bound in
Thm. 3, which does not require this assumption, still ap-
plies to such a method.

A variant of the subsampled Newton approach, studied in
Erdogdu and Montanari (2015), uses a low-rank approxi-
mation of the sample Hessian (attained by truncated SVD),
in lieu of the sample Hessian itself. However, this still falls
in the framework of Assumption 1, and our lower bound
still applies.

A different approach to approximate the full Hessian is via
randomized sketching techniques, which replace the Hes-
sian ∇2F (w) by a low-rank approximation of the form

(∇2F (w))1/2SS(cid:62)(∇2F (w))1/2,

where S ∈ Rd×m, m (cid:28) d is a random sketching matrix,
and (∇2F (w))1/2 is the matrix square root of ∇2F (w).
This approach forms the basis of the Newton sketch al-
gorithm proposed in Pilanci and Wainwright (2015). This
approach currently escapes our lower bound, since it vio-
lates Assumption 1. That being said, this approach is in-
herently expensive in terms of computational resources, as
it requires us to compute the square root of the full Hessian
matrix. Even under favorable conditions, this requires us
to perform a full pass over all functions f1, . . . , fn at ev-
ery iteration. Moreover, existing iteration complexity up-
per bounds have a strong dependence on both µ/λ as well
as the dimension d, and are considerably worse than the
lower bound of Thm. 2. Therefore, we conjecture that this
approach cannot lead to better worst-case results.

Agarwal et al. (2016) develop another line of stochastic
second-order methods, which are based on the observation
that the Newton step (∇2F (w))−1∇F (w) is the solution
of the system of linear equations

∇2F (w)x = ∇F (w).

Thus, one can reduce the optimization problem to solving
this system as efﬁciently as possible. The basic variant of

their algorithm (denoted as LiSSA) relies on operations of
the form

w (cid:55)→ (I − ∇2fi(w))w

(for i sampled uniformly at random), as well as linear com-
binations of such vectors, which satisfy our assumptions. A
second variant, LiSSA-Quad, re-phrases this linear system
as the ﬁnite-sum optimization problem

x(cid:62)∇2F (w)x + ∇F (w)(cid:62)x

min
x

=

1
n

n
(cid:88)

i=1

x(cid:62)∇2fi(w)x + ∇fi(w)(cid:62)x,

and uses some ﬁrst-order method for ﬁnite-sum problems
in order to solve it. Since individual gradients of this objec-
tive are of the form ∇2fi(w)x + ∇fi(w), and most state-
of-the-art ﬁrst-order methods pick indices i obliviously,
this approach also satisﬁes our assumptions, and our lower
bounds apply. Yet another proposed algorithm, LiSSA-
Sample, is based on replacing the optimization problem
above by

min
x

x(cid:62)∇2F (w)B−1x + ∇F (w)(cid:62)x,

(6)

where B is some invertible matrix, solving it (with the
to B(∇2F (w))−1∇F (w)), and
optimum being equal
multiplying the solution by B−1 to recover the solution
(∇2F (w))−1∇F (w) to the original problem.
In order
to get computational savings, B is chosen to be a lin-
ear combination of O(d log(d)) sampled individual hes-
sians ∇2fi(w), where it is assumed that d log(d) (cid:28) n,
and the sampling and weighting is carefully chosen (based
on the Hessians) so that Eq. (6) has strong convexity and
smoothness parameters within a constant of each other. As
a result, Eq. (6) can be solved fast using standard gradi-
ent descent, taking steps along the gradient, which equals
∇2F (w)B−1x + ∇F (w) at any point x. This gradient is
again computable under 1 (using O(n) oracle calls), since
B is a linear combination of d log(d) (cid:28) n sampled indi-
vidual Hessians. Thus, our lower bound (in the form of
Thm. 3) still applies to such methods.

That being said, it is important to note that the complexity
upper bound attained in Agarwal et al. (2016) for LiSSA-
Sample is on the order of

˜O((n + (cid:112)dµ/λ) · polylog(1/(cid:15)))

(at least asymptotically as (cid:15) → 0), which can be better than
our lower bound if d (cid:28) n. There is no contradiction, since
the lower bound in Thm. 3 only applies for a dimension d
much larger than n. Interestingly, our results also indicate
that an adaptive index sampling scheme is necessary to get
this kind of improved performance when d (cid:28) n: Other-
wise, it could violate Thm. 2, which establishes a lower

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

particular:

bound of ˜O(n + (cid:112)nµ/λ) even if the dimension is quite
moderate (d = ˜O(1 + (cid:112)µ/λn), which is (cid:28) n under the
mild assumption that µ/λ (cid:28) n3).

The observation that an adaptive scheme (breaking assump-
tion 2) can help performance when d (cid:28) n is also seen
in the lower bound construction used to prove Thm. 2: If
µ, λ, n are such that the required dimension d is (cid:28) n, then
it means that only the functions fj1, . . . , fjd−1, which are
a small fraction of all n individual functions, are informa-
tive and help us reduce the objective value. Thus, sampling
these functions in an adaptive manner is imperative to get
better complexity than the bound in Thm. 2. Based on the
fact that only at most d−1 out of n functions are relevant in
the construction, we conjecture that the possible improve-
ment in the worst-case oracle complexity of such schemes
may amount to replacing dependencies on n with depen-
dencies on d, which is indeed the type of improvement at-
tained (for small enough (cid:15)) in Agarwal et al. (2016).

Finally, we note that Agarwal et al. (2016) proposes another
algorithm tailored to self-concordant functions, with run-
time independent of the smoothness and strong convexity
parameters of the problem. However, it requires perform-
ing ≥ 1 full Newton steps, so the runtime is prohibitive for
large-scale problems (indeed, for quadratics as used in our
lower bounds, even a single Newton step sufﬁces to com-
pute an exact solution).

5. Summary and Discussion

In this paper, we studied the oracle complexity for opti-
mization problems, assuming availability of a second-order
oracle. This is in contrast to most existing oracle complex-
ity results, which focus on a ﬁrst-order oracle. First, we
formally proved that in the standard setting of strongly-
convex and smooth optimization problems, second-order
information does not signiﬁcantly improve the oracle com-
plexity, and further assumptions (i.e. Lipschitzness of the
Hessians) are in fact necessary. We then presented our
main lower bounds, which show that for ﬁnite-sum prob-
lems with a second-order oracle, under some reasonable
algorithmic assumptions, the resulting oracle complexity
is – again – not signiﬁcantly better than what can be ob-
tained using a ﬁrst-order oracle. Moreover, this is shown
using quadratic functions, which have 0 derivatives of or-
der larger than 2. Hence, our lower bounds apply even if
we have access to an oracle returning derivatives of order p
for all p ≥ 0, and the function is smooth to any order. In
Sec. 4, we studied how our framework and lower bounds
are applicable to most existing approaches.

Although this conclusion may appear very pessimistic, they
are actually useful in pinpointing potential assumptions and
approaches which may circumvent these lower bounds. In

• Our lower bound for algorithms employing adaptive
index sampling schemes (Thm. 3) only hold when the
dimension d is very large. This leaves open the possi-
bility of better (non index-oblivious) algorithms when
d is moderate, as was recently demonstrated in the
context of the LiSSA-Sample algorithm of Agarwal
et al. (2016) (at least for small enough (cid:15)). As discussed
in the previous section, we conjecture that the possi-
ble improvement in the worst-case oracle complexity
of such schemes may amount to replacing dependen-
cies on n with dependencies on d.

• It might be possible to construct algorithms breaking
Assumption 1, e.g. by using operations which are not
linear-algebraic. That being said, we currently con-
jecture that this assumptions can be signiﬁcantly re-
laxed, and similar results would hold for any algo-
rithm which has “signiﬁcantly” cheaper iterations (in
terms of runtime) compared to the Newton method.

• Our lower bounds are worst-case over smooth and
strongly-convex individual functions fi. It could be
that by assuming more structure, better bounds can
be obtained. For example, as discussed in the intro-
duction, an important special case is when fi(w) =
(cid:96)i(x(cid:62)
i w) for some scalar function (cid:96)i and vector xi.
Our construction in Thm. 2 does not quite ﬁt this struc-
ture, although it is easy to show that we still get func-
tions of the form fi(w) = (cid:96)i(X (cid:62)
i w), where Xi has
O(1 + d/n) = ˜O(1 + (cid:112)µ/λn3) rows in expectation,
which is ˜O(1) under a broad parameter regime. We
believe that the difference between ˜O(1) rows and 1
row is not signiﬁcant in terms of the attainable oracle
complexity, but we may be wrong. Another possibil-
ity is to provide results depending on more delicate
spectral properties of the function, beyond its strong
convexity and smoothness, which may lead to better
results and algorithms under favorable assumptions.

• Our lower bounds in Sec. 3, which establish a lin-
ear convergence rate (logarithmic dependence on
log(1/(cid:15))), are non-trivial only if the optimization error
(cid:15) is sufﬁciently small. This does not preclude the pos-
sibility of attaining better initial performance when (cid:15)
is relatively large.

In any case, we believe that our work lays the foundation
for a more comprehensive study of the complexity of efﬁ-
cient second-order methods, for ﬁnite-sum and related op-
timization and learning problems.

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Farbod Roosta-Khorasani and Michael W Mahoney. Sub-
sampled newton methods i: globally convergent algo-
rithms. arXiv preprint arXiv:1601.04737, 2016a.

Farbod Roosta-Khorasani and Michael W Mahoney. Sub-
sampled newton methods ii: Local convergence rates.
arXiv preprint arXiv:1601.04738, 2016b.

Blake Woodworth and Nathan Srebro. Tight complex-
ity bounds for optimizing composite objectives. arXiv
preprint arXiv:1605.08003, 2016.

Peng Xu, Jiyan Yang, Farbod Roosta-Khorasani, Christo-
pher R´e, and Michael W Mahoney. Sub-sampled new-
ton methods with non-uniform sampling. arXiv preprint
arXiv:1607.00559, 2016.

ACKNOWLEDGMENTS

This research is supported in part by an FP7 Marie Curie
CIG grant and an Israel Science Foundation grant 425/13.

References

Alekh Agarwal and Leon Bottou.

for the optimization of ﬁnite sums.
arXiv:1410.0723, 2014.

A lower bound
arXiv preprint

Naman Agarwal, Brian Bullins, and Elad Hazan. Sec-
ond order stochastic optimization in linear time. arXiv
preprint arXiv:1602.03943, 2016.

Yossi Arjevani and Ohad Shamir. Communication com-
plexity of distributed convex learning and optimization.
In Advances in Neural Information Processing Systems,
pages 1756–1764, 2015.

Yossi Arjevani and Ohad Shamir. Dimension-free iteration
complexity of ﬁnite sum optimization problems. In Ad-
vances in Neural Information Processing Systems, pages
3540–3548, 2016a.

Yossi Arjevani and Ohad Shamir. On the iteration com-
plexity of oblivious ﬁrst-order optimization algorithms.
In Proceedings of the 33nd International Conference on
Machine Learning, pages 908–916, 2016b.

Raghu Bollapragada, Richard Byrd, and Jorge Nocedal.
Exact and inexact subsampled newton methods for op-
timization. arXiv preprint arXiv:1609.08502, 2016.

Stephen Boyd and Lieven Vandenberghe. Convex optimiza-

tion. Cambridge university press, 2004.

Murat A Erdogdu and Andrea Montanari. Convergence
rates of sub-sampled newton methods. In Advances in
Neural Information Processing Systems, pages 3052–
3060, 2015.

Guanghui Lan. An optimal randomized incremental gradi-
ent method. arXiv preprint arXiv:1507.02000, 2015.

A. Nemirovsky and D. Yudin. Problem Complexity and
Method Efﬁciency in Optimization. Wiley-Interscience,
1983.

Yurii Nesterov. Introductory lectures on convex optimiza-
tion: A basic course, volume 87. Springer Science &
Business Media, 2013.

Jorge Nocedal and Stephen Wright. Numerical optimiza-

tion. Springer Science & Business Media, 2006.

Mert Pilanci and Martin J Wainwright. Newton sketch: A
linear-time optimization algorithm with linear-quadratic
convergence. arXiv preprint arXiv:1505.02250, 2015.

