Coherence Pursuit: Fast, Simple, and Robust Subspace Recovery

Mostafa Rahmani 1 George Atia 1

Abstract

dimensional subspace by solving

A remarkably simple, yet powerful, algorithm
termed Coherence Pursuit for robust Principal
Component Analysis (PCA) is presented. In the
proposed approach, an outlier is set apart from an
inlier by comparing their coherence with the rest
of the data points. As inliers lie in a low dimen-
sional subspace, they are likely to have strong
mutual coherence provided there are enough in-
liers. By contrast, outliers do not typically admit
low dimensional structures, wherefore an out-
lier is unlikely to bear strong resemblance with
a large number of data points. As Coherence
Pursuit only involves one simple matrix multi-
plication, it is signiﬁcantly faster than the state-
of-the-art robust PCA algorithms. We provide a
mathematical analysis of the proposed algorithm
under a random model for the distribution of the
inliers and outliers. It is shown that the proposed
method can recover the correct subspace even if
the data is predominantly outliers. To the best
of our knowledge, this is the ﬁrst provable ro-
bust PCA algorithm that is simultaneously non-
iterative, can tolerate a large number of outliers
and is robust to linearly dependent outliers.

1. Introduction

Standard tools such as Principal Component Analysis
(PCA) have been instrumental in reducing dimensional-
ity by ﬁnding linear projections of high-dimensional data
along the directions where the data is most spread out to
minimize information loss. These techniques are widely
applicable in a broad range of data analysis problems, in-
cluding computer vision, image processing, machine learn-
ing and bioinformatics (Basri & Jacobs, 2003; Costeira &
Kanade, 1998; Hosseini et al., 2014).
Given a data matrix D ∈ Rm×n, PCA ﬁnds an r-

1University of Central Florida, Orlando, Florida, USA. Corre-

spondence to: Mostafa Rahmani <mostafa@knights.ucf.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

(cid:107)D − ˆU ˆUT D(cid:107)F

subject to

ˆUT ˆU = I,

(1)

min
ˆU

where ˆU ∈ Rm×r is an orthonormal basis for the r-
dimensional subspace, I denotes the identity matrix and
(cid:107).(cid:107)F the Frobenius norm. Despite its notable impact on
exploratory data analysis and multivariate analyses, PCA
is notoriously sensitive to outliers that prevail much of the
real world data since the solution to (1) can arbitrarily de-
viate from the true subspace in presence of a small number
of outlying data points that do not conform with the low di-
mensional model (Hauberg et al., 2016; Lerman & Zhang,
2014; Maronna, 2005; McCoy et al., 2011; Xu et al., 2010a;
Zhang, 2012) .

As a result, much research work was devoted to investigate
PCA algorithms that are robust to outliers. The corrupted
data can be expressed as

D = L + C ,

(2)

where L is a low rank matrix whose columns span a
low-dimensional subspace, and the matrix C models the
data corruption, and is referred to as the outlier matrix.
There are two main models for the outlier matrix that
were considered in the literature, and these two models
are mostly incomparable in theory, practice and analysis
techniques. The ﬁrst corruption model is the element-wise
model in which C is a sparse matrix with arbitrary sup-
port, whose entries can have arbitrarily large magnitudes
(Chandrasekaran et al., 2011; Cand`es et al., 2011; Netra-
palli et al., 2014; Yi et al., 2016). In view of the arbitrary
support of C, any of the columns of L may be affected by
the non-zero elements of C. We do not consider this model
in this paper. The second model, which is the focus of our
paper, is a column-wise model wherein only a fraction of
the columns of C are non-zero, wherefore a portion of the
columns of L (the so-called inliers) remain unaffected by
C (Lerman & Maunu, 2014; Xu et al., 2010b; Chen et al.,
2011; Rahmani & Atia, 2017). Next, we formally describe
the data model adopted in this paper, which only focuses
on the column-wise outlier model.

Data Model 1. The given data matrix D satisﬁes the fol-
lowing.

1. The matrix D can be expressed as

2. Related Work

Submission and Formatting Instructions for ICML 2017

D = L + C = [A B] T ,

(3)

where A ∈ Rm×n1, B ∈ Rm×n2 , and T is an arbitrary
permutation matrix.
2. The columns of A lie in an r-dimensional subspace U,
namely, the column space of L. The columns of B do not
lie entirely in U, i.e., the columns of A are the inliers and
the columns of B are the outliers.

The column-wise model for robust PCA has direct bearing
on a host of applications in signal processing and machine
learning, which spurred enormous progress in dealing with
subspace recovery in the presence of outliers. This work is
motivated by some of the limitations of existing techniques,
which we further detail in Section 2 on related work. The
vast majority of existing approaches to robust PCA have
high computational complexity, which makes them unsuit-
able in high dimensional settings. For instance, many of the
existing iterative techniques incur a long run time as they
require a large number of iterations, each with a Singular
Value Decomposition (SVD) operation. Also, most itera-
tive solvers have no provable guarantees for exact subspace
recovery. Moreover, many of the existing robust PCA al-
gorithms cannot tolerate a large number of outliers as they
rely on sparse outlier models, while others cannot handle
linearly dependent outliers. In this paper, we present a new
provable non-iterative robust PCA algorithm, dubbed Co-
herence Pursuit (CoP), which involves one simple matrix
multiplication, and thereby achieves remarkable speedups
over the state-of-the-art algorithms. CoP can tolerate a
large number of outliers – even if the ratio of inliers to out-
liers n1
approaches zero – and is robust to linearly depen-
n2
dent outliers and to additive noise.

1.1. Notation and deﬁnitions

Bold-face upper-case letters are used to denote matrices
and bold-face lower-case letters are used to denote vectors.
Given a matrix A, (cid:107)A(cid:107) denotes its spectral norm and (cid:107)A(cid:107)∗
its nuclear norm. For a vector a, (cid:107)a(cid:107)p denotes its (cid:96)p-norm
and a(i) its ith element. Given two matrices A1 and A2
with an equal number of rows, the matrix

A3 = [A1 A2]

is the matrix formed by concatenating their columns. For a
matrix A, ai denotes its ith column, and A−i is equal to A
with the ith column removed. The function orth(·) returns
an orthonormal basis for the range of its matrix argument.
Deﬁnition 1. The mutual coherence of two non-zero vec-
tors v1 ∈ Rm×1 and v2 ∈ Rm×1 is deﬁned as

|vT

1 v2|
(cid:107)v1(cid:107)(cid:107)v2(cid:107)

.

Some of the earliest approaches to robust PCA relied on
robust estimation of the data covariance matrix, such as S-
estimators, the minimum covariance determinant, the min-
imum volume ellipsoid, and the Stahel-Donoho estimator
(Huber, 2011). This is a class of iterative approaches that
compute a full SVD or eigenvalue decomposition in each
iteration and generally have no explicit performance guar-
antees. The performance of these approaches greatly de-
grades for n1
n2

≤ 0.5.

To enhance robustness to outliers, another approach is to
replace the Frobenius norm in (1) with other norms (Ler-
man et al., 2011). For example, (Ke & Kanade, 2005) uses
an (cid:96)1-norm relaxation commonly used for sparse vector es-
timation, yielding robustness to outliers (Candes & Tao,
2005; Cand`es et al., 2006; 2011). However, the approach
presented in (Ke & Kanade, 2005) has no provable guaran-
tees and requires C to be column sparse, i.e., a very small
portion of the columns of C can be non-zero. The work in
(Ding et al., 2006) replaces the (cid:96)1-norm in (Ke & Kanade,
2005) with the (cid:96)1,2-norm. While the algorithm in (Ding
et al., 2006) can handle a large number of outliers, the com-
plexity of each iteration is O(nm2) and its iterative solver
has no performance guarantees. Recently, the idea of using
a robust norm was revisited in (Lerman et al., 2012; Zhang
& Lerman, 2014). Therein, the non-convex constraint set is
relaxed to a larger convex set and exact subspace recovery
is guaranteed under certain conditions. The algorithm pre-
sented in (Lerman et al., 2012) obtains the column space
of L and (Zhang & Lerman, 2014) ﬁnds its complement.
However, the iterative solver of (Lerman et al., 2012) com-
putes a full SVD of an m × m weighted covariance ma-
trix in each iteration. Thus, the overall complexity of the
solver of (Lerman et al., 2012) is roughly O(m3 + nm2)
per iteration, where the second term is the complexity of
computing the weighted covariance matrix. Similarly, the
solver of (Zhang & Lerman, 2014) has O(nm2 +m3) com-
plexity per iteration. In (Tsakiris & Vidal, 2015), the com-
plement of the column space of L is recovered via a series
of linear optimization problems, each obtaining one direc-
tion in the complement space. This method is sensitive to
linearly dependent outliers and requires the columns of L
not to exhibit a clustering structure, which in fact prevails
much of the real world data. Also, the approach presented
in (Tsakiris & Vidal, 2015) requires solving m−r linear op-
timization problems consecutively resulting in high compu-
tational complexity and long run time for high dimensional
data.

Robust PCA using convex rank minimization was ﬁrst ana-
lyzed in (Chandrasekaran et al., 2011; Cand`es et al., 2011)
for the element-wise corruption model.
In (Xu et al.,
2010b), the algorithm analyzed in (Chandrasekaran et al.,

Submission and Formatting Instructions for ICML 2017

2011; Cand`es et al., 2011) was extended to the column-
wise corruption model where it was shown that the optimal
point of

min
ˆL, ˆC

(cid:107)ˆL(cid:107)∗ + λ(cid:107) ˆC(cid:107)1,2

subject to

ˆL + ˆC = D

(4)

yields the exact subspace and correctly identiﬁes the out-
liers provided that C is sufﬁciently column-sparse. The
solver of (4) requires too many iterations, each computing
the SVD of an m × n dimensional matrix. Also, the al-
gorithm can only tolerate a small number of outliers – the
ratio n2
n1

should be roughly less than 0.05.

A different approach to outlier detection was proposed
in (Soltanolkotabi & Candes, 2012; Elhamifar & Vidal,
2013), the idea being that outliers do not typically follow
low dimensional structures, thereupon few outliers cannot
form a linearly dependent set. While this approach can re-
cover the correct subspace even if a remarkable portion of
the data is outliers, it cannot detect linearly dependent out-
liers and has O(n3) complexity per iteration (Elhamifar &
Vidal, 2013). In the outlier detection algorithm presented
in (Heckel & B¨olcskei, 2013), a data point is identiﬁed as
an outlier if the maximum value of its mutual coherences
with the other data points falls below a predeﬁned thresh-
old. However, this approach is unable to detect outliers that
lie in close neighborhoods. For instance, any repeated out-
liers will be falsely detected as inliers since their mutual
coherence is 1.

2.1. Motivation and summary of contributions

This work is motivated by the limitations of prior work on
robust PCA as summarized below.
Complex iterations. Most of the state-of-the-art robust
PCA algorithms require a large number of iterations each
with high computational complexity. For instance, many of
these algorithms require the computation of the SVD of an
m×n, or m×m, or n×n matrix in each iteration (Hardt &
Moitra, 2012; Xu et al., 2010b; Lerman et al., 2012), which
leads to long run time.

Guarantees. While the optimal points of the optimization
problems underlying many of the existing robust subspace
recovery techniques yield the exact subspace, there are no
such guarantees for their corresponding iterative solvers.
Examples include the optimal points of the optimization
problems presented in (Xu et al., 2010b; Ding et al., 2006).

Robustness issues. Most existing algorithms are tailored
to one speciﬁc class of outlier models. For example, algo-
rithms based on sparse outlier models utilize sparsity pro-
moting norms, thus can only handle a small number of out-
liers. On the other hand, algorithms such as (Heckel &
B¨olcskei, 2013; Soltanolkotabi & Candes, 2012) can han-

dle a large number of outliers, albeit they fail to locate
outliers with high similarity or linearly dependent outliers.
Spherical PCA (SPCA) is a non-iterative robust PCA al-
gorithm that is also scalable (Maronna et al., 2006).
In
this algorithm, all the columns of D are ﬁrst projected
onto the unit sphere Sm−1, then the subspace is identiﬁed
as the span of the principal directions of the normalized
data. However, in the presence of outliers, the recovered
subspace is never equal to the true subspace and it signiﬁ-
cantly deviates from the underlying subspace when outliers
abound.

To the best of our knowledge, CoP is the ﬁrst algorithm
that addresses these concerns all at once. In the proposed
method, we distinguish outliers from inliers by comparing
their degree of coherence with the rest of the data. The ad-
vantages of the proposed algorithm are summarized below.

• Coherence Pursuit (CoP) is a considerably simple
non-iterative algorithm which roughly involves one
matrix multiplication to compute the Gram matrix. It
also has provable performance guarantees (c.f. Sec-
tion 4).

• CoP can tolerate a large number of outliers.

It is
shown that exact subspace recovery is guaranteed with
high probability even if n1
goes to zero provided that
n2
n1
m
r is sufﬁciently large.
n2

• CoP is robust to linearly dependent outliers since it
measures the coherence of a data point with respect to
all the other data points.

Algorithm 1 CoP: Proposed Robust PCA Algorithm
Initialization: Set p = 1 or p = 2.
1. Data Normalization: Deﬁne matrix X ∈ Rm×n as
xi = di/(cid:107)di(cid:107)2.
2. Mutual Coherence Measurement
2.1 Deﬁne G = XT X and set its diagonal elements to
zero.
2.2 Deﬁne vector p ∈ Rn×1 as p(i) = (cid:107)gi(cid:107)p, i =
1, . . . , n.
3. Subspace Identiﬁcation: Construct matrix Y from the
columns of X corresponding to the largest elements of p
such that they span an r-dimensional subspace.
Output: The columns of Y are a basis for the column
space of L.

3. Proposed Method

In this section, we present the Coherence Pursuit algorithm
and provide some insight into its characteristics. The main
theoretical results are provided in Section 4. The table of

Submission and Formatting Instructions for ICML 2017

Figure 1. The values of vector p for different values of p and n1
n2

.

Algorithm 1 presents the proposed method along with the
deﬁnitions of the used symbols.

Figure 2. The elements of vector p with different values of pa-
rameter τ .

Coherence: The inliers lie in a low dimensional subspace
U. In addition, for most practical purposes the inliers are
highly coherent with each other in the sense of having large
values of the mutual coherence in Deﬁnition 1. By contrast,
the outlying columns do not typically follow low dimen-
sional structures and do not bear strong resemblance with
the rest of the data. As such, in CoP all mutual coherences
between the columns of D are ﬁrst computed. Then, the
column space of A is obtained as the span of those columns
that have strong mutual coherence with the rest of the data.

For instance, assume that the distributions of the inliers and
the outliers follow the following assumption.

Assumption 1. The subspace U is a random r-dimensional
subspace in Rm. The columns of A are drawn uniformly
at random from the intersection of Sm−1 and U. The
columns of B are drawn uniformly at random from Sm−1.
To simplify the exposition and notation, it is assumed that
T in (3) is equal to the identity matrix without any loss of
generality, i.e, D = [A B].

In addition, suppose that v1 and v2 are two inliers and u1
and u1 are two outliers. It can be shown that

E(vT

1 v2)2 = 1/r

while

in which m = 400, r = 5, n1 = 50, and the distributions
of inliers and outliers follow Assumption 1. Fig. 1 shows
the vector p (c.f. Algorithm 1) for different values of p and
n2. In all the ﬁgures, the maximum element is scaled to 1.
One can observe that even if n1/n2 = 0.01, the proposed
technique can recover the exact subspace since there is a
clear gap between the values of p corresponding to outliers
and inliers, more so for p = 2.

3.2. Robustness to noise

In the presence of additive noise, we model the data as

D = [A B] T + E ,

(5)

where E represents the noise component.

The high coherence between the inliers (columns of A) and
the low coherence of the outliers (columns of B) with each
other and with the inliers result in the large gap between
the elements of p observed in Fig. 1 even when n1/n2 <
0.01. This gap gives the algorithm tolerance to high levels
of noise. For example, assume r = 5, n1 = 50, n2 =
500 and the distribution of the inliers and outliers follow
Assumption 1. Deﬁne the parameter τ as

τ =

E(cid:107)e(cid:107)2
E(cid:107)a(cid:107)2

(6)

E(uT

1 u2)2 = 1/m , E(uT

1 v1)2 = 1/m .

Accordingly, if m (cid:29) r, the inliers exhibit much stronger
mutual coherences.

3.1. Large number of outliers

Unlike most robust PCA algorithms which require n2 to
be much smaller than n1, the proposed method tolerates a
large number of outliers. For instance, consider a setting

where a and e are arbitrary columns of A and E, respec-
tively. Fig. 2 shows the entries of p for different values
of τ . As shown, the elements corresponding to inliers are
clearly separated from the ones corresponding to outliers
even at very low signal to noise ratio, e.g. τ = 0.5 and
τ = 1. The mathematical analysis of CoP with noisy data
conﬁrms that the proposed method remains robust to high
levels of noise even with data that is predominantly outliers
(Rahmani & Atia, 2016).

Submission and Formatting Instructions for ICML 2017

Figure 3. The values of vector p when the 301th to 305th columns
are repeated outliers.

3.3. Linearly dependent outliers

Many of the existing robust PCA algorithms cannot de-
tect linearly dependent outliers (Hardt & Moitra, 2012;
Soltanolkotabi & Candes, 2012). For instance,
in the
outlier detection algorithm presented in (Soltanolkotabi &
Candes, 2012) a data column is identiﬁed as an outlier if
it does not admit a sparse representation in the rest of the
data. As such, if some outlying columns are repeated, the
algorithm in (Soltanolkotabi & Candes, 2012) fails to de-
tect them.

At a fundamental level,
the proposed approach affords
a more comprehensive deﬁnition of an outlying column,
namely, a data column is identiﬁed as an outlier if it has
weak total coherency with the rest of the data. This global
view of a data point w.r.t. the rest of the data allows the al-
gorithm to identify outliers that are linearly dependent with
few other data points.

For illustration, assume the given data follows Data model
1, r = 5, n1 = 50, n2 = 500, where the 301th to 305th
columns are repeated outliers. Fig. 3 shows the elements
of vector p with p = 1 and p = 2. All the elements cor-
responding to inliers are clearly greater than the elements
corresponding to outliers and the algorithm correctly iden-
tiﬁes the subspace. Fig. 3 suggests that CoP with p = 1 is
better at handling repeated outliers since the entries of G
with large absolute magnitudes are more gracefully ampli-
ﬁed by the (cid:96)1-norm.

3.4. Subspace identiﬁcation

In the third step of Algorithm 1, we sample the columns
of X with the largest coherence values which span an r-
dimensional space. In this section, we present some tips
for efﬁcient implementation of this step. One way is to
start sampling the columns with the highest coherence val-
ues and stop when the rank of the sampled columns is equal
to r. However, if the columns of L admit a clustering
structure and their distribution is highly non-uniform, this
method will sample many redundant columns, which can
in turn increase the run time. In this section, we propose
two low-complexity techniques to accelerate the subspace
identiﬁcation step.

1. In many applications, we may have an upper bound on
n2/n. For instance, suppose we know that up to 40 percent
of the data could be outliers. In this case, we simply remove
40 percent of the columns corresponding to the smallest
values of p and obtain the subspace using the remaining
data points.

2. The second technique is an adaptive sampling method
presented in the table of Algorithm 2. First, the data is
projected onto a random kr-dimensional subspace to re-
duce the computational complexity for some integer k > 1.
According to the analysis presented in (Rahmani & Atia,
2017; Li & Haupt, 2015), even k = 2 is sufﬁcient to pre-
serve the rank of A and the structure of the outliers B, i.e.,
the rank of ΦA is equal to r and the columns of ΦB do
not lie in the column space of ΦA, where Φ is the projec-
tion matrix. The parameter ν that thresholds the (cid:96)2-norms
of the columns of the projected data is chosen based on
the noise level (if the data is noise free, ν = 0). In Algo-
rithm 2, the data is projected onto the span of the sampled
columns (step 2.3). Thus, a newly sampled column brings
innovation with respect to the previously sampled columns.
Therefore, redundant columns are not sampled.

Algorithm 2 Adaptive Column Sampling for the Subspace
Identiﬁcation Step (step 3) of CoP
Initialization: Set k equal to an integer greater than 1, a
threshold ν greater than or equal to 0, and F an empty ma-
trix.
1. Data Projection: Deﬁne Xφ ∈ Rkr×n as Xφ = ΦX,
where Φ ∈ Rkr×m projects the columns of X onto a ran-
dom kr-dimensional subspace.
2. Column Sampling
For i from 1 to r do
2.1 Set equal to zero the elements of p corresponding to
columns of Xφ with (cid:96)2-norms less than or equal to ν.
2.2 Deﬁne j := arg max

p(k), update F = orth

[F xj]

(cid:16)

(cid:17)

,

k

and set p(j) = 0.
2.3 Update Xφ = Xφ − FFT Xφ.
End For
Output Construct Y using the columns of X that corre-
spond to the columns that formed F.

Remark 1. Suppose we run Algorithm 2 h times (each
time the sampled columns are removed from the data and
newly sampled columns are added to Y). If the given data
is noisy, the ﬁrst r singular values of Y are the dominant
ones and the rest correspond to the noise component. If we
increase h, the span of the dominant singular vectors will
be closer to the column space of A. However, if h is chosen
unreasonably high, the sampler may also sample outliers.

Submission and Formatting Instructions for ICML 2017

3.5. Computational complexity

The main computational complexity is in the second step
of Algorithm 2 which is of order O(mn2). If we utilize
Algorithm 2 as the third step of Algorithm 1, the overall
complexity is of order O(mn2 + r3n). However, since the
algorithm roughly involves only one matrix multiplication,
it is very fast and very simple for hardware implementation
(c.f. Section 5.2 on run time). Moreover, if we utilize the
randomized designs presented in (Rahmani & Atia, 2017;
Li & Haupt, 2015), the overall complexity can be reduced
to O(r4).

4. Theoretical Investigations

In this section, we ﬁrst establish sufﬁcient conditions to en-
sure that the expected value of the elements of the vector
p corresponding to the inliers are much greater than the
elements corresponding to the outliers, in which case the
algorithm is highly likely to yield exact subspace recovery.
Subsequently, we provide two theorems establishing per-
formance guarantees for the proposed approach for p = 1
and p = 2.

The following lemmas establish sufﬁcient conditions for
the expected value of the elements of p corresponding to
inliers to be at least twice as large as those corresponding
to outliers. Due to space limitations, the proofs of all lem-
mas and theorems are deferred to an extended version of
this paper (Rahmani & Atia, 2016).
Lemma 1. Suppose Assumption 1 holds, the ith column is
an inlier and the (n1 + j)th column is an outlier. If

E (cid:107)gi(cid:107)1 > 2 E (cid:107)gn1+j(cid:107)1

recalling that gi is the ith column of matrix G.
Lemma 2. Suppose Assumption 1 holds, the ith column is
an inlier and the (n1 + j)th column is an outlier. If

n1
r

(1 −

) >

+

2r2
m

n2
m

1
r

(8)

then

then

E (cid:107)gi(cid:107)2

2 > 2 E (cid:107)gn1+j(cid:107)2
2 .

Remark 2. The sufﬁcient conditions provided in Lemma 1
and Lemma 2 reveal three important points.

I) The important performance factors are the ratios n1
r and
m . The intuition is that as n1
n2
r increases, the density of the
inliers in the subspace increases, and consequently their

mutual coherence also increases. Similarly, if n2
m increases,
the mutual coherence between the outliers increases. Thus,
the main requirement is that n1
r should be sufﬁciently larger
than n2
m .

II) In real applications, r (cid:28) m and n1 > n2, hence the
sufﬁcient conditions are easily satisﬁed. This fact is ob-
served in Fig. 1 which shows that Coherence Pursuit can
recover the correct subspace even if n1/n2 = 0.01.

III) In high dimensional settings, r (cid:28) m. Therefore, m√
m
could be much greater than r√
r . Accordingly, the con-
ditions in Lemma 1 are stronger than the conditions of
Lemma 2, suggesting that with p = 2 Coherence Pursuit
can tolerate a larger number of outliers than with p = 1.
This is conﬁrmed by comparing the plots in the last row of
Fig. 1.

The following theorems show that the same set of factors
are important to guarantee that the proposed algorithm re-
covers the exact subspace with high probability.
Theorem 3. If Assumption 1 is true and
√

(cid:115)

(cid:32)(cid:114) 2
π

n1√
r

r + 2
√

−

m

(cid:33)

βκ r

√

− 2

n1 −

2n1 log n1
δ
r − 1

(cid:115)

>

n2√
m

√

+ 2

n2 +

2n2 log n2
δ
m − 1

+

1
√
r

,

(9)

then Algorithm 1 with p = 1 recovers the exact sub-
space with probability at
least 1 − 3δ, where β =
max(8 log n2/δ, 8π) and κ = m
Theorem 4. If Assumption 1 is true and

m−1 .

m

4
3

,

(cid:33)

then Algorithm 1 with p = 2 recovers the correct subspace
with probability at least 1 − 4δ, where
(cid:32)

(cid:33)

(cid:114)

η1 = max

log

2rn1
δ

,

4

n1
r

log

2rn1
δ

(cid:32)

(cid:114)

,

,

4

(cid:16)

(cid:17)

4
3

log

log

m−1 .

η2 = max

2mn2
δ

2mn2
n2
δ
m
ζ = max(8π, 8 log n2
δ ), and κ = m
Remark 3. The dominant factors of the LHS and the RHS
(cid:113)
1 − r2
of (10) are n1
m log 2mn2
4 n2
, respectively.
m
r
As in Lemma 2, we see the factor n2
m , but under the square
root. Thus, the requirement of Theorem 4 is less stringent
than that of Lemma 2. This is because Theorem 4 guaran-
tees that the elements corresponding to inliers are greater
than those corresponding to outliers with high probability,
but does not guarantee a large gap between the values as
in Lemma 2.

and

δ

(cid:32)(cid:114) 2
π

n1√
r

−

(cid:114)

(cid:33)

4r2
m

(cid:114) 4
m

+

>

5 n2
√
m
4

+

(cid:114) 2
πr

,

(7)

(cid:18) 1
r

n1

−

r + 4ζκ + 4

ζrκ

(cid:19)

√

− η1 > 2η2 +

(10)

1
r

,

Submission and Formatting Instructions for ICML 2017

Algorithm 3 Subspace Clustering Error Correction
Method
Input: The matrices { ˆDi}L
i=1 represent the clustered data
(the output of a subspace clustering algorithm) and L is the
number of clusters.
Error Correction
For k from 1 to t do
1 Apply the robust PCA algorithm to the matrices { ˆDi}L
Deﬁne the orthonormal matrices { ˆUi}L
bases for the inliers of { ˆDi}L
2 Update the data clustering with respect to the obtained
bases { ˆUi}L
i=1 are updated), i.e.,
a data point d is assigned to the ith cluster if i =
arg max
k
End For
Output: The matrices { ˆDi}L
data and the matrices { ˆUi}L
for the identiﬁed subspaces.

i=1 represent the clustered
i=1 are the orthonormal bases

i=1.
i=1 as the learned

i=1 (the matrices { ˆDi}L

i=1, respectively.

(cid:107)xT ˆUk(cid:107)2.

5. Numerical Simulations

In this section, the performance of the proposed method is
investigated with both synthetic and real data. We com-
pare the performance of CoP with the state-of-the-art ro-
bust PCA algorithms including FMS (Lerman & Maunu,
2014), GMS (Zhang & Lerman, 2014), R1-PCA (Ding
et al., 2006), OP (Xu et al., 2010b), and SPCA (Maronna
et al., 2006).

5.1. Phase transition plot

Our analysis has shown that CoP yields exact subspace re-
covery with high probability if n1/r is sufﬁciently greater
than n2/m. In this experiment, we investigate the phase
transition of CoP in the n1/r and n2/m plane. Suppose
m = 100, r = 10, and the distributions of inliers/outliers
follow Assumption 1. Deﬁne U and ˆU as the exact and
recovered orthonormal bases for the span of inliers, respec-
tively. A trial is considered successful if

(cid:16)

(cid:107)U − ˆU ˆUT U(cid:107)F /(cid:107)U(cid:107)F

≤ 10−5 .

(cid:17)

In this simulation, we construct the matrix Y using 20
columns of X corresponding to the largest 20 elements of
the vector p. Fig. 4 shows the phase transition plot. White
indicates correct subspace recovery and black designates
incorrect recovery. As shown, if n2/m increases, we need
higher values of n1/r. However, one can observe that with
n1/r > 4, the algorithm can yield exact recovery even if
n2/m > 30.

Figure 4. The phase transition plot of CoP versus n1/r and
n2/m.

5.2. Running time

In this section, we compare the run time of CoP to the ex-
isting approaches. Table 1 presents the run time in seconds
for different data sizes. In all experiments, n1 = n/5 and
n2 = 4n/5. One can observe that CoP is remarkably faster
given its simplicity (single step algorithm).

Table 1. Running time of the algorithms

m = n CoP
0.02
1000
0.7
2000
5.6
5000
27
10000

FMS
1
5.6
60
401

OP
45
133
811
3547

R1-PCA
1.45
10.3
83.3
598

5.3. Subspace recovery in presence of outliers

In this experiment, we assess the robustness of CoP to out-
liers in comparison to existing approaches. It is assumed
that m = 50, r = 10, n1 = 50 and the distribution of
inliers/outliers follows Assumption 1. Deﬁne U and ˆU as
before, and the recovery error as

Log-Recovery Error = log10

(cid:16)

(cid:107)U − ˆU ˆUT U(cid:107)F /(cid:107)U(cid:107)F

(cid:17)

.

In this simulation, we use 30 columns to form the matrix
Y. Fig. 5 shows the recovery error versus n2/n1 for dif-
ferent values of n2. In addition to its remarkable simplicity,
CoP gives the highest accuracy and yields exact subspace
recovery even if the data is overwhelmingly outliers.

5.4. Clustering error correction

In this section, we consider a very challenging robust sub-
space recovery problem with real data. We use the robust
PCA algorithm as a subroutine for error correction in a sub-
space clustering problem. In this experiment, the outliers

5101520253035n2 / m246810n1 / rSubmission and Formatting Instructions for ICML 2017

Figure 5. The subspace recovery error versus n2/n1.

can be close to the inliers and can also be linearly depen-
dent.

i=1 lie in the linear subspaces {Si}L

The subspace clustering problem is a general form of PCA
in which the data points lie in a union of an unknown num-
ber of unknown linear subspaces (Vidal, 2011; Rahmani &
Atia, 2015). A subspace clustering algorithm identiﬁes the
subspaces and clusters the data points with respect to the
subspaces. The performance of the subspace clustering al-
gorithms – especially the ones with scalable computational
complexity – degrades in presence of noise or when the
subspaces are closer to each other. Without loss of gener-
ality, suppose D = [D1 ... DL] is the given data where the
columns of {Di}L
i=1,
respectively, and L is the number of subspaces. Deﬁne
{ ˆDi}L
i=1 as the output of some clustering algorithm (the
clustered data). Deﬁne the clustering error as the ratio of
misclassiﬁed points to the total number of data points. With
the clustering error, some of the columns of ˆDi believed to
lie in Si may actually belong to some other subspace. Such
columns can be viewed as outliers in the matrix ˆDi. Ac-
cordingly, the robust PCA algorithm can be utilized to cor-
rect the clustering error. We present Algorithm 3 as an error
correction algorithm which can be applied to the output of
any subspace clustering algorithm to reduce the clustering
error. In each iteration, Algorithm 3 applies the robust PCA
algorithm to the clustered data to obtain a set of bases for
the subspaces. Subsequently, the obtained clustering is up-
dated based on the obtained bases.

In this experiment, we imagine a subspace clustering algo-
rithm with 20 percent clustering error and apply Algorithm
3 to the output of the algorithm to correct the errors. We use
the Hopkins155 dataset which contains video sequences of
2 or 3 motions (Tron & Vidal, 2007). The data is generated
by extracting and tracking a set of feature points through

Figure 6. The clustering error after each iteration of Algorithm 2.

In motion segmentation, each motion corre-
the frames.
sponds to one subspace. Thus, the problem here is to clus-
ter data lying in two or three subspaces (Vidal, 2011). We
use the trafﬁc data sequences, which include 8 scenarios
with two motions and 8 scenarios with three motions.

When CoP is applied, 50 percent of the columns of X are
used to form the matrix Y. Fig. 6 shows the average clus-
tering error (over all trafﬁc data matrices) after each iter-
ation of Algorithm 3 for different robust PCA algorithms.
CoP clearly outperforms the other approaches. As a matter
of fact, most of the robust PCA algorithms fail to obtain
the correct subspaces and end up increasing the clustering
error.

Acknowledgment
This work was supported by NSF CAREER Award CCF-
1552497 and NSF Grant CCF-1320547.

References

Basri, Ronen and Jacobs, David W. Lambertian reﬂectance
and linear subspaces. Pattern Analysis and Machine In-
telligence, IEEE Transactions on, 25(2):218–233, 2003.

Candes, Emmanuel J and Tao, Terence. Decoding by linear
programming. Information Theory, IEEE Transactions
on, 51(12):4203–4215, 2005.

Cand`es, Emmanuel J, Romberg, Justin, and Tao, Terence.
Robust uncertainty principles: Exact signal reconstruc-
tion from highly incomplete frequency information. In-
formation Theory, IEEE Transactions on, 52(2):489–
509, 2006.

Cand`es, Emmanuel J, Li, Xiaodong, Ma, Yi, and Wright,
John. Robust principal component analysis? Journal of
the ACM (JACM), 58(3):11, 2011.

Chandrasekaran, Venkat, Sanghavi, Sujay, Parrilo,

5101520n2 / n1-15-10-50log-Recovery ErrorCoPFMSR1-PCAGMSSPCAOP02468Iteration Number00.050.10.150.20.250.3Clustering ErrorCoPFMSR1-PCAGMSSPCASubmission and Formatting Instructions for ICML 2017

Pablo A, and Willsky, Alan S. Rank-sparsity inco-
herence for matrix decomposition. SIAM Journal on
Optimization, 21(2):572–596, 2011.

Chen, Yudong, Xu, Huan, Caramanis, Constantine, and
Sanghavi, Sujay. Robust matrix completion with cor-
rupted columns. arXiv preprint arXiv:1102.2254, 2011.

Costeira, Jo˜ao Paulo and Kanade, Takeo. A multibody fac-
torization method for independently moving objects. In-
ternational Journal of Computer Vision, 29(3):159–179,
1998.

Ding, Chris, Zhou, Ding, He, Xiaofeng, and Zha,
Hongyuan. R1-PCA: rotational invariant l 1-norm prin-
cipal component analysis for robust subspace factoriza-
tion. In Proceedings of the 23rd international conference
on Machine learning, pp. 281–288. ACM, 2006.

Elhamifar, Ehsan and Vidal, Rene. Sparse subspace cluster-
ing: Algorithm, theory, and applications. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on, 35
(11):2765–2781, 2013.

Hardt, Moritz and Moitra, Ankur. Algorithms and hard-
arXiv preprint

ness for robust subspace recovery.
arXiv:1211.1041, 2012.

Hauberg, Soren, Feragen, Aasa, Enﬁciaud, Rafﬁ, and
Black, Michael. Scalable robust principal component
analysis using grassmann averages. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 38(11):
2298–2311, Nov 2016.

Heckel, Reinhard and B¨olcskei, Helmut. Robust sub-
arXiv preprint

space clustering via thresholding.
arXiv:1307.4891, 2013.

Hosseini, Mohammad-Parsa, Nazem-Zadeh, Moham-
mad R, Mahmoudi, Fariborz, Ying, Hao, and Soltanian-
Zadeh, Hamid. Support vector machine with nonlinear-
kernel optimization for lateralization of epileptogenic
In 2014 36th Annual
hippocampus in mr images.
International Conference of the IEEE Engineering in
Medicine and Biology Society, pp. 1047–1050. IEEE,
2014.

Huber, Peter J. Robust statistics. Springer, 2011.

Ke, Qifa and Kanade, Takeo. Robust l 1 norm factorization
in the presence of outliers and missing data by alternative
convex programming. In Computer Vision and Pattern
Recognition, 2005. CVPR 2005. IEEE Computer Society
Conference on, volume 1, pp. 739–746. IEEE, 2005.

Lerman, Gilad and Maunu, Tyler.
and non-convex subspace recovery.
arXiv:1406.6145, 2014.

Fast,

robust
arXiv preprint

Lerman, Gilad and Zhang, Teng. {l p}-recovery of the
most signiﬁcant subspace among multiple subspaces
with outliers. Constructive Approximation, 40(3):329–
385, 2014.

Lerman, Gilad, Zhang, Teng, et al. Robust recovery of
multiple subspaces by geometric lp minimization. The
Annals of Statistics, 39(5):2686–2715, 2011.

Lerman, Gilad, McCoy, Michael, Tropp, Joel A, and
Zhang, Teng. Robust computation of linear models, or
how to ﬁnd a needle in a haystack. Technical report,
DTIC Document, 2012.

Li, Xingguo and Haupt, Jarvis.

Identifying outliers in
large matrices via randomized adaptive compressive
sampling. Signal Processing, IEEE Transactions on, 63
(7):1792–1807, 2015.

Maronna, RARD, Martin, Douglas, and Yohai, Victor. Ro-
bust statistics. John Wiley & Sons, Chichester. ISBN,
2006.

Maronna, Ricardo. Principal components and orthogonal
regression based on robust scales. Technometrics, 47(3):
264–273, 2005.

McCoy, Michael, Tropp, Joel A, et al. Two proposals for
robust pca using semideﬁnite programming. Electronic
Journal of Statistics, 5:1123–1160, 2011.

Netrapalli, Praneeth, Niranjan, UN, Sanghavi, Sujay,
Anandkumar, Animashree, and Jain, Prateek. Non-
convex robust pca. In Advances in Neural Information
Processing Systems, pp. 1107–1115, 2014.

Rahmani, Mostafa and Atia, George. Innovation pursuit:
A new approach to subspace clustering. arXiv preprint
arXiv:1512.00907, 2015.

Rahmani, Mostafa and Atia, George. Coherence pursuit:
Fast, simple, and robust principal component analysis.
arXiv preprint arXiv:1609.04789, 2016.

Rahmani, Mostafa and Atia, George. Randomized robust
subspace recovery and outlier detection for high dimen-
sional data matrices. IEEE Transactions on Signal Pro-
cessing, 65(6):1580–1594, March 2017.

Soltanolkotabi, Mahdi and Candes, Emmanuel J. A geo-
metric analysis of subspace clustering with outliers. The
Annals of Statistics, pp. 2195–2238, 2012.

Tron, Roberto and Vidal, Ren´e. A benchmark for the com-
parison of 3-d motion segmentation algorithms. In Com-
puter Vision and Pattern Recognition, 2007. CVPR’07.
IEEE Conference on, pp. 1–8. IEEE, 2007.

Submission and Formatting Instructions for ICML 2017

Tsakiris, Manolis C and Vidal, Ren´e. Dual principal com-
ponent pursuit. In Proceedings of the IEEE International
Conference on Computer Vision Workshops, pp. 10–18,
2015.

Vidal, Rene. Subspace clustering. IEEE Signal Processing

Magazine, 2(28):52–68, 2011.

Xu, Huan, Caramanis, Constantine, and Mannor, Shie.
contaminated
arXiv preprint

Principal
data: The high dimensional case.
arXiv:1002.4658, 2010a.

analysis with

component

Xu, Huan, Caramanis, Constantine, and Sanghavi, Sujay.
In Advances in Neural
Robust pca via outlier pursuit.
Information Processing Systems, pp. 2496–2504, 2010b.

Yi, Xinyang, Park, Dohyung, Chen, Yudong, and Carama-
nis, Constantine. Fast algorithms for robust pca via gra-
dient descent. arXiv preprint arXiv:1605.07784, 2016.

Zhang, Teng. Robust subspace recovery by geodesically
convex optimization. arXiv preprint arXiv:1206.1386,
2012.

Zhang, Teng and Lerman, Gilad. A novel m-estimator for
robust pca. The Journal of Machine Learning Research,
15(1):749–808, 2014.

