Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

The supplementary materials is organized as follows. In Appendices A, B and C, we provide detailed proofs of the theoretical
results in the paper. In Appendix D, we provide additional ﬁgures for the experiments described in Section 5.

A. Proof of Theorem 1

In this appendix we prove the minimax bound of Theorem 1. The result is obtained by combining the following two lower
bounds:
Theorem 3 (Lower bound 1). For each problem instance such that Eµ[ρ2σ2] < ∞, we have

Rn(π; λ, µ, σ, Rmax) ≥

Eµ[ρ2σ2]
32en



1 −

(cid:104)
ρ2σ21

Eµ

(cid:16)
ρσ2 > Rmax
Eµ[ρ2σ2]

(cid:112)nEµ[ρ2σ2]/2

(cid:17)(cid:105)

2




.

Theorem 4 (Lower bound 2). Assume that Eµ[ρ2R2
and γ(cid:48) := max{γ, δ}. Then there exist functions ˆR(x, a) and ˆρ(x, a) such that

max] < ∞, and we are given γ ∈ [0, 1] and δ ∈ (0, 1]. Write ξ := ξγ

ˆR2(x, a) ≤ R2

max(x, a) ≤ (1 + δ) ˆR2(x, a) ,

ˆρ2(x, a) ≤ ρ2(x, a) ≤ (1 + δ)ˆρ2(x, a)

and the following lower bound holds:

Rn(π; λ, µ, σ, Rmax)

≥

Eµ[ξ ˆρ2 ˆR2]
32en







1 −

(cid:20)
(cid:16)
ξ ˆρ2 ˆR21

Eµ

(cid:113)

ξ ˆρ ˆR >

nEµ[ξ ˆρ2 ˆR2]/16

Eµ[ξ ˆρ2 ˆR2]

(cid:17)(cid:21)

2






− γ(cid:48) log(cid:0)5/γ(cid:48)(cid:1)(1 + δ)Eµ[ξ ˆρ2 ˆR2] .

The reason for introducing γ(cid:48) in Theorem 4 is to allow γ = 0, which is an important special case of the theorem. Otherwise,
we could just assume 0 < δ ≤ γ. The ﬁrst bound captures the intrinsic difﬁculty due to the variance of reward, and is
present even in a vanilla multi-armed bandit problem without contexts. The second result shows the additional dependence
on R2
max, even when σ ≡ 0, whenever the distribution λ is not too degenerate, and captures the additional difﬁculty of the
contextual bandit problem. We next show how these two lower bounds yield Theorem 1 and then return to their proofs.

Proof of Theorem 1. Throughout the theorem we write ξ := ξγ. We begin by simplifying the two lower bounds. Assume
that Assumption 1 holds with (cid:15). This also means that Eµ[ξ(ρRmax)2+(cid:15)] is ﬁnite as well as Eµ[ξ(ρRmax)2] is ﬁnite and
either both of them are zero or both of them are non-zero. Similarly, Eµ[(ρσ)2+(cid:15)] and Eµ[(ρσ)2] are both ﬁnite and either
both of them are zero or both of them are non-zero, so Cγ is a ﬁnite constant. Let p = 1 + (cid:15)/2 and q = 1 + 2/(cid:15), i.e.,
1/p + 1/q = 1. Further, let ˆR and ˆρ be the functions from Theorem 4. Then the deﬁnition of Cγ means that

C 1/((cid:15)q)
γ

= C 1/(2+(cid:15))
γ

= 2 · max

(cid:2)ξ(ρ2R2
Eµ

max)
(cid:2)ξρ2R2

2+(cid:15)

2+(cid:15)

2 (cid:3) 2
(cid:3)

,

max

Eµ

2+(cid:15)

2 (cid:3) 2

2+(cid:15)

(cid:2)(ρ2σ2)
Eµ

(cid:2)ρ2σ2(cid:3)









Eµ


(cid:40) Eµ

= 2 · max

≥ 2 · max

,

max)p(cid:3)1/p
(cid:3)

(cid:2)ξ(ρ2R2
(cid:2)ξρ2R2
Eµ
(cid:40) Eµ
(cid:2)ξ(ˆρ2 ˆR2)p(cid:3)1/p
(cid:3)
Eµ

(cid:2)ξρ2R2

max

,

max

Eµ

Eµ

(cid:2)(ρ2σ2)p(cid:3)1/p
(cid:2)ρ2σ2(cid:3)
Eµ
(cid:2)(ρ2σ2)p(cid:3)1/p
(cid:2)ρ2σ2(cid:3)
Eµ

(cid:41)

(cid:41)

,

and recall that we assume that

(cid:110)

n ≥ max

16C 1/(cid:15)
γ

, 2C 2/(cid:15)
γ

Eµ[σ2/R2

max]

(cid:111)

.

(10)

(11)

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

First, we simplify the correction term in the lower bound of Theorem 3. Using Hölder’s inequality and Eq. (10), we have

(cid:20)
ρ2σ21

(cid:18)

Eµ

ρσ2 > Rmax

nEµ[ρ2σ2]/2

(cid:113)

(cid:104)

(cid:19)(cid:21)

(cid:113)

≤ Eµ

(cid:104)(cid:0)ρ2σ2(cid:1)p(cid:105)1/p

· Pµ

ρσ2 > Rmax

nEµ[ρ2σ2]/2

(cid:105)1/q

≤

Eµ[ρ2σ2] · C 1/((cid:15)q)

· Pµ

γ

(cid:104)
ρσ2/Rmax >

(cid:113)

nEµ[ρ2σ2]/2

(cid:105)1/q

.

We further invoke Markov’s inequality, Cauchy-Schwartz inequality, and Eq. (11) in the following three steps to simplify
this event as

≤

Eµ[ρ2σ2] · C 1/((cid:15)q)

·

γ

(cid:32) Eµ

(cid:2)ρσ · (σ/Rmax)(cid:3)
(cid:112)nEµ[ρ2σ2]/2

(cid:33)1/q

≤

Eµ[ρ2σ2] · C 1/((cid:15)q)

·

γ

(cid:32) (cid:112)Eµ[ρ2σ2] · (cid:112)Eµ[σ2/R2

max]

(cid:33)1/q

=

Eµ[ρ2σ2] ·

C 2/(cid:15)
γ

·

(cid:18)

2Eµ[σ2/R2

max]

n

≤

Eµ[ρ2σ2] .

1
2

(cid:112)nEµ[ρ2σ2]/2
(cid:19)1/2q

For the correction term in Theorem 4, we similarly have

(cid:20)
ξ ˆρ2 ˆR21

(cid:18)

Eµ

(cid:113)

ξ ˆρ ˆR >

nEµ[ξ ˆρ2 ˆR2]/16

(cid:19)(cid:21)

≤ Eµ

(cid:104)(cid:0)ξ ˆρ2 ˆR2(cid:1)p(cid:105)1/p

(cid:104)

· Pµ

ξ ˆρ ˆR >

nEµ[ξ ˆρ2 ˆR2]/16

(cid:105)1/q

(cid:113)

≤

Eµ[ξρ2R2

max] · C 1/((cid:15)q)

γ

· Pµ

(cid:104)
ξ ˆρ2 ˆR2 > nEµ[ξ ˆρ2 ˆR2]/16

(cid:105)1/q

,

so that Markov’s inequality and Eq. (11) further yield

≤

Eµ[ξρ2R2

max] · C 1/((cid:15)q)

γ

·

(cid:32) Eµ[ξ ˆρ2 ˆR2]

(cid:33)1/q

nEµ[ξ ˆρ2 ˆR2]/16
(cid:19)1/q

16
n

1
2

=

Eµ[ξρ2R2

max] ·

C 1/(cid:15)
γ

·

≤

Eµ[ξρ2R2

max] ≤

(cid:18)

(1 + δ)2
2

Eµ[ξ ˆρ2 ˆR2] .

(13)

Using Eq. (12), the bound of Theorem 3 simpliﬁes as

Rn(π; λ, µ, σ, Rmax)

Eµ[ρ2σ2]
32en



1 −

(cid:104)
ρ2σ21

(cid:16)

Eµ

ρσ2 > Rmax
Eµ[ρ2σ2]

(cid:112)nEµ[ρ2σ2]/2

(cid:17)(cid:105)

2




≥

≥

Eµ[ρ2σ2]
32en

(cid:18)

1 −

(cid:19)2

1
2

=

Eµ[ρ2σ2]
128en

.

(12)

(14)

1
2

1
2

1
2

1
2

1
2

1
2

1
2

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Similarly, by Eq. (13), Theorem 4 simpliﬁes as

(cid:20)
(cid:16)
ξ ˆρ2 ˆR21

Eµ

(cid:113)

ξ ˆρ ˆR >

nEµ[ξ ˆρ2 ˆR2]/16

Eµ[ξ ˆρ2 ˆR2]

(cid:17)(cid:21)

2






− γ(cid:48) log(5/γ(cid:48))(1 + δ)Eµ[ξ ˆρ2 ˆR2]

Rn(π; λ, µ, σ, Rmax)

≥

Eµ[ξ ˆρ2 ˆR2]
32en

1 −







(cid:20)

≥

=

≥

Eµ[ξ ˆρ2 ˆR2]
32en
Eµ[ξ ˆρ2 ˆR2]
128en
Eµ[ξρ2R2
128en

1 −

(cid:21)2

(1 + δ)2
2

− γ(cid:48) log(5/γ(cid:48))(1 + δ)Eµ[ξ ˆρ2 ˆR2]

(cid:0)1 − 2δ − δ2(cid:1)2

− γ(cid:48) log(5/γ(cid:48))(1 + δ)Eµ[ξ ˆρ2 ˆR2]

max]

(cid:0)1 − 2δ − δ2(cid:1)2
(1 + δ)2

− γ(cid:48) log(5/γ(cid:48))(1 + δ)Eµ[ξρ2R2

max] .

Since this bound is valid for all δ > 0, taking δ → 0, we obtain

Rn(π; λ, µ, σ, Rmax) ≥

− γ log(5/γ)Eµ[ξρ2R2

max] .

Eµ[ξρ2R2
128en

max]

Combining this bound with Eq. (14) yields

Rn(π; λ, µ, σ, Rmax)

≥

≥

=

Eµ[ρ2σ2]
128en

·

1
2
Eµ[ρ2σ2]
700n
(cid:104)
1
700n

+

Eµ[ξρ2R2
128en

max]

−

1
2

·

+

1
2
Eµ[ξρ2R2
700n

max]

−

1
2

· γ log(5/γ)Eµ[ξρ2R2

max]

· γ log(5/γ)Eµ[ξρ2R2

max]

Eµ[ρ2σ2] + Eµ[ξρ2R2

max]

1 − 350nγ log(5/γ)

.

(cid:16)

(cid:17)(cid:105)

It remains to prove Theorems 3 and 4. They are both proved by a reduction to hypothesis testing, and invoke Le Cam’s
argument to lower-bound the error in this testing problem. As in most arguments of this nature, the key contribution lies in
the construction of an appropriate testing problem that leads to the desired lower bounds. Before proving the theorems, we
recall the basic result of Le Cam which underlies our proofs. We point the reader to the excellent exposition of Lafferty et al.
(2008, Section 36.4) on more details about Le Cam’s argument.

Theorem 5 (Le Cam’s method, Lafferty et al., 2008, Theorem 36.8). Let P be a set of distributions, let X1, . . . , Xn be an
i.i.d. sample from some P ∈ P, let θ(P ) be any function of P ∈ P, let ˆθ(X1, . . . , Xn) be an estimator, and d be a metric.
For any pair P0, P1 ∈ P,

inf
ˆθ

sup
P ∈P

EP [d(ˆθ, θ(P ))] ≥

e−nDKL(P0(cid:107)P1)

∆
8

(15)

where ∆ = d(θ(P0), θ(P1)), and DKL(P0(cid:107)P1) = (cid:82) log(dP0/dP1)dP0 is the KL-divergence.

While the proofs of the two theorems share a lot of similarities, they have to use reductions to slightly different testing
problems given the different mean and variance constraints in the two results. We begin with the proof of Theorem 3, which
has a simpler construction.

A.1. Proof of Theorem 3

The basic idea of this proof is to reduce the problem of policy evaluation to that of Gaussian mean estimation where there is
a mean associated with each x, a pair. We now describe our construction.

Creating a family of problems Since we aim to show a lower bound on the hardness of policy evaluation in general,
it sufﬁces to show a particular family of hard problem instances, such that every estimator requires the stated number of

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

samples on at least one of the problems in this family. Recall that our minimax setup assumes that π, µ and λ are ﬁxed
and the only aspect of the problem which we can design is the conditional reward distribution D(r | x, a). For Theorem 3,
this choice is further constrained to satisfy E[r | x, a] ≤ Rmax(x, a) and Var(r | x, a) ≤ σ2(x, a). In order to describe our
construction, it will be convenient to deﬁne the shorthand E[r | x, a] = η(x, a). We will identify a problem in our family
with the function η(x, a) as that will be the only changing element in our problems. For a chosen η, the policy evaluation
η = E[r(x, a)], where the contexts x are chosen according to λ, actions are drawn from
question boils down to estimating vπ
π(x, a) and the reward distribution Dη(r | x, a) is a normal distribution with mean η(x, a) and variance σ2(x, a)

Dη(r | x, a) = N (η(x, a), σ2(x, a)).

Clearly this choice meets the variance constraint by construction, and satisﬁes the upper bound so long as η(x, a) ≤
Rmax(x, a) almost surely. Since the evaluation policy π is ﬁxed throughout, we will drop the superscript and use vη to
η in the remainder of the proofs. With some abuse of notation, we also use Eη[·] to denote expectations where
denote vπ
contexts and actions are drawn based on the ﬁxed choices λ and µ corresponding to our data generating distribution, and the
rewards drawn from η. We further use Pη to denote this entire joint distribution over (x, a, r) triples.

Given this family of problem instances, it is easy to see that for any pair of η1, η2 which are both pointwise upper bounded
by Rmax, we have the lower bound:

Rn(λ, π, µ, σ2, Rmax) ≥ inf
ˆv

max
η∈η1,η2

(cid:104)

Eη

(cid:105)

,

(ˆv − vη)2
(cid:124)
(cid:123)(cid:122)
(cid:125)
(cid:96)η(ˆv)

where we have introduced the shorthand (cid:96)η(ˆv) to denote the squared error of ˆv to vη. For a parameter (cid:15) > 0 to be chosen
later, we can further lower bound this risk for a ﬁxed ˆv as

Rn(ˆv) ≥ max
η∈η1,η2
(cid:104)
(cid:15)
2

≥

Eη[(cid:96)η(ˆv)] ≥ max
η∈η1,η2

(cid:15)Pη((cid:96)η ≥ (cid:15))
(cid:105)
Pη1((cid:96)η1 (ˆv) ≥ (cid:15)) + Pη2 ((cid:96)η2(ˆv) ≥ (cid:15))

,

where the last inequality lower bounds the maximum by the average. So far we have been working with an estimation
problem. We next describe how to reduce this to a hypothesis testing problem.

Reduction to hypothesis testing For turning our estimation problem into a testing problem, the idea is to identify a pair
η1, η2 such that they are far enough from each other so that any estimator which gets a small estimation loss can essentially
identify whether the data generating distribution corresponds to Pη1 or Pη2. In order to do this, we take any estimator ˆv
and identify a corresponding test statistic which maps ˆv into one of η1, η2. The way to do this is essentially identiﬁed in
Eq. (16), and we describe it next.

Note that since we are constructing a hypothesis test for a speciﬁc pair of distributions Pη1 and Pη2 , it is reasonable to
consider test statistics which have knowledge of η1 and η2, and hence the corresponding distributions. Consequently, these
tests also know the true policy values vη1 and vη2 and the only uncertainty is which of them gave rise to the observed data
samples. Therefore, for any estimator ˆv, we can a associate a statistic φ(ˆv) = argminη {(cid:96)η1(ˆv), (cid:96)η2(ˆv)}.
Given this hypothesis test, we are interested in its error rate Pη(φ(ˆv) (cid:54)= η). We ﬁrst relate the estimation error of ˆv to the
error rate of the test. Suppose for now that

(cid:96)η1 (ˆv) + (cid:96)η1 (ˆv) ≥ 2(cid:15),
so that at least one of the losses is at least (cid:15). Suppose that the data comes from η1. Then if (cid:96)η1 (ˆv) < (cid:15), we know that the test
is correct, because by Eq. (17) the other loss is greater than (cid:15), and therefore φ(ˆv) = η1. This means that the error under η1
can only occur if (cid:96)η1(ˆv) ≥ (cid:15). Similarly, the error under η2 can only occur if (cid:96)η2(ˆv) ≥ (cid:15), so the test error can be bounded as

(17)

max
η∈η1,η2

Pη(φ(ˆv) (cid:54)= η) ≤ Pη1(φ(ˆv) (cid:54)= η1) + Pη2(φ(ˆv) (cid:54)= η2)

≤ Pη1((cid:96)η1 (ˆv) ≥ (cid:15)) + Pη2((cid:96)η2(ˆv) ≥ (cid:15))

≤

Rn(ˆv),

2
(cid:15)

(16)

(18)

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

where the ﬁnal inequality uses our earlier lower bound in Eq. (16).

To ﬁnish connecting our the estimation problem to testing, it remains to establish our earlier supposition (17). Assume for
now that η1 and η2 are chosen such that

(vη1 − vη2)2 ≥ 4(cid:15).

(19)

Then an application of the inequality (a + b)2 ≤ 2a2 + 2b2 yields

4(cid:15) ≤ (vη1 − vη2)2 ≤ 2(ˆv − vη1)2 + 2(ˆv − vη2 )2 = 2(cid:96)η1(ˆv) + 2(cid:96)η2 (ˆv),

which yields the posited bound (16).

Invoking Le Cam’s argument So far we have identiﬁed a hypothesis testing problem and a test statistic whose error is
upper bounded in terms of the minimax risk of our problem. In order to complete the proof, we now place a lower bound on
the error of this test statistic. Recall the result of Le Cam (15), which places an upper bound on the attainable error in any
testing problem. In our setting, this translates to

max
η∈η1,η2

Pη(φ(ˆv) (cid:54)= η) ≥

e−nDKL(Pη1 || Pη2 ).

1
8

Since the distribution of the rewards is a spherical Gaussian, the KL-divergence is given by the squared distance between the
means, scaled by the variance, that is

DKL(Pη1 || Pη2) = E

(cid:20) (η1(x, a) − η2(x, a))2
2σ2(x, a)

(cid:21)

,

where we recall that the contexts and actions are drawn from λ and µ respectively. Since we would like the probability of
error in the test to be a constant, it sufﬁces to choose η1 and η2 such that
(cid:21)

E

(cid:20) (η1(x, a) − η2(x, a))2
2σ2(x, a)

≤

1
n

.

Picking the parameters So far, we have not made any concrete choices for η1 and η2, apart from some constraints which
we have introduced along the way. Note that we have the constraints (19) and (20) which try to ensure that η1 and η2 are not
too close that an estimator does not have to identify the true parameter, or too far that the testing problem becomes trivial.
Additionally, we have the upper and lower bounds of 0 and Rmax on η1 and η2. In order to reason about these constraints, it
is convenient to set η2 ≡ 0, and pick η1(x, a) = η1(x, a) − η2(x, a) = ∆(x, a). We now write all our constraints in terms
of ∆.

Note that vη2 is now 0, so that the ﬁrst constraint (19) is equivalent to

vη1 = Eη1[ρ(x, a)r(x, a)] = E∆[ρ(x, a)r(x, a)] ≥ 2

√

(cid:15),

where the importance weighting function ρ is introduced since Pη1 is based on choosing actions according to µ and we seek
to evaluate π. The second constraint (20) is also straightforward

E

(cid:21)

(cid:20) ∆2
2σ2

≤

1
n

.

Finally, the bound Rmax and non-negativity of η1 and η2 are enforced by requiring 0 ≤ ∆(x, a) ≤ Rmax(x, a) almost
surely.

The minimax lower bound is then obtained by the largest (cid:15) in the constraint (19) such that the other two constraints can be
satisﬁed. This gives rise to the following variational characterization of the minimax lower bound:

max
∆

(cid:15)

such that

E∆[ρ(x, a)r(x, a)] ≥ 2

√

(cid:15),

(cid:21)

E

(cid:20) ∆2
2σ2

1
n
0 ≤ ∆(x, a) ≤ Rmax(x, a).

≤

,

(20)

(21)

(22)

(23)

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Instead of ﬁnding the optimal solution, we just exhibit a feasible setting of ∆ here. We set

∆ = min

, Rmax

, where α =

(cid:26) ασ2ρ

Eµ[ρ2σ2]

(cid:27)

(cid:114)

2Eµ[ρ2σ2]
n

.

(24)

This setting satisﬁes the bounds (23) by construction. A quick substitution also veriﬁes that the constraint (22) is satisﬁed.
Consequently, it sufﬁces to set (cid:15) to the value attained in the constraint (21). Substituting the value of ∆ in the constraint, we
see that

Putting all the foregoing bounds together, we obtain that for all estimators ˆv

E∆[ρ(x, a)r(x, a)] = Ex∼λ,a∼µ[ρ(x, a)∆(x, a)]
(cid:16)

(cid:20)

ρ

1

ρσ2α ≤ RmaxEµ[ρ2σ2]

ασ2ρ
Eµ[ρ2σ2]
(cid:2)ρ2σ21(cid:0)ρσ2α > RmaxEµ[ρ2σ2](cid:1)(cid:3)
Eµ[ρ2σ2]

(cid:33)

(cid:17)(cid:21)

≥ Ex∼λ,a∼µ

(cid:32)

Eµ

= α

1 −

√

=: 2

(cid:15).

Rn(ˆv) ≥

Pη(φ(ˆv) (cid:54)= η)

(cid:17)

e−nDKL(Pη1 || Pη2 )

(cid:15)
2
(cid:15)
2
(cid:15)
2

(cid:16)

·

·

·

max
η∈η1,η2
1
8
1
8e

=

1
16e

·

α2
4

(cid:15)
16e
(cid:32)

1 −

≥

≥

=

Eµ

(cid:2)ρ2σ21(cid:0)ρσ2 > RmaxEµ[ρ2σ2]/α(cid:1)(cid:3)
Eµ[ρ2σ2]

(cid:33)2

=

Eµ[ρ2σ2]
32en



1 −

Eµ

(cid:104)
ρ2σ21(cid:0)ρσ2 > Rmax
Eµ[ρ2σ2]

(cid:112)nEµ[ρ2σ2]/2(cid:1)(cid:105)

2




.

A.2. Proof of Theorem 4

We now give the proof of Theorem 4. While it shares a lot of reasoning with the proof of Theorem 3, it has one crucial
difference. In Theorem 3, there is a non-trivial noise in the reward function, unlike in Theorem 4. This allowed the proof to
work with just two candidate mean-reward functions, since any realization in the data is corrupted with noise. However, in
the absence of added noise, the task of mean identiﬁcation becomes rather trivial: an estimator can just check whether η1 or
η2 matches the observations exactly.

To prevent such a strategy, we instead construct a richer family of reward functions. Instead of merely two mean rewards,
our construction will involve a randomized design of the expected reward function from an appropriate prior distribution.
The draw of the mean reward from a prior will essentially generate noise even though any given problem is noiseless. The
construction will also highlight the crucial sources of difference between the contextual and multi-armed bandit problems,
since the arguments here rely on having access to a rich context distribution, by which we mean distribution that puts
non-trivial probability on many contexts. In the absence of this property, the bound of Theorem 4 becomes weaker.

Creating a family of problems Our family of problems will be parametrized by the two reals δ and γ from the statement
of the theorem. Our construction begins with a discretization step at the resolution δ, whose goal is to create a countable
partition of the set of pairs X × A. If sets X and A are countable or ﬁnite, this step is vacuous, but if the sets of contexts or
actions have continuous parts, this step is required.

First, let µ(x, a) denote the joint probability measure obtained by ﬁrst drawing x ∼ λ and then a ∼ µ(· | x). In Lemma 1, we
show that X × A can be split into countably many disjoint sets Bi, (cid:85)
i∈I Bi = X × A, such that the following conditions
are satisﬁed:

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

• Each i ∈ I is associated with numbers Ri ≥ 0, ρi ≥ 0 and ξi ∈ {0, 1} such that

R2

max(x, a) ∈ [R2

i , (1 + δ)R2

i ] ,

ρ2(x, a) ∈ [ρ2

i , (1 + δ)ρ2

i ] ,

ξγ(x, a) = ξi

for all (x, a) ∈ Bi.

• Each Bi either satisﬁes µ(Bi) ≤ δ or consists of a single pair (xi, ai).

The numbers Ri and ρi will be exactly ˆR(x, a) and ˆρ(x, a) from the theorem statement.

As before, we parametrize the family of reward distributions in terms of the mean reward function η(x, a). However, now
η(x, a) is itself a random variable, which is drawn from a prior distribution. The reward function η(x, a) will be constant on
each Bi, and its value on Bi, written as η(i), will be drawn from a scaled Bernoulli, parametrized by a prior function θ(i) as
follows:

η(i) =

(cid:26) ξiRi
0

with probability θ(i),
with probability 1 − θ(i).

(25)

We now set Dη(r | x, a) = η(i) whenever (x, a) ∈ Bi. This clearly satisﬁes the constraints on the mean since 0 ≤
E[r | x, a] ≤ Ri ≤ Rmax(x, a) from the property of the partition, and also Var(r | x, a) = 0 as per the setting of Theorem 4.
The goal of an estimator is to take n samples generated by drawing x ∼ λ, a | x ∼ µ and r | x, a ∼ Dη, and output an
estimate ˆv such that Eη[(ˆv − vπ
η )2] is small. We recall our earlier shorthand vη to denote the value of π under the reward
distribution generated by η. For showing a lower bound on this quantity, it is clearly sufﬁcient to pick any prior distribution
(cid:2)Eη[(ˆv − vη)2 | η](cid:3). If this expectation is
governed by a parameter θ, as in Eq. (25), and lower bound the expectation Eθ
large for some estimator ˆv, then there must be some realization η, which induces a large error least one function η(x, a)
which induces a large error Eη[(ˆv − vη)2 | η], as desired. Consequently, we focus in the proof on lower bounding the
expectation Eθ[·]. This expectation can be decomposed with the use of the inequality a2 ≥ (a + b)2/2 − b2 as follows:

(cid:105)
(cid:104)
Eη[(ˆv − vη)2 | η]

Eθ

≥

(cid:105)
(cid:104)
Eη[(ˆv − Eθ[vη])2 | η]

Eθ

− Eθ

(vη − Eθ[vη])2(cid:105)
(cid:104)

.

1
2

Taking the worst case over all problems in the above inequality, we obtain

sup
η

Eη[(ˆv − vη)2] ≥ sup
θ

(cid:104)
Eη[(ˆv − vη)2 | η]

(cid:105)

Eθ

≥ sup

Eθ

Eη[(ˆv − Eθ[vη])2 | η]

(cid:104)

1
2

θ

(cid:124)

(cid:123)(cid:122)
T1

(cid:105)

(cid:125)

Eθ

− sup
θ

(cid:124)

(cid:104)
(vη − Eθ[vη])2(cid:105)
(cid:125)

.

(cid:123)(cid:122)
T2

(26)

This decomposition says that the expected MSE of an estimator in estimating vη can be related to the MSE of the same
estimator in estimating the quantity Eθ[vη], as long as the variance of the quantity vη under the distribution generated by θ is
not too large. This is a very important observation, since we can now choose to instead study the MSE of an estimator in
estimating Eθ[vη] as captured by T1. Unlike the distribution Dη which is degenerate, this problem has a non-trivial noise
arising from the randomized draw of η according to θ. Thus we can use similar techniques as the proof of Theorem 3, albeit
where the reward distribution is a scaled Bernoulli instead of Gaussian. For now, we focus on controlling T1, and T2 will be
handled later.

In order to bound T1, we will consider two carefully designed choices θ1 and θ2 to induce two different problem instances
and show that T1 is large for any estimator under one of the two parameters. In doing this, it will be convenient to use the
additional shorthand (cid:96)θ(ˆv) = (ˆv − Eθ[vη])2. Proceeding as in the proof of Theorem 3, we have

T1 =

sup
θ

(cid:105)
(cid:104)
Eη[(ˆv − Eθ[vη])2 | η]

Eθ

(cid:105)
(cid:104)
Eη[(cid:96)θ(ˆv) | η]

1
2

Eθ

=

sup
θ
Pθ ((cid:96)θ(ˆv) ≥ (cid:15))

≥

≥

Pθ ((cid:96)θ(ˆv) ≥ (cid:15)) ≥

sup
θ
(cid:105)
Pθ1 ((cid:96)θ1(ˆv) ≥ (cid:15)) + Pθ2 ((cid:96)θ2(ˆv) ≥ (cid:15))

max
θ∈θ1,θ2

(cid:104)

.

(cid:15)
2

1
2
(cid:15)
2
(cid:15)
4

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Reduction to hypothesis testing As in the proof of Theorem 3, we now reduce the estimation problem into a hypothesis
test for whether the data is generated according to the parameter θ1 or θ2. The arguments here are similar to the earlier proof,
so we will be terser in this presentation.

As before, our hypothesis test has entire knowledge of Dη as well as θ1 and θ2. Consequently, we construct a test based on
picking θ1 whenever (cid:96)θ1 (ˆv) ≤ (cid:96)θ2 (ˆv). As before, we will ensure that |Eθ1[vη] − Eθ2[vη]| ≥ 2
(cid:15) so that for any estimator ˆv,
we have

√

Under this assumption, we can similarly conclude that the error of our hypothesis test is at most

(cid:96)θ1 (ˆv) + (cid:96)θ2 (ˆv) ≥ 2(cid:15).

Pθ1 ((cid:96)θ1(ˆv) ≥ (cid:15)) + Pθ2 ((cid:96)θ2(ˆv) ≥ (cid:15)) .

Invoking Le Cam’s argument Once again, we can lower bound the error rate of our test by invoking the result of Le
Cam. This requires an upper bound on the KL-divergence DKL(Pθ1(cid:107)Pθ2). The only difference from our earlier argument is
that these distributions are now Bernoulli instead of Gaussian, based on the construction in Eq. (25). More formally, we have

DKL(Pθ1(cid:107)Pθ2 ) =

(cid:88)

(cid:88)

log

(cid:19)

(cid:18) p(r; θ1(i))
p(r; θ2(i))

p(r; θ1(i))µ(Bi)

i∈I
r∈{0,xiiRi}
(cid:104)

= Eµ

ξiDKL

(cid:0) Ber(θ1(i)) (cid:13)

(cid:13) Ber(θ2(i)) (cid:1) (cid:105)
,

where i is treated as a random variable under µ, and ξi is included, because the two distributions assign r = 0 with
probability one if ξi = 0.

It remains to carefully choose θ1 and θ2. We deﬁne θ2(i) ≡ 0.5, and let θ1(i) = θ2(i) + ∆i,
Picking the parameters
where ∆i will be chosen to satisfy certain constraints as before. Then, by Lemma 3, the KL divergence in Eq. (27) can be
bounded as

It remains to choose ∆i. Following a similar logic as before, we seek to ﬁnd a good feasible solution of the maximization
problem

DKL(Pθ1 (cid:107)Pθ2) ≤

Eµ

(cid:2)ξi∆2

i

(cid:3) .

1
4

max
∆

(cid:15)

such that

(cid:3) ≥ 2

√

(cid:15),

(cid:2)ρ(x, a)ξi∆iRi
1
n

Eµ
1
4
0 ≤ ∆i ≤ 0.5.

(cid:2)ξi∆2

(cid:3) ≤

Eµ

,

i

∆i = min

(cid:26) ξiρiRiα
Eµ[ξiρ2
i R2
i ]

(cid:27)

, 0.5

.

For some α > 0 to be determined shortly, we set

The bound constraint (30) is satisﬁed by construction and we set α = (cid:112)4Eµ[ξiρ2
obtain a feasible choice of (cid:15), we bound Eµ[ρ(x, a)ξi∆iRi] as follows:
(cid:2)ρ(x, a)ξi∆iRi

(cid:3) ≥ Eµ

Eµ

(cid:3)

i R2

i ]/n to satisfy the constraint (29). To

(cid:2)ξiρi∆iRi
(cid:34)
i R2
ξiρ2

≥ Eµ

(cid:32)

Eµ

= α

1 −

√

=: 2

(cid:15).

i ]/2α(cid:1)

(cid:35)

i R2

i α1(cid:0)ξiρiRi ≤ Eµ[ξiρ2
Eµ[ξiρ2
i R2
i ]
i 1(ξiρiRi > Eµ[ξiρ2

(cid:2)ξiρ2

i R2

i R2

i ]/2α)(cid:3)

(cid:33)

Eµ[ξiρ2

i R2
i ]

(27)

(28)

(29)

(30)

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Collecting our arguments so far, we have established that

(cid:15)
4
(cid:15)
4
(cid:15)
4

·

·

·

(cid:16)

(cid:17)
Pθ1 ((cid:96)θ1 (ˆv) ≥ (cid:15)) + Pθ2 ((cid:96)θ2 (ˆv) ≥ (cid:15))
1
8
1
8e

e−nDKL(Pθ1 (cid:107)Pθ2 )

=

T1 ≥

≥

≥

=

1
32e

·

α2
4

(cid:15)
32e
(cid:32)

1 −



=

i R2
i ]

Eµ[ξiρ2
32en

1 −

Eµ

(cid:2)ξiρ2

i R2

i 1(ξiρiRi > Eµ[ξiρ2

i R2

i ]/2α)(cid:3)

(cid:33)2

i R2
i ]

Eµ[ξiρ2
(cid:16)

Eµ

(cid:104)
ξiρ2

i R2
i 1

ξiρiRi > (cid:112)nEµ[ξiρ2
i R2
i ]

Eµ[ξiρ2

(cid:17)(cid:105)

2


i R2

i ]/16



.

In order to complete the proof, we need to further upper bound T2 in the decomposition (26).

Bounding T2 We need to bound the supremum over all priors θ. Consider an arbitrary prior θ and assume that η is drawn
(cid:2)(vη − Eθ[vη])2(cid:3), we view (vη − Eθ[vη])2 as a random variable under θ and bound it
according to Eq. (25). To bound Eθ
using Hoeffding’s inequality.

We begin by bounding its range. From the deﬁnition of η and vη,

0 ≤ vη ≤ Eπ[ξiRi] = Eµ[ρ(x, a)ξiRi] ≤ (1 + δ)1/2Eµ[ξiρiRi] ,

so also 0 ≤ Eθ[vη] ≤ (1 + δ)1/2Eµ[ξiρiRi]. Hence, |vη − Eθ[vη]| ≤ (1 + δ)1/2Eµ[ξiρiRi], and we obtain the bound

(vη − Eθ[vη])2 ≤ (1 + δ)(Eµ[ξiρiRi])2 ≤ (1 + δ)Eµ[ξiρ2

i R2
i ].

(31)

The proof proceeds by applying Hoeffding’s inequality to control the probability that (vη − Eθ[vη])2 ≥ t2 for a suitable t.
Then we can, with high probability, use the bound (vη − Eθ[vη])2 ≥ t2, and with the remaining small probability apply the
bound of Eq. (31).

To apply Hoeffding’s inequality, we write vη explicitly as

i := Eµ[ρ(x, a) | (x, a) ∈ Bi]. Thus, vη can be written as a sum of countably many independent variables, but we
where ρ(cid:48)
can only apply Hoeffding’s inequality to their ﬁnite subset. Note that the variables Yi are non-negative and upper-bounded by
a summable series, namely Yi ≤ µ(Bi)ρ(cid:48)
max] < ∞.
This means that for any δ0 > 0, we can choose a ﬁnite set I0 such that (cid:80)
Yi ≤ δ0. We will determine the sufﬁciently
small value of δ0 later; for now, consider the corresponding set I0 and deﬁne an auxiliary variable

iRi, where the summability follows because Eµ[ρRmax] ≤ 1 + Eµ[ρ2R2
i(cid:54)∈I0

vη =

µ(Bi)ρ(cid:48)

iηi =:

(cid:88)

i∈I

(cid:88)

i∈I

Yi

v(cid:48)
η :=

Yi ,

(cid:88)

i∈I0

which by construction satisﬁes v(cid:48)

η ≤ vη ≤ v(cid:48)

η + δ0. Note that the summands Yi can be bounded as

because ρ(cid:48)
Hoeffding’s inequality, we thus have

i ≤ (1 + δ)1/2ρi and ξiµ(Bi) ≤ ξi

0 ≤ Yi ≤ ξiρ(cid:48)

iRiµ(Bi) ≤ ξi(1 + δ)1/2ρiRi
(cid:112)µ(Bi)

√

(cid:112)µ(Bi)(cid:112)γ(cid:48)

γ(cid:48), because ξi = 0 whenever µ(Bi) > max{γ, δ} = γ(cid:48). By

P(|v(cid:48)

η − Eθv(cid:48)

η| ≥ t) ≤ 2 exp

−

(cid:40)

(cid:26)

2t2
ξiρ2

(1 + δ) (cid:80)
2t2

i∈I0

i R2
i µ(Bi)γ(cid:48)
(cid:27)

.

(cid:41)

≤ 2 exp

−

(1 + δ)γ(cid:48)Eµ[ξiρ2

i R2
i ]

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Now take t = (cid:112)γ(cid:48) log(4/γ(cid:48))(1 + δ)Eµ[ξiρ2

i R2

i ]/2 in the above bound, which yields

(cid:104)
P

(v(cid:48)

η − Eθv(cid:48)

η)2 ≥ t2(cid:105)

(cid:104)
= P
|v(cid:48)

η − Eθv(cid:48)

(cid:105)
η| ≥ t

≤

γ(cid:48)
2

.

Now, we can go back to analyzing vη. We set δ0 sufﬁciently small, so t + δ0 ≤ (cid:112)γ(cid:48) log(5/γ(cid:48))(1 + δ)Eµ[ξiρ2
using Eq. (31), we have

i R2

i ]/2. Thus,

Eθ

(cid:2)(vη − Eθvη)2(cid:3) ≤ (t + δ0)2 · P

(vη − Eθvη)2 < (t + δ0)2(cid:105)
(cid:104)

+ (1 + δ)Eµ[ξiρ2

i R2

≤ (t + δ0)2 + (1 + δ)Eµ[ξiρ2
γ(cid:48) log(5/γ(cid:48))(1 + δ)Eµ[ξiρ2

=

2

≤ γ(cid:48) log(5/γ(cid:48))(1 + δ)Eµ[ξiρ2

i R2

i ] .

(cid:104)
i ] · P

(vη − Eθvη)2 ≥ (t + δ0)2(cid:105)
(cid:104)
i ] · P
i R2
i R2
i ]

η)2 ≥ t2(cid:105)

η − Eθv(cid:48)

+ (1 + δ)Eµ[ξiρ2

(v(cid:48)

i R2

i ] ·

γ(cid:48)
2

Combining this bound with the bound on T1 yields the theorem.
Lemma 1. Let Z := X × A be a subset of Rd, let µ be a probability measure on Z and Rmax and ρ be non-negative
measurable functions on Z. Given γ ∈ [0, 1], deﬁne a random variable ξγ(z) := 1(µ(z) ≤ γ). Then for any δ ∈ (0, 1],
there exists a countable index set I and disjoint sets Bi ⊆ Z alongside non-negative reals Ri, ρi and ξi ∈ {0, 1} such that
the following conditions hold:

• Sets Bi form a partition of Z, i.e., Z = (cid:93)i∈IBi.
• Reals Ri and ρi approximate Rmax and ρ, and ξi equals ξγ as follows:

R2

max(z) ∈ [R2

i , (1 + δ)R2

i ] ,

ρ2(z) ∈ [ρ2

i , (1 + δ)ρ2

i ] ,

ξγ(z) = ξi

for all z ∈ Bi.

• Each set Bi either satisﬁes µ(Bi) ≤ δ or consists of a single z ∈ Z.

Proof. Let Z := X × A. We begin our construction by separating out atoms, i.e., the elements z ∈ Z such that µ(z) > 0.
Speciﬁcally, we write Z = Z na (cid:93) Z a where Z a consists of atoms and Z na of all non-atoms. The set Z a is either ﬁnite or
countably inﬁnite, so Z na is measurable.

By a theorem of Sierpi´nski (1922), since µ does not have any atoms on Z na, it must be continuous on Z na in the sense that
if A is a measurable subset of Z na with µ(A) = a then for any b ∈ [0, a], there exists a measurable set B ⊆ A such that
µ(B) = b. This means that we can decompose Z na into N := (cid:100)1/δ(cid:101) sets Z na
N such that each has a measure at
most δ and Z na = (cid:85)N

2 , . . . , Z na

1 , Z na

j=1 Z na
j .

We next ensure the approximation properties for Rmax and ρ. We begin by a countable decomposition of non-negative reals.
We consider the countable index set J := Z ∪ {−∞} and deﬁne the sequence aj := (1 + δ)j/2, for j ∈ Z. Positive reals
can then be decomposed into the following intervals indexed by J :

I−∞ := {0} ,

Ij := (aj, aj+1]

for j ∈ Z.

It will also be convenient to set a−∞ := 0. Thus, the construction of Ij guarantees that for all j ∈ J and all t ∈ Ij we have
j ≤ t2 ≤ (1 + δ)a2
a2
j .
The desired partition, with the index set I = Z a ∪ [N ] × J 2, is as follows:

for i = z ∈ Z a :
for i = (j, jR, jρ) ∈ [N ] × J 2:

Bi := {z}, Ri := Rmax(z), ρi := ρ(z), ξi := ξγ(z);
max(IjR ) ∩ ρ−1(Ijρ ),
Bi := Z na
Ri := ajR , ρi := ajρ , ξi := 1.

j ∩ R−1

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

B. Proof of Theorem 2

Let Ax := {a ∈ A : ρ(x, a) ≤ τ }. For brevity, we write Ai := Axi. We decompose the mean squared error into the squared
bias and variance and control each term separately,

MSE(ˆvSWITCH) = (cid:12)

(cid:12)E[ˆvSWITCH] − vπ(cid:12)
(cid:12)

2

+ Var[ˆvSWITCH].

We ﬁrst calculate the bias. Note that bias is incurred only in the terms that fall in Ac

x, so

E[ˆvSWITCH] − vπ = E

ˆr(x, a)π(a|x)

E[r|x, a] π(a|x)









(cid:88)

a∈Ac
x


 − E





(cid:88)

a∈Ac
x

= Eπ
= Eπ

(cid:104)(cid:0)ˆr(x, a) − E[r|x, a](cid:1) 1(a ∈ Ac
x)
(cid:2) (cid:15)(x, a) 1(ρ > τ ) (cid:3)

(cid:105)

where we recall that (cid:15)(x, a) = ˆr(x, a) − E[r|x, a].

Next we upper bound the variance. Note that the variance contributions from the IPS part and the DM part are not
independent, since the indicators ρ(xi, a) > τ and ρ(xi, a) ≤ τ are mutually exclusive. To simplify the analysis, we use the
following inequality that holds for any random variable X and Y :

Var(X + Y ) ≤ 2Var(X) + 2Var(Y ).

This allows us to calculate the variance of each part separately.

Var[ˆvSWITCH] ≤ 2 Var

[riρi1(ai ∈ Ai)]

+ 2 Var

(cid:34)

1
n

n
(cid:88)

i=1

(cid:35)
ˆr(xi, a)π(a|xi)1(a ∈ Ac
i )

=

Varµ

(cid:2)rρ1(a ∈ Ax)(cid:3) +

(cid:35)



Var



2
n

(cid:88)

a∈Ac
x

(cid:34)

1
n

n
(cid:88)

(cid:88)

i=1

a∈A

ˆr(x, a)π(a|x)





2
n

2
n

2
n

2
n

=

≤

≤

EµVar(cid:2)rρ1(a ∈ Ax) (cid:12)

(cid:12) x, a(cid:3) +

VarµE(cid:2)rρ1(a ∈ Ax) (cid:12)

(cid:12) x, a(cid:3) +

ˆr(x, a)π(a|x)

EµVar(cid:2)rρ1(a ∈ Ax) (cid:12)

(cid:12) x, a(cid:3) +

(cid:104)

E[rρ1(a ∈ Ax) | x, a]2 (cid:105)

Eµ

+

ˆr(x, a)π(a|x)

Eµ

(cid:2)σ2ρ21(a ∈ Ax)(cid:3) +

Eµ

(cid:2)R2

maxρ21(a ∈ Ax)(cid:3) +

2
n

(cid:34)

E

2
n

(cid:16) (cid:88)

a∈Ac
x

ˆr(x, a)π(a|x)

(cid:17)2 (cid:35)

.



(cid:88)

Var



2
n

(cid:34)

E

2
n

a∈Ac
x

(cid:16) (cid:88)

a∈Ac
x





(cid:17)2 (cid:35)

2
n

2
n

To complete the proof, note that the last term is further upper bounded using Jensen’s inequality as

(cid:34)

E

(cid:16) (cid:88)

a∈Ac
x

ˆr(x, a)π(a|x)

(cid:17)2 (cid:35)

(cid:34)
= E

π(a|x)

(cid:16) (cid:88)

a∈Ac
x

(cid:16) (cid:88)

a∈Ac
x

(cid:34)
≤ E

ˆr(x, a)π(a|x)
(cid:80)
π(a|x)

a∈Ac
x

(cid:19)2 (cid:35)

(cid:19) (cid:35)

(cid:17)2 (cid:18) (cid:88)

a∈Ac
x
(cid:17) (cid:18) (cid:88)

a∈Ac
x

π(a|x)

ˆr(x, a)2π(a|x)

≤ Eπ

(cid:2)R2

max1(ρ > τ )(cid:3),

where the ﬁnal inequality uses (cid:80)

a∈Ac
x

π(a|x) ≤ 1 and ˆr(x, a) ∈ [0, Rmax(x, a)] almost surely.

Combining the bias and variance bounds, we get the stated MSE upper bound.

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

C. Utility Lemmas

Lemma 2 (Hoeffding, 1963, Theorem 2). Let Xi ∈ [ai, bi] and X1, ..., Xn are drawn independently. Then the empirical
mean ¯X = 1

n (X1 + ... + Xn) obeys

Lemma 3 (Bernoulli KL-divergence). For 0 < p, q < 1, we have

−
P(| ¯X − E[ ¯X]| ≥ t) ≤ 2e

(cid:80)n

i=1

2n2t2
(bi−ai)2 .

DKL(Ber(p)(cid:107)Ber(q)) ≤ (p − q)2(

+

1
q

1
1 − q

).

Proof.

DKL(Ber(p)(cid:107)Ber(q)) = p log

+ (1 − p) log

(cid:19)

(cid:18) p
q

≤ p

p − q
q

= (p − q)2

+ (1 − p)

(cid:18) 1
q

+

q − p
1 − q
(cid:19)
1
1 − q

.

(cid:19)

(cid:18) 1 − p
1 − q
(p − q)2
q

=

+ (p − q) +

+ (q − p)

(p − q)2
1 − q

D. Additional Figures from the Experiments

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

(a) ecoli / deterministic reward

(b) ecoli / noisy reward

(c) glass / deterministic reward

(d) glass / noisy reward

(e) page-blocks / deterministic reward

(f) page-blocks / noisy reward

(g) satimage / deterministic reward

(h) satimage / noisy reward

IPSDMDRSWITCH-DRoracle-SWITCH-DRoracle-Trim/TrunIPSSWITCH-DR-magicn100150200250300MSE10-310-210-1100ecoli: n = 336, d = 7, k = 8n100150200250300MSE10-310-210-1ecoli: n = 336, d = 7, k = 8n100120140160180200MSE10-310-210-1glass: n = 214, d = 9, k = 6n100120140160180200MSE10-310-210-1glass: n = 214, d = 9, k = 6n102103MSE10-510-410-310-210-1100page-blocks: n = 5473, d = 10, k = 5n102103MSE10-510-410-310-210-1100page-blocks: n = 5473, d = 10, k = 5n102103MSE10-210-1100satimage: n = 6435, d = 36, k = 6n102103MSE10-310-210-1100satimage: n = 6435, d = 36, k = 6Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

(a) pendigits / deterministic reward

(b) pendigits / noisy reward

(c) letter / deterministic reward

(d) letter / noisy reward

(e) vehicle / deterministic reward

(f) vehicle / noisy reward

(g) wdbc / deterministic reward

(h) wdbc / noisy reward

IPSDMDRSWITCH-DRoracle-SWITCH-DRoracle-Trim/TrunIPSSWITCH-DR-magicn102103104MSE10-210-1100pendigits: n = 10992, d = 16, k = 10n102103104MSE10-210-1100pendigits: n = 10992, d = 16, k = 10n102103104MSE10-210-1letter: n = 20000, d = 16, k = 26n102103104MSE10-210-1letter: n = 20000, d = 16, k = 26n100200300400500600700800MSE10-310-210-1100vehicle: n = 846, d = 18, k = 4n100200300400500600700800MSE10-310-210-1vehicle: n = 846, d = 18, k = 4n100200300400500MSE10-210-1100wdbc: n = 569, d = 30, k = 2n100200300400500MSE10-310-210-1100wdbc: n = 569, d = 30, k = 2