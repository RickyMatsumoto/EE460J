Learning Deep Architectures via Generalized Whitened Neural Networks

Ping Luo 1 2

Abstract

Whitened Neural Network (WNN) is a recent
advanced deep architecture, which improves con-
vergence and generalization of canonical neural
networks by whitening their internal hidden rep-
resentation. However, the whitening transforma-
tion increases computation time. Unlike WNN
that reduced runtime by performing whitening
every thousand iterations, which degenerates
convergence due to the ill conditioning, we
present generalized WNN (GWNN), which has
three appealing properties.
First, GWNN is
able to learn compact representation to reduce
computations.
it enables whitening
transformation to be performed in a short period,
preserving good conditioning. Third, we propose
a data-independent estimation of the covariance
matrix to further improve computational efﬁcien-
cy. Extensive experiments on various datasets
demonstrate the beneﬁts of GWNN.

Second,

1. Introduction

Deep neural networks (DNNs) have improved perfor-
mances of many applications, as the non-linearity of DNNs
provides expressive modeling capacity, but it also makes
DNNs difﬁcult to train and easy to overﬁt the training data.

Whitened neural network (WNN) (Desjardins et al., 2015),
a recent advanced deep architecture, is ideally to solve the
above difﬁculties. WNN extends batch normalization (BN)
(Ioffe & Szegedy, 2015) by normalizing the internal hidden
representation using whitening transformation instead of
standardization. Whitening helps regularize each diagonal
block of the Fisher Information Matrix (FIM) to be an

1Guangdong Provincial Key Laboratory of Computer Vi-
sion and Virtual Reality Technology, Shenzhen Institutes of
Advanced Technology, Chinese Academy of Sciences, Shen-
zhen, China 2Multimedia Laboratory, The Chinese University
of Hong Kong, Hong Kong. Correspondence to: Ping Luo
<pluo@ie.cuhk.edu.hk>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

approximation of the identity matrix. This is an appeal-
ing property, as training WNN using stochastic gradient
descent (SGD) mimics the fast convergence of natural
gradient descent (NGD) (Amari & Nagaoka, 2000). The
whitening transformation also improves generalization. As
demonstrated in (Desjardins et al., 2015), WNN exhib-
ited superiority when being applied to various network
architectures, such as autoencoder and convolutional neural
network, outperforming many previous works including
SGD, RMSprop (Tieleman & Hinton, 2012), and BN.

Although WNN is able to reduce the number of training
iterations and improve generalization, it comes with a price
of increasing training time, because eigen-decomposition
occupies large computations. The runtime scales up when
the number of hidden layers that require whitening trans-
formation increases. We revisit WNN by breaking down
its performance and show that its main runtime comes
from two aspects, 1) computing full covariance matrix
for whitening and 2) solving singular value decomposition
(SVD). Previous work (Desjardins et al., 2015) suggests to
overcome these problems by a) using a subset of training
data to estimate the full covariance matrix and b) solving
the SVD every hundreds or thousands of training iterations.
Both of them rely on the assumption that the SVD holds in
this period, but it is generally not true. When this period
becomes large, WNN degenerates to canonical SGD due to
ill conditioning of FIM.

We propose generalized WNN (GWNN), which possesses
the beneﬁcial properties of WNN, but signiﬁcantly reduces
its runtime and improves its generalization. We introduce
two variants of GWNN, including pre-whitening and post-
whitening GWNNs. The former one whitens a hidden
layer’s input values, whilst the latter one whitens the pre-
activation values (hidden features). GWNN has three ap-
pealing characteristics. First, compared to WNN, GWNN
is capable of learning more compact hidden representa-
tion, such that the SVD can be approximated by a few
top eigenvectors to reduce computation. This compact
representation also improves generalization. Second, it
enables the whitening transformation to be performed in
a short period, maintaining conditioning of FIM. Third,
by exploiting knowledge of the distribution of the hidden
features, we calculate the covariance matrix in an analytical
form to further improve computational efﬁciency.

Generalized Whitened Neural Network

Figure 1. Comparisons of differnet architectures. An ordinary fully-connected (fc) layer can be adapted into (b) a whitened fc layer, (c)
a pre-GWNN layer, and (d) a post-GWNN layer. (c) and (d) learn more compact representation than (b) does.

2. Notation and Background

(a) into a whitened fc layer. Its information ﬂow becomes

We begin by deﬁning the basic notation for feed-forward
neural network. A neural network transforms an input
vector o0 to an output vector o(cid:96) through a series of (cid:96)
hidden layers {oi}(cid:96)
i=1. We assume each layer has identical
dimension for the simplicity of notation i.e. ∀oi ∈ Rd×1.
In this case, all vectors and matrixes in the following
should have d rows unless otherwise stated. As shown
in Fig.1 (a), each fully-connected (fc) layer consists of
a weight matrix, W i, and a set of hidden neurons, hi,
each of which receives as input a weighted sum of outputs
from the previous layer. We have hi = W ioi−1.
In
this work, we take fully-connected network as an example.
Note that the above computation can be also applied to a
convolutional network, where an image patch is vectorized
as a column vector and represented by oi−1 and each row
of W i represents a ﬁlter.

As the recent deep architectures typically stack a batch
normalization (BN) layer before the pre-activation values,
we do not explicitly include a bias term when computing
hi, because it
is normalized in BN, such that φi =
hi−E[hi]
√
, where the expectation and variance are computed
Var[hi]

over a minibatch of samples. LeCun et al. (2002) showed
that such normalization speeds up convergence even when
the hidden features are not decorrelated. Furthermore,
output of each layer is calculated by a nonlinear activation
function. A popular choice is the rectiﬁed linear unit,
relu(x) = max(0, x). The precise computation for an
output is oi = max(0, diag(αi)φi + βi), where diag(x)
represents a matrix whose diagonal entries are x. αi and βi
are two vectors that scale and shift the normalized features,
in order to maintain the network’s representation capacity.

2.1. Whitened Neural Networks

This section revisits whitened neural networks (WNN).
Any neural architecture can be adapted to a WNN by
stacking a whitening transformation layer after the layer’s
input. For example, Fig.1 (b) adapts a fc layer as shown in

(cid:101)oi−1 = P i−1(oi−1 − µi−1), ˆhi = ˆW i
φi =

, oi = max(0, diag(αi)φi + βi),

(cid:101)oi−1,

ˆhi√

(1)

Var[ˆhi]

where µi−1 represents a centering variable, µi−1 =
E[oi−1]. P i−1 is a whitening matrix whose rows are
obtained from eigen-decomposition of Σi−1, which is
the covariance matrix of the input, Σi−1 = E[(oi−1 −
µi−1)(oi−1 − µi−1)T]. The input is decorrelated by P i−1
in the sense that its covariance matrix becomes an identity
matrix, i.e. E[(cid:101)oi−1
] = I. To avoid ambiguity, we use
‘ˆ’ to distinguish the variables in WNN and the canonical
fc layer whenever necessary. For instance, ˆW i represents a
whitened weight matrix.
In Eqn.(1), computation of the
BN layer has been simpliﬁed because we have E[ˆhi] =
ˆW iP i−1(E[oi−1] − µi−1) = 0.

(cid:101)oi−1T

We deﬁne θ to be a vector consisting of all
the
whitened weight matrixes concatenated together, θ =
{vec( ˆW 1)T, vec( ˆW 2)T, ..., vec( ˆW (cid:96))T}, where vec(·) is an
operator that vectorizes a matrix by stacking its columns.
Let L(o(cid:96), y; θ) denote a loss function of WNN, which
measures the disagreement between a prediction o(cid:96) made
by the network, and a target y. WNN is trained by
minimizing the loss function with respect to the parameter
vector θ and two constraints

min
θ

L(o(cid:96), y; θ)

(2)

s.t. E[(cid:101)oi−1

(cid:101)oi−1T

] = I, hi − E[hi] = ˆhi, i = 1...(cid:96).

2 U i−1T

To satisfy the ﬁrst constraint, P i−1 is obtained by decom-
posing the covariance matrix, Σi−1 = U i−1Si−1U i−1T
.
We choose P i−1 = (Si−1)− 1
, where Si−1 is a
diagonal matrix whose diagonal elements are eigenvalues
and U i−1 is an orthogonal matrix of eigenvectors. The
ﬁrst constraint holds under the construction of eigen-
decomposition.
The second constraint, hi − E[hi] = ˆhi, enforces that
the centered hidden features are the same, before and after

oi-1oiWioi-1Pi-1hioi-1Pioi-1Pi-1hi(a)fully-connected layer (fc)(b) whitened fc layer of WNN(c) pre-whitening GWNN(d)post-whitening GWNNWiWiWirelubnϕioioioioioioiˆˆϕiϕiϕiϕiϕiϕioi-1～oi-1～oi-1～oi-1～ˆˆhiweight matrixwhitening matrixhiˆhiˆˆhihid d d d d d d 2 = (cid:107)(W ioi−1 − W iµi−1) − ˆW i

adapting a fc layer to WNN, as shown in Fig.1 (a) and (b).
In other words, it ensures that their representation powers
are identical. By combing the computations in Fig.1 (a) and
Eqn.(1), the second constraint implies that (cid:107)(hi − E[hi]) −
ˆhi(cid:107)2
2 = 0, which
has a closed-form solution, ˆW i = W i(P i−1)−1. To see
this, we have ˆhi = W i(P i−1)−1P i−1(oi−1 − µi−1) =
W i(oi−1−µi−1) = hi−E[hi]. The representation capacity
can be preserved by mapping the whitened weight matrix
from the ordinary weight matrix.

(cid:101)oi−1(cid:107)2

Conditioning of the FIM. Here we show that WNN
improves training efﬁciency by conditioning the Fisher
information matrix (FIM) (Amari & Nagaoka, 2000). A
FIM, denoted as F , consists of (cid:96) × (cid:96) blocks. Each block is
indexed by Fij, representing the covariance (co-adaptation)
between the whitened weight matrixes of the i-th and j-th
layers. We have Fij = E[vec(δ ˆW i)vec(δ ˆW j)T], where
δ ˆW i indicates the gradient of the i-th whitened weight
matrix. For example, the gradient of ˆW i is achieved by
(cid:101)oi−1(δˆhi)T, as illustrated in Eqn.(1). We have vec(δ ˆW i) =
vec((cid:101)oi−1(δˆhi)T) = δˆhi ⊗ (cid:101)oi−1, where ⊗ denotes the
In this case, Fij can be rewritten
Kronecker product.
as E[(δˆhi ⊗ (cid:101)oi−1)(δˆhj ⊗ (cid:101)oj−1)T] = E[δˆhi(δˆhj)T ⊗
(cid:101)oi−1((cid:101)oj−1)T]. By assuming δˆh and (cid:101)o are independent as
demonstrated in (Raiko et al., 2012), Fij can be approx-
imated by E[δˆhi(δˆhj)T] ⊗ E[(cid:101)oi−1((cid:101)oj−1)T]. As a result,
when i = j, each diagonal block of F , Fii, has a block
diagonal structure because we have E[(cid:101)oi−1((cid:101)oi−1)T] = I as
shown in Eqn.(2), which improves conditioning of FIM and
thus speeds up training. In general, WNN regularizes the
diagonal blocks of FIM and achieves stronger conditioning
than those methods (LeCun et al., 2002; Tieleman &
Hinton, 2012) that regularized the diagonal entries.

Training WNN. Alg.1 summarizes training of WNN. At
the 1st line, the whitened weight matrix ˆW i
0 is initialized
by W i of the ordinary fc layer, which can be pretrained
or sampled from a Gaussian distribution. The 4th line
shows that ˆW i
t is updated in each iteration t using SGD.
The ﬁrst and second constraints are achieved in the 7th and
8th lines respectively. For example, the 8th line ensures
that the hidden features are the same before and after
updating the whitening matrix. As the distribution of the
hidden representation changes after every update of the
whitened weight matrix, to maintain good conditioning of
FIM, the whitening matrix, P i−1, needs to be reconstructed
frequently by performing eigen-decomposition on Σi−1,
which is estimated using N samples. N is typically
104 in experiments. However, this raw strategy increases
computation time. Desjardins et al. (2015) performed
whitening in every τ iterations as shown in the 5th line of
Alg.1 to reduce computations, e.g. τ = 103.

Generalized Whitened Neural Network

Algorithm 1 Training WNN
1: Init: initial network parameters θ, αi, βi; whitening matrix

P i−1 = I; iteration t = 0; ˆW i

t = W i; ∀i ∈ {1...(cid:96)}.

2: repeat
3:
4:

for i = 1 to (cid:96) do

5:
6:
7:

update whitened weight matrix ˆW i
αi
t using SGD.
if mod(t, τ ) = 0 then

t, βi

t and parameters

store old whitening matrix P i−1
construct new matrix P i−1 by eigen-decomposition
on Σi−1, which is estimated using N samples.
transform weight matrix ˆW i

o = P i−1.

(P i−1)−1.

t = ˆW i

t P i−1
o

8:
end if
9:
end for
10:
t = t + 1.
11:
12: until convergence

Figure 2. Visualizations of different covariance matrixes (a)-(d),
which have different Pearson’s correlations (top) with respect to
an identity matrix. Larger Pearson’s correlation indicates higher
(a,b) are sampled from a uniform distribution
orthogonality.
between 0 and 1.
(c,d) are generated by truncating a random
orthogonal matrix with different numbers of columns. The
colorbar (right) indicates the value of each entry in these matrixes.

g.1? We measure the similarity of the covariance matrix,
E[(cid:101)oi−1((cid:101)oi−1)T], with the identity matrix I. This is called
the orthogonality. We employ Pearson’s correlation1 as the
similarity between two matrixes. Intuitively, this measure
has a value between −1 and +1, representing negative
and positive correlations. Larger values indicate higher
orthogonality. Fig.2 visualizes four randomly generated
covariance matrixes, where (a,b) are sampled from a
uniform distribution between 0 and 1. Fig.2 (c,d) are
generated by truncating different numbers of columns of a
randomly generated orthogonal matrix. For instance, (a,b)
have small similarity with respect to the identity matrix.
In contrast, when the correlation equals 0.65 as shown in
(c), all entries in the diagonal are larger than 0.9 and more
than 80% off-diagonal entries have values smaller than 0.1.
Furthermore, Pearson’s correlation is insensitive to the size
of matrix, such that orthogonality of different layers can
be compared together. For example, although matrixes in

1Given an identity matrix, I, and a covariance matrix, Σ, the
Pearson’s correlation between them is deﬁned as corr(Σ, I) =
√
, where vec(Σ) is a normalized vec-

vec(Σ)Tvec(I)
vec(Σ)Tvec(Σ)·vec(I)Tvec(I)

How good is the conditioning of the FIM by using Al-

tor by subtracting mean of all entries.

0.501.0(a)(b)(c)(d)0.340.350.650.95Generalized Whitened Neural Network

as illustrated in Fig.1 (c). When adapting a fc layer to
pre-GWNN, the whitening matrix is truncated by removing
those eigenvectors that have small eigenvalues, in order to
learn compact representation. This allows the input vector
to vary its length, so as to gradually adapt the learned
representation to informative patterns with high variations,
but not noises. Learning pre-GWNN is formulated analo-
gously to learning WNN in Eqn.(2), but with one additional
constraint truncated the rank of the whitening matrix,

min
θ

L(o(cid:96), y; θ)

(3)

s.t. rank(P i−1) ≤ d(cid:48), E[(cid:101)oi−1

d(cid:48) (cid:101)oi−1T
d(cid:48)
hi − E[hi] = ˆhi, i = 1...(cid:96).

] = I,

d(cid:48) = (Si−1

Let d be the dimension of the original fc layer. By combin-
2 U i−1T ∈ Rd×d,
ing Eqn.(2), we have P i−1 = (Si−1)− 1
T
d(cid:48) )− 1
2 U i−1
which is truncated by using P i−1
∈
d(cid:48)
Rd(cid:48)×d, where Si−1
is achieved by keeping rows and
d(cid:48)
columns associated with the ﬁrst d(cid:48)
large eigenvalues,
whilst U i−1
contains the corresponding d(cid:48) eigenvectors.
The value of d(cid:48) can be tuned using a validation set.
For simplicity, we choose d(cid:48) = d
2 , which works well
throughout our experiments. This is inspired by the ﬁnding
in (Zhang et al., 2015), who disclosed that the ﬁrst 50%
eigenvectors contribute over 95% energy in a deep model.

d(cid:48)

d(cid:48) = P i−1

low-dimensional space, (cid:101)oi−1

More speciﬁcally, pre-GWNN ﬁrst projects an input vector
to a d(cid:48)
(oi−1 −
µi−1) ∈ Rd(cid:48)×1.
The whitened weight matrix then
produces a hidden feature vector of d dimensions, which
has the same length as the ordinary fc layer, i.e. ˆhi =
)−1 ∈ Rd×d(cid:48)
ˆW i
.
The computations of BN and the nonlinear activation are
identical to Eqn.(1).

d(cid:48) ∈ Rd×1, where ˆW i = W i(P i−1
(cid:101)oi−1

d(cid:48)

d(cid:48)

Training pre-GWNN is similar to Alg.1.
The main
modiﬁcation is produced at the 7th line in order to reduce
runtime. Although Alg.1 decreases number of iterations
when training converged, each iteration has additional
computation time for eigen-decomposition. For example,
in WNN, the required computation of full singular value
decomposition (SVD) is typically O(N d2), where N
represents the number of samples employed to estimate
In particular, when we have (cid:96)
the covariance matrix.
whitened layers and T is the number of iterations, all
whitening transformations occupy O( N d2T (cid:96)
) runtime in
the entire training stage. In contrast, pre-GWNN performs
the popular online estimation for the top d(cid:48) eigenvectors
in P i−1
such as online SVD (Shamir, 2015; Povey et al.,
d(cid:48)
instead of using full SVD as WNN did. This
2015),
difference reduces runtime to O( (N +M )d(cid:48)T (cid:96)
), where τ (cid:48)
represents the whitening interval in GWNN and M is the
number of samples used to estimate the top eigenvectors.
We have M = N as employed in previous works.

τ (cid:48)

τ

Figure 3. Comparisons of conditioning when training a network-
in-network (Lin et al., 2014) on CIFAR-10 (Krizhevsky, 2009)
by using (a) WNN and (b) pre-GWNN. Compared to (b),
the orthogonalities of three different layers in (a) have large
ﬂuctuations due to the ill conditioning of whitening, which is
performed in a large period τ . As a result, when τ increases,
WNN will degenerate to the canonical SGD.

(a) and (b) have different sizes, they have similar value
of orthogonality when they are sampled from the same
distribution.

As shown in Fig.3 (a), we adopt network-in-network
(NIN) (Lin et al., 2014) that is trained on CIFAR-10
(Krizhevsky, 2009), and plot the orthogonalities of three
different convolutional layers, which are whitened every
τ = 103 iterations by using Alg.1. We see that or-
thogonality values during training have large ﬂuctuations
except those of the ﬁrst convolutional layer, abbreviated as
‘conv1’. This is because the distributions of deeper layers’
inputs change after the whitened weight matrixes have
been updated, leading to ill-conditions of the whitening
matrixes, which are estimated in a large interval. In fact,
large τ will degenerate WNN to canonical SGD. However,
‘conv1’ uses image data as inputs, whose distribution is
typically stable during training. Its whitening matrix can
be estimated once at the beginning and ﬁxed in the entire
training stage.

In the section below, we present generalized whitened
neural networks to improve conditioning of FIM while
reducing computation time.

3. Generalized Whitened Neural Networks

We present two types of generalized WNN (GWNN), in-
cluding pre-whitening and post-whitening GWNNs. Both
models share beneﬁcial properties of WNN, but have lower
computation time.

3.1. Pre-whitening GWNN

This section introduces pre-whitening GWNN, abbreviated
as pre-GWNN, which performs whitening transformation
before applying the weight matrix (i.e. whiten the input),

00.10.20.30.40.50.60.70.80.911.101234orthogonalityiterations (1e3)conv1conv4conv700.10.20.30.40.50.60.70.80.911.101234orthogonalityiterations (1e3)conv1conv4conv755（a）WNN（b）pre-GWNNGeneralized Whitened Neural Network

For pre-GWNN, reducing runtime and improving condi-
tioning is a tradeoff, since the former requires to increase
τ (cid:48) but the latter requires to decrease it. When M = N
and d(cid:48) = d
2 , we compare the runtime complexity of pre-
GWNN to that of WNN, and obtain a ratio of dτ (cid:48)
τ , which
tells us that whitening can be performed in a short interval
without increasing runtime. For instance, as shown in
Fig.3 (b) when τ (cid:48) = 20, orthogonalities are well preserved
and more stable than those in (a).
In this case, pre-
GWNN reduces computations of WNN by at least 20×
when d > τ , which is a typical choice in recent deep
architectures (Krizhevsky et al., 2012; Lin et al., 2014)
where d ∈ {1024, 2048, 4096}.

3.2. Post-whitening GWNN

Another variant we propose is post-whitening GWNN,
abbreviated as post-GWNN. Unlike WNN and pre-GWNN,
post-GWNN performs whitening transformation after ap-
plying the weight matrix (i.e. whiten the feature), as
In general, post-GWNN reduces
illustrated in Fig.1 (d).
runtime to O( (N (cid:48)+M )d(cid:48)T (cid:96)
), where N (cid:48) (cid:28) N .

τ (cid:48)

Fig.1 (d) shows how to adapt a fc layer to post-GWNN.
Suppose oi−1
has been whitened by P i−1
in the previous
layer, at the i-th layer we have

d(cid:48)

d(cid:48)

ˆhi = ˆW i(oi−1

d(cid:48) − µi−1
d(cid:48) ), hi
d(cid:48) = max(0, diag(αi

d(cid:48) = P i
d(cid:48)
d(cid:48))φi

ˆhi,

, oi

d(cid:48) + βi

d(cid:48)),

(4)

φi
d(cid:48) =

hi
d(cid:48)√
Var[hi

d(cid:48) ]

].

d(cid:48) = E[oi−1
d(cid:48)

where µi−1
In Eqn.(4), a feature vector
ˆhi ∈ Rd×1 is ﬁrst produced by applying a whitened weight
matrix on the input, in order to recover the original feature
length as the fc layer. A whitening matrix then projects
ˆhi to a decorrelated feature vector hi
d(cid:48) ∈ Rd(cid:48)×1. We
)−1 ∈ Rd×d(cid:48)
have ˆW i = W i(P i−1
, where P i−1
=
T
∈ Rd(cid:48)×d, and U i−1 and Si−1 contain
2 U i−1
(Si−1
d(cid:48)
eigenvectors and eigenvalues of the hidden features at the
i − 1-th layer.

d(cid:48) )− 1

d(cid:48)

d(cid:48)

d(cid:48)hiT

Conditioning.
Here we disclose that whitening hid-
den features also enforces good conditioning of FIM. At
this point, we have decorrelated the hidden features by
satisfying E[hi
d(cid:48)] = I. Then hi
d(cid:48) follows a standard
multivariate Gaussian distribution, hi
d(cid:48) ∼ N (0, I). As
a result, the layer’s output follows a rectiﬁed Gaussian
distribution, which is uncorrelated as presented in remark
1. In post-GWNN, whitening hidden features of the i − 1-
th layer improves conditioning for the i-th layer. To see
this, by following the description in Sec.2.1, the diagonal
block of FIM associated with the i-th layer can be written
as Fii ≈ E[δˆhi(δˆhi)T] ⊗ E[(oi−1
d(cid:48) )T],
where the parameters have low correlations since Fii has a
block diagonal structure.

d(cid:48) − µi−1

d(cid:48) − µi−1

d(cid:48) )(oi−1

ˆW i
2: repeat
3:
4:
5:
6:
7:

Algorithm 2 Training post-GWNN
1: Init: initial θ, αi, βi; and t = 0, tw = k, λ = tw

k ; P i−1 = I,

t = W i, ∀i ∈ {1...(cid:96)}.

for i = 1 to (cid:96) do
update ˆW i
t, and βi
t , αi
if mod(t, τ ) = 0 then

t by SGD.

.

d(cid:48)

o = P i−1

store old P i−1
estimate mean and variance of ˆhi by a minibatch of
N (cid:48) samples or following remark 2 when N (cid:48) = 1.
update P i−1
by online SVD.
t = ˆW i
transform ˆW i
tw = 1 and λ = tw
k .

(P i−1
d(cid:48)

t P i−1
o

)−1.

d(cid:48)

8:
9:
10:
11:
12:
13:
14:
15: until convergence

end if
end for
t = t + 1.
if tw < k then tw = tw + 1 end if

Remark 1. Let h ∼ N (0, I) and o = max(0, Ah + b).
Then E[(oj − E[oj])(ok − E[ok])] ≈ 0 if A is a diagonal
matrix, where j, k index any two entries of o and j (cid:54)= k.

d(cid:48)) and b = βi

For remark 1, we have A = diag(αi
d(cid:48). It
tells us three things. First, by using whitening and BN,
covariance of any two different entries of oi
d(cid:48) approaches
zero. Second, at the iteration when we construct P i
d(cid:48), we
can estimate the full covariance matrix of ˆhi using the
, E[ˆhiˆhiT
mean and variance of oi−1
d(cid:48) −
d(cid:48) )T] ˆW iT
d(cid:48) − µi−1
d(cid:48) )(oi−1
µi−1
. The mean and variance can
be estimated with a minibatch of samples i.e. N (cid:48) (cid:28) N .
Third, to the extreme, when N (cid:48) = 1, these statistics can
still be computed in analytical forms leveraging remark 2.

] = ˆW iE[(oi−1

d(cid:48)

Remark 2. Let a random variable x ∼ N (0, 1) and y =
max(0, ax + b). Then E[y] = a√
2π

2a2 + b

e− b2

2 Ψ(− b√
), where Ψ(x) =

2a

)

e− b2
and E[y2] = ab√
2π
1 − erf(x) and erf(x) is the error function.

2 Ψ(− b√

2a2 + a2+b2

2a

The above remark derives the mean and variance of a
rectiﬁed output unit that has shift and scale parameters. It
generalizes (Arpit et al., 2016) that presented a special case
when a = 1 and b = 0. In that case, we have E[y] = 1√
2π
and Var[y] = E[y2]−E[y]2 = 1
2π , which are consistent
with previous work.

2 − 1

Extensions. Remark 1 and 2 can be extended to other
nonlinear activation functions, such as leaky rectiﬁed unit
deﬁned as leakyrelu(x) = max(0, x)+a min(0, x), where
the slope of the negative part is controlled by the coefﬁcient
a, which is ﬁxed in (Maas et al., 2013) and is learned in (He
et al., 2015).

Generalized Whitened Neural Network

3.3. Training post-GWNN

Similar to pre-GWNN, the learning problem can be formu-
lated as

λL(o(cid:96), y; θ) + (1 − λ) (cid:80)(cid:96)

min
θ
s.t. rank(P i) ≤ d(cid:48), E[hi

d(cid:48)hiT

i=1 Lfeat(hi, ˆhi; θ) (5)

d(cid:48)] = I, i = 1...(cid:96).

Eqn.(5) has two loss functions. Different from WNN
and pre-GWNN where the feature equality constraint can
be satisﬁed in a closed form,
this constraint is treated
as an auxiliary loss function in post-GWNN, deﬁned as
2 (cid:107)(hi − E[hi]) − ˆhi(cid:107)2
Lfeat(hi, ˆhi) = 1
2 and minimized in
the training stage. It does not have an analytical solution
because there is a nonlinear activation function between
the weight matrix and the whitening matrix (i.e.
in the
previous layer). In Eqn.(5), λ is a coefﬁcient that balances
the contribution of two loss functions, and 1 − λ is linearly
decayed as 1 − λ = k−tw
, where tw = 1, 2, ..., k. At each
time after we update the whitening matrix, we start decay
by setting tw = 1 and k indicates the iterations at which
we stop annealing.

k

Alg.2 summarizes the training procedure. It preforms on-
line update of the top d(cid:48) eigenvectors of the whitening ma-
trix similar to pre-GWNN. In comparison, it decreases the
runtime of whitening transformation to O( (N (cid:48)+M )d(cid:48)T (cid:96)
),
which is N +M
N (cid:48)+M fold reduction with respect to pre-GWNN.
For example, when N = M and N (cid:48) = 1, post-GWNN
is capable of reducing computations of pre-GWNN and
WNN by 2× and (2τ (cid:48))× respectively, while maintaining
better conditioning than these alternatives by choosing
small τ (cid:48).

τ (cid:48)

4. Empirical Studies

We compare WNN, pre-GWNN, and post-GWNN in the
following aspects, including a) number of iterations when
training converged, b) computation times for training, and
c) generalization capacities on various datasets. We also
conduct ablation studies with respect to 1) effect of the
number of samples N to estimate the covariance matrix for
pre-GWNN and 2) effect of N (cid:48) for post-GWNN. Finally,
we try to tune the value of d(cid:48).

Datasets. We employ the following datasets.
a) MNIST (Lecun et al., 1998) has 60, 000 28 × 28 images
of 10 handwritten digits (0-9) for training and another
10, 000 test images. 5, 000 images from the training set
are randomly selected as a validation set.
b) CIFAR-10 (Krizhevsky, 2009) consists of 50, 000 32 ×
32 color images for training and 10, 000 images for testing.
Each image is categorized into one of the 10 object labels.
For CIFAR-10, 5, 000 images are chosen for validation.
c) CIFAR-100 (Krizhevsky, 2009) has the same number of

images as CIFAR-10, but each image is classiﬁed into 100
categories. For CIFAR-100, we select 5, 000 images from
training set for validation.
d) SVHN (Netzer et al., 2011) consists of color images of
house numbers collected by Google Street View. The task
is to predict the center digit (0-9) of each image, which is
of size 32×32. There are 73, 257 images in the training set,
26, 032 images for test, and 531, 131 additional examples.
We follow (Sermanet et al., 2012) to build a validation set
by selecting 400 samples per class from the training set and
200 samples per class from the additional set. We didn’t
train on validation, which is for tuning hyperparameters.

Experimental Settings.
We have two settings, an
unsupervised and a supervised learning settings. First,
following (Desjardins et al., 2015), we compare the above
three approaches on the task of minimizing reconstruction
error of an autoencoder on MNIST. The encoder consists
layers, which have 1000, 500, 256,
of 4 fc sigmoidal
and 30 hidden neurons respectively.
The decoder is
symmetric and untied with respect to the encoder. Second,
for the task of image classiﬁcation on CIFAR-10, -100,
and SVHN, we employ the same network-in-network
(NIN)
(Lin et al., 2014) architecture, which has 9
convolutional layers and 3 pooling layers deﬁned as2:
conv(192, 5)-conv(160, 1)-maxpool(3, 2)-conv(96, 1)-
conv(192, 5)-conv(192, 1)-avgpool(3, 2)-conv(192, 1)-
conv(192, 5)-conv(192, 1)-conv(l, 1)-avgpool(8, 8),
where l = 10 for CIFAR-10 and SVHN and l = 100 for
CIFAR-100. For all models, we use SGD with momentum
of 0.9.

4.1. Comparisons of Convergence and Computations

We record the number of epochs and computation time,
when training WNN, pre-, and post-GWNN on MNIST
and CIFAR-100, respectively. We employ the ﬁrst setting
above for MNIST and the second setting for CIFAR-
100. For both settings, hyperparamters are chosen by grid
search on the validation sets. The search speciﬁcations of
minibatch size, learning rate, and whitening interval τ are
{64, 128, 256}, {0.1, 0.01, 0.001}, and {20, 50, 100, 103},
respectively. In particular, for WNN and pre-GWNN, the
number of samples used to estimate the covariance matrix,
N , is picked up from {103, 104
2 , 104}. For post-GWNN,
N (cid:48) is chosen to be the same as the minibatch size and the
decay period k = 0.1τ . For a fair comparison, we report
the best performance on validation set for each approach,
and didn’t employ any data augmentation such as random
image cropping and ﬂipping.

2The ‘conv’, ‘maxpool’, and ‘avgpool’ represent convolution,
max pooling, and average pooling respectively. Each convolu-
tional layer is deﬁned as conv(number of ﬁlters, ﬁlter size).
For each pooling layer, we have pool(kernel size, stride). All
convolutions have stride 1.

Generalized Whitened Neural Network

Figure 4. Training of WNN, pre-, post-GWNN on CIFAR-100 (a,b) and MNIST (c,d). (a) and (b) plot the convergence and computation
time on the validation and test set of CIFAR-100 respectively. (c) and (d) report corresponding results on MNIST.

The convergence and computation time are reported in
important observations.
Fig.4 (a-d). We have several
First, all three approaches converge much faster than the
canonical network trained by SGD. Second, pre- and post-
GWNN achieve better convergence than WNN on both
datasets as shown in (a) and (c). Moreover, post-GWNN
outperforms pre-GWNN. Third, post-GWNN signiﬁcantly
reduces computation time compared to all the other meth-
ods, as illustrated in (b) and (d). We see that although
WNN reduces the number of epochs, it takes long time to
train because its whitening transformation occupies large
computations.

4.2. Performances on various Datasets

We evaluate WNN, pre-, and post-GWNN on CIFAR-10,
-100, and SVHN datasets, and compare their classiﬁcation
accuracies to existing state-of-the-art methods. For all
the datasets and approaches, we utilize the same network
structure as mentioned in the second setting above. For
two CIFAR datasets, we adopt minibatch size 64 and initial
learning rate 0.1, which is reduced by half after every 25
epochs. We train for 250 epochs. As SVHN is a large
dataset, we train for 100 epochs with minibatch size 128
and initial learning rate 0.05, which is reduced by half after
every 10 epochs. We train on CIFAR-10 and -100 using
both without and with data augmentation, which includes
random cropping and horizontal ﬂipping. For SVHN, we
didn’t augment data following (Sermanet et al., 2012).
For all the methods, we shufﬂe samples at the beginning
of every epoch. We use N = 104
for WNN and pre-
2
GWNN and N (cid:48) = 64 for post-GWNN. For both pre-
and post-GWNN, we have M = N and d(cid:48) = d
2 . The
other experimental settings are similar to Sec.4.1. Table
1 shows the results. We see that pre- and post-GWNN
consistently achieve better results than those of WNN, and
also outperform previous state-of-the-art approaches.

Figure 5. Training of pre- and post-GWNN on CIFAR-100. (a)
visualizes the impact of different values of N for pre-GWNN,
showing that performance degrades when N is small. (b) plots
the impact of N (cid:48) for post-GWNN, which is insensitive to small
values of N (cid:48).

4.3. Ablation Studies

The following experiments are conducted on CIFAR-100
using pre- or post-GWNN. The ﬁrst
two experiments
follow the setting as mentioned in Sec.4.1. First, we
evaluate the effect of the number of samples, N , used to
estimate the covariance matrix in pre-GWNN. We compare
performances of using different values of N picked up
from {102, 103, 3 × 103, 5 × 103, 104}. Fig.5 (a) plots the
results. We see that performance can drop because of ill
conditioning when N is small e.g. N = 100. When it is
too large e.g. N = 104, we observe slightly overﬁtting.
Second, Fig.5 (b) highlights the effect of N (cid:48)
in post-
GWNN. We see that post-GWNN can work reasonably
well when N (cid:48) is small.

instead of treating d(cid:48) = d

2 as a constant in
Finally,
training, we study the effect of tuning its value on
the validation set using a simple heuristic strategy.
If
the validation error reduces more than 2% over 4 con-
secutive evaluations, we have d(cid:48) = d(cid:48) − rate × d(cid:48).

00.10.20.30.40.50.60.70.805101520253035test	errorminutesSGDpost‐GWNNpre‐GWNNWNN00.10.20.30.40.50.60.70.8024681012141618validation	errorepochsSGDpost‐GWNNpre‐GWNNWNN0.40.50.60.70.80.9100.20.40.60.811.21.4test	errorhourSGDpost‐GWNNpre‐GWNNWNN0.40.50.60.70.80.9103691215182124validation	errorepochsSGDpost‐GWNNpre‐GWNNWNN(a)(b)(c)(d)0.40.50.60.70.80.903691215182124epochspost‐GWNN	N'(cid:3404)128post‐GWNN	N'(cid:3404)64post‐GWNN	N'(cid:3404)10.40.50.60.70.80.9024681012141618202224validation	errorepochspre‐GWNN	N(cid:3404)100pre‐GWNN	N(cid:3404)1000pre‐GWNN	N(cid:3404)3000pre‐GWNN	N(cid:3404)5000pre‐GWNN	N(cid:3404)10000(a)(b)Generalized Whitened Neural Network

5. Conclusion

Table 1. Comparisons of test errors on various datasets. The top
two performances are highlighted for each dataset.

n NIN (Lin et al., 2014)

CIFAR-10

n NIN (Lin et al., 2014)

CIFAR-100

Acknowledgements

o
i
t
a
t
n
e
m
g
u
a

t
u
o
h
t
i

w

n
o
i
t
a
t
n
e
m
g
u
a

h
t
i

w

o
i
t
a
t
n
e
m
g
u
a

t
u
o
h
t
i

w

n
o
i
t
a
t
n
e
m
g
u
a

h
t
i

w

NIN+ALP (Agostinelli et al., 2015)
Normalization Propagation (Arpit et al., 2016)
BatchNorm (Ioffe & Szegedy, 2015)
DSN (Lee et al., 2015)
Maxout (Goodfellow et al., 2013)
WNN (Desjardins et al., 2015)
pre-GWNN
post-GWNN
NIN (Lin et al., 2014)
NIN+ALP (Agostinelli et al., 2015)
Normalization Propagation (Arpit et al., 2016)
BatchNorm (Ioffe & Szegedy, 2015)
DSN (Lee et al., 2015)
Maxout (Goodfellow et al., 2013)
WNN (Desjardins et al., 2015)
pre-GWNN
post-GWNN

NIN+ALP (Agostinelli et al., 2015)
Normalization Propagation (Arpit et al., 2016)
BatchNorm (Ioffe & Szegedy, 2015)
DSN (Lee et al., 2015)
Maxout (Goodfellow et al., 2013)
WNN (Desjardins et al., 2015)
pre-GWNN
post-GWNN
NIN (Lin et al., 2014)
NIN+ALP (Agostinelli et al., 2015)
Normalization Propagation (Arpit et al., 2016)
BatchNorm (Ioffe & Szegedy, 2015)
DSN (Lee et al., 2015)
WNN (Desjardins et al., 2015)
pre-GWNN
post-GWNN

SVHN
NIN (Lin et al., 2014)
NIN+ALP (Agostinelli et al., 2015)
Normalization Propagation (Arpit et al., 2016)
BatchNorm (Ioffe & Szegedy, 2015)
DSN (Lee et al., 2015)
Maxout (Goodfellow et al., 2013)
WNN (Desjardins et al., 2015)
pre-GWNN
post-GWNN

Error(%)
10.47
9.59
9.11
9.41
9.69
11.68
9.87
9.34
8.97
8.81
7.51
7.47
7.25
7.97
9.38
8.02
7.38
7.10
Error(%)
35.68
34.40
32.19
35.32
34.57
38.57
34.78
32.08
31.10
33.37
30.83
29.24
30.26
32.16
30.02
28.78
28.10
Error(%)
2.35
2.04
1.88
2.25
1.92
2.47
1.93
1.82
1.74

is

the error has no re-
If
duction over this period,
d(cid:48)
increased by the
same rate as above. We
use post-GWNN and fol-
low experimental setting in
Sec.4.2. We take two d-
ifferent rates {0.1, 0.2} as
examples. Fig.6 plots the
variations of dimensions
when d = 192 and shows
their test errors. We ﬁnd
that keeping d(cid:48) as a constant generally produces better
result than those obtained by the above strategy, but this
strategy yields less runtime because more dimensions are
pruned.

Figure 6. Effect of tuning d(cid:48).

We presented generalized WNN (GWNN) to reduce run-
time and improve generalization of WNN. Different from
WNN that reduces computation time by whitening with a
large period, leading to ill conditioning of FIM, GWNN
learns compact internal representation, such that SVD is
approximated by the top eigenvectors in an online manner,
making GWNN not only reduces computations but also
improves generalization. By exploiting the knowledge of
the hidden representation’s distribution, we showed that
post-GWNN is able to compute the covariance matrix in
a closed form, which can be also extended to the other
activation function. Extensive experiments demonstrated
the effectiveness of GWNN.

This work is partially supported by the National Natu-
ral Science Foundation of China (61503366, 61472410,
U1613211), the National Key Research and Developmen-
t Program of China (No.2016YFC1400700), the Exter-
nal Cooperation Program of BIC, Chinese Academy of
Sciences (No.172644KYSB20160033), and the Science
and Technology Planning Project of Guangdong Province
(2015B010129013, 2014B050505017).

References

Agostinelli, Forest, Hoffman, Matthew, Sadowski, Peter,
Learning activation functions to

and Baldi, Pierre.
improve deep neural networks. In ICLR, 2015.

Amari, Shun-ichi and Nagaoka, Hiroshi. Methods of
In Tanslations of Mathematical

information geometry.
Monographs, 2000.

Arpit, Devansh, Zhou, Yingbo, Kota, Bhargava U., and
Govindaraju, Venu. Normalization propagation: A
parametric technique for removing internal covariate
shift in deep networks. In ICML, 2016.

Desjardins, Guillaume, Simonyan, Karen, Pascanu, Raz-
van, and Kavukcuoglu, Koray. Natural neural networks.
In NIPS, 2015.

Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi,
Courville, Aaron, and Bengio, Yoshua. Maxout network-
s. In ICML, 2013.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Delving deep into rectiﬁers: Surpassing human-
level performance on imagenet classiﬁcation. In ICCV,
2015.

Ioffe, Sergey and Szegedy, Christian. Batch normalization:

04080120160050100150200250dimensionsepochsrate(cid:3404)0.1	(cid:4666)28.57(cid:4667)rate(cid:3404)0.2	(cid:4666)29.79(cid:4667)Generalized Whitened Neural Network

Accelerating deep network training by reducing internal
covariate shift. In ICML, 2015.

Krizhevsky, Alex. Learning multiple layers of features

from tiny images. In Technical Report, 2009.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In NIPS, 2012.

Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
based learning applied to document recognition.
In
Proceeding of IEEE, 1998.

LeCun, Yann, Bottou, Leon, Orr, Genevieve B., and Mller,
Klaus Robert. Efﬁcient backprop. In Neural Networks:
Tricks of the Trade, 2002.

Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick, Zhang,
Zhengyou, and Tu, Zhuowen. Deeply-supervised nets.
In AISTATS, 2015.

Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in

network. In ICLR, 2014.

Maas, Andrew L., Hannun, Awni Y., , and Ng, Andrew Y.
Rectiﬁer nonlinearities improve neural network acoustic
models. In ICML, 2013.

Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B.,
and Ng, A. Y. Reading digits in natural images with
In NIPS Workshop on
unsupervised feature learning.
Deep Learning and Unsupervised Feature Learning,
2011.

Povey, Daniel, Zhang, Xiaohui, and Khudanpur, Sanjeev.
Parallel training of dnns with natural gradient and pa-
rameter averaging. In ICLR workshop, 2015.

Raiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep
learning made easier by linear transformations in per-
ceptrons. In AISTATS, 2012.

Sermanet, Pierre, Chintala, Soumith, and LeCun, Yann.
Convolutional neural networks applied to house numbers
digit classiﬁcation. In arXiv:1204.3968, 2012.

Shamir, Ohad. A stochastic pca and svd algorithm with an

exponential convergence rate. In ICML, 2015.

Tieleman, Tijmen and Hinton, Geoffrey. Rmsprop: Divide
the gradient by a running average of its recent magni-
tude. In Neural Networks for Machine Learning (Lecture
6.5), 2012.

Zhang, Xiangyu, Zou, Jianhua, He, Kaiming, and Sun,
Jian. Accelerating very deep convolutional networks for
In IEEE Transactions on
classiﬁcation and detection.
Pattern Analysis and Machine Intelligence, 2015.

