Forest-type Regression with General Losses
and Robust Forest

Alexander Hanbo Li 1 Andrew Martin 2

Abstract

This paper introduces a new general framework
for forest-type regression which allows the de-
velopment of robust forest regressors by select-
ing from a large family of robust loss functions.
In particular, when plugged in the squared error
and quantile losses, it will recover the classical
random forest (Breiman, 2001) and quantile ran-
dom forest (Meinshausen, 2006). We then use ro-
bust loss functions to develop more robust forest-
type regression algorithms. In the experiments,
we show by simulation and real data that our ro-
bust forests are indeed much more insensitive to
outliers, and choosing the right number of nearest
neighbors can quickly improve the generalization
performance of random forest.

1. Introduction

Since its development by Breiman (2001), random forest
has proven to be both accurate and efﬁcient for classiﬁca-
tion and regression problems.
In regression setting, ran-
dom forest will predict the conditional mean of a response
variable by averaging predictions of a large number of re-
gression trees. Later then, many other machine learning
algorithms were developed upon random forest. Among
them, robust versions of random forest have also been pro-
posed using various methodologies. Besides the sampling
idea (Breiman, 2001) which adds extra randomness, the
other variations are mainly based on two ideas: (1) use
more robust criterion to construct regression trees (Galim-
berti et al., 2007; Brence & Brown, 2006; Roy & Larocque,
2012); (2) choose more robust aggregation method (Mein-
shausen, 2006; Roy & Larocque, 2012; Tsymbal et al.,
2006).

Meinshausen (2006) generalized random forest to pre-

1University of California at San Diego, San Diego, Califor-
nia, USA 2Zillow, Seattle, Washington, USA. Correspondence to:
Alexander Hanbo Li <alexanderhanboli@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

dict quantiles by discovering that besides calculating the
weighted mean of the observed response variables, one
could also get information for the weighted distribution of
observed response variables using the sets of local weights
generated by random forest. This method is strongly con-
nected to the adaptive nearest neighbors procedure (Lin &
Jeon, 2006) which we will brieﬂy review in section 1.2.
Different from classical k-NN methods that rely on pre-
deﬁned distance metrics, the dissimilarities generated by
random forest are data dependent and scale-invariant.

Another state-of-the-art algorithm AdaBoost (Freund &
Schapire, 1995; Freund et al., 1996) has been generalized
to be applicable to a large family of loss functions (Fried-
man, 2001; Mason et al., 1999; Li & Bradic, 2016). Recent
development of more ﬂexible boosting algorithms such as
xgboost (Chen & Guestrin, 2016) have become the go-to
forest estimators with tabular or matrix data. One way in
which recent boosting algorithms have an advantage over
the random forest is the ability to customize the loss func-
tion used to reduce the inﬂuence of outliers or optimize a
metric more suited to the speciﬁc problem other than the
mean squared error.

In this paper, we will propose a general framework for
forest-type regression which can also be applied to a broad
family of loss functions.
It is claimed in (Meinshausen,
2006) that quantile random forest is another nonparamet-
ric approach which does not minimize an empirical loss.
However, we will show in fact both random forest and
quantile random forest estimators can be re-derived as re-
gression methods using the squared error or quantile loss
respectively in our framework.
Inspired by the adaptive
nearest neighbor viewpoint, we explore how random forest
makes predictions using the local weights generated by en-
semble of trees, and connect that with locally weighted re-
gression (Fan & Gijbels, 1996; Tibshirani & Hastie, 1987;
Staniswalis, 1989; Newey, 1994; Loader, 2006; Hastie &
Loader, 1993). The intuition is that when predicting the
target value (e.g. E[Y |X = x]) at point x, the observations
closer to x should receive larger weights. Different from
predeﬁning a kernel, random forest assigns the weights
data dependently and adaptively. After we illustrate the re-
lation between random forest and local regression, we will
use random forest weights to design other regression algo-

Forest-type Regression with General Losses and Robust Forest

rithms. By plugging robust loss functions like Huber loss
and Tukey’s redescending loss, we get forest-type regres-
sion methods that are more robust to outliers. Finally, mo-
tivated from the truncated squared error loss example, we
will show that decreasing the number of nearest neighbors
in random forest will also immediately improve its gener-
alization performance.

The layout of this paper is as follows. In Section 1.1 and 1.2
we review random forest and adaptive nearest neighbors.
Section 2 introduces the general framework of forest-type
regression. In Section 3 we plug in robust regression loss
functions to get robust forest algorithms. In Section 4 we
motivate from the truncated squared error loss and inves-
tigate the importance of choosing right number of nearest
neighbors. Finally, we test our robust forests in Section
5 and show that they are always superior to the traditional
formulation in the presence of outliers in both synthetic and
real data set.

1.1. Random forest

Following the notation of Breiman (2001), let θ be the ran-
dom parameter determining how a tree is grown, and data
(X, Y ) ∈ X × Y. For each tree T (θ), let L be the total
number of leaves, and Rl denotes the rectangular subspace
in X corresponding to the l-th leaf. Then for every x ∈ X ,
there is exactly one leaf l such that x ∈ Rl. Denote this
leaf by l(x, θ).

For each tree T (θ), the prediction of a new data point
X = x is the average of data values in leaf l(x, θ), that
is, (cid:98)Y (x, θ) = (cid:80)n
j=1 w(Xi, x, θ)Yi, where

w(Xi, x, θ) =

1I{Xi∈Rl(x,θ)}
#{j : Xj ∈ Rl(x,θ)}

.

(1)

Finally, the conditional mean E[Y |X = x] is approxi-
mated by the averaged prediction of m trees, (cid:98)Y (x) =
m−1 (cid:80)m
t=1 (cid:98)Y (x, θt). After rearranging the terms, we can
write the prediction of random forest as

(cid:98)Y (x) =

w(Xi, x)Yi,

(2)

where the averaged weight w(Xi, x) is deﬁned as

w(Xi, x) =

w(Xi, x, θt).

(3)

From equation (2), the prediction of the conditional expec-
tation E[Y |X = x] is the weighted average of the response
values of all observations. Furthermore, it is easy to show
that (cid:80)n

i=1 w(Xi, x) = 1.

n
(cid:88)

i=1

1
m

m
(cid:88)

t=1

1.2. Adaptive nearest neighbors

Lin and Jeon (2006) studies the connection between ran-
dom forest and adaptive nearest neighbor. They introduced
the so-called potential nearest neighbors (PNN): A sample
point xi is called a k-PNN to a target point x if there exists
a monotone distance metric under which xi is among the k
closest to x among all the sample points.

Therefore, any k-NN method can be viewed as choosing
k points from the k-PNNs according to some monotone
metric. For example, under Euclidean metric, the classical
k-NN algorithm sorts the observations by their Euclidean
distances to the target point and outputs the k closest ones.
This is equivalent to weighting the k-PNNs using inverse
L2 distance.

More interestingly, they prove that those observations with
positive weights (3) all belong to the k-PNNs (Lin & Jeon,
2006). Therefore, random forests is another weighted k-
PNN method, but it assigns weights to the observations dif-
ferent from any k-NN method under a pre-deﬁned mono-
tonic distance metric. In fact, the random forest weights
are adaptive to the data if the splitting scheme is adaptive.

2. General framework for forest-type

regression

In this section, we generalize the classical random forest to
a general forest-type regression (FTR) framework which is
applicable to a broad family of loss functions. In Section
2.1, we motivate the framework by connecting random for-
est predictor with locally weighted regression. Then in Sec-
tion 2.2, we formally propose the new forest-type regres-
sion framework. In Section 2.3, we rediscover the quantile
random forest estimator by plugging the quantile loss func-
tion into our framework.

2.1. Squared error and random forest

Classical random forest can be understood as an estimator
of conditional mean E[Y |X]. As shown in (2), the esti-
mator (cid:98)Y (x) is weighted average of all response Yi’s. This
special form reminds us of the classical least squares re-
gression, where the estimator is the sample mean. To be
more precise, we rewrite (2) as

n
(cid:88)

i=1

w(Xi, x)(Yi − (cid:98)Y (x)) = 0.

(4)

Equation (4) is the estimating equation (ﬁrst order condi-
tion) of the locally weighted least squares regression (Rup-
pert & Wand, 1994):

(cid:98)Y (x) = argmin

λ∈R

n
(cid:88)

i=1

w(Xi, x)(Yi − λ)2

(5)

Forest-type Regression with General Losses and Robust Forest

In classical local regression, the weight w(Xi, x) serves
as a local metric between the target point x and observa-
tion Xi. Intuitively, observations closer to target x should
be given more weights when predicting the response at
x. One common choice of such local metric is kernel
Kh(Xi, x) = K((Xi − x)/h). For example, the tricube
kernel K(u) = (1 − |u|3)3 1I(|u| ≤ 1) will ignore the im-
pact of observations outside a window centered at x and in-
crease the weight of an observation when it is getting closer
to x. The form of kernel-type local regression is as follows:

argmin
λ∈R

n
(cid:88)

i=1

Kh(Xi − x)(Yi − λ)2,

The random forest weight w(Xi, x) (3) deﬁnes a similar
data dependent metric, which is constructed using the en-
semble of regression trees. Using an adaptive splitting
scheme, each tree chooses the most informative predictors
from those at its disposal. The averaging process then as-
signs positive weights to these training responses, which
are called voting points in (Lin & Jeon, 2006). Hence
via the random forest voting mechanism, those observa-
tions close to the target point get assigned positive weights
equivalent to a kernel functionality (Friedman et al., 2001).

2.2. Extension to general loss

Note that the formation (5) is just a special case when using
squared error loss φ(a, b) = (a−b)2. In more general form,
we have the following local regression problem:

(cid:98)Y (x) = argmin

w(Xi, x)φ(s(Xi), Yi)

(6)

n
(cid:88)

i=1

s∈F

where w(Xi, x) is a local weight, F is a family of func-
tions, and φ(·) is a general loss. For example, when local
weight is a kernel and F stands for polynomials of a cer-
tain degree, it reduces to local polynomial regression (Fan
& Gijbels, 1996). Random forest falls into this framework
with squared error loss, a family of constant functions and
local weights (3) constructed from ensemble of trees.

Algorithm 1 Forest-type regression

Step 1: Calculate local weights w(Xi, x) using ensem-
ble or trees.
Step 2: Choose a loss φ(·, ·) and a family F of function.
Then do the locally weighted regression

uses ensemble of trees to recursively partition the covariate
space X . However, there are many other data dependent
dissimilarity measures that can potentially be used, such as
k-NN, mp-dissimilarity (Aryal et al., 2014), shared near-
est neighbors (Jarvis & Patrick, 1973), information-based
similarity (Lin et al., 1998), mass-based dissimilarity (Ting
et al., 2016), etc. And there are many other domain speciﬁc
dissimilarity measures. To avoid distraction, we will only
use random forest weights throughout the rest of this paper.

2.3. Quantile loss and quantile random forest

Meinshausen (2006) proposed the quantile random forest
which can extract the information of different quantiles
rather than just predicting the average. It has been shown
that quantile random forest is more robust than the classi-
cal random forest (Meinshausen, 2006; Roy & Larocque,
2012). In this section, we show quantile random forest es-
It is well
timator is also a special case of Algorithm 1.
known that the τ -th quantile of an (empirical) distribution
is the constant that minimizes the (empirical) risk using τ -
th quantile loss function ρτ (z) = z(τ − 1I{z<0}) (Koenker,
2005). Now let the loss function in Algorithm 1 be the
quantile loss ρτ (·), F be the family of constant functions,
and w(Xi, x) be random forest weights (3). Solving the
optimization problem

(cid:98)Yτ (x) = argmin

w(Xi, x)ρτ (Yi − λ),

n
(cid:88)

i=1

λ∈R

we get the corresponding ﬁrst order condition

w(Xi, x)(τ − 1I {Yi − (cid:98)Yτ (x) < 0}) = 0.

Recall that (cid:80)n

i=1 w(Xi, x) = 1, hence, we have

n
(cid:88)

i=1

n
(cid:88)

i=1

w(Xi, x) 1I {Yi < (cid:98)Yτ (x)} = τ.

(7)

The estimator (cid:98)Yτ (x) in (7) is exactly the same estimator
proposed in (Meinshausen, 2006). In particular, when τ =
0.5, the equation (cid:80)n
i=1 w(Xi, x) 1I {Yi < (cid:98)Y0.5(x)} = 0.5
will give us the median estimator (cid:98)Y0.5(x). Therefore, we
have rediscovered quantile random forest from a totally dif-
ferent point of view as a local regression estimator with
quantile loss function and random forest weights.

(cid:98)Y (x) = argmin

w(Xi, x)φ(Yi, s(Xi)).

n
(cid:88)

i=1

s∈F

3. Robust forest

In Algorithm 1, we summarize the forest-type regression
as a general two-step method. Note that here we only fo-
cus on local weights generated by random forest, which

From the framework 1, quantile random forest is insensi-
tive to outliers because of the more robust loss function. In
this section, we test our framework on other robust losses
and proposed ﬁxed-point method to solve the estimating

Forest-type Regression with General Losses and Robust Forest

equation. In Section 3.1 we choose the famous robust loss
– (pseudo) Huber loss, and in Section 3.2, we further inves-
tigate a non-convex loss – Tukey’s biweight.

3.1. Huber loss

The Huber loss (Huber et al., 1964)

Hδ(y) =

(cid:40) 1

2 y2
δ(|y| − 1

2 δ)

for |y| ≤ δ,
elsewhere

is a well-known loss function used in robust regression.
The penalty acts like squared error loss when the error is
within [−δ, δ] but becomes linear outside this range. In this
way, it will penalize the outliers more lightly but still pre-
serves more efﬁciency than absolute deviation when data
is concentrated in the center and has light tails (e.g. Nor-
mal). By plugging Huber loss into the FTR framework 1,
we get a robust counterpart of random forest. The estimat-
ing equation is

n
(cid:88)

i=1

wi(x) sign( (cid:98)Y (x) − Yi) min( (cid:98)Y (x) − Yi, δ) = 0.

(8)

Direct optimization of (8) with local weights is hard, hence
instead we will investigate the pseudo-Huber loss (see Fig-
ure 1),

Lδ(y) = δ2

(cid:32)(cid:114)

(cid:33)

1 +

(cid:17)2

(cid:16) y
δ

− 1

which is a smooth approximation of Huber loss (Charbon-
nier et al., 1997). The estimating equation

(cid:16)

wpH
i

(x)

(cid:98)YpH (x) − Yi

= 0.

(cid:17)

(9)

n
(cid:88)

i=1

is very similar to that of square error loss if we deﬁne a new
weight

Then the (pseudo) Huber estimator can be expressed as

wpH
i

(x) =

(cid:114)

wi(x)
(cid:16) (cid:98)YpH (x)−Yi
δ

.

(cid:17)2

1 +

(cid:98)YpH (x) =

(cid:80)n

i=1 wpH
(cid:80)n
i=1 wpH

i

i

(x)Yi
(x)

.

Informally, the estimator (11) can be viewed as a weighted
average of all the responses Yi’s. From (10), we know the
new weight for pseudo-Huber loss has an extra scaling fac-
tor

(cid:16)(cid:112)1 + (δ−1u)2

(cid:17)−1

(12)

2 x2 and
Figure 1. In the ﬁrst row, we compare squared error loss 1
pseudo-Huber loss with different δ. In the second row, we plot
the scaling factor (12) of Huber loss. We observe that as δ de-
creases to zero, the Huber loss becomes more linear and ﬂat, and
the scaling factor shrinks more quickly as the input deviates from
zero.

(10)

(11)

and hence will shrink more to zero whenever δ−1| (cid:98)YpH (x)−
Yi| is large. The tuning parameter δ acts like a control of the
level of robustness. A smaller δ will lead to more shrinkage
on the weights of data that have responses far away from
the estimator.

The estimating equation (9) can be solved by ﬁx-point
method which we propose in Algorithm 2. For notation
simplicity, we will use wi,j to denote w(Xi, xj), where Xi
is the i-th training point and xj is the j-th testing point. The
convergence to the unique solution (if exists) is guaranteed
by Lemma 1.

Lemma 1. Deﬁne

Kδ(y) =

(cid:80)n

(cid:80)n

i=1

(cid:113)

i=1

(cid:113)

wiYi
1+( y−Yi
wi
1+( y−Yi

δ

δ

)2

)2

,

Forest-type Regression with General Losses and Robust Forest

Algorithm 2 pseudo-Huber loss (δ)

Input: Test points {xj}m
local weights wi,j, training responses {Yi}n
tolerance (cid:15)0.
while (cid:15) > (cid:15)0 do

j=1, initial guess { (cid:98)Y (0)(xj)},
i=1, and error

(a) Update the weights

w(k)

i,j =

(cid:114)

wi,j

1 +

(cid:16) (cid:98)Y (k−1)(xj )−Yi
δ

(cid:17)2

(b) Update the estimator

(cid:98)Y (k)(xj) =

(cid:80)n

i=1 w(k)
i,j Yi
(cid:80)n
i=1 w(k)

i,j

(c) Calculate error

(cid:15) =

1
m

m
(cid:88)

(cid:16)

j=1

(cid:98)Y k(xj) − (cid:98)Y (k−1)(xj)

(cid:17)2

(d) k ← k + 1

end while
Output the pseudo-Huber estimator:

(cid:98)YpH (xj) = (cid:98)Y (k)(xj)

where (cid:80)n
i=1 wi = 1. Let K = maxi=1,··· ,n |Yi|. Then
Algorithm 2 can be written as (cid:98)Y (k)(x) = Kδ( (cid:98)Y (k−1)),
and converges exponentially to a unique solution as long
as δ > 2K.

From Lemma 1, we know it is important to standardize the
responses Yi so that δ will be of the same scale for different
problems. In practice, we observe that one will not need to
choose δ that satisﬁes the worst-case condition δ > K in
order for convergence, but making δ too small does lead
to slow convergence rate. For assigning the initial guess
(cid:98)Y (0), two simplest ways are to either take the random forest
estimator we got or a constant vector equaling to the sample
mean. Throughout the rest of this paper, we will choose the
weights to be random forest weights (3).

3.2. Tukey’s biweight

Non-convex function has played an important role in the
context of robust regression (Huber, 2011; Hampel et al.,
2011). Unlike convex losses, the penalization on the er-
rors can be bounded and hence the contribution of out-
liers in the estimating equation will eventually vanish. Our
forest regression framework 1 also incorporates the non-
convex losses which will show through the Tukey’s bi-
weight function Tδ(·) (Huber, 2011), which is an example

Figure 2. We plot the scaling factor (13) of Tukey’s biweight.
Compared to Huber scaling factor (see (12)), it has a hard thresh-
old at δ.

of redescending loss whose derivative will vanish to zero
as the input goes outside the interval [−δ, δ]. It is deﬁned
in the following way:

d
dy

Tδ(y) =




y



0

(cid:16)

1 − y2
δ2

(cid:17)2

for |y| ≤ δ,

elsewhere.

Similarly, by rearranging the estimating equation, we have

(cid:98)Ytukey(x) =

(cid:80)n
i=1 wtukey(Xi, x)Yi
(cid:80)n
i=1 wtukey(Xi, x)

where

wtukey(Xi, x) = w(Xi, x) max

1 −






(cid:32)

(cid:98)Ytukey − Yi
δ

(cid:33)2






, 0

with an extra scaling factor (see Figure 2)

(cid:26)

max

1 −

(cid:17)2

(cid:27)

, 0

.

(cid:16) u
δ

(13)

the ﬁnal estimator actually only de-
In another word,
pends on data with responses inside [−δ, δ], and the impor-
tance of any data (Xi, Yi) will be shrinking to zero when
| (cid:98)Ytukey(x) − Yi| gets closer to the boundary value δ.

4. Truncated squared loss and nearest

neighbors

In this section, we will further use the framework 1 to inves-
tigate truncated squared error loss, and use this example to
motivate the relation between random forest generalization
performance and the number of adaptive nearest neighbors.

Forest-type Regression with General Losses and Robust Forest

4.1. Truncated squared error

For the truncated squared error loss

Sδ(y) =

(cid:40) 1

2 y2
2 δ2

1

for |y| ≤ δ,
elsewhere

the corresponding estimating equation is

(cid:88)

w(Xi, x)( (cid:98)Ytrunc(x) − Yi) = 0.

| (cid:98)Ytrunc(x)−Yi|≤δ

If we deﬁne a new weight

wtrunc(Xi, x) = w(Xi, x) 1I{| (cid:98)Ytrunc(x) − Yi| ≤ δ}, (14)

then the estimator for truncated squared loss is
(cid:80)n
i=1 wtrunc(Xi, x)Yi
(cid:80)n
i=1 wtrunc(Xi, x)

(cid:98)Ytrunc(x) =

.

(15)

The estimator (15) is like a trimmed version of the random
forest estimator (2). We ﬁrst sort {Yi}n
i=1 and trim off the
responses where | (cid:98)Ytrunc(x) − Yi| > δ. Therefore, for any
truncation level δ, the estimator (cid:98)Ytrunc(x) only depends on
data satisfying | (cid:98)Ytrunc(x) − Yi| ≤ δ with the same local
random forest weights (1).

4.2. Random Forest Nearest Neighbors

In classical random forest, all the data with positive weights
(3) are included when calculating the ﬁnal estimator (cid:98)Y (x).
However, from section 4.1, we know in order to achieve
robustness, some of the data should be dropped out of con-
sideration. For example, using the truncated squared er-
ror loss, we will only consider the data satisfying |Yi −
(cid:98)Ytrunc(x)| ≤ δ.
In classical random forest, the crite-
rion of tree split is to reduce the mean squared error, then
in most cases, data points inside one terminal node will
tend to have more similar responses. So informally larger
| (cid:98)Ytrim(x)−Yi| will indicate smaller local weight w(Xi, x).
Therefore, instead of solving for (15), we investigate a re-
lated estimator

(cid:98)Ywt(x) =

(cid:80)

w(Xi,x)≥(cid:15) w(Xi, x)Yi
(cid:80)
w(Xi,x)≥(cid:15) w(Xi, x)

(16)

where (cid:15) > 0 is a constant in (0, 1). Recall that in (Lin
& Jeon, 2006), they show all the observations with posi-
tive weights are considered voting points for random forest
estimator. However, (16) implies that we should drop ob-
servations with weights smaller than a threshold in order
for the robustness. More formally, let σ be a permutation
such that w(Xσ(1), x) ≥ · · · ≥ w(Xσ(n0), x) > 0, then (2)
is equivalent to

(cid:98)Y (x) =

w(Xσ(i), x)Yσ(i).

n0(cid:88)

i=1

Then we can deﬁne the k random forest nearest neighbors
(k-RFNN) of x to be {Xσ(1), · · · , Xσ(k)}, k ≤ n0, and get
predictor

(cid:98)Yk(x) =

(cid:101)w(Xσ(i), x)Yσ(i),

(17)

k
(cid:88)

i=1

where (cid:101)w(Xσ(i), x) = w(Xσ(i), x)/ (cid:80)k
j=1 w(Xσ(i), x). In
the numerical experiments (Section 5.3), we will test the
performance of the estimator (17) with different k, and
show that by merely choosing the right number of near-
est neighbors, one can largely improve the performance of
classical random forest.

Shi and Horvath (2006) proposed a similar ensemble tree
based nearest neighbor method. In their approach, if the
observations Xi and Xj lie in the same leaf, then the sim-
ilarity between them is increased by one. At the end, the
similarities are normalized by dividing the total number
of trees in the forest. Therefore, their weights (similar-
ities) w(Xi, x) will be m−1 (cid:80)m
t=1 1I{Xi∈Rl(x,θ)} contrast
to (3). So different from their approach, for random for-
est, the similarity between Xi and Xj will be increased by
1/#{p : Xp ∈ Rl(Xi,θ)} if they both lie in the same leaf
l(Xi, θ). This means the increment in the similarity also
depends on the number of data points in the leaf.

5. Experiments

In this section, we plug in the quantile loss, Huber loss and
Tukey’s biweight loss into the general forest framework
and compare these algorithms with random forest. Unless
otherwise stated, for both Huber and Tukey forest, the error
tolerance is set to be 10−6, and every forest is an ensemble
of 1000 trees with maximum terminal node size 10. The
robust parameter δ are set to be 0.005 and 0.8 for Huber
and Tukey forest, respectively.

5.1. One dimensional toy example

We generate 1000 training data points from a Uniform dis-
tribution on [−5, 5] and another 1000 testing points from
the same distribution. The true underlying model is Y =
X 2 + (cid:15), (cid:15) ∼ N (0, 1). But on the training samples, we
choose 20% of the data and add noise 2T2 to the responses,
where T2 follows t-distribution with degree of freedom 2.

In Figure 3, we plot the true squared curve and different
forest predictions. It is clear that Huber and Tukey forest
achieve competitive robustness as quantile random forest,
and can almost recover the true underlying distribution, but
random forest is largely impacted by the outliers. We also
repeat the experiments for 20 times, and report the aver-
age mean squared error (MSE), mean absolute deviation
(MAD) and median absolute percentage error (MAPE) in
Table 1.

Forest-type Regression with General Losses and Robust Forest

Table 2. Comparison of the four methods in the setting (1). The
average MSE is reported in ﬁrst row, and average MAD in second
row.

MSE

0%

5%

10%

15%

20%

RF
QRF
HUBER
TUKEY

8.19
9.80
9.02
10.56

12.14
11.63
9.86
12.41

20.32
13.30
10.40
18.16

22.61
13.83
10.49
12.34

25.23
14.71
10.88
16.62

MAD

0%

5%

10%

15%

20%

RF
QRF
HUBER
TUKEY

2.10
2.23
2.20
2.37

2.49
2.37
2.28
2.45

2.73
2.66
2.36
2.54

2.89
2.75
2.38
2.52

3.02
2.84
2.43
2.66

Figure 3. One dimensional comparison of random forest, quan-
tile random forest, Huber forest and Tukey forest. All forests are
ensemble of 500 regression trees and the maximum number of
points in terminal nodes is 20.

Table 1. Comparison of random forest (RF), quantile random for-
est (QRF), Huber forest (Huber) and Tukey forest (Tukey) on one
dimensional example.

MEASURE

RF

QRF HUBER

TUKEY

MSE
MAD
MAPE

2.56
1.20
0.16

1.88
1.07
0.13

1.85
1.06
0.12

1.82
1.07
0.12

Table 3. Comparison of the four methods in the setting (2).

MSE

0%

5%

10%

15%

20%

RF
QRF
HUBER
TUKEY

9.21
11.47
11.19
12.84

13.00
12.07
12.08
13.09

13.69
12.21
12.15
13.31

14.92
12.29
12.20
14.52

17.78
13.16
12.74
14.60

MAD

0%

5%

10%

15%

20%

RF
QRF
HUBER
TUKEY

1.88
2.06
2.04
2.26

2.19
2.13
2.15
2.34

2.74
2.28
2.17
2.39

2.80
2.32
2.17
2.35

2.83
2.41
2.22
2.41

5.2. Multivariate example

We generate data from 10 dimensional Normal distribution,
i.e. X ∼ N10((cid:126)0, Σ). Then we test out algorithms on fol-
lowing models.
(1) Y = (cid:80)10
(2) Y = (cid:80)10
0.7).

i + (cid:15) and (cid:15) ∼ N (0, 1), Σ = I.
i + (cid:15) and (cid:15) ∼ N (0, 1), Σ = Toeplitz(ρ =

i=1 X 2
i=1 X 2

Then for each model, we randomly choose η proportion of
the training samples and add noise 15T2 where T2 follows
t-distribution with degree of freedom 2. The noise level
η ∈ {0, 0.05, 0.1, 0.15, 0.2}. The results are summarized
in Table 2 and 3. On the clean data, random forest still play
the best, however, Huber forest’s performance is also com-
petitive and lose less efﬁciency than QRF and Tukey forest.
On the noisy data, all three robust methods outperform ran-
dom forest. Among them, Huber forest is most robust and
stable.

5.3. Nearest neighbors

In this section, we check how the number of adaptive near-
est neighbors k in (17) will have impact on the performance
of k-RFNN. We consider the same two models (1) and
(2), and keep both training sample size and testing sam-
ple size to be 1000. The relations between MSE, MAD and
the number of adaptive nearest neighbors are illustrated in
Figure 4. Recall that k-RFNN with all 1000 neighbors is
equivalent to random forest. From the ﬁgures, we clearly
observe a kink at k = 15, which is much less than 1000.

5.4. Real data

We take two regression datasets from UCI machine learn-
ing repository (Lichman, 2013), and one real estate dataset
from OpenIntro. For each dataset, we randomly choose 2/3
observations for training and the rest for testing. MSE and
MAD are reported by averaging over 20 trials. The results
are presented in Table 4. To further test the robustness, we
then repeat the experiment but add extra T2 noise to 20%

Forest-type Regression with General Losses and Robust Forest

Table 5. Test on real data sets with extra noise.

MSE

RF

QRF

HUBER

TUKEY

CCS
AIRFOIL
AMES(×108)

68.51
18.22
5.77

39.21
10.04
18.20

39.05
14.28
5.28

40.27
16.55
5.39

MAD

RF

QRF

HUBER

TUKEY

CCS
AIRFOIL
AMES(×104)

5.46
3.45
1.64

4.53
2.30
3.23

4.57
3.08
1.47

4.80
3.17
1.55

expect even better performance after carefully tuning the
parameter.

Besides random forest weights, other data dependent sim-
ilarities could also be used in Algorithm 1. We could also
design loss functions which optimizes a metric for speciﬁc
problems. The ﬁxed-point method could be replaced by
other more efﬁcient algorithms. The framework could be
easily extended to classiﬁcation problems. All these will
be potential future work.

Proof. Because (cid:98)Y (k)(x) = Kδ( (cid:98)Y (k−1)) which is a ﬁxed-
(cid:12)
(cid:12)
(cid:12) < 1 in order
δ(y)
point method, we only need to show
for the existence and uniqueness of the solution. Deﬁne the
normalized weight

(cid:12)
(cid:12)
(cid:12)K

(cid:48)

(cid:101)wi =

(cid:114)

wi
(cid:16) y−Yi
δ

(cid:17)2

1 +

(cid:30) n
(cid:88)

(cid:114)

i=1

1 +

wi
(cid:16) y−Yi
δ

,

(cid:17)2

we have (cid:80)n

i=1 (cid:101)wi = 1, and


(cid:12)
(cid:12)
(cid:12)K

(cid:48)

(cid:12)
(cid:12)
δ(y)
(cid:12)

≤

(cid:101)wiYi



(1I(i = j) − (cid:101)wj)

n
(cid:88)

j=1

y − Yj
δ2 + (y − Yj)2





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 2

(cid:101)wi |Yi| max

i=1,··· ,n

(cid:18)

|y − Yi|
δ2 + (y − Yi)2

(cid:19)

= 2

(cid:101)wi |Yi|

mini=1,··· ,n

1
(cid:16) δ2

(cid:17)
|y−Yi| + |y − Yi|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

n
(cid:88)

i=1
n
(cid:88)

i=1

2 if δ > 2 maxi=1,··· ,n |Yi| = 2K.

Figure 4. The performance of k-RFNN against the number of
nearest neighbors.

of the standardized training data response variables every-
time. The results are in Table 5. Robust forests outperform
random forest in most of the cases except for Ames data
sets, on which quantile random forest behaves poorly.

7. Appendix

7.1. Proof of Lemma 1

Table 4. Comparison of the four methods on two UCI repository
datasets: (1) concrete compressive strength (CCS) (Yeh, 1998);
(2) airfoil self-noise (Airfoil); and one OpenIntro dataset: Ames
residential home sales (Ames).

MSE

RF

QRF

HUBER

TUKEY

CCS
AIRFOIL
AMES(×108)

37.22
18.22
4.51

34.79
10.04
12.21

32.98
14.28
5.22

34.42
16.55
5.91

MAD

RF

QRF

HUBER

TUKEY

CCS
AIRFOIL
AMES(×104)

4.62
3.45
1.34

4.25
2.30
2.44

4.17
3.08
1.31

4.30
3.17
1.36

6. Conclusion and discussion

The experimental results show that Huber forest, Tukey for-
est and quantile random forest are all much more robust
than random forest in the presence of outliers. However,
without outliers, Huber forest preserves more efﬁciency
than the other two robust methods. We did not cross vali-
date the parameter δ for different noise levels, so one would

≤ max

i=1,··· ,n

|Yi|

1
δ

.

Therefore,

(cid:12)
(cid:12)
(cid:12)K

(cid:48)

(cid:12)
(cid:12) < 1
(cid:12)
δ(y)

Forest-type Regression with General Losses and Robust Forest

Acknowledgements

We would like to thank Stan Humphrys and Zillow for sup-
porting this research, as well as three anonymous referees
for their insightful comments. Part of the implementation
in this paper is based on Zillow code library.

References

Aryal, Sunil, Ting, Kai Ming, Haffari, Gholamreza, and
Washio, Takashi. mp-dissimilarity: A data dependent
In Data Mining (ICDM), 2014
dissimilarity measure.
IEEE International Conference on, pp. 707–712. IEEE,
2014.

Breiman, Leo. Random forests. Machine learning, 45(1):

5–32, 2001.

Brence, MAJ John R and Brown, Donald E. Improving the
robust random forest regression algorithm. Systems and
Information Engineering Technical Papers, Department
of Systems and Information Engineering, University of
Virginia, 2006.

Charbonnier, Pierre, Blanc-F´eraud, Laure, Aubert, Gilles,
and Barlaud, Michel. Deterministic edge-preserving reg-
ularization in computed imaging. IEEE Transactions on
image processing, 6(2):298–311, 1997.

Chen, Tianqi and Guestrin, Carlos. Xgboost: A scalable
tree boosting system. In Proceedings of the 22Nd ACM
SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, pp. 785–794. ACM, 2016.

Fan, Jianqing and Gijbels, Irene. Local polynomial mod-
elling and its applications: monographs on statistics and
applied probability 66, volume 66. CRC Press, 1996.

Freund, Yoav and Schapire, Robert E. A desicion-theoretic
generalization of on-line learning and an application to
In European conference on computational
boosting.
learning theory, pp. 23–37. Springer, 1995.

Freund, Yoav, Schapire, Robert E, et al. Experiments with
a new boosting algorithm. In icml, volume 96, pp. 148–
156, 1996.

Hampel, Frank R, Ronchetti, Elvezio M, Rousseeuw, Pe-
ter J, and Stahel, Werner A. Robust statistics: the ap-
proach based on inﬂuence functions, volume 114. John
Wiley & Sons, 2011.

Hastie, Trevor and Loader, Clive. Local regression: Auto-
matic kernel carpentry. Statistical Science, pp. 120–129,
1993.

Huber, Peter J. Robust statistics. Springer, 2011.

Huber, Peter J et al. Robust estimation of a location pa-
rameter. The Annals of Mathematical Statistics, 35(1):
73–101, 1964.

Jarvis, Raymond Austin and Patrick, Edward A. Clustering
using a similarity measure based on shared near neigh-
bors. IEEE Transactions on computers, 100(11):1025–
1034, 1973.

Koenker, Roger. Quantile regression. Number 38. Cam-

bridge university press, 2005.

Li, Alexander Hanbo and Bradic, Jelena. Boosting in the
presence of outliers: adaptive classiﬁcation with non-
convex loss functions. Journal of the American Statis-
tical Association, (just-accepted), 2016.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Lin, Dekang et al. An information-theoretic deﬁnition of
similarity. In ICML, volume 98, pp. 296–304. Citeseer,
1998.

Lin, Yi and Jeon, Yongho. Random forests and adaptive
nearest neighbors. Journal of the American Statistical
Association, 101(474):578–590, 2006.

Loader, Clive. Local regression and likelihood. Springer

Science & Business Media, 2006.

Mason, Llew, Baxter, Jonathan, Bartlett, Peter L, and
Frean, Marcus R. Boosting algorithms as gradient de-
scent. In NIPS, pp. 512–518, 1999.

Meinshausen, Nicolai. Quantile regression forests. Journal
of Machine Learning Research, 7(Jun):983–999, 2006.

Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
The elements of statistical learning, volume 1. Springer
series in statistics Springer, Berlin, 2001.

Newey, Whitney K. Kernel estimation of partial means and
a general variance estimator. Econometric Theory, 10
(02):1–21, 1994.

Friedman, Jerome H. Greedy function approximation: a
gradient boosting machine. Annals of statistics, pp.
1189–1232, 2001.

Roy, Marie-H´el`ene and Larocque, Denis. Robustness of
random forests for regression. Journal of Nonparametric
Statistics, 24(4):993–1006, 2012.

Galimberti, Giuliano, Pillati, Marilena, and Soffritti,
Gabriele. Robust regression trees based on m-estimators.
Statistica, 67(2):173–190, 2007.

Ruppert, David and Wand, Matthew P. Multivariate locally
weighted least squares regression. The annals of statis-
tics, pp. 1346–1370, 1994.

Forest-type Regression with General Losses and Robust Forest

Shi, Tao and Horvath, Steve. Unsupervised learning with
random forest predictors. Journal of Computational and
Graphical Statistics, 15(1):118–138, 2006.

Staniswalis, Joan G. The kernel estimate of a regres-
Journal of
sion function in likelihood-based models.
the American Statistical Association, 84(405):276–283,
1989.

Tibshirani, Robert and Hastie, Trevor. Local likelihood es-
timation. Journal of the American Statistical Associa-
tion, 82(398):559–567, 1987.

Ting, Kai Ming, Zhu, Ye, Carman, Mark, Zhu, Yue,
and Zhou, Zhi-Hua. Overcoming key weaknesses of
distance-based neighbourhood methods using a data
In Proceedings of
dependent dissimilarity measure.
the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pp. 1205–1214.
ACM, 2016.

Tsymbal, Alexey, Pechenizkiy, Mykola, and Cunningham,
P´adraig. Dynamic integration with random forests. In
European conference on machine learning, pp. 801–808.
Springer, 2006.

Yeh, I-C. Modeling of strength of high-performance con-
crete using artiﬁcial neural networks. Cement and Con-
crete research, 28(12):1797–1808, 1998.

