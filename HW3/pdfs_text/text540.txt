On the Iteration Complexity of Support Recovery
via Hard Thresholding Pursuit

Jie Shen 1 Ping Li 1

Abstract

2010; Blumensath & Davies, 2009; Bouchot et al., 2016).

O

Recovering the support of a sparse signal from
its compressed samples has been one of the most
important problems in high dimensional statis-
tics. In this paper, we present a novel analysis for
the hard thresholding pursuit (HTP) algorithm,
showing that it exactly recovers the support of
an arbitrary s-sparse signal within
(sκ log κ)
iterations via a properly chosen proxy function,
where κ is the condition number of the problem.
In stark contrast to the theoretical results in the
literature, the iteration complexity we obtained
holds without assuming the restricted isometry
property, or relaxing the sparsity, or utilizing the
optimality of the underlying signal. We further
extend our result to a more challenging scenario,
where the subproblem involved in HTP cannot be
solved exactly. We prove that even in this setting,
support recovery is possible and the computa-
tional complexity of HTP is established. Numer-
ical study substantiates our theoretical results.

1. Introduction

In the last two decades, pursuing a sparse representation
for high dimensional data has become one of the most sig-
niﬁcant problems in machine learning. To seek a sparse
solution, a large body of work is devoted to efﬁcient meth-
ods, including the convex formulation, for instance, ba-
sis pursuit (Chen et al., 1998) and the Lasso (Tibshirani,
1996), as well as greedy pursuits, e.g., orthogonal match-
iterative hard threshold-
ing pursuit (Pati et al., 1993),
ing (Daubechies et al., 2004) and hard thresholding pur-
suit (HTP) (Foucart, 2011), along with elegant theoretical
understanding on parameter estimation and support recov-
ery in either ideal setting or noisy scenario (Cand`es & Tao,
2005; Wainwright, 2009; Tropp & Gilbert, 2007; Cai et al.,

1Rutgers University, Piscataway, New Jersey, USA. Jie Shen:

js2007@rutgers.edu, Ping Li: pingli@stat.rutgers.edu.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Compared to parameter estimation, i.e., bounding the ℓ2
distance between the solution and the desired sparse sig-
nal, support recovery is a much more challenging task and
it usually requires more stringent conditions. See Tropp
(2004); Zhao & Yu (2006); Yuan & Lin (2007); Zhang
(2009) for some early results and Nguyen & Tran (2013);
Loh & Wainwright (2014) for more recent developments.
Nevertheless, if the support of a signal can be predicted by
a method, then the solution returned by the method imme-
diately enjoys the oracle property, i.e., with optimal statisti-
cal rate (Wainwright, 2009). Thereby, support recovery has
received broad attention in recent years (Osher et al., 2016;
Wang et al., 2016; Bouchot et al., 2016).

In this work, we follow the research line with a particular
interest in the hard thresholding pursuit algorithm, which
exhibits encouraging performance among many machine
learning applications. The algorithm was originally pre-
sented by Foucart (2011) for recovering the true signal in
compressed sensing (Donoho, 2006). Yuan et al. (2014)
suggested using the HTP algorithm for general sparsity-
constrained machine learning problems, and they showed
that the solution obtained from HTP converges with a geo-
metric rate. Very recently, a rigorous theoretical analysis on
when HTP guarantees support recovery was independently
carried out by Bouchot et al. (2016) and Yuan et al. (2016).
In Bouchot et al. (2016), they considered the compressed
sensing problem and illustrated that HTP recovers the sup-
port of the true signal in ﬁnite iterations if the restricted
isometry property (RIP) condition holds (Cand`es & Tao,
2005). Yuan et al. (2016) showed that in some situations,
HTP eventually terminates and guarantees support recov-
ery without assuming the RIP condition.

Although these appealing theoretical results characterize
the behavior of HTP in particular regimes, it turns out
that a thorough understanding on when HTP identiﬁes the
support of an arbitrary sparse signal is missing in the lit-
erature. To be more precise, the RIP condition used in
Bouchot et al. (2016) amounts to imposing a small condi-
tion number for the underlying problem, which may not be
practical for machine learning applications where the con-
dition number usually grows with the sample size. To guar-

Support Recovery of Hard Thresholding Pursuit

antee the support recovery of an s-sparse signal, Yuan et al.
(2016) required that the signal of interest is the unique
global minimizer of a sparsity-constrained program (which
invokes the RIP condition), or that HTP maintains denser
iterates. This poses an interesting question of whether HTP
is able to recover the support without the RIP assumption,
or the optimality of the signal, or the relaxed sparsity.

In addition, an insightful analysis on the performance of
HTP in a realistic scenario is missing. For concreteness,
recall that HTP proceeds as follows:

(HTP1) bt+1 = xt
−
(HTP2) St+1 = supp
(HTP3) xt+1 = arg min

η

∇
bt+1, k

F (xt),

,

(cid:0)
supp(x)⊂St+1

F (x),
(cid:1)

bt+1, k

where η > 0 is a step size, supp
denotes the sup-
port of the k largest absolute elements of bt+1 and F (x)
is a properly chosen function. For general machine learn-
ing problems, we are only guaranteed with ǫ-approximate
solutions in the third step, i.e., for all t

0,

(cid:0)

(cid:1)

F (xt+1)

F (xt+1
∗

)

−

≥

ǫ,

≤

∗

where xt+1
is the global minimizer of F (x) restricted on
St+1. Related to the inexact solutions, a natural question
to ask is how the accuracy parameter ǫ affects the recovery
performance of HTP, additively or progressively.

Another issue coming up with the inexact iterates is that
the usually employed stopping criterion St+1 = St may
not be valid, which makes part of the analysis in Yuan et al.
(2016) not applicable to this setting. Note that when exact
solutions are available, HTP becomes stationary as soon as
the detected support does not change, since the solutions
are entirely determined by the support. Yuan et al. (2016)
made use of this feature to establish theoretical guarantee
for HTP. However, allowing approximate iterates quickly
changes the premise because many stochastic solvers, e.g.,
stochastic gradient descent, introduce randomness, render-
ing (HTP3) outputs different results even restricted on the
same support set.

1.1. Contribution

We make the following contribution in this paper. First,
suppose that (HTP3) has exact solutions, we show that un-
der very mild conditions, HTP either terminates early or
guarantees support recovery of an arbitrary s-sparse signal
within
(sκ log κ) iterations. Then we move on to the in-
exact case, and prove that under the RIP condition or using
a relaxed sparsity, support recovery with the same itera-
tion complexity holds provided that the optimization error
ǫ is small compared to the magnitude of the target signal.
As a consequence, we present the ﬁrst bound on the com-
putational complexity of HTP. For concreteness, we relate

O

our deterministic results to two prevalent statistical models,
and show that the conditions involved in our theorems can
be met with high probability.

We also revisit the role of F (x) of the HTP algorithm. Pre-
vious work, for example, Jain et al. (2014), tends to treat
F (x) as an objective function, the choice of which depends
on the underlying problem and the signal, and views HTP
as an optimization procedure towards the optimal solution.
Interestingly, we ﬁnd that F (x) behaves more like a proxy
function that guides HTP to the target signal. Hence, to re-
cover a signal, we have many more choices of F (x) as far
as it satisﬁes the conditions to be present (see Section 4).

From a high level, the paper shares the same merit of
Bouchot et al. (2016); Yuan et al. (2016), i.e., recovering a
sparse signal. Hence, part of our proof is inspired by their
work. Yet, we establish novel RIP-free results based on a
more careful analysis for the problem structure. See a de-
tailed comparison in Section 3.

1.2. Notation

Throughout the paper, we use bold lowercase letters, e.g.,
v, to denote a column vector. The support of a vector v
is denoted by supp (v), whereas that of the largest k ab-
solute elements is denoted by supp (v, k). Both
k0 and
are used to count the non-zeros in v. Suppose
supp (v)
|
Rd, vΩ
is an index set, then for v
that Ω
1, 2, . . . , d
}
can either be explained as an
-dimensional vector or a
d-dimensional vector with the elements outside of Ω set to
zero. The Euclidean norm of a vector v is denoted by
.
k
We write boldface capital letters, e.g., A, for matrices, and
the transpose is denoted by A⊤.

|
⊂ {

Ω

∈

v

v

k

k

|

|

∈

Rd is the target signal we aim to
The s-sparse vector ¯x
recover, and we reserve the capital letter S for its support.
We deﬁne ¯xmin > 0 as the absolute value of the smallest
Rs. With a slight abuse
element (in magnitude) of ¯xS ∈
∇kF (¯x) should be explained as the vector
of the notation,
F (¯x)
consisting of the top k elements (in magnitude) of
F (¯x).
rather than the kth component of

∇

∇

1.3. Roadmap

The remainder of the paper is organized as follows. Sec-
tion 2 introduces the problem setting and some preliminary
results that the main theorems build on. Section 3 presents
the main results of this paper with a detailed comparison
In Section 4, we specialize our
to closely related work.
results to two concrete statistical models. A proof sketch
of the main results is given in Section 5. Next, we ver-
ify our theoretical results with extensive numerical study
in Section 6 and Section 7 concludes the paper. Technical
lemmas and the full proof are deferred to the appendix (see
the supplementary ﬁle).

Support Recovery of Hard Thresholding Pursuit

2. Problem Setup and Preliminary Results

In this section, we introduce the problem setting and some
preliminary consequences on which our main results build.
Rd we consider in this
To be clear, the target signal ¯x
paper is only endowed with sparsity.

∈

Our analysis depends on the following two properties of the
function F (x).
Deﬁnition 1. A differentiable function F (x) is said to be
restricted strongly convex (RSC) with parameter mK > 0,
if for all vectors x and x′ with

x′

x

K,

k

−
x′

k0 ≤
mK

F (x)

F (x′)

F (x′), x

−

− h∇

−

i ≥

2 k

−

x

x′

2 .

k

Deﬁnition 2. A differentiable function F (x) is said to be
restricted smooth (RSS) with parameter MK > 0, if for all
x′
vectors x and x′ with

K,

x
k

−

k0 ≤

F (x)

F (x′)

F (x′), x

x′

−

− h∇

−

i ≤

2 k

−

x′

2 .

k

MK

x

In particular, we require that the RSC condition holds at
sparsity level k + s and the RSS condition holds at sparsity
level 2k, respectively. That is,

(A1) F (x) is mk+s-restricted strongly convex;

(A2) F (x) is M2k-restricted smooth.

Note that the RSC and RSS conditions are now stan-
dard and are widely utilized for establishing performance
guarantees for a variety of popular algorithms. See, for
example, Negahban et al. (2009); Agarwal et al. (2012);
Jain et al. (2014) and Loh & Wainwright (2014). For sim-
plicity, throughout the paper we write m := mk+s and
M := M2k. We also denote κ = M/m which is actually
the (restricted) condition number of the problem.

The ﬁrst result states that if (HTP3) outputs exact solutions,
then HTP decreases the function value with a geometric
rate before the stopping criterion (i.e., St+1 = St) is met.
Formally, we have the following proposition.

Proposition 1. Consider the HTP algorithm with exact so-
lutions in (HTP3). Assume (A1) and (A2), pick η < 1/M
in (HTP1) and set k = s in (HTP2). Then before HTP
terminates, it holds that for all t

0,

F (xt+1)

F (¯x)

µ

−

≤

where

≥
F (xt)

(cid:0)

F (¯x)

,

−

(cid:1)

2ηm(1

ηM )

µ = 1

−

−
1 + s

(0, 1).

∈

holds even for F (xt)
−
tioning that by the proposition, we can deduce

F (¯x) < 0. It is also worth men-

F (xt)

F (¯x)

µt

F (x0)

F (¯x)

.

−

≤

−

(cid:0)

(cid:1)

However, the above inequality does not imply the conver-
F (¯x) is not bounded
gence of
from below. Rather, it is invoked to establish parameter
estimation for HTP.

}t≥0, since F (xt)

F (xt)

−

{

The following proposition shows that when the conditions
in Prop. 1 are satisﬁed, we have an accurate estimate on the
signal in the ℓ2 metric.
Proposition 2. Assume same conditions as in Prop. 1.
0:
Then before HTP terminates, the following holds for t

≥

xt

¯x

−

≤

√2κ(√µ)t

x0

¯x

+

3
m k∇k+sF (¯x)
k

,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
where µ is given in Prop. 1.

(cid:13)
(cid:13)

−

(cid:13)
(cid:13)

In the literature, a variety of work has established the-
oretical guarantees on parameter estimation, either under
the RIP condition (Bouchot et al., 2016) or by relaxing the
sparsity (Yuan et al., 2016). In contrast, neither of the con-
ditions are assumed in Prop. 2, owing to a careful analysis
F (xt) and ¯x. See the supple-
on the connection between
∇
mentary ﬁle for the proof. However, we point out that such
an appealing behavior is not guaranteed if (HTP3) does not
output exact solutions, and in this case, we have to relax
the sparsity or use the RIP condition. In particular, let

xt

∗ = arg min
supp(x)⊂St

F (x),

and consider that (HTP3) outputs xt obeying

supp

xt

St, F (xt)

F (xt
∗)

ǫ.

≤

−

(1)

⊂

(cid:0)

(cid:1)

Note that this is a realistic scenario because even for sim-
ple functions, e.g., F (x) is the logistic loss, convex solvers
only ensure ǫ-approximate solutions. The major issue com-
ing up with the ǫ-approximate solutions is that the gradient
of F (x) evaluated at xt does not vanish on the support St,
which makes our technical analysis of Prop. 2 invalid. Yet,
we can still bound it under proper conditions.

Lemma 3. Assume (A2) and (1). Then at any iteration
t

0, we have

≥

∇St F (xt)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Based on the lemma, we show the following RIP-based re-
sult for parameter estimation.

√2M ǫ.

≤

Remark. Note that we did not assume the optimality of ¯x
with respect to the function F (x). In other words, Prop. 1

Proposition 4. Consider the HTP algorithm with inexact
solutions (1). Suppose that the condition number κ < 1.25

Support Recovery of Hard Thresholding Pursuit

and set k = s in (HTP2). Then picking η = η′/M with
κ

0.25 < η′ < 1 guarantees

−

xt

¯x

−

≤

(√2(κ

η′))t

x0

−

¯x

−

(cid:13)
(cid:13)

(cid:13)
(cid:13)

+

6κ
m k∇k+sF (¯x)
k

(cid:13)
(cid:13)

+

4√M ǫ
(cid:13)
(cid:13)
m

.

As the RIP condition is hard to fulﬁll for many machine
learning problems, Jain et al. (2014) proposed to relax the
in order to alleviate it.
sparsity parameter k =
Shen & Li (2016) further showed that by relaxing the spar-
(cid:0)
sity, a stochastic solver is able to produce an accurate so-
lution for sparsity-constrained programs. Inspired by their
interesting work, we derive the following result for HTP.

κ2s

O

(cid:1)

Proposition 5. Consider the HTP algorithm with inexact
η2m2 . Then
solutions (1). Pick η < 1/M and let k

2s + 8s

≥

xt

¯x

−

≤

(cid:13)
(cid:13)

(cid:13)
(cid:13)

√2κ(√µ)t

x0

¯x

−
(cid:13)
3
(cid:13)
m k∇k+sF (¯x)
k

(cid:13)
(cid:13)
+

+

4ǫ

−

,

µ)

m(1

s

where

µ = 1

−

ηm(1

ηM )

−
2

.

3. Main Results

This section is dedicated to a deterministic analysis on the
performance of HTP. We ﬁrst treat the exact case, i.e.,
(HTP3) outputs exact solutions, along with a detailed com-
parison with previous work in the literature. Then we
demonstrate that even when (HTP3) is solved approxi-
mately up to an ǫ-accuracy, support recovery is still pos-
sible provided that ǫ is small enough compared to the mag-
nitude of the target signal.

The following theorem is one of the main results in the pa-
per. It justiﬁes that under proper conditions, HTP recovers
the support of ¯x using ﬁnite iterations.
Theorem 6. Consider the HTP algorithm with exact solu-
tions in (HTP3). Assume (A1) and (A2). Pick η < 1/M in
(HTP1) and k = s in (HTP2). Then HTP either terminates
early, or recovers the support of ¯x using at most

tmax =

(cid:18)

3 log κ
log(1/µ)

+

2 log(2/(1

λ))

−
log(1/µ)

+ 2

¯x

k0 (2)

k

iterations, provided that for some constant λ

(cid:19)
(0, 1)

∈

¯xmin ≥
Above, the quantity µ is given by

2√2 + √κ
mλ

k∇k+sF (¯x)
k

.

(3)

2mη(1

ηM )

µ = 1

−

−
1 + s

(0, 1).

∈

In the theorem, we recall that ¯xmin is the minimum ab-
solute value of the non-zeros of ¯x. Below we discuss the
important messages conveyed by the theorem and contrast
our result to prior work. For ease of exposition, we write
η = η′/M for some constant η′
(0, 1), and it quickly
(1/κ).
indicates that µ = 1

∈

− O

Iteration complexity. We remind that the ﬁrst term in (2)
plays the most crucial role, since it upper bounds the other
two for sufﬁciently large κ. In the regime where κ itself is
bounded by a constant from above, the iteration complexity
¯x
is simply explained as
k0). Asymptotically, we can
show that the iteration complexity is dominated by κ log κ
as κ tends to inﬁnity, that is,

(
k

O

(

O

tmax =

k0 κ log κ) .

¯x
k
This follows from a simple calculation on the Taylor expan-
sion of log(1/µ) at the point x = 1, with µ being replaced
with 1
(1/κ). Note that the number of iterations we
obtained for support recovery is as few as that for accurate
parameter estimation (see Prop. 2). It is also worth men-
tioning that the linear dependency on the sparsity of ¯x is
nearly optimal, because in the worst case HTP may take
several steps to pick only one correct support.

− O

Conditions. We also emphasize that the condition (3) is
now ubiquitous for analyzing the support recovery perfor-
mance. The quantity ¯xmin involved is natural, because
a signal with large magnitude is easier to recover than
those with small or vanishing components. To see why
is used to lower bound the magnitude of ¯x,
k∇k+sF (¯x)
k
let us consider the compressed sensing problem as an ex-
ample. Suppose that we observe the response vector y,
which obeys y = A¯x + e for a given design matrix A
and some noise e. In order to recover the true parameter
¯x, we may choose F (x) as the least-squares, of which the
derivative evaluated at x = ¯x is given by

F (¯x) = A⊤ (A¯x

y) =

A⊤e.

−

−

∇

Then the RIP condition asserts that

k∇k+sF (¯x)

k ≥

1

δk+s k

e

k

,

−

p

(0, 1) is the (k + s)-th restricted isometry
where δk+s ∈
constant (Cand`es & Tao, 2005). Therefore, imposing the
condition (3) amounts to distinguishing the true signal from
the observation noise.

Comparison to prior work. We contrast our result to the
state-of-the-art work of Yuan et al. (2016). To recover a
sparse signal ¯x, Yuan et al. (2016) required the condition
number κ < 1.14, which might be too restrictive to gen-
eral machine learning problems where the condition num-
ber grows with sample size. In addition, support recovery
was established only for a carefully chosen F (x), i.e., ¯x

Support Recovery of Hard Thresholding Pursuit

Table1. Comparison to previous work on HTP-style algorithm. We present the ﬁrst support recovery guarantee for an arbitrary
sparse signal without assuming the RIP condition or relaxing the sparsity.

Result

Target sparse signal RIP-free No sparsity relaxation

Support recovery

Foucart (2011)
Yuan et al. (2014)
Jain et al. (2014)
Bouchot et al. (2016)
Yuan et al. (2016, Theorem 1)
Yuan et al. (2016, Theorem 3)
Proposed Theorem 6

true signal
arbitrary
optimal solution
true signal
optimal solution
arbitrary
arbitrary

✗
✗
✓
✗
✗
✓
✓

✓
✗
✗
✓
✓
✗
✓

✗
✗
✗
✓
✓
✓
✓

−

Ax

y
k

must be the unique global minimizer of F (x) subject to
a sparsity constraint (see Theorem 1 therein). Such a re-
quirement dramatically excludes many popular and simple
choices of F (x). For example, let us again examine the
compressed sensing problem. With the presence of noise,
it is almost impossible for ¯x to be the global optimum of
2. Hence, one cannot apply the theoret-
F (x) =
k
ical result of Yuan et al. (2016) to justify the performance
of HTP. In comparison, our theorem ensures that support
recovery is possible as far as the selected F (x) fulﬁlls the
condition (3). Though Theorem 3 in Yuan et al. (2016)
does not assume the RIP condition or the optimality of ¯x
with respect to F (x), it requires a relaxed sparsity parame-
ter k =
, whereas the proposed Theorem 6 asserts
that k = s sufﬁces. We also note that iteration complexity
was not provided by Yuan et al. (2016) in the relaxed spar-
sity case, whereas we clearly state the dependency on all
the parameters.

κ2s

O

(cid:0)

(cid:1)

Compared with Bouchot et al. (2016), it is not hard to see
that the problem considered here is more general, since we
aim to recover an arbitrary sparse signal while they targeted
the true parameter of compressed sensing. Bouchot et al.
(2016) also imposed the RIP condition that is not invoked
here. Jain et al. (2011; 2014) presented HTP-style algo-
rithms with analysis on parameter estimation, but a guaran-
tee on support recovery was not considered. We summarize
the comparison in Table 1.

Weakness. We remark that though Theorem 6 is free of the
RIP condition and the relaxed sparsity, it implicitly requires
that HTP should not terminate too early. Otherwise, HTP
may fail to recover the support. We believe that it is a very
interesting future direction to give a lower bound on the
iteration complexity of HTP. In the sequel, we strengthen
our result by providing sufﬁcient conditions which prevent
HTP from early stopping.

In particular, we move on to the practical scenario where
the results to be established also apply to the exact case.
As a reminder, due to the assumption (A1), (HTP3) is vir-
tually solving a convex program. Yet, since F (x) is a gen-

eral function, (HTP3) can only be solved approximately by,
e.g., gradient descent (Nesterov, 2004), stochastic gradi-
ent descent (Bottou & Bousquet, 2007), or the more recent
variance reduced variant (Johnson & Zhang, 2013). The
question to ask is, whether support recovery is possible un-
der such a “noisy” setting, and how the optimization accu-
racy ǫ enters the conditions for this end.

The following theorem presents an afﬁrmative answer,
though the RIP condition is assumed.
Theorem 7. Consider
the HTP algorithm with ǫ-
approximate solutions in (HTP3). Assume (A1) and (A2).
Suppose that the condition number κ < 1.25. Pick η =
0.25 < η′ < 1 and set k = s in (HTP2).
η′/M with κ
Then HTP recovers the support of ¯x using at most

−

tmax =

log κ
log(1/µ)

+

 

log(√2/(1

λ))

−
log(1/µ)

+ 2

¯x

k0

! k

iterations, provided that for some constant λ

(0, 1)

¯xmin ≥

√2 + 3√2κ
mλ

k∇k+sF (¯x)
k

+

Above, the quantity µ is given by

µ = √2(κ

η′)

(0, √2/4).

−

∈

∈

4
mλ

√M ǫ.

(4)

O

(
k

Since the condition number is assumed to be well bounded,
it follows that the iteration complexity is a constant multi-
k0). By examining the ¯xmin
¯x
ple of the sparsity, i.e.,
condition (4), we ﬁnd that the optimization error ǫ does not
propagate in a progressive manner. Rather, it enters the
condition as an additive error. By comparing (4) to (3), the
exact case, one may argue that (4) is more stringent because
it requires ¯xmin ≥ O
while (3) imposes
(κ)
k∇k+sF (¯x)
¯xmin ≥ O
. Yet, we point out that The-
k
orem 7 is based on the RIP condition, i.e., κ < 1.25. So it
is not appropriate to examine the asymptotic behavior for
the condition (4).

k∇k+sF (¯x)
k

(√κ)

Finally, we study under which RIP-free conditions can
HTP guarantee support recovery in the face of approximate
solutions. We have the following result.

Support Recovery of Hard Thresholding Pursuit

Theorem 8. Consider
the HTP algorithm with ǫ-
approximate solutions in (HTP3). Assume (A1) and (A2).
η2m2 in (HTP2). Then
Pick η < 1/M and let k
HTP recovers the support of ¯x using at most

2s + 8s

≥

tmax =

3 log κ
log(1/µ)

+

 

4 log(√2/(1

λ))

−
log(1/µ)

iterations, provided that for some constant λ

¯x

k0

+ 2

! k
(0, 1)

∈

¯xmin ≥

2√2 + √κ
mλ

+ λ−1

−
Above, the quantity µ is given by

 s

k∇k+sF (¯x)
k
2
m(1

µ)

+

2
m

r

!

κ

√ǫ.

(5)

ηm(1

ηM )

µ = 1

−

−
2

(0, 1).

∈

To be clear, due to sparsity relaxation, Theorem 8 only en-
In Yuan et al.
sures support inclusion, i.e., S
(2016), they showed that under the condition

Stmax.

⊂

¯xmin > 1.62

2(F (¯x)

F (x∗))

,

−
m

r

HTP terminates with output xt satisfying supp (xt, s) =
S. However, the iteration number t was not given. Either,
F (x∗) is,
it is not clear how large the difference F (¯x)
where x∗ is the global s-sparse minimizer of F (x) and we
recall that ¯x is an arbitrary signal.

−

In contrast to Theorem 7, the quantity √ǫ here is multiplied
by the condition number κ, which will consume more com-
putational resources in order to fulﬁll the condition. This is
not surprising because enlarging the support increases the
chance of detecting the support but as a price, it also in-
troduces more noise. Fortunately, under the RSC and RSS
assumptions, ﬁrst order solvers converges linearly. For in-
stance, after
(κ log(1/ǫ)) steps, gradient descent guaran-
tees an ǫ-approximate solution.

O

In view of the existing results from convex optimiza-
tion (Nesterov, 2004), together with Theorem 8, we can
show that the total computational complexity of HTP is

d + κ2s log d + κ3s log(1/ǫ)

sκ log κ.

(6)

(d) operations
To see this, note that (HTP1) consumes
(cid:0)
(cid:1)
(k log d). Using gradient descent
and (HTP2) costs
O
(kκ log(1/ǫ)).
to solve (HTP3) results in a complexity
, we ob-
Combining them together and noting k =
tain the above.

O
O

κ2s

O

(cid:0)

(cid:1)

We point out that though Theorem 6 and Theorem 7 need to
know the sparsity s, one can set k to be a quantity smaller
than s. In this case, it follows from our analysis that HTP
recovers the support of the top-k elements. Interested read-
ers may refer to Lemma 19 for more details.

4. Statistical Results

In this section, we relate our main results, Theorem 6 to
Theorem 8, to concrete statistical models. In particular, we
study two prevalent models:
the sparse linear regression
and the sparse logistic regression.

The sparse linear regression model is in essence the one
considered in the compressed sensing community. It as-
sumes that the given response vector y obeys y = A¯x + e,
for a known design matrix A, a true sparse parameter ¯x (to
be estimated) and an unknown noise e. In order to estimate
the signal ¯x, many researchers (e.g., Jain et al. (2014)) con-
sidered the following formulation:

min
x∈Rd

F (x) :=

y
k

−

Ax

2 , s.t.
k

x
k

k0 ≤

s,

and attempted to prove that the (near) optimal solution of
the above program is close enough to ¯x. Yet, it turns
out that we can use more ﬂexible functions F (x), e.g.,
2. To see this, by standard
F (x) =
x
k
k
results (e.g., Vershynin (2010); Shen & Li (2016)), we are
guaranteed that when the entries of A and those of e are
i.i.d. sub-gaussian,

2 + α

Ax

−

y

k

k

k∇k+sF (¯x)

k ≤ O

N −1(k + s) log d

+ α

¯x
k

k

(cid:16)p

(cid:17)

O

N −1(k + s) log d

holds with high probability, where N is the sample size.
, we have
Hence, by picking α =
k∇k+sF (¯x)
vanishes as N increases. In light of such an
(cid:16)p
k
observation and our theorems (speciﬁcally the ¯xmin con-
ditions), we ﬁnd that it is not the sparsity-constrained pro-
gram matters. Rather, it is a properly chosen F (x) that
guides HTP to the target signal.

(cid:17)

The logistic regression model is used for binary classi-
It has been shown in a number of work (see,
ﬁcation.
k∇k+sF (¯x)
is bounded from
e.g., Yuan et al. (2014)) that
k

N −1(k + s) log d

above by
with high probability,
assuming the data is i.i.d. sub-gaussian. Again, we can
add an ℓ2 regularizer to the logistic loss to make it strongly
convex, without loss of the support recovery guarantee.

(cid:16)p

O

(cid:17)

Relating these statistical results to our theorems, we con-
clude that the ¯xmin conditions involved can be satisﬁed
with high probability as soon as the sample size N grows
with (k + s) log d. Moreover, under the same conditions,
the condition number κ is well bounded from above, say
κ < 9, implying a constant iteration complexity
k0)
and a fast computation (see the complexity in (6)). We also
remark that in light of the many more choices of F (x), the
function F (x) essentially acts as a proxy that guides HTP
to the target signal, rather than an objective function being
optimized by HTP.

¯x
k

O

(

5. Proof Sketch

Si:K. Thus,

Support Recovery of Hard Thresholding Pursuit

Our main results, Theorem 6 to Theorem 8, are proved by
mathematical induction. The key idea is partitioning the
support set S into several disjoint subsets S1, S2, . . . , SK
according to the magnitude of the elements (Zhang, 2011;
Bouchot et al., 2016). Then we show that after a few itera-
Sn1.
tions, say n1, HTP identiﬁes the ﬁrst subset, i.e., S1 ⊂
Given this, we further examine how many iterations are
needed to include the ﬁrst two subsets. And we inductively
show that after n1 + n2 · · ·
+ ni steps, the support set pro-
duced by HTP contains the ﬁrst i number of subsets, i.e.,
Sn1+n2···+ni. We then show that each
Si ⊂
S2 · · · ∪
S1 ∪
ni is small, and the sum of them is upper bounded by a
¯x
multiple of
k0. Hence, two components are important to
k
this end. First, we need to construct the subsets properly,
and second, we need to offer an estimate on the ni’s which
should be small enough.

Without loss of generality, suppose that the elements of ¯x
are arranged in descending order. Then each subset Si is
inductively constructed as follows:

Si =

si−1 + 1, . . . , si}

{

, 1

i

K,

≤

≤

where s0 = 0 and for all 1
largest index such that

i

≤

≤

K, si is deﬁned as the

¯xsi |

|

>

1
√2

¯xsi−1+1

.

(cid:12)
(cid:12)
Note that the constant 1/√2 can be replaced with any other
quantity smaller than 1. Since si is the largest one, it fol-
lows that

(cid:12)
(cid:12)

¯xsi+1| ≤

|

1
√2

¯xsi−1+1

,

(cid:12)
(cid:12)

which immediately implies

¯x{si−1+1,...,s}

2(¯xsi )2Si:K,

where

(cid:13)
(cid:13)

(cid:12)
(cid:12)

2

≤

(cid:13)
(cid:13)

K−i

Si:K :=

2−j

Si+j|

|

.

j=0
X
Then we show that given the above and the condition S1 ∪
Sn1+n2···+ni−1 , as soon as HTP decreases
S2 · · · ∪
the distance to ¯x with a geometric rate (which is the theme
of Section 2), we are guaranteed that S1 ∪
Si ⊂
Sn1+n2···+ni. Here, ni is given by

S2 · · · ∪

Si−1 ⊂

> α

βni

Si:K + θ,

¯xsi |

|

·

for some parameters α, β and θ. Now assuming θ <
¯xmin ≤ |
implies that ni is as small as the logarithm of

¯xsi |

p

K

K

tmax =

ni ≤

log Si:K ≤

K log

i=1
X

i=1
X

1
K

K

i=1
X

Si:K.

The result follows by doing some calculation on the sum of
Si:K’s. See the full proof in the supplementary ﬁle.

6. Numerical Study

The HTP algorithm has been studied for several years and
has found plenty of successful applications. There is also a
large volume of empirical study, e.g., Bouchot et al. (2016),
showing that HTP performs better in terms of computa-
tional efﬁciency and parameter estimation than compres-
sive sampling matching pursuit (Needell & Tropp, 2009),
subspace pursuit (Dai & Milenkovic, 2009), iterative hard
thresholding (Blumensath & Davies, 2009), to name a few.
Hence, the focus of our numerical study is to verify the
theoretical ﬁndings in Section 3.

Data. In order to investigate the performance of HTP with
both the exact and inexact solutions, we consider the lin-
ear regression model y = A¯x + σe, where ¯x is a 100-
dimensional vector with a tunable sparsity s. The elements
in the design matrix A and the noise e are i.i.d. normal
variables. The response y is an N -dimensional vector. For
a certain sparsity level s, the support of ¯x is chosen uni-
formly and the non-zero components of ¯x are i.i.d. normal
variables. If not speciﬁed, we set N = 100 and σ = 0.01.

Evaluation metric. In the experiments, we are mainly in-
terested in examining the percentage of successful support
recovery and the iteration number that guarantees it. We
mark a trial as success if before HTP terminates, there is a
solution xt satisfying supp (xt) = supp (¯x). Otherwise,
we mark it as failure. The iteration number is counted only
for those success trials and we report the averaged result.

Solvers. We choose the least-squares loss as the proxy
function F (x), for which an exact solution can be com-
puted in (HTP3). We also implement the gradient de-
scent (GD) algorithm to approximately solve (HTP3). In
order to produce solutions with different accuracy ǫ, we run
the GD algorithm with a various number of gradient oracle
calls. In this way, we are able to examine how ǫ affects
support recovery through the number of oracle calls.

Other settings. The step size η in HTP is ﬁxed as η =
1. We use the true sparsity for the sparsity parameter k in
(HTP2). For each conﬁguration of sparsity, we generate
100 independent copies of ¯x. Hence, all the experiments
are performed with 100 trials.

A notable aspect of our theoretical results is that after
(sκ log κ) iterations, HTP captures the support. For the
O
purpose of justiﬁcation, we vary the sparsity s from 1 to 50,

Support Recovery of Hard Thresholding Pursuit

Exact
GD−10
GD−20
GD−50
GD−100
GD−200

s
n
o
i
t
a
r
e
t
i
#

10

8

6

4

2

 

0
1

 

100

s
s
e
c
c
u
s
 
f
o
 
e
g
a
t
n
e
c
r
e
p

80

60

40

20

 

0
1

 

Exact
GD−10
GD−20
GD−50
GD−100
GD−200

s
n
o
i
t
a
r
e
t
i
#

6

5

4

3

2

 

1

 

100

 

Exact
GD−10
GD−20
GD−50
GD−100
GD−200

s
s
e
c
c
u
s
 
f
o
 
e
g
a
t
n
e
c
r
e
p

80

60

40

20

 

0
1

Exact
GD−10
GD−20
GD−50
GD−100
GD−200

10

20
#non−zeros

30

40

50

10

20
#non−zeros

30

40

50

20

40

60

80

100

#measurements

20

40

60

80

100

#measurements

Figure1. Iteration number and percentage of success against
the sparsity. The number of measurements N = 100. GD–“T ”
means we run the gradient descent algorithm for T steps. As pre-
dicted by our theorem, the iteration number is nearly proportional
to the sparsity (left panel). Note that using approximate solutions
does not affect the iteration complexity. From the right panel,
we observe that gradient descent with 50 steps already ensures
comparable performance to the exact solution, possibly due to the
geometric convergence rate of gradient descent.

and plot the curve of the iteration number used to identify
the support against the true sparsity s. Note that we use the
same design matrix for all trials, hence a ﬁxed condition
number κ. The result is recorded in the left panel of Fig-
ure 1. As predicted by our theorem, the iteration number is
(almost) linear with the sparsity. Interestingly, we also ﬁnd
that HTP uses far fewer steps than expected. For example,
to recover the support of a 20-sparse signal, 4 iterations
sufﬁce in average, suggesting possible improvement of our
theorems in special cases. Also note that for a given spar-
sity level, applying an inexact solver for (HTP3) does not
increase the iteration number of HTP. This is not surpris-
ing since our theorem states that the optimization error in
(HTP3) only enters the ¯xmin condition. In other words, it
only affects the percentage of success as shown in the right
panel of Figure 1. Thanks to the linear convergence of gra-
dient descent, it turns out that using 50 calls of gradient
oracle guarantees an appealing performance.

s log d).

Next, we tune the number of measurements N from 1 to
100, and study the support recovery performance against
the choice of N . Here, the sparsity level s is ﬁxed to
s = 5. With the sub-gaussian design, standard result
shows that the condition number can be upper bounded
See, for exam-
by (C1N + s log d)/(C2N
−
ple, Jain et al. (2014). This indicates that the condition
number is inversely proportional to N after a proper shift-
ing, and hence the iteration number. The curves on the left
panel of Figure 2 matches our assertion. In the right panel,
a phase transition emerges (Donoho & Tanner, 2010). That
is, above a certain threshold (here the threshold is 20), sup-
port recovery is guaranteed with high probability while be-
low that threshold, we have no hope to estimate the sig-
nal. We also ﬁnd that when sufﬁcient measurements are
available, running GD with 10 gradient oracle calls already
brings desirable performance.

Figure2. Iteration number and percentage of success against
the number of measurements. The sparsity s = 5. GD–“T ”
means we run the gradient descent algorithms for T steps. The
left panel shows that the more measurements we have, the faster
we detect the support. The rationale is that the condition num-
ber becomes smaller with additional measurements, and by our
theorem, we need fewer iterations. The right panel shows a phase
transition phenomenon: when we have 20 or more measurements,
HTP guarantees support recovery with high probability while sup-
port recovery is impossible if we do not have sufﬁcient samples.
Again, running GD with 50 gradient oracle calls produces similar
result with the exact solution.

We remind that in Figure 1 and Figure 2, some values of
#iterations are not plotted. For example, we do not have
the iteration number for GD–50 in Figure 1 when s
45.
This is simply because all the trials are marked as failure.
See the associated percentage of success curve.

≥

Now let us return to the ¯xmin condition of Theorem 8, i.e.,
Eq. (5). From Figure 1 and Figure 2, we conclude that
as far as the optimization error is small enough, HTP with
inexact iterates behaves comparably to that with exact solu-
tions. For example, the “GD–200” curve (black solid) and
the “Exact” curve (red dashed) in these two ﬁgures actually
lie on top of each other even the RIP condition is not met
(small N or large s). This suggests that the relaxed sparsity
condition in Theorem 8 may not be vital.

7. Conclusion and Future Work

In this paper, we have studied the iteration complexity of
the hard thresholding pursuit algorithm for recovering the
support of an arbitrary s-sparse signal. We have shown that
if the iterates of HTP are exact solutions, HTP recovers
the support within
(sκ log κ) iterations where κ is the
O
In a more practical machine learning
condition number.
setting, we have proved that even with inexact solutions,
support recovery is still possible with the same iteration
bound. We have also investigated two popular statistical
models, and have established probabilistic arguments under
the standard sub-gaussian design. The numerical study has
conﬁrmed the correctness of our theoretical ﬁndings.

Orthogonal to the present work, an interesting direction for
future study is establishing a lower bound on the iteration
complexity of HTP for support recovery. It is also interest-
ing to investigate the performance on realistic datasets.

Support Recovery of Hard Thresholding Pursuit

Acknowledgements

The work is supported in part by NSF-Bigdata-1419210
and NSF-III-1360971. We thank the anonymous review-
ers for pointing out several related work and for suggesting
improvement on the proof sketch and the experiments. We
also thank Jian Wang and Jing Wang for valuable discus-
sion on the work.

References

Agarwal, Alekh, Negahban, Sahand, and Wainwright, Mar-
tin J. Fast global convergence of gradient methods for
high-dimensional statistical recovery. The Annals of
Statistics, 40(5):2452–2482, 2012.

Blumensath, Thomas and Davies, Mike E. Iterative hard
thresholding for compressed sensing. Applied and Com-
putational Harmonic Analysis, 27(3):265–274, 2009.

Bottou, L´eon and Bousquet, Olivier. The tradeoffs of large
scale learning. In Proceedings of the 21st Annual Con-
ference on Neural Information Processing Systems, pp.
161–168, 2007.

Bouchot, Jean-Luc, Foucart, Simon, and Hitczenko, Pawel.
Hard thresholding pursuit algorithms: number of itera-
tions. Applied and Computational Harmonic Analysis,
41(2):412–435, 2016.

Cai, Tony T., Wang, Lie, and Xu, Guangwu. New bounds
for restricted isometry constants. IEEE Trans. Informa-
tion Theory, 56(9):4388–4394, 2010.

Cand`es, Emmanuel J. and Tao, Terence. Decoding by linear
programming. IEEE Trans. Information Theory, 51(12):
4203–4215, 2005.

Chen, Scott Shaobing, Donoho, David L., and Saunders,
Michael A. Atomic decomposition by basis pursuit.
SIAM Journal on Scientiﬁc Computing, 20(1):33–61,
1998.

Dai, Wei and Milenkovic, Olgica. Subspace pursuit for
compressive sensing signal reconstruction. IEEE Trans.
Information Theory, 55(5):2230–2249, 2009.

Daubechies, Ingrid, Defrise, Michel, and Mol, Chris-
tine De. An iterative thresholding algorithm for linear
inverse problems with a sparsity constraint. Communi-
cations on Pure and Applied Mathematics, 57(11):1413–
1457, 2004.

Donoho, David L. Compressed sensing. IEEE Trans. In-

formation Theory, 52(4):1289–1306, 2006.

Donoho, David L. and Tanner, Jared. Precise undersam-
pling theorems. Proceedings of the IEEE, 98(6):913–
924, 2010.

Foucart, Simon. Hard thresholding pursuit: An algorithm
for compressive sensing. SIAM Journal on Numerical
Analysis, 49(6):2543–2563, 2011.

Jain, Prateek, Tewari, Ambuj, and Dhillon, Inderjit S. Or-
In Pro-
thogonal matching pursuit with replacement.
ceedings of the 25th Annual Conference on Neural In-
formation Processing Systems, pp. 1215–1223, 2011.

Jain, Prateek, Tewari, Ambuj, and Kar, Purushottam. On it-
erative hard thresholding methods for high-dimensional
M-estimation. In Proceedings of the 28th Annual Con-
ference on Neural Information Processing Systems, pp.
685–693, 2014.

Johnson, Rie and Zhang, Tong. Accelerating stochastic
gradient descent using predictive variance reduction. In
Proceedings of the 27th Annual Conference on Neural
Information Processing Systems, pp. 315–323, 2013.

Loh, Po-Ling and Wainwright, Martin J. Support recovery
without incoherence: A case for nonconvex regulariza-
tion. CoRR, abs/1412.5632, 2014.

Needell, Deanna and Tropp, Joel A. CoSaMP: Iterative
signal recovery from incomplete and inaccurate samples.
Applied and Computational Harmonic Analysis, 26(3):
301–321, 2009.

Negahban, Sahand, Ravikumar, Pradeep, Wainwright,
Martin J., and Yu, Bin. A uniﬁed framework for high-
dimensional analysis of M -estimators with decompos-
In Proceedings of the 23rd Annual
able regularizers.
Conference on Neural Information Processing Systems,
pp. 1348–1356, 2009.

Nesterov, Yurii. Introductory lectures on convex optimiza-
tion, volume 87. Springer Science & Business Media,
2004.

Nguyen, Nam H. and Tran, Trac D. Robust lasso with miss-
ing and grossly corrupted observations. IEEE Trans. In-
formation Theory, 59(4):2036–2058, 2013.

Nguyen, Nam H., Needell, Deanna, and Woolf, Tina. Lin-
ear convergence of stochastic iterative greedy algorithms
with sparse constraints. CoRR, abs/1407.0088, 2014.

Osher, Stanley, Ruan, Feng, Xiong, Jiechao, Yao, Yuan,
and Yin, Wotao. Sparse recovery via differential inclu-
sions. Applied and Computational Harmonic Analysis,
41(2):436–469, 2016.

Support Recovery of Hard Thresholding Pursuit

Pati, Yagyensh C., Rezaiifar, Ramin, and Krishnaprasad,
Perinkulam S. Orthogonal matching pursuit: Recur-
sive function approximation with applications to wavelet
In Conference Record of The Twenty-
decomposition.
Seventh Asilomar Conference on Signals, Systems and
Computers, pp. 40–44. IEEE, 1993.

Shen, Jie and Li, Ping. A tight bound of hard thresholding.

CoRR, abs/1605.01656, 2016.

Tibshirani, Robert. Regression shrinkage and selection via
the Lasso. Journal of the Royal Statistical Society: Se-
ries B (Methodological), pp. 267–288, 1996.

Tropp, Joel A. Greed is good: algorithmic results for
sparse approximation. IEEE Trans. Information Theory,
50(10):2231–2242, 2004.

Tropp, Joel A. and Gilbert, Anna C. Signal recovery
from random measurements via orthogonal matching
pursuit. IEEE Trans. Information Theory, 53(12):4655–
4666, 2007.

Vershynin, Roman.

Introduction to the non-asymptotic
analysis of random matrices. CoRR, abs/1011.3027,
2010.

Wainwright, Martin J.

Sharp thresholds for high-
dimensional and noisy sparsity recovery using ℓ1-
IEEE
constrained quadratic programming (Lasso).
Trans. Information Theory, 55(5):2183–2202, 2009.

Wang, Jian, Kwon, Suhyuk, Li, Ping, and Shim, Byonghyo.
Recovery of sparse signals via generalized orthogonal
matching pursuit: A new analysis. IEEE Trans. Signal
Processing, 64(4):1076–1089, 2016.

Yuan, Ming and Lin, Yi. On the non-negative garrotte esti-
mator. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 69(2):143–161, 2007.

Yuan, Xiao-Tong, Li, Ping, and Zhang, Tong. Gradient
hard thresholding pursuit for sparsity-constrained opti-
mization. In Proceedings of the 31st International Con-
ference on Machine Learning, pp. 127–135, 2014.

Yuan, Xiao-Tong, Li, Ping, and Zhang, Tong. Exact recov-
ery of hard thresholding pursuit. In Proceedings of the
30th Annual Conference on Neural Information Process-
ing Systems, pp. 3558–3566, 2016.

Zhang, Tong. On the consistency of feature selection us-
ing greedy least squares regression. Journal of Machine
Learning Research, 10:555–568, 2009.

Zhang, Tong. Sparse recovery with orthogonal matching
pursuit under RIP. IEEE Trans. Information Theory, 57
(9):6215–6221, 2011.

Zhao, Peng and Yu, Bin. On model selection consistency of
lasso. Journal of Machine Learning Research, 7:2541–
2563, 2006.

